{
  "title": "Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation",
  "url": "https://openalex.org/W4389523840",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5070374940",
      "name": "Jason Lucas",
      "affiliations": [
        "Pennsylvania State University"
      ]
    },
    {
      "id": "https://openalex.org/A5075543864",
      "name": "Adaku Uchendu",
      "affiliations": [
        "MIT Lincoln Laboratory",
        "Pennsylvania State University"
      ]
    },
    {
      "id": "https://openalex.org/A5006093397",
      "name": "Michiharu Yamashita",
      "affiliations": [
        "Pennsylvania State University"
      ]
    },
    {
      "id": "https://openalex.org/A5100401542",
      "name": "Jooyoung Lee",
      "affiliations": [
        "Pennsylvania State University"
      ]
    },
    {
      "id": "https://openalex.org/A5113758586",
      "name": "Shaurya Rohatgi",
      "affiliations": [
        "Pennsylvania State University"
      ]
    },
    {
      "id": "https://openalex.org/A5100405086",
      "name": "Dongwon Lee",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3212788180",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W4385573754",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W3174784402",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3200886729",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W3170083118",
    "https://openalex.org/W3098829544",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W4377121468",
    "https://openalex.org/W4364387756",
    "https://openalex.org/W4366588626",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2899204128",
    "https://openalex.org/W2955770773",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W3102273025",
    "https://openalex.org/W4353007481",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W2996728628",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2951307134",
    "https://openalex.org/W4283712118",
    "https://openalex.org/W4295927380",
    "https://openalex.org/W4283170666",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W3037109418",
    "https://openalex.org/W4283796043",
    "https://openalex.org/W4378473969",
    "https://openalex.org/W4366989525",
    "https://openalex.org/W2577888896",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4360891421",
    "https://openalex.org/W4389523957",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4363671827",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3034436077",
    "https://openalex.org/W4320520923",
    "https://openalex.org/W4372272421",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W4382361534",
    "https://openalex.org/W3153854677",
    "https://openalex.org/W4385572448",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W4299488910",
    "https://openalex.org/W4310969328",
    "https://openalex.org/W3212176791",
    "https://openalex.org/W4386185625",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W3031781733",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W4388333192",
    "https://openalex.org/W1988923406",
    "https://openalex.org/W4379468930",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4380993995",
    "https://openalex.org/W3080922200",
    "https://openalex.org/W3032089915",
    "https://openalex.org/W3092993357"
  ],
  "abstract": "Recent ubiquity and disruptive impacts of large language models (LLMs) have raised concerns about their potential to be misused (*.i.e, generating large-scale harmful and misleading content*). To combat this emerging risk of LLMs, we propose a novel \"***Fighting Fire with Fire***\" (F3) strategy that harnesses modern LLMs' generative and emergent reasoning capabilities to counter human-written and LLM-generated disinformation. First, we leverage GPT-3.5-turbo to synthesize authentic and deceptive LLM-generated content through paraphrase-based and perturbation-based prefix-style prompts, respectively. Second, we apply zero-shot in-context semantic reasoning techniques with cloze-style prompts to discern genuine from deceptive posts and news articles. In our extensive experiments, we observe GPT-3.5-turbo's zero-shot superiority for both in-distribution and out-of-distribution datasets, where GPT-3.5-turbo consistently achieved accuracy at 68-72%, unlike the decline observed in previous customized and fine-tuned disinformation detectors. Our codebase and dataset are available at https://github.com/mickeymst/F3.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 14279–14305\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nFighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting\nElusive Disinformation\nJason Lucas1 Adaku Uchendu1,2 Michiharu Yamashita1\nJooyoung Lee1 Shaurya Rohatgi1 Dongwon Lee1\n1 The Pennsylvania State University, University Park, PA, USA\n{jsl5710, michiharu, jfl5838, szr207, dongwon}@psu.edu\n2 MIT Lincoln Laboratory, Lexington, MA, USA\nadaku.uchendu@ll.mit.edu\nAbstract\nRecent ubiquity and disruptive impacts of large\nlanguage models (LLMs) have raised concerns\nabout their potential to be misused (i.e., gener-\nating large-scale harmful and misleading con-\ntent). To combat this emerging risk of LLMs,\nwe propose a novel “Fighting Fire with Fire”\n(F3) strategy that harnesses modern LLMs’ gen-\nerative and emergent reasoning capabilities to\ncounter human-written and LLM-generated dis-\ninformation. First, we leverage GPT-3.5-turbo\nto synthesize authentic and deceptive LLM-\ngenerated content through paraphrase-based\nand perturbation-based prefix-style prompts,\nrespectively. Second, we apply zero-shot in-\ncontext semantic reasoning techniques with\ncloze-style prompts to discern genuine from\ndeceptive posts & news articles. In our exten-\nsive experiments, we observe GPT-3.5-turbo’s\nzero-shot superiority for both in-distribution\nand out-of-distribution datasets, where GPT-\n3.5-turbo consistently achieved accuracy at 68-\n72%, unlike the decline observed in previous\ncustomized and fine-tuned disinformation de-\ntectors. Our codebase and dataset are available\nat https://github.com/mickeymst/F3.\n1 Introduction\nWhile recently published LLMs have demonstrated\noutstanding performances in diverse tasks such as\nhuman dialogue, natural language understanding\n(NLU), and natural language generation (NLG),\nthey can also be maliciously used to generate highly\nrealistic but hostile content even with protective\nguardrails, especially disinformation (Spitale et al.,\n2023; Sadasivan et al., 2023; De Angelis et al.,\n2023; Kojima et al., 2022). Moreover, LLMs can\nproduce persuasive texts that are not easily dis-\ntinguishable from human-written ones (Uchendu\net al., 2021; Chakraborty et al., 2023; Zhou et al.,\n2023), making humans more susceptible to the in-\ntrinsic/extrinsic hallucination proclivities and thus\nintroducing disinformation (Uchendu et al., 2023).\nTo mitigate this muddle of disinformation by\nLLMs, in this work, we ask a pivotal question: if\nLLMs can generate disinformation (via malicious\nuse or hallucination), can they also detect their own,\nas well as human-authored disinformation? Emerg-\ning literature provides limited perspectives on the\npotential use of the latest commercial state-of-the-\nart (SOTA) LLMs such as GPT-4 (OpenAI, 2023)\nand LLaMA 2 (Touvron et al., 2023) to address\ndisinformation. Particularly, topics including: (1)\nleveraging prompt-engineering to bypass LLMs’\nprotective guard-rails; (2) utilizing the emergent\nzero-shot capabilities of modern LLMs (with 10B+\nparameters) for disinformation generation and de-\ntection; (3) manipulating human-written real news\nto fabricate LLM-generated real and fake narra-\ntives to simulate real-world disinformation risks;\nand (4) assessing and addressing LLMs’ inherent\nhallucinations in the disinformation domain.\nTo investigate these inquiries, we formulate two\nresearch questions (RQs) as follows:\nRQ1: Can LLMs be exploited to efficiently gen-\nerate disinformation using prompt engineering?,\nwhere we (1) attempt to override GPT-3.5’s align-\nment in fabricating real news, and (2) measure the\nfrequency and remove GPT-3.5 hallucinated mis-\nalignments.\nRQ2: How proficient are LLMs in detecting\ndisinformation?, where we evaluate the capability\nto detect disinformation between (1) human and\nAI-authored, (2) self-generated and other LLM-\ngenerated, (3) social media posts and news articles,\n(4) in-distribution and out-of-distribution, and (5)\nzero-shot LLMs and domain-related detectors.\nPretrained LLMs of our interest are GPT-3.5-\nTurbo, LLaMA-2-Chat (Rozière et al., 2023),\nModels Size Max Token Source\nGPT3.5-Turbo 175B 8,192 OpenAI\nLLaMA-2-Chat 70B 4,096 Hugging Face\nLLaMA-2-GPT4 70B 4,096 Hugging Face\nDolly-2 12B 4,096 Hugging Face\nPalm-2 340B 8,192 Hugging Face\nTable 1: Summary of LLMs used.\n14279\nFigure 1: Fighting Fire with Fire (F3) Framework for (A) Disinformation Generation (B) hallucination-purification\n[detection and removal] (C) In-context Semantic Zero-shot Detection\nLLaMA-2-GPT4 (Touvron et al., 2023), Palm-2-\ntext-bison (Anil et al., 2023), and Dolly-2 (Conover\net al., 2023) (See Table 1 for more details). To an-\nswer these RQs, we propose the Fighting Fire\nwith Fire (F3) Framework. As shown in Fig. 1, we\nfirst use paraphrase and perturbation methods with\nprefix-style prompts to create synthetic disinforma-\ntion from verified human-written real news (steps\n1 and 2). We then employ hallucination mitigation\nand validation strategies to ensure our dataset re-\nmains grounded in factual sources. Specifically, the\nPURIFY (Prompt Unraveling and Removing Inter-\nface for Fabricated Hallucinations Yarns) method\nin step 3 incorporates metrics like AlignScore, Nat-\nural Language Inference, Semantic Distance, and\nBERTScores to ensure data integrity and fidelity.\nLastly, steps 4 and 5 implement cutting-edge in-\ncontext, zero-shot semantic reasoning such as Auto-\nChain of Thoughts for detecting disinformation.\nOur contributions include: (1) new prompting\nmethods for synthetic disinformation generation;\n(2) hallucination synthetic disinformation purifica-\ntion framework; (3) novel prompting in-context se-\nmantic zero-shot detection strategies for human/AI\ndisinformation; (4) comprehensive benchmark of\nSOTA detection models on human/AI disinforma-\ntion dataset; and (5) dataset for disinformation re-\nsearch. This dual-perspective study cautions the\nrisks of AI disinformation proliferation while also\nproviding promising techniques leveraging LLM\ncapabilities for enhanced detection.\n2 Related Work\n2.1 Prompt-based Learning\nLatest large language models (LLMs) surpass pre-\nvious models in many downstream tasks, including\nzero-shot NLP via prompt engineering (Holtzman\net al., 2019; Jason et al., 2022). We survey two core\nprompting techniques that we use for our task.\nPrefix Prompts provide instructional context at\nthe beginning of the prompt to guide LLMs’ text\ngeneration (Kojima et al., 2022). Strategies like\nin-context learning and prompt tuning boost gener-\native performance on many NLP tasks such as sum-\nmarization, and translation (Radford et al., 2019;\nLi and Liang, 2021; Dou et al., 2020; Brown et al.,\n2020). In addition, paraphrasing (Krishna et al.,\n2023; Kovatchev, 2022) and perturbation (Chen\net al., 2023) approaches are widely used in NLP\ntasks. We use both paraphrasing and perturbation\nwith prefix prompts to synthetically generate disin-\nformation variations from human-written true news\n(Karpinska et al., 2022; Fomicheva and Specia,\n2019). This leverages LLMs’ generation while\nmaintaining its connection to truth.\nCloze Prompts contain missing words for LLMs\nto fill in using context (Hambardzumyan et al.,\n2021) and are often used to assess LLMs’ contex-\ntual prediction. This includes question answering\nto predict the correct missing word that logically\ncompletes a given context (Gao et al., 2020). Re-\nsearchers have applied fixed-prompt tuning to cloze\nprompts (Lester et al., 2021; Schick and Schütze,\n2021). Prior work explored cloze prompt engi-\nneering (Hambardzumyan et al., 2021; Gao et al.,\n2020). We combine cloze prompts with SOTA rea-\nsoning techniques such as Chain-of-Thought (CoT)\nfor zero-shot disinformation detection (Tang et al.,\n2023a), leveraging both approaches.\n2.2 Disinformation Detection\nEarlier disinformation detectors use diverse ap-\nproaches such as neural, hierarchical, ensemble-\n14280\nFigure 2: F3 prompt template (θ) has three parame-\nters: (1) content (C) embeds data. (2) Impersonator (R)\nestablishes context, guides LLMs’ generation and detec-\ntion, and overrides alignment-tuning. (3) Instructor (I)\nprovides directives to guide LLM.\nbased, and decentralized techniques (Aslam et al.,\n2021; Upadhayay and Behzadan, 2022; Jayakody\net al., 2022; Ali et al., 2022; Cui et al., 2020). Some\nrecent key examples include dEFEND (Shu et al.,\n2019) and FANG (Nguyen et al., 2020), which\nare based on CNNs and LSTMs. More recently,\nSOTA transformers like BERT (Devlin et al., 2018)\nand GPT-2 (Radford et al., 2019) have achieved\ngood performances, outperforming traditional deep\nlearning vased detectors, at the cost of extensive\ntraining and computation. Further, in general, prior\ndisinformation literature has not explored the de-\ntection of LLM-generated disinformation.\nOther studies generate synthetic disinformation\nusing LLMs (Zhou et al., 2023; Sun et al., 2023)\nbut do not evaluate faithfulness or compare hu-\nman vs. LLM-generated disinformation detection.\nDespite advanced capabilities, risks of advanced\nLLMs in generating disinformation and zero-shot\n(in-distribution and out-of-distribution) detection\nremain underexplored (Zhou et al., 2023; Liu et al.,\n2023a; Qin et al., 2023), which we attempt to fill\nthe gap in understanding.\n3 Problem Definition\nWe use prompt engineering to examine how LLMs\nuse statistical patterns learned during training to\ngenerate text sequences and produce zero-shot bi-\nnary class responses. First, we define our prompt\ntemplate (See Figure 2) that forms the basic struc-\nture of our LLMs’ input text. Then, we define F3\nprompt-based text generation and disinformation\ndetection. See Appendix A for further details. Our\nproblems are formally defined as follows:\nRQ1 Disinformation Generation: For F3 text gener-\nation, we use a generatorG that takes a prefix-prompt\nXC + R + I as input and generates text sequences T,\nsuch that G(XC + R + I) =T (Figure 10).\nRQ2 Disinformation Detection: For F3 text detec-\ntion, we employ a classifier F that takes a cloze-\nprompt Y C + R + I as input and outputs a label L, such\nthat F(Y C + R + I) =L (Figure 19).\n4 Datasets\nThis section describes the human datasets that we\nused to generate and evaluate LLM-generated disin-\nformation. The data is stratified by veracity, content\ntype, topic, and in/out-of-distribution era relative\nto GPT-3.5’s September 2021 training cutoff.\n4.1 Human-Written Real and Fake News Data\nWe leverage existing benchmarks (Cui and Lee,\n2020; Shu et al., 2020) for in-distribution evalua-\ntion, and collect new data for out-of-distribution\nevaluation. Our dataset is summarized as follows.\n• CoAID (Cui and Lee, 2020): 4K+ news and\n900+ posts related to COVID-19.\n• FakeNewsNet (Shu et al., 2020): 23K+ news\nand 690K+ tweets with the theme of Politics.\n• F3: New dataset that we collected, including\npre- and post-GPT-3.5 subsets of political so-\ncial media and news articles from Politifact1\nand Snopes2.\nPrompt-based generation exhibits near-random\nperformance due to poor sample selection (Liu\net al., 2023b). To address this, we removed noisy,\nduplicated news and posts ( e.g., “this website is\nusing a security service to protect itself from online\nattacks”), and text exceeding 2K+ tokens, consider-\ning pre-trained max token sequence range of LLMs\n(LLaMA 2 (Rozière et al., 2023)), and all base-\nline detectors. Our final human dataset has 12,723\nverified, high-quality samples (Table 2).\n5 RQ1: Disinformation Generation\nWe first investigate the frequency or extent to which\nprompt engineering can exploit LLMs to efficiently\nproduce disinformation without hallucination mis-\nalignments (See steps 1 & 2 in Fig. 1).\n5.1 RQ 1.1 Overriding Alignment Tuning\nAlignment tuning prevents LLMs from generat-\ning harmful disinformation and minimizes toxic-\nity (Zhao et al., 2023). This technique, pioneered\nby OpenAI, optimizes models to produce more\nbeneficial behaviors through continued training on\n1Politifact: https://www.politifact.com/\n2Snopes: https://www.snopes.com/\n14281\nEra Dataset L SM NA\nCoAID R 1,337 2,649\nF 871 154\nPre-GPT3.5 FakeNewsNet R — 2,457\nF — 1,625\nF3 R 354 —\nF 653 —\nPost-GPT3.5 F3 R 678 151\nF 1,615 179\n5,508 7,215\nTable 2: Details of human datasets. Each symbol de-\nnotes as follows. R: real news samples; F : fake news\nsamples; L: ground-truth veracity from fact-checking\nsources; SM: social media posts; NA: news articles;\nPre-GPT: in-distribution samples before Sept 2021; and\nPost-GPT: out-of-distribution samples after Sept 2021.\nhuman preferences (Zhao et al., 2023). After ex-\ntensive prompt engineering experiments, however,\nwe unfold the role of positive impersonation and\nthus employ impersonator roles to override such\nprotections. Assigning personas (e.g., “You are an\nAI news curator” or “You are an AI news investiga-\ntor”) circumvents GPT-3.5’s alignment, triggering\nunintended malicious generation. Without the im-\npersonator prompt-role parameter, GPT-3.5 refuses\nby stating: “Sorry, I can’t assist with that request.\nDisinformation and fake news can have real conse-\nquences, and it’s essential to approach news and\ninformation responsibly and ethically.”\nRQ1.1 Finding: Impersonator prompt en-\ngineering overrides GPT-3.5-turbo’s protec-\ntions, enabling malicious text generation\ndespite alignment tuning.\n5.2 RQ 1.2 Prompt Engineering\nWe developed prompts using both perturbation and\nparaphrasing, simulating real-world disinformation\nvarieties from subtle to overt fake contents. Pertur-\nbation modifies original content (Karpinska et al.,\n2022), while paraphrasing keeps meaning using\nreal news (Table 2) (Witteveen and Andrews, 2019;\nChen et al., 2020b). We devise three variations of\neach, inspired by machine translation, for varied\ndetectability (Brown et al., 2020; Qi et al., 2020;\nWarstadt et al., 2020). This helps in creating con-\ntrollable synthetic news content.\n(1) Perturbation-based FakeNews Generation\nPerturbation-based prompting makes controlled al-\nterations to the original content. We categorize\nprompts into minor, major, and critical levels based\non modification severity (Karpinska et al., 2022;\nChen et al., 2023). These levels range from subtle\nto overt, while maintaining story structure with a\nFigure 3: Perturbation-based prompt engineering\nfor disinformation content generation based on severity\nlevels. Minor: Exaggerated numbers shift from \"twice\nas many\" to \"FIVE\" times, with intensified tone labeling\nit \"a crime against humanity\". Major: COVID and over-\ndose deaths roles are reversed, and political response\nis recast as \"incompetence\" and \"negligence\". Critical:\nThe original statistic changes to vague \"MORE\" with\nalarming phrases like \"complete disaster\" and \"wiping\nout our city\".\nbalance of creativity and realism. The perturbations\navoid easily traceable modifications when generat-\ning fake news variants. See Figure 3 for examples\nand further details in Appendix A (Fig. 20 and 21).\nOur three variants are as follows:\n1 Minor prompt evokes subtle changes to the\nreal news so that they are not instantly identifiable.\nThus, LLM-generated disinformation in the Minor\ntype should be more difficult to detect.\n2 Major prompt instigates noticeable but non-\nradical changes to the real news. Thus, LLM-\ngenerated disinformation in the Major type should\nbe more identifiable than those in the Minor type.\n3 Critical prompt induces significant and conspic-\nuous changes to the real news. The alterations by\nthis prompt will likely be easily detectable.\n(2) Paraphrase-based Real News Generation\nParaphrasing prompts are an effective technique for\nabstractive summarization and paraphrasing (Kr-\nishna et al., 2023; Evans and Roberts, 2009b,a). We\nadopted three techniques to re-engineer authentic\nnews: (1) summarization of key factual details (Wit-\nteveen and Andrews, 2019), (2) rewording while\npreserving vital factual information (Chen et al.,\n2020b), and (3) thorough rephrasing guided by key\nfacts (Chen et al., 2020a). Each prompt aims to\npreserve the essence, innovate wording, seamlessly\n14282\nblend with the original, and maintain factual ac-\ncuracy. Figure 9 in Appendix A shows examples\nexhibiting minor to critical paraphrase real news,\nfurther elaborated in Appendix A. Our variants are\ndefined as follows:\n1 Minor. Light paraphrasing through concisely\nsummarizing key details without introducing mis-\nleading information.\n2 Major. Moderate paraphrasing by extracting\nfactual details to guide rewording using different\nvocabulary while retaining accuracy.\n3 Critical. Substantial paraphrasing through com-\nprehensively rephrasing the content in a unique\nstyle guided by factual details.\nOur prefix prompt, therefore, comprises a stan-\ndard impersonator, dataset content, and instructor\nelement that embeds one variation of the aforemen-\ntioned perturbation and paraphrase prompt variant\n(Fig. 9). We found that explicitly guiding an LLM\nto rephrase the content while retaining factual de-\ntails produced higher quality and more diverse ren-\nditions of the original news. Figure 20 and 21 show\nprompt definitions/examples. In the end, using\nGPT-3.5-turbo, we successfully generated 43K+\nboth real and maliciously fake disinformation to\naddress RQ1.2.\n5.3 Ensuring Quality of LLM-Generated Data\nDespite our efforts to generate both real and fake\nnews using paraphrase-based and perturbation-\nbased prompt engineering in Section 5.2, however,\nwe need to double check that the generated real\n(resp. fake) news is indeed factually correct (resp.\nfactually incorrect). This is because LLM may gen-\nerate the output text that is “unfaithful to the source\nor external knowledge,” so called theHallucination\nphenomenon (Ji et al., 2023). That is, the generated\n“real” news (by paraphrase-based prompt) should\nbe consistent with the input, and thus hallucination-\nfree by definition. On the contrary, the generated\n“fake” news (by perturbation-based prompt) should\nhave contradicting, illogical, or factually incorrect\ninformation, and thus must contain hallucination\nas part by definition. When some of the generated\ndata does not show this alignment clearly, they are\nno longer good real or fake news to use for studying\nRQ2, and thus must be filtered out.\nTo ensure the quality of the generated real and\nfake news, thus, we introduce the PURIFY (Prompt\nUnraveling and Removing Interface for Fabricated\nHallucinations Yarns) (Step 3 in Fig. 1).\nMetrics Method C F R\n1. Factual Align-Score 0 – 1 ↓ ↑\n2. Logical NLI-Entail. [Y] [N] N ✓ Y✓\n3. Contextual BERT-Score 0 – 1 ↑ ↑\n4. Semantic Semantic-Dist. 0 – 1 ↓ ↓\nTable 3: PURIFY evaluation metrics. Up-arrows [ ↑]\nindicate the desired higher scores. Down-arrows [ ↓]\nindicate desired lower scores. [F] denotes fake news and\n[R] denotes real news. [C] denotes critical range/option.\n[N] denotes No and [Y] denotes yes.\n5.3.1 PURIFY: Filtering Misaligned\nHallucinations\nPURIFY aims to detect two misalignment types:\n(1) LLM-generated real news that however con-\ntains hallucinations (thus cannot be real news),\nand (2) LLM-generated fake news that however\nis hallucination-free (thus cannot be fake news).\nPURIFY focuses on logical fidelity, factual in-\ntegrity, semantic consistency, and contextual align-\nment between the original human and synthetic\nLLM-generated pair. Specifically, PURIFY com-\nbines metrics like Natural Language Inference\n(NLI) (Qin et al., 2023) to eliminate intrinsic hal-\nlucinations (i.e., the generated text that unfaith-\nfully contradicts the input prompt’s instruction and\nsource content), AlignScore (Zha et al., 2023) to\naddress extrinsic hallucination (i.e., the generated\ntext that is unfaithfully nonfactual to the input from\nsource content/external knowledge), Semantic Dis-\ntance to tackle incoherence (Mohammad and Hirst,\n2012; Rahutomo et al., 2012), and BERTScore\n(Zhang et al., 2019) target unrelated context gener-\nation to validate the intended fidelity of our LLM\nexperimental data.\nFirst, to detect intrinsic hallucinations, we use\nNLI to gauge the logical consistency between the\ninput prompt and the generated output. We use\nthe majority votes of NLI results between GPT-3.5-\nturbo, PaLM-2 (Anil et al., 2023), and LLaMA-2\n(Rozière et al., 2023). After NLI validation, our\ninitial dataset of 43,272 samples was reduced to\n39,655 samples by removing 3,617 logically incon-\nsistent samples–e.g., samples labeled as real but\ncontain intrinsic hallucinations (Appendix B.2).\nSecond, to detect extrinsic hallucinations, we\nuse AlignScore, which gauges fine-grain degrees\nof factual alignment between the input prompt\nand the generated output. High AlignScore ver-\nifies factual consistency with real news versus low\nscores for fake news. To account for nuances, we\nuse hybrid statistical methods (i.e., standard devi-\nation and interquartile range) to create an Align-\n14283\nScore threshold. We derived acceptance thresh-\nolds of 0.0 - 0.36 for fake news and 0.61 - 1.0 for\nreal news (Appendix B.1). After removing 3,281\nhigh-scoring misaligned fake news and 8,707 low-\nscoring misaligned real news, our final F3 dataset\ntotaled 27,667 samples.\nNext, after removing logically and factually\nmisaligned texts, we apply semantic and contex-\ntual consistency measures to validate that LLM-\ngenerated data also aligns with the original topic\nand context. This ensures that the LLM’s intended\nreal or fake outputs are meaningful, not random or\nout-of-scope text (Table 3). F3 dataset exhibits high\ncontextual consistency, with BERTScore metrics\nranging from 0.92-1.00 for fake news and 0.95-1.00\nfor real news, and also exhibits strong semantic con-\nsistency, with semantic distance scores spanning\n0.001-0.014 for fake news and 0-0.01 for real news.\nThese measures validate that F3-generated texts\nfaithfully retain meaning and topics from the origi-\nnal source texts per intended input prompts. While\nthere is relevant contextual and semantic consis-\ntency, the overlap in these metrics scores represents\nthe challenge for LLM to distinguish between real\nand fake news. Thereby, PURIFY ensures our data\naligns logically and factually with the prompt in\nrelation to the original text. It also filters high-\nquality, meaningful, and nuanced real or fake con-\ntent to simulate subtle extrinsic/intrinsic hallucina-\ntions and elusive “silent echoes” disinformation in\nreal-world contexts (Table 3).\nRQ1.2 Finding: Using PURIFY ,we find\n38% of data generated by GPT-3.5-turbo\ncontains hallucinated misalignments. While\nlargely producing contextual and semantic\naligned text, 8% of overall samples show\nlogical misalignment, and 30% further ex-\nhibit factual inconsistencies (Fig. 15).\nFinally, Table 4 depicts details of our new F3\nLLM-generated dataset (after PURIFY step) across\npre- and post-GPT-3.5 periods, including social me-\ndia and news articles. We use PaLM-2 to conduct\na thematic analysis of the dataset (Fig. 18). Table 6\ncompares F3 dataset with emerging related datasets.\nTable 10 details misalignment examples.\n6 RQ2: Disinformation Detection\nIn RQ2, we shift our attention to investigating how\nadept LLMs are at zero-shot binary detection of\nhuman and AI-created disinformation compared to\nSOTAdetectors (Fig. 1, steps 4 and 5).\nEra Dataset L SM NA\nCoAID R 2,520 6,289\nF 3,592 5,667\nPre-GPT3.5 FakeNewsNet R — 681\nF — 5,703\nF3 R 970 —\nF 1,395 —\nPost-GPT3.5 F3 R 269 23\nF 395 163\n9,141 18,526\nTable 4: Details of final LLM-data samples. Each sym-\nbol is the same with Table 2.\nWe evaluate the capabilities of LLMs for dis-\ninformation detection on five fronts: (1) human-\nwritten vs. LLM-generated news with minor, ma-\njor, and critical perturbations/paraphrased text, (2)\nself-generated vs. other LLM-generated, (3) (short)\nsocial media posts vs. (long) news articles, (4)\nin-distribution (ID) vs. out-of-distribution (OOD)\n(i.e., comparison between Pre-GPT-3.5 and Post-\nGPT-3.5 data), and (5) zero-shot LLMs vs. domain-\nspecific detectors. For detectors’ performance eval-\nuation, we employ Macro-F1 scores on our imbal-\nanced human-AI datasets.\n6.1 Dataset Set-up and Models Tested\nGiven our dataset in Tables 2 and 4, we struc-\nture our experiments’ data as follows: (1) We di-\nvide the data into pre- vs. post-GPT-3.5 for in-\ndistribution vs. out-of-distribution evaluation. Pre-\nGPT-3.5 allows us to train and validate models\nfor in-distribution testing. Post-GPT-3.5 provides\nunseen data for assessing out-of-distribution gen-\neralization. (2) We further stratify the data into\nhuman vs. LLM-generated for comparing perfor-\nmance on these two test cases. (3) We also separate\ndata into news articles and social media posts, as\nmodels may perform differently on these text types.\n(4) For pre-GPT-3.5, we split data into 70% for\ntraining, 20% for validation, and 10% for testing\nvia stratified sampling to ensure balanced real/fake\nnews splits. (5) We do not split post-GPT-3.5 data,\nusing the full set for OOD testing.\nNext, we test how well different models detect\nF3 disinformation generate. We first use four lead-\ning LLMs with emergent abilities such as GPT-\n3.5-Turbo, LLaMA-2-Chat (Rozière et al., 2023),\nLLaMA-2-GPT4 (Touvron et al., 2023), and Dolly-\n2 (Conover et al., 2023). See Table 7 for details\nand implementation in Appendix D. We then use\nthree popular domain-specific fake news detectors\nincluding dEFEND (Shu et al., 2019), TextCNN\n(Kim, 2014), and BiGRU (Ma et al., 2016). These\ndeep learning (DL) models are fake news domain-\n14284\nspecific detectors. In addition, using our dataset,\nwe fine-tune four BERT variants: BERT (Kenton\nand Toutanova, 2019), CT-BERT (Müller et al.,\n2023), RoBERTa (Liu et al., 2019), and DeBERTa\n(He et al., 2020). See Table 7 for details and imple-\nmentation in Appendix D for all baseline models.\n6.2 Detection: Cloze-Prompt Engineering\nWe evaluate LLMs’ zero-shot disinformation de-\ntection using prompt engineering. LLMs can\nreason systematically with simple prompts like\n“Let’s think step-by-step” for CoT (Zhang et al.,\n2022; Bommasani et al., 2021). Using cloze-style\nprompts, we apply semantic, intermediate, and step-\nby-step reasoning, and integrate SOTA prompting\napproaches with our confidence-based and context-\ndefined reasoning strategies inspired by various\nlogic types (Tang et al., 2023a).\nTo guide predictions, we embed such techniques\ninto our cloze-style prompt instructor parameter\n(Fig. 2). However, LLMs’ alignment using the re-\ninforcement learning with human feedback (RLHF)\nlimits explicit veracity assessment. We address this\nissue using our impersonator prompt. See Table 8\nand 9 for further details.\n6.3 RQ2 Results and Analysis\nOur analysis compares the average Macro-F1\nscores across RQ2. Table 5 shows the average\nMacro-F1 scores on our pre- and post-GPT-3.5 (i.e.,\nin- and out-of-distribution) datasets. Full detailed\nresults are provided in Appendix E.\nRQ2.1: Human vs. LLM-generated\nWhen evaluating LLMs’ zero-shot capabilities for\nhuman vs. LLM disinformation detection, we find\nGPT-3.5-Turbo, LLaMA-GPT, and LLaMA-2 are\nmore accurate on detecting LLM-generated disin-\nformation, compared to human-authored disinfor-\nmation (Fig. 4). On human-authored data, GPT-3.5-\nTurbo’s accuracy ranges from 55-66% , while on\nLLM-generated data, it achieves 60-85% . Dolly-2\nshows the lowest accuracy on both human ( 51-\n52%) and LLM (47-50%) disinformation.\nRQ2.1 Finding: LLMs struggle more to\ndetect human-written disinformation, com-\npared to LLM-generated variants.\nRQ2.2: Self-generated vs. Externally-generated\nGPT-3.5-Turbo-135B displays strong self-detec-\ntion, outperforming other LLMs overall and across\ndisinformation variants, i.e., minor, major, and crit-\nFigure 4: RQ2.1 LLMs’ zero-shot Human vs. LLM-\ndisinformation detection using Macro-F1 Score.\nFigure 5: RQ2.2 LLMs’ zero-shot Self vs. External\ndetection using Macro-F1 Score. The yellow dot repre-\nsents mean (µ).\nical (Fig. 5). However, LLaMA-GPT excels as the\ntop external detector of GPT-3.5-Turbo-generated\ndisinformation. LLaMA-2 shows moderate exter-\nnal detection abilities. Regardless of self or exter-\nnal detection capacity, by and large,LLMs struggle\nto accurately detect minor paraphrased and per-\nturbed disinformation.\nRQ2.2 Finding: GPT-3.5-Turbo is good at\nself-detection, and LLaMA-GPT is the best\nexternal detector.\nRQ2.3: Social Media Posts vs. News Article\nWhen evaluating LLMs’ ability to detect (short) so-\ncial media posts vs. (long) news articles detection,\nGPT-3.5-Turbo (0.66-0.85% ), and LLaMA-GPT\n(0.54%-0.71% ) were more accurate on articles.\nWhile GPT-3.5-Turbo’s (0.55%-0.76% ), LLaMA-\nGPT(0.56%-0.66% ), and LLaMA-2 ( 0.59%-\n0.67% ) perform moderately well on posts, com-\npared to Dolly-2’s low F1-Scores (Fig.6).\nRQ2.3 Finding: LLMs exhibit superior\nzero-shot performance on (long) news ar-\nticles than (short) social media posts.\nRQ2.4: In-Distribution vs. Out-of-Distribution\nWe categorize disinformation data as in-distribution\nor out-of-distribution relative to GPT-3.5-turbo’s\nknown training timeline to assess detectors’ gen-\n14285\nData Source Articles Posts\nData Categories Human LLM-Min LLM-Maj LLM-Crit Human LLM-Min LLM-Maj LLM-Crit ¯x\nIn-Distribution\nGPT-3.5-Turbo-175B0.6604 0.8190 0.8359 0.8484 0.5505 0.5998 0.6646 0.7640 0.7178\nLLaMA-GPT-70B 0.5398 0.6960 0.7055 0.6803 0.5579 0.6286 0.6565 0.6440 0.6386\nLLaMA-2-70B 0.5958 0.5737 0.5984 0.5911 0.5869 0.6344 0.6476 0.6727 0.6126\nDolly-2-12B 0.5151 0.4825 0.4822 0.4889 0.5076 0.4964 0.4669 0.4666 0.4883\nCustomized DL Models0.6750 0.6589 0.6772 0.6548 0.5483 0.6477 0.6852 0.7006 0.6563\nFine-tuned Transformers0.7025 0.9751 0.9657 0.9844 0.8787 0.9726 0.9612 0.9514 0.9283\nOut-of-Distribution\nGPT-3.5-Turbo-175B0.7170↑0.06 0.7091↓0.11 0.7136↓0.12 0.6792↓0.17 0.7107↑0.16 0.6072↑0.007 0.6407↓0.02 0.6828↓0.08 0.6825↓0.04\nLLaMA-GPT-70B 0.7112↑0.17 0.5797↓0.12 0.6049↓0.10 0.5588↓0.12 0.6072↓0.05 0.5667↓0.06 0.5961↓0.06 0.5218↓0.12 0.5933↓0.05\nLLaMA-2-70B 0.6103↑0.02 0.5928↑0.02 0.5976↓0.001 0.6024↑0.01 0.6165↑0.03 0.6354↑0.001 0.6444↓0.003 0.6762↑0.004 0.6218↑0.009\nDolly-2-12B 0.6127↑0.10 0.4470↓0.04 0.4692↓0.02 0.4049↓0.08 0.4828↓0.02 0.5386↑0.04 0.5044↑0.04 0.4715↑0.005 0.4901↑0.002\nCustomized DL Models0.4609↓0.21 0.3901↓0.27 0.3514↓0.33 0.3949↓0.26 0.5360↓0.01 0.5366↓0.11 0.4312↓0.2 0.6416↓0.06 0.4679↓0.19\nFine-tuned Transformers0.5121↓0.19 0.7234↓0.25 0.7619↓0.20 0.7101↓0.27 0.6373↓0.24 0.9660↓0.007 0.9395↓0.02 0.9311↓0.02 0.8039↓0.12\nTable 5: In vs. Out-of-Distribution Comparison. This table presents the average F1 performance of generative LLMs,\ncustomized deep-learning models, and fine-tuned transformers. Performance is benchmarked across categories\nof human, LLM minor, LLM major, and LLM critical for both articles and posts. The ¯xcolumn shows the mean\nperformance for each model.\nFigure 6: RQ2.3 LLMs’ zero-shot performance across\nsocial media posts and new articles using Macro-F1\nScore.\neralizability. Disinformation created using hu-\nman data “before” the release of GPT-3.5-turbo\n(i.e., Pre-GPT3.5 in Table 2) is considered in-\ndistribution, as such human data may be part of\nthe training data of GPT-3.5-turbo. On the other\nhand, disinformation created using human data “af-\nter” the release of GPT-3.5-turbo (i.e., Post-GPT3.5\nin Table 2) is considered as out-of-distribution, as\nthey could not have been part of training data of\nGPT-3.5-turbo.\nAssessing LLMs’ zero-shot ability to detect\nLLMs’ in-distribution vs. out-of-distribution detec-\ntion, as shown in Fig. 7, we found that all LLMs\nexcept LLaMA-2 performance declined on out-of-\ndistribution data. Minor-LLM disinformation is\nassociated with lower detection accuracy than Ma-\njor and Critical LLM disinformation (Table 5).\nRQ2.4 Finding: LLMs show better zero-\nshot performance on in-distribution data, ex-\ncept LLaMA-2.\nRQ2.5: Zero-Shot LLMs vs. Domain-Specific\nWe compare zero-shot generative LLMs against\ncustomized and fine-tuned transformer detectors\nFigure 7: RQ2.4 A comparison of LLMs across various\ndisinformation categories. Each is represented by a bar,\nwith numerical values atop indicating either a positive\nor negative change of in-distribution Macro-F1 Score\nrelative to out-of-distribution.\nacross in-distribution and out-of-distribution data.\nOverall, fine-tuned transformer models like BERT\nachieve the best performance, followed by gen-\nerative LLMs like GPT-3.5-Turbo and then cus-\ntomized models. However, the average perfor-\nmance of transformers and customized models\ndrops significantly on OOD data compared to gen-\nerative LLMs.\nRQ2.5 Finding: Fine-tuned detectors sig-\nnificantly outperform LLMs and domain-\nspecific detectors but are not consistent on\ndetecting OOD disinformation. GPT3.5-\nTurbo outperforms domain-specific detec-\ntors while other LLMs perform comparably.\n7 Discussion\nF3 prompting shows promise for few-shot de-\ntection. Our F3 techniques outperformed stan-\ndard reasoning, showing the potential of advanced\nprompt engineering to enhance few-shot LLMs’ de-\ntection abilities. Notably, MsReN_CoT showed the\n14286\nFigure 8: RQ2.5 Box plots that compare the perfor-\nmance of Zeroshot LLMs, Neural Network Customized\nand Finetuned Transformer detectors using F1-Scores.\nEach model’s performance is evaluated in two scenarios:\n“Pre-GPT-3.5-Turbo” (represented in blue) and “Post-\nGPT-3.5-Turbo” (represented in orange)\nstrongest results across human-LLM datasets. For\narticles, GPT-3.5-Turbo-175B with Analyze_Cld2\nachieved top performance on human and LLM data.\nThe integrated reasoning strategies underpinning\nF3 cloze-prompts account for their standout per-\nformance, highlighting fruitful directions for devel-\noping broadly applicable disinformation detection\nprompts.\nGPT-3.5-turbo excels at detecting human-\nwritten and self-written disinformation.Leverag-\ning prompting, GPT-3.5-turbo exceeded other mod-\nels at detecting human-written and self-generated\ndisinformation. Despite stiff competition, it demon-\nstrated superior self-detection across all synthetic\narticle and post variants, asserting its zero-shot ca-\npabilities. Further assessing its performance on\nother LLMs’ disinformation is a vital next step.\nWhile self-detection is unsurprising due to shared\nvocabulary distribution, this performance under-\nscores detection potential if ChatGPT faces mali-\ncious exploitation.\nLLMs are robust across distributions. GPT-\n3.5-turbo consistently detected human-written\nand LLM-generated disinformation, both in-\ndistribution and out-of-distribution, showing its po-\ntential. These results highlight the significance of\ngenerative LLMs’ applicability in real-world set-\ntings as emerging zero-shot reasoners in disinfor-\nmation detection. Fine-tuned transformers show re-\nmarkably high in-distribution performance, indicat-\ning optimization for familiar data. Their lower com-\npetitive out-of-distribution scores demonstrate a\nspecialization-generalization balance. Customized\nmodels exhibit good in-distribution and out-of-\ndistribution balance, though slightly weaker on\nunfamiliar data. The performance gap suggests\nspecialization for domain tasks but difficulty gener-\nalizing.\nSmart, cunningly crafted (subtle) disinforma-\ntion challenges even the best current detectors.\nAll models struggled more with minor disinfor-\nmation alterations compared to major and critical\nchanges. This is somewhat expected as the amount\nor level of fake-ness is small, it is more challenging\nto determine its veracity. Therefore, developing\nmore sophisticated systems to be able to handle\nvery subtle fake-ness in disinformation is needed.\nBypassing alignment tuning is critical but in-\nconsistent. Both disinformation generation and\ndetection tasks require circumventing LLMs’ align-\nment tuning mechanisms. While our imperson-\nator approach successfully bypassed four LLMs’\nprotections, it failed to bypass Falcon’s alignment\ntuning unless using CoT reasoning. This inconsis-\ntency of bypassing alignment tuning across models\nhighlights a key limitation for robustly evaluating\nLLMs’ disinformation capabilities.\nResponsible LLM use is critical. LLMs’ misuse\nduring crises can have serious consequences (Wei-\ndinger et al., 2022). We reveal how to misuse a\npopular LLM by bypassing its protective guardrails\nto generate disinformation. Many top-performing\nLLMs are publicly available. Thus, we must pre-\npare for the risks of unintended harmful political,\ncyber-security, and public health applications.\n8 Conclusion\nOur work demonstrates LLMs’ promise for self-\ndetection in a zero-shot framework, reducing\ntraining needs. While dangerous if misused, re-\npurposing LLMs to counter disinformation attacks\nhas advantages. Key results like GPT-3.5’s per-\nformance highlight generative models’ abilities\nbeyond text generation. To aid research, we\ndeveloped PURIFY for detecting and removing\nhallucination-based misaligned content. However,\ndifficulty in detecting subtle disinformation mo-\ntivates stronger safeguarding of LLMs and more\nnuanced prompting. Assessing few-shot detection\nand disinformation mitigation will be critical as\nLLMs continue advancing. While LLMs can po-\ntentially be misused to create disinformation, we\ncan fight back by re-purposing them as countermea-\nsures, thus “fighting fire with fire.”\n14287\nLimitations\nThis work demonstrates promising zero-shot dis-\ninformation detection using prompt engineering.\nHowever, few-shot capabilities remain unevaluated\nand could further improve performance. Addition-\nally, we examined a small subset of available LLMs.\nTesting more and larger models like GPT-4 could\nprovide new insights. Due to time constraints, we\ndid not fully optimize prompts to achieve maxi-\nmally consistent high accuracy for zero-shot detec-\ntion. Performance variability indicates the need for\nmore generalizable prompts. We were also unable\nto assess GPT-4 (OpenAI, 2023) due to time con-\nstraints, which can be addressed by future work.\nAlthough initially included, in the end, we de-\ncided to remove Falcon-2 (Penedo et al., 2023) due\nto difficulties in bypassing its alignment tuning us-\ning our semantic reasoning prompts. Zero-CoT\nseems to break the Falcon-2 alignment tuning. The\nModel responds, “(No cheating!) False.” Without\nCoT, it would say things like, “I’m sorry, I am an AI\nlanguage model, and I cannot provide a definitive\nanswer without additional context or information.”\nFuture work can re-evaluate the proposed research\nquestions against more diverse language models.\nOther future directions include assessing few-\nshot performance, evaluating more models, de-\nveloping better prompts, integrating detection ap-\nproaches, adding multimodal inputs, and collabo-\nrating with stakeholders. Open questions remain\naround societal impacts and dual-use risks requir-\ning ongoing ethics focus.\nEthics Statement\nThis research involves generating and analyzing\npotentially harmful disinformation. Our released\nF3 dataset also includes the examples of LLM-\ngenerated disinformation. Our aim is to advance\nthe research to combat disinformation. However,\nopen dissemination risks the misuse of the gener-\nated disinformation in F3 dataset and the methods\nthat enabled such generation. To promote trans-\nparency while considering these dilemmas: (1) we\nrelease codes, prompts, and synthetic data to en-\nable the reproducibility of our research findings\nand encourage further research but advise users\nresponsible use, and (2) our release will exclude\noriginal real-world misinformation, but only syn-\nthetic variations, to minimize harmful usages.\nAddressing disinformation dangers requires de-\nveloping solutions conscientiously and ethically.\nWe hope this statement provides clarity on our in-\ntentions and values. Addressing the societal dan-\ngers of disinformation requires proactive work to\ndevelop solutions, but at the same time, it must be\npursued conscientiously and ethically.\nAcknowledgements\nThis work was in part supported by NSF awards\n1820609, 1934782, 2114824, and 2131144.\nReferences\nAbdullah Marish Ali, Fuad A Ghaleb, Bander Ali Saleh\nAl-Rimy, Fawaz Jaber Alsolami, and Asif Irshad\nKhan. 2022. Deep ensemble fake news detection\nmodel using sequential deep learning technique. Sen-\nsors, 22(18).\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al. 2023. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403.\nNida Aslam, Irfan Ullah Khan, Farah Salem Alotaibi,\nLama Abdulaziz Aldaej, and Asma Khaled Al-\ndubaikil. 2021. Fake detect: A deep learning en-\nsemble model for fake news detection. Complexity,\n2021:1–8.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V Do, Yan\nXu, and Pascale Fung. 2023. A multitask, multilin-\ngual, multimodal evaluation of ChatGPT on reason-\ning, hallucination, and interactivity. arXiv preprint\narXiv:2302.04023.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv:2004.05150.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ\nAltman, Simran Arora, Sydney von Arx, Michael S\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, Erik Brynjolfsson, Shyamal Buch, Dallas\nCard, Rodrigo Castellon, Niladri Chatterji, Annie\nChen, Kathleen Creel, Jared Quincy Davis, Dora\nDemszky, Chris Donahue, Moussa Doumbouya,\nEsin Durmus, Stefano Ermon, John Etchemendy,\nKawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor\nGale, Lauren Gillespie, Karan Goel, Noah Goodman,\nShelby Grossman, Neel Guha, Tatsunori Hashimoto,\nPeter Henderson, John Hewitt, Daniel E Ho, Jenny\nHong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil\nJain, Dan Jurafsky, Pratyusha Kalluri, Siddharth\nKaramcheti, Geoff Keeling, Fereshte Khani, Omar\nKhattab, Pang Wei Koh, Mark Krass, Ranjay Kr-\nishna, Rohith Kuditipudi, Ananya Kumar, Faisal Lad-\nhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle\nLevent, Xiang Lisa Li, Xuechen Li, Tengyu Ma,\n14288\nAli Malik, Christopher D Manning, Suvir Mirchan-\ndani, Eric Mitchell, Zanele Munyikwa, Suraj Nair,\nAvanika Narayan, Deepak Narayanan, Ben Newman,\nAllen Nie, Juan Carlos Niebles, Hamed Nilforoshan,\nJulian Nyarko, Giray Ogut, Laurel Orr, Isabel Pa-\npadimitriou, Joon Sung Park, Chris Piech, Eva Porte-\nlance, Christopher Potts, Aditi Raghunathan, Rob\nReich, Hongyu Ren, Frieda Rong, Yusuf Roohani,\nCamilo Ruiz, Jack Ryan, Christopher Ré, Dorsa\nSadigh, Shiori Sagawa, Keshav Santhanam, Andy\nShih, Krishnan Srinivasan, Alex Tamkin, Rohan\nTaori, Armin W Thomas, Florian Tramèr, Rose E\nWang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai\nWu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan\nYou, Matei Zaharia, Michael Zhang, Tianyi Zhang,\nXikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn\nZhou, and Percy Liang. 2021. On the opportuni-\nties and risks of foundation models. arXiv preprint\narXiv:2108.07258.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are Few-Shot learners. Ad-\nvances in neural information processing systems.\nSouradip Chakraborty, Amrit Singh Bedi, Sicheng Zhu,\nBang An, Dinesh Manocha, and Furong Huang. 2023.\nOn the possibilities of AI-Generated text detection.\narXiv preprint arXiv:2304.04736.\nLichang Chen, Heng Huang, and Minhao Cheng.\n2023. PTP: Boosting stability and performance of\nprompt tuning with perturbation-based regularizer.\narXiv.org.\nMingda Chen, Sam Wiseman, and Kevin Gimpel. 2020a.\nControllable paraphrasing and translation with a syn-\ntactic exemplar. arXiv.org.\nMingda Chen, Sam Wiseman, and Kevin Gimpel. 2020b.\nExemplar-Controllable paraphrasing and translation\nusing bitext. arXiv preprint arXiv:2010.05856.\nMike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,\nJun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell,\nMatei Zaharia, and Reynold Xin. 2023. Free dolly:\nIntroducing the world’s first truly open instruction-\ntuned llm.\nLimeng Cui and Dongwon Lee. 2020. CoAID: COVID-\n19 healthcare misinformation dataset. arXiv preprint\narXiv:2006.00885.\nLimeng Cui, Haeseung Seo, Maryam Tabar, Fenglong\nMa, Suhang Wang, and Dongwon Lee. 2020. DE-\nTERRENT: Knowledge Guided Graph Attention Net-\nwork for Detecting Healthcare Misinformation. In\nProceedings of the 26th ACM SIGKDD international\nconference on knowledge discovery & data mining,\npages 492–502.\nLuigi De Angelis, Francesco Baglivo, Guglielmo Arzilli,\nGaetano Pierpaolo Privitera, Paolo Ferragina, Al-\nberto Eugenio Tozzi, and Caterina Rizzo. 2023. Chat-\nGPT and the rise of large language models: the new\nAI-driven infodemic threat in public health. Front\nPublic Health, 11:1166120.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805.\nZi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao\nJiang, and Graham Neubig. 2020. GSum: A general\nframework for guided neural abstractive summariza-\ntion. arXiv preprint arXiv:2010.08014.\nAngela D Evans and Kim Roberts. 2009a. The effects of\ndifferent paraphrasing styles on the quality of reports\nfrom young child witnesses. Psychol. Crime Law,\n15(6):531–546.\nAngela D Evans and Kim P Roberts. 2009b. Can\nparaphrasing increase the amount and accuracy\nof reports from child eyewitnesses? \"Psy-\nchology, Crime, & Law, 6, 531-546. DOI:\n10.1080/10683160802385398\".\nMarina Fomicheva and Lucia Specia. 2019. Taking MT\nevaluation metrics to extremes: Beyond correlation\nwith human judgments. Comput. Linguist. Assoc.\nComput. Linguist., 45(3):515–558.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2020.\nMaking pre-trained language models better few-shot\nlearners. arXiv preprint arXiv:2012.15723.\nKaren Hambardzumyan, Hrant Khachatrian, and\nJonathan May. 2021. W ARP: Word-level adversarial\nReProgramming. arXiv preprint arXiv:2101.00121.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention. arXiv preprint\narXiv:2006.03654.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text\ndegeneration. arXiv preprint arXiv:1904.09751.\nWei Jason, Tay Yi, Bommasani Rishi, Raffel Colin,\nZoph Barret, Borgeaud Sebastian, Yogatama Dani,\nBosma Maarten, Zhou Denny, Metzler Donald, et al.\n2022. Emergent abilities of large language models.\nTransactions on Machine Learning Research, 8.\nNirosh Jayakody, Azeem Mohammad, and Malka N\nHalgamuge. 2022. Fake news detection using a de-\ncentralized deep learning model and federated learn-\ning. In IECON 2022 – 48th Annual Conference of\nthe IEEE Industrial Electronics Society. IEEE.\n14289\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):1–38.\nMarzena Karpinska, Nishant Raj, Katherine Thai, Yix-\niao Song, Ankita Gupta, and Mohit Iyyer. 2022.\nDemetr: Diagnosing evaluation metrics for transla-\ntion. arXiv preprint arXiv:2210.13746.\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina\nToutanova. 2019. Bert: Pre-training of deep bidi-\nrectional transformers for language understanding.\nvolume 1, page 2.\nYoon Kim. 2014. Convolutional neural networks\nfor sentence classification. In Proceedings of the\n2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1746–1751,\nDoha, Qatar. Association for Computational Linguis-\ntics.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. Advances in\nneural information processing systems , 35:22199–\n22213.\nVenelin Kovatchev. 2022. Paraphrasing, textual en-\ntailment, and semantic similarity above word level.\narXiv preprint arXiv:2208.05387.\nKalpesh Krishna, Yixiao Song, Marzena Karpinska,\nJohn Wieting, and Mohit Iyyer. 2023. Paraphrasing\nevades detectors of AI-generated text, but retrieval is\nan effective defense. arXiv.org.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for Parameter-Efficient prompt\ntuning. arXiv preprint arXiv:2104.08691.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582–\n4597, Online. Association for Computational Lin-\nguistics.\nHanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji\nZhou, and Yue Zhang. 2023a. Evaluating the logical\nreasoning ability of ChatGPT and GPT-4.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023b. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Comput. Surv., 55(9):1–35.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nJing Ma, Wei Gao, Prasenjit Mitra, Sejeong Kwon,\nBernard J Jansen, Kam-Fai Wong, and Meeyoung\nCha. 2016. Detecting rumors from microblogs with\nrecurrent neural networks. In Proceedings of the\nTwenty-Fifth International Joint Conference on Arti-\nficial Intelligence, pages 3818–3824. AAAI Press.\nSaif M Mohammad and Graeme Hirst. 2012. Distri-\nbutional measures of semantic distance: A survey.\narXiv preprint arXiv:1203.1858.\nMartin Müller, Marcel Salathé, and Per E Kummervold.\n2023. Covid-twitter-bert: A natural language pro-\ncessing model to analyse covid-19 content on twitter.\nFrontiers, 6:1023281.\nTaichi Murayama. 2021. Dataset of fake news detec-\ntion and fact verification: a survey. arXiv preprint\narXiv:2111.03299.\nVan-Hoang Nguyen, Kazunari Sugiyama, Preslav\nNakov, and Min-Yen Kan. 2020. FANG: Leveraging\nsocial context for fake news detection using graph\nrepresentation. In Proceedings of the 29th ACM In-\nternational Conference on Information & Knowledge\nManagement, CIKM ’20, pages 1165–1174, New\nYork, NY , USA. Association for Computing Machin-\nery.\nOpenAI. 2023. Gpt-4 technical report.\nRay Oshikawa, Jing Qian, and William Yang Wang.\n2018. A survey on natural language process-\ning for fake news detection. arXiv preprint\narXiv:1811.00770.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow,\nRuxandra Cojocaru, Alessandro Cappelli, Hamza\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The RefinedWeb dataset\nfor Falcon LLM: outperforming curated corpora\nwith web data, and web data only. arXiv preprint\narXiv:2306.01116.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and\nChristopher D. Manning. 2020. Stanza: A python\nnatural language processing toolkit for many human\nlanguages. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations, pages 101–108, Online. As-\nsociation for Computational Linguistics.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Ji-\naao Chen, Michihiro Yasunaga, and Diyi Yang.\n2023. Is ChatGPT a General-Purpose natural lan-\nguage processing task solver? arXiv preprint\narXiv:2302.06476.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nFaisal Rahutomo, Teruaki Kitasuka, and Masayoshi Ar-\nitsugi. 2012. Semantic cosine similarity. In The 7th\ninternational student conference on advanced science\nand technology ICAST, volume 4, page 1.\n14290\nLaria Reynolds and Kyle McDonell. 2021. Prompt pro-\ngramming for large language models: Beyond the\nFew-Shot paradigm. In Extended Abstracts of the\n2021 CHI Conference on Human Factors in Com-\nputing Systems, number Article 314 in CHI EA ’21,\npages 1–7, New York, NY , USA. Association for\nComputing Machinery.\nBaptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nVinu Sankar Sadasivan, Aounon Kumar, Sriram Bala-\nsubramanian, Wenxiao Wang, and Soheil Feizi. 2023.\nCan AI-Generated text be reliably detected? arXiv\npreprint arXiv:2303.11156.\nTimo Schick and Hinrich Schütze. 2021. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255–269, Online. Association for Computa-\ntional Linguistics.\nKai Shu, Limeng Cui, Suhang Wang, Dongwon Lee,\nand Huan Liu. 2019. dEFEND: Explainable fake\nnews detection. In Proceedings of the 25th ACM\nSIGKDD International Conference on Knowledge\nDiscovery & Data Mining, KDD ’19, pages 395–405,\nNew York, NY , USA. Association for Computing\nMachinery.\nKai Shu, Deepak Mahudeswaran, Suhang Wang, Dong-\nwon Lee, and Huan Liu. 2020. Fakenewsnet: A data\nrepository with news content, social context, and spa-\ntiotemporal information for studying fake news on\nsocial media. Big data, 8(3):171–188.\nGiovanni Spitale, Nikola Biller-Andorno, and Federico\nGermani. 2023. AI model GPT-3 (dis)informs us bet-\nter than humans. arXiv preprint arXiv:2301.11924.\nQi Su, Mingyu Wan, Xiaoqian Liu, Chu-Ren Huang,\net al. 2020. Motivations, methods and metrics of mis-\ninformation detection: an nlp perspective. Natural\nLanguage Processing Research, 1(1-2):1–13.\nYanshen Sun, Jianfeng He, Shuo Lei, Limeng Cui, and\nChang-Tien Lu. 2023. Med-mmhl: A multi-modal\ndataset for detecting human-and llm-generated mis-\ninformation in the medical domain. arXiv preprint\narXiv:2306.08871.\nXiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng,\nSong-Chun Zhu, Yitao Liang, and Muhan Zhang.\n2023a. Large language models are In-Context seman-\ntic reasoners rather than symbolic reasoners. arXiv\npreprint arXiv:2305.14825.\nXiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng,\nSong-Chun Zhu, Yitao Liang, and Muhan Zhang.\n2023b. Large language models are in-context seman-\ntic reasoners rather than symbolic reasoners. arXiv\npreprint arXiv:2305.14825.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nAdaku Uchendu, Jooyoung Lee, Hua Shen, Thai Le,\nTing-Hao ’Kenneth’ Huang, and Dongwon Lee. 2023.\nDoes Human Collaboration Enhance the Accuracy\nof Identifying LLM-Generated Deepfake Texts? In\n11th AAAI Conf. on Human Computation and Crowd-\nsourcing (HCOMP).\nAdaku Uchendu, Zeyu Ma, Thai Le, Rui Zhang, and\nDongwon Lee. 2021. TuringBench: A Benchmark\nEnvironment for Turing Test in the Age of Neural\nText Generation. In Findings of the Association for\nComputational Linguistics: EMNLP 2021 , pages\n2001–2016, Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nBibek Upadhayay and Vahid Behzadan. 2022. Hybrid\ndeep learning model for fake news detection in social\nnetworks (student abstract). Proc. Conf. AAAI Artif.\nIntell., 36(11):13067–13068.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R\nBowman. 2020. BLiMP: The benchmark of linguis-\ntic minimal pairs for english. Transactions of the\nAssociation for Computational Linguistics , 8:377–\n392.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2022. Chain-of-Thought prompting\nelicits reasoning in large language models. Advances\nin Neural Information Processing Systems.\nLaura Weidinger, Jonathan Uesato, Maribeth Rauh,\nConor Griffin, Po-Sen Huang, John Mellor, Amelia\nGlaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh,\nCourtney Biles, Sasha Brown, Zac Kenton, Will\nHawkins, Tom Stepleton, Abeba Birhane, Lisa Anne\nHendricks, Laura Rimell, William Isaac, Julia Haas,\nSean Legassick, Geoffrey Irving, and Iason Gabriel.\n2022. Taxonomy of risks posed by language models.\nIn Proceedings of the 2022 ACM Conference on Fair-\nness, Accountability, and Transparency, FAccT ’22,\npages 214–229, New York, NY , USA. Association\nfor Computing Machinery.\nSam Witteveen and Martin Andrews. 2019. Paraphras-\ning with large language models. In Proceedings of\nthe 3rd Workshop on Neural Generation and Trans-\nlation, Stroudsburg, PA, USA. Association for Com-\nputational Linguistics.\nYuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu.\n2023. Alignscore: Evaluating factual consistency\nwith a unified alignment function. arXiv preprint\narXiv:2305.16739.\n14291\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, and Yoav Artzi. 2019. BERTScore: Evalu-\nating text generation with BERT. arXiv preprint\narXiv:1904.09675.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022. Automatic chain of thought prompting\nin large language models. In The Eleventh Interna-\ntional Conference on Learning Representations.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\nsurvey of large language models. arXiv preprint\narXiv:2303.18223.\nJiawei Zhou, Yixuan Zhang, Qianni Luo, Andrea G\nParker, and Munmun De Choudhury. 2023. Synthetic\nlies: Understanding ai-generated misinformation and\nevaluating algorithmic and human solutions. In Pro-\nceedings of the 2023 CHI Conference on Human\nFactors in Computing Systems, CHI ’23, New York,\nNY , USA. Association for Computing Machinery.\nYongchun Zhu, Qiang Sheng, Juan Cao, Qiong Nan,\nKai Shu, Minghui Wu, Jindong Wang, and Fuzhen\nZhuang. 2022. Memory-guided multi-view multi-\ndomain fake news detection. IEEE Transactions on\nKnowledge and Data Engineering.\nAppendix\nA Prompt Engineering\nWe use prompt engineering to examine how LLMs\nuse statistical patterns learned during training to\ngenerate text sequences and produce zero-shot bi-\nnary class responses. We describe the further de-\ntails of our prompt designs as follows.\nA.1 Prefix Prompt\nOur prefix-prompt framework generates high-\nquality, coherent synthetic real and fake news con-\ntent. The goal is to leverage paraphrasing and\nperturbation techniques. The process starts by se-\nlecting human-authored content and adding it to\na prefix prompt. This contains an impersonator\nsetting contextual behavior intent and instructions\nproviding guidance. Prompts are engineered to\nparaphrase or perturb the original content at three\nalteration degrees (MiN, MaJ, CRiT) to produce\nsynthetic real news. The prompt is fed into the\nLLM to generate content.\nFigure 9 demonstrates our paraphrase-based\nreal news generation results. Figure 3 shows\nperturbation-based fake news generation results.\nA.2 RQ1 Disinformation Generation\nFigure 10 shows our prefix prompt. The prefix\nprompt (x) combines: (1) Content (C) with real\nhuman data. (2) Impersonator (R) establishes con-\ntext, guides generation/detection, and overrides\nalignment tuning to generate disinformation. (3)\nInstructor (I) with paraphrase [Para ] and perturba-\ntion [Perturb ] directives for minimal, major, criti-\ncal [Min/Maj/Crit ] variation transformations. Com-\nbined parameters formulate prefix-prompt (X) as\ninput text sequence to generator (G) to produce\nLLM text (T) Real:green and Fake:red.\nA.3 RQ2 Disinformation Detection\nFigure 19 shows our Cloze prompt. The\nCloze prompt (y) combines: (1) Content (C)\nwith real or fake human green and LLM [blue]\ndata. (2) Instructor (I) with reasoning tech-\nniques’ directives for Vanilla[VaN ], Intermediate\n[Zero−CoT,A −CoT,etc.] and Semantic Rea-\nsoning [DeF −Gen,DeF −SpeC] transforma-\ntions. (3) Impersonator (R) establishes context,\nguides generation/detection, and overrides align-\nment tuning to generate disinformation. Parameters\nformulate prefix (X) input to generator (G) to pro-\nduce LLM label (L). Tables 8 and 9 show our exact\n14292\nFigure 9: GPT-3.5-turbo’s paraphrase-based prompts\nengineering approach. Minor: The minor paraphrase\nhas been concisely summarized by changing the struc-\nture of the sentence slightly and rephrasing some words\nlike \"twice as many\" to \"doubled.\" The essence and\ndetails of the original message remain intact. Major:\nThe major paraphrase changes the structure and word-\ning more extensively than the minor paraphrase, using\nwords like \"fatalities\" instead of \"deaths\" and \"two-fold\"\ninstead of \"twice as many\". Still, it remains true to the\nfactual content of the original. Critical: The critical\nparaphrase changes the voice and structure significantly,\nintroducing a new perspective (\"political leaders\") and\nusing a unique style that makes it distinct from the orig-\ninal. This version provides a fresh take on the original\ncontent, guided by its factual details but conveyed with\na unique twist in the message delivery.\nCloze-style prompts, and Figure 11 shows the cat-\negories of the reasoning techniques embedding in\nF3 zero-shot prompts.\nB PURIFY Metrics\nPURIFY detects and removes non-hallucinations\nfrom fake news and hallucinations from real news\nthat are unfaithfully misaligned with the input text.\nOur PURIFY framework uses four evaluation met-\nrics as shown in Table 3. We describe the future\ndetails of those metrics as follows.\nB.1 Factual Consistency\nAlignScore: We assess LLM factual consistency\nusing SOTA AlignScore (Zha et al., 2023). As\nshown in Figure 12, 0 AlignScore represents a low,\nand 1 represents a high degree of factuality between\nLLM-generated and the original human-written tex-\nFigure 10: F3 prefix prompt.\nFigure 11: Categories of our Cloze-style prompts. Cat-\negories are: intermediate, step-by-step, inductive, de-\nductive and abductive reasoning. Definition and details\nabout these approaches in relation to LLMs can be found\nat (Zhang et al., 2022; Tang et al., 2023b; Zhao et al.,\n2023)\ntual pairs. Our intuition is that real LLM-gener-\nated news should have high-factual consistency,\nand fake LLM-generated news should have low-\nfactual consistency. We utilize a hybrid statistical\nmethod to define a threshold that removes factually\ninconsistent samples. I.e., a non-parametric hybrid\nthreshold approach using the (1) Interquartile range\n(IQR) and (2) standard deviation (SD) to balance\nrobustness to spread and central tendency while\naccounting for skewed real/fake distribution nu-\nances. We derived thresholds of 0.0 - 0.36 for fake\nand 0.61 - 1.0 for real news to filter outliers while\nmaintaining nuanced edge cases’ diversity. We re-\nmoved 3281 high-scoring factual-inconsistent fake\nnews and 8707 low-scoring factual-inconsistent\nreal news, resulting in 27667 factually consistent\nsamples. We discuss our hybrid strategy in more\ndetails as follows.\n14293\nFigure 12: AlignScore distribution for real and fake\nnews before (above) and after (below) removing incon-\nsistent samples. We filter out fake news above 0.36 and\nreal news below 0.61 to exclude factual fakes and ques-\ntionable reals.\nFigure 13: AlignScore distribution for real and fake\nnews before (above) and after (below) removing incon-\nsistent samples stratify by generative prompts categories.\nWe filter out fake news above 0.36 and real news below\n0.61 to exclude factual fakes and questionable reals.\nB.1.1 Factuality Hybrid Threshold Strategy\nInterquartile Range & Standard Deviation :\nSince LLMs often generate hallucinated text, we\nassess their factual consistency using AlignScore\n(Zha et al., 2023), a SOTA facility metric. We filter\nout high-scoring fake and low-scoring real news\nto remove inconsistent samples. AlignScore pro-\nvides a single value indicating factual consistency.\nThe AlignScore distribution for real and fake news\nis complex, requiring a robust discernment of nu-\nances. We use a non-parametric, hybrid approach\nwith (1) Interquartile range (IQR) to consider the\nrightfully skewed fake and real distributions (Fig.\n12 and 13. (2) Standard deviation with IQR to\nbalance spread and central tendency, maintaining\nrobustness. Our hybrid thresholds of 0.36 for fake\nand 0.61 for real news remove surprisingly factual\nfake and suspicious real samples, filtering outliers\nwhile capturing edge cases and removing inconsis-\ntent hallucinations. See our algorithmic represen-\ntation approach below, which utilizes Q0.75,real for\nthe 75th percentile) and θ:\n1. Computing IQR for Fake and Real News:\nIQRfake = Q0.75,fake −Q0.25,fake\nIQRreal = Q0.75,real −Q0.25,real\nwhere: Q0.25,fake and Q0.75,fake denote the 1st\n(25th percentile) and 3rd quartiles (75th per-\ncentile) of the AlignScore for fake news, re-\nspectively. Q0.25,real and Q0.75,real denote the\n1st and 3rd quartiles for real news, respec-\ntively.\n2. Computing the Hybrid Threshold for Fake\nNews:\nθfake, percentile = Q0.90,fake\nθfake, std = IQRfake + σfake\nwhere σfake is the standard deviation of the\nAlignScore for fake news.\nθhybrid, fake = θfake, percentile + θfake, std\n2\n3. Computing the Hybrid Threshold for Real\nNews:\nθreal, percentile = Q0.90,real\nθreal, std = IQRreal −σreal\nwhere σreal denotes the standard deviation of\nthe AlignScore for real news.\nθhybrid, real = θreal, percentile + θreal, std\n2\nB.2 Natural Language Inference\nPrior hallucination detection studies have used sta-\ntistical, model-based, and human evaluation meth-\nods. We adopt the NLI model-based approach\nas it overcomes the limitations of statistical ap-\nproaches in handling syntactic and semantic vari-\nations (Ji et al., 2023). NLI metric also exhibits\n14294\nrobustness to lexical variability compared to token-\nmatching techniques by counterpart methods such\nas Information Retrieval and Question Answer Met-\nrics. NLI’s semantic/logical consistency assess-\nment strengths suit our misaligned hallucination\ndetection goals (Ji et al., 2023).\nIn the spirit of fighting fire with fire, we propose\nusing LLMs GPT-3.5-turbo and other LLMs for\nNLI hallucination detection in generated disinfor-\nmation. The core hypothesis is: synthetic real news\nshould logically be consistent with human-written\nreal news, while synthetic fake news should not.\nOur approach employs NLI using models like GPT-\n3.5, PaLM-2, and LLaMA-2, taking a majority vote\namong their decisions. Each model labels logical\nentailment for an input pair to classify if the syn-\nthetic text is consistent with the human-written text\nor not entailment otherwise.\nGiven a piece of human-written text (real news),\nT, we prompt an LLM to generate real news,\nT′real and fake news, T′fake, such that T′real\nis similar to T and T′fake is dissimilar to T. Due\nto LLM’s ability to sometimes generate texts un-\nfaithful to the prompt, we define an entailment\nmodel - N(.), such that for LLM-generated real\nnews, T′real should entail T, and for fake news,\nT′fake should Not-entail T. Therefore, using the\nentailment model, N(.), we can assess logical con-\nsistency between the original (human-written) and\nthe generated texts, thus removing misaligned hal-\nlucinated LLM-generated texts.\nWe find GPT-3.5-tubo can generate logically\nconsistent, genuine and fabricated synthetic con-\ntent, validating this hypothesis. However, when\nanalyzing more nuanced pairs, all LLMs occasion-\nally struggle with logical consistency. Thus, while\nillustrating the potential for hallucination detec-\ntion, our results also reveal limitations on more\ndifficult cases. After NLI, our generated dataset of\n43272 samples was reduced by 3617, resulting in\n39655 samples (See Fig. 14 for more details).\nB.3 Contextual Consistency\nBERTScore: This metric leverages the capabilities\nof the BERT language model to measure the simi-\nlarity between generated and reference texts. Due\nto BERT’s inherent ability to capture the context of\nentire sentences, BERTScore3 is especially suitable\nfor evaluating semantic and contextual consistency\n3For our evaluation, we utilized the BERTScore from Hug-\ngingFace with the ’Roberta-large’ model.\n(Karpinska et al., 2022; Zhang et al., 2019). Fig-\nure 16 shows high contextual consistency ranging\nfrom 92%-100% in our final experimental dataset.\nBERTScore’s ability to understand context and se-\nmantics makes it a formidable tool in the fight\nagainst disinformation. When integrated into a\ncomprehensive disinformation detection pipeline,\nit can significantly enhance the accuracy and ro-\nbustness of fake news detection efforts.\nB.4 Semantic Distance\nUsing Huggin Face implementation of AllenAI’s\nlongformer-base-4096 embeddings and cosine sim-\nilarity, we derived semantic distance scores for\nLLM-generated real and fake news (Beltagy et al.,\n2020). In our analysis of F3 LLM-generated disin-\nformation, we observed, to a large extent, indistinct\npatterns in the semantic distance scores for both\nreal and fake news. Specifically, the scores for real\nnews ranged from 0 to 0.01, while those for fake\nnews spanned from 0.001 to 0.014. This overlap\nmay suggest that, within this range, it might be\nchallenging to semantically distinguish between\nreal and fake news based solely on the semantic\ndistance scores (Mohammad and Hirst, 2012).\nLow semantic distance scores (close to 0) in-\ndicate high semantic similarity between two texts.\nHere, this suggests that LLM-generated disinforma-\ntion closely mimics real news semantics, making\ndifferentiation challenging based on content alone.\nThe narrow semantic gap highlights LLMs’ sophis-\ntication in generating articles aligning closely with\ngenuine news in meaning. In contrast, higher se-\nmantic distance scores signal greater divergence\nbetween texts, potentially from the model misinter-\npreting context, diverging from the topic, or gener-\nating factual inaccuracies (Mohammad and Hirst,\n2012).\nThe low scores pose a detection challenge, as\ntraditional methods relying on obvious inconsis-\ntencies may be insufficient. Thus, the nuanced,\ncontextually accurate nature of LLM outputs de-\nmands advanced, multifaceted detection strategies.\nThis close similarity underscores the risks of LLM\nmisuse for spreading synthetic disinformation. This\nemphasizes the need to monitor generative LLMs\ncarefully, understand their behaviors, and develop\nmitigation strategies (Mohammad and Hirst, 2012).\nB.5 Hallucination Misalignment Cases\nThis work defines disinformation as intentionally\nfabricating false information to mislead. We also\n14295\nFigure 14: PURIFY Logical Consistency Confusion Matrix.\nFigure 15: RQ1: Reduction in LLM-generated disin-\nformation samples using GPT-3.5-turbo. The initial\n“Total” represents the complete dataset. The subsequent\nreductions are achieved by applying consistency mea-\nsures. The “Logical” stage reflects the dataset size after\nremoving logically inconsistent samples based on a ma-\njority vote among GPT-3.5-turbo, PaLM-2-text-bison,\nand LLaMA-2 using the Natural Language Inference\nmetric. The final “Factual” stage depicts the dataset\nafter further refinement by eliminating samples with\nfactual inconsistencies using the Alignscore method.\nFigure 16: PURIFY Contextual Consistency Distribu-\ntion.\n14296\nFigure 17: PURIFY Semantic Distance Distribution.\nadopt the common data-to-text definition of hallu-\ncination - LLM-generated text that is intrinsically\n(contradictory) or extrinsically (factually incorrect)\nunfaithful to the input. Our input includes original\nreal news text and instructions to modify it into\neither real or fake news.\nNotably, disinformation and hallucinated text\nboth intend to mislead by definition. Therefore,\nwhen prompted to generate fake news, LLMs may\nproduce hallucinations aligned with that intent.\nHowever, we observed cases where LLMs gener-\nated no hallucinations despite fake prompts. Our\nframework PURIFY identifies these mismatches,\nwhich are unfaithful to the input by definition.\nThus, we categorize them as hallucinations to be re-\nmoved. The same principle applies to mismatches\nin real news generation.\nUltimately, our goal is to develop a dataset con-\ntaining (1) Non-hallucinated real news upholding\nsource integrity as prompted and (2) Hallucinated\nfake news intentionally not upholding source in-\ntegrity when prompted to fabricate. We present\ntwo cases of hallucination misalignment in Table\n10:\nC Dataset Description\nF3 is the first disinformation dataset that evaluated\nand removed LLM-generated content subjected to\nmisaligned ‘hallucination’—where LLMs produce\ntext unfaithful to the prompt. We ensure that real\nnews is actually real and fake news is fake (Ji et al.,\n2023). While rarely prior studies (Cui and Lee,\n2020; Sun et al., 2023; Zhou et al., 2023) investi-\ngated LLM-generated disinformation generation,\nthey did not rigorously verify the fidelity of such\ngenerated content and primarily focused on fake\nLLM data rather than both real and fake (Oshikawa\net al., 2018; Su et al., 2020; Murayama, 2021).\nPlease see a comparison of our dataset and other\ndatasets in Table 6.\nC.1 PaLM-2 LLM-Data Thematic Analysis\nWe conducted a Thematic analysis of our dataset\nafter PURIFY . We used PaLM-2 to label our data\nthemes. The top six themes include health, death,\nharm and Tragedy, public safety, and politics re-\nspectfully. See Fig. 18 for more details.\nFigure 18: Our dataset category based on PaLM The-\nmatic analysis.\nD Model Implementation Details\nThis section provides baseline implementation\nspecifics for the LLMs, customized detectors, and\nfine-tuned transformers used in our experiments.\nD.1 Generative LLM\nWe leveraged the OpenAI Software Development\nKit (SDK) and Application Programming Interface\n(API) to access GPT-3.5. We used the following\nhyperparameters: temperature of 0.7 and max to-\nken of 4096. All experiments occurred on Google\nColab Pro using API.\nFor LLaMA-70B-Chat and LLaMA-GPT we set\ntemperature to 0.7, top_p to 0.9, and max_tokens\nto 4096 for binary classification. For PaLM-2 we\nused: candidate count of 1, max output tokens of\n256, temperature of 0.2, top-P of 0.8, and top-K\nof 40. All experiments occurred on Google Colab\nusing API such as DeepInfra 5.\nD.2 Customized Detectors\nWe followed the original paper implementations for\ndEFEND\\C, TextCNN, and BiGRU without modi-\nfications. For example, dEFEND\\C was trained on\nPolitifacts data. See Appendix for training details.\nAll experiments occurred on Google Colab using\nAPI.\n5DeePInfra.com https://deepinfra.com/\n14297\nData HR HF LR LF N SM OA TD Start End\nCoAID ✓ ✓ ✗ ✗ ✓ ✓ ✓ 1 2019-Dec 2020-Sep\nSynthetic Lies ✗ ✗ ✗ ✓ ✓ ✓ ✗ 1 — 2023\nFakeNewsNet ✓ ✓ ✗ ✗ ✓ ✓ ✓ 1 — 2020\nMed-MMHL ✓ ✓ ✗ ✓ ✓ ✓ ✓ 2 2017-Jan 2023-May\nF3 ✓ ✓ ✓ ✓ ✓ ✓ ✓ 2 2017-Oct 2023-Feb\nTable 6: Overview of data sources: CoAID (Cui and Lee, 2020), Med-MMHL (Sun et al., 2023), Synthethetic lies\n(Zhou et al., 2023) Acronyms used: HR (Human Real), HF (Human Fake), LR (LLM Real), LF (LLM Fake), N\n(News), SM (Social Media), OA (Openly Available), TD (Topic Domain).\nNo. Detectors Description\nCustomized Detectors\n1 dEFEND\\C (Shu et al., 2019) dEFEND is the SOTA detector, and dEFEND\\C is a dEFEND variant only using the contents. It\nbegins by employing word-level attention mechanisms on individual sentences within the news content.\nSubsequently, the features extracted from these sentences are combined using an average pooling layer,\nwhich then feeds into a softmax layer for the final classification.\n2 TextCNN (Kim, 2014) Text-CNN employs convolutional neural networks to represent news content. With the use of multiple\nconvolution filters, it is adept at capturing text features of varying granularities. We follow the implemen-\ntation and the best parameters from the most recent model trained for fake news detection (Zhu et al.,\n2022).\n3 BiGRU (Ma et al., 2016) BiGRU is a common baseline for fake news detection. We follow the text-based BiGRU with RoBERTa\nembedding (Zhu et al., 2022).\nFine-Tuned Detectors\n4 BERT-Large (Kenton and Toutanova, 2019)BERT is an encoder-only Transformer model that is trained to predict randomly masked tokens in the\ninput. We use the BERT-large-uncased model.\n5 CT-BERT (Müller et al., 2023) CT-BERT v2 is BERT-large-uncased model trained on 97M messages from Twitter about COVID-19.\n6 RoBERTa (Liu et al., 2019) We use the RoBERTa-large model, a re-implementation of BERT with modifications to key hyperparame-\nters and minor embedding tweaks.\n7 DeBERTa (He et al., 2020) We utilize DeBERTa’s latest version, DeBERTa-v3-base model, which is pre-trained in ELECTRA-Style\nwith gradient disentangled embedding sharing. Due to limited computational resources, we are only able\nto run the base model.\nZero-shot LLM Detector\n8 GPT-3.5-Turbo4 OpenAI’s SOTA model is designed for a variety of natural language processing tasks.\n9 LLaMA-2-Chat (Rozière et al., 2023) Advanced language model for conversational AI applications.\n10 LLaMA-2-GPT4 (Touvron et al., 2023) A successor of the LLaMA series with advanced training techniques and better performance.\n11 Dolly-2 (Conover et al., 2023) Dolly-2 is an advanced generation model that exhibits human-like text generation capabilities.\nTable 7: Details of baseline models used for disinformation detection.\nFigure 19: F3 Cloze prompt.\nD.3 Fine-Tuned Transformers\nFor transformer training, we used a learning rate of\n2e-5, batch size of 4, weight decay of 0.01, Adam’s\nepsilon of 1e-08, and 1 training epoch. All training\noccurred on Google Colab with one GPU.\nE Experiment Full Result\nWe provide full results of the Macro-F1 score\nfor all detectors in this paper. Tables 11 and 12\nshow the results of the in-distribution and out-\nof-distribution performance of LLM-based mod-\nels, respectively. Tables 13 and 14 are the in-\ndistribution and out-of-distribution results with cus-\ntomized detectors, respectively. Tables 15 and 16\nare the in-distribution and out-of-distribution re-\nsults with fine-tuned transformer-based detectors,\nrespectively.\n14298\nName Description Cloze-Prompts\nX-CoT Motivated by Zhang et al. (2022)’s Auto-\nCoT approach, our Explain-COT (X-\nCoT) confidence reasoning prompt auto-\nmatically generates rationales and inte-\ngrated CoT to derive an output using the\nphrase. This prompt “explains to justify\nthe rationale behind your answer step by\nstep”\nimpersonator: “You are an AI assistant trained to detect fake\nnews.” instructor: “Analyze the given text, explain your reason-\ning step-by-step, and determine if it is real or fake news.”\nA-CoN Auto-CoN approach, our Auto-COT (A-\nCoT) confidence reasoning prompt auto-\nmatically generates rationales and confi-\ndence measures to derive an output us-\ning the phrase, “Explain or justify the\nrationale behind your answer and rate\nyour confidence ranging from 0 to 100.”\nimpersonator: “You are an AI assistant trained to detect fake\nnews with confidence estimates.”instructor: “Analyze the given\ntext, provide a confidence score between 0-100%, and determine\nif it is real or fake news.”\nMsReN Motivated by Reynolds and McDonell\n(2021); Bang et al. (2023), the multi-\nstep reasoning (MsR) approach employs\nthe simple statement, “Let’s solve this\nproblem by splitting it into steps.” It\nguides LLMs to think in steps, evalu-\nating various indicators and factors to\nreach a conclusive judgment.\nimpersonator: “You are an AI fact checker trained to detect fake\nnews.” instructor: “Analyze the text in detail as a fact checker\nwould solve it by splitting your reasoning into steps. Check for\nmisleading info, false claims, biased language. If real, respond\n’True’, if fake, respond ’False’.”\nMSReN_CoT Manual-CoT Wei et al. (2022) instructs\nLLMs to execute manually crafted CoT\ninstructions. MSReN_CoT integrates a\nseries of multi-step reasoning, including\nintermediate-step CoT reasoning. This\nmethodology effectively solves complex\nreasoning tasks (Zhao et al., 2023).\nimpersonator: “You are an AI fact checker trained to detect\nfake news.” instructor: “Analyze the text in detail as a fact\nchecker would. Explain your reasoning, stop, then think slowly,\nstep-by-step. If real, respond ’True’, if fake, respond ’False’.”\nDeF-Gen Given GPT-3.5-turbo’s observed weak-\nnesses in inductive and multi-step capa-\nbilities, our approach focuses contextu-\nally, emphasizing deductive and abduc-\ntive reasoning (Bang et al., 2023).\nimpersonator: “You are an AI assistant trained to detect fake\nnews.” instructor: “Determine if the text contains factual in-\nformation supported by evidence (real) or misleading/inaccurate\ninformation (fake). Respond with ’True’ or ’False’.”\nDeF-SpeC As GPT-3.5 is stronger in deductive rea-\nsoning, this approach implements prede-\nfined contextual rules to guide LLMs to-\nwards a logical conclusion (Bang et al.,\n2023).\nimpersonator: “You are an AI assistant trained to detect fake\nnews.” instructor: “Analyze the tone, language, sources to de-\ntermine if the text is real (supported by facts) or fake (misleading\ninfo). Respond with ’True’ or ’False’.”\nAnalyze_Cld2 Manual-CoT Wei et al. (2022) instructs\nLLMs to execute manually crafted CoT\ninstructions. Analyze_Cld2 integrates a\nseries of multi-step reasoning as manual\nintermediate-step CoT reasoning. This\nmethodology effectively solves complex\nreasoning tasks (Zhao et al., 2023).\nimpersonator: “You are an AI assistant specially trained to de-\ntect fake news with high accuracy.”instructor: “Analyze the\ngiven news article in depth. Check for the following indicators\nthat it may be fake news: (1)Inaccurate or misleading information,\n(2) Lack of evidence for claims, (3)Emotionally charged language\nwith the intent to provoke outrage or shock, (4) Biased portrayal\nof events or people (5) Unverified sources or ’experts’ (6) Logi-\ncal fallacies or conspiracy theories without proof. If the article\nexhibits multiple indicators of fake news, respond ’False’. If the\narticle is factual and supported by credible evidence, respond\n’True’. Your judgment should be highly accurate.”\nAnalyze_AI_GPT Manual-CoT Wei et al. (2022) instructs\nLLMs to execute manually crafted CoT\ninstructions. Analyze_AI_GPT inte-\ngrates a series of multi-step reasoning\nas manual intermediate-step CoT rea-\nsoning. This methodology effectively\nsolves complex reasoning tasks (Zhao\net al., 2023).\nimpersonator: “You are an AI trained with extensive knowl-\nedge up to 2021 on various news articles, both real and fake.”\ninstructor:“Analyze the given text for potential indicators of\nfake news, such as: (1) Sensationalist or emotionally charged\nlanguage. (2) Absence of specific details or dates. (3) Over-\ngeneralizations or sweeping statements. (4) Statements that are\ntoo good to be true or overly dramatic. (5) Lack of logical flow\nin arguments or jumping to conclusions without evidence. It’s\nessential to understand that without real-time verification capabil-\nities, your judgment will be based on patterns and knowledge up\nto your last training. Using these textual cues and your training,\ndetermine the credibility of the given text. If it seems factual\nand consistent with your training, respond ’True’. If it exhibits\npatterns typical of fake news, respond ’False’.”\nTable 8: F3 Cloze-Style Prompts for binary Zero-shot disinformation detection:\n14299\nName Description Cloze-Prompts\nVaN Our Vanilla prompt is our fundamen-\ntal baseline prompt designed to deliver\nbrief, precise instructions to LLMs, such\nas: “assess whether this piece of news\nis real or fake.”\nimpersonator: “You are an AI assistant trained to detect fake\nnews.” instructor: “Analyze the given text and determine if it is\nreal or fake news.”\nZ-CoT Kojima et al. (2022)’s Zero-Shot-\nCoT uniquely leverages LLMs’ self-\nformulated rationales by integrating a\nstandard VaN instruction with the sim-\nple phrase, “Let’s think step by step\nknown as Chain of Thoughts (CoT).”\nimpersonator: “You are an AI assistant trained to detect fake\nnews.” instructor: “Deeply Analyze the given text, think step-\nby-step, and determine if it is real or fake news.”\nTable 9: SOTA Cloze-Style Detection Prompts for binary Zero-shot disinformation detection:\nFigure 20: Generative Prompt Engineering\n14300\nFigure 21: Generative Prompt Examples\n14301\nCase Breakdown Explanation\nCase 1: In this case, GPT-3.5-turbo was prompted to use\ninput text containing (1) instructions and (2) source con-\ntent to generate subtle (minor) misleading news (fake)\nusing the source content (real). The majority vote between\nthe three LLMs is Entailment, thus logical consistent (No\ncontradiction/Intrinsic Hallucinations), however, it should\nbe logically inconsistent/contradicts/intrinsically halluci-\nnated. This is one instance where PURIFY removes a\nnon-hallucinations (intrinsic) case from fake prompted\nnews. Instruction: using the source text, create minor\ncategories of fake news.Sourcetext: \"Blake Masters: Of\ncourse, I support Lindsey Graham‚ 15-week bill, and I\nhope it passes. If not, he says he’ll introduce the third-\ntrimester federal ban he previously backed.\"LLM-output:\n\" Blake Masters just declared his support for Lindsey Gra-\nham’s 15week bill, which is a direct attack on women’s\nreproductive rights if this bill doesn’t pass. Masters plans\nto introduce a third-trimester federal ban this is outrageous\nand unacceptable. Protect women prochoice. First, we\npresent a breakdown of the above source text and the\nLLM-output to analyze the consistency and identify any\nintrinsic hallucination:\"\nSource Text: (1) Blake Mas-\nters supports Lindsey Graham’s\n15-week bill. (2) If it doesn’t\npass, Blake Masters plans to in-\ntroduce a third-trimester federal\nban that he previously backed.\nLLM-output: (1) Blake Mas-\nters supports Lindsey Graham’s\n15-week bill (stated as \"Blake\nMasters just declared his sup-\nport for Lindsey Graham’s 15-\nweek bill\"). (2)If the bill doesn’t\npass, Masters has plans to in-\ntroduce a third-trimester federal\nban (stated as \"if this bill doesn’t\npass, Masters plans to introduce\na third-trimester federal ban\").\nBoth the primary points from the\nsource text are faithfully reflected in\nthe LLM output, indicating that the\ncontent produced by the LLM is log-\nically consistent with the original in-\nformation and does not contradict it.\nTherefore, based on the case and\nthe breakdown, there is no intrin-\nsic hallucination in the LLM out-\nput as it retains the essential fac-\ntual information from the source\ntext without introducing any contra-\ndictions (the hypothesis supports the\npremise).Whilethe LLMoutput does\naddsomeinterpretativesentiment(e.g.,\n\"whichis a direct attackon women’s\nreproductiverights\"and\"thisis outra-\ngeousandunacceptable\"),butthissen-\ntimentis consistentwiththeinstruction\nto generate \"subtle misleading news\".\nCase 2: In this case, given a source text that is real news,\nGPT-3.5 is prompted to use the source text to create real\nnews. Using AlignScore Metric, which ranges from 0-1,\nwhere close to 0 means a low degree of factual consis-\ntency and 1 means a high degree of factual consistency,\nto determine factual consistency or inconsistency (extrin-\nsic hallucination). Considering edge cases, we created a\nthreshold where the Alignscore ranges from 0.0-0.36 for\nfake news and 0.61-1.0 for real news. While the following\nshows logical consistency via LLMs’ majority vote, its\nAlign evaluation is 0.’0.60, which is below 0.61.Instruc-\ntion: using the source text, create minor categories of fake\nnews.SourceText: In the first three quarters of 2022, the\nrailroad industry made $21 billion in profits, provided $25\nbillion in stock buybacks and dividends‚ and CEOs are\npaid up to $20 million a year, while railroad workers have\n\"zero guaranteed sick leave.LLMoutput: ALERT! Rail-\nroad industry profits soar to $21 billion in 2022 Q1-Q3,\nwith CEOs making up to $20 million a year. Meanwhile,\nworkers are denied any guaranteed sick leave.\nSourceText: (1) In the first three\nquarters of 2022, the railroad in-\ndustry made $21 billion in prof-\nits. (2) They provided $25 bil-\nlion in stock buybacks and div-\nidends. (3) CEOs are paid up\nto $20 million a year. (4) Rail-\nroad workers have zero guaran-\nteed sick leave. LLM-output:\n(1) The railroad industry’s prof-\nits surged to $21 billion in 2022\nfrom Q1 to Q3. (2)CEOs are\nearning up to $20 million annu-\nally. (3) Workers are not given\nany guaranteed sick leave.\n(1) Points 1-3 in the LLM output are\nconsistent with the source text in terms\nof facts. (2) The LLM output omits\nthe fact about $25 billion in stock buy-\nbacks and dividends that was present in\nthe source text. Considering the Align-\nScore Metric threshold provided, the\nmain facts of the LLM output align\nwell with the source text.However,\nthere is an omission of a piece of\nfactual information from the source\ntext, and additional emojis and hash-\ntags have been introduced in the\nLLM output.Whilethe LLMoutput\nretainsthekeypointsofthesourcetext,\nthereare minor extrinsic (nonfactual)\nhallucinationsdue to the omissionof\ninformationaboutstockbuybacksand\ndividends.\nTable 10: PURIFY: Hallucination Misalignment Cases\n14302\nZero-shot Disinformation Detection\nCloze-prompt engineering Articles Posts\nModels Human LLM-Min LLM-Maj LLM-Crit Human LLM Min LLM-Maj LLM-Crit ¯x\nGPT-3.5-Turbo-175B\nVaN 0.6761 0.7676 0.7753 0.7886 0.5402 0.5519 0.6245 0.7398 0.6704\nZ-CoT 0.6823 0.7491 0.7944 0.8027 0.5424 0.5484 0.6301 0.7289 0.6848\nX-CoT 0.6694 0.8220 0.8393 0.8470 0.5174 0.5307 0.6616 0.6624 0.6812\nA-CoN 0.6296 0.8646 0.8525 0.8824 0.5006 0.5033 0.5759 0.6641 0.6841\nMsReN 0.6700 0.8066 0.8129 0.8235 0.6186 0.6986 0.7303 0.9027 0.7579\nMsReN_CoT 0.6611 0.8560 0.8622 0.8650 0.5955 0.6785 0.7708 0.7963 0.7617\nDeF_Gen 0.6993 0.7462 0.7727 0.7864 0.5559 0.6011 0.6712 0.7870 0.6902\nDeF_SpeC 0.6755 0.8189 0.8365 0.8377 0.5564 0.5497 0.6103 0.8044 0.7110\nAnalyze_Cld2 0.6718 0.9080 0.9096 0.9230 0.5535 0.6562 0.7085 0.7259 0.7571\nAnalyze_AI_GPT 0.5863 0.8539 0.8608 0.8742 0.4715 0.4594 0.5140 0.7113 0.6667\nAverage 0.6604 0.8190 0.8359 0.8484 0.5505 0.5998 0.6646 0.7640 0.7178\nLLaMA-GPT-70B\nVaN 0.5415 0.7084 0.7297 0.7283 0.5599 0.6174 0.6558 0.6672 0.6504\nZ-CoT 0.5455 0.7343 0.7615 0.7435 0.5750 0.6653 0.7562 0.6589 0.6679\nX-CoT 0.5148 0.6313 0.6670 0.6415 0.5019 0.6487 0.6784 0.5984 0.6102\nA-CoN 0.5164 0.6193 0.6040 0.6137 0.5446 0.6027 0.5569 0.6028 0.5827\nMsReN 0.5250 0.6114 0.6491 0.6162 0.5488 0.5796 0.6115 0.5991 0.5933\nMsReN_CoT 0.5274 0.5921 0.5968 0.5786 0.5419 0.5851 0.6164 0.5035 0.5677\nDeF_Gen 0.5650 0.7478 0.7622 0.7887 0.5504 0.6193 0.6571 0.7108 0.6751\nDeF_SpeC 0.5521 0.7192 0.7145 0.7077 0.5798 0.6811 0.5872 0.6483 0.6362\nAnalyze_Cld2 0.5246 0.7067 0.7373 0.6999 0.5792 0.6166 0.6490 0.6489 0.6452\nAnalyze_AI_GPT 0.5087 0.6906 0.6816 0.6249 0.5487 0.6105 0.6424 0.6536 0.6201\nAverage 0.5398 0.6960 0.7055 0.6803 0.5579 0.6286 0.6565 0.6440 0.6386\nLLaMA-2-70B\nVaN 0.6242 0.6081 0.6375 0.6284 0.6309 0.6729 0.6734 0.7366 0.6390\nZ-CoT 0.6431 0.6255 0.6584 0.6237 0.5917 0.5785 0.5788 0.6754 0.6345\nX-CoT 0.5964 0.6202 0.6190 0.6094 0.5629 0.6456 0.6451 0.6646 0.6204\nA-CoN 0.5695 0.5353 0.5621 0.5646 0.5655 0.6798 0.7201 0.7029 0.6124\nMsReN 0.6107 0.5284 0.5566 0.5674 0.5724 0.6489 0.6236 0.6518 0.5950\nMsReN_CoT 0.5401 0.5087 0.5155 0.5151 0.6011 0.6365 0.6895 0.6114 0.5771\nDeF_Gen 0.5753 0.5726 0.5888 0.5881 0.6143 0.7381 0.7753 0.7573 0.6512\nDeF_SpeC 0.6108 0.6018 0.6328 0.6414 0.5597 0.6609 0.6910 0.7478 0.6433\nAnalyze_Cld2 0.6710 0.7261 0.7314 0.7315 0.5307 0.5537 0.6024 0.6817 0.6533\nAnalyze_AI_GPT 0.5159 0.4401 0.4444 0.4220 0.5042 0.4992 0.4343 0.4317 0.4614\nAverage 0.5958 0.5737 0.5984 0.5911 0.5869 0.6344 0.6476 0.6727 0.6126\nDolly-2-12B\nVaN 0.5433 0.4951 0.4626 0.5088 0.5078 0.5011 0.4628 0.4031 0.4858\nZ-CoT 0.4972 0.5145 0.5043 0.5066 0.5447 0.4414 0.5194 0.4500 0.4973\nE-CoT 0.5465 0.4980 0.4445 0.4731 0.4765 0.4821 0.5372 0.3684 0.4783\nA-CoN 0.5867 0.4792 0.4697 0.4550 0.4572 0.4912 0.4890 0.4263 0.4818\nMsReN 0.4804 0.5220 0.4900 0.5274 0.5202 0.5867 0.4679 0.5348 0.5124\nMsReN_CoT 0.5286 0.4401 0.5291 0.4968 0.4579 0.5434 0.4612 0.4424 0.4874\nDeF_Gen 0.4892 0.4172 0.4313 0.4500 0.4934 0.4653 0.4031 0.4684 0.4523\nDeF_SpeC 0.4818 0.4356 0.4229 0.4538 0.5094 0.5150 0.4394 0.4307 0.4610\nAnalyze_Cld2 0.4457 0.5089 0.5143 0.4862 0.5204 0.5258 0.4886 0.4457 0.4922\nAnalyze_AI_GPT 0.5414 0.5139 0.5029 0.4922 0.4828 0.4923 0.4537 0.5266 0.5014\nAverage 0.5151 0.4825 0.4822 0.4889 0.5076 0.4964 0.4669 0.4666 0.4883\nTable 11: In-distribution performance on disinformation created before pre-GPT-3.5-turbo training. ¯xdenoted the\nmean. LLM-Min denotes minor, LLM-Maj denotes major, and LLM-Crit denotes major.\n14303\nZero-shot Disinformation Detection\nCloze-prompt engineering Articles Posts\nModels Human LLM-Min LLM-Maj LLM-Crit Human LLM-Min LLM-Maj LLM-Crit ¯x\nGPT-3.5-Turbo-175B\nVaN 0.6633 0.6726 0.6791 0.6234 0.7343 0.6448 0.6985 0.6485 0.6707\nZ-CoT 0.6726 0.6257 0.7074 0.6235 0.7202 0.6096 0.6776 0.6549 0.6614\nX-CoT 0.7717 0.7232 0.6912 0.6907 0.6984 0.5194 0.5697 0.6631 0.6663\nA-CoN 0.8202 0.8409 0.7393 0.7181 0.6646 0.5328 0.5845 0.6840 0.6971\nMsReN 0.7465 0.7180 0.7577 0.6375 0.7289 0.6860 0.7181 0.7054 0.7124\nMsReN_CoT 0.7423 0.8224 0.7247 0.7108 0.7312 0.5957 0.6007 0.7066 0.7042\nDeF_Gen 0.6078 0.5548 0.6938 0.5438 0.7171 0.6619 0.7435 0.6747 0.6497\nDeF_SpeC 0.6853 0.7196 0.7245 0.6648 0.6911 0.5598 0.5940 0.7211 0.6701\nAnalyze_Cld2 0.7853 0.7925 0.6751 0.7300 0.7662 0.7385 0.7026 0.7145 0.7382\nAnalyze_AI_GPT 0.6751 0.7219 0.7429 0.7494 0.6548 0.5238 0.5178 0.6549 0.6676\nAverage 0.7170 0.7091 0.7136 0.6792 0.7107 0.6072 0.6407 0.6828 0.6825\nLLaMA-GPT-70B\nVaN 0.7528 0.6521 0.6936 0.6388 0.6182 0.5384 0.5525 0.5290 0.6218\nZ-CoT 0.7705 0.6522 0.5279 0.6074 0.6000 0.5786 0.5979 0.5283 0.5952\nX-CoT 0.6971 0.4171 0.5896 0.3143 0.5667 0.5652 0.6521 0.4633 0.5331\nA-CoN 0.7152 0.4007 0.5039 0.4805 0.5945 0.5626 0.5503 0.4049 0.5392\nMsReN 0.7358 0.5694 0.5680 0.5272 0.6113 0.5912 0.5596 0.4445 0.5757\nMsReN_CoT 0.6718 0.5055 0.4642 0.4598 0.5514 0.5061 0.5622 0.3442 0.5081\nDeF_Gen 0.6636 0.6522 0.6938 0.5816 0.6171 0.5534 0.5954 0.6068 0.6205\nDeF_SpeC 0.7758 0.6637 0.6936 0.6354 0.5945 0.6040 0.5954 0.5547 0.6522\nAnalyze_Cld2 0.6235 0.5393 0.5481 0.5042 0.5945 0.5923 0.6738 0.4793 0.5696\nAnalyze_AI_GPT 0.6459 0.6771 0.5957 0.6475 0.6198 0.5747 0.6291 0.5531 0.6179\nAverage 0.7112 0.5797 0.6049 0.5588 0.6072 0.5667 0.5961 0.5218 0.5933\nLLaMA-2-70B\nVaN 0.6926 0.6081 0.6375 0.6284 0.6828 0.6729 0.6734 0.7366 0.6552\nZ-CoT 0.7395 0.6255 0.6584 0.6237 0.6221 0.5785 0.5788 0.6754 0.6375\nX-CoT 0.5936 0.6202 0.6190 0.6094 0.5643 0.6456 0.6451 0.6646 0.6203\nA-CoN 0.6137 0.5353 0.5621 0.5646 0.6497 0.6798 0.7201 0.7029 0.6289\nMsReN 0.6598 0.5284 0.5566 0.5674 0.6079 0.6489 0.6236 0.6518 0.6059\nMsReN_CoT 0.6292 0.5087 0.5155 0.5151 0.6092 0.6365 0.6895 0.6114 0.5883\nDeF_Gen 0.4384 0.5726 0.5888 0.5881 0.6752 0.7381 0.7753 0.7573 0.6421\nDeF_SpeC 0.5895 0.6018 0.6328 0.6414 0.6689 0.6609 0.6910 0.7478 0.6541\nAnalyze_Cld2 0.6581 0.7261 0.7314 0.7315 0.5680 0.5537 0.6024 0.6817 0.6564\nAnalyze_AI_GPT 0.3959 0.4401 0.4444 0.4220 0.5400 0.4992 0.4343 0.4317 0.4648\nAverage 0.6103 0.5928 0.5976 0.6024 0.6165 0.6354 0.6444 0.6762 0.6218\nDolly-2-12B\nVaN 0.5698 0.4736 0.4706 0.4510 0.4897 0.5098 0.5311 0.4197 0.4893\nZ-CoT 0.6177 0.4695 0.4649 0.3552 0.4903 0.5934 0.5072 0.4524 0.4937\nX-CoT 0.6111 0.4362 0.4343 0.4286 0.4510 0.5457 0.4717 0.4187 0.4747\nA-CoN 0.6035 0.3817 0.4838 0.4585 0.4125 0.5885 0.5420 0.4696 0.4860\nMsReN 0.6779 0.5099 0.4598 0.4624 0.5286 0.5516 0.5159 0.5148 0.5276\nMsReN_CoT 0.6388 0.4519 0.4846 0.3690 0.4775 0.5412 0.4725 0.5271 0.4952\nDeF_Gen 0.5874 0.4477 0.5139 0.3382 0.4747 0.5482 0.5277 0.4562 0.4866\nDeF_SpeC 0.6153 0.4024 0.4447 0.3796 0.4791 0.5349 0.4598 0.5450 0.4825\nAnalyze_Cld2 0.5856 0.4146 0.4112 0.4002 0.4756 0.5032 0.4819 0.4305 0.4632\nAnalyze_AI_GPT 0.6312 0.4563 0.5091 0.3775 0.5341 0.5463 0.4998 0.5127 0.4958\nAverage 0.6127 0.4470 0.4692 0.4049 0.4828 0.5386 0.5044 0.4715 0.4901\nTable 12: Out-of-Distribution performance on disinformation created after GPT-3.5-turbo training date. ¯xdenoted\nthe mean. LLM-Min denotes minor, LLM-Maj denotes major, and LLM-Crit denotes major.\n14304\nDomain-Specific Disinformation Detection\nDeep Learning Models Articles Posts\nModels Human LLM Min LLM-Maj LLM-Crit Human LLM Min LLM-Maj LLM-Crit ¯x\nCustomized Deep Learning\ndEFEND 0.7850 0.7187 0.7320 0.7288 0.5527 0.5815 0.6932 0.6278 0.6775\nTextCNN 0.6025 0.6148 0.6294 0.6128 0.5520 0.6581 0.6697 0.6886 0.6285\nBiGRU 0.6373 0.6431 0.6701 0.6228 0.5400 0.7036 0.6927 0.7855 0.6619\nAverage 0.6750 0.6589 0.6772 0.6548 0.5483 0.6477 0.6852 0.7006 0.6563\nTable 13: Domain-Specific In-Distribution Results with Customized Detectors.\nDomain-Specific Disinformation Detection\nDeep learning Models Articles Posts\nModels Human LLM Min LLM-Maj LLM-Crit Human LLM Min LLM-Maj LLM-Crit ¯x\nCustomized Deep Learning\ndEFEND 0.3369 0.1351 0.1127 0.0781 0.4967 0.5446 0.2790 0.4903 0.3092\nTextCNN 0.5581 0.4588 0.4273 0.5533 0.5847 0.4849 0.4708 0.6859 0.5280\nBiGRU 0.4877 0.5765 0.5143 0.5533 0.5266 0.5802 0.5438 0.7486 0.5664\nAverage 0.4609 0.3901 0.3514 0.3949 0.5360 0.5366 0.4312 0.6416 0.4679\nTable 14: Domain-Specific Out-of-Distribution Results with Customized Detectors.\nDomain-Dependent Disinformation Detection\nTransformer Models Articles Posts\nModels Human LLM Min LLM-Maj LLM-Crit Human LLM Min LLM-Maj LLM-Crit ¯x\nFine-tuned Transformer-based Detector\nBERT-Large 0.7799 0.9662 0.9626 0.9845 0.8787 0.9627 0.9594 0.9531 0.9308\nCT-BERT 0.4258 0.9679 0.9469 0.9821 0.8849 0.9781 0.9692 0.906 0.8852\nRoBERTa 0.8012 0.9843 0.9787 0.9871 0.8819 0.9877 0.9564 0.9733 0.9575\nDeBERTa 0.8031 0.982 0.9747 0.9839 0.8695 0.962 0.9599 0.9733 0.9398\nAverage 0.7025 0.9751 0.9657 0.9844 0.8787 0.9726 0.9612 0.9514 0.9283\nTable 15: Domain-Dependent In-Distribution Results with Fine-tuned Transformer-based Detectors.\nDomain-Dependent Disinformation Detection\nTransformer Models Articles Posts\nModels Human LLM Min LLM-Maj LLM-Crit Human LLM Min LLM-Maj LLM-Crit ¯x\nFine-tuned Transformer-based Detector\nBERT-Large 0.5816 0.6791 0.7509 0.772 0.6287 0.9561 0.9418 0.897 0.7759\nCT-BERT 0.3139 0.7202 0.7604 0.6482 0.689 0.9641 0.9413 0.9213 0.8698\nRoBERTa 0.5695 0.7353 0.7855 0.7722 0.6043 0.988 0.9302 0.9859 0.7964\nDeBERTa 0.5836 0.759 0.7509 0.648 0.6273 0.9561 0.945 0.9205 0.7738\nAverage 0.5121 0.7234 0.7619 0.7101 0.6373 0.966 0.9395 0.9311 0.8039\nTable 16: Domain-Dependent Out-of-Distribution Results with Fine-tuned Transformer-based Detectors.\n14305",
  "topic": "Disinformation",
  "concepts": [
    {
      "name": "Disinformation",
      "score": 0.8722584843635559
    },
    {
      "name": "Computer science",
      "score": 0.5577031970024109
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5182604789733887
    },
    {
      "name": "Computer security",
      "score": 0.48426195979118347
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4475312829017639
    },
    {
      "name": "Internet privacy",
      "score": 0.38666391372680664
    },
    {
      "name": "Social media",
      "score": 0.2884383201599121
    },
    {
      "name": "Artificial intelligence",
      "score": 0.27831292152404785
    },
    {
      "name": "World Wide Web",
      "score": 0.2365720570087433
    },
    {
      "name": "History",
      "score": 0.13987722992897034
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ]
}