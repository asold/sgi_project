{
    "title": "Pre-Training a Language Model Without Human Language",
    "url": "https://openalex.org/W3114619416",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5071482462",
            "name": "Cheng-Han Chiang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5040508737",
            "name": "Hung-yi Lee",
            "affiliations": [
                "National Taiwan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2994967700",
        "https://openalex.org/W2973154008",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W3104570641",
        "https://openalex.org/W3101860695",
        "https://openalex.org/W2126793110",
        "https://openalex.org/W2970120757",
        "https://openalex.org/W3105069964",
        "https://openalex.org/W2946359678",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W3099299360",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2953369973"
    ],
    "abstract": "In this paper, we study how the intrinsic nature of pre-training data contributes to the fine-tuned downstream performance. To this end, we pre-train different transformer-based masked language models on several corpora with certain features, and we fine-tune those language models on GLUE benchmarks. We find that models pre-trained on unstructured data beat those trained directly from scratch on downstream tasks. Our results also show that pre-training on structured data does not always make the model acquire ability that can be transferred to natural language downstream tasks. To our great astonishment, we uncover that pre-training on certain non-human language data gives GLUE performance close to performance pre-trained on another non-English language.",
    "full_text": "Pre-Training a Language Model Without Human Language\nCheng-Han Chiang\nNational Taiwan University, Taiwan\ndcml0714@gmail.com\nHung-yi Lee\nNational Taiwan University, Taiwan\nhungyilee@ntu.edu.tw\nAbstract\nIn this paper, we study how the intrinsic na-\nture of pre-training data contributes to the\nﬁne-tuned downstream performance. To this\nend, we pre-train different transformer-based\nmasked language models on several corpora\nwith certain features, and we ﬁne-tune those\nlanguage models on GLUE benchmarks. We\nﬁnd that models pre-trained on unstructured\ndata beat those trained directly from scratch\non downstream tasks. Our results also show\nthat pre-training on structured data does not al-\nways make the model acquire ability that can\nbe transferred to natural language downstream\ntasks. To our great astonishment, we un-\ncover that pre-training on certain non-human\nlanguage data gives GLUE performance close\nto performance pre-trained on another non-\nEnglish language.\n1 Introduction\nNeural language models (LMs) are prevalent in\nnowadays natural language processing (NLP) com-\nmunity, and they are indispensable to a variety of\nNLP tasks. Researchers have devoted themselves\nto understanding what these models have learned\nand how they work. Probing a trained model is\nwidely used to understand to what extent a model\nlearns certain linguistic features (Kovaleva et al.,\n2019; Hewitt and Manning, 2019; Tenney et al.,\n2019, 2018; Lin et al., 2019). Another line of re-\nsearch focuses more on how training corpora affect\nthe trained LMs (Micheli et al., 2020; Gururangan\net al.; Zhang et al., 2020).\nIn this work, we aim to understand how down-\nstream performance varies across models pre-\ntrained on data of particular traits. The core prob-\nlem we determine to answer is: What factors in the\npre-training data make a pre-trained transformer\nLM perform better on downstream tasks than their\ntrained from scratch counterparts? To answer this\nquestion, we pre-train many different transformer\nLMs on dataset from miscellaneous disciplines,\nranging from amino acid sequences in complex\nliving organisms to artiﬁcial data generated by a\nsimple python script. We then ﬁne-tune them on\nEnglish downstream tasks. The process is illus-\ntrated in Figure 1.\nRecently, Papadimitriou and Jurafsky (2020) pro-\nposed to train an LSTM LM on a non-natural\nlanguage dataset and test the LM’s perplexity\non natural language. They observed that LSTM\nLM trained on structured dataset gives perplexity\nfar lower than those trained on unstructured data.\nWhile the observations are intriguing, this setting\ndoesn’t match the common setting widely applied\nnowadays, in which we ﬁne-tune pre-trained LMs\non downstream tasks. This is the ﬁrst paper inves-\ntigating whether masked language model (MLM)\npre-training on non-natural language aids down-\nstream natural language tasks’ performance.\nBased on the experiments, we have the following\nobservations:\n• We reveal that ﬁne-tuning models pre-trained\non unstructured data outperforms model\ntrained from scratch on downstream tasks.\n• We ﬁnd that structured pre-training data is not\na sufﬁcient condition to a pre-trained model\nthat can perform well on NLP tasks.\n• We discover that pre-training on a simple arti-\nﬁcial dataset with hierarchical structure leads\nto downstream performance comparable to\nmodels pre-trained on human language.\n• Our experiments show that token distribution\nis not the key factors to how well the model\ntransferred to downstream tasks, while the\nnumber of token embeddings used during pre-\ntraining affects downstream performance.\narXiv:2012.11995v1  [cs.CL]  22 Dec 2020\nFigure 1: Work ﬂow of our experiments: We ﬁrst pre-train the whole masked language model on L1 (protein\nsequence in this ﬁgure), and ﬁne-tune the whole model on English downstream tasks. We then test the performance\non the ﬁne-tuned downstream task. It takes about 3 days to ﬁnish the whole process on a single V100.\n2 Experiment Setups\nIn our experiments, we pre-train n RoBERTa-\nbase (Liu et al., 2019) models on n different types\nof pre-training data. We call the pre-training data\nL1 (ﬁrst language). We then evaluate the pre-\ntrained models’ ability by ﬁne-tuning them on dif-\nferent downstream tasks. The overall workﬂow is\nillustrated in Figure 1.\nWe adopt the classic GLUE (Wang et al., 2019)\nbenchmarks to evaluate the models pre-trained on\ndifferent L1s while excluding WNLI following De-\nvlin et al. (2019). For each task, we use a certain set\nof hyperparameters and the same random seed to\nﬁne-tune the model, and we report the results on the\nevaluation set. Details regarding all experiments\ncan be found in Appendix A.\nOur experiment setup may seem to resemble\nthe Test for Inductive Bias via Language Model\nTransfer (TILT) proposed in Papadimitriou and\nJurafsky (2020) at ﬁrst sight, which pre-trains an\nLSTM LM on L1, follows by only ﬁne-tuning word\nembeddings on Spanish, and test the perplexity on\nSpanish. However, the main purpose of TILT is to\nanalyze the encoding of grammatical structure in\nLMs, so they do not ﬁne-tune LSTM on Spanish.\nOn the contrary, our goal is to understand what\nfactors in pre-training data make the pre-trained\nmodel perform better than models trained from\nscratch on downstream tasks.\n3 Pre-training Data\nWe use two baseline pre-training dataset for our\nexperiments: the random baseline and the Zipf\nbaseline, both corpora have 29995 tokens, exclud-\ning 5 special tokens. For the random baseline, we\ndraw the tokens from a uniform distribution and\nform sequences with a length of 90 to 120 tokens.\nFor the Zipf baseline, we sample the tokens from\nthe same uni-gram distribution of English. We also\npre-train an English MLM with a subset of the\nEnglish Wikipedia to serve as the performance up-\nper bound. The pre-training corpora size is around\n80MB for the previous three datasets.\nWe select several pre-training corpora in distinct\ndisciplines that contain structure, including a bio-\nlogical dataset, a programming language corpus,\nan artiﬁcial dataset with a hierarchical structure,\nand a human language.\nThe biological dataset we adopt is amino acid\nsequence corpora obtained from Min et al. (2019).\nThe characteristic of a protein is determined by its\nprimary structure, i.e. the amino acid sequence.\nChemical bonds between amino acids determine\nthe secondary and tertiary structure of the folded\nprotein, which further determines the functions of\nthe protein. We use the one-letter abbreviation\n(A-Z) to represent each amino acid, and the total\nnumber of tokens in this dataset is 36M.\nFor programming language, we use Habeas cor-\npus from Movshovitz-Attias and Cohen (2013),\nwhich contains tokenized Java script. We use the\ncode from Papadimitriou and Jurafsky (2020) to\nextract the data and remove tokens that are labeled\nas a comment, making the training corpus contain\nonly programming language. The total number\nof tokens in the pre-training data is 10M, and the\nvocabulary size of the model is 30K.\nThe artiﬁcial dataset we construct has a vocabu-\n5 2 0 0 2 33 33 5\nFigure 2: An illustration of the artiﬁcial dataset.\nlary size of 28996, and the total number of tokens\nin training data is 23.5M. The dataset is generated\nby the following stack-based grammar, following\nPapadimitriou and Jurafsky (2020): At each time\nstep t, we sample Xt from a Bernoulli distribution\nwith P(Xt = 1) = 0.4. If Xt = 1, we sample\na token based on English’s uni-gram distribution,\nplace the sampled token at position t of the gener-\nated sequence, and push the same token into the\nstack. When Xt = 0, we pop the top element of the\nstack and put the popped token at position t of the\ngenerated sequence. Figure 2 shows a simple exam-\nple. We can observe from Figure 2 that sequence\ngenerated in this manner contains a nesting hierar-\nchical parentheses structure, which is similar to the\ndependency tree structure in natural language.\nThe last dataset used is a human language. We\nselect a human language different from down-\nstream tasks to compare the effect of non-human\nlanguage pre-training data. We use Kannada from\nOSCAR dataset (Suárez et al., 2020). Kannada\nis a language predominantly spoken by the people\nin the southern western region of India. The main\nreason we choose this dataset lies in its subject(S)-\nobject(O)-verb(V) structure, different from the S-\nV-O structure of our target language used in ﬁne-\ntuning. The pre-training corpora size is 160MB,\nand the vocabulary size used in pre-training is 30K.\n4 Experiments and Results\nThe overall results are illustrated in Table 1. In\nthis section, we discuss how certain aspects of the\npre-training corpora affect how good a model can\nbecome. By the word good, we refer to the model’s\nability to be ﬁne-tuned on downstream tasks, which\nis the performance improvement over training the\nmodel from scratch on downstream tasks.\n4.1 Is Structured Data All You Need For\nPre-training?\nWe intend to answer this question: Is structured\ndata the key to a good pre-trained model? We com-\npare the models pre-trained on structured data with\nmodels pre-trained on unstructured baselines. If\nthe downstream performance of models pre-trained\non structured data can beat their unstructured coun-\nterparts, then we may conclude that structure in the\npre-training data is a key factor in the success of\npre-trained transformer language models.\nFrom the ﬁrst two blocks of Table 1, we ﬁnd that\nmodels pre-trained on unstructured data outperform\nthe models trained from scratch. This suggests\nthat the pre-trained model can still aid downstream\nperformance, albeit the seemingly meaningless pre-\ntraining corpora.\nFrom the third block in Table 1, we ﬁnd that pre-\ntraining on structured data may not always lead to\na better model. Models pre-trained on amino acid\nand Java scripts are almost on a par with the models\ntrained from scratch. Not much to our surprise, the\nmodel pre-trained on Kannada performs far better\nthan the two baseline models.\nAmazingly, ﬁne-tuning the model pre-trained on\nartiﬁcial data gives comparable performance com-\npared with the model pre-trained on Kannada. This\nimplies that it might be worth trying to pre-train\na model on this kind of hierarchical nesting struc-\ntured dataset, and ﬁne-tune the model on some low\nresource languages to obtain decent downstream\nperformance. The artiﬁcial dataset consists of no\nsemantic knowledge useful for downstream natu-\nral language tasks, so it is reasonable to infer that\nmost knowledge the model learns from pre-training\nis the skill to model the hierarchical structure and\nlong-term dependency. Equipped with this ability,\nthe model can outrun models trained from unstruc-\ntured data.\nOur results show that models beneﬁt from pre-\ntraining on a certain type of structured corpora,\nwhile not every structured corpus leads to a good\npre-trained model for NLP downstream tasks.\n4.2 Does Pre-training Data Token\nDistribution Affect the Performance on\nDownstream Tasks?\nWe notice that the two baseline models’ perfor-\nmance is similar in almost all downstream tasks.\nThis indicates that the uni-gram distribution of\ntokens in the training corpora makes little differ-\nence to the downstream performance when the pre-\ntraining data themselves are unstructured. We fur-\nther ask whether this is also the case when the data\nis structured. We construct the artiﬁcial dataset as\nin Section 3, and aside from sampling based on\nZipf distribution, we create another dataset whose\ntokens are sampled from the uniform distribution\nL1 STS-B QNLI QQP CoLA SST-2 MNLI MRPC RTE Avg\n1 No Pre-train 0.17 0.60 0.75 0.13 0.83 0.64 0.67 0.50 0.54\nPre-train En 0.76 0.83 0.86 0.34 0.88 0.76 0.77 0.53 0.72\n2 Rand. Baseline 0.29 0.66 0.80 0.14 0.83 0.65 0.77 0.51 0.58\nZipf Baseline 0.38 0.66 0.80 0.11 0.82 0.64 0.81 0.49 0.59\n3\nAmino Acid 0.16 0.65 0.79 0.07 0.82 0.60 0.81 0.44 0.55\nJava Script 0.31 0.67 0.77 0.02 0.81 0.66 0.75 0.51 0.56\nKannada 0.76 0.77 0.83 0.12 0.81 0.69 0.80 0.55 0.67\n4 Artiﬁcial (Uni.) 0.72 0.77 0.80 0.14 0.81 0.69 0.77 0.52 0.65\nArtiﬁcial (Zipf) 0.76 0.77 0.83 0.11 0.82 0.69 0.75 0.53 0.66\n5\nArtiﬁcial (5000) 0.73 0.76 0.82 0.09 0.84 0.69 0.82 0.53 0.66\nArtiﬁcial (500) 0.42 0.68 0.80 0.08 0.81 0.68 0.79 0.51 0.60\nArtiﬁcial (50) 0.18 0.62 0.74 0.06 0.82 0.61 0.77 0.52 0.54\nArtiﬁcial (50-s) 0.65 0.73 0.84 0.06 0.80 0.64 0.75 0.50 0.62\nTable 1: Downstream results of different pre-trained models, and the model trained from scratch on downstream\ntasks (no pre-train in the ﬁrst row). The evaluation metrics of MRPC and QQP are F1 score, Spearman correlation\ncoefﬁcient is reported for STS-B, and the rest tasks are evaluated with accuracy. Results of MNLI are the average\nof matched and mismatched. Please refer to Section 4.2 and Section 4.3 for the meaning of parentheses in the last\ntwo blocks. 50-s stands for 50-substitute in Section 4.3. Abbreviation used: En: English, Rand: random, Uni:\nuniform.\nover tokens except for special tokens. The results,\ndemonstrated in the fourth block in Table 1, show\nthat even when the pre-training data is structured,\ntoken distribution still has little inﬂuence on how\nwell the model can be ﬁne-tuned.\n4.3 Does Token Numbers Mismatch between\nPre-training and Fine-tuning Affect\nDownstream Performance?\nThis section investigates whether the mismatch\nbetween vocabulary size during pre-training1 and\nﬁne-tuning contributes to how well the pre-trained\nmodel performs on downstream tasks. To study the\ninﬂuence of vocabulary size, we construct different\nartiﬁcial data by sampling tokens from different\nbin sizes (50, 500, and 5000). While the vocabu-\nlary size during pre-training is different for those\nmodels, their actual word embedding table sizes\nare still the same.\nFrom the last block in Table 1, we observe that\nthe averaged performance signiﬁcantly degrades in\nthe case when only 50 tokens are used during pre-\ntraining, while the performance gradually recover\nwhen the token number mismatch between pre-\ntraining and ﬁne-tuning narrows. Tokens appearing\nin the pre-training data receive disproportionately\nlarger gradients than tokens not in the pre-training\ndata during pre-training, and this artifact cripples\n1The number of different tokens in pre-training data.\nthe downstream performance.\nThe above observation make it hard to tell\nwhether model pre-trained with amino acid se-\nquence failed to perform well on downstream tasks\ndue to the token number mismatch. Thus, we con-\nduct further experiments to remove the undesirable\nartifact arise from the mismatch. Say we only use\nthe ﬁrst 50 tokens (excluding special tokens) during\npre-training while the rest 29950 token embeddings\nare not used, then before ﬁne-tuning the model on\ndownstream tasks, we substitute those unused to-\nken embeddings with those 50 used token embed-\ndings. We call the above setting 50-substitute. In\nthis case, different tokens will share the same token\nembeddings when the model starts to be ﬁne-tuned.\nFrom the last row in Table 1, we ﬁnd that the\nmodel recovers its ability to be ﬁne-tuned when\npre-trained on artiﬁcial dataset. However, when\nperforming the same substitution on the model\npre-trained with amino acid, the model still fail\nto be ﬁne-tuned. Together with Section 4.1, we can\nconclude that the main reason a pre-trained model\nfailed to transfer to human language downstream\ntasks lies in the intrinsic property of the pre-training\ndata.\n4.4 Further Fine-tuning with English MLM\nbefore Fine-tuning on GLUE\nIt is innate to ﬁne-tune the word embeddings of\npre-trained models on English before ﬁne-tuning\non GLUE. This is for aligning the word embed-\ndings of L1 acquired during pre-training with the\nword embeddings of English. We conduct experi-\nments similar to Table 1, and the only difference\nlies in that we ﬁne-tune the word embeddings and\nlanguage model head of the pre-trained model with\nMLM on English before ﬁne-tuning on GLUE. We\nﬁnd the performance slightly advance mostly, with\nimprovement in Java script being the most salient.\nWe leave detailed results in Appendix B.\n5 Conclusion\nWe study how pre-trained data might and might\nnot affect the downstream performance of a\ntransformer-based pre-trained LM. We ﬁnd that\nﬁne-tuning with models pre-trained on data with-\nout any structures can surpass performance ob-\ntained directly trained from scratch on downstream\ntasks. Our results also show that pre-training with\nstructured non-human language corpora does not\nalways equip the model to perform competently\non downstream tasks in general. We also dis-\ncover that pre-training on a certain artiﬁcial dataset\ngives downstream performance comparable to pre-\ntraining on another natural language. We reveal\nthat token distribution in the pre-training corpora\nmerely affects pre-trained model performance on\ndownstream tasks. Last, our experiments show\nthat the number of token embeddings used dur-\ning pre-training greatly contribute the downstream\nperformance, while this can be mitigate by some\nmanipulations on the token embeddings in certain\ncases. We hope our analysis provides insights into\nwhat kind of pre-training data makes a pre-trained\nmodel a pre-trained model.\nBroader Impact\nWe ﬁnd an surprising simple artiﬁcial dataset\nto pre-train an language model, and we believe\nthat our work have the potential to be applied to\nlow-resource language when pre-training data are\nscarce. We think our work do not cause any ethical\nissues.\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. Don’t stop pretraining: Adapt\nlanguage models to domains and tasks. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 8342–8360.\nJohn Hewitt and Christopher D Manning. 2019. A\nstructural probe for ﬁnding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4129–4138.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof bert. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4356–4365.\nYongjie Lin, Yi Chern Tan, and Robert Frank. 2019.\nOpen sesame: Getting inside bert’s linguistic knowl-\nedge. In Proceedings of the 2019 ACL Workshop\nBlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP, pages 241–253.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nVincent Micheli, Martin d’Hoffschmidt, and François\nFleuret. 2020. On the importance of pre-training\ndata volume for compact language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7853–7858.\nSeonwoo Min, Seunghyun Park, Siwon Kim, Hyun-\nSoo Choi, and Sungroh Yoon. 2019. Pre-training\nof deep bidirectional protein sequence representa-\ntions with structural information. arXiv preprint\narXiv:1912.05625.\nDana Movshovitz-Attias and William Cohen. 2013.\nNatural language models for predicting program-\nming comments. In Proceedings of the 51st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 35–40.\nIsabel Papadimitriou and Dan Jurafsky. 2020. Learn-\ning music helps you read: Using transfer to study\nlinguistic structure in language models. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n6829–6839.\nPedro Javier Ortiz Suárez, Laurent Romary, and Benoît\nSagot. 2020. A monolingual approach to contextual-\nized word embeddings for mid-resource languages.\nIn ACL 2020-58th Annual Meeting of the Associa-\ntion for Computational Linguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBert rediscovers the classical nlp pipeline. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R Bowman, Dipan-\njan Das, et al. 2018. What do you learn from con-\ntext? probing for sentence structure in contextual-\nized word representations. In International Confer-\nence on Learning Representations.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In the Pro-\nceedings of ICLR.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2019.\nHuggingface’s transformers: State-of-the-art natural\nlanguage processing. ArXiv, abs/1910.03771.\nYian Zhang, Alex Warstadt, Haau-Sing Li, and\nSamuel R Bowman. 2020. When do you need bil-\nlions of words of pretraining data? arXiv preprint\narXiv:2011.04946.\nA Experiment Details\nWe give detailed model architectures of our\nRoBERTa-base model and hyperparameters used\nin pre-training.\nA.1 Model\nWe use RoBERTa-base, a 12-layered transformer\nmodel with hidden dimension 768 and 12 attention\nheads per layer. The total number of parameters of\nthe model is around 110M. We pre-train RoBERTa\nusing Huggingface (Wolf et al., 2019) code base.\nA.2 Hyperparameters\nThe hyperparameters used in all pre-training exper-\niments are listed in Table 2\nBatch size 150\nLearning rate 5E-5\nTotal steps 200K\nWarmup steps 10k\nMax Position 128\nTable 2: Pre-training hyperparemeters for BERT.\nA.3 Pre-training Data\nWe put all details related to all pre-training data\nin Table 3. We provide download link to the pre-\ntraining dataset, along with the training and valida-\ntion loss at the end of pre-training. The artiﬁcial\ndata and baseline dataset can be generated follow-\ning the script in our code. The train/evaluation split\ncan be found in the supplementary materials. We\nalso include the vocabulary size (including special\ntokens) of each model on the last column. The\nvocabulary ﬁle is obtained by training a WordPiece\ntokenizer on the training data for Java, Kannada,\nand Wikipedia dataset.\nA.4 Fine-tuning Details\nWe ﬁne-tune GLUE using Huggingface (Wolf et al.,\n2019) code base. The model ﬁne-tuned in this\nsection is RoBERTa base with classiﬁer on top of\nthe last transformer layer. The whole model ﬁne-\ntuned is has 110M parameters.\nA.4.1 Dataset\nWe provide statistics on the 8 GLUE tasks we used\nin Table 4\nA.4.2 Fine-tuning Hyperparameters\nWe list the hyperparameters used in ﬁne-tuning\nGLUE in Tabel 5.\nA.5 Resource\nOut computation resource is V100 GPU. Pre-\ntraining a RoBERTa following our parameters\ngiven in 2 takes 60 hours on a single V100, and\nﬁne-tuning the pre-trained models on the 8 GLUE\ntasks following hyperparameters in 5 takes about\n12 hours on a V100.\nB Fine-tune the Model on English MLM\nBefore Fine-tuning on GLUE\nThis is the detailed experiment data for Section 4.4\nDataset Link Training Loss Eval. Loss Vocab Size\nWikipedia Wikidump 2.204 3.354 30000\nJava Java data 0.03227 1.025 30000\nAmino Acid PLUS 2.041 2.201 28895\nKannada OSCAR 2.366 3.128 30000\nRandom baseline NA 9.428 9.467 30000\nZipf Baseline NA 6.351 6.446 30000\nArtiﬁcial (Uniform) NA 1.996 2.409 29991\nArtiﬁcial (Zipf) NA 1.599 1.774 29991\nArtiﬁcial (50) NA 1.558 1.754 29991\nArtiﬁcial (500) NA 1.563 1.762 29991\nArtiﬁcial (5000) NA 1.548 1.701 29991\nTable 3: Details for dataset used in pre-training.\nStage 1\nL1 MLM pre-train\nStage 2\nEn MLM fine-tune\nStage 3\nGLUE fine-tune\nStage 4\nGLUE testing\nTransformer\nWord Embedding\nTransformer\n(fixed)\nWord Embedding\nTransformer\nWord Embedding\nTransformer\n(fixed)\nWord Embedding \n(fixed)\nLM Head LM Head Classifier Head Classifier Head (fixed)\nL A A B Y U Q English Wikipedia\nFigure 3: Work ﬂow of our experiments for Section 4.4: We ﬁrst pre-train the whole masked language model on L1\n(protein sequence in this ﬁgure), and then only ﬁne-tune the word embedding and language model head on English\nWikipedia. The third stage is ﬁne-tuning the whole model on English downstream tasks, and the last stage is to\ntest the performance on the ﬁne-tuned downstream task.\nTask Examples\nMRPC 3.6K / 0.4K / 1.7K\nRTE 2.4K / 0.2K / 3K\nSTS-B 5.7K / 1.5K / 1.3K\nQNLI 104K / 5.4K / 5.4K\nQQP 363K / 40.4K / 391.0K\nCoLA 8.5K / 1.0K / 1.1K\nMNLI 392.7K / 9.8K + 9.8K / 9.8K + 9.8K\nSST-2 67.4K / 0.9K / 1.8K\nTable 4: Statistics of (train / dev/ test) in GLUE tasks\nMNLI contains matched and mismatched in dev and\ntest set. We didn’t evaluate our models’ performance\non test set.\nLR BSZ RoBERTa DR Classiﬁer DR TS WS MSL\nCoLA 1.00E-05 16 0 0.1 5336 320 128\nSTS-B 2.00E-05 16 0 0.1 3598 214 128\nSST-2 1.00E-05 32 0 0.1 20935 1256 128\nMNLI 3.00E-05 128 0 0.1 10000 1000 128\nQNLI 1.00E-05 32 0 0.1 33112 1986 128\nQQP 5.00E-05 128 0 0.1 14000 1000 128\nRTE 3.00E-05 32 0 0.1 800 200 128\nMRPC 2.00E-05 32 0 0.1 800 200 128\nSQuAD2.0 3.00E-05 48 0 0.1 8144 814 128\nTable 5: Hyperparameters for ALBERT in downstream tasks. LR: Learning Rate. BSZ: Batch Size. DR: Dropout\nRate. TS: Training Steps. WS: Warmup Steps. MSL: Maximum Sequence Length\nL1 STS-B QNLI QQP CoLA SST-2 MNLI MRPC RTE Avg\nNo Pre-train 0.17 0.60 0.75 0.13 0.83 0.65 0.67 0.50 0.54\nPre-train on English 0.76 0.83 0.86 0.34 0.88 0.76 0.77 0.53 0.72\nRandom Baseline 0.28 0.67 0.80 0.12 0.83 0.66 0.71 0.56 0.57\nZipf Baseline 0.34 0.71 0.81 0.17 0.84 0.67 0.81 0.53 0.61\nAmino Acid 0.24 0.65 0.79 0.07 0.82 0.65 0.75 0.50 0.56\nJava Script 0.25 0.78 0.82 0.12 0.82 0.71 0.78 0.51 0.60\nKannada 0.79 0.78 0.84 0.15 0.85 0.71 0.81 0.57 0.69\nArtiﬁcial (Uniform) 0.73 0.79 0.82 0.17 0.82 0.71 0.75 0.55 0.67\nArtiﬁcial (Zipf) 0.79 0.79 0.83 0.11 0.82 0.72 0.75 0.57 0.67\nTable 6: Downstream results of different pre-trained models, and the model trained from scratch on downstream\ntasks (no pre-train in the ﬁrst row). The evaluation metric of MRPC and QQP are F1 score, spearman correlation\ncoefﬁcient is reported for STS-B, and the rest tasks are evaluated with accuracy. Result of MNLI is averaged\nbetween matched and mismatched. Please refer to Section 4.2 and Section 4.3 for the meaning of parentheses in\nthe last two blocks."
}