{
    "title": "Practical Transformer-based Multilingual Text Classification",
    "url": "https://openalex.org/W3166583983",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2100207317",
            "name": "Cindy Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2137637256",
            "name": "Michele Banko",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3082274269",
        "https://openalex.org/W3114968871",
        "https://openalex.org/W3120253119",
        "https://openalex.org/W3023443524",
        "https://openalex.org/W2990704537",
        "https://openalex.org/W2986154550",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W3013840636",
        "https://openalex.org/W2970193165",
        "https://openalex.org/W2995647371",
        "https://openalex.org/W3122890974",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3173954987",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2970752815",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W2963721344",
        "https://openalex.org/W2891555348",
        "https://openalex.org/W2952629768",
        "https://openalex.org/W3100880133",
        "https://openalex.org/W3115903740",
        "https://openalex.org/W3099178230",
        "https://openalex.org/W2973088264",
        "https://openalex.org/W2948017315",
        "https://openalex.org/W2606347107",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2171068337",
        "https://openalex.org/W3008110149",
        "https://openalex.org/W4297805866",
        "https://openalex.org/W3017367042",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2998764526",
        "https://openalex.org/W2740168486",
        "https://openalex.org/W4385681388",
        "https://openalex.org/W3011385529",
        "https://openalex.org/W3088592174",
        "https://openalex.org/W4287993739",
        "https://openalex.org/W2943552823",
        "https://openalex.org/W2954226438",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W4385245566"
    ],
    "abstract": "Transformer-based methods are appealing for multilingual text classification, but common research benchmarks like XNLI (Conneau et al., 2018) do not reflect the data availability and task variety of industry applications. We present an empirical comparison of transformer-based text classification models in a variety of practical monolingual and multilingual pretraining and fine-tuning settings. We evaluate these methods on two distinct tasks in five different languages. Departing from prior work, our results show that multilingual language models can outperform monolingual ones in some downstream tasks and target languages. We additionally show that practical modifications such as task- and domain-adaptive pretraining and data augmentation can improve classification performance without the need for additional labeled data.",
    "full_text": "Proceedings of NAACL HLT 2021: IndustryTrack Papers, pages 121–129\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n121\nPractical Transformer-based Multilingual Text Classiﬁcation\nCindy Wang\nSentropy Technologies\ncindy@sentropy.io\nMichele Banko\nSentropy Technologies\nmbanko@sentropy.io\nAbstract\nTransformer-based methods are appealing for\nmultilingual text classiﬁcation, but common\nresearch benchmarks like XNLI (Conneau\net al., 2018) do not reﬂect the data avail-\nability and task variety of industry applica-\ntions. We present an empirical comparison\nof transformer-based text classiﬁcation mod-\nels in a variety of practical monolingual and\nmultilingual pretraining and ﬁne-tuning set-\ntings. We evaluate these methods on two dis-\ntinct tasks in ﬁve different languages. De-\nparting from prior work, our results show that\nmultilingual language models can outperform\nmonolingual ones in some downstream tasks\nand target languages. We additionally show\nthat practical modiﬁcations such as task- and\ndomain-adaptive pretraining and data augmen-\ntation can improve classiﬁcation performance\nwithout the need for additional labeled data.\n1 Introduction\nWhile the development of natural language un-\nderstanding (NLU) applications often begins with\nhigh-resource languages such as English, there is a\nneed to create products that are accessible to speak-\ners of the world’s nearly 7,000 languages. Only\n5% of the world’s population is estimated to speak\nEnglish as a ﬁrst language.1\nThe growth of NLU-centric products within di-\nverse language markets is evidenced by the increase\nin language support for popular consumer applica-\ntions such as virtual assistants, Web search, and so-\ncial media platforms. As of mid-2020, Google As-\nsistant supported 44 languages on smartphones, fol-\nlowed by Siri (21 languages) and Amazon Alexa (8\nlanguages). At the start of 2021, Google Search and\nMicrosoft Bing supported 149 and 40 languages\nrespectively. Also at this time, Twitter ofﬁcially\nsupported a total of 45 languages with Facebook\nreaching over 100 languages.\n1CIA World Factbook\nAdvances in multilingual language models such\nas multilingual BERT (mBERT; Devlin et al., 2019)\nand XLM-RoBERTa (XLM-R; Conneau et al.,\n2020) which are trained on massive corpora in\nover 100 languages, show promise for fast iteration\nand deployment of NLU applications. In theory,\ncross-lingual approaches reduce the need for la-\nbeled training data in target languages by enabling\nzero- or few-shot learning. Additionally, they en-\nable simpliﬁed model deployment compared to the\nuse of many monolingual models. On the other\nhand, evaluations show that scaling to more lan-\nguages causes dilution (Conneau et al., 2020) and\nconsequently cite the relative under-performance\nof multilingual models on monolingual tasks (Vir-\ntanen et al., 2019; Antoun et al., 2020).\nRecent studies (Hu et al., 2020; Rust et al., 2020)\nhave explored tradeoffs of multi versus monolin-\ngual model paradigms. However, we observe that\nexisting multilingual text classiﬁcation benchmarks\nare designed to measure zero-shot cross-lingual\ntransfer rather than supervised learning (Conneau\net al., 2018; Yang et al., 2019), though the latter is\nmore applicable to industry settings. Thus, the goal\nof this paper is to evaluate multilingual text classiﬁ-\ncation approaches with a focus on real applications.\nOur contributions include:\n• A comparison of state-of-the-art language\nmodels spanning monolingual and multilin-\ngual setups, evaluated across ﬁve languages\nand two distinct tasks;\n• A set of practical recommendations for ﬁne-\ntuning readily available language models for\ntext classiﬁcation; and\n• Analyses of industry-centric challenges such\nas domain mismatch, labeled data availability,\nand runtime inference scalability.\n2 Multilingual Text Classiﬁcation\nWe consider a series of practical components for\nbuilding multilingual text classiﬁcation systems.\n122\nLang. Model Pretraining Corpus Tokenizer Param.\nEN RoBERTa (Liu et al., 2019) Various (160GB) BPE 125M\nDE German BERT (deepset.ai, 2019) German Wikipedia, OpenLegalData, and\nnews articles (12 GB)\nSentencePiece 110M\nES BETO (Cañete et al., 2020) Various (18.4GB) WordPiece 110M\nFR CamemBERT (Martin et al., 2020) OSCAR (138GB) SentencePiece 110M\nJA Japanese BERT (Suzuki and Taka-\nhashi, 2019)\nJapanese Wikipedia (2.6GB) MeCab+Wordpiece 110M\nMULTI XLM-RoBERTa (Conneau et al.,\n2019)\nCC-100 (2.5 TB) EN (301GB), DE (67GB),\nES (53GB), FR (57GB), JA (69GB)\nSentencePiece 270M\nTable 1: Pretraining corpora, tokenizers, and size (# parameters) of the language models used in our experiments.\n2.1 Pretrained Transformer Language\nModels\nTransfer learning using pretrained language models\n(LMs) which are then ﬁne-tuned for downstream\ntasks has emerged as a powerful technique for NLU\napplications. In particular, models using the now-\nubiquitous transformer architecture (Vaswani et al.,\n2017), such as BERT (Devlin et al., 2019) and its\nvariants, have obtained state of the art results in\nmany monolingual and cross-lingual NLU bench-\nmarks (Wang et al., 2019a; Raffel et al., 2020; He\net al., 2021).\nOne drawback of data-hungry transformer mod-\nels is that they are time- and resource-intensive\nto train. In our experiments, we consider LMs\npretrained on both monolingual and multilingual\ncorpora, and analyze the effects of combining these\nmodels with other NLU system components.\nFor monolingual LMs, we use BERT models\npretrained on corpora in each target language. The\none exception is English, where we use RoBERTa,\na BERT reimplementation that exceeds its perfor-\nmance on an assortment of tasks (Liu et al., 2019).\nFor multilingual LMs, we use XLM-R, which\nsigniﬁcantly outperforms mBERT on cross-lingual\nbenchmarks and is competitive with monolingual\nmodels on monolingual benchmarks such as GLUE\n(Wang et al., 2019b). All of the pretrained models\nused are accessible from the Hugging Face (Wolf\net al., 2020) model hub, and their details are sum-\nmarized in Table 1.\n2.2 Domain-Adaptive and Task-Adaptive\nPretraining\nThough pretrained language models have hundreds\nof millions of parameters and are trained on di-\nverse corpora, they are not guaranteed to gener-\nalize to all tasks and domains. For downstream\ntasks, a second phase of pretraining on a smaller\ndomain- or task-speciﬁc corpus has been shown to\nprovide performance improvements. Gururangan\net al. (2020) compare domain-adaptive pretraining\n(DAPT), which uses a large corpus of unlabeled\ndomain-speciﬁc text, and task-adaptive pretrain-\ning (TAPT), which uses only the training data of a\nparticular task. The primary difference is that the\ntask-speciﬁc corpus tends to be much smaller, but\nalso more task-relevant. Therefore, while DAPT\nis helpful in both low- and high-resource settings,\nTAPT is much more resource-efﬁcient and outper-\nforms DAPT when sufﬁcient data is available.\nIn our experiments, we evaluate both approaches,\nusing the classiﬁcation task training data as the\nTAPT corpus and in-domain unlabeled data as the\nDAPT corpus (see Section 3 for details). BERT and\nRoBERTa are pretrained with amasked language\nmodeling (MLM) objective, a cross-entropy loss on\nrandomly masked tokens in the input sequence. We\nsimilarly use the MLM objective when performing\nDAPT and TAPT.\n2.3 Supervised Fine-Tuning\nWe consider three settings for supervised ﬁne-\ntuning of language models for downstream classiﬁ-\ncation tasks (N is the number of target languages).\n• mono-target (N ﬁnal models): Fine-tune a\nmonolingual LM on the training data in each\ntarget language\n• multi-target (N ﬁnal models): Fine-tune\nXLM-R on the training data in each target\nlanguage\n• multi-all (one ﬁnal model): Fine-tune XLM-R\non the concatenation of all training data\nTo represent sequences for classiﬁcation, we use\nthe ﬁnal LM hidden vectorsB ∈Rl×H correspond-\ning to each of the l input tokens.2 We then compute\naverage and max pools over the sequence length\n2Though only the hidden vector for the ﬁrst ([CLS]) to-\nken is typically used (Devlin et al., 2019), we ﬁnd that the\npooled sequence summary attains better results on our tasks.\n123\nDataset Task Lang. Unlab. Train Test\nCLS Sentiment EN 105k 6k 6k\n(AMAZON ) DE 317k 6k 6k\nFR 58k 6k 6k\nJA 294k 6k 6k\nHATEVAL Hate speech EN - 10k 3k\n(TWITTER ) ES - 5k 1.6k\nTable 2: The target tasks, languages, and number of\ntraining and test examples in each dataset.\nlayer and concatenate them to create the aggregate\nrepresentation C ∈R2H. Finally, the summary\nvector C is passed to a classiﬁcation layer where\nwe compute a standard cross-entropy loss.\n2.4 Data Augmentation\nIn real applications, labeled data is often available\nin high resource languages such as English but\nsparse or nonexistent in others. We experiment\nwith machine translation3 as a form of cross-lingual\ndata augmentation, which has been shown to im-\nprove performance on multilingual benchmarks\n(Singh et al., 2019). In single target language set-\ntings, we translate training data from other lan-\nguages into the target language, yielding N times\nthe number of training examples. In the multi-all\nsetting, we translate data from every language into\nevery other language, yielding N(N −1) times\nthe number of training examples. At training time,\nwe directly include the translated examples in the\ntraining corpus. Following the pretraining conven-\ntion of XLM-R, we do not use special markers to\ndenote the input language.\n3 Data\nWe choose sentiment analysis and hate speech de-\ntection as evaluation tasks due to their relevance to\nindustry applications and the availability of mul-\ntilingual datasets. An overview of the datasets is\nshown in Table 2.\n3.1 Sentiment Analysis\nThe Cross-Lingual Sentiment dataset (CLS ; Pret-\ntenhofer and Stein, 2010) 4 consists of AMAZON\nproduct reviews in four languages and three prod-\nuct categories (BOOKS , DVD, and MUSIC ). Each\nreview includes title and body text, which we con-\ncatenate to create the input example. The dataset\n3https://cloud.google.com/translate\n4We use the processed version of this dataset provided by\nEisenschlos et al. (2019).\nHashtag Train Test Test †\n#NoDACA 99.36 34.26 99.60\n#EndDACA 98.31 33.87 98.39\n#BuildThatWall 100.0 24.89 95.99\n#BuildTheDamnWall 100.0 62.07 100.0\n#NoAmnesty 100.0 48.25 100.0\n#SendThemBack 82.02 68.29 87.80\n#DeportThemAll 100.0 83.15 99.46\nTable 3: Percentage of hateful class by anti-immigrant\nhashtags in H ATEVAL (non-exhaustive list). †Denotes\nthe relabeled test set.\ncontains training and test sets with balanced binary\nsentiment labels, as well as 50-320k unlabeled ex-\namples per language. We sample 10k unlabeled\nexamples from each language for DAPT.\n3.2 Hate Speech Detection\nThe HATEVAL dataset (Basile et al., 2019) con-\ntains tweets in English and Spanish annotated for\nthe presence of hate speech targeting women and\nimmigrants. Examples were collected by querying\nTwitter for users with histories of sending or receiv-\ning hateful messages, as well as keywords related\nto women and immigrants.\nRelabeling English Test Data During experi-\nmentation, we found that English example labels\nwere inconsistent across the training and test sets.\nFor instance, many test examples containing anti-\nimmigration hashtags were mislabeled as non-\nhateful while similar examples were labeled as\nhateful in the training set (see Table 3). We man-\nually relabeled 641 examples in the test set and\nrelease the relabeled data for future research.5,6\nUnlabeled Twitter Data Since no unlabeled cor-\npus is provided, we collected a sample of 10k ran-\ndom tweets per language from November 2020,\nwhich we use for DAPT.\n4 Experimental Setup\nPreprocessing and Tokenization We apply min-\nimal preprocessing to both datasets, replacing\nURLs and Twitter usernames with <url> and\n<user> tokens. At all stages of training, we use the\ndefault tokenizers associated with each pretrained\n5Prior work (Stappen et al., 2020) has also noted this\ndiscrepancy and proposed repartitioning the train and test sets.\nWe instead relabeled the test set due to the large number of\nmislabeled examples.\n6https://github.com/sentropytechnologies/\nhateval2019-relabeled\n124\nModel DE FR JA\nmBERT 84.3 86.6 81.2\nMultiFiT 92.2 91.4 86.2\nModel EN ES\nMajority label 36.7 37.0\nSVM + tf-idf 45.1 70.1\n1st place submissions 65.1 73.0\nTable 4: Prior results (macro-F1) for CLS (Eisenschlos\net al., 2019, top) and H ATEVAL (Basile et al., 2019,\nbottom).\nLM (see Table 1) and truncate sequences with more\nthan 512 tokens.\nTraining We use 80% of each training set for\ntraining and the rest for validation. During DAPT\nand TAPT, we train using the MLM objective for\n10 epochs. During supervised ﬁne-tuning, we train\nfor 5 epochs. We use the default hyperparameters\nfor all pretrained LMs and apply dropout of 0.4 to\nthe ﬁnal classiﬁcation layer.\nEvaluation We report the test set macro-\naveraged F1 score for both datasets. (For CLS ,\nthis is equivalent to accuracy since the classes are\nbalanced.) For reference, prior results on CLS and\nHATEVAL are shown in Table 4.\n5 Results and Analysis\nWe report results for all experiments in Table 5. For\nboth datasets, (1) TAPT and DAPT and (2) data\naugmentation with machine translations improve\nmodel performance. These strategies, which re-\nquire no additional labeled data, improve macro-F1\nscore by between 0.6-1.5% for CLS and between\n0.3-4.3% for HATEVAL. Even without DAPT,\nwhich is often the most expensive step, applying\nTAPT and/or data augmentation alone improves\nperformance in all settings and languages except\nHATEVAL EN .\nCLS For languages where extremely high-\nresource monolingual LMs are available (EN and\nFR), models perform best in the mono-target set-\nting, in which a monolingual LM is ﬁne-tuned\non target language data. This is consistent with\nprior ﬁndings that XLM-R suffers from ﬁxed model\ncapacity and vocabulary dilution (Conneau et al.,\n2019). However, for DE and JA, which are not low-\nresource languages but whose monolingual LM\npretraining corpora are relatively limited in size\nand domain (see Table 1), XLM-R models perform\nbetter.\nHATEVAL On average, XLM-R models perform\nbetter on HATEVAL than those ﬁne-tuned from\nmonolingual LMs. Unlike for CLS , this is true\neven in EN, suggesting that for some classiﬁcation\ntasks, the LM pretraining corpus is not as impor-\ntant for downstream task performance as XLM-R’s\nlarger model capacity and cross-lingual transfer.\nThough scores were much higher for the relabeled\nEN dataset than the original, the effects of LM ﬁne-\ntuning, TAPT, DAPT, and data augmentation were\nconsistent.\n5.1 Not All Classiﬁcation Tasks Are Created\nEqual\nThe two text classiﬁcation tasks we evaluate are sig-\nniﬁcantly different from both an annotation and a\nmodeling perspective. Sentiment is a well-deﬁned\nfacet of language, and language model represen-\ntations have even been shown to encode semantic\ninformation about it (Radford et al., 2017). Mean-\nwhile, deﬁning and identifying hate speech is much\nmore nuanced, even for humans. Hate speech de-\ntection is confounded by many factors that require\nnot only immediate context of the input but also\ncultural and social contexts (Schmidt and Wiegand,\n2017). The difference in the types of information\nthat models need to encode for each task may ex-\nplain why monolingual LMs, which tend to encode\nbetter lexical information than multilingual LMs\n(Vuli´c et al., 2020), can outperform XLM-based\nmodels when ﬁne-tuned for sentiment analysis but\nnot for hate speech detection.\n5.2 Cross-lingual Transfer\nPrior work has established that multilingual LMs\nbeneﬁt from the addition of more languages dur-\ning pretraining up to a point, after which limited\nmodel capacity and vocabulary dilution cause per-\nformance to degrade on downstream tasks – this is\nreferred to as the curse of multilinguality(Conneau\net al., 2019). Though this is reﬂected in the results\nof CLS EN and FR, other models ﬁne-tuned from\nXLM-R exhibit gains from cross-lingual transfer.\nIn particular, for CLS JA and HATEVAL EN , the\nbest-performing models beneﬁt not only from mul-\ntilingual pretraining corpora but also from multilin-\ngual task training data.\nThese results suggest that when ﬁne-tuning LMs\nfor downstream tasks, XLM-R is a robust baseline.\n125\nCLS H ATEVAL\nModel Adapt. Aug. EN DE FR JA AVG EN EN † ES AVG AVG †\nmono-target\nRoBERTa (EN)\nBERT (OTHERS )\n× × 94.70.4 90.90.6 95.20.0 88.70.3 92.4 44.45.3 58.56.2 75.60.6 60.0 67.1\n✓ 95.30.3 92.00.2 95.60.3 89.30.02 93.0 46.12.6 60.63.2 76.01.7 61.0 68.3\nTAPT × 94.90.1 91.60.1 95.40.1 89.30.3 92.8 45.41.9 59.92.7 76.11.1 60.8 68.0\n✓ 95.00.4 92.30.4 95.80.2 89.70.4 93.2 44.71.5 59.21.7 76.91.4 60.8 68.0\nTAPT+\nDAPT\n× 94.90.4 91.80.2 95.50.3 89.50.2 92.9 48.01.5 63.12.6 76.31.1 62.2 69.7\n✓ 95.30.1 93.00.8 95.90.1 89.90.4 93.5 46.04.3 60.24.4 76.90.6 61.4 68.5\nmulti-target\nXLM-RoBERTa\n× × 92.50.4 93.00.2 92.50.3 90.40.5 92.1 47.22.0 61.41.9 74.80.5 61.0 68.1\n✓ 93.30.1 94.00.2 93.80.2 90.30.3 92.8 45.61.6 59.32.5 77.01.1 61.3 68.1\nTAPT × 92.70.5 93.50.5 93.90.3 90.30.1 92.6 47.02.7 62.43.3 76.11.4 61.6 69.2\n✓ 93.40.6 94.00.3 93.80.5 90.50.4 92.9 47.91.3 63.51.5 77.90.9 62.9 70.7\nTAPT+\nDAPT\n× 93.10.6 93.00.5 93.60.1 90.80.3 92.6 49.92.5 65.62.4 76.51.0 63.2 71.0\n✓ 94.00.3 94.10.4 93.80.3 91.10.4 93.2 46.62.1 61.72.5 78.10.8 62.3 69.9\nmulti-all\nXLM-RoBERTa\n× × 92.40.3 92.60.4 93.30.4 90.40.4 92.2 48.43.5 63.14.5 77.50.4 62.9 70.3\n✓ 93.40.3 93.30.2 94.00.2 90.40.5 92.8 49.83.5 66.04.6 77.80.9 63.8 71.9\nTAPT × 92.50.4 93.00.3 93.90.3 90.90.3 92.6 48.42.7 64.23.5 77.40.9 62.9 70.8\n✓ 93.50.4 93.40.5 94.10.2 91.10.2 93.0 50.02.2 66.52.6 77.80.6 63.9 72.2\nTAPT+\nDAPT\n× 92.70.3 93.30.2 94.00.3 91.20.3 92.8 47.13.9 62.75.3 77.41.0 62.3 70.1\n✓ 93.50.3 93.80.2 94.30.3 91.40.2 93.3 50.71.1 67.41.4 77.70.7 64.2 72.6\nTable 5: CLS and H ATEVAL results (macro-F1) averaged over ﬁve random seeds. The best results for each target\nlanguage test set are bolded, and standard deviations are shown in subscripts. Model denotes the supervised ﬁne-\ntuning setting. Adapt. denotes the adaptive pretraining setting:×(no adaptive pretraining), TAPT (task-adaptation\nonly), or TAPT+DAPT (task- and domain-adaptation).Aug. denotes whether the training data was augmented with\nmachine-translated examples. For HATEVAL, we report results for both the original and relabeled† test sets.\nModel Data DE FR JA ES\nmulti-target target 94.1 93.8 91.1 78.1\nmulti-all all 93.8 94.3 91.4 77.7\nzero-shot EN 92.7 92.6 88.5 72.1\nTable 6: Zero-shot learning versus best multilingual ap-\nproaches. Data denotes language of training data. We\nﬁne-tune XLM-R and use DAPT, TAPT, and data aug-\nmentation for all models shown.\nIn cases where knowledge transfer from a monolin-\ngual LM might be difﬁcult (e.g. due to a limited\npretraining corpus or specialized downstream task),\nXLM-R may even outperform its monolingual com-\npetitors.\n5.3 Are Target Language Labels Needed?\nZero-shot learning is a topic of signiﬁcant inter-\nest in multilingual NLU research (Conneau et al.,\n2018, 2019; Artetxe and Schwenk, 2019). In this\ncontext, we use zero-shot learningto refer to learn-\ning a classiﬁcation task without observing training\nexamples in the target language. Such an approach\nwould allow practitioners to train a classiﬁcation\nmodel using labeled data in a high-resource lan-\nguage such as EN and deploy it in other languages\nfor which labels are not available.\nTo evaluate the viability of zero-shot approaches\nfor our tasks, we compare the best performing mod-\nels from the experiments in Table 5 with models\ntrained only on EN training data. We report the\ntest set results for each of the non- EN target lan-\nguages in Table 6. Zero-shot models are compet-\nitive with previously published baselines (Table\n4), which demonstrates the effectiveness of cross-\nlingual transfer in models like XLM-R. However,\nmodels trained using target language labels still out-\nperform them by a large margin. Since obtaining a\nsmall number of target language labels is straight-\nforward and typically required for validation in\nreal applications, the need for zero-shot learning is\nreduced in practical scenarios.\n5.4 Speed and Memory Usage\nThe deployment of multilingual NLU systems\nvaries signiﬁcantly depending on the number of\ndownstream task models trained and the model ar-\nchitectures used. For instance, the mono-target and\nmulti-target settings induce one model per target\n126\nFigure 1: Inference time (top) and memory usage\n(bottom) benchmarks. XLM-R results not shown at\nbatch sizes 32 and 64 due to GPU memory restraints.\nEnvironment details: transformers v3.1.0,\nPyTorch v1.4.0, python v3.7.4, Linux.\nCPU: x86_64 (fp16=False, RAM=15GB).\nGPU: Tesla P100-PCIE-16GB, RAM=16GB,\npower=250.0W, perf. state=0) .\nlanguage. Conversely, multi-all models have more\nconsistent end-task performance and do not require\nthe added complexity and latency of language de-\ntection.\nWe use the Hugging Face library to benchmark\nthe pretrained transformer models used in our ex-\nperiments. We measure the inference time and\nmemory usage of a single forward pass on a sin-\ngle Nvidia Tesla P100 GPU. Results are shown in\nFigure 1.\nMonolingual BERT models in different lan-\nguages are nearly identical in inference speed, but\nvary slightly at small batch sizes. RoBERTa has\nmore parameters than BERT, but the impact on\ninference time and memory is small. XLM-R is\nalso comparable with monolingual models at small\nbatch sizes, but its memory usage becomes pro-\nhibitively large at batch sizes larger than 32. For\ncertain applications such as those with real-time\ninference, this may not be important since the most\ncommon batch size is 1. Overall, the main tradeoff\nwe observe is between the complexity of deploying\nN language-speciﬁc models and the high parame-\nter count of a single multilingual model.\n6 Related Work\n6.1 Multilingual Classiﬁcation Benchmarks\nXNLI (Conneau et al., 2018) and PAWS-X (Yang\net al., 2019) are commonly used as representative\nbenchmarks for cross-lingual text classiﬁcation (Hu\net al., 2020; Conneau et al., 2019). However, both\ndatasets are designed for evaluating zero-shot cross-\nlingual transfer. While useful, they do not reﬂect\npractical scenarios where (1) a small amount of\nlabeled data obviates zero-shot approaches, and\n(2) target language test data are not semantically\naligned.\nMeanwhile, benchmarks for supervised multi-\nlingual text classiﬁcation are limited. Artetxe and\nSchwenk (2019) propose Language-Agnostic SEn-\ntence Representations (LASER) and evaluate them\non Multilingual Document Classiﬁcation Corpus\n(MLD OC; Schwenk and Li, 2018). Eisenschlos\net al. (2019) later show that their multilingual ﬁne-\ntuning and bootstrapping approach, MultiFit, out-\nperforms LASER and mBERT on CLS and ML-\nDOC. The recently released Multilingual Amazon\nReviews Corpus (MARC; Keung et al., 2020) is\nsimilar to CLS , but contains a different set of lan-\nguages and large-scale training sets. Rust et al.\n(2020) perform a systematic evaluation similar\nto ours, comparing monolingual and multilingual\nBERT models on seven monolingual sentiment\nanalysis datasets. Unlike our work, they do not con-\nsider multilingual test sets or cross-lingual transfer\nduring training (as in themulti-all setting). None of\nthe above evaluate practical training modiﬁcations,\nXLM-R, or tasks with class imbalance.\n6.2 Hate Speech Detection\nDue to the increased volume and consequence of\nonline content moderation in recent years, there is a\ngrowing body of work on multilingual hate speech\ndata and methodology. The Multilingual Toxic\nComment Classiﬁcation Kaggle challenge (Jigsaw,\n2019) included a multilingual test set of Wikipedia\ntalk page comments annotated for toxicity. More\nrecently, Glavaš et al. (2020) introducedXHATE-\n999, an evaluation set of 999 semantically aligned\ntest instances annotated for abusive language in\nﬁve typologically diverse languages. Similar to our\nwork, they compare state-of-the-art monolingual\nand multilingual transformer models. However,\nboth the Jigsaw dataset and XHATE-999 are de-\nsigned for evaluating zero-shot transfer and do not\ncontain multilingual training data.\n127\nOther multilingual hate speech studies have\nlargely combined separate existing monolingual\ndatasets for evaluation (Pamungkas and Patti, 2019;\nSohn and Lee, 2019; Aluru et al., 2020; Corazza\net al., 2020; Zampieri et al., 2020). To avoid do-\nmain mismatch effects across languages, we use the\nHATEVAL dataset (Basile et al., 2019), for which\nall examples were collected simultaneously.\nPreviously evaluated approaches include LSTM\narchitectures and feature selection (Pamungkas and\nPatti, 2019; Corazza et al., 2020), as well as us-\ning transformers for ﬁne-tuning (Sohn and Lee,\n2019) or feature extraction (Stappen et al., 2020).\nAluru et al. (2020) show that ﬁne-tuning from\ntransformer-based language models generally out-\nperforms other methods, including cross-lingual\nﬁxed representations like LASER.\n7 Conclusion\nWe conduct an empirical evaluation of transformer-\nbased methods for multilingual text classiﬁcation\nin a variety of pretraining and ﬁne-tuning settings.\nWe evaluate our results on two multilingual datasets\nspanning ﬁve languages: CLS (sentiment analysis)\nand HATEVAL (hate speech detection). Addition-\nally, we contribute a relabeled version of HATE-\nVAL to address mislabeled test examples and enable\nmeaningful comparisons in future work.\nOur results and analysis show that practical meth-\nods such as task- and domain-adaptive pretrain-\ning and data augmentation using machine trans-\nlations consistently improve model performance\nwithout requiring additional labeled data. We fur-\nther show that multilingual model performance can\nvary based on task semantics, and that monolingual\nmodels are not always guaranteed to outperform\nmassively multilingual models like XLM-R due to\nits large pretraining corpora and increased capacity.\nOur work points to a number of future direc-\ntions, including cross-domain and cross-task trans-\nfer, low-resource and few-shot learning, and practi-\ncal alternatives to large multilingual models such\nas distillation.\nAcknowledgements\nWe wish to thank Boya (Emma) Peng, Alexander\nWang, and Thomas Boser for discussions and feed-\nback on this work. Thanks also to the anonymous\nreviewers whose detailed suggestions helped im-\nprove its clarity and usefulness.\nReferences\nSai Saket Aluru, Binny Mathew, Punyajoy Saha, and\nAnimesh Mukherjee. 2020. Deep learning mod-\nels for multilingual hate speech detection. arXiv\npreprint arXiv:2004.06465.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020.\nAraBERT: Transformer-based model for Arabic lan-\nguage understanding. In Proceedings of the 4th\nWorkshop on Open-Source Arabic Corpora and Pro-\ncessing Tools, with a Shared Task on Offensive Lan-\nguage Detection, pages 9–15, Marseille, France. Eu-\nropean Language Resource Association.\nMikel Artetxe and Holger Schwenk. 2019. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transac-\ntions of the Association for Computational Linguis-\ntics, 7:597–610.\nValerio Basile, Cristina Bosco, Elisabetta Fersini,\nDebora Nozza, Viviana Patti, Francisco Manuel\nRangel Pardo, Paolo Rosso, and Manuela San-\nguinetti. 2019. SemEval-2019 task 5: Multilin-\ngual detection of hate speech against immigrants and\nwomen in Twitter. In Proceedings of the 13th Inter-\nnational Workshop on Semantic Evaluation, pages\n54–63, Minneapolis, Minnesota, USA. Association\nfor Computational Linguistics.\nJosé Cañete, Gabriel Chaperon, Rodrigo Fuentes, Jou-\nHui Ho, Hojin Kang, and Jorge Pérez. 2020. Span-\nish pre-trained bert model and evaluation data. In\nPML4DC at ICLR 2020.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing. Association for Computa-\ntional Linguistics.\nMichele Corazza, Stefano Menini, Elena Cabrio, Sara\nTonelli, and Serena Villata. 2020. A multilingual\nevaluation for online hate speech detection. ACM\nTrans. Internet Technol., 20(2).\n128\ndeepset.ai. 2019. Open sourcing german bert. https:\n//deepset.ai/german-bert.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJulian Eisenschlos, Sebastian Ruder, Piotr Czapla,\nMarcin Kadras, Sylvain Gugger, and Jeremy\nHoward. 2019. Multiﬁt: Efﬁcient multi-lingual lan-\nguage model ﬁne-tuning. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5706–5711.\nGoran Glavaš, Mladen Karan, and Ivan Vuli ´c. 2020.\nXHate-999: Analyzing and detecting abusive lan-\nguage across domains and languages. In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 6350–6365, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks.\nIn Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics ,\npages 8342–8360. Association for Computational\nLinguistics.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. Deberta: Decoding-enhanced\nbert with disentangled attention.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. Xtreme: A massively multilingual multi-task\nbenchmark for evaluating cross-lingual generaliza-\ntion. CoRR, abs/2003.11080.\nJigsaw. 2019. Jigsaw multilingual toxic comment\nclassiﬁcation. https://www.kaggle.com/c/\njigsaw-multilingual-toxic-comment-\nclassification.\nPhillip Keung, Yichao Lu, György Szarvas, and\nNoah A. Smith. 2020. The multilingual Amazon\nreviews corpus. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 4563–4568, Online. As-\nsociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Suárez, Yoann Dupont, Laurent Romary, Éric\nde la Clergerie, Djamé Seddah, and Benoît Sagot.\n2020. CamemBERT: a tasty French language model.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n7203–7219, Online. Association for Computational\nLinguistics.\nEndang Wahyu Pamungkas and Viviana Patti. 2019.\nCross-domain and cross-lingual abusive language\ndetection: A hybrid approach with deep learning\nand a multilingual lexicon. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics: Student Research Workshop,\npages 363–370, Florence, Italy. Association for\nComputational Linguistics.\nPeter Prettenhofer and Benno Stein. 2010. Cross-\nlanguage text classiﬁcation using structural corre-\nspondence learning. In Proceedings of the 48th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 1118–1127.\nAlec Radford, Rafal Józefowicz, and Ilya Sutskever.\n2017. Learning to generate reviews and discovering\nsentiment. CoRR, abs/1704.01444.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nPhillip Rust, Jonas Pfeiffer, Ivan Vuli ´c, Sebastian\nRuder, and Iryna Gurevych. 2020. How good is your\ntokenizer? on the monolingual performance of mul-\ntilingual language models.\nAnna Schmidt and Michael Wiegand. 2017. A survey\non hate speech detection using natural language pro-\ncessing. In Proceedings of the Fifth International\nWorkshop on Natural Language Processing for So-\ncial Media, pages 1–10, Valencia, Spain. Associa-\ntion for Computational Linguistics.\nHolger Schwenk and Xian Li. 2018. A Corpus for\nMultilingual Document Classiﬁcation in Eight Lan-\nguages. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC 2018), Miyazaki, Japan. European\nLanguage Resources Association (ELRA).\nJasdeep Singh, Bryan McCann, Nitish Shirish Keskar,\nCaiming Xiong, and Richard Socher. 2019. Xlda:\nCross-lingual data augmentation for natural lan-\nguage inference and question answering.\nHajung Sohn and Hyunju Lee. 2019. Mc-bert4hate:\nHate speech detection using multi-channel bert for\ndifferent languages and translations. 2019 Inter-\nnational Conference on Data Mining Workshops\n(ICDMW), pages 551–559.\n129\nLukas Stappen, Fabian Brunn, and B. Schuller. 2020.\nCross-lingual zero- and few-shot hate speech detec-\ntion utilising frozen transformer language models\nand axel. ArXiv, abs/2004.13850.\nMasatoshi Suzuki and Ryo Takahashi. 2019.\nPretrained japanese bert models. https:\n//github.com/cl-tohoku/bert-japanese.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma,\nJuhani Luotolahti, Tapio Salakoski, Filip Ginter, and\nSampo Pyysalo. 2019. Multilingual is not enough:\nBert for ﬁnnish. arXiv preprint arXiv:1912.07076.\nIvan Vuli ´c, Edoardo Maria Ponti, Robert Litschko,\nGoran Glavaš, and Anna Korhonen. 2020. Probing\npretrained language models for lexical semantics. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7222–7240.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. 2019a. Superglue: A\nstickier benchmark for general-purpose language un-\nderstanding systems. In Advances in Neural Infor-\nmation Processing Systems, volume 32, pages 3266–\n3280. Curran Associates, Inc.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In the Pro-\nceedings of ICLR.\nThomas Wolf, Julien Chaumond, Lysandre Debut, Vic-\ntor Sanh, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Morgan Funtowicz, Joe Davison, Sam\nShleifer, et al. 2020. Transformers: State-of-the-\nart natural language processing. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations,\npages 38–45.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason\nBaldridge. 2019. PAWS-X: A cross-lingual adver-\nsarial dataset for paraphrase identiﬁcation. In Pro-\nceedings of EMNLP 2019, pages 3685–3690.\nMarcos Zampieri, Preslav Nakov, Sara Rosenthal, Pepa\nAtanasova, Georgi Karadzhov, Hamdy Mubarak,\nLeon Derczynski, Zeses Pitenis, and Ça˘grı Çöltekin.\n2020. SemEval-2020 task 12: Multilingual offen-\nsive language identiﬁcation in social media (Offen-\nsEval 2020). In Proceedings of the Fourteenth\nWorkshop on Semantic Evaluation, pages 1425–\n1447, Barcelona (online). International Committee\nfor Computational Linguistics."
}