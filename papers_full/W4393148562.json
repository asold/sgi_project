{
  "title": "Collaborative Consortium of Foundation Models for Open-World Few-Shot Learning",
  "url": "https://openalex.org/W4393148562",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1968438403",
      "name": "Shuai Shao",
      "affiliations": [
        "Zhejiang Lab"
      ]
    },
    {
      "id": "https://openalex.org/A2113362690",
      "name": "Yu Bai",
      "affiliations": [
        "Zhejiang Lab",
        "China University of Petroleum, East China"
      ]
    },
    {
      "id": "https://openalex.org/A1484673654",
      "name": "Yan Wang",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2154102932",
      "name": "Baodi Liu",
      "affiliations": [
        "Zhejiang Lab",
        "China University of Petroleum, East China"
      ]
    },
    {
      "id": "https://openalex.org/A1974433614",
      "name": "Bin Liu",
      "affiliations": [
        "Zhejiang Lab",
        "China University of Petroleum, East China"
      ]
    },
    {
      "id": "https://openalex.org/A2113362690",
      "name": "Yu Bai",
      "affiliations": [
        "Zhejiang Lab",
        "China University of Petroleum, East China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4319878528",
    "https://openalex.org/W6785458781",
    "https://openalex.org/W6694286045",
    "https://openalex.org/W12634471",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W2155904486",
    "https://openalex.org/W3205789812",
    "https://openalex.org/W4320477178",
    "https://openalex.org/W6792296345",
    "https://openalex.org/W6810166859",
    "https://openalex.org/W6776482070",
    "https://openalex.org/W6644585003",
    "https://openalex.org/W6966705318",
    "https://openalex.org/W6804723902",
    "https://openalex.org/W6774980544",
    "https://openalex.org/W3184153358",
    "https://openalex.org/W2017814585",
    "https://openalex.org/W6851125663",
    "https://openalex.org/W3213647938",
    "https://openalex.org/W4323323799",
    "https://openalex.org/W6800895557",
    "https://openalex.org/W4362599073",
    "https://openalex.org/W2272331516",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3176709420",
    "https://openalex.org/W3016847592",
    "https://openalex.org/W4312526008",
    "https://openalex.org/W4382458283",
    "https://openalex.org/W4386790226",
    "https://openalex.org/W3205508356",
    "https://openalex.org/W4320853938",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3009213340",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4386075985",
    "https://openalex.org/W3102616566",
    "https://openalex.org/W4383605194",
    "https://openalex.org/W4324311474",
    "https://openalex.org/W4205176217",
    "https://openalex.org/W4311414306",
    "https://openalex.org/W4379539341",
    "https://openalex.org/W3198377975"
  ],
  "abstract": "Open-World Few-Shot Learning (OFSL) is a crucial research field dedicated to accurately identifying target samples in scenarios where data is limited and labels are unreliable. This research holds significant practical implications and is highly relevant to real-world applications. Recently, the advancements in foundation models like CLIP and DINO have showcased their robust representation capabilities even in resource-constrained settings with scarce data. This realization has brought about a transformative shift in focus, moving away from â€œbuilding models from scratchâ€ towards â€œeffectively harnessing the potential of foundation models to extract pertinent prior knowledge suitable for OFSL and utilizing it sensiblyâ€. Motivated by this perspective, we introduce the Collaborative Consortium of Foundation Models (CO3), which leverages CLIP, DINO, GPT-3, and DALL-E to collectively address the OFSL problem. CO3 comprises four key blocks: (1) the Label Correction Block (LC-Block) corrects unreliable labels, (2) the Data Augmentation Block (DA-Block) enhances available data, (3) the Feature Extraction Block (FE-Block) extracts multi-modal features, and (4) the Text-guided Fusion Adapter (TeFu-Adapter) integrates multiple features while mitigating the impact of noisy labels through semantic constraints. Only the adapter's parameters are adjustable, while the others remain frozen. Through collaboration among these foundation models, CO3 effectively unlocks their potential and unifies their capabilities to achieve state-of-the-art performance on multiple benchmark datasets. https://github.com/The-Shuai/CO3.",
  "full_text": "Collaborative Consortium of Foundation Models for\nOpen-World Few-Shot Learning\nShuai Shao1, Yu Bai1,2, Yan Wang3, Baodi Liu2*, Bin Liu1*\n1Zhejiang Lab\n2China University of Petroleum (East China)\n3Beihang University\n{shaoshuai0914, wangyan9509, thu.liubaodi, bliu.81}@gmail.com, baiyu upc@163.com\nAbstract\nOpen-World Few-Shot Learning (OFSL) is a crucial research\nfield dedicated to accurately identifying target samples in\nscenarios where data is limited and labels are unreliable.\nThis research holds significant practical implications and is\nhighly relevant to real-world applications. Recently, the ad-\nvancements in foundation models like CLIP and DINO have\nshowcased their robust representation capabilities even in\nresource-constrained settings with scarce data. This realiza-\ntion has brought about a transformative shift in focus, mov-\ning away from â€œbuilding models from scratchâ€ towards â€œef-\nfectively harnessing the potential of foundation models to ex-\ntract pertinent prior knowledge suitable for OFSL and uti-\nlizing it sensiblyâ€. Motivated by this perspective, we intro-\nduce the Collaborative Consortium of Foundation Models\n(CO3), which leverages CLIP, DINO, GPT-3, and DALL-E\nto collectively address the OFSL problem. CO 3 comprises\nfour key blocks: (1) the Label Correction Block (LC-Block)\ncorrects unreliable labels, (2) the Data Augmentation Block\n(DA-Block) enhances available data, (3) the Feature Extrac-\ntion Block (FE-Block) extracts multi-modal features, and (4)\nthe Text-guided Fusion Adapter (TeFu-Adapter) integrates\nmultiple features while mitigating the impact of noisy labels\nthrough semantic constraints. Only the adapterâ€™s parameters\nare adjustable, while the others remain frozen. Through col-\nlaboration among these foundation models, CO 3 effectively\nunlocks their potential and unifies their capabilities to achieve\nstate-of-the-art performance on multiple benchmark datasets.\nhttps://github.com/The-Shuai/CO3.\nIntroduction\nFew-shot learning (FSL) is a valuable area of research that\nenable identification in data-deficient and resource-limited\nscenarios and has made significant progress (Zhang et al.\n2023c,a; Shao et al. 2021b; Guo et al. 2023; Shao et al.\n2021a; Zhang et al. 2023b). However, methods that exhibit\nexceptional performance in research settings may struggle\nwhen applied to real-world situations. One critical factor is\nthe overly idealized settings often employed in previous FSL\nresearch, which fail to adapt to the complexities of open-\nworld scenarios. For instance, most studies assume that the\nlabel information of available data is cleaned, disregarding\n*Corresponding author.\nCopyright Â© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nAvailable data\npanda\nTo-be-recognized data\npanda pandacat?\nâ€¦\ncat\nFigure 1: An example of Open-World Few-Shot Learning\n(OFSL) as defined by (An et al. 2023). It portrays a scenario\ncalled 2-way 1-shot, where there are two classes, each con-\ntaining only one available sample. However, these samples\nare accompanied by unreliable labels that may include noise.\nNoisy labels come from other categories that have appeared\nor the unseen class.\nthe presence of noise or errors commonly found in practical\napplication scenarios.\nTo tackle this challenge, (An et al. 2023) proposed Open-\nWorld Few-Shot Learning (OFSL) (see Fig. 1). OFSL is\nspecifically designed to address the detrimental impact of\nnoise in training data, which comes from both known and\nunseen categories. Compared to traditional weakly super-\nvised learning (involves a small number of noisy labels\nalongside a large number of clean labels as guidance) and\nunsupervised learning (lacks labeled data entirely), OFSL\nfaces even greater challenges due to its unique circum-\nstances. Specifically, when there are only a few training\nsamples available, especially just one sample per category,\nthe negative impact of incorrect labeling on the model be-\ncomes significantly more detrimental than having no label at\nall. Therefore, finding effective solutions to the OFSL prob-\nlem is of utmost importance. This involves developing tech-\nniques and algorithms that can robustly handle noisy labels\nand effectively utilize the limited available training samples.\nLarge-scale image recognition in the open world has been\na prominent research area since 2015 (Bendale and Boult\n2015), leading to significant advancements (Joseph et al.\n2021; Wang, Ramanan, and Hebert 2019). However, the\ndevelopment of recognition tasks specifically for few-shot\nsamples has only recently gained attention. Various strate-\ngies, such as metric learning and feature aggregation based\napproaches (An et al. 2023; Liang et al. 2022), have been\nproposed to enhance representation ability in the presence of\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n4740\nnoisy labels. In more recent times, there has been increasing\ninterest in foundation models such as CLIP (Radford et al.\n2021) and DINO (Caron et al. 2021). These models are pre-\ntrained on large-scale datasets and possess powerful archi-\ntectures, allowing them to retain strong representation capa-\nbilities even in scenarios with limited data and finite com-\nputational resources. This realization motivates us to shift\nthe focus from â€œdesigning models from scratchâ€ towards\nâ€œharnessing the potential and expertise embedded within\nthese pre-trained foundation models in a sensible manner\nto enhance OFSL performanceâ€. Compared to starting with\na blank slate, foundation models undergo extensive valida-\ntion and tuning, making them more robust against overfitting\nissues commonly encountered in OFSL, while also saving\nmuch time and computational resources.\nIn this paper, we propose the Collaborative Consortium\nof Foundation Models (CO3, see Fig. 2) for OFSL, leverag-\ning the unique capabilities of four foundation models: CLIP\n(Radford et al. 2021), GPT-3 (Brown et al. 2020), DINO\n(Caron et al. 2021), and DALL-E (Ramesh et al. 2021). Each\nmodel contributes to the consortiumâ€™s strength in enabling\ncross-modal comparisons, comprehensive description gen-\neration, feature extraction, and image generation. CO3 com-\nprises 4 distinct blocks: (1) Label Correction Block (LC-\nBlock, see Fig. 3) utilizes CLIP, GPT-3, DINO, and DALL-E\nto achieve accurate label correction. (2) Data Augmentation\nBlock (DA-Block) employs DALL-E to generate diverse\nsamples based on the corrected labels, enriching the avail-\nable training data. (3) Feature Extraction Block (FE-Block)\nadopts CLIP and DINO to obtain three types of features:\nDINO-based image features, CLIP-based image features,\nand CLIP-based text features. (4) Text-guided and Fusion\nAdapter (TeFu-Adapter, see Fig. 4) is specifically designed\nfor the OFSL task. It fuses the aforementioned features and\nutilizes the semantic modality information of the samples to\nconstrain the adapterâ€™s parameters. This approach effectively\nmitigates the impact of noise labels on the model. By incor-\nporating these four blocks, CO3 offers a comprehensive and\nrobust solution to the challenges faced in OFSL.\nOur main contributions are summarized as follows:\nâ€¢ We propose CO 3, which effectively leverages the poten-\ntial and prior knowledge of foundation models to en-\nhance OFSL performance.\nâ€¢ We introduce TeFu-Adapter, a specially designed mech-\nanism that reduces the negative impact of noisy labels\nduring the adapter updating stage.\nâ€¢ We extensively evaluate CO 3 on multiple benchmark\ndatasets, demonstrating significant improvements com-\npared to other state-of-the-art (SOTA) methods.\nRelated Work\nFoundation Models Recently, research on foundation\nmodels is in full swing. Here, we introduce four models used\nin our paper: GPT-3 (Brown et al. 2020) is a Transformer-\nbased language model that captures statistical patterns and\nstructures in language. It predicts the next word in a sen-\ntence to understand contextual relationships, enabling it to\nexcel in tasks like machine translation and text completion.\nCLIP (Radford et al. 2021) is a versatile vision and language\nmodel that aligns image and text representations in a shared\nlatent space. It uses contrastive learning to maximize agree-\nment between matching pairs of image-text inputs while\nminimizing agreement between non-matching pairs. CLIP\nlearns to understand concepts and relationships across dif-\nferent modalities by training on large datasets with diverse\nimages and text.DALL-E (Ramesh et al. 2021) is a powerful\ngenerative model that synthesizes highly diverse and realis-\ntic images from textual descriptions. Through an encoder-\ndecoder architecture, DALL-E encodes natural language de-\nscriptions into latent vectors, which are then used by the de-\ncoder to generate corresponding images.DINO (Caron et al.\n2021) is an unsupervised learning framework consisting of\nteacher and student networks. The teacher network encodes\ndata into high-dimensional representations, which the stu-\ndent network aligns with through contrastive learning. By\nleveraging the teacher-student framework and contrastive\nloss, DINO enhances unsupervised representation learning,\napplicable to tasks like classification and object detection.\nOpen-World Few-Shot Learning Since 2015, open-\nworld object recognition (Bendale and Boult 2015, 2016)\ngained attention by detecting open sets and managing many\nsamples. Now, researchers focus on solving recognition\nproblems in scenarios with limited data, leading to OFSL\n(An et al. 2023; Willes et al. 2023). Thereâ€™s a focus on how\nnoisy labels affect seen and unseen classes. OFSL, akin to\nweakly supervised learning but with few samples, differs\nfrom robust few-shot learning (Lu et al. 2021) by addressing\nlabel noise from both visible and unseen classes. In contrast\nto conventional approaches like metric learning (An et al.\n2023), instance reweighting (Lu et al. 2021), and feature\naggregation (Liang et al. 2022) commonly used to tackle\nthis challenge, this paper introduces a pioneering alternative\nmethod that harnesses the power of foundation models to\neffectively overcome the hurdles associated with OFSL.\nFoundation Solutions on Few-Shot LearningIn the field\nof FSL, several notable works have emerged, harnessing\nthe capabilities of foundation models to achieve remarkable\nprogress. For examples, Tip-Adapter (Zhang et al. 2022) is a\ntraining-free method for enhancing CLIPâ€™s few-shot classifi-\ncation by constructing an adapter through a key-value cache\nmodel, updating the prior knowledge encoded in CLIP via\nfeature retrieval; CaFo (Zhang et al. 2023c) leverages lin-\nguistic prompts from GPT, synthetic images from DALL-\nE, and a learnable cache model to blend predictions from\nCLIP and DINO, achieving advanced performance by har-\nnessing the potential of various pre-trained methods. More-\nover, many follow-up works (Guo et al. 2023; Zhou et al.\n2022; Guo et al. 2023; Zhu et al. 2023; Roy et al. 2022; Cui\net al. 2023; Rong et al. 2023; Palanisamy et al. 2023) also\nmake significant contributions to the research community.\nThe research work that greatly inspired us is CaFo. How-\never, there are three key differences between CaFo and CO3:\n(1) CaFo mainly focuses on general situations and overlooks\nthe open-world case, while our approach takes into account\nboth scenarios; (2) The overall structure of our CO 3 differs\nsignificantly from CaFoâ€™s. (3) Moreover, unlike CaFo, which\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n4741\nbird\ndog\nfish\nLC-Block\nDA-Block\nFE-Block\nTeFu-Adapter\nbird\ndog\nmushroom\nbird\ndog\nmushroom\nâ€¦\nâ€¦\nâ€¦\nFrozen\n Tuned\nğ…ğ‘ğ‘™ğ‘–ğ‘ğ‘¡ğ‘’ğ‘¥\nğ‘ğ‘›\nğ…ğ‘ğ‘™ğ‘–ğ‘ğ‘–ğ‘šğ‘”\nğ…ğ‘‘ğ‘–ğ‘›ğ‘œ\nğ‘™ğ‘œğ‘”ğ‘–ğ‘¡ğ‘ \nFigure 2: The flowchart of our Collaborative Consortium of Foundational Model (CO3).\nuses a mature Tip-Adapter for model adjustment, CO3 incor-\nporates a task-specific TeFu-Adapter designed to precisely\nmeet the requirements of the task. This tailored adaptation\nenables our entire pipeline to operate more effectively.\nProblem Setup\nFew-Shot Learning In the standard FSL, the base set\nDbase contains a large amount of labeled data, which is used\nto pre-train a feature extraction model for downstream tasks.\nThe novel set Dnovel is the general term for all the data in\nthe downstream tasks, and it is divided into support setS and\nquery set Q, where S âˆ© Q= âˆ…. S = {(xi, yi, ti)}NÃ—K\ni=1 con-\ntains a few labeled data, and the Q = {(xi, yi, ti)}NÃ—K+M\ni=NÃ—K\nis the to-be-tested data, where xi denotes the image sample,\nyi âˆˆ Lrepresents its label, ti âˆˆ Tis its category name; L\nand T are the label and category name sets;N is the number\nof classes in S, K is the number of samples per class, they\nare usually called N-way K-shot, and M is the number of\nsamples in Q. FSL aims to use only a few support samples\nto accurately recognize the categories of query data.\nOpen-World Few-Shot Learning. Compared to the FSL,\nOFSL is a more difficult but more valuable task for prac-\ntical applications. Its goal is to efficiently identify the\nquery categories under the premise that the support labels\nare subject to random contamination and lack reliability.\nS = {Sclean, Snoise}NÃ—K\ni=1 , where Sclean = {(xi, yi, ti)},\nSnoise = {(xi, yj, tj)}. yj âˆˆ Ë†L is the noise label, indicat-\ning that the i-th image belongs to class-i, but is erroneously\nlabeled as class-j ; tj âˆˆ Ë†T denotes the corresponding noise\ncategory name. Ë†L and Ë†T denote the noise label set and the\ncategory name set. The samplesâ€™ noise labels come from\nother categories that have appeared in the support set, or the\nunseen class. In the training stage, we do not know which\nsampleâ€™s label is the noise label.\nMethodology\nOverview Unlike conventional OFSL approaches that rely\non additional base data to train a feature extractor, this paper\ntakes a different approach by directly utilizing frozen foun-\ndation models (CLIP, DINO, DALL-E, GPT-3) and incor-\nporating our specially designed adapter. The training frame-\nwork for 3-way 1-shotrecognition is illustrated in Fig. 2. It\ninvolves the following steps:\nâ€¢ Feeding the support data into the Label Correction Block\n(LC-Block) to clean the dataâ€™s noisy labels. The LC-\nBlock consists of parameter-frozen foundation models,\nnamely CLIP, DINO, DALL-E, and GPT-3.\nâ€¢ Sending the corrective data to the Data Augmentation\nBlock (DA-Block) to get more abundant training data.\nThis block is also frozen and uses the DALL-E model.\nâ€¢ Inputting the cleaned original data and augmented data to\nthe Feature Extraction Block (FE-Block) to obtain three\nkinds of features, which are: (1) the image feature ex-\ntracted by DINO encoder; (2) the image feature extracted\nby CLIPâ€™s image encoder; (3) and the text feature cor-\nresponding to the imageâ€™s category name extracted by\nCLIPâ€™s text encoder. All the models are frozen.\nâ€¢ Fusing these features with our designed Text-guided and\nFusion Adapter (TeFu-Adapter) and realizing the classi-\nfication. The TeFu-Adapter is the only block that needs\nfine-tuning in the pipeline.\nIn the inference stage, we only need the FE-Block and\nTeFu-Adapter to achieve classification.\nLabel Correction Block We design a two-pronged LC-\nBlock and illustrate its flowchart in Fig. 3.\nThe first branch combines DALL-E with DINO to achieve\nan initial label correction result, which involves three stages:\n(1) We input the category names of support data (may con-\ntain incorrect labels) into the DALL-E model, which gen-\nerates new images based on these names. Then, we extract\nfeatures from these generated images using DINO. By aver-\naging these features, we obtain pseudo prototypes for each\nimage class. (2) Simultaneously, we utilize DINO to directly\nextract features from the original support images. (3) We\nnext calculate the similarity between these extracted features\nand the prototypes obtained earlier. This enables us to deter-\nmine a coarse-grained label for each image based on their\nfeature similarities. The process can be formulated as:\nË†Fp\ndino = P rototype\n\u0010\nMdino\n\u0010\nMdalle\n\u0010\nË†T\n\u0011\u0011\u0011\n(1)\nË†Fdino = Mdino (X) (2)\nË†U1 = (Ë†Fdino)T Ë†Fp\ndino (3)\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n4742\nbird dog fish\nA dog looks like â€¦\nA bird looks like â€¦\nA fish looks like â€¦\nbird dog fish\n bird dog\n fish\nPrototype\n à· ğ…ğ‘‘ğ‘–ğ‘›ğ‘œ\nğ‘\nà· ğ…ğ‘‘ğ‘–ğ‘›ğ‘œ\nà·¡ğ”1 = (à· ğ…ğ‘‘ğ‘–ğ‘›ğ‘œ)ğ‘‡ à· ğ…ğ‘‘ğ‘–ğ‘›ğ‘œ\nğ‘\nà· ğ…ğ‘ğ‘™ğ‘–ğ‘ğ‘¡ğ‘’ğ‘¥\nğ‘ğ‘›\nà· ğ…ğ‘ğ‘™ğ‘–ğ‘ğ‘–ğ‘šğ‘”\nà·¡ğ”2 = (à· ğ…ğ‘ğ‘™ğ‘–ğ‘ğ‘–ğ‘šğ‘”)ğ‘‡ à· ğ…ğ‘ğ‘™ğ‘–ğ‘ğ‘¡ğ‘’ğ‘¥\nğ‘ğ‘›\nğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(à·¡ğ”1+Î±à·¡ğ”2)\nğ“œğ’…ğ’‚ğ’ğ’ğ’†\nğ“œğ’ˆğ’‘ğ’•\nğ“œğ’…ğ’Šğ’ğ’\nğ“œğ’…ğ’Šğ’ğ’\nğ“œğ’„ğ’ğ’Šğ’‘ğ’•ğ’†ğ’™\nğ“œğ’„ğ’ğ’Šğ’‘ğ’Šğ’ğ’ˆ\nFigure 3: The flowchart of Label Correction Block (LC-Block). It comprises three steps: (1) Firstly, we generate new samples\nusing DALL-E based on category names. Then, we use DINO to extract features from both the generated and original images.\nNext, we compute prototypes for each class using the generated data. Predictions are made by comparing the similarity between\nthe original features and the prototypes. (2) In parallel, we use GPT-3 to capture descriptions of different categories. These\ndescriptions are inputted into the CLIP text encoder to obtain encoded representations. Simultaneously, the original images\nare processed using the CLIP image encoder to extract distinctive features. Predictions are made by comparing the similarity\nbetween image features and the encoded category descriptions. (3) Labels are corrected based on the results from steps (1) and\n(2). All models remain frozen without further updates during the training process.\nwhere Mdalle and Mdino denote the frozen DALL-E and\nDINO; P rototypeis the operator to compute the pseudo\nprototypes for all classes of generated images;X is the set of\noriginal images; Ë†Fp\ndino âˆˆ RdimÃ— Ë†N is the prototype features,\ndim denotes the dimension, and Ë†N represents the length of\nthe noise support label set; Ë†Fdino âˆˆ RdimÃ—NK indicates the\noriginal image features; Ë†U1 âˆˆ RNK Ã— Ë†N denotes the similar-\nity matrix between the original images and the prototypes.\nThe second branch utilizes GPT-3 and CLIP for cross-\nmodal comparisons, combining visual and textual informa-\ntion, which also has three components: (1) Initially, we use\nGPT-3 to generate comprehensive descriptions for the cat-\negory names in the support set. Then, we employ CLIPâ€™s\ntext encoder to encode these descriptions, obtaining infor-\nmative text representations for each class. This step captures\ndetailed textual information that accurately describes each\ncategory. (2) Next, we extract features from the original sup-\nport images using CLIPâ€™s image encoder. This enables us to\ncapture the visual characteristics and details of each sample.\n(3) Following, we calculate the similarity between the text\nfeatures obtained from the class names and the image fea-\ntures derived from the original support set. By measuring the\nsimilarity between these two sets of features, we establish a\nmeaningful correspondence between the textual descriptions\nand the visual content. This allows us to refine and validate\nthe labels associated with the support set samples accurately.\nWe formulated the process as:\nË†Fcn\ncliptex = Mcliptex\n\u0010\nMgpt\n\u0010\nË†T\n\u0011\u0011\n(4)\nË†Fclipimg = Mclipimg (X) (5)\nË†U2 = (Ë†Fclipimg )T Ë†Fcn\ncliptex (6)\nwhere Mgpt, Mcliptex , and Mclipimg denote the frozen\nGPT-3, CLIPâ€™s text encoder and CLIPâ€™s image encoder;\nË†Fcn\ncliptex âˆˆ RdimÃ— Ë†N indicates the features of textual cate-\ngory names; Ë†Fclipimg âˆˆ RdimÃ—NK indicates the original\nimage features; Ë†U2 âˆˆ RNK Ã— Ë†N denotes the similarity ma-\ntrix between the original images and the class names.\nThen we combine the results from both branches, con-\nsidering their strengths and weaknesses. This integrated ap-\nproach guarantees a dependable and comprehensive solu-\ntion, resulting in more accurate and comprehensive results.\nThe process can be summarized as follows:\nË†p = Softmax ( Ë†U1 + Î± Ë†U2) (7)\nË™L, Ë™T = Refinement(Ë†p, Î²) (8)\nwhere Ë†p represents the probability that a sample belongs to\na specific class; Ë™L and Ë™T denote the sets of corrective labels\nand category names, respectively. Î± and Î² are the hyper-\nparameters; Refinement is a sampling process to correct\nlabels, which is outlined below: (1) If the predicted result\naligns with the given label, we consider the label to be cor-\nrect. (2) In cases of inconsistent predictions and given labels,\nwe assess the prediction probability. If it surpasses a thresh-\nold (Î²), we adopt the predicted label as the ground-truth la-\nbel. (3) However, if the prediction probability falls below\nthis threshold (Î²), we retain the original label as the ground-\ntruth label. This approach ensures accurate label correction\nby considering the confidence of predictions while respect-\ning the initial labeling information.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n4743\nData Augmentation Block. DA-Block tackles the limited\ntraining data challenge in FSL tasks by expanding and en-\nriching the available data. Similar to LC-Block, DA-Block\nutilizes DALL-E to generate image information from cate-\ngory names. However, the key difference is that the category\nnames provided as input in DA-Block have undergone la-\nbel correction. To optimize efficiency, DA-Block only gen-\nerates non-overlapping category images between sets Ë™T and\nË†T . This eliminates redundancy and allows direct access to\nthe remaining images from LC-Block, maximizing com-\nputational resource utilization. By incorporating DA-Block,\nwe overcome data scarcity in FSL task by augmenting and\ndiversifying support samples. This augmentation enhances\nthe modelâ€™s generalization ability and performance on query\ndata. The augmented data set is denoted asXaug, which can\nbe formulated as:\nXaug = Mdalle( Ë™T ) (9)\nFeature Extraction Block. FE-Block is a crucial compo-\nnent responsible for extracting essential features from both\nthe original cleaned data and augmented data. It generates\nthree types of features: (1) The DINO encoder is used to\nextract image features, capturing visual characteristics and\npatterns for subsequent analysis and classification.; (2) The\nCLIPâ€™s image encoder is employed within the FE-Block to\nextract rich representations of the visual content in the im-\nages, enhancing a comprehensive understanding of their vi-\nsual attributes; (3) FE-Block also utilizes CLIPâ€™s text en-\ncoder to extract text features from category names associ-\nated with the images. This process enhances semantic un-\nderstanding and facilitates alignment with their respective\nclasses. We formulate the process as:\nFcn\ncliptex = Mcliptex\n\u0010\nMgpt\n\u0010\nË™T\n\u0011\u0011\n(10)\nFclipimg = Mclipimg (X, Xaug) (11)\nFdino = Mdino (X, Xaug) (12)\nwhere Fcn\ncliptex âˆˆ RdimÃ— Ë™N indicates the features of correc-\ntive textual category names; Ë™N represents the length of the\ncorrective support label set; Fclipimg âˆˆ RdimÃ—(NK + Ë™NK â€²)\ndenotes the original and augmented CLIP features; Kâ€² de-\nnotes the augmented shots per class by DALL-E; Fdino âˆˆ\nRdimÃ—(NK + Ë™NK â€²) denotes the original and augmented\nDINO features. By combining these three types of extracted\nfeatures, the FE-Block enables a holistic representation of\nboth the visual and textual aspects of the data. These fea-\ntures serve as valuable inputs for subsequent classification.\nText-guided Fusion Adapter. In light of the preceding\noperations, we have successfully extracted three significant\nfeatures from DINO and CLIP. The focus of this section\nis to maximize the utilization of this diverse information\nwhile mitigating the detrimental impact of label noise. To\naccomplish these goals, an innovative approach called TeFu-\nAdapter is introduced (see Fig. 4). TeFu-Adapter utilizes\na simple multilayer perceptron (MLP) to fuse the visual\nfeatures extracted from DINO and CLIPâ€™s image encoders.\nğ…ğ‘ğ‘™ğ‘–ğ‘ğ‘–ğ‘šğ‘”\nğ…ğ‘‘ğ‘–ğ‘›ğ‘œ\nğ‘™ğ‘œğ‘”ğ‘–ğ‘¡ğ‘ 2 = (ğ…ğ‘ğ‘™ğ‘–ğ‘ğ‘–ğ‘šğ‘”)ğ‘‡ğ…ğ‘ğ‘™ğ‘–ğ‘ğ‘¡ğ‘’ğ‘¥\nğ‘ğ‘›\nğ“œğ’•ğ’†ğ’‡ğ’–\nğ‘™ğ‘œğ‘”ğ‘–ğ‘¡ğ‘  = ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡ğ‘ 2 +ğœ†ğ‘’ğœ”(ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡ğ‘ 1âŠ™ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡ğ‘ 2âˆ’1)\nğ‘™ğ‘œğ‘”ğ‘–ğ‘¡ğ‘ 1 FC\nBN\nGELU\nFC\nğ…ğ‘ğ‘™ğ‘–ğ‘ğ‘–ğ‘šğ‘”\nğ…ğ‘ğ‘™ğ‘–ğ‘ğ‘¡ğ‘’ğ‘¥\nğ‘ğ‘›\nFigure 4: The flowchart of Text-guided Fusion Adapter\n(TeFu-Adapter).\nThe fusion process enables the computation of logits by di-\nrectly incorporating the corrective labels. Leveraging this\napproach, the TeFu-Adapter ensures seamless integration of\nvisual information from both encoders, facilitating precise\ncalculations and predictions. We define this step as follows:\nlogits1 = Mtefu (Fclipimg , Fdino) (13)\nwhere Mtefu denotes the TeFu-Adapter, logits1 âˆˆ\nR(NK + Ë™NK â€²)Ã— Ë™N .\nFurthermore, acknowledging the potential presence of er-\nrors in the corrective labels, the TeFu-Adapter incorporates\nthe text encoding of each category to guide the calculation\nof image features and get the corresponding logits. By in-\ncorporating semantic information derived from the category\nnames, the TeFu-Adapter diminishes reliance on potentially\nerroneous labels, enhancing the approachâ€™s robustness. We\nformulate this step as:\nlogits2 = (Fclipimg )T Fcn\ncliptex (14)\nwhere logits2 âˆˆ R(NK + Ë™NK â€²)Ã— Ë™N .\nFinally, the TeFu-Adapter merges the two sets of logits\nand calculates the cross-entropy loss. By skillfully leverag-\ning the TeFu-Adapter to seamlessly integrate textual and vi-\nsual information while mitigating label noise, this method-\nology bolsters the robustness and precision of the learning\nprocess. We formulate this step as:\nlogits = logits2 + Î»eÏ‰(logits1âŠ™logits2âˆ’1) (15)\nloss = CrossEntropy (Softmax (logits)) (16)\nwhere logits âˆˆ R(NK + Ë™NK â€²)Ã— Ë™N ; âŠ™ denotes the Hadamard\ninner product; Î» and Ï‰ are the hyperparameters.\nExperiments\nDatasets We evaluate the performance of our methods\non multiple well-known publicly available datasets: Ima-\ngeNet (Deng et al. 2009), OxfordPets (Parkhi et al. 2012),\nCaltech101 (Fei-Fei, Fergus, and Perona 2004), Food101\n(Bossard, Guillaumin, and Van Gool 2014), Sun397 (Xiao\net al. 2010). We follow CaFo (Zhang et al. 2023c) and APE\n(Zhu et al. 2023) to train our models using 1, 2, 4, 8, and 16\nlabeled samples per class from the support set, and then test\nthem on the entire query set. We introduce varying propor-\ntions of noisy labels to the support data in each dataset.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n4744\nMethods Time Noisy Label\nProportion\n0.0 0.1 0.2 0.3\n0.4 0.5 0.6 0.7 0.8 0.9 1.0\nCoOp (Zhou et al.\n2022) 45min 57.15 60.04 57.15 56.58 55.90 50.70 40.80 36.91 22.71 9.44 4.57\nTip-Adapter-F (Zhang et al. 2022) 1min 61.32 60.67 60.35 59.97 59.86 59.00 59.94 58.92 58.32 57.35 57.73\nCLIP-Adapter (Gao et al. 2023) 2min 61.20 59.21 57.45 55.19 53.07 51.94 50.44 45.90 38.91 17.74 18.85\nCALIP-FS (Guo et al. 2023) 20min 61.35 58.07 57.56 56.86 57.07 56.23 57.07 56.08 58.08 57.56 58.07\nCaFo (Zhang et al. 2023c) 7min 63.80 61.53 60.16 59.99 59.78 58.70 58.64 57.98 58.52 58.82 59.03\nAPE-T (Zhu et al. 2023) 1min 62.50 58.43 57.00 51.02 54.04 52.90 51.72 51.53 51.17 50.80 50.20\nCO3 (Ours) 7min 63.07 63.03 63.06 62.86\n62.65 62.72 62.71 62.61 62.43 62.50 62.58\nTable 1: 1-shot accuracy (%) of methods on ImageNet. Time denotes the training time on one A100 GPU.\nImplementation Our approach integrates GPT-3, DALL-\nE, CLIP, and DINO. GPT-3 is responsible for generating\ncategory descriptions, while DALL-E generates images for\neach category. We directly adopted the design of CaFo. CLIP\nand DINO serve as feature extractors. We use ResNet50 as\nthe backbone for CLIP. The TeFu-Adapter, comprising two\nlinear layers, is initialized using Kaiming initialization. We\nset the initial learning rate to 0.001 and employ AdamW as\nthe optimizer, along with CosineAnnealingLR as the sched-\nuler. During training, the data undergoes operations such as\nrandom cropping, random flipping, and normalization, with\na batch size of 256. For testing, we use a batch size of 64. To\nintroduce noise in the labels, incorrect labels are randomly\nassigned to support samples.\nPerformance on ImageNet We compare foundation\nmodel based methods utilizing frozen foundation models\nwith added adapters for fine-tuning, including CoOp (Zhou\net al. 2022), Tip-Adapter (Zhang et al. 2022), CLIP-Adapter\n(Gao et al. 2023), CALIP-FS (Guo et al. 2023), CaFo (Zhang\net al. 2023c), and APE-T (Zhu et al. 2023).\nTab. 1 and Fig. 5(left) present the results under 1-shot con-\nditions with varying proportions of noisy labels. Fig. 5(right)\nshows the results with a fixed noisy label ratio of 0.3 and\nvarying numbers of available samples per class. Based on\nour observations, the following conclusions can be drawn:\n(1) CO3 surpasses other SOTAs in the open-world setting,\ndelivering outstanding performance even when the noise ra-\ntio reaches 100%. Furthermore, it maintains low compu-\ntational costs, achieving an advantageous balance between\nperformance and efficiency. (2) In the 1-shot setting with\n0.3 noisy label proportion, CO 3 achieves a remarkable re-\nsult of 62.86%, surpassing all comparison methods across\n16-shots. (3) For most methods, an increase in available sup-\nport data does not significantly enhance model performance\ndue to the negative impact of noisy labels. In fact, many\nmethods experience performance degradation. However, our\nmethod is robust to noise-induced variations. These findings\ndemonstrate that our CO 3 effectively and stably addresses\nthe OFSL problem.\nPerformance on Other Datasets To rigorously evaluate\nthe robustness of our CO 3 across different scenarios, we\nconducted extensive testing on 10 additional datasets. The\nexperimental results for OxfordPets, Caltech101, Food101,\nand Sun397 can be found in Fig. 6. Upon observing the re-\nFigure 5: Performance (%) Comparison on ImageNet.\nsults, our method consistently exhibits leading performance\nacross multiple datasets in open-world cases, reaffirming its\nexceptional robustness. This impressive performance is at-\ntributed to two key factors: the collaborative utilization of di-\nverse foundation models and the design of specific adapters.\nThe consistent superiority of our method over alternative ap-\nproaches highlights its distinct advantage in effectively tack-\nling the OFSL challenge.\nAblation Study We conduct ablation studies and list the\nresults in Tab 2,3 to assess the efficiency of different blocks.\n(1) LC-Block serves a dual role in the pipeline. It can act\nas an auxiliary module to assist in achieving the final classifi-\ncation, or it can independently leverage the foundation mod-\nels for classification purposes. From Tab. 2, we observe that\ndirectly using the LC-Block for classification yields unsatis-\nfactory results (line â‘ ). However, employing it as an auxil-\niary module and making decisions based on thresholds leads\nto improvements of at least 1.8% (lines â‘¡ and â‘¥). These\nobservations highlight the effectiveness of utilizing the LC-\nBlock in a complementary manner within the pipeline.\n(2) DA-Block plays a vital role in effectively expand-\ning few-shot data by leveraging the prior knowledge of the\nDALL-E model. The results of lines â‘¢ and â‘¥ in Tab. 2 em-\nphasize the significance of the DA-Block, as it can lead to a\nnotable improvement of approximately 1.2% in OFSL task.\n(3) FE-Block works in conjunction with the TeFu-\nAdapter, seamlessly integrating CLIP and DINO to collect\ntwo different types of features. By examining linesâ‘£ and â‘¥\nin Tab. 2, it is evident that utilizing fusion features improves\naccuracy by 3.5% compared to solely relying on CLIP fea-\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n4745\nFigure 6: Performance (%) comparison on other datasets.\nLC DA FE TeFu NLP\nCLIP DINO logits1 logits2 0.3\nâœ“ 61.14\nâœ“ âœ“ âœ“ âœ“ âœ“ 61.02\nâœ“ âœ“ âœ“ âœ“ âœ“ 61.69\nâœ“ âœ“ âœ“ âœ“ 59.32\nâœ“ âœ“ âœ“ âœ“ 31.05\nâœ“ âœ“ âœ“ âœ“ âœ“ âœ“ 62.86\nTable 2: Ablation study (%) of different blocks on ImageNet\nwith 1-shot case. NLP is short for Noisy Label Proportion.\nAdapters NLP\n0.1 0.3 0.5 0.7 0.9\nw/o Adapter 20.62 15.17 15.56 13.17 15.92\nTip-Adapter 61.00 59.45 58.24 56.41 52.29\nCLIP-Adapter 59.97 56.33 52.73 43.21 30.08\nTeFu-Adapter 63.03 62.86 62.72 62.61 62.50\nTable 3: Ablation study (%) of different adapters on Ima-\ngeNet with 1-shot case. All comparison methods adopt our\nmodel architecture but utilize different final adapters. NLP\nis short for Noisy Label Proportion.\ntures. Moreover, relying exclusively on DINO features (lines\nâ‘¤ and â‘¥) results in a significant drop in accuracy to around\n30%. These findings emphasize the importance of integrat-\ning both CLIP and DINO features through FE-Block.\n(4) TeFu-Adapter has proven to be effective in facilitat-\ning multimodal fusion, as discussed in relation to the FE-\nBlock above. To further demonstrate its superiority, we con-\nduct a comparison with classic Tip-Adapter, CLIP-Adapter\nstructures, and the absence of any adapters. Results are pre-\nsented in Tab. 3. Significantly, the TeFu-Adapter outper-\nforms other adapters consistently, achieving improvements\nranging from 2% to 10% under various noise conditions.\nAdditionally, the introduction of text guidance in the TeFu-\nAdapter mitigates the impact of noise, resulting in minimal\nfluctuations when noise is introduced. These observations\nhighlight the notable advantages of the TeFu-Adapter, show-\ncasing its ability to enhance performance and maintain sta-\nbility even in the presence of noise.\nConclusion\nTo tackle the challenge posed by OFSL, we introduce\nCO3, an innovative approach that leverages prior knowledge\nwithin foundation models. Extensive experiments on multi-\nple datasets have demonstrated its efficacy. Looking ahead,\nour forthcoming endeavors will be concentrated on two piv-\notal areas: (1) We plan to expand the scope of OFSL by ad-\ndressing a wider range of practical tasks beyond what has\nbeen studied in this paper. This will enable us to bridge\nthe gap between research and real-world applications. (2)\nWhile acknowledging the foundation modelâ€™s accomplish-\nments, we are committed to delving into the underlying\ncauses behind its occasional underperformance, thereby un-\nlocking the untapped potential of the foundation model and\nbolstering its overall effectiveness.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n4746\nAcknowledgments\nThis work was supported by Youth Foundation Project of\nZhejiang Lab (No.K2023RC0AA01), Exploratory Research\nProject of Zhejiang Lab (No.2022RC0AN02), Major Basic\nResearch Project in Shandong Province (No.ZR2023ZD32).\nReferences\nAn, Y .; Xue, H.; Zhao, X.; and Wang, J. 2023. From instance\nto metric calibration: a unified framework for open-world\nfew-shot learning. TPAMI, 9757â€“9773.\nBendale, A.; and Boult, T. 2015. Towards open world recog-\nnition. In CVPR, 1893â€“1902.\nBendale, A.; and Boult, T. E. 2016. Towards open set deep\nnetworks. In CVPR, 1563â€“1572.\nBossard, L.; Guillaumin, M.; and Van Gool, L. 2014.\nFood-101â€“mining discriminative components with random\nforests. In ECCV, 446â€“461.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. In\nNeurIPS, volume 33, 1877â€“1901.\nCaron, M.; Touvron, H.; Misra, I.; JÂ´egou, H.; Mairal, J.; Bo-\njanowski, P.; and Joulin, A. 2021. Emerging properties in\nself-supervised vision transformers. In ICCV, 9650â€“9660.\nCui, Y .; Yu, Z.; Cai, R.; Wang, X.; Kot, A. C.; and\nLiu, L. 2023. Generalized few-shot continual learn-\ning with contrastive mixture of adapters. arXiv preprint\narXiv:2302.05936.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, 248â€“255.\nFei-Fei, L.; Fergus, R.; and Perona, P. 2004. Learning gen-\nerative visual models from few training examples: An incre-\nmental bayesian approach tested on 101 object categories.\nIn CVPRW, 178â€“178.\nGao, P.; Geng, S.; Zhang, R.; Ma, T.; Fang, R.; Zhang, Y .; Li,\nH.; and Qiao, Y . 2023. Clip-adapter: Better vision-language\nmodels with feature adapters. IJCV.\nGuo, Z.; Zhang, R.; Qiu, L.; Ma, X.; Miao, X.; He, X.; and\nCui, B. 2023. Calip: Zero-shot enhancement of clip with\nparameter-free attention. In AAAI, volume 37, 746â€“754.\nJoseph, K.; Khan, S.; Khan, F. S.; and Balasubramanian,\nV . N. 2021. Towards open world object detection. InCVPR,\n5830â€“5840.\nLiang, K. J.; Rangrej, S. B.; Petrovic, V .; and Hassner, T.\n2022. Few-shot learning with noisy labels. In CVPR, 9089â€“\n9098.\nLu, J.; Jin, S.; Liang, J.; and Zhang, C. 2021. Robust few-\nshot learning for user-provided data. TNNLS, 32(4): 1433â€“\n1447.\nPalanisamy, K.; Chao, Y .-W.; Du, X.; Xiang, Y .; et al. 2023.\nProto-clip: Vision-language prototypical network for few-\nshot learning. arXiv preprint arXiv:2307.03073.\nParkhi, O. M.; Vedaldi, A.; Zisserman, A.; and Jawahar, C.\n2012. Cats and dogs. In CVPR, 3498â€“3505. IEEE.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from natural\nlanguage supervision. In ICML, 8748â€“8763.\nRamesh, A.; Pavlov, M.; Goh, G.; Gray, S.; V oss, C.; Rad-\nford, A.; Chen, M.; and Sutskever, I. 2021. Zero-shot text-\nto-image generation. In ICML, 8821â€“8831.\nRong, J.; Chen, H.; Chen, T.; Ou, L.; Yu, X.; and Liu, Y .\n2023. Retrieval-enhanced visual prompt learning for few-\nshot classification. arXiv preprint arXiv:2306.02243.\nRoy, A.; Shah, A.; Shah, K.; Roy, A.; and Chellappa, R.\n2022. DiffAlign: Few-shot learning using diffusion based\nsynthesis and alignment. arXiv preprint arXiv:2212.05404.\nShao, S.; Xing, L.; Wang, Y .; Xu, R.; Zhao, C.; Wang, Y .;\nand Liu, B. 2021a. Mhfc: Multi-head feature collaboration\nfor few-shot learning. In ACMMM, 4193â€“4201.\nShao, S.; Xing, L.; Xu, R.; Liu, W.; Wang, Y .; and Liu, B.\n2021b. Mdfm: Multi-decision fusing model for few-shot\nlearning. TCSVT, 32(8): 5151â€“5162.\nWang, Y .-X.; Ramanan, D.; and Hebert, M. 2019. Meta-\nlearning to detect rare objects. In ICCV, 9925â€“9934.\nWilles, J.; Harrison, J.; Harakeh, A.; Finn, C.; Pavone, M.;\nand Waslander, S. 2023. Bayesian embeddings for few-shot\nopen world recognition. TPAMI.\nXiao, J.; Hays, J.; Ehinger, K. A.; Oliva, A.; and Torralba,\nA. 2010. Sun database: Large-scale scene recognition from\nabbey to zoo. In CVPR, 3485â€“3492.\nZhang, J.; Gao, L.; Hao, B.; Huang, H.; Song, J.; and\nShen, H. 2023a. From Global to Local: Multi-scale Out-\nof-distribution Detection. TIP.\nZhang, J.; Gao, L.; Luo, X.; Shen, H.; and Song, J. 2023b.\nDETA: Denoised Task Adaptation for Few-Shot Learning.\nIn ICCV.\nZhang, R.; Fang, R.; Zhang, W.; Gao, P.; Li, K.; Dai, J.;\nQiao, Y .; and Li, H. 2022. Tip-adapter: Training-free clip-\nadapter for better vision-language modeling. In ECCV.\nZhang, R.; Hu, X.; Li, B.; Huang, S.; Deng, H.; Qiao, Y .;\nGao, P.; and Li, H. 2023c. Prompt, generate, then cache:\nCascade of foundation models makes strong few-shot learn-\ners. In CVPR, 15211â€“15222.\nZhou, K.; Yang, J.; Loy, C. C.; and Liu, Z. 2022. Learning\nto prompt for vision-language models. IJCV, 130(9): 2337â€“\n2348.\nZhu, X.; Zhang, R.; He, B.; Zhou, A.; Wang, D.; Zhao, B.;\nand Gao, P. 2023. Not all features matter: Enhancing few-\nshot clip with adaptive prior refinement. In ICCV.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n4747",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.7109649181365967
    },
    {
      "name": "One shot",
      "score": 0.5351133942604065
    },
    {
      "name": "Shot (pellet)",
      "score": 0.5016813278198242
    },
    {
      "name": "Computer science",
      "score": 0.34197577834129333
    },
    {
      "name": "Engineering",
      "score": 0.31223955750465393
    },
    {
      "name": "Geography",
      "score": 0.19816946983337402
    },
    {
      "name": "Materials science",
      "score": 0.10854688286781311
    },
    {
      "name": "Mechanical engineering",
      "score": 0.09185829758644104
    },
    {
      "name": "Archaeology",
      "score": 0.08054512739181519
    },
    {
      "name": "Metallurgy",
      "score": 0.0
    }
  ]
}