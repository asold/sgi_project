{
  "title": "RaLLe: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models",
  "url": "https://openalex.org/W4389524176",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2528414878",
      "name": "Yasuto Hoshi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1628784624",
      "name": "Daisuke Miyashita",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2736945646",
      "name": "Youyang Ng",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Kento Tatsuno",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2029233081",
      "name": "Yasuhiro Morioka",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1528476537",
      "name": "Osamu Torii",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1898984694",
      "name": "Jun Deguchi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2963469388",
    "https://openalex.org/W4318719006",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4385714635",
    "https://openalex.org/W2948715311",
    "https://openalex.org/W4385572504",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4321392130",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4309088836",
    "https://openalex.org/W3146844750",
    "https://openalex.org/W3174114019",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4308244910",
    "https://openalex.org/W3180230246",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W3169283738",
    "https://openalex.org/W4367000491",
    "https://openalex.org/W4375949262",
    "https://openalex.org/W4287674181",
    "https://openalex.org/W4386576685",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W4240838011",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W4310923309",
    "https://openalex.org/W4281485769",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4389518671",
    "https://openalex.org/W2970360209",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4321177655"
  ],
  "abstract": "Yasuto Hoshi, Daisuke Miyashita, Youyang Ng, Kento Tatsuno, Yasuhiro Morioka, Osamu Torii, Jun Deguchi. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 52–69\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nRALLE: A Framework for Developing and Evaluating\nRetrieval-Augmented Large Language Models\nYasuto Hoshi∗, Daisuke Miyashita∗, Youyang Ng, Kento Tatsuno,\nYasuhiro Morioka, Osamu Torii, Jun Deguchi\nKioxia Corporation, Japan\nyasuto1.hoshi@kioxia.com\nAbstract\nRetrieval-augmented large language models\n(R-LLMs) combine pre-trained large language\nmodels (LLMs) with information retrieval\nsystems to improve the accuracy of factual\nquestion-answering. However, current libraries\nfor building R-LLMs provide high-level ab-\nstractions without sufficient transparency for\nevaluating and optimizing prompts within spe-\ncific inference processes such as retrieval\nand generation. To address this gap, we\npresent RALLE, an open-source framework de-\nsigned to facilitate the development, evaluation,\nand optimization of R-LLMs for knowledge-\nintensive tasks. With RALLE, developers can\neasily develop and evaluate R-LLMs, improv-\ning hand-crafted prompts, assessing individ-\nual inference processes, and objectively mea-\nsuring overall system performance quantita-\ntively. By leveraging these features, developers\ncan enhance the performance and accuracy of\ntheir R-LLMs in knowledge-intensive genera-\ntion tasks. We open-source our code athttps:\n//github.com/yhoshi3/RaLLe.\n1 Introduction\nLarge language models (LLMs) have shown great\npotential for natural language understanding and\ngeneration tasks (Brown et al., 2020; Chowdh-\nery et al., 2022; OpenAI, 2023). However, they\nface challenges when answering factual questions\ndue to hallucinations (or confabulations) (Bang\net al., 2023; Borji, 2023), outdated parametric\nknowledge (Liska et al., 2022), and memory effi-\nciency of parametric knowledge (e.g., Heinzerling\nand Inui, 2021). To address these limitations, re-\nsearchers have turned to the retrieval-augmented\napproach used in open-domain question answering\n(QA) (Chen et al., 2017), hereinafter referred to as\nretrieval-augmented LLMs or R-LLMs.\nIn comparison to closed-book settings where lan-\nguage models generate answers without retrieval,\n∗ These authors contributed equally to this work.\nR-LLMs (open-book settings) enable the retrieval\nof relevant information from external databases\nor corpora (Mialon et al., 2023; Ng et al., 2023),\nwhich has led to improved accuracy in open-\ndomain QA (Shi et al., 2023). Additionally, R-\nLLMs can acquire extended features even without\nadditional training, such as explicit references, re-\nlief from fact hallucination (Nakano et al., 2021),\nand easy updates to the knowledge source (e.g.,\nGuu et al., 2020; Ng et al., 2023).\nRetrieval-augmented generation needs further\nresearch and development to reach its full poten-\ntial. For example, even though the retriever-reader\nsystem has been trained on the Natural Questions\n(NQ) dataset (Kwiatkowski et al., 2019), its F1\nscore on the short answer task is 68.3 and still lags\nbehind the oracle F1 score of 75.7 (Asai and Choi,\n2021). This implies that further improvements can\nbe made to the retrieval-augmented generation ap-\nproach. Additionally, users would be probably\naware that the outputs generated by R-LLMs may\ncontain factual errors, particularly when applied\nto knowledge-intensive tasks. However, there is\ncurrently a lack of accessible evaluation framework\nto assess their output quality. This makes it difficult\nto identify areas for improvement.\nFurthermore, having effective tools for develop-\ning R-LLMs is crucial. These tools should enable\nthe design of inference steps such as retrieve-then-\ngenerate, selecting the combination of retrievers\nand LLMs, evaluating the performance of the en-\ntire system, and testing the prompts used in each\ninference step. Currently available tools, such as\nthe ChatGPT Retrieval Plugin 1, Guidance 2, and\nLangChain3 (Chase, 2023), offer a high degree\nof abstraction, making it challenging to verify the\nfunctionality of individual inference steps or opti-\n1https://github.com/openai/\nchatgpt-retrieval-plugin\n2https://github.com/microsoft/guidance\n3Note: Our code does not use either of these.\n52\nEmbedding and \nIndexing\nKnowledge-source\nDocuments\nDocument\nEncoder\nEmbedding\nVectors\nIndex\nDesign an\nInference Chain\nEvaluate the Developed R-LLM Simple Chat Interface\nEasy to test out the best practices.To track results, configs, and prompts\non MLflow.\nOutput (Answer)\nAction 1\ne.g., Query rewriting\nwith an LLM\nRALLE: Retrieval-Augmented LLM Development and Evaluation framework \nInput (Question)\nWhere does us highway 1 start and end\nU.S. Route 1 starts in Key West, \nFlorida and ends in Fort Kent, Maine.\nAction 2\ne.g., Retrieval using\nthe rewritten question\nAction N\ne.g., Output an answer\nwith an LLM\nPrompt Engineering for Each Action\nAnswer Generation\nPrompt template Prompt LLM output\nHighlights the\ngold answers\nFigure 1: Overview of RALLE, our proposed development and evaluation framework for R-LLMs. Any number of\nactions can be defined for an R-LLM. Each action can be executed individually to test the corresponding prompts.\nExperimental setup and evaluation results can be tracked using MLflow. Additionally, a simple chat interface can be\nbuilt to test out the best practices from the development and evaluation stages in a practical setting.\nmize prompts within each step. This lack of trans-\nparency might hinder the optimization of R-LLMs.\nIn this paper, we propose RALLE, an accessible\nframework for Retrieval-Augmented Large Lan-\nguage model development and Evaluation. We\nalso present evaluation results of several R-LLMs\nthat we have constructed by using open-source re-\ntrievers and LLMs. To the best of our knowledge,\nRALLE is the first framework that empowers R-\nLLM developers and open-domain QA researchers\nto efficiently develop, evaluate, and improve R-\nLLMs using objective metrics.\nRALLE offers several key benefits:\n1. Easy development and testing: users can eas-\nily select, combine, and test various retrievers\nand LLMs, especially open-source models,\nwithin a graphical interface.\n2. Objective evaluation of R-LLMs: RALLE pro-\nvides reproducible experiments with objec-\ntive benchmarks/metrics, enabling objective\nassessments of R-LLM performance.\n3. Transparent prompt engineering: all inputs\n(prompts) and outputs of each action are vis-\nible to developers, allowing for easy explo-\nration and optimization of the prompts.\n2 R ALLE Usage\nFigure 1 presents an overview of the key features\nof the proposed framework4. The primary develop-\nment process involves three stages: (1) embedding\nand indexing the knowledge source documents, (2)\ndesigning an inference chain consisting of an R-\nLLM with customized prompt templates for each\naction, and (3) benchmarking the developed R-\nLLM.\n2.1 Document Embedding and Indexing\nTo begin, the knowledge source documents can be\nencoded using an arbitrary encoder model, such\nas a sparse or dense retriever. For efficient in-\ndexing of dense embeddings, several methods\nare available by default, including Faiss (Johnson\net al., 2019), HNSW (Malkov and Yashunin, 2020),\nand DiskANN (Jayaram Subramanya et al., 2019).\nBy default, an HNSW index is constructed with\nef_construction = 128(the size of the dynamic\nlist for the nearest neighbors) and m = 32 (the\nnumber of links created for every new element dur-\ning graph construction).\n2.2 Chain Construction\nOnce the document embedding and indexing are\ncompleted, the retrievers (and the correspond-\ning indices) and LLMs can be loaded via the\n4Please also review the demonstration screencast.\n53\nGradio5-based GUI (Abid et al., 2019) to es-\ntablish an inference chain that comprises an\nR-LLM. This chain of actions enables users\nto design a pipeline for multi-step inference,\nsuch as [retrieve]-[generate], or more intricate\nworkflows such as [rewrite query]-[retrieve]-\n[generate] proposed in Ma et al. (2023). The ver-\nsatility of this feature is especially beneficial in\ncreating the chains tailored to specific use cases.\nA single-action chain can function as either a\nsimple retriever that returns the retrieved docu-\nments, or a closed-book QA that leverages the para-\nmetric knowledge of an LLM to provide answers\nwithout retrieval. In contrast, a chain with multi-\nple actions that include retrieval enables retrieval-\naugmented generation or open-book QA, allowing\nan LLM to access external documents relevant to a\nquestion. Our default setup for R-LLMs consists\nof two actions: retrieve and generate.\n2.3 Prompt Engineering\nThe RALLE framework allows developers to in-\nteractively craft customized prompt templates for\nLLMs and even for search queries on a per-chain\nbasis. Each action can be executed independently,\nenabling precise control over LLM responses, such\nas specifying the desired output format or suppress-\ning undesirable hallucinations. To enhance the ver-\nsatility of prompt development, RALLE integrates\nsupport for f-strings and eval() function in Python.\n2.4 Experiment Tracking\nWe utilize MLflow (LF Projects, 2023) to track the\nexperiments, along with their associated configura-\ntion files and prompt templates. This allows us to\ncompare the performance of different experiment\nruns objectively, which enables us to develop even\nbetter R-LLMs.\n2.5 Chat AI\nRALLE also provides support for building a sim-\nple chat interface. This enables users to test out\nbest practices from the development and evaluation\nstages in a practical setting.\n3 Experimental Settings\nIn this section, we evaluate the performance of R-\nLLMs constructed with several combinations of\nopen-source retrievers and LLMs on knowledge-\nintensive tasks.\n5https://www.gradio.app/\n3.1 Tasks and Datasets\nWe employ KILT (Knowledge Intensive Language\nTasks) benchmark (Petroni et al., 2021), an ex-\ntensive benchmark that encompasses 11 datasets\nacross five knowledge-intensive natural language\nprocessing tasks: fact checking, entity linking, slot\nfilling, open-domain question answering, and dia-\nlogue (for further details of KILT, see Petroni et al.\n(2021)). We use the training sets for developing\nprompts and the development set for evaluation.\nAs the knowledge source, we utilize the pre-\nprocessed Wikipedia passages provided by KILT.\nThe passages are derived from English Wikipedia\narticles based on the 2019/08/01 Wikipedia dump\ndata, consisting of a total of 5.9 million articles\nand 22.2 million 100-word passages. For both\ndense and sparse retrievers, we use the set of 100-\nword passages after additional pre-processing that\nprepends the title of the article to each passage.\nNote that RALLE is dataset-agnostic, allowing\ndevelopers to use their own QA datasets and cor-\npora for development and evaluation. See Ap-\npendix A.10 for more information.\n3.2 Models\nThis subsection details the retrievers and LLMs\nemployed to build R-LLMs in our experiments.\nRALLE allows practitioners and researchers to eas-\nily experiment with the most recent models avail-\nable in open-source repositories. With the excep-\ntion of BM25, all models are available from Hug-\nging Face (Wolf et al., 2020) (see Appendix A.9\nfor the summary).\n3.2.1 LLMs\nThe LLM used within the R-LLM must compre-\nhend instructions provided in a prompt and gener-\nate appropriate responses based on the given infor-\nmation. To achieve this, we use instruction-tuned\nLLMs with a temperature parameter set to zero for\noptimal performance and reproducibility.\nLlama-2-chat is tuned with supervised fine-\ntuning and reinforcement learning with human\nfeedback (RLHF) (Christiano et al., 2017; Stiennon\net al., 2020) to align to human preferences for help-\nfulness and safety (Touvron et al., 2023b). In our\nexperiments, we utilize both 13-billion (Llama2-\n13B) and 70-billion (Llama2-70B) models.\nWizardVicunaLM-13B6 (W-Vicuna-13B)\n(Lee, 2023) is formed by combining the concepts\n6https://huggingface.co/junelee/\nwizard-vicuna-13b\n54\nModel dim. max len. MTEB Retrieval\nBM25 - - 42.3 ♠\nm-e5 1,024 514 51.43\ne5 1,024 512 50.56\nTable 1: Summary of the retrievers used in our eval-\nuation. Dimensions of a dense embedding vector are\nshown in dim., while the maximum token length of an\ninput sequence is max len.. The evaluation metric for\nMTEB Retrieval is nDCG@10. ♠: Results from Ram\net al. (2022). Results on MTEB Retrieval except BM25\nare copied from MTEB leaderboard7.\nof WizardLM (Xu et al., 2023) (refining the\ninitial instructions with Evol-Instruct method (Xu\net al., 2023)) and Vicuna (Chiang et al., 2023) (a\nfine-tuned LLaMA model (Touvron et al., 2023a)\nwith multi-round conversation data from chatbots).\n3.2.2 Retrievers\nWe experiment with both sparse and dense retriev-\ners for document retrieval. Specifically, we select\ndense retrievers that have achieved high accuracy\non the retrieval task of Massive Text Embedding\nBenchmark (MTEB) (Muennighoff et al., 2023)\nleaderboard7 as of July 2023. A list of the retriev-\ners used in our study can be found in Table 1. In\nthe open-book experiments, the top-5 most relevant\ndocuments are retrieved.\nAs the metrics of retrieval performance, we fol-\nlow Petroni et al. (2021) and use the page-level\nR-precision (Craswell, 2016) and recall@5. The\npage-level R-precision is the percentage of R gold\npages inside each provenance set among the top-R\nretrieved pages. Typically, R-Precision is equiva-\nlent to Precision@1 except FEVER and HotPotQA\n(multi-hop datasets).\nBM25 (Robertson and Zaragoza, 2009) is a\nbag-of-words retrieval function based on the term-\nmatching. We use the Pyserini (Lin et al., 2021)\nimplementation of unigram BM25 with the default\nparameters of k1 = 0.9 (term frequency scaling)\nand b = 0.4 (document length normalization). The\ndocuments for BM25 retrieval is the same 100-\nword passages as the dense retrievers.\ne5-large-v28 (e5) (Wang et al., 2022) is a super-\nvised bi-encoder model with a query encoder and a\n7https://huggingface.co/spaces/mteb/\nleaderboard\n8https://huggingface.co/intfloat/\ne5-large-v2\ndocument encoder. multilingual-e5-large9 (m-e5)\nis a multilingual fine-tuned e5 model.\n3.3 Prompts\nWe utilize custom-designed prompt templates that\nare specifically crafted for each dataset in KILT.\nRALLE accepts templates with non-natural lan-\nguage formats, such as f-strings and eval() func-\ntions in Python. This allows developers to care-\nfully craft their prompt templates for optimal per-\nformance. The prompt templates used in our exper-\niments are shown in Appendix A.11.\nFor entity linking task of KILT (AY2, WnWi,\nand WnCw), we employ a REWRITE-EL template\nby default for search queries. This template ex-\ntracts the specific entity mentions being questioned\nas a query, as employing an entire span of a ques-\ntion is unlikely to find relevant documents (we will\ndiscuss in Section 4.3). After retrieving the rele-\nvant documents, the top-1 Wikipedia title is output\nas an answer. As a result, the downstream accuracy\nin entity linking task is not affected by the number\nof retrieved documents (if one or more).\n4 KILT Benchmark Results\nThis section provides the downstream and retrieval\nperformance of the R-LLMs developed and evalu-\nated using RALLE.\n4.1 Baseline\nWe compare our results with those of the BART-\nlarge model (Lewis et al., 2020a) for the closed-\nbook setting and the RAG model (Lewis et al.,\n2020b) for the open-book setting, which presented\nin Petroni et al. (2021). Notably, these baseline\nmodels were specifically fine-tuned on the KILT\nbenchmark, whereas our chosen LLMs and con-\nstructed R-LLMs were not. See also Appendix A.5\nfor additional information of the baselines.\n4.2 Downstream Performance\nWe summarize the downstream performance10 in\nTable 2. RALLE also includes has_answer per-\ncentage for short answers, a proxy metric to mea-\nsure the proportion of questions that contain gold\nanswers within the final output generated by an\nR-LLM (see Appendix A.3 for more details).\n9https://huggingface.co/intfloat/\nmultilingual-e5-large\n10See also Table 6 in Appendix A.6 for additional results in\na closed-book setting.\n55\nFact Check. Entity Linking Slot Filling Open Domain QA Dial.\nDataset FEV AY2 WnWi WnCw T-REx zsRE NQ HoPo TQA ELI5 WoW\nModel / Metric Accuracy Exact Match RL F1\nBART-large♢(closed-book) 80.7 86.6 47.9 48.0 43.8 3.0 26.2 16.9 32.5 22.7 13.8\nLlama2-70B (closed-book) 33.6(74.9) 39.8(54.5) 42.8(53.8) 39.2(55.7) 28.5(40.5) 11.3(13.6) 19.6(37.4) 13.9(25.1) 67.4(80.8) 23.0 13.3\nRAG♢ 87.7 77.4 49.0 46.7 61.5 47.4 48.8 27.7 61.7 16.1 13.3\ne5 + W-Vicuna-13B10.6(42.4) 51.2(57.9) 48.6(51.4) 45.6(51.4) 31.6(46.1) 23.0(29.3) 18.7(38.0) 19.7(28.3) 43.1(67.7) 21.4 12.3\ne5 + Llama2-13B 66.3(73.5) 51.2(57.9) 48.6(51.4) 45.6(51.4) 17.2(42.3) 31.7(41.1) 36.1(43.3) 14.3(25.5) 56.3(76.2) 20.9 12.3\nBM25 + Llama2-70B46.2(86.3) 18.0(35.9) 19.1(32.2) 14.2(30.9) 25.9(43.0) 31.4(37.8) 25.3(34.3) 25.9(33.4) 65.8(80.0) 21.3 12.2\ne5 + Llama2-70B 49.9(88.6) 51.2(57.9) 48.6(51.4) 45.6(51.4) 28.9(49.2) 35.0(43.2) 36.4(48.8) 28.1(35.8) 71.1(83.9) 21.5 13.2\ne5 (DiskANN)49.9(87.9) 44.3(50.5) 45.3(48.1) 43.0(48.8) 25.3(43.9) 32.1(37.9) 36.1(48.4) 26.7(34.3) 70.4(83.2) 21.5 13.1\ntop-2 49.3(88.1) 51.2(57.9) 48.6(51.4) 45.6(51.4) 23.5(44.9) 34.7(43.0) 33.7(46.2) 23.8(34.2) 71.3(82.9) 21.6 13.3\ntop-10 50.2(88.0) 51.2(57.9) 48.6(51.4) 45.6(51.4) 31.1(49.3) 35.4(42.5) 35.2(48.1) 24.9(35.7) 59.3(82.8) 21.5 13.2\nModel / Metric KILT-Accuracy KILT-EM KILT-RLKILT-F1\nRAG♢ 55.5 77.4 49.0 46.7 25.4 42.6 36.3 3.1 36.1 2.7 7.5\ne5 + W-Vicuna-13B8.4(33.5) 51.2(51.2) 48.6(48.6) 45.5(45.5) 19.0(28.0) 22.2(28.1) 14.4(27.8) 8.6(11.9) 26.6(40.3) 2.7 7.3\ne5 + Llama2-13B 53.1(58.7) 51.2(51.2) 48.6(48.6) 45.5(45.5) 11.5(25.7) 29.8(38.5) 27.5(32.5) 5.6(10.6) 34.7(46.1) 2.7 7.4\nBM25 + Llama2-70B21.9(44.4) 17.6(17.6) 18.9(18.9) 13.9(13.9) 14.5(22.5) 24.9(29.6) 9.3(12.4) 4.5(5.9) 23.6(27.9) 1.5 4.0\ne5 + Llama2-70B 40.2(71.2) 51.2(51.2) 48.6(48.6) 45.5(45.5) 19.2(29.7) 32.8(40.4) 27.7(36.3) 11.3(14.5) 42.8(49.7) 2.7 8.1\ne5 (DiskANN) 38.3(68.5) 44.3(44.3) 45.3(45.3) 42.8(42.8) 19.3(24.2) 30.2(35.5) 27.3(35.9) 9.3(12.1) 42.1(49.0) 2.7 8.0\ntop-2 39.6(70.7) 51.2(51.2) 48.6(48.6) 45.5(45.5) 15.6(28.0) 32.9(40.6) 25.7(35.2) 7.6(13.1) 43.1(49.3) 2.7 8.3\ntop-10 40.4(70.7) 51.2(51.2) 48.6(48.6) 45.5(45.5) 20.5(29.9) 33.2(39.8) 27.1(36.1) 9.9(14.3) 36.1(48.9) 2.7 8.1\nTable 2: Downstream performance on KILT dev set. Following Petroni et al. (2021), we report the results of typical\nmetrics for each dataset, with bold indicating the best result and underlined indicating the second. The metrics with\nthe prefix KILT- award output performance only when R-Prec = 1 (retrieval success). The figures in parentheses\nrepresent has_answer percentage, which corresponds to the proportion of questions with gold answers included in\nthe final output. The figures shown in gray are copied from the column above because they do not change based\non the given setting (we use the Identity function of RALLE for the tasks, rather than an LLM). ♢: Results from\nPetroni et al. (2021).\nOur constructed R-LLM (e5 + Llama2-70B)\nsurpasses the performance of the RAG model on\nboth HoPo and TQA, despite not being fine-tuned\nwith KILT like RAG. Moreover, our constructed\nR-LLMs demonstrate acceptable accuracy levels\non other datasets as well, without any significant\ndrawbacks. The results indicate that the LLMs used\nin this study exhibit certain ability to comprehend\nthe retrieved documents.\nFurthermore, our analysis reveals several fac-\ntors that could contribute to improvement of down-\nstream performance, including retrieval augmen-\ntation (except ELI5), increased model scale (ex-\ncept FEV and T-REx), and referring to more docu-\nments during generation (except NQ, HoPo, TQA\nand WoW). However, some datasets exhibits excep-\ntions to these tendencies or had lower performance\ncompared to their corresponding has_answer per-\ncentage (such as FEV , T-REx, NQ, and TQA). To\naddress this issue, developers can improve the R-\nLLM with RALLE by refining the inference chain\nand the prompt templates. In Section A.4, we pro-\nvide our initial attempts at developing inference\nchains with three actions on several datasets.\nOverall, the downstream evaluation results pro-\nvide valuable insights into how well the constructed\nR-LLMs perform on knowledge-intensive tasks,\nenabling developers to identify areas for improve-\nment.\n4.3 Retrieval Performance\nTable 3 shows retrieval performance of the cho-\nsen retrievers on KILT development set (see also\nTable 8 in Appendix for the results of recall@5).\nAccording to Table 3, e5 (with Faiss Flat index)\nachieves the highest retrieval performance on aver-\nage, though m-e5 is better on MTEB Retrieval task\n(Table 1). Despite the superior retrieval accuracy\nof e5 compared to RAG on KILT, the downstream\nperformance of the R-LLM which employs e5 falls\nshort of that of RAG (Table 2). This indicates that\nthere is potential room for improvement through\nfurther optimized prompts to enhance the perfor-\nmance on a target dataset.\nAs described in Section 3.3, REWRITE-EL\nserves as the default template for search queries\n56\nFact Check. Entity Linking Slot Filling Open Domain QA Dial.\nDataset FEV AY2 WnWi WnCw T-REx zsRE NQ HoPo TQA ELI5 WoWAvg.\nModel R-Precision\nRAG♢ 63.5 77.4 49.0 46.7 29.3 65.4 60.3 30.8 49.3 16.4 46.7 48.6\nBM25 52.1 17.7 20.6 15.3 34.0 57.7 26.3 41.3 31.7 6.8 28.8 30.2\n−REWRITE-EL52.1 3.0 (−14.7) 0.1(−20.5) 2.8*(−12.5) 34.0 57.7 26.3 41.3 31.7 6.8 28.8 25.9(−4.3)\nm-e5 (Flat) 81.7 41.8 45.8 41.6 47.1 81.4 63.0 54.0 56.1 11.9 57.9 52.9\n−REWRITE-EL81.7 3.2 (−38.6) 0.1(−45.7) 3.1(−38.5) 47.1 81.4 63.0 54.0 56.1 11.9 57.9 41.8(−11.1)\ne5 (Flat) 82.0 51.6 51.6 49.2 45.3 81.9 65.2 54.3 56.1 12.9 56.8 55.2\n−REWRITE-EL82.0 3.4 (−48.2) 0.0(−51.6) 2.6(−46.6) 45.3 81.9 65.2 54.3 56.1 12.9 56.8 41.9(−13.3)\ne5 (HNSW) 67.9 38.9 42.3 40.5 23.1 53.0 60.3 34.9 50.4 10.2 54.5 43.3\n−REWRITE-EL67.9 2.9 (−36.0) 0.0(−42.3) 1.6(−38.9) 23.1 53.0 60.3 34.9 50.4 10.2 54.5 32.6(−10.7)\ne5 (DiskANN) 78.8 44.7 47.8 46.0 37.1 74.5 64.9 49.1 55.4 12.9 56.6 51.6\n−REWRITE-EL78.8 3.2 (−41.5) 0.1(−47.7) 1.8(−44.2) 37.1 74.5 64.9 49.1 55.4 12.9 56.6 39.4(−12.2)\nTable 3: Retrieval performances on KILT dev set. We report page-level R-Precision on KILT development set.Avg.\nrefers to macro-average of the retrieval scores in each dataset. Bold indicates the best result.♢: Results from Petroni\net al. (2021). *: BM25 (without REWRITE-EL) failed with long queries (45 out of 5,599 questions) in WnCw.\nRetrieval\nModel Avg. R-Prec Memory sec/Q\nBM25 30.2 - 0.121\ne5 (Flat) 55.2 84.8 GB 0.169\ne5 (HNSW) 43.3 90.4 GB 0.008\ne5 (DiskANN) 51.6 10.9 GB 0.022\nCompletion in the Closed-Book Settingsec/Q\nLlama-70B 6.727\nRetrieval + Generation sec/Q\nBM25 + Llama2-70B 3.637\ne5 + Llama2-70B 3.793\ne5 (DiskANN) + Llama2-70B 3.628\nTable 4: Execution latency in seconds per question\n(sec/Q). Memory in Retrieval indicates the maximum\n(DRAM) memory footprints.\nrelated to entity linking task (AY2, WnWi, and\nWnCw). As shown in Table 3, employing the\nREWRITE-EL template leads to higher retrieval\naccuracy when compared to using the full question\ntext as a search query ( −REWRITE-EL setting).\nThis indicates that omitting unnecessary informa-\ntion from the search queries is helpful especially\nfor entity linking task.\n4.4 Speed Analysis\nRALLE allows users to optimize the trade-off be-\ntween latency (in seconds per question) and ac-\ncuracy by comparing various configurations. As\ndemonstrated in Table 4, employing approximate\nnearest neighbor search (ANNS) algorithms such\nas HNSW and DiskANN can significantly reduce\nretrieval latency at the cost of decreased accuracy.\nNote that, the optimal balance between speed and\naccuracy depends on the specific requirements of\nthe application, and RALLE enables users to easily\nexperiment with diverse ANNS settings to deter-\nmine their impact on both factors.\nNotably, DiskANN achieves an accuracy that\nis only slightly lower than Faiss flat index while\nsignificantly improving search speeds, despite re-\nquiring less memory footprints than both flat and\nHNSW indices. Though the reduction in R-\nLLM execution time achieved through ANNS may\nappear relatively minor, the significantly lower\nDRAM requirements of DiskANN could make it a\nmore practical solution for scenarios where DRAM\ncapacity is limited and the flat index exceeds avail-\nable DRAM capacity. For further details regarding\nlatency, refer to Table 9 in Appendix A.8.\n5 Conclusion\nThis paper introduces RALLE, an accessible frame-\nwork for developing and evaluating R-LLMs. We\nalso report evaluation results of several R-LLMs\nbuilt using open-source retrievers and LLMs on\nknowledge-intensive tasks. Overall, RALLE offers\na significant advancement in retrieval-augmented\ngeneration research, enabling efficient develop-\nment, evaluation, and improvement of R-LLMs.\nWe hope that RALLE will contribute to the devel-\nopment of best practices for R-LLMs.\nLimitations\nAll KILT evaluations presented in this paper were\nconducted using a development set to maintain fair-\n57\nness and consistency across evaluations, as the an-\nswers of the test set remain confidential11.\nWhile R-LLMs exhibit high validity, it falls be-\nhind the smaller yet specialized model, RAG, on\nthe KILT downstream task (refer to Table 2). This\ndisparity can be attributed to various factors, includ-\ning prompt maturity and the ability of LLMs to gen-\nerate responses. Although the employed prompts\nwere carefully developed, it is likely that more opti-\nmal prompts exist (discussed in Section 4.3). More-\nover, fine-tuning LLMs with retrieval-augmented\ngeneration tasks might enhance their performance\non downstream tasks. Therefore, the evaluation\naccuracy reported herein would represent a conser-\nvative estimate.\nPrompt engineering is a crucial aspect of the\nretrieval-augmented generation process, as the gen-\nerated outputs can differ significantly between mod-\nels, even when provided with the same prompt.\nRALLE offers an advantage in this regard, allow-\ning users to effortlessly experiment with diverse\nprompts for varying behaviors, datasets, and intri-\ncate chain of actions.\nIn the realm of prompt development, techniques\nlike Automatic Prompt Engineer (APE) (Zhou\net al., 2023) automate the creation of prompts from\ninput-output pairs and sampling to identify the most\neffective prompts. However, the input-output pairs\nin retrieval-augmented generation are distinctly dif-\nferent from those of the simple instruction induc-\ntion tasks. Because the input text for retrieval-\naugmented generation can often be lengthy and\ncomplex, it is difficult to automatically induce the\neffective prompts from the input-output pairs.\nThis tool enables developers to construct an in-\nference chain with predefined actions, while recent\nadvances have also introduced methods allowing\nLLMs to determine the actions (Yao et al., 2023).\nOne approach entails retrieving documents using\na query rewritten by an LLM and then summariz-\ning them until the desired information is obtained.\nHowever, in our initial experiments (not described\nin this paper), we observed instances where rela-\ntively small LLMs (typically less than 100 billion\nparameters) became trapped in cycles of repeated\nretrieval and summarization, hindering their ability\nto reach the final answer generation. Our tool ad-\ndresses this issue by intentionally building explicit\ninference chains to avoid unintended operations.\n11https://eval.ai/web/challenges/\nchallenge-page/689/overview\nReferences\nAbubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan,\nAbdulrahman Alfozan, and James Zou. 2019. Gradio:\nHassle-free sharing and testing of ml models in the\nwild. arXiv preprint arXiv:1906.02569.\nAkari Asai and Eunsol Choi. 2021. Challenges in\ninformation-seeking QA: Unanswerable questions\nand paragraph retrieval. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 1492–1504, Online. Association\nfor Computational Linguistics.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arXiv\npreprint arXiv:2302.04023.\nAli Borji. 2023. A categorical archive of ChatGPT\nfailures. arXiv preprint arXiv:2302.03494.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nHarrison Chase. 2023. LangChain. https://\nlangchain.com/.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1870–1879,\nVancouver, Canada. Association for Computational\nLinguistics.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing GPT-4 with 90%* Chat-\nGPT quality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\n58\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. PaLM: Scaling language\nmodeling with pathways. arxiv:2204.02311.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Mar-\ntic, Shane Legg, and Dario Amodei. 2017. Deep\nreinforcement learning from human preferences. In\nAdvances in Neural Information Processing Systems,\nvolume 30. Curran Associates, Inc.\nNick Craswell. 2016. R-Precision, pages 1–1. Springer\nNew York, New York, NY .\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In Proceedings of the\n37th International Conference on Machine Learning,\nvolume 119 of Proceedings of Machine Learning\nResearch, pages 3929–3938. PMLR.\nBenjamin Heinzerling and Kentaro Inui. 2021. Lan-\nguage models as knowledge bases: On entity repre-\nsentations, storage capacity, and paraphrased queries.\nIn Proceedings of the 16th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics: Main Volume, pages 1772–1791, Online.\nAssociation for Computational Linguistics.\nSuhas Jayaram Subramanya, Fnu Devvrit, Harsha Vard-\nhan Simhadri, Ravishankar Krishnawamy, and Ro-\nhan Kadekodi. 2019. Diskann: Fast accurate billion-\npoint nearest neighbor search on a single node. In\nAdvances in Neural Information Processing Systems,\nvolume 32. Curran Associates, Inc.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452–466.\nJune Lee. 2023. WizardVicunaLM. https:\n//github.com/melodysdreamj/\nWizardVicunaLM.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020a.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020b.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Advances in Neural Infor-\nmation Processing Systems, volume 33, pages 9459–\n9474. Curran Associates, Inc.\nLF Projects. 2023. MLflow – a platform for the machine\nlearning lifecycle. https://mlflow.org/.\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-\nHong Yang, Ronak Pradeep, and Rodrigo Nogueira.\n2021. Pyserini: A Python toolkit for reproducible\ninformation retrieval research with sparse and dense\nrepresentations. In Proceedings of the 44th Annual\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval (SIGIR\n2021), pages 2356–2362.\nAdam Liska, Tomas Kocisky, Elena Gribovskaya, Tay-\nfun Terzi, Eren Sezener, Devang Agrawal, Cyprien\nDe Masson D’Autume, Tim Scholtes, Manzil Zaheer,\nSusannah Young, Ellen Gilsenan-Mcmahon, Sophia\nAustin, Phil Blunsom, and Angeliki Lazaridou. 2022.\nStreamingQA: A benchmark for adaptation to new\nknowledge over time in question answering models.\nIn Proceedings of the 39th International Conference\non Machine Learning, volume 162 of Proceedings\nof Machine Learning Research, pages 13604–13622.\nPMLR.\nXinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,\nand Nan Duan. 2023. Query rewriting for retrieval-\naugmented large language models. arXiv preprint\narXiv:2305.14283.\nYu A. Malkov and D. A. Yashunin. 2020. Efficient and\nrobust approximate nearest neighbor search using\nhierarchical navigable small world graphs. IEEE\nTrans. Pattern Anal. Mach. Intell., 42(4):824–836.\nGrégoire Mialon, Roberto Dessì, Maria Lomeli, Christo-\nforos Nalmpantis, Ram Pasunuru, Roberta Raileanu,\nBaptiste Rozière, Timo Schick, Jane Dwivedi-Yu,\n59\nAsli Celikyilmaz, Edouard Grave, Yann LeCun, and\nThomas Scialom. 2023. Augmented language mod-\nels: a survey. arXiv preprint arXiv:2302.07842.\nNiklas Muennighoff, Nouamane Tazi, Loic Magne, and\nNils Reimers. 2023. MTEB: Massive text embedding\nbenchmark. In Proceedings of the 17th Conference\nof the European Chapter of the Association for Com-\nputational Linguistics, pages 2014–2037, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021. Webgpt: Browser-assisted question-\nanswering with human feedback. arXiv preprint\narXiv:2112.09332.\nYouyang Ng, Daisuke Miyashita, Yasuto Hoshi, Ya-\nsuhiro Morioka, Osamu Torii, Tomoya Kodama, and\nJun Deguchi. 2023. SimplyRetrieve: A private and\nlightweight retrieval-centric generative ai tool. arXiv\npreprint arXiv:2308.03983.\nOpenAI. 2023. GPT-4 technical report. arXiv preprint\narXiv:2303.08774, abs/2303.08774.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\nRiedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544, Online.\nAssociation for Computational Linguistics.\nOri Ram, Liat Bezalel, Adi Zicher, Yonatan Be-\nlinkov, Jonathan Berant, and Amir Globerson. 2022.\nWhat are you token about? dense retrieval as\ndistributions over the vocabulary. arXiv preprint\narXiv:2212.10380.\nStephen Robertson and Hugo Zaragoza. 2009. The\nprobabilistic relevance framework: BM25 and be-\nyond. Found. Trends Inf. Retr., 3(4):333–389.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2023. RePlug: Retrieval-\naugmented black-box language models. arXiv\npreprint arXiv:2301.12652.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel\nZiegler, Ryan Lowe, Chelsea V oss, Alec Radford,\nDario Amodei, and Paul F Christiano. 2020. Learn-\ning to summarize with human feedback. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 3008–3021. Curran Associates,\nInc.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. LLaMA:\nOpen and efficient foundation language models.\narXiv preprint arXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023b. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing\nJiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. 2022. Text embeddings by weakly-\nsupervised contrastive pre-training. arXiv preprint\narXiv:2212.03533.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. 2023. WizardLM: Empowering large lan-\nguage models to follow complex instructions. arXiv\npreprint arXiv:2304.12244.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik R Narasimhan, and Yuan Cao. 2023.\nReAct: Synergizing reasoning and acting in language\nmodels. In The Eleventh International Conference\non Learning Representations.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nKeiran Paster, Silviu Pitis, Harris Chan, and Jimmy\nBa. 2023. Large language models are human-level\nprompt engineers. In The Eleventh International\nConference on Learning Representations.\nA Appendix\nA.1 Computational Resources\nThe evaluation experiments are conducted on an\nUbuntu 20.04.6 server equipped with Intel(R)\nXeon(R) Gold 6326 CPU at 2.90 GHz CPU cores,\nand one node with 4×NVIDIA A100 Tensor Core\nGPU with 40 GB memory, and a RAID-5 array\nwith a Dell(R) PERC H745 Front controller and\nKIOXIA(R) PM6-R SAS SSDs for storage. The\nCUDA version is 12.2, the Python version is 3.9.16,\nthe PyTorch version is 2.0.1, and the Transformers\nversion is 4.29.2.\n60\nFigure 2: A screenshot of the Development chaintab of RALLE. Developers can create tailored action chains\ncomprising multiple actions of inference. For each action, developers can specify a prompt template, confirm the\nresults of applying the template, and execute the action using the newly defined prompt, individually. Moreover,\nRALLE can highlight the gold answers within the retrieved documents or the output of the LLM, as well as highlight\nthe Wikipedia IDs of successfully retrieved provenance.\n61\nA.2 Development Screen of RALLE\nFigure 2 shows the chain development screen 12.\nDevelopers can create an inference chain for an R-\nLLM on this Develop chaintab. One can choose a\ndataset and specify the desired chain length, which\nrepresents the total number of actions. By default,\nthere are two actions: retrieving with a retriever\nand generating with an LLM.\nPrompt templates for each action can be defined\nusing f-strings or eval functions in Python. The\nresults of applying the template can be confirmed\nwithout executing retrieval and generation. The\nexecution result can be viewed by clicking the In-\nterpret prompt and execute this actionbutton.\nThe available action operators are LLM, Re-\ntriever, and Identity. LLM generates text based\non the given prompt. Retriever retrieves the top k\nmost relevant documents related to the input query.\nAnd Identity simply outputs the original prompt\nwithout employing a retriever or an LLM.\nTo execute the entire chain, click the Execute\nentire chainbutton. At the bottom of this tab, the\nselected question and its corresponding answer can\nbe reviewed. Also, RALLE enables to highlight\nthe gold answers within the retrieved documents\nor the output of the LLM, as well as highlight the\nWikipedia ID of successfully retrieved provenance.\nA.3 Additional Metric: has_answer\nRALLE also includes has_answer percentage (e.g.,\nKarpukhin et al., 2020) for short answers, a proxy\nmetric to measure the proportion of questions that\ncontain gold answers within the final output gen-\nerated by an R-LLM. By tracking this metric, de-\nvelopers can identify situations where the model\ngenerates responses that include gold answers but\nmay be overlooked due to evaluation biases such as\nexact matching. This information can help refine\nprompts to improve overall performance.\nA.4 Attempts to Build 3-action Chain\nAccording to Section 4.2, retrieval augmentation\nhas a significant impact on performance in fact\nchecking, open-domain QA for short answers, and\nslot filling tasks when comparing the closed-book\nand open-book settings of Llama2-70B. In entity\nlinking task (AY2, WnWi, and WnCw), however,\nour approach described in Section 3.3 (retrieve,\nthen output the top-1 retrieved Wikipedia title) may\nnot be effective.\n12Please also review the demonstration screencast.\nTo improve the performance, we construct a3-\naction chain for AY2 dataset: (1) retrieve top-5\nrelevant documents, (2) explain the entity mention\nbeing questioned, and (3) predict the Wikipedia\ntitle based on the explanation and top-5 retrieved\ntitles. Additionally, we explore developing 3-action\nchains for T-REx and NQ datasets, which involves\n(1) retrieval, (2) question rewriting, and (3) answer\ngeneration. Table 12 shows the prompts used in\n3-action chains.\nTable 5 shows the downstream performances\nwith the 3-action chains on AY2, NQ, and T-\nREx datasets. While the 3-action chain outper-\nforms the 2-action (retrieve-then-generate) chain\non NQ dataset, it underperforms the 2-action accu-\nracies on AY2 and T-REx datasets. This suggests\nthat the 3-action chains constructed specifically\nfor these two datasets require further optimization.\nHowever, the has_answer value for AY2 (70.0%)\nis higher than that of the 2-action chain (47.8%),\nindicating that incorporating post-processing steps\ninto the 3-action chain (thus to be 4-action chain)\ncould potentially boost accuracy, particularly for\nAY2.\nOne of the benefits of our tool is that it allows for\neasy definition of such additional inference actions.\nThis means that developers can customize the chain\nto perform specific tasks beyond the default setting,\ngiving them greater flexibility and control over their\ndevelopment.\nA.5 Details of Baseline Model in Open-Book\nSetting\nAs a baseline in open-book setting, we present\nthe results of the Retrieval-Augmented Genera-\ntion (RAG) model (Lewis et al., 2020b) shown in\nPetroni et al. (2021), which achieved strong perfor-\nmance in the KILT benchmark. The RAG model\ncomprises a bi-encoder retriever and a sequence-\nto-sequence generator (BART model (Lewis et al.,\n2020a)), both of which are trained end-to-end. The\ntotal number of trainable parameters in the RAG\nmodel is approximately 626 million. It is important\nto note that the RAG model was trained specifi-\ncally for the KILT benchmark, whereas our chosen\nLLMs and constructed R-LLMs were not.\nA.6 KILT Downstream Performances in\nClosed-Book Setting\nTable 6 summarizes the KILT downstream results\nin a closed-book setting. The baseline (BART-\nlarge) model has been fine-tuned on the KILT\n62\nFact Check. Entity Linking Slot Filling Open Domain QA Dial.\nDataset FEV AY2 WnWi WnCw T-REx zsRE NQ HoPo TQA ELI5 WoW\nModel / Metric Accuracy Exact Match RL F1\nLlama2-70B (closed-book) 33.6(74.9) 39.8(54.5) 42.8(53.8) 39.2(55.7) 28.5(40.5) 11.3(13.6) 19.6(37.4) 13.9(25.1) 67.4(80.8) 23.0 13.3\nRAG♢ 87.7 77.4 49.0 46.7 61.5 47.4 48.8 27.7 61.7 16.1 13.3\ne5 + Llama2-70B 49.9(88.6) 51.2(57.9) 48.6(51.4) 45.6(51.4) 28.9(49.2) 35.0(43.2) 36.4(48.8) 28.1(35.8) 71.1(83.9) 21.5 13.2\n3-action - 24.4 (70.0) - - 16.3 (46.8) - 36.9 (49.3) - - - -\nModel / Metric KILT-Accuracy KILT-EM KILT-RLKILT-F1\nRAG♢ 55.5 77.4 49.0 46.7 25.4 42.6 36.3 3.1 36.1 2.7 7.5\ne5 + Llama2-70B 40.2(71.2) 51.2(51.2) 48.6(48.6) 45.5(45.5) 19.2(29.7) 32.8(40.4) 27.7(36.3) 11.3(14.5) 42.8(49.7) 2.7 8.1\n3-action - 9.5 (27.7) - - 10.4 (27.9) - 28.0 (36.6) - - - -\nTable 5: Downstream performance of the 3-action chain on KILT dev set along with baselines. The figures in\nparentheses represent has_answer percentage, which corresponds to the proportion of questions with gold answers\nincluded in the final output of the LLM. ♢: Results from Petroni et al. (2021).\nFact Check. Entity Linking Slot Filling Open Domain QA Dial.\nDataset FEV AY2 WnWi WnCw T-REx zsRE NQ HoPo TQA ELI5 WoW\nModel / Metric Accuracy Exact Match RL F1\nBART-large♢ 80.7 86.6 47.9 48.0 43.8 3.0 26.2 16.9 32.5 22.7 13.8\nW-Vicuna-13B0.0(58.4) 0.1(52.2) 2.0(44.9) 0.0(48.1) 17.9(33.0) 5.9(8.5) 6.2(27.4) 1.7(17.1) 20.0(64.5) 22.7 12.7\nLlama2-13B 26.3(50.7) 34.6(47.5) 35.0(42.8) 28.5(41.3) 26.9(36.7) 7.8(9.9) 11.5(29.1) 8.3(20.3) 43.0(70.2) 27.6 13.0\nLlama2-70B 33.6(74.9) 39.8(54.5) 42.8(53.8) 39.2(55.7) 28.5(40.5) 11.3(13.6) 19.6(37.4) 13.9(25.1) 67.4(80.8) 23.0 13.3\nTable 6: Downstream performance on KILT development set in aclosed-book setting (generation without retrieval).\nFollowing Petroni et al. (2021), we report the results of typical metrics for each dataset, with bold indicating the\nbest result. The figures in parentheses represent has_answer percentage, which corresponds to the proportion of\nquestions with gold answers included in the final output of the LLM. ♢: Results from Petroni et al. (2021).\ndatasets, while our chosen LLMs have not. Despite\nthis, the LLMs demonstrate superior performance\ncompared to the baseline on several datasets.\nSpecifically, the Llama2-70B model outperforms\nthe BART baseline on the zsRE and TQA datasets,\nand the Llama2-13B model outperforms the base-\nline on the ELI5 dataset. This suggests that the\nparametric knowledge embedded in the LLMs and\ntheir capacity for text generation can be leveraged\neffectively for knowledge-intensive tasks, even\nzero-shot setting. Nevertheless, as described in\nSection 4.2, retrieval augmentation can enhance\nthe performance on downstream tasks, except the\nELI5 dataset. We also present the closed-book per-\nformances of several LLMs on the development set\nof NQ dataset in Table 7.\nA.7 Additional Results for Retrieval\nPerformance\nTable 8 presents the recall@5 of the retrievers used\nin our experiments. Note that even though m-\ne5 outperforms e5 on the MTEB Retrieval task\n(shown in Table 1), e5 still demonstrates superior\nperformance compared to m-e5 in terms of both\nR-precision (shown in Table 3) and recall@5.\nA.8 Details of Speed Analysis\nTable 9 presents the details of speed analysis on\nKILT development set. The search speed of BM25\n(without REWRITE-EL) decreases as the total\nnumber of words in a query increases. In con-\ntrast, for dense vector search, the search speed re-\nmains relatively constant regardless of the size of\nthe query due to the fixed dimensionality of the\nembedding vectors.\nAccording to Table 9, the execution times re-\nquired for generation with an LLM is longer than\nthe times required for retrieval, particularly when\ngenerating lengthy responses such as ELI5 and\nWoW. Therefore, it may seem counterintuitive that\nthe advantages of ANNS used in vector search are\nnot fully realized in terms of execution time of\nR-LLMs. However, as previously discussed in Sec-\ntion 4.4, DiskANN requires less memory compared\nto other vector search algorithms, which means that\nusing such algorithm can actually help conserve\ncomputational resources for R-LLM.\nWe observe that Llama2-13B requires more time\n63\nNQ\nModel Name EM has_answer f1 sec/Q\nLlama-2-70b-chat 19.6 37.4 36.8 2.254\nLlama-2-13b-chat 11.5 29.1 28.1 1.179\nStableBeluga2 16.2 40.9 35.5 2.858\ngpt-3.5-turbo 25.4 38.9 41.1 -\nTable 7: Accuracies on NQ dev set in a closed-book setting. For gpt-3.5-turbo (version 0613), the accuracy was\ncalculated excluding five questions out of 2,837 questions in the NQ development set that were deemed inappropriate\nprompts by OpenAI and were not processed.\nFact Check. Entity Linking Slot Filling Open Domain QA Dial.\nDataset FEV AY2 WnWi WnCw T-REx zsRE NQ HoPo TQA ELI5 WoWAvg.\nModel Recall@5\nRAG♢ 76.1 77.5 49.0 46.7 33.7 73.1 65.5 12.3 56.9 27.3 66.6 53.1\nBM25 74.2 28.8 34.7 30.6 42.7 74.7 42.5 22.8 48.7 12.3 45.1 41.6\n−REWRITE-EL 74.2 7.6 (−21.2) 3.1(−31.6) 5.9*(−24.7) 42.7 74.7 42.5 22.8 48.7 12.3 45.1 34.5(−7.1)\nm-e5 (Flat) 91.0 58.5 60.6 62.2 53.1 87.0 69.5 40.4 65.4 19.1 75.0 62.0\n−REWRITE-EL 91.0 7.8 (−50.7) 3.8(−56.8) 5.5(−56.7) 53.1 87.0 69.5 40.4 65.4 19.1 75.0 47.1(−14.9)\nm-e5 (HNSW)−REWRITE-EL63.2 4.9 3.5 2.6 26.0 48.2 55.6 14.1 48.7 14.6 66.8 31.7\ne5 (Flat) 90.6 66.1 63.3 66.7 52.1 87.2 71.6 40.9 65.421.3 75.3 63.7\n−REWRITE-EL 90.6 7.6 (−58.5) 3.4(−59.9) 4.8(−61.9) 52.1 87.2 71.6 40.9 65.4 21.3 75.3 47.3(−16.4)\ne5 (HNSW) 74.7 49.6 50.7 50.8 26.7 55.9 65.2 19.3 58.6 16.0 70.9 48.9\n−REWRITE-EL 74.7 6.0 (−43.6) 3.3(−47.4) 3.3(−47.5) 26.7 55.9 65.2 19.3 58.6 16.0 70.9 36.4(−12.5)\ne5 (DiskANN) 86.6 57.1 58.3 60.9 42.1 78.7 70.7 34.7 64.6 20.8 75.0 59.0\n−REWRITE-EL 86.6 7.4 (−49.7) 3.3(−55.0) 3.6(−57.3) 42.1 78.7 70.7 34.7 64.6 20.8 75.0 44.3(−14.7)\nTable 8: Retrieval performances (recall@5) on KILT dev set. Avg. refers to macro-average of the scores in each\ndataset. Bold indicates the best result. The figures shown in gray are copied from the column above because they do\nnot change based on the given setting. ♢: Results from Petroni et al. (2021). *: BM25 (without REWRITE-EL)\nfailed with long queries (45 out of 5,599 questions) in WnCw.\nto process each question compared to Llama2-\n70B. Upon further analysis, we discovered that\nthe Llama2-13B model occasionally produced non-\nsensical responses such as multiple newline char-\nacters (“\\n”), partially due to the limitations of our\nprompts.\nA.9 Model Information\nAs shown in Table 10, we utilize several open-\nsource models from Hugging Face, specifically\ntheir officially released versions. We load the dis-\ntributed models in 8-bit precision by default except\nLlama2-70B model (in 4-bit) using Hugging Face\nAccelerate13 library.\nA.10 Using Custom Datasets\nIn addition to utilizing KILT datasets, RALLE en-\nables developers to develop and evaluate R-LLMs\non their own QA datasets and corpora. To use the\n13https://huggingface.co/docs/\naccelerate/index\ncustom datasets with RALLE, you will need to\nperform the following preprocessing:\n• Prepare your corpus as a TSV file containing\nthe document IDs, texts, and titles.\n• Create a JSONL file for your QA dataset.\nThe format should look like this: { \"id\":\n\"\", \"input\": \"\", \"output\":\n[{\"answer\": \"\", \"provenance\":\n[{\"wikipedia_id\": \"\", \"title\":\n\"\"}]}]}, where “input” represents a question.\nSee our repo for more detailed instructions:\nhttps://github.com/yhoshi3/RaLLe.\nA.11 The Prompts used in the Evaluation\nTable 11 summarizes the prompts used in our exper-\niment. Open-book indicates retrieve-then-generate\nsetting. The queries used for retrieval are the\nraw questions without any rewriting, except for\nthe REWRITE-EL settings of AY2, WnWi, and\nWnCw.\n64\nFact Check. Entity Linking Slot Filling Open Domain QA Dial.\nTasks FEV AY2 WnWi WnCw T-REx zsRE NQ HoPo TQA ELI5 WoWAvg.\nModels Completion in Closed-Book Setting (in seconds per question)\nW-Vicuna-13B 1.565 13.040 10.870 9.793 0.983 1.142 2.165 1.969 1.414 22.820 7.122 6.626\nLlama2-13B 0.625 1.077 1.036 1.201 0.940 0.913 1.270 1.185 1.014 40.100 9.522 5.353\nLlama2-70B 1.765 2.936 2.745 2.618 1.953 2.031 2.285 2.188 1.877 42.500 11.100 6.727\nRetrieval + Generation (in seconds per question)\ne5 + W-Vicuna-13B 1.529 1.310 1.368 1.158 1.192 1.453 2.595 1.945 1.734 15.480 10.850 3.692\ne5 + Llama2-13B 1.084 1.165 1.209 1.046 1.300 1.407 1.284 1.975 9.830 32.48 16.76 6.322\nBM25 + Llama2-70B 1.841 0.008 0.009 0.008 2.015 2.296 2.206 2.344 2.249 15.020 12.010 3.637\ne5 + Llama2-70B 1.926 0.133 0.131 0.135 2.135 2.424 2.419 2.346 2.238 16.030 11.810 3.793\ne5 (top-2) + Llama2-70B1.544 0.133 0.131 0.135 1.661 1.908 1.994 1.833 1.759 15.120 10.820 3.367\ne5 (top-10) + Llama2-70B2.811 0.133 0.131 0.135 2.951 3.276 - 13.900 14.400 35.070 24.100 -\ne5 (DiskANN) + Llama2-70B1.803 0.044 0.044 0.043 2.009 2.281 2.166 2.247 2.116 15.780 11.370 3.628\ne5 + Llama2-70B (3-action) - 25.41 - - 4.993 - 16.320 - - - -\nRetrieval (in seconds per question)\nBM25 0.038 0.008 0.009 0.008 0.018 0.013 0.052 0.105 0.086 0.136 0.857 0.121\nBM25 (withoutREWRITE-EL) 0.038 5.700 4.531 5.440 0.018 0.013 0.052 0.105 0.086 0.136 0.857 1.543\nm-e5 (Flat) 0.174 0.164 0.166 0.176 0.187 0.165 0.194 0.156 0.176 0.177 0.165 0.173\nm-e5 (HNSW) 0.008 0.013 0.013 0.015 0.008 0.009 0.009 0.009 0.009 0.011 0.010 0.010\ne5 (Flat) 0.177 0.168 0.172 0.159 0.201 0.170 0.171 0.146 0.155 0.174 0.165 0.169\ne5 (HNSW) 0.008 0.008 0.008 0.008 0.008 0.008 0.008 0.008 0.008 0.008 0.009 0.008\ne5 (DiskANN) 0.018 0.020 0.038 0.020 0.020 0.030 0.020 0.021 0.019 0.019 0.021 0.022\nMean Query Length (tokens)\n11.1±4.0 357.9±149.0 331.5±113.5 505.2±31.1 7.5±2.5 7.6±2.3 9.9±2.1 19.5±6.6 17.9±8.9 21.0±10.7 86.3±58.0\nTable 9: Execution time (in seconds per question) in RALLE. Avg. refers to macro-average of the times in each\ntask. The mean query length and its standard deviation (shown as ±after the value) are also displayed, which were\ncalculated using the e5 tokenizer.\nLanguage Model\nModel Name Size max len. emb dim URL\nwizard-vicuna-13b (Lee, 2023) 13,015,864,320 2,048 - https://huggingface.co/junelee/wizard-vicuna-13b\nLlama-2-13b-chat (Touvron et al., 2023b) 13,015,864,320 4,096 -https://huggingface.co/meta-llama/Llama-2-13b-chat\nLlama-2-70b-chat (Touvron et al., 2023b) 68,976,653,312 4,096 -https://huggingface.co/meta-llama/Llama-2-70b-chat\nStableBeluga2 70B 4,096 - https://huggingface.co/stabilityai/StableBeluga2\nRetriever\nmultilingual-e5-large 559,890,946 514 1,024 https://huggingface.co/intfloat/multilingual-e5-large\ne5-large-v2 (Wang et al., 2022) 335,142,400 512 1,024 https://huggingface.co/intfloat/e5-large-v2\nTable 10: Hugging Face links of the models used in our evaluation. Size refers to the total number of effective\nparameters of each model. max len.refers to the maximum token length of model input.\nClosed-book indicates that an LLM answers to\nthe given question without retrieval. Although\nthese prompts have been our established best prac-\ntices, we recognize that there may be opportunities\nfor improvement (see also Section 5).\n65\nOpen-book Closed-book\nFEVER\nAction 1: Retriever\n{question}\nAction 2: LLM\n{response[0]} ←↩\n←↩\nAnswer IN ONE WORD if the document SUPPORTS or REFUTES\n\"{question}\". ←↩\n←↩\nAnswer:\nAction 1: LLM\nAnswer IN ONE WORD if your knowledge SUPPORTS or REFUTES\n\"{question}\". ←↩\n←↩\nAnswer:\nAY2\nAction 1: Retriever\n'What is \"'+ '{}'.format(question).split(\n'[START_ENT]')[1].split('[END_ENT]')[0][1:-1] + '\" ?'\nAction 2: Identity\n'{}'.format(wiki_id_title[0]).split('; ')[0].split(',\n')[0]\nAction 1: LLM\n'What is the most relevant Wikipedia title to the en-\ntity \"'+ '{}'.format(question).split('[START_ENT]\n')[1].split('[END_ENT]')[0] + '\" in the context of\n\"'+ '{}'.format(question).split('[START_ENT]')[0][-100:]\n+ '{}'.format(question).split('[START_ENT]')[1].split(\n'[END_ENT]')[0] + '{}'.format(question).split(\n'[END_ENT]')[1][:100] + '''...\"?\\n\\nPlease answer only\nthe Wikipedia title.\\n\\nAnswer: '''\nWnWi\nAction 1: Retriever\n'What is \"'+ '{}'.format(question).split(\n'[START_ENT]')[1].split('[END_ENT]')[0][1:-1] + '\" ?'\nAction 2: Identity\n'{}'.format(wiki_id_title[0]).split('; ')[0].split(',\n')[0]\nAction 1: LLM\n'What is the most relevant Wikipedia title to the en-\ntity \"'+ '{}'.format(question).split('[START_ENT]\n')[1].split('[END_ENT]')[0] + '\" in the context of\n\"'+ '{}'.format(question).split('[START_ENT]')[0][-100:]\n+ '{}'.format(question).split('[START_ENT]')[1].split(\n'[END_ENT]')[0] + '{}'.format(question).split(\n'[END_ENT]')[1][:100] + '''...\"?\\n\\nPlease answer only\nthe Wikipedia title.\\n\\nAnswer: '''\nWnCw\nAction 1: Retriever\n'What is \"'+ '{}'.format(question).split(\n'[START_ENT]')[1].split('[END_ENT]')[0][1:-1] + '\" ?'\nAction 2: Identity\n'{}'.format(wiki_id_title[0]).split('; ')[0].split(',\n')[0]\nAction 1: LLM\n'What is the most relevant Wikipedia title to the en-\ntity \"'+ '{}'.format(question).split('[START_ENT]\n')[1].split('[END_ENT]')[0] + '\" in the context of\n\"'+ '{}'.format(question).split('[START_ENT]')[0][-100:]\n+ '{}'.format(question).split('[START_ENT]')[1].split(\n'[END_ENT]')[0] + '{}'.format(question).split(\n'[END_ENT]')[1][:100] + '''...\"?\\n\\nPlease answer only\nthe Wikipedia title.\\n\\nAnswer: '''\nContinued on next page...\nTable 11: Prompt templates used in our experiments. The hook-left arrows ←↩refers to new line. Note that RALLE\nsupports f-strings and eval() function in Python.\n66\nTable 11 – continued from previous page.\nOpen-book Closed-book\nT-REx\nAction 1: Retriever (f-strings)\n{question}\nAction 2: LLM (eval())\n'''Referring to the following document, answer \"what\nis the '''+ '{}'.format(question).split('[SEP]')[1] +\n'of '+ '{}'.format(question).split('[SEP]')[0] + '''?\"\nin 5 words or less.\\n\\n'''+ '{}'.format(response[0]) +\n'''\\n\\n'''+ '{}'.format(question).split('[SEP]')[1] + ':\n'\nAction 1: LLM (eval())\n'What is the '+ '\"'+ '{}'.for-\nmat(question).split('[SEP] ')[1] + '\" of \"'+ '{}'.for-\nmat(question).split('[SEP]')[0] + '\"'+ '''in 5 words or\nless?\\n\\n'''+ '{}'.format(question).split('[SEP] ')[1] +\n': '\nzsRE\nAction 1: Retriever (f-strings)\n{question}\nAction 2: LLM (eval())\nReferring to the following document, answer \"{ques-\ntion}?\" in 5 words or less. ←↩\n←↩\n{response[0]} ←↩\n←↩\nAnswer:\nAction 1: LLM (eval())\n'Tell me the '+ '\"'+ '{}'.for-\nmat(question).split('[SEP] ')[1] + '\" of \"'+ '{}'.for-\nmat(question).split('[SEP]')[0] + '\"'+ '''in 5 words or\nless.\\n\\n'''+ '{}'.format(question).split('[SEP] ')[1] +\n': '\nNQ\nAction 1: Retriever\n{question}\nAction 2: LLM\nReferring to the following document, answer \"{ques-\ntion}?\" in 5 words or less. ←↩\n←↩\n{response[0]} ←↩\n←↩\nAnswer:\nAction 1: LLM\nAnswer '{question}?'in 5 words or less. ←↩\n←↩\nAnswer:\nHoPo\nAction 1: Retriever\n{question}\nAction 2: LLM\nReferring to the following document, answer \"{ques-\ntion}?\" in 5 words or less. ←↩\n←↩\n{response[0]} ←↩\n←↩\nAnswer:\nAction 1: LLM\nAnswer '{question}?'in 5 words or less. ←↩\n←↩\nAnswer:\nContinued on next page...\n67\nTable 11 – continued from previous page.\nOpen-book Closed-book\nTQA\nAction 1: Retriever\n{question}\nAction 2: LLM\nReferring to the following document, answer \"{ques-\ntion}?\" in 5 words or less. ←↩\n←↩\n{response[0]} ←↩\n←↩\nAnswer:\nAction 1: LLM\nAnswer '{question}'in 5 words or less. ←↩\n←↩\nAnswer:\nELI5\nAction 1: Retriever\n{question}\nAction 2: LLM\nReferring to the following document, answer \"{question}\".\n←↩\n←↩\n{response[0]} ←↩\n←↩\nExplain the following questions as if I were five years\nold. ←↩\n{question} ←↩\n←↩\nAnswer:\nAction 1: LLM\nExplain '{question}'as if I were five years old. ←↩\n←↩\nAnswer:\nWoW\nAction 1: Retriever\n{question}\nAction 2: LLM\nReferring to the following document, output a short and\ninformative reply to the conversation. ←↩\n←↩\n{response[0]} ←↩\n←↩\nReferring to the above document, output a short and in-\nformative reply to the following conversation. ←↩\n←↩\nThis conversation ends on your turn. ←↩\n←↩\n{question} ←↩\n←↩\nInformative and short answer: ←↩\n←↩\nAction 1: LLM\nOutput a short and informative reply to the conversation.\nThis conversation ends on your turn. ←↩\n←↩\n{question} ←↩\n←↩\nInformative and short answer:\n68\nAY2\nAction 1: Retriever\n'What is \"'+ '{}'.format(question).split('[START_ENT] ')[1].split('[END_ENT]')[0] + '\" in the context of \"'+\n'{}'.format(question).split('[START_ENT]')[0][-100:] + '{}'.format(question).split('[START_ENT]')[1].split('[END_ENT]')[0]\n+ '{}'.format(question).split('[END_ENT]')[1][:100] + '...\"?'\nAction 2: LLM\n'What is \"'+ '{}'.format(question).split('[START_ENT] ')[1].split('[END_ENT]')[0] + '\" in the context of \"'+\n'{}'.format(question).split('[START_ENT]')[0][-100:] + '{}'.format(question).split('[START_ENT]')[1].split('[END_ENT]')[0]\n+ '{}'.format(question).split('[END_ENT]')[1][:100] + '...\"?\\nAnswer in a short and conc sentence.'+ '''\\n\\nAnswer:\\n'''\nAction 3: LLM\n'Please select the most appropriate title for the word \"'+ '{}'.format(question).split('[START_ENT]\n')[1].split('[END_ENT]')[0] + '\" based on the given Description.'+ '''\\nIf none of these titles suit your needs, please\nsuggest a possible alternative title.'''+ '''\\Titles: \\n '''+ '/ '.join([titleid.split(',')[0] for titleid in '{}'.for-\nmat(wiki_id_title[0]).split('; ')]) + '''\\n\\nDescription:\\n'''+ '{}'.format(response[1]) + '''\\n\\nWikipedia Title:\\n'''\nT-REx\nAction 1: Retriever\n{question}\nAction 2: LLM\nFormulate a question that asks [SEP] in the following sentence: ←↩\n'{question}' ←↩\n←↩\nGenerated question:\nAction 3: LLM\n{response[0]} ←↩\n←↩\nReferring to the document above, answer \"{response[1]}\" in 5 words or less. ←↩\n←↩\nAnswer:\nNQ\nAction 1: Retriever\n{question}\nAction 2: LLM\nPlease rewrite the following question clearly. ←↩\n←↩\n{question}? ←↩\n←↩\nRewritten question:\nAction 3: LLM\nReferring to the following document, answer \"{response[1]}\" in 5 words or less. ←↩\n←↩\n{response[0]} ←↩\n←↩\nAnswer:\nTable 12: Prompt templates used in 3-action chains.\n69",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5942769646644592
    },
    {
      "name": "Natural language processing",
      "score": 0.5301680564880371
    },
    {
      "name": "Natural language",
      "score": 0.5207600593566895
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37726953625679016
    },
    {
      "name": "Process engineering",
      "score": 0.37122878432273865
    },
    {
      "name": "Engineering",
      "score": 0.12441450357437134
    }
  ],
  "institutions": [],
  "cited_by": 11
}