{
  "title": "baller2vec: A Multi-Entity Transformer For Multi-Agent Spatiotemporal Modeling",
  "url": "https://openalex.org/W3128337785",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5057309517",
      "name": "Michael A. Alcorn",
      "affiliations": [
        "Auburn University"
      ]
    },
    {
      "id": "https://openalex.org/A5090435705",
      "name": "Anh‐Tu Nguyen",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2895363383",
    "https://openalex.org/W2962715980",
    "https://openalex.org/W3035717782",
    "https://openalex.org/W2963062607",
    "https://openalex.org/W592244745",
    "https://openalex.org/W2584341106",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3160050461",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2949269657",
    "https://openalex.org/W2041050725",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3097237405",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W3037932933",
    "https://openalex.org/W3159012644",
    "https://openalex.org/W2131744502",
    "https://openalex.org/W3034697317",
    "https://openalex.org/W3034734814",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2950527759",
    "https://openalex.org/W2600812268",
    "https://openalex.org/W2943845043",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2786672974",
    "https://openalex.org/W2601465345",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2553756201",
    "https://openalex.org/W2157331557"
  ],
  "abstract": "Multi-agent spatiotemporal modeling is a challenging task from both an algorithmic design and computational complexity perspective. Recent work has explored the efficacy of traditional deep sequential models in this domain, but these architectures are slow and cumbersome to train, particularly as model size increases. Further, prior attempts to model interactions between agents across time have limitations, such as imposing an order on the agents, or making assumptions about their relationships. In this paper, we introduce baller2vec, a multi-entity generalization of the standard Transformer that can, with minimal assumptions, simultaneously and efficiently integrate information across entities and time. We test the effectiveness of baller2vec for multi-agent spatiotemporal modeling by training it to perform two different basketball-related tasks: (1) simultaneously modeling the trajectories of all players on the court and (2) modeling the trajectory of the ball. Not only does baller2vec learn to perform these tasks well (outperforming a graph recurrent neural network with a similar number of parameters by a wide margin), it also appears to \"understand\" the game of basketball, encoding idiosyncratic qualities of players in its embeddings, and performing basketball-relevant functions with its attention heads.",
  "full_text": "baller2vec: A Multi-Entity Transformer For\nMulti-Agent Spatiotemporal Modeling\nMichael A. Alcorn\nDepartment of Computer Science and Software Engineering\nAuburn University\nAuburn, AL 36849\nalcorma@auburn.edu\nAnh Nguyen\nDepartment of Computer Science and Software Engineering\nAuburn University\nAuburn, AL 36849\nanh.ng8@gmail.com\nAbstract\nMulti-agent spatiotemporal modeling is a challenging task from both an algorithmic\ndesign and computational complexity perspective. Recent work has explored the\nefﬁcacy of traditional deep sequential models in this domain, but these architectures\nare slow and cumbersome to train, particularly as model size increases. Further,\nprior attempts to model interactions between agents across time have limitations,\nsuch as imposing an order on the agents, or making assumptions about their relation-\nships. In this paper, we introduce baller2vec1, a multi-entity generalization of\nthe standard Transformer that can, with minimal assumptions, simultaneously and\nefﬁciently integrate information across entities and time. We test the effectiveness\nof baller2vec for multi-agent spatiotemporal modeling by training it to perform\ntwo different basketball-related tasks: (1) simultaneously modeling the trajectories\nof all players on the court and (2) modeling the trajectory of the ball. Not only does\nballer2vec learn to perform these tasks well (outperforming a graph recurrent\nneural network with a similar number of parameters by a wide margin), it also\nappears to “understand” the game of basketball, encoding idiosyncratic qualities of\nplayers in its embeddings, and performing basketball-relevant functions with its\nattention heads.\n1 Introduction\nWhether it is a defender anticipating where the point guard will make a pass in a game of basketball,\na marketing professional guessing the next trending topic on a social media platform, or a theme\npark manager forecasting the ﬂow of visitor trafﬁc, humans frequently attempt to predict phenomena\narising from processes involving multiple entities interacting through time. When designing learning\nalgorithms to perform such tasks, researchers face two main challenges:\n1. Given that entities lack a natural ordering, how do you effectively model interactions between\nentities across time?\n1All data and code for the paper are available at: https://github.com/airalcorn2/baller2vec.\nPreprint.\narXiv:2102.03291v3  [cs.LG]  28 Sep 2021\n2. How do you efﬁciently learn from the large, high-dimensional inputs inherent to such\nsequential data?\nt = 4 t = 8 t = 13\nFigure 1: After solely being trained to model the trajectory\nof the ball ( ) given the locations of the players and the\nball on the court through time, a self-attention (SA) head\nin baller2vec learned to anticipate passes. When the ball\nhandler ( ) is driving towards the basket at t = 4 , SA\nassigns near-zero weights (black) to all players, suggesting\nno passes will be made. Indeed, the ball handler did not pass\nand dribbled into the lane ( t = 8). SA then assigns a high\nweight (white) to a teammate ( ), which correctly identiﬁes\nthe recipient of the pass at t= 13.\nPrior work in athlete trajectory mod-\neling, a widely studied application\nof multi-agent spatiotemporal model-\ning (MASM; where entities are agents\nmoving through space), has attempted\nto model player interactions through\n“role-alignment” preprocessing steps\n(i.e., imposing an order on the play-\ners) [1, 2] or graph neural networks\n[3, 4], but these approaches may de-\nstroy identity information in the for-\nmer case (see Section 4.2) or limit per-\nsonalization in the latter case (see Sec-\ntion 5.1). Recently, researchers have\nexperimented with variational recur-\nrent neural networks (VRNNs) [5] to\nmodel the temporal aspects of player\ntrajectory data [ 4, 2], but the inher-\nently sequential design of this archi-\ntecture limits the size of models that can feasibly be trained in experiments.\nTransformers [6] were designed to circumvent the computational constraints imposed by other\nsequential models, and they have achieved state-of-the-art results in a wide variety of sequence\nlearning tasks, both in natural language processing (NLP), e.g., GPT-3 [7], and computer vision, e.g.,\nVision Transformers [8]. While Transformers have successfully been applied to static multi-entity\ndata, e.g., graphs [9], the only published work we are aware of that attempts to model multi-entity\nsequential data with Transformers uses four different Transformers to separately process information\ntemporally and spatially before merging the sub-Transformer outputs [10].\nIn this paper, we introduce a multi-entity Transformer that, with minimal assumptions, is capable of\nsimultaneously integrating information across agents and time, which gives it powerful representa-\ntional capabilities. We adapt the original Transformer architecture to suit multi-entity sequential data\nby converting the standard self-attention mask matrix used in NLP tasks into a novel self-attention\nmask tensor. To test the effectiveness of our multi-entity Transformer for MASM, we train it to\nperform two different basketball-related tasks (hence the name baller2vec): (1) simultaneously\nmodeling the trajectories of all players on the court (Task P) and (2) modeling the trajectory of the\nball (Task B). Further, we convert these tasks into classiﬁcation problems by binning the Euclidean\ntrajectory space, which allows baller2vec to learn complex, multimodal trajectory distributions via\nstrictly maximizing the likelihood of the data (in contrast to variational approaches, which maximize\nthe evidence lower bound and thus require priors over the latent variables). We ﬁnd that:\n1. baller2vec is an effective learning algorithm for MASM, obtaining a perplexity of 1.64 on\nTask P(compared to 15.72 when simply using the label distribution from the training set) and\n13.44 on Task B (vs. 316.05) (Section 4.1). Further, compared to a graph recurrent neural\nnetwork (GRNN) with similar capacity, baller2vec is ∼3.8 times faster and achieves a\n10.5% lower average negative log-likelihood (NLL) onTask P (Section 4.1).\n2. baller2vec demonstrably integrates information across both agents and time to achieve\nthese results, as evidenced by ablation experiments (Section 4.2).\n3. The identity embeddings learned by baller2vec capture idiosyncratic qualities of players,\nindicative of the model’s deep personalization capabilities (Section 4.3).\n4. baller2vec’s trajectory bin distributions depend on both the historical and current context\n(Section 4.4), and several attention heads appear to perform different basketball-relevant\nfunctions (Figure 1; Section 4.5), which suggests the model learned to “understand” the\nsport.\n2\n2 Methods\n2.1 Multi-entity sequences\nFigure 2: An example of a binned tra-\njectory. The agent’s starting position is\nat the center of the grid, and the cell\ncontaining the agent’s ending position\nis used as the label (of which there are\nn2 possibilities).\nLet A = {1,2,...,B }be a set indexing B entities\nand P = {p1,p2,...,p K} ⊂A be the K entities\ninvolved in a particular sequence. Further, let Zt =\n{zt,1,zt,2,...,z t,K}be an unordered set of K feature\nvectors such that zt,k is the feature vector at time step tfor\nentity pk. Z= (Z1,Z2,...,Z T ) is thus an ordered se-\nquence of sets of feature vectors over T time steps. When\nK = 1 , Zis a sequence of individual feature vectors,\nwhich is the underlying data structure for many NLP prob-\nlems.\nWe now consider two different tasks: (1) sequential entity\nlabeling, where each entity has its own label at each time\nstep (which is conceptually similar to word-level language\nmodeling), and (2) sequential labeling, where each time\nstep has a single label (see Figure 3). For (1), let V= (V1,V2,...,V T ) be a sequence of sets of\nlabels corresponding to Zsuch that Vt = {vt,1,vt,2,...,v t,K}and vt,k is the label at time step tfor\nthe entity indexed by k. For (2), let W = (w1,w2,...,w T ) be a sequence of labels corresponding\nto Zwhere wt is the label at time step t. The goal is then to learn a function f that maps a set of\nentities and their time-dependent feature vectors Zto a probability distribution over either (1) the\nentities’ time-dependent labelsVor (2) the sequence of labels W.\n2.2 Multi-agent spatiotemporal modeling\nIn the MASM setting, P is a set of K different agents and Ct =\n{(xt,1,yt,1),(xt,2,yt,2),..., (xt,K,yt,K)} is an unordered set of K coordinate pairs such\nthat (xt,k,yt,k) are the coordinates for agent pk at time step t. The ordered sequence of sets\nof coordinates C = ( C1,C2,...,C T ), together with P, thus deﬁnes the trajectories for the K\nagents over T time steps. We then deﬁne zt,k as: zt,k = g([e(pk),xt,k,yt,k,ht,k]), where g is\na multilayer perceptron (MLP), e is an agent embedding layer, and ht,k is a vector of optional\ncontextual features for agent pk at time step t. The trajectory for agent pk at time step tis deﬁned as\n(xt+1,k −xt,k,yt+1,k −yt,k). Similar to Zheng et al. [11], to fully capture the multimodal nature of\nthe trajectory distributions, we binned the 2D Euclidean space into an n×ngrid (Figure 2) and\ntreated the problem as a classiﬁcation task. Therefore, Zhas a corresponding sequence of sets of\ntrajectory labels (i.e., vt,k = Bin(∆xt,k,∆yt,k), so vt,k is an integer from one to n2), and the loss\nfor each sample in Task P is: L= ∑T\nt=1\n∑K\nk=1 −ln(f(Z)t,k[vt,k]), where f(Z)t,k[vt,k] is the\nprobability assigned to the trajectory label for agent pk at time step tby f; i.e., the loss is the NLL of\nthe data according to the model.\nFor Task B, the loss for each sample is: L = ∑T\nt=1 −ln(f(Z)t[wt]), where f(Z)t[wt] is the\nprobability assigned to the trajectory label for the ball at time step tby f, and the labels correspond\nto a binned 3D Euclidean space (i.e., wt = Bin(∆xt,∆yt,∆ζt), so wt is an integer from one to n3).\n2.3 The multi-entity Transformer\nWe now describe our multi-entity Transformer,baller2vec (Figure 3). For NLP tasks, the Trans-\nformer self-attention mask M takes the form of a T ×T matrix (Figure 4) where T is the length of\nthe sequence. The element at Mt1,t2 thus indicates whether or not the model can “look” at time step\nt2 when processing time step t1. Here, we generalize the standard Transformer to the multi-entity\nsetting by employing a T×K×T×Kmask tensor where element Mt1,k1,t2,k2 indicates whether or\nnot the model can “look” at agentpk2 at time step t2 when processing agent pk1 at time step t1. Here,\nwe mask all elements where t2 >t1 and leave all remaining elements unmasked, i.e., baller2vec\nis a “causal” model.\nIn practice, to be compatible with Transformer implementations in major deep learning li-\nbraries, we reshape M into a TK ×TK matrix (Figure 4), and the input to the Trans-\n3\nTransformer\n Transformer\nFigure 3: An overview of our multi-entity Transformer, baller2vec. Each time step tconsists\nof an unordered set Zt of entity feature vectors (colored circles) as the input, with either ( left) a\ncorresponding set Vt of entity labels (colored diamonds) or (right) a single label wt (gray triangle) as\nthe target. Matching colored circles/diamonds across time steps correspond to the same entity. In our\nexperiments, each entity feature vector zt,k is produced by an MLP gthat takes a player’s identity\nembedding e(pk), raw court coordinates (xt,k,yt,k), and a binary variable indicating the player’s\nfrontcourt ht,k as input. Each entity label vt,k is an integer indexing the trajectory bin derived from\nthe player’s raw trajectory, while eachwt is an integer indexing the ball’s trajectory bin.\nformer is a matrix with shape TK ×F where F is the dimension of each zt,k. Irie\net al. [12] observed that positional encoding [ 6] is not only unnecessary, but detrimental for\nTransformers that use a causal attention mask, so we do not use positional encoding with\nballer2vec. The remaining computations are identical to the standard Transformer (see code). 2\nFigure 4: Left: the standard self-attention mask matrix M.\nThe element at Mt1,t2 indicates whether or not the model can\n“look” at time stept2 when processing time step t1. Right:\nthe matrix form of our multi-entity self-attention mask tensor.\nIn tensor form, element Mt1,k1,t2,k2 indicates whether or\nnot the model can “look” at agent pk2 at time step t2 when\nprocessing agent pk1 at time step t1. In matrix form, this\ncorresponds to element Mt1K+k1,t2K+k2 when using zero-\nbased indexing. The M shown here is for a static, fully\nconnected graph, but other, potentially evolving network\nstructures can be encoded in the attention mask tensor.\n3 Experiments\n3.1 Dataset\nWe trainedballer2vec on a publicly\navailable dataset of player and ball tra-\njectories recorded from 631 National\nBasketball Association (NBA) games\nfrom the 2015-2016 season.3 All 30\nNBA teams and 450 different players\nwere represented. Because transition\nsequences are a strategically impor-\ntant part of basketball, unlike prior\nwork, e.g., Felsen et al. [1], Yeh et al.\n[4], Zhan et al. [2], we did not termi-\nnate sequences on a change of posses-\nsion, nor did we constrain ourselves to\na ﬁxed subset of sequences. Instead,\neach training sample was generated\non the ﬂy by ﬁrst randomly sampling\na game, and then randomly sampling a starting time from that game. The following four seconds of\ndata were downsampled to 5 Hz from the original 25 Hz and used as the input.\nBecause we did not terminate sequences on a change of possession, we could not normalize the\ndirection of the court as was done in prior work [ 1, 4, 2]. Instead, for each sampled sequence,\nwe randomly (with a probability of 0.5) rotated the court 180 ◦(because the court’s direction is\n2See also “The Illustrated Transformer” ( https://jalammar.github.io/\nillustrated-transformer/) for an introduction to the architecture.\n3https://github.com/linouk23/NBA-Player-Movements\n4\narbitrary), doubling the size of the dataset. We used a training/validation/test split of 569/30/32\ngames, respectively (i.e., 5% of the games were used for testing, and 5% of the remaining 95%\nof games were used for validation). As a result, we had access to ∼82 million different (albeit\noverlapping) training sequences (569 games ×4 periods per game ×12 minutes per period ×60\nseconds per minute ×25 Hz ×2 rotations), ∼800x the number of sequences used in prior work.\nFor both the validation and test sets, ∼1,000 different, non-overlapping sequences were selected for\nevaluation by dividing each game into ⌈1,000\nN ⌉non-overlapping chunks (where N is the number of\ngames), and using the starting four seconds from each chunk as the evaluation sequence.\n3.2 Model\nWe trained separate models forTask Pand Task B. For all experiments, we used a single Transformer\narchitecture that was nearly identical to the original model described in Vaswani et al. [6], with\ndmodel = 512 (the dimension of the input and output of each Transformer layer), eight attention heads,\ndff = 2048 (the dimension of the inner feedforward layers), and six layers, although we did not use\ndropout. For both Task P and Task B, the players and the ball were included in the input, and both\nthe players and the ball were embedded to 20-dimensional vectors. The input features for each player\nconsisted of his identity, his (x,y) coordinates on the court at each time step in the sequence, and a\nbinary variable indicating the side of his frontcourt (i.e., the direction of his team’s hoop).4 The input\nfeatures for the ball were its (x,y,ζ ) coordinates at each time step.\nThe input features for the players and the ball were processed by separate, three-layer MLPs before\nbeing fed into the Transformer. Each MLP had 128, 256, and 512 nodes in its three layers, respectively,\nand a ReLU nonlinearity following each of the ﬁrst two layers. For classiﬁcation, a single linear layer\nwas applied to the Transformer output followed by a softmax. For players, we binned an 11 ft ×11 ft\n2D Euclidean trajectory space into an 11 ×11 grid of 1 ft ×1 ft squares for a total of 121 player\ntrajectory labels. Similarly, for the ball, we binned a 19 ft ×19 ft ×19 ft 3D Euclidean trajectory\nspace into a 19 ×19 ×19 grid of 1 ft ×1 ft ×1 ft cubes for a total of 6,859 ball trajectory labels.\nWe used the Adam optimizer [13] with an initial learning rate of 10−6, β1 = 0.9, β2 = 0.999, and\nϵ= 10−9 to update the model’s parameters, of which there were∼19/23 million for Task P/Task B,\nrespectively. The learning rate was reduced to 10−7 after 20 consecutive epochs of the validation loss\nnot improving. Models were implemented in PyTorch and trained on a single NVIDIA GTX 1080 Ti\nGPU for seven days (∼650 epochs) where each epoch consisted of 20,000 training samples, and the\nvalidation set was used for early stopping.\n3.3 Baselines\nTable 1: The perplexity per trajectory bin\non the test set when using baller2vec\nvs. the marginal distribution of the tra-\njectory bins in the training set (“Train”)\nfor all predictions. baller2vec consid-\nerably reduces the uncertainty over the\ntrajectory bins.\nballer2vec Train\nTask P 1.64 15.72\nTask B 13.44 316.05\nAs our naive baseline, we used the marginal distribution of\nthe trajectory bins from the training set for all predictions.\nFor our strong baseline, we implemented a baller2vec-\nlike graph recurrent neural network (GRNN) and trained\nit on Task P (code is available in the baller2vec repos-\nitory).5 Speciﬁcally, at each time step, the player and ball\ninputs were ﬁrst processed using MLPs as inballer2vec,\nand these inputs were then fed into a graph neural net-\nwork (GNN) similar to Yeh et al. [4]. The node and\nedge functions of the GNN were each a Transformer-\nlike feedforward network (TFF), i.e., TFF(x) = LN(x+\nW2ReLU(W1x+ b1) + b2), where LN is Layer Normal-\nization [14], W1 and W2 are weight matrices, b1 and b2\nare bias vectors, and ReLU is the rectiﬁer activation function. For our RNN, we used a gated recurrent\nunit (GRU) RNN [15] in which we replaced each of the six weight matrices of the GRU with a TFF.\nEach TFF had the same dimensions as the Transformer layers used in baller2vec. Our GRNN had\n4We did not include team identity as an input variable because teams are collections of players and a coach,\nand coaches did not vary in the dataset because we only had access to half of one season of data; however, with\nadditional seasons of data, we would include the coach as an input variable.\n5We chose to implement our own strong baseline because baller2vec has far more parameters than models\nfrom prior work (e.g., ∼70x Felsen et al. [1]).\n5\n∼18M parameters, which is comparable to the ∼19M in baller2vec. We also trained our GRNN\nfor seven days (∼175 epochs).\n3.4 Ablation studies Table 2: The average NLL (lower is better) on the Task P test\nset and seconds per training epoch (SPE) for baller2vec\n(b2v) and our GRNN. baller2vec trains ∼3.8 times faster\nper epoch compared to our GRNN, and baller2vec outper-\nformed our GRNN by 10.5% when given the same amount\nof training time. Even when only allowed to train for half\n(“0.5x”) and a quarter (“0.25x”) as long as our GRNN,\nballer2vec outperformed our GRNN by 9.1% and 1.5%,\nrespectively..\nb2v b2v (0.5x) b2v (0.25x) GRNN\nNLL 0.492 0.499 0.541 0.549\nSPE ∼900 ∼900 ∼900 ∼3,400\nTo assess the impacts of the multi-\nentity design and player embeddings\nof baller2vec on model perfor-\nmance, we trained three variations\nof our Task P model using: (1) one\nplayer in the input without player\nidentity, (2) all 10 players in the\ninput without player identity, and\n(3) all 10 players in the input with\nplayer identity. In experiments where\nplayer identity was not used, a single\ngeneric player embedding was used\nin place of the player identity embeddings. We also trained two variations of our Task B model: one\nwith player identity and one without. Lastly, to determine the extent to which baller2vec uses\nhistorical information in its predictions, we compared the performance of our best Task P model on\nthe full sequence test set with its performance on the test set when only predicting the trajectories for\nthe ﬁrst frame (i.e., we applied the same model to only the ﬁrst frames of the test set).\n4 Results\n4.1 baller2vec is an effective learning algorithm for multi-agent spatiotemporal modeling.\nTable 3: The average NLL on the test set\nfor each of the models in our ablation experi-\nments (lower is better). For Task P, using all\n10 players improved model performance by\n18.0%, while using player identity improved\nmodel performance by an additional 4.4%.\nFor Task B, using player identity improved\nmodel performance by 2.7%. 1/10 indicates\nwhether one or 10 players were used as input,\nrespectively, while I/NI indicates whether or\nnot player identity was used, respectively.\nTask 1-NI 10-NI 10-I\nTask P 0.628 0.515 0.492\nTask B N/A 2.670 2.598\nThe average NLL on the test set for our best Task\nP model was 0.492, while the average NLL for our\nbest Task B model was 2.598. In NLP, model perfor-\nmance is often expressed in terms of the perplexity\nper word, which, intuitively, is the number of faces on\na fair die that has the same amount of uncertainty as\nthe model per word (i.e., a uniform distribution over\nMlabels has a perplexity ofM, so a model with a per\nword perplexity of six has the same average uncer-\ntainty as rolling a fair six-sided die). In our case, we\nconsider the perplexity per trajectory bin, deﬁned as:\nPP = e\n1\nNTK\n∑N\nn=1\n∑T\nt=1\n∑K\nk=1 −ln(p(vn,t,k)), where\nN is the number of sequences. Our best Task P\nmodel achieved a PP of 1.64, i.e., baller2vec was,\non average, as uncertain as rolling a 1.64-sided fair\ndie (better than a coin ﬂip) when predicting player\ntrajectory bins (Table 1). For comparison, when using the distribution of the player trajectory bins\nin the training set as the predicted probabilities, the PP on the test set was 15.72. Our best Task B\nmodel achieved a PP of 13.44 (compared to 316.05 when using the training set distribution).\nCompared to our GRNN, baller2vec was ∼3.8 times faster and had a 10.5% lower average NLL\nwhen given an equal amount of training time (Table 2). Even when only given half as much training\ntime as our GRNN, baller2vec had a 9.1% lower average NLL.\n4.2 baller2vec uses information about all players on the court through time, in addition to\nplayer identity, to model spatiotemporal dynamics.\nResults for our ablation experiments can be seen in Table 3. Including all 10 players in the input\ndramatically improved the performance of our Task P model by 18.0% vs. only including a single\nplayer. Including player identity improved the model’s performance a further 4.4%. This stands\nin contrast to Felsen et al. [1] where the inclusion of player identity led to slightly worse model\nperformance; a counterintuitive result given the range of skills among NBA players, but possibly\n6\nFigure 5: As can be seen in this 2D UMAP of the player embeddings, by exclusively learning to\npredict the trajectory of the ball, baller2vec was able to infer idiosyncratic player attributes. The\nleft-hand side of the plot contains tall post players ( , ), e.g., Serge Ibaka, while the right-hand\nside of the plot contains shorter shooting guards (9) and point guards (+), e.g., Stephen Curry. The\nconnecting transition region contains forwards ( , ) and other “hybrid” players, i.e., individuals\npossessing both guard and post skills, e.g., LeBron James. Further, players with similar defensive\nabilities, measured here by the cube root of the players’ blocks per minute in the 2015-2016 season\n[16], cluster together.\na side effect of their role-alignment procedure. Additionally, when replacing the players in each\ntest set sequence with random players, the performance of our best Task P model deteriorated by\n6.2% from 0.492 to 0.522. Interestingly, including player identity only improved our Task B model’s\nperformance by 2.7%. Lastly, our best Task P model’s performance on the full sequence test set\n(0.492) was 70.6% better than its performance on the single frame test set (1.67), i.e., baller2vec is\nclearly using historical information to model the spatiotemporal dynamics of basketball.\n4.3 baller2vec’s learned player embeddings encode individual attributes.\nRussell Westbrook\nDerrick Rose\nPau Gasol Kawhi Leonard\nJimmy ButlerMarc Gasol\nFigure 6: Nearest neighbors in\nballer2vec’s embedding space are\nplausible doppelgängers, such as the ex-\nplosive point guards Russell Westbrook\nand Derrick Rose, and seven-foot tall\nbrothers Pau and Marc Gasol. Images\ncredits can be found in Table S1.\nNeural language models are widely known for their\nability to encode semantic relationships between words\nand phrases as geometric relationships between embed-\ndings—see, e.g., Mikolov et al. [17, 18], Le and Mikolov\n[19], Sutskever et al. [20]. Alcorn [21] observed a similar\nphenomenon in a baseball setting, where batters and pitch-\ners with similar skills were found next to each other in the\nembedding space learned by a neural network trained to\npredict the outcome of an at-bat. A 2D UMAP [22] of the\nplayer embeddings learned by baller2vec for Task B\ncan be seen in Figure 5. Like (batter|pitcher)2vec\n[21], baller2vec seems to encode skills and physical\nattributes in its player embeddings.\nQuerying the nearest neighbors for individual players re-\nveals further insights about the baller2vec embeddings.\nFor example, the nearest neighbor for Russell Westbrook,\nan extremely athletic 6’3\" point guard, is Derrick Rose,\na 6’2\" point guard also known for his athleticism (Figure\n6). Amusingly, the nearest neighbor for Pau Gasol, a 7’1\"\ncenter with a respectable shooting range, is his younger\nbrother Marc Gasol, a 6’11\" center, also with a respectable\nshooting range.\n4.4 baller2vec’s predicted trajectory bin distributions depend on both the historical and\ncurrent context.\nBecause baller2vec explicitly models the distribution of the player trajectories (unlike variational\nmethods), we can easily visualize how its predicted trajectory bin distributions shift in different\nsituations. As can be seen in Figure 7, baller2vec’s predicted trajectory bin distributions depend\n7\non both the historical and current context. When provided with limited historical information,\nballer2vec tends to be less certain about where the players might go. baller2vec also tends to be\nmore certain when predicting trajectory bins at “easy” moments (e.g., a player moving into open space)\nvs. “hard” moments (e.g., an offensive player choosing which direction to move around a defender).\nFigure 7: baller2vec’s trajectory predicted tra-\njectory bin distributions are affected by both\nthe historical and current context. At t = 1 ,\nballer2vec is fairly uncertain about the target\nplayer’s ( ; k= 8) trajectory (left grid and dotted\nred line; the blue-bordered center cell is the “sta-\ntionary” trajectory), with most of the probability\nmass divided between trajectories moving towards\nthe ball handler’s sideline (right grid; black = 1.0;\nwhite = 0.0). After observing a portion of the\nsequence (t= 6), baller2vec becomes very cer-\ntain about the target player’s trajectory (f6,8), but\nwhen the player reaches a decision point (t= 13),\nballer2vec becomes split between trajectories\n(staying still or moving towards the top of the\nkey). Additional examples can be found in Fig-\nure S1. = ball, = offense, = defense, and\nft,k = f(Z)t,k.\n4.5 Attention heads in baller2vec appear\nto perform basketball-relevant functions.\nOne intriguing property of the attention mecha-\nnism [23–26] is how, when visualized, the atten-\ntion weights often seem to reveal how a model\nis “thinking”. For example, Vaswani et al. [6]\ndiscovered examples of attention heads in their\nTransformer that appear to be performing var-\nious language understanding subtasks, such as\nanaphora resolution. As can be seen in Figure\n8, some of the attention heads in baller2vec\nseem to be performing basketball understand-\ning subtasks, such as keeping track of the ball\nhandler’s teammates, and anticipating who the\nball handler will pass to, which, intuitively, help\nwith our task of predicting the ball’s trajectory.\n5 Related Work\n5.1 Trajectory modeling in sports\nThere is a rich literature on MASM, particularly\nin the context of sports, e.g., Kim et al. [27],\nZheng et al. [11], Le et al. [28, 29], Qi et al.\n[30], Zhan et al. [31]. Most relevant to our work\nis Yeh et al.[4], who used a variational recurrent\nneural network combined with a graph neural\nnetwork to forecast trajectories in a multi-agent setting. Like their approach, our model is permutation\nequivariant with regard to the ordering of the agents; however, we use a multi-head attention\nmechanism to achieve this permutation equivariance while the permutation equivariance in Yeh et al.\n[4] is provided by the graph neural network. Speciﬁcally, Yeh et al. [4] deﬁne: v →e : ei,j =\nfe([vi,vj,ti,j]) and e→v: oi = fv(∑\nj∈Ni[ei,j,ti]), where vi is the initial state of agent i, ti,j is\nan embedding for the edge between agents iand j, ei,j is the representation for edge (i,j), Ni is the\nneighborhood for agent i, ti is a node embedding for agent i, oi is the output state for agent i, and fe\nand fv are deep neural networks.\nAssuming each individual player is a different “type” infe (i.e., attempting to maximize the level of\npersonalization) would require 4502 = 202,500 (i.e., B2) different ti,j edge embeddings, many of\nwhich would never be used during training and thus inevitably lead to poor out-of-sample performance.\nReducing the number of type embeddings requires making assumptions about the nature of the\nrelationships between nodes. By using a multi-head attention mechanism, baller2vec learns to\nintegrate information about different agents in a highly ﬂexible manner that is both agent and time-\ndependent, and can generalize to unseen agent combinations. The attention heads in baller2vec\nare somewhat analogous to edge types, but, importantly, they do not require a priori knowledge about\nthe relationships between the players.\nAdditionally, unlike recent works that use variational methods to train their generative models [4, 1, 2],\nwe translate the multi-agent trajectory modeling problem into a classiﬁcation task, which allows us to\ntrain our model by strictly maximizing the likelihood of the data. As a result, we do not make any\nassumptions about the distributions of the trajectories nor do we need to set any priors over latent\nvariables. Zheng et al. [11] also predicted binned trajectories, but they used a recurrent convolutional\nneural network to predict the trajectory for a single player trajectory at a time at each time step.\n8\n5.2 Transformers for multi-agent spatiotemporal modeling\nGiuliari et al. [32] used a Transformer to forecast the trajectories of individual pedestrians, i.e., the\nmodel does not consider interactions between individuals. Yu et al. [10] used separate temporal and\nspatial Transformers to forecast the trajectories of multiple, interacting pedestrians. Speciﬁcally, the\ntemporal Transformer processes the coordinates of each pedestrian independently (i.e., it does not\nmodel interactions), while the spatial Transformer, which is inspired by Graph Attention Networks\n[9], processes the pedestrians independently at each time step. Sanford et al. [33] used a Transformer\nto classify on-the-ball events from sequences in soccer games; however, only the coordinates of the\nK-nearest players to the ball were included in the input (along with the ball’s coordinates). Further,\nthe order of the included players was based on their average distance from the ball for a given\ntemporal window, which can lead to speciﬁc players changing position in the input between temporal\nwindows. As far as we are aware, baller2vec is the ﬁrst Transformer capable of processing all\nagents simultaneously across time without imposing an order on the agents.\n6 Limitations\nFigure 8: The attention outputs from baller2vec\nsuggest it learned basketball-relevant functions.\nLeft: attention head 2-7 (layer-head) appears to\nfocus on teammates of the ball handler ( ). Mid-\ndle and right: attention head 6-2 seems to predict\n(middle; ) who the ball handler will be in a future\nframe (right). Players are shaded according to the\nsum of the attention weights assigned to the play-\ners through time with reference to the ball in the\ncurrent frame (recall that each player occurs mul-\ntiple times in the input). Higher attention weights\nare lighter. For both of these attention heads, the\nsum of the attention weights assigned to the ball\nthrough time was small (0.01 for both the left and\nmiddle frames where the maximum is 1.00). Ad-\nditional examples can be found in Figures S2 and\nS3.\nWhile baller2vec does not have a mechanism\nfor handling unseen players, a number of differ-\nent solutions exist depending on the data avail-\nable. For example, similar to what was proposed\nin Alcorn [21], a model could be trained to map\na vector of (e.g., NCAA) statistics and physical\nmeasurements to baller2vec embeddings. Al-\nternatively, if tracking data is available for the\nother league, a single baller2vec model could\nbe jointly trained on all the data.\nAt least two different factors may explain\nwhy including player identity as an input to\nballer2vec only led to relatively small per-\nformance improvements. First, both player and\nball trajectories are fairly generic—players tend\nto move into open space, defenders tend to move\ntowards their man or the ball, point guards tend\nto pass to their teammates, and so on. Further,\nthe location of a player on the court is often in-\ndicative of their position, and players playing\nthe same position tend to have similar skills and\nphysical attributes. As a result, we might expect baller2vec to be able to make reasonable guesses\nabout a player’s/ball’s trajectory just given the location of the players and the ball on the court.\nSecond, baller2vec may be able toinfer the identities of the players directly from the spatiotemporal\ndata. Unlike (batter|pitcher)2vec [21], which was trained on several seasons of Major League\nBaseball data, baller2vec only had access to one half of one season’s worth of NBA data for\ntraining. As a result, player identity may be entangled with season-speciﬁc factors (e.g., certain\nrosters or coaches) that are actually exogenous to the player’s intrinsic qualities, i.e.,baller2vec\nmay be overﬁtting to the season. To provide an example, the Golden State Warriors ran a very speciﬁc\nkind of offense in the 2015-2016 season—breaking the previous record for most three-pointers made\nin the regular season by 15.4%—and many basketball fans could probably recognize them from a\nbird’s eye view (i.e., without access to any identifying information). Given additional seasons of data,\nballer2vec would no longer be able to exploit the implicit identifying information contained in\nstatic lineups and coaching strategies, so including player identity in the input would likely be more\nbeneﬁcial in that case.\n7 Conclusion\nIn this paper, we introduced baller2vec, a generalization of the standard Transformer that can\nmodel sequential data consisting of multiple, unordered entities at each time step. As an architecture\n9\nthat both is computationally efﬁcient and has powerful representational capabilities, we believe\nballer2vec represents an exciting new direction for MASM. As discussed in Section 6, training\nballer2vec on more training data may allow the model to more accurately factor players away\nfrom season-speciﬁc patterns. With additional data, more contextual information about agents (e.g.,\na player’s age, injury history, or minutes played in the game) and the game (e.g., the time left in\nthe period or the score difference) could be included as input, which might allow baller2vec to\nlearn an even more complete model of the game of basketball. Although we only experimented with\nstatic, fully connected graphs here, baller2vec can easily be applied to more complex inputs—for\nexample, a sequence of graphs with changing nodes and edges—by adapting the self-attention mask\ntensor as appropriate. Lastly, as a generative model (see Alcorn and Nguyen[34] for a full derivation),\nballer2vec could be used for counterfactual simulations (e.g., assessing the impact of different\nrosters), or combined with a controller to discover optimal play designs through reinforcement\nlearning.\n8 Author Contributions\nMAA conceived and implemented the architecture, designed and ran the experiments, and wrote the\nmanuscript. AN partially funded MAA, provided the GPUs for the experiments, and commented on\nthe manuscript.\n9 Acknowledgements\nWe would like to thank Sudha Lakshmi, Katherine Silliman, Jan Van Haaren, Hans-Werner Van Wyk,\nand Eric Winsberg for their helpful suggestions on how to improve the manuscript.\nReferences\n[1] Panna Felsen, Patrick Lucey, and Sujoy Ganguly. Where will they go? predicting ﬁne-grained\nadversarial multi-agent motion using conditional variational autoencoders. In Proceedings of\nthe European Conference on Computer Vision (ECCV), pages 732–747, 2018.\n[2] Eric Zhan, Stephan Zheng, Yisong Yue, Long Sha, and Patrick Lucey. Generating multi-agent\ntrajectories using programmatic weak supervision. In International Conference on Learning\nRepresentations, 2019. URL https://openreview.net/forum?id=rkxw-hAcFQ.\n[3] Boris Ivanovic, Edward Schmerling, Karen Leung, and Marco Pavone. Generative modeling of\nmultimodal multi-human behavior. In 2018 IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS), pages 3088–3095. IEEE, 2018.\n[4] Raymond A Yeh, Alexander G Schwing, Jonathan Huang, and Kevin Murphy. Diverse genera-\ntion for multi-agent sports games. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 4610–4619, 2019.\n[5] Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron Courville, and Yoshua\nBengio. A recurrent latent variable model for sequential data. InAdvances in Neural Information\nProcessing Systems, 2015.\n[6] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-\ntion Processing Systems, pages 5998–6008, 2017.\n[7] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\nJakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\nrecognition at scale. In International Conference on Learning Representations, 2021. URL\nhttps://openreview.net/forum?id=YicbFdNTTy.\n10\n[9] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua\nBengio. Graph attention networks. In International Conference on Learning Representations,\n2018.\n[10] Cunjun Yu, Xiao Ma, Jiawei Ren, Haiyu Zhao, and Shuai Yi. Spatio-temporal graph transformer\nnetworks for pedestrian trajectory prediction. In Proceedings of the European Conference on\nComputer Vision (ECCV), August 2020.\n[11] Stephan Zheng, Yisong Yue, and Jennifer Hobbs. Generating long-term trajectories using deep\nhierarchical networks. Advances in Neural Information Processing Systems, 29:1543–1551,\n2016.\n[12] Kazuki Irie, Albert Zeyer, Ralf Schlüter, and Hermann Ney. Language modeling with deep\ntransformers. In Proc. Interspeech 2019, pages 3905–3909, 2019. doi: 10.21437/Interspeech.\n2019-2225. URL http://dx.doi.org/10.21437/Interspeech.2019-2225.\n[13] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Interna-\ntional Conference on Learning Representations, 2015.\n[14] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. In Advances in\nNeural Information Processing Systems, 2016.\n[15] Kyunghyun Cho, Bart van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder–\ndecoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pages 1724–1734, Doha, Qatar, October\n2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1179. URL https:\n//www.aclweb.org/anthology/D14-1179.\n[16] Basketball-Reference.com. 2015-16 nba player stats: Totals, February 2021. URL https:\n//www.basketball-reference.com/leagues/NBA_2016_totals.html.\n[17] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed\nrepresentations of words and phrases and their compositionality. In Advances in Neural\nInformation Processing Systems, 2013.\n[18] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word\nrepresentations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n[19] Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In\nInternational conference on machine learning, pages 1188–1196. PMLR, 2014.\n[20] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, 2014.\n[21] Michael A Alcorn. (batter|pitcher)2vec: Statistic-free talent modeling with neural player\nembeddings. In MIT Sloan Sports Analytics Conference, 2018.\n[22] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation\nand projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018.\n[23] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850, 2013.\n[24] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint\narXiv:1410.5401, 2014.\n[25] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In International\nConference on Learning Representations, 2015.\n[26] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. In International Conference on Learning Representations, 2015.\n11\n[27] Kihwan Kim, Matthias Grundmann, Ariel Shamir, Iain Matthews, Jessica Hodgins, and Irfan\nEssa. Motion ﬁelds to predict play evolution in dynamic sport scenes. In 2010 IEEE Computer\nSociety Conference on Computer Vision and Pattern Recognition, pages 840–847. IEEE, 2010.\n[28] Hoang M Le, Yisong Yue, Peter Carr, and Patrick Lucey. Coordinated multi-agent imitation\nlearning. In International Conference on Machine Learning, volume 70, pages 1995–2003,\n2017.\n[29] Hoang M Le, Peter Carr, Yisong Yue, and Patrick Lucey. Data-driven ghosting using deep\nimitation learning. In MIT Sloan Sports Analytics Conference, 2017.\n[30] Mengshi Qi, Jie Qin, Yu Wu, and Yi Yang. Imitative non-autoregressive modeling for trajectory\nforecasting and imputation. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 12736–12745, 2020.\n[31] Eric Zhan, Albert Tseng, Yisong Yue, Adith Swaminathan, and Matthew Hausknecht. Learning\ncalibratable policies using programmatic style-consistency. In International Conference on\nMachine Learning, pages 11001–11011. PMLR, 2020.\n[32] Francesco Giuliari, Irtiza Hasan, Marco Cristani, and Fabio Galasso. Transformer networks for\ntrajectory forecasting. In International Conference on Pattern Recognition, 2020.\n[33] Ryan Sanford, Siavash Gorji, Luiz G Hafemann, Bahareh Pourbabaee, and Mehrsan Javan.\nGroup activity detection from trajectory and video data in soccer. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 898–\n899, 2020.\n[34] Michael A. Alcorn and Anh Nguyen. baller2vec++: A look-ahead multi-entity transformer\nfor modeling coordinated agents. arXiv preprint arXiv:2104.11980, 2021.\n12\nSupplementary Materials\nballer2vec: A Multi-Entity Transformer For\nMulti-Agent Spatiotemporal Modeling\nFigure S1: Additional examples of predicted player trajectory bin distributions in different contexts.\nEach row contains a different sequence, and the ﬁrst column always contains the ﬁrst frame from the\nsequence.\nFigure S2: Additional examples of attention outputs for the head that focuses on the ball handler’s\nteammates.\n13\nFigure S3: Additional examples of attention outputs for the head that anticipates passes. Each column\ncontains a different sequence, and the top frame precedes the bottom frame in time.\nTable S1: Image credits for Figure 6.\nImage Source URL\nRussell Westbrook Erik Drost https://en.wikipedia.org/wiki/Russell_Westbrook#/media/File:Russell_Westbrook_shoots_against_Cavs_%28cropped%29.jpg\nPau Gasol Keith Allison https://en.wikipedia.org/wiki/Pau_Gasol#/media/File:Pau_Gasol_boxout.jpg\nKawhi Leonard Jose Garcia https://en.wikipedia.org/wiki/Kawhi_Leonard#/media/File:Kawhi_Leonard_Dunk_cropped.jpg\nDerrick Rose Keith Allison https://en.wikipedia.org/wiki/Derrick_Rose#/media/File:Derrick_Rose_2.jpg\nMarc Gasol Verse Photography https://en.wikipedia.org/wiki/Marc_Gasol#/media/File:Marc_Gasol_20131118_Clippers_v_Grizzles_%28cropped%29.jpg\nJimmy Butler Joe Gorioso https://en.wikipedia.org/wiki/Jimmy_Butler#/media/File:Jimmy_Butler_%28cropped%29.jpg\n14",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.827878475189209
    },
    {
      "name": "Basketball",
      "score": 0.7047698497772217
    },
    {
      "name": "Transformer",
      "score": 0.615874171257019
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5743026733398438
    },
    {
      "name": "Machine learning",
      "score": 0.5615811347961426
    },
    {
      "name": "Generalization",
      "score": 0.46337950229644775
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.4221893846988678
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I82497590",
      "name": "Auburn University",
      "country": "US"
    }
  ]
}