{
  "title": "Dual-Path Transformer Network: Direct Context-Aware Modeling for End-to-End Monaural Speech Separation",
  "url": "https://openalex.org/W3045904949",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2075745358",
      "name": "Chen Jing-jing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1526688087",
      "name": "Mao Qirong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095750060",
      "name": "Liu Dong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2143169494",
    "https://openalex.org/W2561557072",
    "https://openalex.org/W185399533",
    "https://openalex.org/W2952218014",
    "https://openalex.org/W2962935966",
    "https://openalex.org/W2937331865",
    "https://openalex.org/W2972853597",
    "https://openalex.org/W1555814299",
    "https://openalex.org/W2221409856",
    "https://openalex.org/W3015794161",
    "https://openalex.org/W2996969697",
    "https://openalex.org/W2981976899",
    "https://openalex.org/W2955805055",
    "https://openalex.org/W2799119527",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2460742184",
    "https://openalex.org/W2972460025",
    "https://openalex.org/W2964089206",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W3008880747",
    "https://openalex.org/W2735663686",
    "https://openalex.org/W2891405874",
    "https://openalex.org/W2980986990",
    "https://openalex.org/W2973054567",
    "https://openalex.org/W3124666641",
    "https://openalex.org/W2734774145",
    "https://openalex.org/W2558649592",
    "https://openalex.org/W80444264",
    "https://openalex.org/W2973143779",
    "https://openalex.org/W2800022361",
    "https://openalex.org/W2962715207",
    "https://openalex.org/W2902458453"
  ],
  "abstract": "The dominant speech separation models are based on complex recurrent or convolution neural network that model speech sequences indirectly conditioning on context, such as passing information through many intermediate states in recurrent neural network, leading to suboptimal separation performance. In this paper, we propose a dual-path transformer network (DPTNet) for end-to-end speech separation, which introduces direct context-awareness in the modeling for speech sequences. By introduces a improved transformer, elements in speech sequences can interact directly, which enables DPTNet can model for the speech sequences with direct context-awareness. The improved transformer in our approach learns the order information of the speech sequences without positional encodings by incorporating a recurrent neural network into the original transformer. In addition, the structure of dual paths makes our model efficient for extremely long speech sequence modeling. Extensive experiments on benchmark datasets show that our approach outperforms the current state-of-the-arts (20.6 dB SDR on the public WSj0-2mix data corpus).",
  "full_text": "Dual-Path Transformer Network: Direct Context-Aware Modeling for\nEnd-to-End Monaural Speech Separation\nJingjing Chen1, Qirong Mao1,2, Dong Liu1\n1School of Computer Science and Communication Engineering, Jiangsu University, China\n2Jiangsu Engineering Research Center of big data ubiquitous perception and intelligent agriculture\napplications, Zhenjiang, China\n2221808071@stmail.ujs.edu.cn, mao qr@ujs.edu.cn, 2111908002@stmail.ujs.edu.cn\nAbstract\nThe dominant speech separation models are based on complex\nrecurrent or convolution neural network that model speech se-\nquences indirectly conditioning on context, such as passing in-\nformation through many intermediate states in recurrent neu-\nral network, leading to suboptimal separation performance. In\nthis paper, we propose a dual-path transformer network (DPT-\nNet) for end-to-end speech separation, which introduces direct\ncontext-awareness in the modeling for speech sequences. By in-\ntroduces a improved transformer, elements in speech sequences\ncan interact directly, which enables DPTNet can model for the\nspeech sequences with direct context-awareness. The improved\ntransformer in our approach learns the order information of the\nspeech sequences without positional encodings by incorporat-\ning a recurrent neural network into the original transformer. In\naddition, the structure of dual paths makes our model efﬁcient\nfor extremely long speech sequence modeling. Extensive exper-\niments on benchmark datasets show that our approach outper-\nforms the current state-of-the-arts (20.6 dB SDR on the public\nWSj0-2mix data corpus).\nIndex Terms: direct context-aware modeling, transformer,\ndual-path network, speech separation, deep learning\n1. Introduction\nSpeech separation, often referred to as the cocktail party prob-\nlem [1, 2], is a fundamental task in signal processing with a\nwide range of real-world applications, such as separating clean\nspeech from noisy speech signals to improve the accuracy of\nautomatic speech recognition. The human auditory system has\nthe remarkable ability to extract separate sources from a com-\nplex mixture, while this task seems to be difﬁcult for automatic\ncalculation system, especially when only a monaural recording\nof mixed-speech is available.\nAlthough there are many challenges in monaural speech\nseparation, a lot of attempts have been made in previous works\nto deal with this problem over the decades. Before the deep\nlearning era, many traditional methods are introduced for this\ntask, such as non-negative matrix factorization (NMF) [3, 4],\ncomputational auditory scene analysis (CASA) [5] and proba-\nbilistic models [6]. However, these models usually only work\nfor closed-set speakers, which signiﬁcantly restricts their prac-\ntical applications. With the success of deep learning techniques\non various domains [7, 8], researches start to design data-based\nmodels to separate the mixture of unknown speakers, which\novercomes the obstacles of the traditional methods. In general,\ndeep learning techniques for monaural speech separation can\nbe divided into two categories: time-frequency (T-F) domain\nmethods and end-to-end time-domain approaches. Based on T-\nF features created by calculating the short-time Fourier trans-\nform (STFT), T-F methods separate the T-F features for each\nsource and then reconstruct the source waveforms by inverse\nSTFT [9, 10, 11, 12, 13]. They usually use the original phase of\nmixture to synthesize the estimated source waveforms, which\nretain the phase of the noisy mixture. This strategy imposes\nan upper limit on the separation performance. To overcome\nthis problem, time-domain approach is proposed in paper [14]\n, which directly model the mixture waveform using an encode-\ndecoder framework and has made great progress in recent years\n[15, 16, 17, 18, 19, 20, 21].\nHowever, the dominant speech separation models are usu-\nally based on recurrent neural network (RNN) or convolution\nneural network (CNN), which cannot model the speech se-\nquences directly conditioning on context [22], leading to subop-\ntimal separation performance. For example, RNN based mod-\nels need to pass information through many intermediate states.\nAnd the models based CNN suffer from the problem of lim-\nited receptive ﬁelds. Fortunately, the transformer based on self-\nattention mechanism can resolve this problem effectively [23],\nin which elements of the inputs can interact directly. Never-\ntheless, the transformer usually only deals with sequences with\nlength of hundreds, while end-to-end time-domain speech sep-\naration systems often model extremely long input sequences,\nwhich can sometimes be tens of thousands. Dual-path network\nis an effective method to deal with this problem [20].\nInspired by the above, we propose a dual-path transformer\nnetwork (DPTNet) for end-to-end monaural speech separa-\ntion, which introduces a improved transformer to allow direct\ncontext-aware modeling on the speech sequences, leading to su-\nperior separation performance. The major contributions of our\nwork are summarized as follows.\n1. To the best of our knowledge, this is the ﬁrst work that in-\ntroduces direct context-aware modeling into speech sep-\naration. This method enables the elements in speech se-\nquences can interact directly, which is beneﬁcial to in-\nformation transmission.\n2. We integrate a recurrent neural network into original\ntransformer to make it can learn the order information\nof the speech sequences without positional encodings.\nAnd we embed this improved transformer into a dual-\npath network, which makes our approach efﬁcient for\nextremely long speech sequence modeling.\n3. Extensive experiments on benchmark datasets show that\nour approach outperforms the current state-of-the-arts\n(20.6 dB SDR on the public WSj0-2mix data corpus).\nThe remains of this paper are organized as follows. We\nintroduces monaural speech separation with DPTNet in Section\narXiv:2007.13975v3  [eess.AS]  14 Aug 2020\nImproved \nTransformer\nRepeat B times\nIntra-transformer Inter-transformer\nSeparation Layer : Dual-Path Transformer Network\nInput\nmixture\n1-D\nConv\nEncoder\n1-D\nTrConv\nSeparated\nSources\nDecoder\nLayer Norm\nSegmentation\nImproved \nTransformer\ntanh σ\nrelu\nOverlap-Add\nMasks\nMixture\nrelu\n2-D\nConv\n1-D Conv 1-D Conv\nDual-path transformer processing\nFigure 1: Framework of speech separation with dual-path transformer network.\n2, present the experiment procedures in Section 3, analyze the\nexperiment results in Section 4, conclude this paper and indicate\nfuture work in Section 5.\n2. Speech separation with dual-path\ntransformer network\nAs depicted in Figure 1, our speech separation system consists\nof three stages: encoder, separation layer and decoder, which is\nsimilar to that of Conv-TasNet in paper [15]. First, an encoder\nis used to convert segments of the mixture waveform into cor-\nresponding features in an intermediate feature space. Then the\nfeatures are feed to the separation layer to construct a mask for\neach source. Finally, the decoder reconstructs the source wave-\nforms by converting the masked features. In the following, we\noutline the encoder and decoder, and describe the separation\nlayer, namely our dual-path transformer network, in detail.\n2.1. Encoder\nIf we denote the speech mixture by x ∈R1×T , then we can di-\nvide it into overlapping vectorsx ∈RL×I of length L samples,\nwhere I is the number of vectors. The encoder receive x and\noutput the speech signal X ∈RN×I as follows:\nX = ReLU(x ∗W) (1)\nwhere the encoder can be characterized as a ﬁlter-bank W with\nN ﬁlters of length L, which is actually a 1-D convolution mod-\nule.\n2.2. Separation layer: dual-path transformer network\nThe separation layer, namely dual-path transformer network, is\ncomposed of three stages: segmentation, dual-path transformer\nprocessing and overlap-add, which is inspired by the common\ndual-path network [20].\n2.2.1. Segmentation\nFirstly, the segmentation stage splits X into overlapped chunks\nof length K and hop size H. Then all the chunks are concate-\nnated to be a 3-D tensor D ∈RN×K×P .\n2.2.2. Dual-path transformer processing\nBroadly speaking, the transformer is composed of an encoder\nand a decoder [23]. The encoder and decoder share the same\nmodel structure, except that the decoder is a left-context-only\nversion for generation. To avoid confusions, the transformer\nin this paper refers specially to the encoder part, and it is\ncomprised of three core modules: scaled dot-product attention,\nmulti-head attention and position-wise feed-forward network.\nScaled dot-product attention is an effective self-attention\nmechanism that associate different positions of input sequences\nto calculate representations for the inputs, which is shown in\nFigure 2(a). The ﬁnal output of this module is computed as a\nweighted sum of the values, where the weight for each value\nis computed by a attention function of the query with the cor-\nresponding keys. Multi-head attention is composed of multi-\nple scaled dot-product attention modules, as depicted in Fig-\nure 2(b). First, it linearly maps the inputs h times with dif-\nferent, learnable linear projections to get parallel queries, keys\nand values respectively. Then the scaled dot-product attention is\nperformed on these mapped queries, keys and values simultane-\nously. Position-wise feed-forward network is a fully connected\nfeed-forward network. It is comprised of two linear transforma-\ntions with a ReLU activation in between. Besides the three\ncore modules, transformer also includes several residual and\nnormalization layers. We present the overall structure of the\ntransformer in Figure 3(a) and it can be formulated as follows:\nQi = ZW Q\ni , Ki = ZW K\ni , Vi = ZW V\ni i ∈[1, h] (2)\nheadi = Attention(Qi, Ki, Vi)\n= softmax(QiKT\ni√\nd\n)Vi\n(3)\nMatMul\nSoftmax\nMask(opt)\nScale\nMatMul\nQ\n K\n V\n(a) Scaled dot-product attention\nScaled Dot-Product Attention\nLinear\n Linear\n Linear\nConcat\nLinear\n(b) Multi-head attention\nh\nFigure 2: Attention mechanism of the transformer.\nMultiHead =Concat(head1,...,headh)WO (4)\nMid = LayerNorm (Z + MultiHead ) (5)\nFFN = ReLU(MidW1 + b1)W2 + b2 (6)\nOutput = LayerNorm (Mid + FFN ) (7)\nHere, Z ∈Rl×d is the input with length l and dimension d, and\nQi, Ki, Vi ∈Rl×d/h are the mapped queries, keys and values.\nWQ\ni , WK\ni , WV\ni ∈Rd×d/h and WO ∈Rd×d are parameter\nmatrices. FFN denotes the output of the position-wise feed-\nforward network, in which W1 ∈ Rd×dff , W2 ∈ Rdff ×d,\nb1 ∈Rdff , b2 ∈Rd, and dff = 4×d.\nThe elements in speech sequences modeled by the trans-\nformer can contact directly without intermediate transmis-\nsion, which introduces direct context-aware modeling into our\nmethod.\nOne thing missed by the above transformer is how to utilize\nthe order information in the speech sequences. The origin trans-\nformer adds positional encodings to the input embeddings to\nrepresent order information, which is sine-and-cosine functions\nor learned parameters. However, we ﬁnd that the positional en-\ncodings are not suitable for dual-path network and usually lead\nto model divergence in the training process. To learn the order\ninformation, we replace the ﬁrst fully connected layer with a re-\ncurrent neural network in the feed-forward network, which is a\ninteresting improvement from paper [22]:\nFFN = ReLU(RNN (Mid))W2 + b2 (8)\nWe show this improved transformer in Figure 3(b) and apply it\nin next dual-path transformer processing stage.\nIn dual-path transformer processing stage, the output D of\nthe segmentation stage is passed to a heap ofB dual-path trans-\nformers (DPTs), as presented in Figure 1. Each DPT consists\nof intra-transformer and inter-transformer, which are commit-\nted to modeling local and global information respectively. The\nintra-transformer processing block ﬁrst model the local chunk\nindependently, which acts on the second dimension of D:\nDintra\nb = IntraTransformer b[Dinter\nb−1 ]\n= [transformer (Dinter\nb−1 [:, :, i]), i= 1, ..., P]\n(9)\nThen the inter-transformer is used to summarize the information\nfrom all chunks to learn global dependency with performing on\nthe last dimension of D:\nDinter\nb = InterTransformer b[Dintra\nb ]\n= [transformer (Dintra\nb [:, j,:]), j= 1, ..., K]\n(10)\nMulti-Head\nAttention\nLayer Norm\nFeed Forward\nNetworks\nLayer Norm\nLinear\nReLU\nLinear\nMulti-Head\nAttention\nLayer Norm\nFeed Forward\nNetworks\nLayer Norm\nLinear\nReLU\nRecurrent Neural \nNetwork\n(a) Origin transformer\n (b) Improved transformer\nFigure 3: Architecture of the origin and improved transformers.\nwhere b = 1, ..., Band Dinter\n0 = D. Note that the layer nor-\nmalization in each sub-transformer is applied to all dimensions.\nIndeed, this structure makes each element in speech se-\nquences interact directly with only some elements and interact\nwith the rest elements through an intermediate element. This\nfact imposes a slight negative impact on the direct context-\naware modeling. However, the structure of dual paths allows\nour approach to model for extremely long speech sequences ef-\nﬁciently. In general, the small shortcoming caused by the dual-\npath structure is far less than the beneﬁts it brings.\n2.2.3. Overlap-Add\nThe output of the last inter-transformer Dinter\nB is used to learn\na mask for each source by a 2-D convolution layer. The masks\nare transformed back into sequences Ms ∈RN×I by overlap-\nadd, and masked encoder features for s-th source are obtained\nby the element-wise multiplication between X and Ms:\nYs = X ·Ms (11)\n2.3. Decoder\nIn decoder, a transposed convolution module is used to recon-\nstruct separated speech signals ys ∈RL×I for s-th source:\nys = Ys ∗V (12)\nwhere values in V ∈RN×L are the parameters of the trans-\nposed convolution module. Then the overlap-add method is ap-\nplied to obtain the ﬁnal waveforms ys ∈R1×T . The structure\nand function of decoder are both symmetrical with those of the\nencoder.\n3. Experiment\n3.1. Dataset\nWe evaluate our proposed model on two-speaker speech sepa-\nration using the WSJ0-2mix [9] and LS-2mix dataset [24].\nThe WSJ0-2mix dataset is derived from the WSJ0 data cor-\npus [25]. The 30 hours of training data and 10 hours of valida-\ntion data contain two-speaker mixtures generated by randomly\nselecting utterances from different speakers in the WSJ0 train-\ning set si tr s, and mixing them at random signal-to-noise ratios\n(SNR) between -5 dB and 5 dB. 5-hours test set is similarly\ngenerated using utterances from unseen speakers in WSJ0 vali-\ndation set si dt 05 and evaluation set si et 05.\nLS-2mix is created based on the Librispeech dataset [24],\nwhich is a new corpus of reading English speech. Two speakers\nare randomly selected from the train-100 set to generate training\nmixtures, at various SNRs uniformly sampled between 0 dB and\n5 dB. The validation and test set are similarly generated using\nutterances from unseen speakers in the Librispeech validation\nand test set. Generated LS-2mix dataset contains 20000, 5000\nand 3000 utterances in the train/validation/test set.\n3.2. Experiment setup\nIn encoder and decoder, the window size is 2 samples and a 50%\nstride size is used. The number of ﬁlters is set to 64. As for the\nseparation layer, the number of dual-path transformers, namely\nB, is set to 6, and h = 4parallel attention layers are employed.\nIn the training stage, we train proposed model for 100\nepochs on 4-second long segments, and the criteria for early\nstopping is no decrease in the loss function on validation set for\n10 epochs. Adam [26] is used as the optimizer and gradient clip-\nping with maximum L2-norm of 5 is applied during training.\nWe increase the learning rate linearly for the ﬁrst warmup n\ntraining steps, and then decay it by 0.98 for every two epochs:\nlrate = k1 ·d−0.5\nmodel ·n ·warmup n−1.5\nwhen n≤warmup n\n= k2 ·0.98epoch//2 when n > warmupn\n(13)\nwhere n is the step number,k1, k2 are tunable scalars, andk1 =\n0.2, k2 = 4e−4, warmup n = 4000in this paper.\nThese hyper-parameters are selected empirically according\nto the setups in the dual-path network [20] and transformer [23].\nA Pytorch implementation of our DPTNet model can be found\nat “https://github.com/ujscjj/DPTNet”.\n3.3. Training objective\nWe train proposed model with utterance-level permutation in-\nvariant training (uPIT) [12] to maximize scale-invariant source-\nto-noise ratio (SI-SNR) [14]. SI-SNR is deﬁned as:\nstarget = ⟨˜x, x⟩x\n∥x∥2 (14)\nenoise = ˜x −starget (15)\nSI −SNR := 10log10\n∥starget∥2\n∥enoise∥2 (16)\nwhere x, ˜x are clean and estimated source respectively, both of\nwhich are normalized to zero-mean before the calculation.\n4. Performance evaluation\nIn all experiments, we report the scale-invariant signal-to-noise\n(SI-SNR) and signal-to-distortion ratio (SDR) to measure the\nseparation performance of our DPTNet, both of which are often\nemployed in various speech separation systems.\nWe ﬁrst report the SI-SNR and SDR scores on the WSJ0-\n2mix dataset obtained by our model and the well-known separa-\ntion methods in recent years. As shown in Table 1, our DPTNet\nachieves 20.2 dB and 20.6 dB on the metrics of SI-SNR and\nSDR respectively, where a new state-of-the-art performance is\nachieved. Beneﬁting from the direct context-aware modeling,\nthe elements in the speech sequences modeled by our DPT-\nNet can interact directly, which results in the optimal monaural\nspeech separation performance. In addition, our model main-\ntains a small model size.\nTable 1: Comparison with other methods on WSJ0-2mix in SI-\nSNR (dB), SDR (dB) and Model Size\nMethod SI-SNR SDR Model Size\nDPCL++ [27] 10.8 - 13.6M\nuPIT-BLSTM-ST [12] - 10.0 92.7M\nDeep Attractor [10] 10.5 - -\nADANet [28] 10.4 10.8 9.1M\nGrid LSTM PIT [29] - 10.2 -\nConvLSTM-GAT [30] - 11.0 -\nChimera++ [31] 11.5 12.0 -\nW A-MISI-5 [32] 12.6 13.1 32.9M\nBLSTM-TasNet [14] 13.2 13.6 -\nConv-TasNet-gLN [15] 15.3 15.6 5.1M\nConv-TasNet+MBT [33] 15.5 15.9 -\nDeep CASA [34] 17.7 18.0 12.8M\nFurcaNeXt [35] - 18.4 51.4M\nDPRNN [20] 18.8 19.0 2.6M\nDPTNet 20.2 20.6 2.69M\nTable 2: Comparison with baselines on the LS-2mix dataset\nMethod SI-SNR SDR Model Size\nConv-TasNet-gLN [15] 12.9 13.5 5.1M\nDPRNN [20] 15.0 15.6 2.6M\nDPTNet 16.2 16.8 2.69M\nTo prove the generalization of our approach, we conduct re-\nlated experiments on the LS-2mix dataset. Compared to those\nin the WSJ0-2mix data corpus, the mixtures in LS-2mix is difﬁ-\ncult to separate, but this does not interfere with the comparison\nbetween our method and the baselines. We reproduce two clas-\nsical methods, namely Conv-TasNet [15] and DPRNN [20], as\nbaselines. Table 2 lists the average SI-SNR and SDR obtained\nby our DPTNet and the two baselines, where our direct context-\naware modeling is still signiﬁcantly superior to the state-of-the-\nart approach DPRNN. This presents the generalization of our\nmethod and further demonstrates the effectiveness of it.\n5. Conclusion and future work\nIn this paper, we propose the dual-path transformer network for\nend-to-end multi-speaker monaural speech separation, which\nmodels the speech sequences directly conditioning on con-\ntext. Our model can learn the order information in speech\nsequences without positional encodings and model effectively\nfor extremely long sequences of speech signals. Experiments\non two benchmark datasets demonstrate the effectiveness of\nproposed model, and we achieve a new state-of-the-art perfor-\nmance on the public WSJ0-2mix data corpus. In the future, we\nwould like to extend this work by directly modeling long speech\nfeature sequences without the dual-path structure. It is promis-\ning to further improve the separation performance.\n6. References\n[1] A. W. Bronkhorst, “The cocktail party phenomenon: A review\nof research on speech intelligibility in multiple-talker conditions,”\nActa Acustica united with Acustica, vol. 86, no. 1, pp. 117–128,\n2000.\n[2] S. Haykin and Z. Chen, “The cocktail party problem,” Neural\nComputation, vol. 17, no. 9, pp. 1875–1902, 2005.\n[3] M. N. Schmidt and R. K. Olsson, “Single-channel speech separa-\ntion using sparse non-negative matrix factorization,” 2006.\n[4] J. Le Roux, J. R. Hershey, and F. Weninger, “Deep nmf for speech\nseparation,” in2015 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 2015, pp. 66–70.\n[5] E. B. D. Wang, G. J. Brown, and C. Darwin, “Computational au-\nditory scene analysis: Principles, algorithms and applications,”\nAcoustical Society of America Journal, vol. 124, p. 13, 2008.\n[6] T. Virtanen, “Speech recognition using factorial hidden markov\nmodels for separation in the feature space,” inNinth International\nConference on Spoken Language Processing, 2006.\n[7] J. Gou, Z. Yi, D. Zhang, Y . Zhan, X. Shen, and L. Du, “Sparsity\nand geometry preserving graph embedding for dimensionality re-\nduction,”IEEE Access, vol. 6, pp. 75 748–75 766, 2018.\n[8] E. N. N. Ocquaye, Q. Mao, H. Song, G. Xu, and Y . Xue, “Dual\nexclusive attentive transfer for unsupervised deep convolutional\ndomain adaptation in speech emotion recognition,” IEEE Access,\nvol. 7, pp. 93 847–93 857, 2019.\n[9] J. R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe, “Deep\nclustering: Discriminative embeddings for segmentation and sep-\naration,” in 2016 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 2016, pp. 31–35.\n[10] Z. Chen, Y . Luo, and N. Mesgarani, “Deep attractor network\nfor single-microphone speaker separation,” in 2017 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2017, pp. 246–250.\n[11] D. Yu, M. Kolbæk, Z.-H. Tan, and J. Jensen, “Permutation invari-\nant training of deep models for speaker-independent multi-talker\nspeech separation,” in 2017 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP). IEEE, 2017,\npp. 241–245.\n[12] M. Kolbæk, D. Yu, Z.-H. Tan, and J. Jensen, “Multitalker speech\nseparation with utterance-level permutation invariant training of\ndeep recurrent neural networks,”IEEE/ACM Transactions on Au-\ndio, Speech, and Language Processing, vol. 25, no. 10, pp. 1901–\n1913, 2017.\n[13] G.-P. Yang, C.-I. Tuan, H.-Y . Lee, and L.-s. Lee, “Improved\nspeech separation with time-and-frequency cross-domain joint\nembedding and clustering,” inProc. Interspeech, 2019, pp. 1363–\n1367.\n[14] Y . Luo and N. Mesgarani, “Tasnet: time-domain audio separation\nnetwork for real-time, single-channel speech separation,” in 2018\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). IEEE, 2018, pp. 696–700.\n[15] Y . Luo and Mesgarani, “Conv-tasnet: Surpassing ideal time–\nfrequency magnitude masking for speech separation,”IEEE/ACM\nTransactions on Audio, Speech, and Language Processing ,\nvol. 27, no. 8, pp. 1256–1266, 2019.\n[16] Z. Shi, H. Lin, L. Liu, R. Liu, J. Han, and A. Shi, “Deep attention\ngated dilated temporal convolutional networks with intra-parallel\nconvolutional modules for end-to-end monaural speech separa-\ntion,” inProc. Interspeech, 2019, pp. 3183–3187.\n[17] Z. Shi, H. Lin, L. Liu, R. Liu, S. Hayakawa, S. Harada, and\nJ. Han, “End-to-end monaural speech separation with multi-scale\ndynamic weighted gated dilated convolutional pyramid network,”\nin Proc. Interspeech, 2019, pp. 4614–4618.\n[18] N. Takahashi, S. Parthasaarathy, N. Goswami, and Y . Mitsufuji,\n“Recursive speech separation for unknown number of speakers,”\nin Proc. Interspeech, 2019, pp. 1348–1352.\n[19] D. Ditter and T. Gerkmann, “A multi-phase gammatone ﬁl-\nterbank for speech separation via tasnet,” arXiv preprint\narXiv:1910.11615, 2019.\n[20] Y . Luo, Z. Chen, and T. Yoshioka, “Dual-path rnn: efﬁcient long\nsequence modeling for time-domain single-channel speech sepa-\nration,”arXiv preprint arXiv:1910.06379, 2019.\n[21] N. Zeghidour and D. Grangier, “Wavesplit: End-to-end\nspeech separation by speaker clustering,” arXiv preprint\narXiv:2002.08933, 2020.\n[22] M. Sperber, J. Niehues, G. Neubig, S. St ¨uker, and A. Waibel,\n“Self-attentional acoustic models,” Proc. Interspeech 2018, pp.\n3723–3727, 2018.\n[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin Advances in neural information processing systems, 2017, pp.\n5998–6008.\n[24] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-\nrispeech: an asr corpus based on public domain audio books,”\nin 2015 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP). IEEE, 2015, pp. 5206–5210.\n[25] J. Garofolo, D. Graff, D. Paul, and D. Pallett, “Csr-i (wsj0) com-\nplete ldc93s6a,” Web Download. Philadelphia: Linguistic Data\nConsortium, vol. 83, 1993.\n[26] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti-\nmization,”arXiv preprint arXiv:1412.6980, 2014.\n[27] Y . Isik, J. Le Roux, Z. Chen, S. Watanabe, and J. R. Hershey,\n“Single-channel multi-speaker separation using deep clustering,”\nInterspeech 2016, pp. 545–549, 2016.\n[28] Y . Luo, Z. Chen, and N. Mesgarani, “Speaker-independent speech\nseparation with deep attractor network,”IEEE/ACM Transactions\non Audio, Speech, and Language Processing, vol. 26, no. 4, pp.\n787–796, 2018.\n[29] C. Xu, W. Rao, X. Xiao, E. S. Chng, and H. Li, “Single channel\nspeech separation with constrained utterance level permutation in-\nvariant training using grid lstm,” in2018 IEEE International Con-\nference on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2018, pp. 6–10.\n[30] C. Li, L. Zhu, S. Xu, P. Gao, and B. Xu, “Cbldnn-based speaker-\nindependent speech separation via generative adversarial train-\ning,” in2018 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP). IEEE, 2018, pp. 711–715.\n[31] Z.-Q. Wang, J. Le Roux, and J. R. Hershey, “Alternative objective\nfunctions for deep clustering,” in 2018 IEEE International Con-\nference on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2018, pp. 686–690.\n[32] Z.-Q. Wang, J. L. Roux, D. Wang, and J. R. Hershey, “End-to-end\nspeech separation with unfolded iterative phase reconstruction,”\narXiv preprint arXiv:1804.10204, 2018.\n[33] M. W. Lam, J. Wang, D. Su, and D. Yu, “Mixup-breakdown:\na consistency training method for improving generalization of\nspeech separation models,” arXiv preprint arXiv:1910.13253,\n2019.\n[34] Y . Liu and D. Wang, “Divide and conquer: A deep casa\napproach to talker-independent monaural speaker separation,”\nIEEE/ACM Transactions on Audio, Speech, and Language Pro-\ncessing, vol. 27, no. 12, pp. 2092–2102, 2019.\n[35] L. Zhang, Z. Shi, J. Han, A. Shi, and D. Ma, “Furcanext: End-\nto-end monaural speech separation with dynamic gated dilated\ntemporal convolutional networks,” inInternational Conference on\nMultimedia Modeling. Springer, 2020, pp. 653–665.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7528790831565857
    },
    {
      "name": "Transformer",
      "score": 0.7092726826667786
    },
    {
      "name": "End-to-end principle",
      "score": 0.5703846216201782
    },
    {
      "name": "Speech recognition",
      "score": 0.5649591088294983
    },
    {
      "name": "Recurrent neural network",
      "score": 0.5035385489463806
    },
    {
      "name": "Artificial neural network",
      "score": 0.49624162912368774
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43527549505233765
    },
    {
      "name": "Engineering",
      "score": 0.11236387491226196
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I115592961",
      "name": "Jiangsu University",
      "country": "CN"
    }
  ]
}