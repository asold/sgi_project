{
  "title": "Supporting Student Decisions on Learning Recommendations: An LLM-Based Chatbot with Knowledge Graph Contextualization for Conversational Explainability and Mentoring",
  "url": "https://openalex.org/W4399258659",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2955785884",
      "name": "Hasan Abu-Rasheed",
      "affiliations": [
        "University of Siegen"
      ]
    },
    {
      "id": "https://openalex.org/A5111113757",
      "name": "Mohamad Hussam Abdulsalam",
      "affiliations": [
        "University of Siegen"
      ]
    },
    {
      "id": "https://openalex.org/A2031392306",
      "name": "Christian Weber",
      "affiliations": [
        "University of Siegen"
      ]
    },
    {
      "id": "https://openalex.org/A2126101128",
      "name": "Madjid Fathi",
      "affiliations": [
        "University of Siegen"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1970209023",
    "https://openalex.org/W4220699816",
    "https://openalex.org/W3185181255",
    "https://openalex.org/W4385632485",
    "https://openalex.org/W4287964718",
    "https://openalex.org/W4386203924",
    "https://openalex.org/W4206183212",
    "https://openalex.org/W4387162221",
    "https://openalex.org/W4235114045",
    "https://openalex.org/W4399452982"
  ],
  "abstract": "Student commitment towards a learning recommendation is not separable from their understanding of the reasons it was recommended to them; and their ability to modify it based on that understanding. Among explainability approaches, chatbots offer the potential to engage the student in a conversation, similar to a discussion with a peer or a mentor. The capabilities of chatbots, however, are still not sufficient to replace a human mentor, despite the advancements of generative AI (GenAI) and large language models (LLM). Therefore, we propose an approach to utilize chatbots as mediators of the conversation and sources of limited and controlled generation of explanations, to harvest the potential of LLMs while reducing their potential risks at the same time. The proposed LLM-based chatbot supports students in understanding learning-paths recommendations. We use a knowledge graph (KG) as a human-curated source of information, to regulate the LLM’s output through defining its prompt’s context. A group chat approach is developed to connect students with human mentors, either on demand or in cases that exceed the chatbot’s pre-defined tasks. We evaluate the chatbot with a user study, to provide a proof-of-concept and highlight the potential requirements and limitations of utilizing chatbots in conversational explainability.",
  "full_text": "Supporting Student Decisions on Learning\nRecommendations: An LLM-Based Chatbot with\nKnowledge Graph Contextualization for Conversational\nExplainability and Mentoring\nHasan Abu-Rasheed1,*, Mohamad Hussam Abdulsalam 1, Christian Weber 1 and Madjid Fathi 1\n1University of Siegen, Siegen, Germany\nAbstract\nStudent commitment towards a learning recommendation is not separable from their understanding of the reasons\nit was recommended to them; and their ability to modify it based on that understanding. Among explainability\napproaches, chatbots offer the potential to engage the student in a conversation, similar to a discussion with\na peer or a mentor. The capabilities of chatbots, however, are still not sufficient to replace a human mentor,\ndespite the advancements of generative AI (GenAI) and large language models (LLM). Therefore, we propose an\napproach to utilize chatbots as mediators of the conversation and sources of limited and controlled generation of\nexplanations, to harvest the potential of LLMs while reducing their potential risks at the same time. The proposed\nLLM-based chatbot supports students in understanding learning-paths recommendations. We use a knowledge\ngraph (KG) as a human-curated source of information, to regulate the LLM’s output through defining its prompt’s\ncontext. A group chat approach is developed to connect students with human mentors, either on demand or\nin cases that exceed the chatbot’s pre-defined tasks. We evaluate the chatbot with a user study, to provide a\nproof-of-concept and highlight the potential requirements and limitations of utilizing chatbots in conversational\nexplainability.\nKeywords\nExplainable AI (XAI), Decision support, Generative AI (GenAI), Large language models (LLM), Chatbot, Conver-\nsational explanations, Recommender systems, ChatGPT, GPT-4\n1. Introduction\nSupporting students with continuous assistance, guidance, and feedback during the learning process\nis an important concept that has strong foundations in social constructivism and Vygotsky’s Zone of\nProximal Development [1], as well as scaffolding theory [2]. The availability of an experienced peer or\na mentor is, however, one of the main challenges in online learning, especially with the vast amount\nof online learning resources. Therefore, the research on recommendation systems (RS), explainability\nand automated feedback systems has been a growing interest in the domain of technology enhanced\nlearning (TEL) and decision support in education.\nPresenting learners with explanations and additional information about the educational recommenda-\ntions they receive has shown promising results in improving their acceptance of these suggestions [3].\nLearning-recommendation explainability serves a dual purpose: clarifying the reasons behind specific\ncontent suggestions, and empowering the learner’s ability to make an informed decision about following\nthe automated suggestion. The effectiveness of this decision-making process depends significantly\non the type and amount of information provided through these explanations. Recent literature has\nexplored various forms of explanations, with a growing interest in harnessing the capabilities of LLMs\nfor conversational explanations. This involves engaging learners in a multi-step dialogue to enhance\ntheir understanding of the recommended content. While the use of chatbots in education is not new\n[4], the use of LLM-powered chatbots for generating learning explanations is still not thoroughly\nFirst international workshop on Generative AI for Learning Analytics (GenAI-LA),in the 14th International Conference on Learning\nAnalytics and Knowledge (LAK24)\n*Corresponding author.\n/envel⌢pe-⌢penhasan.abu.rasheed@uni-siegen.de (H. Abu-Rasheed)\n© 2024 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).\ninvestigated, due to the substantial limitations of LLMs in the critical field of education. Wollny et al.\nsurveyed the tasks that chatbots are used for in education. The authors found that 20% of the chatbots\nwere used in assisting tasks, while 15% of the chatbots were used for mentoring. The latter covered three\nmain methods: scaffolding, recommending, and informing. Wollny’s classification intersects with the\nnine categories found by Yan et al. for the use of LLMs in education: “profiling and labelling, detection,\nassessment and grading, teaching support, prediction, knowledge representation, feedback, content\ngeneration and recommendation. ” [5]. In their survey, the authors also investigated the practical and\nethical challenges that face LLMs in education. They point out that the technology readiness level\n(TRL) of the majority of surveyed papers did not exceed TRL-2. On the ethical front, the majority of\nthe surveyed papers did not reach a transparency tier more than Tier 1 [5] according to the three tiers\nof transparency [6], meaning that the proposed approach was only transparent to researchers and\npractitioners. A transparent approach to educational stakeholders, such as learners and teachers, is not\nreached by any of the papers the authors surveyed.\nThis concern is also shared by [7], where the authors point out several negative effects of ChatGPT\nin education, such as the lack of originality of its answers that can be meaningless and fail to motivate\nexploration or imagination by being linear or flat. The authors, however, also emphasize that the use of\nsuch technology in education still shows potential, and blocking it is not an option. Therefore, active\nresearch is ongoing to mitigate the effects of LLM’s inconsistencies, hallucinations, and bias in education.\nAmong those approaches are model fine-tuning methods and model prompt contextualization. While\nthe former approach requires labeled, domain-specific datasets, the latter approach utilizes the data\nstructures available without the need for labeling. The use of data structures to enrich the LLM’s\nprompt with additional information that reflects the context of their request is evaluated by [8], who\ninvestigate the role of KGs in enhancing the accuracy of LLMs. The authors find that asking the model\nquestions that are posted on a KG representation considerably enhanced the accuracy of the response.\nWe build on this concept, and the potential that KGs offer for modeling educational content, to support\nan LLM-based chatbot in generating more relevant explanations for a learning recommendation. Our\ncontributions in this paper can be summarized in three points: 1) we propose an approach for using\nchatbots in recommendation explainability tasks. We use the term “conversational explainability” in\nthis paper and define it as the explainability process that takes place through a bi-directional, multi-step\ninteraction between a user and the system, which happens in a certain use-case and within one context.\nWe consider this type of explanations a natural extension of the conversations between a student\nand his/her peers or mentors, and thus utilize it in this educational application. 2) We propose a\nKG-based contextualization approach with experts-in-the-loop, where the context of a GPT-4 prompt is\nconstructed from four categories of information, to enhance the model’s responses. 3) We introduce the\nrole of the chatbot as a potential mediator in a group chat that includes the student, the mentor, and\nthe chatbot. We build an infrastructure to support this use case, which can be extended to peer-group\nchatting, with the chatbot assistance.\n2. Methodology\nTo address the above-mentioned challenges, we design and implement a chatbot module as a part of a\nweb application for learning-path recommendation. The proposed module accommodates the potential\nof LLMs for providing information and explanations on learning recommendations and provides the\nstudent with a channel to connect with human mentors. Four strategies were adopted in this research\nto achieve the goal of utilizing the capabilities of LLMs and avoiding their susceptibility to errors, bias,\nand hallucinations:\n1. We limit the scope of tasks that the LLM-based chatbot is responsible for. This strategy is meant\nto use the LLM in tasks where it is less likely to generate irrelevant or wrong information. For\nexample, A user’s question about a general topic, which is not related to the recommendation, is\nnot to be answered by the chatbot in our approach.\n2. We design the dialogue to ensure that the system understands the user’s question. Re-prompting\nis utilized in this strategy to confirm the intent of the user. The chatbot will follow up a user\nquestion with a statement on how it understood the question. If the user does not confirm the\nmeaning of the question, the chatbot will request rephrasing it, ensuring that the questions lie\nwithin the supported tasks. The chatbot will suggest contacting a human if it is not able to\nunderstand the user’s question at all.\n3. We enrich the LLM prompt with thorough contextual information.The context of an LLM request is\nutilized to guide the text generation towards more relevant content for the user. A prompt’s context\nmay include information about the situation of the user, the previous state of the conversation,\nadditional descriptions of learning materials, domain terminologies, etc. We utilize the KG to\nextract detailed information on the learning materials and their relations. We also equip the\nprompt’s context with expert-defined rules regarding the output shape and limits, as well as\ninformation from the learning platform and the chat history.\n4. We provide access to mentor support within all the automated tasks. The connection to a mentor\nplays two roles in our approach: it allows group chat, in which the student, the mentor, and the\nchatbot can converse, and it acts as a fallback strategy that the chatbot suggests in the cases\nwhere it does not understand a user question, even after re-prompting.\nThe chatbot lies at the center of the proposed system architecture, see Figure 1. It acts as a focal point\nbetween the user, the LLM, and the KG database. At the core of the chatbot is a dialogue manager that\ncontrols the flow of information and applies the four strategies mentioned above. To limit the scope of\ntasks that the chatbot performs, we design an intent classifier, with a pre-defined set of action-intents\nthat are allowed in the conversation. If a user request does not belong to one of the allowed action\nclasses, the system tries to redirect the user to use one of the supported tasks. The following set of tasks\nare supported in this approach:\n• Asking about the reason behind the recommendation.\n• Asking about the content on the recommendation page in the web application.\n• Asking about the benefits that will be gained from learning a certain learning material in the\nrecommended path.\n• Asking about the relations and similarities between recommended materials and those in the KG.\n• Asking for additional information about the recommended materials.\n• Asking about the relation of recommended materials to the student’s context (e.g., their daily\nwork).\nAny query that does not belong to the previous tasks is classified as “other” and triggers the dialogue\nmanager to request a rephrasing of the query or suggest mentor support. This is accomplished through\na state-machine approach, in which we define confirmation states for user queries. The first response\nto a vague user query goes through a re-prompting state, where the chatbot requests the user to clarify\ntheir question. If this does not solve the problem in three re-prompting tries, the dialogue goes to the\nfallback state, in which the chatbot suggests a connection to a human mentor.\nThe mentoring-request state is also reachable by a direct request from the student. Mentoring request\nis initiated from any chat session. The request is then forwarded to available mentors registered in the\nsystem. Once a mentor accepts the student’s request, a new session is created by the session manager,\nin which the mentor, the student, and the chatbot are members, see Figure 2 and Figure 3.\nThe mentor and student can chat together directly. At the same time, they can ask questions to the\nchatbot in that session, using the unique identifier (@) in the question, which triggers the chatbot to\nread that question, call a limited number of previous interactions between the mentor and the student\nin the chat before the question to use it as contextual information, and then generate an answer. Figure\n2 shows the chatbot interface for the tasks of user interaction, mentoring request, and user-mentor chat\nwith the chatbot support. Image- and PDF-file upload feature is supported to utilize the multimodal\ncapabilities of LLMs like GPT-4.\nFigure 1:Chatbot system architecture within the Angular Web-App, and its connections to the KG, recommen-\ndation algorithm, and the LLM.\nFigure 2:Chatbot interface for user-chatbot and mentor request sessions.\n2.1. Prompt contextualization\nOur third strategy for designing the chatbot is based on the concept of considering the context of a\nuser query to generate a more relevant LLM response. While this concept has shown potential for\ninfluencing the output of the LLM, we do not indicate here that more context is necessarily better\nfor the LLM’s output, because it is essential to select the type and volume of contextual information\ncarefully for a positive influence on the output. For the conversational explainability task, the chatbot\nneeds sufficient information about the learning path and its individual elements. It is also important to\nensure that the LLM is not generating answers or explanations that a human educator will not accept\nor use with their students. Therefore, we enroll pedagogy experts and educators in the design phase of\nprompt contexts, see Figure 4. We survey experts and educators to extract sets of rules that the LLM\nshould follow, such as the length of the response, the type of information to be provided (or not to be\nprovided), and the definitions that the chatbot should consider when generating a response, such as the\nFigure 3:Group chat session.\nexact meanings of domain-specific terminology. The integration of domain experts in the design of our\nsystem is based on the explainability framework in [9], which describes a set of roles of experts in the\nKG- and explanation-design processes.\nIn addition to experts, the other main source of context information in our system is the KG itself. The\nKG in our case represents a human-curated data structure, which provides comprehensive information\non the learning materials and their relationships to each other. Relations among learning materials\ncan form graph communities, in which a learning material is connected to others that appear in the\nsame domain or application area [10]. For example, a course on “data analysis” can be created by a\ncomputer scientist and thus connected to other similar courses in the domain of computer science, such\nas “data visualization”, “database management”, and “Python modules for data science”. The same course\ncan be created by a health expert, and thus connected to another set of courses, such as “patient data\nprivacy”, “digital health records” and “understanding X-ray scans”. Explaining the course “data analysis”\nis considerably dependent on the context in which it appears. In the KG, course relations can reveal that\ncontext, through the other courses connected to it. We use this potential to enrich the LLM prompt’s\ncontext with information about the course connections, and the potential KG communities to which it\nbelongs. The KG as a data structure also provides information about the individual courses, the topics\nthey are composed of, and the learning materials in each topic. This includes their metadata, similarity\nscores to other materials, which relation extraction (RE) algorithms calculated, and the hierarchical\nconnections to the other taxonomical levels, which reflect their curricular format.\nRecommended courses and their learning materials are ordered in the learning path by the recom-\nmendation algorithm. The path is also a part of the contextual information that the context builder\nextracts from the RS and the web page on which the recommended path is shown. Including the web\npage is needed since our Web-App presents the recommendation in multiple ways: textual, structural,\nand visual. One of the tasks allowed in our chatbot is asking questions about those formats.\nTo build the prompt’s context from these recourses, we divide the context into four sections: the roles\nthat the chatbot plays, the definitions from the domain, the rules that are to be followed in generating\nthe explanation, and the additional content that is retrieved from the KG. This context is then added\nto the main task, or set of tasks, that the dialogue manager defines based on the user’s request. It is\nnoticeable that this arrangement of the final prompt demands a larger volume of text to be transferred\nto the LLM. This is a compromise that we find necessary to reduce the risk of generating irrelevant\noutputs, even if the output is not wrong. Depending on the LLM used, the allowed size of context may\ndiffer, limiting the amount of contextual text that can be added to the prompt.\nFigure 4:Constructing the prompt’s context from multiple information sources.\n3. Proof-of-Concept Evaluation and Results\nTo evaluate the system, we test first the performance of the proposed intent classifier. Then, we run\na user study to evaluate the features of the explainability chatbot. For the intent classification test,\nwe collect and label a set of 182 user requests, spread equally over the 7 supported intent categories.\nFigure 5 shows the confusion matrix of the classifier’s performance. The classifier has reached an\naccuracy level of 88% over all classes. Table 1 shows the values for the precision, recall, and F1 measure\ncalculated per class. From the performance scores, one can notice that the classifier mostly confuses the\nquestions about the recommendation reasoning with the questions from the third and fourth classes.\nBy analyzing user queries in the test, we notice that several queries about the benefit from a learning\nmaterial (class 3) and the relations to other learning materials (class 4) were phrased in a way that\ndemands a “justification” of the benefit or the material connections. This may explain the reason why\nthe classifier considered those as questions about the reasoning that justifies the whole recommendation\nand classified them under class 1.\nTo evaluate the chatbot features, we conduct a preliminary user study, which is intended as a proof-\nof-concept, before running a larger-scale evaluation. We design the experiment and survey the test\nparticipants to measure their:\n• satisfaction with the chatbot design.\n• satisfaction with the quality of its answers.\n• perception of the correctness of the chatbot answers based on the user intent.\n• satisfaction with the speed of responses.\n• perception of the chatbot’s responses to out-of-scope questions, i.e., in the “other” intent class.\nFor points 2-5, we design a set of eight scenarios, depicted by short stories. Six scenarios require\nthe user to validate each of the intent classes, except class 6 that covers the relation to the user’s daily\nwork. This is because the same recommendation was offered to all users, to avoid introducing a new\ndependent variable to the experiment through personalization. The seventh scenario validates the\nFigure 5:Confusion matrix for the intent classifier.\nTable 1\nPrecision, Recall, and F1 measure for each intent class\nClass ID Class description P R F1\n1 Query about the reason behind the recommendation. 0.50 1 0.67\n2 Query about the content on the recommendation page in the web application. 0.89 1 0.94\n3 Query about the benefits that will be gained from learning a certain learning material in the recommended path. 0.68 1 0.81\n4 Query about the relations and similarities between recommended materials and those in the KG. 0.86 1 0.92\n5 Query for additional information about the recommended materials. 1 0.88 0.94\n6 Query about the relation of recommended materials to the student’s context (e.g., their daily work). 1 0.77 0.87\n7 Other queries 0.89 1 0.94\nFigure 6:User evaluation of the chatbot menus, group chat, answer quality, and speed.\nchatbot answers in cases it is not able to generate a response. The eighth scenario is meant to validate\nthe mentor-chat session. Likert scales (1-5) and (1-10) were used to record the user answers.\nWe conduct this qualitative experiment with a small sample of nine participants from the target group.\nThe participants were from the academic field and included one post-doctorate, two PhD candidates,\nand six graduate students. The test duration ranged between 60 and 90 minutes, during which one of\nthe authors took the role of the mentor, due to their knowledge about the recommended path. User\nevaluation of the chatbot’s design and menus, see Figure 6, got an average of 4.7/5. The quality of\nanswers was evaluated with an average of 4.4/5. The speed of response scored an average of 4.6/5. It\nis important to mention that the speed of responses is dependent on different factors, including the\nspecifications of the servers hosting the chatbot, the speed of the KG search and information retrieval\nalgorithm, and the response time from the API. While we do not have an influence on the API’s response\nFigure 7:Average user evaluation of the chatbot responses for each of the eight scenarios.\ntime, we measure the speed of our KG search algorithm and record a response time ranging between\n0.72 and 0.9 seconds. The server used for the experiment was equipped with a CPU 13th Gen Intel(R)\nCoreTM i7-13700KF, 3.40 GHz, and 32.0 GB of RAM. The response time recorded for the complete\nprocess, from the user query to the final answer, ranged between four and eight seconds.\nFigure 7 shows the results of the eight test scenarios. It can be noticed that the scenarios that cover\ncases in which the chatbot was allowed to provide an answer received a higher score than the two\nscenarios (6 and 7), for which the chatbot should avoid providing an answer, but rather re-direct the\nuser to another conversation state, e.g., the fallback state of contacting a mentor.\n4. Discussion and Limitations\nWhile our evaluation above provides a preliminary proof-of-concept for the proposed approach, one\ncannot argue that it reflects statistically sufficient evidence that the chatbot has an influence on the\nlearning performance of a student. A larger and more thorough test process is being therefore designed\nwithin our ongoing research to test the effect of the chatbot on students, in a longer-term learning\nsetting. This follows the growing concerns, which the authors share, that the rapid development of\nGen-AI and the approaches based on it are not accompanied by the same level of real-world testing of\nthe learning effect that those approaches have in real educational settings. From our user study and the\ninvolvement of domain experts in the design and evaluation phases of our chatbot, we find evidence for\nseveral lessons learned, which we summarize in the following points:\n• The student utilization of the chatbot is greatly influenced by the way they phrase their questions.\nEven when intent classifiers have high accuracy, two different students may still use very similar\nsentences and mean different things. LLMs are one solution to support intent classification.\nHowever, they require thorough contextualization to understand the sentence’s meaning.\n• LLM outputs are mostly used exactly as the models generate them. However, using parts of\nthe output, or arranging several partial outputs in the pre-defined slots of a larger explanation\ntemplate offers educators more flexibility for controlling the final explanation content.\n• • Quantitative evaluation of LLM responses does not necessarily reflect its quality for an education\nuse case. Pedagogy experts pointed out in some of our interviews that several LLM-generated\ntexts are not wrong, but they do not offer a high value-added to students. One expert expressed\nthis idea as: “[the chatbot response] is not wrong. It is quite fine by me. But I wouldn’t give this\nanswer to my students because it doesn’t enable them to reflect. [. . . ] Reflection needs additional\ninformation, which is not simply an answer to the question. ”\n• The context added to an LLM query may be phrased in different ways, even when the same\ninformation is included. The evaluation of the phrasing’s effect is important, but it presents a\ncomplex challenge. In our system, we adjust the context phrasing for each intent, to keep the\noverall prompt objective -to the best of our ability- and to keep the LLM’s focus on the task itself,\nwithout being distracted by the contextual information that may not represent the user intent.\nThe latter point is a limitation of our study since it requires another test for the effect of context\nphrasing. Other limitations we address for our study include:\n• The limited sample size of the user study. We consider this evaluation mainly qualitative, which\nis meant to prove the concept. A larger scale test is being designed within our ongoing research\nto acquire statistically accepted results, and to focus on the real effect of using the chatbot\nexplanations in learning. For that experiment, an A/B test is designed to compare the effect of\nconversation explainability against single-step textual explanation modality.\n• In this research, we depend mainly on GPT-4. A comparison is needed to evaluate the results and\ntheir potential differences when using other LLMs.\n• Our context does not include user-profile data, which is meant to comply with the GDPR, since\nwe are using an API from a third party. This presented a limitation for the information we use in\nthe context. To solve this issue, a local LLM will be used in the next step to enrich the context\nwith user-specific data from the profiles and study the role of this type of information on the\nLLM response.\n5. Conclusion\nIn this paper, we proposed an LLM-supported chatbot approach for conversational explainability of\nlearning recommendations. We focus on harnessing the potential of a GPT-4 LLM while reducing the\nrisks it presents in education. A KG-based design of the LLM prompt’s context was proposed to enrich\nthe LLM’s prompt with thorough information about the context of the student’s query, to enhance\nits chances for generating relevant and useful output for the student. Our approach is designed to\ninvolve educators and domain experts in the design phase of the prompt context and the final shape\nof the explanation. The chatbot played a mediator role in our system, in which it not only connected\nthe LLM, the KG, and the user query, but also enabled a group chat feature, connecting the student to\na human mentor or an experienced peer, to get support in the tasks that the LLM may not perform\nwell. We evaluate the proposed approach quantitatively for validating the intent classification task,\nand qualitatively through a user study to evaluate the user perception of- and satisfaction with the\nchatbot’s features and performance. Our preliminary results show a proof-of-concept for the proposed\nconversational explainability approach and reveal important lessons learned from the design and\nimplementation phases.\nReferences\n[1] L. S. Vygotsky, Mind in Society: Development of Higher Psychological Processes, Harvard\nUniversity Press, 1978. URL: http://www.jstor.org/stable/10.2307/j.ctvjf9vz4. doi: 10.2307/j.\nctvjf9vz4.\n[2] D. Wood, J. S. Bruner, G. Ross, THE ROLE OF TUTORING IN PROBLEM SOLVING * 17 (1976) 89–\n100. URL: https://acamh.onlinelibrary.wiley.com/doi/10.1111/j.1469-7610.1976.tb00381.x. doi:10.\n1111/j.1469-7610.1976.tb00381.x.\n[3] J. Ooge, S. Kato, K. Verbert, Explaining recommendations in e-learning: Effects on adolescents’\ntrust, in: 27th International Conference on Intelligent User Interfaces, ACM, 2022, pp. 93–105.\nURL: https://dl.acm.org/doi/10.1145/3490099.3511140. doi:10.1145/3490099.3511140.\n[4] S. Wollny, J. Schneider, D. Di Mitri, J. Weidlich, M. Rittberger, H. Drachsler, Are we there yet? - a\nsystematic literature review on chatbots in education 4 (2021) 654924. URL: https://www.frontiersin.\norg/articles/10.3389/frai.2021.654924/full. doi:10.3389/frai.2021.654924.\n[5] L. Yan, L. Sha, L. Zhao, Y. Li, R. Martinez-Maldonado, G. Chen, X. Li, Y. Jin, D. Gašević,\nPractical and ethical challenges of large language models in education: A systematic scop-\ning review (2023) bjet.13370. URL: http://arxiv.org/abs/2303.13379. doi:10.1111/bjet.13370.\narXiv:2303.13379 [cs].\n[6] M. A. Chaudhry, M. Cukurova, R. Luckin, A transparency index framework for AI in edu-\ncation, in: M. M. Rodrigo, N. Matsuda, A. I. Cristea, V. Dimitrova (Eds.), Artificial Intelli-\ngence in Education. Posters and Late Breaking Results, Workshops and Tutorials, Industry and\nInnovation Tracks, Practitioners’ and Doctoral Consortium, volume 13356, Springer Interna-\ntional Publishing, 2022, pp. 195–198. URL: https://link.springer.com/10.1007/978-3-031-11647-6_33.\ndoi:10.1007/978-3-031-11647-6_33 , series Title: Lecture Notes in Computer Science.\n[7] M. Fullan, C. Azorín, A. Harris, M. Jones, Artificial intelligence and school leadership: challenges,\nopportunities and implications (2023) 1–8. URL: https://www.tandfonline.com/doi/full/10.1080/\n13632434.2023.2246856. doi:10.1080/13632434.2023.2246856.\n[8] J. Sequeda, D. Allemang, B. Jacob, A benchmark to understand the role of knowledge graphs on\nlarge language model’s accuracy for question answering on enterprise SQL databases, 2023. URL:\nhttp://arxiv.org/abs/2311.07509. arXiv:2311.07509 [cs].\n[9] H. Abu-Rasheed, C. Weber, J. Zenkert, M. Dornhöfer, M. Fathi, Transferrable framework\nbased on knowledge graphs for generating explainable results in domain-specific, intelligent\ninformation retrieval 9 (2022) 6. URL: https://www.mdpi.com/2227-9709/9/1/6. doi: 10.3390/\ninformatics9010006, number: 1 Publisher: Multidisciplinary Digital Publishing Institute.\n[10] H. Abu-Rasheed, M. Dornhöfer, C. Weber, G. Kismihók, U. Buchmann, M. Fathi, Building contextual\nknowledge graphs for personalized learning recommendations using text mining and semantic\ngraph completion, in: 2023 IEEE International Conference on Advanced Learning Technologies\n(ICALT), IEEE, 2023, pp. 36–40. URL: https://ieeexplore.ieee.org/document/10260850/. doi: 10.\n1109/ICALT58122.2023.00016.",
  "topic": "Contextualization",
  "concepts": [
    {
      "name": "Contextualization",
      "score": 0.9516133069992065
    },
    {
      "name": "Chatbot",
      "score": 0.9443033933639526
    },
    {
      "name": "Computer science",
      "score": 0.5470014214515686
    },
    {
      "name": "Graph",
      "score": 0.5023396015167236
    },
    {
      "name": "Knowledge graph",
      "score": 0.49116846919059753
    },
    {
      "name": "Psychology",
      "score": 0.3617856800556183
    },
    {
      "name": "Knowledge management",
      "score": 0.3368285298347473
    },
    {
      "name": "World Wide Web",
      "score": 0.2551681399345398
    },
    {
      "name": "Artificial intelligence",
      "score": 0.22119098901748657
    },
    {
      "name": "Programming language",
      "score": 0.06342649459838867
    },
    {
      "name": "Interpretation (philosophy)",
      "score": 0.0
    },
    {
      "name": "Theoretical computer science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I206895457",
      "name": "University of Siegen",
      "country": "DE"
    }
  ],
  "cited_by": 14
}