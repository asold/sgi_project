{
  "title": "Transformer and seq2seq model for Paraphrase Generation",
  "url": "https://openalex.org/W2984811147",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2988933584",
      "name": "Elozino Egonmwan",
      "affiliations": [
        "University of Lethbridge"
      ]
    },
    {
      "id": "https://openalex.org/A108026667",
      "name": "Yllias Chali",
      "affiliations": [
        "University of Lethbridge"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963418779",
    "https://openalex.org/W2962953307",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2964045208",
    "https://openalex.org/W2755124548",
    "https://openalex.org/W4297747548",
    "https://openalex.org/W2963558220",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2963463583",
    "https://openalex.org/W174630521",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4311469498",
    "https://openalex.org/W2145685230",
    "https://openalex.org/W2741049976",
    "https://openalex.org/W1980519283",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2889518897",
    "https://openalex.org/W2963126845",
    "https://openalex.org/W2531908596",
    "https://openalex.org/W2963658612",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2964053384",
    "https://openalex.org/W2557480356",
    "https://openalex.org/W2963847417",
    "https://openalex.org/W2167170026",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1776056560",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2729046720",
    "https://openalex.org/W2045274176",
    "https://openalex.org/W2962985882",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2804292122"
  ],
  "abstract": "Paraphrase generation aims to improve the clarity of a sentence by using different wording that convey similar meaning. For better quality of generated paraphrases, we propose a framework that combines the effectiveness of two models – transformer and sequence-to-sequence (seq2seq). We design a two-layer stack of encoders. The first layer is a transformer model containing 6 stacked identical layers with multi-head self attention, while the second-layer is a seq2seq model with gated recurrent units (GRU-RNN). The transformer encoder layer learns to capture long-term dependencies, together with syntactic and semantic properties of the input sentence. This rich vector representation learned by the transformer serves as input to the GRU-RNN encoder responsible for producing the state vector for decoding. Experimental results on two datasets-QUORA and MSCOCO using our framework, produces a new benchmark for paraphrase generation.",
  "full_text": "Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019), pages 249–255\nHong Kong, China, November 4, 2019.c⃝2019 Association for Computational Linguistics\nwww.aclweb.org/anthology/D19-56%2d\n249\nTransformer and seq2seq model for Paraphrase Generation\nElozino Egonmwan and Yllias Chali\nUniversity of Lethbridge\nLethbridge, AB, Canada\n{elozino.egonmwan, yllias.chali}@uleth.ca\nAbstract\nParaphrase generation aims to improve the\nclarity of a sentence by using different word-\ning that convey similar meaning. For better\nquality of generated paraphrases, we propose\na framework that combines the effectiveness\nof two models – transformer and sequence-to-\nsequence (seq2seq). We design a two-layer\nstack of encoders. The ﬁrst layer is a trans-\nformer model containing 6 stacked identical\nlayers with multi-head self-attention, while the\nsecond-layer is a seq2seq model with gated re-\ncurrent units (GRU -RNN ). The transformer en-\ncoder layer learns to capture long-term depen-\ndencies, together with syntactic and semantic\nproperties of the input sentence. This rich vec-\ntor representation learned by the transformer\nserves as input to the GRU -RNN encoder re-\nsponsible for producing the state vector for de-\ncoding. Experimental results on two datasets-\nQUORA and MSCOCO using our framework,\nproduces a new benchmark for paraphrase\ngeneration.\n1 Introduction\nParaphrasing is a key abstraction technique used\nin Natural Language Processing (NLP ). While ca-\npable of generating novel words, it also learns\nto compress or remove unnecessary words along\nthe way. Thus, gainfully lending itself to ab-\nstractive summarization (Chen and Bansal, 2018;\nGehrmann et al., 2018) and question generation\n(Song et al., 2018) for machine reading compre-\nhension ( MRC ) (Dong et al., 2017). Paraphrases\ncan also be used as simpler alternatives to input\nsentences for machine translation ( MT) (Callison-\nBurch et al., 2006) as well as evaluation of nat-\nural language generation ( NLG ) texts (Apidianaki\net al., 2018).\nExisting methods for generating paraphrases,\nfall in one of these broad categories – rule-based\n(McKeown, 1983), seq2seq (Prakash et al., 2016),\nreinforcement learning (Li et al., 2018), deep gen-\nerative models (Iyyer et al., 2018) and a varied\ncombination (Gupta et al., 2018; Mallinson et al.,\n2017) of the later three.\nIn this paper, we propose a novel framework\nfor paraphrase generation that utilizes the trans-\nformer model of Vaswani et al. (2017) and seq2seq\nmodel of Sutskever et al. (2014) speciﬁcally GRU\n(Cho et al., 2014). The multi-head self attention of\nthe transformer complements the seq2seq model\nwith its ability to learn long-range dependencies in\nthe input sequence. Also the individual attention\nheads in the transformer model mimics behavior\nrelated to the syntactic and semantic structure of\nthe sentence (Vaswani et al., 2017, 2018) which is\nkey in paraphrase generation. Furthermore, we use\nGRU to obtain a ﬁxed-size state vector for decod-\ning into variable length sequences, given the more\nqualitative learned vector representations from the\ntransformer.\nThe main contributions of this work are:\n•We propose a novel framework for the task of\nparaphrase generation that produces quality\nparaphrases of its source sentence.\n•For in-depth analysis of our results, in addi-\ntion to using BLEU (Papineni et al., 2002) and\nROUGE (Lin, 2004) which are word-overlap\nbased, we further evaluate our model us-\ning qualitative metrics such as Embedding\nAverage Cosine Similarity ( EACS ), Greedy\nMatching Score ( GMS ) from Sharma et al.\n(2017) and METEOR (Banerjee and Lavie,\n2005), with stronger correlation with human\nreference.\n2 Task Deﬁnition\nGiven an input sentence S = (s1,...,s n) with n\nwords, the task is to generate an alternative output\n250\nS: What are the dumbest questions ever asked\non Quora?\nG: what is the stupidest question on quora?\nR: What is the most stupid question asked on\nQuora?\nS: How can I lose fat without doing any aero-\nbic physical activity\nG: how can i lose weight without exercise?\nR: How can I lose weight in a month without\ndoing exercise?\nS: How did Donald Trump won the 2016 USA\npresidential election?\nG: how did donald trump win the 2016 presi-\ndential\nR: How did Donald Trump become presi-\ndent?\nTable 1: Examples of our generated paraphrases on\nthe QUORA sampled test set, where S, G, R repre-\nsents Source, Generated and Reference sentences re-\nspectively.\nsentence Y = (y1,...,y m) | ∃ym ̸∈ S with m\nwords that conveys similar semantics as S, where\npreferably, m<n but not necessarily.\n3 Method\nIn this section, we present our framework for para-\nphrase generation. It follows the popular encode-\ndecode paradigm, but with two stacked layers of\nencoders. The ﬁrst encoding layer is a trans-\nformer encoder, while the second encoding layer\nis a GRU -RNN encoder. The paraphrase of a given\nsentence is generated by a GRU -RNN decoder.\n3.1 Stacked Encoders\n3.1.1 Encoder – TRANSFORMER\nWe use the transformer-encoder as sort of a pre-\ntraining module of our input sentence. The goal\nis to learn richer representation of the input vector\nthat better handles long-term dependencies as well\nas captures syntactic and semantic properties be-\nfore obtaining a ﬁxed-state representation for de-\ncoding into the desired output sentence. The trans-\nformer contains 6 stacked identical layers mainly\ndriven by self-attention implemented by Vaswani\net al. (2017, 2018).\n3.1.2 Encoder – GRU -RNN\nOur architecture uses a single layer uni-directional\nGRU -RNN whose input is the output of the trans-\nS: Three dimensional rendering of a kitchen area\nwith various appliances.\nG: a series of photographs of a kitchen\nR: A series of photographs of a tiny model\nkitchen\nS: a young boy in a soccer uniform kicking a ball\nG: a young boy kicking a soccer ball\nR: A young boy kicking a soccer ball on a green\nﬁeld.\nS: The dog is wearing a Santa Claus hat.\nG: a dog poses with santa hat\nR: A dog poses while wearing a santa hat.\nS: the people are sampling wine at a wine tasting.\nG: a group of people wine tasting.\nR: Group of people tasting wine next to some\nbarrels.\nTable 2: Examples of our generated paraphrases on\nthe MSCOCO sampled test set, where S, G, R repre-\nsents Source, Generated and Reference sentences re-\nspectively.\nformer. The GRU -RNN encoder (Chung et al.,\n2014; Cho et al., 2014) produces ﬁxed-state vector\nrepresentation of the transformed input sequence\nusing the following equations:\nz= σ(xtUz + st−1Wz) (1)\nr= σ(xtUr + st−1Wr) (2)\nh= tanh(xtUh + (st−1 ⊙r)Wh) (3)\nst = (1−z) ⊙h+ z⊙st−1 (4)\nwhere rand zare the reset and update gates re-\nspectively,W and Uare the network’s parameters,\nst is the hidden state vector at timestep t, xt is the\ninput vector and ⊙represents the Hadamard prod-\nuct.\n3.2 Decoder – GRU -RNN\nThe ﬁxed-state vector representation produced by\nthe GRU -RNN encoder is used as initial state for\nthe decoder. At each time step, the decoder re-\nceives the previously generated word, yt−1 and\nhidden state st−1 at time step t−1. The output\nword, yt at each time step, is a softmax probability\nof the vector in equation 3 over the set of vocabu-\nlary words, V.\n251\n50K\nMODEL BLEU METEOR R -L EACS GMS\nVAE-SVG -EQ (Gupta et al., 2018) 17.4 22.2 - - -\nRbM-SL (Li et al., 2018) 35.81 28.12 - - -\nTRANS (ours) 35.56 33.89 27.53 79.72 62.91\nSEQ (ours) 34.88 32.10 29.91 78.66 61.45\nTRAN SEQ (ours) 37.06 33.73 30.89 80.81 63.63\nTRAN SEQ + beam (size=6) (ours) 37.12 33.68 30.72 81.03 63.50\n100 K\nMODEL BLEU METEOR R -L EACS GMS\nVAE-SVG -EQ (Gupta et al., 2018) 22.90 25.50 - - -\nRbM-SL (Li et al., 2018) 43.54 32.84 - - -\nTRANS (ours) 37.46 36.04 29.73 80.61 64.81\nSEQ (ours) 36.98 34.71 32.06 79.65 63.49\nTRAN SEQ (ours) 38.75 35.84 33.23 81.50 65.52\nTRAN SEQ + beam (size=6) (ours) 38.77 35.86 33.07 81.64 65.42\n150 K\nMODEL BLEU METEOR R -L EACS GMS\nVAE-SVG -EQ (Gupta et al., 2018) 38.30 33.60 - - -\nTRANS (ours) 39.00 38.68 32.05 81.90 65.27\nSEQ (ours) 38.50 36.89 34.35 80.95 64.13\nTRAN SEQ (ours) 40.36 38.49 35.84 82.84 65.99\nTRAN SEQ + beam (size=6) (ours) 39.82 38.48 35.40 82.48 65.54\nTable 3: Performance of our model against various models on the QUORA dataset with 50k,100k,150k training\nexamples. R-L refers to the ROUGE-L F1 score with 95% conﬁdence interval\n4 Experiments\nWe describe baselines, our implementation set-\ntings, datasets and evaluation of our proposed\nmodel.\n4.1 Baselines\nWe compare our model with very recent models\n(Gupta et al., 2018; Li et al., 2018; Prakash et al.,\n2016) including the current state-of-the-art (Gupta\net al., 2018) in the ﬁeld. To further highlight the\ngain of stacking 2 encoders we use each compo-\nnent – Transformer (T RANS ) and seq2seq (S EQ)\nas baselines.\n•VAE-SVG -EQ (Gupta et al., 2018): This is\nthe current state-of-the-art in the ﬁeld, with\na variational autoencoder as its main compo-\nnent.\n•RbM-SL (Li et al., 2018): Different from\nthe encoder-decoder framework, this is\na generator-evaluator framework, with the\nevaluator trained by reinforcement learning.\n•Residual LSTM (Prakash et al., 2016): This\nimplements stacked residual long short term\nmemory networks (LSTM ).\n•TRANS : Encoder-decoder framework as de-\nscribed in Section 3 but with a single trans-\nformer encoder layer.\n•SEQ: Encoder-decoder framework as de-\nscribed in Section 3 but with a single GRU -\nRNN encoder layer.\n4.2 Implementation\nWe used pre-trained 300-dimensional gloVe1\nword-embeddings (Pennington et al., 2014) as the\ndistributed representation of our input sentences.\nWe set the maximum sentence length to 15 and 10\nrespectively for our input and target sentences fol-\nlowing the statistics of our dataset.\nFor the transformer encoder, we used the\ntransformer base hyperparameter setting from\n1https://nlp.stanford.edu/projects/\nglove/\n252\nMODEL BLEU METEOR R -L EACS GMS\nResidual LSTM (Prakash et al., 2016) 37.0 27.0 - - -\nVAE-SVG -EQ (Gupta et al., 2018) 41.7 31.0 - - -\nTRANS (ours) 41.8 38.5 33.4 79.6 70.3\nSEQ (ours) 40.7 36.9 35.8 78.9 70.0\nTRAN SEQ (ours) 43.4 38.3 37.4 80.5 71.1\nTRAN SEQ + beam (size=10) (ours) 44.5 40.0 38.4 81.9 71.3\nTable 4: Performance of our model against various models on the MSCOCO dataset. R-L refers to the ROUGE-L\nF1 score with 95% conﬁdence interval\nthe tensor2tensor library (Vaswani et al., 2018) 2,\nbut set the hidden size to 300. We set dropout to\n0.0 and 0.7 for MSCOCO and QUORA datasets re-\nspectively. We used a large dropout for QUORA\nbecause the model tends to over-ﬁt to the training\nset. Both the GRU -RNN encoder and decoder con-\ntain 300 hidden units.\nWe pre-process our datasets, and do not use the\npre-processed/tokenized versions of the datasets\nfrom tensor2tensor library. Our target vocabulary\nis a set of approximately 15,000 words. It con-\ntains words in our target training and test sets that\noccur at least twice. Using this subset of vocabu-\nlary words as opposed to over 320,000 vocabulary\nwords contained in gloVe improves both training\ntime and performance of the model.\nWe train and evaluate our model after each\nepoch with a ﬁxed learning rate of 0.0005, and\nstop training when the validation loss does not\ndecrease after 5 epochs. The model learns\nto minimize the seq2seq loss implemented in\ntensorflow API3 with AdamOptimizer.\nWe use greedy-decoding during training and vali-\ndation and set the maximum number of iterations\nto 5 times the target sentence length. For test-\ning/inference we use beam-search decoding.\n4.3 Datasets\nWe evaluate our model on two standard datasets\nfor paraphrase generation – QUORA 4 and\nMSCOCO (Lin et al., 2014) as described in Gupta\net al. (2018) and used similar settings. The\nQUORA dataset contains over 120k examples\nwith a 80k and 40k split on the training and\ntest sets respectively. As seen in Tables 1 and\n2https://github.com/tensorflow/\ntensor2tensor\n3https://www.tensorflow.org/api_docs/\npython/tf/contrib/seq2seq/sequence_loss\n4https://data.quora.com/\nFirst-Quora-Dataset-Release-Question-Pairs\n2, while the QUORA dataset contains question\npairs, MSCOCO contains free form texts which\nare human annotations of images. Subjective\nobservation of the MSCOCO dataset reveals that\nmost of its paraphrase pairs contain more novel\nwords as well as syntactic manipulations than\nthe QUORA pairs making it a more interesting\nparaphrase generation corpora. We split the\nQUORA dataset to 50k, 100k and 150k training\nsamples and 4k testing samples in order to align\nwith baseline models for comparative purposes.\n4.4 Evaluation\nFor quantitative analysis of our model, we use\npopular automatic metrics such as BLEU , ROUGE ,\nMETEOR . Since BLEU and ROUGE both mea-\nsure n −gram word-overlap with difference in\nbrevity penalty, we report just theROUGE -L value.\nWe also use 2 additional recent metrics – GMS\nand EACS by (Sharma et al., 2017) 5 that measure\nthe similarity between the reference and generated\nparaphrases based on the cosine similarity of their\nembeddings on word and sentence levels respec-\ntively.\n4.5 Result Analysis\nTables 3 and 4 report scores of our model on both\ndatasets. Our model pushes the benchmark on all\nevaluation metrics compared against current pub-\nlished top models evaluated on the same datasets.\nSince several words could connote similar mean-\ning, it is more logical to evaluate with metrics that\nmatch with embedding vectors capable of measur-\ning this similarity. Hence we also report GMS and\nEACS scores as a basis of comparison for future\nwork in this direction.\nBesides quantitative values, Tables 1 and 2\nshow that our paraphrases are well formed, ab-\nstractive (e.g dumbest – stupidest, dog is wearing\n5https://github.com/Maluuba/nlg-eval\n253\n– dog poses), capable of performing syntactic ma-\nnipulations (e.g in a soccer uniform kicking a ball\n– kicking a soccer ball) and compression. Some of\nour paraphrased sentences even have more brevity\nthan the reference, and still remain very meaning-\nful.\n5 Related Work\nOur baseline models – VAE-SVG -EQ (Gupta et al.,\n2018) and RbM- SL (Li et al., 2018) are both\ndeep learning models. While the former uses a\nvariational-autoencoder and is capable of generat-\ning multiple paraphrases of a given sentence, the\nlater uses deep reinforcement learning. In tune,\nwith part of our approach, ie, seq2seq, there exists\nample models with interesting variants – residual\nLSTM (Prakash et al., 2016), bi-directional GRU\nwith attention and special decoding tweaks (Cao\net al., 2017), attention from the perspective of se-\nmantic parsing (Su and Yan, 2017).\nMT has been greatly used to generate para-\nphrases (Quirk et al., 2004; Zhao et al., 2008) due\nto the availability of large corpora. While much\nearlier works have explored the use of manually\ndrafted rules (Hassan et al., 2007; Kozlowski et al.,\n2003).\nSimilar to our model architecture, Chen et al.\n(2018) combined transformers and RNN -based en-\ncoders for MT. Zhao et al. (2018) recently used the\ntransformer model for paraphrasing on different\ndatasets. We experimented using solely a trans-\nformer but got better results with T RAN SEQ. To\nthe best of our knowledge, our work is the ﬁrst\nto cross-breed the transformer and seq2seq for the\ntask of paraphrase generation.\n6 Conclusions\nWe proposed a novel framework, T RAN SEQ that\ncombines the efﬁciency of a transformer and\nseq2seq model and improves the current state-of-\nthe-art on the QUORA and MSCOCO paraphras-\ning datasets. Besides quantitative results, we pre-\nsented examples that highlight the syntactic and\nsemantic quality of our generated paraphrases.\nIn the future, it will be interesting to apply this\nframework for the task of abstractive text summa-\nrization and other NLG -related problems.\nAcknowledgments\nWe would like to thank the anonymous review-\ners for their useful comments. The research re-\nported in this paper was conducted at the Univer-\nsity of Lethbridge and supported by Alberta Inno-\nvates and Alberta Education.\nReferences\nMarianna Apidianaki, Guillaume Wisniewski, Anne\nCocos, and Chris Callison-Burch. 2018. Automated\nparaphrase lattice creation for hyter machine trans-\nlation evaluation. In Proceedings of the 2018 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 2 (Short Papers), pages\n480–485.\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\nautomatic metric for mt evaluation with improved\ncorrelation with human judgments. In Proceedings\nof the acl workshop on intrinsic and extrinsic evalu-\nation measures for machine translation and/or sum-\nmarization, pages 65–72.\nChris Callison-Burch, Philipp Koehn, and Miles Os-\nborne. 2006. Improved statistical machine transla-\ntion using paraphrases. In Proceedings of the main\nconference on Human Language Technology Con-\nference of the North American Chapter of the Asso-\nciation of Computational Linguistics , pages 17–24.\nAssociation for Computational Linguistics.\nZiqiang Cao, Chuwei Luo, Wenjie Li, and Sujian Li.\n2017. Joint copying and restricted generation for\nparaphrase. In Thirty-First AAAI Conference on Ar-\ntiﬁcial Intelligence.\nMia Xu Chen, Orhan Firat, Ankur Bapna, Melvin\nJohnson, Wolfgang Macherey, George Foster, Llion\nJones, Mike Schuster, Noam Shazeer, Niki Parmar,\net al. 2018. The best of both worlds: Combining\nrecent advances in neural machine translation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 76–86.\nYen-Chun Chen and Mohit Bansal. 2018. Fast abstrac-\ntive summarization with reinforce-selected sentence\nrewriting. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), volume 1, pages 675–686.\nKyunghyun Cho, Bart Van Merri ¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using rnn encoder-decoder\nfor statistical machine translation. arXiv preprint\narXiv:1406.1078.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho,\nand Yoshua Bengio. 2014. Empirical evaluation of\ngated recurrent neural networks on sequence model-\ning. arXiv preprint arXiv:1412.3555.\nLi Dong, Jonathan Mallinson, Siva Reddy, and Mirella\nLapata. 2017. Learning to paraphrase for question\n254\nanswering. In Proceedings of the 2017 Conference\non Empirical Methods in Natural Language Pro-\ncessing, pages 875–886.\nSebastian Gehrmann, Yuntian Deng, and Alexander\nRush. 2018. Bottom-up abstractive summarization.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n4098–4109.\nAnkush Gupta, Arvind Agarwal, Prawaan Singh, and\nPiyush Rai. 2018. A deep generative framework for\nparaphrase generation. In Thirty-Second AAAI Con-\nference on Artiﬁcial Intelligence.\nSamer Hassan, Andras Csomai, Carmen Banea, Ravi\nSinha, and Rada Mihalcea. 2007. Unt: Subﬁnder:\nCombining knowledge sources for automatic lex-\nical substitution. In Proceedings of the Fourth\nInternational Workshop on Semantic Evaluations\n(SemEval-2007), pages 410–413.\nMohit Iyyer, John Wieting, Kevin Gimpel, and Luke\nZettlemoyer. 2018. Adversarial example generation\nwith syntactically controlled paraphrase networks.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 1875–1885.\nRaymond Kozlowski, Kathleen F McCoy, and K Vijay-\nShanker. 2003. Generation of single-sentence para-\nphrases from predicate/argument structure using\nlexico-grammatical resources. In Proceedings of the\nsecond international workshop on Paraphrasing-\nVolume 16, pages 1–8. Association for Computa-\ntional Linguistics.\nZichao Li, Xin Jiang, Lifeng Shang, and Hang Li.\n2018. Paraphrase generation with deep reinforce-\nment learning. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 3865–3878.\nChin-Yew Lin. 2004. Rouge: A package for auto-\nmatic evaluation of summaries. Text Summarization\nBranches Out.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In European confer-\nence on computer vision, pages 740–755. Springer.\nJonathan Mallinson, Rico Sennrich, and Mirella Lap-\nata. 2017. Paraphrasing revisited with neural ma-\nchine translation. In Proceedings of the 15th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Volume 1, Long Pa-\npers, pages 881–893.\nKathleen R McKeown. 1983. Paraphrasing questions\nusing given and new information. Computational\nLinguistics, 9(1):1–10.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th annual meeting on association for compu-\ntational linguistics, pages 311–318. Association for\nComputational Linguistics.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 confer-\nence on empirical methods in natural language pro-\ncessing (EMNLP), pages 1532–1543.\nAaditya Prakash, Sadid A Hasan, Kathy Lee, Vivek\nDatla, Ashequl Qadir, Joey Liu, and Oladimeji\nFarri. 2016. Neural paraphrase generation with\nstacked residual lstm networks. arXiv preprint\narXiv:1610.03098.\nChris Quirk, Chris Brockett, and William Dolan.\n2004. Monolingual machine translation for para-\nphrase generation. In Proceedings of the 2004 con-\nference on empirical methods in natural language\nprocessing, pages 142–149.\nShikhar Sharma, Layla El Asri, Hannes Schulz, and\nJeremie Zumer. 2017. Relevance of unsuper-\nvised metrics in task-oriented dialogue for evalu-\nating natural language generation. arXiv preprint\narXiv:1706.09799.\nLinfeng Song, Zhiguo Wang, Wael Hamza, Yue Zhang,\nand Daniel Gildea. 2018. Leveraging context infor-\nmation for natural question generation. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 2\n(Short Papers), pages 569–574.\nYu Su and Xifeng Yan. 2017. Cross-domain seman-\ntic parsing via paraphrasing. In Proceedings of the\n2017 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1235–1246.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in neural information process-\ning systems, pages 3104–3112.\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Fran-\ncois Chollet, Aidan Gomez, Stephan Gouws, Llion\nJones, Łukasz Kaiser, Nal Kalchbrenner, Niki Par-\nmar, et al. 2018. Tensor2tensor for neural machine\ntranslation. In Proceedings of the 13th Conference\nof the Association for Machine Translation in the\nAmericas (Volume 1: Research Papers) , volume 1,\npages 193–199.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\n255\nSanqiang Zhao, Rui Meng, Daqing He, Andi Saptono,\nand Bambang Parmanto. 2018. Integrating trans-\nformer and paraphrase rules for sentence simpliﬁ-\ncation. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 3164–3173, Brussels, Belgium. Associ-\nation for Computational Linguistics.\nShiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and\nSheng Li. 2008. Combining multiple resources to\nimprove smt-based paraphrasing model. In Pro-\nceedings of ACL-08: HLT, pages 1021–1029.",
  "topic": "Paraphrase",
  "concepts": [
    {
      "name": "Paraphrase",
      "score": 0.8476758003234863
    },
    {
      "name": "Computer science",
      "score": 0.7805495262145996
    },
    {
      "name": "Transformer",
      "score": 0.7387890815734863
    },
    {
      "name": "Encoder",
      "score": 0.6631542444229126
    },
    {
      "name": "Sentence",
      "score": 0.5989184975624084
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5447230935096741
    },
    {
      "name": "Natural language processing",
      "score": 0.509606122970581
    },
    {
      "name": "Decoding methods",
      "score": 0.4615263044834137
    },
    {
      "name": "Speech recognition",
      "score": 0.39099621772766113
    },
    {
      "name": "Algorithm",
      "score": 0.18366655707359314
    },
    {
      "name": "Engineering",
      "score": 0.07391056418418884
    },
    {
      "name": "Voltage",
      "score": 0.0705145001411438
    },
    {
      "name": "Electrical engineering",
      "score": 0.06870472431182861
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I107949126",
      "name": "University of Lethbridge",
      "country": "CA"
    }
  ]
}