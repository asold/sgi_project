{
  "title": "Symbolic Knowledge Distillation: from General Language Models to Commonsense Models",
  "url": "https://openalex.org/W3207166518",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1830218503",
      "name": "Peter West",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2071644166",
      "name": "Chandra Bhagavatula",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2032101315",
      "name": "Jack Hessel",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2915762893",
      "name": "Jena Hwang",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2111206870",
      "name": "Liwei Jiang",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A1967926312",
      "name": "Ronan Le Bras",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2346695291",
      "name": "Ximing Lu",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2751594449",
      "name": "Sean Welleck",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2133417374",
      "name": "Yejin Choi",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2970161131",
    "https://openalex.org/W2963644680",
    "https://openalex.org/W2471366537",
    "https://openalex.org/W2964150944",
    "https://openalex.org/W3034998021",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3093166897",
    "https://openalex.org/W3034602344",
    "https://openalex.org/W277886906",
    "https://openalex.org/W3169976744",
    "https://openalex.org/W3103291112",
    "https://openalex.org/W3010293452",
    "https://openalex.org/W2971600926",
    "https://openalex.org/W3174202502",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W3193231083",
    "https://openalex.org/W2963456134",
    "https://openalex.org/W2964207259",
    "https://openalex.org/W3154863804",
    "https://openalex.org/W3168921656",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2942882688",
    "https://openalex.org/W2962736243",
    "https://openalex.org/W1975879668",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3203259592",
    "https://openalex.org/W2963101081",
    "https://openalex.org/W2769188878",
    "https://openalex.org/W3100801259",
    "https://openalex.org/W3021526287",
    "https://openalex.org/W2164777277",
    "https://openalex.org/W2995404354",
    "https://openalex.org/W2963120843",
    "https://openalex.org/W2968297680",
    "https://openalex.org/W3034385177",
    "https://openalex.org/W2963736842",
    "https://openalex.org/W4287817164",
    "https://openalex.org/W2094728533",
    "https://openalex.org/W3012590175",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W1552847225",
    "https://openalex.org/W2250653840",
    "https://openalex.org/W2971869958",
    "https://openalex.org/W3022006665",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2998184481",
    "https://openalex.org/W4221143736",
    "https://openalex.org/W3174464510",
    "https://openalex.org/W2116064496",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3034918576",
    "https://openalex.org/W3135877092",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2785896739",
    "https://openalex.org/W3023293091",
    "https://openalex.org/W3032773829",
    "https://openalex.org/W3201241024",
    "https://openalex.org/W3035032873",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W3120948048",
    "https://openalex.org/W2970442950",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W3093232105"
  ],
  "abstract": "Peter West, Chandra Bhagavatula, Jack Hessel, Jena Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, Yejin Choi. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 4602 - 4625\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nSymbolic Knowledge Distillation:\nfrom General Language Models to Commonsense Models\nPeter West†‡* Chandra Bhagavatula‡ Jack Hessel‡ Jena D. Hwang‡\nLiwei Jiang†‡ Ronan Le Bras‡ Ximing Lu†‡ Sean Welleck†‡ Yejin Choi †‡*\n†Paul G. Allen School of Computer Science & Engineering, University of Washington\n‡Allen Institute for Artificial Intelligence\nAbstract\nThe common practice for training common-\nsense models has gonefrom–human–to–corpus–\nto–machine: humans author commonsense\nknowledge graphs in order to train common-\nsense models. In this work, we investigate\nan alternative, from–machine–to–corpus–to–\nmachine: general language models author these\ncommonsense knowledge graphs to train com-\nmonsense models.\nOur study leads to a new framework, Sym-\nbolic Knowledge Distillation. As with prior\nart in Knowledge Distillation (Hinton et al.,\n2015), our approach uses larger models to teach\nsmaller models. A key difference is that we\ndistill knowledge symbolically–as text–in ad-\ndition to the resulting neural model. We distill\nonly one aspect–the commonsense of a general\nlanguage model teacher, allowing the student\nto be a different type of model, a common-\nsense model. Altogether, we show that careful\nprompt engineering and a separately trained\ncritic model allow us to selectively distill high-\nquality causal commonsense from GPT-3, a\ngeneral language model.\nEmpirical results demonstrate that, for the first\ntime, a human-authored commonsense knowl-\nedge graph is surpassed by our automatically\ndistilled variant in all three criteria: quantity,\nquality, and diversity. In addition, it results in\na neural commonsense model that surpasses\nthe teacher model’s commonsense capabilities\ndespite its 100x smaller size. We apply this to\nthe ATOMIC resource, and will share our new\nsymbolic knowledge graph and commonsense\nmodels1.\n1 Introduction\nPrior works have suggested that pre-trained lan-\nguage models possess limited understanding of\ncommonsense knowledge (Merrill et al., 2021; Tal-\nmor et al., 2021; Davis and Marcus, 2017) despite\n1We will share this following the anonymity period. We\nhave permission from OpenAI to release GPT-3 generations\nGPT-3\n175B Parameters \nGeneral Model\n!\nATOMIC10X \n6.5M Examples \nCommonsense KG\nCOMETdistil \n1.5B Parameters \nCommonsense Model\nSymbolic Knowledge\nDistillation\n!CRITIC \nFine-tuned RoBERTa \nﬁlters for quality\nFigure 1: Symbolic knowledge distillation extracts the\ncommonsense from the large, general language model\nGPT-3, into 2 forms: a large commonsense knowledge\ngraph ATOMIC 10x, and a compact commonsense model\nCOMET DIS\nTIL . The quality of this knowledge can be con-\ntrolled and improved by adding a critic model, making\nGPT-3 a stronger teacher.\notherwise stellar performance on leaderboards. As\na result, symbolic commonsense knowledge graphs\n(Speer et al., 2017; Sap et al., 2019; Hwang et al.,\n2021) and corresponding neural representations\n(Bosselut et al., 2019; Hwang et al., 2021; Zhang\net al., 2020b) have supplemented past models with\ncommonsense capabilities. This has enabled di-\nverse downstream applications, including interac-\ntive learning through a conversational interface\n(Arabshahi et al., 2021), persona- and affect-aware\nconversation models (Kearns et al., 2020), figura-\ntive language understanding (Chakrabarty et al.,\n2020, 2021), story telling (Ammanabrolu et al.,\n2021a) and fantasy games (Ammanabrolu et al.,\n2021b).\nThe common practice for commonsense knowl-\nedge graph construction sees humans spell out\nas many pieces of knowledge as possible. This\npipeline goes from–human–to–corpus–to–machine,\nwith commonsense models trained from human-\n4602\nauthored knowledge graphs. Yet, high-quality,\nhuman-authored knowledge is expensive to scale,\nlimiting coverage; this motivates an alternative:\nfrom–machine–to–corpus–to–machine. Prior ef-\nforts toward automatic commonsense knowledge\ngraphs have resulted in considerably lower qual-\nity than human-written data (Hwang et al., 2021;\nZhang et al., 2020b), which in turn leads to less\nreliable neural models (Hwang et al., 2021). Broad\nliterature consistently shows machine-authored\nknowledge graphs underperform human-authored\ngraphs (Etzioni et al., 2011; Mitchell et al., 2015;\nBollacker et al., 2008).\nIn this work, we propose Symbolic knowledge\ndistillation, a new conceptual framework towards\nhigh-quality automatic knowledge graphs for com-\nmonsense, leveraging state-of-the-art models and\nnovel methodology. Most prior art for automatic\nknowledge graph construction extracts knowledge\nfrom raw text (Bhakthavatsalam et al., 2020; Zhang\net al., 2020a; Zhou et al., 2020; Zhang et al., 2020b;\nLi et al., 2020). In contrast, our approach is mo-\ntivated by knowledge distillation (Hinton et al.,\n2015) wherein a larger teacher model transfers\nknowledge to a compact student model (§2.1). Our\nmethod differs from prior knowledge distillation in\nkey ways: we distill a symbolic knowledge graph\n(i.e., generated text) in addition to a neural model,\nand we distill only a selective aspect of the teacher\nmodel. This selectively allows the student model\nto be of a different type (commonsense model),\ncompared to the teacher (general language model),\nenriching the scope of distillation. An added ben-\nefit is that knowledge distilled as text is human\nreadable: it can be understood and evaluated.\nA general language model–GPT-3 in our case–is\nan imperfect commonsense teacher on its own, and\nthe ability to evaluate distilled knowledge is useful\nin improving it. We empirically demonstrate that,\nby training a separate critic model to judge sym-\nbolic generation quality, a more precise teacher can\nbe defined. Knowledge from this critical teacher\nis higher quality–even exceeding human-authored\nknowledge. Yet even before training a critic, our\nstudy makes the unexpected finding that the student\nmodel surpasses the commonsense of GPT-3, our\nknowledge source.\nTo test symbolic knowledge distillation against\nthe human–to–corpus–to–machine paradigm, we\ncompare with ATOMIC 20\n20 (Hwang et al., 2021),\nwhich is a human-authored commonsense knowl-\nedge graph. We find thatATOMIC 10x, our machine-\ngenerated corpus, exceeds the human generated\ncorpus in scale, accuracy, and diversity with re-\nspect to 7 commonsense inference types that we\nfocus on in this study. The resulting commonsense\nmodel, COMET DIS\nTIL , not only surpasses the human-\ntrained equivalent COMET 20\n20, but is also smaller,\nmore efficient, and produces commonsense at a\nhigher accuracy than its own teacher–GPT-3.\nSymbolic knowledge distillation offers a promis-\ning new role for general language models, as com-\nmonsense knowledge sources, and humans, as\nsmall-scale evaluators to train critic models rather\nthan authors of commonsense knowledge. Our\nwork demonstrates that humans and LMs can be\neffective collaborators for curating commonsense\nknowledge graphs and training efficient and perfor-\nmant commonsense models.\n2 Overview and Key Findings\nThroughout our work, we describe the machine–\nto–corpus–to–machine methodology of symbolic\nknowledge distillation. We first go machine–to–\ncorpus (§3), by decoding from GPT-3, then im-\nprove our knowledge with a specialized critic\nmodel (§4), and finally distill this knowledge\ninto an efficient commonsense model (§5), going\ncorpus–to–machine. Throughout this process, we\nevaluate against a human knowledge source, com-\nparing our automatic knowledge graph ATOMIC 10x\nand commonsense model COMET DIS\nTIL to the human-\nauthored ATOMIC 20\n20 and resulting modelCOMET 20\n20\n(Hwang et al., 2021).\n2.1 Symbolic Knowledge Distillation\nOur proposed methodology parallels knowledge\ndistillation (Hinton et al., 2015), a method for com-\npressing a large or complicated teacher distribution\nPt into a smaller/simpler student distribution Ps.\nKey to knowledge distillation2 is the notion of min-\nimizing the cross-entropy between Pt and Ps:\nH(Pt, Ps) =−\n∑\ny∈Y\nPt(y) logPs(y) (1)\nKnowledge is transferred to the student by encour-\naging it to match teacher predictions. Hinton et al.\n(2015) apply this to conditional classification: for\n2In its simplest case, with temperature set to 1.0\n4603\nX starts running xEffect\nso, X gets in shape X sings a song HinderedBy\nbut not if\nX can't remember \nthe lyrics\nX and Y engage in \nan argument\nxWant\nso, X wants to avoid Y X is not well \nliked\nxReact\nso, X feels lonely\nX learns to type \nfast\nxNeed\nX needed\nto have taken \ntyping lessons\nX takes care of \na monkey\nxAttr \nX is seen as kind\nX steals his \ngrandfather's sword\nxEffect\nso, X\nis punished by \nhis grandfather X butts in HinderedBy\nbut not if\nX is too shy to \nspeak up\nX takes up new \nemployment\nxIntent\nbecause X wants\nto be self \nsufficient\nX waits for the \nstorm to break\nxEffect\nso, X\nis safe from the \nstorm\nFigure 2: Example automatically generated ATOMIC triples from our ATOMIC 10x commonsense knowledge graph.\nEach example includes a generated event, relation (with natural language interpretation), and generated inference.\neach training input, Pt and Ps are model predic-\ntions over label setY . Typically Y is a tractable set,\nover which this sum can reasonably be calculated.\nFor distilling the knowledge of generative mod-\nels, we can think of an unconditional language\nmodel (LM e.g. GPT-3) as Pt. This makes Y the\nset of all strings, over which LMs define probability.\nUnfortunately Y is an exponential set, intractable\nto sum over in Eq 1. Kim and Rush (2016) address\nthis problem by simply taking the mode of Pt over\nY , truncating most of the teacher distribution to the\nmost likely sequence and discarding information.\nInstead, we consider a sampling-based interpre-\ntation of the same objective:\nH(Pt, Ps) = E\ny∼Pt(y)\n[−log Ps(y)] (2)\nwhich exactly equals the cross-entropy of Eq 1, at\nthe limit under pure sampling from Pt.3\nYet distillingall knowledge from the teacher may\nnot be desirable–our work is specifically focused\non distlling commonsense knowledge from GPT-\n3. The ideal teacher Pt is a commonsense expert,\nbut GPT-3 can approximate such a teacher, off-the-\nshelf, via prompting. This ability to select informa-\ntion is one explicit benefit of the sampling-based\ninterpretation of Eq 2: while Eq 1 uses continu-\nous logits over existing data, sampling gives dis-\ncrete control over transferred information, by se-\nlecting which samples are elicited and used. For\nthe general language model GPT-3, We encour-\nage domain/quality with prompting, and sample\ntruncation (Holtzman et al., 2020). We call this\nthe loose teacher PL\nt –knowledge is generated and\ntransferred from GPT-3, but without critical assess-\nment of correctness (§3).\n3A useful consequence of this framing is that access to the\nfull model distribution is not required. Our experiments (§3)\nuse GPT-3, for which the distribution is not available, thus\nour method is applicable while knowledge distillation is not.\nIn fact, sampling knowledge in Eq 2 offers even\nmore control, as generations can be individually\ninterpreted and judged. Given an indicator function\nA(x) for which knowledge x is correct, we can\ndefine a stronger teacher model. Using a Product of\nExperts (Hinton, 2002) between the loose teacher\nPL\nt and and the critic A(x), we define a critical\nteacher:\nPt(x) ∝PL\nt (x|p) ·A(x) (3)\nIn practice, A(x) is a textual classifier learned on\nhuman judgements, 1 for knowledge predicted to\nbe correct and 0 otherwise. Thus, the critic gives\ncontrol over the correctness and confidence of the\nknowledge that is transferred (§4).\n2.2 Key Findings\nApplying symbolic knowledge distillation in prac-\ntice results in promising and surprising findings:\n1. Learning symbolic knowledge from language\nmodels can be framed as a symbolic extension\nto knowledge distillation. In §2.1, we describe\nlearning commonsense as a symbolic extension to\nknowledge distillation, with GPT-3 a knowledge\nsource. We elaborate on this process with positive\nresults in §3,4, and 5.\n2. Symbolic knowledge distillation constructs\na high quality knowledge graph at scale. Our\nmethod naturally yields a machine-generated com-\nmonsense knowledge graph, which can achieve\nimpressive quality (§4), beyond that of human-\nauthored data. An effective critic which filters\nincorrect generated knowledge is key.\n3. A critical teacher results in a higher quality\nstudent. In §4, we show that making the teacher\nmore critical results in higher quality knowledge,\neven as it reduces the scale of knowledge trans-\nferred. This demonstrates that quality matters, not\n4604\njust quantity, as higher quality knowledge results in\na higher quality commonsense model in §5 despite\nsmaller scale data.\n4. Critical teacher or not, a student can outper-\nform the knowledge source. In §5, we show the\nunexpected result that all student models exceed\nthe quality of GPT-3, the knowledge source.\n5. Machines can win over humans for automatic\nknowledge graph construction. In §4 and §5,\nwe show that machine generated knowledge and the\nresulting commonsense model can outperform their\nequivalents that use a human knowledge source.\nOur symbolic knowledge exceeds humans at scale,\nquality, and diversity. The resulting commonsense\nmodel achieves the most accurate commonsense\nKG completions.\n3 Machine-to-Corpus Verbalization\nSymbolic knowledge distillation begins by going\nmachine–to–corpus, i.e. generating many com-\nmonsense facts, which results in a commonsense\nknowledge graph. §2.1 frames this as sampling\nto estimate the knowledge distillation objective–a\nstudent commonsense model learns from the gen-\nerations of a teacher (GPT-3).\nWe start with aloose teacher, transferring knowl-\nedge by prompted generation with truncated sam-\npling alone–this is in contrast to thecritical teacher\n(§4) which explicitly judges and filters the gen-\nerated samples. The loose teacher uses few-shot\nprompting as in Brown et al. (2020). We use a\nfew-shot template:\n<TASK-PROMPT>\n<EX1-INP><EX1-OUT>\n. . .\n<EXN−1-INP><EXN−1-OUT>\n<EXN-INP>\nwhere <EXi-INP>/<EXi-OUT> are human-\nauthored, natural language ATOMIC entries,\nand <TASK-PROMPT> is a description of the\nproblem. Given such a prompt, GPT-3 generates\nthe missing piece, output <EXN -OUT> for input\n<EXN -INP>, following the pattern of earlier\nexamples (1 to N-1). We find important aspects for\nproducing high-quality commonsense knowledge:\n• Examples should be numbered. e.g.\n<EX5-INP> might begin with \"5)\" to in-\ndicate it is the 5th example.\n• The format of <EXi-INP> and <EXi-OUT>\nshould linguistically imply the relationship be-\ntween them. See below for examples.\n• <TASK-PROMPT> can be used to give extra\nspecification to complicated problems.\n3.1 Data: A TOMIC\nWe demonstrate symbolic knowledge distillation\non the ATOMIC if-then resource (Sap et al., 2019).\nThis follows an event-relation-inference (triple) for-\nmat. The corpus links events (e.g. X attacks Y) to\nrelations, e.g. HinderedBy which describes what\nmight hinder an event. For a relation/event, the\ngoal is to generate a resulting inference, e.g. X\nattacks Y HinderedBy X is restrained.\nOf the 23 relations from the most recent version–\nATOMIC 20\n20–we limit our investigation to 7 relations\nthat correspond to causal commonsense knowl-\nedge: xAttr (how X is perceived after event), xRe-\nact (how X reacts in response to event), xEffect\n(what X does after event), xIntent (X’s intent in\nevent), xWant (what X wants after event), xNeed\n(what X needed for event to take place) and Hin-\nderedBy. We describe how verbalization is ap-\nplied to ATOMIC data in 2 steps: generating under-\nlying events (heads), then full examples (inference\ngiven event).\n3.2 Event Generation\nEvents are context-free premises in ATOMIC\ninvolving PersonX (and sometimes a second\nPersonY) in various scenarios. These events form\nheads in knowledge graph triples. We generate\nevents by filling in the elements of our template:\n1. Event: X overcomes evil with good\n2. Event: X does not learn from Y\n. . .\n10. Event: X looks at flowers\n11.\nThe format is simple, as events are generated un-\nconditionally. We use 100 high-quality events from\nthe ATOMIC 20\n20 corpus for our prompt, selected\nto avoid grammatical or logical errors, and min-\nimize semantic overlap. We randomly sample 10\nof these seed events for each generation batch, re-\nsulting in randomized prompts. We use nucleus\nsampling (p = 0.9) (Holtzman et al., 2020), and\npresence/frequency penalties of 0.5 from the GPT-\n3 interface. We generate 165K unique events using\nthe 175B-parameter Davinci model4 from Brown\n4the largest available version of GPT-3\n4605\net al. (2020) (human-authored ATOMIC 20\n20 contains\nonly 6.2K events).\n3.3 Inference Generation\nGenerating ATOMIC inferences requires reasoning\nabout events and relations together. We design ver-\nbalization templates fo reach relation, with iterative\ndesign and small-scale verification by the authors5\ne.g. we prompt the xNeed relation as follows:\nWhat needs to be true for this\nevent to take place?\n. . .\nEvent <i>: X goes jogging\nPrerequisites: For this to\nhappen, X needed to wear running\nshoes\n. . .\nEvent <N>: X looks at flowers\nPrerequisites: For this to\nhappen,\nThe language of this template implies the relation-\nspecific task, both \"Prerequisites:\" and beginning\nwith \"for this to happen\" suggest the xNeed re-\nlation. As well, we include an xNeed-specific\n<TASK-PROMPT>. We use 10 few-shot examples\nfor each prompt.6\nFor each event/relation (165K X 7) we gener-\nate 10 inferences with the Curie GPT-3 model 7\nand earlier hyperparameters. Removing duplicate\nand degenerate (e.g. fewer than 3 characters) gen-\nerations yields 6.46M ATOMIC -style data triples\n(examples in Figure 2). We call this ATOMIC 10x,\nas it contains an order of magnitude more triples\nthan ATOMIC 20\n20 for the 7 relations we study.\n3.4 Evaluating a Generated Commonsense\nKnowledge Graph\nMachine generation enables a large scale of unique\ngenerations at a much lower cost than human-\nauthored knowledge (Table 1), but what kind of\nexamples are produced by GPT-3, and how does\nit differ from knowledge produced by humans? In\nthis section, we conduct an in-depth analysis to\nanswer these questions.\n5See Appendix D for full prompts.\n6We also replace anonymous names (“X”) with sampled\ngeneric names as this improved quality, See Appendix D. Once\ngeneration is complete, we substitute in generic markers (“X”)\nfor the final dataset.\n7for the largest, Davinci, 12M generations is computation-\nally/monetarily intractable.\nRelation ATOMIC 20\n20 ATOMIC 10x\nHinderedBy 77,616 1,028,092\nxNeed 100,995 760,232\nxWant 109,098 730,223\nxIntent 54,839 965,921\nxReact 62,424 1,033,123\nxAttr 113,096 884,318\nxEffect 90,868 1,054,391\nTotal Count 608,936 6,456,300\nEst Total Cost ~$40,000 ~$6,000\nEst Cost Per Triple ~$0.06 ~$0.001\nTable 1: Number of unique triples with the given\nrelation, |(·, relation, ·)|. The estimated cost for\nATOMIC 10x comes at a fraction of a conservative estima-\ntion for ATOMIC 20\n20 crowdsourcing costs.\nLexical Differences: Diversity and Uniqueness\nRecent work finds that machine generations can be\nrepetitive and lack diversity (Welleck et al., 2020;\nHoltzman et al., 2020); one way generated knowl-\nedge may differ from human-authored is less cre-\native word choice, diversity, or more repetition.\nTo test this, we begin with lexical diversity\n(i.e. unique words used, Table 2). While there\nis variation by relation, the diveristy of ATOMIC 10x\nactually exceeds ATOMIC 20\n20 here, 5.2M unique\nwords to 1.5M. In addition, it contains significantly\nmore strictly unique generated inferences (Table 2,\nunique tails).\nBLEU Soft Uniqueness. Exact match (above)\nfails to capture the notion of similar text. Follow-\ning the intuition of self-BLEU (Zhu et al., 2018),\nwe define soft uniqueness to describe diversity of\ngenerations in a corpus. An inference x is softly-\nunique if:\nBLEU2(C, x) < 0.5\nwhere C is the set of inferences for a given in-\nput (in our case, event + relation), and 0.5 is an\nempirical threshold. To find soft-uniqueness of a\ncorpus, we iteratively remove examples until all\nare softly unique, i.e. low mutual lexical over-\nlap; higher diversity means more such examples\n(thus a larger softly unique corpus is preferable).\nSoftly-unique corpus sizes are given in Table 4\n(“Size (div)”). ATOMIC 10x has a smaller fraction\nof softly-unique examples than ATOMIC 20\n20, yet it\ncontains many more such examples. ATOMIC 10x\ncontains 4.38M such examples (full size 6.5M) vs.\nATOMIC 20\n20, which has 560K (full size 600K).\n4606\nUnique Unique\nLength Tokens (K) Tails (K)\nA20\n20 A10x A20\n20 A10x A20\n20 A10x\nxWant 4.69 5.16 322 784 69 152\nxAttr 1.42 2.73 15 21 11 8\nxEffect 3.92 4.66 216 864 55 185\nxIntent 4.59 5.92 136 800 30 135\nxNeed 4.51 5.97 289 1378 64 231\nxReact 4.03 1.77 48 5 12 2\nHinderedBy 7.93 7.49 522 1775 290 874\nEvents 5.20 5.32 109 881 6.2 165\nTable 2: Average length, total unique tokens and total\nunique examples (in K, i.e. 1000s) by relation type\nand in events (bottom row) from ATOMIC 20\n20 (A20\n20) and\nATOMIC 10x (A10x).\nEntropy Cross Entropy KL Divergence\nH(D1) = 1.27 H(D1, D2) = 9.31 DKL(D1||D2) = 8.04\nH(D2) = 7.80 H(D2, D1) = 41.48 DKL(D2||D1) = 33.68\nTable 3: Entropy, cross-entropy, and divergence of\nATOMIC 20\n20 (D1) and ATOMIC 10x (D2).\nModel-based Diversity Measurement. Lexical\nnotions of diversity reward differences in surface\nform, which may not always reflect diversity of\ninformation, only format. Thus, we next study\ninformation-theoretic measures for diversity. In-\ntuitively, diverse information should be less pre-\ndictable, or higher entropy. With GPT-2 XL mod-\nels finetuned on ATOMIC 20\n20 and ATOMIC 10x (§5)\nwe estimate entropy–roughly, how difficult it is\nfor a model to capture the corpus information (Ta-\nble 3). This is 4 times higher for ATOMIC 10x,\nsuggesting more content from a modeling per-\nspective. We also estimate cross-entropy–how\nwell a model trained on one corpus describes the\nother. From ATOMIC 10x to ATOMIC 20\n20, this is 9.31,\nonly 2 points higher than its entropy suggesting\nATOMIC 20\n20 is describable with information from\nATOMIC 10x. In reverse, this is 41.48 suggesting\nmuch of ATOMIC 10x is not captured by ATOMIC 20\n20–\nATOMIC 10x is surprising given only information\nfrom ATOMIC 20\n20.\nHuman Evaluation of Quality. Perhaps most\nimportantly, we study the quality of knowledge in\neach corpus. We conduct human evaluation with\nAmazon Mechanical Turk. 3 annotators rate each\ntriple resulting in “accepted”, “rejected” or “no\njudgement”. We evaluate 3000 examples 8 from\n8this ensures at least 1000 after filtering by the critic §4)\nCorpus Accept Reject N/A Size Size (div)\nATOMIC 20\n20 86.8 11.3 1.9 0.6M 0.56M\nATOMIC 10x 78.5 18.7 2.8 6.5M 4.38M\n88.4 9.5 2.1 5.1M 3.68M\n(criticlow) 91.5 6.8 1.7 4.4M 3.25M\n95.3 3.8 1.0 3.0M 2.33M\n(critichigh) 96.4 2.7 0.8 2.5M 2.00M\n+ GPT-J 72.0 27.6 0.4 - -\n+ T5-11B LM 71.7 26.9 1.4 - -\nTable 4: Attributes of ATOMIC 10x and ATOMIC 10x (row\n2) including the critic model (§4, rows 3 - 6) with var-\nious filtering cutoffs. Accept and Reject are by ma-\njority human vote unless any mark N/A. Size is in\nunique examples 9. The highest precision corpus is\nATOMIC 10x with (critichigh), but multiple versions sur-\npass ATOMIC 20\n20. We also include alternate models (GPT-\nJ and T5-11B) as the loose teacher.\nATOMIC 10x, and 1000 from ATOMIC 20\n20 (Table 4).\nWe find Fleiss’ kappa (Fleiss, 1971) of 40.8 indicat-\ning moderate agreement (Landis and Koch, 1977),\nand 90.5% accuracy agreement. We require work-\ners meet an Amazon Mechanical Turk qualification\nfor annotation quality based on past commonsense\nevaluations. We compensate workers $0.17 per\ntask, which we estimate require 30 seconds. Fur-\nther details and task template are in appendix §A.\nFor the loose teacher, consider the top row of\nATOMIC 10x in Table 4 (other rows add the critic\n§4). ATOMIC 10x exceeds ATOMIC 20\n20 in scale, but\nis somewhat less acceptable by human raters–by\nroughly 8 percentage points. Yet, the larger scale of\nATOMIC 10x implies a significantly higher number\nof accurate examples. Increasing the proportion of\nthese is the main objective of the critic (§4).\nHow do Knowledge Sources Compare? To un-\nderstand the robustness of our approach, we assess\nother language models as the knowledge source\n(i.e. loose teacher): GPT-J (Wang and Komat-\nsuzaki, 2021) and T5-11B adapted for language\nmodelling (Lester et al., 2021). We substitute both\nfor GPT-3 as in §3.2,3.3, generating a small-scale\ncorpus to evaluate. We conduct human evaluation\non 1000 examples as above (Table 4). Both mod-\nels attain roughly 72% accuracy, 6 points below\nGPT-3 (78.5). This suggests strong potential, but\nhigher quality from GPT-3. We explore this further\nin Appendix B.\n9Size of ATOMIC 20\n20 is given as the number of comparable\ndatapoints, i.e. those with the same relations as ATOMIC 10x.\n4607\n4 Making the Teacher More Critical\nSymbolic knowledge distillation requires a strong\nteacher model to maximize the quality of the gener-\nated knowledge graph and resulting student model\n(§5). While the loose teacher (GPT-3 alone) re-\nsults in a viable commonsense knowledge graph,\nevaluation shows this isn’t a perfect commonsense\nteacher. Thus, we multiply in a critic model, to fil-\nter lower-quality knowledge,correcting the teacher\n(§2.1). With modest supervision (a small-scale hu-\nman evaluation) we train a classifier to predict and\ndiscriminate unacceptable examples. We multiply\nthis with the loose teacher §3, creating a critical\nteacher product of experts. In practice this means\nfiltering ATOMIC 10x to create new corpora that are\nhigher quality, yet still larger scale than human-\nauthored ATOMIC 20\n20.\nTraining a knowledge critic We gather a train-\ning set of correct vs. incorrect human judgments\non a randomly-sampled set of 10K entries of\nATOMIC 10x, as in §3.4 but with one annotation per\nexample. We take a (random) train/dev/test split of\n8k/1k/1k. While this step requires human annota-\ntion, humans take on the role of high-level supervi-\nsors here–critiquing a small number of generations\nrather than authoring the entire knowledge graph\nas in previous work. Indeed, the cost/complexity\nof this step is similar to a typical human evaluation,\nmaking it far cheaper/easier than eliciting human-\nauthored knowledge in past work.\nWe train binary classifiers (critics) for human ac-\nceptability using RoBERTa-Large (Liu et al., 2019).\nWe find pretraining on MNLI results in the best\nmodel in terms of precision and recall, and we sug-\ngest this technique for future studies. We give more\ndetail in Appendix C, including baselines. Our best\nmodel vastly improves the accuracy of ATOMIC 10x\n(Table 4), demonstrating that a small amount of\nhuman supervision can consistently help to correct\nGPT-3’s mistakes.\nSize-accuracy trade-off Using our critic to fil-\nter knowledge results in a natural trade-off be-\ntween size and accuracy. We test several cut-\noffs for ATOMIC 10x, i.e. confidence at which\nthe critic rejects examples. We report human-\nmeasured accuracy (Accept/Reject column Ta-\nble 4) following §3.4. We compare the loose\nteacher (unfiltered) to critical teachers. Discard-\ning 20% of instances that the critic judges as least\nacceptable (reducing corpus size from 6.5M to\nRandom Inf Event EMAP Full\nAP 79.3 81.9 86.2 87.1 94.0\nTable 5: Average Precision for ablated critic models.\nThe critic not only filters awkward phrasings which can\nbe identified by either the event ( Event) or inference\n(Inf) in isolation (EMAP only identifies these), but also\nlogical misalignments, which require modeling interac-\ntions between event/inference, i.e. the full critic (Full).\n5.1M), ATOMIC 10x’s accuracy rises 78.5 →88.4;\nhuman-authored ATOMIC 20\n20 contains 600K entries\nat 86.8% accuracy. Reducing to total size to 2.5M\nexamples (38% of full size), we attain 96.4% accu-\nracy, nearly 10 points above ATOMIC 20\n20 while still\n4X larger.\nWhat gets filtered out? We qualitatively identify\ntwo types of filtered triples: 1) logical misalign-\nments, events/inferences joined in an inconsistent\nmanner. Recognizing these requires understand-\ning events-inference interactions, e.g., X cannot\nfind his shirt as a result Xis wearing a shirt ; 2)\nawkward phrasings, in which events/inferences are\nindividually incoherent e.g. PersonX has a fire in\nthe bath–resulting triples are invalid as the event is\nimplausible.\nTo understand what is filtered, we ablate the\ncritic (Table 5): our full model is compared to a\nrandom predictor, event-only model, and inference-\nonly model. We also compare to an EMAP (Hessel\nand Lee, 2020) version, i.e. an ensemble of event\nand inference-only, without interactions between\nevent/inference (needed for logical misalignments).\nWe find GPT-3 produces both independent\nawkwardly-phrased events/inferences (filtered by\nX-only models) and logical misalignments. The\nclassifier, trained on validated knowledge triples,\nhelps in both cases. The EMAP of our full model\n(identifies only awkward phrasings) achieves 87%\nAP, and our full model (which additionally identi-\nfies logical misalignments) improves to 94% AP.\nDoes filtering hurt diversity? One concern is\nthat the critic may keep only similar “safe” ex-\namples, lacking novelty. We repeat our diversity\nanalysis (§3.4) for critical corpora (Table 4, “Size\n(div)”, higher=better). As we filter, we surprisingly\nobserve proportionally more diverse examples: full\nATOMIC 10x has a diverse subset 68% of its size;\nrising to 80% with the most extreme filtering. One\npossibility is that GPT-3 gravitates towards com-\n4608\nCKG Completion Train Corpus\nModel Acc Accept Reject N/A\nGPT2-XL zero-shot - 45.1 50.3 4.6\nGPT-3 - 73.3 24.1 2.6\nCOMET 20\n20 86.8 81.5 16.3 2.2\nCOMET DIS\nTIL 78.5 78.4 19.2 2.4\n+criticlow 91.5 82.9 14.9 2.2\n+critichigh 96.4 87.5 10.2 2.3\nTable 6: Model performance on knowledge base com-\npletion, measured by human judgement. Inferences are\ngenerated on held-out events from ATOMIC 20\n20. Models\nbesides GPT-3 use GPT-2 XL architecture. COMET DIS\nTIL\nwith a strong critic ( +critichigh) achieves the highest\nacceptance rate overall–87.5.\nmon sentence structures for inconsistent knowl-\nedge. These would be recognizable to the critic,\nand removing them would increase both quality\nand diversity. This surprising result warrants fur-\nther study.\n5 Corpus-to-Machine: Distillation\nThe final step of symbolic knowledge distillation\ntrains a compact model on the generated natural\nlanguage knowledge graph. Our base model is\nGPT2-XL trained on all of ATOMIC 10x: we denote\nthis model by COMET DIS\nTIL . We additionally train the\nmodel on critical versions of ATOMIC 10x–critlow\ndenotes training on the corpus achieving 91.5% ac-\ncuracy, and crithigh on the 96.4% accuracy corpus.\nModels are trained for 1 epoch, with default param-\neters using the Huggingface Transformers library\n(Wolf et al., 2019).\n5.1 Evaluating a Symbolically Distilled Model\nEvaluation follows past work (Hwang et al., 2021;\nBosselut et al., 2019; Sap et al., 2019) testing the\nability of models to do knowledge base completion,\ni.e. generating inferences for test events, specif-\nically from the ATOMIC 20\n20 test set. We use hu-\nman evaluation10 following Section 3.4, on 1000\ninputs (event + relation), with results in Table 6. We\ncompare to the GPT2-XL-based COMET 20\n20 model\ntrained on human-generated ATOMIC 20\n20, and GPT-\n3 using the same generation method as §3–in ef-\nfect, comparing the student COMET DIS\nTIL to the loose\nteacher GPT-3. We omit the critical teacher (GPT-\n3 + critic), which is not assured to produce an in-\n10We find Fleiss’ kappa (Fleiss, 1971) of 47.1 for accep-\ntance, indicating moderate agreement. (Landis and Koch,\n1977), and accuracy agreement of 88.7%.\nference for each input, as the critic may reject all\ntails for some inputs. We also compare to zero-shot\nGPT2-XL (Radford et al., 2019) using the same\nmethodology (Table 6).\nHow does COMET DIS\nTIL compare to GPT-3? In\nknowledge distillation, the student model often de-\nteriorates in performance (Hinton et al., 2015; Kim\nand Rush, 2016) compared to its teacher. Compar-\ning our base teacher–GPT-3–to the simplest version\nof COMET DIS\nTIL (top-row COMET DIS\nTIL of Table 6) sur-\nprisingly shows the student surpasses GPT-3, the\nmodel that generates its training data11. We posit\nthat the superior performance of COMET DIS\nTIL may\nhave to do with mistakes of GPT-3 being filtered by\nverbalization and training of GPT-2, and possibly\nthe focus of COMET DIS\nTIL on one commonsense do-\nmain while GPT-3 covers a more general domain.\nWe leave further study of this effect for future work.\nHow does COMET DIS\nTIL compare to human knowl-\nedge? While COMET DIS\nTIL without the critic is\nslightly outperformed by COMET 20\n20 in terms of ac-\ncuracy, this reverses with the critic. For both cutoffs\ntested, COMET DIS\nTIL surpasses COMET 20\n20, with more\nfiltering resulting in a wider gap.\nUsefulness of COMET DIS\nTIL For on-demand infer-\nence, where a single high quality inference for\nsome input event/relation is required, COMET DIS\nTIL\nis the best available model: the most performant\nversion surpasses COMET 20\n20 by 5 points and GPT-3\nby over 10. The critical teacher (GPT-3 + critic)\nyields a more accurate corpus, but may filter all\ninferences for an input, giving no output.\nLimits and Future Work The success of\nsymbolic knowledge distillation is a first step–\ndemonstrating superior performance to human au-\nthoring on the commonsense relations tested here.\nNo aspect of our approach is specific to these rela-\ntions, yet further work is needed to explore the fea-\nsibility of generation for other aspects of common-\nsense and knowledge, beyond these relations, to\nconcepts like physical or temporal commonsense.\n6 Related Work\nCommonsense Knowledge Graphs (CKG)\nCKGs provide knowledge for commonsense rea-\nsoning. Some are manually constructed, e.g.\n11The slight difference in acceptability for GPT-3 from\nTable 4 is likely due to variance in raters between rounds of\nevaluation, and a different distribution of events–Table 4 uses\ngenerated events while Table 6 uses events from ATOMIC 20\n20.\n4609\nATOMIC (Sap et al., 2019; Hwang et al., 2021).\nConceptNet (Speer et al., 2017) contains taxonomy\nand physical commonsense, authored by humans\nor compiled from such sources. Some CKGs are\nautomatically constructed: TransOMCS (Zhang\net al., 2020a) extracts 18.48M tuples from syntactic\nparses and CausalBank (Li et al., 2020) extracts\n314M cause-effect pairs by pattern-matching. In\ncontrast, we generate commonsense.\nExtracting Knowledge from LMs Past work\nuses models for automatic knowledge graph com-\npletion (Bosselut et al., 2019; Hwang et al., 2021;\nLi et al., 2020). Yet, models are trained on existing\nresources; ATOMIC 10x is generated without these.\nOther works mine factual/commonsense knowl-\nedge directly from off-the-shelf LMs (Petroni et al.,\n2019; Davison et al., 2019; Xiong et al., 2020), but\nnot resulting in the quality at scale of ATOMIC 10x.\nKnowledge Distillation Other works use knowl-\nedge distillation (Hinton et al., 2015) for genera-\ntion. (Sanh et al., 2019) follow a label smoothing\nformulation, while Kim and Rush (2016) follow a\nsimilar formulation to us (§2.1), use the mode of\nthe teacher distribution rather than sampling. Our\nwork is unique in distilling specific information\n(commonsense) from a general language model.\nData Generation While manual dataset creation\nis expensive and complex (Schwartz et al., 2017;\nAgrawal et al., 2018; Tsuchiya, 2018; Bras et al.,\n2020),crowdsourcing is the most popular method\nfor goal-oriented, high quality/coverage datasets.\nPast automatic data mainly use extractive ap-\nproaches, e.g. syntactic parsing (Zhang et al.,\n2020a) or pattern matching (Li et al., 2020) from\nunstructured text (Lehmann et al., 2015; Buck et al.,\n2014). These scale, but are noisy and limited in\nformat–ATOMIC knowledge will not appear simply\nin natural text. Some works explore automatic data\nsynthesis/expansion by finetuning LMs on existing\nlabeled data (Anaby-Tavor et al., 2020; Papaniko-\nlaou and Pierleoni, 2020; Kumar et al., 2020; Yang\net al., 2020), but are limited by data quality.\n7 Conclusions\nWe introduce symbolic knowledge distillation, a\nmachine–to–corpus–to–machine pipeline for com-\nmonsense that does not require human-authored\nknowledge–instead, using machine generation.\nKnowledge is transferred from a large, general\nmodel to a compact commonsense model, through\na commonsense corpus–yielding a commonsense\nknowledge graph and model. Our resulting sym-\nbolic knowledge graph has greater scale, diversity,\nand quality than human authoring. symbolic knowl-\nedge distillation offers an alternative to human-\nauthored knowledge in commonsense research.\nAcknowledgments\nThis work was funded in part by the Natural Sci-\nences and Engineering Research Council of Canada\n(NSERC) (funding reference number 401233309),\nDARPA MCS program through NIWC Pacific\n(N66001-19-2-4031), and the Allen Institute for\nAI.\nEthical Considerations\nOne aspect of our work with the potential for ethi-\ncal pitfalls is large-scale generation from pretrained\nlanguage models, in constructing ATOMIC 10x. Re-\ncent work (Bender et al., 2021) has highlighted the\nrisks of models trained on massive text resources,\nas GPT-3 (Brown et al., 2020) is, which we use\nfor generation. Indeed, open generations from pre-\ntrained language models can often contain harmful,\nbiased, or offensive aspects. We argue here that\nthis risk is largely mitigated in our work, mainly\ndue to the narrow and constrained nature of our\ngenerations. The goal of our work is characterising\nsimple and generic anonymous situations, specifi-\ncally in terms of commonsense causes and effects.\nWe ensure generations are focused on these top-\nics through careful prompting, which we found to\nbe quite effective at keeping these generations on-\ntopic. As such, the potential for harmful generation\nis very low; indeed, in a manual inspection of 100\ngenerated examples, we found none that were sig-\nnificant harmful, besides one that contained adult\ncontent.\nA related concern is the potential for large mod-\nels and training sets to make automated oppression\nor exploitation possible, for instance in surveillance\nor generating fake news. As above, we argue that\nthe generic, commonsense nature of our data and\nmodels makes this concern less relevant here. Our\ndata does not contain any information directly re-\nlated to these harmful domains (e.g. social media\nor fake news generation). While our data may as-\nsist machines in understanding basic situations, this\nis unlikely to be useful for harmful models given\nthe simplicity of our data and still-flawed com-\nmonsense capabilities of even the most advanced\n4610\nmodels.\nFinally, we note that we ensure fair and gener-\nous compensation for all human evaluators we hire\nthrough Amazon Mechanical Turk. Based on our\nestimates of time required per task, we ensure that\nthe effective pay rate is at least $15 per hour.\nReferences\nAishwarya Agrawal, Dhruv Batra, Devi Parikh, and\nAniruddha Kembhavi. 2018. Don’t just assume; look\nand answer: Overcoming priors for visual question\nanswering. 2018 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 4971–\n4980.\nPrithviraj Ammanabrolu, Wesley Cheung, William\nBroniec, and Mark O. Riedl. 2021a. Automated sto-\nrytelling via causal, commonsense plot ordering. In\nAAAI.\nPrithviraj Ammanabrolu, Jack Urbanek, Margaret Li,\nArthur D. Szlam, Tim Rocktaschel, and Jason Weston.\n2021b. How to motivate your dragon: Teaching goal-\ndriven agents to speak and act in fantasy worlds. In\nNAACL.\nAteret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich,\nAmir Kantor, George Kour, Segev Shlomov, Naama\nTepper, and Naama Zwerdling. 2020. Do not have\nenough data? deep learning to the rescue! Proceed-\nings of the AAAI Conference on Artificial Intelligence,\n34:7383–7390.\nForough Arabshahi, Jennifer Lee, Antoine Bosselut,\nYejin Choi, and Tom. Mitchell. 2021. Conversa-\ntional multi-hop reasoning with neural commonsense\nknowledge and symbolic logic rules. In EMNLP.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language mod-\nels be too big? In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, FAccT ’21, page 610–623, New York, NY ,\nUSA. Association for Computing Machinery.\nSumithra Bhakthavatsalam, Chloe Anastasiades, and\nPeter E. Clark. 2020. Genericskb: A knowledge base\nof generic statements. ArXiv, abs/2005.00660.\nKurt D. Bollacker, Colin Evans, Praveen K. Paritosh,\nTim Sturge, and Jamie Taylor. 2008. Freebase: a\ncollaboratively created graph database for structuring\nhuman knowledge. In SIGMOD Conference.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, A. Çelikyilmaz, and Yejin Choi.\n2019. Comet: Commonsense transformers for auto-\nmatic knowledge graph construction. In ACL.\nRonan Le Bras, Swabha Swayamdipta, Chandra Bha-\ngavatula, Rowan Zellers, Matthew E. Peters, Ashish\nSabharwal, and Yejin Choi. 2020. Adversarial filters\nof dataset biases. In ICML.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nChristian Buck, Kenneth Heafield, and Bas Van Ooyen.\n2014. N-gram counts and language models from the\ncommon crawl. In LREC, volume 2, page 4. Citeseer.\nTuhin Chakrabarty, Debanjan Ghosh, Smaranda Mure-\nsan, and Nanyun Peng. 2020. Rˆ3: Reverse, retrieve,\nand rank for sarcasm generation with commonsense\nknowledge. In ACL.\nTuhin Chakrabarty, Xurui Zhang, Smaranda Muresan,\nand Nanyun Peng. 2021. MERMAID: Metaphor gen-\neration with symbolism and discriminative decoding.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 4250–4261, Online. Association for Computa-\ntional Linguistics.\nErnest Davis and Gary Marcus. 2017. Causal genera-\ntive models are just a start. Behavioral and Brain\nSciences, 40.\nJoe Davison, Joshua Feldman, and Alexander M Rush.\n2019. Commonsense knowledge mining from pre-\ntrained models. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 1173–1178.\nOren Etzioni, Anthony Fader, Janara Christensen,\nStephen Soderland, et al. 2011. Open information\nextraction: The second generation. In Twenty-Second\nInternational Joint Conference on Artificial Intelli-\ngence.\nJoseph L Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological bulletin,\n76(5):378.\nJack Hessel and Lillian Lee. 2020. Does my multimodal\nmodel learn cross-modal interactions? it’s harder to\ntell than you might think! In EMNLP.\nGeoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. 2015.\nDistilling the knowledge in a neural network. In\nNIPS Deep Learning and Representation Learning\nWorkshop.\nGeoffrey E Hinton. 2002. Training products of experts\nby minimizing contrastive divergence. Neural com-\nputation, 14(8):1771–1800.\n4611\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text\ndegeneration. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nJena D. Hwang, Chandra Bhagavatula, Ronan Le Bras,\nJeff Da, Keisuke Sakaguchi, Antoine Bosselut, and\nYejin Choi. 2021. Comet-atomic 2020: On symbolic\nand neural commonsense knowledge graphs. AAAI.\nWilliam R. Kearns, Neha Kaura, Myra Divina,\nCuong Viet V o, Dong Si, Teresa M. Ward, and\nWeichao Yuwen. 2020. A wizard-of-oz interface\nand persona-based methodology for collecting health\ncounseling dialog. Extended Abstracts of the 2020\nCHI Conference on Human Factors in Computing\nSystems.\nYoon Kim and Alexander M. Rush. 2016. Sequence-\nlevel knowledge distillation. In EMNLP.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam:\nA method for stochastic optimization. CoRR,\nabs/1412.6980.\nVarun Kumar, Ashutosh Choudhary, and Eunah Cho.\n2020. Data augmentation using pre-trained trans-\nformer models. In Proceedings of the 2nd Workshop\non Life-long Learning for Spoken Language Systems,\npages 18–26, Suzhou, China. Association for Com-\nputational Linguistics.\nJ Richard Landis and Gary G Koch. 1977. The mea-\nsurement of observer agreement for categorical data.\nbiometrics, pages 159–174.\nJens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,\nD. Kontokostas, Pablo N. Mendes, Sebastian Hell-\nmann, M. Morsey, Patrick van Kleef, S. Auer, and\nC. Bizer. 2015. Dbpedia - a large-scale, multilingual\nknowledge base extracted from wikipedia. Semantic\nWeb, 6:167–195.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In EMNLP.\nZhongyang Li, Xiao Ding, Ting Liu, J. Edward Hu,\nand Benjamin Van Durme. 2020. Guided generation\nof cause and effect. In Proceedings of the Twenty-\nNinth International Joint Conference on Artificial\nIntelligence, IJCAI-20.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nWilliam Merrill, Yoav Goldberg, Roy Schwartz, and\nNoah A. Smith. 2021. Provable limitations of acquir-\ning meaning from ungrounded form: What will future\nlanguage models understand? Transactions of the\nAssociation for Computational Linguistics, 9:1047–\n1060.\nTom Michael Mitchell, William W. Cohen, Estevam R.\nHruschka, Partha P. Talukdar, Bo Yang, Justin Bet-\nteridge, Andrew Carlson, Bhavana Dalvi, Matt Gard-\nner, Bryan Kisiel, Jayant Krishnamurthy, N. Lao,\nKathryn Mazaitis, Thahir Mohamed, Ndapandula\nNakashole, Emmanouil Antonios Platanios, Alan Rit-\nter, Mehdi Samadi, Burr Settles, Richard C. Wang,\nD. Wijaya, Abhinav Gupta, Xinlei Chen, Abulhair\nSaparov, Malcolm Greaves, and Joel Welling. 2015.\nNever-ending learning. Communications of the ACM,\n61:103 – 115.\nYannis Papanikolaou and A. Pierleoni. 2020. Dare:\nData augmented relation extraction with gpt-2.\nArXiv, abs/2004.13845.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n2463–2473.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. ArXiv,\nabs/1910.01108.\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chan-\ndra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\nBrendan Roof, Noah A Smith, and Yejin Choi. 2019.\nAtomic: An atlas of machine commonsense for if-\nthen reasoning. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 33, pages\n3027–3035.\nRoy Schwartz, Maarten Sap, Ioannis Konstas, Leila\nZilles, Yejin Choi, and Noah A. Smith. 2017. The\neffect of different writing tasks on linguistic style:\nA case study of the ROC story cloze task. In Pro-\nceedings of the 21st Conference on Computational\nNatural Language Learning (CoNLL 2017) , pages\n15–25, Vancouver, Canada. Association for Compu-\ntational Linguistics.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the AAAI Confer-\nence on Artificial Intelligence, volume 31.\nAlon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bha-\ngavatula, Yoav Goldberg, Yejin Choi, and Jonathan\nBerant. 2021. Commonsenseqa 2.0: Exposing the\nlimits of ai through gamification.\nMasatoshi Tsuchiya. 2018. Performance impact caused\nby hidden bias of training data for recognizing tex-\ntual entailment. In Proceedings of the Eleventh In-\nternational Conference on Language Resources and\n4612\nEvaluation (LREC 2018), Miyazaki, Japan. European\nLanguage Resources Association (ELRA).\nBen Wang and Aran Komatsuzaki. 2021. GPT-\nJ-6B: A 6 Billion Parameter Autoregressive\nLanguage Model. https://github.com/\nkingoflolz/mesh-transformer-jax.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-\nnan, Kyunghyun Cho, and Jason Weston. 2020. Neu-\nral text generation with unlikelihood training. In\nInternational Conference on Learning Representa-\ntions.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1112–1122. Association for\nComputational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pierric\nCistac, Tim Rault, R’emi Louf, Morgan Funtowicz,\nand Jamie Brew. 2019. Huggingface’s transformers:\nState-of-the-art natural language processing. ArXiv,\nabs/1910.03771.\nWenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2020. Pretrained encyclopedia:\nWeakly supervised knowledge-pretrained language\nmodel. In International Conference on Learning\nRepresentations.\nYiben Yang, Chaitanya Malaviya, Jared Fernandez,\nSwabha Swayamdipta, Ronan Le Bras, Ji-Ping Wang,\nChandra Bhagavatula, Yejin Choi, and Doug Downey.\n2020. Generative data augmentation for common-\nsense reasoning. In Findings of the Association for\nComputational Linguistics: EMNLP 2020 , pages\n1008–1025, Online. Association for Computational\nLinguistics.\nHongming Zhang, Daniel Khashabi, Y . Song, and\nD. Roth. 2020a. TransOMCS: From linguistic graphs\nto commonsense knowledge. In IJCAI.\nHongming Zhang, Xin Liu, Haojie Pan, Y . Song, and\nC. Leung. 2020b. Aser: A large-scale eventuality\nknowledge graph. Proceedings of The Web Confer-\nence 2020.\nBen Zhou, Qiang Ning, Daniel Khashabi, and Dan Roth.\n2020. Temporal common sense acquisition with min-\nimal supervision. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 7579–7589, Online. Association for\nComputational Linguistics.\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan\nZhang, Jun Wang, and Yong Yu. 2018. Texygen: A\nbenchmarking platform for text generation models.\nIn The 41st International ACM SIGIR Conference on\nResearch & Development in Information Retrieval,\npages 1097–1100.\n4613\nA Human Evaluation Details\nWe conduct human evaluations on Amazon Me-\nchanical Turk using the template of Figures 4,5.\nWorkers are presented with ATOMIC -style triples,\nreplacing relations with natural language templates\n(e.g. HinderedBy becomes “can be hindered by”).\n3 annotators rate each triple, with options for ac-\nceptability: “always/often”, “sometimes/likely”,\n“farfetched/never”, “invalid”, or “too unfamiliar to\njudge”. The first two are considered “accepted”,\nthe second two “rejected” and the final is “no judge-\nment”. For reporting acceptance rates, and training\na critic model, we only distinguish between “ac-\ncepted” and not “accepted”.\nWorkers are compensated $0.17 per task (i.e.\ncompleting all questions in the evaluation template\nFigures 4,5). We estimate an upper bound of 30s to\ncomplete a single task, which gives an hourly rate\nof $20.4. Workers are selected based on an Amazon\nMechanical Turk qualification, specifically filtering\nfor workers with high accuracy on past knowledge\nbase triple evaluations. We follow the same setup\nfor all evaluations, besides number of annotators.\nThis setup is shown to result in consistent and reli-\nable annotations, with an inter-annotator agreement\ngiven by Fleiss’ kappa (Fleiss, 1971) of 40.8 when\nevaluating with 3 annotators, in §3.4.\nB Using Alternate Models as Knowledge\nSources\nOne natural question that arises from the strong\nperformance of symbolic knowledge distillation is\nwhether other sources of knowledge (i.e. language\nmodels) would similarly benefit from this method.\nIn this section, we particularly measure the capacity\nof other language models to serve as the “loose\nteacher” which generated the base knowledge of\nthe resulting corpus.\nWe expand our study beyond GPT-3 here (the\nmodel used in our work), to include 2 contempo-\nrary large language models, GPT-J (Wang and Ko-\nmatsuzaki, 2021) and T5-11B (Lester et al., 2021)\nfinetuned for language modelling. For knowledge\ngeneration (verbalization) we follow the same pro-\ncedure as §3 along with simple adjustments to im-\nprove quality. We are investigating the effect of\nthe critic on knowledge precision here, so we also\ninclude ATOMIC 20\n20 to probe the usefulness of auto-\nmatic filtering for human-authored knowledge.\nFor each knowledge source, we follow the hu-\nman evaluation setup in §3.4 to obtain quality an-\nnotations of 2000 examples, with 1 annotation per\nexample. This follows a similar setup to §4–indeed,\nwe are replicating the earlier critic experiments but\nat a smaller scale (2000 annotations vs. 10000)\nto allow for more knowledge sources. For each\nknowledge source, we randomly split into sizes of\n1400/300/300 for train, dev, and test sets. We fol-\nlow §4 to train a critic model for each knowledge\nsource.\nWe plot different thresholds (% of corpus fil-\ntered) against the resulting precision (proportion\nof corpus that is judged to be “valid” knowledge)\nin Figure 3, and give numbers at various sizes\nin Table 7. One striking aspect is that a critic\nmodel can raise the precision of any of these knowl-\nedge sources to approximately 90% while retaining\n30% of the original corpus size. While this dis-\ncards a significant portion of the original generated\nknowledge, it raises the exciting prospect of using\nmore cost-effective models at a large scale to gener-\nate strong commonsense corpora like ATOMIC 10x.\nGPT-J and T5-11B can both be run locally by\nresearchers, unlike GPT-3 which uses a pay-per-\ngeneration API. Thus, one can imagine producing\na large and high-quality corpus like ATOMIC 10x at\na lower cost by instead generating a larger volume\nof knowledge from such an accessible model, and\nsimply filtering to a greater extent.\nAnother interesting aspect is how the various\nknowledge sources diverge. Under little to no criti-\ncal filtering (i.e. corpus size = 1.0), the precision\nof various knowledge sources is widely spread. Be-\nfore applying a critic, quality of knowledge source\nis very important. Indeed, precision is ordered\nby cost of generation: human ATOMIC 20\n20 has the\nhighest precision while being the most expensive,\nfollowed by GPT-3 (used here) which is pay-per-\ngeneration, and finally the two publicly available\nmodels. Another point of divergence is for extreme\nfiltering (at approximately 20% of the original cor-\npus size. All knowledge sources but GPT-3 plateau\nat approximately 90% accuracy, while GPT-3 rises\ntowards 100%. Indeed, this supports our use of\nGPT-3 in this work, as a high-quality automatic\nknowledge source.\nC Critic Model\nWe train binary classifiers (critics) for human ac-\nceptability using RoBERTa-Large (Liu et al., 2019),\nfine-tuning all parameters, along with a 2-layer\nMLP on the [CLF] representation. We conduct\n4614\nPrecision at Corpus Size (%)\nKnowledge Source 100 90 80 70 60 50 40 30 20 10\nATOMIC 20\n20 84.0 86.3 87.9 89.0 88.3 88.7 91.7 90.0 90.0 90.0\nGPT-J 71.7 76.7 81.7 83.8 86.7 88.0 88.3 87.8 93.3 90.0\nT5-11B 64.7 66.7 70.8 74.8 79.4 84.7 89.2 92.2 91.7 93.3\nGPT-3 curie 79.3 81.5 85.0 86.2 88.3 90.7 91.7 90.0 98.3 100.0\nTable 7: Knowledge precision at various corpus sizes (from 100% to 10%) based on filtering by the critic model.\nPrecision is calculated by human annotation of valid or invalid knowledge. We consider 4 knowledge sources, as\ndescribed in Appendix B. This corresponds to the data plotted in Figure 3.\n0 20 40 60 80 100\ncorpus size (%)\n65\n70\n75\n80\n85\n90\n95\n100precision (%)\nATOMIC2020\nGPT-J\nT5-11B\nGPT-3 curie\nFigure 3: Precision resulting from the critic step from\n§4, with various thresholds. We include corpora gen-\nerated by GPT-3 ( ATOMIC 10x), GPT-J, T5-11B, and\nhumans (ATOMIC 20\n20). Without filtering (corpus size =\n1.0), different corpora have a variety of precisions. As\nmore examples are filtered by the critic, precision rises\nsignificantly demonstrating the strong value of the critic\nstep.\na small grid search on the validation set finding\nbatch size 128, dropout .1, and Adam (Kingma\nand Ba, 2015) learning rate 5e-6 to be effective.\nWe use early stopping and decay learning rate on\nvalidation performance plateauing, to maximize\nR@80% on the validation set. We find RoBERTa\npretrained on MNLI (Williams et al., 2018) effec-\ntive, outperforming other options. As well, we\nsubstitute randomly-sampled names in for person\ndesignations “X”/“Y”. We include as a baseline an\nunsupervised filtration metric inspired by (Davison\net al., 2019): they propose a model estimate of\nPMI to score mined commonsense triples. In our\ncase, we use Negative Log-Likelihood (NLL) and\ntoken-mean-NLL from GPT-3 itself.\nThe validation precision/recall of our best per-\nforming model, the baselines, and the in-optimal\nhyperparameter configurations are given in Fig-\nure 6. Once fixing our model, we applied it to the\ntest set (also in Fig 6), verifying that it generalizes\nto ATOMIC 10x entries. Overall, our trained critic\nmodel is more effective than the baselines in iden-\ntifying high and low quality teacher generations at\nall levels of precision and recall. This result demon-\nstrates that a small amount of human supervision\ncan consistently help to correct GPT-3’s mistakes.\nD A TOMIC 10x Generation Prompts\nWe include example prompts for all generations\nwe do, from Table 8 to 15. Note that elements\nof generation prompts are randomized for each\nbatch. For event generation, the few-shot examples\nand order are randomly sampled from a seed set\nof 100 high-quality examples from ATOMIC 20\n20 in\neach batch. For inference generation, the natural\nnames used for PersonX and PersonY are randomly\nsampled from a small predefined set of names.\n4615\nInstructions (click to expand/collapse)\n(WARNING: This HIT may contain adult content. Worker discretion is advised.)\nThanks for participating in this HIT!\nIf the data is good, it's good. If bad, then bad. Please annotate as you see not worrying about how many of each label\nyou ! nd yourself assigning! If you understand the words but the Phrases or the complete assertation makes poor\nsense, please mark as INVALID. Thank you!\nYou will evaluate how often assertions are true. Each assertion is comprised of 3 parts: Phrase A , Relation,\nPhrase B\nFor each assertion, determine how true it is:\nIf you see \"nothing in particular\" for Phrase B , assess Phrase B in context:\nSometimes certain actions can simply be responded to by doing nothing!\nOther times, doing nothing in particular is simply a weird or unlikely reaction to something.\nSee examples under tricky relations tagged with nothing in particular example\nPlease report any prejudiced or inappropriate language :\nProfane or o \" ensive content (NSFW, R-rated material etc)\nPrejudiced assumptions or derogatory language that villainizes  people.\nHOWEVER, please note, not all negative content is derogatory especially if Phrase B is intrinsically what Phrase\nA means. For example:\ncriminals  are characterized by committing crime  is OK .\n↳  This isn't necessarily villianizing people since \"criminal\" means \"a person who has commited a crime\".\nhomeless  are characterized by being lazy  is prejudiced .\n↳  There are many reason a person is rendered homeless. This is a gratuitous prejudice about homelessness.\nMaterial that people may ! nd disturbing, o \" -putting, or improper\nA couple NOTES:\nPlease be forgiving  of spelling or grammatical errors\nIf the terms are too obscure or you don't know the truth of the fact at the top of your head, it is okay to mark is\n\"too unfamiliar to judge\". If you can answer (e.g., based on likelihood), please provide a response.\nPhrase A , Phrase B Short phrases. May describe objects, object properties, events, actions, etc.\nRelation How A  relates to B .\nalways/often Always or quite often true.\nsometimes/likely Sometimes is true or true for some people. -or- Likely true.\nfarfetched/never False or farfetched, at best. -or- Unlikely to be true.\ninvalid This assertion makes no sense (i.e., \"what does this even mean?!\").\ntoo unfamiliar to judge Cannot make a fair evaluation. Unfamiliar with one or both of the phrase.\nTricky Relations (click to expand/collpase)\nExamples (click to expand/collapse)\nFigure 4: Page 1 of template used for human evaluation.\n4616\n1) PersonX approaches PersonY's aunt , as a result, PersonX feels, awkward\nHow often does the assertion hold true?\nThis fact is true but outdated\nI would count this as an inappropriate, prejudiced or o \" ensive material\n2) PersonX asked PersonY out on a date , can be hindered by, PersonX is still dating Sarah\nHow often does the assertion hold true?\nThis fact is true but outdated\nI would count this as an inappropriate, prejudiced or o \" ensive material\n3) PersonX fails to go home , as a result, PersonX, is grounded\nHow often does the assertion hold true?\nThis fact is true but outdated\nI would count this as an inappropriate, prejudiced or o \" ensive material\n4) PersonX makes her own clothes , as a result, PersonX feels, artistic\nHow often does the assertion hold true?\nThis fact is true but outdated\nI would count this as an inappropriate, prejudiced or o \" ensive material\n5) PersonX notices PersonY's response , can be hindered by, PersonX is distracted by the music\nHow often does the assertion hold true?\nThis fact is true but outdated\nI would count this as an inappropriate, prejudiced or o \" ensive material\nalways/often sometimes/likely farfetched/never invalid too unfamiliar to judge\nalways/often sometimes/likely farfetched/never invalid too unfamiliar to judge\nalways/often sometimes/likely farfetched/never invalid too unfamiliar to judge\nalways/often sometimes/likely farfetched/never invalid too unfamiliar to judge\nalways/often sometimes/likely farfetched/never invalid too unfamiliar to judge\n(Optional) Please let us know if anything was unclear, if you experienced any\nissues, or if you have any other fedback for us.\nYou must ACCEPT the HIT before you can submit the results.\nFigure 5: Page 2 of template used for human evaluation.\n4617\n1. Event: PersonX unwraps PersonY’s hands\n2. Event: PersonX overcomes evil with good\n3. Event: PersonX is fed up with the present situation\n4. Event: PersonX breaks PersonX’s back\n5. Event: PersonX calls no one\n6. Event: PersonX never gets angry\n7. Event: PersonX does not learn from PersonY\n8. Event: PersonX refuses to touch PersonY’s hands\n9. Event: PersonX looks at flowers\n10. Event: PersonX unloads an atomic bomb\n11. Event:\nTable 8: Prompt for head generation.\n0.0 0.2 0.4 0.6 0.8 1.0\nrecall\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00precision Best Model Val\nBest Model Test\nGPT-3 NLL\nGPT-3 mean NLL\nFigure 6: Precision vs. recall of our critic model on\nthe human labelled validation set. The best trained\nmodels are labelled, and other hyper-parameter settings\nare shown as faded lines. We also include generation\nnegative log-likelihood (nll) and token-wise mean nll\nas cutoff measures–these perform much worse than the\nsupervised model.\n4618\nNext, how are people seen in each situation? Examples:\nSituation 1: Devin bullies Jean.\nDevin is seen as dominant.\nSituation 2: Jamie moves to another city.\nJamie is seen as adventurous.\nSituation 3: Sydney changes Ryan’s mind.\nSydney is seen as influential.\nSituation 4: Lindsay writes a story.\nLindsay is seen as creative.\nSituation 5: Rowan covers Pat’s expenses.\nRowan is seen as wealthy.\nSituation 6: Lee takes time off.\nLee is seen as carefree.\nSituation 7: Riley advises Noel.\nRiley is seen as informed.\nSituation 8: Adrian bursts into tears.\nAdrian is seen as depressed.\nSituation 9: Hunter deals with problems.\nHunter is seen as responsible.\nSituation 10: Sam follows Charlie.\nSam is seen as suspicious.\nSituation 11: Alex makes Chris wait.\nAlex is seen as\nTable 9: Prompt for generating xAttr.\n4619\nNext, what do situations make people do? Examples:\nSituation 1: Devin gets a divorce.\nAs a result, Devin dates someone new.\nSituation 2: Jamie lifts weights.\nAs a result, Jamie has sore muscles.\nSituation 3: Sydney takes Ryan to a bar.\nAs a result, Sydney gets drunk.\nSituation 4: Lindsay decides to hire a tutor.\nAs a result, Lindsay gets better grades.\nSituation 5: Rowan buys Pat drinks.\nAs a result, Rowan is thanked by Pat.\nSituation 6: Lee hears bad news.\nAs a result, Lee begins to cry.\nSituation 7: Riley buys a chocolate bar.\nAs a result, Riley gets change.\nSituation 8: Adrian does a lot of work.\nAs a result, Adrian gets mental fatigue.\nSituation 9: Hunter attends a concert.\nAs a result, Hunter hears a new song.\nSituation 10: Sam gets the job done.\nAs a result, Sam gets more responsibilities.\nSituation 11: Alex makes Chris wait.\nAs a result, Alex\nTable 10: Prompt for generating xEffect.\n4620\nFor each situation, describe the intent. Examples:\nSituation 1: Devin gets the newspaper.\nDevin intends to read the newspaper.\nSituation 2: Jamie works all night.\nJamie intends to meet a deadline.\nSituation 3: Sydney destroys Ryan.\nSydney intends to punish Ryan.\nSituation 4: Lindsay clears her mind.\nLindsay intends to be ready for a new task.\nSituation 5: Rowan wants to start a business.\nRowan intends to be self sufficient.\nSituation 6: Lee ensures Ali’s safety.\nLee intends to be helpful.\nSituation 7: Riley buys lottery tickets.\nRiley intends to become rich.\nSituation 8: Alex makes Chris wait.\nAlex intends\nTable 11: Prompt for generating xIntent.\n4621\nNext, we will discuss what people need for certain situations. Examples:\n1. Before Devin makes many new friends, Devin has to spend time with people.\n2. Before Jamie gets a date, Jamie has to ask someone out.\n3. Before Sydney changes Ryan’s mind, Sydney has to think of an argument.\n4. Before Lindsay gets a job offer, Lindsay has to apply.\n5. Before Rowan takes a quick nap, Rowan has to lie down.\n6. Before Lee tries to kiss Ali, Lee has to approach Ali.\n7. Before Riley rides Noel’s skateboard, Riley has to borrow it.\n8. Before Adrian eats the food, Adrian has to prepare a meal.\n9. Before Hunter watches Netflix, Hunter has to turn on the TV .\n10. Before Sam has a baby shower, Sam has to invite some friends.\n11. Before Alex makes Chris wait, Alex has\nTable 12: Prompt for generating xNeed.\n4622\nNext, how do people feel in each situation? Examples:\nSituation 1: Devin lives with Jean’s family.\nDevin feels loved.\nSituation 2: Jamie expects to win.\nJamie feels excited.\nSituation 3: Sydney comes home late.\nSydney feels tired.\nSituation 4: Lindsay sees dolphins.\nLindsay feels joyful.\nSituation 5: Rowan causes Pat anxiety.\nRowan feels guilty.\nSituation 6: Lee goes broke.\nLee feels embarrassed.\nSituation 7: Riley has a drink.\nRiley feels refreshed.\nSituation 8: Adrian has a heart condition.\nAdrian feels scared about their health.\nSituation 9: Hunter shaves Avery’s hair.\nHunter feels helpful.\nSituation 10: Sam loses all of Charlie’s money.\nSam feels horrible.\nSituation 11: Alex makes Chris wait.\nAlex feels\nTable 13: Prompt for generating xReact.\n4623\nNext, what do people want in each situation? Examples:\nSituation 1: Devin mows the lawn.\nDevin wants to take a shower.\nSituation 2: Jamie is going to a party.\nJamie wants to take an Uber home.\nSituation 3: Sydney bleeds a lot.\nSydney wants to go to the ER.\nSituation 4: Lindsay works as a cashier.\nLindsay wants to find a better job.\nSituation 5: Rowan gets dirty.\nRowan wants to do a load of laundry.\nSituation 6: Lee stays up all night studying.\nLee wants to rest.\nSituation 7: Riley gets Noel’s autograph.\nRiley wants to tell some friends.\nSituation 8: Adrian sees Taylor’s point.\nAdrian wants to agree with Taylor.\nSituation 9: Hunter leaves Avery’s bike.\nHunter wants to keep the bike safe.\nSituation 10: Sam wants a tattoo.\nSam wants to find a tattoo design.\nSituation 11: Alex makes Chris wait.\nAlex wants\nTable 14: Prompt for generating xWant.\n4624\nNext, what can hinder each situation? Examples:\nSituation 1: Devin makes a doctor’s appointment,\nThis is hindered if Devin can’t find the phone to call the doctor.\nSituation 2: Jamie rubs Wyatt’s forehead,\nThis is hindered if Jamie is afraid to touch Wyatt.\nSituation 3: Sydney eats peanut butter,\nThis is hindered if Sydney is allergic to peanuts.\nSituation 4: Lindsay looks perfect,\nThis is hindered if Lindsay can’t find any makeup.\nSituation 5: Rowan goes on a run,\nThis is hindered if Rowan injures her knees.\nSituation 6: Lee takes Ali to the emergency room,\nThis is hindered if Ali has no health insurance to pay for medical care.\nSituation 7: Riley spends time with Noel’s family,\nThis is hindered if Noel’s family doesn’t like spending time with Riley.\nSituation 8: Adrian moves from place to place,\nThis is hindered if Adrian can’t afford to move.\nSituation 9: Hunter protests the government,\nThis is hindered if Hunter is arrested.\nSituation 10: Sam has a huge fight,\nThis is hindered if Sam does not like confrontation.\nSituation 11: Alex makes Chris wait,\nThis is hindered if\nTable 15: Prompt for generating HinderedBy.\n4625",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5914515852928162
    },
    {
      "name": "Computational linguistics",
      "score": 0.5325034856796265
    },
    {
      "name": "Language model",
      "score": 0.46912717819213867
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4269223213195801
    },
    {
      "name": "Natural language processing",
      "score": 0.3578832745552063
    },
    {
      "name": "Cognitive science",
      "score": 0.35371702909469604
    },
    {
      "name": "Linguistics",
      "score": 0.34458085894584656
    },
    {
      "name": "Philosophy",
      "score": 0.1916569471359253
    },
    {
      "name": "Psychology",
      "score": 0.12860289216041565
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210156221",
      "name": "Allen Institute for Artificial Intelligence",
      "country": "US"
    }
  ]
}