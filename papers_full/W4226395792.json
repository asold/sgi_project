{
    "title": "Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation",
    "url": "https://openalex.org/W4226395792",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2096281016",
            "name": "Xinyi Wang",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2140581490",
            "name": "Sebastian Ruder",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A277131583",
            "name": "Graham Neubig",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2933138175",
        "https://openalex.org/W3214173179",
        "https://openalex.org/W3112849432",
        "https://openalex.org/W3035032094",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W2126725946",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W3186617466",
        "https://openalex.org/W2970854433",
        "https://openalex.org/W2619984792",
        "https://openalex.org/W2915774325",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W2295781714",
        "https://openalex.org/W2964114970",
        "https://openalex.org/W2963002901",
        "https://openalex.org/W2742113707",
        "https://openalex.org/W3104723404",
        "https://openalex.org/W3115778530",
        "https://openalex.org/W2757931423",
        "https://openalex.org/W3191290329",
        "https://openalex.org/W1486697269",
        "https://openalex.org/W3206010442",
        "https://openalex.org/W3100198908",
        "https://openalex.org/W3093721400",
        "https://openalex.org/W3207937903",
        "https://openalex.org/W3101498587",
        "https://openalex.org/W3106274667",
        "https://openalex.org/W3169425228",
        "https://openalex.org/W3098466758",
        "https://openalex.org/W4230579319",
        "https://openalex.org/W3013840636",
        "https://openalex.org/W3177252310",
        "https://openalex.org/W3037109418",
        "https://openalex.org/W3103727211",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W3029889931",
        "https://openalex.org/W2270364989",
        "https://openalex.org/W2970529259",
        "https://openalex.org/W2538358357",
        "https://openalex.org/W2970963828",
        "https://openalex.org/W3173172013",
        "https://openalex.org/W2896457183"
    ],
    "abstract": "The performance of multilingual pretrained models is highly dependent on the availability of monolingual or parallel text present in a target language. Thus, the majority of the world's languages cannot benefit from recent progress in NLP as they have no or limited textual data. To expand possibilities of using NLP technology in these under-represented languages, we systematically study strategies that relax the reliance on conventional language resources through the use of bilingual lexicons, an alternative resource with much better language coverage. We analyze different strategies to synthesize textual or labeled data using lexicons, and how this data can be combined with monolingual or parallel text when available. For 19 under-represented languages across 3 tasks, our methods lead to consistent improvements of up to 5 and 15 points with and without extra monolingual text respectively. Overall, our study highlights how NLP methods can be adapted to thousands more languages that are under-served by current technology.",
    "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 863 - 877\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nExpanding Pretrained Models to Thousands More Languages\nvia Lexicon-based Adaptation\nXinyi Wang1 Sebastian Ruder2 Graham Neubig1\n1Language Technology Institute, Carnegie Mellon University\n2Google Research\nxinyiw1@cs.cmu.edu,ruder@google.com,gneubig@cs.cmu.edu\nAbstract\nThe performance of multilingual pretrained\nmodels is highly dependent on the availability\nof monolingual or parallel text present in a tar-\nget language. Thus, the majority of the world’s\nlanguages cannot beneﬁt from recent progress\nin NLP as they have no or limited textual data.\nTo expand possibilities of using NLP technol-\nogy in these under-represented languages, we\nsystematically study strategies that relax the\nreliance on conventional language resources\nthrough the use of bilingual lexicons, an al-\nternative resource with much better language\ncoverage. We analyze different strategies to\nsynthesize textual or labeled data using lexi-\ncons, and how this data can be combined with\nmonolingual or parallel text when available.\nFor 19 under-represented languages across 3\ntasks, our methods lead to consistent improve-\nments of up to 5 and 15 points with and with-\nout extra monolingual text respectively. Over-\nall, our study highlights how NLP methods can\nbe adapted to thousands more languages that\nare under-served by current technology.1\n1 Introduction\nMultilingual pretrained models (Devlin et al., 2019;\nConneau and Lample, 2019; Conneau et al., 2020)\nhave become an essential method for cross-lingual\ntransfer on a variety of NLP tasks (Pires et al.,\n2019; Wu and Dredze, 2019). These models can\nbe ﬁnetuned on annotated data of a down-stream\ntask in a high-resource language, often English,\nand then the resulting model is applied to other\nlanguages. This paradigm is supposed to beneﬁt\nunder-represented languages that do not have an-\nnotated data. However, recent studies have found\nthat the cross-lingual transfer performance of a\nlanguage is highly contingent on the availability\nof monolingual data in the language during pre-\ntraining (Hu et al., 2020). Languages with more\n1Code and data are available at: https:\n//github.com/cindyxinyiwang/\nexpand-via-lexicon-based-adaptation .\nmBERT/NMT\nWikipedia/ \nCommonCrawl\nBible\nLexicons\nCovered Language Varieties \n0 1750 3500 5250 7000\n1%\n4%\n23%\n70%\nFigure 1: The percentage of the world’s ≈7,000 languages\ncovered by mBERT, monolingual data sources and lexicons.\nmonolingual data tend to have better performance\nwhile languages not present during pretraining sig-\nniﬁcantly lag behind.\nSeveral works propose methods to adapt the\npretrained multilingual models to low-resource\nlanguages, but these generally involve continued\ntraining using monolingual text from these lan-\nguages (Wang et al., 2020; Chau et al., 2020; Pfeif-\nfer et al., 2020, 2021). Therefore, the performance\nof these methods is still constrained by the amount\nof monolingual or parallel text available, making it\ndifﬁcult for languages with little or no textual data\nto beneﬁt from the progress in pretrained models.\nJoshi et al. (2020) indeed argue that unsupervised\npretraining makes the ‘resource-poor poorer’.\nFig. 1 plots the language coverage of multilin-\ngual BERT (mBERT; Devlin et al., 2019), a widely\nused pre-trained model, and several commonly\nused textual data sources.2 Among the 7,000 lan-\nguages in the world, mBERT only covers about\n1% of the languages while Wikipedia and Com-\nmonCrawl, the two most common resources used\nfor pretraining and adaptation, only contain textual\ndata from 4% of the languages (often in quite small\nquantities, partially because language IDs are difﬁ-\ncult to obtain for low-resource languages (Caswell\net al., 2020)). Ebrahimi and Kann (2021) show\nthat continued pretraining of multilingual models\non a small amount of Bible data can signiﬁcantly\nimprove the performance of uncovered languages.\nAlthough the Bible has much better language cov-\nerage of 23%, its relatively small data size and\n2Statistics taken from Ebrahimi and Kann (2021) and\npanlex.org.\n863\nconstrained domain limits its utility (see § 6)—and\n70% of the world’s languages do not even have\nthis resource. The failure of technology to adapt\nto these situations raises grave concerns regarding\nthe fairness of allocation of any beneﬁt that may be\nconferred by NLP to speakers of these languages\n(Joshi et al., 2020; Blasi et al., 2021). On the other\nhand, linguists have been studying and document-\ning under-represented languages for years in a vari-\nety of formats (Gippert et al., 2006). Among these,\nbilingual lexicons or word lists are usually one of\nthe ﬁrst products of language documentation, and\nthus have much better coverage of the worlds’ lan-\nguages than easily accessible monolingual text, as\nshown in Fig. 1. There are also ongoing efforts\nto create these word lists for even more languages\nthrough methodologies such as “rapid word col-\nlection” (Boerger, 2017), which can create an ex-\ntensive lexicon for a new language in a number of\ndays. As Bird (2020) notes:\nAfter centuries of colonisation, mission-\nary endeavours, and linguistic ﬁeldwork,\nall languages have been identiﬁed and\nclassiﬁed. There is always a wordlist.\n. . .In short, we do not need to “discover”\nthe language ex nihilo (L1 acquisition)\nbut to leverage the available resources\n(L2 acquisition).\nHowever, there are few efforts on understanding\nthe best strategy to utilize this valuable resource\nfor adapting pretrained language models. Bilingual\nlexicons have been used to synthesize bilingual\ndata for learning cross-lingual word embeddings\n(Gouws and Søgaard, 2015; Ruder et al., 2019)\nand task data for NER via word-to-word transla-\ntion (Mayhew et al., 2017), but both approaches\nprecede the adoption of pre-trained multilingual\nLMs. Khemchandani et al. (2021) use lexicons to\nsynthesize monolingual data for adapting LMs, but\ntheir experimentation is limited to several Indian\nlanguages and no attempt was made to synthesize\ndownstream task data while Hu et al. (2021) argue\nthat bilingual lexicons may hurt performance.\nIn this paper, we conduct a systematic study of\nstrategies to leverage this relatively under-studied\nresource of bilingual lexicons to adapt pretrained\nmultilingual models to languages with little or\nno monolingual data. Utilizing lexicons from an\nopen-source database, we create synthetic data\nfor both continued pretraining and downstream\nFigure 2: Results for baselines and adaptation using synthetic\ndata for both resource settings across three NLP tasks.\ntask ﬁne-tuning via word-to-word translation. Em-\npirical results on 19 under-represented languages\non 3 different tasks demonstrate that using syn-\nthetic data leads to signiﬁcant improvements on all\ntasks (Fig. 2), and that the best strategy depends on\nthe availability of monolingual data (§ 5, § 6). We\nfurther investigate methods that improve the qual-\nity of the synthetic data through a small amount of\nparallel data or by model distillation.\n2 Background\nWe focus on the cross-lingual transfer setting where\nthe goal is to maximize performance on a down-\nstream task in a target language T. Due to the\nfrequent unavailability of labeled data in the target\nlanguage, a pretrained multilingual model M is\ntypically ﬁne-tuned on labeled data in the down-\nstream task DS\nlabel = {(xS\ni , yS\ni )}N\ni=1 in a source\nlanguage S where xS\ni is a textual input, yS\ni is the\nlabel, and N is the number of labeled examples.\nThe ﬁne-tuned model is then directly applied to\ntask data DT\ntest = {xT\ni , yT\ni }i in language T at test\ntime.3 The performance on the target language T\ncan often be improved by further adaptation of the\npretrained model.\n2.1 Adaptation with Text\nThere are two widely adopted paradigms for adapt-\ning pretrained models to a target language using\nmonolingual or parallel text.\nMLM Continued pretraining on monolingual\ntext DT\nmono = {xT\ni }i in the target language\n(Howard and Ruder, 2018; Gururangan et al., 2020)\nusing a masked language model (MLM) objective\nhas proven effective for adapting models to the\ntarget language (Pfeiffer et al., 2020). Notably,\nEbrahimi and Kann (2021) show that using as little\nas several thousand sentences can signiﬁcantly im-\nprove the model’s performance on target languages\nnot covered during pretraining.\n3We additionally examine the few-shot setting where some\ntask data DT\nlabel in T is available for ﬁne-tuning in § 7.\n864\nTrans-Train For target languages with sufﬁcient\nparallel text with the source language DST\npar =\n{(xS\ni , xT\ni )}i, one can train a machine translation\n(MT) system that translates data from the source\nlanguage into the target language. Using such an\nMT system, we can translate the labeled data in\nthe source language DS\nlabel into target language\ndata ˆDT\nlabel = {(ˆxT\ni , yS\ni )}N\ni=1, and ﬁne-tune the pre-\ntrained multilingual model on both the source and\ntranslated labeled data DS\nlabel ∪ˆDT\nlabel. This method\noften brings signiﬁcant gains to the target language,\nespecially for languages with high-quality MT sys-\ntems (Hu et al., 2020; Ruder et al., 2021).\n2.2 Challenges with Low-resource Languages\nBoth methods above require DT\nmono or DST\npar in tar-\nget language T, so they cannot be directly extended\nto languages without this variety of data. Joshi et al.\n(2020) classiﬁed the around 7,000 languages of the\nworld into six groups based on the availability of\ndata in each language. The two groups posing the\nbiggest challenges for NLP are:\n“The Left-Behinds,” languages with virtually no\nunlabeled data. We refer to this as the No-Text\nsetting.\n“The Scraping-Bys,” languages with a small\namount of monolingual data. We refer to this\nas the Few-Textsetting.\nThese languages make up 85% of languages in the\nworld, yet they do not beneﬁt from the development\nof pretrained models and adaptation methods due\nto the lack of monolingual and parallel text. In this\npaper, we conduct a systematic study of strategies\ndirectly targeted at these languages.\n3 Adapting to Under-represented\nLanguages Using Lexicons\nSince the main bottleneck of adapting to under-\nrepresented languages is the lack of text, we adopt a\ndata augmentation framework (illustrated in Fig. 3)\nthat leverages bilingual lexicons, which are avail-\nable for a much larger number of languages.\n3.1 Synthesizing Data Using Lexicons\nGiven a bilingual lexicon DST\nlex between the source\nlanguage S and a target language T, we create\nsynthetic sentences ˜xT\ni in T using sentences xS\ni\nin S via word-to-word translation, and use this\nsynthetic data in the following adaptation methods.\nPretrained Model Pseudo MLM Pseudo Trans-train\nS Labeled\nLexicon\nS Mono\nS-T \n Parallel T Mono\nT Pseudo \nMono\nT Pseudo \nLabeled\nLabel Distill\nFigure 3: Pipelines for synthesizing data for both No-text and\nFew-text settings and utilizing extra data for the Few-Text\nsetting. Solid lines indicate adaptation methods and dashed\nlines are synthetic data reﬁnement methods.\nPseudo MLM Using monolingual text DS\nmono =\n{xS\ni }i, we generate pseudo monolingual text\n˜DT\nmono = {˜xT\ni }i for T by replacing the words in\nxS\ni with their translation in T based on the lexicon\nDST\nlex . We keep the words that do not exist in the\nlexicon unchanged, so the pseudo text ˜xT\ni can in-\nclude words in both S and T. We then adapt the\npretrained multilingual model on ˜DT\nmono using the\nMLM objective. For the Few-Text setting where\nsome gold monolingual data DT\nmono is available,\nwe can train the model jointly on the pseudo and\nthe gold monolingual data ˜DT\nmono ∪DT\nmono.\nPseudo Trans-train Given the source labeled\ndata DS\nlabel = {(xS\ni , yS\ni )}N\ni=1, for each text exam-\nple xS\ni we use DST\nlex to replace the words in xS\ni\nwith its corresponding translation in T, resulting in\npseudo labeled data ˜DT\nlabel = {(˜xT\ni , yS\ni )}N\ni=1. We\nkeep the original word if it does not have an entry in\nthe lexicon. We then ﬁne-tune the model jointly on\nboth pseudo and gold labeled data ˜DT\nlabel ∪DS\nlabel.\nSince these methods only require bilingual lexi-\ncons, we can apply them to both No-Text and Few-\nText settings. We can use either of the two methods\nor the combination of both to adapt the model.\nChallenges with Pseudo Data Our synthetic\ndata ˜DT could be very different from the true data\nDT because the lexicons do not cover all words\nin S or T, and we do not consider morphologi-\ncal or word order differences between T and S.4\nNonetheless, we ﬁnd that this approach yields sig-\nniﬁcant improvements in practice (see Tab. 3). We\nalso outline two strategies that aim to improve the\nquality of the synthetic data in the next section.\n3.2 Reﬁning the Synthetic Data\nLabel Distillation The pseudo labeled data\n˜DT\nlabel = {(˜xT\ni , yS\ni )}N\ni=1 is noisy because the syn-\n4In fact, we considered more sophisticated methods using\nmorphological analyzers and inﬂectors, but even models with\nrelatively broad coverage (Anastasopoulos and Neubig, 2019)\ndid not cover many languages we used in experiments.\n865\nengxS∈DSmono Anarchism calls for the abolition of the state , which it holds to be undesirable , unnecessary , and harmful .\nPseudo Mono˜xT∈˜DTmono Anarchism calls gal il abolition ta’ il stat , lima hi holds gal tkun undesirable , bla bzonn , u harmful .\nengxS∈DSlabel I suspect the streets of Baghdad will look as if a war is looming this week .\nPseudo Labeled˜xT∈˜DTlabeljien iddubita il streets ta’ Bagdad xewqa hares kif jekk a gwerra is looming dan˙gimga .\nPseudo LabeledyS∈˜DTlabel PRON VERB DET NOUN ADP PROPN AUX VERB SCONJ SCONJ DET NOUN AUX VERB DET NOUN PUNCT\nLabel Distilled˜yT∈˜DTdistillPRON VERB DET NOUN ADP PROPN NOUN NOUN SCONJ SCONJ DET NOUN AUX VERB DET NOUN PUNCT\nTable 1: Examples of pseudo monolingual data and pseudo labeled data for POS tagging for Maltese (mlt). Words in red have\ndifferent labels between the source language and the label distilled data. This is because “xewqa” in Maltese is a noun meaning\n“desire,will”, while the word “will” is not used as a noun in the original English sentence.\nthetic examples ˜xT\ni could have a different label\nfrom the original label yS\ni (see Tab. 1). To alleviate\nthis issue, we propose to automatically “correct”\nthe labels of pseudo data using a teacher model.\nSpeciﬁcally, we ﬁne-tune the pretrained multilin-\ngual model as a teacher model using only DS\nlabel.\nWe use this model to generate the new pseudo la-\nbeled data ˜DT\ndistill = {(˜xT\ni , ˜yT\ni )}N\ni=1 by predicting\nlabels ˜yT\ni for the pseudo task examples ˜xT\ni . We\nthen ﬁne-tune the pretrained model on both the\nnew pseudo labeled data and the source labeled\ndata ˜DT\ndistill ∪DS\nlabel.\nInduced Lexicons with Parallel Data For the\nFew-Text setting, we can leverage the available par-\nallel data DST\npar to further improve the quality of the\naugmented data. Speciﬁcally, we use unsupervised\nword alignment to extract additional word pairs\n˜DST\nlex from the parallel data, and use the combined\nlexicon ˜DST\nlex ∪DST\nlex to synthesize the pseudo data.\n4 General Experimental Setting\nIn this section, we outline the tasks and data setting\nused by all experiments. We will then introduce\nthe adaptation methods and results for the No-Text\nsetting in § 5 and the Few-Text setting in § 6.\n4.1 Tasks, Languages and Model\nWe evaluate on the gold test sets of three differ-\nent tasks with relatively good coverage of under-\nrepresented languages: named entity recogni-\ntion (NER), part-of-speech (POS) tagging, and de-\npendency parsing (DEP). We use two NER datasets:\nWikiAnn NER (Pan et al., 2017; Rahimi et al.,\n2019) and MasakhaNER (Adelani et al., 2021). We\nuse the Universal Dependency 2.5 (Nivre et al.,\n2018) dataset for both the POS and DEP tasks.\nWe use English as the source language for all\nexperiments. For each dataset, we use the English\ntraining data and select the checkpoint with the\nbest performance on the English development set.\nFor MasakhaNER, which does not have English\ntraining data, we follow Adelani et al. (2021) and\nLanguage iso Family Task Lex Count\nAcehnese ace AustronesianNER 0.5k\nBashkir bak Turkic NER 3.4k\nCrimean Turkishcrh Turkic NER 4.4k\nHakka Chinesehak Sino-TibetanNER 8.5k\nIgbo ibo Niger-CongoNER 3.6k\nIlokano ilo AustronesianNER 4.0k\nKinyarwandakin Niger-CongoNER 4.7k\nEastern Marimhr Uralic NER 21.7k\nMaltese mlt Afro-AsiaticAll 1.0k\nMaori mri AustronesianNER 13.8k\nHausa hau Niger-CongoNER 5.6k\nWolof wol Niger-CongoAll 1.9k\nLuganda lug Niger-CongoNER 3.5k\nLuo luo NER 0.7k\nBambara bam Mande POS,Parsing4.4k\nManx glv Indo-EuropeanPOS,Parsing37.6k\nAncient Greekgrc Indo-EuropeanPOS,Parsing8.0k\nSwiss Germangsw Indo-EuropeanPOS,Parsing2.5k\nErzya myv Uralic POS,Parsing7.4k\nTable 2: Languages used for evaluation.\nuse the CoNLL-2003 English NER training data.\nWe run each ﬁne-tuning experiment with 3 ran-\ndom seeds and report the average performance. For\nNER and POS tagging, we follow the data process-\ning and ﬁne-tuning hyper-parameters in Hu et al.\n(2020). We use the Udify (Kondratyuk and Straka,\n2019) codebase and conﬁguration for parsing.\nLanguages For each task, we select languages\nthat have task data but are not covered by the\nmBERT pretraining data. The languages we use\ncan be found in Tab. 2. Most fall under the Few-\nText setting (Joshi et al., 2020). We employ the\nsame languages to simulate the No-Text setting as\nwell.\nModel We use the multilingual BERT\nmodel (mBERT) because it has competitive perfor-\nmance on under-represented languages (Pfeiffer\net al., 2020). We ﬁnd that our mBERT performance\non WikiNER and POS is generally comparable or\nexceeds the XLM-R large results in Ebrahimi and\nKann (2021). We additionally verify our results\nalso hold for XLM-R in § 7.\n866\n4.2 Adaptation Data\nLexicon We extract lexicons between English\nand each target language from the PanLex\ndatabase.5 The number of lexicon entries varies\nfrom about 0.5k to 30k, and most of the lexicons\nhave around 5k entries. The lexicon statistics for\neach language can be found in Tab. 2.\nPseudo Monolingual Data English Wikipedia\narticles are used to synthesize monolingual data.\nWe ﬁrst tokenize the English articles using\nStanza (Qi et al., 2020) and keep the ﬁrst 200k\nsentences. To create pseudo monolingual data for\na given target language, we replace each English\nword with its translation if the word exists in the\nbilingual lexicon. We randomly sample a target\nword if the English word has multiple possible\ntranslations because it is difﬁcult to estimate trans-\nlation probabilities due to lack of target text.\nPseudo Labeled Data Using the English train-\ning data for each task, we simply replace each En-\nglish word in the labeled training data with its cor-\nresponding translation and retain its original label.\nFor the sake of simplicity, we only use lexicon\nentries with a single word.\n5 No-Text Setting\nWe analyze the results of the following adaptation\nmethods for the setting where we do not have any\nmonolingual data.\nPseudo MLM The mBERT model is trained on\nthe pseudo monolingual data using the MLM ob-\njective. We train the model for 5k steps for the\nNER tasks and 10k steps for the POS tagging and\nParsing tasks.\nPseudo Trans-train We ﬁne-tune mBERT or the\nmodel adapted with Pseudo MLM for a down-\nstream task on the concatenation of both the En-\nglish labeled data and the pseudo labeled data.\nLabel Distillation We use the model adapted\nwith Pseudo MLM as the teacher model to gen-\nerate new labels for the pseudo labeled data, which\nwe use jointly with the English labeled data to ﬁne-\ntune the ﬁnal model.\n5.1 Results\nThe average performance of different adaptation\nmethods averaged across all languages in each task\n5https://panlex.org/snapshot/\ncan be found in Tab. 3.\nPseudo Trans-train is the best method for No-\nText. Pseudo MLM and Pseudo Trans-train can\nboth bring signiﬁcant improvements over the\nmBERT baseline for all tasks. Pseudo Trans-train\nleads to the best aggregated result across all tasks,\nand it is also the best method or very close to the\nbest method for each task. Adding Pseudo Trans-\ntrain on top of Pseudo MLM does not add much\nimprovement. Label Distillation generally leads to\nbetter performance, but overall it is comparable to\nonly using Pseudo Trans-train.\n6 Few-Text Setting\nWe test same adaptation methods introduced in § 5\nfor the Few-Text setting where we have a small\namount of gold data. First we introduce the addi-\ntional data and adaptation methods for this setting.\n6.1 Gold Data\nGold Monolingual Data We use the JHU Bible\nCorpus (McCarthy et al., 2020) as the monolingual\ndata. Following the setup in Ebrahimi and Kann\n(2021), we use the verses from the New Testament,\nwhich contain 5000 to 8000 sentences for each\ntarget language.\nGold Parallel Data We can use the parallel data\nbetween English and the target languages from the\nBible to extract additional word pairs. We use an\nexisting unsupervised word alignment tool, eﬂo-\nmal (Östling and Tiedemann, 2016), to generate\nword alignments for each sentence in the parallel\nBible data. To create high quality lexicon entries,\nwe only keep the word pairs that are aligned more\nthan once, resulting in about 2k extra word pairs\nfor each language. We then augment the PanLex\nlexicons with the induced lexicon entries.\n6.2 Adaptation Methods\nGold MLM The mBERT model is trained on the\ngold monolingual Bible data in the target language\nusing the MLM objective. Following the setting in\nEbrahimi and Kann (2021), we train for 40 epochs\nfor the NER task, and 80 epochs for the POS and\nParsing tasks.\nPseudo MLM We conduct MLM training on\nboth the Bible monolingual data and the pseudo\nmonolingual data in the target language. The Bible\ndata is up-sampled to match the size of the pseudo\nmonolingual data. We train the model for 5k steps\n867\nMethod Lexicon WikiNER∆ MasakhaNER∆ POS ∆ Parsing ∆ Avg. ∆\nNo-Text\nmBERT - 47.6 - 46.1 - 36.1 - 16.5 - 36.5 -\nPseudo Trans-trainPanLex 49.8 2.2 54.4 8.3 51.1 15.0 25.9 9.4 45.2∗ 8.7\nPseudo MLM PanLex 49.8 2.2 52.6 6.5 48.9 12.8 25.2 8.7 44.1∗ 7.6\nBoth PanLex 48.5 0.9 54.6 8.5 48.7 12.6 25.9 9.4 44.4∗ 7.9\nBoth+Label DistillationPanLex 50.6 2.1 53.5 -1.1 50.3 1.6 26.0 0.1 45.1∗ 0.7\nFew-Text\nGold MLM - 49.5 - 53.6 - 60.6 - 40.2 - 50.9 -\nPseudo Trans-trainPanLex 50.2 0.7 59.4 5.8 59.3 -1.3 37.0 -3.2 51.4 0.5\nPseudo MLM PanLex 50.7 1.2 57.4 3.8 65.4 4.8 43.5 3.3 54.2∗ 3.3\nPanLex+Induced 52.2 1.5 58.5 0.9 64.7 -0.7 41.5 -2.0 54.2∗ 0.0\nBoth PanLex 50.1 0.6 59.2 5.6 60.7 0.1 38.3 -1.9 52.0∗ 1.1\nPanLex+Induced 52.6 2.5 61.1 1.9 59.5 -1.2 35.3 -3.0 52.0† 0.0\nBoth+Label DistillationPanLex 51.7 1.6 58.4 -0.8 66.2 5.5 41.9 3.6 54.5∗ 2.5\nPanLex+Induced 53.2 1.5 59.4 1.0 65.8 -0.4 40.7 -1.2 54.7∗ 0.2\nTable 3: Average F1 score for languages in each task. We record F1 of the LAS for Parsing. We compare three adaptation\nmethods (∆ indicates gains over baselines): Pseudo Trans-train, Pseudo MLM, and Both. We also examine two data reﬁnement\nmethods: Label Distillation (∆ is gains over Both) and PanLex+Induced (∆ is gains over PanLex). Bold is the best result for\neach dataset, and underline indicates the best improvements among the three adaptation methods over the baselines. We test the\nsigniﬁcance of the average gains over the baselines in the last column using paired bootstrap resampling. * indicates signiﬁcant\ngains with p <0.001 and †indicates signiﬁcant gains with p <0.05.\nfor the NER task and 10k steps for the POS tagging\nand Parsing tasks.\n6.3 Results\nThe average performance in each task for Few-Text\ncan be found in Tab. 3.\nPseudo MLM is the competitive strategy for\nFew-Text. Unlike the No-Text setting, Pseudo\nTrans-train only marginally improves or even de-\ncreases the performance for three out of the four\ndatasets we consider. On the other hand, Pseudo\nMLM, which uses both gold and pseudo mono-\nlingual data for MLM adaptation, consistently and\nsigniﬁcantly improves over Gold MLM for all tasks.\nAgain, using Pseudo Trans-train on top of Pseudo\nMLM does not help and actually leads to relatively\nlarge performance loss for the syntactic tasks, such\nas POS tagging and Parsing.\nLabel Distillation brings signiﬁcant improve-\nments for the two syntactic tasks. Notably, it\nis the best performing method for POS tagging,\nbut it still lags behind Pseudo MLM for Parsing.\nThis is likely because Parsing is a much harder task\nthan POS tagging to generate correct labels. The\neffect of Label Distillation on the NER task is less\nconsistent—it improves over Pseudo Trans-train\nfor WikiNER but not for MasakhaNER. This is\nbecause the named entity tags of the same words\nin different languages likely remain the same so\nthat the pseudo task data probably has less noise\nfor Label Distillation to have consistent beneﬁts.\nPROPNNOUN\nADJVERBADVPRONOther\n0.0\n0.1\n0.2\n0.3\n0.4\nRatio\nBible Induced\nPanLex\nPROPNNOUN\nADJ VERBADVPRON\n−0.04\n−0.02\n0.00\n0.02\nAcc. gain by\ninduced lexicons\nFigure 4: left: Ratio of words with different POS tags in each\nlexicon. right: POS accuracy gain of test words with different\nPOS tags by using induced lexicons. The induced lexicons\nhave more verbs but lead to worse performance on verbs.\nAdding Induced Lexicons We examine the ef-\nfect of using the lexicons augmented by word pairs\ninduced from the Bible parallel data. The results\ncan be found in Tab. 3. Adding the induced lexi-\ncon signiﬁcantly improves the NER performance,\nwhile it hurts the two syntactic tasks.\nTo understand what might have prevented the\nsyntactic tasks from beneﬁting from the extra lex-\nicon entries, we plot the distribution of the part-\nof-speech tags of the words in PanLex lexicons\nand the lexicons induced from the Bible in Fig. 4.\nPanLex lexicons have more nouns than the Bible\nlexicons while the Bible lexicons cover more verbs\nthan PanLex. However, the higher verb coverage\nin induced lexicons actually leads to a larger pre-\ndiction accuracy drop for verbs in the POS tagging\ntask. We hypothesize that the pseudo monolingual\ndata created using the induced lexicons would con-\ntain more target language verbs with the wrong\nword order, which could be more harmful for syn-\ntactic tasks than tasks that are less sensitive to word\norder such as NER.\n868\nbam glv mlt myv\nGold MLM (Ours) 59.7 64.1 58.5 70.6\nEbrahimi and Kann (2021)60.5 59.7 59.6 66.6\n+Pseudo Trans-train57.4 63.2 69.1 63.8\n+Pseudo MLM 68.5 67.5 72.3 73.8\n+Both 60.3 64.5 69.3 65.9\n+Both(Label Distillation)69.4 68.8 72.1 74.3\nTable 4: Results for POS tagging with XLM-R. Our methods\nfollow similar trend as on mBERT and they lead to signiﬁcant\ngains compared to prior work.\nDiscrepancies between the two NER datasets\nWhile WikiNER, along with POS tagging and Pars-\ning, beneﬁt the most from Pseudo MLM for Few-\nText, MasakhaNER achieves the best result with\nPseudo Trans-train. One possible explanation is\nthat MasakhaNER contains data from the news do-\nmain, while WikiNER is created from Wikipedia.\nThe pseudo monolingual data used for MLM is cre-\nated from English Wikipedia articles, which could\nbeneﬁt WikiNER much more than MasakhaNER.\nOn the other hand, the English NER training data\nfor MasakhaNER is from the news domain, which\npotentially makes Pseudo Trans-train a stronger\nmethod for adapting the model simultaneously to\nthe target language and to the news domain. One\nadvantage of Pseudo MLM is that the English\nmonolingual data is much cheaper to acquire, while\nPseudo Trans-train is constrained by the amount\nof labeled data for a task. We show in § A.4 that\nPseudo MLM has more beneﬁt for MasakhaNER\nwhen we use a subset of the NER training data.\n7 Analyses\nPerformance with XLM-R We mainly use\nmBERT because it has competitive performance\nfor under-represented languages and it is more com-\nputationally efﬁcient due to the smaller size. Here\nwe verify our methods have the same trend when\nused on a different model XLM-R (Conneau et al.,\n2020). We focus on a subset of languages in the\nPOS tagging task for the Few-Text setting and the\nresults are in Tab. 4. We use the smaller XLM-R\nbase for efﬁciency, and compare to the best result\nin prior work, which uses XLM-R large (Ebrahimi\nand Kann, 2021). Tab. 4 shows that our baseline is\ncomparable or better than prior work. Similar to\nthe conclusion in § 6, Pseudo MLM is the competi-\ntive strategy that brings signiﬁcant improvements\nover prior work. While adding Pseudo Trans-train\nto Pseudo MLM does not help, using Label Distil-\nlation further improves the performance.\n33\nwol\n35\nluo\n48\nhau\n50\nlug\n52\nkin\n55\nibo\n0\n5\n10\n15\n20F1 gain over mBERT\nMasakhaNER\nPseudo\nTrans-train\nPseudo\nMlm\n21\nmlt\n29\nwol\n32\nglv\n32\nbam\n34\ngrc\n40\nmyv\n60\ngsw\n−10\n0\n10\n20\n30\nF1 gain over mBERT\nPOS\nFigure 5: F1 gain over the baselines for languages with in-\ncreasing baseline performance from left to right. Pseudo data\ntends to help more for languages with lower performance.\nEffect of Baseline Performance Using pseudo\ndata might be especially effective for languages\nwith lower performance. We plot the improvement\nof different languages over the baseline in Fig. 5,\nwhere languages are arranged with increasing base-\nline performance from left to right. We mainly plot\nPseudo MLM and Pseudo Trans-train for simplicity.\nFig. 5 shows that for both resource settings, lower\nperforming languages on the left tend to have more\nperformance improvement by using pseudo data.\nUsing NMT Model to Synthesize Data One\nproblem with the pseudo data synthesized using\nword-to-word translation is that it cannot capture\nthe correct word order or syntactic structure in the\ntarget language. If we have a good NMT system\nthat translates English into the target language, we\nmight be able to get more natural pseudo monolin-\ngual data by translating the English sentences to\nthe target language.\nSince the target languages we consider are usu-\nally not supported by popular translation services,\nwe train our own NMT system by ﬁne-tuning an\nopen sourced many-to-many NMT model on the\nBible parallel data from English to the target lan-\nguage (details in § A.2). Instead of creating pseudo\nmonolingual data using the lexicon, we can simply\nuse the ﬁne-tuned NMT model to translate English\nmonolingual data into the target language.\nThe results of using NMT as opposed to lexicon\nfor Pseudo MLM on all four tasks can be found in\nTab. 5. Unfortunately, NMT is consistently worse\nthan word-to-word translation using lexicons. We\nﬁnd that the translated monolingual data tend to\nhave repeated words and phrases that are common\nin the Bible data, although the source sentence is\nfrom Wikipedia. This is because the NMT model\noverﬁts to the Bible data, and it fails to generate\ngood translation for monolingual data from a dif-\nferent domain such as Wikipedia.\nComparison to Few-shot Learning Lauscher\net al. (2020) found that using as few as 10 labeled\n869\nWikiNERMasakaNERPOS Parsing\nLexicon 45.0 56.0 63.7 40.7\nNMT 42.2 55.8 58.9 37.7\nTable 5: F1 of using Pseudo MLM for Few-Text. Synthesizing\ndata with NMT is consistently worse.\nMethod hau wol lug ibo kin luo\nmBERT 48.7 33.9 50.9 55.2 52.4 35.3\nBest Adapted 74.4 60.3 61.6 63.6 63.8 42.6\n10-shot 44.5 49.1 52.7 56.2 51.2 46.2\n100-shot 64.0 56.9 58.3 65.5 55.7 51.6\nBest Adapt+100-shot76.1 57.3 61.3 63.2 62.6 49.4\nTable 6: Results on MasakhaNER for k-shot learning. We\ncompare to the zero-shot mBERT baseline and our best\nadapted model.\nexamples in the target language can signiﬁcantly\noutperform the zero-shot transfer baseline for lan-\nguages included in mBERT. We focus on the zero-\nshot setting in this paper because the languages\nwe consider have very limited data and it could\nbe expensive or unrealistic to annotate data in ev-\nery task for thousands of languages. Nonetheless,\nwe experiment with k-shot learning to examine\nits performance on low-resource languages in the\nMasakhaNER task. Tab. 6 shows that using 10\nlabeled examples brings improvements over the\nmBERT baseline for a subset of the languages, and\nit is mostly worse than our best adapted model\nwithout using any labeled data. When we have\naccess to 100 examples, few-shot learning begins\nto reach or exceed our zero-shot model. In gen-\neral, few-shot learning seems to require more data\nto consistently perform well for under-represented\nlanguages while our adaptation methods bring con-\nsistent gains without any labeled data. Combining\nthe best adapted model with few-shot learning leads\nto mixed results. More research is needed to under-\nstand the annotation cost and beneﬁt of few-shot\nlearning for low-resource languages.\n8 Related Work\nSeveral methods have been proposed to adapt pre-\ntrained language models to a target language. Most\nof them rely on MLM training using monolingual\ndata in the target languages (Wang et al., 2020;\nChau et al., 2020; Muller et al., 2021; Pfeiffer\net al., 2020; Ebrahimi and Kann, 2021), compet-\nitive NMT systems trained on parallel data (Hu\net al., 2020; Ponti et al., 2021), or some amount of\nlabeled data in the target languages (Lauscher et al.,\n2020). These methods cannot be easily extended to\nlow-resource languages with no or limited amount\nof monolingual data, which account for more than\n80% of the World’s languages (Joshi et al., 2020).\nBilingual lexicons have been commonly used for\nlearning cross-lingual word embeddings (Mikolov\net al., 2013; Ruder et al., 2019). Among these,\nsome work uses lexicons to synthesize pseudo bilin-\ngual (Gouws and Søgaard, 2015; Duong et al.,\n2016) or pseudo multilingual corpora (Ammar\net al., 2016). Mayhew et al. (2017) propose to\nsynthesize task data for NER using bilingual lexi-\ncons. More recently, Khemchandani et al. (2021)\nsynthesize monolingual data in Indian languages\nfor adapting pretrained language models via MLM.\nHu et al. (2021) argue that using bilingual lexi-\ncons for alignment hurts performance compared\nto word-level alignment based on parallel corpora.\nSuch parallel corpora, however, are not available\nfor truly under-represented languages. Reid and\nArtetxe (2021) employ a dictionary denoising ob-\njective where a word is replaced with its translation\ninto a random language with a certain probability.\nThis can be seen as text-to-text variant of our ap-\nproach applied to multilingual pre-training. None\nof the above works provide a systematic study of\nmethods that utilize lexicons and limited data re-\nsources for adapting pretrained language models to\nlanguages with no or limited text.\n9 Conclusion and Discussion\nWe propose a pipeline that leverages bilingual\nlexicons, an under-studied resource with much\nbetter language coverage than conventional data,\nto adapt pretrained multilingual models to under-\nrepresented languages. Through comprehensive\nstudies, we ﬁnd that using synthetic data can signif-\nicantly boost the performance of these languages\nwhile the best method depends on the data avail-\nability. Our results show that we can make concrete\nprogress towards including under-represented lan-\nguages into the development of NLP systems by\nutilizing alternative data sources.\nOur work also has some limitations. Since we\nfocus on different methods of using lexicons, we\nrestrict experiments to languages in Latin script\nand only use English as the source language for\nsimplicity. Future work could explore the effect\nof using different source languages and combining\ntransliteration (Muller et al., 2021) or vocabulary\nextension (Pfeiffer et al., 2021) with lexicon-based\ndata augmentation for languages in other scripts.\nWe also did not test the data augmentation methods\non higher-resourced languages as MLM ﬁne-tuning\n870\nand translate-train are already effective in that set-\nting and our main goal is to support the languages\nwith little textual data. Nonetheless, it would be\ninteresting to examine whether our methods can de-\nliver gains for high-resource languages, especially\nfor test data in specialized domains.\nWe point to the following future directions: First,\nphrases instead of single word entries could be used\nto create pseudo data. Second, additional lexicons\nbeyond PanLex could be leveraged.6 Third, more\neffort could be spent on digitizing both existing\nmonolingual data such as books (Gref, 2016) and\nlexicons into a format easily accessible by NLP\npractitioners. Although PanLex already covers over\n5000 languages, some language varieties have only\nas little as 10 words in the database, while there ex-\nist many paper dictionaries that could be digitized\nthrough technologies such as OCR (Rijhwani et al.,\n2020).7 Lexicon collection is also relatively fast,\nwhich could be a more cost effective strategy to\nsigniﬁcantly boost the performance of many lan-\nguages without lexicons. Finally, the quality of\nsynthetic data could be improved by incorporating\nmorphology. However, we ﬁnd that there is vir-\ntually no existing morphological analysis data or\ntoolkits for the languages we consider. Future work\ncould aim to improve the morphological analysis\nof these low-resource languages.\nAcknowledgements\nThis work was supported in part by the National\nScience Foundation under Grant Numbers 1761548\nand 2040926. XW was supported in part by an\nApple Graduate Fellowship. The authors would\nlike to thank Aditi Chaudhary, Arya McCarthy,\nShruti Rijhwani for discussions about the project,\nand Daan van Esch for the general feedback and\npointing out additional linguistic resources.\nReferences\nDavid Ifeoluwa Adelani, Jade Abbott, Graham Neu-\nbig, Daniel D’souza, Julia Kreutzer, Constantine\nLignos, Chester Palen-Michel, Happy Buzaaba,\nShruti Rijhwani, Sebastian Ruder, Stephen May-\nhew, Israel Abebe Azime, Shamsuddeen Muham-\nmad, Chris Chinenye Emezue, Joyce Nakatumba-\nNabende, Perez Ogayo, Anuoluwapo Aremu,\nCatherine Gitau, Derguene Mbaye, Jesujoba Al-\nabi, Seid Muhie Yimam, Tajuddeen Gwadabe, Ig-\n6We provide a list of resources in Appendix A.5.\n7https://panlex.org/source-list/ contains\na list of undigitized dictionaries.\nnatius Ezeani, Rubungo Andre Niyongabo, Jonathan\nMukiibi, Verrah Otiende, Iroro Orife, Davis\nDavid, Samba Ngom, Tosin Adewumi, Paul\nRayson, Mofetoluwa Adeyemi, Gerald Muriuki,\nEmmanuel Anebi, Chiamaka Chukwuneke, Nkiruka\nOdu, Eric Peter Wairagala, Samuel Oyerinde,\nClemencia Siro, Tobius Saul Bateesa, Temilola\nOloyede, Yvonne Wambui, Victor Akinode, Deb-\norah Nabagereka, Maurice Katusiime, Ayodele\nAwokoya, Mouhamadane MBOUP, Dibora Gebrey-\nohannes, Henok Tilaye, Kelechi Nwaike, Degaga\nWolde, Abdoulaye Faye, Blessing Sibanda, Ore-\nvaoghene Ahia, Bonaventure F. P. Dossou, Kelechi\nOgueji, Thierno Ibrahima DIOP, Abdoulaye Diallo,\nAdewale Akinfaderin, Tendai Marengereke, and Sa-\nlomey Osei. 2021. Masakhaner: Named entity\nrecognition for african languages. In TACL.\nWaleed Ammar, George Mulcaire, Yulia Tsvetkov,\nGuillaume Lample, Chris Dyer, and Noah A Smith.\n2016. Massively multilingual word embeddings.\narXiv preprint arXiv:1602.01925.\nAntonios Anastasopoulos and Graham Neubig. 2019.\nPushing the limits of low-resource morphological in-\nﬂection. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), Hong\nKong, China. Association for Computational Lin-\nguistics.\nSteven Bird. 2020. Decolonising speech and lan-\nguage technology. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics ,\npages 3504–3519, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nDamián Blasi, Antonios Anastasopoulos, and Gra-\nham Neubig. 2021. Systematic inequalities in lan-\nguage technology performance across the world’s\nlanguages. arXiv preprint arXiv:2110.06733.\nBrenda Boerger. 2017. Rapid word collection, dictio-\nnary production, and community well-being.\nIsaac Caswell, Theresa Breiner, Daan van Esch, and\nAnkur Bapna. 2020. Language ID in the wild:\nUnexpected challenges on the path to a thousand-\nlanguage web text corpus. In COLING.\nEthan C. Chau, Lucy H. Lin, and Noah A. Smith. 2020.\nParsing with multilingual BERT, a small corpus, and\na small treebank. In Findings of EMNLP 2020.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nACL.\nAlexis Conneau and Guillaume Lample. 2019.\nCrosslingual language model pretraining. In\nNeurIPS.\n871\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL.\nLong Duong, Hiroshi Kanayama, Tengfei Ma, Steven\nBird, and Trevor Cohn. 2016. Learning crosslingual\nword embeddings without bilingual corpora. arXiv\npreprint arXiv:1606.09403.\nAbteen Ebrahimi and Katharina Kann. 2021. How to\nadapt your pretrained multilingual model to 1600\nlanguages. In ACL, Online. Association for Com-\nputational Linguistics.\nJost Gippert, Nikolaus Himmelmann, Ulrike Mosel,\net al. 2006. Essentials of language documentation .\nMouton de Gruyter Berlín.\nStephan Gouws and Anders Søgaard. 2015. Simple\ntask-speciﬁc bilingual word embeddings. In Pro-\nceedings of the 2015 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n1386–1390, Denver, Colorado. Association for Com-\nputational Linguistics.\nEmily Kennedy Gref. 2016. Publishing in North Amer-\nican Indigenous Languages . Ph.D. thesis, Univer-\nsity of London.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nACL, Online.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nLanguage Model Fine-tuning for Text Classiﬁcation.\nIn Proceedings of ACL 2018.\nJunjie Hu, Melvin Johnson, Orhan Firat, Aditya Sid-\ndhant, and Graham Neubig. 2021. Explicit Align-\nment Objectives for Multilingual Bidirectional En-\ncoders. In Proceedings of NAACL 2021.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Graham\nNeubig, Orhan Firat, and Melvin Johnson. 2020.\nXtreme: A massively multilingual multi-task bench-\nmark for evaluating cross-lingual generalization. In\nICML.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the NLP\nworld. In ACL, Online. Association for Computa-\ntional Linguistics.\nYash Khemchandani, Sarvesh Mehtani, Vaidehi Patil,\nAbhijeet Awasthi, Partha Talukdar, and Sunita\nSarawagi. 2021. Exploiting language relatedness for\nlow web-resource language model adaptation: An\nIndic languages study. In ACL, Online. Association\nfor Computational Linguistics.\nDan Kondratyuk and Milan Straka. 2019. 75 lan-\nguages, 1 model: Parsing universal dependencies\nuniversally. In EMNLP, Hong Kong, China.\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli ´c, and\nGoran Glavaš. 2020. From zero to hero: On the lim-\nitations of zero-shot language transfer with multilin-\ngual Transformers. In EMNLP, Online. Association\nfor Computational Linguistics.\nStephen Mayhew, Chen-Tse Tsai, and Dan Roth. 2017.\nCheap translation for cross-lingual named entity\nrecognition. In EMNLP, Copenhagen, Denmark. As-\nsociation for Computational Linguistics.\nArya D. McCarthy, Rachel Wicks, Dylan Lewis, Aaron\nMueller, Winston Wu, Oliver Adams, Garrett Nico-\nlai, Matt Post, and David Yarowsky. 2020. The\nJohns Hopkins University Bible corpus: 1600+\ntongues for typological exploration. In LREC, pages\n2884–2892, Marseille, France. European Language\nResources Association.\nTomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013.\nExploiting similarities among languages for ma-\nchine translation. arXiv preprint arXiv:1309.4168.\nBenjamin Muller, Antonios Anastasopoulos, Benoît\nSagot, and Djamé Seddah. 2021. When being un-\nseen from mBERT is just the beginning: Handling\nnew languages with multilingual language models.\nIn NAACL, Online.\nJoakim Nivre, Mitchell Abrams, Željko Agi ´c, Lars\nAhrenberg, Lene Antonsen, Maria Jesus Aranzabe,\nGashaw Arutie, Masayuki Asahara, Luma Ateyah,\nMohammed Attia, et al. 2018. Universal dependen-\ncies 2.2.\nRobert Östling and Jörg Tiedemann. 2016. Efﬁ-\ncient word alignment with Markov Chain Monte\nCarlo. Prague Bulletin of Mathematical Linguistics,\n106:125–146.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nNAACL-HLT 2019: Demonstrations.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel\nNothman, Kevin Knight, and Heng Ji. 2017. Cross-\nlingual name tagging and linking for 282 languages.\nIn ACL, pages 1946–1958, Vancouver, Canada.\nACL.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn EMNLP, Online. Association for Computational\nLinguistics.\nJonas Pfeiffer, Ivan Vuli´c, Iryna Gurevych, and Sebas-\ntian Ruder. 2021. UNKs Everywhere: Adapting\nMultilingual Language Models to New Scripts. In\nProceedings of EMNLP 2021.\n872\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In ACL,\nFlorence, Italy.\nEdoardo Maria Ponti, Julia Kreutzer, Ivan Vuli ´c, and\nSiva Reddy. 2021. Modelling latent translations for\ncross-lingual transfer. In Arxiv.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton,\nand Christopher D. Manning. 2020. Stanza: A\nPython natural language processing toolkit for many\nhuman languages. In ACL.\nAfshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Mas-\nsively multilingual transfer for NER. In ACL, pages\n151–164, Florence, Italy. Association for Computa-\ntional Linguistics.\nMachel Reid and Mikel Artetxe. 2021. PAR-\nADISE: Exploiting Parallel Data for Multilingual\nSequence-to-Sequence Pretraining. arXiv preprint\narXiv:2108.01887.\nShruti Rijhwani, Antonios Anastasopoulos, and Gra-\nham Neubig. 2020. OCR Post Correction for En-\ndangered Language Texts. In EMNLP, Online. As-\nsociation for Computational Linguistics.\nSebastian Ruder, Noah Constant, Jan Botha, Aditya\nSiddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Jun-\njie Hu, Graham Neubig, and Melvin Johnson. 2021.\nXTREME-R: Towards More Challenging and Nu-\nanced Multilingual Evaluation. In Proceedings of\nEMNLP 2021.\nSebastian Ruder, Ivan Vuli ´c, and Anders Søgaard.\n2019. A survey of cross-lingual word embedding\nmodels. Journal of Artiﬁcial Intelligence Research ,\n65:569–631.\nZihan Wang, Karthikeyan K, Stephen Mayhew, and\nDan Roth. 2020. Extending multilingual BERT to\nlow-resource languages. In EMNLP-Findings, On-\nline. Association for Computational Linguistics.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, becas:\nThe surprising cross-lingual effectiveness of BERT.\nIn EMNLP.\n873\n1k luo2k wol3k lug4k ibo5k kin5k hau\n0.0\n2.5\n5.0\nF1 gain over PanL \nMasakhaNER\n2k wol2k gsw4k bam7k myv8k grc10k mlt37k glv\n−2\n−1\n0\nPOS\nFigure 6: Improvements of using combined lexicons compared\nto PanLex lexicons for Pseudo MLM. Languages with fewer\nPanLex lexicons tend to beneﬁt more from the combined\nlexicons.\nA Appendix\nA.1 Experiment Details\nFor all experiments using MLM training for NER\ntasks, we train 5000 steps, or about equivalent to\n40 epochs on Bible; for MLM training for POS\ntagging and Parsing, we train 10000 steps, or equiv-\nalent to 80 epochs on Bible. We use learning rate\nof 2e−5, batch size of 32, and maximum sequence\nlength of 128. We did not tune these hyperparame-\nters because we mostly follow the ones provided in\n(Ebrahimi and Kann, 2021).\nTo ﬁnetune the model for a downstream task,\nwe use learning rate of 2e −5 and batch size of\n32. We train all models for 10 epochs and pick\nthe checkpoint with the best performance on the\nEnglish development set.\nWe use a single GPU for all adaptation and ﬁne-\ntuning experiments. Pseudo MLM usually takes\nless than 5 hours. Pseudo Trans-train and other\ntask speciﬁc ﬁne-tuning usually takes around 2 to\n3 hours.\nA.2 NMT Models\nWe use the many-to-many NMT models provided\nin the fairseq repoo (Ott et al., 2019). We use\nthe model with 175M parameters and ﬁnetune the\nNMT model for 50 epochs on the parallel data from\nthe Bible.\nWe use beam size of 5 to generate translations.\nA.3 Induced lexicons help languages with\nFewer PanLex Entries\nWe plot the performance difference between using\ncombined lexicons and PanLex for the Few-Text\nin Fig. 6. The languages are arranged from left to\nright based on increasing amount of PanLex entries.\nFor MasakhaNER, the three languages with fewer\nentries in PanLex have much more signiﬁcant gains\nby using the combined lexicon. While using the\ncombined lexicons generally hurts POS tagging,\nthe languages with fewer entries in PanLex tend to\nhave less performance decrease.\n500 5000 Full\n55\n56\n57\n58\n59\n60F1\nPseudo Trans-train\nPseudo MLM\nBoth\nFigure 7: F1 on MasakhaNER with different amount of labeled\ndata. Pseudo MLM becomes beneﬁcial when the labeled\ntraining data is small.\nA.4 Effect of Task Data Size\nOur experiments in Tab. 3 show that MasakhaNER\nbeneﬁts more from Pseudo Trans-train, likely be-\ncause the labeled data is closer to the domain of\nthe test data. However, this result might not hold\nwhen the amount of labeled data is limited. One ad-\nvantage of Pseudo MLM over Pseudo Trans-train\nis that it only requires English monolingual data\nto synthesize pseudo training data, while Pseudo\nTrans-train is constrained by the availability of la-\nbeled data. We subsample the amount of English\nNER training data for MasakhaNER and plot the\naverage F1 score of Pseudo Trans-train, pseudo\nMLM and using both. Fig. 7 shows that the ad-\nvantage of Pseudo Trans-train on MasakhaNER\ndecreases as the number of labeled data decreases,\nand using both methods is more competitive when\nthe task data is small.\nA.5 List of Bilingual Lexicons\nWe provide a list of bilingual lexicons beyond Pan-\nLex:\n• Swadesh lists in about 200 languages in\nWikipedia8\n• Words in 3156 language varietities in CLICS9\n• Intercontinental Dictionary Series in about\n300 languages10\n• 40-item wordlists in 5,000+ languages in\nASJP11\n• Austronesian Basic V ocabulary Database in\n1,700+ languages12\n• Diachronic Atlas of Comparative Linguistics\nin 500 languages13\n8https://en.wiktionary.org/wiki/\nAppendix:Swadesh_lists\n9https://clics.clld.org/\n10https://ids.clld.org/\n11https://asjp.clld.org/\n12https://abvd.shh.mpg.de/austronesian/\n13https://diacl.ht.lu.se/\n874\nA.6 Lexicon Extraction\nWe use a simple python script to extract the lexi-\ncons from the PanLex database, and directly use\nthem for synthesizing the pseudo data. We will\nopen-source the script in our codebase.\nA.7 Performance for Individual Language\n875\nMethod Lexicon hau wol lug ibo kin luo\nNo-Text\nmBERT 48.7 33.9 50.9 55.2 52.4 35.3\nPseudo Trans-train PanLex 70.4 48.3 52.1 62.2 56.8 36.6\nPseudo MLM PanLex 62.9 48.7 53.6 58.7 57.8 33.7\nBoth PanLex 69.5 52.6 55.3 62.3 57.3 31.9\nBoth+Label Distillation PanLex 64.1 47.4 55.0 62.1 58.3 34.3\nFew-Text\nGold MLM 54.3 48.4 59.8 58.4 58.2 42.7\nPseudo Trans-train PanLex 71.5 58.1 60.8 63.4 61.2 41.4\nPseudo MLM PanLex 64.3 55.0 58.5 63.6 62.1 40.9\nPanLex+Induced 64.3 57.2 63.6 62.4 61.9 41.6\nBoth PanLex 73.5 58.3 60.6 63.1 62.5 37.0\nPanLex+Induced 74.4 60.3 61.6 63.6 63.8 42.6\nBoth+Label Distillation PanLex 65.02 56.4 60.8 64.7 62.5 40.8\nPanLex+Induced 66.8 56.1 62.7 63.6 64.2 43.2\nTable 7: Average F1 score for languages in MasakaNER\nMethod Lexicon bam glv grc gsw mlt myv wol\nNo-Text\nmBERT 32.8 32.5 34.9 60.8 21.8 40.4 29.2\nPseudo Trans-train PanLex 51.8 59.6 39.9 58.0 52.4 50.6 45.3\nPseudo MLM PanLex 43.5 57.5 43.1 52.6 47.7 55.3 42.7\nBoth PanLex 51.8 59.0 36.4 50.3 50.6 50.2 42.8\nBoth+Label Distillation PanLex 45.3 59.0 43.0 54.3 49.8 56.1 44.4\nFew-Text\nGold MLM 57.2 61.7 40.8 65.0 64.0 69.2 66.3\nPseudo Trans-train PanLex 56.8 62.2 44.9 62.8 61.6 63.4 63.1\nPseudo MLM PanLex 66.5 64.3 48.6 67.1 70.4 72.1 68.9\nPanLex+Induced 65.4 64.3 48.2 66.3 68.1 72.5 68.4\nBoth PanLex 59.5 63.0 42.1 65.2 63.1 65.9 66.4\nPanLex+Induced 60.4 63.0 42.2 62.8 60.1 70.3 57.6\nBoth+Label Distillation PanLex 66.9 65.3 50.1 68.5 71.0 72.5 69.5\nPanLex+Induced 65.6 64.7 49.7 68.9 70.0 72.9 69.3\nTable 8: Average F1 score for languages in UDPOS\n876\nMethod Lexicon bam glv grc gsw mlt myv wol\nNo-Text\nmBERT 10.5 8.4 17.4 45.2 7.7 16.9 9.7\nPseudo Trans-train PanLex 15.2 41.1 19.3 31.7 35.0 22.4 16.6\nPseudo MLM PanLex 15.4 39.5 20.6 30.7 28.2 25.9 16.5\nBoth PanLex 16.3 42.0 17.4 30.1 33.3 24.5 17.5\nBoth+Label Distillation PanLex 16.5 41.6 20.1 29.8 31.8 26.1 16.2\nFew-Text\nGold MLM 25.1 43.2 21.9 49.8 50.4 44.9 46.0\nPseudo Trans-train PanLex 22.4 43.9 24.4 42.4 48.3 38.9 39.0\nPseudo MLM PanLex 31.2 50.0 25.9 50.5 53.1 45.9 48.1\nPanLex+Induced 28.9 48.9 23.9 44.3 50.5 46.7 47.5\nBoth PanLex 23.2 45.3 20.7 45.5 49.9 39.5 44.2\nPanLex+Induced 24.5 45.2 20.3 37.7 48.2 38.8 32.0\nBoth+Label Distillation PanLex 29.1 50.4 24.6 46.8 52.1 44.3 45.8\nPanLex+Induced 28.2 50.4 24.4 40.7 51.6 45.7 43.7\nTable 9: Average F1 score for languages in Parsing\nMethod Lexicon ace bak crh hak ibo ilo kin mhr mlt mri\nNo-Text\nmBERT 39.4 57.9 48.2 28.5 41.7 59.8 57.3 47.7 53.1 42.7\nPseudo Trans-train PanLex 41.1 63.2 47.1 30.9 49.4 62.8 56.7 49.9 63.4 32.7\nPseudo MLM PanLex 38.4 60.1 46.9 30.2 46.8 62.4 60.2 51.8 59.3 42.5\nBoth PanLex 38.8 57.2 43.9 30.2 48.5 63.3 57.4 51.1 62.8 32.1\nBoth+Label DistillationPanLex 38.4 59.3 46.4 32.3 48.6 65.8 62.7 51.5 64.1 36.6\nFew-Text\nGold MLM 38.7 57.9 48.4 37.2 48.0 60.5 56.4 51.4 64.5 32.2\nPseudo Trans-train PanLex 38.2 60.9 48.6 37.0 50.1 63.6 56.9 52.7 62.4 32.0\nPseudo MLM PanLex 41.4 58.2 47.7 36.0 50.7 65.7 61.4 50.5 62.4 33.3\nPanLex+Induced43.3 57.5 47.8 37.6 47.4 66.9 59.6 53.1 63.5 45.2\nBoth PanLex 41.5 57.9 47.8 35.5 50.0 65.4 56.9 50.9 62.4 32.4\nPanLex+Induced40.7 57.6 51.3 40.2 48.8 67.4 60.5 56.8 65.3 37.3\nBoth+Label DistillationPanLex 46.0 56.5 50.0 35.3 49.6 65.1 61.5 52.6 65.9 34.8\nPanLex+Induced45.7 61.6 52.1 38.7 49.1 63.6 63.0 55.2 66.9 36.3\nTable 10: Average F1 score for languages in WikiNER\n877"
}