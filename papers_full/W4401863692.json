{
    "title": "CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent",
    "url": "https://openalex.org/W4401863692",
    "year": 2024,
    "authors": [
        {
            "id": null,
            "name": "Ning, Liang-bo",
            "affiliations": [
                "Hong Kong Polytechnic University"
            ]
        },
        {
            "id": "https://openalex.org/A1553742533",
            "name": "Wang, Shijie",
            "affiliations": [
                "Hong Kong Polytechnic University"
            ]
        },
        {
            "id": "https://openalex.org/A2643408872",
            "name": "Fan Wenqi",
            "affiliations": [
                "Hong Kong Polytechnic University"
            ]
        },
        {
            "id": "https://openalex.org/A1935408072",
            "name": "Li Qing",
            "affiliations": [
                "Hong Kong Polytechnic University"
            ]
        },
        {
            "id": "https://openalex.org/A2020688285",
            "name": "Xu Xin",
            "affiliations": [
                "Hong Kong Polytechnic University"
            ]
        },
        {
            "id": "https://openalex.org/A1963498686",
            "name": "Chen Hao",
            "affiliations": [
                "Hong Kong Polytechnic University"
            ]
        },
        {
            "id": "https://openalex.org/A2389221335",
            "name": "Huang, Feiran",
            "affiliations": [
                "Jinan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4386728933",
        "https://openalex.org/W4286750695",
        "https://openalex.org/W2972646741",
        "https://openalex.org/W4313591195",
        "https://openalex.org/W3175142666",
        "https://openalex.org/W4284704639",
        "https://openalex.org/W2914721378",
        "https://openalex.org/W2972774416",
        "https://openalex.org/W4368232818",
        "https://openalex.org/W2962818281",
        "https://openalex.org/W3104423855",
        "https://openalex.org/W4296591867",
        "https://openalex.org/W3045200674",
        "https://openalex.org/W2605350416",
        "https://openalex.org/W2996851481",
        "https://openalex.org/W2963367478",
        "https://openalex.org/W3031339255",
        "https://openalex.org/W2984100107",
        "https://openalex.org/W4393065402",
        "https://openalex.org/W4223982309",
        "https://openalex.org/W4384642512",
        "https://openalex.org/W4389520259",
        "https://openalex.org/W4394994587",
        "https://openalex.org/W2783666221",
        "https://openalex.org/W3106181667"
    ],
    "abstract": "Recently, Large Language Model (LLM)-empowered recommender systems (RecSys) have brought significant advances in personalized user experience and have attracted considerable attention. Despite the impressive progress, the research question regarding the safety vulnerability of LLM-empowered RecSys still remains largely under-investigated. Given the security and privacy concerns, it is more practical to focus on attacking the black-box RecSys, where attackers can only observe the system's inputs and outputs. However, traditional attack approaches employing reinforcement learning (RL) agents are not effective for attacking LLM-empowered RecSys due to the limited capabilities in processing complex textual inputs, planning, and reasoning. On the other hand, LLMs provide unprecedented opportunities to serve as attack agents to attack RecSys because of their impressive capability in simulating human-like decision-making processes. Therefore, in this paper, we propose a novel attack framework called CheatAgent by harnessing the human-like capabilities of LLMs, where an LLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our method first identifies the insertion position for maximum impact with minimal input modification. After that, the LLM agent is designed to generate adversarial perturbations to insert at target positions. To further improve the quality of generated perturbations, we utilize the prompt tuning technique to improve attacking strategies via feedback from the victim RecSys iteratively. Extensive experiments across three real-world datasets demonstrate the effectiveness of our proposed attacking method.",
    "full_text": "CheatAgent: Attacking LLM-Empowered Recommender\nSystems via LLM Agent\nLiang-bo Ningâˆ—\nThe Hong Kong\nPolytechnic University\nHong Kong, China\nBigLemon1123@gmail.com\nShijie Wangâˆ—\nThe Hong Kong\nPolytechnic University\nHong Kong, China\nshijie.wang@connect.polyu.hk\nWenqi Fanâ€ \nThe Hong Kong\nPolytechnic University\nHong Kong, China\nwenqifan03@gmail.com\nQing Li\nThe Hong Kong\nPolytechnic University\nHong Kong, China\nqing-prof.li@polyu.edu.hk\nXin Xu\nThe Hong Kong\nPolytechnic University\nHong Kong, China\nxin.xu@polyu.edu.hk\nHao Chen\nThe Hong Kong\nPolytechnic University\nHong Kong, China\nsundaychenhao@gmail.com\nFeiran Huang\nJinan University\nGuangzhou, China\nhuangfr@jnu.edu.cn\nABSTRACT\nRecently, Large Language Model (LLM)-empowered recommender\nsystems (RecSys) have brought significant advances in personalized\nuser experience and have attracted considerable attention. Despite\nthe impressive progress, the research question regarding the safety\nvulnerability of LLM-empowered RecSys still remains largely under-\ninvestigated. Given the security and privacy concerns, it is more\npractical to focus on attacking the black-box RecSys, where attack-\ners can only observe the systemâ€™s inputs and outputs. However,\ntraditional attack approaches employing reinforcement learning\n(RL) agents are not effective for attacking LLM-empowered RecSys\ndue to the limited capabilities in processing complex textual inputs,\nplanning, and reasoning. On the other hand, LLMs provide unprece-\ndented opportunities to serve as attack agents to attack RecSys\nbecause of their impressive capability in simulating human-like\ndecision-making processes. Therefore, in this paper, we propose\na novel attack framework called CheatAgent by harnessing the\nhuman-like capabilities of LLMs, where an LLM-based agent is de-\nveloped to attack LLM-Empowered RecSys. Specifically, our method\nfirst identifies the insertion position for maximum impact with min-\nimal input modification. After that, the LLM agent is designed to\ngenerate adversarial perturbations to insert at target positions. To\nfurther improve the quality of generated perturbations, we utilize\nthe prompt tuning technique to improve attacking strategies via\nfeedback from the victim RecSys iteratively. Extensive experiments\nacross three real-world datasets demonstrate the effectiveness of\nour proposed attacking method.\nâˆ—Both authors contributed equally to this research.\nâ€ Corresponding author: Wenqi Fan, Department of Computing, and Department of\nManagement and Marketing, The Hong Kong Polytechnic University.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0490-1/24/08\nhttps://doi.org/10.1145/3637528.3671837\nCCS CONCEPTS\nâ€¢ Security and privacy â†’Vulnerability management; â€¢ Infor-\nmation systems â†’Recommender systems.\nKEYWORDS\nRecommender Systems, Adversarial Attacks, Large Language Mod-\nels, LLM-Empowered Recommender Systems, LLMs-based Agent.\nACM Reference Format:\nLiang-bo Ning, Shijie Wang, Wenqi Fan, Qing Li, Xin Xu, Hao Chen, and Feiran\nHuang. 2024. CheatAgent: Attacking LLM-Empowered Recommender\nSystems via LLM Agent. In Proceedings of the 30th ACM SIGKDD Con-\nference on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“\n29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:\n//doi.org/10.1145/3637528.3671837\n1 INTRODUCTION\nRecommender Systems ( RecSys) play a vital role in capturing\nusersâ€™ interests and preferences across various fields [ 11], such\nas e-commerce (e.g., Amazon, Taobao), social media (e.g., Twitter,\nFacebook), etc. Traditional RecSys typically rely on usersâ€™ historical\ninteractions to analyze user behaviors and item characteristics [21].\nRecent developments in deep learning (DL) have introduced neu-\nral networks like Graph Neural Networks (GNNs) and Recurrent\nNeural Networks (RNNs) in RecSys to further improve recommenda-\ntion performance [12, 20]. Although DL-based methods effectively\nmodel the representations of users and items, they struggle with\nencoding textual information (e.g., item titles, user reviews) for rea-\nsoning on userâ€™s prediction [29, 50]. Recently, due to the powerful\nlanguage understanding and in-context learning capabilities, Large\nLanguage Models (LLMs) have provided great potential to revo-\nlutionize RecSys [2, 18, 34]. For instance, P5 [18] leverages LLMâ€™s\n(i.e. T5 [31]) capabilities to significantly enhance recommendation\nperformance by understanding nuanced user preferences and item\ndescriptions. Despite the aforementioned success, there is a critical\nissue that remains largely unexplored: the safety vulnerability\nof LLM-empowered recommender systems under adversarial\nattacks, which hinders their adoption in various real-world appli-\ncations, especially those high-stake environments like finance and\nhealthcare.\narXiv:2504.13192v2  [cs.CR]  24 Apr 2025\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Liang-bo Ning et al.\n(a) Benign Prompt\nI have bought Skirt,\nHeels. What should I\nbuy next?\nYes. I want.\nYou may want to buy\nDresses.\nNo. I don't want.\nYou may want to buy\nSuit.\nI haveÂ  Â  Â  Â  Â  Â  Â  Â  Â  bought\nSkirt,Â Â  Â  Â  Â  Â  Heels. What\nshould I buy next?\nConversationsKnowledge\n...\nReasoning\nUser\nUser\nLLM-Empowered\nRecSys LLM-Empowered\nRecSys\nUser\n(b) AdversarialÂ Prompt\nUser\n[...]Insert\ntokens\nBrainÂ (Human-level Intelligence)\n[...]Insert\nitems\n[possible]\n[Tie,]\nLLM Agent (Attacker)\nInsert\nInsert\nAdversarialÂ Perturbations\nFigure 1: The illustration of the adversarial attack for rec-\nommender systems in the era of LLMs. Attackers leverage\nthe LLM agent to insert some tokens (e.g., words) or items\nin the userâ€™s prompt to manipulate the LLM-empowered rec-\nommender system to make incorrect decisions.\nGiven the need for security and privacy, a practical attacking\nstrategy in black-box recommender systems involves utilizing rein-\nforcement learning (RL) agents to conduct poisoning attacks [9, 15].\nTo be specific, under the black-box setting, attackers have no access\nto the models or parameters of the victim RecSys. Instead, they are\nlimited to observing the systemâ€™s inputs and outputs only. For exam-\nple, most existing solutions, such as KGAttack [4], PoisonRec [33],\nand CopyAttack [9], develop RL-based agents to obtain malicious\nuser profiles (i.e., a series of items) and inject them into the victim\nRecSys for manipulating systemâ€™s decision. Despite the impressive\nprogress in attacking recommender systems under the black-box\nsetting, most existing attack approaches still suffer from several\nlimitations. First, vanilla RL-based agents struggle with processing\nthe textual input (e.g., itemâ€™s title and descriptions) and context\nawareness, resulting in difficulty in attacking LLM-empowered\nRecSys which mainly takes text as input and generates relevant\nresponses in natural language. Second, due to the lack of a vast\namount of open-world knowledge, most existing methods optimize\nthe RL-based agent attackers from scratch without human-level in-\ntelligence, which subsequently leads to poor capability in planning\nand reasoning the attacking strategies under the black-box set-\nting. Hence, it is desirable to design a novel paradigm for attacking\nblack-box recommender systems in the era of LLMs.\nMore recently, Large Language Models (LLMs) have achieved\ngreat success in various fields, such as psychology [1], drug discov-\nery [28], and health [46], demonstrating their remarkable potential\nin approximating human-level intelligence. This impressive capa-\nbility is attributed to the training on vast textual corpora (i.e., open-\nworld knowledge) with a huge amount of model parameters [49, 50].\nAs such, LLMs can well comprehend human common sense in nat-\nural language and perform complex reasoning, so as to simulate\nhuman-like decision-making processes [36]. Given their advantages,\nLLMs provide unprecedented opportunities to overcome the limita-\ntions faced by current RL-based attack methods and serve as attack\nagents to attack RecSys. Therefore, in this work, we propose a novel\nattacking strategy to attack the LLM-empowered recommender sys-\ntems by taking advantage of LLM as the autonomous agent for\nmaking human-like decisions. As shown in Figure 1, an LLM-based\nagent with human-like intelligence is introduced to generate an\nadversarial prompt by adding slight perturbations (e.g., words and\nitems) on the original prompt, so as to mislead LLM-empowered\nRecSys to make unsatisfactory recommendations.\nIn this paper, we propose a novel attack framework (CheatAgent)\nto investigate the safety vulnerability of LLM-empowered RecSys\nunder the black-box setting. Specifically, an LLM is introduced as\nan intelligence agent to generate adversarial perturbations in usersâ€™\nprompts for attacking the LLM-based system. To address the vast\nsearch space on insertion position and perturbation selection for\nthe LLM agent, we first propose insertion positioning to identify\nthe input position for maximum impact with minimal input modifi-\ncation. After that, LLM agent-empowered perturbation generation\nis proposed to generate adversarial perturbations to insert at target\npositions. Due to the domain-specific knowledge gap between the\nattack agent and LLM-empowered RecSys, we further develop a\nself-reflection policy optimization to enhance the effectiveness of\nthe attacks. Our major contributions of this paper are as follows:\nâ€¢We study a novel problem of whether the existing LLM-\nempowered recommender systems are robust to slight ad-\nversarial perturbations. To the best of our knowledge, this is\nthe first work to investigate the safety vulnerability of the\nLLM-empowered recommender systems.\nâ€¢We introduce a novel strategy to attack black-box recom-\nmender systems in the era of LLMs, where an LLM-based\nagent is developed to generate adversarial perturbations on\ninput prompts, so as to mislead LLM-empowered recom-\nmender systems for making incorrect decisions.\nâ€¢We propose a novel framework CheatAgent to attack LLM-\nempowered recommender systems under the black-box set-\nting via the LLM-based attack agent, which efficiently crafts\nimperceptible perturbations in usersâ€™ prompt to perform\neffective attacks.\nâ€¢We conduct extensive experiments on three real-world datasets\nto demonstrate the safety vulnerability of the LLM-empowered\nrecommender systems against adversarial attacks and the\nattacking effectiveness of our proposed attack method.\n2 PROBLEM STATEMENT\n2.1 Notation and Definitations\nThe objective of RecSys is to understand usersâ€™ preferences by mod-\neling the interactions (e.g., clicks, purchases, etc.) between users\nğ‘ˆ = {ğ‘¢1,ğ‘¢2,Â·Â·Â· ,ğ‘¢|ğ‘ˆ|}and items ğ‘‰ = {ğ‘£1,ğ‘£2,Â·Â·Â· ,ğ‘£|ğ‘‰|}. Within the\nframework of a general LLM-empowered RecSys ğ‘…ğ‘’ğ‘Î˜ with param-\neters Î˜, we denote an input-output sequence pair as(ğ‘‹,ğ‘Œ ), consist-\ning of a recommendation prompt template ğ‘ƒ = [ğ‘¥1,ğ‘¥2,Â·Â·Â· ,ğ‘¥|ğ‘ƒ|],\nuser ğ‘¢ğ‘–, and the userâ€™s historical interactions towards items ğ‘‰ğ‘¢ğ‘– =\n[ğ‘£1,ğ‘£2,Â·Â·Â· ,ğ‘£|ğ‘‰ğ‘¢ğ‘– |](i.e., userâ€™s profile). Based on the above defini-\ntion, a typical input can be denoted as:\nğ‘‹ = [ğ‘ƒ;ğ‘¢ğ‘–;ğ‘‰ğ‘¢ğ‘– ]= [ğ‘¥1,Â·Â·Â· ,user_ğ‘¢ğ‘–,Â·Â·Â· ,items_ğ‘‰ğ‘¢ğ‘– ,Â·Â·Â· ,ğ‘¥|ğ‘ƒ|].\nCheatAgent: Attacking LLM-Empowered Recommender\nSystems via LLM Agent KDD â€™24, August 25â€“29, 2024, Barcelona, Spain\nFor instance, as shown in Figure 2, a specific input-output pair with\nuser-item interaction in the language model for recommendation\ncan be represented as:\nğ‘‹ = [What, is, the, top, recommended, item, for, User_637, who,\nhas, interacted, with, item_1009,..., item_4045,?],\nğ‘Œ =[item_1072],\nwhere ğ‘¢ğ‘– = [ğ‘ˆğ‘ ğ‘’ğ‘Ÿ_637]and ğ‘‰ğ‘¢ğ‘– = [item_1009,..., item_4045]. The\nother tokens belong to the prompt template ğ‘ƒ.\nAfter that, LLM-empowered RecSys will generate recommen-\ndations based on the textual input. The auto-regressive language\ngeneration loss (i.e., Negative Log-Likelihood) is employed to eval-\nuate the discrepancy between the predictions and the target output,\ndefined as follows:\nLğ‘…ğ‘’ğ‘(ğ‘‹,ğ‘Œ )= 1\n|ğ‘Œ|\nÃ|ğ‘Œ|\nğ‘¡=1 âˆ’log ğ‘(ğ‘Œğ‘¡|ğ‘‹,ğ‘Œ<ğ‘¡),\nwhere ğ‘(ğ‘Œğ‘¡|ğ‘‹,ğ‘Œ<ğ‘¡)represents the probability assigned to the item\nthat users are interested in. Small Lğ‘…ğ‘’ğ‘(ğ‘‹,ğ‘Œ )indicates that RecSys\ncan accurately predict the target label ğ‘Œ and vice versa.\n2.2 Attackerâ€™s Capabilities\nIn this work, we will focus on attacking black-box LLM-empowered\nrecommender systems, where inherent details of the victim LLM-\nempowered recommender system, including architectures, gradi-\nents, parameters, etc., are restricted from access. In other words, the\nattackers can devise adversarial perturbations by solely querying\nthe target system and observing the resulting output probabilities,\nsimilar to the soft-label black-box setting in [22, 30].\n2.3 Attackerâ€™s Objective\nThe overall objective of attackers is to conductuntargeted attacks by\nundermining the overall performance of the victim LLM-empowered\nRecSys, specifically by causing the target RecSys to prioritize ir-\nrelevant items that are of no interest to users. Note that these ma-\nlicious manipulations can undermine the overall user experience\nand compromise the trustworthiness of RecSys. More specifically,\nto generate incorrect recommendations for user ğ‘¢ğ‘–, attackers aim\nto carefully craft adversarial perturbations and insert them into\nthe input ğ‘‹ = [ğ‘ƒ;ğ‘¢ğ‘–;ğ‘‰ğ‘¢ğ‘– ]as Ë†ğ‘‹ = I(ğ‘‹,ğ›¿ |ğ‘ )to deceive the vic-\ntim RecSys to learn the usersâ€™ preference, where I(ğ‘‹,ğ›¿ |ğ‘ )repre-\nsent to insert perturbation ğ›¿ at the position ğ‘  of the input ğ‘‹. In\nthe context of LLM-based recommender systems, two operations\ncan be designed for attackers to generate adversarial perturba-\ntions on input: 1) insert the tailored perturbations into the prompt\ntemplate (i.e., Ë†ğ‘‹ = [Ë†ğ‘ƒ;ğ‘¢ğ‘–;ğ‘‰ğ‘¢ğ‘– ] = [I(ğ‘ƒ,ğ›¿ |ğ‘ );ğ‘¢ğ‘–;ğ‘‰ğ‘¢ğ‘– ]), and 2) per-\nturb the usersâ€™ profiles to distort their original preference (i.e.,\nË†ğ‘‹ = [ğ‘ƒ;ğ‘¢ğ‘–; Ë†ğ‘‰ğ‘¢ğ‘– ]= [ğ‘ƒ;ğ‘¢ğ‘–; I(ğ‘‰ğ‘¢ğ‘– ,ğ›¿|ğ‘ )]).\nGiven these two different attacking operations, adversarial per-\nturbations applied to the recommendation prompt ğ‘ƒ and usersâ€™\nprofiles ğ‘‰ğ‘¢ğ‘– differ in nature. Specifically, words or characters can\nbe used as perturbations inserted into the recommendation prompt\nğ‘ƒ, while items serve as perturbations inserted into user profilesğ‘‰ğ‘¢ğ‘– .\nFor the simplicity of notation,ğ›¿is employed to uniformly represent\nthese two forms of perturbations. Mathematically, adversarial per-\nturbations ğ›¿ can be generated by decreasing the recommendation\nperformance, and the overall objective is formulated as follows:\nğ›¿ = arg max\nğ›¿:âˆ¥Ë†ğ‘‹âˆ’ğ‘‹âˆ¥0 â‰¤â–³\nLğ‘…ğ‘’ğ‘(Ë†ğ‘‹,ğ‘Œ ),\nwhere âˆ¥Ë†ğ‘‹âˆ’ğ‘‹âˆ¥0 is the Hamming distance between the benign input\nand adversarial input [48] and the â–³is the predefined upper bound\nto constrain the magnitude of perturbations.\n3 METHODOLOGY\n3.1 An Overview of the Proposed CheatAgent\nIn order to conduct black-box attacks on target LLM-empowered\nRecSys, adversarial perturbations are generated to modify the input\nprompts to mislead the generation of LLM-empowered systems. To\nachieve this goal, we propose a novel attacking strategy, in which\nan LLM-based agent (attacker) is developed to effectively craft input\nprompts, due to the powerful language comprehension, reasoning\nabilities, and rich open-world knowledge of LLMs. However, devel-\noping malicious LLM-based agents to perform attacks under the\nblack-box setting faces challenges due to numerous options for\nboth insertion positions and perturbation selection.\nTo address these challenges, we propose a novel framework\n(CheatAgent), which utilizes the prompt tuning techniques to learn\nattacking strategies and generate high-quality adversarial pertur-\nbations via interactions with the victim RecSys iteratively. As illus-\ntrated in Figure 2, the overall framework of our proposed method\nconsists of two main components: Insertion Positioning and LLM\nAgent-Empowered Perturbation Generation. First, we aim to posi-\ntion the inserting tokens to achieve maximum impact with minimal\ninput modification. Specifically, we identify the tokens within the\nprompt that possess the substantial impact to deceive the victim\nmodel by employing minimal perturbations. Second, LLM agent-\nempowered perturbation generation is proposed to fully leverage\nthe powerful capabilities of LLMs in comprehending and generating\nnatural language, as well as reasoning with open-world knowledge\nto generate adversarial perturbations to deceive the target system.\nThe proposed approach contains two processes: initial policy gen-\neration and self-reflection policy optimization. These two processes\ninitialize and fine-tune the attack policy based on the feedback from\nthe target system by utilizing prompt tuning techniques to perform\neffective attacks.\n3.2 Insertion Positioning\nAs the impact of each token within the prompt can vary signifi-\ncantly, positioning the insertion tokens is crucial for conducting\neffective attacks [16, 17]. Consequently, we propose to insert new\ntokens adjacent to the tokens that contribute more towards the\nfinal prediction and can achieve maximum impact with minimal\ninput modification. Therefore, we first evaluate the importance of\neach word/item within the input ğ‘‹ and locate the token with the\nmaximum impact. As shown in the first component of Figure 2,\nwe propose to mask the token from the input sentence and then\nexamine the change it brings to the final predictions, so as to evalu-\nate the token importance of the input prompt. Given an input ğ‘‹\ncontaining |ğ‘‹|tokens, masking a specific ğ‘–-th token from the input\nğ‘‹ can be defined as follows: ğ‘‹ğ‘€ğ´ğ‘†ğ¾ğ‘– = I(ğ‘‹, [ğ‘€ğ´ğ‘†ğ¾]|ğ‘–). The signif-\nicance of the ğ‘–-th token is determined by measuring the variation\nin prediction performance between the original input and the input\nwith the token masked, as follows:\nğ¼ğ‘– = Lğ‘…ğ‘’ğ‘(ğ‘‹ğ‘€ğ´ğ‘†ğ¾ğ‘– ,ğ‘Œ)âˆ’L ğ‘…ğ‘’ğ‘(ğ‘‹,ğ‘Œ ).\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Liang-bo Ning et al.\n#1Â Insertion Positioning \nEmbedding Projection\n...\n:whisper\n :lately\n :item\n :possibly\n...\n:whisper\n :lately\n :item\n :possibly\nPrefix Prompt\n#2.1Â Initial Policy Generation\n#2Â LLMÂ Agent-Empowered Perturbation Generation\nAdversarial PromptÂ Â \nPotential haveisthetop recommendeditemforuser_637who interactedwith item_1009,item_2298,item_4045?What\n#2.2Â Self-Reflection Policy Optimization\nPosition SelectionÂ Â \nPerturbation Generation\nLoss CalculationÂ \n Backpropagation\nToken Importance CalculationÂ \nis thetoprecommendeditemforuser_637whointeractedwithitem_1009,item_2298,item_4045?What\nBenign Prompt\ngoods\nPleaseÂ generate some letters or wordsÂ that canÂ change your\npredictionÂ of theÂ {Benign Prompt}Â after inserting it into theÂ Â  Â  Â  Â .\nMalicious InstructionÂ \nPerturbation SelectionÂ  Perturbation\nÂ Perturbation Generation\nis the recommendeditem ...[MASK] foruser_637who\n...\ntop\nToken Mask\nAttack Instruction\nÂ Embedding\nPrefix Prompt\nPrefix Prompt\nÂ Perturbation Generation\n...\n:whisper\n :lately\n :item\n :possibly\n...\n:Potential\n :Capability\n :Promise\n :Prospect\n:Potential\nPrefix Prompt\n...\n:breeze\n :cascade\n :input\n :potential\nLLM-Based AgentLLM-Empowered\nRecSysAttackers Frozen Parameters\nTrainable ParametersMasked Position\nUnmasked Position\n:whisperÂ \n:breezeÂ \n:PotentialPerturbations Forward\nBackward\nEmbeddings\nPrefix Prompt SelectionÂ \nFigure 2: The overall framework of the proposed CheatAgent. Insertion positioning first locates the token with the maximum\nimpact. Then, LLM agent-empowered perturbation generation is proposed to leverage the LLM as the attacker agent to generate\nadversarial perturbations. It contains two processes: 1) Initial Policy Generation searches for a great attack policy initialization,\nand 2) Self-Reflection Policy Optimization fine-tunes the prefix prompt to update the attack policy of the LLM-based agent.\nAfter calculating the importance for |ğ‘‹|tokens respectively, we\ncan obtain the importance list [ğ¼1,ğ¼2,...,ğ¼ |ğ‘‹|]. Then, a position list\nis generated by selecting the tokens with top-â–³importance scores,\ndefined by: S= [ğ‘ 1,ğ‘ 2,Â·Â·Â· ,ğ‘ â–³].\n3.3 LLM Agent-Empowered Perturbation\nGeneration\nOnce the tokens with the highest impact have been identified, the\nnext crucial step is to determine the perturbations to be inserted.\nDue to the superiority of the LLM-based agent in comprehending\nnatural language and its abundant knowledge derived from abun-\ndant training data, we propose an LLM-based agent paradigm to\nattack LLM-empowered RecSys, where an auxiliary large language\nmodel is designed as the attack agent to generate high-quality per-\nturbations for the specific positions. However, manipulating the\ntarget RecSys needs to select the most effective token as an adver-\nsarial perturbation from a vast collection of options, which is a\nhighly complex and challenging task. Direct utilization of adver-\nsarial perturbations generated by the LLM-based agent based on\nthe initial attack policy often fails to achieve the desired attack per-\nformance due to the lack of domain-specific knowledge. Moreover,\ndue to the extensive number of internal parameters in the LLM, it\nis impractical and inefficient to fine-tune the entire LLM agent by\ninteracting with the target RecSys.\nTo address these challenges, as shown in Figure 2, we propose a\nprompt tuning-based attack policy optimization strategy, in which\na trainable prefix prompt Fis designed to integrate into the attack-\nersâ€™ instruction Pin the embedding space. Meanwhile, we only\nfine-tune the prefix prompt Fby interacting with the target Rec-\nSys to optimize the attack policy of the LLM-based agent. Given\nthat the task performance of large language models is significantly\ninfluenced by the quality of the input prompts [47], freezing the pa-\nrameters of the LLM-based agent results in the attack policy being\nhighly dependent on the input instruction provided by attackers.\nTherefore, the LLM-based agent can adjust the attack policy by\nfine-tuning the task-specific instruction given by attackers, thereby\neffectively reducing the computational burden and time consump-\ntion of retraining the entire LLM.\nThe proposed method in this component is comprised of two\nmain steps: 1) Initial Policy Generation, and 2) Self-Reflection Policy\nOptimization. To be specific, Initial Policy Generation aims to search\nfor an appropriate prefix prompt to initialize a benchmark attack\npolicy to minimize subsequent iterations for policy tuning. Then,\ngiven the initialized prefix prompt, we propose a self-reflection\npolicy optimization strategy to fine-tune the prefix prompt and\nupdate the attack policy of the LLM-based agent by utilizing the\nfeedback from the victim RecSys.\n3.3.1 Initial Policy Generation. Before updating the attack pol-\nicy by fine-tuning the trainable prefix prompt, the agent must gen-\nerate an initial policy to start optimization. Poor initialization can\nlead the agent to get stuck in local optimal when learning the attack\npolicy [6], bringing difficulties in effectively attacking the target sys-\ntem. Therefore, to enhance the attack performance of the generated\nperturbations and decrease the number of subsequent policy tuning\niterations, we propose to search for an appropriate prefix prompt\nto initialize the attack policy in the LLM-based attacker agent. To\nachieve this goal, we randomly initialize multiple prefix prompts\nand combine them with the attackâ€™s instructions respectively to\ngenerate multiple adversarial perturbations. Each perturbation is\nevaluated for its attack performance, and the prefix prompt that can\ngenerate the perturbation with the greatest impact in misleading\nthe target RecSys is deemed the optimal initialization.\nWe use Pâˆˆ{P ğ‘ƒ,Pğ‘‰ğ‘¢ğ‘– }to represent the attackerâ€™s instructions,\nwhich is exploited to guide the LLM-based agent to generate per-\nturbations. As we mentioned in Section 2.3, ğ›¿ has two forms of\nadversarial perturbations in attacking LLM-empowered RecSys, so\ndistinct instructions Pğ‘ƒ and Pğ‘‰ğ‘¢ğ‘– are employed to generate pertur-\nbations that are inserted to the prompt ğ‘ƒ and usersâ€™ profiles ğ‘‰ğ‘¢ğ‘–\n(more details about the instructions given by attackers are shown\nin Table 6 of Appendix B.2). Technically, we first initializeğ‘˜prefix\nCheatAgent: Attacking LLM-Empowered Recommender\nSystems via LLM Agent KDD â€™24, August 25â€“29, 2024, Barcelona, Spain\nprompts [F1,..., Fğ‘˜], each prefix is combined with the attackerâ€™s\ninstruction Pin the embedding space and fed into the LLM-based\nagent Ato generate ğ‘›perturbation candidates, defined by:\nBğ‘— = A(Fğ‘— âŠ•P), (1)\nwhere âŠ•is the combination operator andBğ‘— = [ğ›¿ğ‘—1,ğ›¿ğ‘—2,...,ğ›¿ ğ‘—ğ‘›],ğ‘— âˆˆ\n{1,ğ‘˜}is the perturbation candidates generated by the LLM-based\nagent Abased on the combined prompt Fğ‘— âŠ•P. After that, each\nperturbation candidate of Bğ‘— is iteratively inserted into the prompt\nğ‘‹ at the position ğ‘ ğ‘–. The perturbation that maximally undermines\nthe prediction performance of the victim system is selected from\nall candidates, and the prefix used to generate this perturbation is\nconsidered as the initial prefix F0, defined by:\nF0 = arg max\nA(Fğ‘— âŠ•P)\nLğ‘…ğ‘’ğ‘(I(ğ‘‹,ğ›¿ ğ‘—ğ‘š|ğ‘ ğ‘–),ğ‘Œ),ğ‘— âˆˆ{1,ğ‘˜},ğ‘š âˆˆ{1,ğ‘›}. (2)\nHere we use Lğ‘šğ‘ğ‘¥\nğ‘…ğ‘’ğ‘ = maxLğ‘…ğ‘’ğ‘(I(ğ‘‹,ğ›¿ ğ‘—ğ‘š|ğ‘ ğ‘–),ğ‘Œ)to denote the\nmaximum loss after inserting all candidates at position ğ‘ ğ‘– respec-\ntively, where ğ‘— âˆˆ{1,ğ‘˜}and ğ‘š âˆˆ{1,ğ‘›}.\n3.3.2 Self-Reflection Policy Optimization. Due to the domain-\nspecific knowledge gap between the attack agent and the LLM-\nempowered RecSys that may be fine-tuned on the recommendation\ndata, the initial attack policy based on the given prefix prompt can\nbe sub-optimal. To further optimize the attack policy and enhance\nthe attack performance, it is necessary to fine-tune the initialized\nprefix prompt F0 in LLM-based agent via the feedback (i.e., out-\nput) from the victim system under the black-box setting. Specifi-\ncally, we propose a black-box self-reflection prompt tuning strategy,\nwhich aims to determine the optimization direction according to\nthe feedback produced by the target RecSys. First, the perturbations\nB0 = [ğ›¿1,...,ğ›¿ ğ‘›]generated by A(F0 âŠ•P) are divided positive and\nnegative categories. Subsequently, we optimize the attack policy in\na direction that enables the LLM-based agent to generate a higher\nnumber of positive perturbations, while minimizing the production\nof negative perturbations it generates. As the overall objective is\nto maximize Lğ‘…ğ‘’ğ‘(Ë†ğ‘‹,ğ‘Œ ), by evaluating the effect of the perturba-\ntion on attack loss, we can classify perturbations into positive and\nnegative, defined by: T(ğ›¿ğ‘–), where T is an indicator function:\nT(ğ›¿ğ‘–)=\n(\n1, if Lğ‘…ğ‘’ğ‘(I(ğ‘‹,ğ›¿ ğ‘—|ğ‘ ğ‘–),ğ‘Œ)â‰¥L ğ‘šğ‘ğ‘¥\nğ‘…ğ‘’ğ‘ ,\nâˆ’1, if Lğ‘…ğ‘’ğ‘(I(ğ‘‹,ğ›¿ ğ‘—|ğ‘ ğ‘–),ğ‘Œ)< Lğ‘šğ‘ğ‘¥\nğ‘…ğ‘’ğ‘ , (3)\nwhere T(ğ›¿ğ‘–)= 1 means ğ›¿ğ‘– can further enhance the attack perfor-\nmance, and it is considered as the positive perturbation. If ğ›¿ğ‘– is a\nnegative perturbation, we compute the gradient of ğ›¿ğ‘– with respect\nto F0 and update F0 in the direction of gradient ascent. This en-\nsures that F0 âŠ•P minimally guides the LLM to generate negative\nperturbations. Based on the above definition, we can formulate the\noptimization problem as follows:\nLF0 =\nğ‘›âˆ‘ï¸\nğ‘–=1\nT(ğ›¿ğ‘–)Â·L A(F0 âŠ•P,ğ›¿ğ‘–)\n=\nğ‘›+âˆ‘ï¸\nğ‘–=1\nLA(F0 âŠ•P,ğ›¿+\nğ‘– )âˆ’\nğ‘›âˆ’âˆ‘ï¸\nğ‘—=1\nLA(F0 âŠ•P,ğ›¿âˆ’\nğ‘— ),\n(4)\nwhere LA(F0 âŠ•P,ğ›¿ğ‘–)= 1\n|ğ›¿ğ‘– |\nÃ|ğ›¿ğ‘– |\nğ‘¡=1 âˆ’log ğ‘(ğ›¿ğ‘¡\nğ‘–|F0 âŠ•P,ğ›¿<ğ‘¡\nğ‘– )is the\nnegative log-likelihood loss. ğ‘›+and ğ‘›âˆ’are the number of positive\nperturbations ğ›¿+\nğ‘– and negative perturbations ğ›¿âˆ’\nğ‘— , respectively. Mini-\nmizing Eq (4) promotes the LLM-based agent Ato update its attack\npolicy to generate more positive perturbations with a significant\nimpact on the manipulation of target systemâ€™s predictions. The\noptimization process is defined by: Fğ‘‡ = Fğ‘‡âˆ’1 âˆ’ğ›¾ Â·âˆ‡Fğ‘‡ âˆ’1 LFğ‘‡ âˆ’1 ,\nwhere ğ›¾ = 0.1 is the learning rate and ğ‘‡ âˆˆ{1,5}is the number of\npolicy optimization iterations.\n3.3.3 Final Perturbation Selection. Through backpropagation,\nwe can obtain an optimized prefix prompt Fğ‘‡ that equips the LLM-\nbased agent Awith the powerful attack policy to generate high-\nquality perturbations Bğ‘‡ = [ğ›¿1\nğ‘‡,...,ğ›¿ ğ‘›\nğ‘‡]. Finally, the perturbation Ë†ğ›¿ğ‘‡,\nwhich can not only induce the largest decrease in the performance\nof the target RecSys but also preserve high semantic similarity, is\nconsidered the optimal solution and inserted into the input prompt\nğ‘‹. The optimal perturbation selection process is defined by:\nË†ğ›¿ğ‘‡ = arg max\nğ›¿ğ‘š\nğ‘‡\nLğ‘…ğ‘’ğ‘(I(ğ‘‹,ğ›¿ğ‘š\nğ‘‡ |ğ‘ ğ‘–),ğ‘Œ)+ğœ†Â·ğ‘†ğ‘–ğ‘š(I(ğ‘‹,ğ›¿ğ‘š\nğ‘‡ |ğ‘ ğ‘–),ğ‘‹), (5)\nwhere ğ‘†ğ‘–ğ‘š(I(ğ‘‹,ğ›¿ğ‘š\nğ‘‡ |ğ‘ ğ‘–),ğ‘‹)is the cosine similarity between the per-\nturbed prompt I(ğ‘‹,ğ›¿ğ‘š\nğ‘‡ |ğ‘ ğ‘–)and the benign prompt ğ‘‹, and ğœ†= 0.01\nis the hyper-parameter to balance the impact of these two aspects.\nThe semantic similarity is computed by introducing an additional\nembedding model bge-large-en [41]. The whole process of the pro-\nposed CheatAgent is shown in Algorithm 1 (Appendix A).\n4 EXPERIMENTS\nIn this section, comprehensive experiments are conducted to demon-\nstrate the effectiveness of the proposed method. Due to the space\nlimitation, some details of the experiments and discussions are\nshown in Appendix B and Appendix D.\n4.1 Experimental Details\n4.1.1 Datasets. All experiments are conducted on three commonly-\nused datasets in RecSys: Movielens-1M (ML1M) [19], Taobao [51],\nand LastFM [43] datasets. The ML1M dataset provides movie rat-\nings and user information, theTaobaodataset contains e-commerce\ntransaction data, and the LastFM dataset offers user listening his-\ntories and music information. The details of these datasets are\nsummarised in Appendix B.1.\n4.1.2 Victim LLM-based Recommender Systems. P5 [18] and\nTALLRec [2] are exploited to investigate the safety vulnerability\nof LLM-empowered recommender systems:\nâ€¢P5 first converts all data, including user-item interactions,\nuser descriptions, etc., to natural language sequences. It\nproposes several item indexing strategies, introduces the\nwhole-word embedding to represent items, and fine-tunes\nthe T5 [31] to improve the recommendation performance.\nâ€¢TALLRec transfers the recommendation problem to a binary\ntextual classification problem. It fine-tunes the LLaMA [35]\non the recommendation task and utilizes the userâ€™s interac-\ntion history to forecast their interest in a forthcoming item\nby integrating item titles into a pre-defined prompt.\n4.1.3 Baselines. Multiple baselines are employed to investigate\nthe vulnerability of the LLM-empowered RecSys, shown as follows:\nâ€¢MD manually designs an adversarial prompt with the oppo-\nsite semantic meaning to the original prompt ğ‘‹ by inserting\n\"not\". The used prompt is shown in Appendix B.2 Table 5.\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Liang-bo Ning et al.\nâ€¢RL [13] uses the Proximal Policy Optimization (PPO) [32] to\ntrain the attack policy to generate adversarial perturbations.\nâ€¢GA [26] employs the genetic algorithm to find the adversarial\nperturbation and insert them to the end of the benign input.\nâ€¢BAE [17] masks the crucial words within the input prompt\nand exploits the language model, i.e., BERT [25], to predict\nthe contextually appropriate perturbations.\nâ€¢LLMBA [44] directly utilizes large language models to gen-\nerate adversarial perturbations and insert them to the end\nof the benign input. The prompts used for perturbation gen-\neration are shown in Table 6 of Appendix B.2.\nâ€¢RP selects items randomly from the item set and inserts\nthem at a random position in usersâ€™ profiles.\nâ€¢RT selects words randomly from the vocabulary and inserts\nthem at a random position in the benign prompt.\nâ€¢RPGP selects tokens randomly and inserts them at the posi-\ntion specified by the proposed method.\nâ€¢C-w/o PT directly uses prompts to guide the LLM-based\nagent to generate perturbations without policy tuning.\nâ€¢CheatAgent uses prompt-tuning to guide the LLM-based\nagent to produce high-quality perturbations.\n4.1.4 Implementation. The proposed methods and all baselines\nare implemented by Pytorch. All victim models (P5 and TALLRec)\nare implemented according to their official codes. ForP5 model, we\nuse two different item indexing methods (i.e., random indexing and\nsequential indexing) to demonstrate the robustness of the gener-\nated adversarial perturbations. For TALLRec model, since it needs\nratings to divide the user-interested items and user-hated items, we\nfine-tune the LLaMA model on a textual dataset reconstructed by\nML1M dataset and test its vulnerability on this dataset.\nWe initialize the population with a quantity of 50 and iterate for\n10 epochs to obtain the final perturbation for GA. Bert [25] is used\nto generate 50 candidates, and BAE selects the perturbation that is\nmost effective in undermining the recommendation performance.\nAs for the proposed CheatAgent, we use distinct prompts P âˆˆ\n{Pğ‘ƒ,Pğ‘‰ğ‘¢ğ‘– }to generate candidates as mentioned in Section 2.3. The\nprompts used for perturbation generation are shown in Table 6\nof Appendix B.2. For P5, we set ğ‘˜ = 10 and ğ‘› = 10 as defaults,\nand for TALLRec, we set ğ‘˜ = 6 and ğ‘› = 12. T5 [31] is employed\nas the LLM-based agent A. â–³is set to 3 for all methods, which\nmeans we can only insert three perturbed words/items into the\ninput prompt ğ‘‹. Besides, during experiments, for the item within\nthe userâ€™s profile ğ‘‰ğ‘¢ğ‘– , we observe that masking a pair of items\nand inserting perturbations to the middle of the maximum-impact\nitems can achieve better attack performance. We argue that this may\nbe due to the significant impact of the order of item interactions\non user preferences. More experiments and discussion about this\nphenomenon are shown in Table 4 of Appendix B.3.\n4.1.5 Evaluation Metrics. For P5 model, we consider two met-\nrics, formulated as ASR-H@ğ‘Ÿ = 1 âˆ’ÂšH@ğ‘Ÿ/H@ğ‘Ÿ and ASR-N@ğ‘Ÿ =\n1 âˆ’ÂšN@ğ‘Ÿ/N@ğ‘Ÿ. H@ğ‘Ÿ and N@ğ‘Ÿ are Top-ğ‘Ÿ Hit Ratio and Normalized\nDiscounted Cumulative Gain [4, 18], which are two widely-used\nmetrics for evaluating the performance of LLM-empowered RecSys.\nÂšH@ğ‘Ÿ and ÂšN@ğ‘Ÿ are the Top-ğ‘Ÿ Hit Ratio and Normalized Discounted\nCumulative Gain when the victim model is under attack. The larger\nthe decrease in H@ğ‘Ÿ and N@ğ‘Ÿ, the better the algorithmâ€™s attack\nperformance. In this paper, ğ‘Ÿ is set to 5 and 10, respectively. For\nTALLRec model, the recommendation results only contain \"Yes\"\nand \"No, \" which can be considered as a binary classification task.\nWe adopt Area Under the Receiver Operating Characteristic (AUC)\nas the metric to measure the recommendation performance, which\nis consistent with the work of Bao et al. [2]. ASR-A = 1âˆ’ÂšAUC/AUC\nis introduced to evaluate the attack performance, whereÂšAUC is the\nAUC when the TALLRec is under attacks.\n4.2 Attack Effectiveness\nWe first evaluate the attack effectiveness of the proposed method\nin this subsection. The attack performance of different approaches\nbased on P5 are summarised in Table 1 and Table 3 (Appendix B.3).\nFor TALLRec, the AUC and ASR-A are illustrated in Figure 3. Based\non comprehensive experiments, we have some following insights:\nâ€¢As shown in Table 1, the recommendation performance de-\ncreases by randomly inserting some token or item pertur-\nbations (e.g., RT and RP), indicating that the existing LLM-\nempowered recommender systems are highly vulnerable.\nThis observation will inspire researchers to pay more atten-\ntion to the robustness and trustworthiness of utilizing LLMs\nfor other downstream tasks.\nâ€¢We have discovered that the manually designed adversarial\nexamples, i.e., MD, cannot deceive the target victim model\neffectively by comparing it with other baselines. Therefore,\nwe require more potent attack strategies instead of relying\nsolely on the manual construction of adversarial examples\nto explore the vulnerability of LLM-empowered RecSys.\nâ€¢As shown in Table 1 and Table 3 (Appendix B.3), the pro-\nposed method outperforms other baselines and undermines\nthe recommendation performance dramatically, indicating\nthe effectiveness of the proposed method. Despite the nu-\nmerous distinctions between P5 and TALLRec, the proposed\nmethod effectively deceives both, showcasing its resilience\nagainst the architecture of the victim RecSys.\nâ€¢By comparing RPGP with RP and RT, we can observe that\ninserting random perturbations adjacent to the important to-\nkens leads to a rise in attack performance. This demonstrates\nthe effectiveness of the proposed insertion positioning.\nâ€¢Based on the results of C-w/o PT, we observe that perturba-\ntions generated by the LLM-based agent can effectively at-\ntack the RecSys even without prompt tuning, demonstrating\nthe potential of the LLM-based agent in performing attacks.\nBesides, this phenomenon also leads us to speculate that\ndespite the fine-tuning of existing LLM-empowered RecSys\non downstream recommendation tasks, they still retain some\nvulnerabilities of LLMs.\nâ€¢By comparing the experimental results of C-w/o PT with\nCheatAgent, we have observed a significant improvement\nin the attack performance of the agent through policy tun-\ning, demonstrating the effectiveness of the proposed prompt\ntuning-based attack policy optimization strategy.\n4.3 Semantic Similarity\nIn this subsection, we test whether inserting adversarial perturba-\ntions will change the semantic information of the benign prompt.\nCheatAgent: Attacking LLM-Empowered Recommender\nSystems via LLM Agent KDD â€™24, August 25â€“29, 2024, Barcelona, Spain\n0.41\n0.46\n0.51\n0.56\n0.61\n0.66\n0.71AUC\nBenign MD\nRP RT\nRL GA\nBAE LLMBA\nRPGP C-w/o PT\nCheatAgent\n(a) AUC\n0.09\n0.14\n0.19\n0.24\n0.29\n0.34\n0.39ASR-A\nMD RP\nRT RL\nGA BAE\nLLMBA RPGP\nC-w/o PT CheatAgent (b) ASR-A\nFigure 3: Attack performance of different methods (Victim\nmodel: TALLRec).\n0.98\n0.98\n0.99\n0.99\n1.00\n1.00\n1.01\n1.01Cosine Similarity\nBenign MD\nRP RT\nRL GA\nBAE LLMBA\nRPGP C-w/o PT\nCheatAgent\n(a) Cosine similarity\n1.00\n2.00\n3.00\n4.00\n5.00\n6.00\n7.00\n8.001-Norm\nBenign MD\nRP RT\nRL GA\nBAE LLMBA\nRPGP C-w/o PT\nCheatAgent (b) 1-Norm\nFigure 4: The semantic similarity between the benign and\nadversarial prompts.\nTable 1: Attack Performance of different methods. (Victim\nModel: P5; Indexing: Sequential)\nMethodsH@5â†“H@10â†“N@5â†“N@10â†“ASR-H@5â†‘ASR-H@10â†‘ASR-N@5â†‘ASR-N@10â†‘\nML1M\nBenign0.2116 0.3055 0.1436 0.1737 / / / /MD 0.1982 0.2818 0.1330 0.1602 0.0634 0.0775 0.0735 0.0776RP 0.2051 0.2940 0.1386 0.1671 0.0305 0.0374 0.0347 0.0380RT 0.1949 0.2800 0.1317 0.1591 0.0790 0.0835 0.0826 0.0839RL 0.1917 0.2788 0.1296 0.1576 0.0939 0.0873 0.0974 0.0926GA 0.08290.1419 0.0532 0.0721 0.60800.5355 0.6298 0.5849BAE 0.1606 0.2440 0.1047 0.1315 0.2410 0.2011 0.2712 0.2432LLMBA0.1889 0.2825 0.1284 0.1585 0.1072 0.0753 0.1061 0.0876RPGP 0.1733 0.2588 0.1164 0.1439 0.1808 0.1528 0.1893 0.1715C-w/o PT0.0844 0.13920.05310.0706 0.6009 0.5442 0.6303 0.5935CheatAgent0.0614 0.1132 0.0389 0.0555 0.7097 0.6293 0.7290 0.6805\nLastFM\nBenign0.0404 0.0606 0.0265 0.0331 / / / /MD 0.0339 0.0477 0.0230 0.0274 0.1591 0.2121 0.1333 0.1713RP 0.0394 0.0550 0.0241 0.0291 0.0227 0.0909 0.0921 0.1195RT 0.0413 0.0550 0.0271 0.0315 -0.0227 0.0909 -0.0216 0.0463RL 0.0294 0.0468 0.0200 0.0256 0.2727 0.2273 0.2460 0.2272GA 0.0248 0.0431 0.0156 0.0216 0.3864 0.2879 0.4111 0.3477BAE 0.0165 0.0339 0.0093 0.0149 0.5909 0.4394 0.6480 0.5497LLMBA0.0404 0.0541 0.0291 0.0336 0.0000 0.1061 -0.0969 -0.0150RPGP 0.0294 0.0514 0.0184 0.0253 0.2727 0.1515 0.3076 0.2349C-w/o PT0.01380.02750.00910.0135 0.6591 0.5455 0.6580 0.5924CheatAgent0.0119 0.0257 0.0072 0.0118 0.7045 0.5758 0.7269 0.6445\nTaobao\nBenign0.1420 0.1704 0.1100 0.1191 / / / /MD 0.1365 0.1624 0.1085 0.1170 0.0392 0.0471 0.0130 0.0180RP 0.1250 0.1512 0.0977 0.1061 0.1200 0.1125 0.1117 0.1091RT 0.1396 0.1658 0.1090 0.1174 0.0173 0.0269 0.0092 0.0145RL 0.1376 0.1650 0.1075 0.1163 0.0311 0.0317 0.0222 0.0234GA 0.1294 0.1579 0.0993 0.1086 0.0888 0.0731 0.0966 0.0886BAE 0.1278 0.1519 0.0989 0.1066 0.1003 0.1087 0.1009 0.1050LLMBA0.1353 0.1624 0.1050 0.1138 0.0473 0.0471 0.0452 0.0448RPGP 0.1258 0.1512 0.0971 0.1053 0.1142 0.1125 0.1167 0.1159C-w/o PT0.10170.12580.07370.0815 0.2837 0.2615 0.3298 0.3161CheatAgent0.0985 0.1229 0.0717 0.0796 0.3068 0.2788 0.3480 0.3319\nBoldfonts and underlinesindicate the best and second-best attack performance, respectively.\nWe use the bge-large-en model [41] to map the adversarial and\nbenign prompt to a 512-dimension vector. Cosine similarity and\n1-Norm difference are calculated to measure the semantic similarity.\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n6 8 10 12 14\nH@5 H@10 N@5 N@10\n(a) H @ğ‘Ÿ and N@ğ‘Ÿ w.r.t.ğ‘˜\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n6 8 10 12 14\nASR-H@5 ASR-H@10\nASR-N@5 ASR-N@10 (b) ASR-A @ğ‘Ÿ and ASR-N@ğ‘Ÿ w.r.t.ğ‘˜\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n6 8 10 12 14\nH@5 H@10 N@5 N@10\n(c) H @ğ‘Ÿ and N@ğ‘Ÿ w.r.t.ğ‘›\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n6 8 10 12 14\nASR-H@5 ASR-H@10\nASR-N@5 ASR-N@10 (d) ASR-A@ğ‘Ÿ and ASR-N@ğ‘Ÿ w.r.t.ğ‘›\nFigure 5: Effect of the hyper-parameters ğ‘˜ and ğ‘›.\nFirst, as shown in Figure 4, all methods exhibit a high cosine\nsimilarity and a low 1-norm difference, primarily due to the im-\nposed constraint on the intensity of perturbations. Second, there is a\nminimal semantic discrepancy between RP and the benign prompt,\nindicating that inserting perturbations to the usersâ€™ profiles ğ‘‰ğ‘¢ğ‘– is\nmore stealthy than perturbing input prompts ğ‘ƒ. Third, apart from\nRP, our proposed method achieves the highest cosine similarity\nand the smallest 1-norm difference, demonstrating the effectiveness\nof our approach in attacking RecSys while maintaining stealthi-\nness. This characteristic makes our method more difficult to detect,\nthereby posing a greater threat.\n4.4 Ablation Study\nIn this subsection, some ablation studies are constructed to in-\nvestigate the effectiveness of each proposed component. Three\nvariants are introduced here for comparison: 1) CheatAgent-RP\nuses the LLM agent-empowered perturbation generation to pro-\nduce perturbations and insert them into the random positions. 2)\nCheatAgent-I fine-tunes the prefix prompt with random initializa-\ntion. 3) CheatAgent-T directly employs the initial prefix prompt\nto produce the adversarial perturbations without further policy\ntuning. The results are shown in Table 2. Through the comparison\nof CheatAgent with CheatAgent-RP, we demonstrate that the in-\nsertion of perturbations into random positions within the input\nleads to a significant decrease in attack performance. Therefore,\nit is imperative to identify the token with the maximum impact\nin order to enhance the attack success rate. By comparing the re-\nsults of CheatAgent with those of CheatAgent-I and CheatAgent-T,\nwe demonstrate that both the initial policy generation and the\nself-reflection policy optimization processes are necessary for the\nLLM-based agent to increase the attack performance.\n4.5 Parameter Analysis\nIn this subsection, we study the impact of model hyper-parameters.\nThere are mainly two hyper-parameters, i.e., ğ‘›and ğ‘˜, associated\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Liang-bo Ning et al.\nTable 2: Comparison between CheatAgent and its variants on three datasets. Bold fonts denotes the best performance.\nDatasets Methods H@5 â†“ H@10â†“ N@5â†“ N@10â†“ ASR-H@5â†‘ ASR-H@10â†‘ ASR-N@5â†‘ ASR-N@10â†‘\nLastFM\nCheatAgent0.0119 0.0257 0.0072 0.0118 0.7045 0.5758 0.7269 0.6445\nCheatAgent-RP 0.0193 0.0358 0.0111 0.0166 0.5227 0.4091 0.5816 0.4995\nCheatAgent-I 0.0147 0.0284 0.0096 0.0140 0.6364 0.5303 0.6377 0.5769\nCheatAgent-T 0.0128 0.0259 0.0074 0.0120 0.6818 0.5730 0.7199 0.6371\nML1M\nCheatAgent0.0614 0.1132 0.0389 0.0555 0.7097 0.6293 0.7290 0.6805\nCheatAgent-RP 0.1336 0.2036 0.0881 0.1107 0.3685 0.3333 0.3866 0.3630\nCheatAgent-I 0.0810 0.1354 0.0512 0.0686 0.6174 0.5566 0.6437 0.6050\nCheatAgent-T 0.0727 0.1205 0.0456 0.0608 0.6565 0.6054 0.6825 0.6497\nTaobao\nCheatAgent0.0985 0.1229 0.0717 0.0796 0.3068 0.2788 0.3480 0.3319\nCheatAgent-RP 0.1258 0.1497 0.0960 0.1037 0.1142 0.1212 0.1271 0.1293\nCheatAgent-I 0.1024 0.1263 0.0744 0.0821 0.2791 0.2587 0.3233 0.3107\nCheatAgent-T 0.0985 0.1243 0.0718 0.0802 0.3068 0.2702 0.3468 0.3272\nwith the attack performance. ğ‘˜ is the number of the randomly ini-\ntialized prefix prompt during the initial policy generation process.\nGiven an attack instruction,ğ‘›is the number of the generated pertur-\nbations of the LLM-based agent. We fix one of them and gradually\nvary the other, observing its impact on the attack performance.\nThe results are illustrated in Figure 5. With the change of ğ‘˜, the\nH@ğ‘Ÿ, N@ğ‘Ÿ, ASR-A@ğ‘Ÿand ASR-N@ğ‘Ÿfluctuate within a small range,\nwhich demonstrates the robustness of the proposed method to the\nhyper-parameters ğ‘˜. As for ğ‘›, the attack performance gradually\nstrengthens asğ‘›increases. However, largeğ‘›will consume abundant\ntime. Consequently, we set ğ‘› = 10 as the default in this paper to\nachieve a balance of the attack performance and efficiency.\n5 RELATED WORK\nIn this section, we briefly overview some related studies focusing\non adversarial attacks for recommender systems. Due to the space\nlimitation, some studies about the LLM-empowered RecSys and\nvulnerabilities of LLM are reviewed in Appendix C.\nGenerally, adversarial attacks for recommender systems are\nbroadly divided into two categories [14]: 1) Evasion Attack hap-\npens during the inference phase. Given a fixed, well-trained RecSys,\nattackers aim to modify the userâ€™s profiles to manipulate the recom-\nmendation outcome. 2) Poisoning Attack occurs during the data\ncollection before model training. The attackers inject the poisoned\nfake users into the training set to misguide the model training and\nundermine its overall performance.\nEarly methods including heuristic attacks [3, 38] and gradient-\nbased attacks [5, 27] have demonstrated a high rate of success in\nattacking white-box recommendation models. However, these meth-\nods cannot be directly applied to attack black-box recommender\nsystems (RecSys) due to the limited knowledge about the victim\nmodel. Recently, reinforcement learning has emerged as a viable\napproach for attacking the black-box victim model. PoisonRec is\nthe first black-box attack framework, which leverages the reinforce-\nment learning architecture to automatically learn effective attack\nstrategies [33]. Chen et al. [4] propose a knowledge-enhanced black-\nbox attack by exploiting itemsâ€™ attribute features (i.e., Knowledge\nGraph) to enhance the item sampling process. Instead of generating\nfake usersâ€™ profiles from scratch, Fan et al. [9, 15] have developed a\ncopy-based mechanism to obtain real user profiles for poisoning the\ntarget black-box RecSys. MultiAttack [13] also considers utilizing\nsocial relationships to degrade the performance of RecSys.\n6 CONCLUSION\nIn this paper, we propose a novel attack frameworkCheatAgent by\nintroducing an autonomous LLM agent to attack LLM-empowered\nrecommender systems under the black-box scenario. Specifically,\nour method first identifies the insertion position for maximum im-\npact with minimal input modification. Subsequently, CheatAgent\ncrafts subtle perturbations to insert into the prompt by leveraging\nthe LLM as the attack agent. To improve the quality of adversarial\nperturbations, we further develop prompt tuning techniques to\nimprove attacking strategies via feedback from the victim RecSys it-\neratively. Comprehensive experiments on three real-world datasets\nshow the effectiveness of our proposed methods and highlight the\nvulnerability of LLM-empowered recommender systems against\nadversarial attacks.\nACKNOWLEDGMENTS\nThe research described in this paper has been partly supported\nby the National Natural Science Foundation of China (project no.\n62102335), General Research Funds from the Hong Kong Research\nGrants Council (project no. PolyU 15200021, 15207322, and 15200023),\ninternal research funds from The Hong Kong Polytechnic Uni-\nversity (project no. P0036200, P0042693, P0048625, P0048752, and\nP0051361), Research Collaborative Project no. P0041282, and SHTM\nInterdisciplinary Large Grant (project no. P0043302).\nREFERENCES\n[1] Gati V Aher, Rosa I Arriaga, and Adam Tauman Kalai. 2023. Using large language\nmodels to simulate multiple humans and replicate human subject studies. In\nInternational Conference on Machine Learning . PMLR, 337â€“371.\n[2] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan\nHe. 2023. TALLRec: An Effective and Efficient Tuning Framework to Align\nLarge Language Model with Recommendation. In Proceedings of the 17th ACM\nConference on Recommender Systems .\n[3] Robin Burke, Bamshad Mobasher, and Runa Bhaumik. 2005. Limited knowledge\nshilling attacks in collaborative filtering systems. In Proceedings of 3rd interna-\ntional workshop on intelligent techniques for web personalization (ITWP 2005), 19th\ninternational joint conference on artificial intelligence (IJCAI 2005) . 17â€“24.\n[4] Jingfan Chen, Wenqi Fan, Guanghui Zhu, Xiangyu Zhao, Chunfeng Yuan, Qing\nLi, and Yihua Huang. 2022. Knowledge-enhanced Black-box Attacks for Recom-\nmendations. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge\nDiscovery and Data Mining . 108â€“117.\n[5] Konstantina Christakopoulou and Arindam Banerjee. 2019. Adversarial attacks\non an oblivious recommender. In Proceedings of the 13th ACM Conference on\nRecommender Systems . 322â€“330.\n[6] Amit Daniely, Roy Frostig, and Yoram Singer. 2016. Toward deeper understanding\nof neural networks: The power of initialization and a dual view on expressivity.\nAdvances in neural information processing systems 29 (2016).\nCheatAgent: Attacking LLM-Empowered Recommender\nSystems via LLM Agent KDD â€™24, August 25â€“29, 2024, Barcelona, Spain\n[7] Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu\nWang, Tianwei Zhang, and Yang Liu. 2023. Jailbreaker: Automated Jailbreak\nAcross Multiple Large Language Model Chatbots. arXiv preprint arXiv:2307.08715\n(2023).\n[8] Swati Dongre and Jitendra Agrawal. 2023. Deep Learning-Based Drug Recommen-\ndation and ADR Detection Healthcare Model on Social Media. IEEE Transactions\non Computational Social Systems (2023).\n[9] Wenqi Fan, Tyler Derr, Xiangyu Zhao, Yao Ma, Hui Liu, Jianping Wang, Jiliang\nTang, and Qing Li. 2021. Attacking black-box recommendations via copying\ncross-domain user profiles. In 2021 IEEE 37th International Conference on Data\nEngineering (ICDE) . IEEE, 1583â€“1594.\n[10] Wenqi Fan, Xiaorui Liu, Wei Jin, Xiangyu Zhao, Jiliang Tang, and Qing Li. 2022.\nGraph Trend Filtering Networks for Recommendation. In Proceedings of the 45th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval. 112â€“121.\n[11] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin.\n2019. Graph neural networks for social recommendation. In The world wide web\nconference. 417â€“426.\n[12] Wenqi Fan, Yao Ma, Dawei Yin, Jianping Wang, Jiliang Tang, and Qing Li. 2019.\nDeep social collaborative filtering. In Proceedings of the 13th ACM Conference on\nRecommender Systems . 305â€“313.\n[13] Wenqi Fan, Shijie Wang, Xiao-yong Wei, Xiaowei Mei, and Qing Li. 2023.\nUntargeted Black-box Attacks for Social Recommendations. arXiv preprint\narXiv:2311.07127 (2023).\n[14] Wenqi Fan, Xiangyu Zhao, Xiao Chen, Jingran Su, Jingtong Gao, Lin Wang,\nQidong Liu, Yiqi Wang, Han Xu, Lei Chen, et al. 2022. A Comprehensive Survey\non Trustworthy Recommender Systems. arXiv preprint arXiv:2209.10117 (2022).\n[15] Wenqi Fan, Xiangyu Zhao, Qing Li, Tyler Derr, Yao Ma, Hui Liu, Jianping Wang,\nand Jiliang Tang. 2023. Adversarial Attacks for Black-Box Recommender Systems\nVia Copying Transferable Cross-Domain User Profiles. IEEE Transactions on\nKnowledge and Data Engineering (2023).\n[16] Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. 2018. Black-box genera-\ntion of adversarial text sequences to evade deep learning classifiers. In 2018 IEEE\nSecurity and Privacy Workshops (SPW) . IEEE, 50â€“56.\n[17] Siddhant Garg and Goutham Ramakrishnan. 2020. BAE: BERT-based Adversarial\nExamples for Text Classification. InProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) . 6174â€“6181.\n[18] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022.\nRecommendation as language processing (rlp): A unified pretrain, personalized\nprompt & predict paradigm (p5). In Proceedings of the 16th ACM Conference on\nRecommender Systems . 299â€“315.\n[19] F Maxwell Harper and Joseph A Konstan. 2015. The movielens datasets: History\nand context. Acm transactions on interactive intelligent systems (2015).\n[20] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng\nWang. 2020. Lightgcn: Simplifying and powering graph convolution network for\nrecommendation. In ACM SIGIR.\n[21] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng\nChua. 2017. Neural collaborative filtering. In Proceedings of the 26th international\nconference on world wide web . 173â€“182.\n[22] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2020. Is bert really\nrobust? a strong baseline for natural language attack on text classification and\nentailment. In Proceedings of the AAAI conference on artificial intelligence .\n[23] Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tat-\nsunori Hashimoto. 2023. Exploiting programmatic behavior of llms: Dual-use\nthrough standard security attacks. arXiv preprint arXiv:2302.05733 (2023).\n[24] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom-\nmendation. In 2018 IEEE international conference on data mining . 197â€“206.\n[25] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of NAACL-HLT . 4171â€“4186.\n[26] Raz Lapid, Ron Langberg, and Moshe Sipper. 2023. Open Sesame! Universal Black\nBox Jailbreaking of Large Language Models. arXiv preprint arXiv:2309.01446\n(2023).\n[27] Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik. 2016. Data poi-\nsoning attacks on factorization-based collaborative filtering. Advances in neural\ninformation processing systems 29 (2016).\n[28] Jiatong Li, Yunqing Liu, Wenqi Fan, Xiao-Yong Wei, Hui Liu, Jiliang Tang, and\nQing Li. 2023. Empowering Molecule Discovery for Molecule-Caption Trans-\nlation with Large Language Models: A ChatGPT Perspective. arXiv preprint\narXiv:2306.06615 (2023).\n[29] Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu\nZhu, Huifeng Guo, Yong Yu, Ruiming Tang, et al . 2023. How Can Recom-\nmender Systems Benefit from Large Language Models: A Survey. arXiv preprint\narXiv:2306.05817 (2023).\n[30] Han Liu, Zhi Xu, Xiaotong Zhang, Feng Zhang, Fenglong Ma, Hongyang Chen,\nHong Yu, and Xianchao Zhang. 2023. HQA-Attack: Toward High Quality Black-\nBox Hard-Label Adversarial Attack on Text. In Thirty-seventh Conference on\nNeural Information Processing Systems .\n[31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of\ntransfer learning with a unified text-to-text transformer. The Journal of Machine\nLearning Research 21, 1 (2020), 5485â€“5551.\n[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\n2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347\n(2017).\n[33] Junshuai Song, Zhao Li, Zehong Hu, Yucheng Wu, Zhenpeng Li, Jian Li, and\nJun Gao. 2020. Poisonrec: an adaptive data poisoning framework for attacking\nblack-box recommender systems. In 2020 IEEE 36th International Conference on\nData Engineering (ICDE) . IEEE, 157â€“168.\n[34] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.\n2019. BERT4Rec: Sequential recommendation with bidirectional encoder rep-\nresentations from transformer. In Proceedings of the 28th ACM international\nconference on information and knowledge management . 1441â€“1450.\n[35] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971 (2023).\n[36] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang,\nZhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2023. A survey on large\nlanguage model based autonomous agents.arXiv preprint arXiv:2308.11432 (2023).\n[37] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2023. Jailbroken: How\ndoes llm safety training fail? arXiv preprint arXiv:2307.02483 (2023).\n[38] Chad Williams and Bamshad Mobasher. 2006. Profile injection attack detection\nfor securing collaborative recommender systems.DePaul University CTI Technical\nReport (2006), 1â€“47.\n[39] Chuhan Wu, Fangzhao Wu, Yongfeng Huang, and Xing Xie. 2023. Personal-\nized news recommendation: Methods and challenges. ACM Transactions on\nInformation Systems 41, 1 (2023), 1â€“50.\n[40] Yiqing Wu, Ruobing Xie, Zhao Zhang, Yongchun Zhu, Fuzhen Zhuang, Jie Zhou,\nYongjun Xu, and Qing He. 2023. Attacking Pre-trained Recommendation. In\nProceedings of the 46th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval . 1811â€“1815.\n[41] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighof. 2023. C-pack:\nPackaged resources to advance general chinese embedding. arXiv preprint\narXiv:2309.07597 (2023).\n[42] Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, and Muhao Chen. 2023.\nInstructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for\nLarge Language Models. arXiv preprint arXiv:2305.14710 (2023).\n[43] Shuyuan Xu, Wenyue Hua, and Yongfeng Zhang. 2023. OpenP5: Benchmarking\nFoundation Models for Recommendation. arXiv preprint arXiv:2306.11134 (2023).\n[44] Xilie Xu, Keyi Kong, Ning Liu, Lizhen Cui, Di Wang, Jingfeng Zhang, and Mohan\nKankanhalli. 2023. An LLM can Fool Itself: A Prompt-Based Adversarial Attack.\narXiv preprint arXiv:2310.13345 (2023).\n[45] Jiaqi Xue, Mengxin Zheng, Ting Hua, Yilin Shen, Yepeng Liu, Ladislau BÃ¶lÃ¶ni,\nand Qian Lou. 2023. TrojLLM: A Black-box Trojan Prompt Attack on Large\nLanguage Models. In Thirty-seventh Conference on Neural Information Processing\nSystems.\n[46] Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Jianquan Li,\nGuiming Chen, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, et al. 2023. HuatuoGPT,\ntowards Taming Language Model to Be a Doctor. arXiv preprint arXiv:2305.15075\n(2023).\n[47] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic Chain\nof Thought Prompting in Large Language Models. In The Eleventh International\nConference on Learning Representations .\n[48] Zhen Zhang, Guanhua Zhang, Bairu Hou, Wenqi Fan, Qing Li, Sijia Liu, Yang\nZhang, and Shiyu Chang. 2023. Certified Robustness for Large Language Models\nwith Self-Denoising. arXiv preprint:2307.07171 (2023).\n[49] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey\nof large language models. arXiv preprint arXiv:2303.18223 (2023).\n[50] Zihuai Zhao, Wenqi Fan, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Zhen\nWen, Fei Wang, Xiangyu Zhao, Jiliang Tang, et al. 2024. Recommender systems\nin the era of large language models (llms). IEEE Transactions on Knowledge and\nData Engineering (2024).\n[51] Han Zhu, Xiang Li, Pengye Zhang, Guozheng Li, Jie He, Han Li, and Kun Gai.\n2018. Learning tree-based deep model for recommender systems. In Proceedings\nof the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data\nMining. 1079â€“1088.\n[52] Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang,\nFurong Huang, Ani Nenkova, and Tong Sun. 2023. AutoDAN: Automatic and\nInterpretable Adversarial Attacks on Large Language Models. arXiv preprint\narXiv:2310.15140 (2023).\n[53] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. 2023. Universal\nand transferable adversarial attacks on aligned language models. arXiv preprint\narXiv:2307.15043 (2023).\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Liang-bo Ning et al.\nA WHOLE PROCESS OF CHEATAGENT\nAlgorithm 1: CheatAgent\nInput:\nInput ğ‘‹, LLM agent A, Attackerâ€™s Instruction\nPâˆˆ{P ğ‘ƒ,Pğ‘‰ğ‘¢ğ‘– }, iteration ğ‘‡.\nOutput: Adversarial perturbations Ë†ğ›¿ğ‘‡.\nProcedure:\n1 Mask each token within ğ‘‹ and find the tokens Swith\nmaximal impact for perturbation insertion ;\n2 for ğ‘ ğ‘– in Sdo\n3 Randomly initialize ğ‘˜ prefix prompts [F1,..., Fğ‘˜];\n4 Generate perturbation candidates Bğ‘—,ğ‘— âˆˆ{1,ğ‘˜}\naccording to Eq (1) ;\n5 Select the optimal initialization of the prefix prompt F0\naccording to Eq (2) ;\n6 for t in 1:T do\n7 Generate a set of perturbations Bğ‘‡ ;\n8 Divide the perturbation into positive and negative\ncategories according to Eq (3) ;\n9 Compute the loss according to Eq (4) ;\n10 Update the prefix prompt according to\nFğ‘‡+1 = Fğ‘‡ âˆ’ğ›¾ Â·âˆ‡Fğ‘‡ LFğ‘‡ ;\n11 Select the optimal perturbation Ë†ğ›¿ğ‘‡ according to Eq (5) ;\n12 end for\nB EXPERIMENTAL DETAILS\nDue to the space limitation, some details of the experiments and\ndiscussions are shown in this section.\nB.1 Datasets Statistics\nWe utilize three datasets, i.e., ML1M, LastFM, and Taobao, to con-\nstruct comprehensive experiments. The ML1M dataset is a widely-\nused benchmark dataset in the field of recommender systems, which\ncontains rating data from the MovieLens website, specifically col-\nlected from around 6,040 users and their interactions with around\n3,000 movies. The dataset provides information such as user ratings,\nmovie attributes, and timestamps, making it suitable for various rec-\nommendation tasks and evaluation of recommendation algorithms.\nThe LastFM dataset is another popular dataset, which consists of\nuser listening histories from the Last.fm music streaming service.\nThe dataset includes information about user listening sessions,\nsuch as artist and track names, timestamps, and user profiles. The\nTaobao dataset is a large-scale e-commerce dataset collected from\nthe Taobao online shopping platform. It contains a rich set of user\nbehaviors, including browsing, searching, clicking, and purchasing\nactivities. The dataset provides valuable insights into user prefer-\nences, purchasing patterns, and item characteristics.\nFor P5 model, all used datasets are processed according to the\nwork of Geng et al. [18], Xu et al. [43]. For TALLRec model, we\nprocess the ML1M dataset according to the work of Bao et al .\n[2]. It should be noted that TALLRec divides the usersâ€™ profiles\nwith extensive interactions into multiple segments, resulting in\nnumerous similar users with only one or two different items in\ntheir profiles. To be more efficient, we randomly select 1,000 users\nfrom the generated datasets to test the performance of different\nmethods.\nB.2 Implementation Details\nFor MD, we manually design two adversarial prompts to reverse the\nsemantic information of the benign input to guide the victim RecSys\nto produce opposite recommendations. The manually-designed\nadversarial prompts are shown in Table 5. As we mentioned in\nSection 3.3, we use distinct prompts to generate perturbations. The\nused prompts are shown in Table 6. ForLLMBA, we design a similar\nprompt to generate perturbations, which is also shown in Table 6.\nB.3 Additional Experiments\nAttack Effectiveness. Due to the space limitation, the results\nbased on the P5 model that uses random indexing strategy are\nshown in Table 3. We can observe that, except for the LastFM\ndataset, the proposed method consistently outperforms other base-\nlines and significantly undermines the recommendation perfor-\nmance. We argue that the effectiveness of the proposed method\non the LastFM dataset is hindered due to the poor recommenda-\ntion performance of the target RecSys. Consequently, the limited\nvaluable information for policy tuning may impede CheatAgentâ€™s\nattack performance on this dataset.\nInsertion positioning strategy. As mentioned in Section 4.1.4, we\nobserve that masking a pair of items and inserting perturbations to\nthe middle of the maximum-impact items can achieve better attack\nperformance. To indicate the effectiveness of this strategy, we use a\nvariant of the proposed method for comparison. The results are illus-\ntrated in Table 4. CheatAgent-MI masks each word/item within the\ninput ğ‘‹ and inserts perturbations adjacent to the maximum-impact\nwords/item. From the experiment, we observe that the proposed\nmethod outperforms the variant on three datasets, demonstrating\nthe effectiveness of this strategy.\nC RELATED WORK\nC.1 LLM-Empowered Recommender Systems\nThe recent breakthrough of LLMs has initiated a new era for RecSys.\nDue to its powerful capability of understanding and reasoning, LLM\nhas been widely used to facilitate various recommendation tasks,\nsuch as news recommendation [ 39], drug recommendations [ 8],\netc. For example, BERT4Rec adopts Bidirectional Encoder Repre-\nsentations (i.e., BERT) to model usersâ€™ sequential behavior for rec-\nommendations [34]. Furthermore, TALLRec aligns the LLM (i.e.,\nLLaMA-7B) with recommendation data for sequential recommenda-\ntion [2]. Additionally, by studying the userâ€™s historical behavior and\npreferences, P5 can perform various recommendation tasks such as\nrating prediction and sequential recommendation and explain the\nrecommendations [18]. In conclusion, LLM-Empowered RecSys is\na fast-growing field, and it is necessary to study its vulnerabilities.\nCheatAgent: Attacking LLM-Empowered Recommender\nSystems via LLM Agent KDD â€™24, August 25â€“29, 2024, Barcelona, Spain\nTable 3: Attack Performance of different methods. We use bold fonts and underlines to indicate the best and second-best attack\nperformance, respectively. (Victim Model: P5; Indexing: Random)\nDatasets Methods H@5 â†“ H@10â†“ N@5â†“ N@10â†“ ASR-H@5â†‘ ASR-H@10â†‘ ASR-N@5â†‘ ASR-N@10â†‘\nML1M\nBenign 0.1058 0.1533 0.0693 0.0847 / / / /\nMD 0.0945 0.1459 0.0619 0.0785 0.1064 0.0486 0.1065 0.0728\nRP 0.0859 0.1320 0.0579 0.0728 0.1878 0.1393 0.1639 0.1401\nRT 0.0901 0.1328 0.0580 0.0718 0.1487 0.1339 0.1631 0.1522\nRL 0.0975 0.1419 0.0648 0.0792 0.0782 0.0745 0.0646 0.0650\nGA 0.0808 0.1248 0.0531 0.0673 0.2363 0.1857 0.2342 0.2046\nBAE 0.0942 0.1384 0.0611 0.0753 0.1095 0.0972 0.1181 0.1104\nLLMBA 0.0785 0.1137 0.0528 0.0643 0.2582 0.2581 0.2375 0.2407\nRPGP 0.0783 0.1219 0.0525 0.0665 0.2598 0.2052 0.2420 0.2142\nC-w/o PT 0.0517 0.0836 0.0329 0.0433 0.5117 0.4546 0.5245 0.4889\nCheatAgent0.0449 0.0742 0.0283 0.0377 0.5759 0.5162 0.5923 0.5546\nLastFM\nBenign 0.0128 0.0248 0.0072 0.0110 / / / /\nMD 0.0147 0.0303 0.0078 0.0128 -0.1429 -0.2222 -0.0944 -0.1586\nRP 0.0156 0.0229 0.0107 0.0131 -0.2143 0.0741 -0.4967 -0.1867\nRT 0.0092 0.0220 0.0045 0.0087 0.2857 0.1111 0.3678 0.2135\nRL 0.0064 0.0174 0.0032 0.0068 0.5000 0.2963 0.5501 0.3860\nGA 0.0073 0.0183 0.0038 0.0073 0.4286 0.2593 0.4756 0.3411\nBAE 0.0046 0.0119 0.0026 0.0050 0.6429 0.5185 0.6421 0.5463\nLLMBA 0.0165 0.0312 0.0094 0.0142 -0.2857 -0.2593 -0.3129 -0.2857\nRPGP 0.0119 0.0284 0.0068 0.0121 0.0714 -0.1481 0.0496 -0.0967\nC-w/o PT 0.0073 0.0174 0.0031 0.0062 0.4286 0.2963 0.5687 0.4331\nCheatAgent 0.0101 0.0183 0.0050 0.0075 0.2143 0.2593 0.3067 0.3174\nTaobao\nBenign 0.1643 0.1804 0.1277 0.1330 / / / /\nMD 0.1584 0.1764 0.1237 0.1296 0.0359 0.0218 0.0315 0.0258\nRP 0.1345 0.1547 0.0983 0.1049 0.1815 0.1426 0.2306 0.2114\nRT 0.1625 0.1797 0.1254 0.1310 0.0110 0.0036 0.0181 0.0149\nRL 0.1609 0.1766 0.1244 0.1296 0.0209 0.0209 0.0259 0.0258\nGA 0.1560 0.1740 0.1189 0.1248 0.0508 0.0354 0.0688 0.0619\nBAE 0.1517 0.1692 0.1172 0.1229 0.0768 0.0618 0.0827 0.0762\nLLMBA 0.1592 0.1766 0.1235 0.1291 0.0309 0.0209 0.0330 0.0292\nRPGP 0.1384 0.1563 0.1005 0.1063 0.1575 0.1335 0.2134 0.2007\nC-w/o PT 0.1150 0.1378 0.0808 0.0883 0.3001 0.2361 0.3671 0.3361\nCheatAgent0.1071 0.1317 0.0742 0.0823 0.3480 0.2698 0.4189 0.3815\nTable 4: Attack performance of different masking methods. (Victim Model: P5; Indexing: Sequential)\nDatasets Methods H@5 â†“ H@10â†“ N@5â†“ N@10â†“ ASR-H@5â†‘ ASR-H@10â†‘ ASR-N@5â†‘ ASR-N@10â†‘\nLastFM CheatAgent 0.0119 0.0257 0.0072 0.0118 0.7045 0.5758 0.7269 0.6445\nCheatAgent-MI 0.0128 0.0259 0.0074 0.0121 0.6818 0.5730 0.7227 0.6352\nML1M CheatAgent 0.0614 0.1132 0.0389 0.0555 0.7097 0.6293 0.7290 0.6805\nCheatAgent-MI 0.0697 0.1189 0.0444 0.0603 0.6706 0.6108 0.6908 0.6531\nTaobao CheatAgent 0.0985 0.1229 0.0717 0.0796 0.3068 0.2788 0.3480 0.3319\nCheatAgent-MI 0.1045 0.1278 0.0760 0.0835 0.2641 0.2500 0.3092 0.2991\nTable 5: Manually-designed adversarial prompts.\nVictim Model Methods Used Prompt\nP5\nBenign What is the top recommended item for dataset user_id who interacted with dataset item history?\nMD What is notthe top recommended item for dataset user_id who interacted with dataset item history?\nTALLRec\nBenign Given the userâ€™s preference and unpreference, identify whether the user will like the target movie by answering \"Yes. \" or \"No. \".\nMD Given the userâ€™s preference and unpreference, identify whether the user willnotlike the target movie by answering \"Yes. \" or \"No. \".\nC.2 Adversarial Attack for LLM\nBased on the generation method of adversarial prompts, the existing\nadversarial attacks for large language models can be broadly divided\ninto three categories:\n1) Artificial-based Methods. In the early stage of investigating the\nvulnerability of LLMs, multiple studies manually devised prompts\nto bypass the defense mechanism and guide LLMs to generate some\nKDD â€™24, August 25â€“29, 2024, Barcelona, Spain Liang-bo Ning et al.\nTable 6: Used prompt for perturbation generation.\nMethods Used Prompt\nCheatAgentPğ‘ƒ\nThe original input is{Original Prompt}and the prediction is{Target}. Please generate a few letters or words that\ncan change your prediction after inserting it as the{position}-th word of the input.\nPğ‘‰ğ‘¢ğ‘– According to the userâ€™s historical sequence{Userâ€™s Profile}, please predict the items that the user is not interested\nin.\nLLMBA The original input is{Original Prompt}and the prediction is{Target}. Please generate a few letters or words that\ncan change your prediction after inserting it after inserting it at the end of the input sentence.\nharmful content [23, 37], which is time-consuming and ineffectively\nwith the development of the LLMs.\n2) Optimization-based Methods. These methods exploit diverse\noptimization strategies, such as genetic algorithm [26], gradient-\nbased search [52, 53], reinforcement learning [45], to find the op-\ntimal perturbation. For example, Zou et al. [53] create the desired\nadversarial postfix by generating a candidate set according to the\ngradient and replacing the word from a candidate randomly. Lapid\net al. [26] propose to exploit the genetic algorithm to iteratively\ngenerate the universal adversarial prompt.\n3) LLM-based Methods. LLM is employed to generate adversarial\nsamples automatically, which is more efficient and diverse [7, 42].\nDeng et al. [7] propose to exploit the time-based characteristics\nintrinsic to deconstruct the defense mechanism of LLMs. An au-\ntomatic method for the generation of adversarial prompts is also\npresented by fine-tuning the LLM. Xu et al. [42] leverage the LLM to\ngenerate poisoned instructions and insert the backdoor into LLMs\nvia instruction tuning.\nD DISCUSSIONS\nDifference between APRec [ 40] and CheatAgent. The objective\nof APRec [40] is entirely different from this work. The recommenda-\ntion model employed by APRec is SASRec [24], which is not a large\nlanguage model and lacks the ability to comprehend textual lan-\nguage in LLM-based recommendations. Therefore, the vulnerability\nof LLM-empowered recommender systems is still not explored. To\nfill the gap in this area, our work takes the pioneering investigation\ninto the vulnerability of LLM-empowered RecSys.\nPractical Applications. The main goal of our research is to inves-\ntigate the vulnerability of existing LLM-empowered RecSys, so as\nto spread awareness about the trustworthiness of recommender sys-\ntems. From the industry perspective , our proposed CheatAgent\ncan assist them in evaluating the vulnerabilities of their deployed\nLLMs-based recommender systems. The enterprise desires that the\nLLM-empowered RecSys it employs is robust to small perturbations\n(e.g., random/bait clicks [10]). Assume that non-English-speaking\nusers who utilize LLM-empowered Shopping Assistant (e.g., Ama-\nzon AI Shopping Assistant â€˜Rufusâ€™) may unintentionally input their\nprompts with incorrect singular or plural forms, resulting in an\nadditional character â€˜aâ€™, considered as the token perturbation. Al-\nternatively, they may encounter enticing product titles and click on\nthem despite not genuinely liking the products, thereby introducing\nitem perturbation to their history interaction. If such perturbations\ncan significantly impact the recommendation outcomes of LLM-\nempowered RecSys, leading to the recommendation of undesired\nproducts to users, it would undermine their user experience. To\nprevent such occurrences, the company must investigate the vul-\nnerability of the LLM-empowered RecSys before deploying. In this\ncase, the attacker is the owner (e.g., system manager, system de-\nsigner, and algorithm developer) of the LLM-empowered RecSys\nand possesses the ability to access user interaction histories and\nmodify prompts, which is entirely plausible.\nNote that the assumptions required for the attack paradigm\nproposed in this paper are slightly strong since attackers are not\nalways the systemâ€™s owner and may not be able to manipulate and\nmodify the prompt directly. As our work is the first to investigate\nthe vulnerability of LLM-Enpowered RecSys, we believe that the\ninsights presented in this paper can enhance peopleâ€™s attention to\nthe security aspects of the system. We also hope that our work\ncan inspire future work to develop more advanced approaches and\npromote the trustworthiness of LLM-empowered recommender\nsystems.\nQuery Number and Running Time. We summarize the number\nof queries and time required to generate an adversarial example for\ndeceiving the victim system, shown as follows:\nTable 7: Query number and running time of various methods.\nMethods Query Number Running Time (s)\nGA 550 1.22\nBAE 151 2.72\nRL 501 5.37\nCheatAgent 490 4.50\nHere are some insightful observations from this experiment: 1)\nWe can observe that the proposed CheatAgent can achieve the best\nattack performance without significantly increasing the number of\nqueries, demonstrating the effectiveness of the proposed method.\nBesides, during applications, by leveraging the batch processing\ncapabilities of GPUs/TPUs, we can generate multiple adversarial\nexamples, store them in a list, and feed them into the target system\ntogether to significantly decrease the query times. 2) Due to the\nlarge action space, the reinforcement learning-based agent (RL)\nrequires more time to generate adversarial examples compared to\nCheatAgent, which demonstrates the efficiency of the proposed\nLLM-based agent. 3) Regarding methods such as GA and BAE,\nwhich utilize the genetic algorithm and BERT for perturbation\ngeneration, they are faster than the proposed method. The reason\nis that the proposed CheatAgent introduces an LLM to generate\nperturbations, which increases the time consumption. However,\nthe discrepancy in running time is marginal and acceptable."
}