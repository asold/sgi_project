{
  "title": "Benchmarking Large Language Model Capabilities for Conditional Generation",
  "url": "https://openalex.org/W4385570801",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2554187936",
      "name": "Joshua Maynez",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2122590769",
      "name": "Priyanka Agrawal",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2580156231",
      "name": "Sebastian Gehrmann",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W3035252911",
    "https://openalex.org/W3173360659",
    "https://openalex.org/W4297435087",
    "https://openalex.org/W3119878165",
    "https://openalex.org/W3186655327",
    "https://openalex.org/W3131933120",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4225727438",
    "https://openalex.org/W4205646617",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W3045462440",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W2795413297",
    "https://openalex.org/W3105424285",
    "https://openalex.org/W3186081172",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3102690631",
    "https://openalex.org/W2251926178",
    "https://openalex.org/W4385573234",
    "https://openalex.org/W4287633642",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4378908626",
    "https://openalex.org/W3120929527",
    "https://openalex.org/W2739874095",
    "https://openalex.org/W2250342921",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4389518729",
    "https://openalex.org/W3094040156",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3106445907",
    "https://openalex.org/W2994963504",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3172076499",
    "https://openalex.org/W4200634402",
    "https://openalex.org/W4312089323",
    "https://openalex.org/W4306808908",
    "https://openalex.org/W3174634068",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W3098495697",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W88719000",
    "https://openalex.org/W2996176596",
    "https://openalex.org/W2964237709",
    "https://openalex.org/W2963912046",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4225390052",
    "https://openalex.org/W2324349273"
  ],
  "abstract": "Pre-trained large language models (PLMs) underly most new developments in natural language processing. They have shifted the field from application-specific model pipelines to a single model that is adapted to a wide range of tasks. Autoregressive PLMs like GPT-3 or PaLM and associated techniques like fewshot learning, have additionally shifted the output modality to generation instead of classification or regression. Despite their ubiquitous use, the generation quality of language models is rarely evaluated when these models are introduced. Additionally, it is unclear how existing generation tasks–while they can be used to compare systems at a high level–relate to the real world use cases for which people have been adopting them. In this work, we discuss how to adapt existing application-specific generation benchmarks to PLMs and provide an in-depth, empirical study of the limitations and capabilities of PLMs in natural language generation tasks along dimensions such as scale, architecture, input and output language. Our results show that PLMs differ in their applicability to different data regimes and their generalization to multiple languages. They further inform practitioners as to which PLMs to use for a given generation task setup. We share best practices to be taken into consideration when benchmarking generation capabilities during the development of upcoming PLMs.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 9194–9213\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nBenchmarking Large Language Model Capabilities for Conditional\nGeneration\nJoshua Maynez\nGoogle DeepMind\njoshuahm@google.com\nPriyanka Agrawal\nGoogle DeepMind\npriyankagr@google.com\nSebastian Gehrmann\nGoogle Research\ngehrmann@google.com\nAbstract\nPre-trained large language models (PLMs) un-\nderlie most new developments in natural lan-\nguage processing. They have shifted the ﬁeld\nfrom application-speciﬁc model pipelines to a\nsingle model that is adapted to a wide range\nof tasks. Autoregressive PLMs like GPT-3\nor PaLM, alongside techniques like few-shot\nlearning, have additionally shifted the output\nmodality to generation instead of classiﬁca-\ntion or regression. Despite their ubiquitous\nuse, the generation quality of language mod-\nels is rarely evaluated when these models are\nintroduced. Additionally, it is unclear how ex-\nisting generation tasks—-while they can be\nused to compare systems at a high level—-\nrelate to the real world use cases for which\npeople have been adopting them. In this work,\nwe discuss how to adapt existing application-\nspeciﬁc generation benchmarks to PLMs and\nprovide an in-depth, empirical study of the\nlimitations and capabilities of PLMs in nat-\nural language generation tasks along dimen-\nsions such as scale, architecture, input and out-\nput language. Our results show that PLMs\ndiffer in their applicability to different data\nregimes and their generalization to multiple\nlanguages and inform which PLMs to use for\na given generation task setup. We share best\npractices to be taken into consideration when\nbenchmarking generation capabilities during\nthe development of upcoming PLMs.\n1 Introduction\nNatural language generation tasks require gener-\nating understandable text given textual or non-\nlinguistic information as input, such as documents,\ntables, or other structured forms. These texts seek\nto achieve a communicative goal (e.g., summarize\na document). The standard approach to tackle\nthese problems over the last years has been to\nstart with a pretrained encoder-decoder model like\nT5 (Raffel et al., 2020a) or BART (Lewis et al.,\n2020a) and ﬁnetune it on a corpus that captures\nthe downstream task. The recent much larger\npretrained language models use a decoder-only\narchitecture and upended this paradigm. These\nmodels enabled few-shot or in-context learning ap-\nproaches in which a model is presented with one or\nmore examples and tasked to continue generating\nwithout any ﬁnetuning. We refer to both kinds of\npretrained models as PLMs.\nDue to the lack of grounding in the speciﬁc task\nsetup, few-shot learning in generation settings\nleads to a model approaching the communicative\ngoal from very different angles. These diverse\nrange of outputs make the typical reference-based\nautomatic evaluation strategies largely incompati-\nble. While human evaluation can be used to over-\ncome this shortcoming, it is infeasible to monitor\nthe performance of an actively training model this\nway or to re-run all evaluations every time a new\nmodel is introduced. This leads to the question\nhow one should reliably monitor generation ca-\npabilities, a question that is only growing in im-\nportance as more tasks are approached by casting\nthem into generation setups.\nIn this work, we evaluate 8 models in few-shot\nand ﬁnetuning settings on 27 generation tasks cov-\nering 14 languages via automatic evaluation, pre-\nsenting the ﬁrst large-scale benchmark of PLMs\nin conditional NLG settings. We discuss design\nchoices and challenges to ensure a fair comparison\nbetween the different systems, including suitable\nmethods, tasks, and metrics. Based on our em-\npirical results, we derive recommendations that\ncould be used for future benchmarks during the\ndevelopment of PLMs. To combat the need for\nrepeating computationally expensive explorations,\nwe investigate how many evaluation examples are\nnecessary to identify differences between models\nand ﬁnd that, in many cases, fewer than 500 exam-\nples are sufﬁcient, which opens the path for future\nevaluation-only task developments.\n9194\n2 Background and Related Work\nThe shift from specialized pipelines toward pre-\ntrained language models has led to signiﬁcant\nchanges in how models are evaluated. We now fo-\ncus more on questions such as “how good are the\nlearned representations?” instead of user-facing\nmeasures of utility. The changes manifested in\nleaderboards and standard benchmarks that aim\nto characterize a wide range of model capabili-\nties (Ethayarajh and Jurafsky, 2020).\nAn additional recent shift is that from ﬁnetuning\ntoward few-shot learning. Models like T5 (Raffel\net al., 2020a), BART (Lewis et al., 2020a), and\nmT5 (Xue et al., 2021) were ﬁnetuned on super-\nvised datasets covering tasks including translation\nand summarization, and their outputs are com-\npared to “ground truth” outputs via widely used\nmetrics like ROUGE (Lin, 2004) which provide a\nnoisy indication of the “quality” of the output and\nwhich can be used to determine whether a model\nis better than others. 1 In contrast, large PLMs\nwith autoregressive language modeling pretrain-\ning objectives are more capable to produce results\nwithout explicit ﬁnetuning and are thus typically\nevaluated via few-shot and in-context approaches,\nwhere the model is given the task description and\nexemplars showing how the task should be com-\npleted. GPT-3 (Brown et al., 2020) and models that\nfollowed such as GLaM (Du et al., 2022), Gopher\n(Rae et al., 2021), and LaMDA (Thoppilan et al.,\n2022), have achieved few-shot state-of-the-art re-\nsults on a large number of tasks at their time of\npublication. However, few-shot approaches work\nbest for tasks with a clear answer such as classiﬁ-\ncation or span-based question-answering.2\nGeneration metrics penalize systems when their\nwriting style differs from how the references are\nwritten (Mathur et al., 2020; Freitag et al., 2020;\nMille et al., 2021). Without ﬁnetuning, there is no\nguarantee that PLMs produce outputs that look like\nthe ground truth, both in style and content. Recent\nwork found that these differences leads to sharp dif-\nferences in how humans and automatic metrics rate\nthe generation quality (Goyal et al., 2022). Due\nto this uncertainty, most evaluations of new PLMs\n1For an in-depth review of the usefulness of automatic\nmetrics, we refer to Gehrmann et al. (2022b) and point to\nSection 4 for a discussion of the application of metrics to\nbenchmarks.\n2We refer to the two task types as NLU and NLG tasks but\nnote that this distinction becomes fuzzy with autoregressive\nmodels since technically all answers are “generated”.\nare limited to NLU benchmarks such as Super-\nGLUE (Wang et al., 2019). For example, LaMDA\n(Thoppilan et al., 2022) did not evaluate on NLG\ntasks, GLaM (Du et al., 2022) limited its genera-\ntion evaluation to short span question answering\ntasks, and GPT-3 (Brown et al., 2020) evaluated\nonly on machine translation. A ﬁrst autoregressive\nPLM with broad NLG evaluation, PaLM (Chowdh-\nery et al., 2022), benchmarked summarization and\ndata-to-text tasks in multiple languages.\nThe recent Holistic Evaluation of Language Mod-\nels project (HELM, Liang et al., 2022) aims to\nstandardize evaluation of language models. With\nthe explicit goal to broaden the task and metric\ncoverage, HELM has established an impressive\nfew-shot benchmark for many natural language\ntasks. Corroborating the prior ﬁndings, they also\nconclude that human evaluation is necessary for\nNLG. This distinction means that the reference-\nbased approach for generated text that the ﬁeld\nhas used since the advent of deep learning may no\nlonger sufﬁcient and that we need clear evaluation\nprotocols that continue to allow us to answer broad\nquestions about “generation quality” of a model.\nComplementing this work, we take a deeper look\nat a wider set of NLG tasks and explore LLMs in\nﬁnetuning and few-shot setups to identify whether\nreference-based automatic evaluation can still be\nused to produce system rankings.\nResearch Questions We aim to deﬁne a method-\nology that allows us to answer the question “How\ngood are learned representations of a model for\ngenerating natural language?” via few-shot and\nﬁnetuning approaches. To develop and apply this\nmethodology we seek to answer the following\nthree research questions:\nR1 How do different model architectures compare\nin terms of automatic metrics?\nWe aim to identify patterns that emerge in eval-\nuations and to uncover aspects inherent to the\ntasks, e.g. have metrics on speciﬁc tasks satu-\nrated?, and to the models’ architectural choices,\ne.g., are encoder-decoders better suited for par-\nticular task formulations? (Section 4)\nR2 What set of tasks, methods, and metrics is best\nsuited for the monitoring of improvements in\nlanguage generation capabilities?\nUsing the results of R1, we aim to select a sub-\nset of tasks, methods, and metrics that robustly\nproduce reliable model rankings. (Section 5)\n9195\nLength Size\nDataset Languages Input Output Training Test\nE2E en 146 135 35k 4.7kWebNLG en,ru 169.5 157 14k–35k 1.1k-1.8kToTTo en 357 120k 7.7kCzech Rest. cs 70 80 3.5k 842XSum en 1845 153 23k 1.2kWikiLingua en,es,ru,tr,vi1k–5k 159–4895k–3.8M 900-29kMLSum es,de 4152 147 220k–250k 10k-13kXL-Sum ar,bn,ja,id,sw,1k–10k 137–6141.3k–300k 500-9kko,ru,te,th,tr,es,vi,hi\nTable 1: Details of the datasets evaluated in this paper:\nlanguages, lengths in number of tokens according to\nthe mT5 tokenizer (Xue et al., 2021), and size of the\ntraining and test splits.\nR3 What are the broader implications for how the\nquality of newly developed models should be\nmonitored?\nRobustly ranking systems is particularly impor-\ntant when monitoring a system during training\nand when comparing across many tasks. In\nline with the “reality check” theme track at\nACL 2023, we discuss the implications of our\nﬁndings on how evaluation results should be\nproduced and interpreted. (Section 6)\n3 Method\n3.1 Data\nWe select a combination of data-to-text and text-\nto-text datasets as different input modalities. The\nselected datasets capture different input and output\nlengths, domains, languages, and communicative\ngoals. The text-to-text task with most available\nmultilingual datasets is summarization which we\npick for this paper.3 We pick the following tasks:4\n• MLSum (Scialom et al., 2020) – Summarize a\nnews article in multiple sentences.\n• WikiLingua (Ladhak et al., 2020) – Gener-\nate section headers for step-by-step instructions\nfrom WikiHow.\n• XSum (Narayan et al., 2018) – Generate the ﬁrst\nsentence of a news article.\n• Clean E2E NLG (Novikova et al., 2017; Dušek\net al., 2019) – Given a set of key-value attribute\n3Since benchmarks for machine translation are well-\nestablished (e.g., Akhbardeh et al., 2021) we exclude it from\nour scope. However, any methodological outcomes of our\nwork can be applied to translation or similar tasks.\n4All datasets were retrieved via the Generation Evaluation\nand Metrics benchmark (Gehrmann et al., 2021, 2022a). We\nuse these datasets for research purposes only in line with their\nintended use.\npairs, describe a restaurant in one or two sen-\ntences.\n• Czech Restaurant response genera-\ntion (Dusek and Jurvc’ivcek, 2019) – Given a\ndialog context and a dialog act representation,\ngenerate a one sentence long response.\n• WebNLG 2020 (Gardent et al., 2017; Ferreira\net al., 2020) – Verbalize subject-predicate-object\ntriples in one or more sentences.\n• ToTTo (Parikh et al., 2020) – Describe high-\nlighted cells in a table in a single sentence.\n• XL-Sum (Hasan et al., 2021) – Summarize a\nnews article, in the same language, in a single\nsentence.\nTable 1 provides an overview of these datasets in\nterms of languages, the lengths of input and output\nand split sizes. For highly multilingual datasets,\nwe evaluate on a subset of typologically diverse\nlanguages following the selection by Clark et al.\n(2020). To this selection, we add languages that\nappear bothin WikiLingua and XL-Sum.\n3.2 Models\nPrior results for the benchmarked tasks primarily\ncome from ﬁnetuning T5 (Raffel et al., 2020b),\nmT5 (Xue et al., 2021), or BART (Lewis et al.,\n2020b), which are encoder-decoder models pre-\ntrained with an inﬁlling objectives. These models\nare signiﬁcantly smaller than newer models like\nGPT-3, with sizes ranging from 130M to 13B pa-\nrameters. Encoder-decoder models trained for in-\nﬁlling often outperform larger decoder-only LMs\nin the ﬁnetuning setting (Tay et al., 2022), while\nthe latter work better for few-shot setting. There\nhas also been recent work on reducing the com-\nputational cost of large models by ∼10x by using\na mixture of experts (Zoph et al., 2022). It is im-\nportant to compare these diverse set of models to\nunderstand how scale plays a role with the model’s\narchitecture and its pretraining. We benchmark the\nfollowing models:5\n• PaLM PaLM is a pretrained decoder-only\ntransformer-based model trained with standard\nleft-to-right language modeling objective. It is\npretrained on a range of multilingual corpora in-\ncluding Wikipedia, news, and code. In this work,\nwe use two models scales: 8B parameters and\n540B parameters.\n5Model names omitted for anonymity.\n9196\n• GPT-3.5 (Ouyang et al., 2022b) GPT-3.5 is\na 175B parameter decoder-only transformer-\nmodel of the GPT-3 family (Brown et al., 2020)\nbut trained on a blend of text and code from\nbefore Q4 2021. This model, named code-\ndavinci-002, was introduced as the base model\nfor InstructGPT-3 (Ouyang et al., 2022b) with-\nout the supervision on human-written demon-\nstrations and human-vetted model samples.6\n• ST-MoE (Zoph et al., 2022) ST-MoE is a 269B\nsparse pretrained variant of a dense encoder-\ndecoder transformer-based model.\n• LaMDA (Thoppilan et al., 2022) LaMDA (137B\nparameters) is a decoder-only transformer-based\nlanguage model specialized for dialog applica-\ntions. It is pretrained on dialog data as well as\nweb text data followed by rank-based tuning.\n• T5 (Raffel et al., 2020a) T5-XXL (11B parame-\nters) is a pretrained encoder-decoder transformer-\nbased model trained on a span corruption objec-\ntive with a novel uniﬁed text-to-text format. It\nis pretrained on Common Crawl data, mostly\ncontaining English-only documents.\n• mT5 (Xue et al., 2021) mT5-XXL (11B param-\neters) is a multilingual variant of T5 that was\npretrained on a multilingual corpus, mC4, cover-\ning 101 languages.\n• LongT5 (Guo et al., 2021) LongT5 (3B param-\neters) a similar architecture as T5, where the\nencoder is extended to have global-local atten-\ntion sparsity patterns to handle long inputs.\n3.3 Few-shot evaluation methodology\nTo evaluate the models for few-shot inference, we\nconcatenate a task-speciﬁc prompt 7 to the input\nand prepend an output prompt to the output. To\nhandle the oftentimes very long inputs or outputs\nfor tasks such as summarization, inputs were trun-\ncated to 2048 tokens and inference was done pro-\nviding only one exemplar at a time, referred to as 1-\nshot. These simple prompts are analogous to those\nused in related work (Chowdhery et al., 2022; Scao\net al., 2022). We do not tune the prompts or use\nmore complex strategies to keep fair comparisons\nbetween multiple systems, as prompt selection can\nlead to overﬁtting. The exemplars are separated\nthrough double linebreaks, which are also used\n6More details can be found at\nhttps://beta.openai.com/docs/model-index-for-researchers\n7For Summarization, this prompt was “Summarize the\nfollowing article:”, and for Data-to-Text it was “Verbalize:”.\nThis was translated to the appropriate language.\nto truncate output predictions for evaluation. All\nfew-shot exemplars are randomly sampled from\nthe training corpus. From early experimentation,\nwe found this particularly important since it avoids\noverﬁtting to exemplars that work well for one\nmodel but not another.\n3.4 Finetuning methodology\nTo use the decoder-only architectures during ﬁne-\ntuning, inputs and targets are concatenated. The\nconcatenated sequences are truncated to 2048 to-\nkens, the training context used during pretraining,\nwith 512 tokens reserved for the target. Only sum-\nmarization tasks required input truncation. We\nﬁnetuned models with standard hyperparameters;\nrefer to Appendix-B for thorough details. The best\nmodel checkpoint for each dataset was selected by\nthe best performing geometric mean of ROUGE-1,\nROUGE-2 and ROUGE-L scores on the valida-\ntion set. Decoding was done with beam-search\nwith a beam size of 4 for encoder-decoder models,\nwhile inference in decoder-only PLMs (LaMDA,\nPaLM, ST-MoE) was performed using top-k sam-\npling with k=10, due to issues with scaling beam\nsearch at the time of publication.\n3.5 Metrics\nFollowing the suggestions by Gehrmann et al.\n(2022b), we report a combination of lexical and\nlearned metrics, starting with ROUGE-2 and\nROUGE-L (Lin, 2004). Since the default ROUGE\nimplementation uses English-speciﬁc tokenization,\nstemming and punctuation normalization, it is in-\ncompatible with other languages. Hasan et al.\n(2021) extended ROUGE by integrating additional\nstemmers and tokenizers to cover up to the 45 lan-\nguages. To support more languages, and avoid\ndependency on varying implementations, we use\na SentencePiece tokenizer (Kudo and Richardson,\n2018) which, provided a vocabulary distribution\nﬁle, is self-contained and has sensible fall-backs\nto unexpected words. Speciﬁcally, we used mT5’s\nSentencePiece vocabulary.\nFor the same reason, we also evaluate with\nChrF (Popovi´c, 2015), which is a character-level\nn-gram overlap metrics and thus independent from\ntokenizers. BLEURT (Sellam et al., 2020; Pu et al.,\n2021) is a multilingual model-based evaluation\nmetric for generation designed to compute the sim-\nilarity between a pair of sentences i.e. a reference\nand a candidate. It ﬁnetunes RemBERT (Chung\n9197\net al., 2021) on synthetic sentence pairs and gold\nratings. In contrast to the lexical metrics, BLEURT\nis meant to capture the non-trivial semantic simi-\nlarities between two texts.\nFor brevity, the main text of this section focuses\non the F-measure of ROUGE-L for English and\nSentencePiece-ROUGE-L for all other languages\nwhile the remaining results are in Appendix A.\nWe additionally investigate the agreement between\nmetrics in Section 5.8\n4 Empirical Observations\nFew-shot learning falls behind ﬁnetuning For\nmany generation tasks, including multilingual sum-\nmarization tasks, we observe a large gap between\nﬁnetuning and few-shot results, indicating that ﬁne-\ntuning will play an important role when it comes to\nmaximizing automatic scores. On data-to-text, the\nfew-shot results follow a similar trend as in sum-\nmarization, but the gap to the best ﬁnetuned results\nshrinks drastically. Moreover, the ﬁnetuning result\ndo not always follow a trend according to scale or\narchitecture. We hypothesize that multiple tasks\nhave saturated to the metrics. If this is the case, ap-\nproaching them as few-shot generation tasks may\nstill yield insights but it is no longer productive to\nuse them to benchmark ﬁnetuned models.\nFinetuned decoder-only PLMs can match\nencoder-decoder performance with scale In\nsummarization, ﬁnetuned decoder-only PLMs,\nsuch as PaLM-540B, closely match or exceeds\nthe best reported prior results on all English gen-\neration tasks. This demonstrates that PLMs can\nmake up their architectural disadvantage through\nits vastly increased scale. While ﬁnetuning PLMs\nis computationally expensive, it serves as an im-\nportant upper bound for few-shot predictions.\nMultilingual generation capabilities are highly\ndependent on pretraining data The PLMs\nevaluated are mostly pretrained on English cor-\npora: 99+% for T5, LongT5, ST-MoE; 90% for\nPaLM, LaMDA; contrarily mT5 is explicitly pre-\n8For ROUGE, we used the python implementa-\ntion at https://github.com/google-research/\ngoogle-research/tree/master/rouge at commit\nf935042 and whitespace-tokenized references and predic-\ntions before calling the library. For BLEURT, we used\nBLEURT-20 checkpoint from the library at https://\ngithub.com/google-research/bleurt and com-\nmit c6f2375.\ntrained in a multilingual corpus.9 PaLM achieves\nbest results in 3 out of 4 English generation tasks\nwhich generate English text, even when the input\nis non-English. However, the much smaller mT5\nbests the other models in 10 out of 14 non-English\nsummarization tasks, and the relative difference\nbetween few-shot and ﬁnetuning is larger for non-\nEnglish generation. This suggests that English-\ncentric PLMs are better at processing non-English\ninput than generating non-English output.\nAnalyzing the effects of input context length\nTasks with long inputs suffer from models’ limita-\ntion to process said inputs. Inputs are thus usually\ntransformed (e.g. cropped, re-ranked, etc) to ﬁt\ninto the model. We found that a several of the\nevaluated tasks, such as WikiLingua and MLSum\nbeneﬁt from a longer input context in models even\nif the long-context model is smaller (i.e., LongT5\nvs T5). In contrast, the performance is comparable\nfor the rest of short-context tasks.\nFigure 1: General recommendations when monitoring\nor benchmarking PLMs.\n5 Deriving Evaluation Practices\nFigure 1 summarizes the recommendations we\ndeveloped from challenges we faced and our ob-\nserved empirical results. These recommendations\nare best understood in the context of monitoring\n9The language breakdown for GPT-3.5 is unknown.\n9198\nOne-shot Finetuning\nTask PaLM\n8B\nPaLM\n540B\nLaMDA\n137B\nGPT-3.5\n175B\nPaLM\n8B\nPaLM\n540B\nST-MoE\n32B\nT5\n11B\nmT5\n11B\nLongT5\n3B\nData-To-Text\nE2E (en) 37.7 46.6 7.1 46.6 52.9 52.3 51.5 52.9 52.2 53.1\nWebNLG (en) 45.3 54.7 8.4 54.6 56.8 58.0 56.4 50.8 47.7 58.0\nToTTo (en) 40.2 50.7 5.6 51.9 65.8 67.5 67.0 66.1 65.5 66.3\nCzech Restaurant (cs) 16.9 34.1 3.3 38.5 45.5 45.5 40.7 45.4 39.4 44.8\nWebNLG (ru) 16.8 33.7 4.5 33.3 40.9 40.5 28.2 41.2 41.1 41.6\nEnglish Generation\nXSum (en) 19.9 28.6 10.0 34.0 31.4 36.5 38.3 36.5 33.2 36.0\nXLSum (en) 16.8 22.7 8.4 27.9 34.6 44.3 45.4 43.1 41.8 42.6\nWikiLingua (en) 6.5 6.4 5.9 7.7 8.0 7.5 7.8 7.9 7.9 7.8\nCrosslingual Generation\nWikiLingua (es →en) 6.5 6.1 5.0 7.7 7.7 7.6 7.3 7.8 7.6 7.9\nWikiLingua (ru →en) 10.2 17.5 0.7 18.9 29.9 35.7 25.1 27.9 31.7 30.8\nWikiLingua (tr →en) 10.1 20.0 7.7 21.2 31.1 38.8 31.5 26.8 36.7 28.2\nWikiLingua (vi →en) 7.7 14.5 2.2 16.2 28.9 32.9 22.9 22.7 31.0 28.5\nMultilingual Generation [SentencePiece-ROUGE-2]\nMLSum (es) 12.8 14.3 5.2 13.0 23.0 24.5 25.0 24.3 25.7 25.6\nMLSum (de) 13.6 21.3 3.9 22.6 35.2 41.4 44.1 43.5 43.3 43.7\nXLSum (ar) 12.2 19.0 10.8 18.0 36.2 39.9 15.7 15.2 42.3 6.2\nXLSum (bn) 5.8 6.9 6.1 11.7 26.4 31.1 11.1 10.2 36.5 11.0\nXLSum (ja) 11.3 15.1 5.4 18.3 38.7 42.5 4.5 4.5 43.7 4.6\nXLSum (id) 16.8 20.4 9.0 20.1 35.5 43.5 41.1 41.6 43.5 40.8\nXLSum (sw) 16.7 24.5 11.5 15.4 32.7 36.4 37.0 37.4 40.7 36.3\nXLSum (ko) 16.1 18.2 7.9 17.6 33.8 37.3 20.3 19.5 45.0 19.9\nXLSum (ru) 12.6 16.1 10.8 19.1 30.3 38.3 18.1 17.8 38.6 17.7\nXLSum (te) 6.5 7.7 6.2 13.1 20.5 30.0 15.1 15.1 33.5 14.8\nXLSum (th) 6.7 8.6 5.2 13.3 23.4 29.5 13.5 13.7 34.3 13.1\nXLSum (tr) 15.2 17.7 8.0 16.8 33.3 42.4 30.3 30.4 42.3 29.7\nXLSum (es) 15.7 17.4 8.3 16.9 25.2 34.3 31.9 32.5 33.9 32.3\nXLSum (vi) 13.2 14.9 6.9 15.4 25.9 41.5 27.7 27.3 41.0 26.7\nXLSum (hi) 10.0 12.1 9.3 15.2 37.7 43.6 13.7 2.3 43.5 2.3\nTable 2: ROUGE-L and SentencePiece-ROUGE-L results on data-to-text and compression datasets. Best results in bold.\nFew-shot results lag behind ﬁnetuned results and the gap increases as tasks become more complex. The non-English performance\nmostly follows the trend that higher percentages of non-English pretraining data leads to better performance. Despite their much\nsmaller size, encoder-decoder model frequently much larger decoder-only models after ﬁnetuning.\nand benchmarking PLMs during training or infer-\nence.\nComparable few-shot learning evaluation As\nmentioned in Section 3, our design choices were\nmade to ensure that results are comparable across\nPLMs. Primarily, prompts were deliberately kept\nextremely simple and all few-shot exemplars were\nrandomly sampled. While highly curated prompts\nor methods like chain-of-thought prompting can\nincrease the performance considerably (Wei et al.,\n2022b), it can also lead to overﬁtting to the partic-\nular model the prompt was developed on, in turn\nmaking a comparison to other models unfair and\nproducing unrealistic expectations when people\nhave single interactions with it.\nOverlap-based metrics are not calibrated to\nevaluate few-shot learning Few-shot genera-\ntion suffers from not being able to predict out-\nput length properly given the few exemplars pro-\nvided. While encoder-decoder models utilize end-\nof-string tokens, these are not always learned dur-\ning decoder-only pretraining. To circumvent this\nissue, researchers rely on PLMs match to the few-\nshot format provided e.g. line-breaks that separate\nexemplars. We observed PLMs fail to follow the\nformat a signiﬁcant number of times, producing\nthe largest allowed length on occasion. In our\nexperiments, we tried to avoid very long outputs\nby trimming outputs to the 95-percentile length\nseen in the targets.10 Still, few-shot output lengths\n10This simple method avoids discrepancies across PLMs\nwhich might have different maximum decoding lengths.\n9199\nare on average 2-3 times the average target length\nwhile ﬁnetuned model’s output average 80% the\naverage target length, across all tasks. Overlap\nmetrics used in generation are sensitive to length\n(Sun et al., 2019) making a natural disadvantage\nfor few-shot learners. We do not recommend using\noverlap-based metrics to compare few-shot results\nwithout length normalization.\nComputational costs can be decreased without\nsacriﬁcing relative model performance The\ncomputational cost of evaluating large datasets,\nsome with more than 10K examples, are pro-\nhibitive and perhaps unnecessary. To that end,\nwe investigate if a model ranking can be produced,\nwith a high degree of certainty, while only con-\nsidering a random subset of the test set, saving\ncompute cost to possibly evaluate on more tasks\ninstead. To investigate this effect, we ran the\nfollowing experiment: (1) Sample n datapoints\nfrom a dataset and all corresponding model scores.\n(2) Following Kocmi et al. (2021) and Graham\net al. (2014), we perform Wilcoxon Rank Sum\ntest (Wilcoxon, 1946) to assess the stability of the\nranking. (3) Repeat steps 1&2 k times and record\nthe fraction of runs in which models scores from\nany two models were not distinguishable from\neach other (those with a p-value of > 0.05). Since\nwe are considering 10 model settings in this work,\nthis experiment considers all 45 possible pairs.\nThe result shown in Figure 2 provides insight into\nthe required number of data points to produce rank-\nings. For most datasets, we can produce stable\nmodel rankings with only 500 examples, some\nwith as little as 100. Tasks where models achieve\nvery similar scores tend to require more test ex-\namples, since smaller score differences require\nmore examples to be distinguishable from each\nother (Wei and Jia, 2021).11\nAnalyzing metrics utility We use different au-\ntomated metrics to evaluate the generation quality\nof the models. These metrics attempt to capture\nthe similarity between system generated output\nand the reference text. While ROUGE and chrF\naccount for the lexical overlap, BLEURT is meant\nto compute the semantic similarity. It is important\nto understand the agreement between these met-\nrics. We compute the the system-level agreement\nvia Spearman correlation coefﬁcient (Spearman,\n1987) between the scores given by the metrics to\n11Full results available in Appendix A.\nFigure 2: Empirical probability of p-value of\nWilcoxon Rank Sum test < 0.05 for any combination\nbetween 1-shot and ﬁnetuned models.\nFigure 3: Spearman correlation coefﬁcients between\nmetrics: (SP)ROUGE-L, BLEURT and ChrF.\nthe ﬁne-tuned set of models. Figure 3 shows the\ncorrelation between ROUGE-L (RL), BLEURT\nand ChrF. We observe that the metrics are highly\ncorrelated for most datasets. Similar to Figure 2,\non the tasks where the models have similar per-\nformance, we notice less correlation among the\nmetrics. Such tasks are may have either saturated\nperformance, e.g., ToTTo (en) or all models per-\nform poorly, e.g., Wikilingua (es-> en). Due to the\nsmall differences between models, metrics fail to\nproduce the same rankings.\n6 Discussion and Reality Check\nIn line with our goal to provide a “reality check”\nvia empirical and theoretical research, and to re-\nﬂect on the ways in which reported performance\nimprovements are meaningful, we want to situate\nour ﬁndings in the context of the broader NLP\ncommunity. Openly accessible APIs and publicly\navailable large models have led to increased at-\ntention on large pretrained models, but they have\nalso led to a “release-then-test” philosophy where\nmodels are released without extensive evaluations.\nWhile the ﬁndings we present in this paper do not\n9200\nsolve this issue, agreeing on a shared evaluation\nprocess could lead to more realistic claims about\nmodel performance (and shortcomings), and allow\nfor a more accurate monitoring of models during\ntraining.\nWhat claims can we not make? Empirical ﬁnd-\nings demonstrate that incorporating generation into\nNLU tasks via Chain-of-Thought leads to better\nmodel performance (Wei et al., 2022b; Suzgun\net al., 2022). Providing additional grounding via\nﬁnetuning on instructions and aligning a model to\nhuman feedback leads to better task-speciﬁc per-\nformance without supervision (Wei et al., 2022a;\nOuyang et al., 2022a). However, we lack the sci-\nentiﬁc methods to quantify these advances. While\nbenchmarks provide an indication whether a model\nis performing better than a previous iteration, and\nprojects like BIG-bench (Srivastava et al., 2022)\nand HELM (Liang et al., 2022) enable evaluation\non a very wide range of possible tasks, they are\nalso inherently limited.\nWhen benchmarking models in few-shot settings,\nespecially models for which little information\nabout their training data is available, it is hard\nto disambiguate model performance from mem-\norization, i.e. if the examples were seen during\npretraining. Instruction tuning further blur the line\nbetween ﬁnetuning and few-shot, which can lead\nto very different outputs and are not fully compa-\nrable. It is thus near impossible to make claims\nabout why a model is succeeding at one particular\ntask without having access to its training data.\nAs mentioned earlier, the target of this work is to\nderive best practices for comparing models in gen-\neration settings with constrained computational\nbudgets, for example when monitoring a training\nmodel or when trying to compare on many dif-\nferent tasks. Our ﬁndings are grounded in much\nprior work that ﬁnds that metrics have a very high\nagreement with human judgments on the system-\nlevel (e.g., Kocmi et al., 2021), but are essentially\nmeaningless on the segment-level. For that reason,\nwe cannot derive claims beyond these rankings\nabout utility of a model or whether a particular\nmodel would actually produce useful outputs for a\ntask. To derive such insights, we point to work on\nextrinsic evaluation which requires comprehensive\nhuman evaluations (e.g., Lee et al., 2022).\nHow can our ﬁndings be applied to improve the\nstatus quo? Since the generation capabilities of\nPLMs are currently not extensively monitored or\nevaluated, we set out to derive best practices for\nhow these evaluations can look. We found that\nmany of the “easy” tasks, on which ﬁnetuned mod-\nels saturate the metrics, still yield insights for few-\nshot approaches. We further identiﬁed the tension\nbetween doing a computationally expensive full\nevaluation on a dataset and adding more evaluation\nsets for different tasks or languages. Our ﬁndings\nsuggest that evaluation on small subsets of more\ntasks can be beneﬁcial to the overall results.\nTo further motivate this suggestion, consider the\nfollowing thought experiment: We have two tasks,\nA and B. At 500 examples, they have a risk of\nproducing a “wrong” ranking of 10%. At 1,000\nexamples, they have a risk of producing a wrong\nranking of 5%. These risks are not correlated, i.e.,\ntheir covariance is 0. Given a computational bud-\nget of evaluating on 1,000 examples, the risk of\nonly evaluating on one dataset is 5%, and the risk\nof producing two wrong ratings after evaluating\non A and B is only 1%. While additional datasets\nintroduce a larger risk of one individual dataset pro-\nducing misleading results (18% in this case), one\ncan easily expand this argument to a whole port-\nfolio of tasks to hedge against individual dataset\nrisk (Stuart and Markowitz, 1959). Many exist-\ning NLU benchmarks like BIG bench (Srivastava\net al., 2022) already follow such a strategy and we\nbelieve that generation evaluation, especially con-\nsidering the additional risk due to metrics, should\nfollow this approach for the use cases discussed\nin this work. To further minimize the individual\ndataset risk, they can be switched out once they\nsaturate or their sample sizes increased.\n7 Conclusion\nIn this work, we produced an extensive evaluation\nof a diverse set of state-of-the-art pre-trained lan-\nguage models (PLMs) for 27 different multilingual\ngeneration tasks under few-shot learning and ﬁne-\ntuning settings. We discuss empirical results that\nhelp inform practitioners which tasks, methods\nand metrics are suitable. We provide recommenda-\ntions on how best to monitor conditional genera-\ntion capabilities of PLMs, including how to fairly\nbenchmark few-shot learning, automated metrics\nand their utility, and how to efﬁciently utilize com-\nputational resources. We hope that such ﬁndings\nand recommendations could positively inﬂuence\nnatural language evaluation in future work.\n9201\n8 Limitations\nIn this work, we have presented results that help in-\nform us what tasks, methods and metrics are best\nsuited for monitoring as well as methodologies\nand empirical information about the current set of\nmodels. We provide detailed information of how\nthese results can be reproduced, to the extend that\nresearch have access to the PLMs in question, but\nthese results have limitations, in order to reduce\ncosts, many languages were not evaluated which\nmight have left unforeseen patterns not discussed\nin this work. Moreover, few-shot learning, in par-\nticular, could exhibit large variance if different\nprompts were chosen, or a different set of exem-\nplars chosen. Because of the high costs involved\nour work does not explore the performance differ-\nence when multiple sets of hyper-parameters were\nchosen.\nOn the conceptual level, we make the assump-\ntion that system-level improvements on our tasks\ntranslate to downstream usefulness. While prior\nwork suggests that this is the case, tools like chat-\nGPT have signiﬁcantly expanded the possible ap-\nplication space beyond the realm of “typical” NLP\ntasks, and we don’t know how well our ﬁndings\ngeneralize to this space of tasks.\n9 Ethics Statement\nThis paper focuses on conditional generation tasks\nwhere models are free to generate long text se-\nquences. Typical issues associated with text gen-\neration such as hallucinations, memorization of\nprivate information publicly available, toxic and\ndiscriminatory language, or sensitive generated\ncontent could and are likely to arise. measuring\nthe extent to which these issues occur is a neces-\nsary and crucial additional dimension of model\nevaluation which we do not include in this work,\nwhich should be seen as supplemental.\nReferences\nFarhad Akhbardeh, Arkady Arkhangorodsky, Mag-\ndalena Biesialska, Ond ˇrej Bojar, Rajen Chatter-\njee, Vishrav Chaudhary, Marta R. Costa-jussa,\nCristina España-Bonet, Angela Fan, Christian Fed-\nermann, Markus Freitag, Yvette Graham, Roman\nGrundkiewicz, Barry Haddow, Leonie Harter, Ken-\nneth Heaﬁeld, Christopher Homan, Matthias Huck,\nKwabena Amponsah-Kaakyire, Jungo Kasai, Daniel\nKhashabi, Kevin Knight, Tom Kocmi, Philipp Koehn,\nNicholas Lourie, Christof Monz, Makoto Morishita,\nMasaaki Nagata, Ajay Nagesh, Toshiaki Nakazawa,\nMatteo Negri, Santanu Pal, Allahsera Auguste Tapo,\nMarco Turchi, Valentin Vydrin, and Marcos Zampieri.\n2021. Findings of the 2021 conference on machine\ntranslation (WMT21). In Proceedings of the Sixth\nConference on Machine Translation, pages 1–88, On-\nline. Association for Computational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder,\nMelanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, T. J. Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. ArXiv,\nabs/2005.14165.\nAakanksha Chowdhery, Sharan Narang, Jacob De-\nvlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Se-\nbastian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek B Rao,\nParker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar\nPrabhakaran, Emily Reif, Nan Du, Benton C. Hutchin-\nson, Reiner Pope, James Bradbury, Jacob Austin,\nMichael Isard, Guy Gur-Ari, Pengcheng Yin, Toju\nDuke, Anselm Levskaya, Sanjay Ghemawat, Sunipa\nDev, Henryk Michalewski, Xavier García, Vedant\nMisra, Kevin Robinson, Liam Fedus, Denny Zhou,\nDaphne Ippolito, David Luan, Hyeontaek Lim, Bar-\nret Zoph, Alexander Spiridonov, Ryan Sepassi, David\nDohan, Shivani Agrawal, Mark Omernick, Andrew M.\nDai, Thanumalayan Sankaranarayana Pillai, Marie Pel-\nlat, Aitor Lewkowycz, Erica Moreira, Rewon Child,\nOleksandr Polozov, Katherine Lee, Zongwei Zhou,\nXuezhi Wang, Brennan Saeta, Mark Díaz, Orhan Fi-\nrat, Michele Catasta, Jason Wei, Kathleen S. Meier-\nHellstern, Douglas Eck, Jeff Dean, Slav Petrov, and\nNoah Fiedel. 2022. Palm: Scaling language modeling\nwith pathways. ArXiv, abs/2204.02311.\nHyung Won Chung, Thibault Fevry, Henry Tsai,\nMelvin Johnson, and Sebastian Ruder. 2021. Rethink-\ning embedding coupling in pre-trained language mod-\nels. In International Conference on Learning Repre-\nsentations.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jen-\nnimaria Palomaki. 2020. TyDi QA: A benchmark for\ninformation-seeking question answering in typologi-\ncally diverse languages. Transactions of the Associ-\nation for Computational Linguistics, 8:454–470.\nNan Du, Yanping Huang, Andrew M. Dai, Simon\nTong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat,\nBarret Zoph, Liam Fedus, Maarten Bosma, Zongwei\nZhou, Tao Wang, Yu Emma Wang, Kellie Webster,\n9202\nMarie Pellat, Kevin Robinson, Kathleen S. Meier-\nHellstern, Toju Duke, Lucas Dixon, Kun Zhang,\nQuoc V . Le, Yonghui Wu, Z. Chen, and Claire Cui.\n2022. Glam: Efﬁcient scaling of language models\nwith mixture-of-experts. In International Conference\non Machine Learning.\nOndrej Dusek and Filip Jurvc’ivcek. 2019. Neural gen-\neration for czech: Data and baselines.\nOndˇrej Dušek, David M Howcroft, and Verena Rieser.\n2019. Semantic Noise Matters for Neural Natural Lan-\nguage Generation. In Proceedings of the 12th Inter-\nnational Conference on Natural Language Generation\n(INLG 2019), pages 421–426, Tokyo, Japan.\nKawin Ethayarajh and Dan Jurafsky. 2020. Utility is\nin the eye of the user: A critique of NLP leaderboards.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) ,\npages 4846–4853, Online. Association for Computa-\ntional Linguistics.\nThiago Castro Ferreira, Claire Gardent, Nikolai\nIlinykh, Chris van Der Lee, Simon Mille, Diego\nMoussallem, and Anastasia Shimorina. 2020. The\n2020 Bilingual, Bi-Directional WebNLG+ Shared\nTask Overview and Evaluation Results (WebNLG+\n2020). In Proceedings of the 3rd International Work-\nshop on Natural Language Generation from the Se-\nmantic Web (WebNLG+), Dublin/Virtual, Ireland.\nMarkus Freitag, George Foster, David Grangier, and\nColin Cherry. 2020. Human-paraphrased references\nimprove neural machine translation. In Proceedings\nof the Fifth Conference on Machine Translation, pages\n1183–1192, Online. Association for Computational\nLinguistics.\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\nand Laura Perez-Beltrachini. 2017. Creating training\ncorpora for nlg micro-planners. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers) , pages\n179–188. Association for Computational Linguistics.\nSebastian Gehrmann, Tosin Adewumi, Karmanya\nAggarwal, Pawan Sasanka Ammanamanchi, Aremu\nAnuoluwapo, Antoine Bosselut, Khyathi Raghavi\nChandu, Miruna Clinciu, Dipanjan Das, Kaustubh D.\nDhole, Wanyu Du, Esin Durmus, Ond ˇrej Dušek,\nChris Emezue, Varun Gangal, Cristina Garbacea,\nTatsunori Hashimoto, Yufang Hou, Yacine Jernite,\nHarsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir\nKale, Dhruv Kumar, Faisal Ladhak, Aman Madaan,\nMounica Maddela, Khyati Mahajan, Saad Mahamood,\nBodhisattwa Prasad Majumder, Pedro Henrique Mar-\ntins, Angelina McMillan-Major, Simon Mille, Emiel\nvan Miltenburg, Moin Nadeem, Shashi Narayan, Vi-\ntaly Nikolaev, Rubungo Andre Niyongabo, Salomey\nOsei, Ankur Parikh, Laura Perez-Beltrachini, Niran-\njan Ramesh Rao, Vikas Raunak, Juan Diego Ro-\ndriguez, Sashank Santhanam, João Sedoc, Thibault\nSellam, Samira Shaikh, Anastasia Shimorina, Marco\nAntonio Sobrevilla Cabezudo, Hendrik Strobelt, Nis-\nhant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola,\nand Jiawei Zhou. 2021. The gem benchmark: Natural\nlanguage generation, its evaluation and metrics.\nSebastian Gehrmann, Abhik Bhattacharjee, Abinaya\nMahendiran, Alex Wang, Alexandros Papangelis,\nAman Madaan, Angelina McMillan-Major, Anna\nShvets, Ashish Upadhyay, Bingsheng Yao, et al. 2022a.\nGemv2: Multilingual nlg benchmarking in a single\nline of code. arXiv preprint arXiv:2206.11249.\nSebastian Gehrmann, Elizabeth Clark, and Thibault\nSellam. 2022b. Repairing the cracked foundation: A\nsurvey of obstacles in evaluation practices for gener-\nated text. ArXiv, abs/2202.06935.\nTanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022.\nNews summarization and evaluation in the era of GPT-\n3. CoRR, abs/2209.12356.\nYvette Graham, Timothy Baldwin, Alistair Moffat,\nand Justin Zobel. 2014. Is machine translation getting\nbetter over time? In Proceedings of the 14th Confer-\nence of the European Chapter of the Association for\nComputational Linguistics , pages 443–451, Gothen-\nburg, Sweden. Association for Computational Linguis-\ntics.\nMandy Guo, Joshua Ainslie, David C. Uthus, Santi-\nago Ontañón, Jianmo Ni, Yun-Hsuan Sung, and Yin-\nfei Yang. 2021. LongT5: Efﬁcient text-to-text trans-\nformer for long sequences. CoRR, abs/2112.07916.\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is-\nlam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang,\nM. Sohel Rahman, and Rifat Shahriyar. 2021. XL-\nsum: Large-scale multilingual abstractive summariza-\ntion for 44 languages. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021 ,\npages 4693–4703, Online. Association for Computa-\ntional Linguistics.\nTom Kocmi, Christian Federmann, Roman Grund-\nkiewicz, Marcin Junczys-Dowmunt, Hitokazu Mat-\nsushita, and Arul Menezes. 2021. To ship or not to\nship: An extensive evaluation of automatic metrics for\nmachine translation. In Proceedings of the Sixth Con-\nference on Machine Translation, pages 478–494, On-\nline. Association for Computational Linguistics.\nTaku Kudo and John Richardson. 2018. Sentence-\nPiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium. As-\nsociation for Computational Linguistics.\nFaisal Ladhak, Esin Durmus, Claire Cardie, and Kath-\nleen McKeown. 2020. WikiLingua: A new benchmark\ndataset for cross-lingual abstractive summarization. In\nFindings of the Association for Computational Linguis-\ntics: EMNLP 2020, pages 4034–4048, Online. Associ-\nation for Computational Linguistics.\n9203\nMina Lee, Megha Srivastava, Amelia Hardy, John\nThickstun, Esin Durmus, Ashwin Paranjape, Ines\nGerard-Ursin, Xiang Lisa Li, Faisal Ladhak, Frieda\nRong, Rose E. Wang, Minae Kwon, Joon Sung\nPark, Hancheng Cao, Tony Lee, Rishi Bommasani,\nMichael S. Bernstein, and Percy Liang. 2022. Eval-\nuating human-language model interaction. CoRR,\nabs/2212.09746.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020a. Bart:\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and comprehen-\nsion. In Annual Meeting of the Association for Com-\nputational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020b.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics ,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nPercy Liang, Rishi Bommasani, Tony Lee, Dim-\nitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, Benjamin Newman, Binhang Yuan, Bobby Yan,\nCe Zhang, Christian Cosgrove, Christopher D. Man-\nning, Christopher R’e, Diana Acosta-Navas, Drew A.\nHudson, E. Zelikman, Esin Durmus, Faisal Lad-\nhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue\nWang, Keshav Santhanam, Laurel J. Orr, Lucia Zheng,\nMert Yuksekgonul, Mirac Suzgun, Nathan S. Kim,\nNeel Guha, Niladri S. Chatterji, O. Khattab, Peter\nHenderson, Qian Huang, Ryan Chi, Sang Michael\nXie, Shibani Santurkar, Surya Ganguli, Tatsunori\nHashimoto, Thomas F. Icard, Tianyi Zhang, Vishrav\nChaudhary, William Wang, Xuechen Li, Yifan Mai,\nYuhui Zhang, and Yuta Koreeda. 2022. Holistic evalu-\nation of language models. ArXiv, abs/2211.09110.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summarization\nBranches Out, pages 74–81, Barcelona, Spain. Associ-\nation for Computational Linguistics.\nNitika Mathur, Johnny Wei, Markus Freitag, Qingsong\nMa, and Ond ˇrej Bojar. 2020. Results of the WMT20\nmetrics shared task. In Proceedings of the Fifth Con-\nference on Machine Translation, pages 688–725, On-\nline. Association for Computational Linguistics.\nSimon Mille, Kaustubh D. Dhole, Saad Mahamood,\nLaura Perez-Beltrachini, Varun Gangal, Mihir Sanjay\nKale, Emiel van Miltenburg, and Sebastian Gehrmann.\n2021. Automatic construction of evaluation suites for\nnatural language generation datasets. In Proceedings\nof the Neural Information Processing Systems Track\non Datasets and Benchmarks 1, NeurIPS Datasets and\nBenchmarks 2021, December 2021, virtual.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for extreme\nsummarization. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 1797–1807, Brussels, Belgium. Associ-\nation for Computational Linguistics.\nJekaterina Novikova, Ond ˇrej Dušek, and Verena\nRieser. 2017. The E2E dataset: New challenges for\nend-to-end generation. In Proceedings of the 18th\nAnnual SIGdial Meeting on Discourse and Dialogue ,\npages 201–206, Saarbrücken, Germany. Association\nfor Computational Linguistics.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F. Christiano, Jan Leike, and Ryan Lowe. 2022a.\nTraining language models to follow instructions with\nhuman feedback. CoRR, abs/2203.02155.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke E. Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Francis Christiano, Jan Leike, and Ryan J. Lowe.\n2022b. Training language models to follow instruc-\ntions with human feedback. ArXiv, abs/2203.02155.\nAnkur P. Parikh, Xuezhi Wang, Sebastian Gehrmann,\nManaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Di-\npanjan Das. 2020. Totto: A controlled table-to-text\ngeneration dataset. ArXiv, abs/2004.14373.\nMaja Popovi´c. 2015. chrF: character n-gram F-score\nfor automatic MT evaluation. In Proceedings of\nthe Tenth Workshop on Statistical Machine Transla-\ntion, pages 392–395, Lisbon, Portugal. Association for\nComputational Linguistics.\nAmy Pu, Hyung Won Chung, Ankur Parikh, Sebastian\nGehrmann, and Thibault Sellam. 2021. Learning com-\npact metrics for MT. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 751–762, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes\nWelbl, Sumanth Dathathri, Saffron Huang, Jonathan\nUesato, John F. J. Mellor, Irina Higgins, Antonia\nCreswell, Nathan McAleese, Amy Wu, Erich Elsen,\nSiddhant M. Jayakumar, Elena Buchatskaya, David\nBudden, Esme Sutherland, Karen Simonyan, Michela\nPaganini, L. Sifre, Lena Martens, Xiang Lorraine\n9204\nLi, Adhiguna Kuncoro, Aida Nematzadeh, Elena\nGribovskaya, Domenic Donato, Angeliki Lazaridou,\nArthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-\npoukelli, N. K. Grigorev, Doug Fritz, Thibault Sotti-\naux, Mantas Pajarskas, Tobias Pohlen, Zhitao Gong,\nDaniel Toyama, Cyprien de Masson d’Autume, Yujia\nLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\nAidan Clark, Diego de Las Casas, Aurelia Guy,\nChris Jones, James Bradbury, Matthew G. Johnson,\nBlake A. Hechtman, Laura Weidinger, Iason Gabriel,\nWilliam S. Isaac, Edward Lockhart, Simon Osindero,\nLaura Rimell, Chris Dyer, Oriol Vinyals, Kareem W.\nAyoub, Jeff Stanway, L. L. Bennett, Demis Hassabis,\nKoray Kavukcuoglu, and Geoffrey Irving. 2021. Scal-\ning language models: Methods, analysis & insights\nfrom training gopher. ArXiv, abs/2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020a. Exploring the limits of\ntransfer learning with a uniﬁed text-to-text transformer.\nJournal of Machine Learning Research, 21(140):1–67.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020b. Exploring the\nlimits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research ,\n21(140):1–67.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-parameter\nopen-access multilingual language model. arXiv\npreprint arXiv:2211.05100.\nThomas Scialom, Paul-Alexis Dray, Sylvain Lamprier,\nBenjamin Piwowarski, and Jacopo Staiano. 2020. ML-\nSUM: the multilingual summarization corpus. CoRR,\nabs/2004.14900.\nThibault Sellam, Dipanjan Das, and Ankur P. Parikh.\n2020. Bleurt: Learning robust metrics for text genera-\ntion. In Annual Meeting of the Association for Compu-\ntational Linguistics.\nC. Spearman. 1987. The proof and measurement of\nassociation between two things. by c. spearman, 1904.\nThe American journal of psychology, 100 3-4:441–71.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R. Brown, Adam Santoro, Aditya Gupta,\nAdrià Garriga-Alonso, Agnieszka Kluska, Aitor\nLewkowycz, Akshat Agarwal, Alethea Power, Alex\nRay, Alex Warstadt, Alexander W. Kocurek, Ali\nSafaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen\nNie, Aman Hussain, Amanda Askell, Amanda Dsouza,\nAmeet Rahane, Anantharaman S. Iyer, Anders An-\ndreassen, Andrea Santilli, Andreas Stuhlmüller, An-\ndrew M. Dai, Andrew La, Andrew K. Lampinen, Andy\nZou, Angela Jiang, Angelica Chen, Anh Vuong, Ani-\nmesh Gupta, Anna Gottardi, Antonio Norelli, Anu\nVenkatesh, Arash Gholamidavoodi, Arfa Tabassum,\nArul Menezes, Arun Kirubarajan, Asher Mullokandov,\nAshish Sabharwal, Austin Herrick, Avia Efrat, Aykut\nErdem, Ayla Karakas, and et al. 2022. Beyond the im-\nitation game: Quantifying and extrapolating the capa-\nbilities of language models. CoRR, abs/2206.04615.\nAlan L. Stuart and Harry M. Markowitz. 1959. Port-\nfolio selection: Efﬁcient diversiﬁcation of investments.\nA Quarterly Journal of Operations Research, 10:253.\nSimeng Sun, Ori Shapira, Ido Dagan, and Ani\nNenkova. 2019. How to compare summarizers\nwithout target length? pitfalls, solutions and re-\nexamination of the neural summarization literature. In\nProceedings of the Workshop on Methods for Opti-\nmizing and Evaluating Neural Language Generation ,\npages 21–29, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nMirac Suzgun, Nathan Scales, Nathanael Schärli,\nSebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V . Le, Ed H. Chi, Denny\nZhou, and Jason Wei. 2022. Challenging big-bench\ntasks and whether chain-of-thought can solve them.\nCoRR, abs/2210.09261.\nYi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won\nChung, William Fedus, Jinfeng Rao, Sharan Narang,\nVinh Q Tran, Dani Yogatama, and Donald Metzler.\n2022. Scaling laws vs model architectures: How\ndoes inductive bias inﬂuence scaling? arXiv preprint\narXiv:2207.10551.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam M. Shazeer, Apoorv Kulshreshtha, Heng-\nTze Cheng, Alicia Jin, Taylor Bos, Leslie Baker,\nYu Du, Yaguang Li, Hongrae Lee, Huaixiu Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Yanqi Zhou, Chung-Ching Chang,\nI. A. Krivokon, Willard James Rusch, Marc Pickett,\nKathleen S. Meier-Hellstern, Meredith Ringel Mor-\nris, Tulsee Doshi, Renelito Delos Santos, Toju Duke,\nJohnny Hartz Søraker, Ben Zevenbergen, Vinodkumar\nPrabhakaran, Mark Díaz, Ben Hutchinson, Kristen Ol-\nson, Alejandra Molina, Erin Hoffman-John, Josh Lee,\nLora Aroyo, Ravindran Rajakumar, Alena Butryna,\nMatthew Lamm, V . O. Kuzmina, Joseph Fenton,\nAaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise\nAguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and\nQuoc Le. 2022. Lamda: Language models for dialog\napplications. ArXiv, abs/2201.08239.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. 2019. Superglue: A\nstickier benchmark for general-purpose language un-\nderstanding systems. ArXiv, abs/1905.00537.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022a. Finetuned lan-\nguage models are zero-shot learners. In The Tenth In-\nternational Conference on Learning Representations,\n9205\nICLR 2022, Virtual Event, April 25-29, 2022. OpenRe-\nview.net.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Huai hsin Chi, Quoc Le, and Denny Zhou.\n2022b. Chain of thought prompting elicits reasoning\nin large language models. ArXiv, abs/2201.11903.\nJohnny Wei and Robin Jia. 2021. The statistical advan-\ntage of automatic NLG metrics at the system level. In\nProceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics and the 11th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers) , pages 6840–6854,\nOnline. Association for Computational Linguistics.\nFrank Wilcoxon. 1946. Individual comparisons of\ngrouped data by ranking methods. Journal of eco-\nnomic entomology, 39(2):269–270.\nLinting Xue, Noah Constant, Adam Roberts, Mihir\nKale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua,\nand Colin Raffel. 2021. mT5: A massively multilin-\ngual pre-trained text-to-text transformer. In Proceed-\nings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguis-\ntics: Human Language Technologies , pages 483–498,\nOnline. Association for Computational Linguistics.\nBarret Zoph, Irwan Bello, Sameer Kumar, Nan Du,\nYanping Huang, Jeff Dean, Noam Shazeer, and\nWilliam Fedus. 2022. St-moe: Designing stable and\ntransferable sparse expert models.\n9206\nA Additional empirical results\nTable 3, Table 4 and Table 5 report ROUGE-2\nand BLEURT and ChrF results respectively for all\ntasks. These results are in line with the discussed\nresults in 4\nB Technical details\nFinetuning and inference was done in the t5x\nframework for public and closed access models.\nFew-shot learning task methodology is well de-\nscribed in 3, for public access models inference\nwas done via their respective public API, whilst all\nother models were loaded from the standard check-\npoint into TPU accelerators and inference was\ndone on batches of 64. Finetuning was carried out\nin TPU accelerators, for PaLM we used a constant\nlearning rate of 5×10−5, 20x smaller than during\npretraining and reset the optimizer’s (Adafactor)\naccumulators, for T5, mT5 and LongT5 we used a\nconstant learning rate of 1×10−4.\nC Computational Cost and\nEnvironmental Impact\nIn our work we benchmark 27 generation tasks\nwhich require a substantial amount of computa-\ntional resources. Inference on PLMs is exceed-\ningly more efﬁcient than ﬁnetuning. We report\nthe number of test examples across all tasks to be\n194,844. Inference over this number of examples\ntimes 10 models evaluated amounts to 2 million\ninference examples. Finetuning on the other hand,\nrequires all parameters to be trained and training\ndataset sizes are considerably larger. We estimate\nthe compute usage for ﬁnetuning to be 256 TPU\nv3 chips for 200 hours. One of the goals of this\nwork is to encourage benchmarking in the future\nto avoid these costs by more efﬁciently selecting\nsmaller test size and persuade researchers to only\nevaluate ﬁnetuning approaches when suitable.\n9207\nFigure 4: Empirical probability of p-value of Wilcoxon Rank Sum test< 0.05 for any combination between 1-shot\nand ﬁnetuned models.\n9208\nOne shot Finetuning\nTask PaLM\n8B\nPaLM\n540B\nLaMDA\n137B\nGPT-3.5\n175B\nPaLM\n8B\nPaLM\n540B\nST-MoE\n32B\nT5\n11B\nmT5\n11B\nLongT5\n3B\nData-To-Text\nE2E (en) 26.7 37.3 4.2 37.9 45.7 45.3 44.2 45.2 45.5 46.3\nWebNLG (en) 33.8 45.8 5.4 46.0 47.7 49.2 47.6 39.6 35.8 48.8\nToTTo (en) 26.4 37.8 3.2 38.1 53.9 55.9 55.2 54.1 53.3 54.5\nCzech Restaurant (cs) 7.9 18.1 0.9 22.3 30.2 30.6 25.4 28.8 25.0 29.9\nWebNLG (ru) 4.9 16.5 1.2 16.8 22.4 23.4 13.0 23.1 23.2 24.2\nEnglish Generation\nXSum (en) 8.0 14.4 3.4 19.9 16.3 21.2 22.8 21.0 17.5 20.8\nXLSum (en) 6.5 11.7 2.7 17.0 19.6 29.5 30.6 28.0 26.5 27.4\nWikiLingua (en) 0.7 0.4 0.7 0.9 0.4 0.4 0.4 0.4 0.4 0.4\nCrosslingual Generation\nWikiLingua (es →en) 0.7 0.5 0.4 0.6 0.4 0.4 0.3 0.4 0.4 0.4\nWikiLingua (ru →en) 3.1 6.8 0.1 7.8 14.0 18.7 12.0 12.7 15.1 14.4\nWikiLingua (tr →en) 3.1 8.7 2.1 10.1 16.6 23.0 17.7 13.8 20.8 14.3\nWikiLingua (vi →en) 2.4 5.5 0.4 6.8 13.4 0.4 10.3 9.7 14.8 13.2\nMultilingual Generation [SentencePiece-ROUGE-2]\nMLSum (es) 3.7 4.5 0.7 4.9 10.7 0.7 13.1 12.1 13.5 13.6\nMLSum (de) 8.8 16.8 1.2 16.9 26.9 33.4 36.5 36.1 35.9 36.3\nXLSum (ar) 4.5 11.7 2.4 9.6 25.8 30.0 1.9 2.0 32.1 0.6\nXLSum (bn) 1.0 1.8 0.5 2.7 18.5 23.5 0.2 0.1 29.4 0.1\nXLSum (ja) 4.0 6.7 0.3 9.6 27.1 31.8 0.1 0.1 31.5 0.1\nXLSum (id) 7.2 11.3 3.7 12.6 25.0 33.0 30.5 30.7 32.7 30.3\nXLSum (sw) 7.9 16.2 6.5 6.6 22.7 26.8 27.8 27.6 31.3 26.8\nXLSum (ko) 6.9 9.6 1.6 9.4 23.0 26.7 4.1 3.7 34.9 4.0\nXLSum (ru) 6.0 9.2 3.7 10.9 20.8 29.4 4.5 6.1 29.6 6.1\nXLSum (te) 2.4 3.3 1.1 4.8 13.3 22.7 3.2 3.2 26.5 3.3\nXLSum (th) 2.9 4.0 0.3 6.2 16.4 22.5 2.4 2.5 26.9 2.4\nXLSum (tr) 7.5 10.5 3.2 10.7 23.7 32.7 17.6 17.8 32.2 17.7\nXLSum (es) 5.8 9.0 3.1 9.6 14.2 23.7 20.1 20.6 23.0 20.6\nXLSum (vi) 4.8 6.8 1.5 7.5 20.2 35.9 11.9 13.2 35.5 13.1\nXLSum (hi) 4.4 6.4 1.8 7.0 29.0 35.7 1.8 0.0 35.4 0.0\nTable 3: ROUGE-2 and SentencePiece-ROUGE-2 results in data-to-text, English and multilingual generation\ndatasets.\n9209\nOne shot Finetuning\nTask PaLM\n8B\nPaLM\n540B\nLaMDA\n137B\nGPT-3.5\n175B\nPaLM\n8B\nPaLM\n540B\nST-MoE\n32B\nT5\n11B\nmT5\n11B\nLongT5\n3B\nData-To-Text\nE2E (en) 46.1 57.8 15.1 57.8 62.5 61.9 61.1 62.1 60.9 61.8\nWebNLG (en) 47.5 61.8 17.0 61.8 63.6 65.2 64.1 59.4 55.8 65.4\nToTTo (en) 43.5 55.8 12.6 55.2 67.5 69.4 68.3 67.3 66.8 67.7\nCzech Restaurant (cs) 17.6 35.6 7.9 41.5 48.0 48.1 36.6 38.2 44.1 40.1\nWebNLG (ru) 21.8 45.5 15.3 45.3 62.7 62.6 24.4 31.8 63.5 32.1\nEnglish Generation\nXSum (en) 24.6 32.7 18.5 37.6 34.4 38.9 41.3 39.3 36.8 39.0\nXLSum (en) 23.8 29.9 13.9 33.5 35.8 45.8 47.0 46.0 43.9 44.4\nWikiLingua (en) 14.7 14.3 15.4 15.8 15.1 14.6 15.2 15.8 15.8 15.3\nCrosslingual Generation\nWikiLingua (es →en) 16.8 13.2 10.2 14.6 14.2 14.9 13.0 15.1 14.8 15.7\nWikiLingua (ru →en) 19.1 21.5 1.5 22.5 30.6 35.9 24.0 28.8 34.3 33.3\nWikiLingua (tr →en) 19.3 24.6 12.3 26.7 34.4 39.4 32.6 30.8 39.0 32.7\nWikiLingua (vi →en) 16.4 19.9 4.4 21.3 31.8 14.2 23.2 27.6 32.3 32.5\nMultilingual Generation\nMLSum (es) 21.3 22.9 5.3 20.6 28.0 18.4 29.1 28.7 30.7 30.3\nMLSum (de) 28.9 37.0 5.7 34.4 41.9 48.8 50.9 50.5 50.1 51.5\nXLSum (ar) 14.2 22.7 11.4 24.4 35.0 39.6 0.2 0.2 41.6 0.1\nXLSum (bn) 10.3 12.7 4.5 17.5 28.6 34.0 0.0 0.0 37.8 0.0\nXLSum (ja) 8.8 11.6 1.2 13.8 26.0 31.3 0.8 0.9 30.6 0.9\nXLSum (id) 21.0 26.0 9.0 26.2 36.8 45.3 43.2 42.3 43.0 43.4\nXLSum (sw) 24.0 33.0 15.0 21.5 36.2 42.0 40.1 41.1 44.4 41.6\nXLSum (ko) 6.9 9.4 1.6 10.0 18.0 21.8 1.4 1.2 27.5 1.4\nXLSum (ru) 15.0 19.8 9.4 26.5 29.1 38.6 14.4 20.1 40.3 19.9\nXLSum (te) 11.3 13.6 4.7 16.8 18.0 29.8 0.3 0.2 30.4 0.3\nXLSum (th) 14.7 16.8 4.4 21.5 27.1 33.4 0.3 0.3 33.9 0.3\nXLSum (tr) 20.3 24.9 6.2 24.5 32.7 43.1 31.2 33.1 42.6 33.8\nXLSum (es) 19.0 22.9 7.3 22.0 24.5 33.4 31.5 31.9 32.6 32.8\nXLSum (vi) 10.9 13.1 2.4 14.2 21.8 37.1 16.9 20.2 34.3 21.1\nXLSum (hi) 12.2 15.1 6.6 18.8 33.2 39.6 0.2 0.0 39.1 0.0\nTable 4: ChrF results in data-to-text, English and multilingual generation datasets.\n9210\nOne shot Finetuning\nTask PaLM\n8B\nPaLM\n540B\nLaMDA\n137B\nGPT-3.5\n175B\nPaLM\n8B\nPaLM\n540B\nST-MoE\n32B\nT5\n11B\nmT5\n11B\nLongT5\n3B\nData-To-Text\nE2E (en) 60.0 71.8 44.2 72.3 76.5 75.8 75.1 76.4 75.9 76.2\nWebNLG (en) 62.3 74.5 43.5 74.7 75.8 76.8 75.6 71.2 67.8 76.3\nToTTo (en) 56.9 69.4 33.1 69.5 76.8 77.9 77.0 76.6 76.8 76.7\nCzech Restaurant (cs) 34.7 66.8 32.2 72.2 75.8 74.4 48.8 51.8 72.9 48.8\nWebNLG (ru) 39.2 67.8 19.7 66.9 77.5 78.0 25.9 29.8 78.4 29.9\nEnglish Generation\nXSum (en) 43.0 46.9 28.2 53.4 51.0 55.5 58.5 56.4 53.2 56.4\nXLSum (en) 32.7 41.1 22.0 51.1 52.6 61.9 63.0 61.8 60.3 60.8\nWikiLingua (en) 33.3 34.0 27.9 34.3 32.2 32.2 31.3 32.4 32.6 32.1\nCrosslingual Generation\nWikiLingua (es →en) 32.9 33.7 16.8 33.4 32.3 32.6 31.0 32.5 33.0 32.5\nWikiLingua (ru →en) 38.8 43.3 6.4 45.9 50.6 54.4 46.4 49.1 52.3 51.5\nWikiLingua (tr →en) 39.3 44.0 19.3 46.4 49.2 54.4 48.6 45.4 52.8 46.8\nWikiLingua (vi →en) 35.6 38.2 5.7 40.8 50.6 32.8 45.4 45.5 51.4 50.0\nMultilingual Generation\nMLSum (es) 21.0 21.5 -1.3 26.7 30.6 6.3 25.9 24.2 32.0 27.0\nMLSum (de) 39.4 50.1 4.6 49.3 57.8 63.4 62.5 61.6 61.5 61.4\nXLSum (ar) 19.8 28.5 2.5 27.7 44.6 50.1 2.8 3.5 53.0 4.3\nXLSum (bn) 31.8 41.8 0.2 27.4 46.6 57.1 2.9 3.6 62.7 2.8\nXLSum (ja) 28.1 31.4 -1.2 34.7 47.0 52.2 -0.3 -0.3 53.0 -0.3\nXLSum (id) 41.2 47.4 9.5 53.8 58.7 68.0 61.4 65.5 66.8 65.6\nXLSum (sw) 25.5 36.3 14.3 24.0 45.8 52.9 48.6 50.2 59.1 48.9\nXLSum (ko) 25.6 31.6 -0.3 33.0 40.5 47.1 0.8 1.6 54.6 1.4\nXLSum (ru) 30.1 37.3 3.2 33.0 47.9 59.6 14.2 16.7 58.0 17.0\nXLSum (te) 29.6 35.0 6.5 22.7 32.0 49.1 10.9 11.5 51.6 12.4\nXLSum (th) 22.0 27.2 -0.1 16.3 31.9 43.6 -1.1 -0.9 46.0 -1.1\nXLSum (tr) 30.8 34.5 3.3 40.0 49.8 63.8 21.4 26.4 62.5 26.3\nXLSum (es) 21.2 26.3 0.0 30.6 31.5 46.2 33.3 36.1 45.2 35.7\nXLSum (vi) 14.5 14.5 -1.6 16.4 24.7 46.5 -4.0 -4.6 45.0 -4.5\nXLSum (hi) 33.9 40.4 7.0 33.7 50.7 57.5 5.7 4.6 57.3 4.6\nTable 5: BLEURT results in data-to-text, English and multilingual generation datasets.\n9211\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nWe added a section on Limitations.\n□\u0013 A2. Did you discuss any potential risks of your work?\nWe discuss in the ethics section the risks of autoregressive text generation, in particular how it can\nproduce misinformation or sensitive data.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nThe introduction summarizes the paper’s main claims.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nWe used artifacts throughout\n□\u0013 B1. Did you cite the creators of artifacts you used?\nWe used generation datasets found through the GEM benchmark (gem-benchmark.com). We cite the\nresearchers that released these data in section 3.1.\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nLicenses: MLSum (Scialom et al., 2020) – MIT License https://github.com/ThomasScialom/MLSUM/blob/master/LICENSE\nWikiLingua(Ladhak et al., 2020) – cc-by-3.0 https://gem-benchmark.com/data_cards/wiki_lingua\nXSum(Narayan et al., 2018) – cc-by-sa-4.0 https://gem-benchmark.com/data_cards/xsum\nClean E2E NLG(Novikova et al., 2017; Duseket al., 2019) – cc-by-sa-4.0 https://gem-benchmark.com/data_cards/e2e_nlg\nCzech Restaurants (Dusek and Jurvcivcek, 2019) – cc-by-sa-4.0https://gem-benchmark.com/data_cards/cs_restaurants\nWebNLG 2020(Gardent et al., 2017; Ferreiraet al., 2020) – cc-by-nc-4.0 https://gem-benchmark.com/data_cards/web_nlg\nToTTo(Parikh et al., 2020) – cc-by-sa-3.0 https://gem-benchmark.com/data_cards/totto\nXL-Sum(Hasan et al., 2021) – cc-by-nc-sa-4.0 https://gem-benchmark.com/data_cards/xlsum\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nAll the data mentioned above has intended research purposes which is consistent with this work’s\nuse. We mention this in the paper\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo data was collected. The data has been revised outside this work by community efforts such as\nGEM.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nWe provided information on the languages covered by each dataset in Table 1, as well as the publicly\navailable information on model’s pretraining data language distribution.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n9212\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nWe provided information on the statistic of the datasets used in Table 1,\nC □\u0013 Did you run computational experiments?\nSection 4 and 5\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nNumber of parameters per model is reported in section 3.2. Computing infrastructure is mentioned\nin the appendix B. The total computational budget is discussed in its own section.\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nHyper parameter selection is discussed in appendix B. Experimental setup is discussed in the\nmethodology section.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nMost of the contributions for this paper are statistics of empirical results and method to obtain them.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nWe provide this information in the main text, in footnotes.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n9213",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8278236985206604
    },
    {
      "name": "Benchmarking",
      "score": 0.8249084949493408
    },
    {
      "name": "Machine learning",
      "score": 0.5581456422805786
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5543022155761719
    },
    {
      "name": "Natural language understanding",
      "score": 0.544977605342865
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.543836772441864
    },
    {
      "name": "Generalization",
      "score": 0.5097119212150574
    },
    {
      "name": "Language model",
      "score": 0.49442362785339355
    },
    {
      "name": "Natural language generation",
      "score": 0.46254202723503113
    },
    {
      "name": "Task (project management)",
      "score": 0.4617249369621277
    },
    {
      "name": "Natural language",
      "score": 0.4351430833339691
    },
    {
      "name": "Field (mathematics)",
      "score": 0.4180997908115387
    },
    {
      "name": "Natural language processing",
      "score": 0.35033130645751953
    },
    {
      "name": "Systems engineering",
      "score": 0.10787129402160645
    },
    {
      "name": "Engineering",
      "score": 0.09473276138305664
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    }
  ]
}