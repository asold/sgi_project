{
    "title": "The role of large language models in improving the readability of orthopaedic spine patient educational material",
    "url": "https://openalex.org/W4410769187",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A5117710785",
            "name": "Melissa Romoff",
            "affiliations": [
                "University of California, Irvine Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A5088538431",
            "name": "Madison Brunette",
            "affiliations": [
                "University of California, Irvine Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A3018970921",
            "name": "Melanie K Peterson",
            "affiliations": [
                "University of California, Irvine Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A2617378578",
            "name": "Sohaib Z. Hashmi",
            "affiliations": [
                "University of California, Irvine Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A2115023894",
            "name": "Michael S. Kim",
            "affiliations": [
                "University of California, Irvine Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A5117710785",
            "name": "Melissa Romoff",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5088538431",
            "name": "Madison Brunette",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3018970921",
            "name": "Melanie K Peterson",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2617378578",
            "name": "Sohaib Z. Hashmi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2115023894",
            "name": "Michael S. Kim",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2131484877",
        "https://openalex.org/W2104860609",
        "https://openalex.org/W2122867552",
        "https://openalex.org/W1966425470",
        "https://openalex.org/W1996548435",
        "https://openalex.org/W4322616465",
        "https://openalex.org/W2063059082",
        "https://openalex.org/W2792805190",
        "https://openalex.org/W2143386650",
        "https://openalex.org/W1993568272",
        "https://openalex.org/W4406167360",
        "https://openalex.org/W4392867067",
        "https://openalex.org/W4367307887",
        "https://openalex.org/W2189635277",
        "https://openalex.org/W4285019978",
        "https://openalex.org/W3034857645",
        "https://openalex.org/W4408030086",
        "https://openalex.org/W4391623833",
        "https://openalex.org/W4405421677",
        "https://openalex.org/W4399489002",
        "https://openalex.org/W4404936930",
        "https://openalex.org/W4408972767",
        "https://openalex.org/W2054562460"
    ],
    "abstract": "Abstract Introduction Patient education is crucial for informed decision-making. Current educational materials are often written at a higher grade level than the American Medical Association (AMA)-recommended sixth-grade level. Few studies have assessed the readability of orthopaedic materials such as American Academy of Orthopaedic Surgeons (AAOS) OrthoInfo articles, and no studies have suggested efficient methods to improve readability. This study assessed the readability of OrthoInfo spine articles and investigated the ability of large language models (LLMs) to improve readability. Methods A cross-sectional study analyzed 19 OrthoInfo articles using validated readability metrics (Flesch-Kincaid Grade Level and Reading Ease). Articles were simplified iteratively in three steps using ChatGPT, Gemini, and CoPilot. LLMs were prompted to summarize text, followed by two clarification prompts simulating patient inquiries. Word count, readability, and accuracy were assessed at each step. Accuracy was rated by two independent reviewers using a three-point scale (3 = fully accurate, 2 = minor inaccuracies, 1 = major inaccuracies). Statistical analysis included one-way and two-way ANOVA, followed by Tukey post-hoc tests for pairwise comparisons. Results Baseline readability exceeded AMA recommendations, with a mean Flesch-Kincaid Grade Level of 9.5 and a Reading Ease score of 51.1. LLM summaries provided statistically significant improvement in readability, with the greatest improvements in the first iteration. All three LLMs performed similarly, though ChatGPT achieved statistically significant improvements in Reading Ease scores. Gemini incorporated appropriate disclaimers most consistently. Accuracy remained stable throughout, with no evidence of hallucination or compromise in content quality or medical relevance. Discussion LLMs effectively simplify orthopaedic educational content by reducing grade levels, enhancing readability, and maintaining acceptable accuracy. Readability improvements were most significant in initial simplification steps, with all models performing consistently. These findings support the integration of LLMs into patient education workflows, offering a scalable strategy to improve health literacy, enhance patient comprehension, and promote more equitable access to medical information across diverse populations.",
    "full_text": "Romoff et al. \nJournal of Orthopaedic Surgery and Research          (2025) 20:531  \nhttps://doi.org/10.1186/s13018-025-05955-1\nRESEARCH Open Access\n© The Author(s) 2025. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http://creativecommons.org/licenses/by/4.0/.\nJournal of Orthopaedic\nSurgery and Research\nThe role of large language models \nin improving the readability of orthopaedic \nspine patient educational material\nMelissa Romoff1, Madison Brunette1, Melanie K. Peterson1, Sohaib Z. Hashmi1 and Michael S. Kim1* \nAbstract \nIntroduction Patient education is crucial for informed decision-making. Current educational materials are often \nwritten at a higher grade level than the American Medical Association (AMA)-recommended sixth-grade level. Few \nstudies have assessed the readability of orthopaedic materials such as American Academy of Orthopaedic Sur-\ngeons (AAOS) OrthoInfo articles, and no studies have suggested efficient methods to improve readability. This study \nassessed the readability of OrthoInfo spine articles and investigated the ability of large language models (LLMs) \nto improve readability.\nMethods A cross-sectional study analyzed 19 OrthoInfo articles using validated readability metrics (Flesch-Kincaid \nGrade Level and Reading Ease). Articles were simplified iteratively in three steps using ChatGPT, Gemini, and CoPilot. \nLLMs were prompted to summarize text, followed by two clarification prompts simulating patient inquiries. Word \ncount, readability, and accuracy were assessed at each step. Accuracy was rated by two independent reviewers using \na three-point scale (3 = fully accurate, 2 = minor inaccuracies, 1 = major inaccuracies). Statistical analysis included one-\nway and two-way ANOVA, followed by Tukey post-hoc tests for pairwise comparisons.\nResults Baseline readability exceeded AMA recommendations, with a mean Flesch-Kincaid Grade Level of 9.5 \nand a Reading Ease score of 51.1. LLM summaries provided statistically significant improvement in readability, \nwith the greatest improvements in the first iteration. All three LLMs performed similarly, though ChatGPT achieved \nstatistically significant improvements in Reading Ease scores. Gemini incorporated appropriate disclaimers most con-\nsistently. Accuracy remained stable throughout, with no evidence of hallucination or compromise in content quality \nor medical relevance.\nDiscussion LLMs effectively simplify orthopaedic educational content by reducing grade levels, enhancing read-\nability, and maintaining acceptable accuracy. Readability improvements were most significant in initial simplification \nsteps, with all models performing consistently. These findings support the integration of LLMs into patient education \nworkflows, offering a scalable strategy to improve health literacy, enhance patient comprehension, and promote \nmore equitable access to medical information across diverse populations.\nIntroduction\nEffective education is essential for supporting informed \ndecision-making and protecting patient autonomy [1–5]. \nHowever, many educational materials remain inaccessi -\nble to the general population due to readability challenges \n[6]. The National Assessment of Adult Literacy reports \nthat the average U.S. adult reads at an eighth-grade level \n*Correspondence:\nMichael S. Kim\nmichak14@hs.uci.edu\n1 Department of Orthopaedic Surgery, University of California, Irvine, \nSchool of Medicine, 101 The City Dr S, Pavilion 3, Building 29 A, Orange, \nCA 92868, USA\nPage 2 of 9Romoff et al. Journal of Orthopaedic Surgery and Research          (2025) 20:531 \n[7], while Medicaid enrollees average at a fifth-grade level \n[8]. To address this discrepancy, the American Medical \nAssociation (AMA) recommends that patient educa -\ntional materials be written at or below a sixth-grade read-\ning level to ensure accessibility [9].\nDespite these guidelines, a 2018 systematic review \nfound that many health education resources are writ -\nten at a 10 th- to 15 th-grade level [10]. In the field of \northopaedics, the American Academy of Orthopaedic \nSurgeons (AAOS) provides public-facing educational \ncontent through its OrthoInfo website. Previous studies \nhave demonstrated that orthopaedic educational mate -\nrials frequently exceed recommended reading levels \n[11–19], but OrthoInfo’s spine content, despite its com -\nplexity and clinical importance, has not been systemati -\ncally evaluated.\nRecent work in other medical specialties has shown \nthat large language models (LLMs) can improve the \nreadability of patient education without compromising \naccuracy [20–22]. However, adoption in clinical settings \nremains limited due to concerns surrounding factual \nreliability, medicolegal implications, and the absence of \nformal guidelines or validated implementation strategies \n[23]. These limitations underscore the need for additional \nresearch evaluating LLM performance across diverse \nclinical domains.\nAlthough LLM-based readability analyses have been \nconducted in orthopaedics, prior studies have not spe -\ncifically examined spine-related articles from OrthoInfo, \na widely used, patient-facing educational platform. Given \nthe procedural complexity and specialized terminology \ncommon in spine surgery, general findings from other \nfields – or even from broader orthopaedic topics – may \nnot fully apply. In this study, we applied a structured, \niterative prompting protocol across multiple LLMs to \nsimplify OrthoInfo spine content, evaluating readability \nimprovements, content accuracy, and the potential for \neffective integration of LLMs into orthopaedic patient \neducation.\nMethods\nA cross-sectional observational study of public-facing \neducational material published by the American Acad -\nemy of Orthopaedic Surgeons on OrthoInfo.AAOS.org \nwas conducted. Institutional Review Board approval was \nnot required as no protected health information was \ninvolved in this study. No large language models (LLMs) \nwere used in drafting this manuscript text itself; however, \nLLMs were employed for content transformation as part \nof the study design.\nAll OrthoInfo web pages associated with the treat -\nment of spinal conditions were identified by selecting \n“Treatment” from the homepage toolbar and reviewing \narticles under “Neck” and “Back. ” Web pages present -\ning video content without text substantial enough to \nenable analysis were excluded. Articles were classified \ninto three categories: 1) Background: Articles primar -\nily providing general education about conditions; 2) \nProcedure: Articles primarily focused on surgical or \nnon-surgical interventions; 3) Opinion: Articles offer -\ning physician perspectives or decision-making advice. \nContent from each article was extracted as plain text. \nWords and figures were counted. Date of last review \nwas recorded.\nText was then submitted to the three most used LLMs: \nChatGPT GPT-4o (OpenAI, San Francisco, CA), Copi -\nlot (Microsoft, Redmond, WA), and Gemini 1.5 Flash-8B \n(Alphabet, Mountain View, CA), preceded by the request \n“Please summarize the following. ” A new chat session was \nopened for each article to prevent response bias. Summa-\nrization was prompted through three iterative steps: (1) \n“Please summarize the following, ” (2) “I don’t understand, \nplease clarify, ” and (3) “I still don’t understand, please \nclarify. ” Each response was documented and analyzed for \nword count, readability, accuracy, and the presence of an \nappropriate disclaimer.\nReadability was evaluated using Readable (Added \nBytes, Hassocks, UK), a software that calculates validated \nmeasures of readability. These included the Flesch-Kin -\ncaid Grade Level (estimated U.S. school grade level) and \nFlesch Reading Ease (scored from 0 to 100, with higher \nscores indicating easier readability). Given the inherent \nvariability of measures of readability, an overall mean \nreadability was calculated by averaging readability meas -\nures that provide estimated grade level as had been done \nin a previous study to allow direct comparison.\nAccuracy was independently assessed by two phy -\nsician authors using a three-point scale: summaries \nthat were fully accurate without requiring correction \nreceived three points; those with minor inaccuracies \nor hallucinations received two points; and those with \nsubstantial inaccuracies or hallucinations that affected \nmedical relevance received one point. A three-point \nscale was selected to facilitate consistent reviewer grad -\ning and focus on clinically meaningful content accuracy \nrather than stylistic variation. In cases of disagreement, \nscores were reviewed collaboratively, and a consensus \nwas reached between the two reviewers and senior \nauthor to ensure consistency and minimize individual \nbias in the final dataset.\nStatistical analysis was performed using R. Descriptive \nstatistics, mean and standard deviations, were calculated \nfor article characteristics and LLM outputs. One-way \nand two-way ANOVA were used to compare readability, \nword count, and accuracy across LLMs and steps. Two-\nway ANOVA was used specifically to evaluate interaction \nPage 3 of 9\nRomoff et al. Journal of Orthopaedic Surgery and Research          (2025) 20:531 \n \neffects between model and simplification step. Assump -\ntions of normality and homogeneity of variants were \nassessed prior to performing ANOVA tests. Significant \nANOVA results were followed by Tukey’s post-hoc tests \nfor pairwise comparisons. Linear regression evaluated \ntrends between readability metrics and baseline article \ncharacteristics. Statistical significance was defined as p < \n0.05, and visualizations were generated using GraphPad \nPrism (GraphPad Software, San Diego, CA).\nResults\nBaseline characteristics\nOut of the 23 webpages initially reviewed, 19 met the \ninclusion criteria (8 procedure, 10 background, and \n1 ortho-opinion), while 4 were excluded due to being \nvideo-based content with minimal written material. No \nhallucinations were observed during the analysis.\nThe baseline readability analysis of the 19 included \narticles revealed a mean Flesch-Kincaid Grade Level of \n9.5 (SD: 1.4), exceeding the AMA’s recommended sixth-\ngrade level, and a mean Flesch Reading Ease score of 51.1 \n(SD: 7.5), indicating “fairly difficult” readability (Table  1, \nFig.  1). On average, the articles contained 1,269 words \nand included 2.8 figures (Table  1). Procedure articles \nwere shorter, averaging 1,033 words, but were more diffi -\ncult to read (Grade Level: 9.8; Reading Ease: 49) and con -\ntained more figures. (4 per article) (Fig.  1). In contrast, \nBackground articles were longer, averaging 1,523 words, \nslightly easier to read (Grade Level: 8.7; Reading Ease: \n54), and included fewer figures. (2 per article) (Fig. 1).\nStatistical tests\nT-tests revealed that Procedure articles had significantly \nfewer words than Background articles (p = 0.03), but no \nsignificant differences were found in figures, readabil -\nity scores, or grade levels. Pearson correlation analyses \nshowed no significant relationships between readability \nmetrics (Flesch-Kincaid Grade Level and Flesch Reading \nEase) and variables such as word count, number of fig -\nures, or review age (all p > 0.05).\nLLM analysis\nEach LLM (ChatGPT, Gemini, CoPilot) was evaluated \nindependently across simplification steps. Significant \nword count reductions were observed for all models (p < \n0.0001), with the largest reduction occurring between \nthe baseline and Step 1, followed by a plateau in subse -\nquent steps (Table  2, Fig.  2A). Reading Ease improved \nsignificantly across steps (p < 0.0001), with the greatest \nimprovement between the original articles and Step 1, \nand smaller but still significant gains from Step 1 to Step \n2 (Table  3, Fig.  2B). Grade Level changes were signifi -\ncant (p < 0.0001) across steps, but an unexpected initial \nincrease was observed between the original articles and \nStep 1, with a negative mean difference indicating move -\nment away from the AMA’s sixth-grade recommendation. \nSubsequent steps showed gradual reductions, ultimately \ntrending toward the desired grade level (Table 3, Figs. 2C, \n3, 4 and 5).\nAll LLMs performed similarly in reducing word count, \nimproving reading ease, and lowering grade levels, with \nno statistically significant differences observed between \nmodels or article types. However, a two-way ANOVA \nrevealed that Gemini provided significantly more dis -\nclaimers than ChatGPT (mean difference = 10.0, p = \n0.002) and CoPilot (mean difference = 8.3, p = 0.005), \nwhile no difference was observed between ChatGPT and \nCoPilot (Table  4). Simplification steps did not influence \ndisclaimer frequency (p = 0.71) (Table 4).\nLLM specific findings\nThe two-way ANOVA compared all models (ChatGPT, \nGemini, and CoPilot) across steps to assess differences \nin performance. Significant reductions in Word Count \nwere observed across steps (p < 0.0001), with no signifi -\ncant differences between models (p = 0.80) or interac -\ntion effects (p = 0.997) (Supplementary Table 1). Reading \nEase improved significantly across steps (p < 0.0001), \nwith minor differences between models (p = 0.03) and \na significant interaction effect (p = 0.03) (Supplemen -\ntary Table 1); post-hoc tests indicated ChatGPT slightly \noutperformed Gemini (mean difference = 4.1, p < 0.05), \nwhile CoPilot performed similarly to both (Supplemen -\ntary Table  2). Grade Level reductions were driven by \nsteps (p < 0.0001), with small differences between mod -\nels ( p = 0.01) but no significant interaction effects (p = \n0.11) (Supplementary Table  1); Post-hoc tests revealed \nthat ChatGPT achieved slightly lower grade levels than \nTable 1 Baseline Descriptive Statistics for Readability Metrics\nMetric Overall (Mean ± SD) Procedure (Mean ± SD) Background (Mean ± SD)\nWord Count 1269 ± 493.86 1033 ± 348.24 1523 ± 474.56\n2.79 ± 2.49 4.0 ± 1.28 2.0 ± 1.02\nFlesch-Kincaid Grade 9.52 ± 1.36 9.8 ± 1.18 8.65 ± 1.12\nFlesch Reading Ease 51.07 ± 7.48 49.0 ± 5.85 54.0 ± 8.11\nPage 4 of 9Romoff et al. Journal of Orthopaedic Surgery and Research          (2025) 20:531 \nGemini (mean difference = −0.8, p < 0.05), with no sig -\nnificant differences involving CoPilot (Supplementary \nTable 2). Accuracy varied slightly across steps (p = 0.03) \nbut showed no significant differences between models \n(p = 0.95) or interaction effects (p = 0.17) (Supplementary \nTable 1, Supplementary Table 3).\nDiscussion\nThis study evaluated the ability of large language models \n(LLMs)—ChatGPT, Gemini, and CoPilot—to simplify \northopedic educational content and improve accessibil -\nity for patients. The findings demonstrate that LLMs are \neffective at significantly improving readability metrics \nFig. 1 Baseline readability characteristics of OrthoInfo educational materials. Readability metrics are shown for overall (n = 19), procedure (n = \n8), and background (n = 10) sections. AMA recommendations are indicated by the red dashed line, and error bars represent standard deviations. \nA Mean Flesch-Kincaid Grade Level for overall, procedure, and background sections. B Mean Flesch Reading Ease scores for overall, procedure, \nand background sections\nPage 5 of 9\nRomoff et al. Journal of Orthopaedic Surgery and Research          (2025) 20:531 \n \nsuch as word count, grade level, and reading ease, with \nconsistent performance across models.\nBaseline analysis confirmed that OrthoInfo exceeded \nthe AMA’s recommended sixth-grade reading level, high-\nlighting the need for simplification to make educational \nmaterials more accessible. Significant improvements in \nreadability metrics occurred across all models, particu -\nlarly between the original text and Step 1, with dimin -\nishing returns in subsequent steps. While additional \niterations may offer further refinement, our data suggest \na plateau effect between Steps 2 and 4. We therefore lim -\nited the process to three simplifications to balance ana -\nlytical depth and practical considerations. Future studies \ncould explore extended prompting protocols to deter -\nmine optimal iteration thresholds.\nInterestingly, grade level initially increased after the \nfirst simplification step. This may reflect the LLMs’ ten -\ndency to introduce polysyllabic or technical language \nin early summarization attempts, particularly when \nlacking clear context about the target audience. These \nchanges can inadvertently inflate readability scores, even \nif the overall content becomes more concise. Subsequent \nprompts likely encouraged more targeted simplifica -\ntion leading to final outputs that better aligned with the \nsixth-grade target. Despite, these fluctuations, accuracy \nremained stable across steps and models, suggesting that \nLLMS can simplify complex medical content without \ncompromising factual integrity.\nMinimal differences were observed between models. \nChatGPT slightly outperformed Gemini in both improv -\ning reading ease and lowering grade levels, while CoPi -\nlot performed similarly to both models. Although some \ndifferences were statistically significant, their practical \nimplications appear limited. Importantly, this study was \nnot designed to establish the superiority of any specific \nLLM, but rather to descriptively assess their current \ncapabilities in simplifying orthopaedic educational con -\ntent. These findings suggest that the choice of model may \nbe less critical than the simplification process itself.\nSeveral limitations warrant discussion. This study \nfocused exclusively on simplify existing content and did \nnot assess LLMs’ ability to generate new or patient-spe -\ncific materials. While readability metrics are valuable, \nthey do not fully capture comprehension, particularly \nfor individuals with limited health literacy. Furthermore, \nalthough new chat sessions were used for each article to \nminimize confounding, slight variability in LLM outputs \nis inherent to generative models. Given our standardized \napproach and outcome focus, this variability likely had \nminimal impact.\nWhile this study focused on quantitative outcomes \nsuch as readability, word count, and accuracy, it did not \ninclude a quantitative or thematic analysis of how con -\ntent was transformed. Future studies should explore how \nLLMs affect tone, phrasing and message structure in \nways that may influence patient perception. Additionally, \nwhile we assessed accuracy using a standardized scale, \nfuture studies may benefit from more granular error clas-\nsification to better understand how LLMs might alter \ncritical content in patient facing materials.\nPractical applications of these results involve integrat -\ning LLM-based simplification pipelines into electronic \nhealth record (EHR) systems to automatically generate \naccessible versions of home care instructions and proce -\ndure notes. Similarly, online educational platforms such \nas OrthoInfo could integrate LLM-driven readability \nchecks and iterative simplifications prior to publication, \nensuring that patient-facing materials meet health liter -\nacy standards. Future research should focus on adapting \nthese tools to the needs of specific demographic groups \nand incorporating direct measures of user comprehen -\nsion, such as patient surveys or health literacy assess -\nments, to evaluate real-world applicability and maximize \nimpact.\nConclusion\nThis study demonstrates the potential of large language \nmodels (LLMs) like ChatGPT, Gemini, and CoPilot to \nimprove the readability of orthopedic educational con -\ntent, addressing the gap between current materials and \nthe AMA’s recommended sixth-grade reading level. By \nsignificantly reducing grade levels and improving read -\ning ease, these models simplify medical content without \ncompromising accuracy. While ChatGPT showed slightly \ngreater readability, all models achieved progressive \nTable 2 ANOVA Results for Readability Metrics Across ChatGPT, \nGemini, and CoPilot\nMetric LLM F-Statistic P-value Significant?\nWord Count ChatGPT 89.54  < 0.0001 ****\nGemini 95.87  < 0.0001 ****\nCoPilot 86.89  < 0.0001 ****\nGrade Level ChatGPT 36.26  < 0.0001 ****\nGemini 17.23  < 0.0001 ****\nCoPilot 35.26  < 0.0001 ****\nReading Ease ChatGPT 45.38  < 0.0001 ****\nGemini 23.03  < 0.0001 ****\nCoPilot 26.84  < 0.0001 ****\nPage 6 of 9Romoff et al. Journal of Orthopaedic Surgery and Research          (2025) 20:531 \nsimplifications across steps, highlighting the feasibility of \niterative text refinement.\nWhile promising, this study’s focus on simplify -\ning existing materials leaves unexplored opportunities \nfor generating patient-specific content and addressing \nbroader health literacy needs. Importantly, our findings \ndo not assess how LLM-generated content performs \nacross diverse patient demographics or literacy levels. \nFig. 2 Readability and word count improvements across steps for ChatGPT, Gemini, and CoPilot. Mean values are presented for Baseline (Original) \nand Steps 1–3, with error bars representing standard deviations. A Mean Word Count for ChatGPT, Gemini, and CoPilot across steps. B Mean \nFlesch-Kincaid Reading Ease scores for ChatGPT, Gemini, and CoPilot across steps. C Mean Flesch-Kincaid Grade Level for ChatGPT, Gemini, \nand CoPilot across steps\nPage 7 of 9\nRomoff et al. Journal of Orthopaedic Surgery and Research          (2025) 20:531 \n \nTable 3 Post-Hoc Results for One-way ANOVA Readability Metrics Across ChatGPT, Gemini, and CoPilot\nMetric Comparison ChatGPT: Mean Difference \n(P-value)\nGemini: Mean Difference \n(P-value)\nCoPilot: Mean \nDifference \n(P-value)\nWord Count Baseline vs. Step 1 1019 (**** < 0.0001) 1071 (**** < 0.0001) 1031 (**** < 0.0001)\nBaseline vs. Step 2 1110 (**** < 0.0001) 1133 (**** < 0.0001) 1102 (**** < 0.0001)\nBaseline vs. Step 3 1119 (**** < 0.0001) 1114 (**** < 0.0001) 1129 (**** < 0.0001)\nGrade Level Baseline vs. Step 1 −1.595 (* 0.0140) −1.626 (** 0.0089) −1.195 (ns 0.0977)\nBaseline vs. Step 2 1.484 (* 0.0255) −0.07895 (ns 0.9986) 0.6842 (ns 0.5394)\nBaseline vs. Step 3 3.463 (**** < 0.001) 1.942 (** 0.0012) 2.347 (*** 0.0001)\nReading Ease Baseline vs. Step 1 18.05 (**** < 0.0001) 16.34 (**** < 0.0001) 14.41 (**** < 0.0001)\nBaseline vs. Step 2 −4.474 (ns 0.4819) 5.889 (ns 0.2836) 0.9579 (ns 0.9883)\nBaseline vs. Step 3 −17.90 (**** < 0.0001) −10.19 (* 0.0141) −9.321 (* 0.0128)\nFig. 3 Performance of Gemini in improving readability and reducing word count across steps. Mean and standard deviations for readability metrics \n(Word Count, Flesch-Kincaid Reading Ease, and Flesch-Kincaid Grade Level) of Baseline articles and across Steps 1–3 using Gemini. Significance \nlevels are indicated within the figure. Error bars represent standard deviations. A Word Count for Baseline and Gemini Steps 1–3. B Flesch-Kincaid \nReading Ease scores for Baseline and Gemini Steps 1–3, with the AMA-recommended reading ease threshold shown as a red dashed line. C \nFlesch-Kincaid Grade Level for Baseline and Gemini Steps 1–3, with the AMA’s recommended sixth-grade reading level shown as a red dashed line\nFig. 4 Performance of CoPilot in improving readability and reducing word count across steps. Mean and standard deviations for readability metrics \n(Word Count, Flesch-Kincaid Reading Ease, and Flesch-Kincaid Grade Level) of Baseline articles and across Steps 1–3 using CoPilot. Significance \nlevels are indicated within the figure. Error bars represent standard deviations. A Word Count for Baseline and CoPilot Steps 1–3. B Flesch-Kincaid \nReading Ease scores for Baseline and CoPilot Steps 1–3, with the AMA-recommended reading ease threshold shown as a red dashed line. C \nFlesch-Kincaid Grade Level for Baseline and CoPilot Steps 1–3, with the AMA’s recommended sixth-grade reading level shown as a red dashed line\nPage 8 of 9Romoff et al. Journal of Orthopaedic Surgery and Research          (2025) 20:531 \nFuture research should focus on validating these find -\nings through qualitative studies, including user testing \namong patients with varying literacy levels and direct \npatient input to assess comprehension. Systematic efforts \nare needed to refine LLMs and ensure patient education \nmaterials are accessible and effective for all populations.\nAcknowledgements\nNot applicable.\nAuthors’ contributions\nM.R. performed data extraction and analysis, prepared figures and tables, and \nwrote the manuscript. M.B. performed data extraction and edited the manu-\nscript. M.K.P . performed LLM accuracy assessment and edited the manuscript. \nS.Z.H.edited the manuscript and supervised the project. M.S.K conceived the \nproject, performed LLM accuracy assessment, and wrote the manuscript.\nFunding\nThe authors declare there was no funding for this study.\nData availability\nNo datasets were generated or analysed during the current study.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nReceived: 13 March 2025   Accepted: 22 May 2025\nReferences\n 1. Entwistle VA, Carter SM, Cribb A, McCaffery K. Supporting patient \nautonomy: the importance of clinician-patient relationships. J Gen Intern \nMed. 2010;25(7):741–5. https:// doi. org/ 10. 1007/ s11606- 010- 1292-2.\n 2. Kukla R. Conscientious autonomy: displacing decisions in health care. \nHastings Cent Rep. 2005;35(2):34–44.\n 3. Beach MC, Inui T; Relationship-Centered Care Research Network. \nRelationship-centered care. A constructive reframing. J Gen Intern Med. \n2006;21 Suppl 1(Suppl 1):S3-S8. https:// doi. org/ 10. 1111/j. 1525- 1497. 2006. \n00302.x. \n 4. Walker RL. Medical ethics needs a new view of autonomy. J Med Philos. \n2008;33(6):594–608. https:// doi. org/ 10. 1093/ jmp/ jhn033.\nFig. 5 Performance of ChatGPT in improving readability and reducing word count across steps. Mean and standard deviations for readability \nmetrics (Word Count, Flesch-Kincaid Reading Ease, and Flesch-Kincaid Grade Level) of Baseline articles and across Steps 1–3 using ChatGPT. \nSignificance levels are indicated within the figure. Error bars represent standard deviations. A Word Count for Baseline and ChatGPT Steps 1–3. \nB Flesch-Kincaid Reading Ease scores for Baseline and ChatGPT Steps 1–3, with the AMA-recommended reading ease threshold shown as a red \ndashed line. C Flesch-Kincaid Grade Level for Baseline and ChatGPT Steps 1–3, with the AMA’s recommended sixth-grade reading level shown \nas a red dashed line\nTable 4 ANOVA and Post-Hoc Results for Disclaimer Frequency\nEffect/Comparison F-value P-value Mean Difference Adjusted P value Significance\nANOVA: LLM (Row Factor) 40.79 0.0022 N/A N/A N/A\nANOVA: Step (Column Factor) 0.368 0.7131 N/A N/A N/A\nChatGPT vs Gemini N/A N/A −10.00 0.0024 **\nGemini vs CoPilot N/A N/A 8.333 0.0048 **\nChatGPT vs CoPilot N/A N/A −1.667 0.4206 No\nPage 9 of 9\nRomoff et al. Journal of Orthopaedic Surgery and Research          (2025) 20:531 \n \n 5. Davies M, Elwyn G. Advocating mandatory patient “autonomy” in health-\ncare: adverse reactions and side effects. Health Care Anal. 2008;16(4):315–\n28. https:// doi. org/ 10. 1007/ s10728- 007- 0075-3.\n 6. Kelly PA, Haidet P . Physician overestimation of patient literacy: a potential \nsource of health care disparities. Patient Educ Couns. 2007;66(1):119–22. \nhttps:// doi. org/ 10. 1016/j. pec. 2006. 10. 007.\n 7. Keene Woods N, Ali U, Medina M, Reyes J, Chesser AK. Health Literacy, \nHealth Outcomes and Equity: A Trend Analysis Based on a Population \nSurvey. J Prim Care Community Health. 2023;14:21501319231156132. \nhttps:// doi. org/ 10. 1177/ 21501 31923 11561 32.\n 8. Weiss B, Blanchard JS, McGee DL, Hart G, Warren B, Burgoon M, et al. \nIlliteracy among Medicaid recipients and its relationship to health care \ncosts. J Health Care Poor Underserved. 1994;5(2):99–111.\n 9. Weiss BD. Removing Barriers to Better, Safer Care: Health Literacy and \nPatient Safety: Help Patients Understand: Manual for Clinicians (2nd ed). \nChicago, IL. American Medical Association Foundation; 2007\n 10. Daraz L, Morrow AS, Ponce OJ, Farah W, Katabi A, Majzoub A, et al. Read-\nability of online health information: a meta-narrative systematic review. \nAm J Med Qual. 2018;33(5):487–92.\n 11. Badarudeen S, Sabharwal S. Readability of patient education materials \nfrom the American Academy of Orthopaedic Surgeons and Pediatric \nOrthopaedic Society of North America web sites. J Bone Joint Surg Am. \n2008;90(1):199–204. https:// doi. org/ 10. 2106/ JBJS.G. 00347.\n 12. Albright J, de Guzman C, Acebo P , Paiva D, Faulkner M, Swanson J. Read-\nability of patient education materials: implications for clinical practice. \nAppl Nurs Res. 1996;9(3):139–43. https:// doi. org/ 10. 1016/ s0897- 1897(96) \n80254-0.\n 13. Andalib S, Solomon SS, Picton BG, Spina AC, Scolaro JA, Nelson AM. \nSource Characteristics Influence AI-Enabled Orthopaedic Text Sim-\nplification: Recommendations for the Future. JB JS Open Access. \n2025;10(1):e24.00007. Published 2025 Jan 8. https:// doi. org/ 10. 2106/ JBJS. \nOA. 24. 00007. \n 14. Caterson J, Ambler O, Cereceda-Monteoliva N, Horner M, Jones A, \nPoacher AT. Application of generative language models to orthopaedic \npractice. BMJ Open. 2024;14(3):e076484. Published 2024 Mar 14. https:// \ndoi. org/ 10. 1136/ bmjop en- 2023- 076484. \n 15. Kirchner GJ, Kim RY, Weddle JB, Bible JE. Can Artificial Intelligence \nImprove the Readability of Patient Education Materials? Clin Orthop Relat \nRes. 2023;481(11):2260–7. https:// doi. org/ 10. 1097/ CORR. 00000 00000 \n002668.\n 16. Ryu JH, Yi PH. Readability of Spine-Related Patient Education Materi-\nals From Leading Orthopedic Academic Centers. Spine (Phila Pa 1976). \n2016;41(9):E561-E565. https:// doi. org/ 10. 1097/ BRS. 00000 00000 001321. \n 17. Michel C, Dijanic C, Abdelmalek G, et al. Readability assessment of patient \neducational materials for pediatric spinal deformity from top academic \northopedic institutions. Spine Deform. 2022;10(6):1315–21. https:// doi. \norg/ 10. 1007/ s43390- 022- 00545-1.\n 18. Doinn TÓ, Broderick JM, Abdelhalim MM, Quinlan JF. Readability of \nPatient Educational Materials in Hip and Knee Arthroplasty: Has a Decade \nMade a Difference? J Arthroplasty. 2020;35(11):3076–83. https:// doi. org/ \n10. 1016/j. arth. 2020. 05. 076.\n 19. Reaver CN, Pereira DE, Vail EV, et al. Evaluating the performance of \nartificial intelligence for improving readability of online English- and \nSpanish-language orthopaedic patient educational material: challenges \nin bridging the digital divide. J Bone Joint Surg Am. 2025. https:// doi. org/ \n10. 2106/ JBJS. 24. 01078.\n 20. Roster K, Kann RB, Farabi B, Gronbeck C, Brownstone N, Lipner SR. Read-\nability and Health Literacy Scores for ChatGPT-Generated Dermatology \nPublic Education Materials: Cross-Sectional Analysis of Sunscreen and \nMelanoma Questions. JMIR Dermatol. 2024;7:e50163. Published 2024 Mar \n6. https:// doi. org/ 10. 2196/ 50163. \n 21. Dihan QA, Brown AD, Chauhan MZ, et al. Leveraging large language \nmodels to improve patient education on dry eye disease. Eye (Lond). \n2025;39(6):1115–22. https:// doi. org/ 10. 1038/ s41433- 024- 03476-5.\n 22. Gomez-Cabello CA, Borna S, Pressman SM, Haider SA, Forte AJ. Large \nLanguage Models for Intraoperative Decision Support in Plastic Surgery: \nA Comparison between ChatGPT-4 and Gemini. Medicina. 2024;60(6):957. \nhttps:// doi. org/ 10. 3390/ medic ina60 060957.\n 23. Armitage RC. Implications of Large Language Models for Clinical Practice: \nEthical Analysis Through the Principlism Framework. J Eval Clin Pract. \n2025;31(1):e14250. https:// doi. org/ 10. 1111/ jep. 14250.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations."
}