{
  "title": "Learning Long- and Short-Term User Literal-Preference with Multimodal Hierarchical Transformer Network for Personalized Image Caption",
  "url": "https://openalex.org/W2998409107",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5100756628",
      "name": "Wei Zhang",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A5103494094",
      "name": "Yue Ying",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A5008156638",
      "name": "Pan Lu",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A5046703129",
      "name": "Hongyuan Zha",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2737766105",
    "https://openalex.org/W1897761818",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W2115752676",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W1969616664",
    "https://openalex.org/W2149172860",
    "https://openalex.org/W6698480918",
    "https://openalex.org/W2913538332",
    "https://openalex.org/W6760982045",
    "https://openalex.org/W6750815538",
    "https://openalex.org/W6607333740",
    "https://openalex.org/W2607579284",
    "https://openalex.org/W2797029597",
    "https://openalex.org/W2176263492",
    "https://openalex.org/W2560313346",
    "https://openalex.org/W6755592957",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W6639657675",
    "https://openalex.org/W2790951685",
    "https://openalex.org/W2941381173",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2141200610",
    "https://openalex.org/W2951385221",
    "https://openalex.org/W2579732515",
    "https://openalex.org/W2740167620",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2066134726",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W2963138277",
    "https://openalex.org/W2739992143",
    "https://openalex.org/W3106439889",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2923366293",
    "https://openalex.org/W2982260276",
    "https://openalex.org/W179875071"
  ],
  "abstract": "Personalized image caption, a natural extension of the standard image caption task, requires to generate brief image descriptions tailored for users' writing style and traits, and is more practical to meet users' real demands. Only a few recent studies shed light on this crucial task and learn static user representations to capture their long-term literal-preference. However, it is insufficient to achieve satisfactory performance due to the intrinsic existence of not only long-term user literal-preference, but also short-term literal-preference which is associated with users' recent states. To bridge this gap, we develop a novel multimodal hierarchical transformer network (MHTN) for personalized image caption in this paper. It learns short-term user literal-preference based on users' recent captions through a short-term user encoder at the low level. And at the high level, the multimodal encoder integrates target image representations with short-term literal-preference, as well as long-term literal-preference learned from user IDs. These two encoders enjoy the advantages of the powerful transformer networks. Extensive experiments on two real datasets show the effectiveness of considering two types of user literal-preference simultaneously and better performance over the state-of-the-art models.",
  "full_text": "The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-20)\nLearning Long- and Short-Term User Literal-Preference with\nMultimodal Hierarchical Transformer Network for Personalized Image Caption\nWei Zhang,1,2∗ Yue Ying,1 Pan Lu,3 Hongyuan Zha4\n1School of Computer Science and Technology, East China Normal University\n2Key Laboratory of Artiﬁcial Intelligence, Ministry of Education, Shanghai\n3Departments of Statistics and Computer Science, University of California, Los Angeles\n4School of Computational Science and Engineering, Georgia Institute of Technology\n{zhangwei.thu2011, yingyue2011, lupantech}@gmail.com, zha@cc.gatech.edu\nAbstract\nPersonalized image caption, a natural extension of the stan-\ndard image caption task, requires to generate brief image de-\nscriptions tailored for users’ writing style and traits, and is\nmore practical to meet users’ real demands. Only a few re-\ncent studies shed light on this crucial task and learn static user\nrepresentations to capture their long-term literal-preference.\nHowever, it is insufﬁcient to achieve satisfactory perfor-\nmance due to the intrinsic existence of not only long-term\nuser literal-preference, but also short-term literal-preference\nwhich is associated with users’ recent states. To bridge this\ngap, we develop a novel multimodal hierarchical transformer\nnetwork (MHTN) for personalized image caption in this pa-\nper. It learns short-term user literal-preference based on users’\nrecent captions through a short-term user encoder at the low\nlevel. And at the high level, the multimodal encoder inte-\ngrates target image representations with short-term literal-\npreference, as well as long-term literal-preference learned\nfrom user IDs. These two encoders enjoy the advantages of\nthe powerful transformer networks. Extensive experiments on\ntwo real datasets show the effectiveness of considering two\ntypes of user literal-preference simultaneously and better per-\nformance over the state-of-the-art models.\nIntroduction\nInspired by the success of learning multi-modal representa-\ntions in recent years, image caption (Karpathy and Li 2015;\nXu et al. 2015) has become a hotspot for scientiﬁc and in-\ndustrial exploration, aiming at generating natural language\ndescriptions for target images. It ﬁnds a wide range of appli-\ncations such as the reduction of heavy manual cost of writ-\ning descriptions for tens of thousands of images and the pro-\nmotion of visual understanding for machines. Typically, the\npipeline for this task involves the following two most fun-\ndamental components: a visual understanding module (e.g.,\nconvolutional neural network (Krizhevsky, Sutskever, and\nHinton 2012)) and a language-oriented decoder (e.g., recur-\nrent neural network (RNN) (Mikolov et al. 2010)).\nDespite the remarkable progress in the traditional image\ncaption task, there is an intrinsic limitation that the gener-\n∗Corresponding author.\nCopyright c⃝ 2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nUser 1\nMultimodal hierarchical \ntransformer\nStarting the day with some \nfruit.\nUser 2\nSeriously strawberries are the \nbest fruit ever.\nU\nU\nU\n 2\nRecent Captions Recent Captions\nCNN Extractor CNN Extractor\nGenerated Captions Generated Captions \nMultimodal hierarchical \ntransformer\nFigure 1: The sketch of personalized image caption with\nmultimodal hierarchical transformer and users’ recent cap-\ntions. The two real examples are gotten from Instagram.\nated captions are not tailored for individual users. In other\nwords, through the above pipeline, the generated caption of\nthe same image keeps always the same for different users\nwho would like to manually write the captions or mark their\nlives with photo annotation. Actually, each user has its own\nliteral-preference depending on different writing styles and\nuser states. For example, as shown in Figure 1, the two im-\nages are much the same, with strawberries in a plate. How-\never, the captions provided by User 1 and User 2 are appar-\nently different, for User 1 has an objective statement of his\nbreakfast, while user 2 expresses his love to strawberries.\nAs such, it is more practical to conduct personalized image\ncaption to meet users’ real demands.\nRegarding to this task, only a few pioneering studies in-\nvestigate the impact of user literal-preference in generating\neffective personalized captions (Park, Kim, and Kim 2017;\nWang et al. 2018; Long, Y ang, and Xu 2019; Shuster et al.\n2019; Park, Kim, and Kim 2019). They rely on users’ ac-\ntive vocabularies and self-descriptions (e.g., tags), as well as\ntheir unique IDs to learn latent user representations. They\nare further integrated with visual representations to generate\nﬁnal captions. Since the user representations are associated\n9571\nwith each user’s static characteristics, they are deemed to be\nable to capture long-term user literal-preference, resulting in\nimprovements over traditional models without considering\nuser personality.\nHowever, we argue that only using the long-term repre-\nsentation is insufﬁcient to achieve satisfactory performance\ndue to the intrinsic existence of both long- and short-term\nuser literal-preference (see Table 1 and Figure 2 for veriﬁ-\ncation). On the one hand, long-term literal-preference com-\nmonly reﬂects a user’s personal writing style and its active\nvocabulary. On the other hand, it is intuitive that a user’s\nrecent state will impact his short-term literal-preference,\nwhich in turn affects the image caption to be given. Taking\na real example from our datasets for illustration. A user ﬁrst\nposted an image with the caption “wedding time good luck”\nand several hours later, he delivered another image with the\ncaption “wedding breakfast”. It is obvious that the second\nimage caption depends on the ﬁrst caption due to the user’s\nspeciﬁc state. As a result, it is promising to consider the two\ntypes of literal-preference into a uniﬁed model.\nTo this end, we develop a novel multimodal hierarchi-\ncal transformer network (MHTN) for personalized image\ncaption (see Figure 1). It is partially inspired by the pow-\nerful transformer network (V aswani et al. 2017) which can\nmodel the complex dependencies among different elements\nand acquire contextualized representations for each of them.\nIn particular, we ﬁrst learn to encode users’ recent captions\nthrough a short-term user encoder at the low level of MHTN,\nfollowed by a user-guided attention to obtain the short-term\nrepresentation of user literal-preference. Afterwards, at the\nhigh level, another multimodal encoder is applied to jointly\nmodel user short-term representations and target image rep-\nresentation, as well as long-term user representations of\nliteral-preference encoded by user IDs. The contextualized\nmultimodal representations are ﬁnally utilized to generate\ntarget image captions through a transformer decoder. By this\nway, our model augments the original transformer network\nwith the ability to encode short-term literal-preference, as\nwell as to capture the multimodal interactions among user\nID, text, and image in the task.\nWe summarize the contributions of this paper as follows:\n(1) To our best knowledge, we are the ﬁrst to address\nthe joint learning of both long- and short-term user literal-\npreference in the personalized image caption task.\n(2) We devise a novel multimodal hierarchical trans-\nformer network to encode the two types of literal-preference,\nas well as to combine target image representation.\n(3) We conduct extensive experiments on two publicly\navailable datasets, demonstrating our MHTN achieves the\nbest performance in image caption and the beneﬁt of learn-\ning the two types of literal-preference.\nRelated Work\nIn this section, we brieﬂy review the literature from the fol-\nlowing two aspects, image caption and personalized content\ngeneration.\nImage caption. Image caption has been a long-standing\ntask which involves both textual and visual modalities, thus\nattracting researchers from both natural language process-\ning and computer vision communities. Some previous stud-\nies (Jia, Salzmann, and Darrell 2011; Kuznetsova et al.\n2012) formulate image caption as a retrieval task by search-\ning similar images in the database and their corresponding\ncaptions are taken as the captions of query images. Another\nline of researches (Farhadi et al. 2010; Kulkarni et al. 2011;\nLu et al. 2018) focuses on utilizing basic templates to ﬁll the\nwords relevant to the images. Recent studies have shown that\ndeep neural networks with an encoder-decoder framework\nare effective and ﬂexible in image caption task (Karpathy\nand Li 2015; Xu et al. 2015), which is motivated by the suc-\ncess in machine translation (Cho et al. 2014). In addition, to\novercome the exposure bias (Ranzato et al. 2016) suffered\nin the decoding stage, techniques like reinforcement learn-\ning have been leveraged (Rennie et al. 2017).\nDespite much progress in general image caption, person-\nalized image caption, which is more practical to meet users’\nreal demands, did not received attention until the recent\nseveral years. The pioneering studies (Park, Kim, and Kim\n2017; 2019) address the personalized image caption task by\nincorporating each user’s active vocabularies into memory\nnetworks to capture their writing styles. Since users might\nbe associated with self-annotated tags, (Wang et al. 2018)\nregards these tags as the reﬂection of users’ preference to\ncaptions. (Shuster et al. 2019) speciﬁes 215 different per-\nsonality traits to characterize each user and makes the cap-\ntion generation dependent on them. However, descriptions\nof users, including user tags and personality traits, might\nnot always exist in every scenario. An alternative is to learn\nuser representations based on user IDs to denote user la-\ntent preference (Long, Y ang, and Xu 2019). However, all\nof the above approaches only learn user static representa-\ntions to capture the long-term literal-preference, motivateing\nthis work to simultaneously consider both long- and short-\nterm literal-preference which is learned based on users’ re-\ncent captions and thus is dynamic over time.\nPersonalized content generation. In the era of user-\ngenerated content, automatically generating personalized\ncontent has incurred great interest and gotten a thriving de-\nvelopment. The researches (Li et al. 2017; 2019) couple the\ntwo tasks of personalized rating score prediction (Zhang et\nal. 2016) and tip generation to beneﬁt each other. (Zhou et\nal. 2017) generates reviews given user and item factors, as\nwell as sentiment polarity. (Li et al. 2016) also learns from\nuser IDs to incorporate personalization into dialogue gen-\neration. (Zeng et al. 2019) utilizes user descriptions such\nas age and gender to generate social media comments. In\naddition to the above text generation which commonly has\na similar encoder-decoder framework (Sutskever, Vinyals,\nand Le 2014), (Lin et al. 2019) investigate the personalized\nfashion generation by generating images through a decon-\nvolutional neural network (Zeiler, Taylor, and Fergus 2011).\n(Wang, Zhang, and He 2019) synthesizes continuous states\nand medication dosages of patients with generative adversar-\nial networks. Although the above studies share some spirits\nwith personalized image caption, their problem settings are\nnot exactly the same. Moreover, the short-term user prefer-\nence is overlooked to some extent by these studies as well.\n9572\nPreliminaries\nWe now give the basic notations and formulation of the per-\nsonalized image caption problem, followed by a real data\nanalysis to verify the motivation of considering both long-\nand short-term user literal-preference.\nProblem Formulation\nAssume we have a set of image-caption-user tuples (posts),\ni.e., D = {(I\ni,Ci,Ui)}M\ni=1, where M is the total size of\nthe set. Ii is the raw pixel input of the i-th image. Ci is\nthe caption of the image which contains a list of one-hot\nencoding of words from a predeﬁned vocabulary V, i.e.,\nC\ni = {wi\n1,··· ,wi\nLi } where Li is the length of the caption.\nUi consists of two parts, i.e.,Ui = {ui,CU\ni }, whereui is the\none-hot encoding of the corresponding user ID andCU\ni cov-\ners one or more of the user’s recently posted captions, which\nis utilized for modeling short-term user literal-preference.\nGiven the above formulations, the goal of personalized\nimage caption is to learn a model:f(I\n∗,U∗) → C∗, which\ncan generate a caption for any given target image (*) with a\nspeciﬁed user. In what follows, we empirically demonstrate\nthe existence of both long- and short-term literal-preference.\nData Veriﬁcation of Long- and Short-term\nLiteral-preference\nWe have two real datasets (Park, Kim, and Kim 2019) which\ncome from Instagram and Flickr, respectively. They are\nnamed as Instagram and YFCC100M for short.\nWe ﬁrst show the existence of user long-term literal-\npreference by comparing the text similarities of captions be-\nlonging to the same user and captions of different users.\nIn particular, each caption is represented by a commonly\nadopted term frequency−inverse document frequency (TF-\nIDF) based vector. Given this, we deﬁneintra-user caption\nsimilarity as the average cosine similarity of the TF-IDF\nbased vectors for a single user, andinter-user caption simi-\nlarity as the average cosine similarity for different users. As\nshown in Table 1, the degrees of intra-user caption similarity\nare obviously greater than those of inter-user caption simi-\nlarity. Since the above similarity calculation covers a long\ntime interval, the comparison shows that each user has its\nown long-term literal preference.\nTable 1: Caption similarity analysis\nUser-intra caption similarity User-inter caption similarity\nInstagram 0.0225 0.0086\nYFCC100M 0.0450 0.0055\nIn each user caption set, we sort the captions in a chrono-\nlogical order and further calculate the average caption sim-\nilarity w.r.t. the number of position interval between two\ncaptions. This is an in-depth analysis of caption similarity\nby considering the temporal information in similarity com-\nputation. The results in Figure 1 depict an interesting phe-\nnomenon that as the position interval gets larger, the cap-\ntion similarity becomes smaller, with a dramatic decline in\nthe ﬁrst several position intervals. This consistent observa-\ntion on the two datasets indicates that even for the same\nFigure 2: Caption similarity w.r.t. different number of inter-\nvals.\nuser, the captions have an intrinsic regularity of similarity\nchange. Consequently, we can draw a conclusion that the\ncurrent caption of a user is more relevant to his recent cap-\ntions, demonstrating the existence of short-term user literal-\npreference.\nThe above analysis motivates our study of incorporating\nlong- and short-term literal-preference into personalized im-\nage caption. Speciﬁcally, user IDs are leveraged to learn\nlong-term user literal-preference and users’ recently gener-\nated captions are employed to encode short-term user literal-\npreference.\nProposed Approach\nWe present multimodal hierarchical transformer network to\nconsolidate the textual and visual modalities, as well as the\nlong- and short-term user literal-preference. The model con-\nsists of an input representation module, a hierarchical trans-\nformer encoder, and a transformer decoder. The input rep-\nresentation module involves the encoding of target images,\nwords in users’ recent captions, and user IDs. Transformer\nencoder hierarchically encodes short-term literal preference\nand multimodal representations. The transformer decoder is\nemployed for generating captions as usual. In what follows,\nwe take the image-caption-user tuple(I\ni,Ci,Ui) as an ex-\nample to illustrate the details of our approach.\nInput Representation\nImage feature extraction We adopt 101-layer\nResNet (He et al. 2016) pretrained on the ImageNet\ndataset as our feature extractor to obtain image feature as\nfollows:\ni\ni = WI CNN(Ii), (1)\nwhere CNN returns the pool5 feature of RestNet, follow-\ning (Park, Kim, and Kim 2017).WI is used to convert the\noutput to the multimodal embedding space, with the dimen-\nsion K = 512.\n9573\n[SEP]\n૚ …૚૛ …૛ …\nCaption 1 Caption 2\nࢋ࢜ࢇࢎࢋ\nࢃ… ࢋ[ࡼࡱࡿ]\nࢃࢊ࢕࢕ࢍࢋ \nࢃ… ࢋ[࢖ࢋ࢙]\nࢃ… \nࡿࡻࡼࢋ… ࡿࡻࡼࢋࡿࡻࡼࢋ …ࡿࡻࡼࢋ …\nInput\nTemporal\nemb.\nWord emb.\nPositional \nemb.\n…have good … [SEP] ...\nFigure 3: The input embeddings of short-term user encoder.\nHybrid Word Embedding As shown in Figure 3, we em-\nploy three types of embeddings to represent each wordwj\nin CU\ni , wherein the ﬁrst two types are commonly used. To\nbe speciﬁc, the ﬁrst type is standard word embedding ob-\ntained through a look-up operation, i.e., e\nW\nj = EW wj,\nwhere EW is an embedding matrix. The second type is the\npositional encoding based on sine and cosine functions pro-\nposed in (V aswani et al. 2017), which we denote ase\nPOS\nj\ncorrespondingly.\nSince the words might come from different captions\nwhich were posted at different time, we propose temporal\nembedding to characterize the time interval. The intuition\nbehind temporal embeddings is hoping to learn to concen-\ntrate more on the captions which were posted more recently.\nIn particular, we empirically set a time interval threshold set\nas T = {10min,30min,2h,6h,1d,3d,6d,10d,1month,\n3month,+∞}, wherein each threshold is associated with a\ntemporal embeddinge\nT to be learned. A speciﬁc time inter-\nval is represented by the closest threshold larger than it. We\nhave also tried other similar settings for the threshold set and\nfound the results are close.\nFinally, for a given word, its input representation is con-\nstructed by summing the above three types of embeddings,\ndenoted asˆe\nW . And all the word representations inCU\ni com-\npose an input embedding sequenceˆEW\ni ∈ RK×|CU\ni |, which\nis later fed into short-term user encoder. It is worth noting\nthat for transformer decoder, the input word representation\nmatrix ¯E\nW decoded in previous steps only involves the ﬁrst\ntwo types of embeddings.\nLong-term user representation Users who post their\ncaptions online are typically associated with user IDs. We\naim to leverage the IDs to learn static user representations to\ncapture their long-term literal-preference in image caption.\nWe deﬁne a user embedding matrixE\nU . And the long-term\nuser representation is then obtained through a look-up oper-\nation as well, i.e.,eUL\ni = EU ui.\nHierarchical Transformer Encoder\nHierarchical transformer encoder is composed of a low-level\nshort-term user encoder and a high-level multimodal en-\ncoder.\nShort-term user encoder We ﬁrst adopt transformer en-\ncoder to model the dependencies between different words\nThe beautiful \nbride xx\nLong-term User \nRepresentation\nHybrid word \nEmbedding\nSelf Attention\nK        V       Q\nAdd & Norm\nAdd & Norm\nFeed Forward \nShort-term User EncoderUser Guided \nAttention \nMultimodal Attention\nK           V          Q\nV   \n   \nQ\nAdd & Norm\nAdd & Norm\nFeed Forward \nRecent posts\nMultimodal Attention\nK           V          Q\nAdd & Norm\nAdd & Norm\nFeed Forward \nFor\nMultimodal Encoder\nMasked Self \nAttention\nK         V       Q\n  \nV   \n Q\nK   \nAdd & Norm\nOutput \nEmbedding\nutp\nQ\nV   \nK   \nn\nLinear & Softmax\n&\n S\nThe beautiful \nbride xx\nwedding time \ngood luck jay n lou\nx 3 x\nCONCAT\nAT\nNC\nUser id\nResNet\nCaption generation \nGood day, wedding breakfast jay\nCON\nrGGu\nSelf Att\nK        V\nAdd &\ned \nnnn  \nrid ord\nFigure 4: The architecture of multimodal hierarchical trans-\nformer network.\nin the user’s recent captions, which is beneﬁcial for obtain-\ning contextualized word representations. Speciﬁcally, multi-\nhead attention (V aswani et al. 2017) is used, where each\nheader associates all positions in the word sequence with the\nweighted combination of input word embeddings. Formally,\nit is deﬁned as follows:\nHm(ˆEW\ni ) = softmax\n((WQ\nm ˆEW\ni )⊤ (WK\nm ˆEW\ni )√\nK/M\n)\n·(WV\nm ˆEW\ni )⊤ ,\n(2)\nwhere M is number of headers andm ∈{ 1,··· ,M}. WQ\nm,\nWK\nm, and WV\nm ( ∈ RK/M×K) correspond to the trainable\nparameters of query, key, and value, respectively. The repre-\nsentations from each header is fused to form a multi-header\nbased representation as follows:\nMH(ˆE\nW\ni )=M L P ( [ H1(ˆEW\ni );··· ;HM(ˆEW\ni )]⊤ ), (3)\nwhere MLP denotes a multi-layer perceptron for linear\ntransformation and [;] indicates a row-wise concatena-\ntion. Furthermore, residual connection, layer norm (LN),\nand MLP are combined together to get the contextualized\nword embeddings as follows:\nˆE\nW\n1i =L N\n(ˆEW\ni +MLP(LN( ˆEW\ni +MH( ˆEW\ni )))\n)\n, (4)\nwhere ˆEW\n1i denotes the output of the ﬁrst transformer en-\ncoder. In practice, the transformer encoder could be stacked\nL times and ﬁnally the output word embedding matrix is\nrepresented as ˆE\nW\nLi.\nTo learn short-term user representation, we introduce a\nsimple user-guided attention mechanism overˆEW\nLi. That is,\nwe leverage user long-term representation as a query to\nattend each word embedding ˆEW\nLij (j ∈{ 1,··· ,|CU\ni |}).\n9574\nSpeciﬁcally, the attention weight for each word is given as:\nαj = softmax\n(\nω⊤ tanh(WAT T\nU eUL\ni\n+WAT T\nW ˆEW\nLij +b)\n)\n,\n(5)\nwhere WAT T\nU and WAT T\nW are matrix parameters, whileω\nand b are vector parameters. After that, the short-term user\nrepresentation is encoded as,\neUS\ni =\n|CU\ni |∑\nj=1\nαj ˆEW\nLij. (6)\nMultimodal encoder The multimodal encoder takes user\nlong- and short-term representations, as well as image rep-\nresentation as input, and adopts another transformer encoder\nto model their inter-modal interaction. A multimodal em-\nbedding matrix is ﬁrst formed based on the column-wise\nconcatenation E\nM\ni =[ eUL\ni ,eUS\ni ,ii]. In a similar way as de-\nscribed in Equation 2, 3, and 4, we obtain a contextualized\nmultimodal embedding matrix, i.e.,E\nM\nLi, where the encoder\nis also stackedL times, without loss of generality.\nCaption Generation and Training\nIn caption generation, the transformer decoder develops a\nmasked self-attention operation to ensure the word genera-\ntion for position j to be only inﬂuenced by the generated\nwords before this position. In multimodal attention of the\ndecoder, the obtained multimodal embedding matrix is em-\nployed as key and value, and the output word embedding\nis regarded as query. This ensures that the word generation\nis directly affected by both long- and short-term user rep-\nresentations, as well as target image representations. The\ntraining target of our model is to maximize the likelihood\nof generating true descriptions for images in the datasetD.\nWe leave the incorporation of other training methods such\nas reinforcement learning as future work.\nExperiments\nExperimental Setup\nDatasets As aforementioned, we have Instagram and\nYFCC100M based on the InstaPIC-1.1M dataset and the\nYFCC100M benchmark dataset respectively (Park, Kim,\nand Kim 2019). Since the original InstaPIC-1.1M dataset\nhas not stored the time information of each post, we crawl\nthe raw data of the posts from the website via each user name\nappearing in the dataset. For both datasets, we follow (Park,\nKim, and Kim 2019) to remove duplicate posts and lengthy\ncaptions. To prevent models from peeking users’ future lit-\neral preference, we sort all posts of each user in a chronolog-\nical order. We split the two datasets by taking the ﬁrst 85%\nposts as training sets, then 5% of posts as validation sets, and\nthe last 10% of posts as test sets. The main statistics of the\ntwo datasets are summarized in Table 2.\nImplementation details We tune the hyper-parameters\nof all adopted models by their performance on validation\ndatasets for a fair comparison. To train the MHTN model,\nwe use the Adam optimizer withα =0 .9,β =0 .999,ϵ =\n1 × 10\n−8, and the batch size to be 100. We set the dropout\nTable 2: Statistics of the datasets.\nData Post User Time Span V ocab. Size\nInstagram 363,656 2,888 2010-2016 40,000\nYFCC100M 353,259 5,868 2004-2014 40,000\nratio to 0.1 for intermediate layers. We also apply label\nsmoothing (Szegedy et al. 2016) with factor of 0.1 to our\ntraining procedure. Gradient clipping is used with the range\n[−0.1,0.1]. The hyper-parameters of transformer areN =\n6, M=8 , K = 512. The default number of recent posts\nconsidered by our model is set to 5.\nFollowing (Park, Kim, and Kim 2017), we report the cap-\ntion generation results by decoding each position with the\nmost likely word for all approaches. Beam search with dif-\nferent small sizes are also conducted and similar conclu-\nsions w.r.t. performance comparison can be drawn. Due to\nthe space limitation, we do not report these results.\nBaselines The involved baselines are as follows:\n1NN-IM, 1NN-Usr, and 1NN-UsrIM (Park, Kim, and\nKim 2017):They are retrieval based baselines by taking the\ncaptions of the nearest training image and nearest user as\ngenerated captions.\nShowTell (Vinyals et al. 2015):ShowTell is a pioneering\nencoder-decoder based model for generating captions with\nan RNN decoder.\nShowAttTell (Xu et al. 2015):ShowAttTell incorporates\na visual attention computation to capture the importance of\neach image region in word decoding.\nTransformer (Vaswani et al. 2017):We take the image\nrepresentation by outputing of the last convolutional layer of\n101-layer ResNet with the size196 × 2048, as the input of\ntransformer encoder.\nAttend2u (Park, Kim, and Kim 2017):Attend2u is the\nﬁrst model for personalized image caption by modeling a\nuser’s active vocabulary as its memory context.\nCDPIC (Long, Yang, and Xu 2019):This model utilizes\nuser IDs and takes their frequently used words as context,\nand also adopts an RNN based encoder-decoder framework.\nEICP (Shuster et al. 2019):The one-hot encoding of per-\nsonality is used in EICP with the UPDOWN (Anderson et\nal. 2018) strategy for image caption. To ensure fairness, we\nregard user ID as user personality, and only use top-down at-\ntention since bottom-up attention involves image region box\ndetection which is out the scope of this paper.\nExperimental Results\nModel comparison Table 3 mainly presents the cap-\ntion generation performance on the two adopted datasets\nby MHTN and compared models. The evaluation metrics\ninclude language similarity metrics (BLEU, CIDEr, ME-\nTEOR, and ROUGE-L) and the tailored image caption per-\nformance metric (SPICE). The retrieval based models in\nthe ﬁrst part of the table perform poorly compared with\nother generative models. In the second part where all models\ndo not consider personalization, Transformer outperforms\nShowTell and ShowAttTell, showing its good modeling ca-\npability in image caption. The three baselines in the third\n9575\nTable 3: Evaluation results by our model and compared models on the Instagram and YFCC100M datasets.\nInstagram\nMethods BLEU1 BLEU2 BLEU3 BLEU4 CIDEr METEOR ROUGE-L SPICE Time-TR Time-TE\n1NN-Im 0.026 0.001 0.000 0.000 0.011 0.009 0.026 0.005 ——\n1NN-Usr 0.042 0.008 0.002 0.001 0.021 0.020 0.038 0.004 ——\n1NN-UsrIm 0.037 0.008 0.002 0.001 0.018 0.019 0.034 0.003 ——\nShowTell 0.055 0.016 0.006 0.002 0.045 0.020 0.061 0.009 0.22s 0.15s\nShowAttTell 0.049 0.015 0.005 0.003 0.056 0.021 0.063 0.014 ——\nTransformer 0.060 0.019 0.008 0.004 0.079 0.026 0.070 0.019 0.89s 0.91s\nAttend2u 0.065 0.020 0.008 0.004 0.076 0.026 0.069 0.013 ——\nCDPIC 0.057 0.020 0.009 0.005 0.080 0.024 0.071 0.020 ——\nEICP 0.062 0.023 0.011 0.006 0.094 0.028 0.078 0.022 0.86s 0.88s\nMHTN 0.093 0.036 0.017 0.010 0.125 0.042 0.089 0.025 0.56s 0.58s\nYFCC100M\nMethods BLEU1 BLEU2 BLEU3 BLEU4 CIDEr METEOR ROUGE-L SPICE Time-TR Time-TE\n1NN-Im 0.046 0.014 0.005 0.003 0.057 0.017 0.042 0.002 ——\n1NN-Usr 0.038 0.009 0.003 0.002 0.018 0.012 0.032 0.004 ——\n1NN-UsrIm 0.042 0.010 0.003 0.001 0.016 0.012 0.032 0.004 ——\nShowTell 0.070 0.024 0.010 0.001 0.069 0.021 0.064 0.016 0.22s 0.15s\nShowAttTell 0.079 0.032 0.017 0.011 0.101 0.024 0.078 0.026 ——\nTransformer 0.082 0.036 0.020 0.013 0.138 0.029 0.085 0.032 0.89s 0.91s\nAttend2u 0.076 0.025 0.010 0.004 0.075 0.029 0.083 0.017 ——\nCDPIC 0.101 0.053 0.034 0.024 0.205 0.037 0.100 0.042 ——\nEICP 0.117 0.066 0.044 0.032 0.263 0.044 0.112 0.047 0.86s 0.89s\nMHTN 0.145 0.091 0.066 0.053 0.408 0.063 0.133 0.059 0.55s 0.58s\npart behave better in most of the metrics, indicating the ne-\ncessity of considering personality in achieving good cap-\ntion performance. By comparing our model MHTN with the\nother baselines, we can see consistent and signiﬁcant im-\nprovements. Speciﬁcally, the improvements of MHTN over\nstate-of-the-art approach EICP are statistically signiﬁcant,\nfrom 0.094 to 0.125 by CIDEr on Instagram and from 0.047\nto 0.059 by SPICE on YFCC100M. Since EICP also learns\nfrom user IDs and uses advanced attention mechanism for\nencoder-decoder modeling, the comparison with EICP re-\nveals our improvements are attributed to the advantage of\nencoding short-term user literal-preference and the power-\nful hierarchical multimodal transformer architecture.\nIn addition, the right region of Table 3 shows the aver-\nage running time of MHTN and several other baselines w.r.t.\ntraining (Time-TR) and testing (Time-TE) on each batch\ndata. The number of training epochs needed to converge is\nsimilar for them. We ﬁnd MHTN runs faster than EICP and\nTransformer but slower than ShowTell. In total, the training\nof MHTN can be completed in less than 18 hours with only\n1 GPU, which is feasible for these image caption datasets.\nAblation study We conduct ablation experiments to fur-\nther verify the contribution of individual component design\nin our model. In particular, we consider the following vari-\nants: 1) “w/o Temporal Emb.” removes temporal embedding\nfrom the hybrid input embeddings of short-term user en-\ncoder; 2) “w/o Image” does not input target image; 3) “w/o\nLong-term User Rep.” removes the long-term user represen-\ntation; 4) “w/o Short-term User Rep.” removes short-term\nuser encoder; and 5) “RNN+Transformer” replaces trans-\nformer based short-term user encoder with time-aware RNN\nbased encoder which also considers temporal embedding to\nget the short-term user representation.\nTable 4 shows the results of MHTN and its variants. From\na whole perspective, temporal embedding makes less con-\ntribution than other components, but still make a positive\ncontribution on the two datasets. Moreover, the performance\ndegradation of “w/o Image” reveals the visual content is\nan indispensable component in personalized image caption,\nwhich conﬁrms to intuition. By further comparing MHTN\nwith “w/o Long-term User Rep.” and “w/o Short-term User\nRep.”, we can see the performance goes through signiﬁcant\nimprovements, showing the indeed positive effects brought\nby both long- and short-term user representations. Finally,\nwe compare our model with “RNN+Transformer” and the\nbetter results show the beneﬁt of proposing to use trans-\nformer as short-term user encoder.\nQualitative analysis Figure 5 shows some selected im-\nages and their captions in two parts. The left part, out of\nthe dotted box, compares the captions generated by different\nmethods. It is undeniable that image caption is a hard task\nbecause the ground-truth captions from different users in-\nvolve diverse perspectives, literal-preference, and even some\nnamed entities. For example, the ﬁrst image in the second\ncolumn contains a location entity “washington” in its cap-\ntion. However, the generated captions by the selected models\nare relevant to the images to some extent. More importantly,\nthe colored words captured by our model provide some de-\ntails about the images and seem to be related to users’ recent\nstate or literal preference. In addition, the captions from our\nmodel apparently have a richer vocabulary than other mod-\nels, making the caption more descriptive. In the part of the\ndotted box, we can ﬁnd: (i) different query users indeed gen-\nerate captions from different perspectives for the same im-\nages; and (ii) considering users’ recent captions beneﬁt cap-\nturing more details might relevant to the images.\n9576\nTable 4: Ablation study of MHTN on the two datasets.\nInstagram\nMethods BLEU1 BLEU2 BLEU3 BLEU4 CIDEr METEOR ROUGE-L SPICE\nMHTN 0.093 0.036 0.017 0.01 0.125 0.042 0.089 0.025\nw/o Temporal Emb. 0.089 0.034 0.015 0.008 0.119 0.039 0.084 0.022\nw/o Image 0.078 0.028 0.016 0.007 0.087 0.036 0.079 0.016\nw/o Long-term User Rep. 0.078 0.028 0.012 0.007 0.097 0.034 0.080 0.022\nw/o Short-term User Rep. 0.080 0.030 0.014 0.007 0.107 0.036 0.082 0.022\nRNN+Transformer 0.089 0.032 0.015 0.008 0.117 0.038 0.088 0.025\nYFCC100M\nMethods BLEU1 BLEU2 BLEU3 BLEU4 CIDEr METEOR ROUGE-L SPICE\nMHTN 0.145 0.091 0.066 0.053 0.408 0.063 0.133 0.059\nw/o Temporal Emb. 0.138 0.085 0.060 0.046 0.361 0.060 0.126 0.055\nw/o Image 0.130 0.082 0.060 0.048 0.350 0.053 0.118 0.045\nw/o Long-term User Rep. 0.126 0.070 0.050 0.038 0.297 0.050 0.119 0.049\nw/o Short-term User Rep. 0.118 0.072 0.052 0.040 0.332 0.051 0.113 0.051\nRNN+Transformer 0.141 0.081 0.058 0.046 0.364 0.058 0.130 0.058\n(CDPIC) a panda in the shade \n(ShowAttTell) a squirrel in \nthe garden\n(Ours) red panda in san diego \nzoo\n(GT) stayans red panda \n(EICP) a little tree\n(Transformer) a little bit of a tree\n(Ours) fallen trees at the park\n(GT) at the edge of the campsite\n(CDPIC) the path to the UNK\n(ShowTell) the view from the \ntop\n(Ours) the view from the \nbridge\n(GT) on the moors above \nhomfirth\nade \n (\nC(EICP) sunset at the lake\n(ShowTell) sunset at the \nbeach\n(Ours) sunset at lake \nwashington\n(GT) sunset at washington \npark\n(\nEIC\nNK (EICP) a budapest building in \nBudapest\n(Transformer) the national \nmuseum of canada\n(Ours) the church in budapest\n(GT) a peculiar building with \nnice shapes in budapest\n(EICP) breakfast of the day\n(Transformer) breakfast of \nchampions\n(Ours) homemade french toast \nwith fruit and strawberry \npancakes #breakfast\n(GT) starting my weekend right \nwith blueberry strawberry \npancakes #brunch #breakfast \n#food #nutritious #weekend\n(EICP) blueberry pancakes \nwith chocolate and chocolate\n(Transformer) breakfast with \nberries and fresh blueberries\n(Ours) fresh blueberries and \ngreek yogurt\n(GT) blueberries greek yogurt \nand bran buds great way to \nstart the day \n(CDPIC) the road to the \nforest\n(ShowAttTell) the trees \nare the best\n(Ours) a beautiful fall day \nin #puremichigan\n(GT) beautiful day for a \nfall hike \n(CDPIC) the view from the \ntop of the world\n(ShowTell) the view from the \ntop of the hill\n(Ours) the view from the top \nof the hill\n(GT) view of robben island \nfrom on top of table \nmountain\n(EICP)\n breakfast of the day\n (\nEICP)\n blueberryp ancake\n(\n (\nCDPIC)\n the road to the\ns\n (\n (\nCDPIC)\n the view from the\n(\n (EICP) sunset in the sun\n(ShowAttTell) the sunset was a \nbeautiful sunset\n(Ours) fiery sunset over the \nridge mountains\n(GT) fiery sunset over the \nsilhouette of magnificent mt \ntaranaki\n(U1)matsumoto castles and \nin the background\n(܃1)the palace of hoi an\n(U1) i love peonies\n(U2) the roses are blooming\n(U3) flowers from my \ngarden\n(U4) the most beautiful \nflowers ever\n(U1)look at that cute \nlittle guinea pig is so \ncute\n(܃1) look at this little \nguy\n(U1)sunset through the \ngrass\n(U2)sun through the \ntrees\n(U3) sunset at the back\n(U4) sunset with ducks\nFigure 5: Examples from YFCC100M (top) and Instagram (bottom). For each image out of the dotted box, we present its grond\ntruth (GT) caption, accompanied by the ones generated by our model MHTN and some strong baselines. And for each image in\nthe dotted bos, we present captions by different query users, denoted byU1for example. ¯U1corresponds to captions generated\nwithout considering the user’s recent posts, in comparison with the full version of MHTN.\nEffect of the number of recent postsTable 5 shows that,\nas the number of posts increases, “w/o Time-emb.” gains\nbetter performance in terms of CIDEr at ﬁrst and then the\nperformance drops, implying the latest posts have larger\ncontributions on caption generation. By contrast, the results\nof MHTN become better at ﬁrst and remain stable. This is\nbecause temporal embedding could help differentiate the re-\ncent posts from other older posts.\nConclusion\nIn this paper, we develop a novel multimodal hierarchical\ntransformer network to encode both long- and short-term\nuser literal-preference for personalized image caption. The\ngoal is achieved by the low-level user-dependent transformer\nencoder to learn short-term user representations from users’\nrecent posts, and the high-level multimodal transformer en-\ncoder to integrate short-term user representations and long-\n9577\nTable 5: Results of different number of recent posts.\nMethod Length Instagram YFCC100M\nMHTN\n1 0.120 0.386\n2 0.120 0.394\n5 0.125 0.408\n10 0.126 0.407\nw/o Temporal Emb.\n1 0.117 0.356\n2 0.119 0.361\n5 0.113 0.344\n10 0.110 0.346\nterm user representations of user IDs, as well as image repre-\nsentations. We have conducted experiments on two publicly\navailable datasets, showing the superiority of our model and\nvalidating the contributions of its main components.\nAcknowledgments\nThe authors would like to thank the anonymous review-\ners for their valuable suggestions. This work is sup-\nported by National Key Research and Development Program\n(2019YFB2102600), NSFC (61702190), Shanghai Sailing\nProgram (17YF1404500), NSFC-Zhejiang (U1609220), and\nthe foundation of Key Laboratory of Artiﬁcial Intelligence,\nMinistry of Education, P .R. China.\nReferences\nAnderson, P .; He, X.; Buehler, C.; Teney, D.; Johnson, M.; Gould,\nS.; and Zhang, L. 2018. Bottom-up and top-down attention for\nimage captioning and VQA. InCVPR, 6077–6086.\nCho, K.; van Merrienboer, B.; G ¨ulc¸ehre, C¸ .; Bahdanau, D.;\nBougares, F.; Schwenk, H.; and Bengio, Y . 2014. Learning phrase\nrepresentations using RNN encoder-decoder for statistical machine\ntranslation. In EMNLP, 1724–1734.\nFarhadi, A.; Hejrati, S. M. M.; Sadeghi, M. A.; Y oung, P .;\nRashtchian, C.; Hockenmaier, J.; and Forsyth, D. A. 2010. Every\npicture tells a story: Generating sentences from images. InECCV,\n15–29.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learn-\ning for image recognition. InCVPR, 770–778.\nJia, Y .; Salzmann, M.; and Darrell, T. 2011. Learning cross-\nmodality similarity for multinomial data. InICCV, 2407–2414.\nKarpathy, A., and Li, F. 2015. Deep visual-semantic alignments\nfor generating image descriptions. InCVPR, 3128–3137.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Imagenet\nclassiﬁcation with deep convolutional neural networks. In NIPS,\n1106–1114.\nKulkarni, G.; Premraj, V .; Dhar, S.; Li, S.; Choi, Y .; Berg, A. C.;\nand Berg, T. L. 2011. Baby talk: Understanding and generating\nsimple image descriptions. InCVPR, 1601–1608.\nKuznetsova, P .; Ordonez, V .; Berg, A. C.; Berg, T. L.; and Choi, Y .\n2012. Collective generation of natural image descriptions. InACL,\n359–368.\nLi, J.; Galley, M.; Brockett, C.; Spithourakis, G. P .; Gao, J.; and\nDolan, W. B. 2016. A persona-based neural conversation model.\nIn ACL.\nLi, P .; Wang, Z.; Ren, Z.; Bing, L.; and Lam, W. 2017. Neural\nrating regression with abstractive tips generation for recommenda-\ntion. In SIGIR, 345–354.\nLi, P .; Wang, Z.; Bing, L.; and Lam, W. 2019. Persona-aware tips\ngeneration? In TheW ebConf, 1006–1016.\nLin, Y .; Ren, P .; Chen, Z.; Ren, Z.; Ma, J.; and de Rijke, M. 2019.\nImproving outﬁt recommendation with co-supervision of fashion\ngeneration. In TheW ebConf, 1095–1105.\nLong, C.; Y ang, X.; and Xu, C. 2019. Cross-domain personalized\nimage captioning. Multimedia T ools and Applications.\nLu, D.; Whitehead, S.; Huang, L.; Ji, H.; and Chang, S. 2018.\nEntity-aware image caption generation. InEMNLP, 4013–4023.\nMikolov, T.; Karaﬁ´at, M.; Burget, L.; Cernock´y, J.; and Khudanpur,\nS. 2010. Recurrent neural network based language model. In\nINTERSPEECH, 1045–1048.\nPark, C. C.; Kim, B.; and Kim, G. 2017. Attend to you: Person-\nalized image captioning with context sequence memory networks.\nIn CVPR, 6432–6440.\nPark, C. C.; Kim, B.; and Kim, G. 2019. Towards personal-\nized image captioning via multimodal memory networks.TPAMI\n41(4):999–1012.\nRanzato, M.; Chopra, S.; Auli, M.; and Zaremba, W. 2016. Se-\nquence level training with recurrent neural networks. InICLR.\nRennie, S. J.; Marcheret, E.; Mroueh, Y .; Ross, J.; and Goel, V .\n2017. Self-critical sequence training for image captioning. In\nCVPR, 1179–1195.\nShuster, K.; Humeau, S.; Hu, H.; Bordes, A.; and Weston, J. 2019.\nEngaging image captioning via personality. In CVPR, 12516–\n12526.\nSutskever, I.; Vinyals, O.; and Le, Q. V . 2014. Sequence to se-\nquence learning with neural networks. InNIPS, 3104–3112.\nSzegedy, C.; V anhoucke, V .; Ioffe, S.; Shlens, J.; and Wojna, Z.\n2016. Rethinking the inception architecture for computer vision.\nIn CVPR, 2818–2826.\nV aswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is\nall you need. InNIPS, 5998–6008.\nVinyals, O.; Toshev, A.; Bengio, S.; and Erhan, D. 2015. Show and\ntell: A neural image caption generator. InCVPR, 3156–3164.\nWang, L.; Chu, X.; Zhang, W.; Wei, Y .; Sun, W.; and Wu, C. 2018.\nSocial image captioning: Exploring visual attention and user atten-\ntion. Sensors 18(2):646.\nWang, L.; Zhang, W.; and He, X. 2019. Continuous patient-centric\nsequence generation via sequentially coupled adversarial learning.\nIn DASF AA, 36–52.\nXu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A. C.; Salakhutdinov,\nR.; Zemel, R. S.; and Bengio, Y . 2015. Show, attend and tell:\nNeural image caption generation with visual attention. InICML,\n2048–2057.\nZeiler, M. D.; Taylor, G. W.; and Fergus, R. 2011. Adaptive de-\nconvolutional networks for mid and high level feature learning. In\nICCV, 2018–2025.\nZeng, W.; Abuduweili, A.; Li, L.; and Y ang, P . 2019. Automatic\ngeneration of personalized comment based on user proﬁle. InACL,\nStudent Research W orkshop, 229–235.\nZhang, W.; Y uan, Q.; Han, J.; and Wang, J. 2016. Collaborative\nmulti-level embedding learning from reviews for rating prediction.\nIn IJCAI, 2986–2992.\nZhou, M.; Lapata, M.; Wei, F.; Dong, L.; Huang, S.; and Xu, K.\n2017. Learning to generate product reviews from attributes. In\nEACL, 623–632.\n9578",
  "topic": "Literal (mathematical logic)",
  "concepts": [
    {
      "name": "Literal (mathematical logic)",
      "score": 0.8349156975746155
    },
    {
      "name": "Computer science",
      "score": 0.7230108380317688
    },
    {
      "name": "Transformer",
      "score": 0.6538479328155518
    },
    {
      "name": "Preference",
      "score": 0.6455097198486328
    },
    {
      "name": "Term (time)",
      "score": 0.4901941418647766
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45363128185272217
    },
    {
      "name": "Natural language processing",
      "score": 0.4184775948524475
    },
    {
      "name": "Human–computer interaction",
      "score": 0.35894426703453064
    },
    {
      "name": "Engineering",
      "score": 0.12019234895706177
    },
    {
      "name": "Programming language",
      "score": 0.0749427080154419
    },
    {
      "name": "Mathematics",
      "score": 0.06704151630401611
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I66867065",
      "name": "East China Normal University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I161318765",
      "name": "University of California, Los Angeles",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    }
  ],
  "cited_by": 26
}