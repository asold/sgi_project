{
  "title": "Leveraging Multilingual Transformers for Hate Speech Detection",
  "url": "https://openalex.org/W3119860226",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287591288",
      "name": "Roy, Sayar Ghosh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287591289",
      "name": "Narayan, Ujwal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287591287",
      "name": "Raha, Tathagata",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287591290",
      "name": "Abid, Zubair",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2529489090",
      "name": "Varma, Vasudeva",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963291843",
    "https://openalex.org/W80056832",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W1979296086",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W3103061166",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3140418309",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W3100806282",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2922580172",
    "https://openalex.org/W2160685721",
    "https://openalex.org/W2963012544"
  ],
  "abstract": "Detecting and classifying instances of hate in social media text has been a problem of interest in Natural Language Processing in the recent years. Our work leverages state of the art Transformer language models to identify hate speech in a multilingual setting. Capturing the intent of a post or a comment on social media involves careful evaluation of the language style, semantic content and additional pointers such as hashtags and emojis. In this paper, we look at the problem of identifying whether a Twitter post is hateful and offensive or not. We further discriminate the detected toxic content into one of the following three classes: (a) Hate Speech (HATE), (b) Offensive (OFFN) and (c) Profane (PRFN). With a pre-trained multilingual Transformer-based text encoder at the base, we are able to successfully identify and classify hate speech from multiple languages. On the provided testing corpora, we achieve Macro F1 scores of 90.29, 81.87 and 75.40 for English, German and Hindi respectively while performing hate speech detection and of 60.70, 53.28 and 49.74 during fine-grained classification. In our experiments, we show the efficacy of Perspective API features for hate speech classification and the effects of exploiting a multilingual training scheme. A feature selection study is provided to illustrate impacts of specific features upon the architecture's classification head.",
  "full_text": "Leveraging Multilingual Transformers for Hate\nSpeech Detection\nSayar Ghosh Roy, Ujwal Narayan, Tathagata Raha, Zubair Abid and Vasudeva Varma\nInformation Retrieval and Extraction Lab, International Institute of Information Technology, Hyderabad, India\nAbstract\nDetecting and classifying instances of hate in social media text has been a problem of interest in Natural\nLanguage Processing in the recent years. Our work leverages state of the art Transformer language\nmodels to identify hate speech in a multilingual setting. Capturing the intent of a post or a comment on\nsocial media involves careful evaluation of the language style, semantic content and additional pointers\nsuch as hashtags and emojis. In this paper, we look at the problem of identifying whether a Twitter\npost is hateful and offensive or not. We further discriminate the detected toxic content into one of the\nfollowing three classes: (a) Hate Speech (HATE), (b) Offensive (OFFN) and (c) Profane (PRFN). With a\npre-trained multilingual Transformer-based text encoder at the base, we are able to successfully identify\nand classify hate speech from multiple languages. On the provided testing corpora, we achieve Macro\nF1 scores of 90.29, 81.87 and 75.40 for English, German and Hindi respectively while performing hate\nspeech detection and of 60.70, 53.28 and 49.74 during fine-grained classification. In our experiments, we\nshow the efficacy of Perspective API features for hate speech classification and the effects of exploiting\na multilingual training scheme. A feature selection study is provided to illustrate impacts of specific\nfeatures upon the architecture‚Äôs classification head.\nKeywords\nHASOC 2020, Hate Speech Detection, XLM-RoBERTa, Perspective API\n1. Introduction\nWith a rise in the number of posts made on social media, an increase in the amount of toxic\ncontent on the web is witnessed. Measures to detect such instances of toxicity is of paramount\nimportance in today‚Äôs world with regards to keeping the web a safe and healthy environment\nfor all. Detecting hateful and offensive content in typical posts and comments found on the web\nis the first step towards building a system which can flag items with possible adverse effects\nand take steps necessary to handle such behavior.\nIn this paper, we look at the problem of detecting hate speech and offensive remarks within\ntweets. More specifically, we attempt to solve two classification problems. Firstly, we try to\nassign a binary label to a tweet indicating whether it is hateful and offensive (class HOF) or not\nFIRE ‚Äô20, Forum for Information Retrieval Evaluation, December 16‚Äì20, 2020, Hyderabad, India\n/envelope-opensayar.ghosh@research.iiit.ac.in (S. Ghosh Roy); ujwal.narayan@research.iiit.ac.in (U. Narayan);\ntathagata.raha@research.iiit.ac.in (T. Raha); zubair.abid@research.iiit.ac.in (Z. Abid); vv@iiit.ac.in (V. Varma)\n/globehttps://sayarghoshroy.github.io/ (S. Ghosh Roy); https://www.ujwalnarayan.ml/ (U. Narayan);\nhttps://github.com/tathagata-raha/ (T. Raha); https://zubairabid.com/ (Z. Abid); https://irel.iiit.ac.in/vasu/index.html\n(V. Varma)\n/orcid\n¬© 2020 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).\nCEUR\nWorkshop\nProceedings\nhttp://ceur-ws.org\nISSN 1613-0073\nCEUR Workshop Proceedings (CEUR-WS.org)\narXiv:2101.03207v1  [cs.CL]  8 Jan 2021\n(class NOT). Secondly, if the tweet belongs to class HOF, we classify it further into one of the\nfollowing three possible classes: (a) HATE: Contains hate speech, (b) OFFN: Is offensive, and (c)\nPRFN: Contains profanities.\nThe language in use on the web is in a different text style as compared to day-to-day speech,\nformally written articles, and webpages. In order to fully comprehend the social media style of\ntext, a model needs to have knowledge of the pragmatics of emojis and smileys, the specific\ncontext in which certain hashtags are being used, and it should be able to generalize to various\ndomains. Also, social media text is full of acronyms, abbreviated forms of words and phrases,\northographic deviations from standard forms such as dropping of vowels from certain words,\nand contains instances of code mixing.\nThe escalation in derogatory posts on the internet has prompted certain agencies to make\ntoxicity detection modules available for web developers as well as for the general public. A\nnotable work in this regard is Google‚Äôs Perspective API1 which uses machine learning models\nto estimate various metrics such as toxicity, insult, threat, etc., given a span of text as input. We\nstudy the usefulness of these features for hate speech detection tasks in English and German.\nIn recent years, utilizing Transformer-based [1] Language Models pre-trained with certain\nobjectives on vast corpora [2] has been crucial to obtaining good representations of textual\nsemantics. In our work, we leverage the advances in language model pre-training research and\napply the same to the task of hate speech detection. Lately, we have witnessed the growing\npopularity of multilingual language models which can work upon input text in a language\nindependent manner. We hypothesize that such models will be effective on social media texts\nacross a collection of languages and text styles. Our intuition is experimentally verified as we\nare able to obtain respectable results on the provided testing data for the two tasks in question.\n2. Related Work\nIn this section, we will provide a brief overview of the variety of methods and procedures\napplied in attempts to solve the problem of hate speech detection. Approaches using Bag of\nWords (BoW) [3] typically lead to a high number of false positives. They also suffer from data\nsparsity issues. In order to deal with the large number of false positives, efforts were made to\nbetter characterize and understand the nature of hate speech itself. This led to the formation\nof finer distinctions between the types of hate speech [ 4]; in that, hate speech was further\nclassified into ‚Äúprofane\" and ‚Äúoffensive\". Features such as N-gram graphs [5] or Part of Speech\nfeatures [6] were also incorporated into the classification models leading to an observable rise\nin the prediction scores.\nLater approaches used better representation of words and sentences by utilizing semantic\nvector representations such as word2vec [ 7] and GloVe [8]. These approaches outshine the\nearlier BoW approaches as similar words are located closer together in the latent space. Thus,\nthese continuous and dense representations replaced the earlier binary features resulting in a\nmore effective encoding of the input data. Support Vector Machines (SVMs) with a combination\nof lexical and parse features have been shown to perform well for detecting hate speech as well.\n[6]\n1https://www.perspectiveapi.com/\nLanguage Train Test\nEnglish 3708 814\nGerman 2373 526\nHindi 2963 663\nTable 1\nDataset Size (in number of tweets)\nThe recent trends in deep learning led to better representations of sentences. With RNNs, it\nbecame possible to model larger sequences of text. Gated RNNs such as LSTMs [9] and GRUs\n[10] made it possible to better represent long term dependencies. This boosted classification\nscores, with LSTM and CNN-based models significantly outperforming character and word\nbased N-gram models. [11] Character based modelling with CharCNNs [12] have been applied\nfor hate speech classification. These approaches particularly shine in cases where the offensive\nspeech is disguised with symbols like ‚Äò*‚Äô, ‚Äò$‚Äô and so forth. [13]\nMore recently, attention based approaches like Transformers [1] have been shown to cap-\nture contextualized embeddings for a sentence. Approaches such as BERT [ 2] which have\nbeen trained on massive quantities of data allow us to generate robust and semantically rich\nembeddings which can then be used for downstream tasks including hate speech detection.\nThere have also been a variety of open or shared tasks to encourage research and development\nin hate speech detection. The TRAC shared task [ 14] on aggression identification included\nboth English and Hindi Facebook comments. Participants had to detect abusive comments and\ndistinguish between overtly aggressive comments and covertly aggressive comments. OffensEval\n(SemEval-2019 Task 6) [15] was based on the the Offensive Language Identification Dataset\n(OLID) containing over 14,000 tweets. This SemEval task had three subtasks: discriminating\nbetween offensive and non-offensive posts, detecting the type of offensive content in a post\nand identifying the target of an offensive post. At GermEval, [ 16] there was a task to detect\nand classify hurtful, derogatory or obscene comments in the German language. Two sub-tasks\nwere continued from their first edition, namely, a coarse-grained binary classification task and a\nfine-grained multi-class classification problem. As a novel sub-task, they introduced the binary\nclassification of offensive tweets into explicit and implicit.\n3. Dataset\nThe datasets for the tasks were provided by the organizers of the HASOC ‚Äô202. [17] The data\nconsists of tweets from three languages: English, German and Hindi, and was annotated on two\nlevels. The coarse annotation involved a binary classification task with the given tweet being\nmarked as hate speech (HOF) or not (NOT). In the finer annotation, we differentiate between\nthe types of hate speech and have four different formal classes:\n1. HATE: This class contains tweets which highlight negative attributes or deficiencies of cer-\ntain groups of individuals. This class includes hateful comments towards individuals based\n2https://competitions.codalab.org/competitions/26027\nLanguage NOT HOF\nEnglish 1852 1856\nGerman 1700 673\nHindi 2116 847\nTable 2\nTraining set label distribution: Task 1\nLanguage NONE HATE OFFN PRFN\nEnglish 1852 158 321 1377\nGerman 1700 146 140 387\nHindi 2116 234 465 148\nTable 3\nTraining set label distribution: Task 2\non race, political opinion, sexual orientation, gender, social status, health condition, etc.\nExample: ‚ÄúRT @Lubchansky: good to know rich people have always been dumb as shit\nhttps://t.co/otdmH0wquk‚Äù\n2. OFFN: This class contains tweets which are degrading, dehumanizing or insulting towards\nan individual. It encompasses cases of threatening with violent acts.\nExample: ‚ÄúBy shitting yourself and taking the backdoor out, instead of fronting up to the\npublic. ‚Äù\n3. PRFN: This class contains tweets with explicit content, profane words or unacceptable\nlanguage in the absence of insults and abuse. This typically concerns the usage of\nswearwords and cursing.\nExample: ‚Äú@HermesCxbin turn that shit off‚Äù\n4. NONE: This class contains the tweets which do not fit into the above three classes i.e it\ndoes not contain instances of hate and offence.\nExample: ‚Äú@AskPlayStation I can‚Äôt get the 14 days free trial please fix I don‚Äôt have money\nfor ps plus I need this. ‚Äù\nIn table 1, we list the data size in number of tweets, and in tables 2 and 3, we provide the\nnumber of instances of different classification labels.\n4. Approach\nIn this section, we outline our approach towards solving the task at hand.\n4.1. Preprocessing\nWe utilized the python libraries tweet-preprocessor3 and ekphrasis4 for tweet tokenization and\nhashtag segmentation respectively. For extracting English and German cleaned tweet texts,\n3https://github.com/s/preprocessor\n4https://github.com/cbaziotis/ekphrasis\ntweet-preprocessor‚Äôs clean functionality was used. For Hindi tweets, we tokenized the tweet\ntext on whitespaces and symbols including colons, commas and semicolons. This was followed\nby removal of hashtags, smileys, emojis, URLs, mentions, numbers and reserved words (such as\n@RT which indicates Retweets) to yield the pure Hindi text within the tweet.\n4.2. Feature Engineering\nIn addition to the cleaned tweet, we utilize tweet-preprocessor to populate certain information\nfields which can act as features for our classifiers. We include the hashtag text which is\nsegmented into meaningful tokens using the ekphrasis segmenter for the twitter corpus. We\nalso save information such as URLs, name mentions such as ‚Äò@derCarsti‚Äô, quantitative values\nand smileys. We extract emojis which can be processed in two ways. We initially experimented\nwith the emot5 python library to obtain the textual description of a particular emoji. For example,\n‚Äò\n ‚Äô maps to ‚Äòsmiling face with open mouth & cold sweat‚Äô and ‚Äò\n ‚Äô maps to ‚Äòpanda‚Äô. We later\nchose to utilize emoji2vec [18] to obtain a semantic vector representing the particular emoji.\nThe motivation behind this is as follows: the text describing the emoji‚Äôs attributes might not\ncapture all the pragmatics and the true sense of what the emoji signifies in reality. As a concrete\nexample, consider ‚Äò\n ‚Äô, the tongue emoji. The textual representation will not showcase the\nemoji‚Äôs association with ‚Äòjoking around, laughter and general goofiness‚Äô which is its real world\nimplication. We expect emoji2vec to capture these kinds of associations.\n4.3. Perspective API Features\nWe perform experiments with features extracted from the Perspective API. The API uses machine\nlearning models to estimate various numerical metrics modeling the perceived impact which a\npost or a comment might have within a conversation. Right now, the Perspective API does not\nsupport Hindi natural language text in Devanagari script. Thus, our experiments are on German\nand English. On German text, the API provides scores which are real numbers between 0 and 1\nfor the following fields: ‚Äòtoxicity‚Äô, ‚Äòsevere toxicity‚Äô, ‚Äòidentity attack‚Äô, ‚Äòinsult‚Äô and ‚Äòprofanity and\nthreat‚Äô. For English text, in addition to the fields for German, the API provides similar scores\nfor the fields: ‚Äòsexually explicit‚Äô, ‚Äòobscene‚Äô and ‚Äòtoxicity fast‚Äô (which simply uses a faster model\nfor computing toxicity levels on the back-end).\nFor both English and German tweets, we extract perspective API scores for all available fields\nusing (a) the complete tweet as is, and (b) the extracted cleaned tweet text excluding emojis,\nsmileys, URLs, mentions, numbers, hashtags and reserved words. Thus, we have 18 features for\nEnglish tweets and 12 features for German tweets to work with.\nWe trained multi-layer perceptron classifiers for English and German using a concatenation\nof these features as the input vector. In addition to these classifiers trained in the monolingual\nsetting, we trained an English-German multilingual classifier using the 12 perspective API\nfeatures which are common to English and German. The datapoints in the corresponding\ntraining sets were randomly shuffled and standardized. The same standardization values were\nused on the test set during inference. We tried out multiple training settings with different\n5https://github.com/NeelShah18/emot\nRaw Tweet Text\nRT\t@jeonggukpics:\tDon‚Äô t \ndisturb\tplease,\the\tis \nenjoying\this\tsnacks\twhile \nmaking\tthose\tlittle\tdance \n\u0000\u0000\u0000\u0000\u0000\n#BBMAsT opSocial\tBTS \n#JUNGKOOK\t# Ï†ïÍµ≠ ‚Ä¶ \nCleaned Text\n:\tDont\tdisturb\tplease, \nhe\tis\tenjoying\this \nsnacks\twhile\tmaking \nthose\tlittle\tdance\tBTS \nEmojis\n\u0000 \t\t \u0000\n\u0000 \t\t \u0000\n\u0000\nHashtags\n#BBMAsT opSocial \n#JUNGKOOK \n# Ï†ïÍµ≠\nemoji2vec \nStatic \nT ransformer \nT ransformer \nEncoder ClassiÔ¨Åer \nTrue Label\nPredicted\nLabel\nconcat\nloss \nFigure 1:Model Overview\nactivation functions and optimization techniques. The best results with Perspective features are\npresented in Section 5.\n4.4. Proposed Transformer-based Models\nWe leverage Transformer-based [1] masked language models to generate semantic embeddings\nfor the cleaned tweet text. In addition to the cleaned tweet‚Äôs embedding, we generate and\nutilize semantic vector representations for all the emojis and segmented hashtags available\nwithin the tweet. The segmented hash embeddings are generated using the same pre-trained\nTransformer model such that the text and hashtag embeddings are grounded in the same latent\nspace. emoji2vec is used to create the emojis‚Äô semantic embeddings. The Transformer layers\nencoding the cleaned tweet text are updated during the fine-tuning process on the available\ntraining data. For classification, we use the concatenation of the cleaned tweet‚Äôs embedding\nwith the collective embedding vector for segmented hashtags and emojis.\nWe are required to encode a list of emojis & a list of segmented hashtags, both of which can\nbe of variable lengths. Therefore, we average the vector representations of all the individual\nemojis or segmented hashtags as the case may be, to generate the centralised emoji or hashtag\nrepresentation. This is simple, intuitive, and earlier work on averaging local word embeddings\nto generate global sentence embeddings [19] has showed that this yields a comprehensive vector\nrepresentation for sentences. We assume the same to hold true for emojis and hashtags as well.\nThe concatenated feature-set is then passed to a two layer multi-layer perceptron (MLP). The\nloss from the classifier is propagated back through the cleaned tweet Transformer encoder during\ntraining. We experimented with XLM-RoBERTa (XLMR) [20] as our pre-trained Transformer in\nvarious training settings. XLM-RoBERTa has outperformed similar multilingual Transformer\nActivation Optimization English German\nTask 1 Task 2 Task 1 Task 2\nmonolingual identity adam (early-stop) 89.68 53.90 75.40 41.84\ntanh adam (early-stop) 88.93 47.07 79.25 43.00\nmultilingual\nidentity sgd (adaptive LR) 88.82 47.02 72.89 38.86\nidentity adam (early-stop) 88.44 46.00 72.63 42.83\ntanh sgd (adaptive LR) 87.69 44.86 75.38 38.80\ntanh adam (early-stop) 87.95 46.03 76.68 46.40\nTable 4\nPerspective API Experiments(Best results highlighted in bold)\nmodels such as mBERT(multilingual BERT) [2] and multilingual-distilBERT [21] on various\ndownstream tasks. We therefore chose XLMR as our base Transformer model for the purpose\nof the shared task. A high level overview of our model flow is shown in figure 1.\nFor fine-tuning our XLMR Transformer weights, we perform learning rate scheduling based\non the actual computed macro F1-scores on the validation split instead of using the validation\nloss. As opposed to simply using early-stopping to prevent overfitting, we consider the change\nin validation performance at the end of each training iteration. If the validation performance\ngoes down across an iteration, we trace back to the previous model weights and scale down our\nlearning rate. Training stops when the learning rate reaches a very small value ùúñ6. Although ex-\npensive, this form of scheduling ensures that we maximize our Macro F1-score on the validation\nsplit. For further details on specific implementation nuances and choice of hyperparameters,\nrefer to Section 6.\n5. Results\nIn this section, we provide quantitative performance evaluations of our approaches on the\nprovided testing-set, the evaluation metric used throughout being the macro F1-score.\nIn table 4, we present our study on usage of Perspective API features with a multi-layer\nperceptron classifier for English and German tasks. We notice that these features are able to\nprovide respectable results on the hate and offensive content detection but cannot compete with\nthe Transformer-based models when fine-grained classification is required. In the monolingual\nmode, our exhaustive grid search showed that the use of identity activation for English and\ntanh activation for German are the most effective MLP hidden layer activation settings. Table 4\nlists the best activation functions and optimization techniques for particular (task, language)\npairs. We observe that German Task 2 benefits from the multilingual mode and we attribute this\nto the additional data from the English training examples which allow the model to generalize\nbetter. However, a drop in the English results is witnessed which might be due to the reduction\nin the number of available features.\nIn table 5, we present results using our proposed Transformer-based models. We present\n6Set to 1e-12 in our experiments.\nModel English German Hindi\nTask 1 Task 2 Task 1 Task 2 Task 1 Task 2\nXLMR-freeze-mono 83.92 52.38 66.85 41.52 68.25 40.45\nXLMR-freeze-multi 82.02 51.02 68.34 48.60 66.27 41.59\nXLMR-adaptive 90.29 59.03 81.04 52.99 75.40 45.87\nXLMR-tuned 90.05 60.70 81.87 53.28 74.29 49.74\nTable 5\nPerformance of Proposed Transformer-based Models(Best results highlighted in bold)\nFeatures English German Hindi\nTask 1 Task 2 Task 1 Task 2 Task 1 Task 2\nmonolingual\ncleaned text 83.27 49.12 69.23 39.12 68.45 45.18\ncleaned text + emoji 83.60 49.78 68.05 40.54 68.21 45.48\ncleaned text + hashtag 83.17 53.60 66.23 41.91 66.98 50.08\nmultilingual\ncleaned text 80.47 47.88 71.07 46.66 64.84 44.39\ncleaned text + emoji 82.73 51.90 72.73 43.24 67.83 41.83\ncleaned text + hashtag 81.22 50.06 68.52 47.31 68.19 44.71\nTable 6\nFeature Selection Study(Best results highlighted in bold)\nXLMR-freeze-mono and XLMR-freeze-multi as baselines in which we use the pre-trained XLM-\nRoBERTa Transformer weights without any fine-tuning7. Only the classifier head is trained in\nthese models. We train six separate models for the three languages (two tasks per language)\nand report corresponding results in the monolingual mode. In multilingual mode, we only train\ntwo models on the aggregated training data for the two tasks and use that for inference across\nthe three languages.\nThe models: XLMR-adaptive and XLMR-tuned use our proposed adaptive learn rate schedul-\ning. In XLMR-tuned, the epsilon value of the Adam optimizer was set to 1e-7 as this experimental\nsetting provided gains on the validation split in our hyper-parameter tuning phase. In both of\nthese models, we jointly fine-tune the XLM-RoBERTa Transformer weights and the classifier\nhead in a multilingual setting. Our proposed models significantly outperform baselines with\nfrozen Transformer weights which is both intuitive and expected.\nFinally, in table 6, we show results for a study on feature selection using pre-trained XLM-\nRoBERTa as the Transformer architecture for generating text embeddings. Note that our\nprimary models including XLMR-freeze utilize all of the discussed features. Like XLMR-freeze,\nthe Transformer layers are frozen and not fine-tuned during the training process. The table\nis separated into monolingual and multilingual modes of training. Results are showed using\ndifferent feature collections, namely, ‚Äòcleaned tweet text only‚Äô, ‚Äòcleaned tweet + hashtags‚Äô, and\n‚Äòcleaned tweet + emojis‚Äô as inputs to the classifier. We observe a performance drop for English\n7We used UKPLab‚Äôs sentence-transformers library‚Äôs pre-trained model: ‚Äòxlm-r-100langs-bert-base-nli-mean-\ntokens‚Äô for this task. The model is available at https://github.com/UKPLab/sentence-transformers.\nand Hindi and a considerable performance gain for German while moving from monolingual to\nmultilingual training settings.\n6. Experimental Details\nWe used Hugging Face‚Äôs8 implementation of XLM-RoBERTa in our proposed architecture. Our\narchitectures using Transformer models with custom classification heads were implemented\nusing pytorch9. We used Adam optimizer for training with an initial learning rate of 2e-5,\ndropout probability of 0.2 with other hyper-parameters set to their default values. We updated\nweights based on cross-entropy loss values. For studies with Perspective API Features and\nexperiments where we do not fine-tune the Transformer weights, we used scikit-learn‚Äôs [22]\nimplementation of a multi-layer perceptron and UKPLab‚Äôs sentence-transformers library [23]\nwhenever applicable.\nIn our Perspective API experiments, we used deep multi-layer perceptrons with 12 and 9\nhidden layers for the binary and multi-class classification modes respectively. Across all our\nexperimental settings, we used a batch size of 200 with other hyper-parameter values set to\ndefault. We performed an exhaustive grid search for every multi-layer perceptron model varying\nthe activation function, size of hidden layer, optimization algorithm and type of learning rate\nscheduling. We reported results using the grid search settings which performed the best on a\n4-fold cross validation on the training set. Our experimentation code is publicly available at\nhttps://github.com/sayarghoshroy/Hate-Speech-Detection.\n7. Conclusion\nIn this paper, we have leveraged the recent advances in large scale Transformer-based language\nmodel pre-training to build models for coarse detection and fine-grained classification of\nhateful and offensive content in social media posts. Our experiments showcase the utility\nand effectiveness of language models pre-trained with multi-lingual training objectives on a\nvariety of languages. Our studies show the efficacy of Perspective API metrics by using them as\nstandalone features for hate speech detection. Our best model utilized semantic embeddings for\ncleaned tweet text, emojis, and segmented hashtags as features, and a customized two-layer feed-\nforward neural network as the classifier. We further conducted a feature selection experiment\nto view the impact of individual features on the classification performance. We concluded that\nthe usage of hashtags as well as emojis add valuable information to the classification head. We\nplan to further explore other novel methods of capturing social media text semantics as part of\nfuture work.\n8https://huggingface.co/\n9https://pytorch.org/\nReferences\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, ≈Å. Kaiser, I. Polo-\nsukhin, Attention is all you need, in: Advances in neural information processing systems,\n2017, pp. 5998‚Äì6008.\n[2] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep bidirectional\ntransformers for language understanding, arXiv preprint arXiv:1810.04805 (2018).\n[3] I. Kwok, Y. Wang, Locate the hate: Detecting tweets against blacks, in: AAAI, 2013.\n[4] W. Wang, L. Chen, K. Thirunarayan, A. P. Sheth, Cursing in english on twitter, in:\nProceedings of the 17th ACM conference on Computer supported cooperative work &\nsocial computing, 2014, pp. 415‚Äì425.\n[5] S. Themeli, Hate Speech Detection using different text representations in online user\ncomments, Ph.D. thesis, 2018. doi:10.13140/RG.2.2.12991.25764.\n[6] Y. Chen, Y. Zhou, S. Zhu, H. Xu, Detecting offensive language in social media to protect\nadolescent online safety, in: 2012 International Conference on Privacy, Security, Risk and\nTrust and 2012 International Confernece on Social Computing, IEEE, 2012, pp. 71‚Äì80.\n[7] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, J. Dean, Distributed representations\nof words and phrases and their compositionality, in: Advances in neural information\nprocessing systems, 2013, pp. 3111‚Äì3119.\n[8] J. Pennington, R. Socher, C. D. Manning, Glove: Global vectors for word representation, in:\nProceedings of the 2014 conference on empirical methods in natural language processing\n(EMNLP), 2014, pp. 1532‚Äì1543.\n[9] I. Sutskever, O. Vinyals, Q. V. Le, Sequence to sequence learning with neural networks, in:\nAdvances in neural information processing systems, 2014, pp. 3104‚Äì3112.\n[10] J. Chung, C. Gulcehre, K. Cho, Y. Bengio, Empirical evaluation of gated recurrent neural\nnetworks on sequence modeling, arXiv preprint arXiv:1412.3555 (2014).\n[11] P. Badjatiya, S. Gupta, M. Gupta, V. Varma, Deep learning for hate speech detection\nin tweets, in: Proceedings of the 26th International Conference on World Wide Web\nCompanion, WWW ‚Äô17 Companion, International World Wide Web Conferences Steering\nCommittee, Republic and Canton of Geneva, CHE, 2017, p. 759‚Äì760. URL: https://doi.org/\n10.1145/3041021.3054223. doi:10.1145/3041021.3054223.\n[12] X. Zhang, J. Zhao, Y. LeCun, Character-level convolutional networks for text classification,\nin: Advances in neural information processing systems, 2015, pp. 649‚Äì657.\n[13] Y. Mehdad, J. Tetreault, Do characters abuse more than words?, 2016, pp. 299‚Äì303.\ndoi:10.18653/v1/W16-3638.\n[14] R. Kumar, A. K. Ojha, M. Zampieri, S. Malmasi (Eds.), Proceedings of the First Workshop\non Trolling, Aggression and Cyberbullying (TRAC-2018), Association for Computational\nLinguistics, Santa Fe, New Mexico, USA, 2018. URL: https://www.aclweb.org/anthology/\nW18-4400.\n[15] M. Zampieri, S. Malmasi, P. Nakov, S. Rosenthal, N. Farra, R. Kumar, SemEval-2019\ntask 6: Identifying and categorizing offensive language in social media (OffensEval), in:\nProceedings of the 13th International Workshop on Semantic Evaluation, Association for\nComputational Linguistics, Minneapolis, Minnesota, USA, 2019, pp. 75‚Äì86. URL: https:\n//www.aclweb.org/anthology/S19-2010. doi:10.18653/v1/S19-2010.\n[16] J. Stru√ü, M. Siegel, J. Ruppenhofer, M. Wiegand, M. Klenner, Overview of germeval task 2,\n2019 shared task on the identification of offensive language, 2019.\n[17] T. Mandl, S. Modha, G. K. Shahi, A. K. Jaiswal, D. Nandini, D. Patel, P. Majumder, J. Sch√§fer,\nOverview of the HASOC track at FIRE 2020: Hate Speech and Offensive Content Iden-\ntification in Indo-European Languages), in: Working Notes of FIRE 2020 - Forum for\nInformation Retrieval Evaluation, CEUR, 2020.\n[18] B. Eisner, T. Rockt√§schel, I. Augenstein, M. Bosnjak, S. Riedel, emoji2vec: Learning\nemoji representations from their description, CoRR abs/1609.08359 (2016). URL: http:\n//arxiv.org/abs/1609.08359.\n[19] S. Arora, Y. Liang, T. Ma, A simple but tough-to-beat baseline for sentence embeddings\n(2016).\n[20] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzm√°n, E. Grave,\nM. Ott, L. Zettlemoyer, V. Stoyanov, Unsupervised cross-lingual representation learning at\nscale, 2020. arXiv:1911.02116.\n[21] V. Sanh, L. Debut, J. Chaumond, T. Wolf, Distilbert, a distilled version of bert: smaller,\nfaster, cheaper and lighter, 2020. arXiv:1910.01108.\n[22] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,\nP. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,\nM. Perrot, E. Duchesnay, Scikit-learn: Machine learning in Python, Journal of Machine\nLearning Research 12 (2011) 2825‚Äì2830.\n[23] N. Reimers, I. Gurevych, Making monolingual sentence embeddings multilingual using\nknowledge distillation, arXiv preprint arXiv:2004.09813 (2020). URL: http://arxiv.org/abs/\n2004.09813.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6895731091499329
    },
    {
      "name": "Computer science",
      "score": 0.571604311466217
    },
    {
      "name": "Voice activity detection",
      "score": 0.521985650062561
    },
    {
      "name": "Speech recognition",
      "score": 0.3500301241874695
    },
    {
      "name": "Speech processing",
      "score": 0.2281501591205597
    },
    {
      "name": "Engineering",
      "score": 0.19709637761116028
    },
    {
      "name": "Electrical engineering",
      "score": 0.11289802193641663
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}