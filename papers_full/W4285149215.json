{
  "title": "Extracting Latent Steering Vectors from Pretrained Language Models",
  "url": "https://openalex.org/W4285149215",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2560100215",
      "name": "Nishant Subramani",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2332192761",
      "name": "Nivedita Suresh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108007937",
      "name": "Matthew Peters",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1915251500",
    "https://openalex.org/W3176618728",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4205635927",
    "https://openalex.org/W2997195635",
    "https://openalex.org/W2025768430",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2752172973",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3115894062",
    "https://openalex.org/W2143177362",
    "https://openalex.org/W3018305985",
    "https://openalex.org/W3169754167",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W2964222296",
    "https://openalex.org/W3214124416",
    "https://openalex.org/W3004665584",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3179534853",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W2125320996",
    "https://openalex.org/W2617566453",
    "https://openalex.org/W3098929340",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W3198527962",
    "https://openalex.org/W3005116366",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3085190015",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2956352737",
    "https://openalex.org/W3137010024",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2887005207",
    "https://openalex.org/W2973154008",
    "https://openalex.org/W2963223306",
    "https://openalex.org/W2963141266",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2914442349"
  ],
  "abstract": "Prior work on controllable text generation has focused on learning how to control language models through trainable decoding, smart-prompt design, or fine-tuning based on a desired objective. We hypothesize that the information needed to steer the model to generate a target sentence is already encoded within the model. Accordingly, we explore a different approach altogether: extracting latent vectors directly from pretrained language model decoders without fine-tuning. Experiments show that there exist steering vectors, which, when added to the hidden states of the language model, generate a target sentence nearly perfectly (> 99 BLEU) for English sentences from a variety of domains. We show that vector arithmetic can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark, with performance comparable to models tailored to this task. We find that distances between steering vectors reflect sentence similarity when evaluated on a textual similarity benchmark (STS-B), outperforming pooled hidden states of models. Finally, we present an analysis of the intrinsic properties of the steering vectors. Taken together, our results suggest that frozen LMs can be effectively controlled through their latent steering space.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2022, pages 566 - 581\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nExtracting Latent Steering Vectors from Pretrained Language Models\nNishant Subramani† Nivedita Suresh♢ Matthew E. Peters†\n†Allen Institute for Artificial Intelligence, Seattle, W A, USA\n♢Arrive Bio, San Francisco, CA, USA\n{nishants,matthewp}@allenai.org\n{nive}@arrivebio.com\nAbstract\nPrior work on controllable text generation has\nfocused on learning how to control language\nmodels through trainable decoding, smart-\nprompt design, or fine-tuning based on a de-\nsired objective. We hypothesize that the infor-\nmation needed to steer the model to generate\na target sentence is already encoded within the\nmodel. Accordingly, we explore a different\napproach altogether: extracting latent vectors\ndirectly from pretrained language model de-\ncoders without fine-tuning. Experiments show\nthat there exist steering vectors, which, when\nadded to the hidden states of the language\nmodel, generate a target sentence nearly per-\nfectly (> 99 BLEU) for English sentences from\na variety of domains. We show that vector arith-\nmetic can be used for unsupervised sentiment\ntransfer on the Yelp sentiment benchmark, with\nperformance comparable to models tailored to\nthis task. We find that distances between steer-\ning vectors reflect sentence similarity when\nevaluated on a textual similarity benchmark\n(STS-B), outperforming pooled hidden states\nof models. Finally, we present an analysis of\nthe intrinsic properties of the steering vectors.\nTaken together, our results suggest that frozen\nLMs can be effectively controlled through their\nlatent steering space. 1\n1 Introduction\nLeveraging large pretrained language models\ntrained on massive Web corpora has become the\ngo-to approach to solve natural language process-\ning tasks (Peters et al., 2018; Radford et al., 2018;\nDevlin et al., 2018; Brown et al., 2020). As a result,\ncontrolling these models has become paramount as\nmany applications of NLP technology require con-\ntrol over the generations of the model. Prior work\naims to learn how to control language models and\nfalls in three categories: trainable decoding (Gu\n1Code is available at https://github.com/nis\nhantsubramani/steering_vectors.\net al., 2017; Deng et al., 2020), smart-prompt de-\nsign (Shin et al., 2020; Lester et al., 2021), and\nfine-tuning based on a desired objective (Krause\net al., 2021; Weng, 2021). Further, many works opt\nto train auto-encoder based models for controllable\ntext generation (Shen et al., 2017, 2020; Mai et al.,\n2020). These approaches make controllability eas-\nier by learning a latent space that is more easily\nmanipulated to encourage models to generate text\ncorresponding to a target attribute such as positive\nsentiment in the case of sentiment transfer.\nWe take a more direct approach and explore\nwhether it is possible to extract latent vectors di-\nrectly from pretrained language model decoders\nwithout fine-tuning. We call these vectors steering\nvectors and define the latent steering space of a\nsentence under a language model by the set of ex-\ntracted steering vectors, which steer the model to\ngenerate that sentence exactly. During decoding,\nwe add our steering vector to the hidden states of\nthe language model to generate the target sentence.\nRather than training a model to learn steering vec-\ntors, we provide several methods to extract fixed-\nlength steering vectors directly from pretrained lan-\nguage model decoders. Experiments show that we\ncan extract steering vectors effectively, achieving\nnearly perfect recovery for English sentences from\na variety of domains without fine-tuning the under-\nlying language model at all.\nNext, we take our extracted steering vectors and\nexplore whether they can be used for unsupervised\nsentiment transfer on the Yelp sentiment bench-\nmark (Zhang et al., 2015). We find that adding an\noffset vector to extracted steering vectors performs\ncomparably to carefully designed, autoencoder-\nbased models. To see whether steering vectors\nencode semantics, we explore whether they can be\nused for unsupervised textual similarity. On the\nsemantic textual similarity benchmark (STS-B, Cer\net al. (2017)), our steering vectors outperform ex-\ntractive methods such as averaging language model\n566\nFigure 1: Our approach adds a vector zsteer to the activations of a pretrained transformer decoder to steer it to\ndecode a desired target sentence. We experiment with adding zsteer to different locations inside a GPT-2 model at\ndifferent timesteps. Experiments reveal that our approach can recover sequences nearly perfectly and that injecting\nthe steering vector in the middle layers of the transformer stack performs best. Layer normalizations and residual\nconnections inside the transformer block are omitted for clarity.\nhidden states and GloVe vectors (Pennington et al.,\n2014) when measuring the cosine similarity be-\ntween vectors, but fall short of lexical methods tai-\nlored to semantic similarity tasks and methods that\nfinetune on natural language inference datasets.\nLastly, we analyze the intrinsic properties of the\nlatent space of our steering vectors. Experiments\nshow that decoding from interpolations in the latent\nspace produces meaningful output, and that steer-\ning vectors from different domains cluster together.\nAlso, we find that our methods do not simply mem-\norize the target sequence like a naive compression\nalgorithm, and instead leverage the model. Taken\ntogether, our results suggest that frozen language\nmodels can be controlled effectively through their\nlatent steering space.\n2 Extracting Steering Vectors\nThis section discusses our method for extracting a\nsteering vector for a target sentence from a frozen,\npretrained language model. Throughout this pa-\nper, we use GPT2 as our language model and use\nits 117M parameter model size (Radford et al.,\n2019), although our approach can be directly ap-\nplied to any transformer-based autoregressive lan-\nguage model decoder (Vaswani et al., 2017).\n2.1 Steering Vectors\nIn controllable text generation and textual style\ntransfer, prior work based on denoising and vari-\national autoencoders opt for a disentangling ap-\nproach. These approaches encode the source se-\nquence into a fixed-length vector using an encoder,\napply style transformations using a controller, and\nfinally decode from the transformed vector using a\ndecoder (Shen et al., 2017; Jin et al., 2020). Instead\nof learning an encoder and controller to uncover a\nrepresentation, we ask whether its possible to ex-\ntract a vector directly from a pretrained language\nmodel decoder in order to steer the model.\nDue to the success of hidden layer manipula-\ntions for language models including adapter-based\nfine-tuning (Houlsby et al., 2019), plug-and-play\nlanguage models (Dathathri et al., 2019), and offset-\nvector-based recovery and style transfer among oth-\ners (Subramani et al., 2019; Shen et al., 2020; Mai\net al., 2020; Montero et al., 2021), we choose to\nmanipulate the hidden states as well.\nOur method works by adding a fixed-length vec-\ntor zsteer to the hidden states of a pretrained and\nfrozen LM. For a desired target sentence, we ran-\ndomly initialize zsteer and optimize it via gradient\ndescent to maximize the likelihood of the model\ngiven the target sentence. At decoding time, we\n567\nfeed a zsteer to the model and perform decoding as\nusual. The choice of a fixed-length vector makes\nanalysis more meaningful, allowing us to com-\npare vectors for different sentences with different\nlengths in the same representation space.\n2.2 Discovering steering vectors\nWe define our steering vectors zsteer ∈ Rd′\n. In\nour experiments, d′ ≤ d, where d is the hidden\ndimension of the underlying language model (for\nGPT2-117M, d = 768 ). If d′ < d, we project\nzsteer using a semi-orthogonal matrix, Wsteer ∈\nRd′×d, which preserves scale. Wsteer is initialized\nrandomly, never trained, and never updated.\nWe estimate a steering vector ˆzsteer ∈ Rd′\nvia\nthe language model for a sentencex by maximizing\nthe log probability ofx, while keeping the language\nmodel fixed:\nˆzsteer = argmax\nzsteer∈Z\nTX\nt=1\nlog p(xt|x<t, zsteer) (1)\nHere, Z ∈Rd′\n. Note: we find a single steering\nvector zsteer for each sentence x. We use stochas-\ntic gradient descent with the Adam (Kingma and\nBa, 2014) optimizer and cross entropy loss to find\nthe best ˆzsteer, while freezing the language model.\nSee algorithm 1 for the pseudocode.\nSince our method adds zsteer to the activations\nof the model, the layer we add zsteer to affects re-\ncoverability. We experiment with injectingzsteer at\ndifferent layers (injection locations): at the embed-\nding layer, right before language model head (LM\nHead), after self-attention layer in the transformer\nstack, after feed-forward layer in the transformer\nstack as well as combinations of them. In addi-\ntion to varying injection locations, we also vary the\ntimesteps where zsteer gets added. We experiment\nwith adding zsteer at just the first timestep and at\nevery timestep. See Figure 1 for details.\n2.3 Steering Language Models\nWe steer the language model using zsteer to gener-\nate a target sentence x by passing in a beginning-\nof-sentence token and zsteer to the model. Since\nwe are interested in exact generation, all results\npresented use greedy decoding without assuming\na true length. We stop when decoding produces\nan end-of-sentence token or produces 1024 tokens,\nthe maximum length that GPT-2 can generate.\nALGORITHM 1:Extracting zsteer for a sentence\nInput : x – target sentence\nM – pretrained language model\nθ – pretrained language model weights\nIL– injection location\nIT– injection timestep\nd – dimension of zsteer\nOutput :zsteer – extracted candidate steering vector\n1 zsteer ∼ xavier_normal(d)\n2 for i ← [1, 2, ..., N]do\n3 logits = Mθ.forward (x, zsteer, IL, IT )\n4 L = XENT (logits, x)\n5 L.backward()\n6 zsteer = zsteer + lr ∗ ∂L\n∂zsteer\n7 end\n8 return zsteer\n3 Can we extract steering vectors?\nHere, we show that we can robustly extract steering\nvectors that generate target sentences perfectly.\n3.1 Experimental setup\nWe gather a broad corpus spanning four different\ndomains and measure the extent to which our ap-\nproach can extract a steering vector for each sen-\ntence under a variety of experimental conditions,\nwhere we vary injection locations and timesteps.\nData Collection For these experiments on sen-\ntence recoverability, we create a dataset which\ncombines four corpora from different domains:\nmovie dialogs (movies), classic books (books),\nnews articles (news), and Wikipedia (wiki). For\nmovies, we choose the Cornell Movie Dialogs\ncorpus (Danescu-Niculescu-Mizil and Lee, 2011),\nwhich consists of fictional conversations from\nmovie scripts. We choose NLTK’s Gutenberg\ndataset for our books portion, which consists of\na subset of texts from Project Gutenberg (Lebert,\n2008). Our news subset comes from the Gigaword\ndataset for abstractive summarization (Graff et al.,\n2003). Lastly, our Wikipedia portion comes from\nWikiText-103 (Merity et al., 2017). For movies,\nnews, and wiki, we extract sentences from its pre-\nspecified validation set. For books, since NLTK’s\nGutenberg dataset lacks a pre-specified data split,\nwe consider the entire dataset.\nData Preprocessing We sentence tokenize all\ndatasets using NLTK’s sentence tokenizer. To con-\nstruct our dataset, we group sentences by sentence\nlength into 8 bins: 5-10, 10-15, 15-20, 20-25, 25-\n30, 30-35, 35-40, and 40-128 using NLTK’s word-\nlevel, regular expression tokenizer. Next, we ran-\n568\nInjection location Timestep BLEU-4\nEmbedding all timesteps 33.99\nLayer 6 (self attn) all timesteps 100.0\nLayer 6 (self attn) first timestep 99.80\nLayer 7 (feed fwd) all timesteps 100.0\nLayer 7 (feed fwd) first timestep 99.25\nAll layers\n(self attn + feed fwd) all timesteps 100.0\nAll layers\n(self attn + feed fwd) first timestep 91.72\nLM head all timesteps 6.72\nTable 1: Sentence recovery for steering vectors when\ninjected into different layers of the transformer model\n(Figure 1) and at multiple timesteps (all timesteps or\nfirst timestep). Results show that injecting a steering\nvector into the transformer stack, even at just the first\ntimestep, can lead to nearly perfect recovery as long as\nit is in the middle of the network (layers 6 or 7 of 12).\ndomly sample 8 sentences from each bin to ex-\namine the efficacy of our method for a variety of\nsequence lengths.\nMeasuring the Effectiveness of Steering Given\na target sentence s, we measure how well the steer-\ning vector zsteer can recover the target sentence by\nfirst greedily decoding from the language model\nwith zsteer, and then computing smoothed BLEU-4\nusing the target sentence s and our decoded recon-\nstruction ˆs (Papineni et al., 2002; Chen and Cherry,\n2014).\nHyperparameter Search Our initial experi-\nments showed little variation to most hyperparam-\neters such as initialization method and learning\nrate schedule, so we fixed them in subsequent ex-\nperiments using the values in Table 6 in the ap-\npendix. We choose GPT2-117M as our language\nmodel and evaluate recovery on our dataset while\nvarying injection locations and injection timesteps,\nthe two hyperparameters that affect results signifi-\ncantly. We present a subset of the results in Table 1\nand the full set in the appendix (Tables 7, 8, and 9).\n3.2 Recovery effectiveness\nTable 1 shows reconstruction performance for sev-\neral injection methods and indicates that we can re-\ncover a target sentence with perfect recovery when\ninjecting zsteer in the middle of the transformer\nstack (layers 6 or 7 of 12) at just the first timestep\nand at all timesteps, for sequences up to 128 to-\nFigure 2: TSNE projection of 8 steering vectors initial-\nized from different random seeds for 20 different sen-\ntences (injected at layer 6, after self-attention). zsteer is\nwell-separated for different sentences, and the different\nseeds are tightly clustered for the same target sentence,\nindicating that our extraction method is robust.\nkens. We surmise that the middle layers of the\ntransformer stack encode sufficiently rich feature\nrepresentations that a small perturbation of a hid-\nden layer, a steering vector, is sufficient to recover\na sentence. The success of steering vectors when\ninjected in the middle of the transformer could help\nexplain why adapter-based fine-tuning is effective.\nIn contrast, we find that we cannot steer GPT-2\nat either the embedding or final language model\nhead locations. We suspect this is due to the fact\nthat the embedding layer solely captures low-level\ninformation (Lin et al., 2019; Ethayarajh, 2019;\nRogers et al., 2020). Poor recovery at the LM\nhead location is somewhat surprising, but could\nbe explained by noting that the model has very\nlow capacity above this layer. This suggests that\nalternative steering mechanisms, such as DExperts,\nthat intervene at the output layers could potentially\nbe improved by modifying hidden states elsewhere\nin the transformer stack (Liu et al., 2021).\nRobustness Now that we have established that\nsteering vector extraction is possible, we explore\nwhether there exist multiple steering vectors which\nrecover the same sentence, and if so, what the re-\nlationship is between these vectors. To do this,\nwe take all 64 sentences from the books subset\nof the main dataset and initialize 8 different steer-\ning vectors for each sentence from different seeds.\nExperiments reveal that for most sentences (63 of\n64) all initializations recover the target sentence\n569\nperfectly, confirming the robustness of our method.\nLatent geometry in text-based auto-encoders\nstruggle with mapping vectors from one space to\nanother consistently (e.g. token space to latent\nspace) (Bowman et al., 2016; Shen et al., 2020).\nThe denoising auto-encoder offers a more consis-\ntent token space to latent space mapping (Vincent\net al., 2008). To explore whether our steering vec-\ntors have a distance-preserving mapping, we cluster\nthe different initializations of steering vectors. We\nextract 8 steering vectors for each of 20 sentences\nfrom the books corpus and down-project them into\ntwo-dimensions via TSNE (Maaten and Hinton,\n2008). Figure 2 shows 20 distinct clusters, one\nfor each sentence. This indicates that distances be-\ntween different vectors representing the same target\nsentence are much smaller than distances between\nvectors representing different sentences, and that\ndistances in token space could be reflected in the\nlatent steering space.\nMotivated by the clustering results, we investi-\ngate whether the mean vector of the 8 extracted\nsteering vectors for each target sentence recover\nthe same sentence. Experiments show that mean\nvectors are able to recover target sentences nearly\nperfectly, leading to a BLEU-4 of 99.4, further\nestablishing the robustness of our method.\n4 Is unsupervised style transfer in the\nlatent steering space possible?\nWe explore whether vector arithmetic in this space\nis possible in the context of unsupervised style\ntransfer. In other words, we measure whether\nadding an offset vector, which captures the de-\nsired style transfer, to the steering vector effectively\nchanges the style of the generated sentence. Here,\nwe show that unsupervised vector arithmetic with\nsteering vectors is effective for unsupervised sen-\ntiment transfer, with performance comparable to\nmodels tailored to this task.\nAfter extracting steering vectors for each sen-\ntence, we compute offset vectors by averaging steer-\ning vectors for a set of sentences in the source style\n¯zsource and subtracting from the average of a set of\nsteering vectors for the target style ¯ztarget. Next,\nwe flip the style of each sentence in our test set by\nadding the respective style transfer vector directly\nto its steering vector after scaling it by λ:\nztotarget = ¯ztarget − ¯zsource (2)\nˆztarget = zsource + λ · ztotarget (3)\nFigure 3: Evaluation of unsupervised sentiment transfer\non the Yelp dataset. The plot shows accuracy vs. self-\nBLEU by varying λ = (0.25, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0,\n4.0, 5.0, 10.0) for our method. Overall, the steering\nvectors perform comparably to prior work.\nUnsupervised Sentiment Transfer Using the\nYelp Sentiment dataset preprocessed by Shen et al.\n(2017), we take 100 sentences from the validation\nset from each of the two classes of sentiment to\ncompute offset vectors and evaluate on the test set.\nFollowing prior work (Shen et al., 2017), we mea-\nsure how well this approach flips the sentiment\nof the sentence by measuring the accuracy of a\nRoBERTA-base model fine-tuned on the Yelp sen-\ntiment dataset. We also measure the BLEU-4 be-\ntween the style transferred sentences and the origi-\nnal and report the results in Figure 3. We call this\nSelf-BLEU following prior work. For this experi-\nment, our steering vectors are injected after the 7th\nself-attention layer at the first timestep.\nWe find that simple vector arithmetic via our\nsteering vectors, which is fully unsupervised, per-\nforms comparably to Shen et al. (2017), who learn\nan autoencoder-based model for the task in a fully\nsupervised manner. Our method also compares\nwell with the Autobot (Montero et al., 2021), AAE,\nand DAAE models (Shen et al., 2020), which al-\nthough are unsupervised, either require training on\nin-domain data or require pretraining on millions\nof tokens in order to be effective. Other methods\nthat use techniques from unsupervised machine\ntranslation to leverage the unpaired data in the task\noutperform all of these methods significantly (Hu\net al., 2017; Lample et al., 2019; He et al., 2020).\n570\nSteering vectors\nPositive Input the taste is excellent!\n+0.5 ∗ ztonegative the taste is excellent!\n+1.0 ∗ ztonegative the taste is excellent!\n+1.5 ∗ ztonegative\nthe taste is bitter and bitter\ntaste is bitter taste is bitter\n+2.0 ∗ ztonegative the taste is unpleasant.\nNegative Input the desserts were very bland.\n+0.5 ∗ ztopositive the desserts were very bland.\n+1.0 ∗ ztopositive the desserts were very bland .\n+1.5 ∗ ztopositive the desserts were very tasty.\n+2.0 ∗ ztopositive the desserts were very tasty.\nTable 2: Examples of transferring sentiment using steer-\ning vectors for a positive input sentence (top) and nega-\ntive input sentence (bottom). These results show fluency\nand accuracy in transfers while preserving the content\nof the input sentence.\nThese methods are not directly comparable to ours,\nas they evaluate on a different test set altogether and\nuse the training set to train directly. Our method\nonly requires access to 100 labeled examples per\nclass to compute ¯zsource and ¯ztarget, far fewer than\nother baselines. With as few as 10 examples per\nclass, performance of our method remains compet-\nitive with autoencoder-based baselines.\nTable 2 shows examples generated by our\nmethod for two input sentences. We find that re-\nsulting sentences become more positive or negative\nwith increasing λ and often modify adjectives by\nswapping them out. On closer inspection, we find\nthat fluency is often challenging for higher values\nof λ and that the generated sequences repeat in-\ndividual words or phrases. In addition, we find\nthat negative to positive sentiment transfer is qual-\nitatively more fluent and accurate than positive to\nnegative sentiment transfer; see Table 12 in the ap-\npendix for more example generations. Lastly, we\nevaluate on 19 paired style transfer tasks from the\nStylePTB dataset (Lyu et al., 2021), but modify\nthe tasks to be unsupervised, following the same\napproach as above. We find that our method is sim-\nilarly effective on these tasks; see Table 10 in the\nappendix for details.\n5 Do distances between steering vectors\nreflect sentence similarity?\nPreviously, we found there exist multiple steering\nvectors that recover a target sentence and that those\nsteering vectors are close together. This indicates\nthe potential for distances in token space to be re-\nflected in distances in the latent space occupied\nFigure 4: On the test split of STS-B, we measure Spear-\nman rank correlation (ρ · 100) between sentence similar-\nity scores and cosine similarities between the steering\nvectors extracted from GPT2-117M when injected at\ndifferent layers at the first timestep for those sentences.\nThe vertical lines indicate extractive baselines: mean-\npooled final hidden states for GPT2-117M and BERT-\nbase as well as mean-pooled GloVe vectors. Results\nshow that extracted steering vectors outperform these.\nby steering vectors. In this section, we explore\nwhether distances relate to semantic similarity. To\ndo so, we use the STS-B test dataset, which con-\nsists of sentence pairs and similarity scores. To\nevaluate our method we extract steering vectors for\neach sentence separately, compute cosine similarity,\nand then correlate cosine similarity with annotator\nsimilarity via Spearman rank correlation.\nIn Figure 4, we show how well extracted steering\nvectors perform when injected at different layers\nand at the first timestep in the transformer stack.\nThis observation mirrors the results from the exper-\niment on recovery effectiveness: middle layers in\nthe transformer stack are ideal for steering, lead-\ning to perfect recovery and highest performance on\nsemantic similarity. We outperform mean pooling\nthe final hidden states of GPT2-117M and BERT-\nbase as well as averaged GloVe vectors. Even\nthough our method is fully extractive, cosine dis-\ntances reflect semantic similarity well. We take\nour two best performing configurations, the 7th\nself-attention layer and the 7th feedforward layer,\nand compare with unsupervised methods for tex-\n571\nMethod Spearman Pearson\nExtractive methods\nAvg GPT2-117M embeddings 25.92 16.52\nAvg Bert embeddings 47.29 47.91\nAvg GloVe embeddings 42.53 40.25\nLayer-7 self attention (ours) 52.04 51.17\nLayer-7 feedforward (ours) 52.08 51.18\nNLI-finetuned methods\nAutoBot-base 58.49 -\nInferSent - GloVe 68.03 -\nSBERT-NLI-base 77.03 -\nLexical methods\nGloVe+UP - 71.5\nGloVe+WR - 72.0\nTable 3: We evaluate performance on the STS-B test set\nby measuring Spearman rank correlation and Pearson\ncorrelation (ρ · 100). We take our two best perform-\ning configurations from Figure 4 and compare them\nwith three classes of unsupervised methods: extractive,\nNLI-finetuned, and lexical methods. Our method out-\nperforms the extractive methods, but performs worse\nthan the other methods, which are tailored for this task.\ntual similarity. Table 3 shows that our extracted\nsteering vectors out-perform prior extractive unsu-\npervised methods. Predictably, however, methods\nwhich pretrain or fine-tune models on natural lan-\nguage inference datasets such as AutoBot (Mon-\ntero et al., 2021), InferSent (Conneau et al., 2017),\nand SBERT (Reimers and Gurevych, 2019) per-\nform better. Lexical methods tailored for semantic\nsimilarity such as GloVe with uSIF-weighting and\npiecewise component removal (GloVE + UP; Etha-\nyarajh (2018)) and GloVe + WR (Arora et al., 2017)\nalso outperform our method.\n6 Analysis of Properties\n6.1 Interpolation\nPrevious experiments indicate that the latent space\noccupied by steering vectors could be well-formed\nand smooth. To evaluate this qualitatively, we show\nlinear interpolations of two pairs of steering vectors\nextracted from the Yelp Sentiment dataset in Fig-\nure 5. The space between the vectors look smooth\nwith well-formed grammatical sentences that mix\nthe content of two sentences effectively. The first\ninterpolation (sentence pair 1) in Figure 5 shows\nthat the positive sentiment of the first sentence car-\nFigure 5: Interpolation between steering vectors ex-\ntracted from two pairs of random sentences from the\nYelp Sentiment test set. Decoding from interpolated vec-\ntors from two sentences produces well-formed output\nthat incrementally changes the sentiment and meaning.\nries all the way to λ = 0.7, despite the content of\nthe sentence changing to the second sentence. The\nsecond interpolation (sentence pair 2) in Figure 5\nindicates that the latent space could encode some\nsemantics relating to time. The second sentence\nincludes the word \"young\" and so the transition be-\ntween the two in λ = 0.3, 0.4 combines the word\n\"four\" from the first sentence with the temporal\ncomponent of \"years ago\" to relate the two sen-\ntences. Lastly, for each individual sentence there\nexists a radius around it where those vectors also\nsteer the language model to generate the same tar-\nget sentence. This could indicate that sentences\nhave a representative volume from which, if any\nvector was sampled, could recover the sentence.\n6.2 Sampling\nPrevious experiments show distances reflect seman-\ntic similarity and hint at the possibility that the\nlatent space is smooth. Given this, we evaluate\nwhether we can sample from this space. We take\n4000 extracted steering vectors from the Yelp Sen-\ntiment test set. We treat each dimension of the\nsteering vector as an independent random variable\nthat is normally distributed with a mean and vari-\nance equal to the mean and variance across that\ndimension over this set of steering vectors. Ta-\nble 11 shows the results of sampling 24 steering\nvectors and generating from them. We observe\n572\nmixed results: 5 samples lead to fully-formed sen-\ntences, and the remaining 19 lead to single tokens\nor phrases, indicating that treating steering vectors\nas samples from a multivariate Gaussian is not a\nreliable approach for sampling well-formed text.\n6.3 Intrinsic Dimension & Space Complexity\nWe define the intrinsic dimension of the task of\nsteering a language model as the minimum dimen-\nsion of zsteer that achieves perfect recovery on a set\nof sentences. To measure intrinsic dimension, we\nvary the dimensions of zsteer, choosing 192, 384,\n576, 768. We observe that reconstruction BLEU in-\ncreases as the steering vector dimension increases,\nindicating that 768 dimensions may be needed to\nrecover sequences nearly perfectly. Given this, we\nconclude that the intrinsic dimension is at most 768.\nHowever, a lower-dimensional representation can\nrecover most sentences: 384 dimensions led to a\nreconstruction BLEU of 83.29. See Table 4 for\nmore details. Additionally, we find that sentence\nlength and reconstruction BLEU are inversely cor-\nrelated, i.e. longer sequences are harder to recover.\nThis is well-known; the number of bits needed to\nencode a sequence grows linearly with its length.\nWe find that all four dimensions of steering vectors\ncan recover short sentences, but lower dimensional\nsteering vectors struggle to recover longer ones.\nSteering vector\ndimension 192 384 576 768\nReconstruction\nBLEU-4 43.43 83.29 93.93 100.00\nTable 4: Reconstruction BLEU for different steering\nvector dimensions. Sentence recovery increases mono-\ntonically as the dimension increases, up to 100% recov-\nery at the model’s hidden dimension.\nSince steering vectors do not depend on se-\nquence length, space complexity may not be a prob-\nlem. For a sequence of length 128, assuming 7\ncharacters per word on average (including spaces),\nstorage as a string takes 128 ∗ 7 = 896 bytes. Our\n768d steering vector uses 1536 bytes (fp16), but we\ncan compress it by a factor of 2 (384d) sacrificing\na little recovery (see Table 4) and store it using 768\nbytes, less than its string representation.\n6.4 Memorization\nOur nearly perfect recoverability performance indi-\ncates that steering vectors could either be encoding\nimportant properties by leveraging the language\nFigure 6: We measure reconstruction BLEU for steer-\ning vectors learned for three datasets: books, shuffled,\nand gibberish. Reconstruction BLEU for gibberish and\nshuffled data is lower than books indicating that the\nsteering vector isn’t just memorizing the sequence, but\nalso leveraging the language model well.\nmodel, which would help generalization, or just\nsimply be memorizing arbitrary sequences with-\nout using the underlying language model at all.\nIn order to evaluate this, we randomly sample 64\nsentences with lengths matching that of the books\nsubset of our dataset, where each token is sampled\nuniformly at random with replacement from the\nvocabulary, and call this the gibberish fold of our\ndataset, following Subramani et al. (2019). Sec-\nondly, to measure whether both content and word\norder affect recoverability, we construct another\nfold, the shuffled fold, by randomly shuffling the\ntokens in the sentences in the books subset.\nFigure 6 shows the results of injecting steering\nvectors into the 6th layer after the self-attention\nblock in the transformer for all timesteps and the\nfirst timestep across all three datasets. We observe\nthat recoverability is highest for books, then shuf-\nfled, and lastly gibberish. The gap between per-\nformance on books and gibberish indicates that\nsteering vectors are not simply memorizing. Since\nrecovery on books is greater than recovery on shuf-\nfled, we conclude that steering vectors encode some\ninformation about word order. Lastly, we notice\nthat only passing the steering vector at the first\ntimestep may reduce unwanted memorization ca-\npability because the relative difference in recovery\nbetween gibberish and the other sets is large.\n6.5 Connection to Prompting\nMotivated by the successes of prompt-based meth-\nods on zero-shot tasks with large generative lan-\n573\nguage models such as GPT-3 (Brown et al., 2020),\nwe evaluate a prompt-based version of our method.\nInstead of adding zsteer to the hidden states of the\nlanguage model, we concatenate k steering vectors\nwith the input embeddings, so that all tokens can\nattend to these zsteer vectors. Experiments on the\nbooks subset show that recovery is much lower\nwith this prompt-based approach than when inject-\ning steering vectors directly into the transformer\nstack of the model. Even with k = 50 steering\nvectors injected via this prompt-based approach,\nrecovery fails to match that of a single steering\nvector zsteer injected into the hidden states of the\nlanguage model.\nNum prompt\nvectors 1 5 10 20 50\nReconstruction\nBLEU-4 81.7 94.3 98.7 98.6 98.5\nTable 5: We measure reconstruction BLEU using a\nprompt-based approach, where latent steering vectors\nare concatenated to the embeddings. Even though each\nprompt vector is 768 dimensional, reconstruction BLEU\nis much lower in this setting than injecting a single\nsteering vector into the layers of the transformer stack.\n7 Related Work\nThere exist many works, often using text-based au-\ntoencoders that try to induce a sentence representa-\ntion space for controllable text generation by learn-\ning new models (Hu et al., 2017; Shen et al., 2017,\n2020; Mai et al., 2020; Montero et al., 2021). Our\nwork concludes that we can extract steering vec-\ntors from pretrained models that have latent spaces\nthat allow operations like this, without having to\ntrain any new models at all. Other approaches\ncontrol language models by adapting their hidden\nstates using steerable layers, adapters, or steering\ntheir logits using auxiliary language models (Gul-\ncehre et al., 2015; Dathathri et al., 2019; Houlsby\net al., 2019; Zhang et al., 2020; Liu et al., 2021;\nKrause et al., 2021). Our method differs from all\nof these: we extract steering vectors directly from\na language model and operate on the latent space\noccupied by these vectors, never fine-tuning any\ncomponent of the model. Subramani et al. (2019)\ninvestigate whether LSTM-based language mod-\nels have sentence representations from which they\ncan generate the original sentence. Although this\npremise relates to our first question: can we ex-\ntract steering vectors, we extend far beyond that\nshowing that vector arithmetic in the context of\nunsupervised style transfer is effective in our latent\nsteering space.\n8 Conclusion\nIn this paper we introduce a different approach\nto controllable text generation, where we extract\nlatent steering vectors directly from a pretrained\nlanguage model without fine-tuning. Further, we\nfind that our steering vectors lead to near perfect\nrecovery on English sentences from a variety of\ndomains. We show that vector arithmetic can be\nused in the context of unsupervised style transfer\non the Yelp sentiment dataset and StylePTB bench-\nmark, performing comparably to models tailored\nto these tasks. Experiments reveal that distances\nbetween steering vectors reflect sentence similarity\nwhen evaluated on STS-B, outperforming extrac-\ntive methods. Finally, we analyze properties of the\nsteering vectors. Our results indicate that we can\ncontrol frozen pretrained language models effec-\ntively through their latent steering space.\n9 Ethics Statement\nWe introduce a new approach for controllable text\ngeneration by extracting vectors from a pretrained\nlanguage model, leveraging information that is al-\nready encoded in the language model. Large pre-\ntrained models are known to be biased and our\nmethod of extracting steering vectors can reflect\nbiases already present in these large pretrained lan-\nguage models (Bender et al., 2021). The methods\nwe present for controllable text generation could po-\ntentially be used for many downstream tasks such\nas unsupervised style transfer, abstractive summa-\nrization, and offensive content removal. Unfortu-\nnately, this also means that this technology has\nthe potential to be misused to perpetuate biases or\ngenerate offensive or toxic text.\nOur technology does not guarantee removal of\ntoxic content, even in the case of unsupervised style\ntransfer from toxic to nontoxic text. To use this\nmethod, we encourage readers to first take steps to\naddress biases that are already present in the un-\nderlying language model. Further we recommend\nthat this technology not be used in high-stakes set-\ntings, especially those where deployment of this\ntechnology could cause harm.\n574\nReferences\nSanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017. A\nsimple but tough-to-beat baseline for sentence em-\nbeddings. In ICLR.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? . FAccT.\nSamuel R Bowman, Luke Vilnis, Oriol Vinyals, An-\ndrew M Dai, Rafal Jozefowicz, and Samy Bengio.\n2016. Generating sentences from a continuous space.\nCoNLL 2016.\nTom B. Brown, Benjamin Pickman Mann, Nick Ry-\nder, Melanie Subbiah, Jean Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish\nSastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-V oss, G. Krüger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel M. Ziegler, Jeffrey\nWu, Clemens Winter, Christopher Hesse, Mark Chen,\nEric J Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. arXiv preprint arXiv:2005.14165.\nIsaac Caswell, Julia Kreutzer, Lisa Wang, Ahsan Wa-\nhab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Al-\nlahsera Auguste Tapo, Nishant Subramani, Artem\nSokolov, Claytone Sikasote, Monang Setyawan,\nSupheakmungkol Sarin, Sokhar Samb, Benoît Sagot,\nClara Rivera, Annette Rios Gonzales, Isabel Papadim-\nitriou, Salomey Osei, Pedro Ortiz Suarez, Iroro Orife,\nKelechi Ogueji, Rubungo Andre Niyongabo, Toan Q.\nNguyen, Mathias Muller, Andr’e Muller, Shamsud-\ndeen Hassan Muhammad, Nanda Firdausi Muham-\nmad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov,\nTapiwanashe Matangira, Colin Leong, Nze Lawson,\nSneha Kudugunta, Yacine Jernite, M. Jenny, Orhan\nFirat, Bonaventure F. P. Dossou, Sakhile Dlamini,\nNisansa de Silva, Sakine cCabuk Balli, Stella Rose\nBiderman, Alessia Battisti, Ahmed Baruwa, Ankur\nBapna, Pallavi N. Baljekar, Israel Abebe Azime, Ay-\nodele Awokoya, Duygu Ataman, Orevaoghene Ahia,\nOghenefego Ahia, Sweta Agrawal, and Mofetoluwa\nAdeyemi. 2022. Quality at a glance: An audit of\nweb-crawled multilingual datasets. TACL.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada. Association for Computational Linguistics.\nBoxing Chen and Colin Cherry. 2014. A systematic\ncomparison of smoothing techniques for sentence-\nlevel bleu. In Proceedings of the Ninth Workshop on\nStatistical Machine Translation.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loïc\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670–680, Copen-\nhagen, Denmark. Association for Computational Lin-\nguistics.\nCristian Danescu-Niculescu-Mizil and Lillian Lee. 2011.\nChameleons in imagined conversations: A new ap-\nproach to understanding coordination of linguistic\nstyle in dialogs. In CMCL@ACL.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2019. Plug and play language models:\nA simple approach to controlled text generation. In\nICLR.\nYuntian Deng, Anton Bakhtin, Myle Ott, and Arthur D.\nSzlam. 2020. Residual energy-based models for text\ngeneration. In ICLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. CoRR.\nKawin Ethayarajh. 2018. Unsupervised random walk\nsentence embeddings: A strong but simple baseline.\nIn Proceedings of The Third Workshop on Represen-\ntation Learning for NLP, pages 91–100, Melbourne,\nAustralia. Association for Computational Linguistics.\nKawin Ethayarajh. 2019. How contextual are contextu-\nalized word representations? Comparing the geom-\netry of BERT, ELMo, and GPT-2 embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 55–65,\nHong Kong, China. Association for Computational\nLinguistics.\nDavid Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.\n2003. English gigaword. Linguistic Data Consor-\ntium, Philadelphia.\nJiatao Gu, Kyunghyun Cho, and Victor O.K. Li. 2017.\nTrainable greedy decoding for neural machine trans-\nlation. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1968–1978, Copenhagen, Denmark. Associa-\ntion for Computational Linguistics.\nCaglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun\nCho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. 2015. On\nusing monolingual corpora in neural machine trans-\nlation. arXiv preprint arXiv:1503.03535.\nJunxian He, Xinyi Wang, Graham Neubig, and Taylor\nBerg-Kirkpatrick. 2020. A probabilistic formulation\nof unsupervised text style transfer. In ICLR.\n575\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In\nICML.\nZhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan\nSalakhutdinov, and Eric P. Xing. 2017. Toward con-\ntrolled generation of text. In ICML.\nDi Jin, Zhijing Jin, Zhiting Hu, Olga Vechtomova, and\nRada Mihalcea. 2020. Deep learning for text style\ntransfer: A survey. ArXiv, abs/2011.00416.\nYoon Kim. 2021. Sequence-to-sequence learning with\nlatent neural grammars. ArXiv, abs/2109.01135.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nBen Krause, Akhilesh Deepak Gotmare, Bryan McCann,\nNitish Shirish Keskar, Shafiq Joty, Richard Socher,\nand Nazneen Fatema Rajani. 2021. GeDi: Gener-\native discriminator guided sequence generation. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021 , pages 4929–4952, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nGuillaume Lample, Sandeep Subramanian,\nEric Michael Smith, Ludovic Denoyer, Marc’Aurelio\nRanzato, and Y-Lan Boureau. 2019. Multiple-\nattribute text rewriting. In ICLR.\nMarie Lebert. 2008. Project gutenberg (1971-2008).\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nYongjie Lin, Yi Chern Tan, and Robert Frank. 2019.\nOpen sesame: Getting inside BERT’s linguistic\nknowledge. In Proceedings of the 2019 ACL Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP, pages 241–253, Florence, Italy.\nAssociation for Computational Linguistics.\nAlisa Liu, Maarten Sap, Ximing Lu, Swabha\nSwayamdipta, Chandra Bhagavatula, Noah A. Smith,\nand Yejin Choi. 2021. DExperts: Decoding-time con-\ntrolled text generation with experts and anti-experts.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n6691–6706, Online. Association for Computational\nLinguistics.\nYiwei Lyu, Paul Pu Liang, Hai Pham, Eduard Hovy,\nBarnabás Póczos, Ruslan Salakhutdinov, and Louis-\nPhilippe Morency. 2021. StylePTB: A compositional\nbenchmark for fine-grained controllable text style\ntransfer. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2116–2138, Online. Association\nfor Computational Linguistics.\nL. V . D. Maaten and Geoffrey E. Hinton. 2008. Visual-\nizing data using t-sne. JMLR.\nFlorian Mai, Nikolaos Pappas, Ivan Montero, Noah A.\nSmith, and James Henderson. 2020. Plug and play\nautoencoders for conditional text generation. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 6076–6092, Online. Association for Computa-\ntional Linguistics.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. ArXiv, abs/1609.07843.\nIvan Montero, Nikolaos Pappas, and Noah A. Smith.\n2021. Sentence bottleneck autoencoders from trans-\nformer language models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1822–1831, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In ACL.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 1532–1543, Doha, Qatar.\nAssociation for Computational Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke S.\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In NAACL-HLT.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. Unpublished ms.\navailable through a link at https://blog.ope\nnai.com/language-unsupervised/.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\n576\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know about\nhow BERT works. Transactions of the Association\nfor Computational Linguistics, 8:842–866.\nAlexis Ross, Tongshuang (Sherry) Wu, Hao Peng,\nMatthew E. Peters, and Matt Gardner. 2021. Tai-\nlor: Generating and perturbing text with semantic\ncontrols. ArXiv, abs/2107.07150.\nTianxiao Shen, Tao Lei, Regina Barzilay, and\nT. Jaakkola. 2017. Style transfer from non-parallel\ntext by cross-alignment. In NIPS.\nTianxiao Shen, Jonas Mueller, Regina Barzilay, and\nT. Jaakkola. 2020. Educating text autoencoders: La-\ntent representation guidance via denoising. In ICML.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric\nWallace, and Sameer Singh. 2020. AutoPrompt: Elic-\niting Knowledge from Language Models with Auto-\nmatically Generated Prompts. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4222–4235,\nOnline. Association for Computational Linguistics.\nNishant Subramani, Samuel Bowman, and Kyunghyun\nCho. 2019. Can unconditional language models re-\ncover arbitrary sentences? In NeurIPS.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS.\nPascal Vincent, H. Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. 2008. Extracting and com-\nposing robust features with denoising autoencoders.\nIn ICML.\nLilian Weng. 2021. Controllable neural text generation.\nlilianweng.github.io/lil-log.\nJeffrey O. Zhang, Alexander Sax, Amir Roshan Za-\nmir, Leonidas J. Guibas, and Jitendra Malik. 2020.\nSide-tuning: A baseline for network adaptation via\nadditive side networks. In ECCV.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. NIPS.\n577\nA Appendix\nA.1 Extracting steering vectors\nIn this section, we show the hyperparameter config-\nurations used for extracting steering vectors from\nGPT2-117M. Table 6 contains the list of final hy-\nperparameters that we use to extract steering vec-\ntors for the different analyses in this paper. Table 7\nshows the recovery performance of steering vec-\ntors when injected at different layers in the trans-\nformer stack on our compiled dataset. These ex-\nperiments reveal that injecting in the middle of the\ntransformer stack either after the self attention layer\nor the feedforward layer leads to the highest BLEU-\n4 performance. In fact, any layer other than the first\nor last layer achieves nearly perfect recovery.\nIn Table 8 we look at recovery performance\nwhen injecting steering vectors at the embedding\nlayer, transformer stack, and language modeling\nhead, as well as different combinations of them.\nInjecting steering vectors at every layer in the trans-\nformer stack performed best. Table 9 shows how\nrecoverability changes with respect to how many\ntimesteps zsteer is injected at. Injecting at all\ntimesteps performs negligibly better than injecting\nat just the first timestep.\nHyperparameters Values\nModel GPT-2-117M\nMax train steps 500\nVector initialization\nstrategy Xavier normal\nLearning rate [0.01, 1.0]\nOptimizer Adam\nLearning rate\nScheduler Decay on a plateau\nScheduler\ndecay factor 0.9\nScheduler\ndecay patience 1.0\nTable 6: List of hyperparameter configurations used to\nextract zsteer from GPT2-117M.\nA.2 Unsupervised Sentiment Transfer\nYelp Sentiment We also include generations\nfrom the unsupervised sentiment transfer exper-\niment on the Yelp dataset. Table 12 shows 8 more\ngenerations. These generations highlight the same\ntrends as before: with increasing λ, sentiment trans-\nfer strength increases. We find that some genera-\nInjection\nlocation layers timestep lr BLEU-4\nself_attn 0 all timesteps 1 33.25\nfeedforward 0 all timesteps 1 97.68\nself_attn 1 all timesteps 1 98.06\nfeedforward 1 all timesteps 1 99.54\nself_attn 2 all timesteps 1 100.00\nfeedforward 2 all timesteps 1 99.69\nself_attn 3 all timesteps 1 100.00\nfeedforward 3 all timesteps 1 100.00\nself_attn 4 all timesteps 1 100.00\nfeedforward 4 all timesteps 1 100.00\nself_attn 5 all timesteps 1 100.00\nfeedforward 5 all timesteps 1 100.00\nself_attn 6 all timesteps 1 100.00\nfeedforward 6 all timesteps 1 99.62\nself_attn 7 all timesteps 1 99.62\nfeedforward 7 all timesteps 1 100.00\nself_attn 8 all timesteps 1 100.00\nfeedforward 8 all timesteps 1 98.84\nself_attn 9 all timesteps 1 99.22\nfeedforward 9 all timesteps 1 98.61\nself_attn 10 all timesteps 1 97.50\nfeedforward 10 all timesteps 1 95.24\nself_attn 11 all timesteps 1 86.04\nfeedforward 11 all timesteps 1 6.29\nTable 7: This table shows the reconstruction BLEU-4\nfor steering vectors from our compiled dataset when\ninjected after different self attention and feedforward\nlayers in the transformer stack. Injecting at the middle\nlayer of the language model performs best.\ntions do more than just flip the sentiment of the\nmajor adjective in the sentence such as adding the\nphrase \"a great way to get a good laugh\" in the 4th\nnegative to positive generation when λ = 2.5.\nStylePTB For this study, we use 19 of 21 paired\nstyle transfer tasks from the StylePTB dataset (Lyu\net al., 2021), but modify the tasks to be unsuper-\nvised, following the same approach as sentiment\ntransfer. We randomly sample 100 sentences for\neach class from the training split for each of the\nstyle classes and use those to compute offset vec-\ntors. This offset vector is then added to the steering\nvector of the sentence to transfer style. We fol-\nlow the evaluation in Lyu et al. (2021) because\nwe have ground truth data and compare with fully\n578\nInjection location timestep lr BLEU-4\nembedding all timesteps 0.01 33.99\nevery_layer all timesteps 0.01 100.00\nlm_head all timesteps 0.01 6.72\nembedding+every_layer all timesteps 0.01 96.52\nevery_layer+lm_head all timesteps 0.01 100.00\nembedding+lm_head all timesteps 0.01 83.27\nembedding+every_layer+lm_head all timesteps 0.01 98.11\nevery_layer_self_attn all timesteps 0.01 99.62\nevery_layer+every_layer_self_attn all timesteps 0.01 100.00\nevery_layer_self_attn+embedding+lm_head all timesteps 0.01 97.31\nevery_layer_self_attn+lm_head all timesteps 0.01 99.62\nevery_layer_self_attn+embedding all timesteps 0.01 94.28\nTable 8: Here, we present the reconstruction BLEU-4 results for steering vectors on our multi-domain compiled\ndataset. We vary injection location here and observe that injecting into the transformer stack is necessary for good\nrecovery. Injecting at the embedding or language model head performs poorly.\nInjection location timestep lr BLEU-4\nevery_layer+every_layer_self_attn all timesteps 0.01 100.0\nevery_layer+every_layer_self_attn first timestep 0.01 91.7\nLayer 7 (feedforward) all timesteps 1 100.0\nLayer 7 (feedforward) first timestep 1 99.2\nLayer 6 (self_attn) all timesteps 1 100.0\nLayer 6 (self_attn) first timestep 1 99.8\nTable 9: In this table, we vary the timestep where we inject zsteer (all timesteps or first timestep) for three of our\nbest injection locations. We again evaluate on our multi-domain compiled dataset and find that injecting at just the\nfirst timestep has a negligible decrease in recovery performance.\nsupervised methods. Experiments show that un-\nsupervised vector arithmetic with steering vectors\nperforms comparably using BLEU-1 to supervised\nmethods designed for style transfer on tasks that re-\nquire minimal edits (adjective emphasis (AEM), ac-\ntive to passive (ATP), information addition (IAD),\nand PP front to back (PFB)). We report BLEU-1\nfollowing prior work. See Table 10 for results on\nall 19 tasks. Note Lyu et al. (2021) do not report\nany baseline numbers for AAR, ASR, LFS, MFS,\nNAR, NSR, and VSR for any of their models.\nA.3 Sampling\nIn order to evaluate whether we can sample steering\nvectors reliably, we collect 4,000 extracted steering\nvectors from the Yelp Sentiment test set. To gen-\nerate, we consider each dimension of the steering\nvector as an independent random variable that is\nnormally distributed. The dimension means and\nvariances are equal to the mean and variance for\nthat dimension across this set of steering vectors.\nIn Table 11, we show the results of sampling 24\nsteering vectors from these independent normally\ndistributed random variables and generating from\nthem using GPT2-117M as our language model.\nThese results are mixed with approximately 20%\nof the generations leading to fully formed sentences\nand the remaining 80% corresponding to individual\nwords or short phrases. This could perhaps be par-\ntially explained by the fact that text from the web,\nincluding the corpora GPT2 was trained on, can\noften be of poor quality, especially when automati-\ncally crawled (Caswell et al., 2022). Alternatively,\nour choice of considering d-dimensional steering\nvectors as samples from d independent normally\ndistributed random variables could be an incorrect\nassumption. Alternative formulations could lead to\nmore fluent and reliable generations.\n579\nOurs: λ = 0.25 GPT2-finetune Seq2seq TAILOR Neural QCFG + copy Retrieve-Edit\nAAR 0.825 - - - - -\nAEM 0.774 0.263 0.187 - 0.676 0.387\nARR 0.721 0.647 0.450 0.781 - 0.897\nASR 0.819 - - - - -\nATP 0.666 0.476 0.373 0.556 0.836 0.681\nIAD 0.772 0.479 0.345 - - 0.493\nLFS 0.396 - - - - -\nMFS 0.748 - - - - -\nNAR 0.825 - - - - -\nNSR 0.677 - - - - -\nPFB 0.819 0.398 0.393 0.842 - 0.541\nPPR 0.393 0.763 0.330 0.717 - 0.798\nPTA 0.574 0.433 0.339 - - 0.714\nSBR 0.120 0.430 0.317 - - 0.706\nTFU 0.699 0.895 0.527 0.873 - 0.899\nTPA 0.478 0.836 0.478 0.884 - 0.935\nTPR 0.692 0.754 0.516 0.710 - 0.909\nVEM 0.548 0.309 0.289 - 0.664 0.416\nVSR 0.739 - - - - -\nTable 10: In this table, we show performance on StylePTB. Although our method is unsupervised, we outperform\nGPT2-finetune and seq2seq on most tasks. For minimal edit tasks such as AEM, ARR, ATP, and PFB, we achieve\ncomparable performance to TAILOR, Neural QCFG + copy, and Retrieve-Edit, which are models trained specifically\nfor these types of tasks. Note: we obtain the numbers for GPT2-finetune, Seq2seq, and Retrieve-Edit from (Lyu\net al., 2021), for TAILOR from (Ross et al., 2021), and for Neural QCFG+copy from (Kim, 2021).\nSampled Sequences\n... mobile\nwine.. the first time that we’ve seen a team that looked\ngood on paper.\npeopled by. Gathering around the world, we can all agree that\nthe next step is to get our voices heard.\nkitchen..... x\nlife item link\nnomnomnomnom appointments\nof kitate.com\nWe’re going to make sure that we have a safe and\nsecure environment for our employees. 3\napp hotel\nracial imagine a world where every day we see a new\nvoice in our communities.\napplify\n(AAP) - The United States and its European allies\nare pressing ahead with plans to boost the number\nof refugees arriving in the country from Iraq and\nSyria.\\n \\nThe United States and its allies are\npressing ahead on the issue as they work to boost\nthe number and scope of refugees arriving in Europe.\niv the best.\nTable 11: Here we show results from our sampling experiment, where we treat steering vectors as samples from d\nindependent normally distributed random variables. We sample 24 steering vectors, pass them to GPT2-117M, and\ndecode, resulting in the 24 generations presented here.\n580\nUnsupervised sentiment transfer using steering vectors\nPositive to negative Negative to positive\ninput i highly recommend this place! input my goodness it was so gross.\n+0.5 ∗ ztonegative i highly recommend this place! +0.5 ∗ ztopositive my goodness it was so gross.\n+1.0 ∗ ztonegative i highly recommend this place! +1.0 ∗ ztopositive my goodness it was so gross.\n+1.5 ∗ ztonegative i highly recommend this place! +1.5 ∗ ztopositive my goodness it was so gross.\n+2.0 ∗ ztonegative i was very disappointed. +2.0 ∗ ztopositive my goodness it was so good.\ninput it is always good to find quality\nlocal spots when traveling. input went here for the first time to try\nsomething new ... bad idea.\n+0.5 ∗ ztonegative it is always good to find quality\nlocal spots when traveling.\n+0.5 ∗ ztopositive went here for the first time to try\nsomething new.\n+1.0 ∗ ztonegative it is always good to find quality\nlocal spots when traveling.\n+1.0 ∗ ztopositive went here for the first time to try\nsomething new.\n+1.5 ∗ ztonegative it is always good to find\nlocal spots when traveling.\n+1.5 ∗ ztopositive went here for the first time to try\nsomething new.\n+2.0 ∗ ztonegative it was always going to be a long time. +2.0 ∗ ztopositive went here for the first time to try\nsomething new. I’m really looking\nforward to trying something new\nfor the first time.\ninput it was delicious! input if i could give them a zero\nstar review i would!\n+0.5 ∗ ztonegative it was delicious! +0.5 ∗ ztopositive if i could give them a star i would!\n+1.0 ∗ ztonegative it was delicious! +1.0 ∗ ztopositive if i could give them a star i would!\n+1.5 ∗ ztonegative it was a very bad night. +1.5 ∗ ztopositive if i could give them a star i would!\n+2.0 ∗ ztonegative it was a very bad night. +2.0 ∗ ztopositive if i could give them a star i would!\ninput the food is fresh and the\nenvironment is good.\ninput fries are n’t worth coming back.\n+0.5 ∗ ztonegative the food is fresh and the\nenvironment is good.\n+0.5 ∗ ztopositive fries are good.\n+1.0 ∗ ztonegative the food is fresh and the\nenvironment is good.\n+1.0 ∗ ztopositive fries are good.\n+1.5 ∗ ztonegative the food is fresh and the\nenvironment is good.\n+1.5 ∗ ztopositive fries are good.\n+2.0 ∗ ztonegative the food is bad. +2.0 ∗ ztopositive fries are good.\n+2.5 ∗ ztonegative the food was produced in the past. +2.5 ∗ ztopositive fries are a great way to get a\ngood laugh.\nTable 12: This table shows some generations from unsupervised sentiment transfer of steering vectors. Sentences\nare from the Yelp dataset. We find that with increasing λ sentiment transfers more strongly towards positive or\nnegative, often switching at λ = 1.5.\n581",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.811102032661438
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.7980377078056335
    },
    {
      "name": "Sentence",
      "score": 0.70377516746521
    },
    {
      "name": "Language model",
      "score": 0.6826815605163574
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5922327637672424
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.5726397037506104
    },
    {
      "name": "Decoding methods",
      "score": 0.5702427625656128
    },
    {
      "name": "Task (project management)",
      "score": 0.5644215941429138
    },
    {
      "name": "Encoding (memory)",
      "score": 0.4944429397583008
    },
    {
      "name": "Sentiment analysis",
      "score": 0.4350099265575409
    },
    {
      "name": "Natural language processing",
      "score": 0.3718159794807434
    },
    {
      "name": "Speech recognition",
      "score": 0.32492703199386597
    },
    {
      "name": "Algorithm",
      "score": 0.07324165105819702
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210156221",
      "name": "Allen Institute for Artificial Intelligence",
      "country": "US"
    }
  ]
}