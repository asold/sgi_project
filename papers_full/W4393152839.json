{
  "title": "Customizing Language Model Responses with Contrastive In-Context Learning",
  "url": "https://openalex.org/W4393152839",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1993071709",
      "name": "Xiang Gao",
      "affiliations": [
        "Intuit (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2130991957",
      "name": "Kamalika Das",
      "affiliations": [
        "Intuit (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1993071709",
      "name": "Xiang Gao",
      "affiliations": [
        "Intuit (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2130991957",
      "name": "Kamalika Das",
      "affiliations": [
        "Intuit (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4308244910",
    "https://openalex.org/W3098258760",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4384616038",
    "https://openalex.org/W3131870090",
    "https://openalex.org/W4385571789",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W4297633153",
    "https://openalex.org/W4361806892",
    "https://openalex.org/W2936695845"
  ],
  "abstract": "Large language models (LLMs) are becoming increasingly important for machine learning applications. However, it can be challenging to align LLMs with our intent, particularly when we want to generate content that is preferable over others or when we want the LLM to respond in a certain style or tone that is hard to describe. To address this challenge, we propose an approach that uses contrastive examples to better describe our intent. This involves providing positive examples that illustrate the true intent, along with negative examples that show what characteristics we want LLMs to avoid. The negative examples can be retrieved from labeled data, written by a human, or generated by the LLM itself. Before generating an answer, we ask the model to analyze the examples to teach itself what to avoid. This reasoning step provides the model with the appropriate articulation of the user's need and guides it towards generting a better answer. We tested our approach on both synthesized and real-world datasets, including StackExchange and Reddit, and found that it significantly improves performance compared to standard few-shot prompting.",
  "full_text": "Customizing Language Model Responses with Contrastive In-Context Learning\nXiang Gao, Kamalika Das\nIntuit AI Research, 2700 Coast Avenue, Mountain View, CA 94043\n{xiang gao, kamalika das}@intuit.com\nAbstract\nLarge language models (LLMs) are becoming increasingly\nimportant for machine learning applications. However, it can\nbe challenging to align LLMs with our intent, particularly\nwhen we want to generate content that is preferable over oth-\ners or when we want the LLM to respond in a certain style\nor tone that is hard to describe. To address this challenge, we\npropose an approach that uses contrastive examples to better\ndescribe our intent. This involves providing positive exam-\nples that illustrate the true intent, along with negative exam-\nples that show what characteristics we want LLMs to avoid.\nThe negative examples can be retrieved from labeled data,\nwritten by a human, or generated by the LLM itself. Before\ngenerating an answer, we ask the model to analyze the exam-\nples to teach itself what to avoid. This reasoning step provides\nthe model with the appropriate articulation of the user’s need\nand guides it towards generting a better answer. We tested\nour approach on both synthesized and real-world datasets, in-\ncluding StackExchange and Reddit, and found that it signifi-\ncantly improves performance compared to standard few-shot\nprompting.\nIntroduction\nIn recent years, large language models like GPT, Llama,\nand PaLM series have made significant progress in natural\nlanguage processing (Bommasani et al. 2021; Brown et al.\n2020; Touvron et al. 2023), enabling them to generate coher-\nent and contextually relevant responses. However, despite\ntheir impressive capabilities, these models can still struggle\nto align with our intent, particularly when it comes to gener-\nating content that is preferable over others or when we want\nthe LLM to respond in a certain style or tone that is hard\nto describe. As a result, there has been a growing interest in\ndeveloping techniques that can help us better steer the out-\nput of LLMs towards our desired goals. In this work, we\npropose an approach that leverages contrastive examples to\nbetter describe our intent, which can significantly improve\nthe performance of LLMs in generating desirable responses.\nIn light of the challenges faced by current LLMs in align-\ning with user intent, it is crucial to develop novel techniques\nthat can effectively guide these models towards generating\nmore desirable responses. Prior research has demonstrated\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Contrastive examples provide positive examples\nthat illustrate the true intent, along with negative examples\nthat show what characteristics we want LLMs to avoid.\nthe benefits of few-shot learning (Brown et al. 2020), fine-\ntuning with smaller models (Gao, Fisch, and Chen 2020),\nselective annotation frameworks (Su et al. 2022), and vi-\nsual language modeling (Alayrac et al. 2022) for enhanc-\ning LLM performance. However, these approaches do not\nexplicitly address the challenge of guiding LLMs to gen-\nerate content that adheres to specific preferences, styles,\nor tones. Additionally, although contrastive learning tech-\nniques have shown promise in areas such as image represen-\ntation (Radford et al. 2021), dialogue response ranking (Gao\net al. 2020), and self-supervised learning (Meng et al. 2021),\ntheir application to content generation in LLMs remains un-\nderexplored. Furthermore, while recent work on prompt op-\ntimization (Honovich et al. 2022; Zhou et al. 2022; Sun et al.\n2023) has highlighted the importance of effective prompts in\nsteering LLMs, there remains a need for more robust meth-\nods that can better capture user intent through diverse and\ncontrastive examples. In this paper, we propose a novel ap-\nproach that addresses these gaps by leveraging contrastive\nexamples, including positive and negative instances, to more\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18039\naccurately define user intent and guide LLMs in generat-\ning responses that are better aligned with desired outcomes.\nBy incorporating this contrastive reasoning step, our method\naims to overcome the limitations of existing techniques and\nsubstantially enhance the performance of LLMs in generat-\ning preferable content.\nOur proposed approach involves providing the LLM with\nboth positive and negative examples to better understand our\nintent (Figure 1). The positive examples showcase the de-\nsired outcomes, while the negative examples highlight what\ncharacteristics the LLM should avoid. By analyzing both\ntypes of examples before generating an answer, the model\ncan reason about our intent and make a more informed deci-\nsion about what to generate. It may be challenging for LLMs\nto understand negative instruction, but we observed that us-\ning negative examples helps aligning our preference with\nLLMs. Moreover, the negative examples can be retrieved\nfrom labeled data, written by a human or generated by the\nmodel itself, making it a flexible and scalable technique. Our\nexperiments show that this approach can significantly im-\nprove the performance of LLMs in generating desirable re-\nsponses, making them more useful for a wide range of natu-\nral language processing applications.\nIn summary, our proposed approach of using contrastive\nexamples to better describe our intent significantly improves\nthe performance of LLMs in generating desirable responses.\nThis approach can help address the challenge of aligning\nLLMs with our intended goals and make them more useful\nfor a wide range of natural language processing applications.\nThe key contributions of this work are:\ni) Providing a novel approach that leverages contrastive\nexamples to improve the performance of LLMs in generat-\ning desirable responses.\nii) Demonstrating the effectiveness of this approach on\nboth synthesized and real-world datasets, including Stack-\nExchange and Reddit.\niii) Highlighting the potential of previously discarded\nnegative examples to make LLMs more useful for a wide\nrange of applications by better aligning them with our in-\ntended goals.\nRelated Work\nRecent advancements in natural language processing have\nled to the development of large language models (LLMs)\nthat are capable of few-shot learning, where they can learn\nnew tasks with only a small number of annotated exam-\nples. Significant works in this area include Brown et al.\n(2020), who demonstrated the impressive few-shot perfor-\nmance of GPT-3, Gao, Fisch, and Chen (2020), who pro-\nposed LM-BFF for fine-tuning smaller language models, Su\net al. (2022), who presented a selective annotation frame-\nwork, and Alayrac et al. (2022), who introduced Flamingo,\na family of visual language models. These works highlight\nthe importance of few-shot learning in LLMs and the need\nfor efficient annotation and prompt generation techniques.\nThe use of contrastive learning for LLMs has also gained\nattention, with Radford et al. (2021) proposing Contrastive\nLanguage-Image Pre-training (CLIP) for learning image\nrepresentations, Gao et al. (2020) leveraging social me-\ndia feedback data for training a dialogue response ranking\nmodel, and Meng et al. (2021) presenting COCO-LM, a\nself-supervised learning framework that pretrains LLMs by\ncorrecting and contrasting corrupted text sequences. These\nworks emphasize the importance of providing diverse and\ncontrastive examples to LLMs and suggest that contrastive\nlearning is a promising direction for LLM research.\nAdditionally, recent works have focused on improving the\nperformance of LLMs by optimizing the prompts used to\nsteer them towards a desired task or outcome. Honovich\net al. (2022) introduce the instruction induction challenge,\nZhou et al. (2022) propose Automatic Prompt Engineer\n(APE) for instruction generation and selection, and Sun et al.\n(2023) present AutoHint, a framework for automatic prompt\nengineering and optimization. To apply these approaches\nfor preference alignment, however, is challenging when the\ncharacteristics are hard to describe in instruction or mea-\nsured by automated metrics.\nOur work builds upon these previous efforts by proposing\nan approach that uses contrastive examples to better describe\nour intent, which we tested on both synthesized and real-\nworld datasets. This approach significantly improves perfor-\nmance compared to standard few-shot prompting, empha-\nsizing the potential for LLMs to become more human-like\nin their ability to generate natural language instructions.\nContrastive In-Context Learning\nOur method comprises two main components: (1) obtaining\npaired positive and negative examples and (2) forming the\nprompt. See Figure 2 for an illustration.\nObtaining Paired Contrastive Examples\nThere are several ways to obtain contrastive examples.\nUsing Labeled Feedback In some tasks, there exist\nmultiple natural outputs for a single input, and feedback for\nthese outputs is available. For example, Reddit and StackEx-\nchange posts typically receive multiple responses, and users\nprovide feedback through upvotes and downvotes. Similarly,\nin business applications like email and copywriting, multiple\nversions might be generated given the same constraints, and\nfeedback (e.g., click-through rate) can be obtained through\nA/B testing or other experiments. For these tasks, we can\nuse the response with the highest feedback as the “posti-\ntive” example and the one with the lowest feedback as the\n“negative” example. It is important to note that a “negative”\nexample here does not necessarily mean it is incorrect or un-\nacceptable, but rather less preferred given the specific audi-\nence and scenario. It is also important to note, that the high-\nest versus lowest votes is an indication of popularity which\nmay or may not represent personal preference, but we are\nusing this popularity (or lack thereof) signal as a surrogate\nfor general preference of a particular response over others,\nignoring confounding factors such as time of response etc.\nUsing LLM-Generated Responses In cases where la-\nbeled feedback is unavailable or negative examples do not\ncapture the characteristics we want LLMs to avoid, we pro-\npose a second method. We let the target LLM generate a\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18040\nFigure 2: Contrastive in-context learning prompting utilize contrastive few-shot examples, which consists of a “postitive” ex-\nample and a “negative” example for each demonstration input, and a prompt to elicit analysis of the characteristics of the\npositive/negative examples, before generating an output for the new input. This strategy can be applied to both conversational\nand non-conversational LLMs. For non-conversational LLMs, the labels and instructions are provided as system messages (the\ngear symbols).\nresponse and use it as the negative example. LLMs gener-\nated responses often appears mechanical, lacking emotion,\nand impersonal. Pairing this with a highly preferred labeled\nresponse as the positive example guides the LLM to generate\na response more aligned with the positive example.\nUsing Automated Evaluator For some tasks, it is pos-\nsible to define automated evaluation rules or apply classifiers\nto measure the quality. The evaluation score can be used to\nselect “positive” and “negative” examples. We demonstrate\nthis with a words-constrained generation task, with accu-\nracy determined by whether all given words were included\n(scored as 1) or not (scored as 0) in the generated sentences.\nForming the Prompt\nOnce we have obtained paired positive and negative exam-\nples, we consider two prompting strategies:\nContrastive Examples as Few-Shot Examples In this\nstrategy, we provide the contrastive example pairs as few-\nshot examples. Labels are included in the prompt, such as\n“preferred answer” for the positive example and “less pre-\nferred answer” for the negative example, to indicate the role\nof each part in the prompt. The LLM is then asked to gener-\nate a “preferred answer” for the new input.\nReasoning and Analysis In addition to the first strategy,\nwe ask the LLM to analyze the reasons for preference and\nthe characteristics of the examples before generating a “pre-\nferred answer” for the new input. This reasoning step is in-\nspired by the Chain-of-Thought prompting (Wei et al. 2022)\nand is designed to summarize the characteristics from the\ncontrastive examples, allowing the LLM to automatically\ngenerate instructions for itself. Coupled with contrastive ex-\namples, this guidance helps the LLM to better align with the\npreferred intent.\nExperiments\nIn this section, we describe the experiments conducted to\nevaluate the effectiveness of our proposed approach using\ncontrastive examples. We focus on text generation tasks\nwhere user preferences play a significant role, rather than\ntasks with objective right or wrong answers. We first out-\nline the datasets used in our experiments, which include both\nreal-world and synthetic datasets.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18041\nDatasets\nWe consider the following datasets to investigate the impact\nof user preferences on text generation tasks:\nHuman Preference Datasets\nUser preference is expressed via different forms of feedback\npublicly available at a large scale on many platforms.\nStackExchange People post questions on StackEx-\nchange, and other users provide answers, with upvotes and\ndownvotes used to rate the responses. Our focus is on\nsubjective preferences, so we created a dataset using data\nfrom cooking.stackexchange.com, which includes how-to\ntype cooking-related questions. Cooking often does not have\nright or wrong answers, and different people have different\npreferences or ways to make a dish they consider good.\nReddit Users post questions or topics on Reddit to trig-\nger discussions, and other users can comment (comments\ncan have nested comments). In our study, we only con-\nsidered the comments of the posts as they share a simi-\nlar context. Redditors can provide upvotes or downvotes as\nfeedback for each post and comment. We experimented a\nsubreddit called ‘NoStupidQuestions‘. ‘NoStupidQuestions‘\nencourages people to ask any question without fear of being\njudged by conventional social norms. The subjects cover a\nwide range of topics, including objective subjects and so-\ncial or personal experiences. These characteristics make this\nsubreddit ideal for our study, as personal (or group collec-\ntive) preference is an essential factor, rather than objective\nright or wrong.\nFor both datasets, We filtered the posts to include those\nwith multiple answers (to have answers with varying “qual-\nity” or level of preference). For the remaining posts, we used\nthe top-rated answer as the “positive” example, and bottom-\nrated answer as the “negative” example. We randomly se-\nlected 500 samples for evaluation.\nSynthetic Stylistic Datasets\nHuman preferences play a crucial role in the afore-\nmentioned real datasets. However, it is challenging to\ninterpret and analyze the preferences, as it is often difficult\neven for humans to articulate what exactly makes an answer\nhighly voted. Therefore, we also considered a few simplified\nsynthetic datasets. Using the cooking.stackexchange.com\ndataset, we leveraged ChatGPT to generate the following\ndatasets: i) Funny vs. Serious, ii) Concise vs. Detailed, and\niii) British vs. American\nWe then used the generated responses as contrastive few-\nshot examples. It is essential to note that by “postitive” and\n“negative”, we do not mean right or wrong, and the choice of\n“postitive” is arbitrary in this case for synthesized datasets.\nThe synthetic datasets aim to illustrate the ability of the pro-\nposed method to guide LLMs to generate output in a given\ndirection.\nConstrained Generation Dataset\nThe datasets mentioned above focus on controlling\nimplicit stylistic aspects of text generation, whether match-\ning a specific attribute such as level of conciseness (as in\nthe synthetic datasets) or an overall holistic style (as in the\nhuman perference datasets). In addition to these datasets,\nwe further experiment with a dataset focused on the control\nof lexical content, constrained generation. This also allows\nus to test the ability of our method to generalize to tasks\nwith explicit constraints.\nFollowing Zhou et al. (2023), the language model is\nprompted to craft sentences using specific seed words as\nconstraints. We randomly selected 500 questions from the\nStackExchange dataset and randomly chose 5 words from\neach question to serve as lexical constraints. We use the\nwrong results generated by the LLM as the negative exam-\nples.\nThe model’s performance is measured by the success rate,\ndefined as the percentage of sentences generated that contain\nall 5 given seed words. Success requires an exact uncased\nmatch between the generated sentence and constraint words.\nPrompt Settings\nIn our experiments, we consider two types of large language\nmodels (LLMs): non-conversational LLMs, such as GPT-\n3, and conversational LLMs, including ChatGPT (GPT-3.5-\nturbo) and GPT-4. We evaluate these LLMs under two dif-\nferent settings: zero-shot and few-shot.\nZero-Shot For non-conversational LLMs, we use a\nprompt consisting of the post with the prefix “Question:”\nfollowed by the second line “Answer:”. For conversational\nLLMs, we use a system prompt stating, “You are a good\nStackExchange/Reddit user,” followed by the user prompt\ncontaining the post. In both cases, we input the post title.\nWe exclude the post body, as it often includes edits after the\noriginal poster has read the replies, potentially leaking pref-\nerence information.\nFew-Shot For each query, we randomly selectk labeled\nexamples as few-shot examples 1. For non-conversational\nLLMs, we include the obtained few-shot examples in the\nprompt, using the “Question:” and “Answer:” prefixes. For\nconversational LLMs, we use the few-shot examples to cre-\nate a conversation history between the user (post) and the\nbot (top-rated reply). We experimented with k = 1to k = 4\nand stops when performance does not increase significantly\nas k increases.\nContrastive Examples Based Prompts\nFor the few-shot setting, we compare the standard ap-\nproach, which only includes positive examples, with three\nsettings involving contrastive examples:\nContrastive Examples Only For each few-shot exam-\nple, we provide a positive and a negative example. For non-\nconversational LLMs, we use the prefixes “top-rated” and\n1We experimented with retrieving few-shot examples based on\nquery embedding similarity but did’t observe significant perfor-\nmance difference compared to randomly selected few-shot exam-\nples\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18042\n“low-rated” to indicate positive and negative examples, re-\nspectively. For conversational LLMs, we use system mes-\nsages (“provide a top-rated answer” and “provide a low-\nrated answer”) to inform the LLMs of the preference.\nContrastive Instruction Only Given the contrastive\nexamples (with labels defined in method 1), we ask LLMs\nto automatically generate an instruction based on the prefer-\nence revealed by the positive and negative examples. A sam-\nple prompt could be, “Summarize the characteristics of the\npreferred and not preferred asnwer.” We then use the gener-\nated analysis, followed by “generate a top-rated answer for\ninput,” as the instruction for the test input. The contrastive\nexamples are only used to generate the analysis and are not\nincluded in the final prompt.\nContrastive Examples + Instruction This method\ncombines the previous two approaches. We first provide\nthe contrastive examples, followed by the instruction ask-\ning LLMs to perform an analysis. Then, the LLM generates\na new response. When generating the final answer, both the\ncontrastive examples and the generated analysis are avail-\nable.\nThese three settings involving contrastive examples allow\nus to conduct an ablation study to examine the contributions\nof specific contrastive examples and more general instruc-\ntions.\nEvaluation Method\nIn order to evaluate the performance of our approach,\nwe consider two distinct methods: reference-based and\nreference-free evaluation.\nReference-Based Evaluation The reference-based eval-\nuation method measures the similarity between the gener-\nated responses and the top-rated reference answer. We em-\nploy two metrics, BERT Score (Zhang et al. 2019) and\nEmb. Similarity. The latter is the cosine similarity of the\nsentence embeddings obtained using the Sentence-BERT\nmodel (Reimers and Gurevych 2019).\nReference-Free Evaluation An answer that deviates\nfrom the reference could still be preferred by many readers.\nTherefore, we also consider two reference-free evaluation\nmethod. i) We leverageDialogRPT (Gao et al. 2020), a pre-\ntrained dialog response ranking model. This model is trained\non Reddit data and shows a high correlation with human up-\nvotes (Gao et al. 2020), so we apply it to the Reddit dataset\nonly. ii) In addition, we employ LLM as an evaluator follow-\ning Liu et al. (2023). We compute a GPT Score by prompt-\ning GPT4 to score the generated results using the positive\nand negative examples as in-context few shot examples. The\nLLM generated score and human evaluation labels exhibit\npositive correlation (StackExchange: 0.65, Reddit: 0.58).\nResults\nWe evaluated the performance of the baselines and con-\ntrastive in-context prompting strategies on the synthetic and\ntwo real datasets. The results are shown in Table 1 and Ta-\nble 2. The “contrastive-combined” approach (which com-\nbines the contrastive examples and the derived instruction)\nFigure 3: The dependence of the performance on prompt\nlength. For standard few-shot, we experimented with k =\n1, 2, 3, 4, shown by the four black dots from left to right in\neach subfigure. For contrastive prompts, empty circles do-\nnates prompts using generated answers as negative exam-\nples, and filled symbols for prompts using low-rated human\nanswers as negative examples.\nachieved the best performance for most cases. Using simi-\nlar number of tokens in prompt, this contrastive in-context\nlearning approach performs significantly better than stan-\ndard few-shot approach, as shown in Figure 3. For Red-\ndit dataset, we observe the most obvious improvement with\nChatGPT model. Standard two-shot prompt only performs\nbetter than zero-shot approach for about 64 % test cases,\nwhile the “contrastive-combined” approach wins 76 % test\ncases. For StackExchange, standard few-shot approaches\ndo not show obvious improvement compared to zero-shot,\nwhile “contrastive-combined” improve BERT Score by ap-\nproximately 0.01. For synthetic datasets, the improvement\nof is more obvious, with BERT Score increased by 0.02 to\n0.03, and much higher than the improvement achieved with\nstandard few-shot prompts. As an ablation study, we also\nevaluated the results for “contrastive - examples only” and\n“contrastive - instruction only”. Both approaches improve\nthe performance compared to zero shot, and the “contrastive\n- examples only” performs better than “contrastive - instruc-\ntion only” for many cases. Combining them together further\nimproves the results, as discussed above.\nThe Impact of Contrastive Examples As mentioned\nabove, using contrastive in-context learning by introducing\nnegative examples improve the performance compared to\nstandard few-shot approaches. We futher investigate differ-\nent ways to obtain negative examples. The first method, “hu-\nman”, uses human-written answers, where low-rated replies\nserve as “negative” examples. The second method, “gen-\nerated”, uses zero-shot generated replies as “negative” ex-\namples. Interestingly, we observed that the second method\nperforms on par with, and sometimes even better than, the\nfirst method (see Tables 1 and 2). This finding demonstrates\nthat the proposed contrastive method is not limited to cases\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18043\nDataset Funny vs. Serious Concise vs. Detailed British vs. American\nMethod Emb. Similarity BERT Score Emb. Similarity BERT Score Emb. Similarity BERT Score\nGPT-3\nZero-shot 0.606 0.858 0.849 0.903 0.785 0.883\nFew-shot, k=1 0.611 0.863 0.887 0.916 0.813 0.892\nFew-shot, k=2 0.629 0.864 0.886 0.920 0.808 0.887\nContrastive, k=1 human gen. human gen. human gen. human gen. human gen. human gen.\n- Examples only 0.614 0.620 0.857 0.859 0.817 0.865 0.895 0.910 0.827 0.807 0.891 0.885\n- Instruction only 0.645 0.595 0.865 0.861 0.834 0.845 0.895 0.902 0.824 0.784 0.888 0.879\n- Combined 0.655 0.617 0.864 0.863 0.869 0.897 0.910 0.922 0.826 0.838 0.896 0.891\nChatGPT\nZero-shot 0.604 0.841 0.826 0.878 0.783 0.867\nFew-shot, k=1 0.613 0.842 0.829 0.880 0.807 0.870\nFew-shot, k=2 0.610 0.846 0.834 0.884 0.806 0.874\nContrastive, k=1 human gen. human gen. human gen. human gen. human gen. human gen.\n- Examples only 0.608 0.599 0.847 0.844 0.850 0.856 0.881 0.891 0.825 0.818 0.882 0.880\n- Instruction only 0.617 0.617 0.849 0.850 0.831 0.835 0.882 0.888 0.819 0.821 0.876 0.876\n- Combined 0.638 0.630 0.862 0.862 0.873 0.883 0.914 0.915 0.830 0.827 0.891 0.891\nGPT-4\nZero-shot 0.596 0.838 0.803 0.873 0.786 0.864\nFew-shot, k=1 0.607 0.843 0.805 0.877 0.792 0.867\nFew-shot, k=2 0.624 0.846 0.809 0.880 0.807 0.868\nContrastive, k=1 human gen. human gen. human gen. human gen. human gen. human gen.\n- Examples only 0.609 0.611 0.844 0.843 0.828 0.855 0.888 0.893 0.817 0.821 0.870 0.871\n- Instruction only 0.624 0.612 0.843 0.841 0.853 0.852 0.893 0.895 0.821 0.820 0.868 0.870\n- Combined 0.616 0.630 0.855 0.857 0.875 0.853 0.919 0.917 0.813 0.824 0.886 0.884\nTable 1: Results on the synthetic dataset. k is the number of demonstration inputs. The “negative” examples are obtained via\ntwo methods, “human” and “gen.” (generated by the LLM itself).\nwhere developers have to provide human-written pairs of\npositive and negative examples. Instead, it requires the same\ninputs as standard few-shot settings, which only need a few\npositive examples since we can generate the negative exam-\nples.\nThis observation also supports our assumption that neg-\native examples obtained from human-written data may not\ncapture all characteristics we want LLMs to avoid. For in-\nstance, in the Reddit dataset, a human-written reply may\nreceive downvotes because the content is rude, disrespect-\nful, or violates some rules of the subreddit, such as self-\npromotion. However, recent LLMs like ChatGPT are gen-\nerally trained to follow social norms and sometimes even\napologize too frequently. As a result, it is highly unlikely\nfor these LLMs to generate offensive language. Providing\nhuman-written negative examples, therefore, may not of-\nfer much helpful signal in such cases. In contrast, provid-\ning generated responses as negative examples can supply\nthe signals of certain characteristics we want the LLM to\navoid. For example, although LLM-generated responses are\ntypically fluent and relevant to the question, they often lack\nemotion, details, examples, or elements that trigger readers’\npersonal feelings. These characteristics, however, are essen-\ntial factors that make users prefer certain answers. By pro-\nviding zero-shot generated responses, we can guide LLMs\nto move away from the machine-generated style and toward\na more human-preferred style.\nDistilling Contrastive Examples as an Instruction Al-\nthough the primary focus of this work is not to automat-\nically generate instructions for LLMs, we were interested\nin understanding what instructions LLMs could derive from\nthe contrastive examples. Therefore, we asked the LLMs\nto summarize the characteristics of the provided “postitive”\nand “negative” examples. We then used this analysis in the\nprompt, instead of the actual contrastive examples, to offer\nthe LLMs insight into user preferences. This compress the\ncontrastive examples into a shorter instruction, and reduce\nprompt length and cost.\nOur experimental results showed that, with this analysis,\nLLMs performed better than in the zero-shot setting (see Ta-\nbles 1 and 2). However, the improvement was not signifi-\ncantly better than the standard few-shot setting. This could\nbe partially explained by the fact that the preference was\nnot consistent across the examples chosen based on upvotes.\nThere is room for enhancement in the way the preference\nis summarized through the use of automated prompt gen-\neration approaches (Sun et al. 2023). By refining the way\nLLMs derive instructions from contrastive examples, we can\npotentially achieve more significant improvements in perfor-\nmance compared to standard few-shot prompting.\nCombining the Complementary Parts We observed\nthat when combining the contrastive examples and the in-\nstruction (analysis of the characteristics of “postitive” and\n“negative” examples automatically generated based on the\ncontrastive examples) together, an even better performance\nwas achieved (see Tables 1 and 2). This improved perfor-\nmance can be attributed to the fact that the analysis and the\ncontrastive examples provide complementary information to\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18044\nDataset StackExchange Reddit Constrained Gen.\nMethod BERT Score GPT Score DialogRPT score GPT Score Success rate\nGPT-3\nZero-shot 0.840 0.550 0.605 0.533 0.750\nFew-shot, k=1 0.841 0.555 0.596 0.531 0.760\nFew-shot, k=2 0.841 0.561 0.610 0.540 0.772\nContrastive, k=1 human generated human generated human generated human generated\nExamples only 0.846 0.845 0.605 0.600 0.630 0.632 0.634 0.638 0.801\nInstruction only 0.845 0.846 0.605 0.601 0.606 0.610 0.621 0.630 0.790\nCombined 0.847 0.846 0.608 0.606 0.631 0.632 0.635 0.640 0.830\np-value (0.047) (0.049) (0.018) (0.023) (0.030) (0.030) (0.020) (0.019) (0.021)\nChatGPT\nZero-shot 0.839 0.550 0.602 0.531 0.780\nFew-shot, k=1 0.839 0.561 0.636 0.578 0.800\nFew-shot, k=2 0.839 0.570 0.645 0.583 0.810\nContrastive, k=1 human generated human generated human generated d human generated\nExamples only 0.844 0.842 0.592 0.589 0.654 0.663 0.654 0.659 0.866\nInstruction only 0.844 0.843 0.598 0.597 0.640 0.645 0.640 0.642 0.840\nCombined 0.845 0.846 0.601 0.603 0.656 0.663 0.690 0.701 0.872\n(p-value) (0.048) (0.044) (0.033) (0.031) (0.048) (0.041) (0.023) (0.021) (0.045)\nGPT-4\nZero-shot 0.839 0.575 0.657 0.580 0.850\nFew-shot, k=1 0.840 0.578 0.655 0.585 0.870\nFew-shot, k=2 0.841 0.582 0.656 0.590 0.900\nContrastive, k=1 human generated human generated human generated human generated\nExamples only 0.844 0.842 0.595 0.590 0.658 0.665 0.660 0.671 0.923\nInstruction only 0.842 0.842 0.590 0.591 0.658 0.660 0.661 0.672 0.903\nCombined 0.847 0.845 0.610 0.607 0.660 0.665 0.665 0.670 0.943\n(p-value) (0.047) (0.049) (0.023) (0.025) (0.043) (0.040) (0.034) (0.032) (0.045)\nTable 2: Results on real-world datasets.k is the number of demonstration inputs. The “negative” examples are obtained via two\nmethods, “human” and “generated”. p-value indicate the statistical significance of the comparison between the “Combined”\ncontrastive prompting and the standard few-shot (k = 2) prompting.\nguide the LLM about user preferences.\nThe generated instruction exhibits better generalization\nbut may lack the necessary clarity and specificity. This is be-\ncause the instructions can be vague or provide only general\ndescriptions, which can be challenging for LLMs to inter-\npret. In contrast, the actual contrastive examples offer more\ndetailed information by providing demonstrations of the de-\nsired and undesired characteristics.\nIn summary, our approach of using contrastive examples\nalong with an analysis of the characteristics of positive and\nnegative examples has proven to be a valuable method for\naligning LLMs with user prference. This approach not only\nenhances performance but also provides LLMs with a better\nunderstanding of the user’s preferences, resulting in more\naccurate and satisfactory responses.\nPrompt Token Efficiency In real-world and industrial\napplications, the number of prompt tokens plays a crucial\nrole as it is directly related to latency and monetary cost.\nIdeally, a good prompt should be short yet lead to high-\nquality output. Therefore, we measure the token efficiency\nto understand the effectiveness of our approach. As shown\nin Figure 3, the standard few-shot strategies do exhibit an\nincrease in performance as we increase the number of ex-\namples. However, the performance, as measured by BERT\nscore, is still lower than the contrastive in-context learning\nstrategies using a similar number of tokens. Furthermore, the\ncontrastive instruction strategy, which summarizes the con-\ntrastive examples as an instruction, utilizes fewer tokens but\nachieves better performance than few-shot examples. This\nindicates that, given the same budget of prompt tokens, con-\ntrastive in-context learning strategies outperform the stan-\ndard few-shot approaches, thereby demonstrating higher ef-\nficiency.\nConclusion\nThis research introduced an approach to enhance the align-\nment of large language models (LLMs) with user preference\nusing contrastive examples. By integrating positive exam-\nples showcasing desired outputs and negative ones empha-\nsizing unwanted LLM traits, we tested our methodology on\nboth synthesized and real-world datasets, including Stack-\nExchange and Reddit.\nThe results confirmed the superiority of contrastive ex-\namples over the standard few-shot prompting, particularly\nin terms of performance and prompt token efficiency. No-\ntably, negative examples generated from zero-shot outputs\nwere as effective as those from human-written data. Future\nendeavors might delve into refining LLM instructions based\non these examples and innovating automatic prompt gener-\nation techniques.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18045\nReferences\nAlayrac, J.-B.; Donahue, J.; Luc, P.; Miech, A.; Barr, I.; Has-\nson, Y .; Lenc, K.; Mensch, A.; Millican, K.; Reynolds, M.;\net al. 2022. Flamingo: a visual language model for few-shot\nlearning. Advances in Neural Information Processing Sys-\ntems, 35: 23716–23736.\nBommasani, R.; Hudson, D. A.; Adeli, E.; Altman, R.;\nArora, S.; von Arx, S.; Bernstein, M. S.; Bohg, J.; Bosselut,\nA.; Brunskill, E.; et al. 2021. On the opportunities and risks\nof foundation models. arXiv preprint arXiv:2108.07258.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems, 33: 1877–\n1901.\nGao, T.; Fisch, A.; and Chen, D. 2020. Making pre-trained\nlanguage models better few-shot learners. arXiv preprint\narXiv:2012.15723.\nGao, X.; Zhang, Y .; Galley, M.; Brockett, C.; and Dolan, B.\n2020. Dialogue response ranking training with large-scale\nhuman feedback data. arXiv preprint arXiv:2009.06978.\nHonovich, O.; Shaham, U.; Bowman, S. R.; and Levy,\nO. 2022. Instruction induction: From few examples\nto natural language task descriptions. arXiv preprint\narXiv:2205.10782.\nLiu, Y .; Iter, D.; Xu, Y .; Wang, S.; Xu, R.; and Zhu, C. 2023.\nGpteval: Nlg evaluation using gpt-4 with better human align-\nment. arXiv preprint arXiv:2303.16634.\nMeng, Y .; Xiong, C.; Bajaj, P.; Bennett, P.; Han, J.; Song,\nX.; et al. 2021. Coco-lm: Correcting and contrasting text se-\nquences for language model pretraining.Advances in Neural\nInformation Processing Systems, 34: 23102–23114.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from nat-\nural language supervision. In International conference on\nmachine learning, 8748–8763. PMLR.\nReimers, N.; and Gurevych, I. 2019. Sentence-bert: Sen-\ntence embeddings using siamese bert-networks. arXiv\npreprint arXiv:1908.10084.\nSu, H.; Kasai, J.; Wu, C. H.; Shi, W.; Wang, T.; Xin, J.;\nZhang, R.; Ostendorf, M.; Zettlemoyer, L.; Smith, N. A.;\net al. 2022. Selective annotation makes language models\nbetter few-shot learners. arXiv preprint arXiv:2209.01975.\nSun, H.; Li, X.; Xu, Y .; Homma, Y .; Cao, Q.; Wu, M.;\nJiao, J.; and Charles, D. 2023. AutoHint: Automatic\nPrompt Optimization with Hint Generation. arXiv preprint\narXiv:2307.07415.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023. Llama: Open and efficient founda-\ntion language models. arXiv preprint arXiv:2302.13971.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;\nChi, E.; Le, Q. V .; Zhou, D.; et al. 2022. Chain-of-\nthought prompting elicits reasoning in large language mod-\nels. Advances in Neural Information Processing Systems,\n35: 24824–24837.\nZhang, T.; Kishore, V .; Wu, F.; Weinberger, K. Q.; and Artzi,\nY . 2019. Bertscore: Evaluating text generation with bert.\narXiv preprint arXiv:1904.09675.\nZhou, Y .; Muresanu, A. I.; Han, Z.; Paster, K.; Pitis,\nS.; Chan, H.; and Ba, J. 2022. Large language mod-\nels are human-level prompt engineers. arXiv preprint\narXiv:2211.01910.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18046",
  "topic": "Context (archaeology)",
  "concepts": [
    {
      "name": "Context (archaeology)",
      "score": 0.5926965475082397
    },
    {
      "name": "Computer science",
      "score": 0.5776647329330444
    },
    {
      "name": "Contrastive analysis",
      "score": 0.46727028489112854
    },
    {
      "name": "Linguistics",
      "score": 0.44773852825164795
    },
    {
      "name": "Language acquisition",
      "score": 0.43682026863098145
    },
    {
      "name": "Psychology",
      "score": 0.42083922028541565
    },
    {
      "name": "Natural language processing",
      "score": 0.38204148411750793
    },
    {
      "name": "Mathematics education",
      "score": 0.11870929598808289
    },
    {
      "name": "History",
      "score": 0.09636718034744263
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I88773910",
      "name": "Intuit (United States)",
      "country": "US"
    }
  ],
  "cited_by": 4
}