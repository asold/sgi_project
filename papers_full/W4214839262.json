{
  "title": "Medical Image Segmentation Using Transformer Networks",
  "url": "https://openalex.org/W4214839262",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2104215118",
      "name": "Davood Karimi",
      "affiliations": [
        "Harvard University",
        "Boston Children's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2119574533",
      "name": "Haoran Dou",
      "affiliations": [
        "University of Leeds"
      ]
    },
    {
      "id": "https://openalex.org/A1943132322",
      "name": "Ali Gholipour",
      "affiliations": [
        "Boston Children's Hospital",
        "Harvard University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2036885187",
    "https://openalex.org/W91070975",
    "https://openalex.org/W2065875833",
    "https://openalex.org/W2033358375",
    "https://openalex.org/W1597166651",
    "https://openalex.org/W2035964780",
    "https://openalex.org/W2395611524",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W6755891590",
    "https://openalex.org/W2804047627",
    "https://openalex.org/W2301358467",
    "https://openalex.org/W2959828872",
    "https://openalex.org/W2979999210",
    "https://openalex.org/W2805284222",
    "https://openalex.org/W2947263797",
    "https://openalex.org/W3035665735",
    "https://openalex.org/W2594179300",
    "https://openalex.org/W6778380380",
    "https://openalex.org/W2807122651",
    "https://openalex.org/W2886943560",
    "https://openalex.org/W6790275670",
    "https://openalex.org/W2620296437",
    "https://openalex.org/W2803522971",
    "https://openalex.org/W2963717741",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2154579312",
    "https://openalex.org/W2145889472",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W2884436604",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W6750469568",
    "https://openalex.org/W2888493720",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W3115552777",
    "https://openalex.org/W6684665197",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W3157259822",
    "https://openalex.org/W2751665805",
    "https://openalex.org/W2910094941",
    "https://openalex.org/W6761292208",
    "https://openalex.org/W6795140394",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3102446692"
  ],
  "abstract": "Deep learning models represent the state of the art in medical image segmentation. Most of these models are fully-convolutional networks (FCNs), namely each layer processes the output of the preceding layer with convolution operations. The convolution operation enjoys several important properties such as sparse interactions, parameter sharing, and translation equivariance. Because of these properties, FCNs possess a strong and useful inductive bias for image modeling and analysis. However, they also have certain important shortcomings, such as performing a fixed and pre-determined operation on a test image regardless of its content and difficulty in modeling long-range interactions. In this work we show that a different deep neural network architecture, based entirely on self-attention between neighboring image patches and without any convolution operations, can achieve more accurate segmentations than FCNs. Our proposed model is based directly on the transformer network architecture. Given a 3D image block, our network divides it into non-overlapping 3D patches and computes a 1D embedding for each patch. The network predicts the segmentation map for the block based on the self-attention between these patch embeddings. Furthermore, in order to address the common problem of scarcity of labeled medical images, we propose methods for pre-training this model on large corpora of unlabeled images. Our experiments show that the proposed model can achieve segmentation accuracies that are better than several state of the art FCN architectures on two datasets. Our proposed network can be trained using only tens of labeled images. Moreover, with the proposed pre-training strategies, our network outperforms FCNs when labeled training data is small.",
  "full_text": "Received January 21, 2022, accepted February 24, 2022, date of publication March 4, 2022, date of current version March 21, 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.3156894\nMedical Image Segmentation Using\nTransformer Networks\nDAVOOD KARIMI\n 1, HAORAN DOU2, AND ALI GHOLIPOUR\n1, (Senior Member, IEEE)\n1Department of Radiology, Boston Children’s Hospital, Harvard Medical School, Boston, MA 02115, USA\n2Centre for Computational Imaging & Simulation Technologies in Biomedicine (CISTIB), School of Computing, University of Leeds, Leeds LS2 9JT, U.K.\nCorresponding author: Davood Karimi (davood.karimi@childrens.harvard.edu)\nThis work was supported in part by the National Institute of Biomedical Imaging and Bioengineering and the National Institute of\nNeurological Disorders and Stroke of the National Institutes of Health (NIH) under Award R01EB031849, Award R01NS106030, and\nAward R01EB032366; in part by the Ofﬁce of the Director of the NIH under Award S10OD0250111; and in part by a Technological\nInnovations in Neuroscience Award from the McKnight Foundation. The work of Haoran Dou was supported by the EPSRC Doctoral\nTraining Partnership (DTP) Studentship. The content of this publication is solely the responsibility of the authors and does not necessarily\nrepresent the ofﬁcial views of the NIH or the McKnight Foundation.\nABSTRACT Deep learning models represent the state of the art in medical image segmentation. Most of\nthese models are fully-convolutional networks (FCNs), namely each layer processes the output of the preced-\ning layer with convolution operations. The convolution operation enjoys several important properties such\nas sparse interactions, parameter sharing, and translation equivariance. Because of these properties, FCNs\npossess a strong and useful inductive bias for image modeling and analysis. However, they also have certain\nimportant shortcomings, such as performing a ﬁxed and pre-determined operation on a test image regardless\nof its content and difﬁculty in modeling long-range interactions. In this work we show that a different deep\nneural network architecture, based entirely on self-attention between neighboring image patches and without\nany convolution operations, can achieve more accurate segmentations than FCNs. Our proposed model is\nbased directly on the transformer network architecture. Given a 3D image block, our network divides it\ninto non-overlapping 3D patches and computes a 1D embedding for each patch. The network predicts the\nsegmentation map for the block based on the self-attention between these patch embeddings. Furthermore,\nin order to address the common problem of scarcity of labeled medical images, we propose methods for pre-\ntraining this model on large corpora of unlabeled images. Our experiments show that the proposed model can\nachieve segmentation accuracies that are better than several state of the art FCN architectures on two datasets.\nOur proposed network can be trained using only tens of labeled images. Moreover, with the proposed pre-\ntraining strategies, our network outperforms FCNs when labeled training data is small.\nINDEX TERMS Deep learning, medical image segmentation, self-attention, transformer networks.\nI. INTRODUCTION\nImage segmentation is needed for quantifying the size and\nshape of the volume/organ of interest, population studies,\ndisease quantiﬁcation, and computer-aided treatment and sur-\ngical planning. Given the importance and the difﬁculty of\nthis task in medical applications, manual segmentation by\na medical expert is regarded as the ground truth. However,\nmanual segmentation is costly, time-consuming, and subject\nto inter and intra-observer disagreement. Automatic segmen-\ntation methods, on the other hand, have the potential to offer\nmuch faster, cheaper, and more reproducible results.\nClassical techniques for medical image segmentation\ninclude region growing [1], deformable models [2], graph\ncuts [3], clustering methods [4], and Bayesian approaches [5].\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was G. R. Sinha\n.\nAtlas-based methods are another very popular and powerful\nset of techniques [6]. With the introduction of deep learn-\ning methods for image segmentation [7], [8], these meth-\nods were quickly adopted for medical image segmentation.\nDeep learning methods have achieved unprecedented levels\nof performance on a range of medical image segmentation\ntasks [9]–[14]. One can argue that deep learning methods\nhave largely replaced the classical methods for medical image\nsegmentation.\nRecent reviews of the main lines of research and recent\nadvancements on the application of deep learning for med-\nical image segmentation can be found in [15], [16]. Most\nrecent studies have aimed at improving the network archi-\ntecture, loss function, and training procedures. Recent works\nhave shown that standard deep learning models can be\ntrained using small numbers of labeled training images [17],\n[18]. Despite the large variability in the proposed network\n29322\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ VOLUME 10, 2022\nD. Karimiet al.: Medical Image Segmentation Using Transformer Networks\narchitectures, the one common feature in all of these works is\nthat they all use the convolution operation as the main build-\ning block. The proposed architectures differ with regard to the\narrangement of the convolutional operations, but they all rely\non the same basic convolution operation. A few studies have\nproposed alternative network architectures based on recurrent\nneural networks [19], [20] and attention mechanisms [21].\nThere have also been attempts to improve the accuracy and\nrobustness of these methods by modeling the statistical vari-\nation in the shape of the organ of interest and incorporating\nthis shape information in the deep learning method [12],\n[22], [23]. However, all of those models still build upon the\nconvolution operation. Some recent studies have suggested\nthat a basic encoder-decoder-type fully convolutional net-\nwork (FCN) can handle various segmentation tasks and be\nas accurate as more elaborate network architectures [24].\nThe convolution operation is also the main building block\nof the network architectures that have successfully addressed\nother central computer vision tasks such as image classiﬁca-\ntion and object detection [25], [26]. These results attest to the\neffectiveness of the convolution operation for modeling and\nanalyzing images. This effectiveness has been attributed to\na number of key properties, including: 1) local (sparse) con-\nnections, 2) parameter sharing, and 3) translation equivari-\nance [27], [28]. In fact, a convolutional layer can be regarded\nas a fully connected layer with an ‘‘inﬁnitely strong prior’’\nover its parameters [29].\nThe properties of the convolution operation that we men-\ntioned above are, in part, inspired by neuroscience of the\nmammalian primary visual cortex [30]. They give convolu-\ntional neural networks (CNNs), including FCNs, a strong and\nuseful inductive bias, which makes them highly effective and\nefﬁcient in tackling different vision tasks. However, these\nsame properties also put CNNs at some disadvantage. For\nexample, the network weights are determined at training time\nand subsequently they are ﬁxed. Therefore, these networks\ntreat different images and different parts of an image equally.\nIn other words, they lack a mechanism to change their weights\ndepending on the image content. Furthermore, due to the\nlocal nature of convolution operations with small kernel sizes,\nCNNs cannot easily learn long-range interactions between\ndistant parts of an image.\nAttention-based neural network models have the potential\nto address some of the limitations of convolution-based mod-\nels. In short, these models aim at learning the relationship\nbetween different parts of a sequence [31]. Most importantly,\nunlike CNNs, in attention-based networks not all network\nweights are ﬁxed upon training. Rather, only a portion of\nthe network weights are learned from training data and the\nrest of the weights are determined at test time based on the\ncontent of the input. Attention-based networks have become\nthe dominant neural network architectures in natural language\nprocessing (NLP) applications. Transformers are the most\ncommon attention-based models in NLP [31]. Compared with\nrecurrent neural networks, transformers can learn more com-\nplex and longer-range interactions much more effectively.\nMoreover, they overcome some of the central limitations of\nrecurrent neural networks such as vanishing gradients. They\nalso allow for parallel processing of inputs, which can lead to\nsigniﬁcantly shorter training time on modern hardware.\nDespite the potential advantages of transformer networks,\nso far they have not been widely adopted in computer vision\napplications. A recent survey of the relevant works on this\ntopic can be found in [32]. Application of attention-based\nneural networks for computer vision applications faces sev-\neral important challenges. The number of pixels in a typical\nimage is much larger than the length of a signal sequence\n(e.g., number of words) in typical NLP applications. This\nmakes it impossible to directly apply standard attention mod-\nels to images. The second main reason has been the training\ndifﬁculty. The strong inductive bias of CNNs that we have\nmentioned above makes them highly data-efﬁcient. Trans-\nformer networks, on the other hand, require much more train-\ning data because they incorporate minimal inductive bias.\nRecent studies have proposed practical solutions to these\ntwo challenges. To address the ﬁrst challenge, vision trans-\nformer (ViT) proposed considering image patches, rather\nthan pixels, as the units of information in an image [33].\nViT embeds image patches into a shared space and learns\nthe relation between these embeddings using self-attention\nmodules. It was shown that, given massive datasets of labeled\nimages and vast computational resources, ViT could surpass\nCNNs in image classiﬁcation accuracy. One possible solution\nto the second challenge was proposed in [34], where the\nauthors used knowledge distillation from a CNN teacher to\ntrain a transformer network. It was shown that with this\ntraining strategy, transformer networks could achieve image\nclassiﬁcation accuracy levels on par with CNNs using the\nsame amount of labeled training data [34].\nIn this work, we propose a self-attention-based deep neural\nnetwork for 3D medical image segmentation. Our proposed\nnetwork is based on self-attention between linear embeddings\nof 3D image patches, without any convolution operations.\nGiven the fact that self-attention models generally require\nlarge labeled training datasets, we also propose unsupervised\npre-training methods that can exploit large unlabeled medical\nimage datasets. We compare our proposed model with several\nstate of the art FCNs on two medical image segmentation\ndatasets.\nThe speciﬁc contributions of this work are as follows:\n1) We propose the ﬁrst convolution-free deep neural net-\nwork architecture for segmentation of 3D medical\nimages.\n2) We show that our proposed network can achieve seg-\nmentation performance levels that are better than or\nat least on par with the state of the art FCNs. Even\nthough prior works have suggested that massive labeled\ntraining datasets are needed to effectively train trans-\nformer networks for NLP and vision applications, we\nexperimentally show that our network can be trained\nusing datasets of only ∼20 −200 labeled images.\n3) We propose methods for pre-training our network on\nlarge corpora of unlabeled images. We show that when\nlabeled training images are fewer in number, with these\nVOLUME 10, 2022 29323\nD. Karimiet al.: Medical Image Segmentation Using Transformer Networks\npre-training strategies, our network performs better\nthan a state of the art FCN with pre-training.\nII. MATERIALS AND METHODS\nA. PROPOSED NETWORK\nOur proposed transformer network for 3D medical image\nsegmentation is shown in Figure 1. The input to our network\nis a 3D image block B ∈ IRW ×W ×W ×c, where W denotes\nthe extent of the block (in voxels) in each dimension and c\ndenotes the number of image channels. Working with image\nsub-blocks is a common approach in processing large vol-\numetric images. It enables processing of large images of\narbitrary size on limited GPU memory. Furthermore, it func-\ntions as an implicit data augmentation method because during\ntraining sub-blocks are sampled from random locations in the\ntraining images.\nThe image block B is divided into n3 non-overlapping 3D\npatches {pi ∈IRw×w×w×c}N\ni=1, where w =W /n is the side\nlength of each patch and N = n3 denotes the number of\npatches in the block. In the experiments presented in this\npaper we choose n ∈{3, 4, 5}, resulting in N ∈{27, 64, 125}\npatches in each block. The proposed transformer network\nembeds each patch into a lower-dimensional space and pre-\ndicts the segmentation map corresponding to the image block\nB based on the self-attention between these embeddings. The\nsteps of the proposed method are described below.\nEach of the N patches {pi}N\ni=1 is ﬁrst reshaped into a\nvector of size IR w3c and embedded into IR D using a train-\nable linear mapping E ∈ IRD×w3c. This step is similar\nto the ﬁrst step in the ViT model for image classiﬁcation.\nThe ViT model appended an extra ‘‘class token’’ to the\nsequence of embedded patches. This class token is inherited\nfrom NLP applications. We did not use such a token in the\nexperiments presented in this work because our preliminary\nexperiments showed that it did not improve the segmenta-\ntion performance of our network in any way. Hence, the\nsequence of embedded patches X0 =[Ep1;. . .;EpN ] +Epos\nconstitutes the input to our transformer network. The matrix\nEpos ∈IRD×N , which is added to the embedded patches is\nintended to learn a positional encoding. This is a common\nfeatures of self-attention models because the attention mech-\nanism is permutation-invariant. In other words, without such\npositional information, the transformer network ignores the\nordering of the patches in the input sequence. In most NLP\napplications, the positional encoding has proved to be crucial\nfor achieving optimal results. For 2D image classiﬁcation\nwith the ViT model, positional encoding resulted in relatively\nsmall improvements in performance and a simple 1D raster\nencoding was as good as more elaborate 2D positional encod-\ning strategies [33]. Because we do not know a priori what\ntype of positional encoding would be useful in the application\nconsidered in this work, we leave Epos as a free parameter to\nbe learned along with the network parameters during training.\nIn Section III, we present the results of experiments with\ndifferent positional encoding strategies for our network.\nAs shown in Figure 1, our proposed network includes\nonly the encoder section of the original transformer network\nproposed in [31]. The network has K identical stages,\neach consisting of a multi-head self-attention (MSA) and a\nsubsequent two-layer fully-connected feed-forward network\n(FFN). All MSA and FFN modules include residual con-\nnections, ReLU activations, and layer normalization [35].\nStarting with the input sequence of embedded and position-\nencoded patches, X0 described above, the kth stage of\nthe network performs the following operations to map Xk\nto Xk+1:\n1) Xk goes through nh independent heads in MSA. The\nith head:\na) Computes the query, key, and value sequences\nfrom the input sequence using linear operations:\nQk,i =Ek,i\nQ LN(Xk ), Kk,i =Ek,i\nK LN(Xk ),\nV k,i =Ek,i\nV LN(Xk )\nwhere EQ, EK , Ev ∈ IRDh×D and LN denotes\nlayer normalization.\nb) Computes the self-attention matrix and then the\ntransformed values:\nAk,i =Softmax(QT K)/\n√\nDh\nSAk,i =Ak,iV k,i\nThe above equation highlights one of the central\ndifferences between transformer networks and\nCNNs. It shows that the mapping (A k,i) used to\ntransform the features from one network layer to\nthe next layer is computed based on the input\nitself. Hence, this mapping depends on the con-\ntent of the input at test time, rather than being\nﬁxed and the same for all inputs as in CNNs.\n2) Outputs of the nh self-attention heads are stacked\ntogether and re-projected back onto IR D:\nMSAk =Ek\nreproj[SAk,0;. . .;SAk,nh ]T\nwhere Ereproj ∈IRD×Dhnh\n3) The output of the current multi-head self-attention\nmodule is computed using a residual operation:\nXk\nMSA =MSAk +Xk\n4) Xk\nMSA goes through a two-layer FFN to obtain the\noutput of the kth stage of the network:\nXk+1 =Xk\nMSA +Ek\n2\n(\nReLU\n(\nEk\n1 LN(Xk\nMSA)+bk\n1\n))\n+bk\n2\nThe output of the last stage, XK , is passed through the ﬁnal\nFFN layer that projects it onto IR W 3nclass . This is then reshaped\ninto I RW ×W ×W ×nclass . Here, nclass denotes the number of\nclasses (for binary segmentation, nclass =2).\nˆY =Softmax\n(\nEoutXK +bout)\n)\n.\nˆY is the predicted segmentation map for the block (as shown\nin Figure 1). Since our network predicts segmentation maps\nfor image sub-blocks, in order to process a test image of\narbitrary size, we apply the network in a sliding window\nfashion on the image.\n29324 VOLUME 10, 2022\nD. Karimiet al.: Medical Image Segmentation Using Transformer Networks\nFIGURE 1. The proposed convolution-free network for 3D medical image segmentation. Left: An overall schematic of the proposed method: (a) an image\nblock is divided inton3 non-overlapping 3D patches, (b) each patch is reshaped into a vector and embedded into a lower dimension, (c) positional\nencoding is added to the embedding, (d) position-encoded signals go through the transformer network, (e) the output of the network is re-projected back\ninto the shape of the original patches, (f) the network output is the segmentation of the organ of interest for the location corresponding to the location of\nthe extracted block. Right: One of theK stages of the transformer network.\nB. IMPLEMENTATION AND TRAINING\nWe implemented the network in TensorFlow 1.16 and trained\nit on an NVIDIA GeForce GTX 1080 GPU on a Linux\nmachine with 120 GB of memory and 16 CPU cores. We com-\npare our model with the following FCN architectures:\n• 3D UNet++ [36]. This is a re-design of the UNet\nmodel [37]. The main difference between UNet++ and\nthe basic UNet is a set of dense skip connections between\nthe encoder and decoder sections of UNet++.\n• Attention UNet [38]. This model is based on attention\ngates, which are meant to learn to automatically focus\non the target organ. These attention gates enable the net-\nwork to suppress irrelevant features and to learn useful\nsoft region proposals, thereby improving segmentation\nperformance.\n• SE-FCN [39]. This network architecture is based on\nincorporating squeeze & excitation (SE) blocks [40] into\nFCNs for medical image segmentation. The purpose of\nSE blocks is to adaptively adjust the importance given\nto different feature maps, i.e., to promote more useful\nfeatures and to down-weight less informative features.\n• 3D Deeply Supervised Residual Network\n(DSRNet) [41]. This is an encoder-decoder FCN archi-\ntecture that uses deep supervision [42] and skip connec-\ntions between all corresponding encoder and decoder\nstages.\nWe trained the networks using a Dice similarity coefﬁcient\n(DSC)-based loss function [43]:\nL( ˆY , Y ) =−\n∑\ni ˆYiYi\n∑\ni ˆY 2\ni +∑\ni Y 2\ni\n,\nwhere Y is the ground truth segmentation map corresponding\nto the image block B and the index i runs over all voxels in\nthe block. For training of our own network and the competing\nmodels we used the Adam optimization algorithm [44] with\na batch size of 8. Furthermore, for all models we used blocks\nof size 24 3 voxels. For our own network we used a learning\nrate of 10 −4. For UNet++ a larger initial learning rate of\n3×10−4 was used because that led to the best results with\nUNet++. For Attention UNet, SE-FCN, and DSRNet we\nused a learning rate of 10 −4.\nC. PRE-TRAINING\nManual segmentation of complex structures such as the brain\ncortical plate can take several hours of a medical expert’s time\nfor a single 3D image. Therefore, methods that can achieve\nhigh performance with fewer labeled training images are\nhighly advantageous. This is especially important for trans-\nformer networks. As we mentioned above, transformer net-\nworks lack much of the built-in inductive bias that many other\nnetworks such as CNNs enjoy merely by the virtue of their\narchitectural design. Therefore, compared with those archi-\ntectures, transformers typically need much larger labeled\ntraining datasets in order to learn the underlying patterns\ndirectly from data. In NLP applications, a very common\napproach is to pre-train the network using unsupervised train-\ning on massive unlabeled datasets [45]. In the same spirit,\nwe propose pretext tasks that can be used to train our network\non unlabeled 3D medical image datasets.\n1) PRE-TRAINING WITH IMAGE DENOISING\nIn this approach, we add noise to the input image block B\nand feed the noisy block Bnoisy to the network. We train the\nnetwork to reconstruct the clean image block using an ℓ2 loss:\nL(Bnoisy, B) =∥B noisy−B∥2.\nThe noise added to each voxel is independent and identi-\ncally distributed Gaussian noise with SNR =10 dB.\n2) PRE-TRAINING WITH IMAGE COMPLETION/INPAINTING\nIn this pre-training approach, we mask 10% of the image vox-\nels at random. This is done by creating a random mask, M ∈\n{0, 1}W ×W ×W ×c, where each element of M is a Bernoulli\nrandom variable with p =0.1 and multiplying M with B in\nan element-wise fashion. The loss function used in this pre-\ntraining approach is similarly:\nL(B, M) =∥B−M ◦B∥2.\nFor model pre-training with each of the above two strate-\ngies, we use a different output layer (without the softmax\noperation). In order to ﬁne-tune the pre-trained network for\nthe segmentation task, we introduce a new output layer with\nthe softmax activation and train the network on the labeled\nVOLUME 10, 2022 29325\nD. Karimiet al.: Medical Image Segmentation Using Transformer Networks\ndata as explained above. We ﬁne-tune the entire network,\nrather than only the output layer, on the labeled images\nbecause we have found that ﬁne-tuning the entire network\nfor the segmentation task leads to much better results.\nPre-training methods are also commonly used for FCNs.\nPrior studies have shown that pre-training might lead to sub-\nstantial improvements in segmentation performance of FCNs,\nespecially when the segmentation task is difﬁcult and the\nsize of labeled training data is small [17], [46]. Therefore,\nwe will use the same denoising and inpainting tasks described\nabove to pre-train the FCNs. Moreover, we will also use\nthe semi-supervised FCN training method proposed in [47].\nThe method of [47] is based on an alternating optimization\nstrategy. It alternately updates the network parameters and the\nestimated labels for the unlabeled images in parallel.\nD. DATASETS AND EVALUATION CRITERIA\nTable 1 shows the datasets used for model training and eval-\nuation in this work. The images were randomly split into\ntraining and test sets, with no patient data appearing in both\ntraining and test sets. The same training/test splits were used\nfor all networks. For each dataset, we used approximately\n20% of the training images for initial validation experiments\nto decide on training settings such as the learning rate for\neach network. After choosing the training settings, each\nnetwork was trained on all training images. The only data\naugmentation was the implicit augmentation via sampling of\nimage blocks from random locations in the training images.\nVoxel intensities of all images were normalized to have a\nzero mean and unit standard deviation. Moreover, all images\nwere interpolated using 3D spline interpolation into isotropic\nvoxel sizes shown in the table. The corresponding ground\ntruth segmentations were interpolated using nearest neighbor\ninterpolation. We compare our proposed method with the\ncompeting networks in terms of DSC, the 95 percentile of the\nHausdorff Distance (HD95), and Average Symmetric Surface\nDistance (ASSD).\nIII. RESULTS AND DISCUSSION\nTable 2 compares the segmentation performance of the pro-\nposed method with the competing FCNs on the brain cortical\nplate and hippocampus datasets. As described in Section II-A,\nthe proposed network includes several hyper-parameters that\ncan inﬂuence the segmentation results. The results presented\nin Table 2 were obtained with: K = 5, W = 24, n =\n3, D = 1024, Dh = 256, nh = 4. These are our default\nsettings for network hyper-parameters that we have used\nin all experiments reported in the rest of the paper, unless\notherwise speciﬁed. We arrived at these parameters using\ncross-validation experiments on the training images in the\nbrain cortical plate and hippocampus datasets as well as other\ndatasets not presented in the paper. We present experimental\nresults on the effects of different hyper-parameters on the\nsegmentation performance below.\nThe results presented in Table 2 show that the proposed net-\nwork has achieved segmentation performance levels that are\nsuperior to the competing FCNs. For each dataset and each of\nFIGURE 2. Example segmentations predicted by the proposed method\nand the four FCNs. The segmentation legend is shown at the top of the\nfigure. Three example slices from the brain cortical plate dataset and two\nexample slices from the hippocampus dataset are shown. In each\nexample, the first row shows the image slice and the ground-truth\nsegmentation map. The second row shows the predictions of the four\nFCNs and the proposed transformer network.\nthe three criteria, we performed paired t-tests to see if the dif-\nferences were statistically signiﬁcant. As shown in the table,\nsegmentation performance of the proposed convolution-free\nnetwork was signiﬁcantly better than the four FCNs in terms\nof DSC, HD95, and ASSD at p < 0.01. Speciﬁcally, for\npaired t-tests between the proposed model and UNet++ on\nthe brain cortical plate dataset the p-values for DSC, HD95,\nand ASSD were, respectively, 0.0044, 31 ×10−5, and 0.0082.\nFor the hippocampus dataset, the p-values for these compar-\nisons were, respectively, 20 ×10−6, 0.0032, and 74 ×10−6.\nThe results obtained with the proposed method were espe-\ncially superior in terms of the distance metrics, i.e., HD95\nand ASSD. Among the FCN architectures, UNet++ per-\nformed substantially better than the other architectures on\nboth datasets, but its segmentation performance was signif-\nicantly inferior to that of our proposed method.\nFigure 2 shows example slices from test images in each\ndataset and the segmentations predicted by the proposed\nmethod and the four FCNs. Visual inspection of the results\nshows that the proposed network is capable of accurately\nsegmenting ﬁne and intricate structures such as the brain\ncortical plate. On both datasets, Attention UNet, DSRnet, and\nSEFCN often resulted in false positive predictions far away\nfrom the target organ, which is the reason behind their poor\nperformance in terms of the distance metrics presented in\nTable 2.\nWe further assessed the segmentation performance of our\nproposed network with reduced number of labeled training\nimages. The goal of this experiment was to investigate if\nthe pre-training tasks proposed in Section II-C could help\nthe network achieve a good segmentation performance with\n29326 VOLUME 10, 2022\nD. Karimiet al.: Medical Image Segmentation Using Transformer Networks\nTABLE 1. Datasets used for experiments in this work.\nTABLE 2. Comparison of the segmentation performance of the proposed method and several competing FCNs on the brain cortical plate and\nhippocampus datasets. Better results for each dataset/criterion have been marked using bold type. We used paired t-tests to find statistically significant\ndifferences; asterisks denote significantly better results atp < 0.01.)\na small number of labeled training images. In this experi-\nment, we compared our model with UNet++, which was\nmore accurate than the other three FCNs in the experiments\npresented in Table 2. For this experiment, we trained our\nmethod and UNet++ using ntrain =5, 10, and 15 labeled\ntraining images from cortical plate and hippocampus datasets.\nWe pre-trained our network using either the denoising or\nthe in-painting tasks described in Section II-C. We pre-\ntrained UNet++ using the same denoising and inpainting\npre-training tasks and also using the method proposed in [47].\nFurthermore, we performed this experiment in two different\nways:\n1) Pre-training on data with a similar distribution.\nFor brain cortical plate segmentation, we used 500 T2\nbrain images from the developing Human Connectome\nProject (dHCP) dataset [14] for pre-training. The sub-\njects in the dHCP dataset range in age between 29 and\n44 gestational weeks, which is close enough to the\nage range of our in-house dataset: between 16 and\n39 gestational weeks. For hippocampus segmentation,\nwe used the remaining training images (i.e., 220 −ntrain)\nfor pre-training.\n2) Pre-training on data with a different distribution.\nSometimes even unlabeled images with the same dis-\ntribution are not available. To simulate such a scenario,\nwe used a pool of publicly available computed tomog-\nraphy (CT) images. Speciﬁcally, we used 130 liver\nCT [48] and 300 kidney CT [49] images to pre-\ntrain our network and UNet++ for both brain corti-\ncal plate segmentation and hippocampus segmentation.\nAs we had done for our target MRI images described\nabove, we also normalized voxel intensities of these CT\nimages to have a zero mean and unit standard deviation.\nFigure 3 shows the results of this experiment. The results\nshow that with the proposed pre-training, our convolution-\nfree network achieves signiﬁcantly more accurate segmen-\ntations with fewer labeled training images. As expected,\non both datasets there was a drop in the segmentation perfor-\nmance as the number of labeled training images was reduced.\nHowever, this drop was smaller for the proposed network\nthan for UNet++. We have observed very similar results\nwith other FCN architectures. For our network as well as\nfor UNet++, the proposed inpainting pre-training leads to\nslightly better results than the other pre-training methods.\nMoreover, overall, pre-training on similar images leads to bet-\nter segmentation performance than pre-training on a dataset\nof different images. As shown in Figure 3, for both the\nproposed network and UNet++ the segmentation perfor-\nmance is, slightly but consistently, higher when pre-training\nis performed on a similar dataset. This indicates that both\nthe proposed network and UNet++ can learn the existing\npatterns in unlabeled images and leverage this information\nto achieve better segmentation results.\nThis is a very interesting and promising observation\nbecause it shows that the proposed network can be trained\nusing a handful of labeled images for segmenting complex\nstructures in 3D medical images. This result is even more\nnoteworthy when we consider the results reported by recent\nimage classiﬁcation studies. As we explained in Section I,\nimage classiﬁcation studies that used a similar approach\n(i.e., applying a transformer network on patch embeddings)\nrequired massive labeled datasets [33] or relied on knowledge\ndistillation from a CNN teacher model [34]. Our results,\non the other hand, show that only a handful of labeled training\nimages are sufﬁcient to train a similar network for 3D medical\nimage segmentation. This can be attributed to several factors:\n1) In image classiﬁcation there are signiﬁcant variations in\nrelevant image features (even among images that belong to\nthe same class). In the segmentation tasks considered here,\non the other hand, there is signiﬁcant similarity across sub-\njects and even among different patches in the same image.\n2) There are far fewer class labels (only two) in the segmen-\ntation tasks considered here compared with image classiﬁ-\ncation applications. 3) Working with image sub-blocks acts\nVOLUME 10, 2022 29327\nD. Karimiet al.: Medical Image Segmentation Using Transformer Networks\nFIGURE 3. Segmentation performance (in terms of DSC) for the proposed network and UNet++with reduced number of\nlabeled training images on the brain cortical plate dataset (left) and the hippocampus dataset (right). The top two plots\nare for the experiment where the images used for pre-training have a distribution that is similar to the target test images.\nThe bottom two plots are for the experiment where the images used for pre-training (liver and kidney CT images) are very\ndifferent from the target test images.\nas a strong data augmentation strategy and enables optimal\nutilization of labeled training images. As a result, despite their\nminimal inductive bias, transformer networks appear to be\nwell suited for medical image segmentation tasks.\nThe experimental results presented above show that\nthe proposed method can achieve segmentation perfor-\nmance on par with or better than FCNs with as few as\n10-20 labeled training images. This is an important and\nencouraging result because in the medical imaging domain\nmanual labels are not easy to obtain. Nonetheless, in order\nto assess the performance of the proposed method on larger\ndatasets, we conducted another experiment with the new-\nborn brain scans in the developing Human Connectome\nProject (dHCP) dataset [14]. This dataset includes 558 T2\nMRI brain scans with cortical plate segmentation. We ran-\ndomly selected 58 of these scans as test images. We then\ntrained our model and UNet++ on all 500 remaining images\nas well as subsets of 100 and 10 images. In addition to the\nimplicit data augmentation caused by sampling patches from\nrandom locations in the training images, we applied random\nﬂip and rotation and we added random Gaussian noise to\nthe images. We also experimented with random down/up-\nscaling of the images and random elastic deformation, but\nthese augmentations had a negative impact on segmentation\nperformance because they reduced the accuracy of train-\ning labels for ﬁne and complex cortical plate segmentation.\nTherefore, we did not use these latter augmentation methods.\nThe results of this experiment are presented in Table 3. The\nresults indicate that the proposed method achieves better\nsegmentation results than UNet++ with either 10, 100, or\n500 labeled training images. Paired t-tests on the 58 test\nimages showed that, with 500 labeled training images, the\nproposed method achieved signiﬁcantly higher DSC (p =\n0.0015) and signiﬁcantly lower ASSD (p =84 ×10−6).\nIn Figures 5 and 4, we have shown example attention\nmaps of the proposed network for two different datasets.\nAs mentioned above, in order to process a test image of\narbitrary size, we apply our network in a sliding window\nfashion. To generate the attention maps for the whole image,\nat each location of the sliding window the attention matrices\n(which are of size IR N×N ) are summed along their columns to\ndetermine the total attention paid to each of the N patches by\nthe other patches in the block. Performing this computation\nin a sliding window fashion and computing the voxel-wise\naverage gives us the attention maps shown in these ﬁgures.\nThey indicate how much attention is paid to every part of the\nimage in the process of generating the segmentation map.\nThe attention maps shown in Figure 4 were generated\non pancreas CT images from the Medical Segmen-\ntation Decathlon challenge (https://decathlon-10.grand-\nchallenge.org/). The attention maps show that, overall, the\nearly stages of the network have a wider attention scope.\n29328 VOLUME 10, 2022\nD. Karimiet al.: Medical Image Segmentation Using Transformer Networks\nTABLE 3. Results of an experiment to investigate the performance of the proposed network with different numbers of training images. This experiment\nwas performed on brain cortical plate segmentation labels from the dHCP dataset [14]. Asterisks denote statistically significant differences atp = 0.01,\ncomputed using paired t-tests.\nTABLE 4. Effect of some of the network hyperparameters on the segmentation performance on the brain cortical plate dataset. The baseline network\n(shown in the first row of this table) corresponds to these settings:K = 5, W = 24, n = 3, D = 1024,Dh = 256, nh = 4, which are the hyperparameter\nvalues that we used in all experiments reported in this paper other than in this table. In the remaining rows of this table, we have changed the\nhyperparameters, one at a time, and trained the network.\nThey attend to other structures and anatomical features that\nsurround the organ of interest (here, the pancreas). The deeper\nstages of the network are more focused on the pancreas itself.\nA rather similar pattern can be observed in the segmentation\nmaps for brain cortical plate segmentation that are shown in\nFigure 5. In the earlier stages, the network attends to the entire\nbrain, while in the deeper layers the network’s attention tends\nto be more focused to the regions around the cortical plate.\nAnother observation from these ﬁgures, especially\nFigure 4, is the variability among the attention patterns of\ndifferent heads in a multi-head self-attention (MSA) module.\nIn each stage, the four separate heads of the MSA module\nadopt quite different attention strategies. This may suggest\nthat the multi-head design of the MSA module gives the\nnetwork more ﬂexibility, enabling it to learn more com-\nplex attention patterns that help improve the segmentation\nperformance. The importance of multi-head design is well\ndocumented in natural language processing applications [31],\nand our results show that it is important for 3D medical\nimage segmentation as well. We further show this below by\nquantifying the effect of the number of attention heads on\nsegmentation performance.\nTable 4 shows the results of a set of experiments on the\nbrain cortical plate dataset to investigate the effects of some of\nthe network hyper-parameters on segmentation performance.\nIn this table, the baseline network (ﬁrst row) corresponds to\nthe settings that we have used in the experiments reported\nabove, i.e., K =5, W =24, n =3, D =1024, Dh =256,\nand nh =4. We chose these settings based on preliminary\nexperiments and we have found them to work well for differ-\nent datasets.\nThe results presented in this table show that, overall,\nthe performance of the network is not very sensitive to\nthe hyper-parameter settings. For example, changing the\nnumber or the size of the patches typically leads to slight\nchanges in performance. We have also observed that a\nnetwork depth of K ∈ [5, 7] leads to best results,\nwhereas much deeper or shallower networks were not bet-\nter. Furthermore, using a ﬁxed positional encoding or no\npositional encoding slightly reduces the segmentation per-\nformance compared with free-parameter/learnable positional\nencoding. Finally, using a single-head attention signiﬁcantly\nreduces the segmentation performance, which indicates the\nimportance of the multi-head design to enable the net-\nwork to learn a more complex relation between neighboring\npatches.\nMany of the above observations are consistent with the\nexperimental results that have been reported in other appli-\ncations. For example, our results show that increasing the\nnumber of MSA heads (n h) or the network depth (K ) beyond\na certain limit has a negative impact on segmentation per-\nformance. This observation is similar to some of the exper-\nimental results reported in [33], [50], where networks with a\nlarger number of MSA heads and/or larger number of layers\nresulted in lower image classiﬁcation accuracy on several\ndatasets. Similar results have been reported in natural lan-\nguage processing applications [51]. For example, one study\nshowed that it was possible to prune 50-72% of the atten-\ntion heads without a signiﬁcant reduction in model accuracy\nin a machine translation application [52]. This is because,\ndepending on the application, a certain number of heads are\nsufﬁcient to learn the attention patterns between the signals\nin a sequence (i.e., patch embeddings in our application).\nFurther increasing the number of heads will only increase the\nnumber of network parameters without providing any useful\ncapacity to the network.\nVOLUME 10, 2022 29329\nD. Karimiet al.: Medical Image Segmentation Using Transformer Networks\nFIGURE 4. Example attention maps for two pancreas images. In this\nexperiment, a network with a depthK = 7 was used. Attention maps for\ndepths 1, 4, and 7 are shown. Each row shows the attention map for one\nof the four heads.\nThere may be other factors that can inﬂuence the relative\nadvantages of the proposed transformer network compared\nwith FCNs. Some of these factors are image resolution, size of\nthe organ/volume of interest, and patch size. Our experiments\nshow that these factors do not affect the superiority of the\nproposed method over FCNs. For example, we resampled\nthe brain cortical plate and hippocampus datasets to isotropic\nvoxel sizes of 0.5 mm and repeated our experiments. The\nresults showed that the proposed network achieved signiﬁ-\ncantly higher DSC and signiﬁcantly lower HD95 and ASSD\nthan UNet++ on both datasets. We also applied UNet++\non larger patch sizes of 48 3 and 64 3 voxels. This change\ndid not improve the performance of UNet++ on the brain\ncortical plate dataset. It slightly improved the segmentation\nperformance of UNet++on the hippocampus dataset (DSC:\n0.877±0.029, HD95: 1.448 ±1.430 mm, and ASSD: 0.502 ±\n0.201 mm). However, these were still statistically inferior to\nthose obtained with our proposed network (Table 2). Increas-\ning the input image block size to 48 3 or 64 3 voxels did\nnot signiﬁcantly improve the performance of our proposed\nnetwork either.\nTable 5 shows the number of learnable parame-\nters, number of ﬂoating point operations (FLOPS), and\nFIGURE 5. Example attention maps for a cortical plate image. In this\nexperiment, a network with a depthK = 7 was used. Attention maps for\ndepths 1, 4, and 7 are shown. Each row shows the attention map for one\nof the four heads.\nTABLE 5. The number of free parameters (nparam), number of floating\npoint operations (FLOPS), and frames per second (FPS) for each of the\nFCNs and the proposed network. FLOPS and FPS are computed for\nprocessing patches of size 243 voxels.\nframes per second (FPS). FLOPS and FPS are reported for\nprocessing patches of size 24 3 voxels. We computed the FPS\nfor all models on an NVIDIA RTX 2080TI GPU. Overall,\nthe models have relatively similar number of parameters and\ncomputational costs. Our proposed network has a slightly\nsmaller number of parameters than the compared FCNs.\nOn the other hand, the number of FLOPS for the proposed\nnetwork is higher, which is due to the large matrix multipli-\ncations involved in the attention modules. In terms of training\ntime, our network converged in approximately 24 hours\nof GPU time, whereas the FCNs converged in approxi-\nmately 4 hours of training. This might be due to the fact that\ntransformer networks need additional training time in order to\ninternalize the spatial patterns in the image, whereas FCNs’\narchitecture makes this learning easier.\nIV. CONCLUSION\nThe convolution operation has a strong basis in the structure\nof the mammalian primary visual cortex and it is well suited\n29330 VOLUME 10, 2022\nD. Karimiet al.: Medical Image Segmentation Using Transformer Networks\nfor developing powerful techniques for image modeling and\nimage understanding. In recent years, CNNs have been shown\nto be highly effective in tackling various computer vision\nproblems. However, there is no reason to expect that no other\nmodel can outperform CNNs on a speciﬁc vision task. Med-\nical image analysis applications, in particular, pose speciﬁc\nchallenges such as 3D nature of the images and small number\nof labeled images. In such applications, other models could\nbe more effective than CNNs. In this work we presented a new\nmodel for 3D medical image segmentation. Unlike all recent\nmodels that use convolutions as their main building blocks,\nour model is based on self-attention between neighboring\n3D patches. Our results show that the proposed network can\noutperform state of the art FCNs on three medical image\nsegmentation datasets. With pre-training for denoising and\nin-painting tasks on unlabeled images, our network also per-\nformed better than an FCN when only 5-15 labeled training\nimages were available. We expect that the network proposed\nin this paper should be effective for other tasks in medical\nimage analysis such as anomaly detection and classiﬁcation.\nREFERENCES\n[1] P. Gibbs, D. L. Buckley, S. J. Blackband, and A. Horsman, ‘‘Tumour\nvolume determination from MR images by morphological segmentation,’’\nPhys. Med. Biol., vol. 41, no. 11, p. 2437, 1996.\n[2] Y. Wang, Q. Guo, and Y. Zhu, ‘‘Medical image segmentation based\non deformable models and its applications,’’ in Deformable Models.\nNew York, NY, USA: Springer, 2007, pp. 209–260.\n[3] D. Mahapatra and J. M. Buhmann, ‘‘Prostate MRI segmentation using\nlearned semantic knowledge and graph cuts,’’ IEEE Trans. Biomed. Eng.,\nvol. 61, no. 3, pp. 756–764, Mar. 2014.\n[4] A. F. Goldszal, C. Davatzikos, D. L. Pham, M. X. H. Yan, R. N. Bryan,\nand S. M. Resnick, ‘‘An image-processing system for qualitative and quan-\ntitative volumetric analysis of brain images,’’ J. Comput. Assist. Tomogr.,\nvol. 22, no. 5, pp. 827–837, Sep. 1998.\n[5] J. L. Prince, D. Pham, and Q. Tan, ‘‘Optimization of MR pulse\nsequences for Bayesian image segmentation,’’ Med. Phys., vol. 22, no. 10,\npp. 1651–1656, Oct. 1995.\n[6] P. M. Thompson and A. W. Toga, ‘‘Detection, visualization and animation\nof abnormal anatomic structure with a deformable probabilistic brain atlas\nbased on random vector ﬁeld transformations,’’ Med. Image Anal., vol. 1,\nno. 4, pp. 271–294, Sep. 1997.\n[7] E. Shelhamer, J. Long, and T. Darrell, ‘‘Fully convolutional networks for\nsemantic segmentation,’’ IEEE Trans. Pattern Anal. Mach. Intell., vol. 39,\nno. 4, pp. 640–651, Apr. 2017.\n[8] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille,\n‘‘DeepLab: Semantic image segmentation with deep convolutional nets,\natrous convolution, and fully connected CRFs,’’ IEEE Trans. Pattern Anal.\nMach. Intell., vol. 40, no. 4, pp. 834–848, Apr. 2018.\n[9] S. Bakas et al., ‘‘Identifying the best machine learning algorithms for\nbrain tumor segmentation, progression assessment, and overall survival\nprediction in the BRATS challenge,’’ 2018, arXiv:1811.02629.\n[10] O. Bernard et al., ‘‘Deep learning techniques for automatic MRI\ncardiac multi-structures segmentation and diagnosis: Is the problem\nsolved?’’ IEEE Trans. Med. Imag., vol. 37, no. 11, pp. 2514–2525,\nNov. 2018.\n[11] K. Kamnitsas, C. Ledig, V. F. J. Newcombe, J. P. Simpson, A. D. Kane,\nD. K. Menon, D. Rueckert, and B. Glocker, ‘‘Efﬁcient multi-scale 3D CNN\nwith fully connected CRF for accurate brain lesion segmentation,’’ Med.\nImage Anal., vol. 36, pp. 61–78, Feb. 2017.\n[12] D. Karimi, Q. Zeng, P. Mathur, A. Avinash, S. Mahdavi, I. Spadinger,\nP. Abolmaesumi, and S. E. Salcudean, ‘‘Accurate and robust deep\nlearning-based segmentation of the prostate clinical target volume\nin ultrasound images,’’ Med. Image Anal., vol. 57, pp. 186–196,\nOct. 2019. [Online]. Available: http://www.sciencedirect.com/\nscience/article/pii/S1361841519300623\n[13] Q. Zeng, D. Karimi, E. H. T. Pang, S. Mohammed, C. Schneider,\nM. Honarvar, and S. E. Salcudean, ‘‘Liver segmentation in magnetic reso-\nnance imaging via mean shape ﬁtting with fully convolutional neural net-\nworks,’’ inProc. Int. Conf. Med. Image Comput. Comput.-Assist. Intervent.\nCham, Switzerland: Springer, 2019, pp. 246–254.\n[14] M. Bastiani, J. L. R. Andersson, L. Cordero-Grande, M. Murgasova,\nJ. Hutter, A. N. Price, A. Makropoulos, S. P. Fitzgibbon, E. Hughes,\nD. Rueckert, S. Victor, M. Rutherford, A. D. Edwards, S. M. Smith,\nJ.-D. Tournier, J. V. Hajnal, S. Jbabdi, and S. N. Sotiropoulos, ‘‘Automated\nprocessing pipeline for neonatal diffusion MRI in the developing human\nconnectome project,’’ NeuroImage, vol. 185, pp. 750–763, Jan. 2019.\n[15] M. H. Hesamian, W. Jia, X. He, and P. Kennedy, ‘‘Deep learning techniques\nfor medical image segmentation: Achievements and challenges,’’ J. Digit.\nImag., vol. 32, no. 4, pp. 582–596, Aug. 2019.\n[16] S. A. Taghanaki, K. Abhishek, J. P. Cohen, J. Cohen-Adad, and\nG. Hamarneh, ‘‘Deep semantic segmentation of natural and medical\nimages: A review,’’ Artif. Intell. Rev., vol. 54, no. 1, pp. 1–42, 2020.\n[17] M. Ghafoorian, A. Mehrtash, T. Kapur, N. Karssemeijer, E. Marchiori,\nM. Pesteie, C. R. G. Guttmann, F.-E. de Leeuw, C. M. Tempany,\nB. van Ginneken, A. Fedorov, P. Abolmaesumi, B. Platel, and\nW. M. Wells, III, ‘‘Transfer learning for domain adaptation in MRI:\nApplication in brain lesion segmentation,’’ in Proc. Int. Conf. Med. Image\nComput. Comput.-Assist. Intervent.Cham, Switzerland: Springer, 2017,\npp. 516–524.\n[18] D. Karimi, S. K. Warﬁeld, and A. Gholipour, ‘‘Critical assessment of\ntransfer learning for medical image segmentation with fully convolutional\nneural networks,’’ 2020, arXiv:2006.00356.\n[19] Y. Gao, J. M. Phillips, Y. Zheng, R. Min, P. T. Fletcher, and G. Gerig,\n‘‘Fully convolutional structured LSTM networks for joint 4D medical\nimage segmentation,’’ in Proc. IEEE 15th Int. Symp. Biomed. Imag. (ISBI),\nApr. 2018, pp. 1104–1108.\n[20] W. Bai, H. Suzuki, C. Qin, G. Tarroni, O. Oktay, P. M. Matthews,\nand D. Rueckert, ‘‘Recurrent neural networks for aortic image sequence\nsegmentation with sparse annotations,’’ in Proc. Int. Conf. Med. Image\nComput. Comput.-Assist. Intervent.Cham, Switzerland: Springer, 2018,\npp. 586–594.\n[21] J. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille,\nand Y. Zhou, ‘‘TransUNet: Transformers make strong encoders for medical\nimage segmentation,’’ 2021, arXiv:2102.04306.\n[22] O. Oktay, E. Ferrante, K. Kamnitsas, M. Heinrich, W. Bai, J. Caballero,\nS. A. Cook, A. De Marvao, T. Dawes, D. P. O’Regan, and B. Kainz,\n‘‘Anatomically constrained neural networks (ACNNs): Application to car-\ndiac image enhancement and segmentation,’’ IEEE Trans. Med. Imag.,\nvol. 37, no. 2, pp. 384–395, Feb. 2017.\n[23] D. Karimi, G. Samei, C. Kesch, G. Nir, and S. E. Salcudean, ‘‘Prostate\nsegmentation in MRI using a convolutional neural network architecture\nand training strategy based on statistical shape models,’’ Int. J. Comput.\nAssist. Radiol. Surg., vol. 13, no. 8, pp. 1211–1219, Aug. 2018, doi:\n10.1007/s11548-018-1785-8.\n[24] F. Isensee, P. Kickingereder, W. Wick, M. Bendszus, and\nK. H. Maier-Hein, ‘‘No new-net,’’ in Proc. Int. MICCAI Brainlesion\nWorkshop. Cham, Switzerland: Springer, 2018, pp. 234–244.\n[25] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‘‘ImageNet classiﬁcation\nwith deep convolutional neural networks,’’ in Proc. Adv. Neural Inf. Pro-\ncess. Syst., 2012, pp. 1097–1105.\n[26] S. Ren, K. He, R. Girshick, and J. Sun, ‘‘Faster R-CNN: Towards real-time\nobject detection with region proposal networks,’’ in Proc. Adv. Neural Inf.\nProcess. Syst., 2015, pp. 91–99.\n[27] Y. LeCun, Y. Bengio, and G. Hinton, ‘‘Deep learning,’’ Nature, vol. 521,\nno. 7553, p. 436, 2015.\n[28] Y. Le Cun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard,\nand L. Jackel, ‘‘Handwritten digit recognition with a back-propagation\nnetwork,’’ in Proc. 2nd Int. Conf. Neural Inf. Process. Syst., 1989,\npp. 396–404.\n[29] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio, Deep Learning,\nvol. 1. Cambridge, MA, USA: MIT Press, 2016.\n[30] B. A. Olshausen and D. J. Field, ‘‘Emergence of simple-cell receptive ﬁeld\nproperties by learning a sparse code for natural images,’’ Nature, vol. 381,\nno. 6583, pp. 607–609, Jul. 1996.\n[31] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ 2017,\narXiv:1706.03762.\n[32] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah,\n‘‘Transformers in vision: A survey,’’ 2021, arXiv:2101.01169.\nVOLUME 10, 2022 29331\nD. Karimiet al.: Medical Image Segmentation Using Transformer Networks\n[33] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, ‘‘An image is worth 16×16 words: Trans-\nformers for image recognition at scale,’’ 2020, arXiv:2010.11929.\n[34] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou,\n‘‘Training data-efﬁcient image transformers & distillation through atten-\ntion,’’ 2020, arXiv:2012.12877.\n[35] J. Lei Ba, J. R. Kiros, and G. E. Hinton, ‘‘Layer normalization,’’ 2016,\narXiv:1607.06450.\n[36] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, ‘‘UNet++:\nA nested U-Net architecture for medical image segmentation,’’ in Deep\nLearning in Medical Image Analysis and Multimodal Learning for Clinical\nDecision Support. Cham, Switzerland: Springer, 2018, pp. 3–11.\n[37] O. Ronneberger, P. Fischer, and T. Brox, ‘‘U-Net: Convolutional networks\nfor biomedical image segmentation,’’ in Proc. Int. Conf. Med. Image\nComput. Comput.-Assist. Intervent., 2015, pp. 234–241.\n[38] O. Oktay, J. Schlemper, L. Le Folgoc, M. Lee, M. Heinrich, K. Misawa,\nK. Mori, S. McDonagh, N. Y. Hammerla, B. Kainz, B. Glocker, and\nD. Rueckert, ‘‘Attention U-Net: Learning where to look for the pancreas,’’\n2018, arXiv:1804.03999.\n[39] A. G. Roy, N. Navab, and C. Wachinger, ‘‘Recalibrating fully convolutional\nnetworks with spatial and channel ‘squeeze and excitation’ blocks,’’ IEEE\nTrans. Med. Imag., vol. 38, no. 2, pp. 540–549, Aug. 2018.\n[40] J. Hu, L. Shen, and G. Sun, ‘‘Squeeze-and-excitation networks,’’ in\nProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018,\npp. 7132–7141.\n[41] H. Dou, D. Karimi, C. K. Rollins, C. M. Ortinau, L. Vasung,\nC. Velasco-Annis, A. Ouaalam, X. Yang, D. Ni, and A. Gholipour,\n‘‘A deep attentive convolutional neural network for automatic cortical plate\nsegmentation in fetal MRI,’’ IEEE Trans. Med. Imag., vol. 40, no. 4,\npp. 1123–1133, Apr. 2021.\n[42] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu, ‘‘Deeply-supervised\nnets,’’ in Proc. Artif. Intell. Statist., 2015, pp. 562–570.\n[43] F. Milletari, N. Navab, and S.-A. Ahmadi, ‘‘V-Net: Fully convolutional\nneural networks for volumetric medical image segmentation,’’ in Proc. 4th\nInt. Conf. 3D Vis. (3DV), Oct. 2016, pp. 565–571.\n[44] D. P. Kingma and J. Ba, ‘‘Adam: A method for stochastic optimization,’’\nin Proc. 3rd Int. Conf. Learn. Represent. (ICLR), 2014, pp. 1–13.\n[45] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ 2018,\narXiv:1810.04805.\n[46] D. Karimi, S. K. Warﬁeld, and A. Gholipour, ‘‘Transfer learning\nin medical image segmentation: New insights from analysis of the\ndynamics of model parameters and learned representations,’’ Artif.\nIntell. Med., vol. 116, Jun. 2021, Art. no. 102078. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S0933365721000713\n[47] W. Bai, O. Oktay, M. Sinclair, H. Suzuki, M. Rajchl, G. Tarroni, B. Glocker,\nA. King, P. M. Matthews, and D. Rueckert, ‘‘Semi-supervised learning for\nnetwork-based cardiac mr image segmentation,’’ in Proc. Int. Conf. Med.\nImage Comput. Comput.-Assist. Intervent.Cham, Switzerland: Springer,\n2017, pp. 253–260.\n[48] P. Bilic et al., ‘‘The liver tumor segmentation benchmark (LiTS),’’ 2019,\narXiv:1901.04056.\n[49] N. Heller, N. Sathianathen, A. Kalapara, E. Walczak, K. Moore,\nH. Kaluzniak, J. Rosenberg, P. Blake, Z. Rengel, M. Oestreich, J. Dean,\nM. Tradewell, A. Shah, R. Tejpaul, Z. Edgerton, M. Peterson, S. Raza,\nS. Regmi, N. Papanikolopoulos, and C. Weight, ‘‘The KiTS19 challenge\ndata: 300 kidney tumor cases with clinical context, CT semantic segmen-\ntations, and surgical outcomes,’’ 2019, arXiv:1904.00445.\n[50] I. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner,\nJ. Yung, A. Steiner, D. Keysers, J. Uszkoreit, M. Lucic, and\nA. Dosovitskiy, ‘‘MLP-mixer: An all-MLP architecture for vision,’’\n2021, arXiv:2105.01601.\n[51] E. Voita, D. Talbot, F. Moiseev, R. Sennrich, and I. Titov, ‘‘Analyzing multi-\nhead self-attention: Specialized heads do the heavy lifting, the rest can be\npruned,’’ 2019, arXiv:1905.09418.\n[52] M. Behnke and K. Heaﬁeld, ‘‘Losing heads in the lottery: Pruning trans-\nformer attention in neural machine translation,’’ in Proc. Conf. Empirical\nMethods Natural Lang. Process. (EMNLP), 2020, pp. 2664–2674.\nDAVOOD KARIMI received the Ph.D. degree in\nelectrical and computer engineering from The Uni-\nversity of British Columbia (UBC), Canada. His\nPh.D. dissertation was focused on image recon-\nstruction and enhancement for cone-beam com-\nputed tomography. After completing his Ph.D.\ndegree, he worked as a Postdoctoral Research\nFellow at UCLA and UBC mostly on projects cen-\ntered on developing machine learning-based meth-\nods for medical image segmentation and cancer\ndetection and grading in digital histopathology. He is currently an Instructor\nin radiology with the Harvard Medical School, and a Scientist with the\nComputational Radiology Laboratory, Radiology Department, Boston Chil-\ndren’s Hospital. His research at IMAGINE involves development of new\ndeep learning algorithms for medical image analysis, including improved\nalgorithms and techniques for motion-robust fetal and newborn imaging for\nthe analysis of early brain development.\nHAORAN DOUreceived the B.Eng. degree from\nSichuan University, in 2017, and the M.Eng.\ndegree from Shenzhen University. He is currently\npursuing the Ph.D. degree with the Center for\nComputational Imaging & Simulation Technolo-\ngies in Biomedicine, School of Computing, Uni-\nversity of Leeds. His current research interests\ninclude medical image segmentation and genera-\ntive model.\nALI GHOLIPOUR (Senior Member, IEEE)\nreceived the B.S. and M.S. degrees in electrical\nengineering from the University of Tehran, in\n2001 and 2003, respectively, and the Ph.D. degree\nin electrical engineering from the University of\nTexas at Dallas, in 2008. He is currently an\nAssociate Professor in radiology with the Harvard\nMedical School and the Director of the Intelligent\nMedical Imaging Research Group and the Trans-\nlational Research with the Radiology Department,\nBoston Children’s Hospital. His research interests include medical imaging\nand image analysis, in particular on the development of new imaging\ntechnologies and methods to study early human brain development.\n29332 VOLUME 10, 2022",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8156993389129639
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7015042304992676
    },
    {
      "name": "Segmentation",
      "score": 0.6401201486587524
    },
    {
      "name": "Embedding",
      "score": 0.6390728950500488
    },
    {
      "name": "Convolution (computer science)",
      "score": 0.5931918621063232
    },
    {
      "name": "Deep learning",
      "score": 0.5887623429298401
    },
    {
      "name": "Image segmentation",
      "score": 0.5609580874443054
    },
    {
      "name": "Transformer",
      "score": 0.5461295247077942
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5307799577713013
    },
    {
      "name": "Block (permutation group theory)",
      "score": 0.5102474689483643
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5036038756370544
    },
    {
      "name": "Image (mathematics)",
      "score": 0.44326266646385193
    },
    {
      "name": "Network architecture",
      "score": 0.4392361640930176
    },
    {
      "name": "Artificial neural network",
      "score": 0.3720472455024719
    },
    {
      "name": "Mathematics",
      "score": 0.09987753629684448
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1288882113",
      "name": "Boston Children's Hospital",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210150980",
      "name": "Boston Children's Museum",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I130828816",
      "name": "University of Leeds",
      "country": "GB"
    }
  ]
}