{
    "title": "Task-specific Objectives of Pre-trained Language Models for Dialogue Adaptation",
    "url": "https://openalex.org/W3083978629",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2117167659",
            "name": "Junlong Li",
            "affiliations": [
                "Shanghai Jiao Tong University"
            ]
        },
        {
            "id": "https://openalex.org/A2431633693",
            "name": "Zhuosheng Zhang",
            "affiliations": [
                "Shanghai Jiao Tong University"
            ]
        },
        {
            "id": "https://openalex.org/A2112311038",
            "name": "Hai Zhao",
            "affiliations": [
                "Shanghai Jiao Tong University"
            ]
        },
        {
            "id": "https://openalex.org/A2096484270",
            "name": "Xi Zhou",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2097535850",
            "name": "Xiang Zhou",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2996035354",
        "https://openalex.org/W2099471712",
        "https://openalex.org/W2891416139",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W3034600233",
        "https://openalex.org/W2944815030",
        "https://openalex.org/W3034424015",
        "https://openalex.org/W2086511124",
        "https://openalex.org/W3034950505",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2912904516",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2123301721",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2938830017",
        "https://openalex.org/W2963866450",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W2997561853",
        "https://openalex.org/W3034927876",
        "https://openalex.org/W2972664115",
        "https://openalex.org/W2996403597",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2784400615",
        "https://openalex.org/W2963527228",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W3000779003",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W3097517997",
        "https://openalex.org/W2971164557",
        "https://openalex.org/W2962854379",
        "https://openalex.org/W3002535714",
        "https://openalex.org/W2729046720",
        "https://openalex.org/W3034446185",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2988937804",
        "https://openalex.org/W3011752461",
        "https://openalex.org/W2963544536",
        "https://openalex.org/W2937845937",
        "https://openalex.org/W2964178377",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2095652037",
        "https://openalex.org/W2270070752",
        "https://openalex.org/W2916772188",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2997200074"
    ],
    "abstract": "Pre-trained Language Models (PrLMs) have been widely used as backbones in lots of Natural Language Processing (NLP) tasks. The common process of utilizing PrLMs is first pre-training on large-scale general corpora with task-independent LM training objectives, then fine-tuning on task datasets with task-specific training objectives. Pre-training in a task-independent way enables the models to learn language representations, which is universal to some extent, but fails to capture crucial task-specific features in the meantime. This will lead to an incompatibility between pre-training and fine-tuning. To address this issue, we introduce task-specific pre-training on in-domain task-related corpora with task-specific objectives. This procedure is placed between the original two stages to enhance the model understanding capacity of specific tasks. In this work, we focus on Dialogue-related Natural Language Processing (DrNLP) tasks and design a Dialogue-Adaptive Pre-training Objective (DAPO) based on some important qualities for assessing dialogues which are usually ignored by general LM pre-training objectives. PrLMs with DAPO on a large in-domain dialogue corpus are then fine-tuned for downstream DrNLP tasks. Experimental results show that models with DAPO surpass those with general LM pre-training objectives and other strong baselines on downstream DrNLP tasks.",
    "full_text": "Dialogue-adaptive Language Model\nPre-training From Quality Estimation\nJunlong Lia,b, Zhuosheng Zhanga,b, Hai Zhaoa,b,∗\naDepartment of Computer Science and Engineering, Shanghai Jiao Tong University\nbKey Laboratory of Shanghai Education Commission for Intelligent Interaction\nand Cognitive Engineering, Shanghai Jiao Tong University\nAbstract\nPre-trained language models (PrLMs) have achieved great success on a wide range of natural language processing\ntasks by virtue of the universal language representation ability obtained by self-supervised learning on a large corpus.\nThese models are pre-trained on standard plain texts with general language model (LM) training objectives, which\nwould be insufﬁcient to model dialogue-exclusive attributes like speciﬁcity and informativeness reﬂected in these\ntasks that are not explicitly captured by the pre-trained universal language representations. In this work, we propose\ndialogue-adaptive pre-training objectives (DAPO) derived from quality estimation to simulate dialogue-speciﬁc fea-\ntures, namely coherence, speciﬁcity, and informativeness. As the foundation for model pre-training, we synthesize a\nnew dialogue corpus and build our training set with two unsupervised methods: 1) coherence-oriented context cor-\nruption, including utterance ordering, insertion, and replacement, to help the model capture the coherence inside the\ndialogue contexts; and 2) speciﬁcity-oriented automatic rescoring, which encourages the model to measure the quality\nof the synthesized data for dialogue-adaptive pre-training by considering speciﬁcity and informativeness. Experi-\nmental results on widely used open-domain response selection and quality estimation benchmarks show that DAPO\nsigniﬁcantly improves the baseline models and achieves state-of-the-art performance on the MuTual leaderboard,\nverifying the effectiveness of estimating quality evaluation factors into pre-training.\nKeywords: Pre-trained Language Models, Dialogue-adaptive Pre-training, Dialogue Quality Estimation,\nOpen-domain Dialogue Systems.\n1. Introduction\nPre-trained language models (PrLMs) have achieved impressive performance in a series of natural language pro-\ncessing tasks. Some prominent examples of PrLMs are BERT [1], GPT [2], XLNet [3], RoBERTa [4], ERNIE\n[5, 6], ALBERT [7] and ELECTRA [8]. They can also be used in more expansive ﬁelds like Bioinformatics [9, 10].\n∗Corresponding author. This paper was partially supported by Key Projects of National Natural Science Foundation of China (U1836222 and\n61733011).\nEmail addresses: lockonn@sjtu.edu.cn (Junlong Li), zhangzs@sjtu.edu.cn (Zhuosheng Zhang),\nzhaohai@cs.sjtu.edu.cn (Hai Zhao)\nPreprint submitted to Neurocomputing October 21, 2022\narXiv:2009.04984v2  [cs.CL]  20 Oct 2022\nFigure 1: Comparison between two previous workﬂows and ours: (a) Original PrLM workﬂow. (b) Existing dialogue-adaptive PrLM workﬂow.\n(c) Our dialogue-adaptive PrLM workﬂow, where QE stands for Quality Estimation, and we instantiate the QE objective as DAPO in this work.\nThe PrLMs are commonly employed through a pre-training then ﬁne-tuning paradigm: the models are ﬁrst trained\non large-scale unlabeled task-independent corpora with general training objectives, like masked language modeling\n(MLM) [11] or next sentence prediction (NSP) [1], to learn universal language representations; then, the trained mod-\nels are ﬁne-tuned on the smaller datasets of downstream tasks by leveraging extra task-speciﬁc modules for adaption.\nHowever, the learned universal language representations for dialogue tasks would not sufﬁciently and accurately cover\nthe dialogue-aware features because the literary style of dialogues and plain texts varies dramatically. More specif-\nically, as opposed to general plain texts, dialogue involves multiple speakers, intentions, topics; thus, the utterances\nare full of transitions. As a result, directly ﬁne-tuning PrLMs on the dialogue datasets would be sub-optimal to model\ndialogues that contain exclusive attributes.\nTo help PrLMs adapt to dialogue-related tasks, recent studies have investigated further dialogue-adaptive pre-\ntraining on PrLMs before ﬁne-tuning them on dialogue tasks. [12] pre-trained GPT further with a conventional\nLanguage Model (LM) objective on a large dialogue corpus, Reddit, and get DialoGPT for response generation tasks;\n[13] and [14] pre-trained BERT with Mask Language Model (MLM) objective on the target datasets before ﬁne-tuning\non response selection tasks. Besides merely pre-training on dialogue datasets with general objectives, some studies\npropose particular auxiliary tasks or objectives for dialogue-adaptive pre-training. Notably, [15] proposed a response\ncontrastive loss to match the context with the corresponding response and distinguish from the randomly sampled\nnegative response. [16] employed auxiliary objectives to measure the utterance consistency of a dialogue session and\nrestore the corrupted utterances when ﬁne-tuning models for response selection tasks.\nDespite the progress made by the methods mentioned above, the guideline of dialogue-adaptive pre-training is still\nnot fully exploited, especially for open-domain dialogues as the main focus of this work. The major focus of existing\ndialogue-adaptive pre-training strategies revolves around merely one attribute, coherence, either on token-level or\nsentence-level. Therefore, the previous studies can be referred to as coherence-oriented objectives [16], which are\ncommonly implemented in a discriminative way: corrupting the dialogue context by masking, randomly sampling, or\nre-ordering as negative samples, and restoring the original context as the positive ones.\n2\nAs opposed to written language texts, dialogues, as spoken language texts, are full of redundant or uninforma-\ntive utterances. As it is pointed out by recent dialogue evaluation studies [17, 18, 19, 20, 21], estimating whether\nan open-domain dialogue is informative is also essential for assessing its quality, which motivates us to improve\ndialogue-adaptive pre-training by considering informativeness to better simulate dialogue-speciﬁc characteristics. In\naddition, measuring the informativeness of the utterance would provide a more ﬁne-grained self-supervision like in-\nformativeness degree during training, and guide the model to get rid of meaningless utterances. Therefore, in this\nwork, we make a ﬁrst attempt to bridge comprehensive dialogue quality estimation and pre-training and present\ndialogue-adaptive pre-training objectives (DAPO) by considering the salient characteristics, including coherence and\nspeciﬁcity. The overall workﬂow of utilizing DAPO and the comparison between existing methods are shown in\nFigure 1. A detailed explanation of the terms are shown as follows:\n1. coherence: whether a dialogue is coherent in its semantics and logic so that readers feel easy to read it.\n2. speciﬁcity: whether the tokens, phrases, and expressions in a dialogue are speciﬁc and diverse to avoid being\ndull and monotonous.\nAs the foundation for model pre-training, we ﬁrst synthesize a new open-domain dialogue corpus used for pre-\ntraining with each dialogue in it as a positive sample to facilitate discriminative pre-training. Inspired by [22, 23, 24],\nwe propose coherence-oriented context corruption, including utterance ordering, utterance insertion and utterance re-\nplacement, to generate incoherent dialogues as negative samples.1 Next, we do speciﬁcity-oriented automatic rescor-\ning by multiplying the scores of positive samples with a token-speciﬁcity coefﬁcient measured by n-gram normalized\ninverse document frequency (N-NIDF) to further distinguish the samples by quantifying how speciﬁc they are. Finally,\nthe PrLMs are trained on all these unsupervised annotated samples with a regression task.2\nIn summary, our contributions in the paper are three-fold:\n1. We bridge the gap between dialogue-adaptive pre-training and dialogue estimation to facilitate a new research\nline of dialogue-adaptive pre-training from quality estimation by considering the salient characteristics, includ-\ning coherence and informativeness.\n2. To simulate dialogue-speciﬁc features, we propose dialogue-adaptive pre-training objectives including coherence-\noriented context corruption and token-speciﬁcity rescoring, for open-domain dialogue modeling;\n3. We empirically verify that the quality estimation methods improve PrLMs on both response selection and quality\nevaluation tasks by modeling the diverse aspects of dialogue-related characteristics.\n1The negative samples are scored as 0 while the positive ones as 1.\n2Our codes are publicly available in https://github.com/lockon-n/DAPO.\n3\n2. Background and Related Works\n2.1. Pre-trained Language Models\nRecent works have explored various architecture choices and training objectives for large-scale LM pre-training.\nMost of the PrLMs are based on the encoder in Transformer, among which Bidirectional Encoder Representations\nfrom Transformers (BERT) [1] is one of the most representative works. It adopts masked language modeling (MLM)\nand next sentence prediction (NSP) as its pre-training objectives. MLM is also referred as a Cloze task. It ﬁrst\nmasks out some tokens from the input sentences and then trains the model to predict them by the rest of the tokens.\nNSP is another widely used pre-training objective. It trains the model to distinguish whether two input sentences are\ncontinuous segments from the training corpus. Several subsequent variants have been proposed further to enhance the\ncapacity of PrLMs, such as XLNet [3] trained with a permutation language model objective, ALBERT [7] trained with\nMLM and sentence order prediction, and ELECTRA [8] trained with replaced token detection to distinguish between\noriginal and generated tokens.\n2.2. Dialogue-adaptive Pre-training\nTo make PrLMs more compatible in the dialogue scenario and corresponding downstream task, existing works\nhave tried to do adaptive further pre-training on another dialogue corpus or the target task dataset. A part of the\nstudies perform training on large conversational data like Reddit for response selection or generation tasks [25, 26, 27,\n12, 13, 28, 14, 29], with conventional objectives such as MLM and NSP. There are also works that take a step further\non the basis of traditional losses. [30] and [31] predict the token order within utterances as well as the utterance\norder in the dialogues. TOD-BERT [15] trains BERT further with the combination of a newly-designed Response\nContrastive loss and MLM for task-oriented dialogues. [16] do multi-task joint learning with four self-supervised\ntasks: session-level matching, utterance restoration, incoherence detection, and consistency classiﬁcation as auxiliary\nobjectives when ﬁne-tuning models for response selection tasks.\nAlthough with various forms of pre-training objectives, all these models, no matter general or dialogue-adaptive\nones, only concern the coherence aspect for modeling dialogues. The main difference between them is just the num-\nbers and types of granularities. For open-domain dialogues emphasized in this work, many more characteristics should\nbe taken into account as revealed by recent dialogue evaluation works [17, 18, 19, 20, 21], so we are motivated to\nmodel speciﬁcity together with coherence as a way of fusing quality estimation into pre-training. To avoid redundant\ncomparison with all the variants of the above mentioned coherence-oriented objectives, we pick the most representa-\ntive ones, MLM and NSP (one for token-level and the other for sentence-level), in the experimental part to show our\nadvantages over them.\n4\n3. Dialogue Adaption Pre-training Objective from Quality Estimation\n3.1. Pre-training Corpus Construction\nExisting large dialogue corpora such as Reddit [12] or Ubuntu [32] are directly crawled from the Internet forums\nwithout further processing. A considerable proportion of sentences in these large corpora do not follow grammati-\ncal standards or even have syntactic errors and spelling mistakes. As a result, we avoid using them and choose to\nconstruct a new open-domain dialogue corpus based on four manually-proofread, medium-size datasets: DailyDialog\n[33], PERSONA-CHAT [34], Topical-Chat [35], and BlendedSkillTalk [36]. The total number of dialogues from these\ndatasets is 49,930. Dialogues extracted from these datasets with more than 10 utterances are split into several consec-\nutive, overlapping dialogue segments to prevent the length of text from exceeding themax-sequence-length of\nmodels, while the others stay intact. As a result, all dialogues in our corpus have no more than 10 utterances, and they\nare regarded as positive samples.\n3.2. Coherence-oriented Context Corruption\nWe generate incoherent negative dialogue samples through coherence-oriented context corruption, including: ut-\nterance ordering (UO) and utterance insertion (UI) and utterance replacement (UR) [22, 23, 24].\n1. UO: The order of utterances in a dialogue is permuted randomly.\n2. UI: One utterance in a dialogue is removed and then re-inserted in any possible position except the original one\nin the dialogue.\n3. UR: One of the utterances in a dialogue is replaced with another utterance that is randomly selected from other\ndialogues.\nFor each positive sample, we construct three negative samples by coherence-oriented context corruption, i.e., one\nfor each operation. After that, we score all the negative samples as 0 and positive ones as 1 to distinguish whether a\nsample is coherent or not.\nThe corpus is further split into train and dev sets with a ratio of 0.9/0.1. More detailed statistics of our corpus can\nbe found in Table 1.\nTrain Dev\n# of all samples 1045K 116K\n# of positive samples 261K 29K\n# of negative samples 784K 87K\navg. # utter. per sample 9.84 9.84\navg. # tokens per sample 177.09 177.30\nTable 1: Data Statistics: Our open-domaindialogue dataset for pretraining.\n5\n3.3. Speciﬁcity-oriented Automatic Rescoring\nTo take a comprehensive quality estimation factor into the pre-training objective, we further leverage speciﬁcity-\noriented automatic rescoring. The scores of positive samples are additionally multiplied by a token-speciﬁcity coefﬁ-\ncient to judge how much they arespeciﬁc and informative. This coefﬁcient is measured by n-gram normalized inverse\ndocument frequency (N-NIDF), which is extented from normalized inverse document frequency (NIDF) [17] and it\nhas been shown effective for reﬂecting token rareness. The inverse document frequency (IDF) of an n-gram ngis\nIDF(ng) =log(D/cD\nng), (1)\nwhere D is the number of the original dialogues from the four dialogue datasets (i.e. D = 49,930), and cD\nng is the\nnumber of those dialogues that contain ng. Then, normalized IDF (NIDF) for ngis as follows:\nNIDF(ng) =IDF(ng) −min-idf\nmax-idf −min-idf , (2)\nwhere min-idf and max-idf are the minimum and maximum of all IDFs. The N-NIDF of a sample sis the weighted\nmean for all NIDF of n-grams in this sample:\nN-NIDF(s) =\n∑\nng∈sng\nNIDF(ng) × cs\nng\n|sng|, (3)\nwhere sng denotes all the n-grams in this sample, and cs\nng denotes the times ngappears in s.\nFor each positive sample, we calculate N-NIDF of it with n = 3, and use it as the token-speciﬁcity coefﬁcient.\nConsequently, the ﬁnal scores for all the samples in our corpus is as follows:\nscore(s) =\n\n\n\n0 s∈S−\n1 ×3-NIDF(s) ∈[0,1] s∈S+\nwhere S−and S+ denote the negative and positive samples respectively.\nThis unsupervised annotation is able to measure coherence , speciﬁcity and informative simultaneously as an\noverall quality estimation, while it does not require any time- and cost-intensive human labeling, allowing us to take\nfull advantage of the large-scale unlabeled corpus.\n3.4. Model Implementation\nThe discriminator of ELECTRAlarge [8] is the PrLM adopted in our work and referred to as ELECTRA for brief in\nthe following statements. We drop the generator part of it so that it has the same structure as BERT with an additional\ndiscriminator head. ELECTRA requires a textA and an optional textB as the inputs, and insert an [SEP] token between\nthem if textB exists. By setting the number of category as 1, the discriminator head can map the representation of\nthe [CLS] token to a real value. For DAPO, we regard each of our samples as a long text sequence and input it into\nELECTRA as textA while leaving textB as blank. To match the range of scores, a mapping layer is added on top of\n6\nFigure 2: An overview of our model: TextA is the dialogue history, and TextB is the last utterance or the candidate response. Contents in the large\ndotted box are the ELECTRA model, and its outputs are sent into the Sigmoid mapping layer to get a score in [0,1].\nELECTRA. It consists of a sigmoid function, transforming the original output logit of PrLMs to a real number ranging\nfrom 0 to 1. The overall structure is shown in Figure 2.\nDuring pre-training on our open-domain dialogue corpus, the parameters are updated by mean-square error (MSE)\nloss:\nLMSE = 1\nb\nb∑\ni=1\n(si −ˆsi)2, (4)\nwhere bis the batch size, si and ˆsi denote the real score and the prediction score of a sample, respectively.\n4. Settings\n4.1. Task Description and Datasets\nWe evaluate our method on two typical kinds of open-domain dialogue tasks: Response Selection (RS) and Quality\nEvaluation (QE).\nResponse Selection (RS). This task requires models to select the best response from some candidates with a given\ndialogue history. Recall at position n in candidates (R@n) and Mean Reciprocal Rank (MRR) [37] are set to be the\nevaluation metrics.\nMuTual [38] is selected as the dataset. It consists of 8,860 manually annotated open-domain dialogues based on\nChinese student English listening comprehension exams and requires models to handle various reasoning problems.\nExperiments are also conducted on the advanced version of it, MuTual plus, where one of the candidate responses is\nreplaced by a safe response (e.g., Could you repeat that? or I’m really sorry, I didn’t catch that.) for each example.\nIf the original right answer is replaced, then the safe response becomes the best one; otherwise, the original positive\nresponse is still the best one. The introduction of safe responses makes MuTualplus more challenging than MuTual.\n7\nFigure 3: An overview of the ﬁne-tuning adaption: The model architecture is the same as the pre-training stage, except that we need to process\nall the candidate responses in RS tasks.\nQuality Evaluation (QE). This task needs the model to evaluate a speciﬁc response after a given dialogue history by\nassigning a score for this reponse. Each example in the task datasets has been labeled with a set of human judgment\nscores by several annotators to determine the overall impression of this response [19, 20, 21, 17, 18, 39]. Following\nprevious studies, we use Pearson and Spearman correlation to examine whether the prediction scores are correlated\nwith human judgments.\nA subset of dialogues from DailyDialog [33] and PERSONA-CHAT [34] respectively annotated in a previous\nstudy [21] are chosen for response-level evaluation datasets. These two datasets provide the human judgments of the\nOverall Quality for each example. We also try directly applying our model without further training as an individual\ndialogue evaluation module and choose a subset of the repository mentioned in [39] as the datasets. Since human\njudgment scores may have a range unmatched with 0-1 (e.g., 1-5 or 0-3), they are uniformly mapped into the range of\n0-1 to match the prediction scores.\n4.2. Task-speciﬁc Fine-tuning\nThe model architecture for ﬁne-tuning is almost the same as the one for DAPO pre-training shown in Figure 2.\nHere, we show how to adapt it to RS and QE tasks. An overview of the adaption for ﬁne-tuning is shown in Figure 3.\nResponse Selection (RS). The dialogue history is input as textA and each candidate response is input as textB with an\n[SEP] token in the middle. For responses in a dialogue example, the model will output corresponding scores, and use\nthese values to rank them from large to small. The loss function is still MSE as we label the correct response as 1 and\nothers as 0.\nQuality Evaluation (QE). For QE tasks, the inputs are the same as the RS task. Still, the prediction score with the\nmapping layer for each example is used, and we rescale the score to the original range for evaluation, for example to\na range of [1,5].\n8\nModel MuTual MuTual plus\nR@1 R@2 MRR R@1 R@2 MRR\nOn Leaderboard\nRoBERTa-MC 0.686 0.887 0.822 0.643 0.845 0.792\nRoBERTa 0.713 0.892 0.836 0.626 0.866 0.787\nRoBERTa+OCN 0.867 0.958 0.926 - - -\nGRN-v2 0.915 0.983 0.954 0.841 0.957 0.913\nMDFN 0.916 0.984 0.956 - - -\nIn Paper [38]\nDual LSTM 0.266 0.528 0.538 0.266 0.528 0.538\nSMN 0.274 0.524 0.575 0.274 0.524 0.575\nDAM 0.239 0.463 0.575 0.239 0.463 0.575\nBERT 0.657 0.867 0.803 0.657 0.867 0.803\nBERT-MC 0.661 0.871 0.806 0.661 0.871 0.806\nOur Implementation\nELECTRA 0.900 0.979 0.946 0.823 0.947 0.901\nELECTRA-DAPO 0.916 (+1.6%) 0.988 (+0.9%) 0.956 (+1.0%) 0.836 (+1.3%) 0.955 (+0.8%) 0.910 (+0.9%)\nTable 2: Main Results on MuTual and MuTualplus: Top 2 scores for each metric are in bold. We do not include all the results on the leaderboard,\nas some of them are not publicly available now.\n4.3. Baseline Models\nPre-trained ELECTRA without any further task-speciﬁc pre-training is used as one of our baselines. To show the\neffectiveness of our proposed DAPO, we follow the same steps of our method by replacing DAPO with MLM and\nNSP to get ELECTRA-MLM and ELECTRA-NSP as the strong and representative baselines of coherence-oriented\nobjectives. The pre-training data of MLM is directly the original corpus, and for each sentence pair in the corpus, we\ngenerate a negative sample by replacing the second utterance with a random one to get the pre-training data for NSP.\nThe corpora for these two objectives are also split into train/dev set with the ratio of 0.9/0.1. We also combine MLM\nand NSP together (i.e., ELECTRA-MLM+NSP) like BERT and use it as a baseline model.\nBesides our implementation, we compare with the following public works. Some of the results are from corre-\nsponding leaderboards.\nResponse Selection (RS). Individual scoring methods: Dual LSTM [32], SMN[40], DAM [41], BERT [1] and RoBERTa\n[4]. These models score each response in an example individually. Multi-choice method: including BERT-MC [1],\nRoBERTa-MC [4], OCN [42], GRN-v2 [43] and MDFN [44], are multi-choice models that handle all the responses\nat the same time.\nQuality Evaluation (QE). Reference-based metrics: BLEU [45] (we use the best result among BLEU-1,2,3 and 4),\nROUGE [46] , METEOR [47], BERTScore [48], ADEM [49], RUBER [50]. These methods are commonly-used\nmetrics in sequence-to-sequence tasks by calculating the word overlapping in the generated texts and the reference\ntexts. Following previous works in quality evaluation tasks, we use these metrics to meausre the similarity score\nbetween the candidate response and the reference response and use the similarity as the ﬁnal score. Reference-free\n9\nModel DailyDialog PERSONA-CHAT\nDev Test Dev Test\nPearson Spearman Pearson Spearman Pearson Spearman Pearson Spearman\nOur Re-running\nBLEU 0.32 0.14 † 0.31 0.25 0.35 0.31 0.36 0.35\nROUGE 0.34 0.22 0.33 0.26 0.36 0.40 0.32 0.43\nMETEOR 0.37 0.33 0.33 0.27 0.37 0.48 0.34 0.49\nBERTScore 0.38 0.31 0.37 0.39 0.40 0.49 0.41 0.42\nADEM 0.28 0.28 0.42 0.45 0.26 0.24 0.25 0.28\nRUBER 0.18 † 0.15† 0.36 0.30 0.33 0.34 0.38 0.35\nRoBERTa-eval 0.68 0.71 0.62 0.63 0.72 0.72 0.76 0.77\nOur Implementation\nELECTRA 0.47 0.50 0.45 0.46 0.44 0.46 0.52 0.52\nELECTRA-DAPO 0.76 (+29%) 0.75 (+25%) 0.77 (+32%) 0.78 (+32%) 0.74 (+30%) 0.74 (+28%) 0.80 (+28%) 0.81 (+29%)\nTable 3: Main results on DailyDialog and PERSONA-CHAT: Pearson and Spearman correlation with human judgements of overall quality on\nDailyDialog and PERSONA-CHAT datasets. All values that are not statistically signiﬁcant (p-value > 0.05) are marked by †. Scores in bold are\nthe best results. Following [21], we divide the two datasets into train/dev/test set randomly with the ratio 0.8/0.1/0.1, and re-run all the baselines.\nmetric: RoBERTa-eval [21], which relies on the powerful PrLM RoBERTa. It utilizes RoBERTalarge as its backbone\nmodel, which has almost the same number of parameters as the ELECTRAlarge we use. RoBERTa-eval is ﬁrst trained\nin an unsupervised way with an NSP task, and then do supervised training on the human-annotated dialog evaluation\ndataset. It evaluates dialogues with no reference responses and directly maps the response representation to a score.\n4.4. Implementation Details\nOur code is written based on Transformers3 , an open-source github repository. Some baselines in QE tasks\nare from [21], [48], and [51]. We use the ofﬁcial tokenizer for ELECTRA in Transformers to tokenize the input\ntext, which is based on punctuation splitting and wordpieces.\nWe use Adam [52] as the optimizer with ϵ = 1e-8 and no weight decay. The learning rate of our task-speciﬁc\npre-training (DAPO, MLM, and NSP) is 1e-5, batch size per GPU is 10, warmup rate is 0.1, and the max sequence\nlength is 512. We train 5 epochs on our dialogue corpus to get the pre-trained models. Then they are ﬁne-tuned for 8\nepochs on the open-domain dialogue tasks with learning rate, batch size, and warmup rate the same as pre-training.\nWe empirically set these hyperparameters without much tuning to show the robustness of our method.\n5. Experiments\n5.1. Main Results\nTables 2, 3, and 4 show the results on MuTual, MuTualplus, DailyDialog, PERSONA-CHAT, and the datasets in\n[39].4 We also highlight the absolute improvement over the ELECTRA baseline in Tables 2 and 3.\n3https://github.com/huggingface/transformers.\n4DAPO has kept the state-of-the-art results in the MuTual leaderboard for three months.\n10\nModel \\ Dataset GRADE DSTC-6 USR A.V .G.\nConvAI2 DailyDialog Empathetic perason-chat topical-chat\nBLEU 0.00 †/ 0.13 0.08 †/ 0.18 -0.05 †/ 0.00† 0.13 / 0.30 0.14 / 0.09 † 0.22 / 0.30 0.09 / 0.17\nROUGE 0.14 / 0.14 0.15 / 0.15 0.03 †/ -0.01† 0.33 / 0.33 0.07 †/ 0.09† 0.28 / 0.29 0.17 / 0.17\nMETEOR 0.15 / 0.18 0.10 †/ 0.01† 0.12 / 0.06† 0.31 / 0.32 0.25 / 0.27 0.34 / 0.39 0.21 / 0.21\nBERTScore 0.23 / 0.22 0.13 / 0.10 † 0.05†/ 0.03† 0.37 / 0.34 0.15 / 0.12† 0.30 / 0.33 0.21 / 0.19\nADEM -0.06 †/ -0.06† 0.06†/ 0.07† -0.04†/ -0.03† 0.15 / 0.12 -0.14 / -0.09 † -0.06†/ -0.06† -0.02 / -0.01\nRUBER -0.03 †/ -0.04† -0.08†/ -0.09† -0.08†/ -0.04† 0.11 / 0.09 0.13 / 0.19 0.25 / 0.26 0.05 / 0.06\nELECTRA-DAPO 0.29 / 0.30 0.34 / 0.33 0.44 / 0.43 0.23 / 0.24 0.24 / 0.21 0.34 / 0.30 0.31 / 0.30\nTable 4: Main results on the dialogue evaluation repository in [39]: The metrics are formatted as Pearson / Spearman. The repository contains\na large number of dialogue evaluation datasets. We directly apply our model and score the examples in these datasets without further training. All\nthese datasets are QE tasks in response-level. All values that are not statistically signiﬁcant (p-value > 0.05) are marked by †.\nThe results in Table 2 show that ELECTRA-DAPO surpasses most of the baselines signiﬁcantly. Besides, ELECTRA-\nDAPO gets comparable scores with previous SOTA methods (GRN-v2 and MDFN) that relies on exclusively-designed\nmodel architecture for response selection tasks, while we use a quite simple structure. The advantage in response\nselection task demonstrates the strong transfer learning ability of our model by modeling the ﬁne-grain quality of\ndialogues in the pre-training state.\nThe results in Table 3 show that ELECTRA-DAPO can be a good dialogue evaluator when the training set is\nrelatively small. Compared with RoBerta-eval, the previous SOTA, our model is signiﬁcantly better. We also see\nELECTRA-DAPO gets about 30 absolute scores than the baseline of DAPO, which shows the effectiveness of incor-\nporating dialogue-speciﬁc attributes in pre-training.\nELECTRA-DAPO can also serves as a good zero-shot evaluator when there is no further training data. We\ncompare ELECTRA-DAPO with several commonly-used metrics in Table 4, and our model correlates with the human\nannotation scores better. Although in some subsets ELECTRA-DAPO is not the best, it is more robust since it does\nnot fail on any subset (all scores >0.2). The average pearson/spearman correlation score is 0.31/0.30, which is 0.1\nhigher than the best baseline.\n5.2. Ablation Study\nAs mentioned in Section 3.3, we do speciﬁcity-oriented automatic rescoring on positive samples in our dialogue\ncorpus. To evaluate its contributions, we conduct an ablation study by removing it from our method and re-run the\ntasks. Speciﬁcally, a sample s is scored as follows:\nscore(s) =\n\n\n\n0 s∈S−\n1 s∈S+\nThe results are shown in Table 5 . Since the weakened scoring strategy still measures coherence, it surpasses the\nELECTRA baseline. However, without token-speciﬁcity which indicates whether a dialogue sample is Speciﬁc and\n11\nModel MuTual MuTual plus DailyDialog PERSONA-CHAT\nPearson / Spearman\nR@1 / R@2 / MRR R@1 / R@2 / MRR Dev Test Dev Test\nELECTRA 0.900 / 0.979 / 0.946 0.823 / 0.947 / 0.901 0.47 / 0.50 0.45 / 0.46 0.44 / 0.46 0.52 / 0.52\nELECTRA-DAPO w/o TS 0.903 / 0.980 / 0.947 0.819 / 0.945 / 0.899 0.66 / 0.67 0.69 / 0.70 0.63 / 0.65 0.66 / 0.72\nELECTRA-DAPO (3-NIDF) 0.916 / 0.988 / 0.956 0.836 / 0.955 / 0.910 0.76 / 0.75 0.77 / 0.78 0.74 / 0.74 0.80 / 0.81\nELECTRA-DAPO (2-NIDF) 0.907 / 0.981 / 0.950 0.827 / 0.958 / 0.902 0.75 / 0.73 0.65 / 0.69 0.65 / 0.65 0.71 / 0.73\nELECTRA-DAPO (1-NIDF) 0.904 / 0.980 / 0.949 0.819 / 0.940 / 0.904 0.73 / 0.65 0.55 / 0.60 0.63 / 0.60 0.65 / 0.69\nTable 5: Effect of different rescoring module: We re-run the experiments by replacing the automatic rescoring module with other variants, w/o\nTS refers to without token-speciﬁcity.\nFigure 4: Distribution of N-NIDF scores.\nInformative, the model becomes less powerful compared with the complete one. It also holds intuitively because the\ncomplete DAPO leverages more signiﬁcant qualities of dialogues simultaneously in pre-training.\n5.3. The inﬂuence of N in N-NIDF\nResults in Section 5.2 show the importance of speciﬁcity-oriented automatic rescoring in DAPO, thus it is rea-\nsonable to investigate the inﬂuence of N when calculating N-NIDF. We re-run the experiments by using 2-NIDF and\n1-NIDF as the token-speciﬁcity coefﬁcient. The results are also shown in Table 5 , indicating that DAPO with larger\nNin N-NIDF tends to have a better performance. We further explore the distribution ofN-NIDF scores in our corpus.\nFigure 4 shows clearly that the distribution of the 1-NIDF score is more concentrated than those for 2 and 3-NIDF\nscores, which makes it harder for the model to learn how to distinguish different dialogue samples. This may be a\npotential explanation for the above observation. It is also found that 1- to 4-NIDF scores generally follow a normal\ndistribution. To some extent, we believe this reﬂects the general pattern of human dialogues. The distribution of the\n4- and 5-NIDF are similar to 3-NIDF, except that the distribution of 5-NIDF has two peaks. We try N=3,4,5 on the\nzero-shot QE tasks, and the results are shown in Table 6 . We can clearly see that forN ≥3, the performance remains\nvirtually unchanged, so using N = 3is quite suitable.\n12\nModels \\ Correlation Pearson Spearman\nELECTRA + DAPO (3-NIDF) 0.314 0.302\nELECTRA + DAPO (4-NIDF) 0.313 0.307\nELECTRA + DAPO (5-NIDF) 0.307 0.304\nTable 6: More effects of different N in N-NIDF: We try 3,4,5 NIDF and get average Pearson and Spearman correleation values for datasets in\nthe repository [39]\nModel MuTual MuTual plus DailyDialog PERSONA-CHAT\nPearson / Spearman\nR@1 / R@2 / MRR R@1 / R@2 / MRR Dev Test Dev Test\nELECTRA-DAPO (UI+UR+UO) 0.916 / 0.988 / 0.956 0.836 / 0.955 / 0.910 0.76 / 0.75 0.77 / 0.78 0.74 / 0.74 0.80 / 0.81\nELECTRA-DAPO (Ubuntu) 0.274 / 0.519 / 0.539 0.284 / 0.537 / 0.547 0.09 / 0.11 0.25 / 0.21 0.20 / 0.16 0.11 / 0.11\nELECTRA-DAPO (only UI) 0.898 / 0.977 / 0.945 0.819 / 0.947 / 0.900 0.71 / 0.68 0.60 / 0.60 0.61 / 0.63 0.66 / 0.67\nELECTRA-DAPO (only UR) 0.900 / 0.972 / 0.945 0.821 / 0.944 / 0.900 0.73 / 0.70 0.72 / 0.73 0.67 / 0.67 0.75 / 0.77\nELECTRA-DAPO (only UO) 0.892 / 0.970 / 0.940 0.808 / 0.947 / 0.894 0.60 / 0.58 0.45 / 0.46 0.44 / 0.45 0.48 / 0.51\nTable 7: Effect of sample construction module and data source: We re-run the experiments by using only one of the methods in coherence-\noriented context corruption to construct the training set, and also try Ubuntu as the data source which contains much more noise.\n5.4. Construction of pre-training corpus\nSince the corpus of pre-training is newly generated, we further explore how different constructing methods inﬂu-\nence the performance of our models.\nWe ﬁrst re-construct another pre-training corpus with Ubuntu [32], which is noisy and not ﬁnely proofread. We\nalso modify the way of generating negative samples by using only one of UO, UI and UR to do coherence-oriented\ncontext corruption and construct 3 more pre-training corpora. The proportion of positive and negative samples are\nkept the same as before, i.e., |S+|: |S−|= 1 : 3. Experiments are done on these four new pre-training corpora and\nthe results are shown in Table 7 . Clearly, DAPO with all three kinds of negative samples boosts our models the\nmost, while using only one of the generating methods hurts the model performance more or less even with the same\nsized corpus. Accordingly, we argue that training models to tackle different kinds of negative samples in a single pre-\ntraining task makes them more robust. Thus the models can learn to address more complex dialogues through DAPO.\nWe also see that pre-training with the noisy data, Ubuntu, degrades the performance extremely. This can be largely\nattributed to the low Readability and coherence of Ubuntu dataset. Consequently, the model fails on the pre-training\ntask (only 0.009 and 0.011 for the Pearson and Spearman correlation metrics) and, in turn, on the downstream tasks.\n5.5. Comparison with Different Pre-training Objectives\nWe compare DAPO with representative coherence-oriented objectives mentioned in Section 4.3. Table 8 shows\nthe results. According to the results, ELECTRA-MLM, although dialogue-adaptive pre-trained, has much worse\nperformance than ELECTRA on almost all these open-domain dialogue tasks, which indicates that MLM would not be\na suitable pre-training objective for dialogue-based texts. The performance of ELECTRA-NSP is between ELECTRA\n13\nModel MuTual MuTual plus DailyDialog PERSONA-CHAT\nR@1 / R@2 / MRR R@1 / R@2 / MRR Pearson /Spearman Pearson /Spearman\nELECTRA 0.900 / 0.979 / 0.946 0.823 / 0.947 / 0.901 0.45 / 0.46 0.52 / 0.52\n+MLM 0.847 / 0.955 / 0.915 0.737 / 0.916 / 0.853 0.31 / 0.32 0.51 / 0.52\n+NSP 0.903 / 0.979 / 0.949 0.823 / 0.941 / 0.900 0.69 / 0.70 0.70 / 0.71\n+MLM+NSP 0.891 / 0.964 / 0.939 0.818 / 0.942 / 0.898 0.67 / 0.71 0.68 / 0.70\n+DAPO 0.916 / 0.988 / 0.956 0.836 / 0.955 / 0.910 0.77 / 0.78 0.80 / 0.81\nTable 8: Effects of different pre-training objectives: We pre-train our model on the same data source with different training objectives, and\nre-run the downstream tasks, the results on DailyDialog and PERASONA-CHAT are test sets.\nand ELECTRA-DAPO, which indicates that NSP is a feasible pre-training objective for dialogues. For ELECTRA-\nMLM+NSP, since it combines both proper and improper objectives, it is reasonable that it has performance between\nELECTRA-MLM and ELECTRA-NSP.\nIn addition, to verify if this kind of difference is caused by an insufﬁciency of pre-training, so we evaluate the\nmodels on the dev set of its pre-training corpus. The Pearson and Spearman correlation of ELECTRA-DAPO is\n0.810 and 0.690 respectively; the accuracy of ELECTRA-NSP and ELECTRA-MLM+NSP are 94.7% and 92.8%\nrespectively; and the perplexity of ELECTRA-MLM is 4.96. These values show that models are all fully pre-trained\ntowards the given objectives. The ﬁndings above gave another evidence that the performance of models mainly\ndepends on whether its pre-training objective is suitable for an open-domain dialogue corpus.\n6. Conclusion\nIn this work, we propose dialogue-adaptive pre-training objectives from quality estimation to capture exclusive\nattributes for open-domain dialogues. Through coherence-oriented context corruption and token-speciﬁcity rescoring\nsteps, we fuse quality estimation factors into pre-training, which enables models to learn more compatible language\nrepresentations for open-domain dialogue tasks. Experiments on widely-used open-domain dialogue datasets show\nour superiority over baseline methods. Beyond the common practice that merely uses general training objectives\nin domain-adaptive pre-training, our work further incorporates the speciﬁc features of the in-domain texts into pre-\ntraining tasks, empowering the PrLMs by modeling the diverse aspects of dialogue-related characteristics.\nReferences\n[1] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, in:\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, V olume 1 (Long and Short Papers), Association for Computational Linguistics, Minneapolis, Minnesota, 2019, pp. 4171–4186.\ndoi:10.18653/v1/N19-1423.\nURL https://www.aclweb.org/anthology/N19-1423\n[2] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, Improving language understanding by generative pre-training, Technical report,\nOpenAI.\n14\nURL https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/\nlanguage_understanding_paper.pdf\n[3] Z. Yang, Z. Dai, Y . Yang, J. G. Carbonell, R. Salakhutdinov, Q. V . Le, Xlnet: Generalized autoregressive pretraining for language under-\nstanding, in: H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alch´e-Buc, E. B. Fox, R. Garnett (Eds.), Advances in Neural Information\nProcessing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancou-\nver, BC, Canada, 2019, pp. 5754–5764.\nURL https://proceedings.neurips.cc/paper/2019/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.\nhtml\n[4] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, V . Stoyanov, RoBERTa: A robustly optimized bert\npretraining approach, arXiv:1907.11692.\nURL https://arxiv.org/abs/1907.11692\n[5] Y . Sun, S. Wang, Y . Li, S. Feng, X. Chen, H. Zhang, X. Tian, D. Zhu, H. Tian, H. Wu, ERNIE: Enhanced Representation through Knowledge\nIntegration, arXiv:1904.09223.\nURL https://arxiv.org/abs/1904.09223\n[6] M. Chen, S. Zhao, H. Liu, D. Cai, ERNIE 2.0: A continual pre-training framework for language understanding, in: The Thirty-Fourth AAAI\nConference on Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artiﬁcial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New York, NY , USA February 7-12,\n2020, AAAI Press, 2020, pp. 8968–8975.\nURL https://ojs.aaai.org//index.php/AAAI/article/view/6428\n[7] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, R. Soricut, Albert: A lite bert for self-supervised learning of language representations,\nin: International Conference on Learning Representations, 2020.\nURL https://openreview.net/forum?id=H1eA7AEtvS\n[8] K. Clark, M. Luong, Q. V . Le, C. D. Manning, ELECTRA: Pre-training text encoders as discriminators rather than generators, in: 8th\nInternational Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, OpenReview.net, 2020.\nURL https://openreview.net/forum?id=r1xMH1BtvB\n[9] N. Q. K. Le, Q.-T. Ho, Deep transformers and convolutional neural network in identifying dna n6-methyladenine sites in cross-species\ngenomes, Methods.\n[10] N. Q. K. Le, Q.-T. Ho, T.-T.-D. Nguyen, Y .-Y . Ou, A transformer architecture based on bert and 2d convolutional neural network to identify\ndna enhancers from sequence information, Brieﬁngs in bioinformatics 22 (5) (2021) bbab005.\n[11] W. L. Taylor, Cloze procedure: A new tool for measuring readability, Journalism quarterly 30 (4).\n[12] Y . Zhang, S. Sun, M. Galley, Y .-C. Chen, C. Brockett, X. Gao, J. Gao, J. Liu, B. Dolan, DialoGPT : Large-scale generative pre-training for\nconversational response generation, in: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System\nDemonstrations, Association for Computational Linguistics, Online, 2020, pp. 270–278. doi:10.18653/v1/2020.acl-demos.30.\nURL https://www.aclweb.org/anthology/2020.acl-demos.30\n[13] Z. Zhang, J. Li, H. Zhao, Multi-turn dialogue reading comprehension with pivot turns and knowledge, IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing 29 (2021) 1161–1173. doi:10.1109/TASLP.2021.3058616.\n[14] T. Whang, D. Lee, C. Lee, K. Yang, D. Oh, H. Lim, An effective domain adaptive post-training method for BERT in response selection, in:\nProc. Interspeech 2020, 2020, pp. 1585–1589. doi:10.21437/Interspeech.2020-2153.\nURL http://dx.doi.org/10.21437/Interspeech.2020-2153\n[15] C.-S. Wu, S. C. Hoi, R. Socher, C. Xiong, TOD-BERT: Pre-trained natural language understanding for task-oriented dialogue, in: Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics, Online,\n2020, pp. 917–929. doi:10.18653/v1/2020.emnlp-main.66.\nURL https://www.aclweb.org/anthology/2020.emnlp-main.66\n15\n[16] R. Xu, C. Tao, D. Jiang, X. Zhao, D. Zhao, R. Yan, Learning an effective context-response matching model with self-supervised tasks for\nretrieval-based dialogues, CoRR abs/2009.06265. arXiv:2009.06265.\nURL https://arxiv.org/abs/2009.06265\n[17] A. See, S. Roller, D. Kiela, J. Weston, What makes a good conversation? How controllable attributes affect human judgments, in: Proceedings\nof the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,\nV olume 1 (Long and Short Papers), Association for Computational Linguistics, Minneapolis, Minnesota, 2019, pp. 1702–1723. doi:\n10.18653/v1/N19-1170.\nURL https://www.aclweb.org/anthology/N19-1170\n[18] S. Mehri, M. Eskenazi, Unsupervised evaluation of interactive dialog with DialoGPT, in: Proceedings of the 21th Annual Meeting of the\nSpecial Interest Group on Discourse and Dialogue, Association for Computational Linguistics, 1st virtual meeting, 2020, pp. 225–235.\nURL https://www.aclweb.org/anthology/2020.sigdial-1.28\n[19] S. Mehri, M. Eskenazi, USR: An unsupervised and reference free evaluation metric for dialog generation, in: Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics, Online, 2020, pp. 681–707.\ndoi:10.18653/v1/2020.acl-main.64.\nURL https://aclanthology.org/2020.acl-main.64\n[20] B. Pang, E. Nijkamp, W. Han, L. Zhou, Y . Liu, K. Tu, Towards holistic and automatic evaluation of open-domain dialogue generation,\nin: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics,\nOnline, 2020, pp. 3619–3629. doi:10.18653/v1/2020.acl-main.333.\nURL https://www.aclweb.org/anthology/2020.acl-main.333\n[21] T. Zhao, D. Lala, T. Kawahara, Designing precise and robust dialogue response evaluators, in: Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, Association for Computational Linguistics, Online, 2020, pp. 26–33.doi:10.18653/v1/\n2020.acl-main.4.\nURL https://www.aclweb.org/anthology/2020.acl-main.4\n[22] R. Barzilay, M. Lapata, Modeling local coherence: An entity-based approach, in: Proceedings of the 43rd Annual Meeting of the Association\nfor Computational Linguistics (ACL’05), Association for Computational Linguistics, Ann Arbor, Michigan, 2005, pp. 141–148. doi:\n10.3115/1219840.1219858.\nURL https://www.aclweb.org/anthology/P05-1018\n[23] A. Cervone, E. Stepanov, G. Riccardi, Coherence models for dialogue, in: Proc. Interspeech 2018, 2018, pp. 1011–1015. doi:10.21437/\nInterspeech.2018-2446.\nURL http://dx.doi.org/10.21437/Interspeech.2018-2446\n[24] M. Mesgar, S. B ¨ucker, I. Gurevych, Dialogue coherence assessment without explicit dialogue act labels, in: Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, Association for Computational Linguistics, Online, 2020, pp. 1439–1450. doi:\n10.18653/v1/2020.acl-main.133.\nURL https://www.aclweb.org/anthology/2020.acl-main.133\n[25] T. Wolf, V . Sanh, J. Chaumond, C. Delangue, Transfertransfo: A transfer learning approach for neural network based conversational agents,\nCoRR abs/1901.08149. arXiv:1901.08149.\nURL http://arxiv.org/abs/1901.08149\n[26] S. Bao, H. He, F. Wang, H. Wu, H. Wang, PLATO: Pre-trained dialogue generation model with discrete latent variable, in: Proceedings of the\n58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics, Online, 2020, pp. 85–96.\ndoi:10.18653/v1/2020.acl-main.9.\nURL https://www.aclweb.org/anthology/2020.acl-main.9\n[27] M. Henderson, I. Casanueva, N. Mrk ˇsi´c, P.-H. Su, T.-H. Wen, I. Vuli´c, ConveRT: Efﬁcient and accurate conversational representations from\ntransformers, in: Findings of the Association for Computational Linguistics: EMNLP 2020, Association for Computational Linguistics,\n16\nOnline, 2020, pp. 2161–2174. doi:10.18653/v1/2020.findings-emnlp.196.\nURL https://www.aclweb.org/anthology/2020.findings-emnlp.196\n[28] J.-C. Gu, T. Li, Z.-H. Ling, Q. Liu, Z. Su, Y .-P. Ruan, X. Zhu, Deep contextualized utterance representations for response selection and\ndialogue analysis, IEEE/ACM Transactions on Audio, Speech, and Language Processing 29 (2021) 2443–2455. doi:10.1109/TASLP.\n2021.3074788.\n[29] Y . Su, Y . Wang, D. Cai, S. Baker, A. Korhonen, N. Collier, Prototype-to-style: Dialogue generation with style-aware editing on retrieval\nmemory, IEEE/ACM Transactions on Audio, Speech, and Language Processing 29 (2021) 2152–2161. doi:10.1109/TASLP.2021.\n3087948.\n[30] P. Kumar, D. Brahma, H. Karnick, P. Rai, Deep attentive ranking networks for learning to order sentences, Proceedings of the AAAI Confer-\nence on Artiﬁcial Intelligence 34 (05) (2020) 8115–8122. doi:10.1609/aaai.v34i05.6323.\nURL https://ojs.aaai.org/index.php/AAAI/article/view/6323\n[31] X. Gu, K. M. Yoo, J. Ha, Dialogbert: Discourse-aware response generation via learning to recover and rank utterances, CoRR abs/2012.01775.\narXiv:2012.01775.\nURL https://arxiv.org/abs/2012.01775\n[32] R. Lowe, N. Pow, I. Serban, J. Pineau, The Ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems,\nin: Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, Association for Computational\nLinguistics, Prague, Czech Republic, 2015, pp. 285–294. doi:10.18653/v1/W15-4640.\nURL https://www.aclweb.org/anthology/W15-4640\n[33] Y . Li, H. Su, X. Shen, W. Li, Z. Cao, S. Niu, DailyDialog: A manually labelled multi-turn dialogue dataset, in: Proceedings of the Eighth\nInternational Joint Conference on Natural Language Processing (V olume 1: Long Papers), Asian Federation of Natural Language Processing,\nTaipei, Taiwan, 2017, pp. 986–995.\nURL https://www.aclweb.org/anthology/I17-1099\n[34] S. Zhang, E. Dinan, J. Urbanek, A. Szlam, D. Kiela, J. Weston, Personalizing dialogue agents: I have a dog, do you have pets too?, in:\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers), Association for Com-\nputational Linguistics, Melbourne, Australia, 2018, pp. 2204–2213. doi:10.18653/v1/P18-1205.\nURL https://www.aclweb.org/anthology/P18-1205\n[35] K. Gopalakrishnan, B. Hedayatnia, Q. Chen, A. Gottardi, S. Kwatra, A. Venkatesh, R. Gabriel, D. Hakkani-Tur, Topical-Chat: Towards\nknowledge-grounded open-domain conversations, in: Proc. Interspeech 2019, 2019, pp. 1891–1895. doi:10.21437/Interspeech.\n2019-3079.\nURL http://dx.doi.org/10.21437/Interspeech.2019-3079\n[36] E. M. Smith, M. Williamson, K. Shuster, J. Weston, Y .-L. Boureau, Can you put it all together: Evaluating conversational agents’ ability to\nblend skills, in: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational\nLinguistics, Online, 2020, pp. 2021–2030. doi:10.18653/v1/2020.acl-main.183.\nURL https://www.aclweb.org/anthology/2020.acl-main.183\n[37] E. M. V oorhees, D. M. Tice, The TREC-8 question answering track, in: Proceedings of the Second International Conference on Language\nResources and Evaluation (LREC’00), European Language Resources Association (ELRA), Athens, Greece, 2000.\nURL http://www.lrec-conf.org/proceedings/lrec2000/pdf/26.pdf\n[38] L. Cui, Y . Wu, S. Liu, Y . Zhang, M. Zhou, MuTual: A dataset for multi-turn dialogue reasoning, in: Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, Association for Computational Linguistics, Online, 2020, pp. 1406–1416. doi:\n10.18653/v1/2020.acl-main.130.\nURL https://www.aclweb.org/anthology/2020.acl-main.130\n[39] Y . Yeh, M. Esk´enazi, S. Mehri, A comprehensive assessment of dialog evaluation metrics, CoRR abs/2106.03706. arXiv:2106.03706.\nURL https://arxiv.org/abs/2106.03706\n17\n[40] Y . Wu, W. Wu, C. Xing, M. Zhou, Z. Li, Sequential matching network: A new architecture for multi-turn response selection in retrieval-based\nchatbots, in: Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers), Association\nfor Computational Linguistics, Vancouver, Canada, 2017, pp. 496–505. doi:10.18653/v1/P17-1046.\nURL https://www.aclweb.org/anthology/P17-1046\n[41] X. Zhou, L. Li, D. Dong, Y . Liu, Y . Chen, W. X. Zhao, D. Yu, H. Wu, Multi-turn response selection for chatbots with deep attention matching\nnetwork, in: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers), Association\nfor Computational Linguistics, Melbourne, Australia, 2018, pp. 1118–1127. doi:10.18653/v1/P18-1103.\nURL https://www.aclweb.org/anthology/P18-1103\n[42] Q. Ran, P. Li, W. Hu, J. Zhou, Option comparison network for multiple-choice reading comprehension, CoRR abs/1903.03033. arXiv:\n1903.03033.\nURL http://arxiv.org/abs/1903.03033\n[43] Y . Liu, S. Feng, D. Wang, K. Song, F. Ren, Y . Zhang, A graph reasoning network for multi-turn response selection via customized pre-\ntraining, Proceedings of the AAAI Conference on Artiﬁcial Intelligence 35 (15) (2021) 13433–13442.\nURL https://ojs.aaai.org/index.php/AAAI/article/view/17585\n[44] L. Liu, Z. Zhang, H. Zhao, X. Zhou, X. Zhou, Filling the gap of utterance-aware and speaker-aware representation for multi-turn dialogue,\nProceedings of the AAAI Conference on Artiﬁcial Intelligence 35 (15) (2021) 13406–13414.\nURL https://ojs.aaai.org/index.php/AAAI/article/view/17582\n[45] K. Papineni, S. Roukos, T. Ward, W.-J. Zhu, BLEU: A method for automatic evaluation of machine translation, in: Proceedings of the 40th\nAnnual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics, Philadelphia, Pennsylvania,\nUSA, 2002, pp. 311–318. doi:10.3115/1073083.1073135.\nURL https://www.aclweb.org/anthology/P02-1040\n[46] C.-Y . Lin, ROUGE: A package for automatic evaluation of summaries, in: Text Summarization Branches Out, Association for Computational\nLinguistics, Barcelona, Spain, 2004, pp. 74–81.\nURL https://www.aclweb.org/anthology/W04-1013\n[47] S. Banerjee, A. Lavie, METEOR: An automatic metric for MT evaluation with improved correlation with human judgments, in: Proceedings\nof the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, Association for Com-\nputational Linguistics, Ann Arbor, Michigan, 2005, pp. 65–72.\nURL https://www.aclweb.org/anthology/W05-0909\n[48] T. Zhang, V . Kishore, F. Wu, K. Q. Weinberger, Y . Artzi, BERTScore: Evaluating text generation with BERT, in: 8th International Conference\non Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, OpenReview.net, 2020.\nURL https://openreview.net/forum?id=SkeHuCVFDr\n[49] R. Lowe, M. Noseworthy, I. V . Serban, N. Angelard-Gontier, Y . Bengio, J. Pineau, Towards an automatic Turing test: Learning to evaluate\ndialogue responses, in: Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (V olume 1: Long Papers),\nAssociation for Computational Linguistics, Vancouver, Canada, 2017, pp. 1116–1126. doi:10.18653/v1/P17-1103.\nURL https://www.aclweb.org/anthology/P17-1103\n[50] C. Tao, L. Mou, D. Zhao, R. Yan, RUBER: An unsupervised method for automatic evaluation of open-domain dialog systems, in: S. A.\nMcIlraith, K. Q. Weinberger (Eds.), Proceedings of the Thirty-Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th\ninnovative Applications of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artiﬁcial Intelligence\n(EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, AAAI Press, 2018, pp. 722–729.\nURL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16179\n[51] S. Sharma, L. E. Asri, H. Schulz, J. Zumer, Relevance of unsupervised metrics in task-oriented dialogue for evaluating natural language\ngeneration, CoRR.\nURL https://openreview.net/forum?id=r17lFgZ0Z\n18\n[52] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, in: Y . Bengio, Y . LeCun (Eds.), 3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.\nURL http://arxiv.org/abs/1412.6980\n19"
}