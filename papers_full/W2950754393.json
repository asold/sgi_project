{
  "title": "Morphological Verb-Aware Tibetan Language Model",
  "url": "https://openalex.org/W2950754393",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A5072794373",
      "name": "Kuntharrgyal Khysru",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A5012455357",
      "name": "Di Jin",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A5017251198",
      "name": "Jianwu Dang",
      "affiliations": [
        "Tianjin University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1798551699",
    "https://openalex.org/W2155388323",
    "https://openalex.org/W6731803677",
    "https://openalex.org/W6691746754",
    "https://openalex.org/W6684738654",
    "https://openalex.org/W2613532868",
    "https://openalex.org/W2112874453",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W2137387514",
    "https://openalex.org/W2020073413",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W1976519809",
    "https://openalex.org/W1985258458",
    "https://openalex.org/W6663217074",
    "https://openalex.org/W6713736167",
    "https://openalex.org/W2172083727",
    "https://openalex.org/W2001331936",
    "https://openalex.org/W6630646690",
    "https://openalex.org/W2963451498",
    "https://openalex.org/W2765300901",
    "https://openalex.org/W1634817683",
    "https://openalex.org/W2805965938",
    "https://openalex.org/W2127836646",
    "https://openalex.org/W2158025800",
    "https://openalex.org/W2912943230",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W6640598943",
    "https://openalex.org/W6678277124",
    "https://openalex.org/W1972594981",
    "https://openalex.org/W6607333740",
    "https://openalex.org/W6674571003",
    "https://openalex.org/W6677328538",
    "https://openalex.org/W6675421170",
    "https://openalex.org/W6694492165",
    "https://openalex.org/W6712235582",
    "https://openalex.org/W2345190899",
    "https://openalex.org/W2047506955",
    "https://openalex.org/W2401969231",
    "https://openalex.org/W6675002571",
    "https://openalex.org/W2524475847",
    "https://openalex.org/W2950607315",
    "https://openalex.org/W2251012068",
    "https://openalex.org/W2271177914",
    "https://openalex.org/W1512898506",
    "https://openalex.org/W2167419393",
    "https://openalex.org/W2097333193",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2397092652",
    "https://openalex.org/W2099873701",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W2050469586",
    "https://openalex.org/W2405047074",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W2117278770",
    "https://openalex.org/W2104441213"
  ],
  "abstract": "The Tibetan language model (TLM) is the key to Tibetan natural language processing. In this paper, we first observe that, different from widely used languages, Tibetan contains many morphological verbs that rarely appear in natural sentences but play a key role in accurate text prediction. This property is usually ignored by existing methods and makes traditional training strategies less effective in constructing accurate and robust TLMs. Hence, we propose a morphological verb-aware TLM by offline learning via a character frequency reweighting strategy and online tuning of discriminative weights conditioned on morphological verbs. However, because of the influence of morphological verbs on the tense and semantics of sentences, it is necessary to consider the morphological verbs in Tibetan. As a result, compared with state-of-the-art methods, our method not only reduces the perplexity but also improves the character error on tasks of the text prediction and automatic speech recognition (ASR).",
  "full_text": "Received April 29, 2019, accepted May 15, 2019, date of publication May 27, 2019, date of current version June 17, 2019.\nDigital Object Identifier 10.1 109/ACCESS.2019.2919328\nMorphological Verb-Aware Tibetan\nLanguage Model\nKUNTHARRGYAL KHYSRU\n 1, DI JIN1, (Member, IEEE), AND JIANWU DANG1,2, (Member, IEEE)\n1Tianjin Key Laboratory of Cognitive Computing and Application, College of Intelligence and Computing, Tianjin University, Tianjin 300350, China\n2Japan Advanced Institute of Science and Technology, Ishikawa 9231292, Japan\nCorresponding author: Di Jin (jindi@tju.edu.cn)\nThis work was supported in part by the National Key R & D Program of China under Grant 2018YFC0809800, and in part by the Natural\nScience Foundation of China under Grant 61772361.\nABSTRACT The Tibetan language model (TLM) is the key to Tibetan natural language processing. In this\npaper, we ﬁrst observe that, different from widely used languages, Tibetan contains many morphological\nverbs that rarely appear in natural sentences but play a key role in accurate text prediction. This property is\nusually ignored by existing methods and makes traditional training strategies less effective in constructing\naccurate and robust TLMs. Hence, we propose a morphological verb-aware TLM by ofﬂine learning via\na character frequency reweighting strategy and online tuning of discriminative weights conditioned on\nmorphological verbs. However, because of the inﬂuence of morphological verbs on the tense and semantics\nof sentences, it is necessary to consider the morphological verbs in Tibetan. As a result, compared with state-\nof-the-art methods, our method not only reduces the perplexity but also improves the character error on tasks\nof the text prediction and automatic speech recognition (ASR).\nINDEX TERMS Tibetan language model, text prediction, automatic speech recognition, morphological\nverb-aware model.\nI. INTRODUCTION\nStatistical language models (LMs) represent the probabil-\nity that a sequence of words is a sentence and have been\nwidely used in text prediction tasks, automatic speech recog-\nnition (ASR), machine translation, handwriting recognition\nand information retrieval [1]–[4]. N-gram LMs [5]–[7] are\nnotable models that can be trained efﬁciently and have a\npowerful capacity to generalize. However, n-gram LMs usu-\nally struggle with modeling long-distance context dependen-\ncies. Recently, this problem has been signiﬁcantly alleviated\nby modeling with a recurrent neural network (RNN)-based\nLM (RNNLM) [8]–[11].\nHowever, an RNNLM is a state-of-the-art model that\nrequires a large training dataset to learn effective param-\neters, leading to a severe data sparsity problem in which\nthe language formation cannot be effectively represented by\nthe LM due to the lack of training data or the key words\nrarely appearing;these key words are denoted as rare words.\nThis situation is considerably severe for languages with low\ntraining resources. To alleviate this problem, recent works\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Mohammad Shorif Uddin.\nhave attempted to reduce the number of trainable parameters\nby adding a compression layer into the LMs [7], [12] or\nperforming data augmentation by adapting a large dataset\nto a small dataset. However, such methods cannot solve the\nproblem introduced by rare words. Then, [12] and [13] pro-\nposed building an LM on the morpheme level via feature-rich\nmodeling, which helps the RNN learn a more effective con-\ntext for the LM. In addition, other structure information,\ne.g, subword, character, morph and morphology, have also\nbeen used to improve the LM [14]–[17], [19]. However, all\nof these works mainly focus on widely used languages, e.g.,\nEnglish or Chinese, while ignoring the speciﬁc properties of\nother low-resource languages, e.g., Tibetan. Hence, existing\ntraining methods for generic LMs still have great potential\nto be improved by carefully considering language-related\nproperties.\nThere are few studies on the Tibetan language model\n(TLM), and the existing models basically use the n-gram\nmethod [30]. Recently, inspired by [8], [18] proposed build-\ning a TLM on the radical level instead of the character level\nto capture structural information from characters. Three types\nof embedded fusion methods have been proposed to enhance\nthe model, including using uniform weight (TRU), different\n72896\n2169-3536 \n 2019 IEEE. Translations and content mining are permitted for academic research only.\nPersonal use is also permitted, but republication/redistribution requires IEEE permission.\nSee http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\nVOLUME 7, 2019\nK. Kuntharrgyalet al.: Morphological Verb-Aware TLM\nweights (TRD) and radical combination (TRC). The purpose\nof our proposed model is to embed character embedding\nwith a speciﬁc Tibetan radical unit and interpolation base\nand explore the different characteristics of radical embed-\nding by introducing different radical embedding factors to\nmake the model more ﬂexible. Each radical embedding can\nbe interpolated according to its contribution to the overall\nmeaning of the corresponding character. In addition, there is a\nradical combination phenomenon in Tibetan. Radical embed-\nding combination code can make full use of the nature of\nTibetan radical embedding. Introduced by radical embedding,\ncharacter embedding is more useful for enhancing semantic\ninformation, which can help solve data sparsity problems.\nIn this paper, we address the problem of constructing\na TLM by focusing on Tibetan morphological verbs. Our\ncontributions are three-fold. First, we study the importance\nof morphological verbs in TLM with the observation that:\nalthough morphological verbs rarely appear in Tibetan sen-\ntences, they constitute a large proportion of Tibetan words\nand play a key role in an effective TLM. Second, we pro-\npose an effective ofﬂine learning method by reweighting the\ncharacter frequency, which results in a more powerful TLM.\nThird, we further enhance the importance of morphological\nverbs through online tuning of their discriminative weights\nto make an adaptive ofﬂine learning model. With the above\nefforts, compared with the baseline model, our new TLM\nachieves much better results on tasks of text prediction and\nASR.\nII. RELATED WORK\nIn this section, we investigate and discuss work related to\nLMs. We investigate work based on multiple granular meth-\nods and traditional methods and discuss the proposed state-\nof-the-art model for advancing the state-of-the-art.\nA. DIFFERENT GRANULARITY OF INPUT\nFor low-resource languages, rare word processing is a\ncommon problem. [8] proposed a character embedding\nlevel-based model. They built a deeper network for reduc-\ning the traditional word embedding level to the character\nembedding level by the technology of a highway network,\nwhich avoided the problem of large-scale embedding com-\nputation and low-frequency words and obtained good per-\nformance. The model consisted of two parts, one that put\nthe character embedding level as input to a convolutional\nneural network (CNN) and another that used the output of the\nCNN and the highway network as the input of an RNNLM.\nFurthermore, [4] also presented a model that combines an\nRNN with a character embedding level-based CNN.\nIn [8], by reducing the traditional word embedding level to\nthe character level, large-scale embedding calculations and\nrare words were avoided, and a deeper network was con-\nstructed by highway network technology, which gave good\nresults. The model consisted of two parts, the character level\nas input to the CNN, and input to the RNNLM through\nthe output of the CNN and highway network, but the ﬁnal\nprediction was still a word. Semantic and grammatical infor-\nmation can be obtained by experimenting with multiple lan-\nguage corpora as tests.\nFor morphologically rich languages, it is helpful to study\nthe internal structural formation of words [31]–[33]. English\nmorphology is the study of the relationship between the\ncompositions of a word and the attempt to sort out the rules of\nits composition but without syntax and semantic information.\nThus, language models can apply such information to obtain\nword sequences with high accuracy.\nB. USING AN ADAPTIVE APPROACH\nIn an actual ASR task, a large number of domain matching\ndatasets are not available. The LM requires domain-adaptive\ntechniques to allow the use of multiple extradomain text\nresources [34], [35]. One of the most suitable methods for\ndomain adaptation in language modeling is based on hybrid\nmodels [36], [37]. An adapted model can be constructed by\ncombining LMs and hybrid weighting separately constructed\nfrom extradomain text resources [38]. In [39], because there\nwas out-of-domain data, we applied larger data to adapt\nsmall data to solve the problem of an insufﬁcient data\nvolume.\nTo use the LM built from extradomain text resources\nfor ﬂexible domain adaptation, [28] and [29] developed a\nmethod for model merging in the potential variable space.\nIn the latent variable space, a word is mapped to a potential\nvariable space, so it can be expected to perform more ﬂexible\nstate sharing than possible in the observed word space. Thus,\nthis paper introduces latent word LMs (LWLMs) into hybrid\nmodeling [40]–[43]. The latent variables in the normal class\nof n-gram LMs are only model dependent indices, so each\nmodel has a different potential variable space [44], [45].\nTherefore, the traditional class-based n-gram hybrid model\nmust be performed in the observed text space [39], [46].\nAdditionally, latent variables in LWLMs are represented as\nspeciﬁc potential words, and multiple LWLMs can share a\ncommon potential variable space, which enables us to per-\nform ﬂexible hybrid modeling while considering the potential\nvariable space.\nIII. BACKGROUND\nA. INTRODUCTION OF TIBETAN PROPERTIES\nTibetan belongs to the Tibeto-Burman language family,\nwhich is a language with a long history and great inﬂuence\nin the Sino-Tibetan language family. The Tibetan language\nin China is traditionally divided into the Lhasa dialect, Kham\ndialect and Amdo dialect. There is a certain difﬁculty in the\ndifference between the Lhasa dialect and the Amdo dialect,\nand the Kham dialect is close to the Lhasa dialect. The\ninternal differences in the Tibetan language are quite obvious.\nThere are tones in the Lhasa and Kham dialects, but there are\nno tones in the Amdo dialect. Tibetan grammar is isolated,\nmainly relying on word order and auxiliary words to express\nvarious grammatical relations, and some verbs appear as\ninﬂections.\nVOLUME 7, 2019 72897\nK. Kuntharrgyalet al.: Morphological Verb-Aware TLM\nTABLE 1. Relationship between morphological verbs/morphologically invariant verbs and tense.\nFIGURE 1. Structure of the baseline model: RNNLM.\nTibetan verbs are the key to understanding the meaning of a\nsentence and can be grouped into different subsets according\nto different properties. In particular, Tibetan verbs can be\nclassiﬁed into morphological and morphologically invariant\nverbs according to their morphological properties. Speciﬁ-\ncally, morphological verbs may contain 2 to 4 tense variants,\nincluding future tense, present tense, past tense and command\ntense.\nAccording to the characteristics of the changes in Tibetan\nverbs, we can see in Fig 1 and Fig 2 that they can be\ndivided into morphological verbs and morphological nonvari-\nable words. Morphological change verbs can be divided into\nfour morphological changes, three morphological changes\nand two morphological changes. The four morphological\nchanges refer to changes in character shape in the future,\npresent time, completion time and command time. The three\nmorphological changes mean that the morphological verb\nis generally one of a future time, present time, completion\ntime and command time. The form is the same as one of the\nother three, but there is no law. The two form changes are\nthe future, the present time, the completion time as well as\nthe command time and one of the other three or two forms\nand the other. The two forms are the same, and there is no\nlaw.\nDifferent from morphological verbs of English, morpho-\nlogical verbs of Tibetan are deﬁned on the character level and\ncontain additional tension, i.e, command tension, which is\nvery important for sentence understanding [22]–[25]. Three\ncases of morphological verbs and one case of morphological\ninvariant verbs are shown in Table 1. It can be seen that there\nare different changes in the sentences of different tense verbs,\nand these changes have an effect on the semantics of the\nsentence.\nDue to the importance of morphological verbs for under-\nstanding Tibetan sentences, we propose a morphological\nFIGURE 2. The distribution of the graph above in the corpus: A is the\nproportion of morphological verbs in the Tibetan audio corpus; B is the\nproportion of morphological verbs in the dictionary; C and D are the\nproportions of the training corpus and the testing morphological verbs.\nverb-aware TLM for ofﬂine learning via character frequency\nreweighting and online tuning of the discriminative weight of\nmorphological verbs. Experiments validate the effectiveness\nof our method.\nB. BASELINE MODEL FOR TIBETAN\nWe can use an RNNLM [10] to the model Tibetan lan-\nguage. Speciﬁcally, to predict the current character wi,\nwe can encode the full history of the current character as <\nwi−1,....,w1 >, and compute the probabilities via a three layer\nRNN by\nPRNN (wi|<wi−1,...,w1 >) =PRNN (wi|wi−1,vi−2), (1)\nwhere vi−2 denotes the remaining historical context from\n1 to i −2. We further modify Eq. (1) by adding a class-based\nfactorized output layer structure. Each word in the output\nlayer vocabulary is attributed to a unique class-based on a\nfrequency count. We then obtain\nPRNN (wi|wi−1,vi−2) =PRNN (wi|vi−1)\n=P(wi|ci,vi−1)p(ci|vi−1), (2)\nci denotes the class label. P(ci|vi−1) is the conditional\nprobability based class and is deﬁned as a grouping result\naccording to the character frequency on a training corpus.\nWe also show the structure of Eq. (2) in Fig 2 and Fig 3 It has\nbeen demonstrated that an RNNLM with Eq. (2) can capture\nlong-distance dependencies in English. P(ci|vi−1) plays a\nkey role in such a model and relies on the classiﬁcation of\n72898 VOLUME 7, 2019\nK. Kuntharrgyalet al.: Morphological Verb-Aware TLM\nFIGURE 3. shows the frequency distribution of morphological verbs in our corpus.\ncharacters based on a frequency count. This strategy does\nnot consider the key to speciﬁc language properties, e.g,\nthe importance of morphological verbs, in representing LMs.\nIV. MORPHOLOGICAL VERB-AWARE TIBETAN\nLANGUAGE MODEL (TLM)\nVerbs in natural language are indispensable parts of speech\nand are the core of sentence expression. The division of verbs\nis different because of language differences. Morphological\nverbs of Tibetan relate to not only semantics but also tense.\nHence, it is very important to explore more effective training\nmethods for TLMs considering morphological verbs with low\nresources.\nA. KEY OF MORPHOLOGICAL VERBS IN TLM\nThe character is the smallest and most meaningful unit in\nTibetan, analogous to words in English. As shown in Fig 2,\nwe ﬁnd that the proportions of morphological verbs in\nTibetan are large, i.e., 25% and 22% in the corpus and dic-\ntionary (A and B in Fig 2), respectively, and 14% of the\nproportion of the training set and testing set are part of the\nmorphological verbs (C and D in Fig 2). These verbs affect\nnot only the temporal relationship of sentences but also the\nsemantics of sentences.\nHowever, as shown in Fig 3, the proportion of those impor-\ntant morphological verbs in our training corpus is low, which\ncauses the trained TLM to easily miss semantic information\nfrom morphological verbs and directly affects the accuracy\nand speed of TLM-based recognition tasks. For example,\nwe trained two TLMs based on a traditional RNNLM and\nour morphological verb-aware TLM on our corpus and used\nthem to perform text prediction. Considering morphological\nverbs in both ofﬂine learning and online tuning, our method\nobtained a much lower prediction error.\nIn Table 2 and Fig 3, we can see that morphological verbs in\nTibetan play an important role in the understanding of Tibetan\nsentences. In practice, because Tibetan is a low-resource\nlanguage, audio and text data are limited. Therefore, we have\nlimitations in obtaining morphological verb information in\nTibetan. Therefore, it is necessary to increase the weight\nTABLE 2. Statistics of Tibetan text data.\nof morphological verbs to enhance such words and more\naccurately predict sentence semantics.\nB. OFFLINE LEARNING VIA CHARACTER FREQUENCY\nREWEIGHTING\nIn Eq. (2), P(ci|vi−1) is the conditional probability based on\nclass ci and is obtained according to the character frequency\nin the training corpus. ci denotes the class of character wi\nin the training corpus. P(ci|vi−1) provides a discriminative\nweight, i.e, a prior, about the prediction of wi given historical\ninformation vi−1 and helps the RNN model estimate more\naccurately. However, such a prior based character frequency\nignores the importance of morphological verbs in under-\nstanding sentences. To overcome this problem, we propose\nreweighting the character frequency of morphological verbs\nin the training corpus.\nSpeciﬁcally, after we calculate the character frequency\nof all characters in the corpus, we reweight the character\nfrequency of morphological verbs by multiplying their values\nby a weight β. In this paper, we set β=3.We then classify all\nof the characters according to this new reweighted character\nfrequency, thus leading to a new P(ci|vi−1). Thus, some of the\noriginal low-frequency morphological verbs can be divided\ninto new higher classes to improve the prediction speed\nand accuracy. This method is called RNNLM_character fre-\nquency reweighting (_CFR).\nC. ONLINE TUNING DISCRIMINATIVE WEIGHTS\nTo further understand morphological verbs during testing,\nwe propose online tuning of the discriminative weights,\ni.e,P(ci|vi−1), to improve the prediction accuracy. The entire\nVOLUME 7, 2019 72899\nK. Kuntharrgyalet al.: Morphological Verb-Aware TLM\nTABLE 3. The basic information on the Tibetan audio data.\nFIGURE 4. An example of online tuning discriminative weights.\nprocess is shown in Fig 4. Speciﬁcally, the output of an\nRNNLM is given to a threshold ε. Then, a binary output is\ngenerated\n¯PRNN (wi|vi−1) =PRNN (wi|vi−1) >ε. (3)\nThe values of ¯PRNN (wi|vi−1) that do not belong to the mor-\nphological verb are set to 0, and ˜PRNN (wi|vi−1). We regard\n˜PRNN (wi|vi−1) as the prediction of the RNNLM and\nback-project it to P(ci|vi−1) and obtain\n˜P(ci|vi−1) =\n˜PRNN (wi|vi−1)\nP(wi|ci,vi−1) . (4)\nWe generate a new P(ci|vi−1) by combining it with ˜P(ci|vi−1)\n¯P(ci|vi−1) =P(ci|vi−1) +α˜P(ci|vi−1), (5)\nwhere α denotes the combination weight of ˜P(ci|vi−1).\nWe denote this method as RNNLM_tuning discriminative\nweights (_TDW).\nV. EXPERIMENTS\nIn the experimental section, we ﬁrst introduce the exper-\nimental corpus, including the speech corpus and the text\ncorpus of the Lhasa dialect. Then, based on the baseline\nLM modeling method and our proposed method, we further\ncompare the existing methods with our results in different\nhidden layers.Next, we interpolate our method with the tradi-\ntional n-gram method and compare it with the results of other\ninterpolations. In the experiment, we denoted the Kneser-Ney\nsmoothed 3-gram as KN3. Finally, applying our method to\nASR veriﬁes the effectiveness of our approach.\nA. SETUP\nThrough experiments, we verify the performance of our\nmethod in the Tibetan language. We address the character\nTABLE 4. Comparison between the PPL of the previous method and our\nmethod compare on the same topic.\nTABLE 5. Comparison of the PPL on the same topic after interpolation\nwith our method.\nas input. In the experiment, we chose perplexity (PPL) and\ncharacter error rates (CERs) as the evaluation criteria.\nThe choice of corpus directly affects the quality of the\nTLM, which affects the speech recognition performance. For\nLMs, sentences that are commonly used in real life should\nbe chosen to conform to the habits of people using natural\nlanguage. There is no open database for the Tibetan language.\nIn this paper, we evaluated news data from the Internet to\nbuild an LM, and then divided it into a training set, a valida-\ntion set and a testing set in a 10:1:1 ratio [8], [17]. This small\nTibetan Training dataset (STD), is in-domain with the testing\nset. In a large Tibetan training dataset (LTD), a word is limited\nto the ﬁrst 2,472 characters based on the frequency of the\naudio data. In addition, out-of-vocabulary(OOV) symbols are\nused to render any character that is not in part of the selected\nvocabulary. The size of the corpus and the percentage of OOV\ncharacters are shown in Table 2.\nAs a minority language in China, the Tibetan language\nhas a limited scope of application. Coupled with the govern-\nment’s policy reasons, more data are biased towards the news\nbecause of the limited corpus and the scattered theme. In the\nexperiment we have two data sets, STD and LTD, but in this\n72900 VOLUME 7, 2019\nK. Kuntharrgyalet al.: Morphological Verb-Aware TLM\nTABLE 6. Our method on the STD data set and interpolation on the N-gram interpolation and LTD data sets.\nexperiment we focus on the application of STD data, and the\nLTD is the auxiliary. The STD data are biased towards news,\nwhile the LTD data topics are scattered, including: news,\npolitics, economics, culture, Buddhism and Gesar.\nOur speech corpus is derived from speakers who are col-\nlege students whose mother tongue is the Lhasa dialect.\nIt was collected from 23 Tibetan native speakers, includ-\ning 13 males and 10 females. All speech signals were\nsampled at 16 KHz with 16-bit quantization. For the pur-\npose of building a practical ASR system, the recording\nscripts consist of mainly declarative sentences covering wide\ntopics.\nThere are more than 38,700 sentences in the corpus; among\nthem, 36,090 sentences are used in the training set, and\n2,664 sentences are used in the testing set. There is no overlap\nbetween the training and testing sets in utterances and speak-\ners. Table 3 shows the basics of our audio corpus.\nB. COMPARISON RESULTS ON TEXT PREDICTION\nTable 4 shows the result of our latest method on the STD\ndataset. Based on an RNNLM, we use the radical-based\nTibetan radical uniform weight (_TRU) method as the base-\nline. Our method is approximately 15.5% less than the\nRNNLM and 11.6% less than the baseline _TRU method,\nand confusion is reduced compared to the RNNLM Tibetan\nradical different weight (_TRD) method and the Tibetan rad-\nical combination weight (_TRC) method by approximately\n6.8% and 5.8%, respectively, indicating the effectiveness of\nour method [18].Our method’s confusion is reduced by 8.3%\ncompared to the traditional n-gram method.\nThe RNNLMs and n-gram LMs have their modeling char-\nacteristics as two essentially different LMs. RNNLMs typi-\ncally use a ﬁxed weight of linear interpolation in conjunction\nwith the n-gram LM. Table 5 shows the result of our method\nand n-gram interpolation on the STD dataset, referring to the\nvalue of λin [17], [22], [23] ( λ=0.5).\nIn Table 5, we can see that the proposed method achieves\nbetter results than the traditional n-gram method. In addition,\nwhen our method is combined with the n-gram method, some\nimprovements are achieved. This proves that our method\nand the n-gram method have complementary contributions\nto solve the problem of rate words, which further proves the\neffectiveness of our improved method in solving the sparse-\nness problem of Tibetan data.\nTable 6 is the result of our interpolation of STD data and\nLTD data. KN3(S) refers to the application of the n-gram\nmodel to small data training, while KN3(L) refers to the\nTABLE 7. The latest method and the %CER of our method.\nmodel utilized for larger data training. For STD data, our\nmethod and n-gram are reduced by 7.4% and 9.3%, respec-\ntively, compared to the RNNLM method and n-gram. Our\nmethod and n-gram are 6.1% less than the _TRU method,\nand n-gram confusion is lower by approximately 7.9%. The\nn-gram model trained on the LTD data is different from our\nmethod, which are 11.3% and 13.6% less than the RNNLM\nmethod and the n-gram method, respectively, which are better\nthan the _TRU and the n-gram methods. Confusion is reduced\nby 10% and 12.3%.\nC. COMPARISON RESULTS ON SPEECH RECOGNITION\nIt can be seen that the proposed method of _CFR and\nN-gram(S) difference results in better results than the differ-\nences of other methods. In particular, our proposed difference\nmethod between _TDW and N-gram(L) not only achieves\nbetter results than the existing method, but also increases the\nrelative _CFR method by approximately 4.5% in PPL. This\nresult also shows that our method has a good effect on the\nimprovement of the Tibetan language model.\nThe above experiments are based on PPL as the evaluation\ncriterion to verify the results. We can see that our method,\nrather than the latest TLM research, achieved better results.\nTo verify the validity of our proposed method, we used the\nexperimental results of different granularity and our proposed\nmethod for application to ASR for veriﬁcation. The results\nshow that the RNNLM method is better than the traditional\nn-gram method regarding character granularity.\nThe _TRC method for Tibetan in radical granularity\nachieved the best results. The results in Table 7 show a\n_CFR-based RNNLM relative improvement of %CER from\n3.1%. Using the _TDW-based RNNLM relative improve-\nment, the %CER improves from 4.3%. The method we\nVOLUME 7, 2019 72901\nK. Kuntharrgyalet al.: Morphological Verb-Aware TLM\nTABLE 8. Evaluation result of the %CER with N-best rescoring.\nTABLE 9. Subjective evaluation after a comparison between the baseline\nmodel and our model.\npropose has a good effect on Tibetan speech recognition\ncompared with the latest method.\nWe know that a lattice is a structure decoded once in the\nspeech recognition process that contains a large number of\ncandidate results. Since the neural network uses historical\ninformation to predict the next word, rerating the lattice\nwill result in slow search speeds. Compared to the word\nstructure of the lattice, N-best is more suitable for the model\nextension of long-distance information. This paper uses the\nintermediate result of N-best for rescoring [24], as shown\nin Table 8.\nWe validated our model on the ASR experiment in\nthe Tibetan Lhasa dialect audio dataset [20]. Table 8 is\nthe result of our veriﬁcation of %CER in Tibetan. Our\nmethod is reduced by approximately 3.5% in the N-best\n(n=100 and n =1,000) rescores, indicating that our method\nhas a good effect on the weighting method of rare words in the\nTLM.\nD. DISCUSSION\nThe above experimental results show that the proposed\nRNNLM_character frequency reweighting (_CFR) and the\nRNNLM_tuning discriminative weights (_CFR) are effec-\ntive. Some interesting observations are made as follows.\nAll the research on the LM rate word is not the same; some\nscholars have developed weighted methods based on rate\nwords, and some scholars have proposed adaptive methods.\nThe question with these studies is whether all of these rate\nwords make sense. Therefore, according to the characteristics\nof Tibetan morphological verbs, we propose a morphological\nverb-aware TLM. The use of morphological verb weighting\ncan not only affect the change in type but also improve\nthe predictive capacity. Therefore, increasing the weight of\nmorphological verbs can learn more language features. The\nlanguage features are helpful for improving the performance\nof the TLM.\nIn Tibetan, the morphological verb has a greater inﬂuence\non the tense of the sentence, and there is no uniform standard\nin temporal changes. In Table 1, the morphological verbs in\nTibetan can be divided into morphological verbs and mor-\nphologically invariant verbs. The former is the focus of our\nresearch, because such verbs play a major role in the change\nof the tense in the sentence, while the latter is the same as the\ncharacters we generally encounter. However, these characters\nappear less frequently in the training corpus, which affects\nthe predictive power of the sentence, especially in speech\nrecognition tasks. Therefore, we need to conduct research and\nanalysis on such characters to ﬁnd a more accurate way to\nsolve such problems.\nTo validate our approach, we experimented on ASR.\nWe tested 100 sentences in the set baseline output semantics\nthat are inaccurate, output them in the way we proposed,\nand then compared them with the original sentences. The\nmethod of comparison was to use subjective evaluation, and\nwe looked for 5 experts who worked in the Tibetan language\nto score. Table 9 shows the results of the comparison between\nthe model and the baseline model; the criterion of the score\nwas the original sentence as the standard, and the semantics\nof the 100 sentences were the most similar to the original sen-\ntence. As shown in Table 9, our proposed method was better\nthan the baseline method in semantic understanding, and in\nthe 100 sentences output in the baseline model, our model\ncould accurately represent 47% of the sentence semantics.\nIn summary, the weighted method based on morphologi-\ncal verbs inﬂuences the semantics of sentences to a certain\nextent and achieves good results. Therefore, it is necessary to\nstrengthen morphological verbs in Tibetan.\nVI. CONCLUSION AND FERYURE WORK\nIn this paper, we have shown that Tibetan morphological\nverbs are rare words that are very important for learning\nan effective TLM, and we have proposed a morphological\nverb-aware TLM. We ﬁrst proposed an ofﬂine learning TLM\nby character frequency reweighting to enhance the weights\nof morphological verbs. Furthermore, we proposed online\ntuning of the discriminative weights of morphological verbs\nto make the ofﬂine learned TLM online adaptive. As a result,\nour method outperforms baseline models on tasks of text\nprediction and ASR.\nACKNOWLEDGMENT\nWe would also like to thank Qing Guo for his contribution for\nthis research.\n72902 VOLUME 7, 2019\nK. Kuntharrgyalet al.: Morphological Verb-Aware TLM\nREFERENCES\n[1] P. F. Brown, J. Cocke, S. A. D. Pietra, V . J. D. Pietra, F. Jelinek,\nJ. D. Lafferty, and P. S. Roossin, ‘‘A statistical approach to machine trans-\nlation,’’Comput. Linguistics, vol. 16, no. 2, pp. 79–85, 1990.\n[2] C. Zhai and J. Lafferty, ‘‘A study of smoothing methods for language\nmodels applied to information retrieval,’’ ACM Trans. Inf. Syst., vol. 22,\nno. 2, pp. 179–214, Apr. 2004.\n[3] R. Kuhn and R. de Mori, ‘‘A cache-based natural language model for\nspeech recognition,’’ IEEE Trans. Pattern Anal. Mach. Intell., vol. 12,\nno. 6, pp. 570–583, Jun. 1990.\n[4] R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer, and Y . Wu, ‘‘Exploring\nthe limits of language modeling,’’ Feb. 2016, arXiv:1602.02410. [Online].\nAvailable: https://arxiv.org/abs/1602.02410\n[5] S. F. Chen and J. Goodman, ‘‘An empirical study of smoothing tech-\nniques for language modeling,’’ Comput. Speech Lang., vol. 13, no. 4,\npp. 359–394, Oct. 1999.\n[6] B. Roark, M. Saraclar, and M. Collins, ‘‘Discriminative n-gram language\nmodeling,’’Comput. Speech Lang., vol. 21, no. 2, pp. 373–392, 2007.\n[7] P. F. Brown, P. V . DeSouza, R. L. Mercer, V . J. D. Pietra, and\nJ. C. Lai, ‘‘Class-based n-Gram models of natural language,’’ J. Comput.\nLinguistics, vol. 18, no. 4, pp. 467–479, 1992.\n[8] Y . Kim, Y . Jernite, D. Sontag, and A. M. Rush, ‘‘Character-aware neural\nlanguage models,’’ in Proc. AAAI, 2016, pp. 2741–2749.\n[9] T. Mikolov, M. Karaﬁát, L. Burget, J. Černocký, and S. Khudanpur,\n‘‘Recurrent neural network based language model,’’ in Proc. 11th Annu.\nConf. Int. Speech Commun. Assoc., Chiba, Japan, 2010, pp. 1045–1048.\n[10] T. Mikolov, S. Kombrink, L. Burget, J. Černocký, and S. Khudanpur,\n‘‘Extensions of recurrent neural network language model,’’ in Proc.\nIEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), May 2011,\npp. 5528–5531.\n[11] M. Sundermeyer, H. Ney, and R. Schlüter, ‘‘From feedforward to recurrent\nLSTM neural networks for language modeling,’’ IEEE/ACM Trans. Audio,\nSpeech, Language Process., vol. 23, no. 3, pp. 517–529, Mar. 2015.\n[12] A. E.-D. Mousa, H.-K. J. Kuo, L. Mangu, and H. Soltau, ‘‘Morpheme-\nbased feature-rich language models using deep neural networks for LVCSR\nof Egyptian Arabic,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal\nProcess. (ICASSP)Vancouver, BC, Canada, May 2013, pp. 8435–8439.\n[13] Y . Shi, P. Wiggers, and C. M. Jonker, ‘‘Towards recurrent neural networks\nlanguage models with linguistic and contextual features,’’ in Proc. 13th\nAnnu. Conf. Int. Speech Commun. Assoc., 2012, pp. 1662–1665.\n[14] A. E.-D. Mousa, R. Schlüter, and H. Ney, ‘‘Investigations on the use of\nmorpheme level features in language models for arabic LVCSR,’’ in Proc.\nIEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), Mar. 2012,\npp. 5021–5024.\n[15] Y . Z. He, B. Hutchinson, P. Baumann, M. Ostendorf, E. Fosler-Lussier,\nand J. Pierrehumbert, ‘‘Subword-based modeling for handling OOV words\ninkeyword spotting,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal\nProcess. (ICASSP), Florence, Italy, May 2014, pp. 7864–7868.\n[16] T. He, X. Xiang, Y . Qian, and K. Yu, ‘‘Recurrent neural network language\nmodel with structured word embeddings for speech recognition,’’ in Proc.\nIEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), Apr. 2015,\npp. 5396–5400.\n[17] X. Chen, X. Liu, A. Ragni, Y . Wang, and M. J. F. Gales, ‘‘Future word\ncontexts in neural network language models,’’ in Proc. IEEE Autom.\nSpeech Recognit. Understand. Workshop (ASRU), Dec. 2017, pp. 97–103.\n[18] T. Shen, L. Wang, X. Chen, K. Khysru, and J. Dang, ‘‘Exploiting the tibetan\nradicals in recurrent neural network for low-resource language models,’’ in\nProc. Int. Conf. Neural Inf. Process., 2017, pp. 266–275.\n[19] H. Fang, M. Ostendorf, P. Baumann, and J. Pierrehumbert, ‘‘Exponential\nlanguage modeling using morphological features and multi-task learn-\ning,’’ IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 23, no. 12,\npp. 2410–2421, Dec. 2015.\n[20] T. Shen, L. Wang, X. Chen, K. Khuru, and J. W. Dang, ‘‘Tibetan language\nmodel based on recurrent neural network,’’ Int. Seminar Speech Prod.,\n2017.\n[21] T. Shen, L. Wang, X. Chen, K. Khuru, and J. Dang, ‘‘Investigation of long\nshort-term memory for tibetan language model,’’ in Proc. Nat. Conf. Man-\nMach. Speech Commun., 2017.\n[22] X. Chen, X. Wang, X. Liu, M. J. F. Gales, and P. C. Woodland, ‘‘Efﬁcient\nGPU-based training of recurrent neural network language models using\nspliced sentence bunch,’’ in Proc. 15th Annu. Conf. Int. Speech Commun.\nAssoc. (INTERSPEECH), 2014, pp. 641–645.\n[23] X. Chen, X. Liu, Y . Qian, M. J. F. Gales, and P. C. Woodland, ‘‘CUED-\nRNNLM—An open-source toolkit for efﬁcient training and evaluation\nof recurrent neural network language models,’’ in Proc. IEEE Int. Conf.\nAcoust., Speech Signal Process. (ICASSP), Shanghai, China, Mar. 2016,\npp. 6000–6004.\n[24] X. Liu, X. Chen, Y . Wang, M. J. Gales, and P. C. Woodland, ‘‘Two\nefﬁcient lattice rescoring methods using recurrent neural network language\nmodels,’’IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 24, no. 8,\npp. 1438–1449, Aug. 2016.\n[25] Luosang Tsechum Gyumsto Seduo, Tibetan Grammatical Theories by\nSeduo. 1st ed., Nationalities Publishing house, Feb. 1957.\n[26] X. Tsedan, Detailed Explanation About Tibetan Grammar, 1st ed. Qinghai\nSheng, China: Qinghai Nationalities Publishing House, 1954.\n[27] Lcagsuntharrgyal, A Study of Tibetan Grammar, Qinghai Nationalities\nPublishing House, Qinghai Sheng, China, 2008.\n[28] R. Masumura, T. Asami, T. Oba, H. Masataki, S. Sakauchi, and A. Ito,\n‘‘Domain adaptation based on mixture of latent words language models\nfor automatic speech recognition,’’ IEICE Trans. Inf. Syst., vol. 101, no. 6,\npp. 1581–1590, 2018.\n[29] R. Masumura, T. Asami, T. Oba, H. Masataki, and S. Sakauchi, ‘‘Viterbi\napproximation of latent words language models for automatic speech\nrecognition,’’J. Inf. Process., vol. 27, pp. 168–176, 2019.\n[30] J. Li, H. Wang, L. Wang, J. Dang, K. Khuru, and G. Lobsang, ‘‘Explor-\ning tonal information for Lhasa dialect acoustic modeling,’’ in Proc.\n10th Int. Symp. Chin. Spoken Lang. Process. (ISCSLP) , Oct. 2016,\npp. 1–5.\n[31] A. Lazaridou, M. Marelli, R. Zamparelli, and M. Baroni, ‘‘Compositional-\nly derived representations of morphologically complex words in distribu-\ntional semantics,’’ inProc. 51st Annu. Meeting Assoc. Comput. Linguistics,\nvol. 1, 2013, pp. 1517–1526.\n[32] T. Luong, R. Socher, and C. Manning, ‘‘Better word representations with\nrecursive neural networks for morphology,’’ in Proc. 17th Conf. Comput.\nNatural Lang. Learn., 2013, pp. 104–113.\n[33] E. Yildiz, C. Tirkaz, H. B. Sahin, M. T. Eren, and O. O. Sonmez,\n‘‘A morphology-aware network for morphological disambiguation,’’ in\nProc. 13th AAAI Conf. Artif. Intell., 2016, pp. 1–7.\n[34] J. R. Bellegarda, ‘‘Statistical language model adaptation: Review\nand perspectives,’’ Speech Commun. , vol. 42, pp. 93–108,\nJan. 2004.\n[35] P. Koehn and J. Schroeder, ‘‘Experiments in domain adaptation for statisti-\ncal machine translation,’’ in Proc. 2nd Workshop Stat. Mach. Transl., 2007,\npp. 224–227.\n[36] S. Katz, ‘‘Estimation of probabilities from sparse data for the language\nmodel component of a speech recognizer,’’ IEEE Trans. Audio, Speech\nLang. Process., vol. ASP-35, no. 3, pp. 400–401, Mar. 1987.\n[37] R. M. Iyer and M. Ostendorf, ‘‘Modeling long distance dependence in\nlanguage: Topic mixtures versus dynamic cache models,’’ IEEE Trans.\nSpeech Audio Process., vol. 7, no. 1, pp. 30–39, Jan. 1999.\n[38] R. Iyer, M. Ostendorf, and H. Gish, ‘‘Using out-of-domain data to improve\nin-domain language models,’’ IEEE Signal Process. Lett., vol. 4, no. 8,\npp. 221–223, Aug. 1997.\n[39] T. R. Niesler and P. C. Woodland, ‘‘Combination of word-based and\ncategory-based language models,’’ in Proc. ICSLP, vol. 1, Oct. 1996,\npp. 220–223.\n[40] K. Deschacht, J. De Belder, and M.-F. Moens, ‘‘The latent words lan-\nguage model,’’ Comput. Speech Lang., vol. 26, no. 5, pp. 384–409,\n2012.\n[41] R. Masumura, H. Masataki, T. Oba, O. Yoshioka, and S. Takahashi, ‘‘Use\nof latent words language models in ASR: A sampling-based implementa-\ntion,’’ in Proc. ICASSP, May 2013, pp. 8445–8449.\n[42] R. Masumura, T. Oba, H. Masataki, O. Yoshioka, and S. Takahashi,\n‘‘Viterbi decoding for latent words language models using Gibbs sam-\npling,’’ in Proc. INTERSPEECH, 2013, pp. 3429–3433.\n[43] R. Masumura, T. Adami, T. Oba, H. Masataki, S. Sakauchi, and\nS. Takahashi, ‘‘N-gram approximation of latent words language models\nfor domain robust automatic speech recognition,’’ IEICE Trans. Inf. Syst.,\nvol. E99-D, no. 10, pp. 2462–2470, 2016.\n[44] S. Goldwater and T. Grifﬁths, ‘‘A fully Bayesian approach to unsupervised\npart-of-speech tagging,’’ in Proc. ACL, 2007, pp. 744–751.\n[45] P. Blunsom and T. Cohn, ‘‘A hierarchical Pitman-Yor process HMM\nfor unsupervised part of speech induction,’’ in Proc. ACL, 1996,\npp. 865–874.\n[46] R. C. Moore and W. Lewis, ‘‘Intelligent selection of language model\ntraining data,’’ in Proc. ACL, 2010, pp. 220–224.\nVOLUME 7, 2019 72903\nK. Kuntharrgyalet al.: Morphological Verb-Aware TLM\nKUNTHARRGYAL KHYSRU received the bach-\nelor’s degree in art design and the master’s\ndegree in Tibetan information processing from\nQinghai Nationalities University, Qinghai, China,\nin 2005 and 2012, respectively. His research inter-\nests include Tibetan signal processing and Tibetan\nnatural language processing.\nDI JINreceived the B.S., M.S., and Ph.D. degrees\nfrom Jilin University, Changchun, China, in 2005,\n2008, and 2012, respectively, all in computer sci-\nence. He was a Postdoctoral Research Fellow with\nthe School of Design, Engineering, and Comput-\ning, Bournemouth University, Poole, U.K., from\n2013 to 2014. He is currently an Associate Pro-\nfessor with the College of Intelligence and Com-\nputing, Tianjin University, Tianjin, China. He has\npublished over 40 international journal articles\nand conference papers. His current research interests include data mining,\nmachine learning, and natural language processing.\nJIANWU DANG received the B.E. and M.E.\ndegrees from Tsinghua University, China,\nin 1982 and 1984, respectively, and the Ph.D.\ndegree from Shizuoka University, Japan, in 1992.\nHe was with Tianjin University, Tianjin, China,\nas a Lecturer, from 1984 to 1988. From 1992 to\n2001, he was with ATR Human Information Pro-\ncessing Labs., Japan. Since 2001, he has been on\nthe Faculty of the School of Information Science,\nJapan Advanced Institute of Science and Technol-\nogy (JAIST), as a Professor. He joined the Institute of Communication Parlee\n(ICP), Center of National Research Scientiﬁc, France, as a Research Scien-\ntist, from 2002 to 2003. Since 2009, he has been with Tianjin University.\n72904 VOLUME 7, 2019",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7627689838409424
    },
    {
      "name": "Natural language processing",
      "score": 0.5619097948074341
    },
    {
      "name": "Verb",
      "score": 0.5282415747642517
    },
    {
      "name": "Linguistics",
      "score": 0.49630337953567505
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45824453234672546
    },
    {
      "name": "Philosophy",
      "score": 0.0682046115398407
    }
  ]
}