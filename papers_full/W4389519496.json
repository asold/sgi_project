{
  "title": "Unlearn What You Want to Forget: Efficient Unlearning for LLMs",
  "url": "https://openalex.org/W4389519496",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2946229205",
      "name": "Jiaao Chen",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2127567756",
      "name": "Diyi Yang",
      "affiliations": [
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4377297670",
    "https://openalex.org/W2605717780",
    "https://openalex.org/W3206066344",
    "https://openalex.org/W4283768109",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W2989743967",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2473418344",
    "https://openalex.org/W4321471480",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4283805899",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4286897388",
    "https://openalex.org/W4301194718",
    "https://openalex.org/W4226038148",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4389523957",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4313680121",
    "https://openalex.org/W3177765786",
    "https://openalex.org/W4221159672",
    "https://openalex.org/W4295108597",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4385572399",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W4392669907",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W3135378441",
    "https://openalex.org/W3188505388",
    "https://openalex.org/W3130821513",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W4312091890",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W4385570321",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3157374291",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3035644192",
    "https://openalex.org/W3207429447",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W2109426455",
    "https://openalex.org/W3198599617"
  ],
  "abstract": "Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our proposed methods compared to the state-of-the-art baselines.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12041–12052\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nUnlearn What You Want to Forget: Efficient Unlearning for LLMs\nJiaao Chen\nGeorgia Institute of Technology\njiaaochen@gatech.edu\nDiyi Yang\nStanford University\ndiyiy@cs.stanford.edu\nAbstract\nLarge language models (LLMs) have achieved\nsignificant progress from pre-training on and\nmemorizing a wide range of textual data, how-\never, this process might suffer from privacy\nissues and violations of data protection regu-\nlations. As a result, the ability to easily re-\nmove data related to individual users from such\nmodels while not deteriorating their predictive\nquality after the removal becomes increasingly\nimportant. To address these issues, in this work,\nwe propose an efficient unlearning framework\nthat could efficiently update LLMs without hav-\ning to retrain the whole model after data re-\nmovals, by introducing lightweight unlearning\nlayers learned with a selective teacher-student\nobjective into the transformers. In addition,\nwe introduce a fusion mechanism to effectively\ncombine different unlearning layers that learns\nto forget different sets of data to handle a se-\nquence of forgetting operations. Experiments\non classification and generation tasks demon-\nstrate the effectiveness of our proposed meth-\nods compared to the state-of-the-art baselines1.\n1 Introduction\nUtilizing Large Language Models (LLMs) has be-\ncome the dominant paradigm for various NLP\napplications (Brown et al., 2020; Chowdhery\net al., 2022a; Kojima et al., 2022; Ouyang et al.,\n2022; Brown et al., 2020; Radford et al., 2019;\nLewkowycz et al., 2022; Qin et al., 2023; Touvron\net al., 2023) as LLMs memorize a vast amount of\nknowledge during pre-training or fine-tuning on a\nwide range of textual data (Brown et al., 2020; Rad-\nford et al., 2019; Hoffmann et al., 2022; Webson\nand Pavlick, 2022; Min et al., 2022; Liang et al.,\n2022; Carlini et al., 2022). However, these data\ncould contain sensitive information such as names,\nphone numbers, email addresses, and private clini-\ncal notes (Jang et al., 2022; Kurmanji et al., 2023;\n1The codes are avaiable here: https://github.com/\nSALT-NLP/Efficient_Unlearning/\nKumar et al., 2022).Extensive studies showed that\nLLMs could generate private information such as\nthe Editor-in-Chief of MIT Technology Review in-\ncluding his family members, work address, and\nphone number (Carlini et al., 2022). Recently, the\nEU’s General Data Protection Regulation (GDPR)\nand US’s California Consumer Privacy Act (CCPA)\nhave also required the right to be forgotten, intro-\nducing new regulations that require applications\nto support the deletion of user-generated content\nwhen requested by users (Sekhari et al., 2021; Ku-\nmar et al., 2022). In light of this, it is essential to\nprovide LLMs with an efficient and effective way\nto unlearn the information requested by users.\nRecent attention has been paid to the handling of\nsuch unlearning requests for LLMs through retrain-\ning and data pre-processing like SISA (Bourtoule\net al., 2021; Kumar et al., 2022) where training data\nis stored in different isolated slices and each check-\npoint is saved after training on each slice. When\na deletion request is received, the respective data\npoint will be removed from the slice, and the model\ncheckpoint up to the data point will be used to fur-\nther retrain the model. The effect of unlearning is\noften reflected by the model errors on the deleted\ndata (models cannot predict the deleted data) (Kur-\nmanji et al., 2023; Jang et al., 2022). Other works\nhave also explored the design of algorithms that\nensure differential privacy (DP) (Yu et al., 2021;\nLi et al., 2021; Anil et al., 2021). However, ma-\nchine unlearning approaches like SISA (Bourtoule\net al., 2021) usually require a significantly large\namount of storage space (Bourtoule et al., 2021),\nand DP methods could result in a slow convergence\nand significant deterioration in model performance\n(Nguyen et al., 2022). In addition, both of them\nrequire retraining the whole model, which is ex-\ntremely expensive and time-consuming consider-\ning the model scales of the current LLMs. These\nlimitations also make them unable to dynamically\ndeal with a sequence of unlearning requests which\n12041\nFigure 1: Overall process of our EUL framework. The unlearning layers are plugged into transformer layers after\nthe feed-forward networks. During training, only the unlearning layers are learned to forget requested data while the\noriginal LLMs remain unchanged. For every deletion request, an unlearning layer is learned first and then merged\nwith other unlearning layers via our designed fusion mechanism to form the fused unlearning transformer which\nsatisfies a series of deletion requests.\nis often the need in real-world scenarios (Jang et al.,\n2022; Nguyen et al., 2022).\nTo fill in these gaps, in this work, we propose an\nEfficient Unlearning method forLLMs (EUL) to ef-\nficiently unlearn what needs to be forgotten without\ncompletely retraining the whole model while retain-\ning the performances of the models. Specifically,\nwe propose a lightweight approach to learning the\nunlearning layer that is plugged into transform-\ners through a selective teacher-student formulation\n(Kurmanji et al., 2023) within several updates, with-\nout tuning the large language models. Additionally,\nwe introduce a fusion mechanism to effectively\ncombine the weights of different unlearning layers\nthat learn to forget different sets of data to a single\nunified unlearning layer by minimizing a regression\nobjective. This allows EUL to efficiently address\na sequence of deletion operations. To demonstrate\nthe effectiveness of our proposed EUL, we per-\nform experiments on IMDB (Maas et al., 2011)\nand SAMSum (Gliwa et al., 2019) in different set-\ntings compared to the state-of-the-art unlearning or\nmodel editing baselines. To summarize, our main\ncontributions are threefold:\n• We introduce an efficient unlearning method\nto remove the effect of required data in\na lightweight way via a selective teacher-\nstudent formulation.\n• We design a fusion mechanism to merge un-\nlearning layers that are learned to forget differ-\nent sets of data into a single unlearning layer\nto deal with a sequence of removal operations.\n• We conduct experiments on classification and\ngeneration tasks with backbone models of dif-\nferent scales in different settings, to illustrate\nthe effectiveness of EUL.\n2 Related Work\n2.1 Large Language Models\nLarge language models have witnessed extensive\nprogress recently (Brown et al., 2020; Radford\net al., 2019; Smith et al., 2022; Rae et al., 2021;\nChowdhery et al., 2022b; Touvron et al., 2023),\nespecially in terms of scaling up LLMs such as\nLLAMA (Touvron et al., 2023), Megatron-turing\nNLG (Smith et al., 2022), Gopher (Rae et al., 2021),\nand PaLM Chowdhery et al. (2022b). Other works\nhave also achieved better performance with smaller\nmodels through longer training (Hoffmann et al.,\n2022), instruction tuning (Wang et al., 2022; Zhou\net al., 2023) and human feedback (Ouyang et al.,\n2022). However, recent studies have shown that\ntraining data, such as personally identifiable in-\nformation like names, phone numbers, email ad-\ndresses, and even bank account numbers (Carlini\net al., 2021; Lee et al., 2021; Carlini et al., 2022;\nJagielski et al., 2022), can be easily extracted from\nLLMs because LLMs memorize the training data\nin billions of parameters (Carlini et al., 2022). Our\nwork is proposed to alleviate such issues by allow-\ning efficient unlearning of the requested or private\ndata from the learned parameters in LLMs.\n12042\n2.2 Machine Unlearning for Privacy\nTo mitigate the privacy risks for LLMs, machine un-\nlearning methods have been introduced to remove\nthe contributions of training examples that users re-\nquest to be erased by users (Bourtoule et al., 2021;\nChien et al., 2023) including exact unlearning that\nretrains deep learning models on new datasets after\nremoval (Bourtoule et al., 2021) and approximate\nunlearning (Izzo et al., 2021; Golatkar et al., 2020;\nKurmanji et al., 2023; Jang et al., 2022) which\naims to modify the weights of trained models to\nproduce a new set of weights that approximate the\nweights from retraining. The effect of unlearning\nis often reflected by the model errors on the deleted\ndata (models cannot predict the deleted data) (Kur-\nmanji et al., 2023; Jang et al., 2022). Another line\nof work has focused on Differential Privacy (DP)\nwhich ensures that user information in training data\ncannot be inferred (Dwork, 2008; Yu et al., 2021;\nLi et al., 2021; Anil et al., 2021; Abadi et al., 2016).\nHowever, both types of methods require retraining\nthe whole model, which is extremely expensive\nand time-consuming, especially for large language\nmodels and even impacts the task performances\n(Anil et al., 2021). And thus they can not dynami-\ncally tackle sequences of deletion (Jang et al., 2022;\nNguyen et al., 2022). To overcome these limita-\ntions, we introduce an efficient unlearning method\nas well as a fusion mechanism to efficiently and\ndynamically unlearn sequence of user data.\nOur work is also related to model editing\n(Mitchell et al., 2021; Belinkov et al., 2017; Dai\net al., 2021; Wang et al., 2020) while they usually\nfocus on editing the model output based on several\ngiven linguistic structures or facts about the world\ninstead of forgetting the required data.\n3 Efficient Unlearning for LLMs\nThis section presents our designed Efficient\nUnlearning method for LLMs (EUL) which could\nefficiently and dynamically handle a sequence of\ndeletion requests. The overall process is shown in\nFigure 1. Formally, for a large language modelF(.)\nthat is trained on a dataset D= {(x,y)}where x\nis textual data and yis the corresponding label, and\na deletion request to forget Df = {(xf ,yf }, our\ngoal is to learn an updated modelF′(.) that satisfies\nthe following (Kurmanji et al., 2023):\nI(F(Df ); F′(Df )) = 0\nI(F(Dr); F′(Dr)) = 1 (1)\nwhere Dr = D−Df = {(xr,yr)}refers to the\ndata we would like to retain, and I(.) is the mutual\ninformation. Intuitively, we will update F(.) with\nF(.) to generate similar output for the data we want\nto retain while losing all information about making\npredictions on the data we want to forget.\n3.1 Learning to Forget via Unlearning Layers\nAs the scales of current LLMs and the size of train-\ning data are usually large, updating all the param-\neters in the model F(.) (e.g., re-training F(.) on\nDr\ni ) becomes extremely expensive. Inspired by\nrecent advances in parameter-efficient fine-tuning\n(Houlsby et al., 2019; Chien et al., 2023), we model\nF′(.) by F(f(.)) where f(.; W) is an adapter with\nsignificant smaller amount of parameters W com-\npared to F(.). And we would only update f(.) to\nfulfill the unlearning requests.\nTo effectively achieve the unlearning goals in\nequation 1, we minimize a selective teacher-student\nobjective where the student modelF′(.) =F(f(.))\nis learned to follow the teacher model F(.) on Dr\nwhile disobeyed F(.) on Df :\nLKL =α\n∑\nxr\nKL(F(xr)||F(f(xr)))\n−\n∑\nxf\nKL(F(xf )||F(f(xf )))\n(2)\nwhere αis a hyper-parameter to balance the trade-\noff between forgetting xf and retaining xr. Intu-\nitively, during training, f(.) is leaned to minimize\nthe KL-divergence between the output from the\nupdated model and the original model on the data\nto retain while maximizing the KL-divergence be-\ntween the output from them on the data to forget.\nTo maintain the task performance, we optimize\nf(.) for the task loss on the retain data:\nLTASK =\n∑\nxr\nl(F(f(xr)),yr) (3)\nwhere l(.) is the task-related loss, for example,\ncross-entropy loss, −log P(F(f(xr))), for clas-\nsification tasks.\nFurthermore, we also negate the original training\nobjectives used in LLMs (e.g., masked language\nmodeling objective (Raffel et al., 2020)) to forget\nthe knowledge related to the data, in order to for-\nget in pre-trained parameters and ensure that the\ninformation in the forgotten data cannot be easily\nextracted from F(.):\nLLM = −\n∑\nxf\nl(F(f(xf ))) (4)\n12043\nwhere l(.) is the language model loss used when\npre-training F(.), for example, masked language\nmodel loss, −log P(ˆx|x−ˆx) (ˆxare the randomly\nmasked tokens). In our experiments, we utilize T5\nmodels (Raffel et al., 2020). Thus we add an extra\n“Predict the masked word” at the beginning of the\ninput for this loss term.\nOur final training objective is then the following:\nLEUL = LKL + λLTASK + γLLM (5)\nwhere λand γ are hyper-parameters. In practice,\nfollowing Kurmanji et al. (2023), we alternate the\nupdates for the data to be forgotten and the data\nto be retained to optimize min-max terms in LEUL\nmore stably. Specifically, we iteratively perform an\nepoch of updates on the data to be retained and then\nan epoch of updates on the data to be forgotten.\n3.2 Fusing Unlearning Layers\nTo dynamically handle a sequence of unlearning\nrequests and derive a unified model that could for-\nget all of the requested data, we then introduce a\nfusion mechanism that could merge different un-\nlearning layers fi(.; Wi) which are learned to forget\nDf\ni = (Xf\ni ,Y f\ni ) in the previous section into a sin-\ngle fm(.; Wm). Namely, we would like the output\nof fm(.) on Df\ni being close to fi(.):\nmin\nWm\n∑\ni\n||WT\nmXf\ni −WT\ni Xf\ni ||2\n(6)\nwhich is a linear regression problem and has a\nclosed-form solution:\nWm = (\n∑\ni\nXf\ni\nT\nXf\ni )−1 ∑\ni\n(Xf\ni\nT\nXf\ni Wi) (7)\nSpecifically, to derive the weights Wm for the\nmerged unlearning layer fm, we would use the pre-\ncomputed inner product matrix of the hidden repre-\nsentations before the unlearning layers in LLMs of\nthe forgotten data Xf\ni\nT\nXf\ni and then compute Wm\nfollowing Equation 7.\nThe fusion mechanism ensures efficiency and\nprivacy as it could be performed without any extra\ntraining and only requires storing the inner prod-\nuct matrix of the representations of the data to be\nforgotten instead of the data itself.\n4 Experiments\n4.1 Datasets\nWe conduct experiments on both classification and\ngeneration tasks. For the classification task, we\nDataset Task Train Dev Test\nIMDB Classification 20000 2000 25000\nSAMSum Summarization 14732 818 819\nTable 1: Dataset statistics for IMDB and SUMSum.\nutilize the IMDB dataset(Maas et al., 2011), which\nis a sentiment classification dataset consisting of\nusers’ reviews of movies, directors, actors, etc. For\nthe generation task, we use SAMSum (Gliwa et al.,\n2019), which is a recent popular conversation sum-\nmarization dataset consisting of conversations be-\ntween different speakers. The dataset statistics are\nshown in Table 1.\nWe choose these two datasets because they are\nwidely used (Wang et al., 2021; Yang et al., 2019;\nQin et al., 2023; Ji et al., 2023; Wei et al., 2021;\nSanh et al., 2021; Chen et al., 2022) to evaluate\nlarge language models and both datasets are related\nto cases where the user might require to remove\ntheir data, for example, removing all the reviews of\na specific movie or removing all the conversations\nfrom one specific speaker.\nIn experiments, we use the pre-trained NER\nmodels from AllenNLP2 to extract all the entities\n(names) in IMDB and directly use the speakers’\nnames in SAMSum and simulate the unlearning re-\nquests to remove all the data from or related to cer-\ntain names. Moreover, we substitute all the names\nin the dev and test set with special tokens.\n4.2 Evaluation Metrics\nTo evaluate the performances, following Kurmanji\net al. (2023), we measure several metrics: (1) Per-\nformance on the test set: The task-related perfor-\nmance on the test set, namely, accuracy for IMDB\nand ROUGE for SAMSum. This measures whether\nthe unlearning algorithms affect the model perfor-\nmance or not. (2) Performance on the retained\nset: The task-related performance on the data to\nbe retained. This measures whether the unlearn-\ning algorithms forget the data that need to be re-\ntained. Higher performance means that the model\nremembers the data that is not to be forgotten. (3)\nPerformance on the forgot set: The task-related\nperformance on the data to be forgotten. This mea-\nsures whether the unlearning algorithms effectively\nforget the data requested to be forgotten. Lower\nperformance means that the model is better at for-\n2https://demo.allennlp.org/\n12044\nMethods # Forgot Data Test Set ↑ Retained Set ↑ Forgot Set ↓ MLM Loss ↑ Time (s) ↓\nT5-base\nOriginal - 93.2 100 100 1.46 -\nRe-train\n0.5%\n92.8 100 92.5 1.52 6685\nFine-tune 93.0 100 96.5 1.47 4200\nSISA 92.4 98.2 91.5 1.54 1580\nReverse-Gradient 92.0 97.3 68.6 1.56 4400\nMEND 92.2 98.5 73.5 1.60 34\nEUL† 93.0 100 65.7 1.78 1200\nRe-train\n1%\n92.7 100 91.6 1.55 6610\nFine-tune 92.8 100 96.2 1.48 3950\nSISA 92.2 98.1 90.4 1.55 2930\nReverse-Gradient 91.5 96.4 67.4 1.59 4166\nMEND 91.3 95.5 74.6 1.62 62\nEUL† 93.0 100 64.4 1.84 1526\nRe-train\n10%\n92.1 100 90.2 1.56 6026\nFine-tune 92.0 100 95.8 1.52 3133\nSISA 91.6 98.2 88.4 1.55 2010\nReverse-Gradient 91.0 96.5 65.4 1.62 3228\nMEND 90.8 94.8 76.2 1.66 328\nEUL† 92.2 99.0 57.2 2.01 1828\nT5-3b\nOriginal - 97.0 100 100 1.28 -\nRe-train\n0.5%\n96.6 100 94.8 1.30 26855\nFine-tune 96.7 100 96.2 1.28 20465\nSISA 95.0 97.2 94.1 1.33 16503\nReverse-Gradient 93.3 96.5 78.9 1.42 21826\nMEND 93.0 95.8 89.5 1.30 4980\nEUL† 96.5 100 70.2 1.66 9240\nRe-train\n1%\n96.3 100 94.2 1.30 25280\nFine-tune 96.5 100 96.0 1.28 18466\nSISA 93.8 96.8 92.7 1.35 15680\nReverse-Gradient 92.5 96.0 80.1 1.46 18800\nMEND 92.8 95.0 84.4 1.48 6600\nEUL† 96.5 100 67.5 1.72 9840\nRe-train\n10%\n96.0 100 93.5 1.31 22140\nFine-tune 96.2 100 94.0 1.30 16752\nSISA 93.0 95.5 92.2 1.35 14180\nReverse-Gradient 91.9 95.2 68.4 1.46 17850\nMEND 92.0 94.2 78.5 1.50 12072\nEUL† 96.0 100 60.8 1.92 10460\nTable 2: Performances on IMDB for T5-base and T5-3B after unlearnling different number of privacy-related data.\n†refers to our model. All the results are averaged over 5 random runs.\ngetting the data. (4) MLM Loss : The masked\nlanguage model loses on the data to be forgotten\nwhere related entities or actions are masked. This\nis achieved by adding “Predict the masked word”\nin the beginning. This measure whether the infor-\nmation in the data that needs to be forgotten can\nbe extracted from the LLMs. Higher MLM loss\nmeans that it is harder to extract such information\nfrom the models. (5) Updating time: The time to\nupdate the original model in the forgetting process.\n4.3 Baselines\nWe compare our EUL with several baseline meth-\nods: Re-train (Kumar et al., 2022): Re-training the\nmodel from scratch on the data to be retained with-\nout any forgotten data. Fine-tune (Kurmanji et al.,\n2023): Fine-tuning the original model on the data\nto be retained without any forgotten data. SISA\n(Kumar et al., 2022): Sharded, Isolated, Sliced,\nand Aggregated training where multiple models\nare trained independently on disjoined shards, and\nits slices and model checkpoints are saved for each\n12045\nMethods # Forgot Data Test Set ↑ Retained Set ↑ Forgot Set ↓ MLM Loss ↑ Time (s) ↓\nT5-base\nOriginal - 47.2/23.5/39.6 71.4/42.6/62.7 70.2/42.2/62.7 1.37 -\nRe-train\n0.5%\n46.8/23.0/38.1 71.7/42.8/62.4 42.4/23.2/42.0 1.40 28000\nFine-tune 46.6/23.2/38.1 72.5/44.7/65.2 58.8/34.1/54.1 1.38 27120\nSISA 44.2/22.0/37.4 70.5/41.6/60.5 41.4/23.0/40.8 1.48 22582\nReverse-Gradient 43.2/20.9/35.8 68.8/40.2/58.5 42.3/21.4/38.1 1.64 28800\nEUL† 46.8/23.0/38.5 71.5/42.4/63.3 38.4/20.2/37.2 1.88 17060\nRe-train\n1%\n45.4/22.8/37.5 72.4/43.0/62.8 42.2/22.8/41.6 1.44 26855\nFine-tune 46.4/23.2/38.1 72.9/43.6/64.0 56.4/31.8/52.7 1.40 27210\nSISA 43.1/21.1/36.8 69.8/40.2/60.0 41.4/23.0/40.8 1.50 22420\nReverse-Gradient 42.0/20.0/34.6 68.8/40.2/58.5 42.3/21.4/38.1 1.64 27700\nEUL† 46.5/22.8/38.0 71.5/42.4/63.3 35.8/19.0/36.2 1.95 16820\nRe-train\n10%\n44.2/21.2/35.8 70.4/41.2/60.5 41.4/21.4/40.0 1.48 26155\nFine-tune 45.2/22.1/36.6 71.1/42.6/62.9 51.5/28.6/50.0 1.43 27510\nSISA 41.8/19.6/33.8 68.3/38.8/58.8 40.2/20.1/38.9 1.55 20790\nReverse-Gradient 40.8/18.4/33.0 66.6/38.3/55.5 38.0/19.4/36.6 1.71 27240\nEUL† 45.8/22.4/37.8 70.9/42.0/62.3 33.0/18.3/33.0 2.23 15000\nT5-3b\nOriginal - 53.6/29.6/45.1 78.5/47.6/66.1 74.2/43.5/64.9 1.30 -\nRe-train\n0.5%\n52.8/28.8/44.0 77.4/46.1/65.4 50.4/27.2/43.0 1.34 84480\nFine-tune 53.3/29.0/44.4 78.0/47.1/65.8 60.2/36.1/55.7 1.30 83600\nSISA 51.7/27.2/40.8 74.8/44.8/63.5 49.4/26.8/42.2 1.33 75000\nReverse-Gradient 50.6/25.9/39.9 72.8/42.0/62.8 44.3/23.1/39.0 1.44 83200\nEUL† 53.6/29.4/44.8 77.5/46.3/66.6 41.0/21.8/38.2 1.67 60430\nRe-train\n1%\n52.0/28.2/42.8 76.7/45.8/64.8 49.6/26.6/42.1 1.35 82440\nFine-tune 52.5/28.5/43.6 76.2/45.5/64.2 56.8/32.2/52.4 1.32 81135\nSISA 50.0/26.1/38.9 72.3/43.1/61.1 49.0/25.8/41.1 1.38 73550\nReverse-Gradient 48.6/24.3/37.2 70.6/41.5/60.9 42.2/22.0/37.7 1.45 82485\nEUL† 53.3/29.0/44.4 76.4/45.3/64.3 38.4/19.9/36.0 1.74 60880\nRe-train\n10%\n50.8/26.4/40.5 74.2/45.0/63.2 48.2/25.5/41.4 1.38 81010\nFine-tune 51.4/27.2/41.9 75.2/45.3/64.0 52.1/29.8/49.9 1.35 81800\nSISA 48.2/24.5/36.0 70.4/40.5/59.6 41.2/23.5/40.0 1.40 70400\nReverse-Gradient 44.7/22.0/34.2 68.5/40.9/58.8 40.9/21.0/36.5 1.49 82070\nEUL† 52.0/28.4/42.6 74.9/45.0/63.6 36.2/18.6/34.7 1.78 59900\nTable 3: Performances on SAMSum for T5-base and T5-3B after unlearnling different number of privacy-related\ndata. †refers to our model. All the results are averaged over 3 random runs. The performance on Test, Retained and\nForgot Set are ROUGE-1/2/L scores.\nslice. When forgetting certain data, the correspond-\ning data point is deleted from its slice, and the\nmodel checkpoint up to the data point is used to\nfurther retrain the model. Reverse-Gradient (Liu\net al., 2022): Fine-tuning the original model on\nboth retained data and forgot data while negating\nthe gradient for the forgot data. MEND (Mitchell\net al., 2021): Editing the model to generate out-\nput following the given examples. To adapt the\nmodel in the unlearning setting, we reverse the\nlabels for data in classification tasks as input to\nMEND. However, it is infeasible to apply MEND\nto summarization tasks as it is hard to design the\nnew output to perform the editing.\n4.4 Model Settings\nFor all the experiments, we use T5 models (T5-base\nand T5-3b) (Raffel et al., 2020) as the backbone\nmodels. For SISA, we follow Kumar et al. (2022)\nto split the dataset. For our unlearning layers, we\nonly tune 0.5% (Chen et al., 2023) of the param-\neters. The α = 0.8, λ = 1.0 and γ = 0.2 are se-\nlected from grid searching {0.1,0.2,0.5,0.8,1.0}.\nWe set the linear decay scheduler with a warmup\nratio of 0.06 for training. The maximum sequence\nlength is 128 for IMDB and 800 for SAMSum. The\nbatch size was 256 for base models and 128 for 3b\nmodels on IMDB and 8 for base models and 2 for\n3b models on SAMSum. The maximum learning\n12046\nMethods Test Set ↑ Retained Set ↑ Forgot Set ↓ Updating Time (s) ↓\nOriginal 91.8 100 91.2 -\nRe-train 92.5 100 12.6 6026\nFine-tune 92.3 100 26.8 3133\nSISA 92.2 98.2 12.6 1510\nReverse-Gradient 92.8 98.6 9.0 3228\nMEND 92.2 97.8 16.8 328\nEUL† 93.0 99.0 5.0 1828\nTable 4: Performances on IMDB for T5-base after unlearnling 10% wrong-labeled data. †refers to our model. All\nthe results are averaged over 5 random runs.\nFigure 2: Sequentially unlearnling 1,2,3,4,5 different sets of data for T5-base on IMDB. The results are accuracy on\nthe test set and the accuracy on the forgot set averaging across different orderings. Every single set contains 1% of\nthe training data.\nMetric EUL -KL -TASK -LM\nTest Set ↑ 93.0 91.4 91.0 92.4\nRetained Set ↑ 100 100 97.4 99.0\nForgot Set ↓ 65.7 90.8 67.4 69.0\nMLM Loss ↑ 1.78 1.75 1.78 1.50\nTable 5: Performances on IMDB for T5-base after re-\nmoving 0.5% privacy-related data. We remove one ob-\njective at a time from our EUL methods.\nrate was 5e−5 and the maximum number of train-\ning epochs was set to be3 or 5. All the experiments\nwere performed using 8 A100 GPUs.\n4.5 Results\nUnlearning Privacy-related Data on IMDB We\nrequest the T5-base and T5-3b models that are fine-\ntuned on the IMDB dataset to unlearn 0.5%, 1%\nand 10% of the training data. The data to be for-\ngotten is randomly selected based on the names\nof movies, actors, actresses, directors, etc. For ex-\nample, the model might need to forget all the data\npoints related to “Lena Numan”. This simulates the\nModels Set 2 Set 2, 1 Set 2, 1,3\nRe-train 92.7/91.4 92.5/90.8 91.3/90.0\nFine-tune 92.8/96.0 92.1/94.0 91.0/93.3\nSISA 92.2/90.4 92.0/87.8 91.2/85.8\nReverse-Gradient 91.5/67.9 90.5/67.2 89.8/66.0\nEUL 93.0/64.6 92.1/64.8 91.0/64.2\nEUL-fuse 93.0/64.6 92.8/62.2 92.4/60.8\nTable 6: Accuracy on the test/retained set of after un-\nlearning sets of data following a sequence (set 2 -> set 1\n-> set 3).\ncases where people/companies request to remove\nall the data related to them. The performances are\ndisplayed in Table 2.\nAfter unlearning the requested data from T5-\nbase models, the re-training method hurts the accu-\nracy (e.g., a 1.1 accuracy drop when forgetting 10%\ndata) on the test set because there is fewer data for\ntraining, and the accuracy on the retained set keeps\nunchanged (100%) probably because the model\nmemorizes the retained data. The accuracy on the\nforgot set drops after re-training (e.g., 92.5 com-\n12047\npared to 100 when unlearning 0.5% of the data),\nshowing that the model is forgetting the requested\ndata, and the masked language model loss increases\n(e.g., increasing 0.06 when unlearning 0.5% of the\ndata), indicating that it is harder to extract the in-\nformation of the forgot data after re-training. The\nfine-tuning method shows better test accuracy with\nless updating time, however, it is worse in terms of\nforgetting the data. Even though SISA takes sig-\nnificantly less time (only costing around 1/3 of the\ntime compared to re-training) to derive the updated\nmodel that forgets the requested data, it receives\nlower accuracy on the test and retained set, which\nmeans that the model prediction abilities get worse\nbecause of failing to remember the retained data.\nWhen reversing the gradients for the data to be\nforgotten, the updated model gets better at forget-\nting with lower test accuracy. The model editing\nmethod, MEND, shows better overall performance\non nearly all the metrics but it requires extra data\nto train a model editing module to edit the original\nmodel, making the method hard to be generalized\nto new models and settings. Our EUL approach\nboosts all the metrics with faster speed to update the\nmodel compared to previous unlearning baselines\nafter removing different numbers of privacy-related\ndata (e.g., achieving the lowest accuracy (65.6%)\non forgot set while keeping the best test accuracy\n(93.0%) and 100% retained accuracy with 1/6 of\nthe updating time compared to re-training when\nforgetting 0.5% of the data), suggesting that our\ndesigned unlearning layers that are learned with tai-\nlored objectives could efficiently update the LLMs\nto forget the required data and remain the abilities\nto perform the tasks. When the size of the back-\nbone model scales up to 3b, the improvements of\nour EUL are consistent, indicating that our methods\ncould still forget what the user requests even for\nlarger models that are better at memorizing data.\nUnlearning Privacy-related Data on SAMSum\nWe unlearn 0.5%, 1% and 10% training data from\nT5-base and T5-3B models that are fine-tuned on\nthe SAMSum dataset. The data to be forgotten\nis randomly selected based on the speaker names.\nFor example, the model might need to forget all\nthe conversations from “Jack”. This simulates the\ncases where people request to remove all the data\ngenerated by them. The performances are shown\nin Table 3. Similarly, our EUL method consis-\ntently achieves the best overall performances by\neffectively forgetting the requested data while re-\nmembering the retained data and keeping the test\nROUGE scores with significantly less amount of\ntraining time. This indicates that our objectives\ncould also be generalized to generation tasks.\nUnlearning Mislabeled Data on IMDB We also\ntest a setting where the data to be forgotten is those\nwith wrong labels. In experiments, we randomly\nchange the labels for 10% of the training data and\nthen request the model to unlearn their impact. This\nsimulates the cases where we improve the models\nthat are trained on noisy data by unlearning the\nmislabeled data (Kumar et al., 2022). We report\nthe performances with T5-base models in Table 4.\nWe observe that the accuracy of the test set of the\noriginal model is affected by the mislabeled data.\nAnd our EUL is the most effective approach to\nunlearn and remove the negative impact of those\nmislabeled data to achieve the best test accuracy.\nSequence of Removals We test baseline and our\nmethods in a setting where a sequence of unlearn\nrequests are received, i.e., the models need to forget\ndifferent sets of data sequentially. In experiments,\nwe sequentially unlearn 1,2,3,4,5 sets of data from\nT5-base model on IMDB dataset. For every unlearn\nlength, we test with all the possible sequences and\naverage the accuracy on the test set and the forgot\nset. For example, when the length of the forgetting\nrequests are 2 (set 1, set 2), we test on the sequence\n(set 1 -> set 2) and sequence (set 2-> set 1) and av-\nerage the final performances. We show the results\n(accuracy on the test/retained set) of one possible\nsequence whose length is 3 (set 2 -> set 1 -> set 3)\nin Table 6 as an example. Averaged performances\nover different sequence lengths are visualized in\nFigure 2. EUL means that we keep one unlearning\nlayer to sequentially unlearn different sets of data\nand EUL-fuse means that for every set of forgot\ndata we learn separate unlearning layers and then\nmerge them into a single unlearning layer via our\nproposed fusion mechanism. The results demon-\nstrate that our proposed fusion method that com-\nbines different unlearning layers could effectively\nhandle the sequence of deletion (achieving higher\naccuracy on the test set and lower accuracy on the\nforgot set.) especially when the sequence length\ngets longer compared to baseline models.\n4.6 Ablation Studies\nRemoval of Objectives We perform ablation\nstudies to show the effectiveness of each designed\nobjective in EUL by removing each of them when\n12048\nModels IMDB SAMSum\nOriginal 0.542 0.510\nRe-train 0.550 0.522\nFine-tune 0.568 0.525\nSISA 0.585 0.530\nReverse-Gradient 0.626 0.588\nEUL 0.566 0.530\nTable 7: Accuracy from a trained binary classifier to\npredict whether an input data belongs to the retained set\nor the forgot set.\nlearning the unlearning layers in Table 5. Com-\npared to EUL which utilizes all of the learning\nobjectives, removing each of them would result\nin a performance drop, which demonstrates every\ncomponent contributes to the final performance.\nSpecifically, removing LKL would increase the ac-\ncuracy of the forgot set, indicating that LKL is the\nmain factor to forget the requested data. Remov-\ning LTASK from EUL would drop the accuracy\non the test set, suggesting that LTASK is essen-\ntial to maintain task performance. Removing LLM\ndecreases the MLM Loss, showing that LLM is\nthe main objective to avoid the extraction of the\nrequested information.\nMember Inference Attack We further perform\nMember Inference Attack (MIA) (Kurmanji et al.,\n2023) on IMDB and SAMSum when unlearn 1%\nprivacy-related data for T5-base models. Specif-\nically, we test the accuracy of a binary classifier\nwhich is trained to predict whether the input data\nbelong to the forgotten set or the retained set based\non their representations after the final layer of the\nT5 model. An accuracy closer to 0.5 means that it\nis hard for the classifier to predict the groups of the\ninput data. The accuracies are shown in Table 7.\nWe found that the classifiers could not converge so\nwell on the training set and always had a low accu-\nracy on the test set both before and after unlearning\n(e.g., 0.542 before unlearning and 0.566 after our\nEUL unlearning on IMDB). These showed that the\nrandomly deleted data could not be easily inferred\nboth before and after our EUL unlearning.\n5 Conclusion\nIn this work, we propose EUL, an efficient unlearn-\ning method for LLMs that could efficiently and ef-\nfectively unlearn the user-requested data via learn-\ning unlearning layers through the selective teacher-\nstudent objective. We further introduce a fusion\nmechanism that could merge different unlearning\nlayers into one unified layer to dynamically un-\nlearn a sequence of data. Experiments on different\nsettings (different datasets, different model sizes,\ndifferent forget set sizes) demonstrated the effec-\ntiveness of our proposed EUL method compared to\nstate-of-the-art baselines.\n6 Limitations\nIn this work, we mainly perform experiments on\nT5-base/3b models with fine-tuned tasks. We en-\ncourage future work to explore how to update dif-\nferent backbone models with larger sizes such as\nLLAMA models or even close-sourced models like\nChatGPT to forget the requested data such asemn\nprivacy-related data, toxic data, or misinformation\nin the pre-training corpus. Also, we mainly fol-\nlow the previous work to measure the unlearning\nthrough performance on the test set, retained set,\nand forgot set, together with the MLM loss. Fu-\nture work might explore how to evaluate unlearning\nmethods more comprehensively, such as whether\nthe model could recall forgotten content or whether\nmethods would make forgotten data identifiable. In\naddition, we perform all the experiments in simu-\nlated settings. Future work might apply our meth-\nods to real-world applications to deal with actual\nuse cases or introduce new benchmarks for evaluat-\ning unlearning methods.\nAcknowledgment\nWe would like to thank all reviewers and the SALT\nLab for their valuable feedback. This work was\npartially sponsored by NSF grant IIS-2247357 and\nIIS-2308994.\nReferences\nMartin Abadi, Andy Chu, Ian Goodfellow, H Bren-\ndan McMahan, Ilya Mironov, Kunal Talwar, and\nLi Zhang. 2016. Deep learning with differential pri-\nvacy. In Proceedings of the 2016 ACM SIGSAC con-\nference on computer and communications security,\npages 308–318.\nRohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar,\nand Pasin Manurangsi. 2021. Large-scale differen-\ntially private bert. arXiv preprint arXiv:2108.01624.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Has-\nsan Sajjad, and James Glass. 2017. What do neural\n12049\nmachine translation models learn about morphology?\narXiv preprint arXiv:1704.03471.\nLucas Bourtoule, Varun Chandrasekaran, Christopher A\nChoquette-Choo, Hengrui Jia, Adelin Travers, Baiwu\nZhang, David Lie, and Nicolas Papernot. 2021. Ma-\nchine unlearning. In 2021 IEEE Symposium on Secu-\nrity and Privacy (SP), pages 141–159. IEEE.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\nKatherine Lee, Florian Tramer, and Chiyuan Zhang.\n2022. Quantifying memorization across neural lan-\nguage models. arXiv preprint arXiv:2202.07646.\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom B Brown, Dawn Song, Ul-\nfar Erlingsson, et al. 2021. Extracting training data\nfrom large language models. In USENIX Security\nSymposium, volume 6.\nJiaao Chen, Mohan Dodda, and Diyi Yang. 2022.\nHuman-in-the-loop abstractive dialogue summariza-\ntion.\nJiaao Chen, Aston Zhang, Xingjian Shi, Mu Li,\nAlex Smola, and Diyi Yang. 2023. Parameter-\nefficient fine-tuning design spaces. arXiv preprint\narXiv:2301.01821.\nEli Chien, Chao Pan, and Olgica Milenkovic. 2023. Ef-\nficient model updates for approximate unlearning of\ngraph-structured data. In The Eleventh International\nConference on Learning Representations.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022a. Palm: Scaling language\nmodeling with pathways.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Se-\nbastian Gehrmann, et al. 2022b. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2021. Knowledge neu-\nrons in pretrained transformers. arXiv preprint\narXiv:2104.08696.\nCynthia Dwork. 2008. Differential privacy: A survey\nof results. In Theory and Applications of Models of\nComputation: 5th International Conference, TAMC\n2008, Xi’an, China, April 25-29, 2008. Proceedings\n5, pages 1–19. Springer.\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Alek-\nsander Wawer. 2019. Samsum corpus: A human-\nannotated dialogue dataset for abstractive summa-\nrization. arXiv preprint arXiv:1911.12237.\nAditya Golatkar, Alessandro Achille, and Stefano\nSoatto. 2020. Eternal sunshine of the spotless net: Se-\nlective forgetting in deep networks. In Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 9304–9312.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In In-\nternational Conference on Machine Learning, pages\n2790–2799. PMLR.\nZachary Izzo, Mary Anne Smart, Kamalika Chaudhuri,\nand James Zou. 2021. Approximate data deletion\nfrom machine learning models. In International Con-\nference on Artificial Intelligence and Statistics, pages\n2008–2016. PMLR.\nMatthew Jagielski, Om Thakkar, Florian Tramer,\nDaphne Ippolito, Katherine Lee, Nicholas Carlini,\nEric Wallace, Shuang Song, Abhradeep Thakurta,\nNicolas Papernot, et al. 2022. Measuring forget-\nting of memorized training examples. arXiv preprint\narXiv:2207.00099.\n12050\nJoel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha,\nMoontae Lee, Lajanugen Logeswaran, and Minjoon\nSeo. 2022. Knowledge unlearning for mitigating\nprivacy risks in language models. arXiv preprint\narXiv:2210.01504.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):1–38.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In Thirty-sixth\nConference on Neural Information Processing Sys-\ntems (NeurIPS 2022).\nVinayshekhar Bannihatti Kumar, Rashmi Gangadhara-\niah, and Dan Roth. 2022. Privacy adhering machine\nun-learning in nlp. arXiv preprint arXiv:2212.09573.\nMeghdad Kurmanji, Peter Triantafillou, and Eleni Tri-\nantafillou. 2023. Towards unbounded machine un-\nlearning. arXiv preprint arXiv:2302.09880.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. 2021. Deduplicating training\ndata makes language models better. arXiv preprint\narXiv:2107.06499.\nAitor Lewkowycz, Anders Andreassen, David Dohan,\nEthan Dyer, Henryk Michalewski, Vinay Ramasesh,\nAmbrose Slone, Cem Anil, Imanol Schlag, Theo\nGutman-Solo, et al. 2022. Solving quantitative\nreasoning problems with language models. arXiv\npreprint arXiv:2206.14858.\nXuechen Li, Florian Tramer, Percy Liang, and Tatsunori\nHashimoto. 2021. Large language models can be\nstrong differentially private learners. arXiv preprint\narXiv:2110.05679.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, et al. 2022. Holistic evaluation of language\nmodels. ArXiv preprint, abs/2211.09110.\nBo Liu, Qiang Liu, and Peter Stone. 2022. Continual\nlearning and private unlearning. In Conference on\nLifelong Learning Agents, pages 243–254. PMLR.\nAndrew Maas, Raymond E Daly, Peter T Pham, Dan\nHuang, Andrew Y Ng, and Christopher Potts. 2011.\nLearning word vectors for sentiment analysis. In\nProceedings of the 49th annual meeting of the associ-\nation for computational linguistics: Human language\ntechnologies, pages 142–150.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstra-\ntions: What makes in-context learning work? arXiv\npreprint arXiv:2202.12837.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\nFinn, and Christopher D Manning. 2021. Fast model\nediting at scale. arXiv preprint arXiv:2110.11309.\nThanh Tam Nguyen, Thanh Trung Huynh, Phi Le\nNguyen, Alan Wee-Chung Liew, Hongzhi Yin, and\nQuoc Viet Hung Nguyen. 2022. A survey of machine\nunlearning. arXiv preprint arXiv:2209.02299.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023. Is\nchatgpt a general-purpose natural language process-\ning task solver? arXiv preprint arXiv:2302.06476.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, page 9.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. JMLR, 21(140):1–67.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun\nRaja, et al. 2021. Multitask prompted training en-\nables zero-shot task generalization. arXiv preprint\narXiv:2110.08207.\nAyush Sekhari, Jayadev Acharya, Gautam Kamath, and\nAnanda Theertha Suresh. 2021. Remember what you\nwant to forget: Algorithms for machine unlearning.\nAdvances in Neural Information Processing Systems,\n34:18075–18086.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, et al. 2022. Using deep-\nspeed and megatron to train megatron-turing nlg\n530b, a large-scale generative language model. arXiv\npreprint arXiv:2201.11990.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\n12051\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuanjing Huang, Guihong Cao, Daxin Jiang, Ming\nZhou, et al. 2020. K-adapter: Infusing knowledge\ninto pre-trained models with adapters. arXiv preprint\narXiv:2002.01808.\nSinong Wang, Han Fang, Madian Khabsa, Hanzi Mao,\nand Hao Ma. 2021. Entailment as few-shot learner.\narXiv preprint arXiv:2104.14690.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\nguage model with self generated instructions. arXiv\npreprint arXiv:2212.10560.\nAlbert Webson and Ellie Pavlick. 2022. Do prompt-\nbased models really understand the meaning of their\nprompts? In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2300–2344, Seattle, United States.\nAssociation for Computational Linguistics.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. Advances in neural informa-\ntion processing systems, 32.\nDa Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi,\nHuseyin A Inan, Gautam Kamath, Janardhan Kulka-\nrni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz,\net al. 2021. Differentially private fine-tuning of lan-\nguage models. arXiv preprint arXiv:2110.06500.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao\nSun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\nLili Yu, et al. 2023. Lima: Less is more for alignment.\narXiv preprint arXiv:2305.11206.\n12052",
  "topic": "Forgetting",
  "concepts": [
    {
      "name": "Forgetting",
      "score": 0.8453572392463684
    },
    {
      "name": "Computer science",
      "score": 0.6780613660812378
    },
    {
      "name": "Memorization",
      "score": 0.6150690913200378
    },
    {
      "name": "Transformer",
      "score": 0.490085244178772
    },
    {
      "name": "Process (computing)",
      "score": 0.479579359292984
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46159765124320984
    },
    {
      "name": "Engineering",
      "score": 0.16279125213623047
    },
    {
      "name": "Mathematics education",
      "score": 0.12221881747245789
    },
    {
      "name": "Psychology",
      "score": 0.12182044982910156
    },
    {
      "name": "Cognitive psychology",
      "score": 0.09664034843444824
    },
    {
      "name": "Programming language",
      "score": 0.0917622447013855
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}