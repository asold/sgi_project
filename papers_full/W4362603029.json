{
  "title": "Modeling air quality PM2.5 forecasting using deep sparse attention-based transformer networks",
  "url": "https://openalex.org/W4362603029",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2081757089",
      "name": "Z Zhang",
      "affiliations": [
        "Taizhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2096629101",
      "name": "Zhang S",
      "affiliations": [
        "Taizhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2081757089",
      "name": "Z Zhang",
      "affiliations": [
        "Taizhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2096629101",
      "name": "Zhang S",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3030845602",
    "https://openalex.org/W3024071797",
    "https://openalex.org/W3206110539",
    "https://openalex.org/W2981982271",
    "https://openalex.org/W2077865456",
    "https://openalex.org/W3128592650",
    "https://openalex.org/W3021936800",
    "https://openalex.org/W2889009471",
    "https://openalex.org/W2791942352",
    "https://openalex.org/W3000104524",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W3161873870",
    "https://openalex.org/W3091985306",
    "https://openalex.org/W2898947564",
    "https://openalex.org/W3080185158",
    "https://openalex.org/W2467755164",
    "https://openalex.org/W3170630188",
    "https://openalex.org/W4254816979",
    "https://openalex.org/W3189604654",
    "https://openalex.org/W3013537470",
    "https://openalex.org/W2793563492",
    "https://openalex.org/W2009692134",
    "https://openalex.org/W3095733646",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2033178790",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3126075911",
    "https://openalex.org/W2041471468",
    "https://openalex.org/W2130231837",
    "https://openalex.org/W3167456680",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W3161454960",
    "https://openalex.org/W2530443992",
    "https://openalex.org/W2174574259",
    "https://openalex.org/W2186933649",
    "https://openalex.org/W3162541787",
    "https://openalex.org/W3124650867",
    "https://openalex.org/W3034264063",
    "https://openalex.org/W2965353672",
    "https://openalex.org/W3093869481",
    "https://openalex.org/W1598069255",
    "https://openalex.org/W2989156240",
    "https://openalex.org/W3081466337",
    "https://openalex.org/W2569758175",
    "https://openalex.org/W2031478711",
    "https://openalex.org/W2130189616",
    "https://openalex.org/W209050710",
    "https://openalex.org/W3038323027",
    "https://openalex.org/W4221101267",
    "https://openalex.org/W3187631632",
    "https://openalex.org/W2899742462",
    "https://openalex.org/W3133613421",
    "https://openalex.org/W2580112876",
    "https://openalex.org/W3086892812",
    "https://openalex.org/W2789849108",
    "https://openalex.org/W3083825610",
    "https://openalex.org/W3192789775",
    "https://openalex.org/W2969433991",
    "https://openalex.org/W3107101618",
    "https://openalex.org/W3008191852",
    "https://openalex.org/W2009331722",
    "https://openalex.org/W2523989733",
    "https://openalex.org/W2995296280",
    "https://openalex.org/W3001762048",
    "https://openalex.org/W3137937485",
    "https://openalex.org/W4206836229",
    "https://openalex.org/W3025906443",
    "https://openalex.org/W1969865391",
    "https://openalex.org/W3177318507",
    "https://openalex.org/W3217348554",
    "https://openalex.org/W2891364071"
  ],
  "abstract": "Abstract Air quality forecasting is of great importance in environmental protection, government decision-making, people's daily health, etc. Existing research methods have failed to effectively modeling long-term and complex relationships in time series PM2.5 data and exhibited low precision in long-term prediction. To address this issue, in this paper a new lightweight deep learning model using sparse attention-based Transformer networks (STN) consisting of encoder and decoder layers, in which a multi-head sparse attention mechanism is adopted to reduce the time complexity, is proposed to learn long-term dependencies and complex relationships from time series PM2.5 data for modeling air quality forecasting. Extensive experiments on two real-world datasets in China, i.e ., Beijing PM2.5 dataset and Taizhou PM2.5 dataset, show that our proposed method not only has relatively small time complexity, but also outperforms state-of-the-art methods, demonstrating the effectiveness of the proposed STN method on both short-term and long-term air quality prediction tasks. In particular, on singe-step PM2.5 forecasting tasks our proposed method achieves R 2 of 0.937 and reduces RMSE to 19.04 µg/m 3 and MAE to 11.13 µg/m 3 on Beijing PM2.5 dataset. Also, our proposed method obtains R 2 of 0.924 and reduces RMSE to 5.79 µg/m 3 and MAE to 3.76 µg/m 3 on Taizhou PM2.5 dataset. For long-term time step prediction, our proposed method still performs best among all used methods on multi-step PM2.5 forecasting results for the next 6, 12, 24, and 48 h on two real-world datasets.",
  "full_text": "Vol.:(0123456789)1 3\nInternational Journal of Environmental Science and Technology (2023) 20:13535–13550 \nhttps://doi.org/10.1007/s13762-023-04900-1\nORIGINAL PAPER\nModeling air quality PM2.5 forecasting using deep sparse \nattention‑based transformer networks\nZ. Zhang1 · S. Zhang2 \nReceived: 23 March 2022 / Revised: 4 January 2023 / Accepted: 16 March 2023 / Published online: 4 April 2023 \n© The Author(s) 2023\nAbstract\nAir quality forecasting is of great importance in environmental protection, government decision-making, people's daily health, \netc. Existing research methods have failed to effectively modeling long-term and complex relationships in time series PM2.5 \ndata and exhibited low precision in long-term prediction. To address this issue, in this paper a new lightweight deep learn-\ning model using sparse attention-based Transformer networks (STN) consisting of encoder and decoder layers, in which a \nmulti-head sparse attention mechanism is adopted to reduce the time complexity, is proposed to learn long-term dependencies \nand complex relationships from time series PM2.5 data for modeling air quality forecasting. Extensive experiments on two \nreal-world datasets in China, i.e., Beijing PM2.5 dataset and Taizhou PM2.5 dataset, show that our proposed method not \nonly has relatively small time complexity, but also outperforms state-of-the-art methods, demonstrating the effectiveness of \nthe proposed STN method on both short-term and long-term air quality prediction tasks. In particular, on singe-step PM2.5 \nforecasting tasks our proposed method achieves R 2 of 0.937 and reduces RMSE to 19.04 µg/m 3 and MAE to 11.13 µg/m 3 \non Beijing PM2.5 dataset. Also, our proposed method obtains R 2 of 0.924 and reduces RMSE to 5.79 µg/m 3 and MAE to \n3.76 µg/m3 on Taizhou PM2.5 dataset. For long-term time step prediction, our proposed method still performs best among \nall used methods on multi-step PM2.5 forecasting results for the next 6, 12, 24, and 48 h on two real-world datasets.\nKeywords Air quality forecasting · Deep learning · Sparse attention · Transformer · Long-term dependency\nIntroduction\nWith the acceleration and deepening of industrialization and \nurbanization, air pollution has been a more and more serious \nproblem, which heavily threatens to human health with a \nvariety of respiratory diseases such as chronic pharyngitis, \nchronic bronchitis, and bronchial asthma (Chang et al. 2020; \nSchwartz 1993; Yan et al. 2020). Besides, heavy air pollu-\ntion will lead to a haze, resulting in the low atmosphere vis-\nibility, traffic accidents, flight delays, and so on. Therefore, \nhow to realize an accurate air quality forecasting has gradu-\nally drawn extensive attentions in recent years, due to its \nimportance in environmental protection (Liao et al. 2015), \ngovernment decision-making (Zheng et al. 2015), people's \ndaily health (Ha Chi and Kim Oanh 2021), etc.\nSo far, a large number of big cities have established air \nquality monitoring stations in urban areas to observe the \ncity’s real-time PM2.5 and other air pollutants such as \nPM10, CO,  O3,  NO2,  SO2, etc. (Li and Cheng 2021; Wang \net al. 2022a, b). In China, the air quality status of different \ncities in the east, north, and northeast of China is sometimes \nmore notable in the world, since prior studies have been \nreported the chemical composition and mass concentra-\ntion of PM2.5 in these areas of China (Gautam et al. 2019). \nLong-term exposure to PM2.5 easily causes the respiratory \ndiseases (Chai et al. 2019; Yang et al. 2020). As a result, air \npollution caused by PM2.5 has been regarded as a crucial \nproblem threatening to people's daily health. Hence, it is \nof great importance to perform early diagnosis of air pol-\nlution occurrence and PM2.5 concentration estimation for \nair quality forecasting. At present, tremendous efforts have \nEditorial responsibility: Samareh Mirkia.\n * S. Zhang \n tzczsq@163.com\n1 Zhejiang Provincial Key Laboratory of Evolutionary \nEcology and Conservation, Taizhou University, \nTaizhou 318000, Zhejiang, China\n2 Institute of Intelligent Information Processing, \nTaizhou University, Taizhou 318000, Zhejiang, \nPeople’s Republic of China\n13536 International Journal of Environmental Science and Technology (2023) 20:13535–13550\n1 3\nbeen made to focus on air quality forecasting (Janarthanan \net al. 2021; Liu et al. 2021; Mao et al. 2021; Voukantsis et al. \n2011; Yi et al. 2019; Zhu et al. 2018). Existing approaches \nfor air quality prediction can be divided into two categories: \ndeterministic methods and statistical methods. In particular, \ndeterministic methods usually work in a model-driven man-\nner. That is, they utilize the aerodynamic theory to construct \na numeric model to simulate the pollutant discharge and \ndiffusion of atmospheric pollution concentration. The repre-\nsentative deterministic methods contain Nested Air Quality \nPrediction Modeling System (NAQPMS) (Wang et al. 2001), \nChemical Transport Models (CTMs) (Mihailovic et al. 2009; \nPonomarev et al. 2020), Weather Research and Forecast-\ning (WRF) (Powers et al. 2017), Community Multiscale \nAir Quality (CMAQ) (Zhang et al. 2014), the complicated \nWRF-SMOKE-CMAQ model (de Almeida Albuquerque \net al. 2018), and so on. However, these deterministic meth-\nods may provide inaccurate prediction results owing to the \nlack of real observations (Kukkonen et al. 2003). In addition, \nsince a variety of parameters in these models are required to \nbe decided by experience, they easily suffer from the expen-\nsive computation cost (Xu et al. 2017).\nBy contrast, statistical methods usually work in a data-\ndriven manner. In other words, based on the observed data \nthey directly employ a statistical modeling strategy to fore-\ncast air pollutant concentrations. The conventional linear \nstatistical methods for air quality prediction include Autore-\ngressive Moving Average (ARMA) (Graupe et al. 1975 ), \nAutoregressive Integrated Moving Average (ARIMA) \n(Cekim 2020; Jian et al. 2012), Autoregressive Distributed \nLag (ARDL) (Abedi et al. 2020). Nevertheless, these linear \nstatistical methods are based on the assumption that there \nexist linear relationships between data variables and tar -\nget labels. This does not comform to the non-linearity of \nreal-world observed data. Therefore, these linear statistical \nmethods may not obtain promising performance on air qual-\nity forecasting tasks. To address this issue, an alternative to \nthese liner statistial methods is to adopt nonlinear statisti-\ncal machine learning methods for air quality forecasting. \nThe representative nonlinear statistical machine learning \nmethods are Support Vector Regression (SVR) (Chu et al. \n2021; Yang et al. 2018), Artificial Neural Network (ANN) \n(Agarwal et al. 2020; Arhami et al. 2013), Random For -\nest (RF) (Gariazzo et al. 2020), eXtreme Gradient Boosting \n(XGBoost) (Chen and Guestrin 2016), and so on. Among \nthese nonlinear statistical machine learning methods, \nANNs have become one of the most popular approaches \nfor air quality forecasting. For instance, Ding et al. (2016) \nemployed sparse response back-propagation training feed-\nforward neural networks to predict air pollutant concentra-\ntion. Zhao et al. (2020) integrated forward neural networks \nand recurrent neural networks to predict air quality hourly \nin Northwest of China. Liu and Zhang (2021) developed a \nmethod of AQI (air quality index) time series prediction by \nmeans of a hybrid data decomposition and echo state net -\nworks. In recent years, ensemble learning for different ANNs \nhas been an attractive direction. In particular, an ensemble \nmethod based on 10 distinct ANNs was used to estimate \nair pollution health risks (Araujo et al. 2020). Wang et al. \n(2020) proposed a double decomposition and optimal com-\nbination ensemble learning method for interval-valued AQI \nforecasting. However, due to the used single-layer network \nstructure, these tranditional nonlinear statistical learning \nmethods belong to shallow leaning methods, resuting in their \nlimited feature learning ability and prediction performance \non air quality forecasting tasks.\nTo allievate the above-mentioned problem, recently \nemerged deep learning techqniques (Hinton and Salakhut-\ndinov 2006; LeCun et al. 2015) may present a possible solu-\ntion. With the aid of deep multi-layer network structures, \ndeep learning techqniques are capable of learning high-level \nfeature representations from input data and exhibit excel-\nlent performance in the fields of computer vision, natural \nlanguage processing, signal processing, and so on. The \nwell-known deep learning techniques contain Deep Belief \nNetwork (DBN) (Hinton and Salakhutdinov 2006), Convo-\nlutional Neural Network (CNN) (Krizhevsky et al. 2012), \nRecurrent Neural Network (RNN) (Elman 1990) and its \nvariant of Long Short-term Memory (LSTM) (Hochreiter \nand Schmidhuber 1997), and so on. At present, a variety \nof deep learning techniques have been successfully applied \nfor air quality forecasting (Akbal and Ünlü 2022; Dhakal \net al. 2021; Wong et al. 2021; Yang et al. 2021; Zhang et al. \n2020a, 2022; Zhou et al. 2022). For instance, a deep stacked \nautoencoder (AE) model (Li et al. 2016), as a variant of \nDBN, was used to learn inherent air features for air quality \nprediction. Image-based air quality prediction based on CNN \n(Chakma et al. 2017; Zhang et al. 2016) was proposed, in \nwhich CNNs were leaverged to recognize natural images \ninto different categories on the basis of their PM2.5 concen-\ntrations. An end-to-end deep learning model comprising of \nCNNs and Gradient Boosting Machine (GBM) (Luo et al. \n2020) was proposed for PM2.5 concentration prediction in \nShanghai City, China. A Graph-based LSTM (GLSTM) \nmodel (Gao and Li 2021) was presented to predict PM2.5 \nconcentration in Gansu Province of Northwest in China.\nIn recent years, various hybrid deep learning structures \nhave drawn extensive attention for air quality forecasting. \nIn particular, a hybrid deep learning framework combining \nVariational Mode Decomposition (VMD) and Bi-directional \nLSTM (BiLSTM) (Zhang et al. 2021) was developed to pre-\ndict PM2.5 changes in cities in China. A transfer learning-\nbased BiLSTM (Ma et al. 2019) was utilized to improve \nair quality prediction performance. A spatio-temporal Con-\nvolutional LSTM Extended (C-LSTME) model (Wen et al. \n2019), in which CNNs and LSTMs were integrated to learn \n13537International Journal of Environmental Science and Technology (2023) 20:13535–13550 \n1 3\nhigh-level spatio-temporal features, was presented to pre-\ndict air quality concentration. Although these deep learn-\ning methods mentioned above have achieved good perfor -\nmance on air quality forecasting tasks, they may still have a \ndrawback. That is, owing to the existed “gradient vanishing \nand exploding” problems in RNNs and LSTMs, as well as \nthe limited spatial learning ability of convolutional filters \nin CNNs, these sequence-aligned methods are restricted in \nmodeling long-term and complex relationships in time series \nPM2.5 data.\nTo mitigate the above-mentioned issue, in recent year \nthe developed Transformer (Vaswani et al. 2017) method, \noriginally proposed for machine translation tasks in natural \nlanguage processing, provides possible cues for long-term \nair quality prediction. The original Transformer model is \nconstructed based on self-attention mechanisms without \nany recurrent structures and convolutions. The motivation \nof the used self-attention mechanisms in the Transformer is \ntwofold. First, compared with recurrent structures it can deal \nwith more direct information flow across the whole sequence \ndata, thereby allowing for more direct gradient flow. Sec-\nond, it can perform faster training than recurrent structures, \nsince most operations can be implemented in parallel. So \nfar, self-attention-based Transformers have shown superior \nperformance to RNNs and LSTMs in the ability of capturing \nlong-range dependencies in the fields of machine translation \n(Neishi and Yoshinaga 2019; Vaswani et al. 2017), speech \nrecognition (Chen et al. 2021 ; Zeyer et al. 2019 ), image \nsegmentation and classification (Bazi et al. 2021; Duke \net al. 2021; Lanchantin et al. 2021), electricity-consuming \nload analysis (Yue et al. 2020 ; Zhou et al. 2021 ), and so \non. Although self-attention-based Transformers may own \npowerful capability of modeling long-range dependencies \nof sequence data, they still need large time and memory that \nincreases quadratically with the sequence length. Besides, \nfew studies attempt to explore Transformer-based methods \nfor long-term air quality forecasting. To address these two \nissues, this paper proposes a new lightweight deep learning \nmodel for air quality forecasting based on sparse attention-\nbased Transformer networks (STN) so as to model long-\nterm and complex relationships from time series PM2.5 \ndata. In our STN, a multi-head sparse attention mechanism \nis designed to learn long-term dependencies on the long span \nof time series PM2.5 data and meanwhile reduce the time \ncomplexity. Moreover, the proposed STN method can deal \nwith the whole time series PM2.5 data for each time employ \nwith the aid of self-attention mechanisms.\nFig. 1  Distribution of China's air quality monitoring stations (the \ncolor of each station denotes the rank of daily average PM2.5 on \nNovember 1, 2019, as depicted in the bottom right of the figure. For \ninterpretation related to color in this figure legend, the readers see the \ndetails from the website https:// www. aqist udy. cn/)\n13538 International Journal of Environmental Science and Technology (2023) 20:13535–13550\n1 3\nThe main contributions of this paper are summarized in \nthree aspects: (1) a new lightweight deep learning model \nbased on sparse attention-based Transformer networks \n(STN) is designed to learn long-term dependencies and \ncomplex relationships from time series PM2.5 data for deep \nair quality forecasting. The proposed STN method adopts a \nmulti-head sparse attention mechanism in the encoder and \ndecoder to learn long-term temporal dynamical information \nfrom time series PM2.5 data, and reduce time complexity \nsimultaneously; (2) to the best of our knowledge, this is the \nfirst attempt to exploit deep sparse attention-based Trans-\nformer networks for air quality forecasting. The proposed \nSTN method can process the entire time series PM2.5 data \nat the same time owing to the used self-attention mechanism. \nUnlike previous sequence-aligned methods, our method does \nnot need to deal with time series PM2.5 data in an ordered \nFig. 2  Methodology structure \nof modeling air quality PM2.5 \nforecasting based on shallow \nlearning and deep learning \nmethods\nFig. 3  Framework of proposed STN model for air quality (PM2.5 concentration) forecasting\n13539International Journal of Environmental Science and Technology (2023) 20:13535–13550 \n1 3\nsequence way; (3) this paper presents a comparative analy -\nsis of traditional ARIMA, SVR, RF, XGBoost, and recently \ndeveloped deep learning models like CNN, LSTM, the \noriginal Transformer as well as our STN method. Extensive \nexperiments on two real-world datasets in China, i.e., Bei-\njing PM2.5 dataset and Taizhou PM2.5 dataset, show that \nour method not only has relatively small time complexity, \nbut also outperforms state-of-the-arts, demonstrating the \neffectiveness of the proposed STN method on both short-\nterm and long-term air quality prediction tasks.\nMaterials and methods\nTo evaluate the performance of the proposed method on \nair quality forecasting tasks, we employ two real-world air \nquality PM2.5 databases to conduct air quality forecasting \nexperiments. One is the Beijing PM2.5 dataset (Liang et al. \n2015) available at https:// www. kaggle. com/ djhav era/ beiji ng- \npm25- data- data- set. The other is Taizhou PM2.5 dataset, \nwhich was collected by our teams from Taizhou city.\nStudy area\nIn this work, we choose two typical cities, i.e., Beijing and \nTaizhou, for studying air quality prediction, as depicted in \nFig. 1. Beijing city is the Capital of China and at 116°66ʹ \neast longitude and 40°13ʹ  north latitude. Taizhou city is \nlocated in the southeast of Zhejiang Province and at 121°42ʹ \neast longitude and 28°65ʹ north latitude. Figure 1 shows the \ndistribution of China's all air quality monitoring stations and \nthe ranking of PM2.5 values corresponding to each station \non November 1, 2019. Here, the rank of PM2.5 in Fig.  1 is \ndetermined by the Ambient Air Quality Standard (GB 3095-\n2012) in China (Zhang et al. 2020b).\nData description\nThe used Beijing PM2.5 dataset (Liang et al. 2015) is hourly \nair quality database consisting of PM2.5 data (http:// www.  \nmee. gov. cn/) of the US Embassy in Beijing and meteorologi-\ncal data (http:// tianqi. 2345. com/) from Beijing Capital Inter-\nnational Airport. This dataset includes eight feature items, \ni.e., PM2.5 concentration (µg/m 3), dew point, temperature, \npressure, combined wind direction, cumulated wind speed \n(m/s), cumulated hours of snow, cumulated hours of rain. \nThe original dataset is recorded with an hourly interval \nranging from 01/01/2010 to 12/31/2014, yielding a total of \naround 43,800 records. For year-independent experiments, \nthe first four-year data are used for training, whereas the \nlast year data (01/01/2014–12/31/2014) are selected as the \ntesting set. For model validation, we randomly split 10% of \nthe whole training set as the validation set. In this case, we \nkeep that the training, and testing sets come from differ -\nent years, thereby making such year-independent air quality \nforecasting experiments more practical. Note that such year-\nindependent experiments are more difficult than the common \nyear-dependent experiments in which the training and testing \nsets are derived from the same year.\nThe used hourly Taizhou PM2.5 dataset is collected from \nthe single Hongjia monitoring station, which is located in \nJiaojiang urban district from Taizhou city in Zhejiang Prov-\nince. This dataset also contains eight feature items, includ-\ning PM2.5 concentration (µg/m 3), dew point, temperature, \npressure, combined wind direction, cumulated wind speed \n(m/s), cumulated hours of rain, cumulated hours of relative \nhumidity. It consists of around 26,000 hourly records rang-\ning from 01/01/2017 to 12/31/2019. In our experiments, the \nfirst two-year data are used as the training set, and the last \nyear data (01/01/2019–12/31/2019) are adopted as the test-\ning set. The randomly divided 10% of the whole training set \nis employed as the validation set.\nMethods\nFigure 2 shows the methodology structure of modeling air \nquality PM2.5 forecasting based on shallow learning and \ndeep learning methods. The methodology structure starts \nwith data collection and processing. In particular, histori-\ncal PM2.5 concentration and meteorological data are col-\nlected from monitoring stations and then cleaned by means \nof eliminating outliers and padding missing values with \na linear interpolation way. Data normalization for all air \nquality time series data is performed before feeding data \ninto the used models. In the next stage of temporal mod -\neling, various models, including shallow learning models \nlike ARIMA, SVR, RF, XGBoost, as well as deep learning \nmodels like CNN, LSTM, Transformer, and our designed \nSTN, are employed to model temporal dynamics from time \nseries PM2.5 data for air quality forecasting. All used mod-\nels are trained and evaluated on the collected training and \ntesting data sets. Finally, we present the result comparison \nand analysis according to the used typical evaluation metrics \nlike root mean square error (RMSE), mean absolute error \n(MAE), and the coefficient of determination (R 2).\nSimilar to the conventional Transformer (Vaswani \net al. 2017), our designed sparse attention-based Trans-\nformer networks (STN) consist of encoder and decoder \nlayers depending on self-attention mechanisms, as shown \nin Fig.  3. In order to learn long-term dependencies and \ncomplex relationships from time series PM2.5 data, this \nframework integrates two different self-attention mecha-\nnisms, including a multi-head sparse attention mechanism \nused in the encoder and decoder, in which a sparse atten-\ntion block is designed to learn important queries for reduc-\ning time complexity, and a standard multi-head attention \n13540 International Journal of Environmental Science and Technology (2023) 20:13535–13550\n1 3\nmechanism (Vaswani et al. 2017) in the decoder. In the \nfollowing, we will elaborate the details related to the \ndesigned STN model.\nProblem description\nGiven input time series data /u1D431={ x1 ,x2 ,… ,xLx\n} ( xi ∈ ℝdx ) \nwith a length Lx (historical meteorological data and PM2.5 \nconcentration data) and input dimension dx , the proposed \nmethod aims to predict the corresponding time series data \n/u1D432={ y1 ,y2 ,… ,yLy\n} ( yi ∈ ℝdy ) with a length Ly and input \ndimension dy . The encoder maps input time series data \n/u1D431={ x1 ,x2 ,… ,xLx\n} into a hidden continuous representation \n/u1D433={ z1 ,z2 ,… ,zLz\n} . Then, the decoder generates an output \nof /u1D432={ y1 ,y2 ,… ,yLy\n} from the given /u1D433={ z1 ,z2 ,… ,zLz\n} . \nThis inference is realized by using an step-by-step operation \nin which the decoder calculates a new hidden representation \n/u1D433k+1 from the previous /u1D433k and other outputs in k-th step, and \nthen forecasts the (k + 1)-th time series data /u1D432k+1.\nPosition embedding\nSince the original Transformer model (Vaswani et al. 2017) \ndoes not have recurrent structures and convolutions, it has \nno ability of leveraging the temporal information of time \nseries data. It is thus needed to extract the relative or abso-\nlute position information of the tokens in time series data. To \nthis end, position embedding, which is conducted with the \nnonlinear sine and cosine functions (Vaswani et al. 2017), \nis utilized to encode the temporal information of time series \ndata. Position embedding is usually added at the bottoms of \nthe encoder and decoder of the used Transformer model, as \ndescribed in Fig. 3.\nEncoder\nGiven input time series data /u1D431 , consisting of normalized his-\ntorical meteorological data and PM2.5 concentration data, \nposition embedding is used to encode the temporal informa-\ntion of /u1D431 and generate the resulting vector with the length \nof Lx as inputs of the encoder. The designed encoder aims \nto compute the interrelationship of PM2.5-related data at \neach time point in the sequence data by means of using a \nsparse self-attention mechanism in an effort to capture the \nrelevance and importance of PM2.5-related data at different \ntimes in the sequence data. For such self-attention encoder, \nthe attention weights can be calculated by means of using \nthe scaled dot-product attention of the tuple input (query, \nkey, value).\nDifferent from the original Transformer model (Vaswani \net al. 2017) with the single branch, the designed encoder \ncontains two-branch parallel pipelines: (1) one sparse \nattention block and (2) two sparse attention blocks cascaded \nwith a 1D convolution with a kernel width 3 and a max-\npooling with stride 2. Each sparse attention block consists of \na multi-head sparse attention layer, a fully connected feed-\nforward network, followed by layer normalization. A resid-\nual connection (He et al. 2016) is used around each of two \nsub-layers. Here, the used 1D convolution and max-pooling \noperations are adopted for the self-attention distilling opera-\ntion to extract the dominant attention, thereby decreasing \nthe network size. In addition, the first branch path with one \nsparse attention block receives halving inputs 1\n2 Lx , thereby \nreducing the number of self-attention distilling layers and \nimproving robustness. In a concatenated layer, the learned \nfeature maps of two-branch parallel pipelines are merged as \nthe output /u1D433 of the encoder.\nDecoder\nThe decoder aims to learn the weighted attention compo -\nsition of feature maps, and meanwhile, output predicted \nPM2.5 concentration data in a generative manner. The \ndecoder is composed of a masked sparse attention block, a \nmulti-head attention layer, a fully connected feed-forward \nnetwork, and each of them is followed by layer normaliza-\ntion. Similar to the encoder, a residual connection (He et al. \n2016) is also employed around each of three sub-layers. A \nlinear mapping layer is used at the top of the decoder to \noutput the PM2.5 prediction results /u1D432 . The masked sparse \nattention is obtained in the process of sparse attention com-\nputing by setting masked dot products to −∞ , avoiding \nauto-regressive. The decoder receives time series input data \n/u1D431de ={ /u1D431token,/u1D4310 } , where /u1D431token represents the started tokens \nand /u1D4310 denotes the placeholder for target time series data.\nSelf‑attention mechanism and sparse analysis\nGiven an input times series data matrix /u1D417∈ ℝL×dx with a \nlength L and input dimension d , in terms of the tuple input \n(query, key, value) the standard self-attention mechanism \n(Vaswani et al. 2017) computes the scaled dot-product as\nwhere the query matrix /u1D410∈ ℝL×d , key matrix /u1D40A∈ ℝL×d , \nvalue matrix /u1D415∈ ℝL×d are separately defined as\n(1)Att(/u1D410,/u1D40A,/u1D415)=softmax\n�\n/u1D410/u1D40AT\n√\nd\n�\n,\n(2)\n/u1D410= /u1D417/u1D416q,\n/u1D40A= /u1D417/u1D416k,\n/u1D415= /u1D417/u1D416v,\n13541International Journal of Environmental Science and Technology (2023) 20:13535–13550 \n1 3\nwhere /u1D416q, /u1D416k, /u1D416v denote the projection matrices. Equa-\ntion (1) can be reformulated as its vector form. In particular, \ngiven the i-th query qi from /u1D410 , the attention score on the j\n-th key from /u1D40A can be computed by\nThen, the self-attention score of qi over /u1D40A can be defined as\nIn this case, the time complexity of the standard self-atten-\ntion mechanism (Vaswani et al. 2017) is O(L2) . For the query \nmatrix, there is a potential sparsity, that is, a lot of redundant \ncalculations are conducted to obtain attention scores for all \nqueries. It is needed to choose important queries in which \nthe calculated attention scores over all keys are far from the \nuniform distribution. To measure important queries, the Kull-\nback–Leibler (K-L) divergence (Hershey and Olsen 2007) \nbetween the true distribution P of p(k j/uni007C.x/uni007C.xqi ) and the uniform \ndistribution U is used, as described below.\nAfter dropping the constant ln L , the sparse measurement \nof qi can be expressed as\nAccording to the obtained values of M sparse , larger M sparse \ncorresponds to more important queries in the self-attention \n(3)p(kj��qi)= e\nqikT\nj\n�√\nd\n∑L\nl=1 e\nqikT\nl\n�√\nd\n.\n(4)Att (qi,/u1D40A,/u1D415)=\nL/uni2211.s1\nj=1\np(k j/uni007C.x/uni007C.xqi )vj.\n(5)KL (P‖U )=ln\nL�\nj=1\ne\nqikT\nj√\nd − 1\nL\nL�\nj=1\nqikT\nj\n√\nd\n− lnL\n(6)M sparse(qi,/u1D40A)=ln\nLK�\nj=1\ne\nqikT\nj√\nd − 1\nLK\nLK�\nj=1\nqikT\nj\n√\nd\nmechanism. However, computing Eq. (6) is still expensive, \nsince traversing all queries is needed to calculate every dot-\nproduct pairs. To further alleviate the computation issue, \nEq. (6) can be approximated by using sampling ways:\nwhere ̃/u1D40A denotes the random sampling key matrix and ̃L \ndenotes the random sampling number. After figuring out \ñM sparse for each query, only top u dominant queries are \nemployed to calculate self-attention, filling other pairs with \nzero. In this case, the time complexity is O(L lnL) for a given \nsequence length of L.\nPerformance evaluation criteria\nTo evaluate the performance of different methods on air \nquality forecasting tasks, three typical evaluation metrics, \nsuch as root mean square error (RMSE), mean absolute error \n(MAE), and the coefficient of determination (R 2), were uti-\nlized for experiments. These three evaluation metrics are \nexpressed below.\n(7)̃M sparse(qi, ̃/u1D40A)=max\nj\n�qikT\nj\n√\nd\n�\n− 1\ñL\ñL�\nj=1\nqikT\nj\n√\nd\n(8)RMSE (y,̂y)=\n/uni221A.t/uni221A.x/uni221A.x/uni221A.s41\nn\nn/uni2211.s1\ni=1\n(yi − ̂yi)2,\n(9)MAE (y,̂y)= 1\nn\nn/uni2211.s1\ni=1\n/uni007C.x/uni007C.xyi − ̂yi/uni007C.x/uni007C.x,\n(10)R2 = 1 −\n∑n\ni=1 (yi − ̂yi)2\n∑n\ni=1 (yi − ymean\ni )2 ,\nTable 1  Comparisons of \ndifferent methods on singe-step \nPM2.5 forecasting results for \nthe next 1 h\nForward-step prediction size is 1 for the next 1 h (h1). Bold emphasis denotes the best method for smallest \nRSME (µg/m3), MAE (µg/m3), and the largest R2. Execution time is measured in seconds\nModels Beijing PM2.5 dataset Execution time Taizhou PM2.5 dataset Execution time\nRMSE MAE R2 RMSE MAE R2\nSVR-POLY 35.75 28.28 0.821 0.61 9.00 7.03 0.821 0.24\nSVR-RBF 40.37 34.24 0.807 0.72 8.98 7.17 0.825 0.28\nSVR-LINEAR 24.17 16.83 0.899 0.53 6.83 5.08 0.873 0.21\nARIMA 22.55 14.67 0.902 8.24 6.74 4.81 0.882 4.46\nRF 25.71 13.68 0.915 2.06 6.07 3.86 0.914 2.33\nXGBoost 25.72 13.76 0.912 2.32 6.25 4.05 0.909 2.45\nCNN 28.55 19.07 0.875 0.83 9.56 6.86 0.793 0.81\nLSTM 20.63 12.91 0.926 1.33 6.32 4.55 0.893 1.65\nTransformer 19.53 11.57 0.931 3.41 6.16 4.07 0.920 4.32\nSTN 19.04 11.13 0.937 2.18 5.79 3.76 0.924 2.78\n13542 International Journal of Environmental Science and Technology (2023) 20:13535–13550\n1 3\nwhere yi represents the observed PM2.5 value of i-th sample, \n̂yi denotes the predicted PM2.5 value of of i-th sample, ymean\ni  \nis the mean value of observed PM2.5 values, and n is the \ntotal number of samples. The smaller the RMSE and MAE \nare, the better the final prediction performance is. In this \ncase, R2 is often relatively larger.\nImplementation details\nAll the experiments are implemented on a PC server config-\nured with a NVIDIA Quadro P6000 graphics card which has \na 24G memory. We adopt the open source machine learning \nframework, i.e., Pytorch (https:// pytor ch. org) and Sklearn \n(https:// scikit- learn. org/), to build all machine learning \nmethods for air quality forecasting. In particular, the open-\nsource Tensorflow library (https:// github. com/ tenso rflow/) \nis used to configure deep learning and Transformer models. \nFor these models, the Adam optimizer is employed, the ini-\ntial learning rate is  le−4, the batch size is 32, the maximum \nof epochs is 200, and the mean squared error loss function \nis adopted. All air quality time series data are normalized to \n[0, 1]. The lookup size (window size), representing historical \nobservations as input size of all used models, is set to 24 for \nits best performance. We compared our STN method with \nother typical techniques, including the traditional shallow \nlearning models such as ARIMA and SVR, RF, XGBoost, \nas well as recently developed CNNs, LSTMs, original Trans-\nformer methods. They are described below in brief.\nARIMA is a typical linear statistical model for forecasting \ntime series data. SVR is a kernel model based on nonlin-\near statistical machine learning theories which also can be \nused for time series data prediction. SVR was adopted with \nthree different kernels (RBF, poly, and linear) with default \nparameter settings, i.e., the penalty coefficient is 1, and the \npolynomial degree is 3. RF is a simple ensemble learning \ntechniques based on decision tree predictors, and the number \nof trees in RF is set as 200. XGBoost is a tree-based boosting \nmodel that combines multiple tree models with low perfor -\nmance to build a stronger model, and the number of trees \nin XGBoost is also set as 200. CNNs are a typical deep \nlearning model for 2D image data processing. Here, we use \n1D-CNN for air quality prediction since time-series PM2.5 \ndata are 1D. The used 1D-CNN contains 256 convolution \nkernels with a kernel width of 5 and a stride of 1, followed \nby a batch normalization layer, max-pooling layer, rectified \nlinear unit layer, a dropout (0.3) layer, and a fully connected \nlayer. LSTMs are a special kind of recurrent architecture \nused for modeling long-range dependencies more accurately \non time series data in comparison with simple RNNs. We \nadopt BiLSTM for air quality forecasting, in which a for -\nward LSTM and a backward LSTM are included. Since air \nquality data change significantly over time and has a strong \nrelationship with the state before and after, BiLSTM may \nbe appropriate for predicting PM2.5 data. In this study, we \nused a two-layer BiLSTM for air quality prediction, each \nof which has 256 hidden neurons, followed by a dropout \n(0.05) layer. For the original Transformer model (Vaswani \net al. 2017) and the proposed STN method, we employ three \nencoders and two decoders for its promising performance. \nIn the following section, we provided experimental results \nin two aspects: single-step forecasting for the next 1 h and \nmulti-step forecasting for the next multiple hours.\nResults and discussion\nSingle‑step forecasting results\nTable 1 shows a comparative analysis of single-step PM2.5 \nforecasting quantitative results (RMSE, MAE, R 2) for the \nnext 1 h (h1) obtained by different used methods, including \nSVR (poly, rbf and linear kernel), ARIMA, RF, XGBoost, \nCNN, LSTM, Transformer, and the proposed STN method, \nTable 2  Comparisons of \ndifferent methods on multi-step \nPM2.5 forecasting results for \nthe next 6 h on two real-world \ndatasets\nForward-step prediction size is 6 for the next 6 h (h6). Bold emphasis denotes the best method for smallest \nRSME (µg/m3), MAE (µg/m3), and the largest R2\nModels Beijing PM2.5 dataset Taizhou PM2.5 dataset\nRMSE MAE R2 RMSE MAE R2\nSVR-POLY 52.69 42.18 0.654 13.87 10.88 0.584\nSVR-RBF 53.64 44.02 0.653 13.97 11.12 0.578\nSVR-LINEAR 45.47 33.15 0.707 13.53 9.30 0.615\nRF 47.65 28.38 0.737 11.79 8.81 0.699\nXGBoost 48.03 28.66 0.731 13.21 9.34 0.612\nCNN 42.40 28.01 0.742 13.06 9.24 0.622\nLSTM 38.72 24.28 0.744 12.50 8.77 0.711\nTransformer 36.95 23.12 0.752 11.54 7.74 0.716\nSTN 36.41 22.09 0.782 11.04 7.19 0.731\n13543International Journal of Environmental Science and Technology (2023) 20:13535–13550 \n1 3\nTable 3  Comparisons of different methods on multi-step PM2.5 forecasting results for the next 12 h on Beijing and Taizhou PM2.5 datasets\nForward-step prediction size is 12 for the next 12 h (h12). Bold emphasis denotes the best method for smallest RSME (µg/m 3), MAE (µg/m 3), \nand the largest R2\nDatasets Models RMSE MAE R2\n1 h-3 h 4 h-6 h 7 h-12 h 1 h-3 h 4 h-6 h 7 h-12 h 1 h-3 h 4 h-6 h 7 h-12 h\nBeijing SVR-POLY 44.06 60.09 70.72 35.13 49.21 57.58 0.747 0.562 0.348\nSVR-RBF 47.51 59.13 70.90 39.57 48.44 57.96 0.737 0.571 0.344\nSVR-LINEAR 34.04 54.56 69.77 23.90 42.41 56.47 0.812 0.591 0.351\nRF 36.50 56.15 71.83 20.84 35.95 48.10 0.820 0.614 0.411\nXGBoost 36.85 56.35 71.96 21.12 36.19 48.59 0.817 0.597 0.390\nCNN 33.18 48.89 60.31 23.15 32.92 42.96 0.807 0.623 0.485\nLSTM 31.17 46.14 59.40 19.68 30.50 41.39 0.829 0.658 0.487\nTransformer 29.70 43.98 56.02 19.26 29.67 39.27 0.831 0.662 0.492\nSTN 28.31 42.40 55.49 18.27 29.01 38.80 0.844 0.686 0.509\nTaizhou SVR-POLY 11.58 15.83 17.72 9.04 12.71 14.26 0.710 0.458 0.323\nSVR-RBF 11.77 15.86 17.74 9.37 12.86 14.35 0.700 0.456 0.319\nSVR-LINEAR 11.45 14.69 17.23 8.25 11.35 13.53 0.713 0.461 0.326\nRF 11.32 14.57 16.56 7.94 10.43 11.99 0.743 0.534 0.404\nXGBoost 11.41 14.65 17.27 7.25 11.29 13.58 0.718 0.537 0.356\nCNN 11.21 14.43 16.33 8.09 10.38 11.86 0.729 0.543 0.419\nLSTM 11.01 13.72 15.94 7.75 9.37 11.58 0.783 0.617 0.448\nTransformer 10.06 13.28 15.83 7.51 9.35 11.29 0.785 0.616 0.464\nSTN 9.82 13.01 15.52 6.76 9.30 11.08 0.792 0.632 0.482\nTable 4  Comparisons of different methods on multi-step PM2.5 forecasting results for the next 24 h on Beijing and Taizhou PM2.5 datasets\nForward-step prediction size is 24 for the next 24 h (h1-h24). Bold emphasis denotes the best method for smallest RSME (µg/m3), MAE (µg/m3), \nand the largest R2\nDatasets Models RMSE MAE R2\n1–3 h 4–6 h 7–12 h 13–24 h 1–3 h 4–6 h 7–12 h 13–24 h 1–3 h 4–6 h 7–12 h 13–24 h\nBeijing SVR-POLY 44.03 60.06 70.68 79.43 35.09 49.16 57.53 64.86 0.747 0.562 0.359 0.262\nSVR-RBF 47.49 59.13 70.87 79.99 39.54 48.42 57.91 65.47 0.737 0.571 0.360 0.269\nSVR-LINEAR 40.06 54.58 69.75 77.85 28.91 42.40 56.44 62.81 0.772 0.591 0.366 0.289\nRF 36.53 56.29 71.59 85.09 26.86 36.00 48.16 59.97 0.779 0.608 0.415 0.298\nXGBoost 36.92 56.34 71.88 85.11 26.13 36.24 48.70 60.73 0.784 0.613 0.411 0.294\nCNN 36.91 49.85 63.74 72.71 26.73 36.77 42.08 55.70 0.782 0.619 0.484 0.341\nLSTM 35.82 48.34 60.41 70.52 24.11 33.25 41.75 50.72 0.796 0.626 0.487 0.379\nTransformer 32.93 45.69 57.44 70.00 21.68 30.12 39.96 48.46 0.803 0.651 0.490 0.384\nSTN 32.35 44.87 56.47 69.12 20.95 29.53 38.48 47.71 0.814 0.666 0.495 0.390\nTaizhou SVR-POLY 12.58 15.83 17.72 19.01 9.04 12.71 14.26 15.29 0.710 0.459 0.324 0.248\nSVR-RBF 12.77 15.87 17.74 19.08 9.36 12.86 14.34 15.34 0.702 0.450 0.319 0.240\nSVR-LINEAR 11.90 14.68 17.22 18.82 9.24 11.34 13.51 14.91 0.788 0.537 0.357 0.264\nRF 11.58 14.32 16.13 17.88 8.15 10.02 11.49 13.68 0.723 0.588 0.453 0.312\nXGBoost 11.64 14.48 16.23 18.03 8.20 10.23 11.88 13.72 0.717 0.556 0.423 0.304\nCNN 11.79 14.59 16.31 17.98 8.23 10.38 11.93 13.69 0.708 0.544 0.421 0.310\nLSTM 11.43 14.04 16.10 17.76 8.10 9.93 11.36 13.16 0.725 0.595 0.458 0.331\nTransformer 11.07 13.98 15.58 17.31 6.96 9.78 11.32 12.70 0.760 0.600 0.461 0.357\nSTN 9.89 13.41 14.37 16.98 6.69 9.53 11.23 12.22 0.780 0.615 0.474 0.363\n13544 International Journal of Environmental Science and Technology (2023) 20:13535–13550\n1 3\nfor the next 1 h on two real-world datasets, i.e., Beijing and \nTaizhou PM2.5 datasets. To evaluate the time computation \nefficiency of all models, Table 1 also presents the compari-\nsons of the execution time for all used models, which is \nmeasured with the model’s run-time implemented on the \ntesting data.\nFrom Table 1, we can make the following three observa-\ntions, as described below.\n1. Among all used methods, our STN method obtains the \nsmallest RSME, MAE, and the highest R 2 on two real-\nworld datasets. In particular, our method achieves the \nlargest R2 of 0.937 and reduces RMSE to 19.04 µg/m 3 \nand MAE to 11.13 µg/m 3 on Beijing PM2.5 dataset. \nAlso, our STN method gives the largest R2 of 0.924 and \nreduces RMSE to 5.79 µg/m3 and MAE to 3.76 µg/m3 on \nTaizhou PM2.5 dataset. This shows that compared with \nother methods such as SVR, ARIMA, RF, XGBoost, \nCNN, LSTM, Transformer, our STN method has more \npowerful ability of learn long-term dependencies and \ncomplex relationships from time series PM2.5 data for \nair quality forecasting. Additionally, our STN method \noutperforms the original Transformer method, demon-\nstrating the advantages of our STN method on air quality \nforecasting tasks. The reason is that the used multi-head \nsparse attention mechanism in our STN has stronger \nability of modeling long-term temporal dynamics from \ntime series PM2.5 data on air quality forecasting tasks.\n2. Most deep learning methods, such as LSTM, Trans-\nformer and our STN method, are superior to traditional \nshallow learning methods like SVR, ARIMA, RF, \nXGBoost on air quality prediction tasks. This indicates \nthe advantages of deep learning methods over tradi-\ntional shallow learning methods on air quality predic-\ntion tasks. Nevertheless, CNN does perform better than \nSVR, ARIMA, RF, and XGBoost on single-step PM2.5 \nprediction tasks. This shows that 2D image-based CNN \nis not very effective to process 1D time series PM2.5 \ndata.\n3. Among all used shallow learning methods, tree-based \nmethods such as RF and XGBoost outperform SVR and \nARIMA, demonstrating the superiority of tree-based \nmethods to SVR and ARIMA. In addition, RF slightly \nperforms better than XGBoost in terms of RSME, MAE, \nand R2\n.\n4. As for the computation efficiency, the ranking order of \nexecution time for all used models is ARIMA, Trans-\nformer, STN, XGBoost, RF, LSTM, CNN, SVR-RBF, \nSVR-POLY, and SVR-LINEAR. Note that our STN \nmethod, as an improved version of the original Trans-\nTable 5  Comparisons of different methods on multi-step PM2.5 forecasting results for the next 48 h on Beijing and Taizhou PM2.5 datasets\nForward-step prediction size is 48 for the next 48 h (h1-h48). Bold emphasis denotes the best method for smallest RSME (µg/m3), MAE (µg/m3), \nand the largest R2\nDatasets Models RMSE MAE R2\n1–6 h 7–12 h 13–24 h 25–48 h 1–6 h 7–12 h 13–24 h 25–48 h 1–6 h 7–12 h 13–24 h 25–48 h\nBeijing SVR-POLY 52.54 70.62 79.40 84.00 42.01 57.45 64.78 68.19 0.656 0.349 0.282 0.157\nSVR-RBF 53.57 70.81 79.93 84.09 43.92 57.85 65.37 68.31 0.655 0.345 0.281 0.152\nSVR-LINEAR 49.46 69.74 77.81 83.68 33.12 56.41 62.72 67.82 0.707 0.361 0.296 0.162\nRF 47.75 67.48 75.08 80.11 29.42 48.12 59.70 67.07 0.723 0.438 0.285 0.190\nXGBoost 48.07 67.87 75.15 80.06 29.72 48.63 59.97 67.12 0.717 0.431 0.284 0.173\nCNN 44.54 61.19 71.74 78.97 29.44 42.91 50.01 58.42 0.721 0.474 0.364 0.253\nLSTM 47.24 63.26 72.28 80.90 34.55 44.97 52.63 58.70 0.696 0.426 0.348 0.239\nTransformer 42.22 58.63 70.11 78.25 28.66 42.20 48.97 57.92 0.728 0.481 0.375 0.267\nSTN 40.53 57.46 69.94 77.80 27.52 40.03 48.23 57.27 0.736 0.498 0.397 0.295\nTaizhou SVR-POLY 14.86 17.72 19.00 20.06 10.87 14.26 15.28 16.20 0.584 0.324 0.219 0.132\nSVR-RBF 14.96 17.75 19.00 20.08 11.11 14.35 15.34 16.28 0.579 0.319 0.212 0.123\nSVR-LINEAR 14.51 17.22 18.81 19.91 10.28 13.52 14.90 15.97 0.593 0.357 0.235 0.146\nRF 14.32 17.83 18.64 20.59 9.19 12.90 14.43 15.16 0.611 0.414 0.268 0.158\nXGBoost 14.43 17.91 18.92 20.77 9.32 13.01 14.57 15.78 0.607 0.394 0.251 0.151\nCNN 13.32 16.41 17.95 19.14 9.45 12.53 13.15 14.64 0.601 0.416 0.307 0.191\nLSTM 13.83 16.99 18.21 19.55 9.79 12.85 13.41 14.96 0.598 0.413 0.302 0.186\nTransformer 13.15 15.57 17.68 18.93 9.33 12.06 12.70 14.33 0.605 0.419 0.310 0.223\nSTN 12.72 15.06 17.40 18.13 9.09 11.29 12.51 13.90 0.628 0.424 0.320 0.249\n13545International Journal of Environmental Science and Technology (2023) 20:13535–13550 \n1 3\nformer, takes less execution time compared with the \noriginal Transformer. In particular, STN separately saves \n1.23 and 1.54 s on Beijing and Taizhou datasets than \nTransformer. This is because, in comparison with Trans-\nformer, the used multi-head sparse attention mechanism \nin our STN method can reduce the time complexity from \nO(L2) to O(L lnL) , thereby yielding less execution time. \nThis demonstrates the effectiveness of our STN method \nover Transformer on the time computation complexity.\nMulti‑step forecasting results\nTable 2 presents the multi-step quantitative results of differ-\nent methods on forecasting PM2.5 tasks for the next 6 h on \ntwo real-world datasets. In Table 2, the testing error of dif-\nferent models is the mean prediction error values in the next \nforward 6 h (h1–h6), thereby giving a comparative analysis \nof RMSE, MAE, and R 2 of SVR (poly, rbf, and linear ker -\nnel), RF, XGBoost, CNN, LSTM, Transformer, and our STN \nmethod.\nAs shown in Table  2, among all used models our STN \nmethod still obtains the smallest RSME, MAE, and the \nhighest R2 on the Beijing and Taizhou datasets, followed \nby Transformer, LSTM, CNN, RF, XGBoost, SVR-LIN-\nEAR, SVR-POLY, and SVR-RBF. In particular, our STN \nmethod individually yields the highest R2 of 0.782 on Bei-\njing PM2.5 dataset and the highest R2 of 0.731 on Taizhou \nPM2.5 dataset. Additionally, our STN method reduces \nMAE to 22.09 µg/m 3 on Beijing PM2.5 dataset and MAE \nto 7.19 µg/m 3 on Taizhou PM2.5 dataset, respectively. It \nis worth pointing out that CNN yields better performance \nthan traditional SVR-LINEAR and XGBoost on multi-step \nPM2.5 forecasting tasks for the next 6 h (h1–h6). On the \ncontrary, CNN performs worse than SVR-LINEAR and \nXGBoost on single-step PM2.5 forecasting tasks for the \nnext 1 h (h1). This indicates that CNN improves the pre-\ndiction performance when the forward-step prediction size \nincreases from the next 1–6 h.\nFor long-term time step prediction, Tables  3, 4 and 5 \nseparately present performance comparisons of different \nmethods on multi-step PM2.5 forecasting results for the next \n12, 24, and 48 h on two real-world datasets. Note that for \nmore than 6 h prediction, we split them into several inter -\nvals and trained independent models for each interval. Then, \nwe reported the average prediction results for each inter -\nval. For instance, for the next 12 h (h1–h12) prediction, we \n(a)CNN for the next 48th hour (h48) prediction( b) LSTM for the next 48th hour (h48) prediction\n(c)Transformerf or then ext4 8th hour (h48) prediction (d)O ur method fort he next 48th hour (h48)p rediction\nFig. 4  Comparisons of multi-step ground truth and predicted PM2.5 \nvalues (µg/m 3) for the next 48  h (h48) obtained by CNN, LSTM, \nTransformer, and our STN method during one month (10/01/2014–\n10/31/2014) on Beijing PM2.5 dataset. (Each observation point in the \nhorizontal axis represents the timescale (hour) corresponding to the \nobtained PM2.5 value, as depicted in the vertical axis in this figure)\n13546 International Journal of Environmental Science and Technology (2023) 20:13535–13550\n1 3\ndivided it into three groups: 1–3, 4–6, and 7–12 h, as shown \nin Tables 3 and 4. For the next 24 h (h1–h24) prediction, four \ngroups such as 1–3, 4–6, 7–12 and 13–24 h are adopted. For \nthe next 48 h (h1–h48) prediction, four groups such as 1–6, \n7–12, 13–24, 25–48 h are used.\nFrom the results in Tables  3, 4 and 5 , we can see that \nwhen the prediction time step increases, the multi-step \nPM2.5 forecasting performances of all used models gradu-\nally decrease. Nevertheless, it can be observed that com -\npared with other methods, our STN method also achieves \nthe lowest prediction error (RMSE, MAE), and the highest \nR2 versus different forward prediction sizes. In addition, for \nthe next 48 h (h1–h48), CNN performs better than LSTM, \nRF, XGBoost, SVR-LINEAR, demonstrating the further \nperformance improvement in CNN on long-term air quality \nprediction.\nTo further exhibit the advantages of our STN method, \nwe present the visualization of multi-step PM2.5 fore-\ncasting results of four deep models for the next 48 h (h1-\nh48) on two real-world datasets. Specially, Fig.  4 shows \na comparison of multi-step ground truth and predicted \nPM2.5 values for the next 48 h (h48) obtained by CNN, \nLSTM, Transformer, and our STN method during one \nmonth (10/01/2014–10/31/2014) on Beijing PM2.5 data-\nset. Figure  5 presents a comparison of multi-step ground \ntruth and predicted PM2.5 values for the next 48 h (h48) \nobtained by CNN, LSTM, Transformer, and our STN \nmethod during one month (03/01/2019–03/31/2019) on \nTaizhou PM2.5 dataset. The results in Figs.  4 and 5 indi-\ncate that our STN method performs better than other used \nmethods when predicting PM2.5 values, especially in the \ntime period of wave valley and peak of air quality PM2.5 \ntesting data. Here, an illustration of the differences of dif-\nferent used methods is labeled with a red circle in Figs.  4 \nand 5.\nIn summary, the results in Tables  1, 2, 3, 4 and 5  and \nFigs.  4 and 5  on Beijing PM2.5 dataset and Taizhou \nPM2.5 dataset indicate that our STN method not only \nhas relatively small time complexity, but also outper -\nforms other used methods. This shows the advantages of \nour STN method on both short-term and long-term air \nquality prediction tasks over other used methods. More \nspecially, on singe-step PM2.5 forecasting tasks our STN \nmethod achieves R 2 of 0.937, RMSE of 19.04, and MAE \n(a)CNN for then ext 48th hour( h48) prediction (b) LSTM for then ext4 8th hour (h48) prediction\n(c)Transformerf or then ext4 8th hour (h48) prediction( d) Ourm ethod for then ext4 8th hour( h48) prediction\nFig. 5  Comparisons of multi-step ground truth and predicted \nhourly PM2.5 values (µg/m 3) for the next 48  h (h48) obtained by \nCNN, LSTM, Transformer, and our STN method during one month \n(03/01/2019–03/31/2019) on Taizhou PM2.5 dataset (each observa-\ntion point in the horizontal axis represents the timescale (hour) cor -\nresponding to the obtained PM2.5 value, as depicted in the vertical \naxis in this figure)\n13547International Journal of Environmental Science and Technology (2023) 20:13535–13550 \n1 3\nof 11.13 on Beijing PM2.5 dataset. On Taizhou PM2.5 \ndataset, our STN method obtains R 2 of 0.924, RMSE of \n5.79, and MAE of 3.76. For long-term PM2.5 forecasting, \nour STN method still gives better performance than other \nused methods on multi-step PM2.5 forecasting results for \nthe next 6, 12, 24, and 48 h on two real-world datasets. \nIn addition, it is found that the performance of all used \nmethod decreases with the increasing forward prediction \nsize. In particular, the prediction results for the next 48 h \nare the worst, followed by the next 24, 12, 6, and 1 h. \nBesides, deep learning methods usually outperform shal-\nlow learning methods, especially for on multi-step PM2.5 \nforecasting tasks.\nConclusion\nIn this paper, we present a new lightweight method of mod-\neling deep air quality forecasting based on sparse attention-\nbased Transformer networks (STN) for single-step forward \nand multi-step forward air quality PM2.5 prediction. Our \nSTN method, which adopts a multi-head sparse attention \nmechanism in the encoder and decoder to reduce the time \ncomplexity, is designed to learn long-term dependencies \nand complex relationships from time series PM2.5 data \nfor air quality forecasting. Our STN method is capable of \nprocessing the entire time series PM2.5 data at the same \ntime owing to the used self-attention mechanisms. We pre-\nsent a comparative analysis of traditional ARIMA, SVR, \nRF, XGBoost, as well as recently developed CNN, LSTM, \nTransformer, and our STN method. Experiment results on \nBeijing PM2.5 dataset and Taizhou PM2.5 dataset demon-\nstrate that our STN method not only has relatively small \ntime complexity, but also achieves better performance than \nother used methods, i.e., the recently emerged deep models \nlike the original Transformer, LSTM, CNN, and traditional \nARIMA, RF, XGBoost, SVR-LINEAR, SVR-POLY, and \nSVR-RBF on both short-term and long-term air quality \nprediction tasks.\nIn future, it is interesting and challenging to take into \naccount the abrupt variation in air pollution time series \ndata for air quality forecasting. This is because such \nsuccessful forecasting in advance for the sudden varia-\ntion in air pollution is very beneficial to environmental \nprotection, government decision-making, people's daily \nhealth, etc. In addition, it is also meaningful to explore \nmore advanced deep learning models on long-term air \nquality prediction under different forecasting conditions. \nBesides, this work evaluates the performance of the pro-\nposed method based on measurement samples at two air \nmonitoring sites in China. Therefore, it is also interesting \nto exploit the generalizability of the proposed STN method \nin larger geographical regions. Moreover, our STN method \nshows less time complexity than the original Transformer, \nbut the time complexity of our STN method is still larger \nthan traditional shallow learning methods. Therefore, how \nto further reduce the time complexity of our STN method \nis an important direction in future.\nFunding This work was supported by Zhejiang Provincial National \nScience Foundation of China under Grant No. LY20E080013, and \nLZ20F020002.\nData availability statement The datasets generated during the current \nstudy are not publicly available due to the privacy but are available \nfrom the corresponding author on reasonable request.\nDeclarations \nConflict of interest The authors declare no competing interests.\nEthical approval and consent to participate Not applicable.\nConsent for publication Not applicable.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article's Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\nAbedi A, Baygi MM, Poursafa P, Mehrara M, Amin MM, Hemami \nF, Zarean M (2020) Air pollution and hospitalization: an \nautoregressive distributed lag (ARDL) approach. Environ \nSci Pollut Res 27(24):30673–30680. https:// doi. org/ 10. 1007/  \ns11356- 020- 09152-x\nAgarwal S, Sharma S, R S, Rahman MH, Vranckx S, Maiheu B, Blyth \nL, Janssen S, Gargava P, Shukla VK, Batra S, (2020) Air quality \nforecasting using artificial neural networks with real time dynamic \nerror correction in highly polluted regions. Sci Total Environ \n735:139454. https:// doi. org/ 10. 1016/j. scito tenv. 2020. 139454\nAkbal Y, Ünlü KD (2022) A deep learning approach to model daily \nparticular matter of Ankara: key features and forecasting. Int J \nEnviron Sci Technol 19(7):5911–5927. https:// doi. org/ 10. 1007/ \ns13762- 021- 03730-3\nAraujo LN, Belotti JT, Alves TA, Tadano YdS, Siqueira H (2020) \nEnsemble method based on Artificial Neural Networks to esti -\nmate air pollution health risks. Environ Model Softw 123:104567. \nhttps:// doi. org/ 10. 1016/j. envso ft. 2019. 104567\nArhami M, Kamali N, Rajabi MM (2013) Predicting hourly air pollut-\nant levels using artificial neural networks coupled with uncertainty \n13548 International Journal of Environmental Science and Technology (2023) 20:13535–13550\n1 3\nanalysis by Monte Carlo simulations. Environ Sci Pollut Res \n20(7):4777–4789. https:// doi. org/ 10. 1007/ s11356- 012- 1451-6\nBazi Y, Bashmal L, Rahhal MMA, Dayil RA, Ajlan NA (2021) Vision \nTransformers for remote sensing image classification. Remote \nSens 13(3):516. https:// doi. org/ 10. 3390/ rs130 30516\nCekim HO (2020) Forecasting PM 10 concentrations using time \nseries models: a case of the most polluted cities in Turkey. Envi-\nron Sci Pollut Res 27(20):25612–25624. https:// doi. org/ 10. 1007/ \ns11356- 020- 08164-x\nChai G, He H, Sha Y, Zhai G, Zong S (2019) Effect of PM2.5 on daily \noutpatient visits for respiratory diseases in Lanzhou. China Sci \nTotal Environ 649:1563–1572. https:// doi. org/ 10. 1016/j. scito tenv. \n2018. 08. 384\nChakma A, Vizena B, Cao T, Lin J, Zhang J (2017) Image-based air \nquality analysis using deep convolutional neural network. In: 2017 \nIEEE international conference on image processing (ICIP), Bei-\njing, China, pp 3949–3952\nChang Q, Zhang H, Zhao Y (2020) Ambient air pollution and daily \nhospital admissions for respiratory system–related diseases in a \nheavy polluted city in Northeast China. Environ Sci Pollut Res \n27:10055–10064. https:// doi. org/ 10. 1007/ s11356- 020- 07678-8\nChen T, Guestrin C (2016) Xgboost: a scalable tree boosting system. \nIn: Proceedings of the 22nd ACM SIGKDD international confer-\nence on knowledge discovery and data mining, San Francisco, \nCalifornia, USA, pp 785–794\nChen X, Wu Y, Wang Z, Liu S, Li J (2021) Developing real-time \nstreaming transformer transducer for speech recognition on large-\nscale dataset. In: ICASSP 2021–2021 IEEE international confer -\nence on acoustics, speech and signal processing (ICASSP). IEEE, \nToronto, pp 5904–5908\nChu J, Dong Y, Han X, Xie J, Xu X, Xie G (2021) Short-term pre-\ndiction of urban PM 2.5 based on a hybrid modified variational \nmode decomposition and support vector regression model. \nEnviron Sci Pollut Res 28(1):56–72. https:// doi. org/ 10. 1007/  \ns11356- 020- 11065-8\nde Almeida Albuquerque TT, de Fátima AM, Ynoue RY, Moreira DM, \nAndreão WL, Dos Santos FS, Nascimento EGS (2018) WRF-\nSMOKE-CMAQ modeling system for air quality evaluation in \nSão Paulo megacity with a 2008 experimental campaign data. \nEnviron Sci Pollut Res 25(36):36555–36569. https:// doi. org/ 10. \n1007/ s11356- 018- 3583-9\nDhakal S, Gautam Y, Bhattarai A (2021) Exploring a deep LSTM \nneural network to forecast daily PM2.5 concentration using \nmeteorological parameters in Kathmandu Valley, Nepal. Air \nQual Atmos Health 14(1):83–96. https:// doi. org/ 10. 1007/  \ns11869- 020- 00915-6\nDing W, Zhang J, Leung Y (2016) Prediction of air pollutant concen-\ntration based on sparse response back-propagation training feed-\nforward neural networks. Environ Sci Pollut Res 23(19):19481–\n19494. https:// doi. org/ 10. 1007/ s11356- 016- 7149-4\nDuke B, Ahmed A, Wolf C, Aarabi P, Taylor GW (2021) Sstvos: \nsparse spatiotemporal transformers for video object segmenta-\ntion. In: Proceedings of the IEEE/CVF conference on computer \nvision and pattern recognition, pp 5912–5921\nElman JL (1990) Finding structure in time. Cogn Sci 14(2):179–211. \nhttps:// doi. org/ 10. 1016/ 0364- 0213(90) 90002-E\nGao X, Li W (2021) A graph-based LSTM model for PM2.5 forecast-\ning. Atmos Pollut Res 12(9):101150. https:// doi. org/ 10. 1016/j. \napr. 2021. 101150\nGariazzo C, Carlino G, Silibello C, Renzi M, Finardi S, Pepe N, \nRadice P, Forastiere F, Michelozzi P, Viegi G, Stafoggia M \n(2020) A multi-city air pollution population exposure study: \ncombined use of chemical-transport and random-Forest models \nwith dynamic population data. Sci Total Environ 724:138102. \nhttps:// doi. org/ 10. 1016/j. scito tenv. 2020. 138102\nGautam S, Patra AK, Kumar P (2019) Status and chemical charac-\nteristics of ambient PM2.5 pollutions in China: a review. Envi-\nron Dev Sustain 21(4):1649–1674. https:// doi. org/ 10. 1007/ \ns10668- 018- 0123-1\nGraupe D, Krause D, Moore J (1975) Identification of autoregressive \nmoving-average parameters of time series. IEEE Trans Auto-\nmat Contr 20(1):104–107. https://  doi. org/ 10. 1109/ TAC. 1975. \n11008 55\nHa Chi NN, Kim Oanh NT (2021) Photochemical smog modeling of \nPM2.5 for assessment of associated health impacts in crowded \nurban area of Southeast Asia. Environ Technol Innov 21:101241. \nhttps:// doi. org/ 10. 1016/j. eti. 2020. 101241\nHe K, Zhang X, Ren S, Sun J (2016) Deep residual learning for image \nrecognition. In: Proceedings of the IEEE conference on computer \nvision and pattern recognition, Nevada, USA, pp 770–778\nHershey JR, Olsen PA (2007) Approximating the Kullback Leibler \ndivergence between Gaussian mixture models. In: 2007 IEEE \ninternational conference on acoustics, speech and signal process-\ning (ICASSP'07). IEEE, Honolulu, pp IV-317–IV-320\nHinton GE, Salakhutdinov RR (2006) Reducing the dimensionality of \ndata with neural networks. Science 313(5786):504–507. https://  \ndoi. org/ 10. 1126/ scien ce. 112764\nHochreiter S, Schmidhuber JJNc, (1997) Long short-term memory. \nNeural Comput 9(8):1735–1780. https://  doi. org/ 10. 1162/ neco. \n1997.9. 8. 1735\nJanarthanan R, Partheeban P, Somasundaram K, Navin Elamparithi \nP (2021) A deep learning approach for prediction of air quality \nindex in a metropolitan city. Sustain Cities Soc 67:102720. https:// \ndoi. org/ 10. 1016/j. scs. 2021. 102720\nJian L, Zhao Y, Zhu Y-P, Zhang M-B, Bertolatti D (2012) An applica-\ntion of ARIMA model to predict submicron particle concentra-\ntions from meteorological factors at a busy roadside in Hangzhou, \nChina. Sci Total Environ 426:336–345. https:// doi. org/ 10. 1016/j. \nscito tenv. 2012. 03. 025\nKrizhevsky A, Sutskever I, Hinton GE (2012) Imagenet classification \nwith deep convolutional neural networks. In: Advances in neural \ninformation processing systems, Lake Tahoe, pp 1097–1105\nKukkonen J, Partanen L, Karppinen A, Ruuskanen J, Junninen H, Kole-\nhmainen M, Niska H, Dorling S, Chatterton T, Foxall R, Cawley \nG (2003) Extensive evaluation of neural network models for the \nprediction of NO2 and PM10 concentrations, compared with a \ndeterministic modelling system and measurements in central Hel-\nsinki. Atmos Environ 37(32):4539–4550. https:// doi. org/ 10. 1016/ \nS1352- 2310(03) 00583-1\nLanchantin J, Wang T, Ordonez V, Qi Y (2021) General multi-label \nimage classification with transformers. In: Proceedings of the \nIEEE/CVF conference on computer vision and pattern recogni-\ntion, pp 16478–16488\nLeCun Y, Bengio Y, Hinton G (2015) Deep learning. Nature \n521(7553):436–444. https:// doi. org/ 10. 1038/ natur e14539\nLi T, Cheng X (2021) Estimating daily full-coverage surface ozone \nconcentration using satellite observations and a spatiotemporally \nembedded deep learning approach. Int J Appl Earth Obs Geoinf \n101:102356. https:// doi. org/ 10. 1016/j. jag. 2021. 102356\nLi X, Peng L, Hu Y, Shao J, Chi T (2016) Deep learning architecture \nfor air quality predictions. Environ Sci Pollut Res 23(22):22408–\n22417. https:// doi. org/ 10. 1007/ s11356- 016- 7812-9\nLiang X, Zou T, Guo B, Li S, Zhang H, Zhang S, Huang H, Chen SX \n(2015) Assessing Beijing’s PM2.5 pollution: severity, weather \nimpact, APEC and winter heating. Proc R Soc a: Math Phys Eng \nSci 471(2182):20150257. https:// doi. org/ 10. 1098/ rspa. 2015. 0257\n13549International Journal of Environmental Science and Technology (2023) 20:13535–13550 \n1 3\nLiao X, Tu H, Maddock JE, Fan S, Lan G, Wu Y, Yuan ZK, Lu Y \n(2015) Residents’ perception of air quality, pollution sources, \nand air pollution control in Nanchang, China. Atmos Pollut Res \n6(5):835–841. https:// doi. org/ 10. 5094/ APR. 2015. 092\nLiu H, Zhang X (2021) AQI time series prediction based on a hybrid \ndata decomposition and echo state networks. Environ Sci and Pol-\nlut Res. https:// doi. org/ 10. 1007/ s11356- 021- 14186-w\nLiu H, Yan G, Duan Z, Chen C (2021) Intelligent modeling strategies \nfor forecasting air quality time series: a review. Appl Soft Comput \n102:106957. https:// doi. org/ 10. 1016/j. asoc. 2020. 106957\nLuo Z, Huang F, Liu H (2020) PM2.5 concentration estimation using \nconvolutional neural network and gradient boosting machine. J \nEnviron Sci 98:85–93. https:// doi. org/ 10. 1016/j. jes. 2020. 04. 042\nMa J, Cheng JCP, Lin C, Tan Y, Zhang J (2019) Improving air qual -\nity prediction accuracy at larger temporal resolutions using \ndeep learning and transfer learning techniques. Atmos Environ \n214:116885. https:// doi. org/ 10. 1016/j. atmos env. 2019. 116885\nMao W, Wang W, Jiao L, Zhao S, Liu A (2021) Modeling air quality \nprediction using a deep learning approach: Method optimization \nand evaluation. Sustain Cities Soc 65:102567. https:// doi. org/ 10. \n1016/j. scs. 2020. 102567\nMihailovic DT, Alapaty K, Podrascanin Z (2009) Chemical transport \nmodels. Environ Sci Pollut Res 16(2):144–151. https:// doi. org/ 10. \n1007/ s11356- 008- 0086-0\nNeishi M, Yoshinaga N (2019) On the relation between position \ninformation and sentence length in neural machine translation. \nIn: Proceedings of the 23rd conference on computational natural \nlanguage learning (CoNLL), Hong Kong, China, pp 328–338\nPonomarev N, Elansky N, Kirsanov A, Postylyakov O, Borovski A, \nVerevkin YM (2020) Application of atmospheric chemical trans-\nport models to validation of pollutant emissions in Moscow. \nAtmos Ocean Opt 33(4):362–371. https:// doi. org/ 10. 1134/ S1024 \n85602 00400 90\nPowers JG, Klemp JB, Skamarock WC, Davis CA, Dudhia J, Gill \nDO, Coen JL, Gochis DJ, Ahmadov R, Peckham SE (2017) The \nweather research and forecasting model: overview, system efforts, \nand future directions. Bull Am Meteorol Soc 98(8):1717–1737. \nhttps:// doi. org/ 10. 1175/ BAMS-D- 15- 00308.1\nSchwartz J (1993) Particulate air pollution and chronic respiratory \ndisease. Environ Res 62(1):7–13. https://  doi. org/ 10. 1006/ enrs. \n1993. 1083\nVaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, \nKaiser Ł, Polosukhin I (2017) Attention is all you need. In: \nAdvances in neural information processing systems, Long Beach, \npp 5998–6008\nVoukantsis D, Karatzas K, Kukkonen J, Räsänen T, Karppinen A, \nKolehmainen M (2011) Intercomparison of air quality data using \nprincipal component analysis, and forecasting of PM10 and \nPM2.5 concentrations using artificial neural networks, in Thessa-\nloniki and Helsinki. Sci Total Environ 409(7):1266–1276. https:// \ndoi. org/ 10. 1016/j. scito tenv. 2010. 12. 039\nWang Z, Maeda T, Hayashi M, Hsiao L-F, Liu K-Y (2001) A nested air \nquality prediction modeling system for urban and regional scales: \napplication for high-ozone episode in Taiwan. Water Air Soil Pol-\nlut 130(1):391–396. https:// doi. org/ 10. 1023/A: 10138 33217 916\nWang Z, Chen L, Zhu J, Chen H, Yuan H (2020) Double decompo-\nsition and optimal combination ensemble learning approach for \ninterval-valued AQI forecasting using streaming data. Environ \nSci Pollut Res 27(30):37802–37817. https:// doi. org/ 10. 1007/  \ns11356- 020- 09891-x\nWang Y, Yuan Q, Li T, Zhu L (2022a) Global spatiotemporal estima-\ntion of daily high-resolution surface carbon monoxide concentra-\ntions using Deep Forest. J Clean Prod 350:131500. https:// doi. org/ \n10. 1016/j. jclep ro. 2022. 131500\nWang Y, Yuan Q, Zhu L, Zhang L (2022b) Spatiotemporal estimation \nof hourly 2-km ground-level ozone over China based on Hima-\nwari-8 using a self-adaptive geospatially local model. Geosci \nFront 13(1):101286. https:// doi. org/ 10. 1016/j. gsf. 2021. 101286\nWen C, Liu S, Yao X, Peng L, Li X, Hu Y, Chi T (2019) A novel spa-\ntiotemporal convolutional long short-term neural network for air \npollution prediction. Sci Total Environ 654:1091–1099. https://  \ndoi. org/ 10. 1016/j. scito tenv. 2018. 11. 086\nWong P-Y, Lee H-Y, Chen Y-C, Zeng Y-T, Chern Y-R, Chen N-T, \nCandice Lung S-C, Su H-J, Wu C-D (2021) Using a land use \nregression model with machine learning to estimate ground level \nPM2.5. Environ Pollut 277:116846. https:// doi. org/ 10. 1016/j.  \nenvpol. 2021. 116846\nXu Y, Du P, Wang J (2017) Research and application of a hybrid model \nbased on dynamic fuzzy synthetic evaluation for establishing air \nquality forecasting and early warning system: a case study in \nChina. Environ Pollut 223:435–448. https:// doi. org/ 10. 1016/j.  \nenvpol. 2017. 01. 043\nYan X, Zang Z, Luo N, Jiang Y, Li Z (2020) New interpretable deep \nlearning model to monitor real-time PM2.5 concentrations from \nsatellite data. Environ Int 144:106060. https:// doi. org/ 10. 1016/j. \nenvint. 2020. 106060\nYang W, Deng M, Xu F, Wang H (2018) Prediction of hourly PM2.5 \nusing a space-time support vector regression model. Atmos Envi-\nron 181:12–19. https:// doi. org/ 10. 1016/j. atmos env. 2018. 03. 015\nYang M et al (2020) Is PM1 similar to PM2.5? A new insight into \nthe association of PM1 and PM2.5 with children’s lung function. \nEnviron Int 145:106092. https:// doi. org/ 10. 1016/j. envint. 2020. \n106092\nYang J, Yan R, Nong M, Liao J, Li F, Sun W (2021) PM2.5 concentra-\ntions forecasting in Beijing through deep learning with differ -\nent inputs, model structures and forecast time. Atmos Pollut Res \n12(9):101168. https:// doi. org/ 10. 1016/j. apr. 2021. 101168\nYi L, Mengfan T, Kun Y, Yu Z, Xiaolu Z, Miao Z, Yan S (2019) \nResearch on PM2.5 estimation and prediction method and chang-\ning characteristics analysis under long temporal and large spatial \nscale—a case study in China typical regions. Sci Total Environ \n696:133983. https:// doi. org/ 10. 1016/j. scito tenv. 2019. 133983\nYue Z, Witzig CR, Jorde D, Jacobsen H-A (2020) BERT4NILM: a \nbidirectional transformer model for non-intrusive load monitor -\ning. In: Proceedings of the 5th International Workshop on Non-\nIntrusive Load Monitoring, New York, pp 89–93\nZeyer A, Bahar P, Irie K, Schlüter R, Ney H (2019) A comparison of \ntransformer and LSTM encoder decoder models for ASR. In: 2019 \nIEEE automatic speech recognition and understanding workshop \n(ASRU), Singapore, pp 8–15\nZhang H, Chen G, Hu J, Chen S-H, Wiedinmyer C, Kleeman M, Ying \nQ (2014) Evaluation of a seven-year air quality simulation using \nthe Weather Research and Forecasting (WRF)/Community Mul-\ntiscale Air Quality (CMAQ) models in the eastern United States. \nSci Total Environ 473:275–285. https:// doi. org/ 10. 1016/j. scito \ntenv. 2013. 11. 121\nZhang C, Yan J, Li C, Rui X, Liu L, Bie R (2016) On estimating air \npollution from photos using convolutional neural network. In: \nProceedings of the 24th ACM international conference on Multi-\nmedia, Amsterdam, pp 297–301\nZhang B, Zhang H, Zhao G, Lian J (2020a) Constructing a PM2.5 \nconcentration prediction model by combining auto-encoder with \nBi-LSTM neural networks. Environ Model Softw 124:104600. \nhttps:// doi. org/ 10. 1016/j. envso ft. 2019. 104600\nZhang F, Shi Y, Fang D, Ma G, Nie C, Krafft T, He L, Wang Y \n(2020b) Monitoring history and change trends of ambient air \n13550 International Journal of Environmental Science and Technology (2023) 20:13535–13550\n1 3\nquality in China during the past four decades. J Environ Manage \n260:110031. https:// doi. org/ 10. 1016/j. jenvm an. 2019. 110031\nZhang Z, Zeng Y, Yan K (2021) A hybrid deep learning tech-\nnology for PM2.5 air quality forecasting. Environ Sci Pol-\nlut Res 28(29):39409–39422. https://  doi. org/ 10. 1007/  \ns11356- 021- 12657-8\nZhang L, Xu L, Jiang M, He P (2022) A novel hybrid ensemble model \nfor hourly PM2.5 concentration forecasting. Int J EnvironSci \nTechnol. https:// doi. org/ 10. 1007/ s13762- 022- 03940-3\nZhao Z, Qin J, He Z, Li H, Yang Y, Zhang R (2020) Combining forward \nwith recurrent neural networks for hourly air quality prediction in \nNorthwest of China. Environ Sci Pollut Res 27(23):28931–28948. \nhttps:// doi. org/ 10. 1007/ s11356- 020- 08948-1\nZheng Y, Yi X, Li M, Li R, Shan Z, Chang E, Li T (2015) Forecast-\ning fine-grained air quality based on big data. In: Proceedings of \nthe 21th ACM SIGKDD international conference on knowledge \ndiscovery and data mining, Sydney, pp 2267–2276\nZhou H, Zhang S, Peng J, Zhang S, Li J, Xiong H, Zhang W (2021): \nInformer: beyond efficient transformer for long sequence time-\nseries forecasting. In: Proceedings of AAAI, pp 11106–11115\nZhou H, Zhang F, Du Z, Liu R (2022) A theory-guided graph networks \nbased PM2.5 forecasting method. Environ Pollut 293:118569. \nhttps:// doi. org/ 10. 1016/j. envpol. 2021. 118569\nZhu S, Yang L, Wang W, Liu X, Lu M, Shen X (2018) Optimal-com-\nbined model for air quality index forecasting: 5 cities in North \nChina. Environ Pollut 243:842–850. https:// doi. org/ 10. 1016/j.  \nenvpol. 2018. 09. 025",
  "topic": "Air quality index",
  "concepts": [
    {
      "name": "Air quality index",
      "score": 0.6970138549804688
    },
    {
      "name": "Beijing",
      "score": 0.6807198524475098
    },
    {
      "name": "Computer science",
      "score": 0.6522033214569092
    },
    {
      "name": "Term (time)",
      "score": 0.5745052695274353
    },
    {
      "name": "Data mining",
      "score": 0.47246408462524414
    },
    {
      "name": "Artificial intelligence",
      "score": 0.466880738735199
    },
    {
      "name": "Machine learning",
      "score": 0.439369797706604
    },
    {
      "name": "Deep learning",
      "score": 0.41210564970970154
    },
    {
      "name": "Mean squared error",
      "score": 0.4104680120944977
    },
    {
      "name": "Statistics",
      "score": 0.16915497183799744
    },
    {
      "name": "Mathematics",
      "score": 0.14806219935417175
    },
    {
      "name": "China",
      "score": 0.12355473637580872
    },
    {
      "name": "Meteorology",
      "score": 0.0840630829334259
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I82760581",
      "name": "Taizhou University",
      "country": "CN"
    }
  ],
  "cited_by": 51
}