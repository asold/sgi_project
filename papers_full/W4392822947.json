{
  "title": "A Hybrid Transformer-LSTM Model With 3D Separable Convolution for Video Prediction",
  "url": "https://openalex.org/W4392822947",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5029079165",
      "name": "Mareeta Mathai",
      "affiliations": [
        "Santa Clara University"
      ]
    },
    {
      "id": "https://openalex.org/A5100414451",
      "name": "Ying Liu",
      "affiliations": [
        "Santa Clara University"
      ]
    },
    {
      "id": "https://openalex.org/A5018686979",
      "name": "Nam Ling",
      "affiliations": [
        "Santa Clara University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3167995751",
    "https://openalex.org/W3015256491",
    "https://openalex.org/W2963610939",
    "https://openalex.org/W3016155278",
    "https://openalex.org/W3015023262",
    "https://openalex.org/W2963109922",
    "https://openalex.org/W3024526820",
    "https://openalex.org/W4309591648",
    "https://openalex.org/W2883991972",
    "https://openalex.org/W2894961607",
    "https://openalex.org/W6677326919",
    "https://openalex.org/W2962839378",
    "https://openalex.org/W6628877408",
    "https://openalex.org/W6712884540",
    "https://openalex.org/W6757613341",
    "https://openalex.org/W6745829810",
    "https://openalex.org/W2967033144",
    "https://openalex.org/W6771200186",
    "https://openalex.org/W3138340468",
    "https://openalex.org/W4312804153",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3204976424",
    "https://openalex.org/W3196890755",
    "https://openalex.org/W4285110782",
    "https://openalex.org/W4383751689",
    "https://openalex.org/W6786361841",
    "https://openalex.org/W6846539651",
    "https://openalex.org/W4312305807",
    "https://openalex.org/W6810308439",
    "https://openalex.org/W3214897310",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2591342762",
    "https://openalex.org/W3194242486",
    "https://openalex.org/W2146395539",
    "https://openalex.org/W3202918664",
    "https://openalex.org/W6739112683",
    "https://openalex.org/W6750259706",
    "https://openalex.org/W6774413459",
    "https://openalex.org/W3194409728",
    "https://openalex.org/W3166105568",
    "https://openalex.org/W6691096134",
    "https://openalex.org/W2963444790",
    "https://openalex.org/W2966687987",
    "https://openalex.org/W4385695861",
    "https://openalex.org/W6737664043",
    "https://openalex.org/W2989579525",
    "https://openalex.org/W3090739904",
    "https://openalex.org/W3204882374",
    "https://openalex.org/W3094911310",
    "https://openalex.org/W4206169575",
    "https://openalex.org/W3208596572",
    "https://openalex.org/W4249279051",
    "https://openalex.org/W2101032778",
    "https://openalex.org/W2115579991",
    "https://openalex.org/W2107775979",
    "https://openalex.org/W4205701977",
    "https://openalex.org/W3153143402",
    "https://openalex.org/W2970378723",
    "https://openalex.org/W4226345037",
    "https://openalex.org/W4309384924",
    "https://openalex.org/W3109635183",
    "https://openalex.org/W4297775537"
  ],
  "abstract": "Video prediction is an essential vision task due to its wide applications in real-world scenarios. However, it is indeed challenging due to the inherent uncertainty and complex spatiotemporal dynamics of video content. Several state-of-the-art deep learning methods have achieved superior video prediction accuracy at the expense of huge computational cost. Hence, they are not suitable for devices with limitations in memory and computational resource. In the light of Green Artificial Intelligence (AI), more environment friendly deep learning solutions are desired to tackle the problem of large models and computational cost. In this work, we propose a novel video prediction network 3DTransLSTM, which adopts a hybrid transformer-long short-term memory (LSTM) structure to inherit the merits of both self-attention and recurrence. Three-dimensional (3D) depthwise separable convolutions are used in this hybrid structure to extract spatiotemporal features, meanwhile enhancing model efficiency. We conducted experimental studies on four popular video prediction datasets. Compared to existing methods, our proposed 3DTransLSTM achieved competitive frame prediction accuracy with significantly reduced model size, trainable parameters, and computational complexity. Moreover, we demonstrate the generalization ability of the proposed model by testing the model on dataset completely unseen in the training data.",
  "full_text": "Received 11 January 2024, accepted 2 March 2024, date of publication 14 March 2024, date of current version 20 March 2024.\nDigital Object Identifier 10.1 109/ACCESS.2024.3375365\nA Hybrid Transformer-LSTM Model With 3D\nSeparable Convolution for Video Prediction\nMAREETA MATHAI\n , (Graduate Student Member, IEEE), YING LIU\n, (Member, IEEE),\nAND NAM LING\n , (Life Fellow, IEEE)\nDepartment of Computer Science and Engineering, Santa Clara University, Santa Clara, CA 95053, USA\nCorresponding author: Mareeta Mathai (mmathai@scu.edu)\nThis work was supported in part by the National Science Foundation under Grant ECCS-2138635.\nABSTRACT Video prediction is an essential vision task due to its wide applications in real-world scenarios.\nHowever, it is indeed challenging due to the inherent uncertainty and complex spatiotemporal dynamics\nof video content. Several state-of-the-art deep learning methods have achieved superior video prediction\naccuracy at the expense of huge computational cost. Hence, they are not suitable for devices with limitations\nin memory and computational resource. In the light of Green Artificial Intelligence (AI), more environment\nfriendly deep learning solutions are desired to tackle the problem of large models and computational\ncost. In this work, we propose a novel video prediction network 3DTransLSTM, which adopts a hybrid\ntransformer-long short-term memory (LSTM) structure to inherit the merits of both self-attention and\nrecurrence. Three-dimensional (3D) depthwise separable convolutions are used in this hybrid structure to\nextract spatiotemporal features, meanwhile enhancing model efficiency. We conducted experimental studies\non four popular video prediction datasets. Compared to existing methods, our proposed 3DTransLSTM\nachieved competitive frame prediction accuracy with significantly reduced model size, trainable parameters,\nand computational complexity. Moreover, we demonstrate the generalization ability of the proposed model\nby testing the model on dataset completely unseen in the training data.\nINDEX TERMS 3D separable convolution, deep learning, depthwise convolution, LSTM, pointwise\nconvolution, self-attention, spatiotemporal modeling, transformer, visual communications, video prediction.\nI. INTRODUCTION\nWith the increasing advent of powerful graphics processing\nunits (GPUs), deep learning is the foremost option of many\nartificial intelligence (AI) applications and it has been a\ncrucial part in the advancement of many computer vision\n(CV) algorithms. In this work, we focus on the task of\nvideo frame prediction. A video frame prediction model\ngenerates future frames from past frames, by learning the\ncomplex spatiotemporal content and dynamics of the video\ndata. It finds a wide range of real-world applications such\nas video coding [1], autonomous vehicles [2] and anomaly\ndetection [3].\nMany existing video prediction models are based on\n2-dimensional convolutional neural networks (2D CNNs)\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Dian Tjondronegoro\n.\n[4], [5], [6], [7], [8], 3-dimensional (3D) CNNs [9], [10],\nor recurrent neural networks (RNNs) [11], [12]. While CNNs\ncan extract local features, RNNs are specifically used to\nlearn sequential representations. To benefit from both CNNs\nand RNNs, other approaches [13], [14], [15], [16], [17],\n[18], [19], [20] proposed to combine CNN and RNN and\nlearn spatiotemporal features from video data. Among these\nworks, many adopted the long short-term memory (LSTM) as\ntheir RNN structure, which led to the family of ConvLSTM\nmodels.\nTransformers which have primarily demonstrated success\nin natural language processing (NLP) [21] and several\nvision tasks [22], [23], [24], [25] were also recently utilized\nfor video prediction [26], [27], [28]. Transformer models\nare capable of capturing long-range dynamics without\nthe vanishing gradient problem of recurrent networks and\nhave the advantage of parallelism with the self-attention\nVOLUME 12, 2024\n\n 2024 The Authors. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ 39589\nM. Mathai et al.: Hybrid Transformer-LSTM Model With 3D Separable Convolution\nmechanism [21]. However, the accuracy of transformer\nmodels usually comes at the price of huge computational\ncost [29].\nRecently, researchers across the NLP and CV commu-\nnities [30], [31] advocated to shift the focus to Green AI\nwith energy-efficient deep learning solutions, rather than\ncontinuously pushing red AI methods to reach state-of-the-\nart (SOTA) results using massive computational power. Our\nwork focuses on developing an efficient video frame predic-\ntion model with reduced model size, fewer parameters, and\nlow computational complexity, while achieving competitive\nprediction accuracy.\nSince both transformer and ConvLSTM models have\nachieved superior accuracy in predictive learning, in this\nwork, we propose a hybrid transformer-LSTM\n(3DTransLSTM) model to predict future video frames.\nTo learn spatiotemporal dynamics from video data, the\nproposed 3DTransLSTM network adopted 3D separable\nconvolutions to extract features along the temporal, height,\nand width dimensions. The 3D separable convolutions not\nonly offer higher prediction accuracy than 2D convolutions,\nbut also reduce the computational complexity compared to\nstandard 3D convolutions. Our main contributions can be\nsummarized as follows:\n• For the first time in the literature, we proposed a hybrid\ntransformer-LSTM (3DTransLSTM) network for the\nvideo prediction task. On the one hand, the transformer\nmodule can leverage long-range correlations among\nmultiple successive video frames, and parallelize the\ncomputation with its self-attention mechanism. On the\nother hand, the LSTM module can enable spatiotempo-\nral information flow vertically within each time step and\nhorizontally among multiple time steps.\n• The proposed 3DTransLSTM adopts 3D convolutions\nto effectively learn spatiotemporal dynamics. To reduce\ncomputational cost, the standard 3D convolution is\ndecomposed into a 3D depthwise convolution and\na pointwise convolution, which reduced the model\nsize, trainable parameters, and floating-point operations\n(FLOPs). To the best of our knowledge, this is the first\ntime that 3D separable convolution is utilized in a hybrid\ntransformer-LSTM network.\n• Qualitative and quantitative experimental results on\npopular video prediction datasets show that, compared\nto SOTA methods, the proposed model achieves compet-\nitive video frame prediction accuracy with significantly\nsmaller model size, fewer model parameters, and\nless computational cost. Further, we demonstrated the\ngeneralization ability of the proposed model by testing\nthe model on video sequences completely unseen in the\ntraining dataset.\nThe remaining of the paper is organized as follows.\nIn Section II, we discuss existing video frame prediction\nmethods and explain the motivation behind our proposed\nmodel. In Section III, we provide the preliminaries on 3D\ndepthwise separable convolutions. Section IV elaborates on\nour proposed 3DTransLSTM model in detail. Section V\npresents experiments on four video prediction datasets and\ncomparison studies with prior arts. Section VI concludes the\npaper and discusses future research directions.\nII. RELATED WORK\nA. VIDEO PREDICTION METHODS\nExisting video prediction methods can be broadly classified\ninto four categories: CNN-based methods, RNN-based meth-\nods, generative adversarial network (GAN)-based methods,\nand transformer-based methods.\nMany 2D CNN-based video prediction approaches [4], [5],\n[32], [33] were devised to model spatiotemporal dynamics in\nvideo data. In [4], the content encoder and motion encoder\nfocused on the static scene information and the temporal\ndynamics of consecutive frames, respectively. In [32],\na convolutional encoder-decoder network was proposed to\nexplicitly incorporate a time-related input variable to model\ntemporal correlations. Deformable convolutions were used to\nfuse features from previous frames in [33].\nCertain 2D CNN-based frame prediction schemes were\nproposed for inter-frame prediction [6], [7], [8] in traditional\nvideo coding such as the high efficiency video coding\n(HEVC) [34] and versatial video coding (VVC) [35], or in\nlearning-based video coding. For example, a 2D CNN-\nbased deep network was proposed for both uni-directional\nand bi-directional frame prediction in HEVC and avoided\ncoding additional motion information [6]. Another CNN-\nbased multi-resolution video prediction network (VPN) [7]\nutilized two sub-VPN architectures in cascade to generate\nvirtual reference frame from previously coded frames in\nHEVC. Further, recurrent and bi-directional in-loop predic-\ntion modules were proposed in [8] as part of a deep learning-\nbased video compression system.\n3D CNN is another way to extract spatiotemporal features.\nIt was used along with optical flow images to predict future\nframes based on a single image in [9]. Spatially displaced\nconvolution network (SDC-Net) [10] utilized a 3D CNN for\nvideo prediction, conditioning on both past frames and past\noptical flows.\nUsing CNNs alone can only take into account local\nstructures or short-range dependencies in video data due to\nthe limited size of convolution kernels. To effectively capture\nlong-range correlations in a video sequence, methods based\non RNNs [11], [12] were proposed to predict future frames.\nFor example, an LSTM-based encoder-decoder network was\ndeveloped in [11]. It used an encoder LSTM to map an\ninput video sequence into a fixed length representation, which\nwas then decoded using single or multiple decoder LSTMs\nto predict future frames. Folded recurrent neural network\n(FRNN) [12] presented a recurrent auto-encoder with state\nsharing between the encoder and the decoder. It utilized\nstacked double-mapping gated recurrent unit (GRU) layers to\nenable bidirectional information flow between the input and\nthe output.\n39590 VOLUME 12, 2024\nM. Mathai et al.: Hybrid Transformer-LSTM Model With 3D Separable Convolution\nAlthough RNNs effectively learn sequential representa-\ntions, they fail to accurately learn spatial content [13].\nTo address this issue, convolutions were incorporated into\nLSTMs to form ConvLSTMs [13], where the internal\nfully connections in LSTM were replaced by convolution\noperations. For example, ConvLSTM was utilized in [4]\nfor motion prediction, and was used in [5] to generate\nappearance features of the next frame from two previous\nframes. In addition, the stacked ConvLSTM architecture\nwas explored in the dynamic neural advection (DNA)\nmodule [14], which predicted the distribution of each pixel\nin the current frame based on the previous frame. E3D-\nLSTM [15] integrated 3D convolutions into LSTM to capture\nshort-term frame dependencies and utilized a gate-controlled\nself-attention module to perceive long-term correlations. The\nPredRNN [16] network proposed the popular spatiotem-\nporal LSTM (ST-LSTM) structure. It adopted a temporal\nmemory cell and a novel spatiotemporal memory cell to\nsimultaneously memorize spatial and temporal information.\nLater on, several video prediction methods adopted ST-\nLSTM as their building blocks [17], [18], [19], [20].\nFor example, the memory-in-memory (MIM) [17] model\nimproved PredRNN [16] by replacing the simple forget\ngate in the ST-LSTM block with two cascaded memory\ntransitions, which more effectively captured non-stationary\ndynamics. CrevNet [18] used a reversible auto-encoder and\nstacked ST-LSTM blocks for future frame prediction and\nobject detection. PredRNN-V2 [19] improved PredRNN [16]\nby introducing a memory decoupling loss to ST-LSTM\nto keep the memory cells from learning redundant fea-\ntures. Other ConvLSTM or convolutional GRU (ConvGRU)\napproaches such as TrajGRU [36], PredRNN++ [37], Conv-\nTTLSTM [38], STGRU [39] and ASTM [40] were also\ndeveloped for the video prediction task.\nDue to the mean-squared error (MSE) loss adopted in\nmodel training, CNN-based video prediction models tend to\ngenerate blurry predicted frames which are inconsistent with\nhuman perception. To overcome this limitation, GAN-based\nmodels adopt adversarial training such that the predicted\nframes are sharper and present more details than pure CNN-\nbased methods. BeyondMSE [41] was the pioneer in applying\nadversarial training for video prediction. It used a multi-\nscale architecture and an image gradient difference loss along\nwith the MSE loss. Dual-Motion GAN (DM-GAN) [42]\nused a dual adversarial training mechanism with two pairs\nof generator and discriminator to generate future frames\nand future flows simultaneously. CycleGAN [43] adopted a\nforward-backward prediction scheme by training a generator\nto produce both future frames and past frames. Attention-\nbased inter-frame prediction method in [44] enhanced\ncoding efficiency of VVC by incorporating GAN-based\ndeep attention map estimation and deep frame interpolation\nmethods.\nIn recent years, transformers have been developed for\nNLP and CV tasks. Compared with RNN-based methods, the\ntransformer architecture can extract long-term dependencies\nmore efficiently and get rid of the limitation of seriality.\nIn particular, a few approaches combined transformers and\nCNNs for video frame prediction. For instance, ConvTrans-\nformer [26] used an end-to-end encoder-decoder transformer\narchitecture for video interpolation and extrapolation tasks.\nIt proposed multi-head convolutional self-attention layers\nwith 2D convolutions in both the encoder and decoder.\nThe temporal convolutional transformer network (TCTN)\n[27] used a transformer-based encoder for video prediction,\nwhere 3D convolutional layers were employed to extract\nshort-term dependencies and masked self-attention layers\nwere used to capture long-term dependencies. The video\nprediction transformer (VPTR) [28] proposed to separately\nperform spatial attention and temporal attention. First, spatial\nattention was performed locally on each feature patch\nusing multi-head self-attention (MHSA), followed by a 2D\nseparable convolution-based feed-forward neural network.\nAfterwards, a temporal MHSA was adopted to model the\ntemporal dependency between frames.\nB. MOTIVATION OF THE PROPOSED METHOD\nAlthough the aforementioned SOTA methods achieved\naccurate video prediction results, their accuracy comes at a\nprice of big model size, large amount of model parameters\nand heavy computational complexity. For example, the\ntransformer-based models TCTN [27] and VPTR [28]\nhave large model size and FLOPs due to standard 3D\nconvolutions and complicated attention mechanisms, respec-\ntively. The ConvLSTM-based models E3D-LSTM [15]\nand CrevNet [18] adopted standard 3D convolutions too.\nMIM [17] also has relatively larger model size and FLOPs\nsince it adopted additional memory modules inside the\noriginal ST-LSTM blocks.\nIn this work, we aim at developing a lightweight video\nprediction network which still offers competitive frame\nprediction accuracy. To achieve this goal, we design a hybrid\narchitecture that benefits from both transformer and LSTM\nstructures. This is the first time in the literature that such\nhybrid structure is proposed for video frame prediction.\nBesides, while existing video prediction networks adopt 2D\nconvolutions [4], [5], [6], [8], [19], [26], [28], [32], [33] or\nstandard 3D convolutions [15], [27], our proposed network\nadopts the idea of 3D separable convolution. Separable\nconvolution was first conceived in MobileNet [45] to develop\nlightweight models suitable for mobile and embedded\ndevices. Later on, 2D separable CNN was utilized for faster\nvideo segmentation [46], moving object detection [47], and\nviolence detection [48]. To alleviate the computation burden\nof standard 3D convolution in deep networks, 3D separable\nCNN was proposed for dynamic hand gesture recognition and\nvideo moving object segmentation [49], [50], [51]. In our\npreliminary work [20], 3D separable convolution-based ST-\nLSTM was developed with a reversible architecture for\nvideo frame prediction. In this work, we propose to use\n3D separable convolutions in a hybrid transformer-LSTM\nVOLUME 12, 2024 39591\nM. Mathai et al.: Hybrid Transformer-LSTM Model With 3D Separable Convolution\nnetwork. This not only leverages spatiotemporal correlations,\nbut also effectively reduces model size, trainable parameters,\nand computational cost.\nIII. PRELIMINARIES\nIn this section, we contrast the 2D, 2D separable, 3D and 3D\nseparable convolutions and explain the rationale behind the\nusage of depthwise separable convolutions.\nA. 2D CONVOLUTION VERSUS 2D SEPARABLE\nCONVOLUTION\nFor an RGB image of size C ×H ×W where C is the number\nof channels, H is the height and W is the width of the image,\n2D convolutions use N 3D filters of size C ×K ×K (channel\n× height × width) to convolve the image in the height and\nwidth directions to produce feature maps of size N × ˇH ×\nˇW . To reduce its computational complexity, a 2D convolution\ncan be separated into a 2D depthwise convolution and a 1D\npointwise convolution. In a 2D depthwise convolution, filters\nof size 1 ×K ×K (channel × height × width) convolve with\neach input channel to produce an intermediate output C ×\nˇH × ˇW . Afterwards, 1D pointwise convolutions convolve the\nintermediate output in the channel direction with N filters of\nsize C × 1 × 1 (channel × height × width), to generate the\nfinal output feature maps of size N × ˇH × ˇW .\nB. 3D CONVOLUTION VERSUS 3D SEPARABLE\nCONVOLUTION\nVideo prediction needs a model to learn spatiotemporal\ninformation abundantly and deeply. To achieve this goal, prior\narts adopted standard 3D convolutions [15], [18], [27]. Let\nXin ∈ RC×L×H×W be a 4D video tensor, where C, L, H, and\nW represent the channel, number of successive video frames,\nheight, and width. As illustrated in Fig. 1 (a), a standard 3D\nconvolution uses a 4D filter of size C × K × K × K (channel\n× time × height × width) which moves in three directions\n(time, height, width) to generate a 3D output tensor of size\nL′ × H′ × W ′. N such filters would create the final output\ntensor Xout ∈ RN×L′×H′×W ′\n. The number of floating-point\nmultiplications involved in such a standard 3D convolution is\nC × K × K × K × N × L′ × H′ × W ′.\nAlthough standard 3D convolution is amply used for\nvideo prediction, it has made the network architectures much\ncomplex and model sizes bigger. To reduce the computational\ncomplexity, a standard 3D convolution can be separated into\na 3D depthwise convolution and a pointwise convolution,\nwhich are combinedly termed as 3D depthwise separable\nconvolution. As shown in Fig. 1 (b) Step 1, the depthwise\nconvolution applies filters of size 1 × K × K × K to each\nof the C input channels to produce an intermediate output\nof size C × L′ × H′ × W ′. This process involves C × K ×\nK × K × L′ × H′ × W ′ multiplications. The intermediate\noutput goes through the pointwise convolution described in\nFig. 1 (b) Step 2, where filters of size C × 1 × 1 × 1 are\napplied along the channel direction to produce an output of\nsize 1 × L′ × H′ × W ′. The final 4D output tensor of size\nFIGURE 1. (a) The standard 3D convolution, and (b) the 3D depthwise\nseparable convolution.\nN ×L′ ×H′ ×W ′ is generated by applying N such pointwise\nfilters. Such a pointwise convolution involves C × N × L′ ×\nH′ × W ′ multiplications. To compare the computational cost\nof the 3D depthwise separable convolution with the standard\n3D convolution, we compute the ratio of the number of\nmultiplications involved in these two types of convolutions\nas\n3D depthwise separable convolution\nstandard 3D convolution\n=\nC × K × K × K × L′ × H′ × W ′+\nC × N × L′ × H′ × W ′\nC × K × K × K × N × L′ × H′ × W ′\n= 1\nN + 1\nK3 (1)\nTherefore, the decomposed convolution can reduce the\ncomputational cost of the standard 3D convolution by 1\nN + 1\nK3\nwhere N is the number of output channels and K is the filter\ndimension in time, height, and width.\nIV. PROPOSED METHOD\nIn this section, we elaborate our algorithm in detail. Fig. 2\nshows the overall framework of the proposed architecture\nfor three time steps t − 1, t, t + 1. The proposed video\nframe prediction network enables temporal information flow\nindicated by the four arrows connecting adjacent time steps.\nA. PROBLEM STATEMENT\nConsider the video prediction process at time step t. The\nnetwork takes a 4D tensor Xt ∈ RC×L×H×W as the input,\nwhich represents L successive video frames with frame\nindices t, t + 1, · · ·, t + L − 1, where C, H and W represent\nthe channel, height and width of each frame. The problem\nof video prediction is to predict the L frames ˆXt+1 ∈\nRC×L×H×W with frame indices t +1, t +2, · · ·, t +L given\n39592 VOLUME 12, 2024\nM. Mathai et al.: Hybrid Transformer-LSTM Model With 3D Separable Convolution\nXt and can be formulated as\nˆXt+1 = Net(Xt |θ), (2)\nwhere Net(·) represents the frame prediction network, and θ\nis the collection of trainable model parameters.\nB. ALGORITHM OVERVIEW\nAs shown in the middle column of Fig. 2, at time step t,\neach frame in the 4D video tensor Xt ∈ RC×L×H×W is\nspatially split into p × p patches, forming a tensor Pt ∈\nRCp2×L×H\np ×W\np , where H\np × W\np is the resulting number of\npatches, and Cp2 is the length of each flattened patch. Next,\nPt is processed by spatial embedding and positional encoding\nto generate an output tensor Zt ∈ Rdmodel×L×H\np ×W\np , where\ndmodel is the embedded dimension. Zt is then passed through\nsix 3DTransLSTM blocks, which leverage transformers and\nLSTM to extract spatiotemporal features and generate an\noutput tensor ˆZt+1 ∈ Rdmodel×L×H\np ×W\np . It is then processed by\nthe prediction head to generate the predicted patches ˆPt+1 ∈\nRCp2×L×H\np ×W\np , which is reshaped to form the final output\nframes ˆXt+1 ∈ RC×L×H×W . In the following subsections,\nwe elaborate the proposed network components in detail.\nC. SPATIAL EMBEDDING AND POSITIONAL ENCODING\nThe proposed spatial embedding module processes the input\npatches Pt ∈ RCp2×L×H\np ×W\np to produce the embedded feature\nmaps. It adopts two 2D depthwise separable convolutional\nlayers. As shown in (3), 2D depthwise separable convolution\nG is adopted to output intermediate feature map J1 ∈\nRdmodel×L×H\np ×W\np , followed by another 2D depthwise sepa-\nrable convolution S, which outputs J2 ∈ Rdmodel×L×H\np ×W\np .\nBoth G and S are applied frame by frame. J1 and J2 are then\nadded and passed through the Dropout layer to output the\nfinal embedded feature maps Embt ∈ Rdmodel×L×H\np ×W\np .\nJ1 = LeakyReLU\n(\nGCp2×1×1 ∗ (G1×7×7 ⊗ Pt )\n)\nJ2 = LeakyReLU\n(\nSdmodel×1×1 ∗ (S1×5×5 ⊗ J1)\n)\nEmbt = Dropout (J1 + J2) (3)\nIn (3), ⊗ is the 2D depthwise convolution operation, and ∗\nis the 1D pointwise convolution operation. G adopts a 2D\ndepthwise convolution with Cp2 filters of size 1 × 7 × 7\nfollowed by a 1D pointwise convolution with dmodel filters\nof size (Cp 2) × 1 × 1. Similarly, S adopts a 2D depthwise\nconvolution with dmodel filters of size 1 × 5 × 5 followed\nby a 1D pointwise convolution with dmodel filters of size\ndmodel × 1 × 1.\nTo preserve the positional information of the input video\nsequence, fixed positional encoding Pos ∈ Rdmodel×L×H\np ×W\np\nis calculated via (4) [26], where i represents the channel\nindex, 0 ≤ i < dmodel, l is the temporal index 0 ≤ l < L − 1,\n(h, w) are the spatial indices, and k = 104.\nPosi,l,h,w =\n\n\n\n\n\n\nsin\n(\nl\nk\ni\ndmodel\n)\n, i even,\ncos\n(\nl\nk\ni−1\ndmodel\n)\n, i odd.\n(4)\nPos is added to the embedded feature maps Embt to generate\nthe output Zt ∈ Rdmodel×L×H\np ×W\np by\nZt = Embt + Pos. (5)\nD. 3D TRANSFORMER-LSTM\nThe stack of six 3DTransLSTM blocks across three time\nsteps t − 1, t, t + 1 as shown in Fig. 2 are described in\ndetail in Fig. 3 (a). Take time step t as an example: the\npositioned feature maps Zt ∈ Rdmodel×L×H\np ×W\np pass through\nthese six 3DTransLSTM blocks to output ˆZt+1. In each\n3DTransLSTM block, the input is processed by two parallel\nbranches. Branch 1 is indicated by the blue arrow. It extracts\nfeatures within time step t through transformers, and outputs\nZout1\nt . Branch 2 enables information flow across three time\nsteps t −1, t, t +1 through ST-LSTM structures, and outputs\nZout2\nt . In the following we give detailed descriptions of\nBranch 1 and Branch 2.\nBranch 1: Its network structure is separately depicted\nin Fig. 4. It adopts two sub-blocks: 1) 3D separable\nconvolution-based self-attention (3DSepSA), and 2) 3D sepa-\nrable convolution-based feed-forward network (3DSepFFN).\nLayer normalization (LN ) is applied before each sub-block,\nand residual connection is applied after each sub-block.\n1) 3DSepSA\nAs shown in Fig. 4 (a), the input tensor Zt first goes through\na LN layer by\nZattn_in\nt = LN(Zt ). (6)\nAs shown in Fig. 4 (b), the 3DSepSA module then takes\nZattn_in\nt ∈ Rdmodel×L×H\np ×W\np as the input and calculates three\ntensors Q, K, V of dimension dmodel × L × H\np × W\np using 3D\nseparable convolutional kernels Wq, Wk , Wv as follows\nQ = W dmodel×1×1×1\nq ∗\n(\nW 1×3×3×3\nq ⊛ Zattn_in\nt\n)\n,\nK = W dmodel×1×1×1\nk ∗\n(\nW 1×3×3×3\nk ⊛ Zattn_in\nt\n)\n,\nV = W dmodel×1×1×1\nv ∗\n(\nW 1×3×3×3\nv ⊛ Zattn_in\nt\n)\n, (7)\nwhere ∗ denotes 1D pointwise convolution and ⊛ denotes 3D\ndepthwise convolution.\nTo calculate the attention of the above tensors,\nwe first transpose the tensors Q, K and V to dimension\nH\np × W\np × L × dmodel. Let H\np × W\np be the batch size, then the\nbatch has H\np × W\np samples, and each sample is a tensor of\nsize L × dmodel, denoted by Q ∈ RL×dmodel , K ∈ RL×dmodel ,\nVOLUME 12, 2024 39593\nM. Mathai et al.: Hybrid Transformer-LSTM Model With 3D Separable Convolution\nFIGURE 2. The overall architecture of the proposed network for three time stepst − 1, t, andt + 1.\nFIGURE 3. (a) The stack of six 3DTransLSTM blocks for time stepst − 1, t, t + 1. Each 3DTransLSTM block consists of Branch 1 (blue arrow) and Branch 2\n(green arrow). Branch 2 further consists of 4 3D separable convolution-based ST-LSTM (3D SepST-LSTM) layers. (b) The structure of thel-th 3D\nSepST-LSTM layer at time stept.\nand V ∈ RL×dmodel . Within each sample, the self-attention\nZ ∈ RL×dmodel among L temporal elements is calculated as\nZ = Softmax\n(\nMask( QKT\n√dmodel\n)\n)\nV. (8)\nWe use a single-head masked self-attention where the\nmasking mechanism only allows a position to look at the\nprevious tokens and prevents the leaking of information to\nfuture tokens [27]. We mask out the attention to future tokens\nby setting their attention scores to −∞, which generates zero\nweights after they are passed through the Softmax(·) function.\nNext, the self-attentions of all samples in the batch are\ngrouped and transposed to form the output self-attention\n˜Zattn\nt ∈ Rdmodel×L×H\np ×W\np , which is then processed by 3D\ndepthwise separable convolution WA and a Dropout layer as\nshown in (9),\nZattn\nt = Dropout\n(\nW dmodel×1×1×1\nA ∗ (W 1×3×3×3\nA ⊛ ˜Zattn\nt )\n)\n.\n(9)\nAfterwards, a residual connection from the input Zt is applied\nto Zattn\nt to produce the final output of the attention module\nZattn_out\nt as\nZattn_out\nt = Zattn\nt + Zt . (10)\n39594 VOLUME 12, 2024\nM. Mathai et al.: Hybrid Transformer-LSTM Model With 3D Separable Convolution\nFIGURE 4. (a) Branch 1 of the 3DTransLSTM block, (b)3DSepFFN module (top),3DSepSA\nmodule (bottom).\n2) 3DSepFFN\nThe second sub-block of Branch 1, 3DSepFFN, takes the\nlayer normalized Zattn_out\nt as the input,\nZff _in\nt = LN(Zattn_out\nt ). (11)\nAs shown in Fig. 4 (b), inside the 3DSepFFN sub-block, there\nare two 3D depthwise separable convolutional layers Wff 1 and\nWff 2, each followed by a Dropout layer,\nZff 1\nt = LeakyReLU\n(\nW dmodel×1×1×1\nff 1 ∗\n(W 1×3×3×3\nff 1 ⊛ Zff _in\nt )\n)\n,\nZff 2\nt = W dmodel×1×1×1\nff 2 ∗\n(\nW 1×3×3×3\nff 2 ⊛ Dropout(Zff 1\nt )\n)\n,\nZff _out\nt = Dropout\n(\nZff 2\nt\n)\n. (12)\nThe residual connection from the 3DSepSA sub-block\nZattn_out\nt ∈ Rdmodel×L×H\np ×W\np and the output from the\n3DSepFFN sub-block Zff _out\nt ∈ Rdmodel×L×H\np ×W\np are added\nto form the final output Zout1\nt ∈ Rdmodel×L×H\np ×W\np of Branch 1,\nZout1\nt = Zff _out\nt + Zattn_out\nt . (13)\nBranch 2: At time step t, Branch 2 shown by\nthe green arrow in Fig. 3 (a) enables spatiotemporal\ninformation flow. It contains 4 layers of 3D separable\nconvolution-based ST-LSTM (3D SepST-LSTM), and\noutputs Zout2\nt ∈ Rdmodel×L×H\np ×W\np .\nAs shown in Fig. 3 (a), at the l-th 3D SepST-LSTM layer,\nl = 1, 2, 3, 4, the inputs are the temporal memory cell\nCl\nt−1 and hidden state Hl\nt−1 delivered horizontally from the\nsame l-th layer of the previous time step t − 1, as well as\nthe spatiotemporal memory cell Ml−1\nt and the hidden state\nHl−1\nt delivered vertically from the (l − 1)-th 3D SepST-\nLSTM layer of the current time step t. The outputs are the\nspatiotemporal memory cell Ml\nt , the hidden state Hl\nt , and the\ntemporal memory cell Cl\nt . While Ml\nt is delivered vertically\nto the (l + 1)-th 3D SepST-LSTM layer, Cl\nt is delivered\nhorizontally to the (t + 1)-th time step, and Hl\nt is delivered\nboth vertically and horizontally. It is noteworthy that when\nl = 1, the input spatiotemporal memory cell M0\nt = M4\nt−1,\nwhich is the output spatiotemporal memory cell of the 4-th\n3D SepST-LSTM layer of the previous time step t − 1, and\nthe input hidden state H0\nt = Zt , which is the positional feature\ninput of the entire stack of six 3DTransLSTM blocks.\nAll the gates and the input Zt , hidden state Hl\nt and memory\ncells Ml\nt and Cl\nt are 4D tensors of size dmodel × L × H\np × W\np ,\nwhere dmodel is the number of channels, L is the temporal\nlength, H\np and W\np are the height and width.\nFig. 3 (b) shows the structure of the l-th 3D SepST-LSTM\nlayer at time step t. One set of input gate it , forget gate ft , and\ninput modulation gate gt are generated by processing hidden\nstates Hl−1\nt and Hl\nt−1 by 3D separable convolution as shown\nin (14a). Here ⊛ is the 3D depthwise convolution, ∗ is the 1D\npointwise convolution, σ is the sigmoid function and τ is the\ntanh function.\nit = σ\n(\nWxi ∗ (Wxi ⊛ Hl−1\nt ) + Whi ∗ (Whi ⊛ Hl\nt−1)\n)\nft = σ\n(\nWxf ∗ (Wxf ⊛ Hl−1\nt ) + Whf ∗ (Whf ⊛ Hl\nt−1)\n)\ngt = τ\n(\nWxg ∗ (Wxg ⊛ Hl−1\nt ) + Whg ∗ (Whg ⊛ Hl\nt−1)\n)\n(14a)\nAn extra set of input gate i′\nt , forget gate f ′\nt , and input\nmodulation gate g′\nt are generated by processing the hidden\nstate Hl−1\nt and spatiotemporal memory cell Ml−1\nt again using\nVOLUME 12, 2024 39595\nM. Mathai et al.: Hybrid Transformer-LSTM Model With 3D Separable Convolution\n3D separable convolution, as shown in (14b).\ni′\nt = σ\n(\nW ′\nxi ∗ (W ′\nxi ⊛ Hl−1\nt ) + Wmi ∗ (Wmi ⊛ Ml−1\nt )\n)\nf ′\nt = σ\n(\nW ′\nxf ∗ (W ′\nxf ⊛ Hl−1\nt ) + Wmf ∗ (Wmf ⊛ Ml−1\nt )\n)\ng′\nt = τ\n(\nW ′\nxg ∗ (W ′\nxg ⊛ Hl−1\nt ) + Wmg ∗ (Wmg ⊛ Ml−1\nt )\n)\n(14b)\nAfterwards, the output temporal memory cell Cl\nt and spa-\ntiotemporal memory cell Ml\nt are generated by (14c) and (14d),\nrespectively, where ⊚ represents the Hadamard product.\nCl\nt = ft ⊚ Cl\nt−1 + it ⊚ gt (14c)\nMl\nt = f ′\nt ⊚ Ml−1\nt + i′\nt ⊚ g′\nt (14d)\nThe output gate ot is generated by processing\nHl−1\nt , Hl\nt−1, Ml\nt , and Cl\nt again by 3D separable convolution,\nas shown in (14e).\not = σ\n(\nWxo ∗ (Wxo ⊛ Hl−1\nt ) + Who ∗ (Who ⊛ Hl\nt−1)\n+ Wmo ∗ (Wmo ⊛ Ml\nt ) + Wco ∗ (Wco ⊛ Cl\nt )\n)\n(14e)\nFinally, the hidden state Hl\nt is generated by\nHl\nt = ot ⊚ τ\n(\nW 2dmodel×1×1×1 ∗ [Cl\nt , Ml\nt ]\n)\n, (14f)\nwhere [C l\nt , Ml\nt ] is the channel concatenation of Cl\nt and\nMl\nt . In equations (14a), (14b), (14e), the depthwise 3D\nconvolution kernal size is set as 1 ×3×3×3, and the pointwise\nconvolution kernel size is set as dmodel × 1 × 1 × 1.\nBranch 1+ Branch 2:As shown in Fig. 3 (a), the output\nZout1\nt from Branch 1 and the output Zout2\nt from Branch 2\nare added together to form the output of one 3DTransLSTM\nblock. Six such 3DTransLSTM blocks are stacked to generate\nthe final output ˆZt+1 ∈ Rdmodel×L×H\np ×W\np .\nE. PREDICTION HEAD\nIn Fig. 2, the final output of the 3DTransLSTM blocks ˆZt+1 is\nprocessed by a prediction head using pointwise convolution\nWˆZ with Cp2 filters of size dmodel × 1 × 1 × 1 to output the\npredicted frame patches ˆPt+1 ∈ RCp2×L×H\np ×W\np as\nˆPt+1 = W dmodel×1×1×1\nˆZ ∗ ˆZt+1. (15)\nThese frame patches are reshaped to generate the L predicted\nframes ˆXt+1 ∈ RC×L×H×W with frame indices t + 1 : t + L.\nV. EXPERIMENTAL STUDIES\nIn this section, we demonstrate the effectiveness of the\nproposed method through experiments conducted on four\nvideo prediction datasets, including both synthetic and real-\nword datasets. We analyze the performance of the proposed\nmodel in terms of prediction accuracy and model efficiency,\nand compare it with SOTA methods.\nA. TRAINING AND INFERENCE STRATEGY\nTo train the network, we choose the widely used MSE as the\nloss function. The MSE between the ground-truth frame Yk\nwith time index k and the corresponding predicted frame ˆYk\nis calculated as follows:\nMSE = 1\nC × H × W\nC∑\nc=1\nH−1∑\ni=0\nW −1∑\nj=0\n(\nYk (c, i, j)\n− ˆYk (c, i, j)\n)2 (16)\nwhere C, H, and W denotes the number of channels, the\nheight and width of the frame, respectively.\nAs described in Section IV, in each time step, our proposed\nnetwork takes L frames as the input (for example, frame t\nto frame t + L − 1), and generates L predicted frames (for\nexample, frame t + 1 to frame t + L), with one time index\nshift. It is noteworthy that the video sequence length L can be\ndifferent during the training and inference stages, and L can\nalso vary during the inference stage, while the dimensions of\nthe trained filters are fixed.\nWe train our model Net to predict the next L frames ˆXt+1 =\n{ˆYt+1, ˆYt+2, · · ·, ˆYt+L } ∈RC×L×H×W by learning from the\nprevious L frames Xt = {Yt , Yt+1, · · ·, Yt+L−1} in three\nsuccessive iterations. In iteration 1, Xt is used to predict ˆXt+1.\nIn iteration 2, Xt+1 ∈ RC×L×H×W is used to predict ˆXt+2 ∈\nRC×L×H×W . In iteration 3, Xt+2 ∈ RC×L×H×W is used to\npredict ˆXt+3 ∈ RC×L×H×W . Since there are three iterations\nand each iteration has L predicted frames, the MSE loss used\nto train the network is averaged over 3L predicted frames.\nDuring inference, the model takes the previous K frames\nXt = {Y t , Yt+1, · · ·, Yt+K−1} ∈ RC×K×H×W as\nthe input, and predicts the future N frames ˆXt+K =\n{ˆYt+K , ˆYt+K+1, · · ·, ˆYt+K+N−1} ∈ RC×N×H×W . The\ninference process is given in Algorithm 1.\nAlgorithm 1The Inference Process\nInput: Xt = {Yt , Yt+1, · · ·, Yt+K−1}\nXin ← Xt\nfor i = 0, 1, 2, to N − 1 do\n{ˆYt+1, ˆYt+2, · · ·, ˆYt+K+i} ←Net(Xin)\nXin ← Concat(Xin, ˆYt+K+i)\nend for\nOutput: ˆXt+K = {ˆYt+K , ˆYt+K+1, · · ·, ˆYt+K+N−1}\nAs described in Algorithm 1, the inference process\nconsists of N iterations. In the i-th iteration, the last\npredicted frame ˆYt+K+i is concatenated with the current\ninput video sequence to form a new input sequence Xin ∈\nRC×(K+i+1)×H×W of K + i + 1 frames. After N iterations,\na total of N future frames are predicted, denoted as\nˆXt+K = {ˆYt+K , ˆYt+K+1, · · ·, ˆYt+K+N−1}.\nThe input and output sequence length L during training, the\ninput video frame number K and the predicted future frame\nnumber N during inference are provided for each dataset in\nSection V-B.\n39596 VOLUME 12, 2024\nM. Mathai et al.: Hybrid Transformer-LSTM Model With 3D Separable Convolution\nB. DATASETS\nWe conducted experimental studies on the synthetic Mov-\ningMNIST (MMNIST) dataset [13] and real-world datasets\nKTH Action [52] and Human 3.6m [53]. To evaluate the\ngeneralization ability of the proposed method, we also trained\nthe model on the KITTI [54] dataset and tested it on an unseen\nCaltech Pedestrian [55] dataset.\n1) MMNIST\nThis is a widely used synthetic grayscale video prediction\ndataset, with a frame size of 1 ×64 ×64. The first dimension\nrepresents the grayscale channel. Two digits continuously\nmove in the frame, initialized at a random location. The digits\nmove in a constant velocity and angle, bouncing off the edges\nof the frame. Following TCTN [27], we used two subsets\nof this dataset for training: MMNIST-2K with 2,000 video\nsequences and MMNIST-10K with 10,000 video sequences.\nEach sequence has 20 frames. We trained one model on each\nsubset and tested both trained models on another unseen\nMMNIST subset of 3,000 video sequences (MMNIST-3K).\nDuring training, the input and output sequence lengths were\nboth L = 17. During testing, we used K = 10 previous\nframes to predict N = 10 future frames.\n2) KTH ACTION\nKTH is a grayscale dataset originally used for action\nrecognition. It has video sequences of 25 individuals doing\nsix types of actions: walking, running, jogging, boxing,\nhand waving and clapping. Videos are shot with static\ncameras at 25 frames per second (fps) in four settings,\nnamely, outdoors, outdoors with scale variations, outdoors\nwith different clothes, and indoors. Individuals 1-16 were\nused for training and individuals 17-25 were used for testing.\nThe training set contained 8,488 sequences and the testing set\nhad 5,041 sequences. Each sequence had 20 frames, and each\nframe was resized to 1 × 64 × 64, where the first dimension\nrepresented the grayscale channel. Similar to the MMNIST\ndataset, during training, the input and output sequence length\nis L = 17 frames. During testing, the previous K = 10 frames\nwere used to predict N = 10 future frames.\n3) HUMAN 3.6m\nThis is a complex human pose action dataset, originally with\n3.6 million RGB images. It comprises of video sequences\nwith humans performing different types of actions. Each\nsequence has 8 frames. We followed the experiment setting\nin [19] and chose only the ‘‘walking’’ scenario. The original\n3 ×1000 ×1000 resolution frames were resized to 3 ×128 ×\n128 resolution in our experiments, where the first dimension\nrepresented the RGB channels. Subjects S1, S5, S6, S7, S8\nwere used for training which had 2,624 video sequences,\nand subjects S9, S11 were used for testing which had 1,135\nvideo sequences. For training we set the input and output\nsequence length as L = 5 and for testing we used the previous\nK = 4 frames to predict the future N = 4 frames.\n4) KITTI AND CALTECH PEDESTRIAN\nThe KITTI and Caltech Pedestrian datasets are two car-\nmounted camera video datasets with real-world scenarios,\nwidely used for video frame prediction. The KITTI dataset\nwas used for model training. It consisted of 3,150 sequences\nand each sequence had 13 frames. A subset of the Caltech\ndataset was used for model testing. This dataset was collected\nfrom a vehicle driving through regular traffic in an urban\nenvironment. It had 1,983 sequences and each sequence had\n11 frames. We followed [56] to preprocess the datasets. The\nframes from both datasets were center-cropped and resized\nto 3 × 128 × 160, where the first dimension represented the\nRGB channels. During training, we set the input and output\nsequence length to be L = 10. During testing, the previous\nK = 10 frames were used to predict the next N = 1 frames.\nC. EXPERIMENT SETTINGS\nThe models were trained with the PyTorch framework using\nan NVIDIA Tesla V100 32 GB GPU. The ADAM optimizer\nwas used to minimize the MSE loss between the ground-truth\nand the predicted frames. To prevent overfitting, dropout lay-\ners of the proposed network adopt a dropout rate of 0.05 dur-\ning training. The model was trained for 200 epochs for\nMMNIST and KTH Action, and 300 epochs for Human 3.6m\nand KITTI. We set the patch size as p = 4 and the hidden\ndimension as dmodel = 128 for MMNIST, KTH Action, and\nHuman 3.6m. For KITTI dataset, we set the patch size as p =\n2 and the hidden dimension as dmodel = 256. We used a stride\nof 1 for the depthwise separable convolutions in our network.\nD. METHODS IN COMPARISON\nWe compared the performance of our proposed model to\nseven existing methods. They included three transformer-\nbased models: ConvTransformer [26], TCTN [27], and\nVPTR [28], and four RNN-based models: FRNN [12], E3D-\nLSTM [15], MIM [17], and PredRNN-V2 [19]. All the\nmodels were implemented with the same experiment settings\nproposed in their original works. To verify the generalization\nability of our proposed model, we chose the following\nexisting methods for comparison studies: the ConvLSTM-\nbased models DNA [14], CrevNet [18], the GAN-based DM-\nGAN [42], the transformer-based VPTR [28], as well as the\nCNN-based BeyondMSE [41].\nE. EVALUATION METRICS\nThe quality of the predicted frame ˆYk compared to the\noriginal frame Yk , was evaluated using the peak signal-to-\nnoise ratio (PSNR) defined as\nPSNR = 10 log10\n2552\nMSE(Yk , ˆYk )\n. (17)\nBesides, the structural similarity index (SSIM) was also\ncalculated. It is a metric consistent with human subjective\nopinion and is defined as [57]:\nSSIM(Yk , ˆYk ) = l(Yk , ˆYk )α × c(Yk , ˆYk )β × g(Yk , ˆYk )γ ,\n(18)\nVOLUME 12, 2024 39597\nM. Mathai et al.: Hybrid Transformer-LSTM Model With 3D Separable Convolution\nTABLE 1. Quantitvative results on MMNIST-3K and KTH Action datasets. The best, second-best, and third-best results of each metric are highlighted in\nred, blue and brown, respectively.\nFIGURE 5. The visual results on the MMNIST-3K dataset[13] for models\ntrained on the MMNIST-10K dataset.\nwhere l, c, and g are the luminance, contrast and structure\ncomparison measures between Yk and ˆYk , and α, β and\nγ are parameters to define the relative importance of these\nthree components [57]. The final reported PSNR and SSIM\nwere averaged over all N predicted future frames. Higher\nPSNR and SSIM values indicate that the predicted frames\nhave higher quality.\nBesides the aforementioned accuracy metrics, the model\nefficiency is also evaluated by calculating the model size\nmeasured in megabytes (MB) and the number of trainable\nparameters measured in millions (M). We also use the\nevaluation metric - giga floating point operations (GFLOPs)\nto infer the computational complexity of the model. GFLOPs\ncalculate the total number of floating point operations, such\nas addition, subtraction, multiplication and division needed\nfor model inference. Lower GFLOPs usually indicate less\ncomputationally expensive models.\nF. EXPERIMENTAL RESULTS\n1) MMNIST\nThough the dynamics of MMNIST seem simple, the frequent\nocclusion and overlapping of the digits make the prediction\ncomplex. Table 1 compares the frame prediction accuracy\nperformance between the proposed model and existing\nmodels on the MMNIST-3K test set after the models were\ntrained on the MMNIST-2K and MMNIST-10K datasets.\nThe best, second-best, and third-best results of each metric\nare highlighted in red, blue and brown, respectively. Our\nproposed model 3DTransLSTM achieved the highest PSNR\nand SSIM scores when it was trained on MMNIST-2K,\nand it achieved the second highest PSNR and SSIM scores\nwhen it was trained on MMNIST-10K. It is superior in\nterms of model size and parameters, and it requires the\nsecond fewest GFLOPs for inference. When the models were\ntrained on MMNIST-10K, PredRNN-V2 [19] achieved the\nhighest PSNR, but our proposed model has 39.2% decrease\nin model size and 51.5% decrease in parameters, compared\nto PredRNN-V2. To visually demonstrate the effectiveness\nof our proposed model, Fig. 5 shows the qualitative results\nof models trained on MMNIST-10K. We observe that\nConvTransformer [26] and VPTR [28] generated blurry\nresults and E3D-LSTM [15] failed to correctly predict the\nrelative positions of the digits. The results of our proposed\n3DTransLSTM model correctly captured the trajectory of\nmoving digits and looked similar to the ground truth without\nblurriness. MIM [17], PredRNN-V2 [19], and TCTN [27]\nproduced good visual results at the expense of huge model\nsize, parameters and computational complexity compared to\nour proposed model.\n2) KTH ACTION\nThe KTH Action dataset has a similar frame size (64 × 64)\nand input/output sequence length as the MMNIST dataset.\nThe quantitative results are also summarized in Table 1.\nObviously, the proposed 3DTransLSTM model achieved the\nthird highest PSNR and SSIM values. Although the MIM [17]\nand VPTR [28] models achieved slightly higher PSNR and\nSSIM values than 3DTransLSTM, their model sizes were\nmuch larger, and they required much more parameters and\nGFLOPs. Fig. 6 shows the predicted frames of different\nmodels. We observe that the proposed 3DTransLSTM model\npredicted the motion of the hands correctly and it preserved\nthe structure of the person. In contrast, the transformer-\nbased models VPTR [28], ConvTransformer [26], and the\nConvLSTM-based model E3D-LSTM [15] generated blurry\nresults and had missing arms. The visual results of MIM [17],\nTCTN [27], and PredRNN-V2 [19] are close to the proposed\n39598 VOLUME 12, 2024\nM. Mathai et al.: Hybrid Transformer-LSTM Model With 3D Separable Convolution\nFIGURE 6. The visual results on the KTH Action dataset [52]. The proposed 3DTransLSTM model predicted the motion of\nthe hands correctly and preserved the structure of the person, unlike the blurry results of VPTR[28], ConvTransformer[26]\nand E3D-LSTM[15].\nTABLE 2. Quantitative results on human 3.6m dataset.∗ The results are\nreported in[17]. The best, second-best, and third-best results of each\nmetric are highlighted in red, blue and brown, respectively.\nTABLE 3. Quantitative results on Caltech Pedestrian dataset.∗ The results\nare reported in [18].† The results are reported in[58]. The best,\nsec‘ond-best, and third-best results of each metric are highlighted in red,\nblue and brown, respectively.\n3DTransLSTM, but they require much more computational\ncost.\n3) HUMAN 3.6m\nThis human pose dataset has video frames of dimension\n3 × 128 × 128, where the first dimension represents the\nRGB channels. Our proposed model predicts the future four\nframes based on previous four frames. Table 2 summarizes\nthe quantitative results of the compared models. The proposed\nFIGURE 7. The visual results on the human 3.6m dataset [53].\n3DTransLSTM achieved the smallest model size, fewest\nnumber of parameters and GFLOPs. In terms of prediction\naccuracy, it achieved the second highest PSNR and SSIM\nvalues. Although VPTR [28] achieved the highest frame\nVOLUME 12, 2024 39599\nM. Mathai et al.: Hybrid Transformer-LSTM Model With 3D Separable Convolution\nTABLE 4. Ablation study on MMNIST-3K. The best and second-best results of each metric are highlighted in red and blue, respectively.\nFIGURE 8. The visual results on the Caltech Pedestrian dataset[55] shows that our proposed model 3DTransLSTM\nproduces results similar to high computational models such as CrevNet[18] and VPTR[28].\nprediction accuracy, it was 44 times the size of our model,\nand it required 17 times more parameters than our model.\nFig. 7 shows the visual quality of the predicted frames.\nWe observe that the proposed 3DTransLSTM model is\nable to preserve the structure of the person’s body and\nshape of arms. In contrast, the solely ConvLSTM-based\nmodels E3D-LSTM [15], MIM [17] and PredRNN-V2\n[19] produced vague results and had inconsistencies in the\nphysical appearance of the person, especially the shape of\narms. Besides, the transformer-based model TCTN [27] also\ngenerated blurry results. The quantitative and qualitative\nresults show that our model works effectively for RGB video\nframes.\n4) KITTI AND CALTECH PEDESTRIAN\nTo evaluate the generalization ability of video prediction\nmodels, we trained the models on the KITTI dataset [54]\nusing the previous 10 frames to predict the next one frame\nand evaluated the trained models on the Caltech Pedestrian\ndataset [55]. Both the KITTI and Caltech Pedestrian datasets\nare complex datasets comprised of real-world scenarios with\nmultiple moving objects in the background. We observe\nfrom Table 3 that the proposed 3DTransLSTM achieved\nthe best model efficiency in terms of model size, number\nof parameters, and GFLOPs. Besides, it achieved the third\nhighest PSNR and SSIM values. Although CrevNet [18]\nhas the highest PSNR and SSIM values, it required much\nlarger model size, much more parameters and GFLOPs\nthan our model did. Fig. 8 shows that our proposed model\ngenerated accurate visual results similar to CrevNet [18] and\nVPTR [28], producing clear objects and texts in the predicted\nframes.\nG. ABLATION STUDY\nWe conducted a series of experiments with various designs of\nthe proposed TransLSTM network to validate the advantages\nof using 3D separable convolutions. The models were trained\non the MMNIST-2K dataset and tested on the MMNIST-3K\ndataset. The results are summarized in Table 4.\nFirst, we designed Model A, which is the TransLSTM\nnetwork with traditional 2D convolutions in the spatial\nembedding module, the self-attention and feed-forward\nlayers of Branch 1, and in the ST-LSTM layers of Branch 2.\nIt achieved an average PSNR of 19.44 dB and an average\nSSIM of 0.844. Its model size is 745MB. It has 78.7 M\ntrainable parameters, and it requires 934.7 GFLOPs to\nconduct inference.\nTo reduce the model size, parameters, and computational\ncomplexity of Model A, we designed Model B, which\nis the TransLSTM network with 2D separable convolu-\ntions. It apparently reduces the model size, parameters,\nand computation complexity to 50.9 MB, 10.5 M, and\n126.09 GFLOPs, respectively. Nevertheless, the video frame\nprediction accuracy has decreased to an average PSNR of\n18.03 dB and an average SSIM of 0.791.\nTo leverage temporal information in the video sequence,\nwe further designed Model C, the TransLSTM with tradi-\ntional 3D convolutions in self-attention and feed-forward\nlayers of Branch 1 and ST-LSTM layers of Branch 2.\nThe results in Table 4 showed that the prediction accuracy\nwas significantly improved compared to traditional 2D\nconvolutions (Model A), achieving an average PSNR of\n21.48 dB and an average SSIM of 0.913. However, this\ncomes at a price of dramatically increased model size (1,100\nMB), number of trainable parameters (231.5 M) and GFLOPs\n(2,914.3).\nIn contrast, our proposed model, 3DTransLSTM with\n3D separable convolutions, significantly reduces the model\nsize, number of parameters, and GFLOPs by 94.9%, 95%,\nand 95.2%, respectively, compared to Model C. Meanwhile,\nit only slightly decreased the prediction accuracy by 2.1% for\nthe PSNR metric and by 2.2% for the SSIM metric.\n39600 VOLUME 12, 2024\nM. Mathai et al.: Hybrid Transformer-LSTM Model With 3D Separable Convolution\nThe above ablation studies thoroughly validate the ben-\nefits of the proposed 3DTransLSTM with 3D separable\nconvolutions. This network architecture not only leverages\nthe spatiotemporal correlations in the video to provide\ncompetitive frame prediction accuracy, but also significantly\nreduces the model size, trainable parameters and computa-\ntional complexity compared to traditional 3D convolutions.\nTherefore, it offers a good trade-off between model accuracy\nand model complexity.\nVI. CONCLUSION\nVideo frame prediction is a challenging yet essential vision\ntask in various real-world scenarios. In this era of Green AI,\nit is extremely important for machine learning models to offer\nboth model efficiency and accuracy. In this paper, we devise\na novel video prediction framework 3DTransLSTM, incorpo-\nrating both transformer and LSTM structures with 3D separa-\nble convolutions. Extensive experimental results demonstrate\nthe effectiveness of our proposed scheme on both synthetic\nand real-world datasets. Compared to existing approaches,\nour method is able to achieve competitive prediction accuracy\nwith significantly reduced model size, number of parameters,\nand computational complexity. Hence, our model is more\nsuitable for memory-constrained and computation resource-\nlimited platforms, such as mobile and embedded devices.\nIn the future, we plan to adapt this approach to various\nvideo processing tasks, such as reference frame generation\nin learning-based video coding, video super-resolution, and\nomnidirectional video frame prediction.\nREFERENCES\n[1] B. Liu, Y. Chen, S. Liu, and H.-S. Kim, ‘‘Deep learning in latent space for\nvideo prediction and compression,’’ in Proc. IEEE/CVF Conf. Comput. Vis.\nPattern Recognit. (CVPR), Nashville, TN, USA, Jun. 2021, pp. 701–710.\n[2] Y. Zhou, H. Dong, and A. El Saddik, ‘‘Deep learning in next-frame predic-\ntion: A benchmark review,’’ IEEE Access, vol. 8, pp. 69273–69283, 2020.\n[3] W. Liu, W. Luo, D. Lian, and S. Gao, ‘‘Future frame prediction for\nanomaly detection—A new baseline,’’ in Proc. IEEE/CVF Conf. Comput.\nVis. Pattern Recognit., Salt Lake City, UT, USA, Jun. 2018, pp. 6536–6545.\n[4] X. Lin, Q. Zou, X. Xu, Y. Huang, and Y. Tian, ‘‘Motion-aware feature\nenhancement network for video prediction,’’ IEEE Trans. Circuits Syst.\nVideo Technol., vol. 31, no. 2, pp. 688–700, Feb. 2021.\n[5] S. Li, J. Fang, H. Xu, and J. Xue, ‘‘Video frame prediction by deep multi-\nbranch mask network,’’ IEEE Trans. Circuits Syst. Video Technol., vol. 31,\nno. 4, pp. 1283–1295, Apr. 2021.\n[6] H. Choi and I. V. Bajic, ‘‘Deep frame prediction for video coding,’’\nIEEE Trans. Circuits Syst. Video Technol., vol. 30, no. 7, pp. 1843–1855,\nJul. 2020.\n[7] J.-K. Lee, N. Kim, S. Cho, and J.-W. Kang, ‘‘Deep video prediction\nnetwork-based inter-frame coding in HEVC,’’ IEEE Access, vol. 8,\npp. 95906–95917, 2020.\n[8] R. Yang, R. Timofte, and L. Van Gool, ‘‘Advancing learned video\ncompression with in-loop frame prediction,’’ IEEE Trans. Circuits Syst.\nVideo Technol., vol. 33, no. 5, pp. 2410–2423, May 2023.\n[9] Y. Li, C. Fang, J. Yang, Z. Wang, X. Lu, and M. H. Yang, ‘‘Flow-grounded\nspatial–temporal video prediction from still images,’’ in Proc. Eur. Conf.\nComput. Vis., Munich, Germany, Sep. 2018, pp. 600–615.\n[10] F. A. Reda, G. Liu, K. J. Shih, R. Kirby, J. Barker, D. Tarjan, and\nB. Catanzaro, ‘‘SDC-Net: Video prediction using spatially-displaced\nconvolution,’’ in Proc. Eur. Conf. Comput. Vis., Munich, Germany,\nSep. 2018, pp. 718–733.\n[11] N. Srivastava, E. Mansimov, and R. Salakhudinov, ‘‘Unsupervised learning\nof video representations using LSTMs,’’ in Proc. PMLR Int. Conf. Mach.\nLearn., vol. 37, Lille, France, Jul. 2015, pp. 843–852.\n[12] M. Oliu, J. Selva, and S. Escalera, ‘‘Folded recurrent neural networks\nfor future video prediction,’’ in Proc. Eur. Conf. Comput. Vis., Munich,\nGermany, Sep. 2018, pp. 716–731.\n[13] X. Shi, Z. Chen, H. Wang, D. Y. Yeung, W. K. Wong, and W. C. Woo,\n‘‘Convolutional LSTM network: A machine learning approach for\nprecipitation nowcasting,’’ in Proc. Int. Conf. Neural Inf. Process. Syst.,\nvol. 28, Montreal, QC, Canada, Dec. 2015, pp. 802–810.\n[14] C. Finn, I. Goodfellow, and S. Levine, ‘‘Unsupervised learning for physical\ninteraction through video prediction,’’ in Proc. Adv. Neural Inf. Proc. Syst.,\nvol. 29, Barcelona, Spain, Dec. 2016, pp. 1–9.\n[15] Y. Wang, L. Jiang, M. H. Yang, L.-J. Li, M. Long, and L. Fei-Fei, ‘‘Eidetic\n3D LSTM: A model for video prediction and beyond,’’ in Proc. Int. Conf.\nLearn. Represent., New Orleans, LA, USA, May 2019, pp. 1–14.\n[16] Y. Wang, M. Long, J. Wang, Z. Gao, and S. Y. Philip, ‘‘PredRNN:\nRecurrent neural networks for predictive learning using spatiotemporal\nLSTMs,’’ in Proc. Int. Conf. Neural Inf. Process. Syst., vol. 30, Long\nBeach, CA, USA, Dec. 2017, pp. 879–888.\n[17] Y. Wang, J. Zhang, H. Zhu, M. Long, J. Wang, and P. S. Yu, ‘‘Memory\nin memory: A predictive neural network for learning higher-order non-\nstationarity from spatiotemporal dynamics,’’ in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit., Long Beach, CA, USA, Jun. 2019,\npp. 9154–9162.\n[18] W. Yu, Y. Lu, S. Easterbrook, and S. Fidler, ‘‘Efficient and information-\npreserving future frame prediction and beyond,’’ in Proc. Int. Conf. Learn.\nRepresent., Apr. 2020, pp. 1–14.\n[19] Y. Wang, H. Wu, J. Zhang, Z. Gao, J. Wang, P. S. Yu, and M. Long,\n‘‘PredRNN: A recurrent neural network for spatiotemporal predictive\nlearning,’’ IEEE Trans. Pattern Anal. Mach. Intell., vol. 45, no. 2,\npp. 2208–2225, Feb. 2023.\n[20] M. Mathai, Y. Liu, and N. Ling, ‘‘A lightweight model with separable CNN\nand LSTM for video prediction,’’ in Proc. IEEE Int. Symp. Circuits Syst.\n(ISCAS), Austin, TX, USA, May 2022, pp. 516–520.\n[21] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Proc. Syst., vol. 30, Long Beach, CA, USA, Dec. 2017,\npp. 1–12.\n[22] J. Li, W. Wang, J. Chen, L. Niu, J. Si, C. Qian, and L. Zhang, ‘‘Video\nsemantic segmentation via sparse temporal transformer,’’ in Proc. ACM\nInt. Conf. Multimedia, Chengdu, China, Oct. 2021, pp. 59–68.\n[23] H. Yuan, Z. Cai, H. Zhou, Y. Wang, and X. Chen, ‘‘TransAnomaly: Video\nanomaly detection using video vision transformer,’’ IEEE Access, vol. 9,\npp. 123977–123986, 2021.\n[24] M. Fujitake and A. Sugimoto, ‘‘Video sparse transformer with attention-\nguided memory for video object detection,’’ IEEE Access, vol. 10,\npp. 65886–65900, 2022.\n[25] J. Wensel, H. Ullah, and A. Munir, ‘‘ViT-ReT: Vision and recurrent\ntransformer neural networks for human activity recognition in videos,’’\nIEEE Access, vol. 11, pp. 72227–72249, 2023.\n[26] Z. Liu, S. Luo, W. Li, J. Lu, Y. Wu, S. Sun, C. Li, and L. Yang,\n‘‘ConvTransformer: A convolutional transformer network for video frame\nsynthesis,’’ 2020, arXiv:2011.10185.\n[27] Z. Yang, X. Yang, and Q. Lin, ‘‘PTCT: Patches with 3D-temporal\nconvolutional transformer network for precipitation nowcasting,’’ 2021,\narXiv:2112.01085.\n[28] X. Ye and G. A. Bilodeau, ‘‘VPTR: Efficient transformers for video\nprediction,’’ in Proc. IEEE Int. Conf. Pattern Recognit., Montréal, QC,\nCanada, Aug. 2022, pp. 3492–3499.\n[29] W. Li, X. Wang, X. Xia, J. Wu, J. Li, X. Xiao, M. Zheng, and S. Wen,\n‘‘SepViT: Separable vision transformer,’’ 2022, arXiv:2203.15380.\n[30] R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni, ‘‘Green AI,’’ Commun.\nACM, vol. 63, no. 12, pp. 54–63, Nov. 2020, doi: 10.1145/3381831.\n[31] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell, ‘‘On the\ndangers of stochastic parrots: Can language models be too big?’’ in Proc.\nACM Conf. Fairness, Accountability, Transparency, Virtual Event Canada,\nMar. 2021, pp. 610–623.\n[32] V. Vukotić, S. L. Pintea, C. Raymond, G. Gravier, and J. C. V. Gemert,\n‘‘One-step time-dependent future video frame prediction with a convolu-\ntional encoder–decoder neural network,’’ in Proc. Int. Conf. Image Anal.\nProcess., Catania, Italy, Sep. 2017, pp. 140–151.\n[33] M. A. Yilmaz and A. M. Tekalp, ‘‘DFPN: Deformable frame prediction\nnetwork,’’ inProc. IEEE Int. Conf. Image Process. (ICIP), Anchorage, AK,\nUSA, Sep. 2021, pp. 1944–1948.\nVOLUME 12, 2024 39601\nM. Mathai et al.: Hybrid Transformer-LSTM Model With 3D Separable Convolution\n[34] G. J. Sullivan, J.-R. Ohm, W.-J. Han, and T. Wiegand, ‘‘Overview of the\nhigh efficiency video coding (HEVC) standard,’’ IEEE Trans. Circuits Syst.\nVideo Technol., vol. 22, no. 12, pp. 1649–1668, Dec. 2012.\n[35] B. Bross, Y.-K. Wang, Y. Ye, S. Liu, J. Chen, G. J. Sullivan, and\nJ.-R. Ohm, ‘‘Overview of the versatile video coding (VVC) standard and\nits applications,’’IEEE Trans. Circuits Syst. Video Technol., vol. 31, no. 10,\npp. 3736–3764, Oct. 2021.\n[36] X. J. Shi, Z. Gao, L. Lausen, H. Wang, D. Y. Yeung, W. K. Wong, and\nW. C. Woo, ‘‘Deep learning for precipitation nowcasting: A benchmark\nand a new model,’’ in Proc. Adv. Neural Inf. Proc. Syst., vol. 30, Long\nBeach, CA, USA, Dec. 2017, pp. 5617–5627.\n[37] Y. Wang, Z. Gao, M. Long, J. Wang, and P. S. Yu, ‘‘PredRNN++: Towards\na resolution of the deep-in-time dilemma in spatiotemporal predictive\nlearning,’’ in Proc. PMLR Int. Conf. Mach. Learn., Stockholm, Sweden,\nJul. 2018, pp. 5123–5132.\n[38] J. Su, W. Byeon, J. Kossaifi, F. Huang, J. Kautz, and A. Anandkumar,\n‘‘Convolutional tensor-train LSTM for spatio-temporal learning,’’ in Proc.\nAdv. Neural Inf. Process. Syst., vol. 33, 2020, pp. 13714–13726.\n[39] Z. Chang, X. Zhang, S. Wang, S. Ma, and W. Gao, ‘‘IPRNN: An\ninformation-preserving model for video prediction using spatiotemporal\nGRUs,’’ in Proc. IEEE Int. Conf. Image Process., Anchorage, AK, USA,\nSep. 2021, pp. 2703–2707.\n[40] Z. Chang, X. Zhang, S. Wang, S. Ma, Y. Ye, and W. Gao, ‘‘ASTM:\nAn attention based spatiotemporal model for video prediction using 3D\nconvolutional neural networks,’’ in Proc. IEEE Int. Conf. Multimedia Expo\n(ICME), Shenzhen, China, Jul. 2021, pp. 1–6.\n[41] M. Mathieu, C. Couprie, and Y. LeCun, ‘‘Deep multi-scale video prediction\nbeyond mean square error,’’ in Proc. 4th Int. Conf. Learn. Represent., San\nJuan, Puerto Rico, May 2016, pp. 1–14.\n[42] Z. Yi, H. Zhang, P. Tan, and M. Gong, ‘‘DualGAN: Unsupervised dual\nlearning for image-to-image translation,’’ in Proc. IEEE Int. Conf. Comput.\nVis. (ICCV), Venice, Italy, Oct. 2017, pp. 2868–2876.\n[43] Y.-H. Kwon and M.-G. Park, ‘‘Predicting future frames using retrospective\ncycle GAN,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Long Beach, CA, USA, Jun. 2019, pp. 1811–1820.\n[44] Q. N. Tran and S.-H. Yang, ‘‘Attention-based inter-prediction for versatile\nvideo coding,’’ IEEE Access, vol. 11, pp. 84313–84322, 2023.\n[45] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand,\nM. Andreetto, and H. Adam, ‘‘MobileNets: Efficient convolutional neural\nnetworks for mobile vision applications,’’ 2017, arXiv:1704.04861.\n[46] A. Pfeuffer and K. Dietmayer, ‘‘Separable convolutional LSTMs for faster\nvideo segmentation,’’ in Proc. IEEE Intell. Transp. Syst. Conf. (ITSC),\nAuckland, New Zealand, Oct. 2019, pp. 1072–1078.\n[47] B. Hou, Y. Liu, and N. Ling, ‘‘A super-fast deep network for moving object\ndetection,’’ inProc. IEEE Int. Symp. Circuits Syst. (ISCAS), Seville, Spain,\nOct. 2020, pp. 1–5.\n[48] Z. Islam, M. Rukonuzzaman, R. Ahmed, Md. H. Kabir, and M. Farazi,\n‘‘Efficient two-stream network for violence detection using separable\nconvolutional LSTM,’’ in Proc. Int. Joint Conf. Neural Netw. (IJCNN),\nShenzhen, China, Jul. 2021, pp. 1–8.\n[49] X. Zhang, Y. Tie, and L. Qi, ‘‘Dynamic gesture recognition based on 3D\nseparable convolutional LSTM networks,’’ in Proc. IEEE 11th Int. Conf.\nSoftw. Eng. Service Sci. (ICSESS), Beijing, China, Oct. 2020, pp. 180–183.\n[50] B. Hou, Y. Liu, N. Ling, L. Liu, Y. Ren, and M. K. Hsu, ‘‘F3DsCNN: A fast\ntwo-branch 3D separable CNN for moving object detection,’’ in Proc. Int.\nConf. Vis. Commun. Image Process. (VCIP), Munich, Germany, Dec. 2021,\npp. 1–5.\n[51] B. Hou, Y. Liu, N. Ling, L. Liu, and Y. Ren, ‘‘A fast lightweight 3D\nseparable convolutional neural network with multi-input multi-output for\nmoving object detection,’’ IEEE Access, vol. 9, pp. 148433–148448, 2021.\n[52] C. Schuldt, I. Laptev, and B. Caputo, ‘‘Recognizing human actions: A\nlocal SVM approach,’’ in Proc. 17th Int. Conf. Pattern Recognit., vol. 3,\nCambridge, U.K., Aug. 2004, pp. 32–36.\n[53] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu, ‘‘Human3.6M: large\nscale datasets and predictive methods for 3D human sensing in natural\nenvironments,’’ IEEE Trans. Pattern Anal. Mach. Intell., vol. 36, no. 7,\npp. 1325–1339, Jul. 2014.\n[54] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, ‘‘Vision meets robotics:\nThe KITTI dataset,’’ Int. J. Robot. Res., vol. 32, no. 11, pp. 1231–1237,\nSep. 2013.\n[55] P. Dollar, C. Wojek, B. Schiele, and P. Perona, ‘‘Pedestrian detection: A\nbenchmark,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Miami,\nFL, USA, Jun. 2009, pp. 304–311.\n[56] W. He, T. Xiong, H. Wang, J. He, X. Ren, Y. Yan, and L. Tan, ‘‘Radar\necho spatiotemporal sequence prediction using an improved ConvGRU\ndeep learning model,’’ Atmosphere, vol. 13, no. 1, p. 88, Jan. 2022.\n[57] Y. Liu, P. Du, and Y. Li, ‘‘Hierarchical motion-compensated deep network\nfor video compression,’’ Proc. SPIE, vol. 11730, pp. 124–131, Apr. 2021.\n[58] Y.-H. Ho, C.-Y. Cho, and W.-H. Peng, ‘‘Deep reinforcement learning for\nvideo prediction,’’ in Proc. IEEE Int. Conf. Image Process. (ICIP), Taipei,\nTaiwan, Sep. 2019, pp. 604–608.\nMAREETA MATHAI(Graduate Student Member,\nIEEE) received the B.Comp. degree in computer\nengineering from the National University of Sin-\ngapore, in 2011, and the M.Eng. degree from\nNanyang Technological University, Singapore,\nin 2016. She is currently pursuing the Ph.D. degree\nin computer science and engineering with Santa\nClara University. She is working with advisors Dr.\nNam Ling and Dr. Ying Liu on efficient deep-\nlearning methods for video prediction and video\ncoding. Her research interests include deep learning, computer vision,\npredictive modeling, and video prediction.\nYING LIU (Member, IEEE) received the B.S.\ndegree in communications engineering from Bei-\njing University of Posts and Telecommunications,\nBeijing, China, in 2006, and the M.S. and\nPh.D. degrees in electrical engineering from The\nState University of New York, Buffalo, NY, USA,\nin 2008 and 2012, respectively. She is currently\nan Assistant Professor with the Department of\nComputer Science and Engineering, Santa Clara\nUniversity, Santa Clara, CA, USA. Her main\nresearch interests include image and video processing, deep learning, and\ncomputer vision. She serves as an Associate Editor for IEEE TRANSACTIONS\nON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY.\nNAM LING (Life Fellow, IEEE) received the\nB.Eng. degree from the National University of Sin-\ngapore, in 1981, and the M.S. and Ph.D. degrees\nfrom the University of Louisiana at Lafayette,\nLafayette, LA, USA, in 1985 and 1989, respec-\ntively. From 2002 to 2010, he was the Associate\nDean of the School of Engineering, Santa Clara\nUniversity, Santa Clara, CA, USA. He was the\nSanfilippo Family Chair Professor. He is currently\nthe Wilmot J. Nicholson Family Chair Professor\nand the Chair of the Department of Computer Science and Engineering,\nSanta Clara University. He is/was also a Chair/Distinguished/Guest and\nConsulting Professor with several universities internationally. He has\nauthored or coauthored more than 250 publications and seven adopted\nstandard contributions. He has been granted 20 U.S. patents so far. He has\ndelivered more than 120 invited colloquia worldwide. He is an IEEE Fellow\ndue to his contributions to video coding algorithms and architectures. He is\nalso an IET Fellow. He was a recipient of the IEEE ICCE Best Paper\nAward (First Place) and the Umedia Best/Excellent Paper Award (thrice).\nHe received six awards from Santa Clara University, four at the university\nlevel and two at the school/college level. He has served as the General\nChair/the Co-Chair for IEEE Hot Chips, VCVP (twice), IEEE ICME,\nUmedia (seven times), IEEE SiPS, and IEEE VCIP. He has also served as\nthe Technical Program Co-Chair for IEEE ISCAS (twice), APSIPA ASC,\nIEEE APCCAS, IEEE SiPS (twice), DCV, and IEEE VCIP. He was the\nTechnical Committee Chair of IEEE CASCOM TC and IEEE TCMM. He is\nthe Chair of APSIPA U.S. Chapter. He has served as a Guest/Associate\nEditor for IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS—I: REGULAR PAPERS,\nIEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING, JSPS (Springer),\nand MSSP (Springer). He was named as an IEEE Distinguished Lecturer\n(twice) and an APSIPA Distinguished Lecturer. He was a Keynote Speaker of\nIEEE APCCAS, VCVP (twice), JCPC, IEEE ICAST, IEEE ICIEA, IET FC\nUmedia, IEEE Umedia, IEEE ICCIT, ICNLP/SSPS/CVPS, and Workshop at\nXUPT (twice).\n39602 VOLUME 12, 2024",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7176797389984131
    },
    {
      "name": "Convolution (computer science)",
      "score": 0.5984455943107605
    },
    {
      "name": "Transformer",
      "score": 0.5615834593772888
    },
    {
      "name": "Separable space",
      "score": 0.5583373308181763
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5133482813835144
    },
    {
      "name": "Computer vision",
      "score": 0.34144502878189087
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.33948814868927
    },
    {
      "name": "Mathematics",
      "score": 0.13808736205101013
    },
    {
      "name": "Voltage",
      "score": 0.11859381198883057
    },
    {
      "name": "Electrical engineering",
      "score": 0.08753123879432678
    },
    {
      "name": "Artificial neural network",
      "score": 0.08020040392875671
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I16269868",
      "name": "Santa Clara University",
      "country": "US"
    }
  ]
}