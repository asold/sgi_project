{
  "title": "AstroLLaMA: Towards Specialized Foundation Models in Astronomy",
  "url": "https://openalex.org/W4392669797",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2096493883",
      "name": "Tuan Dung Nguyen",
      "affiliations": [
        "California University of Pennsylvania",
        "Australian National University"
      ]
    },
    {
      "id": "https://openalex.org/A4213578152",
      "name": "Yuan-Sen Ting",
      "affiliations": [
        "Australian National University",
        "The Ohio State University"
      ]
    },
    {
      "id": "https://openalex.org/A1964641442",
      "name": "Ioana Ciuca",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2675771371",
      "name": "Charles O'Neill",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2171943839",
      "name": "Ze Chang Sun",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A4381925371",
      "name": "Maja Jabłońska",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2530030123",
      "name": "Sandor Kruk",
      "affiliations": [
        "European Space Astronomy Centre"
      ]
    },
    {
      "id": "https://openalex.org/A3041815072",
      "name": "Ernest Perkowski",
      "affiliations": [
        "European Space Astronomy Centre"
      ]
    },
    {
      "id": "https://openalex.org/A2125652900",
      "name": "Jack Miller",
      "affiliations": [
        "Australian National University"
      ]
    },
    {
      "id": null,
      "name": "Jason Jason Jingsh Li",
      "affiliations": [
        "IBM Research - Australia"
      ]
    },
    {
      "id": "https://openalex.org/A3019506566",
      "name": "J. E. G. Peek",
      "affiliations": [
        "Space Telescope Science Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2590405618",
      "name": "Kartheik Iyer",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2479618011",
      "name": "Tomasz Różański",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5092865527",
      "name": "Pranav Khetarpal",
      "affiliations": [
        "Indian Institute of Technology Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A5040143467",
      "name": "Sharaf Zaman",
      "affiliations": [
        "Australian National University"
      ]
    },
    {
      "id": "https://openalex.org/A2551349021",
      "name": "David Brodrick",
      "affiliations": [
        "Australian National University"
      ]
    },
    {
      "id": null,
      "name": "Sergio J. Rodriguez Mendez",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2476454107",
      "name": "Thang Bui",
      "affiliations": [
        "Australian National University"
      ]
    },
    {
      "id": "https://openalex.org/A2620724811",
      "name": "Alyssa Goodman",
      "affiliations": [
        "Harvard University Press"
      ]
    },
    {
      "id": "https://openalex.org/A2692098784",
      "name": "Alberto Accomazzi",
      "affiliations": [
        "Deleted Institution"
      ]
    },
    {
      "id": "https://openalex.org/A2439077505",
      "name": "Jill Naiman",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2619888663",
      "name": "Jesse Cranney",
      "affiliations": [
        "Australian National University"
      ]
    },
    {
      "id": "https://openalex.org/A1975846344",
      "name": "Kevin Schawinski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2788175196",
      "name": "Roberta Raileanu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4381586769",
    "https://openalex.org/W4365597201",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W1708826858",
    "https://openalex.org/W3111815510",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W3217652161",
    "https://openalex.org/W4366330503",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2962784628"
  ],
  "abstract": "Tuan Dung Nguyen, Yuan-Sen Ting, Ioana Ciuca, Charles O'Neill, Ze-Chang Sun, Maja Jabłońska, Sandor Kruk, Ernest Perkowski, Jack Miller, Jason Jason Jingsh Li, Josh Peek, Kartheik Iyer, Tomasz Rozanski, Pranav Khetarpal, Sharaf Zaman, David Brodrick, Sergio J. Rodriguez Mendez, Thang Bui, Alyssa Goodman, Alberto Accomazzi, Jill Naiman, Jesse Cranney, Kevin Schawinski, Roberta Raileanu. Proceedings of the Second Workshop on Information Extraction from Scientific Publications. 2023.",
  "full_text": "Proceedings of the 2nd Workshop on Information Extraction from Scientiﬁc Publications, pages 49–55\nNov 1, 2023. ©2023 Association for Computational Linguistics\n49\nAstroLLaMA\n : Towards Specialized Foundation Models in Astronomy\nTuan Dung Nguyen1,2*,Yuan-Sen Ting2,3*, Ioana Ciuc˘a2*,\nCharles O’Neill2†, Ze-Chang Sun4†, Maja Jabło´nska2†, Sandor Kruk5†,\nErnest Perkowski5, Jack Miller2, Jason Jingshi Li6, Josh Peek7, Kartheik Iyer8,\nTomasz Ró˙za´nski2,9, Pranav Khetarpal10, Sharaf Zaman2, David Brodrick2,\nSergio J. Rodríguez Méndez2, Thang Bui2, Alyssa Goodman11, Alberto Accomazzi12,\nJill Naiman13, Jesse Cranney2, Kevin Schawinski14, Roberta R˘aileanu15, UniverseTBD\n1University of Pennsylvania, United States 2Australian National University, Australia\n3Ohio State University, United States 4Tsinghua University, China\n5European Space Agency, ESAC, Spain 6Learning Machines, Australia\n7Space Telescope Science Institute, United States\n8Columbia University, United States 9Wrocław University, Poland\n10Indian Institute of Technology Delhi, India 11Harvard University, United States\n12NASA Astrophysics Data System, Center for Astrophysics, United States\n13University of Illinois at Urbana-Champaign 14Modulos AG, Switzerland\n15 University College London, United Kingdom\nAbstract\nLarge language models often excel in many\nhuman-language tasks but tend to falter in\nhighly specialized domains like scholarly as-\ntronomy. To bridge this gap, we introduce As-\ntroLLaMA, a 7-billion-parameter model fine-\ntuned from LLaMA-2 using over 300,000 as-\ntronomy abstracts from arXiv. Optimized for\ntraditional causal language modeling, AstroL-\nLaMA shows marked domain adaptation by\nachieving a 30% lower perplexity than LLaMA-\n2. Compared to state-of-the-art foundation\nmodels, AstroLLaMA generates more insight-\nful and scientifically relevant text completions\nand embedding extraction despite having signif-\nicantly fewer parameters. AstroLLaMA serves\nas a highly domain-specific model with broad\nfine-tuning potential: Its public release aims\nto spur astronomy-focused research, including\nautomatic paper summarization, conversational\nagent development and hypothesis generation.\n1 Introduction\nThe advent of Large Language Models (LLMs) has\nsparked interdisciplinary interest thanks to a conflu-\nence of factors: accumulation of massive datasets,\nleaps in computational power, and breakthroughs\nin neural architectures. Flagship models like GPT-\n4 (OpenAI, 2023), PaLM (Chowdhery et al., 2022;\nGoo) and LLaMA (Touvron et al., 2023; Meta,\n2023) have exhibited exceptional versatility in a\nvariety of tasks from logical reasoning and compre-\nhension to creative writing, often accomplished via\n*Lead contribution. Email: joshtn@seas.upenn.edu\n†Major contribution.\nmethods like prompting, fine-tuning, and human-\nin-the-loop reinforcement learning.\nThe astronomy discipline presents both a unique\nchallenge and a fertile ground for the application\nof LLMs. The corpus of scholarly texts in astron-\nomy likely constitutes but a minuscule portion of\nthe data on which generic LLMs are trained, re-\nsulting in limitations like hallucinations in favor\nof more “generic” responses. Only about 2.5%\nof LLaMA-2’s training set, for example, likely\ncomes from arXiv, of which less than 5% belongs\nto the astronomy literature. The nature of astro-\nnomical research, on the other hand, often involves\ncross-disciplinary insights due to universally ap-\nplicable physical processes. When well-curated,\nLLMs could meaningfully assist with this effort,\nsuch as through hypothesis generation.\nExisting scales based on in-context prompting\nand instruction learning, primarily involving GPT-\n4, have already demonstrated significant potential\nfor generating substantive hypotheses (Ciuc˘a and\nTing, 2023; Ciuc ˘a et al., 2023). Further, the as-\ntronomy community’s “open sky” policy, which\ngrants public access to the majority of its datasets\neither immediately or after a brief proprietary pe-\nriod (Almeida et al., 2023; Fabricius et al., 2021),\npairs well with the wealth of resources available\nin archives like NASA’s Astrophysics Data System\n(Accomazzi et al., 2015; Borgman and Wofford,\n2021). Such an open-access policy can facilitate\ndeep engagement with the astronomical literature.\nDespite their general capabilities, LLMs fre-\nquently lag behind specialized, smaller models in\n50\ndomain-specific applications. This disparity stems\nfrom two primary factors: (i) the eclectic nature of\nthe pre-training datasets, which dilutes the focus on\nspecialized subjects in favor of general predictive\nperformance, and (ii) the design ethos of LLMs\nas “foundation models” aimed at subsequent fine-\ntuning tailored to specific tasks. The existing land-\nscape for LLMs in astronomy remains limited, how-\never. To our knowledge, the only specialized model\nis astroBERT (Grezes et al., 2021), which has 110\nmillion parameters, fine-tuned on nearly 400,000\nADS papers. As an non-generative model, how-\never, astroBERT’s utility remains primarily limited\nto discriminative tasks.\nMotivated by these gaps, we present AstroL-\nLaMA, a state-of-the-art generative language\nmodel fine-tuned from LLaMA-2. Our model lever-\nages a corpus of 300,000 astronomy abstracts from\narXiv and boasts an architecture approximately 67\ntimes larger than that of astroBERT. AstroLLaMA\naspires to build upon astroBERT’s foundation by\noffering more improved performance in generating\nspecialized information and broader fine-tuning op-\nportunities for astronomical research. We describe\nour methodology in Sec. 2, provide some evalua-\ntion results in Sec. 3, and finally concluding with\nsome remarks in Sec. 4.\n2 AstroLLaMA\nIn this section, we discuss AstroLLaMA’s imple-\nmentation, focusing on the curation of its dataset,\nbase model architecture, and fine-tuning settings.\n2.1 Dataset\nWe derive our dataset from the arXiv repository,\navailable on Kaggle.a Our curated subset focuses\non papers classified under the astrophysics category\n(astro-ph), resulting in a collection of 326,238\narticles spanning from April 1992 to July 2023.\nWe extract these papers’ abstracts to form a corpus\nconsisting of approximately 95 million tokens. The\nmedian length of these abstracts is 291 tokens. To\nenable effective model evaluation, we randomly\ndesignate 20% of this curated dataset for testing.\n2.2 Base model\nOur base model is LLaMA-2, a 6.7 billion-\nparameter model developed by Meta (Meta, 2023).\nOriginally pre-trained on a corpus containing 2 tril-\nlion tokens, LLaMA-2 features a context window\nahttps://www.kaggle.com/Cornell-University/\narxiv\n0 50 100 150 200\nProcessed tokens (millions)\n7\n8\n9\n10Training perplexity\nEpoch 1 Epoch 2 Epoch 3\nFigure 1: Learning curve of AstroLLaMA during its\nfine-tuning on the arXiv astrophysics dataset. The fig-\nure tracks the evolution of perplexity, a measure of the\nmodel’s next-token prediction performance. The light\nblue curve shows the training perplexity after each pa-\nrameter update step, while the dark black curve provides\na smoothed average of the same metric taken over every\n10-step interval.\nof 4,096 tokens. For tokenization, the model em-\nploys a bytepair encoding strategy (Sennrich et al.,\n2016; Kudo and Richardson, 2018), with a vocabu-\nlary of 32,000 unique tokens.\n2.3 Fine-tuning settings\nWe rely on our curated training set, which includes\n77 million tokens. The setting of the fine-tuning\nphase largely follows from Meta (2023). First, spe-\ncial [BOS] (Beginning Of Sequence) and [EOS]\n(End Of Sequence) tokens are prepended and ap-\npended to each training sequence. These sequences\nare then concatenated and divided into fixed-length\nchunks, each comprising 512 tokens.\nWe follow the causal language modeling ob-\njective employed during the model’s pre-training\nphase, where the the next token is to be predicted\nusing its preceding context. We use the AdamW\noptimizer (Loshchilov and Hutter, 2018) with hy-\nperparameters β1 = 0.9, β2 = 0.95, ϵ= 10−5 and\na batch size of 32. The learning rate follows a co-\nsine schedule with a linear warmup to a peak value\nof 3 × 10−4 in the first 10% of the optimization\nsteps and a final learning rate of 10% of its peak.\nAdditional settings include weight decay and gra-\ndient clipping values of 0.1 and 1.0, respectively.\nNote that these hyperparameters are set according\nto LLaMA-2’s pre-training phase.\nWe fine-tune LLaMA over nearly three epochs,\ncorresponding to about 230 million processed\ntokens, using four NVIDIA A100 GPUs each\nequipped with 40GB of VRAM. To achieve re-\n51\nThe Magellanic Stream (MS) - an enormous ribbon of gas spanning 140∘ of the southern sky trailing the Magellanic Clouds - has been exquisitely mapped in the five decades since its discovery. However, despite concerted efforts, no stellar counterpart to the MS has been conclusively identified. This stellar stream would reveal the distance and 6D kinematics of the MS, constraining its formation and the past orbital history of the Clouds. We have been conducting a spectroscopic survey of the most distant and luminous red giant stars in the Galactic outskirts. From this dataset, we have discovered a prominent population of 13 stars matching the extreme angular momentum of the Clouds, spanning up to 100∘ along the MS at distances of 60−120 kpc. Furthermore, these kinemetically-selected stars lie along a [α/Fe]-deficient track in chemical space from −2.5<[Fe/H]<−0.5, consistent with their formation in the Clouds themselves. We identify these stars as high-confidence members of the Magellanic Stellar Stream. Half of these stars are metal-rich and closely follow the gaseous MS, whereas the other half are more scattered and metal-poor. We argue that the metal-rich stream is the recently-formed tidal counterpart to the MS, and speculate that the metal-poor population was thrown out of the SMC outskirts during an earlier interaction between the Clouds. The Magellanic Stellar Stream provides a strong set of constraints - distances, 6D kinematics, and birth locations - that will guide future simulations towards unveiling the detailed history of the Clouds. \nTheMagellanicStream(MS)-anenormousribbonofgasspanning140∘ofthesouthernskytrailingtheMagellanicClouds-hasbeenexquisitelymappedinthefivedecadessinceitsdiscovery.However,despiteconcertedefforts,nostellarcounterparttotheMShasbeenconclusivelyidentified.Thisstellarstreamwouldrevealthedistanceand6DkinematicsoftheMS,constrainingitsformationandthepastorbitalhistoryoftheClouds.Wepropose a novel approach to search for the elusive stellar counterpart of the Magellanic Stream. By employing advanced photometric and spectroscopic techniques, combined with state-of-the-art data mining and machine learning algorithms, we aim to systematically analyze and cross-reference multi-wavelength survey data to identify potential candidate stars belonging to the hypothesized stellar stream. Our methodology involves leveraging the distinctive chemical signatures and proper motions associated with stars originating from the Magellanic Clouds, allowing us to differentiate them from the foreground and background contaminants. If successful, this discovery would not only provide essential insights into the origin and evolution of the Magellanic Stream, but also enhance our understanding of the interaction between the Magellanic Clouds and the Milky Way. Furthermore, the proposed approach establishes a powerful framework for investigating other unresolved galactic structures and tracing their underlying dynamical histories.\nTheMagellanicStream(MS)-anenormousribbonofgasspanning140∘ofthesouthernskytrailingtheMagellanicClouds-hasbeenexquisitelymappedinthefivedecadessinceitsdiscovery.However,despiteconcertedefforts,nostellarcounterparttotheMShasbeenconclusivelyidentified.Thisstellarstreamwouldrevealthedistanceand6DkinematicsoftheMS,constrainingitsformationandthepastorbitalhistoryoftheClouds.We1) present a new method to select MS candidate stars from wide field optical imaging, 2) apply this method to the MS, 3) compare the results to existing catalogs, and 4) identify the most promising MS candidate stars. We apply our method to 2547 stars in the Gaia EDR3 catalog that have Gaia proper motions consistent with the MS and use these as a pilot sample to optimize our method. We then apply our method to the entire Gaia EDR3 catalog to identify 46,000 stars that are candidates for the MS, and we use the Gaia-ESO Survey to identify 13,000 of these stars that are likely to be MS candidates. We identify 266 stars with (G-K) colours and [Fe/H] that are consistent with previous estimates of the MS. These stars have proper motions and radial velocities that are consistent with the MS, and we place them on a 3D map of the MS. We find a previously unknown stellar stream, which we call the Southeastern Stream (SES), in the direction of the Clouds and MS. The SES has an M31 distance of 32.2+/-0.5 kpc and a 6D velocity of 136+/-10 km/s. The SES is about 20 times brighter in stars than the MS and about 40 times brighter in stars than the SMC. The SES is also 2.7 times more metal-rich than the MS, which suggests that it is a separate population. \nThe MagellanicStream (MS) -an enormous ribbon of gas spanning 140∘of the southern sky trailing the MagellanicClouds -has been exquisitely mapped in the five decades since its discovery. However, despite concerted efforts, no stellar counterpart to the MS has been conclusively identified. This stellar stream would reveal the distance and 6D kinematics of the MS, constraining its formation and the past orbital history of the Clouds. We ﬁnd a stellar stream in the MS, with a 6D velocity of 100 km s−1, and a distance of 100 kpc. The stream is 100 pc wide and 1000 pc long, and is consistent with the MS. The stream is 1000 times more massive than the Magellanic Clouds, and is likely to be a tidal stream from the Large Magellanic Cloud.\nOriginal abstractCompleted by GPT-4\nCompleted by LLaMA-2 Completed by AstroLLaMA\nFigure 2: Completion of an astronomy abstract from the arXiv database (ID: 2306.15719) using three different\nmodels: GPT-4, LLaMA-2, and AstroLLaMA. Each model is prompted with the same short text snippet, highlighted\nin their respective boxes, and then produces the rest of the abstract. Two authors of this paper subsequently judge\nthe quality of each completed abstract. GPT-4 tends to produce over-generic statements, while LLaMA-2 often\ngives off-topic generations. AstroLLaMA demonstrates the most robust completion, offering more relevant concepts\nand deeper insights specific to the field of astronomy, thus significantly outperforming LLaMA-2 and GPT-4.\nsource efficiency, we employ 4-bit quantization of\nthe model’s parameters and utilize LoRA, a fine-\ntuning technique based on low-rank matrix decom-\nposition (Hu et al., 2021). Specifically, we set\nLoRA’s hyperparametersα and dropout rate to 32\nand 0.05, respectively. This process is implemented\nusing Hugging Face’s library in Python.\n2.4 Fine-tuning evaluation\nFig. 1 depicts the performance of AstroLLaMA\nduring its fine-tuning phase. Here, we present per-\nplexity, a commonly used metric for evaluating\ncausal language models. Perplexity is defined as\nthe exponentiation of the training loss, with lower\nvalues indicating a better fit.\nOur initial observations reveal that LLaMA-2\nperforms suboptimally on our dataset, with an av-\nerage perplexity close to 10. By the conclusion of\nthree epochs, AstroLLaMA achieves an average\nperplexity of 6.55. This represents a 32.5% reduc-\ntion in perplexity compared to the base LLaMA-2\nmodel, signifying a substantial improvement in the\nmodel’s new-token prediction accuracy. Consider-\ning LLaMA-2 as a strong pre-trained baseline for\nlanguage modeling, we believe this performance\nimprovement is substantial in this application.\n3 Results\nAs illustrated in the previous section, AstroLLaMA\noutperforms its pre-trained counterpart, LLaMA-2,\nin terms of context-awareness during token predic-\ntion within astronomy abstracts. To delve deeper\ninto the advantages of fine-tuning, we assess As-\ntroLLaMA’s general abilities in two key aspects:\ntext generation and embedding space quality. We\ncompare its performance against multiple models,\nincluding LLaMA-2, GPT-4 and GPT-3 (ada-002)\nto provide a comprehensive evaluation.\n3.1 Text generation\nWe task AstroLLaMA, LLaMA-2 and GPT-4 with\ncompleting a number of astronomy abstracts, al-\nlowing us to gauge their ability to comprehend the\ncontext and generate a meaningful continuation.\nFig. 2 presents an example. In particular, we give\neach model the first few sentences of an abstract as\na prompt and use that model to generate the rest of\nthe abstract. For GPT-4, we utilize ChatGPT and\ninstruct it to limit the completion to a single para-\ngraph. AstroLLaMA and LLaMA-2 are deployed\nusing standard sampling methods, with the temper-\nature set to 0.3 and a maximum new tokens limit of\n1,024. We find that altering the temperature setting\n52\ndoes not substantively improve LLaMA-2’s results.\nOur observations on all generated abstracts\nlargely echo the patterns depicted in Fig. 2.\nLLaMA-2 frequently deviates from the intended\ncontext after generating only a short and often\noff-topic continuation, resulting in inferior com-\npletions. While GPT-4 produces more coherent\ntext, its responses are too generic to capture the\nnuanced understanding required in the astronomy\ndomain. Even when explicitly prompted to focus\non astronomy-related topics, GPT-4’s generated\ntext remains largely off-target or generically appli-\ncable rather than domain-specific.\nIn stark contrast, AstroLLaMA exhibits remark-\nable context-awareness in its completions by show-\ning a deep understanding of astronomical concepts.\nIn Fig. 2, for example, AstroLLaMA comprehends\nthat an effective search for stars in the Magellanic\nStream involves a three-step process: initial wide-\nfield imaging, followed by refinement using astro-\nmetric data from Gaia, and then further curation\nwith spectroscopic data. The model also under-\nstands Gaia-ESO is surveying the southern sky\nand hence can observe (part of) the Magellanic\nStream. It also demonstrates nuanced knowledge\nof the Magellanic Stream, understanding the impor-\ntance of bifurcation within the stream. As a result,\nit appropriately completes the text by discussing\nthe southeast stream and exploring metallicity dif-\nferences to ascertain their origins.\n3.2 Embedding space quality\nWe assess models’ ability to reflect semantic sim-\nilarities among astronomy texts. We randomly\nchoose 10,000 abstracts from our dataset and em-\nbed them using AstroLLaMA and GPT-3. Specif-\nically, we use OpenAI’s API to invoke the text\nembedding function for GPT-3 (ada-002). To get\ntext embeddings from AstroLLaMA, we pass an\ninput through the model and extract its final hidden\nstates, which contain embeddings for all tokens in\nthe input. Then, we omit the [BOS] token and take\nthe average of all other tokens’ embeddings to get\nthe final result. For each pair of abstracts we cal-\nculate their cosine similarity (the normalized dot\nproduct) between on their vector embeddings.\nThe top panel of Fig. 3 presents the distribution\nof these pairwise similarities for the two embed-\nding methods. We find that the embeddings by\nGPT-3 are overly generic with similarities cluster-\ning around relatively high values of 0.7–0.9, sug-\n0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nPairwise cosine similarity\n0\n100\n200\n300\n400\n500Density + Shift\nGPT-3 (ada) embedding\nAstroLLaMA embedding\nPaper 1: Astrophysical gyrokinetics: kinetic and ﬂuid turbulent cascades   \n               in magnetized weakly collisional plasma  \nPaper 2: Comment on modiﬁed Coulomb law in a strongly magnetised vaccum  \nGPT-3 cosine similarity score: 78.5%  \nAstroLLaMA cosine similarity score: 36.3%  \n  \nPaper 1: A Spitzer census of the IC 348 nebula  \nPaper 2: Sequential and spontaneous star formation around the mid-infrared  \n               halo HII region KR 14  \nGPT-3 cosine similarity score: 82.4%  \nAstroLLaMA cosine similarity score: 92.8%\nFigure 3: Top: Distribution of pairwise cosine similari-\nties among 10,000 randomly selected abstracts from our\ncorpus, divided into 10 equal bins based on similarity\nlevels from GPT-3. Bottom: Two representative exam-\nples illustrating divergent cosine similarity values when\ncomparing AstroLLaMA and GPT-3 embeddings.\ngesting a lack of discriminative power (most papers\nare embedded very similarly). AstroLLaMA’s em-\nbeddings, on the other hand, exhibit much higher\nvariance within each bin. This suggests that our\nfine-tuned model is more adept at representing the\nspecialized semantic variance inherent to the field\nof astronomy, which may enable a more granu-\nlar representation of astronomical content and can\nfacilitate higher-quality document retrieval and se-\nmantic analysis.\nThe bottom panel of Fig. 3 provides two repre-\nsentative examples where AstroLLaMA and GPT-3\nclassifications diverge. In the first example, GPT-3\nfixates on the keyword “magnetized,” resulting in\nan inflated similarity score despite the contents be-\ning markedly different. AstroLLaMA, on the other\nhand, successfully distinguishes between these dis-\nparate contexts. In the second example, AstroL-\nLaMA accurately identifies that the study of Spitzer\nis closely related to star formation. GPT-3, how-\never, fails to make this connection due to the ab-\nsence of matching keywords.\n4 Conclusion\nIn this work, we introduce AstroLLaMA, a 7-\nbillion-parameter language model fine-tuned on a\ndataset encompassing over 300,000 abstracts from\nastronomical research papers. Compared to its\nbase model, LLaMA-2, and even GPT-4, a cur-\n53\nrent state-of-the-art general LLM, AstroLLaMA\nexhibits marked improvements in generating high-\nquality abstracts and a competent grasp of relevant\ninformation in this specialized literature.\nThe efficacy of AstroLLaMA demonstrated in\nthis paper suggests a multitude of avenues wor-\nthy of exploration for subsequent work. With\nwell-curated instruction datasets, researchers can\nfine-tune our model to perform tasks such as ques-\ntion answering, scientific paper summarization and\nacademic writing assistance. Combining AstroL-\nLaMA with other information retrieval models can\nlead to promising systems for hypothesis genera-\ntion. Finally, AstroLLaMA is a potential candidate\nto be incorporated into specialized multi-modal\nmodels (Liu et al., 2023), going beyond the limits\nof text in astronomical research.\nAstroLLaMA, nevertheless, is not without lim-\nitations. During its evaluation, the most salient\ndrawback we find is the model’s knowledge gaps in\ncertain areas of astronomy. In Fig. 2, for example,\nAstroLLaMA’s estimation of potential star candi-\ndates from Gaia-ESO data is notably inaccurate.\nAnother concern lies in the model’s tendency to\ngenerate hallucinated or fictitious numerical data,\nan issue most likely attributed to our simple focus\non next-token prediction—a pure NLP objective—\nrather than explicitly steering the model toward\nfactual accuracy. Achieving a desirable balance of\n“faithfulness” (respecting scientific evidence and\naccuracy) and “creativity” (being able to come up\nwith interesting hypotheses) remains an open chal-\nlenge in research at the intersection of generative\nmodels and other scientific disciplines.\nThere are a number of on-going efforts to ad-\ndress the limitations of AstroLLaMA as well as\nexplore its broad capabilities in this sphere. We are\nin the process of enriching AstroLLaMA’s train-\ning data by including each paper’s full LaTeX\nsources, going beyond its abstracts and thereby\nincreasing the token count by approximately two\norders of magnitude. Although this requires a non-\ntrivial data quality control procedure, it will almost\ncertainly improve our model’s predictive perfor-\nmance substantially, making it even more adapted\nto this literature and less prone to hallucination.\nA more systematic evaluation of AstroLLaMA—\nincluding a larger set of candidate abstracts for\ncompletion, a more well-defined evaluation scheme\nand a larger, more diverse set of judging experts—\nwill lead to more grounded comparison with state-\nof-the-art models. Finally, the potential of AstroL-\nLaMA to generate high-quality and creative hy-\npotheses through novel prompting and fine-tuning\ntechniques is being extensively studied.\nAstroLLaMA stands as a compelling prototype\nfor specialized LLMs in astronomy, showing supe-\nrior context-aware capabilities compared to GPT-\n4 despite having much fewer parameters. Our\nmethodology is simple and general enough for re-\nsearcher to explore even more specific areas of\nastrophysics or even to be adapted to other areas of\nscientific research.\nWe have made AstroLLaMA’s weights, training\ndata and code for reproducibility publicly available\nto researchers who are aiming to leverage LLMs\nfor astronomy-centric applications. Along with\nthis, we are establishing various “playgrounds” on\nHugging Face to invite interested readers to ex-\nplore AstroLLaMA and further refine this robust\nstarting point for a variety of relevant downstream\napplications.b\nAcknowledgments\nWe thank the Microsoft Accelerate Foundation\nModels Academic Research Initiative. Access to\nadvanced AI capabilities from Microsoft Research\nhas greatly accelerated our work in applying lan-\nguage models to automate the analysis of the astro-\nnomical literature. We also thank the anonymous\nreviewers who gave useful insights and suggestions,\nespecially on the potential applications of AstroL-\nLaMA within and beyond astronomy.\nEthics Statement\nWe obtain the pre-trained weights for LLaMA-2\nfrom Meta, which offers these models for down-\nload on Hugging Face. The arXiv dataset used in\nthis paper is publicly available on Kaggle. While\nwe have demonstrated that AstroLLaMA is capa-\nble of generating high-quality, relevant abstracts\nfor astronomical research papers, we have noted\nthat it has the potential to generate inaccurate data\nand measurements. This should serve as a cau-\ntion for researchers aiming to use this model for\ndownstream tasks, and we invite the adoption of\nalignment strategies in future work to ameliorate\nthis issue.\nbAll details can be found at https://huggingface.co/\nuniverseTBD/astrollama.\n54\nReferences\nGoogle AI PaLM 2. https://ai.google/discover/palm2/.\nA. Accomazzi, M. J. Kurtz, E. A. Henneken, R. Chyla,\nJ. Luker, C. S. Grant, D. M. Thompson, A. Holachek,\nR. Dave, and S. S. Murray. 2015. ADS: The Next\nGeneration Search Platform. In Open Science at the\nFrontiers of Librarianship, volume 492 of Astronom-\nical Society of the Pacific Conference Series , page\n189.\nAndrés Almeida, Scott F. Anderson, Maria Argudo-\nFernández, Carles Badenes, Kat Barger, Jorge K.\nBarrera-Ballesteros, Chad F. Bender, Erika Benitez,\nFelipe Besser, Dmitry Bizyaev, Michael R. Blan-\nton, John Bochanski, Jo Bovy, William Nielsen\nBrandt, Joel R. Brownstein, Johannes Buchner, Esra\nBulbul, Joseph N. Burchett, Mariana Cano Díaz,\nJoleen K. Carlberg, Andrew R. Casey, Vedant Chan-\ndra, Brian Cherinka, Cristina Chiappini, Abigail A.\nCoker, Johan Comparat, Charlie Conroy, Gabriella\nContardo, Arlin Cortes, Kevin Covey, Jeffrey D.\nCrane, Katia Cunha, Collin Dabbieri, James W.\nDavidson Jr. au2, Megan C. Davis, Nathan De\nLee, José Eduardo Méndez Delgado, Sebastian De-\nmasi, Francesco Di Mille, John Donor, Peter Dow,\nTom Dwelly, Mike Eracleous, Jamey Eriksen, Xiao-\nhui Fan, Emily Farr, Sara Frederick, Logan Fries,\nPeter Frinchaboy, Boris T. Gaensicke, Junqiang\nGe, Consuelo González Ávila, Katie Grabowski,\nCatherine Grier, Guillaume Guiglion, Pramod Gupta,\nPatrick Hall, Keith Hawkins, Christian R. Hayes,\nJ. J. Hermes, Lorena Hernández-García, David W.\nHogg, Jon A. Holtzman, Hector Javier Ibarra-\nMedel, Alexander Ji, Paula Jofre, Jennifer A. John-\nson, Amy M. Jones, Karen Kinemuchi, Matthias\nKluge, Anton Koekemoer, Juna A. Kollmeier, Marina\nKounkel, Dhanesh Krishnarao, Mirko Krumpe, Ivan\nLacerna, Paulo Jakson Assuncao Lago, Chervin La-\nporte, Ang Liu, Chao Liu, Xin Liu, Alexandre Roman\nLopes, Matin Macktoobian, Viktor Malanushenko,\nDan Maoz, Thomas Masseron, Karen L. Masters,\nGal Matijevic, Aidan McBride, Ilija Medan, An-\ndrea Merloni, Sean Morrison, Natalie Myers, Sz-\nabolcs Mészáros, C. Alenka Negrete, David L. Nide-\nver, Christian Nitschelm, Audrey Oravetz, Daniel\nOravetz, Kaike Pan, Yingjie Peng, Marc H. Pin-\nsonneault, Rick Pogge, Dan Qiu, Anna Barbara\nde Andrade Queiroz, Solange V . Ramirez, Hans-\nWalter Rix, Daniela Fernández Rosso, Jessie Run-\nnoe, Mara Salvato, Sebastian F. Sanchez, Felipe A.\nSantana, Andrew Saydjari, Conor Sayres, Kevin C.\nSchlaufman, Donald P. Schneider, Axel Schwope,\nJavier Serna, Yue Shen, Jennifer Sobeck, Ying-Yi\nSong, Diogo Souto, Taylor Spoo, Keivan G. Stassun,\nMatthias Steinmetz, Ilya Straumit, Guy Stringfellow,\nJosé Sánchez-Gallego, Manuchehr Taghizadeh-Popp,\nJamie Tayar, Ani Thakar, Patricia B. Tissera, An-\ndrew Tkachenko, Hector Hernandez Toledo, Benny\nTrakhtenbrot, Jose G. Fernandez Trincado, Nicholas\nTroup, Jonathan R. Trump, Sarah Tuttle, Natalie Ul-\nloa, Jose Antonio Vazquez-Mata, Pablo Vera Alfaro,\nSandro Villanova, Stefanie Wachter, Anne-Marie\nWeijmans, Adam Wheeler, John Wilson, Leigh Wo-\njno, Julien Wolf, Xiang-Xiang Xue, Jason E. Ybarra,\nEleonora Zari, and Gail Zasowski. 2023. The eigh-\nteenth data release of the sloan digital sky surveys:\nTargeting and first spectra from sdss-v.\nChristine L. Borgman and Morgan F. Wofford. 2021.\nFrom Data Processes to Data Products: Knowledge\nInfrastructures in Astronomy. arXiv e-prints.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. PaLM: Scaling Language\nModeling with Pathways.\nIoana Ciuc ˘a and Yuan-Sen Ting. 2023. Galactic\nChitChat: Using Large Language Models to Con-\nverse with Astronomy Literature. arXiv e-prints.\nIoana Ciuc˘a, Yuan-Sen Ting, Sandor Kruk, and Kartheik\nIyer. 2023. Harnessing the Power of Adversarial\nPrompting and Large Language Models for Robust\nHypothesis Generation in Astronomy. arXiv e-prints.\nC. Fabricius, X. Luri, F. Arenou, C. Babusiaux,\nA. Helmi, T. Muraveva, C. Reylé , F. Spoto,\nA. Vallenari, T. Antoja, E. Balbinot, C. Barache,\nN. Bauchet, A. Bragaglia, D. Busonero, T. Cantat-\nGaudin, J. M. Carrasco, S. Diakité, M. Fabrizio,\nF. Figueras, A. Garcia-Gutierrez, A. Garofalo,\nC. Jordi, P. Kervella, S. Khanna, N. Leclerc, E. Li-\ncata, S. Lambert, P. M. Marrese, A. Masip, P. Ramos,\nN. Robichon, A. C. Robin, M. Romero-Gómez,\nS. Rubele, and M. Weiler. 2021. igaia/iearly data\nrelease 3. Astronomy & Astrophysics, 649:A5.\nFelix Grezes, Sergi Blanco-Cuaresma, Alberto Acco-\nmazzi, Michael J. Kurtz, Golnaz Shapurian, Edwin\nHenneken, Carolyn S. Grant, Donna M. Thomp-\nson, Roman Chyla, Stephen McDonald, Timothy W.\nHostetler, Matthew R. Templeton, Kelly E. Lockhart,\nNemanja Martinovic, Shinyi Chen, Chris Tanner, and\nPavlos Protopapas. 2021. Building astroBERT, a\nlanguage model for Astronomy & Astrophysics.\n55\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. LoRA: Low-Rank Adaptation\nof Large Language Models. arXiv e-prints.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for Neural Text Processing.\nIn Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023. Visual instruction tuning.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nWeight Decay Regularization. In International Con-\nference on Learning Representations.\nMeta. 2023. Llama 2: Open Foun-\ndation and Fine-Tuned Chat Models.\nhttps://ai.meta.com/research/publications/llama-2-\nopen-foundation-and-fine-tuned-chat-models/.\nOpenAI. 2023. GPT-4 Technical Report.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. LLaMA: Open\nand Efficient Foundation Language Models.",
  "topic": "Miller",
  "concepts": [
    {
      "name": "Miller",
      "score": 0.751718282699585
    },
    {
      "name": "Art history",
      "score": 0.6083193421363831
    },
    {
      "name": "Foundation (evidence)",
      "score": 0.5774304270744324
    },
    {
      "name": "Art",
      "score": 0.44413623213768005
    },
    {
      "name": "Humanities",
      "score": 0.3744693994522095
    },
    {
      "name": "Environmental ethics",
      "score": 0.3629339337348938
    },
    {
      "name": "Library science",
      "score": 0.35455024242401123
    },
    {
      "name": "Philosophy",
      "score": 0.2867298722267151
    },
    {
      "name": "Computer science",
      "score": 0.25703519582748413
    },
    {
      "name": "Archaeology",
      "score": 0.2338079810142517
    },
    {
      "name": "History",
      "score": 0.20911285281181335
    },
    {
      "name": "Geology",
      "score": 0.09358787536621094
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I118347636",
      "name": "Australian National University",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I36788626",
      "name": "California University of Pennsylvania",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I52357470",
      "name": "The Ohio State University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210093174",
      "name": "European Space Astronomy Centre",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I4210120068",
      "name": "IBM Research - Australia",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I1297127228",
      "name": "Space Telescope Science Institute",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I78577930",
      "name": "Columbia University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I68891433",
      "name": "Indian Institute of Technology Delhi",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I2801851002",
      "name": "Harvard University Press",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4389424196",
      "name": "Deleted Institution",
      "country": null
    },
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I45129253",
      "name": "University College London",
      "country": "GB"
    }
  ],
  "cited_by": 10
}