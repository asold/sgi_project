{
  "title": "Ground Manipulator Primitive Tasks to Executable Actions Using Large Language Models",
  "url": "https://openalex.org/W4391116616",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2104077081",
      "name": "Yue Cao",
      "affiliations": [
        "Purdue University West Lafayette"
      ]
    },
    {
      "id": "https://openalex.org/A2131747406",
      "name": "C. S. George Lee",
      "affiliations": [
        "Purdue University West Lafayette"
      ]
    },
    {
      "id": "https://openalex.org/A2104077081",
      "name": "Yue Cao",
      "affiliations": [
        "Purdue University West Lafayette"
      ]
    },
    {
      "id": "https://openalex.org/A2131747406",
      "name": "C. S. George Lee",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2885909793",
    "https://openalex.org/W1983910986",
    "https://openalex.org/W2105227386",
    "https://openalex.org/W6636725810",
    "https://openalex.org/W6653203626",
    "https://openalex.org/W4322716420",
    "https://openalex.org/W2585691643",
    "https://openalex.org/W1508977358",
    "https://openalex.org/W6685086406",
    "https://openalex.org/W1552395109",
    "https://openalex.org/W4221152848",
    "https://openalex.org/W1534401054",
    "https://openalex.org/W2001006503",
    "https://openalex.org/W2129980106",
    "https://openalex.org/W2105660272",
    "https://openalex.org/W46490633",
    "https://openalex.org/W2126284968",
    "https://openalex.org/W2144717132",
    "https://openalex.org/W6627195111",
    "https://openalex.org/W6785894819",
    "https://openalex.org/W6654846027",
    "https://openalex.org/W2134831400",
    "https://openalex.org/W2277684984",
    "https://openalex.org/W2236233024",
    "https://openalex.org/W3139333591",
    "https://openalex.org/W3097806205",
    "https://openalex.org/W2016958754",
    "https://openalex.org/W4385430679",
    "https://openalex.org/W2012187514",
    "https://openalex.org/W1652032257",
    "https://openalex.org/W4377130745",
    "https://openalex.org/W4383108457",
    "https://openalex.org/W4388660746",
    "https://openalex.org/W3100118710",
    "https://openalex.org/W4224912544",
    "https://openalex.org/W2151992157",
    "https://openalex.org/W3207601508",
    "https://openalex.org/W2171565246"
  ],
  "abstract": "Layered architectures have been widely used in robot systems. The majority of them implement planning and execution functions in separate layers. However, there still lacks a straightforward way to transit high-level tasks in the planning layer to the low-level motor commands in the execution layer. In order to tackle this challenge, we propose a novel approach to ground the manipulator primitive tasks to robot low-level actions using large language models (LLMs). We designed a program-function-like prompt based on the task frame formalism. In this way, we enable LLMs to generate position/force set-points for hybrid control. Evaluations over several state-of-the-art LLMs are provided.",
  "full_text": "Ground Manipulator Primitive Tasks to Executable Actions\nUsing Large Language Models\nYue Cao and C.S. George Lee\nElmore Family School of Electrical and Computer Engineering, Purdue University\n{yuecao, csglee}@purdue.edu\nAbstract\nLayered architectures have been widely used in robot sys-\ntems. The majority of them implement planning and execu-\ntion functions in separate layers. However, there still lacks\na straightforward way to transit high-level tasks in the plan-\nning layer to the low-level motor commands in the execution\nlayer. In order to tackle this challenge, we propose a novel\napproach to ground the manipulator primitive tasks to robot\nlow-level actions using large language models (LLMs). We\ndesigned a program-function-like prompt based on the task\nframe formalism. In this way, we enable LLMs to generate\nposition/force set-points for hybrid control. Evaluations over\nseveral state-of-the-art LLMs are provided.\nIntroduction\nIn robotics, the layered architectures break down the sys-\ntem design into multiple layers, then separately configure\neach layer. The development of layered architectures can be\ntraced back to the Stanford Research Institute’s Shakey mo-\nbile robot (Nilsson 1969). Its system was programmed by\nhigh-level functions solving planning problems in first-order\nlogic and low-level functions specifying motor commands.\nBy the early 1990s, a number of layered architectures have\nbeen developed and summarized as the sense-model-plan-\nact (SMPA) paradigm in (Brooks 1991a). Nowadays, the\nlayered architectures typically consisting of a planning layer\nand an execution player have been widely used in many\nrobot systems (Kortenkamp, Simmons, and Brugali 2016).\nHowever, there is a long-lasting challenge in the layered\nsystem – the transition from planning to execution. Back\nin 1987, Rodney Brooks had pointed out the drawbacks of\nseparating planning and execution layers (Brooks 1987). To\ndeal with this issue, Brooks proposed a biological-system-\ninspired subsumption architecture (Brooks 1991b). But af-\nter years, the subsumption architecture didn’t achieve much\nprogress in modern robot control. Most robot systems stay\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n†This work was supported in part by the National Science\nFoundation under Grant IIS-1813935. Any opinion, findings, and\nconclusions or recommendations expressed in this material are\nthose of the authors and do not necessarily reflect the views of the\nNational Science Foundation.\nwith the layered architectures and one common solution for\nthe planning-to-execution transition problem is using prim-\nitive tasks. As early as in the Shakey robot, it introduced a\nlibrary of intermediate-level actions such as “push object”\nor “go through door” to communicate between the high-\nlevel logic and low-level motor commands (Nilsson 1984).\nA classic 3-layer architecture in (Firby 1989) was separated\ninto high-level planning, middle-level execution, and low-\nlevel hardware control layers. Its middle-level layer imple-\nments a reactive-action-package (RAP) system (Firby 1992)\nconsisting of primitive tasks to bridge between the symbolic\nplans and low-level control actions. Nevertheless, the exist-\ning primitive-task-based approaches require laborious man-\nual specification. They usually yield in some libraries of a\nsmall number of primitive tasks, thereby limiting the task\nadaptability and generalizability of robots.\nTo seek a new method for the planning-to-execution tran-\nsition problem, we turn our attention to the state of the\nart – large language models (LLMs). The large language\nmodels are neural-network-based language models with a\nhuge number of parameters and trained over massive amount\nof data. They significantly outperform other natural lan-\nguage processing (NLP) models and exhibit remarkable ca-\npabilities in generating textual responses to a broad range\nof questions. Since the release of OpenAI ChatGPT, the\nemergent abilities of LLMs have drawn the interests of re-\nsearchers from a variety of disciplines, including robotics.\nIn robotics, pilot studies have primarily focused on the plan-\nning layer (Huang et al. 2022; Ahn et al. 2022; Cao and Lee\n2023; Xiang et al. 2023; Lin et al. 2023). They utilized the\nlarge language models to generate high-level task plans in\na zero shot or few shot manner. However, the generated ab-\nstract plans are described in natural languages, making them\nuninterpretable by the execution layer.\nIn this paper, we focus on the manipulator tasks and study\nthe ability of LLMs to solve the planning-to-execution tran-\nsition problem. Specifically, given a manipulator primitive\ntask described in the natural language domain, we seek a\nLLM-based solution to translate it to low-level motor com-\nmands as shown in Figure 1. We propose a novel approach\nto ground textual manipulator primitive tasks to executable\nactions using LLMs. Specifically, we design a program-like\nprompt based on the task frame formalism (TFF), an object-\ncentric specification that facilitates task transfer across dif-\nAAAI Fall Symposium Series (FSS-23)\n502\n(a)\n(b)\nFigure 1: (a) Previous work in LLM & robotics mainly stud-\nied task generation in the planning layer. (b) Our proposed\napproach targets the planning-to-execution transition prob-\nlem.\nferent manipulators. The prompt takes the text of a primitive\ntask as input and outputs a set of position/force set-points\nin the task frame. These position/force set-points will allow\nmanipulators to compute their low-level motor commands.\nMajor contributions of this paper are:\n(1) We propose a LLM-based approach that enables\nlanguage-described manipulator primitive tasks to be\nconverted to the set-points for robot position/force hy-\nbrid control. It offers a new solution for the planning-to-\nexecution transition problem in layered architectures of\nmanipulators.\n(2) The remarkable generalizability of LLMs makes it possi-\nble to apply many manipulator primitive tasks described\nin natural language within our approach. Hence, it has\nthe potential to reduce the burden on end-users when it\ncomes to designing a complex library of primitive tasks.\nRelated Work\nLanguage Grounding to Robot Actions\nLanguage grounding was introduced to robotics in (Roy\n2005) to establish connections between natural languages\nand robot capabilities, including perception and action. Lan-\nguage grounding to action is generally referred to as asso-\nciating high-level language commands with the low-level\nrobot control system (Chai et al. 2016). In the area of lan-\nguage grounding to action, the study of grounding schema\nfollows one step behind the development of NLP techniques.\nThe early work in (Kress-Gazit, Fainekos, and Pappas\n2008) enforced strict grammar rules to transform task spec-\nifications to linear temporal logic form. Later on, grammar\nparsers (Tellex et al. 2011; Matuszek et al. 2012) such as\nthe Stanford parser (de Marneffe, MacCartney, and Manning\n2006) were applied to decompose sentence instructions into\ndifferent grammatical elements, mainly involving verbs and\nobjects. Subsequently, word-based statistical machine trans-\nlation models were investigated in (Squire et al. 2015) to en-\nable generalization to new environments. In the beginning of\nthe deep-learning era, word embeddings and recurrent neu-\nral networks (RNNs) became the main steam and were later\nintroduced in the robot grounding schema (Arumugam et al.\n2019; Toyoda et al. 2021).\nSince the 2020s, large language models have achieved\ntremendous progress and become the primary focus of NLP.\nUsing LLMs, one can bypass analyzing grammatical struc-\nture or word semantics, as was used in previous work.\nTask Frame Formalism\nThe concept of the task frame formalism originated from\nthe study of compliant motions (Mason 1981). It was then\nexplicitly formulated as an intermediate transfer between\ntask planning and force/position control (Bruyninckx and\nDe Schutter 1996). The task frame, also known as the com-\npliance frame, is a local coordinate frame attached to the\nobject being manipulated. Its translational and rotational di-\nrections are configured to be either force controlled or po-\nsition controlled. Such formalism allows end-users to im-\nplement hybrid position/force control strategy (Raibert and\nCraig 1981) alongside separate frame directions.\nBased on the task frame formalism, a variety of manipu-\nlator primitive tasks have been explored (Ballard and Hart-\nman 1986; Morrow and Khosla 1997; Kroger, Finkemeyer,\nand Wahl 2004; Vuong, Pham, and Pham 2021). Neverthe-\nless, they mainly focused on the taxonomy of low-level ac-\ntion representation and provided little consideration to the\nassociation with high-level task nomenclature. For example,\nmanipulator primitive tasks were specified as “rotate about\ny 5 deg” or “translate along x until next contact” in (Vuong,\nPham, and Pham 2021). But there is no straightforward\nway to translate natural-language-described tasks like “in-\nsert key” or “open bottle” to such low-level movements.\nProposed Approach\nPreprocess: Identify Manipulator Primitive Tasks\nTo start with, we need to ensure that the given primitive\ntask conforms to the task frame formalism. For instance, we\ngenerally consider tasks like “insert peg” or “open bottle”\nas primitive tasks because they can be accomplished by a\nsingle control strategy without changing the coordinate set-\nting. Tasks such as “assembly GPU” or “make coffee” are\nexamples of non-primitive tasks because they must be fur-\nther decomposed into multiple steps that use different con-\ntrol strategies and coordinates.\nWe use LLMs to assess the primitiveness of tasks. In fact,\ncurrent LLMs including OpenAI ChatGPT, Google Bard,\nand Meta LLaMA-2, have some knowledge of the task frame\nformalism. For example, they can respond to the following\nprompt with reasonable answers.\nDo you understand task frame formalism in robotics?\nBut when it comes to identifying the manipulator primi-\ntive tasks, their criteria become inconsistent. Therefore, we\nwrite a prompt including detailed explanation to query the\nLLM, as shown below.\nWe specify primitive tasks based on the concept of task\nframe formalism in robotics.\nFor example, “open door”, “slice bread”, “insert card”\nare primitive tasks because they use a single control\nstrategy and coordinate setting. While “assembly com-\n503\nputer”, “make coffee”, “drive car” are not primitive\ntasks because they must be decomposed into several\nlow-level tasks in single control strategies and coordi-\nnate settings.\nSo, is “open bottle” a primitive task? Just answer yes\nor no.\nWith such prompt, the LLMs can effectively determine\nwhether a given manipulator task is primitive or not. If a\ncertain task is not primitive, we need to perform task decom-\nposition. Since the task decomposition is out of the scope of\nthis work, we refer to other literature (Huang et al. 2022;\nAhn et al. 2022; Cao and Lee 2023).\nTFF-based Prompt Design\nAfter obtaining a manipulator primitive task, we use LLMs\nto transit it to low-level execution in a TFF fashion. Since\nLLMs tend to produce lengthy answers and detailed expla-\nnations to casual questions, we need to design a specific\nprompt to regulate their output into a structured form. In-\nspired by the work (Singh et al. 2022) that uses code blocks\nfor robot-plan generation, we design our prompt in a func-\ntion format in computer programming. The details of our\nprompt design are listed below, with a Python example in\nFigure 2.\nFigure 2: An exemplary 3-shot generation in Python syntax.\nThe 3-shot prompt is shown in the grey background. Details\nin the source function 2 and 3 are skipped. The text gen-\nerated by LLM is displayed in the green background. Note\nthat the prompt can also be written in other common pro-\ngramming languages. More source functions are preferred.\n• Outside of the function, we use program comments to in-\ndicate whether a function is set for source specification or\ntarget generation. We recommend using multiple source\nfunctions with different TFF configurations to provide\nbetter guidance for the LLMs.\n• The function name is configured as the text of the prim-\nitive task. For example, the task “turn screw” is written\nas “def turn\nscrew( )”. Stop words such as “a” and “the”\ncan be removed from the task name. In addition, the func-\ntion parameters are specified as six-directional motions\nin the task frame formalism.\n• In the function, we first set the coordinates in the function\ncomment. Then, we designate position/force set-points in\na TFF manner. If one direction is inactive, we simply as-\nsign 0 as its value. For the active directions, we use dif-\nferent units to distinguish whether they are position- or\nforce-controlled. For the example in Figure 2, we activate\ntwo out of the six directions using “translational\nz=−5 #\nN” and “angular z=5 # rad/sec”. This specification im-\nplies that we rotate the screw about thez axis at a velocity\nof 5 rad/sec while simultaneously applying a downward\nforce of 5 newtons.\n• The target function sets the desired manipulator primi-\ntive task that we want to generate for. In this function,\nthe target task name is the only element that needs to be\nfilled.\nOnce we input the whole prompt to a LLM, it will au-\ntomatically generate the coordinate setting and specification\nof six-directional motions for the given target task. Using the\ngenerated TFF-based coordinates and set-points, robots can\ncompute control commands based on their own manipulator\nconfigurations.\nEvaluation\nIn this section, we evaluate our TFF-based prompt with 4\nstate-of-the-art large language models – OpenAI GPT-3.5-\nturbo, OpenAI GPT-4, Google Bard, and Meta LLaMA-2-\n70B.\nEvaluation Setting\nFor the GPT-3.5-turbo and GPT-4 models, we applied the\nsame setting. The temperature hyperparameter was set to be\n0 to minimize the randomness. The top P, frequency penalty,\nand presence penalty hyperparameters were configured to 1,\n0, and 0, respectively.\nFor the Google Bard model, there is no hyperparameter\nconfiguration available towards end-users. Thereby we can-\nnot guarantee the deterministicness of the generated out-\ncomes. We entered the prompt in its web GUI and only\nrecorded the initial results generated.\nThe Meta LLaMA-2 model is the most recently released\nLLM among them. We chose its 70B (70 billion) version, the\none with the most parameters. We also set its temperature\nhyperparameter to the minimum value 0.01. The repetition\npenalty was set to 1 to grant repeated words with no penalty.\nAnother hyperparameter top P was set to 1.\nWe assessed the proposed prompt in a set of 30 manip-\nulator primitive tasks as listed in Table 1. We conducted\n504\n1. cut pizza 11. rasp wood 21. open door from hinge\n2. scrub desk with bench brush 12. scrape substance from surface 22. slide block over vertical surface\n3. spear cake with fork 13. peel potato 23. turn steering wheel\n4. fasten screw with screwdriver 14. slice cucumber 24. shake cocktail bottle\n5. loosen screw with screwdriver 15. flip bread 25. cut banana\n6. unlock lock with key 16. shave object 26. crack egg\n7. fasten nut with wrench 17. use roller to roll out dough 27. press button\n8. loosen nut with wrench 18. insert peg into pegboard 28. insert GPU into socket\n9. spread paint with brush 19. brush across tray 29. open bottle\n10. hammer in nail 20. insert straw through cup lid 30. open childproof bottle\nTable 1: Manipulator primitive tasks used in our evaluation. The tasks in the first two columns were selected from the Daily\nInteractive Manipulation dataset (Paulius, Eales, and Sun 2020) with minor modification, while the tasks listed in the third\ncolumn were created by us.\ntests in 0-shot, 1-shot, 3-shot and 5-shot manners. The 5-\nshot prompt was consisted of 5 source tasks as outlined in\nthe block below.\n# Source function 1\ndef turn\nscrew(...)\n...\n# Source function 2\ndef wipe\ntable(...)\n...\n# Source function 3\ndef open\ndoor from doorknob(...)\n...\n# Source function 4\ndef cut\nsandwich(...)\n...\n# Source function 5\ndef slide\nbox upward on wall(...)\n...\nFor simplicity, we only show the task indicators and func-\ntion names in this block while omitting other details. The\n3-shot prompt utilized the first three source tasks from the\n5-shot prompt, whereas the 1-shot prompt only used the first\none from it.\nResults\nThe evaluations for GPT and Bard were conducted in early\nJuly, 2023. The test for LLAMA-2 was carried out in late\nJuly, 2023, shortly after its release. In order to validate the\ngenerated TFF specifications, We manually set up metrics\nfor all 30 manipulator primitive tasks. The details of our\nmetrics can be found in Appendix. The overall correct rates\nare reported in Table 2.\n0-shot 1-shot 3-shot 5-shot\nGPT-3.5-turbo 0 0.30 0.70 0.67\nGPT-4 0 0.47 0.63 0.83\nBard 0 0 0.47 0.70\nLLaMA-2-70B 0 0 0.07 0.13\nTable 2: Correct rates in the evaluation over 30 manipulator\nprimitive tasks.\nFrom the results, we notice that none of LLMs were able\nto produce a valid response in the 0-shot test. When we\nincreased the number of source tasks to 5, GPT-3.5-turbo,\nGPT-4, and Bard generated plenty of correct answers. No-\ntably, the GPT-4 in the 5-shot setting achieved the highest\ncorrect rate of 0.83. Meanwhile, the LLaMA-2-70B failed\nthe majority of tests. For the 5-shot tests, the correctness of\neach manipulator primitive task is shown in Table 3.\nTask No. Correctness\nGPT-3.5-turbo\n1-10 •••••◦••••\n11-20 ◦◦◦••◦◦•••\n21-30 •◦•◦•◦•••◦\nGPT-4\n1-10 ••••••••••\n11-20 ••◦••◦•••◦\n21-30 •••••◦•••◦\nBard\n1-10 •◦•••••◦••\n11-20 ◦◦••••◦••◦\n21-30 •◦••◦◦••••\nLLaMA-2-70B\n1-10 •◦◦◦•◦◦◦◦◦\n11-20 ◦◦◦◦•◦◦◦◦◦\n21-30 ◦◦◦◦•◦◦◦◦◦\nTable 3: The correctness of each manipulator primitive task\nin the 5-shot tests. Solid circles indicate the correct genera-\ntion for tasks, while hollow circles represent incorrect ones.\nTask No. corresponds to the numbering in Table 1, with ev-\nery 10 tasks sequentially lined-up in a row.\nFor GPT-3.5-turbo, GPT-4, and Bard models, they strictly\nfollowed the desired “# Coordinate setting; translational ...;\nangular ... ” format even since the one-shot prompt. Both\nGPT models can also figure out some task specification just\ndepending on the 1-shot prompt. For the Bard in the 1-\nshot setting, most of its target task specifications essentially\nduplicated the direction setting from the source task “turn\nscrew.” The only improvisational part was the new object\nregarding the z axis in the coordinate setting.\nNext, we take a close look into the GPT-4 5-shot case, the\none with the best performance. In total, its five failed cases\nwere “peel potato”, “shave object”, “insert straw through\ncup lid”, “crack egg”, and “open childproof bottle.” These\nfailed tasks are quite divergent from the source tasks and\n505\npose more challenges to LLMs. Take the “insert straw\nthrough cup lid” task as an example, the GPT-4 generated\na single active direction “translational z = -5 # N ”, which\nimplied using force control in the straw-insertion direction.\nHowever, this task should be position controlled because the\nstraw has no interaction with any outside rigid object and\nthere is only minor friction between the straw and the cup\nlid. For another failed task “open childproof bottle”, it is\nsupposed to have the opposite force direction compared to\nthe normal “open bottle” task. However, the GPT-4 model\ngenerated an identical answer for these two tasks.\nRegarding the LLaMA-2-70B, our current prompt has dif-\nficulty in guiding it to the desired specification. Even up to\nthe 5-shot prompt, it barely generated any suitable response.\nOne typical response is shown below. Note that some re-\ndundant details of the function parameters are skipped using\n“...”.\n# Combine the five source functions\nturn\nscrew(...)\nwipe table(...)\nopen door from doorknob(...)\ncut sandwich(...)\nslide box upward on wall(...)\nThe LLaMA-2-70B just repeatedly called the source func-\ntions. It was not capable of generating any new coordinate\nsetting or position/force set-points with our prompt.\nIn addition to focusing our attention on the direction set-\ntings, we also inspected the numerical values that were gen-\nerated. We used values such as5 (rad/sec), 5 (N) and 2 (cm)\nin the prompt. Most of the generated values were close to\nthese ones and sometimes reached 10. Among all generated\nvalues in the 5-shot tests, we noticed two unusual values,\nboth produced by Bard – 1) “hammer in nail” task: “transla-\ntional\nz = 50 # N”, 2) “cut banana” task: “angular z = 100 #\nrad/sec”. In the hammer case, it is reasonable to set a large\nforce. However, the direction setting the banana cutting case\nis incorrect, and more notably, the generated100 (rad/s) set-\npoint is excessively high for common manipulators to exe-\ncute. This raises an additional concern: if the LLMs gener-\nate abnormal set-point values, might it potentially result in\nsafety issues in robot execution?\nConclusions and Discussions\nThis paper presented a LLM-based approach to convert tex-\ntual manipulator primitive tasks to TFF-based position/force\nset-points. By integrating the generalizability of LLMs and\nthe cross-task transferability of TFF together, we have pro-\nvided a new solution for the planning-to-execution transition\nproblem in layered architectures. The evaluations showed\nthe effectiveness of our prompt in three LLMs – OpenAI\nGPT-3.5-turbo, OpenAI GPT-4, and Google Bard.\nSince the correct rate increases as more source tasks are\noffered, we suggest building a more comprehensive prompt\nthat includes tasks in all contact and movement types. In ad-\ndition, further study can be conducted to guide the LLaMA-\n2 model in producing correct specifications.\nFurthermore, the evaluation on the LLM-generated con-\ntent remains a challenging problem. We believe that there\nexists a necessity to design a comprehensive and standard-\nized evaluation set for assessing the language grounding to\nactions. The recent work of Google RT-1 (Brohan et al.\n2023) established a large-scale real-world dataset connect-\ning images and natural language instructions with manipu-\nlator actions but is limited to a few verbs. In the future, we\nwill establish an evaluation set that includes a broader range\nof verbs that can better capture the contact-rich characteris-\ntics of manipulators.\nAppendix: Evaluation Metrics\nIn this appendix, we provide detailed metrics used in the\nEvaluation section. In certain cases, the x, y, and z axes can\nbe interchangeable depending on the coordinate setting.\n1. cut pizza: 1 translational direction activated.\n2. scrub desk with bench brush: 1 or 2 translational direc-\ntions activated, must apply force on plane.\n3. spear cake with fork: only z translational direction\nactivated.\n4. fasten screw with screwdriver: z angular direction\nactivated.\n5. loosen screw with screwdriver: opposite as Task 4.\n6. unlock lock with key: z angular direction activated.\n7. fasten nut with wrench: z angular direction activated.\n8. loosen nut with wrench: opposite as Task 7.\n9. spread paint with brush: translational direction on x − y\nplane activated.\n10. hammer in nail: only z translational direction activated.\n11. rasp wood: same as Task 2.\n12. scrape substance from surface: same as Task 2.\n13. peel potato: 1 translational direction activated, must\napply force on plane.\n14. slice cucumber: same as Task 1.\n15. flip bread: 1 angular direction activated.\n16. shave object: same as Task 2.\n17. use roller to roll out dough: same as Task 2.\n18. insert peg into pegboard: same as Task 3.\n19. brush across tray: same as Task 2 or Task 9.\n20. insert straw through cup lid: 1 translational direction\nactivated, position control.\n21. open door from hinge: only z angular direction acti-\nvated.\n22. slide block over vertical surface:z translational direction\nactivated, must apply force on x − y plane.\n23. turn steering wheel: same as Task 6.\n24. shake cocktail bottle: angular direction activated,\nposition control.\n25. cut banana: same as Task 1.\n26. crack egg: same as Task 20.\n27. press button: same as Task 3.\n28. insert GPU into socket: same as Task 3.\n29. open bottle: same as Task 4.\n30. open childproof bottle: add downside force versus Task\n29.\n506\nReferences\nAhn, M.; Brohan, A.; Brown, N.; Chebotar, Y .; Cortes, O.;\nDavid, B.; Finn, C.; Gopalakrishnan, K.; Hausman, K.; Her-\nzog, A.; et al. 2022. Do As I Can, Not As I Say: Ground-\ning Language in Robotic Affordances. arXiv preprint\narXiv:2204.01691.\nArumugam, D.; Karamcheti, S.; Gopalan, N.; Williams,\nE. C.; Rhee, M.; Wong, L. L. S.; and Tellex, S. 2019.\nGrounding Natural Language Instructions to Semantic Goal\nRepresentations for Abstraction and Generalization. Auton.\nRobots, 43(2): 449–468.\nBallard, D. H.; and Hartman, L. 1986. Task Frames: Primi-\ntives for Sensory-Motor Coordination. Comput. Vis. Graph.\nImage Process., 36(2-3): 274–297.\nBrohan, A.; Brown, N.; Carbajal, J.; et al. 2023. RT-1:\nRobotics Transformer for Real-World Control at Scale. In\nRobotics: Science and Systems.\nBrooks, R. A. 1987. Planning is Just a Way of Avoiding\nFiguring Out What To Do Next. Working Papers WP-303,\nMIT Artificial Intelligence Laboratory.\nBrooks, R. A. 1991a. Intelligence Without Reason. In Proc.\nInt. Joint Conf. Artif. Intell. (IJCAI), 569–595.\nBrooks, R. A. 1991b. New Approaches to Robotics.Science,\n253(5025): 1227–1232.\nBruyninckx, H.; and De Schutter, J. 1996. Specification of\nForce-Controlled Actions in the “Task Frame Formalism”-A\nSynthesis. IEEE Trans. Robotics Autom., 12(4): 581–589.\nCao, Y .; and Lee, C. S. G. 2023. Robot Behavior-Tree-Based\nTask Generation with Large Language Models. In Proc.\nAAAI Spring Symp. Challenges Requiring the Combination\nof Machine Learning and Knowledge Engineering.\nChai, J. Y .; Fang, R.; Liu, C.; and She, L. 2016. Collabo-\nrative Language Grounding Toward Situated Human-Robot\nDialogue. AI Mag., 37(4): 32–45.\nde Marneffe, M.; MacCartney, B.; and Manning, C. D. 2006.\nGenerating Typed Dependency Parses from Phrase Structure\nParses. In Proc. Int. Conf. Language Resources and Evalu-\nation, (LREC), 449–454.\nFirby, R. J. 1989. Adaptive Execution in Complex Dynamic\nWorlds. PhD Thesis, Yale University.\nFirby, R. J. 1992. Building Symbolic Primitives with Con-\ntinuous Control Routines. In Artificial Intelligence Planning\nSystems, 62–69. Elsevier.\nHuang, W.; Abbeel, P.; Pathak, D.; and Mordatch, I. 2022.\nLanguage Models as Zero-Shot Planners: Extracting Action-\nable Knowledge for Embodied Agents. In Proc. Int. Conf.\nMachine Learning (ICML), 9118–9147.\nKortenkamp, D.; Simmons, R. G.; and Brugali, D. 2016.\nRobotic Systems Architectures and Programming. In Si-\nciliano, B.; and Khatib, O., eds., Springer Handbook of\nRobotics, 283–306. Springer.\nKress-Gazit, H.; Fainekos, G. E.; and Pappas, G. J. 2008.\nTranslating Structured English to Robot Controllers. Adv.\nRobotics, 22(12): 1343–1359.\nKroger, T.; Finkemeyer, B.; and Wahl, F. M. 2004. A Task\nFrame Formalism for Practical Implementations. In Proc.\nIEEE Int. Conf. Robot. Autom. (ICRA), volume 5, 5218–\n5223.\nLin, K.; Agia, C.; Migimatsu, T.; Pavone, M.; and Bohg, J.\n2023. Text2Motion: From Natural Language Instructions to\nFeasible Plans. arXiv preprint arXiv:2303.12153.\nMason, M. T. 1981. Compliance and Force Control for Com-\nputer Controlled Manipulators. IEEE Trans. Syst. Man Cy-\nbern., 11(6): 418–432.\nMatuszek, C.; Herbst, E.; Zettlemoyer, L.; and Fox, D. 2012.\nLearning to Parse Natural Language Commands to a Robot\nControl System. In Int. Symp. Experimental Robotics, vol-\nume 88, 403–415. Springer.\nMorrow, J. D.; and Khosla, P. K. 1997. Manipulation Task\nPrimitives for Composing Robot Skills. In Proc. IEEE Int.\nConf. Robot. Autom. (ICRA), volume 4, 3354–3359.\nNilsson, N. J. 1969. A Mobile Automaton: An Applica-\ntion of Artificial Intelligence Techniques. In Proc. Int. Joint\nConf. Artif. Intell. (IJCAI), 509–520.\nNilsson, N. J. 1984. Shakey the Robot. Technical Report\n323, SRI AI Center.\nPaulius, D.; Eales, N.; and Sun, Y . 2020. A Motion Taxon-\nomy for Manipulation Embedding. InRobotics: Science and\nSystems.\nRaibert, M. H.; and Craig, J. J. 1981. Hybrid Position/Force\nControl of Manipulators. ASME. J. Dyn. Sys., Meas., Con-\ntrol., 103(2): 126–133.\nRoy, D. 2005. Semiotic Schemas: A Framework for Ground-\ning Language in Action and Perception. Artificial Intelli-\ngence, 167(1-2): 170–205.\nSingh, I.; Blukis, V .; Mousavian, A.; Goyal, A.; Xu, D.;\nTremblay, J.; Fox, D.; Thomason, J.; and Garg, A. 2022.\nProgPrompt: Generating Situated Robot Task Plans using\nLarge Language Models. arXiv preprint arXiv:2209.11302.\nSquire, S.; Tellex, S.; Arumugam, D.; and Yang, L. 2015.\nGrounding English Commands to Reward Functions. In\nRobotics: Science and Systems.\nTellex, S.; Kollar, T.; Dickerson, S.; Walter, M. R.; Banerjee,\nA. G.; Teller, S. J.; and Roy, N. 2011. Understanding Natural\nLanguage Commands for Robotic Navigation and Mobile\nManipulation. In Proc. AAAI Conf. Artificial Intelligence,\nvolume 25, 1507–1514.\nToyoda, M.; Suzuki, K.; Mori, H.; Hayashi, Y .; and Ogata, T.\n2021. Embodying Pre-Trained Word Embeddings Through\nRobot Actions. IEEE Robotics Autom. Lett., 6(2): 4225–\n4232.\nVuong, N.; Pham, H.; and Pham, Q.-C. 2021. Learning Se-\nquences of Manipulation Primitives for Robotic Assembly.\nIn Proc. IEEE Int. Conf. Robot. Autom. (ICRA), 4086–4092.\nXiang, J.; Tao, T.; Gu, Y .; Shu, T.; Wang, Z.; Yang, Z.;\nand Hu, Z. 2023. Language Models Meet World Models:\nEmbodied Experiences Enhance Language Models. arXiv\npreprint arXiv:2305.10626.\n507",
  "topic": "Executable",
  "concepts": [
    {
      "name": "Executable",
      "score": 0.8928440809249878
    },
    {
      "name": "Computer science",
      "score": 0.6645838618278503
    },
    {
      "name": "Robot",
      "score": 0.6372371912002563
    },
    {
      "name": "Formalism (music)",
      "score": 0.5957757830619812
    },
    {
      "name": "Task (project management)",
      "score": 0.5210199356079102
    },
    {
      "name": "Function (biology)",
      "score": 0.4443427324295044
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4402173161506653
    },
    {
      "name": "Human–computer interaction",
      "score": 0.42754727602005005
    },
    {
      "name": "Programming language",
      "score": 0.33377885818481445
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3079853057861328
    },
    {
      "name": "Engineering",
      "score": 0.1606351137161255
    },
    {
      "name": "Systems engineering",
      "score": 0.10448762774467468
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Evolutionary biology",
      "score": 0.0
    },
    {
      "name": "Musical",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I219193219",
      "name": "Purdue University West Lafayette",
      "country": "US"
    }
  ]
}