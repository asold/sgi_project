{
  "title": "GatorTron: A Large Language Model for Clinical Natural Language Processing",
  "url": "https://openalex.org/W4214584464",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2104885275",
      "name": "Xi Yang",
      "affiliations": [
        "UF Health Cancer Center",
        "University of Florida Health",
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A4313872569",
      "name": "Nima PourNejatian",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4220189314",
      "name": "Hoo Chang Shin",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2658025527",
      "name": "Kaleb E. Smith",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2806574362",
      "name": "Christopher Parisien",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4220189317",
      "name": "Colin Compas",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2153871117",
      "name": "Cheryl Martin",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2151813069",
      "name": "Mona G. Flores",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1985094872",
      "name": "Ying Zhang",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A1711072056",
      "name": "Tanja Magoč",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A2292424595",
      "name": "Christopher A Harle",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A2803212715",
      "name": "Gloria Lipori",
      "affiliations": [
        "University of Florida",
        "University of Florida Health",
        "University of Florida Health Science Center",
        "UF Health Shands Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2673619806",
      "name": "Duane A Mitchell",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A2063760299",
      "name": "William R. Hogan",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A4202335738",
      "name": "Elizabeth A Shenkman",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A1985857611",
      "name": "Jiang Bian",
      "affiliations": [
        "UF Health Cancer Center",
        "University of Florida",
        "University of Florida Health"
      ]
    },
    {
      "id": "https://openalex.org/A2101355887",
      "name": "Yonghui Wu",
      "affiliations": [
        "UF Health Cancer Center",
        "University of Florida",
        "University of Florida Health"
      ]
    },
    {
      "id": "https://openalex.org/A2104885275",
      "name": "Xi Yang",
      "affiliations": [
        "University of Florida",
        "University of Florida Cancer Hospital",
        "University of Florida Health"
      ]
    },
    {
      "id": "https://openalex.org/A4313872569",
      "name": "Nima PourNejatian",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4220189314",
      "name": "Hoo Chang Shin",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2658025527",
      "name": "Kaleb E. Smith",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2806574362",
      "name": "Christopher Parisien",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4220189317",
      "name": "Colin Compas",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2153871117",
      "name": "Cheryl Martin",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2151813069",
      "name": "Mona G. Flores",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1985094872",
      "name": "Ying Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1711072056",
      "name": "Tanja Magoč",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A2292424595",
      "name": "Christopher A Harle",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A2803212715",
      "name": "Gloria Lipori",
      "affiliations": [
        "University of Florida Health Science Center",
        "UF Health Shands Hospital",
        "University of Florida",
        "University of Florida Health"
      ]
    },
    {
      "id": "https://openalex.org/A2673619806",
      "name": "Duane A Mitchell",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2063760299",
      "name": "William R. Hogan",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A4202335738",
      "name": "Elizabeth A Shenkman",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A1985857611",
      "name": "Jiang Bian",
      "affiliations": [
        "University of Florida",
        "University of Florida Cancer Hospital",
        "University of Florida Health"
      ]
    },
    {
      "id": "https://openalex.org/A2101355887",
      "name": "Yonghui Wu",
      "affiliations": [
        "University of Florida",
        "University of Florida Health",
        "University of Florida Cancer Hospital"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2748099698",
    "https://openalex.org/W2114388055",
    "https://openalex.org/W2911462778",
    "https://openalex.org/W3201408301",
    "https://openalex.org/W2169818249",
    "https://openalex.org/W3095319910",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2768488789",
    "https://openalex.org/W2993873509",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2158899491",
    "https://openalex.org/W2940685834",
    "https://openalex.org/W2993961432",
    "https://openalex.org/W2970198438",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3104415840",
    "https://openalex.org/W3176762756",
    "https://openalex.org/W2985884876",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2467995757",
    "https://openalex.org/W2769851464",
    "https://openalex.org/W2398489001",
    "https://openalex.org/W2922174756",
    "https://openalex.org/W3027260829",
    "https://openalex.org/W3026565924",
    "https://openalex.org/W3005237274",
    "https://openalex.org/W3109225475",
    "https://openalex.org/W2739351760",
    "https://openalex.org/W2972297345",
    "https://openalex.org/W2800055384",
    "https://openalex.org/W2889272240",
    "https://openalex.org/W2888285200",
    "https://openalex.org/W4235488732",
    "https://openalex.org/W3109919947",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3100468923",
    "https://openalex.org/W2913352150",
    "https://openalex.org/W2891113091",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2168041406",
    "https://openalex.org/W2137407193",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W3101613385",
    "https://openalex.org/W3035204084"
  ],
  "abstract": "ABSTRACT Objective To develop a large pretrained clinical language model from scratch using transformer architecture; systematically examine how transformer models of different sizes could help 5 clinical natural language processing (NLP) tasks at different linguistic levels. Methods We created a large corpus with &gt;90 billion words from clinical narratives (&gt;82 billion words), scientific literature (6 billion words), and general English text (2.5 billion words). We developed GatorTron models from scratch using the BERT architecture of different sizes including 345 million, 3.9 billion, and 8.9 billion parameters, compared GatorTron with three existing transformer models in the clinical and biomedical domain on 5 different clinical NLP tasks including clinical concept extraction, relation extraction, semantic textual similarity, natural language inference, and medical question answering, to examine how large transformer models could help clinical NLP at different linguistic levels. Results and Conclusion GatorTron scaled up transformer-based clinical language models to a size of 8.9 billion parameters and achieved state-of-the-art performance on 5 clinical NLP tasks of different linguistic levels targeting various healthcare information documented in unstructured electronic health records (EHRs). The proposed GatorTron models performed remarkably better in much complex clinical NLP tasks such as natural language inference (9.6% and 7.5% improvements) and question answering (9.5% and 7.77% improvements) compared with existing smaller clinical transformer models (i.e., BioBERT and ClinicalBERT), demonstrating the potential of large transformer-based clinical models for advanced medical artificial intelligent (AI) applications such as question answering.",
  "full_text": " \n \nGatorTron: A Large Language Model for Clinical Natural Language Processing \nAuthors: Xi Yang, PhD1,2, Nima PourNejatian, PhD3, Hoo Chang Shin, PhD3, Kaleb E Smith, \nPhD3, Christopher Parisien, PhD3, Colin Compas, PhD3, Cheryl Martin, BS3, Mona G Flores, \nMD3, Ying Zhang, MS4, Tanja Magoc, PhD5, Christopher A Harle, PhD1,5, Gloria Lipori, \nMBA5,6, Duane A Mitchell, MD7, PhD, William R Hogan, MD, MS1, Elizabeth A Shenkman, \nPhD1, Jiang Bian, PhD1,2, Yonghui Wu, PhD1,2 * \n \nAffiliation of the authors:  \n1Department of Health Outcomes and Biomedical Informatics, College of Medicine, University \nof Florida, Gainesville, Florida, USA. \n2Cancer Informatics and eHealth core, University of Florida Health Cancer Center, Gainesville, \nFlorida, USA. \n3NVIDIA, Santa Clara, California, USA. \n4Research Computing, University of Florida, Gainesville, Florida, USA. \n5Integrated Data Repository Research Services, University of Florida, Gainesville, Florida, USA. \n6University of Florida Health and Shands Hospital, Gainesville, FL \n7Lillian S. Wells Department of Neurosurgery, UF Clinical and Translational Science Institute, \nUniversity of Florida. \n \nCorresponding author:  Yonghui Wu, PhD \nClinical and Translational Research Building \n2004 Mowry Road, PO Box 100177 \nGainesville, FL, USA, 32610 \nPhone: 352-294-8436 \nEmail: yonghui.wu@ufl.edu \n \nKeywords:   Natural Language Processing \n    Transformer Model \n    Deep Learning \n    Electronic Health Records \n \nWord count: 3,795 \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 18, 2022. ; https://doi.org/10.1101/2022.02.27.22271257doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n \n \nABSTRACT \nObjective \nTo develop a large pretrained clinical language model from scratch using transformer \narchitecture; systematically examine how transformer models of different sizes could help 5 \nclinical natural language processing (NLP) tasks at different linguistic levels.   \nMethods \nWe created a large corpus with >90 billion words from clinical narratives (>82 billion words), \nscientific literature (6 billion words), and general English text (2.5 billion words).  We developed \nGatorTron models from scratch using the BERT architecture of different sizes including 345 \nmillion, 3.9 billion, and 8.9 billion parameters, compared GatorTron with three existing \ntransformer models in the clinical and biomedical domain on 5 different clinical NLP tasks \nincluding clinical concept extraction, relation extraction, semantic textual similarity, natural \nlanguage inference, and medical question answering, to examine how large transformer models \ncould help clinical NLP at different linguistic levels. \nResults and Conclusion \nGatorTron scaled up transformer-based clinical language models to a size of 8.9 billion \nparameters and achieved state-of-the-art performance on 5 clinical NLP tasks of different \nlinguistic levels targeting various healthcare information documented in unstructured electronic \nhealth records (EHRs).  The proposed GatorTron models performed remarkably better in much \ncomplex clinical NLP tasks such as natural language inference (9.6% and 7.5% improvements) \nand question answering (9.5% and 7.77% improvements) compared with existing smaller clinical \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 18, 2022. ; https://doi.org/10.1101/2022.02.27.22271257doi: medRxiv preprint \n \n \ntransformer models (i.e., BioBERT and ClinicalBERT), demonstrating the potential of large \ntransformer-based clinical models for advanced medical artificial intelligent (AI) applications \nsuch as question answering.   \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 18, 2022. ; https://doi.org/10.1101/2022.02.27.22271257doi: medRxiv preprint \n \n \nINTRODUCTION \nThere has been an increasing interest in developing artificial intelligence (AI) systems by \nleveraging large electronic health records (EHRs).  A critical step in developing medical AI \nsystems is to enable machines to use patients’ clinical characteristics captured in longitudinal \nEHRs.  The more information we know about patients, the better medical AI systems that we can \ndevelop.  In recent decades, hospitals and medical practices in the United States (US) rapidly \nadopted EHR systems[1,2], resulting in massive stores of electronic patient data,  including \nstructured (e.g., disease codes, medication codes) and unstructured (i.e., clinical narratives such \nas physicians’ progress notes and discharge summaries).  Physicians and other healthcare \nworkers widely use clinical narratives to document detailed patient information as free text in \nEHRs. [3]  There is an increasing number of studies exploring the rich, more fine-grained \ninformation about the patients in clinical narratives that led to improved diagnostic and \nprognostic models.[4,5]  Nevertheless, free-text narratives cannot be easily used in \ncomputational models that usually require structured data.  Researchers have increasingly turned \nto natural language processing (NLP) as the key technology to fill the gap of using clinical \nnarratives in clinical studies[6].    \n \nRecently, transformer-based deep learning models have become state-of-the-art for many clinical \nNLP tasks.  Compared with traditional machine learning models, transformer-based NLP models \nusually have very large number of parameters (e.g., 345 million parameters in BERT) to enable \nautomated knowledge learning from a massive amount of text data.  There is an increasing \ninterest in examining how scaling up the model size could improve NLP.  In the general NLP \ndomain, many large transformer-based NLP models have been developed, such as the Generative \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 18, 2022. ; https://doi.org/10.1101/2022.02.27.22271257doi: medRxiv preprint \n \n \nPre-trained Transformer 3 (GPT-3) model [7], which has 175 billion parameters and was trained \nusing >400 billion words of text.  However, few studies have explored large (i.e., billions of \nparameters) transformer models in the clinical domain.  To date, the largest transformer model \nusing clinical narratives, ClinicalBERT[8], has 110 million parameters and was trained using 0.5 \nbillion words of clinical text.  It is unclear how transformer-based models developed using \nsignificantly more clinical narrative text and more parameters may improve medical AI systems.  \nIn this study, we developed a large clinical transformer model, GatorTron, using >90 billion \nwords of clinical narratives, scientific literature, and general English text.  We trained GatorTron \nfrom scratch using UF’s HiperGator-AI cluster and empirically evaluated three models with \ndifferent settings including (1) a base model with 345 million parameters, (2) a medium model \nwith 3.9 billion parameters, and (3) a large model with 8.9 billion parameters.  We compared \nGatorTron models with existing large transformer models trained using biomedical literature and \nclinical narratives on 5 clinical NLP tasks including clinical concept extraction (or clinical \nnamed entity recognition [CNER]), medical relation extraction (MRE), semantic textual \nsimilarity (STS), natural language inference (NLI), and medical question answering (MQA).  \nGatorTron outperformed previous transformer models on 5 clinical NLP tasks at different \nlinguistic levels targeting various patient information. \nBACKGROUND \nResearchers have applied various methods including rule-based, machine learning-based, and \nhybrid solutions in clinical NLP. [9,10]  At present, most state-of-the-art NLP models are based \non machine learning models.  For a long time, researchers had to train different machine learning \nmodels for different NLP tasks.  Today, most state-of-the-art clinical NLP solutions are based on \ndeep learning models[11] implemented using neural network architectures – a fast-developing \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 18, 2022. ; https://doi.org/10.1101/2022.02.27.22271257doi: medRxiv preprint \n \n \nsub-domain of machine learning.  Convolutional neural networks[12] (CNN) and recurrent \nneural networks[13] (RNN) have been successfully applied to NLP in the early stage of deep \nlearning.  The RNN model implemented using bidirectional long-short term memory (LSTM) \nwith a CRFs layer (LSTM-CRFs) has been widely used in CNER and relation extraction. [14–\n16]  More recently, the transformer architectures[17] (e.g., BERT) implemented with a self-\nattention mechanism[18] have become state-of-the-art, achieving best performance on many \nNLP benchmarks. [19–22]  In the general domain, the transformer-based NLP models have \nachieved state-of-the-art performance for many NLP tasks including name entity recognition[23–\n25], relation extraction[26–30], sentence similarity[31–33], natural language inference[33–36], \nand question answering[33,34,37,38].  Notably, transformers perform more effectively by \ndecoupling of language model pretraining (i.e., pretrain language models using large unlabeled \ntext corpora) and fine-tuning (i.e., applying the learned language models solving specific tasks \noften with labeled training data) into two independent phases.  After successful pretraining, the \nlearned language model can be used to solve a variety of NLP subtasks through fine-tuning, \nwhich is known as transfer learning – a strategy to learn knowledge from one task and apply it in \nanother task[39].  Human language has a very large sample space – the possible combinations of \nwords and sentences are innumerable.  Recent studies show that large transformer models trained \nusing massive text data are remarkably better than traditional NLP models in terms of emergence \nand homogenization.[39]   \n \nIn the clinical domain, researchers have identified several fundamental NLP tasks such as \nCNER, MRE, STS, NLI, and MQA.  CNER is to recognize phrases that have important clinical \nmeanings (e.g., medications, treatments, adverse drug events).  The NLP system has to determine \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 18, 2022. ; https://doi.org/10.1101/2022.02.27.22271257doi: medRxiv preprint \n \n \nthe boundaries of a concept and classify it into predefined semantic categories.  Early systems for \nclinical concept extract are often rule-based, yet, most recent systems are based on machine \nlearning models such as conditional random fields (CRFs)[40,41], CNN [12,42], and LSTM-\nCRFs [13,14].  Current state-of-the-art solutions for CNER are mainly based on \ntransformers[43].  MRE is to establish medical-related relations (e.g., drug induce adverse \nevents) among clinical concepts (e.g., drugs, adverse events).  MRE is usually approached as a \nclassification problem – identify and classify pairs of concepts with valid relations.  Various \nmachine learning-based classifiers such as support vector machines (SVMs), random forests \n(RF), and gradient boosting trees (GBT)[16] have been applied. With the emergence of deep \nlearning models, researchers have explored the LSTM architecture for RE in both general and \nclinical domains[44,45].  Most recently, several studies adopted the BERT architecture and \ndemonstrated superior performance for MRE on various datasets[43,46–50].  The STS task is to \nquantitatively assess the semantic similarity between two text snippets (e.g., sentences), which is \nusually approached as a regression task where a real-value score was used to quantify the \nsimilarity between two text snippets.  In the general domain, the STS benchmark (STS-B) dataset \ncurated by the Semantic evaluation (SemEval) challenges between 2012 and 2017[51] is widely \nused for evaluating STS systems[19].  Various machine learning methods have been \nexamined[52–54] but transformer-based systems such as RoBERTa[31], T5[33], and \nALBERT[34] are leading the state-of-the-art models for STS.  In the clinical domain, the \nMedSTS dataset[55] that consists of over 1,000 annotated sentence pairs from clinical notes at \nMayo Clinic was widely used as the benchmark.  MedSTS was used as the gold standard in two \nclinical NLP open challenges including the 2018 BioCreative/Open Health NLP (OHNLP) \nchallenge[56] and 2019 n2c2/OHNLP ClinicalSTS shared task[57].  Similar to the general \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 18, 2022. ; https://doi.org/10.1101/2022.02.27.22271257doi: medRxiv preprint \n \n \ndomain, pretrained transformer-based models using clinical text and biomedical literature, \nincluding ClinicalBERT and BioBERT[58], are current solutions for STS.  NLI is also known as \nrecognizing textual entailment (RTE) - a directional relation between text fragments (e.g., \nsentences)[59].  The goal of NLI is to determine if a given hypothesis can be inferred from a \ngiven premise.  In the general domain, two benchmark datasets - the MultiNLI[60] and the \nStanford NLI[61] are widely used.  On both datasets, pretrained transformer models achieved \nstate-of-the-art performances[33,35].  There are limited resources for NLI in the clinical domain.  \nUntil recently, the MedNLI – a dataset annotated by doctors based on the medical history of \npatients[62] was developed as a benchmark dataset in the clinical domain.  A previous study[8] \nshowed that a pretrained clinical BERT model achieved the state-of-the-art performance and \noutperformed the baseline (InferSent[63]) by ~9% accuracy.  The MQA task is to build NLP \nsystems that automatically answer medical questions in a natural language.  Unlike other tasks \nfocusing on phrases and sentences, MQA is a document-level task that requires information from \nthe whole document to generate answers according to questions.  In the general domain, the \nStanford Question Answering Datasets (SQuAD 1.1 and 2.0)[64,65] have been widely used as \nbenchmarks.  Transformer-based models are the state-of-the-art for both SQuAD1.1[24] and \nSQuAD2.0[37].  There are several MQA datasets developed in the past few years such as the \nMESHQA[66], MedQuAD[67], and emrQA[68].   \n \nThe promise of transformer-based NLP models has led to further interest in exploring how \nincreases in model and data size may improve large (e.g., >billions of parameters) transformer \nmodels processing clinical narratives.  In the biomedical domain, researchers developed \nBioBERT[17] (with 110 million parameters) and PubMedBERT[69] (110 million parameters) \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 18, 2022. ; https://doi.org/10.1101/2022.02.27.22271257doi: medRxiv preprint \n \n \ntransformer models using text from PubMed literature.  Previously, we developed BioMegatron \nmodels in the biomedical domain with different sizes from 345 million to 1.2 billion \nparameters[70] using PubMed literature.  However, few studies have explored large-size \ntransformer models in the clinical domain due to the sensitive nature of clinical narratives that \ncontain Protected Health Information (PHI) and the requirement of massive computing power.  \nBy developing not only larger models, but models that use clinical narratives, NLP may perform \nbetter in utilizing patient information in ways that can be applied to medical AI systems.  To \ndate, the largest transformer model using clinical narratives is ClinicalBERT[8].  ClinicalBERT \nhas 110 million parameters and was trained using 0.5 billion words from the publicly available \nMedical Information Mart for Intensive Care III[71] (MIMIC-III) dataset.  It is unclear how \ntransformer-based models developed using significantly more clinical narrative text and more \nparameters may improve medical AI systems in extracting and utilizing patient information.   \nMATERIALS AND METHODS \nData Source \nThe primary data source for this study is the clinical narratives from UF Health Integrated Data \nRepository (IDR), a research data warehouse of UF Health.  We collected a total of 290,482,002 \nclinical notes from 2011 to early 2021.  The data included >82 billion medical words from >290 \nmillion notes related to >2 million patients and >50 million patient care encounters.  We applied \na standard preprocessing pipeline to remove duplicated notes and clean the clinical text – unify \ncharacter encoding, identify tokens and sentence boundaries.  Then, we merged the >82 billion \nwords of clinical corpus with 6 billion words from PubMed (combining PubMed abstracts and \nfull-text commercial-collection)[70], 2.5 billion words from Wikipedia[70], and 0.5 billion \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 18, 2022. ; https://doi.org/10.1101/2022.02.27.22271257doi: medRxiv preprint \n \n \nwords from the MIMIC-III corpus[71] to generate a corpus with > 90 billion words.  This study \nwas approved by the UF Institutional Review Board (IRB202100049). \nStudy design \n \nFigure 1. An overview of study design. \n \nFigure 1 shows an overview of the study design.  We seek to train a large clinical transformer \nmodel, GatorTron, using >90 billion words and examine how and whether scaling up mode size \nimproves clinical NLP tasks.  Following standard practice, we first pretrained GatorTron using \nthe >90 billion words as an unsupervised learning procedure and then applied GatorTron to 5 \ndifferent clinical NLP tasks using a supervised fine-tuning procedure.  We adopted the BERT \narchitecture implemented in MagaTron-LM[70] and explored three different settings including a \nbase model of 345 million parameters (i.e., GatorTron-base), a medium model of 3.9 billion \nparameters (i.e., GatorTron-medium), and a large model of 8.9 billion parameters (i.e., \nGatorTron-large).  Then we compared GatorTron models with an existing transformer model \nfrom the clinical domain, ClinicalBERT (trained with 110 million parameters) and two \ntransformer models from the biomedical domain, including, BioBERT (345 million parameters) \n82 billion\nClinical text \n6 billion\nPubMed text\n2.5 billion\nWikipedia text\nTask1: Clinical concept extraction: problem, lab, treatment, …\n“ The patient had a moderately dilated aortic root , …”\nTask5: Medical question answering\nQuestion: “When did the patient last receive a homograft \nreplacement? ”\nAnswer: 08/31/96 ascending aortic root replacement with \nhomograft with omentopexy.”\nGatorTron345 million, 3.9 billion, \nand 8.9 billion models.\nTask2: Medial relation extraction: drug-Adverse events\n“Thalidomide at 100 mg had to be discontinued secondary to \nsignificant skin rash”\nTask3: Semantic textual similarity: similarity score = 0\n“Patient discharged ambulatory without further questions ”\n“Please contact location at phone number with any questions ”\nTask4: Natural language inference: entailment, contradiction, neutral\n“Labs were notable for Cr 1.7”\n“Patient has elevated Cr ”\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 18, 2022. ; https://doi.org/10.1101/2022.02.27.22271257doi: medRxiv preprint \n \n \nand BioMegatron (1.2 billion parameters).  We examined the models on 5 clinical NLP tasks, \nincluding CNER, MRE, STS, NLI, and MQA.  We used 6 public clinical benchmark datasets \n(Table 2 and Table 3) following the default training/test settings and calculated evaluation \nscores using official evaluation scripts associated with each benchmark dataset.   \nTraining environment \nWe used a total number of 992 Nvidia DGX A100 GPUs from 124 superPOD nodes at UF’s \nHiperGator-AI cluster to train GatorTron models by leveraging both data-level and model-level \nparallelisms implemented by the Megatron-LM package[72].  We monitored the training \nprogress by training loss and validation loss and stopped the training when there was no further \nimprovement (i.e., the loss plot became flat). \nGatorTron Model Configuration \nWe developed GatorTron models using three configurations and determined the number of \nlayers, hidden sizes, and number of attention heads according to the guidelines for optimal depth-\nto-width parameter allocation proposed by Levin et al[73] as well as our previous experience in \ndeveloping BioMegatron[70].  Table 1 provides detailed information for the three settings.  The \nGatorTron-base model has 24 layers of transformer blocks, which is similar to the architecture of \nBERT large model.  For each layer, we set the number of hidden units as 1024 and attention \nheads as 16.  The GatorTron-medium model scaled up to 3.9 billion parameters (~10 times of the \nbase setting) and the GatorTron-large model scaled up to 8.9 billion parameters, which is similar \nto BioMegatron[72] ( with 8.3 billion parameters). \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 18, 2022. ; https://doi.org/10.1101/2022.02.27.22271257doi: medRxiv preprint \n \n \nTable 1.  Three configurations of GatorTron model. \nModel # Layers # Hidden Size # Attention Heads # Parameters \nGatorTron-base 24 1024 16 345 million \nGatorTron-medium 48 2560 40 3.9 billion \nGatorTron-large 56 3584 56 8.9 billion \n \nExisting transformer models for comparison \nBioBERT.[17] The BioBERT model was developed by further training the original BERT-large \nmodel (345 million parameters, 24 layers, 1024 hidden units, and 16 attention heads) using \nbiomedical literature from PubMed Abstracts (4.5 billion words) and PMC Full-text articles \n(13.5 billion words).  In this study, we used version 1.1.   \nClinicalBERT.[8] The ClinicalBERT model was developed by further training the BioBERT \n(base version; 110 million parameters with 12 layers, 768 hidden units, and 12 attention heads) \nusing clinical text from the MIMIC-III[71] corpus. \nBioMegatron.[70] The BioMegatron models adopted the BERT architecture with a different \nnumber of parameters from 345 million to 1.2 billion.  Different from BioBERT and \nClinicalBERT, the BioMegatron was trained from scratch without leveraging the original BERT \nmodel.   \nClinical NLP tasks, evaluation matrices, and benchmark datasets \nWe evaluated GatorTron models using 5 clinical NLP tasks and 6 public benchmark datasets.  \nFor CNER, we used three benchmark datasets developed by the 2010 i2b2 challenge, 2012 i2b2 \nchallenge, and 2018 n2c2 challenge to evaluate GatorTron models on identifying various \nimportant medical concepts from clinical text.  We used standard precision, recall, and F1-score \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 18, 2022. ; https://doi.org/10.1101/2022.02.27.22271257doi: medRxiv preprint \n \n \nfor evaluation.  For MRE, we used the dataset developed by the 2018 n2c2 challenge with a \nfocus on relations between medications and adverse drug events. The standard precision, recall, \nand F1-score were used for evaluation.  For STS, we used the dataset developed by the 2019 \nn2c2/OHNLP challenge on clinical semantic textural similarity[57].  We used the Pearson \ncorrelation score for evaluation.  For NLI, we evaluated the Gatortron models using the \nMedNLI dataset and used accuracy for comparison.  We used the emrQA dataset, a benchmark \ndataset widely used for MQA, to evaluate GatorTron.  We particularly focused on medications \nand relations-related questions as Yue et al.[74] found that the two subsets are more consistent.  \nWe utilized both F1-score and exact match score for evaluation. \nRESULTS \nTable 2 and Table 3 compare GatorTron models with two existing biomedical transformer \nmodels (BioBERT and BioMegatron) and one clinical transformer model (Clinical BERT) on 5 \nclinical NLP tasks.  \nTable 2. Comparison of GatorTron with existing biomedical and clinical transformer models for \nCNER and medical MRE. \n   \n \n  \nCNER MRE \n2010 i2b2[75] 2012 i2b2[76] 2018 n2c2[16] 2018 n2c2[16] \nTransformer Pre Rec F1  Pre Rec F1  Pre Rec F1  Pre Rec F1  \nBioBERT 0.8693 0.8653 0.8673 0.7478 0.8037 0.7747 0.8634 0.8921 0.8775 0.9663 0.9451 0.9555 \nClinicalBERT NA NA 0.8780 NA NA 0.7890 0.8592 0.8832 0.8710 0.9678 0.9414 0.9544 \nBioMegatron 0.8614 0.8761 0.8687 0.7591 0.8031 0.7805 0.8707 0.8915 0.8810 0.9711 0.9434 0.9571 \nGatorTron-base 0.8748 0.9043 0.8893 0.7644 0.8221 0.7922 0.8759 0.9038 0.8896 0.9719 0.9482 0.9599 \nGatorTron-\nmedium 0.8869 0.9122 0.8994 0.7812 0.8245 0.8022 0.8954 0.9035 0.8994 0.9721 0.9503 0.9611 \nGatorTron-large 0.8880 0.9116 0.8996 0.7862 0.8333 0.8091 0.8979 0.9021 0.9000 0.9776 0.9482 0.9627 \nCNER: clinical named entity recognition; MRE: medical relation extraction; Pre: precision; Rec: recall; F1: F1 -score; Clinical concepts in 2010 \ni2b2 and 2012 i2b2 challenges: problems, treatments, lab tests; clinical concepts in 2018 n2c2 challenge: drugs,  adverse events, and drug-related \nattributes (e.g., dose).  Medical relation in 2018 n2c2 challenge: drug induced adverse events. Best F1 scores are bolded. NA: scores not reported. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 18, 2022. ; https://doi.org/10.1101/2022.02.27.22271257doi: medRxiv preprint \n \n \nTable 3. Comparison of GatorTron with existing biomedical and clinical transformer models for \nSTS, NLI, and MQA. \n \n  \nSTS NLI MQA \n2019 n2c2[57] MedNLI[62] emrQA \nMedication[68] \nemrQA Relation[68] \nTransformer Pearson correlation Accuracy F1 score Exact Match F1 score Exact Match \nBioBERT 0.8744 0.8050 0.6997 0.2475 0.9262 0.8361 \nClinicalBERT 0.8787 0.8270 0.6905 0.2406 0.9306 0.8533 \nBioMegatron 0.8806 0.8390 0.7231 0.2882 0.9405 0.879 \nGatorTron-base 0.8810 0.8670 0.7181 0.2978 0.9543 0.9029 \nGatorTron-medium 0.8903 0.8720 0.7354 0.3018 0.9677 0.9243 \nGatorTron-large 0.8896 0.9020 0.7408 0.3155 0.9719 0.9310 \nSTS: semantic textual similarity; NLI: natural language inference; MQA: medial question answering; The best evaluation scores are bolded.  \nRecognize clinical concepts and medical relations. As shown in Table 2, all three GatorTron \nmodels outperformed existing biomedical and clinical transformer models in recognizing various \ntypes of clinical concepts on the three benchmark datasets (i.e., 2010 i2b2[75] and 2012 \ni2b2[76]: problem, treatments, lab tests; 2018 n2c2[16]: drug, adverse events, and drug-related \nattributes).  The GatorTron-large model outperformed the other two smaller GatorTron models \nand achieved the best F1-scores of 0.8996, 0.8091, and 0.9000, respectively, demonstrating \nperformance gain from scaling up the size of the model.  For MRE, the GatorTron-large model \nalso achieved the best F1-score of 0.9627 for identifying drug-cause-adverse event relations \noutperforming existing biomedical and clinical transformers and the other two smaller GatorTron \nmodels.  We observed performance improvement when scaling up the size of GatorTron model. \nAssess semantic textual similarity.  As shown in Table 3, all GatorTron models outperformed \nexisting biomedical and clinical transformer models in assessing STS.  Among the three \nGatorTron models, the GatorTron-medium model achieved the best Pearson correlation score of \n0.8903, outperforming both GatorTron-base and GatorTron-large.  Although we did not observe \nconsistent improvement by scaling up the size of the GatorTron model, the GatorTron-large \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 18, 2022. ; https://doi.org/10.1101/2022.02.27.22271257doi: medRxiv preprint \n \n \nmodel significantly outperformed GatorTron-base and its performance is very close to the \nGatorTron-medium model (0.8896 vs. 0.8903). \nNatural language inference. GatorTron models outperformed existing biomedical and clinical \ntransformers, and the GatorTron-large model achieved the best accuracy of 0.9020, \noutperforming the BioBERT and ClinicalBERT by 9.6% and 7.5%, respectively.  We observed a \nmonotonic performance improvement by scaling up the size of GatorTron. \nMedical question answering.  All GatorTron models outperformed existing biomedical and \nclinical transformer models in MQA (e.g., “What lab results does patient have that are pertinent \nto diabetes diagnosis?”).  For medication-related questions, the GatorTron-large model achieved \nthe best exact match score of 0.3155, outperforming the BioBERT and ClinicalBERT by 6.8% \nand 7.5%, respectively.  For relation-related questions, GatorTron-large also achieved the best \nexact match score of 0.9301, outperforming BioBERT and ClinicalBERT by 9.5% and 7.77%, \nrespectively.  We also observed a monotonic performance improvement by scaling up the model \nsize of GatorTron. \n \nDISCUSSION  \nIn this study, we developed a large pretrained language model, GatorTron, using a corpus of >90 \nbillion words.  We trained GatorTron from scratch with different model sizes and evaluated its \nperformance on 5 clinical NLP tasks at different linguistic levels (phrase level, sentence level, \nand document level) using 6 publicly-available benchmark datasets from the clinical domain.  \nThe experimental results show that GatorTron models outperformed existing biomedical and \nclinical transformers for all 5 clinical NLP tasks.  We observed monotonic improvements by \nscaling up the model size of GatorTron for 4 of the 5 tasks, excluding the STS task.  Our \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 18, 2022. ; https://doi.org/10.1101/2022.02.27.22271257doi: medRxiv preprint \n \n \nGatorTron model also outperformed the BioMegatron[70], a transformer model with a similar \nmodel size developed in our previous study using >8.5 billion words from PubMed and \nWikipedia (a small proportion of the >90 billion words of corpus for developing GatorTron).  \nThis study scaled up the clinical transformer models to 8.9 billion parameters in the clinical \ndomain and demonstrated performance improvements.  To the best of our knowledge, \nGatorTron-large is the largest transformer model in the clinical domain. \nScaling up model size and performance improvement.  There is an increasing interest in \nexamining massive-size deep learning models in NLP as they demonstrated novel abilities such \nas emergence and homogenization[39].   In the general domain, the Megatron-Turing NLG \nmodel has scaled up to 530 billion parameters following the GPT-3[7] model with 175 billion \nparameters.  However, there are limited studies examining large transformer models in the \nclinical domain due to the sensitive nature of clinical text and massive computing requirements.  \nPrior to our study, the largest transformer in the clinical domain was ClinicalBERT with 110 \nmillion parameters trained using 0.5 billion words.  Our study scaled the transformer to 8.9 \nbillion parameters and demonstrated performance improvement for 5 clinical NLP tasks on 6 \npublic benchmark datasets.  Among the 5 tasks, GatorTron achieved significant improvements \nfor sentence-level and document-level NLP tasks such as NLI and MQA, but moderate \nimprovements for phrase-level tasks such as CNER and MRE, indicating that large transformer \nmodels are more helpful to sentence-level and document-level NLP tasks. \nModel size and converge speed.  GatorTron was pretrained using unsupervised learning to \noptimize a mask language model (MLM).  We monitored training loss and calculated validation \nloss using a subset set of the clinical text (5%) to determine when to stop the training.  Figure 2 \nshows the training loss and validation loss for GatorTron models with three different settings.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 18, 2022. ; https://doi.org/10.1101/2022.02.27.22271257doi: medRxiv preprint \n \n \nWe observed that the larger GatorTron models converged faster than the base model.  For \nexample, the GatorTron-base model converged in 10 epochs, whereas the medium and large \nmodels converged in 7 epochs.  This may indicate that larger transformer models learn faster \nthan smaller models.  The training of the GatorTron-large model used about 6 days on 992 GPUs \nfrom 124 Nvidia SuperPOD nodes. \n \nFigure 2. Training loss and validation loss for GatorTron base (345 million), medium (3.9 \nbillion), and large (8.9 billion) models. \n \nPotentials in improving healthcare delivery and patient outcomes.  GatorTron models \nperform better in utilizing patient information in clinical narratives, which can be applied to \nvarious medical AI systems.  The rich, fine-grained patients’ information captured in clinical \nnarratives is a critical resource powering medical AI system.  With better performance in \ninformation extraction tasks (e.g., CNER and MRE), GatorTron models have potential to provide \nmore accurate patients’ information to identify research-standard patient cohorts using \ncomputable phenotypes, support physicians making data-informed decisions by clinical decision \nsupport systems, and identify adverse events associated with drug exposures via \npharmacovigilance.  The significant improvements in STS, NLI, and MQA can be applied for \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 18, 2022. ; https://doi.org/10.1101/2022.02.27.22271257doi: medRxiv preprint \n \n \ndeduplication of clinical text, mining medial knowledge, and developing next-generation medical \nAI systems that can interact with patients using human language.  The emergence and \nhomogenization abilities[39] inherited from a large transformer architecture make it convenient \nto apply GatorTron to many other AI tasks through fine-tuning.  We believe that GatorTron will \nimprove the use of clinical narratives in developing various medical AI systems for better \nhealthcare delivery and health outcomes. \n \nThis study has limitations. We mainly focused on medication and relation-related questions \nwhen evaluating GatorTron models due to the limitation of the benchmark dataset for MQA.  \nFuture studies should examine the benefit of large clinical transformer models to downstream \nmedical applications such as disease phenotyping and patient cohort construction. \nCONCLUSION \nLarge pretrained clinical language models could benefit a number of downstream clinical NLP \ntasks, especially for complex NLP tasks such as MQA. \nACKNOWLEDGMENTS \nNone \nFUNDING STATEMENT \nThis study was partially supported by a Patient-Centered Outcomes Research Institute® \n(PCORI®) Award (ME-2018C3-14754), a grant from the National Cancer Institute, \n1R01CA246418 R01, grants from the National Institute on Aging, NIA R56AG069880 and \nR21AG062884, and the Cancer Informatics and eHealth core jointly supported by the UF Health \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 18, 2022. ; https://doi.org/10.1101/2022.02.27.22271257doi: medRxiv preprint \n \n \nCancer Center and the UF Clinical and Translational Science Institute.  The content is solely the \nresponsibility of the authors and does not necessarily represent the official views of the funding \ninstitutions. \nSupport from UF Research Computing: We would like to thank the UF Research Computing \nteam, led by Dr. Erik Deumens, for providing computing power through UF HiperGator-AI \ncluster. \nCOMPETING INTERESTS STATEMENT \nAuthors have no competing financial interests. \nCONTRIBUTORSHIP STATEMENT \nXY, YW, JB, NP, and MGF were responsible for the overall design, development, and \nevaluation of this study.  XY had full access to all the data in the study, conducted all the \nexperiments, and takes responsibility for the integrity of the data and the accuracy of the data \nanalysis.  XY, YW, JB, and WH did the bulk of the writing, EAS, DAM, TM and CAH also \ncontributed to writing and editing of this manuscript. All authors reviewed the manuscript \ncritically for scientific content, and all authors gave final approval of the manuscript for \npublication. \nETHICS STATEMENT \nIRB (202100049) of the University of Florida gave approval for this work as exempt.  The \napproval includes but is not limited to HIPAA waiver to enroll. \nSUPPLEMENTARY MATERIAL \nNone. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 18, 2022. ; https://doi.org/10.1101/2022.02.27.22271257doi: medRxiv preprint \n \n \nREFERENCES \n1  Adoption of Electronic Health Record Systems among U.S. Non-Federal Acute Care \nHospitals: 2008-2015. /evaluations/data-briefs/non-federal-acute-care-hospital-ehr-adoption-\n2008-2015.php (accessed 20 Dec 2019). \n2  Adler-Milstein J, Holmgren AJ, Kralovec P, et al. Electronic health record adoption in US \nhospitals: the emergence of a digital “advanced use” divide. J Am Med Inform Assoc \n2017;24:1142–8. \n3  Meystre SM, Savova GK, Kipper-Schuler KC, et al. Extracting information from textual \ndocuments in the electronic health record: a review of recent research. Yearb Med Inform \n2008;:128–44. \n4  Liang H, Tsui BY, Ni H, et al. Evaluation and accurate diagnoses of pediatric diseases using \nartificial intelligence. Nat Med 2019;25:433–8. \n5  Yang J, Lian JW, Chin Y-P (Harvey), et al. Assessing the Prognostic Significance of Tumor-\nInfiltrating Lymphocytes in Patients With Melanoma Using Pathologic Features Identified \nby Natural Language Processing. JAMA Network Open 2021;4:e2126337. \n6  Nadkarni PM, Ohno-Machado L, Chapman WW. Natural language processing: an \nintroduction. J Am Med Inform Assoc 2011;18:544–51. \n7  Floridi L, Chiriatti M. GPT-3: Its Nature, Scope, Limits, and Consequences. Minds & \nMachines 2020;30:681–94. \n8  Alsentzer E, Murphy J, Boag W, et al. Publicly Available Clinical BERT Embeddings. In: \nProceedings of the 2nd Clinical Natural Language Processing Workshop. Minneapolis, \nMinnesota, USA: : Association for Computational Linguistics 2019. 72–\n8.https://www.aclweb.org/anthology/W19-1909 (accessed 12 Jun 2020). \n9  Wang Y, Wang L, Rastegar-Mojarad M, et al. Clinical information extraction applications: \nA literature review. J Biomed Inform 2018;77:34–49. \n10  Wu S, Roberts K, Datta S, et al. Deep learning in clinical natural language processing: a \nmethodical review. J Am Med Inform Assoc 2020;27:457–70. \n11  LeCun Y, Bengio Y, Hinton G. Deep learning. Nature 2015;521:436–44. \n12  Collobert R, Weston J, Bottou L, et al. Natural Language Processing (Almost) from Scratch. \nJ Mach Learn Res 2011;12:2493–537. \n13  Lample G, Ballesteros M, Subramanian S, et al. Neural Architectures for Named Entity \nRecognition. arXiv:160301360 [cs] Published Online First: 4 March \n2016.http://arxiv.org/abs/1603.01360 (accessed 2 Mar 2018). \n14  Wu Y, Yang X, Bian J, et al. Combine factual medical knowledge and distributed word \nrepresentation to improve clinical named entity recognition. In: AMIA Annual Symposium \nProceedings. American Medical Informatics Association 2018. 1110. \n15  Yang X, Lyu T, Li Q, et al. A study of deep learning methods for de-identification of clinical \nnotes in cross-institute settings. BMC Med Inform Decis Mak 2019;19:232. \n16  Yang X, Bian J, Fang R, et al. Identifying relations of medications with adverse drug events \nusing recurrent convolutional neural networks and gradient boosting. Journal of the \nAmerican Medical Informatics Association 2020;27:65–72. \n17  Lee J, Yoon W, Kim S, et al. BioBERT: a pre-trained biomedical language representation \nmodel for biomedical text mining. Bioinformatics Published Online First: \n2019.https://doi.org/10.1093/bioinformatics/btz682 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 18, 2022. ; https://doi.org/10.1101/2022.02.27.22271257doi: medRxiv preprint \n \n \n18  Vaswani A, Shazeer N, Parmar N, et al. Attention is All you Need. In: Advances in Neural \nInformation Processing Systems. Curran Associates, Inc. 2017. \nhttps://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-\nAbstract.html (accessed 28 Oct 2021). \n19  Wang A, Singh A, Michael J, et al. GLUE: A Multi-Task Benchmark and Analysis Platform \nfor Natural Language Understanding. arXiv:180407461 [cs] Published Online First: 22 \nFebruary 2019.http://arxiv.org/abs/1804.07461 (accessed 21 Mar 2020). \n20  Wang A, Pruksachatkun Y, Nangia N, et al. SuperGLUE: A Stickier Benchmark for \nGeneral-Purpose Language Understanding Systems. arXiv:190500537 [cs] Published Online \nFirst: 12 February 2020.http://arxiv.org/abs/1905.00537 (accessed 8 May 2021). \n21  Qiu X, Sun T, Xu Y, et al. Pre-trained Models for Natural Language Processing: A Survey. \narXiv:200308271 [cs] Published Online First: 24 April 2020.http://arxiv.org/abs/2003.08271 \n(accessed 8 May 2021). \n22  Tay Y, Dehghani M, Bahri D, et al. Efficient Transformers: A Survey. arXiv:200906732 [cs] \nPublished Online First: 16 September 2020.http://arxiv.org/abs/2009.06732 (accessed 8 May \n2021). \n23  Yu J, Bohnet B, Poesio M. Named Entity Recognition as Dependency Parsing. \narXiv:200507150 [cs] Published Online First: 13 June 2020.http://arxiv.org/abs/2005.07150 \n(accessed 29 Oct 2021). \n24  Yamada I, Asai A, Shindo H, et al. LUKE: Deep Contextualized Entity Representations with \nEntity-aware Self-attention. arXiv:201001057 [cs] Published Online First: 2 October \n2020.http://arxiv.org/abs/2010.01057 (accessed 29 Oct 2021). \n25  Li X, Sun X, Meng Y, et al. Dice Loss for Data-imbalanced NLP Tasks. arXiv:191102855 \n[cs] Published Online First: 29 August 2020.http://arxiv.org/abs/1911.02855 (accessed 29 \nOct 2021). \n26  Xu B, Wang Q, Lyu Y, et al. Entity Structure Within and Throughout: Modeling Mention \nDependencies for Document-Level Relation Extraction. arXiv:210210249 [cs] Published \nOnline First: 19 February 2021.http://arxiv.org/abs/2102.10249 (accessed 29 Oct 2021). \n27  Ye D, Lin Y, Sun M. Pack Together: Entity and Relation Extraction with Levitated Marker. \narXiv:210906067 [cs] Published Online First: 10 October \n2021.http://arxiv.org/abs/2109.06067 (accessed 29 Oct 2021). \n28  Cohen AD, Rosenman S, Goldberg Y. Relation Classification as Two-way Span-Prediction. \narXiv:201004829 [cs] Published Online First: 17 April 2021.http://arxiv.org/abs/2010.04829 \n(accessed 29 Oct 2021). \n29  Lyu S, Chen H. Relation Classification with Entity Type Restriction. arXiv:210508393 [cs] \nPublished Online First: 18 May 2021.http://arxiv.org/abs/2105.08393 (accessed 29 Oct 2021). \n30  Wang J, Lu W. Two are Better than One: Joint Entity and Relation Extraction with Table-\nSequence Encoders. arXiv:201003851 [cs] Published Online First: 8 October \n2020.http://arxiv.org/abs/2010.03851 (accessed 29 Oct 2021). \n31  Jiang H, He P, Chen W, et al. SMART: Robust and Efficient Fine-Tuning for Pre-trained \nNatural Language Models through Principled Regularized Optimization. Proceedings of the \n58th Annual Meeting of the Association for Computational Linguistics 2020;:2177–90. \n32  Yang Z, Dai Z, Yang Y, et al. XLNet: Generalized Autoregressive Pretraining for Language \nUnderstanding. In: NeurIPS. 2019.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 18, 2022. ; https://doi.org/10.1101/2022.02.27.22271257doi: medRxiv preprint \n \n \n33  Raffel C, Shazeer N, Roberts A, et al. Exploring the Limits of Transfer Learning with a \nUnified Text-to-Text Transformer. arXiv:191010683 [cs, stat] Published Online First: 24 \nOctober 2019.http://arxiv.org/abs/1910.10683 (accessed 27 Mar 2020). \n34  Lan Z-Z, Chen M, Goodman S, et al. ALBERT: A Lite BERT for Self-supervised Learning \nof Language Representations. ArXiv 2019;abs/1909.11942. \n35  Wang S, Fang H, Khabsa M, et al. Entailment as Few-Shot Learner. arXiv:210414690 [cs] \nPublished Online First: 29 April 2021.http://arxiv.org/abs/2104.14690 (accessed 29 Oct \n2021). \n36  Zhang Z, Wu Y, Zhao H, et al. Semantics-aware BERT for Language Understanding. \narXiv:190902209 [cs] Published Online First: 4 February \n2020.http://arxiv.org/abs/1909.02209 (accessed 29 Oct 2021). \n37  Zhang Z, Yang J, Zhao H. Retrospective Reader for Machine Reading Comprehension. \narXiv:200109694 [cs] Published Online First: 11 December \n2020.http://arxiv.org/abs/2001.09694 (accessed 29 Oct 2021). \n38  Garg S, Vu T, Moschitti A. TANDA: Transfer and Adapt Pre-Trained Transformer Models \nfor Answer Sentence Selection. arXiv:191104118 [cs] Published Online First: 20 November \n2019.http://arxiv.org/abs/1911.04118 (accessed 29 Oct 2021). \n39  Bommasani R, Hudson DA, Adeli E, et al. On the Opportunities and Risks of Foundation \nModels. arXiv:210807258 [cs] Published Online First: 18 August \n2021.http://arxiv.org/abs/2108.07258 (accessed 16 Oct 2021). \n40  Wu Y, Xu J, Jiang M, et al. A Study of Neural Word Embeddings for Named Entity \nRecognition in Clinical Text. AMIA Annu Symp Proc 2015;2015:1326–33. \n41  Soysal E, Wang J, Jiang M, et al. CLAMP – a toolkit for efficiently building customized \nclinical natural language processing pipelines. Journal of the American Medical Informatics \nAssociation 2018;25:331–6. \n42  Wu Y, Jiang M, Lei J, et al. Named Entity Recognition in Chinese Clinical Text Using Deep \nNeural Network. Stud Health Technol Inform 2015;216:624–8. \n43  Yang X, Yu Z, Guo Y, et al. Clinical Relation Extraction Using Transformer-based Models. \narXiv preprint arXiv:210708957 2021. \n44  Kumar S. A Survey of Deep Learning Methods for Relation Extraction. arXiv:170503645 \n[cs] Published Online First: 10 May 2017.http://arxiv.org/abs/1705.03645 (accessed 8 May \n2021). \n45  Lv X, Guan Y, Yang J, et al. Clinical relation extraction with deep learning. International \nJournal of Hybrid Information Technology 2016;9:237–48. \n46  Wei Q, Ji Z, Si Y, et al. Relation Extraction from Clinical Narratives Using Pre-trained \nLanguage Models. AMIA Annu Symp Proc 2020;2019:1236–45. \n47  Guan H, Devarakonda M. Leveraging Contextual Information in Extracting Long Distance \nRelations from Clinical Notes. AMIA Annu Symp Proc 2020;2019:1051–60. \n48  Alimova I, Tutubalina E. Multiple features for clinical relation extraction: A machine \nlearning approach. Journal of Biomedical Informatics 2020;103:103382. \n49  Mahendran D, McInnes BT. Extracting Adverse Drug Events from Clinical Notes. \narXiv:210410791 [cs] Published Online First: 21 April 2021.http://arxiv.org/abs/2104.10791 \n(accessed 26 May 2021). \n50  Yang X, Zhang H, He X, et al. Extracting Family History of Patients From Clinical \nNarratives: Exploring an End-to-End Solution With Deep Learning Models. JMIR Medical \nInformatics 2020;8:e22982. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 18, 2022. ; https://doi.org/10.1101/2022.02.27.22271257doi: medRxiv preprint \n \n \n51  Cer D, Diab M, Agirre E, et al. Semeval-2017 task 1: Semantic textual similarity-\nmultilingual and cross-lingual focused evaluation. arXiv preprint arXiv:170800055 2017. \n52  Farouk M. Measuring Sentences Similarity: A Survey. ArXiv Published Online First: 2019. \n53  Ramaprabha J, Das S, Mukerjee P. Survey on Sentence Similarity Evaluation using Deep \nLearning. J Phys: Conf Ser 2018;1000:012070. \n54  Gomaa WH, Fahmy A. A Survey of Text Similarity Approaches. Published Online First: \n2013. \n55  Wang Y, Afzal N, Fu S, et al. MedSTS: a resource for clinical semantic textual similarity. \nLang Resources & Evaluation 2020;54:57–72. \n56  Rastegar-Mojarad M, Liu S, Wang Y, et al. BioCreative/OHNLP Challenge 2018. In: \nProceedings of the 2018 ACM International Conference on Bioinformatics, Computational \nBiology, and Health Informatics. New York, NY, USA: : ACM 2018. 575–\n575.http://doi.acm.org/10.1145/3233547.3233672 \n57  Wang Y, Fu S, Shen F, et al. Overview of the 2019 n2c2/OHNLP Track on Clinical \nSemantic Textual Similarity. JMIR Medical Informatics 2020. \n58  Mahajan D, Poddar A, Liang JJ, et al. Identification of Semantically Similar Sentences in \nClinical Notes: Iterative Intermediate Training Using Multi-Task Learning. JMIR Medical \nInformatics 2020;8:e22508. \n59  Dagan I, Glickman O, Magnini B. The PASCAL Recognising Textual Entailment Challenge. \nIn: Quiñonero-Candela J, Dagan I, Magnini B, et al., eds. Machine Learning Challenges. \nEvaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual \nEntailment. Berlin, Heidelberg: : Springer Berlin Heidelberg 2006. 177–90. \n60  Williams A, Nangia N, Bowman SR. A Broad-Coverage Challenge Corpus for Sentence \nUnderstanding through Inference. arXiv:170405426 [cs] Published Online First: 19 February \n2018.http://arxiv.org/abs/1704.05426 (accessed 16 Aug 2020). \n61  Bowman SR, Angeli G, Potts C, et al. A large annotated corpus for learning natural language \ninference. arXiv:150805326 [cs] Published Online First: 21 August \n2015.http://arxiv.org/abs/1508.05326 (accessed 8 Nov 2021). \n62  Shivade C. MedNLI — A Natural Language Inference Dataset For The Clinical Domain. \n2017.https://physionet.org/content/mednli/ (accessed 23 Apr 2021). \n63  Conneau A, Kiela D, Schwenk H, et al. Supervised Learning of Universal Sentence \nRepresentations from Natural Language Inference Data. arXiv:170502364 [cs] Published \nOnline First: 8 July 2018.http://arxiv.org/abs/1705.02364 (accessed 6 Sep 2021). \n64  Rajpurkar P, Zhang J, Lopyrev K, et al. SQuAD: 100,000+ Questions for Machine \nComprehension of Text. arXiv:160605250 [cs] Published Online First: 10 October \n2016.http://arxiv.org/abs/1606.05250 (accessed 16 Aug 2020). \n65  Rajpurkar P, Jia R, Liang P. Know What You Don’t Know: Unanswerable Questions for \nSQuAD. arXiv:180603822 [cs] Published Online First: 11 June \n2018.http://arxiv.org/abs/1806.03822 (accessed 8 Nov 2021). \n66  Zhu M, Ahuja A, Juan D-C, et al. Question Answering with Long Multiple-Span Answers. \nIn: Findings of the Association for Computational Linguistics: EMNLP 2020. Online: : \nAssociation for Computational Linguistics 2020. 3840–\n9.https://aclanthology.org/2020.findings-emnlp.342 (accessed 8 Nov 2021). \n67  Ben Abacha A, Demner-Fushman D. A question-entailment approach to question answering. \nBMC Bioinformatics 2019;20:511. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 18, 2022. ; https://doi.org/10.1101/2022.02.27.22271257doi: medRxiv preprint \n \n \n68  Pampari A, Raghavan P, Liang J, et al. emrQA: A Large Corpus for Question Answering on \nElectronic Medical Records. arXiv:180900732 [cs] Published Online First: 3 September \n2018.http://arxiv.org/abs/1809.00732 (accessed 24 Oct 2021). \n69  Gu Y, Tinn R, Cheng H, et al. Domain-Specific Language Model Pretraining for Biomedical \nNatural Language Processing. ACM Trans Comput Healthcare 2022;3:1–23. \n70  Shin H-C, Zhang Y, Bakhturina E, et al. BioMegatron: Larger Biomedical Domain \nLanguage Model. arXiv:201006060 [cs] Published Online First: 13 October \n2020.http://arxiv.org/abs/2010.06060 (accessed 5 Apr 2021). \n71  Johnson AEW, Pollard TJ, Shen L, et al. MIMIC-III, a freely accessible critical care \ndatabase. Scientific Data 2016;3:160035. \n72  Shoeybi M, Patwary M, Puri R, et al. Megatron-LM: Training Multi-Billion Parameter \nLanguage Models Using Model Parallelism. arXiv:190908053 [cs] Published Online First: \n13 March 2020.http://arxiv.org/abs/1909.08053 (accessed 20 Oct 2021). \n73  Levine Y, Wies N, Sharir O, et al. The Depth-to-Width Interplay in Self-Attention. \narXiv:200612467 [cs, stat] Published Online First: 17 January \n2021.http://arxiv.org/abs/2006.12467 (accessed 23 Oct 2021). \n74  Yue X, Gutierrez BJ, Sun H. Clinical Reading Comprehension: A Thorough Analysis of the \nemrQA Dataset. arXiv:200500574 [cs] Published Online First: 1 May \n2020.http://arxiv.org/abs/2005.00574 (accessed 6 Sep 2021). \n75  Uzuner Ö, South BR, Shen S, et al. 2010 i2b2/VA challenge on concepts, assertions, and \nrelations in clinical text. J Am Med Inform Assoc 2011;18:552–6. \n76  Sun W, Rumshisky A, Uzuner O. Evaluating temporal relations in clinical text: 2012 i2b2 \nChallenge. J Am Med Inform Assoc 2013;20:806–13. \n \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 18, 2022. ; https://doi.org/10.1101/2022.02.27.22271257doi: medRxiv preprint ",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7783745527267456
    },
    {
      "name": "Natural language processing",
      "score": 0.7210506796836853
    },
    {
      "name": "Computer science",
      "score": 0.7070907354354858
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6759912371635437
    },
    {
      "name": "Inference",
      "score": 0.6656742095947266
    },
    {
      "name": "Language model",
      "score": 0.5958598256111145
    },
    {
      "name": "Question answering",
      "score": 0.5836233496665955
    },
    {
      "name": "Architecture",
      "score": 0.4751981496810913
    },
    {
      "name": "Biomedical text mining",
      "score": 0.4624860882759094
    },
    {
      "name": "Natural language",
      "score": 0.4516453444957733
    },
    {
      "name": "Text mining",
      "score": 0.2752828896045685
    },
    {
      "name": "Engineering",
      "score": 0.10495781898498535
    },
    {
      "name": "History",
      "score": 0.09487330913543701
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}