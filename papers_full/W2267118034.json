{
  "title": "Tool Use as Gesture: new challenges for maintenanceand rehabilitation",
  "url": "https://openalex.org/W2267118034",
  "year": 2010,
  "authors": [
    {
      "id": "https://openalex.org/A5088133649",
      "name": "Manish Parekh",
      "affiliations": [
        "University of Birmingham"
      ]
    },
    {
      "id": "https://openalex.org/A5080376515",
      "name": "Chris Baber",
      "affiliations": [
        "University of Birmingham"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2109945587",
    "https://openalex.org/W2418204917",
    "https://openalex.org/W2166618391",
    "https://openalex.org/W2168685189",
    "https://openalex.org/W2038045050",
    "https://openalex.org/W1999663359",
    "https://openalex.org/W2121731156",
    "https://openalex.org/W2106947414",
    "https://openalex.org/W2110210934",
    "https://openalex.org/W2003363905",
    "https://openalex.org/W2135233422",
    "https://openalex.org/W1480626548",
    "https://openalex.org/W98188630",
    "https://openalex.org/W2132106596",
    "https://openalex.org/W2112628161",
    "https://openalex.org/W2017337590",
    "https://openalex.org/W2045222441",
    "https://openalex.org/W2121252750",
    "https://openalex.org/W1502050276",
    "https://openalex.org/W2168907388",
    "https://openalex.org/W1916165388",
    "https://openalex.org/W2478502689",
    "https://openalex.org/W2005449132",
    "https://openalex.org/W2118383064",
    "https://openalex.org/W2050282683",
    "https://openalex.org/W2095736475",
    "https://openalex.org/W1539777654",
    "https://openalex.org/W2404096302",
    "https://openalex.org/W2167686873",
    "https://openalex.org/W2105046342",
    "https://openalex.org/W2122928453",
    "https://openalex.org/W2036732918",
    "https://openalex.org/W2126247312",
    "https://openalex.org/W1497385253",
    "https://openalex.org/W2108449315",
    "https://openalex.org/W1996717807",
    "https://openalex.org/W2075124737",
    "https://openalex.org/W2007324434",
    "https://openalex.org/W1548671326",
    "https://openalex.org/W1540198138",
    "https://openalex.org/W2157915866"
  ],
  "abstract": "There are many ways to capture human gestures. In this paper, consideration is given to an extension to the growing trend to use sensors to capture movements and interpret these as gestures. However, rather than have sensors on people, the focus is on the attachment of sensors (i.e., strain gauges and accelerometers) to the tools that people use. By instrumenting a set of handles, which can be fitted with a variety of effectors (e.g., knives, forks, spoons, screwdrivers, spanners, saws etc.), it is possible to capture the variation in grip force applied to the handle as the tool is used and the movements made using the handle. These data can be sent wirelessly (using Zigbee) to a computer where distinct patterns of movement can be classified. Different approaches to the classification of activity are considered. This provides an approach to combining the use of real tools in physical space with the representation of actions on a computer. This approach could be used to capture actions during manual tasks, say in maintenance work, or to support development of movements, say in rehabilitation.",
  "full_text": "Tool Use as Gesture: new challenges for maintenanceand rehabilitation\nManish Parekh, Chris Baber \nTool Use as Gesture: new challenges for \nmaintenanceand rehabilitation\nThere are many ways to capture human gestures. In this paper,  consideration is given to an extension \nto the growing trend to use  sensors to capture movements and interpret these as gestures.  However, \nrather than have sensors on people, the focus is on the  attachment of sensors (i.e., strain gauges \nand accelerometers) to  the tools that people use. By instrumenting a set of handles, which  can be \nfitted with a variety of effectors (e.g., knives, forks,  spoons, screwdrivers, spanners, saws etc.), it \nis possible to capture  the variation in grip force applied to the handle as the tool is used  and the \nmovements made using the handle. These data can be sent  wirelessly (using Zigbee) to a computer \nwhere distinct patterns of  movement can be classified. Different approaches to the  classification \nof activity are considered. This provides an  approach to combining the use of real tools in physical \nspace with  the representation of actions on a computer. This approach could  be used to capture \nactions during manual tasks, say in  maintenance work, or to support development of movements, \nsay  in rehabilitation. \nUbiquitous computing, Tangible User Interface, Sensor-based  interaction, Rehabilitation, Maintenance Support \n1.  INTRODUCTION  \nResearchers in the field of Human-Computer \nInteraction (HCI)  have expended a great deal of time \nand effort on the design and  psychology of graphical \nuser interfaces, but significantly less  attention to the \ndesign of the devices with which users can interact  \nwith these interfaces [4][7][8]. Our current range of \ninteraction  devices restrict people to a very limited \nnumber of actions which  tend to be performed in \nseries and tend to require only one hand.  Many \naspects of our everyday lives involve the use of our \nhands  to grasp, manipulate and operate objects in \nthe world around us.  We have a well-developed \nrepertoire of movements to allow fine  motor control \nof our hands and fingers, and yet these movements  \nare rarely supported in HCI. More often than not, \nthe flexibility  of the human hand is ignored and \nmovements are reduced to  either pressing buttons \nor grasping a mouse to make small,  constrained \nmovements in order to control a cursor on the screen  \n[4][7][8]. \nDeveloping interactive technology that reflects the \nrichness of  dexterous behavior remains a challenge \nfor HCI. There are, of  course, exceptions to this \nstatement. Over the past decade or two,  pen-based \ncomputing has allowed people to hold a stylus and  \nmanipulate it much like a pen to write and draw on \nscreen or to  hold a stylus to manipulate objects in \nvirtual environments and  receive haptic feedback \n[4], and in the past few years, gaming  devices, \nsuch as the Nintendo Wii, have supported hand and \narm  movements that are similar to those used in \nsport and dance. One  reason for this trend is the \ndesire to produce ‘mulit-functional’ devices, such \nas the mouse, which can be used to perform a  \nvariety of functions. These devices share a common \nunderlying  approach: the device that is held in the \nhand is first and foremost  intended to be used to \nact upon virtual objects on the computer  screen \nor in the virtual world. This can be contrasted with \nthe  many ways that we use devices (or tools) to \nact upon real objects  in the real world. Not only \nare the compliances and behaviours of  these real \nobjects more complex than their virtual counterparts \nbut  also the range of actions we perform with tools \nto exploit these  compliances are more varied. \nNotwithstanding the fact that a  single device is \nunlikely to satisfy all of the requirements of all  \nusers of a computer [43], there is an obvious irony \nin the use of  the term multifunctional to describe a \nmouse. The functionality of  the mouse lies not in \nthe device (that can only offer the functions  of linear \nmovement in the horizontal plane and depression of \none, two or three buttons) but in the objects on the \ngraphical user  interface that the mouse is used to \nmanipulate. While this provides  a means of linking \nphysical activity to a graphical display, in  many of \nour everyday activities, the use of the tool provides  \nimplicit feedback to the user. This feedback takes the \nform of the  feel of the tool and the effects that the \nuser can make on objects in  the world using the tool. \nAn alternative perspective would be to  define the \nphysical device in terms of its functionality, and to  \nManish Parekh Chris Baber \nSchool of Electronic, Electrical and Computer Engineering \nThe University of Birmingham Birmingham B15 2TT \nmxp534@bham.ac.uk c.baber@bham.ac.uk \nManish Parekh, Chris Baber \n241\n© 2010, the Authors\nTool Use as Gesture: new challenges for maintenanceand rehabilitation\nManish Parekh, Chris Baber \ncapture user behaviour to manage HCI. This is the \napproach that  underlies Tangible User Interfaces. \nIn broad terms, interaction with tangible user \ninterfaces can be considered as follows: ‘a user \nmanipulates a physical artifact with  physical \ngestures, this is sensed by the system, acted upon, \nand feedback is given’ [32] p.253). The physical \nartifacts can range  from models of real objects [9]\n[11][38], to construction blocks [40] to everyday \nobjects that have been adapted to connect to a  \ndigital environment, such as the MediaCup [14]. It \nis this latter  class that is the focus of this paper. \nThus, one can conclude that  ‘ interaction devices \ncan be developed as significant components of the \ncomputer systems, not only acting as transducers \nto convert  user action to computer response, but \ncommunicating all manner  of feedback to the user \nand supporting a greater variety of physical activity.’ \n[4] p. 276). \nThe manipulation of pointing devices, such as mouse \nor joystick  or game controllers, requires specific \ncontrol movements; users  can not simply adapt \nmovements that are familiar to them but  need to learn \nnew ones. Admittedly, this learning is not  particularly \nonerous because the range of movements permitted \nis  so small. However, it does mean that there is \na gap between making a movement in ‘real-life’ \nand making a movement in order to control virtual \nobjects on a screen. This latter type of  movement \nhas the goal of performing actions in order to control  \nsomething. Some devices, such as styli and pens, \nare able to  support movements that are similar to \nlearned movements, such as  drawing and writing. \nHowever, it is interesting to note that there  are \nstill some differences between performing these \nmovements  with pen and paper versus stylus and \nscreen [4]. This means that  the movements that \na person is performing can be considered partly \n‘natural’ (ie learened and practiced in everyday \nlife) and partly a response to the demands of the \ncomputer. Rather than the  user having to learn \nsets of movements that the computer can  interpret, \nthe computer could be made to adapt to the sets \nof  movements that the person naturally wants to \nmake. By presenting  people with a well-defined \ntask, such as hitting a tennis ball on a  screen, it \nis possible to produce a good specification of the \nrange  of motion that might be expected and, using \naccelerometers or  vision-tracking, it is possible to \nmeasure this motion from a handheld  unit in order to \nrecognize an action (which after all, is the  approach \ntaken by the Nintendo Wii). In this latter case, the  \nperson performs as action which is intended to be \nfunctionally  equivalent to that performed in real-life \nand expects the computer  to make an appropriate \nresponse, by having the avatar that the  person \nis controlling perform the same action. Current  \ncommercial approaches to capturing such actions \nrely on  accelerometers to respond to movements of \nthe device held in the  hand. This provides a reliable \nmeans of capturing gross movement  but struggles \nwith finer movements that might characterize many  \ndexterous actions. Thus, in order for the capture of \nhuman movement to develop there is a need to allow \ncomputers to  respond to fine motor control. \nFor this paper, the focus will be on ways in which \none can capture  the actions that people perform \nwith objects in the real world, and  treat these in \nmuch the same way as gestures are treated, i.e., as  \nactions that can be recognized and evaluated. One \nway in which  this can be achieved is through further \nrefinement of the objects  that the person holds \nwhen interacting with the computer. In this  paper, \nthe focus lies on capturing data from the handles \nof  domestic tools and using these data to model \ndifferent types of  performance. It is proposed that \nsuch developments not only  provide an interesting \nground for exploring ways of analyzing  human \nactivity but also lead to the development of novel \nforms of  interaction device. There are a number \nof domains in which such  recognition could prove \nvaluable, and in this paper considers two  of these: \n(i.)  Capturing and recording everyday actions of \npeople  undergoing healthcare or rehabilitation, \nnot in the  laboratory but in their own home; \n(ii.)  Monitoring the actions of technicians involved in  \nmaintenance work and producing a computer log \nof  the work. \nThese domains of application are considered further \nin the  discussion section. In the next section, a review \nof approaches to  analysing human activity from \nsensor data will be presented. This  is followed by a \ndiscussion of capturing data from human  interaction \nwith tool handles, and a description of a prototype  \nsystem. Then the results of initial trials and analysis \nof different  activities is presented, before the paper \nconcludes with a  discussion of future developments. \n1.1 Using Sensors  to Analyse  Human  Activity  \nPrevious research has looked at the automated \nanalysis of  ambulatory motion, with some real-time \nfeedback, to aid in  rehabilitation of walking [19][28], \nand at the use of activity  recognition to monitor \narm movement [15][17]. Such systems  enable \nrehabilitation to be carried out at home, with the use \nof  unobtrusive sensor systems at a reasonable cost \ncompared to  current hospital medical systems [6]. \nAmft and Tröster [1][2] used  a range of sensors on \nthe person to define movements involved in  eating, \nas part of a diet monitoring application. For example, \na microphone and electromyography sensor \nwas used to recognize  chewing and swallowing, \nand accelerometers on the lower arm  indicated \nmovements towards the mouth. \n242\n© 2010, the Authors\nTool Use as Gesture: new challenges for maintenanceand rehabilitation\nManish Parekh, Chris Baber \nIn maintenance work, Ogris et al. [30] combined a \nbody-worn  ultrasonic unit to track hand location \nwith an accelerometer to  track hand movements \nwhen people performed bicycle repair  tasks. By \ncapturing the action performed, the system was \nable to  provide guidance and feedback to the user \nregarding appropriate  actions to perform. Another \npaper reports a system in which  combined RFID \nand bar-code reading is used to identify tools and  \ncomponents, and accelerometers on wrists, to \ndefine actions, with  a head-mounted web-camera \nto record and check maintenance  activities [31]. \nIn this case, recognition was used to both guide  \nfeedback to the user and also to capture novel \napproaches to a task  (which could then be filmed, \nusing the head-mounted camera, for inclusion in \nfuture training videos). In a similar manner, Maurtua  \net al. [25] developed a system to recognize picking \nup a tool or  component and using this to determine \nwhether a car assembly  task was being performed \ncorrectly. Stiefmeier et al. [34] defined  car assembly \nas a series of sub-tasks and sought to recognize  \nwhen each sub-task had been completed. In these \npapers,  recognition had the primary goal of checking \nmaintenance procedures against ‘good practice’.\n1.2 Using Sensors  to Record Grasp  \nIn the field of ergonomics, grasp is often evaluated \nthrough grip  dynamometry. This involves the person \npulling against a sprung  handle; the amount of force \nused to pull the handle is measured  off a calibrated \nscale. This shows effective grip strength but does  \nnot provide an indication of how well the person can \ngrasp an  object or how grasp varies with activity. \nThe instrumentation of tools to measure grip force \nhas been  explored previously in many specific \napplications such as golf  grip [22] and children’s \nhandwriting [9]. The approach in these  studies was \nto cover the handle of the tool in a force sensing mat.  \nBoth studies used the Tekscan 9811 sensor, which \nconsists of a 0.1 mm array of force sensing cells that \nrespond to force with a  linear change in resistance. \nHowever, there are more traditional  forms of sensor \nthat are much cheaper and which could provide  \nusable data, in the form of strain gauges. Murphy et \nal. [29] use  strain gauges on the top and sides of \na knife blade, near the  handle, in order to measure \nforces applied during cutting.  Memberg and Crago \n[27] designed a two sided handle, with strain  gauges \non each side. This design was used as the basis \nfor the  initial prototype in this paper (see figure 1). \nMcGorry [26] used a  three sided handle, with strain \ngauges on each side, and this was  used as the \nbasis for the design of the second prototype (figure \n2).  \nThese previous studies, regardless of the sensors \nused,  concentrated on the design of the handle \nand the collection of data  from the use of the \nsensors. However, there was little attempt at  the \nusing these data to interpret the activity beyond \nsimple visual  analysis. If these devices are to be \nuseful in HCI, there is a  requirement to develop \ntechniques for classifying and recognising  activity \nfrom instrumented tools. To this end, Kranz et al. \n[23]  fitted a torque sensor between the handle and \nblade of a large chef’s knife. The data collected from \nthis sensor, combined with the data from load-cells \nunder a cutting board, could be used to  characterise \nthe cutting of different foods. This shows how the \nuse  of instrumented tools can provide data to \nsupport activity  recognition. However, the Kranz et \nal. [23] study was concerned  with the forces applied \nthrough the tool’s blade rather than the  interaction \nbetween hand and handle. It is, therefore, of interest \nto  ask whether hand-handle interactions can be \ncaptured with  sufficient reliability to allow actions to \nbe classified. In this paper,  our aim is to model the \nhand-handle interactions (through motion  and grip) \nand it would be interesting to consider whether this  \napproach could be comparable to that used by Kranz \net al. [23].  Consequently, the testing procedure that \nis employed requires  users to perform activities \nusing an instrumented knife; the  activities include \ncutting different foods and spreading butter. \nFigure  1: Design of prototype  1 \nFigure  2: Initial design of prototype  2 \n2. CLASSIFYING ACTIONS  \nModelling of human performance, on the basis of \naccelerometer  data, has been performed by neural \nnetwork analysis [24][39],  through hidden Markov \nModelling, [3][20][42] or through  Gaussian Mixture \nModels [31]. Each approach has the potential to  \nbe computationally intensive and in this project, \nthe objective was  to use a technique which could \n243\n© 2010, the Authors\nTool Use as Gesture: new challenges for maintenanceand rehabilitation\nManish Parekh, Chris Baber \nrun on a low power processor, so  would be less \ncomputationally demanding. This could involve the  \nuse of classifiers, such as Naive Bayes and C4.5 [5]\n[33][36]. The  features in the Naive Bayes classifier \nwere modelled using a  Gaussian distribution. \nTraining data are used to calculate  parameters that \ndefine a probability distribution for each feature in  \neach class. These parameters form the classification \nmodel.  Classification involves using a probability \ndistribution function  with the parameters of the \nmodel to calculate the probability of  each feature \nof the unknown sample data. Naturally varying  \nphenomena, such as human actions, tend to vary \nwith a Gaussian  distribution; hence it is deemed \nuseful in human activity  recognition. The C4.5 \nalgorithm generates decision trees, where  the \nleaves are the classifications, and the rest of the \nnodes above  them are the features. The decision \ntree is then used to classify  unknown data samples \nby traversing down the tree based on the  value of \neach feature of the unknown sample. When a leaf \nof the  tree is reached, the classification is found. \nIn contrast to Naive  Bayes, decision trees strongly \nmodel interdependence of features.  The C4.5 \nalgorithm is a well developed algorithm for building  \ntrees which deals with issues such as over-fitting \ndata. The  implementation of the C4.5 algorithm \ndecision tree and naive  Bayes classifiers is relatively \nsimple, compared to the  implementation of many of \nother classifiers. \n2.1 Software  Development  \nThe project required a number of software modules \nto be  developed to capture of data from the \ninstrumented handles and  perform classification. \nWhile there are various commercial  packages \nthat can do some of these tasks, it was felt that  \ndeveloping modules in-house would allow greater \ncontrol over the  manner in which the data were \nprocessed. All modules were  written in C#, running \nunder Windows .Net. \nThe first module was an application for real-time \nvisualisation and  recording incoming data from the \nsensors (Figure 3). It displayed  graphs for analysing \nthe real-time output of the three  accelerometer \naxes, and the strain gauge output. \nFollowing the capture of data, the next step required \nsegmentation  to be done with relatively minimal \neffort from the user while  maintaining a high level \nof precision (Figure 4). A second  module allowed \nthe recorded data to be visualised on a scrollable  \ngraph, and segmented and categorised simply by \nclicking on the  graph. This allowed rapid removal of \nall irrelevant and null data. \nFigure3: Theprogram for visualising and recording data \nFigure  4: Program for segmentation of data \nA third module was written to calculate large sets of \nfeatures from  the samples. The features included \n128 and 256 point discrete  Fourier transforms \n(DFT), mean, variance and energy. The DFT  was \nparticularly useful due to its ability to characterise \nrepetitive  data and has been used with great \nsuccess in many activity  recognition applications [5] \n[31]. The application outputs the  feature data in a \nformat readable by the WEKA workbench\n1. \nBefore classification, two feature subset selection \n(FSS) methods  were used to remove features \ncalculated from samples that had  low salience \nto the classification of the data. This is important  \n1 The WEKA workbench is a set of machine learning algorithms created  \nby The University of Waikato in New Zealand. It contains a wide range  of \nfeature subset selection, classifier and clustering algorithms and has a  \ngraphical user interface with many useful ways to visualise the data, all  of \nwhich proved useful for analysing the potential of the data.\n \n244\n© 2010, the Authors\nTool Use as Gesture: new challenges for maintenanceand rehabilitation\nManish Parekh, Chris Baber \nbecause features that do not help characterisation \ncan dramatically  reduce the accuracy of recognition. \nThere are various approaches  to FSS that can be \nused. Although it is theoretically possible to  test \nevery possible subset against the classification \nalgorithm, in  practice this is impractical. This project \nuses over 100 features  resulting in over 10\n100 calls \nto the classifier algorithm (which  would take years \nof processing time on conventional computers).  For \nthis reason, FSS methods generally use some kind \nof search  method, which involves gradually building \nup the feature set  using heuristics to reduce the \nsearch space. The wrapper method [21] is one of \nthe more powerful methods and involves the use \nof  the classifier algorithm to help evaluate the best \nsubset. The  wrapper method usually gives superior \nresults to filter methods  (methods that do not use \nthe classification algorithm) due to the  fact that \nthey produce results specifically suitable for the  \nclassification method [16].  \nFilter methods use similar search methods to \nthe wrapper method,  but they do not use the \nclassification algorithm; instead they use a  function \nthat evaluates the merit of the features against \nthe  training data. These methods tend to be much \nfaster than the  wrapper method [16] and they also \nhave the potential to be useful  with many different \nclassification methods. Correlation-based  Feature \nSelection (CFS) is demonstrated by [16] and shown \nto  give significant improvement when used with a \nNaive Bayes  Classifier (this kind of classifier is also \nused in this work). \nBoth filter and wrapper methods can have different \nsearch  methods applied to them. Kohavi and John \n[21] show that the  Best First search method, which \nuses some simple heuristics,  generally finds better \nsubsets than a simple greedy search. Both  wrapper \nand CFS filter FSS were individually tested in this  \nproject using best first searches. \n3.  ACTIVITIES FOR  CLASSIFICATION  \nThe instrumented knife was used to perform a range \nof simple  tasks that could be commonly used in \ndomestic settings. These  tasks include basic action \non a variety of materials. The actions,  and materials, \nwere as follows: \ni. cut_cheese_block  \nii. cut_cheese_flat (slice)  \niii. cut_cucumber  \niv. cut_cucumber_flat (slice)  \nv. cut_orange (through peel)  \nvi. cut_toast  vii. get_butter  \nviii. spread_butter_toast  \nEach action was performed 15 times by each of \nthe five volunteers  who participated in the data \ncollection phase. After processing,  this gave some \n600 samples of data for testing. \nThe classifier algorithms were evaluated using hold \nout testing;  the dataset was split into three equal \nsets, and then every two set  combination was used \nas the training set, with the third used for  testing. \nThis makes sure that the test data has never been \nseen by  the classifier; this is important because the \npoint of a classification  algorithm is to recognise \nunknown data. Although testing on the  training data \nwould show that the classifier is working, it would  \nnot tell you if it has the ability to cope with real data \nwith random  variation. It is possible for a classifier \nto get perfect classification  results on the training \nset but completely fail in a real example  because \nproblems such as over-fitting of data would not be \nshown  in testing against the training data. \nThe test set-up was the same for all participants \nand activities; the  participant, sitting at a table, \nwas presented with items on a plate.  These items \n(i.e., cheese, orange, cucumber, toast) were cut or  \notherwise acted upon using the instrumented knife. \nEach activity  started and ended with picking up \nand putting down the knife,  with multiple cutting/\nspreading/slicing actions performed in  between. All \nof the data that did not contain any action related  \ndata (irrelevant leading and trailing data and long \npauses) were  removed, splitting some of the \nactions into multiple samples. All  the samples were \nautomatically segmented into uniformly sized  sub-\nsamples suitable for feature creation. Each dataset \nused in the  leave-one-out testing was acquired from \nseparate occasions of  data collection. \nFigure5: Accelerometer plotsshowing two activities\nAs Figure 5 illustrates, the accelerometer showed \nsome variation  in terms of broad type of activity. In \ncomparison with the ‘spreading’ activity, the cutting \nactivities generally returned very  small degrees of \nmotion as demonstrated in the second part of the  \nabove diagram. \n245\n© 2010, the Authors\nTool Use as Gesture: new challenges for maintenanceand rehabilitation\nManish Parekh, Chris Baber \nFigure  6: Plots  of the  strain gauge  and 3 axes  of the  \naccelerometer \nIn Figure 6, the activity of cutting a piece of toast \nis recorded. The  uppermost plot shows variation in \ngrip, as measured by the strain  gauge, and the other \nthree plots show movement in the three axes  of the \naccelerometer. It can be seen that the cutting action \nis  preceded by an increase in grip force, which is \nmaintained until  the cut has been made, and then \nthe force reduces. \nCombinations of the data from the different sensors \nwere used to  classify the actions. For example, \nFigure 7 clearly shows the  separation of multiple \nactivity classes by two accelerometer  features; \nsome of the classification was successfully carried \nout  using only accelerometer features. \nFigure  7: Feature  plotsshowing separation of activities \n3.1 Recognition Accuracy  \nIt was pointed out, in section 2.1, that the classifiers \nused in this  study had been selected because of their \nrelatively low  computational overhead. This means \nthat they might be expected  to perform less well than \nmore sophisticated methods. In terms of  recognition \nperformance, both the naïve Bayes and the C4.5  \nclassifiers achieved precision and recalls above \n60% which  indicates that these classifiers work. On \naverage, the naïve Bayes  classifier performed better \nthan the C4.5 decision tree classifier.  Across both \ndatasets the errors were most common between \nthe  two cheese cutting and two cucumber cutting \nactivities, indicating  that these activities are similar. \nIf these pairs of similar classes  were regarded as the \nsame, the naïve Bayes classifier achieved  precision \nand recall values of 90% and above. \nFigure 8: Performance of the classifiers\nNone of the feature reduction methods improved the \nresults when  using the C4.5 classifier. The wrapper \nFSS method was the most  beneficial, but due to \nits extreme computational complexity, it  could take \nprohibitively long times to run when used on larger  \ndatasets. The CFS method only slightly improved \nprecision and reduced recall, but the reduction of the \nfeatures is still useful as it  reduces the computation \ntime. The most useful feature subsets  found did not \nreject the features calculated from grip force, which  \nshows that there was value to including the force \nsensor. However  some features calculated from \nthe accelerometer were ranked as  more valuable, \nindicating that a combination is required. \n4.  DISCUSSION  \nThis paper demonstrates the development of a \nprototype  instrumented handle. The use of the off-\nthe-shelf sensors means  that the device is potentially \ncheap to produce and the simple  classifiers that \nhave been implemented show that it is possible to  \ndetermine which actions are being performed. While \nthe  recognition rates for individual actions vary from \n60% to 90%+, it  is likely that the handles would be \nused in conjunction with other  sensors, e.g., RFID, \nwhich would provide additional data to  support \n246\n© 2010, the Authors\nTool Use as Gesture: new challenges for maintenanceand rehabilitation\nManish Parekh, Chris Baber \nthe classification of activity. If the electronics were  \nreduced further (e.g., through the implementation of \na MEMS  solution) it would be possible to embed the \nsensors, processor and  communications entirely in \nthe handle of the tool. The challenge  lies less in the \nimplementation of the sensing components and  more \nin the capture and processing of the data that are \nproduced.  In terms of HCI, the concept underlying \nthis design is to provide a  means of allowing people \nto use familiar, everyday tools and  objects in their \nnormal environments. This allows them to focus  on \nthe physical tasks that they would normally perform, \nwith a  computer being able to record specific \nactions. In a previous study  of maintenance work \n[31], we demonstrated how the capture of  activity \nconcerning user movement, from sensors on the \nperson,  and RFID could be used to both generate \nsets of instructions for  performing the tasks (in the \nform of training videos for  uncommon tasks) and the \nlogging of actions (which could be  compared against \na job-list or procedures). This paper shows how  it \nmight be possible to have the sensors fitted on the \ntools that a  person uses, which we argue would be \nless intrusive than having  the sensors on the person. \nIn terms of rehabilitation, the ability to capture \nbehaviours in the person’s normal and familiar \nenvironment, in terms of re-learning  simple domestic \ntasks, could prove an interesting and beneficial  \ndevelopment. Having a means of capturing sensor \ndata and  classifying specific actions could provide \nan indication of changes  in performance. In terms \nof maintenance, the ability to monitor  tool use could \nnot only provide a way of tracking performance  (and \ncomparing this against the standard procedures that \nneed to  be followed, particularly in safety critical \nsystems) but also to  assess condition and wear of \ntools or level of ability of the tool  user. This information \ncould form part of tool-replacement  program in \npreventative maintenance or an indication of the \nneed  for refresher training of personnel. \nIt may be possible to recognise anticipatory grip force \nbefore the  user starts different phases of the activity. \nIn familiar situations,  where an increase in load is \npredictable, e.g., when picking up an  object, grip \nforce is typically adjusted in phase with changes in  \nload [12][13][41]. Studies such as [18] and [37] show \nthat grip  force adjustments in holding a tool, prior \nto a collision, anticipate  the impact force in terms \nof velocity. These studies imply that  people adjust \ntheir grip force, on the handles that they are holding,  \nin anticipation of future actions or effects. This \nnotion could be  used to further refine the modelling \nprocesses, e.g., either in terms  of structuring the \nactivity into phases, or in terms of defining the \nsequences with which actions are performed. We \ncould then  compare the time spent in anticipation \nor action across different  types of user or different \nconditions. This could, for example,  provide a \nfine-grain measure to compare performance over \ntime in  order to see if the performance of the user \nhas improved, perhaps  as the result of practice or \ntraining. \nThe focus of this paper has been on the use of \nsimple  classification schemes to label particular \ntasks performed with  instrumented handles. This \ncan provide a record when tasks were  performed \n(by logging them in a time-stamped database), \nperhaps  for monitoring maintenance work or for \nrecording everyday  behavior in a home-setting for \nrehabilitation. Further work can be  applied beyond \nthe simple classification to consider the  performance \nof individual tasks. For example, by comparing \nthe  pattern of activity performed by an individual \nagainst a template representing ‘good’ performance, \nit is possible to compare experts against novices \n(in maintenance work) or to evaluate changes in  \nperformance (in rehabilitation). While these analyses \nare beyond  the scope of this paper, the prototypes \nand data collection  capabilities we have developed \nwill support this as the next stage  of development \nfor the work. \n5.  REFERENCES  \n[1]  Amft, O., Junker, H. and Tröster, G., 2005, \nDetection of  eating and drinking arm gestures \nusing inertial body worn  sensors, 9th International \nSymposiumon Wearable  Computers,  Los Alamitos, \nCA: IEEE Computer Society,  160-163  \n[2]  Amft, G. and Tröster, G., 2008, Recognition \nof dietary  activity events using on-body sensors, \nArtificial Intelligence  in Medicine,  42,  121-136  \n[3]  Ashbrook, D. and Starner, T., 2002, Learning \nsignificant  locations and predicting user movement \nwith GPS, 6th  International Symposiumon Wearable  \nComputers,  Los  Alamitos, CA: IEEE Computer \nSociety, 101-108 \n[4]  Baber, C., 1998, Beyond the  Desktop,  New \nYork: Academic  Press  \n[5]  Bao, L. and Intille, S.S., 2004, Activity Recognition \nfrom  User-Annotated Acceleration Data,” In \nProceedings of  Second International Conference on \nPervasive Computing  (Pervasive 2004), 1 -17  \n[6]  Bonato, P., 2005, Advances in wearable \ntechnology and  applications in physical medicine \nand rehabilitation, Journal  of  Neuroengineering and \nRehabilitation,  2,  \n[7]  Buxton, W.A., 1983, Lexical and pragmatic \nconsiderations  of input structures, Computer \nGraphics,  17, 31-37  \n[8]  Buxton, W.A., 1986, There’s more to interaction \nthan meets  the eye: some issues in manual input, In \nD.A. Norman and  S.W. Draper (eds.), UserCentred \nSystemDesign,  Hillsdale,  NJ: LEA  \n[9]  Chau, T., Ji, J., Tam, C. and Schwellnus, H., \n2006, A novel  instrument for quantifying grip activity \nduring handwriting,  Archives  of  physical medicine  \n247\n© 2010, the Authors\nTool Use as Gesture: new challenges for maintenanceand rehabilitation\nManish Parekh, Chris Baber \nand rehabilitation 87, 1542-7  \n[10] Fitzmaurice, G., Ishii, H. and Buxton, W., 1995, \nBricks:  laying the foundation for graspable user \ninterfaces, CHI ‘95,  New York: ACM, 442-449  \n[11] Fjeld, M., Lauche, K., Dierssen, S., Bichsel, M. \nand  Rauterberg, M., 1998, BUILD-IT: a brick-based \nintegral  solution supporting multidisciplinary design \nteams, In A.  Sutcliffe, J. Ziegler and P. Johnson \n(eds.) Designing  Effective  and Usable  Multimedia \nSystems, Boston: Kluwer,  131-142  \n[12] Flanagan, J.R., Tresilian, J. and Wing, A.M., \n1993, Coupling  of grip force and load force during \narm movements with  grasped objects. Neuroscience  \nLetters,  152, 53-56  \n[13] Flanagan, J.R and Wing, A.M., 1993, Modulation \nof grip  force with load force during point to point \narm movements.  Experimental Brain Research,  95,  \n131-143  \n[14] Gellerson, H-W., Beigl, M. and Krull, H., 1999, \nMediacup:  awareness technology enabled in an \neveryday object. In: HW.  Gellerson (Ed.), HUC ‘99, \nSpringer, Berlin, 308••310  \n[15] Giorgino, T, P Tormene, G Maggioni, D Capozzi, \nS  Quaglini, and C Pistarini, 2009, Assessment of \nsensorized  garments as a flexible support to self-\nadministered post- stroke physical rehabilitation, \nEuropean Journal of  Physical  and Rehabilitation \nMedicine  45, 75-84  \n[16] Hall, M.A. and Smith, L.A., 1997, Feature subset \nselection: a  correlation based filter approach, \nProceedings  ofthe  Fourth  International Conference  \non Neural Information Processing  and Intelligent  \nInformation Systems, 855••858.  \n[17] Hester, T., R. Hughes, D.M. Sherrill, B. Knorr, M. \nAkay, J.  Stein, and P. Bonato., 2006, Using Wearable \nSensors to  Measure Motor Abilities following \nStroke. International  Workshop on Wearable  and \nImplantable  Body Sensor  Networks  (BSN’06), New \nYork: IEEE, 57.  \n[18] Johansson, R.S. and Westling, G., 1984, Roles \nof glabrous  skin receptors and sensorimotor memory \nin automatic control  of precision grip when lifting \nrougher or more slippery  objects, Experimental \nBrain Research,  56,  550-564  \n[19] Jovanov, E., Milenkovic, A., Otto, C. and de \nGroen, P.C.,  2005, A wireless body area network \nof intelligent motion  sensors for computer assisted \nrehabilitation, Journal of  Neuroengineering and \nRehabilitation,  2, \n[20] Junker, H., Amft, O., Lukowicz, P. and Tröster, \nG., 2008,  Gesture spotting with body-worn inertial \nsensors to detect  user activities, Pattern Recognition,  \n41,  2010-2024  \n[21] Kohavi, Ron and John, G.H., 1997,Wrappers for \nfeature  subset selection, Artificial intelligence  97, \n273-324.  \n[22] Komi, E R, J R Roberts, and S J Rothberg, 2008,  \nMeasurement and analysis of grip force during a golf \nshot,  Proceedings  ofthe  Institution of  Mechanical \nEngineers,  Part P: Journal of  Sports  Engineering \nand Technology 222,  2335.  \n[23] Kranz, M., Schmidt, A., Maldonado, A., Rusu, \nR.B., Beetz,  M., Hornler, B. and Rigoll, G., 2007, \nContext-aware kitchen  utilities, Proceedings  ofthe  \n1st  international conference  on  Tangible  and \nembedded interaction -TEI ‘07 (New York,  New \nYork, USA: ACM Press  \n[24] Van Laerhoven, K., Aidoo, K.A., and Lowette, \nSteven, 2001,  Real-time analysis of data from many \nsensors with neural  networks, Proceedings  Fifth \nInternational Symposium on  Wearable  Computers. \n[25] Maurtua, I., P.T. Kirisci, T. Stiefmeier, M.L. \nSbodio, and H.  Witt, 2007, A Wearable Computing \nPrototype for supporting  training activities in \nAutomotive Production, Proceedings  of  the  4th \nIFAWC  \n[26] McGorry, R W, 2001, A system for the \nmeasurement of grip  forces and applied moments \nduring hand tool use, Applied  ergonomics  32, 271-\n9.  \n[27] Memberg, W D and Crago, P E, 1997, \nInstrumented objects  for quantitative evaluation of \nhand grasp, Journal of  rehabilitation research and \ndevelopment  34, 82-90.  \n[28] Milenkovic, M. , 2002, An accelerometer-\nbased physical  rehabilitation system, System \nTheory,  2002.  Proceedings  of  the  Thirty-Fourth \nSoutheastern Symposium 57-60.  \n[29] Murphy, P., 2000, Design and performance of \na manual task  evaluator, International Journal of  \nIndustrial Ergonomics  25, 257-264  \n[30] Ogris, G., Stiefmeier, Junker, H., Lukowicz, P. \nand Tröster,  G. (2005) Using ultrasonic hand tracking \nto augment motion  analysis based recognition \nof manipulative gestures, 9th  International \nSymposiumon Wearable  Computers,  Los  Alamitos, \nCA: IEE Computer Society, 152-159 \n[31] Schwirtz, A.J. and Baber, C., 2005, Smart tools \nfor smart  maintenance, IEE and MoD HFIDTC \nSymposiumon People  and Systems,  London: IEE, \n145-153  \n[32] Quigley, A., 2010, From GUI to UUI: interfaces \nfor  ubiquitous computing, In J. Krumm (ed.) \nUbiquitous  Computing Fundamentals,  New York: \nCRC Press, 237-284  \n[33] Ravi, N. Dandekar, N., Mysore, P and Littman, \nM.L., 1999,  Activity recognition from accelerometer \ndata, Proceedings  ofthe  National Conference  on \nArtificial Intelligence,  20,  Cambridge, MA: MIT \nPress, 1541\n[34] Stiefmeier, T., C. Lombriser, H. Junker, D. \nRoggen, G.  Troster, and G. Ogris, 2006, Event-\nbased activity tracking in  work environments, \nProceedings  ofthe  3rd IFAWC 91••100.  \n[35] Sung, M., Marci, C. and Pentland, A., 2005, \nWearable  feedback systems for rehabilitation, \nJournal of  Neuroengineering and Rehabilitation,  2,  \n[36] Tapia, E., Intille, S.S., and Larson, K., 2004, \n248\n© 2010, the Authors\nTool Use as Gesture: new challenges for maintenanceand rehabilitation\nManish Parekh, Chris Baber \nActivity  Recognition in the Home using Simple and \nUbiquitous  Sensors,” In Proceedings  of  Second \nInternational  Conference  on Pervasive  Computing \n(Pervasive  2004), 158175  \n[37] Turrell, Y.N., Li, F.-X. and Wing, A.M., 1999, \nGrip force  dynamics in the approach to a collision, \nExperimental Brain  Research,  128, 86-91  \n[38] Underkoffler, J. and Ishii, H., 1999, Urp: a \nluminous- tangible workbench for urban planning \nand design, CHI ‘99,  New York: ACM, 386-393  \n[39] Van Laerhoven, K., Aidoo, K. and Lowette, S., \n2001, Real- time analysis of data from many sensors \nwith neural  networks, 5th International Symposium \non Wearable  Computers,  Los Alamitos, CA: IEEE \nComputer Society,  115-123 Our thanks to ACM \nSIGCHI for allowing us to  modify templates they had \ndeveloped  \n[40] Weller, M.P., Do, E, Y-L. and Gross, M.D., 2008, \nPosey:  instrumenting a poseable hub and strut \nconstruction toy,  Proceedings  of  2nd International \nConference  on Tangible  and Embedded Interaction,  \nNew York: ACM, 39-46 \n[41] Westling, G. and Johansson, R.S., 1984, Factors \ninfluencing  the force control during precision grip, \nExperimental Brain  Research,  53,  277-284  \n[42] Westyn, T., Brashear, H., Atrash, A. and Starner, \nT., 2003,  GeorgiaTech Gesture Toolkit: supporting \nexperiments in  gesture recognition, ICMI03 ±  5th \nInternational Conference  on Multimodal Interfaces, \nNew York:ACM, 85-92  \n[43] Whitefield, A., 1986, Human factors aspects \nof pointing as  an input technique in interactive \ncomputing systems, Applied  Ergonomics,  17,  97-\n104 \n249\n© 2010, the Authors",
  "topic": "Gesture",
  "concepts": [
    {
      "name": "Gesture",
      "score": 0.9039943218231201
    },
    {
      "name": "Accelerometer",
      "score": 0.7115755677223206
    },
    {
      "name": "Computer science",
      "score": 0.6991723775863647
    },
    {
      "name": "Human–computer interaction",
      "score": 0.6733317971229553
    },
    {
      "name": "Motion capture",
      "score": 0.6333252191543579
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5574449300765991
    },
    {
      "name": "Focus (optics)",
      "score": 0.5480629205703735
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.5200173854827881
    },
    {
      "name": "Representation (politics)",
      "score": 0.516228437423706
    },
    {
      "name": "Gesture recognition",
      "score": 0.5074478983879089
    },
    {
      "name": "Movement (music)",
      "score": 0.4611506760120392
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38734740018844604
    },
    {
      "name": "Motion (physics)",
      "score": 0.3158414959907532
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Aesthetics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I79619799",
      "name": "University of Birmingham",
      "country": "GB"
    }
  ],
  "cited_by": 6
}