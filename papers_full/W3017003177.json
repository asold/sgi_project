{
  "title": "Adversarial Training for Large Neural Language Models",
  "url": "https://openalex.org/W3017003177",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1971393576",
      "name": "Liu Xiaodong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2035056196",
      "name": "Cheng Hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2581488112",
      "name": "He, Pengcheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2049887450",
      "name": "Chen, Weizhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2047520290",
      "name": "Wang Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221580603",
      "name": "Poon, Hoifung",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2119363152",
      "name": "Gao, Jianfeng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2396767181",
    "https://openalex.org/W3007759824",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2738015883",
    "https://openalex.org/W2950787360",
    "https://openalex.org/W2789566302",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2525127255",
    "https://openalex.org/W2886424491",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2047782770",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W2994689640",
    "https://openalex.org/W3015777882",
    "https://openalex.org/W2947469743",
    "https://openalex.org/W3007728469",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2970049488",
    "https://openalex.org/W2885185669",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W2946232455",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2788496822",
    "https://openalex.org/W2963207607",
    "https://openalex.org/W2965595599",
    "https://openalex.org/W2970317235",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3036267641",
    "https://openalex.org/W3007863415",
    "https://openalex.org/W3035204084",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2971970905",
    "https://openalex.org/W1989898472",
    "https://openalex.org/W2982295985",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2806120502",
    "https://openalex.org/W2963823140",
    "https://openalex.org/W2154142897",
    "https://openalex.org/W2295072214",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2964159205",
    "https://openalex.org/W3006935033",
    "https://openalex.org/W2965210982",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W2963501948",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2964253222",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2169099542",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3006647218"
  ],
  "abstract": "Generalization and robustness are both key desiderata for designing machine learning methods. Adversarial training can enhance robustness, but past work often finds it hurts generalization. In natural language processing (NLP), pre-training large neural language models such as BERT have demonstrated impressive gain in generalization for a variety of tasks, with further improvement from adversarial fine-tuning. However, these models are still vulnerable to adversarial attacks. In this paper, we show that adversarial pre-training can improve both generalization and robustness. We propose a general algorithm ALUM (Adversarial training for large neural LangUage Models), which regularizes the training objective by applying perturbations in the embedding space that maximizes the adversarial loss. We present the first comprehensive study of adversarial training in all stages, including pre-training from scratch, continual pre-training on a well-trained model, and task-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide range of NLP tasks, in both regular and adversarial scenarios. Even for models that have been well trained on extremely large text corpora, such as RoBERTa, ALUM can still produce significant gains from continual pre-training, whereas conventional non-adversarial methods can not. ALUM can be further combined with task-specific fine-tuning to attain additional gains. The ALUM code is publicly available at https://github.com/namisan/mt-dnn.",
  "full_text": "1\nAdversarial Training for Large Neural Language Models\nXiaodong Liu†, Hao Cheng†, Pengcheng He‡, Weizhu Chen‡, Yu Wang†, Hoifung Poon†,\nJianfeng Gao†\n†Microsoft Research ‡Microsoft Dynamics 365 AI\n{xiaodl,chehao,penhe,wzchen,yuwan,hoifung,jfgao}@microsoft.com\nAbstract\nGeneralization and robustness are both key\ndesiderata for designing machine learning\nmethods. Adversarial training can enhance\nrobustness, but past work often ﬁnds it hurts\ngeneralization. In natural language process-\ning (NLP), pre-training large neural language\nmodels such as BERT have demonstrated im-\npressive gain in generalization for a variety\nof tasks, with further improvement from ad-\nversarial ﬁne-tuning. However, these mod-\nels are still vulnerable to adversarial attacks.\nIn this paper, we show that adversarial pre-\ntraining can improve both generalization and\nrobustness. We propose a general algorithm\nALUM ( Adversarial training for large neu-\nral LangUage Models), which regularizes the\ntraining objective by applying perturbations in\nthe embedding space that maximizes the ad-\nversarial loss. We present the ﬁrst comprehen-\nsive study of adversarial training in all stages,\nincluding pre-training from scratch, continual\npre-training on a well-trained model, and task-\nspeciﬁc ﬁne-tuning. ALUM obtains substan-\ntial gains over BERT on a wide range of\nNLP tasks, in both regular and adversarial\nscenarios. Even for models that have been\nwell trained on extremely large text corpora,\nsuch as RoBERTa, ALUM can still produce\nsigniﬁcant gains from continual pre-training,\nwhereas conventional non-adversarial meth-\nods can not. ALUM can be further combined\nwith task-speciﬁc ﬁne-tuning to attain addi-\ntional gains. The ALUM code is publicly avail-\nable at https://github.com/namisan/mt-dnn.\n1 Introduction\nGeneralization and robustness are two fundamental\nconsiderations in assessing machine learning meth-\nods. Ideally, a learned model should perform well\non unseen test examples and withstand adversarial\nattacks. In natural language processing (NLP), pre-\ntraining neural language models on unlabeled text\nhas proven very effective to improve generaliza-\ntion performance for a variety of downstream tasks,\nas exempliﬁed by BERT (Devlin et al., 2018) and\nother transformer-based models (Liu et al., 2019c;\nRadford et al., 2018; Clark et al., 2020; Dong et al.,\n2019; Bao et al., 2020). However, these models\nmay still suffer catastrophic failures in adversarial\nscenarios (Nie et al., 2019; Hsieh et al., 2019). For\nexample, Jin et al. (2019) show that classiﬁcation\naccuracy on a Yelp dataset drops from 95.6% on\nstandard test to 6.8% on robust test for a BERT\nmodel.\nAdversarial training (Madry et al., 2017; Good-\nfellow et al., 2014) has been well studied in com-\nputer vision, but past work shows that it often hurts\ngeneralization (Raghunathan et al., 2019; Min et al.,\n2020). In NLP, there is growing interest in adver-\nsarial training, but existing work typically focuses\non assessing the impact on generalization (Zhu\net al., 2019; Jiang et al., 2019; Cheng et al., 2019;\nWang et al., 2019). Moreover, adversarial training\nis generally limited to task-speciﬁc ﬁne-tuning 1.\nSee Minaee et al. (2020a) for a recent survey.\nIn this paper, we present the ﬁrst comprehensive\nstudy on adversarial pre-training, and show that it\ncan improve both generalization and robustness for\na wide range of NLP tasks. We propose a unifying\nalgorithm ALUM (Adversarial training for large\nneural LangUage Models), which augments the\nstandard training objective with an additional term\nthat maximizes the adversarial loss via applying\nperturbation in the embedding space. ALUM is\ngenerally applicable to pre-training and ﬁne-tuning,\non top of any Transformer-based language models.\nWe conduct a comprehensive evaluation on vari-\nous NLP tasks across multiple benchmark datasets,\nincluding GLUE, SQuAD v1.1/v2.0, SNLI, Sci-\nTail for assessing model generalization, and ANLI,\n1A notable exception is Wang et al. (2019), but it only\napplied adversarial training to generative language modeling.\narXiv:2004.08994v2  [cs.CL]  29 Apr 2020\n2\nHELLSW AG, SW AG, Adversarial SQuAD for as-\nsessing model robustness. Experimental results\nshow that by conducting adversarial pre-training,\nALUM attains signiﬁcant improvements, often out-\nperforming previous state of the art by a large mar-\ngin. This is true even for the extremely well-trained\nRoBERTa model, where continual pre-training\nwithout adversarial training fails to attain any gain.\nRemarkably, in addition to improving generaliza-\ntion, we ﬁnd that adversarial pre-training also sub-\nstantially improves robustness, as exempliﬁed by\nthe resulting large gains in adversarial datasets such\nas ANLI, Adversarial-SQuAD, HELLASWAG,\nwhich signiﬁcantly reduces the gap between stan-\ndard errors and robust errors for popular models\nlike BERT and RoBERTa. This suggests that ad-\nversarial training on unlabeled data can provide a\npromising direction to reconcile the apparent con-\nﬂict between generalization and robustness as ob-\nserved in prior work (Raghunathan et al., 2019;\nMin et al., 2020). We also show that adversarial\npre-training can be combined with adversarial ﬁne-\ntuning, resulting in extra gains.\nOur contributions are summarized as follows:\n•We propose ALUM, a general algorithm to in-\ncorporate adversarial training for pre-training\nand ﬁne-tuning large neural language models.\n•We conduct a comprehensive evaluation on a\nwide range of NLP tasks and assess the impact\nof adversarial training in pre-training from\nscratch, continual pre-training, task-speciﬁc\nﬁne-tuning, and their combinations.\n•We obtain signiﬁcant improvements over prior\nstate of the art, including extremely well-\ntrained models such as RoBERTa, in both gen-\neralization and robustness.\n•To facilitate research, we will release our code\nand pre-trained models.\n2 Preliminary\nIn this section, we give a quick overview of lan-\nguage model pre-training, using BERT (Devlin\net al., 2018) as a running example for transformer-\nbased neural language models.\n2.1 Input Representation\nWe assume that the input consists of text spans\n(typically sentences) separated by a special to-\nken [SEP]. To address the problem of out-of-\nvocabulary words, tokens are divided into subword\nunits, using Byte-Pair Encoding (BPE) (Sennrich\net al., 2015) or its variants (Kudo and Richardson,\n2018), which generates a ﬁxed-size subword vo-\ncabulary to compactly represent words in training\ntext corpora.\n2.2 Model Architecture\nFollowing recent pre-training methods (Devlin\net al., 2018; Liu et al., 2019c), we use transformer-\nbased models (Vaswani et al., 2017) to lever-\nage a multi-head attention mechanism, which\nhave demonstrated superiority in parallel computa-\ntion and modeling long-range dependencies, com-\npared to recurrent neural networks such as LSTM\n(Hochreiter and Schmidhuber, 1997). The input is\nﬁrst passed to a lexical encoder, which combines\na token embedding, a (token) position embedding\nand a segment embedding (i.e., which text span the\ntoken belongs to) by element-wise summation. The\nembedding layer is then passed to multiple layers\nof transformer modules to generate the contextual\nrepresentation (Vaswani et al., 2017).\n2.3 Self Supervision\nA key innovation in BERT (Devlin et al., 2018) is\nthe use of Masked Language Model (MLM)for\nself-supervised pre-training. Instead of predicting\nthe next token based on the preceding tokens, as\nin traditional generative language models, MLM\nrandomly replaces a subset of tokens by a special\ntoken (e.g., [MASK]), and asks the model to pre-\ndict them. Essentially, it is a cloze task (Taylor,\n1953), where the training objective is the cross-\nentropy loss between the original tokens and the\npredicted ones. In BERT and RoBERTa, 15% of\nthe input tokens are chosen, among which a ran-\ndom 80% are replaced by [MASK], 10% are left\nunchanged and 10% are randomly replaced by a\ntoken from the vocabulary. In our experiments,\ninstead of using a ﬁxed masked rate of 15%, we\ngradually increase it from 5% to 25% with 5% in-\ncrement for every 20% of training epochs, as we\nﬁnd this makes pre-training more stable.\nAdditionally, BERT also uses Next Sentence\nPrediction (NSP), which is a binary classiﬁca-\ntion task that for a given sentence pair determines\nwhether one sentence follows the other in the orig-\ninal text. There have debates on how much NSP\nhelps (Liu et al., 2019c). But we include it in our\nexperiments for a fair comparison with BERT.\n3\n3 ALUM (Adversarial training for large\nneural LangUage Models)\nIn this section, we ﬁrst present a unifying view of\nstandard training objectives and prior approaches\nto adversarial training. We then present ALUM,\nwhich is a general adversarial training algorithm\napplicable to pre-training and ﬁne-tuning, on top\nof any transformer-based neural language models.\n3.1 Standard Training Objectives\nBoth pre-training and ﬁne-tuning can be viewed\nas minimizing the standard error on training data,\nwith the training objectives derived from self-\nsupervision (MLM and NSP in pre-training) or di-\nrect supervision (labeled examples in task-speciﬁc\nﬁne-tuning), respectively.\nSpeciﬁcally, the training algorithms seek to learn\na function f(x; θ) : x→C, parametrized by θ. In\nMLM, Cis the vocabulary, andf(x; θ) tries to pre-\ndict the masked token y. In ﬁne-tuning, C is the\ntask-speciﬁc label set, and f(x; θ) is the classiﬁer.\nGiven a training dataset D of input-output pairs\n(x,y) and the loss function l(.,.) (e.g., cross en-\ntropy), f(x; θ) is trained to minimize the empirical\nrisk:\nmin\nθ\nE(x,y)∼D[l(f(x; θ),y)] (1)\n3.2 Adversarial Training\nPre-training a large neural language model such\nas BERT has proven effective to improve gener-\nalization performance in task-speciﬁc ﬁne-tuning\n(Devlin et al., 2018). However, such models can\nstill suffer catastrophic loss in adversarial scenar-\nios (Nie et al., 2019; Hsieh et al., 2019; Madry\net al., 2017; Jin et al., 2019), with attacks as simple\nas replacing a few words in input sentences while\npreserving the semantics.\nTo improve model robustness and withstand ad-\nversarial attacks, adversarial training has been pro-\nposed and studied extensively, predominantly in\ncomputer vision literature (Goodfellow et al., 2014;\nMadry et al., 2017). The key idea is to modify the\ntraining objective by applying small perturbation\nto input images that maximize the adversarial loss:\nmin\nθ\nE(x,y)∼D[max\nδ\nl(f(x+ δ; θ),y)] (2)\nwhere the inner maximization can be solved by\nrunning a number of projected gradient descent\nsteps (Madry et al., 2017).\nWhile adversarial training has been successful\nin mitigating adversarial attacks, past work often\nencounters an apparent conﬂict between general-\nization and robustness (Raghunathan et al., 2019,\n2020; Min et al., 2020), as adversarial training\ncould hurt generalization performance.\n3.3 The ALUM Algorithm\nIn NLP, applying adversarial training is not straight-\nforward, since the input are discrete elements (to-\nken or subword sequences), but there have been\nsome recent successes (Zhu et al., 2019; Jiang et al.,\n2019; Cheng et al., 2019; Wang et al., 2019; Mi-\nnaee et al., 2020b). However, aside from Wang et al.\n(2019), there has not been any prior work on ad-\nversarial pre-training, and Wang et al. (2019) only\napplied adversarial training to generative language\nmodeling using LSTM.\nALUM is applicable to both pre-training and\nﬁne-tuning. It builds on several key ideas that have\nproven useful in prior work. First, instead of ap-\nplying perturbation to the input text directly, one\nwould perturb the embedding space. Namely, x\nis the sub-word embedding in f(x; θ) (Jiang et al.,\n2019; Zhu et al., 2019).\nSecond, instead of adopting the adversarial train-\ning objective of Eq. 2, as in Zhu et al. (2019)\nand most other approaches, we follow Jiang et al.\n(2019) to regularize the standard objective using\nvirtual adversarial training (Miyato et al., 2018):\nmin\nθ\nE(x,y)∼D[l(f(x; θ),y)+\nαmax\nδ\nl(f(x+ δ; θ),f(x; θ))]\n(3)\nEffectively, the adversarial term favors label\nsmoothness in the embedding neighborhood, and\nαis a hyperparameter that controls the trade-off\nbetween standard errors and robust errors.\nWe found that virtual adversarial training is su-\nperior to conventional adversarial training, espe-\ncially when labels might be noisy. E.g., BERT pre-\ntraining uses the masked words as self-supervised\nlabels, but in many cases, they could be replaced by\nother words to form completely legitimate text. Em-\npirically, we veriﬁed that this is indeed the case, as\npre-training beneﬁts from larger α. We set α= 10\nfor pre-training, and α = 1 for ﬁne-tuning in all\nour experiments.\nCompared to standard training, adversarial train-\ning is rather expensive due to the inner maximiza-\ntion. Zhu et al. (2019) adopted the free adversar-\nial training idea in Shafahi et al. (2019) for accel-\neration, by reusing the backward pass for gradi-\nent computation to carry out the inner ascent step\n4\nAlgorithm 1ALUM\nInput: T: the total number of iterations, X =\n{(x1,y1),..., (xn,yn)}: the dataset, f(x; θ):\nthe machine learning model parametrized by θ,\nσ2: the variance of the random initialization of\nperturbation δ, ϵ: perturbation bound, K: the\nnumber of iterations for perturbation estima-\ntion, η: the step size for updating perturbation,\nτ: the global learning rate, α: the smoothing\nproportion of adversarial training in the aug-\nmented learning objective, Π: the projection\noperation.\n1: for t= 1,..,T do\n2: for (x,y) ∈X do\n3: δ∼N(0,σ2I)\n4: for m= 1,..,K do\n5: gadv ←∇δl(f(x; θ),f(x+ δ; θ))\n6: δ←Π∥δ∥∞≤ϵ(δ+ ηgadv)\n7: end for\n8: gθ ←∇θl(f(x; θ),y)\n+α∇θl(f(x; θ),f(x+ δ; θ))\n9: θ←θ−τgθ\n10: end for\n11: end for\nOutput: θ\nand outer descent step simultaneously. Inspired\nby ERNIE (Sun et al., 2019) and other continual\npre-training approaches, we instead adopt a curricu-\nlum learning approach: ﬁrst train the model using\nthe standard objective (1); and then continue the\ntraining with virtual adversarial training (3).\nJiang et al. (2019) also incorporated a momen-\ntum term using the Bregman proximate point\nmethod, which can be quite costly in training time.\nWe found that our curriculum learning approach\nlargely rendered this unnecessary and simpliﬁed\nour algorithm without using this term.\nAlgorithm 1 shows the details of ALUM. Line\n4-6 run Kprojected gradient steps to ﬁnd the per-\nturbation δthat maximizes the adversarial loss (vi-\nolation of local smoothness). Note that a larger K\nleads to better approximation (Madry et al., 2017;\nQin et al., 2019), but it is more expensive. To attain\na good trade-off between speed and performance,\nwe set K = 1 in all our experiments.\n3.4 Generalization vs. Robustness\nEmpirically, we found that by applying adversarial\npre-training using ALUM, we were able to improve\nboth generalization and robustness for a wide range\nof NLP tasks, as seen in Section 4. This is very\ninteresting as prior work often ﬁnds that adversarial\ntraining hurts generalization, even with theoretical\njustiﬁcation (Raghunathan et al., 2019, 2020; Min\net al., 2020).\nWe hypothesize that adversarial pre-training\nmight be the key for reconciling this apparent in-\ncongruence, as prior work on the conﬂict between\ngeneralization and robustness generally focuses on\nthe supervised learning setting. Interestingly, some\nnascent results in reconciling the two also leverage\nunlabeled data, such as self-training (Raghunathan\net al., 2020). Additionally, we hypothesize that\nby perturbing the embedding space rather than the\ninput space, adversarial training in NLP might in-\nadvertently bias toward on-manifold perturbation\nthan regular perturbation, which helps generaliza-\ntion (Stutz et al., 2019). We leave the theoretical\nanalysis of all these connections to future work.\n4 Experiments\nIn this section, we present a comprehensive study\nof adversarial training on large neural language\nmodels. We show that ALUM substantially im-\nproves both generalization and robustness in a wide\nrange of NLP tasks, for both the standard BERT\nmodel and the extremely well-trained RoBERTa\nmodel. We also show that ALUM can be applied to\nadversarial pre-training and ﬁne-tuning alike and\nattain further gain by combining the two.\n4.1 Datasets\nPre-training: For BERT pre-training, we use\nWikipedia (English Wikipedia dump 2; 13GB).\nFor continual pre-training of RoBERTa, we use\nWikipedia (13GB), OPENWEBTEXT (public Red-\ndit content (Gokaslan and Cohen); 38GB), STO-\nRIES (a subset of CommonCrawl (Trinh and Le,\n2018); 31GB).\nNLP application benchmarks: To assess the\nimpact of adversarial training on generalization,\nwe use standard benchmarks such as GLUE (Wang\net al., 2018) and SQuAD (v1.1 and v2.0) (Ra-\njpurkar et al., 2016, 2018), as well as three named\nentity recognition (NER) tasks in the biomedical\ndomain. To evaluate the impact of adversarial\ntraining on robustness, we use ANLI (Nie et al.,\n2019), Adversarial SQuAD (Jia and Liang, 2017),\nand HELLASWAG (Hampel, 1974). To assess\nthe combination of adversarial pre-training and\n2https://dumps.wikimedia.org/enwiki/\n5\nﬁne-tuning, we follow Jiang et al. (2019) and use\nMNLI (Williams et al., 2018) (from GLUE), ANLI,\nSW AG (Zellers et al., 2018), SNLI (Bowman et al.,\n2015), SciTail (Khot et al., 2018). These bench-\nmarks cover a wide range of NLP tasks such as\nnamed entity recognition, textual entailment, and\nmachine reading comprehension, spanning classi-\nﬁcation, ranking, and regression. For details, see\nAppendix A.\n4.2 Implementation Details\nWe perform three types of adversarial training in\nour experiments: pre-training from scratch, con-\ntinual pre-training on a well-trained model, and\ntask-speciﬁc ﬁne-tuning.\nWe pre-train BERT models from scratch us-\ning Wikipedia3. The training code is based on\nMegatron, implemented in PyTorch (Shoeybi et al.,\n2019)4. We use ADAM (Kingma and Ba, 2014) for\nthe optimizer with a standard learning rate schedule\nthat increases linearly from zero to the peak rate\nof 1 ×10−4 in ﬁrst one percent of steps, and then\ndecays linearly to zero in the remaining 99% of\nsteps. Following Devlin et al. (2018), training is\ndone for one million steps with batch size of 256.\nWe set the perturbation size ϵ= 1 ×10−5, the step\nsize η = 1 ×10−3, and the variance for initializ-\ning perturbation σ = 1 ×10−5. We set α = 10\nfor heightened regularization in virtual adversarial\ntraining, and set K = 1 for training efﬁciency (i.e.,\none projected gradient step). The training takes 10\ndays on one DGX-2 machine with 16 V100 GPUs.\nFor continual pre-training of RoBERTa (Liu\net al., 2019c), we use RoBERTa’s default train-\ning parameters, except a smaller learning rate\n(4 ×10−5), and run for 100K training steps with a\nbatch size of 256 on the union of Wikipedia, OPEN-\nWEBTEXT, and STORIES (total size 82GB). The\ncode is based on FairSeq 5. The training takes 7\ndays on two DGX-2 machines.\nFor ﬁne-tuning with or without adversarial train-\ning, we use the MT-DNN open-sourced toolkit (Liu\net al., 2020, 2015)6. We follow Jiang et al. (2019)\nfor head-to-head comparison, which uses ADAM\n(Kingma and Ba, 2014) and RADAM (Liu et al.,\n2019a) as our optimizers, with peak learning rates\nof {5 ×10−6,8 ×10−6,1 ×10−5,2 ×10−5}, and\nbatch sizes of 16, 32 or 64, depending on the tasks.\n3BookCorpus is no longer publicly available.\n4https://github.com/NVIDIA/Megatron-LM\n5https://github.com/pytorch/fairseq\n6https://github.com/namisan/mt-dnn\nThe dropout rate is set to0.1 for all the task-speciﬁc\nlayers, except 0.3 for MNLI and 0.05 for CoLA. To\navoid gradient exploding, the gradient is clipped to\nkeep the norm within 1. All the texts are tokenized\nusing WordPiece and chopped to spans up to 512\ntokens. We conduct ﬁne-tuning for up to 10 epochs\nand pick the best model using the dev set.\n4.3 Improving Generalization\nIn this subsection, we study the impact of adver-\nsarial pre-training on generalization, by comparing\nthe performance of pre-trained models in various\ndownstream applications. First, we study the sce-\nnario of pre-training from scratch, by comparing\nthree BERT models:\n•BERTBASE is the standard BERT base model\ntrained using the same setting as Devlin et al.\n(2018) (i.e., 1M steps with a batch size of\n256).\n•BERT+BASE is similar to BERTBASE, except\nthat it is trained with 1.6M steps, which takes\nroughly the same amount of time as that of ad-\nversarial pre-training (see ALUMBERT-BASE\nbelow).\n•ALUMBERT-BASE is a BERT model trained\nusing the same setting as BERTBASE, except\nthat ALUM is used in the last 500K steps.\nEach adversarial training step takes approxi-\nmately 1.5 times longer than a step in standard\ntraining7.\nModel\nSQuAD v1.1/v2.0 MNLI\nm/mm\nF1/EM F1/EM Acc\nBERTBASE 88.5/81.0 76.5/72.9 84.5/84.4\nBERT+BASE 89.6/82.4 77.8/74.0 85.0/84.8\nALUMBERT-BASE 90.8/83.7 80.2/76.6 85.8/86.1\nTable 1: Comparison of standard and adversar-\nial pre-training on SQuAD (v1.1 and v2.0) and\nMNLI (in-domain and out-domain). BERT BASE and\nALUMBERT-BASE both use 1M pre-training steps, and\nBERT+BASE use 1.6M steps.\nTable 1 compares these pre-trained models on\nthree standard benchmarks (SQuAD v1.1 (Ra-\njpurkar et al., 2016) and v2.0 (Rajpurkar et al.,\n7With K=1 in Algorithm 1, ALUM requires two more\nforward passes and one more backward pass compared to\nstandard training.\n6\nFigure 1: Comparison of the standard and adversarial\npre-training on the MNLI development set.\nModel BC2GM NCBI JNLPBA\nF1 F1 F1\nBERTBASE 79.7 84.6 75.7\nALUMBERT-BASE 81.1 86.9 76.5\nTable 2: Comparison of standard and adversarial pre-\ntraining on biomedical NER. Scores are entity-level F1.\n2018), and MNLI from GLUE (Wang et al., 2018)),\nusing the same standard ﬁne-tuning setting (with-\nout adversarial training). The standard BERT mod-\nels trained using only the Wikipedia data attain sim-\nilar results as in Devlin et al. (2018), thus provide\na good baseline for comparison. ALUMBERT-BASE\nconsistently outperforms the standard BERT mod-\nels across all the datasets, even adjusting for the\nslightly longer trainng time. E.g., on SQuAD v1.1,\nALUMBERT-BASE gains 2.3% points in F1 over\nBERTBASE and 1.2% points over BERT+BASE. Fig-\nure 1 shows ALUM at work on the development\nset of MNLI. Once adversarial training is applied\nin the middle (after ﬁrst 500K steps), ALUM starts\noutperforming BERT and the gap is widening.\nWe also assess the impact of adversarial pre-\ntraining in the biomedical domain, which is sub-\nstantially different from the Wikipedia corpus used\nin pre-training. Table 2 shows the results on stan-\ndard biomedical name entity recognition (NER)\ndatasets: BC2GM (Smith et al., 2008), NCBI (Do-\ngan et al., 2014), JNLPBA (Collier and Kim, 2004).\nInterestingly, ALUM still outperforms the standard\nBERT model on all three tasks, even though the\napplication domain is substantially different from\nthe pre-training one.\nNext, we assess the impact of adversarial train-\ning in the continual pre-training setting. We use our\npre-training dataset (Wikipedia, OPENWEBTEXT,\nModel MNLI-m/mm SST-2\nAcc Acc\nRoBERTaLARGE 90.2/90.2 96.4\nRoBERTa+LARGE 90.3/90.2 96.3\nTable 3: RoBERTa is an extremlly well-trained model:\nstandard continual pre-training without adversarial\ntraining fails to improve generalization performance in\ndownstream tasks. (Scores are accuracy.)\nSTORIES; 82GB)8, and run 100K steps in all our\ncontinual pre-training experiments. We choose the\nRoBERTa models as the baseline, which use the\nsame neural model as BERT, but were pre-trained\non an order of magnitude more text (160GB vs\n13GB). They are the state-of-the-art pre-trained lan-\nguage models, outperforming the standard BERT\nmodels in many NLP tasks.\nRoBERTa models are extremely well-trained.\nStandard continual pre-training fails to attain\nany gains in downstream applications such as\nMNLI (Williams et al., 2018) and SST (Socher\net al., 2013) from GLUE (Wang et al., 2018), as\nshown in Table 3. On the other hand, ALUM\nis able to attain further gain from continual pre-\ntraining of RoBERTa, as shown in Table 4. E.g.,\nALUMROBERTA-BASE outperforms RoBERTaBASE\nby +0.5%, and ALUMROBERTA-LARGE outperforms\nRoBERTaLARGE by +0.7% on the MNLI develop-\nment set. This is rather remarkable, as by contrast\nstandard continual pre-training is unable to attain\nany gain.\n4.4 Improving Robustness\nIn this subsection, we assess the impact of ad-\nversarial pre-training on the model’s robustness\nagainst adversarial attacks, using three standard\nadversarial NLP benchmarks: ANLI (Nie et al.,\n2019), HELLASW AG (Zellers et al., 2019) and ad-\nversarial SQuAD (Jia and Liang, 2017). On ANLI,\nwe follow the experimental setting of Nie et al.\n(2019) to enable a head-to-head comparison, which\ncombines four datasets (ANLI, MNLI, SNLI and\nFEVER (Thorne et al., 2018)) for ﬁne-tuning.\nAdversarial pre-training substantially improves\nmodel robustness, as shown in Table 5 and Ta-\nble 6. In all three adversarial datasets, ALUM\nconsistently outperformed the standard pre-training\ncounterparts, for BERT and RoBERTa alike. For\n8This is a subset of the data (160GB) used in RoBERTa\npre-training.\n7\nModel MNLI-m SST-2 QNLI CoLA RTE MRPC QQP STS-B\nRoBERTaBASE 87.6 94.8 92.8 63.6 78.7 90.2 91.9 91.2\nALUMROBERTA-BASE 88.1 95.3 93.1 63.6 80.2 90.9 92.0 91.1\nRoBERTaLARGE 90.2 96.4 94.7 67.8 86.6 90.9 92.2 92.4\nALUMROBERTA-LARGE 90.9 96.6 95.1 68.2 87.3 91.1 92.2 92.1\nTable 4: Comparison of standard and adversarial pre-training on the GLUE development set. Results for\nALUMROBERTA-BASE and ALUM ROBERTA-LARGE are averaged over ﬁve runs. Results of RoBERTa BASE and\nRoBERTaLARGE are taken from Liu et al. (2019c).\nMethod Dev Test\nR1 R2 R3 All R1 R2 R3 All\nMNLI + SNLI + ANLI + FEVER\nBERTBASE 55.7 46.3 43.4 48.2 55.1 45.0 43.1 47.4\nBERT+BASE 57.5 47.3 43.0 48.9 57.7 43.7 43.0 47.8\nALUMBERT-BASE 62.0 48.6 48.1 52.6 61.3 45.9 44.3 50.1\nBERTLARGE (Nie et al., 2019) 57.4 48.3 43.5 49.3 - - - 44.2\nXLNetLARGE (Nie et al., 2019) 67.6 50.7 48.3 55.1 - - - 52.0\nRoBERTaLARGE (Nie et al., 2019) 73.8 48.9 44.4 53.7 - - - 49.7\nALUMROBERTA-LARGE 73.3 53.4 48.2 57.7 72.3 52.1 48.4 57.0\nTable 5: Comparison of standard and adversarial pre-training on the adversarial dataset ANLI. R1, R2 and R3 are\nrounds with increasing difﬁculty. Note that Nie et al. (2019) did not represent results for individual rounds, as\nsigniﬁed by “-”.\nMethod\nAdversarial SQuAD HELLASW AG\nAddSent AddOneSent Dev Test\nEM/F1 EM/F1 Accuracy Accuracy\nBERTBASE 48.9/54.0 59.0/64.8 39.5 -\nBERT+BASE 50.1/56.2 60.5/65.7 40.3 -\nALUMBERT-BASE 54.6/60.4 63.2/69.8 44.0 -\nRoBERTaLARGE 72.3/66.0 79.3/72.9 85.0 85.2\nALUMROBERTA-LARGE 75.5/69.4 81.4/75.0 86.2 85.6\nTable 6: Comparison of standard and adversarial pre-training on adversarial datasets Adversarial SQuAD and HEL-\nLASW AG. The test result on HELLASW AG is taken from the ofﬁcial leaderboard: rowanzellers.com/hellaswag;\nwe couldn’t get results for BERT base models as the organizers restrict the number of submissions.\nexample, on ANLI, ALUM ROBERTA-LARGE gains\n7.3% points in test accuracy over RoBERTaLARGE ,\noutperforms XLNet (Yang et al., 2019) by 5.0%\npoints, creating a new state-of-the-art result. The\ngains on Adversarial SQuAD and HELLASWAG\nare equally signiﬁcant. For example, for Ad-\nversarial SQuAD, ALUMBERT-BASE outperforms\nBERTBASE by +6.4% F1 in the AddSent set-\nting and +5.0% F1 in the AddOneSent setting.\nAgainst RoBERTaLARGE , ALUM ROBERTA-LARGE\ngains +3.4% F1 in AddSent and +2.1% F1 in Ad-\ndOneSent.\n4.5 Combining Adversarial Pre-Training and\nFine-tuning\nAdversarial training has been shown to be ef-\nfective in task-speciﬁc ﬁne-tuning (Jiang et al.,\n2019; Zhu et al., 2019). In this subsection,\nwe explore combining adversarial pre-training\nwith adversarial ﬁne-tuning. Speciﬁcally, we use\nRoBERTaLARGE as the base model, and compare it\nwith ALUMROBERTA-LARGE , which uses adversar-\nial continual pre-training but standard ﬁne-tuning,\nand ALUMRoBERTA-LARGE-SMART, which uses ad-\nversarial training in both continual pre-training and\n8\n(a) Results on MNLI\n(b) Results on ANLI\nFigure 2: Combining adversarial pre-training and ﬁne-\ntuning attaining the best results on the development sets\nof MNLI and ANLI, two representative GLUE tasks.\nModel Dev Test\nSNLI Dataset (Accuracy%)\nGPT (Radford et al., 2018) - 89.9\nBERTLARGE 91.7 91.0\nMT-DNNLARGE(Liu et al., 2019b) 92.2 91.6\nALUMROBERTA-LARGE 93.1 93.0\nALUMROBERTA-LARGE -SMART 93.6 93.4\nSciTail Dataset (Accuracy%)\nGPT (Radford et al., 2018) - 88.3\nBERTLARGE(Liu et al., 2019b) 95.7 94.4\nMT-DNNLARGE(Liu et al., 2019b) 96.3 95.0\nALUMROBERTA-LARGE 97.4 96.3\nALUMROBERTA-LARGE -SMART 98.2 96.8\nTable 7: Combining adversarial pre-training and ﬁne-\ntuning attains the best results on SNLI and SciTail.\nﬁne-tuning. Figure 2 shows the results on the\ndevelopment sets of MNLI and ANLI, two rep-\nModel Dev Test\nSW AG Dataset (Accuracy%)\nGPT (Radford et al., 2018) - 78.0\nBERTLARGE (Devlin et al., 2018) - 86.3\nHuman(Zellers et al., 2018) 88.0 88.0\nRoBERTaLARGE (Liu et al., 2019c) - 89.9\nALUMROBERTA-LARGE 90.7 91.0\nALUMROBERTA-LARGE -SMART 91.2 -\nHELLASW AG Dataset (Accuracy%)\nGPT (Zellers et al., 2019) 41.9 41.7\nBERTLARGE(Zellers et al., 2019) 46.7 47.3\nRoBERTaLARGE(Liu et al., 2019c) - 85.2\nALUMROBERTA-LARGE 86.2 85.6\nALUMROBERTA-LARGE -SMART 86.9 -\nHuman 95.7 95.6\nTable 8: Combining adversarial pre-training and ﬁne-\ntuning attains the best results on SW AG and HEL-\nLASW AG.\nresentative GLUE tasks. Combining adversarial\npre-training and ﬁne-tuning attains the best results,\nand substantially outperforms RoBERTa LARGE .\nE.g., on ANLI, ALUMRoBERTa-SMART outperforms\nALUMROBERTA-LARGE by +1.1% points in accu-\nracy, and outperforms RoBERTaLARGE by +5.1%\npoints. On SNLI, SciTail, SWAG, and HEL-\nLASW AG, we observe similar gains by combining\nadversarial pre-training and ﬁne-tuning, attaining\nnew state-of-the-art results on these tasks. See ta-\nble 7 and 8.\n5 Conclusion\nWe propose ALUM, a general adversarial train-\ning algorithm, and present the ﬁrst comprehen-\nsive study of adversarial training in large neural\nlanguage models. We show that adversarial pre-\ntraining can signiﬁcantly improves both generaliza-\ntion and robustness, which provides a promising\ndirection for reconciling their conﬂicts as observed\nin prior work. ALUM substantially improved ac-\ncuracy for BERT and RoBERTa in a wide range of\nNLP tasks, and can be combined with adversarial\nﬁne-tuning for further gain.\nFuture directions include: further study on the\nrole of adversarial pre-training in improving gen-\neralization and robustness; speed up adversarial\ntraining; apply ALUM to other domains.\n9\nAcknowledgments\nWe thank Haoming Jiang, Tuo Zhao, Zhe Gan,\nKeivn Duh, Yangfeng Ji, Greg Yang, Pengchuan\nZhang, Lei Zhang, Furu Wei, Li Dong, Masayuki\nAsahara, and Lis Pereira for valuable discussions\nand comments, Microsoft Research Technology\nEngineering team for setting up GPU machines.\nReferences\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan\nYang, Xiaodong Liu, Yu Wang, Songhao Piao, Jian-\nfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2020.\nUnilmv2: Pseudo-masked language models for uni-\nﬁed language model pre-training. arXiv preprint\narXiv:2002.12804.\nRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and\nDanilo Giampiccolo. 2006. The second PASCAL\nrecognising textual entailment challenge. In Pro-\nceedings of the Second PASCAL Challenges Work-\nshop on Recognising Textual Entailment.\nLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo\nGiampiccolo, and Bernardo Magnini. 2009. The\nﬁfth pascal recognizing textual entailment challenge.\nIn In Proc Text Analysis Conference (TACâ˘A ´Z09.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\nAssociation for Computational Linguistics.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity-multilingual and\ncross-lingual focused evaluation. arXiv preprint\narXiv:1708.00055.\nYong Cheng, Lu Jiang, and Wolfgang Macherey. 2019.\nRobust neural machine translation with doubly ad-\nversarial inputs. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 4324–4333, Florence, Italy. Associa-\ntion for Computational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In ICLR.\nNigel Collier and Jin-Dong Kim. 2004. Introduc-\ntion to the bio-entity recognition task at JNLPBA.\nIn Proceedings of the International Joint Workshop\non Natural Language Processing in Biomedicine\nand its Applications (NLPBA/BioNLP), pages 73–78,\nGeneva, Switzerland. COLING.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The pascal recognising textual entailment\nchallenge. In Proceedings of the First Inter-\nnational Conference on Machine Learning Chal-\nlenges: Evaluating Predictive Uncertainty Visual\nObject Classiﬁcation, and Recognizing Textual En-\ntailment, MLCW’05, pages 177–190, Berlin, Hei-\ndelberg. Springer-Verlag.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nRezarta Dogan, Robert Leaman, and Zhiyong lu. 2014.\nNcbi disease corpus: A resource for disease name\nrecognition and concept normalization. Journal of\nbiomedical informatics, 47.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei,\nXiaodong Liu, Yu Wang, Jianfeng Gao, Ming\nZhou, and Hsiao-Wuen Hon. 2019. Uniﬁed\nlanguage model pre-training for natural language\nunderstanding and generation. arXiv preprint\narXiv:1905.03197.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The third PASCAL recogniz-\ning textual entailment challenge. In Proceedings of\nthe ACL-PASCAL Workshop on Textual Entailment\nand Paraphrasing, pages 1–9, Prague. Association\nfor Computational Linguistics.\nAaron Gokaslan and Vanya Cohen. Openwebtext cor-\npus.\nIan J Goodfellow, Jonathon Shlens, and Christian\nSzegedy. 2014. Explaining and harnessing adversar-\nial examples. arXiv preprint arXiv:1412.6572.\nFrank R Hampel. 1974. The inﬂuence curve and its\nrole in robust estimation. Journal of the american\nstatistical association, 69(346):383–393.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nYu-Lun Hsieh, Minhao Cheng, Da-Cheng Juan, Wei\nWei, Wen-Lian Hsu, and Cho-Jui Hsieh. 2019. On\nthe robustness of self-attentive models. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 1520–1529.\nRobin Jia and Percy Liang. 2017. Adversarial exam-\nples for evaluating reading comprehension systems.\narXiv preprint arXiv:1707.07328.\nHaoming Jiang, Pengcheng He, Weizhu Chen, Xi-\naodong Liu, Jianfeng Gao, and Tuo Zhao. 2019.\nSmart: Robust and efﬁcient ﬁne-tuning for pre-\ntrained natural language models through princi-\npled regularized optimization. arXiv preprint\narXiv:1911.03437.\n10\nDi Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter\nSzolovits. 2019. Is bert really robust? natural lan-\nguage attack on text classiﬁcation and entailment.\narXiv preprint arXiv:1907.11932.\nTushar Khot, Ashish Sabharwal, and Peter Clark. 2018.\nSciTail: A textual entailment dataset from science\nquestion answering. In AAAI.\nDiederik Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing.\narXiv preprint arXiv:1808.06226.\nHector Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In\nThirteenth International Conference on the Princi-\nples of Knowledge Representation and Reasoning.\nLiyuan Liu, Haoming Jiang, Pengcheng He, Weizhu\nChen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han.\n2019a. On the variance of the adaptive learning rate\nand beyond. arXiv preprint arXiv:1908.03265.\nXiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng,\nKevin Duh, and Ye-Yi Wang. 2015. Representation\nlearning using multi-task deep neural networks for\nsemantic classiﬁcation and information retrieval. In\nProceedings of the 2015 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 912–921.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019b. Multi-task deep neural networks\nfor natural language understanding. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics , pages 4487–4496, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nXiaodong Liu, Yu Wang, Jianshu Ji, Hao Cheng,\nXueyun Zhu, Emmanuel Awa, Pengcheng He,\nWeizhu Chen, Hoifung Poon, Guihong Cao, and\nJianfeng Gao. 2020. The microsoft toolkit of multi-\ntask deep neural networks for natural language un-\nderstanding. arXiv preprint arXiv:2002.07972.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019c.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAleksander Madry, Aleksandar Makelov, Ludwig\nSchmidt, Dimitris Tsipras, and Adrian Vladu. 2017.\nTowards deep learning models resistant to adversar-\nial attacks. arXiv preprint arXiv:1706.06083.\nYifei Min, Lin Chen, and Amin Karbasi. 2020. The\ncurious case of adversarially robust models: More\ndata can help, double descend, or hurt generalization.\narXiv preprint arXiv:2002.11080.\nShervin Minaee, Nal Kalchbrenner, Erik Cambria,\nNarjes Nikzad, Meysam Chenaghlu, and Jianfeng\nGao. 2020a. Deep learning based text classiﬁ-\ncation: A comprehensive review. arXiv preprint\narXiv:2004.03705.\nShervin Minaee, Nal Kalchbrenner, Erik Cambria,\nNarjes Nikzad, Meysam Chenaghlu, and Jianfeng\nGao. 2020b. Deep learning based text classiﬁ-\ncation: a comprehensive review. arXiv preprint\narXiv:2004.03705.\nTakeru Miyato, Shin-ichi Maeda, Masanori Koyama,\nand Shin Ishii. 2018. Virtual adversarial training:\na regularization method for supervised and semi-\nsupervised learning. IEEE transactions on pat-\ntern analysis and machine intelligence, 41(8):1979–\n1993.\nYixin Nie, Adina Williams, Emily Dinan, Mohit\nBansal, Jason Weston, and Douwe Kiela. 2019. Ad-\nversarial nli: A new benchmark for natural language\nunderstanding. arXiv preprint arXiv:1910.14599.\nChongli Qin, James Martens, Sven Gowal, Dilip Kr-\nishnan, Alhussein Fawzi, Soham De, Robert Stan-\nforth, Pushmeet Kohli, et al. 2019. Adversarial ro-\nbustness through local linearization. arXiv preprint\narXiv:1907.02610.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2018. Language\nmodels are unsupervised multitask learners.\nAditi Raghunathan, Sang Michael Xie, Fanny Yang,\nJohn Duchi, and Percy Liang. 2020. Understanding\nand mitigating the tradeoff between robustness and\naccuracy. arXiv preprint arXiv:2002.10716.\nAditi Raghunathan, Sang Michael Xie, Fanny Yang,\nJohn C Duchi, and Percy Liang. 2019. Adversar-\nial training can hurt generalization. arXiv preprint\narXiv:1906.06032.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for squad. arXiv preprint arXiv:1806.03822.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909.\nAli Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng\nXu, John Dickerson, Christoph Studer, Larry S\nDavis, Gavin Taylor, and Tom Goldstein. 2019.\nAdversarial training for free! arXiv preprint\narXiv:1904.12843.\n11\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion\nparameter language models using gpu model paral-\nlelism. arXiv preprint arXiv:1909.08053.\nLarry Smith, Lorraine Tanabe, Rie Ando, Cheng-\nJu Kuo, I-Fang Chung, Chun-Nan Hsu, Yu-Shi\nLin, Roman Klinger, Christoph Friedrich, Kuzman\nGanchev, Manabu Torii, Hongfang Liu, Barry Had-\ndow, Craig Struble, Richard Povinelli, Andreas Vla-\nchos, William Baumgartner Jr, Lawrence Hunter,\nBob Carpenter, and W. Wilbur. 2008. Overview of\nbiocreative ii gene mention recognition. Genome bi-\nology, 9 Suppl 2:S2.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing ,\npages 1631–1642.\nDavid Stutz, Matthias Hein, and Bernt Schiele. 2019.\nDisentangling adversarial robustness and generaliza-\ntion. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , pages\n6976–6987.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao\nTian, Hua Wu, and Haifeng Wang. 2019. Ernie 2.0:\nA continual pre-training framework for language un-\nderstanding. arXiv preprint arXiv:1907.12412.\nWilson L Taylor. 1953. â ˘AIJcloze procedureâ ˘A˙I: A\nnew tool for measuring readability. Journalism\nquarterly, 30(4):415–433.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFever: a large-scale dataset for fact extraction and\nveriﬁcation. arXiv preprint arXiv:1803.05355.\nTrieu H Trinh and Quoc V Le. 2018. A simple\nmethod for commonsense reasoning. arXiv preprint\narXiv:1806.02847.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nDilin Wang, Chengyue Gong, and Qiang Liu. 2019.\nImproving neural language modeling via adversarial\ntraining. In International Conference on Machine\nLearning, pages 6555–6565.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2018. Neural network acceptability judgments.\narXiv preprint arXiv:1805.12471.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122. Association for\nComputational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and\nYejin Choi. 2018. Swag: A large-scale adversarial\ndataset for grounded commonsense inference. arXiv\npreprint arXiv:1808.05326.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really ﬁnish your sentence? In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics.\nChen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Thomas\nGoldstein, and Jingjing Liu. 2019. Freelb: En-\nhanced adversarial training for language understand-\ning. arXiv preprint arXiv:1909.11764.\n12\nA NLP Application Benchmarks\n•GLUE. The General Language Understanding\nEvaluation (GLUE) benchmark is a collection of\nnine natural language understanding (NLU) tasks.\nAs shown in Table 9, it includes question an-\nswering (Rajpurkar et al., 2016), linguistic accept-\nability (Warstadt et al., 2018), sentiment analy-\nsis (Socher et al., 2013), text similarity (Cer et al.,\n2017), paraphrase detection (Dolan and Brockett,\n2005), and natural language inference (NLI) (Da-\ngan et al., 2006; Bar-Haim et al., 2006; Giampic-\ncolo et al., 2007; Bentivogli et al., 2009; Levesque\net al., 2012; Williams et al., 2018). The diversity of\nthe tasks makes GLUE very suitable for evaluating\nthe generalization and robustness of NLU models.\n•SNLI. The Stanford Natural Language Inference\n(SNLI) dataset contains 570k human annotated sen-\ntence pairs, in which the premises are drawn from\nthe captions of the Flickr30 corpus and hypothe-\nses are manually annotated (Bowman et al., 2015).\nThis is the most widely used entailment dataset for\nNLI.\n•SciTail. This is a textual entailment dataset de-\nrived from a science question answering (SciQ)\ndataset (Khot et al., 2018). The task involves as-\nsessing whether a given premise entails a given\nhypothesis. In contrast to other entailment datasets\nmentioned previously, the hypotheses in SciTail\nare created from science questions while the cor-\nresponding answer candidates and premises come\nfrom relevant web sentences retrieved from a large\ncorpus. As a result, these sentences are linguis-\ntically challenging and the lexical similarity of\npremise and hypothesis is often high, thus mak-\ning SciTail particularly difﬁcult.\n•ANLI. The Adversarial Natural Language In-\nference (ANLI, Nie et al. (2019)) is a new large-\nscale NLI benchmark dataset, collected via an it-\nerative, adversarial human-and-model-in-the-loop\nprocedure. Speciﬁcally, the instances are chosen to\nbe difﬁcult for the state-of-the-art models such as\nBERT and RoBERTa.\n•SWAG. It is a large-scale adversarial dataset\nfor the task of grounded commonsense inference,\nwhich uniﬁes natural language inference and phys-\nically grounded reasoning (Zellers et al., 2018).\nSW AG consists of 113k multiple choice questions\nabout grounded situations.\n•HELLASWAG. It is similar to SW AG but more\nchallenging (Zellers et al., 2019). For each query in\nHELLASWAG, it also has 4 choices and the goal\nis to ﬁnd the best choice among them.\n•SQuAD v1.1/v2.0. Stanford Question Answer-\ning Dataset (SQuAD) v1.1 and v2.0 (Rajpurkar\net al., 2016, 2018) are popular machine reading\ncomprehension benchmarks. Their passages come\nfrom approximately 500 Wikipedia articles and the\nquestions and answers are obtained by crowdsourc-\ning. The SQuAD v2.0 dataset includes unanswer-\nable questions about the same paragraphs.\n•BC2GM. The Gene Mention Task at the Biocre-\native II workshop (Smith et al., 2008) provides an\nannotated dataset for gene name entity recognition.\n•NCBI. The NCBI disease corpus (Dogan et al.,\n2014) contains annotations of disease mentions\nfrom a collection of PubMed abstracts.\n•JNLPBA. JNLBA is a biomedical entity recogni-\ntion shared task (Collier and Kim, 2004). It is one\nof the largest datasets covering a large fraction of\nmajor taxonomies in molecular biology.\n13\nCorpus Task #Train #Dev #Test #Label Metrics\nSingle-Sentence Classiﬁcation (GLUE)\nCoLA Acceptability 8.5k 1k 1k 2 Matthews corr\nSST Sentiment 67k 872 1.8k 2 Accuracy\nPairwise Text Classiﬁcation (GLUE)\nMNLI NLI 393k 20k 20k 3 Accuracy\nRTE NLI 2.5k 276 3k 2 Accuracy\nWNLI NLI 634 71 146 2 Accuracy\nQQP Paraphrase 364k 40k 391k 2 Accuracy/F1\nMRPC Paraphrase 3.7k 408 1.7k 2 Accuracy/F1\nQNLI QA/NLI 108k 5.7k 5.7k 2 Accuracy\nText Similarity (GLUE)\nSTS-B Similarity 7k 1.5k 1.4k 1 Pearson/Spearman corr\nPairwise Text Classiﬁcation\nSNLI NLI 549k 9.8k 9.8k 3 Accuracy\nSciTail NLI 23.5k 1.3k 2.1k 2 Accuracy\nANLI NLI 163k 3.2k 3.2k 3 Accuracy\nSpan Classiﬁcation\nSQuAD v1.1 MRC 87.6k 10.5k 9.5k - Exact Match (EM)/F1\nSQuAD v2.0 MRC 130.3k 11.9k 8.9k - Exact Match (EM)/F1\nRanking\nSW AG Multiple choice 73.5k 20k 20k - Accuracy\nHELLASW AG Multiple choice 34k 10 10k - Accuracy\nBiomedical Domain\nBC2GM NER 12.6k 2.5k 5k - F1/Precision/Recall\nNCBI NER 5.4k 923 940 - F1/Precision/Recall\nJNLPBA NER 14.7k 3.9k 3.9k - F1/Precision/Recall\nTable 9: Summary information of the NLP application benchmarks.",
  "topic": "Adversarial system",
  "concepts": [
    {
      "name": "Adversarial system",
      "score": 0.9472846388816833
    },
    {
      "name": "Computer science",
      "score": 0.7856805324554443
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6632030010223389
    },
    {
      "name": "Generalization",
      "score": 0.6316074132919312
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.6173639297485352
    },
    {
      "name": "Machine learning",
      "score": 0.5804291367530823
    },
    {
      "name": "Language model",
      "score": 0.5507494211196899
    },
    {
      "name": "Artificial neural network",
      "score": 0.5396959185600281
    },
    {
      "name": "Deep neural networks",
      "score": 0.5303239226341248
    },
    {
      "name": "Embedding",
      "score": 0.4973590672016144
    },
    {
      "name": "Adversary",
      "score": 0.48158907890319824
    },
    {
      "name": "Task (project management)",
      "score": 0.44838523864746094
    },
    {
      "name": "Training (meteorology)",
      "score": 0.41971322894096375
    },
    {
      "name": "Alum",
      "score": 0.4102972447872162
    },
    {
      "name": "Natural language processing",
      "score": 0.32077229022979736
    },
    {
      "name": "Engineering",
      "score": 0.11761227250099182
    },
    {
      "name": "Computer security",
      "score": 0.11133378744125366
    },
    {
      "name": "Mathematics",
      "score": 0.06997916102409363
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Meteorology",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Metallurgy",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 91
}