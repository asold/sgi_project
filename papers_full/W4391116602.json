{
  "title": "Augmenting Cognitive Architectures with Large Language Models",
  "url": "https://openalex.org/W4391116602",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2028382623",
      "name": "Himanshu Joshi",
      "affiliations": [
        "Qualcomm (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2343834513",
      "name": "Volkan Ustun",
      "affiliations": [
        "Creative Technologies (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2028382623",
      "name": "Himanshu Joshi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2343834513",
      "name": "Volkan Ustun",
      "affiliations": [
        "Creative Technologies (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2035239082",
    "https://openalex.org/W3138864939",
    "https://openalex.org/W6680265511",
    "https://openalex.org/W6748634344",
    "https://openalex.org/W2488240401",
    "https://openalex.org/W95274681",
    "https://openalex.org/W4380558503",
    "https://openalex.org/W2137813581",
    "https://openalex.org/W4289030408",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W419646410",
    "https://openalex.org/W2134440356",
    "https://openalex.org/W1517036110",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2088563966",
    "https://openalex.org/W2106602338",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4286973660",
    "https://openalex.org/W4389519585",
    "https://openalex.org/W4205991051"
  ],
  "abstract": "A particular fusion of generative models and cognitive architectures is discussed with the help of the Soar and Sigma cognitive architectures. After a brief introduction to cognitive architecture concepts and Large Language Models as exemplar generative AI models, one approach towards their fusion is discussed. This is then analyzed with a summary of potential benefits and extensions needed to existing cognitive architecture that is closest to the proposal.",
  "full_text": "Augmenting Cognitive Architectures with Large Language Models \nHimanshu Joshi1, Volkan Ustun2 \n1Wireless R&D Qualcomm Research, Qualcomm Technologies Inc \n2University of Southern California, Institute for Creative Technologies \nhimanshu@qti.qualcomm.com, ustun@ict.usc.edu \n \n \n \n \nAbstract \nA particular fusion of generative models and cognitive \narchitectures is discussed with the help of the Soar and Sigma \ncognitive architectures. After a brief introduction to cognitive \narchitecture concepts and Large Language Models as \nexemplar generative AI models, one approach towards their \nfusion is discussed. This is then analyzed with a summary of \npotential benefits and extensions needed to existing cognitive \narchitectures that is closest to the proposal. \n Introduction    \nGenerative AI models such as Large Language Models \n(LLM), Vision Transformer (ViT)  (Dosovitskiy, et al., \n2020), etc. models have captured much interest in the recent \nyears. Several proposals have been made towards leveraging \nstrengths of generative AI fused with other models such as \nLSTMs (Lester, Al -Rfou, & Constant, 2021) , cognitive \narchitectures (Wray, Kirk, & Laird, 2021) , or planning and \nreflection with LLMs (Park, et al., 2023)  to list a few. The \napproach of fusing planning and reflection with an LLM \nyielded good results for the tasks in simulated environment. \nThis has fueled more interest in the question of whether \nLLMs can be leveraged in cognitive architectures in a \nprincipled fashion with the aim of yielding an agent that can \nutilize the strengths of each approach in different situations \nyielding an integrated agent that is greater than the sum of \nits parts. This work proposes one possible fusion between \nLLMs and cognitive ar chitectures. The attempt here is to \npropose a creative fusion that connects ideas from both \ndisciplines to yield an approach that yields a result that is \npotentially better than when either approach – cognitive \narchitectures or LLMs – is used in isolation. The rest of the \npaper is organized as follows: first cognitive architectures \nare discussed summarizing key aspects relevant in this \ncontext, this is followed by a discussion on generative AI \nwith a focus on LLMs. LLMs are presented in cognitive \narchitecture terminology. Subsequently a fusion of LLMs \n \nCopyright © 2023, Association for the Advancement of Artificial \nIntelligence (www.aaai.org). All rights reserved. \nwith cognitive architectures is discussed in the context on a \nparticular kind of task: interactive task learning (Kirk J., \n2019). This is then analyzed with a discussion on the \npotential merits and issues this approach yields. Finally, \nconclusion and future work are discussed. \nCognitive Architectures \nCognitive architectures model fixed structures and \nprocesses underlying human intelligence.  Cognitive \narchitectures have a rich history and are a result of Newell’s \nstrategy (Newell, 1973)  combined with early work on \nhuman problem solving. The key premise is that the problem \nof general human intelligence is difficult because humans \nact in a variety of diverse environments and that this is made \npossible because of the ability of the human mi nd to \ncombine a variety of knowledge sources dynamically on \ndemand  (Newell, 1990) . The focus then – as a research \nstrategy – is to model the common cognitive architecture \nthat enables the combination of large knowledge sources. \nCognitive architectures can be characterized in terms of \nthe structures – memories – that hold the agent’s beliefs, \ngoals, knowledge etc., the representations for these and the \nprocesses that act on them  (Langley, Laird, & Rogers, \n2009). Here, we briefly discuss relevant aspects of cognitive \narchitectures from the perspective of Soar and Sigma like \narchitectures. Very briefly, memories can be either: long \nterm memory, which is further categorized as declarative \n(knowledge capturing asser tions of the agent), procedural \n(encoding knowledge about how a certain task is \nperformed), episodic (encoding knowledge about \nhistory/episodes), and semantic (encoding facts the agent \nbelieves), and short -term working memory, which holds \nknowledge relevan t to the agent in its current situation. \nKnowledge is typically represented in terms of predicates \nand patterns over predicates. Predicates and predicate \npatterns can be purely symbolic – as in Soar (Laird, 2012) – \n \nAAAI Fall Symposium Series (FSS-23)\n281\nor have sub -symbolic aspects as in Sigma  (Rosenbloom, \nDemski, & Ustun, 2016) . In Sigma’s case predicates can \ndesignate learnable functions. In addition to predicates, \nthere are operators in both Sigma and Soar. Operators \nrepresent actions that the agent can take. Central to the \nprocesses that act on this knowledge arranged/organiz ed \nacross various memories is the cognitive cycle – \neponymously named after the human cognitive cycle. The \ncognitive cycle represents about 50ms of human mental \nactivity and involves four phases: integrating new \nperception, elaborating current state, selec ting the next \naction to take – by selection of an operator – to be finally \nfollowed by effecting any changes in the working memory \n(learning) as well as those requiring any output via the motor \nsystem. In Sigma operator selection is aided by numerical \nmetadata – in the form of utilities and in Soar operator \nselection is aided by numeric preferences that can be learnt \n(as a case of reinforcement learning). The cognitive cycle \nimplements a parallel to serial bottleneck where parallel \nprocessing – of multiple rule firings (Soar) or message \npassing (Sigma) – is followed by deliberate selection of an \noperator analogous to the human cognitive cycle. \nKnowledge is organized according to the Problem Space \nComputation Model (PSCM) in Soar and Sigma. The PSCM \nis defined as a goal, associated state in the working memory \nand available operators – all relevant to a particular problem \ndomain. Processing in this setting can be divided into: \n• reactive: the ‘mindless’ aspects of cognition that \nrepresent the activity within a cognitive cycle,  \n• deliberative: the ‘mindful’ aspects of cognition that \nrepresent a series or sequence of decision cycles, and, \n• reflective: the reflective or meta -aspects of cognition \nwhere the agent examines its own state and makes \nmodification to its internal state. \nThis tri -level processing model is supported by the \ndecision cycle via the impasse mechanism, where, upon \nfailure to select an operator in the operator selection phase, \nan impasse results bringing to bear more knowledge by \ncreating a sub-goal to the current goal and a sub-state to the \ncurrent state. \nLearning occurs at multiple levels. Procedural learning \nentails learning rules that prevent future impasses by \ncreating a knowledge fragment with preconditions that led \nto the impasse and predicate change – as the action part of \nthe rule – that resolved the impasse. Soar supports chunking \nbut Sigma does not yet support chunking. Sigma’s cognitive \ncycle is based in graphical models – modified factor graphs \n– and the elaboration phase involves a form of message \npassing. Learning in this context involves upda ting the \nfactors with posterior after message passing. Sigma has \ndemonstrated learning of acoustic models, language models, \nvarious forms of deep learning such as feedforward \nmultilayer perceptrons, recurrent neural networks \n(Rosenbloom, Demski, & Ustun, 2017). When the factors in \na factor graph (Kschischang, Frey, & Loeliger, 2001) – such \nas the kind of network Sigma’s processing and knowledge \nare grounded in – are fully differentiable, factor graphs \nreduce to deep networks and message passing with suitable \nmodifications for regularization can yield learning similar to \ngradient desc ent with backprop. Sigma has also shown \nlearning of fixed word embeddings using random \nprojections. \nThe rest of this work assumes an architecture that is \nsimilar in spirit to Soar and Sigma, with a tri -level \nprocessing that supports PSCM and a cognitive cycle that is \ngrounded in a form of graphical models that is similar to \nSigma and supports chunking like Soar. \nLarge Language Models \nLarge Language Models (LLMs) have gained tremendous \npopularity over the last few years due to their ability to be \nversatile problem solvers in across several natural language \n(NL) tasks and even beyond NL domain. LLMs are trained \non a very large dataset and require significant investment of \nresources to train. LLMs are made of layers of stacked \ntransformer models (Vaswani, et al., 2017) which operate on \nthe concept of ‘attention’ i.e. each word in the input \nsequence determines how much influence or ‘attention’ to \npay to the other words in the input sequence. Attention \ninvolves calculating three intermediate quantities – the \nquery, key, a nd value vectors – for each word in the input \nsequence. Each word is ‘embedded’ in a low dimensional \nspace and then input to the transformer stack. At the top of \nthe transformer stack are classifier ‘heads’ that generate a \ndistribution over the predicted next word. To begin with, the \ninput vocabulary is transformed into sub -word units called \n‘tokens’ using some method such as byte pair encoding \n(BPE) (Shibata, et al., 1999) . This helps the model handle \nout of vocabulary words so that any word the model may \nencounter in the future can be tokenized. These tokens are \nembedded. Training then involves using gradient descent to \npredict a (set of) target word(s) – such as the next word(s), \nor a masked word(s) – in the context of the last several \ntokens in the input sequence. Training then learns the model \nparameters along with embeddings for the input tokens. \n In the context of previously discussed concepts of \ndifferent memories in the cognitive architecture, we can \nthink of the LLM transformer stack to be a form of \ndeclarative memory clubbed with a procedural ‘classifier \nhead.’ The lower layers in the transformer stack learn lexical \nfeatures, the middle layers learn syntactic features and the \ntop layers near the head learn context sensitive semantic \nfeatures of the target token – in the context of the input \ntokens – while the classifier head can be understood a s \npredicate rules that act to choose the next word (‘choose’ \n282\nhere means generating a distribution over the target token). \nThe predicted tokens can be sampled using a sampling \ntechnique and then the sampled tokens can be converted to \nwords using a decoding strategy such as top -k decoder. \nOnce an LLM is trained on a large dataset, there are several \nways to use it in a downstream specialized task: \n• finetune (Peters, et al., 2018) : The LLM is optionally \n“frozen” and a new classifier head is trained tailored to \nthe new task specific dataset, and, \n• prompting (Reynolds & McDonell, 2021): when the LLM \nis provided with an input prompt, it generates an output \nsequence of tokens that when converted to words appears \nvery coherent and meaningful. There are several forms of \nprompting methods, including analogical prompting, \ntemplatized prompting etc. \nPrompting is very popular because the model is frozen after \ninitial training and not subsequently updated. Prompting \ntemplates were initially hand tuned but some work has \nattempted to search good task specific prompts (Shin, \nRazeghi, Logan IV, Wallace, & Singh, 2020) . Here a more \nrelevant approach is that of soft prompt training (Lester, Al-\nRfou, & Constant, 2021) , (Liu, et al.)  where task specific \n‘soft tokens’ – i.e., tokens that were not in the original token \nspace of the model when it was trained – are inserted in the \nprompt of the model. The soft tokens are encoded using an \nLSTM and then inserted with the other tokens in the prompt. \nThen the soft tokens are learnt in a supervised fashion on a \nper task basis. Once training results in soft token \nembeddings, the LSTM is no longer needed, and the soft \ntoken embeddings can be used in the same fashion by \ninserting them in the task prompts using the same template. \nThe advantage of this method is two-fold: the LLM is frozen \nand does not need to be updated, the task specific tokens can \nbe saved, and new tokens initialized and trained for a new \ntask. This results in two benefits: firstl y the amount of \ntraining data required is lower, and secondly, the number of \nparameters trained is far lower than what would be required \nif the LLM was being fine -tuned with no loss in \nperformance for very large LLM sizes (Lester, Al-Rfou, & \nConstant, 2021). \nLLM Usage in Cognitive Architecture \nThere are multiple ways in which LLMs can be used in \ncognitive architectures – as a model of the world, as a \nreasoning agent that can select actions when prompted with \nthe current state of the agent etc. Here one potential method \nof LLM integration is proposed. While the LLM itself may \nnot be trained in a cognitively plausible fashion, the \nintegration of the LLM with cognitive architecture is \nattempted in a cognitively plausible manner. \n To begin with, it is assumed here that the LLM itself is \nnot updated as this is prohibitively costly. The core vision \nhere is that an LLM can be used as a prompt-able declarative \nmemory in a Sigma/Soar like impasse driven architecture \nwhere the architecture can prompt the LLM with a task \nspecific prompt to extract knowledge from the LLM coupled \nwith task specific operators with learnable continuous \nembeddings that are inserted in t he LLM prompt based on \nthe agent’s goals, knowledge of the task, contents of  the \nworking memory – that include the current situation – and \nthe current operators that are proposed.  \n The cognitive cycle supports the ability to learn this \ncontinuous representation by using an algorithm similar to \nSigma’s message passing algorithm. An impasse can be \ntriggered by proposing an operator to impasse which will \nthen create a substate with the  subgoal to bring knowledge \nfrom LLM to bear on current situation. The task specific \noperator embeddings in each such substate can be initialized \nfrom parent state’s corresponding embeddings in \nconjunction with lexical embeddings that will be used to \nprompt the model in subsequent step. Prompting the LLM \ninvolves inserting these task specific learnable soft tokens \nas described in previous section. Multiple prompts can be \ngenerated based on the goal and various ways in which the \nsoft tokens can be embedded with prompt text.  \n The prompts themselves can be stored as task specific or \ngeneral knowledge and several prompt templates can be \nselected to be used in parallel i.e., in a reactive manner in \nthe elaboration phase of the model. Several relevant \ntemplates have been identifie d in (Wray, Kirk, & Laird, \n2021) in the context of Soar’s interactive task learning \nproblem formulation. Knowledge obtained from the LLM \ncan be problematic due to several reasons – LLMs \nhallucinate (McKenna, et al., 2023) , and their output is not \nalways reliable (Wray, Kirk, & Laird, 2021). Once multiple \nresponses are retrieved in parallel, one response can be \nselected by combining knowledge from working memory – \nthat elaborates the current situation – and curated knowledge \nfrom curated long-term memories such as episodic memory, \nsemantic m emory etc. In Sigma, this can potentially be \nimplemented as a simple classifier that scores the responses.  \n During the selection phase of the cognitive cycle learning \nupdates the continuous soft token embeddings and when the \nimpasse resolves, the learnt soft tokens embeddings \nrepresent a description of the knowledge that was required \nfrom the LLM to resolve the  impasse as a function of the \ncurrent state of the agent and its goals for every task \noperator. Having a labeled dataset which can propagate a \nsignal back from the LLM to the soft token embeddings will \nhelp. However, it is important to note the soft token \nembeddings are learnt not just from the signal from the LLM \nbut from the classifier that scores the responses and \npotentially accounts for operator utilities derived from task \nspecific knowledge. If the output of the LLM is not usable \nbecause the agent doe s not have actions available – either \nbecause it does not know how to perform the suggested \n283\naction, or its current state indicates the suggested action is \nimpossible – another impasse can be created to resolve this.  \n When a new action operator is created by the agent \nbecause it does not know how to perform an action \nsuggested by the LLM, the associated soft token embedding \nwith it will be learnt by further querying the LLM to break \nthe complex operator action into a s et of simplified actions \nuntil an action or set of actions are found that the agent can \nperform. The operator specific embedding that was learnt \nfor all action operators can be used to determine semantic \ncloseness of actions and the agent can try to substi tute such \nactions. Unlike semantic embeddings for words that indicate \nsemantic similarity in the embedded space, these operator \nembeddings will be a function not just of the words but the \ncurrent state of the agent as well. When action operators are \navailable to perform the action, the embeddings can aid – in \nconjunction with other numerical metadata such as utilities \n– in planning required to generate a policy over them. When \nthe impasse that led to querying the LLM is resolved, the top \nstate shall have po tentially created (action) operators or \nupdates to predicates with associated embeddings and these \ncan be used to update the predicate/operator embeddings in \nthe top state. These embeddings can be subsequently used \nas task and state specific embeddings and  brought to bear \neither to prevent future impasse in a potentially different \nsituation. \n The work closest in approach to the proposed scheme is \n(Kirk, Lindes, & Peter, 2023) where the authors propose and \nevaluate a Soar based framework (STARS) to query and use \nknowledge from an LLM in the ITL task set. STARS stands \nfor Search Tree, Analyze, Repair, and Select corresponding \nto the phases of the framework where LLM is prompted with \nhierarchical tree templatized prompts and a beam search is \nperformed to narrow the responses to the most probable set \nof responses. These are analyzed and evaluated for their \nusefulness in the current situation and the best one is \nselected by querying  the LLM again. The key difference \nhere is that the whole scheme is working with discrete token \nprompts derived from discrete words. As discussed \npreviously and based on results from (Lester, Al-Rfou, & \nConstant, 2021) , more task specific data is needed when \nworking in the discrete prompt domain without soft tokens. \nIn this context, this would mean the ST, A, and R phases \nhave more work to do. The hierarchical tree-based templates \nare simulating structural properties of  the task domain as \nencoded in the English language. The interspersing of soft \ntokens with state prompt using templates in the scheme \nproposed in this paper corresponds to this ST phase. \nAnalysis will take place similar to what is proposed in \nSTARS but aid ed by availability of embeddings. Finally, \nrepair will take place via an impasse mechanism in this \nproposal. Selection of the next action is left to the cognitive \narchitecture in this proposal. In STARS evaluation, the ‘S’ \nselection phase – where the actio n is selected by the LLM \nitself – did not improve the agent’s task completion \nperformance in the task that was evaluated. In the scheme \nproposed here, action selection is done by the cognitive \narchitecture with the aid of embeddings, utilities on \noperators, and the contents of the working memory i.e., the \nLLM is used to elicit knowledge in a reactive manner only \nand captured in the task specific operator embeddings which \nare subsequently used reactively (generator selection score \nbased on utilities on operators as well as the embedding) as \nwell as deliberatively (operator selection). It is unclear how \nthe STARS phases work in the context of Soar’s tri -level \ncontrol. Furthermore, it is unclear whether the beam search \ninvolved in the STARS is cognitively plausible and to what \nextent Soar’s cognitive cycle supports handling probabilistic \nprocessing. \n The idea to use embeddings to aid search is not new, \nneither is the idea to use LLM’s to aid in planning (learning \na policy over actions), or reflection (impasse processing). \nWhat is new here is the integration of LLM in a cognitive \nsetting with soft tokens on task and state specific operators \nthat can be used to prompt and extract knowledge from the \nLLM. In the p -tuning work where soft tokens were \nintroduced, they experimented with a few prompt insertion \ntemplates. A cognitive architecture can improve upo n this \nmanual search of finding suitable prompt insertion template \nby bringing to bear its mechanisms and knowledge from \nother memories – such as episodic or semantic – to guide \nthis search potentially improving upon the back and forth \nrequired with the LLM i.e., the data required to learn the soft \ntoken embeddings.  \n To evaluate the proposal presented here, the ITL domain \nseems the most natural. Sigma is the most natural candidate \nto consider for evaluating. Sigma’s decision cycle is both \nmixed (symbolic+subsymbolic, including capable of neural \nprocessing) and hybrid (discrete+continuous) as required by \nproposal. Sigma’s cognitive cycle does not yet support \nembeddings on operators and this is an extension that will \nhave to be added. \nConclusion and Future Work \nA method to augment cognitive architectures with \ngenerative LLM memory was proposed. The integration \nassumes a cognitive cycle that is capable of simultaneously \nprocessing symbolic and sub -symbolic information. \nVarious aspects of the integration have been independently \ndemonstrated in Sigma or Soar but some extensions to \nSigma will have to be made to support the proposal. \n \n284\nAcknowledgments \nThe authors would like to thank Taesang Yoo and June \nNamgoong of Wireless R&D, Qualcomm Research for \nseveral fruitful discussions on LLMs and cognitive \narchitectures. \n References  \nDosovitskiy, A., Beyer, L., Kolsenikov, A., Weissenborn, D., Zhai, \nX., Unterthiner, T., & Houlsby, N. 2020. An image is worth 16x16 \nwords: Transformers for image recognition at scale. arXiv preprint, \narXiv:2010.11929. \nJoshi, H., Rosenbloom, P. S., & Ustun, V. 2014. Isolated word \nrecognition in the Sigma cognitive architecture. Biologically \nInspired Cognitive Architectures(10), 1-9. \nKirk, J. , 2019. Learning Hierarchical Compositional Task \nDefinitions through Online Situated Interactive Language \nInstruction. PhD dissertation, Department of Computer Science, \nUniversity of Michigan, Ann Arbor, MI.  \nKirk, J. R., Lindes, P., & Wray, R. 2023. Improving Knowledge \nExtraction from LLMs for Robotic Task Learning through Agent \nAnalysis . arXiv preprint, arXiv:2306.06770 [cs.AI]. Ithaca, NY: \nCornell University Library. \nKschischang, F. R., Frey, B. J., & Loeliger, H. -A. 2001. Factor \ngraphs and the sum -product algorithm. IEEE Transactions on \nInformation Theory, 47, 498-519. \nLaird. 2012. The Soar Cognitive Architecture.  Cambridge, MA: \nMIT Press. \nLangley, P., Laird, J. E., & Rogers, S. 2009. Langley, Pat, John E. \nLaird, and Seth Rogers. Cognitive Systems Research, 10 (2), 141-\n160. \nLester, B., Al-Rfou, R., & Constant, N. 2021. The power of scale \nfor parameter -efficient prompt tuning. arXiv preprint, \narXiv:2104.08691 [cs.CL]. Ithaca, NY: Cornell University \nLibrary. \nMcKenna, N., Li, T., Cheng, L., Hosseini, M. J., Johnson, M., & \nSteedman, M. 2023. Sources of Hallucination by Large Language \nModels on Inference Tasks. arXiv preprint , arXiv:2305.14552 \n[cs.CL]. Ithaca, NY: Cornell University Library. \nNewell. 1973. You can't play 20 questions with nature and win: \nProjective comments on the papers of this symposium. (W. G. \nChase, Ed.) Visual Information processing. \nNewell. 1978. Harpy, production systems and human cognition.  \nPerception and production of fluent speech, 299-380. \nNewell. 1990. Unified theories of cognition.  Cambridge, MA: \nHarvard University Press. \nPark, J. S., O'Brien, J. C., Cai, C. J., Morris, M. R., Liang, P., & \nBernstein, M. S. 2023. Generative agents: Interactive simulacra of \nhuman behavior. arXiv preprint arXiv:2304.03442 [cs.HC]. Ithaca, \nNY: Cornell University Library. \nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., \nLee, K., & Zettlemoyer, L. 2018. Deep contextualized word \nrepresentations. CoRR abs/1802.05365. \nRaschka, S. 2023 Understanding Large Language Models -- A \nTransformative Reading List. \nhttps://sebastianraschka.com/blog/2023/llm-reading-list.html. \nAccessed: 2023-08-01. \nReynolds, L., & McDonell, K. 2021. rompt programming for large \nlanguage models: Beyond the few -shot paradigm. . CHI \nConference on Human Factors in Computing Systems Extended \nAbstracts of the 2021 , pp 1-7. \nRosenbloom. 2012a. Deconstructing reinforcement learning in \nSigma. Proceedings of the 5th Conference on Artificial General \nIntelligence, (pp. 262-271). \nRosenbloom. 2013. The Sigma cognitive architecture and system. \nAISB Quarterly, 136, 4-13. \nRosenbloom, P. S., Demski, A., & Ustun, V. 2016. The Sigma \ncognitive architecture and system: Towards functionally elegant \ngrand unification. Journal of Artificial General Intelligence , 7, 1-\n103. \nRosenbloom, P. S., Demski, A., & Ustun, V. 2016. The Sigma \ncognitive architecture and system: Towards functionally elegant \ngrand unification. Journal of Artificial General Intelligence , 7, 1-\n103. \nShibata, Y., Kida, T., Fukamachi, S., Takeda, M., Shinohara, A., \nShinohara, T., & Arikawa, S. , 1999. Byte Pair encoding: A text \ncompression scheme that accelerates pattern matching.  \nhttps://www.researchgate.net/profile/Takeshi-\nShinohara/publication/2310624_Byte_Pair_Encoding_A_Text_C\nompression_Scheme_That_Accelerates_Pattern_Matching/links/0\n2e7e522f8ea00c318000000/Byte-Pair-Encoding-A-Text-\nCompression-Scheme-That-Accelerates-Pattern-Matching.pdf. \nAccessed: 2023-08-01. \nShin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., & Singh, S. \n2020. Autoprompt: Eliciting knowledge from language models \nwith automatically generated prompts. arXiv preprint , \narXiv:2010.15980 [cs.CL]. Ithaca, NY: Cornell University \nLibrary. \nWray, R. E., Kirk, J. R., & Laird, J. E. 2021. Language Models as \na Knowledge Source for Cognitive Agents. arXiv preprint , \narXiv:2109.08270 [cs.AI]. Ithaca, NY: Cornell University Library. \n285",
  "topic": "Soar",
  "concepts": [
    {
      "name": "Soar",
      "score": 0.954272449016571
    },
    {
      "name": "Cognitive architecture",
      "score": 0.8386882543563843
    },
    {
      "name": "Generative grammar",
      "score": 0.7151054739952087
    },
    {
      "name": "Computer science",
      "score": 0.6951961517333984
    },
    {
      "name": "Cognition",
      "score": 0.693011999130249
    },
    {
      "name": "Architecture",
      "score": 0.5944262146949768
    },
    {
      "name": "Cognitive science",
      "score": 0.553456723690033
    },
    {
      "name": "Artificial intelligence",
      "score": 0.483682245016098
    },
    {
      "name": "Cognitive model",
      "score": 0.4669158458709717
    },
    {
      "name": "Natural language processing",
      "score": 0.33741116523742676
    },
    {
      "name": "Psychology",
      "score": 0.17791670560836792
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I19268510",
      "name": "Qualcomm (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210087747",
      "name": "Creative Technologies (United States)",
      "country": "US"
    }
  ]
}