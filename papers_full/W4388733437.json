{
  "title": "Perils and Opportunities in Using Large Language Models in Psychological Research",
  "url": "https://openalex.org/W4388733437",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3101608876",
      "name": "Suhaib Abdurahman",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2142197048",
      "name": "Mohammad Atari",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2754745577",
      "name": "Farzan Karimi Malekabadi",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A4270989430",
      "name": "Mona J. Xue",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A4201768271",
      "name": "Jackson Trager",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2163000534",
      "name": "Peter S. Park",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3029530119",
      "name": "Preni Golazizian",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A3013656336",
      "name": "Ali Omrani",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2145804492",
      "name": "Morteza Dehghani",
      "affiliations": [
        "University of Southern California"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4294959075",
    "https://openalex.org/W2143474508",
    "https://openalex.org/W2314545043",
    "https://openalex.org/W2043045839",
    "https://openalex.org/W6846260263",
    "https://openalex.org/W2922202159",
    "https://openalex.org/W4327672398",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2788481061",
    "https://openalex.org/W6646014332",
    "https://openalex.org/W3032237992",
    "https://openalex.org/W6667970269",
    "https://openalex.org/W4381163899",
    "https://openalex.org/W6657177047",
    "https://openalex.org/W6795342411",
    "https://openalex.org/W6777560052",
    "https://openalex.org/W6857745965",
    "https://openalex.org/W6676391964",
    "https://openalex.org/W2111267928",
    "https://openalex.org/W4321074112",
    "https://openalex.org/W6784358232",
    "https://openalex.org/W6639660458",
    "https://openalex.org/W4382240547",
    "https://openalex.org/W6855001726",
    "https://openalex.org/W4296169830",
    "https://openalex.org/W4362510886",
    "https://openalex.org/W6728271715",
    "https://openalex.org/W2606089314",
    "https://openalex.org/W6842813255",
    "https://openalex.org/W4380763235",
    "https://openalex.org/W4285192297",
    "https://openalex.org/W6786018678",
    "https://openalex.org/W3174174150",
    "https://openalex.org/W3008655042",
    "https://openalex.org/W6759579507",
    "https://openalex.org/W4225165463",
    "https://openalex.org/W6765162019",
    "https://openalex.org/W1641003075",
    "https://openalex.org/W4360802167",
    "https://openalex.org/W3022133124",
    "https://openalex.org/W6838865847",
    "https://openalex.org/W6683532468",
    "https://openalex.org/W4384261711",
    "https://openalex.org/W4385571890",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W3170390204",
    "https://openalex.org/W4366420437",
    "https://openalex.org/W4294982692",
    "https://openalex.org/W2005151061",
    "https://openalex.org/W6736067386",
    "https://openalex.org/W4375930358",
    "https://openalex.org/W6845398906",
    "https://openalex.org/W4323239061",
    "https://openalex.org/W6759320174",
    "https://openalex.org/W3161588210",
    "https://openalex.org/W1565959449",
    "https://openalex.org/W2072385612",
    "https://openalex.org/W6736575291",
    "https://openalex.org/W4224296643",
    "https://openalex.org/W4365512576",
    "https://openalex.org/W3130319171",
    "https://openalex.org/W652269744",
    "https://openalex.org/W6765510851",
    "https://openalex.org/W18965947",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W3034344071",
    "https://openalex.org/W4308760226",
    "https://openalex.org/W6669465837",
    "https://openalex.org/W2147792648",
    "https://openalex.org/W6850007463",
    "https://openalex.org/W4313197536",
    "https://openalex.org/W6735236233",
    "https://openalex.org/W4366277658",
    "https://openalex.org/W2911300548",
    "https://openalex.org/W4375853569",
    "https://openalex.org/W4375958655",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W4376612976",
    "https://openalex.org/W2140910804",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2809932687",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W6767060491",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W1985063293",
    "https://openalex.org/W2725671465",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2112902824",
    "https://openalex.org/W6849219588",
    "https://openalex.org/W6713134421",
    "https://openalex.org/W2070459595",
    "https://openalex.org/W2618617296",
    "https://openalex.org/W4214903622",
    "https://openalex.org/W3094938719",
    "https://openalex.org/W6821113053",
    "https://openalex.org/W4312056202",
    "https://openalex.org/W2012757283",
    "https://openalex.org/W1584308190",
    "https://openalex.org/W6634627538",
    "https://openalex.org/W2070428205",
    "https://openalex.org/W6675354045",
    "https://openalex.org/W2582743722",
    "https://openalex.org/W4377865309",
    "https://openalex.org/W4365601249",
    "https://openalex.org/W4363671827",
    "https://openalex.org/W4389518978",
    "https://openalex.org/W4301481428",
    "https://openalex.org/W4362673335",
    "https://openalex.org/W4396796749",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4389519325",
    "https://openalex.org/W4367000100",
    "https://openalex.org/W4287207937",
    "https://openalex.org/W3104119924",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W4385430086",
    "https://openalex.org/W4382404016",
    "https://openalex.org/W4366733439",
    "https://openalex.org/W4367679822",
    "https://openalex.org/W4386249236",
    "https://openalex.org/W4249645064",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4292545669",
    "https://openalex.org/W2883775592",
    "https://openalex.org/W4388318971",
    "https://openalex.org/W4379925078",
    "https://openalex.org/W4378770584",
    "https://openalex.org/W4361866126",
    "https://openalex.org/W4367623529",
    "https://openalex.org/W4372272969",
    "https://openalex.org/W4362656192",
    "https://openalex.org/W3153094109",
    "https://openalex.org/W2070907364",
    "https://openalex.org/W2530872785",
    "https://openalex.org/W4322588812",
    "https://openalex.org/W4361806395",
    "https://openalex.org/W3206487987",
    "https://openalex.org/W4233862346",
    "https://openalex.org/W2973379954",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4377098551",
    "https://openalex.org/W4376654357",
    "https://openalex.org/W4379933116",
    "https://openalex.org/W4362700315",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W4387389584",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4383473944",
    "https://openalex.org/W2953384591",
    "https://openalex.org/W4383647972",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4235791242",
    "https://openalex.org/W4382619745",
    "https://openalex.org/W2963119602",
    "https://openalex.org/W4385571830",
    "https://openalex.org/W4235316990",
    "https://openalex.org/W4379259169",
    "https://openalex.org/W4313559133",
    "https://openalex.org/W4319083882",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2953958347",
    "https://openalex.org/W4235765578",
    "https://openalex.org/W3034282334",
    "https://openalex.org/W4246976400",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4254816979",
    "https://openalex.org/W4298050894",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W4385897789",
    "https://openalex.org/W4306808680",
    "https://openalex.org/W4378474285",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W3162966461",
    "https://openalex.org/W4306247398",
    "https://openalex.org/W4231627133",
    "https://openalex.org/W4246724958",
    "https://openalex.org/W4384694026",
    "https://openalex.org/W4239510810",
    "https://openalex.org/W4322008875",
    "https://openalex.org/W4385363516",
    "https://openalex.org/W4390723916",
    "https://openalex.org/W4385734111",
    "https://openalex.org/W4236033185",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2161411308",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4385966247",
    "https://openalex.org/W4322720178",
    "https://openalex.org/W2963499153",
    "https://openalex.org/W2915177913",
    "https://openalex.org/W2242464395",
    "https://openalex.org/W4210984920",
    "https://openalex.org/W2159397589",
    "https://openalex.org/W4389523771",
    "https://openalex.org/W3015368184",
    "https://openalex.org/W4250431948",
    "https://openalex.org/W1576726980",
    "https://openalex.org/W2027473728",
    "https://openalex.org/W4321524280",
    "https://openalex.org/W4291220703",
    "https://openalex.org/W2601450892",
    "https://openalex.org/W4385571495",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2953522645",
    "https://openalex.org/W4391090624",
    "https://openalex.org/W4386696538",
    "https://openalex.org/W4390513007",
    "https://openalex.org/W2489406233",
    "https://openalex.org/W4377231151",
    "https://openalex.org/W4327811957",
    "https://openalex.org/W4361863170",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4399650690",
    "https://openalex.org/W3022499311",
    "https://openalex.org/W4365139036",
    "https://openalex.org/W4378508547",
    "https://openalex.org/W4379255800",
    "https://openalex.org/W4386567020",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4241566321",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W1810943226"
  ],
  "abstract": "The emergence of large language models (LLMs) has sparked considerable interest in their potential application in psychological research, either as a human-like entity used as a model for the human psyche or as a general text-analysis tool. However, carelessly using LLMs in psychological studies, a trend we rhetorically refer to as ``GPTology,'' can have negative consequences, especially given the convenient access to models such as ChatGPT. We elucidate the promises, limitations, and ethical considerations of using LLMs in psychological research. First, LLM-based research should pay attention to the substantial psychological diversity around the globe, as well as demographic diversity within populations. Second, while LLMs are convenient tools, we caution against treating them as a one-size-fits-all method for psychological text analysis. Third, LLM-based psychological research needs to develop methods and standards to compensate for LLMs' opaque black-box nature to facilitate reproducibility, transparency, and robust inference from AI-generated data.While acknowledging the prospects offered by LLMs for easy task automation (e.g., text annotation) and to expand our understanding of human psychology (e.g., by contrasting human and machine psychology), we make a case for diversifying human samples and expanding psychology's methodological toolbox to achieve a truly inclusive and generalizable science, rather than homogenizing samples and methods through over-reliance on LLMs.",
  "full_text": "PERILS AND OPPORTUNITIES OF LLMS 1\nPerils and Opportunities in Using Large Language Models in Psychological\nResearch\nSuhaib Abdurahman1,2,*, Mohammad Atari3,*, Farzan Karimi-Malekabadi1,2, Mona J.\nXue3, Jackson Trager1,2, Peter S. Park4, Preni Golazizian2,5, Ali Omrani2,5, Morteza\nDehghani1,2,5\n1Department of Psychology, University of Southern California\n2Brain and Creativity Institute, University of Southern California\n3Department of Human Evolutionary Biology, Harvard University\n4Department of Physics, Massachusetts Institute of Technology\n5Department of Computer Science, University of Southern California\nAuthor Note\n* These authors contributed equally to this work.\nMohammad Atari is now at the Department of Psychological and Brain Sciences,\nUniversity of Massachusetts Amherst. Correspondence concerning this article should be\naddressed to Mohammad Atari, Department of Psychological and Brain Sciences,\nUniversity of Massachusetts Amherst, 135 Hicks Way, Amherst, MA 01003, USA. E-mail:\nmatari@umass.edu\nPERILS AND OPPORTUNITIES OF LLMS 2\nAbstract\nThe emergence of large language models (LLMs) has sparked considerable interest in their\npotential application in psychological research, either as a human-like entity used as a\nmodel for the human psyche or as a general text-analysis tool. However, carelessly using\nLLMs in psychological studies, a trend we rhetorically refer to as “GPTology,” can have\nnegative consequences, especially given the convenient access to models such as ChatGPT.\nWe elucidate the promises, limitations, and ethical considerations of using LLMs in\npsychological research. First, LLM-based research should pay attention to the substantial\npsychological diversity around the globe, as well as demographic diversity within\npopulations. Second, while LLMs are convenient tools, we caution against treating them as\na one-size-fits-all method for psychological text analysis. Third, LLM-based psychological\nresearch needs to develop methods and standards to compensate for LLMs’ opaque\nblack-box nature to facilitate reproducibility, transparency, and robust inference from\nAI-generated data.\nWhile acknowledging the prospects offered by LLMs for easy task automation (e.g.,\ntext annotation) and to expand our understanding of human psychology (e.g., by\ncontrasting human and machine psychology), we make a case for diversifying human\nsamples and expanding psychology’s methodological toolbox to achieve a truly inclusive\nand generalizable science, rather than homogenizing samples and methods through\nover-reliance on LLMs.\nKeywords: psychology, large language models, natural language processing,\npsychological diversity, psychological text analysis, reproducibility, transparency\nPERILS AND OPPORTUNITIES OF LLMS 3\nPerils and Opportunities in Using Large Language Models in Psychological\nResearch\nTechnological innovations have enabled social and behavioral scientists to gather\ndiverse forms of data about human psychology, paving the way for significant\nbreakthroughs in psychological science and neighboring fields. These advancements have\nplayed a pivotal role in expanding our understanding of psychological processes. The\ndevelopment of neuroimaging (e.g., fMRI), online survey platforms (e.g., Mechanical Turk),\nand eye-tracking technology are just a few examples that have revolutionized psychological\nresearch in the last few decades. The digital revolution and emergence of “big data”\nfacilitated the establishment of new fields such as computational social science (Lazer et al.,\n2009). More recently, there has been a notable paradigm shift in Artificial Intelligence (AI)\nwith the emergence of Large Language Models (LLMs): neural networks characterized by\ntheir deep layers and extensive scale, typically consisting of billions to over a hundred\ntrillion parameters and trained on vast text datasets, enabling an unprecedented ability to\nunderstand, generate, and translate human language with remarkable subtlety and\ncomplexity. These AI models are trained on extensive collections of unlabeled text using\nself-supervised or semi-supervised learning methods, contributing to their remarkable\nlanguage understanding and generation capacity. This new technology has been argued to\npossess the capacity to transform social science research (Grossmann et al., 2023).\nLanguage models have advanced significantly through interdisciplinary\ncollaboration, including contributions from psychology, which laid the foundation for\nmodern language modeling. For example, inspired by the psychology of feedback and\nlearning mechanisms, McClelland and Rumelhart (1985) explored connectionist models\ndemonstrating the potential of neural networks, and Rumelhart et al. (1986) introduced\nkey algorithms like backpropagation. Building upon these developments and motivated by\ntheories in working memory and cognitive processes, Elman (1990) introduced Recurrent\nNeural Networks, enabling researchers to model sequential data in language models.\nPERILS AND OPPORTUNITIES OF LLMS 4\nThe availability of large textual corpora, increased computing power, and\nadvancements in deep-learning techniques have recently contributed to the progress and\nrefinement of language models. Notably, the Transformers architecture (Vaswani et al.,\n2017), which enables understanding of intricate relationships between different input\ncomponents in a remarkably efficient and precise manner substantially improved natural\nlanguage understanding and generation capabilities. Subsequently, LLMs, like ChatGPT\n(OpenAI, 2022), based on different versions of Generative Pre-trained Transformer (GPT;\nBrown et al., 2020; Dale, 2021), have had sizable implications in various domains, powering\napplications such as human-sounding chatbots and language translation systems. Their\nsuccess lies in their impressive language generation capabilities and public accessibility,\nwhich has permeated research in various fields from medicine (Singhal et al., 2023) to\npolitics (Motoki et al., 2023).\nLLM cognition offers promising opportunities to gain previously inaccessible insights\ninto cognition as a whole, and perhaps even human cognition (Kalla & Smith, 2023;\nVan Dis et al., 2023). Additionally, these models’ capabilities in text analysis and\ngeneration could possibly be harnessed by researchers as an easy-to-use method for\nanalyzing textual data, such as coding texts for mental health assessment (Kjell et al.,\n2023). There has recently been a rapid string of psychological research output related to\nand facilitated by these models (Bail, 2023; Floridi, 2023; Park et al., 2023; Van Dis et al.,\n2023). In particular, ChatGPT has been employed in a range of social and behavioral\napplications, from hate-speech classification (Huang et al., 2023) to sentiment analysis\n(Zhu et al., 2023), often with promising results.\nHowever, drawing parallels with the turbulent integration of previous technological\ninnovations into psychological research suggests that the hurried or negligent\nimplementation of LLMs in psychology could give rise to unintended consequences. For\nexample, when fMRI techniques first appeared, some researchers began haphazardly\napplying these techniques, resulting in many nonsensical, but statistically significant neural\nPERILS AND OPPORTUNITIES OF LLMS 5\ncorrelates: a phenomenon cleverly illustrated in an fMRI of post-mortem Atlantic Salmon\n(Bennett et al., 2009) and in a study on “Voodoo Correlations” (i.e., a cautionary term\nabout mysteriously high correlations due to misapplication of statistical methods yielding\nmisleading results; Fiedler, 2011). These two episodes now serve as a lesson for psychology\nstudents and researchers more broadly: that new technologies should be cautiously\nintegrated into psychological research. Specifically, integrating LLMs in the psychological\nresearch pipeline, such as in substitution for human participants, necessitates critically\nexamining its limitations. Researchers should not simply ask themselveshow they can use\na new technology; they need to also ask themselveswhether and why they should do so\n(Crockett & Messeri, 2023).\nBy no means do we uniformly cast doubt on the usefulness of LLMs in psychological\nresearch. Instead, we aim to contextualize current and future opportunities that LLMs may\noffer psychological research, and suggest possible ways to navigate their limitations. While\nacknowledging their potential utility to improve psychological science, we advise caution\nregarding the unchecked application of LLMs, at least in their current state, in\npsychological studies. To prevent issues like the Voodoo Correlations, it is essential to\napproach LLMs with caution, keeping in mind similar challenges the field has faced in\nrecent decades (e.g., the credibility revolution; Vazire, 2018). The following section\nprovides an overview of the downsides of the hurried use of LLMs in psychology, and how it\ncould negatively affect psychological findings if not applied critically and cautiously.\nLLMs Should Not Replace Human Participants\nMany studies of state-of-the-art LLMs have concluded that their outputs are highly\n“human-like” (Bang et al., 2023; H. Liu et al., 2023; Webb et al., 2022). For instance,\nWebb et al. (2022) examined the analogical reasoning abilities of ChatGPT and found that\nit had developed an emergent capacity for zero-shot reasoning, allowing it to solve a wide\nrange of analogy problems without explicit training. Some have argued that if LLMs, such\nPERILS AND OPPORTUNITIES OF LLMS 6\nas ChatGPT, can indeed produce human-like responses to common measures in\npsychological science (e.g., judgments of actions, endorsements of values, perceptions of\nsocial issues), they might as well replace the human subject pool. For example, based on a\nsubstantive correlation between moral judgments made by humans and a language model\n(GPT-3.5), Dillion et al. (2023) argue that these models could replace human participants\nin psychological research. With the “AI as human participants” position (which is typically\nsupported by showing an LLM-human correlation in some psychological domain; e.g.,\nDillion et al., 2023; Park et al., 2023), there is much that we find misleading, perhaps even\ndamaging to developing a generalizable and inclusive science of the human mind.\nFirst, research such as Dillion et al. (2023) does not pay nearly enough attention to\nthe substantial cross-cultural variation in cognitive processes—including moral\njudgments—around the globe (see Atari, Haidt, et al., 2023a; Barrett et al., 2016). Models\nlike GPT have been trained chiefly on WEIRD (Western, Educated, Industrialized, Rich,\nDemocratic; Henrich et al., 2010) people’s textual data primarily in English, perpetuating\nthe English-centricity of psychology (Blasi et al., 2022), hindering efforts to take linguistic\ndiversity seriously. Hence, these models may struggle with accurately representing diverse\npopulations. For example, ChatGPT has shown gender biases favoring male perspectives\nand narratives (Ghosh & Caliskan, 2023; Wong & Kim, 2023), cultural biases toward\nAmerican perspectives (Cao et al., 2023) or majority populations in general (Chen et al.,\n2022), and political biases favoring liberal, environmental, and left-libertarian viewpoints\n(Hartmann et al., 2023; Motoki et al., 2023; Santurkar et al., 2023). These biases also\nextend to personality, morality, and stereotypes (Bozkurt et al., 2023; Deshpande et al.,\n2023; Rutinowski et al., 2023).\nGenerally, these models’ outputs reflect a WEIRD psychology such that the\nAI-human link substantially weakens as we collect “human” data from less WEIRD\npopulations. In other words, the high AI-human correlation does not replicate when the\nhuman sample is less WEIRD (Atari, Xue, et al., 2023). This is especially concerning\nPERILS AND OPPORTUNITIES OF LLMS 7\nbecause the WEIRD-people problem was originally devised as an awareness-raising\nrhetorical device to push researchers away from heavily relying on WEIRD human research\nparticipants (e.g., undergraduate students in North America; see Thalmayer et al., 2021).\nSubstituting human participants with LLM outputs would, therefore, be a step backward.\nIgnoring human diversity in psychology, amplified by the convenience of online samples like\nMTurk, has already led psychology research to be tunnel-visioned toward an extremely thin\nslice of human diversity. GPTology will constitute a move toward an even more myopic and\nless generalizable discipline.\nSecond, LLMs seem to have a “correct answer” bias (Park et al., 2023; Santurkar\net al., 2023). Specifically, LLMs fail to produce much variance in their answers to\npsychology survey questions: even if these questions pertain to topics—like moral\njudgment—for which there is no actual correct answer, and for which human answers\nwould have diversity-of-thought. Simulating human diversity-of-thought using LLMs might\ngenerally be non-trivial. For example, simply prompting an LLM to respond to a question\nmultiple times and then measuring the response variance, a common strategy in the social\nsciences (Almeida et al., 2023; Atari, Xue, et al., 2023; Park et al., 2023), does not equate\nto meaningful variance that can be compared with humans. Generative LLMs, such as\nChatGPT, compute a probability distribution over possible next words to produce an\noutput sequence in an autoregressive manner (Radford et al., 2018; Vaswani et al., 2017).\nSpecifically, the probability of a word being predicted next is given asP(zi) = ezi/T\n∑n\nk=1 ezk/T ,\nwhere zi represents the logit value associated with a potential output wordwi given an\ninput sequence{w1, ..., wi−1}, which is, i.e., the model’s internal representation of the word\nafter processing it (Park et al., 2023). The “temperature” parameter (T) expresses whether\nthe model chooses the most likely word according to the distribution (T →0), expressing\ndeterministic behavior, or samples the words according to the distribution (T →1),\nsupposedly expressing more creative non-deterministic behavior. However, the output\nvariance when repeating the same prompt using a temperature of 1 then simply reflects the\nPERILS AND OPPORTUNITIES OF LLMS 8\noutput probability over the response options and therefore how sure the model is about its\nresponse, which itself is affected by the correct answer bias observed by Park et al. (2023)\nand others.\nConceptually, this is akin to repeatedly asking a question to the same participant\ninstead of different participants. However, psychologists are usually interested in studying\nvariance across participants to make inferences about behavioral patterns and the\nrobustness of psychological phenomena. Thus, researchers using LLMs to study human\nbehavior need to move beyond methods that simulate single responses –such as merely\npredicting group averages or simulating an individual’s response across various tasks (such\nas in Bang et al., 2023; Dillion et al., 2023; H. Liu et al., 2023)– and instead develop robust\nmethods to emulate the complexity of human samples. Furthermore, LLMs are trained on\nvast amounts of data, which can contain many of the items and tasks used in psychological\nexperiments, thus leading the model to rely on its memory instead of making inferences,\nexacerbating the problem above. To get unbiased evaluations of human-like LLM behavior,\nresearchers need to make sure that their tasks are not part of the model’s training data\n(Frank, 2023) or adjust the models to not affect the outcome of their experiments, such as\nby “unlearning” data from an LLM (Eldan & Russinovich, 2023).\nTo showcase how these issues concretely manifest, we analyze LLM responses in the\ndomain of human morality, highlighting how simple prompting strategies fail to capture\nhuman response patterns and particularly human variance. We designed a prompt and\ngave the Moral Foundations Questionnaire-2 (MFQ-2; Atari, Haidt, et al., 2023a) to\nGPT-3.5 1,000 times. The MFQ-2 is a new measure published in 2023, and its items are\nunlikely to exist verbatim in the corpora on which GPT was trained; hence, it does not\nhave the problem of extensively appearing in the training corpora. We then compared\nGPT’s responses to the distributions of a culturally diverse sample of humans from 19\npopulations. As can be seen in Figure 1A, GPT produces substantially smaller variance in\nsix moral domains (i.e., care, equality, proportionality, loyalty, authority, and purity)\nPERILS AND OPPORTUNITIES OF LLMS 9\ncompared with actual human populations (allps < .001). GPT-3.5’s variance was 43–121\ntimes smaller than human data in different moral domains, even when using parameter\nsettings for maximum variability in generated responses. In contrast to the argument by\nDillion et al. (2023) that “silicon sampling allows researchers to simulate a diverse\npopulation of participants,” our results, along with emerging work from others (e.g., Atari,\nOmrani, & Dehghani, 2023a; Park et al., 2023), show that the so-called “silicon sampling”\nfails in mimicking a diverse population of humans: a finding which is consistent with that\nof Atari, Xue, et al. (2023).\nTo show that this phenomenon is robust across domains, we extended this analysis\nof GPT versus human responses to a broad range of self-report measures, with participants\nfrom over 43 countries and spanning various psychological domains, such as personality,\ncognition, political orientation, and emotions: Big Five Inventory (John & Srivastava,\n1999): N = 3, 924, Need for Closure (Webster & Kruglanski, 1994a):N = 315, Need for\nCognition (Cacioppo & Petty, 1982a):N = 900, Right-wing authoritarianism Scale\n(Zakrisson, 2005a): N = 1, 020, Emphasizing-Systemizing Scale (Baron-Cohen et al.,\n2003a): N = 3, 141, Rational-Experiential Inventory Scale (Pacini & Epstein, 1999a):\nN = 1, 456. Across these psychological constructs, we consistently found that ChatGPT\nresponses generally showed significantly less variance across all measures (see Table S18 for\nan overview of variance differences across all surveys) and differed significantly between\nvarious demographics. For example, when responding to personality surveys, ChatGPT\nwas significantly more agreeable than politically liberal individuals (d = −0.230, p < .001,\n95% CI [-0.369, -0.091]), conservatives (d = −0.403, p < .001, 95% CI [-0.550, -0.257]), and\nmoderates (d = 0.285, p < .001, 95% CI [-0.424, -0.145]) as shown in Figure 2. Beyond\npersonality dimensions, ChatGPT, for example, also endorsed significantly less right-wing\nauthoritarianism than male participants (d = 0.44, p < .001, 95%CI [0.24, 0.65]), White\nparticipants (d = 0.35, p < .001, 95%CI [0.16, 0.54]), and younger participants (18-24;\nd = 0.49, p < .001, 95%CI [0.24, 0.74]), but significantly more than explicitly liberal\nPERILS AND OPPORTUNITIES OF LLMS 10\nFigure 1\nChatGPT vs. Human moral judgments\nNote. A) Distributions of moral judgments of humans (light blue) and GPT (light red) in six moral\ndomains. Dashed lines represent averages.B) Inter-correlations between moral values in humans (N =\n3,902) and ChatGPT queries (N = 1,000).C) Network of partial correlations between moral values based\non a diverse sample of humans from 19 nations (Atari et al., 2023) and 1,000 queries of GPT. Blue edges\nrepresent positive partial correlations and red edges represent negative partial correlations.\nPERILS AND OPPORTUNITIES OF LLMS 11\nparticipants (d = −0.23, p = .003, 95%CI [-0.39, -0.07]) as shown in Figure 3. Differences\nof this kind and strength were observed across all surveys and demographic groups. See the\nSupplementary Materials for a detailed summary of all demographic differences, across a\nvariety of psychological constructs.\nFigure 2\nComparing ChatGPT against Humans grouped by political opinion for responses on the Big Five\npersonality questionnaire\n1\n2\n3\n4\n5\nExtraversion Agreeableness Conscientiousness Neuroticism Openness\nConstruct Score\nGPT\nConservative\nLiberal\nModerate\nNote. Figure shows the response distribution of humans and ChatGPT across the five-factor personality\nconstructs and for different human demographics. Figure shows that ChatGPT gives significantly higher\nresponses on Agreeableness, Conscientiousness and significantly lower responses on Openness and\nNeuroticism. Importantly, ChatGPT shows significantly less variance compared with all demographic groups\non all personality constructs.\nThird, we are skeptical that GPT responses can make nomological networks in\nwell-established theoretical frameworks. Dillion et al. (2023) mention that “researchers can\ngive LLMs different questions and see if they act as expected within a nomological net\n(e.g., form a reliable scale).” To demonstrate how GPT constructs a moral psychological\nnomological network, we looked at inter-correlations between moral domains (Figure 1B)\nand the network of moral domains based on partial correlations (Figure 1C) in a diverse\nPERILS AND OPPORTUNITIES OF LLMS 12\nFigure 3\nComparing ChatGPT against Humans across various demographic variables for the\nRight-Wing-Authoritarianism scale\n0\n2\n4\n6\nRWA\nConstruct Score\nGPT\n18−24\nLiberal\nMale\nWhite\nNote. Figure shows the response distribution of humans and ChatGPT on the RWA scale for different\nhuman demographics. ChatGPT shows significantly lower average RWA than male, white, and young\nparticipants but not explicitly liberal participants. Importantly, ChatGPT shows significantly less variance\ncompared with all demographic groups.\nhuman sample (N = 3,902; Atari, Haidt, et al., 2023a) and 1,000 GPT queries. As shown\nin Figures 1B and 1C, the networks of moral values are substantially different. Therefore,\nin contrast to the claim by Dillion et al. (2023), GPT seems to fail in producing previously\nestablished nomological networks.\nIn sum, probing LLMs can be a fruitful direction for future research (Grossmann\net al., 2023; Shiffrin & Mitchell, 2023). However, we caution against the hasty replacement\nof human participants with LLMs, because (1) synthetic AI-simulated sampling would\nignore psychological and linguistic diversity that the field desperately needs in order to\ninvestigate beyond WEIRD psychology (e.g., Apicella et al., 2020; Blasi et al., 2022;\nHenrich et al., 2010); (2) LLMs often fail to show meaningful variance (or diversity) in\ntheir judgments; and (3) the outputs of current LLMs do not seem to replicate previously\nPERILS AND OPPORTUNITIES OF LLMS 13\nestablished nomological networks.\nOverall, probing the psychology of LLMs is scientifically meaningful and practically\nimportant. However, it should not replace the scientific study ofHomo sapiensbut rather\nsupplement it. Of course, these limitations do not imply that all uses of LLMs should be\navoided in psychological research. While Dillion et al. (2023) have portrayed AI as an\n“agent of replacement,” others have argued that AI should instead be an agent of\n“assistance,” “improvement,” or “augmentation” (e.g., Crockett & Messeri, 2023). Framed\nin the latter way, LLMs can be thought of as tools to help improve the research process in\npsychology. It is indeed exciting to try different psychological measures (traditionally\ndeveloped for humans) on AI tools, but such examinations should be cautious and\nexploratory, and their results should not be hyperbolized. Such careful considerations may\nlead to the development of more robust procedures to implement LLMs in psychology,\nsimilar to how psychologists have addressed issues with other methods. For example, when\nresearchers decide to use LLM outputs as a proxy for a human sample, they should take\ninto account task-relevant biases of the models in question (e.g., check for implicit biases\nagainst agents involved in a moral scenario when investigating moral judgments). This can\nbe done in conjunction with efforts to reduce LLM biases (Omrani et al., 2023).\nLLMs Are Not An All-Purpose Method\nIn this section, we discuss the perception of Large Language Models (LLMs) as a\nuniversal off-the-shelf text analysis tool in psychology. We begin by explaining key\ndistinctions in how LLMs can be applied for text analysis. This is followed by an\nexploration of the currently common practice of overly focusing on zero-shot capabilities of\nLLMs and an examination of their potential drawbacks. We then contrast LLMs with\nnon-LLM-based, top-down, theory-based text analysis tools, discussing the advantages of\nthese smaller, interpretable methods and how they can benefit researchers. We highlight\nthat researchers must carefully consider the particular demands and subtleties of their\nPERILS AND OPPORTUNITIES OF LLMS 14\nstudy topics when selecting NLP methods for text analysis in psychology. This might\ninclude leveraging LLM-based approaches in scenarios where large-scale, general-purpose\nlanguage modeling is necessary. Conversely, in situations requiring theory-driven\nexplorations, top-down methods such as thematic content analysis or dictionary-based\ntechniques (e.g., LIWC) may be more suitable. By thoughtfully combining these methods\nbased on the study’s needs, researchers can achieve a more nuanced and comprehensive\nunderstanding of their textual data.\nNLP methods using pre-trained Language Models generally fall into two categories:\nthose involving parameter updates (e.g., gradient-based) and those that do not.\nFine-tuning is a primary method involving parameter updates, where the pretrained LLM\nis further trained on a task-specific dataset. This can range from updating a subset of\nparameters to adjusting the entire model, depending on task complexity and data\navailability (Devlin et al., 2018a; Houlsby et al., 2019; E. J. Hu et al., 2021; Ruder et al.,\n2019) In contrast, zero-shot, one-shot, and few-shot learning, can be used without gradient\nupdates by leveraging a pretrained model’s ability to generalize from limited or no\ntask-specific data (i.e., examples), using the model’s pre-existing knowledge and\nunderstanding (Brown et al., 2020; Radford et al., 2019; Xian et al., 2018).\nRecently, there has been a surge of studies using LLMs zero-shot capabilities, that is\ntheir capability to perform tasks without any prior training on that specific task (Brown\net al., 2020; Kojima et al., 2022; P. Liu et al., 2023; Romera-Paredes & Torr, 2015), for\npsychological text analysis, presumably due to their ease of use and accessibility. For\nexample, Markowitz (2023), Rathje et al. (2023), and Zhu et al. (2023) reported high\nperformance of ChatGPT as an automated text analysis tool, such as for sentiment\nanalysis, offensive language, thinking style, or emotion detection. Rathje et al. (2023)\nfurther concluded that LLMs constitute a viable all-purpose method for psychological text\nanalysis, arguably more convenient than small(er) language models and traditional\ntechniques in NLP, due to their ability to handle diverse tasks within a single model\nPERILS AND OPPORTUNITIES OF LLMS 15\nwithout needing task-specific adjustments, and their user-friendly design that minimizes\nthe need for complex coding, making them more accessible to psychologists and potentially\nencouraging broader research engagement. However, this perspective of LLMs, as a\nconvenient and comprehensive tool in psychological text analysis, is challenged by recent\ncritiques emphasizing their limitations, such as inconsistencies in text annotations,\ndifficulties in explaining complex constructs such as implicit hate speech, and a potential\nlack of depth in specialized or sensitive areas (Huang et al., 2023; Kocoń et al., 2023; Reiss,\n2023). Furthermore, the difference in convenience between zero-shot applications of LLMs\nand fine-tuning of models may not be as stark as commonly perceived. Fine-tuned small(er)\nlanguage models for various tasks are now increasingly publicly available. Similarly, there\nare more and more high-quality and specialized datasets available to researchers for\nfine-tuning language models themselves. Examples of such corpora span across fields, such\nas morality, personality, or sentiment analysis (Alabi et al., 2022; Çöltekin, 2020; Golbeck,\n2016; Hoover et al., 2020; Rahman & Halim, 2022; Trager et al., 2022a).\nNotably, even smaller fine-tuned models can in many cases, where data for\nfine-tuning is available, fare equally well or even better than zero-shot applications of\nLLMs. For instance, in most reported cases in Rathje et al. (2023), small fine-tuned\nlanguage models, including older models such as base-BERT (Bidirectional Encoder\nRepresentations from Transformers; Devlin et al., 2018a), outperform even the newest\ngeneration of ChatGPT (GPT-4) on a variety of text annotation tasks. Notably, in Rathje\net al. (2023) GPT-4 (zero-shot) was outperformed on English language sentiment analysis\nby a fine-tuned model developed before the breakthrough Transformers architecture\n(Cliche, 2017). Additionally, these smaller language models have since then further\nundergone significant improvement, such as Sentence-BERT (Reimers & Gurevych, 2019),\nRoBERTa (Y. Liu et al., 2019), and their subsequent iterations, highlighting additional\npotential performance advantages. While zero-shot LLMs might offer immediate\naccessibility, the most expedient choice is often not the most effective. Thus, researchers\nPERILS AND OPPORTUNITIES OF LLMS 16\nshould be cautious of leaning too heavily on the allure of convenience. We explicitly\nhighlight here that improved methods do not have to exclude LLMs. There are various\nalternative ways to utilize LLMs, such as by fine-tuning the LLM on the respective task, or\nvia few-shot prompting which includes few examples of solving a task in a prompt so that\nthe LLM can generalize to the given examples (Brown et al., 2020; P. Liu et al., 2023; Snell\net al., 2017).\nBuilding on this perspective, we extend these recent studies by investigating\nChatGPT’s ability to annotate moral language across three distinct settings: zero-shot,\nfew-shot, and fine-tuned. Human morality has been argued to be a particularly difficult\nconcept for language models, making it an interesting test case for comparing powerful\nLLMs against less complex smaller language models (Russell, 2019), as well as testing the\nefficacy of various LLM application strategies. Specifically, we gave ChatGPT 2,983 social\nmedia posts that contained moral or non-moral language and prompted it to determine if\nany and what specific type of moral language was used. We then compared it to a small\nBERT model that was fine-tuned on a separate subset of social media posts (see\nSupplementary Materials for details). To supplement our comparison of smaller vs larger\nand complex vs less complex models, we repeated this analysis using the Linguistic Inquiry\nand Word Count (LIWC; Pennebaker et al. (2001)), which is a simple and commonly used\ndictionary-based text-analysis method producing psychologically validated, and\nimportantly interpretable features, from a given text. We compared the annotations of all\nmodels against the “ground truth” of human raters.\nWe find that fine-tuned BERT vastly outperformed ChatGPT applied in a zero-shot\nsetting, achieving an F1 score of 0.48 vs. ChatGPT’s 0.22. Additionally, ChatGPT was\nmore extreme in over- or under-predicting a moral sentiment, while BERT did in all but\none case not significantly deviate from trained human annotators. Notably, even the\nLIWC-based approach outperformed ChatGPT (zero-shot) with an F1 score of 0.27 and\nwas significantly less likely and less extreme in deviating from trained human annotators,\nPERILS AND OPPORTUNITIES OF LLMS 17\ndespite being a magnitudes smaller, less complex, and cheaper model. As anticipated,\nutilizing ChatGPT using few-shot learning or fine-tuning, increased performance.\nSpecifically, ChatGPT applied in a few-shot setting achieved an F1 score of 0.32 and\nfine-tuning ChatGPT on the same data as the BERT model achieved an F1 score of 0.53.\nHowever, in both cases, ChatGPT was still more extreme in over- or under-predicting a\nmoral sentiment compared with BERT. See Table S1 in the Supplementary Materials for\nan overview of our results. It should be noted here that while fine-tuning ChatGPT or\napplying a few-shot paradigm can yield better results, such processes are significantly more\nresource-intensive and expensive compared with LIWC or even BERT.\nAdditionally, ChatGPT and other closed-source models require (if possible at all)\nfine-tuning on a corporate platform which introduces constraints compared with models\nlike BERT which provide full control about the fine-tuning process and can be trained\noffline without additional costs (see the following section for a detailed discussion of issues\nwith closed-source and proprietary vs open-source models). Few-shot learning, although\npromising, hinges on the relevance and quality of examples, potentially making it less\nstable than fine-tuning (Reynolds & McDonell, 2021). As such, researchers should choose a\nmodel and how to apply it based on the availability of task-specific data, available\ncomputing resources, and research-specific considerations (e.g., interpretability, control over\nthe model). Fine-tuning is preferable if training data is available. Smaller models (e.g.,\nBERT) are then preferable to larger models (e.g., ChatGPT) if computing resources (or\nfinancial for commercial services) are limited. In scenarios where gathering extensive\ntask-specific data is impractical or impossible, LLM’s zero-shot or few-shot capabilities are\nadvantageous.\nTo further highlight the limitations of zero-shot LLM applications, we examined to\nwhat extent ChatGPT’s zero-shot annotations were biased toward specific demographics.\nWe did so by using human annotator demographics and psychometrics provided for our\ntest data (see Supplementary Materials for a detailed description of the experimental\nPERILS AND OPPORTUNITIES OF LLMS 18\nprocedures). We found that ChatGPT is less likely to agree with conservative\n(∆log(odds) = −4%, p < .001) or collectivist (∆log(odds) = −30%, p < .001)\nannotators and more likely to agree with individualistic (∆log(odds) = +142% ,\np < .001) annotators. Additionally, it is biased toward younger (∆log(odds) = −1%\nyear ,\np = 0 .033), more open-minded (∆log(odds) = +1244% , p < .001), and agreeable\n(∆log(odds) = +156%, p < .001) annotators. Interestingly in terms of the six moral\nfoundations (Atari, Haidt, et al., 2023a), it is biased toward annotators who have a lower\npreference for equality (∆log(odds) = −97%, p < .001) and loyalty\n(∆log(odds) = −96%, p < .001) and those who endorse more care\n(∆log(odds) = +31 , 159%, p < .001) and proportionality (∆log(odds) = +21 , 215%,\np < .001) values. This aligns with our additional findings of ChatGPT over-weighing the\ncare foundation when responding to the MFQ-2 (see additional analyses in Supplementary\nMaterials). Potentially, this could be linked to post-hoc measures by OpenAI to avoid an\nAI that endorses harm or does not care about people’s emotional well-being. Agreement\nwith proportionality and equality endorsing annotators aligns with past findings that\nChatGPT is (left-)libertarian-leaning (Hartmann et al., 2023; Motoki et al., 2023;\nSanturkar et al., 2023). See Tables S2 - S8 in the Supplementary Materials for a summary\nof annotation biases.\nFinally, we analyzed how LLMs, at least at this stage of their development, fare\nagainst theory-based, top-down constrained methods. This is relevant because LLMs are\nprimarily considered language analysis tools due to their broad capabilities stemming from\ntheir vast parameter sets, training data, and training procedures. However, this flexibility\nand performance come at the price of reduced interpretability and reproducibility (i.e., the\ntrade-off between a higher-performing black box and a lower-performing interpretable\nmethod). An often-stated reason for why some researchers of psychological text analysis\nprefer neural-network-based models over simple, theory-driven methods is the purportedly\nsuperior predictive power of the former. If LLMs cannot outperform top-down methods in\nPERILS AND OPPORTUNITIES OF LLMS 19\nzero or few-shot settings, this would be another reason for psychologists (and other social\nscientists) to consider using top-down models rather than LLMs (at least the existing\nones), because in social sciences, theory and interpretability are of prime concern\n(Muthukrishna & Henrich, 2019).\nWe compared ChatGPT against Contextualized Construct Representation (CCR;\nAtari, Omrani, & Dehghani, 2023a), which is a method that combines psychometric scales\nwith small language models (e.g., s-BERT; Reimers & Gurevych, 2019) to extract\npsychological information from texts. By relying on validated psychometric scales, CCR\nplaces strong, theory-based, top-down constraints on its underlying small language model,\nwhich allows for high interpretability and easy application. Comparing ChatGPT and\nCCR, we found that ChatGPT (zero-shot) fails to outperform CCR in inferring\npsychological outcomes from human-written essays. CCR substantially outperformed\nChatGPT ratings when prompted to infer psychological constructs directly (Dunnett’s\nTest;d = −2.25, p = 0.005, 95%CI [-3.88, -0.62]), that is predicting the construct scores\ndirectly. See Figures S33 and S34 for an overview of model performances across all\nself-report measures (i.e., cultural orientation, personal values, moral judgments, political\nideology, need for cognition, and norm violations). CCR and ChatGPT performed on par\nwhen inferring psychological variables from essays about everyday life (Figure S34) when\nprompted on the item-level, that is, when predicting each scale item separately and then\ncalculating the construct score (Dunnett’s Test;d = 0.21, p = 0.9404, 95%CI [-1.42, 1.84]).\nTaken together, past work and our present findings consistently demonstrate that\nfor many use cases, smaller (fine-tuned) models can be more powerful and less biased than\nthe current large (generative) language models, particularly in zero-shot and few-shot\nsettings. For example, consider a study examining the language used in online support fora\nfor individuals with anxiety disorders. A researcher using a smaller, specialized language\nmodel may be able to uncover subtle nuances and specific patterns of language that are\ndirectly relevant to the domain of interest (e.g., worry, intolerance of uncertainty). This\nPERILS AND OPPORTUNITIES OF LLMS 20\ntargeted approach can yield more profound insights into the experiences of individuals with\nanxiety, shedding light on their unique challenges and potential interventions. By\nleveraging specialized language models or top-down methods such as CCR, or LIWC,\nresearchers can strike a balance between comprehensiveness and granularity, enabling a\nmore nuanced exploration of textual data.\nCrucially, we do not discourage the use of LLMs as a text-analysis tool in all\nrespects. Particularly in cases where data for fine-tuning is scarce, such as with new\nconstructs or understudied populations, LLMs’ zero-shot capabilities may still offer\nacceptable performances and allow researchers to investigate urgent research questions.\nMethods such as few-shot prompting can be effective and efficient in these cases as they\nonly demand a handful of representative examples. Across our experiments, we found that\nLLMs can achieve high performances but stress that this cannot always be achieved using\nthe model’s zero-shot capabilities and instead requires exploring techniques, such as\nfew-shot prompting or outright fine-tuning of the LLM (see also work on optimizing\nprompt design, such as “Chain-of-Thought” (Kojima et al., 2022; Wei et al., 2022), a recent\ntechnique for eliciting complex multi-step reasoning). Similarly, our additional analyses,\nshowing increased performance for ChatGPT when predicting at the item level instead of\nthe construct level, highlight that LLMs can benefit from integrating theory-driven\napproaches. Extending this line of work, developing methods that can combine the benefits\nof both approaches is a promising endeavor for future work. With the constant and rapid\ndevelopment of LLMs that address performance and bias issues, these concerns could\nalready be mitigated in the near future.\nWe emphasize the importance of researchers constantly assessing these limitations\nand opportunities while exercising caution when defaulting to the most convenient choice.\nAs with all empirical methods, LLM-based methods need to be validated and\nbenchmarked. We recommend benchmarking LLM-based findings against more established\ntext-analytic methods to make LLMs more useful for psychological inference. For example,\nPERILS AND OPPORTUNITIES OF LLMS 21\nthe gold-standard measure for text annotation is human annotation, and LLM-based\nannotations should be comprehensively validated—on a task-by-task basis—against a small\nlabeled corpus before using these models at scale (Pangakis et al., 2023). This approach\nhelps establish the validity of the analysis by reducing the potential biases that may arise\nfrom relying solely on automated techniques. Additionally, validation allows researchers to\nassess the inter-rater reliability of the annotations, providing a measure of the robustness of\nthe analysis. The human element in validation brings valuable insights, subjective\njudgments, and contextual understanding that may be challenging for LLMs to capture\naccurately.\nReproducibility Matters\nReproducibility pertains to replicating and verifying results using the same data and\nmethods (Nosek et al., 2022). However, particular challenges arise when applying these\nprinciples to LLMs, particularly proprietary ones. Their black-box nature impedes the\nreproducibility of findings that pertain to them. This limitation poses a significant obstacle\nto achieving reproducibility in studies that rely on LLM-generated data or analyses.\nAdditionally, their biases can also change over time for LLMs that undergo updates\nover time. Each time modifications are made to ChatGPT’s algorithms for performance\nenhancement, the nature and scope of biases embedded in the model may change. This\ncould impact the effectiveness of previously established “best practices” and debiasing\nstrategies (Bail, 2023; Spirling, 2023). Currently, ChatGPT, and to our best knowledge any\nother closed-source models, does not freely provide past versions that allow researchers to\nuse the model from specific points in time (e.g., “gpt3.5-January-2023”) to reproduce\nresearch results. It only provides a snapshot after each major update, which is then\ndeprecated within three months to one year. This means that even when updates address\nand mitigate detected biases, they also introduce the potential for “process reproducibility\nfailure” (Nosek et al., 2022) in the generated data and impede reproducibility, critical for\nPERILS AND OPPORTUNITIES OF LLMS 22\nscientific rigor (Bail, 2023; Spirling, 2023; Zhai, 2023). This also affects currently emerging\npractices for reproducibility, such as researchers sharing prompts and Application\nProgramming Interface (API) parameters because the effect of these parameters (or the\nparameters themselves) and the outputs generated from these prompts can change over\ntime. Importantly, new iterations do not guarantee equal or better performance across all\ntasks. For example, Rathje et al. (2023) report inconsistent results between GPT-3.5 and\nGPT-4 performance on various text-analysis tasks—for example, GPT-4 sometimes\nperforms more poorly than GPT-3.5—which lends support to our concern that\nnontransparent changes to the model can cause unforeseen challenges. Additionally,\ncurrent state-of-the-art LLMs come with additional opaque fine-tuning, such as\nreinforcement learning using human feedback (RLHF; Arumugam et al., 2019; Ziegler\net al., 2019), for which usually neither the training method nor the training data are\nknown. Therefore, researchers should carefully consider the trade-off between forgoing\ncontrol over these procedures (Spirling, 2023) and fine-tuning models themselves.\nResearchers should be aware of the current black-box nature of LLMs (Bail, 2023;\nSpirling, 2023). Not only from an open-science standpoint, but more generally, researchers\nshould be interested in access to high-quality, informative semantic representations and the\nalgorithms used to generate outputs, instead of only the outputs. One of the main\nadvantages of computational models is that they allow us to ‘look under the hood’ and\nthus make inferences about psychological processes which may be difficult to test\notherwise. Thus, using proprietary LLMs may constitute a missed opportunity for\ntheory-based work in psychology that aims to leverage innovations in computer science.\nNotably, the lack of transparency in how ChatGPT generates responses implies that\nresearchers cannot ascertain the underlying mechanisms and origin of biases that may be\ninfluencing the outputs (X. Hu et al., 2023; Pournaras, 2023; Zhai, 2023). Such lack of\ntransparency is antithetical to the scientific principles of openness and replicability that\nshould be central to computational-psychological research (Hofman et al., 2021).\nPERILS AND OPPORTUNITIES OF LLMS 23\nComputational-psychological research that aims to achieve replicable findings with\nLLMs can use models that have a publicly available open architecture, such as BigScience\nLarge Open-science Open-access Multilingual Language Model (BLOOM; Scao et al., 2022)\nand Large Language Model Meta AI (LLaMA; Touvron et al., 2023). These open-source\nmodels provide researchers with access to network architecture, including datasets and\nparameters such as pre-trained weights (Binz & Schulz, 2023). The less black-box nature of\nthese open-source models can help researchers make the exact version used in their works\navailable to others (Spirling, 2023), thus facilitating transparency and reproducibility. In\nthe Supplementary Materials, we detail how a complete research pipeline using a\nLLaMA-based model can be conducted and show that the results are comparable to a\npipeline using a closed-source LLM. Despite the model’s significantly smaller size (7 billion\nvs. 175 billion parameters) and while being self-hosted on consumer-grade hardware, it\nachieved a comparable performance in a text annotation task discussed above (F1 of 0.23\nvs. 0.22) and a similar response pattern when answering self-report psychological measures.\nCompared with closed-source LLMs, open-source LLMs may thus come with a replicability\nbenefit and increased control over the model when it comes to computational-psychological\nresearch. Additionally, certain approaches aimed to allow for more robust inferences\nregarding LLM outputs, such as “unlearning” (Eldan & Russinovich, 2023) data and tasks\nused for evaluating LLM capabilities from the training data, require an open model\narchitecture (e.g., the ability to access and manipulate the probability distribution over the\noutput tokens).\nHowever, the primary focus should not be a debate of closed-source vs. open-source\nLLMs, but instead addressing the specific problems for scientific research using either\napproach. Closed-source models, such as ChatGPT, may introduce ways to save and reload\nor share model weights and modify their models. For example, OpenAI continuously adds\n(but also deprecates) available model parameters. As of writing, increased reproducibility\nhas been announced as in beta development, and a “seed” parameter increasing\nPERILS AND OPPORTUNITIES OF LLMS 24\ndeterministic behavior as well as a “system_fingerprint” value to help track model changes\nwas recently added. However, the “seed” does not guarantee reproducibility, and while the\n“system_fingerprint” increases transparency, allowing researchers to identify potential\nreasons for failure to replicate, it does not increase reproducibility itself. Moreover, some\nproblems that might be more pronounced with proprietary models, such as economic\nmotivations, can still affect open-source models (e.g., LLaMA being developed by Meta, a\nfor-profit company). Additionally, openness is a spectrum requiring researchers to closely\nmonitor to what extent LLMs fulfill relevant openness criteria for their projects. See\nLiesenfeld et al. (2023) discussing of LLM openness in detail as well as their openness\nratings of current LLMs https://opening-up-chatgpt.github.io, including their specific\nopenness criteria and evaluation strategies https://github.com/opening-up-chatgpt/\nopening-up-chatgpt.github.io/tree/main/projects#criteria. As such, researchers should\nconsider to what extent each model and the organization behind it conflicts with scientific\nand ethical principles of research (Bonnefon et al., 2020; Hagendorff, 2020; Windsor, 2006).\nWe caution against mistakenly treating the use of open-source rather than closed-source\nLLMs as sufficient to solve the black-box problem: a problem that remains largely\nunsolved, even for open-source LLMs. The overconfidence with which researchers may trust\nopen-source LLMs’ findings as replicable and transparent can systematically cause such\nresearchers to over-rely on these findings, even in out-of-distribution settings where the\nfindings may not generalize.\nThe reproducibility concerns surrounding computational-psychological research\ndone via LLMs are further intensified by their ability to adopt different perspectives\nthrough prompting: explicit instructions to the model on which the output is conditioned\non (Deshpande et al., 2023; Hwang et al., 2023; H. Jiang et al., 2023). Prompting has been\nshown to be a promising technique to increase the range of applications and versatility of\nLLMs (Wang et al., 2023), and is also discussed as a strategy to mitigate some of the biases\nin these models’ outputs (Shaikh et al., 2022; Si et al., 2022). One idea is to condition\nPERILS AND OPPORTUNITIES OF LLMS 25\nLLMs’ outputs on explicit instructions to take the perspective of different groups or\ndemographics. For example, some work has shown that prompting can improve the\nalignment of group-relevant responses (e.g., if prompted to respond like a Democrat,\nChatGPT will answer opinion poll questions in the direction steered). However, at the\nmoment, prompting nonetheless falls short in closing the gap in demographic group biases\n(Santurkar et al., 2023) and tends to reflect overly-simplistic cultural stereotypes instead of\nperspective-taking (Durmus et al., 2023). Moreover, prompting also introduces significant\nchallenges to reproducibility in psychological research, because prompts can be constructed\nin numerous ways. Past work has shown that slight alterations and modifications in\nphrasing, context, or order can lead to substantially different responses (Fujita et al., 2022;\nGan & Mori, 2023; Lu et al., 2021; Markowitz, 2023; Mishra et al., 2023; Park et al., 2023).\nWe extended this literature by directly testing if changes in prompts would affect\nthe results of our previously presented experiments. We repeated the moral sentiment\nanalysis with a modified prompt (see Supplementary Materials) and tested whether this led\nto different classification outputs. Furthermore, we repeated the survey responses collection\nusing several modified prompts (see Supplementary Materials), and tested whether the\nresponses changed. The changes to the prompts were derived from past work that showed\nhow minor changes to the self-report design, such as adding contextual information or\nchanging the response scale, can elicit different response patterns (Schwarz, 1999). Our\nfindings demonstrate, in line with recent works on the effect of prompting (Fujita et al.,\n2022; Gan & Mori, 2023; Lu et al., 2021; Markowitz, 2023; Park et al., 2023), that minor\nchanges in prompts lead to significant differences in outputs. For example, adding a study\nintroduction when prompting ChatGPT to respond to the Big Five Inventory (BFI; John\n& Srivastava, 1999), significantly reduced the scores for “Openness” (d = −0.30; p < .001;\n95%CI [-0.34, -0.26]), Extraversion (d = −0.54; p < .001; 95%CI [-0.58, -0.50]), and\nAgreeableness (d = −0.14; p < .001; 95%CI [-0.18, -0.10]). Similarly, modifying the order\nin which the types of moral foundations are defined in the prompt changed how texts were\nPERILS AND OPPORTUNITIES OF LLMS 26\nclassified. Using the modified prompt, ChatGPT was significantly less likely to annotate\nthe care foundation (-56%,p < .001), the equality foundation (-62%,p < .001), the loyalty\nfoundation (-33%,p = .009), and significantly more likely to annotate the authority\nfoundation (+71%,p < .001), and purity foundation (+164%,p = .020). See the\nSupplementary Materials for extended results and a detailed overview of our procedures.\nOur research emphasizes the importance of considering context and specific prompts\nwhen using ChatGPT to emulate diverse human behaviors (Rao et al., 2023). Continuous\nalgorithmic changes necessitate ongoing validation of prompting strategies, a task made\nchallenging by the complex and opaque nature of LLMs (E. Jiang et al., 2022; P. Liu et al.,\n2023; Mishra et al., 2023). While these challenges do not negate the applicability of LLMs\nin psychological research, they do highlight the need for robust, standardized methods akin\nto those used in traditional human research (Schwarz, 1999; Schwarz et al., 1991). As this\nis a rapidly evolving field, new AI models may account for all of the above concerns of\ndiversity, variation, transparency, accuracy, and robustness. The progress that is expected\nto be made by LLMs in the near future requires that empirical work be completed to\nhighlight their strengths, weaknesses, and promises for further enhancement Markowitz\n(2023).\nConclusion\nGPTology—which we define as the hurried and unjustified application of LLMs\neither as “replacements” for human participants, or as an off-the-shelf “one-size-fits-all”\nmethod in psychological text analysis—can lead to a proliferation of low-quality research,\nespecially if the convenience of using LLMs such as ChatGPT leads researchers to rely too\nheavily on them. A WEIRD bias and the opaque, and often irreproducible nature of these\nmodels make the current LLMs, specifically proprietary ones, a double-edged sword for\npsychological research. This does not mean that LLMs are inadequate to aid psychological\nresearch, but researchers must actively exercise caution and critically evaluate the\nPERILS AND OPPORTUNITIES OF LLMS 27\nlimitations of these models before incorporating them into their research paradigms. In the\npresent work, we empirically quantified some of the biases and limitations of these models\nacross multiple psychological domains including moral judgments, personality traits,\ncultural orientation, and political ideology, among others. Psychological science, which has\nwitnessed multiple adverse consequences when new technologies were haphazardly and\nheedlessly used, needs to strive for diversification of research samples, validation of\ndifferent methods against one another, transparency, and ethical considerations in\ndeploying LLMs to ensure that the findings are robust, generalizable, and free from\ndemographic biases. A commitment to rigor and replicability should guide the integration\nof AI into psychological research, not convenience.\nPERILS AND OPPORTUNITIES OF LLMS 28\nReferences\nAiyappa, R., An, J., Kwak, H., & Ahn, Y.-Y. (2023). Can we trust the evaluation on\nchatgpt? https://arxiv.org/abs/2303.12767\nAlabi, J. O., Adelani, D. I., Mosbach, M., & Klakow, D. (2022). Adapting pre-trained\nlanguage models to african languages via multilingual adaptive fine-tuning.\nProceedings of the 29th International Conference on Computational Linguistics,\n4336–4349.\nAlmeida, G. F., Nunes, J. L., Engelmann, N., Wiegmann, A., & de Araújo, M. (2023).\nExploring the psychology of gpt-4’s moral and legal reasoning.arXiv preprint\narXiv:2308.01264.\nApicella, C., Norenzayan, A., & Henrich, J. (2020). Beyond weird: A review of the last\ndecade and a look ahead to the global laboratory of the future.Evolution and\nHuman Behavior, 41(5), 319–329.\nArumugam, D., Lee, J. K., Saskin, S., & Littman, M. L. (2019). Deep reinforcement\nlearning from policy-dependent human feedback.arXiv preprint arXiv:1902.04257.\nAtari, M., Haidt, J., Graham, J., Koleva, S., Stevens, S. T., & Dehghani, M. (2023).\nMorality beyond the weird: How the nomological network of morality varies across\ncultures. Journal of Personality and Social Psychology.\nAtari, M., Omrani, A., & Dehghani, M. (2023). Contextualized construct representation:\nLeveraging psychometric scales to advance theory-driven text analysis.\nhttps://psyarxiv.com/m93pd\nAtari, M., Xue, M. J., Park, P. S., Blasi, D. E., & Henrich, J. (2023). Which humans?\npsyarxiv.com/5b26t\nAydın, Ö., & Karaarslan, E. (2023). Is chatgpt leading generative ai? what is beyond\nexpectations? What is Beyond Expectations.\nAzaria, A. (2023). Chatgpt: More human-like than computer-like, but not necessarily in a\ngood way.\nPERILS AND OPPORTUNITIES OF LLMS 29\nBail, C. A. (2023). Can generative ai improve social science?\nURL_WHERE_THE_PREPRINT_CAN_BE_FOUND\nBaktash, J. A., & Dawodi, M. (2023). Gpt-4: A review on advancements and opportunities\nin natural language processing. https://arxiv.org/abs/2305.03195\nBang, Y., Cahyawijaya, S., Lee, N., Dai, W., Su, D., Wilie, B., Lovenia, H., Ji, Z., Yu, T.,\nChung, W., et al. (2023). A multitask, multilingual, multimodal evaluation of\nchatgpt on reasoning, hallucination, and interactivity.\nhttps://arxiv.org/abs/2302.04023\nBaron-Cohen, S., Richler, J., Bisarya, D., Gurunathan, N., & Wheelwright, S. (2003). The\nsystemizing quotient: An investigation of adults with asperger syndrome or\nhigh–functioning autism, and normal sex differences.Philosophical Transactions of\nthe Royal Society of London. Series B: Biological Sciences, 358(1430), 361–374.\nBarrett, H. C., Bolyanatz, A., Crittenden, A. N., Fessler, D. M., Fitzpatrick, S.,\nGurven, M., Henrich, J., Kanovsky, M., Kushnick, G., Pisor, A., et al. (2016).\nSmall-scale societies exhibit fundamental variation in the role of intentions in moral\njudgment. Proceedings of the National Academy of Sciences, 113(17), 4688–4693.\nBennett, C., Miller, M., & Wolford, G. (2009). Neural correlates of interspecies perspective\ntaking in the post-mortem atlantic salmon: An argument for multiple comparisons\ncorrection [Organization for Human Brain Mapping 2009 Annual Meeting].\nNeuroImage, 47, S125. https://doi.org/10.1016/S1053-8119(09)71202-9\nBian, N., Han, X., Sun, L., Lin, H., Lu, Y., & He, B. (2023). Chatgpt is a knowledgeable\nbut inexperienced solver: An investigation of commonsense problem in large\nlanguage models. https://arxiv.org/abs/2303.16421\nBinz, M., & Schulz, E. (2023). Turning large language models into cognitive models.\nhttps://arxiv.org/abs/2306.03917\nBlasi, D. E., Henrich, J., Adamou, E., Kemmerer, D., & Majid, A. (2022). Over-reliance on\nenglish hinders cognitive science.Trends in Cognitive Sciences.\nPERILS AND OPPORTUNITIES OF LLMS 30\nBonnefon, J.-F., Shariff, A., & Rahwan, I. (2020). The moral psychology of ai and the\nethical opt-out problem.Ethics of Artificial Intelligence, 109–126.\nBozkurt, A., Xiao, J., Lambert, S., Pazurek, A., Crompton, H., Koseoglu, S., Farrow, R.,\nBond, M., Nerantzi, C., Honeychurch, S., et al. (2023). Speculative futures on\nchatgpt and generative artificial intelligence (ai): A collective reflection from the\neducational landscape.Asian Journal of Distance Education, 18(1).\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P.,\nNeelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models\nare few-shot learners.Advances in Neural Information Processing Systems, 33,\n1877–1901.\nBuolamwini, J., & Gebru, T. (2018). Gender shades: Intersectional accuracy disparities in\ncommercial gender classification.Proceedings of the Conference on fairness,\naccountability and transparency, 77–91.\nCacioppo, J. T., & Berntson, G. G. (1999). The affect system: Architecture and operating\ncharacteristics. Current Directions in Psychological Science, 8(5), 133–137.\nCacioppo, J. T., & Petty, R. E. (1982). The need for cognition.Journal of Personality and\nSocial Psychology, 42(1), 116.\nCao, Y., Zhou, L., Lee, S., Cabello, L., Chen, M., & Hershcovich, D. (2023). Assessing\ncross-cultural alignment between chatgpt and human societies: An empirical study.\nhttps://arxiv.org/abs/2303.17466\nChen, K., Shao, A., Burapacheep, J., & Li, Y. (2022). A critical appraisal of equity in\nconversational ai: Evidence from auditing gpt-3’s dialogues with different publics on\nclimate change and black lives matter. https://arxiv.org/abs/2209.13627\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H.,\nBurda, Y., Joseph, N., Brockman, G., et al. (2021). Evaluating large language\nmodels trained on code. https://arxiv.org/abs/2107.03374\nPERILS AND OPPORTUNITIES OF LLMS 31\nCliche, M. (2017). Bb_twtr at semeval-2017 task 4: Twitter sentiment analysis with cnns\nand lstms.arXiv preprint arXiv:1704.06125.\nÇöltekin, Ç. (2020). A corpus of turkish offensive language on social media.Proceedings of\nthe Twelfth Language Resources and Evaluation Conference, 6174–6184.\nConte, R., Gilbert, N., Bonelli, G., Cioffi-Revilla, C., Deffuant, G., Kertesz, J., Loreto, V.,\nMoat, S., Nadal, J. .-., Sanchez, A., et al. (2012). Manifesto of computational social\nscience. The European Physical Journal Special Topics, 214, 325–346.\nCrockett, M., & Messeri, L. (2023). Should large language models replace human\nparticipants?\nCronbach, L. J. (1957). The two disciplines of scientific psychology.American psychologist,\n12(11), 671.\nDale, R. (2021). Gpt-3: What’s it good for?Natural Language Engineering, 27(1), 113–118.\nDeng, L., & Liu, Y. (2018).Deep learning in natural language processing. Springer.\nDeshpande, A., Murahari, V., Rajpurohit, T., Kalyan, A., & Narasimhan, K. (2023).\nToxicity in chatgpt: Analyzing persona-assigned language models.\nhttps://arxiv.org/abs/2304.05335\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep\nbidirectional transformers for language understanding.\nhttps://arxiv.org/abs/1810.04805\nDillion, D., Tandon, N., Gu, Y., & Gray, K. (2023). Can ai language models replace human\nparticipants? Trends in Cognitive Sciences.\nDurmus, E., Nyugen, K., Liao, T. I., Schiefer, N., Askell, A., Bakhtin, A., Chen, C.,\nHatfield-Dodds, Z., Hernandez, D., Joseph, N., et al. (2023). Towards measuring the\nrepresentation of subjective global opinions in language models.\nhttps://arxiv.org/abs/2306.16388\nDwivedi, Y. K., Kshetri, N., Hughes, L., Slade, E. L., Jeyaraj, A., Kar, A. K.,\nBaabdullah, A. M., Koohang, A., Raghavan, V., Ahuja, M., et al. (2023). “so what\nPERILS AND OPPORTUNITIES OF LLMS 32\nif chatgpt wrote it?” multidisciplinary perspectives on opportunities, challenges and\nimplications of generative conversational ai for research, practice and policy.\nInternational Journal of Information Management, 71, 102642.\nEdelmann, A., Wolff, T., Montagne, D., & Bail, C. A. (2020). Computational social science\nand sociology.Annual Review of Sociology, 46, 61–81.\nEldan, R., & Russinovich, M. (2023). Who’s harry potter? approximate unlearning in llms.\nElman, J. L. (1990). Finding structure in time.Cognitive Science, 14(2), 179–211.\nEysenck, H. J. (1967).The biological basis of personality(Vol. 689). Transaction publishers.\nFiedler, K. (2011). Voodoo correlations are everywhere—not only in neuroscience.\nPerspectives on Psychological Science, 6(2), 163–171.\nFloridi, L. (2023). Ai as agency without intelligence: On chatgpt, large language models,\nand other generative models.Philosophy & Technology, 36(1), 15.\nFloridi, L., & Chiriatti, M. (2020). Gpt-3: Its nature, scope, limits, and consequences.\nMinds and Machines, 30, 681–694.\nFossati, A., Borroni, S., Marchione, D., & Maffei, C. (2011). The big five inventory (bfi).\nEuropean Journal of Psychological Assessment.\nFrank, M. C. (2023a). Baby steps in evaluating the capacities of large language models.\nNature Reviews Psychology, 1–2.\nFrank, M. C. (2023b). Large language models as models of human cognition.\nFujita, H., et al. (2022). Prompt sensitivity of language model for solving programming\nproblems. New Trends in Intelligent Software Methodologies, Tools and Techniques:\nProceedings of the 21st International Conference on New Trends in Intelligent\nSoftware Methodologies, Tools and Techniques (SoMeT_22), 355, 346.\nGan, C., & Mori, T. (2023). Sensitivity and robustness of large language models to prompt\nin japanese. https://arxiv.org/abs/2305.08714\nGarten, J., Hoover, J., Johnson, K. M., Boghrati, R., Iskiwitch, C., & Dehghani, M. (2018).\nDictionaries and distributions: Combining expert knowledge and large scale textual\nPERILS AND OPPORTUNITIES OF LLMS 33\ndata content analysis: Distributed dictionary representation.Behavior research\nmethods, 50, 344–361.\nGhosh, A., & Bir, A. (2023). Evaluating chatgpt’s ability to solve higher-order questions on\nthe competency-based medical education curriculum in medical biochemistry.\nCureus, 15(4).\nGhosh, S., & Caliskan, A. (2023). Chatgpt perpetuates gender bias in machine translation\nand ignores non-gendered pronouns: Findings across bengali and five other\nlow-resource languages. https://arxiv.org/abs/2305.10510\nGolbeck, J. A. (2016). Predicting personality from social media text.AIS Transactions on\nReplication Research, 2(1), 2.\nGoldberg, Y. (2022).Neural network methods for natural language processing. Springer\nNature.\nGraham, J., Haidt, J., Motyl, M., Meindl, P., Iskiwitch, C., & Mooijman, M. (2018). Moral\nfoundations theory.Atlas of Moral Psychology, 211.\nGraves, A. (2013). Generating sequences with recurrent neural networks.\nhttps://arxiv.org/abs/1308.0850\nGrossmann, I., Feinberg, M., Parker, D. C., Christakis, N. A., Tetlock, P. E., &\nCunningham, W. A. (2023). Ai and the transformation of social science research.\nScience, 380(6650), 1108–1109.\nGuo, Y., Yang, Y., & Abbasi, A. (2022). Auto-debias: Debiasing masked language models\nwith automated biased prompts.Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics, 1012–1023.\nHagendorff, T. (2020). The ethics of ai ethics: An evaluation of guidelines.Minds and\nMachines, 30(1), 99–120.\nHartmann, J., Schwenzow, J., & Witte, M. (2023). The political ideology of conversational\nai: Converging evidence on chatgpt’s pro-environmental, left-libertarian orientation.\nhttps://arxiv.org/abs/2301.01768\nPERILS AND OPPORTUNITIES OF LLMS 34\nHenrich, J., Heine, S. J., & Norenzayan, A. (2010). The weirdest people in the world?\nBehavioral and Brain Sciences, 33(2-3), 61–83.\nHofman, J. M., Watts, D. J., Athey, S., Garip, F., Griffiths, T. L., Kleinberg, J.,\nMargetts, H., Mullainathan, S., Salganik, M. J., Vazire, S., et al. (2021). Integrating\nexplanation and prediction in computational social science.Nature, 595(7866),\n181–188.\nHolterman, B., & van Deemter, K. (2023). Does chatgpt have theory of mind?\nhttps://arxiv.org/abs/2305.14020\nHoover, J., Portillo-Wightman, G., Yeh, L., Havaldar, S., Davani, A. M., Lin, Y.,\nKennedy, B., Atari, M., Kamel, Z., Mendlen, M., et al. (2020). Moral foundations\ntwitter corpus: A collection of 35k tweets annotated for moral sentiment.Social\nPsychological and Personality Science, 11(8), 1057–1071.\nHoulsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A.,\nAttariyan, M., & Gelly, S. (2019). Parameter-efficient transfer learning for nlp.\nInternational Conference on Machine Learning, 2790–2799.\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W.\n(2021). Lora: Low-rank adaptation of large language models.arXiv preprint\narXiv:2106.09685.\nHu, X., Tian, Y., Nagato, K., Nakao, M., & Liu, A. (2023). Opportunities and challenges\nof chatgpt for design knowledge management. https://arxiv.org/abs/2304.02796\nHuang, F., Kwak, H., & An, J. (2023). Is chatgpt better than human annotators? potential\nand limitations of chatgpt in explaining implicit hate speech.arXiv preprint\narXiv:2302.07736.\nHwang, E., Majumder, B. P., & Tandon, N. (2023). Aligning language models to user\nopinions. https://arxiv.org/abs/2305.14929\nPERILS AND OPPORTUNITIES OF LLMS 35\nJiang, E., Olson, K., Toh, E., Molina, A., Donsbach, A., Terry, M., & Cai, C. J. (2022).\nPromptmaker: Prompt-based prototyping with large language models.CHI\nConference on Human Factors in Computing Systems Extended Abstracts, 1–8.\nJiang, H., Zhang, X., Cao, X., Kabbara, J., & Roy, D. (2023). Personallm: Investigating\nthe ability of gpt-3.5 to express personality traits and gender differences.\nhttps://arxiv.org/abs/2305.02547\nJobin, A., Ienca, M., & Vayena, E. (2019). The global landscape of ai ethics guidelines.\nNature Machine Intelligence, 1(9), 389–399.\nJohn, O. P., & Srivastava, S. (1999). The big-five trait taxonomy: History, measurement,\nand theoretical perspectives.\nKalla, D., & Smith, N. (2023). Study and analysis of chat gpt and its impact on different\nfields of study.International Journal of Innovative Science and Research\nTechnology, 8(3).\nKennedy, B., Jin, X., Davani, A. M., Dehghani, M., & Ren, X. (2020). Contextualizing\nhate speech classifiers with post-hoc explanation.Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, 5435–5442.\nKennedy, B., Reimer, N. K., & Dehghani, M. (2021). Explaining explainability:\nInterpretable machine learning for the behavioral sciences.\nhttps://psyarxiv.com/9h6qr/\nKeskar, N. S., McCann, B., Varshney, L. R., Xiong, C., & Socher, R. (2019). Ctrl: A\nconditional transformer language model for controllable generation.\nhttps://arxiv.org/abs/1909.05858\nKjell, O., Kjell, K., & Schwartz, H. A. (2023). Ai-based large language models are ready to\ntransform psychological health assessment.\nKocoń, J., Cichecki, I., Kaszyca, O., Kochanek, M., Szydło, D., Baran, J., Bielaniewicz, J.,\nGruza, M., Janz, A., Kanclerz, K., et al. (2023). Chatgpt: Jack of all trades, master\nof none.Information Fusion, 101861.\nPERILS AND OPPORTUNITIES OF LLMS 36\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2022). Large language models\nare zero-shot reasoners.Advances in neural information processing systems, 35,\n22199–22213.\nKoralus, P., & Wang-Maścianica, V. (2023). Humans in humans out: On gpt converging\ntoward common sense in both success and failure. https://arxiv.org/abs/2303.17276\nKrügel, S., Ostermaier, A., & Uhl, M. (2023). Chatgpt’s inconsistent moral advice\ninfluences users’ judgment.Scientific Reports, 13(1), 4569.\nLazer, D., Pentland, A., Adamic, L., Aral, S., Barabási, A.-L., Brewer, D., Christakis, N.,\nContractor, N., Fowler, J., Gutmann, M., et al. (2009). Computational social\nscience. Science, 323(5915), 721–723.\nLewis, P., Stenetorp, P., & Riedel, S. (2020). Question and answer test-train overlap in\nopen-domain question answering datasets. https://arxiv.org/abs/2008.02637\nLi, J., Yang, Y., Wu, Z., Vydiswaran, V., & Xiao, C. (2023). Chatgpt as an attack tool:\nStealthy textual backdoor attack via blackbox generative model trigger.\nhttps://arxiv.org/abs/2304.14475\nLiesenfeld, A., Lopez, A., & Dingemanse, M. (2023). Opening up chatgpt: Tracking\nopenness, transparency, and accountability in instruction-tuned text generators.\nProceedings of the 5th International Conference on Conversational User Interfaces,\n1–6.\nLin, B. Y., Fu, Y., Yang, K., Ammanabrolu, P., Brahman, F., Huang, S., Bhagavatula, C.,\nChoi, Y., & Ren, X. (2023). Swiftsage: A generative agent with fast and slow\nthinking for complex interactive tasks. https://arxiv.org/abs/2305.17390\nLiscio, E., Araque, O., Gatti, L., Constantinescu, I., Jonker, C., Kalimeri, K., &\nMurukannaiah, P. K. (2023). What does a text classifier learn about morality? an\nexplainable method for cross-domain comparison of moral rhetoric.Proceedings of\nthe 61st Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), 14113–14132.\nPERILS AND OPPORTUNITIES OF LLMS 37\nLiu, H., Ning, R., Teng, Z., Liu, J., Zhou, Q., & Zhang, Y. (2023). Evaluating the logical\nreasoning ability of chatgpt and gpt-4. https://arxiv.org/abs/2304.03439\nLiu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2023). Pre-train, prompt,\nand predict: A systematic survey of prompting methods in natural language\nprocessing. ACM Computing Surveys, 55(9), 1–35.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,\nZettlemoyer, L., & Stoyanov, V. (2019). Roberta: A robustly optimized bert\npretraining approach. https://arxiv.org/abs/1907.11692\nLlorens, A., Tzovara, A., Bellier, L., Bhaya-Grossman, I., Bidet-Caulet, A., Chang, W. K.,\nCross, Z. R., Dominguez-Faus, R., Flinker, A., Fonken, Y., et al. (2021). Gender\nbias in academia: A lifetime problem that needs solutions.Neuron, 109(13),\n2047–2074. https://doi.org/10.1016/j.neuron.2021.06.002\nLo, C. K. (2023). What is the impact of chatgpt on education? a rapid review of the\nliterature. Education Sciences, 13(4), 410.\nLu, Y., Bartolo, M., Moore, A., Riedel, S., & Stenetorp, P. (2021). Fantastically ordered\nprompts and where to find them: Overcoming few-shot prompt order sensitivity.\nhttps://arxiv.org/abs/2104.08786\nMao, R., Liu, Q., He, K., Li, W., & Cambria, E. (2022). The biases of pre-trained language\nmodels: An empirical study on prompt-based sentiment analysis and emotion\ndetection. IEEE Transactions on Affective Computing.\nMarkowitz, D. M. (2023). Can generative ai infer thinking style from language? questioning\nthe utility of ai as a psychological text analysis tool. https://psyarxiv.com/zy8gr/\nMcClelland, J. L., & Rumelhart, D. E. (1985). Distributed memory and the representation\nof general and specific information.Journal of Experimental Psychology: General,\n114(2), 159.\nPERILS AND OPPORTUNITIES OF LLMS 38\nMeade, N., Poole-Dayan, E., & Reddy, S. (2021). An empirical survey of the effectiveness\nof debiasing techniques for pre-trained language models.\nhttps://arxiv.org/abs/2110.08527\nMedin, D., Ojalehto, B., Marin, A., & Bang, M. (2017). Systems of (non-) diversity.Nature\nHuman Behaviour, 1(5), 0088.\nMichaux, C. (2023). Can chat gpt be considered an author? i met with chat gpt and asked\nsome questions about philosophy of art and philosophy of mind.Available at SSRN\n4439607.\nMischel, W. (2013).Personality and assessment. Psychology Press.\nMishra, A., Soni, U., Arunkumar, A., Huang, J., Kwon, B. C., & Bryan, C. (2023).\nPromptaid: Prompt exploration, perturbation, testing and iteration using visual\nanalytics for large language models. https://arxiv.org/abs/2304.01964\nMotoki, F., Pinho Neto, V., & Rodrigues, V. (2023). More human than human: Measuring\nchatgpt political bias.Available at SSRN 4372349.\nMuthukrishna, M., & Henrich, J. (2019). A problem in theory.Nature Human Behaviour,\n3(3), 221–229.\nNosek, B. A., Hardwicke, T. E., Moshontz, H., Allard, A., Corker, K. S., Dreber, A.,\nFidler, F., Hilgard, J., Kline Struhl, M., Nuijten, M. B., et al. (2022). Replicability,\nrobustness, and reproducibility in psychological science.Annual review of\npsychology, 73, 719–748.\nOchsner, K. N., Ray, R. R., Hughes, B., McRae, K., Cooper, J. C., Weber, J.,\nGabrieli, J. D., & Gross, J. J. (2009). Bottom-up and top-down processes in\nemotion generation: Common and distinct neural mechanisms.Psychological\nScience, 20(11), 1322–1331.\nOmrani, A., Salkhordeh Ziabari, A., Yu, C., Golazizian, P., Kennedy, B., Atari, M., Ji, H.,\n& Dehghani, M. (2023). Social-group-agnostic bias mitigation via the stereotype\ncontent model.Proceedings of the 61st Annual Meeting of the Association for\nPERILS AND OPPORTUNITIES OF LLMS 39\nComputational Linguistics (Volume 1: Long Papers), 4123–4139.\nhttps://aclanthology.org/2023.acl-long.227\nOpenAI. (2022, November). Introducing chatgpt.\nOpenAI. (2023). Gpt-4 technical report. https://arxiv.org/abs/2303.08774\nPacini, R., & Epstein, S. (1999). The relation of rational and experiential information\nprocessing styles to personality, basic beliefs, and the ratio-bias phenomenon.\nJournal of Personality and Social Psychology, 76(6), 972.\nPangakis, N., Wolken, S., & Fasching, N. (2023). Automated annotation with generative ai\nrequires validation. https://arxiv.org/abs/2306.00176\nPark, P. S., Goldstein, S., O’Gara, A., Chen, M., & Hendrycks, D. (2023). Ai deception: A\nsurvey of examples, risks, and potential solutions.arXiv preprint arXiv:2308.14752.\nPark, P. S., Schoenegger, P., & Zhu, C. (2023). Diminished diversity-of-thought in a\nstandard large language model. https://arxiv.org/abs/2302.07267\nPaul, J., Ueno, A., & Dennis, C. (2023). Chatgpt and consumers: Benefits, pitfalls and\nfuture research agenda.\nPennebaker, J. W., Francis, M. E., & Booth, R. J. (2001). Linguistic inquiry and word\ncount: Liwc 2001.Mahway: Lawrence Erlbaum Associates, 71(2001), 2001.\nPournaras, E. (2023). Science in the era of chatgpt, large language models and ai:\nChallenges for research ethics review and how to respond.\nhttps://arxiv.org/abs/2305.15299\nRadford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al. (2018). Improving language\nunderstanding by generative pre-training.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. (2019). Language\nmodels are unsupervised multitask learners.OpenAI Blog, 1(8), 9.\nRahman, A. U., & Halim, Z. (2022). Predicting the big five personality traits from\nhand-written text features through semi-supervised learning.Multimedia Tools and\nApplications, 81(23), 33671–33687.\nPERILS AND OPPORTUNITIES OF LLMS 40\nRamezani, A., & Xu, Y. (2023). Knowledge of cultural moral norms in large language\nmodels. https://arxiv.org/abs/2306.01857\nRao, H., Leung, C., & Miao, C. (2023). Can chatgpt assess human personalities? a general\nevaluation framework. https://arxiv.org/abs/2303.01248\nRathje, S., Mirea, D.-M., Sucholutsky, I., Marjieh, R., Robertson, C., & Van Bavel, J. J.\n(2023). Gpt is an effective tool for multilingual psychological text analysis.\nhttps://psyarxiv.com/sekf5/\nRay, P. P. (2023). Chatgpt: A comprehensive review on background, applications, key\nchallenges, bias, ethics, limitations and future scope.Internet of Things and\nCyber-Physical Systems.\nReimers, N., & Gurevych, I. (2019). Sentence-bert: Sentence embeddings using siamese\nbert-networks. https://arxiv.org/abs/1908.10084\nReiss, M. V. (2023). Testing the reliability of chatgpt for text annotation and classification:\nA cautionary remark.arXiv preprint arXiv:2304.11085.\nReynolds, L., & McDonell, K. (2021). Prompt programming for large language models:\nBeyond the few-shot paradigm.Extended Abstracts of the 2021 CHI Conference on\nHuman Factors in Computing Systems, 1–7.\nRomera-Paredes, B., & Torr, P. (2015). An embarrassingly simple approach to zero-shot\nlearning. International conference on machine learning, 2152–2161.\nRoose, K. (2022). The brilliance and weirdness of chatgpt.The New York Times.\nRuder, S., Peters, M. E., Swayamdipta, S., & Wolf, T. (2019). Transfer learning in natural\nlanguage processing.Proceedings of the 2019 conference of the North American\nchapter of the association for computational linguistics: Tutorials, 15–18.\nRumelhart, D. E., Hinton, G. E., McClelland, J. L., et al. (1986). A general framework for\nparallel distributed processing.Parallel Distributed Processing: Explorations in the\nMicrostructure of Cognition, 45–76.\nPERILS AND OPPORTUNITIES OF LLMS 41\nRumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by\nback-propagating errors.Nature, 323(6088), 533–536.\nRussell, S. (2019).Human compatible: Artificial intelligence and the problem of control.\nPenguin.\nRutinowski, J., Franke, S., Endendyk, J., Dormuth, I., & Pauly, M. (2023). The\nself-perception and political biases of chatgpt. https://arxiv.org/abs/2304.07333\nSanturkar, S., Durmus, E., Ladhak, F., Lee, C., Liang, P., & Hashimoto, T. (2023). Whose\nopinions do language models reflect? https://arxiv.org/abs/2303.17548\nScao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., Castagné, R.,\nLuccioni, A. S., Yvon, F., Gallé, M., et al. (2022). Bloom: A 176b-parameter\nopen-access multilingual language model.ArXiv Preprint arXiv:2211.05100.\nSchwarz, N. (1999). Self-reports: How the questions shape the answers.American\npsychologist, 54(2), 93.\nSchwarz, N., Bless, H., Strack, F., Klumpp, G., Rittenauer-Schatka, H., & Simons, A.\n(1991). Ease of retrieval as information: Another look at the availability heuristic.\nJournal of Personality and Social psychology, 61(2), 195.\nShaikh, O., Zhang, H., Held, W., Bernstein, M., & Yang, D. (2022). On second thought,\nlet’s not think step by step! bias and toxicity in zero-shot reasoning.\nhttps://arxiv.org/abs/2212.08061\nShiffrin, R., & Mitchell, M. (2023). Probing the psychology of ai models.Proceedings of the\nNational Academy of Sciences, 120(10), e2300963120.\nSi, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber, J., & Wang, L. (2022).\nPrompting gpt-3 to be reliable. https://arxiv.org/abs/2210.09150\nSinghal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W., Scales, N.,\nTanwani, A., Cole-Lewis, H., Pfohl, S., et al. (2023). Large language models encode\nclinical knowledge.Nature.\nPERILS AND OPPORTUNITIES OF LLMS 42\nSloan, L., Joyner, M., Stakeman, C., & Schmitz, C. (2018).Critical multiculturalism and\nintersectionality in a complex world. Oxford University Press.\nSnell, J., Swersky, K., & Zemel, R. (2017). Prototypical networks for few-shot learning.\nAdvances in neural information processing systems, 30.\nSoice, E. H., Rocha, R., Cordova, K., Specter, M., & Esvelt, K. M. (2023). Can large\nlanguage models democratize access to dual-use biotechnology?arXiv preprint\narXiv:2306.03809.\nSpirling, A. (2023). Why open-source generative ai models are an ethical way forward for\nscience. Nature, 616(7957), 413–413.\nStickland, A. C., & Murray, I. (2019). Bert and pals: Projected attention layers for efficient\nadaptation in multi-task learning.International Conference on Machine Learning,\n5986–5995.\nSundar, S. S., & Liao, M. (2023). Calling bs on chatgpt: Reflections on ai as a\ncommunication source.Journalism & Communication Monographs, 25(2), 165–180.\nSuri, G., Slater, L. R., Ziaee, A., & Nguyen, M. (2023). Do large language models show\ndecision heuristics similar to humans? a case study using gpt-3.5.\nhttps://arxiv.org/abs/2305.04400\nSutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural\nnetworks. Advances in Neural Information Processing Systems, 27.\nSzabo, A. (2023). Chatgpt is a breakthrough in science and education but fails a test in\nsports and exercise psychology.Baltic Journal of Sport and Health Sciences, 1(128),\n25–40.\nTausczik, Y. R., & Pennebaker, J. W. (2010). The psychological meaning of words: Liwc\nand computerized text analysis methods.Journal of Language and Social\nPsychology, 29(1), 24–54.\nThalmayer, A. G., Toscanelli, C., & Arnett, J. J. (2021). The neglected 95% revisited: Is\namerican psychology becoming less american?American Psychologist, 76(1), 116.\nPERILS AND OPPORTUNITIES OF LLMS 43\nTian, H., Lu, W., Li, T. O., Tang, X., Cheung, S.-C., Klein, J., & Bissyandé, T. F. (2023).\nIs chatgpt the ultimate programming assistant–how far is it?\nhttps://arxiv.org/abs/2304.11938\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T.,\nRozière, B., Goyal, N., Hambro, E., Azhar, F., et al. (2023). Llama: Open and\nefficient foundation language models. https://arxiv.org/abs/2302.13971\nTrager, J., Ziabari, A. S., Davani, A. M., Golazazian, P., Karimi-Malekabadi, F.,\nOmrani, A., Li, Z., Kennedy, B., Reimer, N. K., Reyes, M., et al. (2022). The moral\nfoundations reddit corpus. https://arxiv.org/abs/2208.05545\nTurc, I., Chang, M.-W., Lee, K., & Toutanova, K. (2019). Well-read students learn better:\nOn the importance of pre-training compact models.\nhttps://arxiv.org/abs/1908.08962\nVan Dis, E. A., Bollen, J., Zuidema, W., van Rooij, R., & Bockting, C. L. (2023). Chatgpt:\nFive priorities for research.Nature, 614(7947), 224–226.\nVan Rossum, G., & Drake, F. L. (2009).Python 3 reference manual. CreateSpace.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., &\nPolosukhin, I. (2017). Attention is all you need.Advances in Neural Information\nProcessing Systems, 30.\nVazire, S. (2018). Implications of the credibility revolution for productivity, creativity, and\nprogress. Perspectives on Psychological Science, 13(4), 411–417.\nWang, W., Wei, F., Dong, L., Bao, H., Yang, N., & Zhou, M. (2020). Minilm: Deep\nself-attention distillation for task-agnostic compression of pre-trained transformers.\nAdvances in Neural Information Processing Systems, 33, 5776–5788.\nWang, Z., Zhang, G., Yang, K., Shi, N., Zhou, W., Hao, S., Xiong, G., Li, Y., Sim, M. Y.,\nChen, X., et al. (2023). Interactive natural language processing.\nhttps://arxiv.org/abs/2305.13246\nPERILS AND OPPORTUNITIES OF LLMS 44\nWebb, T., Holyoak, K. J., & Lu, H. (2022). Emergent analogical reasoning in large\nlanguage models. https://arxiv.org/abs/2212.09196\nWebster, D. M., & Kruglanski, A. W. (1994). Need for closure scale.Journal of Personality\nand Social Psychology.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al.\n(2022). Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in Neural Information Processing Systems, 35, 24824–24837.\nWindsor, D. (2006). Corporate social responsibility: Three key approaches.Journal of\nManagement Studies, 43(1), 93–114.\nWong, J., & Kim, J. (2023). Chatgpt is more likely to be perceived as male than female.\nhttps://arxiv.org/abs/2305.12564\nWu, Z., Qiu, L., Ross, A., Akyürek, E., Chen, B., Wang, B., Kim, N., Andreas, J., &\nKim, Y. (2023). Reasoning or reciting? exploring the capabilities and limitations of\nlanguage models through counterfactual tasks. https://arxiv.org/abs/2307.02477\nXian, Y., Lampert, C. H., Schiele, B., & Akata, Z. (2018). Zero-shot learning—a\ncomprehensive evaluation of the good, the bad and the ugly.IEEE transactions on\npattern analysis and machine intelligence, 41(9), 2251–2265.\nYang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., & Le, Q. V. (2019). Xlnet:\nGeneralized autoregressive pretraining for language understanding.Advances in\nNeural Information Processing Systems, 32.\nZakrisson, I. (2005). Construction of a short version of the right-wing authoritarianism\n(rwa) scale.Personality and Individual Differences, 39(5), 863–872.\nZhai, X. (2023). Chatgpt for next generation science learning.XRDS: Crossroads, The\nACM Magazine for Students, 29(3), 42–46.\nZhang, C., Zhang, C., Li, C., Qiao, Y., Zheng, S., Dam, S. K., Zhang, M., Kim, J. U.,\nKim, S. T., Choi, J., et al. (2023). One small step for generative ai, one giant leap\nfor agi: A complete survey on chatgpt in aigc era. https://arxiv.org/abs/2304.06488\nPERILS AND OPPORTUNITIES OF LLMS 45\nZhong, Q., Ding, L., Liu, J., Du, B., & Tao, D. (2023). Can chatgpt understand too? a\ncomparative study on chatgpt and fine-tuned bert.\nhttps://arxiv.org/abs/2302.10198\nZhu, Y., Zhang, P., Haq, E.-U., Hui, P., & Tyson, G. (2023). Can chatgpt reproduce\nhuman-generated labels? a study of social computing tasks.\nhttps://arxiv.org/abs/2304.10145\nZiegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D.,\nChristiano, P., & Irving, G. (2019). Fine-tuning language models from human\npreferences. arXiv preprint arXiv:1909.08593.\nPERILS AND OPPORTUNITIES OF LLMS 46\nSupplementary Information\nS1. Text Annotations\nWe compared ChatGPT’s automated text annotation performance against a\nfine-tuned BERT-based model and a Linguistic Inquiry and Word Count (LIWC) based\nmodel. We further analyzed whether ChatGPT exhibited biases, that is, whether it aligned\nmore with specific human annotator demographics on the test data.\nMethods\nWe make all data, study materials, and analysis code available at\nhttps://osf.io/nafzy/. The repository contains all the necessary instructions to replicate\nour analyses.\nData. We utilized the Moral Foundations Reddit Corpus (MFRC; Trager et al.,\n2022b), a collection of 16,123 Reddit comments that have been hand-annotated by at least\nthree trained annotators for 8 categories of moral sentiment (i.e., Care, Proportionality,\nEquality, Purity, Authority, Loyalty, Thin Morality, Non-morality) based on the updated\nMoral Foundations Theory (MFT; Atari, Haidt, et al., 2023b) framework. The MFRC\nfurther gathered demographic and psychometric information about each human annotator.\nWe randomly divided the MFRC into a test set (2,983 samples) to evaluate the annotation\naccuracy of ChatGPT and a BERT-based small language model (fine-tuned with a training\nset of 13,140 samples). See the code repository for instructions to replicate the training and\ntest data.\nBERT-model. We used a Bidirectional Encoder Representations from\nTransformers (BERT)-based (Devlin et al., 2018b) classifier to determine the moral\nlanguage in each MFRC post using only the post text as input. Specifically, we used the\npre-trained BERT model “small BERT” (“bert_uncased_L-12_H-512_A-8”; Turc et al.\n(2019)) withL = 12 hidden layers (i.e., Transformer blocks), a hidden size ofH = 256, and\nA = 4 attention heads. We then added a downstream (multi-label) classification layer to\nPERILS AND OPPORTUNITIES OF LLMS 47\nthe language model to predict which of the eight moral sentiments were expressed in a\ngiven post. We simultaneously trained the classification layer and fine-tuned the\nembedding layers on the training set, which was not used to evaluate the annotation\naccuracy. Model training and fine-tuning was conducted in python 3.9 (Van Rossum &\nDrake, 2009) using the tensorflow (v2.12.0; Abadi et al. (2016)) and keras (v2.12.0; Chollet\net al. (2015)) libraries. See the code repository for instructions to replicate the training and\nfine-tuning procedures.\nLIWC-model. Supplementing our BERT-based analysis, we leveraged the\nLinguistic Inquiry and Word Count (LIWC; version 22) to determine moral sentiments in\nthe MFRC texts. LIWC quantifies the frequency of specific word categories, shedding light\non underlying emotional, cognitive, and structural markers in textual content (Pennebaker\net al., 2001) and is a widely used theory-based psychological text analysis method (Garten\net al., 2018; Pennebaker et al., 2015). LIWC allows for analysis regarding the contribution\nof psychologically validated features and facilitates a comparative analysis between two\ndistinct paradigms: the deep learning approach using BERT’s and ChatGPT’s contextual,\ndata driven, embeddings and top-down, theory-motivated approach via LIWC. We\nextracted the LIWC features, spanning 118 dimensions (e.g., linguistic dimensions,\npsychological processes, personal concerns), using the LIWC-22 software www.liwc.app.\nWe then trained a Support Vector Classifier (SVC; Cortes and Vapnik (1995)), which is a\nclassifier known for its flexibility, its simplicity compared to complex neural networks, and\nits effectiveness in high-dimensional spaces. We used the SVC to predict moral sentiment\nusing the LIWC features on the same subset of MFRC texts as the BERT model. We\ntrained the model in python 3.10 (Van Rossum & Drake, 2009) using the sklearn (v1.3.0;\nPedregosa et al. (2011)) library. See the code repository for instructions to replicate the\nfeature extraction and model training procedure.\nChatGPT. We prompted ChatGPT to annotate posts from the MFRC test data\nvia the OpenAI API (https://platform.openai.com/docs/api-reference?lang=python). To\nPERILS AND OPPORTUNITIES OF LLMS 48\naccess the API and collect ChatGPT’s text annotations, we used the OpenAI library\n(v0.27.8; OpenAI (2023)) in Python. We deployed the “gpt-3.5-turbo-0301” model with a\ntemperature of 0 (for maximal deterministic and thus replicable behavior). Temperature is\na ChatGPT parameter expressing how words are generated based on the underlying\nprobability distribution computed by the model. A lower temperature indicates that\nChatGPT chooses the most likely words when generating a response and a higher\ntemperature indicates that ChatGPT samples words based on their probability (making it\nmore variable and “creative”, see https://platform.openai.com/docs/guides/gpt/faq).\nFor the zero-shot application of ChatGPT, we used the following prompt, which\nmatched the definitions of each moral foundation according to MFT and the human\nannotators’ instructions, to collect the annotations:\n“Determine which moral sentiments are expressed in the following text. “care” if the\ntext is about avoiding emotional and physical damage to another individual, “equality” if\nthe text is about equal treatment and equal outcome for individuals, “proportionality” if\nthe text is about individuals getting rewarded in proportion to their merit or contribution,\n“loyalty” if the text is about cooperating with ingroups and competing with outgroups,\n“authority” if the text is about deference toward legitimate authorities and the defense of\ntraditions, all of which are seen as providing stability and fending off chaos, “purity” if the\ntext is about avoiding bodily and spiritual contamination and degradation, “thin morality”\nif the text has a moral sentiment but cannot be categorized as either of the above,\n“non-moral” if no moral sentiment is expressed in the text. Respond only with these words.\nRespond with all words that apply, comma separated. Here is the text: [TEXT TO\nANNOTATE]”\nFor the few-shot application of ChatGPT, we add two representative texts as\nexamples for each moral sentiment when prompting the model to determine the presence of\nmoral sentiments in a given text. The examples were chosen from the same data used to\nfine-tune the BERT model and manually verified to explicitly contain the respective moral\nPERILS AND OPPORTUNITIES OF LLMS 49\nsentiment. Additionally, we split the annotation task into seven separate sub-tasks each\nindicating whether a specific moral sentiment is or is not expressed in a given text. For\nexample, we ask ChatGPT whether, “care” sentiment is expressed in a text and include\ntwo examples of texts that contain “care”. The model then indicates whether the moral\nsentiment is present in the given text to annotate. We repeat this procedure for each moral\nsentiment (care, equality, proportionality, loyalty, authority, thin-morality) resulting in a\nbinary list for each text indicating which of the moral sentiments are expressed in this text.\nThis change in procedure was made because adding multiple examples for each of the seven\npossible moral sentiments would lead to extremely long prompts (task description,\nsentiment definition, 14 example texts and responses) deteriorating model performance.\nAdditionally, splitting the annotations into separate binary annotation tasks for each\nsentiment likely improves model performance because the model does not have to factor in\nother potential sentiments. However, this comes at the cost of increased API requests since\nthe model has to be prompted for each sentiment separately, leading in our case to 7-times\nhigher costs. The following prompt template was used for all moral sentiments:\n[ROLE USER]: Determine the presence of [MORAL SENTIMENT] in the following text.\nThe text contains [MORAL SENTIMENT] if [MORAL SENTIMENT DEFINITION].\nRespond with “yes” if the sentiment is expressed in the text and “no” if it is not. Respond\nonly with a single word and do not elaborate. Here is the text: [EXAMPLE 1]\n[ROLE ASSISTANT]: “yes”\n[ROLE USER]: Determine the presence of the moral sentiment of [MORAL SENTIMENT]\nin the following text: [EXAMPLE 2]\n[ROLE ASSISTANT]: “yes”\n[ROLE USER]: Determine the presence of the moral sentiment of [MORAL SENTIMENT]\nin the following text: [TEXT TO ANNOTATE]\nNote that [ROLE USER] and [ROLE ASSISTANT] are API parameter, indicating\nwhether a prompt is the input of a user or the output of the model. This is necessary for\nPERILS AND OPPORTUNITIES OF LLMS 50\nthe few-shot examples to show the model what model output should be given to a user’s\nrequest.\nFor fine-tuning ChatGPT, we utilize OpenAI’s dedicated service documented at\nhttps://platform.openai.com/docs/guides/fine-tuning. Due to the service’s current\nlimitations, our customization was restricted to setting the number of epochs. We chose\nthe default setting ofepoch = 2 appropriate for our dataset size (N = 13, 140). Although\nChatGPT and BERT were both fine-tuned on identical data, their data processing\nmethods differ: BERT processes raw texts and emits a binary vector representing the\npresence of various moral sentiments, whereas ChatGPT relies on a prompt-response\nmechanism. Therefore, we transformed our training texts into prompts, each directing the\nmodel to detect moral sentiments in a given text and converted the annotation labels to\ncomma-separated lists indicating the moral sentiments present, e.g.,“care,loyalty”. The\nspecific template employed was:\n[ROLE USER]: Determine which moral sentiments are expressed in the following text. The\ntext contains “care” if the text is about avoiding emotional and physical damage to another\nindividual, “equality” if the text is about equal treatment and equal outcome for\nindividuals, “proportionality” if the text is about individuals getting rewarded in\nproportion to their merit or contribution, “loyalty” if the text is about cooperating with\ningroups and competing with outgroups, “authority” if the text is about deference toward\nlegitimate authorities and the defense of traditions, all of which are seen as providing\nstability and fending off chaos, “purity” if the text is about avoiding bodily and spiritual\ncontamination and degradation, “thin morality” if the text has a moral sentiment but\ncannot be categorized as either of the above, “non-moral” if no moral sentiment is\nexpressed in the text. Respond only with these words. Respond with all words that apply,\ncomma separated. Here is the text: [TEXT TO ANNOTATE]\n[ROLE ASSISTANT]: [MORAL SENTIMENTS COMMA-SEPARATED]\nPERILS AND OPPORTUNITIES OF LLMS 51\nPerformance Evaluation. To evaluate the annotation accuracy of the\nBERT-based model, LIWC-based model, and ChatGPT (zero-shot, few-shot, fine-tuned),\nwe calculated each model’s F1 score (Goutte & Gaussier, 2005), which is a widely used\nsingle metric that assesses the model’s ability to make accurate positive predictions while\nminimizing false positives and false negatives. We calculated the F1 scores with the human\nannotations as ground truth using the Sklearn library (v1.3.0; Pedregosa et al. (2011)) in\nPython. We calculated the macro averaged F1 score, that is, the unweighted average of the\nF1 scores for the prediction of each type of moral sentiment, which expresses the model’s\naccuracy across all moral sentiments. Finally, we fit a logistic regression model predicting\nthe presence of moral sentiment in an annotation (binary, yes/no) as a function of\nannotator type (Human, ChatGPT, BERT, LIWC). This model expressed how much more\nlikely the computational models were to annotate a moral sentiment compared to human\nground truth. This model was used to evaluate whether ChatGPT, BERT, or LIWC\nsignificantly over-predicted or under-predicted (e.g., false positives, false negatives) the\nspecific moral sentiments.\nWe conducted all statistical analyses in R (v4.1.2; R Core Team (2021)) using the\nstats (v4.1.2; R Core Team (2021)) package (R Core Team, 2021). See the code repository\nfor instructions to replicate the statistical analyses.\nBias Analyses. To determine biases in ChatGPT’s annotations, we investigated\nhow much ChatGPT aligned with annotators of specific demographics. To that end, we\ncreate a binary variable expressing whether ChatGPT’s annotation and the human\nannotators’ rating aligned (1 if a ChatGPT and human annotation were equal; 0 if not). If\nthere were multiple sentiments in one post, we compared each one separately. We then fit\nlogistic regression models that predicted human-ChatGPT alignment as a function of\nannotator demographics and psychometric information. These models describe how likely\nChatGPT is to align with an annotator of certain demographics (e.g., race, gender, age).\nIn total, we tested for biases toward annotators’ age, sex, religion, moral values (based on\nPERILS AND OPPORTUNITIES OF LLMS 52\nMFT), personality (Five-Factor Model), creativity (creative imagination, aesthetic\nsensitivity, intellectual curiosity), mental health (anxiety, depression, emotional volatility),\npolitical orientation (collectivism, individualism, cultural-tightness, social-conservatism,\neconomical-conservatism), interpersonal attitudes (compassion, trust, respectfulness), and\nefficiency (productiveness, organization, responsibility).\nWe conducted all statistical analyses in R (v4.1.2; R Core Team (2021)) using the\nstats (v4.1.2; R Core Team (2021)) package (R Core Team, 2021). See the code repository\nfor instructions to replicate the statistical analyses.\nResults\nOverall, we found that fine-tuned BERT outperformed zeroshot-ChatGPT by a\nlarge margin. Specifically, BERT achieved an F1 score of 0.49, while ChatGPT achieved an\nF1 score of 0.22. Additionally, ChatGPT was more extreme in over or under-predicting the\nmoral sentiments. Notably, ChatGPT extremely under-predicted nonmoral texts.\nEssentially, ChatGPT classified every text as moral. Additionally, it vastly over-classified\n“Thin-morality”, which may be connected to its over-sensitivity to moral language\ndetection (e.g., nonmoral texts are misclassified as “Thin-morality”). See Table S1 for an\noverview of BERT and ChatGPT deviation from human ground truth. Interestingly, even\nthe LIWC-based approach outperformed ChatGPT with an F1 score of 0.27 and LIWC\nwas significantly less likely and less extreme to over-predict or under-predict a moral\nsentiment except for Care and Loyalty, comparable to the BERT-based model. Note, that\nwe did not conduct any elaborate preprocessing or parameter tuning (beyond the\nregularization weights) or attempted to train more powerful complex classifiers for the\nLIWC-based method.\nApplying ChatGPT with more elaborate methods, such as few-shot prompting or\noutright fine-tuning, noticeably increased model performance. Fewshot-ChatGPT achieved\nan F1 score of 0.32 and fine-tuned ChatGPT achieved an F1 score of 0.54 even surpassing\nPERILS AND OPPORTUNITIES OF LLMS 53\nfine-tuned BERT. Furthermore, they were in most cases less extreme in their deviations\nfrom groundtruth compared to the zero-shot application. Notably, fewshot-ChatGPT\nimproves zeroshot-ChatGPT’s vast overestimation of the presence of thin-morality\n(+1,240%) to -70% and fine-tuned ChatGPT further improved this to -32%. Similarly,\nfewshot-ChatGPT improved zeroshot-ChatGPT’s deviation from groundtruth of nearly 100\npercent on nonmoral texts (essentially detecting a moral sentiment in nearly all posts) to\n+90% and fine-tuned ChatGPT further improved upon this to -27% deviation. However,\nnote that in most cases fine-tuned BERT still shows the least extreme deviations.\nAdditionally, fine-tuning BERT is significantly less resource-intensive and cheaper than\nfine-tuning ChatGPT which is also restricted to OpenAI’s online platform and currently\ndoes not support modifications to their fine-tuning procedure.\nDemographic Biases. We found that ChatGPT showed significant demographic\nbiases in it’s moral annotations, aligning more so with certain groups, including younger\nover older (-1%,p = .033), female over male ( +286 %,p < .001), and Christian over\nnon-religious (-67%,p < .001). See Table S2 for an overview of demographic biases.\nMoral Biases. We found that ChatGPT showed significant moral biases in it’s\nannotations, aligning more so with human annotators who endorse certain moral values,\nsuch as care ( +31,159%,p < .001) and proportionality (+21,215%,p < .001), while\nmisaligning with the annotators who endorse other values, such as loyalty (-96%,p < .001)\nand equality (-97%,p < .001). See Table S3 for an overview of moral biases.\nPersonality Biases. We found that ChatGPT showed significant personality\nbiases in it’s annotations, aligning more so with human annotators who load highly on\ncertain personality traits, such as openness (+1244%,p < .001) and agreeableness (+156%,\np < .001), while misaligning with annotators who load highly on other personality traits,\nsuch as conscientiousness (-49%,p < .001) and extraversion(-68%,p < .001). See Table S4\nfor an overview of personality biases.\nPERILS AND OPPORTUNITIES OF LLMS 54\nTable S1\nDeviations from ground truth for each model on each prediction class\nClass BERT LIWC ChatGPT (Zero) ChatGPT (FT) ChatGPT (Few)\nCare +12% ( p= .162) -54% (p< .001) +35% (p< .001) 6% (p= .443) -5% (p = .536)\nEquality -11% ( p= .255) -23% (p= .013) +55% (p< .001) +4% (p = .701) -24 (p= .007)\nProportionality -20% (p= .118) -19% (p = 0.136) -55% (p< .001) -86% (p< .001) -48% (p< .001)\nLoyalty -64% ( p< .001) -28% (p= .031) +3% (p < .836) -64% (p< .001) -45 (p< .001)\nAuthority -32% (p = .003) -36% (p= .001) +96% (p< .001) -38% (p< .001) -67% (p< .001)\nPurity -74% ( p< .001) -34% (p = .049) -86% (p< .001) -59% (p< .001) -59% (p< .001)\nThin-Morality+2% (p = .861) -38% (p< .001) +1,240% (p< .001) -70% (p< .001) -31% (p< .001)\nNonmoral +18% (p = .002) +74% (p< .001) -100% (p< .001) +90% (p< .001) -27 (p< .001)\nNote. Table shows the difference in odds to predict a moral sentiment for each classifier (BERT, LIWC,\nChatGPT) compared to the ground truth of trained human annotators. Table shows that BERT deviates\nsignificantly less from ground truth compared to zeroshot-ChatGPT (except on the Loyalty foundation) and\nis comparable to LIWC. For half of the moral sentiments, BERT doesnot deviate significantly (at p < .05)\nfrom ground-truth while zeroshot-ChatGPT deviates significantly from ground-truth for all moral\nsentiments except Loyalty. Table further shows that few-shot (Few) and fine-tuning (FT) improves\nChatGPT performance. However, the deviations remain higher than BERT and in all but three cases (care,\nequality, loyalty). Notably, BERT is less sensitive to falsely moralizing non-moral texts or to overlook moral\nsentiment.\nTable S2\nDemographic Biases of ChatGPT annotations on the MFRC\nDemographic Variable ∆log(odds) p\nFemale (vs. Male) +286 % < .001\nAge (years) -1% .033\nNon-Religious (vs. Christianity) -67% < .001\nNote. Table shows ChatGPT bias towards respective demographics during annotations. That is, ChatGPT\nhad higher (+) or lower (-) odds (%) of aligning with human annotators of said demographic.\nPERILS AND OPPORTUNITIES OF LLMS 55\nTable S3\nMoral Biases of ChatGPT annotations on the MFRC\nMoral Foundation ∆log(odds) p\nCare +31,159% < .001\nEquality -97% < .001\nProportionality +21,215% < .001\nLoyalty -96% < .001\nAuthority +21% .329\nPurity - -\nNote. Table shows ChatGPT bias towards respective moral profiles during annotations. That is, it had\nhigher/lower odds (%) of aligning with human annotators endorsing said moral values. All variables are\n5-point Likert scales.\nTable S4\nPersonality Biases of ChatGPT annotations on the MFRC\nPersonality Variable ∆log(odds) p\nOpenness +1244% < .001\nConscientiousness -49% < .001\nExtraversion -68% < .001\nAgreeableness +156% < .001\nNeuroticism -15% .327\nNote. Table shows ChatGPT bias towards respective demographics during annotations. That is, ChatGPT\nhad higher (+) or lower (-) odds (%) of aligning with human annotators of said personalities. All variables\nare 5-point Likert scales.\nPERILS AND OPPORTUNITIES OF LLMS 56\nSocio-Political Biases. We found that ChatGPT showed significant\nsocio-political biases in it’s annotations, aligning more so with human annotators who have\ncertain political attitudes, such as individualism (+142%,p < .001), while misaligning with\nannotators who have other political attitudes, such as collectivism (-30%,p < .001)\ntightness (-77%,p < .001), social conservatism (-4%,p < .001), and economic conservatism\n(-2%, p < .001). See Table S5 for an overview of political biases.\nTable S5\nPolitical Biases of ChatGPT annotations on the MFRC\nSocio-Political Variable ∆log(odds) p\nCollectivism -30% < .001\nIndividualism +142% < .001\nTightness -77% < .001\nSocial conservatism -4% < .001\nEconomical conservatism -2% < .001\nNote. Table shows ChatGPT bias towards respective demographics during annotations. That is, ChatGPT\nhad higher (+) or lower (-) odds (%) of aligning with human annotators of said political values. All\nvariables are 5-point Likert scales.\nCreativity-related Biases. We found that ChatGPT showed significant\ncreativity-related biases in its annotations, aligning more so with human annotators who\nhave certain traits, such as creative imagination (+1,351%,p < .001), while misaligning\nwith the annotators who have other traits, such as openness (-50%,p = .006) and aesthetic\nsensitivity (-61%,p < .001). See Table S6 for an overview of creativity-related biases.\nMental Health Biases. We found that ChatGPT showed significant mental\nhealth biases in it’s annotations, aligning more so with human annotators who have certain\nmental health traits, such as anxiety (+95%,p < .001) and emotional-volatility +46%,p <\n.001), while misaligning with the annotators who have other mental health traits\nPERILS AND OPPORTUNITIES OF LLMS 57\nTable S6\nCreativity-related Biases of ChatGPT annotations on the MFRC\nVariable ∆log(odds) p\nOpenness -50% .006\nCreative imagination +1,351% < .001\nAesthetic sensitivity -61% < .001\nNote. Table shows ChatGPT bias towards respective demographics during annotations. That is, ChatGPT\nhad higher (+) or lower (-) odds (%) of aligning with human annotators of said creative inclinations. All\nvariables are 5-point Likert scales.\n(depression (-36%,p < .001). See Table S7 for an overview of mental health related biases.\nTable S7\nMental-Health related Biases of ChatGPT annotations on the MFRC\nMental-Health Variable ∆log(odds) p\nAnxiety +95% < .001\nDepression -36% < .001\nEmotional-Volatility +46% < .001\nNote. Table shows ChatGPT bias towards respective demographics during annotations. That is, ChatGPT\nhad higher (+) or lower (-) odds (%) of aligning with human annotators with said mental-health issues. All\nvariables as 5-point Likert scales.\nEfficiency-related Biases. We found that ChatGPT showed significant\nefficiency-related trait biases in it’s annotations, aligning more so with human annotators\nwho have low endorsement of all efficiency-related traits, such as productiveness (-18%,p <\n.001), organization (-62%,p < .001) and responsibility (-52%,p < .001). See Table S8 for\nan overview of efficiency-related biases.\nPERILS AND OPPORTUNITIES OF LLMS 58\nTable S8\nEfficiency-related Biases of ChatGPT annotations on the MFRC\nEfficiency Variable ∆log(odds) p\nProductiveness -18% < .001\nOrganization -62% < .001\nResponsibility -52% < .001\nNote. Table shows ChatGPT bias towards respective demographics during annotations. That is, ChatGPT\nhad higher (+) or lower (-) odds (%) of aligning with human annotators of said efficiency traits. All\nvariables as 5-point Likert scales.\nS2. Survey Responses\nWe collected responses to wide range of psychological questionnaires using ChatGPT\nand compared the responses to human data covering a wide range of demographic groups.\nWe specifically, analyzed to what extent ChatGPT mirrored meaningful human data and\nwhether it was skewed towards or against specific demographics.\nMethods\nData. We conducted our tests on various questionnaires covering a broad range of\npsychologically relevant fields, such as personality, morality, cognition, and social and\npolitical attitudes. Specifically, we used the Big-Five Inventory (BFI;N = 3924; Fossati\net al. (2011)), Need for Closure Survey (N = 315; Webster and Kruglanski (1994b)), Need\nfor Cognition Survey (N = 900; Cacioppo and Petty (1982b)), Right-wing-authoritarianism\nScale (N = 1020; Zakrisson (2005b)), Emphasizing-Systemizing Scale (N = 3141;\nBaron-Cohen et al. (2003b)), and Rational-Experiential Inventory Scale (N = 1456; Pacini\nand Epstein (1999b)). The human response data was collected on\nhttps://www.yourmorals.org and contains the participants’ responses as well as various\ndemographic information about the participants.\nPERILS AND OPPORTUNITIES OF LLMS 59\nChatGPT Responses. We prompted ChatGPT to respond to survey items via\nthe OpenAI API (https://platform.openai.com/docs/api-reference?lang=python). To\naccess the API and to collect the responses we used the OpenAI library (v0.27.8; OpenAI\n(2023)) in Python. We deployed the “gpt-3.5-turbo-0301” model with a temperature of 1\n(to allow for maximal variability in the responses). The prompts mirrored the study\ninstructions given to human respondents and had the general shape of “For a scientific\nstudy, please indicate your level of agreement with the following statement. Use integer\nnumbers from 1 to 5, with 1 meaning strongly disagree, and 5 meaning strongly agree.\nRespond with a single number. The statement is: [Survey item]”\nWe adapted the scale descriptions to the surveys as necessary. We collected 100\nresponses for each item (that is, we repeated the same prompt 100 times) to allow for\nanalyses regarding the variance within ChatGPT responses. See the project repository for\nthe utilized survey items, ChatGPT prompts, and code containing the API calls.\nBias Analyses. First, we compared ChatGPT’s responses to demographic-level\naverage human responses. That is, we investigated the difference between ChatGPT’s\nsurvey responses and the average response of different groups of humans based on their\ndemographics (e.g., age, race, gender). Note, we removed all demographics with less than\n100 samples in the data sets as they did not provide enough information to make inferences\nabout said demographic. We calculated the average ChatGPT response for each construct\nin a given survey and then subtracted it from the group-level average human response on\nsaid construct (e.g., the difference between ChatGPT’s “Openness” score and the average\nmale or female “Openness” score). We then tested these differences for statistical\nsignificance using Dunnett’s test (Dunnett, 1955), which compares each of a number of\ntreatments with a single control, with the demographic groups as treatments and ChatGPT\nas the control, while adjusting for multiple comparisons. We then repeated this analysis for\ndifferences in variance. We investigated whether ChatGPT responses had less variance\ncompared to the different human groups (e.g., the difference in the variance of ChatGPT’s\nPERILS AND OPPORTUNITIES OF LLMS 60\n“Openness” scores vs. the variance in male or female “Openness” scores). We used the\nLevene Test (Levene, 1960) to determine whether the variance in each human group’s\nresponses differed from ChatGPT’s while accounting for multiple comparisons using\nBonferroni correction. Second, we investigated the relationship between the participants’\ndemographic characteristics and the similarity of human to ChatGPT responses. We fit a\nlinear regression model that predicted the absolute deviation of ChatGPT and human\nresponses (aggregated over all survey constructs) as a function of each human participant’s\ndemographic characteristic. These models expressed which participant demographics\nChatGPT is more or less likely to align with and therefore showed its biases (e.g., are\nChatGPT’s responses more similar to older vs. younger participants).\nWe conducted all analyses in R (v4.1.2; R Core Team (2021)) using the stats\n(v4.1.2; R Core Team (2021)), car (v3.0-11; Fox and Weisberg (2019)), and DescTools\n(v0.99.44; Andri et mult. al. (2021)) packages.\nResults\nOverall, we found that ChatGPT responses significantly differed from a wide range\nof human demographics and in some cases produced wildly unnatural data. For example,\nacross questionnaires and psychological constructs, ChatGPT produced significantly less\nvariance than human subjects. On some questionnaires, such as the BFI and Need for\nCognition questionnaires, ChatGPT produced responses significantly deviating from the\nhuman mean (e.g., ChatGPT was unnaturally high on Agreeableness, Conscientiousness\nand low on Neuroticism). Furthermore, ChatGPT was biased towards a range of\ndemographics on most surveys, that is, ChatGPT’s responses were more similar to some\ndemographics than others. See Tables S9 - S24 and Figures S4 - S31 for an overview of\nChatGPT and human differences across the various questionnaires.\nDifferences to political orientations. We found that ChatGPT significantly\ndeviated from responses of various political groups on most surveys. For example,\nPERILS AND OPPORTUNITIES OF LLMS 61\nChatGPT was more agreeable and conscientious than liberals, moderates, and conservatives\nwhen responding to the BFI or had lower Need for Cognition than liberals, moderates, and\nconservatives when responding to the Need for Cognition scale. See Table S9 and Figures\nS4-S9) for on overview of ChatGPT’s deviations from political groups across all surveys.\nTable S9\nDeviation of ChatGPT responses from human responses grouped by political orientation\nSurvey Construct Demographic d p\nBFI Extraversion Liberal 0.017 .978\nBFI Agreeableness Liberal -0.230 < .001\nBFI Conscientiousness Liberal -0.541 < .001\nBFI Neuroticism Liberal 0.780 < .001\nBFI Openness Liberal 0.444 < .001\nNeed for Cognition Need for Cognition Liberal 0.659 < .001\nNeed for Closure Need for Closure Liberal -0.004 .071\nSystemizing and Emphasizing Systemizing Liberal -0.318 < .001\nSystemizing and Emphasizing Emphasizing Liberal 0.201 < .001\nRational-Experiential Inventory Rational Liberal 0.435 < .001\nRational-Experiential Inventory Experiential Liberal 0.123 .166\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism Liberal -0.226 .003\nBFI Extraversion Moderate 0.042 .806\nBFI Agreeableness Moderate -0.284 < .001\nBFI Conscientiousness Moderate -0.416 < .001\nBFI Neuroticism Moderate 0.567 < .001\nBFI Openness Moderate 0.316 < .001\nNeed for Cognition Need for Cognition Moderate 0.544 < .001\nNeed for Closure Need for Closure Moderate - -\nSystemizing and Emphasizing Systemizing Moderate -0.263 < .001\nContinued on next page\nPERILS AND OPPORTUNITIES OF LLMS 62\nTable S9 – continued from previous page\nSurvey Construct Demographic d p\nSystemizing and Emphasizing Emphasizing Moderate 0.09 .091\nRational-Experiential Inventory Rational Moderate 0.344 < .001\nRational-Experiential Inventory Experiential Moderate 0.159 .059\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism Moderate 0.522 < .001\nBFI Extraversion Conservative 0.061 .672\nBFI Agreeableness Conservative -0.403 < .001\nBFI Conscientiousness Conservative -0.378 < .001\nBFI Neuroticism Conservative 0.451 < .001\nBFI Openness Conservative 0.183 .005\nNeed for Cognition Need for Cognition Conservative 0.563 < .001\nNeed for Closure Need for Closure Conservative - -\nSystemizing and Emphasizing Systemizing Conservative -0.187 < .001\nSystemizing and Emphasizing Emphasizing Conservative -0.018 .909\nRational-Experiential Inventory Rational Conservative 0.432 < .001\nRational-Experiential Inventory Experiential Conservative 0.192 .045\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism Conservative 1.621 < .001\nNote: Positive values indicate a higher average response for the political group compared to ChatGPT.\nSignificance determined via Dunnett’s Test.\nPERILS AND OPPORTUNITIES OF LLMS 63\nFigure S4\nComparing ChatGPT against Humans grouped by political opinion for responses on the Big Five\npersonality questionnaire\n1\n2\n3\n4\n5\nExtraversion Agreeableness Conscientiousness Neuroticism Openness\nConstruct Score\nGPT\nConservative\nLiberal\nModerate\nFigure S5\nComparing ChatGPT against Humans grouped by political opinion for responses on the Need for Cognition\nquestionnaire\n1\n2\n3\n4\n5\nNFC\nConstruct Score\nGPT\nConservative\nLiberal\nModerate\nPERILS AND OPPORTUNITIES OF LLMS 64\nFigure S6\nComparing ChatGPT against Humans grouped by political opinion for responses on the Need for Closure\nquestionnaire\n1\n2\n3\n4\n5\nnfcc\nConstruct Score\nGPT\nLiberal\nFigure S7\nComparing ChatGPT against Humans grouped by political opinion for responses on the\nSystematizing-Emphasizing scale\n1\n2\n3\n4\nSystemizing Emphasizing\nConstruct Score\nGPT\nConservative\nLiberal\nModerate\nPERILS AND OPPORTUNITIES OF LLMS 65\nFigure S8\nComparing ChatGPT against Humans grouped by political opinion for responses on the\nRational-Experiential Inventory questionnaire\n1\n2\n3\n4\n5\nRational Experiential\nConstruct Score\nGPT\nConservative\nLiberal\nModerate\nFigure S9\nComparing ChatGPT against Humans grouped by political opinion for responses on the\nRight-Wing-Authoritarianism questionnaire\n2\n4\n6\nRWA\nConstruct Score\nGPT\nConservative\nLiberal\nModerate\nPERILS AND OPPORTUNITIES OF LLMS 66\nDifferences to sexes. We found that ChatGPT significantly deviated from responses\nof both sexes on most surveys. For example, ChatGPT gave responses that are less neurotic and\nopen-minded while more agreeable and conscientious than both Males and Females on the BFI or\nhad lower right-wing-authoritarianism scores than both Males and Females. See Table S11 and\nFigures S11-S15 for on overview of ChatGPT’s deviations from Males and Females across all\nsurveys.\nFigure S10\nComparing ChatGPT against Humans grouped by Sex for responses on the Big Five personality\nquestionnaire\n1\n2\n3\n4\n5\nExtraversion Agreeableness Conscientiousness Neuroticism Openness\nConstruct Score\nGPT\nFemale\nMale\nPERILS AND OPPORTUNITIES OF LLMS 67\nTable S11\nDeviation of ChatGPT responses from human responses grouped by sex\nSurvey Construct Demographic d p\nBFI Extraversion Female 0.102 .277\nBFI Agreeableness Female -0.206 .002\nBFI Conscientiousness Female -0.405 < .001\nBFI Neuroticism Female 0.886 < .001\nBFI Openness Female 0.311 < .001\nNeed for Cognition Need for Cognition Female 0.693 < .001\nNeed for Closure Need for Closure Female 0.124 .757\nSystemizing and Emphasizing Systemizing Female -0.494 < .001\nSystemizing and Emphasizing Emphasizing Female 0.244 < .001\nRational-Experiential Inventory Rational Female 0.277 < .001\nRational-Experiential Inventory Experiential Female 0.302 < .001\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism Female 0.157 .188\nBFI Extraversion Male -0.043 .692\nBFI Agreeableness Male -0.384 < .001\nBFI Conscientiousness Male -0.532 < .001\nBFI Neuroticism Male 0.435 < .001\nBFI Openness Male 0.416 < .001\nNeed for Cognition Need for Cognition Male 0.531 < .001\nNeed for Closure Need for Closure Male 0.050 .260\nSystemizing and Emphasizing Systemizing Male 0.105 .020\nSystemizing and Emphasizing Emphasizing Male -0.004 .985\nRational-Experiential Inventory Rational Male 0.502 < .001\nRational-Experiential Inventory Experiential Male 0.049 .601\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism Male 0.441 < .001\nNote: Positive values indicate a higher average response for the sex category compared to ChatGPT.\nSignificance determined via Dunnett’s Test.\nPERILS AND OPPORTUNITIES OF LLMS 68\nFigure S11\nComparing ChatGPT against Humans grouped by sex for responses on the Need for Cognition questionnaire\n1\n2\n3\n4\n5\nNFC\nConstruct Score\nGPT\nFemale\nMale\nFigure S12\nComparing ChatGPT against Humans grouped by sex for responses on the Need for Closure questionnaire\n1\n2\n3\n4\n5\n6\nnfcc\nConstruct Score\nGPT\nFemale\nMale\nPERILS AND OPPORTUNITIES OF LLMS 69\nFigure S13\nComparing ChatGPT against Humans grouped by sex for responses on the Systematizing and Emphasizing\nquestionnaire\n1\n2\n3\n4\nSystemizing Emphasizing\nConstruct Score\nGPT\nFemale\nMale\nFigure S14\nComparing ChatGPT against Humans grouped by sex for responses on the Rational-Experiential Inventory\nquestionnaire\n1\n2\n3\n4\n5\nRational Experiential\nConstruct Score\nGPT\nFemale\nMale\nPERILS AND OPPORTUNITIES OF LLMS 70\nFigure S15\nComparing ChatGPT against Humans grouped by sex for responses on the Right-Wing-Authoritarianism\nquestionnaire\n0\n2\n4\n6\nRWA\nConstruct Score\nGPT\nFemale\nMale\nPERILS AND OPPORTUNITIES OF LLMS 71\nDifferences to racial groups. We found that ChatGPT significantly deviated from\nresponses of various racial groups on most surveys. For example, ChatGPT had lower experiential\nscores than White but not Asian participants or less neurotic and open-minded while more\nagreeable and conscientious than all races. See Table S12 and Figures S16-S21 for on overview of\nChatGPT’s deviations from racial groups across all surveys.\nTable S12\nDeviation of ChatGPT responses from human responses grouped by race\nSurvey Construct Demographic d p\nBFI Extraversion Asian -0.078 .717\nBFI Agreeableness Asian -0.422 < .001\nBFI Conscientiousness Asian -0.623 < .001\nBFI Neuroticism Asian 0.645 < .001\nBFI Openness Asian 0.263 < .001\nNeed for Cognition Need for Cognition Asian - -\nNeed for Closure Need for Closure Asian - -\nSystemizing and Emphasizing Systemizing Asian -0.259 < .001\nSystemizing and Emphasizing Emphasizing Asian 0.042 .718\nRational-Experiential Inventory Rational Asian 0.263 < .001\nRational-Experiential Inventory Experiential Asian 0.092 .413\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism Asian - -\nBFI Extraversion White 0.039 .940\nBFI Agreeableness White -0.287 < .001\nBFI Conscientiousness White -0.448 < .001\nBFI Neuroticism White 0.630 < .001\nBFI Openness White 0.396 < .001\nNeed for Cognition Need for Cognition White 0.668 < .001\nNeed for Closure Need for Closure White - -\nSystemizing and Emphasizing Systemizing White -0.251 < .001\nContinued on next page\nPERILS AND OPPORTUNITIES OF LLMS 72\nTable S12 – continued from previous page\nSurvey Construct Demographic d p\nSystemizing and Emphasizing Emphasizing White 0.100 .074\nRational-Experiential Inventory Rational White 0.460 < .001\nRational-Experiential Inventory Experiential White 0.167 .027\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism White 0.347 < .001\nBFI Extraversion Hispanic 0.013 .996\nBFI Agreeableness Hispanic -0.370 < .001\nBFI Conscientiousness Hispanic -0.511 < .001\nBFI Neuroticism Hispanic 0.742 < .001\nBFI Openness Hispanic 0.274 < .001\nNeed for Cognition Need for Cognition Hispanic - -\nNeed for Closure Need for Closure Hispanic - -\nSystemizing and Emphasizing Systemizing Hispanic -0.340 < .001\nSystemizing and Emphasizing Emphasizing Hispanic 0.075 .376\nRational-Experiential Inventory Rational Hispanic - -\nRational-Experiential Inventory Experiential Hispanic - -\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism Hispanic - -\nBFI Extraversion Black -0.024 .996\nBFI Agreeableness Black -0.212 .029\nBFI Conscientiousness Black -0.456 < .001\nBFI Neuroticism Black 0.555 < .001\nBFI Openness Black 0.241 .004\nNeed for Cognition Need for Cognition Black - -\nNeed for Closure Need for Closure Black - -\nSystemizing and Emphasizing Systemizing Black - -\nSystemizing and Emphasizing Emphasizing Black - -\nRational-Experiential Inventory Rational Black - -\nContinued on next page\nPERILS AND OPPORTUNITIES OF LLMS 73\nTable S12 – continued from previous page\nSurvey Construct Demographic d p\nRational-Experiential Inventory Experiential Black - -\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism Black - -\nNote: Positive values indicate a higher average response for racial category compared to ChatGPT.\nSignificance determined via Dunnett’s Test.\nFigure S16\nComparing ChatGPT against Humans grouped by race for responses on the Big Five personality\nquestionnaire\n1\n2\n3\n4\n5\n6\nExtraversion Agreeableness Conscientiousness Neuroticism Openness\nConstruct Score\nGPT\nAsian\nBlack\nHispanic\nWhite\nPERILS AND OPPORTUNITIES OF LLMS 74\nFigure S17\nComparing ChatGPT against Humans grouped by race for responses on the Need for Cognition\nquestionnaire\n2\n3\n4\n5\nNFC\nConstruct Score\nGPT\nWhite\nFigure S18\nComparing ChatGPT against Humans grouped by race for responses on the Need for Closure questionnaire\n1\n2\n3\n4\n5\nnfcc\nConstruct Score\nGPT\nWhite\nPERILS AND OPPORTUNITIES OF LLMS 75\nFigure S19\nComparing ChatGPT against Humans grouped by race for responses on the Systematizing-Emphasizing\nscale\n1\n2\n3\n4\nSystemizing Emphasizing\nConstruct Score\nGPT\nAsian\nHispanic\nWhite\nFigure S20\nComparing ChatGPT against Humans grouped by race for responses on the Rational-Experiential Inventory\nquestionnaire\n1\n2\n3\n4\n5\nRational Experiential\nConstruct Score\nGPT\nAsian\nWhite\nPERILS AND OPPORTUNITIES OF LLMS 76\nFigure S21\nComparing ChatGPT against Humans grouped by race for responses on the Right-Wing-Authoritarianism\nquestionnaire\n0\n2\n4\n6\nRWA\nConstruct Score\nGPT\nWhite\nPERILS AND OPPORTUNITIES OF LLMS 77\nDifferences to age groups. We found that ChatGPT significantly deviated from\nresponses of various age groups on most surveys. For example, ChatGPT had significantly lower\nexperiential scores for 18-24 and 35-54 but not for the other age groups or had less neuroticism\nand open-mindedness while more agreeableness and conscientiousness than all age groups. See\nTable S14 and Figures S22-S26 for on overview of ChatGPT’s deviations from age groups across\nall surveys.\nTable S14\nDeviation of ChatGPT responses from human responses grouped by age\nSurvey Construct Demographic d p\nBFI Extraversion 55-74 0.109 .381\nBFI Agreeableness 55-74 -0.180 .021\nBFI Conscientiousness 55-74 -0.217 .009\nBFI Neuroticism 55-74 0.398 < .001\nBFI Openness 55-74 0.563 < .001\nNeed for Cognition Need for Cognition 55-74 0.614 < .001\nNeed for Closure Need for Closure 55-74 - -\nSystemizing and Emphasizing Systemizing 55-74 -0.199 < .001\nSystemizing and Emphasizing Emphasizing 55-74 0.199 < .001\nRational-Experiential Inventory Rational 55-74 0.452 < .001\nRational-Experiential Inventory Experiential 55-74 0.076 .650\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism 55-74 0.151 .445\nBFI Extraversion 35-54 0.045 .840\nBFI Agreeableness 35-54 -0.263 < .001\nBFI Conscientiousness 35-54 -0.364 < .001\nBFI Neuroticism 35-54 0.511 < .001\nBFI Openness 35-54 0.441 < .001\nNeed for Cognition Need for Cognition 35-54 0.691 < .001\nNeed for Closure Need for Closure 35-54 - -\nContinued on next page\nPERILS AND OPPORTUNITIES OF LLMS 78\nTable S14 – continued from previous page\nSurvey Construct Demographic d p\nSystemizing and Emphasizing Systemizing 35-54 -0.180 < .001\nSystemizing and Emphasizing Emphasizing 35-54 0.125 .022\nRational-Experiential Inventory Rational 35-54 0.505 < .001\nRational-Experiential Inventory Experiential 35-54 0.178 .042\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism 35-54 0.314 .009\nBFI Extraversion 25-34 -0.040 .884\nBFI Agreeableness 25-34 -0.349 < .001\nBFI Conscientiousness 25-34 -0.516 < .001\nBFI Neuroticism 25-34 0.636 < .001\nBFI Openness 25-34 0.380 < .001\nNeed for Cognition Need for Cognition 25-34 0.648 < .001\nNeed for Closure Need for Closure 25-34 - -\nSystemizing and Emphasizing Systemizing 25-34 -0.278 < .001\nSystemizing and Emphasizing Emphasizing 25-34 0.079 .188\nRational-Experiential Inventory Rational 25-34 0.448 < .001\nRational-Experiential Inventory Experiential 25-34 0.123 .212\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism 25-34 0.397 < .001\nBFI Extraversion 18-24 0.009 .999\nBFI Agreeableness 18-24 -0.334 < .001\nBFI Conscientiousness 18-24 -0.621 < .001\nBFI Neuroticism 18-24 0.815 < .001\nBFI Openness 18-24 0.183 .005\nNeed for Cognition Need for Cognition 18-24 0.465 < .001\nNeed for Closure Need for Closure 18-24 - -\nSystemizing and Emphasizing Systemizing 18-24 -0.393 < .001\nSystemizing and Emphasizing Emphasizing 18-24 0.013 .984\nContinued on next page\nPERILS AND OPPORTUNITIES OF LLMS 79\nTable S14 – continued from previous page\nSurvey Construct Demographic d p\nRational-Experiential Inventory Rational 18-24 0.236 < .001\nRational-Experiential Inventory Experiential 18-24 0.197 .023\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism 18-24 0.493 < .001\nNote: Positive values indicate a higher average response for demographic groups compared to ChatGPT.\nSignificance determined via Dunnett’s Test.\nFigure S22\nComparing ChatGPT against Humans grouped by age for responses on the Big Five personality\nquestionnaire\n1\n2\n3\n4\n5\nExtraversion Agreeableness Conscientiousness Neuroticism Openness\nConstruct Score\nGPT\n18−24\n25−34\n35−54\n55−74\nPERILS AND OPPORTUNITIES OF LLMS 80\nFigure S23\nComparing ChatGPT against Humans grouped by age for responses on the Need for Cognition questionnaire\n1\n2\n3\n4\n5\nNFC\nConstruct Score\nGPT\n18−24\n25−34\n35−54\n55−74\nFigure S24\nComparing ChatGPT against Humans grouped by age for responses on the Systematizing-Emphasizing scale\n1\n2\n3\n4\nSystemizing Emphasizing\nConstruct Score\nGPT\n18−24\n25−34\n35−54\n55−74\nPERILS AND OPPORTUNITIES OF LLMS 81\nFigure S25\nComparing ChatGPT against Humans grouped by age for responses on the Rational-Experiential Inventory\nquestionnaire\n1\n2\n3\n4\n5\nRational Experiential\nConstruct Score\nGPT\n18−24\n25−34\n35−54\n55−74\nFigure S26\nComparing ChatGPT against Humans grouped by age for responses on the Right-Wing-Authoritarianism\nquestionnaire\n0\n2\n4\n6\nRWA\nConstruct Score\nGPT\n18−24\n25−34\n35−54\n55−74\nPERILS AND OPPORTUNITIES OF LLMS 82\nDifferences to religious groups. We found that ChatGPT significantly deviated\nfrom the responses of various religious groups on most surveys. For example, ChatGPT had lower\nright-wing-authoritarianism than Christians but higher than Agnostics and Atheists or less\nneuroticism and open-mindedness while more agreeableness and conscientiousness than all\nreligious groups. See Table S26 and Figures S27-S31 for on overview of ChatGPT’s deviations\nfrom religious groups across all surveys.\nTable S16\nDeviation of ChatGPT responses from human responses grouped by religion\nSurvey Construct Demographic d p\nBFI Extraversion Christianity 0.103 .333\nBFI Agreeableness Christianity -0.199 .004\nBFI Conscientiousness Christianity -0.381 < .001\nBFI Neuroticism Christianity 0.589 < .001\nBFI Openness Christianity 0.271 < .001\nNeed for Cognition Need for Cognition Christianity 0.524 < .001\nNeed for Closure Need for Closure Christianity - -\nSystemizing and Emphasizing Systemizing Christianity -0.294 < .001\nSystemizing and Emphasizing Emphasizing Christianity 0.113 .035\nRational-Experiential Inventory Rational Christianity 0.347 < .001\nRational-Experiential Inventory Experiential Christianity 0.198 .014\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism Christianity 1.008 < .001\nBFI Extraversion Atheist -0.107 .315\nBFI Agreeableness Atheist -0.414 < .001\nBFI Conscientiousness Atheist -0.532 < .001\nBFI Neuroticism Atheist 0.640 < .001\nBFI Openness Atheist 0.427 < .001\nNeed for Cognition Need for Cognition Atheist 0.678 < .001\nNeed for Closure Need for Closure Atheist - -\nContinued on next page\nPERILS AND OPPORTUNITIES OF LLMS 83\nTable S16 – continued from previous page\nSurvey Construct Demographic d p\nSystemizing and Emphasizing Systemizing Atheist -0.215 < .001\nSystemizing and Emphasizing Emphasizing Atheist 0.039 .581\nRational-Experiential Inventory Rational Atheist 0.484 < .001\nRational-Experiential Inventory Experiential Atheist -0.043 .805\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism Atheist -0.095 .507\nBFI Extraversion Agnostic -0.013 .989\nBFI Agreeableness Agnostic -0.356 < .001\nBFI Conscientiousness Agnostic -0.560 < .001\nBFI Neuroticism Agnostic 0.716 < .001\nBFI Openness Agnostic 0.427 < .001\nNeed for Cognition Need for Cognition Agnostic 0.675 < .001\nNeed for Closure Need for Closure Agnostic - -\nSystemizing and Emphasizing Systemizing Agnostic -0.268 < .001\nSystemizing and Emphasizing Emphasizing Agnostic 0.121 .022\nRational-Experiential Inventory Rational Agnostic 0.416 < .001\nRational-Experiential Inventory Experiential Agnostic 0.217 .006\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism Agnostic 0.135 .263\nNote: Positive values indicate a higher average response for demographic groups compared to ChatGPT.\nSignificance determined via Dunnett’s Test.\nPERILS AND OPPORTUNITIES OF LLMS 84\nFigure S27\nComparing ChatGPT against Humans grouped by Religion for responses on the Big Five personality\nquestionnaire\n1\n2\n3\n4\n5\nExtraversion Agreeableness Conscientiousness Neuroticism Openness\nConstruct Score\nGPT\nAgnostic\nAtheist\nChristianity\nFigure S28\nComparing ChatGPT against Humans grouped by Religion for responses on the Need for Cognition\nquestionnaire\n1\n2\n3\n4\n5\nNFC\nConstruct Score\nGPT\nAgnostic\nAtheist\nChristianity\nPERILS AND OPPORTUNITIES OF LLMS 85\nFigure S29\nComparing ChatGPT against Humans grouped by Religion for responses on the Systematizing-Emphasizing\nscale\n1\n2\n3\n4\nSystemizing Emphasizing\nConstruct Score\nGPT\nAgnostic\nAtheist\nChristianity\nFigure S30\nComparing ChatGPT against Humans grouped by Religion for responses on the Rational-Experiential\nInventory questionnaire\n1\n2\n3\n4\n5\nRational Experiential\nConstruct Score\nGPT\nAgnostic\nAtheist\nChristianity\nPERILS AND OPPORTUNITIES OF LLMS 86\nFigure S31\nComparing ChatGPT against Humans grouped by Religion for responses on the\nRight-Wing-Authoritarianism questionnaire\n0\n2\n4\n6\nRWA\nConstruct Score\nGPT\nAgnostic\nAtheist\nChristianity\nPERILS AND OPPORTUNITIES OF LLMS 87\nVariance Differences. ChatGPT had significantly less variance in its responses to\nevery survey in comparison to human responses. See Table S18 for an overview of response\nvariance across all surveys.\nTable S18\nOverview of variance differences between ChatGPT and human responses\nSurvey Construct d p\nBFI Extraversion 0.68 < .001\nBFI Agreeableness 0.51 < .001\nBFI Conscientiousness 0.59 < .001\nBFI Neuroticism 0.69 < .001\nBFI Openness 0.45 < .001\nNeed for Cognition Need for Cognition 0.47 < .001\nNeed for Closure Need for Closure 0.62 < .001\nSystemizing and Emphasizing Systemizing 0.38 < .001\nSystemizing and Emphasizing Emphasizing 0.38 < .001\nRational-Experiential Inventory Rational 0.45 < .001\nRational-Experiential Inventory Experiential 0.61 < .001\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism 0.82 < .001\nNote. Positive values indicate higher variance for humans compared to ChatGPT. Significance determined\nvia Levene-Test. Table shows that this way of prompting is unable to generate human-like variance. See\nfurther discussion of meaningfulness of ChatGPT response variance in the main text.\nPERILS AND OPPORTUNITIES OF LLMS 88\nBiases on the BFI. ChatGPT showed significant biases in its BFI responses. In\nparticular, ChatGPT was significantly more similar to females then males, Muslims than\nChristians, and Christians compared to Jews, Agnostics, and Atheists. See Table S19 for an\noverview of biases in ChatGPT’s BFI responses.\nTable S19\nOverview of significant ChatGPT biases for the BFI\nBias Group Reference Group b p\nSex Male Female 0.220 < .001\nReligion Islam Christianity -0.379 .040\nReligion Judaism Christianity 0.369 .010\nReligion Agnostic Christianity 0.139 .015\nReligion Atheist Christianity 0.172 .005\nNote. Positive values mean that ChatGPT is biased against a demographic (larger difference to reference\ngroup), and negative values mean that ChatGPT is biased toward a demographic (smaller difference). In\ntotal, we tested prediction biases for sex, age, political orientation, religion, and race.\nPERILS AND OPPORTUNITIES OF LLMS 89\nRWA ChatGPT biases. ChatGPT showed significant biases in its RWA responses.\nIn particular, ChatGPT was significantly biased against younger people, Conservatives, and\nChristians compared to their demographic counterparts. See Table S20 for an overview of RWA\nChatGPT biases.\nTable S20\nOverview of significant ChatGPT biases for the RWA\nBias Group Reference Group b p\nAge - 0 years -0.004 < .001\nPolitical Orientation Liberal Conservative -1.031 < .001\nPolitical Orientation Moderate Conservative -0.876 < .001\nReligion Agnostic Christianity -0.244 < .001\nReligion Atheist Christianity -0.223 < .001\nReligion Other Religion Christianity -0.185 .018\nNote. Positive values mean that ChatGPT predictions are biased against said demographic (larger\ndifference to reference group), and negative values mean that ChatGPT is biased toward said demographic\n(smaller difference). We tested prediction biases for sex, age, political orientation, religion, and race.\nPERILS AND OPPORTUNITIES OF LLMS 90\nBiases on the Emphasizing-Systematizing Scale.ChatGPT showed significant\nbiases in its Emphasizing-Systematizing scale responses. In particular, ChatGPT was significantly\nmore similar to older people, males, Conservatives, and Christians compared to their demographic\ncounterparts. See Table S21 for an overview of Emphasizing-Systematizing ChatGPT biases.\nTable S21\nOverview of significant ChatGPT biases for the Emphasizing-Systemizing Survey\nBias Group Reference Group b p\nAge - 0 years -0.002 < .001\nSex Male Female -0.151 < .001\nPolitical Orientation Liberal Conservative 0.054 .035\nReligion Judaism Christianity 0.124 .008\nNote. Positive values mean that ChatGPT predictions are biased against said demographic (larger\ndifference to reference group), and negative values mean that ChatGPT is biased toward said demographic\n(smaller difference). We tested prediction biases for sex, age, political orientation, religion, and race.\nBiases on the Need for Cognition Scale.ChatGPT showed significant biases in\nits Need for Cognition scale responses. In particular, ChatGPT was significantly more similar to\nfemales, Christians, and Asians compared to their demographic counterparts. See Table S22 for\nan overview of Need for Cognition ChatGPT biases.\nPERILS AND OPPORTUNITIES OF LLMS 91\nTable S22\nOverview of significant ChatGPT biases for the Need for Cognition\nBias Group Reference Group b p\nSex Male Female 0.083 .005\nReligion Atheist Christianity 0.097 .014\nRace Asian White -0.209 .002\nNote. Positive values mean that ChatGPT predictions are biased against said demographic (larger\ndifference to reference group), and negative values mean that ChatGPT is biased toward said demographic\n(smaller difference). We tested prediction biases for sex, age, political orientation, religion, and race.\nPERILS AND OPPORTUNITIES OF LLMS 92\nBiases on the Need for Closure Scale.ChatGPT showed significant biases in its\nNeed for Closure scale responses. In particular, ChatGPT was significantly more similar to\nChristians and White people compared to their demographic counterparts. See Table S23 for an\noverview of Need for Closure ChatGPT biases.\nTable S23\nOverview of significant ChatGPT biases for the Need for Closure\nBias Group Reference Group b p\nReligion Islam Christianity 1.005 .003\nReligion Atheist Christianity 0.153 .039\nReligion Other Religion Christianity 0.229 .037\nRace Asian White 0.242 .023\nNote. Positive values mean that ChatGPT predictions are biased against said demographic (larger\ndifference to reference group), and negative values mean that ChatGPT is biased toward said demographic\n(smaller difference). We tested prediction biases for sex, age, political orientation, religion, and race.\nPERILS AND OPPORTUNITIES OF LLMS 93\nBiases on the Rational-Experiential Scale.ChatGPT showed significant biases\nin its Rational-Experiential Inventory responses. In particular, ChatGPT was significantly more\nsimilar to younger people, females, Moderates, and Christians compared to their demographic\ncounterparts. See Table S24 for an overview of Rational-Experiential Inventory ChatGPT biases.\nTable S24\nOverview of significant ChatGPT biases for the Rational-Experiential Inventory\nBias Group Reference Group b p\nAge - 0 years 0.004 < .001\nSex Male Female 0.190 < .001\nPolitical Orientation Moderate Conservative -0.144 .020\nReligion Hinduism Christianity 0.405 .048\nReligion Atheist Christianity 0.215 < .001\nReligion Other Religion Christianity 0.183 .019\nNote. Positive values mean that ChatGPT predictions are biased against said demographic (larger\ndifference to reference group), and negative values mean that ChatGPT is biased toward said demographic\n(smaller difference). We tested prediction biases for sex, age, political orientation, religion, and race.\nPERILS AND OPPORTUNITIES OF LLMS 94\nS3. Prompt Sensitivity\nWe investigated ChatGPT’s sensitivity to changes in prompts when generating\npsychologically relevant outputs. To that end, we repeated the previous two studies on moral\nsentiment analysis and survey response collection using ChatGPT while modifying the prompts.\nMethods\nData. To test ChatGPT’s sensitivity to prompts during sentiment analysis, we utilized\nthe same randomly selected subset of the MFRC (Trager et al., 2022b) as in the original study\n(2,983 samples). To test ChatGPT’s sensitivity to prompts when responding to psychological\nquestionnaires, we utilized the BFI (Fossati et al., 2011) questionnaire. We make all data, study\nmaterials, and analysis code available at https://osf.io/nafzy/. The repository contains all the\nnecessary instructions to replicate our analyses.\nText Analysis. We prompted ChatGPT to annotate posts from the MFRC test data\nvia the OpenAI API (https://platform.openai.com/docs/api-reference?lang=python). To access\nthe API and collect ChatGPT’s text annotations, we used the OpenAI library (v0.27.8; OpenAI\n(2023)) in Python. We deployed the “gpt-3.5-turbo-0301” model with a temperature of 0 (for\nmaximal deterministic and thus replicable behavior). In the original study, we prompted\nChatGPT in a way analogous to how human annotators are instructed. That is, we described the\ntask of identifying moral sentiment in a text and gave it the definitions of each type of moral\nsentiment. To test ChatGPT’s sensitivity to prompt changes, we modified the prompt\nsystematically, by randomly changing the order in which the definitions of the moral foundations\nare given to the model. Importantly, we did not change the amount or type of information given\nto ChatGPT. We also kept sentence structure and word choice constant. The following prompt\nwas applied: “Determine which moral sentiments are expressed in the following text. “purity if\nthe text is about avoiding bodily and spiritual contamination and degradation, “equality if the\ntext is about equal treatment and equal outcome for individuals, “authority if the text is about\ndeference toward legitimate authorities and the defense of traditions, all of which are seen as\nproviding stability and fending off chaos, “thin morality if the text has a moral sentiment but\ncannot be categorized as either of the above, “loyalty if the text is about cooperating with\nPERILS AND OPPORTUNITIES OF LLMS 95\ningroups and competing with outgroups, “proportionality if the text is about individuals getting\nrewarded in proportion to their merit or contribution, “non-moral” if no moral sentiment is\nexpressed in the text, “care if the text is about avoiding emotional and physical damage to\nanother individual. Respond only with these words. Respond with any of the categories that\napply, comma separated. Here is the text: [Text to annotate]”\nSurvey responses. We prompted ChatGPT to respond to survey items via the\nOpenAI API (https://platform.openai.com/docs/api-reference?lang=python). To access the API\nand to collect the responses we used the OpenAI library (v0.27.8; OpenAI (2023)) in Python. We\ndeployed the “gpt-3.5-turbo-0301” model with a temperature of 1 (to allow for maximal variability\nin the responses). The prompts mirrored the study instructions given to human respondents and\nhad the general shape of “For a scientific study, please indicate your level of agreement with the\nfollowing statement. Use integer numbers from 1 to 5, with 1 meaning strongly disagree and 5\nmeaning strongly agree. Respond with a single number. The statement is: [Survey item]”\nBased on Schwarz (1999), who showed that minor changes in self-report design such as\ncontextually relevant information or even simply the number values of the response scale can\nchange response patterns, we developed two alternative prompts. The first prompt added a study\nintroduction, similar to what human participants may see in a study. The rationale was to give\nthe model context information when responding to the items (e.g., that this is for a study, that\nthese items are measuring personality, etc.). The prompt was as follows: “For a scientific study,\nwe are collecting responses to various psychological questionnaires. In the following, you will be\nshown statements from a personality test. Please indicate your level of agreement with each\nstatement. There are no right or wrong answers. Use numbers from 1 to 5, with 1 meaning\nstrongly disagree and 5 meaning strongly agree. Respond with a single number. The statement is:\n[Survey item]”\nThe second prompt changed the response scale numerical values from 1–5 to -2 – 2. The\ntotal range of the scale, center, and meaning of each point remained the same. We were interested\nin whether a simple change in the numbers presented, with no changes in meaning and\ninformation content, would lead to a change in outputs. The prompt was as follows: “For a\nscientific study, please indicate your level of agreement with the following statement. Use integer\nPERILS AND OPPORTUNITIES OF LLMS 96\nnumbers from -2 to 2, with -2 meaning strongly disagree and 2 meaning strongly agree. Respond\nwith a single number. The statement is: [Survey item]”\nWe then created an additional prompt that had only surface-level semantic changes.\nKeeping the sentence structure and meaning constant, we exchanged words with synonyms or\nanalogous idioms. The rationale was to test if ChatGPT’s output was responsive to minor,\nmeaningless changes in the instructions. The prompt was as follows: “For an ongoing research\nproject, please express how much you agree with the statement shown at the end. Respond with\nan integer between 1 and 5, with 1 indicating a strong disagreement and 5 indicating a strong\nagreement. Respond only with a single digit. Here’s the statement: [Survey item]”\nWe collected 100 responses for each item. That is, we prompted ChatGPT to respond to\nthe same question 100 times to allow for analyses regarding the variance within ChatGPT\nresponses. See the project repository for the utilized survey items, ChatGPT prompts, and code\ncontaining the API calls.\nSensitivity Analysis. We tested whether changes in a prompt lead to significant\nchanges in the generated outputs. For the sentiment analysis task, we tested whether the\nmodified prompt led to changes in sentiment classification across all types of moral sentiment.\nSpecifically, we tested whether each of the moral sentiments is more or less likely to be classified\nas present in a text given the respective prompt. We fit a logistic regression model predicting\nwhether a moral sentiment was classified as present in a text (binary: yes/no) as a function of the\nprompt (original vs. modified). The coefficients of this model expressed how much more or less\nlikely a moral sentiment was for the modified prompt. A significant coefficient, thus, expressed a\nsignificant change in output in response to the prompt changes.\nFor the survey responses, we tested whether the different prompts led to significant\ndifferences in the survey responses. Specifically, we tested across all survey constructs (e.g.,\nOpenness, Conscientiousness, Extraversion, Agreeableness, Neuroticism) whether the prompts led\nto different construct scores compared to the original prompt. We tested these differences for\nstatistical significance using Dunnett’s test (Dunnett, 1955), which compared each of the\nalternative prompts (ALT1: adding context information, ALT2: changing the scale values, ALT3:\nrephrasing using synonyms) against the original prompt, while accounting for multiple\nPERILS AND OPPORTUNITIES OF LLMS 97\ncomparisons. Finally, we tested whether the prompt changes also impacted the variance in survey\nresponses. We were particularly interested in whether prompt changes would lead to increased\nvariance, indicating more diversity in responses.\nWe conducted all statistical analyses in R (v4.1.2; R Core Team (2021)) using the stats\n(v4.1.2; R Core Team (2021)) package (R Core Team, 2021). See the code repository for\ninstructions to replicate the statistical analyses.\nResults\nPrompt Sensitivity in Text Annotations.We found that changing the prompt\norder did not meaningfully change annotation accuracy (F1 = 0.23 vs F1 = 0.22), but lead to\nsignificant changes in the classification of each texts’ moral sentiment. Using the modified prompt,\nChatGPT was significantly less likely to annotate the care foundation (-56%,p < .001), the\nequality foundation (-62%,p < .001), the loyalty foundation (-33%,p = .009), and significantly\nmore likely to annotate the authority foundation (+71%,p < .001), purity foundation (+164%,\np = .02), or thin-morality (+54%,p < .001). Table S25 shows all differences in the likelihood of\nclassifying a text as containing a moral sentiment for the modified vs. original prompt.\nPERILS AND OPPORTUNITIES OF LLMS 98\nTable S25\nDeviation of modified vs. original prompt in classifying prediction class\nClass ∆log odss p value\nCare -56% < .001\nEquality -62% < .001\nProportionality -31% .09\nLoyalty -33% .01\nAuthority +71% < .001\nPurity +164% .02\nThin-Morality +54% < .001\nNon-moral -100% .95\nNote. Table shows the difference in odds for ChatGPT to predict a class using the modified prompt\ncompared to the original prompt. The table shows that the prompt changes led to significant differences in\nclassifying moral sentiment.\nPERILS AND OPPORTUNITIES OF LLMS 99\nPrompt Sensitivity in Survey Responses.We found that changes to the\nresponse scale values, adding additional contextual information in the form of an introduction,\nand surface-level semantic changes led to significant changes in ChatGPT responses to surveys.\nTable S26 shows the differences across constructs of the BFI for each of the modified prompts\ncompared to the original prompt. However, there were no changes in variance in the responses for\nany of the prompts (Bonferroni correctedp of .0025) as shown in Table S27, again highlighting\nthat the issues with producing meaningful response variance might be a characteristic of LLMs\nrobust to these simple prompting strategies.\nFigure S32\nDifferences in responses across prompts with surface-level differences\n2\n3\n4\nExtraversion AgreeablenessConscientiousnessNeuroticism Openness\nConstruct Score\nOriginal Prompt\nAdded Context\nSemantic Changes\nResponse Rescaling\nNote. Figure shows the effect of surface-level changes to the prompt on ChatGPT’s output. In most cases\nthe changes led to significant changes in output.\nOur results highlight how minor changes in prompt design can lead to significant changes\nin output generated by LLMs for scientific purposes. Notably, we found changes in response\naverages but not in variance, indicating LLMs’ difficulty in generating diverse responses. This is\nrelevant as it goes against the notion that LLMs are an unproblematic off-the-shelf text analysis\ntool or even a replacement for human study participants and instead highlights the need for\nfuture work that develops methods for the robust integration of LLMs in psychology research. Of\nPERILS AND OPPORTUNITIES OF LLMS 100\nTable S26\nDifferences in ChatGPT responses on the BFI for modified vs. original prompt\nConstruct Prompt Modification d p\nExtraversion Adds Introduction 0.54 < .001\nAgreeableness Adds Introduction 0.14 < .001\nConscientiousness Adds Introduction 0.01 .927\nNeuroticism Adds Introduction -0.00 1.000\nOpenness Adds Introduction 0.37 < .001\nExtraversion Synonyms 0.25 < .001\nAgreeableness Synonyms 0.11 < .001\nConscientiousness Synonyms -0.11 < .001\nNeuroticism Synonyms 0.15 < .001\nOpenness Synonyms 0.16 < .001\nExtraversion Response scale value 0.36 < .001\nAgreeableness Response scale value 0.04 .057\nConscientiousness Response scale value -0.01 .986\nNeuroticism Response scale value 0.01 .992\nOpenness Response scale value 0.25 < .001\nNote. Positive values indicate a higher average response when using the original prompt compared to the\nmodified prompts. Significance determined via Dunnett’s Test. Table shows that most prompt modifications\nlead to significant changes in the generated responses.\nPERILS AND OPPORTUNITIES OF LLMS 101\nTable S27\nDifferences in ChatGPT response variance on the BFI for modified vs. original prompt\nConstruct Prompt Modification d p\nExtraversion Adds Introduction 0.03 .033\nAgreeableness Adds Introduction -0.02 .435\nConscientiousness Adds Introduction -0.03 .169\nNeuroticism Adds Introduction -0.01 .646\nOpenness Adds Introduction -0.03 .096\nExtraversion Synonyms 0.05 .010\nAgreeableness Synonyms 0.01 .385\nConscientiousness Synonyms -0.05 .002\nNeuroticism Synonyms -0.03 .022\nOpenness Synonyms -0.00 .359\nExtraversion Response scale value 0.03 .132\nAgreeableness Response scale value -0.01 .389\nConscientiousness Response scale value -0.02 .308\nNeuroticism Response scale value -0.03 .046\nOpenness Response scale value -0.00 .571\nNote. Positive values indicate higher variance in response values when using the original prompt compared\nto the modified prompts. Significance determined via Levene-Test.\nPERILS AND OPPORTUNITIES OF LLMS 102\ncourse, future work can and will investigate how prompting can be used to generate more reliable\nand diverse results. However, researchers should be aware of the increased complexity this means\nfor scientific studies (e.g., needing to validate prompts against alternatives) and the current issues\nthat we have showcased.\nPERILS AND OPPORTUNITIES OF LLMS 103\nS4.Theory-driven vs. Bottom-up Methods\nWe compared ChatGPT as a bottom-up, difficult-to-interpret method against\nContextualized Construct Representation (CCR; Atari, Omrani, & Dehghani, 2023b), a\ntop-down, theory-based, easier-to-interpret method. CCR is a theory-driven text analysis method\nthat combines the benefits of contextual language models (e.g., BERT’s powerful and flexible\nsemantic representations) with the interpretability and validity of common psychometric scales.\nIn essence, CCR imposes top-down constraints on an embedding model (based on comparing\nsemantic similarities of texts to psychometric scale items), thus achieving higher performance\nwhile maintaining interpretability. Our main research question is whether LLMs, specifically\nChatGPT, can at this stage of their development already outperform validated top-down\nmethods. This is relevant because, usually, bottom-up methods are utilized because they are more\npowerful (e.g., higher predictive accuracy) and flexible (e.g., can be used on a wider range of data\nand tasks) than their theory-based counterparts. This increase in performance, however, is\nbought with reduced interpretability (e.g., how do text features lead to a prediction in an LLM?\nWhat text features were used?). Note that while CCR does incorporate BERT-based embeddings\n(which is a language model, albeit significantly smaller and more lightweight compared to\nGPT3.5), it does so with strong theoretical top-down constraints, thus allowing it to provide some\nlevel of interpretability. If LLM-based methods cannot outperform these constrained top-down\nmethods, it should be argued that they are not advanced and reliable enough for widespread\nusage in psychology and related research fields. Of course, this can change in the very near future.\nHowever, researchers should be cautious of using these methods before validating their abilities\nand comparing them to alternative approaches.\nMethods\nData. We replicated the analysis in Atari, Omrani, and Dehghani (2023b) that\nintroduced CCR as a method. In this paper, the authors test CCR’s capabilities by using it to\ninfer participants’ responses to a wide range of questionnaires, including the Portrait Values\nQuestionnaire (PVQ-21; Schwartz (2003)), Moral Foundations Questionnaire (MFQ-2; Atari,\nHaidt, et al. (2023b)), Cultural Tightness-Looseness (Oyserman, 1993), Collectivism (Oyserman,\nPERILS AND OPPORTUNITIES OF LLMS 104\n1993), Need for Cognition (NFC; Cacioppo and Petty (1982b)), as well as self-report ratings on\nreligiosity (Abdel-Khalek, 2007; Afhami et al., 2017) and political orientation (liberal vs.\nconservative). The predictions were made based on two types of essays that the participants\nwrote—one about their personal values and one about their everyday life and behaviors. We\nutilized the same essays to infer participant responses about the different constructs (22 in total)\nin the original data set.\nCCR Predictions. We use the CCR analog to the original paper (Atari, Omrani, &\nDehghani, 2023b). CCR creates similarity scores between the participant’s essay and the text of\neach item that was used to measure a participant’s response for a respective construct. Then, a\nlinear regression model is fit for each construct predicting the participants’ construct scores based\non these similarity scores. Finally, the regression model’sR2 score is reported. These scores are\nthen used to compare CCR and ChatGPT performance across all constructs.\nChatGPT Predictions. We prompted ChatGPT to respond to survey items via the\nOpenAI API (https://platform.openai.com/docs/api-reference?lang=python). To access the API\nand to collect the responses we used the OpenAI library (v0.27.8; OpenAI (2023)) in Python. We\ndeployed the “gpt-3.5-turbo-0301” model with a temperature of 0 (to allow for maximal\nreproducibility). We utilized two types of prompts. First, we prompted ChatGPT to predict a\nparticipant’s rating on a psychological construct (e.g., conservatism) based on one of the essays\nthey wrote. This prompt was supposed to test the model’s general ability to make these kinds of\npredictions without any further information. The prompt was as follows: “For a scientific study,\nrate how strongly the author of the following text endorses “Individualism” solely based on the\ntext they have written. Here is the text: [TEXT TO INTERPRET]\nRespond with a single number of up to two decimal points and between 1 and 5, with 1\nmeaning “strongly disagree” and 5 meaning “strongly agree”. Respond only with this single\nnumber and nothing else. Do not use words.”\nThe second prompt was designed to be more similar to CCR’s method, in that it included\nthe item texts that were used to measure the respective participant responses. Specifically, we\nprompted ChatGPT to predict the responses to the respective items, instead of the construct\ndirectly. This could potentially improve performance as the items provide context about what the\nPERILS AND OPPORTUNITIES OF LLMS 105\nconstruct in question means. The prompt was as follows: “For a scientific study, rate how\nstrongly the author of the following text endorses the following psychological items solely based\non the text they have written. Here is the text: [TEXT TO INTERPRET]\nHere are the items: [ITEMS TO INFER]\nRespond to each item with a single digit between 1 and 5, with 1 meaning strongly\ndisagree and 5 meaning strongly agree. Respond with exactly [NUMBER OF ITEMS] numbers,\ncomma separated. Do not use words.”\nWe adapted the scale descriptions to the surveys and constructs as necessary. We\ncollected a separate response for each essay type (that is, we repeated the same prompt twice).\nSee the project repository for the utilized survey items, ChatGPT prompts, and code containing\nthe API calls.\nPerformance Evaluation. We fit one linear regression model for each construct\npredicting the participants’ scores based on ChatGPT’s ratings. We fit these models twice, once\nbased on ChatGPT’s ratings on the construct level and once based on the item level. We then\nreport theR2 scores of each model (construct-level and item-level) for all constructs. TheseR2\nscores were then compared for statistical significance against CCR’sR2 scores using Dunnett’s\nTest, which tests the two ChatGPT models against the CCR model while accounting for multiple\ngroup comparisons. We report the model differences, including p-value and 95% confidence\ninterval. Lastly, we test whether the ChatGPT performance and CCR performance are correlated,\nwhich indicates whether constructs are similarly captured by either method (i.e., constructs that\nare more difficult to capture for one method are also more difficult for the other). We conduct\nthis analysis once for ratings based on values essays, that is, CCR and ChatGPT predictions\nbased on participants’ essays about their individual values, and once for ratings based on\nparticipants’ essays about their everyday life.\nWe conducted all analyses in R (v4.1.2; R Core Team (2021)) using the stats (v4.1.2; R\nCore Team (2021)), car (v3.0-11; Fox and Weisberg (2019)), and DescTools (v0.99.44; Andri et\nmult. al. (2021)) packages.\nPERILS AND OPPORTUNITIES OF LLMS 106\nResults\nWe found no difference between ChatGPT-based ratings and CCR across all tested\nconstructs except for one, where CCR outperformed construct-level ChatGPT predictions in\neveryday essays. Specifically, in the value essays (Figure S33) CCR performed on par with\nChatGPT ratings on the construct-level (Dunnett’s Test;d = −0.791, p = .798, 95%CI [-3.975,\n2.392]) and on the item-level (Dunnett’s Test;d = 2.211, p = .209, 95%CI [-0.972, 5.394]). In the\neveryday essay (Figure S34) CCR outperformed ChatGPT ratings on the construct-level\n((Dunnett’s Test;d = −2.253, p = .005, 95%CI [-3.881, -0.624]) and performed on par with\nChatGPT’s ratings on the item-level (Dunnett’s Test;d = 0.209, p = .940, 95%CI [-1.419,\n1.837]). See Tables S28 and S30 for an overview of model performances across all self-report\nmeasures. Lastly, ChatGPT’s performance correlated highly with CCR across all measures for\nboth essays. Specifically, on the value essays, we found a strong relationship between ChatGPT’s\nperformance when using item-level ratings and CCR (r = .81, d f= 20,p < .001), as well as when\nusing construct-level ratings (r = .70, d f= 20,p < .001). On the everyday essays, we found a\nstrong relationship between ChatGPT’s performance when using construct-level ratings and CCR\n(r = .65, d f= 20,p = .001), but not when using item-level ratings (r = .40, d f= 20,p = .062).\nOverall, we found that ChatGPT could not outperform the less complex, theory-based,\nmethod. Furthermore, the high correlation of performances indicates that ChatGPT does not\npossess capabilities that go notably beyond the more constrained theory-based method (that is,\nwhere ChatGPT performs better, CCR also performs better). Lastly, but importantly, the\nperformance differences between ChatGPT when giving ratings on psychological items, instead of\nconstructs or general concepts, show that even LLMs can benefit from the integration of\ntheory-based methodologies and frameworks. Extending this line of work, and developing\ncomprehensive methods that can combine the benefits of both worlds, is a promising endeavor for\nfuture work. Again, our intention here is not to posit that LLMs cannot be used in psychology\nresearch, be it as a tool or as the focus of a research question. Instead, we aim to show where\nthese methods currently struggle, and how they compare against other established, more\ninterpretable alternatives. The choice of methodology, ultimately, lies with the researcher and\nshould be made after careful consideration of all relevant concerns—and advantages.\nPERILS AND OPPORTUNITIES OF LLMS 107\nTable S28\nModel performances (R2 in %) on the value essays\nMeasures CCR ChatGPT (construct-level) ChatGPT (item-level)\nIndividualism 4.58 4.02 12.10\nCollectivism 3.70 1.50 3.87\nReligiosity 13.30 13.64 28.83\nTightness 5.10 8.07 1.60\nConservatism 10.19 2.55 5.36\nCare 6.23 3.12 3.83\nEquality 1.31 2.38 1.33\nProportionality 3.10 9.30 5.66\nLoyalty 4.81 3.85 5.79\nAuthority 2.59 7.00 10.34\nPurity 9.65 6.58 17.04\nPERILS AND OPPORTUNITIES OF LLMS 108\nTable S29\nModel performances (R2 in %) on the value essays\nMeasures CCR ChatGPT (construct-level) ChatGPT (item-level)\nNeed for Cognition 1.01 0.10 1.51\nAchievement 2.62 2.87 2.79\nBenevolence 0.84 1.76 2.07\nConformity 0.59 0.70 2.28\nHedonism 0.76 3.17 2.31\nPower 0.37 0.00 1.64\nSecurity 0.24 0.73 2.13\nSelf-Direction 1.25 4.48 2.29\nStimulation 2.98 0.17 3.41\nTradition 0.35 0.00 3.00\nUniversalism 3.36 1.12 3.81\nPERILS AND OPPORTUNITIES OF LLMS 109\nTable S30\nModel performances (R2 in %) on the everyday essays\nMeasures CCR ChatGPT (construct-level) ChatGPT (item-level)\nIndividualism 3.07 0.02 4.22\nCollectivism 3.58 1.02 2.67\nReligiosity 8.94 2.58 10.63\nTightness 2.61 0.01 1.88\nConservatism 6.15 0.10 0.24\nCare 4.02 0.02 3.42\nEquality 1.63 0.09 2.22\nProportionality 2.71 0.69 2.62\nLoyalty 1.15 0.05 1.24\nAuthority 2.69 0.33 4.27\nPurity 12.7 1.75 3.45\nPERILS AND OPPORTUNITIES OF LLMS 110\nTable S31\nModel performances (R2 in %) on the everyday essays\nMeasures CCR ChatGPT (construct-level) ChatGPT (item-level)\nNeed for Cognition 3.00 8.66 0.45\nAchievement 1.19 0.59 1.53\nBenevolence 1.02 0.04 3.67\nConformity 0.14 0.15 0.81\nHedonism 0.74 0.42 0.92\nPower 0.28 0.04 1.88\nSecurity 0.16 0.26 0.35\nSelf-Direction 0.09 0.85 0.93\nStimulation 0.85 0.17 3.15\nTradition 2.08 0.55 6.74\nUniversalism 3.36 0.97 0.51\nPERILS AND OPPORTUNITIES OF LLMS 111\nFigure S33\nComparing explained variance for CCR, GPT at the construct-level, and GPT at the item-level scores on\nthe values essay\nAchievement\nAuthority\nBenevolence\nCare\nCollectivism\nConformity\nConservatism\nEquality\nHedonism\nIndividualism\nLoyalty\nNeed For Cognition\nPower\nProportionality\nPurity\nReligiosity\nSecurity\nSelf−Direction\nStimulation\nTightness\nTradition\nUniversalism\n0 10 20 30\nVariance explained (%)\nMethod\nCCR\nGPT (construct−level)\nGPT (item−level)\nNote. Shaded area indicates 95% CI for the average variance explained for each prediction method. Figure\nshows that neither classification method significantly differs from each other.\nPERILS AND OPPORTUNITIES OF LLMS 112\nFigure S34\nComparing explained variance for CCR, GPT at the construct-level, and GPT at the item-level scores on\neveryday essays\nAchievement\nAuthority\nBenevolence\nCare\nCollectivism\nConformity\nConservatism\nEquality\nHedonism\nIndividualism\nLoyalty\nNeed For Cognition\nPower\nProportionality\nPurity\nReligiosity\nSecurity\nSelf−Direction\nStimulation\nTightness\nTradition\nUniversalism\n0 5 10\nVariance explained (%)\nMethod\nCCR\nGPT (construct−level)\nGPT (item−level)\nNote. Shaded area indicates 95% CI for the average variance explained for each prediction method. Figure\nshows that GPT predictions on the construct-level (green) explain significantly variance than CCR (red)\nand GPT predictions on the item-level (blue)\nPERILS AND OPPORTUNITIES OF LLMS 113\nS5. Open vs. Closed Language Models\nTo strengthen our arguments for using open-source LLMs over closed proprietary LLMs,\nwe repeated our main analyses using an open-source model. Specifically, we repeat our\ncomparison of LLMs vs. fine-tuned BERT-based models and the bias analysis towards annotator\ndemographics. We then repeat our collection of survey response data on the same range of\nquestionnaires. We finally investigate whether the open-source LLM’s output is comparable to\nthat of ChatGPT. Overall, we show that we can generate comparable performance/data\ndistribution to ChatGPT using a locally run open-source LLM on strictly consumer-grade\ncomputer hardware.\nSpecifically, we used a model based on the second generation of Meta’s open-source LLM\n(“LLaMA”; (Touvron et al., 2023)). LLaMA is a foundational LLM intended to be fine-tuned for\ndownstream tasks by researchers or private and commercial users. In contrast to GPT, LLaMA\nwas trained exclusively on publicly available data, such as Wikipedia texts, public GitHub\nrepositories, and forum conversations, and can be freely downloaded and applied. Notably,\nLLaMA is significantly more efficient in training and operating than its contemporaries and can\nachieve state-of-the-art performance with significantly fewer parameters. For example, LLaMA\nwith 13 billion parameters outperforms GPT3 with 175 billion parameters on most tasks\n(Touvron et al., 2023). Thus, LLaMA can be fine-tuned and run on consumer-grade hardware.\nRecent advances in quantization methods, used to reduce the number of parameters and\naccelerate inference speed for LLMs, further increase LLaMA’s efficiency and applicability on\nhome-grade computer hardware (Dettmers & Zettlemoyer, 2023). Consequently, a plethora of\nLLaMA derivatives, such as Stanford’s “Alpaca” (Taori et al., 2023) and LMSYS’ Vicuña (Chiang\net al., 2023), have been fine-tuned on public data sets for various tasks, and in the spirit of\nopen-science, have the full model (including model parameters/weights) as well training\nprocedures and data publicly available (see e.g.,\nhttps://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard for an overview of\nbest-performing LLMs on huggingface.com). Thus, LLaMA and its derivatives can be useful tools\nfor researchers who want to be in control of their research pipeline while studying LLMs or\napplying them for further tasks.\nPERILS AND OPPORTUNITIES OF LLMS 114\nS5a. Text Annotations\nData. Analogue to the main analyses, we tested LLaMA’s performance on the Moral\nFoundations Reddit Corpus (MFRC; Trager et al. (2022b)), a collection of 16,123 Reddit\ncomments that have been hand-annotated by at least three trained annotators for 8 categories of\nmoral sentiment (i.e., Care, Proportionality, Equality, Purity, Authority, Loyalty, Thin Morality,\nNon-moral) based on the updated Moral Foundations Theory (MFT; Atari, Haidt, et al. (2023b))\nframework. The MFRC further provides demographic and psychometric information about each\nhuman annotator. We used the same random subsection of the MFRC (2,983 samples) to\nevaluate the annotation accuracy of LLaMA.\nLLaMA Model. We utilized the “Luna-AI-Llama2-Uncensored” model from\nhuggingface.com, which is a Llama2 based chat model that was fine-tuned on over 40,000 long\nform chat discussions. We intentionally used the smallest and most efficient version\n(“Luna-AI-Llama2-Uncensored-GPTQ:gptq-4bit-128g-actorder_True”), which has 7 billion\nparameters and using 4-bit quantization, to show that our pipeline and performance is achievable\nfor the average lab. This version is able to run locally on desktops with a single, mid-range, GPU\nof 6GB VRAM or even on laptops using the CPU. Note that “Uncensored”, here refers to the\nmodel not being fine-tuned for appropriate content generation or any other policies. The model is\nintended to be fine-tuned according to the respective use-cases by the end-user. We chose this\nmodel because we wanted to avoid unnatural responses due to fine-tuning that deviates from\nregular human behavior (e.g., unnaturally helpful, moral, appropriate responses, “As an AI\nmodel, ...”).\nWe prompted ChatGPT to annotate posts from the MFRC test data via\n“text-generation-web-ui” (Anonymous, 2023). This simple interface allows to self-host LLMs on a\nlocal machine. See the GitHub repository for full installation instructions, including a one-click\ninstaller. After installation, we downloaded the respective LLaMA model from Huggingface using\nthe interface (“TheBloke/Luna-AI-Llama2-Uncensored-GPTQ:gptq-4bit-128g-actorder_True”).\nWe then ran all inferences through Python via the interface’s API function. See our code\nrepository for instructions on how to run the LLaMA sentiment analysis via Python interactive\nnotebook. We deployed the LLaMA model with a minimal temperature of 0.01, for maximal\nPERILS AND OPPORTUNITIES OF LLMS 115\nreproducibility. We used the following prompt, which matched the definitions of each moral\nfoundation according to MFT and the human annotators’ instructions, to collect the annotations:\n“USER: These are definitions of moral sentiments: “care” if a text is about avoiding emotional\nand physical damage to another individual, “equality” if a text is about equal treatment and\nequal outcome for individuals, “proportionality” if a text is about individuals getting rewarded in\nproportion to their merit or contribution, “loyalty” if a text is about cooperating with ingroups\nand competing with outgroups, “authority” if a text is about deference toward legitimate\nauthorities and the defense of traditions, all of which are seen as providing stability and fending\noff chaos, “purity” if a text is about avoiding bodily and spiritual contamination and degradation,\n“thin-morality” if a text has a moral sentiment but cannot be categorized as either of the above,\n“none” if no moral sentiment is expressed in the text.\nBased on this definition, name all moral sentiments that are expressed in the following text:\n[TEXT TO ANNOTATE]\nReturn a comma-separated list of all moral sentiments that were expressed in the text.\nASSISTANT: ”\nNote that we slightly changed the prompt from ChatGPT, because our LLaMA version\nuses a different instruction/chat template compared to ChatGPT (e.g., requires “ASSISTANT:”\nto mark the beginning of response generation). Importantly, we did not change any of the moral\nsentiment definitions and did not provide any additional information. The changes are\nsurface-level changes in the presentation of the information (e.g., USER/ASSISTANT template).\nLocal Hardware. We used a consumer-grade computer using an NVIDIA RTX2070s\n8GB VRAM GPU, an AMD Ryzen 5 3600 3.6GHz 6-core CPU, 16GB DDR4 3200MHz RAM,\nand a 1TB HDD. The full set-up, including all additional components, cost less than $1,000 at\nthe time of purchase (February 2021).\nBERT-model. To keep the bias analyses comparable, no new BERT-based model was\ntrained for this analysis. Instead, we utilized the previously trained and fine-tuned BERT model\n(see section X for the details of model training). We used the same model output as in the main\nPERILS AND OPPORTUNITIES OF LLMS 116\nanalysis to compare LLaMA’s annotation performance against fine-tuned BERT’s and for all\nsubsequent analyses, such as the distribution of predicted moral sentiment or deviations from\nground truth.\nPerformance Evaluation. To evaluate the annotation accuracy of the BERT-based\nmodel and LLaMA, we calculated each model’s F1 score (Goutte & Gaussier, 2005), which is a\nwidely used single metric that assesses the model’s ability to make accurate positive predictions\nwhile minimizing false positives and false negatives. We calculated the F1 scores with the human\nannotations as ground truth using the Sklearn library (v1.3; Pedregosa et al. (2011)) in Python.\nWe calculated the macro averaged F1 score, that is the unweighted average of the F1 scores for\nthe prediction of each type of moral sentiment, which expresses the model’s accuracy across all\nmoral sentiments. Finally, we fit a logistic regression model predicting the presence of moral\nsentiment in an annotation (binary, yes/no) as a function of annotator type (Human, LLaMA,\nBERT). This model expressed how much more likely the computational models were to annotate\na moral sentiment compared to human ground truth. This model was used to evaluate whether\nLLaMA or BERT significantly over-predicted or under-predicted (e.g., false positives, false\nnegatives) the specific moral sentiments.\nBias Analyses. To determine biases in LLaMA’s annotations, we investigated how\nmuch LLaMA aligned with annotators of specific demographics. To that end, we create a binary\nvariable expressing whether LLaMA’s annotation and the human annotators’ rating aligned (1 if\nLLaMA and human annotation were equal; 0 if not). If there were multiple sentiments in one\npost we compared each one separately. We then fit logistic regression models that predicted\nhuman-LLaMA alignment as a function of annotator demographics and psychometric information.\nThese models describe how likely LLaMA is to align with an annotator of certain demographics\n(e.g., race, gender, age). In total, we tested for bias towards annotators’ age, sex, religion, moral\nvalues (based on MFT), personality (Five-Factor Model), creativity (creative imagination,\naesthetic sensitivity, intellectual curiosity), mental health (anxiety, depression, emotional\nvolatility), political orientation (collectivism, individualism, cultural-tightness,\nsocial-conservatism, economical-conservatism), interpersonal attitudes (compassion, trust,\nrespectfulness), and efficiency (productiveness, organization, responsibility).\nPERILS AND OPPORTUNITIES OF LLMS 117\nWe conducted all statistical analyses in R (v4.1.2; R Core Team (2021)) using the stats\n(v4.1.2; R Core Team (2021)) package (R Core Team, 2021). See the code repository for\ninstructions to replicate the statistical analyses.\nResults. As expected, we found that fine-tuned BERT outperforms LLaMA by a large\nmargin. Specifically, BERT achieved an F1 score of 0.48 while LLaMA achieved an F1 score of\n0.23. Notably, however, LLaMA achieved a higher F1 score than ChatGPT (0.22) despite its\nsignificantly smaller model size and having been provided less computing power, highlighting the\napplicability of open LLMs.\nWhen comparing the prediction errors across moral sentiments, we found that LLaMA\nand ChatGPT perform comparably. For example, LLaMA was significantly better at predicting\nEquality and Nonmoral sentiment, performed approximately on par when predicting Purity,\nThin-Morality, Authority, and Proportionality sentiment, and performed significantly worse when\npredicting Care and Loyalty sentiment. See Table S32 for an overview of BERT, ChatGPT, and\nLLaMA’s deviation from human ground truth.\nDemographic Biases. We found that LLaMA2 showed significant demographic\nbiases in its moral annotations, aligning more so with certain groups including older over\nyounger(+3%, p < .001), male over female (+60 %,p < .001), and non-religious over Christian\n(+68%, p < .001). See Table S33 for an overview of all annotation biases.\nMoral Biases. We found that LLaMA2 showed significant moral biases in its\nannotations, aligning more so with human annotators who endorse certain moral values, such as\nloyalty (+1052%,p < .001) and equality(+1358%,p < .001), while misaligning with the\nannotators who endorse other values, such as care (-99%,p < .001) and proportionality (-99%,p\n< .001). See Table S34 for an overview of annotation biases regarding annotators’ moral values.\nPersonality Biases. We found that LLaMA2 showed significant personality biases in\nits annotations, aligning more so with human annotators who load highly on some personality\ntraits, such as conscientiousness (+316%,p < .001) and extraversion (+88%,p < .001), while\nmisaligning with annotators who load highly on other personality traits, such as neuroticism\n(-42%, p < .001). See Table S35 for an overview of annotation biases regarding annotators’\npersonalities.\nPERILS AND OPPORTUNITIES OF LLMS 118\nTable S32\nDeviations from ground truth for each model on each prediction class\nClass BERT ChatGPT LLaMA\nCare +12% ( p = .162) +35% ( p < .001) +171% ( p < .001)\nEquality -11% ( p = .255) +55% ( p < .001) -7% ( p < .001)\nProportionality -20% ( p = .118) -54% ( p < .001) +270% ( p < .001)\nLoyalty -64% ( p < .001) +3% ( p < .001) +811% ( p < .001)\nAuthority -32% ( p = .003) +96% ( p < .001) +154% ( p < .001)\nPurity -74% ( p < .001) -86% ( p < .001) +1,042% ( p < .001)\nThin-Morality +2% ( p = .861) +1,240% ( p < .001) -100% ( p < .001)\nNonmoral +69% ( p < .001) -100% ( p < .001) +12% ( p < .001)\nNote. Table shows the difference in odds for the classifier to predict a class compared to ground truth\n(trained human annotators). The table shows that BERT has a smaller deviation from ground truth except\non the Loyalty foundation. For most moral values BERT doesnot deviate significantly (at p < .05) from\nground-truth while both LLMs (GPT, LLaMA2) deviate significantly from ground-truth for all moral values.\nTable S33\nDemographic Biases of LLaMA2 annotations on the MFRC\nDemographic Variable ∆log(odds) p\nFemale (vs. Male) -60 % < .001\nAge (years) +3% < .001\nNon-Religious (vs. Christianity) +68% < .001\nNote. Table shows LLaMA2 bias towards respective demographics during annotations. That is, it had\nhigher (+) or lower (-) odds (%) of aligning with human annotators of said demographic.\nPERILS AND OPPORTUNITIES OF LLMS 119\nTable S34\nMoral Biases of LLaMA2 annotations on the MFRC\nMoral Foundation ∆log(odds) p\nCare -99% < .001\nEquality +1358% < .001\nProportionality -99% < .001\nLoyalty +1052% < .001\nAuthority -22% .072\nPurity - -\nNote. Table shows LLaMA2 bias towards respective moral profiles during annotations. That is, it had\nhigher/lower odds (%) of aligning with human annotators endorsing said moral values. All variables are\n5-point Likert scales.\nTable S35\nPersonality Biases of LLaMA2 annotations on the MFRC\nDemographic Variable ∆log(odds) p\nOpenness +0% .993\nConscientiousness +316% < .001\nExtraversion +88% < .001\nAgreeableness -10% .507\nNeuroticism -42% < .001\nNote. Table shows LLaMA2 bias towards respective demographics during annotations. That is, LLaMA2\nhad higher (+) or lower (-) odds (%) of aligning with human annotators of said personalities. All variables\nare 5-point Likert scales.\nPERILS AND OPPORTUNITIES OF LLMS 120\nPolitical Biases. We found that LLaMA2 showed significant political biases in its\nannotations, aligning more so with human annotators who have certain political attitudes, such as\ncollectivism (+20%,p < .001), tightness (+388%,p < .001), social conservatism (+3%,p < .001),\nand economic conservatism (+4%,p < .001), while misaligning with the annotators who have\nother political attitudes, such as individualism (-77%,p < .001). See Table S36 for an overview of\nannotation biases regarding annotators’ political orientation.\nTable S36\nPolitical Biases of LLaMA2 annotations on the MFRC\nSocio-Political Variable ∆log(odds) p\nCollectivism +20% < .001\nIndividualism -77% < .001\nTightness +388% < .001\nSocial conservatism +3% < .001\nEconomical conservatism +4% < .001\nNote. Table shows LLaMA2 bias towards respective demographics during annotations. That is, LLaMA2\nhad higher (+) or lower (-) odds (%) of aligning with human annotators of said political values. All\nvariables are 5-point Likert scales.\nCreativity-related Biases. We found that LLaMA2 showed significant\ncreativity-related biases in its annotations, aligning more so with human annotators who have\ncertain creativity-related traits, such as openness (+880%,p < .001) and aesthetic sensitivity\n(+81%, p < .001) while misaligning with annotators who have other traits, such as creative\nimagination (-96%,p < .001). See Table S37 for an overview of annotation biases regarding\nannotators’ creativity and aesthetic preferences.\nMental Health Biases. We found that LLaMA2 showed significant mental health\nbiases in its annotations, aligning more so with human annotators who have certain mental health\ntraits, such as depression (+47%,p < .001) while misaligning with the annotators who have other\nmental health traits, such as anxiety (-58%,p < .001) and emotional volatility (-35%,p < .001).\nPERILS AND OPPORTUNITIES OF LLMS 121\nTable S37\nCreativity-related Biases of LLaMA2 annotations on the MFRC\nVariable ∆log(odds) p\nOpenness +880% < .001\nCreative imagination -96% < .001\nAesthetic sensitivity +81% < .001\nNote. Table shows LLaMA2 bias towards respective demographics during annotations. That is, LLaMA2\nhad higher (+) or lower (-) odds (%) of aligning with human annotators of said creative inclinations. All\nvariables are 5-point Likert scales.\nSee Table S38 for an overview of annotation biases regarding annotators’ mental health traits.\nTable S38\nMental-Health related Biases of ChatGPT annotations on the MFRC\nMental-Health Variable ∆log(odds) p\nAnxiety -58% < .001\nDepression +47% < .001\nEmotional-Volatility -35% < .001\nNote. Table shows LLaMA2 bias towards respective demographics during annotations. That is, LLaMA2\nhad higher (+) or lower (-) odds (%) of aligning with human annotators with said mental-health issues. All\nvariables as 5-point Likert scales.\nInterpersonal virtues Biases. We found that LLaMA2 showed significant\ninterpersonal virtues biases in its annotations, aligning more so with human annotators who have\ncertain interpersonal virtues, such as trust (+126%,p < .001) and respectfulness (+140%,p <\n.001) while misaligning with the annotators who have other interpersonal virtues, such as\ncompassion (-36%,p < .001). See Table S39 for an overview of annotation biases regarding\nannotators’ interpersonal virtues.\nPERILS AND OPPORTUNITIES OF LLMS 122\nTable S39\nInterpersonal Virtues related Biases of LLaMA2 annotations on the MFRC\nVirtue Variable ∆log(odds) p\nCompassion -36% < .001\nTrust +126% < .001\nRespectfulness +140% < .001\nNote. Table shows LLaMA2 bias towards respective demographics during annotations. That is, LLaMA2\nhad higher (+) or lower (-) odds (%) of aligning with human annotators of said interpersonal virtues. All\nvariables as 5-point Likert scales.\nEfficiency-related Biases. We found that LLaMA2 showed significant\nefficiency-related trait biases in it’s annotations, aligning more so with human annotators who\nhave high endorsement of all efficiency-related traits, such as productiveness (+25%,p < .001),\norganization (+269%,p < .001) and responsibility (+98%,p < .001). See Table S40 for an\noverview of annotation biases regarding annotators’ efficiency traits.\nTable S40\nEfficiency-related Biases of LLaMA2 annotations on the MFRC\nEfficiency Variable ∆log(odds) p\nProductiveness +25% < .001\nOrganization +269% < .001\nResponsibility +98% < .001\nNote. Table shows LLaMA2 bias towards respective demographics during annotations. That is, LLaMA2\nhad higher (+) or lower (-) odds (%) of aligning with human annotators of said efficiency traits. All\nvariables as 5-point Likert scales.\nPERILS AND OPPORTUNITIES OF LLMS 123\nS5b. Survey Responses\nData. We repeated our tests on the same questionnaires as in the main analyses,\nnamely, the Big-Five Inventory (BFI;N = 3924; Fossati et al. (2011)), Need for Closure Survey\n(N = 315; Webster and Kruglanski (1994b)), Need for Cognition Survey (N = 900; Cacioppo and\nPetty (1982b)), Right-wing-authoritarianism Scale (N = 1020; Zakrisson (2005b)),\nEmphasizing-Systemizing Scale (N = 3141; Baron-Cohen et al. (2003b)), and\nRational-Experiential Inventory Scale (N = 1456; Pacini and Epstein (1999b)). We used the same\nhuman response data as in the main analyses, which was collected on\nhttps://www.yourmorals.org and contains the participants’ responses as well as various\ndemographic information about the participants.\nLLaMA Responses. We prompted LLaMA to respond to survey items via the\n“text-generation-web-ui” interface (Anonymous, 2023) in Python. We deployed the\n“Luna-AI-Llama2-Uncensored-GPTQ:gptq-4bit-128g-actorder_True” model with a temperature\nof 0.76 to allow for variability in the responses. Note that ChatGPT’s and LLaMA’s parameters\nhave different ranges and that LLaMA has significantly more parameters than ChatGPT. Since\ntuning these parameters for better data generation is beyond the scope of this work, we chose a\npopular preset of LLaMA parameters that achieves high performances in instruction following\nand chat response benchmark. However, LLaMA might be able to produce better performances\nwith different sets of parameter values. See our code repository for the set of chosen parameters.\nWe, again, gave LLaMA slightly changed prompts due to its different instruction template\ncompared to ChatGPT. The prompts had this general form:\n“USER: You will indicate your general level of agreement with a statement given to you. You will\nexpress your level of agreement as an integer between 1 and 5, with 1 meaning “strongly disagree”\nand 5 meaning “strongly agree”. You will respond with nothing but this number. How much do\nyou agree with this statement?\n[STATEMENT]\nASSISTANT: ”\nPERILS AND OPPORTUNITIES OF LLMS 124\nWe did not add any additional information to the LLaMA prompt and only changed the\npresentation and tone of the instruction details (this LLaMA version expected more direct\ninstructions than ChatGPT, e.g., marking the beginning of response generation with\n“ASSISTANT”). We adapted the scale descriptions to the surveys as necessary. We collected 100\nresponses for each item (that is, we repeated the same prompt 100 times) to allow for analyses\nregarding the variance within LLaMA responses. See the project repository for the utilized survey\nitems, LLaMA prompts, and code containing the API calls.\nBias Analyses. We compared LLaMA’s responses to demographic-level average\nhuman responses. That is, we investigated the difference between LLaMA’s survey responses and\nthe average response of different groups of humans based on their demographics (e.g., age, race,\ngender). Note, we removed all demographics with less than 100 samples in the data sets as they\ndid not provide enough information to make inferences about the said demographic. We\ncalculated the average LLaMA response for each construct in a given survey and then subtracted\nit from the group-level average human response on said construct (e.g., the difference between\nLLaMA’s “Openness” score and the average male or female “Openness” score). We then tested\nthese differences for statistical significance using Dunnett’s test (Dunnett, 1955), which compares\neach of a number of treatments with a single control, with the demographic groups as treatments\nand LLaMA as the control, while adjusting for multiple comparisons. We then repeated this\nanalysis for differences in variance. We investigated whether LLaMA responses had less variance\ncompared to the different human groups (e.g., the difference in the variance of LLaMA’s\n“Openness” scores vs. the variance in male or female “Openness” scores). We used the Levene\nTest (Levene, 1960) to determine whether the variance in each human group’s responses differed\nfrom LLaMA’s while accounting for multiple comparisons using Bonferroni correction. Second, we\ninvestigated the relationship between the participants’ demographic characteristics and the\nsimilarity of humans to LLaMA responses. We fit a linear regression model that predicted the\nabsolute deviation of LLaMA and human responses (aggregated over all survey constructs) as a\nfunction of each human participant’s demographic characteristic. These models expressed which\nparticipant demographics LLaMA is more or less likely to align with and therefore showed its\nbiases (e.g., are LLaMA’s responses more similar to older vs. younger participants).\nPERILS AND OPPORTUNITIES OF LLMS 125\nWe conducted all analyses in R (v4.1.2; R Core Team (2021)) using the stats (v4.1.2; R\nCore Team (2021)), car (v3.0-11; Fox and Weisberg (2019)), and DescTools (v0.99.44; Andri et\nmult. al. (2021)) packages.\nResults. Overall, we found that LLaMA generates responses comparable to ChatGPT.\nFigures S35 to S45 show the differences between LLaMA, ChatGPT, and human demographic\ngroups. Notably, on some questionnaires, LLaMA achieves significantly more natural responses\nthat are better aligned with the different human demographics. This might be linked to the lack\nof “corporate guardrails” and censorship policies. For example, LLaMA is significantly less\nAgreeable and Conscientious than ChatGPT which might be linked to ChatGPT being fine-tuned\nto be a friendly assistant that avoids sensitive topics. Tables S41 - S49 provide a detailed\noverview of the differences between LLaMA and various human demographic groups.\nTable S41\nDeviation of LLaMA responses from human responses grouped by age\nSurvey Construct Demographic d p\nBFI Extraversion 55-74 -0.19 .076\nBFI Agreeableness 55-74 0.08 .495\nBFI Conscientiousness 55-74 0.21 .015\nBFI Neuroticism 55-74 -0.14 .263\nBFI Openness 55-74 0.38 < .001\nNeed for Cognition Need for Cognition 55-74 0.63 < .001\nNeed for Closure Need for Closure 55-74 - -\nSystemizing and Emphasizing Systemizing 55-74 -0.24 < .001\nSystemizing and Emphasizing Emphasizing 55-74 0.29 < .001\nRational-Experiential Inventory Rational 55-74 0.77 < .001\nRational-Experiential Inventory Experiential 55-74 -0.08 .614\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism 55-74 -0.85 < .001\nBFI Extraversion 35-54 -0.26 .008\nBFI Agreeableness 35-54 -0.01 1.000\nContinued on next page\nPERILS AND OPPORTUNITIES OF LLMS 126\nTable S41 – continued from previous page\nSurvey Construct Demographic d p\nBFI Conscientiousness 35-54 0.06 .669\nBFI Neuroticism 35-54 -0.02 .989\nBFI Openness 35-54 0.25 < .001\nNeed for Cognition Need for Cognition 35-54 0.70 < .001\nNeed for Closure Need for Closure 35-54 - -\nSystemizing and Emphasizing Systemizing 35-54 -0.22 < .001\nSystemizing and Emphasizing Emphasizing 35-54 0.23 < .001\nRational-Experiential Inventory Rational 35-54 0.83 < .001\nRational-Experiential Inventory Experiential 35-54 0.02 .998\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism 35-54 -0.69 < .001\nBFI Extraversion 25-34 -0.35 < .001\nBFI Agreeableness 25-34 -0.09 .339\nBFI Conscientiousness 25-34 -0.088 .437\nBFI Neuroticism 25-34 0.100 .457\nBFI Openness 25-34 0.19 .003\nNeed for Cognition Need for Cognition 25-34 0.66 < .001\nNeed for Closure Need for Closure 25-34 - -\nSystemizing and Emphasizing Systemizing 25-34 -0.32 < .001\nSystemizing and Emphasizing Emphasizing 25-34 0.18 < .001\nRational-Experiential Inventory Rational 25-34 0.77 < .001\nRational-Experiential Inventory Experiential 25-34 -0.04 .960\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism 25-34 -0.60 < .001\nBFI Extraversion 18-24 -0.30 .001\nBFI Agreeableness 18-24 -0.08 .475\nBFI Conscientiousness 18-24 -0.19 .018\nBFI Neuroticism 18-24 0.289 .001\nContinued on next page\nPERILS AND OPPORTUNITIES OF LLMS 127\nTable S41 – continued from previous page\nSurvey Construct Demographic d p\nBFI Openness 18-24 0.06 .608\nNeed for Cognition Need for Cognition 18-24 0.48 < .001\nNeed for Closure Need for Closure 18-24 - -\nSystemizing and Emphasizing Systemizing 18-24 -0.43 < .001\nSystemizing and Emphasizing Emphasizing 18-24 0.11 .052\nRational-Experiential Inventory Rational 18-24 0.56 < .001\nRational-Experiential Inventory Experiential 18-24 0.04 .956\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism 18-24 -0.51 < .001\nNote: Positive values indicate a higher average response for age groups compared to LLaMA. Significance\ndetermined via Dunnett’s Test.\nFigure S35\nComparing LLaMA and ChatGPT against Humans by Age for responses on the BFI\n1\n2\n3\n4\n5\nExtraversion Agreeableness Conscientiousness Neuroticism Openness\nConstruct Score\nLLAMA\nGPT\n18−24\n25−34\n35−54\n55−74\nPERILS AND OPPORTUNITIES OF LLMS 128\nFigure S36\nComparing LLaMA and ChatGPT against Humans by Age for responses on the Need for Cognition\nquestionnaire\n1\n2\n3\n4\n5\nNFC\nConstruct Score\nLLAMA\nGPT\n18−24\n25−34\n35−54\n55−74\nFigure S37\nComparing LLaMA and ChatGPT against Humans by Age for responses on the Rational-Experiential\nInventory\n1\n2\n3\n4\n5\nRational Experiential\nConstruct Score\nLLAMA\nGPT\n18−24\n25−34\n35−54\n55−74\nPERILS AND OPPORTUNITIES OF LLMS 129\nFigure S38\nComparing LLaMA and ChatGPT against Humans by Age for responses on the\nRight-Wing-Authoritarianism Scale\n0\n2\n4\n6\nRWA\nConstruct Score\nLLAMA\nGPT\n18−24\n25−34\n35−54\n55−74\nFigure S39\nComparing LLaMA and ChatGPT against Humans by Age for responses on the Systemizing-Emphasizing\nScale\n1\n2\n3\n4\nSystemizing Emphasizing\nConstruct Score\nLLAMA\nGPT\n18−24\n25−34\n35−54\n55−74\nPERILS AND OPPORTUNITIES OF LLMS 130\nTable S43\nDeviation of LLaMA responses from human responses grouped by sex\nSurvey Construct Demographic d p\nBFI Extraversion Female -0.20 .032\nBFI Agreeableness Female 0.05 .637\nBFI Conscientiousness Female 0.02 .949\nBFI Neuroticism Female 0.35 < .001\nBFI Openness Female 0.13 .057\nNeed for Cognition Need for Cognition Female 0.54 < .001\nNeed for Closure Need for Closure Female -1.27 < .001\nSystemizing and Emphasizing Systemizing Female -0.53 < .001\nSystemizing and Emphasizing Emphasizing Female 0.35 < .001\nRational-Experiential Inventory Rational Female 0.60 < .001\nRational-Experiential Inventory Experiential Female 0.14 .08\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism Female -0.84 < .001\nBFI Extraversion Male -0.35 < .001\nBFI Agreeableness Male -0.126 .089\nBFI Conscientiousness Male -0.11 .241\nBFI Neuroticism Male -0.10 .352\nBFI Openness Male 0.233 < .001\nNeed for Cognition Need for Cognition Male 0.71 < .001\nNeed for Closure Need for Closure Male -1.35 < .001\nSystemizing and Emphasizing Systemizing Male -0.14 .002\nSystemizing and Emphasizing Emphasizing Male 0.10 .064\nRational-Experiential Inventory Rational Male 0.82 < .001\nRational-Experiential Inventory Experiential Male -0.11 .202\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism Male -0.56 < .001\nContinued on next page\nPERILS AND OPPORTUNITIES OF LLMS 131\nTable S43 – continued from previous page\nSurvey Construct Demographic d p\nNote: Positive values indicate a higher average response for sex group compared to LLaMA. Significance\ndetermined via Dunnett’s Test.\nFigure S40\nComparing LLaMA and ChatGPT against Humans by Sex for responses on the BFI\n1\n2\n3\n4\n5\nExtraversion Agreeableness Conscientiousness Neuroticism Openness\nConstruct Score\nLLAMA\nGPT\nFemale\nMale\nPERILS AND OPPORTUNITIES OF LLMS 132\nFigure S41\nComparing LLaMA and ChatGPT against Humans by Sex for responses on the Need for Cognition\nquestionnaire\n1\n2\n3\n4\n5\nNFC\nConstruct Score\nLLAMA\nGPT\nFemale\nMale\nFigure S42\nComparing LLaMA and ChatGPT against Humans by Sex for responses on the Need for Closure Survey\n1\n2\n3\n4\n5\n6\nnfcc\nConstruct Score\nLLAMA\nGPT\nFemale\nMale\nPERILS AND OPPORTUNITIES OF LLMS 133\nFigure S43\nComparing LLaMA and ChatGPT against Humans by Sex for responses on the Rational-Experiential\nInventory\n1\n2\n3\n4\n5\nRational Experiential\nConstruct Score\nLLAMA\nGPT\nFemale\nMale\nFigure S44\nComparing LLaMA and ChatGPT against Humans by Sex for responses on the\nRight-Wing-Authoritarianism Scale\n0\n2\n4\n6\nRWA\nConstruct Score\nLLAMA\nGPT\nFemale\nMale\nPERILS AND OPPORTUNITIES OF LLMS 134\nFigure S45\nComparing LLaMA and ChatGPT against Humans by Sex for responses on the Systemizing-Emphasizing\nScale\n1\n2\n3\n4\nSystemizing Emphasizing\nConstruct Score\nLLAMA\nGPT\nFemale\nMale\nPERILS AND OPPORTUNITIES OF LLMS 135\nTable S45\nDeviation of LLaMA responses from human responses grouped by race\nSurvey Construct Demographic d p\nBFI Extraversion Asian -0.39 < .001\nBFI Agreeableness Asian -0.16 .06\nBFI Conscientiousness Asian -0.20 .042\nBFI Neuroticism Asian 0.11 .538\nBFI Openness Asian 0.08 .512\nNeed for Cognition Need for Cognition Asian - -\nNeed for Closure Need for Closure Asian - -\nSystemizing and Emphasizing Systemizing Asian -0.30 < .001\nSystemizing and Emphasizing Emphasizing Asian 0.14 .029\nRational-Experiential Inventory Rational Asian 0.58 < .001\nRational-Experiential Inventory Experiential Asian -0.07 .706\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism Asian - -\nBFI Extraversion White -0.27 .006\nBFI Agreeableness White -0.29 .974\nBFI Conscientiousness White -0.020 .996\nBFI Neuroticism White 0.09 .566\nBFI Openness White 0.21 < .001\nNeed for Cognition Need for Cognition White 0.68 < .001\nNeed for Closure Need for Closure White -1.38 < .001\nSystemizing and Emphasizing Systemizing White -0.29 < .001\nSystemizing and Emphasizing Emphasizing White 0.20 < .001\nRational-Experiential Inventory Rational White 0.78 < .001\nRational-Experiential Inventory Experiential White 0.01 .998\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism White -0.65 < .001\nBFI Extraversion Hispanic -0.29 .009\nContinued on next page\nPERILS AND OPPORTUNITIES OF LLMS 136\nTable S45 – continued from previous page\nSurvey Construct Demographic d p\nBFI Agreeableness Hispanic -0.11 .326\nBFI Conscientiousness Hispanic -0.08 .66\nBFI Neuroticism Hispanic 0.21 .096\nBFI Openness Hispanic 0.09 .430\nNeed for Cognition Need for Cognition Hispanic - -\nNeed for Closure Need for Closure Hispanic - -\nSystemizing and Emphasizing Systemizing Hispanic -0.38 < .001\nSystemizing and Emphasizing Emphasizing Hispanic 0.18 .008\nRational-Experiential Inventory Rational Hispanic - -\nRational-Experiential Inventory Experiential Hispanic - -\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism Hispanic - -\nBFI Extraversion Black -0.33 .008\nBFI Agreeableness Black 0.05 .935\nBFI Conscientiousness Black -0.03 .994\nBFI Neuroticism Black 0.02 .999\nBFI Openness Black 0.06 .832\nNeed for Cognition Need for Cognition Black - -\nNeed for Closure Need for Closure Black - -\nSystemizing and Emphasizing Systemizing Black - -\nSystemizing and Emphasizing Emphasizing Black - -\nRational-Experiential Inventory Rational Black - -\nRational-Experiential Inventory Experiential Black - -\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism Black - -\nNote: Positive values indicate a higher average response for racial groups compared to LLaMA.\nSignificance determined via Dunnett’s Test.\nPERILS AND OPPORTUNITIES OF LLMS 137\nFigure S46\nComparing LLaMA and ChatGPT against Humans by Race for responses on the BFI\n1\n2\n3\n4\n5\n6\nExtraversion Agreeableness Conscientiousness Neuroticism Openness\nConstruct Score\nLLAMA\nGPT\nAsian\nBlack\nHispanic\nWhite\nFigure S47\nComparing LLaMA and ChatGPT against Humans by Race for responses on the Need for Cognition\nquestionnaire\n2\n3\n4\n5\nNFC\nConstruct Score\nLLAMA\nGPT\nWhite\nPERILS AND OPPORTUNITIES OF LLMS 138\nFigure S48\nComparing LLaMA and ChatGPT against Humans by Race for responses on the Need for Closure Survey\n1\n2\n3\n4\n5\nnfcc\nConstruct Score\nLLAMA\nGPT\nWhite\nFigure S49\nComparing LLaMA and ChatGPT against Humans by Race for responses on the Rational-Experiential\nInventory\n1\n2\n3\n4\n5\nRational Experiential\nConstruct Score\nLLAMA\nGPT\nAsian\nWhite\nPERILS AND OPPORTUNITIES OF LLMS 139\nFigure S50\nComparing LLaMA and ChatGPT against Humans by Race for responses on the\nRight-Wing-Authoritarianism Scale\n0\n2\n4\n6\nRWA\nConstruct Score\nLLAMA\nGPT\nWhite\nFigure S51\nComparing LLaMA and ChatGPT against Humans by Race for responses on the Systemizing-Emphasizing\nScale\n1\n2\n3\n4\nSystemizing Emphasizing\nConstruct Score\nLLAMA\nGPT\nAsian\nHispanic\nWhite\nPERILS AND OPPORTUNITIES OF LLMS 140\nTable S47\nDeviation of LLaMA responses from human responses grouped by religion\nSurvey Construct Demographic d p\nBFI Extraversion Christianity -0.20 .038\nBFI Agreeableness Christianity 0.06 .608\nBFI Conscientiousness Christianity 0.05 .804\nBFI Neuroticism Christianity 0.05 .807\nBFI Openness Christianity 0.09 .051\nNeed for Cognition Need for Cognition Christianity 0.54 < .001\nNeed for Closure Need for Closure Christianity - -\nSystemizing and Emphasizing Systemizing Christianity -0.33 < .001\nSystemizing and Emphasizing Emphasizing Christianity 0.21 < .001\nRational-Experiential Inventory Rational Christianity 0.67 < .001\nRational-Experiential Inventory Experiential Christianity 0,04 .908\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism Christianity 0.01 .999\nBFI Extraversion Atheist -0.42 < .001\nBFI Agreeableness Atheist -0.16 .039\nBFI Conscientiousness Atheist -0.10 .295\nBFI Neuroticism Atheist 0.11 .402\nBFI Openness Atheist 0.24 < .001\nNeed for Cognition Need for Cognition Atheist 0.69 < .001\nNeed for Closure Need for Closure Atheist - -\nSystemizing and Emphasizing Systemizing Atheist -0.25 < .001\nSystemizing and Emphasizing Emphasizing Atheist 0.14 .008\nRational-Experiential Inventory Rational Atheist 0.80 < .001\nRational-Experiential Inventory Experiential Atheist -0.20 .013\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism Atheist -1.10 < .001\nContinued on next page\nPERILS AND OPPORTUNITIES OF LLMS 141\nTable S47 – continued from previous page\nSurvey Construct Demographic d p\nBFI Extraversion Agnostic -0.32 < .001\nBFI Agreeableness Agnostic -0.10 .256\nBFI Conscientiousness Agnostic -0.13 .140\nBFI Neuroticism Agnostic 0.18 .073\nBFI Openness Agnostic 0.23 < .001\nNeed for Cognition Need for Cognition Agnostic 0.69 < .001\nNeed for Closure Need for Closure Agnostic - -\nSystemizing and Emphasizing Systemizing Agnostic -0.31 < .001\nSystemizing and Emphasizing Emphasizing Agnostic .22 < .001\nRational-Experiential Inventory Rational Agnostic 0.74 < .001\nRational-Experiential Inventory Experiential Agnostic 0.06 .736\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism Agnostic -0.87 < .001\nNote: Positive values indicate a higher average response for religious groups compared to LLaMA.\nSignificance determined via Dunnett’s Test.\nFigure S52\nComparing LLaMA and ChatGPT against Humans by Religion for responses on the BFI\n1\n2\n3\n4\n5\nExtraversion Agreeableness Conscientiousness Neuroticism Openness\nConstruct Score\nLLAMA\nGPT\nAgnostic\nAtheist\nChristianity\nPERILS AND OPPORTUNITIES OF LLMS 142\nFigure S53\nComparing LLaMA and ChatGPT against Humans by Religion for responses on the Need for Cognition\nquestionnaire\n1\n2\n3\n4\n5\nNFC\nConstruct Score\nLLAMA\nGPT\nAgnostic\nAtheist\nChristianity\nFigure S54\nComparing LLaMA and ChatGPT against Humans by Religion for responses on the Rational-Experiential\nInventory\n1\n2\n3\n4\n5\nRational Experiential\nConstruct Score\nLLAMA\nGPT\nAgnostic\nAtheist\nChristianity\nPERILS AND OPPORTUNITIES OF LLMS 143\nFigure S55\nComparing LLaMA and ChatGPT against Humans by Religion for responses on the\nRight-Wing-Authoritarianism Scale\n0\n2\n4\n6\nRWA\nConstruct Score\nLLAMA\nGPT\nAgnostic\nAtheist\nChristianity\nFigure S56\nComparing LLaMA and ChatGPT against Humans by Religion for responses on the\nSystemizing-Emphasizing Scale\n1\n2\n3\n4\nSystemizing Emphasizing\nConstruct Score\nLLAMA\nGPT\nAgnostic\nAtheist\nChristianity\nPERILS AND OPPORTUNITIES OF LLMS 144\nTable S49\nDeviation of LLaMA responses from human responses grouped by political orientation\nSurvey Construct Demographic d p\nBFI Extraversion Liberal -0.29 .002\nBFI Agreeableness Liberal 0.028 .926\nBFI Conscientiousness Liberal -0.11 .222\nBFI Neuroticism Liberal 0.25 .008\nBFI Openness Liberal 0.26 < .001\nNeed for Cognition Need for Cognition Liberal 0.67 < .001\nNeed for Closure Need for Closure Liberal -1.40 < .001\nSystemizing and Emphasizing Systemizing Liberal -0.36 < .001\nSystemizing and Emphasizing Emphasizing Liberal 0.30 < .001\nRational-Experiential Inventory Rational Liberal 0.76 < .001\nRational-Experiential Inventory Experiential Liberal -0.04 .931\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism Liberal -1.23 < .001\nBFI Extraversion Moderate -0.26 .005\nBFI Agreeableness Moderate -0.03 .939\nBFI Conscientiousness Moderate 0.01 .998\nBFI Neuroticism Moderate 0.03 .952\nBFI Openness Moderate 0.13 .052\nNeed for Cognition Need for Cognition Moderate 0.56 < .001\nNeed for Closure Need for Closure Moderate - -\nSystemizing and Emphasizing Systemizing Moderate -0.30 < .001\nSystemizing and Emphasizing Emphasizing Moderate 0.19 < .001\nRational-Experiential Inventory Rational Moderate 0.67 < .001\nRational-Experiential Inventory Experiential Moderate 0.000 1.000\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism Moderate -0.48 < .001\nContinued on next page\nPERILS AND OPPORTUNITIES OF LLMS 145\nTable S49 – continued from previous page\nSurvey Construct Demographic d p\nBFI Extraversion Conservative -0.25 .016\nBFI Agreeableness Conservative -0.15 .067\nBFI Conscientiousness Conservative 0.05 .806\nBFI Neuroticism Conservative -0.08 .577\nBFI Openness Conservative -0.000 1.000\nNeed for Cognition Need for Cognition Conservative 0.58 < .001\nNeed for Closure Need for Closure Conservative - -\nSystemizing and Emphasizing Systemizing Conservative -0.23 < .001\nSystemizing and Emphasizing Emphasizing Conservative 0.08 .187\nRational-Experiential Inventory Rational Conservative 0.75 < .001\nRational-Experiential Inventory Experiential Conservative 0.03 .966\nRight-Wing-Authoritarianism Right-Wing-Authoritarianism Conservative 0.62 < .001\nNote: Positive values indicate a higher average response for political groups compared to LLaMA.\nSignificance determined via Dunnett’s Test.\nFigure S57\nComparing LLaMA and ChatGPT against Humans by Political Orientation for responses on the BFI\n1\n2\n3\n4\n5\nExtraversion Agreeableness Conscientiousness Neuroticism Openness\nConstruct Score\nLLAMA\nGPT\nConservative\nLiberal\nModerate\nPERILS AND OPPORTUNITIES OF LLMS 146\nFigure S58\nComparing LLaMA and ChatGPT against Humans by Political Orientation for responses on the Need for\nCognition questionnaire\n1\n2\n3\n4\n5\nNFC\nConstruct Score\nLLAMA\nGPT\nConservative\nLiberal\nModerate\nFigure S59\nComparing LLaMA and ChatGPT against Humans by Political Orienation for responses on the Need for\nClosure Survey\n1\n2\n3\n4\n5\nnfcc\nConstruct Score\nLLAMA\nGPT\nLiberal\nPERILS AND OPPORTUNITIES OF LLMS 147\nFigure S60\nComparing LLaMA and ChatGPT against Humans by Political Orientation for responses on the\nRational-Experiential Inventory\n1\n2\n3\n4\n5\nRational Experiential\nConstruct Score\nLLAMA\nGPT\nConservative\nLiberal\nModerate\nFigure S61\nComparing LLaMA and ChatGPT against Humans by Political Orientaition for responses on the\nRight-Wing-Authoritarianism Scale\n2\n4\n6\nRWA\nConstruct Score\nLLAMA\nGPT\nConservative\nLiberal\nModerate\nPERILS AND OPPORTUNITIES OF LLMS 148\nFigure S62\nComparing LLaMA and ChatGPT against Humans by Political Orientation for responses on the\nSystemizing-Emphasizing Scale\n1\n2\n3\n4\nSystemizing Emphasizing\nConstruct Score\nLLAMA\nGPT\nConservative\nLiberal\nModerate\nPERILS AND OPPORTUNITIES OF LLMS 149\nGeneral Summary\nIn total, we showed that by using a simple pipeline that fully runs on consumer-grade\nhardware, researchers can conduct LLM analyses with open LLMs. During moral sentiment\nanalysis, our LLaMA model achieved marginally higher accuracies and misclassified some moral\nsentiments significantly less frequently compared with ChatGPT. For survey responses, we found\ncomparable output quality with similar response distributions. In some cases, LLaMA achieved\nnotably more natural response patterns and less bias against human demographics (especially on\npersonality). Importantly, these results were achieved with a vastly smaller model. Several larger\nLLaMA alternatives that still run on single GPUs are available (e.g., 13B and 30B parameter\nversions) and thus improved performances are possible even on limited hardware.\nThis has relevant implications. Firstly, using open LLMs researchers can control what\nexact model they use, how it was trained, and on what data, increasing transparency.\nAdditionally, the version applied by a researcher is permanent, meaning it cannot be altered by\ncorporate decisions, policies, or general model updates. Thus, researchers can easily maintain the\nexact model version by simply uploading the model weights used in their study and thus facilitate\nreproducibility. Lastly, since the models were run locally instead of through a commercial API, no\ncosts were associated with the model outputs. That is, we did not pay for the LLaMA text\nannotations and survey responses. In comparison, the ChatGPT model (3.5-turbo) costs\n$0.002/1000 tokens ($0.012-0.016/1000 tokens after fine-tuning) and ChatGPT4 costs between\n$0.03 to $0.12/1000 tokens, which can quickly scale up for large data sets or when collecting\nresponses repeatedly to investigate response variances. However, it should be noted here that\n(non-fine-tuned) ChatGPT3.5 is, currently, quite affordable. In total, we spend around $100\ndollars for the different analyses (in total approximately 90,000 data points). On the other hand,\nfine-tuning currently significantly increases costs. We spend as much on a single fine-tuning task\nas for all other tasks combined (≈$100). Additionally, the cost depends on the number of tokens\nand thus the length of analyzed and generated text. In our analyses, we focused on short\nquestionnaire items or short social media texts and generated very short outputs in the form of\nsingle-digit or word ratings. Since ChatGPT takes the length of both instructions and generated\noutputs into account, affordability strongly depends on both the model quality (e.g., GPT4 being\nPERILS AND OPPORTUNITIES OF LLMS 150\n15-100 times more expensive than GPT3.5) and data size (total length of all inputs). Researchers\nshould thus consider if the long-term costs for large-scale projects using high-quality models (e.g.,\nsocial media studies using millions of messages) are justified.\nPERILS AND OPPORTUNITIES OF LLMS 151\nSupplementary Materials’ References\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S.,\nIrving, G., Isard, M., et al. (2016). Tensorflow: A system for large-scale machine\nlearning. 12th {USENIX}Symposium on Operating Systems Design and\nImplementation ({OSDI}16), 265–283.\nAbdel-Khalek, A. M. (2007). Assessment of intrinsic religiosity with a single-item measure\nin a sample of arab muslims.Journal of Muslim Mental Health, 2(2), 211–215.\nAfhami, R., Mohammadi-Zarghan, S., & Atari, M. (2017). Self-rating of religiosity (srr) in\niran: Validity, reliability, and associations with the big five.Mental Health, Religion\n& Culture, 20(9), 879–887.\nAndri et mult. al., S. (2021).DescTools: Tools for descriptive statistics[R package version\n0.99.44]. https://cran.r-project.org/package=DescTools\nAnonymous. (2023). Text-generation-webui.\nAtari, M., Haidt, J., Graham, J., Koleva, S., Stevens, S. T., & Dehghani, M. (2023).\nMorality beyond the weird: How the nomological network of morality varies across\ncultures. Journal of Personality and Social Psychology.\nAtari, M., Omrani, A., & Dehghani, M. (2023). Contextualized construct representation:\nLeveraging psychometric scales to advance theory-driven text analysis.\nhttps://psyarxiv.com/m93pd\nBaron-Cohen, S., Richler, J., Bisarya, D., Gurunathan, N., & Wheelwright, S. (2003). The\nsystemizing quotient: An investigation of adults with asperger syndrome or\nhigh–functioning autism, and normal sex differences.Philosophical Transactions of\nthe Royal Society of London. Series B: Biological Sciences, 358(1430), 361–374.\nBoyd, R. L., & Schwartz, H. A. (2021). Natural language analysis and the psychology of\nverbal behavior: The past, present, and future states of the field.Journal of\nLanguage and Social Psychology, 40(1), 21–41.\nPERILS AND OPPORTUNITIES OF LLMS 152\nCacioppo, J. T., & Petty, R. E. (1982). The need for cognition.Journal of personality and\nsocial psychology, 42(1), 116.\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S.,\nZhuang, Y., Gonzalez, J. E., Stoica, I., & Xing, E. P. (2023, March). Vicuna: An\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality.\nhttps://lmsys.org/blog/2023-03-30-vicuna/\nChollet, F., et al. (2015).Keras. https://github.com/fchollet/keras\nCortes, C., & Vapnik, V. (1995). Support-vector networks.Machine learning, 20(3),\n273–297.\nDettmers, T., & Zettlemoyer, L. (2023). The case for 4-bit precision: K-bit inference scaling\nlaws.\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep\nbidirectional transformers for language understanding.\nhttps://arxiv.org/abs/1810.04805\nDunnett, C. W. (1955). A multiple comparison procedure for comparing several treatments\nwith a control.Journal of the American Statistical Association, 50(272), 1096–1121.\nFossati, A., Borroni, S., Marchione, D., & Maffei, C. (2011). The big five inventory (bfi).\nEuropean Journal of Psychological Assessment.\nFox, J., & Weisberg, S. (2019).An R companion to applied regression(Third). Sage.\nhttps://socialsciences.mcmaster.ca/jfox/Books/Companion/\nGarten, J., Hoover, J., Johnson, K. M., Boghrati, R., Iskiwitch, C., & Dehghani, M. (2018).\nDictionaries and distributions: Combining expert knowledge and large scale textual\ndata content analysis: Distributed dictionary representation.Behavior research\nmethods, 50, 344–361.\nGelfand, M. J., Raver, J. L., Nishii, L., Leslie, L. M., Lun, J., Lim, B. C., Duan, L.,\nAlmaliach, A., Ang, S., Arnadottir, J., et al. (2011). Differences between tight and\nloose cultures: A 33-nation study.science, 332(6033), 1100–1104.\nPERILS AND OPPORTUNITIES OF LLMS 153\nGoutte, C., & Gaussier, E. (2005). A probabilistic interpretation of precision, recall and\nf-score, with implication for evaluation.European conference on information\nretrieval, 345–359.\nLevene, H. (1960). Robust tests for equality of variances.Contributions to probability and\nstatistics, 278–292.\nOpenAI. (2023, June 6).Openai python package(Version 0.27.8).\nhttps://github.com/openai/openai-python\nOyserman, D. (1993). The lens of personhood: Viewing the self and others in a\nmulticultural society.Journal of personality and social psychology, 65(5), 993.\nPacini, R., & Epstein, S. (1999). The relation of rational and experiential information\nprocessing styles to personality, basic beliefs, and the ratio-bias phenomenon.\nJournal of personality and social psychology, 76(6), 972.\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O.,\nBlondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., et al. (2011). Scikit-learn:\nMachine learning in python.Journal of machine learning research, 12(Oct),\n2825–2830.\nPennebaker, J. W., Boyd, R. L., Jordan, K., & Blackburn, K. (2015).The development and\npsychometric properties of liwc2015(tech. rep.).\nR Core Team. (2021).R: A language and environment for statistical computing. R\nFoundation for Statistical Computing. Vienna, Austria. https://www.R-project.org/\nSchwartz, S. H. (2003). A proposal for measuring value orientations across nations.\nQuestionnaire package of the european social survey, 259(290), 261.\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., &\nHashimoto, T. B. (2023). Stanford alpaca: An instruction-following llama model.\nTrager, J., Ziabari, A. S., Davani, A. M., Golazazian, P., Karimi-Malekabadi, F.,\nOmrani, A., Li, Z., Kennedy, B., Reimer, N. K., Reyes, M., et al. (2022). The moral\nfoundations reddit corpus. https://arxiv.org/abs/2208.05545\nPERILS AND OPPORTUNITIES OF LLMS 154\nTurc, I., Chang, M.-W., Lee, K., & Toutanova, K. (2019). Well-read students learn better:\nOn the importance of pre-training compact models.\nhttps://arxiv.org/abs/1908.08962v2\nVan Rossum, G., & Drake, F. L. (2009).Python 3 reference manual. CreateSpace.\nWebster, D. M., & Kruglanski, A. W. (1994). Need for closure scale.Journal of Personality\nand Social Psychology.\nZakrisson, I. (2005). Construction of a short version of the right-wing authoritarianism\n(rwa) scale.Personality and individual differences, 39(5), 863–872.",
  "topic": "Psychological research",
  "concepts": [
    {
      "name": "Psychological research",
      "score": 0.5130447149276733
    },
    {
      "name": "Psychology",
      "score": 0.45768988132476807
    },
    {
      "name": "Psyche",
      "score": 0.4559217095375061
    },
    {
      "name": "Social psychology",
      "score": 0.37866684794425964
    },
    {
      "name": "Psychoanalysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I63966007",
      "name": "Massachusetts Institute of Technology",
      "country": "US"
    }
  ],
  "cited_by": 10
}