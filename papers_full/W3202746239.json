{
  "title": "SlovakBERT: Slovak Masked Language Model",
  "url": "https://openalex.org/W3202746239",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5042084033",
      "name": "Matúš Pikuliak",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5065441343",
      "name": "Štefan Grivalský",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2279133053",
      "name": "Martin Konopka",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5088719031",
      "name": "Miroslav Blšták",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2498184073",
      "name": "Martin Tamajka",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3204683222",
      "name": "Viktor Bachratý",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2032163927",
      "name": "Marian Simko",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5091488931",
      "name": "Pavol Balážik",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2070950883",
      "name": "Michal Trnka",
      "affiliations": [
        "HiETA Technologies (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A3201784067",
      "name": "Filip Uhlárik",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W3093871477",
    "https://openalex.org/W2970049541",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W3031245789",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2971207485",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2251869843",
    "https://openalex.org/W3104667152",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2250921181",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3088592174",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3039695075",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2973212832",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3033940819",
    "https://openalex.org/W3136336051",
    "https://openalex.org/W2973088264",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W2970925270",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3156159991",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W3099668342",
    "https://openalex.org/W3167303983",
    "https://openalex.org/W3105220303",
    "https://openalex.org/W4299574851",
    "https://openalex.org/W3008110149",
    "https://openalex.org/W2898879711",
    "https://openalex.org/W3173311019",
    "https://openalex.org/W2278629362",
    "https://openalex.org/W3037109418",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3214520975",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3033862527",
    "https://openalex.org/W3018647120",
    "https://openalex.org/W2971863715",
    "https://openalex.org/W2999168658",
    "https://openalex.org/W3097571385",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W3100806282",
    "https://openalex.org/W2792789177",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3098749165",
    "https://openalex.org/W3103187652",
    "https://openalex.org/W2986154550"
  ],
  "abstract": "Matúš Pikuliak, Štefan Grivalský, Martin Konôpka, Miroslav Blšták, Martin Tamajka, Viktor Bachratý, Marian Simko, Pavol Balážik, Michal Trnka, Filip Uhlárik. Findings of the Association for Computational Linguistics: EMNLP 2022. 2022.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 7156–7168\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nSlovakBERT: Slovak Masked Language Model\nMatúš Pikuliak and Štefan Grivalský and Martin Konôpka and Miroslav Blšták\nMartin Tamajka and Viktor Bachratý and Marián Šimko\nKempelen Institute of Intelligent Technologies\nname.surname@kinit.sk\nPavol Balážik and Michal Trnka and Filip Uhlárik\nGerulata Technologies\nname.surname@gerulata.com\nAbstract\nWe introduce a new Slovak masked language\nmodel called SlovakBERT. This is to our best\nknowledge the first paper discussing Slovak\ntransformers-based language models. We eval-\nuate our model on several NLP tasks and\nachieve state-of-the-art results. This evalua-\ntion is likewise the first attempt to establish a\nbenchmark for Slovak language models. We\npublish the masked language model, as well as\nthe fine-tuned models for part-of-speech tag-\nging, sentiment analysis and semantic textual\nsimilarity.\n1 Introduction\nFine-tuning pre-trained large-scale language mod-\nels (LMs) is the dominant paradigm of current NLP.\nLMs proved to be a versatile technology that can\nhelp to improve performance for an array of NLP\ntasks, such as parsing, machine translation, text\nsummarization, sentiment analysis, semantic simi-\nlarity etc. The state-of-the-art performance makes\nLMs attractive for any language community that\nwants to develop its NLP capabilities. In this paper,\nwe concern ourselves with the Slovak language and\naddress the lack of language models, as well as the\nlack of established evaluation standards for this\nlanguage.\nIn this paper, we introduce a new Slovak-only\ntransformers-based language model called Slovak-\nBERT1. Although several multilingual models al-\nready support Slovak, we believe that developing\nSlovak-only models is still important, as it can lead\nto better results and more compute and memory-\nwise efficient processing of the Slovak language.\nSlovakBERT has RoBERTa architecture (Liu et al.,\n2019) and it was trained with a Web-crawled cor-\npus.\nSince no standard evaluation benchmark for Slo-\nvak exists, we created our own set of tests mainly\n1Available at https://github.com/gerulata/\nslovakbert\nfrom pre-existing datasets. We believe that our\nevaluation methodology might serve as a standard\nbenchmark for the Slovak language in the future.\nWe evaluateSlovakBERT with this benchmark and\nwe also compare it to other available (mainly mul-\ntilingual) LMs and other existing approaches. The\ntasks we use for evaluation are: part-of-speech tag-\nging, semantic textual similarity, sentiment analysis\nand document classification. We also publish the\nbest-performing models for selected tasks. These\nmight be used by other Slovak researchers or NLP\npractitioners in the future as strong baselines.\nOur main contributions in this paper are:\n• We published a Slovak-only LM trained on a\nWeb corpus.\n• We established an evaluation methodology for\nthe Slovak language and we apply it on our\nmodel, as well as on other LMs.\n• We published several fine-tuned models based\non our LM, namely a part-of-speech tagger,\na sentiment analysis model and a sentence\nembedding model.\n• We published several additional datasets for\nmultiple tasks, namely sentiment analysis test\nsets and semantic similarity translated datasets\n(including a manually translated test set).\nThe rest of this paper is structured as follows: In\nSection 2 we discuss related work about language\nmodels and their language mutations. In Section 3\nwe describe the corpus crawling efforts and how\nwe train SlovakBERT with the resulting corpus. In\nSection 4 we evaluate the model with four NLP\ntasks.\n2 Related Work\n2.1 Language Models\nLMs today are commonly based on self-attention\nlayers called transformers (Vaswani et al., 2017).\nDespite the common architecture, the models might\n7156\ndiffer in the details of their implementation, as well\nas in the task they are trained with (Xia et al., 2020).\nPerhaps the most common task is the so-called\nmasked language modeling (Devlin et al., 2019a),\nwhere randomly selected parts of text are masked\nand the model is expected to fill these parts with\nthe original tokens. Masked language models are\nuseful mainly as backbones for further fine-tuning.\nAnother approach is to train generative autoregres-\nsive models (Radford et al., 2019), which always\npredict the next word in a sequence, which can\nbe used for various text generation tasks. Variants\nof LMs exist that attempt to make them more ef-\nficient (Clark et al., 2020; Jiao et al., 2020), able\nto handle longer sentences (Beltagy et al., 2020) or\nfulfill various other requirements.\n2.2 Availability in Different Languages\nEnglish is the most commonly used language in\nNLP and a de facto standard for experimental\nwork. Most of the proposed LM variants are in-\ndeed trained and evaluated only on English. Other\nlanguages usually have at most only a few LMs\ntrained, usually with a very safe choice of model\narchitecture (e.g. BERT or RoBERTa). Languages\nwith available native models are, to name only a\nfew, French (Martin et al., 2020), Dutch (Delobelle\net al., 2020), Greek (Koutsikakis et al., 2020), Ara-\nbic (Antoun et al., 2020), Czech (Sido et al., 2021)\nor Polish (Dadas et al., 2020).\nThere is no Slovak-specific large-scale LM avail-\nable so far. There is a Slovak version of WikiB-\nERT model (Pyysalo et al., 2021), but it is trained\nonly on texts from Wikipedia, which is not a large\nenough corpus for proper language modeling at this\nscale. The limitations of this model will be shown\nin the results as well.\n2.3 Multilingual Language Models\nMultilingual LMs are sometimes proposed as an al-\nternative to training language-specific LMs. These\nLMs can handle more than one language, in prac-\ntice often more than 100. Training them is more ef-\nficient than training separate models for all the lan-\nguages. Additionally, cross-lingual transfer learn-\ning might improve the performance with the lan-\nguages being able to learn from each other. This is\nespecially beneficial for low-resource languages.\nThe first large-scale multilingual LM is\nMBERT (Devlin et al., 2019a) trained on 104 lan-\nguages. The authors observed that by simply expos-\ning the model to data from multiple languages, the\nmodel was able to discover the multilingual signal\nand it spontaneously developed interesting cross-\nlingual capabilities, i.e. sentences from different\nlanguages with similar meanings also have simi-\nlar representations. Other models explicitly use\nmultilingual supervision, e.g. dictionaries, parallel\ncorpora or machine translation systems (Conneau\nand Lample, 2019; Huang et al., 2019). XLM-\nR (Conneau et al., 2020) pushed the performance\nof multilingual LMs even further by increasing the\nscale of training by using Web-crawled data and a\nlarger amount of compute.\n3 Training\nIn this section, we describe our own Slovak masked\nlanguage model – SlovakBERT, the data that were\nused for training, the architecture of the model and\nhow it was trained.\n3.1 Data\nWe used a combination of available corpora and\nour own Web-crawled corpus as our training data.\nThe available corpora we used were: Wikipedia\n(326MB of text), Open Subtitles (415MB) and OS-\nCAR 2019 corpus (4.6GB). We crawled .sk top-\nlevel domain webpages, applied language detection\nand extracted the title and the main content of each\npage as clean text without HTML tags (17.4GB).\nThe text was then processed with the following\nsteps:\n• URL and email addresses were replaced with\nspecial tokens.\n• Elongated punctuation was reduced, i.e. if\nthere were sequences of the same punctuation\nmark, these were reduced to one mark (e.g. --\nto -).\n• Markdown syntax was deleted.\n• All text content in braces {.} was eliminated\nto reduce the amount of markup and program-\nming language text.\nWe segmented the resulting corpus into sen-\ntences and removed duplicates to get 181.6M\nunique sentences. In total, the final corpus has\n19.35GB of text.\n3.2 Model Architecture and Training\nThe model itself is a RoBERTa model (Liu et al.,\n2019). The details of the architecture are shown\nin Table 1 in the SlovakBERT column. We use\n7157\nBPE (Sennrich et al., 2016) tokenizer with the vo-\ncabulary size of 50264. The model was trained\nfor 300k training steps (≈70 epochs) with a batch\nsize of 512. Each epoch consists of approximately\n4277 training steps. Samples were limited to a\nmaximum of 512 tokens and for each sample, we\nfit as many full sentences as possible. We used\nAdam optimization algorithm (Kingma and Ba,\n2015) with 5 ×10−4 learning rate and 10k warmup\nsteps. Dropout (dropout rate 0.1) and weight decay\n(λ= 0.01) were used for regularization. We used\nfairseq (Ott et al., 2019) library for training,\nwhich took approximately 248 hours on 4 NVIDIA\nA100 GPUs. We used 16-bit float precision.\n4 Evaluation\nIn this section, we describe the evaluation method-\nology and results for SlovakBERT and other LMs.\nWe conducted the evaluation on four different tasks:\npart-of-speech tagging, semantic textual similarity,\nsentiment analysis and document classification. For\neach task, we introduce the dataset that is used, var-\nious baseline solutions, the LM-based approach we\ntook and the final results for the task. For some\ntasks (part-of-speech tagging and semantic textual\nsimilarity) we also performed layer-wise model\nanalysis.\n4.1 Evaluated Language Models\nWe evaluate and compare several LMs that support\nSlovak language to some extent:\nXLM-R (Conneau et al., 2020) - XLM-R is a\nsuite of multilingual RoBERTa-style LMs. The\nmodels support 100 languages, including Slovak.\nTraining data are based on CommonCrawl Web-\ncrawled corpus. The Slovak part has 23.2 GB (3.5B\ntokens). The XLM-R models differ in their size,\nranging from Base model with 270M parameters\nto XXL model with 10.7B parameters.\nMBERT (Devlin et al., 2019b) - MBERT is a\nmultilingual version of the original BERT model\ntrained with Wikipedia-based corpus containing\n104 languages. The authors do not mention the\namount of data for each language, but considering\nthe size of Slovak Wikipedia, we assume that the\nSlovak part has tens of millions of tokens.\nWikiBERT (Pyysalo et al., 2021) - WikiBERT\nis a series of monolingual BERT-style models\ntrained on dumps of Wikipedia. The Slovak model\nwas trained with 39M tokens.\nNote that both XLM-R and MBERT models\nwere trained in a cross-lingually unsupervised man-\nner, i.e. no additional signal about how sentences\nor words from different languages relate to each\nother was provided. The models were trained with\na multilingual corpora only, although language bal-\nancing was performed.\nIn Table 1 we provide basic quantitative mea-\nsures for all the models. We compare their ar-\nchitecture and training data, and we also measure\ntokenization productivity (how many tokens are\ngenerated from given text) on Universal Depen-\ndencies (Nivre et al., 2020) train set. We show the\naverage length of tokens for each model. Longer\ntokens are considered to be better because they can\nbe more semantically meaningful and also because\nthey are more computationally efficient. We also\nshow how many unique tokens were used (effective\nvocabulary) for the tokenization of this particular\ndataset. Multilingual LMs have a smaller portion\nof their vocabulary used since they contain many\ntokens useful mainly for other languages, but not\nfor Slovak. These tokens are effectively redundant\nfor Slovak text processing.\n4.2 Part-of-Speech Tagging\nThe goal of part-of-speech (POS) tagging is to as-\nsign a certain POS tag to each word. This task\nmainly evaluates the syntactic capabilities of the\nmodels.\n4.2.1 Data\nWe use Slovak Dependency Treebank from Uni-\nversal Dependencies dataset (Zeman, 2017; Nivre\net al., 2020) (UD). It contains annotations for both\nUniversal (UPOS, 17 tags) and Slovak-specific\n(XPOS, 19 tags) POS tagsets. XPOS uses a more\ncomplicated system and it encodes not only POS\ntags, but also other morphological categories in the\nlabel. In this work, we only use the first letter from\neach XPOS label, which corresponds to a typical\nPOS tag. The tagsets and their relations are shown\nin Table 8.\n4.2.2 Previous work\nSince Slovak is an official part of the UD dataset,\nsystems that attempt to cover multiple or all UD\nlanguages often support Slovak as well. The follow-\ning systems were trained on UD data and support\nboth UPOS and XPOS tagsets:\n7158\nModel SlovakBERT XLM-R-Base XLM-R-Large MBERT WikiBERT\nArchitecture RoBERTa RoBERTa BERT BERT\nNum. layers 12 12 24 12 12\nNum. attention head 12 12 16 12 12\nHidden size 768 768 1024 768 768\nNum. parameters 125M 278M 560M 178M 102M\nLanguages 1 100 100 104 1\nTraining dataset size (tokens) 4.6B 167B n/a 39M\nSlovak dataset size (tokens) 4.6B 3.2B 25-50M 39M\nV ocabulary size 50K 250K 120K 20K\nUniversal Dependencies train set tokenization\nAverage token length (chars) 3.23 2.84 2.40 2.70\nAverage word length (tokens) 1.43 1.63 1.93 1.71\nEffective vocabulary 16.6K 9.6K 6.7K 5.8K\nEffective vocabulary (%) 33.05 3.86 5.62 29.10\nTable 1: Basic statistics about the evaluated LMs.\nUDPipe 2 (Straka, 2018) - A deep learning\nmodel based on multilayer bidirectional LSTM\narchitecture with pre-trained Slovak word embed-\ndings. The model supports multiple languages, but\nthe models themselves are monolingual.\nStanza (Qi et al., 2020) - Stanza is a very similar\nmodel to UDPipe, it is also based on multilayer\nbidirectional LSTM with pre-trained word embed-\ndings.\nTrankit (Nguyen et al., 2021) - Trankit is\nbased on adapter-style fine-tuning (Bapna and\nFirat, 2019) of XLM-R-Base. The adapters are\nfine-tuned for specific languages and they are able\nto handle multiple tasks at the same time.\n4.2.3 Our Fine-Tuning\nWe use a standard setup for fine-tuning the LMs\nfor token classification. The final layer of an LM\nthat is used to predict the masked tokens is dis-\ncarded. A classifier linear layer with dropout and\nsoftmax activation function is used in its place to\ngenerate a probability vector for each token. The\nloss function for the batch of samples is defined\nas an average cross-entropy across all the tokens.\nNote that there is a discrepancy between what we\nperceive as words and what the models use as to-\nkens. Some words might be tokenized into multiple\ntokens. In that case, we only make the prediction\non the first token, and the final classifier layer is not\napplied to the subsequent tokens for this word. We\nuse Hugging Face Transformers library\nfor LM fine-tuning.\nModel UPOS XPOS\nUDPipe 2.0 92.83 94.74\nUDPipe 2.6 97.30 97.87\nStanza 96.03 97.29\nTrankit 97.85 98.03\nWikiBERT 94.41 96.54\nMBERT 97.50 98.03\nXLM-R-Base 97.61 98.23\nXLM-R-Large 97.96 98.34\nSlovakBERT 97.84 98.37\nTable 2: Results for POS tagging (accuracy).\n4.2.4 Results\nWe have performed a random hyperparameter\nsearch with SlovakBERT. The range of individual\nhyperparameters is shown in Table 6. We have\nfound out that weight decay is a beneficial regu-\nlarization technique, while label smoothing proved\nitself to be inappropriate for our case. Other hy-\nperparameters showed to have very little reliable\neffect, apart from the learning rate, which proved to\nbe very sensitive. We have not repeated this tuning\nfor other LMs, instead, we only tuned the learning\nrate. We have found out that it is appropriate to use\na learning rate of 1 ×10−5 for all the models, but\nXLM-R-Large. XLM-R-Large, the biggest model\nwe tested, needs a smaller learning rate of1×10−6.\nThe results for POS tagging are shown in Ta-\nble 2. We report accuracy for both XPOS and\nUPOS tagsets. WikiBERT seems to be the worst-\nperforming LM, probably because of its small train-\ning set. SlovakBERT seems to be on par with larger\nXLM-R-Large. Other models lag behind slightly.\nFrom the existing solutions, only transformers-\nbased Trankit seems to be able to keep up.\nWe measured the POS performance for Slovak-\nBERT checkpoints (a checkpoint was made each\n1000 steps) as well to see how soon the model ac-\n7159\n0 25 50 75 100 125\nSteps (thousands)\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Accuracy\n1 2 3 4 5 6 7 8 9 101112\nLayer\n20\n40\n60\n80\n100\n120\nSteps (thousands)\nFigure 1: Analysis of POS tagging learning dynamics.\nLeft: Accuracy after fine-tuning the different check-\npoints. Right: Accuracy of probes on all the layers of\ndifferent checkpoints. Each line represents one check-\npoint and its results on all the layers.\nquired basic morphosyntactic capabilities. We can\nsee in Figure 1, that the model was saturated w.r.t\nPOS performance quite soon, after approximately\n15k steps (≈3.5 epochs). We stopped the analysis\nafter the first 125k steps ( ≈30 epochs) since the\nresults seemed to be stable.\n4.2.5 Probing\nWe performed probing by training linear classi-\nfier on representations from individual layers of\nfrozen models (Belinkov et al., 2017). We show\nthe performance of these probes for all the layers\nfor checkpoints from SlovakBERT training in Fig-\nure 1. The probing is done on models that are not\nfine-tuned for POS tagging. Layer-wise, the perfor-\nmance peaks quite soon at layer 6 and then plateaus.\nThe last layers even have degraded performance.\nThe results are in accord with the current under-\nstanding of how LMs work, i.e. that they process\ntext in a bottom-up manner and the morphosyn-\ntactic information needed for POS tagging is be-\ning processed mainly in the middle layers (Tenney\net al., 2019). We can also see that the performance\nfor individual layers peaks quite soon during the\ntraining, with a slight lag for earlier layers.\n4.3 Semantic Textual Similarity\nSemantic textual similarity (STS) is an NLP task\nwhere similarity between pairs of sentences is mea-\nsured. In our work, we train the LMs to generate\nsentence embeddings and then we measure how\nmuch the cosine similarity between embeddings\ncorrelates with the ground truth labels provided by\nhuman annotators. We can use the resulting mod-\nels to generate universal sentence embeddings for\nSlovak.\n4.3.1 Data\nCurrently, there is no native Slovak STS dataset.\nWe decided to machine translate existing English\ndatasets STSbenchmark (Cer et al., 2017) and\nSICK (Marelli et al., 2014) into Slovak. These\ndatasets use a ⟨0,5⟩scale that expresses the simi-\nlarity of two sentences. The meaning of individual\nsteps on this scale is shown in Table 9. We used\nM2M100 (1.2B parameters variant) machine trans-\nlation system (Fan et al., 2021). The test set from\nSTSbenchmark was manually translated by the au-\nthors. These translations were used for evaluation\nonly and are published as well.\n4.3.2 Previous Work\nNo Slovak-specific sentence embedding model has\nbeen published yet. We use a naive solution based\non Slovak word embeddings and several available\nmultilingual models for comparison:\nfastText (Bojanowski et al., 2017) - We use pre-\ntrained Slovak fastText word embeddings to gener-\nate representations for individual words. The sen-\ntence representation is an average of all its words.\nThis represents a very naive baseline since it com-\npletely omits the word order.\nLASER (Artetxe and Schwenk, 2019) - LASER\nis a model trained to generate multilingual sentence\nembeddings. It is based on an encoder-decoder\nLSTM machine translation system that is trained\nwith 93 languages. The encoder is shared across\nall the languages and as such, it is able to generate\nmultilingual representations.\nLaBSE (Feng et al., 2020) - LaBSE is an\nMBERT model fine-tuned with parallel corpus to\nproduce multilingual sentence representations.\nXLM-REN (Reimers and Gurevych, 2020) -\nXLM-R model fine-tuned with English STS-related\ndata (SNLI, MNLI and STSbenchmark datasets).\nThis is a zero-shot cross-lingual learning setup,\ni.e. no Slovak data are used and only English fine-\ntuning is done.\n4.3.3 Our Fine-Tuning\nWe use a setup similar to (Reimers and Gurevych,\n2020). A pre-trained LM is used to initialize a\nSiamese network. Both branches of the network\nare identical LMs with a mean-pooling layer at\nthe top that generates the final sentence embed-\ndings. The embeddings from the two sentences are\ncompared using cosine similarity. The network is\n7160\nTranslation\nModel Manual M2M100\nfastText 0.366 0.383\nLASER 0.706 0.711\nLaBSE 0.730 0.739\nXLM-REN 0.804 0.801\nWikiBERT 0.652 0.673\nMBERT 0.726 0.734\nXLM-R-Base 0.785 0.791\nXLM-R-Large 0.794 0.790\nSlovakBERT 0.793 0.799\nTable 3: Spearman correlation between cosine similarity\nof generated representations and desired similarities on\nSTSbenchmark dataset translated to Slovak.\ntrained as a regression model, i.e. the final com-\nputed similarity is compared with the ground truth\nsimilarity with mean squared error loss function.\nWe use SentenceTransformers library for\nthe fine-tuning.\n4.3.4 Results\nWe compare the systems using Spearman correla-\ntion between the cosine similarity of the generated\nsentence representations and the ground truth data.\nThe original STS datasets are using ⟨0,5⟩scale.\nWe normalize these scores to ⟨0,1⟩range so that\nthey can be directly compared to the cosine simi-\nlarities. We performed a hyperparameter search in\nthis case as well. Again, we have found out that\nthe results are quite stable across various hyper-\nparameter values, with the learning rate being the\nmost sensitive hyperparameter. The details of the\nhyperparameter tuning are shown in Table 7. We\nshow the main results in Table 3.\nWe can see that the results are fairly similar to\nPOS tagging w.r.t. how the LMs are relatively or-\ndered. The existing solutions are worse, except\nfor XLM-REN trained with English data, which\nis actually the best-performing model in our ex-\nperiments. It seems that their model fine-tuned\nwith real data without machine-translation-induced\nnoise works better, even if it has to perform the\ninference cross-lingually on Slovak data. We have\nfound out that manual translation of the test set did\nnot yield significantly different results compared\nto machine translation, despite the fact that most\nof the machine-translated samples were quite noisy\naccording to our manual inspection. This shows\nthat we can measure how good STS systems are\neven with noisy machine-translated data.\nWe also experimented with Slovak-translated\nNLI data in a way where the model was first fine-\ntuned on the NLI task and then the final STS fine-\n0 100 200 300\nSteps (thousands)\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Spearman's \n1 2 3 4 5 6 7 8 9 101112\nLayer\n50\n100\n150\n200\n250\n300\nSteps (thousands)\nFigure 2: Analysis of STS learning dynamics. Left:\nSpearman correlation after fine-tuning with various\ncheckpoints. Right: Spearman correlation on all the\nlayers with selected checkpoints. Each line represents\none checkpoint and its results on all the layers.\ntuning was performed. However, we were not able\nto outperform the purely STS fine-tuning with this\napproach and the results remained virtually the\nsame. This result is in contrast with the usual case\nfor English training, where the NLI data regularly\nimprove the results (Reimers and Gurevych, 2019).\nWe theorize that this effect might be caused by\nnoisy machine translation.\nFigure 2 shows the learning dynamics of STS.\nOn the left, we can see that the performance takes\nmuch longer to plateau than in the case of POS.\nThis shows that the model needs longer time to\nlearn about semantics. Still, we can see that the\nperformance ultimately stabilizes just below 0.8\nscore.\nWe also performed a layer-wise analysis, where\nwe analyzed which layers have the most viable\nrepresentations for this task. We conducted the\nmean-pooling at different layers and ignored all\nthe subsequent layers. We can see that the best-\nperforming layers are actually the last layers of the\nmodel.\n4.4 Sentiment Analysis\nThe goal of sentiment analysis is to identify the\naffective sentiment of a given text. It requires se-\nmantic analysis of the text, as well as a certain\namount of emotional understanding.\n4.4.1 Data\nWe use a Twitter-based dataset (Mozeti ˇc et al.,\n2016) annotated on a scale with three values: nega-\ntive, neutral and positive. Some of the tweets have\nalready been removed since the dataset was created.\nTherefore, we work with a subset of the original\ndataset.\nWe cleaned the data by removing URLs, retweet\nprefixes, hashtags, user mentions, quotes, asterisks,\nredundant whitespaces and trailing punctuation.\n7161\nWe have also deduplicated the samples, as there\nwere cases of identical samples (i.e. retweets) or\nvery similar samples (i.e. automatically generated\ntweets). These duplicates had in some cases differ-\nent labels. After the deduplication, we were left\nwith 41084 tweets with 11160 negative samples,\n6668 neutral samples and 23256 positive samples.\nAdditionally, we have also manually annotated a\nseries of test sets containing reviews from various\ndomains: accommodation, books, cars, games, mo-\nbiles and movies. Each domain has approximately\n100 manually labeled samples. These are published\nalong with this paper. They serve to check how\nwell the model behavior transfers to other domains.\nThis dataset is called Reviews in the results below,\nwhile the original Twitter-based dataset is called\nTwitter.\n4.4.2 Previous Work and Baselines\nThe original paper introducing the Twitter dataset\nintroduced an array of traditional classifiers (Naive\nBayes and 5 SVM variants) to solve the task. The\nauthors report macro-F1 score for positive and neg-\native classes only. Additionally, unlike us, they\nworked with the whole dataset. Approximately\n10K tweets have been deleted since the dataset was\nintroduced. (Pecar et al., 2019) use the same ver-\nsion of the dataset as we do. They use approaches\nbased on word embeddings and ELMO (Peters\net al., 2018) to solve the task. Note that both pub-\nlished works use cross-validation, but no canonical\ndataset split is provided in either of them.\nThere are several existing approaches we use for\ncomparison:\nNLP4SK2 - A rule-based sentiment analysis sys-\ntem for Slovak that is available online\nAmazon - We also translated the Slovak data\ninto English and used Amazon’s commercial\nsentiment analysis API and tested its performance\non our test sets.\nWe implemented several baseline classifiers that\nwere trained with the same training data as the LMs\nin our experiments:\nTF-IDF linear classifier - A perceptron trained\nwith SGD algorithm. The text is represented with\nTF-IDF using N-grams as basic text units.\n2http://arl6.library.sk/nlp4sk/webapi/\nanalyza-sentimentu\nfastText classifier - We used the built-in fastText\nclassifier with and without pre-trained Slovak word\nembedding models.\nOur STS embedding linear classifier - A\nperceptron trained with SGD algorithm. The text is\nrepresented using the sentence embedding model\nwe have trained for STS.\nWe performed a random search hyperparameter\noptimization for all the approaches.\n4.4.3 Our Fine-Tuning\nWe fine-tuned the LMs as classifiers with 3 classes.\nThe topmost layer of an LM is discarded and in-\nstead, a multilayer perceptron classifier with one\nhidden layer and dropout is applied to the repre-\nsentation of the first token. The categorical cross-\nentropy loss function is used as the loss function.\nThe class with the highest probability coming from\nthe softmax function is selected as the predicted\nlabel during inference. We use Hugging Face\nTransformers library for fine-tuning.\n4.4.4 Results\nWe report macro-F1 scores for all three classes as\nour main performance measure. The LMs were\ntrained on the Twitter dataset. We calculate the av-\nerage F1 from our Reviews dataset as an additional\nmeasure.\nAgain, we have performed a hyperparameter op-\ntimization of SlovakBERT. The results are similar\nto results from POS tagging and STS. We have\nfound out that the learning rate is the most sen-\nsitive hyperparameter and that a small amount of\nweight decay is a beneficial regularization. The\nmain results are shown in Table 4. We can see that\nwe were able to obtain better results than the re-\nsults that had been reported previously. However,\nthe comparison is not perfect, as we use slightly\ndifferent datasets for the aforementioned reasons.\nThe LMs are ordered in performance similarly\nto how they are ordered in the two previous tasks.\nSlovakBERT seems to be among the best perform-\ning models, along with the larger XLM-R-Large.\nThe LMs were also able to successfully transfer\ntheir sentiment knowledge to new domains and\nthey achieve up to 0.617 macro-F1 in the reviews\nas well. However, both Amazon commercial sen-\ntiment API and NLP4SK have even better scores,\neven though their performance on Twitter data was\nnot very impressive. This is probably caused by the\nunderlying training data they use in their systems,\n7162\nModel Twitter F1 Reviews F1\n3-class 2-class 3-class\n(Mozetiˇc et al., 2016)* - 0.682 -\n(Pecar et al., 2019)* 0.669 - -\nAmazon 0.502 0.472 0.766\nNLP4SK 0.489 0.468 0.815\nTF-IDF 0.571 0.603 0.412\nfastText 0.591 0.622 0.416\nfastText w/ emb. 0.606 0.631 0.426\nSTS embeddings 0.581 0.597 0.582\nWikiBERT 0.580 0.597 0.398\nMBERT 0.587 0.622 0.453\nXLM-R-Base 0.620 0.651 0.518\nXLM-R-Large 0.655 0.716 0.617\nSlovakBERT 0.672 0.705 0.583\nTable 4: Macro-F1 scores for sentiment analysis task.\nThe 2-class F1 score for Twitter is calculated only from\npositive and negative classes – a methodology intro-\nduced in the original dataset paper. *Indicates different\nevaluation sets.\nwhich might match our Reviews datasets more than\nthe tweets used for our fine-tuning.\n4.5 Document Classification\nThe final task which we evaluate our LMs on is a\nclassification of documents into 5 news categories.\nThe goal of this task is to ascertain how well LMs\nhandle common classification problems. We use\na Slovak Categorized News Corpus (Hladek et al.,\n2014) that contains 4.7K news articles classified\ninto 6 classes: Sports, Politics, Culture, Economy,\nHealth and World. We do not use the Culture cate-\ngory, since it contains a significantly smaller num-\nber of samples.\nUnfortunately, no existing work has used this\ndataset for document classification, so there are no\nexisting results publicly available. We use the same\nset of baselines and LM fine-tuning as in the case\nof sentiment analysis since both these tasks are\ntext classification tasks, see Section 4.4 for more\ndetails.\n4.5.1 Results\nThe main results from our experiment are shown\nin Table 5. We can see that the LMs are again the\nbest-performing approach. In this case, the results\nare quite similar with SlovakBERT being the best\nby a narrow margin. The baselines achieved sig-\nnificantly worse results. Note that our sentence\nembedding model has the worst results on this task,\nwhile it had competitive performance in sentiment\nclassification. We theorize, that the sentence em-\nbedding model was trained on sentences and is,\ntherefore, less capable of handling longer texts,\nModel F1\nTF-IDF 0.953\nfastText 0.963\nfastText w/ emb. 0.963\nSTS embeddings 0.935\nWikiBERT 0.935\nMBERT 0.985\nXLM-R-Base 0.987\nXLM-R-Large 0.985\nOur model 0.990\nTable 5: Macro-F1 scores for document classification\ntask.\ntypical for the dataset used here.\n5 Conclusions\nWe have trained and published SlovakBERT – a\nnew large-scale transformers-based Slovak masked\nlanguage model using 19.35GB of Web-crawled\nSlovak text. We proposed an evaluation bench-\nmark with multiple tasks for Slovak language and\nevaluated several models. We conclude, that Slo-\nvakBERT achieves state-of-the-art results on this\nbenchmark, but multilingual language models are\nstill competitive, especially larger but computation-\nally less efficient models such as XLM-R-Large.\nWe also release fine-tuned models for the Slovak\ncommunity.\nThe lack of evaluation benchmarks is still an\nissue for many mid-resource languages, i.e. lan-\nguages that have a sizeable corpus of text available\non the Web, but do not have annotated natural lan-\nguage understanding datasets available. Our work\nwas limited by this as well, as we were forced to\nuse datasets created by machine translation (in case\nof STS), noisy datasets (in case of sentiment analy-\nsis), or datasets with almost saturated performance\n(in case of document classification). Creating new\nhigh-quality datasets for the evaluation of Slovak\nis our future work.\n6 Limitations\nLimited performance evaluation. As we have\nalready noted in Section 5, the lack of annotated\ndata limits our ability to properly evaluate the per-\nformance of SlovakBERT. The existing datasets\nhave some issues:\n• UD for POS tagging. The performance for\nPOS tagging seems already saturated.\n• Sentiment analysis. The training data for sen-\ntiment analysis are quite noisy and many sam-\nples are misclassified. The data are also based\n7163\non Twitter and as such, they are not easily ac-\ncessible and the number of samples available\nis constantly decreasing. This is a threat to the\nreplicability of our results.\n• Semantic textual similarity. We use translated\nEnglish STS data for training. The translation\nitself is a noisy process. The data are also\nUS-centric in their nature and they might not\nexactly match the needs of Slovak speakers.\n• Document classification. The performance for\nthis task is saturated as well.\nWe addressed some issues by manually creating\nevaluation sets for both sentiment analysis and se-\nmantic textual similarity. In the future, it would\nbe appropriate to develop new datasets for higher-\nlevel NLP tasks, such as natural language inference\nor question answering.\nLimited ethical evaluation. For similar reasons\nas above, there is no evaluation of bias in the\nSlovak-processing language models. As of now,\nit is not clear how biased the models are, since eval-\nuation benchmarks were not yet designed for the\nSlovak language. We made note of this issue in the\nEthical Consideration section as well.\n7 Ethical Consideration\nSlovakBERT was trained using a Web-crawled cor-\npus. This is a common practice in current NLP,\nyet, it raises some ethical concerns. Models trained\nwith huge poorly documented corpora might en-\ncode in them various societal biases. The Slovak\ntexts written on the Web are not representative of all\nSlovak users. Certain demographic groups might\nbe underrepresented and the model might not re-\nflect them accordingly. We do not study these ef-\nfects in this work and we do not recommend using\nour model for sensitive applications without fur-\nther analysis. Unfortunately, there are no datasets,\nbenchmarks, or other resources able to measure\nthese effects in the Slovak language as of yet.\nAcknowledgments\nThis research was partially supported by the\nCentral European Digital Media Observatory\n(CEDMO), a project funded by the European Union\nunder the Contract No. 2020-EU-IA-0267.\nReferences\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020.\nAraBERT: Transformer-based model for Arabic lan-\nguage understanding. In Proceedings of the 4th Work-\nshop on Open-Source Arabic Corpora and Process-\ning Tools, with a Shared Task on Offensive Language\nDetection, pages 9–15, Marseille, France. European\nLanguage Resource Association.\nMikel Artetxe and Holger Schwenk. 2019. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transactions\nof the Association for Computational Linguistics ,\n7:597–610.\nAnkur Bapna and Orhan Firat. 2019. Simple, scal-\nable adaptation for neural machine translation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1538–\n1548, Hong Kong, China. Association for Computa-\ntional Linguistics.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Has-\nsan Sajjad, and James Glass. 2017. What do neural\nmachine translation models learn about morphology?\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 861–872, Vancouver, Canada.\nAssociation for Computational Linguistics.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. CoRR,\nabs/2004.05150.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada. Association for Computational Linguistics.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: pre-\ntraining text encoders as discriminators rather than\ngenerators. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\n7164\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances\nin Neural Information Processing Systems 32: An-\nnual Conference on Neural Information Processing\nSystems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada, pages 7057–7067.\nSlawomir Dadas, Michal Perelkiewicz, and Rafal\nPoswiata. 2020. Pre-training polish transformer-\nbased language models at scale. In Artificial Intelli-\ngence and Soft Computing - 19th International Con-\nference, ICAISC 2020, Zakopane, Poland, October\n12-14, 2020, Proceedings, Part II, volume 12416 of\nLecture Notes in Computer Science, pages 301–314.\nSpringer.\nPieter Delobelle, Thomas Winters, and Bettina Berendt.\n2020. RobBERT: a Dutch RoBERTa-based Lan-\nguage Model. In Findings of the Association for\nComputational Linguistics: EMNLP 2020 , pages\n3255–3265, Online. Association for Computational\nLinguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019a. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019b. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\nMa, Ahmed El-Kishky, Siddharth Goyal, Mandeep\nBaines, Onur Celebi, Guillaume Wenzek, Vishrav\nChaudhary, et al. 2021. Beyond english-centric mul-\ntilingual machine translation. Journal of Machine\nLearning Research, 22(107):1–48.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Ari-\nvazhagan, and Wei Wang. 2020. Language-agnostic\nBERT sentence embedding. CoRR, abs/2007.01852.\nDaniel Hladek, Jan Stas, and Jozef Juhar. 2014. The\nSlovak categorized news corpus. In Proceedings\nof the Ninth International Conference on Language\nResources and Evaluation (LREC’14), pages 1705–\n1708, Reykjavik, Iceland. European Language Re-\nsources Association (ELRA).\nHaoyang Huang, Yaobo Liang, Nan Duan, Ming Gong,\nLinjun Shou, Daxin Jiang, and Ming Zhou. 2019.\nUnicoder: A universal language encoder by pre-\ntraining with multiple cross-lingual tasks. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP) , pages 2485–2494,\nHong Kong, China. Association for Computational\nLinguistics.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao\nChen, Linlin Li, Fang Wang, and Qun Liu. 2020.\nTinyBERT: Distilling BERT for natural language un-\nderstanding. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pages 4163–\n4174, Online. Association for Computational Lin-\nguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nJohn Koutsikakis, Ilias Chalkidis, Prodromos Malaka-\nsiotis, and Ion Androutsopoulos. 2020. GREEK-\nBERT: the greeks visiting sesame street. In SETN\n2020: 11th Hellenic Conference on Artificial Intelli-\ngence, Athens, Greece, September 2-4, 2020, pages\n110–117. ACM.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nMarco Marelli, Luisa Bentivogli, Marco Baroni, Raf-\nfaella Bernardi, Stefano Menini, and Roberto Zam-\nparelli. 2014. SemEval-2014 task 1: Evaluation of\ncompositional distributional semantic models on full\nsentences through semantic relatedness and textual\nentailment. In Proceedings of the 8th International\nWorkshop on Semantic Evaluation (SemEval 2014),\npages 1–8, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Suárez, Yoann Dupont, Laurent Romary, Éric\nde la Clergerie, Djamé Seddah, and Benoît Sagot.\n2020. CamemBERT: a tasty French language model.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 7203–\n7219, Online. Association for Computational Lin-\nguistics.\nIgor Mozetiˇc, Miha Grˇcar, and Jasmina Smailovi´c. 2016.\nMultilingual twitter sentiment classification: The role\nof human annotators. PloS one, 11(5).\nMinh Van Nguyen, Viet Dac Lai, Amir Pouran Ben Vey-\nseh, and Thien Huu Nguyen. 2021. Trankit: A light-\nweight transformer-based toolkit for multilingual nat-\nural language processing. In Proceedings of the 16th\n7165\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics: System Demon-\nstrations, pages 80–90, Online. Association for Com-\nputational Linguistics.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Jan Haji ˇc, Christopher D. Manning, Sampo\nPyysalo, Sebastian Schuster, Francis Tyers, and\nDaniel Zeman. 2020. Universal Dependencies v2:\nAn evergrowing multilingual treebank collection. In\nProceedings of the 12th Language Resources and\nEvaluation Conference, pages 4034–4043, Marseille,\nFrance. European Language Resources Association.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for\nsequence modeling. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Associa-\ntion for Computational Linguistics (Demonstrations),\npages 48–53, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nSamuel Pecar, Marian Simko, and Maria Bielikova.\n2019. Improving sentiment classification in Slovak\nlanguage. In Proceedings of the 7th Workshop on\nBalto-Slavic Natural Language Processing , pages\n114–119, Florence, Italy. Association for Computa-\ntional Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nSampo Pyysalo, Jenna Kanerva, Antti Virtanen, and\nFilip Ginter. 2021. WikiBERT models: Deep trans-\nfer learning for many languages. In Proceedings\nof the 23rd Nordic Conference on Computational\nLinguistics (NoDaLiDa), pages 1–10, Reykjavik, Ice-\nland (Online). Linköping University Electronic Press,\nSweden.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and\nChristopher D. Manning. 2020. Stanza: A python\nnatural language processing toolkit for many human\nlanguages. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations, pages 101–108, Online. As-\nsociation for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nNils Reimers and Iryna Gurevych. 2020. Making\nmonolingual sentence embeddings multilingual us-\ning knowledge distillation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4512–4525,\nOnline. Association for Computational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nJakub Sido, Ondrej Prazák, Pavel Pribán, Jan Pasek,\nMichal Seják, and Miloslav Konopík. 2021. Czert\n- czech bert-like model for language representation.\nCoRR, abs/2103.13031.\nMilan Straka. 2018. UDPipe 2.0 prototype at CoNLL\n2018 UD shared task. In Proceedings of the CoNLL\n2018 Shared Task: Multilingual Parsing from Raw\nText to Universal Dependencies , pages 197–207,\nBrussels, Belgium. Association for Computational\nLinguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nPatrick Xia, Shijie Wu, and Benjamin Van Durme. 2020.\nWhich *BERT? A survey organizing contextualized\nencoders. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 7516–7533, Online. Association for\nComputational Linguistics.\nDaniel Zeman. 2017. Slovak dependency treebank\nin universal dependencies. Journal of Linguis-\ntics/Jazykovedn`y casopis, 68(2):385–395.\n7166\nA Hyperparameter Values\nHyperparameter Range Selected\nLearning rate [10−7, 10−3] 10−5\nBatch size {8, 16, 32, 64, 128} 32\nWarmup steps {0, 500, 1000, 2000} 1000\nWeight decay [0, 0.1] 0.05\nLabel smoothing [0, 0.2] 0\nLearning rate scheduler Various3 linear\nTable 6: Hyperparameters used for POS tagging. Adam\nwas used as an optimization algorithm.\nHyperparameter Range Selected\nLearning rate [10−7, 10−3] 10−5\nBatch size {8, 16, 32, 64, 128} 32\nWarmup steps {0, 500, 1000, 2000} 1000\nWeight decay [0, 0.2] 0.15\nLearning rate scheduler Various4 cosine with hard restarts\nTable 7: Hyperparameters used for STS tagging. Adam\nwas used as an optimization algorithm.\n3See the list of schedulers supported by Hugging Face\nTransformers library.\n4See the list of schedulers supported by the Sentence Trans-\nformers library.\n7167\nB Tagging Schemata\nXPOS UPOS\nTag Description Tag Description\nA adjective ADJ adjectiveG participle\nE preposition ADP adposition\nD adverb ADV adverb\nY conditional morpheme AUX auxiliary\nV verb VERB verb\nO conjuction CCONJ coordinating conjunction\nSCONJ subordinating conjunction\nP pronoun DET determiner\nPRON pronounR reflexive pronoun\nJ interjection INTJ interjection\nS noun NOUN noun\nPROPN proper noun\nN numeral NUM numeral0 digit\nT particle PART particle\nZ punctuation PUNCT punctuation\nW abbreviation\nX otherQ unidentifiable\n# non-word element\n% citation in foreign language\nSYM symbol\nTable 8: Slovak POS tagsets and their mapping (Zeman,\n2017).\nLabel Meaning\n0 The two sentences are completely dissimilar.\n1 The two sentences are not equivalent, but are on the same topic.\n2 The two sentences are not equivalent, but share some details.\n3 The two sentences are roughly equivalent, but some important information\ndiffers.\n4 The two sentences are mostly equivalent, but some unimportant details differ.\n5 The two sentences are completely equivalent, as they mean the same thing.\nTable 9: Annotation schema for STS datasets (Marelli\net al., 2014).\n7168",
  "topic": "Slovak",
  "concepts": [
    {
      "name": "Slovak",
      "score": 0.9166665077209473
    },
    {
      "name": "Computer science",
      "score": 0.6501104831695557
    },
    {
      "name": "Linguistics",
      "score": 0.46097177267074585
    },
    {
      "name": "Natural language processing",
      "score": 0.44913119077682495
    },
    {
      "name": "Philosophy",
      "score": 0.08549636602401733
    },
    {
      "name": "Czech",
      "score": 0.08424484729766846
    }
  ]
}