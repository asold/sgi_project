{
  "title": "SyntaxGym: An Online Platform for Targeted Evaluation of Language Models",
  "url": "https://openalex.org/W3037115370",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2530850308",
      "name": "Jon Gauthier",
      "affiliations": [
        "Massachusetts Institute of Technology",
        "Institute of Cognitive and Brain Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2161984462",
      "name": "Jennifer Hu",
      "affiliations": [
        "Massachusetts Institute of Technology",
        "Institute of Cognitive and Brain Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2201425052",
      "name": "Ethan Wilcox",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A1975658276",
      "name": "Peng Qian",
      "affiliations": [
        "Institute of Cognitive and Brain Sciences",
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1982032445",
      "name": "Roger Lévy",
      "affiliations": [
        "Institute of Cognitive and Brain Sciences",
        "Massachusetts Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3034510440",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W3044103552",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2996728628",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2921890305",
    "https://openalex.org/W2963073938",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2531882892",
    "https://openalex.org/W2963411763",
    "https://openalex.org/W4289373464",
    "https://openalex.org/W2971016963",
    "https://openalex.org/W3168987555"
  ],
  "abstract": "Targeted syntactic evaluations have yielded insights into the generalizations learned by neural network language models. However, this line of research requires an uncommon confluence of skills: both the theoretical knowledge needed to design controlled psycholinguistic experiments, and the technical proficiency needed to train and deploy large-scale language models. We present SyntaxGym, an online platform designed to make targeted evaluations accessible to both experts in NLP and linguistics, reproducible across computing environments, and standardized following the norms of psycholinguistic experimental design. This paper releases two tools of independent value for the computational linguistics community: 1. A website, syntaxgym.org, which centralizes the process of targeted syntactic evaluation and provides easy tools for analysis and visualization; 2. Two command-line tools, 'syntaxgym' and 'lm-zoo', which allow any user to reproduce targeted syntactic evaluations and general language model inference on their own machine.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 70–76\nJuly 5 - July 10, 2020.c⃝2020 Association for Computational Linguistics\n70\nSyntaxGym:\nAn Online Platform for Targeted Evaluation of Language Models\nJon Gauthier1, Jennifer Hu1, Ethan Wilcox2, Peng Qian1, and Roger Levy1\n1 Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology\n2 Department of Linguistics, Harvard University\njon@gauthiers.net, wilcoxeg@g.harvard.edu\n{jennhu,pqian,rplevy}@mit.edu\nAbstract\nTargeted syntactic evaluations have yielded in-\nsights into the generalizations learned by neu-\nral network language models. However, this\nline of research requires an uncommon con-\nﬂuence of skills: both the theoretical knowl-\nedge needed to design controlled psycholin-\nguistic experiments, and the technical proﬁ-\nciency needed to train and deploy large-scale\nlanguage models. We present SyntaxGym,\nan online platform designed to make targeted\nevaluations accessible to both experts in NLP\nand linguistics, reproducible across comput-\ning environments, and standardized follow-\ning the norms of psycholinguistic experimen-\ntal design. This paper releases two tools of in-\ndependent value for the computational linguis-\ntics community:\n1. A website, syntaxgym.org, which\ncentralizes the process of targeted syntac-\ntic evaluation and provides easy tools for\nanalysis and visualization;\n2. Two command-line tools, syntaxgym\nand lm-zoo, which allow any user to\nreproduce targeted syntactic evaluations\nand general language model inference on\ntheir own machine.\n1 Introduction\nRecent work in evaluating neural network lan-\nguage models focuses on investigating models’\nﬁne-grained prediction behavior on carefully de-\nsigned examples. Unlike broad-coverage language\nmodeling metrics such as perplexity, these evalu-\nations are targeted to reveal whether models have\nlearned speciﬁc knowledge about the syntactic\nstructure of language (see e.g. Warstadt et al., 2020;\nFutrell et al., 2019; Marvin and Linzen, 2018).\nResearch in this line of work requires an un-\ncommon intersection of skills: a) the engineering\nstrength of NLP researchers necessary to train and\nExperiments\nModels\nLinguists\nNLP/ML\nresearchers\nAutomatic\nevaluation\nSyntaxGym\nOpen results\nVisualization\n+Analysis\nFigure 1: SyntaxGym allows linguists to easily design\nand run controlled experiments on the syntactic knowl-\nedge of language models, and allows NLP experts to\ntest their own models against these standards. Users\nsubmit targeted syntactic evaluation experiments to the\nsite, and they are automatically evaluated on language\nmodels available in the Gym. SyntaxGym analyzes and\nvisualizes these evaluation results.\ndeploy large-scale neural network language models,\nand b) the linguistic knowledge of language scien-\ntists necessary to design controlled, theoretically\ninteresting psycholinguistic experiments.\nIn this paper, we introduce SyntaxGym: an\nonline platform and open-source framework that\nmakes targeted syntactic evaluations more ac-\ncessible to experts in NLP and linguistics (Fig-\nure 1). The core of SyntaxGym is a website,\nsyntaxgym.org, that automates the entire eval-\nuation pipeline: collecting tests and models, run-\nning evaluations, and displaying results through\ninteractive visualizations. Language scientists can\nuse the site to design and submit targeted syntactic\nevaluations, testing whether language models have\nderived human-like syntactic knowledge. Indepen-\n71\ndently, NLP experts can submit their own language\nmodels for evaluation on these assays. By separat-\ning the tasks performed by these two user groups,\nthe SyntaxGym site lowers the barrier to entry for\nthe broader community of language researchers.\nWhile SyntaxGym will serve as a centralized\nrepository of syntactic evaluations and language\nmodels, we also release a set of command-line\ntools that allow users to reproduce the site’s eval-\nuations ofﬂine. The computation underlying the\nSyntaxGym site is structured around a command-\nline tool syntaxgym, which allows any user to\nrun targeted syntactic evaluations on their own com-\nputer. We accomplish this by developing a new\nstandard API for interacting with state-of-the-art\nneural network language models, operationalized\nin a second tool lm-zoo.\nTaken together, these tools create a platform that\nmakes the process of targeted syntactic evaluation\nmore standardized, reproducible, and accessible\nto the broader communities of NLP experts and\nlanguage scientists. Our goal is for SyntaxGym\nto facilitate the advancement of language model\nevaluation, leading to the development of models\nwith more human-like linguistic knowledge.\n2 Background\nBefore presenting the SyntaxGym framework, we\nbrieﬂy introduce the targeted syntactic evaluation\nparadigm as a way to assess the quality of neural\nlanguage models.\n2.1 Perplexity\nStandard left-to-right language models are trained\nto predict the next token given a context of previous\ntokens. Language models are typically assessed by\ntheir perplexity, the inverse geometric mean of the\njoint probability of wordsw1, . . . , wN in a held-out\ntest corpus:\nPPL(C) =p(w1, w2, . . . wN)− 1\nN (1)\nHowever, a broad-coverage metric such as per-\nplexity may not be ideal for assessing whether a\nlanguage model has human-like syntactic knowl-\nedge. Recent empirical results suggest that models\nwith similar perplexity measures can still exhibit\nsubstantial variance in syntactic knowledge (Hu\net al., 2020; van Schijndel et al., 2019), accord-\ning to evaluation paradigms described in the next\nsection.\n2.2 Targeted tests for syntactic generalization\nAlternatively, a language model can be evaluated\non its ability to make human-like generalizations\nfor speciﬁc syntactic phenomena. The targeted\nsyntactic evaluation paradigm (Linzen et al., 2016;\nLau et al., 2017; Gulordava et al., 2018; Marvin and\nLinzen, 2018; Futrell et al., 2019; Warstadt et al.,\n2020) incorporates methods from psycholinguistic\nexperiments, designing sentences which hold most\nlexical and syntactic features of each sentence con-\nstant while minimally varying features that deter-\nmine grammaticality or surprise characteristics of\nthe sentence. For example, the following minimal-\npair sentences differ in subject–verb agreement:\n(1) The farmer near the clerks knows many people.\n(2) *The farmer near the clerks know many people.\nA model that has learned the proper subject–verb\nnumber agreement rules for English should assign\na higher probability to the grammatical plural verb\nin the ﬁrst sentence than to the ungrammatical sin-\ngular verb in the second (Linzen et al., 2016).\n3 SyntaxGym\nThe targeted syntactic evaluation paradigm allows\nus to focus on highly speciﬁc measures of language\nmodeling performance, which more directly distin-\nguish models with human-like representations of\nsyntactic structure. SyntaxGym was designed to\nserve as a central repository for these evaluations,\nand to make the evaluations reproducible and ac-\ncessible for users without the necessary technical\nskills or computational resources.\nSection 3.1 ﬁrst describes the standards we de-\nsigned for specifying and executing these targeted\nsyntactic evaluations. Section 3.2 then offers a tour\nof the SyntaxGym site, which is built around these\nstandards.\n3.1 Standardizing targeted syntactic\nevaluation\nWe represent targeted syntactic evaluations astest\nsuites, visualized in Figure 2. These test suites are\nthe core component of psycholinguistic assessment,\nand should be familiar to those experienced in psy-\ncholinguistic experimental design. We will present\nthe structure of a test suite using the running exam-\nple of subject–verb agreement, introduced in the\nprevious section. We describe the components of a\ntest suite from bottom-up:\n72\nCondition Regions\nintro np subj prep the prep np matrix verb continuation\nThe farmer near the clerks knows many people\nThe farmer near the clerks know many people\nThe farmers near the clerk knows many people\nThe farmers near the clerk know many people\nThe manager to the side of the architects likes to gamble\nThe manager to the side of the architects like to gamble\nThe managers to the side of the architect likes to gamble\nThe managers to the side of the architect like to gamble\n···\nItem 1\nItem 2\nmatch sing\nmismatch sing\nmismatch plural\nmatch plural\nmatch sing\nmismatch sing\nmismatch plural\nmatch plural\nPrediction:\n⇣\nmatch sing .matrix verb < mismatch sing .matrix verb\n⌘\n⇣\nmatch plural .matrix verb < mismatch plural .matrix verb\n⌘\n&\nFigure 2: SyntaxGym test suites evaluate predictions about language models’ surprisal values (negative log-\nprobabilities) within regions (columns above) across experimental conditions (leftmost column). A prediction\ncan assert the conjunction of multiple inequalities across conditions. Prediction results are aggregated across items\n(vertical blocks above) to yield overall accuracy estimates.\nRegions The atomic unit of a test suite is a\nregion: a (possibly empty) string, such as the\nmatrix verb region in Figure 2. Regions can\nbe concatenated to form full sentences.\nConditions Regions vary systematically across\nexperimental conditions, shown as colored pill\nshapes in Figure 2. Here the matrix verb and\nnp subj regions vary between their respective\nsingular and plural forms, as described by the con-\ndition.\nItems Items are groups of related sentences\nwhich vary across experimental conditions. An\nitem is characterized by its lexical content and\ntakes different forms across conditions. For ex-\nample, The farmer near the clerkknows and *The\nfarmer near the clerkknow are different sentences\nunder two conditions of the same item.\nPredictions Test suites are designed with a hy-\npothesis in mind: if a model has correctly learned\nsome relevant syntactic generalization, then it\nshould assign higher probability to grammatical\ncontinuations of sentences. Test suite predic-\ntions operationalize these hypotheses as expected\ninequalities between total model surprisal val-\nues in different experimental conditions (i.e., be-\ntween rows within item blocks in Figure 2). The\nSyntaxGym standard allows for arbitrarily complex\ndisjunctions and conjunctions of such inequalities.\nFigure 2 shows a prediction with two inequalities\nbetween model surprisals atmatrix verb across\ntwo pairs of conditions.\nWe designed a standard JSON schema for de-\nscribing the structure and content of test suites\nusing the above concepts. Interested readers can\nﬁnd the full schema and documentation on the\nSyntaxGym site.1\n3.1.1 A standard API for language models\nReproducing research results with modern neural\nnetwork architectures can be notoriously difﬁcult,\ndue to variance in computing environments and due\nto each individual project’s tangled web of pack-\nage dependencies. In addition, inconsistencies in\ndata preprocessing — for example, in tokenization\npractices and the management of out-of-vocabulary\nitems — often make it difﬁcult to evaluate even the\nsame model on different datasets. In order to ad-\ndress these difﬁculties, we designed a standardized\nAPI for interacting with trained language models,\nbuilt to solve these reproducibility issues and allow\nfor highly portable computing with state-of-the-art\nlanguage models. Users can easily connect with\nthis API through the lm-zoo command-line tool,\ndescribed later in Section 4.\nThe standard is built around the Docker con-\ntainerization system. We expect each language\nmodel to be wrapped in a Docker image, includ-\ning a thin API exposing a set of standardized bi-\nnary commands: tokenize, which preprocesses\nnatural-language sentences exactly as a language\n1http://docs.syntaxgym.org\n73\nmodel expects; get-surprisals, which com-\nputes per-token language model surprisals on natu-\nral language input; and unkify, which indicates\nexactly which tokens in an input text ﬁle are in-\nvocabulary for the language model.\nLanguage model creators or third-party main-\ntainers can produce such Docker images wrapping\nlanguage model code. At present, this API is de-\nsigned to mainly serve the needs of theSyntaxGym\nevaluation process. In the future, however, we plan\nto extend the API for other common uses of lan-\nguage models: for example, to extract the next-\nword predictive distributions from the model, and\nto extract the model’s internal word and sentence\nrepresentations. This standard is documented in\nfull at cpllab.github.io/lm-zoo.\n3.2 The SyntaxGym website\nThe SyntaxGym website provides a centralized do-\nmain for collecting targeted syntactic evaluations\nand evaluating them on state-of-the-art language\nmodels. It provides intuitive, user-friendly tools\nfor visualizing the behavior of any language model\non any syntactic test suite, and also exposes all of\nthe resulting raw data to interested advanced users.\nThis section presents a brief tour through the major\nfeatures of the SyntaxGym site.\nCreate test suites Non-technical users can use\nSyntaxGym’s browser-based interface to design\nand submit their own psycholinguistic test suites\n(Figure 3). Separately, the site supports uploading\npre-made test suites as a JSON-formatted ﬁle. This\nfunctionality may be useful for advanced users who\nprefer to automatically generate test suites.2\nFigure 3: Non-technical users can design their own test\nsuites with a browser-based form.\nSubmit language models Users interested in\nevaluating their own language models ﬁrst cre-\nate a public Docker image conforming to the\n2In a future release, we will also allow users to import test\nsuites from spreadsheets as CSV-formatted ﬁles.\nAPI speciﬁed by the SyntaxGym standard (Sec-\ntion 3.1.1). After users submit these language mod-\nels on the SyntaxGym site, the models are automat-\nically validated for conformity to the API by the\nSyntaxGym backend. Valid models are added to\nthe SyntaxGym collection, and will be evaluated\non all past and future available test suites in the\nGym.\nAutomatic evaluation Whenever novel test\nsuites or language models are submitted,\nSyntaxGym automatically evaluates the relevant\nsuites and models in the cloud. For each test\nsuite and model, the evaluation yields a prediction\naccuracy — the number of items for which the\nprediction holds. These prediction accuracies,\nalong with the raw surprisal data, are stored in\nthe SyntaxGym database and made available in\nvisualizations such as Figure 4b.\nVisualization and data analysis The site pro-\nvides a variety of interactive charts that allow users\nto visualize results at different levels of granularity.\nOn the coarsest level, users can compare aggregate\nperformance across language models and groups\nof theoretically related test suites called tags (see\nFigure 1). Users can also compare accuracy across\nmodels on a single test suite (Figure 4a), across tags\nfor a single model, and across test suites within\na single tag. On the ﬁnest level, users can view\nraw region-by-region surprisal values to analyze in-\ndepth performance of a particular language model\non a particular test suite (Figure 4b).\n3.3 Seed data and results\nWe have seeded the SyntaxGym website with a\ncollection of test suites and language models by\naggregating prior research. These materials and\nrelevant evaluation results are separately presented\nin Hu et al. (2020). Here we provide only a brief\nsummary in order to illustrate the features of the\nSyntaxGym website.\n1. We wrapped 8 modern neural network lan-\nguage models (summarized in Table 1) to be\ncompatible with the lm-zoo standard, using\nopen-source research code or standard Python\nframeworks such as Hugging Face Transform-\ners (Wolf et al., 2019).\n2. We aggregated past research on targeted syn-\ntactic evaluation into 33 test suites, each prob-\ning language models’ performance on distinct\ngrammatical phenomena.\n74\n(a) The site automatically evaluates language\nmodels on test suites and visualizes summary\nresults (here, for a subject–verb number agree-\nment test). (click to see page)\n(b) Users can also view all of the raw language model data behind these\nanalyses (here, average per-region surprisal values of GPT-2 for a subject–\nverb number agreement test) and download the raw data for further analysis.\n(click to see page)\nFigure 4: Screenshots of example visualizations from the SyntaxGym website.\nModel Reference Training data (# tokens)\nGPT-2 Radford et al. (2019) WebText (∼8B)\nGPT-2 XL Radford et al. (2019) WebText (∼8B)\nTransformer XL Dai et al. (2019) WikiText-103 (103M)\nJRNN Jozefowicz et al. (2016)1B Benchmark (1.04B)\nGRNN Gulordava et al. (2018) Wikipedia (90M)\nOrdered Neurons Shen et al. (2019) BLLIP (42M)\nLSTM Hochreiter and Schmid-\nhuber (1997)\nBLLIP (42M)\nRNNG Dyer et al. (2016) BLLIP (42M)\nTable 1: Language models currently supported in the\nSyntaxGym framework.\nInterested readers can ﬁnd more details on\nthese test suites and language models, along with\nthe evaluation results and visualizations, on the\nSyntaxGym site.\n4 Command-line tools\nWhile the SyntaxGym website allows for easy cen-\ntralization of test suites and public access to results,\nall of its underlying infrastructure is also available\nindependently for researchers to use. We release\ntwo command-line tools, available to any user with\nPython and Docker installed.\n4.1 lm-zoo: black-box access to SOTA\nlanguage models\nWe ﬁrst designed a general command-line tool for\ninteracting with state-of-the-art neural language\nmodels, called lm-zoo. Figure 5b demonstrates\nhow this tool can be used to easily extract pre-\ndiction data from an arbitrary language model.\nFull documentation and installation instructions are\navailable at cpllab.github.io/lm-zoo.\n4.2 syntaxgym: targeted syntactic\nevaluation\nUsers can completely reproduce the targeted syn-\ntactic evaluation paradigm of SyntaxGym outside\nof the website using a second command-line tool,\nsyntaxgym, shown in Figure 5a. This tool\ndoes the work of converting test suites into ac-\ntual natural-language sentences appropriately for-\nmatted for a particular language model, execut-\ning the model, and mapping the results back to a\nSyntaxGym-friendly format ready for analysis. It\ndeals with the wide variation in tokenization and\nout-of-vocabulary token handling across models.\nFull documentation and installation instructions are\navailable at syntaxgym.org/cli.\n5 Related work\nMarvin and Linzen (2018) release a dataset of\nminimal-pair sentences designed to test language\nmodels’ syntactic generalization capabilities. How-\never, the syntactic coverage of the dataset is limited\nto a small set of phenomena: subject-verb agree-\nment, reﬂexive anaphor licensing, and negative po-\nlarity items.\nWarstadt et al. (2020) release a large dataset ag-\ngregating a broad collection of targeted syntactic\nevaluations from prior research, known as BLiMP.\nLike the Marvin and Linzen dataset, BLiMP con-\nsists of a collection of minimal-pair sentences\nwhich contrast in grammaticality, following the\nstandard shown in Examples (1) and (2). The\nBLiMP evaluation requires that language models\nassign a higher total probability to the grammatical\n(1) than the ungrammatical (2). The authors de-\nsign abstract templates which specify grammatical–\n75\n$ syntaxgym list models\ngpt-2, gpt-2-xl, transformer-xl, ...\n$ syntaxgym list suites\nnumber-orc, number-src, mvrr, ...\n# Evaluate model \"gpt-2\" on suite \"mvrr\"\n$ syntaxgym evaluate gpt-2 mvrr\nAccuracy: 0.7857 (22/28 correct)\n# Evaluate arbitrary model on custom suite\n$ syntaxgym evaluate \\\n> docker://me/my-model my-suite.json\nAccuracy: 0.575 (23/40 correct)\n(a) The syntaxgym tool allows users to evaluate language\nmodels on test suites — both models and suites hosted by\nSyntaxGym, and models and suites created by the user.\n$ echo \"This is a sentence.\" > foo.txt\n$ lm-zoo list models\ngpt-2, gpt-2-xl, transformer-xl, ...\n$ lm-zoo tokenize transformer-xl foo.txt\nThis is a sentence .\n$ lm-zoo get-surprisals transformer-xl foo.txt\nsentence_id token_id token surprisal\n1 1 This 0.0000\n1 1 is 4.1239\n1 1 a 1.0126\n...\n(b) The lm-zoo tool provides lower-level access to\nSyntaxGym-hosted language models, allowing users to retrieve\nmodels’ predictions, tokenization choices, and more.\nFigure 5: We built SyntaxGym around command-line tools for probing and evaluating neural network language\nmodels, which can be used independently of the SyntaxGym site.\nungrammatical pairs for many linguistic phenom-\nena, and then generate example sentences based on\nthese templates.\nWhile BLiMP and SyntaxGym are similarly mo-\ntivated, they differ slightly in methodology. First,\nBLiMP requires models to satisfy only a single in-\nequality between sentence probabilities. While the\nSyntaxGym system can support such predictions,\nit is designed to support much stricter tests of lan-\nguage models, such as the conjunction of inequali-\nties across multiple conditions (see Figure 2). Sec-\nond, BLiMP compares model judgments about to-\ntal sentence probabilities. In contrast, SyntaxGym\nis designed to compare model judgments only in\ncritical test regions, which allows us to more fairly\nevaluate language model predictions only in pre-\nspeciﬁed spans of interest. Finally, the BLiMP sen-\ntences are automatically generated from abstract\ngrammars exemplifying syntactic phenomena of in-\nterest. Since automatic methods can easily yield a\nlarge number of sentences, they can help us control\nfor other possible sources of noise in test materials.\nHowever, many grammatical phenomena of interest\nare ﬁendishly difﬁcult to capture in abstract gram-\nmars, and require careful design by native speak-\ners.3 This BLiMP data is thus complementary to\nthe hand-designed test suites currently presented\non the SyntaxGym site. We plan to adapt such\nlarge-scale test suites on SyntaxGym in the future.\n3For example, one such phenomenon is the garden-path\ndisambiguation effect (Futrell et al., 2019), which is highly\nsensitive to nuanced lexical and world-knowledge features of\nsentences.\n6 Conclusion\nThis paper presented SyntaxGym, an online plat-\nform and open-source framework for targeted syn-\ntactic evaluation of neural network language mod-\nels. SyntaxGym promises to advance the progress\nof language model evaluation by uniting the theo-\nretical expertise of linguists with the technical skills\nof NLP researchers. The site is fully functional at\nsyntaxgym.org, and the entire framework is\navailable as open-source code.\nSyntaxGym is continually evolving: we plan to\nadd new features to the site, and to develop further\nin response to user feedback. In particular, we plan\nto incorporate human performanceas a reference\nmetric, integrating psycholinguistic experimental\nresults and supporting easy experimental design\nstarting from the test suite format.\nWe also plan to further incorporate language\nmodels into the lm-zoo tool, allowing broader ac-\ncess to state-of-the-art language models in general.\nWe welcome open-source contributions to the web-\nsite and to the general framework, and especially\nencourage the NLP community to contribute their\nmodels to the lm-zoo repository.\nAcknowledgments\nJ.G. is supported by an Open Philanthropy AI Fel-\nlowship. J.H. is supported by the NIH under award\nnumber T32NS105587 and an NSF Graduate Re-\nsearch Fellowship. R.L. is supported by a Google\nFaculty Research Award. This work was also sup-\nported by the MIT-IBM Watson AI Lab.\n76\nReferences\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-xl: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A Smith. 2016. Recurrent neural network\ngrammars. In Proceedings of NAACL-HLT, pages\n199–209.\nRichard Futrell, Ethan Wilcox, Takashi Morita, Peng\nQian, Miguel Ballesteros, and Roger Levy. 2019.\nNeural language models as psycholinguistic sub-\njects: Representations of syntactic state. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 32–42, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of NAACL-HLT, pages 1195–1205.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation,\n9(8):1735–1780.\nJennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox,\nand Roger Levy. 2020. A systematic assessment of\nsyntactic generalization in neural language models.\nIn Proceedings of the Association of Computational\nLinguistics.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring\nthe limits of language modeling. arXiv preprint\narXiv:1602.02410.\nJey Han Lau, Alexander Clark, and Shalom Lappin.\n2017. Grammaticality, acceptability, and probabil-\nity: A probabilistic view of linguistic knowledge.\nCognitive Science, 5:1202–1247.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. In Transactions of\nthe Association for Computational Linguistics, vol-\nume 4, pages 521–535.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202,\nBrussels, Belgium. Association for Computational\nLinguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nMarten van Schijndel, Aaron Mueller, and Tal Linzen.\n2019. Quantity doesn’t buy quality syntax with\nneural language models. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5835–5841.\nYikang Shen, Shawn Tan, Alessandro Sordoni, and\nAaron Courville. 2019. Ordered neurons: Integrat-\ning tree structures into recurrent neural networks. In\nInternational Conference on Learning Representa-\ntions.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R\nBowman. 2020. BLiMP: A benchmark of linguis-\ntic minimal pairs for English. In Proceedings of the\nSociety for Computation in Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8365814089775085
    },
    {
      "name": "Inference",
      "score": 0.5331814289093018
    },
    {
      "name": "Visualization",
      "score": 0.515480101108551
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4964807629585266
    },
    {
      "name": "Natural language processing",
      "score": 0.49044105410575867
    },
    {
      "name": "Process (computing)",
      "score": 0.47079601883888245
    },
    {
      "name": "Computational linguistics",
      "score": 0.4433113932609558
    },
    {
      "name": "Programming language",
      "score": 0.20989182591438293
    }
  ]
}