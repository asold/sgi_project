{
  "title": "Emerging Properties in Self-Supervised Vision Transformers",
  "url": "https://openalex.org/W3159481202",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2006835859",
      "name": "Mathilde Caron",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2951399046",
      "name": "Hugo Touvron",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2132762296",
      "name": "Ishan Misra",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A1140831031",
      "name": "Hervé Jégou",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A1982785356",
      "name": "Julien Mairal",
      "affiliations": [
        "Centre Inria de l'Université Grenoble Alpes",
        "Université Grenoble Alpes",
        "Centre National de la Recherche Scientifique"
      ]
    },
    {
      "id": "https://openalex.org/A1882694979",
      "name": "Piotr Bojanowski",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2512114774",
      "name": "Armand Joulin",
      "affiliations": [
        "Meta (Israel)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2990205821",
    "https://openalex.org/W2963465221",
    "https://openalex.org/W3034915791",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W6675122589",
    "https://openalex.org/W6726497184",
    "https://openalex.org/W6777265123",
    "https://openalex.org/W2798991696",
    "https://openalex.org/W3035160371",
    "https://openalex.org/W6685380521",
    "https://openalex.org/W3096338464",
    "https://openalex.org/W6638523607",
    "https://openalex.org/W6761825139",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2962852342",
    "https://openalex.org/W6791742336",
    "https://openalex.org/W6761903662",
    "https://openalex.org/W6701655646",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W2998388430",
    "https://openalex.org/W6770196601",
    "https://openalex.org/W6748108687",
    "https://openalex.org/W6739622702",
    "https://openalex.org/W6787508319",
    "https://openalex.org/W6787207808",
    "https://openalex.org/W6779326418",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W6682948231",
    "https://openalex.org/W6695676441",
    "https://openalex.org/W6787339664",
    "https://openalex.org/W2964700958",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W6685522000",
    "https://openalex.org/W6777179611",
    "https://openalex.org/W6928992657",
    "https://openalex.org/W6733814495",
    "https://openalex.org/W2964045208",
    "https://openalex.org/W6774314701",
    "https://openalex.org/W2148809531",
    "https://openalex.org/W6779977557",
    "https://openalex.org/W6774670964",
    "https://openalex.org/W6786614245",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W2144796873",
    "https://openalex.org/W2020308406",
    "https://openalex.org/W6780730929",
    "https://openalex.org/W6758139636",
    "https://openalex.org/W6679434410",
    "https://openalex.org/W6736894310",
    "https://openalex.org/W2987741655",
    "https://openalex.org/W6753000030",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W6779997284",
    "https://openalex.org/W2990519439",
    "https://openalex.org/W6602403235",
    "https://openalex.org/W6784531446",
    "https://openalex.org/W2605229288",
    "https://openalex.org/W2086161653",
    "https://openalex.org/W2767621619",
    "https://openalex.org/W2962369866",
    "https://openalex.org/W3036224891",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2031489346",
    "https://openalex.org/W2941964676",
    "https://openalex.org/W3121052760",
    "https://openalex.org/W2963588253",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W3116557712",
    "https://openalex.org/W3170336723",
    "https://openalex.org/W3109440167",
    "https://openalex.org/W3099570996",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2158131535",
    "https://openalex.org/W4288581820",
    "https://openalex.org/W3035060554",
    "https://openalex.org/W2152790380",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2953070460",
    "https://openalex.org/W3026092005",
    "https://openalex.org/W2284050935",
    "https://openalex.org/W3107668149",
    "https://openalex.org/W3171007011",
    "https://openalex.org/W2963927126",
    "https://openalex.org/W2963685250",
    "https://openalex.org/W2607510315",
    "https://openalex.org/W2911925209",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2943152387",
    "https://openalex.org/W2250384498",
    "https://openalex.org/W2174726731",
    "https://openalex.org/W2574872930",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2963495051",
    "https://openalex.org/W3100859887",
    "https://openalex.org/W3103455452",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3168405954",
    "https://openalex.org/W2896060389",
    "https://openalex.org/W2963154697",
    "https://openalex.org/W3018265077",
    "https://openalex.org/W2622263826",
    "https://openalex.org/W3134652006",
    "https://openalex.org/W3197396431",
    "https://openalex.org/W3106428938",
    "https://openalex.org/W2995181141",
    "https://openalex.org/W3128934284",
    "https://openalex.org/W3122325173",
    "https://openalex.org/W2100664567",
    "https://openalex.org/W3041919418",
    "https://openalex.org/W2883725317",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W2787017828",
    "https://openalex.org/W2964074409",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W3037618862",
    "https://openalex.org/W3009561768",
    "https://openalex.org/W2533598788",
    "https://openalex.org/W3027083471",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3093929102",
    "https://openalex.org/W4287636287",
    "https://openalex.org/W3014490631",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W59018853",
    "https://openalex.org/W3022061250",
    "https://openalex.org/W2134670479",
    "https://openalex.org/W2951541198",
    "https://openalex.org/W3136604105",
    "https://openalex.org/W3135715136",
    "https://openalex.org/W4288024349",
    "https://openalex.org/W3128099838",
    "https://openalex.org/W3035305184",
    "https://openalex.org/W2996080391",
    "https://openalex.org/W2950344723",
    "https://openalex.org/W2916743882",
    "https://openalex.org/W3149106692",
    "https://openalex.org/W2326925005"
  ],
  "abstract": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.",
  "full_text": "Emerging Properties in Self-Supervised Vision Transformers\nMathilde Caron1,2 Hugo Touvron1,3 Ishan Misra1 Herv´e Jegou1\nJulien Mairal2 Piotr Bojanowski1 Armand Joulin1\n1 Facebook AI Research 2 Inria∗ 3 Sorbonne University\nFigure 1: Self-attention from a Vision Transformer with 8 ×8 patches trained with no supervision. We look at the self-attention of\nthe [CLS] token on the heads of the last layer. This token is not attached to any label nor supervision. These maps show that the model\nautomatically learns class-speciﬁc features leading to unsupervised object segmentations.\nAbstract\nIn this paper, we question if self-supervised learning pro-\nvides new properties to Vision Transformer (ViT) [19] that\nstand out compared to convolutional networks (convnets).\nBeyond the fact that adapting self-supervised methods to this\narchitecture works particularly well, we make the follow-\ning observations: ﬁrst, self-supervised ViT features contain\nexplicit information about the semantic segmentation of an\nimage, which does not emerge as clearly with supervised\nViTs, nor with convnets. Second, these features are also ex-\ncellent k-NN classiﬁers, reaching 78.3% top-1 on ImageNet\nwith a small ViT. Our study also underlines the importance of\nmomentum encoder [33], multi-crop training [10], and the\nuse of small patches with ViTs. We implement our ﬁndings\ninto a simple self-supervised method, called DINO, which\nwe interpret as a form of self- distillation with no labels.\nWe show the synergy between DINO and ViTs by achieving\n80.1% top-1 on ImageNet in linear evaluation with ViT-Base.\n∗Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000\nGrenoble, France.\nCorrespondence: mathilde@fb.com\nCode: https://github.com/facebookresearch/dino\n1. Introduction\nTransformers [70] have recently emerged as an alternative\nto convolutional neural networks (convnets) for visual recog-\nnition [19, 69, 83]. Their adoption has been coupled with\na training strategy inspired by natural language processing\n(NLP), that is, pretraining on large quantities of data and\nﬁnetuning on the target dataset [18, 55]. The resulting Vision\nTransformers (ViT) [19] are competitive with convnets but,\nthey have not yet delivered clear beneﬁts over them: they\nare computationally more demanding, require more training\ndata, and their features do not exhibit unique properties.\nIn this paper, we question whether the muted success of\nTransformers in vision can be explained by the use of super-\nvision in their pretraining. Our motivation is that one of the\nmain ingredients for the success of Transformers in NLP was\nthe use of self-supervised pretraining, in the form of close\nprocedure in BERT [18] or language modeling in GPT [55].\nThese self-supervised pretraining objectives use the words\nin a sentence to create pretext tasks that provide a richer\nlearning signal than the supervised objective of predicting\na single label per sentence. Similarly, in images, image-\nlevel supervision often reduces the rich visual information\ncontained in an image to a single concept selected from a\npredeﬁned set of a few thousand categories of objects [60].\nWhile the self-supervised pretext tasks used in NLP are\n1\narXiv:2104.14294v2  [cs.CV]  24 May 2021\ntext speciﬁc, many existing self-supervised methods have\nshown their potential on images with convnets [10, 12, 30,\n33]. They typically share a similar structure but with differ-\nent components designed to avoid trivial solutions (collapse)\nor to improve performance [16]. In this work, inspired from\nthese methods, we study the impact of self-supervised pre-\ntraining on ViT features. Of particular interest, we have\nidentiﬁed several interesting properties that do not emerge\nwith supervised ViTs, nor with convnets:\n• Self-supervised ViT features explicitly contain the\nscene layout and, in particular, object boundaries, as\nshown in Figure 1. This information is directly accessi-\nble in the self-attention modules of the last block.\n• Self-supervised ViT features perform particularly well\nwith a basic nearest neighbors classiﬁer (k-NN) without\nany ﬁnetuning, linear classiﬁer nor data augmentation,\nachieving 78.3% top-1 accuracy on ImageNet.\nThe emergence of segmentation masks seems to be a\nproperty shared across self-supervised methods. However,\nthe good performance with k-NN only emerge when com-\nbining certain components such as momentum encoder [33]\nand multi-crop augmentation [10]. Another ﬁnding from our\nstudy is the importance of using smaller patches with ViTs\nto improve the quality of the resulting features.\nOverall, our ﬁndings about the importance of these\ncomponents lead us to design a simple self-supervised ap-\nproach that can be interpreted as a form of knowledge\ndistillation [35] with no labels. The resulting framework,\nDINO, simpliﬁes self-supervised training by directly pre-\ndicting the output of a teacher network—built with a mo-\nmentum encoder—by using a standard cross-entropy loss.\nInterestingly, our method can work with only a centering\nand sharpening of the teacher output to avoid collapse, while\nother popular components such as predictor [30], advanced\nnormalization [10] or contrastive loss [33] add little beneﬁts\nin terms of stability or performance. Of particular impor-\ntance, our framework is ﬂexible and works on both convnets\nand ViTs without the need to modify the architecture, nor\nadapt internal normalizations [58].\nWe further validate the synergy between DINO and ViT\nby outperforming previous self-supervised features on the\nImageNet linear classiﬁcation benchmark with 80.1% top-1\naccuracy with a ViT-Base with small patches. We also con-\nﬁrm that DINO works with convnets by matching the state\nof the art with a ResNet-50 architecture. Finally, we discuss\ndifferent scenarios to use DINO with ViTs in case of limited\ncomputation and memory capacity. In particular, training\nDINO with ViT takes just two 8-GPU servers over 3 days\nto achieve 76.1% on ImageNet linear benchmark, which\noutperforms self-supervised systems based on convnets of\ncomparable sizes with signiﬁcantly reduced compute require-\nments [10, 30].\nstudent gθs\nx\nx2x1\nteacher gθt\ncentering\nsg\nsoftmax\np1 p2\nsoftmax\nloss:  \n- p2 log p1\nema\nFigure 2: Self-distillation with no labels. We illustrate DINO in\nthe case of one single pair of views ( x1, x2) for simplicity. The\nmodel passes two different random transformations of an input\nimage to the student and teacher networks. Both networks have\nthe same architecture but different parameters. The output of the\nteacher network is centered with a mean computed over the batch.\nEach networks outputs a K dimensional feature that is normalized\nwith a temperature softmax over the feature dimension. Their\nsimilarity is then measured with a cross-entropy loss. We apply a\nstop-gradient (sg) operator on the teacher to propagate gradients\nonly through the student. The teacher parameters are updated with\nan exponential moving average (ema) of the student parameters.\n2. Related work\nSelf-supervised learning. A large body of work on self-\nsupervised learning focuses on discriminative approaches\ncoined instance classiﬁcation [12, 20, 33, 73], which con-\nsiders each image a different class and trains the model\nby discriminating them up to data augmentations. How-\never, explicitly learning a classiﬁer to discriminate be-\ntween all images [ 20] does not scale well with the num-\nber of images. Wu et al . [ 73] propose to use a noise\ncontrastive estimator (NCE) [32] to compare instances in-\nstead of classifying them. A caveat of this approach is\nthat it requires comparing features from a large number\nof images simultaneously. In practice, this requires large\nbatches [12] or memory banks [ 33, 73]. Several variants\nallow automatic grouping of instances in the form of cluster-\ning [2, 8, 9, 36, 42, 74, 80, 85].\nRecent works have shown that we can learn unsupervised\nfeatures without discriminating between images. Of par-\nticular interest, Grill et al. [30] propose a metric-learning\nformulation called BYOL, where features are trained by\nmatching them to representations obtained with a momentum\nencoder. Methods like BYOL work even without a momen-\ntum encoder, at the cost of a drop of performance [16, 30].\nSeveral other works echo this direction, showing that one\ncan match more elaborate representations [26, 27], train fea-\ntures matching them to a uniform distribution [6] or by using\nwhitening [23, 81]. Our approach takes its inspiration from\nBYOL but operates with a different similarity matching loss\nand uses the exact same architecture for the student and the\nteacher. That way, our work completes the interpretation\ninitiated in BYOL of self-supervised learning as a form of\nMean Teacher self-distillation [65] with no labels.\nSelf-training and knowledge distillation. Self-training\naims at improving the quality of features by propagating\na small initial set of annotations to a large set of unlabeled\ninstances. This propagation can either be done with hard\nassignments of labels [ 41, 78, 79] or with a soft assign-\nment [76]. When using soft labels, the approach is often\nreferred to as knowledge distillation [ 7, 35] and has been\nprimarily designed to train a small network to mimic the\noutput of a larger network to compress models. Xie et\nal. [76] have shown that distillation could be used to propa-\ngate soft pseudo-labels to unlabelled data in a self-training\npipeline, drawing an essential connection between self-\ntraining and knowledge distillation. Our work builds on\nthis relation and extends knowledge distillation to the case\nwhere no labels are available. Previous works have also\ncombined self-supervised learning and knowledge distilla-\ntion [25, 63, 13, 47], enabling self-supervised model com-\npression and performance gains. However, these works rely\non a pre-trained ﬁxed teacher while our teacher is dynam-\nically built during training. This way, knowledge distilla-\ntion, instead of being used as a post-processing step to self-\nsupervised pre-training, is directly cast as a self-supervised\nobjective. Finally, our work is also related to codistilla-\ntion [1] where student and teacher have the same architecture\nand use distillation during training. However, the teacher in\ncodistillation is also distilling from the student, while it is\nupdated with an average of the student in our work.\n3. Approach\n3.1. SSL with Knowledge Distillation\nThe framework used for this work, DINO, shares the same\noverall structure as recent self-supervised approaches [ 10,\n16, 12, 30, 33]. However, our method shares also similarities\nwith knowledge distillation [ 35] and we present it under\nthis angle. We illustrate DINO in Figure 2 and propose a\npseudo-code implementation in Algorithm 1.\nKnowledge distillation is a learning paradigm where we\ntrain a student network gθs to match the output of a given\nteacher network gθt , parameterized by θs and θt respectively.\nGiven an input image x, both networks output probability\ndistributions over Kdimensions denoted by Ps and Pt. The\nprobability P is obtained by normalizing the output of the\nnetwork gwith a softmax function. More precisely,\nPs(x)(i) = exp(gθs (x)(i)/τs)∑K\nk=1 exp(gθs (x)(k)/τs)\n, (1)\nwith τs > 0 a temperature parameter that controls the\nAlgorithm 1 DINO PyTorch pseudocode w/o multi-crop.\n# gs, gt: student and teacher networks\n# C: center (K)\n# tps, tpt: student and teacher temperatures\n# l, m: network and center momentum rates\ngt.params = gs.params\nfor x in loader: # load a minibatch x with n samples\nx1, x2 = augment(x), augment(x) # random views\ns1, s2 = gs(x1), gs(x2) # student output n-by-K\nt1, t2 = gt(x1), gt(x2) # teacher output n-by-K\nloss = H(t1, s2)/2 + H(t2, s1)/2\nloss.backward() # back-propagate\n# student, teacher and center updates\nupdate(gs) # SGD\ngt.params = l*gt.params + (1-l)*gs.params\nC = m*C + (1-m)*cat([t1, t2]).mean(dim=0)\ndef H(t, s):\nt = t.detach() # stop gradient\ns = softmax(s / tps, dim=1)\nt = softmax((t - C) / tpt, dim=1) # center + sharpen\nreturn - (t * log(s)).sum(dim=1).mean()\nsharpness of the output distribution, and a similar formula\nholds for Pt with temperature τt. Given a ﬁxed teacher\nnetwork gθt , we learn to match these distributions by min-\nimizing the cross-entropy loss w.r.t. the parameters of the\nstudent network θs:\nmin\nθs\nH(Pt(x),Ps(x)), (2)\nwhere H(a,b) =−alog b.\nIn the following, we detail how we adapt the problem\nin Eq. (2) to self-supervised learning. First, we construct\ndifferent distorted views, or crops, of an image with multi-\ncrop strategy [10]. More precisely, from a given image, we\ngenerate a set V of different views. This set contains two\nglobal views, xg\n1 and xg\n2 and several local views of smaller\nresolution. All crops are passed through the student while\nonly the global views are passed through the teacher, there-\nfore encouraging “local-to-global” correspondences. We\nminimize the loss:\nmin\nθs\n∑\nx∈{xg\n1,xg\n2}\n∑\nx′∈V\nx′̸= x\nH(Pt(x),Ps(x′)). (3)\nThis loss is general and can be used on any number of\nviews, even only 2. However, we follow the standard setting\nfor multi-crop by using 2 global views at resolution 2242\ncovering a large (for example greater than 50%) area of the\noriginal image, and several local views of resolution 962\ncovering only small areas (for example less than 50%) of\nthe original image. We refer to this setting as the basic\nparametrization of DINO, unless mentioned otherwise.\nBoth networks share the same architecture gwith differ-\nent sets of parameters θs and θt. We learn the parameters θs\nby minimizing Eq. (3) with stochastic gradient descent.\nTable 1: Networks conﬁguration. “Blocks” is the number of\nTransformer blocks, “dim” is channel dimension and “heads” is the\nnumber of heads in multi-head attention. “# tokens” is the length\nof the token sequence when considering 2242 resolution inputs, “#\nparams” is the total number of parameters (without counting the\nprojection head) and “im/s” is the inference time on a NVIDIA\nV100 GPU with 128 samples per forward.\nmodel blocks dim heads #tokens #params im/s\nResNet-50 – 2048 – – 23M 1237\nViT-S/16 12 384 6 197 21M 1007\nViT-S/8 12 384 6 785 21M 180\nViT-B/16 12 768 12 197 85M 312\nViT-B/8 12 768 12 785 85M 63\nTeacher network. Unlike knowledge distillation, we do\nnot have a teacher gθt given a priori and hence, we build it\nfrom past iterations of the student network. We study dif-\nferent update rules for the teacher in Section 5.2 and show\nthat freezing the teacher network over an epoch works sur-\nprisingly well in our framework, while copying the student\nweight for the teacher fails to converge. Of particular in-\nterest, using an exponential moving average (EMA) on the\nstudent weights, i.e., a momentum encoder [ 33], is partic-\nularly well suited for our framework. The update rule is\nθt ←λθt + (1−λ)θs,with λfollowing a cosine schedule\nfrom 0.996 to 1 during training [30]. Originally the momen-\ntum encoder has been introduced as a substitute for a queue\nin contrastive learning [33]. However, in our framework, its\nrole differs since we do not have a queue nor a contrastive\nloss, and may be closer to the role of the mean teacher used\nin self-training [65]. Indeed, we observe that this teacher per-\nforms a form of model ensembling similar to Polyak-Ruppert\naveraging with an exponential decay [51, 59]. Using Polyak-\nRuppert averaging for model ensembling is a standard prac-\ntice to improve the performance of a model [38]. We observe\nthat this teacher has better performance than the student\nthroughout the training, and hence, guides the training of the\nstudent by providing target features of higher quality. This\ndynamic was not observed in previous works [30, 58].\nNetwork architecture. The neural network gis composed\nof a backbone f (ViT [19] or ResNet [34]), and of a projec-\ntion head h: g = h◦f. The features used in downstream\ntasks are the backbone f output. The projection head con-\nsists of a 3-layer multi-layer perceptron (MLP) with hidden\ndimension 2048 followed by ℓ2 normalization and a weight\nnormalized fully connected layer [61] with K dimensions,\nwhich is similar to the design from SwA V [10]. We have\ntested other projection heads and this particular design ap-\npears to work best for DINO (Appendix C). We do not use a\npredictor [30, 16], resulting in the exact same architecture in\nboth student and teacher networks. Of particular interest, we\nnote that unlike standard convnets, ViT architectures do not\nuse batch normalizations (BN) by default. Therefore, when\napplying DINO to ViT we do not use any BN also in the\nprojection heads, making the system entirely BN-free.\nAvoiding collapse. Several self-supervised methods dif-\nfer by the operation used to avoid collapse, either through\ncontrastive loss [73], clustering constraints [8, 10], predic-\ntor [30] or batch normalizations [30, 58]. While our frame-\nwork can be stabilized with multiple normalizations [ 10],\nit can also work with only a centering and sharpening of\nthe momentum teacher outputs to avoid model collapse. As\nshown experimentally in Section 5.3, centering prevents\none dimension to dominate but encourages collapse to the\nuniform distribution, while the sharpening has the oppo-\nsite effect. Applying both operations balances their effects\nwhich is sufﬁcient to avoid collapse in presence of a momen-\ntum teacher. Choosing this method to avoid collapse trades\nstability for less dependence over the batch: the centering\noperation only depends on ﬁrst-order batch statistics and\ncan be interpreted as adding a bias term c to the teacher:\ngt(x) ←gt(x) +c. The center cis updated with an expo-\nnential moving average, which allows the approach to work\nwell across different batch sizes as shown in Section 5.5:\nc←mc+ (1−m) 1\nB\nB∑\ni=1\ngθt (xi), (4)\nwhere m >0 is a rate parameter and B is the batch size.\nOutput sharpening is obtained by using a low value for the\ntemperature τt in the teacher softmax normalization.\n3.2. Implementation and evaluation protocols\nIn this section, we provide the implementation details to\ntrain with DINO and present the evaluation protocols used\nin our experiments.\nVision Transformer. We brieﬂy describe the mechanism\nof the Vision Transformer (ViT) [ 19, 70] and refer to\nVaswani et al. [70] for details about Transformers and to\nDosovitskiy et al. [19] for its adaptation to images. We fol-\nlow the implementation used in DeiT [69]. We summarize\nthe conﬁguration of the different networks used in this pa-\nper in Table 1. The ViT architecture takes as input a grid\nof non-overlapping contiguous image patches of resolution\nN ×N. In this paper we typically use N = 16 (“/16”)\nor N = 8 (“/8”). The patches are then passed through a\nlinear layer to form a set of embeddings. We add an extra\nlearnable token to the sequence [ 18, 19]. The role of this\ntoken is to aggregate information from the entire sequence\nand we attach the projection head hat its output. We refer\nto this token as the class token [CLS] for consistency with\nprevious works[18, 19, 69], even though it is not attached\nto any label nor supervision in our case. The set of patch\ntokens and [CLS] token are fed to a standard Transformer\nnetwork with a “pre-norm” layer normalization [11, 39]. The\nTransformer is a sequence of self-attention and feed-forward\nlayers, paralleled with skip connections. The self-attention\nlayers update the token representations by looking at the\nother token representations with an attention mechanism [4].\nImplementation details. We pretrain the models on the\nImageNet dataset [ 60] without labels. We train with the\nadamw optimizer [44] and a batch size of 1024, distributed\nover 16 GPUs when using ViT-S/16. The learning rate is\nlinearly ramped up during the ﬁrst 10 epochs to its base\nvalue determined with the following linear scaling rule [29]:\nlr= 0.0005 ∗batchsize/256. After this warmup, we decay\nthe learning rate with a cosine schedule [ 43]. The weight\ndecay also follows a cosine schedule from 0.04 to 0.4. The\ntemperature τs is set to 0.1 while we use a linear warm-up\nfor τt from 0.04 to 0.07 during the ﬁrst 30 epochs. We\nfollow the data augmentations of BYOL [30] (color jittering,\nGaussian blur and solarization) and multi-crop [10] with a\nbicubic interpolation to adapt the position embeddings to\nthe scales [19, 69]. The code and models to reproduce our\nresults is publicly available.\nEvaluation protocols. Standard protocols for self-\nsupervised learning are to either learn a linear classiﬁer\non frozen features [ 82, 33] or to ﬁnetune the features\non downstream tasks. For linear evaluations, we apply\nrandom resize crops and horizontal ﬂips augmentation\nduring training, and report accuracy on a central crop.\nFor ﬁnetuning evaluations, we initialize networks with\nthe pretrained weights and adapt them during training.\nHowever, both evaluations are sensitive to hyperparameters,\nand we observe a large variance in accuracy between runs\nwhen varying the learning rate for example. We thus also\nevaluate the quality of features with a simple weighted\nnearest neighbor classiﬁer ( k-NN) as in [ 73]. We freeze\nthe pretrain model to compute and store the features of the\ntraining data of the downstream task. The nearest neighbor\nclassiﬁer then matches the feature of an image to the k\nnearest stored features that votes for the label. We sweep\nover different number of nearest neighbors and ﬁnd that\n20 NN is consistently working the best for most of our\nruns. This evaluation protocol does not require any other\nhyperparameter tuning, nor data augmentation and can be\nrun with only one pass over the downstream dataset, greatly\nsimplifying the feature evaluation.\nTable 2: Linear and k-NN classiﬁcation on ImageNet. We report\ntop-1 accuracy for linear and k-NN evaluations on the validation\nset of ImageNet for different self-supervised methods. We focus\non ResNet-50 and ViT-small architectures, but also report the best\nresults obtained across architectures. ∗ are run by us. We run the\nk-NN evaluation for models with ofﬁcial released weights. The\nthroughput (im/s) is calculated on a NVIDIA V100 GPU with 128\nsamples per forward. Parameters (M) are of the feature extractor.\nMethod Arch. Param. im/s Linear k-NN\nSupervised RN50 23 1237 79.3 79.3\nSCLR [12] RN50 23 1237 69.1 60.7\nMoCov2 [15] RN50 23 1237 71.1 61.9\nInfoMin [67] RN50 23 1237 73.0 65.3\nBarlowT [81] RN50 23 1237 73.2 66.0\nOBoW [27] RN50 23 1237 73.8 61.9\nBYOL [30] RN50 23 1237 74.4 64.8\nDCv2 [10] RN50 23 1237 75.2 67.1\nSwA V [10] RN50 23 1237 75.3 65.7\nDINO RN50 23 1237 75.3 67.5\nSupervised ViT-S 21 1007 79.8 79.8\nBYOL∗ [30] ViT-S 21 1007 71.4 66.6\nMoCov2∗ [15] ViT-S 21 1007 72.7 64.4\nSwA V∗ [10] ViT-S 21 1007 73.5 66.3\nDINO ViT-S 21 1007 77.0 74.5\nComparison across architectures\nSCLR [12] RN50w4 375 117 76.8 69.3\nSwA V [10] RN50w2 93 384 77.3 67.3\nBYOL [30] RN50w2 93 384 77.4 –\nDINO ViT-B/16 85 312 78.2 76.1\nSwA V [10] RN50w5 586 76 78.5 67.1\nBYOL [30] RN50w4 375 117 78.6 –\nBYOL [30] RN200w2 250 123 79.6 73.9\nDINO ViT-S/8 21 180 79.7 78.3\nSCLRv2 [13] RN152w3+SK 794 46 79.8 73.1\nDINO ViT-B/8 85 63 80.1 77.4\n4. Main Results\nWe ﬁrst validate the DINO framework used in this study\nwith the standard self-supervised benchmark on ImageNet.\nWe then study the properties of the resulting features for\nretrieval, object discovery and transfer-learning.\n4.1. Comparing with SSL frameworks on ImageNet\nWe consider two different settings: comparison with the\nsame architecture and across architectures.\nComparing with the same architecture. In top panel of\nTable 2, we compare DINO with other self-supervised meth-\nods with the same architecture, either a ResNet-50 [34] or a\nViT-small (which follows the design of DeiT-S [69]). The\nchoice of ViT-S is motivated by its similarity with ResNet-50\nalong several axes: number of parameters (21M vs 23M),\nTable 3: Image retrieval. We compare the performance in retrieval\nof off-the-shelf features pretrained with supervision or with DINO\non ImageNet and Google Landmarks v2 (GLDv2) dataset. We\nreport mAP on revisited Oxford and Paris. Pretraining with DINO\non a landmark dataset performs particularly well. For reference, we\nalso report the best retrieval method with off-the-shelf features [57].\nROx RPar\nPretrain Arch. Pretrain M H M H\nSup. [57] RN101+R-MAC ImNet 49.8 18.5 74.0 52.1\nSup. ViT-S/16 ImNet 33.5 8.9 63.0 37.2\nDINO ResNet-50 ImNet 35.4 11.1 55.9 27.5\nDINO ViT-S/16 ImNet 41.8 13.7 63.1 34.4\nDINO ViT-S/16 GLDv2 51.5 24.3 75.3 51.6\nthroughput (1237/sec VS 1007 im/sec) and supervised per-\nformance on ImageNet with the training procedure of [69]\n(79.3% VS 79.8%). We explore variants of ViT-S in Ap-\npendix D. First, we observe that DINO performs on par\nwith the state of the art on ResNet-50, validating that DINO\nworks in the standard setting. When we switch to a ViT\narchitecture, DINO outperforms BYOL, MoCov2 and SwA V\nby +3.5% with linear classiﬁcation and by +7.9% withk-NN\nevaluation. More surprisingly, the performance with a sim-\nple k-NN classiﬁer is almost on par with a linear classiﬁer\n(74.5% versus 77.0%). This property emerges only when us-\ning DINO with ViT architectures, and does not appear with\nother existing self-supervised methods nor with a ResNet-50.\nComparing across architectures. On the bottom panel of\nTable 2, we compare the best performance obtained across\narchitectures. The interest of this setting is not to compare\nmethods directly, but to evaluate the limits of a ViT trained\nwith DINO when moving to larger architectures. While\ntraining a larger ViT with DINO improves the performance,\nreducing the size of the patches (“/8” variants) has a bigger\nimpact on the performance. While reducing the patch size\ndo not add parameters, it still leads to a signiﬁcant reduction\nof running time, and larger memory usage. Nonetheless, a\nbase ViT with 8 ×8 patches trained with DINO achieves\n80.1% top-1 in linear classiﬁcation and 77.4% with a k-NN\nclassiﬁer with 10×less parameters and 1.4×faster run time\nthan previous state of the art [13].\n4.2. Properties of ViT trained with SSL\nWe evaluate properties of the DINO features in terms of\nnearest neighbor search, retaining information about object\nlocation and transferability to downstream tasks.\nTable 4: Copy detection. We report the mAP performance in copy\ndetection on Copydays “strong” subset [ 21]. For reference, we\nalso report the performance of the multigrain model [ 5], trained\nspeciﬁcally for particular object retrieval.\nMethod Arch. Dim. Resolution mAP\nMultigrain [5] ResNet-50 2048 2242 75.1\nMultigrain [5] ResNet-50 2048 largest side 800 82.5\nSupervised [69] ViT-B/16 1536 2242 76.4\nDINO ViT-B/16 1536 2242 81.7\nDINO ViT-B/8 1536 3202 85.5\n4.2.1 Nearest neighbor retrieval with DINO ViT\nThe results on ImageNet classiﬁcation have exposed the\npotential of our features for tasks relying on nearest neighbor\nretrieval. In this set of experiments, we further consolidate\nthis ﬁnding on landmark retrieval and copy detection tasks.\nImage Retrieval. We consider the revisited [53] Oxford\nand Paris image retrieval datasets [50]. They contain 3 differ-\nent splits of gradual difﬁculty with query/database pairs. We\nreport the Mean Average Precision (mAP) for the Medium\n(M) and Hard (H) splits. In Table 3, we compare the perfor-\nmance of different off-the-shelf features obtained with either\nsupervised or DINO training. We freeze the features and\ndirectly apply k-NN for retrieval. We observe that DINO\nfeatures outperform those trained on ImageNet with labels.\nAn advantage of SSL approaches is that they can be\ntrained on any dataset, without requiring any form of anno-\ntations. We train DINO on the 1.2M clean set from Google\nLandmarks v2 (GLDv2) [ 72], a dataset of landmarks de-\nsigned for retrieval purposes. DINO ViT features trained on\nGLDv2 are remarkably good, outperforming previously pub-\nlished methods based on off-the-shelf descriptors [68, 57].\nCopy detection. We also evaluate the performance of ViTs\ntrained with DINO on a copy detection task. We report the\nmean average precision on the “strong” subset of the INRIA\nCopydays dataset [ 21]. The task is to recognize images\nthat have been distorted by blur, insertions, print and scan,\netc. Following prior work [5], we add 10k distractor images\nrandomly sampled from the YFCC100M dataset [66]. We\nperform copy detection directly with cosine similarity on the\nfeatures obtained from our pretrained network. The features\nare obtained as the concatenation of the output[CLS] token\nand of the GeM pooled [54] output patch tokens. This results\nin a 1536d descriptor for ViT-B. Following [ 5], we apply\nwhitening on the features. We learn this transformation on\nan extra 20K random images from YFCC100M, distincts\nfrom the distractors. Table 4 shows that ViT trained with\nDINO is very competitive on copy detection.\nTable 5: DA VIS 2017 Video object segmentation. We evaluate\nthe quality of frozen features on video instance tracking. We report\nmean region similarity Jm and mean contour-based accuracy Fm.\nWe compare with existing self-supervised methods and a supervised\nViT-S/8 trained on ImageNet. Image resolution is 480p.\nMethod Data Arch. (J&F)m Jm Fm\nSupervised\nImageNet INet ViT-S/8 66.0 63.9 68.1\nSTM [48] I/D/Y RN50 81.8 79.2 84.3\nSelf-supervised\nCT [71] VLOG RN50 48.7 46.4 50.0\nMAST [40] YT-VOS RN18 65.5 63.3 67.6\nSTC [37] Kinetics RN18 67.6 64.8 70.2\nDINO INet ViT-S/16 61.8 60.2 63.4\nDINO INet ViT-B/16 62.3 60.7 63.9\nDINO INet ViT-S/8 69.9 66.6 73.1\nDINO INet ViT-B/8 71.4 67.9 74.9\nFigure 3: Attention maps from multiple heads. We consider\nthe heads from the last layer of a ViT-S/8 trained with DINO and\ndisplay the self-attention for [CLS] token query. Different heads,\nmaterialized by different colors, focus on different locations that\nrepresents different objects or parts (more examples in Appendix).\n4.2.2 Discovering the semantic layout of scenes\nAs shown qualitatively in Figure 1, our self-attention maps\ncontain information about the segmentation of an image. In\nthis study, we measure this property on a standard benchmark\nas well as by directly probing the quality of masks generated\nfrom these attention maps.\nVideo instance segmentation. In Tab. 5, we evaluate the\noutput patch tokens on the DA VIS-2017 video instance seg-\nmentation benchmark [52]. We follow the experimental pro-\ntocol in Jabri et al. [37] and segment scenes with a nearest-\nneighbor between consecutive frames; we thus do not train\nany model on top of the features, nor ﬁnetune any weights\nfor the task. We observe in Tab. 5 that even though our\ntraining objective nor our architecture are designed for dense\ntasks, the performance is competitive on this benchmark.\nSince the network is not ﬁnetuned, the output of the model\nmust have retained some spatial information. Finally, for\nthis dense recognition task, the variants with small patches\n(“/8”) perform much better (+9.1% (J&F)m for ViT-B).\nProbing the self-attention map. In Fig. 3, we show that\ndifferent heads can attend to different semantic regions of an\nimage, even when they are occluded (the bushes on the third\nrow) or small (the ﬂag on the second row). Visualizations are\nobtained with 480p images, resulting in sequences of 3601\ntokens for ViT-S/8. In Fig. 4, we show that a supervised\nViT does not attend well to objects in presence of clutter\nboth qualitatively and quantitatively. We report the Jaccard\nsimilarity between the ground truth and segmentation masks\nobtained by thresholding the self-attention map to keep 60%\nof the mass. Note that the self-attention maps are smooth\nand not optimized to produce a mask. Nonetheless, we see\na clear difference between the supervised or DINO models\nwith a signiﬁcant gap in terms of Jaccard similarities. Note\nthat self-supervised convnets also contain information about\nsegmentations but it requires dedicated methods to extract it\nfrom their weights [31].\n4.2.3 Transfer learning on downstream tasks\nIn Tab. 6, we evaluate the quality of the features pretrained\nwith DINO on different downstream tasks. We compare\nwith features from the same architectures trained with super-\nvision on ImageNet. We follow the protocol used in Tou-\nvron et al. [69] and ﬁnetune the features on each downstream\ntask. We observe that for ViT architectures, self-supervised\npretraining transfers better than features trained with su-\npervision, which is consistent with observations made on\nconvolutional networks [10, 33, 62]. Finally, self-supervised\npretraining greatly improves results on ImageNet (+1-2%).\n5. Ablation Study of DINO\nIn this section, we empirically study DINO applied to\nViT. The model considered for this entire study is ViT-S. We\nalso refer the reader to Appendix for additional studies.\n5.1. Importance of the Different Components\nWe show the impact of adding different components from\nself-supervised learning on ViT trained with our framework.\nSupervised\nDINO\nRandom Supervised DINO\nViT-S/16 22.0 27.3 45.9\nViT-S/8 21.8 23.7 44.7\nFigure 4: Segmentations from supervised versus DINO. We vi-\nsualize masks obtained by thresholding the self-attention maps to\nkeep 60% of the mass. On top, we show the resulting masks for\na ViT-S/8 trained with supervision and DINO. We show the best\nhead for both models. The table at the bottom compares the Jac-\ncard similarity between the ground truth and these masks on the\nvalidation images of PASCAL VOC12 dataset.\nTable 6: Transfer learning by ﬁnetuning pretrained models on\ndifferent datasets. We report top-1 accuracy. Self-supervised\npretraining with DINO transfers better than supervised pretraining.\nCifar10 Cifar100 INat18 INat19 Flwrs Cars INet\nViT-S/16\nSup. [69] 99.0 89.5 70.7 76.6 98.2 92.1 79.9\nDINO 99.0 90.5 72.0 78.2 98.5 93.0 81.5\nViT-B/16\nSup. [69] 99.0 90.8 73.2 77.7 98.4 92.1 81.8\nDINO 99.1 91.7 72.6 78.6 98.8 93.0 82.8\nIn Table 7, we report different model variants as we add\nor remove components. First, we observe that in the absence\nof momentum, our framework does not work (row 2) and\nmore advanced operations, SK for example, are required to\navoid collapse (row 9). However, with momentum, using\nSK has little impact (row 3). In addtition, comparing rows 3\nand 9 highlights the importance of the momentum encoder\nfor performance. Second, in rows 4 and 5, we observe that\nmulti-crop training and the cross-entropy loss in DINO are\nimportant components to obtain good features. We also ob-\nserve that adding a predictor to the student network has little\nimpact (row 6) while it is critical in BYOL to prevent col-\nlapse [16, 30]. For completeness, we propose in Appendix B\nan extended version of this ablation study.\nImportance of the patch size. In Fig. 5, we compare the\nk-NN classiﬁcation performance of ViT-S models trained\nTable 7: Important component for self-supervised ViT pre-\ntraining. Models are trained for 300 epochs with ViT-S/16. We\nstudy the different components that matter for the k-NN and linear\n(“Lin.”) evaluations. For the different variants, we highlight the\ndifferences from the default DINO setting. The best combination\nis the momentum encoder with the multicrop augmentation and\nthe cross-entropy loss. We also report results with BYOL [ 30],\nMoCo-v2 [15] and SwA V [10].\nMethod Mom. SK MC Loss Pred. k-NN Lin.\n1 DINO ✓ \u0017 ✓ CE \u0017 72.8 76.1\n2 \u0017 \u0017 ✓ CE \u0017 0.1 0.1\n3 ✓ ✓ ✓ CE \u0017 72.2 76.0\n4 ✓ \u0017 \u0017 CE \u0017 67.9 72.5\n5 ✓ \u0017 ✓ MSE \u0017 52.6 62.4\n6 ✓ \u0017 ✓ CE ✓ 71.8 75.6\n7 BYOL ✓ \u0017 \u0017 MSE ✓ 66.6 71.4\n8 MoCov2 ✓ \u0017 \u0017 INCE \u0017 62.0 71.6\n9 SwA V \u0017 ✓ ✓ CE \u0017 64.7 71.8\nSK: Sinkhorn-Knopp, MC: Multi-Crop, Pred.: Predictor\nCE: Cross-Entropy, MSE: Mean Square Error, INCE: InfoNCE\n102 103\nthroughput (im/s)\n72\n74\n76\n78\nImageNet top-1\n5x5 8x8\n16x16\n8x8\n16x16\nViT-B DeiT-S\nFigure 5: Effect of\nPatch Size. k-NN eval-\nuation as a function of\nthe throughputs for dif-\nferent input patch sizes\nwith ViT-B and ViT-S.\nModels are trained for\n300 epochs.\nwith different patch sizes, 16 ×16, 8 ×8 and 5 ×5. We\nalso compare to ViT-B with16 ×16 and 8 ×8 patches. All\nthe models are trained for 300 epochs. We observe that the\nperformance greatly improves as we decrease the size of the\npatch. It is interesting to see that performance can be greatly\nimproved without adding additional parameters. However,\nthe performance gain from using smaller patches comes at\nthe expense of throughput: when using 5 ×5 patches, the\nthroughput falls to 44 im/s, vs 180 im/s for 8×8 patches.\n5.2. Impact of the choice of Teacher Network\nIn this ablation, we experiment with different teacher\nnetwork to understand its role in DINO. We compare models\ntrained for 300 epochs using the k-NN protocol.\nBuilding different teachers from the student. In\nFig. 6(right), we compare different strategies to build the\nteacher from previous instances of the student besides the\n0 100 200 300epochs\n64\n68\n72val acc@1\nStudent\nTeacher\nTeacher Top-1\nStudent copy 0.1\nPrevious iter 0.1\nPrevious epoch 66.6\nMomentum 72.8\nFigure 6: Top-1 accuracy on ImageNet validation withk-NN classi-\nﬁer. (left) Comparison between the performance of the momentum\nteacher and the student during training. (right) Comparison be-\ntween different types of teacher network. The momentum encoder\nleads to the best performance but is not the only viable option.\nmomentum teacher. First we consider using the student net-\nwork from a previous epoch as a teacher. This strategy has\nbeen used in a memory bank [73] or as a form of clustering\nhard-distillation [8, 2, 14]. Second, we consider using the\nstudent network from the previous iteration, as well as a\ncopy of the student for the teacher. In our setting, using a\nteacher based on a recent version of the student does not\nconverge. This setting requires more normalizations to work.\nInterestingly, we observe that using a teacher from the previ-\nous epoch does not collapse, providing performance in the\nk-NN evaluation competitive with existing frameworks such\nas MoCo-v2 or BYOL. While using a momentum encoder\nclearly provides superior performance to this naive teacher,\nthis ﬁnding suggests that there is a space to investigate alter-\nnatives for the teacher.\nAnalyzing the training dynamic. To further understand\nthe reasons why a momentum teacher works well in our\nframework, we study its dynamic during the training of a\nViT in the left panel of Fig. 6. A key observation is that\nthis teacher constantly outperforms the student during the\ntraining, and we observe the same behavior when training\nwith a ResNet-50 (Appendix D). This behavior has not been\nobserved by other frameworks also using momentum [ 33,\n30], nor when the teacher is built from the previous epoch.\nWe propose to interpret the momentum teacher in DINO\nas a form of Polyak-Ruppert averaging [ 51, 59] with an\nexponentially decay. Polyak-Ruppert averaging is often used\nto simulate model ensembling to improve the performance\nof a network at the end of the training [38]. Our method can\nbe interpreted as applying Polyak-Ruppert averaging during\nthe training to constantly build a model ensembling that has\nsuperior performances. This model ensembling then guides\nthe training of the student network [65].\n5.3. Avoiding collapse\nWe study the complementarity role of centering and tar-\nget sharpening to avoid collapse. There are two forms of\n0 100epochs\n0\n2\n4\n6\n8Target Entropy\nsharpening centering both\n0 100epochs\n0\n2KL divergence\nFigure 7: Collapse study. (left): evolution of the teacher’s target\nentropy along training epochs; (right): evolution of KL divergence\nbetween teacher and student outputs.\nTable 8: Time and memory requirements.We show total running\ntime and peak memory per GPU (“mem.”) when running ViT-S/16\nDINO models on two 8-GPU machines. We report top-1 ImageNet\nval acc with linear evaluation for several variants of multi-crop,\neach having a different level of compute requirement.\n100 epochs 300 epochs\nmulti-crop top-1 time top-1 time mem.\n2×2242 67.8 15.3h 72.5 45.9h 9.3G\n2×2242 + 2 ×962 71.5 17.0h 74.5 51.0h 10.5G\n2×2242 + 6 ×962 73.8 20.3h 75.9 60.9h 12.9G\n2×2242 + 10×962 74.6 24.2h 76.1 72.6h 15.4G\ncollapse: regardless of the input, the model output is uniform\nalong all the dimensions or dominated by one dimension.\nThe centering avoids the collapse induced by a dominant\ndimension, but encourages an uniform output. Sharpening\ninduces the opposite effect. We show this complementarity\nby decomposing the cross-entropy H into an entropy hand\nthe Kullback-Leibler divergence (“KL”)DKL:\nH(Pt,Ps) =h(Pt) +DKL(Pt|Ps). (5)\nA KL equal to zero indicates a constant output, and hence\na collapse. In Fig. 7, we plot the entropy and KL during\ntraining with and without centering and sharpening. If one\noperation is missing, the KL converges to zero, indicating\na collapse. However, the entropy hconverges to different\nvalues: 0 with no centering and −log(1/K) with no sharp-\nening, indicating that both operations induce different form\nof collapse. Applying both operations balances these effects\n(see study of the sharpening parameter τt in Appendix D).\n5.4. Compute requirements\nIn Tab. 8, we detail the time and GPU memory require-\nments when running ViT-S/16 DINO models on two8-GPU\nmachines. We report results with several variants of multi-\ncrop training, each having a different level of compute re-\nquirement. We observe in Tab. 8 that using multi-crop im-\nproves the accuracy / running-time tradeoff for DINO runs.\nFor example, the performance is 72.5% after 46 hours of\ntraining without multi-crop (i.e. 2×2242) while DINO in\n2×2242+10×962 crop setting reaches74.6% in 24 hours only.\nThis is an improvement of+2% while requiring 2×less time,\nthough the memory usage is higher (15.4Gversus 9.3G). We\nobserve that the performance boost brought with multi-crop\ncannot be caught up by more training in the 2×2242 setting,\nwhich shows the value of the “local-to-global” augmentation.\nFinally, the gain from adding more views diminishes (+.2%\nform 6×to 10×962 crops) for longer trainings.\nOverall, training DINO with Vision Transformers\nachieves 76.1 top-1 accuracy using two 8-GPU servers for 3\ndays. This result outperforms state-of-the-art self-supervised\nsystems based on convolutional networks of comparable\nsizes with a signiﬁcant reduction of computational require-\nments [30, 10]. Our code is available to train self-supervised\nViT on a limited number of GPUs.\n5.5. Training with small batches\nbs 128 256 512 1024\ntop-1 57.9 59.1 59.6 59.9\nTable 9: Effect of batch\nsizes. Top-1 with k-NN\nfor models trained for 100\nepochs without multi-crop.\nIn Tab. 9, we study the impact of the batch size on the\nfeatures obtained with DINO. We also study the impact\nof the smooth parameter m used in the centering update\nrule of Eq. 4 in Appendix D. We scale the learning rate lin-\nearly with the batch size [29]: lr= 0.0005 ∗batchsize/256.\nTab. 9 conﬁrms that we can train models to high perfor-\nmance with small batches. Results with the smaller batch\nsizes (bs= 128) are slightly below our default training setup\nof bs= 1024, and would certainly require to re-tune hyper-\nparameters like the momentum rates for example. Note that\nthe experiment with batch size of 128 runs on only 1 GPU.\nWe have explored training a model with a batch size of 8,\nreaching 35.2% after 50 epochs, showing the potential for\ntraining large models that barely ﬁt an image per GPU.\n6. Conclusion\nIn this work, we have shown the potential of self-\nsupervised pretraining a standard ViT model, achieving per-\nformance that are comparable with the best convnets speciﬁ-\ncally designed for this setting. We have also seen emerged\ntwo properties that can be leveraged in future applications:\nthe quality of the features in k-NN classiﬁcation has a po-\ntential for image retrieval where ViT are already showing\npromising results [22]. The presence of information about\nthe scene layout in the features can also beneﬁt weakly super-\nvised image segmentation. However, the main result of this\npaper is that we have evidences that self-supervised learning\ncould be the key to developing a BERT-like model based on\nViT. In the future, we plan to explore if pretraining a large\nViT model with DINO on random uncurated images could\npush the limits of visual features [28].\nAcknowledgement. We thank Mahmoud Assran, Matthijs\nDouze, Allan Jabri, Jure Zbontar, Alaaeldin El-Nouby, Y-\nLan Boureau, Kaiming He, Thomas Lucas as well as the\nThoth and FAIR teams for their help, support and discussions\naround this project. Julien Mairal was funded by the ERC\ngrant number 714381 (SOLARIS project) and by ANR 3IA\nMIAI@Grenoble Alpes (ANR-19-P3IA-0003).\nReferences\n[1] Rohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Or-\nmandi, George E Dahl, and Geoffrey E Hinton. Large scale\ndistributed neural network training through online distillation.\narXiv preprint arXiv:1804.03235, 2018. 3\n[2] Yuki Markus Asano, Christian Rupprecht, and Andrea\nVedaldi. Self-labelling via simultaneous clustering and repre-\nsentation learning. In ICLR, 2020. 2, 9\n[3] Mahmoud Assran, Nicolas Ballas, Lluis Castrejon, and\nMichael Rabbat. Recovering petaﬂops in contrastive semi-\nsupervised learning of visual representations. preprint\narXiv:2006.10803, 2020. 14\n[4] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.\nNeural machine translation by jointly learning to align and\ntranslate. preprint arXiv:1409.0473, 2014. 5\n[5] Maxim Berman, Herv ´e J ´egou, Vedaldi Andrea, Iasonas\nKokkinos, and Matthijs Douze. MultiGrain: a uniﬁed im-\nage embedding for classes and instances. arXiv preprint\narXiv:1902.05509, 2019. 6\n[6] Piotr Bojanowski and Armand Joulin. Unsupervised learning\nby predicting noise. In ICML, 2017. 2\n[7] Cristian Buciluˇa, Rich Caruana, and Alexandru Niculescu-\nMizil. Model compression. In SIGKDD, 2006. 3\n[8] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and\nMatthijs Douze. Deep clustering for unsupervised learning of\nvisual features. In ECCV, 2018. 2, 4, 9, 16\n[9] Mathilde Caron, Piotr Bojanowski, Julien Mairal, and Ar-\nmand Joulin. Unsupervised pre-training of image features on\nnon-curated data. In ICCV, 2019. 2, 16\n[10] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal,\nPiotr Bojanowski, and Armand Joulin. Unsupervised learn-\ning of visual features by contrasting cluster assignments. In\nNeurIPS, 2020. 1, 2, 3, 4, 5, 7, 8, 10, 14, 15, 16, 17, 18\n[11] Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson,\nWolfgang Macherey, George Foster, Llion Jones, Niki Parmar,\nMike Schuster, Zhifeng Chen, et al. The best of both worlds:\nCombining recent advances in neural machine translation.\npreprint arXiv:1804.09849, 2018. 5\n[12] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geof-\nfrey Hinton. A simple framework for contrastive learning of\nvisual representations. preprint arXiv:2002.05709, 2020. 2,\n3, 5, 16, 17\n[13] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad\nNorouzi, and Geoffrey Hinton. Big self-supervised models\nare strong semi-supervised learners. In NeurIPS, 2020. 3, 5,\n6, 14\n[14] Weijie Chen, Shiliang Pu, Di Xie, Shicai Yang, Yilu Guo,\nand Luojun Lin. Unsupervised image classiﬁcation for deep\nrepresentation learning. arXiv preprint arXiv:2006.11480,\n2020. 9, 15\n[15] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.\nImproved baselines with momentum contrastive learning.\npreprint arXiv:2003.04297, 2020. 5, 8, 14, 15, 18\n[16] Xinlei Chen and Kaiming He. Exploring simple siamese\nrepresentation learning. preprint arXiv:2011.10566, 2020. 2,\n3, 4, 8, 14, 16, 18\n[17] Marco Cuturi. Sinkhorn distances: Lightspeed computation\nof optimal transport. In NeurIPS, 2013. 15\n[18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional transform-\ners for language understanding. preprint arXiv:1810.04805,\n2018. 1, 4, 5, 19\n[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Transform-\ners for image recognition at scale. preprint arXiv:2010.11929,\n2020. 1, 4, 5, 13\n[20] Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springen-\nberg, Martin Riedmiller, and Thomas Brox. Discriminative\nunsupervised feature learning with exemplar convolutional\nneural networks. TPAMI, 2016. 2\n[21] Matthijs Douze, Herv´e J´egou, Harsimrat Sandhawalia, Lau-\nrent Amsaleg, and Cordelia Schmid. Evaluation of gist de-\nscriptors for web-scale image search. In CIVR, 2009. 6\n[22] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and\nHerv´e J´egou. Training vision transformers for image retrieval.\npreprint arXiv:2102.05644, 2021. 10\n[23] Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto,\nand Nicu Sebe. Whitening for self-supervised representation\nlearning. preprint arXiv:2007.06346, 2020. 2\n[24] Mark Everingham, Luc Van Gool, Christopher KI Williams,\nJohn Winn, and Andrew Zisserman. The pascal visual object\nclasses (voc) challenge. IJCV, 2010. 13\n[25] Zhiyuan Fang, Jianfeng Wang, Lijuan Wang, Lei Zhang,\nYezhou Yang, and Zicheng Liu. Seed: Self-supervised distil-\nlation for visual representation. 2021. 3\n[26] Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick\nP´erez, and Matthieu Cord. Learning representations by pre-\ndicting bags of visual words. In CVPR, 2020. 2\n[27] Spyros Gidaris, Andrei Bursuc, Gilles Puy, Nikos Komodakis,\nMatthieu Cord, and Patrick P ´erez. Online bag-of-visual-\nwords generation for unsupervised representation learning.\narXiv preprint arXiv:2012.11552, 2020. 2, 5\n[28] Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min\nXu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy\nLiptchinsky, Ishan Misra, Armand Joulin, et al. Self-\nsupervised pretraining of visual features in the wild. preprint\narXiv:2103.01988, 2021. 10\n[29] Priya Goyal, Piotr Doll ´ar, Ross Girshick, Pieter Noord-\nhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,\nYangqing Jia, and Kaiming He. Accurate, large minibatch\nsgd: Training imagenet in 1 hour. preprint arXiv:1706.02677,\n2017. 5, 10\n[30] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin\nTallec, Pierre H Richemond, Elena Buchatskaya, Carl Do-\nersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham-\nmad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R´emi\nMunos, and Michal Valko. Bootstrap your own latent: A new\napproach to self-supervised learning. In NeurIPS, 2020. 2, 3,\n4, 5, 8, 9, 10, 14, 15, 16, 18\n[31] Shir Gur, Ameen Ali, and Lior Wolf. Visualization of su-\npervised and self-supervised neural networks via attribution\nguided factorization. preprint arXiv:2012.02166, 2020. 7\n[32] Michael Gutmann and Aapo Hyv ¨arinen. Noise-contrastive\nestimation: A new estimation principle for unnormalized\nstatistical models. In International Conference on Artiﬁcial\nIntelligence and Statistics, 2010. 2\n[33] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In CVPR, 2020. 1, 2, 3, 4, 5, 7, 9,\n16\n[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR, 2016.\n4, 5\n[35] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the\nknowledge in a neural network. preprint arXiv:1503.02531,\n2015. 2, 3\n[36] Jiabo Huang, Qi Dong, Shaogang Gong, and Xiatian Zhu.\nUnsupervised deep learning by neighbourhood discovery. In\nICML, 2019. 2\n[37] Allan Jabri, Andrew Owens, and Alexei A Efros. Space-time\ncorrespondence as a contrastive random walk. 2020. 7\n[38] S´ebastien Jean, Kyunghyun Cho, Roland Memisevic, and\nYoshua Bengio. On using very large target vocabulary for\nneural machine translation. preprint arXiv:1412.2007, 2014.\n4, 9\n[39] Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart,\nand Alexander M Rush. Opennmt: Open-source toolkit for\nneural machine translation. preprint arXiv:1701.02810, 2017.\n5\n[40] Zihang Lai, Erika Lu, and Weidi Xie. Mast: A memory-\naugmented self-supervised tracker. In CVPR, 2020. 7\n[41] Dong-Hyun Lee et al. Pseudo-label: The simple and efﬁcient\nsemi-supervised learning method for deep neural networks.\nIn Workshop on challenges in representation learning, ICML,\n2013. 3\n[42] Junnan Li, Pan Zhou, Caiming Xiong, and Steven C.H. Hoi.\nPrototypical contrastive learning of unsupervised representa-\ntions. ICLR, 2021. 2\n[43] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient\ndescent with warm restarts. preprint arXiv:1608.03983, 2016.\n5\n[44] Ilya Loshchilov and Frank Hutter. Fixing weight decay regu-\nlarization in adam. 2018. 5\n[45] Julien Mairal. Cyanure: An open-source toolbox for empirical\nrisk minimization for python, c++, and soon more. preprint\narXiv:1912.08165, 2019. 13, 14\n[46] Maria-Elena Nilsback and Andrew Zisserman. Automated\nﬂower classiﬁcation over a large number of classes. In 2008\nSixth Indian Conference on Computer Vision, Graphics &\nImage Processing, 2008. 13\n[47] Mehdi Noroozi, Ananth Vinjimoor, Paolo Favaro, and Hamed\nPirsiavash. Boosting self-supervised learning via knowledge\ntransfer. In CVPR, 2018. 3\n[48] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo\nKim. Video object segmentation using space-time memory\nnetworks. In ICCV, 2019. 7\n[49] Hieu Pham, Qizhe Xie, Zihang Dai, and Quoc V Le. Meta\npseudo labels. preprint arXiv:2003.10580, 2020. 14\n[50] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic,\nand Andrew Zisserman. Lost in quantization: Improving\nparticular object retrieval in large scale image databases. In\nCVPR, 2008. 6\n[51] Boris T Polyak and Anatoli B Juditsky. Acceleration of\nstochastic approximation by averaging. SIAM journal on\ncontrol and optimization, 30(4):838–855, 1992. 4, 9, 17\n[52] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar-\nbel´aez, Alex Sorkine-Hornung, and Luc Van Gool. The\n2017 davis challenge on video object segmentation. preprint\narXiv:1704.00675, 2017. 7\n[53] Filip Radenovi ´c, Ahmet Iscen, Giorgos Tolias, Yannis\nAvrithis, and Ond ˇrej Chum. Revisiting oxford and paris:\nLarge-scale image retrieval benchmarking. 2018. 6\n[54] Filip Radenovi´c, Giorgos Tolias, and Ond ˇrej Chum. Fine-\ntuning cnn image retrieval with no human annotation. IEEE\ntransactions on pattern analysis and machine intelligence ,\n2018. 6\n[55] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsuper-\nvised multitask learners. 1\n[56] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaim-\ning He, and Piotr Doll´ar. Designing network design spaces.\nIn CVPR, 2020. 13\n[57] Jerome Revaud, Jon Almaz´an, Rafael S Rezende, and Cesar\nRoberto de Souza. Learning with average precision: Training\nimage retrieval with a listwise loss. In ICCV, 2019. 6\n[58] Pierre H Richemond, Jean-Bastien Grill, Florent Altch ´e,\nCorentin Tallec, Florian Strub, Andrew Brock, Samuel Smith,\nSoham De, Razvan Pascanu, Bilal Piot, et al. Byol works even\nwithout batch statistics. preprint arXiv:2010.10241, 2020. 2,\n4\n[59] David Ruppert. Efﬁcient estimations from a slowly conver-\ngent robbins-monro process. Technical report, 1988. 4, 9\n[60] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, Alexander C Berg, and Li\nFei-Fei. Imagenet large scale visual recognition challenge.\nIJCV, 2015. 1, 5, 13\n[61] Tim Salimans and Diederik P Kingma. Weight normalization:\nA simple reparameterization to accelerate training of deep\nneural networks. NeurIPS, 2016. 4, 16\n[62] Mert Bulent Sariyildiz, Yannis Kalantidis, Diane Larlus, and\nKarteek Alahari. Concept generalization in visual representa-\ntion learning. arXiv preprint arXiv:2012.05649, 2020. 7\n[63] Zhiqiang Shen, Zechun Liu, Jie Qin, Lei Huang, Kwang-\nTing Cheng, and Marios Savvides. S2-bnn: Bridging\nthe gap between self-supervised real and 1-bit neural net-\nworks via guided distribution calibration. arXiv preprint\narXiv:2102.08946, 2021. 3\n[64] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang,\nNicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han Zhang,\nand Colin Raffel. Fixmatch: Simplifying semi-supervised\nlearning with consistency and conﬁdence. In NeurIPS, 2020.\n14\n[65] Antti Tarvainen and Harri Valpola. Mean teachers are\nbetter role models: Weight-averaged consistency targets\nimprove semi-supervised deep learning results. preprint\narXiv:1703.01780, 2017. 3, 4, 9, 17\n[66] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin\nElizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia\nLi. Yfcc100m: The new data in multimedia research. arXiv\npreprint arXiv:1503.01817, 2015. 6\n[67] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan,\nCordelia Schmid, and Phillip Isola. What makes for good\nviews for contrastive learning. NeurIPS, 2020. 5\n[68] Giorgos Tolias, Ronan Sicre, and Herv ´e J´egou. Particular\nobject retrieval with integral max-pooling of cnn activations.\narXiv preprint arXiv:1511.05879, 2015. 6\n[69] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv´e J´egou. Training\ndata-efﬁcient image transformers & distillation through atten-\ntion. preprint arXiv:2012.12877, 2020. 1, 4, 5, 6, 7, 8, 13,\n17\n[70] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 1, 4\n[71] Xiaolong Wang, Allan Jabri, and Alexei A Efros. Learning\ncorrespondence from the cycle-consistency of time. In CVPR,\n2019. 7\n[72] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim.\nGoogle landmarks dataset v2-a large-scale benchmark for\ninstance-level recognition and retrieval. 2020. 6\n[73] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\nUnsupervised feature learning via non-parametric instance\ndiscrimination. In CVPR, 2018. 2, 4, 5, 9, 18\n[74] Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised\ndeep embedding for clustering analysis. In ICML, 2016. 2\n[75] Qizhe Xie, Zihang Dai Dai, Eduard Hovy, Minh-Thang Lu-\nong, and Quoc V . Le. Unsupervised data augmentation for\nconsistency training. preprint arXiv:1904.12848, 2020. 14\n[76] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V\nLe. Self-training with noisy student improves imagenet clas-\nsiﬁcation. In CVPR, 2020. 3\n[77] Haohang Xu, Xiaopeng Zhang, Hao Li, Lingxi Xie, Hongkai\nXiong, and Qi Tian. Seed the views: Hierarchical seman-\ntic alignment for contrastive representation learning. arXiv\npreprint arXiv:2012.02733, 2021. 16\n[78] Qiantong Xu, Tatiana Likhomanenko, Jacob Kahn, Awni\nHannun, Gabriel Synnaeve, and Ronan Collobert. Iter-\native pseudo-labeling for speech recognition. preprint\narXiv:2005.09267, 2020. 3\n[79] I Zeki Yalniz, Herv´e J´egou, Kan Chen, Manohar Paluri, and\nDhruv Mahajan. Billion-scale semi-supervised learning for\nimage classiﬁcation. preprint arXiv:1905.00546, 2019. 3\n[80] Jianwei Yang, Devi Parikh, and Dhruv Batra. Joint unsuper-\nvised learning of deep representations and image clusters. In\nCVPR, 2016. 2\n[81] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St´ephane\nDeny. Barlow twins: Self-supervised learning via redundancy\nreduction. arXiv preprint arXiv:2103.03230, 2021. 2, 5\n[82] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful\nimage colorization. In ECCV, 2016. 5\n[83] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring\nself-attention for image recognition. In CVPR, 2020. 1\n[84] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Tor-\nralba, and Aude Oliva. Learning deep features for scene\nrecognition using places database. In NeurIPS, 2014. 13\n[85] Chengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. Local\naggregation for unsupervised learning of visual embeddings.\nIn ICCV, 2019. 2\nAppendix\nA. Additional Results\nk-NN classiﬁcation. In Tab. 10, we evaluate the frozen\nrepresentations given by ResNet-50 or ViT-small pre-trained\nwith DINO with two evaluation protocols: linear or k-NN.\nFor both evaluations, we extract representations from a pre-\ntrained network without using any data augmentation. Then,\nwe perform classiﬁcation either with weighted k-NN or with\na linear regression learned with cyanure library [45]. In\nTab. 10 we see that ViT-S accuracies are better than accu-\nracies obtained with RN50 both with a linear or a k-NN\nclassiﬁer. However, the performance gap when using the\nk-NN evaluation is much more signiﬁcant than when consid-\nering linear evaluation. For example on ImageNet 1%, ViT-S\noutperforms ResNet-50 by a large margin of +14.1% with\nk-NN evaluation. This suggests that transformers architec-\ntures trained with DINO might offer more model ﬂexibility\nthat beneﬁts the k-NN evaluation. K-NN classiﬁers have\nthe great advantage of being fast and light to deploy, without\nrequiring any domain adaptation. Overall, ViT trained with\nDINO provides features that combine particularly well with\nk-NN classiﬁers.\nSelf-supervised ImageNet pretraining of ViT. In this ex-\nperiment, we study the impact of pretraining a supervised\nViT model with our method. In Tab. 11, we compare the\nperformance of supervised ViT models that are initialized\nwith different pretraining or guided during training with an\nadditional pretrained convnet. The ﬁrst set of models are\nTable 10: k-NN and linear evaluation for ViT-S/16 and ResNet-\n50 pre-trained with DINO. We use ImageNet-1k [ 60] (“Inet”),\nPlaces205 [ 84], PASCAL VOC [ 24] and Oxford-102 ﬂowers\n(“FLOWERS”) [46]. ViT trained with DINO provides features\nthat are particularly k-NN friendly.\nLogistic k-NN\nRN50 ViT-S ∆ RN50 ViT-S ∆\nInet 100% 72.1 75.7 3.6 67.5 74.5 7.0\nInet 10% 67.8 72.2 4.4 59.3 69.1 9.8\nInet 1% 55.1 64.5 9.4 47.2 61.3 14.1\nPl. 10% 53.4 52.1 -1.3 46.9 48.6 1.7\nPl. 1% 46.5 46.3 -0.2 39.2 41.3 2.1\nVOC07 88.9 89.2 0.3 84.9 88.0 3.1\nFLOWERS 95.6 96.4 0.8 87.9 89.1 1.2\nAverage ∆ 2.4 5.6\nTable 11: ImageNet classiﬁcation with different pretraining.\nTop-1 accuracy on ImageNet for supervised ViT-B/16 models using\ndifferent pretrainings or using an additional pretrained convnet to\nguide the training. The methods use different image resolution\n(“res.”) and training procedure (“tr. proc.”), i.e., data augmentation\nand optimization. “MPP” is Masked Patch Prediction.\nPretraining\nmethod data res. tr. proc. Top-1\nPretrain on additional data\nMMP JFT-300M 384 [19] 79.9\nSupervised JFT-300M 384 [19] 84.2\nTrain with additional model\nRand. init. - 224 [69] 83.4\nNo additional data nor model\nRand. init. - 224 [19] 77.9\nRand. init. - 224 [69] 81.8\nSupervised ImNet 224 [69] 81.9\nDINO ImNet 224 [69] 82.8\npretrained with and without supervision on the large curated\ndataset composed of 300M images. The second set of mod-\nels are trained with hard knowledge distillation from a pre-\ntrained supervised RegNetY [56]. The last set of models do\nnot use any additional data nor models, and are initialized ei-\nther randomly or after a pretraining with DINO on ImageNet.\nCompare to random initialization, pretraining with DINO\nleads to a performance gain of +1%. This is not caused by a\nlonger training since pretraining with supervision instead of\nDINO does not improve performance. Using self-supervised\npretraining reduces the gap with models pretrained on extra\ndata or distilled from a convnet.\nTable 12: Low-shot learning on ImageNet with frozen ViT fea-\ntures. We train a logistic regression on frozen features (FROZEN ).\nNote that this FROZEN evaluation is performed without any ﬁne-\ntuning nor data augmentation . We report top-1 accuracy. For\nreference, we show previously published results that uses ﬁnetun-\ning and semi-supervised learning.\nTop 1\nMethod Arch Param. 1% 10%\nSelf-supervised pretraining with ﬁnetuning\nUDA [75] RN50 23 – 68.1\nSimCLRv2 [13] RN50 23 57.9 68.4\nBYOL [30] RN50 23 53.2 68.8\nSwA V [10] RN50 23 53.9 70.2\nSimCLRv2 [16] RN50w4 375 63.0 74.4\nBYOL [30] RN200w2 250 71.2 77.7\nSemi-supervised methods\nSimCLRv2+KD [13] RN50 23 60.0 70.5\nSwA V+CT [3] RN50 23 – 70.8\nFixMatch [64] RN50 23 – 71.5\nMPL [49] RN50 23 – 73.9\nSimCLRv2+KD [13] RN152w3+SK 794 76.6 80.9\nFrozen self-supervised features\nDINO -FROZEN ViT-S/16 21 64.5 72.2\nLow-shot learning on ImageNet. We evaluate the fea-\ntures obtained with DINO applied on ViT-S on low-shot\nlearning. In Tab. 12, we report the validation accuracy of\na logistic regression trained on frozen features ( FROZEN )\nwith 1% and 10% labels. The logistic regression is trained\nwith the cyanure library [ 45]. When comparing mod-\nels with a similar number of parameters and image/sec, we\nobserve that our features are on par with state-of-the-art\nsemi-supervised models. Interestingly, this performance\nis obtained by training a multi-class logistic regression on\nfrozen features, without data augmentation nor ﬁnetuning.\nB. Methodology Comparison\nWe compare the performance of different self-supervised\nframeworks, MoCo-v2 [ 15], SwA V [10] and BYOL [ 30]\nwhen using convnet or ViT. In Tab. 13, we see that when\ntrained with ResNet-50 (convnet), DINO performs on par\nwith SwA V and BYOL. However, DINO unravels its poten-\ntial with ViT, outperforming MoCo-v2, SwA V and BYOL\nby large margins (+4.3% with linear and +6.2% with k-NN\nevaluations). In the rest of this section, we perform ablations\nto better understand the performance of DINO applied to ViT.\nIn particular, we provide a detailed comparison with meth-\nods that either use a momentum encoder, namely MoCo-v2\nand BYOL, and methods that use multi-crop, namely SwA V .\nTable 13: Methodology comparison for DEIT-small and\nResNet-50. We report ImageNet linear and k-NN evaluations\nvalidation accuracy after 300 epochs pre-training. All numbers are\nrun by us and match or outperform published results.\nResNet-50 ViT-small\nMethod Linear k-NN Linear k-NN\nMoCo-v2 71.1 62.9 71.6 62.0\nBYOL 72.7 65.4 71.4 66.6\nSwA V 74.1 65.4 71.8 64.7\nDINO 74.5 65.6 76.1 72.8\nRelation to MoCo-v2 and BYOL. In Tab. 14, we present\nthe impact of ablating components that differ between DINO,\nMoCo-v2 and BYOL: the choice of loss, the predictor in the\nstudent head, the centering operation, the batch normaliza-\ntion in the projection heads, and ﬁnally, the multi-crop aug-\nmentation. The loss in DINO is a cross-entropy on sharpened\nsoftmax outputs (CE) while MoCo-v2 uses the InfoNCE con-\ntrastive loss (INCE) and BYOL a mean squared error on\nl2-normalized outputs (MSE). No sharpening is applied with\nthe MSE criterion. Though, DINO surprisingly still works\nwhen changing the loss function to MSE, but this signiﬁ-\ncantly alters the performance (see rows (1, 2) and (4, 9)).\nWe also observe that adding a predictor has little impact (1,\n3). However, in the case of BYOL, the predictor is critical\nto prevent collapse (7, 8) which is consistent with previous\nstudies [16, 30]. Interestingly, we observe that the teacher\noutput centering avoids collapse without predictor nor batch\nnormalizations in BYOL (7, 9), though with a signiﬁcant\nperformance drop which can likely be explained by the fact\nthat our centering operator is designed to work in combina-\ntion with sharpening. Finally, we observe that multi-crop\nworks particularly well with DINO and MoCo-v2, removing\nit hurts performance by 2 −4% (1 versus 4 and, 5 versus 6).\nAdding multi-crop to BYOL does not work out-of-the-box\n(7, 10) as detailed in Appendix E and further adaptation may\nbe required.\nRelation to SwA V . In Tab. 15, we evaluate the differences\nbetween DINO and SwA V: the presence of the momentum\nencoder and the operation on top of the teacher output. In\nabsence of the momentum, a copy of the student with a stop-\ngradient is used. We consider three operations on the teacher\noutput: Centering, Sinkhorn-Knopp or a Softmax\nalong the batch axis. The Softmax is similar to a single\nSinkhorn-Knopp iteration as detailed in the next paragraph.\nFirst, these ablations show that using a momentum encoder\nsigniﬁcantly improves the performance for ViT (3 versus 6,\nand 2 versus 5). Second, the momentum encoder also avoids\ncollapse when using only centering (row 1). In the absence\nFigure 8: Self-attention for a set of reference points. We visualize the self-attention module from the last block of a ViT-S/8 trained with\nDINO. The network is able to separate objects, though it has been trained with no supervision at all.\nTable 14: Relation to MoCo-v2 and BYOL. We ablate the com-\nponents that differ between DINO, MoCo-v2 and BYOL: the loss\nfunction (cross-entropy, CE, versus InfoNCE, INCE, versus mean-\nsquare error, MSE), the multi-crop training, the centering operator,\nthe batch normalization in the projection heads and the student\npredictor. Models are run for 300 epochs with ViT-S/16. We report\ntop-1 accuracy on ImageNet linear evaluation.\nMethod Loss multi-crop Center. BN Pred. Top-1\n1 DINO CE ✓ ✓ 76.1\n2 – MSE ✓ ✓ 62.4\n3 – CE ✓ ✓ ✓ 75.6\n4 – CE ✓ 72.5\n5 MoCov2 INCE ✓ 71.4\n6 INCE ✓ ✓ 73.4\n7 BYOL MSE ✓ ✓ 71.4\n8 – MSE ✓ 0.1\n9 – MSE ✓ 52.6\n10 – MSE ✓ ✓ ✓ 64.8\nTable 15: Relation to SwA V .We vary the operation on the teacher\noutput between centering, a softmax applied over the batch di-\nmension and the Sinkhorn-Knopp algorithm. We also ablate the\nMomentum encoder by replacing it with a hard copy of the student\nwith a stop-gradient as in SwA V . Models are run for 300 epochs\nwith ViT-S/16. We report top-1 accuracy on ImageNet linear evalu-\nation.\nMethod Momentum Operation Top-1\n1 DINO ✓ Centering 76.1\n2 – ✓ Softmax(batch) 75.8\n3 – ✓ Sinkhorn-Knopp 76.0\n4 – Centering 0.1\n5 – Softmax(batch) 72.2\n6 SwA V Sinkhorn-Knopp 71.8\nof momentum, centering the outputs does not work (4) and\nmore advanced operations are required (5, 6). Overall, these\nablations highlight the importance of the momentum en-\ncoder, not only for performance but also to stabilize training,\nremoving the need for normalization beyond centering.\nDetails on the Softmax(batch) variant. The itera-\ntive Sinkhorn-Knopp algorithm [17] used in SwA V [10] is\nimplemented simply with the following PyTorch style code.\n# x is n-by-K\n# tau is Sinkhorn regularization param\nx = exp(x / tau)\nfor _ in range(num_iters): # 1 iter of Sinkhorn\n# total weight per dimension (or cluster)\nc = sum(x, dim=0, keepdim=True)\nx /= c\n# total weight per sample\nn = sum(x, dim=1, keepdim=True)\n# x sums to 1 for each sample (assignment)\nx /= n\nWhen performing a single Sinkhorn iteration\n(num iters=1) the implementation can be highly\nsimpliﬁed into only two lines of code, which is our\nsoftmax(batch) variant:\nx = softmax(x / tau, dim=0)\nx /= sum(x, dim=1, keepdim=True)\nWe have seen in Tab. 15 that this highly simpliﬁed variant\nof SwA V works competitively with SwA V . Intuitively, the\nsoftmax operation on the batch axis allows to select for\neach dimension (or “cluster”) its best matches in the batch.\nValidating our implementation. We observe in Tab. 13\nthat our reproduction of BYOL, MoCo-v2, SwA V matches\nor outperforms the corresponding published numbers with\nResNet-50. Indeed, we obtain 72.7% for BYOL while [30]\nreport 72.5% in this 300-epochs setting. We obtain 71.1%\nfor MoCo after 300 epochs of training while [ 15] report\n71.1% after 800 epochs of training. Our improvement com-\npared to the implementation of [ 15] can be explained by\nthe use of a larger projection head (3-layer, use of batch-\nnormalizations and projection dimension of 256).\nRelation to other works. DINO is also related to\nUIC [14] that use outputs from the previous epoch as hard\npseudo-labels for “unsupervised classiﬁcation”. However,\nwe use centering to prevent collapse while UIC resorts to\nbalance sampling techniques as in [ 8]. Our work can be\ninterpreted as a soft UIC variant with momentum teacher.\nThe concurrent work CsMI [77] also exhibits strong per-\nformance with simple k-NN classiﬁers on ImageNet, even\nwith convnets. As DINO, CsMI combines a momentum net-\nwork and multi-crop training, which we have seen are both\ncrucial for good k-NN performance in our experiments with\nViTs. We believe studying this work would help us identify-\ning more precisely the components important for goodk-NN\nperformance and leave this investigation for future work.\nC. Projection Head\nSimilarly to other self-supervised frameworks, using a\nprojection head [12] improves greatly the accuracy of our\nmethod. The projection head starts with a n-layer multi-\nlayer perceptron (MLP). The hidden layers are 2048d and\nare with gaussian error linear units (GELU) activations. The\nlast layer of the MLP is without GELU. Then we apply a\nℓ2 normalization and a weight normalized fully connected\nlayer [16, 61] with K dimensions. This design is inspired\nfrom the projection head with a “prototype layer” used in\nSwA V [10]. We do not apply batch normalizations.\nBN-free system. Unlike standard convnets, ViT architec-\ntures do not use batch normalizations (BN) by default. There-\nViT-S, 100 epochs heads w/o BN heads w/ BN\nk-NN top-1 69.7 68.6\nfore, when applying DINO to ViT we do not use any BN also\nin the projection heads. In this table we evaluate the impact\nof adding BN in the heads. We observe that adding BN in\nthe projection heads has little impact, showing that BN is not\nimportant in our framework. Overall, when applying DINO\nto ViT, we do not use any BN anywhere, making the system\nentirely BN-free. This is a great advantage of DINO + ViT to\nwork at state-of-the-art performance without requiring any\nBN. Indeed, training with BN typically slows down trainings\nconsiderably, especially when these BN modules need to be\nsynchronized across processes [33, 10, 9, 30].\nL2-normalization bottleneck in projection head. We il-\nlustrate the design of the projection head with or without l2-\nnormalization bottleneck in Fig. 9. We evaluate the accuracy\n# proj. head linear layers 1 2 3 4\nw/ l2-norm bottleneck – 62.2 68.0 69.3\nw/o l2-norm bottleneck 61.6 62.9 0.1 0.1\nof DINO models trained with or without l2-normalization\nbottleneck and we vary the number of linear layers in the\nprojection head. With l2 bottleneck, the total number of\nf\nn-layer MLP\nl2 normalization\nlinear layer\nx B x 3 x 224 x 224\nB x 384\nB x 256\nB x 256\nB x K\ng(x)\nprojection head h\nf\nn-layer MLP\nx\nB x 384\nB x K\ng(x)\nprojection head h\nB x 3 x 224 x 224\nw/ l2-bottleneck w/o l2-bottleneck\nFigure 9: Projection head design w/ or w/o l2-norm bottleneck.\nlinear layers is n+ 1 (n from the MLP and 1 from the\nweight normalized layer) while without bottleneck the to-\ntal number of linear layers is nin the head. In this table,\nwe report ImageNet top-1 k-NN evaluation accuracy after\n100 epochs pre-training with ViT-S/16. The output dimen-\nsionality K is set to 4096 in this experiment. We observe\nthat DINO training fails without the l2-normalization bot-\ntleneck when increasing the depth of the projection head.\nL2-normalization bottleneck stabilizes the training of DINO\nwith deep projection head. We observe that increasing the\ndepth of the projection head improves accuracy. Our default\nis to use a total of 4 linear layers: 3 are in the MLP and one\nis after the l2 bottleneck.\nOutput dimension. In this table, we evaluate the effect\nof varying the output dimensionality K. We observe that a\nK 1024 4096 16384 65536 262144\nk-NN top-1 67.8 69.3 69.2 69.7 69.1\nlarge output dimensionality improves the performance. We\nnote that the use of l2-normalization bottleneck permits to\nuse a large output dimension with a moderate increase in the\ntotal number of parameters. Our default is to use Kequals\nto 65536 and d= 256for the bottleneck.\nGELU activations. By default, the activations used in ViT\nare gaussian error linear units (GELU). Therefore, for consis-\nViT-S, 100 epochs heads w/ GELU heads w/ ReLU\nk-NN top-1 69.7 68.9\ntency within the architecture, we choose to use GELU also\nin the projection head. We evaluate the effect of using ReLU\ninstead of GELU in this table and observe that changing the\nactivation unit to ReLU has relatively little impact.\nD. Additional Ablations\nWe have detailed in the main paper that the combination\nof centering and sharpening is important to avoid collapse in\nDINO. We ablate the hyperparameters for these two opera-\ntions in the following. We also study the impact of training\nlength and some design choices for the ViT networks.\nOnline centering. We study the impact of the smoothing\nparameters in the update rule for the center c used in the\noutput of the teacher network. The convergence is robust\nm 0 0.9 0.99 0.999\nk-NN top-1 69.1 69.7 69.4 0.1\nto a wide range of smoothing, and the model only collapses\nwhen the update is too slow, i.e., m= 0.999.\nSharpening. We enforce sharp targets by tuning the\nteacher softmax temperature parameter τt. In this table,\nwe observe that a temperature lower than 0.06 is required to\navoid collapse. When the temperature is higher than 0.06,\nτt 0 0 .02 0 .04 0 .06 0 .08 0 .04 →0.07\nk-NN top-1 43.9 66.7 69.6 68.7 0.1 69.7\nthe training loss consistently converges to ln(K). However,\nwe have observed that using higher temperature than 0.06\ndoes not collapse if we start the training from a smaller value\nand increase it during the ﬁrst epochs. In practice, we use\na linear warm-up for τt from 0.04 to 0.07 during the ﬁrst\n30 epochs of training. Finally, note that τ →0 (extreme\nsharpening) correspond to the argmax operation and leads\nto one-hot hard distributions.\nLonger training. We observe in this table that longer train-\ning improves the performance of DINO applied to ViT-Small.\nThis observation is consistent with self-supervised results\nDINO ViT-S 100-ep 300-ep 800-ep\nk-NN top-1 70.9 72.8 74.5\nobtained with convolutional architectures [12]. We note that\nin our experiments with BYOL on ViT-S, training longer\nthan 300 epochs has been leading to worse performance com-\npare our 300 epochs run. For this reason we report BYOL\nfor 300 epochs in Tab. 2 while SwA V , MoCo-v2 and DINO\nare trained for 800 epochs.\nThe teacher outperforms the student. We have shown\nin Fig. 6 that the momentum teacher outperforms the student\nwith ViT and we show in this Figure that it is also the case\nwith ResNet-50. The fact that the teacher continually out-\nperforms the student further encourages the interpretation of\nDINO as a form of Mean Teacher [65] self-distillation. In-\ndeed, as motivated in Tarvainen et al. [65], weight averaging\n0 100epochs\n40\n45\n50\n55\n60val acc@1\nStudent\nTeacher\nusually produces a better model than the individual models\nfrom each iteration [51]. By aiming a target obtained with a\nteacher better than the student, the student’s representations\nimprove. Consequently, the teacher also improves since it is\nbuilt directly from the student weights.\nSelf-attention maps from supervised versus self-\nsupervised learning. We evaluate the masks obtained\nby thresholding the self-attention maps to keep 80% of\nthe mass. We compare the Jaccard similarity between the\nViT-S/16 weights\nRandom weights 22.0\nSupervised 27.3\nDINO 45.9\nDINO w/o multicrop 45.1\nMoCo-v2 46.3\nBYOL 47.8\nSwA V 46.8\nground truth and these masks on the validation images of\nPASCAL VOC12 dataset for different ViT-S trained with\ndifferent frameworks. The properties that self-attention\nmaps from ViT explicitly contain the scene layout and, in\nparticular, object boundaries is observed across different\nself-supervised methods.\nImpact of the number of heads in ViT-S. We study the\nimpact of the number of heads in ViT-S on the accuracy and\nthroughput (images processed per second at inference time\non a singe V100 GPU). We ﬁnd that increasing the number\n# heads dim dim/head # params im/sec k-NN\n6 384 64 21 1007 72.8\n8 384 48 21 971 73.1\n12 384 32 21 927 73.7\n16 384 24 21 860 73.8\nof heads improves the performance, at the cost of a slighlty\nworse throughput. In our paper, all experiments are run with\nthe default model DeiT-S [69], i.e. with 6 heads only.\nE. Multi-crop\nIn this Appendix, we study a core component of DINO:\nmulti-crop training [10].\nRange of scales in multi-crop. For generating the dif-\nferent views, we use the RandomResizedCrop method\nfrom torchvision.transforms module in PyTorch.\nWe sample two global views with scale range(s,1) before\n(0.05, s), (s, 1), s: 0.08 0.16 0.24 0.32 0.48\nk-NN top-1 65.6 68.0 69.7 69.8 69.5\nresizing them to 2242 and 6 local views with scale sampled\nin the range (0.05,s) resized to 962 pixels. Note that we\narbitrarily choose to have non-overlapping scaling range for\nthe global and local views following the original design of\nSwA V . However, the ranges could deﬁnitely be overlapping\nand experimenting with ﬁner hyperparameters search could\nlead to a more optimal setting. In this table, we vary the pa-\nrameter sthat controls the range of scales used in multi-crop\nand ﬁnd the optimum to be around 0.3 in our experiments.\nWe note that this is higher than the parameter used in SwA V\nwhich is of 0.14.\nMulti-crop in different self-supervised frameworks.\nWe compare different recent self-supervised learning frame-\nworks, namely MoCo-v2 [15], BYOL [30] and SwA V [10]\nwith ViT-S/16 architecture. For fair comparisons, all models\ncrops 2 ×2242 2 ×2242 + 6×962\neval k-NN linear k-NN linear\nBYOL 66.6 71.4 59.8 64.8\nSwA V 60.5 68.5 64.7 71.8\nMoCo-v2 62.0 71.6 65.4 73.4\nDINO 67.9 72.5 72.7 75.9\nare pretrained either with two 2242 crops or with multi-\ncrop [10] training, i.e. two 2242 crops and six 962 crops for\neach image. We report k-NN and linear probing evaluations\nafter 300 epochs of training. Multi-crop does not beneﬁt all\nframeworks equally, which has been ignored in benchmarks\nconsidering only the two crops setting [16]. The effective-\nness of multi-crop depends on the considered framework,\nwhich positions multi-crop as a core component of a model\nand not a simple “add-ons” that will boost any framework the\nsame way. Without multi-crop, DINO has better accuracy\nthan other frameworks, though by a moderate margin (1%).\nRemarkably, DINO beneﬁts the most from multi-crop train-\ning (+3.4% in linear eval). Interestingly, we also observe\nthat the ranking of the frameworks depends on the evaluation\nprotocol considered.\nTraining BYOL with multi-crop. When applying multi-\ncrop to BYOL with ViT-S, we observe the transfer perfor-\nmance is higher than the baseline without multi-crop for\nthe ﬁrst training epochs. However, the transfer performance\ngrowth rate is slowing down and declines after a certain\n0 100 200 300epochs\n45\n50\n55\n60\n65k-nn val top-1\nw/o mc\nw/ mc\namount of training. We have performed learning rate, weight\ndecay, multi-crop parameters sweeps for this setting and\nsystematically observe the same pattern. More precisely, we\nexperiment with {1e−5, 3e−5, 1e−4, 3e−4, 1e−3, 3e−3}for\nlearning rate base values, with {0.02, 0.05, 0.1}for weight\ndecay and with different number of small crops: {2, 4, 6}.\nAll our runs are performed with synchronized batch normal-\nizations in the heads. When using a low learning rate, we\ndid not observe the performance break point, i.e. the trans-\nfer performance was improving continually during training,\nbut the overall accuracy was low. We have tried a run with\nmulti-crop training on ResNet-50 where we also observe\nthe same behavior. Since integrating multi-crop training to\nBYOL is not the focus of this study we did not push that\ndirection further. However, we believe this is worth investi-\ngating why multi-crop does not combine well with BYOL in\nour experiments and leave this for future work.\nF. Evaluation Protocols\nF.1 k-NN classiﬁcation\nFollowing the setting of Wu et al. [73], we evaluate the qual-\nity of features with a simple weighted kNearest Neighbor\nclassiﬁer. We freeze the pretrained model to compute and\nstore the features of the training data of the downstream task.\nTo classify a test image x, we compute its representation\nand compare it against all stored training features T. The\nrepresentation of an image is given by the output [CLS]\ntoken: it has dimensionality d= 384for ViT-S andd= 768\nfor ViT-B. The topkNN (denoted Nk) are used to make a\nprediction via weighted voting. Speciﬁcally, the class cgets\na total weight of ∑\ni∈Nk\nαi1ci=c, where αi is a contribution\nweight. We use αi = exp(Tix/τ) with τ equals to 0.07 as\nin [73] which we do not tune. We evaluate different values\nfor kand ﬁnd that k= 20is consistently leading to the best\naccuracy across our runs. This evaluation protocol does not\nrequire hyperparameter tuning, nor data augmentation and\ncan be run with only one pass over the downstream dataset.\nF.2 Linear classiﬁcation\nFollowing common practice in self-supervised learning, we\nevaluate the representation quality with a linear classiﬁer.\nThe projection head is removed, and we train a supervised\nlinear classiﬁer on top of frozen features. This linear clas-\nsiﬁer is trained with SGD and a batch size of 1024 during\n100 epochs on ImageNet. We do not apply weight decay.\nFor each model, we sweep the learning rate value. Dur-\ning training, we apply only random resizes crops (with de-\nfault parameters from PyTorch RandomResizedCrop)\nand horizontal ﬂips as data augmentation. We report central-\ncrop top-1 accuracy. When evaluating convnets, the common\npractice is to perform global average pooling on the ﬁnal\nfeature map before the linear classiﬁer. In the following, we\ndescribe how we adapt this design when evaluating ViTs.\nViT-S representations for linear eval. Following the\nfeature-based evaluations in BERT [ 18], we concatenate\nthe [CLS] tokens from the l last layers. We experiment\nconcatenate l last layers 1 2 4 6\nrepresentation dim 384 768 1536 2304\nViT-S/16 linear eval 76.1 76.6 77.0 77.0\nwith the concatenation of a different number lof layers and\nsimilarly to [18] we ﬁnd l= 4to be optimal.\nViT-B representations for linear eval. With ViT-B we\ndid not ﬁnd that concatenating the representations from the\nlast llayers to provide any performance gain, and consider\nthe ﬁnal layer only ( l = 1). In this setting, we adapt the\npooling strategy [CLS] tok. concatenate [CLS] tok.\nonly and avgpooled patch tok.\nrepresentation dim 768 1536\nViT-B/16 linear eval 78.0 78.2\npipeline used in convnets with global average pooling on the\noutput patch tokens. We concatenate these pooled features\nto the ﬁnal [CLS] output token.\nG. Self-Attention Visualizations\nWe provide more self-attention visualizations in Fig. 8\nand in Fig. 10. The images are randomly selected from\nCOCO validation set, and are not used during training of\nDINO. In Fig. 8, we show the self-attention from the last\nlayer of a DINO ViT-S/8 for several reference points.\nH. Class Representation\nAs a ﬁnal visualization, we propose to look at the distribu-\ntion of ImageNet concepts in the feature space from DINO.\nWe represent each ImageNet class with the average feature\nvector for its validation images. We reduce the dimension\nof these features to 30 with PCA, and run t-SNE with a\nperplexity of 20, a learning rate of 200 for 5000 iterations.\nWe present the resulting class embeddings in Fig. 11. Our\nmodel recovers structures between classes: similar animal\nspecies are grouped together, forming coherent clusters of\nbirds (top) or dogs, and especially terriers (far right).\nDINO Supervised DINO Supervised\nFigure 10: Self-attention heads from the last layer. We look at the attention map when using the [CLS] token as a query for the different\nheads in the last layer. Note that the [CLS] token is not attached to any label or supervision.\ntench\ngoldfish\ngreat white sharktiger shark\nhammerheadelectric ray stingray\ncockhen ostrich\nbrambling\ngoldfinch\nhouse finch\njunco\nindigo bunting robin\nbulbuljay\nmagpie\nchickadee\nwater ouzel\nkite\nbald eagle\nvulture\ngreat grey owl\nEuropean fire salamander\ncommon newt eftspotted salamander\naxolotl\nbullfrog\ntree frog\ntailed frog\nloggerheadleatherback turtle\nmud turtle\nterrapin\nbox turtle\nbanded gecko common iguana\nAmerican chameleonwhiptail\nagama\nfrilled lizard\nalligator lizard\nGila monster\ngreen lizard\nAfrican chameleon\nKomodo dragon\nAfrican crocodile\nAmerican alligator\ntriceratops\nthunder snake\nringneck snakehognose snake\ngreen snake\nking snake\ngarter snake\nwater snake\nvine snake\nnight snake\nboa constrictor\nrock python\nIndian cobra\ngreen mamba\nsea snake\nhorned viper\ndiamondback\nsidewinder\ntrilobiteharvestman scorpion\nblack and gold garden spider\nbarn spidergarden spiderblack widow\ntarantula\nwolf spider\ntick centipede\nblack grouse\nptarmigan\nruffed grouse\nprairie chicken\npeacock\nquailpartridge\nAfrican greymacawsulphur-crested cockatoo\nlorikeet\ncoucal\nbee eater\nhornbillhummingbird\njacamar\ntoucan\ndrake red-breasted merganser\ngoose black swan\ntusker\nechidna\nplatypus\nwallaby\nkoala\nwombat\njellyfish\nsea anemone\nbrain coral\nflatworm\nnematode\nconch\nsnailslug\nsea slug\nchiton\nchambered nautilus\nDungeness crab\nrock crab\nfiddler crab\nking crabAmerican lobster\nspiny lobster\ncrayfish\nhermit crab\nisopod\nwhite stork\nblack stork\nspoonbill\nflamingo\nlittle blue heronAmerican egret\nbittern\ncrane\nlimpkin\nEuropean gallinule\nAmerican coot\nbustard\nruddy turnstone\nred-backed sandpiper\nredshank dowitcher\noystercatcher\npelican\nking penguin\nalbatross\ngrey whale\nkiller whale\ndugong\nsea lion\nChihuahua\nJapanese spaniel\nMaltese dog\nPekinese\nShih-Tzu\nBlenheim spanielpapillon\ntoy terrier\nRhodesian ridgeback\nAfghan hound\nbassetbeagle\nbloodhound\nbluetick\nblack-and-tan coonhound\nWalker houndEnglish foxhound\nredbone\nborzoi\nIrish wolfhound\nItalian greyhound\nwhippet\nIbizan hound\nNorwegian elkhound\notterhound\nSaluki\nScottish deerhound\nWeimaraner\nStaffordshire bullterrier\nAmerican Staffordshire terrier\nBedlington terrier\nBorder terrier\nKerry blue terrier Irish terrier\nNorfolk terrier\nNorwich terrier\nYorkshire terrier\nwire-haired fox terrier\nLakeland terrier\nSealyham terrier\nAiredale cairn\nAustralian terrier\nDandie Dinmont\nBoston bull\nminiature schnauzer\ngiant schnauzer\nstandard schnauzer\nScotch terrier\nTibetan terrier\nsilky terrier\nsoft-coated wheaten terrier\nWest Highland white terrier\nLhasa\nflat-coated retriever\ncurly-coated retriever\ngolden retriever\nLabrador retriever\nChesapeake Bay retriever\nGerman short-haired pointer\nvizsla\nEnglish setter\nIrish setter\nGordon setter\nBrittany spaniel\nclumber\nEnglish springer\nWelsh springer spaniel\ncocker spaniel\nSussex spaniel\nIrish water spaniel\nkuvasz\nschipperkegroenendael\nmalinois\nbriard\nkelpie\nkomondorOld English sheepdog\nShetland sheepdog collie\nBorder collie\nBouvier des Flandres\nRottweiler\nGerman shepherd\nDoberman miniature pinscher\nGreater Swiss Mountain dog\nBernese mountain dog\nAppenzeller\nEntleBucher\nboxer\nbull mastiff\nTibetan mastiff\nFrench bulldog\nGreat Dane\nSaint Bernard\nEskimo dog\nmalamute\nSiberian husky\ndalmatian\naffenpinscher\nbasenji\npug\nLeonberg\nNewfoundland\nGreat Pyrenees\nSamoyed\nPomeranian\nchowkeeshond\nBrabancon griffon\nPembrokeCardigan\ntoy poodleminiature poodle\nstandard poodle\nMexican hairless\ntimber wolf\nwhite wolf\nred wolf\ncoyote\ndingo\ndholeAfrican hunting dog\nhyena red fox\nkit fox\nArctic fox\ngrey fox\ntabby\ntiger cat\nPersian cat\nSiamese catEgyptian cat\ncougar\nlynx\nleopardsnow leopardjaguar\nliontiger\ncheetah\nbrown bear\nAmerican black bear\nice bear\nsloth bear\nmongoose\nmeerkat\ntiger beetle\nladybug\nground beetlelong-horned beetle\nleaf beetle\ndung beetle\nrhinoceros beetle\nweevil\nfly\nbee\nantgrasshopper\ncricket\nwalking stick cockroach\nmantis\ncicada\nleafhopper\nlacewing\ndragonfly\ndamselfly\nadmiral\nringlet\nmonarch\ncabbage butterfly\nsulphur butterfly\nlycaenid\nstarfish\nsea urchin\nsea cucumber\nwood rabbit\nhare\nAngorahamster\nporcupine\nfox squirrel\nmarmot\nbeaver\nguinea pig\nsorrel\nzebra\nhog\nwild boarwarthog\nhippopotamus\nox\nwater buffalo\nbison ram\nbighorn\nibex\nhartebeest\nimpala gazelle\nArabian camel\nllama\nweaselmink\npolecatblack-footed ferret\notter\nskunkbadger\narmadillo\nthree-toed sloth\norangutan\ngorilla chimpanzeegibbon siamang\nguenon\npatas\nbaboon\nmacaque\nlangur\ncolobus\nproboscis monkey\nmarmoset\ncapuchin howler monkeytiti spider monkeysquirrel monkey\nMadagascar catindri\nIndian elephant\nAfrican elephant\nlesser panda giant panda\nbarracoutaeel coho\nrock beauty\nanemone fish\nsturgeongar\nlionfish\npuffer\nabacus\nabaya\nacademic gown\naccordion acoustic guitar\naircraft carrier\nairliner\nairship\naltar\nambulance\namphibian\nanalog clock\napiary\napron\nashcan\nassault rifle\nbackpack\nbakery\nbalance beam\nballoon\nballpoint\nBand Aid\nbanjo\nbannister\nbarbell\nbarber chair\nbarbershop\nbarnbarometer\nbarrel\nbarrow\nbaseball\nbasketball\nbassinet\nbassoon\nbathing cap\nbath towel\nbathtub\nbeach wagon\nbeacon\nbeaker\nbearskin\nbeer bottle\nbeer glass\nbell cote\nbib\nbicycle-built-for-two\nbikini\nbinder\nbinoculars\nbirdhouse boathouse\nbobsled\nbolo tie\nbonnet\nbookcase\nbookshop\nbottlecap\nbow\nbow tie\nbrass\nbrassiere\nbreakwater\nbreastplate\nbroom\nbucket\nbuckle\nbulletproof vest\nbullet train\nbutcher shop\ncab\ncaldron\ncandle\ncannon\ncanoe\ncan opener\ncardigan\ncar mirror\ncarousel\ncarpenters kit\ncarton\ncar wheel\ncash machine\ncassette\ncassette player\ncastle\ncatamaran\nCD player\ncello\ncellular telephone\nchain\nchainlink fence\nchain mail\nchain saw\nchest\nchiffonier\nchime\nchina cabinet\nChristmas stocking\nchurch\ncinema\ncleaver\ncliff dwelling\ncloak\nclog\ncocktail shaker\ncoffee mug\ncoffeepot\ncoil\ncombination lock\ncomputer keyboard\nconfectionery\ncontainer ship\nconvertible\ncorkscrew\ncornet\ncowboy boot\ncowboy hat\ncradle\ncrane\ncrash helmet\ncrate\ncrib\nCrock Pot\ncroquet ball\ncrutch\ncuirass\ndam\ndeskdesktop computer\ndial telephone\ndiaper\ndigital clock\ndigital watch\ndining table\ndishrag\ndishwasher\ndisk brake dock\ndogsled\ndome\ndoormat\ndrilling platform\ndrum\ndrumstick dumbbell\nDutch oven\nelectric fan\nelectric guitar\nelectric locomotive\nentertainment center\nenvelope\nespresso maker\nface powder\nfeather boa\nfile\nfireboat\nfire engine\nfire screen flagpole\nflute\nfolding chair\nfootball helmet\nforklift\nfountain\nfountain pen\nfour-poster\nfreight car\nFrench horn\nfrying pan\nfur coat\ngarbage truck\ngasmask\ngas pump\ngoblet\ngo-kart\ngolf ball\ngolfcart\ngondolagong\ngown\ngrand piano\ngreenhouse\ngrille\ngrocery store\nguillotine\nhair slide\nhair spray\nhalf track\nhammer\nhamper\nhand blower\nhand-held computer\nhandkerchief\nhard disc\nharmonica\nharp\nharvester\nhatchet\nholster\nhome theater\nhoneycomb\nhook\nhoopskirt\nhorizontal bar\nhorse cart\nhourglass\niPod\niron\njack-o-lantern\njean\njeep\njersey\njigsaw puzzle\njinrikisha\njoystick\nkimono\nknee pad\nknot\nlab coat\nladle\nlampshade\nlaptop\nlawn mower\nlens cap\nletter opener\nlibrary\nlifeboat\nlighter\nlimousine\nliner\nlipstick\nLoafer\nlotion\nloudspeaker\nloupe\nlumbermill\nmagnetic compass\nmailbag\nmailbox\nmaillot\nmaillot\nmanhole cover\nmaraca\nmarimbamask\nmatchstick maypole\nmaze\nmeasuring cup\nmedicine chest\nmegalith\nmicrophone\nmicrowave\nmilitary uniform\nmilk can\nminibus\nminiskirt\nminivan\nmissile\nmitten\nmixing bowl\nmobile home\nModel T\nmodem\nmonastery\nmonitor\nmoped\nmortar\nmortarboard\nmosque\nmosquito net\nmotor scooter\nmountain bike\nmountain tent\nmouse\nmousetrap\nmoving van\nmuzzle\nnail\nneck brace\nnecklace\nnipple\nnotebook\nobelisk\noboe\nocarina\nodometer oil filter\norgan\noscilloscope\noverskirt\noxcart\noxygen mask\npacket\npaddle\npaddlewheel\npadlockpaintbrush\npajama\npalace\npanpipe\npaper towel\nparachute\nparallel bars\npark bench\nparking meter\npassenger car\npatio\npay-phone\npedestal\npencil box\npencil sharpenerperfume\nPetri dish\nphotocopier\npick\npickelhaube\npicket fence\npickup\npier\npiggy bank\npill bottle\npillow\nping-pong ball\npinwheel\npirate\npitcher\nplane\nplanetarium\nplastic bag\nplate rack\nplow\nplunger\nPolaroid camera\npole\npolice van\nponcho\npool tablepop bottle\npot\npotters wheel\npower drill\nprayer rug\nprinter\nprison projectile\nprojector\npuck\npunching bag\npurse\nquill\nquilt\nracer\nracket\nradiator\nradio\nradio telescope\nrain barrel\nrecreational vehicle\nreel\nreflex camera\nrefrigerator\nremote control\nrestaurant\nrevolver\nrifle\nrocking chair\nrotisserie\nrubber eraser\nrugby ball\nrule\nrunning shoe\nsafe\nsafety pin\nsaltshaker sandal\nsarong\nsax\nscabbard\nscale\nschool bus\nschooner\nscoreboard\nscreen\nscrew\nscrewdriver\nseat belt\nsewing machine\nshield\nshoe shop\nshoji\nshopping basket\nshopping cart\nshovel\nshower cap\nshower curtain\nski\nski mask\nsleeping bag\nslide rule\nsliding door\nslot\nsnorkel\nsnowmobile\nsnowplow\nsoap dispenser\nsoccer ball\nsock\nsolar dish\nsombrero\nsoup bowl\nspace bar\nspace heater\nspace shuttle\nspatula\nspeedboat\nspider web\nspindle\nsports car\nspotlight\nstage\nsteam locomotive\nsteel arch bridge\nsteel drum\nstethoscope\nstole\nstone wall\nstopwatch\nstove\nstrainer\nstreetcar\nstretcher\nstudio couch\nstupa\nsubmarine\nsuit\nsundial\nsunglass\nsunglasses\nsunscreen\nsuspension bridge\nswab\nsweatshirt\nswimming trunks\nswing\nswitch\nsyringe\ntable lamp\ntank\ntape player\nteapot\nteddy\ntelevision\ntennis ball\nthatch\ntheater curtain\nthimble\nthresher\nthrone\ntile roof\ntoaster\ntobacco shop\ntoilet seat\ntorch\ntotem pole\ntow truck\ntoyshop\ntractor\ntrailer truck\ntray\ntrench coat\ntricycle\ntrimaran\ntripod\ntriumphal arch\ntrolleybus\ntrombone\ntub\nturnstile\ntypewriter keyboard\numbrella unicycle\nupright\nvacuum\nvase\nvault\nvelvet\nvending machine\nvestment\nviaduct\nviolin volleyball\nwaffle iron\nwall clock\nwallet\nwardrobe warplane\nwashbasin\nwasher\nwater bottle\nwater jug\nwater tower\nwhiskey jug\nwhistle\nwig\nwindow screen\nwindow shade\nWindsor tie\nwine bottle\nwing\nwok\nwooden spoon\nwool\nworm fence\nwreck yawl\nyurt\nweb site\ncomic book\ncrossword puzzle\nstreet sign\ntraffic light\nbook jacket\nmenu\nplate\nguacamole\nconsomme\nhot pot\ntrifle\nice cream\nice lolly\nFrench loafbagel\npretzel\ncheeseburger\nhotdog\nmashed potato\nhead cabbagebroccoli cauliflower\nzucchini\nspaghetti squash\nacorn squash\nbutternut squash\ncucumber\nartichoke\nbell pepper\ncardoon mushroom\nGranny Smith\nstrawberry\norange\nlemon\nfig\npineapple\nbanana\njackfruit\ncustard apple\npomegranate\nhay\ncarbonara\nchocolate sauce\ndough\nmeat loaf\npizza\npotpieburrito\nred wine\nespresso\ncup\neggnog\nalp\nbubble\ncliff\ncoral reef\ngeyser\nlakeside\npromontory\nsandbarseashore\nvalley\nvolcano\nballplayer\ngroom\nscuba diver\nrapeseed\ndaisy\nyellow ladys slipper\ncorn\nacorn\nhip buckeye\ncoral fungus\nagaric\ngyromitra\nstinkhorn\nearthstar\nhen-of-the-woods\nbolete\near\ntoilet tissue\nFigure 11: t-SNE visualization of ImageNet classes as represented using DINO. For each class, we obtain the embedding by taking the\naverage feature for all images of that class in the validation set.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7024170160293579
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6985760927200317
    },
    {
      "name": "Machine learning",
      "score": 0.6396309733390808
    },
    {
      "name": "Transformer",
      "score": 0.5999557971954346
    },
    {
      "name": "Segmentation",
      "score": 0.58647221326828
    },
    {
      "name": "Supervised learning",
      "score": 0.5790724158287048
    },
    {
      "name": "Encoder",
      "score": 0.46549901366233826
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.41817381978034973
    },
    {
      "name": "Artificial neural network",
      "score": 0.24338597059249878
    },
    {
      "name": "Engineering",
      "score": 0.08943656086921692
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210101348",
      "name": "Centre Inria de l'Université Grenoble Alpes",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I1294671590",
      "name": "Centre National de la Recherche Scientifique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I899635006",
      "name": "Université Grenoble Alpes",
      "country": "FR"
    }
  ],
  "cited_by": 4155
}