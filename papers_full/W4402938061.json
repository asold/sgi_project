{
  "title": "Depression detection in social media posts using transformer-based models and auxiliary features",
  "url": "https://openalex.org/W4402938061",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5107554372",
      "name": "Marios Kerasiotis",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5027752295",
      "name": "Loukas Ilias",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5015748230",
      "name": "Dimitris Askounis",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2623212788",
    "https://openalex.org/W4283825746",
    "https://openalex.org/W4386453710",
    "https://openalex.org/W3157024423",
    "https://openalex.org/W3099215402",
    "https://openalex.org/W2927590866",
    "https://openalex.org/W4319453460",
    "https://openalex.org/W2937452071",
    "https://openalex.org/W3162632106",
    "https://openalex.org/W2912720349",
    "https://openalex.org/W2250553926",
    "https://openalex.org/W2897583329",
    "https://openalex.org/W2602628430",
    "https://openalex.org/W4307802303",
    "https://openalex.org/W3153685859",
    "https://openalex.org/W3002877865",
    "https://openalex.org/W3199225381",
    "https://openalex.org/W4387620261",
    "https://openalex.org/W3210836789",
    "https://openalex.org/W4380881406",
    "https://openalex.org/W4308216426",
    "https://openalex.org/W4303649016",
    "https://openalex.org/W2889097229",
    "https://openalex.org/W6601218445",
    "https://openalex.org/W4297499142",
    "https://openalex.org/W4293690295",
    "https://openalex.org/W3120564254",
    "https://openalex.org/W3033913896",
    "https://openalex.org/W3015879336",
    "https://openalex.org/W3156819959",
    "https://openalex.org/W4287591007",
    "https://openalex.org/W4224326626",
    "https://openalex.org/W2953413710",
    "https://openalex.org/W3005911073",
    "https://openalex.org/W3199956316",
    "https://openalex.org/W3097146093",
    "https://openalex.org/W2740966010",
    "https://openalex.org/W3005929844",
    "https://openalex.org/W2927148761",
    "https://openalex.org/W4388274365",
    "https://openalex.org/W2797568851",
    "https://openalex.org/W3166185110",
    "https://openalex.org/W4387425005",
    "https://openalex.org/W3033857372",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2898967081",
    "https://openalex.org/W4229049385",
    "https://openalex.org/W3110065222",
    "https://openalex.org/W2963261455",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W4320000454",
    "https://openalex.org/W3101267588",
    "https://openalex.org/W2915177913"
  ],
  "abstract": null,
  "full_text": "1\nDepression Detection in Social Media Posts Using\nTransformer-Based Models and Auxiliary Features\nMarios Kerasiotis, Loukas Ilias, Dimitris Askounis\nAbstract—The detection of depression in social media posts\nis crucial due to the increasing prevalence of mental health\nissues. Traditional machine learning algorithms often fail to\ncapture intricate textual patterns, limiting their effectiveness in\nidentifying depression. Existing studies have explored various\napproaches to this problem but often fall short in terms of accu-\nracy and robustness. To address these limitations, this research\nproposes a neural network architecture leveraging transformer-\nbased models combined with metadata and linguistic markers.\nThe study employs DistilBERT, extracting information from the\nlast four layers of the transformer, applying learned weights,\nand averaging them to create a rich representation of the input\ntext. This representation, augmented by metadata and linguistic\nmarkers, enhances the model’s comprehension of each post.\nDropout layers prevent overfitting, and a Multilayer Percep-\ntron (MLP) is used for final classification. Data augmentation\ntechniques, inspired by the Easy Data Augmentation (EDA)\nmethods, are also employed to improve model performance. Using\nBERT, random insertion and substitution of phrases generate\nadditional training data, focusing on balancing the dataset\nby augmenting underrepresented classes. The proposed model\nachieves weighted Precision, Recall, and F1-scores of 84.26%,\n84.18%, and 84.15%, respectively. The augmentation techniques\nsignificantly enhance model performance, increasing the weighted\nF1-score from 72.59% to 84.15%.\nIndex Terms—Depression Detection, Social Media Analysis,\nTransformer Models, BERT, Neural Network Architecture, Ma-\nchine Learning, Natural Language Processing, Textual Analysis,\nFeature Extraction, Mental Health\nI. I NTRODUCTION\nAccording to the World Health Organization (WHO), a\nmental disorder is characterized by a clinically significant\ndisturbance in an individual’s cognition, emotional regulation,\nor behaviour [1]. WHO’s data from 2019 indicated that 1 in\n8 people suffer from mental disorders. However, the outbreak\nof the COVID-19 pandemic brought unprecedented challenges\nand led to a significant increase in the prevalence of mental\nhealth issues. Specifically, anxiety disorders surged by 26%,\nand depression cases rose by 28% [1]. Among the various\ntypes of mental disorders, depression emerged as a major\nconcern. It is estimated that 280 million people suffer from\ndepression, contributing to a distressing number of over 700\nthousand suicides [2]. In the modern digital age, social media\nplatforms have become popular outlets for individuals to\nshare their emotions and experiences. Many of these users\nare affected by mental health disorders, and researchers have\nrecognized the potential of social media data to reveal valuable\ninsights and linguistic patterns related to mental health. By\nThe authors are with the Decision Support Systems Laboratory, School\nof Electrical and Computer Engineering, National Technical University of\nAthens, 15780 Athens, Greece (e-mail: marioskerasiotis@gmail.com; lil-\nias@epu.ntua.gr; askous@epu.ntua.gr).\nanalyzing the vast quantity of information available on social\nmedia, researchers aim to better understand mental health\nissues and their prevalence.\nIn recent years, the field of depression detection in posts\nhas witnessed significant advancements with the emergence\nof state-of-the-art machine learning models, showing promise\nin identifying potential signs of depression from social media\nposts. However, these models have notable limitations. Firstly,\nmany rely on traditional machine learning algorithms such as\nLogistic Regression (LR) [3], [4], Support Vector Machines\n(SVM) [5]–[7], Decision Trees [5], [8], [9], Random Forest\n(RF) [5], [10], [11], AdaBoost [12], [13], and k-Nearest\nNeighbors (k-NN) [5], [7], which, while useful, may lack the\nsophistication needed to fully grasp the complexities of mental\nhealth-related textual data, leading to suboptimal performance.\nAt the same time, extracting a large number of features is a\ntime-consuming process demanding a level of domain exper-\ntise. The optimal set of features may not be found. Addition-\nally, the training of traditional ML algorithms usually achieves\nsuboptimal performance, while these algorithms present issues\nof generalization to new data. To address this limitation,\nresearchers have turned to more advanced techniques, includ-\ning deep learning approaches like Long Short-Term Memory\n(LSTM) [14]–[16], Convolutional Neural Networks (CNN)\n[17]–[20] , and then transformers [8], [21]–[25] like BERT\n[26]. These deep learning methods have shown promise in cap-\nturing intricate patterns within textual content and achieving\nhigher levels of contextual understanding. However, simpler\nneural networks that do not employ transformers may not\nfully capture these intricate patterns and consequently may\nnot perform as effectively as the transformer-based models,\nwhich are pre-trained with vast corpora [27], [28]. On the\nother hand, many researchers have incorporated transformers\ninto their work, but they often neglect the use of other valuable\nmetadata and linguistic markers that could potentially enhance\nthe classification of individuals as either depressed or not.\nTo overcome the limitations of traditional machine learning\nalgorithms and to leverage the strengths of transformer-based\nmodels, this research introduces a novel neural network archi-\ntecture tailored for depression detection in social media posts.\nThe proposed model builds upon the power of distilbert-base-\nuncased [29], a widely used transformer model, by extracting\ninformation from the last four layers. These layers are then\nmultiplied by learned weights and averaged, resulting in a\nrich representation that captures essential linguistic patterns in\nthe input text. The averaged CLS (classification) tokens from\nBERT, serving as powerful contextual embeddings, are then\nconcatenated with additional relevant metadata and linguistic\nmarkers. This strategic fusion of contextual embeddings and\narXiv:2409.20048v1  [cs.CL]  30 Sep 2024\n2\nauxiliary features enhances the model’s ability to grasp a\nholistic understanding of each post, enabling it to identify\npotential signs of depression more accurately. To further im-\nprove the generalization and robustness of the model, dropout\nlayers are introduced to prevent overfitting during training.\nAfter incorporating dropout, the output is passed through a\nMulti-Layer Perceptron (MLP), which serves as the classifi-\ncation head. The MLP is designed to make the final decision\nregarding the post’s depressive status, providing a multi-\nlabel classification output. By combining the transformer-\nbased contextual embeddings with carefully selected features\nand incorporating dropout regularization, the proposed neural\nnetwork achieves a higher level of sophistication compared to\ntraditional algorithms while avoiding overreliance on vast pre-\ntraining corpora. This allows the model to effectively address\nthe intricacies of mental health-related textual data, leading\nto improved performance in identifying depressive posts on\nsocial media platforms. The integration of additional metadata\nand linguistic markers also contributes to the model’s ability\nto contextualize each post, capturing vital nuances that might\notherwise be overlooked, thus making a valuable contribution\nto the field of depression detection in social media content.\nOur main contributions can be summarized as follows:\n• We propose a data augmentation approach to deal with\nthe imbalanced dataset and improve the performance of\nthe introduced model.\n• We present a new method for recognizing the depression\nseverity level, which enables the fusion of contextual\nembeddings from BERT with valuable metadata and\nlinguistic markers.\n• We introduce a weighted average approach to extract\nhidden states from various transformer layers, effectively\ncapturing both simple and complex semantic information.\n• We conduct a series of ablation experiments to prove the\neffectiveness of the proposed architecture.\nII. R ELATED WORK\nThe landscape of machine learning and natural language\nprocessing research is continuously evolving, and a variety of\ntechniques have been employed to detect signs of depression\nin social media posts. Researchers have leveraged data from\nvarious sources including Twitter, Facebook, Reddit, and Insta-\ngram, applying machine learning algorithms to analyze these\nvast datasets. The rich body of existing literature manifests\nthe diverse approaches and methodologies undertaken to detect\ndepressive symptoms through textual analysis.\nA. Works that use traditional machine learning algorithms\nThe paper by Arag ´on et al. [30] proposes a novel approach\nto detect mental disorders in social media users based on their\nfine-grained emotions and temporal patterns. They compare\ntheir approach with previous works that have used linguistic,\nsentiment, and emotion features for the same task. They also\nconsider deep learning models with attention mechanisms that\ncan capture the temporal changes of emotions over time.\nTheir approach combines two representations based on sub-\nemotions, which are clusters of words that express more\nspecific and subtle emotions. They use a fusion strategy to\nintegrate the presence and variability of sub-emotions in the\nusers’ posts. Their approach achieves better results than the\nbaselines and provides some interpretability by highlighting\nthe relevant sub-emotions for each mental disorder. They also\ncontrast the emotional patterns of depression and anorexia,\nfinding some distinctive features.\nThe paper by Guntuku et al. [31] explores the use of social\nmedia language to measure and understand psychological\nstress. They collect Facebook and Twitter data from 601\nusers who also completed a stress questionnaire. They extract\nlinguistic, sentiment, emotion, and topic features from the\nsocial media posts and use them to predict stress scores.\nThey also use LIWC, a psycholinguistic dictionary, and Ten-\nsistrength, a tool to detect stress expressions, to compare\nthe language of stressed and non-stressed users. They find\nthat Facebook language is more predictive of stress than\nTwitter language, and that fine-grained emotions and temporal\npatterns are important indicators of stress. They also apply\ndomain adaptation techniques to transfer the user-level models\nto county-level Twitter language and validate them against\nsurvey-based stress measurements. They show that language-\nbased stress estimates correlate with health behaviors and\nsocioeconomic characteristics of counties.\nThe paper by Cacheda et al. [11] proposes two machine\nlearning methods for early detection of depression based on\nsocial media posts. They use a dataset collected from Reddit,\nwhich contains writings from users who have self-reported\ndepression diagnosis and a control group. They extract tex-\ntual, semantic, and writing features such as the number of\nwords, time gap between posts, and text similarity from\nthe social media posts and use them to train two random\nforest classifiers: one for detecting depressed subjects and\nanother for identifying non-depressed subjects. They evaluate\ntheir methods using a time-aware metric that rewards early\ndetections and penalizes late detections. They show that the\ndual model outperforms the singleton model and the state-of-\nthe-art baselines by more than 10%.\nThe paper by Coppersmith et al. [32] explores the use\nof Twitter language to quantify mental health signals for\nfour disorders: depression, bipolar, PTSD, and SAD. They\nuse a novel method to collect data from users who self-\nreport their diagnoses, and extract features based on LIWC,\nlanguage models, and pattern of life analytics. They show that\nstatistical classifiers can differentiate users with mental health\ndisorders from control users, and that there are linguistic and\nbehavioral differences among the disorders. They also conduct\na correlation analysis to reveal the relationships between the\nfeatures and the disorders. They suggest that Twitter data can\nprovide a rich source of information for mental health research\nat both individual and population levels.\nB. Works that use neural networks\nThe paper by Tadesse et al. [33] proposes a novel approach\nto detect depression in Reddit social media posts based on fine-\ngrained emotions and temporal patterns. They use a dataset\nof 1841 posts collected from subreddits related to depression\n3\nand non-depression. They extract features based on N-grams,\nLIWC, and LDA from the posts and use them to train five\nclassification models: Logistic Regression, Support Vector\nMachine, Random Forest, Adaptive Boosting, and Multilayer\nPerceptron. They show that the combination of LIWC, LDA,\nand bigram features with the Multilayer Perceptron classifier\nachieves the best performance for depression detection with\n91% accuracy and 93% F1 score. They also analyze the\nlinguistic and emotional differences between depressed and\nnon-depressed users.\nThe paper by Chiong et al. [12] proposes a textual-based\nfeaturing approach for depression detection using machine\nlearning classifiers and social media texts. They use two public\nTwitter datasets and three non-Twitter datasets to train and\ntest various single and ensemble classifiers, such as logistic\nregression, support vector machine, multilayer perceptron, ran-\ndom forest, and adaptive boosting. They also apply dynamic\nsampling methods to deal with imbalanced data. They show\nthat their approach can effectively detect depression in social\nmedia texts even without relying on specific keywords, such\nas ‘depression’ and ‘diagnose’. They also demonstrate the\ngenerality of their approach by applying it to different social\nmedia sources, such as Facebook, Reddit, and an electronic\ndiary.\nThe paper by Yao et al. [17] investigates the detection\nof suicidality among opioid users on Reddit using machine\nlearning methods. They use data from subreddits related to\nsuicide, depression, opioid abuse, and control topics, and\nextract features based on TF-IDF, word embeddings, and char-\nacter embeddings. They compare several traditional and neural\nnetwork classifiers, such as logistic regression, random forest,\nsupport vector machine, FastText, recurrent neural network,\nattention-based bidirectional recurrent neural network, and\nconvolutional neural network. They show that convolutional\nneural network achieves the best performance for both tasks:\ndistinguishing between suicidal and non-suicidal language,\nand detecting opioid addiction among suicidal posts. They\nalso use Amazon Mechanical Turk to annotate out-of-sample\ndata and evaluate the prediction ability of the models. They\nconclude that social media platforms such as Reddit can\nprovide valuable information for mental health research and\nmonitoring.\nThe paper by Trotzek et al. [18] presents a novel approach\nto detect depression in social media users based on linguistic\nmetadata and neural networks. They use the eRisk 2017\ndataset, which contains chronological sequences of posts and\ncomments from Reddit users who self-reported depression\ndiagnosis and a control group. They extract 27 features based\non word and grammar usage, readability, and specific phrases\nfrom the text content of the users and use them to train various\nmachine learning classifiers. They also use word embeddings\nbased on word2vec, fastText, and GloVe to vectorize the\ntext content and feed it to convolutional neural networks.\nThey show that the combination of linguistic metadata and\nconvolutional neural networks achieves the best performance\nfor early detection of depression with 71% F1 score and\n12.13% ERDE5 score. They also propose a modification of\nthe ERDE metric to make it more intuitive and adaptable.\nThe paper by Chen and Wang [34] proposes an advanced\nmodel for Twitter sentiment analysis based on the LSTM-\nCNN model presented by Sosa [35]. The authors combine the\nencoder-decoder framework with the multi-layer LSTM-CNN\nmodel, in which LSTM can ’remember’ forward information\nof the sequence and multi-layer CNN can capture and learn\nlocal information effectively. The encoder-decoder part is used\nto reconstruct the input matrix, making the feature extraction\nand learning more intrinsic and effective. The authors compare\ntheir model with single-layer CNN, multi-layer CNN, LSTM-\nCNN, and CNN-LSTM models on a combined dataset of\ntweets labeled as ’positive’ or ’negative’. The results show\nthat their model achieves the state-of-the-art accuracy of\n78.6%, outperforming all the other models. The authors also\nanalyze the effect of different parameters and layers on the\nperformance of their model. The paper acknowledges some\nlimitations of the model, such as the difficulty of handling\nsarcasm, irony, and humor, and the lack of external knowledge\nsources, such as world knowledge and common sense. Lastly\nit suggests some directions for future work, such as connecting\nthe model with Part-of-Speech (POS) tagging.\nAnshul et al. [36] proposed a multimodal framework for\ndepression detection during Covid-19 using social media data,\nintegrating textual, user-specific, and image analysis. They\nextracted features from tweet content, URLs, and images,\nand utilized a Visual Neural Network (VNN) for image\nembeddings. Their model achieved the best results with a\nprecision of 93.3%, recall of 90.6%, F1-score of 91.9%, and\naccuracy of 91.7%. Limitations include the model’s reliance\non labeled data, potential bias due to the dataset’s specific\ndemographic, and the challenge of accurately interpreting mul-\ntimodal data. Future work will focus on enhancing model gen-\neralization across diverse populations, incorporating additional\ndata sources for robustness, and improving interpretability\nthrough explainable AI techniques.\nC. Works that use transformers\nTransformer models, including BERT and its variants such\nas RoBERTa [37] and DistilBERT [29], are playing a pivotal\nrole in the field of mental health detection on social media, as\ndemonstrated by a variety of complex academic papers.\nThe paper by Uban et al. [8] presents a comprehensive\nstudy of mental disorders in social media, from different\nperspectives. They use three datasets from the eRisk Lab,\nwhich contain Reddit posts from users diagnosed with de-\npression, anorexia, or self-harm. They extract features based\non content, style, emotions, and cognitive styles from the posts\nand use them to train various deep learning models, such\nas bidirectional LSTM, CNN, hierarchical attention network,\nand transformers. They show that the hierarchical attention\nnetwork with LSTM encoders achieves the best performance\nfor early detection of mental disorders, with high F1 and AUC\nscores. They also interpret the behavior of their models by\nanalyzing the attention weights, the feature importance, and\nthe correlation between emotions and cognitive styles. They\nreveal some distinctive linguistic and emotional patterns for\neach mental disorder and provide insights for mental health\nresearch.\n4\nThe paper by Lin et al. [22] presents SenseMood, a system\nfor depression detection on social media using multimodal\ndata. They use a Twitter dataset that contains posts from\nusers who self-reported depression diagnosis and a control\ngroup. They extract features based on CNN and BERT from\nthe images and texts posted by the users and use them to\ntrain a neural network classifier. They show that their system\ncan achieve better performance than several baselines and\nprovide an analysis report for each user. They also discuss the\nchallenges and opportunities of using social media for mental\nhealth research and monitoring.\nThe paper by Rao et al. [23] proposes two hierarchical posts\nrepresentations models for identifying depressed individuals\nin online forums, namely MGL-CNN and SGL-CNN. The\nmodels use convolutional neural networks with gated units to\nlearn the key features of the posts and the users’ emotional\nstates. In addition to the MGL-CNN and SGL-CNN models,\nthe paper also introduces a novel approach that combines\nBERT with LSTM. This hybrid model leverages the strengths\nof BERT in understanding the context of words in sentences\nand LSTM’s ability to remember patterns over time. This\ncombination allows the model to capture both the semantic\nmeaning of the posts and the temporal patterns of the users’\nemotional states. The models are evaluated on two datasets:\nthe Reddit Self-reported Depression Diagnosis dataset and the\nEarly Detection of Depression (eRisk) dataset. The results\nshow that the models outperform several baselines and state-\nof-the-art methods in terms of precision, recall, and F1-score.\nThe models also demonstrate robustness and generality across\ndifferent online forums. The paper discusses the limitations\nof the models, such as the difficulty of capturing long-term\ndependency and the lack of external knowledge. For future\nwork, the authors will explore the application of MGL-CNN\nand SGL-CNN to general document-level sentiment analysis.\nThe paper by Kabir et al. [24] presents DEPTWEET, a\nnew dataset for depression detection on social media using\nfine-grained emotions and temporal patterns. They use a\ntypology based on the PHQ-9 questionnaire and the mood\nscale of BipolarUK to label tweets as non-depressed, mildly\ndepressed, moderately depressed, or severely depressed. They\nalso provide a confidence score for each label to reflect the\nannotator agreement. They collect 40191 tweets from Twitter\nusing 88 depression-related keywords and annotate them with\nthe help of expert psychologists. They compare four baseline\nmodels: SVM, BiLSTM, BERT, and DistilBERT, and show\nthat DistilBERT achieves the best performance with 82% F1\nscore. They also analyze the linguistic and emotional features\nof different depression severities.\nThe paper by Murarka et al. [25] proposes a RoBERTa-\nbased classifier for detecting and classifying mental illnesses\non social media using fine-grained emotions and temporal\npatterns. They use a new dataset collected from Reddit,\nwhich contains posts from users who self-reported depression,\nanxiety, bipolar disorder, ADHD, or PTSD diagnosis and a\ncontrol group. They use LSTM, BERT and RoBERTa and\nthey show that their system can achieve better performance\nthan several baselines and provide an analysis report for each\nuser. They also discuss the challenges and opportunities of\nusing social media for mental health research and monitoring\nand as part of their future work they plan to collaborate with\nprofessionals to annotate their dataset.\nThe paper by Wang et al. [38] explores the potential of\ndeep-learning methods with pretrained language represen-\ntation models for depression risk prediction from Chinese\nmicroblogs. The authors use a manually annotated dataset of\n13993 microblogs collected from Sina Weibo, and compare\nthree deep-learning methods: BERT, RoBERTa, and XLNET.\nThey also investigate the effect of further pretraining the lan-\nguage models on a large-scale unlabeled corpus from Weibo.\nThe results show that the deep-learning methods outperform\nprevious methods, and that further pretraining leads to better\nperformance. The authors acknowledge the limitations of their\ndataset, such as data imbalance and ambiguous words, and\nsuggest possible directions for future work, such as expanding\nthe dataset, using user-level context, and incorporating medical\nknowledge. The paper contributes to the field of depression\nhealth care by demonstrating the feasibility and effectiveness\nof using social media data and deep-learning methods to\ndiscover potential patients with depression and to trace their\nmental health conditions.\nThe paper by Malviya et al. [39] presents a novel approach\nfor detecting depression in social media users using trans-\nformer models. They use a dataset collected from Reddit,\nwhich contains posts from users who self-reported depression\ndiagnosis and a control group. The authors extract features\nbased on TF-IDF, word embeddings, and character embed-\ndings from the posts and use them to train various classi-\nfiers, including Support Vector Machines, Linear Classifiers,\nBagging models, Boosting models, and Transformer models.\nThey show that the transformer model achieves the highest\naccuracy (98%) in detecting depression in social media users.\nThe authors conclude that transformer models significantly\noutperform conventional models used for depression detection.\nThe paper by Ilias and Askounis [40] introduces a novel\nmethod for detecting stress and depression in social media\nusing multitask learning. The authors propose two architec-\ntures that use depression detection as the primary task and\nstress detection as the auxiliary task. The first architecture\nconsists of a shared BERT layer and two task-specific BERT\nlayers, while the second architecture adds an attention fusion\nnetwork to weight the shared and task-specific representations.\nThe authors use two different datasets for the primary and\nauxiliary tasks, collected under different conditions, to make\nthe problem more challenging and realistic. The authors com-\npare their approaches with state-of-the-art methods, single-task\nlearning, and transfer learning, and show that their multitask\nlearning frameworks achieve better performance for depression\ndetection.\nThe paper by Haque et al. [41] proposes a novel approach\nto detect suicidal ideation on social media using transformer\nmodels. They use a dataset of 3549 posts from Reddit,\ncollected from the SuicideWatch subreddit and other non-\nsuicidal subreddits. They extract features based on TF-IDF,\nword embeddings, and character embeddings from the posts\nand use them to train various classifiers, such as Bi-LSTM,\nBERT, ALBERT, ROBERTa, and XLNET. They show that\n5\nROBERTa outperforms all the other models with 95.21% accu-\nracy, 98.44% recall, 92.67% precision, and 95.47% F1-score.\nThey conclude that transformer models are more effective\nthan conventional deep learning models for suicidal ideation\ndetection.\nThe paper by Thushari et al. [42] focuses on identifying\npsychological well-being through Reddit posts from mental\nhealth groups like depression, anxiety, bipolar disorder, and\nSuicideWatch using the SWMH dataset. They employed the\nMentalBERT model, which outperformed other models with\na 76.70% accuracy. The study also highlights the importance\nof explainable AI, using LIME to validate the model’s trust-\nworthiness, which is crucial for understanding the complex\ninterrelationships in mental health data.\nThe paper by Vajrobol et al. [43] addresses the challenge of\ndetecting depression in low-resource languages, specifically\nfocusing on Thai. To overcome the limited availability of\nannotated data and language models, the authors propose trans-\nferring knowledge from English to Thai. Their approach uses\nRoBERTa, which achieved the highest accuracy at 77.97%,\nwith strong recall, precision, and F1-scores. The study also\nemphasizes the importance of explainable NLP, demonstrating\nthat RoBERTa effectively captures the context in both depres-\nsion and non-depression classes.\nThese studies demonstrate the importance of pre-processing,\nfeature extraction, and the careful selection of machine learn-\ning algorithms, while also revealing ongoing limitations such\nas over fitting, small dataset sizes, annotator biases, and the\nlack of age and gender awareness. Future work in the field may\ncontinue to refine pre-processing techniques, mitigate dataset\nlimitations, incorporate domain expertise, and investigate hy-\nbrid neural network architectures to enhance the efficacy of\ndepression detection models.\nD. Related Work Review Findings\nIn the context of the related literature, the limitations\ncommonly observed across the existing methods include over\nfitting, small data set sizes, annotator biases, and a lack of\nage and gender awareness. Furthermore, some models tend to\nlean heavily on vast pre-training corpora, often leading to over-\ndependence on data and under performance when faced with\nunique or unfamiliar cases. Others might fall short of capturing\nall the nuances and contextual information necessary for the\naccurate detection of depressive symptoms. Many models may\nalso overlook the crucial role that auxiliary features, such\nas metadata and linguistic markers, can play in enriching\nthe understanding of the text. Consequently, while significant\nstrides have been made in the field, there remains a need\nfor a robust and comprehensive model that can overcome\nthese limitations while effectively addressing the intricacies\nof mental health-related textual data.\nThe proposed model in this research offers several advance-\nments and improvements over the methodologies reported\nin the literature. First, it leverages the power of BERT,\na widely used transformer model, to extract detailed lin-\nguistic patterns from the input text. This transformer-based\napproach overcomes the limitations of traditional machine\nlearning algorithms, offering a more sophisticated method\nfor interpreting the complexities of natural language. For\nexample, shallow ML algorithms and deep neural networks\noften use GloVe, word2vec, or fastText embeddings. On the\ncontrary, transformer-based networks, i.e., BERT, can capture\nthe context of the input sequence effectively. Second, the\nmodel does not solely rely on transformer-based contextual\nembeddings. Instead, it incorporates auxiliary features - meta-\ndata and linguistic markers - into the analysis. This strategic\nfusion enhances the model’s ability to comprehend each post\nholistically, capturing crucial nuances that might otherwise\nbe overlooked. Consequently, this allows for a more accurate\nidentification of potential signs of depression.\nBy overcoming the limitations of previous methodologies\nand introducing new elements such as auxiliary feature in-\ntegration, the proposed model achieves a higher level of\nsophistication. It effectively addresses the challenges inherent\nin identifying depressive symptoms in social media content,\nleading to more accurate and reliable results. As such, this re-\nsearch makes a valuable contribution to the field of depression\ndetection in social media posts.\nIII. D ATASET AND PREPROCESSING\nIn this study, we utilized the ”Depression Severity Dataset”\n[44] for the task of early identification of depression severity\nlevels on Reddit using ordinal classification. The dataset was\ncollected and curated by Naseem et al. and was presented\nat the ACM Web Conference 2022. Researchers interested in\nutilizing the ”Depression Severity Dataset” for their studies\ncan access it from the GitHub repository 1. The main objective\nof the dataset is to aid in the early identification of depression\nseverity levels based on the textual content of Reddit posts.\nEach instance in the dataset consists of two columns:\n1) Text: This column contains the textual content of the\nReddit posts. It serves as the input data for the classifi-\ncation task.\n2) Label: The label column represents the depression\nseverity level associated with each Reddit post. The\nseverity levels are categorized into four ordinal classes:\nMinimum (2587 posts), Mild (290 posts), Moderate\n(394), and Severe (282 posts).\nA. Dataset augmentation\nTo increase the size of the dataset and make it more robust\nfor training our neural network, we applied data augmentation\ntechniques. The goal was to generate additional samples from\nthe existing dataset by perturbing the original text content\nwithout changing the underlying semantics. We focused on\nthe Moderate, Mild, and Severe classes, as these classes had\nfewer instances compared to the Minimum class.\nFirstly, we balanced the dataset to address the class imbal-\nance issue. We sampled 250 instances from the Moderate class,\n290 instances from the Mild class, and 281 instances from the\nSevere class. These samples were carefully selected to ensure\nthat the data are not merely repeated but instead varied through\n1https://github.com/usmaann/Depression Severity Dataset\n6\nphrase changes to create a larger, more diverse dataset while\nmaintaining a balanced augmentation that does not overly\nequalize the data but keeps a proportional representation the\ndepression severity levels.\nIn designing our approach to data augmentation, we took\ninspiration from the paper by Wei and Zou [45]. Their work\nintroduced EDA, a set of four easy but powerful techniques:\nsynonym replacement, random insertion, random swap, and\nrandom deletion. They demonstrated the effectiveness of these\ntechniques in boosting performance, particularly for smaller\ndatasets. Inspired by their work, we adapted and utilized a\nsubset of these techniques, specifically insertion and substitu-\ntion, to augment our selected samples.\nTo implement these techniques, we employed the pre-\ntrained BERT (Bidirectional Encoder Representations from\nTransformers) model with the ”bert-base-uncased” model [26].\nBERT is a powerful language model capable of generating\ncontextual embeddings for words and sentences. In the inser-\ntion process, we randomly inserted new words into the text,\nleveraging BERT’s language modeling capabilities to ensure\nthe inserted words made contextual sense. In the substitution\nprocess, we replaced certain words with their contextual syn-\nonyms provided by BERT, mirroring the synonym replacement\ntechnique proposed by Wei and Zou.\nAfter applying data augmentation to the selected samples,\nwe observed changes in the distribution of depression severity\nlevels. The proportions of the classes were modified as per\nTable I. As a result of the data augmentation process, the\ndataset now contains a total of 4,353 rows, with a more\nbalanced representation of the different depression severity\nlevels. This process is in line with the insights of the EDA\npaper, which emphasized the effectiveness of such augmen-\ntation methods, especially when dealing with imbalanced or\nsmaller datasets. By adopting these principles, we believe our\ndataset is now more robust and suitable for training our neural\nnetwork, aligning with the positive results demonstrated in the\nreferenced work.\nTABLE I\nLABEL DISTRIBUTION BEFORE AND AFTER TEXT AUGMENTATION .\nLabel Before Augmentation (%) After Augmentation (%)\nMinimum 72.68% 61.53%\nMild 11.16% 14.24%\nModerate 8.21% 12.22%\nSevere 7.96% 12.01%\nB. Dataset preparation\nPrior to utilizing the data for training our neural network,\nwe conducted a comprehensive preprocessing routine to ensure\nthe text was in a consistent and clean format. The prepro-\ncessing steps included converting all the text to lowercase\nto maintain uniformity, removing any HTML code present in\nthe textual content, eliminating URLs and hyperlinks as they\ndo not contribute to the semantics of the text, and removing\nsubreddit mentions and user mentions to focus solely on the\nmeaningful content. We also removed emojis, as they are\ngraphical representations and not part of the natural language.\nPunctuation marks were removed to simplify the input data,\nand we resolved abbreviations and expanded contractions to\nensure clarity and consistency. By performing these prepro-\ncessing steps, we obtained a clean and standardized version\nof the textual content from the dataset. This cleaned data will\nfacilitate the training of our neural network, enabling it to\nfocus on the meaningful patterns and linguistic features present\nin the text, ultimately leading to better performance in the task\nof classifying the severity of the posts.\nC. Feature extraction\nFeature extraction is a critical step in natural language\nprocessing, where we transform the preprocessed text into\nnumerical representations that can be fed into machine learn-\ning models. In our study, we employed two different ap-\nproaches for feature extraction: utilizing depression medication\nreferences and leveraging pre-trained emotion and sentiment\nmodels.\nTo incorporate potential markers of depression severity in\nthe textual content, we performed feature extraction based on\nthe presence of depression medication references. We curated\na list of depression medications from a reputable source,\nDrugs.com [46], and searched for their occurrences in the\nposts. The idea behind this approach is that references to\ndepression medication may indicate a higher likelihood of the\nposts belonging to the Moderate or Severe classes, as users\nmight be discussing their treatment or experiences with such\nmedications. This method of feature extraction allows us to\nleverage real-world data to potentially identify more severe\ncases of depression.\nTo capture the emotional content of the Reddit posts, we\nutilized the EmoRoBERTa [47] model, which is a pre-trained\nlanguage model specifically designed for emotion recognition.\nEmoRoBERTa can infer the emotions expressed in the text,\nsuch as happiness, sadness, fear, etc. By extracting emotions\nfrom the posts, we aimed to identify potential emotional\npatterns associated with different depression severity levels.\nFor instance, an increased prevalence of negative emotions\nmight be indicative of higher depression severity.\nSentiment analysis helps us gauge the overall sentiment of\nthe Reddit posts as positive, negative, or neutral. For this pur-\npose, we employed the CardiffNLP Twitter-RoBERTa model\n[48], which is pre-trained on social media text and is well-\nsuited for sentiment classification in informal language. The\nsentiment analysis enables us to understand the overall tone\nand emotional disposition of the posts and how it correlates\nwith the different depression severity levels.\nIV. M ETHODOLOGY\nA. Architecture\nOur research methodology (Figure 1) is based on a hybrid\nmodel that combines a transformer model, specifically the\nDistilbert-base-uncased model, with a Multilayer Perceptron\n(MLP). The Distilbert-base-uncased model is a pre-trained\ntransformer model that has been trained on a large corpus\nof English data in a self-supervised fashion. This model is\n7\nFig. 1. The proposed methodology\nuncased, meaning it does not differentiate between upper- and\nlower-case English letters.\nThe first step in our methodology involves tokenizing the\ninput text. Tokenization is the process of converting the input\ntext into a sequence of tokens or words. The tokenized text is\nthen passed to the transformer model.\nIn our architecture, we use the hidden states from the\nlast four layers of the transformer. These layers are chosen\nbecause deeper layers in the BERT model capture more\ncomplex semantic information [49]. We compute a weighted\naverage of these hidden states, with the weights learned during\ntraining. This allows the model to decide the importance of\nthe output from each layer. The output of this process is a\ntensor that represents the weighted average hidden states of\nthe transformer model. Specifically, we use the CLS tokens,\nwhich are the first tokens in each sequence and are used\nto aggregate the representation of the entire sequence. The\ntransformer accepts an input of 512 tokens and has a hidden\nsize of 768, meaning the dimension of the hidden states is\n768.\nThe averaged CLS token is then combined with additional\nfeatures and metadata to form the input for the MLP. The\nmetadata are the features extracted as described in Section\nIII-C. Before the input is passed to the MLP, it is subjected to\na dropout layer with a dropout rate of 0.1 for regularization.\nThis helps prevent overfitting by randomly setting a fraction\nof input units to 0 at each update during training time.\nIn our hybrid model, the classification head plays a crucial\nrole in transforming the rich contextualized representations\nobtained from the transformer layers into actionable predic-\ntions. Positioned at the end of the architecture, it consists\nof a simple feed-forward neural network, the Multi-Layer\nPerceptron (MLP). The MLP, serving as the classification head\nof the transformer, is composed of a linear layer, a ReLU\nactivation function, and another linear layer. The input to the\nMLP comprises the combined features, including the weighted\n8\naverage hidden states from the Distilbert-base-uncased model,\nalong with additional features and metadata.\nTailoring the versatile information encoded by the trans-\nformer, the classification head acts as the decision-making\nmodule, ensuring adaptability and effectiveness across diverse\nclassification scenarios. The hidden size of the MLP is set to\n512, aligning with the design choices made for the transformer\nlayers. The output dimension of the MLP is determined by the\nnumber of labels in the targeted task, ensuring the model’s\ncapacity to generate accurate predictions. In our case, the task\ninvolves four labels, reflecting the specific requirements of the\nclassification task at hand.\nThe output of the MLP is the final output of the hybrid\nmodel (see Figure 1), with model size of 66 million trainable\nparameters (66,774,536). This architecture allows us to lever-\nage the power of the transformer for representation learning,\nwhile also incorporating additional features and metadata and\na custom MLP for the specific task. The use of a weighted\naverage of the hidden states from the Distilbert-base-uncased\nmodel allows the model to effectively utilize the representa-\ntions learned at different layers of the transformer model. This,\ncombined with the ability to incorporate additional features\nand metadata and a custom MLP, makes this architecture\nflexible and powerful for a variety of tasks.\nThe weighted average of hidden states from the transformer\ncan be represented as follows:\nHavg =\n4X\ni=1\nαiHi\nHere, Havg represents the weighted average hidden states,\nHi represents the hidden states from each of the last four\nlayers, and αi represents the learned weights for each layer.\nThe input to the MLP can be represented as follows:\nMLP input = [CLS avg : additional features]\nWhere CLS avg is the averaged CLS token output from the\ntransformer, and additional features represent the additional\nfeatures and metadata.\nThe output of the MLP can be represented as follows:\nMLP output = MLP(MLP input)\nWhere MLP output represents the final output of the hybrid\nmodel, and MLP denotes the operations performed by the\nMultilayer Perceptron.\nV. E XPERIMENTS\nA. Baselines\nIn our study, we compare our proposed methodology with\nseveral baseline models.\n• The first baseline is the approach presented by Ilias et. al.\n[50]. This baseline model is a transformer-based approach\nthat integrates additional linguistic information into the\nBERT and MentalBERT models. The method begins by\nextracting a variety of linguistic features, including NRC\nSentiment Lexicon, features derived by Latent Dirichlet\nAllocation (LDA) topics, Top2vec, and Linguistic Inquiry\nand Word Count (LIWC) features. These features are then\nprojected to the same dimensionality as the outputs of the\ntransformer models. The authors concatenate the repre-\nsentations obtained by BERT (or MentalBERT) and the\nlinguistic information, applying a Multimodal Adaptation\nGate to control the importance of each representation.\nThe combined embeddings are then passed through a\nBERT (or MentalBERT) model, with the classification\n[CLS] token fed to Dense layers to produce the fi-\nnal prediction. To prevent the models from becoming\noverly confident, the authors employ label smoothing.\nThey tested their proposed approaches on two publicly\navailable datasets, which differentiate stressful from non-\nstressful texts, and posts indicating varying severity of\ndepression (minimal, mild, moderate, and severe), which\nis the one we use.\n• The second baseline model comes from the paper by Yang\net al. [51]. In this approach, the authors utilized BERT\nand MentalBERT for depression detection.\n• The third baseline involves the use of ALBERT with\nan LSTM layer. This baseline method has been used in\n[44]. ALBERT (A Lite BERT) is a more efficient version\nof BERT that reduces the model’s size and training\ntime while maintaining performance. In this baseline, the\nALBERT model is combined with an LSTM layer to\ncapture sequential dependencies within the text data. The\nfinal hidden state from the LSTM layer is used as the\nrepresentation for classification.\n• The fourth baseline is the ALBERT model [52] paired\nwith a Bidirectional LSTM (BiLSTM) layer. This base-\nline method has been used in [44]. This model allows\nthe processing of text data in both forward and backward\ndirections, capturing more contextual information than a\nstandard LSTM. The output representations from both\ndirections are concatenated and passed through dense\nlayers for the final classification.\n• Another baseline utilizes Linguistic Inquiry and Word\nCount (LIWC) [53] features combined with a Random\nForest (RF) classifier. LIWC is a widely used tool for\npsycholinguistic analysis, which provides insights into\nthe emotional, cognitive, and structural components of\nthe text. These features are then input into a Random\nForest classifier, which operates by constructing multiple\ndecision trees and outputting the mode of the classes\npredicted by individual trees.\n• Finally, we consider a baseline using MentalBERT [54]\nwith an LSTM layer. MentalBERT is a domain-specific\nversion of BERT, fine-tuned on mental health-related cor-\npora, making it particularly effective for tasks related to\nmental health. In this baseline, the MentalBERT model’s\noutputs are passed through an LSTM layer, and the final\nhidden state is used for the classification task.\nB. Experimental Setup\nFor the experimental setup, the neural network model was\nimplemented in a Python notebook on Google Colab and\nKaggle, utilizing prominent libraries such as Hugging Face’s\n9\nTransformers [55], PyTorch [56] for model construction and\nnlpaug [57] for text augmentation. The evaluation of the\nmodel’s performance was facilitated by leveraging the com-\nprehensive metrics available in the scikit-learn library [58]. To\naccelerate the training process, a T4 GPU on Google Collab\nand a P100 GPU on Kaggle were employed, optimizing the\ncomputational efficiency. The training phase was conducted\nwith a batch size of 8 and was iterated over 11 epochs, while\n80% of the dataset was utilized for training. We train and test\nour proposed approach five times and report the mean and\nstandard deviation over five runs. For the optimization process,\nthe Cross-Entropy Loss function was adopted, and the Adam\noptimizer with a learning rate of 1e-5 was utilized to fine-tune\nthe model’s parameters effectively. A list of hyperparameters\nis presented in Table II. This experimental setup allowed for\nthe thorough evaluation and classification of depressive posts,\nshowcasing the effectiveness of the proposed neural network\nin addressing the task at hand.\nTABLE II\nHYPERPARAMETER VALUES\nHyperparameter Value\nEpochs 11\nBatch size 8\nTrain size 0.8\nOptimizer Adam\nDistilBert dimension 768\nNumber of Features 31\nMLP layers 3 (Linear, ReLU, Linear)\nMLP hidden size 512\nNumber of layers to concatenate 4\nLearning rate of Adam optimizer 0.00001\nDropout rate 0.1\nC. Evaluation Metrics\nFor the evaluation of the proposed neural network, we em-\nployed precision, recall, and F1-score as our primary metrics,\nall of which were weighted to account for class imbalances\nin the dataset. These metrics are widely recognized and\ncommonly used in natural language processing tasks, includ-\ning sentiment analysis and text classification. The utilization\nof weighted metrics allowed us to appropriately assess the\nmodel’s performance in handling imbalanced classes, partic-\nularly in identifying depressive posts accurately. The choice\nof these evaluation metrics aligns with the approach used by\nIlias et al. [50] By adopting similar evaluation metrics, we\naimed to facilitate comparisons and ensure the reproducibility\nof results across studies in the field of depression detection in\nsocial media content.\nVI. R ESULTS\nIn the course of our experiments, we trained and eval-\nuated multiple configurations of our models. We observed\nthat the classification head MLP model with 512 hidden\nlayers and ’distilbert-base-uncased’ outperformed the other\nconfigurations. This model achieved a weighted precision of\n84.26%, a weighted recall of 84.18%, and a weighted F1-\nscore of 84.15%. The classification head MLP model with 512\nhidden layers using the ’distilbert-base-uncased’ achieved an\nimprovement of approximately 23.86% in the weighted F1-\nscore over the worst-performing baseline model and an im-\nprovement of approximately 10.99% over the best-performing\nbaseline model. A comprehensive comparison of our model’s\nperformance against the baseline models is presented in Table\nIII.\nIn comparison to the baseline models proposed by Ilias et.al.\n[50] and Yang et al. [51], our best-performing model yielded\ncompetitive results. Ilias’s best model, M-MentalBERT (LDA\ntopics), showed a weighted precision of 73.74%, a weighted\nrecall of 73.23%, and a weighted F1-score of 73.16%. Yang\net al.’s BERT model yielded a weighted precision of 72.99%,\na weighted recall of 71.97%, and a weighted F1-score of\n71.00%, while their MentalBERT model showed a weighted\nprecision of 73.35%, a weighted recall of 70.81%, and a\nweighted F1-score of 71.67%. In contrast, Ilias’s worst model,\nM-BERT (NRC), showed a weighted precision of 74.48%,\na weighted recall of 70.08%, and a weighted F1-score of\n69.96%. We further benchmarked our best-performing model\nagainst additional baseline models from other studies, as\ndetailed in Table III. These baselines include ALBERT-based\narchitectures combined with LSTM and BiLSTM layers, as\nwell as models leveraging LIWC features with random forest,\nand a MentalBERT model combined with LSTM. Importantly,\nwe ran these baseline models on our own dataset to ensure\na consistent and fair comparison. The results show that our\nmodel, the classification head MLP with 512 hidden layers\nusing ’distilbert-base-uncased,’ significantly outperforms these\nbaselines across all metrics. Specifically, our model achieved\na weighted precision of 84.26%, a weighted recall of 84.18%,\nand a weighted F1-score of 84.15%. In contrast, the best-\nperforming baseline in this group, ALBERT + BiLSTM,\nachieved only 66.83% in the weighted F1-score, indicat-\ning a substantial improvement of over 17.32% points with\nour approach. This further emphasizes the robustness and\neffectiveness of our model in detecting depression severity\ncompared to a broader range of existing methods.\nLastly Figure VI presents the confusion matrix for the\nclassification model used to detect depression severity, de-\ntailing the performance across four classes: Minimum, Mild,\nModerate, and Severe. The main diagonal cells indicate the\nmodel’s accuracy for each class, with 84% for Minimum, 63%\nfor Mild, 60% for Moderate, and 69% for Severe. The off-\ndiagonal cells show the misclassification rates: Minimum is\nmisclassified 5% as Mild, 6% as Moderate, and 5% as Severe;\nMild is misclassified 19% as Minimum, 7% as Moderate, and\n10% as Severe; Moderate is misclassified 26% as Minimum,\n6% as Mild, and 8% as Severe; Severe is misclassified\n17% as Minimum, 6% as Mild, and 8% as Moderate. This\nconfusion matrix highlights the model’s strengths and areas\nfor improvement in distinguishing between different severity\nlevels of depression.\nVII. A BLATION STUDY\nIn the process of model design and selection, we conducted\nan ablation study to ascertain the impact of the number of\n10\nTABLE III\nPERFORMANCE COMPARISON OF OUR BEST MODEL (AVERAGED OVER 5 RUNS AND CONTAINING THE STANDARD DEVIATION OF THE SCORES ) AND\nBASELINE MODELS . THE IMPROVEMENT OF OUR MODEL OVER THE WORST BASELINE MODEL IN TERMS OF THE WEIGHTED F1- SCORE IS\nAPPROXIMATELY 14.19%, AND OVER THE BEST BASELINE MODEL IS APPROXIMATELY 10.99%.\nModel Weighted Precision (%) Weighted Recall (%) Weighted F1-score (%)\nM-MentalBERT - LDA topics [50] 73.74% 73.23% 73.16%\nM-BERT - NRC [50] 74.48% 70.08% 69.96%\nBERT [51] 72.99% 71.97% 71.00%\nMentalBERT [51] 73.35% 70.81% 71.67%\nALBERT + LSTM 62.87% ± 4.83% 73.93% ± 1.22% 65.37% ± 2.21%\nALBERT + BiLSTM 64.83% ± 1.42% 72.53% ± 0.67% 66.83% ± 0.55%\nLIWC + Random forest 53.50% 71.70% 60.29%\nMentalBERT+ LSTM 62.85% ± 10.06% 72.87% ± 0.28% 66.20% ± 0.48%\nOur: Classification Head MLP\n(512 hidden layers, distilbert-base-uncased) 84.26% ± 0.48 % 84.18% ± 1.26% 84.15% ± 0.09%\nMinimum\nMild\nModerate\nSevere\nMinimum\nMild\nModerate\nSevere\n0.84 0.05 0.06 0.05\n0.19 0.63 0.07 0.1\n0.26 0.06 0.6 0.08\n0.17 0.06 0.08 0.69\nPredicted\nActual\n0.2\n0.4\n0.6\n0.8\nFig. 2. Confusion matrix of the best performing model on multi-class depression classification\nTABLE IV\nABLATION STUDY FOR DIFFERENT LAYERS WITH LSTM CLASSIFICATION HEAD AND BERT -BASE -UNCASED TRANSFORMER .\nClassification Head and Hidden Size number of transformer layers in use Weighted Precision (%) Weighted Recall (%) Weighted Fl-score (%)\nLSTM 512 1 83.65% 83.70% 83.66%\nLSTM 512 2 83.21% 83.35% 82.88%\nLSTM 512 3 83.22% 83.47% 83.16%\nLSTM 512 4 84.01% 82.43% 82.86%\nLSTM 512 5 82.52% 82.89% 82.58%\nLSTM 512 6 84.09% 84.27% 84.13%\ntransformer layers, the type of transformer models, and the\nclassification heads used in the architecture.\nA. Transformer Layers\nWe explored a range of 1 to 6 transformer layers for our\narchitecture with both MLP and LSTM classification layers.\nFor the MLP classification head results showed that the model\nachieved its peak performance with four transformer layers,\nachieving a weighted F1-score of 84.15%. Both a decrease or\nan increase in the number of layers from this point resulted\nin a decrement in the F1-score, highlighting the importance of\nthe right balance in model depth (see Tables IV and V).\nB. Choice of Transformer Models\nWe compared the performance of two transformer models,\nBERT-base-uncased and DistilBERT-base-uncased. Interest-\ningly, the study revealed no significant difference in perfor-\nmance between the two. Both models offered comparable\nweighted F1-scores, suggesting the choice of transformer\nmodel has little impact on the overall performance (see Table\nVI).\nNevertheless, given there was no noticeable difference in\nperformance, we opted for DistilBERT-base-uncased as the\ntransformer model in our architecture. This selection is driven\nby DistilBERT’s characteristics of being lighter and faster than\n11\nTABLE V\nABLATION STUDY FOR DIFFERENT LAYERS WITH MLP CLASSIFICATION HEAD AND DISTILBERT -BASE -UNCASED TRANSFORMER .\nClassification Head and Hidden Size number of transfromer layers in use Weighted Precission (%) Weighted Recall (%) Weighted Fl-score (%)\nMLP 512 1 81.73% 82.09% 81.52%\nMLP 512 2 83.38% 82.43% 82.71%\nMLP 512 3 81.86% 81.86% 81.52%\nMLP 512 4 84.26% 84.18% 84.15%\nMLP 512 5 82.46% 81.86% 81.49%\nMLP 512 6 82.90% 82.89% 82.80%\nBERT, offering reduced computational costs, faster training\nand prediction times, and lower memory requirements. Such\nfeatures make it a more feasible choice, especially for deploy-\nment in resource-constrained environments.\nTABLE VI\nABLATION STUDY FOR DIFFERENT TRANSFORMERS .\nClassification Head\nand Hidden Size models W. Precision (%) W. Recall (%) W. Fl-score (%)\nMLP 512 bert-base-uncased 84.37% 84.27% 84.30%\nMLP 512 distilbert-base-uncased 84.26% 84.18% 84.15%\nLSTM 512 bert-base-uncased 84.01% 82.43% 82.86%\nLSTM 512 distilbert-base-uncased 76.92% 75.38% 74.88%\nC. Classification Heads\nThe classification head, a crucial component in the model,\nplays a pivotal role in processing and interpreting the features\nextracted by the preceding layers.\nWe investigated the performance of five different types of\nclassification heads: MLP, LSTM, MM-Gate and MM-Xatt.\nThe gate mechanisms are used in multiple state-of-the-art\npapers [59], [60] and show promising results.\nOur results showed that the MLP and LSTM classification\nheads delivered the highest performance, with MLP being\nthe superior choice due to its simplicity and efficiency. The\ngating mechanisms, while novel, didn’t contribute to a superior\nperformance in our task (see Table VII-C).\nTABLE VII\nABLATION STUDY FOR DIFFERENT CLASSIFICATION HEADS\nClassification Head\nand Hidden Size Weighted Precision (%) Weighted Recall (%) Weighted Fl-score (%)\nMLP 512 84.26% 84.18% 84.15%\nLSTM 512 76.92% 75.38% 74.88%\nMM-Gate 512 80.43% 80.14% 79.88%\nMM-Xatt 512 51.00% 61.65% 54.71%\nD. Augmentation\nTable VIII presents the difference in weighted scores for\nvarious machine learning models before and after applying\ndata augmentation techniques. When considering the LSTM\nmodel with one layer and BERT (uncased), the Precision\nincreased from 71.22% to 83.65%, Recall from 73.41% to\n83.70%, and F1-score from 71.96% to 83.66%. In a similar\nLSTM configuration but with 6 layers, the F1-score showed a\nsignificant increase from 72.19% to 84.13%. The difference in\nthe F1-score ranged between 8% to nearly 12% for different\nLSTM configurations with BERT. However, for LSTM with\n4 layers using DistilBERT, the change in the F1-score was\nminimal at just 1.13%. In contrast, using an MLP architecture,\nimprovements were also noted but to varying degrees. For\ninstance, with 4 layers and BERT, the F1-score went up from\n73.16% to 84.30%. Interestingly, when using DistilBERT and\n6 layers, the F1-score improved from 74.22% to 82.80%. The\nMM-Gate model with 4 layers and DistilBERT also saw an\nincrease in the F1-score from 71.95% to 79.88%. However,\nthe MM-Xatt configuration with 4 layers and DistilBERT no-\ntably decreased in it’s performance metrics, with the F1-score\ndropping dramatically from 63.54% to just 3.13%. Overall, the\ndata shows that while data augmentation generally improved\nthe performance of LSTM and MLP architectures, the benefits\nwere not universally observed across all configurations and\nmodels.\nVIII. Q UALITATIVE AND ERROR ANALYSIS\nIn depression detection on social media, mismatches in pre-\ndictions can arise due to several factors, including imbalanced\ndata where certain labels are underrepresented. For example,\nthe minimum label might have significantly more posts than\nothers, leading to an over-representation of less severe cases in\nthe dataset. This imbalance can cause models to lean toward\npredicting posts as minimum or moderate even when they\ndescribe more severe symptoms. Another common issue is the\nsimilarity between posts assigned to different labels, leading\nto false predictions. Below are examples that highlight this\nphenomenon:\nExample 1: Actual Label: Severe, Predicted Label: Mod-\nerate\nIn the first post, which is actually labeled as moderate,\nthe individual discusses continuous panic attacks, intrusive\nthoughts, and constant fear:\ni have panic attack after panic attack and... i truly just\ndon´t know what to do anymore. no ono matter how i explain\nthe severity of my situation to people, but... still somehow they\nseem to not understand just how miserable my mind makes me\neveryday. i get intrusive thoughts like about how something\nterrible could happen to my family, or a lot of ” what...\nifs ”. my mind is running 24 / 7 and it ´s driving everyone\ninsane. i have a terrible fear of danger along with an even\nmore crippling risk of had something happening happened\nto dear friends ( my parents, brothers.. ) all day and my\nmind continually plays over thoughts and scenarios that leave\nme sad, scared, constantly wired with fear, and never... all\nexhausted.\nThis post shares many similarities with the one below, which\nis actually labeled as severe but was mismatched as moderate.\nBoth posts describe intense anxiety and intrusive thoughts.\nHowever, the second post goes deeper into feelings of isolation\nand overwhelming distress, which makes the misclassification\nnotable:\n12\nTABLE VIII\nDIFFERENCE OF SCORES BEFORE AND AFTER AUGMENTATION\nBefore Augmentation\n(Weighted Scores)\nAfter Augmentation\n(Weighted Scores)\nDifference\nclassification\nhead\nNumber\nof trans.\nlayers\nin use\nmodels\n(uncased)\nPrec. Rec. Fl-score Prec. Rec. Fl-score Prec. Rec. Fl-score\nLSTM 1 bert 71.22% 73.41% 71.96% 83.65% 83.70% 83.66% 12.42% 10.29% 11.69%\nLSTM 2 bert 74.38% 74.54% 73.88% 83.21% 83.35% 82.88% 8.83% 8.81% 9.00%\nLSTM 3 bert 74.82% 74.26% 74.44% 83.22% 83.47% 83.16% 8.40% 9.21% 8.72%\nLSTM 4 bert 74.67% 72.98% 73.74% 84.01% 82.43% 82.86% 9.33% 9.45% 9.12%\nLSTM 5 bert 72.40% 75.39% 72.86% 82.52% 82.89% 82.58% 10.12% 7.50% 9.72%\nLSTM 6 bert 72.86% 71.00% 72.19% 84.09% 84.27% 84.13% 11.23% 13.27% 11.94%\nLSTM 4 distilbert 73.56% 74.68% 73.75% 76.92% 75.38% 74.88% 3.36% 0.70% 1.13%\nMLP 4 bert 72.52% 74.12% 73.16% 84.37% 84.27% 84.30% 11.84% 10.15% 11.14%\nMLP 1 distilbert 73.86% 75.81% 74.64% 81.73% 82.09% 81.52% 7.87% 6.28% 6.88%\nMLP 2 distilbert 72.24% 67.47% 69.30% 83.38% 82.43% 82.71% 11.14% 14.97% 13.42%\nMLP 3 distilbert 75.69% 72.28% 72.73% 81.86% 81.86% 81.52% 6.17% 9.58% 8.79%\nMLP 4 distilbert 71.99% 73.41% 72.59% 84.26% 84.18% 84.15% 12.27% 10.77% 11.56%\nMLP 5 distilbert 71.67% 74.82% 72.78% 82.46% 81.86% 81.49% 10.79% 7.04% 8.71%\nMLP 6 distilbert 73.88% 74.68% 74.22% 82.90% 82.89% 82.80% 9.02% 8.21% 8.58%\nMM-Gate 4 distilbert 73.25% 71.57% 71.95% 80.43% 80.14% 79.88% 7.18% 8.57% 7.93%\nMM-Xatt 4 distilbert 58.16% 72.98% 63.54% 1.77% 13.32% 3.13% -56.39% -59.66% -60.41%\ni am slightly introverted and however the broken friendship\ni had since my previous roommate made me feel very anxious,\nso i settled into an apartment on my own. my am living\nalone, yet when i have severe anxiety attacks it becomes lonely\nand seems almost unbearable. i sometimes do think so much\nthat it feels like i am going completely crazy. i always have\nterrifying thoughts and often i make up scenarios in which i\nhave undergone some terrible condition, or when i am dying.\nher parents know instinctively that a have anxiety, yes but\nnever took it more seriously.\nExample 2: Actual Label: Minimum, Predicted Label:\nModerate\nIn the first post, which is actually labeled as moderate,\nthe individual goes deeper into trauma-related anxiety and its\nimpact on intimate relationships:\nI don’t remember a lot of it I just remember little snip\nbits of what happened. I don’t even remember if there was\npenetration and I’m hoping there wasn’t. I still have problems\ntrying to remember what happened and I feel like if I remember\nit all I’m going to have a breakdown. Now that I’m older I’m\nstarting to have sex and be intimate with others. I’m noticing\na pattern where my body is like rejecting my partner and I’m\nconcerned it might be caused by my abuse at an early age.\nThis post expresses ongoing anxiety and emotional turmoil,\nwhich makes it similar to the post below. However, the\nfollowing post, labeled as minimum but predicted as moderate,\ndiscusses the same trauma but in a less intense way, showing\nthe mismatch:\nI’m noticing a pattern where my body is like rejecting my\npartner and I’m concerned it might be caused by my abuse at\nan early age. Should I seek counseling? But I’m afraid if I do\nI’m going to have to talk more about what happened and I’m\ngoing to break. I’ve talked to therapist before but whenever\nthe topic of the abuse arises I tense up and can’t remember\nanything. I’m sorry if I’m rambling on it’s just a hard subject\nfor me to talk about and I don’t know how to put into words\nthe emotions I feel towards these events.\nExample 3: Actual Label: Mild, Predicted Label: Moderate\nIn the first post, which is actually labeled as moderate,\nthe individual describes persistent anxiety and how it disrupts\ndaily life:\nAs of Sunday i have experience sever anxiety. Its too the\npoint i am unable to do normal everyday functions and doing\nmy job is harder. All i can think about is how i was ripped\naway from my daughter and how she is without me. Its to the\npoint my therapist uas suggested i used cbd oil to manage my\nanxiety instead of narcotics. They help take the edge off and\nlet me relax to the point i can sleep.\nThis post is very similar to the one below, where the\nperson describes episodic spikes of anxiety but has general\ncontrol over it. The model likely predicted the following post\nas moderate due to these similarities, despite its actual mild\nclassification:\nNormally, my anxiety is very well controlled. I meditate\nevery morning for ˜15 minutes and have been in therapy for\nthe better part of the last 3 or 4 years. I feel WAY better than I\nused to, and on a day to day basis things are great. Buttt every\nonce in a while (A handful of times a year, tops) something\nwill realllly set me over the edge, and send me into an intense\nanxiety spiral where I compulsively ask 5 or so different friends\nfor advice on what to do, post a lot of threads online about\nwhat I should do, and ruminate on the topic for days or weeks.\nSometimes I’ll have chats in messenger about whatever it is\nthat will draw out over an entire 3-4 hour period.\nExample 4: Actual Label: Moderate, Predicted Label: Se-\nvere\nIn the first post, which is labeled as severe, the person\ndescribes ongoing abusive experiences and the emotional toll\nthey have taken:\nhe still forcefully holds me back in life. although he repeat-\nedly still finds ways rann has finally fit inside me. eventually\ni will talk with him. and i always feel very soon after i do.\ni’and ve tried blocking him in our social media, but he still\nfinds ways to even get use to me.\n13\nThis post shares many of the same abusive themes as the one\nbelow, which was labeled as moderate but was mismatched as\nsevere due to the intensity of the described events, even though\nthe emotional toll was less pronounced:\nhe has made most me almost die until i pretty literally threw\nsomething up, nearly forced me to eat my vomit. he plays\ncomputer games that are pure torture. so he knows how to\nbreak me down mentally until eventually i somehow just might\nbecome ruined over a period of their time. he also point guns\nat me. he made me play russian orthodox roulette ( turns out\nthe gun wasn’t loaded but he used a hand switch to make you\nappear loaded.\nIX. D ISCUSSION\nA. Limitations\nHowever, this study comes with some limitations. Specifi-\ncally, we did not perform hyperparameter tuning due to limited\naccess to GPU resources. It is known that hyperparameter\ntuning is a procedure yielding to higher evaluation results.\nAdditionally, this study requires access to labelled datasets.\nAccess to labelled datasets is often a difficult task. For\nthis reason, methods including self-supervised learning have\nbeen developed aiming to address the problem of labels’\nscarcity. Additionally, we tested our approach only on one\ndataset. Finally, this study did not include an explainable ap-\nproach, which will explain why this transformer-based network\nreached a specific decision.\nB. Findings\nFrom the results of this study, we found that:\n• Finding 1: We found that the data augmentation method\nincreased the evaluation performance of the proposed\nmodel. Specifically, Weighted Precision, Recall, and F1-\nscore presented a surge of 12.26%, 10.75%, and 11.52%\nrespectively.\n• Finding 2: Results showed that the introduced approach\noutperformed competitive baselines in weighted Precision\nby 9.78-11.27%, weighted Recall by 10.95-14.10%, and\nweighted F1-score by 10.99-14.19%.\n• Finding 3: Findings from a series of ablation studies\nshowed the effectiveness of the introduced architectures.\nC. Generalization to other social media platforms\nOur introduced approach can be easily adapted to other\nsocial media platforms. This can be justified by the fact that\nour proposed approach uses only posts on Reddit. Thus, one\ncan use very easily posts on other social media platforms,\nincluding X (Twitter), Meta (Facebook), etc., and employ our\nintroduced approach.\nHowever, it must be noted that the access of data to other\nsocial media platforms is a challenging task. For instance,\nFacebook does not easily share data with researchers [61].\nAdditionally, Twitter has decided to end free access to their\nAPI. Specifically, the paid plans provide very limited data. For\ninstance, social media researchers [62] were obliged to cancel,\nsuspend, or modify more than 100 studies about Twitter.\nD. Challenges of deploying the model in real-world settings\nDeploying our introduced model in real-world settings en-\ntails significant challenges. Although we have used Distil-\nBERT, which is the distilled version of BERT and appears\nto be smaller, faster, cheaper, and lighter than BERT, its de-\nployment still demands a lot of computational resources. Our\napproach has been developed for mental health monitoring.\nThus, it needs to respond quickly in real-time applications\nprocessing at the same time huge amount of data. However,\nlatency issues may arise. At the same time, maintaining the\ninfrastructure, such as GPUs, cloud servers, etc., required for\ntraining and storing the AI models is a costly procedure.\nAnother significant challenge arisen by the deployment in a\nreal-world framework is the loss of accuracy due to different\ndata. Specifically, our proposed model needs to be trained\ncontinuously so as to adapt to different data. Additionally,\nensuring that the proposed approach performs equally well\nacross different demographics or languages can be challeng-\ning. Deploying the model without thorough fairness testing\ncould lead to biased outcomes. Finally, the deployment in real-\nworld frameworks entails security issues. Specifically, machine\nlearning models are vulnerable to adversarial attacks. Thus,\nrobust AI models must be designed.\nX. C ONCLUSION\nIn this study, we addressed the critical task of early identifi-\ncation of depression severity levels in Reddit posts, leveraging\na comprehensive approach that combines natural language\nprocessing techniques, deep learning, and data augmentation.\nOur research explored the limitations commonly observed in\nexisting methodologies, such as overfitting, small dataset sizes,\nannotator biases, and the lack of age and gender awareness,\nand proposed a novel model to overcome some of these\nchallenges.\nThe proposed model offers several advancements over the\nmethodologies reported in the literature. By leveraging the\npower of BERT, a widely used transformer model, we ex-\ntracted detailed linguistic patterns from the input text. This\ntransformer-based approach overcame the limitations of tradi-\ntional machine learning algorithms, offering a more sophis-\nticated method for interpreting the complexities of natural\nlanguage. Additionally, our model did not solely rely on\ntransformer-based contextual embeddings but also incorpo-\nrated auxiliary features, including metadata and linguistic\nmarkers. This strategic fusion enhanced the model’s ability to\ncomprehend each post holistically, capturing crucial nuances\nthat might otherwise be overlooked. Consequently, our model\nenabled a more accurate identification of potential signs of\ndepression.\nThrough extensive experiments, we demonstrated that our\nproposed model achieved competitive results compared to\nstate-of-the-art baseline models. In particular, our model\noutperformed the best-performing baseline by approximately\n10.94% in terms of weighted F1-score. These results highlight\nthe effectiveness of our approach in handling the nuanced\ntask of depression severity level classification in social media\ncontent.\n14\nIn our ablation study, we examined the impact of various\nmodel configurations, including the number of transformer\nlayers, the choice of transformer models, classification heads\nand the use of data augmentation. These experiments allowed\nus to fine-tune our architecture and make informed decisions,\nultimately leading to the selection of a model configuration\nthat delivered the best performance.\nFurthermore, we applied data augmentation techniques to\naddress the challenges posed by class imbalance and dataset\nsize. While data augmentation generally improved the perfor-\nmance of LSTM and MLP architectures, the benefits were\nnot universally observed across all configurations and models.\nNevertheless, our research contributes to the growing body of\nknowledge in the field of depression detection in social media\ncontent by offering a robust and comprehensive model that\novercomes limitations and delivers more accurate results.\nIn conclusion, our study provides valuable insights into\nthe early identification of depression severity levels in social\nmedia posts, with the proposed model showcasing significant\nadvancements in performance. As the field continues to evolve,\nthe contributions made here serve as a foundation for further\nresearch in the critical area of mental health awareness and\nearly intervention through natural language processing and\ndeep learning techniques. Moreover, the potential impact of\nour study extends to mental health policy makers. By utilizing\nour model for fast monitoring of social media posts, it is\npossible to gauge the mental health of users in real-time,\nthereby enabling more timely and targeted interventions. This\ncapability could transform how mental health crises are iden-\ntified and managed, leading to better outcomes for individuals\nand communities. Finally, this study could be used as a tool\nfor epidemiological research. Analyzing social media data for\nidentifying severity of depression could assist public health\nauthorities identify regions, age groups, or demographics\nparticularly affected by depression. Epidemiological research\nconstitutes a vital tool for improving public health outcomes,\ndeveloping preventive strategies, and guiding healthcare pol-\nicy.\nXI. F UTURE DIRECTIONS\nDelving into the interaction between linguistic cues and hu-\nman emotions, this paper has paved the way for the detection\nof signs of depression in social media posts. While significant\nprogress has been made, this work opens avenues for further\nexploration:\n1) Using Different Datasets: Social media, with its vast\nglobal reach, provides a rich source of data. Utilizing\nmore extensive and diverse datasets can enhance the\nmodel’s universality and sensitivity to various sociolin-\nguistic nuances. For instance, we aim to use the Mental\nHealth Blog Dataset, which includes posts written in\nEnglish [63]. The publicly available datasets presented\nin [64], [65] can be exploited. Datasets collected through\nthe Covid-19 pandemic can also be used [36], [66]. Our\napproaches could also be evaluated in datasets of the\neRisk depression severity estimation tasks [67]–[69].\n2) Merging Gate Mechanisms with Neural Classification\nNetworks: Combining gate functions with neural clas-\nsification networks such as LSTM or CNN can capture\nricher linguistic insights. This hybrid approach could\nmore effectively uncover complex patterns indicative of\ndepressive tendencies.\n3) Employing Larger and More Complex Transformers:\nLeveraging state-of-the-art models like Chat GPT and\nLlama 2 could boost our model’s accuracy. These trans-\nformers, with their advanced text processing capabilities,\nmay reveal subtle signs of depression often overlooked.\n4) Multi-dimensional Feature Analysis: Beyond the main\ntext, features like posting timestamps can offer addi-\ntional insights. Tools like LIWC or POS tagging can\nfurther dissect posts, unraveling the embedded emotional\nand syntactic layers.\n5) User History Integration: Incorporating a user’s post-\ning history could provide information about the evolving\nmental state. This approach could be invaluable in\ndistinguishing the cyclic or episodic nature of depressive\ntendencies, potentially aiding in early interventions.\n6) Expanding to Multimodalities: As multimedia con-\ntent becomes integral to social media posts, our model\ncould benefit from deciphering emotions embedded in\nimages, videos, or links. Computer vision or content-\nbased emotion analysis techniques can be applied to\nthese expressions.\n7) Broadening the Scope of Mental Health: While our\nprimary focus was depression, the framework could\nbe adapted to detect other mental health conditions\nexpressed on social media, from anxiety to PTSD or\neven anorexia.\n8) Explainability: To increase model explainability and\nunderstand its decisions better, we can explore the use\nof explainable AI methods [70] [71]. These methods\ncan provide additional insights into how the model un-\nderstands and categorizes emotions in posts. Integrating\nthese methods into our model can aid researchers and\nmental health professionals in better comprehending the\nresults and predictions related to depression and other\nmental health conditions on social media.\nIn this interdisciplinary landscape of linguistics, technol-\nogy, and mental well-being, the current research holds great\npromise. The horizon ahead is vast, inviting interdisciplinary\ncollaboration and ongoing exploration, striving for a compre-\nhensive understanding and support of the digital reflections of\nthe human psyche.\nREFERENCES\n[1] World Health Organization. Mental disorders.\nhttps://www.who.int/news-room/fact-sheets/detail/mental-disorders,\nJune 2023.\n[2] World Health Organization. Depressive disorder (depression).\nhttps://www.who.int/news-room/fact-sheets/detail/depression, June\n2023.\n[3] Johannes C. Eichstaedt, Robert J. Smith, Raina M. Merchant, Lyle H.\nUngar, Patrick Crutchley, Daniel Preot ¸iuc-Pietro, David A. Asch, and\nH. Andrew Schwartz. Facebook language predicts depression in\nmedical records. Proceedings of the National Academy of Sciences ,\n115(44):11203–11208, October 2018.\n15\n[4] Yan Huang, Xiaoqian Liu, and Tingshao Zhu. Suicidal ideation detection\nvia social media analytics. In Danijela Milo ˇsevi´c, Yong Tang, and\nQiaohong Zu, editors, Human Centered Computing , Lecture Notes in\nComputer Science, pages 166–174, Cham, 2019. Springer International\nPublishing.\n[5] Victor Leiva and Ana Freire. Towards suicide prevention: Early detection\nof depression on social media. In Ioannis Kompatsiaris, Jonathan Cave,\nAnna Satsiou, Georg Carle, Antonella Passani, Efstratios Kontopoulos,\nSotiris Diplaris, and Donald McMillan, editors, Internet Science, Lecture\nNotes in Computer Science, pages 428–436, Cham, 2017. Springer\nInternational Publishing.\n[6] Maryam Mohammed Aldarwish and Hafiz Farooq Ahmad. Predicting\ndepression levels using social media posts. In 2017 IEEE 13th Interna-\ntional Symposium on Autonomous Decentralized System (ISADS) , pages\n277–280, March 2017.\n[7] Md. Rafiqul Islam, Muhammad Ashad Kabir, Ashir Ahmed, Abu Rai-\nhan M. Kamal, Hua Wang, and Anwaar Ulhaq. Depression detection\nfrom social network data using machine learning techniques. Health\nInformation Science and Systems , 6(1):8, August 2018.\n[8] Ana-Sabina Uban, Berta Chulvi, and Paolo Rosso. An emotion and\ncognitive based analysis of mental health disorders from social media\ndata. Future Generation Computer Systems , 124:480–494, November\n2021.\n[9] Iqra Ameer, Muhammad Arif, Grigori Sidorov, Helena G `omez-Adorno,\nand Alexander Gelbukh. Mental illness classification on social media\ntexts using deep learning and transfer learning, July 2022.\n[10] Sangeeta R. Kamite and V . B. Kamble. Detection of depression in\nsocial media via twitter using machine learning approach. In 2020\nInternational Conference on Smart Innovations in Design, Environment,\nManagement, Planning and Computing (ICSIDEMPC) , pages 122–125,\nOctober 2020.\n[11] Fidel Cacheda, Diego Fernandez, Francisco J. Novoa, and Victor\nCarneiro. Early detection of depression: Social network analysis and\nrandom forest techniques. Journal of Medical Internet Research ,\n21(6):e12554, June 2019.\n[12] Raymond Chiong, Gregorius Satia Budhi, Sandeep Dhakal, and Fabian\nChiong. A textual-based featuring approach for depression detection\nusing machine learning classifiers and social media texts. Computers in\nBiology and Medicine , 135:104499, August 2021.\n[13] Ramin Safa, Peyman Bayat, and Leila Moghtader. Automatic detection\nof depression symptoms in twitter using multimodal analysis. The\nJournal of Supercomputing , 78(4):4709–4744, March 2022.\n[14] Faisal Muhammad Shah, Farzad Ahmed, Sajib Kumar Saha Joy, Sifat\nAhmed, Samir Sadek, Rimon Shil, and Md. Hasanul Kabir. Early\ndepression detection from social network using deep learning techniques.\nIn 2020 IEEE Region 10 Symposium (TENSYMP) , pages 823–826, June\n2020.\n[15] Min Yen Wu, Chih-Ya Shen, En Tzu Wang, and Arbee L. P. Chen. A\ndeep architecture for depression detection using posting, behavior, and\nliving environment data. Journal of Intelligent Information Systems ,\n54(2):225–244, April 2020.\n[16] Qing Cong, Zhiyong Feng, Fang Li, Yang Xiang, Guozheng Rao, and\nCui Tao. X-a-BiLSTM: A deep learning approach for depression\ndetection in imbalanced data. In 2018 IEEE International Conference on\nBioinformatics and Biomedicine (BIBM) , pages 1624–1627, December\n2018.\n[17] Hannah Yao, Sina Rashidian, Xinyu Dong, Hongyi Duanmu, Richard N.\nRosenthal, and Fusheng Wang. Detection of suicidality among opioid\nusers on reddit: Machine Learning–Based approach. Journal of Medical\nInternet Research, 22(11):e15293, November 2020.\n[18] Marcel Trotzek, Sven Koitka, and Christoph M. Friedrich. Utilizing\nneural networks and linguistic metadata for early detection of depression\nindications in text sequences. IEEE Transactions on Knowledge and\nData Engineering, 32(3):588–601, March 2020.\n[19] Andrew Yates, Arman Cohan, and Nazli Goharian. Depression and self-\nharm risk assessment in online forums. In Proceedings of the 2017\nConference on Empirical Methods in Natural Language Processing ,\npages 2968–2978, Copenhagen, Denmark, September 2017. Association\nfor Computational Linguistics.\n[20] George Gkotsis, Anika Oellrich, Sumithra Velupillai, Maria Liakata, Tim\nJ. P. Hubbard, Richard J. B. Dobson, and Rina Dutta. Characterisation of\nmental health conditions in social media using Informed Deep Learning.\nScientific Reports, 7(1):45141, March 2017.\n[21] Ziyi Chen, Ren Yang, Sunyang Fu, Nansu Zong, Hongfang Liu, and\nMing Huang. Detecting reddit users with depression using a hybrid\nneural network, February 2023.\n[22] Chenhao Lin, Pengwei Hu, Hui Su, Shaochun Li, Jing Mei, Jie Zhou,\nand Henry Leung. SenseMood: Depression detection on social media.\nIn Proceedings of the 2020 International Conference on Multimedia\nRetrieval, ICMR ’20, pages 407–411, New York, NY , USA, June 2020.\nAssociation for Computing Machinery.\n[23] Guozheng Rao, Yue Zhang, Li Zhang, Qing Cong, and Zhiyong Feng.\nMGL-CNN: A hierarchical posts representations model for identifying\ndepressed individuals in online forums. IEEE Access , 8:32395–32403,\n2020.\n[24] Mohsinul Kabir, Tasnim Ahmed, Md. Bakhtiar Hasan, Md Tahmid Rah-\nman Laskar, Tarun Kumar Joarder, Hasan Mahmud, and Kamrul Hasan.\nDEPTWEET: A typology for social media texts to detect depression\nseverities. Computers in Human Behavior , 139:107503, February 2023.\n[25] Ankit Murarka, Balaji Radhakrishnan, and Sushma Ravichandran. De-\ntection and Classification of mental illnesses on social media using\nRoBERTa, November 2020.\n[26] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBERT: Pre-training of deep bidirectional transformers for language\nunderstanding. CoRR, abs/1810.04805, 2018.\n[27] Wikimedia Foundation. Wikimedia downloads.\nhttps://dumps.wikimedia.org, January 2022.\n[28] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel\nUrtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies:\nTowards story-like visual explanations by watching movies and reading\nbooks. In The IEEE International Conference on Computer Vision\n(ICCV), December 2015.\n[29] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.\nDistilBERT, a distilled version of BERT: Smaller, faster, cheaper and\nlighter. ArXiv, abs/1910.01108, 2019.\n[30] Mario Ezra Arag ´on, Adrian Pastor L ´opez-Monroy, Luis Carlos\nGonz´alez-Gurrola, and Manuel Montes-y-G ´omez. Detecting mental\ndisorders in social media through emotional patterns - the case of\nanorexia and depression. IEEE Transactions on Affective Computing ,\n14(1):211–222, January 2023.\n[31] Sharath Chandra Guntuku, Anneke Buffone, Kokil Jaidka, Johannes\nEichstaedt, and Lyle Ungar. Understanding and measuring psychological\nstress using social media, April 2019.\n[32] Glen Coppersmith, Mark Dredze, and Craig Harman. Quantifying\nmental health signals in twitter. In Proceedings of the Workshop on\nComputational Linguistics and Clinical Psychology: From Linguistic\nSignal to Clinical Reality , pages 51–60, Baltimore, Maryland, USA,\nJune 2014. Association for Computational Linguistics.\n[33] Michael M. Tadesse, Hongfei Lin, Bo Xu, and Liang Yang. Detection\nof depression-related posts in reddit social media forum. IEEE Access,\n7:44883–44893, 2019.\n[34] Nan Chen and Peikang Wang. Advanced combined LSTM-CNN\nmodel for twitter sentiment analysis. In 2018 5th IEEE International\nConference on Cloud Computing and Intelligence Systems (CCIS), pages\n684–687, November 2018.\n[35] Pedro M. Sosa. Twitter sentiment analysis using combined LSTM-CNN\nmodels.\n[36] Ashutosh Anshul, Gumpili Sai Pranav, Mohammad Zia Ur Rehman, and\nNagendra Kumar. A multimodal framework for depression detection\nduring covid-19 via harvesting social media. IEEE Transactions on\nComputational Social Systems , 11(2):2872–2888, 2024.\n[37] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi\nChen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\nRoBERTa: A robustly optimized BERT pretraining approach. CoRR,\nabs/1907.11692, 2019.\n[38] Xiaofeng Wang, Shuai Chen, Tao Li, Wanting Li, Yejie Zhou, Jie Zheng,\nQingcai Chen, Jun Yan, and Buzhou Tang. Depression risk prediction for\nchinese microblogs via deep-learning methods: Content analysis. JMIR\nMedical Informatics, 8(7):e17958, July 2020.\n[39] Keshu Malviya, Bholanath Roy, and SK Saritha. A transformers\napproach to detect depression in social media. In 2021 International\nConference on Artificial Intelligence and Smart Systems (ICAIS) , pages\n718–723, March 2021.\n[40] Loukas Ilias and Dimitris Askounis. Multitask learning for recognizing\nstress and depression in social media. Online Social Networks and\nMedia, 37-38:100270, 2023.\n[41] Farsheed Haque, Ragib Un Nur, Shaeekh Al Jahan, Zarar Mahmud, and\nFaisal Muhammad Shah. A transformer based approach to detect suicidal\nideation using pre-trained language models. In 2020 23rd International\nConference on Computer and Information Technology (ICCIT) , pages\n1–5, December 2020.\n[42] Pahalage Dona Thushari, Nitisha Aggarwal, Vajratiya Vajrobol,\nGeetika Jain Saxena, Sanjeev Singh, and Amit Pundir. Identifying\n16\ndiscernible indications of psychological well-being using ml: explainable\nai in reddit social media interactions. Social Network Analysis and\nMining, 13(1):141, 2023.\n[43] Vajratiya Vajrobol, Nitisha Aggarwal, Unmesh Shukla, Geetika Jain\nSaxena, Sanjeev Singh, and Amit Pundir. Explainable cross-lingual\ndepression identification based on multi-head attention networks in thai\ncontext. International Journal of Information Technology , pages 1–16,\n2023.\n[44] Usman Naseem, Adam G. Dunn, Jinman Kim, and Matloob Khushi.\nEarly identification of depression severity levels on reddit using ordinal\nclassification. In Proceedings of the ACM Web Conference 2022, WWW\n’22, pages 2563–2572, New York, NY , USA, April 2022. Association\nfor Computing Machinery.\n[45] Jason Wei and Kai Zou. EDA: Easy data augmentation techniques for\nboosting performance on text classification tasks. In Proceedings of the\n2019 Conference on Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP) , pages 6382–6388, Hong Kong, China,\nNovember 2019. Association for Computational Linguistics.\n[46] https://www.facebook.com/Drugscom. List of 98 de-\npression medications compared (page 4). Drugs.com.\nhttps://www.drugs.com/condition/depression.html?page number=4,\nJanuary 1970.\n[47] Rohan Kamath, Arpan Ghoshal, Sivaraman Eswaran, and Prasad Hon-\nnavalli. An enhanced context-based emotion detection model using\nRoBERTa. In 2022 IEEE International Conference on Electronics,\nComputing and Communication Technologies (CONECCT) , pages 1–6,\nJuly 2022.\n[48] Francesco Barbieri, Jose Camacho-Collados, Luis Espinosa Anke, and\nLeonardo Neves. TweetEval: Unified benchmark and comparative\nevaluation for tweet classification. In Findings of the Association for\nComputational Linguistics: EMNLP 2020 , pages 1644–1650, Online,\nNovember 2020. Association for Computational Linguistics.\n[49] Youwei Song, Jiahai Wang, Zhiwei Liang, Zhiyue Liu, and Tao Jiang.\nUtilizing BERT intermediate layers for aspect based sentiment analysis\nand natural language inference, February 2020.\n[50] Loukas Ilias, Spiros Mouzakitis, and Dimitris Askounis. Calibration of\ntransformer-based models for identifying stress and depression in social\nmedia. IEEE Transactions on Computational Social Systems , pages 1–\n12, 2023.\n[51] Kailai Yang, Tianlin Zhang, and Sophia Ananiadou. A mental state\nKnowledge–aware and Contrastive Network for early stress and depres-\nsion detection on social media. Information Processing & Management,\n59(4):102961, July 2022.\n[52] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,\nPiyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised\nlearning of language representations. arXiv preprint arXiv:1909.11942 ,\n2019.\n[53] James W Pennebaker, Roger J Booth, Ryan L Boyd, and Martha E\nFrancis. The development and psychometric properties of liwc2015.\nAustin, TX: University of Texas at Austin , 2015.\n[54] Yiwen Ji, Shenglin Pan, Qingyang Liu, and Hongzhan Zhou. Mentalbert:\nPublicly available pretrained language models for mental healthcare.\narXiv preprint arXiv:2208.03156 , 2022.\n[55] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement\nDelangue, Anthony Moi, Pierric Cistac, Tim Rault, R ´emi Louf, Morgan\nFuntowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma,\nYacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,\nMariama Drame, Quentin Lhoest, and Alexander M. Rush. Transform-\ners: State-of-the-art natural language processing. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natural Language Pro-\ncessing: System Demonstrations , pages 38–45, Online, October 2020.\nAssociation for Computational Linguistics.\n[56] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Brad-\nbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein,\nLuca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary\nDeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit\nSteiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An\nimperative style, high-performance deep learning library. In Advances\nin Neural Information Processing Systems 32, pages 8024–8035. Curran\nAssociates, Inc., 2019.\n[57] Edward Ma. NLP augmentation, 2019.\n[58] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion,\nO. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vander-\nplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay. Scikit-learn: Machine learning in python. Journal of Machine\nLearning Research, 12:2825–2830, 2011.\n[59] Loukas Ilias, Dimitris Askounis, and John Psarras. A multimodal\napproach for dementia detection from spontaneous speech with tensor\nfusion layer. In 2022 IEEE-EMBS International Conference on Biomed-\nical and Health Informatics (BHI) , pages 1–5, September 2022.\n[60] Loukas Ilias, Dimitris Askounis, and John Psarras. Detecting dementia\nfrom speech and transcripts using transformers. Computer Speech &\nLanguage, 79:101485, April 2023.\n[61] Kai-Cheng Yang, Onur Varol, Alexander C. Nwala, Mohsen Sayyadi-\nharikandeh, Emilio Ferrara, Alessandro Flammini, and Filippo Menczer.\nSocial bots: Detection and challenges. in: Yasseri, t. (ed.). Handbook of\nComputational Social Science, Edward Elgar Publishing Ltd , 2024.\n[62] Sheila Dang. Exclusive: Elon Musk’s X restructuring curtails disinfor-\nmation research, spurs legal fears. https://www.reuters.com/technology/\nelon-musks-x-restructuring-curtails-disinformation-research-spurs-legal-fears-2023-11-06/.\n[Accessed 30-06-2024].\n[63] Sravani Boinepelli, Tathagata Raha, Harika Abburi, Pulkit Parikh, Niyati\nChhaya, and Vasudeva Varma. Leveraging mental health forums for\nuser-level depression detection on social media. In Nicoletta Calzo-\nlari, Fr ´ed´eric B ´echet, Philippe Blache, Khalid Choukri, Christopher\nCieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard,\nJoseph Mariani, H ´el`ene Mazo, Jan Odijk, and Stelios Piperidis, editors,\nProceedings of the Thirteenth Language Resources and Evaluation\nConference, pages 5418–5427, Marseille, France, June 2022. European\nLanguage Resources Association.\n[64] Inna Pirina and C ¸ a ˘grı C ¸¨oltekin. Identifying depression on Reddit:\nThe effect of training data. In Graciela Gonzalez-Hernandez, Davy\nWeissenbacher, Abeed Sarker, and Michael Paul, editors, Proceedings\nof the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining\nfor Health Applications Workshop & Shared Task, pages 9–12, Brussels,\nBelgium, October 2018. Association for Computational Linguistics.\n[65] Guangyao Shen, Jia Jia, Liqiang Nie, Fuli Feng, Cunjun Zhang, Tian-\nrui Hu, Tat-Seng Chua, and Wenwu Zhu. Depression detection via\nharvesting social media: A multimodal dictionary learning solution.\nIn Proceedings of the Twenty-Sixth International Joint Conference on\nArtificial Intelligence, IJCAI-17 , pages 3838–3844, 2017.\n[66] Hamad Zogan, Imran Razzak, Shoaib Jameel, and Guandong Xu.\nHierarchical convolutional attention network for depression detection\non social media and its impact during pandemic. IEEE Journal of\nBiomedical and Health Informatics , 28(4):1815–1823, 2024.\n[67] David E. Losada, Fabio Crestani, and Javier Parapar. Overview of\nerisk 2019 early risk prediction on the internet. In Fabio Crestani,\nMartin Braschler, Jacques Savoy, Andreas Rauber, Henning M ¨uller,\nDavid E. Losada, Gundula Heinatz B ¨urki, Linda Cappellato, and Nicola\nFerro, editors, Experimental IR Meets Multilinguality, Multimodality,\nand Interaction , pages 340–357, Cham, 2019. Springer International\nPublishing.\n[68] David E. Losada, Fabio Crestani, and Javier Parapar. erisk 2020: Self-\nharm and depression challenges. In Joemon M. Jose, Emine Yilmaz,\nJo˜ao Magalh˜aes, Pablo Castells, Nicola Ferro, M ´ario J. Silva, and Fl´avio\nMartins, editors, Advances in Information Retrieval , pages 557–563,\nCham, 2020. Springer International Publishing.\n[69] Javier Parapar, Patricia Mart ´ın-Rodilla, David E. Losada, and Fabio\nCrestani. Overview of erisk 2021: Early risk prediction on the internet.\nIn K. Selc ¸uk Candan, Bogdan Ionescu, Lorraine Goeuriot, Birger Larsen,\nHenning M ¨uller, Alexis Joly, Maria Maistro, Florina Piroi, Guglielmo\nFaggioli, and Nicola Ferro, editors, Experimental IR Meets Multilin-\nguality, Multimodality, and Interaction , pages 324–344, Cham, 2021.\nSpringer International Publishing.\n[70] Loukas Ilias and Dimitris Askounis. Explainable identification of\ndementia from transcripts using transformer networks. IEEE Journal\nof Biomedical and Health Informatics , 26(8):4153–4164, August 2022.\n[71] Loukas Ilias, Felix Soldner, and Bennett Kleinberg. Explainable verbal\ndeception detection using transformers, October 2022.",
  "topic": "Social media",
  "concepts": [
    {
      "name": "Social media",
      "score": 0.5856506824493408
    },
    {
      "name": "Transformer",
      "score": 0.5535493493080139
    },
    {
      "name": "Computer science",
      "score": 0.4619612395763397
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3611902594566345
    },
    {
      "name": "Engineering",
      "score": 0.1718711256980896
    },
    {
      "name": "Electrical engineering",
      "score": 0.12614861130714417
    },
    {
      "name": "Voltage",
      "score": 0.11997592449188232
    },
    {
      "name": "World Wide Web",
      "score": 0.11310657858848572
    }
  ]
}