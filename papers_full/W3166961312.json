{
  "title": "Towards a Comprehensive Understanding and Accurate Evaluation of Societal Biases in Pre-Trained Transformers",
  "url": "https://openalex.org/W3166961312",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2662522191",
      "name": "Andrew Silva",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2894391065",
      "name": "Pradyumna Tambwekar",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4208328415",
      "name": "Matthew Gombolay",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2954275542",
    "https://openalex.org/W2972413484",
    "https://openalex.org/W2963526187",
    "https://openalex.org/W2962787423",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W4252004167",
    "https://openalex.org/W2804545603",
    "https://openalex.org/W2952349219",
    "https://openalex.org/W2970583189",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W4287774713",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2898081668",
    "https://openalex.org/W3035102548",
    "https://openalex.org/W3091818438",
    "https://openalex.org/W3017701505",
    "https://openalex.org/W3034600233",
    "https://openalex.org/W2972668795",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W2963078909",
    "https://openalex.org/W3105882417",
    "https://openalex.org/W2802105481",
    "https://openalex.org/W2963723885",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W3019416653",
    "https://openalex.org/W2963457723",
    "https://openalex.org/W2483215953",
    "https://openalex.org/W4297816198",
    "https://openalex.org/W3109867714",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W4288029087"
  ],
  "abstract": "Andrew Silva, Pradyumna Tambwekar, Matthew Gombolay. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
  "full_text": "Towards a Comprehensive Understanding and Accurate Evaluation of\nSocietal Biases in Pre-Trained Transformers\nAndrew Silva∗ Pradyumna Tambwekar∗\nSchool of Interactive Computing\nGeorgia Institute of Technology\nAtlanta, GA\n{andrew.silva, ptambwekar3, matthew.gombolay}@gatech.edu\nMatthew Gombolay\nAbstract\nThe ease of access to pre-trained transformers\nhas enabled developers to leverage large-scale\nlanguage models to build exciting applications\nfor their users. While such pre-trained models\noffer convenient starting points for researchers\nand developers, there is little consideration\nfor the societal biases captured within these\nmodel risking perpetuation of racial, gender,\nand other harmful biases when these models\nare deployed at scale. In this paper, we inves-\ntigate gender and racial bias across ubiquitous\npre-trained language models, including GPT-2,\nXLNet, BERT, RoBERTa, ALBERT and Dis-\ntilBERT. We evaluate bias within pre-trained\ntransformers using three metrics: WEAT, se-\nquence likelihood, and pronoun ranking. We\nconclude with an experiment demonstrating\nthe ineffectiveness of word-embedding tech-\nniques, such as WEAT, signaling the need for\nmore robust bias testing in transformers.\n1 Introduction\nTransformer models represent the state-of-the-art\nfor many natural language processing (NLP) tasks,\nsuch as question-answering (Devlin et al., 2019),\ndialogue (Smith et al., 2020), search results (Nayak,\n2019), and more. Popular pre-trained models, such\nas those available from Hugging Face (Wolf et al.,\n2019), allow developers without extensive compu-\ntation power to beneﬁt from these models. How-\never, it is important to fully understand the latent\nsocietal biases within these black-box transformer\nmodels. Without appropriately considering inher-\nent biases, development on top of pre-trained trans-\nformers risks exacerbating and propagating racial,\ngender, and other biases writ large.\nBefore transformers, word embedding models\nsuch as Word2Vec (Mikolov et al., 2013) and\nGloVe (Pennington et al., 2014) were shown to ex-\nhibit systematic sexist (Bolukbasi et al., 2016) and\n∗ These authors contributed equally to this work\nracist (Manzini et al., 2019) biases. Initial investi-\ngations into bias for transformers (Vig et al., 2020;\nBasta et al., 2019; Bommasani et al., 2020) have\nfound that these new language models are similarly\nbiased. As transformers are increasingly common-\nplace, a more complete view of the inequalities,\nbiases, or under-representations within pre-trained\ntransformers becomes increasingly important.\nYet, discovering bias in transformer models has\nproven to be more nuanced than bias-discovery in\nword embedding models (Kurita et al., 2019; May\net al., 2019). Prior work on bias in modern trans-\nformer models has used only a single test or metric\nat a time, which we show in this paper provides\nan incomplete view of the problem. Furthermore,\nwe ﬁnd evidence that certain tests are ill-suited\nto understanding bias in transformer architectures,\nsupported by prior work (Blodgett et al., 2020).\nMoreover, we show that employing multiple tests\nis necessary for a full picture of the issue as no\nsingle test is currently sufﬁcient.\nIn the context of our work, “bias” refers speciﬁ-\ncally to the preference of a model for one gender or\nrace in the presence of an otherwise neutral context.\nAs an example, consider the sequence “[MASK]\nwept upon arriving to the scene.” With no addi-\ntional information, an equitable system would ex-\nhibit no preference forfemale over male, or African-\nAmerican over European-American names; how-\never, our results indicate that there is often a sta-\ntistically signiﬁcant preference ( p <0.0001) for\nassociating female and African-American identi-\nﬁers with being more “emotional.”\nWe provide two key contributions to understand-\ning and mitigating bias in contextual language mod-\nels. First, we conduct a comprehensive, compar-\native evaluation of gender and racial bias using\nmultiple tests for widely-used pretrained models.\nSecond, we construct a novel experiment for de-\nbiasing a contextual language model on a down-\nstream task (Zellers et al., 2018). Our experiment\nModel Name WC WM WS SEQA SEQF SEQS SEQJ PNA PNF PNS PNJ\nUncased:\nBERT-Base 1.47 -0.33 -0.3 4.53∗ 3.70 2.53 4.02 ∗ 5.29∗ -3.31 -2.65 -1.62\nBERT-Large 1.10 -0.55 -0.16 0.53 0.33 0.83 1.07 5.42∗ -3.15 -3.62 -2.11\nBERT-LargeM 1.60 -0.24 -0.33 -2.90 -2.14 -2.39 -2.48 1.41 0.64 -0.71 1.38\nDistilBERT 1.64 -0.37 -0.34 5.85∗ 6.20∗ 6.08∗ 6.08∗ 2.82∗ -4.71∗ -5.22∗ -5.06∗\nALBERT-Base 1.41 1.61 1.51 -3.98∗ -3.48 -3.27 -3.15 -19.4∗ -19.7∗ -19.3∗ -19.9∗\nALBERT-Large 1.46 1.42 1.05 -3.75 -2.79 -3.55 -3.61 0.96 -2.47 -2.94 -6.00 ∗\nALBERT-XLarge 1.52 1.54 1.55 1.47 2.02 1.37 0.99 3.90∗ 0.32 1.55 -4.56 ∗\nALBERT-XXLarge 1.47 1.38 1.39 -2.45 -1.39 -0.97 -1.44 5.89∗ 4.85∗ 2.30 -0.09\nCased:\nBERT-Base 0.30 -0.04 0.57 8.83∗ 10.8∗ 10.6∗ 10.6∗ 4.17∗ 0.17 -1.65 -3.12\nBERT-Large 0.53 -0.44 -0.05 5.17∗ 5.47∗ 4.50∗ 5.47∗ 1.44 -0.91 -1.66 -1.18\nBERT-LargeM 0.18 0.23 -0.15 2.63 3.78 4.15 ∗ 3.93∗ 2.27 -0.55 -1.79 -3.21\nDistilBERT 0.14 -0.27 0.57 11.1∗ 11.6∗ 11.7∗ 11.7∗ 2.15 -6.17 ∗ -7.11∗ -9.19∗\nRoBERTa-Base 0.91 0.59 0.67 4.19∗ 4.59∗ 4.44∗ 4.36∗ -0.99 -4.80 ∗ -5.14∗ -4.10∗\nRoBERTa-Large 0.56 0.64 0.68 3.95∗ 4.54∗ 5.41∗ 5.55∗ 2.09 -2.92 -1.01 -1.67\nDistilRoBERTa 1.00 0.66 0.56 12.6∗ 12.6∗ 12.4∗ 12.6∗ -2.47 -8.55 ∗ -8.19∗ -8.28∗\nGPT-2 0.78 -0.03 -0.31 -2.99 -1.95 -3.38 -2.55 1.88 2.31 2.45 1.50\nGPT-2-Medium 0.24 -0.21 0.07 1.51 2.92 2.21 2.11 0.26 0.19 0.38 0.31\nGPT-2-Large 0.54 0.04 -0.46 3.43 3.92 ∗ 3.02 3.72 -0.59 -0.50 -0.03 -1.37\nGPT-2-XLarge 0.53 -0.23 0.13 3.18 4.06 ∗ 2.90 3.24 7.51∗ 1.35 2.96 6.33 ∗\nXLNet-Base 0.60 0.69 0.36 1.75 2.63 1.99 1.08 0.46 0.96 1.07 1.00\nXLNet-Large 0.16 0.10 0.42 2.34 2.94 5.74 ∗ 3.67 -0.01 3.09 1.01 0.64\nTable 1: Bias scores along the gender dimension. Positive indicates bias towards Male; negative indicates bias\ntowards Female. Asterisks denote statistical signiﬁcance α= 0.05/336.\nrefutes the validity of WEAT for contextual models,\nsignaling a need for new bias metrics.\n2 Related Work\nAfter the seminal work of Bolukbasi et al. (2016),\nbias has been found ubiquitous in word embedding\nmodels (Amorim et al., 2018; Brunet et al., 2018;\nRudinger et al., 2018; Zhao et al., 2017; Costa-juss`a\net al., 2019; Silva et al., 2020). Researchers have\napplied association tests between word embeddings\nto look for inappropriate correlations. Caliskan\net al. (2017) introduce the Word Embedding As-\nsociate Test (WEAT) to estimate implicit biases\nin word embeddings by measuring average cosine\nsimilarities of target and attribute sets. The WEAT\nhas been extended into a sequence test (May et al.,\n2019), though the efﬁcacy of both tests remains in\nquestion for transformers (Ethayarajh et al., 2019;\nKurita et al., 2019).\nPrior work has also devised methods to mea-\nsure contextual bias. Kiritchenko and Moham-\nmad (2018) introduce the Equity Evaluation Cor-\npus (EEC), which includes templated sequences\nsuch as “⟨TARGET⟩feels ⟨ATTRIBUTE⟩,” where\ngendered or racial tokens are the “targets” and emo-\ntional words are the “attributes.” The average of the\ndifference in likelihoods for target sets constitutes\nthe bias score. We leverage this in our work as the\nsequence ranking test (SEQ).\nKurita et al. (2019) and Vig et al. (2020) devise\na pronoun-ranking test for BERT by comparing\nrelative likelihoods of target words. Rather than\nsequence likelihood, the authors instead measure\ncontextual likelihood, which helps to control for\na model’s overarching bias. We extend this work,\napplying the pronoun-ranking test (PN) to score\nthe most commonly used transformer models and\ncontextualizing the results with SEQ scores.\nInvestigations of biases in contextual language\nmodels, e.g. transformers, have yielded mixed re-\nsults. Basta et al. (2019) found that BERT and GPT\nexhibit a reduced bias-dimension relative to word\nembedding models, whereas Kurita et al. (2019)\nfound that BERT is biased and that conventional\ntests, e.g. WEAT, are inappropriate. Recent work\nhas also looked to identify bias by crowdsourcing a\nsterotype dataset (Nadeem et al., 2020; Zhao et al.,\n2018; Nangia et al., 2020). These approaches de-\nvelop a bias analysis metric by empirically comput-\ning a pretrained model’s preference towards stereo-\ntyped sentences. However, such work is speciﬁ-\ncally focused on showcasing the effectiveness of\nthese speciﬁc datasets for identifying bias. Our\nresults paint a more complete picture, providing in-\nsight into speciﬁc aspects of gender and racial bias\nand unifying disparate viewpoints of prior work.\nFurthermore, we present a targeted investigation\ninto the relevance of the WEAT for transformers.\nModel Name WR SEQA SEQF SEQS SEQJ PNA PNF PNS PNJ\nUncased:\nBERT-Base 0.66 -10.8∗ -12.7∗ -12.3∗ -13.5∗ -0.74 1.45 1.70 3.82 ∗\nBERT-Large 0.02 6.91∗ 8.11∗ 4.40∗ 6.34∗ 0.90 2.82 2.73 3.61\nBERT-LargeM 0.44 -13.7∗ -14.3∗ -13.6∗ -13.4∗ 5.13∗ 11.4∗ 9.34∗ 5.65∗\nDistilBERT 1.15 -21.3∗ -22.4∗ -22.2∗ -22.4∗ 5.84∗ 6.80∗ 13.5∗ 14.8∗\nALBERT-Base 0.45 -18.4∗ -18.2∗ -17.8∗ -17.6∗ 17.5∗ 17.4∗ 17.5∗ 17.6∗\nALBERT-Large 0.62 -16.9∗ -19.2∗ -19.7∗ -19.6∗ 19.0∗ 19.3∗ 20.0∗ 20.0∗\nALBERT-XLarge 0.85 0.26 -1.53 0.72 -1.80 7.87∗ 8.81∗ 6.68∗ 12.89∗\nALBERT-XXLarge 0.48 -5.05∗ -5.98∗ -5.43∗ -5.21∗ 5.24∗ 6.18∗ 5.97∗ 7.26∗\nCased:\nBERT-Base -0.22 -22.4∗ -24.3∗ -24.2∗ -23.6∗ 9.7∗ 10.3∗ 10.4∗ 13.4∗\nBERT-Large 0.17 -18.9∗ -20.6∗ -18.6∗ -20.8∗ 1.61 2.31 2.65 2.30\nBERT-LargeM 0.003 -23.8∗ -27.4∗ -25.6∗ -23.9∗ 6.81∗ 9.63∗ 11.69∗ 7.74∗\nDistilBERT -0.03 -28.7∗ -29.8∗ -29.1∗ -29.1∗ 15.5∗ 13.9∗ 19.3∗ 17.4∗\nRoBERTa-Base 0.22 -20.8∗ -20.7∗ -20.5∗ -20.2∗ 2.81 5.26 ∗ 2.77 5.09 ∗\nRoBERTa-Large 0.94 -21.2∗ -22.0∗ -22.6∗ -21.9∗ 2.18 4.37 ∗ 3.75 5.33 ∗\nDistilRoBERTa 0.14 -11.17∗ -10.8∗ -10.6∗ -10.5∗ 6.94∗ 5.00∗ 7.19∗ 11.7∗\nGPT-2 0.46 -2.25 -0.95 -0.21 0.29 -0.06 0.29 -0.18 -0.18\nGPT-2-Medium 0.53 -4.31∗ -3.81∗ -3.00 -2.52 -0.09 -0.38 -0.13 0.08\nGPT-2-Large 0.33 -1.66 -1.00 -0.09 -0.17 -5.78∗ -2.51 -2.41 -1.59\nGPT-2-XLarge -0.16 -0.81 -0.27 0.56 0.88 -18.84∗ -9.83∗ -2.57 -5.12 ∗\nXLNet-Base -0.17 -2.84 -4.05 ∗ -3.22 -4.73 ∗ 0.58 0.71 1.03 0.67\nXLNet-Large -0.03 -15.3∗ -16.6∗ -15.9∗ -12.2∗ 3.01 2.46 1.70 4.36 ∗\nTable 2: Bias scores along the racial dimension. Positive indicates bias towards European-American; negative\nindicates bias towards African-American. Asterisks denote statistical signiﬁcance at α= 0.05/336.\n3 Approach and Results\nWe apply three tests (i.e. the WEAT (W), sequence\nlikelihood ( SEQ), and pronoun ranking ( PN))\nto popular pre-trained transformers from Hugging\nFace (Wolf et al., 2019), including the cased and\nuncased1 BERT and DistilBert models, the un-\ncased ALBERT models, and the cased RoBERTa,\nDistilRoBERTa, GPT-2, and XLNet models. For\ngender, we compare the WEAT tests for career\n(WC), math (WM ), and science (WS), against the\nsequence likelihood and pronoun ranking tests for\nanger (SEQA and PNA), fear (SEQF and PNF ),\nsadness (SEQS and PNS), and joy (SEQJ and\nPNJ) evaluated between male and female target\nwords. For race, we use the only WEAT avail-\nable for race (WR) as well as the same SEQ and\nPN tests evaluated between African-American and\nEuropean-American targets.\nThe results of our WEAT, sequence likelihood,\nand pronoun ranking bias tests are presented in Ta-\nbles 1 and 2. The quantity listed for each model/test\npair is the effect size for that two-sided t-test test un-\nder they hypothesis that there is a signiﬁcant differ-\nence between the mean likelihoods across the two\ngroups. Using multiple tests is important; many\nmodels exhibit systematic preference for one target\naccording to SEQ, while the PN reveals contex-\n1Casing is a design decision affecting the tokenization for\na model. For all models, we test every size available.\ntual preference in a different direction. The models\noften assign higher likelihood to male sequences,\nbut when speciﬁcally considering the subject of\nan emotional sentence, female subjects are more\nlikely. To address inherent model bias, it is impor-\ntant to understand how this bias manifests which\nwe discuss below.\nModel size and bias – Examining the SEQ and\nPN results for distilled models DistilBERT and\nDistilRoBERTa, we see that these models almost\nalways exhibit statistically signiﬁcant bias and that\nthe effect sizes for these biases are often much\nstronger than the original models from which they\nwere distilled (BERT and RoBERTa). This ﬁnding\nis in line with contemporary work by Hooker et al.\n(2020), who show that distillation in vision models\ndisproportionately harms underrepresented groups.\nWe show that the same is true for transformers.\nThe opposite is not true: increasing model capac-\nity does not remove bias. While prior work (Gilburt,\n2019; Tan and Celis, 2019) has reported increasing\nmodel size correlates with decreasing bias, we ﬁnd\nthat this is not always the case (see GPT2-Base\nvs. GPT2-Large), as supported by Nadeem et al.\n(2020) in stereotype-likelihood tests.\nTokenization matters – We consider four archi-\ntectures that come in cased and uncased versions,\ndiffering only in tokenization BERT-Base, BERT-\nLarge, BERT-LargeM, and DistilBERT. Across\nModel Name WC WM WS WR SEQA SEQF SEQS SEQJ PNA PNF PNS PNJ\nGender:\nSW AG-Only 0.91 0.63 0.70 – 14.4∗ 14.2∗ 14.8∗ 16.5∗ -10.6∗ -7.98∗ -10.15∗ -0.13\n+WEAT -0.006 0.003 0.0002 – -7.74∗ -9.95∗ -10.9∗ -11.4∗ -37.3∗ -36.8∗ -37.9∗ -37.77∗\nRace:\nSW AG-Only – – – 0.21 -13.5∗ -15∗ -14.6∗ -13.3∗ 0.03 -2.70 -1.30 3.89 ∗\n+WEAT – – – -0.002 -8.62∗ -9.85∗ -9.02∗ -7.95∗ 2.57 5.86 ∗ 6.99∗ 10.6∗\nTable 3: Positive indicates bias towards European-American or male; negative indicates bias towards African-\nAmerican or female. Asterisks denote statistical signiﬁcance at α< 0.05/72. Lowest effect sizes are bold.\nrace and gender, the uncased models exhibit less\nbias and greater diversity for names and pronouns.\nThe effects of tokenization may also play a\nrole in WEAT’s underperformance, as the mean-\nembeddings used to estimate a WEAT effect do\nnot accurately reﬂect the expected words for the\ntest. For example, under the ALBERT tokenizer,\n“Nichelle” becomes “niche” and “lle”, two sub-\nwords which may not average out to a name.\nWEAT is inconsistent – We ﬁnd that WEAT is a\npoor predictor of contextual bias and an internally-\ninconsistent metric. The WEAT for math ( WM )\nand science (WS) use words which are very similar\nand, at times, even overlapping. As such, we would\nexpect the WM and WS scores to indicate bias in\nthe same direction for every model. Instead, we see\nthat the WEAT results show differing magnitudes\nand occasionally point in different directions.\nGiven the inconsistency of WEAT and its poor\ncorrelation with SEQ and PN effects, we propose\na debiasing scheme using the WEAT effect. If neu-\ntralizing the WEAT effect also neutralizes SEQ\nand PN bias, then the WEAT remains a useful\ntest for transformers. However, if neutralizing the\nWEAT has no effect on theSEQ and PN scores,\nwe can conclude that the WEAT is simply not ap-\npropriate for contextual models.\n4 Debiasing Transformers with WEAT\nWe now employ WEAT scores as a loss regularizer\nto “de-bias” a RoBERTa model being trained on the\nSituations With Adversarial Generations (SW AG)\ndataset, a commonsense inference dataset in which\neach sample is a sentence with four possible end-\nings (Zellers et al., 2018). The SWAG training\nobjective is to minimize the model’s cross-entropy\nloss, LMC , for choosing the correct ending. In ad-\ndition to this loss, we incorporate WEAT scores as\na regularizer, as shown in Equation 1. Here, λw is\na hyper-parameter, and WM ,WR,WC,WS are the\nWEAT scores for each category. We hypothesize\nthat, even if a model is able to minimize WEAT\neffects, the model will remain signiﬁcantly biased.\nL= LMC + λw(WM + WR + WC + WS) (1)\n4.1 Results\nWe measure the accuracy of our ﬁne-tuned models\non SW AG and ﬁnd that the debiased model exhibits\ncompetitive accuracy. The WEAT-regularized\nmodel achieves 82.2% accuracy, compared to\n82.8% for a human (Zellers et al., 2018) and 83.3%\nfor the best RoBERTa-base model.\nThe results from the WEAT regularization are in\nTable 3. Table 3 shows that ﬁne-tuning with SW AG\nalone (without any bias regularizers) yields signiﬁ-\ncant bias toward male and African-American SEQ\ntests (8/8 attribute tests show signiﬁcance), and fe-\nmale and European-American for PN tests (4/8\nattribute tests show signiﬁcance). Furtheremore,\nwe ﬁnd that even though our “de-biased” model\nshows ≈0 effect for WEAT, Table 3 shows that\nthis model remains signiﬁcantly biased on both the\nSEQ and PN tests. De-biasing with WEAT has\nexaggerated gender bias for the PN test compared\nto the SWAG-only model, whereas for the SEQ\ntests the bias has been ﬂipped to being signiﬁcantly\nbiased towards female. Tests for racial bias are like-\nwise reﬂective of this trend. These results demon-\nstrate that the WEAT is an insufﬁcient measure of\nbias. Neutralizing word-piece embeddings does\nnot remove the contextual aspect of bias learned by\nRoBERTa and may even exacerbate biases.\n4.2 Discussion\nOur results demonstrate that bias is a signiﬁcant\nproblem for nearly all pre-trained models. Unfortu-\nnately, the problem is not simply solved by using\nlarger networks or more data. As shown in Tables 1\n& 2, the approach with the most data, RoBERTa, is\namong the most consistently biased transformers in\nour study, while the largest model, GPT-2 XLarge,\nexhibits greater bias than GPT-2 Base. Tokeniza-\ntion also has an immense impact on the equitable\nuse of language models, and is often overlooked\nwithin discourse surrounding bias. We encourage\nthe community to consider these effects on minor-\nity communities whose names or vernacular will\nbe distorted more than majority communities due\nto the nature of word-piece tokenization.\nDeveloping tests that can contextually identify\nbias within transformers remains vital. Our “de-\nbiasing” results show that relying on ill-ﬁtting tests\ncan lead to harmful false positives. We show that\n“successfully” de-biasing a model via a WEAT reg-\nularizer results in continued or even ampliﬁed bias\non both the SEQ and PN tests, despite that near-\nzero WEAT effects. We conclude that contextually-\nand globally-sensitive bias tests are needed for fu-\nture debiasing research, as mitigating bias accord-\ning to WEAT fails to truly neutralize pre-trained\ntransformer models.\n5 Conclusion\nWe systematically quantify bias in commonly used\npre-trained transformers, presenting a uniﬁed view\nof bias in the form of gender and racial likelihoods\nacross a range of popular pre-trained transformers.\nWe analyze factors inﬂuencing bias in transform-\ners using three tests, SEQ, PN, and WEAT , and\ndemonstrate the inadequacies of word-embedding\nneutralization for contextual models. We call for fu-\nture work to develop robust bias tests and carefully\nconsider the ramiﬁcations of design choices.\nEthics & Impact Statement\nOur work targets the subject of inherent, societal\nbiases captured by large pre-trained transformer\nmodels which are publicly available and widely\nused. Our results indicate that bias is a signiﬁcant\nproblem for the community to tackle, and that all\npre-trained models currently exhibit some form of\nbiased prediction of gendered or racial tokens in\notherwise neutral contexts.\nBeneﬁciaries – Our work seeks to clarify the\nways in which commonly used pre-trained trans-\nformers exhibit biases. Practitioners building on\nthe power of pre-trained transformers would beneﬁt\nfrom knowing, the inherent biases of each model,\nand thereby taking appropriate steps to ensure that\ntheir downstream task is as neutralized as possible.\nFurther, we hope to contribute knowledge which\nwill eventually make all NLP systems more equi-\ntable for all people.\nNegatively affected parties – Our work does\nnot investigate bias in many other areas, from racial\ngroups outside of European-American/African-\nAmerican to religious biases or any other inappro-\npriate societal prejudices. Unfortunately, there are\nfew widely-accepted target-set identiﬁers for NLP\nresearch into these biases, and even those which do\nexist may be poor predictors of underlying demo-\ngraphics (such as the use of ﬁrst names for racial\ncategorization).\nLimitations in scope – As discussed above, our\nwork omits investigations into groups which lack\nwidely-accepted target sets (identifying nouns or\npronouns). Even for target sets which do exist,\nsuch as Male/Female, target sets may be imperfect.\nFor example, many gendered target sets use ﬁrst\nnames as identiﬁers, even though there is no gender\ninherently tied to a name.\nAcknowledgments\nThis work was supported by Georgia Institute of\nTechnology state funding.\nReferences\nEvelin Amorim, Marcia Canc ¸ado, and Adriano Veloso.\n2018. Automated essay scoring in the presence of\nbiased ratings. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n229–237, New Orleans, Louisiana. Association for\nComputational Linguistics.\nChristine Basta, Marta R. Costa-juss `a, and Noe Casas.\n2019. Evaluating the underlying gender bias in con-\ntextualized word embeddings. In Proceedings of the\nFirst Workshop on Gender Bias in Natural Language\nProcessing, pages 33–39, Florence, Italy. Associa-\ntion for Computational Linguistics.\nSu Lin Blodgett, Solon Barocas, Hal Daum ´e III, and\nHanna Wallach. 2020. Language (technology) is\npower: A critical survey of” bias” in nlp. arXiv\npreprint arXiv:2005.14050.\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou,\nVenkatesh Saligrama, and Adam T Kalai. 2016.\nMan is to computer programmer as woman is to\nhomemaker? debiasing word embeddings. In Ad-\nvances in neural information processing systems,\npages 4349–4357.\nRishi Bommasani, Kelly Davis, and Claire Cardie.\n2020. Interpreting pretrained contextualized repre-\nsentations via reductions to static embeddings. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4758–\n4781.\nMarc-Etienne Brunet, Colleen Alkalay-Houlihan, Ash-\nton Anderson, and Richard Zemel. 2018. Under-\nstanding the origins of bias in word embeddings.\narXiv preprint arXiv:1810.03611.\nAylin Caliskan, Joanna J Bryson, and Arvind\nNarayanan. 2017. Semantics derived automatically\nfrom language corpora contain human-like biases.\nScience, 356(6334):183–186.\nMarta R. Costa-juss `a, Christian Hardmeier, Will Rad-\nford, and Kellie Webster, editors. 2019. Proceed-\nings of the First Workshop on Gender Bias in Natu-\nral Language Processing. Association for Computa-\ntional Linguistics, Florence, Italy.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nKawin Ethayarajh, David Duvenaud, and Graeme Hirst.\n2019. Understanding undesirable word embedding\nassociations. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1696–1705, Florence, Italy. Associa-\ntion for Computational Linguistics.\nBen Gilburt. 2019. Gender bias in gpt-2.\nSara Hooker, Nyalleng Moorosi, Gregory Clark, Samy\nBengio, and Emily Denton. 2020. Characteris-\ning bias in compressed models. arXiv preprint\narXiv:2010.03058.\nSvetlana Kiritchenko and Saif M Mohammad. 2018.\nExamining gender and race bias in two hun-\ndred sentiment analysis systems. arXiv preprint\narXiv:1805.04508.\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,\nand Yulia Tsvetkov. 2019. Measuring bias in con-\ntextualized word representations. arXiv preprint\narXiv:1906.07337.\nThomas Manzini, Lim Yao Chong, Alan W Black,\nand Yulia Tsvetkov. 2019. Black is to criminal\nas caucasian is to police: Detecting and removing\nmulticlass bias in word embeddings. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 615–621, Minneapo-\nlis, Minnesota. Association for Computational Lin-\nguistics.\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\nBowman, and Rachel Rudinger. 2019. On measur-\ning social biases in sentence encoders. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 622–628, Minneapo-\nlis, Minnesota. Association for Computational Lin-\nguistics.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efﬁcient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nMoin Nadeem, Anna Bethke, and Siva Reddy.\n2020. Stereoset: Measuring stereotypical bias\nin pretrained language models. arXiv preprint\narXiv:2004.09456.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R Bowman. 2020. Crows-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models. arXiv preprint arXiv:2010.00133.\nPandu Nayak. 2019. Understanding searches better\nthan ever before.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 1532–1543.\nRachel Rudinger, Jason Naradowsky, Brian Leonard,\nand Benjamin Van Durme. 2018. Gender bias in\ncoreference resolution. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers),\npages 8–14, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nAndrew Silva, Rohit Chopra, and Matthew Gombolay.\n2020. Using cross-loss inﬂuence functions to ex-\nplain deep network representations. arXiv preprint\narXiv:2012.01685.\nEric Michael Smith, Mary Williamson, Kurt Shuster,\nJason Weston, and Y-Lan Boureau. 2020. Can\nyou put it all together: Evaluating conversational\nagents’ ability to blend skills. arXiv preprint\narXiv:2004.08449.\nYi Chern Tan and L Elisa Celis. 2019. Assessing so-\ncial and intersectional biases in contextualized word\nrepresentations. In Advances in Neural Information\nProcessing Systems, pages 13209–13220.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\nSharon Qian, Daniel Nevo, Yaron Singer, and Stuart\nShieber. 2020. Causal mediation analysis for inter-\npreting neural nlp: The case of gender bias. arXiv\npreprint arXiv:2004.12265.\nThomas Wolf, L Debut, V Sanh, J Chaumond, C De-\nlangue, A Moi, P Cistac, T Rault, R Louf, M Fun-\ntowicz, et al. 2019. Huggingface’s transformers:\nState-of-the-art natural language processing. ArXiv,\nabs/1910.03771.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and\nYejin Choi. 2018. Swag: A large-scale adversarial\ndataset for grounded commonsense inference. arXiv\npreprint arXiv:1808.05326.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente\nOrdonez, and Kai-Wei Chang. 2017. Men also\nlike shopping: Reducing gender bias ampliﬁca-\ntion using corpus-level constraints. arXiv preprint\narXiv:1707.09457.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018. Gender bias in\ncoreference resolution: Evaluation and debiasing\nmethods. arXiv preprint arXiv:1804.06876.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5511507391929626
    },
    {
      "name": "Transformer",
      "score": 0.5435535311698914
    },
    {
      "name": "Computational linguistics",
      "score": 0.4883813261985779
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3289486765861511
    },
    {
      "name": "Engineering",
      "score": 0.21292263269424438
    },
    {
      "name": "Electrical engineering",
      "score": 0.13434955477714539
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    }
  ]
}