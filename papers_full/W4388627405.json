{
  "title": "DG-Affinity: predicting antigen–antibody affinity with language models from sequences",
  "url": "https://openalex.org/W4388627405",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1988620429",
      "name": "Ye Yuan",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A3138132220",
      "name": "Qushuo Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101199611",
      "name": "Jun Mao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114474984",
      "name": "Guipeng Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2059247565",
      "name": "Xiaoyong Pan",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A1988620429",
      "name": "Ye Yuan",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A3138132220",
      "name": "Qushuo Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101199611",
      "name": "Jun Mao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114474984",
      "name": "Guipeng Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2059247565",
      "name": "Xiaoyong Pan",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4284671925",
    "https://openalex.org/W4220851881",
    "https://openalex.org/W3084170404",
    "https://openalex.org/W2820768494",
    "https://openalex.org/W2971940586",
    "https://openalex.org/W2994597968",
    "https://openalex.org/W2995755986",
    "https://openalex.org/W2970803602",
    "https://openalex.org/W3093149563",
    "https://openalex.org/W3046413935",
    "https://openalex.org/W3162216940",
    "https://openalex.org/W3173769174",
    "https://openalex.org/W2999958881",
    "https://openalex.org/W3165032628",
    "https://openalex.org/W3155123963",
    "https://openalex.org/W4206634366",
    "https://openalex.org/W4205647504",
    "https://openalex.org/W3112792752",
    "https://openalex.org/W3161196577",
    "https://openalex.org/W3211169825",
    "https://openalex.org/W4284989724",
    "https://openalex.org/W3207667351",
    "https://openalex.org/W4285293142",
    "https://openalex.org/W2896617527",
    "https://openalex.org/W3203311843",
    "https://openalex.org/W4313272769",
    "https://openalex.org/W2092285329",
    "https://openalex.org/W1915490970",
    "https://openalex.org/W2157807259",
    "https://openalex.org/W2953103783",
    "https://openalex.org/W2283896596",
    "https://openalex.org/W3128461460",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W4366998330",
    "https://openalex.org/W4210520848",
    "https://openalex.org/W2957436444",
    "https://openalex.org/W4382982095",
    "https://openalex.org/W3209435597",
    "https://openalex.org/W4378471523",
    "https://openalex.org/W2901953282",
    "https://openalex.org/W2114850508",
    "https://openalex.org/W4385347236",
    "https://openalex.org/W6763868836",
    "https://openalex.org/W2898402099",
    "https://openalex.org/W4283068487",
    "https://openalex.org/W2950118240",
    "https://openalex.org/W3206187363",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W3111174583",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2804521888",
    "https://openalex.org/W2757172890",
    "https://openalex.org/W1948562438",
    "https://openalex.org/W2512372054",
    "https://openalex.org/W2036309115",
    "https://openalex.org/W3207204660",
    "https://openalex.org/W2130479394",
    "https://openalex.org/W4213099919",
    "https://openalex.org/W2971227267",
    "https://openalex.org/W4226085666"
  ],
  "abstract": null,
  "full_text": "DG‑Affinity: predicting antigen–antibody \naffinity with language models from sequences\nYe Yuan1*†, Qushuo Chen2†, Jun Mao2, Guipeng Li2 and Xiaoyong Pan1* \nBackground\nAntibody-mediated immune response is a central component of human immune sys -\ntem. Antibodies are a special protein that can specifically recognize invading antigens, \nsuch as viruses, by binding to epitopes on the antigens through the two ends of their \nY-shaped structure, known as the complementarity-determining regions (CDRs) [1–3]. \nDue to the high diversity of CDRs, they show binding specificity toward specific antigens \n[4]. The biopharmaceutical industry has utilized this specificity to develop monoclonal \nantibodies (MAbs) as therapeutic drugs, which have high success rates and efficacy for \ndiseases. In addition, they suffer to minimal side effects [5–8]. With the advancement of \nbiotechnology techniques, such as antibody–drug conjugates (ADCs), even traditional \nAbstract \nBackground: Antibody-mediated immune responses play a crucial role \nin the immune defense of human body. The evolution of bioengineering has led \nthe progress of antibody-derived drugs, showing promising efficacy in cancer \nand autoimmune disease therapy. A critical step of this development process is obtain-\ning the affinity between antibodies and their binding antigens.\nResults: In this study, we introduce a novel sequence-based antigen–antibody \naffinity prediction method, named DG-Affinity. DG-Affinity uses deep neural net-\nworks to efficiently and accurately predict the affinity between antibodies and anti-\ngens from sequences, without the need for structural information. The sequences \nof both the antigen and the antibody are first transformed into embedding vectors \nby two pre-trained language models, then these embeddings are concatenated \ninto an ConvNeXt framework with a regression task. The results demonstrate \nthe superiority of DG-Affinity over the existing structure-based prediction meth-\nods and the sequence-based tools, achieving a Pearson’s correlation of over 0.65 \non an independent test dataset.\nConclusions: Compared to the baseline methods, DG-Affinity achieves the best per-\nformance and can advance the development of antibody design. It is freely available \nas an easy-to-use web server at https:// www. digit algen eai. tech/ solut ion/ affin ity.\nKeywords: Affinity, Deep learning, Sequence embedding, Antibody–antigen \ninteraction\nOpen Access\n© The Author(s) 2023. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://\ncreativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdo-\nmain/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nRESEARCH\nYuan et al. BMC Bioinformatics          (2023) 24:430  \nhttps://doi.org/10.1186/s12859‑023‑05562‑z\nBMC Bioinformatics\n†Ye Yuan and Qushuo Chen are \nco-authors.\n*Correspondence:   \nyuanye_auto@sjtu.edu.cn; \n2008xypan@sjtu.edu.cn\n1 Institute of Image Processing \nand Pattern Recognition, \nShanghai Jiao Tong University, \nand Key Laboratory of System \nControl and Information \nProcessing, Ministry of Education \nof China, Shanghai 200240, China\n2 DigitalGene, Ltd, \nShanghai 200240, China\nPage 2 of 12Yuan et al. BMC Bioinformatics          (2023) 24:430 \n\"undruggable\" targets of diseases can be targeted. Antibodies can be used to treat vari -\nous cancers, as well as autoimmune diseases like rheumatoid arthritis, attracting huge \nresearch attention and development efforts [8–12]. Since the approval of the first mono -\nclonal antibody, antibodies have become popular drugs, occupying more than half of the \ntherapeutic market [13]. The latest application of monoclonal antibodies is the treatment \nof the 2019 coronavirus disease (COVID-19), since some patients may not be suitable for \nvaccination due to severe allergic reactions or inability to generate protective immune \nresponses from the vaccine. Recently, monoclonal antibodies against SARS-CoV-2, such \nas bebtelovimab, tixagevimab and cilgavima, have been approved by the FDA for the \ntreatment or pre-exposure prevention of COVID-19, demonstrating that monoclonal \nantibodies can be an effective complement to vaccines against COVID-19 [14–21].\nDetermining the affinity of antibody–antigen interactions is an important step in \nantibody development. Experimental methods for affinity determination include radio -\nimmunoassay (RIA), enzyme-linked immunosorbent assay (ELISA), surface plasmon \nresonance (SPR), and bio-layer interferometry (BLI) [22–26]. However, some of these \nexperimental methods are resource-intensive and time-consuming. Moreover, these \nexperimental methods are not suitable for large-scale high-throughput antibody screen -\ning [27]. Fortunately, extensive immunological databases from experiments have been \nestablished, generating a wealth of experimental affinity data for antigen and antibody \nstudies [28–33]. With the advancement of artificial intelligence technologies, especially, \ndeep learning performs better than traditional machine learning methods on large data -\nsets. For example, ConvNeXt outperforms the Swin-T model in multiple classification \nand recognition tasks. The model with ConvNeXt as the backbone has also achieved \ngood results in fields such as medical imaging and traditional Chinese medicine. It has \nbecome possible to build predictive models based on these collected data and deep \nlearning methods to predict antibody–antigen affinity [34–36] with high accuracy. For \nexample, PIPR is a sequence-based method and employs a residual RCNN [37] to pre -\ndict binding affinity using information from antigen–antibody pairs. It achieves good \ngeneralization performance on various tasks. The RCNN framework in PTPR adopts \na bidirectional gated recursive unit module (GRU), however, GRU has the drawbacks \nof slow learning efficiency and convergence speed [38]. Another model is the CSM-AB \nmodel [39], it first requires docking of antibody and antigen structures or utilizes known \ncomplex structures, and then obtains geometric information of the contact interface to \nestablish a predictive model using Extra Trees algorithm. Recently, the AREA-AFFIN -\nITY was developed to predict antibody–antigen binding affinity [40]. It built different \nmodels including linear model, neural network, random forest and mixed model. The \nmixed model yields the best performance than other compared methods. Similar to \nCSM-AB, the AREA-AFFINITY is also a structure-based model. However, the limitation \nlies in the requirement for antigen–antibody complex structure information, which is \ndifficult to acquire.\nIn this study, we propose a sequence-based method DG-Affinity for predicting antibody–\nantigen binding affinity. It is trained on a larger and more comprehensive dataset than \nCSM-AB, and only utilizes sequence information to predict the affinity between antibod-\nies and antigens. DG-Affinity combined two pre-trained embeddings (TAPE for antigen \nsequences and Ablang for antibody sequences) on an antibody–antigen interaction dataset, \nPage 3 of 12\nYuan et al. BMC Bioinformatics          (2023) 24:430 \n \nand used a ConvNeXt framework [41] to learn the relationship of antibody–antigen binding \naffinity. DG-Affinity outperforms other existing methods in an independent test dataset.\nMaterials and methods\nBenchmark datasets\nThe benchmark antigen–antibody data comes from two primary sources. One is the sdAb-\nDB database (http:// www. sdab- db. ca) [42], which is a freely available repository that col-\nlects antibody–antigen data. The other is the Round A data from the Baidu PaddlePaddle \n2021 Global Antibody Affinity Prediction Competition. In this study, we combine the two \ndatasets by removing the shared antibody–antigen interactions to construct the benchmark \ndataset. The benchmark dataset comprises 1,673 entries involving 448 distinct antibody–\nantigen complexes [43].\nWe divided the data into five equal parts. Four parts were used for training and one for \nvalidation, take turns to conduct five times of cross-validation. Then, we utilized an inde-\npendent test set to evaluate the model’s generalizability. This independent test set came \nfrom (https:// github. com/ pierc elab/ antib ody_ bench mark) [33], which contains structural \nfiles for 42 antigen–antibody complexes with affinity values not overlapping with the train-\ning data. We selected structures where both the antibody and the antigen are single chains, \ntotaling 26, and used the PDB module in Biopython to extract sequence information from \nthe complex structures as the independent test set [44].\nWe processed the data by treating each antibody–antigen pair and its corresponding \naffinity label separately. As shown in Fig.  1a, the value range of the original dissociation \nconstant (kd) value of the binding affinity is from − 2 to − 16, and was then transformed \nby taking the negative logarithm, as used in [45], and dividing by 10 for normalization as \nfollows:\nwhere ykd is the original value and 591.82 is from the study [45].\nAs shown in Fig.  1b, it can be clearly seen that the original kd value was successfully \nconverted to the range of 0–1, and only a small number of abnormal data values were \nmapped beyond 1.\nylabel = −(ykd ∗1000 )/591.282\nloge(10)/10\nFig. 1 Distribution of affinity values before and after data preprocessing. a Before preprocessing and b after \npreprocessing. The x-axis represents the data label value and the y-axis represents the numbers of data within \nthis value range\nPage 4 of 12Yuan et al. BMC Bioinformatics          (2023) 24:430 \nSequence embedding of antibody and antigen\nFor the antigen sequence embeddings, we used TAPE’s pre-trained model to obtain [46], \na protein language model, the embeddings. A protein language model is a type of lan -\nguage model designed for the protein sequences. It is trained on protein sequences and \nlearns underlying biochemical properties, secondary and tertiary structures, and intrin -\nsic functional patterns.\nTAPE uses bi-directional encoder from the Transformers model, and is trained on 31 \nmillion protein sequences from Pfam53 [47]. The model’s effectiveness was validated \nacross six downstream tasks including remote homology detection, contact prediction, \nand protein engineering tasks.\nConsidering that antibody is different from general proteins, for antibody embeddings, \nwe used AbLang for embeddings [48]. AbLang is an antibody-specific language model \ntrained on the Observed Antibody Space (OAS) database [49, 50], which contains about \n71.98 million sequence data (52.89 million unpaired heavy chains and 19.09 million \nunpaired light chains). It can be used for antibody residue repair, sequence-specific pre -\ndictions, and residue sequence embedding representations, and AbLang provides more \naccurate antibody representation than ESM-1b [51, 52]. Interestingly, AbLang requires \nseparate embeddings for the heavy and light chains of the antibody, as two AbLang mod-\nels were trained, one for the heavy chain and the other for the light chain. One potential \nreason for training separate models is that heavy and light chains have different compo -\nnents: the light chain has two such immunoglobulin domains, whereas the heavy chain \nof the antibody contains four.\nConvNeXt backbone\nThe ConvNeXt network is composed purely of convolutional layers and inspired by the \narchitecture of vision transformer and ResNet [53–55]. ConvNeXt mainly improves the \nmodel performance in the following aspects: (1) Macro design (2) ResNeXt (3) Reverse \nbottleneck (4) Large kernel size (5) Various layered micro designs. Overall, the Con -\nvNeXt network has four stages, with a block stacking ratio of 1:1:3:1 for each stage, and a \nconvolutional layer with the same kernel size of 4 and a step size as the Swin-T network. \nThe ConvNeXt network has also designed an anti-bottleneck structure based on “fine \nend coarse medium” as a reference, replacing RELU with more commonly used GELU \nactivation function, resulting in a slight improvement in model performance. ConvNeXt \nnot only reduces the use of regularization functions, but also replaces Batch Norm with \nLayer Norm. These two operations slightly improve the accuracy of the model. In this \nstudy, we removed the final MLP layer of the original ConvNeXt backbone as one of the \nmodules for DG-Affinity.\nArchitecture of DG‑Affinity\nOur DG-Affinity’s architecture is shown in Figs.  2 and 3, three parallel ConvNeXt \nbackbones accept three different features obtained from TAPE or Ablang feature \nextractors, which are antibody features, antigen features, and antibody–antigen \nconcatenated features. After passing through the ConvNeXt backbone for feature \nPage 5 of 12\nYuan et al. BMC Bioinformatics          (2023) 24:430 \n \nrepresentation learning, the antibody representation and antigen representation are \nmultiplied element-wise and concatenated with the antibody–antigen feature repre -\nsentation learned through the ConvNeXt backbone. Finally, the new representation \nis passed through two layers of MLP to predict the affinity value, among them, each \nlayer of MLP is composed of a linear function followed by relu or sigmoid activate \nfunction. In this study, we used the open-source code to construct an ConvNeXt \nbackbone (https:// github. com/ faceb ookre search/ ConvN eXt). Considering the capac-\nity and performance of the model, we chose the tiny ConvNeXt version. The neu -\nral network was built and trained using the Pytorch library [56], and we trained the \nmodel using 50 epochs with a learning rate of 0.000001, using the ADAM optimizer \n[57]. The network output is a regression value prediction of the affinity.\nFig. 2 The workflow of DG-Affinity. Among them, Ab represents antibodies and Ag represents antigens. \nThe antibody and antigen sequences are respectively fed into the corresponding embedding extractors, \nand three features are obtained: Ab features, Ag features, and Ab-ag features. Then, these three features are \nconcatenated into the MLP to predict binding affinity values\nFig. 3 Network architecture of DG-Affinity, “cat” symbol represents concatenation, “ × ” symbol represents \nelement-wise multiplication, “Ag feature” is antigen feature, “Ab feature” is antibody feature, “Ab–ag feature” is \nconcatenated feature of antigen feature and antibody feature and MLP is a single linear layer\nPage 6 of 12Yuan et al. BMC Bioinformatics          (2023) 24:430 \nPerformance metrics\nOur model’s predictive ability was measured using the Pearson correlation coefficient \n(R), R-squared (R2), Root Mean Square Deviation (RMSD) and Mean Absolute Error \n(MAE). To better evaluate our model, we also tested other well-known structure-based \nantigen–antibody affinity prediction model (e.g. CSM-AB and AREA-AFFINITY). We \ninput the 26 complex structures from the independent set into their online prediction \nwebsite and manually calculated the R, R2, RMSD and MAE values based on the true \nand predicted binding affinity values for each method, respectively.\nBaseline methods\nCSM-AB It is the first scoring function specifically designed for antibody antigen dock -\ning and binding affinity prediction. By adjusting the graph-based structure, this method \ncan capture close-contact features and surrounding structural information related to \nantibody antigen binding.\nAREA-AFFINITY  It integrates 60 effective area-based protein–protein affinity predic -\ntion models and 37 effective area models for antibody protein antigen binding affinity \nprediction.\nLISA It is an empirical affinity function based on the atomic-atomic contact model and \na radial function based on the density functional theory.\nCIPS It is a new pair potential combining interface composition with residue–residue \ncontact preference.\nProdigy It is based on the counting of atom–atom contacts at the interface and on the \ncharge distribution at the non-interacting surface of the complex.\nNIS It considers distinguished physico-chemical properties of residues lying on the \ncomplex surface.\nCCharPPI It integrates over a hundred tools into a single web server, including elec -\ntrostatic, solvent removal, and hydrogen bonding models, as well as interface filling and \ncomplementarity scores, empirical potentials of various resolutions, docking potentials, \nand composite scoring functions.\nWe follow the comparison protocol of CSM-AB and LISA [58]. In the experiment, \nLISA, CIPS [59], NIS [60] and PRODIGY [61] were standalone scripts, AREA-AFFIN -\nITY [40] and CSM-AB were one-line webserver, other models or tools (e. g. FIREDOCK) \nwere calculated using CCharPPI webserver based on physical potentials and composite \ndescriptors [62]. We collected structure data from SAbDab database and Protein Data \nBank since most of the existing comparative models are based on structure [63, 64].\nResults\nComparison DG‑Affinity with other baseline methods\nTo demonstrate the advantages of DG-Affinity, we evaluated other methods on the \ntraining set, the results in Fig.  4a and Additional file  1: Table S1 show that DG-Affinity \nsignificantly outperformed all of them. At the same time, we also evaluate the stability \nand consistency of our model’s performance using tenfold cross validation, 15-fold cross \nvalidation, and 20-fold cross validation. The results are stable and consistent (shown \nin Additional file  1: Figure S1), showing that there is no significant sampling bias for \nPage 7 of 12\nYuan et al. BMC Bioinformatics          (2023) 24:430 \n \nDG-Affinity. After manually adjusting the model parameters to the best performance on \nthe validation set, we compared it with baseline methods on the independent test set. \nAs shown in Fig.  4b and Additional file  1: Table S2, DG-Affinity achieves the best R of \nover 0.6556 in the independent set. Most of the baseline models yield an R from 0.3 to \n0.5, which is much lower than that of DG-Affinity. Moreover, two models have nega -\ntive Pearson’s correlation, such as CP_PIE (-0.3332) and AREA-AFFINITY (-0.2019). In \naddition, our method outperforms other methods in all the metrics (Additional file  1: \nTable S2).\nComparing the effectiveness of different architectures in DG‑Affinity\nDG-Affinity uses ConvNeXt as the backbone network, to demonstrate the effective -\nness, we compare it with the other 12 widely used backbone networks, i.e. convolutional \nnetwork, transformer, and several classic models. The source codes for these architec -\ntures are downloaded from the corresponding github repository (https:// github. com/ \nweiai cunzai/ pytor ch- cifar 100), and slightly modified to remove the final MLP layer and \nreplace it with the global mean pooling. These backbones replace all ConvNeXt mod -\nules in DG-Affinity, and the replaced model structure is trained and validated using \nthe aforementioned training set and independent test set. For detailed hyperparameter \ninformation, please refer to Additional file  1: Table S3 in the supporting material. The \nresults are shown in Fig.  5 and Additional file  1: Table S4, S5. Of the 13 network back -\nbones, the ConvNeXt achieves relatively better performance than other backbone net -\nworks in the fivefold cross validation and in the independent test dataset.\nExploration of model ablation studies\nIn this section, we investigated the contribution of each module in DG-Affinity. During the \nexperiment, we maintain the parameter consistency. After respectively removing the Con-\nvNeXt module for learning antigen features, antibody features, and concatenated features \nantibody and antigen embeddings, we concatenate the output representation vectors of \nthese remaining ConvNeXt modules and input them into the MLP layer to make regres -\nsion. As shown in the Fig.  6, it is evident that the lack of the module for learning antigen \nfeatures has a significant impact on the performance of DG-Affinity. The results show that \nantigen information may be more closely related to the interactions than antibodies. We \nFig. 4 performance distribution of the top 8 models on the independent set (a) and the training set (b), \nrespectively\nPage 8 of 12Yuan et al. BMC Bioinformatics          (2023) 24:430 \nhave also found that when using full feature, the performance on the independent data -\nsets in Fig. 5 is higher than that of five-fold. One potential reason is that the independent \ndatasets are not large enough and the samples are imbalanced, a more number of features \npotentially introduce better generalizability, resulting in a better performance on the inde-\npendent test set.\nFig. 5 Performance of different network backbones in DG-Affinity on the training set (five-fold) and \nindependent set\nFig. 6 The ablation experiments of our model on the training set (five-fold) and independent set. The results \nof model in training and independent sets are represented by purple and gray bars\nPage 9 of 12\nYuan et al. BMC Bioinformatics          (2023) 24:430 \n \nDiscussion\nThe development of bioengineering has led the development of antibody drugs, \ndemonstrating good therapeutic effects in the treatment of cancer and autoimmune \ndiseases. The key step in this development process is to obtain the affinity between \nantibodies and their binding antigens. However, the limitation of current methods lies \nin the requirement for structural information of antigen antibody complexes, which is \ndifficult to obtain. To address the above issues, in this study, we propose a deep learn -\ning based model for predicting antigen–antibody binding affinity with pre-trained \nemebddings from sequences, our model has achieved promising performance due \nto the followings: (1) the training dataset is larger, while traditional structure-based \nmethods have much smaller structure data set than sequences, making them prone \nto overfitting and low generalization performance. (2) The ConvNeXt framework has \nrecently been widely used in various fields and has been proven to achieve good pre -\ndiction results. (3) The pre-trained emeddings on the large unlabeled sequences for \nproteins and antibody.\nConclusion\nIn this study, we propsoed a new sequence-based antigen–antibody binding affinity \nprediction method, named DG-Affinity, based on protein and antibody language mod -\nels. Antigen and antibody sequences are first transformed into the embedding vectors \nthrough two pre-trained methods (TAPE and Ablang), then a ConvNeXt-backbone \nbased network is used to learn the affinity relationship between antigen and antibody. \nThe results on benchmark datasets indicate that DG-Affinity outperforms existing meth-\nods, including the popular structure-based antigen antibody affinity prediction methods \nas well as the traditional tools, for both the fivefold and independent validation, achiev -\ning a Pearson correlation coefficient of over 0.65 on the independent test datasets. In \naddition, we developed an easy-to-use website version of DG-Affinity. It can be expected \nthat our method DG-Affinity will advance the progress and development of antibody \ndrug.\nAbbreviations\nCDRs  Complementarity-determining regions\nMAbs  Monoclonal antibodies\nADCs  Antibody–drug conjugates\nRIA  Adioimmunoassay\nELISA  Enzyme-linked immunosorbent assay\nSPR  Surface Plasmon resonance\nBLI  Bio-layer interferometry\nGRUs  Bidirectional gated recursive unit module\nPIPR  Protein–Protein Interaction Prediction Based on Siamese Residual RCNN\nOAS  Observed Antibody Space\nESM  Evolutionary Scale Modeling\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. org/ 10. 1186/ s12859- 023- 05562-z.\nAdditional file 1: Table S1. Performance comparation between DG-Affinity and other models on training set. \nTable S2. Performance comparation between DG-Affinity and other models on independent set. Table S3. Parame-\nter settings for each model backbone on DG Affinity. Table S4. Performance of DG-Affinity with different backbones \non 5-fold cross validation. Table S5. Performance of DG-Affinity with different backbones on independent datasets. \nPage 10 of 12Yuan et al. BMC Bioinformatics          (2023) 24:430 \nFig. S1. Performance distribution of DG-Affinity per fold under different validation schemes including, 5-fold, 10-fold \nand 20-fold cross validation, Prove the consistency and robustness of this method.\nAcknowledgements\nWe thank Haotian Teng for support on the online webserver.\nAuthor contributions\nXP and YY designed the project, QC and JM implemented the methods and analyzed the data. XP , YY, QC and JM wrote \nthe original manuscript. XP , YY, GL revised the manuscript, all authors approved the manuscript.\nFunding\nThis work was supported by the National Natural Science Foundation of China (No. 62103262), the Science and Technol-\nogy Commission of Shanghai Municipality (20S11902100) and the Shanghai Pujiang Programme (no. 21PJ1407700).\nAvailability of data and materials\nThe website is provided at https:// www. digit algen eai. tech/ solut ion/ affin ity, this study does not include any human \ninvolvement or human data which is not publicly available.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 21 September 2023   Accepted: 6 November 2023\nReferences\n 1. Oostindie SC, Lazar GA, Schuurman J, Parren PWHI. Avidity in antibody effector functions and biotherapeutic drug \ndesign. Nat Rev Drug Discov. 2022;21(10):715–35.\n 2. Hviid L, Lopez-Perez M, Larsen MD, Vidarsson G. No sweet deal: the antibody-mediated immune response to \nmalaria. Trends Parasitol. 2022;38(6):428–34.\n 3. Rascio F, Pontrelli P , Netti GS, Manno E, Infante B, Simone S, Castellano G, Ranieri E, Seveso M, Cozzi E, Gesualdo L, \nStallone G, Grandaliano G. IgE-mediated immune response and antibody-mediated rejection. Clin J Am Soc Neph-\nrol. 2020;15(10):1474–83.\n 4. Kapingidza AB, Kowal K, Chruszcz M. Antigen–antibody complexes. Subcell Biochem. 2020;94:465–97.\n 5. Bayer V. An overview of monoclonal antibodies. Semin Oncol Nurs. 2019;35(5):150927.\n 6. Posner J, Barrington P , Brier T, Datta-Mannan A. Monoclonal antibodies: past, present and future. Handb Exp Pharma-\ncol. 2019;260:81–141.\n 7. Castelli MS, McGonigle P , Hornby PJ. The pharmacology and therapeutic applications of monoclonal antibodies. \nPharmacol Res Perspect. 2019;7(6):e00535.\n 8. Le Basle Y, Chennell P , Tokhadze N, Astier A, Sautou V. Physicochemical stability of monoclonal antibodies: a review. J \nPharm Sci. 2020;109(1):169–90.\n 9. Hafeez U, Parakh S, Gan HK, Scott AM. Antibody-drug conjugates for cancer therapy. Molecules. 2020;25(20):4764.\n 10. Ponziani S, Di Vittorio G, Pitari G, Cimini AM, Ardini M, Gentile R, Iacobelli S, Sala G, Capone E, Flavell DJ, Ippoliti R, \nGiansanti F. Antibody-drug conjugates: the new frontier of chemotherapy. Int J Mol Sci. 2020;21(15):5510.\n 11. Baah S, Laws M, Rahman KM. Antibody-drug conjugates-a tutorial review. Molecules. 2021;26(10):2943.\n 12. Jin Y, Schladetsch MA, Huang X, Balunas MJ, Wiemer AJ. Stepping forward in antibody-drug conjugate develop-\nment. Pharmacol Ther. 2022;229:107917.\n 13. Lu RM, Hwang YC, Liu IJ, Lee CC, Tsai HZ, Li HJ, Wu HC. Development of therapeutic antibodies for the treatment of \ndiseases. J Biomed Sci. 2020;27(1):1.\n 14. Corti D, Purcell LA, Snell G, Veesler D. Tackling COVID-19 with neutralizing monoclonal antibodies. Cell. \n2021;184(12):3086–108.\n 15. Taylor PC, Adams AC, Hufford MM, de la Torre I, Winthrop K, Gottlieb RL. Neutralizing monoclonal antibodies for \ntreatment of COVID-19. Nat Rev Immunol. 2021;21(6):382–93.\n 16. Hwang YC, Lu RM, Su SC, Chiang PY, Ko SH, Ke FY, Liang KH, Hsieh TY, Wu HC. Monoclonal antibodies for COVID-19 \ntherapy and SARS-CoV-2 detection. J Biomed Sci. 2022;29(1):1.\n 17. Bakkari MA, Moni SS, Sultan MH, Madkhali OA. Monoclonal antibodies and their target specificity against SARS-\nCoV-2 Infections: perspectives and challenges. Recent Pat Biotechnol. 2022;16(1):64–78.\n 18. Cruz-Teran C, Tiruthani K, McSweeney M, Ma A, Pickles R, Lai SK. Challenges and opportunities for antiviral monoclo-\nnal antibodies as COVID-19 therapy. Adv Drug Deliv Rev. 2021;169:100–17.\nPage 11 of 12\nYuan et al. BMC Bioinformatics          (2023) 24:430 \n \n 19. Tabll AA, Shahein YE, Omran MM, Elnakib MM, Ragheb AA, Amer KE. A review of monoclonal antibodies in COVID-\n19: Role in immunotherapy, vaccine development and viral detection. Hum Antib. 2021;29(3):179–91.\n 20. Asdaq SMB, Rabbani SI, Alkahtani M, Aldohyan MM, Alabdulsalam AM, Alshammari MS, Alajlan SA, Binrokan A, \nMohzari Y, Alrashed A, Alshammari MK, Imran M, Nayeem N. A patent review on the therapeutic application of \nmonoclonal antibodies in COVID-19. Int J Mol Sci. 2021;22(21):11953.\n 21. Focosi D, McConnell S, Casadevall A, Cappello E, Valdiserra G, Tuccori M. Monoclonal antibody therapies against \nSARS-CoV-2. Lancet Infect Dis. 2022;22(11):e311–26. https:// doi. org/ 10. 1016/ S1473- 3099(22) 00311-5. (Erratum in: \nLancet Infect Dis. 2022;22(9): e239).\n 22. Liao J, Madahar V, Dang R, Jiang L. Quantitative FRET (qFRET) technology for the determination of protein–protein \ninteraction affinity in solution. Molecules. 2021;26(21):6339.\n 23. Tabatabaei MS, Ahmed M. Enzyme-linked immunosorbent assay (ELISA). Methods Mol Biol. 2022;2508:115–34.\n 24. Sparks RP , Jenkins JL, Fratti R. Use of surface plasmon resonance (SPR) to determine binding affinities and kinetic \nparameters between components important in fusion machinery. Methods Mol Biol. 2019;1860:199–210.\n 25. Rhea K. Determining the binding kinetics of peptide macrocycles using bio-layer interferometry (BLI). Methods Mol \nBiol. 2022;2371:355–72.\n 26. Mir DA, Mehraj U, Qayoom H, et al. Radioimmunoassay (RIA) (2020).\n 27. Guo Z, Yamaguchi R. Machine learning methods for protein-protein binding affinity prediction in protein design. \nFront Bioinform. 2022;2:1065703.\n 28. Wang R, Fang X, Lu Y, Wang S. The PDBbind database: collection of binding affinities for protein-ligand complexes \nwith known three-dimensional structures. J Med Chem. 2004;47(12):2977–80.\n 29. Kastritis PL, Moal IH, Hwang H, Weng Z, Bates PA, Bonvin AM, Janin J. A structure-based benchmark for protein-\nprotein binding affinity. Protein Sci. 2011;20(3):482–91.\n 30. Moal IH, Fernández-Recio J. SKEMPI: a structural kinetic and energetic database of mutant protein interactions and \nits use in empirical models. Bioinformatics. 2012;28(20):2600–7.\n 31. Jankauskaite J, Jiménez-García B, Dapkunas J, Fernández-Recio J, Moal IH. SKEMPI 2.0: an updated benchmark \nof changes in protein-protein binding energy, kinetics and thermodynamics upon mutation. Bioinformatics. \n2019;35(3):462–9.\n 32. Sirin S, Apgar JR, Bennett EM, Keating AE. AB-Bind: antibody binding mutational database for computational affinity \npredictions. Protein Sci. 2016;25(2):393–409.\n 33. Guest JD, Vreven T, Zhou J, Moal I, Jeliazkov JR, Gray JJ, Weng Z, Pierce BG. An expanded benchmark for anti-\nbody–antigen docking and affinity prediction reveals insights into antibody recognition determinants. Structure. \n2021;29(6):606-621.e5.\n 34. Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, Tunyasuvunakool K, Bates R, Žídek A, Potapenko \nA, Bridgland A, Meyer C, Kohl SAA, Ballard AJ, Cowie A, Romera-Paredes B, Nikolov S, Jain R, Adler J, Back T, Petersen \nS, Reiman D, Clancy E, Zielinski M, Steinegger M, Pacholska M, Berghammer T, Bodenstein S, Silver D, Vinyals O, \nSenior AW, Kavukcuoglu K, Kohli P , Hassabis D. Highly accurate protein structure prediction with AlphaFold. Nature. \n2021;596(7873):583–9.\n 35. Ruffolo JA, Chu LS, Mahajan SP , Gray JJ. Fast, accurate antibody structure prediction from deep learning on massive \nset of natural antibodies. Nat Commun. 2023;14(1):2389.\n 36. Abanades B, Georges G, Bujotzek A, Deane CM. ABlooper: fast accurate antibody CDR loop structure prediction with \naccuracy estimation. Bioinformatics. 2022;38(7):1877–80.\n 37. Chen M, Ju CJ, Zhou G, Chen X, Zhang T, Chang KW, Zaniolo C, Wang W. Multifaceted protein-protein interaction \nprediction based on Siamese residual RCNN. Bioinformatics. 2019;35(14):i305–14.\n 38. Lee M. Recent advances in deep learning for protein–protein interaction analysis: a comprehensive review. Mol-\necules. 2023;28(13):5169.\n 39. Myung Y, Pires DEV, Ascher DB. CSM-AB: graph-based antibody–antigen binding affinity prediction and docking \nscoring function. Bioinformatics. 2022;38(4):1141–3.\n 40. Yang YX, Huang JY, Wang P , Zhu BT. AREA-AFFINITY: a web server for machine learning-based prediction of protein–\nprotein and antibody-protein antigen binding affinities. J Chem Inf Model. 2023;63(11):3230–7.\n 41. Liu Z, Mao H, Wu CY, Feichtenhofer C, Darrell T, Xie S. A convnet for the 2020s. arXiv e-prints.\n 42. Wilton EE, et al. sdAb-DB: the single domain antibody database. ACS Synth Biol. 2018;7:2480–4.\n 43. Wilton EE, Opyr MP , Kailasam S, Kothe RF, Wieden HJ. sdAb-DB: the single domain antibody database. ACS Synth Biol. \n2018;7(11):2480–4.\n 44. Cock PJ, Antao T, Chang JT, Chapman BA, Cox CJ, Dalke A, Friedberg I, Hamelryck T, Kauff F, Wilczynski B, de Hoon MJ. \nBiopython: freely available Python tools for computational molecular biology and bioinformatics. Bioinformatics. \n2009;25(11):1422–3.\n 45. Cruz VL, Souza-Egipsy V, Gion M, Perez-Garcia J, Cortes J, Ramos J, Vega JF. Binding affinity of trastuzumab and pertu-\nzumab monoclonal antibodies to extracellular HER2 domain. Int J Mol Sci. 2023;24:12031.\n 46. Rao R, Bhattacharya N, Thomas N, Duan Y, Chen X, Canny J, Abbeel P , Song YS. Evaluating protein transfer learning \nwith TAPE. Adv Neural Inf Process Syst. 2019;32:9689–701.\n 47. El-Gebali S, Mistry J, Bateman A, Eddy SR, Luciani A, Potter SC, Qureshi M, Richardson LJ, Salazar GA, Smart A, Sonn-\nhammer ELL, Hirsh L, Paladin L, Piovesan D, Tosatto SCE, Finn RD. The Pfam protein families database in 2019. Nucleic \nAcids Res. 2019;47(D1):D427–32.\n 48. Olsen TH, Moal IH, Deane CM. AbLang: an antibody language model for completing antibody sequences. Bioinform \nAdv. 2022;2(1):046.\n 49. Kovaltsuk A, Leem J, Kelm S, Snowden J, Deane CM, Krawczyk K. Observed antibody space: a resource for data min-\ning next-generation sequencing of antibody repertoires. J Immunol. 2018;201(8):2502–9.\n 50. Olsen TH, Boyles F, Deane CM. Observed antibody space: a diverse database of cleaned, annotated, and translated \nunpaired and paired antibody sequences. Protein Sci. 2022;31(1):141–6.\nPage 12 of 12Yuan et al. BMC Bioinformatics          (2023) 24:430 \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 51. Rives A, Meier J, Sercu T, Goyal S, Lin Z, Liu J, Guo D, Ott M, Zitnick CL, Ma J, Fergus R. Biological structure and \nfunction emerge from scaling unsupervised learning to 250 million protein sequences. Proc Natl Acad Sci USA. \n2021;118(15):e2016239118.\n 52. Rao R, Meier J, Sercu T, et al. Transformer protein language models are unsupervised structure learners. Biorxiv. \n2020;15:422761.\n 53. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai XH, Unterthiner T et al. An image is worth 16×16 words: \ntransformers for image recognition at scale. In: Proceedings of the 9th international conference on learning repre-\nsentations, OpenReview.net, Vienna, 2021; 3–7\n 54. Han K, Wang Y, Chen H, Chen X, Guo J, Liu Z, Tang Y, Xiao A, Xu C, Xu Y, Yang Z, Zhang Y, Tao D. A survey on vision \ntransformer. IEEE Trans Pattern Anal Mach Intell. 2023;45(1):87–110.\n 55. He K, Zhang X, Ren S, et al. Deep residual learning for image recognition. IEEE; 2016.\n 56. Paszke A, Gross S, Massa F, et al. PyTorch: an imperative style, high-performance deep learning library; 2019.\n 57. Kingma DP , Ba J. Adam: a method for stochastic optimization. Available from: http:// arxiv. org/ abs/ 1412. 6980\n 58. Raucci R, Laine E, Carbone A. Local interaction signal analysis predicts protein-protein binding affinity. Structure. \n2018;26(6):905-915.e4.\n 59. Nadalin F, Carbone A. Protein-protein interaction specificity is captured by contact preferences and interface com-\nposition. Bioinformatics. 2018;34(3):459–68.\n 60. Vangone A, Bonvin AM. Contacts-based prediction of binding affinity in protein-protein complexes. Elife. \n2015;4:e07454.\n 61. Xue LC, Rodrigues JP , Kastritis PL, Bonvin AM, Vangone A. PRODIGY: a web server for predicting the binding affinity \nof protein-protein complexes. Bioinformatics. 2016;32(23):3676–8.\n 62. Moal IH, Jiménez-García B, Fernández-Recio J. CCharPPI web server: computational characterization of protein-\nprotein interactions from structure. Bioinformatics. 2015;31(1):123–5.\n 63. Schneider C, Raybould MIJ, Deane CM. SAbDab in the age of biotherapeutics: updates including SAbDab-nano, the \nnanobody structure tracker. Nucleic Acids Res. 2022;50(D1):D1368–72.\n 64. Berman HM, Westbrook J, Feng Z, et al. The protein data bank. Nucleic Acids Res. 2000;28(1):235–42.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Antigen",
  "concepts": [
    {
      "name": "Antigen",
      "score": 0.6724070906639099
    },
    {
      "name": "Antibody",
      "score": 0.6563107371330261
    },
    {
      "name": "Affinity maturation",
      "score": 0.6194719672203064
    },
    {
      "name": "Computational biology",
      "score": 0.555841863155365
    },
    {
      "name": "Computer science",
      "score": 0.5137773156166077
    },
    {
      "name": "Immune system",
      "score": 0.43101435899734497
    },
    {
      "name": "DNA microarray",
      "score": 0.4221054017543793
    },
    {
      "name": "Immunology",
      "score": 0.3171190917491913
    },
    {
      "name": "Biology",
      "score": 0.31285160779953003
    },
    {
      "name": "Gene",
      "score": 0.17885950207710266
    },
    {
      "name": "Genetics",
      "score": 0.12102201581001282
    },
    {
      "name": "Gene expression",
      "score": 0.11569976806640625
    }
  ]
}