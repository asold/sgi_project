{
  "title": "Glancing Transformer for Non-Autoregressive Neural Machine Translation",
  "url": "https://openalex.org/W3175665465",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2146690738",
      "name": "Lihua Qian",
      "affiliations": [
        null,
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2103794919",
      "name": "Hao Zhou",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2113361811",
      "name": "Yu Bao",
      "affiliations": [
        "Nanjing University"
      ]
    },
    {
      "id": "https://openalex.org/A2132499817",
      "name": "Mingxuan Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2033443608",
      "name": "Lin Qiu",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2119756314",
      "name": "Weinan Zhang",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2104644608",
      "name": "Yu Yong",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2098784551",
      "name": "Lei Li",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2296073425",
    "https://openalex.org/W2342204193",
    "https://openalex.org/W2946375144",
    "https://openalex.org/W3035416964",
    "https://openalex.org/W2963536265",
    "https://openalex.org/W2963434219",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W3114869109",
    "https://openalex.org/W1647671624",
    "https://openalex.org/W3039805635",
    "https://openalex.org/W3035289598",
    "https://openalex.org/W3034892578",
    "https://openalex.org/W2962969034",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3000840023",
    "https://openalex.org/W3015162217",
    "https://openalex.org/W2988975212",
    "https://openalex.org/W2970832665",
    "https://openalex.org/W3030343076",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2996843693",
    "https://openalex.org/W2988536374",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2981648103",
    "https://openalex.org/W2962915948",
    "https://openalex.org/W648786980",
    "https://openalex.org/W2990372437",
    "https://openalex.org/W2892213699",
    "https://openalex.org/W2952649152",
    "https://openalex.org/W3004979489",
    "https://openalex.org/W3100753857",
    "https://openalex.org/W3129170862",
    "https://openalex.org/W2990389671",
    "https://openalex.org/W2990004017",
    "https://openalex.org/W2996987694",
    "https://openalex.org/W2907945666",
    "https://openalex.org/W2966746916",
    "https://openalex.org/W3167880383",
    "https://openalex.org/W3175164646",
    "https://openalex.org/W3101012756",
    "https://openalex.org/W2971167892"
  ],
  "abstract": "Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, Lei Li. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 1993–2003\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1993\nGlancing Transformer for Non-Autoregressive\nNeural Machine Translation\nLihua Qian1,2∗ Hao Zhou2 Yu Bao3 Mingxuan Wang2\nLin Qiu1 Weinan Zhang1 Yong Yu1 Lei Li2\n1 Shanghai Jiao Tong University 2 ByteDance AI Lab 3 Nanjing University\n{qianlihua,lqiu,wnzhang,yyu}@apex.sjtu.edu.cn\n{zhouhao.nlp,wangmingxuan.89,lileilab}@bytedance.com\nbaoy@smail.nju.edu.cn\nAbstract\nRecent work on non-autoregressive neural ma-\nchine translation (NAT) aims at improving the\nefﬁciency by parallel decoding without sac-\nriﬁcing the quality. However, existing NAT\nmethods are either inferior to Transformer or\nrequire multiple decoding passes, leading to\nreduced speedup. We propose the Glancing\nLanguage Model (GLM) for single-pass par-\nallel generation models. With GLM, we de-\nvelop Glancing Transformer (GLAT) for ma-\nchine translation. With only single-pass par-\nallel decoding, GLAT is able to generate\nhigh-quality translation with 8×-15×speedup.\nNote that GLAT does not modify the net-\nwork architecture, which is a training method\nto learn word interdependency. Experiments\non multiple WMT language directions show\nthat GLAT outperforms all previous single\npass non-autoregressive methods, and is nearly\ncomparable to Transformer, reducing the gap\nto 0.25-0.9 BLEU points.\n1 Introduction\nTransformer has been the most widely used ar-\nchitecture for machine translation (Vaswani et al.,\n2017). Despite its strong performance, the decod-\ning of Transformer is inefﬁcient as it adopts the\nsequential auto-regressive factorization for its prob-\nability model (Figure 1a). Recent work such as\nthe non-autoregressive transformer (NAT), aims to\ndecode target tokens in parallel to speed up the gen-\neration (Gu et al., 2018). However, the vanilla NAT\nstill lags behind the Transformer in translation qual-\nity – with a gap of about 7.0 BLEU points. NAT\nassumes the conditional independence of the target\ntokens given the source sentence. We suspect that\nNAT’s conditional independence assumption pre-\nvents learning word interdependency in the target\n∗The work was done when the ﬁrst author was an intern at\nBytedance.\n̂y1\n̂y2\n̂y4\n̂y5\n̂y3\nh1\nh2\nh3\nh4\nh5\nNAT DecodingH H′\u0000 Glancing Sampling\nHamming \nDistance\nN( ̂Y, Y) = 3\ny1\ny2\ny3\ny4\ny5\n̂y1\n̂y2\n̂y4\n̂y5\n̂y3\ny1\ny3\ny5\nReplace \nInputs\n0.8\n0.5\n0.7\n0.6\n0.9\nh2\nh4\ny1 y2\ny1 y2 y5\nDecoder\nEncoder\nx1 x2 x3 x4\ny3 y4\ny3 y4\n[BOS]\ny1 y2 y5\nDecoder\nEncoder\nx1 x2 x3 x4\ny3 y4\nh2 h4h1 h3 h5\n attention\nRandom \nMasking\nDecoder\nEncoder\nx1 x2 x3 x4\ny1 y4 y5[MASK] [MASK]\ny2 y3y1 y5y4\n attention\nan  apple  in    the   car\nein    Apfel    im     Auto \nDecoder\nEncoder\nx1 x2 x3 x4\ny1 h2 y5h4y3\ny2 y4\nGlancing \nSampling\ny3y1 y5\nein    Apfel    im     Auto \nan  apple  in    the   car\n attention\nein    Apfel    im     Auto ein    Apfel    im     Auto \n      apple  in         apple         the  \nan                    the   car\nan  apple  in    the\nan             in            car\n attention\n(a) Sequential LM\n̂y1\n̂y2\n̂y4\n̂y5\n̂y3\nh1\nh2\nh3\nh4\nh5\nNAT DecodingH H′\u0000 Glancing Sampling\nHamming \nDistance\nN( ̂Y, Y) = 3\ny1\ny2\ny3\ny4\ny5\n̂y1\n̂y2\n̂y4\n̂y5\n̂y3\ny1\ny3\ny5\nReplace \nInputs\n0.8\n0.5\n0.7\n0.6\n0.9\nh2\nh4\ny1 y2\ny1 y2 y5\nDecoder\nEncoder\nx1 x2 x3 x4\ny3 y4\ny3 y4\n[BOS]\ny1 y2 y5\nDecoder\nEncoder\nx1 x2 x3 x4\ny3 y4\nh2 h4h1 h3 h5\n attention\nRandom \nMasking\nDecoder\nEncoder\nx1 x2 x3 x4\ny1 y4 y5[MASK] [MASK]\ny2 y3y1 y5y4\n attention\nan  apple  in    the   car\nein    Apfel    im     Auto \nDecoder\nEncoder\nx1 x2 x3 x4\ny1 h2 y5h4y3\ny2 y4\nGlancing \nSampling\ny3y1 y5\nein    Apfel    im     Auto \nan  apple  in    the   car\n attention\nein    Apfel    im     Auto ein    Apfel    im     Auto \n      apple  in         apple         the  \nan                    the   car\nan  apple  in    the\nan             in            car\n attention (b) Cond. Independent LM\n̂y1\n̂y2\n̂y4\n̂y5\n̂y3\nh1\nh2\nh3\nh4\nh5\nNAT DecodingH H′\u0000 Glancing Sampling\nHamming \nDistance\nN( ̂Y, Y) = 3\ny1\ny2\ny3\ny4\ny5\n̂y1\n̂y2\n̂y4\n̂y5\n̂y3\ny1\ny3\ny5\nReplace \nInputs\n0.8\n0.5\n0.7\n0.6\n0.9\nh2\nh4\ny1 y2\ny1 y2 y5\nDecoder\nEncoder\nx1 x2 x3 x4\ny3 y4\ny3 y4\n[BOS]\ny1 y2 y5\nDecoder\nEncoder\nx1 x2 x3 x4\ny3 y4\nh2 h4h1 h3 h5\n attention\nRandom \nMasking\nDecoder\nEncoder\nx1 x2 x3 x4\ny1 y4 y5[MASK] [MASK]\ny2 y3y1 y5y4\n attention\nan  apple  in    the   car\nein    Apfel    im     Auto \nDecoder\nEncoder\nx1 x2 x3 x4\ny1 h2 y5h4y3\ny2 y4\nGlancing \nSampling\ny3y1 y5\nein    Apfel    im     Auto \nan  apple  in    the   car\n attention\nein    Apfel    im     Auto ein    Apfel    im     Auto \n      apple  in         apple         the  \nan                    the   car\nan  apple  in    the\nan             in            car\n attention\n(c) Masked LM (MLM)\n̂y1\n̂y2\n̂y4\n̂y5\n̂y3\nh1\nh2\nh3\nh4\nh5\nNAT DecodingH H′\u0000 Glancing Sampling\nHamming \nDistance\nN( ̂Y, Y) = 3\ny1\ny2\ny3\ny4\ny5\n̂y1\n̂y2\n̂y4\n̂y5\n̂y3\ny1\ny3\ny5\nReplace \nInputs\n0.8\n0.5\n0.7\n0.6\n0.9\nh2\nh4\ny1 y2\ny1 y2 y5\nDecoder\nEncoder\nx1 x2 x3 x4\ny3 y4\ny3 y4\n[BOS]\ny1 y2 y5\nDecoder\nEncoder\nx1 x2 x3 x4\ny3 y4\nh2 h4h1 h3 h5\n attention\nRandom \nMasking\nDecoder\nEncoder\nx1 x2 x3 x4\ny1 y4 y5[MASK] [MASK]\ny2 y3y1 y5y4\n attention\nan  apple  in    the   car\nein    Apfel    im     Auto \nDecoder\nEncoder\nx1 x2 x3 x4\ny1 h2 y5h4y3\ny2 y4\nGlancing \nSampling\ny3y1 y5\nein    Apfel    im     Auto \nan  apple  in    the   car\n attention\nein    Apfel    im     Auto ein    Apfel    im     Auto \n      apple  in         apple         the  \nan                    the   car\nan  apple  in    the\nan             in            car\n attention (d) Glancing LM (GLM)\nFigure 1: Probabilistic models for machine translation\nmethods. (b) Vanilla NAT uses conditional indepe-\ndent LM. (c) Mask-Predict NAT uses MLM and re-\nquires multiple passes of decoding. (d) Our proposed\nGLM leverages the decoder prediction to decide glanc-\ning sampling policy during training and only requires\none pass of decoding during inference.\nsentence. Notice that such word interdependency\nis crucial, as the Transformer explicitly captures\nthat via decoding from left to right (Figure 1a).\nSeveral remedies are proposed (Ghazvininejad\net al., 2019; Gu et al., 2019) to capture word inter-\ndependency while keeping parallel decoding. Their\ncommon idea is to decode the target tokens itera-\ntively while each pass of decoding is trained using\nthe masked language model (Figure 1c). Since\nthese methods require multiple passes of decod-\ning, its generation speed is measurably slower than\nthe vanilla NAT. With single-pass generation only,\nthese methods still largely lag behind the autore-\ngressive Transformer.\n1994\nOne open question is whether a complete par-\nallel decoding model can achieve comparable ma-\nchine translation performance to the Transformer.\nIt should be non-autoregressive and take only one\npass of decoding during the inference time.\nTo address the quest, we propose glancing lan-\nguage model (GLM), a new method to train a prob-\nabilistic sequence model. Based on GLM, we de-\nvelop the glancing Transformer (GLAT) for neural\nmachine translation. It achieves parallel text gener-\nation with only single decoding. Yet, it outperforms\nprevious NAT methods and achieves comparable\nperformance as the strong Transformer baseline in\nmultiple cases. Intuitively, GLM adopts a adaptive\nglancing sampling strategy, which glances at some\nfragments of the reference if the reference is too\ndifﬁcult to ﬁt in the training of GLAT. Correspond-\ningly, when the model is well tuned, it will adap-\ntively reduce the percentage of glancing sampling,\nmaking sure that the resulting model could learn\nto generate the whole sentence in the single-pass\nfashion. The gradual learning process smooths the\nlearning curve of single-pass parallel generation.\nSpeciﬁcally, our proposed GLM differs from\nMLM in two aspects. Firstly, GLM proposes an\nadaptive glancing sampling strategy, which enables\nGLAT to generate sentences in a one-iteration way,\nworking by gradual training instead of iterative in-\nference (see Figure 1d). Generally, GLM is quite\nsimilar to curriculum learning (Bengio et al., 2009)\nin spirit, namely ﬁrst learning to generate some\nfragments and gradually moving to learn the whole\nsentences (from easy to hard). To achieve the adap-\ntive glancing sampling, GLM performs decoding\ntwice in training. The ﬁrst decoding is the same as\nthe vanilla NAT, and the prediction accuracy indi-\ncates whether the current reference is “difﬁcult” for\nﬁtting. In the second decoding, GLM gets words\nof the reference via glancing sampling according\nto the ﬁrst decoding, and learn to predict the re-\nmaining words that are not sampled. Note that\nonly the second decoding will update the model pa-\nrameters. Secondly, instead of using the [MASK]\ntoken, GLM directly uses representations from the\nencoder at corresponding positions, which is more\nnatural and could enhance the interactions between\nsampled words and signals from the encoder.\nNote that GLAT does not modify the network ar-\nchitecture, which is a training method to explicityly\nlearn word interdependency. Experimental results\nshow that GLAT obtains signiﬁcant improvements\n(about 5 BLEU) on standard benchmarks compared\nto the vanilla NAT, without losing inference speed-\nup. GLAT achieves competitive results against iter-\native approaches like Mask-Predict (Ghazvininejad\net al., 2019), even outperforming the Mask-Predict\nmodel on WMT14 DE-EN and WMT16 RO-EN.\nCompared to the strong AT baseline, GLAT can\nstill close the performance gap within 0.9 BLEU\npoint while keeping 7.9×speed-up. Empirically,\nwe even ﬁnd that GLAT outperforms AT when the\nlength of the reference is less than 20 on WMT14\nDE-EN. We speculate this is because GLM could\ncapture bidirectional context for generation while\nits left-to-right counterpart is only unidirectional,\nwhich indicates the potential of parallel generation\napproaches like GLAT.\n2 Probability Models of Machine\nTranslation\nWe state and compare different probability mod-\nels for machine translation. A machine translation\ntask can be formally deﬁned as a sequence to se-\nquence generation problem: given the source sen-\ntence X = {x1,x2,...,x N }, to generate the target\nsentence Y = {y1,y2,...,y T }according to the con-\nditional probability P(Y|X; θ), where θ denotes\nthe parameter set of a network. Different methods\nfactorize the conditional probability differently.\nThe Transformer uses the autoregressive factor-\nization to maximize the following likelihood:\nLAT = log P(Y|X; θ) =\nT∑\nt=1\nlog p(yt|y<t,X; θ),\nwhere y<t = {[BOS],y1,...,y t−1}. For simplic-\nity, we omit the number of samples in the equation.\nNote the training of AT adopts left-to-right teacher\nforcing on the target tokens (Vaswani et al., 2017).\nThe word interdependency is learned in a unidi-\nrectional way. During inference, the preceding\npredicted token is fed into the decoder to generate\nthe next token.\nThe vanilla NAT consists of the same encoder as\nthe Transformer and a parallel decoder with layers\nof multi-head attention (Gu et al., 2018). During\ntraining, it uses the conditional independent factor-\nization for the target sentence:\nLNAT =\nT∑\nt=1\nlog P(yt|X; θ).\n1995\nNAT \nDecodingH H′\u0000 Glancing Sampling\ny1\ny3\ny5\nh2\nh4\nHamming \nDistance\nN( ̂Y, Y) = 3\nReplace \nInputs\ny1\ny2\ny3\ny4\ny5\n̂y1\n̂y2\n̂y4\n̂y5\n̂y3\n0.8\n0.5\n0.7\n0.6\n0.9\nParallel \nDecoderH H′\u0000 GlancingEncoder Parallel \nDecoder\n̂y1\n̂y2\n̂y4\n̂y5\n̂y3\nh1\nh2\nh3\nh4\nh5\nh1\nh2\nh3\nh4\nh5\nX Compute loss \nLGLM\ny1\ny3\ny5\nh2\nh4\nCompute \nDistance\n̂y1\n̂y2\n̂y4\n̂y5\n̂y3\ny1\ny2\ny3\ny4\ny5\nSample  \n words S(Y, ̂Y)\nReplace \nInput H\n̂Y Y\nFigure 2: The training procedure with glancing sampling in GLAT. H is the representation computed by the\nencoder. ˆy’s are the initial predicted tokens of the parallel decoder. y’s are the ground-truth target tokens. H′ is\nfed into the decoder again to calculate the training loss.\nNotice that, NAT’s log-likelihood is an approxima-\ntion to the full log-likelihood log P(Y|X; θ). Dur-\ning inference, the encoder representation is copied\nas the input to the decoder, therefore all tokens on\nthe target side can be generated in parallel. Such\na conditional independence assumption does not\nhold in general, which explains the inferior perfor-\nmance of NAT.\nMulti-pass iterative decoding approaches such as\nMask-Predict (Ghazvininejad et al., 2019) extends\nthe vanilla NAT. It still uses the conditional inde-\npendent factorization, together with the random\nmasking scheme:\nLMLM =\n∑\nyt∈RM(Y )\nlog p\n(\nyt|Φ\n(\nY,RM(Y)\n)\n,X; θ\n)\n,\nwhere RM(Y) is a set of randomly selected words\nfrom Y, and Φ(·) replaces these selected words\nin Y with the [MASK] token. For example in\nFigure 1c, RM(Y) = {y2,y3}, Φ\n(\nY,RM(Y)\n)\n=\n{y1,[MASK],[MASK],y4,y5}. The number of\nmasked tokens distributes uniformly from 1 to the\ntotal number of tokens in the target sequence. Such\ntraining objective is used to learn a reﬁnement\nmodel θthat can predict the masked tokens given\nthe source sentence X and words generated in the\nprevious iteration.\nThe vanilla NAT breaks word interdependency,\nwhile MLM requires multiple passes of decoding to\nre-establish the word interdependency. Our goal in\nthis work is to design a better probability model and\na training objective to enable word interdependency\nlearning for single-pass parallel generation.\n3 Glancing Transformer\nIn this section, we present GLAT in detail. GLAT\nuses the same encoder-decoder architecture as the\nvanilla NAT (Gu et al., 2018). GLAT differs from\nthe vanilla NAT in that it explicitly encourages\nword interdependency via training with glancing\nlanguage model (GLM). It differs from the iterative\nNAT with MLM in that it is trained to produce\nsingle pass parallel decoding while MLM is used\nfor prediction reﬁnement.\n3.1 The Glancing Language Model\nGiven the input source sentence X =\n{x1,x2,...,x N }, the task is to predict\nY = {y1,y2,...,y T }. The glancing Trans-\nformer (GLAT) formulates a glancing language\nmodel (GLM) during training. It maximizes the\nfollowing:\nLGLM =\n∑\nyt∈GS(Y, ˆY )\nlog p(yt|GS(Y, ˆY),X; θ)\n(1)\nWhere, ˆY is the initial predicted tokens, and\nGS(Y, ˆY) is a subset of tokens selected via the\nglancing sampling strategy (Figure 2, described in\ndetail in the next section). The glancing sampling\nstrategy selects those words from the target sen-\ntence by comparing the initial prediction against\nthe ground-truth tokens. It selects more tokens and\nfeeds the embeddings of these tokens into the de-\ncoder input if the network’s initial prediction is\nless accurate. GS(Y, ˆY) is the remaining subset of\ntokens within the target Y but not selected. The\ntraining loss above is calculated against these re-\nmaining tokens.\n1996\nGLAT adopts similar encoder-decoder archi-\ntecture as the Transformer with some modiﬁca-\ntion (Figure 1d). Its encoder fencis the same multi-\nhead attention layers. Its decoder fdec include\nmultiple layers of multi-head attention where each\nlayer attends to the full sequence of both encoder\nrepresentation and the previous layer of decoder\nrepresentation.\nDuring the initial prediction, the input to the\ndecoder H = {h1,h2,...,h T }are copied from\nthe encoder output using either uniform copy or\nsoft copy (Wei et al., 2019). The initial tokens\nˆY are predicted using argmax decoding with\nfdec(fenc(X; θ),H; θ).\nTo calculate the loss LGLM, we compare the ini-\ntial prediction ˆY against the ground-truth to select\ntokens within the target sentence, i.e. GS(Y, ˆY).\nWe then replace those sampled indices of h’s with\ncorresponding target word embeddings, H′ =\nRP(Embyt∈GS(Y, ˆY )(yt),H), where RP replaces\nthe corresponding indices. Namely, if a token in\nthe target is sampled, its word embedding replaces\nthe corresponding h. Here the word embeddings\nare obtained from the softmax embedding matrix\nof the decoder. The updated H′is then fed into\nthe decoder fdec again to calculate the output token\nprobability. Speciﬁcally, the output probabilities of\nremaining tokens p(yt|GS(Y, ˆY),X; θ) are com-\nputed with fdec(H′,fenc(X; θ); θ).\n3.2 The Glancing Sampling Strategy\nOne important component of GLM is to adaptively\nselect the positions of tokens from the target sen-\ntence. Those selected tokens provide “correct” in-\nformation from the ground-truth target, therefore it\nhelps training the decoder to predict the rest non-\nselected tokens. Intuitively, our adaptive sampling\nstrategy guides the model to ﬁrst learn the gener-\nation of fragments and then gradually turn to the\nwhole sentences. Our glancing sampling strategy\nselects many words at the start of the training, when\nthe model is not yet well tuned. As the model gets\nbetter progressively, the sampling strategy will sam-\nple fewer words to enable the model to learn the\nparallel generation of the whole sentence. Note\nthat the sampling strategy is crucial in the training\nof GLAT.\nAs illustrated in Figure 2, the glancing sampling\ncould be divided into two steps: ﬁrst deciding a\nsampling number S, and then randomly selecting\nSwords from the reference. The sampling number\nSwill be larger when the model is poorly trained\nand decreases along the training process. Note that\nwe choose to randomly select the Swords from the\nreference. The random reference word selection is\nsimple and yields good performance empirically.\nFormally, given the input X, its predicted sen-\ntence ˆY and its reference Y, the goal of glancing\nsampling function GS(Y, ˆY) is to obtain a subset\nof words sampled from Y:\nGS(Y, ˆY) = Random(Y,S(Y, ˆY)) (2)\nHere, Random(Y,S) is randomly selecting S to-\nkens from Y, and S is computed by comparing\nthe difference between ˆY and Y, S(Y, ˆY) = λ·\nd(Y, ˆY). The sampling ratio λis a hyper-parameter\nto more ﬂexibly control the number of sampled to-\nkens. d(Y, ˆY) is a metric for measuring the differ-\nences between Y and ˆY. We adopt the Hamming\ndistance (Hamming, 1950) as the metric, which\nis computed as d(Y, ˆY) = ∑T\nt=1(yt ̸= ˆyt). With\nd(Y, ˆY), the sampling number can be decided adap-\ntively considering the current trained model’s pre-\ndiction capability. For situations thatY and ˆY have\ndifferent lengths, d(Y, ˆY) could be other distances\nsuch as Levenshtein distance (Levenshtein, 1966).\nAlternative glancing sampling strategy can be\nadopted as well. For example, one simple alterna-\ntive strategy is to set the number of sampled tokens\nto be proportional to the target sentence length, i.e.\nS = λ∗T. We will evaluate the effects of these\nvariations in the experiment.\n3.3 Inference\nGLAT only modiﬁes the training procedure. Its in-\nference is fully parallel with only a single pass. For\nparallel generation, we need to decide the output\nlengths before decoding. A simple way to decide\nthe output lengths is predicting length with repre-\nsentations from the encoder.\nIn GLAT, the length prediction is implemented\nas in Ghazvininejad et al. (2019). An additional\n[LENGTH] token is added to the source input, and\nthe encoder output for the [LENGTH] token is\nused to predict the length.\nWe also use two more complex methods to bet-\nter decide the output lengths: noisy parallel decod-\ning (NPD) and connectionist temporal classiﬁca-\ntion (CTC). For NPD (Gu et al., 2018), we ﬁrst\npredict mtarget length candidates, then generate\noutput sequences with argmax decoding for each\ntarget length candidate. Then we use a pre-trained\n1997\nModels Idec\nWMT14 WMT16 Speed UpEN-DE DE-EN EN-RO RO-EN\nAT Models Transformer (Vaswani et al., 2017) T 27.30 / / / /\nTransformer (ours) T 27.48 31.27 33.70 34.05 1.0×†\nIterative NAT\nNAT-IR (Lee et al., 2018) 10 21.61 25.48 29.32 30.19 1.5×\nLaNMT (Shu et al., 2020) 4 26.30 / / 29.10 5.7×\nLevT (Gu et al., 2019) 6+ 27.27 / / 33.26 4.0×\nMask-Predict (Ghazvininejad et al., 2019)10 27.03 30.53 33.08 33.31 1.7×\nJM-NAT (Guo et al., 2020b) 10 27.31 31.02 / / 5.7×\nFully NAT\nNAT-FT (Gu et al., 2018) 1 17.69 21.47 27.29 29.06 15.6×\nMask-Predict (Ghazvininejad et al., 2019)1 18.05 21.83 27.32 28.20 /\nimit-NAT (Wei et al., 2019) 1 22.44 25.67 28.61 28.90 18.6×\nNAT-HINT (Li et al., 2019) 1 21.11 25.24 / / /\nFlowseq (Ma et al., 2019) 1 23.72 28.39 29.73 30.72 1.1×\nNAT-DCRF (Sun et al., 2019) 1 23.44 27.22 / / 10.4×\nw/ CTC NAT-CTC (Libovick`y and Helcl, 2018) 1 16.56 18.64 19.54 24.67 /\nImputer (Saharia et al., 2020) 1 25.80 28.40 32.30 31.70 18.6×\nw/ NPD\nNAT-FT + NPD (m=100) 1 19.17 23.20 29.79 31.44 2.4×\nimit-NAT + NPD (m=7) 1 24.15 27.28 31.45 31.81 9.7×\nNAT-HINT + NPD (m=9) 1 25.20 29.52 / / /\nFlowseq + NPD (m=30) 1 25.31 30.68 32.20 32.84 /\nNAT-DCRF + NPD (m=9) 1 26.07 29.68 / / 6.1×\nOurs\nNAT-base† 1 20.36 24.81 28.47 29.43 15.3×†\nCTC† 1 25.52 28.73 32.60 33.46 14.6×†\nGLAT 1 25.21 29.84 31.19 32.04 15.3×†\nGLAT + CTC 1 26.39 29.54 32.79 33.84 14.6×†\nGLAT + NPD (m=7) 1 26.55 31.02 32.87 33.51 7.9×†\nTable 1: Results on WMT14 EN-DE/DE-EN and WMT16 EN-RO/RO-EN benchmarks. Idec is the number of\ndecoding iterations and mis the number of length reranking candidates. NPD represents noisy parallel decoding,\nCTC represents connectionist temporal classiﬁcation. †indicate the results are obtained by our implementation.\nNote that our work and previous work may use different hardware settings and implementation, the speed-up may\nnot be fair to compare directly.\ntransformer to rank these sequences and identify\nthe best overall output as the ﬁnal output. For\nCTC (Graves et al., 2006), following Libovick `y\nand Helcl (2018), we ﬁrst set the max output length\nto twice the source input length, and remove the\nblanks and repeated tokens after generation.\n4 Experiments\nIn this section, we ﬁrst introduce the settings of our\nexperiments, then report the main results compared\nwith several strong baselines. Ablation studies and\nfurther analysis are also included to verify the ef-\nfects of different components used in GLAT.\n4.1 Experimental Settings\nDatasets We conduct experiments on three ma-\nchine translation benchmarks: WMT14 EN-DE\n(4.5M translation pairs), WMT16 EN-RO (610k\ntranslation pairs), and IWSLT16 DE-EN (150K\ntranslation pairs). These datasets are tokenized\nand segmented into subword units using BPE en-\ncodings (Sennrich et al., 2016). We preprocess\nWMT14 EN-DE by following the data preprocess-\ning in Vaswani et al. (2017). For WMT16 EN-RO\nand IWSLT16 DE-EN, we use the processed data\nprovided in Lee et al. (2018).\nKnowledge Distillation Following previous\nwork (Gu et al., 2018; Lee et al., 2018; Wang et al.,\n2019), we also use sequence-level knowledge\ndistillation for all datasets. We employ the\ntransformer with the base setting in Vaswani et al.\n(2017) as the teacher for knowledge distillation.\nThen, we train our GLAT on distilled data.\nBaselines and Setup We compare our method\nwith the base Transformer and strong representa-\ntive NAT baselines in Table 1. For all our tasks,\nwe obtain other NAT models’ performance by di-\nrectly using the performance ﬁgures reported in\ntheir papers if they are available.\nWe adopt the vanilla model which copies\nsource input uniformly in Gu et al. (2018) as\nour base model (NAT-base) and replace the Uni-\nformCopy with attention mechanism using po-\nsitions. Note that the output length does not\nequal the length of reference in models using\nCTC. Therefore, for GLAT with CTC, we adopt\nlongest common subsequence distance for compar-\n1998\n25 26 27 28 29 30 31\nPerformance (BLEU)\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5Relative Decoding Speed-up\nGLAT\nTransformer\nNAT-base\nNAT-DCRF\nimit-NAT\nCTC\nMask-Predict\nFigure 3: The trade-off between speed-up and BLEU\non WMT14 DE-EN\ning Y and ˆY, and the glancing target is the tar-\nget alignment that maximize the output probability\nargmaxa∈B−1(Y ) P(a|X; θ). B−1 is the mapping\nproposed in (Graves et al., 2006), which expand\nthe reference to the length of output by inserting\nblanks or repeating words.\nFor WMT datasets, we follow the hyperparam-\neters of the base Transformer in Vaswani et al.\n(2017). And we choose a smaller setting for\nIWSLT16, as IWSLT16 is a smaller dataset. For\nIWSLT16, we use 5 layers for encoder and decoder,\nand set the model size dmodel to 256. Using Nvidia\nV100 GPUs, We train the model with batches\nof 64k/8k tokens for WMT/IWSLT datasets, re-\nspectively. We set the dropout rate to 0.1 and\nuse Adam optimizer (Kingma and Ba, 2014) with\nβ = (0.9,0.999). For WMT datasets, the learning\nrate warms up to 5e−4 in 4k steps and gradually\ndecays according to inverse square root schedule\nin Vaswani et al. (2017). As for IWSLT16 DE-EN,\nwe adopt linear annealing (from 3e−4 to 1e−5)\nas in Lee et al. (2018). For the hyper-parameter λ,\nwe adopt linear annealing from 0.5 to 0.3 for WMT\ndatasets and a ﬁxed value of 0.5 for IWSLT16. The\nﬁnal model is created by averaging the 5 best check-\npoints chosen by validation BLEU scores. We re-\nport tokenized BLEU for all the datasets used in\nexperiment. We measure the average latency per\nsentence on a single Nvidia 1080TI GPU.\n4.2 Main Results\nThe main results on the benchmarks are presented\nin Table 1. GLAT signiﬁcantly improves the trans-\nlation quality and outperforms strong baselines by a\nlarge margin. Our method introduces explicit word\ninterdependency modeling for the decoder and\ngradually learns simultaneous generation of whole\nsequences, enabling the model to better capture the\nunderlying data structure. Compared to models\n[0,20) [20,40) [40,60) >=60\nSource Input Length\n26\n28\n30\n32\n34Performance (BLEU)\nNAT-base\nTransformer\nGLAT\nFigure 4: Performance under different source input\nlength on WMT14 DE-EN\nwith iterative decoding, our method completely\nmaintains the inference efﬁciency advantage of\nfully non-autoregressive models, since GLAT gen-\nerate with a single pass. Compared with the base-\nlines, we highlight our empirical advantages:\n•GLAT is highly effective. Compared with the\nvanilla NAT-base models, GLAT obtains sig-\nniﬁcant improvements (about 5 BLEU) on EN-\nDE/DE-EN. Additionally, GLAT also outper-\nforms other fully non-autoregressive models\nwith a substantial margin (almost +2 BLEU\npoints on average). The results are even very\nclose to those of the AT model, which shows\ngreat potential.\n•GLAT is simple and can be applied to other\nNAT models ﬂexibly, as we only modify the\ntraining process by reference glancing while\nkeeping inference unchanged. For compari-\nson, NAT-DCRF utilizes CRF to generate se-\nquentially; NAT-IR and Mask-Predict models\nneed multiple decoding iterations.\n•CTC and NPD use different approaches to de-\ntermine the best output length, and they have\ntheir own advantages and disadvantages. CTC\nrequires the output length to be longer than\nthe exact target length. With longer output\nlengths, the training will consume more time\nand GPU memory. As for NPD, with a certain\nnumber of length reranking candidates, the\ninference speed will be slower than models\nusing CTC. Note that NPD can use pretrained\nAT models or the non-autoregressive model\nitself to rerank multiple outputs.\nWe also present a scatter plot in Figure 3, dis-\nplaying the trend of speed-up and BLEU with dif-\nferent NAT models. It is shown that the point of\n1999\n1 2 3 4 5 6 7 8\nDecoding Iterations\n26\n27\n28\n29\n30\n31\n32BLEU\nWMT14 DE-EN\nWMT14 EN-DE\nFigure 5: The BLEU scores of GLAT with different\ndecoding iterations\nModel WMT14\nEN-DE DE-EN\nNAT-base 8.32% 7.10%\nGLAT 1.19% 1.05%\nGLAT w/ NPD 0.32% 0.16%\nTable 2: Token repetition ratio on WMT14 EN-DE and\nWMT14 DE-EN\nGLAT is located on the top-right of the competing\nmethods. Obviously, GLAT outperforms our com-\npetitors in BLEU if speed-up is controlled, and in\nspeed-up if BLEU is controlled. This indicates that\nGLAT outperforms previous NAT methods. Al-\nthough iterative models like Mask-Predict achieves\ncompetitive BLEU scores, they only maintain mi-\nnor speed advantages over AT. In contrast, fully\nnon-autoregressive models remarkably improve the\ninference speed.\n4.3 Analysis\nEffect of Source Input Length To analyze the\neffect of source input length on the models’ per-\nformance, we split the source sentences into differ-\nent intervals by length after BPE and compute the\nBLEU score for each interval. The histogram of\nresults is presented in Figure 4. NAT-base’s perfor-\nmance drops sharply for long sentences, while the\ngradual learning process enables GLAT to boost\nthe performance by a large margin, especially for\nlong sentences. We also ﬁnd that GLAT outper-\nforms autoregressive Transformer when the source\ninput length is smaller than 20.\nGLAT Reduces Repetition We also measure\nthe percentage of repeated tokens on test set of\nWMT14 EN-DE and WMT14 DE-EN. Table 2\npresents the token repetition ratio of sentences gen-\nerated by NAT-base and GLAT. The results show\nthat GLAT signiﬁcantly reduces the occurrence of\nrepetition, and the repetition ratio can be further\nSampling Number λ BLEU\nFixed\n0.0 24.66\n0.1 24.91\n0.2 27.12\n0.3 24.98\n0.4 22.96\nAdaptive - 29.61\nTable 3: Performances on IWSLT16 with ﬁxed sam-\npling ratio.\nSampling Number λs λe BLEU\nDecreasing\n0.5 0 27.80\n0.5 0.1 28.21\n0.5 0.2 27.15\n0.5 0.3 23.37\nAdaptive - 29.61\nTable 4: Performances on IWSLT16 with decreasing\nsampling ratio.\nreduced with NPD. We think an important cause\nof the improvement is better interdependency mod-\neling. Since GLAT explicitly encourages word\ninterdependency modeling to better capture the de-\npendency between target tokens, wrong generation\npatterns, such as repetition, can be largely avoided.\nGLAT Achieves Strong Results without Multi-\nple Iterations We conduct experiments of GLAT\nwith more than one decoding iteration in inference.\nWe adopt the inference algorithm in Mask-Predict\nfor multiple-iteration decoding. The results are\nshown in Figure 5. We ﬁnd that GLAT can achieve\ndecent performances with only one decoding iter-\nation, while further iterations only obtain minor\nimprovements of 0.2∼0.3 BLEU.\n4.4 Ablation Study\nEffectiveness of the Adaptive Sampling Num-\nber To validate the effectiveness of the adap-\ntive sampling strategy for the sampling number\nS(Y, ˆY), we also introduce two ﬁxed approaches\nfor comparison. The ﬁrst one decides the sampling\nnumber with λ∗T, where T is the length of Y, and\nλis a constant ratio. The second one is relatively\nﬂexible, which sets a start ratio of λs and an end\nratio λe, and linearly reduces the sampling number\nfrom λs ∗T to λe ∗T along the training process.\nAs shown in Table 3 and Table 4, our adaptive\napproach (Adaptive in the table) outperforms the\nbaseline models with big margins. The results con-\nﬁrm our intuition that the sampling schedule affects\nthe generation performance of our NAT model. The\n2000\nSelection Strategy GLAT GLAT w/ NPD\nrandom 25.21 26.55\npref 24.87 25.83\n1 − pref 25.37 26.52\nmost certain 24.99 26.22\nmost uncertain 24.86 26.13\nTable 5: Performance on WMT14 EN-DE with differ-\nent reference word selection strategies.\nMethod Distance WMT14\nEN-DE DE-EN\nGLAT Levenshtein 24.56 28.96\nHamming 25.21 29.84\nGLAT w/ NPD Levenshtein 26.21 30.85\nHamming 26.55 31.02\nTable 6: Performance on WMT14 EN-DE and\nWMT14 DE-EN with different distances.\nsampling strategy, which ﬁrst offers relatively easy\ngeneration problems and then turns harder, bene-\nﬁts the ﬁnal performance. Besides, even with the\nsimplest constant ratio, GLAT still achieves remark-\nable results. When set λ= 0.2, it even outperforms\nthe baseline λ= 0.0 by 2.5 BLEU points.\nThe experiments potentially support that it is ben-\neﬁcial to learn the generation of fragments at the\nstart and gradually transfer to the whole sequence.\nThe ﬂexible decreasing ratio method works better\nthan the constant one, and our proposed adaptive\napproaches achieve the best results.\nInﬂuence of Reference Word Selection To ana-\nlyze how the strategies of selecting reference words\naffect glancing sampling, we conduct experiments\nwith different selection strategies. By default, we\nassume all the words in the reference are equally\nimportant and randomly choose reference words\nfor glancing. Besides the random strategy, we de-\nvise four other selection methods considering the\nprediction of ﬁrst decoding. Forpref and 1−pref, the\nsampling probability of each reference word is pro-\nportional to the output probability for the reference\nword pref and the probability 1 −pref, respectively.\nSimilar to the word selection strategy for masking\nwords during inference in Mask-Predict, we also\nadd two strategies related to the prediction conﬁ-\ndence: \"most certain\" and \"most uncertain.\" We\nchoose the positions where predictions have higher\nconﬁdence for \"most certain\", and vise versa for\n\"most uncertain.\" The results for different selection\nmethods are listed in Table 5.\nIn comparisons, the model with the selection\nMethod WMT14\nEN-DE DE-EN\nGLAT w/ uniform sampling 19.16 23.56\nGLAT w/ [MASK] inputs 24.99 29.48\nGLAT 25.21 29.84\nTable 7: Ablation study for comparing GLAT and\nMask-Predict on WMT14 EN-DE and DE-EN.\nstrategy 1 −pref outperforms the one with pref, in-\ndicating that words hard to predict are more im-\nportant for glancing in training. And we ﬁnd that\nthe random strategy performs a little better than\nthe two conﬁdence-based strategies. We think this\nindicates that introducing more randomness in sam-\npling enable GLAT to explore more interdepen-\ndency among target words. We adopt the random\nstrategy for its simplicity and good performance.\nComparison of Different Distances for Glanc-\ning Sampling We conduct experiments with two\ndistances for comparing the predictions of the ﬁrst\ndecoding and references, and the results are pre-\nsented in Table 6. Experimental results show that\nboth distances can be used to improve the quality of\none-iteration generation, and GLAT with Hamming\ndistance is better than GLAT with Levenshtein dis-\ntance. Especially when there is no target length\nreranking, GLAT with Hamming distance outper-\nforms GLAT with Levenshtein distance by about\n0.7 BLEU and 0.9 BLEU on WMT14 EN-DE and\nDE-EN respectively. We think Hamming distance\nis more strict than Levenshtein distance because\nonly the same words on the corresponding positions\nare regarded as correct, which is more consistent\nwith the training of GLAT.\nAdvantages of GLAT over Mask-Predict To\nstudy the effects of sampling strategy and decoder\ninputs of GLAT, we conduct experiments for re-\nplacing these two modules in GLAT with the corre-\nsponding part in Mask-Predict, respectively. The\nresults are presented in Table 7. GLAT employs\nglancing sampling strategy instead of the uniform\nsampling strategy used in Mask-Predict, and re-\nplaces the [MASK] token inputs with source rep-\nresentations from the encoder. The results show\nthat the glancing sampling strategy outperforms the\nuniform sampling strategy by 5∼6 BLEU points,\nand feeding representations from the encoder as\nthe decoder input could still improve the strong\nbaseline by 0.2∼0.3 BLEU points after adopting\nglancing sampling. To sum up, the adaptive glanc-\n2001\ning sampling approach contributes the most to the\nﬁnal improvement, and the use of representations\nfrom the encoder also helps a bit.\n5 Related Work\nFully Non-Autoregressive Models A line of\nwork introduces various forms of latent variables\nto reduce the model’s burden of dealing with de-\npendencies among output words (Gu et al., 2018;\nMa et al., 2019; Bao et al., 2019; Ran et al., 2019;\nBao et al., 2021). Another branch of work consid-\ners transferring the knowledge from autoregressive\nmodels to non-autoregressive models (Wei et al.,\n2019; Li et al., 2019; Guo et al., 2020a; Sun and\nYang, 2020). Besides, there are also some work\nthat apply different training objectives to train non-\nautoregressive models (Libovick`y and Helcl, 2018;\nShao et al., 2020; Ghazvininejad et al., 2020a), add\nregularization terms (Wang et al., 2019; Guo et al.,\n2019).\nNon-Autoregressive Models with Structured\nDecoding To model the dependencies between\nwords, Sun et al. (2019) introduces a CRF inference\nmodule in NAT and performs additional sequential\ndecoding after the non-autoregressive computation\nin inference. Deng and Rush (2020) proposes cas-\ncaded CRF decoding. Since GLAT only performs\nsingle-pass non-autoregressive generation, our ap-\nproach is orthogonal to the method proposed in Sun\net al. (2019). We can also combine our approach\nwith the structured decoding methods.\nNon-Autoregressive Models with Iterative Re-\nﬁnement A series of work are devoted to semi-\nautoregressive models that reﬁne the outputs with\nmulti-pass iterative decoding (Lee et al., 2018;\nMiao et al., 2019; Gu et al., 2019; Ghazvinine-\njad et al., 2019, 2020b; Kasai et al., 2020; Li et al.,\n2020). Lee et al. (2018) proposed a method of it-\nerative reﬁnement based on denoising autoencoder.\nGu et al. (2019) utilized insertion and deletion to\nreﬁne the outputs in inference. Ghazvininejad et al.\n(2019) trained the model with the masked language\nmodel, and the model iteratively replaces masked\ntokens with new outputs. (Li et al., 2020) ﬁrst pre-\ndict the left token and right token for each position,\nand decode the ﬁnal token at the current position\nconditioned on the left-and-right tokens predicted\nbefore. Despite the relatively better accuracy, the\nmultiple decoding iterations reduce the inference\nefﬁciency of non-autoregressive models.\nScheduled Sampling To alleviate exposure bias\nin autoregressive models, previous work attempts\nto close the gap between training and inference\nby scheduled sampling (Bengio et al., 2015; Mi-\nhaylova and Martins, 2019). Although scheduled\nsampling also modiﬁes decoder inputs in training,\nthere are mainly two differences between our work\nand scheduled sampling. Firstly, scheduled sam-\npling mixes up the predicted sequence and the\ngold target sequence, and our method does not\nmix predicted sequences into decoder inputs. Be-\nsides, GLAT aims to learn word interdependency\nfor single-pass parallel generation and scheduled\nsampling is designed for alleviating exposure bias.\n6 Conclusion\nIn this paper, we propose Glancing Transformer\nwith a glancing language model to improve the per-\nformance of single-pass parallel generation models.\nWith the glancing language model, the model starts\nfrom learning the generation of sequence fragments\nand gradually moving to whole sequences. Experi-\nmental results show that our approach signiﬁcantly\nimproves the performance of non-autoregressive\nmachine translation with single-pass parallel gener-\nation. As GLAT achieves competitive performance\ncompared with autoregressive models, applying our\napproach to other generation tasks is a promising\ndirection for future work.\nAcknowledgments\nWe thank all the anonymous reviewers for their\nvaluable comments. Hao Zhou and Lei Li are cor-\nresponding authors.\nReferences\nYu Bao, Shujian Huang, Tong Xiao, Dongqi Wang,\nXinyu Dai, and Jiajun Chen. 2021. Non-\nautoregressive translation by learning target categori-\ncal codes. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5749–5759.\nYu Bao, Hao Zhou, Jiangtao Feng, Mingxuan Wang,\nShujian Huang, Jiajun Chen, and Lei Li. 2019.\nNon-autoregressive transformer by position learning.\narXiv preprint arXiv:1911.10677.\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and\nNoam Shazeer. 2015. Scheduled sampling for se-\nquence prediction with recurrent neural networks.\nIn Proceedings of the 28th International Conference\n2002\non Neural Information Processing Systems-Volume\n1, pages 1171–1179.\nYoshua Bengio, Jérôme Louradour, Ronan Collobert,\nand Jason Weston. 2009. Curriculum learning. In\nICML, pages 41–48.\nYuntian Deng and Alexander Rush. 2020. Cascaded\ntext generation with markov transformers.Advances\nin Neural Information Processing Systems, 33.\nMarjan Ghazvininejad, Vladimir Karpukhin, Luke\nZettlemoyer, and Omer Levy. 2020a. Aligned cross\nentropy for non-autoregressive machine translation.\nIn International Conference on Machine Learning ,\npages 3515–3523. PMLR.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-predict: Parallel de-\ncoding of conditional masked language models. In\nEMNLP-IJCNLP, pages 6114–6123.\nMarjan Ghazvininejad, Omer Levy, and Luke Zettle-\nmoyer. 2020b. Semi-autoregressive training im-\nproves mask-predict decoding. arXiv preprint\narXiv:2001.08785.\nAlex Graves, Santiago Fernández, Faustino Gomez,\nand Jürgen Schmidhuber. 2006. Connectionist\ntemporal classiﬁcation: labelling unsegmented se-\nquence data with recurrent neural networks. In\nICML, pages 369–376.\nJiatao Gu, James Bradbury, Caiming Xiong, Vic-\ntor O.K. Li, and Richard Socher. 2018. Non-\nautoregressive neural machine translation. In ICLR.\nJiatao Gu, Changhan Wang, and Junbo Zhao. 2019.\nLevenshtein transformer. In NeurIPS, pages 11179–\n11189.\nJunliang Guo, Xu Tan, Di He, Tao Qin, Linli Xu, and\nTie-Yan Liu. 2019. Non-autoregressive neural ma-\nchine translation with enhanced decoder input. In\nAAAI, volume 33, pages 3723–3730.\nJunliang Guo, Xu Tan, Linli Xu, Tao Qin, Enhong\nChen, and Tie-Yan Liu. 2020a. Fine-tuning by cur-\nriculum learning for non-autoregressive neural ma-\nchine translation. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence , volume 34, pages\n7839–7846.\nJunliang Guo, Linli Xu, and Enhong Chen. 2020b.\nJointly masked sequence-to-sequence model for non-\nautoregressive neural machine translation. In ACL,\npages 376–385.\nRichard W Hamming. 1950. Error detecting and error\ncorrecting codes. The Bell system technical journal,\n29(2):147–160.\nJungo Kasai, James Cross, Marjan Ghazvininejad, and\nJiatao Gu. 2020. Non-autoregressive machine trans-\nlation with disentangled context transformer. In In-\nternational Conference on Machine Learning, pages\n5144–5155. PMLR.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nJason Lee, Elman Mansimov, and Kyunghyun Cho.\n2018. Deterministic non-autoregressive neural\nsequence modeling by iterative reﬁnement. In\nEMNLP, pages 1173–1182.\nVladimir I Levenshtein. 1966. Binary codes capable\nof correcting deletions, insertions, and reversals. In\nSoviet physics doklady, pages 707–710.\nXiaoya Li, Yuxian Meng, Arianna Yuan, Fei Wu,\nand Jiwei Li. 2020. Lava nat: A non-\nautoregressive translation model with look-around\ndecoding and vocabulary attention. arXiv preprint\narXiv:2002.03084.\nZhuohan Li, Di He, Fei Tian, Tao Qin, Liwei Wang,\nand Tie-Yan Liu. 2019. Hint-based training for non-\nautoregressive translation. In EMNLP-IJCNLP.\nJindˇrich Libovick `y and Jind ˇrich Helcl. 2018. End-\nto-end non-autoregressive neural machine transla-\ntion with connectionist temporal classiﬁcation. In\nEMNLP, pages 3016–3021.\nXuezhe Ma, Chunting Zhou, Xian Li, Graham Neu-\nbig, and Eduard Hovy. 2019. FlowSeq: Non-\nautoregressive conditional sequence generation with\ngenerative ﬂow. In EMNLP-IJCNLP, pages 4273–\n4283, Hong Kong, China.\nNing Miao, Hao Zhou, Lili Mou, Rui Yan, and Lei Li.\n2019. CGMH: Constrained sentence generation by\nMetropolis-Hastings sampling. In AAAI.\nTsvetomila Mihaylova and André FT Martins. 2019.\nScheduled sampling for transformers. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics: Student Research\nWorkshop, pages 351–356.\nQiu Ran, Yankai Lin, Peng Li, and Jie Zhou. 2019.\nGuiding non-autoregressive neural machine transla-\ntion decoding with reordering information. arXiv\npreprint arXiv:1911.02215.\nChitwan Saharia, William Chan, Saurabh Saxena, and\nMohammad Norouzi. 2020. Non-autoregressive ma-\nchine translation with latent alignments. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n1098–1108.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In ACL, pages 1715–1725.\nChenze Shao, Jinchao Zhang, Yang Feng, Fandong\nMeng, and Jie Zhou. 2020. Minimizing the bag-of-\nngrams difference for non-autoregressive neural ma-\nchine translation. In AAAI, pages 198–205.\n2003\nRaphael Shu, Jason Lee, Hideki Nakayama, and\nKyunghyun Cho. 2020. Latent-variable non-\nautoregressive neural machine translation with deter-\nministic inference using a delta posterior. In Pro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, volume 34, pages 8846–8853.\nZhiqing Sun, Zhuohan Li, Haoqing Wang, Di He,\nZi Lin, and Zhihong Deng. 2019. Fast structured\ndecoding for sequence models. In NeurIPS, pages\n3016–3026.\nZhiqing Sun and Yiming Yang. 2020. An em ap-\nproach to non-autoregressive conditional sequence\ngeneration. In International Conference on Machine\nLearning, pages 9249–9258. PMLR.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS, pages 5998–6008.\nYiren Wang, Fei Tian, Di He, Tao Qin, ChengXiang\nZhai, and Tie-Yan Liu. 2019. Non-autoregressive\nmachine translation with auxiliary regularization. In\nAAAI.\nBingzhen Wei, Mingxuan Wang, Hao Zhou, Junyang\nLin, and Xu Sun. 2019. Imitation learning for non-\nautoregressive neural machine translation. In ACL.",
  "topic": "Zhàng",
  "concepts": [
    {
      "name": "Zhàng",
      "score": 0.7631978988647461
    },
    {
      "name": "Machine translation",
      "score": 0.7151482105255127
    },
    {
      "name": "Autoregressive model",
      "score": 0.6631090641021729
    },
    {
      "name": "Transformer",
      "score": 0.57914799451828
    },
    {
      "name": "Computer science",
      "score": 0.5565888285636902
    },
    {
      "name": "Natural language processing",
      "score": 0.5194848775863647
    },
    {
      "name": "Artificial intelligence",
      "score": 0.476149320602417
    },
    {
      "name": "Computational linguistics",
      "score": 0.45398229360580444
    },
    {
      "name": "Artificial neural network",
      "score": 0.43901723623275757
    },
    {
      "name": "Speech recognition",
      "score": 0.4088031053543091
    },
    {
      "name": "Engineering",
      "score": 0.25578567385673523
    },
    {
      "name": "Electrical engineering",
      "score": 0.2028558850288391
    },
    {
      "name": "History",
      "score": 0.19309565424919128
    },
    {
      "name": "Mathematics",
      "score": 0.14103341102600098
    },
    {
      "name": "Econometrics",
      "score": 0.08895987272262573
    },
    {
      "name": "China",
      "score": 0.07531806826591492
    },
    {
      "name": "Voltage",
      "score": 0.07495737075805664
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I881766915",
      "name": "Nanjing University",
      "country": "CN"
    }
  ],
  "cited_by": 99
}