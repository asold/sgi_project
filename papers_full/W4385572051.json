{
  "title": "Mixed Orthographic/Phonemic Language Modeling: Beyond Orthographically Restricted Transformers (BORT)",
  "url": "https://openalex.org/W4385572051",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2326818495",
      "name": "Robert Gale",
      "affiliations": [
        "Oregon Health & Science University"
      ]
    },
    {
      "id": null,
      "name": "Alexandra Salem",
      "affiliations": [
        "Oregon Health & Science University"
      ]
    },
    {
      "id": "https://openalex.org/A2026954565",
      "name": "Gerasimos Fergadiotis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1975412413",
      "name": "Steven Bedrick",
      "affiliations": [
        "Oregon Health & Science University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2022319782",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W3197324626",
    "https://openalex.org/W2046932483",
    "https://openalex.org/W2052564696",
    "https://openalex.org/W4362474015",
    "https://openalex.org/W3186084730",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W1018654938",
    "https://openalex.org/W4372267432",
    "https://openalex.org/W1987601328",
    "https://openalex.org/W2052264861",
    "https://openalex.org/W2323425210",
    "https://openalex.org/W4296068816",
    "https://openalex.org/W1999318234",
    "https://openalex.org/W4320896618",
    "https://openalex.org/W4210851094",
    "https://openalex.org/W2972437137",
    "https://openalex.org/W2748199382",
    "https://openalex.org/W2306676374",
    "https://openalex.org/W3203147359",
    "https://openalex.org/W3096912371",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W4318755671",
    "https://openalex.org/W2034922898",
    "https://openalex.org/W2739697855",
    "https://openalex.org/W3135427360",
    "https://openalex.org/W4253871295",
    "https://openalex.org/W1939314753",
    "https://openalex.org/W2251658484",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2566552364",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W3197744084",
    "https://openalex.org/W2063042856",
    "https://openalex.org/W2141552142",
    "https://openalex.org/W4376642649",
    "https://openalex.org/W46679369",
    "https://openalex.org/W1992119553",
    "https://openalex.org/W2084413241",
    "https://openalex.org/W3095387535",
    "https://openalex.org/W4308553833",
    "https://openalex.org/W4311018845",
    "https://openalex.org/W3193875190",
    "https://openalex.org/W2946286144",
    "https://openalex.org/W3087538850"
  ],
  "abstract": "Speech language pathologists rely on information spanning the layers of language, often drawing from multiple layers (e.g. phonology & semantics) at once. Recent innovations in large language models (LLMs) have been shown to build powerful representations for many complex language structures, especially syntax and semantics, unlocking the potential of large datasets through self-supervised learning techniques. However, these datasets are overwhelmingly orthographic, favoring writing systems like the English alphabet, a natural but phonetically imprecise choice. Meanwhile, LLM support for the international phonetic alphabet (IPA) ranges from poor to absent. Further, LLMs encode text at a word- or near-word level, and pre-training tasks have little to gain from phonetic/phonemic representations. In this paper, we introduce BORT, an LLM for mixed orthography/IPA meant to overcome these limitations. To this end, we extend the pre-training of an existing LLM with our own self-supervised pronunciation tasks. We then fine-tune for a clinical task that requires simultaneous phonological and semantic analysis. For an \"easy\" and \"hard\" version of these tasks, we show that fine-tuning from our models is more accurate by a relative 24% and 29%, and improved on character error rates by a relative 75% and 31%, respectively, than those starting from the original model.",
  "full_text": "Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023), pages 212–225\nJuly 13, 2023 ©2023 Association for Computational Linguistics\nMixed Orthographic/Phonemic Language Modeling:\nBeyond Orthographically Restricted Transformers (BORT)\nRobert C. Gale†\ngaler@ohsu.edu\nAlexandra C. Salem†\nsalem@ohsu.edu\nGerasimos Fergadiotis‡\ngf3@pdx.edu\nSteven Bedrick†\nbedrick@ohsu.edu\n†Oregon Health & Science University\nPortland, Oregon, USA\n‡Portland State University\nPortland, Oregon, USA\nAbstract\nSpeech language pathologists rely on information\nspanning the layers of language, often drawing\nfrom multiple layers (e.g. phonology & semantics)\nat once. Recent innovations in large language mod-\nels (LLMs) have been shown to build powerful rep-\nresentations for many complex language structures,\nespecially syntax and semantics, unlocking the po-\ntential of large datasets through self-supervised\nlearning techniques. However, these datasets are\noverwhelmingly orthographic, favoring writing sys-\ntems like the English alphabet, a natural but phonet-\nically imprecise choice. Meanwhile, LLM support\nfor the international phonetic alphabet (IPA) ranges\nfrom poor to absent. Further, LLMs encode text at\na word- or near-word level, and pre-training tasks\nhave little to gain from phonetic/phonemic repre-\nsentations. In this paper, we introduce BORT, an\nLLM for mixed orthography/IPA meant to over-\ncome these limitations. To this end, we extend\nthe pre-training of an existing LLM with our own\nself-supervised pronunciation tasks. We then fine-\ntune for a clinical task that requires simultaneous\nphonological and semantic analysis. For an “easy”\nand “hard” version of these tasks, we show that\nfine-tuning from our models is more accurate by a\nrelative 24% and 29%, and improves on character\nerror rates by a relative 75% and 31%, respectively,\nthan those starting from the original model.\n1 Introduction\nRecently, large language models (LLMs) have shown\nnotable success in capturing information across several\nlinguistic layers, developing rich representations of\nsyntactic and semantic structures within their hidden\nlayers (Rogers et al., 2020). This is accomplished\nthrough the use of self-supervised techniques, in which\nLLMs are pre-trained on large corpora to perform\ngeneric, contrived tasks. With a well-designed task, the\nmodel can make the most of vast quantities of unlabeled\ntext, gleaning the structural patterns of the language(s).\nFor example, in masked language modeling (MLM),\na model is trained to restore partially obscured text\nto its original form. Following this relatively generic\npre-training task, the resulting model can then be used\nas a starting point, and its weights fine-tuned (or its\narchitecture augmented with additional output layers) in\na task-specific manner. Crucially, the task-specific train-\ning can be accomplished using orders of magnitude less\ndata than is required for the original pre-training step.\nThis approach is well-suited to many linguistic tasks,\nparticularly those that rely on syntax and semantics.\nHowever, tasks that also depend on explicit represen-\ntation ofphonologyare under-served by the current\nparadigm. Examples of such tasks include: analysis\nof code-switched language; processing ambiguous\nand noisy output from automated speech recognition\nsystems; handling of names and neologisms; and\nanalysis of clinical language samples in the context\nof communication disorders.\nThere are two underlying reasons for this issue, the\nfirst of which involves input representation. As with any\ncomputational model, LLMs require language to be en-\ncoded into a numerical form, and a model’s choice of en-\ncoding technique has a profound impact on its function-\nality. Today’s LLMs typically rely on sub-word repre-\nsentations such as WordPieces (Schuster and Nakajima,\n2012) or Byte-Pair Encoding (BPE) (Gage, 1994; Sen-\nnrich et al., 2016), which strike a balance between vo-\ncabulary size and semantic precision while allowing un-\nrestricted input and avoiding the limitations arising from\na fixed word vocabulary. Such tokenization schemes are\ngenerally optimized for representing textual input for\nword-level processing, based on the distributional prop-\nerties of a training corpus, and as such in practice the\nmost prevalent segments in a tokenized input sentence\nrepresent entire words or large word fragments, with\nsub-word token fragment boundaries only incidentally\nco-occurring with morphological boundaries.\nWhile it has long been known that neural language\nmodels are able to capture implicit information about\nphonology from orthography (Elman, 1990; Prince\nand Smolensky, 1997), the extent to which this occurs\nwill depend on the degree to which the model’s unit of\nrepresentation maps to the writing system in question’s\nrepresentation of phonology. Furthermore, the self-\nsupervised pre-training techniques at the foundation\nof LLM tend to work (roughly) at a word-level or\nword-fragment scope; a task like MLM has little to gain\n212\nfrom learning the sound relationships between words,\nso we have no reason to expect these models to adapt\nto phonetic tasks as well as they do semantic ones.1\nThe second underlying reason for LLMs’ phono-\nlogical naïveté is data-related. Phonology is typically\nexpressed in written form using a writing system such\nas the International Phonetic Alphabet (IPA) or ARPA-\nbet. One advantage of the tokenization schemes used by\nLLMs is that they are, at a technical level, able to repre-\nsent arbitrary character input sequences: sequences of\npreviously-unseen characters (e.g., a sequence of IPA\nglyphs representing a phonemically-transcribed utter-\nance) simply resolve to a sequence of single-character\ntokens (rather than lumping together into sub-word\nunits as do more commonly-seen sequences of charac-\nters). More recently, new techniques have emerged for\ntraining \"token-free\" LLMs that operate at the character\nlevel (e.g.CANINE (Clark et al., 2022)) yet retain many\nof the semantic benefits of word-level models; however,\ntheir pre-training remains exclusively orthographic in\nnature, and thus will only learn phonology indirectly\n(and in a way that is mediated by the specifics of\nthe writing system in question). Furthermore, and\nmost importantly, the data used to pre-train an LLM\nincidentally contains little IPA content if any at all.\nAs such, regardless of the modeling strategy used, the\nresulting embedding of these tokens is likely to be\nuninformative from the a phonological perspective.\nPrevious attempts to augment LLMs with phonemic\ninformation have focused on speech-centered applica-\ntions, and have emphasized phonology at the expense\nof orthography (Jia et al., 2021; Sundararaman et al.,\n2021; Li et al., 2023; Zhang et al., 2022). For many\napplications however, particularly including clinical\napplications in speech-language pathology,both are\ncrucial, as expressive and receptive language depend\non both phonology and semantics. In this paper, we\nintroduce BORT, an LLM that accepts a mixture of\nEnglish pronunciations in IPA and English orthography,\nand demonstrate its use on a task motivated by a\nreal-world clinical problem: analysis of speech errors\nmade by individuals with aphasia following a stroke.\nIn §3, we create the BORT models by extending the\npre-training of an existing LLM, BART (Lewis et al.,\n2020). Our self-supervised task focuses on a novel\nIPA-to-orthography translation task: given a document,\nwe transform some words into IPA, then train the model\nto restore the orthography. Hypothesizing that we could\nbolster the pre-trained models with two additional\ntransforms, we experiment with configurations that\n1Though we note that Itzhak and Levy (2022) have demon-\nstrated that LLMs working at the word and sub-word level do\nimplicitly learn a certain amount about the character-level contents\nof their tokens.\ninclude spelling and noise transforms. In §4, we\nevaluate the utility of BORT by fine-tuning to two\nclinically-motivated tasks: a) an “easy” task, another\nmixed IPA/orthography to orthography translation task,\nbut in the context of aphasic speech; and b) a “hard”\ntask, in which the model must predict the intended\nword for several types of word errors, including\nphonologically and semantically related errors. We\nmake our pre-trained models available for anyone to\ndownload and fine-tune.2\n2 Background\n2.1 Speech Language Pathology\nSpeech language pathologists (SLPs) work to diagnose\nand treat speech and language disorders. Language\ndisorders typically include breakdowns in the linguistic\nsystem that supports the abilities to activate the\nsemantic representation of a concept; retrieve its\nlexical/syntactical representation; and, encode its\nphonological or orthographic form. Speech disorders\ninclude deficits that may stem from underdeveloped\nor faulty perceptual representations of speech sounds;\ndifficulties specifying a motor plan for the articulatory\ngestures required for producing a word; and/or execut-\ning the motor plan. Such disorders often co-exist, and\nmay be developmental (affecting primarily pediatric\npopulations, as in the case of Specific Language\nImpairment), or acquired and seen primarily in adult\npopulations (e.g., dementia, aphasia, dysarthria). In ad-\ndition, they might affect different modalities including\nspoken (e.g., anomia) or written output (e.g., agraphia).\nThe use case described in the present work focuses\non aphasia, a disorder in which an individual has\nan impairment to one or both of their expressive or\nreceptive language abilities. In expressive language,\nthis may take the form of difficulties in word retrieval or\nproduction; these difficulties may involve the inability\nto produce a word, the production of an unintended\nword, or the mispronunciation of a produced word.\nAphasia typically follows an injury to be brain (such\nas a stroke or a traumatic brain injury), though it may\nalso be a sign of certain neurodegenerative conditions.\nTo arrive at a diagnosis, clinical professionals\ntypically elicit productions from patients, and then,\nbased on the relationship between the intended target\nand the realized unexpected or atypical production, they\ndraw inferences regarding the nature of the cognitive-\nlinguistic or motoric deficits of the patient. Inherent\nin this diagnostic process is identifying what was the\nintended word of a speaker. That requires a clinician\nto combine multiple sources of information including\nsemantic, phonemic, and/or orthographic information.\nConsider for example the response/b@gæn@/when\n2https://github.com/rcgale/bort\n213\na stroke patient is asked to name the picture of an apple.\nA clinician, naturally, will recognize the phonological\nsimilarity of the production to the candidate intended\nword/b@næn@/(“banana”) and given the semantic sim-\nilarity of “banana” and “apple,” the clinician will arrive\nat the conclusion that most likely the speaker’s intended\ntarget was “banana.” This step is critical in the diag-\nnostic process across clinical populations and disorders.\nTherefore, the development of a robust computational\ntool that can combine multiple sources of information\nto predict a speaker’s intended words during a\nparaphasic speech event is of great clinical significance.\nThere exist several settings in which tools such these\nmay find applications. There are a variety of highly\naccurate and informative assessment techniques that\nare regularly used in research settings but rarely used in\nclinical practice due to the large amount of effort they\nrequire for delivery and scoring (Edmonds and Kiran,\n2006; Abel et al., 2007; Kendall et al., 2013; Minkina\net al., 2015; Walker and Hickok, 2016); automation has\nthe potential to streamline this process greatly, thereby\nenabling their clinical use. Additionally, automation\nof this sort would be a key part in many telemedicine\nand remote assessment scenarios, which is an area\nof great clinical interest (V an De Sandt-Koenderman,\n2004; Kiran et al., 2014) as there exist major challenges\naround access to care for many individuals in need of\nspeech and language services (Hou et al., 2023).\nUse cases such as these currently are limited by two\ncategories of technical barrier. The first is the need for\nrobust automated speech recognition algorithms able to\naccurately process the disordered speech characteristic\nof individuals with speech and language disabilities, and\nproduce detailed phonemic transcriptions; this is needed\ngiven the impracticality of detailed manual transcription\nin a fast-paced clinical setting. The second category\nis a lack of specialized algorithms designed to process\nthe resulting data to identify features of clinical interest,\nfor example in speech error classification (Casilio et al.,\n2023). Both types of technology are necessary, and\nneither on their own are sufficient, to leverage NLP in\nthis clinical domain. The present work, by design, only\naddresses the second of these categories; however, it is\nimportant to note that the first is an area of very active\nresearch (Fraser et al., 2013; Le et al., 2017; Jacks et al.,\n2019; Perez et al., 2020; Torre et al., 2021; Gale et al.,\n2022), with major strides being made in recent years.\nA second setting of use for automation in the analysis\nof language produced by people with aphasia (PW A) is\nthat of aphasiological research. Standard research prac-\ntice typically results in the creation of recorded sessions\nwith participants (for example, in a discourse elicitation\ntask), which are then transcribed to a very high degree\nof accuracy by specially-trained research staff, for use\nin analysis. Often, this transcription is done at a mixture\nof orthographic and phonemic levels, with particular\nphonemic attention paid to clinically-relevant phe-\nnomena such as neologisms (non-word productions),\nmis-pronunciations, etc. There exist large databases\nof such transcripts, for example TalkBank and its many\nsub-projects (see https://talkbank.org; MacWhinney,\n2000), and automated analysis of these datasets is\nextremely valuable from a scientific perspective.\nNotably, in this scenario, one need not assume\nthe existence of an ASR system robust to disordered\nspeech in order for automation to be useful, as the\ndata are transcribed as part of their collection and data\nmanagement process. However, the specifics of this\ntranscription process tend to be very closely linked to\nthe scientific needs of the research team conducting\nthe study, and while the amounts of data generated tend\nto be far more than humans can conveniently analyze\nby hand, they tend to be relatively small in comparison\nto the datasets commonly used in natural language\nprocessing. As such, techniques such as transfer\nlearning have become crucial tools in this space.\n2.2 Automating Clinical Language Evaluation\nThere exists a long history of use of NLP techniques\nin clinical language evaluation, across a wide variety\nof disorders including Alzheimer’s disease (Petti\net al., 2020), Autism Spectrum Disorder (Virnes\net al., 2015; MacFarlane et al., 2023), and various\nforms of aphasia (Fraser et al., 2014; Azevedo et al.,\n2023). From a computational perspective, this typically\ntakes the form of a pipeline accepting language\nsamples of some sort as input, and producing as output\nsome sort of relevant analysis, such as a score on a\nvalidated assessment instrument. The language samples\nused may consist of spontaneous speech, a patient’s\nresponses to a structured interaction of some kind, or a\nmixture of the two, and may feature continuous speech,\nor single-word productions. The input may be actual\naudio recordings, or transcriptions thereof.\nRecent work has taken advantage of LLMs for\nautomating clinical language evaluation tasks. Bal-\nagopalan et al. (2020) fine-tuned BERT to detect\nAlzheimer’s disease from transcribed spontaneous\nspeech, and found that BERT performed better than a\nstandard model based on hand-crafted features. Liu et al.\n(2022) evaluated transformer models for use in identi-\nfying relevant pragmatic features of transcribed speech\nfrom adults with Autism Spectrum Disorder; their anal-\nysis identified both advantages and limitations of an\nLLM-based approach over previous methods. Gale et al.\n(2021) described a system that scored tests for Specific\nLanguage Impairment in children, finding that the Dis-\ntilBERT architecture was adaptable to clinical language\nevaluation spanning several linguistic layers. Salem\n214\net al. (2022) fine-tuned DistilBERT for the automatic\ndetermination of semantic similarity in an aphasia test\nwith accuracy 95.3%, improving over earlier methods\nthat relied on word2vec (Fergadiotis et al., 2016).\nLLMs demonstrate improved performance at many\nof these tasks, and bring two additional benefits over\nearlier methods: they are much more flexible with\nregard to their input representation, and they are\nwell-suited for transfer learning via fine-tuning. Both\nattributes are crucial for work in clinical language\nevaluation, given the heterogeneity and limited size of\nthe datasets used in this space. However, we note that\ntechnical approaches in this space tend to “live” in either\nthe orthographic or phonemic space; this distinction is\nlogical from a technical standpoint, given the nature and\nhistory of language technologies, but quite contrary to\nthe actual clinical manifestation of speech and language\ndisorders (and, of course, the way in which clinicians\nmake use of language sampleswhere). From this\nperspective, we see BORT bridging the gap between\naudio recordings of spoken language tests—transcribed\nmanually or by an automatic speech recognizer—and\ndownstream language evaluation tasks.\n2.3 Considering alternative methods\nConceptually, BORT enhances BART with phoneme-\nto-grapheme functionality. Existing English\ngrapheme-to-phoneme (G2P) systems are highly\naccurate, with recent transformer models achieving a\n5.23% character error rate and a 22.1% word error rate\non CMUDict (Y olchuyeva et al., 2019). These systems\nare trained and evaluated on word-length samples, so\nintegration with a contextual language model would\nrequire novel architectural adapters, lest translation\nerrors propagate through to downstream tasks. Explicit\nphoneme-to-grapheme (P2G) systems are uncommon\nby comparison; however, the hidden Markov model and\nGaussian mixture model (HMM-GMM) architecture\nused in last-generation ASR systems (Mohri et al.,\n2001) might be described as an n-gram language\nmodel with a phoneme-to-grapheme component. This\narchitecture assumes predefined mappings between\nwords and its known pronunciations, and thus cannot\ncapture the open-ended variability of disordered\nspeech (in which a production might bear little or no\nphonological relationship to a target word). Further,\nconsidering the benefits of transfer learning for clinical\n(see §2.2), HMM-GMM models lack the flexibility\nand ergonomics of pre-trained transformer models.\n3 BORT\n3.1 Model Selection\nOur models are a direct continuation of a pre-trained\nBART model as described by Lewis et al. (2020). We\nchose BART for several reasons. First, it uses a BPE\ntokenizer, which is able to encode arbitrary Unicode\ncharacters, including the entirety of the IPA. By contrast,\nBERT models use WordPiece tokenizers with finite\ntoken inventories. None of the pretrained BERT-like\nmodels we considered covered the IPA symbols used\nin English phonology, and expanding a WordPiece\ninventory is a non-trivial task. We were also motivated\nby the denoising task behind BART, since the synthesis\nof word errors is at least tangentially relevant to speech\nlanguage pathology. Finally, unlike most models\nderived from the BERT architecture, BART features\na left-to-right decoder ideal for generative tasks, and\nits pre-training task allows a mask token to represent\none or more tokens, thus enabling us to overcome key\nlimitations we encountered while revisiting our earlier\nwork on analysis of connected speech from aphasic\nspeakers (Adams et al., 2017) using techniques and\nmodels developed by Salem et al. (2022).\n3.2 An IPA-to-Orthography Translation Task\nOur self-supervised approach was formulated as a trans-\nlation task, restoring partially-transformed documents\nback to the originals. We experimented with three word\ntransforms: pronunciation, spelling, and noise. Ulti-\nmately, we were aiming for a system that could convert\na mixture of IPA and orthography to its all-orthographic\nequivalent. However, despite an abundance of ortho-\ngraphic text, our pronunciation dictionary (described in\nmore detail in §3.3) was limited to only 98K words dur-\ning training. In an effort to avoid overfitting, we exper-\nimented with the other two less-constrained transforms.\nPronunciation transform. Our self-supervised\napproach was formulated as an IPA-to-orthography\ntranslation task. We used Wikipedia articles as a\nresource for orthographic text. Similar to masked lan-\nguage modeling, we randomly obscured words in each\narticle, but instead of a special mask token, we replaced\nwords with their pronunciations written in the IPA.\nSince the IPA includes letters from the English alphabet,\nwhich the tokenizer is likely to merge, we strategically\ninserted bullet symbols to maintain separation between\nphonemes (e.g. “shakedown” was transformed into\n“Se·k·d·aUn”). We then trained the model to restore the\ntext to its original orthographic form.\nSpelling transform. Considering how English\northography is related to its phonology (however\ndifficult a relationship it may be), we experimented\nwith a spelling transform that could be formulated\naround any word, even those outside our pronunciation\ndictionary. For this task, for randomly selected words,\nwe inserted a bullet symbol before each letter, forcing\nthe tokenizer to treat each letter as a discrete token (e.g.\n“pizza\" becomes “·P·I·Z·Z·A\"). Note that we also used\nuppercase letters to avoid any overlap with the English\n215\nIPA symbols.\nNoise transform. We also include a denoising task\nlike the one used in pre-training the BART models\n(Lewis et al., 2020), wherein input tokens are inserted,\ndeleted, and replaced at random. We used these same\nkinds of transforms, except we only apply noise to\npronounced or spelled words. For these words, we\nrandomly replace, insert, and delete tokens in the word.\nInsertions were limited to letters from the appropriate\nalphabet (either English or IPA).\nExperimental configurations. We experimented\nwith several variations of the above transforms,\nreplacing 10% of pronounceable words with IPA,\n10% of words with spellings, as well as a combined\npronounced/spelled configuration (at 10% each). We\nrepeated these configurations with noise added at a\n5% probability. Table 1 summarizes the pre-training\nconfigurations used in this paper.\n3.3 Data Preparation\nData sources. We based our pronunciation dictionary\non version 0.7b of CMUDict (Carnegie Mellon\nUniversity, 2014).3 We converted their ARPABet\nentries to IPA using hard-coded rules, removing\nstress symbols. As carrier text for our self-supervised\ntask, we used the 20220301.en version of Wikipedia\nprovided by Huggingface Datasets.4\nWord/article associations.For most purposes, the\nsynthetic dataset was designed to be as open-ended as\nresources allowed, applying the transforms according\nto the word frequency distributions of Wikipedia.\nHowever, considering how the pronunciation dictionary\nwas by far the data bottleneck (and thus the primary\nfocus of our validation strategies), we needed a way to\nintentionally and efficiently find a high quality context\nfor a given word. To this end, we paired each word in\nthe pronunciation dictionary with a unique article based\non an algorithm based on word frequencies. Tallying\nhow many times our dictionary words appeared in each\narticle, we assigned each dictionary word a unique\narticle with a simple algorithm: beginning with the\nrarest word, we chose the next available article with\nthe highest count for that word. We used a few simple\nrules to avoid specific types of low-quality articles.5\nData splits. We split the final list of 122K words\nand their associated Wikipedia articlesinto training,\nvalidation, and test sets (80%, 10%, and 10%,\n3https://github.com/Alexir/CMUdict/blob/7a37de7/\ncmudict-0.7b\n4https://huggingface.co/datasets/wikipedia/\n5We noticed our algorithm favored articles like “List of\npeople with surname Carpenter” and “Mercury (disambiguation),”\nwhich have high word frequencies for “carpenter” and “mercury,”\nrespectively. In Wikipedia, these articles function only as lists of\nlinks to other articles, and we used them only as a last resort.\nrespectively). The remaining 6.5M Wikipedia articles\nwith no associated word were added to the training\nset. Anticipating approximately 3100 target words that\nwe would use to evaluate fine-tuning in §4, we placed\nthese words in the test set. Three pairs of words were\nfound only in overlapping articles; we placed these\nwords in the training split.\nPronunciation transforms were only allowed for\nwords which could be found in the split’s dictionary. A\nspelling transform was only allowed for words which\nwere not be found inanothersplit’s dictionary.\nTraining and validation inputs.We iterated over\neach Wikipedia article, transforming and noising pro-\nnounceable/spellable words at the rates specified in\n§3.2. Training inputs were limited 1000 BPE tokens to\nfit within BART’s attention limit. If the article had an\nassociated word, the sample was trimmed so the median-\nposition occurrence of that word was at the center, other-\nwise a trim window was chosen at random. To minimize\ncomputation required for the validation set, each sample\nwas limited to 100 tokens, with the median-position\noccurrence of that word at the center. If the experiment\nincluded a pronunciation transform, the associated dic-\ntionary word in the center was always pronounced.\nSince the training set contained the most words,\nand the test set contained a number of high frequency\nwords held out for fine-tuning evaluation, we found\nthat the validation set only transformed at a word rate\nof 7–8% in practice, short of the 10% observed in the\ntraining inputs.\nTest data. For evaluation purposes, only a single\ninstance of the associated dictionary word was\npronounced. Inputs were limited to 100 tokens, with\nthe median-position occurrence of that word at the\ncenter. Neither the spelling nor the noise transforms\nwere applied during testing.\n3.4 Pre-training\nModel weights were initialized from the 140M parame-\nter BART-BASE pre-trained model found on the Fairseq\nwebsite.6 We based our hyperparameters on (Lewis\net al., 2020), using the fairseq toolkit (Ott et al., 2019).\nTraining targeted a categorical cross-entropy loss, with\na learn rate of10−5 and a maximum batch size of\n12288 tokens, resulting in 317K batches per epoch. We\ncomputed the validation loss every 1K batches, and\nrestored the best model after validation loss failed to\nimprove for 63K batches (20% of the Wikipedia data).\n3.5 Evaluation\nWe evaluated our pre-trained models in terms of\naccuracy and character error rate (CER) of the test set.\n6https://github.com/facebookresearch/fairseq/tree/main/\nexamples/bart\n216\nPre-trained Model Pron. Spell. Noise Example of Transformed Text\nBORT-PR 10% — — he retaliates by s·pôEdINfalse rumours\nBORT-SP — 10% — he ·R·E·T·A·L·I·A·T·E·S by spreading false rumours\nBORT-PR-SP 10% 10% — he ·R·E·T·A·L·I·A·T·E·S bys·pôEdINfalse rumours\nBORT-PR-NOISY 10% — 5% he retaliates by OIs·pEdIN false rumours\nBORT-SP-NOISY — 10% 5% he ·E·T·A·K·I·A·T·E·S by spreading false rumours\nBORT-PR-SP-NOISY 10% 10% 5% he ·E·T·A·K·I·A·T·E·S byOIs·pEdIN false rumours\nTable 1: The various self-supervised training configurations for our models, indicating the percentage of words replaced with\neither pronunciations in the IPA or spelled, with or without noise added to the replacements. Example text from Wikipedia\nis shown to demonstrate the transformations. Bullet characters are inserted to enforce separation between those letters which\nwould otherwise be merged during BPE text encoding.\nRecall that in the test set we only applied the transform\nwe were most interested in for this work: pronunciation.\nConsidering how each correct output contains nearly a\nhundred words of text also found in the input—a trivial\ntask to predict—we defined CER as the number of\ncharacter errors divided by the length ofonly the target\nword. Text case and whitespace were ignored during\nevaluation. Only the models applying the pronunciation\ntransform could be directly evaluated in this manner,\nso we do not include a baseline for this evaluation, and\nsome models are only evaluated indirectly in §4.\n3.6 Results\nConfigurations which included the noise transform\ndid better overall than those without. The one with\nspelling (BORT-PR-SP-NOISY) was the overall best\nwith a 15.1% CER, compared toBORT-PR-NOISY at\n19.5%. To a lesser extent, spelling improved CER in\nthe models without a noise transform:BORT-PR-SP\nand BORT-PR had a CER of 23.4% and 22.4%,\nrespectively. Accuracy followed a similar pattern, with\nBORT-PR-SP-NOISY, BORT-PR-NOISY, BORT-PR-SP,\nBORT-PR showing accuracies of 64.5%, 61.8%, 51.6%,\nand 55.0%, respectively.\nAs for our overfitting concerns, most of our models\nshowed an increase in validation loss (i.e. early\nstopping was triggered) before completing a full\nepoch of 6.5M Wikipedia articles. The pronunciation-\nonly model BORT-PR after about 1.0M articles.\nAdding only the spelling transform forBORT-PR-SP\nnearly tripled the training duration to about 2.8M\narticles, while adding only the noise transform for\nBORT-PR-NOISY actually shortened training to about\n0.6M articles. The combination of spelling and noise\nfor BORT-PR-SP-NOISY trained for about 4.4M articles.\nThe only models which completed a full epoch were\nthose trained without the pronunciation transform, with\nBORT-SP and BORT-SP-NOISY training for about 6.9M\nand 7.9M articles, respectively.\nConfiguration CER Accuracy\nBORT-PR 0.234 0.550\nBORT-PR-SP 0.224 0.516\nBORT-PR-NOISY 0.195 0.618\nBORT-PR-SP-NOISY 0.151 0.645\nTable 2: Character error rates (CER) and accuracies for the\npre-training task, Only those configurations which applied\nthe pronunciation transform are shown.\n3.7 Discussion\nOut of the pre-training configurations we evaluated\nwith CER, the best performance was seen with\nBORT-PR-SP-NOISY, the model trained on all three\ntransforms (phonology, spelling, and noise). This\nindicates that all three transforms were useful for\nthe task of restoring a word from its pronunciation.\nAdditionally, noise was clearly helpful for these models,\nsince the next best configuration also used the noise\ntransform (BORT-PR-NOISY).\nThe evaluation at this stage was lower than we\nexpected, but this can largely be explained in terms\nof how the problem and its evaluation were formulated.\nOur hold-out rules were unusually strict to ensure\nthe model could not memorize any of the words used\nduring fine-tuning, heavily biasing the test data toward\ncommon English words. Second, CER is an imperfect\nevaluation measure for a model which operates on\nsubword tokens, and one which isn’t strictly a P2G\ntranslator. Additionally, as we emphasize in §4,\nevaluation on 1-best is a poor measure of the usefulness\nof a model intended for use in a complex pipeline.\n4 Fine-tuning BORT\n4.1 Data\nFine-tuning data consisted of 2,234 transcripts from\n339 people with aphasia (PW A) from the English\nAphasiaBank database (MacWhinney et al., 2011),\n217\na widely-used repository of recorded and transcribed\nadministrations of a standardized protocol consisting\nof a variety of tasks including discourse tasks, meant\nfor use in studying language and cognitive sequelae\nof post-stroke aphasia.7 Demographic characteristics\nof the participants are included in Appendix A. The\ntranscripts used in this study were from one of nine\ntasks designed to elicit discourse; the tasks themselves\nare described in more detail in Appendix B. The tasks\nwere transcribed by human annotators according to\nthe CHA T transcription manual (Codes for the Human\nAnalysis of Transcripts; MacWhinney 2000).\nIn the AphasiaBank transcripts, non-word para-\nphasias are transcribed in IPA, whereas lexical\nparaphasias are written in their orthographic form. For\neach paraphasia, whenever possible, the human anno-\ntator also identified the target word for that production\n(i.e., the word the person intended to say). Given the\nparaphasia-target pair, the annotator also categorized\nthe paraphasia according to whether it was a real word,\nwhether it was phonologically related to the target, and\nwhether it was semantically related to the target.\nWe filtered the transcripts to just the PW A ’s language,\nand removed annotations irrelevant to the task (e.g.,\ngestures). Then, we prepared the transcripts for training\nin two ways: an “easy” way and a “hard” way. In\nthe “hard” task, the model learned the task that the\nhuman annotators performed in AphasiaBank: to\npredict the target word given the pronunciation of the\nparaphasia (and the surrounding context). In the “easy”\ntask, we instead replaced the paraphasia pronunciation\nwith the correct pronunciation for its associated target\nword. That is, in the “easy” task, we train the model\nto fill in correct pronunciations for words with their\ncorresponding orthographic form, and in the “hard” task\nwe train the model to fill in paraphasias (i.e., incorrect\npronunciations) with the intended orthographic word.\nFor both the “easy” and “hard” tasks, we only consid-\nered paraphasias with a known target provided by the\nhuman annotator. For the “easy” task, we additionally\nremoved paraphasias where there was not a known pro-\nnunciation of the orthographic form of the target in our\npronunciation dictionary. This left us with 10,120 para-\nphasias. With the “hard” task, for the real word para-\nphasias, we instead removed paraphasias where there\nwas not a known pronunciation of the orthographic\nform of theparaphasiain the pronunciation dictionary.\nThis left us with 9,781 paraphasias for the “hard” task.\nEach usable production was prepared with the full\ncontext of its transcript: all productions in the transcript\nwere prepared as phonemes separated by bullets (as\n7Our snapshot of the transcripts was copied from AphasiaBank\non March 8th, 2023, and the corresponding demographic metadata\ndownloaded on April 23, 2023.\nin §3.2), whether it be the target pronunciation or the\nparaphasia itself, and the production for the model to\npredict was marked with surrounding angle brackets.\nSome of these prepared samples were too long when\ntokenized, and thus trimmed to the maximum length\n(1024 tokens) in such a way that the maximum possible\ncontext on either side of the paraphasia was preserved.\nPart of a prepared example with the target “screwed”\nis shown below:\n“Easy”: and it <s·kôu·d > up. but I went\nto to thehAs·pIt@l. and my brains·kæn·d.\n“Hard”: and it <Skôu·d > up. but I went\nto to theAs·pIt@l. and my brains·tæn·d.\nFor “easy,” we substituted the correct pronunciation\nfor screwed (“screwed”), while in “hard” we included\nthe paraphasia (“shcrewed”).\n4.2 Training\nGiven either a correct (“easy”) or incorrect (“hard”)\npronunciation, we fine-tuned each pre-trained BORT\nmodel to predict the intended word. As a baseline,\nwe fine-tuned from the unmodified source model,\nBART-BASE. . As in pre-training, we adapted code\nfrom the fairseq sequence modeling toolkit (Ott\net al., 2019). We used 10-fold cross validation with\nparticipant as the grouping factor, sequentially holding\none fold out as valid set, a second fold as test set, and\nthe remaining eight folds as the training set. We trained\neach model until early stopping occurred using loss on\nthe validation set after 20 epochs without improvement.\nTraining hyperparameters were the same as §3.4 but\nwith an effective batch size of 4000 tokens.\n4.3 Evaluation\nWe evaluated performance for the “easy” and “hard”\ntasks using CER and accuracy. As we did in §3.5, we\ncalculated CER between the top model prediction and\nthe human identified target for each paraphasia in the\ntest set. We also calculated top 1 accuracy (the top\nmodel prediction matched the human identified target)\nand top 5 accuracy (the human identified target was\nwithin the top 5 model predictions) for each of the\nfine-tuned models’ predictions on the test set. We deter-\nmined whether disagreements between top 1 accuracy\nof the different models were significant using McNe-\nmar’s test with continuity correction (McNemar, 1947)\nand Bonferroni correction (Haynes, 2013). We con-\nducted this test for all six models versus the baseline, as\nwell as the best performing model versus all other mod-\nels, for both “easy” and “hard” tasks, leading to 24 com-\nparisons in total. Accounting for multiple comparisons\nand using an alpha of0.05, a p-value of<0.00208was\nretained as the level of statistical significance . Finally,\n218\nfor the best configuration in the “hard” task, we also\ncalculated top 1 accuracy stratified by AphasiaBank\ntask and error type, presented in Appendix C.\n4.4 Results\nCER, top 1 accuracy, and top 5 accuracy on the test set\nis shown for the “easy” and “hard” tasks in Tables 3a\nand 3b respectively. For both “easy” and “hard” tasks,\nall of the BORT models had significantly higher perfor-\nmance thanBART-BASE at top 1 accuracy according to\nMcNemar’s test withp< 0.00208 for all comparisons.\nIn the “easy” task, the baseline configuration\n(BART-BASE) led to top 1 accuracy 72.5%, and CER\n22.8%. Out of the models trained with noise, best\nperformance was seen inBORT-PR-SP-NOISY with\ntop 1 accuracy 89.5%. However,BORT-PR-SP saw\nthe best performance out of all models, with a 90.1%\nchance of correctly predicting the appropriate word\nfor a given correct pronunciation and a CER of just\n5.7%, improving on the baseline by a relative 24%\nand 75%, respectively. The accuracy was significantly\nhigher than all other models withp <0.00208 for\nall comparisons. Allowing for five chances to get the\ncorrect prediction, this model achieved 94.7% accuracy.\nFor the “hard” task, the baseline achieved 36.3%\ntop 1 accuracy and 60.6% CER. Out of the config-\nurations which did not apply noise,BORT-PR-SP\nachieved the highest top 1 accuracy of 45.6% and\nCER 44.7%. The best pre-training configuration\nwas BORT-PR-SP-NOISY, which applied all three\ntransforms. It achieved top 1 accuracy 46.7% and CER\n42.0%, improving on the baseline by a relative 29% and\n31%, respectively. The top 1 accuracy was significantly\nhigher than most models withp <0.00208, except\nfor BORT-PR-SP (p = 0.020) and BORT-PR-NOISY\n(p=0.023). BORT-PR-SP-NOISY also had 65.6% top 5\naccuracy. Accuracy within top 1–20 predictions of\nthe baseline and best performing models for the “easy”\nand “hard” tasks can be seen in Figure 1. Performance\nincreases for all models as we allow more chances to\nfind the correct target, but the order of performance\nremains the same. Additional results—namely those\nstratified by AphasiaBank task and error types—can\nbe found in Appendix C.\n4.5 Discussion\nThe fine-tuning configurations without the pronunci-\nation transform (BORT-SP and BORT-SP-NOISY) were\nthe lowest performing of the BORT models, but they\nstill had significantly higher top 1 accuracy than the\nfine-tuned BART-BASE model. Moreover, for both\n“easy” and “hard” tasks, the best performing models\nwere trained with both the pronunciation and spelling\ntransforms, and either with or without noise. This\npattern implies that there is enough overlap between\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\n# top predicted targets considered\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Accuracy within top 1-20 predicted targets\n0.901\n0.725\n0.467\n0.363\n0.947\n0.820\n0.656\n0.533\nBORT-PR-SP (easy)\nBaseline (easy)\nBORT-PR-SP-NOISY (hard)\nBaseline (hard)\nFigure 1: Accuracy within top 1-20 predictions of\nbaseline and best performing models for “easy” and “hard”\nfine-tuning tasks.\northography and phonology in the English language\nthat pre-training the LLM to spell helped the model\nto perform the task at hand. Considering the G2P\nframe of reference—though we note again that this\nis a loose comparison (see §3.7)—a 5.7% CER is on\npar with what could be expected from a model strictly\ntranslating between phonemes and graphemes.\nAphasic speech is characterized by paraphasias,\nwhich can be considered “noisy” productions, so it\nstands to reason that learning to de-noise productions\nwould help the model with the “hard” fine-tuning\ntask. As we hypothesized, the “hard” task saw the\nbest performance from the pre-training configuration\nwith all three transforms (pronunciation, spelling, and\nnoise), although its performance was notsignificantly\ndifferent than the configuration without spelling\n(BORT-PR-NOISY) or without noise (BORT-PR-SP).\nContrary to what we observed in §3.5, for the “easy”\ntask, noise did not seem to help the models, and the\nbest performing configuration was one that did not\ninclude noise in pre-training (BORT-PR-SP).\nMoreover, the top 1 accuracy performance of\nthis model was significantly different than all other\n“easy” models. This is surprising since the evaluation\nfor pre-training and the “easy” task were quite\nsimilar, being a phoneme-to-grapheme translation task\nwith and without context, respectively. This might\nbe explained by the stricter hold-out rules during\npre-training—we had no vocabulary restrictions during\nfine-tuning—or by the shift in data domain (Wikipedia\nvs. AphasiaBank). It is difficult to say with certainty\nwhy this discrepancy occurred, but perhaps noise was\nmost helpful for language from a very diverse corpora\n(Wikipedia), while in the more constrained tasks from\nAphasiaBank, the more limited vocabulary did not\nbenefit from synthetic variability.\n219\nPre-training CER Accuracy\nConfiguration Top 1 Top 1 Top 5\nBORT-PR 0.083 0.869 0.931\nBORT-SP 0.106 0.843 0.906\nBORT-PR-SP 0.057 0.901 0.947\nBORT-PR-NOISY 0.089 0.863 0.925\nBORT-SP-NOISY 0.096 0.848 0.911\nBORT-PR-SP-NOISY 0.060 0.895 0.947\nBART-BASE 0.228 0.725 0.820\n(a) “Easy” Task\nPre-training CER Accuracy\nConfiguration Top 1 Top 1 Top 5\nBORT-PR 0.462 0.451 0.634\nBORT-SP 0.526 0.401 0.579\nBORT-PR-SP 0.447 0.456 0.641\nBORT-PR-NOISY 0.452 0.458 0.640\nBORT-SP-NOISY 0.469 0.446 0.625\nBORT-PR-SP-NOISY 0.420 0.467 0.656\nBART-BASE 0.606 0.363 0.533\n(b) “Hard” Task\nTable 3: Accuracies for each pre-trained model after fine-tuning to our two tasks. Bold font indicates accuracy was\nsignificantly different from all other models, with the exception of those italicized, according to McNemar’s test.\n5 Conclusion\nIn §3, we pre-trained BORT to accept a mixture of\northography and IPA. During training and validation,\nwe used one to three different transforms of words\n(pronunciation, spelling, noise) and trained the model\nto restore the words to their original form. We directly\nevaluated the four models trained on pronunciation\nusing a test set of the Wikipedia data by testing the\nCER of their performance at restoring a word from\nits pronunciation. In §4, we evaluated all six models\n(and the baseline) by further fine-tuning them to\nrestore words from productions in aphasic speech. This\nallowed us to evaluate the applicability of our mixed\northography/IPA LLM to a clinical task.\nOur best BORT configurations achieved high\naccuracy and low CER rates for the “easy” fine-tuning\ntask, with the fine-tuned accuracy as high as 90%.\nThis indicates we were able to successfully produce\na LLM that can accept both orthography and IPA.\nMoreover, observing differences between accuracy and\nCER revealed that even when our fine-tuned models\nincorrectly predicted the target word, they still may\nhave found close-by words. For instance, considering\nthe “easy” task, the best performing model picked\nthe wrong target 10% of the time, but it achieved an\naverage CER of just 5.7%. This indicates that there\nwere likely instances where even though the LLM\npredicted the wrong target, it picked a similar word\nwith overlapping letters with the target word.\nFuture work will improve on these pre-trained\nmodels. In §3.1 we hypothesized BART’s generative\narchitecture and denoising task were advantages for our\nuse case. With model selection, though, we find certain\ntradeoffs, and we would like to test whether these ad-\nvantages outweigh unique functionalities found in other\nmodels. In particular, models designed to operate at a\ncharacter level (e.g.CANINE, Clark et al., 2022) could\novercome other limitations of BORT (e.g. our explicit\ndemarcation of phonemic and orthographic regions),\nand is perhaps generally well-suited for the task at hand.\nAn additional area of future work will consist of\nexploring alternative training strategies. In the present\nwork, we were quite strict with regard to preparing the\ntest split, withholding the most frequent English words\nbecause they appeared in our AphasiaBank evaluations.\nAs our focus turns more toward downstream tasks, we\nwill update our holdout methods to prioritize a stronger\npre-trained model, holding out only as much data as\nneeded for validation. Further, seeing how our models\ndid not train for an entire pass through the Wikipedia\ndata, we will adjust the training schedule (e.g. a\nramp-up in the learning rate) and task configuration\n(e.g. word transform rates) to ensure we get the most\nout of the pre-training stage.\nFor our aphasia-specific application, we see room\nto improve the noise transform with more strategic\napproaches. Phoneme errors could be made more\nrealistic with statistically or linguistically informed\napproaches (e.g. replacing phonemes with similar\nphonemes). To better prepare a model for semantic\nerrors, whole-word replacements could be made with\nsemantically similar words.\nLimitations\nThe models presented here were trained with the basic\ninventory of English phonemes found in CMUDict.\nHowever, a more fine-grained phonetic analysis\nwould require a pronunciation dictionary with more\nnarrowly defined entries. Additionally, while this\npaper focused on models trained with English-only\nresources (pre-trainedBART-BASE, English Wikipedia\ntext, CMUDict, and the English AphasiaBank), the\ntechniques should be applicable to non-English\nlanguage models as well. Finally, from a clinical\nstandpoint, the model we describe in this paper assumes\nthe existence of transcribed input (from either a manual\nor automated source, discussed in detail in § 2.1); in its\n220\ncurrent form, this represents a limitation to its clinical\nimplementation, though not to its use in research\nsettings with archival or newly-transcribed datasets.\nEthics Statement\nOur use of the AphasiaBank data was governed by the\nTalkBank consortium’s data use agreement, and the\nunderlying recordings were collected and shared with\napproval of the contributing sites’ institutional review\nboards. Limitations exist regarding accents and dialect,\nwhich in turn would affect the scenarios in which a sys-\ntem based on our model could (and should) be used. It\nshould also be noted that these models and any derived\ntechnology are not meant to be tools to diagnose med-\nical conditions, a task best left to qualified clinicians.\nAcknowledgements\nWe thank our anonymous reviewers for their helpful\ninsights and detailed feedback. This work was\nsupported by the National Institute on Deafness and\nOther Communication Disorders of the National\nInstitutes of Health under award 5R01DC015999\n(Principal Investigators: Bedrick & Fergadiotis). The\ncontent is solely the responsibility of the authors and\ndoes not necessarily represent the official views of the\nNational Institutes of Health.\nReferences\nStefanie Abel, Klaus Willmes, and Walter Huber. 2007.\nModel-oriented naming therapy: Testing predictions of\na connectionist model.Aphasiology, 21(5):411–447.\nJoel Adams, Steven Bedrick, Gerasimos Fergadiotis, Kyle\nGorman, and Jan van Santen. 2017. Target word\nprediction and paraphasia classification in spoken\ndiscourse. In BioNLP 2017, pages 1–8, V ancouver,\nCanada,. Association for Computational Linguistics.\nNancy Azevedo, Eva Kehayia, Gonia Jarema, Guylaine\nLe Dorze, Christel Beaujard, and Marc Yvon. 2023. How\nartificial intelligence (AI) is used in aphasia rehabilitation:\nA scoping review.Aphasiology, pages 1–32.\nAparna Balagopalan, Benjamin Eyre, Frank Rudzicz, and\nJekaterina Novikova. 2020. To BERT or not to BERT:\nComparing speech and language-based approaches for\nAlzheimer’s disease detection. InInterspeech 2020,\npages 2167–2171. ISCA.\nCarnegie Mellon University. 2014. Carnegie mellon\nuniversity pronouncing dictionary (CMUDict), version\n0.7b. http://www.speech.cs.cmu.edu/cgi-bin/cmudict.\nMarianne Casilio, Gerasimos Fergadiotis, Alexandra C.\nSalem, Robert C. Gale, Katy McKinney-Bock, and Steven\nBedrick. 2023. Paralg: A paraphasia algorithm for multi-\nnomial classification of picture naming errors.Journal of\nSpeech, Language, and Hearing Research, pages 1–21.\nSoojin Cho-Reyes and Cynthia K. Thompson. 2012. V erb\nand sentence production and comprehension in aphasia:\nNorthwestern Assessment of V erbs and Sentences\n(NA VS).Aphasiology, 26(10):1250–1277.\nJonathan H. Clark, Dan Garrette, Iulia Turc, and John Wiet-\ning. 2022. Canine: Pre-training an efficient tokenization-\nfree encoder for language representation.Transactions of\nthe Association for Computational Linguistics, 10:73–91.\nLisa A. Edmonds and Swathi Kiran. 2006. Effect\nof Semantic Naming Treatment on Crosslinguistic\nGeneralization in Bilingual Aphasia.Journal of Speech,\nLanguage, and Hearing Research, 49(4):729–748.\nJeffrey L. Elman. 1990. Finding structure in time.Cognitive\nScience, 14(2):179–211.\nGerasimos Fergadiotis, Kyle Gorman, and Steven Bedrick.\n2016. Algorithmic classification of five character-\nistic types of paraphasias. American Journal of\nSpeech-Language Pathology, 25(4S):S776–S787.\nKathleen Fraser, Frank Rudzicz, Naida Graham, and Eliza-\nbeth Rochon. 2013. Automatic speech recognition in the\ndiagnosis of primary progressive aphasia. InProceedings\nof the F ourth W orkshop on Speech and Language Process-\ning for Assistive Technologies, pages 47–54, Grenoble,\nFrance. Association for Computational Linguistics.\nKathleen C Fraser, Jed A Meltzer, Naida L Graham, Carol\nLeonard, Graeme Hirst, Sandra E Black, and Elizabeth\nRochon. 2014. Automated classification of primary\nprogressive aphasia subtypes from narrative speech\ntranscripts. Cortex, 55:43–60.\nPhilip Gage. 1994. A new algorithm for data compression.\nC Users Journal, 12(2):23–38.\nRobert Gale, Julie Bird, Yiyi Wang, Jan van Santen, Emily\nPrud’hommeaux, Jill Dolata, and Meysam Asgari. 2021.\nAutomated scoring of tablet-administered expressive\nlanguage tests.Frontiers in Psychology, 12.\nRobert C. Gale, Mikala Fleegle, Gerasimos Fergadiotis,\nand Steven Bedrick. 2022. The post-stroke speech\ntranscription (PSST) challenge. InProceedings of the\nRaPID W orkshop - Resources and ProcessIng of linguistic,\npara-linguistic and extra-linguistic Data from people\nwith various forms of cognitive/psychiatric/developmental\nimpairments - within the 13th Language Resources and\nEvaluation Conference, pages 41–55, Marseille, France.\nEuropean Language Resources Association.\nWinston Haynes. 2013. Bonferroni Correction. In Werner\nDubitzky, Olaf Wolkenhauer, Kwang-Hyun Cho, and\nHiroki Y okota, editors,Encyclopedia of Systems Biology,\npages 154–154. Springer New Y ork, New Y ork, NY .\nYvette Hou, Aileen Zhou, Laura Brooks, Daniella Reid, Lyn\nTurkstra, and Sheila MacDonald. 2023. Rehabilitation\naccess for individuals with cognitive-communication\nchallenges after traumatic brain injury: A co-design\nstudy with persons with lived experience.International\nJournal of Language & Communication Disorders, pages\n1460–6984.12895.\n221\nItay Itzhak and Omer Levy. 2022. Models in a spelling\nbee: Language models implicitly learn the character\ncomposition of tokens. InProceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 5061–5068, Seattle,\nUnited States. Association for Computational Linguistics.\nA Jacks, K L Haley, G Bishop, and T G Harmon. 2019.\nAutomated speech recognition in adult stroke survivors:\nComparing human and computer transcriptions.F olia\nPhoniatrica et Logopaedica, 71(5-6):286–296.\nY e Jia, Heiga Zen, Jonathan Shen, Y u Zhang, and Y onghui\nWu. 2021. PnG BERT: Augmented BERT on Phonemes\nand Graphemes for Neural TTS. InProc. Interspeech\n2021, pages 151–155.\nEdith Kaplan, Harold Goodglass, and Sandra Weintraub,\neditors. 2001. Boston naming test, 2. ed edition.\nLippincott, Williams & Wilkins, Philadelphia.\nDiane L Kendall, Rebecca Hunting Pompon, C Elizabeth\nBrookshire, Irene Minkina, and Lauren Bislick. 2013.\nAn analysis of aphasic naming errors as an indicator of\nimproved linguistic processing following phonomotor\ntreatment. American Journal of Speech-Language\nPathology, 22(2):S240–S249.\nAndrew Kertesz. 2012. Western Aphasia Battery–Revised.\nTechnical report, American Psychological Association.\nType: dataset.\nSwathi Kiran, Carrie Des Roches, Isabel Balachandran, and\nElsa Ascenso. 2014. Development of an Impairment-\nBased Individualized Treatment Workflow Using an\niPad-Based Software Platform.Seminars in Speech and\nLanguage, 35(01):038–050.\nDuc Le, Keli Licata, and Emily Mower Provost. 2017.\nAutomatic Paraphasia Detection from Aphasic Speech:\nA Preliminary Study. InProc. Interspeech 2017, pages\n294–298.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvinine-\njad, Abdelrahman Mohamed, Omer Levy, V eselin\nStoyanov, and Luke Zettlemoyer. 2020. BART:\nDenoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension. In\nProceedings of the 58th Annual Meeting of the Associ-\nation for Computational Linguistics, pages 7871–7880,\nOnline. Association for Computational Linguistics.\nYinghao Aaron Li, Cong Han, Xilin Jiang, and Nima\nMesgarani. 2023. Phoneme-level bert for enhanced\nprosody of text-to-speech with grapheme predictions.\nDuanchen Liu, Zoey Liu, Qingyun Y ang, Y ujing Huang,\nand Emily Prud’hommeaux. 2022. Evaluating the\nperformance of transformer-based language models\nfor neuroatypical language. InProceedings of the 29th\nInternational Conference on Computational Linguistics,\npages 3412–3419, Gyeongju, Republic of Korea.\nInternational Committee on Computational Linguistics.\nHeather MacFarlane, Alexandra C Salem, Steven Bedrick,\nJill K Dolata, Jack Wiedrick, Grace O Lawley, Lizbeth H\nFinestack, Sara T Kover, Angela John Thurman, Leonard\nAbbeduto, and Eric Fombonne. 2023. Consistency\nand reliability of automated language measures across\nexpressive language samples in autism.Autism research\n: official journal of the International Society for Autism\nResearch, 16(4):802–816.\nBrian MacWhinney. 2000.The CHILDES project: Tools\nfor analyzing talk, 3rd edition. Lawrence Erlbaum\nAssociates, Mahwah, NJ.\nBrian MacWhinney, Davida Fromm, Margaret Forbes,\nand Audrey Holland. 2011. Aphasiabank: Methods for\nstudying discourse.Aphasiology, 25(11):1286–1307.\nQuinn McNemar. 1947. Note on the sampling error of the\ndifference between correlated proportions or percentages.\nPsychometrika, 12(2):153–157.\nIrene Minkina, Megan Oelke, Lauren P Bislick, C Elizabeth\nBrookshire, Rebecca Hunting Pompon, Joann P Silkes,\nand Diane L Kendall. 2015. An investigation of aphasic\nnaming error evolution following phonomotor treatment.\nAphasiology, pages 1–19.\nMehryar Mohri, Fernando Pereira, and Michael Riley. 2001.\nWeighted finite-state transducers in speech recognition.\nDepartmental Papers (CIS), page 11.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam\nGross, Nathan Ng, David Grangier, and Michael Auli.\n2019. fairseq: A fast, extensible toolkit for sequence mod-\neling. InProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational\nLinguistics (Demonstrations), pages 48–53, Minneapolis,\nMinnesota. Association for Computational Linguistics.\nMatthew Perez, Zakaria Aldeneh, and Emily Mower Provost.\n2020. Aphasic Speech Recognition Using a Mixture of\nSpeech Intelligibility Experts. InInterspeech 2020, pages\n4986–4990. ISCA.\nUlla Petti, Simon Baker, and Anna Korhonen. 2020. A\nsystematic literature review of automatic Alzheimer’s\ndisease detection from speech and language.Journal of\nthe American Medical Informatics Association : JAMIA,\n27(11):1784–1797.\nAlan Prince and Paul Smolensky. 1997. Optimality:\nFrom neural networks to universal grammar.Science,\n275(5306):1604–1610.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020.\nA primer in BERTology: What we know about how\nBERT works. Transactions of the Association for\nComputational Linguistics, 8:842–866.\nAlexandra C. Salem, Robert Gale, Marianne Casilio, Mikala\nFleegle, Gerasimos Fergadiotis, and Steven Bedrick.\n2022. Refining semantic similarity of paraphasias using a\ncontextual language model.Journal of Speech, Language,\nand Hearing Research, pages 1–15.\n222\nMike Schuster and Kaisuke Nakajima. 2012. Japanese\nand korean voice search. In2012 IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 5149–5152.\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016.\nNeural machine translation of rare words with subword\nunits. In Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics (V olume\n1: Long Papers), pages 1715–1725, Berlin, Germany.\nAssociation for Computational Linguistics.\nMukuntha Narayanan Sundararaman, Ayush Kumar, and\nJithendra V epa. 2021. PhonemeBERT: Joint Language\nModelling of Phoneme Sequence and ASR Transcript.\nIn Proc. Interspeech 2021, pages 3236–3240.\nIván G. Torre, Mónica Romero, and Aitor Álvarez. 2021.\nImproving Aphasic Speech Recognition by Using Novel\nSemi-Supervised Learning Methods on AphasiaBank\nfor English and Spanish.Applied Sciences, 11(19):8872.\nMieke V an De Sandt-Koenderman. 2004. High-tech\nAAC and aphasia: Widening horizons?Aphasiology,\n18(3):245–263.\nMarjo Virnes, Eija Kärnä, and Virpi V ellonen. 2015. Review\nof research on children with autism spectrum disorder\nand the use of technology.Journal of Special Education\nTechnology, 30(1):13–27.\nGrant M. Walker and Gregory Hickok. 2016. Bridging\ncomputational approaches to speech production: The\nsemantic-lexical-auditory-motor model (SLAM).\nPsychonomic Bulletin & Review, 23(2):339–352.\nSevinj Y olchuyeva, Géza Németh, and Bálint Gyires-Tóth.\n2019. Transformer Based Grapheme-to-Phoneme Con-\nversion. InInterspeech 2019, pages 2095–2099. ISCA.\nGuangyan Zhang, Kaitao Song, Xu Tan, Daxin Tan, Y uzi\nY an, Y anqing Liu, Gang Wang, Wei Zhou, Tao Qin,\nTan Lee, and Sheng Zhao. 2022. Mixed-phoneme bert:\nImproving bert with mixed phoneme and sup-phoneme\nrepresentations for text to speech.\nAppendix\nA AphasiaBank Demographics\nDemographic characteristics from the 339 participants\nare summarized in Table 4.\nB Description of AphasiaBank Tasks\nWe used transcripts from nine tasks from AphasiaBank.\nDescriptions of each task are provided in Table 5. More\ninformation can be found on the AphasiaBank website.8\nC Detailed Results\nWe calculated top 1 accuracy for our best performing\nmodel in the “hard” task, BORT-PR-SP-NOISY,\nstratified by AphasiaBank task type. These results are\nshown in Table 6. Performance was lowest for Aphasia-\nBank’s “Free Speech Samples” section with accuracies\nranging from 33%–36%, followed by most of “Picture\nDescriptions” at 41.1%–44.5%, with the exception of\nUmbrella. The best performance was seen in the “Story\nNarrative” and “Procedural Discourse” sections with\n53.3% and 52.2%, respectively, as well as the Umbrella\npicture description (61.8%). This pattern makes sense,\nsince the fine-tuned models can use exposure to the\ntask domain to learn what common vocabulary occurs\nin the tasks. Topics that are very open-ended, like the\nFree Speech Samples, instead could have a large range\nof possible targets for paraphasias.\n8https://aphasia.talkbank.org/protocol/english/\nmaterials-aphasia\n223\nCharacteristic Value\nAge\nM (SD) 61.8 (12.3)\nMin - Max 25.6–90.7\nMissing (N) 3\nRace\nWhite (N) 284\nAfrican American (N) 37\nAsian (N) 2\nHispanic/Latino (N) 9\nNative Hawaiian\n/ Pacific Islander (N) 2\nAmerican Indian\n/ Alaska Native (N) 1\nMixed (N) 2\nOther (N) 1\nUnavailable (N) 1\nGender\nM (N) 199\nF (N) 140\nY ears of Education\nM (SD) 15.4 (2.7)\nMin - Max 8–25\nMissing (N) 14\nCharacteristic Value\nAphasia Duration\nM (SD) 5.4 (5.4)\nMin - Max 0.08–44\nMissing (N) 16\nW AB-R AQ\nM (SD) 71.0 (19.6)\nMin - Max 10.8-99.6\nMissing (N) 37\nBNT-SF\nM (SD) 7.1 (4.6)\nMin - Max 0-15\nMissing (N) 69\nVNT\nM (SD) 14.3 (6.6)\nMin - Max 0-22\nMissing (N) 56\nTable 4: Demographic characteristics for the 339 participants at their first session, where available. W AB-R AQ is the\nWestern Aphasia Battery-Revised Aphasia Quotient, and captures overall aphasia severity with higher values indicating\nlower severity (Kertesz, 2012). BNT-SF is the raw score from the Boston Naming Test-Short Form (Kaplan et al., 2001).\nVNT is the raw score from the V erb Naming Test (Cho-Reyes and Thompson, 2012). The BNT-SF and VNT are both\nconfrontation picture naming tests, where the BNT-SF captures word retrieval deficits of object words and the VNT captures\nword retrieval deficits of action words.\nSection Task Description\nI: Free Speech Samples Speech The participant describes how their speech is currently.\nStroke The participant’s story of his or her stroke.\nImportant Event A personal narrative with a wide range of possible topics.\nII. Picture Descriptions Window A picture description task.\nUmbrella A picture description task.\nCat A picture description task.\nFlood A picture description task.\nIII. Story Narrative Cinderella The participant recounts a narrative of Cinderella, after\nreviewing pictures of central events of it.\nIV . Procedural DiscourseSandwich The participant is asked to describe how to make a peanut\nbutter & jelly sandwich.\nTable 5: Descriptions of nine AphasiaBank tasks\n224\nAphasiaBank Task N Accuracy\nI. Free Speech Samples\nSpeech 265 0.340\nStroke 1620 0.357\nImportant Event 900 0.330\nII. Picture Descriptions\nWindow 711 0.414\nUmbrella 1103 0.618\nCat 1197 0.445\nFlood 180 0.411\nIII. Story Narrative\nCinderella 3065 0.533\nIV . Procedural Discourse\nSandwich 740 0.522\nTable 6: Number of samples (N) and top 1 accuracy for\nBORT-PR-SP-NOISY after fine-tuning to the “hard” task,\nstratified by AphasiaBank task.\nError Type N Accuracy\nPhonological 4557 0.473\nSemantic 3137 0.485\nNeologism 1570 0.445\nMorphological 417 0.350\nDysfluency 11 0.455\nMultiple Types 82 0.488\nUnknown Type 7 0.286\nTable 7: Number of samples (N) and top 1 accuracy for\nBORT-PR-SP-NOISY after fine-tuning to the “hard” task,\nstratified by AphasiaBank error type annotations.\n225",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8067299723625183
    },
    {
      "name": "Pronunciation",
      "score": 0.6817870140075684
    },
    {
      "name": "Natural language processing",
      "score": 0.6505752205848694
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5802791118621826
    },
    {
      "name": "Phonology",
      "score": 0.558475136756897
    },
    {
      "name": "Orthography",
      "score": 0.5522506237030029
    },
    {
      "name": "Language model",
      "score": 0.5019662380218506
    },
    {
      "name": "Orthographic projection",
      "score": 0.46911126375198364
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.44380781054496765
    },
    {
      "name": "Syntax",
      "score": 0.4365866184234619
    },
    {
      "name": "Task (project management)",
      "score": 0.4138282835483551
    },
    {
      "name": "Speech recognition",
      "score": 0.3712989091873169
    },
    {
      "name": "Linguistics",
      "score": 0.33893024921417236
    },
    {
      "name": "Reading (process)",
      "score": 0.12213286757469177
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165690674",
      "name": "Oregon Health & Science University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I126345244",
      "name": "Portland State University",
      "country": "US"
    }
  ],
  "cited_by": 2
}