{
    "title": "Faithful AI in Medicine: A Systematic Review with Large Language Models and Beyond",
    "url": "https://openalex.org/W4389301995",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2126157100",
            "name": "Qianqian Xie",
            "affiliations": [
                "Weill Cornell Medicine",
                "Cornell University"
            ]
        },
        {
            "id": "https://openalex.org/A2548939160",
            "name": "Edward J Schenck",
            "affiliations": [
                "New York Hospital Queens",
                "Presbyterian Hospital",
                "NewYork–Presbyterian Hospital",
                "Cornell University"
            ]
        },
        {
            "id": "https://openalex.org/A2348087827",
            "name": "He S Yang",
            "affiliations": [
                "Cornell University"
            ]
        },
        {
            "id": "https://openalex.org/A2104208150",
            "name": "Yong Chen",
            "affiliations": [
                "California University of Pennsylvania"
            ]
        },
        {
            "id": "https://openalex.org/A2099815532",
            "name": "Yifan Peng",
            "affiliations": [
                "Cornell University",
                "Weill Cornell Medicine"
            ]
        },
        {
            "id": "https://openalex.org/A1983569093",
            "name": "Fei Wang",
            "affiliations": [
                "Weill Cornell Medicine",
                "Cornell University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2895763047",
        "https://openalex.org/W2908201961",
        "https://openalex.org/W4205164650",
        "https://openalex.org/W3205235328",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W4296027312",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W4365143687",
        "https://openalex.org/W3177828909",
        "https://openalex.org/W55204438",
        "https://openalex.org/W4297253404",
        "https://openalex.org/W6601065604",
        "https://openalex.org/W6810242208",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W6600575766",
        "https://openalex.org/W2950304420",
        "https://openalex.org/W6840334356",
        "https://openalex.org/W6850668563",
        "https://openalex.org/W1989335536",
        "https://openalex.org/W4285807172",
        "https://openalex.org/W3101118213",
        "https://openalex.org/W4244323536",
        "https://openalex.org/W4309302776",
        "https://openalex.org/W1502957213",
        "https://openalex.org/W251438302",
        "https://openalex.org/W4255386139",
        "https://openalex.org/W1981276685",
        "https://openalex.org/W1700952868",
        "https://openalex.org/W4323651065",
        "https://openalex.org/W3213990450",
        "https://openalex.org/W4313447794",
        "https://openalex.org/W4327810494",
        "https://openalex.org/W1847618513",
        "https://openalex.org/W4293918660",
        "https://openalex.org/W2489487449",
        "https://openalex.org/W4249111509",
        "https://openalex.org/W4238846128",
        "https://openalex.org/W91851626",
        "https://openalex.org/W4239019441",
        "https://openalex.org/W4287887264",
        "https://openalex.org/W4238371447",
        "https://openalex.org/W2165010366",
        "https://openalex.org/W4241901867",
        "https://openalex.org/W3201023666",
        "https://openalex.org/W3166664235",
        "https://openalex.org/W4360891289",
        "https://openalex.org/W4309674289",
        "https://openalex.org/W4285265395",
        "https://openalex.org/W4377010595",
        "https://openalex.org/W4385456320",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W3098800931",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W648786980",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4287017694",
        "https://openalex.org/W4385546024",
        "https://openalex.org/W4387356888",
        "https://openalex.org/W4385572542",
        "https://openalex.org/W3197043999",
        "https://openalex.org/W1507711477"
    ],
    "abstract": "<title>Abstract</title> Objective While artificial intelligence (AI), particularly large language models (LLMs), offers significant potential for medicine, it raises critical concerns due to the possibility of generating factually incorrect information, leading to potential long-term risks and ethical issues. This review aims to provide a comprehensive overview of the faithfulness problem in existing research on AI in healthcare and medicine, with a focus on the analysis of the causes of unfaithful results, evaluation metrics, and mitigation methods. Materials and Methods Using PRISMA methodology, we sourced 5,061 records from five databases (PubMed, Scopus, IEEE Xplore, ACM Digital Library, Google Scholar) published between January 2018 to March 2023. We removed duplicates and screened records based on exclusion criteria. Results With 40 leaving articles, we conducted a systematic review of recent developments aimed at optimizing and evaluating factuality across a variety of generative medical AI approaches. These include knowledge-grounded LLMs, text-to-text generation, multimodality-to-text generation, and automatic medical fact-checking tasks. Discussion Current research investigating the factuality problem in medical AI is in its early stages. There are significant challenges related to data resources, backbone models, mitigation methods, and evaluation metrics. Promising opportunities exist for novel faithful medical AI research involving the adaptation of LLMs and prompt engineering. Conclusion This comprehensive review highlights the need for further research to address the issues of reliability and factuality in medical AI, serving as both a reference and inspiration for future research into the safe, ethical use of AI in medicine and healthcare.",
    "full_text": "Page 1/23\nFaithful AI in Medicine: A Systematic Review with\nLarge Language Models and Beyond\nQianqian Xie \nWeill Cornell Medicine\nEdward J. Schenck \nNew York-Presbyterian Hospital, Weill Cornell Medical Center\nHe S. Yang \nWeill Cornell Medical College\nYong Chen \nUniversity of Pennsylvania\nYifan Peng \nWeill Cornell Medicine\nFei Wang \nWeill Cornell Medicine\nResearch Article\nKeywords: large language models, factuality and reliability, generative medical AI, evaluation metrics\nPosted Date: December 4th, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-3661764/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nPage 2/23\nAbstract\nObjective\nWhile arti\u0000cial intelligence (AI), particularly large language models (LLMs), offers signi\u0000cant potential for\nmedicine, it raises critical concerns due to the possibility of generating factually incorrect information,\nleading to potential long-term risks and ethical issues. This review aims to provide a comprehensive\noverview of the faithfulness problem in existing research on AI in healthcare and medicine, with a focus\non the analysis of the causes of unfaithful results, evaluation metrics, and mitigation methods.\nMaterials and Methods\nUsing PRISMA methodology, we sourced 5,061 records from \u0000ve databases (PubMed, Scopus, IEEE\nXplore, ACM Digital Library, Google Scholar) published between January 2018 to March 2023. We\nremoved duplicates and screened records based on exclusion criteria.\nResults\nWith 40 leaving articles, we conducted a systematic review of recent developments aimed at optimizing\nand evaluating factuality across a variety of generative medical AI approaches. These include\nknowledge-grounded LLMs, text-to-text generation, multimodality-to-text generation, and automatic\nmedical fact-checking tasks.\nDiscussion\nCurrent research investigating the factuality problem in medical AI is in its early stages. There are\nsigni\u0000cant challenges related to data resources, backbone models, mitigation methods, and evaluation\nmetrics. Promising opportunities exist for novel faithful medical AI research involving the adaptation of\nLLMs and prompt engineering.\nConclusion\nThis comprehensive review highlights the need for further research to address the issues of reliability\nand factuality in medical AI, serving as both a reference and inspiration for future research into the safe,\nethical use of AI in medicine and healthcare.\nIntroduction\nPage 3/23\nArti\u0000cial intelligence (AI) has been gradually applied in different aspects of healthcare and medicine1–3.\nIts bene\u0000ts are seen in various \u0000elds such as quickening medical research, assisting in the detection and\ndiagnosis of diseases, providing tailored health recommendations, and much more (Fig. 1). The success\nof Medical AI has been closely tied to the development of fundamental AI algorithms [4] and the\navailability of various biomedical data, including abundant unlabeled data and labeled data annotated by\nexperts.\nRecently, pre-trained language models (PLMs) [5] such as BERT [6], which are pre-trained on a huge\namount of unlabeled language texts in a self-supervised learning manner (typically based on the new\nbackbone neural network architecture called Transformer [7] ), have revolutionized natural language\nprocessing (NLP). They demonstrated strong potential in Medical AI [5], including tasks beyond NLP like\npathology detection [8, 9]. More recently, large language models (LLMs) [10], exempli\u0000ed by ChatGPT\n[11] and GPT-4 [12] developed by OpenAI, have exhibited impressive capabilities to understand complex\nnatural language input and produce human-like text content. LLMs, with their signi\u0000cantly larger model\nand pre-training data sizes, exhibit remarkably stronger generalizability than PLMs [13], and unlocked\nnew abilities such as complex reasoning and in-context learning; namely LLMs can perform new tasks\nwithout task-speci\u0000c training but seeing a few examples with task-speci\u0000c natural language explanation\ntexts [10]. For example, GPT-4 has been reported to pass the threshold of the United States Medical\nLicensing Exam (USMLE) and showed great potential in assisting clinical decision-making [12, 14]. These\nrecent advancements represent a potential in\u0000ection point for medical AI in several ways: 1) remarkable\ngeneralization ability and in-context learning ability, allowing them to be applied to diverse medical tasks\nwithout task-speci\u0000c training, 2) user-friendly design, accepting natural language inputs, enabling easy\naccess for laypeople and medical professionals, 3) fast growing, constantly push forward the\ndevelopment of medical AI.\nHowever, a key concern is their potential risk of generating non-factual or unfaithful information,\ncommonly referred as the faithfulness problem [15]. Speci\u0000cally, generative AI methods can generate\ncontent that is factually inaccurate or biased. The faithfulness problem can pose long-term risks such as\npreventing medical innovation due to providing wrong guidance in medical research and misallocating\nmedical resources due to providing unfaithful information on disease treatment. Moreover, ethical issues\ncan be caused by the problem, such as deteriorating trust in healthcare by providing misleading medical\ninformation to patients and practitioners. This could lead to unintended consequences, such as\nmisleading clinical decisions and a negative impact on patient care.\nWhat is the Faithfulness Problem?\nDe\u0000nition and Categorization\nGenerative medical AI systems16–18 learn to map from various types of medical data such as EHRs,\nmedical images or protein sequences, to desired output such as the summarization or explanation of\nPage 4/23\nmedical scans and three-dimensional (3D) protein structures. A medical AI system is considered\nunfaithful or to have a factual inconsistency issue, also known as \"hallucination\" in some studies19,20, if it\ngenerates content that is not supported by existing knowledge, reference, or data. The factual\ninconsistency issue in generative medical AI generally can also be categorized into the following two\nmajor types:\nIntrinsic Error: the generated output contradicts existing knowledge, reference, or data. For example,\nif there is a sentence in the radiology report summary generated by the AI system \"The left-sided\npleural effusion has increased in size\", but the actual sentence in the radiology report is \"There is no\nleft pleural effusion\". Then the summary contradicts facts contained in the input data.\nExtrinsic Error: the generated output cannot be con\u0000rmed (either supported or contradicted) by\nexisting knowledge, reference, or data. For example, if the content of the radiology report includes:\n\"There is associated right basilar atelectasis/scarring, also stable. Healed right rib fractures are\nnoted. Linear opacities projecting over the lower lobe are also compatible with scarring, unchanged.\nThere is no left pleural effusion\", and the generated summary includes: \"Large right pleural effusion\nis unchanged in size\". Then this piece of information cannot be veri\u0000ed by the given radiology report,\nsince the information about \"right pleural effusion\" is not mentioned there.\nDifferent medical tasks utilize varied reference standards to evaluate the accuracy and faithfulness of AI-\ngenerated content. For instance: In a text summarization system, where the goal is to condense medical\ndocuments like study protocols, biomedical literature, or clinical notes, the primary reference is the\noriginal document itself. This is to ensure that the summarized content remains true to the source. To\noffer a clearer perspective, Table 1 summarizes the reference standards used in various medical AI tasks\nto evaluate the factual consistency of AI-generated outputs.\nPage 5/23\nTable 1\nThe reference of different exemplar generative medical tasks for evaluating the factual consistency ofthe generated output by AI systems. “Ground truth” means the expected output of the model used fortraining.\nTask Input Ground truth Output Reference\nMedical textsummarization Medical textsSummary written byexperts Shortsummary Input\nRadiology reportsummarization Radiologyreport Impression writtenby radiologists Impression Impression writtenby radiologists\nMedical textsimpli\u0000cation Medical textsSimpli\u0000ed textswritten by experts Simpli\u0000edtexts Input\nMedical dialoguegeneration Dialoguefrom patientsResponse writtenby experts Response Dialogue history\nMedical questionanswering Medicalquestion Correct answerwritten by experts Answering Correct answerwritten by experts\nRadiology reportgeneration Chest X-rayimage Radiology reportwritten byradiologists\nRadiologyreport Radiology reportwritten byradiologists\nWhy is there a Faithfulness Problem?\nThe factual inconsistency of medical AI systems can be attributed to a wide range of reasons.\nLimitation of backbone language model\nPLMs and the latest LLMs, commonly used for representing and processing diverse medical data, have\nlimitations on remembering and recognizing medical facts. Several factors contribute to this: 1) Most\nPLMs and LLMs weren't initially trained with an extensive amount of medical data, making their\nunderstanding of the domain limited. 2) Smaller Scale of Biomedical Training Data: While there are\nmedical-focused PLMs like BioBERT21and BlueBERT22, they're trained on smaller datasets compared to\ngeneral-purpose PLMs and LLMs. 3) Privacy Restrictions: Essential biomedical records, such as\nElectronic Health Records (EHRs) or radiology reports, are often inaccessible for training due to privacy\nconcerns. As a result, existing domain-speci\u0000c PLMs were mainly pre-trained with biomedical literature\ntexts, that are easy to access on a large scale. While abundant, it might not always cover the day-to-day\nrealities of medicine.\nData discrepancy\nThe factual inconsistency problem also arises from the discrepancies between the \"ground truth\" (the\ndesired model output used for training) and the reference standards (the standard used for evaluating\nPage 6/23\nthe factuality of generated outputs) as shown in Table 1 on some medical generation tasks19,20. For\nexample, in the task of text summarization, the 'ground truth' consists of summaries of biomedical\narticles crafted by domain experts. Models are trained to produce summaries that mirror these expert-\nwritten ground truths. When evaluating the model’s outputs for accuracy and factuality, the reference\nused is the comprehensive original content of the biomedical articles. However, a challenge emerges:\nwhile the expert-written summaries aim to encapsulate the main points, they might occasionally omit\ncertain key aspects from the original articles. Consequently, even if a model's output aligns perfectly with\nthe ground truth, it may inadvertently miss details crucial to the reference. This isn't necessarily a \u0000aw in\nthe model's learning but highlights the potential for the discrepancy between the ground truth and the\nreference.\nLimitations of decoding\nAnother cause for the factual inconsistency problem is the limitation of medical AI systems’ decoding\nstrategy, namely the way they generate information. One notable issue is the \"exposure bias\"\nproblem23,24. When training these systems, they learn to create the next piece of information (referred to\nas a 'token', which is like a word or phrase with a speci\u0000c meaning) by looking at previous correct\nexamples. However, in real-world situations, these AI systems can’t always see the correct examples.\nThey must make predictions based on their own past responses. Many of the current methods prioritize\ngenerating a smooth \u0000ow of text over ensuring that every fact they produce is accurate, which might\nincrease the chances of producing incorrect information.\nObjective\nIn this review, we provide an overview of the research on the faithfulness problem in existing medical AI\nstudies, including cause analysis, evaluation metrics, and mitigation methods. We comprehensively\nsummarize the recent progress in maintaining factual correctness in various generative medical AI\nmodels, including knowledge-grounded large-language models, text-to-text generation tasks such as\nmedical text summarization and simpli\u0000cation, multimodality-to-text generation tasks such as radiology\nreport generation, and automatic medical fact-checking. We discuss the challenges of maintaining the\nfaithfulness of AI-generated information in the medical domains, as well as the forthcoming\nopportunities for developing faithful medical AI methods. The goal of this review is to provide\nresearchers and practitioners with a blueprint of the research progress on the faithfulness problem in\nmedical AI, help them understand the importance and challenges of the faithfulness problem, and offer\nthem guidance for using AI methods in medical practice and future research.\nMethods\nSearch Strategy\nPage 7/23\nWe conducted a comprehensive search for articles published between January 2018 to March 2023\nfrom multiple databases, including PubMed, Scopus, IEEE Xplore, ACM Digital Library, and Google\nScholar. We used two groups of search queries: 1) faithful biomedical language models:\nfactuality/faithfulness/hallucination, biomedical/medical/clinical language models,\nbiomedical/medical/clinical knowledge, 2) mitigation methods and evaluation metrics:\nfactuality/faithfulness/hallucination, evaluation metrics, biomedical/medical/clinical summarization,\nbiomedical/medical/clinical text simpli\u0000cation; radiology report summarization, radiology report\ngeneration, medical fact-checking.\nFiltering Strategy\nA total of 5,061 records were sourced from \u0000ve databases, with 2487 articles remaining post-duplicate\nremoval. These were screened based on title, abstract, and exclusion criteria such as being a review\npaper, non-English content, irrelevance to the medical domain, or the factuality issue, leaving 49 articles\n(see Fig. 2 for the full process of article selection). Two independent reviewers (the \u0000rst author and a\nfellow Ph.D. candidate with expertise in NLP and bioinformatics) screened these articles. Before\ninitiating the screening, both reviewers collaboratively de\u0000ned and standardized the exclusion criteria to\nensure an objective and consistent evaluation process. Areas of disagreement between the two\nreviewers were resolved through discussion until a consensus was reached. If an agreement could not\nbe achieved, a third reviewer was consulted (a postdoc with expertise in NLP and bioinformatics). After a\nfull-text review, 9 were further excluded due to low content quality per the GRADE criteria77. The same\ntwo reviewers further adapted GRADE to evaluate individual research articles for quality and relevance.\nThe assessment using GRADE revolved around four main criteria: 1) Risk of Bias: Each article was\nevaluated for potential biases, including selection bias, performance bias, detection bias, attrition bias,\nand reporting bias. Articles that exhibited high risks in multiple domains were \u0000agged for potential\nexclusion. 2) Inconsistency: We checked for any unexplained heterogeneity or variability in study results.\nInconsistencies in the reported results or methodologies without adequate explanations led to a\ndowngrade in quality. 3) Indirectness: Articles that didn’t directly address the objectives of our review or\nthat had populations, interventions, or outcomes that varied signi\u0000cantly from our scope were\nconsidered less direct. 4) Imprecision: Studies with wide con\u0000dence intervals, sparse data, or that didn't\nprovide su\u0000cient detail to assess the reliability of their results were \u0000agged for potential exclusion. For\neach article, the reviewers assigned a quality grade of high, moderate, low, or very low based on the\naccumulation of these criteria. Articles with a 'very low' quality grade, indicating high levels of bias,\ninconsistency, indirectness, or imprecision, were excluded from the review.\nResults\nTo improve the faithfulness of LLMs, many efforts have been focused on improving the backbone model\nwith medical knowledge and optimizing the factual correctness of AI medical systems.\nPage 8/23\nTable 2\nMethods for improving factuality of LLMs in medical.\nPaper Model Research Method EvaluationTask Key Results\nYuan et\nal79\nKeBioLM Improve biomedical PLMswith UMLS BiomedicalNER, RE Improve the performanceof multiple biomedicalPLMs such as BioBERT andPubMedBERT\nJha et\nal80\nBiobert-continual Integrate multipleknowledge bases intobiomedical PLMs\nBiomedicalNER, RE, QA Improve the performanceof multiple biomedicalPLMs such as BioBERT andPubMedBERT\nSinghal\net al25\nMed-PaLM Align a 540B LLM PaLM78\ninto the medical domainby instruction prompttuning\nMedical QA Reduced factual errors inPaLM\nZakka et\nal27\nAlmanac Augment LLMs withknowledge retrieval Medical QA Superior to ChatGPT onanswering 20 medicalquestions\nNori et\nal28\nGPT-4 Comprehensive study onGPT-4 Medical QA Highlighted GPT-4'spotential and limitations,such as factualinconsistency, and biases\nFaithfulness in Large Language Models (LLMs)\nSome efforts (see Table 2) have focused on explicitly incorporating extra medical knowledge to address\nthe factuality problem in domain-speci\u0000c PLMs and LLMs. Yuan et al79 proposed to train a knowledge-\naware language model by infusing entity representations, as well as entity detection and entity linking\npre-training tasks based on the Uni\u0000ed Medical Language System (UMLS) knowledge base. The\nproposed method improves the performance of a series of biomedical language models, such as\nBioBERT and PubMedBERT, on the named entity recognition (NER) and relation extraction (RE) tasks. Jha\net al80 proposed to prob diverse medical knowledge bases into language models with the continual\nknowledge infusion mechanism to avoid forgetting encoded knowledge previously when injecting\nmultiple knowledge bases. This improves several biomedical language models, such as BioBERT and\nPubMedBERT, in medical question answering, named entity recognition and relation extraction. Singhal\net al25 proposed the Med-PaLM that aligns a 540-billion parameter LLM PaLM26 into the medical\ndomains by instruction prompt tuning, which greatly alleviates errors of PaLM on scienti\u0000c grounding,\nharm, and bias from low-quality feedback. Zakka et al27 proposed the knowledge-grounded LLMs\nAlmanac, which retrieves information from medical databases for replying to clinical queries, rather than\ndirectly generating content with LLMs. Almanac has shown better performance than ChatGPT on\nmedical Q&A based on the 20 questions derived from their ClinicalQA dataset and mitigates the factual\ninconsistency problem. Nori et al28 conducted a comprehensive study on the ability of GPT-4 on medical\nPage 9/23\ncompetency examinations and medical Q&A benchmarks recently, where GPT-4 shows much better\nperformance than GPT-3.5. Although their assessment highlights the great potential of GPT-4 in assisting\nhealthcare professionals, they suggested GPT-4 still has a large gap in safe adoption in the medical\ndomain like prior LLMs, due to several limitations such as the risk of error generations, biases, and\nsocietal issues.\nFaithfulness of AI Models in Different Medical Tasks\nMany efforts (see Table 3) have been devoted to optimizing the factuality of generative methods in\nmedicine and healthcare, as well as their factuality evaluation for a speci\u0000c task with various techniques\nsuch as incorporating medical knowledge, reinforcement learning, and prompt learning.\nMedical Text Summarization\nMedical text summarization29–33 is an important generative medical task, with the goal of condensing\nmedical texts such as scienti\u0000c articles, clinical notes, or radiology reports into short summaries.\nEvaluation Metrics Existing commonly used evaluation metrics in text summarization, such as ROUGE36\nand BERTScore45, have been proven to be ineffective in evaluating factual correctness, especially in the\nmedical domain. For RRS, Zhang et al17 proposed the CheXpert F1 score that calculates the overlap of\n14 clinical observations, such as \"enlarged cardiom\" and \"cardiomegaly\", between the generated\nsummary and the reference summary. Delbrouck et al35 further proposed a RadGraph score that\ncalculates the overlap of medical entities and relations46 between the generated summary and the gold\nsummary and can be used for various modalities and anatomies. For the biomedical literature\nsummarization, Wallace et al40 proposed \u0000ndings-Jensen-Shannon Distance (JSD) calculating the\nagreement of evidence directions of the generated summary and reference summary of the systematic\nreview, according to JSD. Based on \u0000ndings-JSD, Deyoung et al38 further proposed the improved metric\nDEI that calculates the agreement of the intervention, outcome, and evidence direction based on the\nJensen-Shannon Distance, between the generated summary and the input medical studies. Otmakhova\net al47 proposed the human evaluation approach for medical study summarization, where they de\u0000ned\nseveral quality dimensions, including PICO correctness, evidence direction correctness, and modality to\nevaluate the factuality of the generated summary. Based on the human evaluation protocol, Otmakhova\net al48 further developed the Dloss to evaluate the factual correctness of the generated summary on\ndifferent aspects such as strong claim, no evidence, no claim, etc., and evidence directions. Adams et\nal49 did a meta-evaluation on existing automatic evaluation metrics, including BARTScore, BERTScore,\nCTC50 and SummaC51 on assessing long-form hospital-course summarization. They found that\nautomatic evaluation metrics correlate well with human annotations in the biomedical domain but\nstruggle to assess factual errors needing deep clinical knowledge, like missingness and incorrectness.\nOptimization Methods The factual inconsistency problem was \u0000rst explored in the radiology report\nsummarization (RRS). Speci\u0000cally, Zhang et al34 found that nearly 30% of radiology report summaries\nPage 10/23\ngenerated from the neural sequence-to-sequence models contained factual errors. To deal with the\nproblem, Zhang et al17 proposed to optimize the factual correctness of the radiology report\nsummarization methods with reinforcement learning (RL). Their method with RL could improve the\nfactuality evaluation metric CheXpert F1 score by 10% when compared with the baseline method.\nDelbrouck et al35 designed a summarization method that optimized the RadGraph score (a factuality\nevaluation metric) reward with RL. Their method could consistently improve the factual correctness and\nquality of the generated summary, where the RadGraph score, CheXpert F1, ROUGE-L36 are improved by\n2.28%-4.96%, 3.61%-5.1%, and 0.28%-0.5%. Xie et al37 proposed the two-stage summarization method\nFactReranker, which aims to select the best summary from all candidates based on their factual\ncorrectness scores by incorporating the medical factual knowledge based on the RadGraph.\nFactReranker achieves the new SOTA on the MIMIC-CXR dataset and improves the RadGraph score,\nF1CheXbert, and ROUGE-L by 4.84%, 4.75%, and 1.5%.\nFor medical studies, Deyoung et al38 found that while BART-based summarizers39 could create \u0000uent\nsummaries for medical studies, maintaining faithfulness was a challenge. For instance, only 54% of the\nsummaries agreed on the intervention’s effect direction as compared to the original systematic review.\nWallace et al40 proposed the decoration and sorting strategy that explicitly informed the model of the\nposition of inputs conveying key \u0000ndings, to improve the factual correctness of the generated summary\nfrom the BART-based summarizer for published reports of randomized controlled trials (RCTs). Alambo41\nstudied the factual inconsistency problem of a transformer-based encoder decoder summarization\nmethod. They proposed to integrate the biomedical named entities detected in input articles and medical\nfacts retrieved from the biomedical knowledge base to improve the model’s faithfulness. Yadav et al42\nproposed to improve the factual correctness of the generated summary for medical questions, by\nmaximizing the question type identi\u0000cation reward and question focus recognition reward with the policy\ngradient approach. Chintagunta et al43 investigated using GPT-3 to generate higher-quality labeled data,\nwhich has proven to be able to train the summarizer with better factual correctness. Liu et al44 proposed\nthe task of automatically generating discharge instructions based on the patients’ electronic health\nrecords and the Re3Writer method for the task, which retrieved related information from discharge\ninstructions of previous patients and medical knowledge to generate faithful patient instructions.\nPage 11/23\nTable 3\nMethods for improving the factuality of medical AI methods for different generation tasks.\nPaper Task Method for Improving Factuality Evaluation Metrics\nZhang et al17 SummarizationRL CheXpert F1, ROUGE\nDelbrouck et\nal35\nSummarizationRL with RadGraph reward RadGraph, CheXpert F1,ROUGE\nXie et al37 SummarizationRerank with RadGraph RadGraph, CheXpert F1,ROUGE\nWallace et al40 Summarizationdecoration and sorting strategy ROUGE, DEI\nAlambo41 SummarizationIncorporate medical NER andfacts ROUGE, UMLS F1,BioBERTScore\nYadav et al42 SummarizationRL maximizes medical questionaccuracy ROUGE\nChintagunta et\nal43\nSummarizationGPT-3 enhances data labelingROUGE, Concept F1,negation F1\nLiu et al44 Summarizationmedical knowledge retrieval andreasoning ROUGE, METEOR\nLu et al52 Simpli\u0000cationprompts enhance key phraseretention ROUGE, BERTScore,BLEURT\nJeblick et at54 Simpli\u0000cationChatGPT Human Evaluation\nLyu et al55 Simpli\u0000cationChatGPT Human Evaluation\nNishino et al58 Generation RL with clinical reconstructionscore ROUGE, CRS\nMiura59 Generation RL with entity matching ROUGE, CheXpert F1,factENT\nDelbrouck et\nal60\nGeneration RL with RadGraph reward CheXpert F1, factENT,\nRadGraph\nNishino et al61 Generation RL with coordinated planning ROUGE, ContentOrdering\nMedical Text Simpli\u0000cation\nMedical text simpli\u0000cation33 aims to simplify highly technical medical texts to plain texts that are easier\nto understand by non-experts such as patients. It can greatly improve the accessibility of medical\ninformation.\nEvaluation Metrics Similar to the medical text summarization, automatic similarity-based evaluation\nmetrics such as ROUGE were also used for evaluating the semantic similarity between outputs and\nPage 12/23\nreferences in the medical text simpli\u0000cation. Other crucial aspects include readability and simplicity,\nevaluated using metrics like the Flesch-Kincaid grade level (FKGL)56. Intuitively, the PLMs pre-trained on\nthe technical corpus can assign higher likelihoods to the technical terms than that pretrained on the\ngeneral corpus. Based on this intuition, Devaraj et al53 proposed a new readability evaluation metric\ncalculating the likelihood scores of input texts with a masked language model trained on the technical\ncorpus. Devaraj et al57 proposed a RoBERTa-based method to classify factual errors in text\nsimpli\u0000cation, like insertions, deletions, and substitutions.\nOptimization Methods Lu et al52 proposed the summarize-then-simplify method for paragraph-level\nmedical text simpli\u0000cation, that uses narrative prompts with key phrases to encourage the factual\nconsistency between the input and the output. Their proposed method signi\u0000cantly outperforms the\nBART-based simpli\u0000cation method53 by 0.49 on the 5-point scale on the factuality of outputs. Jeblick et\nal54 used ChatGPT to simplify 45 radiology reports and had them evaluated by 15 radiologists. The\nradiologists mostly found the outputs factually correct, but noted errors such as misinterpretation of\nmedical terms, imprecise language, and other factual mistakes. Lyu et al55 also evaluated the\nperformance of ChatGPT on simplifying radiology reports to plain language on 62 chest CT screening\nreports and 76 brain MRI screening reports. ChatGPT showed good performance, scoring 4.268 on a 5-\npoint scale by radiologists, with an average of 0.08 and 0.07 instances of missing and inaccurate\ninformation respectively.\nRadiology Report Generation\nRadiology report generation aims to automatically generate radiology reports illustrating clinical\nobservations and \u0000ndings with the input medical images such as chest X-rays and MRI scans. It can help\nto reduce the workload of radiologists and improve the quality of healthcare.\nEvaluation Metrics To evaluate the clinical correctness of the generated radiology reports, some\nefforts58,59,62 proposed to use of the CheXpert-based metric to evaluate the overlap of 14 clinical\nobservations between generated reports and references annotated by the CheXpert. Delbrouck et al60\nproposed the RadGraph score to calculate the overlap of the clinical entities and relations between\ngenerated reports and references annotated by the RadGraph. Recently, Yu et al63 examined the\ncorrelation between existing automatic evaluation metrics including BLEU64, BERTScore, F1 CheXpert,\nand RadGraph F1, and the score given by radiologists on evaluating the factuality of the generated\nreports. They found that the evaluation results of F1 CheXpert and BLEU were not aligned with those of\nradiologists, and BERTScore and RadGraph F1 were more reliable. They further proposed a new\nevaluation metric RadCliQ, which is the weighted sum of the score from BLEU and RadGraph F1 based\non their optimized coe\u0000cients. It showed better alignment with the evaluation of radiologists than the\nabove four metrics.\nOptimization Methods Most existing efforts adopted RL to optimize the factual correctness of radiology\nreport generation methods. Nishino et al58 proposed an RL method, optimizing a clinical reconstruction\nPage 13/23\nscore for improved factual correctness in reports. This method led to a 5.4% improvement in the\nCheXpert factuality metric’s F1 score compared to the model without RL optimization. Miura59 proposed\nan RL method to optimize the entity match score between generated and reference reports, improving\nconsistency. This method signi\u0000cantly improved the CheXpert F1 score by 22.1% compared to baselines.\nDelbrouck et al60 designed the RadGraph reward calculating the overlap of entities and relations between\nthe generated report and the reference, based on the RadGraph dataset including annotated entities and\nrelations of the Chest X-ray reports. The proposed method improves the factuality evaluation metric F1\nRadGraph score by 5.5% on the MIMIC-CXR dataset when compared with baselines59. Nishino et al61\nfurther proposed an RL-based method Coordinated Planning (CoPlan) with the fact-based and\ndescription-order-based evaluator, to encourage the model to generate radiology reports that are\nfactually and chronologically consistent with reference reports. Their method outperforms the baseline\nT5 model on clinical factual accuracy by 9.1%.\nMedical Fact Checking\nAutomatic medical fact-checking, which veri\u0000es the truthfulness of claims in a medical text, is a\npromising tool for detecting and correcting factual errors in medical generative methods. Many existing\nefforts have contributed to the creation of medical fact-checking data resources65–72, such as\nPUBHEALTH65 and HEALTHVER68 et al. Kotonya et al65 proposed an explainable automatic fact-checking\nmethod using a classi\u0000er based on pre-trained language models like BERT, SciBERT73, BioBERT, and a\nBERTSum-based summarization model for generating explanations. On the PUBHEALTH dataset, the\nSciBERT-based method achieved the highest macro F1, precision, and accuracy scores. Wadden et al66\nproposed an automatic fact-checking pipeline that retrieves abstracts based on input claims using TD-\nIDF similarity, selects rationale sentences, and predicts the labels of abstracts relative to the claims\nusing BERT-based models. Among several sentence encoders like SciBERT, BioMedRoBERTa, and\nRoBERTa-base, RoBERTa-large showed the best performance in label prediction. Wadden et al74\nproposed MULTIVERS for predicting fact-checking labels for a claim and corresponding evidence\nabstract. It uses the Long-former75 encoder to process the long sequences and employs multi-task\nlearning to align abstract-level labels with sentence-level ones. MULTIVERS outperformed existing\nmethods in zero-shot and few-shot settings on three medical fact-checking datasets.\nTable 4\nDatasets and methods for medical fact checking.\nPaper Datasets Method for Fact Checking\nKotonya et al65 PUBHEALTH BERT-based classi\u0000er and BERTSum-based summarizer\nWadden et al66 SCI-FACT BERT-based methods\nWadden et al74 Red-HOT Longerformer encoder with multi-task learning\nPage 14/23\nDiscussion\nWith all the reviews of existing works on faithful AI in healthcare and medicine above, we will discuss the\noverall limitations of existing studies and future research directions.\nDatasets\nUnlabeled Data for Self-supervised Learning PLMs and LLMs trained with self-supervised learning,\nrequiring large-scale unlabeled medical data. However, collecting such data is di\u0000cult due to privacy and\ncost considerations. For example, the unlabeled clinical data used to train the clinical PLMs such as\nClinicalBERT76 is 3.7GB, while that used to train LLMs such as GPT-4 can be up to 45TB. Moreover, most\ndatasets are limited to a single language, where English is predominantly used, and a single data\nmodality. This can hinder the development of faithful medical AI methods in low-resource and rural\nareas. Therefore, it’s crucial to develop multimodal, multilingual PLMs to improve the generalization and\nfaithfulness of medical language models, using various languages and data types, such as text and\nimages.\nAnnotated Data for Supervised Learning and Evaluation Developing and evaluating faithful medical AI\nmethods relies on high-quality annotated medical data. Collecting large-scale high-quality annotated\nmedical data is even more challenging, due to the high cost of both time and expertise. Existing\nannotated datasets are usually small, with no publicly accessible data for the meta-evaluation of\nautomated metrics in most medical tasks. This makes it di\u0000cult to verify the reliability of these metrics.\nThus, building domain expert annotated datasets for various medical tasks is essential to analyze metric\nalignment with expert preferences and develop reliable automatic metrics and effective mitigation\nmethods. methods.\nBackbone Models\nBiomedical Domain-Speci\u0000c Language Models Many existing medical AI methods use biomedical PLMs\nand \u0000ne-tune them with task-speci\u0000c datasets for various downstream tasks. However, these models,\ntypically pre-trained with the biomedical literature texts and a few other types of medical texts, capture\nlimited medical knowledge. Moreover, their sizes are typically small (usually less than 1B parameters).\nFor example, up to now, the largest PLMs in the medical domain, PubMedGPT, has 2.7B parameters,\nwhich is far smaller than the scale of LLMs in the general domain (e.g., GPT-3.5 with 175B parameters).\nTherefore, to improve the reliability of biomedical PLMs, future strategies could include larger model\nsizes and multimodal data training.\nLarge Generative Language Models LLMs have shown amazing natural language understanding and\ngeneration abilities. However, they are not mainly trained with data in the medical domain, and none are\npublicly available, hindering the development of reliable medical AI. Therefore, adapting LLMs for the\nmedical \u0000eld is vital, with strategies like \u0000ne-tuning with domain-speci\u0000c data and prompt tuning with\nhuman feedback. There is a recent work25 that aligns the LLM PaLM with\nPage 15/23\nthe medical domain. Unfortunately, it is still not publicly available.\nFaithful Medical AI Methodologies\nMitigation Methods Although factuality is a critical issue in existing medical AI methods, little effort has\nbeen devoted to\nimproving the faithfulness of backbone language models and medical AI methods for downstream\ntasks. No research has investigated the factuality in medical tasks, including medical dialogue\ngeneration, medical question answering, and drug discovery et al. It is also important to develop\nexplainable medical AI methods, especially LLMs. The explanation can play an important role in\nalleviating the hallucination problem. It helps to understand and trace the causes of factual errors,\nmakes it easier to assess factual errors, and enhances medical AI faithfulness.\nIncorporating Medical Knowledge Existing efforts have proven the effectiveness and importance of\nimproving the factuality in both backbone language models and medical AI methods for speci\u0000c tasks by\nincorporating medical knowledge. However, most focused on extracting medical knowledge from\nexternal biomedical knowledge bases. Recently, there has been an effort21 investigating the e\u0000ciency of\ninstruction prompt tuning on injecting medical knowledge into LLMs, which relies on human feedback\nand thus can be expensive and time-consuming. Effective incorporation of medical knowledge in\ne\u0000cient and scalable ways remains a critical challenge.\nEvaluations\nAutomatic Evaluation Metrics Existing automatic evaluation metrics, calculating the overlap of medical\nfacts between outputs generated by the algorithm and references, fail to distinguish different types of\nfactual errors, such as intrinsic and extrinsic errors, as introduced in Section 3.1. In addition, the\nassessment of factuality relies on human evaluations for many tasks, such as medical question\nanswering, without automatic factuality evaluation metrics. Future work should explore \u0000ne-grained\nautomatic metrics to clarify and assess varied medical factual errors and a uni\u0000ed evaluation guideline\nfor standardized criteria across medical tasks.\nMeta-evaluation Assessing the effectiveness of automatic evaluation metrics is critical for correctly\nevaluating the factuality of methods. Otherwise, the ineffective automatic evaluation metrics can\nmisguide the optimization and evaluation of methods. There is rare work63 investigating the meta-\nevaluation of automatic factuality metrics used in various medical tasks and analyzing their alignment\nwith domain experts. Therefore, it is important to conduct the prospective randomized controlled trial\n(RCT) level assessment for these evaluation metrics to evaluate their reliability and effectiveness.\nConclusion\nThe progress of fundamental AI methods, especially the most recent LLMs, provides great opportunities\nfor medical AI, but there is a severe concern about the reliability, safety, and factuality of generated\nPage 16/23\ncontent by medical AI methods. In this review, we provide the \u0000rst comprehensive overview of the\nfaithfulness problem in medical AI, analyzing causes, summarizing mitigation methods and evaluation\nmetrics, discussing challenges and limitations, and outlooking future directions. Existing research on\ninvestigating the factuality problem in medical AI remains in the initial phase, and there are several\nsigni\u0000cant challenges to data resources, backbone models, mitigation methods, and evaluation metrics\nin this research direction. It is clear more future research efforts should be conducted and there are\nsigni\u0000cant opportunities for novel faithful medical AI research involving adapting LLMs, and prompt\nlearning et al. We hope this review can inspire further research efforts in this direction, as well as serve\nas a guide for researchers and practitioners on the safe use of AI methods in realistic medical practice.\nDeclarations\nAuthor Contributions\nQQX, Conceptualization, Literature search, Figures, Writing - original draft. QQX played a key role in\nconceptualizing the paper, managing the collection and organization of relevant literature, and drafting\nthe original manuscript. EJS, Conceptualization, Writing - review & editing. EJS was involved in the\nconceptualization, reviewed, and edited the manuscript. HSY, Conceptualization, Writing - review &\nediting. HSY was involved in the conceptualization, reviewed, and edited the manuscript. YFP, Writing -\nreview & editing. YFP reviewed and edited the manuscript. YC, Writing - review & editing. YC reviewed and\nedited the manuscript. FW, Conceptualization, Supervision, Funding Acquisition, Project Administration,\nWriting - review & editing. FW reviewed the manuscript, provided overall supervision of the project, was\ninstrumental in acquiring funding for the research, and administrated the entire project.\nData Availability\nThe datasets are available from authors upon reasonable request.\nFunding Statement\nThe work was supported by NSF grant number 1750326, NIH grant number R01AG080624,\nR01AG076448, R01AG080991, R01AG076234, and RF1AG072449.\nDeclaration of Interests\nThe authors declare no competing interests.\nReferences\n1. Yu, K.-H., Beam, A. L. & Kohane, I. S. Arti\u0000cial intelligence in healthcare. Nat. biomedical engineering\n2, 719–731 (2018).\n2. Topol, E. J. High-performance medicine: the convergence of human and arti\u0000cial intelligence. Nat.\nmedicine 25, 44–56 (2019).\nPage 17/23\n3. Rajpurkar, P., Chen, E., Banerjee, O. & Topol, E. J. Ai in health and medicine. Nat. medicine 28, 31–38\n(2022).\n4. LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. nature 521, 436–444 (2015).\n5. Wang, B. et al. Pre-trained language models in biomedical domain: A systematic survey. arXiv\npreprint arXiv:2110.05006 (2021).\n\u0000. Kenton, J. D. M.-W. C. & Toutanova, L. K. Bert: Pre-training of deep bidirectional transformers for\nlanguage understanding. In Proceedings of NAACL-HLT, 4171–4186 (2019).\n7. Vaswani, A. et al. Attention is all you need. Adv. neural information processing systems 30 (2017).\n\u0000. Agrawal, M., Hegselmann, S., Lang, H., Kim, Y. & Sontag, D. Large language models are zero-shot\nclinical information extractors. arXiv preprint arXiv:2205.12689 (2022).\n9. Tiu, E. et al. Expert-level detection of pathologies from unannotated chest x-ray images via self-\nsupervised learning. Nat. Biomed. Eng. 1–8 (2022).\n10. Brown, T. et al. Language models are few-shot learners. Adv. neural information processing systems\n33, 1877–1901 (2020).\n11. OpenAI. Chatgpt. https://openai.com/blog/chatgpt (2022).\n12. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023).\n13. Bubeck, S. et al. Sparks of arti\u0000cial general intelligence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712 (2023).\n14. Kung, T. H. et al. Performance of chatgpt on usmle: Potential for ai-assisted medical education using\nlarge language models. PLOS Digit. Heal. 2, e0000198 (2023).\n15. Moor, M. et al. Foundation models for generalist medical arti\u0000cial intelligence. Nature 616, 259–265\n(2023).\n1\u0000. Jumper, J. et al. Highly accurate protein structure prediction with alphafold. Nature 596, 583–589\n(2021).\n17. Zhang, Y., Merck, D., Tsai, E., Manning, C. D. & Langlotz, C. Optimizing the factual correctness of a\nsummary: A study of summarizing radiology reports. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, 5108–5120 (2020).\n1\u0000. Luo, R. et al. Biogpt: generative pre-trained transformer for biomedical text generation and mining.\nBrie\u0000ngs Bioinforma. 23 (2022).\n19. Li, W. et al. Faithfulness in natural language generation: A systematic survey of analysis, evaluation\nand optimization methods. arXiv preprint arXiv:2203.05227 (2022).\n20. Ji, Z. et al. Survey of hallucination in natural language generation. ACM Comput. Surv. (2022).\n21. Lee, J. et al. Biobert: a pre-trained biomedical language representation model for biomedical text\nmining. Bioinformatics 36, 1234–1240 (2020).\n22. Peng, Y., Yan, S. & Lu, Z. Transfer learning in biomedical natural language processing: an evaluation\nof bert and elmo on ten benchmarking datasets. arXiv preprint arXiv:1906.05474 (2019).\nPage 18/23\n23. Bengio, S., Vinyals, O., Jaitly, N. & Shazeer, N. Scheduled sampling for sequence prediction with\nrecurrent neural networks. Adv. neural information processing systems 28 (2015).\n24. Wang, C. & Sennrich, R. On exposure bias, hallucination and domain shift in neural machine\ntranslation. In 2020 Annual Conference of the Association for Computational Linguistics, 3544–\n3552 (Association for Computational Linguistics (ACL), 2020).\n25. Singhal, K. et al. Large language models encode clinical knowledge. arXiv preprint arXiv:2212.13138\n(2022).\n2\u0000. Chowdhery, A. et al. Palm: Scaling language modeling with pathways. arXiv preprint\narXiv:2204.02311 (2022).\n27. Zakka, C., Chaurasia, A., Shad, R. & Hiesinger, W. Almanac: Knowledge-grounded language models\nfor clinical medicine.arXiv preprint arXiv:2303.01229 (2023).\n2\u0000. Nori, H., King, N., McKinney, S. M., Carignan, D. & Horvitz, E. Capabilities of gpt-4 on medical\nchallenge problems. arXiv preprint arXiv:2303.13375 (2023).\n29. Afantenos, S., Karkaletsis, V. & Stamatopoulos, P. Summarization from medical documents: a survey.\nArtif. intelligence medicine 33, 157–177 (2005).\n30. Xie, Q., Luo, Z., Wang, B. & Ananiadou, S. A survey on biomedical text summarization with pre-trained\nlanguage model.\n31. arXiv preprint arXiv:2304.08763 (2023).\n32. Luo, Z., Xie, Q. & Ananiadou, S. Citationsum: Citation-aware graph contrastive learning for scienti\u0000c\npaper summarization. In Proceedings of the ACM Web Conference 2023, 1843–1852 (2023).\n33. Xie, Q., Bishop, J. A., Tiwari, P. & Ananiadou, S. Pre-trained language models with domain knowledge\nfor biomedical extractive summarization. Knowledge-Based Syst. 252, 109460 (2022).\n34. Luo, Z., Xie, Q. & Ananiadou, S. Readability controllable biomedical document summarization. In\nFindings of the Association for Computational Linguistics: EMNLP 2022, 4667–4680 (2022).\n35. Zhang, Y., Ding, D. Y., Qian, T., Manning, C. D. & Langlotz, C. P. Learning to summarize radiology\n\u0000ndings. In Proceedings of the Ninth International Workshop on Health Text Mining and Information\nAnalysis, 204–213 (2018).\n3\u0000. Delbrouck, J.-B., Varma, M. & Langlotz, C. P. Toward expanding the scope of radiology report\nsummarization to multiple anatomies and modalities. arXiv preprint arXiv:2211.08584 (2022).\n37. Lin, C.-Y. Rouge: A package for automatic evaluation of summaries. In Text summarization branches\nout, 74–81 (2004).\n3\u0000. Xie, Q., Zhou, J., Peng, Y. & Wang, F. Factreranker: Fact-guided reranker for faithful radiology report\nsummarization. arXiv preprint arXiv:2303.08335 (2023).\n39. DeYoung, J., Beltagy, I., van Zuylen, M., Kuehl, B. & Wang, L. Ms^2: Multi-document summarization of\nmedical studies. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing, 7494–7513 (2021).\nPage 19/23\n40. Lewis, M. et al. Bart: Denoising sequence-to-sequence pre-training for natural language generation,\ntranslation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, 7871–7880 (2020).\n41. Wallace, B. C., Saha, S., Soboczenski, F. & Marshall, I. J. Generating (factual?) narrative summaries of\nrcts: Experiments with neural multi-document summarization. AMIA Summits on Transl. Sci. Proc.\n2021, 605 (2021).\n42. Alambo, A., Banerjee, T., Thirunarayan, K. & Raymer, M. Entity-driven fact-aware abstractive\nsummarization of biomedical literature. In 2022 26th International Conference on Pattern\nRecognition (ICPR), 613–620 (IEEE, 2022).\n43. Yadav, S., Gupta, D., Abacha, A. B. & Demner-Fushman, D. Reinforcement learning for abstractive\nquestion summarization with question-aware semantic rewards. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Volume 2: Short Papers), 249–255 (2021).\n44. Chintagunta, B., Katariya, N., Amatriain, X. & Kannan, A. Medically aware gpt-3 as a data generator\nfor medical dialogue summarization. In Machine Learning for Healthcare Conference, 354–372\n(PMLR, 2021).\n45. Liu, F. et al. Retrieve, reason, and re\u0000ne: Generating accurate and faithful patient instructions. In\nAdvances in Neural Information Processing Systems.\n4\u0000. Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q. & Artzi, Y. Bertscore: Evaluating text generation with\nbert. In International Conference on Learning Representations.\n47. Jain, S. et al. Radgraph: Extracting clinical entities and relations from radiology reports. In Thirty-\u0000fth\nConference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1).\n4\u0000. Otmakhova, J., Verspoor, K., Baldwin, T. & Lau, J. H. The patient is more dead than alive: exploring\nthe current state of the multi-document summarisation of the biomedical literature. In Proceedings\nof the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), 5098–5111 (2022).\n49. Otmakhova, J., Verspoor, K., Baldwin, T., Yepes, A. J. & Lau, J. H. M3: Multi-level dataset for multi-\ndocument summarisation of medical studies. In Findings of the Association for Computational\nLinguistics: EMNLP 2022, 3887–3901 (2022).\n50. Adams, G., Zucker, J. & Elhadad, N. A meta-evaluation of faithfulness metrics for long-form hospital-\ncourse summarization. arXiv preprint arXiv:2303.03948 (2023).\n51. Deng, M., Tan, B., Liu, Z., Xing, E. & Hu, Z. Compression, transduction, and creation: A uni\u0000ed\nframework for evaluating natural language generation. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing, 7580–7605 (2021).\n52. Laban, P., Schnabel, T., Bennett, P. N. & Hearst, M. A. Summac: Re-visiting nli-based models for\ninconsistency detection in summarization. Transactions Assoc. for Comput. Linguist. 10, 163–177\n(2022).\nPage 20/23\n53. Lu, J., Li, J., Wallace, B. C., He, Y. & Pergola, G. Napss: Paragraph-level medical text simpli\u0000cation via\nnarrative prompting and sentence-matching summarization. arXiv preprint arXiv:2302.05574 (2023).\n54. Devaraj, A., Wallace, B. C., Marshall, I. J. & Li, J. J. Paragraph-level simpli\u0000cation of medical texts. In\nProceedings of the conference. Association for Computational Linguistics. North American Chapter.\nMeeting, vol. 2021, 4972 (NIH Public Access, 2021).\n55. Jeblick, K. et al. Chatgpt makes medicine easy to swallow: An exploratory case study on simpli\u0000ed\nradiology reports. arXiv preprint arXiv:2212.14882 (2022).\n5\u0000. Lyu, Q. et al. Translating radiology reports into plain language using chatgpt and gpt-4 with prompt\nlearning: Promising results, limitations, and potential. arXiv preprint arXiv:2303.09038 (2023).\n57. Kincaid, J. P., Fishburne Jr, R. P., Rogers, R. L. & Chissom, B. S. Derivation of new readability formulas\n(automated readability index, fog count and \u0000esch reading ease formula) for navy enlisted\npersonnel. Tech. Rep., Naval Technical Training Command Millington TN Research Branch (1975).\n5\u0000. Devaraj, A., She\u0000eld, W., Wallace, B. C. & Li, J. J. Evaluating factuality in text simpli\u0000cation. In\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), 7331–7345 (2022).\n59. Nishino, T. et al. Reinforcement learning with imbalanced dataset for data-to-text medical report\ngeneration. In Findings of the Association for Computational Linguistics: EMNLP 2020, 2223–2236\n(2020).\n\u00000. Miura, Y., Zhang, Y., Tsai, E., Langlotz, C. & Jurafsky, D. Improving factual completeness and\nconsistency of image-to-text radiology report generation. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, 5288–5304 (2021).\n\u00001. Delbrouck, J.-B. et al. Improving the factual correctness of radiology report generation with semantic\nrewards. In Findings of the Association for Computational Linguistics: EMNLP 2022, 4348–4360\n(2022).\n\u00002. Nishino, T. et al. Factual accuracy is not enough: Planning consistent description order for radiology\nreport generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural\nLanguage Processing, 7123–7138 (2022).\n\u00003. Liu, G. et al. Clinically accurate chest x-ray report generation. In Machine Learning for Healthcare\nConference, 249–269 (PMLR, 2019).\n\u00004. Yu, F. et al. Evaluating progress in automatic chest x-ray radiology report generation. medRxiv 2022–\n08 (2022).\n\u00005. Papineni, K., Roukos, S., Ward, T. & Zhu, W.-J. Bleu: a method for automatic evaluation of machine\ntranslation. In Proceedings of the 40th annual meeting of the Association for Computational\nLinguistics, 311–318 (2002).\n\u0000\u0000. Kotonya, N. & Toni, F. Explainable automated fact-checking for public health claims. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 7740–\n7754 (2020).\nPage 21/23\n\u00007. Wadden, D. et al. Fact or \u0000ction: Verifying scienti\u0000c claims. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing (EMNLP), 7534–7550 (2020).\n\u0000\u0000. Poliak, A. et al. Collecting veri\u0000ed covid-19 question answer pairs. In Proceedings of the 1st\nWorkshop on NLP for COVID-19 (Part 2) at EMNLP 2020 (2020).\n\u00009. Sarrouti, M., Abacha, A. B., M’rabet, Y. & Demner-Fushman, D. Evidence-based fact-checking of\nhealth-related claims. In Findings of the Association for Computational Linguistics: EMNLP 2021,\n3499–3512 (2021).\n70. Saakyan, A., Chakrabarty, T. & Muresan, S. Covid-fact: Fact extraction and veri\u0000cation of real-world\nclaims on covid-19 pandemic. In Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), 2116–2129 (2021).\n71. Mohr, I., Wührl, A. & Klinger, R. Covert: A corpus of fact-checked biomedical covid-19 tweets. In\nProceedings of the Thirteenth Language Resources and Evaluation Conference, 244–257 (2022).\n72. Srba, I. et al. Monant medical misinformation dataset: Mapping articles to fact-checked claims. In\nProceedings of the 45th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval, 2949–2959 (2022).\n73. Wadhwa, S., Khetan, V., Amir, S. & Wallace, B. Redhot: A corpus of annotated medical questions,\nexperiences, and claims on social media. arXiv preprint arXiv:2210.06331 (2022).\n74. Beltagy, I., Lo, K. & Cohan, A. Scibert: A pretrained language model for scienti\u0000c text. In Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 3615–3620\n(2019).\n75. Wadden, D. et al. Multivers: Improving scienti\u0000c claim veri\u0000cation with weak supervision and full-\ndocument context. In Findings of the Association for Computational Linguistics: NAACL 2022, 61–\n76 (2022).\n7\u0000. Beltagy, I., Peters, M. E. & Cohan, A. Longformer: The long-document transformer. arXiv preprint\narXiv:2004.05150 (2020).\n77. Alsentzer, E. et al. Publicly available clinical bert embeddings. In Proceedings of the 2nd Clinical\nNatural Language Processing Workshop, 72–78 (2019).\n7\u0000. Guyatt, G. H. et al. Grade: an emerging consensus on rating quality of evidence and strength of\nrecommendations. Bmj 336, 924–926 (2008).\n79. Chowdhery, A. et al. Palm: Scaling language modeling with pathways. arXiv preprint\narXiv:2204.02311 (2022).\n\u00000. Yuan, Z., Liu, Y., Tan, C., Huang, S. & Huang, F. Improving biomedical pretrained language models with\nknowledge. In Proceedings of the 20th Workshop on Biomedical Language Processing, 180–190\n(2021).\n\u00001. Jha, K. & Zhang, A. Continual knowledge infusion into pre-trained biomedical language models.\nBioinformatics 38, 494–502 (2022).\nPage 22/23\nFigures\nFigure 1\nThe example of medical data and realistic medical applications with AI.\nPage 23/23\nFigure 2\nThe process of article selection."
}