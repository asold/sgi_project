{
  "title": "Skeletal Keypoint-Based Transformer Model for Human Action Recognition in Aerial Videos",
  "url": "https://openalex.org/W4390872130",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2160023585",
      "name": "Shahab Uddin",
      "affiliations": [
        "National University of Sciences and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1970368816",
      "name": "Tahir Nawaz",
      "affiliations": [
        "National University of Sciences and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2034726871",
      "name": "James Ferryman",
      "affiliations": [
        "University of Reading"
      ]
    },
    {
      "id": "https://openalex.org/A2141925122",
      "name": "Nasir Rashid",
      "affiliations": [
        "National University of Sciences and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2062897828",
      "name": "Md Asaduzzaman",
      "affiliations": [
        "University of Staffordshire"
      ]
    },
    {
      "id": "https://openalex.org/A2631376360",
      "name": "Raheel Nawaz",
      "affiliations": [
        "University of Staffordshire"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3082001659",
    "https://openalex.org/W2184544926",
    "https://openalex.org/W2464027041",
    "https://openalex.org/W2940856955",
    "https://openalex.org/W2957680200",
    "https://openalex.org/W2603773813",
    "https://openalex.org/W4220756271",
    "https://openalex.org/W2905346541",
    "https://openalex.org/W2195342085",
    "https://openalex.org/W3007287549",
    "https://openalex.org/W3206651298",
    "https://openalex.org/W3015761098",
    "https://openalex.org/W4312947882",
    "https://openalex.org/W1830479552",
    "https://openalex.org/W4312245820",
    "https://openalex.org/W3137338164",
    "https://openalex.org/W4225774587",
    "https://openalex.org/W2962730651",
    "https://openalex.org/W2963876278",
    "https://openalex.org/W2963781481",
    "https://openalex.org/W3113067059",
    "https://openalex.org/W4200301490",
    "https://openalex.org/W3034999503",
    "https://openalex.org/W2948058585",
    "https://openalex.org/W2145546283",
    "https://openalex.org/W2120622427",
    "https://openalex.org/W2007057255",
    "https://openalex.org/W2342662179",
    "https://openalex.org/W6682864246",
    "https://openalex.org/W1522734439",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W2963820951",
    "https://openalex.org/W3134909472",
    "https://openalex.org/W3198803207",
    "https://openalex.org/W2990949296",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W2963447094",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W2963315828",
    "https://openalex.org/W3136257968",
    "https://openalex.org/W4296362984",
    "https://openalex.org/W2625286981",
    "https://openalex.org/W3134822545",
    "https://openalex.org/W3009803092",
    "https://openalex.org/W3200235245",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W4386076325",
    "https://openalex.org/W3020175596",
    "https://openalex.org/W2990501806",
    "https://openalex.org/W4385978197",
    "https://openalex.org/W2796633859",
    "https://openalex.org/W2971866817",
    "https://openalex.org/W3185273257",
    "https://openalex.org/W3097558897"
  ],
  "abstract": "Several efforts have been made to develop effective and robust vision-based solutions for human action recognition in aerial videos. Generally, the existing methods rely on the extraction of either spatial features (patch-based methods) or skeletal key points (pose-based methods) that are fed to a classifier. Unlike the patch-based methods, the pose-based methods are generally regarded to be more robust to background changes and computationally efficient. Moreover, at the classification stage, the use of deep networks has generated significant interest within the community; however, the need remains to develop accurate and computationally effective deep learning-based solutions. To this end, this paper proposes a lightweight Transformer network-based method for human action recognition in aerial videos using the skeletal keypoints extracted using YOLOv8. The effectiveness of the proposed method is shown on a well-known public dataset containing 13 action classes, achieving very encouraging performance in terms of accuracy and computational cost as compared to several existing related methods.",
  "full_text": " \nVOLUME XX, 2017 1 \nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000.  \nDigital Object Identifier 10.1109/ACCESS.2017.Doi Number \nSkeletal Keypoint -Based Transformer \nModel for Human Action Recognition in \nAerial Videos \nShahab Uddin 1,2, Tahir Nawaz 1,2, James Ferryman 3, Nasir Rashid 1,2, MD Asaduzzaman 4, \nRaheel Nawaz4 \n1National University of Sciences and Technology, H-12, Islamabad, Pakistan \n2National Centre of Robotics and Automation (NCRA), H-12, Islamabad, Pakistan \n3University of Reading, Reading, UK  \n4Staffordshire University, Stoke-on-Trent, UK \n \nCorresponding author: Tahir Nawaz (e-mail: tahir.nawaz@ceme.nust.edu.pk). \nThis research is supported by the Higher Education Commission of Pakistan under grant number DF 1009 -0031. \nABSTRACT Several efforts have been made to develop effective and robust vision -based solutions for \nhuman aerial action recognition. Generally, the existing methods rely on the extraction of either spatial \nfeatures (patch-based methods) or skeletal key points (pose-based methods) that are fed to a classifier. The \npose-based methods are generally regarded to be more robust to background changes and computationally \nefficient. Moreover, at the classification stage, the use of deep networks has generated significant inte rest \nwithin the community; however, the need remains to develop accurate and computationally effective deep \nlearning-based solutions. To this end, this paper proposes a lightweight Transformer network-based method \nfor human action recognition in aerial vid eos using the skeletal keypoints extracted with YOLOv8. The \neffectiveness of the proposed method is shown on a well-known public dataset containing 13 action classes, \nachieving very encouraging performance in terms of accuracy and computational cost as com pared to \nseveral existing related methods. \nINDEX TERMS Action recognition; Transformer network; Aerial videos; Video surveillance.\nI.  INTRODUCTION \nHuman Action Recognition aims to understand human \nbehavior and has been an active topic among researchers. \nIndeed, it has diverse applications including human-machine \ninterface [1], [2], motion tracking [3], video surveillance [4], \n[5], and crowd monitoring [6], [7].  \nTraditional methods for action recognition used various \nsensing modalities, including accelerometers, \nmagnetometers, and gyroscopes, to capture body movements, \nfrequency of motion, angles and orientation of body parts, \nvelocity, and ac celeration along with some other advance \nfeatures [8]‚Äì[11]. Although these methods are \ncomputationally efficient, robust to noise and illumination \nchanges, and easily implementable, they are limited in terms \nof their scalability, accuracy and adaptability as compared to \nthe computer vision-based methods. With the availability of \nlarge image datasets, the use of computer vision has been the \ntrending choice for action recognition [12]‚Äì[14]. \nSpecifically, vision-based methods for action recognition \nare cla ssified into two major categories: patch -based and \npose-based. Patch-based methods are generally based on the \nextraction of spatial features at frame level, which are further \nprocessed to extract temporal dependencies across the video \nsequence [12]‚Äì[14]. A limitation of the patch -based \napproaches is that they generally have a higher \ncomputational cost associated with feature extraction. On the \nother hand, pose-based methods  rely on using the 2D \nskeleton data, which provides an outline of the human body \njoints without involving scene context, for action recognition \nmethods [15]‚Äì[17]. These methods are generally considered \nto be more robust to background changes and can inherently \nbetter represent bodily movements than patch -based \nmethods. Additionally, re cent advancements in pose \nestimation techniques [18]‚Äì[20] have made it easier to obtain \naccurate points of human joints, even when they are difficult \nto distinguish or are obscured. Moreover, processing skeleton \ndata also requires lesser computational re sources and has a \nlower training time as compared to the patch-based methods. \nIndeed, there has been a lot of interest among research \ncommunity in employing deep learning -based models for \nhuman action recognition using pose information. Some \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354389\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2017 9 \nmethods have been proposed that are built on Transformer -\nbased models [21], [22]  to solve the problem. Other \napproaches [23], [24] relied on using Graph Convolutional \nNetwork (GCN) for extracting temporal dependencies and \ndemonstrated encouraging performance. Howeve r, these \nmethods [21]‚Äì[24] assumed fixed camera settings and may \nnot be directly applicable for the case of aerial videos (with \ntop-downish view) due to significant viewpoint changes plus \nthe movement of UAVs could cause motion blur. \nTo this end, this paper proposes an efficient and light -\nweight deep learning -based model for human action \nrecognition in aerial videos. The proposed method adopts a \ntwo-stage approach. The first stage is based on extracting \nskeletal keypoints using YOLOv8 pose extractor. In the \nsecond stage, the extracted keypoints are then fed to the \nTransformer network to train it on a wide variety of action \ntypes. Indeed, the use of the Transformer-based model with \nskeletal keypoints for aerial videos has been largely \nunexplored. We showed the effectiveness of the proposed \nmethod on a well -known public dataset containing a wide \nrange of action types and evaluated the performance and \ncomputational complexity with encouraging results as \ncompared to several existing related methods. \n \nII. RELATED WORK  \nThere exist several methods that are based on using \ntraditional machine learning approaches with manual feature \ncrafting for action recognition; however, they have \nlimitations in terms of a trade -off between performance \naccuracy and computational cost. For example, in [25] the \nauthors extracted skeletal keypoints using Kinect sensor and \nthen used Hidden Markov Models to find the temporal \nrelations for action recognition. The authors in [26] utilized \noptical flow for the extraction of motion features, which are \nthen fed to SVM to perform classification. Ohn -Bar et al . \n[27] used skeletal data with Histogram of Oriented Gradients \n(HOG) for feature description to perform classification of \nvarious action types with SVM. \nWith the advancements in deep learning and the \navailability of large datasets, most traditional approaches \ntowards action recognition have become less desirable. In \n[28], [29], the authors employed two-streamed network that \nused 2D CNNs on individual frames follo wed by a 1D \nmodule to aggregate the per-frame features. These methods, \nalthough effective, are limited in their ability to encode \ntemporal information due to the use of 2D CNNs. \nAlternatively, the authors of [30] jointly modeled spatial and \ntemporal info rmation by using 3D CNNs. Other \nmodifications of 3D CNNs such as inflating 2D convolution \nkernels [31] or decomposing 3D convolution kernels [32] \nwere proposed to improve the performance. Sultani et al . \n[33] utilized a disjoint multi-task learning approach based on \n3D CNNs to address the action recognition task, when there \nis an availability of a small dataset. They used the game data \nof GTA and FIFA along with GAN generated aerial data \nfrom actual ground data for training, and then the model is \ntested on real aerial data. Kotecha et al.  [34] designed a \nFaster Motion Feature Modeling (FMFM) based system with \nAccurate Action Recognition (AAR) modeling. Their \nproposed system used a cascade of CNN -based models for \nboth FMFM and AAR. Mliki et al. [35] developed a CNN \nbased algorithm that used AlexNet [36] for detection and \nGoogleNet [37] for activity classification with ten classes. \nThe authors in [38] proposed a model that used VGG16 [39] \nas CNN feature extractor for RGB and optical flow images \nand the Lattice LSTM for classification of temporal \ndependencies. Wang et al. [40] also proposed a framework \nnamed Temporal Segment Network (TSN) for action \nrecognition, in which the videos are divided into segments of \nequal length. Then, a sequence of snippets is created from \nthese segments, which can be of variable length. A consensus \nfunction aggregates the outputs of all the snippets to create \nthe final class hypothesis. In [41], the authors designed an \nonboard UAV model for ten different gestures, which used \nYOLOv3-tiny for human detection, OpenPose [18] for pose \nestimation, and DNN for gesture classification. They used \ntheir own data for training and evaluation. In [42], Ahmad et \nal. used YOLOv5 for object detection in frames and \nStochastic Gradient Boosting for action classification with 12 \ndifferent action types [43]. Ding et al.  [44] proposed a \nlightweight model for action recognition in aerial videos. \nThey employed a TCN based met hod, which used \nMobileNetV3 as feature descriptor and attention module for \nfinding temporal relations among the frames. The authors in \n[45] presented an approach towards action recognition by \nusing semi-supervised and unsupervised domain adaptation. \nSrivastava et al. [46] proposed a system for violent action \ndetection using Part Affinity Fields [18] for pose estimation \nand SVM with RBF Kernel for classification. They also \ncreated their own data for training and evaluation. \nMost of the above-mentioned methods used CNN models \nfor the extraction of spatial features and, in some cases, \ntemporal features as well, and generally have a higher \ncomputation complexity and cost; hence, requiring powerful \nGPUs. This makes them less deployable in real -world \napplications involving aerial camera settings. Moreover, the \nuse of Transformer networks [47] is growing with \nencouraging performance in several vision tasks [48], but \nrelatively less explored for solving the human action \nrecognition problem. \n \nIII.  PROPOSED TRANSFORMER-BASED ACTION \nRECOGNITION METHOD \nThe proposed method uses skeletal body keypoints for pose \nestimation that are extracted using YOLOv8. These \nkeypoints are preprocessed to make them compatible to be \nfed to the Transformer network for training and testing. The \nuse of the Transformer -based network is inspired from an \nearlier work [22] that was aimed at ground -based fixed \ncamera setting. The proposed method involved architectural \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354389\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2017 9 \nchanges including data augmentation and removal of dropout \nlayers to adapt it for the application at hand. Next, we \ndescribe the details of the proposed method. \nA. POSE ESTIMATION \nWe employed YOLOv8 pose extractor, which provides 17 \nkeypoints of the whole body. Compared to other pose \nextractors (OpenPose [18], YOLOv7 [49], EfficientPose \n[50]), we practically observed that YOLOv8 pose extractor is \nfaster and more accurate. Figure 1 illustrates the extracted \nkeypoints on a sample image in which a person is performing \na kicking action. Each input video to the pose extractor  has \nthe form of (T, H, W, C), where T is the number of frames; H, \nW and C are the height, width, and number of channels in the \nvideo. The pose extractor returns the output in the form of (T, \nP) for each video, where P represents the extracted \nkeypoints, After the keypoints have been extracted, they are \npreprocessed to be fed to the Transformer model for training \nand classification. \nFigure 1.  Left: Extracted keypoints of the whole body for the ‚ÄòKicking‚Äô \naction using YOLOv7 on a sample image . Right: Extracted keypoints \nshown in the form of a plot. \nB. TRANSFORMER NETWORK \nThe architecture of the Transformer encoder layer is shown \nin Figure 2. The encoder layer is repeated multiple times, \ndepending upon the requirement and architecture. This model \nwas originally developed for language processing to perform \ntask like Neural Machine Translation. It is very efficient in \nterms of keeping track of temporal dependencies in long \nsequences of data. The primary block res ponsible for \nmemory or temporal relation is the Self Attention block. This \nblock finds the temporal relation of every instance with every \nother instance. Figure 3 shows different steps of calculating \nSelf Attention. Q, K and V are linearly transformed \nembedding vectors (or matrices if stacked) of the input \ninstances. Matrix multiplication of Q and K matrices is \ncalculated, which is then scaled as shown in the Figure 3. \nThe scaled values are then passed through a SoftMax layer, \nwhose output is used to calcul ate the final matrix \nmultiplication with V matrix. \n \nFigure 2.  Block diagram of the Transformer encoder layer. \n \n \n \nFigure 3.  Block diagram illustrating different steps required for Self-\nAttention. \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354389\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2017 9 \nThe preprocessed keypoints of each action are divided into \nSK sequences, where each sequence has the form of (T, P); \nwhere T is the number of frames in each sequence (30 in our \ncase) and P shows the keypoints as follows: \n \nùê¥ = (ùëÜ1, ùëÜ2 ‚Ä¶ ‚Ä¶ , ùëÜùêæ);  ùë§‚Ñéùëíùëüùëí ùëÜùëò = (ùëáùëò,ùëñ, ùëÉùëò,ùëñ,ùëó ) Eq (I) \n \nHere, i is the number of frames in a sequence and j is the \nnumber of keypoints in each frame. The Transformer model \nwill extract the temporal features from 30 consecutive frames \nof each sequence. The keypoints  of the frames are first \nlinearly transformed into an Embedding Matrix and are \nadded with an additional Positional Embedding Matrix, \nwhich provides positional information of each frame, \ncreating Xemb. The dimension of Xemb becomes (T, d model), \nwhere dmodel is the embedding dimension of each vector \n(row). The positional Embedding Matrix has learnable \nparameters. Xemb is then used to create Q, K and V matrices as \nshown below: \n ùëÑ = ùëãùëíùëöùëè ùëäùëÑ, Eq (II) \n ùêæ = ùëãùëíùëöùëè ùëäùêæ , Eq (III) \n ùëâ = ùëãùëíùëöùëè ùëäùëâ . Eq (IV) \nWQ, W K and WV have learnable parameters and their \ndimensions are usually the same i.e., dq=dk=dv. So, the \ndimensions of Q, K and V becomes (T, dq=dk=dv), where T is \nthe number of frames in each sequence and d is the \nembedding size of keypoints of each frame. In our case, \ndmodel= dq=dk=dv, which is 26. Q, K and V matrices are used \nto perform attention as shown in Figure 3. This process of \ncreating Q, K, V and attention is repeated h times, where h is \nthe number of heads used in the model. Then the results of all \nthe heads are concatenated and are transformed again by \nanother layer through W0 which has the dimension of (hdv, \ndmodel). So, the output dimensio n of multi -head attention \nbecomes (T, hd v). This output is then passed to a feed \nforward network, which linearly transforms it by the \nfollowing operations: \n \n ùêπùêπ(ùë•) = ùëöùëéùë•(0, (ùë•ùëä1 + ùëè1))ùëä2 + ùëè2, Eq (V) \n \nwhere W1 and W2 has dimensions of (dmodel, dff) and (dff, \ndmodel) respectively, and x is the output of the multi heads. We \nchose dff = 4 dmodel. This whole operation is illustrated in \nFigure 4. This encoder layer is repeated multiple times. \n \n \nIV.  EXPERIMENTAL VALIDATION  \nThis section presents the experimental  validation and \nanalysis of the proposed method, including the description \nof the dataset followed by the analysis of results. \n \nA. DATASET \nWe used a well -known publicly available dataset for \nevaluation., the Drone -Action dataset [51]. This dataset \ncontains 13 classes and a total of 240 high resolution (1920 \nx 1080) videos with 25 frames per second. It is recorded in \nan outdoor environment with a camera mounted on a low \naltitude and low speed drone. Also, it has used 10 different \nactors so as to introduce a l evel of diversity. The dataset \nwas collected on an unsettled road in the midst of a wheat \nfield from varying top -downish viewpoints. The \nbackground wheat field can also pose a challenge \n(background clutter) to the CNN -based feature extraction \napproaches. T he dataset provides three different splits of \ntraining and test datasets, referred to as Split 1, Split 2, and \nSplit 3. Figure 5 shows some representative images, \nshowing all action classes used in the evaluation. \n \n \nFigure 5.  Sample images for each of the 13 action classes of Drone-\nAction dataset, as used in the experimental evaluation.  \nFigure 4. Data flow and the dimensions of matrices at each step of the \nEncoder layer. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354389\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2017 9 \n \nB. RESULTS & ANALYSIS \nWe performed a detailed evaluation of the proposed method \non Drone Action dataset. We experimented with varying \nnumber of Transformer encoder layers and reported the \nresults accordingly. Also, we have performed data \naugmentation by flipping frame keypoints horizontally \n(along y axis) to raise the number of training samples. \nFigures 6, 7, 8 show the confusion matrices for the three \nsplits with four layers encoder architecture based on the \nexperimental evidence given in Table 1, as discussed below \nin this section. It is cle ar from the figure that the proposed \nmodel architecture shows quite encouraging performance for \nall classes except for ‚ÄòHit_Bottle‚Äô, ‚ÄòHit_Stick‚Äô, and ‚ÄòStab‚Äô. \nThis is due to the fact that, in each of these three classes, the \nactions performed appear quite similar with different object \nin hand, i.e., bottle, stick and knife. And since the pose \nestimator extracts keypoints of only body joints, and not the \nobjects being carried, these classes are difficult to \ndistinguish. Also, the model confuses between action s of \n‚ÄòRunning_fb‚Äô and ‚ÄòJogging_fb‚Äô, which appear quite alike too. \nThe obtained performance by the proposed framework is \nreported separately for each split using the standard \nevaluation measures: Precision, Recall, F1 -score, and \nAccuracy (Table 1). The resu lts show that the performance \nis generally encouraging, considering the number and \ndiversity of action classes under consideration. A point to \nhighlight is that the best performance is mostly obtained \nwith four layers (e.g., see the mean performance scores  in \nTable 1). Overall, the best performance is generally \nobtained on Split 2.   \n \n \nFigure 6. Confusion matrix with Split 1 (four encoder layers).  \n \n \nFigure 7. Confusion matrix with Split 2 (four encoder layers). \n \n \nFigure 8. Confusion matrix with Split 3 (four encoder layers). \n \nTable 2 shows the comparison of the proposed method (in \nthe form of the mean performance on all three splits) with the \napproaches (High-Level Pose Features based method (HLPF) \nand Pose -based Convolutional Neural Networks (P -CNN)) \nthat are reported in the original dataset paper [51], as well as \na recent related method that used YOLOv8 pose extractor in \ncombination with the Long Short -Term Memory (LSTM) \nnetwork [52] for action recognition. It is evident that the \nproposed method s hows the best performance in terms of \nPrecision, Recall and F1 -score, and a comparable \nperformance in terms of Accuracy as compared to existing \nmethods. \n \n \n \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354389\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2017 9 \nTable 1.  Performance analysis on each split with different number of \nencoder layers. \n# Of encoder layers 1 2 3 4 5 \nSplit 1 Precision 0.742 0.755 0.719 0.755 0.806 \nRecall 0.714 0.743 0.719 0.741 0.764 \nF1 Score 0.720 0.748 0.717 0.742 0.771 \nAccuracy \n(%) \n68.55 70.48 70.16 71.61 73.71 \nSplit 2 Precision 0.788 0.768 0.844 0.839 0.818 \nRecall 0.752 0.760 0.830 0.812 0.788 \nF1 Score 0.759 0.759 0.832 0.817 0.780 \nAccuracy \n(%) \n73.43 73.75 79.71 80.35 77.62 \nSplit 3 Precision 0.715 0.711 0.734 0.749 0.744 \nRecall 0.718 0.714 0.745 0.771 0.751 \nF1 Score 0.716 0.711 0.732 0.756 0.741 \nAccuracy \n(%) \n70.69 69.90 71.47 74.29 71.94 \nMean Precision 0.748 0.745 0.766 0.781 0.789 \nRecall 0.728 0.739 0.765 0.775 0.767 \nF1 Score 0.732 0.739 0.760 0.772 0.764 \nAccuracy \n(%) \n70.89 71.38 73.78 75.42 74.42 \n \nTable 2.  Comparison of the mean performance of the proposed \nmethod with the existing methods.  \nModel Precision  Recall   F1- score Accuracy  \nHLPF  0.66 0.63 0.63 64.36% \nP-CNN 0.77 0.77 0.77 75.92% \nPose+LSTM 0.77 0.77 0.76 74.67% \nProposed 0.78 0.77 0.77 75.42% \n \nFor a more holistic performance comparisons, in Table 3, \nwe have provided a comparison of the proposed model with \nseveral other deep learning models (3DResNet, ST -GCN, \nResNet101, ResNet18, LSTM) in terms of performance \naccuracy and inference time per sequence of 30 frames. Here, \nwe practically implemented all of these models on Intel Core \ni3-5005U processors (two physical cores of 2.0 GHz each) \nand 4GB of RAM. The proposed model took approximately \n7 minutes for 1 run of training for 100 epochs. The results \nshow that the proposed model outperforms existing models \nboth in terms of accuracy and inference time. \n \nTable 3.  Comparison of complexity in terms of inference time as well \nas accuracy of Transformer model with several existing models.  \nModel Accuracy (%)) Inference Time \n(millisecond/sequence) \n3D ResNet 64.00 41.87 \nST-GCN 60.45 29.67 \nResNet101 63.75 11.45 \nResNet18 66.85 2.23 \nLSTM 68.69 0.71 \nTransformer 75.42 0.56 \nIn Table 4, we also compared the computational \ncomplexity of the proposed Transformer -based model with \nseveral related state-of-the-art models in terms of the number \nof network parameters (in millions) and the number of \nfloating-point operations (FLOPS) (in billions).  It is evident \nthat the proposed method outperforms all of the existing \nmethods by a clear margin. \n \nTable 4.  Comparison of complexity in terms of inference time as well \nas accuracy of Transformer model with several existing models . \nModel Parameters (x106) Flops (x109) \nPoTion [53] 4.75 0.60 \nPA3D [54] 4.81 0.65 \nPose-SlowOnly [55] 2.00 15.9 \nPose-X3D-s [55] 0.24 0.60 \nEfficientGCN-B4 [56] 2.03 15.24 \nTransformer 0.04 0.000011 \n \nV.  CONCLUSIONS \nIn this paper we presented an effective and efficient \nTransformer-based model that used the skeletal (target \npose) information for human action recognition in aerial \nvideos. We utilized the lightweight attention module for \naction classification without the  use of CNNs in order to \nreduce the computational cost and complexity. The skeletal \nkeypoints are extracted using YOLOv8 pose estimator, \nwhich are fed into the Transformer network. The results \nshow that the proposed method achieved very encouraging \nperformance when compared to existing related methods. \nThe key strength of the proposed method is that the \ncomputational complexity is significantly lower as \ncompared to several related methods. This is expected to \nsubstantially reduce the computational cost, making it more \ndeployable in real-world applications. \n \nACKNOWLEDGMENT \nThe authors would like to  thank the Higher Education \nCommission of  Pakistan for  providing the funding to \nperform this research. \nREFERENCES \n \n[1] M. Zhu, T. He, and C. Lee, ‚ÄúTechnologies toward next \ngeneration human machine interfaces: From machine learning \nenhanced tactile sensing to neuromorphic sensory systems,‚Äù \nAppl Phys Rev, vol. 7, no. 3, Sep. 2020, doi: 10.1063/5.0016485.  \n[2] L. lo Presti and M. la Cascia, ‚Äú3D skeleton -based human action \nclassification: A survey,‚Äù Pattern Recognit, vol. 53, pp. 130‚Äì\n147, May 2016, doi: 10.1016/J.PATCOG.2015.11.019.  \n[3] S. N. Paul and Y. J. Singh, ‚ÄúSurvey on Video Analysis of \nHuman Walking Motion,‚Äù International Journal of Signal \nProcessing, Image Processing and Pattern Recognition , vol. 7, \nno. 3, pp. 99‚Äì122, Jun. 2014, doi: 10.14257/IJSIP.2014.7.3.10.  \n[4] J. G. A. Barbedo, ‚ÄúA review on the use of unmanned aerial \nvehicles and imaging sensors for m onitoring and assessing plant \nstresses,‚Äù Drones, vol. 3, no. 2, pp. 1‚Äì27, Jun. 2019, doi: \n10.3390/DRONES3020040.  \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354389\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2017 9 \n[5] L. Li, T. Nawaz, and J. Ferryman, ‚ÄúPerformance analysis and \nformative assessment of visual trackers using PETS critical \ninfrastructure surveillance datasets,‚Äù J Electron Imaging , vol. 28, \nno. 04, p. 1, Jul. 2019, doi: 10.1117/1.JEI.28.4.043004.  \n[6] A. Al-Kaff et al., ‚ÄúVBII-UAV: Vision-based infrastructure \ninspection-UAV,‚Äù Advances in Intelligent Systems and \nComputing, vol. 570, pp. 221‚Äì231, 2017, doi: 10.1007/978-3-\n319-56538-5_24. \n[7] J. Boyle, T. Nawaz, and J. Ferryman, ‚ÄúUsing Deep Siamese \nnetworks for trajectory analysis to extract motion patterns in \nvideos,‚Äù Electron Lett, vol. 58, no. 9, pp. 356‚Äì359, Apr. 2022, \ndoi: 10.1049/ELL2.12460.  \n[8] A. Mimouna, A. ben Khalifa, and N. E. ben Amara, ‚ÄúHuman \naction recognition using triaxial accelerometer data: Selective \napproach,‚Äù 2018 15th International Multi-Conference on \nSystems, Signals and Devices, SSD 2018 , pp. 491‚Äì496, Dec. \n2018, doi: 10.1109/SSD.2018.8570429. \n[9] F. Attal, S. Mohammed, M. Dedabrishvili, F. Chamroukhi, L. \nOukhellou, and Y. Amirat, ‚ÄúPhysical human activity recognition \nusing wearable sensors,‚Äù Sensors (Switzerland), vol. 15, no. 12, \npp. 31314‚Äì31338, Dec. 2015, doi: 10.3390/S15122985 8. \n[10] M. Al-Amin et al., ‚ÄúAction Recognition in Manufacturing \nAssembly using Multimodal Sensor Fusion,‚Äù Procedia Manuf, \nvol. 39, pp. 158‚Äì167, Jan. 2019, doi: \n10.1016/J.PROMFG.2020.01.288.  \n[11] P. Palimkar, V. Bajaj, A. K. Mal, R. N. Shaw, and A. Ghosh, \n‚ÄúUnique Action Identifier by Using Magnetometer, \nAccelerometer and Gyroscope: KNN Approach,‚Äù Advanced \nComputing and Intelligent Technologies , vol. 218, pp. 607‚Äì631, \n2021, doi: 10.1007/978 -981-16-2164-2_48. \n[12] J. Wu, W. Luo, W. Liu, and C. Zhang, ‚ÄúGlobal a nd Local \nDiscriminative Patches Exploiting for Action Recognition,‚Äù \nICASSP, IEEE International Conference on Acoustics, Speech \nand Signal Processing - Proceedings, vol. 2020-May, pp. 3667‚Äì\n3671, May 2020, doi: 10.1109/ICASSP40776.2020.9054282.  \n[13] S. H. Park, J. Tack, B. Heo, J. W. Ha, and J. Shin, ‚ÄúK -centered \nPatch Sampling for Efficient Video Recognition,‚Äù Lecture Notes \nin Computer Science (including subseries Lecture Notes in \nArtificial Intelligence and Lecture Notes in Bioinformatics) , vol. \n13695 LNCS, pp. 160‚Äì176, 2022, doi: 10.1007/978-3-031-\n19833-5_10/COVER. \n[14] N. Ikizler and P. Duygulu, ‚ÄúHuman action recognition using \ndistribution of oriented rectangular patches,‚Äù Lecture Notes in \nComputer Science (including subseries Lecture Notes in \nArtificial Intelligence and Lecture Notes in Bioinformatics) , vol. \n4814 LNCS, pp. 271‚Äì284, 2007, doi: 10.1007/978 -3-540-75703-\n0_19/COVER. \n[15] H. Duan, Y. Zhao, K. Chen, D. Lin, and B. Dai, ‚ÄúRevisiting \nSkeleton-based Action Recognition,‚Äù pp. 2959 ‚Äì2968, Sep. 2022, \ndoi: 10.1109/cvpr52688.2022.00298.  \n[16] S. Sarker, S. Rahman, T. Hossain, S. Faiza Ahmed, L. Jamal, \nand M. A. R. Ahad, ‚ÄúSkeleton -Based Activity Recognition: \nPreprocessing and Approaches,‚Äù Intelligent Systems Reference \nLibrary, vol. 200, pp. 43‚Äì81, 2021, doi: 10.1007/978-3-030-\n68590-4_2/COVER. \n[17] B. Li, C. Tan, J. Wang, R. Qi, P. Qi, and X. Li, ‚ÄúSkeleton -Based \nAction Recognition with UAV Views,‚Äù 2021 3rd International \nConference on Video, Signal and Image Processing , vol. 5, \n2021, doi: 10.1145/3503961.  \n[18] Z. Cao, G. Hidalgo, T. Simon, S. E. Wei, and Y. Sheikh, \n‚ÄúOpenPose: Realtime Multi-Person 2D Pose Estimation Using \nPart Affinity Fields,‚Äù IEEE Trans Pattern Anal Mach Intell , vol. \n43, no. 1, pp. 172‚Äì186, Jan. 2021, doi: \n10.1109/TPAMI.2019.2929257.  \n[19] R. A. G√ºler, N. Neverova, and I. Kokkinos, ‚ÄúDensePose: Dense \nHuman Pose Estimation in the Wild,‚Äù Proceedings of the IEEE \nComputer Society Conference on Computer Vision and Pattern \nRecognition, pp. 7297‚Äì7306, Dec. 2018, doi: \n10.1109/CVPR.2018.00762.  \n[20] H. S. Fang, S. Xie, Y. W. Tai, and C. Lu, ‚ÄúRMPE: Regional \nMulti-person Pose Estimation,‚Äù Proceedings of the IEEE \nInternational Conference on Computer Vision , vol. 2017-\nOctober, pp. 2353‚Äì2362, Dec. 2017, doi: \n10.1109/ICCV.2017.256.  \n[21] C. Plizzari, M. Cannici, and M. Matteucci, ‚ÄúSkeleton-based \naction recognition via spatial and temporal transformer \nnetworks,‚Äù Computer Vision and Image Understanding , vol. \n208‚Äì209, p. 103219, Jul. 2021, doi: \n10.1016/J.CVIU.2021.103219.  \n[22] V. Mazzia, S. Angarano, F. Salvetti, F. An gelini, and M. \nChiaberge, ‚ÄúAction Transformer: A self -attention model for \nshort-time pose-based human action recognition,‚Äù Pattern \nRecognit, vol. 124, p. 108487, Apr. 2022, doi: \n10.1016/J.PATCOG.2021.108487.  \n[23] P. Zhang, C. Lan, W. Zeng, J. Xing, J. Xue,  and N. Zheng, \n‚ÄúSemantics-guided neural networks for efficient skeleton -based \nhuman action recognition,‚Äù Proceedings of the IEEE Computer \nSociety Conference on Computer Vision and Pattern \nRecognition, pp. 1109‚Äì1118, 2020, doi: \n10.1109/CVPR42600.2020.00119.  \n[24] L. Shi, Y. Zhang, J. Cheng, and H. Lu, ‚ÄúTwo -stream adaptive \ngraph convolutional networks for skeleton -based action \nrecognition,‚Äù Proceedings of the IEEE Computer Society \nConference on Computer Vision and Pattern Recognition , vol. \n2019-June, pp. 12018‚Äì12027, Jun. 2019, doi: \n10.1109/CVPR.2019.01230.  \n[25] L. Xia, C. C. Chen, and J. K. Aggarwal, ‚ÄúView invariant human \naction recognition using histograms of 3D joints,‚Äù IEEE \nComputer Society Conference on Computer Vision and Pattern \nRecognition Workshops, pp. 20‚Äì27, 2012, doi: \n10.1109/CVPRW.2012.6239233.  \n[26] S. Danafar, N. G.-C. V. 2007: 8th Asian, and undefined 2007, \n‚ÄúAction recognition for surveillance applications using optic \nflow and SVM,‚Äù Springer, vol. 4844, no. PART 2, pp. 457‚Äì466, \n2007, doi: 10.1007/978 -3-540-76390-1_45. \n[27] E. Ohn-Bar and M. M. Trivedi, ‚ÄúJoint angles similarities and \nHOG2 for action recognition,‚Äù IEEE Computer Society \nConference on Computer Vision and Pattern Recognition \nWorkshops, pp. 465‚Äì470, 2013, doi: 10.1109/CVPRW.2013. 76. \n[28] C. Feichtenhofer, A. Pinz, and A. Zisserman, ‚ÄúConvolutional \nTwo-Stream Network Fusion for Video Action Recognition,‚Äù \nProceedings of the IEEE Computer Society Conference on \nComputer Vision and Pattern Recognition , vol. 2016-December, \npp. 1933‚Äì1941, Dec. 2016, doi: 10.1109/CVPR.2016.213.  \n[29] K. Simonyan and A. Zisserman, ‚ÄúTwo -Stream Convolutional \nNetworks for Action Recognition in Videos,‚Äù Adv Neural Inf \nProcess Syst, vol. 1, no. January, pp. 568 ‚Äì576, Jun. 2014, doi: \n10.48550/arxiv.1406.2199.  \n[30] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, \n‚ÄúLearning Spatiotemporal Features with 3D Convolutional \nNetworks,‚Äù in 2015 IEEE International Conference on \nComputer Vision (ICCV), IEEE, Dec. 2015, pp. 4489‚Äì4497. doi: \n10.1109/ICCV.2015.510.  \n[31] J. Carreira and A. Zisserman, ‚ÄúQuo Vadis, Action Recognition? \nA New Model and the Kinetics Dataset,‚Äù 2017 IEEE Conference \non Computer Vision and Pattern Recognition (CVPR) , pp. 4724‚Äì\n4733, Jul. 2017, doi: 10.1109/CVPR.2017.502.  \n[32] Z. Qiu, T. Yao, and T. Mei, ‚ÄúLearning Spatio-Temporal \nRepresentation with Pseudo-3D Residual Networks,‚Äù 2017 IEEE \nInternational Conference on Computer Vision (ICCV) , pp. \n5534‚Äì5542, Oct. 2017, doi: 10.1109/ICCV.2017.590.  \n[33] W. Sultani and M. Shah, ‚ÄúHuman action recognition in d rone \nvideos using a few aerial training examples,‚Äù Computer Vision \nand Image Understanding , vol. 206, p. 103186, May 2021, doi: \n10.1016/J.CVIU.2021.103186.  \n[34] K. Kotecha, D. Garg, B. Mishra, P. Narang, and V. K. Mishra, \n‚ÄúBackground Invariant Faster Motio n Modeling for Drone \nAction Recognition,‚Äù Drones 2021, Vol. 5, Page 87 , vol. 5, no. \n3, p. 87, Aug. 2021, doi: 10.3390/DRONES5030087.  \n[35] H. Mliki, F. Bouhlel, and M. Hammami, ‚ÄúHuman activity \nrecognition from UAV-captured video sequences,‚Äù Pattern \nRecognit, vol. 100, p. 107140, Apr. 2020, doi: \n10.1016/J.PATCOG.2019.107140.  \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354389\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2017 9 \n[36] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImageNet \nclassification with deep convolutional neural networks,‚Äù \nCommun ACM, vol. 60, no. 6, pp. 84‚Äì90, Jun. 2017, doi: \n10.1145/3065386. \n[37] C. Szegedy et al., ‚ÄúGoing deeper with convolutions,‚Äù \nProceedings of the IEEE Computer Society Conference on \nComputer Vision and Pattern Recognition , vol. 07-12-June-\n2015, pp. 1‚Äì9, Oct. 2015, doi: 10.1109/CVPR.2015.7298594.  \n[38] L. Sun, K. Jia, K. Chen, D. Y. Yeung, B. E. Shi, and S. Savarese, \n‚ÄúLattice Long Short-Term Memory for Human Action \nRecognition,‚Äù Proceedings of the IEEE International \nConference on Computer Vision, vol. 2017-October, pp. 2166‚Äì\n2175, Dec. 2017, doi: 10.1109/ ICCV.2017.236. \n[39] K. Simonyan and A. Zisserman, ‚ÄúVery Deep Convolutional \nNetworks for Large-Scale Image Recognition,‚Äù 3rd \nInternational Conference on Learning Representations, ICLR \n2015 - Conference Track Proceedings , Sep. 2014, doi: \n10.48550/arxiv.1409.1556. \n[40] L. Wang et al., ‚ÄúTemporal Segment Networks for Action \nRecognition in Videos,‚Äù IEEE Trans Pattern Anal Mach Intell , \nvol. 41, no. 11, pp. 2740‚Äì2755, May 2017, doi: \n10.48550/arxiv.1705.02953.  \n[41] C. Liu and T. Szir√°nyi, ‚ÄúReal -Time Human Detection and \nGesture Recognition for On-Board UAV Rescue,‚Äù Sensors 2021, \nVol. 21, Page 2180, vol. 21, no. 6, p. 2180, Mar. 2021, doi: \n10.3390/S21062180. \n[42] T. Ahmad, M. Cavazza, Y. Matsuo, and H. Prendinger, \n‚ÄúDetecting Human Actions in Drone Images Using YoloV5 a nd \nStochastic Gradient Boosting,‚Äù Sensors 2022, Vol. 22, Page \n7020, vol. 22, no. 18, p. 7020, Sep. 2022, doi: \n10.3390/S22187020. \n[43] M. Barekatain et al., ‚ÄúOkutama-Action: An Aerial View Video \nDataset for Concurrent Human Action Detection,‚Äù IEEE \nComputer Society Conference on Computer Vision and Pattern \nRecognition Workshops, vol. 2017-July, pp. 2153‚Äì2160, Aug. \n2017, doi: 10.1109/CVPRW.2017.267.  \n[44] M. Ding, N. Li, Z. Song, R. Zhang, X. Zhang, and H. Zhou, ‚ÄúA \nLightweight Action Recognition Method for Unma nned-Aerial-\nVehicle Video,‚Äù 2020 IEEE 3rd International Conference on \nElectronics and Communication Engineering, ICECE 2020 , pp. \n181‚Äì185, Dec. 2020, doi: 10.1109/ICECE51594.2020.9353008.  \n[45] J. Choi, G. Sharma, M. Chandraker, and J. bin Huang, \n‚ÄúUnsupervised and semi-supervised domain adaptation for \naction recognition from drones,‚Äù Proceedings - 2020 IEEE \nWinter Conference on Applications of Computer Vision, WACV \n2020, pp. 1706‚Äì1715, Mar. 2020, doi: \n10.1109/WACV45572.2020.9093511.  \n[46] A. Srivastava, T. Badal, A. Garg, A. Vidyarthi, and R. Singh, \n‚ÄúRecognizing human violent action using drone surveillance \nwithin real-time proximity,‚Äù J Real Time Image Process , vol. 18, \nno. 5, pp. 1851‚Äì1863, Oct. 2021, doi: 10.1007/S11554 -021-\n01171-2. \n[47] A. Vaswani et al., ‚ÄúAttention is All you Need,‚Äù Adv Neural Inf \nProcess Syst, vol. 30, 2017. \n[48] K. Han et al., ‚ÄúA Survey on Vision Transformer,‚Äù IEEE Trans \nPattern Anal Mach Intell, vol. 45, no. 1, pp. 87‚Äì110, Jan. 2023, \ndoi: 10.1109/TPAMI.2022.3152247.  \n[49] C.-Y. Wang, A. Bochkovskiy, and H.-Y. M. Liao, ‚ÄúYOLOv7: \nTrainable bag-of-freebies sets new state-of-the-art for real-time \nobject detectors,‚Äù Jul. 2022, doi: 10.48550/arxiv.2207.02696.  \n[50] D. Groos, H. Ramampiaro, and E. A. Ihlen, ‚ÄúEfficientPose : \nScalable single-person pose estimation,‚Äù Applied Intelligence, \nvol. 51, no. 4, pp. 2518‚Äì2533, Apr. 2021, doi: 10.1007/S10489 -\n020-01918-7. \n[51] A. G. Perera, Y. W. Law, and J. Chahl, ‚ÄúDrone -action: An \noutdoor recorded drone video dataset for action recogn ition,‚Äù \nDrones, vol. 3, no. 4, pp. 1‚Äì16, Dec. 2019, doi: \n10.3390/DRONES3040082.  \n[52] S. M. Mustafa, H. Akbar, T. Nawaz, H. Elahi, U. S. Khan , \n‚ÄúBody Pose-Guided Action Recognition with Convolutional \nLSTM in Aerial Videos,‚Äù Applied Sciences, vo. 13, no. 16, 2023.  \n[53] V. Choutas, P. Weinzaepfel, J. Revaud, and C. Schmid, \n‚ÄúPoTion: Pose MoTion Representation for Action Recognition,‚Äù \nProceedings of the IEEE Computer Society Conference on \nComputer Vision and Pattern Recognition , pp. 7024‚Äì7033, Dec. \n2018, doi: 10.1109/CVPR.2018.00734.  \n[54] A. Yan, Y. Wang, Z. Li, and Y. Qiao, ‚ÄúPA3D: Pose -action 3D \nmachine for video recognition,‚Äù Proceedings of the IEEE \nComputer Society Conference on Computer Vision and Pattern \nRecognition, vol. 2019-June, pp. 7914‚Äì7923, Jun. 2019, doi: \n10.1109/CVPR.2019.00811.  \n[55] H. Duan, Y. Zhao, K. Chen, D. Lin, and B. Dai, ‚ÄúRevisiting \nSkeleton-based Action Recognition,‚Äù Proceedings of the IEEE \nComputer Society Conference on Computer Vision and Pattern \nRecognition, vol. 2022-June, pp. 2959‚Äì2968, Apr. 2021, doi: \n10.1109/CVPR52688.2022.00298.  \n[56] Y. F. Song, Z. Zhang, C. Shan, and L. Wang, ‚ÄúConstructing \nStronger and Faster Baselines for Skeleton -based Action \nRecognition,‚Äù IEEE Trans Pattern Anal Mach Intell , Feb. 2022, \ndoi: 10.1109/TPAMI.2022.315 7033. \n  \n \n \nSHAHAB UDDIN  completed his Bachelor of \nScience in Electrical  Engineering from the \nUniversity of Engineering and Technology  \nPeshawar, Pakistan. His research interest s include \nmachine learning and computer vision . He \ndeveloped innovative solutions while working  in \nseveral projects and  participated in many \nworkshops while working at the National Center of \nRobotics & Automation, National University of \nSciences & Technology, Islamabad . Currently, he \nis working as a Research assistant at NUST \nCollege of Aeronautical Engineering, PAF Academy Asghar Khan, \nRisaplur. \n \n \n \nTAHIR NAWAZ  received the M.Sc. degree in \ncomputer vision and robotics under the Erasmus \nMundus Scholarship, a joint master‚Äôs program \nfrom Heriot -Watt University (U.K.), University  \nof Girona (Spain), and University of Burgundy \n(France), and the Ph.D. degree with a \nspecialization in computer vision, a joint \nDoctoral program under the highly prestigious \nErasmus Mundus Fellowship from the Queen \nMary University of London (U.K.) and Alpe n-\nAdria University of Klagenfurt (Austria). He has a strong demonstrated \ntrack record of research and development in the areas of computer vision \n(visible/thermal imagery) and artificial intelligence, with more than 15 \nyears of experience of working in aca demic and industrial sectors across \nmultiple European countries in prestigious organizati ons. He is currently \nworking as an Associate Professor and the Head of Department (Research) \nat the College of Electrical and Mechanical Engineering, National \nUniversity of Sciences and Technology (NUST), Pakistan, where his core \ninterests focus around multi -modal sensing techniques, particularly \ninvestigating cutting -edge technologies pertaining to automated video \nsurveillance and autonomous vehicles. In 2005, he also represented \nPakistan as a Team Leader in Asia ‚ÄìPacific Broadcasting Union (ABU) \nRobocon Contest (an international robot competition), held at Beijing, \nChina. He has published more than 27  papers in prestigious publication \nvenues and has been involved in sev eral international funded projects.  \n \n \n \n \n \n \n \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354389\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \nVOLUME XX, 2017 9 \n \nJAMES FERRYMAN  is a Professor of \nComputational Vision at the University of \nReading, UK. His current research is \nconcerned with the automatic visual \nsurveillance of wide area scenes using \ncomputational vision. T he research has \ncontributed new results in the areas of model -\nbased vision, visual tracking and surveillance, \nespecially using 3D deformable models. \n \n \n \n \nNASIR RASHID  received the B.E. degree \n(Hons.) in Mechanical Engineering from the \nCollege of Electrical and Mechanical \nEngineering (EME), Islamabad, Pakistan, in \n1993, and the M.S. and Ph.D. degrees in \nMechatronics Engineering from the College of \nElectrical and Mechanical Engineering, National \nUniversity of Sciences and Technology (NUST), \nPakistan. He is currently working as an Associate \nProfessor and the Dean at the College of \nElectrical and Mechanical Engineering, NUST. \nHis field of specialization is artificial intelligence \nwith applications in biomedical engineerin g and machine vision. His Ph.D. \nwas in the field of non -invasive brain signal classification (biomedical \nengineering). He has vast experience as a professional engineer in \nPakistan. He is a Lifetime Member of the Pakistan Engineering  Council. \n \n \n \nMD ASADUZ ZAMAN is currently an \nAssociate Professor in Operational Research at \nStaffordshire University, Stoke -on-Trent, U.K., \nwhere he joined as a Lecturer in 2014, and was \npromoted to Senior Lecturer in 2017. He has a \nbackground in Statistics and Operational \nResearch (OR). He received the B.Sc. (Hon‚Äôs) \nand M.Sc. degree in Applied Statistics from the \nUniversity of Dhaka, Dhaka, Bangladesh, in \n1999 and 2001, respectively, the M.Sc. degree \nin Bioinformatics from the Chalmers University of Technology, \nGothenburg, Swede n, in 2007, and the Ph.D. degree in Operational \nResearch from the University of Westminster, London,  UK, in 2010. \n \n \nRAHEEL NAWAZ  is the Pro Vice \nChancellor (Digital Transformation) and a \nProfessor at the Staffordshire University, UK. \nIn the past roles, he has led large teaching \nprovisions, research centres, and work -based \nlearning teams. He has extensive experience of \nestablishing international research and teaching \ncollaborations. Prior to his appointment, he \nspent nearly a decade performing senior roles \nin the public HE sector (Russell Group and \nmodern universities). Before that, he spent \nmany years in leadership roles in the priv ate further and higher education \nsector, including top positions in some of the largest private further and \nhigher education organisations in the UK. \n \n \n \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354389\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8197540044784546
    },
    {
      "name": "Action recognition",
      "score": 0.7751747965812683
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7206597328186035
    },
    {
      "name": "Transformer",
      "score": 0.634872317314148
    },
    {
      "name": "Classifier (UML)",
      "score": 0.6137885451316833
    },
    {
      "name": "Feature extraction",
      "score": 0.555566132068634
    },
    {
      "name": "Machine learning",
      "score": 0.5124436020851135
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.45549970865249634
    },
    {
      "name": "Engineering",
      "score": 0.08730068802833557
    },
    {
      "name": "Class (philosophy)",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I929597975",
      "name": "National University of Sciences and Technology",
      "country": "PK"
    },
    {
      "id": "https://openalex.org/I71052956",
      "name": "University of Reading",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I198012923",
      "name": "University of Staffordshire",
      "country": "GB"
    }
  ],
  "cited_by": 11
}