{
  "title": "Building an Otoscopic screening prototype tool using deep learning",
  "url": "https://openalex.org/W2991130772",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2114645898",
      "name": "Devon Livingstone",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": null,
      "name": "Aron S. Talai",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A2114899727",
      "name": "Justin Chau",
      "affiliations": [
        "University of Calgary"
      ]
    },
    {
      "id": "https://openalex.org/A221650461",
      "name": "Nils D. Forkert",
      "affiliations": [
        "University of Calgary"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2089031163",
    "https://openalex.org/W1518394255",
    "https://openalex.org/W2058578323",
    "https://openalex.org/W2155075970",
    "https://openalex.org/W2132767035",
    "https://openalex.org/W2137246499",
    "https://openalex.org/W2257166582",
    "https://openalex.org/W2557738935",
    "https://openalex.org/W2742415644",
    "https://openalex.org/W2781924583",
    "https://openalex.org/W2807593075",
    "https://openalex.org/W2806487945",
    "https://openalex.org/W2593892063",
    "https://openalex.org/W2795247881"
  ],
  "abstract": "Background Otologic diseases are often difficult to diagnose accurately for primary care providers. Deep learning methods have been applied with great success in many areas of medicine, often outperforming well trained human observers. The aim of this work was to develop and evaluate an automatic software prototype to identify otologic abnormalities using a deep convolutional neural network. Material and methods A database of 734 unique otoscopic images of various ear pathologies, including 63 cerumen impactions, 120 tympanostomy tubes, and 346 normal tympanic membranes were acquired. 80% of the images were used for the training of a convolutional neural network and the remaining 20% were used for algorithm validation. Image augmentation was employed on the training dataset to increase the number of training images. The general network architecture consisted of three convolutional layers plus batch normalization and dropout layers to avoid over fitting. Results The validation based on 45 datasets not used for model training revealed that the proposed deep convolutional neural network is capable of identifying and differentiating between normal tympanic membranes, tympanostomy tubes, and cerumen impactions with an overall accuracy of 84.4%. Conclusion Our study shows that deep convolutional neural networks hold immense potential as a diagnostic adjunct for otologic disease management.",
  "full_text": "ORIGINAL RESEARCH ARTICLE Open Access\nBuilding an Otoscopic screening prototype\ntool using deep learning\nDevon Livingstone 1* , Aron S. Talai 2, Justin Chau 1 and Nils D. Forkert 2\nAbstract\nBackground: Otologic diseases are often difficult to diagnose accurately for primary care providers. Deep learning\nmethods have been applied with great success in many areas of medicine, often outperforming well trained\nhuman observers. The aim of this work was to develop and evaluate an automatic software prototype to identify\notologic abnormalities using a deep convolutional neural network.\nMaterial and methods: A database of 734 unique otoscopic images of various ear pathologies, including 63\ncerumen impactions, 120 tympanostomy tubes, and 346 normal tympanic membranes were acquired. 80% of the\nimages were used for the training of a convolutional neural network and the remaining 20% were used for\nalgorithm validation. Image augmentation was employed on the training dataset to increase the number of\ntraining images. The general network architecture consisted of three convolutional layers plus batch normalization\nand dropout layers to avoid over fitting.\nResults: The validation based on 45 datasets not used for model training revealed that the proposed deep\nconvolutional neural network is capable of identifying and differentiating between normal tympanic membranes,\ntympanostomy tubes, and cerumen impactions with an overall accuracy of 84.4%.\nConclusion: Our study shows that deep convolutional neural networks hold immense potential as a diagnostic\nadjunct for otologic disease management.\nKeywords: Neural network, Machine learning, Automated, Otoscopy, Deep learning, Artificial intelligence\nIntroduction\nAccess to tertiary care ear, nose, throat (ENT) specialists\nis extremely limited in many regions outside of major\nacademic centers worldwide, while lengthy waitlists in\nmetropolitan areas impair timely access to quality care.\nOtologic disease treatment is highly subspecialized,\ncreating diagnostic difficulties for primary care pro-\nviders, ultimately leading to negative impacts on pa-\ntients. The burden of initial diagnosis and triage largely\nfalls upon general practitioners and other allied health\nprofessionals, who are relatively inaccurate with otologic\ndiagnoses, even for common presentations such as acute\notitis media or serous otitis media [ 1– 4]. For example,\ncommunity pediatricians and general practitioners only\nhave a diagnostic accuracy for acute otitis media of 62%\nwith pneumatic otoscopy [ 2]. A multi-national study of\npediatricians and general practitioners showed that the\naverage diagnostic accuracy for acute otitis media and\nserous otitis media using video otoscopy is only 51 and\n46%, respectively [ 4]. Otolaryngologists are more accur-\nate, though very far from perfect, with diagnostic accur-\nacies around 74% [ 4, 5].\nClearly, there is a need for an improved method for\nscreening patients for otologic abnormalities so that\notologic complaints can be correlated with relevant\notoscopic findings. Previously described methods for\nautomatic otoscopic diagnosis have mostly relied on\nhandcrafted feature extr action techniques such as\nedge detection or histogram analysis [ 6, 7]. These ap-\nproaches are limited to a select number of diagnoses,\nsuch as tympanostomy tubes [ 6]o ro t i t i sm e d i a[ 7],\nand are further limited by poor generalizability when\nusing different imaging techniques.\n© The Author(s). 2019 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0\nInternational License ( http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and\nreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to\nthe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver\n(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.\n* Correspondence: dmliving@ucalgary.ca\n1Division of Otolaryngology – Head and Neck Surgery, Department of\nSurgery, University of Calgary, 7th floor, 4448 Front Street SE, Calgary, Alberta\nT3M 1M4, Canada\nFull list of author information is available at the end of the article\nLivingstone et al. Journal of Otolaryngology - Head and Neck Surgery\n          (2019) 48:66 \nhttps://doi.org/10.1186/s40463-019-0389-9\nRecently, deep neural networks have been applied with\ngreat success in various areas of medicine, often achiev-\ning higher accuracy compared to traditional machine\nlearning techniques and comparable performance to\nhighly trained human specialists [ 8– 11]. Convolutional\nneural networks (CNNs) are a variation of deep neural\nnetworks capable of recognizing features within images.\nBroadly speaking, CNNs are a type of machine learning\ntool that are constructed and modelled in a way the mir-\nrors the neural networks found in biological brains. In\ndetail, a CNN can learn to identify the appearance of ab-\nnormalities after being trained using labeled normal and\nabnormal images without the requirement of explicit\nhandcrafting of features to do so. In detail, these features\nare learnt at the same time as the network is optimized\nbased on the training set. In this context, CNNs are an\nemerging adjunct to medical diagnosis and there is a\nlarge body of scientific literature devoted to their use for\nnumerous medical applications [ 8, 9, 12, 13]. Common\notologic diseases, such as acute otitis media, serous otitis\nmedia, and tympanic membrane perforation have unique\nappearances that might be detectable using deep convo-\nlutional neural networks. Despite this high potential for\ndeep convolutional neural networks for diagnosing ear\ndiseases based on digital otoscopic images, this has not\nbeen tested so far.\nTherefore, the objective of this study was to develop\nand evaluate a software prototype using a deep convolu-\ntional neural network model for identifying abnormal-\nities in otoscopic ear images. Such a computer-aided\ndiagnosis tool based on deep neural networks would\nfacilitate appropriate triaging for subspecialist review\nand facilitate timely access to accurate otologic diagnosis\nand treatment.\nMaterial and methods\nPatients and imaging\nEthical approval for this study was obtained from the\nConjoint Health Research and Ethics Board at the\nUniversity of Calgary, AB, Canada. Otoscopic images\nwere collected prospectively from patients presenting to\nAlberta Health Services facility outpatient otolaryngol-\nogy clinics in Calgary, AB, Canada. CellScope [ 14]\nsmartphone adaptors were utilized to obtain the clinical\nimages. All otologic pathologies and normal healthy\ntympanic membranes were included in the database.\nGround truth diagnosis\nAll images were reviewed in consensus by two experi-\nenced ENT specialists (DL & JC) taking into account the\ndiagnosis in the corresponding electronic health care\nrecords. Abnormalities included the presence of a myr-\ningotomy tube, either in appropriate position in the TM\nor extruded, cerumen impaction, and a healthy, normal\ntympanic membrane.\nCNN training, optimization, and testing\nThe three most common diagnostic categories (normal\ntympanic membrane, tympanostomy tube, and cerumen\nimpaction) were utilized to train and test the CNN.\nImage augmentation in the form of rotation and mirror-\ning was employed on the dataset to increase the number\nof training images in each category to achieve balanced\ntraining sets. All images were converted to grayscale be-\nfore being passed to the network. Table 1 represent the\nbreakdown of datasets in each category. A small sized\nnetwork, consisting of three convolutional hidden layers,\nwhere each layer contained a batch normalization (BN),\nand a dropout layer with a fixed ratio of 40% was imple-\nmented in the TensorFlow frame work in Python.\n20% of the overall training datasets were used as a\nvalidation set in order to optimize the CNN ’s hyper-\nparameters via the so-called babysitting approach [ 15].\nFurthermore, 15 images in each diagnostic category were\nisolated for testing the algorithm ’s performance, yielding\na 45-image test set. It is important to note that none of\nthe testing images including augmented versions were\npart of the training or validation datasets.\nResults\nDiagnosis dataset\nOverall, 724 unique otoscopic images of various otologic\ndiagnoses, including 346 normal tympanic membranes,\n120 tympanostomy tubes, 63 cerumen impactions, 50\ntympanic membranes with myringosclerosis, 44 with oti-\ntis externa, and 32 tympanic membranes perforations\nwere acquired and available for this study. While all im-\nages were used for algorithm training, the test set was\nconstrained in terms of available diagnoses; only the\nnormal, tympanostomy tube, and cerumen impaction\ncategories were selected for CNN model testing in this\nwork. These categories had the largest number of im-\nages, which is required for accurate performance of the\nTable 1 Algorithm Performance for Normal, Tube and Cerumen Diagnoses\nDiagnosis Database Images n Post Processing n Algorithm score Misclassifications Accuracy %\nNormal Tympanic Membrane 346 993 14/15 Myringosclerosis 93.3\nTympanostomy Tube 120 1050 13/15 Cerumen Cerumen 86.7\nCerumen Impaction 63 960 11/15 Normal Normal Normal Normal 73.3\nTotal 529 3003 38/ 45 84.4\nLivingstone et al. Journal of Otolaryngology - Head and Neck Surgery           (2019) 48:66 Page 2 of 5\nCNN. The other categories had insufficient image num-\nbers and were therefore not included in the final test set\nused, though the algorithm was constructed to allow for\ndiagnosis of all conditions found in the dataset. Figure 1\nshows a selection of the study images obtained using the\nCellScope attachment.\nAlgorithm performance\nTable 1 shows the performance of the algorithm when\ndifferentiating between normal tympanic membrane, ceru-\nmen impaction, and tympanostomy tube categories. The\noverall accuracy was 84.4% for these three diagnoses.\nThere were 7 misclassifications overall, which can be seen\nin Fig. 2. The misclassifications for each diagnosis were\nconsistent within each diagnostic category: two tympa-\nnostomy tube images were misclassified as cerumen im-\npactions, four cerumen impactions were misclassified as\nnormal, and one normal TM was misclassified as having\nmyringosclerosis.\nDiscussion\nThis work presents a first prototype implementation of a\nconvolutional neural network for the multi-level classifi-\ncation of prominent ear conditions such as having a\ntympanostomy tube or cerumen impaction compared to\na completely healthy ear. In detail, this deep neural net-\nwork framework achieves an accuracy of 93.3, 86.7,\n73.3% for the non-diseased ear, tympanostomy tube, and\ncerumen impaction categories, respectively. Figure 2\nshows the images misclassified by the algorithm, which\nare clearly misclassified, even to the untrained eye.\nThough attempts to deconstruct why a machine learning\nalgorithm mislabels data can only be speculative, doing\nso can help improve the quality of subsequent training\ndata, while improving our understanding of how these\nalgorithms work. For example, both images of ear tubes\nwere misclassified as cerumen impactions. Certainly, in\nthese images, there is a clinically insignificant amount of\ncerumen around the periphery of the external auditory\ncanal that may have been detected by the algorithm.\nWithin this context, it should also be pointed out that\nall images were converted to gray-scale images prior to\nCNN training and testing to prevent overfitting based\non color information. Thus, the bright green color of the\ntympanostomy tube is not taken properly into account\nfor the classification. With more datasets becoming\navailable, this limitation can be addressed by using the\nfull color information for CNN training and testing. Fig-\nure 2 shows the normal tympanic membrane that was\nmisclassified as having myringosclerosis, which may have\nbeen due to the bright white bone of the scutum that\nwas clearly visible through the ear drum in the posterior\nsuperior quadrant. The fact that all four cerumen impac-\ntions were misclassified as normal suggests that the\nFig. 1 Selection of images obtained using CellScope smartphone\nattachment a) normal b) cerumen c) tympanostomy tube\nLivingstone et al. Journal of Otolaryngology - Head and Neck Surgery           (2019) 48:66 Page 3 of 5\nalgorithm is identifying features of wax that are associ-\nated with a non-diseased tympanic membrane, perhaps\nrelating to features such as the surface topography. A\nmultilabel classifier trained using a larger training data-\nset may have led to better classification results.\nA machine learning approach for otoscopic diagnosis,\nas with any computer vision problem, requires a large\nnumber of images to reach optimal performance. This\nstudy was limited by the number of available images, so\nthat image augmentation techniques such as mirroring\nand rotation were necessary to boost the number of im-\nages per category. However, augmentation alone does\nnot provide the same value as a truly larger database for\nmodel optimization. While the testing images that where\nused for the validation of the approach were never part\nof the model training steps, the relatively small number\nof training cases seems to be the primary hindrance to\nhigher accuracy levels, especially by making use of the\nfull color information. Keeping in mind that our training\nsets were rather small, in order to avoid an overfitted\nnetwork, we deliberately chose a smaller network, to\navoid network memorization. Furthermore, we applied\nstrong regularization methods such as dropout, batch\nnormalization, and early stopping. Ultimately, various\nnetwork parameters such as validation and training loss\nwere consistently monitored to check for overfitting.\nOnly images from the same source were deliberately\nutilized in this study to avoid potential complications as-\nsociated with the inclusion of multisource datasets. In\ndetail, multisource images might have different image\ndata attributes, such as lighting level, wavelength, aspect\nratio, image resolution, and the size of otoscope tip. The\ndiagnostic accuracy would likely be greatly decreased if\nmultiple different imaging platforms were used for this\nstudy, especially if one diagnostic category was dispro-\nportionately recorded using a specific imaging modality\nleading to a systematic bias. For example, if a higher\nresolution camera such as a newer smart phone is used\nconsistently in a tertiary otology clinic, with general\nENT clinics using an older smart phone model, the algo-\nrithm might pick up on features associated with image\nresolution and inappropriately associate a disease diag-\nnosis with high image resolution.\nDistinguishing between wax impaction, an ear tube,\nand a normal ear is an easy task for any medical profes-\nsional. Thus, the algorithm in its current form cannot\nsupplant this basic physical exam skill. Our initial re-\nsults, however, demonstrate that with a large enough\ndataset, computer vision could be successfully applied to\nsupport otoscopic diagnosis. Other authors, such as the\nAutoscope group, have leveraged machine learning\napproaches in combination with explicitly programmed\nfeature extraction to expand the diagnostic capabilities\nof their algorithm to include multiple otologic diagnoses\n[11]. A collaborative, multi-institutional approach to\nobtaining high-quality images is necessary, and we wel-\ncome any individuals or groups that would like to work\nwith us to accomplish this task. The 724 image otoscopy\ndatabase generated for this work will be expanded with\nmore diagnoses and images added over time. An open\naccess selection of these images is available online for\nteaching and research purposes [ 16].\nConclusion\nWe have built and implemented a first screening proto-\ntype software tool for otologic disease using a deep con-\nvolutional neural network. Our algorithm, while still\nrudimentary and inaccurate, is capable of identifying and\ndifferentiating between normal tympanic membranes,\ntympanostomy tubes, and cerumen impactions. Diagnos-\ntic performance on these and other otologic pathologies\nwill improve as more images are obtained and incorpo-\nrated into the algorithm training database. We hope that\nFig. 2 Misclassifications made by neural network (incorrect diagnosis\non image)\nLivingstone et al. Journal of Otolaryngology - Head and Neck Surgery           (2019) 48:66 Page 4 of 5\nthis algorithm, once adequately trained, could be utilized\nfor telemedicine and global health applications where ac-\ncess to tertiary care level otolaryngology services is lim-\nited. Ideally, a low-cost and robust standalone image-\ncapable otoscope with integrated software could be dis-\ntributed to primary care providers, allowing them to use\nthis framework as a diagnostic adjunct. This would im-\nprove diagnostic accuracy and would facilitate appropri-\nate and timely referral patterns. In regions where access\nto ENT services is limited, an accurate automated diag-\nnostic tool would represent a significant improvement in\nquality of care.\nAcknowledgements\nMedical Image Processing Lab at the University of Calgary, the resident\ncohort at the University of Calgary ENT program who assisted with image\nacquisition.\nAuthors’ contributions\nDL provided image acquisition, annotation and manuscript writing. JC\nprovided image acquisition and annotation. NF and AT provided neural\nnetwork programming and manuscript writing. All authors read and\napproved the final manuscript.\nFunding\nNone\nAvailability of data and materials\nSubset of otoscopic images available at www.entid.ca/atlas\nEthics approval and consent to participate\nObtained from REB at the University of Calgary via IRISS. Study Approval #:\nREB17–1391.\nConsent for publication\nNot applicable\nCompeting interests\nThe authors declare that they have no competing interests.\nAuthor details\n1Division of Otolaryngology – Head and Neck Surgery, Department of\nSurgery, University of Calgary, 7th floor, 4448 Front Street SE, Calgary, Alberta\nT3M 1M4, Canada. 2Department of Radiology, Hotchkiss Brain Institute,\nUniversity of Calgary, Calgary, Canada.\nReceived: 25 July 2019 Accepted: 1 November 2019\nReferences\n1. Buchanan CM, Pothier DD. Recognition of paediatric otopathology by\ngeneral practitioners. Int J Pediatr Otorhinolaryngol. 2008;72(5):669 –73.\n2. Asher E, Leibovitz E, Press J, Greenberg D, Bilenko N, Reuveni H. Accuracy of\nacute otitis media diagnosis in community and hospital settings. Acta\nPaediatr. 2005;94(4):423 –8.\n3. Legros JM, Hitoto H, Garnier F, Dagorne C, Parot-Schinkel E, Fanello S.\nClinical qualitative evaluation of the diagnosis of acute otitis media in\ngeneral practice. Int J Pediatr Otorhinolaryngol. 2008;72(1):23 –30.\n4. Pichichero ME, Poole MD. Comparison of performance by otolaryngologists,\npediatricians, and general practioners on an otoendoscopic diagnostic\nvideo examination. Int J Pediatr Otorhinolaryngol. 2005;69(3):361 –6.\n5. Pichichero ME, Poole MD. Assessing diagnostic accuracy and\ntympanocentesis skills in the management of otitis media. Arch Pediatr\nAdolesc Med. 2001;155(10):1137 –42.\n6. Wang X, Valdez TA, Bi J. Detecting tympanostomy tubes from otoscopic\nimages via offline and online training. Comput Biol Med. 2015;61:107 –18.\n7. Myburgh HC, van Zijl WH, Swanepoel D, Hellstrom S, Laurent C. Otitis media\ndiagnosis for developing countries using tympanic membrane image-\nanalysis. EBioMedicine. 2016;5:156 –60.\n8. Gulshan V, Peng L, Coram M, Stumpe MC, Wu D, Narayanaswamy A, et al.\nDevelopment and validation of a deep learning algorithm for detection of\ndiabetic retinopathy in retinal fundus photographs. JAMA. 2016;316(22):\n2402–10.\n9. Walczak S, Velanovich V. An evaluation of artificial neural networks in\npredicting pancreatic Cancer survival. J Gastrointest Surg. 2017;21(10):1606 –12.\n10. Tan JH, Hagiwara Y, Pang W, Lim I, Oh SL, Adam M, et al. Application of\nstacked convolutional and long short-term memory network for accurate\nidentification of CAD ECG signals. Comput Biol Med. 2018;94:19 –26.\n11. Senaras C, Moberly AC, Teknos T, Essig G, Elmaraghy C, Taj-Schaal N, et al.\nAutoscope: automated otoscopy image analysis to diagnose ear pathology\nand use of clinically motivated eardrum features: SPIE; 2017.\n12. Johnson KW, Torres Soto J, Glicksberg BS, Shameer K, Miotto R, Ali M, et al.\nArtificial Intelligence in Cardiology. J Am Coll Cardiol. 2018;71(23):2668 –79.\n13. Navarrete-Dechent C, Dusza SW, Liopyris K, Marghoob AA, Halpern AC,\nMarchetti MA. Automated dermatological diagnosis: hype or reality? J invest\nDermatol; 2018.\n14. Cellscope. 2018 [Available from: https://www.cellscope.com. Accessed Jan 2019.\n15. Smith LN. A disciplined approach to neural network hyper-parameters: part\n1 – learning rate, batch size, momentum, and weight decay. US Naval\nResearch Laboratory Tehnical Report. 2018;5510026.\n16. ENTiD. Online Open Access Portal; 2019. Accessible at www.entid.ca.\nAccessed Jan 2019.\nPublisher’sN o t e\nSpringer Nature remains neutral with regard to jurisdictional claims in\npublished maps and institutional affiliations.\nLivingstone et al. Journal of Otolaryngology - Head and Neck Surgery           (2019) 48:66 Page 5 of 5",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5099717378616333
    },
    {
      "name": "Deep learning",
      "score": 0.43093204498291016
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36140042543411255
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3459375500679016
    },
    {
      "name": "Psychology",
      "score": 0.3376837372779846
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I168635309",
      "name": "University of Calgary",
      "country": "CA"
    }
  ],
  "cited_by": 40
}