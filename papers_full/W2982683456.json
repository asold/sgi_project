{
  "title": "Performance and usability of machine learning for screening in systematic reviews: a comparative evaluation of three tools",
  "url": "https://openalex.org/W2982683456",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A5090561309",
      "name": "Allison Gates",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A5104015097",
      "name": "Samantha Guitard",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A5007539248",
      "name": "Jennifer Pillay",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A5050661476",
      "name": "Sarah A. Elliott",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A5069929460",
      "name": "Michele P. Dyson",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A5041090623",
      "name": "Amanda S. Newton",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A5010229710",
      "name": "Lisa Hartling",
      "affiliations": [
        "University of Alberta"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2593758073",
    "https://openalex.org/W2026981750",
    "https://openalex.org/W2095601341",
    "https://openalex.org/W2805303998",
    "https://openalex.org/W2961191798",
    "https://openalex.org/W4313371821",
    "https://openalex.org/W2783690099",
    "https://openalex.org/W1997248429",
    "https://openalex.org/W2953029151",
    "https://openalex.org/W2905431510",
    "https://openalex.org/W2394854659",
    "https://openalex.org/W2921325297",
    "https://openalex.org/W2100053037",
    "https://openalex.org/W2808847453",
    "https://openalex.org/W1760234209",
    "https://openalex.org/W4238188254",
    "https://openalex.org/W2955273711",
    "https://openalex.org/W2093274439",
    "https://openalex.org/W2043566294",
    "https://openalex.org/W2032191478",
    "https://openalex.org/W1924618820",
    "https://openalex.org/W2019241358",
    "https://openalex.org/W2735984040",
    "https://openalex.org/W2406978180",
    "https://openalex.org/W2807522649",
    "https://openalex.org/W2541978711",
    "https://openalex.org/W2147469877",
    "https://openalex.org/W19399978"
  ],
  "abstract": null,
  "full_text": "RESEARCH Open Access\nPerformance and usability of machine\nlearning for screening in systematic\nreviews: a comparative evaluation of three\ntools\nAllison Gates 1 , Samantha Guitard 1, Jennifer Pillay 1, Sarah A. Elliott 1, Michele P. Dyson 1, Amanda S. Newton 2 and\nLisa Hartling 1*\nAbstract\nBackground: We explored the performance of three machine learning tools designed to facilitate title and abstract\nscreening in systematic reviews (SRs) when used to (a) eliminate irrelevant records (automated simulation) and (b)\ncomplement the work of a single reviewer (semi-automated simulation). We evaluated user experiences for each\ntool.\nMethods: We subjected three SRs to two retrospective screening simulations. In each tool (Abstrackr, DistillerSR,\nRobotAnalyst), we screened a 200-record training set and downloaded the predicted relevance of the remaining\nrecords. We calculated the proportion missed and workload and time savings compared to dual independent\nscreening. To test user experiences, eight research staff tried each tool and completed a survey.\nResults: Using Abstrackr, DistillerSR, and RobotAnalyst, respectively, the median (range) proportion missed was 5 (0\nto 28) percent, 97 (96 to 100) percent, and 70 (23 to 100) percent for the automated simulation and 1 (0 to 2)\npercent, 2 (0 to 7) percent, and 2 (0 to 4) percent for the semi-automated simulation. The median (range) workload\nsavings was 90 (82 to 93) percent, 99 (98 to 99) percent, and 85 (85 to 88) percent for the automated simulation\nand 40 (32 to 43) percent, 49 (48 to 49) percent, and 35 (34 to 38) percent for the semi-automated simulation. The\nmedian (range) time savings was 154 (91 to 183), 185 (95 to 201), and 157 (86 to 172) hours for the automated\nsimulation and 61 (42 to 82), 92 (46 to 100), and 64 (37 to 71) hours for the semi-automated simulation. Abstrackr\nidentified 33 –90% of records missed by a single reviewer. RobotAnalyst performed less well and DistillerSR provided\nno relative advantage. User experiences depended on user friendliness, qualities of the user interface, features and\nfunctions, trustworthiness, ease and speed of obtaining predictions, and practicality of the export file(s).\nConclusions: The workload savings afforded in the automated simulation came with increased risk of missing\nrelevant records. Supplementing a single reviewer ’s decisions with relevance predictions (semi-automated\nsimulation) sometimes reduced the proportion missed, but performance varied by tool and SR. Designing tools\nbased on reviewers ’self-identified preferences may improve their compatibility with present workflows.\nSystematic review registration: Not applicable.\nKeywords: Systematic reviews, Machine learning, Automation, Usability, User experience\n© The Author(s). 2019 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0\nInternational License ( http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and\nreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to\nthe Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver\n(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.\n* Correspondence: hartling@ualberta.ca\n1Department of Pediatrics, Alberta Research Centre for Health Evidence and\nthe University of Alberta Evidence-based Practice Center, University of\nAlberta, 11405 87 Ave NW, Edmonton, Alberta T6G 1C9, Canada\nFull list of author information is available at the end of the article\nGates et al. Systematic Reviews           (2019) 8:278 \nhttps://doi.org/10.1186/s13643-019-1222-2\nBackground\nThere is growing recognition that expedited systematic\nreview (SR) processes are needed to keep pace with the\nrapid publication of primary studies [ 1]. Title and ab-\nstract screening is a review step that may be particularly\namenable to automation or semi-automation [ 2– 4].\nThere is increasing interest in ways that reviewers can\nleverage machine learning (ML) tools to expedite screen-\ning while maintaining SR validity [ 5]. One way that ML\ntools expedite screening is by predicting the relevance of\nremaining records after reviewers screen a “training set. ”\nWhat remains unclear is how and when reviewers may\nreliably leverage these predictions to semi-automate\nscreening. Reviewers would benefit from understanding\nthe relative reliability, usability, learnability, and costs of\navailable tools.\nA review of published studies on applications of ML\ntools for screening found that they could be used safely\nto prioritize relevant records and cautiously to replace\nthe work of one of two human reviewers [ 6]. Despite\ntheir promise, the adoption of ML tools among re-\nviewers has been slow [ 6– 8]. O ’Connor et al. summa-\nrized potential barriers to adopting ML tools among\nreviewers. Concerns included distrust in ML approaches\nby reviewers and end users, set-up challenges and in-\ncompatibility with standard workflows, doubts as to\nwhether ML tools can reliably perform SR tasks, and\npoor awareness of available tools [ 9].\nIn light of known barriers to ML tool adoption [ 9– 12],\nwe investigated the relative advantages and risks of using\nML tools to automate or semi-automate title and ab-\nstract screening. For three SRs, we compared how three\nML tools performed when used in the context of (a) sin-\ngle reviewer screening to eliminate irrelevant records\nand (b) dual independent screening to complement the\nwork of one of the human reviewers. We also aimed to\ncompare user experiences across the tools.\nMethods\nConduct\nWe followed an a priori protocol, available upon request.\nMachine learning tools\nWe investigated Abstrackr ( http://abstrackr.cebm.brown.\nedu), DistillerSR (the ML tool being DistillerAI) ( http://\nwww.evidencepartners.com), and RobotAnalyst ( http://\nwww.nactem.ac.uk/robotanalyst/). From a user ’sp e r s p e c t i v e ,\nthe three tools function similar ly. After uploading citations\nto the user interface, titles and abstracts appear on-screen\nand reviewers are prompted to label each as relevant or ir-\nrelevant. The ML algorithms use reviewers ’ relevance labels\nand other data (e.g., relevance terms tagged by reviewers,\ntext mining for MeSH terms and keywords) to predict\nwhich of the remaining records are relevant.\nAlthough many ML tools exist [ 13], we chose\nAbstrackr, DistillerSR, and RobotAnalyst because their\ndevelopment is well-documented [ 14– 16], and at least\nfor Abstrackr and RobotAnalyst, real-world performance\nhas been evaluated [ 17– 19]. We also chose the tools for\npractical reasons. All three allow the user to download\nthe relevance predictions after screening a training set.\nBoth Abstrackr and RobotAnalyst are freely available,\nand although DistillerSR is a pay-for-use software, our\ncenter maintains a user account.\nPerformance testing\nScreening procedure\nWe selected a convenient sample of three SRs of health-\ncare interventions completed or underway at our center\n(Table 1). For each SR, we uploaded all records to each\ntool via RIS (Research Information Systems) files. We\nset up the SRs for single-reviewer screening with the re-\ncords presented in random order. Although we had\nintended to use the “most likely to be relevant ”\nprioritization, we were not successful in applying this\nsetting in all tools (due to server errors or glitches in\nAbstrackr and RobotAnalyst).\nWhen using ML tools for screening, inaccurate labels\nin the training set will result in unreliable predictions.\nThus, for a training set of 200 records, we retrospectively\nreplicated the senior reviewer ’s (the reviewer with the\nmost content expertise or SR experience) screening deci-\nsions in each tool. In a previous evaluation [ 18], we\nfound 200 records to be sufficient to develop predic-\ntions. The developers of DistillerAI recommend a mini-\nmum training set size of 40 excluded and 10 included\nrecords and a maximum size of 300 records [ 22]. Be-\ncause the records appeared in random order, the train-\ning set differed across the tools for each review.\nAlthough this could affect the predictions, in a previous\nevaluation, we found little difference in Abstrackr ’s pre-\ndictions over three independent trials [ 15].\nAt our center, any record marked as “include” or “un-\nsure” by either reviewer during title and abstract screen-\ning is eligible for scrutiny by full text. For this reason,\nour screening files included one of two screening deci-\nsions for each record: include/unsure or exclude. Be-\ncause we were unable to retrospectively ascertain\nwhether the decision for individual records was “include”\nor “unsure,” we entered all “include/unsure” decisions as\n“relevant.”\nAfter screening the training sets, we downloaded the\nrelevance predictions for the remaining records in each\ntool. In DistillerSR and RobotAnalyst, these were avail-\nable immediately. In Abstrackr, they were typically avail-\nable the following day. When the predictions did not\nbecome available within 48 h, we continued to screen in\nbatches of 100 records until they did. The format of the\nGates et al. Systematic Reviews           (2019) 8:278 Page 2 of 11\npredictions varied by tool. Abstrackr produced “hard\nscreening predictions ” (true, i.e., include or false, i.e., ex-\nclude) and relevance probabilities for each remaining\nrecord. We used the hard screening predictions rather\nthan applying custom thresholds based on the probabil-\nities. Both DistillerSR and RobotAnalyst provided binary\npredictions (include or exclude) for all remaining re-\ncords. Although customization was possible in Distil-\nlerSR, we used the “simple review ” function to\nautomatically classify the remaining records.\nRetrospective simulations\nBased on existing reviews [ 2, 6, 11], we postulated that\nthe ML tools ’ relevance predictions could be leveraged\nto (a) automatically exclude irrelevant records or (b)\ncomplement the work of one of the human reviewers.\nWe devised two retrospective screening simulations to\ntest our hypothesis. In the first approach (automated\nsimulation, the automatic exclusion of records), after\nscreening the training set, the senior reviewer down-\nloaded the predictions and excluded all records pre-\ndicted to be irrelevant. To reduce the full-text screening\nworkload, the reviewer continued to screen the records\npredicted to be relevant. Of these, the records that the\nreviewer agreed were relevant moved forward to full-text\nscreening. In the second approach (semi-automated\nsimulation, complementing the work of one human re-\nviewer), we aimed to determine whether the predictions\ncould be leveraged to improve upon the work of a single\nreviewer (as naturally, a single reviewer can be expected\nto erroneously exclude relevant records) [ 23]. In this\nsimulation, the senior reviewer followed the same ap-\nproach as in the automated simulation, and the second\nreviewer screened all of the records as per usual. Any\nrecord marked as relevant by the second reviewer or the\nsenior reviewer/tool ’s predictions moved forward to full-\ntext screening.\nTo test the performance of each approach, we created\na workbook in Excel (v. 2016, Microsoft Corporation,\nRedmond, Washington) for each SR. The workbooks in-\ncluded a row for each record and a column for each of\nthe record identification number, the title and abstract\nscreening decisions for the senior and second reviewers,\nthe full-text consensus decisions, and the relevance pre-\ndictions from each tool. We then determined the title\nand abstract decisions that would have resulted from\neach simulation. As per standard practice at our center,\nwe considered any record marked as “include” by either\nof the reviewers to be relevant for scrutiny by full text.\nUser experience testing\nIn February 2019, we approached a convenient sample\nof 11 research staff at our center to participate in the\nuser experience testing. These staff were experienced in\nproducing SRs (e.g., research assistants, project coordi-\nnators, research associates), but had no or very little ex-\nperience with ML tools for screening. We allowed\ninvited participants 1 month to undertake the study,\nwhich entailed completing a screening exercise in each\ntool and a user experience survey. Participation was vol-\nuntary and completion of the survey implied consent.\nWe received ethical approval for the user experience\ntesting from the University of Alberta Research Ethics\nBoard (Pro00087862).\nWe designed a screening exercise that aligned with\npractices at our center (Additional file 1). The aim of the\nTable 1 Population, intervention, comparator, outcome, and study design (PICOS) criteria for the systematic reviews\nCriteria Antipsychotics [ 20] Bronchiolitis Visual Acuity [ 21]\nPopulation Children and young adults aged\n≤ 24 years experiencing a psychiatric\ndisorder or behavioral issues outside the\ncontext of a disorder\nInfants and young children aged < 24\nmonths experiencing their first episode of\nwheeze or diagnosed with bronchiolitis or\nRSV\nCommunity-dwelling adults aged ≥ 65\nyears with unrecognized impaired visual\nacuity or vision-related functional\nlimitations\nIntervention Any Food and Drug Administration-approved\nfirst- or second-generation antipsychotic\nAny bronchodilator, any corticosteroid,\nhypertonic saline, oxygen therapy,\nantibiotics, heliox\nVision screening tests (alone or within\nmulticomponent screening/assessment)\nperformed by primary healthcare\nprofessionals\nComparators Placebo, no treatment, any other\nantipsychotic, the same antipsychotic in a\ndifferent dose\nPlacebo, usual care, no treatment, normal\nsaline, or another intervention of interest\nNo screening, delayed screening, attention\ncontrol, screening involving all\ncomponents of intervention except vision\ncomponent, usual care\nOutcomes Intermediate and effectiveness outcomes,\nadverse effects and major adverse effects,\nadverse effects limiting treatment, specific\nadverse events, persistence and reversibility\nof adverse effects\nOutpatient admissions, inpatient length of\nstay, change in clinical score, oxygen\nsaturation, respiratory rate, heart rate,\npulmonary function, adverse events,\nescalation of care, length of illness,\nduration of oxygen therapy\nBenefits (e.g., mortality, adverse\nconsequences of poor vision), harms (e.g.,\nserious adverse events), implementation\nfactors (e.g., uptake of referrals)\nStudy\ndesigns\nRCTs and nRCTs, controlled cohort studies,\ncontrolled before-after studies\nRCTs RCTs, controlled experimental\nand observational studies\nnRCT non-randomized controlled trial, RCT randomized controlled trial, RSV respiratory syncytial virus\nGates et al. Systematic Reviews           (2019) 8:278 Page 3 of 11\nexercise was to guide participants through the steps in-\nvolved in setting up a SR, uploading a set of records,\nscreening a training set, and downloading the predic-\ntions in each tool. We provided minimal guidance only\ninstructing participants to use the “Help” function in\neach tool if needed.\nFor the screening exercise, we selected a SR with rela-\ntively straightforward eligibility criteria that was under-\nway at our center (PROSPERO #CRD42017077622). We\nwanted participants to focus on their experience in each\ntool and did not want complex screening criteria to be a\ndistraction. To reduce the risk of response bias, we used\nthe random numbers generator in Excel to randomize\nthe order in which each participant tested the three\ntools.\nThe survey (Additional file 2), hosted in REDCap (Re-\nsearch Electronic Data Capture) [ 24], asked participants\nto complete the System Usability Scale (SUS) [ 25] for\neach tool. The SUS is a 10-item questionnaire that as-\nsesses subjective usability using a Likert-like scale [ 25].\nThe survey also asked participants to elaborate on their\nexperiences with each tool, rank the tools in order of\npreference, and describe the features that supported or\ndetracted from their usability.\nWe made minor changes to the screening exercise (re-\nduced the suggested number of citations to screen to\nminimize participant burden) and survey (edited for\ntypos) following pilot testing by two researchers at our\ncenter. Because the changes were minimal, we retained\nthe data from the two researchers for analysis, with\npermission.\nAnalysis\nPerformance\nWe exported the simulation data from Excel to SPSS\nStatistics (v. 25, IBM Corporation, Armonk, New York)\nfor analysis. We used data from 2 × 2 cross-tabulations\nto calculate standard [ 6] performance metrics for each\nsimulation, as follows:\n/C15Proportion of records missed (i.e., error): of the\nstudies included in the final report, the proportion\nthat would have been excluded during title and\nabstract screening.\nWe made informal comparisons of the proportion\nmissed for each simulation and tool to single\nreviewer screening to estimate the acceptability of\nits performance.\n/C15Workload savings (i.e., absolute screening reduction ):\nof the records that need to be screened at the title\nand abstract stage, the proportion that would not\nneed to be screened manually.\n/C15Estimated time savings : the time saved by not\nscreening records manually. We assumed a\nscreening rate of 0.5 min per record [ 26] and an 8-h\nwork day.\nAdditional file 3 shows sample calculations for the An-\ntipsychotics SR using Abstrackr ’s predictions.\nUser experiences\nWe exported the quantitative survey data from REDCap\nto Excel for analysis and the qualitative survey data to\nWord (v. 2016, Microsoft Corporation, Redmond, Wash-\nington). For each participant, we calculated the overall\nusability score for each tool as recommended by Brooke\n[25]. We calculated the median and interquartile range\nof scores for each tool and categorized their usability as\nrecommended by Bangor et al. [ 27]: not acceptable (0 to\n50), marginal (50 to 70), and acceptable (70 to 100). For\nthe ranking of tools by preference, we calculated counts\nand percentages.\nWe analyzed the qualitative data following standard,\nsystematic approaches to thematic analysis [ 28]. One re-\nsearcher (AG) read the text and applied one or more\ncodes to each line. Next, the researcher identified the\nmost significant and frequent codes, combined similar\ncodes, and developed memos for each theme. To reduce\nthe risk of interpretive bias, a second researcher external\nto the study team reviewed the analysis for differences in\ninterpretation. All disagreements were resolved via\ndiscussion.\nResults\nPerformance\nTable 2 shows the screening characteristics for each SR.\nThe screening workload ranged from 5861 to 12,156 re-\ncords. Across SRs, 2 – 10% of records were retained for\nscrutiny by full text. Across SRs, ≤ 2% of all records were\nincluded in the final reports. The Visual Acuity review\nwas unique in that only one record from the 11,229\nscreened was included in the final report. The final re-\nports for the Antipsychotics and Bronchiolitis reviews\nincluded 127/12156 and 137/5861 records, respectively.\nPredictions were available after screening 200 records\nfor all SRs in all tools with the exception of Visual Acu-\nity in Abstrackr. As planned, we screened an additional\n100 records, and the predictions became available. For\ntwo of the SRs, RobotAnalyst did not upload the full list\nof records from the RIS file. Because all of our trouble-\nshooting attempts (at least six attempts and contact with\nthe developers) failed, we assumed that the additional\n170 records for Bronchiolitis and 183 records for Visual\nAcuity would need to be screened manually. We thus\nused the human reviewers ’ original decisions (include or\nexclude) when applying the simulations.\nIn Abstrackr, DistillerSR, and RobotAnalyst, the train-\ning sets included a median (range) of 12 (4, 15), 14 (2,\nGates et al. Systematic Reviews           (2019) 8:278 Page 4 of 11\n14), and 15 (3, 20) includes respectively, with the balance\nbeing excludes. After screening the training sets,\nAbstrackr, DistillerSR, and RobotAnalyst predicted that\na median (range) 18 (12, 33)%, 0.1 (0, 1)%, and 29 (20,\n29)% of the remaining records were relevant, respect-\nively. Cross-tabulations showing records included in the\nfinal report relative to those deemed relevant via each\nsimulation are in Additional file 4.\nAutomated simulation\nProportion missed\nRecords “missed” are those that would not have moved for-\nward to full-text screening, but were included in the final\nreports. The median (range) proportion missed was 5 (0,\n28)%, 97 (96, 100)%, and 70 (23, 100)% using Abstrackr,\nDistillerSR, and RobotAnalyst, respectively (Fig.1).\nWorkload savings\nThe median (range) workload savings was 90 (82, 93)%,\n99 (98, 99)%, 85 (84, 88)% for Abstrackr, DistillerSR, and\nRobotAnalyst, respectively (Fig. 2).\nEstimated time savings\nThe median (range) time savings was 154 (91, 183), 185\n(95, 201), and 157 (86, 172) hours for Abstrackr, Distil-\nlerSR, and RobotAnalyst, respectively (i.e., a respective\n19 (11, 23), 23 (12, 25), and 20 (11, 21) days) (Fig. 3).\nSemi-automated simulation\nProportion missed\nThe median (range) proportion missed was 1 (0, 2)%, 2\n(0, 7)%, and 2 (0, 4)% for Abstrackr, DistillerSR, and\nRobotAnalyst, respectively (Fig. 4). Important to the\nTable 2 Characteristics of the reviews and screening predictions for each tool\nCharacteristic Antipsychotics, N records (%) Bronchiolitis, N records (%) Visual Acuity, N records (%)\nScreening workload a 12,156 5861 11,229\nIncluded by title/abstract b 1178 (10) 518 (9) 224 (2)\nIncluded in the review b 127 (1) 137 (2) 1 (< 1)\nIncludes/excludes\nin training set\nAbstrackr, 15/185 Abstrackr, 12/188 Abstrackr c, 4/296\nDistillerSR, 14/186 DistillerSR, 14/186 DistillerSR, 2/198\nRobotAnalyst, 20/180 RobotAnalyst, 15/185 RobotAnalyst, 3/197\nScreened by tool d 11,956 (98) 5661 (97) 11,029 (98)\nPredicted relevant\nby Abstrackr\n2117 (18) 656 (12) 3639 (33)\nPredicted relevant\nby DistillerSR\n7 (< 1) 83 (1) 0 (0)\nPredicted relevant\nby RobotAnalyst\n3488 (29) 1082 (19) 3221 (29)\naTotal number of records retrieved via the electronic searches. Each record was screened by two reviewers\nbIncluded following the initial screening by two independent reviewers (retrospective)\ncAll training sets were 200 records, with the exception of the Visual Acuity review which required a 300-record training set in Abstrackr before predic tions\nwere produced\ndAfter a 200-record training set\nFig. 1 Proportion missed (percent) by tool and systematic review, automated simulation\nGates et al. Systematic Reviews           (2019) 8:278 Page 5 of 11\nperformance of the semi-automated simulation is the\ncontribution of each tool ’s predictions to the overall\nscreening accuracy. Had the second reviewer screened\nthe records for Antipsychotics, Bronchiolitis, and Visual\nAcuity independently, a respective 3 (2%), 10 (7%), and 0\nrecords would have been missed. Abstrackr correctly\npredicted the relevance of 1 (33%) and 9 (90%) records\nmissed by the second reviewer in the Antipsychotics and\nBronchiolitis reviews, respectively. DistillerSR did not\ncorrectly predict the relevance of any of the records\nmissed by the second reviewer in either review, thus\nproviding no advantage over single-reviewer screening.\nRobotAnalyst correctly predicted the relevance of 4\n(40%) records missed by the second reviewer in Bron-\nchiolitis, but none of those missed in Antipsychotics.\nWorkload savings\nThe median (range) workload savings was 40 (32, 43)%,\n49 (48, 49)%, and 35 (34, 38)% for Abstrackr, DistillerSR,\nand RobotAnalyst, respectively (Fig. 5).\nEstimated time savings\nThe median (range) time savings was 61 (42, 82), 92 (46,\n100), and 64 (37, 71) hours for Abstrackr, DistillerSR,\nand RobotAnalyst, respectively (i.e., 8 (5, 10), 11 (6, 12),\nand 8 (5, 9) days) (Fig. 6).\nPost hoc analysis\nFollowing our initial testing, we repeated the same pro-\ncedures for a 500-record training set. We undertook the\nsimulations for the larger training set only in Abstrackr,\naccounting for time and resource limitations. For the au-\ntomated simulation, the median proportion missed in-\ncreased from 28 to 41% for Antipsychotics and 5 to 9%\nfor Bronchiolitis. There was no change in the proportion\nmissed for Visual Acuity. The workload savings in-\ncreased from 90 to 95% for Antipsychotics, 93 to 94%\nfor Bronchiolitis, and 82 to 83% for Visual Acuity. The\nestimated time savings increased from 183 to 193 h for\nAntipsychotics, 91 to 92 h for Bronchiolitis, and 154 to\n156 h for Visual Acuity.\nFig. 2 Workload savings (percent) by tool and systematic review, automated simulation\nFig. 3 Estimated time savings (days) by tool and systematic review, automated simulation\nGates et al. Systematic Reviews           (2019) 8:278 Page 6 of 11\nFor the semi-automated simulation, one additional\nrecord was missed for Antipsychotics; however, the pro-\nportion missed did not change. There was no change in\nthe proportion missed for the other SRs. The workload\nsavings increased from 40 to 45% for Antipsychotics, 43\nto 44% for Bronchiolitis, and 32 to 33% for Visual Acu-\nity. The estimated time savings increased from 82 to 92\nh for Antipsychotics, 42 to 43 h for Bronchiolitis, and 61\nto 62 h for Visual Acuity.\nUser experiences\nEight research staff participated in the user experience\ntesting (73% response rate). The median (interquartile\nrange) SUS score was 79 (23), 64 (31), and 31 (8) for\nAbstrackr, DistillerSR, and RobotAnalyst, respectively\n(Table 3). Abstrackr fell in the usable, DistillerSR the\nmarginal, and RobotAnalyst the unacceptable usability\nrange [ 27]. Sixty-two percent of participants chose\nAbstrackr as their first choice and 38% as their second\nchoice. Thirty-eight percent of participants chose\nDistillerSR as their first choice, 50% as their second\nchoice, and 13% as their last choice. Thirteen percent of\nparticipants chose RobotAnalyst as their second choice\nand 88% as their last choice.\nThe qualitative analysis revealed that usability was\ncontingent on six interdependent properties: user friend-\nliness, qualities of the user interface, features and func-\ntions, trustworthiness, ease and speed of obtaining the\npredictions, and practicality of the export files. Add-\nitional file 5 includes focused codes and participant\nquotes for each property.\nParticipants’ comments mirrored the quantitative find-\nings. Most found Abstrackr to be easy to use. Although\nsome described the user interface as “rudimentary,” par-\nticipants generally appreciated that it was simple and\nlacked distractions. Many participants liked the custo-\nmizability of review settings in Abstrackr, although some\nfound it confusing and did not find the user guide to be\nhelpful. Overall, Abstrackr was deemed relatively trust-\nworthy, even if it was sometimes slow or crashed.\nFig. 4 Proportion missed (percent) by tool and systematic review, semi-automated simulation\nFig. 5 Workload savings (percent) by tool and systematic review, semi-automated simulation\nGates et al. Systematic Reviews           (2019) 8:278 Page 7 of 11\nHaving to wait for the predictions was described as an\n“annoyance,” but not serious given the potential for time\nsavings. There was little agreement as to whether\nAbstrackr’s export files were usable or practical.\nParticipants were divided with respect to DistillerSR ’s\nuser friendliness, with some finding it easy to use and\nothers finding it unnecessarily complex. Although most\nliked the user interface, calling it “clean” and “bright,”\nothers found it busy and overwhelming. Most partici-\npants felt that DistillerSR had too many features, making\nit feel sophisticated but overly complicated. Among the\nthree tools, most participants found it to be the most re-\nliable, referencing a professional look and feel, fast server\nspeed, and few error messages. Many participants appre-\nciated that the predictions were available in real time,\nbut some could not figure out how to deploy them.\nDistillerSR’s output files were probably the most prac-\ntical, but it often took a few attempts for participants to\ndownload them in the desired format.\nRobotAnalyst was the least preferred. Most found it\ndifficult to use due to a slow server speed, multiple pop-\nups and error messages, and cumbersome screening\nprocess. The user interface, nevertheless, was described\nas “pretty” and participants liked the colors and layout.\nOne participant appreciated that the relevance predic-\ntions appeared clearly on-screen, but otherwise the\nscreening process was described as inefficient. Due to\nmultiple glitches, most participants did not find the pro-\ngram to be trustworthy. As with DistillerSR, participants\nappreciated that the predictions were available in real\ntime, but noted that applying them was slow. A positive\ncomment about RobotAnalyst ’s export files was that they\nFig. 6 Estimated time savings (days) by tool and systematic review, semi-automated simulation\nTable 3 System Usability Scale responses for each item, per tool a\nItem Abstrackr DistillerSR RobotAnalyst\nI think that I would like to use the tool frequently 3.5 (1) 4 (0.5) 1 (1)\nI found the tool to be unnecessarily complex 2 (1) 3.5 (1.25) 3 (0.5)\nI thought the tool was easy to use 4 (1.25) 2.5 (2) 2 (1.5)\nI think that I would need the support of a\ntechnical person to be able to use the tool\n1 (1) 2.5 (1.25) 4 (1.25)\nI found the various function in the tool were\nwell integrated\n4 (1.25) 3.5 (2.25) 3 (1.25)\nI thought there was too much inconsistency\nin the tool\n2 (0.25) 1 (1.25) 4 (1.25)\nI would imagine that most people would\nlearn to use the tool very quickly\n4.5 (1) 3 (1.25) 3 (0.25)\nI found the tool very cumbersome to use 2 (0.5) 3 (1.25) 5 (0)\nI felt very confident using the tool 4 (1) 3.5 (1.25) 2 (2.25)\nI needed to learn a lot of things before I\ncould get going with the tool\n2 (0.25) 3 (0.5) 2.5 (1)\nOverall score (/100) 79 (23) 64 (31) 31 (8)\nLikert-like scale: 1 = strongly disagree, 3 = neutral, and 5 = strongly agree. Values represent the median (interquartile range) of responses\nGates et al. Systematic Reviews           (2019) 8:278 Page 8 of 11\nwere easy to download. Otherwise, participants consist-\nently found the export to be impractical.\nDiscussion\nSupplementing a single reviewer’s decisions with Abstrackr’s\npredictions (semi-automated simulation) reduced the pro-\nportion missed compared with screening by the single re-\nviewer, but performance varied by SR. Balanced with the\npotential for time savings, this approach could provide an\nacceptable alternative to dual independent screening in\nsome SRs. By contrast, RobotAnalyst performed less well\nand DistillerSR provided no advantage over screening by a\nsingle reviewer. Differences between tools may reflect the\nrelevance thresholds applied (we used standard settings) or\ndifferences in the ML algorithms. Replication on heteroge-\nneous samples of reviews will inform when ML-assisted\nscreening approaches may be wor th the associated risk. Al-\nthough the workload and time savings were superior when\nthe tools were used to exclude irrelevant records (automated\nsimulation), typically, far more studies were missed.\nEmpirical data show that learning increases quickly at\nthe beginning of active learning and more slowly there-\nafter [ 29]. Thus, to obtain reliable predictions, large\ntraining sets can be required [ 14, 30]. It is unsurprising,\nthen, that as a means to eliminate irrelevant records, the\n200-record training produced unreliable predictions. Un-\nfortunately, larger training sets may be impractical in\nreal-world applications of ML tools. The 200-record\ntraining set was sufficient, in many cases, when paired\nwith a single reviewer to capture ≥ 95% of relevant stud-\nies; however, this was not always an improvement over\nsingle reviewer screening. At present, the ideal training\nset size is unknown and likely review-specific [ 5]. In this\nstudy, Abstrackr ’s predictions were most reliable for\nBronchiolitis, which compared to Antipsychotics had\nfewer research questions and included only randomized\ntrials. We speculate that ML may perform better for re-\nviews with a single research question or those that in-\nclude only randomized trials; however, our small sample\nprecludes definitive conclusions.\nEven if ML-supported screen ing approaches were ready\nto deploy, many review teams would remain hesitant pend-\ning widespread acceptance by methods groups, peer re-\nviewers, grant panels, and journal editors [ 9]. Moving\ntoward this ideal, there is a need for standard approaches to\nevaluating the performance and usability of the tools and\nreporting on these evaluations [ 7, 9, 31, 32]. Consistently\nconducted and reported evaluations will facilitate their rep-\nlication across tools and SRs [ 31, 32], which will inform\nevidence-based guidance for their use [9]. The development\nof a set of outcome metrics, based on the consensus of end\nusers (reviewers) and tool developers, may improve upon\nthe value of future studies in this field. For example, the im-\npact of missed studies on a SR ’s conclusions is important to\nreviewers but less frequently co nsidered by tool developers.\nDesigning tools that allow reviewers to customize the level\nof risk (i.e., by setting their own relevance thresholds) may\nalso contribute to garnering trust.\nAnother important contributor to the adoption of\nML tools for screening will be their usability and fit\nwith standard SR workflows [ 9]. The usability of the\nthree tools varied considerably and relied upon mul-\ntiple properties. Although usability will be of little\nconcern once title and abstract screening is fully au-\ntomated, the path toward that ideal begins with the\nacceptance and greater adoption of semi-automated\napproaches. Multiple experienced reviewers within\nour sample were unable to download the predictions\nfrom a number of the tools. Even when they were\ndownloaded, the predictions were often in an imprac-\ntical or unusable format. So long as reviewers cannot\nleverage the tools as intended, adoption is unrealistic.\nGreater attention to usability may improve the appeal\nof ML-assisted screening during early phases of\nadoption.\nStrengths and limitations\nThis is one of few studies to compare performance and\nuser experiences across multiple ML tools for screening\nin SRs. Further, our study responds to a call from the\nInternational Collaboration for Automation of System-\natic Reviews to trial and validate available tools [ 7] and\naddresses reported barriers to their adoption [ 9].\nThe training sets differed for each review across the\ntools. Although this could have affected the findings, in\na previous evaluation, we found that Abstrackr ’s predic-\ntions did not differ substantially across three trials [ 18].\nIn the absence of guidance for customizing the tools ’\nsettings (e.g., deciding review-specific relevance thresh-\nolds), we used the standard settings in each tool to ob-\ntain predictions, which likely best approximated real-\nworld use of the tools. We used a 200-record training\nset and a small sample of three SRs. The size of the\ntraining set can affect the resulting predictions. Our\nfindings should not be generalized to other tools, SRs, or\nsemi-automated screening approaches.\nTime savings was estimated based on the reduced\nscreening workload and a standard screening rate. This\nestimate did not account for time spent troubleshooting\nusability issues, nor for variability in the time spent\nscreening records as reviewers progress through the\nscreening task or for obviously excluded compared to re-\ncords of uncertain relevance [ 29].\nWe did not investigate the impact of the missed stud-\nies on the results of the SRs. Future studies should plan\nfor the time and resources to undertake these analyses\nin their protocols.\nGates et al. Systematic Reviews           (2019) 8:278 Page 9 of 11\nConclusions\nUsing Abstrackr ’s predictions to complement the work\nof a single screener reduced the number of studies that\nwere missed by up to 90%, although performance varied\nby SR. RobotAnalyst provided a lesser advantage, and\nDistiller provided no advantage over screening by a sin-\ngle reviewer. Considering workload and time savings,\nusing Abstrackr to complement the work of a single\nscreener may be acceptable in some cases; however, add-\nitional evaluations are needed before this approach could\nbe recommended. Although using any tool to automatic-\nally exclude irrelevant records could save substantial\namounts of time, the risk of missing larger numbers of\nrelevant records is increased. The usability of the tools\nvaried greatly. Further research is needed to inform how\nML might be best applied to reduce screening workloads\nand to identify the types of screening tasks that are most\nsuitable to semi-automation. Designing (or refining\nexisting) tools based on reviewers ’ preferences may im-\nprove their usability and enhance adoption.\nSupplementary information\nSupplementary information accompanies this paper at https://doi.org/10.\n1186/s13643-019-1222-2.\nAdditional file 1. Screening exercise for the user experiences testing.\nScreening exercise instructions as presented to participants for the user\nexperiences testing.\nAdditional file 2. User experiences survey. Details of the questions and\nresponse options on the user experiences survey.\nAdditional file 3. 2 × 2 tables and calculations for the performance\nmetrics (example from the Antipsychotics review in Abstrackr). 2 × 2\ntables and sample calculations for the proportion missed, workload\nsavings, and estimated time savings for each simulation. This file shows\nan example from the Antipsychotics review in Abstrackr.\nAdditional file 4. 2 × 2 cross-tabulations for each review in each tool.\n2 × 2 cross-tabulations for each review in each tool used to calculate the\nperformance metrics.\nAdditional file 5. Focused codes and supporting quotes for the\nproperties of each tool. Focused codes and supporting quotes for the\nthemes that emerged from the qualitative analysis, for each tool.\nAbbreviations\nML: Machine learning; nRCT: Non-randomized controlled trial;\nPICOS: Population, Intervention, Comparator, Outcome, Study design;\nRCT: Randomized controlled trial; RIS: Research Information Systems;\nRSV: Respiratory syncytial virus; SR: Systematic review\nAcknowledgements\nWe thank Dr. Michelle Gates for piloting the usability survey and screening\nexercise and for suggesting revisions to the manuscript draft. We thank Dr.\nMeghan Sebastianski for reviewing the qualitative analysis. We also thank our\ncolleagues for taking the time to test the ML tools and provide feedback\nand the peer reviewers for providing constructive suggestions for\nimprovement on the manuscript draft.\nAuthors’ contributions\nAG contributed to the development of the protocol, collected performance\ndata, developed the screening exercise and survey, recruited the participants,\nconducted the data analyses, and drafted the manuscript. SG collected\nperformance data and reviewed the draft manuscript for important\nintellectual content. JP, SAE, MPD, and ASN contributed to the development\nof the protocol and reviewed the draft manuscript for important intellectual\ncontent. LH contributed to the development of the protocol, reviewed and\nprovided revisions on the screening exercise and survey, reviewed the draft\nmanuscript for important intellectual content, and oversaw all work related\nto the project. All authors agreed on the final version of the manuscript as\nsubmitted.\nFunding\nThis project was funded under Contract No. 290-2015-00001-I Task Order 1\nfrom the Agency for Healthcare Research and Quality (AHRQ), U.S. Depart-\nment of Health and Human Services (HHS). The authors of this report are re-\nsponsible for its content. Statements in the report do not necessarily\nrepresent the official views of or imply endorsement by AHRQ or HHS. A rep-\nresentative from AHRQ served as a Contracting Officer ’s Technical Represen-\ntative and provided technical assistance during the conduct of the full report\nand provided comments on draft versions of the full report. AHRQ did not\ndirectly participate in the design of the study, in the collection, analysis, and\ninterpretation of data, nor in the writing of the report and decision to submit\nit for publication.\nAvailability of data and materials\nThe datasets used and/or analyzed during the current study are available\nfrom the corresponding author on reasonable request.\nEthics approval and consent to participate\nWe received ethical approval to complete the user experience testing from\nthe University of Alberta Research Ethics Board on 24 January 2019\n(Pro00087862). Completion of the online survey implied consent.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nAuthor details\n1Department of Pediatrics, Alberta Research Centre for Health Evidence and\nthe University of Alberta Evidence-based Practice Center, University of\nAlberta, 11405 87 Ave NW, Edmonton, Alberta T6G 1C9, Canada.\n2Department of Pediatrics, University of Alberta Evidence-based Practice\nCenter, University of Alberta, 11405 87 Ave NW, Edmonton, Alberta T6G 1C9,\nCanada.\nReceived: 10 October 2019 Accepted: 5 November 2019\nReferences\n1. Borah R, Brown AW, Capers PL, Kaiser KA. Analysis of the time and workers\nneeded to conduct systematic reviews of medical interventions using data\nfrom the PROSPERO registry. BMJ Open. 2017;7:e012545. https://doi.org/10.\n1136/bmjopen-2016-012545.\n2. Thomas J, McNaught J, Ananiadou S. Applications of text mining within\nsystematic reviews. Res Synth Methods. 2011;2:1 –14. https://doi.org/10.\n1002/jrsm.27 .\n3. Tsafnat G, Glasziou P, Choong MK, Dunn A, Galgani F, Coiera E. Systematic\nreview automation technologies. Syst Rev. 2014;3:74. https://doi.org/10.\n1186/2046-4053-3-74.\n4. Beller E, Clark J, Tsafnat G, Adams C, Diehl H, Lund H, et al. Making progress\nwith the automation of systematic reviews: principles of the International\nCollaboration for the Automation of Systematic Reviews (ICASR). Syst Rev.\n2018;7:77. https://doi.org/10.1186/s13643-018-0740-7.\n5. Marshall IJ, Wallace BC. Toward systematic review automation: a practical\nguide to using ML tools in research synthesis. Syst Rev. 2019;8:163. https://\ndoi.org/10.1186/s13643-019-1074-9.\n6. O ’Mara-Eves A, Thomas J, McNaught J, Miwa M, Ananiadou S. Using text\nmining for study identification in systematic reviews: a systematic review of\ncurrent approaches. Syst Rev. 2015;4:5. https://doi.org/10.1186/2046-4053-4-5.\n7. O'Connor AM, Tsafnat G, Gilbert SB, Thayer KA, Wolfe MS. Moving toward\nthe automation of the systematic review process: a summary of discussions\nat the second meeting of International Collaboration for the Automation of\nGates et al. Systematic Reviews           (2019) 8:278 Page 10 of 11\nSystematic Reviews (ICASR). Syst Rev. 2018;7:3. https://doi.org/10.1186/\ns13643-017-0667-4.\n8. Thomas J. Diffusion of innovation in systematic review methodology: why is\nstudy selection not yet assisted by automation? OA Evidence-Based Med.\n2013;1:12.\n9. O ’Connor AM, Tsafnat G, Thomas J, Glasziou P, Gilbert SB, Hutton B. A\nquestion of trust: can we build an evidence base to gain trust in systematic\nreview automation technologies? Syst Rev. 2019;8:143. https://doi.org/10.\n1186/s13643-019-1062-0.\n10. van Altena AJ, Spijker R, Olabarriaga SD. Usage of automation tools in\nsystematic reviews. Res Synth Methods. 2019;10:72 –82. https://doi.org/10.\n1002/jrsm.1335.\n11. Paynter R, Bañez LL, Berliner E, Erinoff E, Lege-Matsuura J, Potter S, et al. EPC\nmethods: an exploration of the use of text-mining software in systematic\nreviews. Report no.: 16-EHC023-EF. Rockville: Agency for Healthcare\nResearch and Quality; 2016.\n12. O'Connor AM, Tsafnat G, Gilbert SB, Thayer KA, Shemilt I, Thomas J, et al.\nStill moving toward automation of the systematic review process: a\nsummary of discussions at the third meeting of the International\nCollaboration for Automation of Systematic Reviews (ICASR). Syst Rev. 2019;\n8:57. https://doi.org/10.1186/s13643-019-0975-y.\n13. Marshall C. Systematic review toolbox. 2019. http://systematicreviewtools.\ncom/index.php. Accessed 5 Apr 2019.\n14. Wallace BC, Small K, Brodley CE, Lau J, Trikalinos TA. Deploying An\nInteractive ML System In An Evidence-Based Practice Center: Abstrackr.\nProceedings of the 2nd ACM SIGHIT International Health Informatics\nSymposium; 2012 Jan 28-30; New York, New York: Association for\nComputing Machinery; 2012. doi: https://doi.org/10.1145/2110363.2110464.\n15. The National Centre for Text Mining. RobotAnalyst. 2019. http://www.\nnactem.ac.uk/robotanalyst/. Accessed 5 Apr 2019.\n16. Evidence Partners. Publications. 2019. https://www.evidencepartners.com/\nabout/publications/. Accessed 5 Apr 2019.\n17. Przyby ła P, Brockmeier AJ, Kontonatsios G, Le Pogam MA, McNaught J, von\nElm E, et al. Prioritising references for systematic reviews with RobotAnalyst:\na user study. Res Synth Methods. 2018;9:470 –88. https://doi.org/10.1002/\njrsm.1311.\n18. Gates A, Johnson C, Hartling L. Technology-assisted title and abstract\nscreening for systematic reviews: a retrospective evaluation of the Abstrackr\nmachine learning tool. Syst Rev. 2018;7:45. https://doi.org/10.1186/s13643-0.\n18-0707-8 .\n19. Rathbone J, Hoffmann T, Glasziou P. Faster title and abstract screening?\nEvaluating Abstrackr, a semi-automated online screening program for\nsystematic reviewers. Syst Rev. 2015;4:80 doi:26073974.\n20. Pillay J, Boylan K, Carrey N, Newton A, Vandermeer B, Nuspl M, et al. First-\nand second-generation antipsychotics in children and young adults:\nsystematic review update. Report no.: 17-EHC001-EF. Rockville: Agency for\nHealthcare Research and Quality; 2017.\n21. Pillay J, Freeman EE, Hodge W, MacGregor T, Featherstone R, Vandermeer B,\net al. Screening for impaired visual acuity and vision-related functional\nlimitations in adults 65 years and older in primary health care: systematic\nreview. Edmonton: Evidence Review Synthesis Centre; 2017. http://\ncanadiantaskforce.ca/ctfphc-guidelines/overview/.\n22. Evidence Partners. DistillerAI FAQs. 2019. https://www.evidencepartners.\ncom/resources/distillerai-faqs/. Accessed 5 Apr 2019.\n23. Waffenschmidt S, Knelangen M, Sieben W, Bühn S, Pieper D. Single\nscreening versus conventional double screening for study selection in\nsystematic reviews: a methodological systematic review. BMC Med Res\nMethodol. 2019;19:132. https://doi.org/10.1186/s12874-019-0782-0.\n24. Harris PA, Taylor R, Thielke R, Payne J, Gonzalez N, Conde JG. Research\nelectronic data capture (REDCap) — a metadata-driven methodology and\nworkflow process for providing translational research informatics support. J\nBiomed Inform. 2009;42:377 –81. https://doi.org/10.1016/j.jbi.2008.08.010.\n25. Brooke J. SUS-a quick and dirty usability scale. Usability Eval Industry. 1996;\n189:4–7.\n26. Wallace BC, Trikalinos TA, Lau J, Brodley C, Schmid CH. Semi-automated\nscreening of biomedical citations for systematic reviews. BMC\nBioinformatics. 2010;11:55. https://doi.org/10.1186/1471-2105-11-55.\n27. Bangor A, Kortum PT, Miller JT. An empirical evaluation of the system\nusability scale. Int J Hum Comput Interact. 2008;24:574 –94. https://doi.org/\n10.1080/10447310802205776.\n28. Vaismoradi M, Turunen H, Bondas T. Content analysis and thematic analysis:\nimplications for conducting a qualitative descriptive study. Nurs Health Sci.\n2013;15:398–405. https://doi.org/10.1111/nhs.12048.\n29. Wallace BC, Small K, Brodley CE, Lau J, Trikalinos TA. Modeling annotation\ntime to reduce workload in comparative effectiveness reviews. Proceedings\nof the 1st ACM International Health Informatics Symposium; 2010 Nov 11 –\n12. New York, New York: Association for Computing Machinery; 2010. doi:1\n−/1145/1882992.1882999.\n30. Chen Y. Developing stopping rules for a ML system in citation screening\n[dissertation]. Providence: Brown University; 2019.\n31. Olorisade BK, Brereton P, Andras P. Reproducibility of studies on text mining\nfor citation screening in systematic reviews: evaluation and checklist. J\nBiomed Inform. 2017;73:1 –13. https://doi.org/10.1016/j.jbi.2017.07.010.\n32. Olorisade BK, Quincey E, Brereton P, Andras P. A critical analysis of studies\nthat address the use of text mining for citation screening in systematic\nreviews. Proceedings of the 20th International Conference on Evaluation\nand Assessment in Software Engineering; 2016 Jun 1 –3. Limerick:\nAssociation for Computing Machinery; 2016. https://doi.org/10.1145/\n2915970.2915982.\nPublisher’sN o t e\nSpringer Nature remains neutral with regard to jurisdictional claims in\npublished maps and institutional affiliations.\nGates et al. Systematic Reviews           (2019) 8:278 Page 11 of 11",
  "topic": "Medicine",
  "concepts": [
    {
      "name": "Medicine",
      "score": 0.9371877908706665
    },
    {
      "name": "Usability",
      "score": 0.8380693197250366
    },
    {
      "name": "Medical physics",
      "score": 0.4328545331954956
    },
    {
      "name": "MEDLINE",
      "score": 0.4192371368408203
    },
    {
      "name": "Medical education",
      "score": 0.369648814201355
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3333008289337158
    },
    {
      "name": "Human–computer interaction",
      "score": 0.2705376148223877
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Computer science",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ]
}