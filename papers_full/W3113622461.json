{
    "title": "Arabizi Language Models for Sentiment Analysis",
    "url": "https://openalex.org/W3113622461",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A3117443944",
            "name": "Gaétan Baert",
            "affiliations": [
                "Université de Rouen Normandie",
                "Institut National des Sciences Appliquées Rouen Normandie",
                "Laboratoire d'Informatique, du Traitement de l'Information et des Systèmes"
            ]
        },
        {
            "id": "https://openalex.org/A2047320339",
            "name": "Souhir Gahbiche",
            "affiliations": [
                "Airbus (France)"
            ]
        },
        {
            "id": "https://openalex.org/A2594432719",
            "name": "Guillaume Gadek",
            "affiliations": [
                "Airbus (France)"
            ]
        },
        {
            "id": "https://openalex.org/A361282818",
            "name": "Alexandre Pauchet",
            "affiliations": [
                "Université de Rouen Normandie",
                "Institut National des Sciences Appliquées Rouen Normandie",
                "Laboratoire d'Informatique, du Traitement de l'Information et des Systèmes"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2975059944",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W4324392584",
        "https://openalex.org/W2005624335",
        "https://openalex.org/W1634225020",
        "https://openalex.org/W3024622987",
        "https://openalex.org/W2250212502",
        "https://openalex.org/W2889438178",
        "https://openalex.org/W2250967669",
        "https://openalex.org/W2950577311",
        "https://openalex.org/W2899793821",
        "https://openalex.org/W2553002154",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2986154550",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W2955043905",
        "https://openalex.org/W2952638691",
        "https://openalex.org/W2536583325",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2250243742",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2467186984",
        "https://openalex.org/W4287810359",
        "https://openalex.org/W2407132801",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W2982018869",
        "https://openalex.org/W2250278916",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W2250594687",
        "https://openalex.org/W3022969819",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2963626623",
        "https://openalex.org/W2592699349",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2469994317",
        "https://openalex.org/W2104463314",
        "https://openalex.org/W2507475033"
    ],
    "abstract": "Arabizi is a written form of spoken Arabic, relying on Latin characters and digits. It is informal and does not follow any conventional rules, raising many NLP challenges. In particular, Arabizi has recently emerged as the Arabic language in online social networks, becoming of great interest for opinion mining and sentiment analysis. Unfortunately, only few Arabizi resources exist and state-of-the-art language models such as BERT do not consider Arabizi. In this work, we construct and release two datasets: (i) LAD, a corpus of 7.7M tweets written in Arabizi and (ii) SALAD, a subset of LAD, manually annotated for sentiment analysis. Then, a BERT architecture is pre-trained on LAD, in order to create and distribute an Arabizi language model called BAERT. We show that a language model (BAERT) pre-trained on a large corpus (LAD) in the same language (Arabizi) as that of the fine-tuning dataset (SALAD), outperforms a state-of-the-art multi-lingual pretrained model (multilingual BERT) on a sentiment analysis task.",
    "full_text": "Proceedings of the 28th International Conference on Computational Linguistics, pages 592–603\nBarcelona, Spain (Online), December 8-13, 2020\n592\nArabizi Language Models for Sentiment Analysis\nSouhir Gahbiche and Guillaume Gadek\nAirbus, Advanced Information Processing, France\nGa´etan Baert and Alexandre Pauchet\nLitis, INSA Rouen Normandie\nalexandre.pauchet@insa-rouen.fr\nAbstract\nArabizi is a written form of spoken Arabic, relying on Latin characters and digits. It is informal\nand does not follow any conventional rules, raising many NLP challenges. In particular, Arabizi\nhas recently emerged as the Arabic language in online social networks, becoming of great interest\nfor opinion mining and sentiment analysis. Unfortunately, only few Arabizi resources exist and\nstate-of-the-art language models such as BERT do not consider Arabizi.\nIn this work, we construct and release two datasets: (i) LAD, a corpus of 7.7M tweets written in\nArabizi and (ii) SALAD, a subset of LAD, manually annotated for sentiment analysis. Then, a\nBERT architecture is pre-trained on LAD, in order to create and distribute an Arabizi language\nmodel called BAERT. We show that a language model (BAERT) pre-trained on a large corpus\n(LAD) in the same language (Arabizi) as that of the ﬁne-tuning dataset (SALAD), outperforms a\nstate-of-the-art multi-lingual pretrained model (multilingual BERT) on a sentiment analysis task.\n1 Introduction\nNowadays, Online Social Networks (OSNs) are highly popular means of communication. With pictures,\nvideos, urls, mentions and of course comments, people share ideas, opinions and sentiments on Twitter1,\nFacebook2, LinkedIn3 and many other service providers. OSN posts are of great interest for marketing\nand reputation management, enabling companies to collect feedback about brands and products. Au-\ntomatic systems facilitating these large-scale tasks, including sentiment classiﬁers, have progressively\nreached promising accuracy (Abbasi et al., 2008; Catal and Nangir, 2017; Giatsoglou et al., 2017).\nIn parallel, the NLP ﬁeld has experienced several scientiﬁc developments until the recent language\nmodels. Efﬁcient representation of texts with vectors is difﬁcult and this challenge has seen excellent\nwork: bag-of-words with tf*idf (Robertson et al., 1995), Word2Vec (Mikolov et al., 2013), GloVe (Pen-\nnington et al., 2014), FastText (Joulin et al., 2016), ELMo (Peters et al., 2018), and BERT (Devlin et\nal., 2018) for instance. Each of these scientiﬁc steps has successively outperformed the previous ones\non a variety of NLP tasks including sentiment analysis, with a focus on English texts. Moreover, some\napproaches such as BERT propose pre-trained language models, including a multi-lingual version.\nThe particularity of texts post on OSN is that they frequently mix made-up terms and inventive spelling\nwith more classic syntax. Twitter, for instance, contains invented words, original spelling and typing er-\nrors (Maynard and Funk, 2011), but also includes special entities such as hashtags and user mentions that\noften appear as labels without any syntactic role (Gadek et al., 2017). Even state-of-the-art architectures\nsuch as BERT usually suffer from the particularities of OSN languages, as illustrated on a task of hate\ndetection in tweets (Gertner et al., 2019).\nMoreover, OSN posts are also the opportunity for code-switching, i.e. alternating between two or\nmore alphabets and/or languages in conversation, in particular for many Arabic speakers. As they often\nare familiar with Latin keyboards and use them frequently, writing Arabic using Latin alphabet is easier\nand faster than switching between alphabets. This typing method, called Arabizi, enables to express\n1https://twitter.com\n2https://www.facebook.com\n3https://www.linkedin.com\n593\nboth Modern Standard Arabic (MSA) or Arabic dialects. Arabizi is a non-standard romanization of Ara-\nbic script that is widely adopted for communication over the Internet or for sending messages (Bies et\nal., 2014). It has emerged as the Arabic language of informal communications online. Arabizi appears\nin several challenges for computational linguistics: 1) Arabizi is a written form of non-written Arabic\ndialects; 2) there exists no broadly accepted rules or incitement to correctly write Arabizi: inventive\nspelling is therefore a standard. This language diversity rises signiﬁcant challenges to NLP: Arabizi is\nnot “ofﬁcial”, does not follow any rules to be written and is often mixed with other foreign languages.\nUnfortunately, only very few Arabizi resources are publicly available and existing state-of-the-art lan-\nguage models such as BERT, even in its multilingual version, are not optimised to process Arabizi posts\non OSN.\nThis article contributes to the study of user-generated contents in Arabizi on OSN, by releasing a 7.7M\ntweet corpus, called LAD (Large Arabizi Dataset). A subset of these tweets, named SALAD (Sentiment\nAnnotations from LAD), has been manually annotated according to their sentiment. We illustrate the\ninterest of such resources on a sentiment analysis classiﬁcation task, with an Arabizi-trained BERT-\nbased language model (BAERT). We aim at enabling researchers to exploit LAD on Arabizi NLP tasks\nand SALAD to evaluate the performance of sentiment analysers on Arabizi tweets; the size of SALAD\nmay be insufﬁcient to train greedy machine learning models.\nThe remaining of this article is organised as follows. Section 2 presents the characteristics of Arabizi\nand introduces some works on sentiment analysis for Arabizi. Section 3 introduces our collected corpus\nof Twitter data named LAD, as well as the pre-processing and annotation tasks that lead to SALAD\n(Sentiment Annotations from LAD). Section 4 presents some experiments comparing a multilingual\nBERT model with our Arabizi BAERT model, pre-trained on LAD and tested on SALAD; the results are\npresented in Section 5. Finally, Section 6 concludes this article.\n2 Arabizi, Sentiment Analysis and Language Modeling\nThis section introduces the linguistic phenomenon of Arabizi and covers the technical aspects of text\nclassiﬁcation models and their adequacy to Arabizi.\n2.1 Arabizi as a challenge fo NLP\nArabic language is spoken in twenty-two countries, by about 447 million of speakers. It is the fourth\nmost used language on the Web4. Modern Standard Arabic (MSA), the ofﬁcial written form of Arabic,\nis generally not spoken as a mother tongue, but is mostly used in administrative documents, classrooms,\nmovie subtitles and ofﬁcial news.\nSpoken Arabic exists as dialects that differ according to countries or even regions. Dialect is the\nevery-day-life language of the Arab world: at home, at the grocer’s, with friends and even at school\nout of classrooms. Arabic dialects differ one from another, depending on the historical events of each\ncountry (protectorate, colonisation, sovereignty, ...) and on the geographic location. Country dialects are\ninﬂuenced by the former occupants and by the neighbouring countries. Many classiﬁcations of dialects\nhave been done. (Cotterell and Callison-Burch, 2014) for instance has classiﬁed dialects into ﬁve groups:\nMaghrebi (spoken in the whole North Africa), Egyptian (spoken in Egypt, but understood universally),\nLevantine (spoken primarily in the Levant, Syria and Palestine), Iraqi (spoken in Iraq) and Gulf (spoken\nprimarily in Saudi Arabi, UAE, Kuwait and Qatar). Until the early 2000s, the Arabic dialects were not\nwritten languages, but new technologies (internet, email, SMS, blogs, ...) have prompted Arabic speakers\nto write in their own dialect to facilitate communication. Since most people have Latin keyboards, Arabic\nin mostly written in Latin characters, leading to the emergence of Arabizi. Arabizi is the contraction of\narabi (Arabic) and inglizi (English); a second etymology could be the union ofarabi and easy (Gonzalez-\nQuijano, 2014). Nowadays, Arabizi is the new written form of Arabic all over Internet. It is a rewritten\nform of Arabic using a combination of Latin characters and numbers (LC+N). These numbers represent\nthe letters whose sound does not exist in the Latin alphabet. For example, the number 7 often represents\n4https://www.internetworldstats.com/stats7.htm\n594\nthe sound ha (h letter), the number 3 matches the sound ain (¨ letter).\nWith the intrinsic diversity of OSN, Arabizi reveals a few difﬁculties to adapt classic NLP solutions.\nArabizi is an informal language: no rule ofﬁcially deﬁnes transliterations and several possibilities appear.\nA single sound may have different representations in various dialects, such as the letter qaf that can be\nrepresented by q, 9, k, g, .... As a result, words do not have any ofﬁcial spelling convention. For instance,\n(Cotterell et al., 2014) has identiﬁed 69 different ways to write é<\u000fË@ ZA \u000b\u0011\t\u001d@ (Insh’Allah, God willing). The\nmain differences arise in the pronunciation of vowels and different representations of letters depending\non countries. For example, the number 9 represents the letter sad () in the Middle East countries, and\nqaf ( \u0010. ) in the Maghreb. These differences appear even within the same country (Allehaiby, 2013).\nFinally, Arabizi is mainly used on OSN and therefore exhibits the same inventive usage than in other\nlanguages (typos, inventive spellings, emojis, ...), maybe even more as the written representation of di-\nalects. For instance, a letter can be repeated to express feelings, such asmbrooook! (which approximately\nmeans congratulatioooons!. As most NLP systems are token-based, matching the correct word requires\nintensive character permutations, repetitions and deletions.\nMoreover, Arabizi usually appears tied with another language, such as English or French, depending\non the second language spoken in the country. For instance, “What’s your name ba2a?”(ba2a means\nthen) contains only one word in Arabizi. However, most language models are pre-trained on sentences\nwritten in a single language. Even the multilingual models are pre-trained with a mix of languages at a\nhigher level than the sentence. A language model for Arabizi should be tolerant to the presence of other\nlanguages in the same sentence, i.e. with several syntactic rules and semantics for a same meaning.\n2.2 Arabizi and Sentiment Analysis\nSentiment Analysis (SA) is one of the most interesting NLP tasks with many applications in various\nareas. Brands, companies and services can exploit SA to automatically collect user opinions, in particular\non OSN. As Arabizi is widely used online, SA on Arabizi messages seems an evidence.\nBefore any sentiment process, Arabizi has to be recognised. (Darwish, 2014) identiﬁes Arabizi at the\nword-level: a CRF classiﬁer handles trigrams of characters, and has been trained on a binary classiﬁ-\ncation task (English versus Arabizi). However, language identiﬁcation is more commonly realised at\nsentence-level or document-level (Tobaili, 2016).\nA frequent intuition to process Arabizi texts is to convert them into MSA thanks to expert rules (Es-\nkander et al., 2014), or machine learning (Bies et al., 2014). For instance, (Duwairi et al., 2016) transcript\nArabizi Tweets to Arabic with a rule-based converter, and then annotate the resultant tweets according to\na sentiment (positive, negative or neutral) thanks to crowdsourcing. In (Guellil et al., 2018), the approach\nconsists to automatically classify sentiments of Algerian Arabizi after transliterating to MSA. (Tobaili et\nal., 2019) has created a sentiment lexicon for Arabizi. In a very recent work, (Fourati et al., 2020) has\nreleased a 3,000-comments sentiment analysis dataset named TUNIZI 5, which has been collected from\nsocial networks, preprocessed and manually annotated by Tunisian native speakers.\nAll these works have constituted corpora that could be exploited for SA on Arabizi, particularly on\nSMS 6. Unfortunately, these datasets are not publicly available and/or not sufﬁciently large: 101,292\nmessages in (Bies et al., 2014), 10,000 triples arabizi-MSA-English in the NIST campaign. Even if\nthey enable relevant speciﬁc classiﬁcation models, they are too small to generalise on similar tasks with\n5https://github.com/chaymafourati/TUNIZI-Sentiment-Analysis-Tunisian-Arabizi-Dataset/blob/master/TUNIZI-Dataset.txt\n6E.g. NIST campaign: https://www.nist.gov/itl/iad/mig/openmt-challenge-2015\n595\nslightly different scopes. Concerning Arabic, a few resources exist, such as the annotated Arabic Senti-\nment Tweets Dataset gathered from Twitter and consisting of about 10,000 tweets (Nabil et al., 2015).\nThese tweets are a mixture of MSA and dialect, but written in Arabic alphabet.\n2.3 Language Models\nIndependently of Arabizi, scientiﬁc advancements have reached impressive results on various NLP tasks,\nup to the recent emergence of deep language models. Recurrent neural network models (e.g. LSTM\n(Hochreiter and Schmidhuber, 1997) or GRU (Chung et al., 2014)) exploit internal memory mechanisms,\nenabling to forget useless aspects and to focus on important ones. The main limitation of these models\nfor NLP is that semantically related words are often separated within a sentence and therefore memory\ncells can fail to connect the corresponding words. The Transformer architecture (Vaswani et al., 2017)\ntackles this problem. The key idea is to model the relationships between entities in a sequence regardless\nof their position, using an attention mechanism. Currently, a large majority of the state-of-the art methods\nin NLP are based on this architecture (e.g. ELMo (Peters et al., 2018) or BERT (Devlin et al., 2018)).\nThe Encoder of the Transformer architecture can be pre-trained on unsupervised tasks such as Masked\nLanguage Modeling (MLM) and/or Next Sentence Prediction (NSP) (Devlin et al., 2018). Then, a second\nstep of ﬁne-tuning is used to reach state-of-the-art performance on a wide variety of supervised tasks,\nfrom sentiment analysis to translation (Lample and Conneau, 2019). The pre-training step is generally\nperformed on a large corpus of generic English texts, or large corpora from multiple languages (Pires et\nal., 2019). Recent works demonstrate that better results during the ﬁne-tuning step can be obtained on a\nspeciﬁc language if the model is pre-trained on a corpus from the same language (Martin et al., 2020).\n2.4 Summary and discussion\nArabizi has emerged as the Arabic language in OSN, becoming of great interest for SA. The difﬁculty\nto process Arabizi with NLP techniques comes from the mix of languages, its inventive usage, the lack\nof rules and the variations between local dialects. As a result, Arabizi is a challenge for language mod-\nelling. To the best of our knowledge, currently there exist no Arabizi language model on the most recent\narchitectures such as BERT, nor are there available and sufﬁciently large datasets to learn from scratch\nsuch an Arabizi model. We therefore propose to tackle these two concerns by (1) constituting a linguistic\nresource to train a BERT-like Arabizi language model, and (2) comparing the performance of our own\nmodel against a state-of-the-art text classiﬁer on a sentiment analysis task. A corpus such as TUNIZI\n(Fourati et al., 2020) is insufﬁcient to pre-train a language model but can be exploited for ﬁne-tuning.\nIn this article, we consider that Arabizi represents only the dialects written in LC+N. We mainly\nfocus on Egyptian dialect, as it is the most spoken Arabic dialect with more than 78 million speakers\nworldwide, to constitute an Arabizi resource to learn an Arabizi language model.\nAlthough BERT pre-trained models are well documented and re-training is also frequent, for instance\non tweets (Gertner et al., 2019; M ¨uller et al., 2020), the literature remains unclear whether pre-trained\nmultilingual resources can process dialects and assimilated languages. BERT exists in various versions\napplied on different languages. In particular, BERT-base-multilingual-uncased (BMU) is a language\nmodel learned on 102 languages, including English, French and Arabic languages. As Arabizi is a mix\nof these languages, we believe that this model should be the best available model to serve as baseline for\nour experiments, as well as a good candidate architecture to develop an Arabizi language model.\n3 steps are necessary to solve a NLP task with a Transformer model: First, a vocabulary (token list)\nneeds to be deﬁned using for instance the Byte Pair Encoding algorithm (Sennrich et al., 2016). This step\ncan be omitted by using an already existing vocabulary such as the BMU vocabulary, if this vocabulary\ncan correctly represent the task domain. Secondly, the language model is pre-trained on generic unsuper-\nvised tasks like MLM or NSP (Devlin et al., 2018). These tasks can be started from scratch, continued\nfrom already pre-trained weights, or skipped by using any existing pre-trained weights such as the BMU\nweights. Finally, the language model is ﬁne-tuned on the desired NLP. Concerning Arabizi, 4 options are\ntherefore possible: 1) to build a new vocabulary, to pre-train from scratch the model and then to ﬁne-tune\nit; this model is called BAERT-arz-scratch; 2) to use the BMU vocabulary, to pre-train from scratch\nthe model and then to ﬁne-tune it; this model is called BAERT-bmu-scratch; 3) to re-train from the\n596\nBMU model (therefore using the BMU vocabulary) and then ﬁne-tune it; this model is called BAERT-\nbmu-retrain; 4) to ﬁne-tune directly the BMU model, without any pre-training; this corresponds to the\nbaseline BMU. BAERT stands for Bidirectional Arabizi Encoder Representations from Transformer. In\nthis research, we hypothesise that options 1) and 3) should outperform options 2) and 4), depending of\nthe capacity of a more generic vocabulary to represents the domain of the ﬁne-tuning task.\n3 LAD and SALAD: Arabizi resources\nTo create an open resource in Arabizi and to learn an Arabizi language model, we have collected Arabizi\ntweets using Egyptian keywords as seeds. The corpus is called LAD (Large Arabizi Dataset). Then,\nwe manually annotated a subset of LAD according to their sentiment. This subset is named SALAD\n(Sentiment Annotations from LAD). LAD and SALAD are downloadable on demand7.\n3.1 Data collection and pre-processing (LAD)\nA tweet collector named Twint 8 was exploited to construct the Arabizi dataset. A set of 48 common\nwords in Egyptian, such as “ zehe2t, a7la, la2a, 3ayz”9, served as keyword seeds to collect tweets pub-\nlished between 2015 and 2019. These keywords were incrementally chosen so as to build a general\nEgyptian corpus in Arabizi: they are quite generic in meaning (words such as “but”, ” hello”, “then”,\nbut also “sweet”, “lost”, ’I would”), while covering a wide range of word classes. All these words match\nthe deﬁnition of Arabizi, i.e. they are written using LC+N. False positive inducing keywords (returning\nnon-Arabizi tweets) were removed and we ensured that the tweets have a meaning in Arabic.\nFirst attempts to create the keyword list resulted in data containing (i) Tweets written only in Arabic\nalphabet, when a username contains one of the keywords while some of posted texts are in Arabic, and\n(ii) Urdizi10 tweets. Tweets in Urdu, as well as texts fully written in Arabic characters are excluded,\nwhereas mix texts are included. Since tweets contain a lot of noisy data such as URLs, spams, images\nand so on, the collected data was ﬁltered to remove a large part of this noise.\nTo protect privacy, collected data have been anonymized and all @mentions have been replaced by\nNONAME, after ensuring that it is not present in any tweet. Further on, the tokenNONAME is processed in\norder to have no impact on the semantics of the messages.\nLAD (Large Arabizi Dataset) contains 7.7 million tweets.\n3.2 Annotation according to sentiment (SALAD)\nWe randomly extracted and manually annotated 1,700 tweets from LAD, in order to create SALAD\n(Sentiment Annotations from LAD).\nClassic sentiment annotation is focused on Positive versus Negative, sometimes also including Neu-\ntral class. As many social media posts from LAD are difﬁcult to understand without context, we de-\ncided to use ﬁve classes: Positive, Negative, Neutral, Conflict and ConflictTxtvsEm.\nNeutral tweets do not express any feelings.Conflict class corresponds to tweets that contain both a\nnegative and a positive sentiment. For example, the sentenceMsh 3arfa adhak wala a3ayatwhich means\nin English “I don’t know if I laugh or I cry” is annotated as Conﬂict. ConflictTxtvsEm represents\ntweets containing emojis in conﬂict with the emotion expressed by the text. This class can sometimes be\ninterpreted as irony or sarcasm. For example, “dlw2ty any z3lan,” means “Now I’m angry,”.\nTable 1 provides information about the sentiment classes of SALAD: SALAD contains 50.5% of\nPositive tweets, 24.8% Negative tweets, 11.9% of Neutral tweets without any sentiment, 5%\nof tweets containing aConflict of sentiment and 7.6% of tweets tagged asConflictTxtvsEm. The\nproportions are imbalanced, which makes any supervised learning task more difﬁcult. In comparison,\nSA datasets do not exhibit any particular distribution: for example, SemEval 2015 task 10 (Rosenthal et\n7http://saphirs.projets.litislab.fr/\n8https://github.com/twintproject/twint\n9The list of all the keywords is available on the dataset description.\n10Comes from Urdu-izi: some of the seed keywords are also used in Urdu. Urdu is among the ofﬁcial national languages of\nPakistan and is transliterated similarly as Arabizi in social media.\n597\nClass Tweets Length Emojis Mentions Hashtags\nPositive 859 50.6 0.692 0.064 0.008\nNegative 422 64.6 0.244 0.014 0.004\nNeutral 203 44.4 0.167 0.030 0.010\nConflict 86 72.7 0.418 0.034 0.012\nConflictTxtvsEm 130 45.5 1 0.084 0.007\nTable 1: Class distribution on SALAD. Length, emojis, mentions and hashtags are averaged per tweet.\nal., 2015) contains 3 classes (Positive, Negative and Neutral) with an imbalanced distribution, whereas\nIberEval (Fersini et al., 2018) has only 2 classes (mysoginist or not) with a balanced distribution.\nThe average length of the tweets in SALAD is 54 characters (52 for LAD). On average, a tweet in\nSALAD contains 0.53 emojis (1.03 for LAD), less than 0.01 hashtag (0.06 for LAD) and 0.04 men-\ntions (0.48 for LAD). The difference seems to be important for emojis and mentions, but mentions are\nanonymized and therefore do not provide much information. Over the different classes, emojis are the\nmost variant, from 0.16 to 1. Actually, this is not surprising, asConflictTxtVsEm requires an emoji,\nwhereas Neutral does not express any sentiment and logically exhibits a weaker proportion of emojis.\nA more interesting aspect concerns the difference between Negative and Positive, where emojis\nseems to be more often used to express positive sentiments than negative ones on this dataset.\n4 Experiments: Arabizi language models and sentiment classiﬁers\nLAD is exploited as a pre-training dataset to construct several Arabizi language models which are then\nﬁne-tuned and compared on two different Arabizi SA tasks.\n4.1 Language models: architecture\nAs detailed in Section 2.4, the multilingual version of BERT (Devlin et al., 2018), BERT-base-\nmultilingual-uncased (BMU), serves as baseline for our experiments. The Huggingface implementation\nof BERT11 is used since it obtains the exact same results as the original version (Wolf et al., 2019).\nThree other language models are tested against BMU, all of them grounded on the BMU architecture.\nThe ﬁrst proposed model exploits the BMU vocabulary, i.e. the token list, as well as the BMU weights by\nretraining BMU on LAD (BAERT-bmu-retrain). The second model only uses the token list of BMU but\nis pre-trained from scratch on LAD (BAERT-bmu-scratch). Finally, the third model learns from scratch\nboth an Arabizi-speciﬁc token list and the weights (BAERT-arz-scratch).\n4.2 Language models: pre-training on LAD\nBAERT-bmu-retrain, BAERT-bmu-scratch and BAERT-arz-scratch are pre-trained on LAD, varying ac-\ncording to vocabulary (token list) and to initial weights.\nToken list construction\nBAERT-bmu-retrain and BAERT-bmu-scratch exploit the BMU token vocabulary to tokenize any text as\ninput of the models. Possible modiﬁcations of the vocabulary consist in adding speciﬁc tokens, as BMU\ncontains some unused token slots. We therefore only add a NONAME token to encode the @mentions.\nOn the contrary, BAERT-arz-scratch has his own Arabizi vocabulary. We create a speciﬁc token list,\ncalled Arz-vocabulary, thanks to Byte Pair Encoding (Sennrich et al., 2016) applied on LAD and more\nspeciﬁcally the SentencePiece implementation 12. Arz-vocabulary encodes Egyptian Arabizi.\nMask language modeling as pre-training task\nBERT-like language models can be pre-trained thanks to two different tasks: MLM and/or NSP. NSP\nconsists in learning if two segments of text come from the same text or not. However, several experiments\nhave highlighted better or similar results without NSP (Liu et al., 2019; Joshi et al., 2019; Lan et al.,\n2019). We thus decide to exclude NSP from our pre-training step and to only focus on MLM.\n11https://huggingface.co/transformers/index.html\n12https://github.com/google/sentencepiece\n598\nMLM usually obtains good performance as pre-training step for Transformer model, in order to solve\nNLP tasks, in particular with long sequences of text (Devlin et al., 2018). As tweets are short texts, often\nconsisting of a single sentence, we concatenate tweets up to the model size (512 tokens): it enables to\naccelerate the training (padding is replaced by data and more data is included in each batch).\nClassic BERT parameters for pre-training are used (Devlin et al., 2018): a learning rate of 10−4, a\nL2 weight decay of 0.01. For computational reasons, a batch size of 64 replace the usual 256 one.\nAfter some epochs, the learning rate is divided by 10, as the number of epochs depends on the model\nconvergence. we observed that BAERT-bmu-retrain and BAERT-bmu-scratch converges much faster\nthan BAERT-arz-scratch. We also adopt the dynamic masking of RoBERTa (Liu et al., 2019), as it has\ndemonstrated some improvements compared to the static masking of BERT.\n1% of the training set (∼77,800 tweets) serves as validation set to check the convergence of the models.\n4.3 Fine-tuning for sentiment analysis on Arabizi\nThe four language models are ﬁne-tuned to be evaluated on two Arabizi sentiment-oriented datasets:\nSALAD and TUNIZI dataset (Fourati et al., 2020). TUNIZI contains 22 of the 100 most frequent words\nin common with LAD.\nOn SALAD, 3 different classiﬁers are proposed: a ﬁrst classiﬁer using the initial 5 classes, a second\none with only 4 classes by merging the two conﬂict classes and a last one with only 3 classes (positive,\nnegative and other). We decide to test multiple classiﬁers on SALAD in order to manage the imbalance\ncharacteristic of the dataset. The TUNIZI dataset is annotated into 2 classes, positive and negative.\nAs recommended in (Devlin et al., 2018), the following parameters are used during the ﬁne-tuning\nstep: a learning rate of 3.10−5 with a linear scheduler and a warm-up of 1 epoch, a L2 weight decay\nof 0.01 and a batch size of 32. The test protocol consists in a 10 stratiﬁed shufﬂe split of each dataset,\nwith 90% of the datasets in the training set and 10% in the test set for each split. Every model is ﬁne-\ntuned on each split during 5 epochs, slightly more than recommended in (Devlin et al., 2018) to ensure\ncomplete process. F1-score averages and standard deviations are then computed concerning the best of\neach training session (models are tested every half epoch), for each model on each task.\n5 Results and Discussion\nWe ﬁrst compare BMU-vocabulary with Arz-vocabulary, then the results of the ﬁne-tuning experiments\non the two Arabizi SA tasks are provided.\n5.1 Comparison between BMU and Arz vocabularies\nSome metrics can help to highlight the importance of a vocabulary compared to another one. Actually,\nchanging the token list in a Transformer model breaks any pre-training, except for minor changes such\nas adding a speciﬁc token.\nThe ﬁrst metric proposed consists in counting the number of common tokens between two vocabular-\nies. The BMU vocabulary contains 105,879 tokens. Arz vocabulary contains 32,002 tokens, which is a\nstandard value for a single language model (e.g. the English BERT uncased vocabulary contains almost\n30,500 tokens). On top of that, the Byte Pair Encoding algorithm used to build the Arz-vocabulary sorts\nthe tokens by occurrence: Figure 1 enables to explore if the common tokens are also the most frequent\nones or not. Between Arz and BMU, this is not the case (among the 5,000 most frequent tokens, less\nthan 50% are common tokens). On the 32,002 tokens from Arz-vocabulary, 9,267 tokens are common\nwith the BMU vocabulary: a coverage rate of 28.9% (over the Arz-vocabulary). In other words, more\nthan 70% of the Arz-vocabulary tokens are not in the BMU vocabulary.\nWe then propose a second metric which consists in counting the number of tokens needed to tokenize\na dataset and comparing it to the mean number of tokens per word. Results are presented on Table 2.\nThe difference between both vocabularies is quite important on LAD and on the TUNIZI dataset: almost\n18% more tokens are necessary for both datasets with the BMU vocabulary than with the Arz-vocabulary,\nwhile the number of tokens in the vocabulary is divided by almost 3.3.\n599\nFigure 1: Common tokens between Arz-vocabulary and BMU vocabulary. The Arz-vocabulary tokens\nare sorted by decreasing occurrence.\nFinally, the number of [UNK] tokens over all the tokens can be computed: [UNK] tokens correspond\nto characters that are not present in the vocabulary. Results are presented on Table 3. More [UNK]\ntokens appear on LAD when using the BMU vocabulary compared to using Arz-V ocabulary. In other\nwords, much missing information appear with BMU on LAD. On the other hand, the contrary can be\nobserved on the TUNIZI dataset: the BMU vocabulary can represent more information than the Arz-\nvocabulary, but the proportion of[UNK] tokens for Arz-vocabulary is not high, as it is the similar to the\nproportion on LAD.\nDataset Characters BMU vocabulary Arz-vocabulary\nLAD 5.25 1.97 1.67\nTUNIZI 5.84 2.19 1.86\nTable 2: Tokens per word depending of the vocabulary used. The Characters column corresponds to the\naverage number of characters per word in the dataset.\nDataset BMU [UNK] frequence Arz-vocabulary [UNK] frequence\nLAD 3.1% 0.6%\nTUNIZI 0.01% 0.6%\nTable 3: Frequence of [UNK] token over all tokens needed to represent the corpora.\n5.2 Results of ﬁne-tuning\nTable 4 presents the results of classiﬁcation protocols as described in Section 4.3.\nThe best model on SALAD is BAERT-arz-scratch, and BAERT-bmu-retrain on the TUNIZI dataset.\nThis can be explained by the nature of both datasets: SALAD contains mostly Egyptian Arabizi which\nis often mixed with English, and TUNIZI contains Tunisian Arabizi which is mostly mixed with French.\nThe BMU model is pretrained on 102 languages, including French and English. BAERT-bmu-retrain has\nalready seen French sentences which can help for TUNIZI dataset; on the contrary, BAERT-arz-scratch\nonly encountered Arabizi mixed with English. However, both models obtain close results on this task:\nthe difference between them is lower than their Standard Deviation.\nComparing BAERT-bmu-scratch and BAERT-bmu-retrain corresponds to an ablation study. The lower\nperformance of BAERT-arz-scratch on TUNIZI is probably due to the token list, as there is less difference\n600\nbetween BAERT-bmu-scratch and BAERT-bmu-retrain than between BAERT-bmu-scratch and BAERT-\narz-scratch. Logically, both vocabularies are not ﬁtting the dataset perfectly, but the BMU token list is\nmore generic than the Arz-vocabulary (it can handle better french words, for example, and generates less\n[UNK] tokens on TUNIZI, as shown on Table 3).\nOn the 5 classes task, an analysis of the confusion matrix shows that the main challenges concern the\nConflict class and the ConflictTxtvsEm class. Only BAERT-arz-scratch obtains a decent result\non the last one (average F1 of the class of 70% for BAERT-arz-scratch, 45% for BAERT-bmu-retrain)\nand all the models failed to handle the Conflict class properly (average F1 is no more than 5% for all\nthe models). This is probably due to the data imbalance (the Conflict class represents only 5% of the\ntotal dataset) and the difﬁculty of the task, and explains the large improvements when we merge these\nclasses together or with the Neutral class.\nModel \\Dataset SALAD 5 classes σ 4 classes σ 3 classes σ TUNIZI σ\nBMU 41.6 4.0 51.4 3.7 58.9 3.0 78.8 2.4\nBAERT-bmu-retrain 52.3 2.0 63.6 1.9 68.2 2.7 83.8 1.1\nBAERT-bmu-scratch 52.0 3.9 62.5 2.8 67.4 2.8 83.6 1.6\nBAERT-arz-scratch 59.9 2.8 71.2 2.4 74.4 3.3 82.7 2.6\nTable 4: F1-Score averages and Standard deviation (in percentage) of the protocol described in Sec-\ntion 4.3. Best results for each task appear in bold.\nTo summarize: 1) exploiting the BMU pre-trained weights is almost ineffective concerning perfor-\nmance (even if the convergence is 2x faster, as we needed 200k batches of 64 for BAERT-bmu-retrain,\nand 400k for BAERT-bmu-scratch) and 2) adapting the vocabulary to Arabizi improves the performance.\n6 Conclusion and future work\nArabizi is a challenge for computational linguistic: there exist only few data resources, no ofﬁcial gram-\nmar nor orthography. Moreover, the language is constantly evolving through time and space, as each\ncountry has its speciﬁc dialect. Language models such as ELMo (Peters et al., 2018) or BERT (Devlin\net al., 2018) get state-of-the-art results on most of the NLP tasks. Their pre-trained models, in English\nor multilingual versions, obtain excellent performance on a lot of use-cases, but do not perform correctly\non Arabizi, due to the lack of any large corpus in such language to carry out a pre-training step.\nOur contribution in this article is twofold: Firstly, we provided LAD, a Large Arabizi Dataset consti-\ntuted of tweets. From this dataset, we made SALAD, a smaller manually annotated Arabizi dataset for\nSentiment Analysis, taking into account sentiment conﬂicts in the text or between the text and the emojis.\nSecondly, we trained BAERT, a BERT-like language model speciﬁc to Arabizi and obtained good results\non Tunizi and Egyptizi-like SA task, showing that a pre-training on a local speciﬁc Arabizi dialect can\nhelp to get better results on this speciﬁc dialect but also on other ones. A pre-training from scratch with a\nnew vocabulary seems preferable for a new language, even for a language that is a mix of different ones\n(Arabizi is a mix of English, French and Arabic using Latin alphabet).\nIn the future, we plan to exploit LAD to train a model on different NLP tasks such as translation\nwith English, or transliteration with MSA. Also, the metrics on the efﬁciency of the tokenizer seem an\ninteresting lead to follow to evaluate the interest between pre-training from scratch a Transformer model\nor exploiting pre-trained weights from an existing model. In particular, the frequence of unkown tokens\nseems to be promising, and need to be further investigated, with other Arabizi corpora from different\nlocal dialects for example. Pre-training a Transformer model from scratch is computationally expensive\ncomparing to start with already pre-trained weights, and ﬁnding metrics that can help to make this choice\nwithout any excessive computational cost could be really useful.\nAcknowledgements\nPart of this work was performed using the computing resources of CRIANN (Normandy, France) and\nwithin the SAPhIRS project13.\n13http://saphirs.projets.litislab.fr/\n601\nReferences\nAhmed Abbasi, Hsinchun Chen, and Arab Salem. 2008. Sentiment analysis in multiple languages: Feature\nselection for opinion classiﬁcation in web forums. ACM Transactions on Information Systems (TOIS), 26(3):12.\nWid H. Allehaiby. 2013. Arabizi: An analysis of the Romanization of the Arabic Script from a Sociolinguistic\nPerspective. Arab World English Journal (AWEJ), 4(3):52–62.\nAnn Bies, Zhiyi Song, Mohamed Maamouri, Stephen Grimes, Haejoong Lee, Jonathan Wright, Stephanie Strassel,\nNizar Habash, Ramy Eskander, and Owen Rambow. 2014. Transliteration of arabizi into arabic orthography:\nDeveloping a parallel annotated arabizi-arabic script sms/chat corpus. In Proceedings of the EMNLP 2014\nWorkshop on Arabic Natural Language Processing (ANLP), pages 93–103.\nCagatay Catal and Mehmet Nangir. 2017. A sentiment classiﬁcation model based on multiple classiﬁers. Applied\nSoft Computing, 50:135–141.\nJ. Chung, C. Gulcehre, K.-H. Cho, and Y . Bengio. 2014. Empirical evaluation of gated recurrent neural networks\non sequence modeling. arXiv preprint arXiv:1412.3555.\nRyan Cotterell and Chris Callison-Burch. 2014. A multi-dialect, multi-genre corpus of informal written Arabic.\nIn Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard,\nJoseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Ninth Inter-\nnational Conference on Language Resources and Evaluation (LREC’14), Reykjavik, Iceland, may. European\nLanguage Resources Association (ELRA).\nRyan Cotterell, Adithya Renduchintala, Naomi Saphra, and Chris Callison-Burch. 2014. An algerian arabic-\nfrench code-switched corpus. In LREC Workshop on Free/Open-Source Arabic Corpora and Corpora Process-\ning Tools.\nKareem Darwish. 2014. Arabizi Detection and Conversion to Arabic. ANLP 2014, page 217.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: pre-training of deep bidirec-\ntional transformers for language understanding. CoRR, abs/1810.04805.\nR. M. Duwairi, M. Alfaqeh, M. Wardat, and A. Alrabadi. 2016. Sentiment analysis for Arabizi text. In 2016 7th\nInternational Conference on Information and Communication Systems (ICICS), pages 127–132.\nRamy Eskander, Mohamed Al-Badrashiny, Nizar Habash, and Owen Rambow. 2014. Foreign words and the\nautomatic processing of Arabic social media text written in roman script. EMNLP 2014, page 1.\nElisabetta Fersini, Paolo Rosso, and Maria Anzovino. 2018. Overview of the task on automatic misogyny identi-\nﬁcation at ibereval 2018. In IberEval@SEPLN.\nChayma Fourati, Abir Messaoudi, and Hatem Haddad. 2020. TUNIZI: a Tunisian Arabizi sentiment analysis\nDataset. CoRR, abs/2004.14303.\nGuillaume Gadek, Joseﬁn Betsholtz, Alexandre Pauchet, Stephan Brunessaux, Nicolas Malandain, and Laurent\nVercouter. 2017. Extracting contextonyms from twitter for stance detection. In Proceedings of ICAART17,\npages 132–141.\nAbigail S Gertner, John Henderson, Elizabeth Merkhofer, Amy Marsh, Ben Wellner, and Guido Zarrella. 2019.\nMitre at semeval-2019 task 5: Transfer learning for multilingual hate speech detection. In Proceedings of the\n13th International Workshop on Semantic Evaluation, pages 453–459.\nMaria Giatsoglou, Manolis G V ozalis, Konstantinos Diamantaras, Athena Vakali, George Sarigiannidis, and Kon-\nstantinos Ch Chatzisavvas. 2017. Sentiment analysis leveraging emotions and word embeddings. Expert\nSystems with Applications, 69:214–224.\nYves Gonzalez-Quijano. 2014. Technology literacies of the new media: Phrasing the world in the “arab easy”\n(r)evolution. In Leila Hudson, Adel Iskandar, and Mimi Kirk, editors, Media Evolution on the Eve of the Arab\nSpring, pages 159–166. Palgrave Macmillan, New York.\nImane Guellil, Ahsan Adeel, Faical Azouaou, Fodil Benali, Ala-eddine Hachani, and Amir Hussain. 2018. Arabizi\nsentiment analysis based on transliteration and automatic corpus annotation. InProceedings of the 9th Workshop\non Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 335–341, Brussels,\nBelgium, October. Association for Computational Linguistics.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.\n602\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2019. Spanbert:\nImproving pre-training by representing and predicting spans. CoRR, abs/1907.10529.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. Bag of Tricks for Efﬁcient Text\nClassiﬁcation. arXiv:1607.01759 [cs], August. arXiv: 1607.01759.\nGuillaume Lample and Alexis Conneau. 2019. Cross-lingual language model pretraining. CoRR, abs/1901.07291.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019.\nAlbert: A lite bert for self-supervised learning of language representations.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR,\nabs/1907.11692.\nLouis Martin, Benjamin Muller, Pedro Javier Ortiz Su ´arez, Yoann Dupont, Laurent Romary, ´Eric de la Clergerie,\nDjam´e Seddah, and Beno ˆıt Sagot. 2020. CamemBERT: a tasty French language model. In Proceedings of\nthe 58th Annual Meeting of the Association for Computational Linguistics, pages 7203–7219, Online, July.\nAssociation for Computational Linguistics.\nDiana Maynard and Adam Funk. 2011. Automatic detection of political opinions in tweets. In The semantic web:\nESWC 2011 workshops, pages 88–99. Springer.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efﬁcient Estimation of Word Representations\nin Vector Space. arXiv:1301.3781 [cs], September. arXiv: 1301.3781.\nMartin M¨uller, Marcel Salath´e, and Per E Kummervold. 2020. Covid-twitter-bert: A natural language processing\nmodel to analyse covid-19 content on twitter.\nMahmoud Nabil, Mohamed Aly, and Amir Atiya. 2015. ASTD: Arabic Sentiment Tweets Dataset. InProceedings\nof the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2515–2519, Lisbon,\nPortugal, sep. Association for Computational Linguistics.\nJeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word represen-\ntation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP),\npages 1532–1543.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word representations. arXiv:1802.05365 [cs], February. arXiv:\n1802.05365.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual bert? In Anna Korho-\nnen, David R. Traum, and Llu ´ıs M`arquez, editors, Proceedings of the 57th Conference of the Association for\nComputational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages\n4996–5001. Association for Computational Linguistics.\nStephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, and others.\n1995. Okapi at TREC-3. Nist Special Publication Sp, 109:109.\nSara Rosenthal, Preslav Nakov, Svetlana Kiritchenko, Saif Mohammad, Alan Ritter, and Veselin Stoyanov. 2015.\nSemEval-2015 task 10: Sentiment analysis in twitter. In Proceedings of the 9th International Workshop on\nSemantic Evaluation (SemEval 2015), pages 451–463, Denver, Colorado, June. Association for Computational\nLinguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword\nunits. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016,\nAugust 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics.\nTaha Tobaili, Miriam Fernandez, Harith Alani, Sanaa Sharafeddine, Hazem Hajj, and Goran Glavaˇs. 2019. SenZi:\nA sentiment analysis lexicon for the latinised Arabic (Arabizi). In Proceedings of the International Confer-\nence on Recent Advances in Natural Language Processing (RANLP 2019), pages 1203–1211, Varna, Bulgaria,\nSeptember. INCOMA Ltd.\nTaha Tobaili. 2016. Arabizi Identiﬁcation in Twitter Data. In Proceedings of the ACL 2016 Student Research\nWorkshop, pages 51–57, Berlin, Germany, aug. Association for Computational Linguistics.\n603\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. 2017. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,\nHanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman Garnett, editors, Advances in Neural\nInformation Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9\nDecember 2017, Long Beach, CA, USA, pages 5998–6008.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, R’emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface’s transformers: State-of-the-\nart natural language processing. ArXiv, abs/1910.03771."
}