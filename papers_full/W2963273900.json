{
    "title": "Image-and-Spatial Transformer Networks for Structure-Guided Image Registration",
    "url": "https://openalex.org/W2963273900",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4302259456",
            "name": "Lee, Matthew C. H.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2745071069",
            "name": "Oktay, Ozan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2744596498",
            "name": "Schuh, Andreas",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287899803",
            "name": "Schaap, Michiel",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2745262883",
            "name": "Glocker, Ben",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2752785527",
        "https://openalex.org/W2604920239",
        "https://openalex.org/W2891590469",
        "https://openalex.org/W2963196212",
        "https://openalex.org/W2115167851",
        "https://openalex.org/W2751297520",
        "https://openalex.org/W2809972409",
        "https://openalex.org/W2816936748",
        "https://openalex.org/W603908379",
        "https://openalex.org/W3104164805"
    ],
    "abstract": "Image registration with deep neural networks has become an active field of research and exciting avenue for a long standing problem in medical imaging. The goal is to learn a complex function that maps the appearance of input image pairs to parameters of a spatial transformation in order to align corresponding anatomical structures. We argue and show that the current direct, non-iterative approaches are sub-optimal, in particular if we seek accurate alignment of Structures-of-Interest (SoI). Information about SoI is often available at training time, for example, in form of segmentations or landmarks. We introduce a novel, generic framework, Image-and-Spatial Transformer Networks (ISTNs), to leverage SoI information allowing us to learn new image representations that are optimised for the downstream registration task. Thanks to these representations we can employ a test-specific, iterative refinement over the transformation parameters which yields highly accurate registration even with very limited training data. Performance is demonstrated on pairwise 3D brain registration and illustrative synthetic data.",
    "full_text": "Image-and-Spatial Transformer Networks\nfor Structure-Guided Image Registration\nMatthew C.H. Lee, Ozan Oktay, Andreas Schuh\nMichiel Schaapâˆ—, and Ben Glockerâˆ—\nHeartFlow, USA\nBiomedical Image Analysis Group, Imperial College London, UK\nâˆ— Shared senior authorship.\nAbstract. Image registration with deep neural networks has become an\nactive ï¬eld of research and exciting avenue for a long standing problem in\nmedical imaging. The goal is to learn a complex function that maps the\nappearance of input image pairs to parameters of a spatial transforma-\ntion in order to align corresponding anatomical structures. We argue and\nshow that the current direct, non-iterative approaches are sub-optimal,\nin particular if we seek accurate alignment of Structures-of-Interest (SoI).\nInformation about SoI is often available at training time, for example,\nin form of segmentations or landmarks. We introduce a novel, generic\nframework, Image-and-Spatial Transformer Networks (ISTNs), to lever-\nage SoI information allowing us to learn new image representations that\nare optimised for the downstream registration task. Thanks to these rep-\nresentations we can employ a test-speciï¬c, iterative reï¬nement over the\ntransformation parameters which yields highly accurate registration even\nwith very limited training data. Performance is demonstrated on pairwise\n3D brain registration and illustrative synthetic data.\n1 Introduction\nImage registration remains a fundamental problem in medical image computing,\nwhere the goal is to estimate a spatial transformationTÎ¸ : Rd â†’Rd mapping cor-\nresponding anatomical locations betweend-dimensional images. The most widely\nused approach is intensity-based registration formalised as an optimisation prob-\nlem seeking optimal transformation parameters Î¸ that minimise a dissimilarity\nmeasure (or cost function) L(M â—¦TÎ¸,F), where M is the moving source image\nundergoing spatial transformation and F is the ï¬xed target image. We refer the\nreader to [1] for a detailed overview of what we here calltraditional methods, i.e.,\nnon-learning based approaches making use of iterative optimisation strategies to\nminimise the cost function for a given pair of images.\nRecently, the use of neural networks to learn the complex mapping from\nimage appearance to spatial transformation has become an active ï¬eld of re-\nsearch [2â€“7], providing a new perspective on tackling challenging registration\nproblems. So called supervised approaches [2, 3] have been used, which are sim-\nilar in nature to methods used for image segmentation, where a convolutional\narXiv:1907.09200v1  [cs.CV]  22 Jul 2019\n2 M.C.H. Lee, O. Oktay, A. Schuh, M. Schaap, B. Glocker\nFig. 1: A toy example (vessel trees on white box) illustrating the beneï¬t of\nstructure-guided image registration. The initial alignment of images (a,b) is\nshown in (c) and after intensity-based aï¬ƒne registration in (d). Registration\nfocuses on aligning the white box ignoring Structures-of-Interest (SoI), i.e., the\nvessels. Our ISTNs learn to focus on the SoI yielding accurate alignment in (e).\nneural network is trained to predict the transformation directly using examples of\nimages and their ground truth transformation. Because actual ground truth for\nTÎ¸ is not available, either random transformations are used to generate synthetic\nexamples or a well established traditional registration method is employed to ob-\ntain reference transformations. Neither is optimal, as synthetic transformations\nmight not be realistic and/or yield poor generalisation, while when employing a\ntraditional method the prediction accuracy of the trained network is inherently\nlimited by the accuracy of that method. One might argue that in this case the\nneural network is mostly learning to replicate the traditional method, although\nwith a potentially remarkable computational speed up.\nDue to the limitations of supervised methods, a number of works have then\nconsidered so called unsupervised approaches [6, 7] where a neural network is\ntrained based on the original cost function of traditional intensity-based meth-\nods minimising a dissimilarity measure such as mean squared intensity diï¬€er-\nences and others. While training over large number of examples might indeed be\nbeneï¬cial for optimising the cost function (it could have a regularisation eï¬€ect\nor improve generalisation), these unsupervised approaches cannot be expected\nto perform fundamentally better compared to traditional methods as the exact\nsame function is optimised. One might even argue that traditional methods are\nmore ï¬‚exible, as they can adapt to any new pair of test images, and are not lim-\nited to register images of similar appearance as the training data. For example,\nmany of the traditional methods discussed in [1] can be equally used for brain\nMRI and lung CT with maybe only a few changes to some hyper-parameters.\nA recent hybrid approach [8] is combining an unsupervised and supervised cost\nfunction using a traditional method to generate training deformation ï¬elds.\nOverall, one may argue that neural network based image registration, so\nfar, has not taken full advantage of deep representation learning but mostly led\nto a speed up in the time it takes to register two images. At the same time,\none may argue that registration accuracy is of higher importance than speed\nin many clinical applications. We observe that neither the supervised nor the\nunsupervised methods exploit two key advantages of neural networks, which are\n1) the ability to learn new representations that are optimised for a downstream\nImage-and-Spatial Transformer Networks 3\nğ‘€\nğ¹\nğœƒ\nğ¹â€²\nğ‘€â€²ğ‘€\nğ¹\nğœƒ\nITN\nITN\nmoving fixed\nğ‘†ğ‘€\nğ‘†ğ¹\nSTN STN\nâ„’ğ¼ğ‘‡ğ‘\nğ‘€\nâ„’ğ¼ğ‘‡ğ‘\nğ¹\nğ¹ğ‘€ğœƒ â„’ğ‘†ğ‘‡ğ‘âˆ’ğ‘¢\nğ‘†ğ¹ğ‘†ğ‘€;ğœƒ â„’ğ‘†ğ‘‡ğ‘âˆ’ğ‘ \nğ‘†ğ¹ğ‘€ğœƒ\nâ€² â„’ğ‘†ğ‘‡ğ‘âˆ’ğ‘–\nğ‘€\nğ¹â€²ğ‘†ğ‘€;ğœƒ â„’ğ‘†ğ‘‡ğ‘âˆ’ğ‘–\nğ¹\nSpatial Transformer Network Image-and-Spatial Transformer Network Training Approaches\nUnsupervised STN\nâ„’ğ‘†ğ‘‡ğ‘âˆ’ğ‘¢\nStructure-guided STN\nâ„’ğ‘†ğ‘‡ğ‘âˆ’ğ‘ \nStructure-guided ISTN explicit\nâ„’ğ¼ğ‘‡ğ‘\nğ‘€ +â„’ğ¼ğ‘‡ğ‘\nğ¹ +â„’ğ‘†ğ‘‡ğ‘âˆ’ğ‘ \nStructure-guided ISTN implicit\nâ„’ğ‘†ğ‘‡ğ‘âˆ’ğ‘–\nğ‘€ +â„’ğ‘†ğ‘‡ğ‘âˆ’ğ‘–\nğ¹ +â„’ğ‘†ğ‘‡ğ‘âˆ’ğ‘ \nloss function\nrepresentation mapping\nshared\nweights\nFig. 2: Overview of image registration using Spatial Transformer Networks\n(STNs), with the classical model shown on the left mapping an image pair M,F\ndirectly to parameters Î¸ of a spatial transformation (cf. [6, 7]). Our Image-and-\nSpatial Transformer Networks (ISTNs) introduce a dedicated image transformer\nnetwork (ITN) to learn to produce image representations Mâ€²,Fâ€² optimised for\nthe downstream registration task, as well as predictingÎ¸. This gives raise to mul-\ntiple ways of training ISTNs by combining diï¬€erent loss functions (see Sec 2.2).\ntask, and 2) the ability to incorporate and beneï¬t from additional information\nduring training that is unavailable (or very diï¬ƒcult to obtain) at test time.\nSome exceptions to the second point are works that consider extra informa-\ntion such as segmentations or weak labels during training [4â€“6]. This additional\nsupervision can help to guide the registration at test time in a diï¬€erent way\nthan using image intensities alone. For example, the registration may focus on\nparticular SoI (cf. Fig 1). However, the current approaches do not retain or\nexplicitly extract such extra information, so it cannot be used further at test\ntime, for example, for reï¬ning the predicted, initial transformation parameters.\nIn fact, most of the current works consider neural network based registration\nas a one-pass (non-iterative) process, which might be sub-optimal as we show\nin our results. The few works that discuss subsequent reï¬nement either suggest\nto use a traditional (iterative) method [6], or to use the network in an auto-\nregressive way [7]. As both rely again on optimising the original intensity-based\ncost function the advantage over a traditional method remains unclear, and any\nextra information that was available during training is somewhat lost.\n1.1 Contributions\nTo overcome these limitations, and to make best use of the key ability of neural\nnetworks to learn representations, we introduce Image-and-Spatial Transformer\nNetworks (ISTNs) where a dedicated Image Transformer Network (ITN) is added\nto the head of a Spatial Transformer Network (STN) aiming to extract and retain\ninformation about SoIs, for which annotations are only required during training.\nWhile the STN predicts the parameters of the spatial transformation, the ITN\nproduces a new image representation which is learned in an end-to-end fashion\n4 M.C.H. Lee, O. Oktay, A. Schuh, M. Schaap, B. Glocker\nand optimised for the downstream registration task. This allows us to not only\npredict a good initial transformation at test time, but enables what we call\nstructure-guided registration with an accurate test-speciï¬c, iterative reï¬nement\nusing the exact same model. An illustrative example of what ISTNs can do and\nwhy structure-guided registration can be useful is shown in Fig. 1. A schematic\noverview of our approach and how it relates to previous work that uses STNs\nonly (such as [6,7]) is shown in Fig. 2.\n2 Image-and-Spatial Transformer Networks\nSpatial Transformer Networks [9] are the building block of most of the recent\nworks on neural network based image registration. An STN is a neural network\nin itself commonly consisting of a few convolutional and fully connected layers\nthat are able to learn a mapping from input images ( M,F ) to parameters Î¸ of a\nspatial transformation TÎ¸. Taking advantage of the fact that image re-sampling\nis a diï¬€erentiable operation, STNs can be trained end-to-end, and plugged as\na module into larger networks (as originally used for improving image classiï¬-\ncation [9]). Revisiting the structure of an STN, we observe that there are two\nmain components: a feature extraction part learning a new representation of the\ninput using convolutional layers, and a second part that maps these represen-\ntations to transformation parameters. We indicate this in Fig. 2 using diï¬€erent\ncolours within the STN module. The representation that STNs may learn, how-\never, is not exposed and remains hidden during inference. This is where our main\ncontribution comes into play where we redesign the basic building block of the\ntransformer module of neural network based image registration by introducing\na dedicated Image Transformer Network.\n2.1 Image Transformer Networks\nWe deï¬ne ITNs to be convolutional neural networks that map an input image\nto an output image of the same size and dimension. In this paper we consider\nthe case where the number of channels is the same for the input and output,\nalthough this does not have to be the case and other variants may be considered.\nThe role of the ITN is to expose explicitly a learned image representation that\nis optimal for the downstream registration task solved by the STN. A shared\nITN for inputs M and F learns to generate outputs Mâ€² and Fâ€² which are fed\ninto a regular STN (cf. Fig. 2). This new architecture gives raise to a number\nof training approaches, in particular, when extra information about SoIs are\navailable, such as image segmentations or landmarks.\n2.2 Explicit and Implicit Training of ISTNs\nAs indicated in Fig. 2, diï¬€erent loss functions can be considered for training\nISTNs. Note that the unsupervised case using image intensities only, here corre-\nsponding to LSTNâˆ’u(MÎ¸,F), is a special case of training an ISTN. We use MÎ¸\nImage-and-Spatial Transformer Networks 5\nFig. 3: For a toy example, we show the progress of the output of the ITN module\nin an ISTN model trained with the implicit loss function LISTN âˆ’i (cf. Eq. (2)\nin Sec. 2.2 and Fig. 2). The top row corresponds to a case where the SoI is\na segmentation-like mask (shown on the most right). The bottom row shows\ntraining with landmark maps. The learned image representation allows accurate\nstructure-guided registration with test-speciï¬c reï¬nement at test time.\nas a short form of Mâ—¦TÎ¸. Similarly, we can incorporate auxiliary information in\nform of segmentations as in [6] or other structural or geometric information via a\nâ€˜structure-guidedâ€™ or â€˜supervisedâ€™ loss LSTNâˆ’s(SM;Î¸,SF). Here, Sare images en-\ncoding SoIs (e.g., organ segmentations, anatomical landmarks, centerlines, etc.),\nand â€˜supervisedâ€™ refers to the fact that such SoIs need annotations on the training\ndata. Note, that neither LSTNâˆ’u nor LSTNâˆ’s will encourage the ITN to learn\nany directly useful representations. In order to facilitate this, we propose two\ndiï¬€erent strategies for training ISTNs when auxiliary information about SoIs is\navailable. The loss function for our explicit ISTN is deï¬ned as\nLISTN âˆ’e = LM\nITN + LF\nITN + LSTNâˆ’s (1)\nwhich combines the supervised STN with an ITN loss LITN (SI,Iâ€²) explicitly\npenalizing diï¬€erences between the SoI encoding SI and the ITN output Iâ€² for\ninput image I. While the explicit loss has the desired eï¬€ect of producing repre-\nsentations capturing the SoI information, the ITN and STN losses are somewhat\ndecoupled where the ITN loss plays the role of deep supervison.\nAn intriguing alternative is the implicit ISTN with a loss function deï¬ned as\nLISTN âˆ’i = LM\nSTNâˆ’i + LF\nSTNâˆ’i + LSTNâˆ’s (2)\nHere, the two terms LM\nSTNâˆ’i(Mâ€²\nÎ¸,SF) and LF\nSTNâˆ’i(SM;Î¸,Fâ€²) intertwine the out-\nputs of the ITN ( Mâ€²,Fâ€²), the SoI encodings ( SM,SF) and the estimated trans-\nformation parameters Î¸ from the STN. Combined with the supervised STN loss\nthis gives raise to a fully end-to-end training of image representations that are\noptimised for the downstream registration task. In Fig. 3 we show how the ITN\nrepresentations evolve over the course of training on an illustrative toy example\nboth for the case of segmentations and landmark annotations. Due to space rea-\nsons, we omit the ï¬gure for the explicit ISTN which shows similar results with\nslightly sharper representations due to the ITN loss.\n6 M.C.H. Lee, O. Oktay, A. Schuh, M. Schaap, B. Glocker\nIn this paper, for all above mentioned loss functions we use the mean squared\nerror (MSE) loss. Other losses can be considered but we ï¬nd MSE to work very\nwell. It also allows us to ï¬‚exibly incorporate diï¬€erent types of SoI information\nby simply representing it in the form of real-valued images. This is straightfor-\nward for binary segmentations, and anatomical landmarks can be, for example,\nencoded via distance maps or smoothed centroid maps (cf. bottom row of Fig. 3).\n2.3 Test-speciï¬c Iterative Reï¬nement\nThe key of ISTNs is that they enable structure-guided, test-speciï¬c reï¬nement\nbased on the learned representations Mâ€² and Fâ€² that are exposed by the ITN.\nThough any registration technique can be used for reï¬nement by using the in-\nferred images Mâ€²and Fâ€²as inputs, we can also directly leverage the STN module\nitself to perform the reï¬nement at inference time by iteratively updating the STN\nweights (keeping the ITN weights ï¬xed) through minimization of a reï¬nement\nloss denoted as LSTNâˆ’r(Mâ€²\nÎ¸,Fâ€²). Note, that no annotations are required for the\ntest images to perform this iterative reï¬nement, as the necessary representations\nMâ€²and Fâ€²are generated by the trained ITN.\n2.4 Transformation Models\nSTNs make no assumption about the employed spatial transformation model,\nand hence, our ISTN architecture remains generic allowing the integration of\nvarious linear and non-linear transformation models. In this paper, we consider\ntwo speciï¬c transformation models as a proof-of-concept. We employ a typical\nparameterisation for aï¬ƒne transformations decomposed into\nTaï¬ƒne\nÎ¸ = MtRÏ†Sâˆ’1\nÏˆ DsSÏˆ (3)\nwhere Mt, RÏ†, Ds and SÏˆ are the translation, rotation, scaling and shearing\nmatrices. This decomposition allows us to set intuitive bounds on the individual\ntransformation parameters by using appropriately scaled tanh functions. For\nnon-linear registration we employ a standard B-spline parameterisation using a\nregular grid of control points. Transformation parameters Î¸ then correspond to\ncontrol point displacements. Details on this, with emphasis on their applications\nand implementations within neural network architectures can be found in [7,10].\n3 Experiments\nWe evaluate ISTNs on the task of pairwise registration of brain MRI (similar\nto [6, 8]). In the ï¬rst set of experiments, we use 420 individual subjects from\nthe UK Biobank Imaging Study 1 to form 100 random pairs of moving and ï¬xed\nimages for training, 10 pairs for validation, and 100 pairs for testing. For each\n1 UK Biobank Resource under Application Number 12579\nImage-and-Spatial Transformer Networks 7\nFig. 4: Visual results for aï¬ƒne registration with SoI overlaid in red (moving),\nand green (ï¬xed). On the left: the input images before registration. Close-ups\nfrom left to right: initial alignment, STN-u, STN-s, ISTN-e, ISTN-i.\nimage, segmentations of sub-cortical structures are available. Images are skull-\nstripped and intensity normalised. Binarised segmentation label maps are used\nas the SoI information. Due to space reasons, we focus on the results while\nour publicly available code 2 contains details on architectural choices, hyper-\nparameters and training conï¬gurations.\nWe assess agreement of SoI after registration by calculating Dice scores and\naverage surface distances (ASDs). We compare the explicit and implicit variants\n(ISTN-e and ISTN-i) with unsupervised and supervised models (STN-u and\nSTN-s). These baselines are conceptually similar to the approaches in [6,7]. The\nbaselines have been set up in a competitive way and we checked for proper\nconvergence and best possible performance on the validation set.\nTable 1 summarizes the quantitative results before and after iterative re-\nï¬nement using the four diï¬€erent approaches with aï¬ƒne and non-linear B-spline\ntransformation models. For the B-splines we use a 30 mm control point spacing\nand start registration from rigidly pre-aligned images. For both aï¬ƒne and B-\nsplines we provide numbers for the initial state (Id) and results when the SoI is\nused directly for registration as an upper bound â€œbest-caseâ€ reference. We note\nthat ISTN-e/i outperform STN-u/s for the one-pass predictions. Test-speciï¬c\nreï¬nement boosts the accuracy signiï¬cantly for all approaches. ISTNs achieve\noverall highest Dice scores and lowest ASDs, sometimes close to the best-case.\nSTN-u/s after reï¬nement converge to the same lower accuracy as no SoI infor-\nmation can be leveraged. Fig. 4 shows qualitative results after reï¬nement for\naï¬ƒne registration, while Fig. 5 shows an example of learned SoI representations.\nWe repeat the experiments but with 1,000 pairs for training instead of 100.\nThe one-pass predictions improve for all four methods with Dice scores of 0.75\n(STN-u), 0.77 (STN-s), and 0.80 (ISTN-e/i) for aï¬ƒne, and 0.79, 0.83, and 0.86\n(same order) for non-linear. Iterative reï¬nement yields quasi identical results for\nSTN-u/s as before when trained with 100 pairs, and slightly improves for ISTNs.\nRemarkably, ISTNs trained with only 100 pairs achieve much higher accuracy\nthan STN-u and STN-s trained with 1,000 pairs, indicating excellent data ef-\nï¬ciency and beneï¬t of iterative, test-speciï¬c reï¬nement for image registration.\n2 https://github.com/biomedia-mira/istn\n8 M.C.H. Lee, O. Oktay, A. Schuh, M. Schaap, B. Glocker\nTable 1: Summary of registration results when using 100 training images.\nEvaluated Methods\nTÎ¸ Metric Reï¬ne Id STN-u STN-s ISTN-e ISTN-i SoI\nAï¬ƒne\nDice before 0.53 0.71 0.70 0.75 0.75 0.84after 0.79 0.79 0.83 0.82\nASD before 2.41 1.11 1.10 0.93 0.89 0.50after 0.69 0.69 0.53 0.58\nB-Spline\nDice before 0.70 0.74 0.77 0.80 0.81 0.91after 0.84 0.83 0.86 0.85\nASD before 1.05 0.88 0.74 0.63 0.59 0.27after 0.51 0.52 0.45 0.46\nFig. 5: Overlay of the learned representations of sub-cortical structures as\nheatmaps on top of a test input. The axial, sagittal and coronal slices on the left\ncorrespond to the ITN output of an explicit ISTN-e, the left shows the output\nfor an implicit ISTN-i. Black contours show the ground truth segmentations.\n4 Conclusion\nISTNs are a generic framework for neural network based structure-guided image\nregistration with test-speciï¬c reï¬nement using learned representations. In our ex-\nperiments the explicit and implicit variants perform equally well and outperform\nunsupervised and supervised STNs both before and after reï¬nement. Implicitly\nlearned representations may be beneï¬cial to prevent overï¬tting in cases where\nSoI information contains noise or corruption. This eï¬€ect and other applications\nof ISTNs using diï¬€erent types of SoI will be explored in future work.\nReferences\n1. Sotiras, A., Davatzikos, C., Paragios, N.: Deformable medical image registration:\nA survey. IEEE Transactions on Medical Imaging (2013)\n2. Yang, X., Kwitt, R., Styner, M., Niethammer, M.: Quicksilver: Fast predictive\nimage registrationâ€“a deep learning approach. NeuroImage (2017)\n3. Sokooti, H., de Vos, B., Berendsen, F., Lelieveldt, B.P., IË‡ sgum, I., Staring, M.:\nNonrigid image registration using multi-scale 3d convolutional neural networks.\nIn: In MICCAI. (2017)\n4. RohÂ´ e, M.M., Datar, M., Heimann, T., Sermesant, M., Pennec, X.: Svf-net: Learning\ndeformable image registration using shape matching. In: In MICCAI. (2017)\nImage-and-Spatial Transformer Networks 9\n5. Hu, Y., Modat, M., Gibson, E., Li, W., Ghavami, N., Bonmati, E., Wang, G.,\nBandula, S., Moore, C.M., Emberton, M., Ourselin, S., Noble, J.A., Barratt, D.C.,\nVercauteren: Weakly-supervised convolutional neural networks for multimodal im-\nage registration. Medical Image Analysis (2018)\n6. Balakrishnan, G., Zhao, A., Sabuncu, M.R., Guttag, J., Dalca, A.V.: Voxelmorph: a\nlearning framework for deformable medical image registration. IEEE Transactions\non Medical Imaging (2019)\n7. de Vos, B.D., Berendsen, F.F., Viergever, M.A., Sokooti, H., Staring, M., IË‡ sgum,\nI.: A deep learning framework for unsupervised aï¬ƒne and deformable image reg-\nistration. Medical Image Analysis (2019)\n8. Fan, J., Cao, X., Yap, P.T., Shen, D.: BIRNet: brain image registration using\ndual-supervised fully convolutional networks. Medical Image Analysis (2019)\n9. Jaderberg, M., Simonyan, K., Zisserman, A., Kavukcuoglu, K.: Spatial transformer\nnetworks. In: Advances in Neural Information Processing Systems. (2015)\n10. SandkÂ¨ uhler, R., Jud, C., Andermatt, S., Cattin, P.C.: Airlab: Autograd image\nregistration laboratory. arXiv preprint arXiv:1806.09907 (2018)"
}