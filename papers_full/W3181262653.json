{
  "title": "Long-Short Transformer: Efficient Transformers for Language and Vision",
  "url": "https://openalex.org/W3181262653",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2023943780",
      "name": "Zhu Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1980672870",
      "name": "Ping Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2562332712",
      "name": "Xiao, Chaowei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221882394",
      "name": "Shoeybi, Mohammad",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2744768907",
      "name": "Goldstein, Tom",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202058456",
      "name": "Anandkumar, Anima",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2988481184",
      "name": "Catanzaro, Bryan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3034609440",
    "https://openalex.org/W3139633126",
    "https://openalex.org/W2963088785",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3175515048",
    "https://openalex.org/W2998108143",
    "https://openalex.org/W2947590261",
    "https://openalex.org/W3135921327",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W2953273646",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3037492894",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W3159885121",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W3034363135",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3166738839",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W3123615524",
    "https://openalex.org/W3008736151",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3125056032",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2961301154",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W2962900737",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3134144764",
    "https://openalex.org/W3166398787",
    "https://openalex.org/W2963393721",
    "https://openalex.org/W3173365702",
    "https://openalex.org/W3021293129",
    "https://openalex.org/W2984864519",
    "https://openalex.org/W2780149736",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2984315581",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W2995575179",
    "https://openalex.org/W3036438747",
    "https://openalex.org/W3167695527",
    "https://openalex.org/W3121592593",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3104613728",
    "https://openalex.org/W2963060032",
    "https://openalex.org/W2997347790",
    "https://openalex.org/W3019527251",
    "https://openalex.org/W2178628967",
    "https://openalex.org/W3131500599"
  ],
  "abstract": "Transformers have achieved success in both language and vision domains. However, it is prohibitively expensive to scale them to long sequences such as long documents or high-resolution images, because self-attention mechanism has quadratic time and memory complexities with respect to the input sequence length. In this paper, we propose Long-Short Transformer (Transformer-LS), an efficient self-attention mechanism for modeling long sequences with linear complexity for both language and vision tasks. It aggregates a novel long-range attention with dynamic projection to model distant correlations and a short-term attention to capture fine-grained local correlations. We propose a dual normalization strategy to account for the scale mismatch between the two attention mechanisms. Transformer-LS can be applied to both autoregressive and bidirectional models without additional complexity. Our method outperforms the state-of-the-art models on multiple tasks in language and vision domains, including the Long Range Arena benchmark, autoregressive language modeling, and ImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on enwik8 using half the number of parameters than previous method, while being faster and is able to handle 3x as long sequences compared to its full-attention version on the same hardware. On ImageNet, it can obtain the state-of-the-art results (e.g., a moderate size of 55.8M model solely trained on 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more scalable on high-resolution images. The source code and models are released at https://github.com/NVIDIA/transformer-ls .",
  "full_text": "Long-Short Transformer: Efﬁcient Transformers\nfor Language and Vision\nChen Zhu‡1∗, Wei Ping†2, Chaowei Xiao†2,3, Mohammad Shoeybi2, Tom Goldstein1,\nAnima Anandkumar2,4, and Bryan Catanzaro2\n1University of Maryland, College Park\n2 NVIDIA 3Arizona State University 4California Institute of Technology\n‡chenzhu@cs.umd.edu, †{wping, chaoweix}@nvidia.com\nAbstract\nTransformers have achieved success in both language and vision domains. However,\nit is prohibitively expensive to scale them to long sequences such as long documents\nor high-resolution images, because self-attention mechanism has quadratic time\nand memory complexities with respect to the input sequence length. In this paper,\nwe propose Long-Short Transformer (Transformer-LS), an efﬁcient self-attention\nmechanism for modeling long sequences with linear complexity for both language\nand vision tasks. It aggregates a novel long-range attention with dynamic projection\nto model distant correlations and a short-term attention to capture ﬁne-grained local\ncorrelations. We propose a dual normalization strategy to account for the scale\nmismatch between the two attention mechanisms. Transformer-LS can be applied\nto both autoregressive and bidirectional models without additional complexity.\nOur method outperforms the state-of-the-art models on multiple tasks in language\nand vision domains, including the Long Range Arena benchmark, autoregressive\nlanguage modeling, and ImageNet classiﬁcation. For instance, Transformer-LS\nachieves 0.97 test BPC on enwik8 using half the number of parameters than\nprevious method, while being faster and is able to handle 3×as long sequences\ncompared to its full-attention version on the same hardware. On ImageNet, it can\nobtain the state-of-the-art results (e.g., a moderate size of 55.8M model solely\ntrained on 224 ×224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being\nmore scalable on high-resolution images. The source code and models are released\nat https://github.com/NVIDIA/transformer-ls.\n1 Introduction\nTransformer-based models [ 1] have achieved great success in the domains of natural language\nprocessing (NLP) [2, 3] and computer vision [4–6]. These models beneﬁt from the self-attention\nmodule, which can capture both adjacent and long-range correlations between tokens while efﬁciently\nscaling on modern hardware. However, the time and memory consumed by self-attention scale\nquadratically with the input length, making it very expensive to process long sequences. Many\nlanguage and vision tasks beneﬁt from modeling long sequences. In NLP, document-level tasks\nrequire processing long articles [e.g., 7, 8], and the performance of language models often increases\nwith sequence length [e.g., 9, 10]. In computer vision, many tasks involve high-resolution images,\nwhich are converted to long sequences of image patches before being processed with Transformer\nmodels [4, 6, 11]. As a result, it is crucial to design an efﬁcient attention mechanism for long sequence\nmodeling that generalizes well across different domains.\n∗Work done during an internship at NVIDIA.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2107.02192v3  [cs.CV]  7 Dec 2021\nVarious methods have been proposed to reduce the quadratic cost of full attention. However, an\nefﬁcient attention mechanism that generalizes well in both language and vision domains is less\nexplored. One family of methods is to sparsify the attention matrix with predeﬁned patterns such\nas sliding windows [e.g., 12–15] and random sparse patterns [16]. These methods leverage strong\ninductive biases to improve both computational and model performance, but they limit the capacity\nof a self-attention layer because each speciﬁc token can only attend to a subset of tokens. Another\nfamily of methods leverages low-rank projections to form a low resolution representation of the input\nsequence, but the successful application of these methods has been limited to certain NLP tasks [e.g.,\n17–19]. Unlike sparse attention, this family of methods allows each token to attend to the entire\ninput sequence. However, due to the loss of high-ﬁdelity token-wise information, their performance\nsometimes is not as good as full attention or sparse attention on tasks that require ﬁne-grained local\ninformation, including standard benchmarks in language [20] and vision [21].\nDespite the rapid progress in efﬁcient Transformers, some proposed architectures can only be applied\nto bidirectional models [e.g., 15, 16, 18]. Transformer-based autoregressive models have achieved\ngreat successes in language modeling [22], image synthesis [23], and text-to-image synthesis [24],\nwhich also involve long texts or high-resolution images. It is desirable to design an efﬁcient trans-\nformer that can be applied to both autoregressive and bidirectional models.\nIn this work, we unify a local window attention and a novel long-range attention into a single\nefﬁcient attention mechanism. We show that these two kinds of attention have complementary effects\nthat together yield the state-of-the-art results on a range of tasks in language and vision, for both\nautoregressive and bidirectional models. Speciﬁcally, we make the following contributions:\n• We propose Long-Short Transformer (Transformer-LS), an efﬁcient Transformer that integrates a\ndynamic projection based attention to model long-range correlations, and a local window attention\nto capture ﬁne-grained correlations. Transformer-LS can be applied to both autoregressive and\nbidirectional models with linear time and memory complexity.\n• We compute a dynamic low-rank projection, which depends on the content of the input sequence.\nIn contrast to previous low-rank projection methods, our dynamic projection method is more\nﬂexible and robust to semantic-preserving positional variations (e.g., insertion, paraphrasing).\nWe demonstrate that it outperforms previous low-rank methods [17, 18] on Long Range Arena\nbenchmark [20].\n• We identify a scale mismatch problem between the embeddings from the long-range and short-\nterm attentions, and design a simple but effective dual normalization strategy, termedDualLN, to\naccount for the mismatch and enhance the effectiveness of the aggregation.\n• We demonstrate that Long-Short Transformer, despite its low memory and runtime complexity,\noutperforms the state-of-the-art models on a set of tasks from Long Range Arena, and autore-\ngressive language modeling on enwik8 and text8. In addition, the proposed efﬁcient attention\nmechanism can be easily applied to the most recent vision transformer architectures [6, 11] and\nprovides state-of-the-art results, while being more scalable to high-resolution images. We also\ninvestigate the robustness properties of the Transformer-LS on diverse ImageNet datasets.\n2 Related Work\n2.1 Efﬁcient Transformers\nIn recent years, many methods have been introduced for dealing with the quadratic cost of full atten-\ntion. In general, they can be categorized as follows: i) Sparse attention mechanism with predeﬁned\npatterns (e.g., sliding window), including Sparse Transformer [12], Image Transformer [13], Axial\nTransformer [25] for modeling images, and Longformer [14], blockwise self-attention [26], ETC [15],\nBig Bird [16] for modeling language. ii) Low-rank projection attention, including Linformer [17],\nNyströmformer [18], Synthesizer [ 19]. For example, Linformer uses linear layers to project the\noriginal high resolution keys (K) and values (V ) with length n to low resolution with size r (r ≪n)\nand allows all query tokens ( Q) to attend these compressed representations. iii) Memory-based\nmechanisms like Compressive Transformer [10] and Set Transformer [27], which use extra mem-\nories for caching global long-range information for use in computing attention between distant\ntokens. iv) Kernel-based approximation of the attention matrix, including Performer [ 28], Linear\n2\nK(V)\n×\nWP\nSoftmax\nDynamic Projection\nProjection Matrix\nK(V)\n×\nWK(WV)\n× =\n¯K( ¯V)\nProjected key (value)\nTranspose\nr\nd\nRepeat\nLong-term Attention\nLNG\nLNL\nAggregation\nQ\n˜K( ˜V)\nd\nr\nShort-term Attention\nQ\n˜K( ˜V)pad pad\nw\nd\nn n timesn\nConcatenate\nr2w\nd\nn\nShort-term\nLong-term\nFigure 1: Long-short term attention of a single attention head. Here, the sequence length n = 8, hidden\ndimension d = 3, local window segment size w = 2, and rank of dynamic projection r = 3. Within the ﬁgure,\nK(V ) denotes key K or value V . In the left ﬁgure, we virtually replicate K or V ∈ Rn×d into n rows, and\nhighlight the keys and values within the attention span (denoted as ˜K( ˜V )) of all n queries Q for the short-term\nattention. In the middle ﬁgure, all queries attend to the same projected keys ¯K and values ¯V within the long-term\nattention. In the right ﬁgure, ˜K( ˜V ) and ¯K( ¯V ) are ﬁrst normalized with two sets of LayerNorms, and the queries\nattend to normalized ˜K( ˜V ) and ¯K( ¯V ) within their attention span simultaneously.\nTransformer [29], and Random Feature Attention [30]. vi) Similarity and clustering based methods,\nincluding Reformer [31], Routing Transformer [32], and Sinkhorn Transformer [33].\nOur method seamlessly integrates both low-rank projection and local window attentions, to leverage\ntheir strengths for modeling long-range and short-term correlations. In particular, our long-range\nattention uses a dynamic low-rank projection to encode the input sequence, and outperforms the\nprevious low-rank projection method used by the Linformer [17]. In the similar vein, a few other\nmethods also try to combine the strengths of different methods. For example, Longformer [14] and\nETC [15] augment local window attention with task motivated global tokens. Such global tokens may\nnot be applicable for some tasks (e.g., autoregressive modelling). BigBird [16] further combines local\nwindow and global token attention with random sparse attention. It is not applicable in autoregressive\ntasks because the global token and random sparse pattern are introduced. To compress the model\nfootprint on edge devices, Lite Transformer [34] combines convolution and self-attention, but it still\nhas quadratic complexity for long sequences.\n2.2 Vision Transformers\nVision Transformer (ViT) [ 4] splits images as small patches and treats the patches as the input\nword tokens. It uses a standard transformer for image classiﬁcation and has shown to outperform\nconvolutional neural networks (e.g., ResNet [ 35]) with sufﬁcient training data. DeiT [ 36] has\napplied the teacher-student strategy to alleviate the data efﬁciency problem of ViT and has shown\nstrong comparable performance using only the standard ImageNet dataset [37]. Instead of applying\ntransformer at a single low resolution of patches (e.g., 16 ×16 patches), very recent works, including\nPyramid Vision Transformer (PVT) [5], Swin-Transformer [38], T2T-ViT [39], Vision Longformer\n(ViL) [11] and Convolutional Vision Transformer (CvT) [ 6], stack a pyramid of ViTs to form a\nmulti-scale architecture and model long sequences of image patches at much higher resolution (e.g.,\n56 ×56 = 3136 patches for images with 224 ×224 pixels). Most of these methods have quadratic\ncomplexity of self-attention with respect to the input image size.\nTo reduce the complexity, Swin-Transformer [38] achieves linear complexity by limiting the computa-\ntion of self-attention only within each local window. HaloNet [40] applies local attention on blocked\nimages and only has quadratic complexity with respect to the size of the block. Perceiver [41] uses\ncross-attention between data and latent arrays to replace the self-attention on data to remove the\nquadratic complexity bottleneck. Vision Longformer (ViL) [11], another concurrent work, achieves\nlinear complexity by adapting Longformer [14] to Vision. ViL augments local window attention with\ntask-speciﬁc global tokens, but the global tokens are not applicable for decoding task (e.g., image\nsynthesis [23, 24]). In contrast, our method reduces the quadratic cost to linear cost by combining\nlocal window attention with global dynamic projection attention, which can be applied to both\nencoding and decoding tasks.\n3 Long-Short Transformer\nTransformer-LS approximates the full attention by aggregating long-range and short-term attentions,\nwhile maintaining its ability to capture correlations between all input tokens. In this section, we ﬁrst\n3\nintroduce the preliminaries of multi-head attention in Transformer. Then, we present the short-term\nattention via sliding window, and long-range attention via dynamic projection, respectively. After\nthat, we propose the aggregating method and dual normalization (DualLN) strategy. See Figure 1 for\nan illustration of our long-short term attention.\n3.1 Preliminaries and Notations\nMulti-head attention is a core component of the Transformer [1], which computes contextual represen-\ntations for each token by attending to the whole input sequence at different representation subspaces.\nIt is deﬁned as\nMultiHead(Q, K, V) = Concat(H1, H2, ..., Hh)WO, (1)\nwhere Q, K, V∈Rn×d are the query, key and value embeddings, WO ∈Rd×d is the projection\nmatrix for output, the i-th head Hi ∈Rn×dk is the scaled dot-product attention, and dk = d/h is the\nembedding dimension of each head,\nHi = Attention(QWQ\ni , KWK\ni , V WV\ni ) = softmax\n[\nQWQ\ni\n(\nKW K\ni\n)⊺\n√dk\n]\nV WV\ni = AiV WV\ni , (2)\nwhere WQ\ni , WK\ni , WV\ni ∈Rd×dk are learned projection matrices, and Ai ∈Rn×n denotes the full\nattention matrix for each attention head. The complexity of computing and storing Ai is O(n2),\nwhich can be prohibitive when n is large. For simplicity, our discussion below is based on the case of\n1D input sequences. It is straightforward to extend to the 2D image data given a predetermined order.\n3.2 Short-term Attention via Segment-wise Sliding Window\nWe use the simple yet effective sliding window attention to capture ﬁne-grained local correlations,\nwhere each query attends to nearby tokens within a ﬁxed-size neighborhood. Similar techniques have\nalso been adopted in [14, 16, 11]. Speciﬁcally, we divide the input sequence into disjoint segments\nwith length w for efﬁciency reason. All tokens within a segment attend to all tokens within its\nhome segment, as well as w/2 consecutive tokens on the left and right side of its home segment\n(zero-padding when necessary), resulting in an attention span over a total of 2w key-value pairs. See\nFigure 5 in Appendix for an illustration. For each query Qt at the position t within the i-th head,\nwe denote the 2w key-value pairs within its window as ˜Kt, ˜Vt ∈R2w×d. For implementation with\nPyTorch, this segment-wise sliding window attention is faster than the per-token sliding window\nattention where each token attends to itself and w tokens to its left and right, and its memory\nconsumption scales linearly with sequence length; see [14] and our Figure 3 for more details.\nThe sliding window attention can be augmented to capture long-range correlations in part, by\nintroducing different dilations to different heads of sliding window attention [ 14]. However, the\ndilation conﬁgurations for different heads need further tuning and an efﬁcient implementation of\nmulti-head attention with different dilations is non-trivial. A more efﬁcient alternative is to augment\nsliding window attention with random sparse attention [ 16], but this does not guarantee that the\nlong-range correlations are captured in each layer as in full attention. In the following section, we\npropose our long-range attention to address this issue.\n3.3 Long-range Attention via Dynamic Projections\nPrevious works have shown that the self-attention matrix can be well approximated by the product of\nlow-rank matrices [17]. By replacing the full attention with the product of low-rank matrices [42, 19,\n18, 43, 28], each query is able to attend to all tokens. Linformer [17] is one of the most representative\nmodels in this category. It learns a ﬁxed projection matrix to reduce the length of the keys and values,\nbut the ﬁxed projection is inﬂexible to semantic-preserving positional variations.\nStarting from these observations, we parameterize the dynamic low-rank projection at i-th head as\nPi = f(K) ∈Rn×r, where r ≪n is the low rank size and Pi depends on all the keys K ∈Rn×d of\ninput sequence. It projects the (n ×dk)-dimensional key embeddings KW K\ni and value embeddings\nV WV\ni into shorter, (r ×dk)-dimensional key ¯Ki and value ¯Vi embeddings. Unlike Linformer [17],\nthe low-rank projection matrix is dynamic, which depends on the input sequence and is intended to be\nmore ﬂexible and robust to, e.g., insertion, deletion, paraphrasing, and other operations that change\nsequence length. See Table 2 for examples. Note that, the query embeddingsQWQ\ni ∈Rn×dk are kept\n4\nat the same length, and we let each query attend to ¯Ki and ¯Vi. In this way, the full (n ×n) attention\nmatrix can be decomposed into the product of two matrices with r columns or rows. Speciﬁcally, we\ndeﬁne the dynamic projection matrix Pi ∈Rn×r and the key-value embeddings ¯Ki, ¯Vi ∈Rr×dk of\nlow-rank attention as\nPi = softmax(KW P\ni ), ¯Ki = P⊺\ni KW K\ni , ¯Vi = P⊺\ni V WV\ni , (3)\nwhere WP\ni ∈Rd×r are learnable parameters,2 and the softmax normalizes the projection weights\non the ﬁrst dimension over all n tokens, which stabilizes training in our experiments. Note that\nK = V in all the experiments we have considered, so Pi remains the same if it depends on V . The\ncomputational complexity of Eq. 3 is O(rn).\nTo see how the full attention is replaced by the product of low-rank matrices, we compute each head\nHi ∈Rn×dk of long-range attention as,\n¯Hi = softmax\n[\nQWQ\ni ¯K⊺\ni√dk\n]\n  \n¯Ai\n¯Vi = ¯Ai\n(\nP⊺\ni V WV\ni\n)\n, (4)\nso the full attention is now replaced with the implicit product of two low-rank matrices ¯Ai ∈Rn×r\nand P⊺\ni ∈Rr×n, and the computational complexity is reduced to O(rn). Note the effective attention\nweights of a query on all tokens still sum to 1. Our global attention allows each query to attend\nto all token embeddings within the same self-attention layer. In contrast, the sparse attention\nmechanisms [14, 16] need stack multiple layers to build such correlations.\nApplication to Autoregressive Models: In autoregressive models, each token can only attend to\nthe previous tokens, so the long-range attention should have a different range for different tokens. A\nstraightforward way to implement our global attention is to update ¯Ki, ¯Vi for each query recurrently,\nbut this requires re-computing the projection in Eq. (3) for every token due to the nonlinearity of\nsoftmax, which results in O(rn2) computational complexity. To preserve the linear complexity, for\nautoregressive models, we ﬁrst divide the input sequence into equal-length segments with length l,\nand apply our dynamic projection to extract ¯Ki, ¯Vi from each segment. Each token can only attend to\n¯Ki, ¯Vi of segments that do not contain its future tokens. Formally, let Qt be the query at position t,\nK(l−1)s:ls, V(l−1)s:ls be the key-value pairs from thes-th segment, and st = ⌊t/l⌋. For autoregressive\nmodels, we compute the long-range attention of Qt by attending to Ki,t, Vi,t, deﬁned as\n¯Ki,t = [P⊺\ni,1K1:l; ...; P⊺\ni,st K(l−1)st:lst ]WK\ni , ¯Vi,t = [P⊺\ni,1V1:l; ...; P⊺\ni,st V(l−1)st:lst ]WV\ni . (5)\nIn this way, the dynamic low-rank projection is applied to each segment only once in parallel,\npreserving the linear complexity and the high training speed. By comparison, Random Feature\nAttention [30] is slow at training due to the requirement for recurrence.\n3.4 Aggregating Long-range and Short-term Attentions\nTo aggregate the local and long-range attentions, instead of adopting different attention mechanisms\nfor different heads [ 12, 14, 34], we let each query at i-th head attend to the union of keys and\nvalues from the local window and global low-rank projections, thus it can learn to select important\ninformation from either of them. We ﬁnd this aggregation strategy works better than separating the\nheads in our initial trials with the autoregressive language models. Speciﬁcally, for the i-th head,\nwe denote the global low-rank projected keys and values as ¯Ki, ¯Vi ∈Rr×dk , and the local keys and\nvalues as ˜Kt, ˜Vt ∈R2w×d within the local window of position t for the query Qt. Then the i-th\nattention Hi,t at position t is\nHi,t = softmax\n\n\nQtWQ\ni\n[\n˜KtWK\ni ; ¯Ki\n]⊺\n√dk\n\n[ ˜VtWV\ni ; ¯Vi]. (6)\nwhere [·; ·] denotes concatenating the matrices along the ﬁrst dimension. Furthermore, we ﬁnd a scale\nmismatch between the initial norms of ˜KtWK\ni and ¯Ki, which biases the attention to the local window\nat initialization for both language and vision tasks. We introduce a normalization strategy (DualLN)\nto align the norms and improve the effectiveness of the aggregation in the following.\n2For the CvT-based vision transformer model, we replace WP\ni with a depth-wise separable convolution, just\nas its query, key and value projections.\n5\n1 2 3 4 5 6 7 8 9 10 11 12\nLayer Index\n1.0\n1.5\n2.0\n2.5Norm Ratios\nCompare Local to Global Key/Value Norm Ratios\nw/o DualLN (enwik8)\nw/o DualLN (text8)\nw/o DualLN (ImageNet)\nw/ DualLN\nw/o DualLN (enwik8)\nw/o DualLN (text8)\nw/o DualLN (ImageNet)\nw/ DualLN\n0k 100k 200k 300k 400k\nTraining Iterations\n1.05\n1.10\n1.15\n1.20\n1.25Validation Loss\nCompare Validation Loss\nw/o DualLN (enwik8)\nw/ DualLN (enwik8)\nw/o DualLN (text8)\nw/ DualLN (text8)\nFigure 2: Left: Ratios of the average ℓ2 norms of the local window to global low-rank key/value embeddings\nat initialization. Without DualLN, the sparse and low-rank embeddings have a magnitude mismatch. With\nDualLN, the ratios will be 1.0 at every layer, which will facilitate optimization. Right: The validation loss of\nTransformer-LS with and without DualLN on enwik8 and text8.\nDualLN: For Transformers with Layer Normalization (LN) (see [44] for an illustration), the Ki, Vi\nembeddings are the outputs of LN layers, so they have zero mean and unit variance at initialization.\nThe ℓ2 norm of vectors with zero-mean entries is proportional to their variance in expectation. We\nnote a weighted average will reduce the variance and therefore the norm of such zero-mean vectors.\nAs a result, the embedding vectors from low-rank attention in the weighted average ¯Ki, ¯Vi of Eq. (3)\nwill have smaller norms than the regular key and value embeddings from sliding window attention (see\nFigure 2 Left for an illustration). This scale mismatch causes two side effects. First, the inner product\nQtWQ\ni ¯K⊺\ni from local-rank component tends to have smaller magnitude than the local window one,\nthus the attention scores on long-range attention is systematically smaller. Second, the key-value\npairs ¯Ki, ¯Vi for the low-rank attention will naturally have less impact on the direction of Hi even\nwhen low-rank and local window are assigned with same attention scores, since ¯Vi has smaller norms.\nBoth effects lead to small gradients on the low-rank components and hinders the model from learning\nto effectively use the long-range correlations.\nTo avoid such issues, we add two sets of Layer Normalizations after the key and value projections for\nthe local window and global low-rank attentions, so that their scales are aligned at initialization, but\nthe network can still learn to re-weight the norms after training. Speciﬁcally, the aggregated attention\nis now computed as\nHi,t = softmax\n\n\nQtWQ\ni\n[\nLNL( ˜KtWK\ni ); LNG( ¯Ki)\n]⊺\n√dk\n\n[LNL( ˜VtWV\ni ); LNG( ¯Vi)], (7)\nwhere LNL(·), LNG(·) denote the Layer Normalizations for the local and global attentions respec-\ntively. In practice, to maintain the consistency between the local attention and dynamic projection,\nwe use LNL(K), LNL(V ) instead of K, Vto compute ¯Ki, ¯Vi in Eq. 3. As illustrated in Figure 2\nRight, the Transformer-LS models trained with DualLN has consistently lower validation loss than\nthe models without DualLN.\n4 Experiments\nIn this section, we demonstrate the effectiveness and efﬁciency of our method in both language and\nvision domains. We use PyTorch for implementation and count the FLOPs using fvcore [45].\n4.1 Bidirectional Modeling on Long Range Arena and IMDb\nTo evaluate Long-Short Transformer as a bidirectional encoder for long text, we train our models\non the three NLP tasks, ListOps, Text, and Retrieval, from the recently proposed Long Range\nArena (LRA) benchmark [20], following the setting of Peng et al. [30] and Tay et al. [46]. For fair\ncomparisons, we use the PyTorch implementation and the same data preprocessing/split, training\nhyperparameters and model size from [18], except for Retrieval where we accidentally used more\nwarmup steps and improved the results for all models. See Appendix B for more details. The results\n6\nTable 1: Accuracy (%) and FLOPs (G) on Long Range Arena (LRA), with the model conﬁgs annotated (see\nTable 7 for more). All results are averages of 4 runs with different random seeds.\nTask ListOps Text Retrieval Average\n(mean±std.) of sequence length (888 ±339) (1296 ±893) (3987 ±560)\nModel Acc. FLOPs Acc. FLOPs Acc. FLOPs Acc.\nFull Attention [1] 37.13 1.21 65.35 4.57 82.30 9.14 61.59\nReformer [31] (2) 36.44 0.27 64.88 0.58 78.64 1.15 59.99\nLinformer [17] (k=256) 37.38 0.41 56.12 0.81 79.37 1.62 57.62\nPerformer [28] (r = 256) 32.78 0.41 65.21 0.82 81.70 1.63 59.90\nNyströmformer [18] (l = 128) 37.34 0.61 65.75 1.02 81.29 2.03 61.46\nTransformer-LS (w, r= 8,32) 37.50 0.20 66.01 0.40 81.79 0.80 61.77\nDynamic Projection (best) 37.79 0.15 66.28 0.69 81.86 2.17 61.98\nTransformer-LS (best) 38.36 0.16 68.40 0.29 81.95 2.17 62.90\nTable 2: Comparing the robustness of the models under test-time insertions and deletions. DP refers\nto long-range attention via Dynamic Projection, and Win. refers to sliding window attention.\nTask Text Retrieval\nTest Perturb None Insertion Deletion None Insertion Deletion\nLinformer 56.12 55.94 54.91 79.37 53.66 51.75\nDP 66.28 63.16 58.95 81.86 70.01 64.98\nLinformer + Win. 59.63 56.69 56.29 79.68 52.83 52.13\nDP + Win. (ours) 68.40 66.34 62.62 81.95 69.93 64.19\nTable 3: Comparing the results of pretrained language models ﬁne-tuned on IMDb.\nModel RoBERTa-base RoBERTa-large Longformer-base LS-base LS-large\nAccuracy 95.3 96.5 95.7 96.0 96.8\non these three tasks are given in Table 1. Results of the other two image-based tasks of LRA, as well\nas models implemented in JAX, are given in Appendix C and C.2.\nIn addition, we follow the pretraining procedure of Longformer [14] to pretrain our models based\non RoBERTa-base and RoBERTa-large [47], and ﬁne-tune it on the IMDb sentiment classiﬁcation\ndataset. The results are given in Table 3.\nResults. From Table 3, our base model outperforms Longformer-base, and our large model achieves\nimprovements over RoBERTa-large, demonstrating the beneﬁts of learning to model long sequences.\nComparisons with models on LRA are given in Table 1. Transformer-LS (best) with the best\nconﬁgurations of w, rfor each task are given in Table 7 in Appendix B. We also report the results\nof using ﬁxed hyperparameter w = 8, r= 32 on all tasks. Overall, our Transformer-LS (best) is\nsigniﬁcantly better than other efﬁcient Transformers, and the model with w, r= 8, 32 performs\nfavorably while using only about 50% to 70% computation compared to other efﬁcient Transformers\non all three tasks. The advantage of aggregating local and long-range attentions is the most signiﬁcant\non ListOps, which requires the model to understand the tree structures involving both long-term\nand short-term relations. On Retrieval, where document-level encoding capability is tested, we ﬁnd\nour global attention more effective than window attention. The test accuracy of using only dynamic\nprojection is about 10% higher than Linformer on Text (i.e., 66.28 vs. 56.12), which has the highest\nvariance in sequence length (i.e. standard deviation 893). This demonstrates the improved ﬂexibility\nof dynamic projection at learning representations for data with high variance in sequence length,\ncompared to the learned but ﬁxed projection of Linformer. Similarly, Linformer, Nyströmformer and\nour model outperform full attention on ListOps, indicating they may have better inductive bias, and\nefﬁcient Transformers can have better efﬁcacy beyond efﬁciency.\nRobustness of Dynamic Projection. In Table 2, we compare the robustness of Linformer and the\nproposed Dynamic Projection (DP) against insertion and deletion on Text and Retrieval tasks of LRA.\nWe train the models on the original, clean training sets and only perturb their test sets. For insertion,\nwe insert 10 random punctuations at 10 random locations of each test sample. For deletion, we delete\n7\n0 2000 4000 6000 8000 10000 12000 14000\nSequence Length\n0\n5\n10\n15\n20\n25GPU Memory (GB)\nForward-backprop Peak GPU Memory (Autoregressive LM)\nBatch=1\nBatch=2\nBatch=4\nTransformer-XL\nTransformer-LS\nBatch=1\nBatch=2\nBatch=4\nTransformer-XL\nTransformer-LS\n0 2000 4000 6000 8000 10000 12000 14000\nSequence Length\n0.0\n0.5\n1.0\n1.5\n2.0Time/Iteration (s)\nForward-backprop Time per Iteration (Autoregressive LM)\nBatch=1\nBatch=2\nBatch=4\nTransformer-XL\nTransformer-LS\nBatch=1\nBatch=2\nBatch=4\nTransformer-XL\nTransformer-LS\nFigure 3: Running time and memory consumption of Transformer-XL (full attention) and our Transformer-\nLS on Char-LM. We increase the sequence length until we use up the 32GB of memory on a V100 GPU.\nTransformer-LS is the same smaller model in Table 4. We use dashed lines to represent the full attention\nTransformer and solid lines to represent our model. We use different colors to represent different batch sizes.\nTable 4: BPC (↓) of smaller models on enwik8 and text8 (left), and larger models on enwik8 (right).\nMethod #Param text8 enwik8\nDev Test Dev Test\nT12 [49] 44M - 1.18 - 1.11\nTransformer-XL [9] 41M - - - 1.06\nReformer [31] - - - - 1.05\nAdaptive [50] 38M 1.05 1.11 1.04 1.02\nBP-Transformer [51] 38M - 1.11 - 1.02\nLongformer [20] 41M 1.04 1.10 1.02 1.00\nTransformer-LS 44M 1.03 1.09 1.01 0.99\nMethod #Param Test BPC\nTransformer-XL [9] 88M 1.03\nTransformer-XL [9] 277M 0.99\nRouting [32] 223M 0.99\nLongformer [14] 102M 0.99\nSparse [12] 95M 0.99\nAdaptive [50] 209M 0.98\nCompressive [10] 227M 0.97\nTransformer-LS 110M 0.97\nall punctuations from the test samples. Both transforms are label-preserving in most cases. By design,\ndynamic projection is more robust against location changes.\n4.2 Autoregressive Language Modeling\nWe compare our method with other efﬁcient transformers on the character-level language modeling\nwhere each input token is a character.\nSetup. We train and evaluate our model on enwik8 and text8, each with 100M characters and\nare divided into 90M, 5M, 5M for train, dev, test, following [48]. Our smaller 12-layer and larger\n30-layer models are Pre-LN Transformers with the same width and depth as Longformer [20], except\nthat we add relative position encoding to the projected segments in each layer. We adopt the cache\nmechanism of Transformer-XL [9], setting the cache size to be the same as the input sequence length.\nWe follow similar training schedule as Longformer, and train our model in 3 phases with increasing\nsequence lengths. The input sequence lengths are 2048, 4096 and 8192 respectively for the 3 phases.\nBy comparison, Longformer trains their model in 5 phases on GPUs with 48GB memory (The\nmaximal of ours is 32GB) where the sequence length is 23,040 in the last phase. The window size of\nLongformer increases with depth and its average window size is 4352 in phase 5, while our effective\nnumber of attended tokens is 1280 on average in the last phase. Each experiment takes around 8 days\nto ﬁnish on 8 V100 GPUs. Detailed hyperparameters are shown in Appendix D. For testing, same as\nLongformer, we split the dataset into overlapping sequences of length 32K at a step size of 512, and\nevaluate the BPCs for predicting the next 512 tokens given the previous 32K characters.\nResults Table 4 shows comparisons on text8 and enwik8. Our method has achieved state-of-the-art\nresults. On text8, we achieve a test BPC of 1.09 with the smaller model. On enwik8, our smaller\nmodel achieves a test BPC of 0.99, and outperforms the state-of-the-art models with comparable\nnumber of parameters. Our larger model obtains a test BPC of 0.97, on par with the Compressive\nTransformer with 2×parameters. Our results are consistently better than Longformer which is trained\non longer sequences with 5 stages and 48 GPU memory. In Figure 3, we show our model is much\nmore memory and computational efﬁcient than full attention.\n8\nTable 5: Test accuracies on ImageNet, ImageNet Real [ 52], and ImageNet V2 [ 53] of models trained on\nImageNet-1K. Grey-colored rows are our results. CvT∗-LS denotes our long-short term attention based on\nthe non-ofﬁcial CvT implementation. ViL models with LS sufﬁxes are our long-short term attention based on\nthe ofﬁcial ViL implementation with relative positional bias. We also provide the latency of models tested using\nbatch size 32 on the same V100 GPU. Our improvements over ViL is mainly from a better implementation of\nthe short-term attention.\nModel #Param Image FLOPs ImageNet Real V2 Latency\n(M) Size (G) top-1 (%) top-1 (%) top-1 (%) (s)\nResNet-50 25 224 2 4.1 76.2 82.5 63.3 -\nResNet-101 45 224 2 7.9 77.4 83.7 65.7 -\nResNet-152 60 224 2 11 78.3 84.1 67.0 -\nDeiT-S [36] 22 224 2 4.6 79.8 85.7 68.5 -\nDeiT-B [36] 86 224 2 17.6 81.8 86.7 70.9 -\nPVT-Medium [5] 44 224 2 6.7 81.2 - - -\nPVT-Large [5] 61 224 2 9.8 81.7 - - -\nSwin-S [38] 50 224 2 8.7 83.2 - - -\nSwin-B [38] 88 224 2 15.4 83.5 - - 0.115\nPVTv2-B4 [54] 62.6 224 2 10.1 83.6 - - -\nPVTv2-B5 [54] 82.0 224 2 11.8 83.8 - - -\nViT-B/16 [4] 86 384 2 55.5 77.9 - - -\nViT-L/16 [4] 307 384 2 191.1 76.5 - - -\nDeiT-B [36] 86 384 2 55.5 83.1 - - -\nSwin-B [38] 88 384 2 47.1 84.5 - - 0.378\nCvT-13 [6] 20 224 2 6.7 81.6 86.7 70.4 0.122\nCvT-21 [6] 32 224 2 10.1 82.5 87.2 71.3 0.165\nCvT∗-LS-13 20.3 2242 4.9 81.9 87.0 70.5 0.083\nCvT∗-LS-17 23.7 2242 9.8 82.5 87.2 71.6 -\nCvT∗-LS-21 32.1 2242 7.9 82.7 87.5 71.9 0.122\nCvT∗-LS-21S 30.1 2242 11.3 82.9 87.4 71.7 -\nCvT-13 [6] 20 384 2 31.9 83.0 87.9 71.9 -\nCvT-21 [6] 32 384 2 45.0 83.3 87.7 71.9 -\nCvT∗-LS-21 32.1 3842 23.9 83.2 88.0 72.5 -\nCvT∗-LS-21 32.1 4482 34.2 83.6 88.2 72.9 -\nViL-Small [14] 24.6 224 2 4.9 82.4 - - -\nViL-Medium [14] 39.7 224 2 8.7 83.5 - - 0.106\nViL-Base [14] 55.7 224 2 13.4 83.7 - - 0.164\nViL-LS-Medium 39.8 2242 8.7 83.8 - - 0.075\nViL-LS-Base 55.8 2242 13.4 84.1 - - 0.113\nViL-LS-Medium 39.9 3842 28.7 84.4 - - 0.271\n4.3 ImageNet Classiﬁcation\nWe train and evaluate the models on ImageNet-1K with 1.3M images and 1K classes. We use CvT [6]\nand ViL [11], state-of-the art vision transformer architectures, as the backbones and replace their\nattention mechanisms with our long-short term attention, denoted as CvT ∗-LS and ViL-size-LS\nin Table 5. CvT uses overlapping convolutions to extract dense patch embeddings from the input\nimages and feature maps, resulting in a long sequence length in the early stages (e.g.,56 ×56 = 3136\npatches for images with 2242 pixels). For ViL, our sliding window uses the same group size w, but\neach token attends to at most 2w ×2w (rounding when necessary) tokens inside the window, instead\nof 3w ×3w as ViL, which allows adding our dynamic projection without increasing the FLOPs. We\nset r = 8 for the dynamic projections for both ViL-LS-Medium and ViL-LS-Base. Note that, our\nefﬁcient attention mechanism does not depend on the particular architecture, and it can be applied to\nother vision transformers [e.g., 4, 36, 5]. Please refer to Appendix E for more details.\nClassiﬁcation Results. The results are shown in the Table 5, where we also list test accuracies on\nImageNet Real and ImageNet V2. Except for CvT, we compare with the original ViT [ 4] and the\nenhanced DeiT [36], PVT [5] that also uses multi-scale stragey, ViL [11] that uses window attention\nand global tokens to improve the efﬁciency. Training at high-resolution usually improves the test\n9\nTable 6: Robustness evaluation on various ImageNet datasets. Top-1/Acc.: Top-1 accuracy. mCE: Mean\nCorrupution Error. Mixed-same/Mixed-rand: accuracies on MIXED -SAME /MIXED -RAND subsets.\nModel ParamsImageNetIN-C [56]IN-A [57]IN-R [58] ImageNet-9 [59]\n(M) Top-1 mCE (↓) Acc. Acc. Mixed-same Mixed-rand\nResNet-50 [35]25.6 76.2 78.9 6.2 35.3 87.1 81.6\nDeiT-S [36] 22.1 79.8 57.1 19.0 41.9 89.1 84.2\nCvT-13 20 81.6 59.6 25.4 42.9 90.5 85.7\nCvT-21 32 82.5 56.2 31.1 42.6 90.5 85.0\nCvT∗-LS-13 20.3 81.9 58.7 27.0 42.6 90.7 85.6\nCvT∗-LS-21 32.1 82.7 55.2 29.3 45.0 91.5 85.8\naccuracy of vision transformer. With our long-short term attention, we can easily scale the training\nto higher resolution, and the performance of CvT∗-LS and ViL-LS also improves. Our best model\nwith CvT (CvT∗-LS-21 at 4482) achieves 0.3% higher accuracy than the best reported result of CvT\nwhile using the same amount of parameters and 76% of its FLOPs. In CvT architecture, the spatial\ndimension of feature maps in earlier stages are large, representing more ﬁne-grained details of the\nimage. Similar to training with high-resolution images, the model should also beneﬁt from denser\nfeature maps. With our efﬁcient long-short term attention, we can better utilize these ﬁne-grained\nfeature maps with less concerns about the computational budget. In this way, our CvT ∗-LS-17\nachieves better result than CvT-21 at resolution 224 using fewer parameters and FLOPs, and our\nCvT∗-LS-21S model further improves our CvT∗-LS-21 model.\nOur ViL-LS-Medium and ViL-LS-Base with long-short term attention improve the accuracies of\nViL-Medium and ViL-Base from 83.5 and 83.7 to 83.8 and 84.1 respectively, without an increase in\nFLOPs. When increasing the resolution for training ViL-LS-Medium from 2242 to 3842, the FLOPs\nincreased (approximately) linearly and the accuracy improved by 0.6%, showing our method still\nbeneﬁts greatly from increased resolution while maintaining the linear complexity in practice.\nShort-term Attention Suppresses Oversmoothing. By restricting tokens from different segments\nto attend to different windows, our short-term sparse local attention encourages diversity of the feature\nrepresentations and helps to alleviate the over-smoothing problem [ 55] (where all queries extract\nsimilar information in deeper layers and the attention mechanism is less important), thus can fully\nutilize the depth of the network. As in [55], we provide the cosine similarity of patch embeddings\nof our CvT∗-LS-13 and re-implemented CvT-13 (81.1 accuracy) in Figure 6 within Appendix. This\nis one of the reasons why our efﬁcient attention mechanism can get even better results than the full\nattention CvT model in the same setting.\nRobustness evaluation on Diverse ImageNet Datasets. As vision models have been widely used\nin safety-critical applications (e.g. autonomous driving), their robustness is vital. In addition to\nout-of-distribution robustness (ImageNet-Real and Imageet-v2), we further investigate the robustness\nof our vision transformer against common corruption (ImageNet-C), semantic shifts (ImageNet-R),\nBackground dependence (ImageNet-9) and natural adversarial examples (ImageNet-A). We compare\nour methods with standard classiﬁcation methods, including CNN-based model (ResNet [35]) and\nTransformer-based models (DeiT [36]) with similar numbers of parameters. As shown in Table 6, we\nobserve that our method signiﬁcantly outperforms the CNN-based method (ResNet-50). Compared\nto DeiT, our models also achieve favorable improvements. These results indicate that the design of\ndifferent attention mechanisms plays an important role for model robustness, which sheds new light\non the design of robust vision transformers. More details and results can be found in Appendix E.\n5 Conclusion\nIn this paper, we introduced Long-Short Transformer, an efﬁcient transformer for long sequence\nmodeling for both language and vision domain, including both bidirectional and autoregressive\nmodels. We design a novel global attention mechanism with linear computational and memory\ncomplexity in sequence length based on a dynamic projection. We identify the scale mismatch issue\nand propose the DualLN technique to eliminate the mismatch at initialization and more effectively\naggregate the local and global attentions. We demonstrate that our method obtains the state-of-the-art\nresults on the Long Range Arena, char-level language modeling and ImageNet classiﬁcation. We\nlook forward to extending our methods to more domains, including document QA, object detection\nand semantic segmentation on high-resolution images.\n10\nReferences\n[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NIPS, volume 30, 2017.\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. NAACL, 2019.\n[3] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\n[5] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and\nLing Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.\narXiv preprint arXiv:2102.12122, 2021.\n[6] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\nIntroducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021.\n[7] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for\nquestion answering research. TACL, 7:453–466, 2019.\n[8] Raghavendra Pappagari, Piotr Zelasko, Jesús Villalba, Yishay Carmiel, and Najim Dehak. Hierarchi-\ncal transformers for long document classiﬁcation. In 2019 IEEE Automatic Speech Recognition and\nUnderstanding Workshop (ASRU), pages 838–844. IEEE, 2019.\n[9] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond a ﬁxed-length context. In ACL, 2019.\n[10] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive Transformers\nfor long-range sequence modelling. In ICLR, 2020.\n[11] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao. Multi-\nscale vision longformer: A new vision transformer for high-resolution image encoding. arXiv preprint\narXiv:2103.15358, 2021.\n[12] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\n[13] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin\nTran. Image transformer. In ICML, pages 4055–4064, 2018.\n[14] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150, 2020.\n[15] Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang. Etc: Encoding long and structured inputs in transformers.\nIn EMNLP, pages 268–284, 2020.\n[16] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip\nPham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big Bird: Transformers for longer sequences. In\nNeurIPS, 2020.\n[17] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear\ncomplexity. arXiv preprint arXiv:2006.04768, 2020.\n[18] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas\nSingh. Nyströmformer: A nyström-based algorithm for approximating self-attention. AAAI, 2021.\n[19] Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer: Rethinking\nself-attention in transformer models. In ICML, 2021.\n[20] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,\nSebastian Ruder, and Donald Metzler. Long Range Arena: A benchmark for efﬁcient transformers. In\nICLR, 2021.\n11\n[21] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao. Multi-\nscale vision longformer: A new vision transformer for high-resolution image encoding. arXiv preprint\narXiv:2103.15358, 2021.\n[22] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165, 2020.\n[23] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-ﬁdelity images with vq-vae-2.\narXiv preprint arXiv:1906.00446, 2019.\n[24] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen, and\nIlya Sutskever. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021.\n[25] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidimensional\ntransformers. arXiv preprint arXiv:1912.12180, 2019.\n[26] Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-\nattention for long document understanding. arXiv preprint arXiv:1911.02972, 2019.\n[27] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer:\nA framework for attention-based permutation-invariant neural networks. In ICML, 2019.\n[28] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos,\nPeter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers.\nICLR, 2021.\n[29] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast\nautoregressive transformers with linear attention. In ICML, 2020.\n[30] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. Random\nfeature attention. ICLR, 2021.\n[31] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer. In ICLR, 2020.\n[32] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efﬁcient content-based sparse\nattention with routing transformers. TACL, 9:53–68, 2021.\n[33] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. InICML.\nPMLR, 2020.\n[34] Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han. Lite transformer with long-short range\nattention. arXiv preprint arXiv:2004.11886, 2020.\n[35] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn CVPR, 2016.\n[36] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé\nJégou. Training data-efﬁcient image transformers & distillation through attention. arXiv preprint\narXiv:2012.12877, 2020.\n[37] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In CVPR, 2009.\n[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030,\n2021.\n[39] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng,\nand Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv\npreprint arXiv:2101.11986, 2021.\n[40] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and Jonathon\nShlens. Scaling local self-attention for parameter efﬁcient visual backbones. arXiv preprint\narXiv:2103.12731, 2021.\n[41] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira.\nPerceiver: General perception with iterative attention. arXiv preprint arXiv:2103.03206, 2021.\n12\n[42] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast\nautoregressive transformers with linear attention. In ICML. PMLR, 2020.\n[43] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. Random\nfeature attention. ICLR, 2021.\n[44] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan\nLan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In ICML.\nPMLR, 2020.\n[45] fvcore: Flop counter for pytorch models. https://github.com/facebookresearch/fvcore/blob/\nmaster/docs/flop_count.md, 2021.\n[46] Yi Tay, Mostafa Dehghani, Vamsi Aribandi, Jai Gupta, Philip Pham, Zhen Qin, Dara Bahri, Da-Cheng\nJuan, and Donald Metzler. Omninet: Omnidirectional representations from transformers. ICML, 2021.\n[47] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692, 2019.\n[48] Matt Mahoney. Large text compression benchmark. URL http://mattmahoney.net/dc/textdata, 6, 2009.\n[49] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level language\nmodeling with deeper self-attention. In AAAI, volume 33, pages 3159–3166, 2019.\n[50] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span in\ntransformers. ACL, 2019.\n[51] Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and Zheng Zhang. Bp-transformer: Modelling long-range\ncontext via binary partitioning. arXiv preprint arXiv:1911.04070, 2019.\n[52] Lucas Beyer, Olivier J Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. Are we\ndone with imagenet? arXiv preprint arXiv:2006.07159, 2020.\n[53] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classiﬁers\ngeneralize to imagenet? In International Conference on Machine Learning, pages 5389–5400. PMLR,\n2019.\n[54] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and\nLing Shao. Pvtv2: Improved baselines with pyramid vision transformer. 2021.\n[55] Chengyue Gong, Dilin Wang, Meng Li, Vikas Chandra, and Qiang Liu. Improve vision transformers\ntraining by suppressing over-smoothing. arXiv preprint arXiv:2104.12753, 2021.\n[56] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions\nand perturbations. In International Conference on Learning Representations, 2019.\n[57] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial\nexamples. arXiv preprint arXiv:1907.07174, 2019.\n[58] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai,\nTyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of\nout-of-distribution generalization. arXiv preprint arXiv:2006.16241, 2020.\n[59] Kai Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. Noise or signal: The role of image\nbackgrounds in object recognition. ArXiv preprint arXiv:2006.09994, 2020.\n[60] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing\nhuman-level performance on imagenet classiﬁcation. In CVPR, 2015.\n[61] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural\nnetworks. In AISTATS, 2010.\n[62] Nikita Nangia and Samuel Bowman. Listops: A diagnostic dataset for latent tree learning. In NAACL:\nStudent Research Workshop, pages 92–99, 2018.\n[63] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts.\nLearning word vectors for sentiment analysis. In ACL, 2011.\n13\n[64] Dragomir R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. The acl anthology\nnetwork corpus. Language Resources and Evaluation, 47(4):919–944, 2013.\n[65] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint\narXiv:1608.03983, 2016.\n[66] François Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 1251–1258, 2017.\n[67] Lei Huang, Xianglong Liu, Yang Liu, Bo Lang, and Dacheng Tao. Centered weight normalization in\naccelerating training of deep neural networks. In Proceedings of the IEEE International Conference on\nComputer Vision, pages 2803–2811, 2017.\n[68] Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Micro-batch training with batch-channel\nnormalization and weight standardization. arXiv preprint arXiv:1903.10520, 2019.\n14\nLong-Short Transformer: Efﬁcient Transformers\nfor Language and Vision (Appendix)\nA Details of Norm Comparisons\nAs we have shown in Figure 2, the norms of the key-value embeddings from the long-term and\nshort-term attentions, ¯K, ¯V and ˜K, ˜V , are different at initialization, and the norms of ˜K, ˜V is always\nlarger than ¯K, ¯V on different networks and datasets we have evaluated. Here, we give an explanation.\nIntuitively, at initialization, following similar assumptions as [60, 61], the entries of K, Vshould\nhave zero mean. Since each entry of ¯K, ¯V is a weighted mean of K, V, they have smaller variance\nunless one of the weights is 1. Given that ¯K, ¯V are also zero-mean, the norm of their embedding\nvectors (their rows), which is proportional to the variance, is smaller. For the key-value embeddings\nfrom short-term attention, ˜K, ˜V are just a subset of K, V, so their embedding vectors should have the\nsame norm as rows of K, Vin expectation. Therefore, the norms of embedding vectors from ¯K, ¯V\nwill be smaller than those from ˜K, ˜V in expectation.\nB Details for Experiments on Long Range Arena\nThe tasks. We compare our method with the following three tasks:\n• ListOps. ListOps [62] is designed to measure the parsing ability of models through hierarchically\nstructured data. We follow the setting in [20] in which each instance contains 500-2000 tokens.\n• Text. This is a binary sentiment classiﬁcation task of predicting whether a movie review from\nIMDb is positive or negative [63]. Making correct predictions requires a model to reason with\ncompositional unsegmented char-level long sequences with a maximum length of 4k.\n• Retrieval. This task is based on the ACL Anthology Network dataset [64]. The model needs to\nclassify whether there is a common citation between a pair of papers, which evaluates the model’s\nability to encode long sequences for similarity-based matching. The max sequence length for\neach byte-level document is 4k and the model processes two documents in parallel each time.\nArchitecture. On all tasks, the models have 2 layers, with embedding dimension d = 64, head\nnumber h = 2, FFN hidden dimension 128, smaller than those from [20]. Same as [20], we add a CLS\ntoken as a global token and use its embedding in the last layer for classiﬁcation. We re-implement the\nmethods evaluated by Xiong et al. [18], and report the best results of our re-implementation and those\nreported by Xiong et al. [18]. For our method, the results we run a grid search on the window size w\nand the projected dimension r, and keep 2w + r ≤256 to make the complexity similar to the other\nmethods. The maximum sequence length for ListOps and Text are 2048 and 4096. For Retrieval,\nwe set the max sequence for each of the two documents to 4096.\nTable 7: Conﬁgurations of our method corresponding to the best results (Transformer-LS (best)) in\nTable 1.\nListOps (2k) Text (4k) Retrieval (4k)\nw r w r w r\nDynamic Projection 0 4 0 128 0 256\nTransformer-LS 16 2 1 1 1 254\nHyperparameters for Training. Our hyperparameters are the same as Nyströmformer [18] unless\notherwise speciﬁed. Speciﬁcally, we follow [18] and use Adam with a ﬁxed learning rate of 10−4\nwithout weight decay, batch size 32 for all tasks. The number of warmup training steps Tw and total\ntraining steps T are different due to the difference in numbers of training samples. For Retrieval, we\naccidentally found using Tw = 8000 rather than the default Tw = 800 of [18] improves the results\nfor all models we have evaluated. See Table 8 for the conﬁgurations of each task.\nError bars. We have already provided the average of 4 runs with different random seeds in Table 1.\nHere we also provide the standard deviations for these experiments in Table 9.\n15\nTable 8: Training Hyperparameters for LRA tasks.\nlr batch size Tw T\nListOps 10−4 32 1000 5000\nText 10−4 32 8000 20000\nRetrieval 10−4 32 8000 30000\nTable 9: Accuracy (%) and its standard deviation on Long Range Arena (LRA), with the model\nconﬁgurations and sequence length stats (under the dataset names) annotated. All results are averages\nof 4 runs with different random seeds. Note that, text has the largest variance of length (i.e., 893).\nListOps Text Retrieval Average\n(888±339) (1296 ±893) (3987 ±560)\nModel Acc. FLOPs Acc. FLOPs Acc. FLOPs Acc.\nFull Attention [1] 37.1 ±0.4 1.21 65.4±0.3 4.57 82.3±0.4 9.14 61.59\nReformer [31] (2) 36.4 ±0.4 0.27 64.9±0.4 0.58 78.6±0.7 1.15 59.99\nLinformer [17] (k=256) 37.4 ±0.4 0.41 56.1±1.5 0.81 79.4±0.9 1.62 57.62\nPerformer [28] (r = 256) 32.8 ±9.4 0.41 65.2±0.2 0.82 81.7±0.2 1.63 59.90\nNyströmformer [18] (l = 128) 37.3 ±0.2 0.61 65.8±0.2 1.02 81.3±0.3 2.03 61.46\nTransformer-LS (w, r= 8,32) 37.5±0.3 0.20 66.0±0.2 0.40 81.8±0.3 0.80 61.77\nDynamic Projection (best) 37.8 ±0.2 0.15 66.3±0.7 0.69 81.9±0.5 2.17 61.98\nTransformer-LS (best) 38.4±0.4 0.16 68.4±0.8 0.29 82.0±0.5 2.17 62.90\nC Additional Results on LRA\nC.1 Results on the image-based tasks of LRA\nWe give the results of our model on the image-based tasks, implemented in PyTorch, in Table 10.\nTable 10: Comparing our model (Transformer-LS) with other methods on the image-based tasks of LRA. For\nthe results of other models, we take their highest scores from [18] and [20].\nModel Transformer-LS Linformer Reformer Performer Sparse. Trans. Nystromformer Full Att.\nImage 45.05 38.56 43.29 42.77 44.24 41.58 42.44\nPathﬁnder 76.48 76.34 69.36 77.05 71.71 70.94 74.16\nC.2 Compare models implemented in JAX\nTo compare the results with the implementations from the original LRA paper [20], we re-implement\nour method in JAX and give the comparisons with other methods in Table 11. The accuracies of other\nmethods come from the LRA paper. We evaluate the per-batch latency of all models on A100 GPUs\nusing their ofﬁcial JAX implementation from the LRA paper. Our method still achieves improvements\nwhile being efﬁcient enough. We were unable to run Reformer with the latest JAX since JAX has\ndeleted jax.custom_transforms, which is required by the Reformer implementation, from its\nAPI.3 Note the relative speedups from the LRA paper are evaluated on TPUs.\nD Details for Autoregressive Language Modeling\nAn example of long-short term attention for autoregressive models. We give an illustration for\nthe segment-wise dynamic projection for autoregressive models as discussed in Section 3.3. With the\nsegment-wise formulation, we can ﬁrst compute the low-rank projection for each segment in parallel,\nand each query will only attend to the tokens from segments that do not contain the future token\nor the query token itself. The whole process is efﬁcient and maintain the O(n) complexity, unlike\nRFA [30] which causes a slow-down in training due to the requirement for cumulative sum. However,\n3https://github.com/google/jax/pull/2026\n16\nTable 11: Comparing the test scores and latency of models on LRA, implemented in JAX.\nModel ListOps Text Retrieval\nAcc. Latency (s) Acc. Latency (s) Acc. Latency (s)\nLocal Att 15.82 0.151 52.98 0.037 53.39 0.142\nLinear Trans. 16.13 0.156 65.9 0.037 53.09 0.142\nReformer 37.27 - 56.10 - 53.40 -\nSparse Trans. 17.07 0.447 63.58 0.069 59.59 0.273\nSinkhorn Trans. 33.67 0.618 61.20 0.048 53.83 0.241\nLinformer 35.70 0.135 53.94 0.031 52.27 0.117\nPerformer 18.01 0.138 65.40 0.031 53.82 0.120\nSynthesizer 36.99 0.251 61.68 0.077 54.67 0.306\nLongformer 35.63 0.380 62.85 0.112 56.89 0.486\nTransformer 36.37 0.444 64.27 0.071 57.46 0.273\nBigBird 36.05 0.269 64.02 0.067 59.29 0.351\nTransformer-LS 37.65 0.187 76.64 0.037 66.67 0.201\nQ\nCache Input Sequence\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\nQ7\nQ8\ns1 s2 s3 s4\nAttention Span\n:   from sliding windowK(V)\nQ\nCache Input Sequence\nAttention Span\n: segment in use for the query Qt\ns1 s2 s3 s4\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\nQ7\nQ8\nFigure 4: An illustration of effective attention span (colored regions) in Transformer-LS when the\nsegment size for the low-rank attention isℓ = 4, and the segment size for the sliding window attention\nis w = 2. Left: the attention span of only the low-rank attention (segment-wise dynamic projection).\nRight: the attention span of the aggregated attention.\nin this way, some of the most recent tokens are ignored, as shown in Figure 4 (left). The window\nattention (with segment size w ≥l/2) becomes an indispensable component in this way, since it ﬁlls\nthe gap for the missing recent tokens, as shown in Figure 4.\nExperimental Setup. Throughout training, we set the window size w = 512, the segment length\nl = 16, and the dimension of the dynamic low-rank projectionr = 1, which in our initial experiments\nachieved better efﬁciency-BPC trade-off than using l = 32, r= 1 or l = 64, r= 4. Our small and\nlarge models have the same architecture as Longformer [14], except for the attention mechanisms.\nWe use similar training schedules as Longformer [14]. Speciﬁcally, for all models and both datasets,\nwe train the models for 430k/50k/50k steps with 10k/5k/5k linear learning rate warmup steps, and use\ninput sequence lengths 2048/4096/8192 for the 3 phases. We use constant learning rate after warmup.\nWe compared learning rates from {1.25e-4, 2.5e-4,5e-4,1e-3} for 100k iterations and found 2.5e-4 to\nwork the best for both models on enwik8, and 5e-4 to work the best on text8. The batch sizes for the\n3 phases are 32, 32, 16 respectively. Unlike Longformer and Transformer-XL, we remove gradient\nclipping and found the model to have slightly faster convergence in the beginning while converging\nreliably. For smaller models, we use dropout rate 0.2 and weight decay 0.01. For the larger model,\nwe use dropout 0.4 and weight decay 0.1.\n17\nQ\nK\nFor Bidirectional Models\nQ\nK\nFor Autoregressive Models\npad pad pad pad\nFigure 5: An illustration of our sliding window attention in 1D autoregressive and bidirectional\nmodels. Here, we use a group size w = 2. Each token inside each group are restricted to attend to at\nmost 2w tokens. In the bidirectional model, they attend to w tokens from the home segment, and\nw/2 tokens to the left and right of the home segment respectively. In the autoregressive model, they\nattend to w tokens to the left of the home segment, as well as all tokens within the home segment that\nis not a future token.\nE Details for ImageNet Classiﬁcation\nThe CvT Architecture. We implement the CvT model based on a public repository, 4 because this\nis a concurrent work with no ofﬁcial implementation when we conduct this work. In Table 5, since\nour CvT re-implementation gets worse test results than reported ones in their arxiv paper, we still list\nthe best test accuracy from Wu et al. [6] for fair comparisons. We report the FLOPs of CvT with our\nimplementation for reasonable comparisons, because our CvT∗-LS implementation is based on that.\nSame as CvT, all the models have three stages where the ﬁrst stage downsamples the image by a factor\nof 4 and each of the following stages downsamples the feature map by a factor of 2. CvT∗-LS-13 and\nCvT∗-LS-21 have the same conﬁguration as CvT-13 and CvT-21. CvT∗-LS-17 and CvT∗-LS-21 are\nour customized models with more layers and higher embedding dimensions in the ﬁrst two stages\n([3, 4, 10], [3, 4, 14] layers respectively and [128, 256, 768] dimensions). We train the model for 300\nepochs using a peak learning rate of 5e −4 with the cosine schedule [65] with 5 epochs of warmup.\nWe use the same set of data augmentations and regularizations as other works including PVT [ 5]\nand ViL [11]. In general, CvT∗-LS-13 and CvT∗-LS-21 closely follow the architectural designs of\nCvT for fair comparisons. Speciﬁcally, in CvT ∗-LS, we feed the token embeddings extracted by\nthe depth-wise separable convolution [66] of CvT to our long-short term attention. For dynamic\nprojection, we replace WP\ni in Eq. (3) with a depth-wise separable convolution to maintain consistency\nwith the patch embeddings, but we change its BN layer into a weight standardization [67, 68] on the\nspatial convolution’s weights for simplicity. We do not use position encoding. All of our models have\n3 stages, and the feature map size is the same as CvT in each stage when the image resolutions are\nthe same. CvT∗-LS-13 and CvT∗-LS-21 follow the same layer conﬁgurations as CvT-13 and CvT-21,\ni.e., the number of heads, the dimension of each head and the number of Transformer blocks are\nthe same as CvT in each stage. For all models on resolution 224 ×224, we set r = [64, 16, 4] and\nw = [8, 4, 2]. For higher resolutions, we scale up r and/or w to maintain similar effective receptive\nﬁelds for the attentions. At resolution 384 ×384, we use r = [64, 16, 4] and w = [12, 6, 3] for the 3\nstages. At resolution 448 ×448, we use r = [128, 32, 8] and w = [16, 8, 4].\nBesides maintaining the CvT architectures, we also try other architectures to further explore the\nadvantage of our method. With the efﬁcient long-short term attention, it becomes affordable to\nstack more layers on higher-resolution feature maps to fully utilize the expressive power of attention\nmechanisms. Therefore, we have created two new architectures, CvT ∗-LS-17 and CvT∗-LS-21S,\nthat have more and wider layers in the ﬁrst two stages, as shown in Table 12. Compared with\nCvT-21, CvT∗-LS-17 has 25% fewer parameters, less FLOPs, but obtained the same level of accuracy.\nCvT∗-LS-21S has fewer parameters than CvT ∗-LS-21, more FLOPs, and 0.4% higher accuracy,\ndemonstrating the advantage of focusing the computation on higher-resolution feature maps.\nThe effect of DualLN. We trained the CvT ∗-LS-13 model without DualLN, which has a test\naccuracy of 81.3, lower than the 81.9 with DualLN.\n4https://github.com/rishikksh20/convolution-vision-transformers\n18\nTable 12: Architectures of our CvT∗-LS-17 and CvT∗-LS-21S models. LSTA stands for our Long-\nShort Term Attention.\nOutput Size Layer Name CvT∗-LS-17 CvT∗-LS-21S\nStage 1\n56 ×56 Conv. Embed. 7 ×7, 128, stride 4\n56 ×56\nConv. Proj. \n\n3 ×3, 128\nH = 2, D= 128\nr = 64, w= 8\nR = 4\n\n×3LSTA\nMLP\nStage 2\n28 ×28 Conv. Embed. 3 ×3, 256, stride 2\n28 ×28\nConv. Proj. \n\n3 ×3, 256\nH = 4, D= 256\nr = 16, w= 4\nR = 4\n\n×4LSTA\nMLP\nStage 3\n14 ×14 Conv. Embed. 3 ×3, 384, stride 2\n14 ×14\nConv. Proj. \n\n3 ×3, 384\nH = 6, D= 384\nr = 4, w= 2\nR = 4\n\n×10\n\n\n3 ×3, 384\nH = 6, D= 384\nr = 4, w= 2\nR = 4\n\n×14LSTA\nMLP\n1 2 3 4 5 6 7 8 9 10 11 12 13\nLayer\n0.65\n0.70\n0.75\n0.80\n0.85Cosine Similarity\nCvT Trained\nOurs Trained\nCvT Init.\nOurs Init.\nFigure 6: Pairwise cosine similarity between patch embeddings at different layers of CvT-13 and\nCvT∗-LS-13, averaged on 50k images of ImageNet validation set. The larger cosine similarities at\ndeeper layer suggest that the feature representation is less diverse.\n500 1000 1500 2000 2500\nImage Resolution\n0\n5\n10\n15\n20\n25GPU Memory (GB)\nForward-backprop Peak GPU Memory (Image Classification)\nBatch=1\nBatch=2\nBatch=4\nBatch=8\nCvT\nTransformer-LS\nBatch=1\nBatch=2\nBatch=4\nBatch=8\nCvT\nTransformer-LS\nFigure 7: Running memory consumption of full self-attention (CvT-13) and Long-Short Transformer\non different tasks. We increase the sequence length resolution until the model is out of memory on a\nV100 GPU with 32GB memory.\n19\nF Evaluate the robustness of models trained on ImageNet-1k.\nTable 13: Corruption Error (CE) on ImageNet-C\nArch. Noise Blur Weather Digital\nGauss. Shot ImpulseDefocus Glass Motion ZoomSnow Frost Fog BrightContrast Elastic Pixel JPEG\nResNet-50 34.24 49.25 55.84 56.24 57.04 63.53 63.6864.02 64.04 64.89 69.2570.72 73.14 75.29 75.76\nDeiT-S 26.93 36.81 36.89 39.38 40.14 43.32 43.8044.36 45.71 46.90 47.2748.57 52.15 57.53 62.91\nCvT∗-LS-1325.64 36.89 37.06 38.06 43.78 43.78 44.6245.92 47.77 47.91 49.6049.66 54.92 57.24 68.72\nCvT∗-LS-1725.26 35.06 35.48 37.38 41.37 43.95 44.4746.05 46.17 46.38 49.0849.37 54.29 54.54 69.54\nCvT∗-LS-2124.28 34.95 35.03 35.93 39.86 40.71 41.2741.78 44.72 45.24 45.5047.19 51.84 53.78 67.05\nTable 14: Robustness evaluation on ImageNet-9. We report Top-1 Accuracy.\nModel Params (M) ImageNet (%) ImageNet-9 [59](%)\nOriginal Mixed-same Mixed-rand\nResNet-50 [35] 25.6 76.2 94.9 87.1 81.6\nDeiT-S [36] 22.1 79.8 97.1 89.1 84.2\nCvT∗-LS-13 20.3 81.9 97.0 90.7 85.6\nCvT∗-LS-21 32.1 82.7 97.2 91.5 85.8\nFor a fair comparison, we choose models with similar number of parameters. We select two\nrepresentative models, including the CNN-based model (ResNet) and the transformer-based model\n(DeiT). We give detailed results on all types of image transforms on ImageNet-C in Table 13. We\nevaluate our method on various ImageNet robustness benchmarks as follows:\n• ImageNet-C. ImageNet-C refers to the common corruption dataset. It consists of 15 types of\nalgorithmically common corruptions from noise, blur, weather, and digital categories. Each type\ncontains ﬁve levels of severity. In Table 4, we report the normalized mean corruption error (mCE)\ndeﬁned in Hendrycks and Dietterich [56]. In Table 13, we report the corruption error among\ndifferent types. In both tables, the lower value means higher robustness.\n• ImageNet-A. ImageNet-A is the natural adversarial example dataset. It contains naturally\ncollected images from online that mislead the ImageNet classiﬁers. It contains 7,500 adversarially\nﬁltered images. We use accuracy as our evaluation metric. The higher accuracy refers to better\nrobustness.\n• ImageNet-R. ImageNet-R (Rendition) aims to evaluate the model generalization performance\non out-of-distribution data. It contains renditions of 200 ImageNet classes (e.g. cartoons, grafﬁti,\nembroidery). We use accuracy as the evaluation metric.\n• ImageNet-9. ImageNet-9 aims to evaluate the model background robustness. It designs to\nmeasure the extent of the model relying on the image background. Following the standard\nsetting [59], we evaluate the two categories, includingMIXED -SAME and MIXED -RAND . MIXED -\nSAME refers to replace the background of the selected image with a random background of the\nsame class by GrabCut [ 59]; MIXED -RAND refers to replace the image background with a\nrandom background of the random class.\nFrom table 6, we ﬁnd that our method achieves signiﬁcant improvement compared to CNN-based\nnetwork (ResNet). For instance, our method improves the accuracy by 23.6%, 22.1%, 9.7% compared\nto ResNet on ImageNet-C, ImageNet-A, and ImageNet-R, respectively. For ImageNet-9, our method\nalso achieves favorable improvement by 4.3% on average (Mixed-same and Mixed-rand). It indicates\nthat our method is insensitive to background changes. We guess the potential reasons for these\nimprovements are (1) the attention mechanism and (2) the strong data augmentation strategies during\nthe training for vision transformer [ 4, 36]. The ﬁrst design helps the model focus more on the\nglobal context of the image as each patch could attend to the whole image areas. It reduces the\nlocal texture bias of CNN. The latter design increases the diversity of the training data to improve\nmodel’s generalization ability. Compared to DeiT, we also surprisingly ﬁnd that our method achieves\nslightly better performance. One plausible explanation is that our long-term attention has a favorable\nsmoothing effect on the noisy representations. Such improvements also indicate that different designs\nof attention and network architecture can be essential to improve the robustness. As the goal of this\npaper is not to design a robust vision transformer, the robustness is an additional bonus of our method.\n20\nWe believe that our observation opens new directions for designing robust vision Transformers. We\nleave the in-depth study as an important future work.\nThe detailed results of ImageNet-C and ImageNet-9 are shown in Table 13 and Table 14 respectively.\n21",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7333589792251587
    },
    {
      "name": "Transformer",
      "score": 0.6889811754226685
    },
    {
      "name": "Autoregressive model",
      "score": 0.6150103807449341
    },
    {
      "name": "Scalability",
      "score": 0.5627521872520447
    },
    {
      "name": "Language model",
      "score": 0.541860818862915
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43741440773010254
    },
    {
      "name": "Algorithm",
      "score": 0.39206427335739136
    },
    {
      "name": "Computer engineering",
      "score": 0.37810495495796204
    },
    {
      "name": "Voltage",
      "score": 0.14409390091896057
    },
    {
      "name": "Mathematics",
      "score": 0.11086398363113403
    },
    {
      "name": "Engineering",
      "score": 0.09537994861602783
    },
    {
      "name": "Econometrics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    }
  ],
  "institutions": []
}