{
    "title": "A Model of Zero-Shot Learning of Spoken Language Understanding",
    "url": "https://openalex.org/W2251416368",
    "year": 2015,
    "authors": [
        {
            "id": "https://openalex.org/A5067220251",
            "name": "Majid Yazdani",
            "affiliations": [
                "University of Geneva",
                "Laboratoire d'Informatique de Paris-Nord"
            ]
        },
        {
            "id": "https://openalex.org/A5084321238",
            "name": "James Henderson",
            "affiliations": [
                "Xerox (France)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2140266767",
        "https://openalex.org/W2950577311",
        "https://openalex.org/W1989996186",
        "https://openalex.org/W225503657",
        "https://openalex.org/W2115733720",
        "https://openalex.org/W2141599568",
        "https://openalex.org/W2150295085",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W21006490",
        "https://openalex.org/W2952230511",
        "https://openalex.org/W2406851047",
        "https://openalex.org/W2158899491",
        "https://openalex.org/W2102765684",
        "https://openalex.org/W2251803266",
        "https://openalex.org/W4298416081",
        "https://openalex.org/W2117130368",
        "https://openalex.org/W1806039879",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W2142623206",
        "https://openalex.org/W2998704965",
        "https://openalex.org/W2164019165",
        "https://openalex.org/W2161186120",
        "https://openalex.org/W2885050925",
        "https://openalex.org/W1990549830",
        "https://openalex.org/W2125993116",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2137132216"
    ],
    "abstract": "When building spoken dialogue systems for a new domain, a major bottleneck is developing a spoken language understanding (SLU) module that handles the new domain's terminology and semantic concepts.We propose a statistical SLU model that generalises to both previously unseen input words and previously unseen output classes by leveraging unlabelled data.After mapping the utterance into a vector space, the model exploits the structure of the output labels by mapping each label to a hyperplane that separates utterances with and without that label.Both these mappings are initialised with unsupervised word embeddings, so they can be computed even for words or concepts which were not in the SLU training data.",
    "full_text": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 244–249,\nLisbon, Portugal, 17-21 September 2015.c⃝2015 Association for Computational Linguistics.\nA Model of Zero-Shot Learning of Spoken Language Understanding\nMajid Yazdani\nComputer Science Department\nUniversity of Geneva\nmajid.yazdani@unige.ch\nJames Henderson\nXerox Research Center Europe\njames.henderson@xrce.xerox.com\nAbstract\nWhen building spoken dialogue systems\nfor a new domain, a major bottleneck is\ndeveloping a spoken language understand-\ning (SLU) module that handles the new\ndomain’s terminology and semantic con-\ncepts. We propose a statistical SLU model\nthat generalises to both previously unseen\ninput words and previously unseen out-\nput classes by leveraging unlabelled data.\nAfter mapping the utterance into a vector\nspace, the model exploits the structure of\nthe output labels by mapping each label\nto a hyperplane that separates utterances\nwith and without that label. Both these\nmappings are initialised with unsupervised\nword embeddings, so they can be com-\nputed even for words or concepts which\nwere not in the SLU training data.\n1 Introduction\nSpoken Language Understanding (SLU) in dia-\nlogue systems is the task of taking the utterance\noutput by a speech recognizer and assigning it a\nsemantic label that represents the dialogue actions\nof that utterance accompanied with their associ-\nated attributes and values. For example, the utter-\nance ”I would like Chinese food” is labelled with\ninform(food=Chinese), in which inform is the dia-\nlogue action that provides the value of the attribute\nfood that is Chinese.\nDialogue systems often use hand-crafted gram-\nmars for SLU, such as Phoenix (Ward, 1994),\nwhich are expensive to develop, and expensive\nto extend or adapt to new attributes and values.\nStatistical SLU models are usually trained on the\ndata obtained from a speciﬁc domain and loca-\ntion, using a structured output classiﬁer that can\nbe discriminative (Pradhan et al., 2004; Kate and\nMooney, 2006; Henderson et al., 2012) or genera-\ntive (Schwartz et al., 1996; He and Young, 2005).\nGathering and annotating SLU data is costly and\ntime consuming and therefore SLU datasets are\nsmall compare to the number of possible labels.\nBecause training sets for a new domain are\nsmall, or non-existent, learning is often an in-\nstance of Zero-shot or One-shot learning prob-\nlems (Palatucci et al., 2009; L. Fei-Fei; Fergus,\n2006), in which zero or few examples of some\noutput classes are available during the training.\nFor example, in the restaurant reservation domain,\nnot all possible combinations of foods and dia-\nlogue actions may be included in the training set.\nThe general idea to solve this type of problems is\nto map the input and class labels to a semantic\nspace of usually lower dimension in which simi-\nlar classes are represented by closer points in the\nspace (Palatucci et al., 2009; Weston et al., 2011;\nWeston et al., 2010). Usually unsupervised knowl-\nedge sources are used to form semantic codes of\nthe labels that helps us to generalize to unseen la-\nbels.\nOn the other hand, there are also different ways\nto express the same meaning, and similarly, most\nof them can not be included in the training set.\nFor instance, the system may have seen ”Please\ngive me the telephone number” in training, but the\nuser might ask ”Please give me the phone” at test\ntime. This problem, feature sparsity, is a common\nissue in many NLP tasks. Decomposition of in-\nput feature parameters using vector-matrix mul-\ntiplication (Bengio et al., 2003; Collobert et al.,\n2011; Collobert and Weston, 2008) has addressed\nthis sparsity issue successfully in previous work.\nIn this way, by sharing the word representations\nand composition matrices, we can overcome fea-\nture sparsity by producing similar representations\nfor similar utterances.\nIn order to represent words and concepts we\nuse word embeddings, which are a form of vec-\ntor space model. Word embeddings have proven\nto be effective models of semantic representation\n244\nof words in various NLP tasks (Baroni et al., 2014;\nYazdani and Popescu-Belis, 2013; Collobert et al.,\n2011; Collobert and Weston, 2008; Huang et al.,\n2012; Mikolov et al., 2013b). In addition to pa-\nrameter sharing, these representations enable us\nto leverage large scale unlabelled data. Because\nword embeddings trained on unlabeled data reﬂect\nthe similarity between words, they help the model\ngeneralize from the words in the original training\ncorpora to the words in the new extended domain,\nand help generalize from small amounts of data in\nthe extended domain.\nThe contribution of this paper is to build a rep-\nresentation learning classiﬁer for the SLU task that\ncan generalize to unseen words and labels. For ev-\nery utterance we learn how to compose the word\nvectors to form the semantics of that utterance for\nthis task of language understanding. Furthermore,\nwe learn how to compose the semantics of each la-\nbel from the semantics of the words used to name\nthat label. This enables us to generalize to unseen\nlabels.\nIn this work we use the word2vec software of\nMikolov et al. (2013a) 1 to induce unsupervised\nword embeddings that are used to initialize word\nembedding parameters. For this, we use an En-\nglish Wikipedia dump as our unlabelled training\ncorpus, which is a diverse broad-coverage corpus.\nIt has been shown (Baroni et al., 2014; Mikolov\net al., 2013b) that these embeddings capture lex-\nical similarities even when they are trained on a\ndiverse corpus like Wikipedia. We test our models\non a restaurant booking domain. We investigate\ndomain adaptation by adding new attribute types\n(e.g. goodformeal) and new attribute values (e.g.\nHayes Valley as a restaurant location). Our exper-\niments indicate that our model has better perfor-\nmance compared to a hand-crafted system as well\nas a SVM baseline.\n2 SLU Datasets\nThe dialogue utterances used to build the SLU\ndataset were collected during a trial of online di-\nalogue policy adaptation for a restaurant reserva-\ntion system based in San Francisco. The trial be-\ngan with (area, pricerange and food), and adapted\nthe Interaction Manager online to handle the ad-\nditional attribute types near, allowedforkids, and\ngoodformeal (Gaˇsic et al., 2014). User utterances\nfrom these trials were transcribed and annotated\n1https://code.google.com/p/word2vec/\nwith dialogue acts by an expert, and afterwards\nedited by another expert2. Each user utterance was\nannotated with a set of labels, where each label\nconsists of an act type (e.g. inform, request), an\nattribute type (e.g. foodtype, pricerange), and an\nattribute value (e.g. Chinese, Cheap).\nThe dataset is separated into four subsets,\nSFCore, SF1Ext, SF2Ext and SF3Ext, each\nwith an increasing set of attribute types, as speci-\nﬁed in Table 1. This table also gives the total num-\nber of utterances in each data set. For our ﬁrst ex-\nperiment, we split each dataset into about 15% for\nthe testing set and 85% for the training set. For our\nsecond experiment we use each extended subset\nfor testing and its preceding subsets for training.\nOntologyAttribute types ( # of values )# of utterances\nSFCore food(59),area(155),pricerange(3) 1103\nSF1Ext SFCore +near(39) 1810\nSF2Ext SF1Ext +allowedforkids(2) 1571\nSF3Ext SF2Ext +goodformeal(4) 1518\nTable 1: Domains for San Francisco (SF) restau-\nrants expanding in complexity\n3 A Dialogue Act Representation\nLearning Classiﬁer\nThe SLU model is run on each hypothesis output\nby the ASR component, and tries to predict the\ncorrect set of dialogue act labels for each hypoth-\nesis. This problem is in general an instance of\nmulti-label classiﬁcation, because a single utter-\nance can have multiple dialogue act labels. Also,\nthese labels are structured, since each label consist\nof an act type, an attribute type, and an attribute\nvalue. Each label component also has canonical\ntext associated with it, which is the text used to\nname the label component (e.g. “Chinese” as a\nvalue).\nThe number of possible dialogue acts grows\nrapidly as the domain is extended with new at-\ntribute types and values, making this task one of\nmulti-label classiﬁcation with a very large number\nof labels. One natural approach to this task is to\ntrain one binary classiﬁer for each possible label,\nto decide whether or not to include it in the output.\nIn our case, this requires training a large number\nof classiﬁers, and it is impossible to generalize to\n2This data is publically available from\nhttps://sites.google.com/site/\nparlanceprojectofficial/home/\ndatarepository\n245\ndialogue acts that include attributes or values that\nwere not in the training set since there won’t be\nany parameter sharing among label classiﬁers.\nIn our alternative approach, we build the rep-\nresentation of the utterance and the representation\nof the label from their constituent words, then we\ncheck if these representations match or not. In the\nfollowing we explain in details this representation\nlearning model.\n3.1 Utterance Representation Learning\nIn this section we explain how to build the utter-\nance representation from its constituent words. In\naddition to words, we use bigrams, since they have\nbeen shown previously to be effective features for\nthis task (Henderson et al., 2012). Following the\nsuccess in transfer learning from parsing to under-\nstanding tasks (Henderson et al., 2013; Socher et\nal., 2013), we use dependency parse bigrams in\nour features as well. We learn to build a local rep-\nresentation at each word position in the utterance\nby using the word representation, adjacent word\nrepresentations, and the head word representation.\nLet φ(w) be a d dimensional vector representing\nthe word w, and φ(Ui) be a hdimensional vector\nwhich is the local representation at word position\ni. We compute the local representation as follows:\nφ(Ui) =σ(φ(wi)Wword + φ(wh)WparseRk\n+\nφ(wj)Wprevious + φ(wk)Wnext) (1)\nin which wh is the head word with the depen-\ndency relation Rk to wi, and wj and wk are the\nprevious and next words. Wword is a d× h ma-\ntrix that transforms the word embedding to hidden\nrepresentation inputs. WparseRk\nis a d× h ma-\ntrix for the relation Rk that similarly transforms\nthe head word embedding (so Wparse is a tensor),\nand Wprevious and Wnext similarly transform the\nprevious and next words’ embeddings. Figure 1\ndepicts this representation building at each word.\n3.2 Label Representation Learning\nOne standard way to address the problem of multi-\nlabel classiﬁcation is building binary classiﬁers\nfor each possible label. Large margin classiﬁers\nhave been shown to be an effective tool for this\ntask (Pradhan et al., 2004; Kate and Mooney,\n2006). We use the same idea of binary classiﬁers\nto learn one hyperplane per label, which separates\nthe utterances with this label from all other utter-\nances, with a large margin. In the standard way of\nFigure 1: The multi-label classiﬁer\nbuilding the classiﬁer, each label’s hyperplane is\nindependent of other labels. To extend this model\nto a zero-shot learning classiﬁer, we use parame-\nter sharing among label hyperplanes so that similar\nlabels have similar hyperplanes.\nWe exploit the structure of labels by assuming\nthat each hyperplane representation is a compo-\nsition of representations of the label’s constituent\ncomponents, namely dialogue action, attribute and\nattribute value. We learn the composition function\nand the constituent representations while training\nthe classiﬁers, using the labelled SLU data. The\nconstituent representations are initialised as the\nword embeddings for the label constituent’s name\nstring, such as “inform”, “food” and “Chinese”,\nwhere these embeddings are trained on the unla-\nbelled data. Figure 1 depicts the classiﬁer model.\nWe deﬁne the hyperplane of the labelaj(attk =\nvall) with its normal vector Waj ,attk,vall as:\nWaj ,attk,vall = σ([φ(aj),φ(attk),φ(vall)]Wih)Who\nwhere φ(·) is the same mapping to ddimensional\nword vectors that is used above in the utterance\nrepresentation, Wih is a 3d× hmatrix and Who is\na h× hmatrix. The score of each local represen-\ntation vector φ(Ui) is its distance from this label\nhyperplane, which is computed as the dot product\nof the local vector φ(Ui) with the normal vector\nWaj ,attk,vall .\nWe sum these local scores for each po-\nsition i to build the whole utterance score:∑\niφ(Ui)WT\naj ,attk,vall . Alternatively we can think\nof this computation as summing the local vectors\nto get a whole-utterance representation φ(U) =∑\niφ(Ui) and then doing the dot product. The\n246\npooling method (sum) used in the model is (inten-\ntionally) over-simplistic. We did not want to dis-\ntract from the main contribution of the paper, and\nour dataset did not justify any more complex solu-\ntion since utterances are short. It can be replaced\nby more powerful approaches if it is needed.\nTo train a large margin classiﬁer, we train all\nthe parameters such that the score of an utterance\nis bigger than a margin for its labels and less than\nthe negative margin for all other labels. Thus, the\nloss function is as follows:\nmin\nθ\nλ\n2 θ2+\n∑\nU\nmax(0,1−y\n∑\ni\nφ(Ui)WT\naj ,attk,vall )\n(2)\nwhere θis all the parameters of the model, namely\nφ(wi) (word embeddings), Wword, WParse,\nWprevious, Wnext, Wih, and Who. yis either 1 or\n−1 depending whether the input U has that label\nor not.\nTo optimize this large margin classiﬁer we per-\nform stochastic gradient descent by using the ada-\ngrad algorithm on this primal loss function, sim-\nilarly to Pegasos SVM (Shalev-Shwartz et al.,\n2007), but here we backpropagate the errors to\nthe representations to train the word embeddings\nand composition functions. In each iteration of the\nstochastic training algorithm, we randomly select\nan utterance and its labels as positive examples\nand choose randomly another utterance with a dif-\nferent label as a negative example. When choos-\ning the negative sample randomly, we sample ut-\nterances with the same dialogue act but different\nattribute or value with 4 times higher probability\nthan utterances with a different dialogue act. This\nbiased negative sampling speeds up the training\nprocess since it provides more difﬁcult training ex-\namples to the learner.\nThe model is able to address the adaptivity is-\nsues because the utterance and the dialogue act\nrepresentations are in the same space using the\nsame shared parameters φ(w), which are ini-\ntialised with unsupervised word embeddings. It\nhas been shown that such word embeddings cap-\nture word similarities and hence the classiﬁer is\nno longer ignorant about any new attribute type or\nattribute value. Also, there is parameter sharing\nbetween dialogue acts because these word/label\nembeddings are shared, and the matrices for the\ncomposition of these representations are the same\nacross all dialogue acts. This can help overcome\nsparsity in the SLU training set by transferring\nlearning between similar situations and similar\ndialogue act triples. For example, if the train-\ning set does not contain any examples of the act\n”request(postcode)”, but many examples of ”re-\nquest(phone)”, sharing the parameters can help\nwith the recognition of ”request(postcode)” in ut-\nterances similar to ”request(phone)”. Moreover,\nthe SLU model is to some extent robust against\nparaphrasing in the input utterance because it\nmaps the utterance to a semantic space, and uses\nparse bigrams. More sophisticated vector-space\nsemantic representations of the utterance are an\narea for future work, but should be largely orthog-\nonal to the contribution of this paper.\nTo ﬁnd the set of compatible dialogue acts for a\ngiven utterance, we should check all possible dia-\nlogue acts. This can severely slow down SLU. To\navoid testing all possible dialogue combinations,\nwe build three different classiﬁers: The ﬁrst one\nrecognises the act types in the utterance, the sec-\nond one recognises the attribute types for each of\nthe chosen act types, and the third classiﬁer recog-\nnises the full dialogue acts as we described above,\nbut only for the chosen pairs of act types and at-\ntribute types.\n4 SLU Experiments\nIn the ﬁrst experiment, we measure SLU perfor-\nmance trained on all available data, by building a\ndataset that is the union of all the above datasets.\nThis measures the performance of SLU when there\nis a small amount of data for an extended do-\nmain. This dataset, similarly to SF3Ext, has 6\nmain attribute types. Table 2 shows the perfor-\nmance of this model. We report as baselines the\nperformance of the Phoenix system (hand crafted\nfor this domain) and a binary linear SVM trained\non the same data. The hidden layers have size\nh=d=50. For this experiment, we split each dataset\ninto about 15% for the testing set and 85% for the\ntraining set.\nSystem Outputs Precision Recall F-core\nPhoenix 516 84.10 41.65 55.71\nSVM 690 65.03 52.45 58.06\nOur 932 90.24 81.15 85.45\nTable 2: Performance on union of data (SF-\nCore+SF1Ext+SF2Ext+SF3Ext)\nOur SLU model can adapt well to the extended\ndomain with more attribute types. We observe\n247\nTest set\nmodel, train set SF1Ext SF2Ext SF3Ext\nP—R—F P—R—F P—R—F\nOur SFcore 73.36—66.11—69.54 74.61—59.73—66.34 72.54—53.86—61.81\nSVM SFcore 50.66— 38.7— 43.87 49.64—34.70— 40.84 48.99—30.91—37.90\nOur SF1Ext 83.18—66.08—73.65 78.32—59.98—67.93\nSVM SF1Ext 58.72—41.71—48.77 53.25—34.88—42.15\nOur SF2Ext 84.12—67.78—75.07\nSVM SF2Ext 59.27—42.80—49.70\nTable 3: SLU performance: trained on a smaller domain and tested on more inclusive domains.\nparticularly that the recall is almost twice as high\nas the hand-crafted baseline. This shows that our\nSLU can recognise most of the dialogue acts in\nan utterance, where the rule-based Phoenix sys-\ntem and a classiﬁer without composed output can-\nnot. Overall there are 1042 dialogue acts in the\ntest set. SLU recall is very important in the over-\nall dialogue system performance, as the effect of a\nmissed dialogue act is hard to handle for the Inter-\naction Manager. Both hand-crafted and our system\nshow relatively high precision.\nIn the next experiment, we measure how well\nthe new SLU model performs in an extended do-\nmain without any training examples from that ex-\ntended domain. We train a SLU model on each\nsubset, and test it on each of the more inclusive\nsubsets. Table 3 shows the results.\nNot surprisingly, the performance is better if\nSLU is trained on a similar domain to the test do-\nmain, and adding more attribute types and values\ndecreases the performance more. But our SLU\ncan generalise very well to the extended domain,\nachieving much better generalisation that the SVM\nmodel.\n4.1 Conclusion\nIn this paper, we describe a new SLU model\nthat is designed for improved domain adaptation.\nThe multi-label classiﬁcation problem of dialogue\nact recognition is addressed with a classiﬁer that\nlearns to build an utterance representation and a\ndialogue act representation, and decides whether\nor not they are compatible. The dialogue act repre-\nsentation is a vector composition of its constituent\nlabels’ embeddings, and is trained as the hyper-\nplane of a large margin binary classiﬁer for that di-\nalogue act. The utterance representation is trained\nas a composition of word embeddings. Since the\nutterance and the dialogue act representations are\nboth built using unsupervised word embeddings\nand share these embedding parameters, the model\ncan address the issues of domain adaptation. Word\nembeddings capture word similarities, and hence\nthe classiﬁer is able to generalise from known at-\ntribute types or values to similar novel attribute\ntypes or values. We tested this SLU model on\ndatasets where the number of attribute types and\nvalues is increased, and show much better re-\nsults than the baselines, especially in recall. The\nmodel succeeds in both adapting to an extended\ndomain using relatively few training examples and\nin recognising novel attribute types and values.\nAcknowledgments\nThe research leading to this work was funded by\nthe EC FP7 programme FP7/2011-14 under grant\nagreement no. 287615 (PARLANCE), and Hasler\nfoundation project no. 15019, Deep Neural Net-\nwork Dependency Parser for Context-aware Rep-\nresentation Learning. The authors also would like\nto thank Dr.Helen Hastie for her help in annotating\nthe dataset.\nReferences\nMarco Baroni, Georgiana Dinu, and Germ ´an\nKruszewski. 2014. Don’t count, predict! a\nsystematic comparison of context-counting vs.\ncontext-predicting semantic vectors. In Proceedings\nof the 52nd Annual Meeting of the Association for\nComputational Linguistics, pages 238–247.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Janvin. 2003. A neural probabilistic lan-\nguage model. J. Mach. Learn. Res., 3:1137–1155.\nRonan Collobert and Jason Weston. 2008. A uniﬁed\narchitecture for natural language processing: Deep\nneural networks with multitask learning. In Pro-\nceedings of the 25th International Conference on\nMachine Learning, ICML ’08, pages 160–167.\n248\nRonan Collobert, Jason Weston, L´eon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel Kuksa.\n2011. Natural language processing (almost) from\nscratch. The Journal of Machine Learning Re-\nsearch, 12:2493–2537.\nM Gaˇsic, D Kim, P Tsiakoulis, C Breslin, M Hender-\nson, M Szummer, B Thomson, and S Young. 2014.\nIncremental on-line adaptation of pomdp-based dia-\nlogue managers to extended domains.\nYulan He and Steve Young. 2005. Semantic process-\ning using the hidden vector state model. Computer\nSpeech and Language, 19:85–106.\nMatthew Henderson, Milica Ga ˇsi´c, Blaise Thom-\nson, Pirros Tsiakoulis, Kai Yu, and Steve Young.\n2012. Discriminative Spoken Language Under-\nstanding Using Word Confusion Networks. In Spo-\nken Language Technology Workshop, 2012.\nJames Henderson, Paola Merlo, Ivan Titov, and\nGabriele Musillo. 2013. Multilingual joint parsing\nof syntactic and semantic dependencies with a latent\nvariable model. Comput. Linguist., 39(4):949–998.\nEric H. Huang, Richard Socher, Christopher D. Man-\nning, and Andrew Y . Ng. 2012. Improving Word\nRepresentations via Global Context and Multiple\nWord Prototypes. In Annual Meeting of the Asso-\nciation for Computational Linguistics (ACL).\nRohit J. Kate and Raymond J. Mooney. 2006. Us-\ning string-kernels for learning semantic parsers. In\nProceedings of the 21st International Conference\non Computational Linguistics and the 44th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL-44, pages 913–920.\nR.; Perona L. Fei-Fei; Fergus. 2006. One-shot learning\nof object categories. IEEE Transactions on Pattern\nAnalysis Machine Intelligence, 28:594–611, April.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013a. Efﬁcient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nTomas Mikolov, Wen tau Yih, and Geoffrey Zweig.\n2013b. Linguistic regularities in continuous space\nword representations. In Proceedings of the 2013\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies (NAACL-HLT-2013).\nMark Palatucci, Dean Pomerleau, Geoffrey E. Hinton,\nand Tom M. Mitchell. 2009. Zero-shot learning\nwith semantic output codes. In Advances in Neu-\nral Information Processing Systems 22, pages 1410–\n1418.\nSameer Pradhan, Wayne Ward, Kadri Hacioglu, and\nJames H. Martin. 2004. Shallow semantic parsing\nusing support vector machines.\nRichard Schwartz, Scott Miller, David Stallard, and\nJohn Makhoul. 1996. Language understanding us-\ning hidden understanding models. In Spoken Lan-\nguage, 1996. ICSLP 96. Proceedings., Fourth Inter-\nnational Conference on, volume 2, pages 997–1000.\nIEEE.\nShai Shalev-Shwartz, Yoram Singer, and Nathan Sre-\nbro. 2007. Pegasos: Primal estimated sub-gradient\nsolver for svm. In Proceedings of the 24th Interna-\ntional Conference on Machine Learning, pages 807–\n814.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Y . Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 1631–1642.\nWayne Ward. 1994. Extracting information in sponta-\nneous speech. In ICSLP.\nJason Weston, Samy Bengio, and Nicolas Usunier.\n2010. Large scale image annotation: Learning to\nrank with joint word-image embeddings. Mach.\nLearn., 81.\nJason Weston, Samy Bengio, and Nicolas Usunier.\n2011. Wsabie: Scaling up to large vocabulary image\nannotation. In Proceedings of the Twenty-Second\nInternational Joint Conference on Artiﬁcial Intelli-\ngence - Volume Volume Three, pages 2764–2770.\nMajid Yazdani and Andrei Popescu-Belis. 2013. Com-\nputing text semantic relatedness using the contents\nand links of a hypertext encyclopedia. Artif. Intell.,\n194:176–202.\n249"
}