{
    "title": "Rethinking Masked Language Modeling for Chinese Spelling Correction",
    "url": "https://openalex.org/W4385572215",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2103240464",
            "name": "Hongqiu Wu",
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Shanghai Municipal Education Commission"
            ]
        },
        {
            "id": "https://openalex.org/A2114954971",
            "name": "Shaohua Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2114693050",
            "name": "Yuchen Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2112311038",
            "name": "Hai Zhao",
            "affiliations": [
                "Shanghai Municipal Education Commission",
                "Shanghai Jiao Tong University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3176484937",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4226034517",
        "https://openalex.org/W1494632860",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2251568283",
        "https://openalex.org/W3034797320",
        "https://openalex.org/W3035309733",
        "https://openalex.org/W3102725307",
        "https://openalex.org/W3174595604",
        "https://openalex.org/W2892311186",
        "https://openalex.org/W2997636815",
        "https://openalex.org/W3176140329",
        "https://openalex.org/W3173375787",
        "https://openalex.org/W3173175058",
        "https://openalex.org/W3173859131",
        "https://openalex.org/W4285293661",
        "https://openalex.org/W3177012523",
        "https://openalex.org/W1972099155",
        "https://openalex.org/W2251157340",
        "https://openalex.org/W4375959173",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3212092655",
        "https://openalex.org/W4285233025",
        "https://openalex.org/W2127610924",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2575782020",
        "https://openalex.org/W2952500220",
        "https://openalex.org/W3173712076"
    ],
    "abstract": "In this paper, we study Chinese Spelling Correction (CSC) as a joint decision made by two separate models: a language model and an error model. Through empirical analysis, we find that fine-tuning BERT tends to over-fit the error model while under-fit the language model, resulting in poor generalization to out-of-distribution error patterns. Given that BERT is the backbone of most CSC models, this phenomenon has a significant negative impact. To address this issue, we are releasing a multi-domain benchmark LEMON, with higher quality and diversity than existing benchmarks, to allow a comprehensive assessment of the open domain generalization of CSC models. Then, we demonstrate that a very simple strategy – randomly masking 20% non-error tokens from the input sequence during fine-tuning – is sufficient for learning a much better language model without sacrificing the error model. This technique can be applied to any model architecture and achieves new state-of-the-art results on SIGHAN, ECSpell, and LEMON.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 10743–10756\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nRethinking Masked Language Modeling for Chinese Spelling Correction\nHongqiu Wu1,2,∗\nand Shaohua Zhang3 and Yuchen Zhang3 and Hai Zhao1,2,†\n1Department of Computer Science and Engineering, Shanghai Jiao Tong University\n2Key Laboratory of Shanghai Education Commission for Intelligent Interaction\nand Cognitive Engineering, Shanghai Jiao Tong University\n3ByteDance\nwuhongqiu@sjtu.edu.cn,zhang.shaohua.cs@gmail.com\nzhangyuc@gmail.com,zhaohai@cs.sjtu.edu.cn\nAbstract\nIn this paper, we study Chinese Spelling Cor-\nrection (CSC) as a joint decision made by two\nseparate models: a language model and an er-\nror model. Through empirical analysis, we\nfind that fine-tuning BERT tends to over-fit\nthe error model while under-fit the language\nmodel, resulting in poor generalization to out-\nof-distribution error patterns. Given that BERT\nis the backbone of most CSC models, this phe-\nnomenon has a significant negative impact. To\naddress this issue, we are releasing a multi-\ndomain benchmark LEMON, with higher qual-\nity and diversity than existing benchmarks, to\nallow a comprehensive assessment of the open\ndomain generalization of CSC models. Then,\nwe demonstrate that a very simple strategy –\nrandomly masking 20% non-error tokens from\nthe input sequence during fine-tuning – is suffi-\ncient for learning a much better language model\nwithout sacrificing the error model. This tech-\nnique can be applied to any model architec-\nture and achieves new state-of-the-art results\non SIGHAN, ECSpell, and LEMON1.\n1 Introduction\nChinese Spelling Correction (CSC) is a crucial task\nin natural language processing (NLP) behind many\ndownstream applications, e.g, web search (Mar-\ntins and Silva, 2004; Gao et al., 2010), named en-\ntity recognition, optical character recognition (Afli\net al., 2016; Gupta et al., 2021). It aims to detect\nand correct the potential spelling errors in a sen-\ntence. BERT (Devlin et al., 2019) and its enhanced\nvariants have achieved state-of-the-art results in the\ncurrent CSC community (name a few) (Zhang et al.,\n2020; Liu et al., 2021; Zhu et al., 2022).\nFrom a high-level perspective, CSC requires\na language model and an error model working\n∗Work was done during a cooperation with ByteDance.\n†Corresponding author; This paper was partially sup-\nported by Key Projects of National Natural Science Foun-\ndation of China (U1836222 and 61733011).\n1https://github.com/gingasan/lemon\n我买的鸟会发出公鸽子的(生硬 -> 声音)。\n第一个是跟(生音 -> 声音)有问题。\n新的机器声影少一点。\nThe bird I bought would make the (stiff -> sound)...\nTraining Stage\nPredict\nThe first has a problem with the (raw -> sound).\nThe new machine has less shadow.\n新的机器声影少一点。\nThe new machine has less shadow.\n(声影 -> 声音)\n(shadow -> sound)\n我买的鸟声音很生硬。\nThe bird I bought sounds stiff.\n我买的鸟声音很(生硬 -> 声音)。\nThe bird I bought sounds (stiff -> sound).\nOver correction\nNo detection\nmodel output\nground truthtest input\ntrain pair\nTesting Stage\nNo correction\nFigure 1: Mistakes made by regularly fine-tuned BERT.\ncollaboratively to make a decision (Kernighan\net al., 1990). Suppose that the input sentence con-\ntains n characters X = (x1, ..., xn). The model\npredicts the corrected character at each position\nY = (y1, ..., yn). At each position i, let x−i indi-\ncate the characters at all other positions, then by\nBayes Rule (Kernighan et al., 1990), we have:\nP(yi|X) ∝ P(yi|x−i)  \nlanguage model\n·P(xi|yi, x−i)  \nerror model\n(1)\nwhere the language model decides the distribution\nof the character yi given the context, while the\nerror model represents the distribution of the poten-\ntial misspelled character xi given the context and\nits correct form (see Appendix A for the deriva-\ntion). According to the BERT architecture, these\ntwo models are jointly trained and evaluated. How-\never, their respective performances have not been\nthroughout studied by previous work.\nIn this paper, we make a key observation that\nBERT-based CSC models typically over-fit the er-\nror model, yet under-fit the language model, be-\n10743\ncause the error model is much easier to memorize\ncompared to the language model. As a result, the\nmodel generalizes very poor to unseen edit pairs\n(xi, yi) and fails to exploit the context x−i. We\nillustrate this fact in Figure 1. Here, the model has\nbeen exposed to edit pairs “生硬→声音” (correct\nstiff to sound) and “生音→声音” (correct raw to\nsound) during training. During testing, the model\nfails to detect an unseen edit pair “ 声影→声音”\n(correct shadow to sound) and meanwhile over-\ncorrects “ 生硬→声音” (correct stiff to sound).\nThis is due to the fact that the model naively mem-\norizes the training edit pairs, failing to identify if\nthey fit the broader context. We will present qualita-\ntive analysis of this phenomenon in later sections.\nThe consequence of a sub-optimal or under-fit\nlanguage model is that the model struggles to gener-\nalize to new contexts and new domains. SIGHAN\nis the current most widely-used benchmark in CSC,\nbut it is limited in two ways: (1) a narrow sentence\ncorpus sourced exclusively from the Chinese es-\nsays by foreign speakers (Wu et al., 2013); (2) a\nlow diversity of edit pairs (i.e. 370 edit pairs in its\ntest set). As a result, it does not pose enough chal-\nlenge to the model’s generalization ability. To this\nend, we present LEMON, a new benchmark that\nis a large-scale multi-domain dataset with natural\nspelling errors, which spans 7 domains and con-\ntains over 22,000 examples with 7,627 distinct edit\npairs collected from real human daily writing. It\nprovides a comprehensive evaluation of CSC mod-\nels in real-world scenarios.\nBased on LEMON and other public benchmarks,\nwe demonstrate that a very simple method can ef-\nfectively enhance language modeling without caus-\ning adverse effect to error modeling, thus signif-\nicantly improves CSC model performances. The\nmethod is to randomly mask 20% of the non-error\ntokens from the input sentence during fine-tuning\n(this is different from masking 15% tokens during\npre-training in BERT). If xi is masked, it forces\nthe model to predict yi given x−i without any clue\nabout xi, equivalent to training P(yi|x−i). This\nmasked-fine-tuning (Masked-FT) technique is un-\nlike other data augmentation methods based on\nhomophone substitution, random substitution or\nconfusion sets (Zhao and Wang, 2020; Liu et al.,\n2021), in that it does not impose any assumption\nabout human errors. As a result, it enables learn-\ning a completely unbiased error model from real\nhuman data. This property let Masked-FT achieve\nnew state-of-the-art across CSC benchmarks.\nWe also show that Masked-FT is effective in do-\nmain transfer. Suppose that there is an annotated\nparallel corpus for a certain domain, and we want\nto transfer the model of such a domain to a new\ndomain where only monolingual (i.e. unannotated)\ncorpus is available. We propose to train the model\nwith the parallel data along with a masked language\nmodeling (MLM) loss from the monolingual cor-\npus. The idea behind is to transfer the language\nmodel to the new domain while preserving the er-\nror model that is learned through the parallel data.\nEmpirical results demonstrate that this way of us-\ning monolingual data produces a better model than\ndata synthesis methods based on confusion sets.\nOur contributions are summarized as follows.\n(1) We perform empirical analysis showing that\nBERT-based CSC models learn a sub-optimal lan-\nguage model, resulting in a bad performance on\nout-of-distribution edit pairs. (2) We release a large-\nscale and multi-domain benchmark for CSC, which\nis more challenging than existing ones. (3) We\ndemonstrate that a simple masked-fine-tuning strat-\negy significantly enhance language modeling with-\nout hurting error modeling, leading to new state-of-\nthe-art results across benchmarks.\n2 Analysis of BERT fine-tuning\nIn this section, we report empirical analysis on\nBERT-based models. We study their top-k perfor-\nmance, generalization to unseen edit pairs, and gra-\ndient scales during training. The observation is that\nthe BERT-based models, with regular fine-tuning,\neasily over-fits the edit pairs in the training set and\nlearns a degenerated language model. For some\nanalyses, we also include the result of masked-FT\n(randomly mask 20% input tokens) for comparative\nstudy.\n2.1 Top-k Predictions\nCSC typically cares about the top-1 prediction at\neach position. But here, we print out the top-5\npredictions in order to get a sense of its language\nmodeling capability. We find that the fine-tuned\nBERT model tends to predict homophones and ho-\nmographs of the input character, regardless of its\ncontextual appropriateness. Note that homophones\nand homographs are the two main forms of spelling\nerrors in Chinese. Thus, it reveals that the error\nmodel has dominated the prediction. In contrast,\nthe model trained with Masked-FT tends to predict\n10744\nsource 吴阿姨年级大了。\nFT 吴阿姨年(纪,级,机,轻,青)大了。\nMasked-FT 吴阿姨年(纪,级,龄,岁,代)大了。\nsource 新的机器有可能声影少一点。\nFT 新的机器有可能声(影,景,应,音,引)少一点。\nMasked-FT 新的机器有可能声(音,影,声,响,味)少一点。\nTable 1: Top-k results each model recalls on the same\nsentence. The models here are trained on SIGHAN. FT\nrefers to regular fine-tuning.\ncharacters that fits the context better.\nWe demonstrate two cases in Table 1. In the first\ncase, both models make the correct top-1 prediction.\nAt top 2-5, however, the fine-tuned model predicts\na list of homophones: “ 年纪”, “年机” and “ 年\n轻”, “ 年青”. None of them makes any sense in\nthe context. Masked-FT predicts “年龄”, “年岁”,\nand “年代”, all carrying the meaning of age in\nChinese, which fits the context. In the second case,\nthe fine-tuned model predicts the correct answer\nat top-4, but through top 2-3, the predictions “景”\n(a homograph of “影”) and “应” (a homophone of\n“影”) don’t fit the context at all. In contrast, the\nMasked-FT model predicts “ 声音”, “声声”, and\n“声响”, which all represent the correct meaning:\nsound. All the homophones and homographs that\nthe FT model predicts come from the popular edit\npairs in the training data.\n2.2 Seen vs. Unseen Edit Pairs\nIn this experiment, we separate the test set of\nSIGHAN (Tseng et al., 2015) into two subsets, INC\n(shorthand for inclusive, representing edit pairs that\noverlap with the training set) and EXC (shorthand\nfor exclusive, with edit pairs that do not emerge in\nthe training set). Table 2 shows the comparison.\nThe fine-tuned BERT fits INC well (F1=64.1), but\nthe performance sharply drops on EXC (F1=6.3).\nIt suggests that the model generalizes poorly to\nunseen edit pairs where the error model does not\nprovide any useful signal.\nIt is worth noting that for many unseen edit pairs,\nalthough they never appear in the training data, they\ncan actually be corrected by human based on the\nPrec. Rec. F1\nfine-tuned INC 73.5 56.8 64.1\nEXC 10.7 ↓62.8 4.4 ↓52.4 6.3 ↓57.8\nvanilla BERT INC 51.5 48.5 49.9\nEXC 46.3 45.0 45.6\nTable 2: CSC performance crash on unseen edit pairs.\n0 1000 2000 3000\nStep\n0\n1\n2\n3GradNorm\nfine-tune\nmasked-fine-tune\n(a) Gradient Norm\n0 1000 2000 3000\nStep\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0F1\nfine-tune\nmasked-fine-tune (b) F1\nFigure 2: Gradient and model convergence. In (a), we\ncompute the L2-norm of gradients over all model pa-\nrameters. In (b), we evaluate the model each 15 steps.\n年 纪 轻 就 惨 遭 谢 顶 。 Sum\nFT 0.09 0.07 0.19 0.07 0.03 0.05 0.05 0.04 0.02 0.79\nMFT 0.27 0.10 0.40 0.19 0.53 0.68 1.16 0.92 0.26 4.92\nTable 3: Gradient on each token embedding. We choose\na model checkpoint at the early stage of training (two\nepochs). The sentence is “(年级 →年纪)轻轻就惨遭\n谢顶。” (Shedding of hair at a young (grade→age).).\ncontext. To illustrate this fact, we attempt to utilize\na vanilla BERT to correct the errors by replacing\nthe misspelled token by [MASK]. Surprisingly, we\nfind that the vanilla BERT can actually achieve a de-\ncent accuracy (F1=45.6) on EXC, much better than\nthe fine-tuned BERT (F1=6.3). This result high-\nlights the fact that a well-trained language model\nhas a great potential to handle unseen error pat-\nterns.\n2.3 Gradient Norm\nWe notice that the error model is relevant to most\nof the spelling errors, and it is easy to fit the model\nby memorizing the popular error patterns. As a re-\nsult, the CSC fine-tuning process converges quickly.\nWe plot the gradient norm curve during training in\nFigure 2. For BERT fine-tuning, the gradient de-\ncays quickly. After the gradient norm drops to\nvery small (less than 0.05) in the first few hundreds\nsteps, the F1 score stops increasing. It means that\nthe model has already converged. In contrast, the\ngradient norm of the Masked-FT model stays at a\nhigh level and the F1 score keeps improving.\nTable 3 reports the gradient norm on each indi-\nvidual token for an example sentence. The gradient\nproduced by BERT fine-tuning is much smaller\nthan that produced by Masked-FT (MFT), indicat-\ning that BERT fine-tuning involves less efficient\ntoken-level parameter updates across tokens.\n10745\n3 LEMON Benchmark\nSIGHAN (Tseng et al., 2015) is the current most\nwidely-used benchmark in CSC, but as described in\nthe introduction, it doesn’t pose enough challenge\nto test the generalization ability of CSC models.\nSIGHAN is exclusively collected from the Chinese\nessays written by foreign speakers (Wu et al., 2013).\nThat includes 1,100 test examples with a narrow\ncontent coverage. Besides, there are 370 distinct\nedit pairs in the test set, with nearly 70% overlap\nwith the training set. As a result, a model can\nachieve a decent score by memorizing the error\npatterns.\nIn this paper, we present LEMON, a large-scale\nmulti-domain dataset with natural spelling errors,\nwhich spans 7 domains, including game (GAM),\nencyclopedia (ENC), contract (COT), medical care\n(MEC), car (CAR), novel (NOV), and news (NEW).\nAs opposed to ECSpell (Lv et al., 2022), where the\ntypos are deliberately created by human on correct\nsentences, LEMON consists of over 22,000 exam-\nples with natural spelling errors identified from\ndaily human writing, annotated by well-educated\nnative Chinese speakers. The idea is to be as close\nto the real-life language distribution as possible.\nLEMON contains 7,627 edit pairs from all domains,\nwhich is much more diversified than SIGHAN.\nFigure 3 shows some concrete pieces of exam-\nples in LEMON. In MEC, for example, we see\ntyrosinase is misspelled, which is a professional\nword in medicine. The model thus requires certain\nexpertise to correct it. Additionally, the language\nstyle of context varies greatly from one domain to\nanother. For example, the expressions in GAM are\nidiomatic while those in COT are relatively regu-\nlarized and formal.\nThe bottom part of each block shows the his-\ntogram of all characters in this domain, indicating\nits lexical distribution. We can see that the lexicon\nof each domain varies greatly, suggesting different\ndomain-specific language styles. Due to space lim-\nitation, further analysis for LEMON is reported in\nAppendix B.\n4 Masked Fine-Tuning\nThe intuition behind masked fine-tuning (Masked-\nFT) is simple: we want to enhance the learning of\nlanguage model without perturbing the error model.\nBy equation (1), the language model predicts a to-\nken given all other tokens. Thus, we propose to\nrandomly mask a fraction of tokens and train the\n志 之 所 趋 , 无 远 勿 届\n志 之 所 趋 , 无 远 弗 届\nAspirations no boundaries\nAspirations without boundaries\n粗 狂 而 不 失 细 腻\n粗 旷 而 不 失 细 腻\nWild but delicate\nRugged but delicate\n其 代 表 元 权 解 除 本 合 同\n其 代 表 无 权 解 除 本 合 同\nYuan entitled to cancel\nNot entitled to cancel\n紫 外 线 可 以 激 活 络 氨 酸 酶\n紫 外 线 可 以 激 活 酪 氨 酸 酶\nUV can activate tirosinase\nUV can activate tyrosinase\n检 查 油 门 脚 蹋 板 回 位 弹 簧\n检 查 油 门 脚 踏 板 回 位 弹 簧\nCheck the foot pεdal of gas\nCheck the foot pedal of gas \n巨 斧 家 呆 着 无 边 的 旋 风\n巨 斧 夹 带 着 无 边 的 旋 风\nAxe houses a huge whirlwind\nAxe carries a huge whirlwind\n凤 城 玫 瑰 重 获 新 生\n风 城 玫 瑰 重 获 新 生\nPhoenix City Assassin is reborn\nWindy City Assassin is reborn\n我 每 天 六 天 半 起 床\n我 每 天 六 点 半 起 床\nI get up at six days a half\nI get up at six and a half\nMEC\nGAM\nCOT\nCAR NOV\nNEW SIGHAN\nENC\nFigure 3: A snapshot of LEMON. We also include the\nSIGHAN-15 test set here for comparison.\nmodel to restore them. For training with parallel\ndata, this is equivalent to randomly substituting a\nfraction of input tokens by a special mask token.\nThe mask token can be any token, as long as it\nnever occurs in an ordinary input. It can be under-\nstood as a special “typo” that human never makes,\nthus introducing zero bias to the error model. This\ntechnique can be applied to any model architec-\nture. Empirically, we find that masking 20% of\nnon-error tokens by [MASK] is the most effective.\nOther variants, such as using a different masking\nrate, selecting from both error and non-error tokens,\nand substituting by [unused], also works, but they\nachieve slightly worse results. The ablation study\nis presented in Section 6.\nFor training with both parallel (annotated) data\nand monolingual (unannotated) data, we propose to\nrandomly mask 20% tokens from the monolingual\ndata, then construct MLM loss (Devlin et al., 2019)\nand add it to the training objective. This is different\nfrom generating parallel data by corrupting 20%\ntokens. Any corruption rule (e.g. confusion sets)\nwould make assumptions on human errors, thus\nintroduce a bias to the error model. The MLM\nloss does not introduce any error model bias, and\nas Section 5 shows, it achieves better results in\ndomain transfer.\n5 Empirical Results\nIn this section, we compare regular fine-tuning with\nMasked-FT on a variety of model architectures,\n10746\nand evaluate them on SIGHAN-15, ECSpell, and\nLEMON. Our implementation is based on trans-\nformers (Wolf et al., 2020).\n5.1 Baseline Approaches\nWe briefly describe several baseline approaches.\n•BERT: We fine-tune the BERT model2.\n•Soft-Masked BERT: Zhang et al. (2020) apply\na GRU network as the detector and mask the likely\nerrors in the sequence in a soft way.\n•SpellGCN: Cheng et al. (2020) leverage GCN\nto integrate phonological and visual features.\n•ConfusBERT: Liu et al. (2021) use the confu-\nsion set to guide the mask strategy in MLM pre-\ntraining. To idea is to narrow the gap between CSC\nand MLM.\n•MDCSpell: Zhu et al. (2022) design an en-\nhanced detector-corrector network, where two mod-\nules are paralleled. The idea is to effectively incor-\nporate the detection clues for decision making.\n•CRASpell: Liu et al. (2022) introduce addi-\ntional errors to the original examples and enhances\nthe local smoothness of the model using KL diver-\ngence. The idea is to keep the model robust from\nnoisy context (i.e. with errors).\n•BERT-AT: Li et al. (2021) obtain the adversar-\nial examples through character-wise replacement\nusing the confusion set. However, this is time-\nconsuming. As an alternative, we adopt CreAT\n(Wu et al., 2023), an end-to-end adversarial training\nmethod to obtain the adversarial examples, which\nperturbs the input embeddings.\nWe do not take autoregressive models into ac-\ncount in this paper. It is worth noting that in CSC,\nBERT-base models remain the primary architec-\nture due to its ability to perform inference for each\ntoken in parallel. It has been shown that in previ-\nous work autoregressive models like GPT2 (Brown\net al., 2020) can work much worse on the concern-\ning CSC tasks (Li and Shi, 2021).\n5.2 SIGHAN\nSIGHAN-15 (Tseng et al., 2015) is a widely-used\nbenchmark in CSC, which contains 6,476 training\nexamples and 1,100 test examples. We follow the\ncommon practice to convert it to simplified Chinese.\nIn addition, we follow the two-stage training setting\nin most previous work (Liu et al., 2021; Zhu et al.,\n2022), pre-training the model on the public aug-\nmented data (271,329 examples) using OCR- and\n2https://huggingface.co/bert-base-chinese\nPrec. Rec. F1\nBERT 73.0 72.6 72.8\nw/. Masked-FT 76.7↑3.7 79.1↑6.5 77.9↑5.1\nSoft-Masked BERT 67.6 72.8 70.1\nw/. Masked-FT 76.3↑8.7 81.8↑9.0 79.0↑8.9\nMDCSpell† 78.4 78.2 78.3\nSpellGCN† 72.1 77.7 75.9\nConfusBERT† 72.7 76.1 74.4\nDCN† 74.5 78.2 76.3\nPLOME† 75.3 79.3 77.2\nREALISE† 75.9 79.9 77.8\nPHMOSpell† 89.6 69.2 78.1\nTable 4: Fine-tuning results on SIGHAN-15. The results\nin the bottom part requires additional pre-training. †\nindicates the result we quote (DCN (Wang et al., 2021),\nPLOME (Liu et al., 2021), REALISE (Xu et al., 2021),\nPHOMOSpell (Huang et al., 2021)).\nMethod I-F1 E-F1 F1\nLAW\nvanilla BERT 49.6 35.7 -\nBERT 68.4 10.0 40.2\nw/. Masked-FT 84.9↑16.5 65.9↑55.9 76.8↑36.6\nMDCSpell 69.0 13.7 42.2\nw/. Masked-FT 86.1↑17.1 73.2↑59.5 81.1↑38.9\nMED\nBERT 35.6 5.7 26.9\nw/. Masked-FT 46.7↑11.1 43.2↑37.5 63.8↑36.9\nMDCSpell 32.1 7.4 25.7\nw/. Masked-FT 47.9↑15.8 47.8↑40.4 72.4↑46.7\nODW\nBERT 54.4 7.4 26.7\nw/. Masked-FT 71.3↑16.9 42.4↑35 62.9↑36.2\nMDCSpell 55.9 6.7 27.5\nw/. Masked-FT 75.1↑19.2 51.2↑44.5 72.0↑44.5\nTable 5: Fine-tuning results on ECSpell.\nASR-based generation (Wang et al., 2018), then in\nthe second stage, training on its own labeled data.\nWe select the best learning rate and batch size in\n{1e-5, 2e-5, 5e-5} and {32, 128} respectively for\neach stage. We train each model for 100,000 steps\nfor the first stage and 10,000 steps for the second.\nTable 4 summarizes the results on SIGHAN-15.\nWith BERT, Masked-FT achieves very competitive\nresults (improves F1 from 72.8 to 77.9). With Soft-\nMasked BERT, it achieves the new state-of-the-art\non SIGHAN (79.0 F1). Although we have not\ntrained other baseline models with Masked-FT, it\nis likely that they can get a similar performance\nboost.\n5.3 ECSpell\nECSpell (Lv et al., 2022) is a newly shared CSC\ndataset with three domains, LAW (1,960 training\n10747\nGAM ENC COT MEC CAR NOV NEW SIG Avg\nBERT 27.1 41.6 63.9 47.9 47.6 34.2 50.7 50.6 45.5\nw/. MFT 33.3↑6.2 45.5↑3.9 64.1↑0.2 50.9↑3.0 52.3↑4.7 36.0↑1.8 56.0↑5.3 53.4↑2.8 48.9↑3.4\nSoft-Mased 26.3 43.5 63.8 48.8 47.7 34.3 52.7 50.5 45.9\nw/. MFT 29.8↑3.5 44.6↑1.1 65.0↑1.2 49.3↑0.5 52.0↑4.3 37.8↑3.5 55.8↑3.1 53.4↑3.0 48.4↑2.5\nMDCSpell 28.2 42.4 63.1 49.4 49.1 35.4 53.9 53.2 46.5\nw/. MFT 31.2↑3.0 45.9↑3.5 65.4↑2.3 52.0↑2.6 52.6↑3.5 38.6↑3.2 57.3↑3.4 54.7↑1.5 49.7↑3.2\nCRASpell 22.6 44.5 63.8 48.0 49.6 35.5 53.0 52.4 46.2\nw/. MFT 30.7↑8.1 48.1↑3.6 66.0↑2.2 51.7↑3.7 51.7↑2.1 38.6↑3.1 55.9↑2.9 55.1↑2.7 49.7↑3.5\nBERT-AT 25.6 43.0 62.6 49.4 47.5 33.9 51.6 51.0 45.6\nw/. MFT 34.4↑8.8 47.1↑4.3 66.8↑4.2 52.0↑2.6 51.6↑4.1 36.5↑2.6 55.0↑3.4 53.8↑2.8 49.7↑4.1\nTable 6: Performances on LEMON. We report the F1 scores and also include SIGHAN as the 8th domain (SIG).\nand 500 test examples), MED (medical treatment,\n3,000 training and 500 test) and ODW (official doc-\nument writing, 1,728 training and 500 test). The hy-\nperparameter search is similar to that in SIGHAN\nand we train each model for 5,000 steps.\nDifferent form SIGHAN, the test set of ECSpell\ncontains a high proportion ( ≈70%) of edit pairs\nthat never emerge in the training set. As in Sec-\ntion 2.2, let EXC be the test subset where the edit\npairs are not in the the training set, and INC be the\ncomplementary set. We define two new metrics,\ninclusive F1 (I-F1) and exclusive F1 (E-F1), to\nmeasure the model performance on the two subsets.\nA higher E-F1 suggests that the model is better at\ngeneralizing to unseen errors.\nFrom Table 5, we see that Masked-FT improves\nthe BERT model’s E-F1 by a large scale on all\nthree domains (55.9, 37.5 and 35.0 absolute points).\nIt also generates significant gains on I-F1 (16.5,\n11.1 and 16.9 absolute points). This is because\nthat a better language model can assist the error\nmodel in making more contextual decisions, even\non popular head error patterns. With Masked-FT,\nBERT and MDCSpell achieve the new state-of-the-\nart F1 scores on all three domains of ECSpell.\nWe note that the vanilla BERT performs better\nthan the fine-tuned BERT on E-F1 when the er-\nror position is known, but consistently worse than\nMasked-FT. It means that regular fine-tuning can\nlead to contextual degeneration, while Masked-FT\nactually learns a better language model than vanilla\nBERT.\n5.4 LEMON\nWe report two experiments on LEMON. In the\nfirst experiment, only monolingual data is used\nto train the model. We collect monolingual sen-\ntences from two general databases wiki2019zh and\nnews2016zh3 and use the confusion set in Liu et al.\n(2021) to synthesize paired sentences for training.\nSpecifically, we uniformly choose a Chinese char-\nacter in a sentence and replace it with a counter-\npart in its confusion set (40% →same pronuncia-\ntion; 30% →similar pronunciation; 20% →similar\nglyph; 10% →random). It finally generates 34 mil-\nlion training sentence pairs. We use the same con-\nfusion set in the following part, unless otherwise\nspecified.\nWe select the learning rate in {1e-5, 2e-5, 5e-\n5} and use 8192 as the batch size. Each model is\ntrained for 30,000 steps (more than 7 epochs). We\nuniformly sample 20% examples in each domain\n(no more than 200 examples) and put them together\nas the development set.\nTable 6 summarizes the results. We find Masked-\nFT (shorthand MFT) consistently improves every\nmodel and across every domain. It is worth noting\nthat although BERT-AT performs comparably with\nfine-tuning BERT (only 0.1 gain), the gap grows\nwider with Masked-FT (0.8 gain). It is known that\nadversarial training enhances the optimization of\nthe objective function. With regular fine-tuning,\nit mainly improves error modeling. With Masked-\nFT, it improves both error modeling and language\nmodeling, resulting in greater performance gains.\nIn the second experiment, we evaluate on domain\ntransfer. In this setting, we have 2.8M sentence\npairs from the news (NEW) domain, annotated by\nhuman editors. Our goal is to deploy a model for\nthe medical care (MEC) and the car (CAR) domain.\nFor each of these two domains, we have 10k sen-\n3https://github.com/brightmart/nlp_chinese_\ncorpus\n10748\nTraining data Transfer Method NEW MEC CAR\nNEW - 70.7 55.3 64.1\nNEW + MEC MLM Loss 72.2 62.1 -\nNEW + MEC Synthesis (Masked-FT) 71.4 58.1 -\nNEW + MEC Synthesis (FT) 61.4 54.6 -\nNEW + CAR MLM Loss 71.1 - 68.4\nNEW + CAR Synthesis (Masked-FT) 69.4 - 65.4\nNEW + CAR Synthesis (FT) 61.6 - 59.5\nTable 7: Domain transfer results (F1 score). All models\nare trained with Masked-FT, unless specified as FT, re-\nferring to regular fine-tuning.\ntences without any human annotation. We explore\ntwo methods to utilize the unannotated data: (1)\nconstruct and train with MLM loss, as described in\nSection 4; (2) generate synthetic data by corrupting\nunannotated sentences with a confusion set (train\nwith either regular FT or Masked-FT). For both\nstrategies, the model is jointly trained on the 2.8M\nannotated data along with 10k monolingual data.\nFrom Table 7, we find that incorporating MLM\nloss on the unannotated data gives higher F1 scores\nthan training with the 2.8M annotated data alone.\nFurthermore, the MLM loss method works bet-\nter than the data synthesis method (with or with-\nout Mask-FT). We conjecture that the high-quality\nannotated data has contributed to a precise error\nmodel. The additional MLM loss helps learning a\nbetter language model for the new domain without\nchanging the error model. On the other hand, the\ndata synthesis method introduces a new error distri-\nbution, thus impairs the error model. Overall, the\nbest combination is to jointly train the model on\nparallel data with Masked-FT, and on monolingual\ndata with MLM loss.\n6 Further Analysis\nMask Rate We investigate the impact from the\nmask rate p. A large p can hurt the training as it\nwipes out too much contextual information. From\nTable 8, we see that the model improves as p goes\nfrom 0 to 20%. Even p = 5% substantially im-\nproves E-F1. However, an overly high p can hurt\nthe performance as the context is spoiled.\nMask Strategy We default to masking the input\ntokens with the [MASK] token. In fact, any token\nthat does not appear in ordinary inputs can be cho-\nsen to perform Masked-FT. From Table 9, we find\nthat masking with [unused] results in similar but\nslightly lower performance gains. We hypothesize\nthat since [MASK] matches the training of vanilla\nMask rate F1 I-F1 E-F1\n0% 40.2 68.4 10.0\n5% 62.0 73.9 47.7\n10% 70.1 81.3 55.2\n15% 75.6 83.1 64.8\n20% 76.8 84.9 65.9\n30% 75.7 83.2 62.3\n50% 66.7 75.6 60.7\nTable 8: Impact of mask ratio on ECSpell-LAW.\nMask strategy ENC CAR NEW Avg\nfine-tuning 41.6 47.6 50.7 46.6\nw/. [MASK] 45.5 52.3 56.0 51.3 (↑)\nw/. [unused] 44.9 52.2 55.5 50.9 ( ↑)\nw/. [UNK] 39.1 45.2 47.1 43.8 ( ↓)\nmask non-error 45.5 52.3 56.0 51.3 (↑)\nmask error 42.9 48.2 52.2 47.8 ( ↑)\nmask any 45.0 49.5 53.8 49.4 ( ↑)\nTable 9: Comparison of mask strategies on three\nLEMON domains (F1 score). The mask rate is 0.2.\nBERT, it is initialized with a better embedding than\nthat of [unused]. On the other hand, masking\nwith [UNK] leads to a poor result. This is because\nthat [UNK] can occur in ordinary inputs to encode\nunknown characters. Masking with this token in-\ntroduces an implicit assumption that when an un-\nknown character appears in the input, it is very\nlikely a spelling error, which is obviously not true.\nThis result highlights the necessity of keeping the\nerror model intact.\nAnother decision factor is the position to mask.\nIn Table 9, we compare three strategies: masking\nnon-error tokens only, masking error tokens only,\nand masking any token. We find that the “mask-\ning non-error tokens only” strategy works the best.\nThis is because that the error model can only be\nlearned from error tokens. Masking error tokens\nreduces the amount of training data for error model-\ning, resulting in a slightly worse error model. How-\never, Masked-FT consistently outweighs regular\nfine-tuning no matter where we mask.\nvs. Data Augmentation via Confusion Set A\npopular data augmentation strategy is to randomly\nsubstitute a certain fraction of tokens with a mis-\nspelled token from the confusion set. Liu et al.\n(2021) use the confusion set to guide the masking\nstrategy in MLM pre-training. We apply the same\nconfusion set substitution rules to fine-tuning. As\nshown in Table 10, using a confusion set for data\naugmentation helps in the pre-training stage, but it\ndoes not help in the fine-tuning stage. Again, this\n10749\nis due to the fact that any confusion set introduces a\nbias to the error model. In particular, the confusion\nset substitution injects large amount of errors that\nhumans would not make in practice. As a result,\nthe model will learn to detect and correct errors in\nan overly aggressive manner.\nMethod Prec. Rec. F1\nSIGHAN\nMasked-FT 76.7 79.1 77.9\nconfusion-FT 63.9 75.2 69.1\nconfusion-pretrain† 72.7 76.1 74.4\nTable 10: Masked-FT vs. confusion set (F1 score).\nMethod ENC CAR NEW\nLEMON\nMasked-FT 45.5 52.3 56.0\nconfusion-FT 35.2 43.4 46.3\nmixed-FT 40.7 47.4 50.5\nTable 11: Masked-FT vs. confusion set (F1 score).\nTable 11 reports a similar comparison on\nLEMON. Again, Masked-FT consistently outper-\nforms fine-tuning with confusion set substitution.\nWe also compare with the “mixed” strategy pro-\nposed by (Zhao and Wang, 2020): with 50% proba-\nbility, masking the sentence, and with the remain-\ning 50% probability, corrupting the sentence via the\nconfusion set. The result of the “mixed” strategy\ninterpolates between the two extremes, suggesting\nthat a mixing strategy cannot offset the error model\nbias caused by the confusion set.\nCase Study We study two concrete examples in\nTable 12 where CSC is context dependent. For the\nfirst case (It seems no one has ever found out sil-\nver taels.), the fine-tuned model wants to correct\nfound out to be took out, while Mask-FT does not\nmake any change. Both found out silver taels and\ntook out silver taels are reasonable combinations.\nAccording to the context, however, we can reason\nthat someone is digging for treasure. Hence, found\nout silver taels is more appropriate. For the second\ncase (There was a smart person who applied for\na job with a salary of 1 yuan for the first year, 2\nyears (→yuan) for the second...), we can reason\nthe second year should be corrected to yuan be-\ncause the previous context mentions salary, while\nthe fine-tuned model is not able to do so.\nError analysis Though Masked-FT exhibits\npowerful potential, we further study its error cases\nto enlighten future research. We illustrate two typi-\ncal error cases in Table 13. For the first case, “洛\n汀新” (Lotensin) is a particular kind of pill, while\nsource 但好像从没见人淘出过银两。\ntarget 但好像从没见人淘出过银两。\nFT 但好像从没见人掏出过银两。\nMasked-FT 但好像从没见人淘出过银两。\nsource 有一聪明人应聘年薪只要1元,第二年2年...\ntarget 有一聪明人应聘年薪只要1元,第二年2元...\nFT 有一聪明人应聘年薪只要1元,第二年2年...\nMasked-FT 有一聪明人应聘年薪只要1元,第二年2元...\nTable 12: Case study selected from LEMON.\nsource 可以换成洛听新，一天一片...\ntarget 可以换成洛汀新，一天一片...\nMasked-FT 可以换成洛听新，一天一片...\nsource 不要随便使用化妆品，保持皮肤洁净...\ntarget 不要随便使用化妆品，保持皮肤洁净...\nMasked-FT 不要随便使用化浴品，保持皮肤洁净...\nTable 13: Error analysis selected from ECSpell-MED.\nMask-FT cannot allow the model to acquire profes-\nsional knowledge. It suggests that a universal cor-\nrection system necessitates domain-specific data or\nknowledge for stronger adaption to some domain\nlike medicine, science, with a wide range of ex-\npertise. For the second case, the model wrongly\ncorrects “妆” (makeup) to “浴” (bathing) because\nof the subsequent context “保持皮肤洁净” (keep\nskin clean). It implies a subtle trade-off between\nlanguage model and error model. Of course, this is\nan extreme case, which rarely occurs.\n7 Related Work\nFor Chinese spelling correction, BERT (Devlin\net al., 2019; Liu et al., 2019; Cui et al., 2020) is\nthe straightforward backbone model. There is a\nline of work on improving the model architecture\non top of BERT, such as imposing masking sig-\nnals to those potential error tokens to improve er-\nror detection (Zhang et al., 2020), incorporating\nmulti-modal knowledge (e.g. pronunciation, glyph)\n(Cheng et al., 2020; Liu et al., 2021; Huang et al.,\n2021; Xu et al., 2021; Zhang et al., 2021), using\nmulti-task network to explicitly let the model de-\ntect (Zhu et al., 2022) or predict the pronunciation\n(Liu et al., 2021). Another major category is data\naugmentation, with the goal of synthesizing effi-\ncient training data. Existing data augmentation\ntechniques are based on homophone substitution,\nrandom substitution or confusion sets (Wang et al.,\n2018, 2019; Liu et al., 2021; Guo et al., 2021).\nThe decomposition of CSC into a language\nmodel and an error model is inspired by the classi-\n10750\ncal noisy channel theory (Kernighan et al., 1990).\nThe masked-FT method proposed in this paper is\nsimilar to the “dynamic masking” method proposed\nby Zhao and Wang (2020). However, there are\na few differences between the two studies. First,\nZhao and Wang (2020) describes dynamic mask-\ning as a data augmentation method, and proposes\nto mix it with other data augmentation techniques\nsuch as confusion set substitution; in contrast, we\ndescribe masked-FT as a mean to enhance lan-\nguage modeling without perturbing error modeling,\ndemonstrating both theoretically and empirically\nthat it should be carried out alone without mixing\nwith data augmentation. Second, we study domain\ntransfer with monolingual data, showing that MLM\ntraining performs better than training with synthe-\nsized data. Again, it verifies our language/error\ndecomposition theory and to the best of our knowl-\nedge, was not discussed in previous work.\n8 Conclusion\nThis paper presents qualitative analysis and shows\nthat existing CSC models lean to over-fit the error\nmodel and under-fit the language model. A simple\nyet effective method is thus presented to encourage\na better language model learning. Empirical results\ndemonstrate that the simple method achieves new\nstate-of-the-art results on public benchmarks, in-\ncluding on LENON, a new large-scale challenging\nbenchmark released with this paper.\nLimitations\nWe have not tested all possible recent methods on\nLEMON. We have used expensive GPU resources\nto speed up the training process on LEMON, with\n8 NVIDIA A100 sheets, but consistent results can\nalso be obtained with 8 V100 sheets. Our work\nfocuses on Chinese. Other languages, such as\nJapanese and Korean, could benefit from the same\ntechnique, but have not been studied in this work.\nReferences\nHaithem Afli, Zhengwei Qiu, Andy Way, and Páraic\nSheridan. 2016. Using SMT for OCR error correc-\ntion of historical texts. In Proceedings of the Tenth\nInternational Conference on Language Resources\nand Evaluation LREC 2016, Portorož, Slovenia, May\n23-28, 2016. European Language Resources Associ-\nation (ELRA).\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nXingyi Cheng, Weidi Xu, Kunlong Chen, Shaohua\nJiang, Feng Wang, Taifeng Wang, Wei Chu, and Yuan\nQi. 2020. Spellgcn: Incorporating phonological and\nvisual similarities into language models for chinese\nspelling check. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2020, Online, July 5-10, 2020, pages\n871–881. Association for Computational Linguistics.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin\nWang, and Guoping Hu. 2020. Revisiting pre-trained\nmodels for chinese natural language processing. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020, Online Event, 16-20 Novem-\nber 2020, volume EMNLP 2020 of Findings of ACL,\npages 657–668. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nJianfeng Gao, Xiaolong Li, Daniel Micol, Chris Quirk,\nand Xu Sun. 2010. A large scale ranker-based sys-\ntem for search query spelling correction. In COLING\n2010, 23rd International Conference on Computa-\ntional Linguistics, Proceedings of the Conference,\n23-27 August 2010, Beijing, China, pages 358–366.\nTsinghua University Press.\nZhao Guo, Yuan Ni, Keqiang Wang, Wei Zhu, and Guo-\ntong Xie. 2021. Global attention decoder for chinese\nspelling error correction. In Findings of the Associa-\ntion for Computational Linguistics: ACL/IJCNLP\n2021, Online Event, August 1-6, 2021 , volume\nACL/IJCNLP 2021 of Findings of ACL, pages 1419–\n1428. Association for Computational Linguistics.\nHarsh Gupta, Luciano Del Corro, Samuel Broscheit,\nJohannes Hoffart, and Eliot Brenner. 2021. Unsu-\npervised multi-view post-ocr error correction with\nlanguage models. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2021, Virtual Event / Punta Cana,\n10751\nDominican Republic, 7-11 November, 2021 , pages\n8647–8652. Association for Computational Linguis-\ntics.\nLi Huang, Junjie Li, Weiwei Jiang, Zhiyu Zhang,\nMinchuan Chen, Shaojun Wang, and Jing Xiao. 2021.\nPhmospell: Phonological and morphological knowl-\nedge guided chinese spelling check. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual\nEvent, August 1-6, 2021, pages 5958–5967. Associa-\ntion for Computational Linguistics.\nMark D. Kernighan, Kenneth Ward Church, and\nWilliam A. Gale. 1990. A spelling correction pro-\ngram based on a noisy channel model. In 13th Inter-\nnational Conference on Computational Linguistics,\nCOLING 1990, University of Helsinki, Finland, Au-\ngust 20-25, 1990, pages 205–210.\nChong Li, Cenyuan Zhang, Xiaoqing Zheng, and Xuan-\njing Huang. 2021. Exploration and exploitation: Two\nways to improve chinese spelling correction models.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing, ACL/IJCNLP 2021, (Volume 2:\nShort Papers), Virtual Event, August 1-6, 2021, pages\n441–446. Association for Computational Linguistics.\nPiji Li and Shuming Shi. 2021. Tail-to-tail non-\nautoregressive sequence prediction for chinese gram-\nmatical error correction. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing, ACL/IJCNLP\n2021, (Volume 1: Long Papers), Virtual Event, Au-\ngust 1-6, 2021 , pages 4973–4984. Association for\nComputational Linguistics.\nShulin Liu, Shengkang Song, Tianchi Yue, Tao Yang,\nHuihui Cai, Tinghao Yu, and Shengli Sun. 2022.\nCraspell: A contextual typo robust approach to im-\nprove chinese spelling correction. In Findings of\nthe Association for Computational Linguistics: ACL\n2022, Dublin, Ireland, May 22-27, 2022, pages 3008–\n3018. Association for Computational Linguistics.\nShulin Liu, Tao Yang, Tianchi Yue, Feng Zhang, and\nDi Wang. 2021. PLOME: pre-training with mis-\nspelled knowledge for chinese spelling correction.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing, ACL/IJCNLP 2021, (Volume 1:\nLong Papers), Virtual Event, August 1-6, 2021, pages\n2991–3000. Association for Computational Linguis-\ntics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nQi Lv, Ziqiang Cao, Lei Geng, Chunhui Ai, Xu Yan, and\nGuohong Fu. 2022. General and domain adaptive chi-\nnese spelling check with error consistent pretraining.\nCoRR, abs/2203.10929.\nBruno Martins and Mário J. Silva. 2004. Spelling cor-\nrection for search engine queries. In Advances in\nNatural Language Processing, 4th International Con-\nference, EsTAL 2004, Alicante, Spain, October 20-22,\n2004, Proceedings, volume 3230 of Lecture Notes in\nComputer Science, pages 372–383. Springer.\nYuen-Hsien Tseng, Lung-Hao Lee, Li-Ping Chang, and\nHsin-Hsi Chen. 2015. Introduction to SIGHAN 2015\nbake-off for chinese spelling check. In Proceedings\nof the Eighth SIGHAN Workshop on Chinese Lan-\nguage Processing, SIGHAN@IJCNLP 2015, Beijing,\nChina, July 30-31, 2015, pages 32–37. Association\nfor Computational Linguistics.\nBaoxin Wang, Wanxiang Che, Dayong Wu, Shijin\nWang, Guoping Hu, and Ting Liu. 2021. Dynamic\nconnected networks for chinese spelling check. In\nFindings of the Association for Computational Lin-\nguistics: ACL/IJCNLP 2021, Online Event, August\n1-6, 2021, volume ACL/IJCNLP 2021 of Findings\nof ACL, pages 2437–2446. Association for Computa-\ntional Linguistics.\nDingmin Wang, Yan Song, Jing Li, Jialong Han, and\nHaisong Zhang. 2018. A hybrid approach to auto-\nmatic corpus generation for chinese spelling check.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, Brus-\nsels, Belgium, October 31 - November 4, 2018, pages\n2517–2527. Association for Computational Linguis-\ntics.\nDingmin Wang, Yi Tay, and Li Zhong. 2019.\nConfusionset-guided pointer networks for chinese\nspelling check. In Proceedings of the 57th Confer-\nence of the Association for Computational Linguis-\ntics, ACL 2019, Florence, Italy, July 28- August 2,\n2019, Volume 1: Long Papers , pages 5780–5785.\nAssociation for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nHongqiu Wu, Yongxiang Liu, Hanwen Shi, hai zhao,\nand Min Zhang. 2023. Toward adversarial training\non contextualized language representation. In The\nEleventh International Conference on Learning Rep-\nresentations.\n10752\nShih-Hung Wu, Chao-Lin Liu, and Lung-Hao Lee.\n2013. Chinese spelling check evaluation at SIGHAN\nbake-off 2013. In Proceedings of the Seventh\nSIGHAN Workshop on Chinese Language Processing,\nSIGHAN@IJCNLP 2013, Nagoya, Japan, October\n14-18, 2013, pages 35–42. Asian Federation of Natu-\nral Language Processing.\nHeng-Da Xu, Zhongli Li, Qingyu Zhou, Chao Li,\nZizhen Wang, Yunbo Cao, Heyan Huang, and Xian-\nLing Mao. 2021. Read, listen, and see: Leveraging\nmultimodal information helps chinese spell checking.\nIn Findings of the Association for Computational Lin-\nguistics: ACL/IJCNLP 2021, Online Event, August\n1-6, 2021, volume ACL/IJCNLP 2021 of Findings of\nACL, pages 716–728. Association for Computational\nLinguistics.\nRuiqing Zhang, Chao Pang, Chuanqiang Zhang, Shuo-\nhuan Wang, Zhongjun He, Yu Sun, Hua Wu, and\nHaifeng Wang. 2021. Correcting chinese spelling\nerrors with phonetic pre-training. In Findings\nof the Association for Computational Linguistics:\nACL/IJCNLP 2021, Online Event, August 1-6, 2021,\nvolume ACL/IJCNLP 2021 of Findings of ACL ,\npages 2250–2261. Association for Computational\nLinguistics.\nShaohua Zhang, Haoran Huang, Jicong Liu, and Hang\nLi. 2020. Spelling error correction with soft-masked\nBERT. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2020, Online, July 5-10, 2020, pages 882–890. Asso-\nciation for Computational Linguistics.\nZewei Zhao and Houfeng Wang. 2020. Maskgec: Im-\nproving neural grammatical error correction via dy-\nnamic masking. In The Thirty-Fourth AAAI Con-\nference on Artificial Intelligence, AAAI 2020, The\nThirty-Second Innovative Applications of Artificial\nIntelligence Conference, IAAI 2020, The Tenth AAAI\nSymposium on Educational Advances in Artificial In-\ntelligence, EAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 1226–1233. AAAI Press.\nChenxi Zhu, Ziqiang Ying, Boyu Zhang, and Feng Mao.\n2022. Mdcspell: A multi-task detector-corrector\nframework for chinese spelling correction. In Find-\nings of the Association for Computational Linguistics:\nACL 2022, Dublin, Ireland, May 22-27, 2022, pages\n1244–1253. Association for Computational Linguis-\ntics.\nA Derivation of Equation (1)\nLet the input sentence be X = (x1, ..., xn) and\noutput sentence be Y = ( y1, ..., yn). Given\nX, the BERT model predicts each element of Y\nseparately, namely computing P(yi|X) for i =\n1, 2, ..., n. Let x−i = (x1, ..., xi−1, xi+1, ..., xn),\nthen P(yi|X) =P(yi|xi, x−i). By Bayes Rule:\nP(yi|xi, x−i) = P(yi|x−i)P(xi|yi, x−i)\nP(xi|x−i)\nNE NPE SL NEC NEP\nGame 400 155 33.0 1.16 133\nEncyclopedia 3434 1712 39.8 1.28 1217\nContract 1026 474 40.1 1.19 331\nMedical care 2090 1053 39.3 1.33 674\nCar 3451 1762 43.6 1.35 1236\nNovel 6000 3014 36.3 1.13 5819\nNews 5892 2946 25.1 1.11 1963\nSIGHAN-15 1100 541 30.6 1.30 370\nTable 14: Data statistics for LEMON (NE: number of\nexamples, NPE: number of positive examples, SL: sen-\ntence length, NEC: number of error characters per ex-\nample, NEP: number of edit pairs). SIGHAN-15 refers\nto the SIGHAN-15 test set.\nNotice that P(xi|x−i) is a constant for varying\nyi, thus the left-hand side is proportional to the\nnumerator, namely\nP(yi|xi, x−i) ∝P(yi|x−i)P(xi|yi, x−i),\nwhich gives question (1).\nB LEMON\nChinese Spelling Correction (CSC) in recent years\nmakes a great stride, with many methods emerging\nand making impressive performances on general\nbenchmarks like SIGHAN-2015. However, an ul-\ntimate CSC system must be able to cope with di-\nverse domains and contexts simultaneously and of-\nfer appropriate error correction recommendations.\nWe find that the current well-trained models on a\nsingle-domain still suffer from poor performances\non multi-domain scenarios. The community is now\nin great need of another general benchmark to eval-\nuate and study the generalization ability of a CSC\nsystem. We thus present LEMON, a large-scale\nmulti-domain dataset with natural spelling errors.\nLEMON spans 7 domains, including game\n(GAM), encyclopedia (ENC), contract (COT), med-\nical care (MEC), car (CAR), novel (NOV), and\nnews (NEW). As opposed to prior work, where the\ntypos are deliberately created on correct sentences,\nLEMON consists of 23 thousand examples with\nnatural spelling errors picked from daily writing of\nhuman, which admittedly requires more annotation\nresources. Our idea is to stick close to the real\nhuman language distribution.\nLEMON contains a diverse collection of edit\npairs and context, e.g. some cases requiring the\ndomain-specific knowledge, some requiring the in-\nference. This section presents a more concrete look\nat the examples in LEMON. For each case, we are\n10753\ngoing to demonstrate the source sentence, target\nsentence (human annotated), as well as the model\nprediction. As it turns out, the current model can\nhardly address those challenging cases.\nCase 1: expertise (from MEC)\n•头孢过敏可以用大环类酯。 「SRC」\n•头孢过敏可以用大环内酯。 「TRG」\n•头孢过敏可以用大环类酯。 「BERT」\nA professional word 大环类酯(macrolides an-\ntibiotics) is misspelled here, which can be very hard\nto correct if the model is not exposed to specific\nknowledge during the training process.\nCase 2: referential inference (from MEC)\n•色盲眼镜是用于矫正色觉障碍的一种眼睛。\n「SRC」\n•色盲眼镜是用于矫正色觉障碍的一种眼镜。\n「TRG」\n•色盲眼镜是用于矫正色觉障碍的一种眼睛。\n「BERT」\n眼镜 (glasses) is misspelled to 眼睛 (eyes) here.\nWe notice that glasses is mentioned earlier in the\nsentence, which requires the model to make the\nassociation based on the global context, albeit this\nis easy for human.\nCase 3: unusual expression but globally cor-\nrect (from GAM)\n•但好像从没见人淘出过银两。 「SRC」\n•但好像从没见人淘出过银两。 「TRG」\n•但好像从没见人掏出过银两。 「BERT」\n淘出(find out) is rarely expressed compared to\n掏出 (take out). The model is inclined to miscor-\nrect those unusual expressions. Both find out coins\nand take out coins are correct expressions. Accord-\ning to the global context, however, we can know the\nbackground here is someone who digs for treasure.\nHence, it should be found out here.\nCase 4: fixed pair (from ENC)\n•可爱的动物共同构成了一副让人惊艳不已的\n画面。 「SRC」\n•可爱的动物共同构成了一幅让人惊艳不已的\n画面。 「TRG」\n•可爱的动物共同构成了一副让人惊艳不已的\n画面。 「BERT」\nSince one will use 一副a pair of with 画面\n(scene), it should be corrected to 一幅 (a picture\nof ) here. However, there is a long attributive that\nseparates them apart. The model fails to make it as\na result.\nCase 5: locally correct but globally incorrect\nexpression (from CAR)\n•发动机发生故障切记盲目拆检。 「SRC」\n•发动机发生故障切忌盲目拆检。 「TRG」\n•发动机发生故障切记盲目拆检。 「BERT」\n切记(remember) and 切忌(remember not) are\nantonyms and both of them are correct expressions.\nAccording to the global context, what it means here\nis not to do something. Hence, remember should\nbe corrected to remember not.\nWe can find that most of the cases here are\nexpertise-free, but rather require more or less con-\ntextual comprehension and inference. Unfortu-\nnately, the current model is still weak in inference,\nperhaps more contextualized CSC methods could\nbe developed in future study.\nCase 6: multiple typos (from COT)\n•由于上述原因试乙方无法履行保证时以方不\n承担责任。 「SRC」\n•由于上述原因使乙方无法履行保证时乙方不\n承担责任。 「TRG」\n•由于上述原因使乙方无法履行保证时以方不\n承担责任。 「BERT」\nThis case contains more than one errors.\n10754\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nThe Limitations section\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSec. 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSec. 5\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSec. 5\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSec. 5\nC □\u0013 Did you run computational experiments?\nSec. 2, 5\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSec. 5\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n10755\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSec. 2, 5\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nNot applicable. Left blank.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nSec. 3\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n10756"
}