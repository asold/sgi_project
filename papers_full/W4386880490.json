{
  "title": "Evaluating the strengths and weaknesses of large language models in answering neurophysiology questions",
  "url": "https://openalex.org/W4386880490",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4377616275",
      "name": "⦁\tHassan Shojaee-Mend",
      "affiliations": [
        "Gonabad University of Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2114819138",
      "name": "Reza Mohebbati",
      "affiliations": [
        "Gonabad University of Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2136641962",
      "name": "Mostafa Amiri",
      "affiliations": [
        "Gonabad University of Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2566432697",
      "name": "Alireza Atarodi",
      "affiliations": [
        "Gonabad University of Medical Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6854308750",
    "https://openalex.org/W6853544979",
    "https://openalex.org/W4386120650",
    "https://openalex.org/W4378387269",
    "https://openalex.org/W4382020836",
    "https://openalex.org/W4324129637",
    "https://openalex.org/W4321459182",
    "https://openalex.org/W4385900159",
    "https://openalex.org/W4378603221",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4385430748",
    "https://openalex.org/W2111761700",
    "https://openalex.org/W2327037637",
    "https://openalex.org/W6838865847",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4385430086",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4384264317",
    "https://openalex.org/W4384561707"
  ],
  "abstract": "<title>Abstract</title> <bold>Background: </bold>Large language models (LLMs), such as ChatGPT, Google's Bard, and Anthropic's Claude, demonstrate impressive natural language capabilities. Assessing their competence in specialized domains such as neurophysiology is important for determining their utility in research, education, and clinical applications. <bold>Objectives:</bold>This study evaluates and compares the performance of LLMs in answering neurophysiology questions in English and Persian across different topics and cognitive levels. <bold>Methods:</bold>Twenty questions spanning 4 topics (general, sensory system, motor system, and integrative) and 2 cognitive levels (lower-order and higher-order) were presented to the LLMs. Physiologists scored the essay-style responses from 0-5 points. Statistical analysis compared the scores at themodel, language, topic, and cognitive levels. <bold>Results:</bold>Overall,the models performed well (mean score=3.56/5), with no significant difference between language or cognitive levels. Performance was the strongest in themotor system (mean=4.52) and the weakest in integrative topics (mean=2.1). Detailed qualitative analysis revealed inconsistencies and gaps in reasoning. <bold>Conclusions:</bold> Thisstudy provides insights into LLMs’ capabilities and limitations in neurophysiology. The models exhibit competence in fundamental concepts but face challenges in advanced reasoning and integration. Targeted training could address gaps in knowledge and causal reasoning. As LLMs evolve, rigorous domain-specific assessments will be important to gauge progress.",
  "full_text": "Page 1/17\nEvaluating the strengths and weaknesses of large\nlanguage models in answering neurophysiology\nquestions\nHassan Shojaee-Mend \nGonabad University of Medical Sciences\nReza Mohebbati \nGonabad University of Medical Sciences\nMostafa Amiri \nGonabad University of Medical Sciences\nAlireza Atarodi \nGonabad University of Medical Sciences\nArticle\nKeywords: large language models, neurophysiology, evaluation, Bloom's taxonomy\nPosted Date: September 21st, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-3348418/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nVersion of Record: A version of this preprint was published at Scienti\u0000c Reports on May 11th, 2024. See\nthe published version at https://doi.org/10.1038/s41598-024-60405-y.\nPage 2/17\nAbstract\nBackground: Large language models (LLMs), such as ChatGPT, Google's Bard, and Anthropic's Claude,\ndemonstrate impressive natural language capabilities. Assessing their competence in specialized\ndomains such as neurophysiology is important for determining their utility in research, education, and\nclinical applications.\nObjectives:This study evaluates and compares the performance of LLMs in answering neurophysiology\nquestions in English and Persian across different topics and cognitive levels.\nMethods:Twenty questions spanning 4 topics (general, sensory system, motor system, and integrative)\nand 2 cognitive levels (lower-order and higher-order) were presented to the LLMs. Physiologists scored\nthe essay-style responses from 0-5 points. Statistical analysis compared the scores at themodel,\nlanguage, topic, and cognitive levels.\nResults:Overall,the models performed well (mean score=3.56/5), with no signi\u0000cant difference between\nlanguage or cognitive levels. Performance was the strongest in themotor system (mean=4.52) and the\nweakest in integrative topics (mean=2.1). Detailed qualitative analysis revealed inconsistencies and gaps\nin reasoning.\nConclusions: Thisstudy provides insights into LLMs’ capabilities and limitations in neurophysiology. The\nmodels exhibit competence in fundamental concepts but face challenges in advanced reasoning and\nintegration. Targeted training could address gaps in knowledge and causal reasoning. As LLMs evolve,\nrigorous domain-speci\u0000c assessments will be important to gauge progress.\nIntroduction\nThe world is undergoing tremendous changes, with new tools and technology permeating every corner.\nPeople are shocked, contemplating the pros and cons and wondering how these advancements will\naffect us. Can we trust them? To \u0000nd answers, researchers are delving into different approaches. They\nenter arti\u0000cial intelligence (AI), a captivating and signi\u0000cant phenomenon of our time, with versatile\ncapabilities applicable to a wide range of tasks. Recently, there have been remarkable advancements in\nnatural language processing (NLP). This progress has given rise to sophisticated large language models\n(LLMs) that can engage with humans in a remarkably human-like manner. In particular, chatbot\nplatforms have made strides, providing accurate and contextually appropriate responses to users’\nqueries 1. As this momentum continues, there is an increasing need for reliable and e\u0000cient question-\nanswering systems in specialized domains, such as neurophysiology.\nRapid progress in conversational AI has given rise to advanced language models capable of generating\nhumanlike texts. With their wide range of functionalities, such as humanlike response generation,\npro\u0000ciency in professional exams, complex problem-solving, and more, these models have captivated\ninterest 2. LLMs are gaining increasing popularity in both academia and industry owing to their\nPage 3/17\nunprecedented performance in various applications. As LLMs continue to play a vital role in both\nresearch and daily use, their evaluation becomes increasingly critical, not only at the task level but also\nat the society level for a better understanding of their potential risks. Over the past years, signi\u0000cant\nefforts have been made to examine LLMs from various perspectives 3.\nWith the popularization of software such as OpenAI’s ChatGPT, Google’s Bard and Anthropic's Claude,\nLLMs have pervaded many aspects of life and work. They can be used to provide customized recipes,\nsuggesting substitutions for missing ingredients. It can be used to draft research proposals, write\nworking code in many programming languages, translate text between languages, assist in policy\nmaking, and more. Users interact with LLMs through “prompts” or natural language instructions.\nCarefully designed prompts can lead to signi\u0000cantly better outputs 4. These models, which mimic human\nintelligence, utilize statistical analyses to understand patterns and connections between words and\nphrases 1.\nNeurophysiology, a key branch of neuroscience, is dedicated to unraveling the complex mechanisms\nunderlying the functioning of the nervous system. Investigating neurophysiological phenomena requires\na profound understanding of various concepts, theories, and experimental methodologies. Consequently,\nhaving a highly competent question-answering system that can address neurophysiology queries is of\nutmost importance to researchers, clinicians, and students in this \u0000eld. In the question-answering\nsystem, questions can be divided into two categories, lower-order and higher-order questions, based on\nBloom's taxonomy, and the ability of language models to answer the questions of each of the categories\ncan be evaluated. Bloom's taxonomy, a framework widely used in educational settings, categorizes\ncognitive levels into six domains: knowledge, comprehension, application, analysis, synthesis, and\nevaluation 5. By applying Bloom's taxonomy in the evaluation of LLMs, their performance in answering\nquestions across different cognitive levels, including neurophysiology, can be assessed 6. By considering\nhow well ChatGPT, Bard, and Claude perform at different topics and different levels of Bloom's taxonomy,\ntheir abilities to comprehensively and accurately answer neurophysiology questions can be determined.\nPrevious publications evaluating LLMs in various \u0000elds included, for example, gastroenterology 7,\npathology 8, and solving case vignettes in physiology 9. Most studies have investigated the ability of\nLLMs to provide correct answers for multiple-choice questions 10–12. It seems that to \u0000nd the strengths\nand weaknesses of these models in different \u0000elds, it is necessary to examine the capabilities of these\nmodels in answering essay questions, including questions from all cognitive levels. Neurophysiology is\none of the areas where questions can be asked at different levels, and its examination can be useful in\nidentifying the strengths and weaknesses of LLMs.\nThis study compares the performance of three language models, namely, ChatGPT, Bard, and Claude, in\nanswering neurophysiology questions in both the Persian and English languages. It focuses on different\ncognitive levels based on Bloom's Taxonomy and evaluates the models' reasoning process by asking for\nthe rationale behind their answers. The aim is to determine whether the models rely on memorization or\ndemonstrate analytical reasoning and logical explanations. The study provides insights into the\nPage 4/17\ncapabilities and limitations of the models in neurophysiology and their integration into medical education\nand clinical decision-making. The manuscript assesses and compares the performance of the models\nand discusses their potential applications in research, education, and clinical practice.\nMethodology\nThis exploratory, applicational and cross-sectional study was carried out using AI-driven chat\napplications, including ChatGPT (chat.openai.com), Claude (claude.ai), and Bard (bard.google.com),\nwhich provide free services for research. The researchers aim to assess the strengths and weaknesses\nof the selected LLMs in their ability to answer neurophysiology questions.\nQuestions\nA total of 20 questions were selected from four topics in neurophysiology, including general, sensory,\nmotor, and integrative systems. Each topic included 5 questions. For all the questions that included\ntrue/false, multiple-choice, and essay questions, the LLMs were requested to explain the reason for the\nchosen answer. Therefore, all the questions were actually essay questions so that a score from 0–5\npoints could be assigned to the answers. Furthermore, the questions were categorized by cognitive skills\n(lower-order and higher-order skills). Each topic included 3 lower-order questions and 2 higher-order\nquestions. It is noteworthy to mention that based on Bloom’s taxonomy, memorization and recall are\ncategorized as lower-level cognitive skills, demanding only a minimal degree of comprehension. In\ncontrast, the application of knowledge and critical thinking fall under the category of higher-level\ncognitive skills, requiring deep conceptual understating 13. An expert physiologist with experience in\nteaching and research designed and con\u0000rmed the validity of the questions. The questions along with\nthe topics and cognitive skills are listed in Supplementary Table 1.\nData collection\nThe most recent versions of ChatGPT 3.5 (August 3, 2023), Claude 2 (August 28, 2023), and Bard (July\n13, 2023) were prompted with questions in both the Persian and English languages. These versions are\nbeing tested by the public for academic study. The Persian and English questions, along with the\nanswers generated by three selected LLMs, were stored in separate \u0000les for evaluation by the\nphysiologists. Like a student answering neurophysiology questions, each question was given to the\nLLMS only once, and the answer received was considered the \u0000nal answer of the models.\nA group of three skilled physiologists were selected to evaluate and score each of the questions\nindividually. They were university lecturers who had at least two years of teaching experience in\nneurophysiology to medical students. The physiologists were asked to score each question from 0 to 5\npoints, where a score of 5 represented a full and comprehensive response to the question. All data were\nstored in an Excel \u0000le for further analysis.\nStatistical analysis\nPage 5/17\nThe mean, median, and standard deviation were used to provide a statistical overview of the data. The\nFriedman test was used to assess whether there were statistically signi\u0000cant differences in the scores of\nLLMs regarding Persian and English languages, with each group having 20 questions. Furthermore, the\nKruskal ‒ Wallis test was carried out to assess the signi\u0000cance of the difference in scores across four\ntopics and two levels of cognitive skills. The intraclass correlation coe\u0000cient (ICC), a two-way random\nmodel with absolute agreement, 14 was used to evaluate the level of agreement between the\nphysiologists' scores. Moreover, the Wilcoxon signed rank test was used to evaluate the signi\u0000cant\ndifference between the scores of LLMs in Persian and English. A p value of less than 0.05 was\nconsidered statistically signi\u0000cant. All statistical analyses were performed using SPSS software, version\n22.\nResults\nThe answers of the three LLMs, ChatGPT, Bard, and Claude, were obtained in the Persian and English\nlanguages. Three experienced physiologists evaluated the answers. Like a student answering\nneurophysiology questions, each question was given to the LLMs only once. As a result, the ambiguity in\nthe questions or the lack of understanding of the LLMs of the importance of the content of the answer or\nthe unimportant content that should not be mentioned in the answer may affect the scores that the\nLLMs received from each question. The evaluation results of the experts along with the average of their\nscores are shown in Supplementary Table 2.\nThe evaluation results with ICC showed good agreement among the physiologists in the scores. The ICC\nvalues for different topics ranged from 0.935 to 0.993. The ICC value for all questions was 0.985 (F = \n71.428, p < 0.001). This high level of agreement between the physiologists' scores can be considered the\nreliability of expert opinions. The results of the ICC test between the physiologists are shown in Table 1.\nTable 1\n– Interrater agreement among physiologists\nTopic N Intraclass correlation 95% Con\u0000dence interval Sig\nLower boundUpper bound\nGeneral 30 0.993 0.987 0.996 < 0.001\nSensory system 30 0.978 0.958 0.989 < 0.001\nMotor system 30 0.935 0.882 0.967 < 0.001\nIntegrative 30 0.988 0.978 0.994 < 0.001\nTotal 120 0.985 0.980 0.989 < 0.001\nGiven the good agreement between the raters, the mean of their scores was used as the score for each\nquestion in the subsequent analysis. The evaluation results of the physiologists showed that the overall\nperformance of selected LLMs in answering the questions as well as the performance of each of the\nPage 6/17\nLLMs in both English and Persian languages were satisfactory (Table 2). The overall mean obtained for\nthe questions was 3.56 ± 2.02. As shown in Fig. 1, the mean scores for different LLMs in the Persian and\nEnglish languages ranged from 3.17 (Claude in Persian) to 4.02 (Bard in English). However, the results of\nthe Friedman test did not show any statistically signi\u0000cant difference between LLMs scores in Persian\n(p = 0.204) and English (p = 0.518). Overall, the average scores in English (Mean = 3.79, Median = 4.89)\nwere better than those in Persian (Mean = 3.32, Median = 4.67). However, the Wilcoxon signed rank test\nshowed that this difference was not statistically signi\u0000cant (p = 139).\nTable 2\nMean ± SD (Median) of scores by language, topic, and LLM\n  General Sensorysystem Motorsystem Integrative Total\nEnglish Bard 4 ± 2.24 (5) 4.67 ± 0.75(5) 4.87 ± 0.3(5) 2.53 ± 2.38(2.5) 4.02 ± 1.81(5)\nChatGPT 4.2 ± 1.79(5) 3.73 ± 2.17(5) 3.2 ± 2.36(4.67) 2.63 ± 2.29(2.17) 3.44 ± 2.07(5)\nClaude 3.87 ± 2.17(4.67) 4.47 ± 1.02(5) 4.73 ± 0.43(5) 2.63 ± 2.39(3) 3.93 ± 1.77(4.75)\nPersian Bard 4 ± 2.24 (5) 2.8 ± 2.43 (4)4.8 ± 1.83(4.67) 1.5 ± 2.15(0.33) 3.28 ± 2.22(4.67)\nChatGPT 4.07 ± 2.09(5) 3.1 ± 2.22(4.33) 4.93 ± 0.15(5) 2 ± 2.46(0.67) 3.53 ± 2.12(4.83)\nClaude 3.93 ± 2.2(5) 2.9 ± 2.44(4.33) 4.6 ± 0.29(4.67) 1.23 ± 2.03(0) 3.17 ± 2.02(4.5)\nTotal 4.01 ± 1.94(5) 3.61 ± 1.94(4.67) 4.52 ± 1.1(5) 2.1 ± 2.15(1.25) 2.08 ± 2.16(1.25)\nRegarding different topics, the highest scores were related to the motor system topic. Additionally, the\nlowest score was obtained for the integrative topic (Table 2). Based on the results, the performance of\nLLMs can be generally evaluated as excellent for general and motor system topics, good for sensory\nsystem, and weak for integrative. The best scores for the Persian questions were for the motor system\ntopic, and the weakest scores for the English questions were for the integrative topic (Fig. 2). The results\nof the Kruskal ‒ Wallis test showed a signi\u0000cant difference between the scores of the integrative topic\ncompared to other topics (p < 0.001).\nMoreover, regarding the cognitive level of the questions, the results of the Kruskal ‒ Wallis test showed\nthat there was no signi\u0000cant difference between the scores (p = 0.885). The lowest score of 3.08 was for\nlower-order questions in Persian, and the highest score of 3.9 was for lower-order questions in English\n(Fig. 3).\nPage 7/17\nFigures 4 and 5 show the mean scores for different questions in the Persian and English languages.\nWhen the curves are closer together, it indicates the closeness of scores in different LLMs. Additionally,\nthe closer the curves are to the outer edge of the diagram, the higher the score obtained for that question\nis. The diagrams show that for most questions, there is approximately the same capability between\ndifferent LLMs. However, this similarity does not exist for some questions. For example, in the Persian\nquestions, for the Integrative_4 question, ChatGPT was able to give a completely correct answer and\nreceived a score of 5, Bard received a score of approximately 2, and Claude received a score of zero. In\naddition, for Sensory_4, the scores of ChatGPT and Bard were fairly good, but Claude was unable to\nanswer the question. In contrast, for Sensory_1, both ChatGPT and Bard were unable to respond, but\nClaude was able to receive an almost perfect score for the question (Fig. 4).\nFor English questions, there are also questions where there is no similarity in capability among LLMs. For\nexample, both Bard and Claude received full scores for Sensory_4 and Motor_1, but ChatGPT was unable\nto give correct answers to these questions (Fig. 5).\nIn addition to this lack of consistency in responses, in some questions, almost none of the LLMs were\nable to adequately respond to the question. For further analysis, the questions that LLMs were unable to\nadequately respond to were selected. The total scores of the three language models for each question in\nPersian and English were 15. A mean score of less than 2 for each LLM was considered the criterion for\nselecting questions. Therefore, questions for which the total LLMs scores were less than 6 were\nselected. In Persian, these questions included General_5, Sensory_1, Sensory_3, Integrative_1,\nIntegrative_2 and Integrative_3 questions. Additionally, for the English questions, the total score was less\nthan 6 for the General_5, Integrative_1 and Integrative_2 questions.\nGeneral_5 question: Is myelination of postganglionic\nsympathetic \u0000bers done by Schwann cells? Why?\nThe correct answer to this question is that postganglionic sympathetic \u0000bers do not have myelin. The\nuse of the phrase \"by Schwann cells\" in the question stem is a misleading phrase. In the Persian\nlanguage, none of the language models could give the correct answer even after removing the\nmisleading phrase from the question. With more questions, it became clear that in Persian,\npostganglionic sympathetic \u0000bers are considered to be of type A, when in fact they are of type C. Hence,\nthe cause of the wrong answer in the Persian language can be considered as \"having wrong information\"\nin the LLMs, but by removing the misleading phrase from the question, all LLMs were able to give the\ncorrect answer in the English language. Therefore, the cause of the initial wrong answer in English can be\nconsidered a \"misleading phrase in the question\".\nSensory_1 question: Is the sensation of cold, like acute pain and unlike heat sensation, transmitted\nthrough myelinated A δ  (A delta) \u0000bers? Why?\nPage 8/17\nThe correct answer to this question is \"Yes\". Only the Bard and ChatGPT models in Persian could not give\nthe correct answer. All of them answered this question correctly in English. The Bard and ChatGPT in\nPersian stated that cold sensation is transmitted to the central nervous system via C-type nerve \u0000bers\nand not A δ  (A delta), which is incorrect. Therefore, the cause of the wrong answer in the Persian\nlanguage in these LLMs can be considered as \"having wrong information\".\nSensory_3 question: State the most important nuclei and\nneurotransmitters of the CNS analgesic pathway. Why?\nThe correct answer is “the PAG projects enkephalinergic neurons to the Raphe, and after stimulation, the\nserotonergic projections go to the spine and stimulate the enkephalinergic neurons that cause pain\ninhibition”. In response to this essay question, the LLMs did not mention some important nuclei or\nmentioned nuclei that were of little importance. This means that the most important phrase in the\nquestion was not considered. This lack of attention to importance was present in both the Persian and\nEnglish languages, but it was more intense in Persian. Thus, the cause of the wrong answer to this\nquestion can be stated as \"not considering the importance and priority\" and \"insigni\u0000cant additional\nexplanation\" compared to a person who has information in this \u0000eld.\nIntegrative_1 question: In medical science and neurophysiology, is knowing \"my birthday is January 10,\n1998\" an example of semantic explicit memory? Why?\nThe correct answer is \"No\". Because stating my birthday date is only a claim about a past event, it can be\nconsidered a veri\u0000ed fact if there is evidence that con\u0000rms that event. None of the LLMs could give the\ncorrect answer either in Persian or English. They took this statement as a fact. Most likely, the reason for\nthat is the lack of a sentence similar to this sentence in the previous texts that were used to train the\nLLMs. Therefore, the reason for the wrong answer to this question can be considered \"using nonexistent\nexample\" and \"lack of reasoning power\" for questions that require reasoning based on previous\nknowledge and applying that knowledge to the current situation.\nIntegrative_2 question: In dementia, which neurons are\nmore damaged? Why?\nThe correct answer is cholinergic neurons that release the neurotransmitter acetylcholine and GABAergic\nneurons that mainly release GABA, which can disrupt the balance of various neurotransmitters, including\nglutamate and gamma-aminobutyric acid.\nAccording to the question, because the reason for more damage in neurons was asked, the LLMs should\nhave mentioned neurotransmitters in the answer, but none of them mentioned them in the \u0000rst answer to\nthe question and wrote additional and redundant explanations in the answer. Of course, for the English\nlanguage, after asking the neurotransmitters again, the LLMs were able to point to them correctly, but\nagain, even after asking the neurotransmitters again in Persian, the answers were still incomplete.\nHence, the cause of the wrong answer in the Persian language can be considered as \"having wrong\nPage 9/17\ninformation\" and \"ambiguity in the question\". The LLMs for the English language had enough information\nfor this question, but the reason for the initial incomplete answer can be considered \"ambiguity in the\nquestion\" and \"not considering the importance and priority\" in answering because a person who has\ninformation in this \u0000eld should point to the complete answer in the \u0000rst question without additional\nexplanations.\nIntegrative_3 question: Do X cells have the highest\nabundance among retinal ganglion cells? Why?\nThe correct answer is \"No\". The most abundant retinal ganglion cells are midgut cells, which make up\napproximately 80% of all retinal ganglion cells. X cells, along with Y cells, are a broad class of retinal\nganglion cells. They make up approximately 10% of retinal ganglion cells. LLMs in English answered this\nquestion correctly, but LLMs in Persian incorrectly answered this question. It seems that since in Persian,\nthe material in this regard mostly includes the translation of Guyton's book 15, and in this book, the most\nabundant cells in the retinal ganglion cells are considered to be X cells, the answer of LLMs in Persian to\nthis question is yes. Therefore, the cause of the wrong answer for the Persian language can be\nconsidered as \"having wrong information\".\nDiscussion\nThree LLMs, ChatGPT, Bard, and Claude, were used to evaluate the ability to provide comprehensive and\nlogical answers to neurophysiology essay questions in Persian and English. These LLMs can respond to\ncomplex commands by analyzing and comprehending the supplied text, utilizing their highly developed\nnatural language processing capabilities and their extensive training data 8. The results showed that,\noverall, the models demonstrated good capabilities in responding to neurophysiology questions.\nNevertheless, some differences between the models were observed depending on the topic of the\nquestions.\nAmong the different topics examined, the LLMs performed the best on questions related to the motor\nsystem and general neurophysiology, indicating their strength in addressing fundamental concepts. For\nsensory system topics, the performance was moderately good, suggesting that the models can\ncomprehend and explain sensory neurophysiology to some extent. However, for integrative questions,\nthe scores were markedly lower. This highlights a current limitation of the models in tackling complex,\nmultistep reasoning requiring integration of knowledge across neurophysiology topics. Targeted training\non integrative concepts could help improve LLMs’ capabilities in this aspect 16.\nInterestingly, while there was no signi\u0000cant difference in the scores of the models in Persian and English\nor between lower-order and higher-order questions, a detailed analysis revealed some inconsistencies. A\nqualitative analysis of the responses revealed some gaps in reasoning abilities, especially for unfamiliar\nquestion scenarios that demand \u0000exible application of knowledge. For certain questions, one model\nperformed well, while others did not, with no clear pattern. This lack of consistency suggests knowledge\nPage 10/17\ngaps and variations in the training of the different models 17. Additionally, all three models struggled with\nseveral complex questions in both languages, scoring poorly. This further highlights the limitations of\nthese models in advanced reasoning and dealing with ambiguous and multifaceted questions.\nWhen comparing languages, the scores were largely similar between Persian and English for all the\nLLMs. The models appear to have acquired su\u0000cient linguistic knowledge to comprehend and respond\naccurately in both languages. However, a few wrong answers unique to Persian highlighted gaps in the\ninformation encoded in the models for that language. Overall, the results validate the usefulness of the\nLLMs for addressing neurophysiology questions in multiple languages.\nAn in-depth review of the incorrect answers sheds light on the speci\u0000c limitations of the LLMs. Providing\n\u0000awed information and the inability to discern key aspects of questions emerged as some of the\nde\u0000ciencies. However, some studies have reported a good reasoning level for LLMs 18, and a lack of\nreasoning for unfamiliar examples emerged as one of the de\u0000ciencies in providing correct answers to\nseveral questions. These gaps need to be addressed through more extensive training of the models\nusing high-quality data covering diverse topics, contexts, and linguistic expressions related to\nneurophysiology. The lower performance on integrative questions can be attributed to the models'\nreliance on memorization and pattern recognition from the training data rather than a deeper\nunderstanding of the concepts. Although large datasets help them to remember facts and terminology, it\nis still di\u0000cult for them to integrate knowledge across topics to solve new problems. Furthermore,\nreasoning on unstructured, open-ended neurophysiology questions requires models to go beyond their\ntraining methods. Additional training focused on building causal models of physiology, rather than\nstatistical associations, could address this issue.\nThe results of Mahowald et al. 19 and Tuckute et al. 20 are also consistent with the results we found in\nour study. They mentioned that LLMs excel in formal language abilities but have limitations in real-world\nlanguage understanding and cognitive skills. They lack reasoning, world knowledge, situation modeling,\nand social cognition 19,20. Moreover, Schubert et al. concluded that higher-order cognitive tasks proved\nmore challenging for both GPT-4 and GPT-3.5 21. Some results express a little cautiously in these cases\nand express their opinions such as Puchert et al., LLMs have transformed natural language processing\nand shown remarkable abilities. However, they are prone to hallucinations, providing inaccurate\ninformation in their responses. To ensure accurate evaluation, diligent assessment methods are crucial.\nEvaluations of LLM performance in speci\u0000c knowledge domains, based on question-and-answer\ndatasets, often rely on a single accuracy metric for the entire \u0000eld, which hampers transparency and\nmodel enhancement 22. Loconte et al. claimed that while ChatGPT was well known to exhibit\noutstanding performance on generative linguistic tasks, its performance on prefrontal tests was\ninhomogeneous, as they reached the results, with some tests well above average, others in the lower\nrange, and others frankly impaired 23.\nOverall, the study \u0000ndings demonstrate that LLMs such as ChatGPT, Bard, and Claude have achieved\nimpressive competence in answering neurophysiology questions but still face challenges in some\nPage 11/17\naspects of knowledge application, reasoning, and integration. It is clear that the way the models work\nshould be improved, especially to answer complex and ambiguous questions that require multistep\nreasoning and integration of knowledge across different topics. The variability between models also\nhighlights the need for continued evaluation. As LLMs continue to evolve, rigorous assessment across\ndifferent knowledge domains will be crucial.\nConclusion\nThis study provides insights into the capabilities of LLMs in answering neurophysiology questions. The\nresults indicate that ChatGPT, Bard, and Claude can successfully address many fundamental concepts\nbut face limitations in more advanced reasoning and integration of knowledge across topics.\nOverall, the models demonstrated relatively strong performance on general neurophysiology and motor\nsystem questions and moderate capability in sensory neurophysiology but struggled with integrative\nquestions requiring multistep inference. There was no signi\u0000cant difference between languages or\ncognitive levels; however, qualitative analysis revealed inconsistencies and gaps, indicating that the\nmodels rely heavily on memorization rather than deeper conceptual understanding.\nThe incorrect responses highlight de\u0000ciencies in reasoning, discerning key information, and dealing with\nunfamiliar questions. Targeted training on causal physiologic models rather than statistical associations\ncould address these limitations. As LLMs continue to evolve, rigorous multidisciplinary assessments will\nbe essential to gauge progress.\nThis study provides a robust evaluation methodology and benchmark for future work on enhancing the\nneurophysiology knowledge and reasoning competence of these models. The insights can inform efforts\nto improve LLMs through advanced training techniques and evaluation of complex integrative tasks.\nWith focused improvements, these models hold immense promise in advancing neurophysiology\neducation, research, and clinical practice.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAvailability of data and materials\nThe authors declare that there is no relevant data available for this study. All data used in the analysis\nand preparation of this manuscript have been included in the manuscript.\nPage 12/17\nAuthors' contributions\nH.S. and R.M. designed and performed the research and wrote the paper; H.S., R.M., M.A., and A.A.\ncontributed to the analysis and revised the paper critically. All authors approved the version to be\npublished.\nReferences\n1. Thirunavukarasu, A. J. et al. Large language models in medicine. Nature Medicine, 1–11 (2023).\n2. Ahmed, I. et al. ChatGPT vs. Bard: A Comparative Study. UMBC Student Collection (2023).\n3. Tang, L. et al. Evaluating large language models on medical evidence summarization. npj Digital\nMedicine 6, 158 (2023).\n4. Lim, S. & Schmälzle, R. Arti\u0000cial intelligence for health message generation: an empirical study using\na large language model (LLM) and prompt engineering. Frontiers in Communication 8, 1129082\n(2023).\n5. Rakhmonova, S. & Rakhmatov, B. BLOOM’S TAXIONOMY AND DIDACTIC SIGNIFICANCE OF\nCRITICAL THINKING METHOD IN THE EDUCATIONAL PROCESS. Innovative Development in\nEducational Activities 2, 94–98 (2023).\n\u0000. Agarwal, M., Sharma, P. & Goswami, A. Analysing the applicability of ChatGPT, Bard, and Bing to\ngenerate reasoning-based multiple-choice questions in medical physiology. Cureus 15 (2023).\n7. Lahat, A. et al. Evaluating the use of large language model in identifying top research questions in\ngastroenterology. Scienti\u0000c Reports 13, 4164, doi:10.1038/s41598-023-31412-2 (2023).\n\u0000. Sinha, R. K., Deb Roy, A., Kumar, N. & Mondal, H. Applicability of ChatGPT in Assisting to Solve\nHigher Order Problems in Pathology. Cureus 15, e35237, doi:10.7759/cureus.35237 (2023).\n9. Dhanvijay, A. K. D. et al. Performance of large language models (ChatGPT, Bing Search, and Google\nBard) in solving case vignettes in physiology. Cureus 15 (2023).\n10. Duong, D. & Solomon, B. D. Analysis of large-language model versus human performance for\ngenetics questions. European Journal of Human Genetics, doi:10.1038/s41431-023-01396-8 (2023).\n11. Gilson, A. et al. How Does ChatGPT Perform on the United States Medical Licensing Examination?\nThe Implications of Large Language Models for Medical Education and Knowledge Assessment.\nJMIR Med Educ 9, e45312, doi:10.2196/45312 (2023).\n12. Khorshidi, H. et al. Application of ChatGPT in multilingual medical education: How does ChatGPT\nfare in 2023's Iranian residency entrance examination. Informatics in Medicine Unlocked 41, 101314\n(2023).\n13. Crowe, A., Dirks, C. & Wenderoth, M. P. Biology in bloom: implementing Bloom's taxonomy to\nenhance student learning in biology. CBE—Life Sciences Education 7, 368–381 (2008).\n14. Koo, T. K. & Li, M. Y. A Guideline of Selecting and Reporting Intraclass Correlation Coe\u0000cients for\nReliability Research. Journal of Chiropractic Medicine 15, 155–163,\nPage 13/17\ndoi:https://doi.org/10.1016/j.jcm.2016.02.012 (2016).\n15. Hall, J. E. & Hall, M. E. Guyton and Hall textbook of medical physiology e-Book. (Elsevier Health\nSciences, 2020).\n1\u0000. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y. & Iwasawa, Y. Large language models are zero-shot\nreasoners. Advances in neural information processing systems 35, 22199–22213 (2022).\n17. Singhal, K. et al. Large language models encode clinical knowledge. Nature 620, 172–180,\ndoi:10.1038/s41586-023-06291-2 (2023).\n1\u0000. Webb, T., Holyoak, K. J. & Lu, H. Emergent analogical reasoning in large language models. Nature\nHuman Behaviour, doi:10.1038/s41562-023-01659-w (2023).\n19. Mahowald, K. et al. Dissociating language and thought in large language models: a cognitive\nperspective. arXiv preprint arXiv:2301.06627 (2023).\n20. Tuckute, G. et al. Driving and suppressing the human language network using large language\nmodels. bioRxiv, 2023.2004. 2016.537080 (2023).\n21. Schubert, M. C., Wick, W. & Venkataramani, V. Evaluating the Performance of Large Language\nModels on a Neurology Board-Style Examination. medRxiv, 2023.2007. 2013.23292598 (2023).\n22. Puchert, P., Poonam, P., van Onzenoodt, C. & Ropinski, T. LLMMaps–A Visual Metaphor for Strati\u0000ed\nEvaluation of Large Language Models. arXiv preprint arXiv:2304.00457 (2023).\n23. Loconte, R., Orrù, G., Tribastone, M., Pietrini, P. & Sartori, G. Challenging ChatGPT'Intelligence'with\nHuman Tools: A Neuropsychological Investigation on Prefrontal Functioning of a Large Language\nModel. Intelligence (2023).\nFigures\nPage 14/17\nFigure 1\nMean scores for all LLMs in Persian and English\nFigure 2\nPage 15/17\nMean scores for LLMs in each topic and language\nFigure 3\nMean scores for cognitive skills in Persian and English\nPage 16/17\nFigure 4\nScores of LLMs to Persian questions\nPage 17/17\nFigure 5\nScores of LLMs to English questions\nSupplementary Files\nThis is a list of supplementary \u0000les associated with this preprint. Click to download.\nSupplememtary.docx",
  "topic": "Strengths and weaknesses",
  "concepts": [
    {
      "name": "Strengths and weaknesses",
      "score": 0.8178597688674927
    },
    {
      "name": "Neurophysiology",
      "score": 0.5121874213218689
    },
    {
      "name": "Question answering",
      "score": 0.4690433144569397
    },
    {
      "name": "Computer science",
      "score": 0.4360499680042267
    },
    {
      "name": "Psychology",
      "score": 0.4098752737045288
    },
    {
      "name": "Cognitive science",
      "score": 0.3753587007522583
    },
    {
      "name": "Linguistics",
      "score": 0.34893497824668884
    },
    {
      "name": "Natural language processing",
      "score": 0.32748764753341675
    },
    {
      "name": "Neuroscience",
      "score": 0.1774716079235077
    },
    {
      "name": "Philosophy",
      "score": 0.1560370922088623
    },
    {
      "name": "Social psychology",
      "score": 0.10014799237251282
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210097772",
      "name": "Gonabad University of Medical Sciences",
      "country": "IR"
    }
  ],
  "cited_by": 3
}