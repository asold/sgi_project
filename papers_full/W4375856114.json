{
    "title": "Designing highly potent compounds using a chemical language model",
    "url": "https://openalex.org/W4375856114",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2775508793",
            "name": "Hengwei Chen",
            "affiliations": [
                "University of Bonn"
            ]
        },
        {
            "id": "https://openalex.org/A192274276",
            "name": "Jürgen Bajorath",
            "affiliations": [
                "University of Bonn"
            ]
        },
        {
            "id": "https://openalex.org/A2775508793",
            "name": "Hengwei Chen",
            "affiliations": [
                "University of Bonn"
            ]
        },
        {
            "id": "https://openalex.org/A192274276",
            "name": "Jürgen Bajorath",
            "affiliations": [
                "University of Bonn"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1908851533",
        "https://openalex.org/W2056881083",
        "https://openalex.org/W1973974137",
        "https://openalex.org/W2578119541",
        "https://openalex.org/W2073021822",
        "https://openalex.org/W2893435209",
        "https://openalex.org/W2952635193",
        "https://openalex.org/W2738737407",
        "https://openalex.org/W2937307539",
        "https://openalex.org/W1964357740",
        "https://openalex.org/W2894887848",
        "https://openalex.org/W2895884529",
        "https://openalex.org/W3113447514",
        "https://openalex.org/W3185391990",
        "https://openalex.org/W4311436943",
        "https://openalex.org/W2970175280",
        "https://openalex.org/W4312810284",
        "https://openalex.org/W1975875968",
        "https://openalex.org/W2911155903",
        "https://openalex.org/W2497919018",
        "https://openalex.org/W1991286793",
        "https://openalex.org/W1975147762",
        "https://openalex.org/W3100358278",
        "https://openalex.org/W4220802400",
        "https://openalex.org/W4362664882",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2970971581"
    ],
    "abstract": "Abstract Compound potency prediction is a major task in medicinal chemistry and drug design. Inspired by the concept of activity cliffs (which encode large differences in potency between similar active compounds), we have devised a new methodology for predicting potent compounds from weakly potent input molecules. Therefore, a chemical language model was implemented consisting of a conditional transformer architecture for compound design guided by observed potency differences. The model was evaluated using a newly generated compound test system enabling a rigorous assessment of its performance. It was shown to predict known potent compounds from different activity classes not encountered during training. Moreover, the model was capable of creating highly potent compounds that were structurally distinct from input molecules. It also produced many novel candidate compounds not included in test sets. Taken together, the findings confirmed the ability of the new methodology to generate structurally diverse highly potent compounds.",
    "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2023) 13:7412  | https://doi.org/10.1038/s41598-023-34683-x\nwww.nature.com/scientificreports\nDesigning highly potent \ncompounds using a chemical \nlanguage model\nHengwei Chen  & Jürgen Bajorath *\nCompound potency prediction is a major task in medicinal chemistry and drug design. Inspired \nby the concept of activity cliffs (which encode large differences in potency between similar active \ncompounds), we have devised a new methodology for predicting potent compounds from weakly \npotent input molecules. Therefore, a chemical language model was implemented consisting of a \nconditional transformer architecture for compound design guided by observed potency differences. \nThe model was evaluated using a newly generated compound test system enabling a rigorous \nassessment of its performance. It was shown to predict known potent compounds from different \nactivity classes not encountered during training. Moreover, the model was capable of creating highly \npotent compounds that were structurally distinct from input molecules. It also produced many novel \ncandidate compounds not included in test sets. Taken together, the findings confirmed the ability of \nthe new methodology to generate structurally diverse highly potent compounds.\nCompound design is one of the major tasks for computational approaches in medicinal chemistry. The primary \naim is the generation of compounds with desired properties, first and foremost, compounds with activity against \nindividual pharmaceutical targets and high potency. For compound design and potency predictions, a variety \nof computational methods have been developed or adapted. Mainstays include quantitative structure–activity \nrelationship (QSAR)  analysis1 for the design of increasingly potent analogues of active compounds and methods \nfor ligand- or structure-based virtual  screening2,3 to identify new hits. Ligand- and structure-based methods \nhave different requirements. For example, for docking  calculations4, a variety of scoring functions have been \ndeveloped to evaluate the quality and strength of receptor-ligand interactions and estimate binding  energies5,6. \nFor the structure-based prediction of relative potencies of congeneric compounds, free energy perturbation \nmethods have been  introduced7,8. At the ligand level, machine learning (ML) methods are widely used for hit \nidentification and non-linear QSAR  modeling9. For potency prediction, support vector regression (SVR) 10 has \nbecome a standard ML approach. Furthermore, for both computational compound screening and potency predic-\ntion, deep neural network (DNN) architectures are also increasingly  investigated11–13. Recently, a methodological \nframework was developed for evaluating the performance of deep generative models and a recurrent neural \nnetwork (RNN) was used to explore predictions based on sparse training  data14. However, the analysis mainly \nfocused on physicochemical properties. For potency prediction, the assessment and comparison of different \nmethods typically relies on the use of standard benchmark settings. Such benchmark calculations are required \nbut not sufficient to evaluate potency prediction methods and their potential for practical applications. Moreover, \nsuch calculations should be considered with caution. Notably, in benchmark settings, nearest neighbor analysis \nand mean or median value regression often meet the accuracy of increasingly complex ML  methods15. The \nhigh performance of these simple reference methods is supported by potency value distributions in commonly \nused compound data  sets15. In addition, narrow error margins separating ML-based and randomized potency \nvalue predictions limit conclusions that can be drawn from conventional  benchmarking15. Such findings call for \nalternatives to conventional benchmarking such as focusing predictions on the most potent data set compounds, \nconsistent with the final goal of compound optimization efforts.\nWhile potency predictions are mostly carried out for individual compounds, they can also be applied to assess \npotency differences in compound pairs such as activity cliffs (ACs), which are formed by structurally similar \n(analogous) active compounds with large differences in  potency16. In principle, ACs can be predicted by explicitly \ncalculating potency differences between compounds in pairs or by distinguishing between ACs and other pairs of \nanalogues using classification methods, which implicitly accounts for potency differences of varying magnitude.\nOPEN\nDepartment of Life Science Informatics and Data Science, B-IT, LIMES Program Unit Chemical Biology and Medicinal \nChemistry, Rheinische Friedrich-Wilhelms-Universität, Friedrich-Hirzebruch-Allee 5/6, 53115 Bonn, Germany. \n*email: bajorath@bit.uni-bonn.de\n2\nVol:.(1234567890)Scientific Reports |         (2023) 13:7412  | https://doi.org/10.1038/s41598-023-34683-x\nwww.nature.com/scientificreports/\nPreviously, we have reported a deep learning approach for the prediction of ACs that further extended other \nML classification methods by its ability to not only predict ACs, but also generate new AC  compounds17. Since \nACs encode large potency differences, we have reasoned that this methodology might be adapted and further \nextended for the design of highly potent compounds. Therefore, in this work, we have devised and implemented \na chemical language model (CLM) for the prediction of highly potent compounds from weakly potent ones \nused as input. These predictions do not depend on conventional benchmark settings and are thus not affected \nby their intrinsic limitations.\nMethods\nCompounds, activity data, and analogue series. From ChEMBL (release 29)18, bioactive compounds \nwith high-confidence activity data were assembled. Only compounds with reported direct interactions (assay \nrelationship type: “D”) with human targets at the highest assay confidence level (assay confidence score 9) \nwere considered. As potency measurements, only numerically specified equilibrium constants  (Ki values) were \naccepted and recorded as (negative logarithmic)  pKi values. If multiple measurements were available for the \nsame compound, the geometric mean was calculated as the final potency annotation, provided all values fell \nwithin the same order of magnitude; otherwise, the compound was disregarded. Qualifying compounds were \norganized into target-based activity classes. A total of 496 activity classes were obtained.\nFor each activity class, a systematic search for analogue series (ASs) was conducted using the compound-core \nrelationship (CCR)  method19, which uses a modified matched molecular pair (MMP) fragmentation  procedure20 \nbased on retrosynthetic  rules21 to systematically identify ASs with single or multiple (maximally five) substitution \nsites. The core structure of an AS was required to consist of at least twice the number of non-hydrogen atoms of \nthe combined  substituents19.\nUltimately, 10 classes comprising ligands of different G protein coupled receptors were extracted as test cases \nfor compound predictions that each contained more than 900 compounds and more than 100 analogue series. \nTable 1 summarizes the targets and composition of these activity classes (first four columns from the left) and \nFig. 1 shows exemplary ASs with single or multiple substitution sites.\nTable 1.  Activity classes.\nChEMBL ID Target name Compounds ASs CCR pairs AC-CCR pairs\n218 Cannabinoid CB1 receptor 1118 250 8889 585\n226 Adenosine A1 receptor 1924 318 18,623 1207\n233 Mu opioid receptor 1216 169 10,430 1110\n234 Dopamine D3 receptor 1529 213 21,008 755\n237 Kappa opioid receptor 940 129 19,277 2897\n251 Adenosine A2a receptor 1825 312 16,084 870\n256 Adenosine A3 receptor 2033 434 42,621 6219\n3371 Serotonin 6 receptor 1535 201 36,735 2485\n4792 Orexin receptor 2 1133 131 12,368 1271\n5113 Orexin receptor 1 1086 155 23,169 817\nFigure 1.  Exemplary analogue series. Shown are small ASs with single (left) or multiple substitution sites \n(right). Core structures are colored blue and substituents red.\n3\nVol.:(0123456789)Scientific Reports |         (2023) 13:7412  | https://doi.org/10.1038/s41598-023-34683-x\nwww.nature.com/scientificreports/\nFor each of 10 activity classes, the number of compounds, ASs, CCR pairs, and AC-CCR pairs are provided. \nIn addition, for each class, the ChEMBL target ID, target name, and abbreviation are given. AS, CCR, and AC \nstand for analogue series, compound-core relationship, and activity cliff, respectively.\nFrom each of the activity classes, all possible pairs of analogues (termed All_CCR  pairs) were extracted, as \nillustrated in Fig.  2 that shows All_CCR pairs for two different ASs. The 496 activity classes yielded a total of \n881,990 All_CCR pairs.\nTokenization. For use by a CLM, compounds and potency differences must be tokenized. All compounds were \nrepresented as molecular-input line-entry system (SMILES)  strings22 generated using  RDKit23 and tokenized \nusing a single chemical character with the exception of two-character tokens (i.e., “Cl” and “Br”) and tokens \nin brackets (e.g. “[nH]” and “[O-]”). For the conditional transformer, potency differences must also be trans-\nformed into input tokens. For tokenization of value ranges, different approaches have been introduced includ-\ning  binning17,24,25 and, more recently, numerical  tokenization26. Since human readability of token sequences \nsupported by numerical approaches played no role for our analysis and encoding of drug discovery-relevant \ncompound potency ranges via binning has yielded accurate predictions  previously17, we continued to use binned \ntokens herein. Accordingly, potency differences between source and target compounds, ranging from − 6.62 to \n6.52  pKi units, were partitioned into 1314 binned tokens of a constant width of 0.01. This granularity (resolution) \ndefines the limits of experimental potency measurements and was thus most appropriate for our analysis. Each \nbin was encoded by a single token and each potency difference was assigned to the token of the corresponding \n bin17.\nTokenization of compound SMILES strings and potency ranges yielded the chemical vocabulary for our \nmodel. In addition, the two special tokens “start” and “end” were added to the vocabulary indicating the start \nand end point of a sequence, respectively.\nGenerative chemical language model. Architecture. For compound design, a CLM with the trans-\nformer architecture previously reported for the DeepAC approach for AC  prediction17 was used. The trans-\nformer architecture consisted of multiple encoder-decoder neural modules with attention  mechanism27. In the \nmodel, a stack of encoding sub-layers including a multi-head self-attention sub-layer and a fully connected \nfeed-forward network sub-layer constituted the encoder module. The encoder read an input sequence and com-\npressed it into a context vector in its final hidden state. The context vector served as the input for the decoder \nblock that interpreted the vector to predict an output sequence. Subsequently, the decoder module, which was \ncomposed of a feed-forward sub-layer and two multi-head attention sub-layers, re-converted the encodings into \na sequence of tokens (one token at a time). Both encoder and decoder utilized the attention mechanism during \ntraining to comprehensively learn from feature space.\nDuring pre-training, the model was supposed to learn mappings of source to target compounds based on \npotency differences resulting from changes in substituent(s) (termed chemical transformations):\nThen, given a new (Source compound, Potency difference) test instance, the model was applied to generate a \nset of candidate target compounds meeting the potency difference constraints, that is, having higher potency \nthan the source compound (according to the given potency difference).\nDuring pre-training, distinguishing between different activity classes was not required because at this stage, \nthe model should learn the syntax of textual molecular representations and, in addition, a variety of analogue \n(Source compound , Potency diﬀerence) →\n(\nTarget compound\n)\n.\nFigure 2.  Analogue pairs. For each of two exemplary ASs, three representative All_CCR pairs are shown (top, \nmiddle, and bottom; increasing potency from the left to the right). The Markush structure representing each \nAS is displayed in the center. Core structures are colored blue and substituents red. For each compound, its  pKi \nvalue is reported.\n4\nVol:.(1234567890)Scientific Reports |         (2023) 13:7412  | https://doi.org/10.1038/s41598-023-34683-x\nwww.nature.com/scientificreports/\npair-associated potency differences caused by chemical transformations. By contrast, during fine-tuning, activ-\nity class (target) information was required to focus the model on specific compound series or classes, as further \ndiscussed below.\nModel derivation. The transformer model was implemented using  Pytorch28. Default hyperparameter settings \nwere used together with a batch size of 64, learning rate of 0.0001, and encoding dimension of 256. The models \nwere derived over 200 epochs on the basis of the general training set. During training, the transformer model \nminimized the cross-entropy loss between the ground-truth and output sequence. A checkpoint was saved at \neach epoch and for a validation set, minimal loss was determined for selecting the final model.\nModel pre‑training. A general data set for model pre-training was derived from the 881,990 All_CCR \npairs of the 496 activity classes. From All_CCR pairs, All_CCR triples (Cpd A, CpdB, PotB-PotA) were generated \nby recording the potency difference for an All_CCR pair. Here, Cpd A represented the source compound  that \nwas concatenated with the potency difference (PotB-PotA) and CpdB represented the target compound. For each \nAll_CCR pair, two triples were obtained such that each All_CCR compound was used once as the source and \ntarget compound. To avoid data ambiguities, All_CCR pairs were eliminated if (1) a given source compound and \npotency difference was associated with multiple target compounds from different activity classes or (2) multiple \npotency values from different classes were available for a pair. On the basis of these criteria, a curated general \ndata set of 522,331 qualifying All_CCR triples was obtained and used for pre-training.\nFor each triple, the SMILES representation of the source compound concatenated with the binned token of \nthe associated potency difference served as the input sequence for the encoder that was converted into a latent \nrepresentation. Based on this representation, the decoder iteratively generated output SMILES sequences until \nthe end token was detected.\nModel fine‑tuning. For model fine-tuning and evaluation, the 10 activity classes in Table 1 were used. For \nfine-tuning, All_CCR pairs were extracted from each of the 10 activity classes and divided into subsets of so-\ncalled CCR  pairs with a less than 100-fold potency difference and AC-CCR  pairs capturing an at least 100-fold \ndifference in potency. Accordingly, AC-CCR pairs represented analogue pairs forming ACs. Depending on the \nactivity class, 8889–42,621 CCR pairs and 585–6219 AC-CCR pairs were obtained (Table 1, last two columns on \nthe right). AC-CCR triples were ordered such that CpdB was highly and CpdA weakly potent.\nThe pre-trained model was then separately fine-tuned and tested for each activity class. Therefore, AC-CCR \npairs from each class were randomly divided into 80% fine-tuning and 20% test instances such that there was \nno overlap in core structures between these sets. Thus, the fine-tuning set exclusively consisted of AC-CCR \npairs and was selected to train the model on activity class dependent analogue pairs with large potency differ -\nences. CCR pairs sharing core structures with the fine-tuning set were omitted from further consideration. The \nremaining CCR pairs were added to the test set. Hence, the fine-tuning and test sets were structurally distinct. \nModel evaluation is detailed below.\nResults\nStudy concept. Our study had three primary goals. First, we aimed to devise a novel approach specifically \nfor predicting highly potent compounds from weakly potent input molecules. Thus, rather than striving for \nprediction of potency values across large ranges, as is conventionally attempted using SVR or other machine \nlearning methods, the primary focus was on potent compounds, in line with the practical relevance of potency \npredictions. Second, we aimed to generate a structural spectrum of output compounds, ranging from analogues \nof input molecules to structurally distinct compounds, thereby increasing medicinal chemistry novelty of pre-\ndicted candidates. Third, it was intended to evaluate the methodology in a way that was not affected by limita-\ntions of conventional benchmarking of potency predictions, as discussed above, and enabled a non-ambiguous \nassessment of the ability to predict potent compounds. To meet the first two goals, which were central to our \nstudy, we implemented a CLM consisting of a chemical transformer architecture conditioned on compound \npotency differences. To meet the third goal, we designed a new compound test system.\nCompound pair‑based test system. For model evaluation, a compound pair-based test system was gen-\nerated using the test set. By design, the fine-tuning and test sets were structurally distinct. Furthermore, in con-\ntrast to the fine-tuning set, the test set contained analogue pairs capturing small or large differences in potency \n(i.e., CCR and AC-CCR pairs, respectively). Table 2 summarizes the composition of the test set.\nFor each activity class, the test set contained varying numbers of CCR pairs and AC-CCR pairs yielding vary-\ning numbers of unique CCR and AC-CCR compounds. In the following, SC and TC are used as abbreviations for \nsource (input) and target compound, respectively. For the evaluation of the fine-tuned CLM, test set compounds \nwere divided into instances with maximally 1 μmol potency (corresponding to a  pKi value of 6), which served \nas SCs, and candidate compounds with higher than 1 μmol potency  (pKi > 6), which served as known candidate \ncompounds (KCCs) for comparison with newly generated TCs.\nIn addition, the model generated varying numbers of novel (hypothetical) TCs. For each activity class, smaller \nnumbers of SCs than KCCs were available. With the exception of activity class 251 (3838 KCCs), the test set \ncontained 366–824 KCCs for the activity classes (Table  2), with on average 576 KCCs per class. Each CCR-SC \n (pKi ≤ 6) and AC-CCR-SC  (pKi ≤ 6) was once used as an input compound for the model and in each case, 50 TCs \nwere sampled, canonicalized, and compared to KCCs to search for exact matches, that is, fully reproduced com-\npounds with known potency. Because the model generated novel TCs, probabilities for re-generating known TCs \ncould not be derived in a meaningful way. Consequently, the main measure for establishing proof-of-principle \n5\nVol.:(0123456789)Scientific Reports |         (2023) 13:7412  | https://doi.org/10.1038/s41598-023-34683-x\nwww.nature.com/scientificreports/\nfor the ability of the model to predict potent compounds was the reproduction of any KCCs . For each activity \nclass, compound statistics were derived over three independent sampling trials, as reported below.\nTable 3 reports the possible predictions outcomes for the compound pair-based test system.\nFor each SC, a TC could be a known CCR or AC-CCR compound or a novel (hypothetical) compound \nrepresenting a TC not contained in the fine-tuning or test set. Taking core structure matches into consideration \n(that is, a TC either contained the same core structure as a SC or not), a total of 12 formally defined prediction \noutcomes were possible, including six each for CCR-SCs and AC-CCR-SCs, as identified by indices 1.1.–1.6. \nand 2.1.–2.6. in Table 3, respectively. Accordingly, a newly generated compound might be a structural analogue \nof a given SC (having the same core structure) or contain a different core structure. Furthermore, SCs and \nTCs might be distinguished by single or multiple substituents. On the basis of this classification scheme, CLM \npredictions were rigorously evaluated focusing on the reproduction of known active compounds, as explained \nabove. This was the most relevant measure of model performance because it enabled the exact determination of \npotency differences between SCs and TCs and hence the ability of the CLM to predict highly potent compounds. \nFor novel (hypothetical) compounds generated by the model, no assessment was possible (without subsequent \nexperimental evaluation).\nModel performance. For the SCs from all activity classes, systematic compound predictions were carried \nout using the CLM. The model only produced 0.5–2% invalid SMILES (assessed using RDKit) for all activity \nclasses.\nWith the exception of class 251 (1391 SCs), the test set contained 40–359 SCs for the activity classes, with on \naverage 162 compounds per class (Table 2). The predictions were then assessed on the basis of well-defined pair \ncategories detailed above, as reported in Table 4.\nFor each activity class and compound pair category indexed according to Table  3 (top row), the number of \nunique TCs produced by the CLM is reported. With the exception of categories 1.5., 1.6., 2.5., and 2.6., which \nreport novel (hypothetical) candidate compounds not contained in the fine-tuning or test set, the TCs represent \nKCCs, as defined in the text.\nEncouragingly, for all activity classes, the CLM successfully reproduced large numbers of KCCs for all SCs \n(categories 1.1.–1.4. and 2.1.–2.4., respectively). Frequently, multiple KCCs were obtained for the same SC. Fur-\nthermore, depending on the activity class, the model produced varying numbers of TCs with the same or different \ncore structure, thus confirming its ability to generate frequent core structure transformations. In many cases, \nmore structurally unique TCs were generated than analogues of SCs. Moreover, large numbers of hypothetical \ncandidate compounds not contained in the training set were obtained (categories 1.5.–1.6. and 2.5.–2.6., respec-\ntively). The reproducibility of the limited numbers of available KCCs representing known ACs (12–84 unique \ncompounds per activity class) was of particular interest (categories 2.1.–2.4.). AC-CCR KCCs were consistently \nreproduced and for five activity classes, the total count exceeded the number of unique AC-CCR KCCs per class \n(due to multiple reproductions of individual KCCs). Table 5 reports statistics for reproduction of KCCs.\nReported are statistics for the re-generation of KCCs including the mean number of KCCs over three inde -\npendent sampling trials and the proportion of reproduced KCCs relative to all available KCCs with standard \ndeviations (±). In addition, the mean number of non-KCCs over three independent trials is provided.\nThe proportion of exactly reproduced KCCs over independent sampling trials ranged from ~ 7 to ~ 37%, \ndepending on the activity class (with generally small standard deviations). For nine, six, and two classes, more \nthan 10, 20, and 30% of all available KCCs were reproduced, respectively. Applying the most rigorous criterion \nof exact re-generation of known potent compounds as a performance measure (see above), the observed num -\nbers and proportions represented unexpectedly good predictions, which clearly established proof-of-concept \nfor the approach.\nFor each activity class, ASs were also extracted from newly generated (predicted) compounds. Table 6 reports \nthe number of ASs (multiple compounds having the same core structure) and singletons (compounds with \nTable 2.  Test set. CPD stands for compound, SC for source compound, and KCC for known candidate \ncompound. According to our analysis scheme, target compounds (TCs) produced by the model were compared \nto KCCs.\nChEMBL ID CCR pairs Unique CCR CPDs AC-CCR pairs Unique AC-CCR CPDs Overlapping CPDs\nUnique \nCCR + AC-CCR CPDs SCs (pki ≤ 6) KCCs (pki > 6)\n218 2198 579 6 12 9 582 129 453\n226 5950 1174 144 84 80 1178 359 819\n233 2332 590 36 36 33 593 76 517\n234 7790 913 50 53 53 913 89 824\n237 1032 477 31 24 20 481 115 366\n251 4706 5210 85 57 38 5229 1391 3838\n256 5012 888 40 44 42 890 250 640\n3371 4420 722 42 44 44 722 40 682\n4792 1941 615 49 50 48 617 146 471\n5113 7543 664 13 15 15 664 256 408\n6\nVol:.(1234567890)Scientific Reports |         (2023) 13:7412  | https://doi.org/10.1038/s41598-023-34683-x\nwww.nature.com/scientificreports/\nTable 3.  Possible predictions.\nIndex same/different core Compound pair category\n1.1./1.2. (CCR-SC, CCR-TC)\n1.3./1.4. (CCR-SC, AC-CCR-TC)\n1.5./1.6. (CCR-SC, novel CPD)\n2.1./2.2. (AC-CCR-SC, AC-CCR-TC)\n2.3./2.4. (AC-CCR-SC, CCR-TC)\n2.5./2.6. (AC-CCR-SC, novel CPD)\nTable 4.  Prediction results.\nChEMBL \nID 1.1. 1.2. 1.3. 1.4. 1.5. 1.6. 2.1. 2.2. 2.3. 2.4. 2.5. 2.6.\n218 73 192 2 5 436 3301 3 3 4 11 24 34\n226 262 433 4 25 1067 5030 11 11 27 79 129 529\n233 217 179 2 14 252 570 6 9 0 10 21 45\n234 141 92 3 2 286 705 3 2 6 7 24 13\n237 488 250 0 11 181 766 9 26 14 4 4 10\n251 2367 1400 235 128 1031 13,523 17 5 36 13 55 199\n256 112 66 1 2 657 5336 10 7 0 12 13 359\n3371 60 116 0 4 42 1202 7 4 3 8 33 101\n4792 224 662 7 42 253 1222 7 6 7 17 17 25\n5113 433 349 1 5 304 1638 5 2 11 2 15 24\nTable 5.  Reproducibility of known candidate compounds.\nChEMBL ID KCCs Non-KCCs Reproduced KCCs (%)\n218 103 3445 22.74 ± 1.10\n226 211 5139 25.76 ± 0.49\n233 143 1005 27.66 ± 1.35\n234 92 825 11.17 ± 0.24\n237 128 839 34.97 ± 1.37\n251 251 4996 6.54 ± 0.29\n256 76 5165 11.88 ± 0.63\n3371 72 2145 10.56 ± 1.17\n4792 172 1084 36.52 ± 1.91\n5113 117 1499 28.68 ± 1.72\nTable 6.  Structural organization of predicted compounds. “Reproduced cores” reports the percentage of the \ncore structures contained in each original activity class that were detected in predicted compounds.\nChEMBL ID ASs Singletons Reproduced cores (%)\n218 858 1235 4\n226 905 1762 4\n233 188 255 12\n234 90 245 9\n237 175 303 7\n251 1304 978 4\n256 1414 1386 7\n3371 321 1022 4\n4792 146 219 18\n5113 233 440 9\n7\nVol.:(0123456789)Scientific Reports |         (2023) 13:7412  | https://doi.org/10.1038/s41598-023-34683-x\nwww.nature.com/scientificreports/\nunique core structures not belonging to any AS). Depending on the activity class, 90–1414 ASs and 219–1762 \nsingletons were obtained, respectively.\nSince each AS and singleton contained a unique core structure (scaffold), the core structure diversity of newly \ngenerated compounds was generally high. Between 4 and 18% of the core structures contained in the original \nactivity classes (from ASs and singletons) were reproduced by the model, as also reported in Table 6.\nHaving confirmed the ability of the CLM to generate structurally analogous and diverse TCs including KCCs, \nthe key question then was whether or not the model would produce TCs that had much higher potency than the \ncorresponding SCs. Figure 3 shows the distributions of potency differences between pairs of known source and \ntarget compounds with experimental potency values involving compounds from ACs. For five activity classes, \nthe median potency difference fell between one and two orders of magnitude (10–100-fold) and for the other \nfive classes, the median value exceeded two orders of magnitude (100-fold). Furthermore, for all but one class, \nmultiple compounds with at least 1000-fold higher potency than the corresponding SCs were generated (includ-\ning highly potent statistical outliers). Thus, these observations unambiguously confirmed the ability of the CLM \nto generate highly potent compounds from weakly potent (micromolar) input molecules.\nFigure 4 shows exemplary pairs of SCs and newly designed compounds (TCs) with different structural rela-\ntionships. Given our design strategy, all SCs were known compounds with experimentally determined potency. \nThe generated TCs included known potent analogues of SCs (Fig. 4a), structurally distinct known potent com-\npounds (Fig. 4b), and novel (hypothetical) compounds (Fig. 4c). Taken together, these examples illustrate suc-\ncessful CLM predictions.\nConclusion\nThe underlying idea for the development of the approach reported herein was to predict highly potent com-\npounds from individual weakly potent input molecules. For all practical purposes, this represents an ultimate \ngoal of potency prediction, especially for compound optimization in medicinal chemistry. This prediction task \ncould not be addressed using conventional regression models. In addition, going beyond the applicability domain \nof standard QSAR modeling, we also aimed to design structurally diverse compounds, in addition to analogues. \nTherefore, a different methodological framework was required and we adapted a conditional transformer archi-\ntecture previously used for AC predictions. These predictions established that compound generation could be \nconditioned on potency differences. However, since AC predictions were also confined to structurally analogous \ncompounds, it remained unclear whether or not potency difference conditioning was transferable to the design \nof structurally diverse compounds with high potency. The CLM reported herein was fine-tuned on pairs of SCs \nand TCs with associated potency differences and we then examined its ability to predict structurally diverse \ncompounds with large increases in potency relative to input molecules. Therefore, a compound pair-based test \nsystem was generated that covered all possible prediction outcomes and enabled a well-defined and rigorous \nassessment of model performance. Our analysis confirmed the ability of the model to reproduce known potent \ncompounds not encountered during training at unexpectedly high rates, including both analogues of weakly \npotent SCs and structurally distinct compounds. With median potency increases close to or above 100-fold \nacross activity classes and multiple predictions with more than 1000-fold increases in compound potency, model \nperformance was generally high. In addition, the CLM also produced large numbers of novel compounds for the \nactivity classes that were not contained in the fine-tuning or test set.\nTaken together, our findings indicate that the approach reported herein should have considerable potential \nfor practical applications. In compound optimization, we envision that the CLM will be fine-tuned using sets of \nFigure 3.  Potency difference distribution. For all activity classes, boxplots report the distributions of \nlogarithmic potency differences between pairs of known source and target compounds involving compounds \nfrom ACs. In boxplots, the median value is represented by the horizontal line, and the box defines upper and \nlower quantile. Upper and lower whiskers represent the maximum and minimum value, respectively. Diamond \nsymbols mark statistical outliers.\n8\nVol:.(1234567890)Scientific Reports |         (2023) 13:7412  | https://doi.org/10.1038/s41598-023-34683-x\nwww.nature.com/scientificreports/\nactive compounds for a target of interest and that the predictions will then focus on input compounds prioritized \nby medicinal chemistry. For these and other applications, the CLM is made freely available as a part of our study.\nData availability\nAll calculations were carried out using publicly available programs and compound data. Python scripts used for \nimplementing CLMs and the activity classes used herein are freely available via the following link: https://  doi. \norg/ 10. 5281/ zenodo. 77447 63.\nFigure 4.  Exemplary predictions. Shown are pairs of corresponding source compounds (left of the arrow) and \nnew compounds generated by the CLM (right) including (a) potent known compounds with conserved core \nstructures (black, distinguishing substituents are red), (b) potent known compounds with distinct structures \n(blue), and (c) hypothetical compounds (green). For hypothetical compounds, no potency values were available. \nNumbers on arrows identify activity classes according to Table 1. Potency differences between SCs and KCCs are \nreported.\n9\nVol.:(0123456789)Scientific Reports |         (2023) 13:7412  | https://doi.org/10.1038/s41598-023-34683-x\nwww.nature.com/scientificreports/\nReceived: 2 February 2023; Accepted: 5 May 2023\nReferences\n 1. Lewis, R. A. & Wood, D. Modern 2D QSAR for drug discovery. WIREs Comput. Mol. Sci. 4, 505–522 (2014).\n 2. Geppert, H., Vogt, M. & Bajorath, J. Current trends in ligand-based virtual screening: Molecular representations, data mining \nmethods, new application areas, and performance evaluation. J. Chem. Inf. Model. 50, 205–216 (2010).\n 3. Cheng, T., Li, Q., Zhou, Z., Wang, Y . & Bryant, S. H. Structure-based virtual screening for drug discovery: A problem-centric \nreview. AAPS J. 14, 133–141 (2012).\n 4. Pagadala, N. S., Syed, K. & Tuszynski, J. Software for molecular docking: A review. Biophys. Rev. 9, 91–102 (2017).\n 5. Liu, J. & Wang, R. Classification of current scoring functions. J. Chem. Inf. Model. 55, 475–482 (2015).\n 6. Guedes, I. A., Pereira, F . S. & Dardenne, L. E. Empirical scoring functions for structure-based virtual screening: Applications, \ncritical aspects, and challenges. Front. Pharmacol. 9, e1089 (2018).\n 7. Mobley, D. L. & Gilson, M. K. Predicting binding free energies: Frontiers and benchmarks. Annu. Rev. Biophys. 46, 531–558 (2017).\n 8. Williams-Noonan, B. J., Yuriev, E. & Chalmers, D. K. Free energy methods in drug design: Prospects of “ Alchemical perturbation” \nin medicinal chemistry. J. Med. Chem. 61, 638–649 (2018).\n 9. Vamathevan, J. et al. Applications of machine learning in drug discovery and development. Nat. Rev. Drug. Discov.  18, 463–477 \n(2019).\n 10. Smola, A. J. & Schölkopf, B. A tutorial on support vector regression. Stat. Comput. 14, 199–222 (2004).\n 11. Hou, F . et al. Comparison study on the prediction of multiple molecular properties by various neural networks. J. Phys. Chem. A  \n122, 9128–9134 (2018).\n 12. Feinberg, E. N. et al. PotentialNet for molecular property prediction. ACS Cent. Sci. 4, 1520–1530 (2018).\n 13. Walters, W . P . & Barzilay, R. Applications of deep learning in Molecule generation and molecular property prediction. Acc. Chem. \nRes. 54, 263–270 (2020).\n 14. Skinnider, M. A., Stacey, R. G., Wishart, D. S. & Foster, L. J. Chemical language models enable navigation in sparsely populated \nchemical space. Nat. Mach. Intell. 3, 759–770 (2021).\n 15. Janela, T. & Bajorath, J. Simple nearest-neighbour analysis meets the accuracy of compound potency predictions using complex \nmachine learning models. Nat. Mach. Intell. 4, 1246–1255 (2022).\n 16. Stumpfe, D., Hu, H. & Bajorath, J. Evolving concept of activity cliffs. ACS Omega 4, 14360–14368 (2019).\n 17. Chen, H., Vogt, M. & Bajorath, J. DeepAC—Conditional transformer-based chemical language model for the prediction of activity \ncliffs formed by bioactive compounds. Digital Discov. 1, 898–909 (2022).\n 18. Bento, A. P . et al. The CHEMBL bioactivity database: An update. Nucleic Acids Res. 42, D1083–D1090 (2014).\n 19. Naveja, J. J., Vogt, M., Stumpfe, D., Medina-Franco, J. L. & Bajorath, J. Systematic extraction of analogue series from large compound \ncollections using a new computational compound–core relationship method. ACS Omega 4, 1027–1032 (2019).\n 20. Stumpfe, D., Dimova, D. & Bajorath, J. Computational method for the systematic identification of analog series and key compounds \nrepresenting series and their biological activity profiles. J. Med. Chem. 59, 7667–7676 (2016).\n 21. Lewell, X. Q., Judd, D. B., Watson, S. P . & Hann, M. M. RECAP - retrosynthetic combinatorial analysis procedure: A powerful \nnew technique for identifying privileged molecular fragments with useful applications in combinatorial chemistry. J. Chem. Inf. \nComput. Sci. 38, 511–522 (1998).\n 22. Weininger, D. SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. J. Chem. \nInf. Comput. Sci. 28, 31–36 (1988).\n 23. RDKit: Cheminformatics and Machine Learning Software. http:// www. rdkit. org (accessed on 1 July 2021).\n 24. He, J. et al. Molecular optimization by capturing chemist’s intuition using Deep Neural Networks. J. Cheminform. 13, 26 (2021).\n 25. He, J. et al. Transformer-based molecular optimization beyond matched Molecular Pairs. J. Cheminform. 14, 18 (2022).\n 26. Born, J. & Manica, M. Regression transformer enables concurrent sequence regression and generation for molecular language \nmodelling. Nat. Mach. Intell. 5, 432–444 (2023).\n 27. Vaswani, A. et al. Attention is all you need. Adv. Neural Inf. Process. Syst. 30, 6000–6010 (2017).\n 28. Aszke, A. et al. PyTorch: An imperative style, high-performance deep learning library. Adv. Neural Inf. Process. Syst. 32, 8026–8037 \n(2019).\nAcknowledgements\nThe authors thank Martin Vogt for many helpful suggestions. H.C. is supported by the China Scholarship Council \n(CSC).\nAuthor contributions\nAll authors contributed to designing and conducting the study, analyzing the results, and preparing the \nmanuscript.\nFunding\nOpen Access funding enabled and organized by Projekt DEAL.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to J.B.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\n10\nVol:.(1234567890)Scientific Reports |         (2023) 13:7412  | https://doi.org/10.1038/s41598-023-34683-x\nwww.nature.com/scientificreports/\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023"
}