{
  "title": "Dual-Level Collaborative Transformer for Image Captioning",
  "url": "https://openalex.org/W3124043039",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2345332389",
      "name": "Luo Yunpeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4214295927",
      "name": "Ji, Jiayi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A125454976",
      "name": "Sun Xiaoshuai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2378737927",
      "name": "Cao, Liujuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2067166808",
      "name": "Wu Yongjian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2635098287",
      "name": "Huang, Feiyue",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222165473",
      "name": "Lin, Chia-Wen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2264742060",
      "name": "Ji, Rongrong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3034655362",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2997443031",
    "https://openalex.org/W2986670728",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W2506483933",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W2998988444",
    "https://openalex.org/W2890531016",
    "https://openalex.org/W2575842049",
    "https://openalex.org/W2971310675",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W3035497460",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3035160838",
    "https://openalex.org/W2963084599",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2904551248",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W3035284526",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2997248215",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2950178297",
    "https://openalex.org/W2983141445",
    "https://openalex.org/W1895577753"
  ],
  "abstract": "Descriptive region features extracted by object detection networks have played an important role in the recent advancements of image captioning. However, they are still criticized for the lack of contextual information and fine-grained details, which in contrast are the merits of traditional grid features. In this paper, we introduce a novel Dual-Level Collaborative Transformer (DLCT) network to realize the complementary advantages of the two features. Concretely, in DLCT, these two features are first processed by a novelDual-way Self Attenion (DWSA) to mine their intrinsic properties, where a Comprehensive Relation Attention component is also introduced to embed the geometric information. In addition, we propose a Locality-Constrained Cross Attention module to address the semantic noises caused by the direct fusion of these two features, where a geometric alignment graph is constructed to accurately align and reinforce region and grid features. To validate our model, we conduct extensive experiments on the highly competitive MS-COCO dataset, and achieve new state-of-the-art performance on both local and online test sets, i.e., 133.8% CIDEr-D on Karpathy split and 135.4% CIDEr on the official split. Code is available at https://github.com/luo3300612/image-captioning-DLCT.",
  "full_text": "Dual-Level Collaborative Transformer for Image Captioning\nYunpeng Luo1, Jiayi Ji1, Xiaoshuai Sun1*, Liujuan Cao1,\nYongjian Wu3, Feiyue Huang3, Chia-Wen Lin4, Rongrong Ji1,2\n1 Media Analytics and Computing Lab, Department of Artiﬁcial Intelligence,\nSchool of Informatics, Xiamen University, 361005, China\n2 Institute of Artiﬁcial Intelligence, Xiamen University\n3 Tencent Youtu Lab 4 National Tsing Hua University\nlyricpoem1997@gmail.com, jjyxmu@gmail.com,xssun@xmu.edu.cn, caoliujuan@xmu.edu.cn,\nlittlekenwu@tencent.com, garyhuang@tencent.com, cwlin@ee.nthu.edu.tw, rrji@xmu.edu.cn\nAbstract\nDescriptive region features extracted by object detection net-\nworks have played an important role in the recent advance-\nments of image captioning. However, they are still criticized\nfor the lack of contextual information and ﬁne-grained de-\ntails, which in contrast are the merits of traditional grid fea-\ntures. In this paper, we introduce a novel Dual-Level Col-\nlaborative Transformer (DLCT) network to realize the com-\nplementary advantages of the two features. Concretely, in\nDLCT, these two features are ﬁrst processed by a novelDual-\nway Self Attenion(DWSA) to mine their intrinsic properties,\nwhere a Comprehensive Relation Attentioncomponent is also\nintroduced to embed the geometric information. In addition,\nwe propose a Locality-Constrained Cross Attentionmodule\nto address the semantic noises caused by the direct fusion\nof these two features, where a geometric alignment graph\nis constructed to accurately align and reinforce region and\ngrid features. To validate our model, we conduct extensive\nexperiments on the highly competitive MS-COCO dataset,\nand achieve new state-of-the-art performance on both local\nand online test sets, i.e., 133.8% CIDEr on Karpathy split\nand 135.4% CIDEr on the ofﬁcial split. Code is available at\nhttps://github.com/luo3300612/image-captioning-DLCT.\nIntroduction\nImage captioning is the task of generating a descriptive\nstatement automatically for an input image. Its main chal-\nlenges not only lie in the comprehensive understanding of\nobjects and relationships in the image, but also in the genera-\ntion of ﬂuent sentences that match the visual semantics. With\nyears of developments, the great success of image caption-\ning has been supported by a ﬂurry of methods (Rennie et al.\n2017; Anderson et al. 2018; Zhou et al. 2020) and bench-\nmark datasets (Lin et al. 2014).\nAmong these advancements, a milestone in image cap-\ntioning is the introduction of visual region features extracted\nby object detection networks (Anderson et al. 2018), e.g.,\nFaster R-CNN (Ren et al. 2015). Compared with the grid\n*Corresponding Author\nCopyright © 2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: (a) Limitations of region features on characteriz-\ning contextual (up) and detailed (down) information. (b) An\nexample of region features (left), grid features (right) and\ntheir geometric alignment. Our model enables the interac-\ntion between two kinds of features based on their semantic\nalignment constructed according to their geometric proper-\nties. (c) An illustration ofsemantic noise problem. Blue re-\ngions are the top-k attended regions (from deep to shallow)\nby the red grid. In Transformer (left), the top-5 attended re-\ngions are all semantically unrelated. In our DLCT (right),\nthe red grid only attends to two semantically related regions.\nfeatures1 used in earlier methods (Vinyals et al. 2015), re-\ngion features can provide object-level information, since\nmost salient regions in an image can be recognized and rep-\nresented by a feature vector. Hence, region features greatly\nreduce the difﬁculty of visual-semantic embeddings, based\non which recent endeavors have greatly boosted the perfor-\nmance of image captioning (Huang et al. 2019; Cornia et al.\n2020; Pan et al. 2020).\nDespite the great success, region features are still criti-\ncized for the lack of contextual information and ﬁne-grained\ndetails. As illustrated in Fig.1-(a), the detected regions may\nnot cover the entire image, leading to the inability to cor-\nrectly describe the global scenes, e.g., in front of a store.\nMeanwhile, each region is represented by a single fea-\n1The feature maps of the pre-trained convolution neural net-\nworks (CNN).\narXiv:2101.06462v2  [cs.CV]  3 Aug 2021\nture vector, which inevitably loses object details in large\namounts, e.g., the colors of trains. However, these shortcom-\nings are the merits of grid features which in contrast cover\nall the content of a given image in a more fragmented form.\nTo this end, it is a natural thought to use both features as\nthe visual input, which however results in a new issue. To\nexplain, most recent methods in image captioning (Huang\net al. 2019; Cornia et al. 2020; Pan et al. 2020) use the self-\nattention modules to model the relationships of visual fea-\ntures. Under this setting, the direct use of two sources of\nfeatures is prone to producing semantic noises during the at-\ntention process. For instance, a grid may interact with incor-\nrect regions just because they have similar appearances,e.g.,\nthe cat’s belly and the white remote controller, as shown in\nFig.1-(c). Such a case not only hinders the complementarity\nof two features but also degrades the overall performance,\ni.e., using two features might be worse than using one, which\nhas also been validated in Tab.5.\nIn this paper, we propose a novel Dual-Level Collabo-\nrative Transformer(DLCT) network to realize the comple-\nmentary advantages of region and grid features for image\ncaptioning. Concretely, as shown in Fig.2, the two sources\nof features are ﬁrst processed by a novel Dual-Way Self-\nAttention (DWSA) module to explore their intrinsic prop-\nerties, where a Comprehensive Relation Attention (CRA)\nscheme is equipped to embed absolute and relative geome-\ntry information of input features. In addition, we further pro-\npose a Locality-Constrained Cross Attention(LCCA) mod-\nule to address the aforementioned degradation issue, where\na geometric alignment graph is constructed to guide the se-\nmantic alignment between two sources of features. With this\ngeometric alignment graph, LCCA can accurately enable the\ninteraction between features of two sources. More impor-\ntantly, it can reinforce each type of feature by cross-attention\nfusions, such as transferring objectness information from re-\ngion features to grid ones and supplementing ﬁne-grained\ndetails from grid features to region ones.\nTo validate the proposed DLCT, we conduct extensive\nexperiments on MS-COCO dataset (Lin et al. 2014), and\nachieve new state-of-the-art performances for image cap-\ntioning, i.e., 133.8% CIDEr scores on Karpathy test set\n(Karpathy and Fei-Fei 2015) and 135.4% CIDEr scores on\nthe online test.\nWe summarize the contributions of this paper as follows:\n• We propose an Dual-level Collaborative Transformernet-\nwork to achieve the complementarity of region and grid\nfeatures. Extensive experiments on MS-COCO dataset\ndemonstrate the superior performance of our method\ncompared with the state-of-the-arts.\n• We propose Locality-Constrained Cross Attention to ad-\ndress the issue of semantic noise aroused by the direct\nfusion of two sources of features. With the constructed\ngeometric alignment graphs, LCCA can not only enables\nthe interaction between features of different sources ac-\ncurately, but also reinforce each kind of feature via cross-\nattention fusions.\n• To our best knowledge, we also present the ﬁrst attempt to\nexplore the absolute position information for image cap-\ntioning. By integrating absolute and relative location in-\nformation, we further improve the modeling of intra- and\ninter-level relationships.\nRelated Work\nExisting image captioning approaches typically follow the\nencoder-decoder architecture (Xu et al. 2015; Huang et al.\n2019; Guo et al. 2020; Cornia et al. 2020; Zhao, Wu, and\nZhang 2020; Seo et al. 2020), which takes an image as in-\nput and generates a description in the form of natural lan-\nguage. Earlier works (Xu et al. 2015; Lu et al. 2017; Jiang\net al. 2020) apply grid-based features as input to generate\ncaptions, which are ﬁxed-size patches extracted from the\nCNN (He et al. 2016; ?) model. Recently, region-level fea-\ntures extracted by Faster-RCNN (Ren et al. 2015) have also\nbeen introduced to captioning models, signiﬁcantly improv-\ning the quantitative performance of image captioning (An-\nderson et al. 2018; Herdade et al. 2019; Huang et al. 2019;\nCornia et al. 2020; Guo et al. 2020). Nevertheless, they have\na deﬁciency of predicting sentences by using only one kind\nof feature.\nHAN (Wang, Chen, and Hu 2019) proposes a hierarchical\nattention network to combine text, grids, and regions with a\nrelation module to exploit the inherent relationship among\ndiverse features. However, it fails to integrate location in-\nformation of visual features and coarsely model appearance\nrelationship while ignoring to ﬁlter semantic noises. GCN-\nLSTM (Yao et al. 2018) and Object Relation Transformer\n(Herdade et al. 2019) utilize bounding boxes of regions to\nmodel location relationships between regions in a relative\nmanner. However, by modeling location relatively, they can\nintegrate appearance features and geometry features but still\nfail to grab the absolute locations of features in an image.\nDual-Level Collaborative Transformer\nIn this section, we introduce a novel image captioning\nmodel, named Dual-Level Collaborative Transformer, which\nuses both grid and region features to achieve the comple-\nmentarity of them. The overall structure of our model is il-\nlustrated in Fig. 2.\nIntegrating Position Information\nPrevious methods only model location relationships of re-\ngions in a relative manner. Thus we propose Comprehen-\nsive Relation Attention (CRA) to model complex visual and\nlocation relationships between input features by integrating\nboth absolute and relative location information.\nAbsolute Postional Encoding Absolute positional encod-\ning (APE) tells the model where the feature is, which is\nimportant information. Suppose there are two objects with\nidentical appearance features: one locates in the corner and\nthe other locates at the center. In this case, APE facilitates\nthe model to distinguish them accurately. For APE, we con-\nsider two kinds of visual features,i.e., grids and regions. For\ngrids, we use the concatenation of two 1-d sine and cosine\nembeddings to get the grid positional encoding (GPE):\nGPE(i,j) = [PEi; PEj], (1)\nFigure 2: Overview of the proposed Dual-Level Collaborative Transformer architecture. We devise the Comprehensive Relation\nAttention to integrate position information in both absolute and relative manners. The Dual-Way Self Attention is applied to\nmine the intrinsic properties of two kinds of features, followed by the Locality-Constrained Cross Attention (LCCA) which\nenables the interaction between regions and grids. With the geometric alignment graph, LCCA can eliminate semantic noises\nand achieve inter-level fusion effectively.\nwhere i,jare the row index and column index of the grid and\nPEi,PEj ∈Rdmodel/2 are deﬁned as:\nPE(pos,2k) = sin(pos/100002k/(dmodel/2)),\nPE(pos,2k+ 1) = cos(pos/100002k/(dmodel/2)),\n(2)\nwhere pos denotes the position and k is the dimen-\nsion. For regions, we embed 4-d bounding box Bi =\n(xmin,ymin,xmax,ymax) in region positional encoding\n(RPE):\nRPE(i) = BiWemb, (3)\nwhere iis the index of box, (xmin,ymin) and (xmax,ymax)\nrespectively denote the top-left and bottom-right corners of\nthe box and Wemb ∈Rdmodel×4 is an embedding parameter\nmatrix.\nRelative Positional Encoding To better integrate relative\nlocation information of visual features, we add relative lo-\ncation information according to the geometric structure of\nbounding boxes. The bounding box of a region can be repre-\nsented as (x,y,w,h ) where x, y, w, and hdenote the box’s\ncenter coordinates and its width and height. Note that a grid\nis a special case of a bounding box. So grids can also be\nrepresented as (x,y,w,h ) according to its respective ﬁeld.\nThus for boxi and boxj, we can represent their geometric\nrelationship as a 4-d vector:\nΩ(i,j) =\n(\nlog (|xi −xj|\nwi\n),log (|yi −yj|\nhi\n),log (wi\nwj\n),log (hi\nhj\n)\n)T\n.\n(4)\nThen Ω(i,j) is embeded in a high-dimensional embedding\nby the Emb method in (Vaswani et al. 2017). Finally,Ω(i,j)\nis mapped to a scalar which conveys the geometric relation-\nship between two boxes:\nΩ(i,j) = ReLU(Emb(Ω(i,j))WG), (5)\nwhere WG is a learned parameter matrix.\nComprehensive Relation Attention Once absolute infor-\nmation and relative information are extracted, we can in-\ntegrate them by Comprehensive Relation Attention (CRA).\nFor APE, we modify the queries and keys at the attention\nlayer:\nW = (Q+ posq)(K+ posk)T\n√dk\n, (6)\nwhere posq and posk are APE of queries and keys respec-\ntively. Then we utilize relative location information to adjust\nattention weights by:\nW′\nij = Wij + log(Ω(i,j)). (7)\nFinally, softmax is applied to normalize weights and calcu-\nlate the outputs of CRA. Our Multi-Head CRA (MHCRA)\ncan be formalized as:\nMHCRA(Q,K,V ) = Concat(head1,··· ,headh)WO,\n(8)\nheadi = CRA(QWQ\ni ,KW K\ni ,VW V\ni ,posq,posk,Ω), (9)\nwhere\nCRA(Q,K,V,pos q,posk,Ω) =\nsoftmax((Q+ posq)(K+ posk)T\n√dk\n+ log(Ω))V. (10)\nFigure 3: Example of a geometric alignment graph. Re-\ngions and grids with intersections (highlighted with the same\ncolor) are connected by undirected edges to eliminate se-\nmantically unrelated information. Note each node has a self-\nconnected edge.\nDual-Level Collaborative Encoder\nGiven an image, we ﬁrstly extract its grid and region features\nrespectively dubbed as VG = {vi}NG and VR = {vi}NR.\nNG and NR are numbers of corresponding features. Our en-\ncoder consists of two sub-modules: Dual-Way Self Attention\nand Locality-Constrained Cross Attention.\nDual-Way Self Attention In general, visual features are\nextracted by locally-connected convolutions, which make\nthem isolated and relation-agnostic. It is believed that Trans-\nformer Encoder contributes signiﬁcantly to the performance\nof image captioning, because it can model relationships be-\ntween the inputs to enrich visual features by self-attention.\nTo better model intra-level relationships of two kinds of fea-\ntures, we devise a Dual-Way Self Attention (DWSA) which\nconsists of two independent self-attention modules.\nSpeciﬁcally, the hidden states of regions H(l)\nr and grids\nH(l)\ng are fed into the(l+1)-th DWSA to learn relation-aware\nrepresentaion:\nC(l)\nr = MHCRA(H(l)\nr ,H(l)\nr ,H(l)\nr ,RPE,RPE,Ωrr), (11)\nC(l)\ng = MHCRA(H(l)\ng ,H(l)\ng ,H(l)\ng ,GPE,GPE,Ωgg). (12)\nwhere H(0)\nr = VR, H(0)\ng = VG. Ωrr and Ωgg are relative\nlocation matrix of regions and grids respectively. Then we\nadopt two independent position-wise feedforward networks\nFFN for each type of visual features:\nC\n′(l)\nr = FFNr(C(l)\nr ), (13)\nC\n′(l)\ng = FFNg(C(l)\ng ). (14)\nAfter that, the relation-aware representations are fed into the\nnext module.\nLocality-Constrained Cross Attention We propose\nLocality-Constrained Cross Attention (LCCA) to model\ncomplex interactions between regions and grids for inter-\nlevel fusion. To avoid introducing semantic noises, we ﬁrst\ncreate a geometric alignment graph G= (V,E). All region\nand grid features are represented as independent nodes to\nform a visual node set V. For edge set E, a grid node is\nconnected to a region node if and only if their bounding\nboxes have intersections. Following the above rules, we\ncan construct an undirected graph, as illustrated in Fig. 3.\nBased on the geometric alignment graph, we apply LCCA\nto identify attention across two different kinds of visual\nfeature ﬁelds: the source ﬁeld and the target ﬁeld. In LCCA,\nthe source ﬁeld serves as queries and the target ﬁeld serves\nas keys and values. LCCA aims at reinforcing representation\nof the source ﬁeld by embedding information of the target\nﬁeld into the source ﬁeld. Like Equ. (1)(2), we integrate the\nabsolute and relative location information to get the weight\nmatrix W\n′\nand normalize it:\nαij = eW\n′\nij\n∑\nj∈A(vi) eW′\nij\n, (15)\nwhere vi is the visual node and A(vi) is the set of neigh-\nboring visual nodes of vi. The weighted sum is applied as\nMi =\n∑\nj∈A(vi)\nα(l)\nij Vj, (16)\nwhere Vj is the j-th visual node value. For simplicity, we\nformulate this stage as\nM = graph-softmax\nG\n(W′)V, (17)\nwhere graph-softmax assign 0 weight to non-neighboring vi-\nsual nodes and apply softmax like Equ. (15) based on G.\nOverall, our Multi-Head LCCA (MHLCCA) can be formu-\nlated as\nMHLCCA(Q,K,V ) = Concat(head1,··· ,headh)WO,\n(18)\nheadi = LCCA(QWQ\ni ,KW K\ni ,VW V\ni ,posq,posk,Ω,G),\n(19)\nwhere\nLCCA(Q,K,V,pos q,posk,Ω,G) =\ngraph-softmax\nG\n((Q+ posq)(K+ posk)T\n√dk\n+ log(Ω))V.\n(20)\nIn this stage, the grid features and region features serve as\nthe source ﬁeld and target ﬁeld alternately. For the l-th out-\nput of DWSA:\nM(l)\nr = MHLCCA(C\n′(l)\nr ,C\n′(l)\ng ,C\n′(l)\ng ,RPE,GPE,Ωrg,G),\n(21)\nM(l)\ng = MHLCCA(C\n′(l)\ng ,C\n′(l)\nr ,C\n′(l)\nr ,GPE,RPE,Ωgr,G),\n(22)\nwhere Ωrg is the relative position matrix between regions\nand grids and Ωgr is the relative position matrix between\ngrids and regions.\nBy LCCA, we embed regions into grids and vise versa\nto reinforce two kinds of features. Speciﬁcally, grid fea-\ntures attend to regions to get high-level object information,\nwhile regions attend to grids to supplement detailed and con-\ntextual information. With the geometric alignment graph,\nLCCA constrains information from semantically unrelated\nvisual features to eliminate semantic noises and apply cross-\nattention effectively.\nNote that a region can align with one or more grids while a\ngrid can align with zero or more regions. There might exist a\ngrid that aligns with no region. So we create a self-connected\nedge for each node in the geometric alignment graph. Be-\nsides, self-connected edges give the attention module an ex-\ntra choice of not attending to any other features. In the l-th\nlayer, the attention module is followed by two independent\nFFN like in DWSA:\nH(l+1)\nr = FFN′\nr(M(l)\nr ), (23)\nH(l+1)\ng = FFN′\ng(M(l)\ng ). (24)\nNote that the output of LCCA serves as the input of DWSA.\nAfter multi-layer encoding, grid features and region features\nare concatenated and fed into decoder layers.\nObjectives\nGiven ground truth sequence y∗\n1:T and a captioning model\nwith parameters θ. We optimize the following cross-entropy\n(XE) loss:\nLXE = −\nT∑\nt=1\nlog(pθ(y∗\nt|y∗\n1:t−1)). (25)\nThen we continually optimize the non-differentiable\nCIDER-D score by Self-Critical Sequence Training (Rennie\net al. 2017) (SCST) following (Cornia et al. 2020):\n∇θLRL(θ) = −1\nk\nk∑\ni=1\n(r(yi\n1:T) −b)∇θlog pθ(yi\n1:T), (26)\nwhere k is the beam size, r is the CIDEr-D score function,\nand b= (∑\nir(yi\n1:T))/kis the baseline.\nExperiments\nDatasets\nWe conduct our experiments on the benchmark image cap-\ntioning dataset COCO (Lin et al. 2014). The dataset contains\n123,287 images, each annotated with 5 different captions.\nFor ofﬂine evaluation, we follow the widely adopted Karpa-\nthy split (Karpathy and Fei-Fei 2015), where 113,287, 5,000,\n5,000 images are used for training, validation, and testing\nrespectively. We also upload generated captions of COCO\nofﬁcial testing set for online evaluation.\nExperimental settings\nTo extract visual features, we use the pre-trained Faster-\nRCNN (Ren et al. 2015) provided by (Jiang et al. 2020), that\nuses delated stride-1 C5 backbone and 1 ×1 RoIPool with\ntwo FC layers as the detection head to train Faster R-CNN\non the VG dataset. In the feature extraction stage, it removes\nthe delation and uses a normal C5 layer to extract grid fea-\ntures. For grid features, we leverage their grid features and\nFigure 4: Examples of image captioning results by standard\nTransformer and our proposed DLCT with ground truth sen-\ntences and the corresponding CIDEr scores. Generally, our\nmethod can generate more accurate and descriptive captions.\naverage-pool them to 7×7 grid size. For region features, we\nuse the same model to extract 2048-d features after the ﬁrst\nFC-layer of the detection head.\nIn our implementation, we setdmodel to 512 and the num-\nber of heads to 8. The number of layers for both encoder\nand decoder is set to 3. In the XE pre-training stage, we\nwarm up our model for 4 epochs with the learning rate lin-\nearly increased to 1 ×10−4. Then we set the learning rate\nto 1 ×10−4 between 5 ∼10 epoches, 2 ×10−6 between\n11 ∼12 epoches, 4 ×10−7 afterwards. The batch size is\nset to 50. After the 18-epoch XE pre-training stage, we start\nto optimize our model with CIDEr reward with 5 ×10−6\nlearning rate and 100 batch size. We use Adam optimizer\nin both stages and the beam size is set to 5. Following the\nstandard evaluation criterion, we utilize BLEU@N (Pap-\nineni et al. 2002), METEOR (Banerjee and Lavie 2005),\nROUGE-L (Lin 2004), CIDEr (Vedantam, Lawrence Zit-\nnick, and Parikh 2015), and SPICE (Anderson et al. 2016)\nto evaluate our model.\nPerformance Comparison\nOfﬂine Evaluation Table 1 summarizes the performance\nof the state-of-the-art models and our approach on the of-\nﬂine test split. We also report the results of ensembled mod-\nels for a comprehensive comparison. The compared models\ninclude: SCST (Rennie et al. 2017), Up-Down (Anderson\net al. 2018), HAN (Wang, Chen, and Hu 2019), GCN-LSTM\n(Yao et al. 2018), SGAE (Yang et al. 2019), ORT (Herdade\net al. 2019), SRT (Wang et al. 2020), AoA (Huang et al.\n2019), HIP (Yao et al. 2019), M2 (Cornia et al. 2020) and\nSingle Model Ensemble Model\nModel B-1 B-4 M R C S B-1 B-4 M R C S\nSCST (ResNet-101)cvpr2017 - 34.2 26.7 57.7 114.0 - - 35.4 27.1 56.6 117.5 -\nUp-Down (ResNet-101)cvpr2018 79.8 36.3 27.7 56.9 120.1 21.4 - - - - - -\nHAN (ResNet-101)aaai2019 80.9 37.6 27.8 58.1 121.7 21.5 - - - - - -\nGCN-LSTM (ResNet-101)eccv2018 80.5 38.2 28.5 58.5 128.3 22.0 80.9 38.3 28.6 58.5 128.7 22.1\nSGAE (ResNet-101)cvpr2019 80.8 38.4 28.4 58.6 127.8 22.1 81.1 39.0 28.4 58.9 129.1 22.2\nORT (ResNet-101)nips2019 80.5 38.6 28.7 58.4 127.8 22.1 - - - - - -\nSRT (ResNet-101)aaai2020 80.3 38.5 28.7 58.4 129.1 22.4 - - - - - -\nAoA (ResNet-101)iccv2019 80.2 38.9 29.2 58.8 129.8 22.4 81.6 40.2 29.3 59.4 132.0 22.8\nAoA (ResNeXt-101 Grid)iccv2019 80.7 39.0 28.9 58.7 129.5 22.6 - - - - - -\nHIP (SENet-154)iccv2019 - 39.1 28.9 59.2 130.6 22.3 - - - - - -\nM2 (ResNet-101)cvpr2020 80.8 39.1 29.2 58.6 131.2 22.6 82.0 40.5 29.7 59.5 134.5 23.5\nM2 (ResNeXt-101 Region)cvpr2020 80.6 38.8 29.0 58.4 130.8 22.4 - - - - - -\nM2 (ResNeXt-101 Grid)cvpr2020 80.8 38.9 29.1 58.5 131.7 22.6 - - - - - -\nX-Transformer (ResNet-101)cvpr2020 80.9 39.7 29.5 59.1 132.8 23.4 81.7 40.7 29.9 59.7 135.3 23.8\nX-Transformer (ResNeXt-101 Grid)cvpr2020 81.0 39.7 29.4 58.9 132.5 23.1 - - - - - -\nOurs (ResNeXt-101) 81.4 39.8 29.5 59.1 133.8 23.0 82.2 40.8 29.9 59.8 137.5 23.3\nTable 1: Performance comparisons on COCO Karpathy test split. B-1, B-4, M, R, C, and S are short for BLEU-1, BLEU-4,\nMETEOR, ROUGE, CIDEr, SPICE scores, respectively. Note that 4 models are used for the ensemble. The backbone is listed\nin brackets.\nModel B-1 B-2 B-3 B-4 M R C\nc5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40\nSCST (ResNet-101) 78.1 93.7 61.9 86.0 47.0 75.9 35.2 64.5 27.0 35.5 56.3 70.7 114.7 116.7\nUp-Down (ResNet-101) 80.2 95.2 64.1 88.8 49.1 79.4 36.9 68.5 27.6 36.7 57.1 72.4 117.9 120.5\nHAN (ResNet-101) 80.4 94.5 63.8 87.7 48.8 78.0 36.5 66.8 27.4 36.1 57.3 71.9 115.2 118.2\nGCN-LSTM (ResNet-101) 80.8 95.2 65.5 89.3 50.8 80.3 38.7 69.7 28.5 37.6 58.5 73.4 125.3 126.5\nSGAE (ResNet-101) 81.0 95.3 65.6 89.5 50.7 80.4 38.5 69.7 28.2 37.2 58.6 73.6 123.8 126.5\nAoA (ResNet-101) 81.0 95.0 65.8 89.6 51.4 81.3 39.4 71.2 29.1 38.5 58.9 74.5 126.9 129.6\nHIP (SENet-154) 81.6 95.9 66.2 90.4 51.5 81.6 39.3 71.0 28.8 38.1 59.0 74.1 127.9 130.2\nM2 (ResNet-101) 81.6 96.0 66.4 90.8 51.8 82.7 39.7 72.8 29.4 39.0 59.2 74.8 129.3 132.1\nX-Transformer (ResNet-101) 81.3 95.4 66.3 90.0 51.9 81.7 39.9 71.8 29.5 39.0 59.3 74.9 129.3 131.4\nX-Transformer (SENet-154) 81.9 95.7 66.9 90.5 52.4 82.5 40.3 72.4 29.6 39.2 59.5 75.0 131.1 133.5\nDLCT (ResNeXt-101) 82.0 96.2 66.9 91.0 52.3 83.0 40.2 73.2 29.5 39.1 59.4 74.8 131.0 133.4\nDLCT (ResNeXt-152) 82.4 96.6 67.4 91.7 52.8 83.8 40.6 74.0 29.8 39.6 59.8 75.3 133.3 135.4\nTable 2: COCO online leaderboard of published state-of-the art image captioning models. The backbone is listed in brackets.\nX-Transformer (Pan et al. 2020).\nAs shown in Table 1, our single model consistently ex-\nhibits better performance than the others. Our DLCT sur-\npasses all the other models in terms of BLEU-1, BLEU-4,\nCIDEr while being comparable on Meteor and Rouge with\nthe strongest competitor X-Transformer. In sum, our DLCT\noutperforms X-Transformer in most of the metrics and per-\nforms slightly worse in SPICE. The CIDEr score of our\nDLCT reaches 133.8%, which advances X-Transformer by\n1%. The boost of performance demonstrates the advantages\nof our DLCT which uses the complementary appearance and\ngeometry features of regions and grids, and models intra-\nand inter-level for detailed and comprehensive visual repre-\nsentations. Our ensembled model achieves the best results\nin BLEU-1, BLEU-4, Rouge and a particularly high score in\nCIDEr. Our Meteor score is comparable with the best model\nas well while the SPICE score is slightly worse. For a fair\ncomparison, we also run M2 (Cornia et al. 2020) based on\nour features. The results show that our DLCT still outper-\nforms M2 in all metrics.\nOnline Evaluation We submit the generated captions on\nthe ofﬁcial testing set to the online testing server and re-\nport the results in Table 2, which shows the performance\nleaderboard with 5 reference captions (c5) and 40 reference\ncaptions (c40). For online evaluation, we ensemble 4 mod-\nels and adopt two different backbones: ResNeXt-101 and\nResNeXt-152 (Xie et al. 2017). Compared to all the other\nstate-of-the-arts, our model with ResNeXt-152 achieves the\nbest performance in all metrics. Notably, our model with\nthe ResNeXt-101 can achieve comparable performance to\nX-Transformer with SENet-154 (Hu et al. 2020).\nAblation Study\nWe conduct several ablative studies to quantify the contribu-\ntion of each design in our model.\nFeatures To better understand the effect of our features,\nwe conduct several experiments on our features using Stan-\ndard Transformer as shown in Table 3. As we can see, the\nresults of every single feature and concatenation of both fea-\ntures are trivial and our approach with both features can\nachieve much better results.\nCRA To better demonstrate the effectiveness of CRA,\nwe conduct several ablative experiments as shown in Ta-\nble 4. CRA can improve the performance of both the model\nwith grid feature and the model with region feature. And it\nFigure 5: Attention visualization of region-based Transformer (a) and our DLCT (b). For each word, we show top-3 attended\nregions (red, blue, green respectively) and the attention heatmap on grids (only available in DLCT) with the highest attention\nweight in the title. Both Transformer and our DLCT can attend to corresponding regions when generating words. When gener-\nating words like “yellow” and “tracks”, our DLCT can attend to corresponding grids with detailed and contextual information.\nB-1 B-4 M R C S\nGrid (G) 81.2 39.0 29.0 58.6 131.2 22.4\nRegion (R) 80.1 39.0 28.9 58.6 130.1 22.4\nG + R 80.9 38.9 29.2 58.6 131.6 22.7\nDLCT (G+R) 81.4 39.8 29.5 59.1 133.8 23.0\nTable 3: Performance comparison of different feature set-\ntings.\nFeature Model B-1 B-4 M R C S\nG Transformer 81.2 39.0 29.0 58.6 131.2 22.4\nTransformer+PE 81.2 39.0 29.2 58.9 131.7 22.6\nTransformer+CRA81.1 39.3 29.4 58.9 132.5 22.9\nR Transformer 80.1 39.0 28.9 58.6 130.1 22.4\nTransformer+PE 80.6 38.3 29.0 58.4 129.7 22.5\nTransformer+CRA80.9 39.0 29.2 58.6 131.0 22.5\nG+R DLCT w/o CRA 81.0 39.3 29.3 58.8 133.0 23.0\nDLCT 81.4 39.8 29.5 59.1 133.8 23.0\nTable 4: Performance with / without CRA for grids(G) and\nregions(R). PE represents traditional positional encoding\nmethod which directly adds positional encoding to inputs.\ncan also improve the performance by cooperating with our\nLCCA, which boosts the CIDEr-D score from 133.0% to\n133.8%. By integrating absolute and relative location infor-\nmation, the captioning model can better understand the ap-\npearance features and the relationships among them.\nLCCA We also conduct several experiments to demon-\nstrate the effectiveness of our LCCA, which are shown in\nTable 5. Two alternatives are considered: one is our DLCT\nwithout LCCA, and the other is LCCA with a complete\nbipartite graph (CBG) in which cross attention is applied\nbetween all grid nodes and region nodes. They both show\nworse performance than LCCA, which demonstrate the su-\nperiority of our LCCA. Note that DLCT with CBG is even\nworse than standard Transformer with grid feature inputs,\nwhich shows the damage of semantic noises introduced by\ncoarsely modeling relationships between regions and grids.\nB-1 B-4 M R C S\nDLCT w/o LCCA 81.2 39.2 29.2 58.6 132.6 22.8\nLCCA + CBG 80.8 38.7 29.0 58.7 130.8 22.7\nDLCT 81.4 39.8 29.5 59.1 133.8 23.0\nTable 5: Performance with / without LCCA, where CBG\nmeans the complete bipartite graph.\nQualitative results and visualization\nFig. 4 illustrates several example image captions generated\nby Transformer and DLCT. As indicated by these examples,\ngenerally, our DLCT can grab detailed and contextual infor-\nmation to generate more accurate and descriptive captions.\nIn order to better qualitatively evaluate the encoded visual\nrepresentations, we visualize the contribution of each visual\nfeature to the model output in Fig 5. Technically, we aver-\nage attention weights of 8 heads in the last Enc-Dec Multi-\nhead Attention Layer. We can see that both Transformer\nand DLCT are able to attend to the corresponding regions\nwhen generating words. In addition, our DLCT can attend\nto corresponding grids when it generates the word “blue”\nand “yellow”. When generating the word “tracks”, the atten-\ntion heatmap on grids provides a more ﬁne-grained semantic\nsegmentation of tracks, which demonstrates the advantages\nof our DLCT.\nConclusion\nIn this paper, we proposed a Dual-Level Collaborative\nTransformer to achieve the complementarity of region and\ngrid features for image captioning. Our model integrates ap-\npearance and geometry features of regions and grids by ap-\nplying intra-level fusion via Comprehensive Relation Atten-\ntion (CRA) and Dual-Way Self Attention (DWSA). We also\nproposed a geometric alignment graph to apply Locality-\nConstrained Cross Attention (LCCA) which helps reinforce\ntwo kinds of features effectively and address the issue of se-\nmantic noises aroused by the direct fusion of two sources of\nfeatures. Extensive results demonstrate the superiority of our\napproach that achieves a new state-of-the-art on both ofﬂine\nand online test splits. In our feature work, we plan to extend\nthe proposed collaborative features to other multi-media ar-\neas which requires detailed and contextual information.\nAcknowledgments\nThis work is supported by the National Science Fund\nfor Distinguished Young (No.62025603), the National Nat-\nural Science Foundation of China (No.U1705262, No.\n62072386, No. 62072387, No. 62072389, No. 62002305,\nNo.61772443, No.61802324 and No.61702136) and and\nGuangdong Basic and Applied Basic Research Foundation\n(No.2019B1515120049).\nReferences\nAnderson, P.; Fernando, B.; Johnson, M.; and Gould, S.\n2016. Spice: Semantic propositional image caption evalu-\nation. In ECCV, 382–398. Springer.\nAnderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.;\nGould, S.; and Zhang, L. 2018. Bottom-up and top-down at-\ntention for image captioning and visual question answering.\nIn CVPR.\nBanerjee, S.; and Lavie, A. 2005. METEOR: An automatic\nmetric for MT evaluation with improved correlation with hu-\nman judgments. In ACL, 65–72.\nCornia, M.; Stefanini, M.; Baraldi, L.; and Cucchiara, R.\n2020. Meshed-Memory Transformer for Image Captioning.\nIn CVPR.\nGuo, L.; Liu, J.; Zhu, X.; Yao, P.; Lu, S.; and Lu, H. 2020.\nNormalized and Geometry-Aware Self-Attention Network\nfor Image Captioning. In CVPR.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. In CVPR, 770–778.\nHerdade, S.; Kappeler, A.; Boakye, K.; and Soares, J. 2019.\nImage Captioning: Transforming Objects into Words. In\nNeurIPS.\nHu, J.; Shen, L.; Albanie, S.; Sun, G.; and Wu, E. 2020.\nSqueeze-and-Excitation Networks. IEEE Transactions on\nPattern Analysis and Machine Intelligence42: 2011–2023.\nHuang, L.; Wang, W.; Chen, J.; and Wei, X.-Y . 2019. Atten-\ntion on Attention for Image Captioning. In ICCV.\nJiang, H.; Misra, I.; Rohrbach, M.; Learned-Miller, E.; and\nChen, X. 2020. In Defense of Grid Features for Visual Ques-\ntion Answering. In CVPR.\nKarpathy, A.; and Fei-Fei, L. 2015. Deep visual-semantic\nalignments for generating image descriptions. In CVPR,\n3128–3137.\nLin, C.-Y . 2004. Rouge: A package for automatic evaluation\nof summaries. In Text summarization branches out, 74–81.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll ´ar, P.; and Zitnick, C. L. 2014. Microsoft\ncoco: Common objects in context. In ECCV, 740–755.\nSpringer.\nLu, J.; Xiong, C.; Parikh, D.; and Socher, R. 2017. Knowing\nwhen to look: Adaptive attention via a visual sentinel for\nimage captioning. In CVPR.\nPan, Y .; Yao, T.; Li, Y .; and Mei, T. 2020. X-Linear Attention\nNetworks for Image Captioning. In CVPR, 10971–10980.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBLEU: a method for automatic evaluation of machine trans-\nlation. In ACL, 311–318.\nRen, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster r-\ncnn: Towards real-time object detection with region proposal\nnetworks. In NeurIPS.\nRennie, S. J.; Marcheret, E.; Mroueh, Y .; Ross, J.; and Goel,\nV . 2017. Self-critical sequence training for image caption-\ning. In CVPR, 7008–7024.\nSeo, P. H.; Sharma, P.; Levinboim, T.; Han, B.; and Soricut,\nR. 2020. Reinforcing an Image Caption Generator Using\nOff-Line Human Feedback. In AAAI.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998–6008.\nVedantam, R.; Lawrence Zitnick, C.; and Parikh, D. 2015.\nCider: Consensus-based image description evaluation. In\nCVPR, 4566–4575.\nVinyals, O.; Toshev, A.; Bengio, S.; and Erhan, D. 2015.\nShow and tell: A neural image caption generator. In CVPR,\n3156–3164.\nWang, L.; Bai, Z.; Zhang, Y .; and Lu, H. 2020. Show, Re-\ncall, and Tell: Image Captioning with Recall Mechanism. In\nAAAI.\nWang, W.; Chen, Z.; and Hu, H. 2019. Hierarchical attention\nnetwork for image captioning. In AAAI.\nXie, S.; Girshick, R. B.; Doll´ar, P.; Tu, Z.; and He, K. 2017.\nAggregated Residual Transformations for Deep Neural Net-\nworks. CVPR 5987–5995.\nXu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A.; Salakhudi-\nnov, R.; Zemel, R.; and Bengio, Y . 2015. Show, attend and\ntell: Neural image caption generation with visual attention.\nIn ICML.\nYang, X.; Tang, K.; Zhang, H.; and Cai, J. 2019. Auto-\nencoding scene graphs for image captioning. In CVPR,\n10685–10694.\nYao, T.; Pan, Y .; Li, Y .; and Mei, T. 2018. Exploring visual\nrelationship for image captioning. In ECCV, 684–699.\nYao, T.; Pan, Y .; Li, Y .; and Mei, T. 2019. Hierarchy Parsing\nfor Image Captioning. ICCV 2621–2629.\nZhao, W.; Wu, X.; and Zhang, X. 2020. MemCap: Memo-\nrizing Style Knowledge for Image Captioning. In AAAI.\nZhou, L.; Palangi, H.; Zhang, L.; Hu, H.; Corso, J. J.; and\nGao, J. 2020. Uniﬁed Vision-Language Pre-Training for Im-\nage Captioning and VQA. In AAAI.",
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.9243345260620117
    },
    {
      "name": "Computer science",
      "score": 0.7972117066383362
    },
    {
      "name": "Transformer",
      "score": 0.6827282905578613
    },
    {
      "name": "Grid",
      "score": 0.6409556865692139
    },
    {
      "name": "Locality",
      "score": 0.6075239777565002
    },
    {
      "name": "Graph",
      "score": 0.4463115930557251
    },
    {
      "name": "Image (mathematics)",
      "score": 0.41760870814323425
    },
    {
      "name": "Code (set theory)",
      "score": 0.4134905934333801
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3867553174495697
    },
    {
      "name": "Information retrieval",
      "score": 0.38368484377861023
    },
    {
      "name": "Data mining",
      "score": 0.347499281167984
    },
    {
      "name": "Theoretical computer science",
      "score": 0.23619848489761353
    },
    {
      "name": "Programming language",
      "score": 0.08742168545722961
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 24
}