{
  "title": "Enhancing Language Models with Plug-and-Play Large-Scale Commonsense",
  "url": "https://openalex.org/W3197474963",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5103026537",
      "name": "Wanyun Cui",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5103256609",
      "name": "Xingran Chen",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2925905394",
    "https://openalex.org/W2406611863",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W2967584709",
    "https://openalex.org/W3093166897",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W2806710540",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W2945290257",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W3014521650",
    "https://openalex.org/W3034503989",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W2952570576",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W3034555021",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2964532710",
    "https://openalex.org/W3021393209",
    "https://openalex.org/W2798280648",
    "https://openalex.org/W2991612931",
    "https://openalex.org/W2579903943",
    "https://openalex.org/W3098266846",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2807873315",
    "https://openalex.org/W2963259903",
    "https://openalex.org/W3114916066"
  ],
  "abstract": "We study how to enhance language models (LMs) with textual commonsense knowledge. Previous work (e.g., KnowBERT) has focused on the integrating entity knowledge from knowledge graphs. In order to introduce the external entity embeddings, they learn to jointly represent the original sentences and external knowledge by pre-training on a large scale corpus. However, when switching to textual commonsense, unlike the light entity embeddings, the encoding of commonsense descriptions is heavy. Therefore, the pre-training for learning to jointly represent the target sentence and external commonsense descriptions is unaffordable. On the other hand, since pre-trained LMs for representing the target sentences alone are readily available, is it feasible to introduce commonsense knowledge in downstream tasks by fine-tuning them only? In this paper, we propose a plug-and-play method for large-scale commonsense integration without pre-training. Our method is inspired by the observation that in the regular fine-tuning for downstream tasks where no external knowledge was introduced, the variation in the parameters of the language model was minor. Our method starts from a pre-trained LM that represents the target sentences only (e.g., BERT). We think that the pre-training for joint representation learning can be avoided, if the joint representation reduces the impact of parameters on the starting LM. Previous methods such as KnowBERT proposed complex modifications to the vanilla LM to introduce external knowledge. Our model (Cook-Transformer, COmmOnsense Knowledge-enhanced Transformer), on the other hand, hardly changes the vanilla LM except adding a knowledge token in each Transformer layer. In a variety of experiments, COOK-Transformer-based BERT/RoBERTa improve their effect without any pre-training.",
  "full_text": "Enhancing Natural Language Representation with Large-Scale\nOut-of-Domain Commonsense\nWanyun Cui, Xingran Chen\nShanghai University of Finance and Economics\ncui.wanyun@sufe.edu.cn, xingran.chen.sufe@gmail.com\nAbstract\nWe study how to enhance text representation\nvia textual commonsense. We point out that\ncommonsense has the nature of domain dis-\ncrepancy. Namely, commonsense has different\ndata formats and is domain-independent from\nthe downstream task. This nature brings chal-\nlenges to introducing commonsense in general\ntext understanding tasks. A typical method\nof introducing textual knowledge is continu-\ning pre-training over the commonsense corpus.\nHowever, it will cause catastrophic forgetting\nto the downstream task due to the domain dis-\ncrepancy. In addition, previous methods of di-\nrectly using textual descriptions as extra input\ninformation cannot apply to large-scale com-\nmonsense.\nIn this paper, we propose to use large-scale\nout-of-domain commonsense to enhance text\nrepresentation. In order to effectively incor-\nporate the commonsense, we proposed OK-\nTransformer (O ut-of-domain K nowledge en-\nhanced Transformer ). OK-Transformer ef-\nfectively integrates commonsense descriptions\nand enhances them to the target text repre-\nsentation. In addition, OK-Transformer can\nadapt to the Transformer-based language mod-\nels (e.g. BERT, RoBERTa) for free, without\npre-training on large-scale unsupervised cor-\npora. We have veriﬁed the effectiveness of\nOK-Transformer in multiple applications such\nas commonsense reasoning, general text clas-\nsiﬁcation, and low-resource commonsense set-\ntings. 1\n1 Introduction\nAlthough unsupervised language models have\nachieved big success on many tasks (Devlin et al.,\n2019), they are incapable of learning low-frequency\nknowledge. For example, in the masked language\nmodel task in Fig. 1, even if we replace “Kevin\nwas” (left) with “Jim was” (right), BERT (Devlin\n1The code is available in https://github.com/\nchenxran/ok-transformer\net al., 2019) still predicts the masked word as sick,\ncrying, dying, etc. This is because similar texts\nin its training corpus rarely describe the subject\nof “comforted”. To improve the model’s ability to\ngeneralize and understand low-frequency knowl-\nedge, we propose to incorporate commonsense into\nlanguage models. In Fig. 1, to make correct predic-\ntions, we need to enhance the language model with\nthe commonsense c1.\nHowever, commonsense has the nature of do-\nmain discrepancy. The downstream task and the\ncommonsense knowledge have distribution discrep-\nancies. Taking the commonsense knowledge base\nwe use (i.e. ATOMIC2020 (Hwang et al., 2020)) as\nan example, the distribution discrepancy is specif-\nically manifested in (1) their data formats. The\nformat of a commonsense description usually be-\nlongs to some speciﬁc patterns (e.g. “... As a result\n...”, “... Because ...”), while the downstream tasks\ncan have arbitrary patterns. (2) The commonsense\nbelongs to the domain of event causality, while the\ndownstream tasks may belong to arbitrary domains.\nHere we highlight the challenges caused by the\ndomain discrepancy. To introduce external tex-\ntual knowledge to a pre-trained language model, a\ncommon practice is to continue pre-training the lan-\nguage model on the corpus of the external knowl-\nedge (Gururangan et al., 2020; Sun et al., 2019).\nHowever, the study (Gururangan et al., 2020) also\nfound that continuing pre-training requires external\nknowledge and downstream tasks to have similar\ndomains. Due to its domain discrepancy, introduc-\ning commonsense through continuing pre-training\nwill cause catastrophic forgetting to downstream\ntasks, thereby injuring the effectiveness. We have\nveriﬁed this empirically in Sec 6.3. Therefore, the\ndomain discrepancy prevents us from introducing\ncommonsense by continuing pre-training.\nTo enhance the representation of the target text\nwith external commonsense, we propose to directly\nuse its candidate commonsense as an extra input.\narXiv:2109.02572v3  [cs.CL]  13 Mar 2022\n0.064\n0.071\n0.110\ndying\ncrying\nsick\nJim comforted Kevin because Kevin was [MASK]. \nc3:  If PersonX comfort PersonY, then \nPersonX want to keep PersonY calm\nc2:  If PersonX comfort PersonY, then \nPersonX want to be PersonY’s friend\nc1 : If PersonX comfort PersonY, then \nPersonX is sympathetic\n0.043\n0.055\n0.092\ndying\ncrying\nsick\nJim comforted Kevin because Jim was [MASK]. \nCandidate commonsense for “comforted” from Atomic2020\nFigure 1: The prediction of [MASK] by BERT. BERT cannot distinguish betweenJim and Kevin in Jim comforted\nKevin because.\nOur setup is different from a typical natural lan-\nguage understanding setup since the latter one only\ntakes the target text as the input (Devlin et al.,\n2019). We argue that our setup – where the com-\nmonsense is introduced explicitly as input – is a\nmore practicable setup to introduce out-of-domain\ncommonsense that cannot be learned through pre-\ntraining. As far as we know, ExpBERT (Murty\net al., 2020) is the closest setup to us. It also uses ex-\nternal knowledge (manually constructed templates)\nas the input.\nAnother challenge is the scale of the common-\nsense. Although ExpBERT also allows extra tex-\ntual commonsense as input, it only captures small-\nscale commonsense with a ﬁxed size. In addi-\ntion, when we introduce commonsense from a\nlarge-scale knowledge base for general purpose\n(i.e. ATOMIC2020), unrelated commonsense (e.g.\nc2 and c3 in Fig. 1) will certainly occur. How-\never, ExpBERT lacks the ability to distinguish re-\nlated and unrelated commonsense. Therefore, the\npower of large-scale commonsense knowledge was\nrestricted in ExpBERT. We will verify this empiri-\ncally in Sec 6.3.\nIn order to incorporate the large-scale out-\nof-domain commonsense, we propose the OK-\nTransformer (Out-of-domain Knowledge enhanced\nTransformer) on the basis of Transformer (Vaswani\net al., 2017). OK-Transformer has two modules.\nThe knowledge enhancement module is used to\nencode the target text with commonsense, and the\nknowledge integration module is used to encode\nand integrate all candidate commonsense. OK-\nTransformer has two advantages. First, it fully\nrepresents the contextual information of the tex-\ntual commonsense. Second, it can be adapted to\nexisting pre-trained language models (e.g. BERT\nand RoBERTa) for free. That is, we are able to\nadapt OK-Transformer to the pre-trained language\nmodels, without pre-training OK-Transformer over\nlarge-scale unsupervised corpora from scratch.\nSome other methods are related to our work,\nsuch as introducing structured knowledge (Peters\net al., 2019; Zhang et al., 2019; Guan et al., 2020;\nZhou et al., 2018) and plain text knowledge (Guu\net al., 2020) in language models. These methods\ndo not represent the speciﬁc inductive bias of com-\nmonsense knowledge and therefore are not suitable\nto introduce commonsense. We will compare these\nstudies with more details in Sec 2.\n2 Related work\nIn this section, we compare different ways to in-\ntroduce knowledge into language models. We di-\nvide the knowledge introduction methods into (1)\ncontinuing pre-training method (Gururangan et al.,\n2020; Sun et al., 2019) and (2) explicit introduction\nin the downstream task (Guu et al., 2020; Murty\net al., 2020).\nContinuing pre-training the language model is\neffective when the external knowledge is similar\nto the downstream task (Gururangan et al., 2020;\nSun et al., 2019). However, commonsense and\ndownstream tasks have domain discrepancies, so\ncontinuing pre-training is unsuitable for introduc-\ning commonsense. We have empirically veriﬁed\nthis in Sec 6.3.\nIntroducing explicit knowledge in down-\nstream tasks We classify the knowledge into struc-\ntured knowledge, plain text, and semi-structured\nknowledge, depending on its form. The entries\nof structured knowledge are represented as in-\ndividual embeddings (Peters et al., 2019; Zhang\net al., 2019; Guan et al., 2020; Zhou et al., 2018),\nwhile commonsense descriptions in this paper can\nbe represented more accurately by the contextual\ninformation of their word sequences.\n3 Problem Setup: Commonsense as the\nExtra Input\nWe consider a text classiﬁcation task where the text\nxand its label yare provided for training. Assum-\ning that the candidate commonsense descriptions\nfor enhancing xcome from a large-scale common-\nsense knowledge base (i.e. ATOMIC2020), we\nretrieve candidate commonsense for xas the extra\ninput. We denote the commonsense descriptions\nfor x as cs(x) = {c1 ···cn}, where each ci is a\ncommonsense description. The retrieval process\nwill be shown in Sec 6. The model takes bothxand\ncs(x) as the input. Since ATOMIC2020 contains\nif-then knowledge for general purposes, the prob-\nlem setup can be expanded to a broad range of text\nunderstanding tasks. The goal of training is to ﬁnd\nparameter θthat minimizes the loss of training ex-\namples given the texts and candidate commonsense\ndescriptions:\narg minθE(x,y)∈trainL(f(x,cs(x)|θ),y) (1)\nwhere f(·|θ) is the model taking x and cs(x) as\ninputs, Lis the loss function.\n4 OK-Transformer\nIn this section, we propose OK-Transformer based\non Transformer to introduce extra commonsense\ndescriptions. We ﬁrst show OK-Transformer on an\nabstract level in Sec 4.1. Then we elaborate two\nmodules within it, i.e. knowledge enhancement\nand knowledge integration, in Sec 4.2 and Sec 4.3,\nrespectively.\n4.1 Framework\nIn this subsection, we show how our OK-\nTransformer works at an abstract level. For the\ntarget sentence x, OK-Transformer takes both x\nand cs(x) as inputs. To incorporate all the in-\nformation of x and cs(x), the OK-Transformer\ncontains three vanilla Transformers, denoted by\nTransformer(1)(2)(3). The knowledge enhance-\nment module uses Transformer(1) to encode the\ntarget text. Compared with the vanilla Trans-\nformer, Transformer(1) leverages a new knowl-\nedge token to represent the commonsense that in-\nteracts with other words. The knowledge inte-\ngration module encodes each individual common-\nsense description by Transformer(2), and then in-\ntegrates all candidate commonsense descriptions\nby Transformer(3). This is shown in Fig. 2.\n…\nTransformer(3)\nAdd & Norm\nMulti-Head\nAttention\nTransformer(1)\nAdd & Norm\nAdd & Norm\nMulti-Head\nAttention\nFeed\nForward\nx: Jim  comforted Kevin[k]\nemb1 embnemb0\nNull\nq K V\n[ki-1,Hi-1]\nemb\n[ki’,Hi’]\n[ki,Hi]\nc1: PersonX comfort PersonY to       keep   PersonY calm\nTransformer(2)\nVanilla Transformer\n… Transformer(2)\nVanilla Transformer\ncn\nAdd & Norm\nFeed\nForward\ncsemb\n…\n…\nFigure 2: OK-Transformer. Transformer(1) encodes\nthe target text x with enhanced commonsense ki.\nTransformer(2) encodes each individual commonsense\ndescription. Transformer(3) integrates all candidate\ncommonsense descriptions and transfers knowledge to\nTransformer(1).\n4.2 Knowledge Enhancement Module\nThe knowledge enhancement module allows com-\nmonsense knowledge to enhance the representation\nof the target text.\nInteraction between words and common-\nsense. We use Transformer(1) to represent the\ninteraction between words of the target text x.\nIn addition, we introduce a special token [k] to\nrepresent the commonsense knowledge. We de-\nnote it as the knowledge token. Transformer(1)\nencodes all words and the knowledge token to-\ngether via multi-head attention. Formally, given\nword sequence x = w1,··· ,wn, Transformer(1)\naccepts a sequence of n+ 1word-piece tokens:\n[k], w1,···wn. We denote the knowledge em-\nbedding and word embeddings produced by the\ni-th layer of Transformer(1) as ki ∈ Rd and\nHi ∈ Rn×d, respectively. The Transformer(1)\nblock ﬁrst uses a multi-head self-attention layer\nfollowed by a residual connection and a layer nor-\nmalization to model their interactions:\nk′\ni,H′\ni = LayerNorm([ki−1,Hi−1]+\nMultiHeadAttn([ki−1,Hi−1],[ki−1,Hi−1],[ki−1,Hi−1]))\n(2)\nwhere [ki−1,Hi−1] ∈R(n+1)×d means appending\nki−1 at the front of Hi−1. [ki−1,Hi−1] is used as\nthe query, key, and value in the multi-head atten-\ntion.\nKnowledge update The vanilla Transformer\nprojects k′\ni, H′\ni in Eq. (2) to the output space with\na multi-layer perceptron neural network (MLP).\nCompared to the vanilla Transformer, we use an ex-\ntra update operation to update the knowledge token\nby the integrated commonsense knowledge after\nthe MLP. As in the vanilla Transformer, the update\nlayer is followed by a residual connection and a\nlayer normalization. This can be formulated by:\nki = LayerNorm(k′\ni + MLP(k′\ni) + csemb)\nHi = LayerNorm(H′\ni + MLP(H′\ni))\n(3)\nwhere csemb is the embedding of the commonsense\ncomputed by the knowledge integration module in\nSec 4.3.\n4.3 Knowledge Integration Module\nThe knowledge integration module encodes all can-\ndidate commonsense descriptions and integrates\nthem. We ﬁrst use Transformer(2) to represent\neach candidate commonsense description. Then,\nwe use Transformer(3) to integrate all candidate\ncommonsense, and transfer the integrated knowl-\nedge to the knowledge enhancement module.\nRepresenting single commonsense We use a\nvanilla Transformer as Transformer(2) to model\neach candidate commonsense description. For all\nthe retrieved commonsense cs(x) ={c1,··· ,cn},\nwe compute the embedding embj of each common-\nsense description cj by:\nembj = Transformer(2)(cj) (4)\nKnowledge integration We integrate all can-\ndidate commonsense by Transformer(3). Since\nnot all the candidate commonsense leads to high\nconﬁdence prediction as we have discussed in\nSec 1, we need to select relevant commonsense\nand ignore irrelevant commonsense. Transformer\nis adequate to conduct this selection. Speciﬁ-\ncally, in the query-key-value mechanism in Trans-\nformer, we use the embedding of the knowl-\nedge token in Transformer(1) as the query of\nTransformer(3). and the commonsense embed-\ndings by Transformer(2) as keys and values of\nTransformer(3). Then, we integrate representa-\ntions of all different commonsense descriptions\nbased on their similarities with the knowledge to-\nken.\nTransformer(3) also uses multi-head attention\nto allow the knowledge token to interact with the\ncandidate commonsense in multiple ways. The\noutput of multi-head self-attention is followed by a\nresidual connection and a layer normalization.\ncsemb =LayerNorm(ki−1\n+ MultiHeadAttn(ki−1,emb,emb)) (5)\nwhere emb = [emb1,··· ,embn] denotes the se-\nquence of embeddings of all candidate common-\nsense descriptions. We then apply a residual con-\nnection and a layer normalization to it.\nNull Commonsense Some target texts may not\nhave valid commonsense from ATOMIC2020 to\nenhance their representations. Therefore, we refer\nto the settings of REALM (Guu et al., 2020) to\nadd a null commonsense into the candidate com-\nmonsense of all target texts. We denote the null\ncommonsense as c0. Matching to the null common-\nsense indicates that the commonsense knowledge\nbase cannot help enhance the target text.\n5 Adaptation to Pre-trained Language\nModels\nIn this section, we take BERT as an example to\nillustrate how we adapt OK-Transfomer to ex-\nisting pre-trained language models. We denote\nthe adapted model as OK-BERT. An important\nmanifestation of the effectiveness of the Trans-\nformer structure is its applications in large-scale\npre-trained models (e.g. BERT, RoBERTa). In or-\nder to introduce external knowledge, many other\nstudies conduct training over large-scale unsuper-\nvised corpus (Peters et al., 2019; Xiong et al.,\n2019). However, OK-Transformer is able to di-\nrectly adapt to the existing pre-trained language\nmodels for free. In other words, when adapting\nOK-Transformer to OK-BERT, we directly use the\nparameters of each Transformer layer of BERT to\ninitialize the OK-Transformer layers of OK-BERT.\nThis property greatly improves the applicability of\nOK-BERT. In the rest of this section, we will de-\nscribe how Transformer(1), Transformer(2), and\nTransformer(3) are adapted respectively in Sec 5.1,\nand how to ﬁne-tune OK-BERT in Sec 5.2.\n5.1 Layer-by-Layer Adaptation\nThe OK-BERT we designed uses two origi-\nnal BERTs to serve as Transformer(1) and\nTransformer(2), respectively. We denote them\nas BERT1 and BERT2. We connect the\nTransformer(1) and Transformer(2) in the corre-\nsponding layer of each BERT by Transformer(3).\nTherefore, OK-BERT makes full use of the multi-\nlayer structure of BERT, while allowing common-\nsense in the knowledge token to fully interact with\nthe target text in each layer. The architecture is\nshown in Fig. 3.\ni-th\nlayer\n(i-1)-th\nlayer\nTransformer(1)\nTransformer(1)\nTransformer(1) *12\n…\n… …\n…\nBERT1 BERT2\n(i-1)-th\nlayer\nTransformer(2)\nTransformer(2) *12\n…\n…\nTransformer(2)\ni-th\nlayer\nTransformer(3) *12\nTransformer(3)\ni-th\nlayer\n(i-1)-th\nlayer\nTransformer(3)\nFigure 3: The architecture of OK-BERT. We only draw\nedges that connect to the i-th layer.\nTransformer(1) We adapt the Transformer\nof BERT1 to Transformer(1) in the knowl-\nedge enhancement module of OK-Transformer.\nNote that the original BERT’s tokens are\n[CLS] w1 ···wL [SEP] (for a single sentence) or\n[CLS] w1 ···wm [SEP] wm+1 ···wL [SEP] (for\na sentence pair). We follow (Wang et al.,\n2020) and use a special token [k] as the knowl-\nedge token. When tokenizing sentences, we\ninsert the [k] token after the [CLS] token\nfor each given text. In this way, the in-\nput tokens become [CLS] [k] w1 ···wL [SEP] or\n[CLS] [k] w1 ···wm [SEP] wm+1 ···wL [SEP] ,\nrespectively. This simple modiﬁcation allows us to\nuse [k] as the knowledge token in the knowledge\nenhancement module.\nTransformer(2) We adapt each Transformer\nlayer of BERT2 to the Transformer(2) layer. The\nadaptation is straightforward since Transformer(2)\nuses the vanilla Transformer structure. We use the\nencoding of the[CLS] token in each corresponding\nlayer as the commonsense representation embj to\nenhance the representation of the corresponding\nlayer in BERT1.\nTransformer(3) For each pair of correspond-\ning Transformer(1) and Transformer(2) from the\nsame layer, we use one Transformer(3) to connect\nthem to transfer the information from BERT2 to\nBERT1.\nIn summary, when adapting to BERT-base\nwith 12 Transformer layers, OK-BERT con-\ntains 12 Transformer(1) layers for BERT1,\n12 Transformer(2) layers for BERT2, and 12\nTransformer(3) layers for layer-wise knowledge\nintegration.\n5.2 Parameter Initialization and Model\nTraining\nIn our implementation, BERT1 and BERT2 have\nindependent parameters. We use the parameters of\nBERT to initialize both BERT1 and BERT2. The\nparameters of Transformer(3) layers are randomly\ninitialized. For downstream tasks, we then ﬁne-\ntune all the parameters in the fashion of end2end.\n6 Experiments\nWe evaluate the effectiveness of our proposed mod-\nels in three scenarios: cloze-style commonsense\nreasoning, text classiﬁcation, and low-resource\ncommonsense settings. All the experiments run\nover a computer with 4 Nvidia Tesla V100 GPUs.\nModels We consider adapting OK-Transformer\nto BERT and RoBERTa, which are denoted as OK-\nBERT and OK-RoBERTa, respectively. We use the\nBERT-base and RoBERTa-large from the Hugging-\nFace Transformer library (Wolf et al., 2020).\nImplementation details for candidate knowl-\nedge retrieval For a given text x, we retrieve can-\ndidate commonsense from ATOMIC2020. We\nuse the if-then descriptions in ATOMIC2020 (e.g.\nFig. 1). Since these descriptions cover 173k differ-\nent verb phrases – one of the fundamental elements\nof language – the retrieval is applicable to a broad\nrange of downstream text understanding tasks.\nWe use a simple retrieval method. We simply\nconsider word segments with window size 5 of the\ninput text x. All the commonsense descriptions\nmatching one of these text segments will be re-\ngarded as the candidate commonsense descriptions\nci ∈cs(x).\n6.1 Commonsense Reasoning\n6.1.1 Setup\nDatasets We consider the following commonsense\nreasoning benchmarks: WSC273 (Levesque et al.,\n2012), PDP (Morgenstern et al., 2016), Winogen-\nder (Rudinger et al., 2018), WinoGrande (Sak-\naguchi et al., 2019), CommonsenseQA (Talmor\net al., 2019) and PhysicalQA (Bisk et al., 2020).\nModel details Due to the different implemen-\ntations between (Kocijan et al., 2019b) and (Sak-\naguchi et al., 2019), in this paper, we also follow\ntheir settings to compare with them, respectively.\nFor (Kocijan et al., 2019b), we conduct disambigua-\ntion tasks directly through masked language mod-\neling in OK-BERT. For the latter one, we convert\ncloze-style problems to multiple-choice classiﬁca-\ntion problems in OK-RoBERTa. In particular, we\nreplace the target pronoun of one query sentence\nwith each candidate reference, then put the new\nsentences into the language model. We use a single\nlinear layer and a softmax layer over the encod-\ning of its [CLS] token to compute the probability\nof each new sentence, and select the one with the\nhighest probability as the pronoun disambiguation\nresult.\nHyperparameters of pre-training We fol-\nlow (Kocijan et al., 2019b; Sakaguchi et al., 2019)\nto ﬁrst pre-train models for 30 and 3 epochs over\nWSCR (Kocijan et al., 2019b) or WinoGrande (Sak-\naguchi et al., 2019), respectively. Then we ﬁne-\ntune models over speciﬁc tasks. We use AdamW\nas the optimizer with learning rate 5e-6, which is\nselected from {2e−5,1e−5,5e−6}. We set the\nbatch size to 8.\nModel WSC PDP\nKEE(Liu et al., 2016) 52.8 58.3\nWKH (Emami et al., 2018) 57.1 -\nMAS (Klein and Nabi, 2019) 60.3 68.3\nDSSM (Wang et al., 2019) 63.0 75.0\nLM(Trinh and Le, 2018) 63.8 70.0\nCSS (Klein and Nabi, 2020) 69.6 90.0\nGPT2 (Radford et al., 2019) 70.7 -\nBERT-large+WSCR (Kocijan et al., 2019b) 71.4 79.2\nHNN (He et al., 2019) 75.1 90.0\nHuman (Sakaguchi et al., 2019) 96.5 92.5\nBERT+WSCR 66.3 85.0\nOK-BERT+WSCR 67.4 86.7\nRoB.+WinoGrande 90.1 87.5\nOK-RoB.+WinoGrande 91.6 91.7\nTable 1: Results on WSC and PDP. RoB. denotes\nRoBERTa.\nModel WinoGen. WinoGran.\nWikiCREM (Kocijan et al., 2019a) 82.1 -\nWinoGrande (Sakaguchi et al., 2019) 94.6 79.3\nBERT+WSCR 68.2 51.4\nOK-BERT+WSCR 72.4 53.4\nRoB.+WinoGrande 94.6 79.3\nOK-RoB.+WinoGrande 96.2 79.6\nTable 2: Results on WinoGender and WinoGrande.\n6.1.2 Results\nWe compare our models with state-of-the-art com-\nmonsense reasoning models in Table 1, 2, and 3.\nModel CommonsenseQA PhysicalQA\nBERT 55.86 68.71\nOK-BERT 56.27 69.09\nRoBERTa 73.55 79.76\nOK-RoBERTa 75.92 80.09\nTable 3: Results on CommonsenseQA and Physi-\ncalQA.\nIt can be seen that our models outperform other\nmodels in most settings. This veriﬁes the effec-\ntiveness of our proposed models for commonsense\nreasoning.\nAblations In Table 1, 2, and 3 we also com-\npare OK-BERT with BERT. We found that OK-\nBERT with OK-Transformers effectively improved\nthe accuracy of BERT with Transformers. Sim-\nilar results can be found between OK-RoBERTa\nand RoBERTa. This shows that the proposed OK-\nTransformer improves pre-trained language models\nby adapting to them for free, i.e. without retraining\non large-scale unsupervised corpora.\n6.2 General Text Classiﬁcation\nWe use MRPC, CoLA, RTE, STS-B, SST-2, and\nQNLI in the GLUE dataset (Wang et al., 2018)\nto verify the effectiveness of the proposed models\non general text classiﬁcation tasks. We did not\nevaluate over MNLI, because our model needs to\nrepresent the corresponding n commonsense for\neach sentence, which is too costly for MNLI. We\nbelieve that this efﬁciency problem can be solved\nby further applying model compression (Iandola\net al., 2020), but this is beyond the scope of this\npaper. It can be seen from Table 4 that OK-BERT\nand OK-RoBERTa outperform their baselines.\n6.3 Commonsense Introduction Methods\nContinue pre-train In the introduction section, we\nmentioned that a typical method of introducing tex-\ntual knowledge is continuing pre-training (Guru-\nrangan et al., 2020; Sun et al., 2019). However,\ndue to the domain discrepancy of commonsense,\nthis method will cause catastrophic forgetting. To\nverify this intuition, in this subsection we com-\npare with the continuing pre-trained model. We\nﬁrst continue pre-training the language model over\nATOMIC2020, then ﬁne-tune it over the target task.\nExpBERT (Murty et al., 2020) We also com-\npare our OK-Transformer with ExpBERT, another\nmodel that is able to introduce textual knowledge.\nIn Sec 1, we mentioned that ExpBERT is not appli-\nGLUE Task MRPC CoLA RTE QNLI STS-B SST-2\nBERT 86.27/90.21 59.50 71.43 91.20 89.35/88.93 91.97\nOK-BERT 87.25/90.84 58.29 73.65 91.58 89.82/89.46 93.69\nRoBERTa 90.44/93.15 66.57 84.11 94.00 91.83/91.95 95.70\nOK-RoBERTa 91.91/94.24 66.89 86.28 94.41 92.41/92.20 96.10\nTable 4: Results on text classiﬁcation tasks. Models are evaluated by the dev split from GLUE.\ncable to large-scale commonsense knowledge bases\nfor its disability to select related commonsense and\nignore unrelated commonsense. To verify this, we\nuse the retrieved candidate commonsense descrip-\ntions from ATOMIC2020 as the additional expla-\nnations for ExpBERT. ExpBERT concatenates all\nthe embedding of a ﬁxed number of commonsense,\nwhich is inﬂexible for ATOMIC2020. For this rea-\nson, we ﬁx the number of commonsense to 48. If\nthere are more than 48 candidate commonsense\ndescriptions for one sample, we will randomly se-\nlect 48 of them. Otherwise, we will pad null com-\nmonsense to it. In our experiments, we also apply\nExpBERT to RoBERTa (Liu et al., 2019) (i.e. Ex-\npRoBERTa).\nWe show the results in Table 5. We do not report\nthe results of ExpBERT on WSC273, as ExpBERT\ncannot solve the cloze-style problems. It can be\nseen that the performance of language models was\nsuffered when we simply continue pre-training the\nmodels on the commonsense knowledge base. This\nveriﬁes that the continuing pre-training on the out-\nof-domain commonsense will cause catastrophic\nforgetting and injure the effectiveness. On the other\nhand, using OK-Transformer to introduce common-\nsense as the extra input signiﬁcantly improves the\naccuracy. The results also suggest that ExpBERT is\nnot applicable to large-scale commonsense knowl-\nedge bases.\n6.4 Why is OK-Transformer effective?\nWe now analyze why OK-Transformer can effec-\ntively introduce out-of-domain commonsense with-\nout pre-training. We are inspired by an observation\nof language model ﬁne-tuning LMs (Radiya-Dixit\nand Wang, 2020), i.e., the parameters after ﬁne-\ntuning are close to those before ﬁne-tuning. There-\nfore, we argue that the key to effective introduction\nis whether the parameters of the meta LM is good\ninitialization for the commonsense-enhanced LM,\nthat the parameters do not change much before and\nafter ﬁne-tuning.\nTo verify this, we compare the parameter\nFigure 4: L1 distances in parameter space between pre-\ntrained and ﬁne-tuned meta LMs. We show the metrics\nof WI across the 12 Transformer layers.\n0.00\n0.20\n0.40\n0.60\n0.80\n1 4 7 10\nLoss\nEpoch\nKnowBertOurs[CLS] tokenverbs\nFigure 5: Losses of dif-\nferent knowledge integra-\ntion methods in SST-2.\nThe [CLS] token method\ndoes not converge.\n5060708090100\nK=8K=16K=32K=64\nAccuracy\nBERTOK-BERT\nFigure 6: Effect in low-\nresource commonsense\nsettings with different ks\nover SST-2.\nchanges of different knowledge integration meth-\nods. These methods include (1) OK-Transformer,\n(2) KnowBERT (Peters et al., 2019), (3) using\nthe original [CLS] token instead of the proposed\nknowledge token, and (4) abandoning the knowl-\nedge token and instead calculating the csemb of\neach verb phrase of the target sentence separately,\nand adding them to these verb phrases’ hidden\nstates in Hi−1. We follow (Radiya-Dixit and Wang,\n2020) to use the L1 as the distance metric. (Radiya-\nDixit and Wang, 2020) found that the main change\nin parameters occurs on theWI matrix of the Trans-\nformer. Our experimental results also follow this\nphenomenon. Therefore, for greater clarity, we\nonly show the distances of the WI matrices after\nﬁne-tune. We show the distances of different meth-\nods in Fig. 4, and their training losses in Fig. 5.\nMRPC CoLA RTE QNLI STS-B SST-2 WSC273\nBERT 86.27/90.21 59.50 71.43 91.20 89.35/88.93 91.97 66.30\nBERT-continue 83.58/88.81 54.70 62.09 90.24 87.41/87.46 91.74 63.00\nExpBERT 85.78/89.79 58.29 62.82 87.06 84.78/84.67 91.51 –\nOK-BERT 87.25/90.84 58.29 73.65 91.58 89.82/89.46 93.69 67.40\nRoBERTa 90.44/93.15 66.57 84.11 94.00 91.83/91.95 95.70 90.10\nRoBERTa-continue 87.01/90.38 61.74 74.01 93.61 89.57/89.66 95.99 87.91\nExpRoBERTa 89.46/92.22 66.90 83.39 93.78 89.81/89.94 95.99 –\nOK-RoBERTa 91.91/94.24 66.89 86.28 94.41 92.41/92.20 96.10 91.58\nTable 5: Comparison of different commonsense introduction approaches. Continuing pre-training even injures the\neffectiveness. On the other hand, using OK-Transformers to introduce external knowledge achieves better results\nthan using Transformer.\nIt can be seen that the distances of OK-\nTransformer are much smaller than other methods,\nexcept the [CLS] token method, which does not\nconverge as shown in Fig. 5. This ﬁts our intuition\nof reducing the parameter variations to introduce\nexternal knowledge more effectively.\n6.5 Effect in Low-Resource Commonsense\nSettings\nSince there is a large number of commonsense de-\nscriptions in ATOMIC2020, a large portion of de-\nscriptions only occur a few times in the training\nset. In this subsection, we want to verify for these\nrare descriptions, can the model still beneﬁt from\nit? If so, we think it means that the model uses\nthe contextual information of the commonsense to\nimprove the understanding of the commonsense.\nTo do this, we proposed a low-resource common-\nsense setting. We evaluate the effect of the model if\nthe training dataset only contains k= 8/16/32/64\nsamples. Therefore the frequency of the appeared\ncommonsense descriptions is low. In order to ex-\nclude the inﬂuence of other samples, we only use\ntest samples whose candidate commonsense de-\nscriptions have already occurred in the ktraining\nsamples. For example, when k= 8, we randomly\nselect 8 samples from the training set for training,\nand use all samples in the test set which contains\nthe commonsense of the8 training samples for eval-\nuation. We show the results over the SST-2 dataset\nin Fig. 6. It can be seen that our models still beneﬁt\nfrom low-frequency commonsense.\n6.6 Does OK-Transformer Provide\nInterpretability?\nIn this subsection, we try to answer if the inte-\ngration of candidate commonsense descriptions by\nOK-Transformer is interpretable. To answer this\nquestion, we calculate the inﬂuence of different\ncommonsense descriptions on the model’s predic-\ntions. We follow (Wu et al., 2020) to quantify\nthe inﬂuence of a commonsense description ci as:\nIf ci is removed from cs(x), how much will the\nprediction change? This change is measured by\nthe Euclidean distance between the prediction by\ncs(x) −ci and by cs(x). The greater the change\nin the prediction, the greater the inﬂuence of this\ncommonsense.\nJohn promised Bill to leave, so an hour later [John] left.\nPersonX promises PersonY .\n1. ··· As a result, PersonX wants to fulﬁll his promise.\n2. ··· PersonX is seen as truthful\n3. ··· PersonX is seen as trustworthy.\n4. ··· Before, PersonX needed to talk to PersonY .\n5. ··· Before, PersonX needed to go to PersonY’s house.\nTable 6: A case study of top 5 commonsense descrip-\ntions.\nThrough the case studies of the samples in\nWSC273, we found that although commonsense\nwith higher inﬂuence is somewhat interpretable for\npeople, the interpretability is not signiﬁcant. We\nshow some examples in Table 6. We believe that\nthis is because some commonsense for people has\nbeen learned in pre-training. Therefore, the out-\nof-domain commonsense that these pre-trained lan-\nguage models need to incorporate for downstream\ntasks is inconsistent with human understanding.\n7 Conclusion\nIn this paper, we study how to use commonsense\nto enhance the general text representation. We ﬁrst\nanalyzed the challenges brought by the domain dis-\ncrepancy of commonsense. Then, we propose OK-\nTransformer to allow commonsense integration and\nenhancement. In the experiments, we veriﬁed the\neffectiveness of our proposed models in a variety\nof scenarios, including commonsense reasoning,\ngeneral text classiﬁcation, and low-resource com-\nmonsense. Our models consistently outperform the\nbaselines. We have also empirically analyzed other\nproperties (e.g. interpretability) of the model.\nAcknowledgments and Disclosure of\nFunding\nWe thank Wenting Ba for her valuable plotting as-\nsistance. This paper was supported by National Nat-\nural Science Foundation of China (No. 61906116),\nby Shanghai Sailing Program (No. 19YF1414700).\nReferences\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin\nChoi, et al. 2020. Piqa: Reasoning about physical\ncommonsense in natural language. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence ,\nvolume 34, pages 7432–7439.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL, pages 4171–4186.\nAli Emami, Adam Trischler, Kaheer Suleman, and\nJackie Chi Kit Cheung. 2018. A generalized knowl-\nedge hunting framework for the winograd schema\nchallenge. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Student Research Work-\nshop, pages 25–31.\nJian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and\nMinlie Huang. 2020. A knowledge-enhanced pre-\ntraining model for commonsense story generation.\nTransactions of the Association for Computational\nLinguistics, 8:93–108.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. arXiv\npreprint arXiv:2004.10964.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. arXiv\npreprint arXiv:2002.08909.\nPengcheng He, Xiaodong Liu, Weizhu Chen, and\nJianfeng Gao. 2019. A hybrid neural network\nmodel for commonsense reasoning. arXiv preprint\narXiv:1907.11983.\nJena D Hwang, Chandra Bhagavatula, Ronan Le Bras,\nJeff Da, Keisuke Sakaguchi, Antoine Bosselut, and\nYejin Choi. 2020. Comet-atomic 2020: On symbolic\nand neural commonsense knowledge graphs. arXiv\npreprint arXiv:2010.05953.\nForrest Iandola, Albert Shaw, Ravi Krishna, and Kurt\nKeutzer. 2020. Squeezebert: What can computer vi-\nsion teach nlp about efﬁcient neural networks? In\nProceedings of SustaiNLP: Workshop on Simple and\nEfﬁcient Natural Language Processing , pages 124–\n135.\nTassilo Klein and Moin Nabi. 2019. Attention is (not)\nall you need for commonsense reasoning. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4831–\n4836.\nTassilo Klein and Moin Nabi. 2020. Contrastive\nself-supervised learning for commonsense reason-\ning. arXiv preprint arXiv:2005.00669.\nVid Kocijan, Oana-Maria Camburu, Ana-Maria Cretu,\nYordan Yordanov, Phil Blunsom, and Thomas\nLukasiewicz. 2019a. Wikicrem: A large unsuper-\nvised corpus for coreference resolution. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 4294–4303.\nVid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu,\nYordan Yordanov, and Thomas Lukasiewicz. 2019b.\nA surprisingly robust trick for winograd schema\nchallenge. In The 57th Annual Meeting of the As-\nsociation for Computational Linguistics (ACL).\nHector Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In\nThirteenth International Conference on the Princi-\nples of Knowledge Representation and Reasoning.\nQuan Liu, Hui Jiang, Zhen-Hua Ling, Xiaodan Zhu,\nSi Wei, and Yu Hu. 2016. Commonsense knowledge\nenhanced embeddings for solving pronoun disam-\nbiguation problems in winograd schema challenge.\narXiv preprint arXiv:1611.04146.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nLeora Morgenstern, Ernest Davis, and Charles L Ortiz.\n2016. Planning, executing, and evaluating the wino-\ngrad schema challenge. AI Magazine, 37(1):50–54.\nShikhar Murty, Pang Wei Koh, and Percy Liang. 2020.\nExpbert: Representation engineering with natural\nlanguage explanations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2106–2113.\nMatthew E Peters, Mark Neumann, Robert Logan, Roy\nSchwartz, Vidur Joshi, Sameer Singh, and Noah A\nSmith. 2019. Knowledge enhanced contextual word\nrepresentations. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 43–54.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nEvani Radiya-Dixit and Xin Wang. 2020. How ﬁne\ncan ﬁne-tuning be? learning efﬁcient language mod-\nels. In International Conference on Artiﬁcial Intelli-\ngence and Statistics, pages 2435–2443. PMLR.\nRachel Rudinger, Jason Naradowsky, Brian Leonard,\nand Benjamin Van Durme. 2018. Gender bias in\ncoreference resolution. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers) ,\npages 8–14.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhaga-\nvatula, and Yejin Choi. 2019. Winogrande: An ad-\nversarial winograd schema challenge at scale. arXiv\npreprint arXiv:1907.10641.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to ﬁne-tune bert for text classiﬁcation?\nIn Proceedings of China National Conference on\nComputational Linguistics, pages 194–206.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. Commonsenseqa: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4149–4158.\nTrieu H Trinh and Quoc V Le. 2018. A simple\nmethod for commonsense reasoning. arXiv preprint\narXiv:1806.02847.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of the 31st International\nConference on Neural Information Processing Sys-\ntems, pages 6000–6010.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: An-\nalyzing and Interpreting Neural Networks for NLP ,\npages 353–355.\nShuohang Wang, Yuwei Fang, Siqi Sun, Zhe Gan,\nYu Cheng, Jingjing Liu, and Jing Jiang. 2020.\nCross-thought for sentence encoder pre-training. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 412–421.\nShuohang Wang, Sheng Zhang, Yelong Shen, Xi-\naodong Liu, Jingjing Liu, Jianfeng Gao, and Jing\nJiang. 2019. Unsupervised deep structured semantic\nmodels for commonsense reasoning. arXiv preprint\narXiv:1904.01938.\nThomas Wolf, Julien Chaumond, Lysandre Debut, Vic-\ntor Sanh, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Morgan Funtowicz, Joe Davison, Sam\nShleifer, et al. 2020. Transformers: State-of-the-\nart natural language processing. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations,\npages 38–45.\nZhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. 2020.\nPerturbed masking: Parameter-free probing for ana-\nlyzing and interpreting bert. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4166–4176.\nWenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2019. Pretrained encyclopedia:\nWeakly supervised knowledge-pretrained language\nmodel. In International Conference on Learning\nRepresentations.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. Ernie: Enhanced\nlanguage representation with informative entities. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 1441–\n1451.\nHao Zhou, Tom Young, Minlie Huang, Haizhou Zhao,\nJingfang Xu, and Xiaoyan Zhu. 2018. Com-\nmonsense knowledge aware conversation generation\nwith graph attention. In IJCAI, pages 4623–4629.\nA Experimentation Details\nWhen continuing pre-training BERT-\ncontinue/RoBERTa-continue in Table 5, we\nfollow (Kocijan et al., 2019b) and set learning rate\nto 1e−5, batch size to 64, and train the model for\nonly one epoch.\nWhen ﬁne-tuning the models in Sec 6.2 and\nSec 6.3, we train the models for 10 epochs. We\nuse grid search to select their learning rates and\nbatch sizes from {1e −5,2e −5,5e −5}and\n{8,16,32,64}, respectively.\nDataset WSC PDP WinoGender WinoGrande\nDataset size 273 60 720 40938/1267\nMatched ratio 67% 83% 65% 71%\nAverage |cs(x)| 129.71 189.68 80.63 140.56\nAverage length of c 17.88 17.91 16.83 17.91\nTable 7: Statistical results on commonsense reasoning datasets.\nDataset MRPC CoLA RTE QNLI STS-B SST-2\nDataset size 3668/408 8551/1043 2490/277 104743/5463 5749/1500 67349/872\nMatched ratio 59% 40% 72% 52% 56% 25%\nAverage |cs(x)| 80.71 84.85 122.60 81.35 117.00 83.07\nAverage length of c 17.47 17.60 17.71 17.59 17.34 17.59\nTable 8: Statistical results on sentence classiﬁcation datasets.\nB Statistics of Commonsense\nDescriptions\nIn Table 7 and Table 8, we report statistics about\ndown-stream tasks and their commonsense descrip-\ntions. Our report includes the size of the train/test\nsplits for the downstream tasks, the proportion of\nsamples that matched to at least one commonsense\ndescription (Matched proportion) in each task, the\naverage number of matched commonsense descrip-\ntions per sample (Average |cs(x)|), and the average\nlength of each matched commonsense description\n(Average length of c).\nFrom the results, we found that more than half of\nthe samples matched to at least one commonsense\ndescription in most of the datasets. This indicates\nthat the OOD commonsense used in this paper is\ngeneralizable to different datasets. Also, the aver-\nage length of the matched commonsense descrip-\ntions is short (about 17), thus encoding them via\nTransformer is efﬁcient.",
  "topic": "Commonsense knowledge",
  "concepts": [
    {
      "name": "Commonsense knowledge",
      "score": 0.8113976716995239
    },
    {
      "name": "Computer science",
      "score": 0.8099405765533447
    },
    {
      "name": "Transformer",
      "score": 0.694731593132019
    },
    {
      "name": "Commonsense reasoning",
      "score": 0.6402954459190369
    },
    {
      "name": "Language model",
      "score": 0.6044445037841797
    },
    {
      "name": "Sentence",
      "score": 0.5675349831581116
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5528662204742432
    },
    {
      "name": "Security token",
      "score": 0.5414159297943115
    },
    {
      "name": "Natural language processing",
      "score": 0.5285167694091797
    },
    {
      "name": "Question answering",
      "score": 0.4381117820739746
    },
    {
      "name": "Natural language understanding",
      "score": 0.4179912209510803
    },
    {
      "name": "Knowledge representation and reasoning",
      "score": 0.29883718490600586
    },
    {
      "name": "Natural language",
      "score": 0.177015483379364
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 1
}