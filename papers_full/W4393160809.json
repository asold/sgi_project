{
  "title": "EcomGPT: Instruction-Tuning Large Language Models with Chain-of-Task Tasks for E-commerce",
  "url": "https://openalex.org/W4393160809",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2588464066",
      "name": "Yang-ning Li",
      "affiliations": [
        "Tsinghua University",
        "Peng Cheng Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2142424115",
      "name": "Shi-rong Ma",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2129051579",
      "name": "Xiaobin Wang",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2110550660",
      "name": "Shen Huang",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2106987312",
      "name": "Chengyue Jiang",
      "affiliations": [
        "ShanghaiTech University"
      ]
    },
    {
      "id": "https://openalex.org/A2508172955",
      "name": "Hai-Tao Zheng",
      "affiliations": [
        "Tsinghua University",
        "Peng Cheng Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2944937469",
      "name": "Pengjun Xie",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A1936961387",
      "name": "Fei Huang",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2018623175",
      "name": "Yong Jiang",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2588464066",
      "name": "Yang-ning Li",
      "affiliations": [
        "Tsinghua University",
        "Peng Cheng Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2142424115",
      "name": "Shi-rong Ma",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2106987312",
      "name": "Chengyue Jiang",
      "affiliations": [
        "ShanghaiTech University"
      ]
    },
    {
      "id": "https://openalex.org/A2508172955",
      "name": "Hai-Tao Zheng",
      "affiliations": [
        "Peng Cheng Laboratory",
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3028928082",
    "https://openalex.org/W2989766279",
    "https://openalex.org/W3112254312",
    "https://openalex.org/W6846505686",
    "https://openalex.org/W4387848696",
    "https://openalex.org/W6783516984",
    "https://openalex.org/W4304700915",
    "https://openalex.org/W4309800148",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W3196731672",
    "https://openalex.org/W3021191241",
    "https://openalex.org/W4291961075",
    "https://openalex.org/W2973004186",
    "https://openalex.org/W3102116369",
    "https://openalex.org/W3161138172",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W3155220426",
    "https://openalex.org/W3157182169",
    "https://openalex.org/W2989224055",
    "https://openalex.org/W4385572845",
    "https://openalex.org/W3173727191",
    "https://openalex.org/W3100283070",
    "https://openalex.org/W4386081520",
    "https://openalex.org/W4310924890",
    "https://openalex.org/W3175892112",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W4306317315",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2997771882",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W3031414376",
    "https://openalex.org/W3034828204",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4388329494",
    "https://openalex.org/W3177219148",
    "https://openalex.org/W3088204197",
    "https://openalex.org/W3037013468"
  ],
  "abstract": "Recently, instruction-following Large Language Models (LLMs) , represented by ChatGPT, have exhibited exceptional performance in general Natural Language Processing (NLP) tasks. However, the unique characteristics of E-commerce data pose significant challenges to general LLMs. An LLM tailored specifically for E-commerce scenarios, possessing robust cross-dataset/task generalization capabilities, is a pressing necessity. To solve this issue, in this work, we proposed the first E-commerce instruction dataset EcomInstruct, with a total of 2.5 million instruction data. EcomInstruct scales up the data size and task diversity by constructing atomic tasks with E-commerce basic data types, such as product information, user reviews. Atomic tasks are defined as intermediate tasks implicitly involved in solving a final task, which we also call Chain-of-Task tasks. We developed EcomGPT with different parameter scales by training the backbone model BLOOMZ with the EcomInstruct. Benefiting from the fundamental semantic understanding capabilities acquired from the Chain-of-Task tasks, EcomGPT exhibits excellent zero-shot generalization capabilities. Extensive experiments and human evaluations demonstrate that EcomGPT outperforms ChatGPT in term of cross-dataset/task generalization on E-commerce tasks. The EcomGPT will be public at https://github.com/Alibaba-NLP/EcomGPT.",
  "full_text": "EcomGPT: Instruction-Tuning Large Language Models\nwith Chain-of-Task Tasks for E-commerce\nYangning Li1,4*, Shirong Ma1*, Xiaobin Wang3, Shen Huang3, Chengyue Jiang2\nHai-Tao Zheng1,4† , Pengjun Xie3, Fei Huang3, Yong Jiang3 †\n1SIGS, Tsinghua University\n2ShanghaiTech University\n3DAMO Academy, Alibaba Group\n4PengCheng Laboratory\nAbstract\nRecently, instruction-following Large Language Models\n(LLMs) , represented by ChatGPT, have exhibited excep-\ntional performance in general Natural Language Process-\ning (NLP) tasks. However, the unique characteristics of E-\ncommerce data pose significant challenges to general LLMs.\nAn LLM tailored specifically for E-commerce scenarios, pos-\nsessing robust cross-dataset/task generalization capabilities,\nis a pressing necessity. To solve this issue, in this work, we\nproposed the first E-commerce instruction dataset EcomIn-\nstruct, with a total of 2.5 million instruction data. EcomIn-\nstruct scales up the data size and task diversity by construct-\ning atomic tasks with E-commerce basic data types, such\nas product information, user reviews. Atomic tasks are de-\nfined as intermediate tasks implicitly involved in solving a\nfinal task, which we also call Chain-of-Task tasks. We devel-\noped EcomGPT with different parameter scales by training the\nbackbone model BLOOMZ with the EcomInstruct. Benefit-\ning from the fundamental semantic understanding capabilities\nacquired from the Chain-of-Task tasks, EcomGPT exhibits\nexcellent zero-shot generalization capabilities. Extensive ex-\nperiments and human evaluations demonstrate that EcomGPT\noutperforms ChatGPT in term of cross-dataset/task general-\nization on E-commerce tasks. The EcomGPT will be public at\nhttps://github.com/Alibaba-NLP/EcomGPT.\nIntroduction\nIn the field of E-commerce, the progress made in natural\nlanguage processing (NLP) and deep learning (DL) has sig-\nnificantly contributed to the advancement of E-commerce\ntechnology. These advancements have unlocked diverse capa-\nbilities ranging from product information extraction (Cheng\net al. 2021; Wang et al. 2021; Li et al. 2022) to user query\nunderstanding (Zhao, Chen, and Yin 2019; Ahmadvand et al.\n2020). Recently, instruction-following Large Language Mod-\nels (LLMs) (Ouyang et al. 2022; Taori et al. 2023; Chiang\n*These authors contributed equally. Work done at Alibaba.\n†Corresponding author: zheng.haitao@sz.tsinghua.edu.cn and\njiangyong.ml@gmail.com.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nAttribute\nExtraction\nInput: 撞色拼接的领口以及大口袋 (Colour blocked neckline and\nlarge pockets)\nChatGPT: I’m sorry, but I can’t provide the response you’re looking for.\n$\nEcomGPT: 图案: 撞色 (Pattern: Colour blocked) \"\nExplanation: Sentences composed solely of entities, even shorter than\nthat of prompts, poses challenges to the model’s comprehension.\nReview T\nopic Classification\nInput: My personal favorite is Nick and Joe’s. Candidate Topic: Food,\nPrice, ..., Anecdotes/Miscellaneous\nChatGPT: Food $\nEcomGPT: Anecdotes/Miscellaneous \"\nExplanation: Emerging entities, such as shop names, which not in the\npre-training corpus, require stronger generalization capability for model.\nProduct T\nitle Generation\nInput: #锅类型#:#少油烟、不易粘锅#;#流行元素#:#复古#;#型\n号#:#L70846#;#材质#:#熟铁#;#品牌#:#other/其他#; ......\nChatGPT: 【其他品牌】32cm中式复古熟铁燃气灶木质锅盖少油\n烟商用锅 L70846 ([Other brands] 32cm Chinese Retro Cooked Iron\nGas Stove Wooden Pot Lid Less Fume Commercial Cooker L70846) $\nOutput from EcomGPT: 复古铁锅炒菜不粘锅家用商用炒菜\n锅炒肉不糊锅炒鸡蛋平底锅 (Vintage iron skillet frying non-stick\nhousehold commercial frying pan frying meat without paste pan\nscrambled eggs pan) \"\nExplanation: Textual content with complex structureis difficult to\nunderstand by general LLMs. Hence, in title generation, ChatGPT me-\nchanically splices attribute values regardless of the attribute importance.\nTable 1: Real cases that general LLMs cannot handle.\net al. 2023; Yu et al. 2023), such as ChatGPT, have demon-\nstrated exceptional performance in general natural language\nprocessing tasks (Zhao et al. 2023; Cheng et al. 2023a,b).\nThese LLMs can accomplish various tasks by transforming\nthem into generative paradigms. One noteworthy aspect is the\nremarkable zero-shot capabilities exhibited by LLMs, which\ncan be attributed to instruction tuning.\nHowever, despite their numerous merits, general LLMs\nare not specifically designed for the E-commerce sector. This\ncan lead to suboptimal performance for various E-commerce\ntasks. Table 1 illustrates the distinctive characteristics of\nE-commerce data (Tsagkias et al. 2021; Jiang et al. 2022)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18582\ncompared to general domains. Firstly, E-commerce data pos-\nsesses a complex syntactic structure that differs from co-\nherent sentences in general. For example, product titles are\ntypically composed of discrete entities and are much shorter\nthan regular sentences. Considering another example, product\ninformation often consists of attribute-attribute value pairs\nseparated by special symbols (e.g., “##”), which poses chal-\nlenges for general LLMs to comprehend. Secondly, the word\ndistribution of E-commerce data significantly varies from\nthat of general domains due to the abundance of unique enti-\nties and concepts found in E-commerce platforms (Escursell,\nLlorach-Massana, and Roncero 2021). Moreover, these novel\nentities and concepts are highly dynamic and continuously\nupdated as new products, users, and trends emerge daily, re-\nquiring exceptional generalization capabilities to effectively\nhandle such dynamics. Consequently, there is an urgent need\nfor the LLM specifically tailored for E-commerce scenar-\nios, equipped with robust cross-dataset/task generalization\ncapabilities.\nIn the BERT era, numerous efforts (Zhang et al. 2021; Qiu\net al. 2022; Xu et al. 2021) have been made to enhance the\nmodels’ generalization ability by integrating domain knowl-\nedge. For instance, E-BERT (Poerner, Waltinger, and Sch¨utze\n2020) further pre-trains BERT on the Amazon dataset to in-\ncorporate semantic knowledge of the E-commerce domain\ninto BERT. However, these efforts primarily rely on encoder-\nonly architectures like BERT, limiting their capacity for in-\nstruction learning and achieving stronger generalization capa-\nbilities. Furthermore, the parameter sizes of these models are\nrelatively small (less than 1 billion), making it challenging to\ncapture and represent complex linguistic knowledge, thereby\nrestricting their generalization capabilities.\nTo enhance models’ generalization ability cross dataset/-\ntasks, this work presents the first E-commerce instruction\ndataset, EcomInstruct, comprising a total of 2.5 million in-\nstruction data and 134 tasks. EcomInstruct are built from\ntwo main sources. Firstly, we manually collect a wide range\nof E-commerce natural language processing (NLP) datasets\nfrom open data sources, such as academic websites and data\ncompetition platforms. They cover a broad range of tasks,\nincluding E-commerce named entity recognition, review-\nbased Q&A, product classification, multi-turn dialogue, and\nother traditional NLP tasks. The benefit of these open-source\ndatasets is that they are expert-calibrated and high-quality.\nSecondly, we identified several basic data types that are com-\nmon in E-commerce scenarios, including product informa-\ntion, user reviews, user dialogue, and search queries. Around\nthese basic data types, we build a large number of atomic\ntasks. Formally, atomic tasks are defined as intermediate tasks\nimplicitly involved in solving a final task. The fundamental\nsemantic understanding capabilities learned from the atomic\ntasks are also used when solving other unseen tasks, thus\ncan greatly enhances the model’s generalization capabilities.\nWith this motivation, we further construct a large number\nof atomic tasks around these basic data types, as shown in\nFigure 1. Since these atomic tasks are the link in the chain of\ntask solution, we refer to them as Chain-of-Task tasks (CoT\ntasks), in reference to previous work on Chain-of-thought\n(Wei et al. 2022; Wang et al. 2022a). After collecting the\nLang.\nTask Para. # task\n# train inst. # test inst.\nEN\nCLS 15 130,596\n34,189\nExt 15 82,397\n47,284\nGen 22 353,486\n96,585\nOther 10 61,756\n36,481\nZH\nCLS 18 324,062\n362,845\nExt 9 131,814\n54,725\nGen 37 444,503\n353,486\nOther 8 111,814\n36,481\nALL 134 1,533,300\n1,023,076\nTable 2: Statistics for EcomInstruct.\nabove two parts of raw data, expert-written task-specific in-\nstruction schema and raw data are combined to obtain final\ninstruction data.\nBy training the backbone model BLOOMZ with\nEcomInstruct, we developed the instruction-following LLM\nEcomGPT for E-commerce. EcomGPT exhibits exceptional\ngeneralization capabilities compared to ChatGPT on various\nunseen E-commerce dataset and tasks. The further ablation\nexperiments highlight the effectiveness of the Chain-of-Task\ntasks. This strongly implies that we can enhance the model’s\ngeneralization ability by constructing diverse atomic tasks\nspecifically tailored to the domain data, especially when the\ndomain data is limited.\nIn summary, the contributions of this work are threefold:\n1. We proposed the first E-commerce instruction dataset\nEcomInstruct, with a total of 2.5 million instruction data.\nEcomInstruct scales up the data size and task diversity by\nconstructing Chain-of-Task tasks (atomic tasks).\n2. We proposed the first instruction-following LLM specif-\nically designed for E-commerce. Benefiting from numerous\nChain-of-Task tasks, EcomGPT exhibits superior zero-shot\ngeneralization ability.\n3. Extensive experiments demonstrate the effectiveness\nof EcomGPT compared to ChatGPT with larger parameter\nscales. Furthermore, the detailed ablation experiments pro-\nvide guidance for the design of LLMs in vertical domains.\nEcomInstruct: E-commerce Instruction Tuning\nDataset\nOverview of the EcomInstruct\nIn this section, we present our EcomInstruct dataset for in-\nstruction tuning on E-commerce tasks, which primarily built\nfrom two sources. Firstly, we manually collected a diverse set\nof E-commerce natural language processing (NLP) datasets\nfrom various open data sources, including academic websites\nand data competition platforms. They cover a broad range\nof tasks, such as E-commerce named entity recognition and\nintent detection. These datasets are typically of high quality\nas they have been carefully curated by experts in the field.\nSecondly, we identified several basic data types that are\ncommon in E-commerce scenarios, including product in-\nformation, user reviews, user dialogue, and search queries.\nAround these basic data types, we build a large number of\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18583\natomic tasks. Formally, atomic tasks are defined as intermedi-\nate tasks implicitly involved in solving a final task. The funda-\nmental semantic understanding capabilities learned from the\natomic tasks are also used when solving other unseen tasks,\nthus can greatly enhances the model’s generalization capabili-\nties. For instance, when performing named entity recognition,\nthe model needs to perform entity span detection and entity\nclassification sequentially. Meanwhile, entity span detection\nis also implicitly used when conducting review sentiment\nanalysis, as the model needs to detect entities with sentiment\ntendencies. Since these atomic tasks are the link in the chain\nof task solution, we refer to them as Chain-of-Task tasks (CoT\ntasks), in reference to previous work on Chain-of-thought. In\nEcomInstruct, these atomic tasks are divided into two parts.\nOne part is transformed from complete information in the\nhigh quality dataset through heuristic strategies, while the\nother part is constructed by utilizing ChatGPT to annotate\npseudo-labelling.\nAfter collecting the above two parts of raw data, we com-\nbined the data samples with task-specific instruction schema\nto obtain instruction data. Table 2 shows the detailed statis-\ntics of EcomInstruct, which includes a total of 134 tasks and\n2.6 million instruction data. In the following sections, we\nwill describe the collection of raw data for the open-source\nE-commerce NLP tasks and the atomic tasks. Additionally,\nwe will describe how to map raw data samples to instruction\ndata.\nRaw Data from Open-Source Benchmarks\nWe collected publicly available and widely used NLP bench-\nmark datasets in the E-commerce domain as our raw data,\nmainly sourced from research websites and data competition\nplatforms. Based on this, we identified several major task\nparadigms:\n• Classification: Classification tasks play a vital role in\nE-commerce, as it helps to automatically organize and cate-\ngorize textual data, such as product descriptions, customer\nreviews, and inquiries. The main objective of these tasks is\nto accurately predict the category, topic, or intent accurately\nbased on the input content. These tasks can take the form of\nmulti-class classification, binary classification, or multi-label\nclassification.\n• Extraction: Extraction tasks are widely utilized to ex-\ntract important information from unstructured textual data.\nFor instance, review-based extractive question-answering in-\nvolves extracting relevant information from customer reviews\nto answer specific questions.\n• Generation: Generation tasks are designed to produce\nnovel content that fulfills the given requirements, such as\ndialogue reply, copywriting, title. For example, title genera-\ntion aims to produce brief but distinctive title based on the\nattribute key-value pairs of the products, which can help to\npromote the product sales.\n• Others: other E-commerce NLP tasks. In our EcomIn-\nstruct dataset, it primarily refers to the task of Named Entity\nRecognition (NER) within various label schemes, such as\naddress-related NER and product attribute-related NER. As\nthe output of NER encompasses both the original input text\nAddress\nClassification\nExtraction\nGeneration Dialogue\nExtractionGeneration\nOther\nClassification\nExtraction\nGeneration\nProduct Description\nExtraction\nGeneration\nProduct \nInformation\nClassification\nGeneration\nProduct Title\nClassification\nExtraction\nGeneration\nReviewClassification\nExtraction\nGeneration\nSearch Query\nClassification\nExtraction\nGeneration\nFigure 1: The schema of the atomic tasks.\n(entities corresponding to positive labels) and the novel con-\ntent generated by the model (None output corresponding to\nnegative labels), it thus constitutes a hybrid task of extraction\nand generation.\nIn this step, we collected 65 public E-commerce NLP bench-\nmarks in total.\nRaw Data from Atomic Tasks\nBased on the data derived from open-source benchmarks,\nwe decomposed them into various atomic tasks. These tasks\nare transformed into datasets for instruction tuning, as de-\nscribed in previous subsection, to further expand the scale\nand diversity of the instruction data.\nOn the one hand, atomic tasks can be constructed by lever-\naging the complete information from the original data, includ-\ning the ground truth labels that either exists in the original\ndataset or can be inferred from it. Specifically, 3 main strate-\ngies are employed for constructing atomic tasks: (1) Task\nSimplification. We can adjust the model inputs and ground\ntruth labels to simplify the original tasks. For example, we\ncan obtain entity detection and entity typing tasks by simpli-\nfying named entity recognition (NER) task. (2) Task Reversal.\nFor some original tasks, we can switch the order of model\ninput and output to construct new tasks. For instance, we can\nbuild a question generation task from the question answering\n(QA) task, and the task of generating product description\ngiven product title can be transformed into a title generation\ntask. (3) Sample Recombination. We can also use informa-\ntion from multiple samples in a dataset to form a new sample,\nthereby obtaining different tasks. For example, based on the\nproduct matching task given two product titles and attributes,\nwe can split and shuffle the product titles and attributes in\nthese samples to construct a task that matches a product title\nand a product attribute.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18584\nDialogue Intent Detection\nNamed Entity Recognition\nEcomGPT\nTaskDescription:Below is an instruction that describes thenamed entity recognition task…Prompt: Extract all named entity with type about Attribute, Brand, Component, Product in the sentence.Input: The Intel Pentium 4 processor Extreme Edition supporting HT Technology features 2MB of L3 cache and offers high levels of performance targeted specifically for high-end gamers and computing power users.Output:\nTaskDescription:Below is an instruction that describes theproductselecttask…Prompt: Select documents from the candidates that best match the product query. Input: Query: 家庭花卉自浇水系统\\nCandidateDocument: 房车风力发电机车载风车输出微型自动汽车风能j房车风力发电机。, ..., 手机wifi控制自动浇花器定时家庭花园草坪花槽喷雾滴灌雾化微喷, ...OutputConstrain:Output the document content only.Output:\nTaskDescription:Below is an instruction that describes theintentdetectiontask…Prompt: Classify the intent of the last sentence according to the dialogue. Input: 您好，有什么可以帮您#E-s[数字x]\\n我点了个取消订单\\n不想取消了\\n帮我解除掉那个申请Candidate Labels:礼品卡获得, 配送工作时间, 商家入驻联系方式,订单状态解释, 使用咨询, 退款异常, 恢复订单……Output:\nProductSelect\nDialogue Intent Detection\n…\n…\n手机wifi控制自动浇花器定时家庭花园草坪花槽喷雾滴灌雾化微喷\nAttribute:2MB,Brand:Intel,Component:L3cache,Product:Pentium4processorExtremeEdition\n恢复订单\nProductSelect\nNamed Entity Recognition\n…\nFigure 2: An overview of multi-task instruction tuning of EcomGPT for diverse E-commerce tasks.\nOn the other hand, we can construct instruction datas based\non basic E-commerce information within the datasets, such\nas product metadata and user queries without ground truth\nlabels from the original data. For these input-only datas, we\nutilize ChatGPT to generate outputs as pseudo-labels for\nmodel training. For instance, we can devise various instruc-\ntion tasks based on search queries, such as query rewriting,\nquery segmentation, and query-based question generation, to\ncompose a diverse set of atomic tasks. The complete schema\nof the atomic tasks is shown in Figure 1.\nMapping Raw Data to Instruction Data\nBuilding upon the raw data, we further developed the instruc-\ntion data. Firstly, we devised the schema of the instruction\ndata, which encompasses six primary components:\n1. Task Description: a high-level overview of the task at\nhead.\n2. Prompts: sentences that provide a crucial depiction of\nthe task that the model is expected to accomplish.\n3. Input Text: E-commerce data needs to be processed,\nsuch as product information and user reviews.\n4. Candidate Labels (Optional): this component is intended\nspecifically for classification tasks and NER tasks, wherein\ncandidate labels are deemed necessary.\n5. Output Constraints (Optional): supplemental descrip-\ntions that clearly specify the requirements for the output\nformat or style.\n6. Output: the ground truth output desired by the user.\nWe asked domain experts to write dataset-specific task\ndescriptions, prompts and output constraints for each dataset,\nwhich is a non-trivial work. Whereas for input text, candidate\nlabels and output, we filled them with content from original\ndata. Examples of instruction data can be found in Figure 2.\nDespite the relatively high quality of data from open source\nbenchmark datasets, it is inevitable that some noise will be\npresent. Therefore, EcomInstruct underwent two data filtering\nand human calibration processes. Firstly, we implemented a\nrule-based filtering approach that primarily excluded data in-\nstances containing illegal characters in the input, null output,\nand excessively long data instances. We also standardized the\nwhitespace characters in the content. Secondly, we applied a\nmodel-based filtering approach utilizing Alpaca GarbageCol-\nlector1 to flag low-quality instructional data to be discarded.\nAdditionally, for each dataset, we ensured that at least one\nannotator conducted a secondary check on a random sample\nof 200 data instances.\nEcomGPT: Training E-commerce Large\nLanguage Model with EcomInstruct\nOur EcomGPT is constructed by fine-tuning BLOOMZ with\nour EcomInstruct dataset. Specifically, EcomGPT was trained\nwith four different parameter scales: 560m, 1.7b, 3b, and\n7.1b. AdamW (Loshchilov and Hutter 2017) optimizer is\nemployed for model training, with learning rate set of 2e-\n1https://huggingface.co/argilla/alpaca-garbage-collector-\nmultilingual\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18585\n5 and weight decay of 0. We utilize a cosine learning rate\nschedule, warming up over 3% of the training steps. The\nmodel is fine-tuned with 3 epochs, with the batch size per\ndevice set to 4 and the gradient accumulation step set to 8.\nThe maximum sequence length is 1024. All experiments are\nrun on 4 NVIDIA A100 SXM4 80GB GPUs.\nDuring model training, we expect the model to learn to\ngenerate response given the instruction and input text, thus we\ncompute the loss function by considering only the response\ntokens and ignoring the input tokens.\nExperiments\nExperiment Setup\nBaselines We classified our baseline models into two cate-\ngories: foundational pre-trained large models and instruction-\nfollowing large language models. The former includes the\nBLOOM (Scao et al. 2022), which has a decoder-only ar-\nchitecture and ranges from 560 million to 176 billion pa-\nrameter scales. The latter includes BLOOMZ (Muennighoff\net al. 2022), which applies multi-task instruction tuning to the\nBLOOM models to obtain instruction-following variants, and\nChatGPT, the most advanced commercially available large\nlanguage model . ChatGPT applies instruction fine-tuning\nand RLHF techniques to fine-tune and align GPT3.\nTo compare our EcomGPT model with BLOOM and\nBLOOMZ, we selected the 560m, 1.7b, 3b, and 7.1b-\nparameters models. We estimated the upper bound on the\ngeneralization performance of the 7b-parameters model on\nunseen dataset or tasks. Specifically, we randomly selected\n800 training data for each evaluation task, and independently\ntrained BLOOMZ 7.1b, taking the average of the perfor-\nmance of these models on the corresponding task as the\nupper bound on performance.\nEvaluation Metric. In EcomInstruct, all tasks can be con-\nverted into generative paradigms, thus we can evaluate them\nwith automatic evaluation metrics for text generation. For var-\nious tasks, ROUGE-L (Lin 2004) is employed to evaluate the\nmodel outputs following previous works (Wang et al. 2022b;\nMishra et al. 2022).\nAdditionally, for classification and NER tasks, we also\nutilize precision, recall and F1 as evaluation metrics, and\nreport both micro-average and macro-average results. For\nopen-domain generation tasks such as product title genera-\ntion, we contend that automatic reference-based evaluation\nmetrics such as ROUGE-L do not sufficiently reflect the\nmodel performance, which is also an exceedingly complex\nissue in the natural language generation domain (Celikyil-\nmaz, Clark, and Gao 2020). Therefore, we further conducted\nhuman evaluation to measure the model performance.\nDataset Split. The EcomInstruct dataset is divided into two\npartitions, namely training and testing. The test set comprises\n12 tasks chosen from diverse datasets, encompassing four ma-\njor categories, namely classification (e.g., coarse-grained/fine-\ngrained product classification, review topic classification),\ngeneration (e.g., product title generation), extraction (e.g.,\nreview-based QA, attribute value detection), and others (e.g.,\nE-commerce named entity recognization). To ensure efficient\nDataset Lang\n. Task Metric\nLenov\ne EN Named Entity Recognization F1, Rouge\nEntity Span\nDetection Rouge\nReddit EN\nExtractive QA Rouge\nABSA EN\nReview Topic Classification F1, Rouge\nMEPA\nVE ZH Attribute Value Recognization F1, Rouge\nAttribute\nValue Detection Rouge\nMulti-CPR ZH\nProduct Select Rouge\nOpenBG 2 ZH\nProduct Align F1,\nRouge\nTitle\nAttritube Matching F1, Rouge\nFine-grain Product\nClassify F1, Rouge\nCoarse-grain Product\nClassify F1, Rouge\nTitle\nGenerate Rouge\nTable 3: The details of our evaluation datasets.\ntesting, 500 instances of each task were randomly selected as\ntest data, resulting in a final test set of 6,000 data instances.\nThe remaining 122 datasets were allocated for training, from\nwhich up to 800 data instances were sampled for each dataset\nas the training set. Ultimately, the EcomGPT was trained\non a total of 85,746 instances of E-commerce data. For a\nmore detailed scaling experiments on the number of training\nsamples for each dataset, please refer to Appendix.\nGeneralization Types.Conventional supervised learning\nevaluates a model’s capacity to generalize within a given\ndistribution, wherein the model learns from labeled instances\nof specific domains and tasks, and is subsequently tested\non data that conforms to the same distribution for the same\ndomain and task. In contrast, for E-commerce LLM, our em-\nphasis lies in the model’s ability to generalize to data outside\nthe distribution. In this study, we correspond a data instance\nto three levels, namely task paradigm (e.g., generation task,\nclassification task), task (e.g., the classification paradigm\ncomprises tasks with different objectives like product item\nclassification, intent detection, etc.), and dataset (e.g., for\nthe intent detection task, it encompasses SGD (Rastogi et al.\n2020) and JDDC (Chen et al. 2020) datasets, consisting of\ndistinct label sets). The model’s ability to generalize to un-\nseen tasks/datasets at the task and dataset levels represents the\nmost desirable and practical feature. Therefore, we primar-\nily focus on the model’s generalization capability on unseen\ntasks/datasets in the main experiment. Additionally, in Ap-\npendix, we evaluate the model’s performance under cross\ntask paradigms and cross-language settings.\nMain Experiments\nTable 4 presents the results of the automated metrics-based\nevaluation conducted on new datasets and tasks, from which\nwe can conclude that: (1) In terms of average performance\non unseen datasets, EcomGPT, even with the lowest number\nof parameters (560 million), outperforms ChatGPT, which\nhas over 100 billion parameters (exceeding EcomGPT by\n100,000 times). Moreover, EcomGPT’s performance consis-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18586\nModel T\nype Model\nUnseen Dataset Unseen Task\nMi-F1 Ma-F1\nRouge Poduct Align Review Topic Classify Product Select\nMi-F1 Ma-F1\nRouge Mi-F1 Ma-F1 Rouge Rouge\nPLM\nBLOOM (560m) 3.33\n2.10 5.64 0.17 0.15 6.76 13.22 10.96 1.26 6.06\nBLOOM (1b7) 4.15\n2.78 6.00 0.10 0.10 1.60 16.17 14.95 6.72 6.72\nBLOOM (3b) 2.94\n1.43 7.89 0.10 0.20 1.86 0.38 0.18 5.50 7.99\nBLOOM(7b1) 4.29 2.50\n7.31 0.10 0.13 0.97 7.11 3.61 4.96 9.47\nInstruction\nBLOOMZ (560m) 24.62\n25.60 24.03 21.80 21.80 55.53 30.49 32.13 23.60 0.00\nBLOOMZ (1b7) 18.60\n18.87 15.10 0.40 0.40 0.40 32.06 34.01 26.38 2.27\nBLOOMZ (3b) 29.80\n30.05 26.38 10.42 10.80 16.53 30.81 32.14 23.25 11.65\nBLOOMZ (7b1) 26.75\n27.07 25.21 6.00 6.00 8.00 49.37 50.39 41.43 15.14\nChatGPT 37.30 40.71\n43.92 41.60 41.60 71.02 51.22 51.80 42.55 27.39\nOurs\nEcomGPT (560m) 41.28\n38.21 48.88 50.15 50.15 81.41 42.39 50.88 32.25 10.74\nEcomGPT (1b7) 42.30\n39.07 53.24 51.20 52.20 81.23 47.38 52.68 37.81 32.38\nEcomGPT (3b) 48.37\n45.04 59.20 53.20 53.20 82.13 53.91 56.12 44.99 52.53\nEcomGPT (7b1) 52.89\n50.17 62.83 55.20 55.20 84.67 59.03 60.74 50.25 56.39\nUpper-bound(est.)\nSFT(7b1) 74.73 71.01 73.87 67.90 67.90 89.06 85.86 89.22 82.96 97.60\nTable 4: Performance on unseen dataset and tasks.\ntently improves as the model parameters scale, demonstrating\nits remarkable generalization ability for E-commerce tasks.\n(2) By training on EcomInstruct data, EcomGPT achieved\na substantial improvement of over 20 points compared to\nthe baseline model BLOOMZ. This suggests that excellent\ngeneralization performance of EcomGPT is not solely depen-\ndent on the backbone model. (3) Due to the lack of dialogue\ncapability, the pre-trained language model BLOOM demon-\nstrates poor performance, approaching 0 and being unstable.\nInterestingly, the difference between the performance boost\nachieved by the xP3 dataset, which contains over 78 million\ngeneral instruction data, and that obtained by the EcomInstuct\ndataset, which has roughly 200,000 E-commerce instruction\ndata for training, is approximately 4 points. This highlights\nthe more effective role of domain-specific instruction data for\nvertical scenarios in enhancing model generalization capabil-\nity. (4) We conducted supervised fine-tuning of BLOOMZ 7b\nusing the training set of the test tasks to estimate the upper\nbound of the model’s generalization performance. Our find-\nings indicate that the current EcomGPT still has significant\nroom for improvement in terms of generalization capability.\nFurthermore, in order to enhance the reliability of the eval-\nuation, particularly for the generation tasks, where automated\nevaluation metrics fall short in reflecting the performance of\nthe model, a human evaluation was deliberately incorporated.\nAs illustrated in Figure 3, we randomly selected 100 samples\nper task and ask the annotators to judge which one of the\noutputs of EcomGPT and ChatGPT is better or tied. The\nresults show that, with the exception of generation tasks, the\nwinning or tying rate of EcomGPT in the human evaluation\nmaintains the same overall trend as the Rouge value. The\nPearson coefficient between the two is 0.2, indicating a posi-\ntive correlation overall and confirming the reliability of the\nhuman evaluation. Upon analyzing the output, we observed\nthat for certain tasks with complex input or output formats,\nsuch as named entity recognition, ChatGPT struggled and\noften displayed a meaningless response like “sorry, I can’t re-\ntrieve the information”. In the case of generation tasks, such\nas product title generation, ChatGPT typically generated ex-\ncessively long sentences, which were inconsistent with the\nconcise and attention-grabbing style of human written titles.\nWhile ChatGPT was able to solve some relatively simple\ntasks, such as product selection (with a solution rate of 78%\nin human evaluation), the model’s Rouge value remained low.\nWe attributed this to the abundance of redundant replies in the\noutput of ChatGPT, which hindered its practical application,\nsince time-consuming task-specific parsing of model out-\nput is required. In conclusion, EcomGPT exhibited superior\nsemantic understanding of E-commerce data.\n0.00\n0.15\n0.30\n0.45\n0.60\n0.75\n0.90\n0\n20\n40\n60\n80\n100\nZH-OpenBG-Generate-Title\nGenerate\nWin Tie Loss Rouge\nFigure 3: Human Evaluation results.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18587\nAblation Experiments on CoT Tasks\nAs described in Dataset Section, a considerable proportion\nof EcomInstruct consists of atomic tasks that are constructed\nusing data specific to the E-commerce domain. These atomic\ntasks encompass a variety of generic semantic understanding\ncapabilities, which are extensively utilized during the inter-\nmediate stage of the model’s solution of the original task.\nDrawing a parallel with prior research on Chain-of-Thought\n(Wei et al. 2022; Wang et al. 2022a), we refer to these atomic\ntasks as Chain-of-Task tasks (CoT tasks). The CoT task em-\npowers the model to imbibe generic capabilities that are\nimplicitly utilized while handling E-commerce tasks, thereby\nplaying a pivotal role in enhancing the model’s generalization\nability. To validate our assumptions and the effectiveness of\nthe CoT task, we conduct ablation experiments on the CoT\ntask at a high level. Furthermore, we take a deeper look into\nthe benefits of CoT tasks across varied dimensions, including\ndata, tasks, and task paradigms.\nOverall Gain from CoT TasksThe CoT tasks were de-\nrived from a combination of two sources: data with pseudo-\nlabels generated by ChatGPT and high-quality raw data with\ngolden labels. As illustrated in Table 5, when both compo-\nnents of the CoT data are sequentially removed, there is a\nsignificant degradation in the performance of the EcomGPT.\nFurthermore, the model trained solely using original E-\ncommerce data fails to outperform ChatGPT’s performance\nin Table 4. This observation suggests that solely relying on\ndomain data for instruction learning is insufficient to enhance\nthe generalization ability of the pendant domain model. Addi-\ntionally, we observe a more substantial drop in performance\nupon removal of the CoT task constructed from high-quality\ndata containing golden labels, which is due to the fact that\nthe amount of data built from ChatGPT is relatively small\nwhile containing some errors or noise.\nThe significant improvement achieved with the CoT task\ninspires us to even with limited domain data, a series of\natomic tasks constructed from the domain data can endow\nthe model with superior generalization capabilities.\nTraining\nDataset Micro F1 Macro F1 Rouge\nFull 48.37\n45.04 59.20\nw/o pseudo\nlabel CoT 44.98 41.79 55.46\nw/o golden\nlabel CoT 26.64 23.64 35.02\nTable 5: Overall abaltion on CoT Tasks. w/o pseudo label\nCoT means without CoT task whose label is generate by\nChatGPT. w/o golden label CoT represents without CoT task\nwhose label is inferred from the original golden labels.\nCross Gain from CoT TasksIn this section, we conduct\nextensive ablation experiments on CoT data, aiming to inves-\ntigate the benefits of CoT data at the dataset, task, and task\nparadigm levels.\nDataset Level.In the Table 6, we remove the CoT task\nassociated with a specific dataset from the training set to\nobserve its impact. To prevent data leakage, we avoided in-\ntroducing CoT tasks corresponding to the test dataset in the\nTraining\nEcom Youku Amazon CCKS JDDC Avg\nFull 73.79\n91.42 61.31 70.40 31.80 65.74\nw/o Ecom-R\n72.77 90.67 62.55 74.00 38.20 67.64\nw/o Y\nouku-R 73.10 91.07 59.67 76.00 36.20 67.21\nw/o Amazon-R\n73.85 90.55 60.63 72.00 26.20 64.65\nw/o CCKS-R\n74.47 91.30 59.90 69.60 37.20 66.49\nw/o JDDC-R\n73.73 91.19 58.13 71.20 27.80 64.41\nTable 6: Ablation experiments on CoT tasks at dataset level.\n“w/o *-R” denotes without CoT data that is related to the “*”.\nTraining\nQA NER IC Unseen Dataset\nMi F1\nMa F1 Rouge\nFull 59.23\n80.67 65.30 48.37 45.05 59.20\nw/o QA-R\n56.75 79.78 61.55 40.18 37.89 52.37\nw/o NER-R\n59.00 77.55 63.30 45.50 43.97 55.14\nw/o IC-R\n57.54 80.49 60.40 41.12 36.94 52.28\nTable 7: Ablation experiments on CoT tasks at task level.\ntraining set of EcomInstruct. So at the dataset level, we per-\nformed held-in evaluation, i.e., evaluating the selected tasks\nin the training set. Our findings indicate that CoT tasks de-\nrived from the same dataset provided steady gains for the\noriginal task. However, in cross-dataset scenarios, the efficacy\nof CoT tasks is dependent on the data types corresponding to\nthe two datasets: for the same type of data that overlap in the\ntask chain, the CoT tasks can provide a collaborative effect.\nFor instance, the Ecom and Youku datasets both contain prod-\nuct titles, resulting in mutual gains. Conversely, there is no\ngain between CCKS and JDDC datasets, as their data types\nare addresses and dialogues, respectively, despite belonging\nto the same classification task.\nTask Level.In the Table 7, we eliminate all CoT tasks as-\nsociated with a given task and report the model’s performance\non unseen tasks and data. For example, for NER task, we ex-\nclude all entity detection and entity classification tasks from\nthe training set. Our results demonstrate that CoT tasks are\nadvantageous for both similar and dissimilar tasks. Notably,\nCoT tasks related to QA exhibit the greatest enhancement\nin generalization capacity to other tasks, while concurrently\nexhibiting greater difficulty in generalizing from CoT tasks\nfrom other tasks, which aligns with the finding in prior work\n(Zhou et al. 2022). We argue that, for instruction-following\nLLMs, tasks can be naturally abstracted to QA tasks, thereby\nplaying a crucial role in enhancing generalization ability.\nTask Paradigm Level.As demonstrated in Table 6, cer-\ntain CoT tasks of classification do not exhibit advantage over\nheld-in tasks of other paradigms at the dataset level. How-\never, as shown in Table 8, when viewed from a higher-level\nperspective of task paradigms, there is greater overlap among\ndata or task formats. Consequently, CoT tasks from different\nparadigms display a consistent gain for each other, with the\nCoT tasks of classification even exhibiting a greater gain over\nother paradigm tasks than on its own.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18588\nTraining\nCLS Ext Other Unseen Dataset\nMi F1\nMa F1 Rouge\nFull 67.87\n52.17 80.67 48.37 45.05 59.20\nw/o CLS-R\n65.69 47.49 80.38 46.87 43.75 57.00\nw/o Ext-R\n58.71 27.47 79.14 43.73 42.81 47.36\nw/o Gen-R\n56.67 50.87 80.58 41.38 40.20 54.39\nTable 8: Ablation on CoT tasks at task paradigm level.\nConclusion\nThis paper presents EcomInstruct, the first instruction-tuning\ndataset tailored for the E-commerce domain, encompassing\ntwo different part of instruction data, while the second part\ncomprises atomic tasks based on the basic data types in the E-\ncommerce domain, also known as Chain-of Task (CoT) tasks.\nThese CoT tasks are intermediate tasks implicitly involved\nin solving a targeted final task. Benefiting from the funda-\nmental semantic understanding capabilities acquired from the\nChain-of-Task tasks, EcomGPT , trained with EcomInstruct,\noutperforms ChatGPT in term of cross-dataset/task general-\nization on E-commerce tasks. The advantages of leveraging\nCoT tasks suggest that, within vertical domain scenarios, we\ncan devise diverse atomic tasks specifically tailored to the\ndomain data to enhance the model’s generalization ability.\nAcknowledgments\nThis research is supported by National Natural Science\nFoundation of China (Grant No.62276154), Research Cen-\nter for Computer Network (Shenzhen) Ministry of Educa-\ntion, the Natural Science Foundation of Guangdong Province\n(Grant No. 2023A1515012914), Basic Research Fund of\nShenzhen City (Grant No. JCYJ20210324120012033 and\nJSGG20210802154402007), the Major Key Project of PCL\nfor Experiments and Applications (PCL2021A06), Overseas\nCooperation Research Fund of Tsinghua Shenzhen Interna-\ntional Graduate School (HW2021008), and Shenzhen Science\nand Technology Program (WDZC20231128091437002).\nReferences\nAhmadvand, A.; Kallumadi, S.; Javed, F.; and Agichtein, E.\n2020. Jointmap: joint query intent understanding for mod-\neling intent hierarchies in e-commerce search. In Proceed-\nings of the 43rd International ACM SIGIR Conference on\nResearch and Development in Information Retrieval, 1509–\n1512.\nCelikyilmaz, A.; Clark, E.; and Gao, J. 2020. Evaluation of\ntext generation: A survey. arXiv preprint arXiv:2006.14799.\nChen, M.; Liu, R.; Shen, L.; Yuan, S.; Zhou, J.; Wu, Y .;\nHe, X.; and Zhou, B. 2020. The JDDC Corpus: A Large-\nScale Multi-Turn Chinese Dialogue Dataset for E-commerce\nCustomer Service. In Proceedings of the Twelfth Language\nResources and Evaluation Conference, 459–466.\nCheng, X.; Bowden, M.; Bhange, B. R.; Goyal, P.; Packer,\nT.; and Javed, F. 2021. An end-to-end solution for named\nentity recognition in ecommerce search. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 35,\n15098–15106.\nCheng, X.; Dong, Q.; Yue, F.; Ko, T.; Wang, M.; and Zou,\nY . 2023a. M 3 st: Mix at three levels for speech transla-\ntion. In ICASSP 2023-2023 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), 1–5.\nIEEE.\nCheng, X.; Xu, W.; Zhu, Z.; Li, H.; and Zou, Y . 2023b. To-\nwards spoken language understanding via multi-level multi-\ngrained contrastive learning. In Proceedings of the 32nd\nACM International Conference on Information and Knowl-\nedge Management, 326–336.\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang,\nH.; Zheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; et al.\n2023. Vicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality. See https://vicuna. lmsys. org\n(accessed 14 April 2023).\nEscursell, S.; Llorach-Massana, P.; and Roncero, M. B. 2021.\nSustainability in e-commerce packaging: A review. Journal\nof cleaner production, 280: 124314.\nJiang, H.; Cao, T.; Li, Z.; Luo, C.; Tang, X.; Yin, Q.; Zhang,\nD.; Goutam, R.; and Yin, B. 2022. Short Text Pre-training\nwith Extended Token Classification for E-commerce Query\nUnderstanding. CoRR, abs/2210.03915.\nLi, Y .; Chen, J.; Li, Y .; Yu, T.; Chen, X.; and Zheng, H.-\nT. 2022. Embracing Ambiguity: Improving Similarity-\noriented Tasks with Contextual Synonym Knowledge. CoRR\nabs/2211.10997 (2022).\nLin, C.-Y . 2004. ROUGE: A Package for Automatic Evalu-\nation of Summaries. In Text Summarization Branches Out,\n74–81. Barcelona, Spain: Association for Computational Lin-\nguistics.\nLoshchilov, I.; and Hutter, F. 2017. Fixing weight de-\ncay regularization in adam. arXiv 2017. arXiv preprint\narXiv:1711.05101.\nMishra, S.; Khashabi, D.; Baral, C.; and Hajishirzi, H. 2022.\nCross-Task Generalization via Natural Language Crowd-\nsourcing Instructions. In 60th Annual Meeting of the Associ-\nation for Computational Linguistics, ACL 2022, 3470–3487.\nAssociation for Computational Linguistics (ACL).\nMuennighoff, N.; Wang, T.; Sutawika, L.; Roberts, A.;\nBiderman, S.; Scao, T. L.; Bari, M. S.; Shen, S.; Yong,\nZ.-X.; Schoelkopf, H.; et al. 2022. Crosslingual gen-\neralization through multitask finetuning. arXiv preprint\narXiv:2211.01786.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;\nMishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; et al.\n2022. Training language models to follow instructions with\nhuman feedback. Advances in Neural Information Processing\nSystems, 35: 27730–27744.\nPoerner, N.; Waltinger, U.; and Sch¨utze, H. 2020. E-BERT:\nEfficient-Yet-Effective Entity Embeddings for BERT. In\nFindings of the Association for Computational Linguistics:\nEMNLP 2020, 803–818. Online: Association for Computa-\ntional Linguistics.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18589\nQiu, Y .; Zhao, C.; Zhang, H.; Zhuo, J.; Li, T.; Zhang, X.;\nWang, S.; Xu, S.; Long, B.; and Yang, W.-Y . 2022. Pre-\ntraining Tasks for User Intent Detection and Embedding Re-\ntrieval in E-commerce Search. In Proceedings of the 31st\nACM International Conference on Information & Knowledge\nManagement, 4424–4428.\nRastogi, A.; Zang, X.; Sunkara, S.; Gupta, R.; and Khaitan, P.\n2020. Towards scalable multi-domain conversational agents:\nThe schema-guided dialogue dataset. In Proceedings of the\nAAAI conference on artificial intelligence, volume 34, 8689–\n8696.\nScao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ili´c, S.; Hesslow,\nD.; Castagn´e, R.; Luccioni, A. S.; Yvon, F.; Gall´e, M.; et al.\n2022. Bloom: A 176b-parameter open-access multilingual\nlanguage model. arXiv preprint arXiv:2211.05100.\nTaori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y .; Li, X.;\nGuestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stanford\nalpaca: An instruction-following llama model.\nTsagkias, M.; King, T. H.; Kallumadi, S.; Murdock, V .; and\nde Rijke, M. 2021. Challenges and research opportunities\nin ecommerce search and recommendations. In ACM Sigir\nForum, volume 54, 1–23. ACM New York, NY , USA.\nWang, X.; Jiang, Y .; Bach, N.; Wang, T.; Huang, Z.; Huang,\nF.; and Tu, K. 2021. Improving Named Entity Recognition\nby External Context Retrieving and Cooperative Learning. In\nProceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing (Volume\n1: Long Papers), 1800–1812.\nWang, X.; Wei, J.; Schuurmans, D.; Le, Q. V .; Chi, E. H.;\nNarang, S.; Chowdhery, A.; and Zhou, D. 2022a. Self-\nConsistency Improves Chain of Thought Reasoning in Lan-\nguage Models. In The Eleventh International Conference on\nLearning Representations.\nWang, Y .; Mishra, S.; Alipoormolabashi, P.; Kordi,\nY .; Mirzaei, A.; Naik, A.; Ashok, A.; Dhanasekaran,\nA. S.; Arunkumar, A.; Stap, D.; et al. 2022b. Super-\nNaturalInstructions: Generalization via Declarative Instruc-\ntions on 1600+ NLP Tasks. In Proceedings of the 2022\nConference on Empirical Methods in Natural Language Pro-\ncessing, 5085–5109.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi,\nE.; Le, Q. V .; Zhou, D.; et al. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35: 24824–24837.\nXu, S.; Li, H.; Yuan, P.; Wang, Y .; Wu, Y .; He, X.; Liu, Y .; and\nZhou, B. 2021. K-PLUG: Knowledge-injected Pre-trained\nLanguage Model for Natural Language Understanding and\nGeneration in E-Commerce. In Findings of the Association\nfor Computational Linguistics: EMNLP 2021, 1–17.\nYu, T.; Jiang, C.; Lou, C.; Huang, S.; Wang, X.; Liu, W.;\nCai, J.; Li, Y .; Li, Y .; Tu, K.; Zheng, H.-T.; Zhang, N.; Xie,\nP.; Huang, F.; and Jiang, Y . 2023. SeqGPT: An Out-of-the-\nbox Large Language Model for Open Domain Sequence\nUnderstanding. arXiv:2308.10529.\nZhang, W.; Wong, C.-M.; Ye, G.; Wen, B.; Zhang, W.; and\nChen, H. 2021. Billion-scale pre-trained e-commerce product\nknowledge graph model. In 2021 IEEE 37th International\nConference on Data Engineering (ICDE), 2476–2487. IEEE.\nZhao, J.; Chen, H.; and Yin, D. 2019. A dynamic product-\naware learning model for e-commerce query intent under-\nstanding. In Proceedings of the 28th ACM International\nConference on Information and Knowledge Management,\n1843–1852.\nZhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y .;\nMin, Y .; Zhang, B.; Zhang, J.; Dong, Z.; et al. 2023. A survey\nof large language models. arXiv preprint arXiv:2303.18223.\nZhou, J.; Lin, Z.; Zheng, Y .; Li, J.; and Yang, Z. 2022. Not\nAll Tasks Are Born Equal: Understanding Zero-Shot Gen-\neralization. In The Eleventh International Conference on\nLearning Representations.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18590",
  "topic": "Task (project management)",
  "concepts": [
    {
      "name": "Task (project management)",
      "score": 0.7366006374359131
    },
    {
      "name": "Computer science",
      "score": 0.7229875326156616
    },
    {
      "name": "Chain (unit)",
      "score": 0.5118881464004517
    },
    {
      "name": "Natural language processing",
      "score": 0.40465012192726135
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3427528440952301
    },
    {
      "name": "Engineering",
      "score": 0.07850828766822815
    },
    {
      "name": "Systems engineering",
      "score": 0.055779218673706055
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Astronomy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210136793",
      "name": "Peng Cheng Laboratory",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210086143",
      "name": "Alibaba Group (Cayman Islands)",
      "country": "KY"
    },
    {
      "id": "https://openalex.org/I30809798",
      "name": "ShanghaiTech University",
      "country": "CN"
    }
  ],
  "cited_by": 22
}