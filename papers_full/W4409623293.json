{
  "title": "Large Language Models: Pioneering New Educational Frontiers in Childhood Myopia",
  "url": "https://openalex.org/W4409623293",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A3158705612",
      "name": "Mohammad Delsoz",
      "affiliations": [
        "University of Tennessee Health Science Center"
      ]
    },
    {
      "id": "https://openalex.org/A2190337206",
      "name": "Amr Hassan",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A2123678023",
      "name": "Amin Nabavi",
      "affiliations": [
        "University of Tennessee Health Science Center"
      ]
    },
    {
      "id": "https://openalex.org/A3088866846",
      "name": "Amir Rahdar",
      "affiliations": [
        "University of Tennessee Health Science Center"
      ]
    },
    {
      "id": "https://openalex.org/A2126371951",
      "name": "Brian Fowler",
      "affiliations": [
        "University of Tennessee Health Science Center"
      ]
    },
    {
      "id": "https://openalex.org/A2105757149",
      "name": "Natalie C. Kerr",
      "affiliations": [
        "University of Tennessee Health Science Center"
      ]
    },
    {
      "id": null,
      "name": "Lauren Claire Ditta",
      "affiliations": [
        "University of Tennessee Health Science Center"
      ]
    },
    {
      "id": "https://openalex.org/A4314475673",
      "name": "Mary E. Hoehn",
      "affiliations": [
        "University of Tennessee Health Science Center"
      ]
    },
    {
      "id": "https://openalex.org/A2139505646",
      "name": "Margaret M. DeAngelis",
      "affiliations": [
        "University at Buffalo, State University of New York",
        "VA Western New York Healthcare System"
      ]
    },
    {
      "id": "https://openalex.org/A1854755823",
      "name": "Andrzej Grzybowski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2006203058",
      "name": "Yih‐Chung Tham",
      "affiliations": [
        "National University of Singapore",
        "National University Health System"
      ]
    },
    {
      "id": "https://openalex.org/A2300720113",
      "name": "Siamak Yousefi",
      "affiliations": [
        "University of Tennessee Health Science Center"
      ]
    },
    {
      "id": "https://openalex.org/A3158705612",
      "name": "Mohammad Delsoz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2190337206",
      "name": "Amr Hassan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2123678023",
      "name": "Amin Nabavi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3088866846",
      "name": "Amir Rahdar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2126371951",
      "name": "Brian Fowler",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105757149",
      "name": "Natalie C. Kerr",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Lauren Claire Ditta",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4314475673",
      "name": "Mary E. Hoehn",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2139505646",
      "name": "Margaret M. DeAngelis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1854755823",
      "name": "Andrzej Grzybowski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2006203058",
      "name": "Yih‐Chung Tham",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2300720113",
      "name": "Siamak Yousefi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4402757343",
    "https://openalex.org/W2801240042",
    "https://openalex.org/W4403749724",
    "https://openalex.org/W3157556831",
    "https://openalex.org/W2273479788",
    "https://openalex.org/W1996409699",
    "https://openalex.org/W4394796437",
    "https://openalex.org/W3135672007",
    "https://openalex.org/W4401814064",
    "https://openalex.org/W4400849717",
    "https://openalex.org/W4392099631",
    "https://openalex.org/W4386726071",
    "https://openalex.org/W4386733942",
    "https://openalex.org/W4391251019",
    "https://openalex.org/W4403483733",
    "https://openalex.org/W4394912119",
    "https://openalex.org/W1987094633",
    "https://openalex.org/W74150004",
    "https://openalex.org/W4220983040",
    "https://openalex.org/W4387440167",
    "https://openalex.org/W4220678262",
    "https://openalex.org/W2123109647",
    "https://openalex.org/W4386117408",
    "https://openalex.org/W2590021844",
    "https://openalex.org/W2101950949",
    "https://openalex.org/W4295169240",
    "https://openalex.org/W2902325099",
    "https://openalex.org/W2138484699",
    "https://openalex.org/W1507711477",
    "https://openalex.org/W4368340908",
    "https://openalex.org/W4367175039",
    "https://openalex.org/W4386110374",
    "https://openalex.org/W4391815795",
    "https://openalex.org/W4392303707",
    "https://openalex.org/W4385294579",
    "https://openalex.org/W4367311080",
    "https://openalex.org/W6602984445",
    "https://openalex.org/W4382774929",
    "https://openalex.org/W4386776401",
    "https://openalex.org/W4387232979",
    "https://openalex.org/W1646365943",
    "https://openalex.org/W4405524400",
    "https://openalex.org/W4405942507"
  ],
  "abstract": "ChatGPT-4o (o1 Preview) demonstrates potential in producing accurate, good-quality, understandable PEMs, and in improving online PEMs on childhood myopia.",
  "full_text": "Vol.:(0123456789)\nOphthalmol Ther (2025) 14:1281–1295 \nhttps://doi.org/10.1007/s40123-025-01142-x\nORIGINAL RESEARCH\nLarge Language Models: Pioneering New Educational \nFrontiers in Childhood Myopia\nMohammad Delsoz · Amr Hassan · Amin Nabavi · Amir Rahdar · Brian Fowler · \nNatalie C. Kerr · Lauren Claire Ditta · Mary E. Hoehn · Margaret M. DeAngelis · \nAndrzej Grzybowski · Yih‑Chung Tham · Siamak Yousefi \nReceived: February 12, 2025 / Accepted: April 2, 2025 / Published online: April 21, 2025 \n© The Author(s) 2025\nABSTRACT\nIntroduction: This study aimed to evaluate \nthe performance of three large language mod‑\nels (LLMs), namely ChatGPT ‑3.5, ChatGPT ‑4o \n(o1 Preview), and Google Gemini, in produc‑\ning patient education materials (PEMs) and \nimproving the readability of online PEMs on \nchildhood myopia.\nMethods : LLM‑generated responses were \nassessed using three prompts. Prompt  A \nrequested to “Write educational material on \nchildhood myopia.” Prompt B added a modi‑\nfier specifying “a sixth‑grade reading level using \nthe FKGL (Flesch‑Kincaid Grade Level) read‑\nability formula.” Prompt C aimed to rewrite \nexisting PEMs to a sixth‑grade level using FKGL. \nReponses were assessed for quality (DISCERN \ntool), readability (FKGL, SMOG (Simple Measure \nof Gobbledygook)), Patient Education Materials \nSupplementary Information The online version \ncontains supplementary material available at \nhttps:// doi. org/ 10. 1007/ s40123‑ 025‑ 01142‑x.\nM. Delsoz · A. Nabavi · A. Rahdar · B. Fowler · \nN. C. Kerr · L. C. Ditta · M. E. Hoehn · S. Yousefi (*) \nHamilton Eye Institute, Department \nof Ophthalmology, University of Tennessee Health \nScience Center, 930 Madison Ave., Suite 471, \nMemphis, TN 38163, USA\ne‑mail: Siamak.Yousefi@uthsc.edu\nA. Hassan \nDepartment of Ophthalmology, Gavin Herbert Eye \nInstitute, University of California, Irvine, CA, USA\nM. M. DeAngelis \nDepartement of Ophthalmology, University \nat Buffalo, Buffalo, NY, USA\nM. M. DeAngelis \nResearch Service, VA Western New York Healthcare \nSystem, Buffalo, NY, USA\nA. Grzybowski \nInstitute for Research in Ophthalmology, \nFoundation for Ophthalmology Development, \nPoznan, Poland\nY.‑C. Tham \nYong Loo Lin School of Medicine, National \nUniversity of Singapore, Singapore, \nRepublic of Singapore\nY.‑C. Tham \nCentre of Innovation and Precision Eye Health, \nDepartment of Ophthalmology, Yong Loo \nLin School of Medicine, National University \nof Singapore and National University Health \nSystem, Singapore, Republic of Singapore\nS. Yousefi \nDepartment of Genetics, Genomics, \nand Informatics, University of Tennessee Health \nScience Center, Memphis, TN, USA\n1282 Ophthalmol Ther (2025) 14:1281–1295\nAssessment Tool (PEMAT, understandability/\nactionability), and accuracy.\nResults:  ChatGPT ‑4o (01) and ChatGPT ‑3.5 \ngenerated good‑quality PEMs (DISCERN 52.8 and \n52.7, respectively); however, quality declined \nfrom prompt  A to prompt  B ( p = 0.001 and \np = 0.013). Google Gemini produced fair‑qual‑\nity (DISCERN 43) but improved with prompt B \n(p = 0.02). All PEMs exceeded the 70% PEMAT \nunderstandability threshold but failed the 70% \nactionability threshold (40%). No misinforma‑\ntion was identified. Readability improved with \nprompt B; ChatGPT ‑4o (01) and ChatGPT ‑3.5 \nachieved a sixth‑grade level or below (FGKL \n6 ± 0.6 and 6.2 ± 0.3), while Google Gemini did \nnot (FGKL 7  ± 0.6). ChatGPT ‑4o (01) outper ‑\nformed Google Gemini in readability (p < 0.001) \nbut was comparable to ChatGPT‑3.5 (p = 0.846). \nPrompt C improved readability across all LLMs, \nwith ChatGPT‑4o (o1 Preview) showing the most \nsignificant gains (FKGL 5.8 ± 1.5; p < 0.001).\nConclusions : ChatGPT‑4o (o1 Preview) dem‑\nonstrates potential in producing accurate, good‑\nquality, understandable PEMs, and in improving \nonline PEMs on childhood myopia.\nKeywords: Large language models; Patient \neducation materials; Childhood myopia\nKey Summary Points \nThis study evaluates the ability of artificial \nintelligence (AI)‑powered tools, including \nChatGPT‑3.5, ChatGPT‑4o (o1 Preview), and \nGoogle Gemini, to generate and improve the \nreadability of existing educational materials \non childhood myopia.\nIt demonstrates that these models, particu‑\nlarly ChatGPT‑4o (o1 Preview), can produce \ngood‑quality, understandable, accurate, user‑\nfriendly content that meets readability stand‑\nards, and improves the readability of existing \nonline educational materials on childhood \nmyopia at or below the sixth‑grade level.\nThe study also highlights the importance of \nprompt customization in enhancing content \nclarity and provides practical recommen‑\ndations for leveraging AI tools to improve \nhealth education.\nThe study identifies areas for improvement, \nsuch as enhancing actionability and incorpo‑\nrating multimedia elements.\nINTRODUCTION\nChildhood myopia, commonly referred to as \nnearsightedness, has emerged as a significant \nglobal public health concern. Recent studies \nindicate that approximately one‑third of chil‑\ndren and adolescents worldwide are affected \nby myopia, with projections estimating that \nthe global prevalence will reach nearly 40% by \n2050, equating to over 740 million cases in this \nage group [1 ]. The prevalence among children \nis particularly alarming; studies have reported \nthat in East Asian countries, up to 80–90% of \nyoung adults are myopic [2 ]. In the USA, the \nprevalence of myopia in children has increased \nsubstantially over recent decades [3 ].\nThe increasing incidence of myopia in chil ‑\ndren has been related to various factors, includ‑\ning environmental influences, and genetic \npredisposition [4]. Notably, the COVID‑19 pan‑\ndemic and associated lockdowns have exacer‑\nbated this issue by increasing screen time and \nreducing outdoor activities, both of which are \nsignificant risk factors for the development and \nprogression of myopia [1]. This shift in behavior \nhas led to a notable rise in myopia cases among \nchildren during and after the pandemic period.\nEffective patient education materials (PEMs) are \ncrucial in managing and mitigating this condi‑\ntion, as they have the ability to enhance societal \nunderstanding of myopia and encourage proac‑\ntive health behaviors. Effective management and \nmitigation of childhood myopia heavily rely on \ncomprehensive PEMs [5]. These resources are vital \nfor enhancing understanding and promoting pro‑\nactive health behaviors among patients and their \n1283Ophthalmol Ther (2025) 14:1281–1295 \nfamilies. However, existing PEMs often exceed \nthe sixth‑grade reading level as recommended by \nAmerican Medical Association (AMA), rendering \nthem less accessible to the general population \n[6–8].\nRecently, the advent of large language models \n(LLMs), including OpenAI’s ChatGPT, Google’s \nGemini, and other artificial intelligence (AI) chat‑\nbots, has introduced innovative applications in \nmedicine, particularly in ophthalmology. Recent \nstudies have demonstrated promising results \nshowing that their proficiency in natural language \nprocessing enables them to interpret complex \nmedical data, assist in diagnosing ocular condi‑\ntions, helping in research, and provide personal‑\nized treatment recommendations [9 –15]. These \nAI‑driven models have demonstrated potential \nin health information sector across various medi‑\ncal fields. Yet, their effectiveness in creating PEMs \nspecifically tailored for childhood myopia and \nimproving the readability of the existing PEMs \nremains underexplored.\nThis study purposes to assess the quality, read‑\nability, actionability, and accuracy of PEMs on \nchildhood myopia generated by LLMs, including \nChatGPT‑3.5, ChatGPT‑4o (01 Preview version), \nand Google Gemini. By assessing these models’ \ncapabilities in producing understandable and \nactionable health information and enhancing the \nreadability of existing online resources, we seek to \ndetermine their viability as supplementary tools \nin patient education.\nMETHODS\nThe study was exempt from ethical review of \nThe University of Tennessee Health Science \nCenter as it did not involve human participants \nor their personal data, focusing instead on evalu‑\nating the performance of the latest AI models. \nThe focus on publicly available data and AI‑gen‑\nerated text ensured compliance with privacy and \nresearch ethics standards. The study took place \nfrom October to December 2024, following the \nprinciples of the Declaration of Helsinki.\nStudy Design\nThis study aimed to assess how useful LLMs are \nin creating PEMs for childhood myopia and \nimproving the readability of current online \nresources. It compared the performance of Chat‑\nGPT‑3.5, ChatGPT‑4o (o1 Preview), and Gemini \nin generating new PEMs and rewriting existing \nones to make them more accessible (Supplemen‑\ntal‑1 in the Supplementary Material).\nLarge Language Models Selections\nThe LLMs evaluated in this study were Chat‑\nGPT‑3.5, ChatGPT‑4o (o1 Preview), and Gemini. \nThese models were selected for their widespread \naccessibility and demonstrated capabilities in \ngenerating and refining text. ChatGPT models \nwere accessed via the OpenAI platform, and \nGemini was accessed through its dedicated web \ninterface.\nPrompt Design\nTo generate PEMs on childhood myopia, two \ndistinct prompts were crafted:\n• Prompt A (control): Write educational mate‑\nrial on childhood myopia that the average \nAmerican can easily understand. This gen‑\neral prompt was deliberately crafted without \nspecific readability guidelines to establish a \nbaseline understanding of the inherent read‑\nability and quality of LLM‑generated PEMs.\n• Prompt  B (modified): “Since the average \nAmerican can read at a sixth‑grade reading \nlevel, utilizing the FKGL (Flesch ‑Kincaid \nGrade Level) readability formula, write edu ‑\ncational material on childhood myopia that \nis easy for the average American to under‑\nstand?” (as advised by the American Medi ‑\ncal Association) (Supplemental‑2 in the Sup‑\nplementary Material). This prompt explicitly \ninstructed the models to generate materials \nsuitable for a sixth‑grade reading level, ref‑\nerencing FKGL readability formula directly. \nWe chose this detailed wording to assess how \neffectively LLMs could produce targeted, \n1284 Ophthalmol Ther (2025) 14:1281–1295\nreadable content when provided with explicit \nguidelines, as recommended by AMA stand‑\nards for PEM readability.\nIn prompt B, we referenced the Flesch‑Kincaid \nGrade readability formula when creating patient \neducational materials for patient‑targeted level \nby each LLM primarily because it is widely rec‑\nognized and endorsed by prominent organi‑\nzations, including the AMA and the National \nInstitutes of Health (NIH) [16, 17], for assess‑\ning PEMs. Unlike other readability metrics, \nFKGL specifically evaluates sentence length \nand word complexity, two critical components \nthat affect comprehension, making it especially \nsuited for health information targeted toward a \nbroad audience. It is a valuable tool, especially \nin educational and publishing fields, to ensure \nthat information is clear and accessible to the \nintended audience [17].\nEach LLM was tasked with responding to \nthese prompts (A and B) in 20 trials, ensuring \nseparate and unbiased chat instances for each \ntrial to avoid the influence of Reinforcement \nLearning from Human Feedback (RLHF) features \nof the LLM chatbots [18, 19].\nFor improving already available online \nresources, first 20 eligible existing online PEMs \non childhood myopia were sourced from the \nfirst two pages of a Google search engine using \nthe keyword “childhood myopia” from Octo‑\nber to December 2024. We acknowledge the \nconcern that restricting our search to the first \n20 eligible Google search results may introduce \nselection bias, as these listings might not rep‑\nresent all existing online resources. However, \nour rationale for using this approach is rooted \nin real‑world user behavior. Numerous stud‑\nies indicate that most individuals searching \nfor health information online seldom venture \nbeyond the first two pages of search results, \nas they account for more than 95% of total \nweb traffic among search engine users [20, 21]. \nBy focusing on the materials patients are most \nlikely to encounter, we aimed to evaluate the \neducational content that exerts the greatest \npractical impact on health literacy and patient \neducation. While we recognize this does not \ncapture every possible resource, it does reflect \nthe sources patients are statistically more likely \nto see. Excluded materials included advertise‑\nments, academic articles, chapters from books, \nmultimedia content such as videos, personal \nweblogs, websites in languages other than Eng‑\nlish, resources for clinical decision‑making, \nand non‑patient‑targeted sources. Each PEM \nwas inputted into the LLMs with the follow‑\ning prompt:\nPrompt C: “Since patient education materi‑\nals (PEMs) are recommended to be written at \na sixth‑grade reading level, could you rewrite \nthe following text to meet this standard using \nthe FKGL readability formula? [insert text]?” \nPrompt C is designed to evaluate the LLMs’ \nability to revise existing materials; this prompt \ninstructed the models to rewrite existing PEMs \nspecifically to meet a sixth‑grade reading level \nusing FKGL. The explicit reference to FKGL \nagain allowed us to assess the models’ profi‑\nciency in adjusting existing content complex‑\nity to patient‑target level, reflecting real‑world \nscenarios where existing PEMs must be adapted \nfor improved readability. Finally, our approach \ninvolved collecting 20 PEMs per LLM (60 in \ntotal) in prompt A, generating another 20 per \nLLM in prompt B (60 more), and evaluating \n20 existing online educational resources and \nimproving 20 existing online educational \nresources per three LLMs (60 total). This \nresulted in the analysis of 200 educational \nmaterials handouts overall. We used online \nreada bilit yform ulas. com, a free online tool for \ncalculating readability scores and other key \nmetrics [22]. This software employs proven \nreadability tools for its calculations and has \nbeen widely used in notable medical studies \n[23–25]. Essential readability metrics, such \nas syllable count, word count, complex word \ncount, sentence count, SMOG (Simple Measure \nof Gobbledygook) readability score, and FKGL \nscore, were assessed for all PEMs.\nEvaluation Metrics of the Generated PEMs\nWe utilized the full 16‑item validated DISCERN \ninstrument for the quality and reliability of \nthe generated PEMs and rewritten PEMs for \nevaluations [26]. This evaluation tool includes \na 16‑item questionnaire designed for use by \n1285Ophthalmol Ther (2025) 14:1281–1295 \nreviewers. Each item is rated on a scale from 1 \nto 5, where scores of 1–2 denote low quality, 3 \nrepresents moderate quality, and 4–5 indicate \nhigh quality [27]. The DISCERN questionnaire \nis organized into three sections. The first sec‑\ntion (questions 1–8) focuses on the reliability \nof the publication. It evaluates aspects such \nas the publication’s purpose, the credibility \nof its information sources, its relevance, and \nwhether it offers additional resources about the \ncondition. The second section (questions 9–15) \nexamines the details provided about treat‑\nments, including how they work, their poten‑\ntial risks and benefits, and whether alternative \ntreatment options are presented. Finally, the \nthird section (question 16) assesses the over‑\nall quality of the publication as a resource for \ntreatment information (Supplemental‑3A in \nthe Supplementary Material). We calculated \nthe total DISCERN score by summing all 16 \nitem ratings (range 16–80) for each reviewer \nper educational material. We then mapped the \nDISCERN score to the corresponding quality \nlevel (Table  1), following the recommended \ncalculation methods and analyzed it. A grad‑\ning system, outlined in Table  1, was developed \nbased on the scores earned by each PEMs [28].\nTwo evaluators (ophthalmologists) assessed \nthe generated and rewritten PEM of 120 hand‑\nouts. To minimize bias, evaluators were blinded \nto each other’s assessments, the identities of \nthe sources generating the responses were \nremoved, and the final score for each PEM was \ndetermined by taking the median score from \nthe reviewers. Additionally, evaluators assessed \nthe understandability and actionability of the \nPEMs using the Patient Education Materials \nAssessment Tool (PEMAT), a validated instru‑\nment from the Agency for Healthcare Research \nand Quality (AHRQ) [29, 30] (Supplemental‑3B \nin the Supplementary Material). Understand‑\nability, defined as the ability of diverse audi‑\nences to comprehend and explain core mes‑\nsages, was calculated as an overall percentage \nbased on 12 yes/no questions covering items \nsuch as clarity and word choice. Actionability, \nreferring to the ease with which patients could \nidentify actionable next steps, was measured \nusing five targeted yes/no questions. Materi ‑\nals scoring 70% or higher were classified as \n“understandable” or “actionable” [31 , 32]. \nBeyond this threshold, PEMAT scores provided \na basis for comparative evaluations. Lastly, the \naccuracy of the generated PEMs was evaluated \nusing a Likert scale for misinformation, ranging \nfrom 1 to 5. A score of 1 meant “no misinfor‑\nmation,” 3 indicated “moderate misinforma‑\ntion,” and 5 represented “high misinforma‑\ntion” [27]. This thorough assessment provided \na well‑rounded analysis of the quality, usabil‑\nity, and reliability of educational materials on \nchildhood myopia.\nReadability Assessment of PEMs\nTo evaluate how easy it is to read and understand \nall the PEMs, we used two well‑known readability \nmetrics—the SMOG and the FKGL formulas—via \nthe online readability calculator Reada ble. com. \nThese metrics assess the grade level required to \ncomprehend the material, with SMOG focusing \non estimates of how many years of education \nsomeone would need to understand a given piece \nof writing. SMOG focuses on the number of poly‑\nsyllabic words within a set number of sentences, \nproviding a grade‑level score [33]. The FKGL metric \ndetermines readability by looking at the average \nnumber of syllables in each word and the number \nof words in each sentence, yielding a US school \ngrade level [34]. Scores ranging from 1 through \nT able 1  DISCERN score grading and its quality equiva -\nlent levels\nDISCERN score Out of 100 (%) Quality level\n64–80 80 and above Excellent\n52–63 65–79 Good\n41–51 51–64 Fair\n30–40 37–50 Poor\n16–29 20–36 V ery poor\n1286 Ophthalmol Ther (2025) 14:1281–1295\n12 reflect reading difficulty levels that correspond \nto grades 1 through 12, scores between 13 and \n16 indicate text complexity at the college level, \nand any score of 17 or higher signifies that a text \ndemands a more advanced understanding than a \ncollege education typically provides [35, 36]. For‑\nmulas below represent details of these two metrics:\nStatistical Analysis\nWe used two‑sample t tests to compare readabil‑\nity metrics and Mann–Whitney U tests for qual‑\nity, understandability, actionability, and accu‑\nracy. We then compared the readability scores \nacross all three language models using a one‑way \nanalysis of variance (ANOVA). To identify any \nsignificant differences in performance between \nthe models, we followed up with Tukey’s hon‑\nestly significant difference (HSD) test. A signifi‑\ncance level of p < 0.05 was set for all analyses. \nStatistical analyses were conducted using SPSS \n(version 29, IBM Corp, USA) software.\nRESULTS\nAll generated PEMs in prompt A by ChatGPT ‑\n4o (01‑Preview version) and ChatGPT ‑3.5 were \nof good quality (median DISCERN score of 52.8 \nand 52.7, respectively). The quality of PEMs \ngenerated by ChatGPT ‑4o (o1  Preview) and \nChatGPT‑3.5 showed a significant decline from \nprompt A to prompt B, respectively (p  = 0.001 \nand p = 0.013). The PEMs generated by Google \nGemini were of fair quality with median DIS‑\nCERN score of 43. However, the quality of PEMs \nincreased from prompt A to prompt B (p = 0.02). \nAll responses generated by the three LLMs \nexceeded the 70% threshold for being classi‑\nfied as “understandable” (based on the PEMAT \nFlesch− Kincaid Grade Level\n= 0.39\n(\ntotal words/total sentences\n)\n+ 11.8\n(\ntotal syllables/total words\n)\n15.59\nSMOG grade level = 1.0430×\n√ (\nnumber of polysyllabic words×\n(\n30 ÷ number of sentences\n))\n+ 3.1291\nunderstandability scale, which ranges from 0 to \n100%, with scores of 70% or higher considered \n“understandable”) (Table 2).\nTherefore, based on the PAMET understand‑\nability scale, all the generated PEMs had under‑\nstandable details and information, clear purpose \nand include only the most important informa‑\ntion, avoiding any distracting details, sufficient \nlayout and design, logical sequence, with head‑\ners to separate sections and a summary of key \npoints. None of the responses generated by the \nLLMs met the criteria to be classified as “action‑\nable” (as defined by the PEMAT actionability \nscale, where scores range from 0 to 100%, with \n70% or higher considered actionable). Across \nall 120 responses, the LLMs consistently scored \n40%. This indicates that the content and struc‑\nture of the generated educational materials did \nnot clearly provide step‑by‑step guidance for \npatients to take actionable steps.\nAll 120 responses received a score of 1 on the \nLikert misinformation scale, indicating that \nthese three LLMs did not produce any misinfor‑\nmation across the 120 newly generated PEMs.\nOur readability analysis revealed that \nprompt  B produced more readable PEMs \ncompared to prompt A, as reflected by lower \nSMOG and FKGL scores for both ChatGPT ‑3.5 \nand ChatGPT ‑4o (o1 Preview) (p  < 0.001). This \nimprovement was reflected in key readability \nmetrics, such as a reduction in syllables, word \ncount, 3+ syllable words (complex words), and \nsentence count (p < 0.05) (Table 3).\nAmong these LLMs, both ChatGPT‑4o (o1 Pre‑\nview) and ChatGPT ‑3.5 could generate educa‑\ntional materials at or below the sixth‑grade read‑\ning level in response to prompt B, respectively \n(FGKL scores 6 ± 0.6; FGKL scores 6.2 ± 0.3), while \nGoogle Gemini could not produce material at \nthis grade level (FGKL scores; 7 ± 0.6).\nIn head‑to‑head analysis, ChatGPT‑4o (o1 Pre‑\nview) generated PEMs more readable (lower \nSMOG and FGKL scores; 5.8 ± 0.7 and 6 ± 0.6, \nrespectively) than the Google Gemini (p < 0.001), \nalthough the difference was not statistically \n1287Ophthalmol Ther (2025) 14:1281–1295 \nsignificant compared to ChatGPT‑3.5 (p = 0.846) \n(Fig. 1).\nUsing prompt  C, the three LLMs signifi‑\ncantly improved the readability of existing \nonline educational resources, as evidenced by \na marked enhancement in average readability \nscores (p  ≤ 0.001) (Fig.  2). The original online \nPEMs about the childhood myopia had a mean \nSMOG score of 10.3 ± 2.2 and an FKGL score of \n9.7 ± 1.9 which is well above the recommended \nsixth‑grade reading level (Table 4).\nAfter applying the LLMs, significant improve‑\nments were observed (p  < 0.001). ChatGPT‑3.5 \nreduced the readability scores to a SMOG of \n7.6 ± 1.2 and an FKGL of 7.7 ± 1.4. Google Gemini \nalso contributed to improved readability, with \na SMOG of 7.8 ± 1.3 and an FKGL of 7.5 ± 1.1, \nwhile ChatGPT ‑4o (o1 Preview) demonstrated \nthe most significant enhancement, achieving \nor staying below the specified sixth‑grade read‑\ning level (SMOG 5.3 ± 1.6, FKGL 5.8 ± 1.5). Addi‑\ntionally, in a head‑to‑head analysis of readability \nimprovements made by the LLMs, ChatGPT ‑4o \n(o1 Preview) consistently outperformed both \nChatGPT‑3.5 and Google Gemini by a signifi‑\ncant margin (p  < 0.001 for both comparisons) \n(Fig. 3). These results highlight the transforma‑\ntive potential of LLMs, especifically ChatGPT ‑\n4o (o1 Preview), in simplifying complex medi ‑\ncal materials, making them more accessible and \nuser‑friendly for patients.\nDISCUSSION\nTo our knowledge, this is the first study to \nexplore how LLMs can assist parents in under‑\nstanding online health information and gen‑\nerating PEMs about childhood myopia. Unlike \nearlier studies that primarily relied on standard‑\nized exams and related queries for evaluation \n[37–39], our research takes a different approach \nby exploring realistic scenarios where worried \nT able 2  Comparing prompt A and prompt B: evaluating the quality and understandability of large language models in gen -\nerated patient education materials\nLLM large language model, PEMAT Patient Education Materials Assessment T ool\n*Mann–Whitney U test conducted between prompt A and B (significance at p < 0.05)\nLLMs Discern points Median (range) N Mean ranks Sum ranks p value*\nQuality (DISCERN)\n Gemini Prompt A 43 (43–43) 20 16.52 330 0.021\nPrompt B 49 (37–57) 20 24.51 490\n ChatGPT3.5 Prompt A 52.72 (51–53) 20 24.18 483.52 0.013\nPrompt B 51 (49–53) 20 16.83 336.54\n ChatGP-4o (01 Preview) Prompt A 52.81 (49–53) 20 25.38 507.53 0.001\nPrompt B 50 (41–53) 20 15.63 312.52\nUnderstandability (PEMA T %)\n Gemini Prompt A 75 (75–83.3)% 20 20.53 410 1.00\nPrompt B 75 (75–83.3)% 20 20.52 410\n ChatGPT3.5 Prompt A 75 (75–83.3)% 20 19 380  0.389\nPrompt B 83.31 (75–83.3)% 20 22 440\n ChatGP-4o (01 Preview) Prompt A 75 (75–83.3)% 20 20 400 0.771\nPrompt B 83.32 (75–83.3)% 20 21 420\n1288 Ophthalmol Ther (2025) 14:1281–1295\nparents might turn to these emerging tools \nfor help. This highlights the critical need to \nassess how accurate and reliable the responses \nfrom LLM chatbots are in real‑world situations. \nTherefore, we assessed the performance of LLMs, \nnamely ChatGPT‑4o (01Preview), ChatGPT‑3.5, \nand Google Gemini, using a comprehensive \napproach that included DISCERN scores to \nevaluate the quality of the materials, PEMAT \nscores to measure their understandability and \nactionability, a misinformation scale to check \nfor accuracy, and readability metrics to ensure \nT able 3  Performance of LLMs based on readability metrics on prompt A vs prompt B\nLLM large language model, SMOG Simple Measure of Gobbledygook\nReadability \nmetrics\nChatGPT-3.5 ChatGPT-4o (01 preview) Google Gemini \nPrompt A Prompt B p value (A \nvs B)\nPrompt A Prompt B p value (A \nvs B)\nPrompt A Prompt B p value (A \nvs B)\nSyllables 789.7 \n(137.5)\n562.4 \n(76.9)\n< 0.001 861.9 \n(145.8)\n534.4 (80) < 0.001 558.9 \n(109.2)\n407.5 \n(107.8)\n< 0.001\nWo r d s 500.2 \n(96.9)\n381.6 \n(51.2)\n< 0.001 564.5 \n(91.8)\n370.5 \n(51.6)\n< 0.001 326.6 \n(60.2)\n272.2 \n(60.7)\n< 0.001\n3+ syllable \nwords\n37.3 (6.3) 18.4 (4.1) < 0.001 36.7 \n(10.8)\n15.35 (5) < 0.001 35.2 \n(10.9)\n17.6 (7.3) < 0.001\nSentences 27.5 (5.6) 27.3 (3.7) < 0.001 31.7 (4.9) 24.5 (4.3) < 0.001 19.7 (4.2) 17.6 (4.9) < 0.001\nSMOG \nReadability \nScore\n8.03 (0.6) 5.9 (0.4) < 0.001 7.7 (0.7) 5.8 (0.7) < 0.001 9.07 \n(0.62)\n6.7 (0.8) < 0.001\nFlesch-Kin-\ncaid Grade \nLevel\n7.7 (0.5) 6.2 (0.3) < 0.001 7.5 (0.58) 6 (0.6) < 0.001 8.5 (0.7) 7.0 (0.6) < 0.001\nFig. 1  Comparing the performance of large language \nmodels for prompt B based on SMOG (Simple Measure of \nGobbledygook), and FKGL (Flesch-Kincaid Grade Level) \nscores. One-way ANOVA (one-way analysis of variance), \npost hoc T ukey test\n1289Ophthalmol Ther (2025) 14:1281–1295 \nFig. 2  Readability of Online educational resources and performance of ChatGPT-4o (01 Preview), ChatGPT-3.5, and \nGoogle Gemini in improving the readability of online original resources. SMOG Simple Measure of Gobbledygook\nT able 4  Readability of original online resources and performance of LLMs for improving the readability of the original \nonline resources\nLLM large language model, SMOG Simple Measure of Gobbledygook\nReadability \nmetrics\nOriginal \nresources\nChatGPT-4o \n(01 preview)\np value \n(Orig. vs \nGPT-4o)\nChatGPT-3.5 p value \n(Orig. vs \nGPT-3.5)\nGoogle \nGemini\np value (Orig. \nvs Gemini)\nSyllables 1457.2 \n(736.9)\n330.4 (159.6) < 0.001 984.5 (412.7) 0.008 808.4 (205.7) < 0.001\nWo r d s 871.0 (392.7) 230.4 (107.9) < 0.001 649.2 (256.2) 0.21 521.9 (122.4) < 0.001\n3+ syllable \nwords\n91.0 (60.7) 13.3 (10.4) < 0.001 43.0 (23.1) 0.001 38.5 (12.6) < 0.001\nSentences 41.5 (17.1) 18.4 (7.8) 0.04 41.0 (16.6) 0.9 31.1 (9.0) 0.02\nSMOG Read-\nability Score\n10.3 (2.2) 5.3 (1.6) < 0.001 7.6 (1.2) < 0.001 7.8 (1.3) < 0.001\nFlesch-Kin-\ncaid Grade \nLevel\n9.7 (1.9) 5.8 (1.5) < 0.001 7.7 (1.4) < 0.001 7.5 (1.1) < 0.001\n1290 Ophthalmol Ther (2025) 14:1281–1295\nthe content was easy to comprehend. All gener‑\nated PEMs in prompt A by ChatGPT‑4o (o1 Pre‑\nview) and ChatGPT ‑3.5 were of good quality, \nwhile the PEMs generated by Google Gemini \nwere of fair quality. However, this should not \nbe understood as a clear superiority as ChatGPT \nover Google Gemini. Previous studies found \nthat each AI model has its own strengths and \nweaknesses with Google Gemini being superior \nin aspects like language understanding but lack‑\ning in other aspects in which ChatGPT excels \n[40]. All responses generated by the three LLMs \nwere understandable but none were actionable, \nas in they did not clearly provide step‑by‑step \nguidance for patients to take actionable steps. \nWe identified shortcomings in two key areas of \nactionability (PEMAT items 23 and 26), high‑\nlighting opportunities for improvement. First, \nthe responses lacked practical tools such as \naction reminder checklists or planners to help \npatients follow through with the information. \nSecond, since the responses were entirely text‑\nbased, they missed the added clarity and engage‑\nment that instructional visual aids could pro ‑\nvide. It is worth considering that incorporating \npatient action tools into the phrasing of prompts \ncould be a promising area for future research to \nenhance the actionability of upcoming PEMs. \nThis finding is very interesting because despite \nAI showing promise in developing management \nplans for patients in previous studies [41], our \npaper shows that it is unreliable in generating \naction plans for patients aiming to organize their \nchecklists when dealing with childhood myopia. \nThis is most likely not an inherent weakness in \nAI itself, but rather in the datasets on which it \nwas trained [41].\nAlthough health educational materials must \nbe understandable and easily readable, they \nmust also have high‑quality information. There \nwas not any misinformation across the 120 \nnewly generated PEMs. Our findings highlight \nthat the LLMs evaluated in this study did not \nproduce any AI “confabulations” or generate \nfalse or misleading information, a common issue \nreported in other studies [42, 43].\nAfter evaluations of the top 20 search results, \nas they account for more than 95% of total web \ntraffic among users of online web search engines \n[20], all results exceed the sixth‑grade reading \nlevel recommended by the NIH and the AMA, \nwhich is suggested for ensuring the average \nAmerican can understand health information \n[6, 8, 44]. All the evaluated LLMs demonstrated \nthe ability to rewrite or improve the readabil‑\nity of online educational resources. However, \nFig. 3  Performance of large language models for rewrit -\ning original handouts based on SMOG (Simple Measure of \nGobbledygook), and FKGL (Flesch-Kincaid Grade Level) \nscores. One-way ANOVA (one-way analysis of variance), \npost hoc T ukey test\n1291Ophthalmol Ther (2025) 14:1281–1295 \nChatGPT‑4o (o1 Preview) outperformed other \nmodels, including ChatGPT ‑3.5 and Google \nGemini, by significantly enhancing readability \nand consistently meeting or staying below the \ngrade level recommended by organizations like \nthe NIH and AMA for the average American to \nunderstand health information [ 16, 44]. Chat‑\nGPT‑4o (o1 Preview) adapted the content to a \nreading level at or below the average person’s \nproficiency, making the information more acces‑\nsible and easier to understand for a broader audi‑\nence. While ChatGPT ‑3.5 and Google Gemini \nimproved the readability of online PEMs, they \ndid not consistently meet the recommended \nstandards set by the NIH and AMA for under‑\nstanding health information. This underscores \nChatGPT‑4o (o1 Preview)’s superior ability to \nsimplify complex medical language into more \ndigestible formats for users.\nChatGPT‑4o (o1 Preview) has shown superi ‑\nority in generating the most readable LLMs in \nother diseases as well. In one study, ChatGPT ‑\n4o (o1 Preview) was tested for its usefulness \nin teaching individuals about dermatological \nconditions, and researchers found it could cre‑\nate paragraphs understandable by those with \na reading level ranging from high school to \nearly college [45]. Another evaluation explored \nChatGPT‑4o (o1 Preview)’s ability to produce \ninformation on uveitis by prompting it to write \npatient‑targeted health details about this con‑\ndition. Appropriateness and readability were \nthen assessed with the latter measured using \nthe FKGL formula; in all attempts, the content \nproved both suitable and easily adjustable [46]. \nIn another highly relevant study, researchers \nevaluated the performance of LLM chatbots for \nproducing patient‑targeted health materials on \nchildhood glaucoma using a prompting method \nfor gauging text comprehensibility. ChatGPT‑4o \n(o1 Preview) demonstrated promising results [7]. \nThis could be because ChatGPT‑4o (o1 Preview) \nwas trained on a larger dataset of written mate‑\nrials and has more parameters than its earlier \nversions [47]. However, as of now, ChatGPT ‑4o \n(o1 Preview) is only available through a paid \nsubscription, which may make it less accessi‑\nble to the general public. Our findings reveal \nthat the way a prompt is structured can signifi‑\ncantly impact the readability of the responses \ngenerated by an LLM. All three models created \nmore readable content when provided with a \nmore detailed query (prompt B) compared to a \nless specific one (prompt A). Prompt B was more \ndetailed, as it included a modifier statement that \nspecified the target grade level for readability \nand explicitly instructed the use of the “FKGL \nreadability formula.” Our findings highlight the \ncritical importance of using validated tools, such \nas readability formulas, to achieve the desired \nlevel of readability in educational materials. This \napproach aligns with recent studies, empha‑\nsizing that incorporating such tools ensures \ncontent is both accessible and effective for its \nintended audience [7, 46].\nThis study highlights the immense potential \nof LLMs, particularly ChatGPT‑4o (o1 Preview), \nas a tool for bridging gaps in health literacy and \nproviding tailored, patient‑centered educational \nmaterials. However, the lack of actionability in \nthe generated content underscores the need for \nfurther advancements in AI model development \nto create step‑by‑step, actionable health guid‑\nance for users. Future research should focus on \nexploring methods to optimize LLM outputs, \nsuch as integrating domain‑specific datasets \nand refining prompt engineering techniques to \nelicit more actionable responses. Additionally, \nstudies incorporating diverse linguistic and cul‑\ntural contexts would help generalize these find‑\nings beyond English‑speaking populations and \naddress global health literacy challenges. Given \nthe increasing reliance on visuals in health com‑\nmunication, future studies should also evaluate \nthe ability of LLMs to generate or recommend \nhigh‑quality visual aids, such as infographics \nand interactive tools, to complement textual \nmaterials. Such advancements could facilitate \nthe integration of LLMs into clinical workflows, \nenabling healthcare providers to create more \npersonalized and comprehensible patient edu‑\ncation resources in real time.\nIn summary, these findings highlight the abil‑\nity of LLMs to serve as a versatile tool for deliv‑\nering clear, producing good quality of PEMs, \nenhancing the readability of online educational \nresources, accessible health information and this \ncould open the door for integrating LLM chat ‑\nbots into the management of myopia care after \nmore advancement and refinements.\n1292 Ophthalmol Ther (2025) 14:1281–1295\nOur study has some limitations. As LLMs are \nnot perfect, the responses generated by LLMs \nare also limited to the data on which they are \ntrained. Moreover, the way prompts are phrased \nis inherently subjective. To minimize potential \nbias, we carefully broke down the prompts into \ndistinct control and modifier sub‑statements \nand conducted multiple repeated trials for each \nprompt while also relying on established prin‑\nciples of prompt engineering [47]. Furthermore, \nwe only evaluated PEM in the English language, \nwhich, although the most commonly used, but \nis not always the specific language in which \npatients would be interested in reading about \ntheir medical conditions. Moreover, for evaluat‑\ning existing online resources, we recognize that \nlimiting our scope to the top 20 eligible PEMs \nin Google has the potential to exclude lesser‑\nranked sites that could also provide valuable or \nalternative information. However, capturing the \nmost commonly accessed PEMs reflects a real‑\nworld usage pattern and therefore remains a \npractical and meaningful sample. Additionally, \nthis study primarily focuses on the US popula‑\ntion, as most of the research on readability and \nhealth literacy comes from studies conducted in \nthe USA. That said, it is important to acknowl‑\nedge that inadequate health literacy is a wide‑\nspread issue in many other major countries as \nwell [48, 49]. Finally, many patient‑focused \nhealth resources incorporate visuals, such as \ngraphics, images, videos, and demonstrations, \nto help improve understanding. However, our \nstudy did not evaluate these features.\nCONCLUSIONS\nThis study highlights the promising potential \nof large language models, particularly Chat‑\nGPT‑4o (o1 Preview), which, at the time of \nthis investigation, had the best performance \nof the evaluated LLMs in generating good‑\nquality, readable, and understandable patient \neducation materials for childhood myopia. \nWhile ChatGPT ‑4o (o1 Preview) outperformed \nother models in enhancing readability and \naccessibility, a significant limitation was the \nlack of actionable guidance in the generated \ncontent. This highlights the need for contin‑\nued improvements in AI models to develop \nmore actionable health information. Future \nresearch should focus on refining prompt engi‑\nneering, incorporating diverse linguistic and \ncultural contexts, and exploring multimodal \nfeatures like visuals to fully realize the poten‑\ntial of LLMs in patient education. With con‑\ntinued improvements, LLMs could play a key \nrole in making healthcare information more \naccessible, personalized, and effective.\nMedical Writing/Editorial Assistance. Dur‑\ning the preparation of this manuscript, the \nauthors utilized ChatGPT ‑4 from OpenAI for \nediting and proofreading purposes to enhance \nreadability. Subsequently, the authors carefully \nreviewed and refined the content as necessary \nand assume full responsibility for the final ver‑\nsion of the publication.\nAuthor Contributions. Mohammad Delsoz, \nAmr Hassan, Amin Nabavi, Amir Rahdar, Brian \nFowler, Natalie C. Kerr, Lauren Claire Ditta, Mary \nE. Hoehn, Margaret M DeAngelis, Andrzej Grzy‑\nbowski, Yih‑Chung Tham, and Siamak Yousefi  \ncontributed to conception of the study. Moham‑\nmad Delsoz, Amr. Hassan, Brian Fowler, Lauren \nClaire Ditta, Margaret M DeAngelis, Andrzej \nGrzybowski, Yih ‑Chung Tham, and Siamak \nYousefi contributed to study design. Mohammad \nDelsoz, Amr. Hassan, Amin Nabavi, Amir Rah‑\ndar, and Siamak Yousefi contributed to acquisi‑\ntion of the data. Mohammad Delsoz and Siamak \nYousefi contributed to contributed to analysis, \ninterpretation of the data, figures, and tables. \nMohammad Delsoz, Amr. Hassan, and Siamak \nYousefi accessed and verified each dataset during \nthe study. The research planning and execution \nwere supervised by Siamak Yousefi, Yih‑Chung \nTham, Margaret M DeAngelis, Andrzej Grzy ‑\nbowski, Brian Fowler, and Lauren Claire Ditta, \nMohammad Delsoz, Amr Hassan, Lauren Claire \nDitta, Andrzej Grzybowski, Yih‑Chung Tham, \nand Siamak Yousefi drafted the manuscript. All \nauthors read and approved the final version of \nthe manuscript.\n1293Ophthalmol Ther (2025) 14:1281–1295 \nFunding. This work was supported by \nHamilton Eye Institute (SY). The funder had no \nrole in study design, data collection and analy‑\nsis, decision to publish, or preparation of the \nmanuscript. The journal’s Rapid Service Fee was \nfunded by Dr. Siamak Yousefi.\nData Availability. All essential data for rep‑\nlicating our results is in the Supplementary file, \nexcept raw grader scores, which are available \nupon request.\nDeclarations \nConflict of Interest. Mohammad Delsoz, \nAmr Hassan, Amin Nabavi, Amir Rahdar, Brian \nFowler, Natalie C. Kerr, Lauren Claire Ditta, \nMary E. Hoehn, Margaret M DeAngelis, and Yih‑\nChung Tham have nothing to disclose. Andrzej \nGrzybowski is an Editorial Board member of  \nOphthalmology and Therapy. Andrzej Grzybowski \nwas not involved in the selection of peer review‑\ners for the manuscript nor any of the subsequent \neditorial decisions. Siamak Yousefi: Received pro‑\ntotype instruments from Remidio, M&S Tech‑\nnologies, and Visrtucal Fields. He gives consulta‑\ntions to the InsihgtAEye and Enolink.\nEthical Approval. The study was exempt \nfrom ethical review of The University of Tennes‑\nsee Health Science Center as it did not involve \nhuman participants or their personal data, \nfocusing instead on evaluating the performance \nof the latest AI models. The focus on publicly \navailable data and AI‑generated text ensured \ncompliance with privacy and research ethics \nstandards. The study took place from October \nto December 2024, following the principles of \nthe Declaration of Helsinki.\nOpen Access.  This article is licensed under a \nCreative Commons Attribution‑NonCommercial \n4.0 International License, which permits any \nnon‑commercial use, sharing, adaptation, distri‑\nbution and reproduction in any medium or for‑\nmat, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link \nto the Creative Commons licence, and indicate \nif changes were made. The images or other third \nparty material in this article are included in the \narticle’s Creative Commons licence, unless indi‑\ncated otherwise in a credit line to the material. \nIf material is not included in the article’s Crea‑\ntive Commons licence and your intended use is \nnot permitted by statutory regulation or exceeds \nthe permitted use, you will need to obtain per‑\nmission directly from the copyright holder. To \nview a copy of this licence, visit http:// creat iveco \nmmons. org/ licen ses/ by‑ nc/4. 0/.\nREFERENCES\n 1. Liang J, Pu Y, Chen J, et al. Global prevalence, \ntrend and projection of myopia in children and \nadolescents from 1990 to 2050: a comprehen‑\nsive systematic review and meta‑analysis. Br J \nOphthalmol. 2024. https:// doi. org/ 10. 1136/  \nbjo‑ 2024‑ 325427.\n 2. Modjtahedi BS, Ferris FL, Hunter DG, Fong DS. \nPublic health burden and potential interventions \nfor myopia. Ophthalmology. 2018;125(5):628–30.\n 3. Schweitzer K. With nearsightedness in children \non the rise, experts push for outdoor time, dis‑\nease designation. JAMA. 2024;332(19):1599–601. \nhttps:// doi. org/ 10. 1001/ jama. 2024. 21043.\n 4. Morgan IG, Wu P‑C, Ostrin LA, et al. IMI risk \nfactors for myopia. Investig Ophthalmol Vis Sci. \n2021;62(5):3–3. https:// doi. org/ 10. 1167/ iovs.  \n62.5.3.\n 5. Huang J, Wen D, Wang Q, et al. Efficacy compari‑\nson of 16 interventions for myopia control in chil‑\ndren: a network meta‑analysis. Ophthalmology. \n2016;123(4):697–708.\n 6. Stossel LM, Segar N, Gliatto P, Fallar R, Karani \nR. Readability of patient education materials \navailable at the point of care. J Gen Intern Med. \n2012;27:1165–70.\n 7. Dihan Q, Chauhan MZ, Eleiwa TK, et al. Using \nlarge language models to generate educational \nmaterials on childhood glaucoma. Am J Ophthal‑\nmol. 2024;265:28–38.\n 8. Rooney MK, Santiago G, Perni S, et al. Readability \nof patient education materials from high‑impact \nmedical journals: a 20‑year analysis. J Patient Exp. \n2021;8:2374373521998847.\n 9. Raja H, Huang X, Delsoz M, et al. Diagnosing glau‑\ncoma based on the ocular hypertension treatment \nstudy dataset using chat generative pre‑trained \n1294 Ophthalmol Ther (2025) 14:1281–1295\ntransformer as a large language model. Ophthal‑\nmol Sci. 2025;5(1):100599.\n 10. Huang X, Raja H, Madadi Y, et al. Predicting glau‑\ncoma before onset using a large language model \nchatbot. Am J Ophthalmol. 2024. https:// doi. org/  \n10. 1016/j. ajo. 2024. 06. 035.\n 11. Delsoz M, Madadi Y, Raja H, et al. Performance \nof ChatGPT in diagnosis of corneal eye diseases. \nCornea. 2024;43(5):664–70.\n 12. Delsoz M, Raja H, Madadi Y, et al. The use of \nChatGPT to assist in diagnosing glaucoma \nbased on clinical case reports. Ophthalmol Ther. \n2023;12(6):3121–32.\n 13. Madadi Y, Delsoz M, Lao PA, et  al. ChatGPT \nassisting diagnosis of neuro‑ophthalmology dis‑\neases based on case reports. J Neuroophthalmol. \n2022;128:1356.\n 14. Madadi Y, Delsoz M, Khouri AS, Boland M, Grzy‑\nbowski A, Yousefi S. Applications of artificial intel‑\nligence‑enabled robots and chatbots in ophthal‑\nmology: recent advances and future trends. Curr \nOpin Ophthalmol. 2024;35(3):238–43.\n 15. Bellanda VC, Santos MLD, Ferraz DA, Jorge R, Melo \nGB. Applications of ChatGPT in the diagnosis, \nmanagement, education, and research of retinal \ndiseases: a scoping review. Int J Retina Vitreous. \n2024;10(1):79.\n 16. Weiss BD. Health literacy. Am Med Assoc. \n2003;253:358.\n 17. Readability Scoring System. Readability formulas. \nhttps:// www. reada bilit yform ulas. com. Accessed 7 \nMar 2024.\n 18. Delsoz M, Raja H, Madadi Y, et al. A response to: \nletter to the editor regarding “The use of ChatGPT \nto assist in diagnosing glaucoma based on clinical \ncase reports.” Ophthalmol Ther. 2024;13(6):1817–\n9. https:// doi. org/ 10. 1007/ s40123‑ 024‑ 00937‑8.\n 19. OpenAI. Introducing ChatGPT. OpenAI. 2022.\n 20. Insights C. The value of Google result positioning. \nWestborough: Chitika; 2013. p. 0–10.\n 21. Morahan‑ Martin JM. How internet users find, \nevaluate, and use online health information: \na cross‑cultural review. CyberPsychol Behav. \n2004;7(5):497–510. https:// doi. org/ 10. 1089/ cpb. \n2004.7. 497.\n 22. Readability Scoring System. Readability formulas. \nhttps:// www. reada bilit yform ulas. com/.\n 23. Martin CA, Khan S, Lee R, et al. Readability and \nsuitability of online patient education mate‑\nrials for glaucoma. Ophthalmol Glaucoma. \n2022;5(5):525–30.\n 24. Decker H, Trang K, Ramirez J, et al. Large lan‑\nguage model‑based chatbot vs surgeon‑gen‑\nerated informed consent documentation \nfor common procedures. JAMA Netw Open. \n2023;6(10):e2336997.\n 25. Crabtree L, Lee E. Assessment of the readability \nand quality of online patient education materials \nfor the medical treatment of open‑angle glaucoma. \nBMJ Open Ophthalmol. 2022;7(1):e000966.\n 26. Charnock D, Shepperd S, Needham G, Gann R. \nDISCERN: an instrument for judging the quality \nof written consumer health information on treat‑\nment choices. J Epidemiol Community Health. \n1999;53(2):105–11.\n 27. Pan A, Musheyev D, Bockelman D, Loeb S, Kabar‑\nriti AE. Assessment of artificial intelligence chatbot \nresponses to top searched queries about cancer. \nJAMA Oncol. 2023;9(10):1437–40.\n 28. San Giorgi MRM, de Groot OSD, Dikkers FG. Qual‑\nity and readability assessment of websites related \nto recurrent respiratory papillomatosis. Laryngo‑\nscope. 2017;127(10):2293–7. https:// doi. org/ 10.  \n1002/ lary. 26521.\n 29. Shoemaker SJ, Wolf MS, Brach C. The patient edu‑\ncation materials assessment tool (PEMAT) and \nuser’s guide. Rockville: Agency for Healthcare \nResearch and Quality; 2020.\n 30. Shoemaker SJ, Wolf MS, Brach C. Development \nof the Patient Education Materials Assessment \nTool (PEMAT): a new measure of understand ‑\nability and actionability for print and audio‑\nvisual patient information. Patient Educ Couns. \n2014;96(3):395–403.\n 31. Veeramani A, Johnson AR, Lee BT, Dowlatshahi \nAS. Readability, understandability, usability, \nand cultural sensitivity of online patient edu‑\ncational materials (PEMS) for lower extremity \nreconstruction: a cross‑sectional study. Plast Surg. \n2024;32(3):452–9.\n 32. Loeb S, Sengupta S, Butaney M, et al. Dissemi‑\nnation of misinformative and biased informa‑\ntion about prostate cancer on YouTube. Eur Urol. \n2019;75(4):564–7.\n 33. Edmunds MR, Barry RJ, Denniston AK. Readability \nassessment of online ophthalmic patient informa‑\ntion. JAMA Ophthalmol. 2013;131(12):1610–6. \nhttps:// doi. org/ 10. 1001/ jamao phtha lmol. 2013. \n5521.\n1295Ophthalmol Ther (2025) 14:1281–1295 \n 34. Lois C, Edward L. Assessment of the readability \nand quality of online patient education materi‑\nals for the medical treatment of open‑angle glau‑\ncoma. BMJ Open Ophthalmol. 2022;7(1):e000966. \nhttps:// doi. org/ 10. 1136/ bmjop hth‑ 2021‑ 000966.\n 35. Kincaid P, Fishburne RP, Rogers RL, Chissom BS. \nDerivation of new readability formulas (automated \nreadability index, fog count and Flesch reading \nease formula) for Navy Enlisted Personnel. Mil‑\nlington: Naval Air Station Memphis; 1975.\n 36. Mc Laughlin GH. SMOG grading—a new readabil ‑\nity formula. J Read. 1969;12(8):639–46.\n 37. Antaki F, Touma S, Milad D, El‑Khoury J, Duval R. \nEvaluating the performance of ChatGPT in oph‑\nthalmology: an analysis of its successes and short ‑\ncomings. Ophthalmol Sci. 2023;3(4):100324.\n 38. Mihalache A, Popovic MM, Muni RH. Performance \nof an artificial intelligence chatbot in ophthal‑\nmic knowledge assessment. JAMA Ophthalmol. \n2023;141(6):589–97.\n 39. Lim ZW, Pushpanathan K, Yew SME, et al. Bench‑\nmarking large language models’ performances \nfor myopia care: a comparative analysis of Chat‑\nGPT‑3.5, ChatGPT‑4.0, and Google Bard. EBioMed‑\nicine. 2023;95:104770.\n 40. Masalkhi M, Ong J, Waisberg E, Lee AG. Google \nDeepMind’s Gemini AI versus ChatGPT: a \ncomparative analysis in ophthalmology. Eye. \n2024;38(8):1412–7. https:// doi. org/ 10. 1038/  \ns41433‑ 024‑ 02958‑w.\n 41. Satapathy SK, Kunam A, Rashme R, Sudarsanam \nPP, Gupta A, Kumar HK. AI‑assisted treatment \nplanning for dental implant placement: clini‑\ncal vs AI‑generated plans. J Pharm Bioallied Sci. \n2024;16(Suppl 1):S939.\n 42. Hua H‑U, Kaakour A‑H, Rachitskaya A, Srivastava \nS, Sharma S, Mammo DA. Evaluation and com‑\nparison of ophthalmic scientific abstracts and ref‑\nerences by current artificial intelligence chatbots. \nJAMA Ophthalmol. 2023;141(9):819–24. https://  \ndoi. org/ 10. 1001/ jamao phtha lmol. 2023. 3119.\n 43. Brender TD. Medicine in the era of artificial intelli ‑\ngence: hey chatbot, write me an H&P. JAMA Intern \nMed. 2023;183(6):507–8. https:// doi.  org/ 10. 1001/ \njamai ntern med. 2023. 1832.\n 44. Doak LG, Doak CC, Meade CD. Strategies to \nimprove cancer education materials. Oncol Nurs \nForum. 1996;23:1305–12.\n 45. Mondal H, Mondal S, Podder I. Using ChatGPT for \nwriting articles for patients’ education for derma‑\ntological diseases: a pilot study. Indian Dermatol \nOnline J. 2023;14(4):482–6.\n 46. Kianian R, Sun D, Crowell EL, Tsui E. The use \nof large language models to generate education \nmaterials about uveitis. Ophthalmol Retina. \n2024;8(2):195–201.\n 47. Brin D, Sorin V , Vaid A, et al. Comparing ChatGPT \nand GPT‑4 performance in USMLE soft skill assess ‑\nments. Sci Rep. 2023;13(1):16492.\n 48. DeWalt DA, Berkman ND, Sheridan S, Lohr KN, \nPignone MP. Literacy and health outcomes: a sys‑\ntematic review of the literature. J Gen Intern Med. \n2004;19:1228–39.\n 49. Schillinger D. Social determinants, health lit‑\neracy, and disparities: intersections and con‑\ntroversies. HLRP Health Literacy Res Pract. \n2021;5(3):e234–43.",
  "topic": "Optometry",
  "concepts": [
    {
      "name": "Optometry",
      "score": 0.373928964138031
    },
    {
      "name": "Medicine",
      "score": 0.3139018416404724
    }
  ]
}