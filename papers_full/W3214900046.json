{
  "title": "A Unified Pruning Framework for Vision Transformers",
  "url": "https://openalex.org/W3214900046",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2111034494",
      "name": "Yu Hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114996999",
      "name": "Wu, Jianxin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963351448",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3034340181",
    "https://openalex.org/W2138011018",
    "https://openalex.org/W1977295328",
    "https://openalex.org/W2051840895",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2964233199",
    "https://openalex.org/W3188427387",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2928560789",
    "https://openalex.org/W2963399829",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W1797268635",
    "https://openalex.org/W2962965870",
    "https://openalex.org/W3172801447",
    "https://openalex.org/W2047643928",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3125056032",
    "https://openalex.org/W3202406646",
    "https://openalex.org/W3205764742",
    "https://openalex.org/W3178702014",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W3102446692",
    "https://openalex.org/W3169793979",
    "https://openalex.org/W2031489346",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W1846799578",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3035682985",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W2152161678",
    "https://openalex.org/W3203606893",
    "https://openalex.org/W2996159613",
    "https://openalex.org/W3204575409",
    "https://openalex.org/W2797977484",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W3205814820",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2963363373",
    "https://openalex.org/W3034674511",
    "https://openalex.org/W3175515048"
  ],
  "abstract": "Recently, vision transformer (ViT) and its variants have achieved promising performances in various computer vision tasks. Yet the high computational costs and training data requirements of ViTs limit their application in resource-constrained settings. Model compression is an effective method to speed up deep learning models, but the research of compressing ViTs has been less explored. Many previous works concentrate on reducing the number of tokens. However, this line of attack breaks down the spatial structure of ViTs and is hard to be generalized into downstream tasks. In this paper, we design a unified framework for structural pruning of both ViTs and its variants, namely UP-ViTs. Our method focuses on pruning all ViTs components while maintaining the consistency of the model structure. Abundant experimental results show that our method can achieve high accuracy on compressed ViTs and variants, e.g., UP-DeiT-T achieves 75.79% accuracy on ImageNet, which outperforms the vanilla DeiT-T by 3.59% with the same computational cost. UP-PVTv2-B0 improves the accuracy of PVTv2-B0 by 4.83% for ImageNet classification. Meanwhile, UP-ViTs maintains the consistency of the token representation and gains consistent improvements on object detection tasks.",
  "full_text": "A Uniﬁed Pruning Framework for Vision Transformers\nHao Yu Jianxin Wu *\nState Key Laboratory for Novel Software Technology\nNanjing University, Nanjing, China\nyuh@lamda.nju.edu.cn, wujx2001@gmail.com\nAbstract\nRecently, vision transformer (ViT) and its variants have\nachieved promising performances in various computer vi-\nsion tasks. Yet the high computational costs and train-\ning data requirements of ViTs limit their application in\nresource-constrained settings. Model compression is an ef-\nfective method to speed up deep learning models, but the\nresearch of compressing ViTs has been less explored. Many\nprevious works concentrate on reducing the number of to-\nkens. However, this line of attack breaks down the spatial\nstructure of ViTs and is hard to be generalized into down-\nstream tasks. In this paper, we design a uniﬁed framework\nfor structural pruning of both ViTs and its variants, namely\nUP-ViTs. Our method focuses on pruning all ViTs compo-\nnents while maintaining the consistency of the model struc-\nture. Abundant experimental results show that our method\ncan achieve high accuracy on compressed ViTs and vari-\nants, e.g., UP-DeiT-T achieves 75.79% accuracy on Ima-\ngeNet, which outperforms the vanilla DeiT-T by 3.59% with\nthe same computational cost. UP-PVTv2-B0 improves the\naccuracy of PVTv2-B0 by 4.83% for ImageNet classiﬁca-\ntion. Meanwhile, UP-ViTs maintains the consistency of the\ntoken representation and gains consistent improvements on\nobject detection tasks.\n1. Introduction\nThe transformer [35] architecture has been widely used\nfor natural language processing (NLP) tasks over the past\nyears. Inspired by its excellent performance in NLP, ViT [7]\nemploys a pure transformer structure to image classiﬁcation\ntasks and achieves remarkable accuracy. Following the suc-\ncesses of ViT, transformer-based models have established\nmany new state-of-the-art records in various computer vi-\nsion (CV) tasks, such as image classiﬁcation [33], object\ndetection [2] and video segmentation [39]. In spite of these\nadvances, most of these transformer-based structures suffer\n*J. Wu is the corresponding author.\nfrom large model size, huge run-time memory consump-\ntion and high computational costs. Therefore, there exists\nimpending needs to develop and deploy lightweight and ef-\nﬁcient vision transformers.\nNetwork pruning is a useful technique to strike a good\nbalance between model accuracy and inference speed and\nmemory usage. Although numerous effective pruning al-\ngorithms have been proposed to compress and accelerate\nconvolutional neural networks (CNNs) in CV tasks, how\nto prune vision transformers is a much less explored topic.\nSome previous NLP research efforts [10, 18] focus on ex-\nploring new multi-head self-attention (MHSA) mechanisms\nor knowledge distillation algorithms. However, the most\ntime-consuming module in a transformer is in fact the feed-\nforward network (FFN) [29], but efforts in pruning FFN\nremain scarce. Recent ViT pruning methods [29, 32] are\nmainly attracted to recursively sampling informative tokens\n(or equivalently, their corresponding image patches) to in-\ncrease the inference speed in image classiﬁcation, which\nachieves similar accuracy as that of using all tokens with\nfewer computations. Unfortunately, these token sampling\nmethods often impede the vision transformers’ generaliza-\ntion ability on downstream tasks, e.g., object detection and\ninstance segmentation. In addition, this kind of sampling is\nalso difﬁcult to be applied to NLP tasks, which limits the\napplication domain of these methods.\nHence, we believe that in order to successfully accel-\nerate and slim vision transformers, we need a uniﬁed ap-\nproach which simultaneously prunes all components in a\ntransformer structure, does not alter the transformer struc-\nture, generalizes well to downstream tasks withhigh accura-\ncies, and applies to not only ViTs but also its manyvariants.\nTo fulﬁll these goals, we propose UP-ViTs, a uniﬁed\npruning framework for vision transformers, which prunes\nthe channels in ViTs in a uniﬁed manner, including those\nboth inside and outside of the residual connections in all\nthe blocks, MHSAs, FFNs, normalization layers, and con-\nvolution layers in ViT variants. We ﬁrst devise an efﬁcient\nevaluation module to estimate the importance score of each\nﬁlter in a pre-trained ViT model. Then based on the com-\n1\narXiv:2111.15127v1  [cs.CV]  30 Nov 2021\npression goals, all redundant channels will be simultane-\nously removed, which leads to a thinner structure. In par-\nticular, when compressing the attention layers, we investi-\ngate the inﬂuence of MHSA and propose a novel method\nfor throwing channels away. We also design an effective\nprogressive block pruning method, which removes the least\nimportant block and proposes new hybrid blocks in ViTs.\nExperiments on ImageNet show that UP-ViTs signiﬁcantly\noutperforms previous ViTs with the same or even higher\nthroughput. Our contributions are:\n• We propose a novel framework for structured compres-\nsion of ViTs and its variants. Our method can be easily\napplied to both vanilla ViTs and its variants, and the re-\nsulting compressed models achieve higher accuracy than\nprevious state-of-the-art ViTs and existing pruning algo-\nrithms.\n• Our method maintains the consistency of the token rep-\nresentation, therefore we can generalize the compressed\nmodel to various downstream tasks. For example, we\ncompare the accuracy of our pruned UP-PVTv2 and pre-\nvious PVTv2 models in object detection, and achieve\nhigher detection accuracy.\n2. Related Works\nWe ﬁrst brieﬂy review the related works.\n2.1. Vision transformers\nTransformer [35] utilizes the MHSA mechanism to build\nlong-range dependencies between pairs of input tokens for\nNLP tasks, and recently it has been introduced into many\ncomputer vision tasks. ViT ﬁrst [7] shows that a standard\ntransformer can achieve state-of-the-art accuracies in image\nclassiﬁcation tasks when the training data is sufﬁcient (e.g.,\nusing JFT-300M). DeiT [33] further proposes a token-based\ndistillation method for training and explores existing data\naugmentation and regularization strategies, such as Rand-\nAugment [5] and stochastic depth [14]. With the same ar-\nchitecture as ViT, DeiT is also effective using the smaller\nImageNet-1K dataset. Since then many works attempt to\nintroduce the local dependency into vision transformers.\nT2T-ViT [42] introduces the tokens-to-token (T2T) mod-\nule to aggregate neighboring tokens, which can be general-\nized into other efﬁcient transformers like Performer [3]. For\nadapting ViTs to dense prediction tasks such as object de-\ntection, Wang et al. proposed Pyramid Vision Transformer\n(PVT) [38], which introduces convolution and the pyramid\nhierarchical structure to the design of transformer back-\nbones. They also updated their model later as PVTv2 [37],\nwhich achieves higher accuracy with less computation bur-\nden. Swin Transformer [24] proposes a shifted windowing\nscheme to compute representation. LV-ViT [16] proposes a\ntoken labeling training method to improve the performance\nof ViT. In this paper, we will show that our compression\nmethod can deal with both the original ViTs and its vari-\nants.\n2.2. Compressing transformers and sparse tokens\nTraining ViTs is time-consuming and the trained mod-\nels are often massive, which necessitates compression algo-\nrithms. Starting in NLP applications, LayerDrop [9] ran-\ndomly drops layers at training time, and at test time it se-\nlects sub-network to any desired depth. Behnke et al. [1] it-\nerated between pruning attention heads and ﬁne-tuning. Li\net al. [21] showed that the most compute-efﬁcient training\nscheme is to stop training after a small number of iterations,\nand then heavily compress them. Another related ﬁeld is to\nreduce the complexity of the MHSA module via various ap-\nproximations [10, 18].\nLately, some pruning algorithms have been proposed for\nViTs. VTP [46] reduces embedding dimensionality by in-\ntroducing control coefﬁcients. Moreover, sparse training of\nViTs seeks to adaptively identify highly informative sparse\ntokens. Tang et al. [32] designed a patch slimming method\nto discard useless tokens. Evo-ViT [40] updates the se-\nlected informative and uninformative tokens with different\ncomputation paths. However, these token sampling meth-\nods are unstructured and are difﬁcult to use in downstream\ntasks (e.g., object detection). Jia et al. [15] explored a ﬁne-\ngrained manifold distillation approach that calculates train-\ning loss from three perspectives. NViT [41] observes the di-\nmension trend of every component in ViTs. Our method is\nmore general and may be potentially combined with them.\n2.3. Pruning CNN channels\nChannel pruning methods remove less important chan-\nnels in CNNs, and then the pruned sub-model can be ﬁne-\ntuned to recover its accuracy. Hence it is important to dis-\ntinguish important and redundant channels. Li et al. [20]\npruned ﬁlters using their ℓ1-norm values. He et al. [13] pro-\nposed a LASSO based channel selection method. Luo et\nal. [27] pruned ﬁlters based on statistics computed from its\nnext layer. After that, they further pruned the residual con-\nnection to get a wallet-shape model, which is a better model\nstructure when the shortcut connection exists [26]. If some\nﬁlters in one layer are pruned, the output features of this\nlayer and the input of the next layer should be changed cor-\nrespondingly. He et al. [12] pruned channels based on the\ngeometric median. In this work, we apply KL-divergence\nto evaluate the importance of ﬁlters in vision transformers,\nwhich has been proved to be effective in CNN [26].\n3. The UP-ViTs Method\nWe will now propose our UP-ViTs. First we review the\nstandard ViT brieﬂy in Section 3.1, then our method to eval-\nuate the importance of each ﬁlter in Section 3.2. We elab-\norate on the details of pruning into a sub-model and ﬁne-\n2\ntuning it in Section 3.3. Finally, in Section 3.4 we further\npropose an efﬁcient algorithm to prune blocks in ViTs.\n3.1. Architecture of ViT\nThe standard transformer receives an input as a 1D se-\nquence of token embeddings. Following this design, ViT\nreshapes an image into ﬂattened sequential patches and lin-\nearly projects each patch into an embedding vector. It also\nconcatenates the patch embeddings with a trainable classiﬁ-\ncation token and adds a position embedding. Consider a ViT\nmodel with Lblocks. Given an image x ∈RC×H×W , ViT\nﬁrst reshapes it into ﬂattened 2D patches xp ∈RN×(CP 2),\nwhere H ×W is the original input resolution, C is num-\nber of input channels, P×P is the resolution of each patch,\nN = HW/P2 is the number of patches. Then, the sequence\nof embeddings x0 is formed by\nxpatch = FCpatch(xp) , (1)\nx0 = [xcls |xpatch] +xpos , (2)\nwhere FCpatch is the linear projection with Doutput chan-\nnels. xcls ∈R1×D and xpatch ∈R(N+1)×D are the learn-\nable classiﬁcation token and position embeddings, respec-\ntively, [·|·] is the column-wise concatenation.\nThe embedding x0 is subsequently fed into the trans-\nformer blocks. ViT stacks several blocks to construct the\nwhole model, and each consists of two parts: the attention\nlayer and FFN. In the attention layer, ViT applies Layer-\nNorm and three linear projections ( FCq, FCk and FCv) in\nparallel to generate queries Q, keys K and values V, i.e.,\nx′\nk−1 = LN(xk−1) , (3)\nQ = FCq(x′\nk−1) , (4)\nK = FCk(x′\nk−1) , (5)\nV = FCv(x′\nk−1) , (6)\nwhere LN is layer normalization, xk−1 is the input embed-\nding for k = 1,...,L . Then, a multi-head self-attention\n(MHSA) and a linear projection FCproj are performed:\nx′′\nk−1 = xk−1 + FCproj(MHSA(Q,K,V)), (7)\nwhere x′′\nk−1 is the output of the attention layer in the kth\nblock. The input and output dimensions of these linear pro-\njections are all D. Note that the MHSA module does not\ncontain any parameters. It reshapes the queries, keys and\nvalues and applies scaled dot-product attention (cf. [7] for\ndetails). Then, x′′\nk−1 will be sent into FFN, which contains\ntwo FC layers (FC1 and FC2) with a GELU non-linearity:\nxk = x′′\nk−1 + MLP(LN(x′′\nk−1)) , (8)\nMLP(x) = FC2 (GELU(FC1(x))) . (9)\nAs a whole, there are 6 learnable linear projections and\n2 LayerNorm modules in each ViT block. It is worth not-\ning that ViT only extracts the classiﬁcation token in the last\nblock as the ﬁnal representation, which is a different design\nfrom the practice in CNNs (i.e., global average pooling of\nall patch embeddings).\n3.2. Calculating the importance scores\nSince our goal is to compress ViT and also to preserve\nthe consistency of token representations, structured channel\npruning stands out as a reasonable choice. Firstly, we focus\non calculating the importance scores of all channels (i.e.,\nthe D dimensions). Our goal is to minimize the informa-\ntion loss of the last layer after pruning channels. Generally\nspeaking, we divide ViTs into several uncorrelated compo-\nnents and evaluate the performance change after removing\neach channel in every component.\nLet us take one ViT block as an example. As shown in\nFigure 1, we divide the block into three irrelevant structural\ncomponents, namely:\n• Component 1: The shortcut connections that chain rep-\nresentations across all blocks, i.e., the input channels of\nFCq, FCk, FCv and FC1, the output channels of FCproj\nand FC2, and the two LN layers;\n• Component 2: The attention embedding ﬁlters inside the\nattention layer in every block, i.e., the input channels of\nFCproj and the output channels of FCq, FCk and FCv;\n• Component 3: The FFN inter-layer ﬁlters in every block,\ni.e., the input channels of FC2 and the output channels of\nFC1.\nIn particular, one crucial difﬁculty arises out of this\nstructure. Because of the existence of shortcut connections,\nthe following numbers must be the same in the entire net-\nwork after pruning: the number of channels in all compo-\nnents linked by the shortcut connections, the patch embed-\nding dimensionality, and the ﬁnal FC layer dimensionality.\nTo make sure the validity of all summation and concatena-\ntion operations, the dimensionality of the classiﬁcation to-\nken and the position embedding must be the same as this\nnumber, too.\nTo measure the channel importance, we randomly select\n2000 images from the training dataset to establish a proxy\ndataset D. We will extract the output logits on Dand eval-\nuate the performance change before/after removing a spe-\nciﬁc channel, e.g., when calculating the kth channel’s im-\nportance score of the component 3, we will remove the kth\ninput channel of FC2 and the kth output channel of FC1.\nInspired by CURL [26], the score is calculated by the KL-\nDivergence between two models with and without this par-\nticular channel, i.e.:\ns=\n∑\ni∈D\nDKL(qi||pi), (10)\n3\n/g39/g16/g71/g76/g80\n/g47/g49\n/g48/g43/g54/g36\n/g39/g16/g71/g76/g80\n/g36/g87/g87/g72/g81/g87/g76/g82/g81/g3/g47/g68/g92/g72/g85\n/g42/g40/g47/g56\n/g41/g72/g72/g71/g16/g41/g82/g85/g90/g68/g85/g71/g3\n/g49/g72/g87/g90/g82/g85/g78\n/g41/g38/g84/g29/g3/g3/g3/g3/g3/g3/g3/g3/g3/g3/g1830/g3400/g1830 /g41/g38/g89/g29/g1830/g3400/g1830\n/g41/g38/g83/g85/g82/g77/g29/g1830/g3400/g1830\n/g41/g38/g78/g29/g1830/g3400/g1830\n/g41/g38/g20/g29/g1829/g3400/g1830\n/g47/g49\n/g41/g38/g21/g29/g1830/g3400/g1829\nFigure 1. Illustration of three uncorrelated components in a ViT\nblock. Our method prunes not only the channels inside the at-\ntention layer (the numbers shown in gold color) and the FFN (the\nnumbers shown in blue), but also the channels across shortcut con-\nnections (the numbers shown in red). The two numbers in each\nrectangle represent the values of output and input channels of the\nlinear projections, respectively.\nwhere i enumerates a sample from D, pi is the output of\nmodel without this particular channel, andqi is the output of\nthe original model. The larger the score s, the more impor-\ntant this channel is. Note that because MHSA contains a re-\nshaping operation, when calculating the importance scores\nof the component 2, we will mask the target channel as 0 in-\nstead of removing it, otherwise the reshaping operation will\nbecome invalid.\nOur method can evaluate the impact of all ﬁlters, and\nafter generating all importance scores, we can produce more\nthan one sub-network from a well-trained large model. The\nset of channels to be removed depend on these scores and\ndifferent pre-set compression goals, which is ﬂexible.\n3.3. Pruning and ﬁne-tuning\nGiven the original model and every channel’s importance\nscore, we ﬁrst generate the sub-model candidate. In each\ncomponent and each block, based on the pre-set compres-\nsion ratio, we independently rank the importance scores and\ndelete those less important ones. Note that for simplicity,\nwe use the same compression ratio in all components and\nall blocks.\nIn particular, the attention layer in ViT is beneﬁted from\nthe multi-head attention mechanism, which captures richer\ninformation by using multiple different heads. However, it\nalso brings difﬁculties to compress the component 2. There-\nfore, we speciﬁcally design a simple but effective method to\nprune the multi-heads. In detail, given a Db dimensional at-\ntention layer with hb heads, we need to prune it into Dt di-\nmensions with ht heads. Note that every head contains the\nsame number of dimensions across all attention layers and\nhb is divisible by ht. The two settings are common in the\ntransformer-based models. When pruning multi-head atten-\ntion, we ﬁrst merge every hb/ht heads into 1 head, hence\nthe attention layers will each have ht heads with Db di-\nmensions. Then, we remove Db−Dt\nht\ndimensions from every\nhead, and the remaining module is the attention layer we\nwant.\nLastly, we ﬁne-tune the sub-model with all training sam-\nples, and use the original model as the teacher to distill the\nsub-model. Different from the training strategy of DeiT, we\nuse the classic soft distillation to measure the training loss:\nL= LCE(y,p) +αLKL(q,p) , (11)\nwhere pis the output probability (after softmax) of the sub-\nmodel, q is the output probability of the teacher (i.e., the\noriginal model), and y is the true label. LCE and LKL de-\nnote the cross-entropy loss and the KL-divergence loss, re-\nspectively. Later, we will show that it is a better loss crite-\nrion than hard distillation when compressing ViTs, which is\nthe opposite of the observation from DeiT.\n3.4. Pruning blocks\nFinally, we propose a new way to prune blocks in\ntransformer-based models. More precisely, when pruning\nwidth, we ﬁrst calculate all channels’ important scores and\nthen prune all redundant channels at once. Different from\nthis pipeline, we construct a progressive method to com-\npress blocks.\n• First, for each block, calculate the KL-Divergence with\n& without it;\n• Second, remove the least important block and reserve the\npruned model;\n• Third, if the pruned model has not reached the com-\npression target, then use the ﬁrst two steps to prune one\nmore block; otherwise we obtain a sub-model with fewer\nblocks and ﬁne-tune it.\nNote that the output of one block is its next block’s input,\nso once we remove a block, the importance of every remain-\ning block will change, and blocks adjacent to the deleted\n4\nTable 1. Summary of small-scale datasets.\nDataset Train Validation Categories\nCIFAR-100 [19] 50000 10000 100\nCUB-200 [36] 5994 5794 200\nCars [17] 8144 8041 196\nAircraft [28] 6667 3333 100\nIndoor67 [31] 5360 1340 67\nPets [30] 3680 3669 37\nDTD [4] 3760 1880 47\niNaturalist-2019 [34] 265213 3030 1010\nblock will become more important. Therefore, we prune\none block at a time. Later we will show this progressive\nblock pruning method is better than pruning many blocks at\nonce.\nUnlike CNNs, given a ViT model with Lblocks, if we\nremove the FFN of thekth (k∈{1,2,...,L −1}) block and\nthe attention layer of the next block, the remaining model is\nstill guaranteed to be a legal ViT with L−1 blocks. There-\nfore, when compressing blocks, we actually compute2L−1\ninstead of Limportance scores. The L−1 new hybrid block\npruning candidates we propose here are novel and useful.\n4. Experiments\nWe now evaluate the performance of our UP-ViTs. We\nﬁrst prune DeiT-B and PVTv2-B2, and test the effectiveness\non ImageNet-1k. In addition, more results on downstream\nsmall-scale classiﬁcation and object detection datasets will\nbe presented. We will show that UP-ViTs achieve better or\ncomparable accuracies on these tasks. Finally, we will end\nthis section with several analyses. All the experiments are\nconducted with PyTorch.\n4.1. Datasets and metrics\nClassiﬁcation. The ImageNet-1k [6] dataset consists of\n1.28 million training and 50000 validation images. Those\nimages have various spatial resolutions and come from 1000\ndifferent categories. ImageNet-1k is usually used as the\nbenchmark for model pruning.\nBesides ImageNet-1k, we also evaluate UP-ViTs on\ndownstream small-scale datasets. Table 1 summarizes the\ninformation of these datasets, including training and valida-\ntion sizes, and the number of categories.\nObjection Detection. We evaluate object detection\nperformance on the MS COCO2017 [23] and the Pascal\nVOC07+12 [8] datasets. MS COCO 2017 contains 80 cat-\negories with 118K training and 5K validation images, re-\nspectively. Pascal VOC07+12 has 20 classes. Speciﬁcally,\nVOC07 contains a train-val set of 5011 images and a test\nset of 4952 images, and VOC12 contains a train-val set of\n11540 images. We use mean Average Precision (mAP) to\nmeasure the detection accuracy.\n4.2. Pruning DeiT-B on ImageNet-1k\nWe ﬁrst prune DeiT-B on ImageNet-1k to DeiT-S & T,\nand to further show the validity of our new block pruning\nstrategy, we also prune blocks on the basis of the pruned\nDeiT-T.\nImplementation details. First we prune DeiT-B into\nUP-DeiT-S & T. Note that our compressed UP-DeiT-S & T\nshare the same structure as the original DeiT-S (for small) &\nDeiT-T (for tiny). After all channel scores were calculated,\nwe removed all redundant channels at once, then ﬁne-tuned\nit with knowledge distillation in 200 epochs. We initial-\nized the learning rate as 3e-4 and set α in Equation 11 as\n0.5 when ﬁne-tuning UP-DeiT-S. During ﬁne-tuning UP-\nDeiT-T, the learning rate and αwere 1e-3 and 0.2, respec-\ntively. After obtaining the compressed UP-DeiT-T model,\nwe continue to delete 2 more blocks and ﬁne-tune the sub-\nmodel. During ﬁne-tuning, we set the learning rate as 3e-4\nand trained 50 epochs with knowledge distillation with α\nbeing 0.4.\nIn the above experiments, we used the AdamW [25] opti-\nmizer and the cosine decay schedule. The weight decay was\n1e-3 and the mini-batch size was 256. Random cropping,\nrandom horizontal ﬂipping, color jittering and CutMix [43]\nwere applied as data augmentations. The mixing ratio of\nCutMix was 0.5. In particular, unlike DeiT, we did not use\nmixup [44], random erasing [45] or Rand-Augment [5] in\nany of our experiments.\nResults. Table 2 shows the pruning results of DeiT-B.\nWe tested model accuracy on the ImageNet-1k validation\ndataset. During testing, the shorter side was resized as 256\nby bilinear interpolation and then we cropped the 224×224\nimage patch in the center. The accuracy of the last epoch\nwas reported. We also list the throughput in a TITAN Xp\nGPU with a ﬁxed 32 mini-batch size.\nPerformance comparison between DeiT and UP-DeiT\nproves the effectiveness of our framework. Besides this,\nUP-DeiT also consistently outperforms previous state-of-\nthe-art ViT variants, such as T2T-ViT, Swin Transformer\nand PVT. And compared with DeiT-T, the UP-DeiT-T-10\nachieves 1.58% higher accuracy with only 10 blocks. Fur-\nthermore, UP-DeiT-S achieves 81.53% accuracy with 3.03x\n(603.1 / 199.2) acceleration. This is better than Evo-ViT\n(state-of-the-art token-slimming method), which achieves\n81.11% accuracy with 1.53x acceleration.\nTransferring to small datasets.We also tested the com-\npressed models’ transferring ability. We adopted mini-batch\nsize 256 and learning rate 1e-3 when ﬁne-tuning the DeiT-T\nmodel, and for the compressed models the learning rate was\n3e-3. During ﬁne-tuning, we applied the AdamW optimizer\nand the CutMix augmentation strategy.\nTable 3 shows the results. The pruned model UP-DeiT-\n5\nTable 2. Results of pruning DeiT-B in ImageNet-1k. UP-DeiT-S &\nT and UP-DeiT-T-10 are our pruned model with 12 and 10 blocks\nrespectively. Note that we denote the method proposed by [15]\nas MD-DeiT (manifold distillation DeiT). ⋆ methods denote the\nresults copied from the original paper.\nModel Throughput #Param. Top-1 Acc.\nDeiT-B [33] 199.2 86.6M 81.84%\nDeiT-S [33] 603.1 22.1M 79.85%\nT2T-ViT-14 [42] 456.6 21.5M 81.38%\nSwin-T [24] 384.5 28.5M 81.17%\nPVT-S [38] 389.9 24.5M 79.79%\nVTP⋆ [46] - 48.0M 80.70%\nNViT-S⋆ [41] - 23.0M 81.22%\nEvo-ViT [40] 305.7 87.3M 81.11%\nMD-DeiT-S⋆ [15] 603.1 22.1M 81.48%\nUP-DeiT-S 603.1 22.1M 81.56%\nDeiT-T [33] 1408.5 5.7M 72.20%\nT2T-ViT-10 [42] 857.9 5.9M 75.00%\nPVT-T [38] 691.7 13.2M 75.00%\nNViT-T⋆ [41] - 6.4M 73.91%\nMD-DeiT-T⋆ [15] 1408.5 5.7M 75.06%\nUP-DeiT-T 1408.5 5.7M 75.79%\nUP-DeiT-T-10 1674.2 4.8M 73.97%\nTable 3. Accuracy (%) on different small-scale classiﬁcation\ndatasets. We train models with 50 epochs.\nDataset DeiT-T UP-DeiT-T UP-DeiT-T-10\nCIFAR-100 85.01 86.52 85.98\nCUB-200 76.75 81.34 80.93\nCars 85.24 87.34 86.27\nAircraft 72.58 75.08 75.81\nIndoor67 74.70 78.21 77.99\nPets 88.93 92.20 91.69\nDTD 71.28 73.83 72.50\niNaturalist-2019 69.54 70.86 69.17\nT always outperforms DeiT-T on all 8 datasets, which in-\ndicates that our UP-ViTs method boosts the generalization\nof DeiT signiﬁcantly. Also, except on the iNaturalist-2019\ndataset, the 10-block UP-DeiT-T-10 outperforms the 12-\nblock DeiT-T on other datasets.\n4.3. Pruning PVTv2 on ImageNet-1k\nIn this section, we prune PVTv2 on ImageNet-1k. Our\ngoal is to demonstrate that our method applies not only to\npure transformer structures, but also to those variants that\ncombine MHSA and convolution layers. We adopted two\ntasks to show our method’s effectiveness, i.e., only prune\ndepth, and prune both depth and width.\nImplementation details. We conducted two experi-\nments for comparison. In the ﬁrst one, we deleted blocks\nand pruned PVTv2-B2 into UP-PVTv2-B1; in the second,\nTable 4. Results of pruning PVTv2 on ImageNet-1k. We report\nthe accuracy of the last epoch on ImageNet-1k validation. UP-\nPVTv2-B1 & B0 are our compressed models.\nMethod Throughput #Param. Top-1 Acc.\nPVTv2-B2 238.0 25.36M 82.08%\nPVTv2-B1 407.2 14.01M 78.62%\nUP-PVTv2-B1 79.48%\nPVTv2-B0 739.1 3.67M 70.47%\nUP-PVTv2-B0 75.30%\nTable 5. mAP of UP-PVTv2 on Pascal VOC07 test. We trained\nRetinaNet on the Pascal VOC07+12 train-val dataset.\nBackbone #Param. FLOPs mAP\nPVTv2-B1 22.5M 108.8G 81.9\nUP-PVTv2-B1 83.0\nPVTv2-B0 11.7M 85.0G 80.5\nUP-PVTv2-B0 82.0\nwe directly pruned PVTv2-B2 into UP-PVTv2-B0. Note\nthat the structures of UP-PVTv2-B1 & B0 are exactly the\nsame as those of PVTv2-B1 & B0, respectively. It is worth\nnoting that PVTv2 introduces a convolutional layer into the\nattention module to reduce the computational cost, so we\nadded one extra component when calculating the impor-\ntance scores.\nDuring training, we followed the data augmentation and\nlearning rate setting of pruning DeiT-B. In particular, when\npruning PVTv2-B2 to UP-PVTv2-B0, we used a learning\nrate of 1e-3 and an αof 0.2. The sub-model was ﬁne-tuned\nfor 100 epochs. When we pruned PVTv2-B2 to UP-PVTv2-\nB1, the learning rate were initialized as 3e-4 and we trained\n50 epochs. Especially, we found that directly distilling the\nfeatures in the penultimate layer (after the global average\npooling and before the ﬁnal classiﬁer) is advantageous, so\nwe applied the MSE loss instead of the KL-divergence loss\nas the distillation loss. The αwas set to 20.\nResults. Table 4 shows the results on PVTv2, and sim-\nilar to the previous experiments, our UP-PVTv2 models\nobtain signiﬁcantly better results when compared with the\noriginal models.\nTransferring to object detection. To further validate\nthe effectiveness of our method on a larger object detection\ndataset, we investigate UP-PVTv2’s performances on object\ndetection with Mask R-CNN [11] and RetinaNet [22]. First,\nwe adopted both PVTv2-B0/B1 and our UP-PVTv2-B0/B1\nas backbones of RetinaNet on Pascal VOC07+12.\nAs reported in Table 5, the compressed UP-PVTv2 mod-\nels signiﬁcantly outperform original models on Pascal VOC\ndetection. For example, UP-PVTv2-B0 achieves 82.0 mAP,\nwhich surpasses the original PVTv2-B0 model by 1.5 AP\npoints, and is on par with the performance of the original\n6\nPVTv2-B1.\nWe also investigated the performance of (UP-)PVTv2-\nB1 backbone on both one-stage and two-stage object detec-\ntors, Mask R-CNN and RetinaNet. From the results shown\nin Table 6 on the larger MS-COCO dataset, our method sig-\nniﬁcantly outperforms original PVTv2-B1.\n4.4. Analyses\nTo explore the impact of different modules of our\nmethod, we performed three analyses in this section.\n4.4.1 Pruning multi-head self-attention\nWe ﬁrst evaluate the inﬂuence of our attention pruning\nstrategy when pruning component 2 by conducting an ex-\nperiment of pruning DeiT-B into UP-DeiT-T. In particular,\nDeiT-B and UP-DeiT-T are similar in structure, but DeiT-B\ncontains 768 dimensions with 12 heads in its attention layer,\nand UP-DeiT-T has 192 dimensions with only 3 heads. Af-\nter calculating the attention importance score of DeiT-B, we\ncompare three strategies of pruning the component 2:\n• Strategy 1: Our strategy of pruning the attention layer in\nSection 3.3. For the 12 heads, we remove 192 dimensions\nfrom every four heads.\n• Strategy 2: Do not consider the structure difference and\ndelete 576 dimensions across all heads;\n• Strategy 3: For the 12 heads, we remove 48 dimensions\nfor each attention head;\nAfter deleting those redundant dimensions, we merged\nthe remaining ﬁlters into 3 heads and extracted three dif-\nferent UP-DeiT-T sub-models. We sampled the original\nImageNet-1k to a smaller subset with one-tenth of the to-\ntal images, which is named SImageNet-1k. Then we ﬁne-\ntuned the three sub-models 50 epochs on SImageNet-1k and\ntested on the original ImageNet-1k validation set. The other\ntraining settings were the same as those Section 4.2. We\nshow the results in Table 7, and strategy 1 achieves the high-\nest accuracy.\n4.4.2 Knowledge distillation\nWe then evaluate the inﬂuence of knowledge distillation\nwhen ﬁne-tuning UP-ViTs. For a fair comparison, we adopt\nthe same training strategies as Section 4.2 and Section 4.3.\nWe illustrate the results in Table 8, which shows our method\nstill obtains competitive accuracy when training without\nknowledge distillation, and UP-DeiT-T even has better per-\nformance without distillation. We further elongate the train-\ning process for random initialization, without distillation\nand with DeiT-B distillation for DeiT-T. Our pruned model\nare more accurate than model training from scratch with\nmore epochs, which shows that our pruning strategy is bet-\nter than directly training a light model from scratch.\nWe also compare several different distillation strategies.\nBesides soft distillation, we consider the following two\nmethods.\nHard distillation. Let yt = arg max(q) be the hard de-\ncision of the teacher, where qis the teacher’s output proba-\nbilities. The loss function of hard-label distillation is\nLCE (p,y) +αLCE (p,yt) . (12)\nSoft + Patch distillation. Inspired by LV-ViT [16], we\ndistill both the classiﬁcation token and the patch tokens.\nAssume the student’s output patch tokens are t1\ns,...,t N\ns\nand the corresponding outputs of the teacher aret1\nt ,...,t N\nt .\nNote that the number of embedding dimensions in ti\ns and\nti\nt are different, so we introduce a new linear projection\nFCtoken into the loss function. The soft + patch distillation\nobjective is:\nLCE (p,y)+ αLKL(p,q)+ β\nN∑\ni=1\nLMSE\n(\nFCtoken(ti\ns),ti\nt\n)\n.\n(13)\nBased on the three strategies, we investigate the perfor-\nmance of distilling UP-DeiT-T with DeiT-B. During ﬁne-\ntuning, we set αof hard distillation as 2.0. When applying\nsoft + patch distillation, we set α and β as 0.2 and 1e-3,\nrespectively. As the distillation results in Table 9 clearly\nshow, soft distillation achieves the highest accuracy.\n4.4.3 Progressive vs. One-time pruning\nThe last sets of experiments are about progressive vs. one-\ntime pruning. We compare two pipelines during compress-\ning DeiT-B. In the ﬁrst pipeline, after calculating all impor-\ntance scores, we remove all redundant channels at once and\nﬁne-tune the model. In the second pipeline, we divide the\nwhole pruning process into 4 steps:\n• Step 1: Calculate the importance scores of component 1\nin DeiT-B. There are 768 channels and we throw away\n576 of them and ﬁne-tune the sub-model.\n• Step 2: Continue to calculate the importance scores of\ncomponent 2 per block and prune it. In particular, we\nthrow away 48 dimensions for every attention head and\nkeep 12 attention heads, then ﬁne-tune the sub-model.\n• Step 3: Similar to the second step, we prune component 3\nand ﬁne-tune the model. Especially, the dimensionalities\nof the FFN hidden layer are reduced from 3072 to 768.\n• Step 4: Adjust the attention heads from 12 to 3 and ﬁne-\ntune the model.\nNote that during ﬁne-tuning, we apply knowledge distil-\nlation and the teacher model is always DeiT-B. The results\nare shown in Table 10. We can observe that the performance\nof progressive pruning (74.51%) is not as good as pruning\nat once (75.79%).\n7\nTable 6. mAP of (UP-)PVTv2-B1 on the MS COCO2017 validation dataset.\nBackbone Methods #Param. FLOPs AP AP 50 AP75 APS APM APL\nPVTv2-B1 RetinaNet 1x 23.75M 235.9G 40.5 61.0 43.3 24.0 43.9 53.1\nUP-PVTv2-B1 41.1 62.0 43.5 25.1 44.2 54.6\nPVTv2-B1 Mask R-CNN 1x 33.66M 252.2G 40.8 63.1 44.5 25.3 44.5 53.0\nUP-PVTv2-B1 41.6 64.1 45.5 25.9 45.0 54.0\nTable 7. Results on the ImageNet-1k validation set with different\nattention pruning strategies.\nStrategy 1 Strategy 2 Strategy 3\nAccuracy 48.51% 48.45% 48.05%\nTable 8. Results on the ImageNet-1k validation set with & without\nknowledge distillation.\nModel Distillation? Epochs Top 1 Acc.\nDeiT-T \" 500 74.54%\n% 500 72.65%\nUP-DeiT-T \" 200 75.79%\n% 200 76.11%\nUP-PVTv2-B0 \" 100 75.30%\n% 100 74.33%\nTable 9. Results on the ImageNet-1k validation set with 3 distil-\nlation strategies. We trained the models in SImageNet-1k with 50\nepochs.\nDistillation Soft Hard Soft + Patch\nAccuracy 51.51% 48.95% 51.49%\nTable 10. Results of the progressive pipeline when pruning to\nDeiT-T on ImageNet-1k. In every step we ﬁne-tuned model with\n50 epochs.\nModel Top-1 Acc. Throughput #Param.\nDeiT-B 81.84% 58.1 86.57M\nStep 1 79.90% 139.4 21.69M\n+ Step 2 79.60% 188.2 16.36M\n+ Step 3 75.05% 296.7 5.72M\n+ Step 4 74.51% 404.5 5.72M\nWe then investigate the performance of progressively\npruning the component 1 of DeiT-B. We compare two\npipelines. In the ﬁrst, we remove 576 redundant ﬁlters at\nonce after calculating the importance of all ﬁlters in com-\nponent 1. In the second pipeline, we imitate the process of\npruning blocks, and construct a loop that only throws away\n192 ﬁlters in component 1 every time, and repeat this loop\nthree times. The results are shown in Table 11. The ﬁrst\npipeline clearly works better, which indicates pruning all\nTable 11. Results of two pipelines when pruning component 1 on\nSImageNet-1k. Pruning at once represents the ﬁrst pipeline and\npruning progressively is the second one. We ﬁne-tuned models\nwith 50 epochs.\nComponent 1 Pruning at once Pruning progressively\nAccuracy 66.95% 65.05%\nTable 12. Results of two pipelines when pruning UP-DeiT-T into\nUP-DeiT-T-10 on ImageNet-1k. We ﬁne-tune models with 30\nepochs and with knowledge distillation.\nBlocks Pruning at once Pruning progressively\nAccuracy 73.20% 73.78%\nredundant ﬁlters at once is most effective when compress-\ning the width of ViTs.\nWe also demonstrate the effectiveness of progressively\npruning blocks. Based on the compressed UP-DeiT-T, we\ncompare two strategies for generating UP-DeiT-T-10. In the\nﬁrst we directly remove the two least unimportant blocks at\none time, and in the second we use our progressive block\npruning strategy. Once we get the two sub-model candi-\ndates, we apply the same training policy to ﬁne-tune model\non ImageNet-1k and show the results in Table 12. The pro-\ngressive block pruning strategy obtains higher accuracy.\n5. Conclusion, Limitations, and Future Work\nIn this paper, we proposed a novel method, UP-ViTs, for\npruning ViTs in a uniﬁed manner. Our framework can prune\nall components in ViT and its variants, maintain the models’\nstructure and generalize well into downstream tasks. Espe-\ncially, UP-ViTs achieved state-of-the-art results when prun-\ning various ViT backbones. Moreover, we studied the trans-\nferring ability of the compressed model and found that our\nUP-ViTs also had better performances than original ViTs.\nAlthough our framework can be applied to various ViT\narchitectures, we have to manually identify the uncorre-\nlated components existing in ViTs (cf. the appendix),\nhence how to automatically divide and prune the irrele-\nvant components is an interesting future direction. Be-\nsides this, we focus on pruning heavy models into exist-\ning lightweight model architectures, such as compressing\n8\nDeiT-B into DeiT-T. Therefore, continuing to prune ViTs to\nmake them more attractive for embedded systems is another\ninteresting direction. Transformer was originally designed\nto solve NLP problems and many previous works proposed\nvarious MHSA mechanisms, so in the future, we will fur-\nther generalize our pruning framework into NLP tasks.\nReferences\n[1] Maximiliana Behnke and Kenneth Heaﬁeld. Losing heads in\nthe lottery: Pruning transformer attention in neural machine\ntranslation. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing (EMNLP),\npages 2664–2674, 2020. 2\n[2] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. In The European\nConference on Computer Vision (ECCV) , volume 12346 of\nLNCS, pages 213–229. Springer, 2020. 1\n[3] Krzysztof Marcin Choromanski, Valerii Likhosherstov,\nDavid Dohan, Xingyou Song, Andreea Gane, Tamas Sar-\nlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin,\nLukasz Kaiser, David Benjamin Belanger, Lucy J Colwell,\nand Adrian Weller. Rethinking attention with Performers.\nIn International Conference on Learning Representations\n(ICLR), 2021. 2\n[4] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A.\nVedaldi. Describing textures in the wild. In The IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 3606–3613, 2014. 5\n[5] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe. RandAugment: Practical automated data augmentation\nwith a reduced search space. In The IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n702–703, 2020. 2, 5\n[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. ImageNet: A large-scale hierarchical im-\nage database. In The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 248–255, 2009. 5\n[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In International Conference on Learning Representa-\ntions (ICLR), 2021. 1, 2, 3\n[8] Mark Everingham, Luc Van Gool, Christopher KI Williams,\nJohn Winn, and Andrew Zisserman. The pascal visual object\nclasses (VOC) challenge. International Journal of Computer\nVision, 88(2):303–338, 2010. 5\n[9] Angela Fan, Edouard Grave, and Armand Joulin. Reducing\ntransformer depth on demand with structured dropout. In In-\nternational Conference on Learning Representations (ICLR),\n2019. 2\n[10] Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xi-\nangyang Xue, and Zheng Zhang. Star-Transformer. In\nProceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short\nPapers), pages 1315–1325, 2019. 1, 2\n[11] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-\nshick. Mask R-CNN. In Proceedings of the IEEE Interna-\ntional Conference on Computer Vision (ICCV), pages 2961–\n2969, 2017. 6\n[12] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi\nYang. Filter pruning via geometric median for deep con-\nvolutional neural networks acceleration. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 4340–4349, 2019. 2\n[13] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning\nfor accelerating very deep neural networks. In Proceedings\nof the IEEE International Conference on Computer Vision\n(ICCV), pages 1389–1397, 2017. 2\n[14] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q\nWeinberger. Deep networks with stochastic depth. In The\nEuropean Conference on Computer Vision (ECCV), volume\n9908 of LNCS, pages 646–661. Springer, 2016. 2\n[15] Ding Jia, Kai Han, Yunhe Wang, Yehui Tang, Jianyuan Guo,\nChao Zhang, and Dacheng Tao. Efﬁcient vision transform-\ners via ﬁne-grained manifold distillation. arXiv preprint\narXiv:2107.01378, 2021. 2, 6\n[16] Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Yujun Shi,\nXiaojie Jin, Anran Wang, and Jiashi Feng. All tokens matter:\nToken labeling for training better vision transformers. arXiv\npreprint arXiv:2104.10858, 2021. 2, 7\n[17] Krause Jonathan, Stark Michael, Deng Jia, and Li Fei-Fei.\n3d object representations for ﬁne-grained categorization. In\nInternational IEEE Workshop on 3D Representation and\nRecognition, pages 554–561, 2013. 5\n[18] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and\nFranc ¸ois Fleuret. Transformers are rnns: Fast autoregressive\ntransformers with linear attention. In International Confer-\nence on Machine Learning, pages 5156–5165, 2020. 1, 2\n[19] Alex Krizhevsky and Geoffrey Hinton. Learning multiple\nlayers of features from tiny images. Technical report, Uni-\nversity of Toronto, 2009. 5\n[20] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and\nHans Peter Graf. Pruning ﬁlters for efﬁcient convnets. In In-\nternational Conference on Learning Representations (ICLR),\n2017. 2\n[21] Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt\nKeutzer, Dan Klein, and Joey Gonzalez. Train big, then\ncompress: Rethinking model size for efﬁcient training and\ninference of transformers. In International Conference on\nMachine Learning, pages 5958–5968, 2020. 2\n[22] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Doll´ar. Focal loss for dense object detection. In Pro-\nceedings of the IEEE International Conference on Computer\nVision (ICCV), pages 2980–2988, 2017. 6\n[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nThe European Conference on Computer Vision (ECCV), vol-\nume 8693 of LNCS, pages 740–755. Springer, 2014. 5\n9\n[24] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin Transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), pages 10012–10022, 2021. 2, 6\n[25] Ilya Loshchilov and Frank Hutter. Decoupled weight de-\ncay regularization. In International Conference on Learning\nRepresentations (ICLR), 2018. 5\n[26] Jian-Hao Luo and Jianxin Wu. Neural network pruning with\nresidual-connections and limited-data. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 1458–1467, 2020. 2, 3\n[27] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A ﬁl-\nter level pruning method for deep neural network compres-\nsion. In Proceedings of the IEEE International Conference\non Computer Vision (ICCV), pages 5058–5066, 2017. 2\n[28] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi.\nFine-grained visual classiﬁcation of aircraft. arXiv preprint\narXiv:1306.5151, 2013. 5\n[29] Dmitrii Marin, Jen-Hao Rick Chang, Anurag Ranjan,\nAnish Prabhu, Mohammad Rastegari, and Oncel Tuzel.\nToken pooling in vision transformers. arXiv preprint\narXiv:2110.03860, 2021. 1\n[30] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and\nC. V . Jawahar. Cats and dogs. In The IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n3498–3505, 2012. 5\n[31] Ariadna Quattoni and Antonio Torralba. Recognizing indoor\nscenes. In The IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 413–420, 2009. 5\n[32] Yehui Tang, Kai Han, Yunhe Wang, Chang Xu, Jianyuan\nGuo, Chao Xu, and Dacheng Tao. Patch slimming for efﬁ-\ncient vision transformers. arXiv preprint arXiv:2106.02852,\n2021. 1, 2\n[33] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herve Jegou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. In International Conference on Machine Learning ,\npages 10347–10357, 2021. 1, 2, 6\n[34] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui,\nChen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and\nSerge Belongie. The inaturalist species classiﬁcation and de-\ntection dataset. In The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 8769–8778, 2018. 5\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Il-\nlia Polosukhin. Attention is all you need. In I. Guyon,\nU. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-\nwanathan, and R. Garnett, editors,Advances in Neural Infor-\nmation Processing Systems , volume 30, pages 5998–6008,\n2017. 1, 2\n[36] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-\nona, and Serge Belongie. The Caltech-UCSD Birds-200-\n2011 Dataset. Technical Report CNS-TR-2011-001, Cali-\nfornia Institute of Technology, 2011. 5\n[37] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\nPVTv2: Improved baselines with pyramid vision trans-\nformer. arXiv preprint arXiv:2106.13797, 2021. 2\n[38] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\nPyramid vision transformer: A versatile backbone for dense\nprediction without convolutions. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision\n(ICCV), pages 568–578, 2021. 2, 6\n[39] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua\nShen, Baoshan Cheng, Hao Shen, and Huaxia Xia. End-\nto-end video instance segmentation with transformers. In\nThe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 8741–8750, 2021. 1\n[40] Yifan Xu, Zhijie Zhang, Mengdan Zhang, Kekai Sheng, Ke\nLi, Weiming Dong, Liqing Zhang, Changsheng Xu, and\nXing Sun. Evo-ViT: Slow-Fast token evolution for dynamic\nvision transformer. arXiv preprint arXiv:2108.01390, 2021.\n2, 6\n[41] Huanrui Yang, Hongxu Yin, Pavlo Molchanov, Hai Li, and\nJan Kautz. NViT: Vision transformer compression and pa-\nrameter redistribution. arXiv preprint arXiv:2110.04869 ,\n2021. 2, 6\n[42] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nZi-Hang Jiang, Francis E.H. Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-Token ViT: Training vision transformers\nfrom scratch on imagenet. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages\n558–567, 2021. 2, 6\n[43] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regular-\nization strategy to train strong classiﬁers with localizable fea-\ntures. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision (ICCV), pages 6023–6032, 2019. 5\n[44] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and\nDavid Lopez-Paz. Mixup: Beyond Empirical Risk Mini-\nmization. In International Conference on Learning Repre-\nsentations (ICLR), 2018. 5\n[45] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\nYi Yang. Random erasing data augmentation. In Proceed-\nings of the AAAI Conference on Artiﬁcial Intelligence, pages\n13001–13008, 2020. 5\n[46] Mingjian Zhu, Kai Han, Yehui Tang, and Yunhe Wang. Vi-\nsual transformer pruning. In KDD 2021 Workshop on Model\nMining, 2021. 2, 6\n10\nA. The components of PVTv2\nPVTv2 proposes the overlapping patch embedding mod-\nule to divide a model into four stages. It also introduces a\nconvolution layer and a 3 ×3 depth-wise convolution into\nthe attention layer and FFN, respectively. As shown in Fig-\nure 2, we demonstrate four uncorrelated components in a\nPVTv2 block:\n/g41/g38/g78 /g29\n/g39/g16/g71/g76/g80\n/g47/g49\n/g41/g38/g84 /g29/g3/g3/g3/g3/g3/g3/g3/g3/g3/g3/g41/g38/g89 /g29\n/g48/g43/g54/g36\n/g41/g38/g83/g85 /g82 /g77 /g29\n/g39/g16/g71/g76/g80\n/g36/g87/g87/g72/g81/g87/g76/g82/g81/g3/g47/g68/g92/g72/g85\n/g41/g38/g20 /g29/g3/g1829/g3400/g1830\n/g47/g49\n/g38/g82/g81/g89/g71/g90 /g29/g3/g1829/g34001/g34003/g34003\n/g53/g72/g86/g75/g68/g83/g72\n/g41/g72/g72/g71/g16/g41/g82/g85/g90/g68/g85/g71/g3/g49/g72/g87/g90/g82/g85/g78\n/g1830/g3400/g1830 /g1830/g3400/g1830 /g1830/g3400/g1830\n/g1830/g3400/g1830\n/g41/g38/g21 /g29/g1830/g3400/g1829\n/g53/g72/g86/g75/g68/g83/g72/g3/g14/g3/g42/g40/g47/g56\n/g38/g82/g81/g89/g86/g85 /g29\n/g47/g49/g3/g14/g3/g53/g72/g86/g75/g68/g83/g72\nD × D × K × K\n/g53/g72/g86/g75/g68/g83/g72\nFigure 2. Illustration of four irrelevant components in a PVTv2\nblock.\n• Component 1: The numbers shown in red, namely the\nshortcut connections that chain representations in every\nstage, i.e., the input channels of FCq, Convsr and FC1,\nthe output channels of FCproj and FC2, and the ﬁrst and\nthird LN layers;\n• Component 2: The numbers shown in gold, namely the\nTable 13. Results of two distillation methods when pruning\nPVTv2-B2 into PVTv2-B1. We ﬁne-tuned UP-PVTv2-B1 model\nwith 50 epochs.\nMethods Distillation Accuracy\nPVTv2-B1 - 78.62%\nUP-PVTv2-B1 KL 79.23%\nMSE 79.48%\nattention embedding ﬁlters inside the attention layer in\nevery block, i.e., the input channels of FCproj and the\noutput channels of FCq, FCk and FCv;\n• Component 3: The numbers shown in blue, namely the\nFFN inter-layer ﬁlters in every block, i.e., the input chan-\nnels of FC2, the ﬁrst channel of Convdw and the output\nchannels of FC1.\n• Component 4: The numbers shown in purple, namely the\nspatial reduction attention embedding ﬁlters inside the at-\ntention layer in every block, i.e., the input channels of\nFCk and FCv, the output channels of Convsr, and the\nsecond LN layer.\nNote that because there are three additional patch em-\nbedding modules, when calculating the importance scores\nof component 1, we divide the whole shortcut connections\ninto four stages and calculate importance separately. In par-\nticular, three are only three components in the ﬁnal stage\nbecause the Convsr layer is removed.\nB. Pruning PVTv2-B2 into PVTv2-B1\nWhen pruning PVTv2-B2 into PVTv2-B1, we compare\ntwo distillation methods. The ﬁrst is the classic soft distilla-\ntion and we set αin Equation 11 as 0.4. The second method\nis, as described in Section 4.3, we use the MSE loss to dis-\ntill model. We show the results in Table 13. When distilling\nmodel with the MSE loss, UP-PVTv2-B1 obtains slightly\nhigher accuracy than that using the KL loss.\n11",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8024219274520874
    },
    {
      "name": "Pruning",
      "score": 0.6813701391220093
    },
    {
      "name": "Security token",
      "score": 0.5803771018981934
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5648806095123291
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.5336504578590393
    },
    {
      "name": "Machine learning",
      "score": 0.5235766172409058
    },
    {
      "name": "Transformer",
      "score": 0.5215611457824707
    },
    {
      "name": "Data mining",
      "score": 0.33442768454551697
    },
    {
      "name": "Agronomy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I881766915",
      "name": "Nanjing University",
      "country": "CN"
    }
  ]
}