{
  "title": "TransVOS: Video Object Segmentation with Transformers",
  "url": "https://openalex.org/W3167332518",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4286260458",
      "name": "Mei, Jianbiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2256225608",
      "name": "Wang Meng-meng",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Lin, Yeneng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1984509676",
      "name": "Yuan Yi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2058749718",
      "name": "Liu Yong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2799157347",
    "https://openalex.org/W3034538699",
    "https://openalex.org/W2086791339",
    "https://openalex.org/W2963253279",
    "https://openalex.org/W3034915791",
    "https://openalex.org/W2916743882",
    "https://openalex.org/W3108819577",
    "https://openalex.org/W3117097536",
    "https://openalex.org/W2470139095",
    "https://openalex.org/W3175132347",
    "https://openalex.org/W2799058067",
    "https://openalex.org/W2963503215",
    "https://openalex.org/W3098041548",
    "https://openalex.org/W3035080658",
    "https://openalex.org/W2986050084",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2031489346",
    "https://openalex.org/W3192871594",
    "https://openalex.org/W2990205821",
    "https://openalex.org/W3035672751",
    "https://openalex.org/W2964218467",
    "https://openalex.org/W2967622921",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3175373394",
    "https://openalex.org/W2889658408",
    "https://openalex.org/W3108995912",
    "https://openalex.org/W2916797271",
    "https://openalex.org/W3202066758",
    "https://openalex.org/W3094664776",
    "https://openalex.org/W3035077787",
    "https://openalex.org/W3122411985",
    "https://openalex.org/W3105891528",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3034798428",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3110030584",
    "https://openalex.org/W2963227409",
    "https://openalex.org/W3168649818",
    "https://openalex.org/W2889986507",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2963732700",
    "https://openalex.org/W2037954058",
    "https://openalex.org/W2998434318"
  ],
  "abstract": "Recently, Space-Time Memory Network (STM) based methods have achieved state-of-the-art performance in semi-supervised video object segmentation (VOS). A crucial problem in this task is how to model the dependency both among different frames and inside every frame. However, most of these methods neglect the spatial relationships (inside each frame) and do not make full use of the temporal relationships (among different frames). In this paper, we propose a new transformer-based framework, termed TransVOS, introducing a vision transformer to fully exploit and model both the temporal and spatial relationships. Moreover, most STM-based approaches employ two separate encoders to extract features of two significant inputs, i.e., reference sets (history frames with predicted masks) and query frame (current frame), respectively, increasing the models' parameters and complexity. To slim the popular two-encoder pipeline while keeping the effectiveness, we design a single two-path feature extractor to encode the above two inputs in a unified way. Extensive experiments demonstrate the superiority of our TransVOS over state-of-the-art methods on both DAVIS and YouTube-VOS datasets.",
  "full_text": "TransVOS: Video Object Segmentation with Transformers\nJianbiao Mei*, Mengmeng Wang*, Yeneng Lin, Yi Yuan, Yong Liu†\nLaboratory of Advanced Perception on Robotics and Intelligent Learning,\nCollege of Control Science and Engineering,\nZhejiang University; NetEase Fuxi AI Lab\n{jianbiaomei, mengmengwang, 3140100486}@zju.edu.cn,\nyuanyi@corp.netease.com, yongliu@iipc.zju.edu.cn\nAbstract\nRecently, Space-Time Memory Network (STM) based meth-\nods have achieved state-of-the-art performance in semi-\nsupervised video object segmentation (VOS). A crucial prob-\nlem in this task is how to model the dependency both among\ndifferent frames and inside every frame. However, most of\nthese methods neglect the spatial relationships (inside each\nframe) and do not make full use of the temporal relation-\nships (among different frames). In this paper, we propose a\nnew transformer-based framework, termed TransVOS, intro-\nducing a vision transformer to fully exploit and model both\nthe temporal and spatial relationships. Moreover, most STM-\nbased approaches employ two separate encoders to extract\nfeatures of two signiﬁcant inputs, i.e., reference sets (his-\ntory frames with predicted masks) and query frame (cur-\nrent frame), respectively, increasing the models’ parameters\nand complexity. To slim the popular two-encoder pipeline\nwhile keeping the effectiveness, we design a single two-path\nfeature extractor to encode the above two inputs in a uni-\nﬁed way. Extensive experiments demonstrate the superior-\nity of our TransVOS over state-of-the-art methods on both\nDA VIS and YouTube-VOS datasets. Codes are available at\nhttps://github.com/sallymmx/TransVOS.git.\nIntroduction\nVideo Object Segmentation (VOS), as a fundamental task\nin the computer vision community, attracts more and more\nattention in recent years due to its potential application in\nvideo editing, autonomous driving, etc. In this paper, we fo-\ncus on semi-supervised VOS, which provides the target ob-\njects’ masks in the ﬁrst frame, and the algorithms should\nproduce the segmentation masks for those objects in the sub-\nsequent frames. Under this setting, VOS remains challeng-\ning due to object occlusion, deformation, appearance varia-\ntion, and similar object confusion in video sequences.\nA key problem in semi-supervised VOS is how to exploit\nthe dependency both among different frames and inside ev-\nery frame. To better depict this point, we deﬁne two relation-\nships in this paper, i.e., temporal relationships(Fig. 1 (b))\nand spatial relationships(Fig. 1 (c)). The former is the rela-\ntionships among pixels in different frames, representing cor-\nrespondence of target objects among all the frames, which\n*Equal contribution\n†Corresponding author\n(a)\nFrame k\nReference frames\nQuery frame\nFrame i\nFrame j\nFame n\n(b) (c)\nFigure 1: (a) relationships between pixels in query frame and\npixels in reference frames. (b) relationships among pixels in\ndifferent frames. (c) relationships among pixels in the same\nframe.\nis vital for learning robust global target object features and\nhelps handle appearance change across frames. The latter is\nthe relationships among pixels in one speciﬁc frame, includ-\ning object appearance information for target localization and\nsegmentation, which is important for learning local target\nobject structure and helps obtain accurate masks. Recently, a\ngroup of matching-based methods (Hu, Huang, and Schwing\n2018; V oigtlaender et al. 2019; Yang, Wei, and Yang 2020;\nOh et al. 2019; Li, Shen, and Shan 2020; Lu et al. 2020; Lai,\nLu, and Xie 2020; Liang et al. 2020; Seong, Hyun, and Kim\n2020; Xie et al. 2021; Wang et al. 2021) provide partial solu-\ntions for capturing above correspondence and achieve com-\npetitive performance. The basic idea of these methods is to\ncompute the similarities of target objects between the cur-\nrent and past frames by feature matching, in which attention\nmechanism is widely used. Among them, the Space-Time\nMemory (STM) based approaches (Oh et al. 2019; Li, Shen,\nand Shan 2020; Lu et al. 2020; Lai, Lu, and Xie 2020; Liang\net al. 2020; Xie et al. 2021; Wang et al. 2021) have achieved\ngreat success. They propose to apply spatio-temporal atten-\ntion between every pixel in previous frames and every pixel\nin the current frame. However, the spatio-temporal atten-\ntion module in previous approaches could not thoroughly\nmodel the temporal and spatial relationships. As illustrated\nin Fig. 1 (a), it only computes attentions among pixels in\nthe query frame against pixels in each reference frame. The\ntemporal relationships are not fully explored since they ig-\nnore the dependency among all the history frames. Besides,\narXiv:2106.00588v2  [cs.CV]  18 Sep 2021\nthe spatial relationships of pixels inside every frame are\nalso neglected. There are a few methods paid attention to\nthese issues. For instance, EGMN (Lu et al. 2020) proposes\na fully-connected graph to capture cross-frame correlation,\nwhich exploits the temporal relationships effectively. How-\never, EGMN still omits the spatial relationships. Our key in-\nsight is that both the temporal and spatial relationships are\nsigniﬁcant to VOS and should be utilized effectively. We\nﬁnd the recent vision transformer (Dosovitskiy et al. 2020)\nis a satisfactory model to cater to the demand since the self-\nattention modules in the transformer could establish depen-\ndency among every elements in the input sequence. There-\nfore, we proposed a new transformer-based architecture for\nVOS, termed TransVOS, to capture the temporal relation-\nships and spatial relationships jointly.\nAnother signiﬁcant but open problem is how to effec-\ntively represent the inputs, i.e., reference sets (history infor-\nmation) and query frames (current information). Many ex-\nisting methods try to encode these two kinds of informa-\ntion with two separate encoders (always termed as mem-\nory/reference encoder and query encoder), since reference\nsets include several history images and their predicted masks\nwhile the query frames only has current images. This two-\nencoder pipeline is swollen and contains plenty of redun-\ndant parameters. Existing ways to slim it are limited. For ex-\nample, AGAME (Johnander et al. 2019) and RANet (Wang\net al. 2019b) employ a siamese network to encode reference\nand query frame and then concatenate features of the refer-\nence frame with its predicted mask to reinforce target fea-\ntures. RGMP (Oh et al. 2018) and AGSS-VOS (Lin, Qi, and\nJia 2019) concatenate the current frame with the previous\nframe’s mask or warped mask to form a 4-channel input,\nso as the reference sets. Then a shared encoder with a 4-\nchannel input layer is used to extract features. These strate-\ngies can reuse features and effectively reduce the amount of\nparameters. Nevertheless, directly concatenating them with\nhigh-level semantic features is insufﬁcient since the abun-\ndant features like contour and edges of the mask are not\nfully leveraged. Besides, due to the padding operation, ob-\nject positions of frame features and the down sampled mask\nmay be misaligned. In addition, concatenating the previous\nframe’s mask with the query frame may bring large displace-\nment shifting error and using optical ﬂow to warp the mask\nis time-consuming. Different from them, in this paper, we\ndevelop a plain yet effective feature extractor which has a\ntwo-path input layer and accepts the reference sets and the\nquery frames in the meanwhile, signiﬁcantly simplifying the\nexisting VOS framework while keeping the effectiveness.\nOur main contributions can be summarized as follows:\n•We proposed a new transformer-based VOS framework,\nTransVOS, to effectively model the temporal and spatial de-\npendency.\n• TransVOS has a concise pipeline, which does not\nneed multiple encoders, dramatically simplifying the exist-\ning VOS pipelines.\n•We comprehensively evaluate the proposed TransVOS\non 3 benchmark datasets including DA VIS 2016/2017 (Per-\nazzi et al. 2016; Pont-Tuset et al. 2017) and YouTube-VOS\n(Xu et al. 2018). The results demonstrate the effectiveness\nand efﬁciency of our method in comparison with the previ-\nous approaches.\nRelated works\nTracking-based methods. These methods (Wang et al.\n2019a; Chen et al. 2020; V oigtlaender et al. 2020; Huang\net al. 2020) integrate object tracking techniques to indicate\ntarget location and spatial area for segmentation and im-\nprove the inference speed. SiamMask (Wang et al. 2019a)\nadds a mask branch on SiamRPN (Li et al. 2018) to narrow\nthe gap between tracking and segmentation. FTAN-DTM\n(Huang et al. 2020) takes object segmentation as a sub-\ntask of tracking, introducing “tracking-by-detection” model\ninto VOS. While the accuracy of tracking often limits these\nmethods’ performance. SAT (Chen et al. 2020) and FTMU\n(Sun et al. 2020) fuse object tracking and segmentation into\na truly uniﬁed pipeline. SAT combines SiamFC++ (Xu et al.\n2020) and proposed an estimation-feedback mechanism to\nswitch between mask box and tracking box, making segmen-\ntation and tracking tasks enhance each other.\nMatching-based methods. Recently, state-of-the-art per-\nformance has been achieved by matching-based methods\n(Hu, Huang, and Schwing 2018; V oigtlaender et al. 2019;\nWang et al. 2021; Oh et al. 2019; Lu et al. 2020; Liang et al.\n2020; Xie et al. 2021), which perform feature matching to\nlearn target object appearances ofﬂine. FEELVOS (V oigt-\nlaender et al. 2019) and CFBI (Yang, Wei, and Yang 2020)\nperform the nearest neighbor matching between the current\nframe and the ﬁrst and previous frames in the feature space.\nSTM (Oh et al. 2019) introduces an external memory to store\npast frames’ features and uses the attention-based matching\nmethod to retrieve information from memory. KMN (Seong,\nHyun, and Kim 2020) proposes to employ a gaussian kernel\nto reduce the non-locality of the STM. RMNet (Xie et al.\n2021) proposes to replace STM’s global-to-global match-\ning with local-to-local matching to alleviate the ambiguity of\nsimilar objects. EGMN (Lu et al. 2020) organizes the mem-\nory network as a fully connected graph to store frames as\nnodes and capture cross-frame relationships by edges. How-\never, these methods do not fully utilize the spatio-temporal\nrelationships among reference sets and query frame. In this\npaper, we introduce a vision transformer to model spatio-\ntemporal dependency, which can help handle large object\nappearance change.\nTransformer-based methods. Recently, transformer has\nachieved great success in vision tasks like image classiﬁ-\ncation (Dosovitskiy et al. 2020), object detection (Carion\net al. 2020), semantic segmentation (Wang et al. 2020),\nobject tracking (Yan et al. 2021), etc. Due to the impor-\ntance of spatial and temporal relationships for segmenting,\nwe also employ the vision transformer into the VOS task,\nwhich is inspired by DETR (Carion et al. 2020). We ﬁnd\ntwo transformer-based methods, SST (Duke et al. 2021) and\nJOINT (Mao et al. 2021). SST uses the transformer’s en-\ncoder with sparse attention to capture the spatio-temporal in-\nformation among the current frame and its preceding frames.\nWhile mask representations are not explored in SST. JOINT\ncombines inductive and transductive learning and extend the\ntransduction branch to a transformer architecture. But its\nnetwork structure is complicated. Besides, both SST and\nJOINT did not employ the transformer’s decoder and could\nnot enjoy the strong power of it.\nMethods\nThe overview of our framework is illustrated in Fig. 2. It\nmainly consists of a feature extractor , a vision transformer,\na target attention block and a segmentation head. When seg-\nmenting a speciﬁc frame, we ﬁrstly use the feature extrac-\ntor to extract the features of the current frame and reference\nsets. The outputs of the extractor are fed into a bottleneck\nlayer to reduce the channel number. Then features are ﬂat-\ntened before feeding into a vision transformer, which models\nthe temporal and spatial relationships. Moreover, the target\nattention block takes both the transformer’s encoder and de-\ncoder’s outputs as input and then outputs the feature maps,\nrepresenting the target mask features. Finally, a segmenta-\ntion head is attached after the target attention block to obtain\nthe predicted object mask.\nFeature Embedding\nTo fully exploit the temporal and spatial information in the\nreference sets and the query frame, we need a delicate fea-\nture extractor that can effectively extract the target object\nfeatures and map them into an embedding space to be ready\nfor feeding into the following vision transformer.\nFeature extractor. TransVOS uses a single feature extrac-\ntor to extract features of the query frame and reference sets\nin a uniﬁed way. Speciﬁcally, we design a two-path input\nlayer to adaptively encode two types of inputs, i.e, RGB\nframes or the pairs of RGB frames with corresponding ob-\nject masks. As shown in Fig. 2, when taking the RGB frames\nas input, it will go through the ﬁrst path which has one regu-\nlar convolution operation. For reference sets, the second path\nwill be used, which contains three convolutions to encode\nthe RGB frame, the object mask’s foreground and back-\nground, respectively. The output features of the three convo-\nlutions are added together to output the features. Our method\ncan use arbitrary convolutional networks as the feature ex-\ntractor by replacing the ﬁrst layer with the two-path input\nlayer. Here we employ the ﬁrst four stages of ResNet (He\net al. 2016) as the feature extractor. After going through the\ntwo-path input layer, the features from the query frame and\nreference sets are ﬁrst concatenated along the temporal di-\nmension and then fed into the convolutional network (CNN).\nFinally, the reference sets and current frame are mapped to\nfeature maps f ∈ R(T+1)×C×H×W , where H, W, C are\nthe height, width and channel. T is the number of the ref-\nerence pairs. The proposed two-path feature extractor has\nmuch fewer parameters (about 20% reduction) than the tra-\nditional two-encoders pipeline while keeping the effective-\nness.\nBefore feeding into the vision transformer, we use a 1x1\nconvolution layer to reduce the spatial channel of the fea-\nture maps from C to d (d < C), resulting in new feature\nmaps f1 ∈R(T+1)×d×H×W . Then, the spatial and temporal\ndimensions of f1 are ﬂattened into one dimension, produc-\ning feature vectors X ∈R(T+1)HW ×d, which servers as the\ninput of the transformer encoder.\nRelationship Modeling\nTransformers have strong capabilities for modeling spatio-\ntemporal relationships. First, the positional encoding explic-\nitly introduces space-time position information, which could\nhelp the encoder model spatio-temporal relationships among\npixels in the input frames. Second, the encoder could learn\nthe target object’s correspondence among the input frames\nand model the target object’s structure in a speciﬁc frame.\nThird, the decoder could predict the spatial positions of the\ntarget objects in the query frame and focus on the most rel-\nevant object, which learns robust target representations for\ntarget object and empowers our network to handle similar\nobject confusion better.\nPositional encoding. We add sinusoidal positional encod-\ning PE (Vaswani et al. 2017) to the embedded featuresX to\nform the inputs Z of the transformer. Mathematically,\nZ = X + PE (1)\nPE(pos,2i) = sin(pos/100002i/d) (2)\nPE(pos,2i+ 1) = cos(pos/100002i/d) (3)\nwhere posand iare the spatio-temporal position and the di-\nmension of the features X, respectively.\nTransformer encoder. The transformer encoder is used to\nmodel the spatio-temporal relationships among pixel-level\nfeatures of the sequence. It takes featuresZ as input and out-\nputs encoded features E. The encoder consists of N encoder\nlayers, each of which has a standard architecture including a\nmulti-head self-attention module and a fully connected feed-\nforward network. The multi-head self-attention module is\nused to capture spatio-temporal relationships from different\nrepresentation sub-spaces. Let zp,t ∈Rd represents an el-\nement of Z, where pand tdenote the spatial and temporal\nposition, respectively. Firstly, for mE-th (mE ≤M) atten-\ntion head, the query/key/value vector ( qmE\np,t /kmE\np,t /vmE\np,t ) is\ncomputed from the representation zp,t:\nqmE\np,t = WmE\nq zp,t,kmE\np,t = WmE\nk zp,t,vmE\np,t = WmE\nv zp,t\n(4)\nThen the multi-head self-attention feature ˆ zp,t is calculated\nby\nˆ zp,t =\nM∑\nmE=1\nWmE\no [\nT+1∑\nt=1\nHW∑\np=1\nσ((qmE\np,t )T kmE\np,t√\ndmE\n) ·vmE\np,t ] (5)\nwhere T represents the size of the referent set,\nWmE\nq ,WmE\nk ,WmE\nv ∈RdmE ×d and WmE\no ∈Rd×dmE are\nlearnable weights ( dmE = d/M by default), σ indicates\nthe softmax function. Note that we compute attention\nalong the spatio-temporal dimension. Thus we can model\nthe spatial relationships and temporal relationships in the\nmeanwhile.\nSequence of  encoded features\nCNNC\n+ \nTransformer Decoder\nTarget Attention\nBlock\nSegmentation\nHead\nC\nSequence of  frame features\nTransformer Encoder\nTarget query\nPositional\nencoding\nTarget prediction\nFeature extractorTransformer\nSegmentation\nPredicted mask\nReference set\nQuery frame\nTwo-path Input Layer\nFigure 2: Overview of our TransVOS. The feature extractor is used to extract the features of the current frame and reference\nsets. The vision transformer is exploited to model the temporal and spatial relationships. The target attention block (TAB) is\nused to extract the target mask features. The segmentation head is designed to obtain the predicted object mask. ” +”, ”C”\nindicate adding and concatenating operation, respectively.\nTransformer decoder. The transformer decoder aims to\npredict the spatial positions of the target and focus on the\nmost relevant object in the query frame. It takes encoded\nfeatures E and target query xo as input and output de-\ncoded features O. We only utilize one target query in the\ndecoder to query the target object features since there is\nonly one prediction. The decoder consists ofN decoder lay-\ners, each of them includes a multi-head self-attention mod-\nule, a multi-head cross-attention module, and a fully con-\nnected feed-forward network. In our TransVOS, the multi-\nhead self-attention module is used to integrate target in-\nformation from different representation sub-space. And the\nmulti-head cross-attention module is mainly leveraged to\nfuse target object features from the encoder. With only one\ntarget query xo ∈Rd, the multi-head self-attention feature\nˆ xso can be expressed as:\nˆ xso =\nM∑\nmD=1\nWmD\no (WmD\nv xo) (6)\nwhere mD indexes the attention head in multi-head self-\nattention module, WmD\nv ∈RdmD ×d and WmD\no ∈Rd×dmD\nare learnable weights (dmD = d/Mby default).\nFor the multi-head cross-attention module, let ep,t ∈Rd\nrepresents an element of E, p and t denote the spatial and\ntemporal position, respectively. For m′\nD-th (m′\nD ≤M) at-\ntention head, the key and value vectors km′\nD\np,t ,vm′\nD\np,t are com-\nputed as:\nkm′\nD\np,t = Wm′\nD\nk ep,t,vm′\nD\np,t = Wm′\nD\nv ep,t (7)\nThen with the query x′o ∈Rd, the cross-attention feature\nˆ xco is calculated:\nˆ xco =\nM∑\nm′\nD=1\nWm′\nD\no [\nT+1∑\nt=1\nHW∑\np=1\nσ((Wm′\nD\nq x′o)T km′\nD\np,t√dm′\nD\n) ·vm′\nD\np,t ]\n(8)\nwhere T denotes the size of the reference set,\nWm′\nD\nq ,Wm′\nD\nk ,Wm′\nD\nv ∈ R\ndm′\nD\n×d\nand Wm′\nD\no ∈ R\nd×dm′\nD\nare learnable weights(dm′\nD\n= d/M by default). σindicates\nthe softmax function.\nSegmentation\nTarget attention block. To obtain the target mask predic-\ntion from the outputs of the transformer, the model needs\nto extract the target’s mask features of the query frame. To\nachieve this goal, we use a Target Attention Block (TAB) to\nget an attention map ﬁrst. TAB computes the similarity be-\ntween encoded features EQ of query frame and the output\nfeatures O from the decoder.O and EQ are fed into a multi-\nhead self-attention module (with M head) to obtain the at-\ntention maps. We concatenate the attention maps withEQ as\nthe input S of the following segmentation head to enhance\nthe target features. The above procedure can be formulated\nas:\nAttni(O,EQ) = σ((Wi\nqO)T (Wi\nkEQ)√di\n) (9)\nS = [EQ,Attn1(O,EQ),···,AttnM (O,EQ)] (10)\nwhere iindexes the attention head, Wi\nq,Wi\nk ∈Rdi×d, are\nlearnable weights (di = d/Mby default).\nSegmentation head. The features S are fed into a segmen-\ntation head which outputs the ﬁnal mask prediction. Here,\nwe use the reﬁne module used in (Oh et al. 2018, 2019)\nas the building block to construct our segmentation head. It\nconsists of two blocks, each of which takes both the output\nof the previous stage and the current frame’s feature maps\nfrom feature extractor at the corresponding scale through\nskip-connections. Gradually upscale the compressed feature\nmaps by a factor of two at a time. And a 2-channel convolu-\ntion and a softmax operation are attached behind blocks to\nattain the predicted mask in 1/4 scale of the input image. Fi-\nnally, we use bi-linear interpolation to upscale the predicted\nmask to the original scale.\nMulti-object segmentation. Our framework can be ex-\ntended to multi-object segmentation easily. Speciﬁcally, the\nnetwork ﬁrst predicts the mask for every target object. Then,\na soft aggregation operation is used to merge all the pre-\ndicted maps. We apply this way during both the training and\ninference to keep consistency on both stages. For each loca-\ntion l in predicted mask Mi of object i(i < N), the prob-\nability pl,i after soft aggression operation can be expressed\nas:\npl,i = σ(logit(ˆpl,i)) = ˆpl,i/(1 −ˆpl,i)∑N−1\nj=0 ˆpl,j/(1 −ˆpl,j)\n(11)\nwhere N is the number of objects. σand logit represent the\nsoftmax and logit functions, respectively. The probability\nof the background is obtained by subtracting from 1 after\ncomputing the output of the merged foreground.\nTraining and Inference\nTraining. Our proposed TransVOS doesn’t require ex-\ntremely long training video clips since it does not have any\ntemporal smoothness assumptions. Even though, TransVOS\ncan still learn long-term dependency. Just like most STM-\nbased methods (Oh et al. 2019; Li, Shen, and Shan 2020; Lu\net al. 2020; Liang et al. 2020; Seong, Hyun, and Kim 2020),\nwe synthesis video clips by applying data augmentations\n(random afﬁne, color, ﬂip, resize and crop) on a static image\nof datasets (Cheng et al. 2014; Lin et al. 2014; Li et al. 2014;\nEveringham et al. 2010). Then we use the synthetic videos\nto pretrain our model. This pre-training procedure helps our\nmodel to be robust against a variety of object appearance and\ncategories. After that, we train our model on real videos. We\nrandomly select T frames from a video sequence of DA VIS\n(Perazzi et al. 2016; Pont-Tuset et al. 2017) or YouTube-\nVOS (Xu et al. 2018) and apply data augmentation on those\nframes to form a training video clip. By doing so, we can\nexpect our model to learn long-range spatio-temporal infor-\nmation. We add cross-entropy loss Lcls and mask IoU loss\nLIoU as the multi-object training loss L, which can be ex-\npressed as:\nL= 1\nN\nN−1∑\ni=0\n[Lcls(Mi,Yi) + LIoU (Mi,Yi)] (12)\nLcls(Mi,Yi) = − 1\n|Ω|\n∑\np∈Ω\n[Yilog( exp(Mi)∑N−1\nj=0 (exp(Mj))\n)]p\n(13)\nLIoU (Mi,Yi) = 1 −\n∑\np∈Ω min(Yp\ni ,Mp\ni )∑\np∈Ω max(Yp\ni ,Mp\ni ) (14)\nwhere Ω denotes the set of all pixels in the object mask,\nMi,Yi represent the predicted mask and ground truth of\nobject i, N is the number of objects. Note that N is set to 1\nwhen segmenting a single object.\nInference. Our model uses past frames with correspond-\ning predicted masks to segment the current frame during the\nonline reference phase. We don’t use external memory to\nstore every past frame’s features but only use the ﬁrst frame\nwith ground truth and the previous frames with its predicted\nmasks as the reference sets. Because the former always pro-\nvides the most reliable information, and the later is the most\nsimilar one to the current frame.\nExperiments\nWe evaluate our approach on DA VIS (Perazzi et al. 2016;\nPont-Tuset et al. 2017) and YouTube-VOS (Xu et al. 2018)\nbenchmarks. Please refer to supplementary materials for\nmore details about the datasets and evaluation metrics.\nImplementation details.\nWe use the ﬁrst four stages of ResNet50 (He et al. 2016)\nand replace its input layer with the proposed two-path in-\nput layer to form our feature extractor. The number of trans-\nformer encoder layers and decoder layers is set to N = 6.\nThe multi-head attention layers have M = 8 heads, width\nd = 256 , while the feed-forward networks have hidden\nunits of 2048. Dropout ratio of 0.1 is used. The TransVOS\nis trained with the input resolution of 480p, and the length\nT of the training video clip is set to 2. We freeze all batch\nnormalization layers and minimize our loss using AdamW\noptimizer (β = (0 .9,0.999), eps = 10 −8, and the weight\ndecay is 10−4) with the initial learning rate lr = 10−4. The\nmodel is trained with batchsize 4 for 160 epochs on 4 TI-\nTAN RTX GPUs, taking about 1.5 days. In the inference\nstage, TransVOS with input resolution 480p only refers to\nthe ﬁrst and previous frames to segment the current frame.\nWe conduct all inference experiments on a single TITAN\nRTX GPU.\nComparison with the State-of-the-art\nDA VIS. We compare the proposed TransVOS with the\nstate-of-the-art methods on DA VIS benchmark (Perazzi\net al. 2016; Pont-Tuset et al. 2017). We also present the\nresults trained with additional data from YouTube-VOS\n(Xu et al. 2018). The evaluation results on DA VIS16-val\nand DA VIS17-val are reported in Table 1. When adding\nYouTube-VOS for training, our method achieves state-of-\nthe-art performance on DA VIS17-val (J&F83.9%), outper-\nforming the online-learning methods with a large margin and\nhave higher performance than the matching-based methods\nsuch as STM, RMNet and CFBI. Speciﬁcally, TransVOS\noutperforms transformer-based SST with 1.4% in (J&F)\nand surpasses JOINT with gap of 0.4% in J&F. When\nonly using DA VIS for training, our model achieves better\nquantitative results than those methods with same conﬁgu-\nration and even better than several methods like FEELVOS\nand AGAME which apply YouTube-VOS for training. On\nDA VIS16-val, TransVOS has comparable performance with\nstate-of-the-art methods. Compared to KMN, our model\nhas the same J&F score with a higher J score while a\nslightly lower F score. Since DA VIS 2016 is a single ob-\nject dataset, segmentation details such as boundaries play an\nimportant role in performance evaluation. We believe that\nthe Hide-and-Seek training strategy, which provides more\nprecise boundaries, helps KMN a lot. We also report the re-\nsults on the DA VIS17 test-dev in Table 2. Our TransVOS\nMethods OL DA VIS16 val DA VIS17 val\nFPS J&F(%) J(%) F(%) J&F(%) J(%) F(%)\nOSVOS (Caelles et al. 2017) ✓ 0.22 80.2 79.8 80.6 60.3 56.7 63.9\nOnA VOS (V oigtlaender and Leibe 2017) ✓ 0.08 85.5 86.1 84.9 67.9 64.5 71.2\nPReMVOS (Luiten, V oigtlaender, and Leibe 2018) ✓ 0.03 86.8 84.9 88.6 77.8 73.9 81.7\nFRTM(+YT) (Robinson et al. 2020) ✓ 21.9 83.5 - - 76.7 - -\nRGMP (Oh et al. 2018) 7.7 81.8 81.5 82.0 66.7 64.8 68.6\nRaNet (Wang et al. 2019b) 30 85.5 85.5 85.4 65.7 63.2 68.2\nAFB-URR (Liang et al. 2020) - - - - 74.6 73.0 76.1\nAGAME(+YT) (Johnander et al. 2019) 14.3 82.1 82.0 82.2 70.0 67.2 72.7\nFEELVOS(+YT) (V oigtlaender et al. 2019) 2.2 81.7 81.1 82.2 71.5 69.1 74.0\nSTM(+YT) (Oh et al. 2019) 6.3 89.3 88.7 89.9 81.8 79.2 84.3\nKMN(+YT) (Seong, Hyun, and Kim 2020) 8.3 90.5 89.5 91.50 82.8 80.0 85.6\nEGMN(+YT) (Lu et al. 2020) - - - - 82.8 80.2 85.2\nCFBI(+YT) (Yang, Wei, and Yang 2020) 6 89.4 88.3 90.5 81.9 79.1 84.6\nJOINT(+YT) (Mao et al. 2021) - - - - 83.5 80.8 86.2\nSST(+YT) (Duke et al. 2021) - - - - 82.5 79.9 85.1\nRMNet(+YT) (Xie et al. 2021) 12 88.8 88.9 88.7 83.5 81.0 86.0\nSwiftNet(+YT) (Wang et al. 2021) 25 90.4 90.5 90.3 81.1 78.3 83.9\nTransVOS 6.6 85.8 85.4 86.2 78.1 75.7 80.5\nTransVOS(+YT) 6.6 90.5 89.8 91.2 83.9 81.4 86.4\nTable 1: Comparison with state-of-the-art on the DA VIS16 and DA VIS17 validation set. ‘OL’ indicates the use of online-learning\nstrategy. ‘+YT’ indicates the use of YouTube-VOS for training. Note that the runtime of other methods was obtained from the\ncorresponding papers.\noutperforms all the online-learning methods. Except slightly\nlower than KMN of 0.3%(J&F), TransVOS surpasses all\nthe methods in the second part.\nYouTube-VOS. Table 2 shows comparison with state-of-\nthe-art methods on YouTube-VOS 2018 validation (Xu et al.\n2018). On this benchmark, our method obtains an overall\nscore of 81.8% and also outperforms all the methods in the\nﬁrst and second parts, which demonstrates that TransVOS is\nrobust and effective. Speciﬁcally, the proposed TransVOS\nsurpasses STM by 2.4% in overall score. Note that we\nonly refer to the ﬁrst and previous frames to segment the\ncurrent frame, while STM contains a large memory bank\nwhich saves a new memory frame every ﬁve frames. Also,\nTransVOS outperforms KMN and CFBI with gaps of both\n0.4% in overall score. Besides, it surpasses the most related\ntransformer-based SST.\nAblation Study\nWe conduct all the ablation experiments on DA VIS17 vali-\ndation (Pont-Tuset et al. 2017). The model used in this sec-\ntion does not do pre-training on synthetic videos and the\ninput resolution is 240p unless speciﬁed. And we test the\nmodel with only the ﬁrst and previous frames referred by de-\nfault. Here we list the ablation studies about two-path feature\nextractor, mask utilization, reference sets, and transformer\nstructure. The exploration of the backbone, input resolution\nand training strategy are presented in the supplementary ma-\nterials.\nTwo-path feature extractor. In Table 4, we compare the\nproposed two-path feature extractor with i) existing ap-\nproach of using two independent encoders (as in STM (Oh\net al. 2019); denoted ‘Independent’), and ii) using a siamese\nencoder and concatenating the object mask to the reference\nframe features (as in AGAME(Johnander et al. 2019); de-\nnoted as ‘Siamese’). Results showed that our two-path fea-\nture extractor employs fewer parameters (about 20% reduc-\ntion) than i), but obtains higher performance (+7.8% inJ&F\nscore) than ii).\nMask utilization. To demonstrate the effectiveness of our\ntwo-path feature extractor, we implement three typical ways\nto utilize the predicted masks of past frames. (1) the pre-\ndicted masks are multiplied with the encoded features of\nRGB frame, denoted as ‘multiply’; (2) the encoded features\nof RGB frame and the predicted mask are multiplied ﬁrstly\nand then added to the former, denoted as ‘residual’; (3) the\npredicted masks and the RGB frame are fed into a two-\npath feature extractor, denoted as ‘ two-path’. As shown in\nTable 3, compared to directly multiply the predicted mask\nwith encoded features (line 1) and fusing mask with resid-\nual structure (line 2), our two-path feature extractor gains\n15.1%(J&F) and 8.0%(J&F) improvement.\nReference sets. We test how reference sets affect the per-\nformance of our proposed model. We experiment with four\ntypes of reference set conﬁgurations: (1) Only the ﬁrst frame\nwith the ground-truth masks; (2) Only the previous frame\nwith its predicted mask; (3) Both the ﬁrst and previous frame\nwith their masks; (4) The reference set is dynamically up-\nMethods OL DA VIS17 test-dev YouTube-VOS 2018 val\nJ&F(%) J(%) F(%) Overall Js(%) Ju(%) Fs(%) Fu(%)\nOSVOS (Caelles et al. 2017) ✓ 50.9 47.0 54.8 58.8 59.8 54.2 60.5 60.7\nOnA VOS (V oigtlaender and Leibe 2017) ✓ 52.8 49.9 55.7 55.2 60.1 46.6 62.7 51.4\nPReMVOS (Luiten, V oigtlaender, and Leibe 2018)✓ 71.6 67.5 75.7 - - - - -\nSTM-cycle (Li et al. 2020) ✓ 58.6 55.3 62.0 70.8 72.2 62.8 76.3 71.9\nRGMP (Oh et al. 2018) 52.9 51.3 54.4 53.8 59.5 45.2 - -\nAGAME (Johnander et al. 2019) - - - 66.1 67.8 60.8 - -\nFEELVOS (V oigtlaender et al. 2019) 57.8 55.2 60.5 - - - - -\nRaNet (Wang et al. 2019b) 55.3 53.4 - - - - - -\nSTM (Oh et al. 2019) 72.2 69.3 75.2 79.4 79.7 72.8 84.2 80.9\nCFBI (Yang, Wei, and Yang 2020) 74.8 71.1 78.5 81.4 81.1 75.3 85.8 83.4\nAFB-URR (Liang et al. 2020) - - - 79.6 78.8 74.1 83.1 82.6\nKMN (Seong, Hyun, and Kim 2020) 77.2 74.1 80.3 81.4 81.4 75.3 85.6 83.3\nEGMN (Lu et al. 2020) - - - 80.2 80.7 74.0 85.1 80.9\nSST (Duke et al. 2021) - - - 81.7 81.2 76.0 - -\nRMNet (Xie et al. 2021) 75.0 71.9 78.1 81.5 82.1 75.7 85.7 82.4\nSwiftNet (Wang et al. 2021) - - - 77.8 77.8 72.3 81.8 79.5\nTransVOS 76.9 73.0 80.9 81.8 82.0 75.0 86.7 83.4\nTable 2: Compare to the state of the art on the DA VIS17 test-dev set and YouTube-VOS 2018 validation set. ‘OL’ indicates the\nuse of online-learning strategy. The subscripts of Jand Fon YouTube-VOS denote seen objects (s) and unseen objects ( u).\nThe metric overall means the average of Js,Ju,Fs,Fu.\nVariants Mask utilization Reference sets JM(%) JR(%) JD(%) FM(%) FR(%) FD(%) J&F(%) FPS\n1 multiply 1st frame 51.4 59.9 13.4 58.3 63.6 14.4 54.9 -\n2 residual 1st frame 58.5 67.1 17.6 65.5 75.0 18.6 62.0 -\n3 two-path 1st frame 66.5 78.2 13.3 73.6 83.5 15.9 70.0 23.0\n4 two-path previous frame 64.3 74.8 11.7 70.5 81.3 14 67.4 17.6\n5 two-path 1st & previous frames 73.1 86.6 1.8 79.7 91.5 5.3 76.4 17.1\n6 two-path Every 5 frames 70.2 82.2 6.0 77.6 89.0 8.1 73.9 5.1\nTable 3: Ablation studies of mask utilization and reference sets with input resolution 240p on DA VIS 2017 validation set.\nFeature extractor J F J &F Parameters FPS(%) (%) (%) (M)\nSiamese 64.9 72.4 68.6 35.61 18.1\nIndependent 72.8 80.3 76.5 43.08 17.0\nTwo-path 73.1 79.7 76.4 34.56 17.0\nTable 4: Impacts of different types of feature extractors.\nModels are tested with the input resolution of 240p on\nDA VIS17-val.\ndated by appending new frames with the predicted masks\nevery 5 frames. As Table 3 shows, even with two frames re-\nferred, our model could achieve superior performance. Inter-\nestingly, we ﬁnd that updating the memory every 5 frames as\nSTM (Oh et al. 2019) for VOS may not be beneﬁcial in all\nmethods. Because low-quality segmentation results of his-\ntorical frames may mislead subsequent mask prediction.\nTransformer structure. We explore the effectiveness and\nnecessity of the transformer decoder in Table 5. It can be\nseen that equipping with transformer decoder, our model\nobtains 1.2%(J&F) improvement over removing it. There-\nComponents JM JR JD FM FR FD J&F\n(%) (%) (%) (%) (%) (%) (%)\nw/o TD 71.7 83.4 5.2 78.7 89.7 7.5 75.2\nw/ TD 73.1 86.6 1.8 79.7 91.5 5.3 76.4\nTable 5: Ablation studies of different components with input\nresolution 240p on DA VIS 2017 validation set. ‘TD’ denotes\nthe transformer decoder.\nfore, it’s essential to employ the transformer’s decoder.\nConclusions\nIn this paper, we propose a novel transformer-based pipeline,\ntermed TransVOS, for semi-supervised video object seg-\nmentation (VOS). Speciﬁcally, we employ the vision trans-\nformer to model the spatial and temporal relationships at the\nsame time among reference sets and query frame. Moreover,\nwe propose a two-path feature extractor to encode the refer-\nence sets and query frames, which dramatically slim the ex-\nisting VOS framework while keeping the performance. Our\nTransVOS achieves the top performance on several bench-\nmarks, which demonstrates its potential and effectiveness.\nReferences\nCaelles, S.; Maninis, K.-K.; Pont-Tuset, J.; Leal-Taix ´e, L.;\nCremers, D.; and Van Gool, L. 2017. One-shot video object\nsegmentation. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 221–230.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In European Conference on Computer\nVision, 213–229. Springer.\nChen, X.; Li, Z.; Yuan, Y .; Yu, G.; Shen, J.; and Qi, D. 2020.\nState-Aware Tracker for Real-Time Video Object Segmenta-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 9384–9393.\nCheng, M.-M.; Mitra, N. J.; Huang, X.; Torr, P. H.; and Hu,\nS.-M. 2014. Global contrast based salient region detection.\nIEEE transactions on pattern analysis and machine intelli-\ngence, 37(3): 569–582.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nDuke, B.; Ahmed, A.; Wolf, C.; Aarabi, P.; and Taylor,\nG. W. 2021. SSTVOS: Sparse Spatiotemporal Trans-\nformers for Video Object Segmentation. arXiv preprint\narXiv:2101.08833.\nEveringham, M.; Van Gool, L.; Williams, C. K.; Winn, J.;\nand Zisserman, A. 2010. The pascal visual object classes\n(voc) challenge. International journal of computer vision ,\n88(2): 303–338.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-\nual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 770–778.\nHu, Y .-T.; Huang, J.-B.; and Schwing, A. G. 2018. Video-\nmatch: Matching based video object segmentation. In Pro-\nceedings of the European conference on computer vision\n(ECCV), 54–70.\nHuang, X.; Xu, J.; Tai, Y .-W.; and Tang, C.-K. 2020. Fast\nVideo Object Segmentation With Temporal Aggregation\nNetwork and Dynamic Template Matching. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, 8879–8889.\nJohnander, J.; Danelljan, M.; Brissman, E.; Khan, F. S.; and\nFelsberg, M. 2019. A generative appearance model for\nend-to-end video object segmentation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 8953–8962.\nLai, Z.; Lu, E.; and Xie, W. 2020. MAST: A memory-\naugmented self-supervised tracker. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 6479–6488.\nLi, B.; Yan, J.; Wu, W.; Zhu, Z.; and Hu, X. 2018. High per-\nformance visual tracking with siamese region proposal net-\nwork. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, 8971–8980.\nLi, Y .; Hou, X.; Koch, C.; Rehg, J. M.; and Yuille, A. L.\n2014. The secrets of salient object segmentation. In Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, 280–287.\nLi, Y .; Shen, Z.; and Shan, Y . 2020. Fast Video Object Seg-\nmentation using the Global Context Module. In European\nConference on Computer Vision, 735–750. Springer.\nLi, Y .; Xu, N.; Peng, J.; See, J.; and Lin, W. 2020. Delving\ninto the Cyclic Mechanism in Semi-supervised Video Object\nSegmentation. arXiv preprint arXiv:2010.12176.\nLiang, Y .; Li, X.; Jafari, N.; and Chen, Q. 2020.\nVideo Object Segmentation with Adaptive Feature Bank\nand Uncertain-Region Reﬁnement. arXiv preprint\narXiv:2010.07958.\nLin, H.; Qi, X.; and Jia, J. 2019. Agss-vos: Attention guided\nsingle-shot video object segmentation. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, 3949–3957.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll ´ar, P.; and Zitnick, C. L. 2014. Microsoft\ncoco: Common objects in context. In European conference\non computer vision, 740–755. Springer.\nLu, X.; Wang, W.; Danelljan, M.; Zhou, T.; Shen, J.; and\nVan Gool, L. 2020. Video object segmentation with episodic\ngraph memory networks. arXiv preprint arXiv:2007.07020.\nLuiten, J.; V oigtlaender, P.; and Leibe, B. 2018. Premvos:\nProposal-generation, reﬁnement and merging for video ob-\nject segmentation. In Asian Conference on Computer Vision,\n565–580. Springer.\nMao, Y .; Wang, N.; Zhou, W.; and Li, H. 2021. Joint Induc-\ntive and Transductive Learning for Video Object Segmenta-\ntion. arXiv preprint arXiv:2108.03679.\nOh, S. W.; Lee, J.-Y .; Sunkavalli, K.; and Kim, S. J. 2018.\nFast video object segmentation by reference-guided mask\npropagation. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, 7376–7385.\nOh, S. W.; Lee, J.-Y .; Xu, N.; and Kim, S. J. 2019. Video\nobject segmentation using space-time memory networks. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 9226–9235.\nPerazzi, F.; Pont-Tuset, J.; McWilliams, B.; Van Gool, L.;\nGross, M.; and Sorkine-Hornung, A. 2016. A benchmark\ndataset and evaluation methodology for video object seg-\nmentation. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, 724–732.\nPont-Tuset, J.; Perazzi, F.; Caelles, S.; Arbel´aez, P.; Sorkine-\nHornung, A.; and Van Gool, L. 2017. The 2017 davis\nchallenge on video object segmentation. arXiv preprint\narXiv:1704.00675.\nRobinson, A.; Lawin, F. J.; Danelljan, M.; Khan, F. S.;\nand Felsberg, M. 2020. Learning fast and robust target\nmodels for video object segmentation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 7406–7415.\nSeong, H.; Hyun, J.; and Kim, E. 2020. Kernelized Mem-\nory Network for Video Object Segmentation. In European\nConference on Computer Vision, 629–645. Springer.\nSun, M.; Xiao, J.; Lim, E. G.; Zhang, B.; and Zhao, Y . 2020.\nFast template matching and update for video object tracking\nand segmentation. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition , 10791–\n10799.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is all you need. arXiv preprint arXiv:1706.03762.\nV oigtlaender, P.; Chai, Y .; Schroff, F.; Adam, H.; Leibe, B.;\nand Chen, L.-C. 2019. Feelvos: Fast end-to-end embedding\nlearning for video object segmentation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 9481–9490.\nV oigtlaender, P.; and Leibe, B. 2017. Online adaptation of\nconvolutional neural networks for video object segmenta-\ntion. arXiv preprint arXiv:1706.09364.\nV oigtlaender, P.; Luiten, J.; Torr, P. H.; and Leibe, B. 2020.\nSiam r-cnn: Visual tracking by re-detection. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, 6578–6588.\nWang, H.; Jiang, X.; Ren, H.; Hu, Y .; and Bai, S. 2021.\nSwiftNet: Real-time Video Object Segmentation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 1296–1305.\nWang, H.; Zhu, Y .; Adam, H.; Yuille, A.; and Chen, L.-C.\n2020. MaX-DeepLab: End-to-End Panoptic Segmentation\nwith Mask Transformers. arXiv preprint arXiv:2012.00759.\nWang, Q.; Zhang, L.; Bertinetto, L.; Hu, W.; and Torr, P. H.\n2019a. Fast online object tracking and segmentation: A uni-\nfying approach. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition , 1328–\n1338.\nWang, Z.; Xu, J.; Liu, L.; Zhu, F.; and Shao, L. 2019b.\nRanet: Ranking attention network for fast video object seg-\nmentation. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, 3978–3987.\nXie, H.; Yao, H.; Zhou, S.; Zhang, S.; and Sun, W. 2021.\nEfﬁcient Regional Memory Network for Video Object Seg-\nmentation. arXiv preprint arXiv:2103.12934.\nXu, N.; Yang, L.; Fan, Y .; Yang, J.; Yue, D.; Liang, Y .;\nPrice, B.; Cohen, S.; and Huang, T. 2018. Youtube-vos:\nSequence-to-sequence video object segmentation. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), 585–601.\nXu, Y .; Wang, Z.; Li, Z.; Yuan, Y .; and Yu, G. 2020.\nSiamfc++: Towards robust and accurate visual tracking with\ntarget estimation guidelines. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, 12549–12556.\nYan, B.; Peng, H.; Fu, J.; Wang, D.; and Lu, H. 2021. Learn-\ning Spatio-Temporal Transformer for Visual Tracking.arXiv\npreprint arXiv:2103.17154.\nYang, Z.; Wei, Y .; and Yang, Y . 2020. Collaborative video\nobject segmentation by foreground-background integration.\nIn European Conference on Computer Vision , 332–348.\nSpringer.\nAppendix\nDatasets and Evaluation Metrics We evaluate our ap-\nproach on DA VIS (Perazzi et al. 2016; Pont-Tuset et al.\n2017) and YouTube-VOS (Xu et al. 2018) benchmarks.\nBoth DA VIS2016 and DA VIS2017 have experimented.\nDA VIS2016 is an annotated single-object dataset contain-\ning 30 training video sequences and 20 validation video se-\nquences. DA VIS2017 is a multi-objects dataset expanded\nfrom DA VIS2016, including 60 training video sequences,\n30 validation video sequences, and 30 test video sequences.\nYouTube-VOS dataset is a large-scale dataset in VOS, hav-\ning 3471 training videos and 474 validation videos. And\neach video contains a maximum of 12 objects. The valida-\ntion set includes seen objects from 65 training categories and\nunseen objects from 26 categories, which is appropriate for\nevaluating algorithms’ generalization performance. We use\nthe evaluation metrics provided by the DA VIS benchmark to\nevaluate our model. J&F evaluates the general quality of\nthe segmentation results, J evaluates the mask IoU and F\nestimates the quality of contours.\nBackbone. We experiment with different backbones,\nResNet18 and ResNet50 (He et al. 2016). As shown in Table\n6, TransVOS with smaller backbone ResNet18 runs faster\n(7fps improvement) than ResNet50 while the performance\ndrops 4.1%J&F. Therefore, we compare our TransVOS\nwith ResNet50 as the backbone to other state-of-the-art\nmethods.\nBackbone JM JR JD FM FR FD J&F FPS(%) (%) (%) (%) (%) (%) (%)\nResNet18 68.8 80.6 7.7 75.9 86.3 9.1 72.3 24.0\nResNet50 73.1 86.6 1.8 79.7 91.5 5.3 76.4 17.0\nTable 6: Ablation studies of different backbone with input\nresolution 240p on DA VIS 2017 validation set.\nTraining strategy. We conduct experiments to explore the\neffectiveness of pre-trainng on synthetic videos. As Table 7\nshows, without pre-training, our model only drops by 1.5%\nin (J&F), which means our proposed TransVOS can learn\ngeneral and robust target object appearance even training\nwith small dataset.\nTraining strategyJM JR JD FM FR FD J&F\n(%) (%) (%) (%) (%) (%) (%)\nw/o pre-training 73.1 86.6 1.8 79.7 91.5 5.3 76.4\nw/ pre-training 74.4 85.6 6.8 81.4 91.3 8.1 77.9\nTable 7: Training data analysis on DA VIS 2017 validation\nset. We do abaltion studies to explore how the pre-training\naffects our model’s performance.\nInput resolution. We adjust the input resolution of the\nmodel as shown in Table 8, from which we can see that our\nmethod achieves better performance with a larger input size.\nTransVOS with half input resolution runs faster (11.8fps im-\nprovement) while the performance drops 4.0%J&F. There-\nfore, we compare our TransVOS with input resolution 480p\nto other state-of-the-art methods.\nInput JM JR JD FM FR FD J&F FPSresolution (%) (%) (%) (%) (%) (%) (%)\n240p 74.4 85.6 6.8 81.4 91.3 8.1 77.9 17.0\n480p 81.4 90.6 7 86.4 93.7 8.8 83.9 5.2\nTable 8: Input resolution analysis. We compared models\nwith different input resolution on DA VIS 2017 validation\nset.",
  "topic": "Computer vision",
  "concepts": [
    {
      "name": "Computer vision",
      "score": 0.5957543849945068
    },
    {
      "name": "Segmentation",
      "score": 0.5914402604103088
    },
    {
      "name": "Transformer",
      "score": 0.5709425806999207
    },
    {
      "name": "Computer science",
      "score": 0.5539195537567139
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5385310649871826
    },
    {
      "name": "Engineering",
      "score": 0.1268949806690216
    },
    {
      "name": "Electrical engineering",
      "score": 0.09740623831748962
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 14
}