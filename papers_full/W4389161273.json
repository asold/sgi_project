{
  "title": "Advancing ECG Biometrics Through Vision Transformers: A Confidence-Driven Approach",
  "url": "https://openalex.org/W4389161273",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4295185773",
      "name": "Onorato d’Angelis",
      "affiliations": [
        "Università Campus Bio-Medico"
      ]
    },
    {
      "id": "https://openalex.org/A3118079133",
      "name": "Luca Bacco",
      "affiliations": [
        "Università Campus Bio-Medico"
      ]
    },
    {
      "id": "https://openalex.org/A47413233",
      "name": "Luca Vollero",
      "affiliations": [
        "Università Campus Bio-Medico"
      ]
    },
    {
      "id": "https://openalex.org/A2016459725",
      "name": "Mario Merone",
      "affiliations": [
        "Università Campus Bio-Medico"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2170941890",
    "https://openalex.org/W1655694665",
    "https://openalex.org/W2007123640",
    "https://openalex.org/W4245920268",
    "https://openalex.org/W3112789885",
    "https://openalex.org/W2917641056",
    "https://openalex.org/W2964009128",
    "https://openalex.org/W4285174537",
    "https://openalex.org/W4280528560",
    "https://openalex.org/W6838948607",
    "https://openalex.org/W2907489514",
    "https://openalex.org/W2809242972",
    "https://openalex.org/W2521641300",
    "https://openalex.org/W2790931684",
    "https://openalex.org/W2916890875",
    "https://openalex.org/W4224993403",
    "https://openalex.org/W3037473497",
    "https://openalex.org/W2083535555",
    "https://openalex.org/W2031230980",
    "https://openalex.org/W3183965693",
    "https://openalex.org/W2049253729",
    "https://openalex.org/W3133447340",
    "https://openalex.org/W4225310655",
    "https://openalex.org/W3179185670",
    "https://openalex.org/W1980497570",
    "https://openalex.org/W4307268240",
    "https://openalex.org/W4378831061",
    "https://openalex.org/W4304013913",
    "https://openalex.org/W4225332066",
    "https://openalex.org/W4328054506",
    "https://openalex.org/W4224437668",
    "https://openalex.org/W2112209396",
    "https://openalex.org/W3083339326",
    "https://openalex.org/W2144458682",
    "https://openalex.org/W2941820175",
    "https://openalex.org/W2326461200",
    "https://openalex.org/W2547102660",
    "https://openalex.org/W2316540906",
    "https://openalex.org/W2763413948",
    "https://openalex.org/W3181534568",
    "https://openalex.org/W4318464409",
    "https://openalex.org/W2022656302",
    "https://openalex.org/W2759456125",
    "https://openalex.org/W2737966920",
    "https://openalex.org/W2614292327",
    "https://openalex.org/W3033523981",
    "https://openalex.org/W4284712117",
    "https://openalex.org/W2949402129",
    "https://openalex.org/W3046475869",
    "https://openalex.org/W4224918592",
    "https://openalex.org/W2811373165",
    "https://openalex.org/W2779797561",
    "https://openalex.org/W3044179938",
    "https://openalex.org/W6823596108",
    "https://openalex.org/W4320712860",
    "https://openalex.org/W2080547538",
    "https://openalex.org/W2401391822",
    "https://openalex.org/W112399678",
    "https://openalex.org/W6712297765",
    "https://openalex.org/W2041040282",
    "https://openalex.org/W4322629503",
    "https://openalex.org/W4386097571",
    "https://openalex.org/W4366452639",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4307090970",
    "https://openalex.org/W2063358975",
    "https://openalex.org/W2094717190",
    "https://openalex.org/W2160439670",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W6712262375",
    "https://openalex.org/W2061185532",
    "https://openalex.org/W2152702278",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4283722269",
    "https://openalex.org/W4241682883",
    "https://openalex.org/W2612690371"
  ],
  "abstract": "Over the past two decades, Electrocardiography (ECG) has gained significant momentum in the field of biometrics, offering a compelling alternative for person identity recognition based on physical/biological traits. Its inherent difficulty to be circumvent and its ability to enable liveness detection make it particularly appealing compared to other popular identifiers such as face, fingerprint, and iris. As a result, ECG has garnered attention from the computer vision community working on biometrics applications. We present a novel biometric method for personal recognition that leverages I-lead signals acquired off-the-person. Through fine-tuning a pre-trained Vision Transformer (ViT) model, we achieve remarkable results in recognizing individuals based on a single 2D image of their ECG recording obtained from as little as three heartbeats. Extensive evaluation on the CYBHi database, with enrollment and testing phases separated by a three-month time window, simulating a real-world, long-term identification scenario, demonstrates the robustness of our multiclass approach. Specifically, our system achieves a remarkable single sample-based identification accuracy of over 70&#x0025; with a pool of 63 individuals, along with an equal error rate of only 0.48&#x0025; in the 1-vs-1 authentication task. Additionally, we evaluated our approach on the very recent Heartprint database to assess the robustness of our approach with more subjects, larger separation time windows, and continuous training settings, obtaining again remarkable performance with respect to the state-of-the-art. While the promising capabilities of ECG-based biometrics are evident, given various security challenges, using such methods as standalone authentication could raise caution among users. To address this concern and enhance the system&#x2019;s dependability, we introduce a confidence-based rejection rule. Integrating this mechanism improves both identification and authentication performances, while it could also enable the system to detect out-of-database individuals.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.0322000\nAdvancing ECG Biometrics through Vision\nTransformers: A Confidence-Driven Approach\nOnorato d’Angelis1, (Fellow, IEEE), Luca Bacco1, Luca Vollero1, (Member, IEEE), Mario\nMerone1,*, (Member, IEEE)\n1Research Unit of Computer Systems and Bioinformatics, Department of Engineering, Università Campus Bio-Medico di Roma, Via Alvaro del Portillo, 21 -\n00128 Rome, Italy (e-mail: onorato.dangelis@unicampus.it, l.bacco@unicampus.it,l.vollero@unicampus.it, m.merone@unicampus.it)\n* Corresponding author: Mario Merone (e-mail: m.merone@unicampus.it).\nThe ﬁrst two authors contributed equally to this work.\nABSTRACT Over the past two decades, Electrocardiography (ECG) has gained signiﬁcant momentum\nin the ﬁeld of biometrics, offering a compelling alternative for person identity recognition based on\nphysical/biological traits. Its inherent difﬁculty to be circumvent and its ability to enable liveness detection\nmake it particularly appealing compared to other popular identiﬁers such as face, ﬁngerprint, and iris. As a\nresult, ECG has garnered attention from the computer vision community working on biometrics applications.\nWe present a novel biometric method for personal recognition that leverages I-lead signals acquired off-the-\nperson. Through ﬁne-tuning a pre-trained Vision Transformer (ViT) model, we achieve remarkable results\nin recognizing individuals based on a single 2D image of their ECG recording obtained from as little as\nthree heartbeats. Extensive evaluation on the CYBHi database, with enrollment and testing phases separated\nby a three-month time window, simulating a real-world, long-term identiﬁcation scenario, demonstrates the\nrobustness of our multiclass approach. Speciﬁcally, our system achieves a remarkable single sample-based\nidentiﬁcation accuracy of over 70% with a pool of 63 individuals, along with an equal error rate of only 0.48%\nin the 1-vs-1 authentication task. Additionally, we evaluated our approach on the very recent Heartprint\ndatabase to assess the robustness of our approach with more subjects, larger separation time windows, and\ncontinuous training settings, obtaining again remarkable performance with respect to the state-of-the-art.\nWhile the promising capabilities of ECG-based biometrics are evident, given various security challenges,\nusing such methods as standalone authentication could raise caution among users. To address this concern\nand enhance the system’s dependability, we introduce a conﬁdence-based rejection rule. Integrating this\nmechanism improves both identiﬁcation and authentication performances, while it could also enable the\nsystem to detect out-of-database individuals.\nINDEX TERMS Biometric Recognition, Identiﬁcation, Authentication, Electrocardiogram, Imposter De-\ntection, Vision Transformer\nI. INTRODUCTION\nElectrocardiography is a well-established medical technol-\nogy, extensively used for diagnose and monitor cardiovas-\ncular diseases. Since the early 2000s, electrocardiograms\n(ECGs), which depict the electrical activity of an individ-\nual’s heart, have been recognized as suitable candidates for\ncomputer-based biometrics systems (CBBSs) even in single-\nlead conﬁgurations [1]–[3].\nAlthough physiological and behavioral traits such as ﬁn-\ngerprints, iris, facial features, gait, and handwriting have\ngained popularity, \"hidden\" and dynamic biometrics like ECG\noffer signiﬁcant advantages for biometric system implemen-\ntation. ECGs meet the universality, uniqueness, and accept-\nability criteria for users, as outlined in various studies [4],\n[5], and provide additional unique beneﬁts. ECG tracings\nenable detection of liveness and are difﬁcult to circumvent,\nensuring robustness against presentation attacks greater than\nother biometric features [6]–[11]. Furthermore, ECGs may\nbe less computationally intensive to process due to their one-\ndimensional nature [12], and more straightforward to acquire\n(measurability/collectability).\nNonetheless, electrocardiography is not without limita-\ntions as a source of biometric information. For instance,\ncollectability is better achieved using one-lead off-the-person\napproaches, which employ dry electrodes to collect signals\nfrom users’ ﬁngertips instead of wet electrodes attached to\nVOLUME 11, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3338191\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nd’Angelis et al.: Preparation of Papers for IEEE ACCESS\nthe skin. This method is less invasive and costly [13], which\nmakes it more suitable for real-world applications [14]–[16].\nHowever, off-the-person recordings are more prone to noise\nand artifacts [13], [17], necessitating a more rigorous pre-\nprocessing stage for ﬁltering and signal cleaning.\nAnother challenge in electrocardiography is its intra-\nsubject variability, which may be inﬂuenced by an indi-\nvidual’s physical and mental state [18]. This is particularly\nevident when considering separate training and testing ses-\nsions [19], [20] and the time elapsed between these ses-\nsions [3], with more signiﬁcant degradation observed over\nlonger periods [21]–[23]. However, ECGs captured from the\nﬁngers are less affected by this issue [24], increasing the\ndemand for off-the-person frameworks that collect ﬁnger-\nbased signals.\nIn this study, we used off-the-person ECG recordings from\nthe CYBHi and Heartprint datasets [25], [26] to introduce a\nnovel system for both authentication (also known as veri-\nﬁcation) of individuals (1-vs-1) and identiﬁcation across a\ngroup of subjects (1-vs-N) while evaluating its resilience to\ntemporal variability. Refer to Section IV-A for an overview\nof the dataset and Section V for the experiments handled in\nthis work.\nGiven the nature of the more common identiﬁers, the Com-\nputer Vision (CV) community has traditionally been at the\nforefront of developing and advancing biometrics tasks. Fur-\nthermore, the landscape is rapidly evolving, and recent break-\nthroughs in the ﬁeld have introduced innovative approaches,\nincluding the revolutionary Vision Transformer (ViT) archi-\ntecture [27], which we employ as our core model. The ViT\nmodel has previously been utilized in various tasks associated\nwith ECG like the detection of cardiac arrhythmias [28], [29],\natrial ﬁbrillation [30], [31], and congestive heart failures [32].\nIn our approach, we ﬁne-tuned a pre-trained ViT model\n(see Section IV-C for more details) to recognize individuals\nbased on a single 2D image of their ECG recording, obtained\nby averaging only three consecutive heartbeats.\nDespite achieving remarkable performances in all the vari-\nous settings (see Section VI), as the other physical/biological\nsources, there is a general concern about the use of biometric\nsystems as standalone authentication methods, granting ac-\ncess to resources to mistakenly recognized users. To address\nthis aspect, we integrate a novel conﬁdence-based rule into\nour system to allow the system to reject doubt samples and\nimprove the reliability of the entire pipeline. To the best of\nour knowledge, this study is the ﬁrst to analyze model outputs\nto reduce false acceptance rates at the expense of rejecting\nmore samples. Furthermore, we investigate the use of such a\nmethod in identifying individuals outside the database.\nOur research thus presents the following notable practical\ncontributions to the ﬁeld of biometrics using electrocardiog-\nraphy and ViTs:\n• This is the ﬁrst study to investigate the application of\nViTs for biometric systems based on electrocardiogra-\nphy;\n• Our system surpasses state-of-the-art results for authen-\ntication on the long-term CYBHi dataset, demonstrating\nhigh reliability even after three months from the enroll-\nment phase;\n• We propose a novel approach for rejecting difﬁcult sam-\nples by analyzing the variance of predictions, resulting\nin a reduced false acceptance rate;\n• We investigate the use of the conﬁdence-based rejection\nmethod as an imposter detector , enabling the identi-\nﬁcation of out-of-database individuals, a concept not\npreviously explored.\nII. RELATED WORKS\nOver the past two decades, numerous studies have been con-\nducted to investigate and improve the feasibility of using\nelectrocardiograms for biometric purposes, speciﬁcally for\nauthentication and identiﬁcation. For authentication (also re-\nferred to as veriﬁcation), a typical approach involves com-\nparing the similarity of incoming sample patterns to individu-\nals’ templates or latent representations using a (dis)similarity\nmetric, such as Euclidean or Mahalanobis distances [33],\n[34]. Another employed technique is the Dynamic Time\nWarping [35], [36], which can be applied to unsynchronized\nsequences of varying lengths. However, this approach ne-\ncessitates storing user templates, posing potential security\nand privacy risks. In contrast, classiﬁer-based approaches\ncommonly used in identiﬁcation tasks use methods such as\nSupport Vector Machines [37], k-Nearest Neighbors [38], and\nRandom Forests [39].\nBiometric approaches can be broadly categorized into two\ntypes based on the type of features they employ: ﬁducial\nand non-ﬁducial. Fiducial features are based on the mor-\nphology of the signal and involve detecting speciﬁc points,\ni.e., the peaks of P, Q, R, S, and T waves. However, ﬁdu-\ncial features require extensive feature engineering [40]–[42].\nExamples of such features include amplitudes, peak ratios,\ntime intervals between the peaks, and distances. Furthermore,\nthey are less robust against noisy signals [12], [43]. On the\nother hand, non-ﬁducial features are obtained from signal\ntransformations, such as discrete cosine and wavelet-based\ntransforms [20], [44], [45], or from statistical features, such\nas autocorrelation [46]. Non-ﬁducial features offer greater\nrobustness as they do not rely on detecting characteristic\npoints, but instead make extensive use of higher SNR signal\nprojections. Moreover, with the rise of deep learning [12],\n[41], [47], [48], a popular approach is to use raw signals\nas input and allow the neural network to learn meaningful\nrepresentations across its layers.\nIn evaluating the effectiveness of our approach using the\nCYBHi dataset, most previous studies have focused on either\nintra-session experiments or term inter-session experiments.\nIntra-session experiments involve obtaining training and test\nsamples from a single session [49]–[51], while term inter-\nsession experiments involve enrollment and testing on data\nseparated by a brief period [52]. Being a completely differ-\nent setup from ours, we did not included works using term\n2 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3338191\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nd’Angelis et al.: Preparation of Papers for IEEE ACCESS\nsetups for the comparison analysis reported in Section VI.\nThese limitations hinder the evaluation of models in real-\nworld scenarios, where enrollment and testing sessions may\nbe separated by a signiﬁcant amount of time. Only a few\nstudies have investigated the long-term robustness of their\nsystems [22], [53]–[56], similar to our approach. However,\nnot all of them employed all the 63 subjects in the dataset.\nFor instance, Jyotishi et al. [55] excluded two individuals\nfrom their inter-sessions tests and Lourenço et al. [57], which\nemployed record from only 32 subjects. We excluded these\nworks from the comparison analysis as well. In addition, some\nworks have exploited the CYBHi dataset for assessing the\ndetection of ﬁducial points and outliers [58], [59].\nConcerning Heartprint, this dataset addressed the chal-\nlenges of authentication and identiﬁcation across both intra-\nand inter-session experiments, encompassing setups for both\nshort-term and long-term durations [26]. In a subsequent\nwork, Ammour et al. [62], as we did, converted signals into\n2D images. However, they employed spectrogram images to\ntrain their system via deep contrastive learning paradigm.\nThey evaluated their system for the identiﬁcation in short-\nterm intra- and inter-session experiments. The dataset was\nemployed under multimodal scenarios, too, fusing ECG\nrecordings with ﬁngerprints signals [63] to robustness to\nspooﬁng attacks: integrating ECG features with others de-\nrived by other kinds of signal is a recent trend in ECG\nbiometrics [64].\nRegarding the core model of our system, Vision Trans-\nformers (ViTs) [27] have gained interest after the Trans-\nformers architecture was introduced in the ﬁeld of Natu-\nral Language Processing [65] and later adapted to vision\nproblems. ViTs have outperformed traditional Convolutional\nNeural Networks (CNNs) by utilizing self-attention mecha-\nnisms, achieving state-of-the-art performance on various vi-\nsual tasks. In the ﬁeld of ECG data, ViTs have been shown\nto improve the classiﬁcation of tetanus severity [32] and con-\ngestive heart failure [66] when combined with other types of\nnetworks. However, their application to ECG-based biometric\ntasks has not been extensively explored. Recently, a partic-\nular version of ViTs, the Data Efﬁcient Image Transformer\n(DEIT), has been employed to extract features from ECGs\nand ﬁngerprints for biometrics [63].\nIII. MOTIVATION\nIn recent years, there has been a growing interest in the ﬁeld\nof ECG biometrics, driven by the quest for more robust and\nefﬁcient authentication and identiﬁcation systems. ECG bio-\nmetrics offers signiﬁcant advantages, especially when com-\npared to other common modalities such as ﬁngerprints, which\nare more vulnerable to circumvention techniques. However,\nharnessing the potential of ECGs for biometric applications\npresents its own set of challenges. Current ECG-based ap-\nproaches often require extensive feature engineering and\nstruggle with noise and variability over time. We saw the po-\ntential of ViTs to enhance ECG-based biometrics, by directly\nprocessing the raw signals as images. ViTs excel at capturing\nintricate patterns and relationships in ECG signals, offering\nan alternative to grid-like CNNs. While ViTs have shown\npromise in ECG recognition tasks, their application to bio-\nmetrics remains relatively unexplored. Thus, our motivation\nwas to explore ViTs’ applicability in biometrics, aiming to\nimprove system accuracy and reliability.\nAdditionally, we introduce a novel conﬁdence-based re-\njection rule to bolster system security and resilience against\nmalicious or erroneous access attempts.\nIV. MATERIALS AND METHODS\nWe propose a biometric system, illustrated in Figure 1. Due\nto the low signal-to-noise ratio of the dataset, we opted for\na non-ﬁducial approach, using raw segments converted into\nimages. However, our pipeline relies on a segmentation al-\ngorithm based on the detection of ﬁducial R peaks, catego-\nrizing our system as a hybrid approach. Indeed, R peaks are\neasily detectable even under very low signal-to-noise ratios.\nSpeciﬁcally, the pre-processing step involves signal ﬁltering,\nsegmentation, and conversion of three consecutive heartbeats\ninto images to be used as input for the model.\nFor the core model, we employed the Vision Transformers\narchitecture [27] and integrated a fully-connected classiﬁer\nlayer comprising 63 neurons (one for each class or subject’s\nidentity) with softmax activation to normalize its outputs\ninto a probability distribution. Additionally, we propose a\nnovel post-processing algorithm based on the variance of the\nmodel’s output to identify and, eventually, reject input heart-\nbeats for which the model shows a low conﬁdence, resulting\nin a more reliable system.\nA. DATASETS\n1) CYBHi\nIn this work, we utilized the CYBHi (Check Your Biosignals\nHere initiative) dataset [25] to evaluate the effectiveness of the\nproposed system for ECG biometrics. This dataset is publicly\navailable1 and was speciﬁcally collected for ECG biometrics\nresearch. The recordings were taken at a sampling frequency\nof 1 kHz with a resolution of 12 bit using dry Ag/AgCl elec-\ntrodes. None of the participants reported any health issues,\nallowing for the exclusion of pathological signals.\nWe chose to work with the CYBHi dataset, as widely\nconsidered the most suitable for testing biometric recognition\nsystems [13]. This choice is primarily due to the speciﬁc\nacquisition hardware and protocols. A crucial aspect of this\ndataset is its incorporation of two acquisition sessions spaced\nover time for each subject. This unique feature enables us\nto thoroughly evaluate the proposed system’s capabilities\nin handling challenges highlighted in the literature, such as\nthe low signal-to-noise ratio [67] resulting from the off-the-\nperson framework and its overall reliability over time.\nIn particular, we employed the long-term subset, which\nconsists of two-minute-long ECG signals recorded from 63\nparticipants (14 males and 49 females) aged between 18 and\n1https://zenodo.org/record/2381823#.ZBMiyHbMKUl\nVOLUME 11, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3338191\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nd’Angelis et al.: Preparation of Papers for IEEE ACCESS\nTABLE 1: Summary table of the literature related works. Following, the employed acronyms: kNN (k-Neirest Neighborhood),\nSVM (Support Vector Machine), CNN (Convolutional Neural Network), LSTM (Long Short-Term Memory), RNN (Recurrent\nNeural Network), CWT (Continous Wavelet Transform), WDM (Wavelet Distance Measure), STFT (Short-Time Fourier\nTransform), DEIT (Data Efﬁcient Image Transformer).\nData Work Task Session setup Features Model\nCYBHi\nLourenço et al. (2012) [57]Authentication / Identiﬁcation Short-term ﬁducial features kNN & SVM\nLourenço et al. (2012) [58] Fiducial points detection Short-term 1D raw signal multiple algorithms combination\nLourenço et al. (2013) [59] Outliers detection Short-term 1D raw signal DMEAN\nSantos et al. (2012) [60] Authentication Short-term PCA coefﬁcients kNN\nda Silva et al. (2013) [61] Authentication Inter- 1D raw signal SVM\nda Silva et al. (2017) [53] Authentication Inter- 1D raw signal + 2D spectrogram 1D & 2D CNNs + Fusion rule\nHammad et al. (2019) [52] Authentication Short-term Hashed ﬁducial features Feed-Forward NN\nHammad et al. (2019) [49] Authentication Intra- Fiducial features + 2 signal image 2D CNN\nJyotishi et al. (2020) [50] Identiﬁcation Intra- sequences of 1D raw signal LSTM\nBelo et al. (2020) [54] Authentication / Identiﬁcation Intra- & Inter- 1D raw signal Temporal CNN / RNN\nIbtehaz et al. (2021) [22] Identiﬁcation Inter- 1D raw signal EDITH\nJyotishi et al. (2021) [55] Authentication Intra- sequences of 1D raw signal Attention-based hierarchical LSTM\nZhu et al. (2022) [51] Authentication Intra- 1D raw signal + 2D spectrogram 1D & 2D SEResNets\nHeartprint\nIslam et al. [26] Authentication / Identiﬁcation All 1D raw signal / CWT WDM / VGGNet CNN\nAmmour et al. [62] Identiﬁcation Intra- & Inter- short-term 2D STFT image EfﬁcientNetB5\nAmmour et al. [63] Identiﬁcation Short-term 2D STFT image + ﬁngerprints DEIT + CNN\n24 years in two sessions three months apart, namely S1 and\nS2.\n2) Heartprint\nThe Heartprint database [26] is a recently published reposi-\ntory of ECG recordings featuring data from 199 individuals\n(comprising 130 males and 69 females) spanning ages 18 to\n68, with a predominant representation from South-Asian and\nArabian ethnic groups. These ECG signals were meticulously\ncaptured at the ﬁngertip level using the dry electrodes of the\nReadMyHeart device 2. Each recording (at least 2 for each\nsession and subject) was gathered for 15 seconds with a\nsampling frequency of 250 Hz.\nLike the CYBHi dataset, the Heartprint dataset is pub-\nlicly accessible 3 and encompasses multiple sessions that are\nspaced across various time intervals. In detail, this compre-\nhensive dataset is composed of four distinct sessions: ses-\nsions S1 and S2, comprising data from all 199 subjects, are\nseparated by a relatively short average interval of 47.5 days;\nsession S3R, involving 109 subjects, was recorded during\nactivities involving reading, with an average temporal sep-\naration of 1054.7 days from Session S1; and session S3L,\nfeaturing 78 subjects, exhibits a considerable time gap of\n1572.2 days when compared to Session S1. In this work, we\nemployed all the four sessions for our experiments.\nB. PRE-PROCESSING\nThe pre-processing phase is a crucial step in ensuring the\nrobustness of the biometric system, especially considering the\nlow signal-to-noise ratio of the targeted ECG signals.\nThe ﬁrst step in the pre-processing phase was to ﬁlter\nthe signals using a zero-phase Butterworth bandpass ﬁlter of\norder 4, with cutoff frequencies at 0.5 Hz and 50 Hz. This\nﬁltering approach helped to remove baseline wander artifacts,\n2https://dailycare.en.ec21.com/ReadMyHeart_-_Handheld_ECG_\nRecording--976239_976240.html\n3https://ﬁgshare.com/articles/dataset/Heartprint_A_Multisession_ECG_\nDataset_for_Biometric_Recognition/20105354/3\npower-line noise, and electromyographic interference that\ntypically occur at high frequencies [41], [68], [69].\nNext, the Hamilton algorithm [70] was used to identify the\nR peaks in the ECG signals, and, for each peak, a time window\nof [-200 ms, +400 ms] was considered to segment the single\nheartbeats. We also employed the DMEAN outlier removal\nalgorithm [59] to detect and remove noisy R peaks.\nTo increase the signal-to-noise ratio (SNR), we considered\nthree consecutive peaks with an overlap of 1 (to increase the\nnumber of samples in the training and the validation sets), we\nsummed the peaks and divided them by the mean. Finally, we\nextracted 2D images from the resulting samples.\nC. VISION TRANSFORMER (VIT)\nSince their advent in 2017 [65], Transformers have revolu-\ntionized the ﬁeld of Natural Language Processing (NLP).\nMore recently, their architecture has been proposed in CV\nproblems [27] to handle 2D images that are presented to\nthe ViT model as a sequence of patches. Such patches are\nembedded subdivision of ﬁxed-size of the input image. For\neach patch, its absolute position within the image is added to\nits representation.\nThese models are usually pre-trained to learn inner rep-\nresentations of the input data (i.e., texts in the NLP scope\nand images in the CV one). The pre-trained model, integrated\nwith a new classiﬁer layer, can be trained on a target task:\nthis step is generally called ﬁne-tuning. The advantage of this\ntwo-steps process is that, thanks to pre-training, the models\ncan be ﬁne-tuned on way smaller datasets achieving larger\nperformance that training the models from scratch, often\nsurpassing previous state-of-the-art results.\nWe thus exploited the ViT model presented by Dosovitskiy\net al. 4 [71], which they ﬁrst pre-trained with images at reso-\nlution 224 ×224 from ImageNet-21k5 and then ﬁne-tuned\non a multi-class classiﬁcation task with images at resolution\n4https://huggingface.co/google/vit-base-patch32-384\n5https://image-net.org/\n4 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3338191\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nd’Angelis et al.: Preparation of Papers for IEEE ACCESS\nFIGURE 1: Schematic overview of the proposed biometric system.\n384 ×384 from ImageNet 20126 [72]. To exploit this model\nin particular, we thus employed patches of resolution 32×32.\nFollowing the pre-training and then ﬁne-tuning paradigm,\nwe ﬁrst replaced the old classiﬁer layer to a newly initialized\none. Then, together with a newly initialized fully-connected\nlayer with softmax activation, we ﬁne-tuned the ViT model\nwith a cross-entropy loss to the target task, i.e., the classiﬁ-\ncation of the identity of the individual presenting in input to\nthe model their ECG-based 2D image. We trained the model\nfor 1000 epochs, incorporating an early stopping mechanism\nmonitoring the loss on the validation set, causing training to\nhalt if the loss fails to decrease for 10 consecutive epochs.\nFurthermore, we incorporated a learning rate scheduler dur-\ning training that gradually decreased the learning rate using\na cosine function. More precisely, the learning rate decayed\nover half a period, with a maximum number of iterations set\nto 10. The initial learning rate was ﬁxed at 2e-5 and the Adam\noptimizer [73] is used.\nD. CONFIDENCE-BASED REJECTION RULE\nDuring inference, before returning the identity classiﬁcation,\nthe entire system employs a threshold mechanism to act as a\nrejection rule to reject samples for which the model shows low\nconﬁdence. This module retrieves the probability distribution\ngiven in output by the classiﬁer layer (i.e., the values given by\nthe softmax activation), and computes its variance.\nThe variance within the softmax values computed for the\ni-th test sample is thus computed with the following equation\nσ2\ni =\n∑N\nj=1 |aj −µa|2\nN (1)\nwhere aj is the softmax activation value for j-th neuron of\nthe classiﬁcation layer, N is the number of neurons of the\nclassiﬁcation layer (i.e., the number of subjects), and µa is\nthe mean value of the softmax activations from the classiﬁer\nlayer.\nAs shown in Figure 2, when the model is particularly\nconﬁdent in its decision (i.e., it shows a high value for one\n6https://image-net.org/challenges/LSVRC/2012/\nsubject), the Gaussian bell results to be relatively large. On\nthe other hand, when the model is not that conﬁdent about\nits decision (i.e., it shows a number of mid-to-low values\nfor several subjects), it shows a narrower bell around the\nmean value, thus, a lower variance. Therefore, if the variance\nsurpasses a predetermined threshold, the system considers\nthe response reliable and thus returns the identity response.\nOtherwise, the model is considered not conﬁdent in providing\na classiﬁcation, which is then rejected. To determine the\nthreshold value, the variance of each softmax distribution\nderived from the correctly classiﬁed samples in the validation\nset. The threshold could thus be selected from the quantiles,\nrepresenting the amounts of validation samples rejected under\na certain threshold. The threshold value is thus a trade-off\nbetween the performances and the percentage of rejected\nsamples.\nV. EXPERIMENTS\nIn our study, we conducted a series of experiments to evaluate\nour system from several perspectives.\nA. CONFIGURATION ANALYSIS\n1) Filtering configuration\nSince different pre-processing pipelines can lead to substan-\ntially different results, we assessed our systems in two other\ndifferent ﬁltering conﬁgurations: (i) in an attempt to remove\nmore high-frequency noise from the electrocardiograms, we\nlowered the high cutoff frequency to 30 Hz from the band-\npass ﬁlter described in Section IV-B; (ii) following the em-\npirical considerations extracted from the heuristic analysis of\nthe signal [74], we employed a 300-order band pass Finite\nImpulse Response (FIR) ﬁlter with Hamming window and\ncutoff frequencies at 5 Hz and 20 Hz.\nIn particular, we compared these conﬁgurations following\nthe most realistic scenario, i.e., the inter-session setup. We\nemployed S1 (enrollment) as the training set and S2 (test) of\nthe CYBHi dataset to assess the performance.\nVOLUME 11, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3338191\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nd’Angelis et al.: Preparation of Papers for IEEE ACCESS\nFIGURE 2: Example of the conﬁdence-based rejection mechanism: by elaborating the ECG image, the system outputs the\nsoftmax values for each class (i.e., subject), represented by the bar plot on the center of the ﬁgure. Then, the mean and the\nstandard deviation (and the variance) are computed. The second graph reports the Gaussian bell derived from the softmax\ndistribution. Based on the value of the variance (i.e., the width of the bell), the system decides to either accept the decision (top)\nor reject it (bottom).\n2) Model configuration\nWe conducted a comparative analysis between two distinct\nmodel architectures: the Multi-Class and the Multi-Expert Vi-\nsion Transformer (MC-ViT and ME-ViT) models. The former\ninvolves a single ViT model with a sole on-top classiﬁer layer,\nfeaturing N neurons, where N corresponds to the number of\nsubjects in the dataset. This model is designed for identifying\nthe individual to whom an ECG sample belongs among the\nN subjects in the database. The identity is determined by\nthe subject-associated neuron ni with the highest conﬁdence,\ncalculated as follows:\nIdentity = argmax\ni∈{1,...,N}\n(Si) , Si = eni\nN∑\nj=1\nenj\n(2)\nwhere Si represents the softmax value of the i-th neuron.\nThe ME-ViT model, on the other hand, offers an alterna-\ntive approach, utilizing multiple ViTs, each specializing in\nrecognizing ECG samples from a speciﬁc subject, framed as\na binary classiﬁcation task. In essence, each individual ViT\nserves as an expert for a single subject, trained to classify\nwhether an ECG sample belongs to that particular subject\nor not. Consequently, the multi-expert system aggregates the\noutputs of these individual models, providing the identity\nassociated with the expert that exhibits the highest conﬁdence\nin positively classifying its respective subject as follows:\nIdentity = argmax\ni∈{1,...,N}\n(Si) , Si = en1\nen1 + en2\n, n1 >n2\n(3)\nwhere Si represents the softmax value of the i-th expert\nmodel, computed for the n1 neuron associated with the posi-\ntive classiﬁcation of the designated subject, where its proba-\nbility is higher than the negative neuron n2.\nOne notable advantage of the ME-ViT model is its practical\napplicability. In scenarios necessitating the addition of a new\nsubject to the system, the ME-ViT model only requires train-\ning a new expert dedicated to that speciﬁc subject. This stands\nin contrast to the MC-ViT model, which would mandate a\ncomplete retraining process for the entire system.\nBoth model conﬁgurations were trained on the CYBHi\nS2 dataset and subsequently assessed using the S1 dataset.\nWithin the ME-ViT framework, the training data for each\nexpert comprises not only the images of the chosen subject\nbut also a selection of randomly sampled images from other\nindividuals.\nB. AUTHENTICATION AND IDENTIFICATION\nTo assess the performance of our system in a more realis-\ntic scenario, we conducted inter-sessions experiments. Here,\ndata from one session (e.g., S1) was used in the enrollment\nphase to train the models, while data from the other session\n(e.g., S2) was used for testing. This allowed us to evaluate the\nperformance of our system when a time gap of three months\nexisted between the two sessions.\nHowever, to provide a comprehensive comparison with\nprevious literature, we also performed intra-session experi-\nments. These experiments involved randomly splitting data\nfrom the same single session (e.g., for CYBHi, either S1 or\n6 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3338191\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nd’Angelis et al.: Preparation of Papers for IEEE ACCESS\nS2) into two subsets for training/validation and testing pur-\nposes. We employed an 80-10-10 ﬁxed training-validation-\ntest split ratio for each subject.\nIn the authentication setup, we aimed to determine if a new\ninput sample (i.e., a new image) belongs to a speciﬁc subject\nby testing the ability of the model to discriminate whether\nthe model recognizes the individual or confuses them with\nsomeone else. To be consistent with previous literature, we\nevaluated the models using the Equal Error Rate (EER), a\ncommonly used metric in biometric applications, particularly\nfor veriﬁcation. The EER is determined as the point on the\nReceiver Operating Characteristic (ROC) curve at which the\nFalse Acceptance and Rejection Rates (FAR and FRR, re-\nspectively) are equal. The FAR represents the rate at which\nthe system incorrectly accepts an imposter, while the FRR\nindicates the rate at which the system incorrectly rejects an\nauthorized user. The EER serves as an overall performance\nmetric for biometric systems, indicating when the system is\nequally likely to accept an imposter as it is to reject a genuine\nuser. A lower EER indicates better performance.\nIn the identiﬁcation task, we evaluated the ability of our\nsystem to classify the identity of a new input sample among\nall 63 individuals in the database. To evaluate the system’s\nperformance, we used accuracy, a popular metric in the litera-\nture measuring the percentage of correctly classiﬁed samples\nout of the entire test set.\nFor both tasks, to have a fair comparison with the state of\nthe art, we assessed the performance of our system both with\nand without the proposed rejection rule.\nC. IMPOSTER DETECTION\nIn this setting, we assessed the overall system’s capability,\nwhich encompasses both the model and the rejection rule\npresented in Section IV-D, to discern individuals who are not\npart of the database. To accomplish this, we conducted the\nfollowing procedure: we randomly selected 13 subjects and\nexcluded them from the training set. Subsequently, we trained\nthe model using the remaining 50 subjects and assessed its\nperformance using data from the 13 excluded subjects. To\nmitigate any potential bias introduced by the random selec-\ntion process, we repeated this procedure ﬁve times and re-\nported the average performance. Performance was measured\nin terms of the detection rate, representing the percentage\nof data samples correctly identiﬁed by the machine learning\nmodel as belonging to the imposter class.\nWithin this framework, the imposter detection mechanism\noperates based on the conﬁdence exhibited by the model’s\noutput. Speciﬁcally, if the variance falls below a predeﬁned\nthreshold, the system categorizes the sample as not belonging\nto any of the subjects in the database. Conversely, if the\nvariance surpasses the threshold, the sample is considered\npart of the database. For establishing the threshold value, we\ncomputed the variance for each softmax distribution derived\nfrom correctly classiﬁed samples in the validation set . We\nthen utilized this list of variances to determine the threshold\nby calculating the quantile, which represents the proportion of\nvalidation samples rejected under that threshold. Our analysis\nalso involved assessing the system’s behavior under multiple\nthresholds.\nVI. RESULTS AND DISCUSSION\nA. CONFIGURATIONS ANALYSIS\n1) Filtering configuration\nRegarding the ﬁltering conﬁguration, Table 2 reports the\nperformance in terms of the identiﬁcation accuracy. As can\nbe noted, the system employing the ﬁltering conﬁguration\nwith the frequency band of 0.5-50 Hz resulted in the highest\naccuracy performance. It thus seems to indicate that consider-\ning frequency information above 30 Hz may contain relevant\ninformation for identifying individuals through ECG signals.\nTABLE 2: Results, in terms of identiﬁcation accuracy (%), obtained by\nour model in the three different ﬁltering conﬁgurations. The experiments\nwere conducted in an inter-session setup, training on S1 (enrollment) and\nevaluating the performance on S2 (test) of the CYBHi dataset.\nFiltering setting Accuracy (%)\nButter-worth (0.5 - 50 Hz) 68.00\nButter-worth (0.5 - 30 Hz) 60.00\nFinite Impulse Response (5 - 20 Hz) 58.00\n2) Model configuration\nRegarding the model conﬁguration, the multi-class model\noutperformed the multi-expert one, both for the authentica-\ntion task (0.48% vs 0.58% of EER) and the identiﬁcation one\n(70% vs 64% of accuracy).\nFurthermore, for the authentication task, in the case of the\nmulti-expert model, since the identity is known, there is also\nthe possibility to have the sample analyzed by the correspond-\ning expert only and then to evaluate only the response of the\nsingle expert. In this additional conﬁguration, we obtained an\naverage EER of 6.65% with a standard deviation of 8.97%.\nIt implies that analyzing the combination of all the experts is\nmore advantageous than using a single expert. In particular,\nwe attribute the relatively high EER to the fact that the single\nexperts tend more toward non-authentication of test samples,\nresulting in being more accurate in recognizing when an\nimage does not belong to their related subject (precision of\n99.8% ±27.8%) as opposed to the vice versa (36.5% ±\n24.6%).\nIn general, these results seem to suggest that exploiting\nthe knowledge acquired from all the subjects helps the ViT\nmodel.\nB. AUTHENTICATION AND IDENTIFICATION\n1) Results without the Rejection Rule\nTo perform a fair comparison with the literature, we analyzed\nthe results of only the MC-ViT model, i.e., without the re-\njection rule. Several works in the literature have presented\ntheir performance results using different datasets like the PTB\ndataset [75] and the FANTASIA dataset [76]. However, a key\nVOLUME 11, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3338191\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nd’Angelis et al.: Preparation of Papers for IEEE ACCESS\naspect worth noting is that these datasets typically consist of\nonly one recording session per subject. Recent works, includ-\ning ours, show high performance for intra-session recognition\ntasks, i.e., tasks where the data used for training and testing\ncome from the same recording. In addition, to provide a\ncomprehensive analysis, we include intra-session results in\nour study, but we primarily direct the reader’s attention to\ninter-session testing, as this is the main focus of our work.\nBy evaluating the performance of our model across recording\nsessions, we intend to assess its ability to generalize to new\ndata belonging to recording sessions other than the one used\nin training, which is critical for real-world applications.\nTable 3 provides an overview of the authentication results\nachieved by our model on the CYBHi dataset in comparison\nto prior research efforts. It is important to note that not all\nstudies conducted experiments across all four combinations\nof intra- and inter-session scenarios. Speciﬁcally, while Zhu\net al. [51] demonstrated commendable performance in the\nS1 vs S1 setup, they did not furnish results for the inter-\nsession settings, preventing us from directly comparing their\nsystem in those scenarios. However, for the inter-session\nsetup, our model yielded remarkable results, boasting an EER\nof 0.51% for S1 vs S2 and 0.48% for S2 vs S1, a substantial\nimprovement over previous results, notably surpassing the\noutcomes reported by da Silva et al. [53] and Jyotishi and\nDandapat [55]. Concerning intra-session performance, our\nmodel achieved results on par with the previous state-of-the-\nart work by Belo et al. [54], exhibiting minimal or no errors.\nHowever, it is crucial to highlight a signiﬁcant advantage of\nour system: it operates with just three heartbeats, whereas\ntheir approach requires over ten seconds of data acquisition.\nThis streamlined experimental setup aligns more closely with\npractical scenarios, where extended acquisition times may not\nbe desirable and could potentially hinder user acceptance of\nthe biometric device.\nTABLE 3: Results of our approach and comparison with the literature for\nthe authentication task on the CYBHi dataset. The scores are reported in\nterms of equal error rate in two different experimental setups: intra-session\n(i.e., S1 vs S1 and S2 vs S2) and inter-sessions (i.e., S1 vs S2 and S2 vs S1),\nin which the ﬁrst term refers to the training session and the second one refer\nto the testing session. We also report the number of heartbeats (hb) or seconds\n(s) employed.\nWork Equal Error Rate (%) Hb / sS1 vs S1 S2 vs S2 S1 vs S2 S2 vs S1\n(2013) [61] - - 9.10 9.37 5 hb\n(2017) [53] 1.33 - 12.78 13.93 1 hb\n(2019) [49] 4.47 (90:10) - - 1 hb\n(2020) [54] 0.2 0.0 2.2 4.1 >10 s\n(2021) [55] 3.98 - 13.43 12.05 2 s\n(2022) [51] 0.86 - - - 1.4 s\nMC-ViT 0.0 0.0 0.51 0.48 3 hb\nTable 4 provides the outcomes in the identiﬁcation task\non the CYBHi dataset. It is essential to acknowledge that\nnot all research works have reported results for all four ex-\nperimental setup combinations. Speciﬁcally, Jyotishi et al.\n[50] and Zhu et al. [51] limited their reporting to the S1 vs\nS1 scenario, while Ibtehaz et al. [22] exclusively presented\ninter-session results. Within the intra-session settings, our\nmodel achieved 99% of accuracy, outperforming, in general,\nprevious research endeavors. Only Belo et al. [54] reported\na higher accuracy. However, it is important to highlight that\ntheir system necessitates over ten seconds for the identiﬁca-\ntion process. For the inter-session tests, our system attained an\naccuracy of 68% in the S2 vs S1 scenario and 70% in the S2 vs\nS1 scenario. While these results are marginally below those\nreported by Ibtehaz et al. [22], comprehensive comparison\nbetween our systems cannot be drawn as they did not furnish\nperformance metrics in terms of EER.\nTABLE 4: Results of our approach and comparison with the literature for\nthe identiﬁcation task on the CYBHi dataset. The scores are reported in terms\nof identiﬁcation accuracy in two different experimental setup: intra-session\n(i.e., S1 vs S1 and S2 vs S2) and inter-sessions (i.e., S1 vs S2 and S2 vs S1),\nin which the ﬁrst term refer to the training session and the second one refer to\nthe testing session. We also report the number of heartbeats (hb) or seconds\n(s) employed.\nWork Identiﬁcation Accuracy (%) Hb / sS1 vs S1 S2 vs S2 S1 vs S2 S2 vs S1\n(2020) [50] 79.37 - - 2 s\n(2020) [54] 100 (50:50) 100 (50:50) 60.30 61.00 >10 s\n(2021) [22] - - 71.60 73.93 1 hb\n(2022) [51] 98.5 (90:10) - - - 1.4 s\nMC-ViT 99.00 (90:10) 99.00 (90:10) 68.00 70.00 3 hb\nFinally, in Table 5, we present the results of our MC-\nViT approach on the Heartprint dataset across three dis-\ntinct experimental setups. As for the CYBHi dataset, the\nﬁrst two sets of settings pertain to intra- and inter-sessions\nexperiments, encompassing all four available sessions. The\nthird setup involved training the model using two sessions\nthat were relatively close in time (S1 + S2) and testing it\non a third session that was considerably separated in time\n(either S3R or S3L). In conducting these experiments, we\nencountered the challenge of dealing with underrepresented\nsubjects, for whom the database provided signiﬁcantly fewer\nsamples compared to CYBHi. Given the well-known data-\nhungry nature of Vision Transformer (ViT) models, lacking\ntranslation equivariance and locality of the CNNs [27], we\nexcluded subjects with ten or fewer samples (after removing\noutliers), resulting in 103, 104, 102, and 74 subjects for S1,\nS2, S3R, and S3L, respectively. Although our results may\nnot be directly comparable to those presented in previous\nliterature, they shed light on several facets of our approach\nand the dataset.\nConsistent with the results obtained on CYBHi, the intra-\nsession experiments demonstrated performances on par with\nor even surpassing the state-of-the-art [26], [62], achieving\napproximately 98% identiﬁcation accuracy and an EER of\naround 5%. In the case of inter-sessions experiments, our\nsystem reported results that were less favorable than those\non CYBHi, but it exhibited signiﬁcantly improved EER com-\npared to the work of Islam et al. [26]. The EER values\ntransitioned from approximately 16% when testing on S1 and\nS2 to around 50% when testing on S3R and S3L, down to\napproximately 9% and 12%, respectively.\nIt is noteworthy that settings involving training on S2\n8 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3338191\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nd’Angelis et al.: Preparation of Papers for IEEE ACCESS\ngenerally outperformed those involving training on S1. This\ncan be attributed to the fact that S3R and S3L are closer in\ntime to S2 than to S1. However, it is essential to clarify that\nthis assumption may have been inﬂuenced by the selection\nof different subjects for S1 and S2. Yet, the hypothesis still\napplies to the third set of results, in which testing on S3R\n(closer in time to S1 and S2) yielded better outcomes than\ntesting on S3L. Interestingly, combining S1 and S2 for train-\ning resulted in improved performance compared to using only\nS1 but lagged behind using only S2. This seems to point more\nto the direction of possible problems with the signals of the\nﬁrst session. To conﬁrm that, training and testing on either S1\nor S2 produced worse outcomes than testing on either S3R or\nS3L.\nTABLE 5: Results of our approach for both the identiﬁcation and au-\nthentication tasks on the Heartprint dataset. The scores are reported in\nterms of identiﬁcation accuracy in three different experimental setups: intra-\nsession (top block), inter-sessions (middle block), and inter-sessions with\ncontinuous training (bottom block), in which the ﬁrst term(s) refer to the\ntraining session(s) and the second one refer to the testing session.\nSessions Id Accuracy (%) EER (%)\nS1 vs S1 98.01 0.00\nS2 vs S2 99.35 0.00\nS3R vs S3R 94.59 7.62\nS3L vs S3L 95.61 4.83\nS1 vs S2 49.02 9.03\nS2 vs S1 52.68 8.47\nS1 vs S3R 53.45 8.93\nS2 vs S3R 67.94 7.62\nS1 vs S3L 48.71 11.66\nS2 vs S3L 56.90 12.17\nS1 + S2 vs S3R 61.42 5.75\nS1 + S2 vs S3L 51.60 8.18\n2) Results with the Rejection Rule\nFigure 3 illustrates the impact that changing the threshold\n(based on the quantiles) for the rejection option has on our\nwhole system: it is evident that increasing the threshold en-\nhances the performance of the model at the cost of reducing\nthe number of classiﬁed samples. For the CYBHi dataset\n(S2 vs S1, Figure 3a), the maximum accuracy, at 97%, is\nachieved with a threshold of 0.90, but only 8% of the samples\nare classiﬁed. On the other hand, the lowest threshold at\n0.05 accepts 89.7% of the samples with an accuracy of 75%.\nTherefore, selecting the appropriate threshold depends on the\nperformance tolerance intended for the biometric system as\nwell as the tolerated rejection rate. In the case of the Heart-\nprint dataset (Figure 3b), we observed that the maximum\naccuracy, a perfect 100%, is achieved when using a relatively\nhigh threshold of 0.25. However, it is important to note that\nthis threshold results in the classiﬁcation of only 17% of the\nsamples. Additionally, the lowest threshold at 0.05 accepts\n48% of the samples with an accuracy of 93%.\nThis behavior can be attributed to the unique characteristics\nof the dataset. Speciﬁcally, due to the limited sample size in\nthe second session, the validation set comprises only a few\nimages for each subject. Consequently, the conﬁdence-based\nalgorithm tends to overﬁt on this small number of samples. In\nsuch scenarios, where additional samples from the recordings\nare not readily available, we recommend considering the use\nof tolerance at lower quantiles as a potential solution.\n(a) CYBHi (S2 vs S1)\n(b) Heartprint (S2 vs S3R)\nFIGURE 3: The plot displays the results obtained at varying rejection rule\ntolerance. The X-axis denotes the threshold quantile, the Y-axis on the left\nrepresents the percentage of the Accuracy (green line) for the identiﬁcation\ntask, and the percentage of the accepted samples (light blue bars), while\nthe Y-axis on the right represents the percentage of the EER metric for the\nauthentication task (red line).\nC. IMPOSTER DETECTION PERFORMANCE\nFigure 4 reports the average performance obtained in the 5-\nfold sessions for the imposter detection presented in Sec-\ntion V-C, and the related standard deviation, for each quantile\nthreshold. For the CYBHi dataset (S2 vs S1), the plot shows\nthat a favorable trade-off is achieved when the threshold is set\nto 0.5, which corresponds to the median value: in this case,\nthe imposter detection rate achieves 91.4%. By choosing a\nhigher threshold, the system achieves nearly 100% in terms\nof imposter detection rate.\nRegarding the Heartprint dataset, we employed the dis-\ncarded subjects (the ones with ≤10 samples in the training\nset) as imposters. For all the chosen quantile thresholds,\nthe system achieved a detection rate of 100%. Again, the\nalgorithm seems to overﬁt on the small-sized validation set.\nVOLUME 11, 2023 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3338191\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nd’Angelis et al.: Preparation of Papers for IEEE ACCESS\nClearly, the system is more restrictive and, therefore, more\nsecure, but, at the same time, it increases the false rejection\nrate. Once more, the choice of the threshold should be based\non a trade-off between being more restrictive or permissive.\nFIGURE 4: The plot displays the results: the X-axis denotes the threshold\nquantile, and the Y-axis represents the imposter detection rate over the\nquantile thresholds. For each quantile threshold, the error bar is also reported.\nVII. CONCLUSIONS\nTo our knowledge, this is the ﬁrst study investigating the\napplication of ViTs for biometric systems based on electro-\ncardiography.\nOur system outperforms state-of-the-art results for authen-\ntication on the long-term CYBHi dataset, demonstrating high\nreliability even after three months from the enrollment phase.\nWe propose a conﬁdence-based rejection method to reject\ndifﬁcult samples by analyzing the variance of predictions,\nresulting in a reduced false acceptance rate. Finally inves-\ntigate the use of the conﬁdence-based rejection method as\nan imposter detector, enabling the identiﬁcation of out-of-\ndatabase individuals, a concept not previously explored.\nAUTHORS CONTRIBUTIONS\nOnorato d’Angelis: Methodology, Software, Data Curation,\nInvestigation, Formal analysis, Writing - Original Draft; Luca\nBacco: Methodology, Software, Formal analysis, Visualiza-\ntion, Investigation, Validation, Writing - Original Draft; Luca\nVollero:Methodology, Supervision, Validation, Writing - Re-\nview & Editing; Mario Merone: Conceptualization, Method-\nology, Formal analysis, Writing - Original Draft, Supervision,\nProject administration.\nCONFLICT OF INTEREST/COMPETING INTERESTS\nThe authors declare that they have no known competing\nﬁnancial interests or personal relationships that could have\nappeared to inﬂuence the work reported in this paper\nDATA AVAILABILITY\nIn this work, we utilized the CYBHi (Check Your Biosig-\nnals Here initiative) and the Heartprint datasets [25], [26],\npublicly available on https://zenodo.org/record/2381823#\n.ZBMiyHbMKUl and https://ﬁgshare.com/articles/dataset/\nHeartprint_A_Multisession_ECG_Dataset_for_Biometric_\nRecognition/20105354/3, respectively.\nFUNDING\nThis research received no speciﬁc grant from any funding\nagency in the public, commercial, or not-for-proﬁt sectors.\nREFERENCES\n[1] Lena Biel, Ola Pettersson, Lennart Philipson, and Peter Wide. ECG\nanalysis: a new approach in human identiﬁcation. IEEE Transactions on\nInstrumentation and Measurement , 50(3):808–812, 2001.\n[2] Tsu-Wang Shen, WJ Tompkins, and YH Hu. One-lead ecg for identity\nveriﬁcation. In Proceedings of the second joint 24th annual conference and\nthe annual fall meeting of the biomedical engineering society][engineering\nin medicine and biology , volume 1, pages 62–63. IEEE, 2002.\n[3] Ikenna Odinaka, Po-Hsiang Lai, Alan D Kaplan, Joseph A O’Sullivan,\nErik J Sirevaag, and John W Rohrbaugh. Ecg biometric recognition: A\ncomparative analysis. IEEE Transactions on Information Forensics and\nSecurity, 7(6):1812–1824, 2012.\n[4] Egon L. van den Broek. Beyond biometrics. Procedia Computer Science ,\n1(1):2511–2519, 2010. ICCS 2010.\n[5] Aditya Singh Rathore, Zhengxiong Li, Weijin Zhu, Zhanpeng Jin, and\nWenyao Xu. A survey on heart biometrics. ACM Computing Surveys\n(CSUR), 53(6):1–38, 2020.\n[6] Ruben Tolosana, Marta Gomez-Barrero, Christoph Busch, and Javier\nOrtega-Garcia. Biometric presentation attack detection: Beyond the visible\nspectrum. IEEE Transactions on Information Forensics and Security ,\n15:1261–1275, 2019.\n[7] Adam Czajka and Kevin W Bowyer. Presentation attack detection for\niris recognition: An assessment of the state-of-the-art. ACM Computing\nSurveys (CSUR), 51(4):1–35, 2018.\n[8] Amitoj Bir Singh and Rajneesh Rani. Iris biometric presentation attack:\nTypes and detection techniques—a review. Soft Computing: Theories and\nApplications: Proceedings of SoCTA 2021 , pages 415–426, 2022.\n[9] S V olkova. Attacks on facial biometrics systems: an overview. Computer\nApplications for Management and Sustainable Development of Production\nand Industry (CMSD2021) , 12251:20–25, 2022.\n[10] Chuanfu Shen, Shiqi Yu, Jilong Wang, George Q Huang, and Liang Wang.\nA comprehensive survey on deep gait recognition: algorithms, datasets and\nchallenges. arXiv preprint arXiv:2206.13732 , 2022.\n[11] Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez, and Javier Ortega-\nGarcia. Presentation attacks in signature biometrics: types and introduction\nto attack detection. Handbook of Biometric Anti-Spooﬁng: Presentation\nAttack Detection, pages 439–453, 2019.\n[12] João Ribeiro Pinto, Jaime S Cardoso, and André Lourenço. Evolution,\ncurrent challenges, and future possibilities in ECG biometrics. IEEE\nAccess, 6:34746–34776, 2018.\n[13] Mario Merone, Paolo Soda, Mario Sansone, and Carlo Sansone. ECG\ndatabases for biometric systems: A systematic review. Expert Systems with\nApplications, 67:189–202, 2017.\n[14] Agam Bansal and Rajnish Joshi. Portable out-of-hospital electrocardiogra-\nphy: A review of current technologies. Journal of arrhythmia , 34(2):129–\n138, 2018.\n[15] Haakon Tillmann Haverkamp, Stig Ove Fosse, and Peter Schuster. Ac-\ncuracy and usability of single-lead ecg from smartphones-a clinical study.\nIndian pacing and electrophysiology journal , 19(4):145–149, 2019.\n[16] Elena Merdjanovska and Aleksandra Rashkovska. Comprehensive survey\nof computational ecg analysis: Databases, methods and applications. Ex-\npert Systems with Applications , page 117206, 2022.\n[17] Mohit Ingale, Renato Cordeiro, Siddartha Thentu, Younghee Park, and\nNima Karimian. Ecg biometric authentication: A comparative analysis.\nIEEE Access, 8:117853–117866, 2020.\n[18] Shahrzad Pouryayevali, Saeid Wahabi, Siddarth Hari, and Dimitrios Hatz-\ninakos. On establishing evaluation standards for ecg biometrics. In\n2014 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 3774–3778. IEEE, 2014.\n[19] Saeid Wahabi, Shahrzad Pouryayevali, Siddarth Hari, and Dimitrios Hatz-\ninakos. On evaluating ecg biometric systems: Session-dependence and\n10 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3338191\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nd’Angelis et al.: Preparation of Papers for IEEE ACCESS\nbody posture. IEEE Transactions on Information Forensics and Security ,\n9(11):2002–2013, 2014.\n[20] Riccardo Sorvillo, Luca Bacco, Mario Merone, Alessandro Zompanti,\nMarco Santonico, Giorgio Pennazza, and Giulio Iannello. Single beat ecg-\nbased identiﬁcation system: development and robustness test in different\nworking conditions. In 2021 IEEE International Workshop on Metrology\nfor Industry 4.0 & IoT (MetroInd4.0&IoT) , pages 538–543, 2021.\n[21] Fabienne Porée, Gaëlle Kervio, and Guy Carrault. Ecg biometric analysis\nin different physiological recording conditions. Signal, image and video\nprocessing, 10(2):267–276, 2016.\n[22] Nabil Ibtehaz, Muhammad EH Chowdhury, Amith Khandakar, Serkan Ki-\nranyaz, M Sohel Rahman, Anas Tahir, Yazan Qiblawey, and Tawsifur Rah-\nman. Edith: Ecg biometrics aided by deep learning for reliable individual\nauthentication. IEEE Transactions on Emerging Topics in Computational\nIntelligence, 6(4):928–940, 2021.\n[23] Kai Jye Chee and Dzati Athiar Ramli. Electrocardiogram biometrics using\ntransformer’s self-attention mechanism for sequence pair feature extractor\nand ﬂexible enrollment scope identiﬁcation. Sensors, 22(9):3446, 2022.\n[24] Mariana S Ramos, João M Carvalho, Armando J Pinho, and Susana Brás.\nOn the impact of the data acquisition protocol on ecg biometric identiﬁca-\ntion. Sensors, 21(14):4645, 2021.\n[25] Hugo Plácido Da Silva, André Lourenço, Ana Fred, Nuno Raposo, and\nMarta Aires-de Sousa. Check your biosignals here: A new dataset\nfor off-the-person ecg biometrics. Computer methods and programs in\nbiomedicine, 113(2):503–514, 2014.\n[26] Md Saiful Islam, Haikel Alhichri, Yakoub Bazi, Nassim Ammour, Naif\nAlajlan, and Rami M. Jomaa. Heartprint: A dataset of multisession ecg\nsignal with long interval captured from ﬁngers for biometric recognition.\nData, 7(10), 2022.\n[27] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weis-\nsenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil\nHoulsby. An image is worth 16x16 words: Transformers for image\nrecognition at scale. CoRR, abs/2010.11929, 2020.\n[28] Yanfang Dong, Miao Zhang, Lishen Qiu, Lirong Wang, and Yong Yu.\nAn arrhythmia classiﬁcation model based on vision transformer with de-\nformable attention. Micromachines, 14(6):1155, 2023.\n[29] Pingping Bing, Yang Liu, Wei Liu, Jun Zhou, and Lemei Zhu. Electrocar-\ndiogram classiﬁcation using tsst-based spectrogram and convit. Frontiers\nin Cardiovascular Medicine , 9:983543, 2022.\n[30] Sawon Pratiher, Apoorva Srivastava, Yedla Bindu Priyatha, Nirmalya\nGhosh, and Amit Patra. A dilated residual vision transformer for atrial\nﬁbrillation detection from stacked time-frequency ecg representations. In\nICASSP 2022-2022 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) , pages 1121–1125. IEEE, 2022.\n[31] Haarika Manda, Shaswati Dash, and Rajesh Kumar Tripathy. Time-\nfrequency domain modiﬁed vision transformer model for detection of atrial\nﬁbrillation using multi-lead ecg signals. In 2023 National Conference on\nCommunications (NCC), pages 1–5. IEEE, 2023.\n[32] Taotao Liu, Yujuan Si, Weiyi Yang, Jiaqi Huang, Yongheng Yu, Gengbo\nZhang, and Rongrong Zhou. Inter-patient congestive heart failure detection\nusing ecg-convolution-vision transformer network. Sensors, 22(9):3283,\n2022.\n[33] Yogendra Narain Singh and Sanjay Kumar Singh. Evaluation of electro-\ncardiogram for biometric authentication. Journal of Information Security ,\n2011.\n[34] Joao Ribeiro Pinto and Jaime S Cardoso. An end-to-end convolutional\nneural network for ecg-based biometric authentication. In 2019 IEEE 10th\nInternational Conference on Biometrics Theory, Applications and Systems\n(BTAS), pages 1–8. IEEE, 2019.\n[35] N Venkatesh and Srinivasan Jayaraman. Human electrocardiogram for\nbiometrics using dtw and ﬂda. In 2010 20th International Conference on\nPattern Recognition, pages 3838–3841. IEEE, 2010.\n[36] Yifan Chu, Haibin Shen, and Kejie Huang. Ecg authentication method\nbased on parallel multi-scale one-dimensional residual network with center\nand margin loss. IEEE Access, 7:51598–51607, 2019.\n[37] Hyun-Soo Choi, Byunghan Lee, and Sungroh Yoon. Biometric authenti-\ncation using noisy electrocardiograms acquired by mobile sensors. IEEE\naccess, 4:1266–1273, 2016.\n[38] Muhammad Najam Dar, M Usman Akram, Anam Usman, and Shoab A\nKhan. Ecg biometric identiﬁcation for general population using mul-\ntiresolution analysis of dwt based features. In 2015 Second International\nConference on Information Security and Cyber Forensics (InfoSec) , pages\n5–10. IEEE, 2015.\n[39] Noureddine Belgacem, A Nait-Ali, R Fournier, and Fethi Bereksi-Reguig.\nEcg based human identiﬁcation using random forests. In Proc. Int. Conf.\nE-Technol. Bus. Web (EBW) , 2013.\n[40] Qingxue Zhang and Dian Zhou. Deep arm/ear-ecg image learning for\nhighly wearable biometric human identiﬁcation. Annals of biomedical\nengineering, 46:122–134, 2018.\n[41] Anthony Ngozichukwuka Uwaechia and Dzati Athiar Ramli. A com-\nprehensive survey on ecg signals as new biometric modality for human\nauthentication: Recent advances and future challenges. IEEE Access ,\n9:97760–97802, 2021.\n[42] Teresa MC Pereira, Raquel C Conceição, Vitor Sencadas, and Raquel Se-\nbastião. Biometric recognition: A systematic review on electrocardiogram\ndata acquisition methods. Sensors, 23(3):1507, 2023.\n[43] John M Irvine, Steven A Israel, W Todd Scruggs, and William J Worek.\neigenpulse: Robust human identiﬁcation from cardiovascular function.\nPattern Recognition, 41(11):3427–3435, 2008.\n[44] João Ribeiro Pinto, Jaime S Cardoso, André Lourenço, and Carlos Car-\nreiras. Towards a continuous biometric system based on ecg signals\nacquired on the steering wheel. Sensors, 17(10):2228, 2017.\n[45] Emna Kalai Zaghouani, Adel Benzina, and Rabah Attia. Ecg based\nauthentication for e-healthcare systems: Towards a secured ecg features\ntransmission. In 2017 13th international wireless communications and\nmobile computing conference (IWCMC) , pages 1777–1783. IEEE, 2017.\n[46] Majid Komeili, Wael Louis, Narges Armanfard, and Dimitrios Hatzinakos.\nFeature selection for nonstationary data: Application to human recognition\nusing medical biometrics. IEEE transactions on cybernetics , 48(5):1446–\n1459, 2017.\n[47] Shenda Hong, Yuxi Zhou, Junyuan Shang, Cao Xiao, and Jimeng Sun.\nOpportunities and challenges of deep learning methods for electrocardio-\ngram data: A systematic review. Computers in biology and medicine ,\n122:103801, 2020.\n[48] Nehemiah Musa, Abdulsalam Ya’u Gital, Nahla Aljojo, Haruna Chiroma,\nKayode S Adewole, Hammed A Mojeed, Nasir Faruk, Abubakar Abdulka-\nrim, Ifada Emmanuel, Yusuf Y Folawiyo, et al. A systematic review and\nmeta-data analysis on the applications of deep learning in electrocardio-\ngram. Journal of ambient intelligence and humanized computing , pages\n1–74, 2022.\n[49] Mohamed Hammad, Shanzhuo Zhang, and Kuanquan Wang. A novel two-\ndimensional ecg feature extraction and classiﬁcation algorithm based on\nconvolution neural network for human authentication. Future Generation\nComputer Systems, 101:180–196, 2019.\n[50] Debasish Jyotishi and Samarendra Dandapat. An lstm-based model for\nperson identiﬁcation using ecg signal. IEEE Sensors Letters , 4(8):1–4,\n2020.\n[51] Guiping Zhu, Mingzhu Ma, Yuwen Huang, Kuikui Wang, and Gongping\nYang. Dual-domain low-rank fusion deep metric learning for off-the-\nperson ecg biometrics. In ICASSP 2022-2022 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing (ICASSP) , pages 2914–\n2918. IEEE, 2022.\n[52] Mohamed Hammad, Gongning Luo, and Kuanquan Wang. Cancelable\nbiometric authentication system based on ecg. Multimedia Tools and\nApplications, 78:1857–1887, 2019.\n[53] Eduardo Jose da Silva Luz, Gladston JP Moreira, Luiz S Oliveira,\nWilliam Robson Schwartz, and David Menotti. Learning deep off-the-\nperson heart biometrics representations. IEEE Transactions on Information\nForensics and Security, 13(5):1258–1270, 2017.\n[54] David Belo, Nuno Bento, Hugo Silva, Ana Fred, and Hugo Gamboa. Ecg\nbiometrics using deep learning and relative score threshold classiﬁcation.\nSensors, 20(15):4078, 2020.\n[55] Debasish Jyotishi and Samarendra Dandapat. An attention based hierar-\nchical lstm architecture for ecg biometric system. preprint, 2021.\n[56] Pietro Melzi, Ruben Tolosana, and Ruben Vera-Rodriguez. Ecg biometric\nrecognition: Review, system proposal, and benchmark evaluation. IEEE\nAccess, 2023.\n[57] André Lourenço, Hugo Silva, and Ana Fred. Ecg-based biometrics: A real\ntime classiﬁcation approach. In 2012 IEEE International Workshop on\nMachine Learning for Signal Processing , pages 1–6. IEEE, 2012.\n[58] André Lourenço, Hugo Silva, Paulo Leite, Renato Lourenço, and Ana LN\nFred. Real time electrocardiogram segmentation for ﬁnger based ecg\nbiometrics. In Biosignals, pages 49–54, 2012.\n[59] André Lourenço, Hugo Silva, Carlos Carreiras, and And Fred. Out-\nlier detection in non-intrusive ecg biometric system. In Image Analy-\nsis and Recognition: 10th International Conference, ICIAR 2013, Póvoa\nVOLUME 11, 2023 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3338191\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nd’Angelis et al.: Preparation of Papers for IEEE ACCESS\ndo Varzim, Portugal, June 26-28, 2013. Proceedings 10 , pages 43–52.\nSpringer, 2013.\n[60] Marta S Santos, Ana LN Fred, Hugo Silva, and André Lourenço. Eigen\nheartbeats for user identiﬁcation. In BIOSIGNALS, pages 351–355, 2013.\n[61] Hugo Plácido da Silva, Ana Fred, André Lourenço, and Anil K. Jain. Finger\necg signal for user authentication: Usability and performance. In 2013\nIEEE Sixth International Conference on Biometrics: Theory, Applications\nand Systems (BTAS) , pages 1–8, 2013.\n[62] Nassim Ammour, Rami M Jomaa, Md Saiful Islam, Yakoub Bazi, Haikel\nAlhichri, and Naif Alajlan. Deep contrastive learning-based model for ecg\nbiometrics. Applied Sciences, 13(5):3070, 2023.\n[63] Nassim Ammour, Yakoub Bazi, and Naif Alajlan. Multimodal approach for\nenhancing biometric authentication. Journal of Imaging , 9(9):168, 2023.\n[64] Emanuele Maiorana, Chiara Romano, Emiliano Schena, and Carlo Mas-\nsaroni. Biowish: Biometric recognition using wearable inertial sensors\ndetecting heart activity. IEEE Transactions on Dependable and Secure\nComputing, pages 1–14, 2023.\n[65] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention\nis all you need. In I. Guyon, U. V on Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems, volume 30. Curran Associates, Inc., 2017.\n[66] Ping Lu, Chenyang Wang, Jannis Hagenah, Shadi Ghiasi, Tingting Zhu,\nLouise Thwaites, David A Clifton, et al. Improving classiﬁcation of tetanus\nseverity for patients in low-middle income countries wearing ecg sensors\nby using a cnn-transformer network. IEEE Transactions on Biomedical\nEngineering, 2022.\n[67] Pietro Melzi, Ruben Tolosana, and Ruben Vera-Rodriguez. Ecg biometric\nrecognition: Review, system proposal, and benchmark evaluation. IEEE\nAccess, 11:15555–15566, 2023.\n[68] S Zahra Fatemian and Dimitrios Hatzinakos. A new ecg feature extractor\nfor biometric recognition. In 2009 16th international conference on digital\nsignal processing, pages 1–6. IEEE, 2009.\n[69] Balwinder Singh, Preeti Singh, and Sumit Budhiraja. Various approaches to\nminimise noises in ecg signal: A survey. In 2015 Fifth International Con-\nference on Advanced Computing & Communication Technologies , pages\n131–137. IEEE, 2015.\n[70] Pat Hamilton. Open source ecg analysis. In Computers in cardiology, pages\n101–104. IEEE, 2002.\n[71] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weis-\nsenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil\nHoulsby. An image is worth 16x16 words: Transformers for image\nrecognition at scale, 2020.\n[72] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-\nFei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE\nconference on computer vision and pattern recognition , pages 248–255.\nIeee, 2009.\n[73] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980 , 2014.\n[74] Filipe Canento, André Lourenço, Hugo Silva, and Ana LN Fred. On\nreal time ecg segmentation algorithms for biometric applications. In\nBIOSIGNALS, pages 228–235, 2013.\n[75] Ralf Bousseljot, Dieter Kreiseler, and Allard Schnabel. Nutzung der ekg-\nsignaldatenbank cardiodat der ptb über das internet. 1995.\n[76] Nikhil Iyengar, CK Peng, Raymond Morin, Ary L Goldberger, and Lewis A\nLipsitz. Age-related alterations in the fractal scaling of cardiac interbeat in-\nterval dynamics. American Journal of Physiology-Regulatory, Integrative\nand Comparative Physiology , 271(4):R1078–R1084, 1996.\nONORATO D’ANGELISgraduated in Biomedical\nEngineering at University Campus Bio-Medico di\nRoma in 2021. He is currently a PhD student in\nInformation Technology in Biomedicine at Uni-\nversity Campus Bio-Medico di Roma. His cur-\nrent research interests include machine learning,\ndeep learning, time-series analysis, computer vi-\nsion, and the application of artiﬁcial intelligence\nto medicine.\nLUCA BACCO, PH.D.,is a Biomedical Engineer\nand received the Ph.D. in Information Technology\nfor Biomedicine at the University Campus Bio-\nMedico di Roma in 2023. His expertise and cur-\nrent research fall into the Artiﬁcial Intelligence\nﬁeld, focusing on Natural Language Processing\nand Machine/Deep Learning for applications in\nhealthcare.\nLUCA VOLLERO, PH.D.,is an Associate Profes-\nsor of Computer Science at the University Campus\nBio-Medico di Roma. His research activities fall in\nthe areas of signal processing and digital imaging,\nAI, distributed systems, and embedded systems op-\ntimization. Luca V ollero is a member of the IEEE,\nIEEE computer society, IEEE communication so-\nciety, ACM, and SIAM; he is co-founder of the\ninnovative startup Heremos, operating in the ﬁeld\nof remote monitoring of frail persons and patients.\nMARIO MERONE, PH.D., is an Assistant Pro-\nfessor at the University Campus Bio-Medico di\nRoma. He received the Ph.D. in Bioengineering\nand Bioscience in 2017. He has extensive experi-\nence in the ﬁeld of artiﬁcial intelligence (Machine\nLearning and Deep Learning for Time-series Anal-\nysis). His research has led to 40 publications, all\nfocused on AI for healthcare. He is currently the\nscientiﬁc leader of several projects involving the\napplication of artiﬁcial intelligence in the medical\nﬁeld. He is a founding partner of an innovative start-up called BPCOMEDIA\nS.R.L., focused on technology transfer of research results. Mario Merone is\na member of the IEEE, IEEE Computer Society, IEEE Computational Life\nSciences and IEEE Sensors Council.\n12 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3338191\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "topic": "Biometrics",
  "concepts": [
    {
      "name": "Biometrics",
      "score": 0.8501132726669312
    },
    {
      "name": "Computer science",
      "score": 0.8187326788902283
    },
    {
      "name": "Liveness",
      "score": 0.7555193305015564
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.6717862486839294
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6413818001747131
    },
    {
      "name": "Fingerprint recognition",
      "score": 0.5368531346321106
    },
    {
      "name": "Machine learning",
      "score": 0.4642871618270874
    },
    {
      "name": "Iris recognition",
      "score": 0.45706313848495483
    },
    {
      "name": "Identifier",
      "score": 0.4446900486946106
    },
    {
      "name": "Facial recognition system",
      "score": 0.41191136837005615
    },
    {
      "name": "Computer vision",
      "score": 0.38840529322624207
    },
    {
      "name": "Feature extraction",
      "score": 0.36347368359565735
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3430442214012146
    },
    {
      "name": "Fingerprint (computing)",
      "score": 0.3404230773448944
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I155125353",
      "name": "Università Campus Bio-Medico",
      "country": "IT"
    }
  ],
  "cited_by": 12
}