{
  "title": "UP-DETR: Unsupervised Pre-training for Object Detection with Transformers",
  "url": "https://openalex.org/W3103465009",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2104341570",
      "name": "Dai Zhigang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2748820552",
      "name": "Cai Bolun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2351057703",
      "name": "Lin Yu-geng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A60215484",
      "name": "Chen Jun-ying",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W607748843",
    "https://openalex.org/W3035396860",
    "https://openalex.org/W2982770724",
    "https://openalex.org/W2886335102",
    "https://openalex.org/W2555182955",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2965182628",
    "https://openalex.org/W2979579363",
    "https://openalex.org/W3095121901",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2929862222",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3035694605",
    "https://openalex.org/W2995181141",
    "https://openalex.org/W3021734639",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2964093967",
    "https://openalex.org/W2935837427",
    "https://openalex.org/W3106005682",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2470394683",
    "https://openalex.org/W2981495453",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W3035473155",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W2138621090",
    "https://openalex.org/W2990139668",
    "https://openalex.org/W3106250896",
    "https://openalex.org/W3101821705",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W3009561768",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2999219213",
    "https://openalex.org/W2798991696",
    "https://openalex.org/W2883725317",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2031489346",
    "https://openalex.org/W2991391304",
    "https://openalex.org/W2799058067"
  ],
  "abstract": "DEtection TRansformer (DETR) for object detection reaches competitive performance compared with Faster R-CNN via a transformer encoder-decoder architecture. However, trained with scratch transformers, DETR needs large-scale training data and an extreme long training schedule even on COCO dataset. Inspired by the great success of pre-training transformers in natural language processing, we propose a novel pretext task named random query patch detection in Unsupervised Pre-training DETR (UP-DETR). Specifically, we randomly crop patches from the given image and then feed them as queries to the decoder. The model is pre-trained to detect these query patches from the input image. During the pre-training, we address two critical issues: multi-task learning and multi-query localization. (1) To trade off classification and localization preferences in the pretext task, we find that freezing the CNN backbone is the prerequisite for the success of pre-training transformers. (2) To perform multi-query localization, we develop UP-DETR with multi-query patch detection with attention mask. Besides, UP-DETR also provides a unified perspective for fine-tuning object detection and one-shot detection tasks. In our experiments, UP-DETR significantly boosts the performance of DETR with faster convergence and higher average precision on object detection, one-shot detection and panoptic segmentation. Code and pre-training models: https://github.com/dddzg/up-detr.",
  "full_text": "1\nUnsupervised Pre-training for Detection\nTransformers\nZhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen,Senior Member, IEEE\nAbstractâ€”DEtection TRansformer (DETR) for object detection reaches competitive performance compared with Faster R-CNN via a\ntransformer encoder-decoder architecture. However, trained with scratch transformers, DETR needs large-scale training data and an\nextreme long training schedule even on COCO dataset. Inspired by the great success of pre-training transformers in natural language\nprocessing, we propose a novel pretext task named random query patch detection in Unsupervised Pre-training DETR (UP-DETR).\nSpecifically, we randomly crop patches from the given image and then feed them as queries to the decoder. The model is pre-trained to\ndetect these query patches from the input image. During the pre-training, we address two critical issues: multi-task learning and\nmulti-query localization. (1) To trade off classification and localization preferences in the pretext task, we find that freezing the CNN\nbackbone is the prerequisite for the success of pre-training transformers. (2) To perform multi-query localization, we develop UP-DETR\nwith multi-query patch detection with attention mask. Besides, UP-DETR also provides a unified perspective for fine-tuning object\ndetection and one-shot detection tasks. In our experiments, UP-DETR significantly boosts the performance of DETR with faster\nconvergence and higher average precision on object detection, one-shot detection and panoptic segmentation. Code and pre-training\nmodels: https://github.com/dddzg/up-detr.\nIndex Termsâ€”Transformer, Unsupervised Pre-training, Self-supervised Learning, Object Detection, One-shot Detection\nâœ¦\n1 I NTRODUCTION\nD\nETECTION TRansformer (DETR) [1] is a recent framework\nthat views object detection as a direct set prediction problem\nvia a transformer encoder-decoder [2]. Without hand-designed\nsample selection [3] and non-maximum suppression (NMS),\nDETR even reaches a competitive performance with Faster R-\nCNN [4]. However, DETR comes with training and optimization\nchallenges, which need a large-scale training dataset and an\nextremely long training schedule even on COCO dataset [5].\nBesides, it is found that DETR performs poorly in PASCAL VOC\ndataset [6] which has insufficient training data and fewer instances\nthan COCO. Fig. 1 shows the PASCAL VOC learning curves of\nDETR and our Unsupervised Pre-training DETR (UP-DETR) .\nUP-DETR converges much faster with a higher AP than DETR.\nWith well-designed pretext tasks, unsupervised pre-training\nmodels achieve remarkable progress in both natural language pro-\ncessing (e.g. GPT [7], [8] and BERT [9]) and computer vision (e.g.\nMoCo [10], [11] and SwA V [12]). In DETR, the CNN backbone\n(ResNet-50 [13] with âˆ¼23.2M parameters) has been pre-trained to\nextract a good visual representation, but the transformer module\nwith âˆ¼18.0M parameters has not been pre-trained. Therefore, it is\na straightforward idea to pre-train the transformer in DETR.\nAlthough unsupervised visual representation learning ( e.g.\ncontrastive learning) attracts much attention in recent studies [10],\n[14], [15], [16], [17], [18], existing pretext tasks can not directly\napply to pre-train the transformer in DETR. We summarize two\nmain reasons from the following perspectives:\nâ€¢ Z. Dai and J. Chen are with the School of Software Engineering, South\nChina University of Technology, and also with the Key Laboratory of Big\nData and Intelligent Robot (SCUT), Ministry of Education, Guangzhou\n510006, China.\nE-mail: zhigangdai@hotmail.com, jychense@scut.edu.cn.\nâ€¢ B. Cai and Y. Lin are with Tencent Wechat AI, Guangzhou 510630, China.\nEmail: arlencai@tencent.com, lincolnlin@tencent.com.\n(Corresponding author: Junying Chen.)\n0 50 100 150 200 250 300\nEpochs\n40\n45\n50\n55\n60\n65\n70\n75\n80\nAP50\nDETR\nUP-DETR\nFig. 1: The PASCAL VOC learning curves (AP 50) of DETR and\nUP-DETR with the ResNet-50 backbone. Here, they are trained\non trainval07+12 and evaluated on test2007. We plot the\nshort and long training schedules, and the learning rate is reduced\nat 100 and 200 epochs respectively for these two schedules. UP-\nDETR converges much faster with a higher AP than DETR.\n(1) Different architectures : Recently, popular unsupervised\nlearning methods are designed for the backbone (feature\nextractor) pre-training [10], [14], [15] to extract the image\nfeature. However, DETR consists of a backbone and a\ntransformer encoder-decoder. The transformer in DETR is\ndesigned to translate the feature into a set of detection targets\ninstead of extracting the visual representation.\narXiv:2011.09094v3  [cs.CV]  24 Jul 2023\n2\n(2) Different feature preference : The transformer in DETR\nmainly focuses on spatial localization learning. Specifically,\nthe transformer is used to translate the instance-level in-\nformation of the image into coordinates. As for the self-\nattention in the decoder, it performs a NMS-like mechanism\nto suppress duplicated bounding boxes. However, existing\npretext tasks are designed for image instance-based [10],\n[14], [15] or cluster-based [16], [17], [18] contrastive learn-\ning. These contrastive learning methods mainly focus on\nfeature discrimination rather than spatial localization.\nAs analyzed above, we need to construct a spatial localization\nrelated task to pre-train the transformer in DETR. To realize this\nidea, we propose an Unsupervised Pre-training DETR (UP-DETR)\nwith a novel unsupervised pretext task named random query\npatch detection to pre-train the detector without any human\nannotations â€” we randomly crop multiple query patches from\nthe given image, and pre-train the transformer for object detection\nto predict the bounding boxes of these query patches in the given\nimage. During the pre-training procedure, we address two critical\nissues as follows:\n(1) Multi-task learning: Object detection is the coupling of\nobject classification and localization. To avoid query patch\ndetection destroying the classification features, we introduce\nthe frozen pre-training backbone and patch feature re-\nconstruction to preserve the feature discrimination of the\ntransformer. In our experiments, we find that the frozen pre-\ntraining backbone is the most important step to preserve the\nfeature discrimination during the pre-training.\n(2) Multi-query localization: Different object queries focus on\ndifferent position areas and box sizes. There is a NMS-like\nmechanism between different object queries. Considering this\nproperty, we propose a multi-query patch detection task\nwith attention mask.\nBesides, the proposed UP-DETR also provides a unified per-\nspective for object detection and one-shot detection. Just changing\nthe input of the decoder, we can easily fine-tune UP-DETR on\nobject detection and one-shot detection. In our experiments, we\npre-train DETR with the random query patch detection task. Our\nUP-DETR performs much better than DETR using exactly the\nsame architecture on PASCAL VOC [6] and COCO [5] object\ndetection datasets with faster convergence speed and better av-\nerage precision. Furthermore, UP-DETR also transfers well with\nstate-of-the-art performance on one-shot detection and panoptic\nsegmentation.\n2 R ELATED WORK\n2.1 Object Detection\nPositive and negative sample assignment is an important com-\nponent for object detection frameworks. Two-stage detectors [4],\n[19] and a part of one-stage detectors [20], [21] construct positive\nand negative samples by hand-crafted multi-scale anchors with\nthe IoU threshold and model confidence. Anchor-free one-stage\ndetectors [22], [23], [24] assign positive and negative samples to\nfeature maps by a grid of object centers. Zhang et al. demonstrate\nthat the performance gap between them is due to the selection of\npositive and negative training samples [3]. DETR [1] is a recent\nobject detection framework that is conceptually simpler without\nhand-crafted process by direct set prediction [25], which assigns\nthe positive and negative samples automatically.\nApart from the positive and negative sample selection prob-\nlem, the trade-off between classification and localization is also\nintractable for object detection. Zhanget al. illustrate that there is a\ndomain misalignment between classification and localization [26].\nWu et al. [27] and Song et al. [28] design two head structures\nfor classification and localization. They point out that these two\ntasks may have opposite feature preferences. As for our pre-\ntraining model, it shares feature for classification and localization.\nTherefore, it is essential to take a well trade-off between these two\ntasks.\n2.2 Unsupervised Pre-training\nUnsupervised pre-training models always follow two steps: pre-\ntraining on a large-scale dataset with the pretext task and fine-\ntuning the parameters on downstream tasks. For unsupervised pre-\ntraining, the pretext task is always invented, and we are interested\nin the learned intermediate representation rather than the final\nperformance of the pretext task.\nTo perform unsupervised pre-training, there are various well-\ndesigned pretext tasks. For natural language processing, utiliz-\ning time sequence relationship between discrete tokens, masked\nlanguage model [9], permutation language model [29] and auto\nregressive model [7], [8] are proposed to pre-train transformers [2]\nfor language representation. For computer vision, unsupervised\npre-training models also achieve remarkable progress recently\nfor visual representation learning, outperforming the supervised\nlearning counterpart in downstream tasks. Instance-based discrim-\nination tasks [30], [31] and clustering-based tasks [17] are two\ntypical pretext tasks in recent studies. Instance-based discrimina-\ntion tasks vary mainly on maintaining different sizes of negative\nsamples [10], [14], [15] with non-parametric contrastive learn-\ning [32]. Instance discrimination can also be realized as parametric\ninstance classification [16]. Moreover, clustering-based tasks vary\non offline [17], [18] or online clustering procedures [12]. Our\nproposed random query patch detection is a novel unsupervised\npretext task, which is designed to pre-train the transformer based\non the DETR architecture for object detection.\n2.3 Vision Transformer\nTransformers are not only used in detectors, but also applied to\nbackbone designs. The pioneering work is Vision Transformer\n(ViT) [33], which applies a transformer on non-overlapping image\npatches for image classification. It achieves an speed-accuracy\ntrade-off compared to convolutional neural networks with very\nlarge-scale training datasets. Based on this work, there are a\nlot of works that modify the ViT architecture for better image\nclassification performance [34], [35], [36].\nThough, ViT and DETR both use transformers, they are\ndesigned for different purposes. ViT uses transformers to capture\nthe relation between image patches for image classification, so\nthere is self-attention between image patches. On the other hand,\nthere are three different attention parts in DETR. The self-attention\nin the encoder of DETR is similar to ViT, which is designed to\ncapture global relation between image patches. However, the cross\nattention and self-attention in the decoder of DETR are designed\nfor set prediction of boxes. Therefore, existing pre-training tasks\nof backbones [10], [11], [37] can not be simply applied to DETR.\n3\n+\nTransformer\nDecoder\nobject\nqueries\nrandom query patch \nCNN\nbipartite\nmatch\n++ +\npositional encoding\ninput image\nfeature\nreconstruct\ná‰\nâ„’ð‘ð‘™ð‘  â‡’ 1or0\nâ„’ð‘ð‘œð‘¥ â‡’ ð‘¥,ð‘¦,ð‘¤,â„Ž\nâ„’ð‘Ÿð‘’ð‘ â‡’ ð‘“ð‘’ð‘Žð‘¡ð‘¢ð‘Ÿð‘’\nfrozen\nbackbone\nâ€¦\nTransformer\nEncoder\nGAP\n(a) single-query patch (N=3, M=1)\n+\nâ€¦\nTransformer\nEncoder\nTransformer\nDecoder\nrandom query patch \nbipartite\nmatch\npositional encoding\ninput image\nfrozen\nbackbone\n1 1 1 0 0 0\n1 1 1 0 0 0\n1 1 1 0 0 0\n0 0 0 1 1 1\n0 0 0 1 1 1\n0 0 0 1 1 1\nattention mask\nfeature reconstruct\ná‰\nâ„’ð‘ð‘™ð‘  â‡’ 1or0\nâ„’ð‘ð‘œð‘¥ â‡’ ð‘¥,ð‘¦,ð‘¤,â„Ž\nâ„’ð‘Ÿð‘’ð‘ â‡’ ð‘“ð‘’ð‘Žð‘¡ð‘¢ð‘Ÿð‘’\n+ + + + + + (b) multi-query patch (N=6, M=2)\nFig. 2: The pre-training procedure of UP-DETR by random query patch detection. (a) There is only a single-query patch which we\nadd to all object queries. (b) For multi-query patches, we add each query patch to N/M object queries with object query shuffle and\nattention mask. Note that CNN is not drawn in the decoder of (b) for neatness.\n3 UP-DETR\nThe proposed UP-DETR contains pre-training and fine-tuning\nprocedures: (a) the transformer is unsupervisedly pre-trained on\na large-scale dataset ( e.g. ImageNet in our experiments) without\nany human annotations; (b) the entire model is fine-tuned with\nlabeled data which is similar as the original DETR [1] on the\ndownstream detection related tasks (e.g. object detection and one-\nshot detection).\n3.1 Pre-training\nIn this section, we mainly describe how to pre-train the trans-\nformer encoder and decoder with random query patch detection\ntask. As shown in Fig. 2, the main idea of random query patch\ndetection is simple but effective.\nEncoder.Firstly, given an input image, a CNN backbone is used to\nextract the visual representation with the feature f âˆˆ RCÃ—HÃ—W ,\nwhere C is the channel dimension and H Ã— W is the feature map\nsize. Here, an image can be treated as a token sequence with the\nlength of H Ã— W constructed by the CNN extractor. Then, the\nfeature f is added with two-dimensional position encodings and\npassed to the multi-layer transformer encoder, which is exactly the\nsame as DETR.\nDecoder. Different from DETR, in our pre-training procedure, we\nrandomly crop patches as the query from the input image and\nrecord the corresponding coordinates, width and height of the\nquery as the ground truth. Therefore, our pre-training process can\nbe done in an unsupervised/self-supervised paradigm. Specifically,\nfor the random cropped query patch, the CNN backbone with\nglobal average pooling (GAP) extracts the patch feature p âˆˆ RC,\nwhich is flatten and supplemented with object queries q âˆˆ RC\nbefore passing it into a transformer decoder. Finally, the decoder\nis trained to predict the bounding boxes corresponding to the\nposition of random query patches in the input image. During the\npre-training, the feature of query patch is added to multiple object\nqueries, which are fed to the decoder. Note that the query patch\nrefers to the cropped patch from the input image but object query\nrefers to position embeddings. It can be understood as asking the\nmodel to find the query patch through these possible positions\n(embedded as object queries). Moreover, the CNN parameters of\nthe input image and query patches are shared in the whole model.\nMulti-query Patches. For fine-tuning object detection tasks, there\nare multiple object instances in each image (e.g. average 7.7 object\ninstances per image in the COCO dataset). In other words, the self-\nattention module in the decoder transformer learns an NMS-like\nmechanism during the training. To make a consistency between\npre-training and fine-tuning, UP-DETR needs to construct a com-\npetition between different object queries with an adjustable box\nsuppression ratio. Fortunately, UP-DETR can be easily extended\nto multi-query patch detection. Assuming that there are M query\npatches by random cropping and N object queries, we divide N\nobject queries into M groups, where each query patch is assigned\nto N/M object queries. In Fig. 2, we illustrate the single-query\npatch detection and multi-query patch detection. We adopt N=100\nand M=10 by default in our pre-training experiments.\nMatching. The model infers a prediction with a fixed-set Ë†y =\n{Ë†yi}N\ni=1 corresponding to N object queries ( N > M). In other\nwords, we get N pairs of bounding box predictions for these M\nquery patches. For detection tasks, the results are invariant to the\npermutation of predicted objects. Hence, following DETR [1], we\ncompute the same match cost between the prediction Ë†yË†Ïƒ(i) and the\nground-truth yi using Hungarian algorithm [25], where Ë†Ïƒ(i) is the\nindex of yi computed by the optimal bipartite matching.\nLoss. For the loss calculation, the predicted result Ë†yi = (Ë†ci âˆˆ\nR2,Ë†bi âˆˆ R4, Ë†pi âˆˆ RC) consists of three elements: Ë†ci is the binary\nclassification of matching the query patch (ci = 1) or not (ci = 0)\nfor each object query; Ë†bi is the vector that defines the box center\ncoordinates and its width and height as {x, y, w, h}, which are\nre-scaled relative to the image size; and Ë†pi is the reconstructed\nfeature with C = 2048 for the ResNet-50 backbone typically.\nWith the above definitions, the Hungarian loss for all matched\npairs is defined as:\nL(y, Ë†y) =\nNX\ni=1\n[Î»{ci}Lcls(ci, Ë†cË†Ïƒ(i)) +1{ci=1}Lbox(bi,Ë†bË†Ïƒ(i))\n+ 1{ci=1}Lrec(pi, Ë†pË†Ïƒ(i))]. (1)\nHere, Lcls is the cross entropy loss over two classes (match\nthe query patch vs. not match), and the class balance weight\nÎ»{ci=1} = 1and Î»{ci=0} = M/N. Lbox is a linear combination\nof â„“1 loss and the generalized IoU loss with the same weight\nhyper-parameters as DETR [1]. Lrec is the reconstruction loss\n4\n+\nâ€¦\nTransformer\nEncoder\nTransformer\nDecoder\nbipartite\nmatch\npositional encoding\ninput image\nlabeled data\n(a) object detection\n+\nâ€¦\nTransformer\nEncoder\nTransformer\nDecoder\nbipartite\nmatch\npositional encoding\ninput image\nlabeled data\ninput query\nCNN\n (b) one-shot detection\nFig. 3: The fine-tuning procedure of UP-DETR on object detection and one-shot detection. The only difference is the input of the\ntransformer decoder. (a) Object detection. There are multiple object queries (learnable embeddings) fed into the decoder. (b) One-shot\ndetection. The input query is extracted by the shared CNN, and the feature is added to object queries, which are fed into the decoder.\nproposed in this paper and only used during the pre-training.\nIt is designed to balance the feature preference of classification\nand localization during the pre-training, which will be further\ndiscussed in Section 3.1.2.\n3.1.1 Frozen Pre-training Backbone\nIn our experiments, we find that the CNN backbone seriously\naffects the transformer pre-training. In other words, the model\ncan not converge well and performs bad if we pre-train the CNN\nbackbone and the transformer from scratch together with random\nquery patch detection. This problem also appears in the original\nDETR1.\nFurthermore, object detection is the coupling of object clas-\nsification and localization, where these two tasks always have\ndifferent feature preferences [26], [27], [28]. However, our pre-\ntraining task only focuses on localization instead of feature clas-\nsification. So, we freeze the pre-training CNN backbone during\nthe transformer pre-training in our experiments. Stable backbone\nparameters are beneficial to the transformer pre-training, and\naccelerate the model pre-training process. In Section 4.5.2, we\nwill analyze and verify the necessity of them with experiments. We\nargue that the frozen pre-training backbone is the most essential\nstep to the success of UP-DETR.\n3.1.2 Tricks\nIn our preliminary experiments, we tried some tricks with our\nprior knowledge in terms of random query patch detection. Here,\nwe discuss two tricks during the pre-training, which are reasonable\nbut not critical to the success of UP-DETR.\nPatch Feature Reconstruction. Our pre-training task only fo-\ncuses on localization instead of feature classification. In other\nwords, missing an explicit branch for the classification task.\nSo, with the frozen pre-training backbone, we propose a feature\nreconstruction loss term Lrec to preserve classification feature\nduring localization pre-training. The motivation of this loss term is\nto preserve the feature discrimination extract by CNN after passing\nfeature to the transformer. Lrec is the mean squared error between\n1. https://github.com/facebookresearch/detr/issues/157\nthe â„“2-normalized patch feature extracted by the CNN backbone,\nwhich is defined as follows:\nLrec(pi, Ë†pË†Ïƒ(i)) =\n\r\r\r\r\r\npi\nâˆ¥piâˆ¥2\nâˆ’ Ë†pË†Ïƒ(i)\r\rË†pË†Ïƒ(i)\n\r\r\n2\n\r\r\r\r\r\n2\n2\n. (2)\nWith the frozen CNN backbone, patch feature reconstruction\nslightly improves the fine-tuning performance as shown in Sec-\ntion 4.5.2.\nAttention Mask. All the query patches are randomly cropped\nfrom the image. Therefore, they are independent without any rela-\ntions. For example, the bounding box regression of the first crop-\nping is not concerned with the second cropping. To satisfy the in-\ndependence of query patches, we utilize an attention mask matrix\nto control the interactions between different object queries. The\nmask matrix X âˆˆ RNÃ—N is added to the softmax layer of self-\nattention [2] in the decoder as softmax\n\u0000\nQKâŠ¤/âˆšdk + X\n\u0001\nV,\nwhere Q = K = V and they refer to the same set of the object\nquery representation in the decoder. Similar to the token mask in\nUniLM [38], the attention mask is defined as:\nXi,j =\n\u001a 0, i, j in the same group\nâˆ’âˆž, otherwise , (3)\nwhere Xi,j determines whether the object query qi participates the\ninteraction with the object query qj. For intuitive understanding,\nthe attention mask in Fig. 2b displays 1 and 0 corresponding to 0\nand âˆ’âˆž in (3), respectively. Attention mask also slightly leads to\na lower loss as demonstrated in Section 4.5.3.\n3.2 Fine-tuning\nUP-DETR provides a unified perspective in terms of detection\nrelated tasks. It can be easily fine-tuned into object detection or\none-shot detection by changing the model input. Fig. 3 shows the\nfine-tuning procedure on object detection and one-shot detection.\nThe CNN, transformer encoder-decoder and object queries are\nexactly the same for object detection and one-shot detection\ntasks, which are initialized from the pre-training procedure. The\nwhole model weights are fine-tuned with the labeled data. The\nonly difference in architecture between these two down-stream\ntasks is the input of transformer decoder. Noting that feature\nreconstruction loss is never used in the fine-tuning procedure.\nObject detection. Given an input image, the model is required\nto predict to the set of objects with the bounding boxes and\n5\nthe corresponding categories. Therefore, the fine-tuning procedure\nof UP-DETR is exactly the same with the training procedure of\nDETR. There are multiple object queries (learnable embeddings)\nused as the input fed into the decoder. Here, different object\nqueries learn different spatial specialization and area preference.\nBenefiting from the design of object query, the model can predict\nmultiple objects in parallel. With supervised object detection data,\nwe compute the match cost between model prediction and the\nannotated ground-truth.\nOne-shot detection. Given an input image and a query image,\nthe model is required to predict the objects with bounding boxes.\nThe objects should be semantically similar to the query image.\nTypically, the query image is constructed by patch from different\nimage with the same category. For one-shot detection, the query\nimage is extracted by the CNN (shared with the input image),\nand the patch feature is added to all object queries. In one-shot\ndetection, we only care about the matching result ( i.e. match to\nthe query or not) of the bounding boxes instead of the specific\ncategory in the object detection. So, in the loss and match cost\ncomputation, there is a binary classification output in one-shot\ndetection instead of multiple category classification in object\ndetection.\n4 E XPERIMENTS\nWe pre-train the model using ImageNet [39] and fine-tune the\nparameters on PASCAL VOC [6] and COCO [5]. In all exper-\niments, we implement the UP-DETR model (41.3M parameters)\nwith ResNet-50 backbone, 6 transformer encoder, 6 decoder layers\nof width 256 with 8 attention heads. Referring to the open source\ncode of DETR2, we use the same hyper-parameters in the proposed\nUP-DETR and our DETR re-implementation. We annotate R50\nand R101 short for ResNet-50 and ResNet-101. Note that UP-\nDETR and DETR have exactly the same model architecture, match\ncost and loss calculation. So, they have exactly the same FLOPs,\nparameters and running FPS.\nPre-training setup. UP-DETR is unsupervisedly pre-trained on\nthe 1.28M ImageNet training set without any ImageNet labels. The\nCNN backbone (ResNet-50) is also unsupervisedly pre-trained\nwith SwA V strategy [12], and its parameters are frozen during\nthe transformer pre-training. As the input image from ImageNet is\nrelatively small, we resize it such that the shortest side is within\n[320, 480] pixels while the longest side is at most 600 pixels.\nFor the given image, we crop the query patches with random\ncoordinate, height and width, which are resized to 128 Ã— 128\npixels and transformed with SimCLR style [14] without horizontal\nflipping, including random color distortion and Gaussian blurring.\nIn the early stage of our work, we conducted experiments about\ndifferent sampling strategies, i.e., grid sampling, and random sam-\npling with different hyper-parameters. However, we noticed that\nthere were no significant differences between different sampling\nstrategies. Therefore, we use random sampling to illustrate the\ngeneralization capability of our method. Moreover, AdamW [40]\nis used to optimize the UP-DETR, with the initial learning rate of\n1 Ã—10âˆ’4 and the weight decay of 1 Ã—10âˆ’4. We use a mini-batch\nsize of 256 on eight Nvidia V100 GPUs to train the model for 60\nepochs with the learning rate multiplied by 0.1 at 40 epochs.\nFine-tuning setup. The model is initialized with the pre-trained\nUP-DETR parameters and fine-tuned for all the parameters (in-\ncluding CNN) on PASCAL VOC and COCO. We fine-tune the\n2. https://github.com/facebookresearch/detr\nTABLE 1: Object detection results trained on PASCAL VOC\ntrainval07+12 and evaluated on test2007. DETR and\nUP-DETR use R50 backbone and Faster R-CNN uses R50-C4\nbackbone. The values in the brackets are the gaps compared to\nDETR with the same training schedule.\nModel/Epoch AP AP 50 AP75\nFaster R-CNN 56.1 82.6 62.7\nDETR/150 49.9 74.5 53.1\nUP-DETR/150 56.1 (+6.2) 79.7 (+5.2) 60.6 (+7.5)\nDETR/300 54.1 78.0 58.3\nUP-DETR/300 57.2 (+3.1) 80.1 (+2.1) 62.0 (+3.7)\nmodel with the initial learning rate of 1Ã—10âˆ’4 for the transformer\nand 5 Ã— 10âˆ’5 for the CNN backbone, and the other settings are\nthe same as DETR [1] on eight V100 GPUs with four images\nper GPU. The model is fine-tuned with short/long schedule for\n150/300 epochs and the learning rate is multiplied by 0.1 at\n100/200 epochs, respectively.\nGap between pre-training and fine-tuning. Object-level pre-\ntraining can boost the fine-tuning performance in object detec-\ntion task [41], but such method usually requires object prior\nknowledge, e.g., using a selective search algorithm to get object\nproposals during pre-training. In our work, we pre-train the model\non ImageNet dataset at patch level, as this is a common practice\nin pre-training on ImageNet and transferring to COCO or VOC\ndatasets [10], [37]. However, the gap between pre-training and\nfine-tuning always exists. For example, ImageNet images mostly\ncontain one object, but COCO images always contain multiple\nobjects. The image size of ImageNet images is relatively small,\nbut the image size of COCO images is large. Moreover, the pre-\ntraining task is conducted at patch level with random sampling in\nthis work, but the fine-tuning task is supervised on labeled data.\nAlthough the aforementioned gap between pre-training and fine-\ntuning cannot be eliminated, our method has good generalization\ncapability using random sampling strategy without any prior\nknowledge.\n4.1 PASCAL VOC Object Detection\nSetup. The model is fine-tuned on PASCAL VOC\ntrainval07+12 (âˆ¼16.5k images) and evaluated on\ntest2007. We report COCO-style metrics, including AP,\nAP50 (default VOC metric) and AP 75. For a full comparison,\nwe also report the result of Faster R-CNN with the R50-C4\nbackbone [12], which performs much better than R50 (C5\nstage) [42]. DETR with R50-C4 significantly increases the\ncomputational cost as compared with R50, so we fine-tune\nUP-DETR with R50 backbone. Even though, UP-DETR still\nperforms well. To emphasize the effectiveness of pre-training\nmodels, we report the results of 150 and 300 epochs for both\nDETR (from the random initialized transformer) and UP-DETR\n(from the pre-trained transformer).\nResults. Table 1 shows the object detection results on PASCAL\nVOC dataset. We find that DETR performs poorly on PASCAL\nVOC, which is much worse than Faster R-CNN by a large gap\nin all metrics. Due to the relatively small-scale data in VOC,\nthe pre-training transformer of UP-DETR significantly boosts the\nperformance of DETR for both short and long schedules: up to\n+6.2 (+3.1) AP, +5.2 (+2.1) AP50 and +7.5 (+3.7) AP75 for 150\n(300) epochs, respectively. Moreover, UP-DETR (R50) achieves a\ncomparable result to Faster R-CNN (R50-C4) with better AP. We\n6\n0 50 100 150 200 250 300\nEpochs\n20\n25\n30\n35\n40\n45\n50\n55\n60\nAP\nDETR\nUP-DETR\n(a) AP learning curves of PASCAL VOC\n0 50 100 150 200 250 300\nEpochs\n20\n25\n30\n35\n40\n45\nAP\nDETR\nUP-DETR (b) AP learning curves of COCO\nFig. 4: AP (COCO style) learning curves of DETR and UP-DETR on PASCAL VOC and COCO datasets. Models are trained with the\nSwA V pre-training ResNet-50 for 150 and 300 epochs, and the learning rate is reduced at 100 and 200 epochs, respectively.\nTABLE 2: Object detection results trained on COCO train2017 and evaluated on val2017. Faster R-CNN, DETR and UP-\nDETR are performed under comparable settings. â€  for values evaluated on COCO test-dev, which are always slightly higher than\nval2017. The values in the brackets are the gaps compared to DETR (SwA V CNN) with the same training schedule.\nModel Backbone Epochs AP AP 50 AP75 APS APM APL\nFaster R-CNN â€  [43] R101-FPN - 36.2 59.1 39.0 18.2 39.0 48.2\nMask R-CNN â€  [44] R101-FPN - 38.2 60.3 41.7 20.1 41.1 50.2\nGrid R-CNN â€  [45] R101-FPN - 41.5 60.9 44.5 23.3 44.9 53.1\nDouble-head R-CNN [27] R101-FPN - 41.9 62.4 45.9 23.9 45.2 55.8\nRetinaNet â€  [20] R101-FPN - 39.1 59.1 42.3 21.8 42.7 50.2\nFCOS â€  [22] R101-FPN - 41.5 60.7 45.0 24.4 44.8 51.6\nDETR [1] R50 500 42.0 62.4 44.2 20.5 45.8 61.1\nFaster R-CNN R50-FPN 3Ã— 40.2 61.0 43.8 24.2 43.5 52.0\nDETR (Supervised CNN) R50 150 39.5 60.3 41.4 17.5 43.0 59.1\nDETR (SwA V CNN) [12] R50 150 39.7 60.3 41.7 18.5 43.8 57.5\nUP-DETR R50 150 40.5 (+0.8) 60.8 42.6 19.0 44.4 60.0\nFaster R-CNN R50-FPN 9Ã— 42.0 62.1 45.5 26.6 45.4 53.4\nDETR (Supervised CNN) R50 300 40.8 61.2 42.9 20.1 44.5 60.3\nDETR (SwA V CNN) [12] R50 300 42.1 63.1 44.5 19.7 46.3 60.9\nUP-DETR R50 300 43.1 (+1.0) 63.4 46.0 21.6 46.8 62.4\nfind that both UP-DETR and DETR perform a little worse than\nFaster R-CNN in AP 50 and AP75. This may come from different\nratios of feature maps (C4 for Faster R-CNN) and no NMS post-\nprocessing (NMS lowers AP but slightly improves AP 50).\nFig. 4a shows the AP (COCO style) learning curves on VOC.\nUP-DETR significantly speeds up the model convergence. After\nthe learning rate reduced, UP-DETR significantly boosts the per-\nformance of DETR with a large AP improvement. Noting that UP-\nDETR obtains 56.1 AP after 150 epochs, however, its counterpart\nDETR (scratch transformer) only obtains 54.1 AP even after 300\nepochs and does not catch up even training longer. It suggests that\npre-training transformer is indispensable on insufficient training\ndata (i.e. âˆ¼ 16.5K images on VOC).\n4.2 COCO Object Detection\nSetup. The model is fine-tuned on COCO train2017 (âˆ¼118k\nimages) and evaluated on val2017. There are lots of small\nobjects in COCO dataset, where DETR performs poorly [1].\nTherefore, we report AP, AP50, AP75, APS, APM and APL for a\ncomprehensive comparison. Moreover, we also report the results\nof highly optimized Faster R-CNN with feature pyramid network\n(FPN) with short (3 Ã—) and long (9 Ã—) training schedules, which\nare known to improve the performance results [46]. To avoid\nsupervised CNN bringing supplementary information, we use\nSwA V pre-training CNN as the backbone of UP-DETR without\nany human annotations.\nResults. Table 2 shows the object detection results on COCO\ndataset. With 150 epoch schedule, UP-DETR outperforms DETR\n(SwA V CNN) by 0.8 AP and achieves a comparable performance\nas compared with Faster R-CNN (R50-FPN) (3 Ã— schedule). With\n300 epoch schedule, UP-DETR obtains 43.1 AP on COCO,\nwhich is 1.0 AP better than DETR (SwA V CNN) and 1.1 AP\nbetter than Faster R-CNN (R50-FPN) (9 Ã— schedule). Overall,\nUP-DETR comprehensively outperforms DETR in detection of\nsmall, medium and large objects with both short and long training\nschedules. Regrettably, UP-DETR is still slightly lagging behind\nFaster R-CNN in AP S, because of the lacking of FPN-like archi-\ntecture [43].\n7\nTABLE 3: One-shot detection results trained on VOC 2007train val and 2012train val sets and evaluated on VOC\n2007test set.\nModel seen class unseen class\nplant sofa tv car bottle boat chair person bus train horse bike dog bird mbike table AP50 cow sheep cat aero AP50\nSiamFC [47] 3.2 22.8 5.0 16.7 0.5 8.1 1.2 4.2 22.2 22.6 35.4 14.2 25.8 11.7 19.7 27.8 15.1 6.8 2.28 31.6 12.4 13.3\nSiamRPN [48] 1.9 15.7 4.5 12.8 1.0 1.1 6.1 8.7 7.9 6.9 17.4 17.8 20.5 7.2 18.5 5.1 9.6 15.9 15.7 21.7 3.5 14.2\nCompNet [49] 28.4 41.5 65.0 66.4 37.1 49.8 16.2 31.7 69.7 73.1 75.6 71.6 61.4 52.3 63.4 39.8 52.7 75.3 60.0 47.9 25.3 52.1\nCoAE [50] 30.0 54.9 64.1 66.7 40.1 54.1 14.7 60.9 77.5 78.3 77.9 73.2 80.5 70.8 72.4 46.2 60.1 83.9 67.1 75.6 46.2 68.2\nLi et al. [51] 33.7 58.2 67.5 72.7 40.8 48.2 20.1 55.4 78.2 79.0 76.2 74.6 81.3 71.6 72.0 48.8 61.1 74.3 68.5 81.0 52.4 69.1\nDETR 11.4 42.2 44.1 63.4 14.9 40.6 20.6 63.7 62.7 71.5 59.6 52.7 60.6 53.6 54.9 22.1 46.2 62.7 55.2 65.4 45.9 57.3\nUP-DETR 46.7 61.2 75.7 81.5 54.8 57.0 44.5 80.7 74.5 86.8 79.1 80.3 80.6 72.0 70.9 57.8 69.0 80.9 71.0 80.4 59.9 73.1\nTABLE 4: Panoptic segmentation results on the COCO val2017 dataset with the same ResNet-50 backbone. The PanopticFPN++,\nUPSNet and DETR results are re-implemented by Carion et al. [1].\nModel PQ SQ RQ PQth SQth RQth PQst SQst RQst APseg\nPanopticFPN++ [52] 42.4 79.3 51.6 49.2 82.4 58.8 32.3 74.8 40.6 37.7\nUPSNet [53] 42.5 78.0 52.5 48.6 79.4 59.6 33.4 75.9 41.7 34.3\nUPSNet-M [53] 43.0 79.1 52.8 48.9 79.7 59.7 34.1 78.2 42.3 34.3\nDETR [1] 44.3 80.0 54.5 49.2 80.6 60.3 37.0 79.1 45.9 32.9\nUP-DETR 44.7 80.5 54.9 49.7 80.9 60.8 37.2 79.3 46.2 34.3\nFig. 4b shows the AP learning curves on COCO. UP-DETR\noutperforms DETR for both 150 and 300 epoch schedules with\nfaster convergence. The performance improvement is more no-\nticeable before reducing the learning rate. After reducing the\nlearning rate, UP-DETR still holds the lead of DETR by âˆ¼ 1.0\nAP improvement. It suggests that the pre-training transformer is\nstill indispensable even on sufficient training data ( i.e. âˆ¼ 118K\nimages on COCO).\n4.3 One-Shot Detection\nSetup. Given a query image patch whose class label is not\nincluded in the training data, one-shot detection aims to detect\nall instances with the same class in a target image. One-shot\ndetection is a promising research direction that can detect unseen\ninstances. With feeding query patches to the decoder, UP-DETR\nis naturally compatible to one-shot detection task. Therefore, one-\nshot detection can also be treated as a downstream fine-tuning task\nof UP-DETR.\nFollowing the same one-shot detection setting as [50], we crop\nout the ground truth bounding boxes as the query image patches.\nDuring fine-tuning, for the given image, we randomly sample a\nquery image patch of a seen class that exists in this image, and\nthe training ground truth is filtered according to the query image\npatch. During evaluation, we first randomly shuffle the query\nimage patches of the class (existing in the image) with a specific\nrandom seed of target image ID, then pick up the first five query\nimage patches, and finally average their AP scores. Every existing\nclass in each image is evaluated and then averaged. The shuffle\nprocedure ensures the results in stable statistics for evaluation. We\ntrain/fine-tune DETR/UP-DETR on VOC 2007train val and\n2012train val sets with 300 epochs, and evaluate on VOC\n2007test set. We follow the same setting as Li et al. [51].\nResults. Table 3 shows the comparison to the state-of-the-art\none-shot detection methods. UP-DETR significantly boosts the\nperformance of DETR on both seen (+22.8 AP50 gain) and unseen\n(+15.8 AP50 gain) classes. Moreover, UP-DETR outperforms all\ncompared methods in both seen (min +7.9 AP50 gain) and unseen\n(min +4.0 AP50 gain) classes of one-shot detection. It further\nverifies the effectiveness of our pre-training pretext task.\nTABLE 5: The ablation results of pre-training models with single-\nquery patch and multi-query patches on PASCAL VOC. The\nvalues in the brackets are the gaps compared to DETR with the\nsame training schedule.\nModel AP AP 50 AP75\nDETR 49.9 74.5 53.1\nUP-DETR (M=1) 53.1 (+3.2) 77.2 (+2.7) 57.4\nUP-DETR (M=10) 54.9 (+5.0) 78.7 (+4.2) 59.1\n4.4 Panoptic Segmentation\nThe original DETR can be easily extended to panoptic segmenta-\ntion [52] by adding a mask head on top of the decoder outputs.\nFollowing the same panoptic segmentation training schema as\nDETR [1], we fine-tune UP-DETR with COCO panoptic segmen-\ntation annotation (extra stuff annotation) for only box annotations\nwith 300 epochs. Then, we freeze all the weights of DETR and\ntrain the mask head for 25 epochs. We find that UP-DETR also\nboosts the fine-tuning performance of panoptic segmentation.\nTable 4 shows the comparison to state-of-the-art methods on\npanoptic segmentation with the ResNet-50 backbone. As seen,\nUP-DETR outperforms DETR3 with +0.4 PQ, +0.5 PQth and +1.4\nAPseg.\n4.5 Ablations\nIn ablation experiments, we train/pre-train DETR/UP-DETR mod-\nels for 15 epochs with the learning rate multiplied by 0.1 at the\n10th epoch. We fine-tune the UP-DETR models on PASCAL VOC\nfollowing the setup in Section 4.1 with 150 epochs. Therefore,\nthe average precision results in ablations are relatively lower than\nthose shown in Section 4.1.\n4.5.1 Single-query patch vs. Multi-query patches\nWe pre-train the UP-DETR model with single-query patch ( M =\n1) and multi-query patches ( M = 10). Other hyper-parameters\nare set as mentioned above.\n3. With a bug fixed in github.com/facebookresearch/detr/issues/247, the\nDETR baseline is better than paper report.\n8\nTABLE 6: Ablation study on the frozen CNN and feature recon-\nstruction for pre-training models with AP 50. The experiments are\nfine-tuned on PASCAL VOC with 150 epochs.\nCase Frozen CNN Feature Reconstruction AP50\nDETR scratch transformer 74.5\n(a) 74.0\n(b) âœ“ 78.7\n(c) âœ“ 62.0\n(d) UP-DETR âœ“ âœ“ 78.7\n0 20 40 60 80 100 120 140\nEpochs\n0\n10\n20\n30\n40\n50\n60\n70\n80\nAP50\nDETR\n(a)\n(b)\n(c)\n(d) UP-DETR\nFig. 5: Learning curves (AP 50) of DETR and four different pre-\ntraining models on PASCAL VOC trained with 150 epochs. The\nmodels correspond to the models in Table 6 one-to-one.\nTable 5 shows the results of single-query patch and multi-\nquery patches. Compared with DETR, UP-DETR surpasses it in\nall AP metrics by a large margin no matter with single-query\npatch or multi-query patches. When pre-training UP-DETR with\nthe different number of query patches, UP-DETR ( M = 10)\nperforms better than UP-DETR ( M = 1) on the fine-tuning task,\nalthough there are about 2.3 instances per image on PASCAL\nVOC. Therefore, we adopt the same UP-DETR with M = 10for\nboth PASCAL VOC and COCO instead of varyingM for different\ndownstream tasks.\n4.5.2 Frozen CNN and Feature Reconstruction\nTo illustrate the importance of frozen CNN backbone and patch\nfeature reconstruction of UP-DETR, we pre-train four different\nmodels with different combinations of whether freezing CNN and\nwhether adding feature reconstruction. Note that all the models\n(including DETR) use the pre-trained CNN on ImageNet.\nTable 6 shows AP 50 of DETR and four different pre-training\nmodels on PASCAL VOC with 150 epochs. As shown in Ta-\nble 6, not all pre-trained models are better than DETR, but pre-\ntraining models (b) and (d) perform better than the others. More\nimportantly, without frozen CNN, pre-training models (a) and (c)\neven perform worse than DETR. This confirms that the frozen\npre-training backbone is essential to pre-train the transformer. In\naddition, it further confirms that the pretext task (random query\npatch detection) may weaken the feature discrimination of the pre-\ntraining CNN, and localization and classification have different\nfeature preferences [26], [27], [28].\n0 2 4 6 8 10 12 14\nEpochs\n13\n14\n15\n16\n17\n18 Loss\nw/  attention mask\nw/o attention mask\nFig. 6: The loss curves of pre-training procedure for UP-DETR w/\nand w/o the attention mask.\nFig. 5 plots the AP 50 learning curves of DETR and four dif-\nferent pre-training models, where the models in Fig. 5 correspond\nto the models in Table 6 one-to-one. As shown in Fig. 5, (d)\nUP-DETR model achieves faster convergence speed at the early\ntraining stage with feature reconstruction. The experiment results\nsuggest that random query patch detection is complementary to the\ncontrastive learning for a better visual representation. The former\nis designed for the spatial localization with position embeddings,\nand the latter is designed for instance or cluster classification.\nIt is worth noting that UP-DETR with frozen CNN and feature\nreconstruction heavily relies on a pre-trained CNN model, e.g.\nSwA V CNN. Therefore, we believe that it is a promising direction\nto further investigate UP-DETR with random query patch detec-\ntion and contrastive learning together to pre-train the whole DETR\nmodel from scratch.\n4.5.3 Attention Mask\nAfter downstream task fine-tuning, we find that there is no notice-\nable difference between the UP-DETR models pre-trained with\nand without attention mask. Instead of the fine-tuning results, we\nplot the loss curves in the pretext task to illustrate the effectiveness\nof attention mask.\nAs shown in Fig. 6, at the early training stage, UP-DETR\nwithout attention mask has a lower loss. However, as the model\nconverging, UP-DETR with attention mask overtakes it with\na lower loss. The curves seem weird at the first glance, but\nit is reasonable because the loss is calculated by the optimal\nbipartite matching. During the early training stage, the model is\nnot converged, and the model without attention mask takes more\nobject queries into attention. Intuitively, the model is easier to be\noptimized due to introducing more object queries. However, there\nis a mismatching between the query patch and the ground truth for\nthe model without attention mask. As the model converging, the\nattention mask gradually takes effect, which masks the unrelated\nquery patches and leads to a lower loss.\n9\nquery\npatches\nFig. 7: The unsupervised localization of patch queries with UP-DETR. The first line is the original image with predicted bounding\nboxes. The second line is query patches cropped from the original image with data augmentation (colorjitter and resizing). The value in\nthe upper left corner of the bounding box is the model confidence. As seen, without any annotations, UP-DETR learns to detect patches\nwith given queries in the unsupervised way.\n(a) fine-tuned on object detection\nquery image (b) fine-tuned on one-shot detection\nFig. 8: The visualization result of fine-tuned experiments on object detection and one-shot detection. These two models are fine-tuned\nwith the same UP-DETR model. (a) With the input image, the object detection model detects the objects with bounding boxes, whose\nclasses exist in the training set. (b) Given input image and query image, the one-shot detection model detects the objects, which are\nsemantically similar to the query image (with the matched tag instead of the class name).\n4.6 Visualization\n4.6.1 Pre-training\nTo further illustrate the ability of the pre-training model, we visu-\nalize the unsupervised localization results of given patch queries.\nSpecifically, for the given image, we manually crop several object\npatches and apply the data augmentation to them. Then, we\nfeed these patches as queries to the model. Finally, we visual-\nize the model output with bounding boxes, whose classification\nconfidence is greater than 0.9. This procedure can be treated as\nunsupervised one-shot detection or deep learning based template\nmatching.\nAs shown in Fig. 7, pre-trained with random query patch de-\ntection, UP-DETR successfully learns to locate the bounding box\nof given query patches and suppresses the duplicated bounding\nboxes4. It demonstrates that UP-DETR with random query patch\ndetection is effective to learn the ability of object localization.\n4. Base picture credit: https://www.piqsels.com/en/public-domain-photo-\njrkkq, https://www.piqsels.com/en/public-domain-photo-smdfn.\n4.6.2 Fine-tuning\nFig. 8 shows the visualization result of fine-tuned models on object\ndetection and one-shot detection. Our fine-tuning models perform\nwell on these two tasks. For object detection task, the model\nis fine-tuned to detect objects with bounding boxes and classes.\nThese classes (e.g. elephant, bird, person in Fig. 8a) must exist in\nthe training set. Differently, for one-shot detection, given the input\nimage and the query image, the model only predicts the similar\nobjects with bounding boxes and 0/1 labels (matching or not).\nThese two tasks can be unified in our UP-DETR framework by\nchanging the input of the transformer decoder.\n5 C ONCLUSION\nWe present a novel pretext task called random query patch\ndetection to unsupervisedly pre-train the transformer in DETR.\nWith unsupervised pre-training, UP-DETR significantly outper-\nforms DETR by a large margin with higher precision and much\nfaster convergence on PASCAL VOC. For the challenging COCO\ndataset with sufficient training data, UP-DETR still surpasses\nDETR even with a long training schedule. It indicates that the\n10\npre-training transformer is indispensable for different scales of\ntraining data in object detection. Besides, UP-DETR also provides\na unified perspective for one-shot detection. It significant boosts\nthe performance on one-shot detection task.\nFrom the perspective of unsupervised pre-training models, the\npre-training CNN backbone and the pre-training transformer are\nseparated now. Recent studies of unsupervised pre-training mainly\nfocus on feature discrimination with contrastive learning instead of\nspecialized modules for spatial localization. But in UP-DETR pre-\ntraining, the pretext task is mainly designed for patch localization\nby positional encodings and learnable object queries. We hope\nan advanced method can integrate the CNN and transformer pre-\ntraining into a unified end-to-end framework and apply UP-DETR\nto more downstream tasks ( e.g. few-shot object detection and\nobject tracking).\nACKNOWLEDGEMENT\nThis work was supported in part by the National Natural Sci-\nence Foundation of China under Grant 61802130, and in part\nby the Guangdong Natural Science Foundation under Grant\n2019A1515012152 and 2021A1515012651. This work is done\nwhen Zhigang Dai was an intern at Tencent Wechat AI.\nREFERENCES\n[1] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, â€œEnd-to-end object detection with transformers,â€ arXiv\npreprint arXiv:2005.12872, 2020.\n[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nÅ. Kaiser, and I. Polosukhin, â€œAttention is all you need,â€ in Advances in\nNeural Information Processing Systems, 2017, pp. 5998â€“6008.\n[3] S. Zhang, C. Chi, Y . Yao, Z. Lei, and S. Z. Li, â€œBridging the gap\nbetween anchor-based and anchor-free detection via adaptive training\nsample selection,â€ inProceedings of IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2020, pp. 9759â€“9768.\n[4] S. Ren, K. He, R. Girshick, and J. Sun, â€œFaster r-cnn: Towards real-time\nobject detection with region proposal networks,â€ in Advances in Neural\nInformation Processing Systems, 2015, pp. 91â€“99.\n[5] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll Â´ar, and C. L. Zitnick, â€œMicrosoft coco: Common objects in\ncontext,â€ in European Conference on Computer Vision. Springer, 2014,\npp. 740â€“755.\n[6] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman,\nâ€œThe pascal visual object classes (voc) challenge,â€ International journal\nof computer vision, vol. 88, no. 2, pp. 303â€“338, 2010.\n[7] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, â€œImproving\nlanguage understanding by generative pre-training,â€ 2018.\n[8] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\nâ€œLanguage models are unsupervised multitask learners,â€ OpenAI Blog ,\nvol. 1, no. 8, p. 9, 2019.\n[9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, â€œBert: Pre-training\nof deep bidirectional transformers for language understanding,â€ arXiv\npreprint arXiv:1810.04805, 2018.\n[10] K. He, H. Fan, Y . Wu, S. Xie, and R. Girshick, â€œMomentum con-\ntrast for unsupervised visual representation learning,â€ in Proceedings\nof IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2020, pp. 9729â€“9738.\n[11] X. Chen, H. Fan, R. Girshick, and K. He, â€œImproved baselines with mo-\nmentum contrastive learning,â€ arXiv preprint arXiv:2003.04297, 2020.\n[12] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin,\nâ€œUnsupervised learning of visual features by contrasting cluster assign-\nments,â€ Advances in Neural Information Processing Systems , vol. 33,\n2020.\n[13] K. He, X. Zhang, S. Ren, and J. Sun, â€œDeep residual learning for image\nrecognition,â€ in Proceedings of IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2016, pp. 770â€“778.\n[14] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, â€œA simple frame-\nwork for contrastive learning of visual representations,â€ arXiv preprint\narXiv:2002.05709, 2020.\n[15] J.-B. Grill, F. Strub, F. Altch Â´e, C. Tallec, P. Richemond, E. Buchatskaya,\nC. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azaret al., â€œBootstrap\nyour own latent-a new approach to self-supervised learning,â€ Advances\nin Neural Information Processing Systems, vol. 33, 2020.\n[16] Y . Cao, Z. Xie, B. Liu, Y . Lin, Z. Zhang, and H. Hu, â€œParametric instance\nclassification for unsupervised visual feature learning,â€ Advances in\nNeural Information Processing Systems, vol. 33, 2020.\n[17] M. Caron, P. Bojanowski, A. Joulin, and M. Douze, â€œDeep clustering for\nunsupervised learning of visual features,â€ in Proceedings of European\nConference on Computer Vision, 2018, pp. 132â€“149.\n[18] Y . Asano, C. Rupprecht, and A. Vedaldi, â€œSelf-labelling via simultaneous\nclustering and representation learning,â€ in International Conference on\nLearning Representations, 2019.\n[19] Z. Cai and N. Vasconcelos, â€œCascade R-CNN: Delving into high quality\nobject detection,â€ in Proceedings of IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2018, pp. 6154â€“6162.\n[20] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. Doll Â´ar, â€œFocal loss for\ndense object detection,â€ inProceedings of IEEE International Conference\non Computer Vision, 2017, pp. 2980â€“2988.\n[21] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y . Fu, and A. C.\nBerg, â€œSsd: Single shot multibox detector,â€ in European Conference on\nComputer Vision. Springer, 2016, pp. 21â€“37.\n[22] Z. Tian, C. Shen, H. Chen, and T. He, â€œFcos: Fully convolutional one-\nstage object detection,â€ in Proceedings of IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2019, pp. 9627â€“9636.\n[23] X. Zhou, D. Wang, and P. KrÂ¨ahenbÂ¨uhl, â€œObjects as points,â€arXiv preprint\narXiv:1904.07850, 2019.\n[24] H. Law and J. Deng, â€œCornernet: Detecting objects as paired keypoints,â€\nin Proceedings of European Conference on Computer Vision , 2018, pp.\n734â€“750.\n[25] R. Stewart, M. Andriluka, and A. Y . Ng, â€œEnd-to-end people detection in\ncrowded scenes,â€ in Proceedings of IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2016, pp. 2325â€“2333.\n[26] H. Zhang and J. Wang, â€œTowards adversarially robust object detection,â€\nin Proceedings of IEEE International Conference on Computer Vision ,\n2019, pp. 421â€“430.\n[27] Y . Wu, Y . Chen, L. Yuan, Z. Liu, L. Wang, H. Li, and Y . Fu, â€œRethinking\nclassification and localization for object detection,â€ in Proceedings of\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2020, pp. 10 186â€“10 195.\n[28] G. Song, Y . Liu, and X. Wang, â€œRevisiting the sibling head in object\ndetector,â€ in Proceedings of IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2020, pp. 11 563â€“11 572.\n[29] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and\nQ. V . Le, â€œXlnet: Generalized autoregressive pretraining for language\nunderstanding,â€ in Advances in Neural Information Processing Systems ,\n2019, pp. 5753â€“5763.\n[30] M. Ye, X. Zhang, P. C. Yuen, and S.-F. Chang, â€œUnsupervised embedding\nlearning via invariant and spreading instance feature,â€ in Proceedings\nof IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2019, pp. 6210â€“6219.\n[31] Z. Wu, Y . Xiong, S. X. Yu, and D. Lin, â€œUnsupervised feature learning via\nnon-parametric instance discrimination,â€ in Proceedings of IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2018, pp.\n3733â€“3742.\n[32] R. Hadsell, S. Chopra, and Y . LeCun, â€œDimensionality reduction by\nlearning an invariant mapping,â€ in 2006 IEEE Computer Society Con-\nference on Computer Vision and Pattern Recognition (CVPRâ€™06), vol. 2.\nIEEE, 2006, pp. 1735â€“1742.\n[33] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Un-\nterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,\nand N. Houlsby, â€œAn image is worth 16x16 words: Transformers for\nimage recognition at scale,â€ In International Conference on Learning\nRepresentations, 2021.\n[34] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, F. E. H. Tay, J. Feng, and\nS. Yan, â€œTokens-to-token vit: Training vision transformers from scratch\non imagenet,â€ 2021 IEEE/CVF International Conference on Computer\nVision (ICCV), pp. 538â€“547, 2021.\n[35] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y . Wang, â€œTransformer\nin transformer,â€ Advances in Neural Information Processing Systems ,\nvol. 34, pp. 15 908â€“15 919, 2021.\n[36] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo,\nâ€œSwin transformer: Hierarchical vision transformer using shifted win-\ndows,â€ 2021 IEEE/CVF International Conference on Computer Vision\n(ICCV), pp. 9992â€“10 002, 2021.\n[37] K. He, X. Chen, S. Xie, Y . Li, P. Doll Â´ar, and R. Girshick, â€œMasked\nautoencoders are scalable vision learners,â€ in Proceedings of IEEE/CVF\n11\nConference on Computer Vision and Pattern Recognition , 2022, pp.\n16 000â€“16 009.\n[38] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y . Wang, J. Gao, M. Zhou,\nand H.-W. Hon, â€œUnified language model pre-training for natural lan-\nguage understanding and generation,â€ in Advances in Neural Information\nProcessing Systems, 2019, pp. 13 063â€“13 075.\n[39] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, â€œImagenet:\nA large-scale hierarchical image database,â€ in Proceedings of IEEE/CVF\nConference on Computer Vision and Pattern Recognition . Ieee, 2009,\npp. 248â€“255.\n[40] I. Loshchilov and F. Hutter, â€œDecoupled weight decay regularization,â€ in\nInternational Conference on Learning Representations, 2018.\n[41] A. Bar, X. Wang, V . Kantorov, C. J. Reed, R. Herzig, G. Chechik,\nA. Rohrbach, T. Darrell, and A. Globerson, â€œDetreg: Unsupervised\npretraining with region priors for object detection,â€ in Proceedings of\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2022, pp. 14 605â€“14 615.\n[42] Y . Li, H. Qi, J. Dai, X. Ji, and Y . Wei, â€œFully convolutional instance-\naware semantic segmentation,â€ in Proceedings of IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2017, pp. 2359â€“2367.\n[43] T.-Y . Lin, P. DollÂ´ar, R. Girshick, K. He, B. Hariharan, and S. Belongie,\nâ€œFeature pyramid networks for object detection,â€ in Proceedings of\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2017, pp. 2117â€“2125.\n[44] K. He, G. Gkioxari, P. Doll Â´ar, and R. Girshick, â€œMask r-cnn,â€ in Pro-\nceedings of IEEE International Conference on Computer Vision , 2017,\npp. 2961â€“2969.\n[45] X. Lu, B. Li, Y . Yue, Q. Li, and J. Yan, â€œGrid r-cnn,â€ in Proceedings\nof IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2019, pp. 7363â€“7372.\n[46] K. He, R. Girshick, and P. Doll Â´ar, â€œRethinking imagenet pre-training,â€\nin Proceedings of IEEE International Conference on Computer Vision ,\n2019, pp. 4918â€“4927.\n[47] L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and P. H. Torr,\nâ€œFully-convolutional siamese networks for object tracking,â€ in European\nconference on computer vision. Springer, 2016, pp. 850â€“865.\n[48] B. Li, J. Yan, W. Wu, Z. Zhu, and X. Hu, â€œHigh performance visual\ntracking with siamese region proposal network,â€ in Proceedings of\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2018, pp. 8971â€“8980.\n[49] T. Zhang, Y . Zhang, X. Sun, H. Sun, M. Yan, X. Yang, and K. Fu,\nâ€œComparison network for one-shot conditional object detection,â€ arXiv\npreprint arXiv:1904.02317, 2019.\n[50] T.-I. Hsieh, Y .-C. Lo, H.-T. Chen, and T.-L. Liu, â€œOne-shot ob-\nject detection with co-attention and co-excitation,â€ arXiv preprint\narXiv:1911.12529, 2019.\n[51] X. Li, L. Zhang, Y . P. Chen, Y .-W. Tai, and C.-K. Tang, â€œOne-shot object\ndetection without fine-tuning,â€ arXiv preprint arXiv:2005.03819, 2020.\n[52] A. Kirillov, K. He, R. Girshick, C. Rother, and P. Doll Â´ar, â€œPanoptic\nsegmentation,â€ in Proceedings of IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2019, pp. 9404â€“9413.\n[53] Y . Xiong, R. Liao, H. Zhao, R. Hu, M. Bai, E. Yumer, and R. Urtasun,\nâ€œUpsnet: A unified panoptic segmentation network,â€ in Proceedings of\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2019, pp. 8818â€“8826.\nZhigang Daireceived the B.E. degree from the\nSchool of Software Engineering, South China\nUniversity of Technology, Guangzhou, China,\nwhere he is currently pursuing the masterâ€™s de-\ngree.\nHis research interests include visual represen-\ntation learning and object detection. He used to\nbe an intern at Tencent Wechat AI and published\na paper at CVPR 2021 as an oral presentation.\nBolun Cai received the M.Eng. and Ph.D. de-\ngrees from the South China University of Tech-\nnology, Guangzhou, China, in 2016 and 2019,\nrespectively. He is currently a Senior Researcher\nwith Tencent WeChat AI.\nHis research interests include computer vi-\nsion, machine learning, and image processing.\nYugeng Linreceived the B.Eng. degree from the\nSun Y at-sen University in 2012. He is currently\nan Expert Researcher with Tencent WeChat AI.\nHis research interests include computer vision\nand image processing.\nJunying Chen(Senior Member, IEEE) received\nthe B.E. degree in electronic and information en-\ngineering from Zhejiang University, Hangzhou,\nChina, in 2007, and the Ph.D. degree in electrical\nand electronic engineering from The University\nof Hong Kong, Hong Kong, in 2013. She is cur-\nrently an Associate Professor with the School\nof Software Engineering, South China University\nof Technology, Guangzhou, China. She is the\nmember of IEEE-HKN, and the senior members\nof IEEE, CCF , CSIG, and CSBME.\nShe published over 40 papers in academic journals and conferences,\nincluding IEEE TNNLS, IEEE TMI, IEEE JBHI, CVPR, ACM MM, ACL,\netc. She received the Second Prize for Technological Invention from\nCCF Science and Technology Award, and the Second Prize of Excel-\nlent Papers of Guangdong Computer Academy. Her research interests\ninclude deep learning, neural networks, computer vision, and image\nprocessing.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7612012624740601
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4938102662563324
    },
    {
      "name": "Object detection",
      "score": 0.4900034964084625
    },
    {
      "name": "Transformer",
      "score": 0.4609231948852539
    },
    {
      "name": "Machine learning",
      "score": 0.3623572885990143
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3022707998752594
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}