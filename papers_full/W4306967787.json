{
  "title": "A survey: object detection methods from CNN to transformer",
  "url": "https://openalex.org/W4306967787",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3196411312",
      "name": "Ershat Arkin",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A17265748",
      "name": "Nurbiya Yadikar",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2117564020",
      "name": "Xuebin Xu",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2025341163",
      "name": "Alimjan Aysa",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A41061535",
      "name": "Kurban Ubul",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A3196411312",
      "name": "Ershat Arkin",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A17265748",
      "name": "Nurbiya Yadikar",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2117564020",
      "name": "Xuebin Xu",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2025341163",
      "name": "Alimjan Aysa",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A41061535",
      "name": "Kurban Ubul",
      "affiliations": [
        "Xinjiang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3197486099",
    "https://openalex.org/W3018757597",
    "https://openalex.org/W2893749619",
    "https://openalex.org/W2490270993",
    "https://openalex.org/W3035524459",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2963849369",
    "https://openalex.org/W2990954877",
    "https://openalex.org/W6779879114",
    "https://openalex.org/W3157528469",
    "https://openalex.org/W3139049060",
    "https://openalex.org/W4299802238",
    "https://openalex.org/W2161969291",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2989604896",
    "https://openalex.org/W2031489346",
    "https://openalex.org/W2037227137",
    "https://openalex.org/W3170778815",
    "https://openalex.org/W2579985080",
    "https://openalex.org/W3184439416",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W6790690058",
    "https://openalex.org/W3174981785",
    "https://openalex.org/W2109255472",
    "https://openalex.org/W2806070179",
    "https://openalex.org/W3193501245",
    "https://openalex.org/W4297775537",
    "https://openalex.org/W2982083293",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W4394670654",
    "https://openalex.org/W2336589871",
    "https://openalex.org/W2962837320",
    "https://openalex.org/W4288083516",
    "https://openalex.org/W2886335102",
    "https://openalex.org/W2883208628",
    "https://openalex.org/W2988452521",
    "https://openalex.org/W4306884143",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W2193145675",
    "https://openalex.org/W2963430933",
    "https://openalex.org/W2988916019",
    "https://openalex.org/W2998564420",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4297575730",
    "https://openalex.org/W2214352687",
    "https://openalex.org/W2883780447",
    "https://openalex.org/W2996603555",
    "https://openalex.org/W4286910290",
    "https://openalex.org/W2307770531",
    "https://openalex.org/W2962721361",
    "https://openalex.org/W3160694286",
    "https://openalex.org/W3175630421",
    "https://openalex.org/W2990792015",
    "https://openalex.org/W4214767101",
    "https://openalex.org/W3044379940",
    "https://openalex.org/W2570343428",
    "https://openalex.org/W4293584584",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W639708223",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W2963813458",
    "https://openalex.org/W2963516811",
    "https://openalex.org/W3134154885",
    "https://openalex.org/W3156313549",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2963918968",
    "https://openalex.org/W3034971973",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2088049833",
    "https://openalex.org/W2164598857",
    "https://openalex.org/W4224139907",
    "https://openalex.org/W2963952323",
    "https://openalex.org/W2796502408",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3211783547",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W3168643403",
    "https://openalex.org/W3176153963",
    "https://openalex.org/W3167095230",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W2963125010",
    "https://openalex.org/W2798355657",
    "https://openalex.org/W3138006590",
    "https://openalex.org/W3106250896"
  ],
  "abstract": "Abstract Object detection is the most important problem in computer vision tasks. After AlexNet proposed, based on Convolutional Neural Network (CNN) methods have become mainstream in the computer vision field, many researches on neural networks and different transformations of algorithm structures have appeared. In order to achieve fast and accurate detection effects, it is necessary to jump out of the existing CNN framework and has great challenges. Transformer’s relatively mature theoretical support and technological development in the field of Natural Language Processing have brought it into the researcher’s sight, and it has been proved that Transformer’s method can be used for computer vision tasks, and proved that it exceeds the existing CNN method in some tasks. In order to enable more researchers to better understand the development process of object detection methods, existing methods, different frameworks, challenging problems and development trends, paper introduced historical classic methods of object detection used CNN, discusses the highlights, advantages and disadvantages of these algorithms. By consulting a large amount of paper, the paper compared different CNN detection methods and Transformer detection methods. Vertically under fair conditions, 13 different detection methods that have a broad impact on the field and are the most mainstream and promising are selected for comparison. The comparative data gives us confidence in the development of Transformer and the convergence between different methods. It also presents the recent innovative approaches to using Transformer in computer vision tasks. In the end, the challenges, opportunities and future prospects of this field are summarized.",
  "full_text": "A survey: object detection methods\nfrom CNN to transformer\nErshat Arkin 1 & Nurbiya Yadikar 1 & Xuebin Xu 1 & Alimjan Aysa 2 & Kurban Ubul 1,2\nAbstract\nObject detection is the most important problem in computer vision tasks. After AlexNet\nproposed, based on Convolutional Neural Network (CNN) methods have become main-\nstream in the computer vision field, many researches on neural networks and different\ntransformations of algorithm structures have appeared. In order to achieve fast and\naccurate detection effects, it is necessary to jump out of the existing CNN framework\nand has great challenges. Transformer ’s relatively mature theoretical support and tech-\nnological development in the field of Natural Language Processing have brought it into\nthe researcher’s sight, and it has been proved that Transformer ’s method can be used for\ncomputer vision tasks, and proved that it exceeds the existing CNN method in some tasks.\nIn order to enable more researchers to better understand the development process of\nobject detection methods, existing methods, different frameworks, challenging problems\nand development trends, paper introduced historical classic methods of object detection\nused CNN, discusses the highlights, advantages and disadvantages of these algorithms.\nBy consulting a large amount of paper, the paper compared different CNN detection\nmethods and Transformer detection methods. Vertically under fair conditions, 13 differ-\nent detection methods that have a broad impact on the field and are the most mainstream\nand promising are selected for comparison. The comparative data gives us confidence in\nthe development of Transformer and the convergence between different methods. It also\npresents the recent innovative approaches to using Transformer in computer vision tasks.\nIn the end, the challenges, opportunities and future prospects of this field are summarized.\nKeywords Computer vision. Object detection. Real-time system. CNN . Transformer\nhttps://doi.org/10.1007/s11042-022-13801-3\n* Kurban Ubul\nkurbanu@xju.edu.cn\n1 College of Information Science and Engineering, Xinjiang University, Urumqi 830046, China\n2 The Key Laboratory of Multilingual Information Technology, Xinjiang University, Urumqi 830046,\nChina\nMultimedia Tools and Applications (2023) 82:21353–21383\nReceived: 11 March 2022 / Revised: 19 July 2022 / Accepted: 5 September 202 /\n# The Author(s) 2022\nPublished online: 21 October 2022\n1 Introduction\nObject detection is the most important research direction in many computer vision (CV) tasks.\nThe task and goal correctly classify the objects category and location in the given picture with\nrectangular bounding box.\nAccording to the development process, object detection can be divided into two stages, first is\nthe traditional object detection method, and second is the current CNN algorithms. The traditional\nobject detection models such as VJ detector [83], HOG [15] etc. are not good enough, calculation\namount is huge, the calculation speed is slow, a nd it may produce multiple correct recognition\nresults instead of the results we want, so they no longer meet the current needs.\nResearchers have proposed many innovative new detection methods and these methods\nalso brought new problems. For example, the detection speed of the two-stage object detection\nmethod is slow, the detection accuracy of the one-stage object detection method is low, and the\ntransformer-based detection method requires a lot of training data. It is important for re-\nsearchers to sort out how these limitations come about and how ideas for improvement come\nabout. This is also the core idea of our survey paper. By reviewing, discussing and comparing\nthese detection methods in the paper, hope to researchers can have better understanding on\nthese detection methods and provide new ideas for proposing new methods.\nAt present, the CNN algorithm has an absolute position in both the general target detection\nmethod and the special object target detection method. Almost all object detection algorithms\nuse CNN as their backbone until this article [ 18] was published. It is the first to use the\ntransformer, which is widely used in NLP, directly in image processing without any modifi-\ncation. Since then, a new path has been opened up for computer vision, and it can be seen from\nrecent research work that this method of transformer used in image processing has a tendency\nto completely replace convolutional neural networks.\nThe paper investigated total of 237 papers from the upstream task model of feature\nextraction to the downstream task model of object detection, observed the problems actually\nsolved by these papers and classified them differently, after discussion, finally according to our\npurpose, that is, help readers to developed more efficient methods in future and consider the\nlength of the paper, 98 papers are referenced. It also investigates different literatures, data and\nexperiments, compare different detection algorithms, and conduct comparative experiments on\ndifferent mainstream algorithms. Article structure shown in Fig. 1.\nThe historical development and technical cha llenges of object detection introduced in the\nsecond part of the article. Different state-of -the-art (SOTA) algorithms used CNN and Trans-\nformer listed and discussed in Section 3. The fourth section mainly i ntroduces the comparative\nanalysis between different types of object detectors. In the fifth section looks forward to the future\nresearch direction of target detection, and paper is concluded in last section.\n2 Background\nObject detection is an extension of object classification in CV. It not only needs to classify the\ntarget in an image but also locate the object. Therefore, computer vision tasks such as instance\nsegmentation [30], object tracking based on frames [ 53] or temporal information [ 37]a r ea l l\nbased on object detection.\nNowadays, with the continuous deepening of related technologies and increasingly pow-\nerful performance of hardware devices, the application of object detection can be seen in\n21354 Multimedia Tools and Applications (2023) 82:21353–21383\nindustry [ 75] and daily life. The object detection can achieve such achievements, it is\ninseparable from the previous research work. Figure 2 shows key milestone of the object\ndetection methods. This figure was made using the improvement of the figure in [ 1].\nAlthough the existing object detection methods have made great advance, there are still\nmany challenges [ 49]a sf o l l o w s :\n& Multi-scale object detection model:\nThe scales of different object instances in real scene images often vary greatly. Among them,\nlarge-scale objects have the characteristics of large area and rich features and are easy to be\ndetected, while dense object objects and small-scale objects are difficult to be detected due to\nFig. 1 Structure of the paper\nFig. 2 Classical methods in the development history of target detection and their proposed time\n21355Multimedia Tools and Applications (2023) 82:21353–21383\nfewer features available. The multi-scale object detection methods ’ [4, 55]g o a li st od e t e c ta l l\nobjects of different scales in an image.\n& Real-time object detection model:\nIn industrial production such as self-driving cars, real-time requirements for object detection\nare put forward. The real-time object detection model needs to consider the memory require-\nments, computation cost, balance the detection performance and real-time requirements. In this\nregard, in except the YOLO series [ 2, 65–67] that have achieved good results, there are other\nalgorithms that have made efforts in lightweight networks [ 8, 32, 33, 35, 71, 91].\n& Weakly-supervised detection model:\nThis type of object detection tasks usually refers to the training samples that only give image-\nlevel labels and lack the object boundary annotation box. The performance of these detection\nmodels is largely determined by the number of labeled training samples. However, collecting\nbounding annotation boxes of objects to be detected in images is a time- and labor-intensive\ntask, and in some special cases, it is hard to get supervise training samples. Moreover, in real\nlife, due to the small number of rare object classes, there are few or no such samples in our\ntraining set, which makes the model unable to detect these classes. Therefore, some researchers\nhave conducted Weakly Supervised Object Detection research [ 85], Few-Shot Object Detec-\ntion [38] and Zero-Shot Object Detection research [ 64].\n& Solve imbalance of training samples in object detection model:\nUnbalanced training samples is a challenge in object detection research. Random sampling\nfrom an image will produce a lot of negative samples. Those negative and positive samples are\nextremely imbalanced. In addition, the difficulty of detection and recognition of training\nsamples is not the same, so there will be an imbalance of difficult and easy samples. In the\ndetection of multiple types of targets, some rarer samples are more difficult to obtain, causing\nthe imbalance of sample classes. In recent years, many researches [ 5, 59, 73] try to solve the\nproblem of unbalanced training samples in the field of target detection by improving the\nsampling method of training samples or adjusting the weight of samples in the loss function,\nand by studying the relation between samples.\n3 Object detection\nIn this section, the paper will introduce different object detection methods. Object detection\nalgorithms divided into two types according to the feature extraction methods used, one is a\nCNN-based object detection method, and the other is a transformer-based object detection\nmethod. Each type contains a different method of object detection, and in order to allow the\nreader to have a better understanding of these methods and more thinking about object\ndetection, The paper list the advantages, disadvantages and areas for improvement of each\nmethod.\n21356 Multimedia Tools and Applications (2023) 82:21353–21383\n3.1 Convolutional neural networks used on object detection\nAfter the great success of deep learning, many researchers have focused on object detection\nmodels using CNN. There have been many object detection models with simple structures and\ngood effects, which make these models can be widely used.\nDetection models need to solve two core problems, one is object classification, and the\nother is object positioning. The detection system first determines whether there are objects of\ninterest and identifies the object category, and output the most likely category scores of these\nobjects. Object positioning problem is based on the image classification. We also want to\nknow where the object is in the image. Use rectangular boxes to frame the identified objects,\nfor example, the rectangular box is given by determining the coordinates of the upper left and\nlower right corners, or by determining the upper left coordinate and rectangular boxes ’ length\nand width.\nIn CNN, definition of a feature map is a new image obtained by applying a convolution\nkernel on an image. There are as many feature maps as convolution kernels in a network layer.\nSize of the receptive field on the feature map is equivalent to how large the area of original\nimage is affected by the pixels in the high-level feature map.\nObject detection algorithms roughly divided into two types: one stage object detection and\ntwo stage object detection.\n3.1.1 Two stage algorithms (candidate-based algorithms)\nTwo stage algorithms require two stages to perform object detection tasks. The two stage\nalgorithms first extracts candidate regions from the image, these called region proposal, and\nthen use CNN to classify and locate the region proposal to get the results. Two stage\nalgorithms results’ accuracy relatively high, but the detection speed is slow. R-CNN, SPP-\nNet, Fast R-CNN, Faster R-CNN are typical two stage object detection algorithms. It will also\nanalyze these classic algorithms from different perspectives.\nThe detection steps of the two-stage algorithms are mainly as follows: first extract features,\nsecond extract region proposal, and third is classification, positioning.\n1) R-CNN (Region-CNN)\nAfter R-CNN [ 26] was launched, an improved R-CNN series appeared on this basis. We can\nsay R-CNN is the earliest successful algorithm, which used deep learning in object detection.\nAlthough R-CNN has successfully used deep learning for object detection tasks, it is actually\nmore accurate to say that it combined the deep learning methods and traditional methods. In\nthe development of R-CNN, the traditional method of selective search [ 81]i su s e dt oe x t r a c t\nregion proposal, and SVM is used for this region proposal classification.\nFigure 3 shows that R-CNN detection divided into four:\n1. Receive input images.\n2. Extract about 2000 region proposals.\n3. Input region proposals and compute CNN features.\n4. Use support vector machines (SVM) to determine classification.\n21357Multimedia Tools and Applications (2023) 82:21353–21383\nAbout the extraction of feature maps, the training and testing of models and other issues\nwill not be introduced in our paper.\nAlthough R-CNN achieved good results at the time, it also had many disadvantages:\n& Training time is long: Training is carried out in multiple stages, and the corresponding\nfeature map of each different candidate region is computed separately.\n& Space occupation is large: Feature map of each candidate area is saved for subsequent\noperations, which will lead to a large amount of space occupation.\n& Long test time: The calculation of the feature map of each region proposal is calculated\nseparately and not shared. Therefore, the amount of calculation for each picture during the\ntest is also huge, and it takes a long time.\n2) SPP-NET (Spatial Pyramid Pooling Network)\nAiming at a series shortcoming of R-CNN, for instance: input images are fixed size images,\nand the feature extraction is inefficient. He et al. proposed SPP-Net [ 29]. Figure 4 shows that\nSPP-Net does not require a fixed size for the input images, but feeds entire image to the\nconvolutional layer.\nThe introduction of the SPP layer is as follows:\nFigure 5 shows the structure of a SPP-Net. As shown in the figure above, a black feature\nmap will be obtained after a picture passed a convolutional layer. Specifically, the input feature\nmap is divided into blocks of different scales, such as 4*4, 2*2, and 1*1 in the three different\nscales (scale of the block is not solid), and then max pooling is used in each block, and then it\nbecomes a vector with a fixed length of 16 + 4 + 1. Therefore, after images of different scales\nare input, they will get vectors of the same length after passing through the SPP layer.\nAlthough SPP-net has made many improvements to R-CNN, it still inherits the multi-stage\nprocess of R-CNN. For example, use the selective search method to extract 2000 candidate\nregions, use CNN network to extract the feature map, and input extracted features into the\nFig. 3 The four steps of R-CNN [ 26]\n21358 Multimedia Tools and Applications (2023) 82:21353–21383\nSVM for classification, etc. However, it also pointed out some shortcomings of R-CNN, as\nsummarized at the beginning, and proposed new methods for these shortcomings. The\ndifference in their operating procedures can be seen more intuitively in Fig. 6.\nThe innovation of SPP-Net:\n& Input a whole picture directly into the convolutional neural network, so as to avoid the\nrepetition of feature extraction separately for every region proposal.\n& After using the SPP-Net layer, model can accept different scale of images.\nFig. 4 SPP-Net directly inputs the image to the convolutional layer without cropping or wrapping [ 29]\nFig. 5 Structure of a spatial pyramid pooling layer. Here N represent for the filter number of the convolutional\nlayer. [29]\n21359Multimedia Tools and Applications (2023) 82:21353–21383\nLimitations of SPP-Net:\nSPP-Net is difficult to fine-tune the parameters of the network before the SPP-layer, so that\nthe efficiency becomes very low, when doing fine-tuning each training sample (RoI: region of\ninterest) comes from a different image, the backpropagation efficiency of the SPP layer\nbecome very low, so that it re-produce a new feature map for each image, which is inefficient,\nand Fast-RCNN has improved it.\n3) Fast-RCNN\nAlthough SPP-Net solves some of the shortcomings of R-CNN, SPP-Net inherits many\nmethods of R-CNN, so it still has some problems to be solved, such as training steps are\ntoo many (including train the SVM classifier) and occupation of hardware space is still big. So\nas to solve the problems of slow R-CNN training speed and large space requirement, the\nauthor of R-CNN, improved R-CNN and proposed Fast R-CNN [ 25], which absorbed the\ncharacteristics of SPP-Net. These modifications greatly improve the detection speed. Its\narchitecture shown in Fig. 7.\nFast R-CNN algorithm achieved amazing results. Compared with Faster R-CNN and SPP-\nNet, the training speed is 9 times faster than the former and 3 times faster than the latter, test\nspeed also much faster than both.\nFast RCNN has the following improvements over RCNN:\n& Change the last convolutional layer to region of interest pooling Layer.\n& Multi-task Loss is proposed, which directly adds the bounding box regression to the\nnetwork for training, and includes the region proposal classification loss and position\nregression loss.\nFig. 6 Difference between R-CNN and SPP-Net is that the former applies feature extraction on 2000 candidate\nregions, while the latter directly apply feature extraction on images [ 29]\n21360 Multimedia Tools and Applications (2023) 82:21353–21383\n& Fast RCNN no longer uses support vector machine (SVM) but uses SoftMax for classi-\nfication, and at the same time uses Multi-task Loss to add bounding box regression to the\nnetwork, so that entire training only includes the two stages of extracting candidate regions\nand convolutional neural network training.\nLimitations of Fast R-CNN:\nThe main limitation is that the extraction of the region proposal uses selective search, which\nis still obtained from outside the network. Most of the object detection time is spent on region\nproposal, which is also one of the improvement directions of the Faster RCNN.\n4) Faster R-CNN\nFrom the release of Faster R-CNN [ 68] in 2015 to the present, its performance is still very\ngood. Instead of using a selective search algorithm, Region Proposal Network (RPN) used in\nthis algorithm. This RPN helps to reduce irrelevant region proposals to a large extent, and this\nmakes the image processing speed of the model greatly improved.\nRPN will let this model directly use convolutional neural networks to generate region\nproposal. Object boundary and score of each boundary box will be predict at the same time.\nRPN can be trained with Fast R-CNN at the same time to achieve end-to-end optimization.\nProcess of Faster R-CNN shown in Fig. 8 [1].\nFaster R-CNN even compared with recent algorithms, it still a good algorithm, but there are\nalso the following improvements:\nFig. 7 Architecture of Fast R-CNN [ 25]\nFig. 8 Detection process of Faster R-CNN [ 1]\n21361Multimedia Tools and Applications (2023) 82:21353–21383\n1. To classify each region proposal, large calculation is still required.\n2. Compared with the previous object detection algorithms, although its speed has been\nimproved a lot, it still cannot achieve real-time detection results.\nLimitations of Faster R-CNN:\n& Relative to the one-stage algorithms, the speed is slower.\n& The background false detection rate is high; only some negative samples are used for\ntraining, and the background learning is relatively not very sufficient.\n& The Non-Maximum Suppression (NMS) is not friendly to occluded objects.\n3.1.2 One stage algorithms (regression based algorithms)\nThe one-stage algorithms directly generate the positioning coordinates and classification\nprobability of objects in the image, without the need to generate region proposal in advance.\nBecause there is one less computationally intensive step, their detection speed is very fast, and\nthe detection result can be obtained directly. Their representative algorithms will be introduced\nbelow.\n1) YOLO series\nThis series is just like its name, You Only Look Once (YOLO) [ 67], this algorithm can detect\nthe target in the image at an extremely fast speed by just looking at the image once.\nIts central idea is to combine the two steps of generate region proposal and detection in the\nobject detection task, and then treat it as a regression problem to solve. Therefore, the YOLO\nseries of algorithms can know the target classification and positioning in the image at one time.\n& YOLOv1\nIf look at the structure of Yolo as a whole, we find that the structure is not very complicated,\nthat is, convolution, pooling, and full connections are added at the end. So, it is similar to the\nordinary CNN object classification model. The most obvious difference is that linear function\nused as activation function at final output layer, because it not only to predict the class of the\nobject, but also to locate the position of the bounding box. The structure shown in Fig. 9.\nFig. 9 Structure of YOLOv1 almost has no difference from other ordinary classification network [ 67]\n21362 Multimedia Tools and Applications (2023) 82:21353–21383\nYOLOv1 has the following advantages:\n1. Pioneeringly transform the detection task into a regression problem. Classification and\npositioning proceed at the same time.\n2. The model has strong generalization and can be extended to other fields.\nYOLOv1 also has the following shortcomings:\n1. If there are multiple objects that are close together, or the objects are relatively small, the\ndetection effect will be poor.\n2. Poor generalization ability for different shapes of the same object.\n3. Because of the loss function, the main reason that affects the detection effect is positioning\nerror.\n& YOLOv2\nRedmon et al. [ 65] released a new version of YOLOv2. They designed a brand new backbone\nnetwork called Darknet-19 which included 19 convolutional layers and 5 max pooling layers.\nYOLOv2 no longer uses dropout and added batch normalization layer after each convolutional\nlayer. A union training method for detection and classification is proposed. A different model\ncalled YOLO9000, which can detect around 9000 types of objects is trained on COCO and\nImageNet dataset under this union training method. Therefore, this article actually contains\ntwo models: YOLOv2 and YOLO9000, but the latter is based on the former, and the main\nstructure of the two models is the same.\n& YOLOv3\nThe biggest changes in YOLOv3 [ 66] include two points: the use of residual module and FPN\narchitecture. The backbone of YOLOv3 is a residual model which called Darknet-53, because\nit contains 53 convolutional layers. From perspective of the network structure, compared with\nDarknet-19 network, it can be built deeper. Another point is to use FPN architecture to achieve\nmulti-scale detection.\n& YOLOv4\nThe YOLOv4 [ 2] algorithm follows the original YOLO framework and adopts the best\noptimization strategy in recent years. It improves in many ways, like data processing, network\ntraining, activation function, loss function, etc. Although there is no theoretical innovation, it is\nwelcomed by many engineers.\nThe following three points are its contribution:\n1. Contributed a high-efficiency and high-accuracy object detection model that can also be\ntrained on ordinary consumer-grade GPU.\n2. Verify the impact of a series of SOTA target detector training methods.\n3. The effect has reached a new benchmark for target detection that achieves a balance\nbetween FPS and Precision.\n21363Multimedia Tools and Applications (2023) 82:21353–21383\nLimitations of different versions of the YOLO series:\n& YOLOv1: Detection of small targets is not very good; Each cell can only generate 2 boxes\nand can only have one class; Low recall rate.\n& YOLOv2: Although many tricks are used to improve performance, detection effect on\nsmall objects is still not well improved.\n& YOLOv3: Compared with the R-CNN series, the accuracy is still lacking; At the same\noutput layer, the same anchor, the data that is filled first, will be overwritten by the latter.\nFor example, if there is a cat and dog of similar size in the same location, yolo may only\ndetect one.\n& YOLOv4: The performance improvement of this method mainly uses a lot of tricks.\n2) SSD (Single Shot Multibox Detector)\nAfter this algorithm [47] was released in 2016, its perform ance surpassed YOLOv1. When this\nalgorithm applies the innovat ive multi-scale feature map to the detection task, the new target\ndetection algorithms [23, 45] proposed since then have more or less used the concept of multi-scale.\nThe positioning of the object is related to the local information, and the local information is\nexpressed by shallow feature maps. Object classification is related to the segmentation\ninformation, and segmentation information is expressed by deep feature maps. Therefore,\nconvolution on multi scale feature maps can raise the detection accuracy.\nSSD maintains the accuracy of Fast-RCNN while maintaining the detection speed of\nYOLO. This is because SSD put regression idea of YOLO and the anchor mechanism of\nFast-RCNN in one model, and uses multi-scale regions in different positions of the image for\nregression.\nBased on VGG-16, using the first five convolutions of VGG, 5 convolution structures\nstarting from Conv6 are added, and the input image requires 300*300. Figure 10 shows the\nstructure of SSD.\nFig. 10 Network structure of SSD [ 47]\n21364 Multimedia Tools and Applications (2023) 82:21353–21383\nThe results achieved by SSD are very good. When mean average precision (mAP) reaches\n74.3%, the speed is also very fast, reaching 59FPS. Although mAP of Faster R-CNN is also\nvery high, the speed is only 7FPS. The mAP of YOLOv1 is 63.4%, and the speed is 45FPS.\nEven if the resolution of the input picture is low, it can still achieve good results. Experimental\nresults show that the method used by the author does not reduce the accuracy due to the\nincrease in speed. Of course, there are some shortcomings. For example, the default box needs\nto be manually set, and the detection performance of small objects are not better than Faster R-\nCNN.\nLimitations of SSD:\n& The min size, max size and aspect ratio values of the prior box need to be set manually;\n& It is still not effective in detecting small objects.\n3) RetinaNet [ 46]\nThe two stage object detection methods have high accuracy and slow speed. When we speed\nup the detection speed of one-stage object detection methods, the accuracy will decrease.\nTherefore, the author brings a new method that can achieve the accuracy of two-stage detection\nin one stage algorithm.\nLow accuracy problem in one-stage object detection methods is caused by the imbalance of\npositive and negative samples. The Focal loss proposed by the author is a modification of the\ncross-entropy loss function, which can balance the imbalance of negative and positive samples.\nThrough the experimental data, we can see that Focal loss effectively solves the imbalance of\npositive and negative samples problem in the one stage object detection, and improves the\naccuracy of the model.\nRetinaNet reached the high accuracy as Faster R-CNN. The formula of Focal Loss is not\nfixed, it can also have other forms, the performance has no big difference, so the expression of\nF o c a lL o s si sn o tc r i t i c a l .\nFor verify the focal loss, a simple one-stage detector is designed. The network structure is\ncalled RetinaNet. It is more like the combination of Resnet + FPN. For feature extraction\nResNet selected as the backbone. The role of FPN is to enhance the utilization of multi-scale\nfeatures formed in Resnet to obtain more expressive feature maps containing multi-scale target\nregion information. Figure 11 shows the basic structure of RetinaNet:\nFocal loss has largely alleviated the imbalance between positive and negative samples, but\nthere are also some areas to be improved:\nIt is susceptible to noise interference, so the correct labeling of samples is very demanding.\n4) CenterNet\nCenterNet [19], released in April 2019, a new anchor-free detection method proposed on the\nbasis of CornerNet [ 40], which constructs triples for object detection, and greatly exceeds all\nexisting one-stage methods on the MSCOCO dataset.\nIn fact, it is difficult to match the upper left corner and lower right corner of the object\nframe, because for most objects, these two corner points are outside of the object, and the\nembedding vector of the two corner points can ’t perceive the internal information of objects\nvery good. Therefore, Cascade Corner Pooling is used instead of the original Corner Pooling.\nFirst it extracts the maximum value of the object boundary (Corner Pooling), then continously\n21365Multimedia Tools and Applications (2023) 82:21353–21383\nextract the maximum value from the boundary, and adds it to the maximum value of the\nboundary to combine more internal information.\nCompared the AP with other one-stage and two-stage methods in the COCO dataset, we\ncan find that the optimal performance surpasses the current one-stage method and most two-\nstage methods. There are also some improvements [ 50, 97] on the basis of CenterNet, and\nachieved good results. Figure 12 is the Structure of CenterNet.\nCenterNet also has disadvantages:\nIn the training and prediction process, if the center point of two objects overlaps after\ndownsampling, then CenterNet can only detect one center point and identify the two objects as\none object.\nFig. 11 RetinaNet basic structure [ 46]\nFig. 12 Structure of CenterNet [ 19]\n21366 Multimedia Tools and Applications (2023) 82:21353–21383\n3.2 Object detection with transformer\nSince AlexNet in 2012, CNN has dominated all directions in the field of CV tasks. With the\ndeepening of research, various methods have emerged. The relationship between the attention\nmechanism in NLP field and the CNN in CV field is becoming more and more obvious. The\nuse of Transformer [ 82] to handle various computer vision tasks has become more and more\nmainstream, and some of the characteristics of Transformer have made up for the shortcomings\nof CNN.\nSeveral works related to Transformer in the field of object detection will introduce in\nbelow:\n& The earlier work that used combination of Transformer and CNN for classification and\ninstance segmentation includes DETR [ 6] and D-DETR [ 98].\n& ViT [ 18], which used pure Transformer direct ly for classification, pioneered a\nTransformer-based computer vision model.\n& With the idea of ViT, a new universal backbone, Swin Transformer [ 51], that uses\nTransformer to handle computer vision tasks is proposed.\n1) Detection Transformer (DETR)\nDETR [ 6] can be regarded as the pioneering work of Transformer in the field of target\ndetection. The result of DETR is very good, and the DETR based on ResNet50 matches the\nperformance of Faster-RCNN after various finetunes.\nIn fact, the entire DETR architecture is easy to understand. It contains three main\ncomponents:\na) CNN backbone network\nb) Encoder-decoder transformer\nc) A simple feedforward networks\nThe CNN backbone network generates feature maps from the input images. Then, the output\nof the CNN backbone network is converted into a one-dimensional feature map and passed to\nTransformer encoder as input. The output of this encoder is N fixed-length embeddings\n(vectors), where N is the number of objects in the image assumed by the model. The\nTransformer decoder use the encoder-decoder attention mechanism to decodes these embed-\ndings into bounding box coordinates.\nFinally, normalized center coordinates, height and width of the bounding box predicted by\nfeedforward neural network, and linear layer uses the SoftMax function to predict the category\nlabel. The above flow and DETR structure shown in Fig. 13.\nAdvantages of DETR:\n1. Propose different path for object detection and need less prior information.\n2. Although the accuracy and efficiency are not the highest, they are comparable to the\nhighly optimized Faster R-CNN on COCO dataset. The detection effect will be better on\nlarge object.\n3. DETR does not require any custom layers, so the model is easy to rebuild, and the\nmodules involved can be found in any deep learning framework.\n21367Multimedia Tools and Applications (2023) 82:21353–21383\nThe disadvantages are also obvious:\nDETR has problems such as slow convergence speed, poor detection accuracy, and low\noperating efficiency\n2) Vision Transformer (ViT)\nThe author tries to apply the standard Transformer directly to the image with minimal\nmodification. The framework of ViT shown in Fig. 14.\nFig. 13 The process of DETR and its structure [ 6]\nFig. 14 Vision Transformer structure [ 18]\n21368 Multimedia Tools and Applications (2023) 82:21353–21383\nThe main task of the ViT is to do classification tasks. The main idea is to use the\nTransformer Encoder part to do classification. Like NLP, the classification token will be\nadded to the picture sequence. The picture sequence is obtained by cutting a picture into\nmultiple patches. Number of patches N = HW/P2,where the H, W is stand for height and width\nof input image, and P2 is the resolution of each image patch.\nIn the model design, the author follows the original transformer as much as possible. One\nadvantage of this deliberately simple setup is that the scalable NLP transformer architecture\nand its effective implementation can be used almost immediately.\nwhen image sliced into patches, location information of each patch must be added before it\nis sent to the Transformer encoder, because in the linear projection process this information\nwill be lost. Insert another vector, which is independent of the analyzed image and represents\nglobal information about the entire image. In fact, the output corresponding to the patch is\npassed to the MLP, and the MLP will return the prediction class. However, the loss of\ninformation in this process is very serious. In fact, during the conversion from patch to vector,\nany information about the position of the pixel in the patch will be lost. Therefore, some\nresearchers have proposed new solutions [ 27] to this problem. In addition, ViT also has many\nproblems such as large demand for data, large amount of computation, and inability to encode\nlocation embedding.\n3) Swin ViT\nIn addition to the ViT mentioned above, there are also work such as iGPT [ 9]t h a tu s e\nTransformer in image classification. But these methods have the following two serious\nproblems:\n1) A picture needs at least a few hundred pixels to express the content. To process so many\npixels in a sequence is not what Transformer good at.\n2) The previous methods are all to find the solution to object classification, but theoretical,\nTransformer is better at finding answer to detection problem, but the ability to solve the\ndense prediction scene of instance segmentation needs to be improved.\nSo as to solve the above problems, the author present Swin Transformer, and successfully\nmade the model achieve SOTA results in classification, detection and instance segmentation.\nAt present, Swin Transformer has become the universal BackBone when Transformer is used\nin visual tasks.\nSwin Transformer does not use small-resized images as input images like the previous ViT\nand iGPT, but directly inputs the original images, so that there will be no information loss\ncaused by resize. One advantage of Swin Transformer is the use of the most commonly used\nhierarchical network structure in CNN. With the deepening of the hierarchical network\nstructure, the receptive field of the node expands. The receptive field of the node will expand\nwith the deepening of the hierarichical network structure. This characteristic is also given to\nthe Swin Transformer, and this give Swin Transformer the ability to detect and segment\nobjects like FPN [ 45] and U-Net [ 69] structures. The hierarchical network structure of ViT and\nSwin Transformer is shown in Fig. 15.\nIts advantages are shown in the following three points:\n1. Achieved faster landing of Transformer in CV field.\n21369Multimedia Tools and Applications (2023) 82:21353–21383\n2. Swin Transformer combines the advantages of Transformer and CNN, and a hierarchical\nstructure used to reduce the resolution and increase the number of channels.\n3. Achieved SOTA results in different visual tasks.\nThere are the following disadvantages:\n& The computational coast is significant.\n& High demands on GPU memory.\n& Precise fine tuning is required in some project applications.\n4) Twins\nTwins [11] proposed two new architectures, named Twins-PCPVT and Twins-SVT.\nThe first architecture, Twins-PCPVT, structure shown in Fig.16, replaces the positional coding\nin PVT [87] (the same fixed-length learnable positional coding as DeiT [80]) with the Conditional\nPositional Encodings proposed by the team in CPVT [12]. Large performance improvements can be\ndirectly obtained on classification and downstr eam tasks, especially on dense tasks. Since the\nconditional position coding CPE [12] supports variable length input, the visual Transformer can\nflexibly process features from different spatial scales. This architecture shows that PVT can match\nor surpass the performance of Swin only through CPVT ’s conditional position coding enhance-\nment. This finding confirms that the reason why PVT performance is inferior to Swin is the use of\ninappropriate position coding. It can be seen that position codes such as CPE, which can flexibly\nhandle varying resolutions, have a great impact on downstream tasks.\nThe second architecture, Twins-SVT, optimizes and improves the attention strategy based\non a detailed analysis of the current global attention. The new strategy integrates the local-\nglobal attention mechanism. The author compares it to the depth-wise separable convolution in\nthe convolution neural network is named Spatially Separable Self-Attention (SSSA). Different\nFig. 15 Main difference between Swin Transformer and ViT [ 51]\n21370 Multimedia Tools and Applications (2023) 82:21353–21383\nfrom the depth separable convolution, the spatially separable self-attention (Fig. 17) proposed\nby Twins-SVT is to group the spatial dimensions of the features to calculate the self-attention\nof each group, and then perform grouping attention results from the global fusion.\nThis approach needs to improve as follows:\n& The training phase requires processing images of a fixed input size\nSome modules designed for CNNs are not available in models.\n3.3 The latest object detection research\nIn recent years, based on the ideas of the above-mentioned classic object detection algorithms,\nnew object detection algorithms have continuously emerged.\nFig. 16 Structure of Twins-PCPVT [ 11]\nFig. 17 Application of LSA and GSA in Twins-SVT. LSA: locally-grouped attention. GSA: global sub-sampled\nattention [11]\n21371Multimedia Tools and Applications (2023) 82:21353–21383\nWhether it is based on CNN or transformer, all thinking that detection system can be faster\nand more accurate.\nThe accuracy of the convolution-based object detection methods largely depends on its\nfeature extraction backbone network. A good backbone network can design an excellent object\ndetection method. The detailed introduction of the classic backbone network is in this article\n[1].Also used the new backbone Densenet [ 34] to design some novel target detection models\nlike STDN [ 96], DSOD [ 72] and TinyDSOD [ 41]. There are also lightweight networks\ndesigned to adapt to devices with limited performance, such as SquezzeNet [ 35], MobileNets\n[32], MobileNetv2 [ 71], MobileNetv3 [ 33], ThunderNet [ 62], ShuffleNet [ 95], ShuffleNetv2\n[54], PeleeNet [ 86] and MnasNet [ 78].\nIn addition, there are some excellent CNN-based classical backbone networks and detectors\nthat researchers are constantly exploring with different techniques, methods and combinations,\nsuch as ConvNet [ 52], YOLOX [ 24]. And some academics use their own unique methods to\nexplore new paths in object detection [ 63].\nAlthough the application of pure Transformer [ 18] in cv has hardly attracted the attention of\nothers, recent researches on target detection has been almost entirely occupied by transformers.\nOn the basis of the above-mentioned transformer-based object detection method, different\nmethods with greatly improved effects have appeared, such as, Focal Transformer [ 92],\nCSWin Transformer [17], CBNetV2 [ 43] and Mobile ViT [ 57], the first to apply Transformer\nto mobile Moreover the research in the direction of object detection, many methods of using\ntransformers to solve problems have also appeared in other directions in CV field, such as,\nSegFormer [90], MaskFormer [ 10], TransGAN [ 36], TNT [ 27], DVT [ 88], YOLOS [ 22].\nObject detection based on CNN and Transformer methods is also gaining more and more\nattention for specific detection tasks outside the mainstream field. For example, [ 76]u s e df o r\nclassification of skin disease, CenterPoint [ 93] for 3D object detection, O2DETR[ 56]for\nremote sensing monitoring, SSPNet [ 31] for small object detection, and so on.\nAccording to recent papers published, transformers have great potential in processing CV\ntasks. The combination of CNN and transformers has also improved efficiency. This trend can\nalso be seen in some of the newly proposed methods, such as the use of Transformer with the\nconcept of convolution [ 28]. Therefore, transformers will go further in computer vision field.\n4 Comparison\nIn this section the paper will compare different angles of the different object detection\nalgorithms discussed above. We will not discuss and compare the algorithm optimization\nand learning rate during training in detail, and the discussion ideas for this can refer to [ 84].\nThe evaluation indicators used for the comparison will also be introduced. In Section 4.2,t h e\noperating results of different mainstream object detection algorithms are visualized and the\ncomputational complexity of these algorithms is compared. Our purpose is not to tell the\nreader which one is the best algorithm, but to be able to see the advantages and disadvantages\nof different algorithms.\n4.1 Data comparison\nIn this part, the paper compared several object detection methods in detail and comprehen-\nsively, and show the comparative data.\n21372 Multimedia Tools and Applications (2023) 82:21353–21383\nFirst, it briefly introduces the data sets and metrics that are often used in object detection\ntasks.\n& PASCAL VOC [ 20, 21]: This dataset has two versions of commonly used as standard\nbenchmarks. The datasets are divided into 4 categories: vehicle, household, animal, and\nperson, for a total of 20 subclasses. VOC 2007 has a total of 9963 images, train+val has\n5011 images, test has 4952, and the total objects are 24,640. VOC 2012 has a total of\n23,080 images, Train+Val has 11,540 images, and the total objects are 54,900.\n& ILSVRC [70]: From 2010 to 2017, one of the most sought after and authoritative academic\ncompetitions in the field of CV has represented the highest level in the field of imaging. It\ncontains the ImageNet [ 16] images.\n& MS-COCO [44]: The largest dataset with semantic segmentation so far. And it also the most\ndifficult and complicated object detection dataset available today. This dataset aims at scene\nunderstanding, mainly intercepted from complex daily scenes, and the objects in the image are\ncalibrated by precise segmentation. It is the largest dataset for semantic segmentation so far,\nthere are 80 categories provided, there are more than 330,000 images, 200,000 of which are\nannotated, and the number of individuals in the entire dataset exceeds 1.5 million.\n& Open Images [39]: About 9 million images span approximately 6000 categories, and these\ntags contain more real-life entities than ImageNet.\nTo estimate the performance of an object detector, there must be a unified standard. Next,\nseveral norms will introduce, those are used to evaluate the performance of a model.\n& Precision: precision is calculated from the perspective of the prediction result, which is the\nratio of true positive samples among the positive samples predicted by the model\n& This is the ratio of true positive samples amongthe positive samples predicted by the model.\n& Recall: recall rate is calculated from the real sample set, which means the number of\npositive samples recovered by the model from the total positive samples.\n& IoU: Intersection over Union iscalculated as the percentage of the overlap between the ground\ntruth and the predicted bounding box and the area of their union. As shown in Fig.18.\n& FPS: FPS is how many pictures the target network can detect per second. FPS is simply to\nunderstand the refresh rate of the image.\n& AP: AP is the average detection precision rates at different recall points.\nThe confidence level interval greatly from model to model, and may be 50% confidence\ninterval in our model equivalent to 80% confidence interval in other models, which makes\nFig. 18 The calculation of precision and recall and the expression form of IoU\n21373Multimedia Tools and Applications (2023) 82:21353–21383\nit difficult to evaluate between different models. To eliminate this assessment discrepancy,\nexperts proposed AP metrics.\n& mAP: mean Average precision measures the quality of the learned model in all categories,\nand it is one of the most important indicators in object detection.\nThe AP value calculation is only for one class, and the calculation of mAP after getting\nthe AP becomes very simple, that is, take the average after the APi of all classes (K) is\ncalculated. This process can be formulated as:\nmAP ¼ ∑K\ni¼1APi\nK ð1Þ\nThe above is most of the indicators used by most existing target detection models to test and\ncompare performance. Dissimilar models may be different in the use of datasets and the goals\nthey want to achieve. For example, some models are optimized for small object detection,\nsome models want to increase the ability of feature extraction, etc., but final goal is to quickly\nand accurately identify and locate objects in view.\nThe object detection technique has a long development history [ 22], and it may be some\nimproprieties in comparing the detection techniqueof different years together. Therefore, the paper\ntries to compare the different methods proposed in recent years using the same dataset and backbone\ns h o w na si nT a b l e1, in order to comparatively see advantages and disadvantages of these methods.\nAlthough there are some unfair factors in this method, for example, the backbone is not VGG when\nthe method proposed, such as, the backbone used by SPP-Net is ZF5 [94]. Some data is removed\nfrom VOC07 for comparison. It also did not compare the best mAP results of each target detection\nmethod, although mAP is the most important performance indicator, the performance of the model\nneeds to be considered comprehensively. For example, R-CNN when uses [74] the mAP result is\nobviously improved, but the calculation time consumption will also be longer.\nAlthough the object detection methods shown in Table2 are not explained in detail in our paper,\nthese methods contribute to the existing detection methods, such as Mask R-CNN [30] integrated the\nobject detection and instance segmentation fu nctions, and the detection and segmentation are\ncalculated in parallel. The paper also conducts mAP comparisons under relatively fair standards.\nAP [0.5,0.95] is added to the performance indicators of the MS COCO dataset, which means the\naverage mAP at different IoU thresholds (from 0.5 to 0.95, step size 0.05).\nTable 2 shows that as the object detection technology continues to update and innovate,\nsome of our understanding is also refreshed. Mask R-CNN is a completely innovative two-\nstage target detection method, and its mAP is also very high. Of course, its FPS is low. This is\nbecause of the characteristics of the two-stage object detection method. But we found that\nEfficientDet maintained a high FPS when mAP surpassed Mask R-CNN. This is what we are\npursuing, high FPS and high precision.\nTable 1 Capability comparison of different methods discussed in our paper in our paper.bb stands for bonding\nbox regression and /diff is without “difficult” examples in voc07\nFramework Backbone Dataset mAP\nR-CNN BB VGG16 VOC07 66.0\nSPP-Net BB VGG16 VOC07\\diff 63.1\nFast R-CNN VGG16 VOC07 66.9\nFaster R-CNN VGG16 VOC07 69.9\n21374 Multimedia Tools and Applications (2023) 82:21353–21383\nAfter Transformer is applied in field of CV, there have been many related studies. The\nmethods of object detection in these studies are compared in Table 3. It should be noted that\nmost of these transformer methods appear as backbones (Table 4). DETR is a combination of\nTransformer and CNN methods, so the existing backbone is used. In order to make the\ncomparison as fair as possible, the pure Transformer method uses the same framework, and\nsome adjustments have been made to the framework used [ 11].\n4.2 Visualization results and model complexity\nFive mainstream object detectors from the detection methods selected to discussed above to\nshow their detection effect as shown in Fig. 19, and compared their calculation cost, param-\neters and detection speed, as shown in Table 5. We can more intuitively see the characteristics\nof these mainstream detection methods from these test results and data.\n5 Future trends\nIn this part, the paper looks forward to the future development trend of object detection:\n& Lightweight detector\nAim to accelerate the compute speed of detection and enable it to run on mobile devices, some\nresearchers have made great efforts in recent years. But the current lightweight detection\nTable 2 Performance comparison between different methods. * refers to this method using Faster R-CNN on\nFPN\nFramework Backbone Dataset AP 0.5 AP [0.5,0.95]\nFaster R-CNN ResNet-101 MS COCO 48.5 27.2\nR-FCN [14] ResNet-101 MS COCO 48.9 27.6\nFPN [45]* ResNet-101 MS COCO 59.1 36.2\nMask R-CNN ResNet-101-FPN MS COCO 62.3 39.8\nDetectoRS [61] ResNeXt-101[89] MS COCO 71.6 53.3\nTridentNet [42] ResNet-101 MS COCO 69.7 48.4\nEfficientDet [79] EfficentNet[ 77]-B6 MS COCO 71.4 52.2\nHTC [7] ResNet-50 MS COCO 62.6 43.6\nTable 3 Performance comparison of one stage algorithms\nFramework Backbone Dataset AP 0.5 AP [0.5,0.95]\nSSD512 VGG16 VOC07 71.6\nSSD512 VGG16 MS COCO 46.5 26.8\nYOLO Modified GoogLeNet VOC07+VOC12 63.4\nYOLOv2 DarkNet-19 MS COCO 44.0 21.6\nYOLOv3 DarkNet-53 MS COCO 51.5 28.2\nYOLOv4 CSPDarknet53 MS COCO 64.9 43.0\nRetinaNet ResNet-101 MS COCO 53.1 34.4\nCenter Net Hourglass-104[ 58] MS COCO 62.4 44.9\n21375Multimedia Tools and Applications (2023) 82:21353–21383\nalgorithm is still not satisfactory. Therefore, in a future development trend of detection\nalgorithms, lightweight, fast and high-precision is the eternal theme of target detection.\n& Multi-task learning\nTable 4 Transformer based detection methods performance comparison on the COCO val2017\nFramework Backbone AP 0.5 AP [0.5,0.95]\nRetinaNet ResNet-101 53.1 34.4\nRetinaNet PVT-Medium 63.1 41.9\nRetinaNet Twins-PCPVT-B 65.6 44.3\nRetinaNet Twins-SVT-B 66.7 45.3\nRetinaNet Swin-S 65.7 44.5\nDETR ResNet-101 64.9 44.9\n(a) Faster R-CNN+ResNet-101 (b) RetinaNet+ResNet-101\n(c) Yolov4+DarkNet-53 (d) Mask R-CNN+Swin-T\n(e) DETR+ResNet-50\nFig. 19 The detection effect of five different mainstream detection methods. a Faster R-CNN + ResNet-101, b\nRetinaNet+ResNet-101,c Yolov4 + DarkNet-53, d Mask R-CNN + Swin-T, e DETR+ResNet-50\n21376 Multimedia Tools and Applications (2023) 82:21353–21383\nAiming at the current problem of low single-task learning and detection performance, a multi-\ntask learning method that combines multiple tasks in the network and multi-level features of\nthe network is proposed to improve detection performance and perform multiple computer\nvision tasks at the same time [ 48].\n& Weakly supervised object detection\nThe training of detection algorithms based on deep learning depends on great number\nof high-quality images with noted datasets, and model training process is often time-\nconsuming and inefficient. The use of weakly-supervised object detection can make the\ndetection algorithm use part of the bounding box labeled dataset for training. Therefore,\nWeakly supervised techniques are important to reduce labor costs and improve detection\nflexibility.\n& GAN-based object detection\nWhether it is based on convolutional neural network or Transformer, it requires great amount\nof image data to train. Using generative adversarial networks to generate fake images to\nproduce a large number of data samples to achieve data expansion. The real scene data is\nmixed with the simulation data generated by the GAN training target detector, so that the\ndetector has stronger robustness and generalization ability [ 3].\n& Small object detection\nDetecting small objects in scene images has been a long-standing challenge in the field of\nobject detection, and some potential applications of small object detection research directions\ninclude: the use of remote sensing images to count the number of wild animals, and detect the\nstatus of some important military targets, so how to solve the problem of small targets has\nalways been a hot topic for researchers.\n& Multi-modal detection\nWith the popularization and development of different sensors, the use of different sensors in\nthe field of target detection, such as depth cameras, lidars and other equipment to obtain target\ninformation, has made some progress in the past few years. In the field of autonomous driving,\ncars are often equipped with a complex array of sensors for accurate and robust environmental\nperception. How these large numbers of different types of sensors complement each other and\nfuse them to facilitate perception is still an open question.\n& Video detection\nTable 5 Model complexity of mainstream detection methods trained on COCO datasets\nMethods Flops (G) Param (M) Inference time (fps)\nFaster R-CNN+ResNet-101 283.14 60.52 15.6\nRetinaNet+ResNet-101 315.39 56.74 15.0\nYolov4+DarkNet-53 195.55 61.95 66.0\nMask R-CNN+Swin-T 263.78 47.79 15.3\nDETR+ResNet-50 91.64 41.3 –\n21377Multimedia Tools and Applications (2023) 82:21353–21383\nReal-time object detection/tracking in high-definition video is of great significance for video\nsurveillance and autonomous driving. Existing object detection algorithms are usually de-\nsigned for object detection in a single image, while ignoring the correlation between video\nframes. Improving detection performance by exploring the spatial and temporal correlations\nbetween sequences of video frames is an important research direction.\n6 Conclusion\nClassic detection methods, key technique, datasets and indicators are intruded in our paper. In\nthe past ten years, the key research content of object detection tasks has been CNN. However,\nthe new method that has appeared recently, that is, the application of Transformer in CV, has\nopened up a new path for object detection and has become a recent research hotspot. Some\npeople even think that Transformer will completely replace CNN in the future.\nThe following are our thoughts on the application of CNN and Transformer on object\ndetection tasks:\n1. CNN uses the convolution kernel to continuously extract high-level abstract features. In\ntheory, its receptive field should cover the entire image, but many studies have shown that\nits actual theoretical receptive field is larger than the receptive field, which is not good for\nto feature extraction with making full use of contextual information. On the other hand,\nthe advantage of the transformer is that it uses attention to capture global contextual\ninformation and build long-range dependencies on objects to extract more useful features.\nFrom the experimental results of ViT, Transformer has also learned the same receptive field\nfrom small to large as CNN, which further illustrates the relationship between Transformer and\nCNN [ 13]. Through the comparative test of CNN-based and Transformer-based object\ndetection methods as shown in Tables 3 and 4, some Transformer-based detection methods\nsuch as DETR and PVT, twins as the backbone network of RetinaNet have surpassed\nRetinaNet with ResNet-101 as the backbone network.\n2. Because CNN has the characteristics of inductive bias and transitional invariance, it is\neasier to deal with image problems. Transformer does not have this feature, so the training\nmodel requires much more data or stronger data enhancement [ 85] to learn image features.\n3. A large part of the current work is the mixed use of CNN and Transformer. The hybrid\nstructure of ViT also uses CNN to input feature maps into ViT, achieving better results\nthan pure Transformer.\nThe paper summarizes the characteristics and shortcomings of mainstream detection methods,\nand through comparative experiments, readers can have a deeper thinking about these test\nmethods. How to improve the shortcomings of these detection algorithms, how to combine the\nadvantages of CNN-based and Transformer-based object detection methods, we hope that our\npaper can give readers some help.\nFor the CNN that has been developed for more than ten years, it is very mature in tasks such\nas object detection, segmentation, and classification. It is difficult for the Transformer method\nto completely replace CNN in a short time. The focus of future work will be to add more\nexcellent features of CNN to Transformer [ 60].\n21378 Multimedia Tools and Applications (2023) 82:21353–21383\nAcknowledgments This work was supported by National Key Research and Development Program of China\n(No.2021YFB2802100) and National Science Foundation of China under Grant (61862061, 62061045).\nData availability All data generated during and/or analysed during the current study are available from the\ncorresponding author on reasonable request.\nDeclarations\nConflict of interests The authors declare that they have no conflict of interest.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and\nindicate if changes were made. The images or other third party material in this article are included in the article's\nCreative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included\nin the article's Creative Commons licence and your intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy\nof this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\n1. Arkin E, Yadikar N, Muhtar Y, Ubul K (2021) \"A Survey of Object Detection Based on CNN and\nTransformer,\" 2021 IEEE 2nd International Conference on Pattern Recognition and Machine Learning\n(PRML), pp. 99 –108, https://doi.org/10.1109/PRML52754.2021.9520732.\n2. Bochkovskiy, A, Wang, CY, Liao, HYM (2020) Yolov4: Optimal speed and accuracy of object detection.\nhttps://doi.org/10.48550/arXiv.2004.10934.\n3. Brock, A, Donahue, J, Simonyan, K (2018) Large scale GAN training for high fidelity natural image\nsynthesis. https://doi.org/10.48550/arXiv.1809.11096.\n4. Cai, Z, Fan, Q, Feris, RS, Vasconcelos, N (2016) A Unified Multi-scale Deep Convolutional Neural\nNetwork for Fast Object Detection. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds) Computer\nVision – ECCV 2016. ECCV 2016. Lecture notes in computer science(), vol 9908. Springer, Cham.\nhttps://doi.org/10.1007/978-3-319-46493-0_22.\n5. Cao Y, Chen K, Loy CC, Lin D (2020) \"Prime Sample Attention in Object Detection,\" 2020 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), pp. 11580 –11588, https://doi.org/10.\n1109/CVPR42600.2020.01160.\n6. Carion, N, Massa, F, Synnaeve, G, Usunier, N, Kirillov, A, Zagoruyko, S (2020) End-to-End Object\nDetection with Transformers. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, JM. (eds) Computer Vision –\nECCV 2020. ECCV 2020. Lecture notes in computer science(), vol 12346. Springer, Cham. https://doi.org/\n10.1007/978-3-030-58452-8_13.\n7. Chen K et al. (2019) \"Hybrid Task Cascade for Instance Segmentation,\" 2019 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pp. 4969 –4978, https://doi.org/10.1109/CVPR.2019.\n00511.\n8. Chen C, Liu M, Meng X, Xiao W, Ju Q (2020) \"RefineDetLite: A Lightweight One-stage Object Detection\nFramework for CPU-only Devices,\" 2020 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition Workshops (CVPRW), pp. 2997 –3007, https://doi.org/10.1109/CVPRW50498.2020.00358.\n9. Chen, M, et al. (2020) “Generative Pretraining From Pixels.” ICML 2020: 37th International Conference on\nMachine Learning, vol. 1, 2020, pp. 1691 –1703\n10. Cheng, B, Schwing, A, Kirillov, A (2021) Per-pixel classification is not all you need for semantic\nsegmentation Advances in Neural Information Processing Systems, 34\n11. Chu, X, et al. (2021) \"Twins: Revisiting the design of spatial attention in vision transformers.\" Advances in\nNeural Information Processing Systems 34 (NeurIPS 2021)\n12. Chu, X, Tian, Z, Zhang, B, Wang, X, Wei, X, Xia, H, Shen, C (2021) Conditional positional encodings for\nvision transformers. https://doi.org/10.48550/arXiv.2102.10882.\n21379Multimedia Tools and Applications (2023) 82:21353–21383\n13. Cordonnier, J-B, et al. (2020) “On the Relationship between Self-Attention and Convolutional Layers. ”\nICLR 2020 : Eighth International Conference on Learning Representations. https://doi.org/10.48550/arXiv.\n1911.03584\n14. Dai J, Li Y, He K, Sun J. (2016) R-FCN: object detection via region-based fully convolutional networks. In\nproceedings of the 30th international conference on neural information processing systems (NIPS'16).\nCurran associates Inc., red hook, NY, USA, 379 –387\n15. Dalal N, Triggs B (2005) \"Histograms of oriented gradients for human detection,\" 2005 IEEE Computer\nSociety Conference on Computer Vision and Pattern Recognition (CVPR'05), pp. 886 –893 vol. 1, https://\ndoi.org/10.1109/CVPR.2005.177.\n16. Deng J, Dong W, Socher R, Li L-J, Li K, Fei-Fei L (2009) \"ImageNet: A large-scale hierarchical image\ndatabase,\" 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248 –255, https://doi.\norg/10.1109/CVPR.2009.5206848.\n17. Dong, X, Bao, J, Chen, D, Zhang, W, Yu, N, Yuan, L, ..., Guo, B. (2021) Cswin transformer: A general\nvision transformer backbone with cross-shaped windows. https://doi.org/10.48550/arXiv.2107.0065.\n18. Dosovitskiy, A, et al. (2020) “An Image Is Worth 16x16 Words: Transformers for Image Recognition at\nScale.” https://doi.org/10.48550/arXiv.2010.11929.\n19. Duan K, Bai S, Xie L, Qi H, Huang Q, Tian Q (2019) \"CenterNet: Keypoint Triplets for Object Detection,\"\n2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 6568 –6577, https://doi.org/10.\n1109/ICCV.2019.00667.\n20. Everingham M et al (2010) The Pascal Visual Object Classes (VOC) Challenge. Int J Comput Vis 88(2):\n303–338\n21. Everingham M et al (2015) The Pascal Visual Object Classes Challenge: A Retrospective. Int J Comput Vis\n111(1):98–136\n22. Fang, Y, Liao, B, Wang, X, Fang, J, Qi, J, Wu, R, ..., Liu, W (2021) You only look at one sequence:\nrethinking transformer in vision through object detection. Adv Neural Inf Proces Syst, 34. https://doi.org/10.\n48550/arXiv.2106.00666\n23. Fu, CY, Liu, W, Ranga, A, Tyagi, A, Berg, AC (2017) Dssd: Deconvolutional single shot detector. https://\ndoi.org/10.48550/arXiv.1701.06659.\n24. Ge, Z, Liu, S, Wang, F, Li, Z, Sun, J (2021) Yolox: Exceeding yolo series in 2021. https://doi.org/10.48550/\narXiv.2107.08430.\n25. Girshick R (2015) \"Fast R-CNN,\" 2015 IEEE International Conference on Computer Vision (ICCV), pp.\n1440–1448, https://doi.org/10.1109/ICCV.2015.169.\n26. Girshick R, Donahue J, Darrell T, Malik J (2014) \"Rich Feature Hierarchies for Accurate Object Detection\nand Semantic Segmentation,\" 2014 IEEE Conference on Computer Vision and Pattern Recognition, pp.\n580–587, https://doi.org/10.1109/CVPR.2014.81.\n27. Han, K, et al. (2021) \"Transformer in transformer.\" Advances in Neural Information Processing Systems 34\n(NeurIPS 2021)\n28. Hassani, A, Walton, S, Li, J, Li, S, Shi, H (2022) Neighborhood Attention Transformer. https://doi.org/10.\n48550/arXiv.2106.03146.\n29. He K et al (2015) Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition. IEEE\nTrans Pattern Anal Mach Intell 37(9):1904 –1916\n30. He K et al (2020) Mask R-CNN. IEEE Trans Pattern Anal Mach Intell 42(2):386 –397\n31. Hong M, Li S, Yang Y, Zhu F, Zhao Q, Lu L (2022, Art no 8018505) SSPNet: Scale Selection Pyramid\nNetwork for Tiny Person Detection From UAV Images. IEEE Geosci Remote Sens Lett 19:1 –5. https://doi.\norg/10.1109/LGRS.2021.3103069\n32. Howard, AG, Zhu, M, Chen, B, Kalenichenko, D, Wang, W, Weyand, T, ..., Adam, H. (2017) Mobilenets:\nEfficient convolutional neural networks for mobile vision applications.https://doi.org/10.48550/arXiv.1704.\n04861.\n33. Howard A et al. (2019) \"Searching for MobileNetV3,\" 2019 IEEE/CVF International Conference on\nC\nomputer Vision (ICCV), pp. 1314 –1324, https://doi.org/10.1109/ICCV.2019.00140.\n34. Huang G, Liu Z, Van Der Maaten L, Weinberger KQ (2017) \"Densely Connected Convolutional Networks,\n\" 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2261 –2269, https://doi.\norg/10.1109/CVPR.2017.243.\n35. Iandola, FN, Han, S, Moskewicz, MW, Ashraf, K, Dally, WJ, Keutzer, K (2016) SqueezeNet: AlexNet-level\naccuracy with 50x fewer parameters and< 0.5 MB model size. https://doi.org/10.48550/arXiv.1602.07360.\n36. Jiang, Y, Chang, S, Wang, Z (2021) Transgan: two pure transformers can make one strong Gan, and that\ncan scale up. Adv Neural Inf Proces Syst, 34\n37. Kang K, Li H, Yan J, Zeng X, Yang B, Xiao T, Zhang C, Wang Z, Wang R, Wang X, Ouyang W\n(Oct. 2018) T-CNN: Tubelets with convolutional neural networks for object detection from videos. IEEE\nTrans Circuits Syst Vid Technol 28(10):2896 –2907. https://doi.org/10.1109/TCSVT.2017.2736553\n21380 Multimedia Tools and Applications (2023) 82:21353–21383\n38. Karlinsky L et al. (2019) \"RepMet: Representative-Based Metric Learning for Classification and Few-Shot\nObject Detection,\" 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\n5192–5201, https://doi.org/10.1109/CVPR.2019.00534.\n39. Kuznetsova A, Rom H, Alldrin N, Uijlings J, Krasin I, Pont-Tuset J, Kamali S, Popov S, Malloci M,\nKolesnikov A, Duerig T, Ferrari V (2020) The open images dataset V4. Int J Comput Vis 128:1956 –1981.\nhttps://doi.org/10.1007/s11263-020-01316-z\n40. Law H, Deng J (2020) CornerNet: detecting objects as paired Keypoints. Int J Comput Vis 128:642 –656.\nhttps://doi.org/10.1007/s11263-019-01204-1\n41. Li Y, Li J, Lin W, Li J (2018) Tiny-DSOD: lightweight object detection for resource-restricted usages.\nhttps://doi.org/10.48550/arXiv.1807.11013\n42. Li Y, Chen Y, Wang N, Zhang Z-X (2019) \"Scale-Aware Trident Networks for Object Detection,\" 2019\nIEEE/CVF International Conference on Computer Vision (ICCV), pp. 6053 –6062, https://doi.org/10.1109/\nICCV.2019.00615.\n43. Liang T, Chu X, Liu Y, Wang Y, Tang Z, Chu W, ... Ling H (2021) Cbnetv2: a composite backbone\nnetwork architecture for object detection. https://doi.org/10.48550/arXiv.2107.00420\n44. Lin, TY. et al. (2014) Microsoft COCO: Common Objects in Context. In: Fleet, D., Pajdla, T., Schiele, B.,\nTuytelaars, T. (eds) Computer Vision – ECCV 2014. ECCV 2014. Lecture notes in computer science, vol\n8693. Springer, Cham. https://doi.org/10.1007/978-3-319-10602-1_48.\n45. Lin T-Y, Dollár P, Girshick R, He K, Hariharan B, Belongie S (2017) “Feature pyramid networks for object\ndetection,” 2017 IEEE conference on computer vision and pattern recognition (CVPR), pp 936 –944. https://\ndoi.org/10.1109/CVPR.2017.106\n46. Lin T-Y, Goyal P, Girshick R, He K, Dollár P (2020) Focal Loss for Dense Object Detection. IEEE Trans\nPattern Anal Mach Intell 42(2):318 –327. https://doi.org/10.1109/TPAMI.2018.2858826\n47. Liu, W et al. (2016) SSD: Single Shot MultiBox Detector. In: Leibe, B., Matas, J., Sebe, N., Welling, M.\n(eds) Computer Vision – ECCV 2016. ECCV 2016. Lecture notes in computer science(), vol 9905.\nSpringer, Cham. https://doi.org/10.1007/978-3-319-46448-0_2.\n48. Liu S, Johns E, Davison AJ (2019) “End-to-end multi-task learning with attention, ” 2019 IEEE/CVF\nconference on computer vision and pattern recognition (CVPR), pp 1871 –1880. https://doi.org/10.1109/\nCVPR.2019.00197\n49. Liu L, Ouyang W, Wang X, Fieguth P, Chen J, Liu X, Pietikäinen M (2020) Deep learning for generic\nobject detection: a survey. Int J Comput Vis 128:261 –318. https://doi.org/10.1007/s11263-019-01247-4\n50. Liu Z, Zheng T, Xu G, Yang Z, Liu H, Cai D (2020) Training-time-friendly network for real-time object\ndetection. Proceedings of the AAAI Conference on Artificial Intelligence 34(07):11685 –11692. https://doi.\norg/10.1609/aaai.v34i07.6838\n51. Liu Z et al. (2021) \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows,\" IEEE/\nCVF International Conference on Computer Vision (ICCV), 2021, pp. 9992 –10002, https://doi.org/10.\n1109/ICCV48922.2021.00986.\n52. Liu, Z, Mao, H, Wu, CY, Feichtenhofer, C, Darrell, T, Xie, S (2022) A ConvNet for the 2020s. https://doi.\norg/10.48550/arXiv.2201.03545.\n53. Ma C, Huang J-B, Yang X, Yang M-H (2015) \"Hierarchical Convolutional Features for Visual Tracking,\"\n2015 IEEE International Conference on Computer Vision (ICCV), pp. 3074 –3082, https://doi.org/10.1109/\nICCV.2015.352.\n54. Ma, N, Zhang, X, Zheng, HT, Sun, J (2018) ShuffleNet V2: Practical Guidelines for Efficient CNN\nArchitecture Design. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds) Computer Vision –\nECCV 2018. ECCV 2018. Lecture notes in computer science(), vol 11218. Springer, Cham. https://doi.org/\n10.1007/978-3-030-01264-9_8.\n5\n5. Ma W et al (2020) MDFN: Multi-Scale Deep Feature Learning Network for Object Detection. Pattern\nRecog 100:107149\n56. Ma, T, Mao, M, Zheng, H, Gao, P, Wang, X, Han, S, ..., Doermann, D. (2021) Oriented object detection\nwith transformer. https://doi.org/10.48550/arXiv.2106.03146.\n57. Mehta, S, Rastegari M (n.d.) \"Mobilevit: light-weight, general-purpose, and mobile-friendly vision trans-\nformer.\" https://doi.org/10.48550/arXiv.2110.02178.\n58. Newell, A, Yang, K, Deng, J (2016) Stacked Hourglass Networks for Human Pose Estimation. In: Leibe,\nB., Matas, J., Sebe, N., Welling, M. (eds) Computer Vision – ECCV 2016. ECCV 2016. Lecture notes in\ncomputer science(), vol 9912. Springer, Cham https://doi.org/10.1007/978-3-319-46484-8_29.\n59. Pang J, Chen K, Shi J, Feng H, Ouyang W, Lin D (2019) “Libra R-CNN: towards balanced learning for\nobject detection,” 2019 IEEE/CVF conference on computer vision and pattern recognition (CVPR), pp 821–\n830. https://doi.org/10.1109/CVPR.2019.00091\n21381Multimedia Tools and Applications (2023) 82:21353–21383\n60. Peng Z et al. (2021) \"Conformer: Local Features Coupling Global Representations for Visual Recognition,\"\n2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 357 –366, https://doi.org/10.\n1109/ICCV48922.2021.00042.\n61. Qiao S, Chen L-C, Yuille A (2021) \"DetectoRS: Detecting Objects with Recursive Feature Pyramid and\nSwitchable Atrous Convolution,\" 2021 IEEE/CV F Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 10208 –10219, https://doi.org/10.1109/CVPR46437.2021.01008.\n62. Qin Z et al. (2019) \"ThunderNet: Towards Real-Time Generic Object Detection on Mobile Devices,\" 2019\nIEEE/CVF International Conference on Computer Vision (ICCV), pp. 6717 –6726, https://doi.org/10.1109/\nICCV.2019.00682.\n63. Qiu H et al. (2021) \"CrossDet: Crossline Representation for Object Detection,\" 2021 IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pp. 3175 –3184, https://doi.org/10.1109/\nICCV48922.2021.00318.\n64. Rahman S, Khan SH, Porikli F (2020) Zero-shot object detection: joint recognition and localization of novel\nconcepts. Int J Comput Vis 128:2979 –2999. https://doi.org/10.1007/s11263-020-01355-6\n65. Redmon J, Farhadi A (2017) \"YOLO9000: Better, Faster, Stronger,\" 2017 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 6517 –6525, https://doi.org/10.1109/CVPR.2017.690.\n66. Redmon, J, Farhadi A (n.d.) “YOLOv3: An Incremental Improvement. ” https://doi.org/10.48550/arXiv.\n1804.02767.\n67. Redmon J, Divvala S, Girshick R, Farhadi A (2016) \"You Only Look Once: Unified, Real-Time Object\nDetection,\" 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 779 –788,\nhttps://doi.org/10.1109/CVPR.2016.91.\n68. Ren S et al (2017) Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.\nIEEE Trans Pattern Anal Mach Intell 39(6):1137 –1149\n69. Ronneberger O, Fischer P, Brox T (2015) U-net: convolutional networks for biomedical image segmenta-\ntion. In: Navab N, Hornegger J, Wells W, Frangi A (eds) Medical image computing and computer-assisted\nintervention– MICCAI 2015. MICCAI 2015. Lecture notes in computer science, vol 9351. Springer, Cham.\nhttps://doi.org/10.1007/978-3-319-24574-4_28\n70. Russakovsky O et al (2015) ImageNet Large Scale Visual Recognition Challenge. Int J Comput Vis 115(3):\n211–252\n71. Sandler M, Howard A, Zhu M, Zhmoginov A, Chen L-C (2018) \"MobileNetV2: Inverted Residuals and\nLinear Bottlenecks,\" 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4510 –\n4520, https://doi.org/10.1109/CVPR.2018.00474.\n72. Shen Z, Liu Z, Li J, Jiang Y, Chen Y, Xue X (2017) \"DSOD: learning deeply supervised object detectors\nfrom scratch,\" 2017 IEEE international conference on computer vision (ICCV), pp. 1937-1945, https://doi.\norg/10.1109/ICCV.2017.212.\n73. Shrivastava A, Gupta A, Girshick R (2016) \"Training Region-Based Object Detectors with Online Hard\nExample Mining,\" 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 761 –\n769, https://doi.org/10.1109/CVPR.2016.89.\n74. Simonyan K, Zisserman A (2015) Very deep convolutional networks for large-scale image recognition. In:\nProceedings of the 3rd International Conference on Learning Representations (ICLR 2015).\nOpenReview.net, : 1–14\n75. Singh S, Ahuja U, Kumar M, Kumar K, Sachdeva M (2021) Face mask detection using YOLOv3 and faster\nR-CNN models: COVID-19 environment. Multimed Tools Appl 80:19753 –19768. https://doi.org/10.1007/\ns11042-021-10711-8\n76. Srinivasu PN, SivaSai JG, Ijaz MF, Bhoi AK, Kim W, Kang JJ (2021) Classification of skin disease using\ndeep learning neural networks with MobileNet V2 and LSTM. Sensors 21:2852. https://doi.org/10.3390/\ns21082852\n77. Tan, M, Le Q (2019) \"Efficientnet: rethinking model scaling for convolutional neural networks.\"\nInternational conference on machine learning. PMLR, https://doi.org/10.48550/arXiv.1905.11946\n78. Tan M et al. (2019) \"MnasNet: Platform-Aware Neural Architecture Search for Mobile,\" 2019 IEEE/CVF\nCo\nnference on Computer Vision and Pattern Recognition (CVPR), pp. 2815 –2823, https://doi.org/10.1109/\nCVPR.2019.00293.\n79. Tan M, Pang R, Le QV (2020) \"EfficientDet: Scalable and Efficient Object Detection,\" 2020 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), pp. 10778 –10787, https://doi.org/10.\n1109/CVPR42600.2020.01079.\n80. Touvron, H, et al. (2021) “Training Data-Efficient Image Transformers & Distillation through Attention. ”\nICML 2021: 38th International Conference on Machine Learning, pp. 10347 –10357.\n81. Uijlings JR et al (2013) Selective search for object recognition. Int J Comput Vis 104(2):154 –171\n21382 Multimedia Tools and Applications (2023) 82:21353–21383\n82. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I. 2017 Attention\nis all you need. In proceedings of the 31st international conference on neural information processing systems\n(NIPS'17). Curran associates Inc., red hook, NY, USA, 6000 –6010\n83. Viola P, Jones M (2001) \"Rapid object detection using a boosted cascade of simple features,\" proceedings\nof the 2001 IEEE computer society conference on computer vision and pattern recognition. CVPR 2001, pp.\n511–518, https://doi.org/10.1109/CVPR.2001.990517.\n84. Vulli A, Srinivasu PN, Sashank MSK, Shafi J, Choi J, Ijaz MF (2022) Fine-tuned DenseNet-169 for breast\nCancer metastasis prediction using FastAI and 1-cycle policy. Sensors 22:2988. https://doi.org/10.3390/\ns22082988\n85. Wan F, Liu C, Ke W, Ji X, Jiao J, Ye Q (2019) \"C-MIL: Continuation Multiple Instance Learning for\nWeakly Supervised Object Detection,\" 2019 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 2194 –2203, https://doi.org/10.1109/CVPR.2019.00230.\n86. Wang RJ et al (2018) “Pelee: a real-time object detection system on mobile devices. ” NIPS’18 Proceedings\nof the 32nd international conference on neural information processing systems, vol 31, pp 1967 –1976\n87. Wang W et al (2021) “Pyramid vision transformer: a versatile backbone for dense prediction without\nconvolutions,” 2021 IEEE/CVF international conference on computer vision (ICCV), 2021, pp 548 –\n558. https://doi.org/10.1109/ICCV48922.2021.00061\n88. Wang Y, Huang R, Song S, Huang Z, Gao H (n.d.) Not All Images Are Worth 16x16 Words: Dynamic\nVision Transformers with Adaptive Sequence Length. Adv Neural Inf Process Syst 34. https://doi.org/10.\n48550/arXiv.2105.15075\n89. Xie S, Girshick R, Dollár P, Tu Z, He K (2017) \"Aggregated Residual Transformations for Deep Neural\nNetworks,\" 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5987 –5995,\nhttps://doi.org/10.1109/CVPR.2017.634.\n90. Xie, E, Wang, W, Yu, Z, Anandkumar, A, Alvarez, JM, Luo, P (2021) SegFormer: simple and efficient\ndesign for semantic segmentation with transformers. Adv Neural Inf Proces Syst, 34\n91. Xiong Y et al. (2021) \"MobileDets: Searching for Object Detection Architectures for Mobile Accelerators,\"\n2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3824 –3833, https://\ndoi.org/10.1109/CVPR46437.2021.00382.\n92. Yang, J, Li, C, Zhang, P, Dai, X, Xiao, B, Yuan, L, Gao, J (2021) Focal self-attention for local-global\ninteractions in vision transformers. https://doi.org/10.48550/arXiv.2107.00641.\n93. Yin T, Zhou X, Krähenbühl P (2021) \"Center-based 3D Object Detection and Tracking,\" 2021 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), pp. 11779 –11788, https://doi.org/10.\n1109/CVPR46437.2021.01161.\n94. Zeiler, MD, Fergus, R (2014) Visualizing and Understanding Convolutional Networks. In: Fleet, D., Pajdla,\nT., Schiele, B., Tuytelaars, T. (eds) Computer Vision – ECCV 2014. ECCV 2014. Lecture notes in\ncomputer science, vol 8689. Springer, Cham https://doi.org/10.1007/978-3-319-10590-1_53.\n95. Zhang X, Zhou X, Lin M Sun J (2018) \"ShuffleNet: An Extremely Efficient Convolutional Neural Network\nfor Mobile Devices,\" 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6848 –\n6856, https://doi.org/10.1109/CVPR.2018.00716.\n96. Zhou P, Ni B, Geng C, Hu J, Xu Y (2018) \"Scale-Transferrable Object Detection,\" 2018 IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 528 –537, https://doi.org/10.1109/CVPR.\n2018.00062.\n97. Zhou, X, Koltun, V, Krähenbühl, P (2021) Probabilistic two-stage detection. https://doi.org/10.48550/arXiv.\n2103.07461.\n98. Zhu, X, Su, W, Lu, L, Li, B, Wang, X, Dai, J (2020) Deformable detr: Deformable transformers for end-to-\nend object detection. In Proc. ICLR, 2021 Oral, PP. 1 –16\nPublisher’sn o t e Springer Nature remains neutral with regard to jurisdictional claims in published maps\nand institutional affiliations.\n21383Multimedia Tools and Applications (2023) 82:21353–21383",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8802833557128906
    },
    {
      "name": "Transformer",
      "score": 0.5979486107826233
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5945981740951538
    },
    {
      "name": "Object detection",
      "score": 0.5878812074661255
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5080311298370361
    },
    {
      "name": "Machine learning",
      "score": 0.4136131703853607
    },
    {
      "name": "Field (mathematics)",
      "score": 0.41105732321739197
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.2515830397605896
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}