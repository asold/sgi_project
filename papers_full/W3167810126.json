{
  "title": "SCRIPT: Self-Critic PreTraining of Transformers",
  "url": "https://openalex.org/W3167810126",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2791611741",
      "name": "Erik Nijkamp",
      "affiliations": [
        "Albert Einstein College of Medicine",
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A333975790",
      "name": "Bo Pang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2152507340",
      "name": "Ying Nian Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095665791",
      "name": "Caiming Xiong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2547875792",
    "https://openalex.org/W3105721709",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2025768430",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3037854022",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2990391581",
    "https://openalex.org/W3105966348"
  ],
  "abstract": "Erik Nijkamp, Bo Pang, Ying Nian Wu, Caiming Xiong. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 5196–5202\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n5196\nSCRIPT: Self-Critic Pretraining of Transformers\nErik Nijkamp∗\nUCLA\nenijkamp@ucla.edu\nBo Pang\nUCLA\nbopang@ucla.edu\nYing Nian Wu\nUCLA\nywu@stat.ucla.edu\nCaiming Xiong\nSalesforce Research\ncxiong@salesforce.com\nAbstract\nWe introduce Self-CRItic Pretraining Trans-\nformers (SCRIPT) for representation learning\nof text. The popular masked language mod-\neling (MLM) pretraining methods like BERT\nreplace some tokens with [MASK] and an en-\ncoder is trained to recover them, while ELEC-\nTRA trains a discriminator to detect replaced\ntokens proposed by a generator. In contrast,\nwe train a language model as in MLM and fur-\nther derive a discriminator or critic on top of\nthe encoder without using any additional pa-\nrameters. That is, the model itself is a critic.\nSCRIPT combines MLM training and dis-\ncriminative training for learning rich represen-\ntations and compute- and sample-efﬁciency.\nWe demonstrate improved sample-efﬁciency\nin pretraining and enhanced representations ev-\nidenced by improved downstream task perfor-\nmance on GLUE and SQuAD over strong base-\nlines. Also, the self-critic scores can be di-\nrectly used as pseudo-log-likelihood for efﬁ-\ncient scoring.\n1 Introduction\nIn natural language processing, the landscape\nof unsupervised learning methods is dominated\nby masked language modeling (MLM) for bi-\ndirectional encoders, such as BERT (Devlin et al.,\n2018; Yang et al., 2019; Liu et al., 2019; Joshi et al.,\n2020; Lan et al., 2019; Lewis et al., 2020; Jiao et al.,\n2019), and causal masking for uni-directional auto-\nregressive decoders (Radford et al., 2018, 2019;\nBrown et al., 2020; Raffel et al., 2020; Lewis et al.,\n2019) such as GPT. In MLM an encoder is pre-\ntrained on a generic corpus of text with the hope of\nlearning universal contextual embeddings, which,\nthen, are ﬁne-tuned on a speciﬁc down-stream task.\nWhereas recent developments in causal masking\naim to learn a large-scale model once and deﬁne\nthe down-stream task as an auto-regressive man-\nner in the form of few-shot evaluation (Brown\n∗Research conducted at Salesforce Einstein\net al., 2020). In practice, while an universal auto-\nregressive neural backbone model without the need\nfor ﬁne-tuning such as GPT-3 is desirable, the com-\nputational complexity at inference time remains an\nopen problem. While the two-stage approach of\nMLM of smaller models is computationally conve-\nnient, the pretraining still incurs a substantial com-\nputational cost. Hence, in this work, we focus on\nlearning contextual bi-directional representations\nwith the goal of improving upon sample efﬁciency.\nIn MLM, the input sequence of tokens is per-\nturbed by randomly masking out a small subset\nof the identities of tokens (Devlin et al., 2018) or\nattention scores to those tokens (Yang et al., 2019).\nThen, the generative model is learned as a denois-\ning auto-encoder (Vincent et al., 2008) which re-\ncovers the masked out tokens. While the learned\ncontextual representations achieve remarkable per-\nformance on down-stream tasks, the pretraining\nrequires substantial compute. This is mainly due to\nlearning from gradients from the restricted subset\nof tokens (Clark et al., 2020).\nIn ELECTRA (Clark et al., 2020), the input se-\nquence is perturbed by replacing a subset of tokens\nby sampled tokens drawn from an auxiliary genera-\ntor model in the form of a bi-directional encoder,\nwhich itself is learned by MLM. Then, the discrim-\ninative model is learned by a binary classiﬁcation\ntask which detects whether a token is unperturbed\nor has been replaced. This approach enjoys remark-\nable sample efﬁciency, which, we believe, stems\nprimarily from reducing the complexity of the clas-\nsiﬁcation task from masked token predictionover a\nlarge set of classes (i.e., a typical vocabulary size\nof 30,522 classes) to replaced token detection(i.e.,\n2 classes).\nDespite it being less efﬁcient, MLM training\nguides the model to learn rich representations.\nELECTRA uses MLM only in learning the auxil-\niary generator which is discarded after pretraining.\nWe propose to combine MLM and discriminative\n5197\nFigure 1: An overview of SCRIPT. We combine MLM and discriminative training in a single transformer encoder, exploiting\nthe rich representations extracted through MLM training and the compute- and sample-efﬁciency though discriminative training,\nresulting in a simple yet effective pretraining approach for representation learning. Pretraining starts with replacing a small\nportion of tokens (e.g., 15%) in a text sequence xxx with [MASK], yielding ˆxxx. The architecture of SCRIPT is a transformer\nencoder with a softmax output layer, producing a distribution over tokens, same as any MLM models like as BERT. In the MLM\nforward pass, SCRIPT takes ˆxxx as input and outputs a distribution for each token. This distribution is ﬁrst used to compute the\nMLM loss, LMLM , the negative log-likelihood of recovering the masked token. It is then used to construct a Gumbel-Softmax\ndistribution, from which ¯xxx is sampled (indicated by the broken arrows in the ﬁgure). The critic forward pass takes ¯xxx as input and\ngoes through the same model. The output softmax distribution is used to construct a binary classiﬁer to discriminate an original\nversus a replaced token. And the discriminative training loss, LDisc, is simply cross-entropy of the derived binary classiﬁer.\nFinally, a single backward pass is guided by the combination of LMLM and LDisc.\ntraining. The resulting model thus has the rich rep-\nresentations from both MLM and discriminative\nlearning and enjoys compute and sample efﬁciency\nfrom its discriminative learning. Furthermore, in-\nstead of learning an auxiliary model in addition\nto the main encoder, our approach learns a single\nmodel which is leveraged to recover masked tokens,\npropose token replacements, and detect replaced\ntokens. Hence the encoder itself is also a critic, giv-\ning the name of our model, Self-CRItic Pretraining\nTransformers (SCRIPT). Our experiments show\nthat SCRIPT has improved compute and sample\nefﬁciency in pretraining and enhanced represen-\ntations, hence outperforming strong baselines in\nﬁne-tuning on downstream tasks.\nContributions. (1) We propose a novel pre-\ntraining approach in which the model acts as a\nself-critic. (2) We demonstrated improved down-\nstream task performance over state-of-the-art under\ncomputational constraints. (3) We show the self-\ncritic scores may serve as computationally efﬁcient\npseudo-log-likelihood for scoring tasks.\n2 Method\nWe propose a pretraining approach which combines\nmasked token recovery and replaced token detec-\ntion and does not introduce any additional parame-\nters compared to a regular BERT. In the following\nsections, we ﬁrst introduce MLM training which\nis the same as that in BERT, and then present self-\ncritic training.\nSuppose xxx = [ x1,...,x t,...,x T] is a text se-\nquence where xt is the tth token. In MLM training,\na portion of tokens (e.g., 15%) are replaced with a\nspecial token [MASK]. Let ˆxxxbe the sequence after\nthe mask replacement and e(ˆxxx) = {et ∈Rd}T\nt=1\nbe the contextual representations computed by the\ntransformer. Let W ∈RV×d be the weight matrix\nof a softmax layer where V is the vocabulary size.\nThe logit or score for token tis st = Wet ∈RV.\nThen the log-likelihood of the sequence xxxis,\nlog pθ(xxx|ˆxxx) =\nT∑\nt=1\nmtlog pθ(xt|ˆxxx) (1)\n=\nT∑\nt=1\nmtlog exp(stv)∑V\nv′=1 exp(stv′)\n(2)\nwhere mt ∈ {0,1}indicates whether xt is a\nmasked token, [MASK]. The loss function for\nMLM is the negative log-likelihood LMLM(θ) =\n−Epdata(xxx) log pθ(xxx|ˆxxx) where pdata is the empiri-\ncal data distribution.\nBesides deﬁning the log-likelihood for MLM\ntraining, pθ(xt|ˆxxx) naturally provides a conditional\ndistribution of xt with which we can construct a\nsampled sequence, ¯xxx= [¯x1,..., ¯xT], by replacing\nxt with ¯xt, a token sampled from pθ(xt|ˆxxx). xt is\nreplaced only if it is masked in ˆxxx(i.e., mt = 1). In\nparticular, the replacement token is sampled from\na Gumbel-Softmax distribution (Jang et al., 2016).\nLet π = {πv}V\nv=1 denote pθ(xt|ˆxxx) for notational\nclarity. Then the probability of sampling the vth\ntoken in the vocabulary for xt is,\np(¯xt|ˆxxx) = exp[(log πv + gv)/τ]∑V\nv′=1 exp[(log πv′+ gv′)/τ]\n(3)\n5198\nwhere {gv′}V\nv′=1 are i.i.d. samples drawn from\nGumbel(0,1)1 and τ is the temperature for sam-\npling. The Gumbel-Softmax distribution π ap-\nproaches one-hot when τ is small (e.g., τ = 0.1)\nand uniform when τ is large (e.g., τ = 10.0).\nTo apply discriminative training to the model,\nwe derive a discriminator from the existing model\nand parameters. ¯xt is considered as a positive to-\nken if ¯xt = xt, while deemed a negative token\nif ¯xt ̸= xt. In the MLM training, the last layer\ndeﬁnes a V-class classiﬁer with the parameters\nW. We can augment W with an extra row for\ncomputing the score or logit for the negative to-\nken class, making it classify V + 1 classes. De-\nnote the augmented weight matrix as W+. Then\nthe classiﬁcation logits are s+\nt = W+et ∈RV+1.\nHowever, it is unnecessary to bring in new param-\neters and over-parameterization since subtracting\nan arbitrary function f(et) ∈R from all the logits,\ns+\ntv−f(et) ∀v= 1,...,V + 1, does not change the\nsoftmax output. Thus we ﬁx the last row of W+ to\nall zeros 000 ∈R1×d. Then we have the logit for the\ntth token,\ns+\nt = W+et =\n{\nWet = st, for xt ∈{1,...,V }\n0, otherwise.\nThen the probability of the tth token in ¯xxxbeing a\nnegative token is,\np(t−|¯xxx) = 1∑V\nv′=1 exp(stv′) + 1\n(4)\nwhile the probability being a positive token is,\np(t+|¯xxx) =\n∑V\nv′=1 exp(stv′)∑V\nv′=1 exp(stv′) + 1\n(5)\nwhere t− and t+ indicate ¯xt is a positive token and\na negative token, respectively. The generator per\nse is thus also a critic or discriminator for replaced\ntoken detection, giving the name of our model, self-\ncritic. The loss of discriminative training is simply\nthe cross-entropy loss,\nLDisc(θ) = −Epdata[\nT∑\nt=1\n1 (t+) logp(t+|¯xxx)+\n1 (t−) logp(t−|¯xxx)]. (6)\nThe overall loss function of SCRIPT com-\nbines MLM and discriminative training, Lθ =\n1The Gumbel(0, 1) distribution can be sampled using in-\nverse transform sampling by drawing u ∼Uniform(0, 1) and\ncomputing g = −log(−log u)\nLMLM(θ) + αLDisc(θ), where αis an coefﬁcient\ndetermining the strength of discriminative train-\ning. The learning of SCRIPT involves two forward\npasses through a single model, one for MLM with\nˆxxxas input, one for discriminative training with ¯xxxas\ninput, and a single backward pass. Figure 1 gives\nan overview of our model.\n3 Experiments\nIn the subsequent empirical evaluations, we shall\naddress the following questions: (1) Does the learn-\ning as self-critic lead to competitive down-stream\ntask performance? (2) Can we treat the self-critic\nscores as pseudo-log-likelihoods? (3) Is the sample\nefﬁciency improved over state-of-the-art baselines?\nHence, we train and evaluate two SCRIPT mod-\nels “small” and “base” with an encoder of the 14M\nand 110M parameters, respectively. For a direct\ncomparison, the models are trained on the Open-\nWebText corpus (Gokaslan and Cohen, 2019) with\nidentical pre-processing and optimization proce-\ndures as in (Devlin et al., 2018) and (Clark et al.,\n2020). We refer to the Appendix for details.\n3.1 Transfer to Downstream Tasks\nWe evaluate the efﬁcacy of our method on the\nGLUE natural language understanding bench-\nmark (Wang et al., 2018) and the SQuAD 1.1 and\n2.0 question answering dataset (Rajpurkar et al.,\n2016a). We report mean scores of GLUE tasks\nover 8 ﬁne-tuning runs with varying random seed.\nFor the evaluation on SQuAD, we re-trained the\n“small” models with a sequence length of 512 to-\nkens. Table 1 depicts improved scores across the\nbenchmarks. The task speciﬁc GLUE scores are\nshown in Table 2.\nGLUE SQuAD 1.1 SQuAD 2.0\nModel Mean EM F1 EM F1\nELECTRA-small 80.38 74.13 81.65 65.91 68.59\nSCRIPT-small 81.32 74.84 82.43 67.03 69.81\nELECTRA-base 85.06 84.57 90.72 80.86 83.52\nSCRIPT-base 85.76 85.43 91.56 81.74 84.25\nTable 1: GLUE and SQuAD dev-set scores for models pre-\ntrained on OpenWebText with identical pre-processing and\noptimization.\n3.2 Efﬁcient Pseudo-Log-Likelihood Scoring\nIn contrast to MLM and ELECTRA pretrain-\ning, SCRIPT allows for efﬁcient computation of\n5199\nModel Params CoLA SST MRPC STS QQP MNLI QNLI RTE Mean\nBERT-small 14M 38.40 88.99 84.55 84.20 87.67 78.07 85.75 61.01 76.08\nELECTRA-small 14M 56.82 88.37 87.41 86.82 88.30 78.94 87.92 68.51 80.38\nSCRIPT-small (ours) 14M 59.46 89.56 88.23 87.16 89.38 80.30 88.04 68.42 81.32\nBERT-base 110M 51.72 92.83 83.93 83.9 88.75 84.55 89.91 65.98 80.19\nELECTRA-base 110M 64.36 91.03 88.23 90.18 91.33 86.21 92.01 77.16 85.06\nSCRIPT-base (ours) 110M 65.04 93.09 90.08 90.01 91.43 86.88 92.29 77.23 85.76\nTable 2: Comparison of small and base models on the GLUE dev set. The models were trained on the OpenWebText\ncorpus (Gokaslan and Cohen, 2019) for 1, 000, 000 and 766, 000 steps, respectively. The GLUE task scores are means of 8 runs\nover a set of random seeds. SCRIPT outperforms ELECTRA while enjoying a simple architecture and learning algorithm.\na pseudo-log-likelihood (PLL) for a given se-\nquence xxx,\nPLL(xxx) =\nT∑\nt=1\nlog p(t+|xxx). (7)\nThe PLL allows for the re-ranking of a set of\nsequences produced by a NMT or ASR system.\nWhile language models seem a natural ﬁt for a\nranking problem, Salazar et al. (2019) show im-\nproved performance when ranking is based on the\nPLL. However, for a sequence with T tokens, this\nwould require T forward passes as each token has\nto be masked out. Instead, we propose to recruit (7)\nas a measure of PLL. Table 3 compares the word\nerror rates (WER) on the LibriSpeech dataset after\nrescoring. SCRIPT performs competitively while\n(7) is computed as a single forward pass.\ndev test\nModel clean other clean other\nbaseline (1-best) 7.17 19.79 7.26 20.37\noracle (100-best) 2.85 12.21 2.81 12.85\nuni-SANLM 6.08 17.32 6.11 18.13\nbi-SANLM 5.52 16.61 5.65 17.44\nBERT-small 5.65 16.97 5.80 17.70\nSCRIPT-small 5.79 17.02 6.12 17.83\nTable 3: WERs on LibriSpeech after rescoring. Baseline,\nSANLM, and oracle numbers are from Shin et al. (2019).\n3.3 Computational Efﬁciency\nWall-clock time. We compare the number of train-\ning steps per second. For direct comparison, we\nmodify the ELECTRA reference code2. For TPU\nv3 with 8 TPU cores, ELECTRA and SCRIPT\nachieve 31.3 and 22.7 training iterations per sec-\n2https://github.com/google-research/\nelectra\nond with a mean MXU utilization of 14.93% and\n17.91% for small models, respectively.\nGLUE. Figure 2 depicts the improvement in\nthe mean GLUE scores for ELECTRA-small and\nSCRIPT-small over the number of training steps.\nWhile the wall-clock time per computational train-\ning step of SCRIPT is increased over ELECTRA,\nthe sample-efﬁciency of SCRIPT in terms of the\nmean GLUE score over training steps is higher.\nHence, the efﬁciency of both methods may be com-\nparable, however, SCRIPT achieves improved over-\nall performance on GLUE.\n100,000200,000400,000600,000800,0001,000,000\n74\n76\n78\n80\nStep\nGLUE Mean\nELECTRASCRIPT\nFigure 2: Comparison between ELECTRA-small and\nSCRIPT-small on the GLUE mean score over training steps\non the OpenWebText corpus.\n4 Conclusion\nThis work presents SCRIPT for representation\nlearning. It is a transformer encoder like BERT.\nIn pretraining, it recovers masked tokens, pro-\nposes negative samples, and acts as a self-critic,\ndiscriminating between sampled and original to-\nkens. The joint MLM and discriminative learn-\ning improves sample efﬁciency in pretraining and\nenhances representation learning, leading to im-\nproved performance over strong baselines on vari-\nous downstream tasks. It also provides an efﬁcient\nway for computing pseudo-log-likelihood for scor-\ning tasks and achieves competitive performance.\n5200\nReferences\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nDaniel M. Cer, Mona T. Diab, Eneko Agirre, Iñigo\nLopez-Gazpio, and Lucia Specia. 2017. Semeval-\n2017 task 1: Semantic textual similarity multilin-\ngual and crosslingual focused evaluation. In Se-\nmEval@ACL.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than genera-\ntors. arXiv preprint arXiv:2003.10555.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn IWP@IJCNLP.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand William B. Dolan. 2007. The third pascal\nrecognizing textual entailment challenge. In ACL-\nPASCAL@ACL.\nAaron Gokaslan and Vanya Cohen. 2019. Openweb-\ntext corpus. http://Skylion007.github.\nio/OpenWebTextCorpus.\nShankar Iyer, Nikhil Dandekar, and Kornél Csernai.\n2017. First Quora dataset release: Question pairs.\nEric Jang, Shixiang Gu, and Ben Poole. 2016. Categor-\nical reparameterization with gumbel-softmax. arXiv\npreprint arXiv:1611.01144.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2019. Tinybert: Distilling bert for natural language\nunderstanding. arXiv preprint arXiv:1909.10351.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predict-\ning spans. Transactions of the Association for Com-\nputational Linguistics, 8:64–77.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learning\nof language representations. In International Con-\nference on Learning Representations.\nMike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Ar-\nmen Aghajanyan, Sida Wang, and Luke Zettlemoyer.\n2020. Pre-training via paraphrasing. Advances in\nNeural Information Processing Systems, 33.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. arXiv preprint arXiv:1910.13461.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016a. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy S. Liang. 2016b. Squad: 100, 000+ questions\nfor machine comprehension of text. In EMNLP.\nJulian Salazar, Davis Liang, Toan Q Nguyen, and Ka-\ntrin Kirchhoff. 2019. Masked language model scor-\ning. arXiv preprint arXiv:1910.14659.\nJoonbo Shin, Yoonhyung Lee, and Kyomin Jung. 2019.\nEffective sentence scoring method using bert for\nspeech recognition. In Asian Conference on Ma-\nchine Learning, pages 1081–1093.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Y . Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In EMNLP.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. 2008. Extracting and\ncomposing robust features with denoising autoen-\ncoders. In Proceedings of the 25th international con-\nference on Machine learning, pages 1096–1103.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\n5201\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2018. Neural network acceptability judgments.\narXiv preprint arXiv:1805.12471.\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2018. A broad-coverage challenge corpus\nfor sentence understanding through inference. In\nNAACL-HLT.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5753–5763.\nAppendix\n4.1 Experiment Details\nWe describe the conﬁguration used for pre-\ntrainined and ﬁne-tuning below.\nPre-Training Hyperparameters. We largely\nuse the same hyperparameters as BERT and ELEC-\nTRA. The coefﬁcient for discriminative learning,α,\nis set to be50. We use dynamic token masking with\nthe masked positions decided on-the-ﬂy. Among\nthe 15% tokens selected for masking, 80% are re-\nplaced with [MASK], 10% are kept to be the same,\n10% are replaced with a random token. The full set\nof hyperparameters are displayed in Table 4.\nFine-Tuning Hyperparameters. We follow the\nﬁne-tuning hyperparameters used in ELECTRA.\nThe full set of hyperparameters is listed in Table 5.\n4.2 GLUE Description\nEach subtask of GLUE is described below.\nMNLI. Multi-genre Natural Language Inference\n(Williams et al., 2018). Given a pair of sentences,\nthe task is to predict whether whether the second\nsentence is an entailment, contradiction, or neutral\nwith respect to the ﬁrst one.\nQQP. Quora Question Pairs (Iyer et al., 2017).\nThe task is to determine whether a pair of questions\nasked on Quora are semantically equivalent.\nQNLI. Question Natural Language Inference.\nIt is a binary classiﬁcation task constructed from\nSQuAD (Rajpurkar et al., 2016b). The task is to\npredict whether a context sentence contains the\nanswer to a question sentence.\nSST.Stanford Sentiment Treebank (Socher et al.,\n2013). This task is binary task to determine if a\nsentence is positive or negative in sentiment.\nSTS. Semantic Textual Similarity (Cer et al.,\n2017). The tasks is to predict how similar two\nsentences are on a 1-5 scale in terms of semantic\nmeaning.\nCoLA. Corpus of Linguistic Acceptability\n(Warstadt et al., 2018). The task is to determine\nwhether a given sentence is linguistically \"accept-\nable\".\nMRPC. Microsoft Research Paraphrase Corpus\n(Dolan and Brockett, 2005). The task is to predict\nwhether two sentences are semantically equivalent.\nRTE. Recognizing Textual Entailment (Gi-\nampiccolo et al., 2007). Given a premise and a hy-\npothesis, the task is to predict whether the premise\nentails the hypothesis.\n5202\nHyperparameter Small Base\nNumber of layers 12 12\nHidden Size 256 768\nFFN inner hidden size 1024 3072\nAttention heads 4 12\nAttention head size 64 64\nEmbedding Size 128 768\nMask percent 15 15\nLearning Rate Decay Linear Linear\nWarmup steps 10000 10000\nLearning Rate 5e-4 2e-4\nAdam ϵ 1e-6 1e-6\nAdam β1 0.9 0.9\nAdam β2 0.999 0.999\nAttention Dropout 0.1 0.1\nDropout 0.1 0.1\nWeight Decay 0.01 0.01\nBatch Size 128 256\nTable 4: Pre-train hyperparameters.\nHyperparameter Value\nLearning Rate 3e-4 for Small, 1e-4 for Base\nAdam ϵ 1e-6\nAdam β1 0.9\nAdam β2 0.999\nLayerwise LR decay 0.8\nLearning rate decay Linear\nWarmup fraction 0.1\nAttention Dropout 0.1\nDropout 0.1\nWeight Decay 0\nBatch Size 32\nTrain Epochs 10 for RTE and STS, 2 for SQuAD, 3 for other tasks\nTable 5: Fine-tune hyperparameters.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5770517587661743
    },
    {
      "name": "Computer science",
      "score": 0.5630178451538086
    },
    {
      "name": "Computational linguistics",
      "score": 0.47843387722969055
    },
    {
      "name": "Linguistics",
      "score": 0.43184694647789
    },
    {
      "name": "Natural language processing",
      "score": 0.38111791014671326
    },
    {
      "name": "Artificial intelligence",
      "score": 0.348810613155365
    },
    {
      "name": "Engineering",
      "score": 0.18722859025001526
    },
    {
      "name": "Philosophy",
      "score": 0.18604019284248352
    },
    {
      "name": "Electrical engineering",
      "score": 0.120322585105896
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I129975664",
      "name": "Albert Einstein College of Medicine",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210155268",
      "name": "Salesforce (United States)",
      "country": "US"
    }
  ]
}