{
  "title": "Can Language Models Understand Physical Concepts?",
  "url": "https://openalex.org/W4389520366",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2098784551",
      "name": "Lei Li",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2114806282",
      "name": "Jingjing Xu",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A3121252400",
      "name": "Qingxiu Dong",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2149832677",
      "name": "Ce Zheng",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2107643647",
      "name": "Xu Sun",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2154593323",
      "name": "Lingpeng Kong",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2088571978",
      "name": "Qi Liu",
      "affiliations": [
        "University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W2771134746",
    "https://openalex.org/W4224912544",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4229673855",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W4312910992",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3103455452",
    "https://openalex.org/W4287325149",
    "https://openalex.org/W2975501350",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2986266667",
    "https://openalex.org/W2346474363",
    "https://openalex.org/W4226095990",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W4380135847",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3184735396",
    "https://openalex.org/W3172838779",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2053154970",
    "https://openalex.org/W4385570016",
    "https://openalex.org/W2947337775",
    "https://openalex.org/W4378465304",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3176641147",
    "https://openalex.org/W2767257667",
    "https://openalex.org/W4376312115",
    "https://openalex.org/W4288265479",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W4323572061",
    "https://openalex.org/W46219734",
    "https://openalex.org/W3199748991",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W4386185600",
    "https://openalex.org/W4226182655",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4286906902",
    "https://openalex.org/W4229004868",
    "https://openalex.org/W2731516819",
    "https://openalex.org/W3110909889",
    "https://openalex.org/W3100307207",
    "https://openalex.org/W1560408190",
    "https://openalex.org/W3101065397",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3193402170",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W4301194718",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W3212456749",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W4320235125",
    "https://openalex.org/W4384918448"
  ],
  "abstract": "Language models (LMs) gradually become general-purpose interfaces in the interactive and embodied world, where the understanding of physical concepts is an essential prerequisite. However, it is unclear whether LMs can understand physical concepts in the human world. To investigate this, we design a benchmark VEC that covers the tasks of (i) Visual concepts, such as the shape and material of objects, and (ii) Embodied Concepts, learned from the interaction with the world such as the temperature of objects. Our zero (few)-shot prompting results show that the understanding of certain visual concepts emerges as scaling up LMs, but there are still basic concepts to which the scaling law does not apply. For example, OPT-175B performs close to humans with a zero-shot accuracy of 85% on the material concept, yet behaves like random guessing on the mass concept. Instead, vision-augmented LMs such as CLIP and BLIP achieve a human-level understanding of embodied concepts. Analysis indicates that the rich semantics in visual representation can serve as a valuable source of embodied knowledge. Inspired by this, we propose a distillation method to transfer embodied knowledge from VLMs to LMs, achieving performance gain comparable with that by scaling up parameters of LMs 134√ó. Our dataset is available at https://github.com/TobiasLee/VEC.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 11843‚Äì11861\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nCan Language Models Understand Physical Concepts?\nLei Li1, Jingjing Xu2, Qingxiu Dong3, Ce Zheng3, Xu Sun3, Lingpeng Kong1, Qi Liu1\n1The University of Hong Kong 2Shanghai AI Lab\n3Peking University\nnlp.lilei@gmail.com {jingjingxu,zce1112zslx,xusun}@pku.edu.cn\ndqx@stu.pku.edu.cn {lpk, liuqi}@cs.hku.hk\nAbstract\nLanguage models (LMs) gradually become\ngeneral-purpose interfaces in the interactive\nand embodied world, where the understand-\ning of physical concepts is an essential prereq-\nuisite. However, it is unclear whether LMs\ncan understand physical concepts in the human\nworld. To investigate this, we design a bench-\nmark VEC that covers the tasks of (i) Visual\nconcepts, such as the shape and material of\nobjects, and (ii) Embodied Concepts, learned\nfrom the interaction with the world such as the\ntemperature of objects. Our zero (few)-shot\nprompting results show that the understanding\nof certain visual concepts emerges as scaling up\nLMs, but there are still basic concepts to which\nthe scaling law does not apply. For example,\nOPT-175B performs close to humans with a\nzero-shot accuracy of 85% on the material con-\ncept, yet behaves like random guessing on the\nmass concept. Instead, vision-augmented LMs\nsuch as CLIP and BLIP achieve a human-level\nunderstanding of embodied concepts. Analysis\nindicates that the rich semantics in visual rep-\nresentation can serve as a valuable source of\nembodied knowledge. Inspired by this, we pro-\npose a distillation method to transfer embodied\nknowledge from VLMs to LMs, achieving per-\nformance gain comparable with that by scaling\nup parameters of LMs 134√ó.1\n1 Introduction\nWith the emergent capabilities such as arith-\nmetic (Brown et al., 2020; Wei et al., 2022) and\nmulti-step reasoning (Chowdhery et al., 2022)\nbrought by large-scale pre-training, language mod-\nels (LMs) are gradually becoming unified inter-\nfaces (Hao et al., 2022), capable of instructing em-\nbodied robots for high-level tasks such as clean-\ning the spilled coke in interactive and embodied\nenvironments (Ahn et al., 2022). Understanding\nphysical concepts is an essential prerequisite for\n1Our dataset is available at https://github.com/\nTobiasLee/VEC.\nthese tasks, e.g., producing correct instructions for\ncleaning the coke requires understanding the visual\ncharacteristics of a coke can, as well as physical\nproperties such as hardness. However, it still re-\nmains unclear whether current LMs can understand\nbasic physical concepts (Driess et al., 2023).\nTo answer the question, we first define an evalua-\ntion suite of physical concepts covering visual and\nembodied concepts. Specifically, visual concepts\nexamine knowledge that can be gained via visual\nperception, including generic visual concepts, such\nas color, shape, and material of common objects,\nand spatial perception, which focuses on the rela-\ntionship between visual stimuli, i.e., relative size\nand height of objects. The ability to deal with vi-\nsual concepts serves as the basis for understanding\nreal-world scenes to perform further instruction.\nEmbodied concepts examine knowledge that re-\nquires more interaction and multimodal sensory ex-\nperience in the embodied world, including knowl-\nedge about the mass, temperature, and hardness of\nobjects, e.g., ice is colder than water. Understand-\ning embodied concepts is essential for an embod-\nied agent to make correct choices when translating\nlanguage into actions (Bisk et al., 2020a). We com-\npose a Visual and Embodied Concepts evaluation\nbenchmark VEC, with examples shown in Table 1.\nWith the benchmark, we examine a wide range\nof LMs. We cover masked language models and\ncausal language models in text-only LMs, includ-\ning BERT (Devlin et al., 2019) and RoBERTa (Liu\net al., 2019b), GPT (OPT)-family (Radford et al.,\n2019; Zhang et al., 2022b) with parameters rang-\ning from 125M to 175B, LLaMA-1/2 (Touvron\net al., 2023a,b) and Vicuna (Chiang et al., 2023).\nFurthermore, as humans understand the world by\nlearning from multiple modalities, especially using\nthe visual modality (Bloom, 2002), we are inter-\nested in whether the vision supervision in recent\nvision-augmented language models (VLMs) (Chen\net al., 2019; Radford et al., 2021; Wang et al., 2022;\n11843\nConcpet Category Instance Label # of Examples\nColor h: melon, t1: green, t2: black green 574\nShape h: lemon, t1: triangle, t2: round round 140\nVisual Concepts Material h: guitar, t1: wood, t2: glass wood 284\nSize h: ant, r: larger than, t: bird false 500\nHeight h: bottle, r: shorter than, t: truck true 500\nMass h: wooden spoon, r: heavier than, t: toaster false 654\nEmbodied Concepts Temperature h: ice, r: colder than, t: water true 422\nHardness h: pearl, r: softer than, t: glass true 1,016\nTable 1: The illustration of VEC benchmark. We design two forms of probing tasks. The former (Color, Shape and\nMaterial) asks models to make a choice between two tail options given the head object. The latter (Size, Height, and\nall embodied concepts) requires LMs to judge whether the relation is valid given the head and the tail.\nMadureira, 2021) could also facilitate the under-\nstanding ability of embodied concepts. CLIP (Rad-\nford et al., 2021) and BLIP (Li et al., 2022a) are\nchosen as representatives of VLMs for evaluation,\ndue to their promising performance and the abil-\nity to deal with textual-only inputs. To eliminate\nthe effects of training corpus (Tan and Bansal,\n2020), we train BERT, OPT, and CLIP on the\nsame caption dataset with a similar Transformer\nmodel (Vaswani et al., 2017) from scratch for a\nfair evaluation. Furthermore, as previous studies\nhave shown that prompting methods that fit the pre-\ntraining paradigm could better elicit the knowledge\nlearned from LMs (Petroni et al., 2019; Schick\nand Sch√ºtze, 2021a; Brown et al., 2020), we adopt\npre-trained-objective style promoting methods to\nnarrow the gap between probing and pre-training.\nOur zero (few)-shot results on the VEC bench-\nmark show that: (i) Moderate-sized LMs such as\nBERT and RoBERTa exhibit a random-level under-\nstanding of both visual and embodied concepts. (ii)\nA decent visual understanding of specific concepts\nemerges as LMs scale up, while they still struggle\nto understand the embodied knowledge with perfor-\nmance slightly better than random guessing. (iii)\nImage-grounded caption text-only pre-training, in-\nstruction tuning, and visual supervision could pro-\nvide performance gain regarding visual concepts,\nyet only the last one enhances the understanding of\nembodied knowledge of LMs.\nWe further investigate the source of embodied\nknowledge in VLMs. A case study demonstrates\nthat embodied knowledge in the VLM of CLIP\nis potentially rooted in the rich semantics of im-\nage representations. We thus devise a knowledge\ndistillation method to transfer the learned embod-\nied knowledge in VLMs into LMs, resulting in an\naverage accuracy gain of 3.38, comparable to the\n4.46 gain achieved by scaling the model parameters\n134x. Nevertheless, the improved LMs still exhibit\ngreat gaps with humans, indicating great potential\nfor further advancements.\n2 VEC Benchmark\nOur VEC benchmark aims to evaluate the under-\nstanding of physical concepts of LMs. Inspired by\nthe world scope definitions by Bisk et al. (2020a),\nwe divide physical knowledge into visual knowl-\nedge and embodied knowledge. The former are\nvisual properties that can be acquired via visual\nperception, while the latter focus on knowledge\nthat requires multimodal sensory interaction.\n2.1 Visual Concepts\nPerception is necessary for language learning be-\ncause it forms the basis for many of our semantic\naxioms (Bisk et al., 2020a). Among the various\ntypes of perception, visual concepts model a vast-\nness of experiences in the world that cannot be\nstated by text alone (Harnad, 1990). In this work,\nwe consider evaluating the visual understanding\nability of LMs by examining their performance\non various visual concepts. Specifically, we com-\nbine the recently proposed visual knowledge prob-\ning datasets, including Spatial Commonsense (Liu\net al., 2022a) and ViComTe (Zhang et al., 2022a).\nThe combined dataset requires not only understand-\ning various generic visual concepts including color,\nshape, and material, but also understanding the re-\nlationship between common objects, such as size\nand height. For generic visual concepts, i.e., color,\nshape, and material identification, we define an an-\nswer selection game: selecting a correct value from\ntwo options for the attribute given an object. For\n11844\nexample, given a head object banana, the model\nshould pick the ground-truth tail answer yellow in-\nstead of an alternative option such asblack. For vi-\nsual relationships, i.e., size and height understand-\ning, we define a comparison game: LMs need to\nperform a comparison between different objects.\nFor example, given a head entity ant and a tail\nentity bird, the LM is asked to compare the size\nof two objects and makes a prediction between the\ncorrect relation smaller and the false one larger.\n2.2 Embodied Concepts\nThe embodied concepts refer to physical realities\nof objects, e.g., mass, and temperature, which in-\nfants could learn by interacting with the environ-\nment (Gopnik et al., 1999). This kind of knowledge\nis the basis of intelligence and enables agent mod-\nels to explore challenging tasks in physical environ-\nments. We are curious about whether current LMs\ncan capture embodied knowledge via large-scale\npre-training. In this work, we define embodied\nknowledge as the knowledge that requires multi-\nmodal sensory interaction with the environments\nbeyond visual perception. We construct embod-\nied knowledge evaluation datasets regarding basic\nphysical properties including mass, temperature,\nand hardness.\nMass Dataset We build the Mass dataset by trans-\nforming the Image2Mass dataset curated by Stan-\ndley et al. (2017), which annotates 56 common\nobjects with corresponding weights. The most\nlight-weight object in the dataset is a red Lego\nbrick, weighing 0.026 lbs, and the heaviest ob-\nject is a 2.664 lbs drill. Directly asking the LM\nfor the absolute mass of objects can be challeng-\ning (Wallace et al., 2019). We define the task in\na comparison format. Specifically, each compar-\nison pair contains two objects with a weight gap\ngreater than 1 lbs. The threshold is set according\nto the Weber‚ÄìFechner laws (Fechner et al., 1966)\nto guarantee that the mass difference is perceivable\nfor humans. We build 654 triplets such as (hair\ndryer, heavier than, red Lego brick) for\nevaluation.\nTemperature Dataset We design a temperature\nprobing dataset by collecting the temperature of\n22 common objects from Wikipedia.2 For exam-\nple, the ice is 0‚ó¶C, and the temperature of water\n2https://en.wikipedia.org/wiki/Orders_of_\nmagnitude_(temperature)\nvapor is 100‚ó¶C. We convert the object with temper-\nature annotations into pairs, and each pair contains\ntwo objects and the corresponding temperature re-\nlation. For example, (ice, colder than, water\nvapor). The temperature gap between two ob-\njects must be greater than a difference threshold,\nwhich is loosely set to 10‚ó¶C for assurance of ther-\nmal perception for human (Jones, 2009). The final\nTemperature dataset consists of 422 pairs in total.\nHardness Dataset Hardness is a measure of the\nresistance to localized plastic deformation in ma-\nterial science. For example, hard metals such as\ntitanium are harder than soft minerals such as talc.\nHumans can perceive the hardness of different ma-\nterials in interaction with the environment by us-\ning tactile organs like fingers (Gueorguiev et al.,\n2016). To investigate whether LMs capture hard-\nness knowledge, we build a Hardness dataset by\ncollecting the Mohs hardness scores of 25 objects\nfrom Wikipedia.3 We define the task in a com-\nparison format. For example, (talc, softer\nthan, titanium) . Each pair contains two ob-\njects. The gap between two objects is greater than\nthe threshold for human-level understanding. The\nfinal dataset contains 1,016 pairs.\n3 Prompting Methods\nRecent studies have shown that prompting methods\nthat fit the pre-training paradigm are more effective\nthan other possible prompting methods (Petroni\net al., 2019; Schick and Sch√ºtze, 2021a). Following\nthese studies, we design specific prompts for LMs\nwith different objectives.\nPrompting Masked Language Models Follow-\ning PET (Schick and Sch√ºtze, 2021a,b), we probe\nthe masked language models by converting knowl-\nedge facts into a question-answering form. For\nexample, a size knowledge fact (coin, smaller\nthan, table) is converted into a sentence with\na special mask token: Question: is a coin\nsmaller than a table? Answer: [MASK] . We\nalso explored other prompts, such as Is a coin\n[MASK] than table. However, our experiments\nshow that a question-answering form can better\ninduce models to generate answers and avoid the\ninfluence of tokenization of different LMs. Given\nmasked inputs, the model is asked to predict the\nprobabilities of the mask token over two choices,\n3https://en.wikipedia.org/wiki/Mohs_scale_of_\nmineral_hardness\n11845\no1: This is a photo of the water.\no2: This is a photo of a frying oil.Attribute: This is a photo of a coldobject.\nNo:  66.3% Yes: 13.1%\nPerplexity of S2: 69.19Perplexity of S1: 87.22\n0.640.17\nOPT\nBERTs1: The wateris colderthan the frying oil.s2: The wateris ho(erthan the frying oil. ùíêùüèCLIP Text Encoder\nüòÑ\nüôÅ\n‚òπYesNo\ncos<ùíêùüè, ùíÇ>\nProbability Distribution of [MASK]Question: Is the watercolderthan the frying oil?Answer: [MASK].\ncos<ùíêùüê, ùíÇ>ùíêùüêùíÇ\nFigure 1: An illustration of prompting methods. For BERT-like models with a masked language head, we convert\nthe knowledge fact to a question and perform prediction with the head over yes or no. For OPT models, we evaluate\nthe perplexity of different assertions and take the one with lower perplexity as a valid fact. For CLIP, we devise a\nmatching-based probing framework.\ni.e., yes for confirming the knowledge fact is valid\nor no for an unreasonable assertion. We observe\nthat in specific LMs, the prediction can be biased\ntoward some answers as investigated by Zhao et al.\n(2021). We calibrate the prediction by normalizing\nthe probabilities according to an estimated prior\nfollowing Zhao et al. (2021).\nPrompting Causal Language Models Differ-\nent from BERT, there is no special [MASK] to-\nken in causal language models like GPT (Radford\net al., 2019). Therefore, introducing a special to-\nken would result in an inconsistency between pre-\ntraining and evaluation. To remedy this, for each\nknowledge fact, we state it in natural sentences ac-\ncording to prompting templates and evaluate the\nsentence perplexity as the proxy metric. Specif-\nically, for size-property evaluation, we convert it\ninto a valid knowledge assertion s1 =A coin is\nsmaller than a table , and an invalid one by\nreplacing the relation with the antonym adjective\ns2 =A coin is larger than a table . The\nsentence with lower perplexity is then chosen as\nthe predicted one. We evaluate the perplexity of\neach sentence s= (w0,w1,¬∑¬∑¬∑ ,wn) as:\nPPL(s)= PM(s)‚àí1\nn = n\nÓµ™Óµ´Óµ´‚àö\nn‚àè\nk=1\n1\nPM(wk | w0,w1,. . . ,wk‚àí1)\nwhere PMdenotes the conditional word probability\nof the causal language model to be probed and n\nis the number of tokens in s. We compare the\nperplexity PPL(s1) and PPL(s2) and choose the\nsentence with lower PPL as a more valid assertion\nand calculate the prediction accuracy accordingly.\nPrompting Vision-augmented Language Models\nof CLIP Unlike text-only LMs that support word\npredictions, the text encoder in CLIP only has one\nsentence representation without any pre-trained lan-\nguage heads. To probe the learned knowledge in\nVLMs such as CLIP, we design a matching-based\nprompting method. In more detail, for the size\nfact stated before, we first obtain two object de-\nscriptions o1 = a photo of a coin , and o2 =\na photo of a table . These two sentences are\nencoded to get the corresponding object vectors via\nthe CLIP language encoder:\no1,o2 = CLIP(o1),CLIP(o2).\nWe then derive an attribute sentencea= a photo\nof a small object, and encode it to an attribute\nadjective vector with the language encoder:\na = CLIP(a).\nThe prediction is then performed by comparing the\ncosine similarity cos(o1,a) and cos(o2,a).4 The\nobject with higher similarity with the attribute de-\nscription is adopted as the answer, i.e., a coin is\nsmaller than a table, if cos(o1,a) > cos(o2,a).\nOtherwise, we assume that the model thinks the\nreversed relation holds. We can also adopt the\nantonym adjective large for getting the attribute\nvectors. The results of the best-performing adjec-\ntive words for CLIP are reported and we discuss\nthe influence of adjective options in ¬ß 4.3.\n4 Experiments\n4.1 Experimental Settings\nModels We cover two kinds of LMs, text-only\nLMs and visual-augmented LMs. Text-only LMs\ninclude BERT-base/large (Devlin et al., 2019),\n4The matching-based prompting also applies to the pooled\nembedding of BERT, yet the results exhibit great variance as\nshown in Appendix A.\n11846\nModel (# of Param.) Color Shape Size Height Material Average\nBERTYFCC-15M (63M) 56.05¬±10.36 53.21¬±1.79 50.34¬±1.27 50.16¬±1.30 55.35¬±4.79 53.02\nOPTYFCC-15M(63M) 65.21¬±15.27 51.25¬±18.99 50.50¬±0.77 49.96¬±1.36 81.41¬±1.53 59.67\nCLIPYFCC-15M (63M) 68.21¬±7.17 67.21¬±7.63 62.64¬±6.01 54.04¬±7.05 62.92¬±6.48 63.00\nBERT-base (110M) 49.29¬±1.60 52.14¬±4.22 49.94¬±0.80 50.56¬±0.59 48.08¬±2.74 50.00\nBERT-large (340M) 49.36¬±1.88 51.21¬±5.06 49.26¬±1.60 49.08¬±2.34 49.72¬±0.58 49.73\nRoBERTa-base (125M) 49.07¬±1.62 49.36¬±3.52 50.32¬±0.57 49.58¬±0.49 49.86¬±1.44 49.64\nRoBERTa-large (355M) 49.66¬±0.54 50.68¬±1.48 50.54¬±1.46 50.14¬±0.45 50.00¬±0.14 50.20\nOPT (125M) 70.02¬±9.59 57.32¬±6.46 45.98¬±4.23 56.76¬±1.36 82.43¬±2.20 62.50\nOPT (1.3B ) 76.92¬±5.97 65.00¬±6.12 51.12¬±2.66 57.82¬±4.46 85.63¬±3.49 67.30\nOPT (13B) 79.62¬±5.28 62.50¬±6.44 57.56¬±6.60 54.58¬±4.53 88.38¬±3.14 68.53\nOPT (175B) 83.10¬±3.13 65.71¬±7.54 59.18¬±9.05 55.84¬±5.33 85.49¬±2.01 69.87\nLLaMa-1 (7B) 63.94¬±4.87 66.19¬±2.36 65.91¬±9.86 50.00¬±0.00 66.76¬±3.88 62.56\nVicuna-v1.3 (7B) 64.31¬±5.44 73.33¬±2.88 62.50¬±8.80 50.02¬±0.11 68.31¬±3.75 63.69\nLLaMa-1 (13B) 66.27¬±3.89 62.38¬±2.36 63.14¬±11.06 50.16¬±0.43 65.46¬±2.95 61.48\nVicuna-v1.3 (13B) 66.11¬±5.62 67.38¬±2.69 64.35¬±13.62 50.92¬±2.53 68.52¬±5.60 63.46\nLLaMa-2 (7B) 63.73¬±3.09 65.24¬±4.22 61.88¬±9.37 50.02¬±0.06 66.34¬±3.25 61.44\nLLaMa-2-Chat (7B) 60.99¬±5.18 70.95¬±2.76 68.03¬±9.91 51.72¬±2.20 67.39¬±4.13 63.82\nLLaMa-2 (13B) 66.59¬±3.40 62.38¬±3.21 68.20¬±11.51 50.10¬±0.18 66.73¬±3.99 62.80\nLLaMa-2-Chat (13B) 64.04¬±4.41 70.71¬±1.75 70.68¬±8.79 51.18¬±1.61 67.96¬±4.63 64.91\nCLIP-ViT/B-32 (63M) 80.07¬±2.57 84.43¬±2.57 61.40¬±6.02 62.28¬±6.40 80.07¬±2.57 73.94\nDeCLIP-ViT/B-32 (63M) 81.48¬±2.63 84.07¬±2.34 76.92¬±1.81 68.12¬±2.15 81.48¬±2.63 78.35\nCLIP-ViT/L-14 (123M) 80.33¬±3.61 85.00¬±4.03 63.96¬±6.10 60.72¬±5.56 80.33¬±3.61 74.21\nBLIP-base (138M) 82.60¬±5.50 84.86¬±2.80 76.00¬±6.40 69.84¬±7.76 80.67¬±1.24 78.79\nTable 2: Zero-shot probing results on visual datasets. Models with the YFCC-15M subscript represents that these\nmodels are trained from scratch on YFCC-15M data. Scaling OPT-family brings clear improvements on size and\ncolor datasets. The scaling law fails on the height dataset.\nRoBERTa-base/large (Liu et al., 2019b) for masked\nlanguage models, and OPT models with parame-\nters ranging from 125M to 175B. We further in-\ncorporate recent variants of causal language mod-\nels into evaluation, including LLaMA-1/2 (7B\nand 13B) (Touvron et al., 2023a), Vicuna models\n(7B and 13B, v1.3) (Chiang et al., 2023) trained\nwith the instruction tuning dataset, and LLaMa-\n2 Chat models (7B and 13B) (Touvron et al.,\n2023b) trained with supervised fine-tuning and\nRLHF (Ouyang et al., 2022). For VLMs, we\ninclude the text encoders of CLIP-ViT-B/32 and\nCLIP-ViT-L/14 (Radford et al., 2021) as a base\nand a large version, respectively. We also include\nan enhanced VLM with masked language mod-\neling as self-supervision, DeCLIP-ViT-B/32 (Li\net al., 2022b) and BLIP, a boosted VLM by uni-\nfying multi-modal understanding and generation\ntasks (Li et al., 2022a). 5 Since directly compar-\ning the VLMs and text-only LMs can be unfair\ndue to the difference in model configuration and\ntraining corpus (Tan and Bansal, 2020), we re-train\nCLIP, BERT, and GPT from scratch with a similar\nTransformer model on the same text corpus, the\n5https://huggingface.co/Salesforce/\nblip-itm-base-coco\ncaption dataset in the YFCC-15M dataset (Thomee\net al., 2016). All models are trained for 32 epochs.\nThe only difference between these models is the\npre-training objective. Detailed model and training\nsettings are elaborated in Appendix B.\nPrompts We manually write several prompts (at\nleast 4 prompts for each task) to eliminate the side-\neffect of the expression variations and report the\naveraged accuracy. Besides, the variance across\ndifferent prompts could also serve as an indicator\nto evaluate the robustness of learned knowledge\nfacts. We report the averaged performance over\nmultiple prompts for all models. All used prompts\ncan be found in Appendix C.\n4.2 Main Findings\nThe ability of certain visual concepts emerges\nas scaling up LMs, but there are still basic vi-\nsual concepts where the scaling law fails. The\nevaluation results on visual datasets are shown\nin Table 2. Interestingly, with the scaling up of\nOPT-family models, the prediction accuracy in-\ncreases obviously on specific visual concepts such\nas color and size. On material and color, the largest\nOPT-175B model even achieves better results than\nVLMs of CLIP-ViT/L-14, which are augmented\n11847\nModel (# of Param.) Mass Temperature Hardness Average\nBERTYFCC-15M(63M) 50.73¬±2.53 49.50¬±1.19 50.91¬±1.04 50.38\nGPTYFCC-15M(63M) 50.02¬±0.05 57.73¬±2.24 50.04¬±2.98 52.61\nCLIPYFCC-15M(63M) 67.45¬±5.16 64.83¬±4.17 62.22¬±3.11 64.83\nBERT-base (110M) 50.35¬±0.56 49.67¬±0.56 50.20¬±0.43 50.07\nBERT-large (340M) 49.97¬±1.31 49.83¬±0.50 49.98¬±0.06 49.93\nRoBERTa-base (125M) 49.65¬±0.51 50.00¬±0.00 48.04¬±2.04 49.23\nRoBERTa-large (355M) 50.08¬±0.23 50.07¬±0.19 49.95¬±0.15 50.03\nOPT (125M) 50.00¬±0.00 54.53¬±4.33 46.16¬±2.45 50.23\nOPT (1.3B) 50.05¬±0.10 50.90¬±5.08 53.03¬±2.69 51.33\nOPT (13B) 50.14¬±0.36 51.85¬±6.34 52.38¬±3.09 51.46\nOPT (175B) 50.21¬±0.24 59.83¬±8.68 57.33¬±3.41 55.79\nLLaMa-1 (7B) 54.88¬±2.49 60.69¬±4.35 51.97¬±2.84 55.84\nVicuna-v1.3 (7B) 54.23¬±1.78 58.85¬±4.36 54.42¬±6.42 55.83\nLLaMa-1 (13B) 53.69¬±3.81 50.76¬±8.69 53.94¬±4.45 52.80\nVicuna-v1.3 (13B) 56.90¬±3.53 53.32¬±6.47 55.50¬±5.73 55.24\nLLaMa-2 (7B) 54.01¬±4.47 56.87¬±6.25 55.22¬±5.89 55.37\nLLaMa-2-Chat (7B) 52.51¬±4.83 61.99¬±3.93 55.65¬±5.28 56.72\nLLaMa-2 (13B) 53.38¬±2.10 57.54¬±7.51 53.01¬±4.57 54.64\nLLaMa-2-Chat (13B) 54.13¬±2.73 56.68¬±6.02 54.12¬±4.64 54.98\nCLIP-ViT/B-32 (63M) 65.20¬±4.75 60.28¬±6.83 59.43¬±2.00 61.64\nDeCLIP-ViT/B-32 (63M) 54.95¬±2.00 68.58¬±3.08 61.10¬±4.14 61.54\nCLIP-ViT/L-14 (123M) 73.15¬±6.34 65.88¬±2.31 69.57¬±2.26 69.53\nBLIP-base (138M) 83.94¬±2.59 74.98¬±5.60 56.93¬±5.56 71.95\nTable 3: Zero-shot results on embodied datasets. LMs struggle to understand embodied knowledge, including OPT\n(175B) and visual-augmented LMs, with 71.95 as the best average performance.\nModel Mass Temperature Hardness Avg.\nZero-shot Best VLMs83.94¬±2.59 74.98¬±5.60 69.57¬±2.26 76.16\nBERT-base 64.72¬±4.77 55.62¬±1.34 51.80¬±1.31 57.38\nBERT-large 65.47¬±4.86 54.19¬±2.31 52.73¬±1.22 57.46\nRoBERTa-base 60.24¬±5.24 60.27¬±4.85 50.44¬±1.13 56.98\nRoBERTa-large 61.18¬±4.01 58.28¬±2.09 50.56¬±1.14 56.67\nTable 4: The few-shot results of BERT variants. With16\ninstances, the fine-tuned BERT variants are still worse\nthan zero-shot visual-augmented LMs.\nwith vision supervision and are supposed to per-\nform better (Zhang et al., 2022a; Liu et al., 2022b).\nA potential reason is that the combination of color\nand material frequently occurs (e.g., red apples)\nin raw texts, and these co-occurrence statistics are\nwell captured by large LMs. The significant per-\nformance improvements after training on visual-\ngrounded text corpus YFCC-15M validate this ex-\nplanation. Besides, OPT-13B and LLaMa-1 13B\nmodels excel in different visual concepts, with\nOPT-13B performing well on material concepts and\nLLaMa-1 13B on relative size comparisons, likely\ndue to the difference of pre-training corpus distri-\nbution. However, increasing LMs to 175B brings\nnegligible improvements in the Height dataset, indi-\ncating that there still remain visual concepts where\nthe scaling law does not hold even though these\nconcepts can be easily captured by humans.\n/uni000003f1/uni000003f4/uni000003f2/uni000003ec/uni000003f2/uni000003ee/uni000003f2/uni000003f0/uni000003f2/uni000003f2/uni000003f2/uni000003f4/uni000003f3/uni000003ec/uni000003f3/uni000003ee/uni000003f3/uni000003f0/uni000003f3/uni000003f2\n/uni00000057/uni0000018c/uni0000011e/uni0000011a/uni0000015d/uni00000110/uni0000019a/uni0000015d/uni0000017d/uni00000176/uni00000003/uni00000004/uni00000110/uni00000110/uni000001b5/uni0000018c/uni00000102/uni00000110/uni000001c7/uni00000003/uni0000037e/uni00000439/uni0000037f\n/uni00000064/uni0000011e/uni00000175/uni00000189/uni0000011e/uni0000018c/uni00000102/uni0000019a/uni000001b5/uni0000018c/uni0000011e\n/uni0000002c/uni00000102/uni0000018c/uni0000011a/uni00000176/uni0000011e/uni00000190/uni00000190\n/uni00000044/uni00000102/uni00000190/uni00000190\n/uni00000057/uni0000018c/uni0000017d/uni0000010f/uni0000015d/uni00000176/uni00000150/uni00000003/uni00000064/uni00000102/uni00000190/uni0000016c\nFigure 2: Few-shot results of OPT-175B with 16 in-\nstances as demonstration on embodied tasks.\nLMs exhibit a poor understanding of embod-\nied concepts. As shown in Table 3, the scaling\nlaw fails again on the embodied concepts, as all\nLMs, including OPT-175B and variants trained\nwith captions data, perform poorly. Among LMs,\nthe LLaMa series shows a better performance in\nembodied concepts, yet still reaches a plateau of\naround 55% overall accuracy. We further conduct\na few-shot prompt evaluation for OPT models by\nconstructing the inputs with k= 16randomly sam-\npled instances and adopt PET (Schick and Sch√ºtze,\n2021a) for masked language models. The results\nare illustrated in Figure 2 and Table 4, respectively.\nWe find that while the performance is boosted, the\naverage results are still worse than the CLIP-ViT/L-\n11848\n/uni00000012/uni0000017d/uni0000016f/uni0000017d/uni0000018c/uni0000005e/uni0000015a/uni00000102/uni00000189/uni0000011e/uni0000005e/uni0000015d/uni000001cc/uni0000011e/uni0000002c/uni0000011e/uni0000015d/uni00000150/uni0000015a/uni0000019a/uni00000044/uni00000102/uni0000019a/uni0000011e/uni0000018c/uni0000015d/uni00000102/uni0000016f/uni00000044/uni00000102/uni00000190/uni00000190/uni00000064/uni0000011e/uni00000175/uni00000189/uni00000358/uni0000002c/uni00000102/uni0000018c/uni0000011a/uni00000176/uni0000011e/uni00000190/uni00000190\n/uni00000064/uni00000102/uni00000190/uni0000016c\n/uni000003f1/uni000003ec\n/uni000003f2/uni000003ec\n/uni000003f3/uni000003ec\n/uni000003f4/uni000003ec\n/uni000003f5/uni000003ec\n/uni000003ed/uni000003ec/uni000003ec/uni00000057/uni0000018c/uni0000011e/uni0000011a/uni0000015d/uni00000110/uni0000019a/uni0000015d/uni0000017d/uni00000176/uni00000003/uni00000004/uni00000110/uni00000110/uni000001b5/uni0000018c/uni00000102/uni00000110/uni000001c7/uni00000003/uni0000037e/uni00000439/uni0000037f\n/uni0000002c/uni000001b5/uni00000175/uni00000102/uni00000176/uni0000004b/uni00000057/uni00000064/uni00000372/uni000003ed/uni000003f3/uni000003f1/uni00000011/uni00000012/uni0000003e/uni0000002f/uni00000057/uni00000372/uni00000073/uni0000015d/uni00000064/uni00000372/uni0000003e/uni0000036c/uni000003ed/uni000003f0/uni00000018/uni0000011e/uni00000012/uni0000003e/uni0000002f/uni00000057/uni00000372/uni00000073/uni0000015d/uni00000064/uni00000372/uni00000011/uni0000036c/uni000003ef/uni000003ee/uni00000011/uni0000003e/uni0000002f/uni00000057/uni00000372/uni0000010f/uni00000102/uni00000190/uni0000011e\nFigure 3: Comparison between the best-performing\nmodels and human annotators on sampled subsets of\nVEC. The best-performing LMs and VLMs achieve\nclose-to-human results on visual datasets, yet far lag\nbehind humans in embodied datasets.\n14 model without any demonstration, which only\nutilizes 0.08% parameters of OPT-175B. These\nfindings show that visual supervision can help learn\nembodied knowledge, but there is still a large gap\nbetween the best results of existing LMs with hu-\nman performance.\nCompared with human annotators, OPT-175B\nand VLMs achieve competitive performance re-\ngarding visual concepts, yet they exhibit great\ngaps with humans on embodied concepts. We\nconduct a human evaluation to better understand\nthe performance of different models. Specifically,\nwe randomly sample 100 examples for each task\nand ask three volunteers to label these examples.\nThe annotators achieve substantial agreements on\nall the tasks with Cohen‚Äôs kappa (Cohen, 1960) Œ∫\nlarger than0.7, except for the Hardness dataset with\na moderate Œ∫= 0.52. The comparison with best-\nperforming models, i.e., OPT-175B, CLIP-ViT/L-\n14 and DeCLIP is illustrated in Figure 3. We find\nthat (i) Regarding visual concepts, both OPT and\nCLIP-like models perform closely to human an-\nnotators. CLIP and DeCLIP even outperform the\nhuman annotators on the shape task, which is poten-\ntially due to the noise introduced by the automatic\nconstruction of the dataset (Zhang et al., 2022a).\nOverall, the close-to-human results indicate that\nvisual knowledge can be effectively acquired by\nlarge-scale cross-modal pre-training or even text-\nonly pre-training with sufficient parameters. (ii)\nRegarding embodied concepts, the best-performing\nCLIP-ViT-L/14 model still has an absolute18.5%\naccuracy gap with the humans. The clear perfor-\nmance gaps reveal that there is still a long way to\ngo in equipping LMs and VLMs with embodied\nknowledge.\n/uni000003ec/uni00000358/uni000003ec/uni000003ec/uni00000358/uni000003ed/uni000003ec/uni00000358/uni000003ee/uni000003ec/uni00000358/uni000003ef/uni000003ec/uni00000358/uni000003f0/uni000003ec/uni00000358/uni000003f1/uni000003ec/uni00000358/uni000003f2/uni000003ec/uni00000358/uni000003f3/uni000003ec/uni00000358/uni000003f4/uni000003ec/uni00000358/uni000003f5/uni000003ed/uni00000358/uni000003ec\n/uni00000012/uni0000017d/uni0000018c/uni0000018c/uni0000011e/uni00000110/uni0000019a/uni00000003/uni0000005a/uni00000102/uni0000019a/uni0000015d/uni0000017d\n/uni000003ec\n/uni000003ee/uni000003ec\n/uni000003f0/uni000003ec\n/uni000003f2/uni000003ec\n/uni000003f4/uni000003ec\n/uni000003ed/uni000003ec/uni000003ec/uni00000012/uni0000017d/uni000001b5/uni00000176/uni0000019a\n/uni00000044/uni00000102/uni0000019a/uni0000011e/uni0000018c/uni0000015d/uni00000102/uni0000016f\n/uni0000002c/uni00000102/uni0000018c/uni0000011a/uni00000176/uni0000011e/uni00000190/uni00000190\nFigure 4: Entity correct ratio histograms of Mass and\nMaterial datasets across different prompts. BERT could\nmake consistent correct predictions for specific entities,\nand the bell curve on the hardness indicates it is chal-\nlenging for BERT to understand embodied concepts.\nInstruction tuning enhances proficiency in both\nvisual and embodied concepts. After post-\ntraining with the instruction tuning dataset, Vicuna\nmodels display enhanced proficiency in both visual\nand embodied concepts, with larger LLMs demon-\nstrating a more significant improvement. For in-\nstance, when using LLaMa-1 (13B) as a baseline\nmodel, the average accuracy in three embodied\ntasks rises from 52.8 to 55.2. Moreover, LLaMa-\n2-Chat models, which are further trained with a\nsupervised instruction tuning dataset and RLHF\ntechniques, show consistent accuracy gains in both\nvisual and embodied concept tasks as well. How-\never, disentangling the influence of instruction tun-\ning and RLHF on these models presents a challenge\nas they are intertwined. Nevertheless, a clear per-\nformance gap still remains between more recent\nLMs and VLMs, indicating the significance of vi-\nsual supervision.\n4.3 Analysis\nDoes BERT behave similarly regarding visual\nand embodied concepts? The overall prediction\nresults of BERT-like models in the visual and em-\nbodied world are both at a random level. We inves-\ntigate this question result by first checking whether\nBERT models perform consistently at a guessing\nlevel for all the entities in the dataset. We compute\nthe entity correct ratio among different prompts for\nthe objects in different datasets and compare the\ndistribution on different tasks with the BERT model\ntrained on YFCC-15M dataset. As illustrated in\nFigure 4, in the Material identification task, there\nare entities that the model that could provide consis-\ntent correct predictions. However, the distribution\nof the Hardness dataset in embodied evaluation ex-\nhibits a bell curve, i.e., most entities are predicted\n11849\n00.10.20.30.40.50.60.70.80.91\nProbabilityofHeavyObject\nElephant StoneToy      Clip   Feather0.40.50.60.70.80.9\nProbabilityofHotObject\n0s       30s          60s   90s Time4e+3  2e+1   1   2e-3  1e-4kg\nFigure 5: Case study showing that the image represen-\ntations in CLIP exhibit embodied knowledge. (Left)\nThe probability of an image being classified as \"hot\"\nincreases as the ice melts being heated in a boiler in\na video. (Right) The probability of an image being\nclassified as \"heavy\" along with corresponding mass\nannotation.\ncorrectly at a random-chance level. The distribu-\ntions of other tasks show similar results and can be\nfound in Appendix D. These results suggest that\nBERT learns visual knowledge for certain entities\nyet indeed struggles regarding embodied concepts.\nExploring learned embodied knowledge in im-\nage representations. We are interested in how\nthe VLMs of CLIP learn embodied knowledge. A\npotential answer is that the images contain rich se-\nmantics regarding embodied knowledge such as the\nheat of the object, and the knowledge can be prop-\nagated to the VLMs via the contrastive learning ob-\njective. To examine this, we perform a case study\nby calculating the attribute similarities among the\nimages. We first take clips from a video of heating\na pile of ice and then perform a binary classifi-\ncation by calculating the cosine similarities with\ntext prompts a photo of a hot object. and a photo\nof a cold object for each frame. The left of Fig-\nure 5 shows that the probability of a hot object\nincreases during the heating procedure. Similarly,\nwe perform a binary classification over heavy and\nlight-weight objects ranging from an elephant to\na feather. The right of Figure 5 shows that the\nimage representations are aware of the mass of dif-\nferent objects. This qualitative study shows that\nimage representations are the potential source of\nembodied knowledge.\nTransferring embodied knowledge from VLMs\nto LMs. We further verify whether the learned\nembodied knowledge in CLIP could be transferred\nto text-only models. Specifically, we perform\nknowledge distillation (Hinton et al., 2015) by treat-\ning the original text-only language model as a stu-\ndent, and the CLIP text encoder as a teacher model\nproviding the learned embodied knowledge. How-\nAphotoofasoftobject.59.43% (std: 2.00%) \nAphotoofahardobject.44.00% (std: 1.26%)\nAphotoofalight-weightobject.35.63% (std: 2.68%)\nAphotoofaheavyobject.65.20% (std: 4.75%)\nAphotoofasoftobject.59.43% (std: 2.00%) \nAphotoofahardobject.44.00% (std: 1.26%)\nAphotoofalight-weightobject.35.63% (std: 2.68%)\nAphotoofaheavyobject.65.20% (std: 4.75%)\nAphotoofasoftobject.59.43% (std: 2.00%) \nAphotoofahardobject.44.00% (std: 1.26%)\nAphotoofalight-weightobject.35.63% (std: 2.68%)\nAphotoofaheavyobject.65.20% (std: 4.75%)\nAphotoofasoftobject.59.43% (std: 2.00%) \nAphotoofahardobject.44.00% (std: 1.26%)\nAphotoofalight-weightobject.35.63% (std: 2.68%)\nAphotoofaheavyobject.65.20% (std: 4.75%)\nFigure 6: Top-5 retrieved images and the prediction\naccuracy with different attribute prompts. The accuracy\ndrops when the text inputs contain ambiguous words\nand compound words, as the retrieved images are biased\ntoward specific meanings.\never, our preliminary study in Appendix F shows\nthat vanilla alignment on the predicted word distri-\nbutions could not be effective. Inspired by our case\nstudy showing that the rich embodied knowledge\ncontained in the representations, we utilize Neu-\nron Selectivity Transfer (Huang and Wang, 2017)\nwhich transfers the inner states such as spatial acti-\nvation patterns of teacher neurons to student neu-\nrons, by aligning the token representations of the\nlast layer between the teacher and student language\nmodels, which is implemented as a squared maxi-\nmum mean discrepancy (MMD) with a polynomial\nkernel to measure the distance between the activa-\ntion patterns of student neurons and teacher neu-\nrons. The total training objective of the language\nmodel is a combination of the original language\nmodeling loss and the MMD loss with a balanc-\ning coefficient. We refer readers to Appendix E\nfor more details. As shown in Table 5, the dis-\ntillation provides a performance boost on embod-\nied concepts understanding, e.g., learning from a\nCLIP-ViT-L/14 teacher model achieves improve-\nment that is comparable with that brought by scal-\ning the model parameter 134x from OPT-1.3B to\nOPT-175B.6 It validates our assumption and in-\ndicates that future studies could utilize the richer\nrepresentations in VLMs for improving LMs, yet\nthe gap between distilled LM and VLM suggests\nthat there is still room for advancement.\nVLMs perform poorly when dealing with am-\nbiguous text descriptions. During our prelimi-\nnary study, we observe that VLMs of CLIP perform\nrelatively poorly for specific adjectives such as\nhard. To further investigate this issue, we examine\nthe retrieved images using prompts with different\nattribute adjectives on the CC12M dataset (Chang-\n6Only OPT is adopted for experiments as the CLIP encoder\ncannot deal with the mask tokens introduced in BERT.\n11850\npinyo et al., 2021). Our results, illustrated in Fig-\nure 6, revealed that for the prompta photo of a hard\nobject, the retrieved images were mostly about ab-\nstract and difficult learning materials, with only\none rock image related to the attribute of hardness.\nAdditionally, for the prompt light-weight, the re-\ntrieved images are biased towards meanings related\nto lighting bulbs and light-toned colors. These ob-\nservations demonstrate that handling semantic am-\nbiguity remains a challenge for VLMs (Ren et al.,\n2023), suggesting that future improvements may\nincorporate more language-side supervision into\nthe text encoder of VLMs (Li et al., 2022b).\n5 Related Work\nProbing Language Models Understanding what\nLMs know after large-scale pre-training is an active\nresearch area (Rogers et al., 2020). Various prob-\ning methods have been developed (Tenney et al.,\n2019b; Petroni et al., 2019), and investigations\nshow that LMs capture linguistic (Tenney et al.,\n2019a; Liu et al., 2019a), factual (Petroni et al.,\n2019; Roberts et al., 2020; Dai et al., 2022), com-\nmonsense knowledge (Wang et al., 2019; Forbes\net al., 2019), and even acquire grounded con-\ncepts (Patel and Pavlick, 2022). For VLMs, stud-\nies demonstrate their potential in acquiring spa-\ntial commonsense (Zhang et al., 2022a; Liu et al.,\n2022a; Xia et al., 2023) and color perception (Ab-\ndou et al., 2021), yet performing worse on NLU\ntasks (Tan and Bansal, 2020) and achieving no sig-\nnificant on lexical grounding (Yun et al., 2021).\nIn this paper, we investigate the ability of LMs\nto understand physical concepts. Different from\nPIQA (Bisk et al., 2020b) consists of questions re-\nquiring physical commonsense reasoning, our VEC\nbenchmark examines the understanding ability of\nthe fundamental physical concepts. The evaluation\non the VEC benchmark demonstrates that text-only\nLMs can learn specific visual concepts after scaling\nup while struggling with the embodied concepts.\nVision-Language Pre-training Unifying cross-\nmodal representations via vision-language pre-\ntraining has achieved promising progress. Pilot\nstudies adopt masked reconstruction to learn shared\nrepresentations across modalities from a mixed vi-\nsual and language inputs (Li et al., 2019; Tan and\nBansal, 2019; Su et al., 2020; Chen et al., 2019;\nLi et al., 2020). CLIP (Radford et al., 2021) in-\ntroduces a contrastive language-image pre-training\nframework, utilizing language as supervision for\nModel Mass Temperature HardnessAvg.\nCLIP-ViT/B-32 (T1)65.20¬±4.75 60.28¬±6.83 59.43¬±2.00 61.64CLIP-ViT/L-14 (T2)73.15¬±6.34 65.88¬±2.31 69.57¬±2.26 69.53\nOPT-1.3B 50.05¬±0.10 50.90¬±5.08 53.03¬±2.69 51.33scale up to 13B50.14¬±0.36 51.85¬±6.34 52.38¬±3.09 51.46(+0.13)scale up to 175B50.21¬±0.24 59.83¬±8.68 57.33¬±3.41 55.79(+4.46)\nOPTYFCC-15M 50.02¬±0.05 57.73¬±2.24 50.04¬±2.98 52.61Distill w/ T149.88¬±0.37 55.76¬±4.01 53.23¬±3.12 52.96(+0.35)Distill w/ T254.27¬±5.20 60.78¬±4.23 52.91¬±1.62 55.99 (+3.38)\nTable 5: Results of embodied distillation. Transferring\nembodied knowledge from CLIP-ViT to OPT brings\na gain of 3.38 points, which is comparable with the\nimprovements by scaling the model from 1.3B to 175B.\nlearning transferable image representations with\nlarge-scale image-text pairs, triggering a series of\nvariants for further improvements (Jia et al., 2021;\nLi et al., 2022b; Yao et al., 2022; Li et al., 2021,\n2022a). Our study uses VLMs of CLIP and BLIP\nto investigate the impact of visual supervision on\nunderstanding physical concepts and our results\nsuggest that visual supervision is crucial for LMs\nto understand embodied concepts, which can be\nutilized to enhance the text-ony LMs.\n6 Conclusion\nIn this paper, we introduce VEC for evaluating the\nunderstanding of physical concepts in LMs. Our\nresults show that large LMs understand specific\nvisual concepts but struggle with embodied knowl-\nedge. VLMs instead perform much better in both\nthe visual and the embodied world, indicating that\nvisual signals are vital for understanding physical\nconcepts. Further analysis suggests that transfer-\nring the VLM representations to LMs effectively\nboosts embodied concepts understanding, shedding\nlight on directions for improving LMs.\nLimitations\nLimited Scopes of Physical Concepts In this\nwork, we focus on evaluating certain physical prop-\nerties such as color, mass, temperature, and hard-\nness. These properties are chosen because they can\nbe measured using well-established metrics and are\neasily sensed by humans. However, this selection\nintroduces a bias in our approximation of embodied\nknowledge. Despite this bias, our results are still\nsufficient to demonstrate the poor performance of\ncurrent text-only language models (LMs) in under-\nstanding embodied concepts. We suggest that in-\ncorporating vision supervision could help improve\nthe understanding of embodied concepts. Addi-\ntionally, our current benchmark only examines the\n11851\nfine-grained understanding ability of specific phys-\nical concepts, while neglecting the more complex\nphysical understanding that involves multiple in-\nteractions or observations within a single example.\nDeveloping a dataset that encompasses composi-\ntional physical concepts holds promise for future\nresearch.\nLimited Adoption of VLMs While there are\nmany multi-modal models available, we restrict our\ninvestigation to visual-linguistic models (VLMs)\nbased on CLIP and its variants. We choose\nCLIP for its superior image representation perfor-\nmance and support for text-only encoders. Since\nour evaluation focuses on language-oriented tasks,\nwe require models that can handle inputs con-\nsisting of pure text. Consequently, VLMs like\nUNITER (Chen et al., 2019), which require multi-\nmodal inputs, are not considered. CLIP is selected\nas a representative work of VLMs for evaluation.\nHowever, it is important to note that the findings\nfrom CLIP may not readily generalize to other\nV+L models, as CLIP utilizes a large dataset of\nmillion-level image-text pairs collected from the\nweb, which could be a significant source of em-\nbodied knowledge itself. Furthermore, there have\nbeen recent proposals for VLM models with vari-\nous architectures and pre-training recipes, such as\nSimVLM (Wang et al., 2022), UniT (Hu and Singh,\n2021), ViLT (Kim et al., 2021), and FLA V A (Singh\net al., 2022), as well as vision-enhanced multi-\nmodal agents like InstructBLIP (Dai et al., 2023),\nQwen-VL (Bai et al., 2023), and Ying-VLM (Li\net al., 2023). These models have shown promis-\ning performance in both cross-modal and single-\nmodality tasks, and we look forward to evaluating\nthese advanced models in our benchmark in the\nfuture.\nAcknowledgments\nWe thank all the anonymous reviewers for their\nconstructive comments, Yuxuan Fan, Shuhuai Ren\nand Sijie Cheng for their valuable suggestions in\npreparing the manuscript.\nReferences\nMostafa Abdou, Artur Kulmizev, Daniel Hershcovich,\nStella Frank, Ellie Pavlick, and Anders S√∏gaard.\n2021. Can language models encode perceptual struc-\nture without grounding? a case study in color. In Pro-\nceedings of the 25th Conference on Computational\nNatural Language Learning, pages 109‚Äì132.\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen\nChebotar, Omar Cortes, Byron David, Chelsea Finn,\nKeerthana Gopalakrishnan, Karol Hausman, Alex\nHerzog, et al. 2022. Do as i can, not as i say: Ground-\ning language in robotic affordances. ArXiv preprint,\nabs/2204.01691.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,\nSinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\nand Jingren Zhou. 2023. Qwen-vl: A frontier large\nvision-language model with versatile abilities. ArXiv\npreprint, abs/2308.12966.\nYonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob\nAndreas, Yoshua Bengio, Joyce Chai, Mirella Lap-\nata, Angeliki Lazaridou, Jonathan May, Aleksandr\nNisnevich, Nicolas Pinto, and Joseph Turian. 2020a.\nExperience grounds language. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 8718‚Äì8735.\nYonatan Bisk, Rowan Zellers, Ronan LeBras, Jianfeng\nGao, and Yejin Choi. 2020b. PIQA: reasoning about\nphysical commonsense in natural language. In The\nThirty-Fourth AAAI Conference on Artificial Intelli-\ngence, AAAI 2020, The Thirty-Second Innovative Ap-\nplications of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020, pages 7432‚Äì\n7439.\nPaul Bloom. 2002. How children learn the meanings of\nwords.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and\nRadu Soricut. 2021. Conceptual 12m: Pushing web-\nscale image-text pre-training to recognize long-tail\nvisual concepts. In IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2021, virtual,\nJune 19-25, 2021, pages 3558‚Äì3568.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El\nKholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2019. Uniter: Learning universal image-\ntext representations. ArXiv.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\n11852\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. ArXiv preprint,\nabs/2204.02311.\nJacob Cohen. 1960. A coefficient of agreement for\nnominal scales. Educational and psychological mea-\nsurement, 20(1):37‚Äì46.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2022. Knowledge neurons in\npretrained transformers. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 8493‚Äì\n8502.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony\nMeng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. 2023. In-\nstructblip: Towards general-purpose vision-language\nmodels with instruction tuning. ArXiv preprint ,\nabs/2305.06500.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171‚Äì4186.\nDanny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey\nLynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe\nYu, Wenlong Huang, Yevgen Chebotar, Pierre Ser-\nmanet, Daniel Duckworth, Sergey Levine, Vincent\nVanhoucke, Karol Hausman, Marc Toussaint, Klaus\nGreff, Andy Zeng, Igor Mordatch, and Pete Florence.\n2023. Palm-e: An embodied multimodal language\nmodel. volume abs/2303.03378.\nGustav Theodor Fechner, Davis H Howes, and Ed-\nwin Garrigues Boring. 1966. Elements of psy-\nchophysics, volume 1.\nMaxwell Forbes, Ari Holtzman, and Yejin Choi. 2019.\nDo neural language representations learn physical\ncommonsense? In CogSci.\nAlison Gopnik, Andrew N Meltzoff, and Patricia K\nKuhl. 1999. The scientist in the crib: Minds, brains,\nand how children learn.\nDavid Gueorguiev, S√©r√©na Bochereau, Andr√© Mouraux,\nVincent Hayward, and Jean-Louis Thonnard. 2016.\nTouch uses frictional cues to discriminate flat materi-\nals. Scientific reports.\nYaru Hao, Haoyu Song, Li Dong, Shaohan Huang,\nZewen Chi, Wenhui Wang, Shuming Ma, and Furu\nWei. 2022. Language models are general-purpose\ninterfaces. ArXiv preprint, abs/2206.06336.\nStevan Harnad. 1990. The symbol grounding problem.\nPhysica D: Nonlinear Phenomena.\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015.\nDistilling the knowledge in a neural network. ArXiv\npreprint, abs/1503.02531.\nRonghang Hu and Amanpreet Singh. 2021. Unit: Mul-\ntimodal multitask learning with a unified transformer.\nIn 2021 IEEE/CVF International Conference on Com-\nputer Vision, ICCV 2021, Montreal, QC, Canada,\nOctober 10-17, 2021, pages 1419‚Äì1429.\nZehao Huang and Naiyan Wang. 2017. Like what you\nlike: Knowledge distill via neuron selectivity transfer.\nArXiv preprint, abs/1707.01219.\nShankar Iyer, Nikhil Dandekar, and Korn√©l Csernai.\n2017. Quora question pairs. First Quora Dataset\nRelease: Question Pairs.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc V . Le, Yun-Hsuan Sung,\nZhen Li, and Tom Duerig. 2021. Scaling up vi-\nsual and vision-language representation learning with\nnoisy text supervision. In Proceedings of the 38th In-\nternational Conference on Machine Learning, ICML\n2021, 18-24 July 2021, Virtual Event, volume 139 of\nProceedings of Machine Learning Research, pages\n4904‚Äì4916.\nLynette Jones. 2009. Thermal touch. Scholarpedia.\nWonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt:\nVision-and-language transformer without convolu-\ntion or region supervision. In Proceedings of the\n38th International Conference on Machine Learning,\nICML 2021, 18-24 July 2021, Virtual Event, volume\n139 of Proceedings of Machine Learning Research,\npages 5583‚Äì5594.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven C. H.\nHoi. 2022a. BLIP: bootstrapping language-image\npre-training for unified vision-language understand-\ning and generation. In International Conference on\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\ntimore, Maryland, USA, volume 162 of Proceedings\nof Machine Learning Research, pages 12888‚Äì12900.\nJunnan Li, Ramprasaath R. Selvaraju, Akhilesh\nGotmare, Shafiq R. Joty, Caiming Xiong, and\nSteven Chu-Hong Hoi. 2021. Align before fuse:\nVision and language representation learning with\nmomentum distillation. In Advances in Neural In-\nformation Processing Systems 34: Annual Confer-\nence on Neural Information Processing Systems 2021,\nNeurIPS 2021, December 6-14, 2021, virtual, pages\n9694‚Äì9705.\n11853\nLei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi\nWang, Shuhuai Ren, Mukai Li, Yazheng Yang,\nJingjing Xu, Xu Sun, Lingpeng Kong, and Qi Liu.\n2023. M 3IT: A large-scale dataset towards multi-\nmodal multilingual instruction tuning. ArXiv\npreprint, abs/2306.04387.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A sim-\nple and performant baseline for vision and language.\nArXiv.\nXiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu,\nPengchuan Zhang, Lei Zhang, Lijuan Wang,\nHoudong Hu, Li Dong, Furu Wei, Yejin Choi,\nand Jianfeng Gao. 2020. Oscar: Object-semantics\naligned pre-training for vision-language tasks. In\nProc. of ECCV.\nYangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui,\nWanli Ouyang, Jing Shao, Fengwei Yu, and Jun-\njie Yan. 2022b. Supervision exists everywhere: A\ndata efficient contrastive language-image pre-training\nparadigm. In The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073‚Äì1094.\nXiao Liu, Da Yin, Yansong Feng, and Dongyan Zhao.\n2022a. Things not written in text: Exploring spatial\ncommonsense from visual signals. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 2365‚Äì2376.\nXiao Liu, Da Yin, Yansong Feng, and Dongyan Zhao.\n2022b. Things not written in text: Exploring spatial\ncommonsense from visual signals. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 2365‚Äì2376.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoBERTa: A robustly optimized BERT pretraining\napproach. ArXiv preprint.\nBrielen Madureira. 2021. Flamingos and hedgehogs\nin the croquet-ground: Teaching evaluation of NLP\nsystems for undergraduate students. In Proceedings\nof the Fifth Workshop on Teaching NLP, pages 87‚Äì91.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730‚Äì27744.\nRoma Patel and Ellie Pavlick. 2022. Mapping language\nmodels to grounded conceptual spaces. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nFabio Petroni, Tim Rockt√§schel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463‚Äì2473.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision. In Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24\nJuly 2021, Virtual Event, volume 139 of Proceedings\nof Machine Learning Research, pages 8748‚Äì8763.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions\nfor machine comprehension of text. In Proceedings\nof the 2016 Conference on Empirical Methods in\nNatural Language Processing, pages 2383‚Äì2392.\nShuhuai Ren, Lei Li, Xuancheng Ren, Guangxiang\nZhao, and Xu Sun. 2023. Delving into the open-\nness of CLIP. In Findings of the Association for\nComputational Linguistics: ACL 2023, pages 9587‚Äì\n9606.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418‚Äì5426.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know about\nhow BERT works. Transactions of the Association\nfor Computational Linguistics, 8:842‚Äì866.\nTimo Schick and Hinrich Sch√ºtze. 2021a. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255‚Äì269.\nTimo Schick and Hinrich Sch√ºtze. 2021b. It‚Äôs not just\nsize that matters: Small language models are also few-\nshot learners. In Proceedings of the 2021 Conference\n11854\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2339‚Äì2352.\nAmanpreet Singh, Ronghang Hu, Vedanuj Goswami,\nGuillaume Couairon, Wojciech Galuba, Marcus\nRohrbach, and Douwe Kiela. 2022. Flava: A founda-\ntional language and vision alignment model. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 15638‚Äì15650.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631‚Äì1642.\nTrevor Standley, Ozan Sener, Dawn Chen, and Silvio\nSavarese. 2017. image2mass: Estimating the mass of\nan object from its image. In Proceedings of the 1st\nAnnual Conference on Robot Learning.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2020. VL-BERT: pre-\ntraining of generic visual-linguistic representations.\nIn 8th International Conference on Learning Repre-\nsentations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020.\nHao Tan and Mohit Bansal. 2019. LXMERT: Learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5100‚Äì5111.\nHao Tan and Mohit Bansal. 2020. V okenization: Im-\nproving language understanding with contextualized,\nvisual-grounded supervision. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 2066‚Äì2080.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593‚Äì\n4601.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R. Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R. Bowman, Dipan-\njan Das, and Ellie Pavlick. 2019b. What do you\nlearn from context? probing for sentence structure\nin contextualized word representations. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nBart Thomee, David A Shamma, Gerald Friedland, Ben-\njamin Elizalde, Karl Ni, Douglas Poland, Damian\nBorth, and Li-Jia Li. 2016. Yfcc100m: The new\ndata in multimedia research. Communications of the\nACM, 59(2):64‚Äì73.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\nBaptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023a. Llama: Open and effi-\ncient foundation language models. ArXiv preprint,\nabs/2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023b. Llama 2: Open founda-\ntion and fine-tuned chat models. ArXiv preprint,\nabs/2307.09288.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998‚Äì6008.\nEric Wallace, Yizhong Wang, Sujian Li, Sameer Singh,\nand Matt Gardner. 2019. Do NLP models know num-\nbers? probing numeracy in embeddings. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 5307‚Äì5315.\nCunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan\nLi, and Tian Gao. 2019. Does it make sense? and\nwhy? a pilot study for sense making and explana-\ntion. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n4020‚Äì4026.\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yu-\nlia Tsvetkov, and Yuan Cao. 2022. Simvlm: Simple\nvisual language model pretraining with weak super-\nvision. In The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022. Emer-\ngent abilities of large language models. Transactions\non Machine Learning Research.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1112‚Äì1122.\nHeming Xia, Qingxiu Dong, Lei Li, Jingjing Xu, Ziwei\nQin, and Zhifang Sui. 2023. Imagenetvc: Zero-shot\nvisual commonsense evaluation on 1000 imagenet\ncategories. arXiv preprint arXiv:2305.15028.\n11855\nLewei Yao, Runhui Huang, Lu Hou, Guansong Lu,\nMinzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo\nLi, Xin Jiang, and Chunjing Xu. 2022. FILIP: fine-\ngrained interactive language-image pre-training. In\nThe Tenth International Conference on Learning Rep-\nresentations, ICLR 2022, Virtual Event, April 25-29,\n2022.\nTian Yun, Chen Sun, and Ellie Pavlick. 2021. Does\nvision-and-language pretraining improve lexical\ngrounding? In Findings of the Association for Com-\nputational Linguistics: EMNLP 2021 , pages 4357‚Äì\n4366.\nChenyu Zhang, Benjamin Van Durme, Zhuowan Li, and\nElias Stengel-Eskin. 2022a. Visual commonsense\nin pretrained unimodal and multimodal models. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5321‚Äì5335.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022b. Opt: Open\npre-trained transformer language models.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nProceedings of the 38th International Conference on\nMachine Learning, ICML 2021, 18-24 July 2021, Vir-\ntual Event, volume 139 of Proceedings of Machine\nLearning Research, pages 12697‚Äì12706.\nAppendix\nA Matching-based Prompting Results for\nBERT\nWe perform the identical prompting method and\ntemplates with CLIP VLMs for LMs of BERT.\nRoBERTa is discarded as there is no pooled em-\nbedding during its pre-training for representing the\nsentence. As shown in Table 9, the performance is\nstill close to a random guessing baseline. Besides,\nthe higher variances on all the tasks compared to\nVLMs of CLIP indicate that this method does not\nfit LMs of BERT well, validating our prompting de-\nsign choice in the main paper to fit the pre-training\nparadigm.\nB Model Configurations\nHere we provide the detailed configurations of eval-\nuated LMs and VLMs in our main paper. For the\nvanilla model, there model configurations are listed\nin Table 6. For the models pre-trained from scratch\nwith the YFCC-15M dataset, they are adopted the\nsame LM architecture as the VLM of CLIP-ViT/B-\n32, and the only difference is the pre-training ob-\njective, as shown in Table 7. All these models are\noptimized using an Adam optimizer with a learning\nrate set to 1e‚àí4, linearly increased at the first 2000\nsteps. The batch size is 2048 and all models are\ntrained with 32 epochs. We use 1% of the data for\nevaluation, and the final OPT-YFCC15M model\ngets a 31.9 validation perplexity.\nC Details of Prompts\nWe provide the used prompts for evaluating differ-\nent models based on their pre-training objectives.\nExamples of Head, Rel and Tail of each dataset\nare shown in Table 1. Due to the sensitivity of\nlanguage models to prompts, we provide diverse\nprompts for each model on each task.\nPrompts for Masked Language Models A\n[MASK] token is placed in the prompt and the mod-\nels are asked to predict the probabilities of the\n[MASK] token. To avoid multiple mask tokens in\nprompts, we follow Schick and Sch√ºtze to convert\nknowledge fact into a cloze-question. For exam-\nple, a temperature fact (water, colder than,\nfrying oil) can be converted into Q: is the\nwater colder than frying oil? A: [MASK]!.\nThe models need to choose the token yes or no to\nfill the mask.\nPrompts for Causal Language Models As there\nis no special [MASK] token during the pre-training\nof causal language models, we do not use [MASK]\ntokens in prompts for causal language models.\nFor Color, Shape and Material datasets of the vi-\nsual concepts we construct two prompts for (Head,\nTail1) and (Head, Tail2); while for other datasets,\nwe construct two prompts for (Head, Rel, Tail) and\n(Head, Rel‚Ä≤, Tail) where Rel‚Ä≤is the antonym rela-\ntion of Rel. The prediction is based on the prompt\nwith lower perplexity.\nPrompts for CLIP Following Radford et al.\n(2021), we use prompts like a photo of ...\nhere. As the language encoder of CLIP encodes\nsentences to a vector and can evaluate similarities\nbetween sentences. We use an attribute prompt\nlike a photo of a cold object and construct\nsame prompts for objects (water and frying oil) in\nthe knowledge fact. We can determine the colder\nobject if the prompt of this object has a higher\nsimilarity to the attribute prompt.\n11856\nModel Hidden Layers Hidden Size Attention Heads Total # of Parameters\nBERT-base 12 768 12 110M\nBERT-large 24 1,024 16 340M\nRoBERTa-base 12 768 12 125M\nRoBERTa-large 24 1,024 16 355M\nOPT-125M 12 768 12 125M\nOPT-1.3B 24 2,048 32 1.3B\nOPT-13B 40 5,120 40 13B\nOPT-175B 96 12,288 96 175B\nCLIP-ViT/B-32 12 512 8 63M\nDeCLIP-ViT/B-32 12 512 8 63M\nCLIP-ViT/L-14 12 768 12 123M\nBLIP-base 12 768 12 138M\nTable 6: Detailed configuration of models evaluated in the paper.\nModel Training Objective Training Dataset\nBERTYFCC-15M Masked Language Modeling (MLM) Captions in YFCC-15M\nGPTYFCC-15M Causal Language Modeling (MLM) Captions in YFCC-15M\nCLIPYFCC-15M Contrastive Image-text Matching (CIM) Image-Text Pair in YFCC-15M\nTable 7: Pre-training objectives and corpus comparison of YFCC-15M models evaluated in the main paper.\nModel SST-2 QQP QNLI MNLI (m / mm) Avg.\nBERT (Wiki) 90.13 83.20 87.57 78.90 / 80.05 83.97\nDistilledOscar 89.33 67.98 82.48 74.46 / 74.82 77.81\nVLM-BERT-base 90.60 90.10 89.47 81.57 / 82.43 86.83\nVLM-RoBERTa-base 90.13 88.44 87.91 80.37 / 80.43 85.46\nModel Color Shape Size Height Material Mass Temperature HardnessAvg.\nBERT (Wiki) 49.41 48.07 51.70 49.46 52.39 48.85 51.07 52.34 50.41\nDistilledOscar 49.97 53.61 49.07 49.80 51.46 51.22 47.94 51.23 50.54\nVLM-BERT-base 50.69 50.07 51.00 50.92 53.89 44.83 50.64 49.22 50.16\nVLM-RoBERTa-base49.53 51.21 49.00 49.22 49.54 49.92 51.11 49.63 49.90\nTable 8: Fine-tuned accuracy of other visual-informed pre-trained language models on NLU tasks and zero-shot\nresults regarding the physical concepts.\nModel Color Shape Size Height Material Mass Temperature HardnessAvg.\nCLIP-ViT/L-14 80.33¬±3.61 85.00¬±4.03 63.96¬±6.10 60.72¬±5.56 80.33¬±3.61 73.15¬±6.34 65.88¬±2.31 69.57¬±2.26 72.37\nBERT-base Pooled43.19¬±5.13 59.64¬±7.24 66.10¬±7.91 65.48¬±6.63 46.55¬±6.18 52.32¬±7.46 56.59¬±5.65 55.51¬±5.50 55.67\nBERT-large Pooled44.74¬±5.93 56.93¬±6.45 53.80¬±5.92 54.84¬±8.01 52.18¬±4.88 57.92¬±7.26 51.90¬±6.56 56.22¬±4.45 53.57\nTable 9: Zero-shot results of BERT models with pooled output as sentence embedding on VEC benchmark.\nD Entity Analysis\nIn our main paper, we investigate the random-level\nperformance of BERT models by exploring the cor-\nrect ratio over different prompts. We provide full\nhistogram plots of all tasks in Figure 7, 8, 9,10,\n11, 12, 13, and 14. It can be found that for visual\nconcepts tasks such as material and shape, there are\nentities that BERT could produce consistent cor-\nrect prediction across different prompts. However,\nfor all embodied tasks, the histograms exhibit bell\ncurves, indicating the poor understanding ability of\nBERT on embodied concepts.\nE Embodied Knowledge Transfer\nWe provide implementation details here for the\nknowledge transfer experiments from VLMs to\nLMs. Specifically, we take the VLM as a teacher\nmodel T (e.g., the text encoder of the CLIP model)\nand the LM as a student model S (e.g., the OPT\nlanguage model). Given a text x from the training\n11857\ndataset D, we transfer the sequential activation pat-\nterns of T(x) ‚ààR|x|√ód to S(x) ‚ààR|x|√ód , where\nT(x) and S(x) denote the last hidden representa-\ntions of the VLM and the LM, respectively. dis\nthe number of hidden units. The squared maximum\nmean discrepancy (MMD) with kernel trick (Huang\nand Wang, 2017) is adopted to measure the distance\nbetween the activation patterns:\nMMD2(x) =1\nd2\nd‚àë\ni=1\nd‚àë\ni‚Ä≤=1\nk\n[\nS(x)‚àó,i; S(x)‚àó,i‚Ä≤\n]\n+ 1\nd2\nd‚àë\nj=1\nd‚àë\nj‚Ä≤=1\nk\n[\nT(x)‚àó,j; T(x)‚àó,j‚Ä≤\n]\n‚àí 2\nd2\nd‚àë\ni=1\nd‚àë\nj=1\nk[S(x)‚àó,i; T(x)‚àó,j]\nWe adopt a polynomial kernel k(x; y) =(\nx‚ä§y + c\n)p with p = 2 and c = 0. The MMD\nobjective LMMD is minimized along with the origi-\nnal language modeling objective LLM:\nL= Llm + Œ≤LMMD\nwhere Œ≤is a weighting factor set to 20 to achieve a\nbalance between objectives.\nF Evaluation and Distillation with Oscar\nWe examine whether other vanilla distillation from\ntraditional V+L pre-training methods brings gains\nregarding visual and embodied knowledge. Specifi-\ncally, following Zhang et al. (2022a), we distill the\nknowledge of Oscar (Li et al., 2020) into a BERT\nmodel by performing knowledge distillation (Hin-\nton et al., 2015) on the image-caption pair dataset.\nSpecifically, the paired text and image are fed into\nthe Oscar model for getting the vision-aware vo-\ncabulary distribution, and a student BERT model\nis performing masked language modeling on the\ntext data only and learns from the soft labels pro-\nvided by the Oscar teacher model. The distillation\nresults in a DistilledOscar model supporting text-\nonly inputs. We also evaluate VLM-BERT learned\nvia V okenziation (Tan and Bansal, 2020), which\ndevises a fine-grained token-voken matching frame-\nwork to utilize visual supervision. The models are\nevaluated on the four largest datasets in GLUE,\nincluding SST-2 (Socher et al., 2013), QQP (Iyer\net al., 2017), QNLI (Rajpurkar et al., 2016) and\nMNLI (Williams et al., 2018) for stable results. As\nshown in Table 8, DistilledOscar performs worse\n/uni000003ec/uni00000358/uni000003f0/uni000003f2/uni000003ec/uni00000358/uni000003f0/uni000003f4/uni000003ec/uni00000358/uni000003f1/uni000003ec/uni000003ec/uni00000358/uni000003f1/uni000003ee/uni000003ec/uni00000358/uni000003f1/uni000003f0\n/uni00000012/uni0000017d/uni0000018c/uni0000018c/uni0000011e/uni00000110/uni0000019a/uni00000003/uni0000005a/uni00000102/uni0000019a/uni0000015d/uni0000017d\n/uni000003ec\n/uni000003ed\n/uni000003ee\n/uni000003ef\n/uni000003f0\n/uni000003f1/uni00000012/uni0000017d/uni000001b5/uni00000176/uni0000019a\n/uni0000005e/uni0000015d/uni000001cc/uni0000011e\nFigure 7: Histogram of entity correct ratio across differ-\nent prompts on the Size dataset.\n/uni000003ec/uni00000358/uni000003ec/uni000003ec/uni00000358/uni000003ee/uni000003ec/uni00000358/uni000003f0/uni000003ec/uni00000358/uni000003f2/uni000003ec/uni00000358/uni000003f4/uni000003ed/uni00000358/uni000003ec\n/uni00000012/uni0000017d/uni0000018c/uni0000018c/uni0000011e/uni00000110/uni0000019a/uni00000003/uni0000005a/uni00000102/uni0000019a/uni0000015d/uni0000017d\n/uni000003ec\n/uni000003ed/uni000003ec\n/uni000003ee/uni000003ec\n/uni000003ef/uni000003ec\n/uni000003f0/uni000003ec\n/uni000003f1/uni000003ec/uni00000012/uni0000017d/uni000001b5/uni00000176/uni0000019a\n/uni0000005e/uni0000015a/uni00000102/uni00000189/uni0000011e\nFigure 8: Histogram of entity correct ratio across differ-\nent prompts on the Shape dataset.\n/uni000003ec/uni00000358/uni000003ec/uni000003ec/uni00000358/uni000003ee/uni000003ec/uni00000358/uni000003f0/uni000003ec/uni00000358/uni000003f2/uni000003ec/uni00000358/uni000003f4/uni000003ed/uni00000358/uni000003ec\n/uni00000012/uni0000017d/uni0000018c/uni0000018c/uni0000011e/uni00000110/uni0000019a/uni00000003/uni0000005a/uni00000102/uni0000019a/uni0000015d/uni0000017d\n/uni000003ec\n/uni000003ee/uni000003ec\n/uni000003f0/uni000003ec\n/uni000003f2/uni000003ec\n/uni000003f4/uni000003ec\n/uni000003ed/uni000003ec/uni000003ec/uni00000012/uni0000017d/uni000001b5/uni00000176/uni0000019a\n/uni00000012/uni0000017d/uni0000016f/uni0000017d/uni0000018c\nFigure 9: Histogram of entity correct ratio across differ-\nent prompts on the Color dataset.\nthan the vanilla BERT in both NLU tasks and prob-\ning tasks regarding visual and embodied knowl-\nedge. Besides, while VLM-BERT achieves im-\nprovements on NLU tasks, it still performs at the\nrandom level on the probed tasks. These indicate\nthat not all VLMs could learn embodied knowledge\nand it is non-trivial to distill the visual supervision\nfrom VLMs to LMs via purely language modeling.\n11858\nTable 10: Prompts for Masked Language Models\nModel Task Prompt\nBERT & RoBERTa\nSize, Height, Temperature, Weight, Hardness\nis the [Head] [Rel] than the [Tail]? [MASK]!\nis the [Head] [Rel] than the [Tail]? [MASK].\nis [Head] [Rel] than [Tail]? [MASK]!\nis [Head] [Rel] than [Tail]? [MASK].\nis [Head] [Rel] compared with [Tail]? [MASK].\nis [Head] [Rel] compared with [Tail]? [MASK]!\ncompared with [Tail], is [Head] [Rel]? [MASK].\ncompared with [Tail], is [Head] [Rel]? [MASK]!\nis [Head] usually [Rel] than [Tail]? [MASK].\nis [Head] usually [Rel] than [Tail]? [MASK]!\nColor\ncan [Head] be of color [Tail]? [MASK]!\ncan [Head] be of color [Tail]? [MASK].\nis the color of a [Head] [Tail]? [MASK]!\nis the color of a [Head] [Tail]? [MASK].\nis [Head] [Tail]? [MASK].\nis [Head] [Tail]? [MASK]!\nis [Head] typically in [Tail]? [MASK].\nis [Head] typically in [Tail]? [MASK]!\nQ: is [Head] of color [Tail]? A: [MASK].\nQuestion: is [Head] of color [Tail]? Answer: [MASK].\nShape\ncan [Head] be the shape of [Tail]? [MASK].\ncan [Head] be the shape of [Tail]? [MASK]!\ndoes the [Head] have a shape of [Tail]? [MASK].\ndoes the [Head] have a shape of [Tail]? [MASK]!\nis [Head] of [Tail]? [MASK].\nis [Head] of [Tail]? [MASK]!\nQ: is [Head] of [Tail]? A: [MASK].\nQuestion: is [Head] of [Tail]? Answer: [MASK].\n[Tail] [Head]? [MASK].\nis [Head] typically [Tail]? [MASK].\nMaterial\ncan [Head] be made of [Tail]? [MASK]!\ncan [Head] be made of [Tail]? [MASK].\nis [Head] made of [Tail]? [MASK]!\nis [Head] made of [Tail]? [MASK].\nis [Tail] the necessary material for making [Head]? [MASK].\nis [Tail] the necessary material for making [Head]? [MASK]!\ndoes [Head] consist of [Tail]? [MASK].\nis [Head] made up of [Tail]? [MASK].\nQ: is [Head] made of [Tail]? A: [MASK].\nQuestion: is [Head] made of [Tail]? Answer: [MASK].\n/uni000003ec/uni00000358/uni000003f0/uni000003f0/uni000003ec/uni00000358/uni000003f0/uni000003f2/uni000003ec/uni00000358/uni000003f0/uni000003f4/uni000003ec/uni00000358/uni000003f1/uni000003ec/uni000003ec/uni00000358/uni000003f1/uni000003ee/uni000003ec/uni00000358/uni000003f1/uni000003f0\n/uni00000012/uni0000017d/uni0000018c/uni0000018c/uni0000011e/uni00000110/uni0000019a/uni00000003/uni0000005a/uni00000102/uni0000019a/uni0000015d/uni0000017d\n/uni000003ec\n/uni000003ee\n/uni000003f0\n/uni000003f2\n/uni000003f4\n/uni000003ed/uni000003ec/uni00000012/uni0000017d/uni000001b5/uni00000176/uni0000019a\n/uni0000002c/uni0000011e/uni0000015d/uni00000150/uni0000015a/uni0000019a\nFigure 10: Histogram of entity correct ratio across dif-\nferent prompts on the Height dataset.\n/uni000003ec/uni00000358/uni000003ec/uni000003ec/uni00000358/uni000003ee/uni000003ec/uni00000358/uni000003f0/uni000003ec/uni00000358/uni000003f2/uni000003ec/uni00000358/uni000003f4/uni000003ed/uni00000358/uni000003ec\n/uni00000012/uni0000017d/uni0000018c/uni0000018c/uni0000011e/uni00000110/uni0000019a/uni00000003/uni0000005a/uni00000102/uni0000019a/uni0000015d/uni0000017d\n/uni000003ec\n/uni000003ee/uni000003ec\n/uni000003f0/uni000003ec\n/uni000003f2/uni000003ec\n/uni000003f4/uni000003ec\n/uni000003ed/uni000003ec/uni000003ec\n/uni000003ed/uni000003ee/uni000003ec\n/uni000003ed/uni000003f0/uni000003ec/uni00000012/uni0000017d/uni000001b5/uni00000176/uni0000019a\n/uni00000044/uni00000102/uni0000019a/uni0000011e/uni0000018c/uni0000015d/uni00000102/uni0000016f\nFigure 11: Histogram of entity correct ratio across dif-\nferent prompts on the Material dataset.\n11859\nTable 11: Prompts for Causal Language Models\nModel Task Prompt\nOPT\nSize, Height, Temperature, Weight, Hardness\nthe [Head] is [Rel] than the [Tail].\n[Head] is [Rel] than [Tail].\nacutally, the [Head] is [Rel] than the [Tail].\nacutally, [Head] is [Rel] than [Tail].\nit is well-known that [Head] is [Rel] than [Tail].\n[Head] is indeed [Rel] than [Tail].\nthe [Head] is indeed [Rel] than [Tail].\ncompared with the [Head], the [Tail] is [Rel].\na/(an) [Head] is [Rel] than a/(an) [Tail].\nyes, [Head] is [Rel] than [Tail].\nColor\n[Head] can be of the color [Tail].\nthe [Head] can be of color [Tail].\nthe color of a(an) [Head] is [Tail].\nthe color of [Head] is [Tail].\nthe [Head] is in [Tail].\n[Head] is [Tail].\nwhat color is the [Head]? [Tail].\n[Head]‚Äôs color is [Tail].\nusually, [Head] is in [Tail].\n[Head] is typically [Tail].\nShape\n[Head] is usually [Tail].\nwhat is the shape of [Head]? [Tail].\n[Head] is typically [Tail].\n[Head]‚Äôs shape is [Tail].\nMaterial\n[Head] is made of [Tail].\nthe [Head] is made of [Tail].\n[Head] consists of [Tail].\nthe main material of [Head] is [Tail].\n[Tail] is necessary material for making [Head].\nthe [Head] consists of [Tail].\nthe [Head] can be made of [Tail].\nthe [Head] is built with [Tail].\nthe [Head] contains [Tail].\nthe [Head] is made up of [Tail].\nTable 12: Prompts used for VLMs of CLIP.\nModel Task Prompt\nCLIP All Tasks\na photo of a [Head]/[Attribute].\na photo of the [Head]/[Attribute].\na blurry photo of a [Head]/[Attribute].\na good photo of a [Head]/[Attribute].\na painting of a [Head]/[Attribute].\na bad photo of a [Head]/[Attribute].\na close-up photo of a [Head]/[Attribute].\na bright photo of the [Head]/[Attribute].\na photo of one [Head]/[Attribute].\na low resolution photo of a [Head]/[Attribute].\n/uni000003ec/uni00000358/uni000003f0/uni000003ec/uni000003ec/uni00000358/uni000003f0/uni000003f1/uni000003ec/uni00000358/uni000003f1/uni000003ec/uni000003ec/uni00000358/uni000003f1/uni000003f1/uni000003ec/uni00000358/uni000003f2/uni000003ec\n/uni00000012/uni0000017d/uni0000018c/uni0000018c/uni0000011e/uni00000110/uni0000019a/uni00000003/uni0000005a/uni00000102/uni0000019a/uni0000015d/uni0000017d\n/uni000003ec\n/uni000003ee\n/uni000003f0\n/uni000003f2\n/uni000003f4/uni00000012/uni0000017d/uni000001b5/uni00000176/uni0000019a\n/uni00000064/uni0000011e/uni00000175/uni00000189/uni0000011e/uni0000018c/uni00000102/uni0000019a/uni000001b5/uni0000018c/uni0000011e\nFigure 12: Histogram of entity correct ratio across dif-\nferent prompts on the Temperature dataset.\n/uni000003ec/uni00000358/uni000003f0/uni000003ee/uni000003f1/uni000003ec/uni00000358/uni000003f0/uni000003f1/uni000003ec/uni000003ec/uni00000358/uni000003f0/uni000003f3/uni000003f1/uni000003ec/uni00000358/uni000003f1/uni000003ec/uni000003ec/uni000003ec/uni00000358/uni000003f1/uni000003ee/uni000003f1/uni000003ec/uni00000358/uni000003f1/uni000003f1/uni000003ec/uni000003ec/uni00000358/uni000003f1/uni000003f3/uni000003f1\n/uni00000012/uni0000017d/uni0000018c/uni0000018c/uni0000011e/uni00000110/uni0000019a/uni00000003/uni0000005a/uni00000102/uni0000019a/uni0000015d/uni0000017d\n/uni000003ec\n/uni000003ee\n/uni000003f0\n/uni000003f2\n/uni000003f4\n/uni000003ed/uni000003ec/uni00000012/uni0000017d/uni000001b5/uni00000176/uni0000019a\n/uni0000002c/uni00000102/uni0000018c/uni0000011a/uni00000176/uni0000011e/uni00000190/uni00000190\nFigure 13: Histogram of entity correct ratio across dif-\nferent prompts on the Hardness dataset.\n11860\n/uni000003ec/uni00000358/uni000003f0/uni000003ec/uni000003ec/uni00000358/uni000003f0/uni000003f1/uni000003ec/uni00000358/uni000003f1/uni000003ec/uni000003ec/uni00000358/uni000003f1/uni000003f1/uni000003ec/uni00000358/uni000003f2/uni000003ec/uni000003ec/uni00000358/uni000003f2/uni000003f1/uni000003ec/uni00000358/uni000003f3/uni000003ec\n/uni00000012/uni0000017d/uni0000018c/uni0000018c/uni0000011e/uni00000110/uni0000019a/uni00000003/uni0000005a/uni00000102/uni0000019a/uni0000015d/uni0000017d\n/uni000003ec/uni00000358/uni000003ec\n/uni000003ee/uni00000358/uni000003f1\n/uni000003f1/uni00000358/uni000003ec\n/uni000003f3/uni00000358/uni000003f1\n/uni000003ed/uni000003ec/uni00000358/uni000003ec\n/uni000003ed/uni000003ee/uni00000358/uni000003f1\n/uni000003ed/uni000003f1/uni00000358/uni000003ec\n/uni000003ed/uni000003f3/uni00000358/uni000003f1/uni00000012/uni0000017d/uni000001b5/uni00000176/uni0000019a\n/uni00000044/uni00000102/uni00000190/uni00000190\nFigure 14: Histogram of entity correct ratio across dif-\nferent prompts on the Mass dataset.\n11861",
  "topic": "Embodied cognition",
  "concepts": [
    {
      "name": "Embodied cognition",
      "score": 0.9093813896179199
    },
    {
      "name": "Computer science",
      "score": 0.7836267948150635
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.5821692943572998
    },
    {
      "name": "Representation (politics)",
      "score": 0.5209152102470398
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4840683043003082
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4586835205554962
    },
    {
      "name": "Human‚Äìcomputer interaction",
      "score": 0.4554266035556793
    },
    {
      "name": "Scaling",
      "score": 0.43328139185905457
    },
    {
      "name": "Programming language",
      "score": 0.18126177787780762
    },
    {
      "name": "Mathematics",
      "score": 0.10119888186454773
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ]
}