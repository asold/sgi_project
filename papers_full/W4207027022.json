{
  "title": "Transformer-Based Approaches for Legal Text Processing",
  "url": "https://openalex.org/W4207027022",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5012147485",
      "name": "Ha-Thanh Nguyen",
      "affiliations": [
        "Japan Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5101813009",
      "name": "Phuong Nguyen",
      "affiliations": [
        "Japan Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5081773521",
      "name": "Thi-Hai-Yen Vuong",
      "affiliations": [
        "VNU University of Science"
      ]
    },
    {
      "id": "https://openalex.org/A5017382143",
      "name": "Minh-Quan Bui",
      "affiliations": [
        "Japan Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5101409204",
      "name": "Minh‐Châu Nguyễn",
      "affiliations": [
        "Japan Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5033962153",
      "name": "Tran-Binh Dang",
      "affiliations": [
        "Japan Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5018668377",
      "name": "Vu Tran",
      "affiliations": [
        "The Institute of Statistical Mathematics"
      ]
    },
    {
      "id": "https://openalex.org/A5077641909",
      "name": "Le-Minh Nguyen",
      "affiliations": [
        "Japan Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5113451385",
      "name": "Ken Satoh",
      "affiliations": [
        "National Institute of Informatics"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W3190892098",
    "https://openalex.org/W31369764",
    "https://openalex.org/W1994559819",
    "https://openalex.org/W1985697096",
    "https://openalex.org/W1978394996",
    "https://openalex.org/W2803462738",
    "https://openalex.org/W3117737817",
    "https://openalex.org/W2963240788",
    "https://openalex.org/W2963105309",
    "https://openalex.org/W6786353593",
    "https://openalex.org/W2947652146",
    "https://openalex.org/W3175636666",
    "https://openalex.org/W2980272922",
    "https://openalex.org/W2106401878",
    "https://openalex.org/W3175032570",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4254557068",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2964259149",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3102561203",
    "https://openalex.org/W4288345926",
    "https://openalex.org/W3106268368",
    "https://openalex.org/W4241366218",
    "https://openalex.org/W2996428491"
  ],
  "abstract": null,
  "full_text": "TRANSFORMER -BASED APPROACHES\nFOR LEGAL TEXT PROCESSING ∗\nHa-Thanh Nguyen, Phuong Minh Nguyen, Quan Minh Bui\nJapan Advanced Institute of Science and Technology\nChau Minh Nguyen, Binh Tran Dang, Minh Le Nguyen\nJapan Advanced Institute of Science and Technology\nVu Tran\nInstitute of Statistical Mathematics\nThi-Hai-Yen Vuong\nUniversity of Engineering and Technology, VNU\nKen Satoh\nNational Institute of Informatics\nABSTRACT\nIn this paper, we introduce our approaches using Transformer-based models for different problems\nof the COLIEE 2021 automatic legal text processing competition. Automated processing of legal\ndocuments is a challenging task because of the characteristics of legal documents as well as the\nlimitation of the amount of data. With our detailed experiments, we found that Transformer-based\npretrained language models can perform well with automated legal text processing problems with\nappropriate approaches. We describe in detail the processing steps for each task such as problem\nformulation, data processing and augmentation, pretraining, ﬁnetuning. In addition, we introduce to\nthe community two pretrained models that take advantage of parallel translations in legal domain,\nNFSP and NMSP. In which, NFSP achieves the state-of-the-art result in Task 5 of the competition.\nAlthough the paper focuses on technical reporting, the novelty of its approaches can also be an useful\nreference in automated legal document processing using Transformer-based models.\nKeywords Transformer ·Legal Text Processing ·COLIEE ·JNLP\n1 Introduction\nCase law and statute law are the two largest sources of law in the world, based on these two sources of law, social\nrelations are adjusted in accordance with international practices and national laws. For case law, the cases that are heard\nﬁrst will be used as the basis for handling the following cases. In statute law, legal documents are the main basis for\ncourt decisions. The competition on building automated legal text processing systems is a challenging and inspiring\none. In COLIEE 2021, there are three kinds of tasks in automated models include retrieval, entailment, and question\nanswering. Taking advantage of Transformer-based models, we achieve competitive performance in this competition. In\nthis paper, we report in detail our approaches and analysis the results in using the Transformer-based models in dealing\nwith legal text processing tasks.\nIn order to serve the readability of the article, we provide the information about 5 tasks of COLIEE 2021. Task 1 is\na case law retrieval problem. With a given case law, the model needs to extract the cases that support it. This is an\nimportant problem in practice. It is actually used in the attorney’s litigation as well as the court’s decision-making. Task\n2 also uses caselaw data, though, the models need to ﬁnd the paragraphs in the existing cases that entail the decision\nof a given case. Task 3, 4, 5 uses statute law data with challenges of retrieval, entailment, and question answering,\nrespectively. From a design perspective, these 5 tasks cover the main tasks in automatic legal document processing.\n∗Citation: Ha-Thanh Nguyen et al. Transformer-based Approaches for Legal Text Processing. Pages: 1-21 DOI:\n10.1007/s12626-022-00102-2\narXiv:2202.06397v1  [cs.CL]  13 Feb 2022\nHa-Thanh Nguyen et al.\nFigure 1: Overall view point of our Transformer-base approaches.\nOne of the main challenges of legal domains in general and COLIEE 2021, in particular, is the scarcity and difﬁculty of\ndata. For the traditional deep learning approach, the amount of data provided by the organizer is difﬁcult for constructing\neffective models. For that reason, we use pretrained models from problems that have much more data and then ﬁnetune\nthem for the current task. In Tasks 1, 2, 3, and 4 we have used lexical score and semantic score to ﬁlter a correct\ncandidate. In our experiments, the ratio is not 50:50 for lexical score and semantic score, we ﬁnd out that the increase in\nthe rate of lexical score leads to efﬁciency in ranking candidates. In task 5, the system relies on a pretrained model with\nmultilingual data to make predictions. We have achieved state-of-the-art performance in the blind test set provided by\nthe organizer for this task.\nThe challenge we always face in real-world competitions is that the amount of labelled data is often quite modest. One\nof the features of our approaches is data engineering. Based on labelled standard data ( i.e., gold data), we generate\naugmented samples (i.e., silver data) in massive quantities, at the expense of the existence of noise points in the data.\nBased on experiments and observations in model construction, we choose the most suitable conﬁguration for each\nspeciﬁc task.\nTransformer [1] architecture and its variants like BERT [2], ALBERT [3], ELECTRA [4], BART [5], GPTs [6, 7, 8]\nhave made many breakthroughs in the ﬁeld of natural language processing. Through problem analysis, we propose\nsolutions using deep learning with Transformer-based models.\nFigure 1 demonstrates our Transformer-based approaches for each COLIEE task. With the multi-headed self-attention\nmechanism and the huge pretraining data amount, these models can deal with the data scarcity. Dealing with Task 1 and\nTask 2, we introduce an approach namely supporting detection. Solving Task 3 and Task 4, we propose the approach of\nrelevance classiﬁcation with other supplement techniques. In Task 5, the idea of using multilingual resources introduced\nin NMSP and NFSP and other techniques in data enrichment is also the novelty of this paper. Our proposals of deep\nlearning methods can be a useful reference for researchers and engineers in automated legal document processing.\n2 Related Works\n2.1 Legal Text Processing\nProcessing legal documents is always challenging and there are many different approaches to different problems in this\nﬁeld. Legal data consists of long sentences, contains many specialized terms, contains complex logical meanings, and\noften requires a high level of understanding. Therefore, legal text processing is a difﬁcult class of problems in NLP to\nwhich for all approaches are difﬁcult to provide a complete solution.\nThe ﬁrst approach is to treat legal documents as collections of logical constraints [ 9]. Logical relationships are an\nimportant feature in the semantics of legal documents. Any regulation of social relations by law must be based on\nlogic. Hence, here is a sound approach. From the application aspect, PROLEG [10] is one of the famous systems that\nimplement logical inference in the legal domain. This system can perform logical inferences to make legal decisions\nbased on the provided rules and facts.\nThe second approach is to use morphological features in processing legal documents. Legal documents often have\nspecialized terms. These words contain large amounts of information when they are used. In addition, these terms\noften convey clear semantics in their expressions. So systems (especially retrieval systems) based on lexical features\ncan achieve acceptable results in the legal domain. These systems use fairly classical NLP techniques and theories\n[11, 12, 13]. At the expense of simplicity, these systems often have problems with tasks that require natural language\nunderstanding.\n2\nHa-Thanh Nguyen et al.\nFigure 2: An example of supporting relationship among legal contents.\nA third approach is to use machine learning to statistically compute features from the data and make predictions. In this\napproach, artiﬁcial neural networks [14, 15] and especially transformer-based models [2, 16, 5], are techniques that\nhave made many achievements in recent years. These methods, although powerful, have the disadvantage of requiring a\nlarge amount of training data and comprehension of the data characteristics in the downstream tasks. This has also been\na strong approach at COLIEE in recent years.\n2.2 Case Law Processing at COLIEE\nIn COLIEE 2018, most teams chose the lexical-based approaches. UBIRLED ranked the candidate cases based on\ntf-idf. They got approximately 25% of the candidate cases with the highest scores. UA and several teams also chose the\nsame approach with UBIRLED. They compared lexical features between a given base case and corresponding candidate\ncases. JNLP team combined lexical matching and deep learning, which achieved state-of-the-art performance on Task 1\nwith the F1 score of 0.6545.\nIn COLIEE 2019, several teams applied machine learning including deep learning to both tasks. JNLP team achieved\nthe best result of Task 1 in COLIEE 2019 [17] using an approach similar to theirs in COLIEE 2018. In Task 2, their\ndeep learning approach achieved lower performance compared to their lexical approach. Team UA’s combination of\nlexical similarity and BERT model achieved the best performance for Task 2 in COLIEE 2019 [18].\nIn COLIEE 2020, the Transformer model and its modiﬁed versions were widely used. TLIR and JNLP [19] teams used\nthem to classify candidate cases with two labels (support/non-support) in Task 1. Team cyber encoded candidate cases\nand the base case in tf-idf space and used SVM to classify. They demonstrated the ability of the approach with the ﬁrst\nrank in Task 1. In Task 2, JNLP [19] continually applied the same approach in Task 1 in the weakly-labeled dataset. It\nmade them surpass team cyber and won Task 2.\nIn COLIEE 2021, the teams applied the combination of linguistic information, machine learning methods and pretrain\nmodels to solve the problem. The UA team used BERT and Naive Bayes classiﬁer. The NM team combined sequence-\nto-sequence models such as monoT5-zero-shot, monoT5 and DeBERTa. JNLP team continues to use supporting models\nto generate weak labels for training. Other teams used BM25 and BERT to choose correct cases.\n2.3 Statute Law Processing at COLIEE\nThe retrieval task (Task 3) is often considered as a ranking problem with similarity features. In COLIEE 2019, most of\nthe teams used lexical methods for calculating the relevant scores. JNLP [20], DBSE [21] and IITP [22] chose tf-idf and\nBM25 to build their models. JNLP used tf-idf of noun phrase and verb phrase as keywords which show the meaning of\nstatements in the cosine-similarity equation. DBSE applied BM25 and Word2vec to encode statements and articles.\nThe document embedding was used to calculate and rank the similarity score. Team KIS [23] represented the article\nand query as a vector by generating a document embedding. Keywords were selected by tf-idf and assigned with high\nweights in the embedding process. In COLIEE 2020, with the popularity of Transformer based methods, the participants\nchange their approaches. The task winner, LLNTU, only used BERT model to classify articles as relevant or not.\nRegarding the entailment task (Task 4), approaches using deep learning have attracted more attention. In COLIEE 2019,\nKIS [24] used predicate-argument structure to evaluate similarity. IITP [22] and TR [25] applied BERT for this task.\nJNLP [26] classiﬁed each query to follow binary classiﬁcation based on big data. In COLIEE 2020, BERT and multiple\n3\nHa-Thanh Nguyen et al.\nFigure 3: Demonstration of mixing lexical and semantic score.\nmodiﬁed versions of BERT were used. JNLP [19] chose a pretrained BERT model on a large legal corpus to predict the\ncorrectness of statements.\nIn COLIEE 2021, the teams continued using BERT as main model of their approaches. The HUKB used BERT and data\naugmentation. JNLP team applied bert-base-japanese-whole-word masking and ensemble models (original multilingual\nBERT, Next Foreign Sentence Prediction pretrain model and Neighbor Multilingual Sentence Prediction pretrain model).\nThe UA team built an information retrieval system based on methods such as BM25 (UA.BM25), TF-IDF(UA.tﬁdf) and\nlanguage model (UA.LM).\n3 Method\n3.1 Task 1 and Task 2. Case Law Processing\nIn COLIEE 2021, there is a signiﬁcant change in the data structure in Task 1, which brings more challenges for all\nparticipated teams. This can be seen in the ofﬁcial results of the organizers, which we will quote later in this Section.\nThe performance of participants’ models drops badly from nearly 70% to 20% or worse The number of candidate legal\ncases for each query increases from 200 in last year’s competition to 4,415 in this year’s competition. With a naive\nestimate, the difﬁculty of this year’s problem is at least 22 times higher than last year’s problem. The larger the search\nspace, the lower the probability that the model correctly predicts a relevant case.\nFigure 3 demonstrates the ﬂow of our approach. 4,415 candidates are too large for a deep learning model to work\neffectively and efﬁciently. Therefore, we use BM25 to ﬁlter out 100 cases that lexically match the query before\nprocessing with the deep learning model. In both Task 1 and Task 2, we evaluate candidates based on the Union Score,\na metric that combines the lexical score and the semantic score. Through experiments, we conﬁrm that this approach is\nmore efﬁcient than using only lexical or semantic features.\nThe previous approaches focus on encoding the whole case law and calculating the similarity of vector representation\nfor each query-candidate pair of cases. In our method, we handle the similarity between candidates on the paragraph\nlevel. “Supporting” and “Relevant” are subtractive deﬁnitions, and we assume that for each paragraph in the query case,\nthere are one or more paragraphs that carry useful information for the given query’s paragraph. As we can see in the\nFigure 2, paragraph [10] has “PRRA ofﬁcer’s” and in the candidate case number [15] also has “PRRA ofﬁcer’s”, there\nseems to be a lexical relationship here. This is evidence that the lexical model could be effective. Along the line of\nthe query paragraph number [14] has “the evidence” corresponding to reasonable grounds in the candidate paragraph\nnumber [31], we can see the semantic relationship between two paragraphs through this example. For discovering the\nrelevant paragraph, we combine the lexical similarity and the semantic similarity score.\nLexical Matching For the purpose of capturing lexical information, we use Rank-BM25 2, a collection of algorithms\nfor querying a set of documents and returning the ones most relevant to the query. For Task 1, after reducing the\nsearching space to 100 queries, we separate the query case and also candidate cases into paragraphs. Assume that the\nquery case has N paragraphs, and the candidate case has M paragraphs, we will calculate the lexical mapping score by\nRank-BM25 for each query paragraph and every single paragraph in the candidate case. Then we obtain the matrix\n2https://pypi.org/project/rank-bm25/\n4\nHa-Thanh Nguyen et al.\nlexical score size N ×M for each query-candidate pair. We keep these matrixes and their union with supporting score\nwhich is introduced later.\nSupporting Matching As we mentioned in the Figure 2, we extract the semantic relationship between query paragraph\nand candidate paragraph. To obtain the relationship score, we use pretrained model BERT provided by huggingface3.\nThe same approach as lexical matching, we sequentially split a query and corresponding candidate cases into paragraphs,\nand obtain a matrix semantic score that has the same size as matrix lexical score. Although BERT is a reputable\npretrained model that achieves state-of-the-art results on many NLP tasks, it still has limitations when applied to\nCOLIEE data. In particular, the law is speciﬁc data while BERT is trained on the general domain data. For that reason,\nwe develop a silver dataset based on the COLIEE dataset for ﬁnetuning BERT and use this for predicting the semantic\nrelationship between paragraphs.\nUnion Score To obtain the most relevant cases for a given query, we ﬁnd a semantic as well as lexical relationship\noverlapped between the two paragraphs. For the exact purpose of developing union score we use the following formula:\nunion_score= α∗scoresupporting + (1−α) ∗scoreBM25 (1)\nIn Task 2, we use the same method as Task 1 with a few improvements. We consider Task 2 as a binary classiﬁcation\nproblem with the training data as a set of sentence pairs. With this approach, we can obtain more gold training data to\ntrain the supporting model. For optimizing the performance of the supporting model on this problem, after ﬁnetuning\non the silver data, we ﬁnetune one more time on the gold data of Task 2.\n3.2 Task 3. The Statute Law Retrieval Task\nThis task requires participants to determine whether a set of articles entail a legal bar exam question: Entails(Ai|1≤i≤n,\nQ) or Entails(Ai|1≤i≤n, not Q), where Qis a legal bar exam question, and Ai|1≤i≤n is a subset of articles in Japanese\nCivil Code. This task faces two main challenges. The ﬁrst challenge is to answer questions about a speciﬁc legal case.\nIt is notable that while articles in statute law are described in an abstract manner, speciﬁc legal case reports a speciﬁc\nevent. This difference requires a model to have reasoning ability. The second challenge is to deal with the long articles.\nIn this section, we focus on tackling the second challenge by employing two techniques: (i) text chunking technique on\nprepared training data, and (ii) self-labeled technique in the model ﬁnetuning phase.\nTraining data preparation. In the training data preparation phase, we employ the method proposed in [19] with a\nmodiﬁcation. Firstly, we obtain positive training examples by pairing a question with its annotated entailing articles.\nThe negative training examples are the pair of a question with any other articles. Because of the characteristics of the\nevaluation dataset, the negative training examples are dominant. We then remove negative training examples based on\nthe tf-idf scores so that the number of negative examples corresponding to a question is 150 at maximum. By using the\ntop 150 Articles most related to the question via Tf-IDF score, the recall score can archieve to 91.25%. The details can\nbe found in our previous paper [19].\nText chunking technique. In this task, we leverage the language understanding ability of Transformer-based pre-\ntrained language models. Because of the 512-token limitation in the design of those models, they have to truncate a\nlong article if it contains more than 512 tokens. It leads to the lack of article information when we process the article.\nFurthermore, in fact, for a question, in most cases, there are only a few parts of articles that entail the question (Table 1).\nTo overcome the 512-token limitation, we utilize a sliding window to obtain one or multiple chunks from an article. We\nexperimented with many values of sliding window and stride to get the most appropriate values. Now, we consider\n(question, chunk) as training examples instead of the original ( question, article), where the label for ( question,\nchunk) is derived from the corresponding (question, article).\nSelf-labeled technique. We observe that there are many noises in the generated training data following the above\nprocess. For example, suppose that sub-articles Ai,1,Ai,2,...,A i,j is obtained from a long article Ai which entails a\nquestion Q, and only Ai,j (the sub-article jth of article Ai) entails Q. In this case, our training data generation process\nwill label all pairs (Q,Ai,1), (Q,Ai,2), ..., (Q,Ai,j) as positive training examples. However, only pair (Q,Ai,j) should\nbe labeled positive, while other pairs, i.e. (Q,Ai,1), (Q,Ai,2), ..., (Q,Ai,j−1), should be labeled as negative examples.\nInspired by the self-labeled techniques [27], we propose to mitigate the number of incorrect generated positive training\nexamples by employing a simple self-labeled technique. Let X = {x1,x2,...,x k}be generated input training samples\nthat are pairs of Qand sub-articles where kis the number of generating training samples, y0 be the initial labels of\nthese samples drawn from the annotated label for pair of Qand corresponding article, Mdenotes the model and also its\nforward function, the self-labeld technique is described in Algorithm 1.\n3https://huggingface.co/models\n5\nHa-Thanh Nguyen et al.\nAlgorithm 1: Self-labeled technique to mitigate incorrect generated positive training examples.\nInput :Pairs of (Question, sub-Article) X, initial labels y0\n1 function (X, y0)\n2 y = y0;\n3 M← ﬁne-tuning(X,y); // ﬁrst ﬁne-tuning to remove noise samples\n4 ˆ y= M(X);\n5 yi = ˆ yi∀i∈[1,k] :yi = True ∧ˆ yi ̸= yi; // modifying the label\n6 M← ﬁne-tuning(X,y);\n7 end\nIn detail, ﬁrstly, the generated training examples are used to ﬁnetune a pretrained language model. After that, the\nﬁnetuned model predicts labels for the generated training examples (where it is ﬁnetuned for). Next, labels of the\ngenerated training data is modiﬁed to mitigate noise samples. The modiﬁed labels then are utilized in the next self-\nlabeled process. For the rule of modifying the label, base on our observation, we only allow the positive label to turn\nto a negative label, but not the reverse direction. Our experiments demonstrate the decrease in the incorrect positive\ntraining examples after employing the self-labeled technique.\nModel ensembling. Since each ﬁnetuned language model has its advantages and disadvantages for different types\nof legal bar exam questions, we ensemble the prediction scores of the models with learned weights. It means that the\nprediction of a model may have a larger weight than the others on contributing to the ﬁnal predictions.\nQ. R01-4-E In cases any party who will suffer any detriment as a result of the fulﬁllment of a condition\nintentionally prevents the fulﬁllment of such condition, the counterparty may deem that such\ncondition has been fulﬁlled.\nArticle 130 Part I General Provisions Chapter V Juridical Acts Section 5 Conditions and Time Limits\n(Prevention of Fulﬁllment of Conditions)\n(1) If a party that would suffer a detriment as a result of the fulﬁllment of a condition\nintentionally prevents the fulﬁllment of that condition, the counterparty may deem that\nthe condition has been fulﬁlled.\n(2) If a party who would enjoy a beneﬁt as a result of the fulﬁllment of a condition wrongfully\nhas that condition fulﬁlled, the counterparty may deem that the condition has not been fulﬁlled.\nTable 1: An example in which only one part of the entailing article entails the corresponding question.\n3.3 Task 4. The Legal Textual Entailment Task\nThis task involves the identiﬁcation of an entailment relationship between relevant articles Ai|1≤i≤n (which is derived\nfrom Task 3’s results) and a question Q. The models are required to determine whether the relevant articles entail\n“Q” or “notQ”. Given a pair of legal bar exam questions and article (Q,Ai), the models return a binary value for\ndetermining whether (Ai) entails (Q). To address this task, we modiﬁed the training data preparation step, then use the\nsame model architecture in Task 3 (Section 3.2) for training.\nData preparation. Based on our observation, the challenge of this task is to extract the relevance between a question\nand articles for classiﬁcation while the number of given articles is relatively small (usually 1 or 2 articles are given). We\nhypothesize that the model can extract information more effectively and consistently if there are more relevant articles\ngiven. Therefore, we adapt the data augmentation technique mentioned in Task 3 (which is based on tf-idf scores)\nto increase the number of relevant articles for each question. Besides, we also use the text chunking and self-labeled\ntechniques introduced in Section 3.2 for dealing with long article challenges.\n3.4 Task 5. Statute Law Question Answering\nTask 5 is a newly introduced task in COLIEE 2021. The goal of this task is to ask systems to perform legal question\nanswering. In detail, with a given statement, the model needs to answer whether the statement is true or false in the\nlegal aspect. In essence, Task 5 is constructed from Task 4, ignoring the step of retrieving the related clauses from the\nJapanese Civil Code.\n6\nHa-Thanh Nguyen et al.\nWe face the following challenges in this task: First, the number of training data is relatively small compared to other\nquestion answering datasets. Second, the language of the law is different from the daily language. Legal documents are\neven hard for the lay reader to read. And our mission is to train a deep learning model, a machine to understand the text\nand give the correct answer. Dealing with these challenges, ﬁrst, we propose to use a pretrained model. This kind of\nmodel is pretrained with a massive amount of data and be able to have high performance with less ﬁne-tuning data.\nSecond, there needs to be a method to model the legal language better.\nDisambiguation Approach The eternal problem of natural language processing in general and legal document\nprocessing, in particular, is ambiguity. The ambiguity can be in the lexical units of the text or in the way a model\nrepresents its semantics. For example, in a text that says “he went to the bank”, we cannot be certain that the word\n“bank” refers to a river’s bank or a ﬁnancial bank. This ambiguity can affect the entire inference chain and may lead to\nserious misunderstandings.\nA translation of a text gives us more information about its meaning than just a set of vocabulary translated into a new\nlanguage. A sentence in a language may contain much different semantics and depending on the context, the translation\nneeds to be the most appropriate sentence in the target language with the same meaning. For example, as in Figure 4\nin Japanese, こんにちはcan be a midday greeting or a formal way to say “hello”. In consequence, in the morning\ncontext, this sentence needs to be translated as “hello” rather than “good afternoon”. Similarly, the two meanings of\n“bank” are translated into Japanese with two completely different words,銀行and 河岸. Likewise, “I” in English can\nbe translated in a multitude of ways in Japanese. Determining which is the correct translation must depend on the\ncontext of the sentence.\nOur novelty in Task 5’s solution is the introduction of two models, which are NMSP and NFSP. The main idea in\nbuilding these two models is to use translation information as means of ambiguity reduction. We argue that a sentence\nin natural language can have many meanings, but in its translation, the most correct meaning will be expressed. In\naddition, the meaning is also determined by the context, that is, the sentences before and after the current sentence.\nIt is important to determine the correct context to correctly understand the meaning of a sentence when dealing with\ndifﬁcult documents such as the law. A correct understanding of semantic will not depend on its language of expression.\nTherefore, using the original version and the translation in parallel can help the model learn the semantic with better\nprecision.\nFigure 4: A single word may have multiple translations.\nPretraining ParaLaw Nets We introduce the pretrained models named ParaLaw Nets [ 28], which are pretrained\non cross-lingual sentence-level tasks before being ﬁnetuned for use in the COLIEE problem. The data we use to\npretrain these models is bilingual Japanese law data provided by Japanese Law Translation website 4. We formulate the\npretraining task for NFSP as a binary classiﬁcation problem and NMSP as a multi-label classiﬁcation problem.\nWe design the pretraining task to force the model to learn the semantic relationship in 2 continuous sentences crossing\ntwo languages. From original sentences as “The weather is nice. Shall we go out?”, their translations “いい天気ね。\nお出掛けしよ？”, and random sentences as “Random sentence.”, “ランダム文。”, we can generate the training\nsamples as in Table 2. Following this pattern, we obtain 239,000 samples for pretraining NFSP, and 718,000 samples\nfor pretraining NMSP.\n4https://www.japaneselawtranslation.go.jp\n7\nHa-Thanh Nguyen et al.\nSentence Pair NFSP Label NMSP Label\nShall we go out? The weather is nice. - 2\nお出掛けしよ？いい天気ね。 - 2\nお出掛けしよ？The weather is nice. - 2\nShall we go out? いい天気ね。 - 2\nいい天気ね。お出掛けしよ？ - 1\nThe weather is nice. Shall we go out? - 1\nThe weather is nice. お出掛けしよ？ 1 1\nいい天気ね。Shall we go out? 1 1\nThe weather is nice. ランダム文。 0 0\nいい天気ね。Random Sentence. 0 0\nThe weather is nice. Random Sentence. - 0\nいい天気ね。ランダム文。 - 0\nTable 2: Examples of pretraining data.\nModel Max Len. Batch Size #Batches Acc.\nNFSP Base 512 16 24,000 94.4%\nNFSP Distilled 512 32 34,000 92.2%\nNMSP Base 512 16 320,000 88.0%\nNMSP Distilled 512 32 496,000 87.7%\nTable 3: Parameters and performances in pretraining the models on the validation set.\nThese models are pretrained until the performance on the validation set does not increase. Through the results in Table 3,\nwe see that the models have better performance on the validation set with NFSP task, base models outperform distilled\nmodels. With these numbers, we can accept the assumption that the NFSP task is more straightforward than the NMSP\ntask.\nFinetuning Next, we ﬁnetune the models for the lawfulness classiﬁcation problem in Task 5 of the COLIEE-2021.\nGiven a statement as a legal question, the model needs to decide whether that statement is true or false. Without the\nsupport of lexical-based retrieval systems, the model needs to really understand the meaning of the previously learned\npropositions, generalize them and apply that knowledge to the question. For example, with the sentence “No abuse of\nrights is permitted.” the model should output a “Yes” prediction and the predicted label should be “No” for the sentence\n“The age of majority is reached when a person has reached the age of 12.”\nTo strengthen the bilingual model, we use original and augmented data in both English and Japanese. Negation is the\nmain method used to create variations of original data. The ﬁrst negation rule that is matched will be used only once\nto avoid the negation of the negation. With English negation rules, we reuse the rules proposed by Nguyen et al [26].\nJapanese negation rules are derived from basic Japanese syntax. There are in total 14 English negation rules and 13\nJapanese negation rules in our rule set.\n4 Experiments\n4.1 Task 1 and Task 2. Case Law Processing\nAs we mentioned in Section 3, we create a supporting training dataset to train BERT model and the analysis of this\ntraining dataset is as Table 4. From 4,415 cases in Task 1 raw dataset, we can extract more than 170K paragraphs.\nHowever, inside these paragraphs, some of them contain a lot of french text. This actively demonstrates that we need a\nﬁlter step to extract clean English text. For the purpose of avoiding noise in the dataset, we use langdetect5 to ﬁlter and\nremove french text from training data. The clean paragraphs are obtained so far, we split every single paragraph into\nsentences (>625K in total), and from these sentences, we apply some techniques to generate supporting examples. The\nmassive number of silver training examples for training BERT is over 350K examples.\nBesides silver data, we utilize the data from Task 2 to extract more training data. As you can see in the Table 4, the\nnumber of gold examples we can extract is more than 18K.\nFor Task 1, we submit 3 runs as follow:\n5https://pypi.org/project/langdetect/\n8\nHa-Thanh Nguyen et al.\nData Source #case #paragraph #sentence #example\nTask 1 4,415 172,495 626,540 378,720\nTask 2 425 - 913 18,238\nTable 4: Supporting dataset.\nRun ID F1 Score\nBM25SD_3_7 0.0019\nBM25SD_7_3 0.0019\nSD 0.0019\nTable 5: Result on Task 1.\n• Run 1 (BM25SD_7_3): The lexical score combine with the semantic score with ratio α7:3.\n• Run 2 (BM25SD_3_7): The lexical score combine with the semantic score with ratio α3:7.\n• Run 3 (SD): Only supporting score.\nFor Task 2, we submit 3 runs as follow:\n• Run1 (BM25Supporting_Denoising): The lexical score combine with the semantic score with ratio α7:3.\n• Run2 (BM25Supporting_Denoising_Finetune): The lexical score combine with the semantic score with ratio\nα7:3 and ﬁnetuning on gold training dataset.\n• Run3 (NFSP_BM25): The lexical score combine with NFSP model’s score.\nTable 5 and Table 6 show our ﬁnal runs’ performance. Our method on Task 1 has bad performance on the test set. This\nmay indicate that our method has not solved the problem of the large search space of 4,415 candidates for this year’s\nproblem. When we limit the searching space from 4,415 to 100 using the lexical matching score, the actual relevant\ncases have been ﬁltered out. Even so, this ﬁnding is useful for our future approaches in the next year’s competition.\nBecause of a smaller search space, this problem does not occur with Task 2 data. For Task 2, our best run achieves 61%\non F1 score, and we are in 5th place in the COLIEE 2021’s leaderboard.\n4.2 Task 3. The Statute Law Retrieval Task\nWe leverage the last year’s dataset to train and evaluate our proposed models. In our experiments, we report the macro-\nprecisions, recals, and F2 scores of class-wise precision means and class-wise recall means.\nMultiple experiments with different settings are conducted to ﬁnd the ﬁnal settings for the text chunking technique.\nWe ﬁnetuned on bert-base-japanese6 pretrained model with 3 epochs and report results in Table 7. Besides, we also\ndid experiments with other pretrained models, e.g., bert-base-japanese-whole-word-masking7 and xlm-roberta-base8.\nWe use <window_size>/<stride> to denote the sliding window parameters. As we can see, the <150>/<50> setting\nseems to be the most appropriate setting for this task. We utilize this setting to conduct experiments relating to the\nself-training technique. In detail, we ﬁrstly ﬁnetuned bert-base-japanese model with e1 epochs, then performed the\nself-labeling process, and continued to ﬁnetune with e2 epochs. We use <e1>/<e2> to denote this settings. As we can\nsee in the Table 8, it suggests that e1 = 2seems to be the “just right” parameter for the ﬁrst ﬁnetuning process (when\ne1 = 3, the ﬁnetuned model seems to start overﬁtting); and as e2 increases, F2 tends to increase. As an observation, our\nproposed methods demonstrate a positive impact where F2 score improve 1.04% to 72.66% when applying the text\nchunking technique, and this number is 1.29% with the self-labeled technique.\n6https://huggingface.co/cl-tohoku/bert-base-japanese\n7https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking\n8https://huggingface.co/xlm-roberta-base\nRun ID F1 Score\nBM25Supporting_Denoising 0.6116\nBM25Supporting_Denoising_Finetune 0.6091\nNFSP_BM25 0.5868\nTable 6: Result on Task 2.\n9\nHa-Thanh Nguyen et al.\nChunking info. Return Retrieved P R F2\nno chunking 118 81 68.24 72.52 71.62\n110/20 190 08 61.20 67.87 66.42\n150/10 122 85 64.77 66.67 66.28\n150/20 129 93 68.09 70.72 70.18\n150/40 131 92 67.12 71.17 70.32\n150/50 132 94 9.74 3.42 2.66\n200/50 139 90 67.12 72.97 71.72\n300/50 110 74 65.39 67.57 67.12\nTable 7: (Task 3) Results of bert-base-japanese pretrained model ﬁnetuning with the text chunking technique. The\nvalues in the Chunking info. column denotes chunking settings with format ⟨window_size⟩/⟨stride⟩.\nSetting Return Retrieved P R F2\n3/0 132 94 69.74 73.42 72.66\n1/1 109 81 68.02 67.57 67.66\n1/2 145 98 66.89 72.52 71.32\n1/3 133 96 68.77 72.97 72.09\n2/1 161 95 64.55 72.52 70.77\n2/2 161 95 64.55 72.52 70.77\n2/3 195 104 62.39 76.13 72.91\n3/1 146 97 63.32 68.92 67.72\n3/2 146 96 64.37 71.17 69.70\n3/3 147 97 60.02 65.77 64.53\nTable 8: (Task 3) Results of bert-base-japanese pretrained model with the self-labeled technique. The values in the\nSetting column follows the format <e1>/<e2>.\nIn the competition, we submitted 3 runs based on the proposed approach. For task 3, our position is the runner-up.\nTable 9 shows the results of ﬁnal runs of all participants.\n4.3 Task 4. The Legal Textual Entailment Task\nSimilar to Task 3, we also trained and evaluated our proposed models with the dataset in the previous year with the\nquestions having id R-01-* as the development set. Because of the relatively small training data, we run 5 times with\neach setting and report the mean and standard deviation values. For this task, most of the hyperparameters follows\nsettings in [19] where batch_size= 16and learning_rate= 1e−5.\nRun ID sid F2 Prec Recall\nOvGU_run1 E/J 0.7302 0.6749 0.7778\nJNLP.CrossLMultiLThreshold E/J 0.7227 0.6000 0.8025\nBM25.UA E/J 0.7092 0.7531 0.7037\nJNLP.CrossLBertJP E/J 0.7090 0.6241 0.7716\nR3.LLNTU E/J 0.7047 0.6656 0.7438\nR2.LLNTU E/J 0.7039 0.6770 0.7315\nR1.LLNTU E/J 0.6875 0.6368 0.7315\nJNLP.CrossLBertJPC15030C15050 E/J 0.6838 0.5535 0.7778\nOvGU_run2 E/J 0.6717 0.4857 0.8025\nTFIDF.UA E/J 0.6571 0.6790 0.6543\nLM.UA E/J 0.5460 0.5679 0.5432\nTR_HB E/J 0.5226 0.3333 0.6173\nHUKB-3 J 0.5224 0.2901 0.6975\nHUKB-1 J 0.4732 0.2397 0.6543\nTR_A V1 E/J 0.3599 0.2622 0.5123\nTR_A V2 E/J 0.3369 0.1490 0.5556\nHUKB-2 J 0.3258 0.3272 0.3272\nOvGU_run3 E/J 0.3016 0.1570 0.7006\nTable 9: (Task 3) Result of ﬁnal runs on the test set, underlined run IDs refer to our models.\n10\nHa-Thanh Nguyen et al.\nModel Origin tf-idf1 tf-idf2 tf-idf5 tf-idf20\nBertJp 55.9 ± 3.7 - - - -\nBertJp2 60.4 ± 4.1 61 .6 ± 5.0 62 .7 ± 5.7 61 .3 ± 4.4 58 .4 ± 2.9\nTable 10: (Task 4) Model accuracies with different tf-idf augmentation settings. The name BertJp, BertJp2 indicates\nthat we used a pre-trained bert-base-japanese and bert-base-japanese-whole-word-masking, respectively. The name\ncolumn follows the format tf-idf<number> where <number> denotes the number of augmented articles appended for\neach question.\n# epochs tf-idf1 tf-idf2 tf-idf5\n3 61.6 ± 5.0 62 .7 ± 5.7 61 .3 ± 4.4\n10 61.8 ± 3.0 62 .9 ± 2.2 64.7 ± 2.5\n20 - 61.6 ± 1.8 64 .0 ± 1.3\nTable 11: (Task 4) Accuracies of BertJp2 with different training epochs.\nPretrained model and Data augmentation Firstly, we conducted the experiments to ﬁnd the most suitable pretrained\nmodel for this task, and the most suitable setting for the tf-idf-based data augmentation method (Table 10). Based on\nthe experimental results, we found that the bert-base-japanese-whole-word-masking pretrained model is more suitable\nfor this task than others. Besides, the tf-idf-based augmentation data method also help increase the model performance.\nPerformance stability In addition, we found that the performance of the model with a small epoch is fairly unstable.\nTherefore, we experimented with a bigger number of training epochs (Table 11). The experimental results demonstrate\nthat the runs with higher epochs tend to achieve more stable accuracies, but the model can be overﬁtted if we increase\nthe number of epochs too much. Besides, the augmentation data also helps the model performance to be more stable.\nLong article challenge. Finally, to address the long article challenge, we conducted the experiments using the text\nchunking and the self-labeled techniques described in Task 3 (Table 12). Although the accuracy of models in this setting\ndid not increase, the variant ranges are smaller. It may be because the text chunking and the self-labeled techniques help\neliminate noises in training data.\nThe results on the blind test set are shown in the Table 13 with the id “ JNLP.Enss5C15050” refers to the model\nBertJp2 using augmentation data tf-idf5; “JNLP.Enss5C15050SilverE2E10” refers to the model BertJp2 using\naugmentation data tf-idf5, and <e1>/<e2> is 2 /10; “JNLP.EnssBest ” refers to the ensemble of both models.\n4.4 Task 5. Statute Law Question Answering\nWe compare our proposed models together and with other cross-lingual and multilingual baselines such as XLM-\nRoBERTa [29] and original BERT Multilingual [ 2]. In the 7,000 augmented samples, we divide the train set and\nvalidation set with the ratio of 9:1. The blind test set is later provided by the competition’s organization.\nOur experiments show that NFSP Base and NMSP Base achieve the best performance and have stable loss decrease.\nNFSP Distilled, NMSP Distilled and XLM-RoBERTa fail to learn from the data and their performance equivalent to\nthat of random sampling. BERT Multilingual is in the middle of the ranked list of models in Table 14. Therefore, we\nchoose NFSP Base, NMSP Base and original BERT Multilingual as candidates for the ﬁnal run.\nTable 15 is the result of the models on the blind test of COLIEE-2021’s organizer. On this test set, NFSP Base\noutperforms other methods and becomes the best system for this task. NMSP is in third place and the original BERT\nMultilingual has the performance below the baseline. These results again support our proposal in pretraining models\nusing sentence-level cross-lingual information.\nSetting tf-idf2 tf-idf5\n1/10 62.9 ±1.7 63 .1 ±1.9\n2/10 64.3 ±0.5 64.3 ±0.5\n3/10 62.2 ±1.7 64 .1 ±1.0\nTable 12: (Task 4) Accuracies of BertJp2 using the self-labeled technique with different settings on chunking data\nwhere window_size = 150, stride = 50. The values in the Setting column denotes <e1> / <e2>.\n11\nHa-Thanh Nguyen et al.\nTeam sid Correct Acc.\nHUKB HUKB-2 57 0.7037\nUA UA_parser 54 0.6667\nJNLP JNLP.Enss5C15050 51 0.6296\nJNLP JNLP.Enss5C15050SilverE2E10 51 0.6296\nJNLP JNLP.EnssBest 51 0.6296\nOVGU OVGU_run3 48 0.5926\nTR TR-Ensemble 48 0.5926\nKIS KIS1 44 0.5432\nUA UA_1st 44 0.5432\nTable 13: (Task 4) Results ﬁnal runs on the test set. The underlined lines refer to our submissions.\nModel Accuracy\nNFSP Base 71.0%\nNFSP Distilled 51.1%\nNMSP Base 79.5%\nNMSP Distilled 48.9%\nXLM-RoBERTa 51.1%\nBERT Multilingual 64.1%\nTable 14: (Task 5) Performance of models on the validation set.\n5 Conclusions\nThis paper presents the approaches of JNLP team at COLIEE 2021 on all 5 tasks of legal document processing. The\ncommon point of the proposed approaches is the implementation of the Transformer architecture and its variants for\nspeciﬁc legal problems. The article is not only a technical report on using Transformer for legal processing tasks but\nalso provides details on the methods such as problem deﬁnition, data processing, model pretraining and ﬁnetuning.\nThese proposed methods all come from a detailed investigation of the properties of the problem and data provided in\nthe competition. Besides, the NFSP and NMSP construction methods proposed in the paper are novel and effective.\nThey deserve to be investigated in problems of similar characteristics.\n6 Acknowledgements\nThis work was supported by JSPS Kakenhi Grant Number 20H04295, 20K20406, and 20K20625. The research also\nwas supported in part by the Asian Ofﬁce of Aerospace R&D (AOARD), Air Force Ofﬁce of Scientiﬁc Research (Grant\nno. FA2386-19-1-4041).\nTeam Run ID Correct Accuracy\nJNLP JNLP.NFSP 49 0.6049\nUA UA_parser 46 0.5679\nJNLP JNLP.NMSP 45 0.5556\nUA UA_dl 45 0.5556\nTR TRDistillRoberta 44 0.5432\nKIS KIS_2 41 0.5062\nKIS KIS_3 41 0.5062\nUA UA_elmo 40 0.4938\nJNLP JNLP.BERT_Multilingual 38 0.4691\nKIS KIS_1 35 0.4321\nTR TRGPT3Ada 35 0.4321\nTR TRGPT3Davinci 35 0.4321\nTable 15: (Task 5) Result of ﬁnal runs on the test set, the underlined lines refer to our models.\n12\nHa-Thanh Nguyen et al.\nReferences\n[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and\nIllia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[3] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A\nlite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.\n[4] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as\ndiscriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.\n[5] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves\nStoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language\ngeneration, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.\n[6] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by\ngenerative pre-training, 2018.\n[7] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[8] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165, 2020.\n[9] Robert Kowalski and Akber Datoo. Logical english meets legal english for swaps and derivatives. Artiﬁcial\nIntelligence and Law, pages 1–35, 2021.\n[10] Ken Satoh, Kento Asai, Takamune Kogawa, Masahiro Kubota, Megumi Nakamura, Yoshiaki Nishigai, Kei\nShirakawa, and Chiaki Takano. Proleg: an implementation of the presupposed ultimate fact theory of japanese\ncivil code by prolog technology. In JSAI International Symposium on Artiﬁcial Intelligence, pages 153–164.\nSpringer, 2010.\n[11] William S Cooper. A deﬁnition of relevance for information retrieval. Information storage and retrieval, 7(1):19–\n37, 1971.\n[12] Hans Peter Luhn. A statistical approach to mechanized encoding and searching of literary information. IBM\nJournal of research and development, 1(4):309–317, 1957.\n[13] Gerard Salton and Christopher Buckley. Term-weighting approaches in automatic text retrieval. Information\nprocessing & management, 24(5):513–523, 1988.\n[14] Keet Sugathadasa, Buddhi Ayesha, Nisansa de Silva, Amal Shehan Perera, Vindula Jayawardana, Dimuthu Lakmal,\nand Madhavi Perera. Legal document retrieval using document vector embeddings and deep learning. In Science\nand Information Conference, pages 160–175. Springer, 2018.\n[15] Phi Manh Kien, Ha-Thanh Nguyen, Ngo Xuan Bach, Vu Tran, Minh Le Nguyen, and Tu Minh Phuong. An-\nswering legal questions by learning neural attentive text representation. In Proceedings of the 28th International\nConference on Computational Linguistics, pages 988–998, Barcelona, Spain (Online), December 2020. Interna-\ntional Committee on Computational Linguistics.\n[16] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\n[17] Vu Tran, Minh Le Nguyen, and Ken Satoh. Building legal case retrieval systems with lexical matching and sum-\nmarization using a pre-trained phrase scoring model. In Proceedings of the Seventeenth International Conference\non Artiﬁcial Intelligence and Law, pages 275–282, 2019.\n[18] Juliano Rabelo, Mi-Young Kim, and Randy Goebel. Combining similarity and transformer methods for case\nlaw entailment. In Proceedings of the Seventeenth International Conference on Artiﬁcial Intelligence and Law,\nICAIL ’19, page 290–296, 2019.\n[19] Ha-Thanh Nguyen, Hai-Yen Thi Vuong, Phuong Minh Nguyen, Binh Tran Dang, Quan Minh Bui, Sinh Trong Vu,\nChau Minh Nguyen, Vu Tran, Ken Satoh, and Minh Le Nguyen. Jnlp team: Deep learning for legal processing in\ncoliee 2020. arXiv preprint arXiv:2011.08071, 2020.\n[20] T.B.Dang, T.Nguyen, and L.M.Nguyen. An approach to statute law retrieval task in coliee-2019., 2019.\n13\nHa-Thanh Nguyen et al.\n[21] S.Wehnert, S.A.Hoque, W.Fenske, and G.Saake. Threshold-based retrieval and textual entailment detection on\nlegal bar exam questions., 2019.\n[22] B.Gain, D.Bandyopadhyay, T.Saikh, and A.Ekbal. Iitp@coliee 2019: Legal information retrieval using bm25 and\nbert., 2019.\n[23] R.Hayashi and Y .Kano. Searching relevant articles for legal bar exam by doc2vec and tf-idf., 2019.\n[24] R.Hoshino, N.Kiyota, and Y .Kano. Question answering system for legal bar examination using predicate argument\nstructures focusing on exceptions., 2019.\n[25] J.Hudzina, T.Vacek, K.Madan, C.Tonya, and F.Schilder. Statutory entailment using similarity features and\ndecomposable attention models., 2019.\n[26] HT Nguyen, V Tran, and LM Nguyen. A deep learning approach for statute law entailment task in coliee-2019.\nProceedings of the 6th Competition on Legal Information Extraction/Entailment. COLIEE, 2019.\n[27] Isaac Triguero, Salvador García, and Francisco Herrera. Self-labeled techniques for semi-supervised learning:\ntaxonomy, software and empirical study. Knowledge and Information systems, 42(2):245–284, 2015.\n[28] Ha-Thanh Nguyen, Vu Tran, Phuong Minh Nguyen, Thi-Hai-Yen Vuong, Quan Minh Bui, Chau Minh Nguyen,\nBinh Tran Dang, Minh Le Nguyen, and Ken Satoh. Paralaw nets–cross-lingual sentence-level pretraining for legal\ntext processing. arXiv preprint arXiv:2106.13403, 2021.\n[29] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán,\nEdouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation\nlearning at scale. arXiv preprint arXiv:1911.02116, 2019.\n14",
  "topic": "Novelty",
  "concepts": [
    {
      "name": "Novelty",
      "score": 0.8408278226852417
    },
    {
      "name": "Transformer",
      "score": 0.7637185454368591
    },
    {
      "name": "Computer science",
      "score": 0.7523620128631592
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5123921036720276
    },
    {
      "name": "Text processing",
      "score": 0.4840752184391022
    },
    {
      "name": "Document processing",
      "score": 0.45581549406051636
    },
    {
      "name": "Task (project management)",
      "score": 0.432426393032074
    },
    {
      "name": "Natural language processing",
      "score": 0.35063329339027405
    },
    {
      "name": "Machine learning",
      "score": 0.3357391059398651
    },
    {
      "name": "Engineering",
      "score": 0.11192291975021362
    },
    {
      "name": "Systems engineering",
      "score": 0.0711478590965271
    },
    {
      "name": "Voltage",
      "score": 0.06564512848854065
    },
    {
      "name": "Theology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I177738480",
      "name": "Japan Advanced Institute of Science and Technology",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I67868205",
      "name": "VNU University of Science",
      "country": "VN"
    },
    {
      "id": "https://openalex.org/I4210134673",
      "name": "The Institute of Statistical Mathematics",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I184597095",
      "name": "National Institute of Informatics",
      "country": "JP"
    }
  ]
}