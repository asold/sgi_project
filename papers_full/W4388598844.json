{
  "title": "Exploring the Sources of Variance in Risky Decision Making with Large Language Models",
  "url": "https://openalex.org/W4388598844",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2591927700",
      "name": "Sudeep Bhatia",
      "affiliations": [
        "University of Pennsylvania",
        "California University of Pennsylvania"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3163824338",
    "https://openalex.org/W6773757383",
    "https://openalex.org/W2937988765",
    "https://openalex.org/W4286882331",
    "https://openalex.org/W3179093522",
    "https://openalex.org/W3161094030",
    "https://openalex.org/W4323859461",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2039223548",
    "https://openalex.org/W2908580366",
    "https://openalex.org/W6646014332",
    "https://openalex.org/W1713503745",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2106098322",
    "https://openalex.org/W6769534353",
    "https://openalex.org/W2149709144",
    "https://openalex.org/W6660648470",
    "https://openalex.org/W3103933699",
    "https://openalex.org/W2761002469",
    "https://openalex.org/W3005755974",
    "https://openalex.org/W3194508968",
    "https://openalex.org/W6674612334",
    "https://openalex.org/W2126523304",
    "https://openalex.org/W2119432837",
    "https://openalex.org/W2078666525",
    "https://openalex.org/W2102297274",
    "https://openalex.org/W6795961182",
    "https://openalex.org/W4361298850",
    "https://openalex.org/W6768206361",
    "https://openalex.org/W6674424674",
    "https://openalex.org/W6684368033",
    "https://openalex.org/W2299585208",
    "https://openalex.org/W6601366451",
    "https://openalex.org/W1986808060",
    "https://openalex.org/W1983578042",
    "https://openalex.org/W2167026971",
    "https://openalex.org/W6759036917",
    "https://openalex.org/W2415973339",
    "https://openalex.org/W6807091881",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W6673109191",
    "https://openalex.org/W6675354045",
    "https://openalex.org/W6682040155",
    "https://openalex.org/W2169863498",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2981523423",
    "https://openalex.org/W6846034341",
    "https://openalex.org/W6681496392",
    "https://openalex.org/W6657833400",
    "https://openalex.org/W1970035964",
    "https://openalex.org/W2131490687",
    "https://openalex.org/W2140650362",
    "https://openalex.org/W6795563969",
    "https://openalex.org/W6673209958",
    "https://openalex.org/W6675474709",
    "https://openalex.org/W3088556343",
    "https://openalex.org/W2140910804",
    "https://openalex.org/W2041815189",
    "https://openalex.org/W1991011913",
    "https://openalex.org/W2030332419",
    "https://openalex.org/W6679441832",
    "https://openalex.org/W2055253603",
    "https://openalex.org/W6791351455",
    "https://openalex.org/W4226435710",
    "https://openalex.org/W2913142708",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4230653330",
    "https://openalex.org/W4242717215",
    "https://openalex.org/W4312184636",
    "https://openalex.org/W3108830233",
    "https://openalex.org/W3135352592",
    "https://openalex.org/W4242671241",
    "https://openalex.org/W4244943366",
    "https://openalex.org/W2983183399",
    "https://openalex.org/W1531590217",
    "https://openalex.org/W2130174911",
    "https://openalex.org/W3125553748",
    "https://openalex.org/W4250042789",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2103379473",
    "https://openalex.org/W3042724247",
    "https://openalex.org/W4302305326",
    "https://openalex.org/W4241259091",
    "https://openalex.org/W3011865677",
    "https://openalex.org/W4256718227",
    "https://openalex.org/W4242609504",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2145100107",
    "https://openalex.org/W4252434862",
    "https://openalex.org/W2030142386",
    "https://openalex.org/W4313428941",
    "https://openalex.org/W4220793486",
    "https://openalex.org/W2096016260",
    "https://openalex.org/W2096958121",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2340149949",
    "https://openalex.org/W2166420173",
    "https://openalex.org/W4307429315"
  ],
  "abstract": "What are the sources of individual-level differences in decision making, and how do they depend on the domain or situation in which the decision is being made? Psychologists currently answer such questions with psychometric methods, which analyze correlations across participant ratings in survey datasets. In this paper we model the psychological mechanisms that give rise to these correlations. Our approach uses 1. Large language models (LLMs) to quantify everyday behaviors in terms of the attributes or reasons that may describe those behaviors, and 2. Decision models to map these attributes and reasons onto participant ratings. We show that LLM-based decision models can explain observed correlations between risky behaviors in terms of the reasons different behaviors elicit, and explain observed correlations between individuals in terms of the weights different individuals place on reasons, thereby providing a process-level foundation for psychometric analysis. Since LLMs provide quantitative representations for nearly any naturalistic decision, they can be used to make accurate out-of-sample predictions for hundreds of everyday behaviors, predict the reasons why people may or may not want to engage in these behaviors, and interpret these reasons in terms of intuitive psychological constructs. Our approach has important theoretical and practical implications for the study of heterogeneity in everyday decision making.",
  "full_text": "SOURCES OF VARIANCE IN DECISION MAKING 1 \n \n \n \n \n \n \n \nExploring the Sources of Variance in Risky Decision Making with Large Language Models \n \nSudeep Bhatia \nUniversity of Pennsylvania  \n \nDecember 12, 2023 \n \n \n \n \n \n \n \nSend correspondence to Sudeep Bhatia, Department of Psychology, University of Pennsylvania, \nPhiladelphia, PA. Email: bhatiasu@sas.upenn.edu. Funding was received from the National \nScience Foundation grant SES-1847794. The design of the studies in this paper was preregistered \nat https://osf.io/amves and code and data is available at https://osf.io/3y6ku.  \nSOURCES OF VARIANCE IN DECISION MAKING 2 \n \nAbstract \nWhat are the sources of individual-level differences in decision making, and how do they depend \non the domain or situation in which the decision is being made? Psychologists currently answer \nsuch questions with psychometric methods, which analyze correlations across participant ratings \nin survey datasets. In this paper we model the psychological mechanisms that give rise to these \ncorrelations. Our approach uses 1. Large language models (LLMs) to quantify everyday \nbehaviors in terms of the attributes or reasons that may describe those behaviors, and 2. Decision \nmodels to map these attributes and reasons onto participant ratings. We show that LLM-based \ndecision models can explain observed correlations between risky behaviors in terms of the \nreasons different behaviors elicit, and explain observed correlations between individuals in terms \nof the weights different individuals place on reasons, thereby providing a process-level \nfoundation for psychometric analysis. Since LLMs provide quantitative representations for \nnearly any naturalistic decision, they can be used to make accurate out-of-sample predictions for \nhundreds of everyday behaviors, predict the reasons why people may or may not want to engage \nin these behaviors, and interpret these reasons in terms of intuitive psychological constructs. Our \napproach has important theoretical and practical implications for the study of heterogeneity in \neveryday decision making. \n \nSOURCES OF VARIANCE IN DECISION MAKING 3 \n \nIntroduction \nThe decisions that people make in everyday life determine their well-being, as well as the \nstructure of our societies, governments, and economies. Understandably, considerable research in \npsychology has focused on understanding why people make the decisions they do, why different \npeople may make different decisions, and why different domains and situations may elicit \ndifferent decisions from the same individual (for reviews see Edwards, 1961; Slovic et al., 1977; \nWeber & Johnson, 2007). Traditionally, much of this work falls within the psychometric \ntradition. Typically, researchers ask individuals to rate their tendencies to think or act in a certain \nway, and based on the correlational structure of the observed data, group individuals and items \ninto coherent clusters, and study how these clusters differ based on situational, demographic, \ncultural, or personality variables.  \nThe psychometric approach has made significant contributions to the study of decision \nmaking, and has been widely adopted by researchers and practitioners working on risky behavior \n(e.g. Lauriola et al., 2007; Slovic, 1987; Weber et al., 2002), consumer behavior (e.g. Rick et al., \n2008; Tian et al., 2001), social behavior (e.g. Graham et al., 2009; Rushton et al., 1981), as well \nas associated decision styles (e.g. Cacioppo & Petty, 1982; Epstein et al., 1996; Schwartz et al., \n2002) and decision outcomes (e.g. Bruine de Bruin et al., 2007; Peters et al., 2006). However, by \nitself, it is unable to provide a complete account of the sources of variance in behavior. \nPsychometrics models the variance revealed by the data, and does not directly try to explain this \nvariance in terms of computational theories of cognitive and decision processes. Additionally, \nthe psychometric approach relies exclusively on the statistical relationships within the collected \ndata. Without knowing the reasons and motivations behind people’s responses, it cannot \nSOURCES OF VARIANCE IN DECISION MAKING 4 \n \nanticipate how they will behave in novel circumstances or respond to new survey items for \nwhich ratings have not yet been collected.  \nA different line of research assumes that engaging in a particular behavior or making a \nparticular decision involves different attributes or reasons, and that decision makers place \nweights on these attributes or reasons depending on their goals or desires. In this way, responses \ncan be captured using a “decision model”, which quantifies a behavior or decision in terms of its \nattribute values and quantifies the decision maker’s preferences using attribute weights. Decision \nmodels have been able to explain and predict various phenomena observed in laboratory tasks in \nwhich the attribute structures of the decision alternatives are explicitly provided to the decision \nmakers. These include risky (Kahneman & Tversky, 1979; He et al., 2022; Markowitz, 1959), \nintertemporal (Loewenstein & Prelec, 1992; He et al., 2023), social (Fehr & Schmidt, 1999; \nCamerer, 2011), managerial (Howard, 1988; Keeny & Raiffa, 1993), and consumer (Green & \nSrinivasan, 1990; Payne et al., 1991) choice. Decision models also relate choice behavior to core \ncognitive mechanisms like memory and attention, which determine how people sample and \nintegrate attributes and reasons over the time course of the decision (Gigerenzer & Gaissmaier, \n2011; Payne et al., 1988; Roe et al, 2001; Weber et al., 2007; Zhao et al., 2022). For this reason, \ndecision models provide a popular framework for describing choice in psychology, as well as \nclosely related fields like behavioral economics, marketing, management, and cognitive \nneuroscience (see Bhatia et al., 2021 and He et al., 2022 for recent reviews of existing models).  \nImportantly, decision models also provide a theoretically grounded explanation for item \nand individual-level differences in behavior. According to the decision modeling approach, the \nsimilarity of responses across different items can be explained by the items’ attribute structures: \nitems that have similar attributes will be judged similarly by the decision makers. Likewise, the \nSOURCES OF VARIANCE IN DECISION MAKING 5 \n \nsimilarity of responses across different individuals can be explained by individuals’ preference \nfunctions: individuals that assign similar weights to the attributes will make similar decisions. \n  Despite their appeal, decision models have limited applicability to real-world behaviors, \nwhere the attribute structures are often implicit and uncertain. In the laboratory, experimenters \ncan manipulate one or two attributes and measure their effects on preferences, but in the real-\nworld, common behaviors (such as investing in a speculative stock, mountain climbing, eating \nhigh cholesterol foods, shoplifting, and trusting a stranger with personal information) have many \nmore complex reasons and attributes that influence them. Therefore, there is a gap between \ndecision models of behavior and the actual behaviors that people exhibit in naturalistic contexts.  \nIn this paper we attempt to solve this problem using large language models (LLMs) \n(Brown et al., 2020; Devlin et al., 2018; Reimers et al., 2019; also see Mikolov et al., 2013). \nLLMs are artificial intelligence systems that can understand natural language after being trained \non massive amounts of text data. They have shown remarkable capabilities in various natural \nlanguage processing tasks, such as question answering, text summarization, content creation, and \nlanguage translation, and are being widely used to model the mental representations that underlie \nhuman cognition and behavior (for reviews, see Bhatia et al., 2019 and Bhatia & Aka, 2022). \nThe reason for these successes is the ability of LLMs to quantify language by transforming \nwords and sentences into numerical representations that capture their meanings. These numerical \nrepresentations are used for prediction and language generation. Our goal is the use \nrepresentations obtained from LLMs to quantify the attributes that underlie everyday behaviors, \nand by doing so, apply the decision modeling approach to both predict how and understand why \nindividuals vary in their behavior, and why different items elicit different behaviors from \nindividuals. \nSOURCES OF VARIANCE IN DECISION MAKING 6 \n \nWe apply our approach to one of the most important topics in decision making research: risk. \nOur empirical plan builds on a prominent existing scale for modeling domain and individual \ndifferences in everyday risky decisions, the Domain Specific Risk-Taking scale (DOSPERT) \n(Blais & Weber, 2006; Weber et al., 2002). DOSPERT measures people’s tendencies to engage \nin several different types of common risky behaviors, which are presented to participants using \nshort phrases and sentences. We present an expanded version of this scale to participants, and, \nwith the use of LLM-derived attribute representations for the behaviors, attempt to both predict \nand understand people’s responses, as well as the covariance structure in responses across items \nand individuals. We also use preference functions derived from people’s choices to model the \nqualitative reasons that people use to explain their behavior. In this way, our approach attempts \nto combine decision modeling and psychometric research traditions, and by doing so, provide a \ncomprehensive theoretical account of the structure of variance in everyday decision making. \n \nOverview of Approach \nPsychometrics \nPsychometrics is central to the study of individual variability in several sub-disciplines of \npsychology. Most relevant to this paper is work that has applied psychometric techniques to \nstudy risky decision making. One of the most prominent instruments in this area, the Domain \nSpecific Risk-Taking Scale (DOSPERT), uses an inventory of 30 or 40 common behaviors \nsampled from five different domains: health, recreation, social behavior, ethical behavior, and \nfinancial decision making. It asks people to rate their propensity of committing each of the \nbehaviors in the scale, and uses observed ratings to understand the differences between different \nSOURCES OF VARIANCE IN DECISION MAKING 7 \n \nindividuals and groups, as well as between different items and domains (Blais & Weber, 2006; \nWeber et al., 2002).   \nFormally, the rating for behavior item i elicited from participant j can be written as rij. \nSubsequently, the list of all participant ratings for behavior i can be written as rbi = [ri1, ri2, … \nriM] and the list of all behavior ratings by a participant j can be written as rpj = [r1j, r2j, … rNj], \nwhere M and N are the number of participants and behaviors (items) respectively. With this \nnotation, it is possible to quantify the similarity between two behaviors, i and i', in terms of their \ncorrelation across the individuals in a dataset, COR(rbi, rbi’), and likewise quantify the similarity \nbetween two participants, j and j’, in terms of their correlation across the behaviors in the dataset, \nCOR(rpj, rpj’). Psychometric methods often use these correlations to uncover latent factors or \ndimensions that describe the structure of variability in people’s behaviors. \nResearchers working with DOSPERT and related measures have found that, contrary to \neconomic theories of decision making that posit a single general risk-taking propensity, risk \ntaking in everyday life varies significantly across domains (Dohmen et al., 2011; Frey et al., \n2017; Highhouse et al., 2016; Weber et al., 2002). For example, the tendency to engage in a \nrecreational risk (like mountain climbing) is strongly correlated with the tendency to engage in \nanother recreational risk (like scuba diving), but not necessarily with the tendency to engage in a \nsocial risk (like trusting a stranger with personal information). Interestingly, the factor structure \nthat best describes the covariance of ratings for behaviors does not perfectly match up with \nexperimenter intuitions about the domains that the behaviors belong to. Thus, for example, in \nStudy 1 of Weber et al. (2002), ethical risks like drunk driving and health risks like cigarette \nsmoking loaded onto the same factor. Conversely, financial risks loaded onto separate factors, \none interpreted as investment risk and the other as gambling risk. Related work has found that \nSOURCES OF VARIANCE IN DECISION MAKING 8 \n \nalthough there are coherent “risk profiles” in the population, these do not cleanly subdivide \nbased on the underlying domains examined by DOSPERT (Frey et al., 2023). For example, some \nindividuals tend to be more likely to take both gambling and ethical risks, others are more likely \nto take gambling and investment risks, and yet others are more likely to take recreational and \nsocial risks. It is not clear why different items and individuals cluster together the way they do. \nIndeed, the psychometric paradigm, which infers underlying constructs (like domain structures \nand risk profiles) using only observed correlations across items or individuals, does not attempt \nto directly answer this question.  \nResearchers have also used DOSPERT and similar inventories, to study how risk taking \nvaries as a function of individual-specific variables, like age, gender and personality (Blais & \nWeber, 2006; Dohmen et al., 2011; Frey et al., 2017, 2021; Harris & Jenkins, 2006; Highhouse \net al., 2016; Josef et al., 2016; Weber et al., 2002; Weller & Tikir, 2010). Unsurprisingly, people \ndiffer in the types of risks they take, and this can be predicted by their demographic and \npsychographic profiles. For example, Weber et al. (2022) and Harris and Jenkins (2006) found \nthat men are more likely to engage in risky behavior than women in most domains, though this \ntendency disappears for social risks. In our own data, discussed below, we replicated this general \ntendency, though we also found important differences between behaviors within each domain. \nFor example, men were less likely than women to engage in social risks like ending a friendship \nbut more likely to engage in social risks like going door to door to sell a product. Again, this \nsuggests that experimenter intuitions about the domain structure of risks may not be best suited \nfor untangling the complex interplay between everyday behavior and individual-specific \nvariables like gender.  \nDecision Modeling \nSOURCES OF VARIANCE IN DECISION MAKING 9 \n \nAn alternate approach to studying behavior is based on the decision modeling paradigm. \nWhile psychometric methods use insights from statistics for conceptualizing the structure of \nvariance in a dataset, decision models attempt to capture the processes that people use to make \ndecisions using formal mathematical functions or computer algorithms (Bhatia et al., 2021; \nBusemeyer et al., 2019; He et al., 2022; see also Busemeyer & Diederich, 2010, for an accessible \nintroduction to cognitive modeling). At the core of most decision models is the assumption that \npeople represent everyday choice alternatives and behaviors in terms of their underlying \nattributes, features, and reasons. People also have preferences over these attributes, which take \nthe form of attribute weights, and subsequently calculate the overall desirabilities of items \nthrough a weighted aggregation of their attribute values (Keeney & Raiffa, 1993). Formally, if \nwe write the attribute values of a behavior i as a vector xi and the attribute weights for individual \nj as a vector wj, this approach would describe the decision maker’s preference (and subsequently \npropensity of engaging in the behavior) as rij ~ wj ∙ xi. Of course, people may not always use \nstrictly linear models, as assumed here. For example, they may apply heuristics that simplify \nattribute weighting and aggregation, use nonlinear decision rules that interact the values of \nattributes, or aggregate attributes sequentially over time instead of all at once. Considerable \nresearch in psychology has focused on evaluating these diverse processes, as well as \nunderstanding how they interact with attention, memory, and other core properties of human \ncognition (Gigerenzer & Gaissmaier, 2011; Payne et al., 1988; Roe et al, 2001; Weber et al., \n2007; Zhao et al., 2022). \nRegardless of the processes that people use to aggregate attributes, decision models \nprovide a useful approach to quantitatively predicting and, more importantly, explaining both \nitem and individual-level differences in decision making. Since behaviors are described in terms \nSOURCES OF VARIANCE IN DECISION MAKING 10 \n \nof attributes, similarities and differences across these behaviors could be understood in terms of \nsimilarities and differences in attributes: Two behaviors that are highly correlated with each \nother likely evoke a similar set of reasons and have similar attribute profiles. Likewise, since \npreferences over behaviors are described in terms of attribute weights, similarities and \ndifferences across individuals can be understood in terms of similarities and differences in these \nweights: Two individuals that are highly correlated with each other likely care about the same \nreasons and place similar weights on the attributes as each other. To the extent that there is \ncoherence in behavior within a domain (e.g. financial risk) or a social group (e.g. men) it would \nbe because behaviors in that domain have similar attribute profiles and individuals in that social \ngroup have similar attribute weights.  \nFormally, we could quantify the similarity of two behaviors, i and i’, as SIM(xi, xi’), and \nthe similarity of two individuals, j and j’, as SIM(wj, wj’), where SIM() is some similarity function \nthat operates on pairs of attribute vectors (below, we will use cosine similarity, which measures \nsimilarities in terms of the directions of the vectors). If decision models are capable of describing \nitem and individual-level differences in data, we would expect COR(rbi, rbi’) ~ SIM(xi, xi’) and \nCOR(rpj, rpj’) ~ SIM(wj, wj’) respectively. In this way, the correlational structure observed in \nsurvey data (and modeled using standard psychometric methods) could be predicted using the \nattribute profiles of behaviors and preferences of individuals over these profiles.  \nSuch an approach could also explain why experimenter intuitions about domain structure \nmay not fully predict observed correlations in behavior. Some behaviors could share attributes or \nreasons with behaviors across multiple domains, explaining why those behaviors do not cluster \nstrongly with other behaviors in their own domain. If this is the case then we would also expect \nbehaviors to covary systematically within domain, which should be predictable by the similarity \nSOURCES OF VARIANCE IN DECISION MAKING 11 \n \nof their attribute vectors. For the same reason, such an approach could explain why some \ndemographic or psychographic variables are highly predictive of risk taking, why others are not, \nand why demographic and psychographic effects may hold for some domains but not others.  \nA final benefit of decision models is their ability to describe out-of-sample behavior. \nSince decision models describe an individual’s preferences using decision weights, it is possible \nto infer these weights from ratings of one set of items, and apply them to a new set of items to \npredict how that individual will rate or evaluate the new items. This capability of decision \nmodels --grounded in the theoretical structures they use to describe item-level and individual-\nlevel variance-- makes them especially valuable for applications to practical problems such as \nthose involved in market research, managerial decision making, financial analysis, and public \npolicy (see e.g. Green & Srinivasan, 1990; Howard, 1988; Markovitz, 1959).  \nIntegrating Psychometrics and Decision Modeling \nAlthough decision models provide an elegant way of conceptualizing item and \nindividual-level differences in decision making, and potentially explaining nuances in empirical \ndata, these models are not easy to apply to common real world behaviors, such as those measured \nusing DOSPERT. Doing so is challenging as currently there is no way to specify the rich space \nof attributes and reasons that characterize common behaviors. What are the xis that describe \neveryday risks like investing in a speculative stock, mountain climbing, eating high cholesterol \nfoods, shoplifting, and trusting a stranger with personal information? Without solving this \npractical problem, decision models remain largely restricted to artificial laboratory settings and \nhighly stylized theoretical applications.  \nIt is worth noting that many applications of DOSPERT-type inventories do ask people to \nrate their perceptions of the expected costs and benefits of various behaviors, and use these \nSOURCES OF VARIANCE IN DECISION MAKING 12 \n \nratings (often in linear models) to predict ratings of behavioral propensities and differences \nacross participants (see e.g. Weber et al., 2002 for an example). Although such applications can \nbe considered simple decision models, they are unable to predict the reasons that cause people to \nperceive some behaviors as having high or low costs and benefits, and why different people may \nvary in these perceptions. For this reason, such approaches are inherently limited for out-of-\nsample prediction, as projecting unseen items onto attributes like costs and benefits requires \nobtaining participant ratings on those attributes.  \n It is also possible to think of the factor structure revealed by psychometric analysis as \nquantifying the attributes at play in decision models. In the case of DOSPERT this would involve \nrepresenting each risky behavior in terms of the five of six dimensions that capture the variance \nin ratings data. This is, in fact, the dominant approach in the closely related problem of risk \nperception. In pioneering work, Fischhoff et al. (1978) elicited human ratings for a set of nine \nexperimenter-generated reasons and used these ratings to explore what causes people to see an \nactivity or technology as risky (see also Slovic, 1987). They found that people’s ratings clustered \ninto two factors, one capturing the familiarity of risks and the knowability of their consequences, \nand the other capturing the degree of dread and the potential for fatality of their consequences.  \nAlthough risk perception is closely related to risky decision making, it may not be \npossible to use the psychometric approach for the latter task as risky decisions involve a complex \nset of social, legal, financial, moral, and emotional variables, that would be hard to capture using \na small set of experimenter-generated reasons. As an example, consider the following two \nDOSPERT items: leaving a child alone at home and pirating software. Although both have legal \nrepercussions, the former also includes concerns about the child’s safety, the desire to make the \nchild independent, and the parents’ need to work and do errands, whereas the latter includes \nSOURCES OF VARIANCE IN DECISION MAKING 13 \n \nconcerns about software quality and security, factors involving convenience and affordability, as \nwell as a desire to support open source products. Each of these reasons reflects several nuanced \nconsiderations, involving both the decision maker and other stakeholders in the decision, and \nmany of these considerations are highly specific to the behavior itself. Although some of these \nreasons could be generated by a theorist or experimenter (as Fischhoff et al. did for risk \nperception), it would be impossible to specify the thousands of possible reasons that could play a \nrole in any possible behavior, and to moreover to get human participants to rate these reasons to \nderive quantitative representations for the behaviors with psychometric techniques. Additionally, \nas discussed above, any psychometric method for measuring attribute structure would be \nfundamentally handicapped in making out-of-sample predictions, as it relies on the ratings of the \nbehaviors themselves to specify their underlying attributes.  \nOne promising new approach to examining the reasons underlying everyday risk uses \nopen-ended natural language listings of reasons. For example, Steiner et al. (2021) examined \nmental representations of risk using a thought generation method in which participants were \nprompted to list all reasons crossing their minds while evaluating themselves on a general risk \ntaking measure. Similarly, Arslan et al. (2020) asked participants to list specific events, \nbehaviors, or situations they considered when rating their own risk taking tendencies. These \nresponses were then manually coded and quantified by either the participants themselves (Steiner \net al.) or separate coders (Arslan et al.). Both papers found that the coded self-reports predicted \nrisk-taking propensity and provided insight into core dimensions influencing people’s \nassessments of their own riskiness. \nWhile insightful, these two approaches do not fully solve the key challenge of building \npredictive decision models for established psychometric inventories like DOSPERT, which is the \nSOURCES OF VARIANCE IN DECISION MAKING 14 \n \nprimary goal of this paper. First, they examine only general risk-taking tendencies and do not \nexplain variability across specific items and domains of risky decision making. Second, they rely \non participants to generate reasons themselves and on manual human coding of listed reasons, \nwhich precludes out-of-sample prediction. Thirdly, human coding yields relatively impoverished \nrepresentations unable to capture rich diversity in participant-generated reasons, and thus does \nenable fully-fledged quantitative modeling. Overall, the techniques in Steiner et al. and Arslan et \nal. correlate self-reports with behavior, which facilitates high-level psychological interpretation, \nbut does not provide us with a complete computational framework for predicting specific risky \ndecisions taken from different domains.  \nNonetheless, these self-report approaches represent an important first step by \ndemonstrating the utility of open-ended natural language rationales and reasons for \nunderstanding risk taking. If aspects could be automatically generated for wider item arrays like \nDOSPERT, and their nuances quantitatively encoded without extensive human effort, then it \ncould be possible to build types of predictive models necessary for capturing the inter-item and \ninter-individual correlations in risk taking currently analyzed using psychometric techniques.  \nLarge Language Models \nThis final barrier can be overcome with large language models. There has been a growth \nof digitized linguistic data over the past few years, as human discourse increasingly takes place \non the internet. Researchers have begun training machine learning models on this data to derive \nrepresentations for words and sentences. These methods all work using the following intuition: \nwords and sentences that appear in similar contexts have similar meanings, and can thus be given \nsimilar representations. The representations take the form of neural network connection weights, \nwhich are multidimensional vectors (often known as embeddings). In this way, nearly any \nSOURCES OF VARIANCE IN DECISION MAKING 15 \n \nconcept or construct can be vectorized as long as it is describable using words and sentences \n(Brown et al., 2020; Devlin et al., 2019; Mikolov et al., 2013; Reimers et al., 2019) \nBuilding on this approach, researchers have begun using LLMs to specify representations \nin psychological applications. These applications include models of similarity judgment (e.g. \nLandauer & Dumais, 1997; Mandera et al., 2017), memory retrieval (Hills et al., 2012; Richie et \nal., 2023), implicit bias (Bhatia & Walasek, 2023; Caliskan et al., 2017), semantic cognition \n(Bhatia & Richie, 2023; Lu et al., 2019), and, most relevant to this paper, models of human \njudgment and decision making (see e.g. Bhatia et al., 2019 and Bhatia & Aka, 2022 for reviews). \nFor example, in Bhatia (2019), Gandhi et al. (2022), and Bhatia et al. (2022), we have used \nvector representations for words and short phrases to predict perceptions of the riskiness of \ntechnologies and activities, healthiness of foods, and leadership qualities of individuals (see also \nRichie et al., 2019). Although the judgment targets in these papers are simple one or two-word \nobjects (like nuclear power, orange juice, and Nelson Mandela respectively) and these papers \nhave not attempted to systematically test the models on their ability to describe item or \nindividual-level differences, these successful applications nonetheless illustrate the promise of \nLLM-derived representations for judgment prediction. In other relevant work, Zhao et al. (2022) \nhave used sentence vector representations to model the memory processes at play in the retrieval \nof reasons in a small set of everyday decisions. Relatedly, Aka & Bhatia (2022) have used \nsentence vector representations to predict how people evaluate descriptions of common diseases \nand health states. Again, even though this work has not examined item and individual-level \ndifferences in decisions, it shows the applicability of sentence vector models for quantifying the \ncomplex attributes and considerations at play during naturalistic deliberation.  \nSOURCES OF VARIANCE IN DECISION MAKING 16 \n \nFinally, and perhaps most importantly, Singh et al. (2022) attempted to use sentence \nvector representations for common behavior phrases, obtained from large language models, to \npredict people’s self-reported propensity to engage in the behavior corresponding to the phrases. \nSingh et al. showed that behavioral propensities can be predicted quite accurately on the \naggregate level, though they had difficulty modeling the effect of demographic (e.g. gender) and \npsychographic (e.g. personality) variables on these behavioral propensities. This difficulty could \nbe due to two reasons. First, Singh et al. used a highly flexible neural network fit concurrently on \nall participants’ data. This model took, as input, both the vector representation of the behavior \nand the vector representation of the demographic and psychographic profile of the individual \nrating the behavior, and attempted to model an individual’s responses by interacting the two sets \nof representations with each other. This modeling approach may not be well suited to analyzing \nindividual heterogeneity as high-dimensional interaction effects are usually quite difficult to fit. \nTo avoid this limitation, we will build decision models instead of flexible neural networks, and \nfurthermore fit our models separately for each individual in our dataset. Subsequently we will try \nto understand variability across individuals in terms of the attribute weights revealed by their \nrespective model fits. Another reason for Singh et al.’s failures could be the behaviors used in \ntheir analysis. Singh et al. extracted thousands of real-world behaviors by identifying the most \ncommon verb phrases in natural language corpora. These behaviors corresponded to common \nacts that people write and think about, such as save a document and finish high school. By \ncontrast we will be using items from existing surveys, such as DOSPERT. Survey items –having \nbeen developed using a combination of theoretical knowledge and empirical data—may better \nreflect the dimensions of variance in human behavior. Note that in addition to these two major \nSOURCES OF VARIANCE IN DECISION MAKING 17 \n \nmodifications, the main focus of this paper is on predicting and interpreting the structure of item \nand individual-level variability in the data, which was not examined in Singh et al.’s initial work.  \nAll of the methods discussed in this section use vector representations of words and \nsentences obtained from connection weights in deep neural networks trained to generate high-\nquality word and sentence representations. This is not the only way to use large language models \nto extract attribute representations for objects, concepts, and behaviors. A complementary \napproach is to explicitly ask generative language models to list reasons for or against engaging in \na given behavior. Generative language models, like GPT (Brown et al., 2020), are a special type \nof LLM that has been optimized for producing language outputs. Although these models also \nrepresent words and sentences as vectors in deep neural network layers, their ability to generate \nlinguistic output can be used to give these high-dimensional representations human-interpretable \nlinguistic labels. Building off the pioneering work of Arslan et al. (2020), Steiner et al. (2021), \nand others, we will use generative LLMs to automatically extract reasons for and against several \nreal-world behaviors, which we will transform into multidimensional vector representations \namenable to decision modeling. We will then use these to both predict and interpret the main \ndimensions of behavioral variability in our datasets. \n \nComputational Methods \nAlgorithmically Derived Reasons \nThe behaviors used in our experiments were taken from the DOSPERT inventory or an \nextended version of this inventory (described in detail below). All items were in the form of \nshort natural language phrases. We used four methods to build quantitative multi-attribute \nrepresentations for these phrases. The first two of these methods relied only on the behavior \nSOURCES OF VARIANCE IN DECISION MAKING 18 \n \nphrase (e.g. investing in a speculative stock), whereas the second two of these methods relied on \nmachine-generated reasons for and against each behavior (e.g. speculative stocks offer the \npotential for greater returns compared to low-risk investments and speculative stocks carry a \ngreater risk than most other investments, due to their unpredictable nature and potential for \nrapid declines in value). We generated these reasons by querying GPT-3.5 (text-davinci-003) \nwith the following prompt: “What are reasons for or against [BEHAVIOR PHRASE]? List five \nreasons for each, and number them.”. We used the OpenAI API instead of ChatGPT since the \nAPI made it possible to algorithmically run and record the responses for a large set of queries. \nThe API also has fewer guardrails, and is willing to give controversial responses, like reasons in \nfavor of drunk driving or consuming heroin (ChatGPT, by contrast, refused to generate \npotentially unethical advice). \nOverall, we were quite surprised by the quality of the reasons generated by GPT. The \nmodel was able to give clear and comprehensive reasons for all behaviors, and was even able to \ngenerate insightful reasons that we had not thought of. We used GPT-generated reasons to \npredict behavior in Study 1. In Study 2, we directly compared GPT’s reasons with those \ngenerated by humans, and also used these reasons to interpret the main sources of item and \nindividual-level variability. GPT-generated reasons for the items are available in the OSF \nrepository for this project (https://osf.io/3y6ku/).  \nVectorizing Behaviors \nWe also considered two different techniques for quantifying the texts (the language used \nto describe the behavior or the GPT-generated reasons for and against the behavior) discussed in \nthe prior section. The first of these was a bag-of-words Word2Vec model. In this model, the text \nwas first preprocessed by tokenizing it into individual words and removing any stop words and \nSOURCES OF VARIANCE IN DECISION MAKING 19 \n \npunctuation. In this way the text was simplified into only the set (“bag”) of its component words. \nThen, each word was assigned a 300-dimensional vector representation obtained using the \npopular Word2Vec model (Mikolov et al., 2013). Word2Vec is a neural network that uses the co-\noccurrence statistics of words in large language corpora (books and news articles) to derive \nrepresentations that capture semantic relationships between words. Similar words are given \nsimilar representations. Although Word2Vec’s representations are only for words, the bag-of-\nwords technique generates a vector representation, xi, for behavior i, by averaging the individual \nword vectors in the behavior phrase or the set of GPT-generated reasons for that behavior. \nOur second technique for vectorizing behavior relied on the Sentence-BERT (SBERT) \nmodel (Reimers et al., 2019). Unlike the prior method, which treats sentences as bags of words, \nSBERT considers the contextual and syntactically constrained meaning of words within a \nsentence. It utilizes a neural network architecture that fine-tunes pretrained transformer models \nlike BERT (Bidirectional Encoder Representations from Transformers) on a sentence similarity \ntask. By training on large amounts of data, SBERT learns to encode sentences into vectors that \ncapture their meaning. These vectors enable efficient computation of sentence similarities and \nhave been shown to be successful for various downstream natural language processing tasks. The \nSBERT model used in the current paper was based on the RoBERTa model (Devlin et al., 2018; \nLiu et al., 2019), which was trained on a large corpora of online text data. We queried this model \nusing huggingface’s API (sentence-transformers/all-roberta-large-v1). Recall that we applied this \nmodel both to textual descriptions of the behavior and to GPT-generated reasons for and against \nthe behavior. In the former case we just passed the item’s text (e.g. investing in a speculative \nstock) through the SBERT model. In the latter case, we first parsed GPT’s textual output into a \nlist of ten reasons (five for and five against). Each of these reasons (e.g. speculative stocks carry \nSOURCES OF VARIANCE IN DECISION MAKING 20 \n \na greater risk than most other investments …) was passed through SBERT and the vectors for \neach of the ten reasons were averaged. Both approaches generated 1,024 dimensional \nrepresentations for each behavior, which we write as xi for behavior i.  \nOverall, the set of models considered in this paper has a 2x2 factorial structure, which \nvaries the linguistic depiction of the behavior (the item’s behavior phrase or GPT-generated \nreasons for the behavior) and the technique for vectorizing this linguistic depiction (bag-of-\nwords Word2Vec or SBERT). We thus refer to these four models as Word2Vec-Items, \nWord2Vec-Reasons, SBERT-Items and SBERT-Reasons respectively. Figure 1A summarizes \nthe four LLM methods used in our analysis pipeline.  \nIt is worth noting that using LLMs to generate reasons for behaviors and then quantifying \nthese behaviors with other LLMs, as in the SBERT-Reasons model, has, from a statistical \nperspective, a certain amount of redundancy, since it is merely doing a transformation in LLM’s \nvector space without adding any additional information. Thus we would not expect SBERT-\nReasons and Word2Vec-Reasons to have higher predictive accuracy than SBERT-Items and \nWord2Vec-Items respectively. Despite this, we included all four models in our analysis as GPT-\ngenerated reasons are useful for interpreting the sources of variance modeled using our \nframework, and additionally allow us to understand the processes that individuals go through as \nthey deliberate. This is one of the major goals of the paper.   \nFitting Decision Models \nOur goal was to use the vectors xi, obtained from the four methods described in the \nprevious section, as inputs into decision models. Although the dimensions of these vectors may \nnot have direct interpretability, similar behaviors nonetheless possess similar vectors and have \nsimilar values on each of the (300 for Word2Vec or 1,024 for SBERT) dimensions. Additionally, \nSOURCES OF VARIANCE IN DECISION MAKING 21 \n \npeople’s preferences for these behaviors can be understood in terms of the weights, wj, that they \nassign to the dimensions. Consequently, a decision model can be constructed to predict the rating \nfor behavior i for individual j, using the following equation: rij ~ wj ∙ xi. In this equation, the \nweights correspond to the preferences individuals have for each dimension, while the dimensions \nrepresent the characteristics or aspects of the behavior captured by the vectors. By assigning \nappropriate weights to the dimensions, the decision model can effectively predict or evaluate \nbehaviors based on the given vector representations.  \nTo determine each individual’s weights in the decision model, we employed a ridge \nregression modeling technique. This is a regularized regression method that aims to minimize the \nsum of squared errors in a linear model while incorporating a penalty term. This penalty term \nrestricts the coefficients of the weights, promoting a balance between accuracy and simplicity in \nthe model. Ridge regression helps mitigate issues such as multicollinearity and overfitting, which \nare potential issues in settings with high-dimensional predictors (as in our paper). For this reason, \nit has performed particularly well in applications that use text vectors to predict human responses \n(e.g. Richie et al., 2019). We implemented the ridge regression using the sci-kit learn machine \nlearning library (Pedregosa et al., 2011), with regularization penalties, a, in the set [0.001, 0.01, \n0.1, 1, 10].  \nWe evaluated our models using leave-one-out cross-validation (LOOCV). LOOCV \nmeasures the performance of a model by leaving out one data point from the training set and \nusing it as the test set. The process is repeated for each data point, ensuring that every instance is \nused as both training and test data. LOOCV provides an unbiased estimate of the model's \nperformance as it evaluates how well the model generalizes to unseen data. We applied LOOCV \nto each individual separately. Thus, for each individual, we fit a decision model to all except for \nSOURCES OF VARIANCE IN DECISION MAKING 22 \n \none of their ratings, and evaluated the model on the held-out rating. We repeated this process for \neach rating given by each individual. This technique is illustrated in Figure 1B. \nAs part of our tests, we calculated how well similarities between xis or between wjs \npredicted correlations between behaviors and between individuals respectively. We used cosine \nsimilarity to measure vector similarity. This is defined as the cosine of the angle of two vectors. \nSpecifically, according to cosine similarity, SIM(a,b) = (a∙b)/(||a||∙||b||). Cosine similarity is equal \nto +1 for two vectors that are in the same direction, -1 for two vectors that are in the opposite \ndirection, and 0 for two vectors that are orthogonal to each other.  \nAdditional Tests \nTo better understand the properties of our modeling approach we also considered six \nadditional model-based tests. The first of these used a random vector model. This model replaced \nthe vectors xi in the above pipeline with randomly generated numbers, each sampled from a \nstandard normal distribution. The dimensionality of these random vectors was the same as \nSBERT’s (1024) dimensionality, allowing us to test whether our model’s performance is simply \ndue to the flexibility inherent in high-dimensional predictors.  \nThe second test used a DOSPERT labels model, which replaced the vectors xi in the \nabove pipeline with five-dimensional binary vectors indicating the experimenter specified \ndomain classification for the items. Thus, for example, an item that was considered (according to \nthe experimenter) to be in the financial domain was given a vector [1,0,0,0,0] whereas a model \nthat was considered to be in the recreational domain was given a vector [0,1,0,0,0]. Although this \nmodel can be seen as a weighted additive multi-attribute decision model, with best-fitting \nweights corresponding to the participant’s weighting of the five DOSPERT domains, it also has \nan alternative, simpler interpretation: At its core, this model simply predicts the rating of an out-\nSOURCES OF VARIANCE IN DECISION MAKING 23 \n \nof-sample target item by averaging the participant’s ratings for items in the training data that \nshare the target item’s domain. Thus, an out-of-sample item from the financial domain will be \ngiven roughly the average rating of financial items that the model was trained on. Overall, the \nDOSPERT labels model serves as a benchmark for our approach as it evaluates the predictive \npower inherent in the intuitively derived taxonomy of the DOSPERT inventory. \nWhile the previous two tests replace the predictors used in the analysis pipeline, the third \ntest replaces the training items. In particular, unlike our main analysis which uses leave-one-out \ncross validation on our full dataset of items, the original DOSPERT items test trains our model \nonly on the ratings for original DOSPERT items, and then tests it on ratings for extended \nDOSPERT items (see details of the DOSPERT extension below). This analysis helps us evaluate \nthe sensitivity of our model performance to dataset size and content (the original DOSPERT \ninventory has far fewer, and somewhat dated, items than our extended inventory). Our final three \ntests are variants of the DOSPERT items test that randomly sample either 25%, 50%, or 75% of \nthe items in our extended DOSPERT inventory for training purposes. As with the DOSPERT \nitems test, fitting our model subsets of the extended DOSPERT inventory and testing it on the \nheld-out items help us evaluate the dependence of our modeling pipeline on dataset size.  \nInterpreting Reasons. The above pipeline is formulated for out-of-sample prediction but \ndoes not help us understand the higher-level constructs that drive variability in ratings for items \nacross individuals. Thus, in addition to using the above LLM representations of reasons to \npredict ratings, we also used LLM representations to code the items on 18 different interpretable \nthemes. We generated these themes separately for each of the five DOSPERT domains. Overall, \nwe came up with between three and five themes that characterized the different types of items \nand attributes in each domain. Then for each of the themes we generated ten different associated \nSOURCES OF VARIANCE IN DECISION MAKING 24 \n \nwords in order to specify a lexicon or dictionary for the theme (e.g. Tausczik & Pennebaker, \n2010) (see Table 1). Subsequently, we obtained the Word2Vec vectors for each of the words in \nthe theme’s lexicon, and averaged these vectors to obtain a single vector representation for the \ntheme. Finally, we measured the extent to which each theme occured in the reasons for each item \nby calculating the cosine similarity of the theme vectors to the Word2Vec-Reasons vector \nrepresentations for the items (these, as discussed above, are average of the Word2Vec vectors for \nthe individual words in the reasons for the items). Recall that we write the vector for item i as xi. \nThus the degree to which theme k is present in item i is simply SIM(xi,tk) = (xi∙tk)/(||xi||∙||tk||), \nwhere tk is the theme vector for a theme k. Intuitively, this similarity measure captures the extent \nto which the words used in a given theme’s lexicon are semantically similar to the words in a \ngiven item’s GPT-generated reasons. Our approach is based on the distributed dictionary \nmethod, which has been shown to be particularly useful for coding small amounts of text on \ncomplex psychological properties (see Garten et al., 2018 for details).  \n \nStudy 1 \nThe goal of Study 1 was to evaluate the utility of LLM-based decision models for \ncapturing item-level and individual-level variability in risky decision making. For this reason, it \nadministered an expanded version of the DOSPERT scale to a sample of US participants, and \nattempted to fit individual-level decision models to the observed data. We evaluated the accuracy \nof various LLM-derived behavior representations in predicting out-of-sample participant \nresponses. Importantly, we also tested how well similarity in LLM representations predicted \nobserved correlations across items and domains, as well as how well similarity in best fit \nattribute weights predicted correlations across individuals and groups.  \nSOURCES OF VARIANCE IN DECISION MAKING 25 \n \nMethods \nParticipants. We recruited 150 participants (79 females, 67 males, 4 other) from Prolific \nAcademic, an online platform for conducting research with human subjects. The participants \nwere aged between 18 and 72 years (mean = 37.64, SD = 13.69), and were US citizens fluent in \nEnglish. All participants gave informed consent before taking part in the study. The study was \napproved by the UPenn IRB (#823184) and was preregistered at https://osf.io/amves/. All code \nand data is available at https://osf.io/3y6ku/. \nProcedure. The participants were asked to rate their relative likelihood of engaging in \n150 behaviors on a scale of 0 (much less likely than others) to 100 (much more likely than \nothers). These behaviors were taken from the extended DOSPERT inventory, detailed below. \nThe 150 behaviors were presented in a random order on the same screen, and the participants \nused a slider to indicate their ratings. The slider had a default start position of 50, which meant \nthat the participants were equally likely as others to engage in the behavior. The participants \ncould change their ratings as many times as they wanted before submitting their responses. \nEmbedded in the set of items was an attention check item which asked participants to respond \nwith a rating of 26. The five participants who failed the attention check were eliminated from the \nstudy, resulting in a final sample size of 145. Note that we had forgotten to preregister the \nattention check and repeating our analysis without this exclusion criteria results in no change to \nthe results.  \nAfter rating all the behaviors, the participants completed a demographic questionnaire \nthat asked about their age, gender, education level, and occupation. They also completed a Big \nFive personality inventory that measured their levels of extraversion, agreeableness, \nconscientiousness, neuroticism, and openness to experience. The inventory consisted of 10 items \nSOURCES OF VARIANCE IN DECISION MAKING 26 \n \nthat were rated on a 5-point Likert scale from 1 (strongly disagree) to 5 (strongly agree) \n(Rammstedt & John, 2007) \nMaterials. The 150 items used in our study were taken from the extended DOSPERT \ninventory. As with the original DOSPERT inventory, the extended DOSPERT inventory covered \nfive established domains of risky choice: financial, ethical, social, health, and recreational. The \npool of behaviors in this inventory was generated by merging and expanding the original \nDOSPERT-30 and DOSPERT-40 inventories (Weber et al., 2002; Blais & Weber, 2006), so that \neach of the five domains of DOSPERT had a total of 30 behaviors each. Out of the 150 items in \nour extended inventory, 44 were taken from DOSPERT-30 and DOSPERT-40 whereas the \nremaining were new items.  \nWhile extending the original DOSPERT inventory, we took into consideration several \nfactors. First, we included several contemporary risks that were not present in the original \nDOSPERT inventory (e.g. investing in cryptocurrency). Secondly, we attempted to add items \nthat maximized the diversity of the types of behaviors in each domain. For this reason, we did \nnot closely replicate existing items, rather we tried to include items and themes that were \nexcluded from the original inventory (examples of new and distinct items are speaking up \nagainst injustice at your workplace, and spending a large amount of time playing video games). \nFinally, we removed nuanced quantitative details from DOSPERT items (e.g. investing 5% of \nyour annual income in a very speculative stock was replaced with investing in a speculative \nstock) and removed items that were very similar to and considered safer alternatives to other \nitems in the inventory (e.g. instead of including both investing in a speculative stock and \ninvesting in a moderate growth diversified fund, which were both present in the original \nDOSPERT inventory, our extended inventory used only the former item). We preregistered the \nSOURCES OF VARIANCE IN DECISION MAKING 27 \n \nbehaviors before launching the study, and the full set of behaviors is provided in the OSF \nrepository for this project.  \nResults \nPredictive Accuracy. First, we wanted to assess the effectiveness of our approach in \npredicting participant ratings. Recall that we used four different models for obtaining \nrepresentations. These models had a 2x2 factor structure based on whether they used the item’s \nbehavior phrases or GPT-generated reasons for the items, as well as whether they used the bag-\nof-words Word2Vec model or the SBERT model for quantifying the texts. Each model's \nrepresentations were passed through a ridge regression, which allowed for one of five flexible \nregularization penalties. To evaluate the models, we employed leave-one-out cross-validation \n(LOOCV) on a participant level. This involved calculating the model's out-of-sample prediction \nfor each rating obtained from each participant and comparing it with the observed data.  \nFigure 2 illustrates the accuracy rates of these predictions. In Figure 2A, we present the \naverage correlation between predicted and observed ratings across all participants. Figure 2B \npresents the average mean squared error (MSE) between each model's out-of-sample predictions \nand the observed ratings. Both figures show that SBERT-Items was the best performing model, \nand that this model performed the best with a ridge penalty value of a = 1 (the default parameter \nin the scikit-learn library). Figure 2C shows the distribution of participant-level out-of-sample \ncorrelations for this model. Here we can see that SBERT-Items achieved positive correlations for \nall except for one participant in the dataset. The average correlation across participants is 0.43, \nwhich is statistically greater than 0 (t(144) = 43.97, p < 0.001, 95% CI = [0.41, 0.45]).  \nAdditional Tests. We also conducted a number of additional tests to evaluate the \npredictive accuracy rates of our model. The results of these tests are summarized in Figure 2D. \nSOURCES OF VARIANCE IN DECISION MAKING 28 \n \nOur first test used a 1024-dimensional random vector model which randomly sampled attribute \nrepresentations from a standard normal distribution. We found that this model performed very \npoorly, achieving statistically zero correlation in out-of-sample tests (average r across \nparticipants = -0.001, t(144) = -0.16, p = 0.87, 95% CI = [-0.02,0.01]). This shows that it is not \nthe dimensionality of the LLM vectors but their resemblance to human semantic representations \nthat matters during prediction. \nOur second test used DOSPERT labels to generate binary attribute vectors that described \nthe domains the items belonged to. Intuitively this model predicts a participant’s rating of an out-\nof-sample item using their ratings on other items in the same (experimenter-specified) \nDOSPERT domain. We trained this model on each participant's responses with a standard linear \nregression, and tested it in out-of-sample data using leave-one-out cross-validation. This model \nperformed decently, achieving an out-of-sample correlation of r = 0.30. Although this is \nsignificantly different to zero (t(144) = 24.32, p < 0.001, 95% CI = [0.27, 0.32]), it is still \nconsiderably worse than our main model, showing that there is meaningful psychological nuance \nwithin each domain that LLM representations (but not experimenter intuition) can capture. \nWe also assessed the predictive ability of the original DOSPERT items for the extended \nDOSPERT items. To do this, we replicated the above analysis with a slight modification: We \ntrained the best performing model, SBERT-Items (with ridge penalty a = 1), on each participant's \nresponses to the 44 existing DOSPERT items in our scale. We then calculated its predictive \naccuracy, measured in terms of out-of-sample correlation, on the 106 new items in our scale. We \nfound that the average correlation between the predicted responses and the observed responses to \nthe new items was 0.30. This is a significant deviation from zero (t(144) = 30.06, p < 0.001, 95% \nSOURCES OF VARIANCE IN DECISION MAKING 29 \n \nCI = [0.28, 0.32]) and shows that our method applied to the original DOSPERT scale can \naccurately predict responses to new behaviors.  \nOur final three tests used a variant of the original DOSPERT items test. In particular, \nthese three tests randomly sampled either 25%, 50%, or 75% of the extended DOSPERT \ninventory, and used a participant’s ratings of these items to predict their rating of a held-out item \nwith the SBERT-Items model (with ridge penalty a = 1).  We found that the average correlation \nbetween the predicted responses and the observed responses was 0.27 (t(144) = 28.10, p < 0.001, \n95% CI = [0.25,0.29]), 0.36 (t(144) = 36.17, p < 0.001, 95% CI = [0.34,0.38]), and 0.40 (t(144) = \n39.82, p < 0.001, 95% CI = [0.38,0.42]) for the three models respectively. These successful \npredictions show that our approach is able to achieve good performance even when using \ndrastically less data. For example, the model that used only 25% of the data did only slightly \nworse than the DOSPERT labels model in the second test, demonstrating that slightly more than \na quarter of the data is needed to replicate prior approaches that rely on experimenter intuition. \nThis analysis should address any reservations about undue dependence on dataset size. \nItem Variability. One of the key reasons for the high accuracy rates observed in our \nstudy is the rich representational structure embedded within our vectors. These vectors have the \nability to capture item similarity, meaning that behaviors that are given similar ratings by \nparticipants are assigned similar attribute vectors by our LLMs. To formally test this explanation, \nwe measured the average cosine similarity between pairs of item vectors within each domain for \nour best performing model, SBERT-Items. We also measured the average cosine similarity \nbetween all items in our inventory. The results of this analysis are in Figure 3A, which shows \nthat the pairwise cosine similarity between items within a domain is significantly higher than the \npairwise cosine similarity between items across the full inventory. In other words, the SBERT-\nSOURCES OF VARIANCE IN DECISION MAKING 30 \n \nItems model is able to capture the domain structure of the DOSPERT inventory by projecting \nitems from each domain onto separate regions of its representational space.  \nTo provide a visual depiction of these patterns, we generated a scatter plot using the first \ntwo principal components (PCA) dimensions of the SBERT-Items model’s vectors. This position \nof items on these two components is shown in Figure 3B. This plot shows domain coherence, \nwith items from each domain having a high degree of proximity to other items in their respective \ndomains. The scatter plot also reveals a graded structure, indicating continuity in the underlying \nrepresentations. For example, the original DOSPERT’s health risks, not wearing a seatbelt and \nwalking alone at night in an unsafe area of town are close to recreational and social risks \nrespectively, whereas other health risks from the original DOSPERT inventory, like regularly \neating high cholesterol foods, are not. This graded structure indicates that our vector \nrepresentations may capture nuanced variations in similarity that go beyond discrete domain \ncategories. \nTo further assess our item vectors' representational quality, we compared the pairwise \ncosine similarity between the 11,175 unique item pairs in the extended DOSPERT inventory to \nthe human correlations observed in the ratings of those items, for our best performing model. \nFormally, we tested whether COR(rbi, rbi’) ~ SIM(xi, xi’), where xi is item i’s 1,024-dimensional \nSBERT-Items vector, rbi is the 145-dimensional vector of ratings given to item i by our 145 \nparticipants, and i and i' are indices in range [1,150] with i ≠ i'.  We found that the correlation \nbetween SIM(xi, xi’) and COR(rbi, rbi’) across all pairs is 0.27, which is significantly different to \nzero (p < 0.001, 95% CI = [0.25, 0.28]). This is illustrated in Figure 3C. For visual clarity, we \ndivided the 11,175 item pairs into twenty equally-sized bins, categorized based on the percentile \nranking of their human correlation values. The figure displays the average cosine similarity of \nSOURCES OF VARIANCE IN DECISION MAKING 31 \n \nitems pairs within each bin. The findings depicted in Figure 3C reveal a positive relationship \nbetween the human correlations between items and the cosine similarities of items. Bins that \ncontain item pairs with higher human correlations also exhibit higher cosine similarities between \nthose items’ vectors. Additionally, this relationship has a convex trend, indicating that the bins \nwith the highest human correlation values have proportionally higher vector similarities.  \n Finally, to assess whether our item vectors effectively capture human ratings beyond \ndiscrete experimenter-generated domains, we repeated the analysis depicted in Figure 3C within \neach of the five domains. For instance, we calculated the pairwise correlations among all \nrecreational risks and compared these with the cosine similarities of the SBERT-Items vectors \nfor recreational risks. If human behavior were solely predicted by domain overlap, we would not \nexpect any effect here. This is because there would be no systematic correlational structure \nwithin each domain to explain. However, if there is graded structure that extends beyond discrete \ndomains, and if our vectors successfully capture this structure, we would expect to observe \npositive correlations. Figure 3D shows that we obtained significantly positive correlations for all \ndomains. For example, drinking sugary drinks and eating processed foods received similar \nratings across individuals (rating correlation of 0.60) and were also given similar vectors by our \nmodel (cosine similarity of 0.67). By contrast, not brushing regularly and using a UV tanning \nmachine received dissimilar ratings across individuals (ratings correlation of 0.02) and also were \ngiven dissimilar vectors by our model (cosine similarity of 0.05). This is despite the fact that \nthese four behaviors are all taken from the health risk domain. Overall, these findings provide \nstrong evidence that there are underlying structures that extend beyond the boundaries of discrete \ndomains and that our LLM-derived vectors are, moreover, able to capture these structures.  \nSOURCES OF VARIANCE IN DECISION MAKING 32 \n \nIndividual Variability. Next, we investigated the extent to which our approach could \naccount for the structure of variability between individuals. Decision models describe people's \npreferences using attribute weights. Consequently, if our approach effectively captures individual \ndifferences, we would expect that similarities in attribute weights among pairs of individuals \npredict the degree to which the individuals engage in similar behaviors. To examine this, we \nestimated attribute weights for each individual by fitting our best performing model, SBERT-\nItems (with ridge penalty a = 1) on their complete ratings dataset. Then we compared the cosine \nsimilarity of attribute weights with the correlation between people's ratings for each of the \n10,440 unique pairs of participants. In other words, we tested whether COR(rpj, rpj’) ~ SIM(wj, \nwj’), where wj is individual j’s 1,024-dimensional attribute weight vector, rpj is their 150-\ndimensional vector of ratings across all items, and j and j’ are in range [1,145] with j ≠ j’.  We \nfound that the correlation between all pairs is 0.98 (p < 0.001, 95% CI = [0.98, 0.98]), showing \nthat our approach captures the structure of variability between individuals almost perfectly. This \nis illustrated in Figure 4A. For clarity, we divided the 10,440 participant pairs into twenty \nequally-sized bins, categorized based on the percentile ranking of their human correlation values. \nThe figure displays the average cosine similarity of weight vectors within each bin. The findings \ndepicted in Figure 4A reveal a clean positive relationship between participant correlations and \nthe cosine similarities of their weight vectors.  \nAlthough the high correlation observed in the above paragraph would not be possible if \nthe LLM vectors did not reflect the core attributes of the items, their high-dimensionality makes \nour analysis vulnerable to criticisms of over fitting. Thus, we performed a variant of the above \ntest, in which we used weights recovered from half the items to predict between-individual \ncorrelations in responses on the other half of the items. In particular, we split the items into two \nSOURCES OF VARIANCE IN DECISION MAKING 33 \n \nportions randomly, obtained weights wj from the first portion and ratings rpj from the second \nportion, and then tested whether COR(rpj, rpj’) ~ SIM(wj, wj’) for pairs of participants. We \nrepeated these analysis 100 times to get a good estimate of the average correlation. This was \nfound to be 0.48 (t(99) = 202.93, p < 0.001, 95% CI = [0.478,0.488]). The full distribution of \ncorrelations across the 100 random splits is shown in Figure 4B. These positive results provide \ndefinitive evidence that our decision model’s weights do not overfit participants idiosyncrasies \nbut rather capture participant heterogeneity in a robust and generalizable manner.  \nNext, we tested whether similarities and differences between different groups of \nindividuals were captured by attribute weights, as would be predicted by a decision modeling \nexplanation for individual variability. For this, we performed binary splits on our full dataset \nbased on each demographic or psychographic variable collected in our study. For example, for \ngender, we divided our sample into male and female, for age into old and young (based on the \nmedian age of 33), for political affiliation into Democrat and Republican or Independent, for \nincome into high and low earners (based on a threshold of $50,000), and for religion into \nreligious and non-religious. For each of the big five personality dimensions, we performed a \nmedian split on that dimension, dividing our sample into equally sized groups that are high or \nlow on that dimension. For each of these groupings we calculated the average within and \nbetween group pairwise correlations in the ratings of pairs of individuals. Thus, for example, we \nmeasured the correlation between every male and every other male in our dataset as well as \nbetween every female and every other female in our dataset, and aggregated these to get the \naverage within-group correlation for gender. We also measured the correlation between every \nmale and every female in our dataset, and aggregated these to get the average between-group \ncorrelation for gender. We also did this for their associated attribute weights, to get the average \nSOURCES OF VARIANCE IN DECISION MAKING 34 \n \nwithin-group and average between-group cosine similarity of weights for the grouping.  The \naveraged correlations and cosine similarities are shown in Figures 4C and 4D. If a given \ndemographic or psychographic variable predicts risk taking behavior, we would expect that \npeople’s ratings are more correlated if they belong to the same grouping on that variable than if \nthey belong to different groupings on that variable. Indeed, we see this pattern for personality \nvariables like conscientiousness and extraversion, as well demographic variables like gender.  \nImportantly, our model captures this pattern by assigning similar attribute weights to individuals \nwithin the same groupings on these variables.  \nTo better understand the nature of heterogeneity in attribute weights we further explored \ntwo of the above variables: conscientiousness and gender. Both have been implicated as \npredictors of risk taking in prior work, which has found that people who are similar in terms of \nthese variables are also similar in terms of the behaviors they are most likely to do (Harris & \nJenkins, 2006; Weber et al., 2002; Weller & Tikir, 2011). Indeed, in our own study we found that \nbehaviors like working in a high stress environment and defending an unpopular issue that you \nbelieve in at a social occasion were rated highly by high conscientiousness participants whereas \nbehaviors like not flossing teeth, not buying home insurance, and consuming marijuana were \nrated highly by low conscientiousness participants. Likewise investing in a speculative stock, \nbodybuilding, and having sex with a stranger were given much higher ratings by men than by \nwomen, whereas behaviors like eating an excessive amount of desserts and sweets, wearing \nprovocative clothing, and ending a friendship were given much higher ratings by women than by \nmen. Of course, these differences exist on the aggregate level and there is a large amount of \nvariability within these groups as well. Figures 4E and 4F visualize this variability in a scatter \nplot of individual-specific attribute weights on their first two principle components. Points are \nSOURCES OF VARIANCE IN DECISION MAKING 35 \n \ncolored based on conscientiousness and gender in the two figures respectively. Here we can see a \nfair amount of clustering, with most participants having closer decision weights to members of \ntheir own group. Roughly speaking, high (low) conscientiousness individuals occupy the bottom \nright (top left) of the plots whereas male (female) participants occupy the top right (bottom left) \nof the plots. Of course, there are also exceptional individuals who are more similar to members \nof other demographic and psychographic groups. This highlights the value of the continuous \nnature of our model’s representations – our approach is able to capture individual heterogeneity \nnot by assigning people to a small number of discrete demographic or psychographic categories, \nbut rather by capturing their complex multidimensional preferences in a graded and flexible \nmanner.  \nDiscussion \nIn this study we examined how well decision models based on large language model \nrepresentations of behaviors predict the behaviors of participants (as measured by the DOSPERT \ninventory), as well as how well they describe variability across items and across individuals. We \nfound high predictive accuracy rates, particularly for sentence vector models that consider \ncontextual and syntactical information when representing the meaning of a sentence. Crucially, \naccuracy remained high even when using just 75%, 50%, or even 25% of the items, \ndemonstrating no undue dependence on sample size. Furthermore, performance dropped to \nchance levels when LLM representations were replaced by random vectors, showing that our \nsuccess relies on a meaningful mapping of behaviors into LLM vector spaces (rather than simply \nflexible high-dimensional vector spaces). Our approach also substantially outperformed an \nexpert-coded model relying on DOSPERT labels for item domains. Taken together, these tests \nSOURCES OF VARIANCE IN DECISION MAKING 36 \n \nillustrate the predictive power of our framework, while ruling out potential limitations around \nscale and generalizability.  \n Our approach is also able to capture heterogeneity across items through its \nrepresentations – behaviors that are rated similarly by human participants (e.g. behaviors that \nbelong to the same domain) have similar attribute representations. Importantly, the graded nature \nof LLM representations allows them to capture item relationships that cannot be described by \ndiscrete experimenter-specified domains. This is why our models account for item heterogeneity \nwithin domain, for example, the fact that different health risks vary in terms of how correlated \nthey are with each other. Additionally, our models capture variability across participants, as \npeople who provide similar ratings to behaviors are given similar attribute weights. For this \nreason, it is also able to reproduce the effects of demographic and psychographic variables on \nbehavior, and account for participant heterogeneity within each grouping on our dataset. In this \nway, our results show that the graded nature of LLM representations allows them to capture \naspects of people’s preferences that cannot be described using a small set of discrete \ndemographic and psychographic variables. \nOne thing to note about the results in this section is that the best performing model was \nSBERT-Items, which used only LLM vectors for the behavior item’s text (e.g. investing in a \nspeculative stock) and not GPT-generated reasons for the behavior. This model did slightly better \nthan SBERT-Reasons, which was based on LLM vectors for GPT-generated reasons. We suspect \nthat this is the case because GPT generates reasons by itself relying on LLM representations of \nthe item’s texts. That is, the reasons that it generates do not provide any additional information \nthat is not contained in and quantified by the rich (1,024 dimensional) vector representation of \nthe item’s text. For this reason, averaging over GPT’s reasons leads to slightly worse \nSOURCES OF VARIANCE IN DECISION MAKING 37 \n \nperformance. Of course, GPT-generated reasons are useful for interpreting the themes implicit in \nthe model’s (and in people’s) representations of behaviors. What are these themes, and can they \nhelp us understand why people do or do not engage in certain behaviors? Examining this is the \ngoal of Study 2.  \n \nStudy 2 \nIn Study 2 we wished to explore the reasons and attributes implicit in the representations \nof our models. For this purpose, we ran an extension of the experiment in Study 1 with a verbal \nprotocol component (Ericsson & Simon, 1980; Schulte-Mecklenbeck et al., 2011; Weber et al., \n2007) in which we asked participants to generate reasons for or against a subset of the behaviors \nin the extended DOSPERT inventory. Our goal was to compare the reasons generated by our \nmodels to those generated by human participants, and to furthermore interpret how the reason \nrepresentations generated by our model capture item and individual heterogeneity in choice. A \nsecondary goal was to test whether the high accuracy rates observed in Study 1 replicate in a new \nsample. This study was preregistered at https://osf.io/amves and all code and data is available at \nhttps://osf.io/3y6ku/.  \nExperimental Methods \nParticipants. We recruited 150 participants (79 females, 66 males, 5 other) from Prolific \nAcademic, an online platform for conducting research with human subjects. The participants \nwere aged between 18 and 70 years (mean = 37.39, SD = 12.25), and were US citizens fluent in \nEnglish. All participants gave informed consent before taking part in the study. The study was \napproved by the UPenn IRB (#823184). \nSOURCES OF VARIANCE IN DECISION MAKING 38 \n \nMaterials and Procedure. Participants completed the extended DOSPERT scale using \nan identical interface to that in Study 1. After this, they were shown five randomly selected \nbehaviors from the extended DOSPERT inventory and asked to list the reasons they choose to \nengage or not engage in that behavior. They were shown one behavior on each screen, were \nrequired to list at least one reason for that behavior, and could list up to ten reasons in ten \nseparate text boxes shown on the screen. Participants were asked to write out their reasons in \nclear and complete sentences. After the verbal protocol task, participants completed the \ndemographic and personality questionnaires used in Study 1. Two participants failed the \nattention check embedded in the DOSPERT inventory and were removed from the study. \nResults \n Replicating Study 1. Before analyzing reasons, we attempted to replicate the main \nfindings of Study 1. As in Study 1, we found that the best performing model was SBERT-Items \n(which used vector representations of items’ texts) with a ridge regression penalty a = 1. This \nmodel achieved an average LOOCV correlation between predicted and observed ratings of 0.42 \n(SE = 0.01). This was slightly better than the performance of SBERT-Reasons, which used \nvector representations of GPT-generated reasons for items. This model achieved a LOOCV \ncorrelation of 0.39 (SE  = 0.01). \nWe also tested whether the pairwise correlation in the ratings of items was predicted by \nthe pairwise cosine similarity of their LLM vectors, that is whether COR(rbi, rbi’) ~ SIM(xi, xi’), \nwhere xi is item i’s 1,024-dimensional vector and rbi is the 148-dimensional vector of ratings \ngiven to item i by individuals.  We found that the correlation between cosine similarity and \ncorrelation pairs was 0.30 (p < 0.001, 95% CI = [0.28, 0.32]) for SBERT-Items and 0.25 for \nSBERT-Reasons (p < 0.001, 95% CI = [0.23, 0.27]). We also compared the cosine similarity of \nSOURCES OF VARIANCE IN DECISION MAKING 39 \n \nattribute weights with the correlation between people's ratings for each pair of participants, that \nis, whether COR(rpj, rpj’) ~ SIM(wj, wj’), where wj is individual j’s 1,024-dimensional attribute \nweight vector, and rpj is their 150-dimensional vector of ratings across all items. We found that \nthe correlation between pairs was 0.98 for SBERT-Items (p < 0.001, 95% CI = [0.98, 0.98]) and \n0.95 for SBERT-Reasons (p < 0.001, 95% CI = [0.95, 0.95]). These results show that the \npredictive performance of Study 1 persisted in Study 2, and moreover that our approach was able \nto capture the structure of variability between items and between individuals using its attribute \nvectors and weight vectors respectively.  \nHuman vs. Machine Reasons. Recall that we asked GPT to list five reasons for and five \nreasons against engaging in each behavior. This led to a total of ten reasons per behavior and \n1,500 reasons for the full inventory. Our verbal protocol analysis gave each behavior to an \naverage of 4.93 participants, who generated an average of 2.17 reasons each. This resulted in an \naverage of 10.69 unique human-generated reasons for each behavior and 1,582 reasons for the \nfull inventory. Overall, the average number of characters used in each GPT-generated reason was \n90.49 (SD = 40.88), whereas the average number of characters used in each human-generated \nreason was 44.50 (SD = 30.45), showing GPT generated substantially longer reasons than our \nparticipants.  \nWe also examined the semantic content of the reasons generated by both humans and \nGPT. To do this, we used the 18 previously defined themes capturing key attribute dimensions \nwithin each DOSPERT domain (see Table 1). We coded the presence of each theme in each item \nby measuring the cosine similarity of theme vectors to the vectors for the item’s GPT-generated \nreasons. Higher similarity indicates the theme is more represented in the reasons. As a first step, \nwe calculated average theme frequencies across the full set of items to examine the prominence \nSOURCES OF VARIANCE IN DECISION MAKING 40 \n \nof the themes in our data. Additionally, we conducted the same analysis for participant-generated \nreasons to enable comparison with human data. These average theme frequencies are shown in \nFigure 5A, which indicates that themes like friendship and mental health are most common in \nboth machine and human generated reasons, and that themes like adventure and education are \nless common in both machine and human generated reasons. Overall frequencies of the 18 \nthemes are significantly correlated between humans and GPT (r = 0.61, p < 0.01). This suggests \nthat GPT-generated reasons exhibit the same sensitivity to psychological factors implicitly \nshaping human explanations and reasons.  \nInterpreting Item Variability. Our themes allow us to interpret sources of variability \nwithin and across DOSPERT domains. At a surface level, the five DOSPERT domains provide \nan intuitive taxonomy for distinguishing types of risk taking. However, they fail to fully explain \nrating patterns, as shown in Study 1. Why might items from the same domain elicit different \nratings and why might items from different domains elicit similar ratings? We explore this in \nFigure 5B, which visualizes average (z-scored) theme frequencies by DOSPERT domain. This \nfigure shows that although themes that are directly associated (and derived from) a given domain \nare most represented in that domain, they also systematically emerge in other types of domains. \nFor instance, the morality theme has a high frequency of occurrence in reasons for social items, \ndespite being derived from the ethical domain. Likewise, career and relaxation themes (original \nderived from social and recreational domains respectively) have a high frequency in ethical and \nhealth reasons respectively. Financial decisions are the most self-contained in terms of themes, \nhowever cross-domain leakage exists there too– gambling themes, for example, frequently \nemerge in the context of recreational reasons. This analysis again shows that the experimenter-\nderived domain structure of DOSPERT is too coarse to fully capture the underlying complexity \nSOURCES OF VARIANCE IN DECISION MAKING 41 \n \nof DOSPERT, as items with similar interpretive themes may be rated similarly regardless of their \nsurface domain label. This analysis also shows how LLM analysis can be useful for interpreting \n(and not just predicting) the sources of item variability in decision making. \nInterpreting Individual Variability.  We can apply a similar technique to interpret \nsources of individual variability in ratings. As in the analysis in Figure 3, we used binary splits \non the ten psychographic and demographic variables to divide participants into high and low \ngroups for these variables. For each item, we then calculated mean ratings separately for each \nhigh and low group, and subtracted these to quantify the strength of association (and direction) of \nthat item with that individual difference variable. This analysis showed, for example, that male \nparticipants rated items like investing in a speculative stock much higher than female \nparticipants.  Finally, we correlated these item-level individual differences to our coded theme \nsimilarities, revealing which themes are most prominent in behaviors favored by people high or \nlow on each psychographic or demographic split. \nFigure 5C visualizes these correlations (after zscoring). It shows systematic patterns in \nhow different themes explain the behaviors of different groups of participants. For example, we \nsee that high conscientiousness individuals tend to engage in behaviors with career risks, \nwhereas low conscientiousness individuals engage in behaviors involving health and legal risks \n(explaining the patterns in Figure 4E). Likewise, men are more likely to engage in gambling, \nlegal risks, and behaviors with the risk of injury, whereas women are more likely to engage in \nbehaviors with social risks and leisure themes (explaining the patterns shown in Figure 4F). \nAs with Figure 5B, we also see why general domain labels may be ill-suited for \nunderstanding individual differences. For instance, law and norms (both of which are ethical \nthemes) have differential relationships with individual difference variables. Norms are more \nSOURCES OF VARIANCE IN DECISION MAKING 42 \n \nreflected in reasons for the behaviors of high conscientiousness, high emotional stability (low \nneurotic), female, and high religiosity individuals, whereas legal themes are more reflected in \nreasons for the behaviors of low conscientiousness, highly neurotic, male, and low religiosity \nindividuals. We also see similar disassociations for financial themes (with e.g. personal finance \nand investment, but not gambling, being more common for older individuals), health themes \n(with e.g. injury being more common for males but mental health being more common for \nfemales), recreational themes (with e.g. adventure being more common for Republicans and \nrelaxation being more common for high emotional stability individuals), and social themes (with \ne.g. family being more common for high emotional stability individuals, friendship and romance \nbeing more common for high extraversion individuals, and friendship in particular being more \ncommon for females). One again this analysis demonstrates that the reasons that drive behavior \nvary in complex and nuanced ways both within and across risk domains. This explains why the \napproach outlined in this paper effectively models individual differences: DOSPERT labels are \nsimply too coarse, missing nuanced interactions, whereas LLM-derived vector representations \ncapture rich attribute structures and describe how different groups disproportionally weigh those \nattributes. \n Predicting Reasons for Items. In the past two sections, we used general themes \nreflected in GPT-generated reasons to interpret the sources of item-level variance in ratings. \nAfter this we wished to examine whether machine generated reasons predicted human-generated \nreasons for the items. We did this by testing whether the reasons generated by GPT for a given \nitem were more similar to the reasons that people generated for that item than they were to the \nreasons that people generated for other items. To quantify human and GPT-generated reasons, \nwe passed each reason’s text through SBERT to obtain its sentence vector. We then averaged the \nSOURCES OF VARIANCE IN DECISION MAKING 43 \n \nvectors for GPT-generated reasons for each item (as with the SBERT-Reasons model) as well as \nthe vectors for human-generated reasons for each item. For item i we write these machine and \nhuman reason vectors as xi and yi respectively. Finally, we calculated the pairwise cosine \nsimilarity between each human and machine vector, and tested, for each i, whether SIM(xi, yi) \nwas larger than SIM(xi, yi’) for i’ ≠ i. Statistically, this was done by calculating the percentile \nrank of SIM(xi, yi) in the list [SIM(xi, y1), SIM(xi, y2) … SIM(xi, y150)]. If machine generated \nreasons for behavior i are more similar to human generated reasons for that behavior than they \nare to human generated reasons for other behaviors, we would expect this percentile rank to be \nabove 50%. In Figure 6A we show the distribution these percentile ranks for each of the 150 \nitems. Here we can see that the vast majority of percentile ranks are above 50%, with nearly all \npercentiles being very close to 100%. Overall, the average percentile is 98.61 which is \nsignificantly different to 50 (t(149) = 72.77, p < 0.001, 95% CI = [97.28, 99.92]). This indicates \nthat machine-generated reasons for an item are indeed more similar to human-generated reasons \nfor that item than they are to human-generated reasons for other items.  \nPredicting Reasons for Individuals. We also examined whether our model could predict \nthe reasons that each individual generated in the verbal protocol task. Recall that the item \nattribute vectors, xi, represent the reasons that could be at play in a given behavior, whereas the \nindividual-specific weights, wj, describe the participant’s preferences over the dimensions of the \nreason space. Thus, a combination of xi and wj should uniquely reflect the reasons that a \nparticipant lists for engaging or not engaging in a given behavior, above and beyond the reasons \nthat other participants list for that behavior. Again, if we quantify human-generated reasons by \npassing them through the SBERT model, and averaging them on the item level, then we would \nexpect that the weighted attribute value on a given dimension should match up with the reason \nSOURCES OF VARIANCE IN DECISION MAKING 44 \n \nvector’s value on that dimension. If we write the 1,024-dimensional vector for the reasons \ngenerated for item i by participant j as zij then we would expect that the kth dimension of wj \nmultiplied by the kth dimension of xi  should be roughly proportional to the kth dimension of zij. \nIntuitively, if an individual j cares about dimension k (i.e. wj has a high value on its kth \ndimension) and that dimension is relevant to behavior i (i.e. xi has a high value on its kth \ndimension) then that dimension should be prominent in the vector representation of the \nparticipant’s listed reasons (zij should have a high value on the kth dimension).  \n To test this rigorously, we attempted a variant of the analysis described in the previous \nsection. For this we first calculated the element-wise product (also known as the Hadamard \nproduct) of wj and xi, wj○xi for each item and individual pairing. This measured the degree to \nwhich each dimension mattered for each item-individual combination. We then calculated the \npairwise cosine similarity of wj○xi with each zij, and tested whether SIM(wj○xi, zij) was larger \nthan SIM(wj○xi, zij’)  for j’ ≠ j. Statistically, this was done by calculating the percentile rank of \nSIM(wj○xi, zij)  in the list [SIM(wj○xi, zi1), SIM(wj○xi, zi2) … SIM(wj○xi, zi148)]. If our model is \nable to capture the reasons that individual j chooses to engage or not engage in behavior i over \nand above the reasons that other individuals list for that behavior i, then would expect this rank \nto be above 50%. In Figure 6B we show the distribution average percentile ranks for each of the \n150 items. Here we can see that the majority of percentile ranks are above 50%, with the average \npercentile rank for the 150 items being 64.30. This is significantly different to 50 (t(149) = 11.52, \np < 0.001, 95% CI = [61.84, 66.75]). \nDiscussion \n Study 2 compared machine-generated reasons with those generated by human \nparticipants in a verbal protocol task. Participants were asked to list reasons why they do or do \nSOURCES OF VARIANCE IN DECISION MAKING 45 \n \nnot engage in a given behavior. We used SBERT to transform these natural language reasons \ninto high-dimensional multi-attribute vectors and attempted to predict them using vector \nrepresentations of GPT-generated reasons. First, we showed that reasons generated by GPT \ncaptured key themes associated with the five original DOSPERT domains, and, by doing so, \nshed light on why items and individuals may vary in terms of their ratings. We also found that \nGPT-generated reasons for an item were more similar to human-generated reasons for that item \nthan they were to human-generated reasons for other items. We additionally showed that best-fit \nattribute weights for participants uniquely predicted the reasons listed by those participants for \nitems. Finally, Study 2 replicated the main results of Study 1 in a new sample of participants. \nOverall, the findings of Study 2 show that LLM-based decision models not only predict people’s \ntendencies to engage in different behaviors, but can also predict (and, importantly, interpret) the \nreasons why different people chose to engage in different behaviors.  \n \nGeneral Discussion \nSummary \nDecision models are powerful tools for describing the mental processes that give rise to \nbehavior, and for explaining how behaviors vary across individuals and across items. However, \nuntil now, the use of decision models has been limited to highly stylized laboratory tasks which \npresent participants with artificial stimuli defined on a small set of explicit attributes. This is due \nto the difficulty inherent in modeling the complex reasons that underlie real-world decision \nmaking. We attempted to solve this problem using large language models (LLMs), which have \nbeen shown to provide high quality quantitative representations of the meanings of sentences, \nincluding sentences used to describe common behaviors and their associated reasons (Brown et \nal., 2020; Devlin et al., 2018; Reimers et al., 2019). We tested our approach in two preregistered \nSOURCES OF VARIANCE IN DECISION MAKING 46 \n \nstudies involving common risky behaviors. In Study 1 we administered an expanded version of \nthe Domain Specific Risk-Taking (DOSPERT) scale (Blais & Weber, 2006; Weber et al., 2002), \nconsisting of 150 behaviors from five domains (finance, health, recreation, social, and ethical). \nParticipants rated their propensity to engage in each behavior before answering several \ndemographic and personality questionnaires. We used various LLM methods to obtain multi-\nattribute representations for each of the behaviors, which we passed through a regularized linear \nregression to fit individual-specific decision weights. We evaluated our models by predicting \nout-of-sample ratings made by each individual for each item, and found that our models achieved \nhigh accuracy rates, surpassing various benchmarks. Additionally, we found that the similarities \nof attribute vectors for items predicted the correlation in ratings between items, and that the \nsimilarities of best-fit attribute weights of individuals predicted the correlation in ratings between \nindividuals. We also found that our LLM-based behavior vectors captured the domain structure \nof risky choice (e.g. financial risks had vectors that were more similar to other financial risks \nthan to recreational risks), and that best-fit attribute weights captured demographic and \npsychographic differences (e.g. men had weights that were more similar to other men than to \nwomen). These results demonstrate the predictive power of our approach and illustrate its ability \nto provide a process-level foundation for psychometric analysis.  \nIn Study 2 we ran a modification of the experiment in Study 1 with a verbal protocol \ncomponent in which we asked participants to generate reasons for or against a subset of the \nbehaviors in the extended DOSPERT inventory. First, we replicated the main results of Study 1. \nThen we used the prominence of 18 different themes in human and GPT-generated reasons to \ninterpret the sources of variance in people’s ratings. Finally, we tested whether the reasons \ngenerated by LLMs for a given item were more similar to the reasons that people generated for \nSOURCES OF VARIANCE IN DECISION MAKING 47 \n \nthat item than they were to the reasons that people generated for other items. We also examined \nwhether our model could predict the reasons that different individuals generated for a given item, \nbased on their best-fit attribute weights. These tests showed that our approach achieved better \npredictions than by chance. Overall, the results of Study 2 show that our approach is not only \nuseful for prediction, but can also be used to predict and interpret the reasons why people choose \nto engage or not engage in different behaviors. \nIntegrating Paradigms \n We believe that the approach advanced in this paper is particularly valuable, as it \ncombines decision modeling and psychometric research traditions, which have different \nstrengths and limitations. Decision modeling can provide an explanation for how people make \ndecisions based on their preferences and decision processes, but it often requires simplifying \nassumptions and idealized scenarios. Psychometric research can provide a descriptive account of \nhow real-world decision making varies across individuals, groups, items, and domains, but does \nnot explain variance in decision making using quantitative models of decision processes. By \nintegrating these two traditions, our approach is able to provide a more comprehensive and \nrealistic psychological account for the sources of variance in real-world decision making.  \nOf course, we are not the first to try to reconcile these two important paradigms. Prior \napplications of the DOSPERT inventory have used simple linear models to predict behavior from \nparticipant ratings of costs and benefits (Weber et al., 2002). Likewise, psychometric analysis of \nrisk perception has derived multi-dimensional factor structures directly from people’s ratings of \nexperimenter-generated reasons and attributes (Fischhoff et al., 1978). These factors have been \nused in linear models to predict risk perceptions. Finally, recent work by Steiner et al. (2021) and \nArslan et al. (2020) has elicited participant self-reports of reasons considered when evaluating \nSOURCES OF VARIANCE IN DECISION MAKING 48 \n \ngeneral risk-taking tendencies. By manually coding these responses, and using the resulting \ncodes in predictive analysis, these papers have investigated the psychological underpinnings of \nrisk taking. \nOur work builds on top of these important contributions by showing the power of LLMs \nfor modeling complex risky decisions. Unlike prior approaches, the methods advanced in the \ncurrent paper do not need human ratings or self-reports to quantify the attributes at play in a \ngiven decision. This allows for our models to be applied in pure out-of-sample prediction. \nMoreover, LLM representations of behaviors, and preference weights on these representations \nderived from multi-attribute decision models, are able to capture the complex web of social, \nlegal, financial and emotional considerations, at play in human behavior. This allows for more \nsuccessful prediction and interpretation than using experimenter-derived or human-coded \ntaxonomies of attribute structures.  \nHeterogeneous Representations Structures \nA core property of the methodology in this paper is its reliance on a singular \nrepresentational structure when modeling subjective decisions. Specifically, all items are passed \nthrough the same LLM pipeline with the assumption that the resulting vectors describe the \nimpressions, associations, and representations of all participants. In reality, people likely have \nidiosyncratic systems of associations reflecting unique backgrounds and experiences. This inter-\nindividual variability in semantic representation cannot be directly captured by applying a single \ninvariant LLM transformation, which appears to be a fundamental constraint of our approach. \nThat said, the proposed approach of fitting individual-level weights on the LLM vectors \ndoes indirectly allow for heterogeneity in representational structures. Conceptually, allowing for \ncustomizable weighting across the semantic dimensions encoded in the vectors is directly \nSOURCES OF VARIANCE IN DECISION MAKING 49 \n \nanalogous to participant-specific tweakings of the representational space itself. More concretely, \nimagine two individuals who associate the activity “drinking alcohol” with largely disjoint \nattributes due to their unique backgrounds. For participant 1, drinking alcohol may cue biological \neffects like liver damage. For participant 2 with personal experience managing alcoholics, \nattributes like impaired motor coordination resulting in accidents and falls may be more central. \nDespite these divergent associative landscapes, our proposed model handles this via the \nindividual weight vectors learned by fitting individual-level ratings. Weights for participant 1 \neffectively stretch alcohol-related dimensions of the space to align more closely with their \npersonal associates, while those for participant 2 independently elongate the coordination \nimpairment dimensions. Thus, rather than concealing variation, the weighting mechanism \nimplicitly allows the LLM vectors themselves to become participant-specific. Essentially, the \nflexible weights selectively prioritize distinct semantic aspects or reasons for each person, \naccommodating heterogeneity in unobserved representational structures.  \nIt is worth noting that this is the main motivation behind transfer learning which is \nresponsible for the current successes of LLMs (see e.g. Devlin et al., 2018 for a discussion). \nTransfer learning involves pretraining a base model on a generic objective (e.g. next word \nprediction) and then fine-tuning on small datasets for the target task (in our case, the participant’s \n150 ratings). The reason why transfer learning is so useful for LLMs is that fine-tuning allows \nthe model to tweak its representational structure to match the target task without having to be \nfully trained on the target task, which generally does not have enough data to train a full \nlanguage model from scratch. In our case, it is impossible to train an LLM on individual-level \nlanguage data, and thus impossible to directly recover individual-specific representational \nstructures for common behaivors. Given this limitation, our approach, which is an application of \nSOURCES OF VARIANCE IN DECISION MAKING 50 \n \ntransfer learning to psychological problems, is the most practical and popular solution to the \nproblem of heterogeneity in representational structure. Indeed, it is precisely due to its ability to \ncapture individual-specific representational structure that our approach is so successful at \npredicting individual differences in behaviors.  \nDimensionality and Dataset Size \nAnother potential limitation involves the high dimensionality of the LLM representations \nand possible dependence on dataset scale when fitting decision weights. While attribute vectors \nexceeding 1,000 dimensions are atypical for decision research, we believe this elevated \ncomplexity is essential for capturing real-world choice nuances - the very phenomenon this paper \nseeks to model. Restricting attribute dimensions to two or three factors may be sufficient for \nsimplified laboratory tasks with artificial objects, but it cannot encapsulate the full richness of \norganic judgment. Additionally, although it is true that model fitting requires more data than \nsimple qualitative analysis, the 150 ratings used in this paper is roughly in line with the number \nof observations used to fit individual-level decision models like cumulative prospect theory in \nlaboratory-based psychological research (e.g. Glöckner & Pachur, 2012; He et al., 2022; \nRieskamp, 2008). Finally, as demonstrated empirically, predictive performance remains high \neven when restricting training data to 25% of the original data, providing strong evidence of \ngeneralizability. Thus, rather than a weakness, the flexibility to represent fine-grained semantic \ndistinctions through high dimensional representations is a strength of the proposed framework.  \nInterpretation \n Another potential critique involves interpretability - by focusing predominately on \npredictive accuracy, the meaning of model parameters and attribute representations can be \nobscured. Additionally, on the surface, LLM-derived vectors appear to be inscrutable high-\nSOURCES OF VARIANCE IN DECISION MAKING 51 \n \ndimensional latent variables rather than discrete, interpretable characteristics or features. For this \nreason, they more likely reflect intuitive associations and impressions flowing from automatic \ncognition, rather than the clear high-level features used for deliberate reasoning. Although this is \na fair point (and indeed, in prior work, we have argued that LLM representations are useful for \nmodeling system 1 cognition – see Bhatia, 2017), LLM technologies nonetheless enable \ninterpretation. We have shown this by coding items in terms of 18 themes reflected in GPT \ngenerated reasons. These themes illustrate why within-domain items can elicit dissimilar ratings \nand between-domain items can show similarities - uncovering cross-cutting psychological \ndimensions missed by existing DOSPERT taxonomies. Additionally, by correlating theme \nprominence with individual differences we are able to interpret the sources of variability across \npsychographic and demographic groups.  \nIt is important to emphasize that the interpretative analysis in this paper simply \ndemonstrates how LLM-derived representations can be used to understand naturalistic decisions, \nand, by doing so, serve an explanatory role analogous to traditional decision science attributes. \nOther researchers could specify alternative themes tailored to their research questions and \ntheoretical orientations, and reuse our pipeline to analyze those themes, without needing to \ncollect any more data. In this way, our paper shows how modern AI can transform qualitative \npsychological concepts into interpretable quantitative spaces amenable to rigorous analysis. \nLimitations and Future Directions \nOur approach, though useful, has its limitations. We use a linear model to represent \ndecision-making, which may oversimplify cognitive processes that likely involve heuristics and \nsequential reasoning. Future studies should integrate our approach with more complex decision \nmodels (e.g. Gigerenzer & Gaissmaier, 2011; Payne et al., 1988; Roe et al, 2001; Zhao et al., \nSOURCES OF VARIANCE IN DECISION MAKING 52 \n \n2022) to better understand how decision variability relates to nuanced cognitive mechanisms. \nSuch an analysis may be particularly useful for the study of ecological rationality, which \nexamines the utility of different decision strategies for real-world cognition and behavior. \nCurrently, most modeling work on decision strategies uses tightly controlled experiments, which \nprecludes the assessment of performance in natural settings. The approach proposed in this paper \ncombines naturalistic elicitation and predictive modeling, and can directly address this limitation, \nopening up new directions for the formal analysis of boundedly rational cognition and behavior.  \nAnother imitation is that our work is solely focused on risk, which, while significant, is \njust one domain. Future research should apply our methods to different domains to broaden their \napplicability and verify the observed patterns. We suspect that our approach can be easily \nextended to both traditional decision-making scenarios such as intertemporal (Loewenstein & \nPrelec, 1992; He et al., 2023), social (Fehr & Schmidt, 1999; Camerer, 2011) or consumer \n(Green & Srinivasan, 1990; Payne et al., 1991) choice, as well as other topics currently studied \nusing psychometric methods that rely on human ratings of lexical items. By converting the \nlanguage used to describe common traits, attitudes, beliefs, and behaviors into vectors, we can \nalso construct high-quality cognitive models capable of engaging in realistic deliberation and \nresponding to psychological surveys in a human-like manner (for a detailed discussion see \nBhatia & Aka, 2022; also see Yarkoni & Westfall, 2017 and Hofman et al., 2021 for discussions \nof the value of prediction in the social and behavioral sciences). In addition to providing better \ntheories of human behavior, such an approach can also help in the development of AI agents that \ncan better interact with people and respond to their behavior. Recent years have seen significant \nadvancements in AI. If psychologists wish to contribute to the study of human behavior in this \nnew era, they must leverage existing theories and combine them with innovative methods to \nSOURCES OF VARIANCE IN DECISION MAKING 53 \n \naddress classic problems. This paper demonstrates the feasibility of this research program, when \napplied to the study of individual differences in decision-making, and we are enthusiastic about \nthe prospects that lie ahead.  \nSOURCES OF VARIANCE IN DECISION MAKING 54 \n \nReferences \nAka, A., & Bhatia, S. (2022). Machine Learning Models for Predicting, Understanding, and \nInfluencing Health Perception. Journal of the Association for Consumer Research, 7(2), \n142-153. \nArslan, R. C., Brümmer, M., Dohmen, T., Drewelies, J., Hertwig, R., & Wagner, G. G. \n(2020). How people know their risk preference. Scientific Reports, 10(1), 15365. \nBhatia, S. (2019). Predicting risk perception: New insights from data science. Management \nScience, 65, 3800-3823. \nBhatia, S., Loomes, G., & Read, D. (2021). Establishing the laws of preferential choice \nbehavior. Judgment and Decision Making, 16(6), 1324-1369.  \nBhatia, S., Olivola, C., Bhatia, N., & Ameen, A. (2022). Predicting leadership perception with \nlarge-scale natural language data. Leadership Quarterly. 33(5), 101535. \nBhatia, S., & Richie, R. (2023). Transformer networks of human conceptual knowledge. \nPsychological Review. \nBhatia, S., & Walasek, L. (2023). Predicting implicit attitudes with natural language data. \nProceedings of the National Academy of Sciences, 120(25), e2220726120. \nBlais, A.-R., & Weber, E. U. (2006) A Domain-Specific Risk-Taking (DOSPERT) scale for \nadult populations. Judgment and Decision Making, 1, 33-47. \nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. \n(2020). Language models are few-shot learners. Advances in Neural Information \nProcessing Systems.  \nSOURCES OF VARIANCE IN DECISION MAKING 55 \n \nBruine de Bruin, W., Parker, A. M., & Fischhoff, B. (2007). Individual differences in adult \ndecision-making competence. Journal of Personality and Social Psychology, 92(5), 938-\n956. doi: 10.1037/0022-3514.92.5.938 \nBusemeyer, J. R., & Diederich, A. (2010). Cognitive modeling. Sage. \nBusemeyer, J. R., Gluth, S., Rieskamp, J., & Turner, B. M. (2019). Cognitive and neural \nbases of multi-attribute, multi-alternative, value-based decisions. Trends in Cognitive \nSciences, 23(3), 251-263. \nCacioppo, J. T., & Petty, R. E. (1982). The need for cognition. Journal of Personality and \nSocial Psychology, 42(1), 116-131. \nCamerer, C. F. (2011). Behavioral game theory: Experiments in strategic interaction. \nPrinceton university press. \nCaliskan, A., Bryson, J. J., & Narayanan, A. (2017). Semantics derived automatically from \nlanguage corpora contain human-like biases. Science, 356(6334), 183-186. \nDevlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT: Pretraining of deep \nbidirectional transformers for language understanding. Annual Conference of the North \nAmerican Chapter of the Association for Computational Linguistics  \nDohmen, T., Falk, A., Huffman, D., Sunde, U., Schupp, J., & Wagner, G. G. (2011). \nIndividual risk attitudes: Measurement, determinants, and behavioral consequences. \nJournal of the European Economic Association, 9(3), 522-550. \nEdwards, W. (1961). Behavioral decision theory. Annual Review of Psychology, 12(1), 473-\n498. \nSOURCES OF VARIANCE IN DECISION MAKING 56 \n \nEpstein, S., Pacini, R., Denes-Raj, V., & Heier, H. (1996). Individual differences in Intuitive-\nExperiential and Analytical-Rational Thinking Styles. Journal of Personality and Social \nPsychology, 71(2), 390-405 \nEricsson, K. A., & Simon, H. A. (1980). Verbal reports as data. Psychological Review, 87(3), \n215. \nFrey, R., Duncan, S. M., & Weber, E. U. (2023). Towards a typology of risk preference: Four \nrisk profiles describe two-thirds of individuals in a large sample of the US population. \nJournal of Risk and Uncertainty, 66(1), 1-17. \nFrey, R., Pedroni, A., Mata, R., Rieskamp, J., & Hertwig, R. (2017). Risk preference shares \nthe psychometric structure of major psychological traits. Science Advances, 3, e1701381.  \nFrey, R., Richter, D., Schupp, J., Hertwig, R., & Mata, R. (2021). Identifying robust correlates \nof risk preference: A systematic approach using specification curve analysis. Journal of \nPersonality and Social Psychology, 120(2), 538–557. \nGandhi, N., Zou, W., Meyer, C., Bhatia, S., & Walasek, L. (2022). Computational methods \nfor predicting and understanding food judgment. Psychological Science, 33(4), 579-594. \nGarten, J., Hoover, J., Johnson, K. M., Boghrati, R., Iskiwitch, C., & Dehghani, M. (2018). \nDictionaries and distributions: Combining expert knowledge and large scale textual data \ncontent analysis: Distributed dictionary representation. Behavior Research Methods, 50, \n344-361. \nGigerenzer, G., & Gaissmaier, W. (2011). Heuristic decision making. Annual Review of \nPsychology, 62, 451-482. \nGlöckner, A., & Pachur, T. (2012). Cognitive models of risky choice: Parameter stability and \npredictive accuracy of prospect theory. Cognition, 123(1), 21-32. \nSOURCES OF VARIANCE IN DECISION MAKING 57 \n \nGraham, J., Haidt, J., & Nosek, B. A. (2009). Liberals and conservatives rely on different sets \nof moral foundations. Journal of Personality and Social Psychology, 96(5), 1029. \nGreen, P. E., & Srinivasan, V. (1990). Conjoint analysis in marketing: new developments \nwith implications for research and practice. Journal of Marketing, 54(4), 3-19. \nHarris, C. R., & Jenkins, M. (2006). Gender differences in risk assessment: why do women \ntake fewer risks than men?. Judgment and Decision making, 1(1), 48-63. \nHe, L., Analytis, P. P., & Bhatia, S. (2022). The wisdom of model crowds. Management \nScience, 68(5), 3635-3659 \nHe, L., Wall, D., Reeck, C., & Bhatia, S. (2023). Information acquisition and decision \nstrategies in intertemporal choice. Cognitive Psychology, 142, 101562. \nHe, L., Zhao, W. & Bhatia, S. (2022). An ontology of decision models. Psychological Review, \n129(1), 49–72. \nHighhouse, S., Nye, C. D., Zhang, D. C., & Rada, T. B. (2016). Structure of the DOSPERT: \nIs there evidence for a general risk factor? Journal of Behavioral Decision Making.  \nHills, T. T., Jones, M. N., & Todd, P. M. (2012). Optimal foraging in semantic memory. \nPsychological Review, 119(2), 431. \nHoward, R. A. (1988). Decision analysis: Practice and promise. Management Science, 34(6), \n679-695. \nJosef, A. K., Richter, D., Samanez-Larkin, G. R., Wagner, G. G., Hertwig, R., & Mata, R. \n(2016). Stability and change in risk-taking propensity across the adult life span. Journal of \nPersonality and Social Psychology, 111(3), 430. \nKahneman, D., & Tversky, A. (1979). Prospect Theory: An Analysis of Decision under Risk. \nEconometrica, 47(2), 263-292. \nSOURCES OF VARIANCE IN DECISION MAKING 58 \n \nKeeney, R. L., & Raiffa, H. (1993). Decisions with multiple objectives: preferences and value \ntrade-offs. Cambridge university press. \nLandauer, T. K., & Dumais, S. T. (1997). A solution to Plato's problem: The latent semantic \nanalysis theory of acquisition, induction, and representation of knowledge. Psychological \nReview, 104(2), 211. \nLauriola, M., Levin, I. P., & Hart, S. S. (2007). Common and distinct factors in decision \nmaking under ambiguity and risk: A psychometric study of individual differences. \nOrganizational Behavior and Human Decision Processes, 104(2), 130-149. \nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019). Roberta: A \nrobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. \nLu, H., Wu, Y. N., & Holyoak, K. J. (2019). Emergence of analogy from relation learning. \nProceedings of the National Academy of Sciences, 116(10), 4176-4181. \nMandera, P., Keuleers, E., & Brysbaert, M. (2017). Explaining human performance in \npsycholinguistic tasks with models of semantic similarity based on prediction and \ncounting: A review and empirical validation. Journal of Memory and Language, 92, 57-78. \nMarkowitz, H. M. (1959). Portfolio selection. New York: Wiley \nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed \nrepresentations of words and phrases and their compositionality. Advances in Neural \nInformation Processing Systems.  \nPayne, J. W., Bettman, J. R., & Johnson, E. J. (1988). Adaptive strategy selection in decision \nmaking. Journal of Experimental Psychology: Learning, Memory, and Cognition, 14(3), \n534. \nSOURCES OF VARIANCE IN DECISION MAKING 59 \n \nPayne, J., Bettman, J. R., & Johnson, E. J. (1991). Consumer decision making. Handbook of \nconsumer behaviour, 50-84. \nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... & \nDuchesnay, E. (2011). Scikit-learn: Machine learning in Python. Journal of Machine \nLearning Research, 12, 2825-2830. \nPeters, E., Västfjäll, D., Slovic, P., Mertz, C. K., Mazzocco, K., & Dickert, S. (2006). \nNumeracy and decision making. Psychological Science, 17(5), 407-413. \nRammstedt, B. John, O.P. (2007). Measuring personality in one minute or less: A 10-item \nshort version of the Big Five Inventory in English and German. Journal of Research in \nPersonality, 41, 203– 212 \nReimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese \nBERT-Networks. Proceedings of the 2019 Conference on Empirical Methods in Natural \nLanguage Processing. \nRichie, R., Zou, W., & Bhatia, S. (2019). Predicting high-level human judgment across \ndiverse behavioral domains. Collabra: Psychology.  \nRichie, R., Aka, A., & Bhatia, S. (2023). Free association in a neural network. Psychological \nReview. \nRick, S. I., Cryder, C. E., & Loewenstein, G. (2008). Tightwads and spendthrifts. Journal of \nConsumer Research, 34(6), 767-782. \nRieskamp, J. (2008). The probabilistic nature of preferential choice. Journal of Experimental \nPsychology: Learning, Memory, and Cognition, 34(6), 1446. \nSOURCES OF VARIANCE IN DECISION MAKING 60 \n \nRoe, R. M., Busemeyer, J. R., & Townsend, J. T. (2001). Multialternative decision field \ntheory: A dynamic connectionst model of decision making. Psychological Review, 108(2), \n370. \nRushton, J. P., Chrisjohn, R. D., & Fekken, G. C. (1981). The altruistic personality and the \nself-report altruism scale. Personality and Individual Differences, 2(4), 293-302.external \nimage clear.gif \nSchulte-Mecklenbeck, M., Kühberger, A., & Johnson, J. G. (2011). A handbook of process \ntracing methods for decision research: A critical review and user’s guide. Psychology \nPress. \nSchwartz, B., Ward, A., Monterosso, J., Lyubomirsky, S., White, K., & Lehman, D. R. \n(2002). Maximizing versus satisficing: Happiness is a matter of choice. Journal of \nPersonality and Social Psychology, 83(5), 1178-1197.  \nSingh, M., Richie, R. & Bhatia, S. (2022). Representing and predicting everyday behavior. \nComputational Brain & Behavior, 5, 1-21. \nSlovic, P. (1987). Perception of risk. Science, 236(4799), 280-285. \nSlovic, P., Fischhoff, B., & Lichtenstein, S. (1977). Behavioral decision theory. Annual \nReview of Psychology, 28(1), 1-39. \nSteiner, M. D., Seitz, F. I., & Frey, R. (2021). Through the window of my mind: Mapping \ninformation integration and the cognitive representations underlying self-reported risk \npreference. Decision, 8(2), 97. \nTausczik, Y. R., & Pennebaker, J. W. (2010). The psychological meaning of words: LIWC \nand computerized text analysis methods. Journal of Language and Social Psychology, \n29(1), 24-54. \nSOURCES OF VARIANCE IN DECISION MAKING 61 \n \nTian, K. T., Bearden, W. O., & Hunter, G. L. (2001). Consumers' need for uniqueness: Scale \ndevelopment and validation. Journal of Consumer Research, 28(1), 50-66. doi: \n10.1086/321947 \nWeber, E. U., Blais, A., & Betz, N. E. (2002). A domain-specific risk-attitude scale: \nMeasuring risk perceptions and risk behaviors. Journal of Behavioral Decision Making, \n15(4), 263-290. \nWeber, E. U., Johnson, E. J., Milch, K. F., Chang, H., Brodscholl, J. C., & Goldstein, D. G. \n(2007). Asymmetric discounting in intertemporal choice: A query-theory account. \nPsychological Science, 18(6), 516-523. \nWeber, E. U., & Johnson, E. J. (2009). Mindful judgment and decision making. Annual \nReview of Psychology, 60, 53-85. \nWeller, J. A., & Tikir, A. (2011). Predicting domain‐specific risk taking with the HEXACO \npersonality structure. Journal of Behavioral Decision Making, 24(2), 180-201. \nZhao, W., Richie, R. & Bhatia, S. (2022). Process and content in decisions from memory. \nPsychological Review, 129(1), 73–106. \n \nSOURCES OF VARIANCE IN DECISION MAKING 62 \n \nA \n \n \n \nB \n \n \n \n \nFigure 1.  Overview of approach. A: The four LLM methods for vectorizing behaviors. We used \nSBERT and bag-of-words Word2Vec to vectorize text, and applied these either directly to the \nbehavior text or to GPT-generated reasons for and against engaging in the behavior. B: \nDescription of an LLM-based decision model for making out-of-sample rating predictions. Here \nitem vectors, obtained from one of the four LLM methods, are passed through an individual-\nspecific decision model, in which attribute dimensions are weighted and aggregated to predict \nratings. The model’s weights are fit on  the participant’s ratings for all items except for one test \nitem, and the model’s accuracy is tested on the held-out rating for the test item. \n\nSOURCES OF VARIANCE IN DECISION MAKING 63 \n \n \n  \n \n \n \n \n \nFigure 2. Predictive accuracy. A: Average LOOCV correlation of four models across \nparticipants, with varying ridge regularization penalties a. B: Average LOOCV MSEs of models. \nC: Histogram of participant-level LOOCV correlations for the best performing model, SBERT-\nItems. D: Average LOOCV correlation obtained from six alternate tests along with that for the \nbest performing model from the main analysis. Error bars indicate +/- 1 SE in panels A, B and D. \n\nSOURCES OF VARIANCE IN DECISION MAKING 64 \n \n \n  \n  \n \n \nFigure 3. Item variability. A: Average pairwise item cosine similarity within the five DOSPERT \ndomains as well as across all items in the inventory (which is also indicated with the dashed \nline). B: Scatter plot of first and second principle components of item vectors, colored by \ndomain. Here E, F, H, R and S correspond to ethical, financial, health, recreational and social \nrespectively. C: Average pairwise cosine similarity of item pairs as a function of their pairwise \ncorrelation in participant ratings. Items are binned into 20 equally sized groups based on the \nparticipant correlations. D: Correlations between pairwise item cosine similarity and pairwise \nitem participant correlations within each of the five domains as well across the full dataset. The \nresults in this figure use vector representations from the SBERT-Items model. Error bars indicate \n+/- 1 SE in panel A and 95% CIs in panel D. \n\nSOURCES OF VARIANCE IN DECISION MAKING 65 \n \n \n  \n  \n  \n \n \nFigure 4. Individual variability. A: Average pairwise cosine similarity of participant weight \nvectors as a function of the pairwise correlation in participant ratings. Items are binned into 20 \nequally sized groups based on the participant correlations. B: Distribution of out-of-sample \ncorrelations between weight vector cosine similarity and ratings correlations from 100 binary \nsplits on the data. C: Within and between group correlations in ratings across participants, for \nvarious demographic and psychographic variables. D: Within and between group cosine \nsimilarities in the weights of participants, for various demographic and psychographic variables. \nE: Scatter plot of first and second principle components of weight vectors, colored by \n\nSOURCES OF VARIANCE IN DECISION MAKING 66 \n \nconscientiousness grouping (high or low). F: scatter plot of first and second principle \ncomponents of weight vectors, colored by gender grouping (male or female). Error bars indicate \n+/- 1 SE in panels A and B. Conc., Ext., Neur., Agr., and Open. refer to conscientiousness, \nextraversion, neuroticism, agreeableness, and openness to experience personality groupings \nrespectively, whereas Relig. and Polit. refer religious and political affiliation demographic \ngroupings respectively.  \n \nSOURCES OF VARIANCE IN DECISION MAKING 67 \n \n \n \n \n\nSOURCES OF VARIANCE IN DECISION MAKING 68 \n \nFigure 5. Interpreting themes at play in  participant ratings. A: Average theme frequencies in \nLLM-generated reasons (x-axis) vs. human-generated reasons (y-axis) in Study 2. B: Average \ntheme frequencies in GPT-generated reasons as a function of the item’s underlying DOSPERT \ndomain. C: Average theme frequencies in GPT-generated reasons for items given high vs. low \nratings in various psychographic and demographic splits of the data. Conc., Ext., Neur., Agr., \nand Open. refer to conscientiousness, extraversion, neuroticism, agreeableness, and openness to \nexperience personality groupings respectively, whereas Relig. and Polit. refer religious and \npolitical affiliation demographic groupings respectively.  \n \nSOURCES OF VARIANCE IN DECISION MAKING 69 \n \n \n \n \n \n \n \n \n \nFigure 6. The relationship between machine and human-generated reasons. A: Percentile ranks \nshowing how similar machine-generated reasons are to human-generated reasons for an item, \nrelative to human-generated reasons for other items. B: Percentile ranks showing how similar \nmachine-generated reasons are to human-generated reasons for an item-individual pair, relative \nto reasons generated by other individuals for that item. We would expect percentile ranks of 50% \nif human and machine-generated were not systematically related. \n  \n\nSOURCES OF VARIANCE IN DECISION MAKING 70 \n \nTable 1: List of themes, and examples of words, used in Study 2 interpretative analysis. \nDomain Themes Words \nEthics Law arrest, illegal, prison … \n Norms obligations, expectations, customs … \n Morality ethics, integrity, virtue, … \nHealth Mental Health wellbeing, anxiety, depression … \n Injury fracture, wound, sprain … \n Disease infection, virus, bacteria … \n Nutrition diet, vitamins, protein … \nSocial Family parents, children, siblings … \n Friendship friend, acquaintance, solidarity … \n Romance love, sex, marriage … \n Career profession, employment, ambition … \n Education learning, school, knowledge … \nRecreation Adventure journey, thrill, expedition … \n Pleasure enjoyment, fun, luxury … \n Relaxation rest, leisure, comfort … \nFinancial Investment stocks, bonds, portfolio … \n Personal Finance budgeting, savings, expenses … \n Gambling betting, casino, odds … \n \n \n ",
  "topic": "Variance (accounting)",
  "concepts": [
    {
      "name": "Variance (accounting)",
      "score": 0.7098231911659241
    },
    {
      "name": "Psychology",
      "score": 0.626905620098114
    },
    {
      "name": "Sample (material)",
      "score": 0.6144678592681885
    },
    {
      "name": "Social psychology",
      "score": 0.5092703104019165
    },
    {
      "name": "Cognitive psychology",
      "score": 0.4530285894870758
    },
    {
      "name": "Process (computing)",
      "score": 0.41813018918037415
    },
    {
      "name": "Computer science",
      "score": 0.23136568069458008
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Chromatography",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Accounting",
      "score": 0.0
    }
  ]
}