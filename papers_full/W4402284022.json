{
  "title": "A Study on Performance Enhancement by Integrating Neural Topic Attention with Transformer-Based Language Model",
  "url": "https://openalex.org/W4402284022",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5107021405",
      "name": "Taehum Um",
      "affiliations": [
        "Gachon University"
      ]
    },
    {
      "id": "https://openalex.org/A5062159536",
      "name": "Namhyoung Kim",
      "affiliations": [
        "Gachon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W3198659451",
    "https://openalex.org/W6639619044",
    "https://openalex.org/W3191804854",
    "https://openalex.org/W1969486090",
    "https://openalex.org/W2001587475",
    "https://openalex.org/W2023440853",
    "https://openalex.org/W2759181158",
    "https://openalex.org/W3118616224",
    "https://openalex.org/W3136943665",
    "https://openalex.org/W2891112648",
    "https://openalex.org/W6683240801",
    "https://openalex.org/W2968713397",
    "https://openalex.org/W2621133045",
    "https://openalex.org/W55204438",
    "https://openalex.org/W6779928756",
    "https://openalex.org/W74439177",
    "https://openalex.org/W3104613320",
    "https://openalex.org/W6685356407",
    "https://openalex.org/W3176380929",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2163455955",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2423124209",
    "https://openalex.org/W2250966211",
    "https://openalex.org/W1880262756",
    "https://openalex.org/W2098062695",
    "https://openalex.org/W2157006255"
  ],
  "abstract": "As an extension of the transformer architecture, the BERT model has introduced a new paradigm for natural language processing, achieving impressive results in various downstream tasks. However, high-performance BERT-based models—such as ELECTRA, ALBERT, and RoBERTa—suffer from limitations such as poor continuous learning capability and insufficient understanding of domain-specific documents. To address these issues, we propose the use of an attention mechanism to combine BERT-based models with neural topic models. Unlike traditional stochastic topic modeling, neural topic modeling employs artificial neural networks to learn topic representations. Furthermore, neural topic models can be integrated with other neural models and trained to identify latent variables in documents, thereby enabling BERT-based models to sufficiently comprehend the contexts of specific fields. We conducted experiments on three datasets—Movie Review Dataset (MRD), 20Newsgroups, and YELP—to evaluate our model’s performance. Compared to the vanilla model, the proposed model achieved an accuracy improvement of 1–2% for the ALBERT model in multiclassification tasks across all three datasets, while the ELECTRA model showed an accuracy improvement of less than 1%.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5582213997840881
    },
    {
      "name": "Transformer",
      "score": 0.5409056544303894
    },
    {
      "name": "Engineering",
      "score": 0.19435733556747437
    },
    {
      "name": "Electrical engineering",
      "score": 0.14521273970603943
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I12832649",
      "name": "Gachon University",
      "country": "KR"
    }
  ]
}