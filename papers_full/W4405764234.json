{
  "title": "Large language models facilitating modern molecular biology and novel drug development",
  "url": "https://openalex.org/W4405764234",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2401701405",
      "name": "Xiao-Huan Liu",
      "affiliations": [
        "Jining Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2397302689",
      "name": "Zhen hua Lu",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A1977132994",
      "name": "Tao Wang",
      "affiliations": [
        "Jining Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2035932820",
      "name": "Fei Liu",
      "affiliations": [
        "Jining Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2401701405",
      "name": "Xiao-Huan Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2397302689",
      "name": "Zhen hua Lu",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A1977132994",
      "name": "Tao Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2035932820",
      "name": "Fei Liu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4386894187",
    "https://openalex.org/W3209056694",
    "https://openalex.org/W4366269319",
    "https://openalex.org/W6850342999",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4381838887",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W6849093697",
    "https://openalex.org/W6847076894",
    "https://openalex.org/W3208957091",
    "https://openalex.org/W4296032638",
    "https://openalex.org/W4366989525",
    "https://openalex.org/W4225000967",
    "https://openalex.org/W4362601804",
    "https://openalex.org/W6853104154",
    "https://openalex.org/W4318350716",
    "https://openalex.org/W4288066876",
    "https://openalex.org/W4229373389",
    "https://openalex.org/W4387764460",
    "https://openalex.org/W4366703942",
    "https://openalex.org/W4365483115",
    "https://openalex.org/W4385454884",
    "https://openalex.org/W4226159083",
    "https://openalex.org/W4210852924",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4327715333",
    "https://openalex.org/W4206563428",
    "https://openalex.org/W4362582720",
    "https://openalex.org/W2954868841",
    "https://openalex.org/W4396597709",
    "https://openalex.org/W4377232911",
    "https://openalex.org/W4388571183",
    "https://openalex.org/W4380995257",
    "https://openalex.org/W4385652578",
    "https://openalex.org/W4291288469",
    "https://openalex.org/W4387496544",
    "https://openalex.org/W4387047777",
    "https://openalex.org/W4385666966",
    "https://openalex.org/W4367049415",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4366823237",
    "https://openalex.org/W4379932151",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W2914635984",
    "https://openalex.org/W3201869122",
    "https://openalex.org/W4386567003",
    "https://openalex.org/W4387843512",
    "https://openalex.org/W4388035208",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4387326101",
    "https://openalex.org/W4388464011",
    "https://openalex.org/W4385849028",
    "https://openalex.org/W4380225176",
    "https://openalex.org/W3200742808",
    "https://openalex.org/W4285392187",
    "https://openalex.org/W4362703444",
    "https://openalex.org/W4212837331",
    "https://openalex.org/W4388423776",
    "https://openalex.org/W4384497798",
    "https://openalex.org/W4383216409",
    "https://openalex.org/W4318718899",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4375949262",
    "https://openalex.org/W4380989429"
  ],
  "abstract": "The latest breakthroughs in information technology and biotechnology have catalyzed a revolutionary shift within the modern healthcare landscape, with notable impacts from artificial intelligence (AI) and deep learning (DL). Particularly noteworthy is the adept application of large language models (LLMs), which enable seamless and efficient communication between scientific researchers and AI systems. These models capitalize on neural network (NN) architectures that demonstrate proficiency in natural language processing, thereby enhancing interactions. This comprehensive review outlines the cutting-edge advancements in the application of LLMs within the pharmaceutical industry, particularly in drug development. It offers a detailed exploration of the core mechanisms that drive these models and zeroes in on the practical applications of several models that show great promise in this domain. Additionally, this review delves into the pivotal technical and ethical challenges that arise with the practical implementation of LLMs. There is an expectation that LLMs will assume a more pivotal role in the development of innovative drugs and will ultimately contribute to the accelerated development of revolutionary pharmaceuticals.",
  "full_text": "Large language models\nfacilitating modern molecular\nbiology and novel drug\ndevelopment\nXiao-huan Liu1, Zhen-hua Lu2, Tao Wang1* and Fei Liu1*\n1School of Biological Science, Jining Medical University, Jining, China,2College of Chemical and\nBiological Engineering, Zhejiang University, Hangzhou, China\nThe latest breakthroughs in information technology and biotechnology have\ncatalyzed a revolutionary shift within the modern healthcare landscape, with\nnotable impacts from artiﬁcial intelligence (AI) and deep learning (DL). Particularly\nnoteworthy is the adept application of large language models (LLMs), which\nenable seamless and efﬁcient communication between scientiﬁc researchers and\nAI systems. These models capitalize on neural network (NN) architectures that\ndemonstrate proﬁciency in natural language processing, thereby enhancing\ninteractions. This comprehensive review outlines the cutting-edge\nadvancements in the application of LLMs within the pharmaceutical industry,\nparticularly in drug development. It offers a detailed exploration of the core\nmechanisms that drive these models and zeroes in on the practical applications of\nseveral models that show great promise in this domain. Additionally, this review\ndelves into the pivotal technical and ethical challenges that arise with the practical\nimplementation of LLMs. There is an expectation that LLMs will assume a more\npivotal role in the development of innovative drugs and will ultimately contribute\nto the accelerated development of revolutionary pharmaceuticals.\nKEYWORDS\nartiﬁcial intelligence, large language models, drug development, ChatGPT, protein\nstructure prediction\n1 Introduction\nDuring the past few decades, the ﬁeld of drug discovery has undergone a\ntransformative revolution, largely due to the rapid advancements in information\ntechnology and modern biotechnology, such as arti ﬁcial intelligence (AI), machine\nlearning (ML), structural revolution (cr ystallography), and synthetic biology\n(Sadybekov and Katritch, 2023 ; Murray et al., 2023; Cova et al., 2022; Roggia et al.,\n2024). A notable paradigm shift is evident in contemporary drug discovery, where\nemerging technologies have streamlined the drug development process, consequently\nreducing associated costs ( Pandey et al., 2022 ). Among them, computational\napproaches guided by molecular modeling techniques with AI for “hit\nidenti ﬁcation ” and “lead optimization ” have garnered signi ﬁcant interest from\nbiotech ﬁrms and research institutions ( Jayatunga et al., 2022 ; Chakraborty et al.,\n2023). “Hit identi ﬁcation ” is the process of screening large compound libraries to\ndiscover molecules that exhibit initial biological activity against a speciﬁc target, serving\nas potential starting points for drug development.“Lead optimization” is the systematic\nprocess of re ﬁning and enhancing the potency, selectivity, and pharmacokinetic\nOPEN ACCESS\nEDITED BY\nChandrabose Selvaraj,\nDr. D. Y. Patil Vidyapeeth, India\nREVIEWED BY\nSucheendra K. Palaniappan,\nThe Systems Biology Institute, Japan\nJagat Pal Yadav,\nSam Higginbottom University of Agriculture,\nTechnology and Sciences, India\n*CORRESPONDENCE\nTao Wang,\ntao-wang@zju.edu.cn\nFei Liu,\nliufei092531@163.com\nRECEIVED 03 July 2024\nACCEPTED 05 December 2024\nPUBLISHED 24 December 2024\nCITATION\nLiu X-h, Lu Z-h, Wang T and Liu F (2024) Large\nlanguage models facilitating modern molecular\nbiology and novel drug development.\nFront. Pharmacol. 15:1458739.\ndoi: 10.3389/fphar.2024.1458739\nCOPYRIGHT\n© 2024 Liu, Lu, Wang and Liu. This is an open-\naccess article distributed under the terms of the\nCreative Commons Attribution License (CC BY).\nThe use, distribution or reproduction in other\nforums is permitted, provided the original\nauthor(s) and the copyright owner(s) are\ncredited and that the original publication in this\njournal is cited, in accordance with accepted\nacademic practice. No use, distribution or\nreproduction is permitted which does not\ncomply with these terms.\nFrontiers inPharmacology frontiersin.org01\nTYPE Review\nPUBLISHED 24 December 2024\nDOI 10.3389/fphar.2024.1458739\nproperties of a promising drug candidate to improve its\ntherapeutic potential and reduce side effects. Nowadays, the\nstrategic application of AI in drug discovery has signi ﬁcantly\nhastened the development timeline and diminished both the cost\nand duration of early-stage drug discovery phases (Chakraborty\net al., 2023; Lamberti et al., 2019).\nLLMs are cutting-edge AI systems crafted on the foundation\nof neural network architectures and reﬁned through exposure to\nhuman language from a plethora of sources, including articles,\nbooks, and news reports. Consequently, LLMs are capable of\ncapture the complicated associative relationships between words\nin a text-based training dataset, harnessing the capabilities of\ndeep learning (Thirunavukarasu et al., 2023). These models have\nbeen effectively integrated into numerousﬁelds, demonstrating a\nversatility that encompasses dialogue and beyond. In particular,\nthe recent surge in advancements within LLMs have paved the\nway for their integration into healthcare and biotechnological\npharmaceutics (Liu et al., 2023a). Boasting the capacity to execute\na multitude of language-centric tasks, LLMs capitalize on neural\nnetworks and are trained on vast repositories of text generated by\nhumans, thus transforming them into invaluable assets for\ninformation retrieval and the delivery of biomedical insights.\nConsequently, they can serve as valuable tools for retrieving\ninformation and providing biomedical solutions\n(Thirunavukarasu et al., 2023 ; Eggmann et al., 2023 ). Among\nthe vanguard of AI-powered LLMs, ChatGPT stands as a notable\nexample, which was instrumental in streamlining the drug\ndiscovery process (De Angelis et al., 2023). Typically, LLMs in\ndrug development could be utilized for understanding disease\nmechanisms, designing and optimizing drug molecules,\npredicting ef ﬁcacy and safety, integrating with AI tools,\ntranslating between molecules a nd indications, and exploring\nfederated learning for enhan ced data utilization and task\ngeneralization. Especially, LLMs can be integrated with other\nAI technologies like machine learning and computational biology\ntools to synergistically accelera te drug discovery. For example,\nmachine learning algorithms can analyze vast databases to\nidentify intricate patterns, leading to the discovery of novel\ntherapeutic targets and prediction of potential drug candidates\nwith better accuracy and speed. Quantitative struc ture-activity\nrelationship (QSAR) modeling and molecular docking\nsimulations are AI-driven predi ctive techniques that provide\ninsights into predicting the biological activity of novel\ncompounds with great accuracy.\nThere is a strong belief that the rapid evolution and widespread\nadoption of AI are deﬁning trends of our time. In this review, the\nrecent development of LLMs was highlighted, including their\narchitectural frameworks and operational mechanisms.\nFurthermore, the manuscript has placed a special emphasis on\nthe practical applications of Large Language Models (LLMs) in\nthe biopharmaceutical sector. Several successful case studies were\ndetailed to highlight the strengths and limitations of these models.\nEach case study includes an in-depth analysis and critical evaluation\nof the respective models. It is especially crucial to diligently evaluate\nand address the associated concerns, risks, and potential pitfalls\n(Borji, 2023). It is believed that the integration of LLMs in drug\ndiscovery would greatly facilitate the acceleration of the drug\ndiscovery pipeline.\n2 Designing an artiﬁcial intelligence-\ndriven platform\n2.1 Large language models (LLMs) for drug\ndevelopment\nArmed with the capabilities of natural language processing\n(NLP) and machine learning technologies, chatbots have\ndemonstrated signi ﬁcant potential and have made substantial\ncontributions across several ﬁelds (Haque and Rubya, 2023; Xu\net al., 2021; Suppadungsuk et al., 2023). In particular, the emergence\nof ChatGPT, with its harnessing of generative models, has\nheightened global awareness of the vast of generative AI (Sallam,\n2023). As this technology continues to evolve, this section aims to\noffer a comprehensive of the recent advances in LLMs within the\nrealm of biotechnological pharmaceutics.\n2.1.1 ChatGPT\nThe chat generative pretrained transformer (ChatGPT),\ndeveloped by OpenAI, stands at the forefront of language model-\nbased chatbots, renowned for its conversational interactivity\n(https://openai.com/blog/chatgpt). On 14 March 2023, an\nupgraded version, GPT-4, was launched, which boasts improved\ncapabilities for addressing complex issues with heightened precision\nand rationality. Furthering its progression, on 6 November 2023, the\nstate-of-the-art model, GPT-4 Turbo, was introduced. This iteration\nis marked by its superior performance, an updated knowledge cutoff\ndate of April 2023, and the introduction of a 128k context window,\nequating to the processing capacity of approximately 300 pages of\ntext within a single prompt. Utilizing a neural network to process\nnatural language, it is adept at generating contextually relevant\nresponses and delivering nuanced, sophisticated answers through\nadvanced modeling techniques (Brown et al., 2020).\nGenerally, ChatGPT can be harnessed in the following\ncapacities. Primarily, ChatGPT serves as an intuitive interface a\nthat facilitates more straightforward interactions between users and\nvarious AI systems, offering an alternative to traditional knowledge\ngraph navigation. Indeed, it has emerged as a leading example of\nsophisticated human – computer interaction (HCI). Secondly,\nChatGPT can be speci ﬁcally applied for drug discovery,\nfunctioning as an advanced search engine tailored to the nuances\nof biological science in different ways (Savage, 2023). For rational\ndrug design, ChatGPT could be used to generate innovative\nchemical structures with a high potential for clinical success and\npredict the absorption, distribution, metabolism, excretion, and\ntoxicity (ADMET) proﬁles of the identiﬁed compounds (Savage,\n2023; Zhao and Wu, 2023). Within this domain, efﬁcient screening\ncan be performed with ultra-large virtual libraries (greatly expanded\ndrug-like chemical spaces), which would signiﬁcantly amplify the\ndrug-like chemical spaces and enhancing the probability of hit\nidentiﬁcation and lead discovery. Thirdly, ChatGPT holds\npromise in the generation of new protein targets for drug\ndevelopment. When equipped with extensive unlabeled data (e.g.,\nthe nearly 250 million protein sequences contained in the UniProt\ndatabase and 1.28 million protein sequences contained in the PDB\ndatabase), ChatGPT can autonomously deduce the intricate\nrelationships between molecular building blocks on its own. In\nthis ﬁeld, its functionality mirrors that of AlphaFold, which\nFrontiers inPharmacology frontiersin.org02\nLiu et al. 10.3389/fphar.2024.1458739\nFIGURE 1\nSchematic representation of the ChatGPT-assisted drug addiction research process. (This process initiates with grasping the roles of AI in drug\naddiction research and generating drug-like molecules. It advances by reﬁning the GNC model to produce leads targeting DAT, NET, and SERT receptors.\nOptimization is driven by the Langevin equation and an enhanced GNC model, prioritizing binding afﬁnity predictions, which led to 15 potential drug leads\npinpointed. ChatGPT supports this process by offering creative input, methodological insights, and coding aid, from debugging to interpretation).\nFIGURE 2\nSchematic of a GRU-enhanced autoencoder for drug design: from SMILES encoding to cocaine addiction treatment leads. (The molecular\ngeneration pipeline uses a GRU-based autoencoder to encode and decode SMILES strings, and the stochastic generator was used to create novel\nmolecules. The process starts from input SMILES through latent space manipulation to generated SMILES. Subsequently ADMET screening and binding\nafﬁnity prediction were achieved for the identiﬁcation of potential cocaine addiction treatment leads).\nFrontiers inPharmacology frontiersin.org03\nLiu et al. 10.3389/fphar.2024.1458739\ndepends heavily on (new) big data analytics and artiﬁcial intelligence\nfor its operations.\nIn a study byWang et al. (2023a), ChatGPT was successfully\napplied for the discovery of an anti-cocaine-addiction drug, which\nfunctions as a virtual guide offering strategic and methodological\ninsights, as well as generative models for optimal drug-like\nmolecules with desired properties ( Figure 1). With the aid of\nChatGPT, a novel platform named the Stochastic Generative\nNetwork Complex (SGNC) was developed. With this project,\nChatGPT primarily serves for idea generation, methodology\nclariﬁcation, and coding assistance ( Figure 2 ). For idea\ngeneration, ChatGPT was augmented with three plugins\n(WebPilot, ScholarAI, and AskYourPDF), which improves its\ncapacity to comprehend the research background of anti-cocaine-\naddiction drug development, providing up-to-date available sources\nand accessing insights from previous works. For methodology\nclariﬁcation, plugins (WebPilot, Link Reader and Wolfram) were\nused to signi ﬁcantly improve the mathematical and statistical\ncapabilities of ChatGPT. In terms of coding assistance, WebPilot,\nChatwithGit, and Prompt Perfect were leveraged to reﬁne coding\nskills and craft perfect prompts.\nChatGPT has been instrumental in successful identiﬁcation of\n15 promising drug leads capable of targeting the dopamine\ntransporter (DAT), norepinephrine transporter (NET), and\nserotonin transporter (SERT). It was clearly indicated that the\n“cognitive abilities ” of ChatGPT have the potentials to\nsigniﬁcantly streamline the development of modern\npharmaceuticals offering potential promising avenues for drug\ndiscovery. However, it was also noted that the application of\nChatGPT for drug development still faces many challenges due\nto the inherent limitations of generative AI. For example, it is still\nsusceptible to generate false narratives and spread misinformation.\nConsequently, it is recommended that the information generated by\nChatGPT-4 undergo rigorous and consistent veriﬁcation to ensure\nits accuracy and reliability.\nMondal et al. explored the proﬁciency of ChatGPT in predicting\nand elucidating common drug-drug interactions (DDIs) (Figure 3)\n(Juhi et al., 2023). Initially, a curated set of 40 pairs of previously\nlisted DDIs were selected for analysisvia ChatGPT through a two-\ntiered questioning approach. The outcomes showed that for the\ninitial query, one response was incorrect, while of the correct\nresponses, 19 were deﬁnitive and 20 remained ambiguous. For\nthe second question, one answer was deemed incorrect, with\n17 correct answers being de ﬁnitive and 22 being inconclusive.\nThese results suggest that ChatGPT serves as a moderately\neffective instrument for assessing DDIs; however, it occasionally\nfalls short in offering comprehensive guidance, indicating the\nnecessity for further re ﬁnements to enhance its accuracy and\nreliability.\nZhang et al. investigated the competencies of ChatGPT in the\nrealms of question-answering, knowledge discovery, and knowledge\nreasoning within the biomedical ﬁeld, speciﬁcally its ability to\nestablish connections between pairs of proposed entities. The\nperformance of ChatGPT was then compared with existing\nbiomedical knowledge graphs (BKGs) ( Hou et al., 2023 ). The\nﬁndings indicated that ChatGPT-4.0 outperformed BKGs in\nterms of providing existing knowledge, although BKGs had a\nhigher conﬁdence level and demonstrated higher reliability in\nterms of information accuracy. Moreover, compared with BKGs,\nChatGPT demonstrated a limited ability to perform novel\ndiscoveries based on the existing information and to provide\nreasoning for knowledge discovery. Therefore, the study\nproposed that strategies integrating LLMs (like ChatGPT) and\nBKGs could be promising to enhance task performance and\nmitigate potential risks.\nXu et al. delved into the prowess and promise of ChatGPT\nwithin the realm of biomedical information retrieval, with a\nparticular focus on its ability to discern associations between\ndrugs and diseases (Gao et al., 2023). Their ﬁndings showed that\nChatGPT achieved an impressive accuracy range of 74.6%– 83.5% in\nidentifying drug-disease associations and an even more remarkable\n96.2%– 97.6% for true and false pairs under varying prompt designs.\nThis revealed that ChatGPT could serve as a valuable\n“assistant” in\nunearthing knowledge related to biotechnological and\npharmaceutical advancements, with a level of reliability that is\nquite satisfactory. Nevertheless, it was also emphasized that the\ninsights gleaned from ChatGPT should undergo thorough\nveriﬁcation before being integrated into clinical practice. In a\nseparate study, ChatGPT was employed to meticulously annotate\nsingle-cell RNA sequencing data, successfully correlating rare cell\ntypes with their functions and uncovering several distinct\ndifferentiation pathways of cell subtypes that had previously\neluded detection (Zehua and Du, 2023).\nBlatz et al. demonstrated the transformative potential of\nChatGPT and other LLMs in the ﬁeld of dental medicine\n(Eggmann et al., 2023 ). It was concluded that LLMs (e.g.,\nChatGPT) could be instrume ntal in several areas ( Sadybekov\nand Katritch, 2023 ): revolutionizing dental practice by\nstreamlining administrative tasks ( Murray et al., 2023 );\nenhancing dental telemedicine through real-time language\ntranslation services, thereb y making consultations more\naccessible and scalable, particu larly in underserved regions\n(Cova et al., 2022 ); bolstering clinical decision support by\nswiftly summarizing volu minous medical records or\naggregating evidence-based medical ﬁndings ( Roggia et al.,\n2024); expediting administra tive tasks such as routine\ncorrespondence and record-keeping ( Pandey et al., 2022 );\nenriching patient education w ith credible health advice and\nFIGURE 3\nThe studyﬂowchart to predict and explain drug-drug\ninteractions with ChatGPT.\nFrontiers inPharmacology frontiersin.org04\nLiu et al. 10.3389/fphar.2024.1458739\nguidance ( Jayatunga et al., 2022 ); advancing dental education\nthrough the creation and administration of multiple-choice\nexams, practical assessments, and supervised patient\ntreatments; and ( Chakraborty et al., 2023 )r eﬁning scienti ﬁc\nwriting, making it more cohe rent for non-native English\nspeakers. In addition, the study underscored the imperative to\naddress several critical challenges effectively ( Sadybekov and\nKatritch, 2023 ): ensuring robust cybersecurity measures to\nsafeguard patient data and medical information against\nmalware attacks (Murray et al., 2023); implementing stringent\npatient data privacy protections to maintain conﬁdentiality and\nsecurity; and (Cova et al., 2022) conducting thorough scientiﬁc\nevaluations and veri ﬁcations of LLM-generated responses to\nmaintain accuracy and reliability.\nAs highlighted in numerous studies, ChatGPT is poised to exert\nprofound inﬂuences on various aspects of natural science and social\nscience in the near future. On the one hand, it is imperative to\nrecognize that at present, ChatGPT might not be a fully-ﬂedged\n“sage” capable of providing responses replete with adequate\nreasoning and evidence-based justi ﬁcations ( Heck, 2023 ).\nConsequently, robust quality control protocols must be\nestablished to safeguard the accuracy, credibility, privacy, and\ncybersecurity of the swift and beneﬁcial information dispensed by\nChatGPT. On the other hand, there remains a critical need to\nimprove the “intelligence” of ChatGPT, thereby transforming it\ninto an indispensable asset for researchers.\n2.1.2 Google bard and microsoft bing\nBard AI, a cutting-edge large language model developed by\nGoogle, signiﬁes a new frontier in theﬁeld of AI-powered chatbots\n(Pichai, 2023). Propelled by the Language Model for Dialogue\nApplications (LaMDA, a state-of-the-art transformer-based\nneural language model), Bard is honed on an expansive dataset,\nensuring its proﬁciency in conversational AI. Sharing a repertoire of\ncapabilities with ChatGPT, Google Bard has demonstrated its\nefﬁcacy across a spectrum of scientiﬁc applications, marking it as\na formidable contender in the landscape of advanced AI\ntechnologies.\nSulaiman et al. conducted a study to assess proﬁciency of Google\nBard in critically evaluating DDI screening, and it was subsequently\ncompared with the authorized Lexicomp\n® Online™ database\n(Sulaiman et al., 2023). The interrater reliability analysis revealed\na minimal concordance between Lexicomp and Google Bard in\nassessing DDI risk, with a Cohen’s kappa (κ) value of 0.01; similarly,\na slight agreement between was observed in their severity ratings\n(κ = 0.02). However, there was a lack of consensus regarding the\nreliability rate, reﬂected by aκ value of−0.02. In a parallel study, AI\nplatforms (ChatGPT, Bard, and Bing) were applied for DDI the\nprediction, and the sensitivity, speciﬁcity, and accuracy of each\nmodel were subsequently evaluated ( Al-Ashwal et al., 2023 ).\nNotably, Microsoft Bing emerged as the top performer in terms\nof speciﬁcity (0.769) and accuracy (0.788). Furthermore, ChatGPT-\n3.5 and ChatGPT-4 exhibited the greatest variability in the\nconsistency of their accuracy, highlighting the nuances in their\npredictive capabilities.\nCheungpasitporn et al. conducted a comparative analysis of\nthe performance of various AI models (ChatGPT 3.5, ChatGPT 4,\nBard AI, and Bing Chat) in identifying potassium and\nphosphorus content in foods (Qarajeh et al., 2023). The study\nrevealed that ChatGPT 4 outperformed others in determining\npotassium content, achieving an overall accuracy of 81%, with\nspeciﬁc rates of 60% for low-potassium foods and an impressive\n99% for high-potassium foods. Comparatively, ChatGPT 3.5,\nBard AI and Bing Chat showed accuracies of 66%, 79% and\n81% accuracy, respectively. In the realm of phosphorus content\nidenti ﬁcation, Bard AI stood out with a perfect 100% accuracy\nrate; in contrast, ChatGPT 3.5, ChatGPT 4 and Bing Chat\nmanaged to correctly identify high-phosphorus foods only\n85%, 77% and 89% of the time, respectively. These ﬁndings\nillustrate the promising pote ntial of AI-powered models in\nsupporting renal diet management, particularly as adjunct\ntools for enhancing nutritional education and counseling.\nNonetheless, it is evident that further enhancements are\nessential to achieve the desired levels of precision and reliability.\nTham et al. evaluated the pro ﬁciency of ChatGPT-3.5,\nChatGPT-4.0, and Google Bard in generating accurate responses\nto inquiries concerning myopia (Beutel et al., 2023). Frequently\nasked myopia care-related questions were categorized into six\ndifferent domains and allocated to the AI models. The responses\ngenerated were subsequently reviewed independently by three\nexpert ophthalmologists, who graded them as poor, borderline, or\ngood. A consensus approach was then used to establish theﬁnal\nassessment of each reply. The results showed that ChatGPT-\n4.0 outperformed in terms of accuracy, with 80.6% of the\nresponses deemed ‘good’, surpassing 54.8% for Google Bard\n(Google Bard: 4.35 and ChatGPT-4.0: 4.23). All the models\nshowed high average comprehensiveness scores and signi ﬁcant\nself-correction capabilities (66.7% for ChatGPT-4.0% and 60%\nfor Google Bard). This highlighted the potential of ChatGPT-\n4.0 and Google Bard to deliver essential answers to myopia-\nrelated queries, although it is clear that their accuracy requires\nfurther enhancement and rigorous evaluation.\nIn a recent investigation, the capacity of ChatGPT and Google\nBard to generate professional-quality responses to inquiries\nregarding ocular symptoms were systematically examined\n(Pushpanathan et al., 2023 ). The answers procured were\nmeticulously appraised and graded by ophthalmologists at the\nconsultant level, based on criteria of accuracy,\ncomprehensiveness, and self-awareness. ChatGPT-4.0 achieved an\nimpressive ‘good’ rating of 89.2%, signi ﬁcantly outperforming\nGoogle Bard, which registered at 40.5%. Although all the models\ngarnered high mean comprehensiveness scores, they were\nconcurrently found to display inadequate self-awareness\ncapabilities. Parallel results were also observed in the accuracy of\nChatGPT and Google Bard when addressing clinical radiology\nchallenges on the Japan Radiology Board Examination (JRBE)\n(Toyama et al., 2023).\nTherefore, although ChatGPT-4.0 demonstrated a distinct\nadvantage in providing logical answers to a broad spectrum of\ninquiries, rigorous validation remains essential to ensure\nreliability and accuracy. In the context of research writing and\ndata collection, it is worth noting that while Bard represents a\nmodest improvement over ChatGPT (ChatGPT3.5) in analyzing the\ndiversity of manuscript bibliographies, it still falls short of reference\nidentiﬁcation capabilities, even with its integration with Google\nsearch (\nKing, 2023).\nFrontiers inPharmacology frontiersin.org05\nLiu et al. 10.3389/fphar.2024.1458739\n2.1.3 Med-PaLM\nRecently, Google and DeepMind introduced MultiMedQA,\nwhich is a comprehensive collection of seven medical question-\nanswering datasets ( Figure 4 ), including LiveQA, MedQA,\nMedMCQA, MedicationQA, Pub MedQA, HealthSearchQA,\nand MMLU clinical topics ( Singhal et al., 2023 ). Utilizing\nMultiMedQA as a foundation, both the pathway language\nmodel (PaLM) ( Chowdhery et al., 2022 ) and its instruction-\ntuned derivative, Flan-PaLM, were subjected to rigorous\nexamination ( Chung et al., 2022 ). The results showed that\nFlan-PaLM excelled in achie ving the highest historical\naccuracy on the aforementioned d atasets. Notably, Flan-PaLM\nscored an impressive 67.6% accuracy on MedQA (US Medical\nLicensing Exam-style questions), surpassing the previous state of\nthe art by over 17%. These results prompted the implementation\nof prompt tuning to further specialize Flan-PaLM for the medical\ndomain, resulting in the Med-PaLM model. Med-PaLM was\ncapable of providing more favo rable answers to the medical\nqueries than clinicians, althou gh its overall performance was\nstill somewhat inferior to that of medical professionals. This\ndemonstrated the effectiveness o f instruction prompt tuning in\nimproving the performance of Med-PaLM.\nHowever, the study also identiﬁed limitations and proposed\nfuture research direction. These include expanding MultiMedQA to\nbetter mirror real-world clinical workﬂows, developing key LLM\ncapabilities for numerous clinically signiﬁcant applications, reﬁning\nhuman evaluation, and addressing issues of fairness, equity, and\nethical considerations.\n2.1.4 DrugChat\nIn a cutting-edge study, a pharmaceutical domain-speciﬁc LLM\nprototype, DrugChat, was developed to utilize ChatGPT-like\ncapabilities for the analysis of drug compounds and the provision\nof insights on drug– molecule graphs (Liang et al., 2023). Like\nChatGPT, DrugChat engages in multi-turn, interactive dialogues\nto address inquiries about uploaded compound molecule graphs. It\nis composed of three core components: a graph neural network\n(GNN), a large language model (LLM), and an adaptor, all of which\nare trained in an end-to-end fashion.\nThe GNN is tasked with interpreting the input compound\nmolecule graph and extracting a meaningful representation. The\nadaptor then converts this graph representation into a format that is\ncompatible with LLM. The LLM processes the transformed\ncompound representation alongside the questions posed about\nthe compound, ultimately generating answers.\nFor the instruction tuning phase, datasets comprising\n10,834 drug compounds and 143,517 question-answer pairs were\ncurated to train DrugChat. In this process, a pretrained GNN and a\npre-trained Vicuna-13b model were utilized, with the weight\nparameters for the GNN and LLMs being ﬁxed. However, the\nweight parameters for the adapter were continuously reﬁned. The\nresults demonstrated the proﬁciency of DrugChat in responding to\nvarious questions about the input compounds, such as“What makes\nthis compound unique?” and “What diseases might this compound\nbe able to treat?”, even when evaluated on the drug compound\ngraphs not present in the training data.\nHowever, as highlighted, the most signi ﬁcant challenge for\nDrugChat could be the phenomenon of “artiﬁcial (molecular)\nhallucinations” stemming from the implanted LLMs. The\ngeneration of unreliable answers and descriptions could seriously\nimpede its practical application in drug discovery, potentially\nleading to undesirable consequences.\n2.1.5 MolReGPT\nRecently, a groundbreaking LLM-based system known as\nMolReGPT has been developed, which showcases the ability to\ntranslating molecule captions into natural language (Li et al., 2023).\nThis system employs a retrieval-based prompt paradigm through in-\ncontext learning for both molecule captioning and text-based\nmolecule generation. This could potentially revolutionize\nmolecule discovery with MolReGPT without the need for ﬁne-\ntuning. MolReGPT is structured around four principal\ncomponents ( Figure 5 ), including molecule caption retrieval\n(identifying the n most analogous examples), prompt\nmanagement (constructing the system prompt), in-context few-\nshot molecule learning (translating molecule caption), and\ngeneration calibration (assessing validity). For the task of\nmolecule caption retrieval, MolReGPT leverages Morgan\nﬁngerprints for molecule captioning and BM25 for text-based\nmolecule generation. Prompt management encompasses four key\nsteps: role identiﬁcation, task description, example generation, and\ninstruction output.\nFIGURE 4\nT Overview of the benchmark MultiMedQA, PaLM and Med-PaLM. (Various medical question-answering datasets are integrated into a multimodal\nframework, which would improve the performance of PaLM through prompting and instruction tuning for clinical applications).\nFrontiers inPharmacology frontiersin.org06\nLiu et al. 10.3389/fphar.2024.1458739\nThe results indicated that MolReGPT surpasses the performance\nof the tested ﬁne-tuned models (e.g., MolT5-base) without any\nadditional ﬁne-tuning, achieving Text2Mol scores of 0.560 for\nmolecule captioning and 0.571 for molecule generation. In terms\nof molecule understanding and text-based molecule generation,\nMolReGPT is comparable to the ﬁne-tuned model MolT5-large.\nThese ﬁndings suggest that MolReGPT could provide an innovative\nand adaptable platform for harnessing the potential of LLMs to\nadvance molecule discovery through in-context learning, which\nmight greatly reduce the cost associated with domain transfer.\n2.1.6 Chemformer\nTo tackle the resource-intensive challenge and multitasking\ndemands in cheminformatics, a transformer-based model named\nChemformer has been introduced, leveraging SMILES notation\n(Irwin et al., 2022). Chemformer, which is based on the BART\nlanguage model, is versatile and can be readily applied to diverse\ntasks, such as sequence-to-sequence (e.g., reaction prediction and\nmolecular optimization) and discriminative cheminformatics (e.g.,\nproperty prediction) tasks, with the encoder stack alone being\nsufﬁcient for many of these applications.\nThe training of Chemformer primarily consists of two stages\n(Figure 6): self-supervised pretraining and downstreamﬁne-tuning.\nIn the pretraining phase, extensive unlabeled SMILES datasets are\nused for model training through three different self-supervised\npretraining tasks (masking, augmentation and a combination of\nmasking and augmentation). During the ﬁne-tuning phase, the\npretrained Chemformer is tailored to a speciﬁc downstream task\nand further reﬁned. A multitask learning strategy is utilized in this\nprocess to optimize multiple tasks concurrently, such as\nmultiproperty prediction and multigene activity prediction. In\nparticular, Chemformer has achieved the highest accuracy\navailable on benchmark datasets for direct synthesis and\nretrosynthesis prediction.\nThe outcomes for chemical reaction prediction, molecular\noptimization and property prediction demonstrate the\nadaptability of Chemformer to various downstream tasks. The\nconvergence rate and performance of Chemformer on\ndownstream tasks could be improved by self-supervised\npretraining. When training time is limited, the synergy of\ntransfer learning and the innovative augmentation strategy can\nproduce state-of-the-art results across all the tested downstream\nFIGURE 5\nThe workﬂow of MolReGPT. (Thisﬁgure illustrates a four-step process for generating molecular captions and text-based molecular designs. Initially,\nit retrieves examples from a database using molecular information. Subsequently, it curates prompts with system instructions for tasks like molecular\ncaptioning and generation. In the third phase, it uses a few examples to facilitate a language model learn and generate responses. Finally, it calibrates the\ngeneration to ensure the responses are accurate and in the correct format for tasks like Mol2Cap, where it describes molecules).\nFrontiers inPharmacology frontiersin.org07\nLiu et al. 10.3389/fphar.2024.1458739\nSeq2seq tasks, including chemical reaction prediction and molecular\noptimization.\n2.1.7 MolGPT\nTo develop a generative pretraining (GPT) model adept at\ngenerating chemical structures with tailored properties or\nsynthesizing drug-like molecules, a transformer-decoder-based\ngenerative model named MolGPT has been proposed ( Bagal\net al., 2022 ). MolGPT is comprised of three principal\ncomponents (Figure 7): the input encoder, transformer-decoder\nmodel, and output decoder. The input encoder translates target\nmolecules represented in the Simpliﬁed Molecular Input Line Entry\nSystem (SMILES) notation into a string of characters. The\ntransformer-decoder model is comprised of multiple transformer\nmodules and a single decoder module. Each transformer module\nfeatures a multi-head masked self-attention mechanism calculated\nby the“Scaled Dot Product Attention”, and a feed-forward network\ndesigned to capture contextual information of the input sequence.\nThe decoder module employs both self-attention and an encoder-\ndecoder attention mechanism to generate the subsequent SMILES\ntoken. The output decoder then translates the generated SMILES\nstring into a molecular structure. In this process, the resultant\nmolecules can be generated based on desired single or multiple\nproperties, a speciﬁed scaffold, or a combination of both.\nBenchmarking experiments for training and evaluation\ndemonstrated that MolGPT boasts exceptionally high validity and\nuniqueness scores, along with commendable Frechet ChemNet\nDistance (FCD) and KL divergence scores for the MOSES and\nGuacaMol datasets. Moreover, in terms of validity and novelty,\nMolGPT outperformed all the other tested methods for the\nGuacaMol dataset. In addition, MolGPT was found to acquire\nhigher-level chemical representations through molecular property\ncontrol, enabling the generation of molecules with intriguing\nproperties or speciﬁed scaffolds. Based on these results, MolGPT\nis poised to play a critical role in the realm of rational drug design.\nIn a recent advancement, a conditional generative pretrained\ntransformer model, cMolGPT, was designed for the autoregressive\ngeneration of target-speci ﬁc de novo molecules using natural\nlanguage processing (NLP) techniques (Wang et al., 2023b). This\nmodel was initially pretrained on an extensive SMILES dataset,\nenabling it to learn the parametric probabilistic distribution of drug-\nlike properties (e.g., LogP and molecular weight), across the SMILES\nvocabulary space in an unsupervised manner. The cMolGPT model\nincorporates a of key-value pairs within the transformer\narchitecture, which is further enforced by target-speci ﬁc\nembeddings to facilitate the conditional generation of multihead\nattention for drug-like compounds. The ﬁndings revealed that\ncMolGPT is adept at generating SMILES strings that represent\nboth drug-like and active compounds. In addition, the generated\nFIGURE 6\nThe workﬂow of Chemformer. (It shows a two-step process for training a molecular model. In Step 1, the model is pre-trained on 100 million\nmolecules, where it learns to encode and decode molecular structures. In Step 2, the model isﬁne-tuned for speciﬁc tasks, such as predicting reactions,\noptimizing molecules, and estimating molecular properties. Each task uses an encoder to understand the molecule and a decoder to generate the result).\nFIGURE 7\nThe architectural structure of MolGPT.\nFrontiers inPharmacology frontiersin.org08\nLiu et al. 10.3389/fphar.2024.1458739\ncompounds not only closely resemble the chemical space of actual\ntarget-speciﬁc molecules but also encompass a signiﬁcant portion of\nnovel compounds. To assess the performance in generating target-\nspeciﬁc compounds, evaluations were conducted on three target-\nbiased datasets: EGFR, HTR1A, and S1PR1. The compounds\ngenerated by cMolGPT were predicted to exhibit higher activity\ncompared to those generated by the tested conditional RNN models.\nThis suggests that cMolGPT is a promising tool in theﬁeld of drug\ndiscovery, capable of expanding the chemical space of potential\ntherapeutic agents.\nTo encapsulate the ﬁndings, it is evident that a transformer-\ndecoder-based generative model could achieve state-of-the-art\nperformance in the rational design and discovery of desired\nchemical structures. Consequently, such a model is regarded as\nan invaluable asset in the realm ofde novodrug design, showcasing\nits potential to revolutionize the way we approach the development\nof novel therapeutics.\n2.1.8 MOLGEN\nTo synthesize molecules with speci ﬁc desired attributes,\nMOLGEN, a sophisticated pretrained molecular language model,\nhas been recently introduced (Fang et al., 2023b). This system\nencompasses two pivotal stages ( Figure 8 ): ( Sadybekov and\nKatritch, 2023 ) a two-stage domain-agnostic molecular\npretraining and (Murray et al., 2023) a self-feedback mechanism\ndesigned to mitigate the occurrence of“molecular hallucinations”.\nDuring the initial phase, the system reconstructs over 100 million\nmolecules using SELFIES, a highly robust molecular language. This\napproach is complemented by the introduction of the domain-\nagnostic molecular preﬁx, which improves the transferability of\nthe knowledge across diverse domains. The subsequent stage\nintroduces a self-feedback paradigm, which is instrumental in\nﬁne-tuning the model’s parameters in accordance with generative\nprobabilities, thereby incrementally reﬁning the optimization of the\ngenerated molecules. This mechanism is pivotal in enabling\nMOLGEN to produce molecules with desired properties while\ncircumventing the pitfalls of“molecular hallucinations”.\nThe efﬁcacy of MOLGEN was subjected to a rigorous evaluation\nthrough extensive testing on established benchmarks. The\nassessments focused on its ability to accurately capture molecular\ndistributions, generate diverse and realistic molecules, pinpoint\ntargeted molecules and re ﬁne molecules under constraints.\nAcross the domains of natural products and synthetic molecules,\nMOLGEN has consistently its proﬁciency in generating molecules\nthat align with desired chemical preferences (e.g., logP\n(octanol– water partition coefﬁcient), QED (quantitative estimate\nof drug likeness). Moreover, it has demonstrated a notable potential\nfor identifying essential molecular substructures and navigating the\nchemical space, highlighting its value in the realm of molecular\ndesign and drug discovery.\n2.1.9 KV-PLM\nRecognizing the limitation s of current machine reading\nmodels, which tend to handle various data types separately, a\nsigniﬁcant divide often emerg es between the nuanced\ninterpretation of molecular structures and the absorption of\nknowledge from biomedical literature. To address this,\nRecently, a groundbreaking ma chine reading system known as\nKV-PLM has been introduced. It is designed to seamlessly\nintegrate molecular structure data with biomedical text within\na single deep learning architecture (Figure 9)( Zeng et al., 2022).\nThe KV-PLM leverages the pretrained language model\nBERT12 as its foundational component. It employs the\nsimpli ﬁed molecular-input line-e ntry system (SMILES) to\nencode molecular structures into a format compatible with the\nbyte pair encoding (BPE) algorithm. This encoding process\ntransforms SMILES strings int o a series of substring patterns.\nThese patterns are then integrated into a comprehensive\nbiomedical data and subjected to pretraining under a uni ﬁed\nlanguage modeling framework. The culmination of this process is\nFIGURE 8\nThe architectural structure of MOLGEN. (In Step 1, the model learns molecular language syntax and meaning by encoding and decoding molecular\nstructures. In Step 2, the model isﬁne-tuned with domain-agnostic molecular preﬁxes to improve the capability of understanding molecular structures. In\nthis process, a self-feedback paradigm was included, where the model is capable of generating candidates, evaluating them based on properties, and\nlearns from the feedback to enhance the performance for generating synthetic and natural product molecules).\nFrontiers inPharmacology frontiersin.org09\nLiu et al. 10.3389/fphar.2024.1458739\nthe acquisition of meta-knowl edge through a self-supervised\nlanguage model, which can be efﬁciently adapted through ﬁne-\ntuning for speciﬁc application within the biomedical domain.\nTo evaluate the efﬁcacy of KV-PLM, a range of mono-source\nbiomedical tasks were conducted, encompassing both molecular\nstructure-related and biomedical text-related tasks. The system’s\nperformance was benchmarked on MoleculeNet for SMILES\nproperty classi ﬁcation across datasets such as BBBP, SIDER,\nTOX21, and HIV, as well as for chemical reaction classiﬁcation\nwas evaluated on USPTO 1k TPL dataset. Additionally, the system\nwas evaluated on biomedical named entity recognition (NER) and\nthe relation extraction (RE) task using BC5CDR and ChemProt. The\nresults indicated that KV-PLM model not only outperformed other\nmodel in these tasks but also demonstrated an ability to engage in\nknowledgeable and versatile reading.\nMoreover, the system’s proﬁciency in versatile reading tasks\ninvolving “cross-information retrieval ”, “match judging ”, and\n“human professional performance ” was con ﬁrmed. KV-PLM\nexcelled in these versatile tasks, notably in facilitating effective\ncross retrieval between substances and property descriptions.\nThese capabilities highlight the immense potential of KV-PLM in\nthe realms of novel drug discovery and molecular property\nprediction, offering researchers a tool to gain a holistic and in-\ndepth comprehension of molecular entities.\n2.1.10 MolT5\nTo facilitate ef ﬁcient communication between molecules\nstructures and natural language, and to address challenge of\nlimited data, Molecular T5 (MolT5), a self-supervised learning\nframework, was developed ( Edwards et al., 2022 ). This\nFIGURE 9\nThe architectural structure of KV-PLM. (The workﬂow of KV-PLM starts with internal molecular structure serialization into SMILES format, then the\nMeta-knowledge mapping is utilized to correlates molecular features to human intelligence. External biomedical text would subsequently be fused with\nmolecular data. The language model pre-training would empower the model with the ability to predict molecular properties. It enables knowledgeable\nmachine reading across different molecular properties (e.g., molecular structure, chemical properties, natural language documentation, and\nstructured biomedical knowledge), which would facilitate tasks like description generation, property prediction, and knowledge extraction).\nFrontiers inPharmacology frontiersin.org10\nLiu et al. 10.3389/fphar.2024.1458739\nframework employs a transformer-based architecture (Figure 10),\nleveraging the T5 that has been pretrained text-to-text model,\nenabling the simultaneous processing of vast amounts of\nunlabeled natural language text and molecular string data for\npre-training purposes. In pursuit of this objective, two new tasks\nwere introduced and formalized: molecular language tasks,\nspeciﬁcally molecule captioning, and text-guided de novo\nmolecule generation. Although these molecule – language tasks\nshare similarities with vision – language tasks, there present\ndistinct challenges, particularly the increased complexity of\nmolecular captioning due to the diverse range of possible\nlanguages used for captioning. The performance of MolT5 was\nrigorously assessed using a suite of evaluation metrics, including\nBLEU, ROUGE, and METEOR, as well as a newly developed cross-\nmodal retrieval similarity metric, the Text2Mol metric. When\nevaluated against the ChEBI-20 dataset using both the Text2Mol\nmetric and BLEU metric, MolT5 achieved superior scores and\noutperformed RNNs and transformers in the two newly deﬁned\ntasks. Notably, the performance of MolT5 further improved as the\nlanguage model size increased.\nMolT5 pretrains models on single-modal data, effectively\nmitigating the issue of data scarcity within the chemical domain.\nFurthermore, a variety metrics were also adopted including a new\ncross-modal embedding-based metric, to evaluate the performance\nof molecule captioning and text-based molecule generation. Results\nshow that MolT5-based models are capable of generating high-\nquality outputs, encompassing both molecules and captions, in\nnumerous instances.\nAs previously mentioned, the effective deployment of\nMolT5 poised to bolster the application of molecular AI,\nempowering researchers to uncov er potential drug candidates\nby engaging with AI through natural language interactions and\nacquiring target chemica l structures with speci ﬁcf u n c t i o n a l\nattributes rather than relying solely on their properties.\nHowever, it is imperative to pay close attention to the\npotential biases introduced by the training dataset, the\nSMILES strings utilized, and the authenticity of the\ncompounds listed in ChEBI-20.\n2.1.11 Text + Chem T5\nTo bridge the gap between human‒machine interactions and\nestablish a cohesive framework for natural language and chemical\nrepresentations, the ﬁrst multidomain, multitask language model,\nMultitask Text and Chemistry T5 (Text + Chem T5 (Figure 11),\nhttps://github.com/GT4SD/gt4sd-core), was developed\n(Christoﬁdellis et al., 2023). This model excels in managing both\nchemical and natural languages in parallel, outperforming others in\ncross-domain tasks across a broad spectrum of NLP-based\nevaluation metrics. Moreover, it negates the need for costly\nmono-domain pretraining and task-speci ﬁc models. The\ncapabilities of Text + Chem T5 was rigorously assessed across a\nrange of tasks, including the predictions of forward and reverse\nchemical reactions, the generation of text-conditional novel\nmolecules, the captioning of molecules spanning various\ndomains, and execution of paragraph-to-action tasks within the\nlinguistic domain. The ﬁndings underscored the effectiveness of\nText + Chem T5 as a versatile multidomain and multitask model,\nadept at generating precise and enlightening captions (with a BLEU-\n2 score of 0.625, a Rouge-1 score of 0.647 and a Rouge-2 score of 0.\n498) and adeptly translating between natural language and the\nSMILES representation of molecules in both text-to-chemistry\nand chemistry-to-text endeavors.\nIn particular, Text + Chem distinguishes itself by its capacity to\nnavigate complex drug discovery workﬂows, such as a hypothetical\nFIGURE 10\nThe workﬂow of MolT5. (MolT5 is trained in two steps (Sadybekov and Katritch, 2023): in pre-training, it learns from public data such as the chemical\nformulas and reactions (Murray et al., 2023). Inﬁne-tuning, it specializes in tasks like creating molecule descriptions and generating new molecules).\nFrontiers inPharmacology frontiersin.org11\nLiu et al. 10.3389/fphar.2024.1458739\nmolecular discovery process, with a uniﬁed model. In this study,\nText + Chem T5 uniquely succeeded in generating the desired\nmolecule for the“text-to-SMILES task”, provide a synthetic route\nidentical to the target reaction for“retrosynthesis”, and conceptually\nsucceed in identifying and proposing an extremely similar reaction\nin a chemistry laboratory for a“paragraphs to actions” task. This\nfascinating capability, previously uncharted, positions Text + Chem\nT5 as superior even to established models such as ChatGPT and\nGalactica 1.3B.\nThe paramount advantage of Text + Chem T5 lies in its\nmultifaceted task management. As indicated in this manuscript,\nText + Chem is poised for targeted application across a variety of\nﬁelds, such as chemical reaction prediction and retrosynthesis,\nsigniﬁcantly and efﬁciently bolstering modern drug development\nFIGURE 11\nThe pipeline of Text + Chem T5. (Text + Chem T5 is capable of dealing with both text and chemistry information, which has a text encoder and a\nchemistry encoder that work together to understand inputs from different domains. The model is capable of understanding a chemical structure and\ngenerate a description for it, or it could receive a description and generate the chemical structure in SMILES format).\nFIGURE 12\nA four-step process of Mol-Instructions for creating and reﬁning molecular descriptions and data. [(Sadybekov and Katritch, 2023) human and AI\nwork together to generate task descriptions about molecules (Murray et al., 2023); information about molecules is gathered from data sources and\nformatted for AI (Cova et al., 2022); biological data is turned into text using templates, such as description of the functions of a speciﬁc protein (Roggia\net al., 2024); the quality of molecules and proteins is checked].\nFrontiers inPharmacology frontiersin.org12\nLiu et al. 10.3389/fphar.2024.1458739\ndiscovery in the physical sciences. It streamlines these processes by\ncircumventing the need for task-speciﬁc ﬁne-tuning and enhancing\nhuman– model interactions.\n2.1.12 Mol-Instructions\nRecently, a novel LLM named Mol-Instructions has been\nintroduced, speci ﬁcally crafted to address the complexities of\nbiomacromolecules, particularly those relevant to structural\nbiology ( Fang et al., 2023a ). This model encompasses a\ncomprehensive instruction dataset that is segmented into three\npivotal components (Figure 12). (Sadybekov and Katritch, 2023)\nMolecule-oriented instructions, which delve into the inherent\nproperties and behaviors of small molecules essential for\nchemical reactions and molecular design (Murray et al., 2023);\nprotein-oriented instructions, which are geared towards\npredicting the structures, functions, and activities of proteins for\nprotein design; and ( Cova et al., 2022 ) biomolecular text\ninstructions, which engage in natural language processing (NLP)\ntasks that are integral to theﬁelds of associated with bioinformatics\nand cheminformatics.\nAs shown in Figure 12, Mol-Instructions demonstrates great\npotential in biomolecular studies. In particular, Mol-Instructions\ncould be applied in three major areas (Sadybekov and Katritch,\n2023): assessment of cross-modal comprehension, which involves\nthe integration of different types of data to enhance understanding\nof biomolecular systems (Murray et al., 2023). Exploration of deeper\nbiomolecular design, enabling the development of more\nsophisticated and effective molecular structures ( Cova et al.,\n2022). Tool learning to address complex biological challenges,\nleveraging advanced computational methods to address intricate\nbiological questions. Mol-Instructions stands as a signi ﬁcant\nadvancement in the integration of computational linguistics and\nmolecular biology, offering a multifaceted approach to\nunderstanding and manipulating biomacromolecules.\nFIGURE 13\nOutline of the ConPLex model architecture and training framework (In step (Sadybekov and Katritch, 2023), a pre-trained protein language model\nand a circularﬁngerprint method are used to analyze molecular structures. In step (Murray et al., 2023), embedding layers process the molecular data,\ncreating a numerical representation. In step (Cova et al., 2022), the model undergoes a series of binary and contrastive epochs to update and reﬁne the\nlearning rate and margin for improved accuracy in predicting molecular interactions).\nFrontiers inPharmacology frontiersin.org13\nLiu et al. 10.3389/fphar.2024.1458739\n2.1.13 ConPLex (https://ConPLex.csail.mit.edu)\nRecently, a sophisticated deep learning model known as\nConPLex has been successfully developed for the sequence-based\nprediction of drug-target interactions with remarkable accuracy,\nbroad adaptivity, and speciﬁcity (Singh et al., 2023). ConPLex boasts\na competitive edge due to the innovative integration of pretrained\nprotein language models (“PLex”, for lexicographic pretraining) and\nprotein-anchored contrastive coembedding (“Con”, for contrastive\nlearning) ( Figure 13 ). The “PLex” component is capable of\nalleviating challenges posed by limited DTI training data, while\nthe “Con” aspect effectively maps target proteins and drugs into a\nuniﬁed latent space, ensuring distinct separation between true\ninteracting partners. Consequently, ConPLex enables more\naccurate predictions of DTIs by leveraging the distance within\nthe learned representations, even when dealing with massive\ncompound libraries and the expansive human proteome.\nThe experimental results have demonstrated the model’se fﬁcacy in\nsuccessfully predicting the tested kinase-drug interactions, with 12 out of\n19 pairs showingK\nD values less than 100 nM, including four with\nsubnanomolar afﬁnity and an efﬁcient EPHB1 inhibitor (PD-166326,\nKD = 1.3 nM). Beyond its broad generalizability and high speciﬁcity,\nConPLex enhances interpretability, rendering the drug-target\nembedding space and the functions of human cell-surface proteins\nmore transparent. In addition to thein silico screening of small-\nmolecular-weight compounds, ConPLex holds potential for screening\nother drugs types, such as antibodies, and for toxicity prediction.\nGiven these signiﬁcant advantages, ConPLex is anticipated to\nrevolutionize in silico drug screening at the genomic scale and to\naccelerate the development of innovative drugs in modern\npharmaceutical research.\n2.2 Deep learning for macromolecular drugs\n(protein structure prediction)\nProtein structures are traditionallyelucidated through experimental\ntechniques such as by X-ray cryst allography, nuclear magnetic\nresonance (NMR) and electron cryomicroscopy (cryo-EM), which\nare known for their precision. However, these methods are complex,\ntime-consuming, and costly, which limits their widespread application.\nIn light of these constraints and the growing need for novel protein\nstructures, there has been a surge in interest in innovative strategies,\nparticularly bioinformatics approaches to obtain novel protein\nstructures. Despite the promise of these methods, they still\nnecessitate considerable experimental effort.\n2.2.1 AlphaFold\nAlphaFold, developed by DeepMind, has revolutionized protein\nstructure prediction with unprecedented accuracy and reliability,\nharnessing the power of neural networks and homology modeling\nfor protein model construction (Pandey et al., 2022). To extend the\ncapabilities to predict protein complexes accurately and efﬁciently,\nAlphaFold-Multimer was introduced, expanding the capabilities of\nAlphafold2 to handle multiple chains (Yin et al., 2022). The latest\nversion, AlphaFold3, has been successfully applied in variousﬁelds,\nincluding modeling of conventional protein structures and\nstructures with novel folds, structural construction of arti ﬁcial\nconstructs and prediction of protein ‒protein interactions. In\nparticular, models generated by AlphaFold typically achieve TM-\nscores greater above 0.9, suggesting that both the overall fold and the\ndetails of the constructed models are theoretically correct (Skolnick\net al., 2021). To date, AlphaFold DB (AlphaFold DB, https://\nalphafold.ebi.ac.uk) has provided open access to more than\n214 million protein structure predictions (Varadi et al., 2023).\nThe exceptional performance of AlphaFold is largely attributed\nto the novel neural network architectures and specialized training\nregimens that incorporate evolutionary, physical and geometric\nconstraints inherent to protein structures (Jumper and Hassabis,\n2022). Alongside the simultaneous generation of multiple sequence\nalignments (MSAs) and pairwise features, two key modules,\nEvoformer and the structure module, play critical roles in protein\nstructure development (Skolnick et al., 2021). Evoformer, a building\nblock of a novel neural network, approaches predict protein\nstructures as a graph inference problem, with graph edge deﬁned\nby the proximity of residues. It consists of two specialized\ntransformers for distinct data types: the MSA transformer and\nthe pair representation transformer. The structure module is\ntasked with local side chain packing rearrangements, prioritizing\nthe orientations of the protein backbone and residues, and\npositioning the side chains of different residues.\nIn a study utilizing the program Accuracy of NMR Structures\nUsing RCI and Rigidity (ANSURR), the accuracy of AlphaFold-\ngenerated structures was compared to NMR structures (Fowler and\nWilliamson, 2022). The results revealed that the AlphaFold models\ngenerally surpass NMR ensembles in accuracy, although there are\nscenarios, particularly those involving dynamic structures, where\nNMR ensembles may be more precise. This suggests that AlphaFold\nmight display relatively low con ﬁdence in predicting dynamic\nstructures. Consequently, it has been proposed that AlphaFold\ncould be instrumental in reﬁning NMR structure. Furthermore,\nstructures generated by AlphaFold and subsequently validated by\nANSURR are likely to satisfy application requirements, potentially\neliminating the need for additional reﬁnement processes.\n2.2.2 MULTICOM\nTo enhance the precision of AlphaFold-Multimer in predicting\ncomplex structure (Zhu et al., 2023), a sophisticated quaternary\nstructure prediction system (MULTICOM) has been developed (Liu\net al., 2023b). It is capable of optimizing the inputs transformed into\nAlphaFold-Multimer, evaluating and reﬁning the resulting outputs.\nIt employs a dual approach, utilizing traditional sequence alignment\nand Foldseek-based structure alignment to generate MSAs and to\nidentify templates for individual monomers. These MSAs for\nmonomers are subsequently merged to form MSAs for\nmultimers. Moreover, the structural predictions generated can be\nappraised using a suite of complementary metrics, and the\nreﬁnement of structural predictions can be achieved through a\nFoldseek-based structure alignment strategy.\nThe results showed that the average TM-score for the initial\npredictions from MULTICOM for CASP15 assembly targets was\n~0.76, making a 5.3% increase over the standard AlphaFold-\nMultimer. The average TM-score for the top 5 predictions by\nMULTICOM was ~0.80, which represents an 8% increase\ncompared with the standard AlphaFold-Multimer. In addition,\nthe Foldseek structure alignment-based multimer structure\ngeneration (FSAMG) method outperformed several prevalent\nFrontiers inPharmacology frontiersin.org14\nLiu et al. 10.3389/fphar.2024.1458739\nsequences alignment-based multimer structure generation methods,\nsuch as NBIS-AF2-multimer predictions.\n2.2.3 ComplexQA\nIn a cutting-edge study, a novel model quality assessment\nmethod, ComplexQA, has been introduced. This method\nleverages a deep graph neural network-based algorithm designed\nto assess the local quality of interfacial residues within protein\ncomplexes ( Figure 14 )( Zhang et al., 2023 ). It does so by\nanalyzing a combination of sequence data, 3D structural\ninformation, and chemical properties. The process begins by\nconverting the protein complex structures into undirected graphs,\nfollowed by the derivation of feature representation for each graph\nnode. All the features, including the hidden features, are\nconcatenated and used for graph learning purposes. To represent\nthe edges of the graph, residue – residue features are acquired,\nprimarily through two newly designed matrices: the adjacent\nmatrix and the edge distance matrix. By integrating these two\nrepresentations, the edge embedding features are generated,\nwhich are then employed for the subsequent edge convolution\noperations within the graph convolutional network block. This\nblock further consists of two subblocks: one for edge convolution\nand another for node convolution. Finally, the output is transformed\ninto a 1D convolutional layer, which employs a linear activation\nfunction to produce theﬁnal results. This sophisticated approach by\nComplexQA offers a comprehensive evaluation of protein complex\nstructures, enhancing our ability to understand and predict\ntheir quality.\nIn comparative evaluations across diverse datasets, ComplexQA\noutperformed the other leading algorithms (DProQA, GNN-DOVE,\nTRScore, GOAP, and ZRANK2). It also displayed commendable\nperformance on challenging targets that featured a sparse number of\nacceptable models. Furthermore, ComplexQA is capable of\ndelivering a detailed assessment of each interface residue, offering\na level of precision that is invaluable in theﬁeld of protein complex\nstructure analysis.\n2.2.4 ProtGPT2\nProtGPT2 ( https://huggingface.co/nferruz/ProtGPT2), a\ncutting-edge autoregressive transformer model grounded in\nlanguage-based principles, has been engineered to de novo\nconstruct protein structures with high throughput ef ﬁciency\n(Ferruz et al., 2022 ). The transformer was trained on an\nexpansive dataset of ~50 million non-annotated sequences from\nthe UniRef50 (UR50) database, encompassing the full spectrum of\nprotein diversity, thereby enabling it to learn and“comprehend” the\nintricacies of protein language in an autoregressive manner. In\naddition to the standard performance metrics, a suite of extrinsic\ntests was meticulously designed to assess the quality of the protein\nsequences generated by ProtGPT2.\nThe ﬁndings were compelling that ProtGPT2 demonstrated an\nimpressive ability to generate sequences that, while remotely related\nto natural counterparts, also bore resemblance to known structural\nspaces. The generated proteins mirrored the natural amino acid\npropensities observed in their naturally occurring counterparts, with\na notable predilection for globular structures, accounting for roughly\n80% of the generated proteins. Moreover, sequences generated by\nProtGPT2 were found to be only distantly related to those found in\nnature. When these results were integrated with similarity network\nanalyses, it became evident that ProtGPT2 possesses the unique\ncapability to explore and sample previously uncharted territories\nwithin the vast protein space.\n2.2.5 ProteinMPNN\nRecently, a novel deep learning-based protein sequence design\nstrategy, ProteinMPNN, has emerged, demonstrating signiﬁcant\nadvantages in both in silico and experimental tests ( Dauparas\net al., 2022 ). This innovative approach is founded on the\nstructured transformer framework ( Figure 15), incorporating a\nmessage-passing neural network (MPNN) architecture that\nencompasses 3 encoder layers, 3 decoder layers and 128 hidden\nlayers. ProteinMPNN was designed to predict target protein\nsequences in an autoregressive manner from the N- to\nC-terminus using protein backbone features as input data. The\nsequence recovery rate of the baseline model was approximately\n41.2%, which was notably increased to more than 50% following a\nseries of improvements. These enhancements were primarily\nfocused on the following aspects (Sadybekov and Katritch, 2023):\nincorporating additional distance metrics between virtual C βs\n(Murray et al., 2023 ), introducing an edge update mechanism\nFIGURE 14\nThe model architecture of ComplexQA with the deep graph convolution network. (ComplexQA is capable of molecular analysis that processes\nvarious molecular features through Conv1D blocks and integrates edge information using Conv2D blocks. Then applies graph convolutional networks\n(GCNs) with multiple blocks was used for feature aggregation and normalization. Finally, it concludes with the generation of a residue interface score to\nassess molecular interactions).\nFrontiers inPharmacology frontiersin.org15\nLiu et al. 10.3389/fphar.2024.1458739\n(Cova et al., 2022), employing random sampling for the decoding\norder within the decoder (Roggia et al., 2024), integrating coding\ninformation regarding the relative position and chain number, and\n(Pandey et al., 2022) the incorporation of Gaussian noise to enhance\nmodel robustness.\nThe results were impressive, with ProteinMPNN demonstrating\nthe ability to design sequences for monomers and cyclic oligomers\nwith remarkable stability and precision. Most of the proteins\nproduced by the MPNN were soluble (96 design sequences,\n73 soluble). The crystal structures and the electron cloud density\nof the core side chain were highly consistent with the intended\ndesign structures. Similar successes were achieved in the design of\nproteins with cyclic and internal repeat symmetries as well as those\nincorporating polyproline II helix motifs.\nProteinMPNN achieved a sequence recovery rate of 52.4%,\nmarking a 19.5% increase compared to that of Rosetta for native\nprotein backbones, and even surpassed AlphaFold in this speciﬁc\ntask. The running time was remarkably swift, averaging\napproximately one second. In particular, ProteinMPNN also\nenabled the coupling of amino acid sequences at various\npositions across single or multiple chains, further expanding its\nversatility and applicability in protein sequence design.\n3 Concluding remarks\nAI has become an indispensable tool for addressing a multitude\nof societal challenges. The future of AI in drug development is set to\nbe a landscape of innovation and efﬁciency, and it has been seeing a\nsigniﬁcant shift towards data-driven approaches, personalized\nmedicine and clinical trials revolution. Take the AI-driven drug\ndiscovery and personalized medicines for examples. AI is expected\nto dominate drug discovery by making more accurate predictions of\ndrug-target interactions and enhancing our understanding of\ndisease physiopathology. AI models will be trained on larger\nbiomedical datasets, including genomics, proteomics, and\nmetabolomics, to identify novel drug candidates and optimize\ndrug design. AI will continue to drive the growth of personalized\nmedicines by leveraging Big Data to tailor treatments to individual\npatients. The ability to analyze genetic, environmental, and lifestyle\ndata will lead to the development of highly personalized treatment\nplans. AI has the potential to revolutionize clinical trials by\nimproving patient recruitment, monitoring, and data analysis.\nAdvanced algorithms will enable the identi ﬁcation of suitable\ncandidates based on genetic and phenotypic pro ﬁles, ensuring\nthat trials are conducted with the most appropriate cohort of\nparticipants. Particularly noteworthy is the proliferation of AI\nalgorithm programs, including DeepMind AlphaFold, Atomwise,\nRecursion Pharmaceuticals, BenevolentAI, and Insilico Medicine.\nThese examples showcase the diverse integration of AI across the\ndrug development spectrum, from the early stages of drug discovery\nto manufacturing processes and post-market surveillance. The\nfuture looks promising, with AI set to play a central role in\nmaking drug development more ef ﬁcient, targeted, and\npersonalized.\nNevertheless, the proliferation of LLMs ha as also sparked\nsigniﬁcant apprehensions, such as the phenomenon of“artiﬁcial\nhallucinations” (Beutel et al., 2023; Ji et al., 2022). The dissemination\nof AI-generated misinformation,ﬁction, or unsubstantiated claims\nposes a risk of misguiding unsuspecting users. To optimize beneﬁts\nand mitigate risks, several key challenges must be surmounted to\nharness the full potential of LLMs (Sadybekov and Katritch, 2023).\nTransparency concerns. This is paramount for academic discourse\nsurrounding generative AI. It is recommended that the judicious use\nof AI in scientiﬁc research be underscored and clearly articulated, as\nthis could signi ﬁcantly bolster credibility ( Tang et al., 2024 ).\nTherefore, tools and techniques that enhance the explain ability\nand interpretability of AI models are crucial. Moreover, the\ntransparency in data governance is essential, providing insight\ninto the quality and suitability of data used for training and\nFIGURE 15\nThe workﬂow of ProteinMPNN. (ProteinMPNN is consisted of a backbone encoder that processes protein backbone coordinates and edge\ninformation, and a sequence decoder capable of generating protein sequences. Nodes and edges are updated in the encoder, and through iterative\ndecoding, it produces a protein sequence with a calculated probability).\nFrontiers inPharmacology frontiersin.org16\nLiu et al. 10.3389/fphar.2024.1458739\ninference in algorithmic decision-making. This includes\ndocumenting the origin of data, collection methods, and any\npreprocessing steps, which is crucial for identifying and\nmitigating potential biases (Murray et al., 2023). Combating AI\nhallucinations. AI hallucinations can occur due to several factors,\nincluding overﬁtting, training data bias/inaccuracy, and high model\ncomplexity. The foundation of preventing AI hallucinations lies in\nusing high-quality, diverse training data that represents real-life\nscenarios without biases and errors. Moreover, regular validation\nusing test datasets and human-in-the-Loop veriﬁcation should also\nbe instituted to preempt the spread of misinformation and to\ncounteract biased responses. Finally, risk-based review systems\nand retrieval-augmented generation (RAG) might also be helpful\nin prioritizing and verifying the review of AI outputs (Cova et al.,\n2022). Dataset limitations. Owing to the nature of AI, the quality and\nscope of available data are pivotal to the design and practical\napplication of AI models. There is a pressing need to focus on\nthe quantity and quality of data, with larger and more diverse\ndatasets being crucial for enhancing model performance (Roggia\net al., 2024). Building trust in models. Trust is established through a\ncombination of technical reliability, transparency, and alignment\nwith user expectations. The factors that foster trust in models\npredominantly center on selecting the appropriate neural\nnetwork architecture and molecular representations, alongside the\nadvancement of innovative architectures imbued with inductive\nbias. For instance, recurrent neural networks (RNNs) are well-\nsuited for sequential data due to their ability to maintain a form\nof memory. However, the choice extends beyond RNNs to include\nother architectures like convolutional neural networks (CNNs),\nwhich are effective for image data, and graph neural networks\n(GNNs), which are particularly adept at handling graph-\nstructured data like molecules. As for molecular representations,\nexcept for SMILES strings, graph representations could capture the\nmolecular structure more directly, including both topological and\ngeometrical information, which is essential for tasks like drug\ndiscovery and material science. Moreover, innovative\narchitectures with inductive bias (e.g., the 3D-CNN architecture\n(Skalic et al., 2019)) should be further developed to better address\nthe nuances of specialized tasks. For example, geodesic 3D\nconvolutional neural networks (gCNNs) use geodesic\nconvolutions that consider the intrinsic geometry of the data,\nwhich is particularly useful in medical applications where the\ncurvature and shape of organs, bones, and tissues are critical.\nThese architectures can lead to improved model accuracy and\ncomputational ef ﬁciency by focusing on the most important\ninformation in the data (Pandey et al., 2022). Data safety and\nprivacy. The safeguarding of personal information in terms of\nsecurity, privacy, and conﬁdentiality is non-negotiable, especially\nin the context of research, standards development, and commercial\napplications (Jayatunga et al., 2022). Computational complexity\n(time complexity, space complexity and scalability). The\nchallenges posed by the computational demands and intricacies\nof contemporary deep learning methods are expected to remain a\nsigniﬁcant factor in the near term. For example, to address the\nscalability challenge, many AI applications leverage distributed\ncomputing and parallel processing techniques. Reducing the\ncomputational complexity of deep learning models can be\nachieved through network compression and acceleration\ntechniques. Moreover, quantum computing offers a potential\nsolution to overcoming computational limitations in AI.\nQuantum algorithms for machine learning, such as Grover ’s\nalgorithm, can potentially reduce the complexity of certain tasks,\nmaking previously intractable problems solvable. Additionally,\nquantum neural networks leveraging qubits could operate with\nhigher efﬁciency and improved processing speed.\nWith unwavering conviction, we are stand atop the pinnacle of\nresearch, an epoch where AI, and especially LLMs, are set to\ntranscend mere advancement and emerge as vital pillars of\ncontemporary pharmaceutical innovation. It is imperative to\naccentuate the sophisticated and pro ﬁcient application of AI\nthroughout the biotechnological pharmaceutical development\ncontinuum, demonstrating its unparalleled ability to catalyze\nscientiﬁc breakthroughs and augment the ef ﬁcacy of drug\ndiscovery endeavors.\nAuthor contributions\nX-hL: Data curation, Formal Analysis, Writing– original draft.\nZ-hL: Methodology, Resources, Software, Writing– original draft.\nTW: Funding acquisition, Project administration, Supervision,\nValidation, Writing– review and editing. FL: Funding acquisition,\nProject administration, Supervision, Writing– review and editing.\nFunding\nThe author(s) declare thatﬁnancial support was received for the\nresearch, authorship, and/or publication of this article. The authors\nexpress their gratitude to the Traditional Chinese Medicine Science\nand Technology Project of Shandong Province (M-2023080), the\nShandong Provincial University Youth Innovation Team, China\n(No. 2022KJ102), the National Natural Science Foundation of China\n(No. 32000194), and the Research Fund for Academician Lin He\nNew Medicine (JYHL2021MS23).\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their af ﬁliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nFrontiers inPharmacology frontiersin.org17\nLiu et al. 10.3389/fphar.2024.1458739\nReferences\nAl-Ashwal, F. Y., Zawiah, M., Gharaibeh, L., Abu-Farha, R., and Bitar, A. N. (2023).\nEvaluating the sensitivity, speciﬁcity, and accuracy of ChatGPT-3.5, ChatGPT-4, bing\nAI, and bard against conventional drug-drug interactions clinical tools.Drug Healthc.\nAnd Patient Saf.15, 137– 147. doi:10.2147/dhps.S425858\nBagal, V., Aggarwal, R., Vinod, P. K., and Priyakumar, U. D. (2022). MolGPT:\nmolecular generation using a transformer-decoder model.J. Chem. Inf. Model.62 (9),\n2064– 2076. doi:10.1021/acs.jcim.1c00600\nBeutel, G., Geerits, E., and Kielstein, J. T. (2023). Artiﬁcial hallucination: GPT on\nLSD? Crit. Care 27 (1), 148. doi:10.1186/s13054-023-04425-6\nBorji, A. (2023). A categorical archive of ChatGPT failures.arXiv Prepr. Arxiv. 2023;\narXiv:2302.03494,1 – 21.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., and Amodei, D. (2020). Language\nmodels are few-shot learners.ArXiv. abs/2005.14165. doi:10.48550/ARXIV.2005.14165\nChakraborty, S., Chopra, H., Akash, S., Chakraborty, C., and Dhama, K. (2023).\nArtiﬁcial intelligence (AI) is paving the way for a critical role in drug discovery, drug\ndesign, and studying drug-drug interactions - correspondence.Int. J. Surg.109 (10),\n3242– 3244. doi:10.1097/js9.0000000000000564\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., et al. (2022).\nPaLM: scaling language modeling with pathways. ArXiv. 2022;abs/2204.\nChristoﬁdellis, D., Giannone, G., Born, J., Winther, O., Laino, T., and Manica, M.\n(2023). Unifying molecular and textual representations via multi-task language\nmodelling. arXiv Prepr. arXiv:2301.12586.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., et al. (2022). Scaling\ninstruction-ﬁnetuned Language Models. ArXiv. 2022;abs/2210.11416.\nCova, T., Vitorino, C., Ferreira, M., Nunes, S., Rondon-Villarreal, P., and Pais, A.\n(2022). Artiﬁcial intelligence and quantum computing as the next pharma disruptors.\nMethods Mol. Biol.2390, 321– 347. doi:10.1007/978-1-0716-1787-8_14\nDauparas, J., Anishchenko, I., Bennett, N., Bai, H., Ragotte, R. J., Milles, L. F., et al.\n(2022). Robust deep learning-based protein sequence design using ProteinMPNN.\nScience 378 (6615), 49– 56. doi:10.1126/science.add2187\nDe Angelis, L., Baglivo, F., Arzilli, G., Privitera, G. P., Ferragina, P., Tozzi, A. E., et al.\n(2023). ChatGPT and the rise of large language models: the new AI-driven infodemic\nthreat in public health. Front. Public Health 11, 1166120. doi:10.3389/fpubh.2023.\n1166120\nEdwards, C. N., Lai, T., Ros, K., Honke, G., and Ji, H. (2022). Translation between\nmolecules and natural language.ArXiv. doi:10.48550/arXiv.2204.11817\nEggmann, F., Weiger, R., Zitzmann, N. U., and Blatz, M. B. (2023). Implications of\nlarge language models such as ChatGPT for dental medicine.J. Esthetic Restor. Dent.35\n(7), 1098– 1102. doi:10.1111/jerd.13046\nFang, Y., Liang, X., Zhang, N., Liu, K., Huang, R., Chen, Z., et al. (2023a). Mol-\ninstructions: a large-scale biomolecular instruction dataset for Large Language Models.\nArXiv. 2023;abs/2306.08018.\nFang, Y., Zhang, N., Chen, Z., Guo, L., Fan, X., and Chen, H. (2023b). Domain-\nagnostic molecular generation with self-feedback. arXiv Prepr. Arxiv. 2301, 11259.\ndoi:10.48550/arXiv.2301.11259\nFerruz, N., Schmidt, S., and Höcker, B. (2022). ProtGPT2 is a deep unsupervised\nlanguage model for protein design.Nat. Commun. 13 (1), 4348. doi:10.1038/s41467-\n022-32007-7\nFowler, N. J., and Williamson, M. P. (2022). The accuracy of protein structures in\nsolution determined by AlphaFold and NMR.Structure 30 (7), 925– 933.e2. doi:10.1016/\nj.str.2022.04.005\nGao, Z., Li, L., Ma, S., Wang, Q., Hemphill, L., and Xu, R. (2023). Examining the\npotential of ChatGPT on biomedical information retrieval: fact-checking drug-disease\nassociations. Ann. Biomed. Eng.52, 1919– 1927. doi:10.1007/s10439-023-03385-w\nHaque, M. D. R., and Rubya, S. (2023). An Overview of chatbot-based mobile mental\nhealth apps: insights from app description and user reviews.JMIR MHealth UHealth11,\ne44838. doi:10.2196/44838\nHeck, T. G. (2023). What artiﬁcial intelligence knows about 70 kDa heat shock\nproteins, and how we will face this ChatGPT era.Cell stress & Chaperones 28 (3),\n225– 229. doi:10.1007/s12192-023-01340-1\nHou, Y., Yeung, J., Xu, H., Su, C., Wang, F., and Zhang, R. (2023). From answers to\ninsights: unveiling the strengths and limitations of ChatGPT and biomedical knowledge\ngraphs. Res. square. doi:10.21203/rs.3.rs-3185632/v1\nIrwin, R., Dimitriadis, S., He, J., and Bjerrum, E. J. (2022). Chemformer: a pre-trained\ntransformer for computational chemistry.Mach. Learn. Sci. Technol. 3 (1), 015022.\ndoi:10.1088/2632-2153/ac3ffb\nJayatunga, M. K. P., Xie, W., Ruder, L., Schulze, U., and Meier, C. (2022). AI in small-\nmolecule drug discovery: a coming wave?Nat. Rev. Drug Discov.21 (3), 175– 176. doi:10.\n1038/d41573-022-00025-1\nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., et al. (2022). Survey of hallucination in\nnatural language generation.ACM Comput. Surv.55, 1– 38. doi:10.1145/3571730\nJuhi, A., Pipil, N., Santra, S., Mondal, S., Behera, J. K., and Mondal, H. (2023). The\ncapability of ChatGPT in predicting and explaining common drug-drug interactions.\nCureus 15 (3), e36272. doi:10.7759/cureus.36272\nJumper, J., and Hassabis, D. (2022). Protein structure predictions to atomic accuracy\nwith AlphaFold. Nat. Methods 19 (1), 11– 12. doi:10.1038/s41592-021-01362-6\nKing, M. R. (2023). Can bard, google’s experimental chatbot based on the LaMDA\nLarge Language Model, help to analyze the gender and racial diversity of authors in your\ncited scientiﬁc references? Cellular and. Mol. Bioeng. 16 (2), 175– 179. doi:10.1007/\ns12195-023-00761-3\nL a m b e r t i ,M .J . ,W i l k i n s o n ,M . ,D o n z a n t i ,B .A . ,W o h l h i e t e r ,G .E . ,P a r i k h ,S . ,\nWilkins, R. G., et al. (2019). A study on the application and use of arti ﬁcial\nintelligence to support drug development. Clin. Ther. 41 (8), 1414– 1426. doi:10.\n1016/j.clinthera.2019.05.018\nLi, J., Liu, Y., Fan, W., Wei, X., Liu, H., Tang, J., et al. (2023). Empowering molecule\ndiscovery for molecule-caption translation with Large Language Models: a ChatGPT\nperspective. IEEE Trans. Knowl. Data Eng. 36, 6071– 6083. doi:10.1109/tkde.2024.\n3393356\nLiang, Y., Zhang, R., Zhang, L., and Xie, P. (2023). DrugChat: towards enabling\nChatGPT-like capabilities on drug molecule graphs. doi:10.36227/techrxiv.22945922.v1\nLiu, J., Guo, Z., Wu, T., Roy, R. S., Quadir, F., Chen, C., et al. (2023b). Enhancing\nalphafold-multimer-based protein complex structure prediction with MULTICOM in\nCASP15. Commun. Biol. 6 (1), 1140. doi:10.1038/s42003-023-05525-3\nLiu, J., Wang, C., and Liu, S. (2023a). Utility of ChatGPT in clinical practice.J. Med.\nInternet Res. 25, e48568. doi:10.2196/48568\nMurray, J. D., Lange, J. J., Bennett-Lenane, H., Holm, R., Kuentz, M., O\n’Dwyer, P. J.,\net al. (2023). Advancing algorithmic drug product development: recommendations for\nmachine learning approaches in drug formulation.Eur. J. Pharm. Sci. 191, 106562.\ndoi:10.1016/j.ejps.2023.106562\nPandey, M., Fernandez, M., Gentile, F., Isayev, O., Tropsha, A., Stern, A. C., et al.\n(2022). The transformational role of GPU computing and deep learning in drug\ndiscovery. Nat. Mach. Intell.4 (3), 211– 221. doi:10.1038/s42256-022-00463-x\nP i c h a i ,S .( 2 0 2 3 ) .An important next step on our AI journey. California, United\nStates: Google. Available at: https://blog. google/technology/ai/bard-google-ai-\nsearch-updates/.\nPushpanathan, K., Lim, Z. W., Er Yew, S. M., Chen, D. Z., Hui’En Lin, H. A., Lin, G.\nJ. H., et al. (2023). Popular large language model chatbots’accuracy, comprehensiveness,\nand self-awareness in answering ocular symptom queries.iScience 26 (11), 108163.\ndoi:10.1016/j.isci.2023.108163\nQarajeh, A., Tangpanithandee, S., Thongprayoon, C., Suppadungsuk, S., Krisanapan,\nP., Aiumtrakul, N., et al. (2023). AI-powered renal diet support: performance of\nChatGPT, bard AI, and bing chat. Clin. Pract. 13 (5), 1160– 1172. doi:10.3390/\nclinpract13050104\nRoggia, M., Natale, B., Amendola, G., Di Maro, S., and Cosconati, S. (2024).\nStreamlining large chemical library docking with arti ﬁcial intelligence: the\nPyRMD2Dock approach. J. Chem. Inf. Model. 64 (7), 2143– 2149. doi:10.1021/acs.\njcim.3c00647\nSadybekov, A. V., and Katritch, V. (2023). Computational approaches streamlining\ndrug discovery. Nature 616 (7958), 673– 685. doi:10.1038/s41586-023-05905-z\nSallam, M. (2023). ChatGPT utility in healthcare education, research, and practice:\nsystematic review on the promising perspectives and valid concerns.Healthcare 11 (6),\n887. doi:10.3390/healthcare11060887\nSavage, N. (2023). Drug discovery companies are customizing ChatGPT: here’s how.\nNat. Biotechnol. 41 (5), 585– 586. doi:10.1038/s41587-023-01788-7\nSingh, R., Sledzieski, S., Bryson, B., Cowen, L., and Berger, B. (2023). Contrastive\nlearning in protein language space predicts interactions between drugs and protein\ntargets. Proc. Natl. Acad. Sci.120 (24), e2220778120. doi:10.1073/pnas.2220778120\nSinghal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W., et al. (2023). Large\nlanguage models encode clinical knowledge.Nature 620 (7972), 172– 180. doi:10.1038/\ns41586-023-06291-2\nSkalic, M., Jiménez, J., Sabbadin, D., and De Fabritiis, G. (2019). Shape-based\ngenerative modeling for de novo drug design. J. Chem. Inf. Model. 59 (3),\n1205– 1214. doi:10.1021/acs.jcim.8b00706\nSkolnick, J., Gao, M., Zhou, H., and Singh, S. (2021). AlphaFold 2: why it works and its\nimplications for understanding the relationships of protein sequence, structure, and\nfunction. J. Chem. Inf. Model.61 (10), 4827– 4831. doi:10.1021/acs.jcim.1c01114\nSulaiman, D. M., Shaba, S. S., Almufty, H. B., Sulaiman, A. M., and Merza, M. A.\n(2023). Screening the drug-drug interactions between antimicrobials and other\nprescribed medications using Google bard and Lexicomp ® Online™ database.\nCureus 15 (9), e44961. doi:10.7759/cureus.44961\nSuppadungsuk, S., Thongprayoon, C., Miao, J., Krisanapan, P., Qureshi, F., Kashani,\nK., et al. (2023). Exploring the potential of chatbots in critical care nephrology.\nMedicines 10 (10), 58. doi:10.3390/medicines10100058\nFrontiers inPharmacology frontiersin.org18\nLiu et al. 10.3389/fphar.2024.1458739\nTang, A., Li, K. K., Kwok, K. O., Cao, L., Luong, S., and Tam, W. (2024). The\nimportance of transparency: declaring the use of generative artiﬁcial intelligence (AI) in\nacademic writing. J. Nurs. Scholarsh.56 (2), 314– 318. doi:10.1111/jnu.12938\nThirunavukarasu, A. J., Ting, D. S. J., Elangovan, K., Gutierrez, L., Tan, T. F., and\nTing, D. S. W. (2023). Large language models in medicine.Nat. Med.29 (8), 1930– 1940.\ndoi:10.1038/s41591-023-02448-8\nToyama, Y., Harigai, A., Abe, M., Nagano, M., Kawabata, M., Seki, Y., et al. (2023).\nPerformance evaluation of ChatGPT, GPT-4, and bard on the of ﬁcial board\nexamination of the Japan radiology society.Jpn. jJournal Radiology 42 (2), 201– 207.\ndoi:10.1007/s11604-023-01491-2\nVaradi, M., Bertoni, D., Magana, P., Paramval, U., Pidruchna, I., Radhakrishnan, M.,\net al. (2023). AlphaFold Protein Structure Database in 2024: providing structure\ncoverage for over 214 million protein sequences.Nucleic Acids Res.52, D368– D375.\ndoi:10.1093/nar/gkad1011\nWang, R., Feng, H., and Wei, G. W. (2023a). ChatGPT in drug discovery: a case study\non anti-cocaine addiction drug development with chatbots. doi:10.48550/arXiv.2308.\n06920\nWang, Y., Zhao, H., Sciabola, S., and Wang, W. (2023b). cMolGPT: a conditional\ngenerative pre-trained transformer for target-speciﬁc de novo molecular generation.\nMolecules 28 (11), 4430. doi:10.3390/molecules28114430\nXu, L., Sanders, L., Li, K., and Chow, J. C. L. (2021). Chatbot for health care and\noncology applications using artiﬁcial intelligence and machine learning: systematic\nreview. JMIR Cancer 7 (4), e27850. doi:10.2196/27850\nYin, R., Feng, B. Y., Varshney, A., and Pierce, B. G. (2022). Benchmarking AlphaFold\nfor protein complex modeling reveals accuracy determinants.Protein Sci.31 (8), e4379.\ndoi:10.1002/pro.4379\nZehua, Z., and Du, H. (2023). Revolutionizing single cell analysis: the power of large\nlanguage models for cell type annotation.ArXiv. 2023;abs/2304.02697. doi:10.48550/\narXiv.2304.02697\nZeng, Z., Yao, Y., Liu, Z., and Sun, M. (2022). A deep-learning system bridging\nmolecule structure and biomedical text with comprehension comparable to human\nprofessionals. Nat. Commun. 13 (1), 862. doi:10.1038/s41467-022-28494-3\nZhang, L., Wang, S., Hou, J., Si, D., Zhu, J., and Cao, R. (2023). ComplexQA: a deep\ngraph learning approach for protein complex structure assessment. Brieﬁngs\nBioinforma. 24 (6), bbad287. doi:10.1093/bib/bbad287\nZhao, A., and Wu, Y. (2023). Future implications of ChatGPT in pharmaceutical industry: drug\ndiscovery and development.Front. Pharmacol.14, 1194216. doi:10.3389/fphar.2023.1194216\nZhu, W., Shenoy, A., Kundrotas, P., and Elofsson, A. (2023). Evaluation of AlphaFold-\nMultimer prediction on multi-chain protein complexes.Bioinformatics 39 (7), btad424.\ndoi:10.1093/bioinformatics/btad424\nFrontiers inPharmacology frontiersin.org19\nLiu et al. 10.3389/fphar.2024.1458739",
  "topic": "Drug development",
  "concepts": [
    {
      "name": "Drug development",
      "score": 0.5552039742469788
    },
    {
      "name": "Computer science",
      "score": 0.4757256805896759
    },
    {
      "name": "Engineering ethics",
      "score": 0.4265994131565094
    },
    {
      "name": "Artificial intelligence",
      "score": 0.358673095703125
    },
    {
      "name": "Data science",
      "score": 0.34553366899490356
    },
    {
      "name": "Cognitive science",
      "score": 0.3352397084236145
    },
    {
      "name": "Drug",
      "score": 0.2333093285560608
    },
    {
      "name": "Engineering",
      "score": 0.2200809121131897
    },
    {
      "name": "Medicine",
      "score": 0.20670759677886963
    },
    {
      "name": "Psychology",
      "score": 0.20041993260383606
    },
    {
      "name": "Pharmacology",
      "score": 0.16391411423683167
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I51185872",
      "name": "Jining Medical University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I76130692",
      "name": "Zhejiang University",
      "country": "CN"
    }
  ]
}