{
    "title": "iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries",
    "url": "https://openalex.org/W4392617724",
    "year": 2024,
    "authors": [
        {
            "id": null,
            "name": "Coscia, Adam",
            "affiliations": [
                "Georgia Institute of Technology"
            ]
        },
        {
            "id": null,
            "name": "Holmes, Langdon",
            "affiliations": [
                "Vanderbilt University"
            ]
        },
        {
            "id": null,
            "name": "Morris, Wesley",
            "affiliations": [
                "Vanderbilt University"
            ]
        },
        {
            "id": null,
            "name": "Choi, Joon Suh",
            "affiliations": [
                "Georgia State University"
            ]
        },
        {
            "id": "https://openalex.org/A3099553039",
            "name": "Crossley Scott",
            "affiliations": [
                "Vanderbilt University"
            ]
        },
        {
            "id": "https://openalex.org/A3174101223",
            "name": "Endert, Alex",
            "affiliations": [
                "Georgia Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3133702157",
        "https://openalex.org/W2135415614",
        "https://openalex.org/W4288059323",
        "https://openalex.org/W4293464023",
        "https://openalex.org/W2602387228",
        "https://openalex.org/W2953001123",
        "https://openalex.org/W3093452197",
        "https://openalex.org/W2144988340",
        "https://openalex.org/W2733082571",
        "https://openalex.org/W2898193199",
        "https://openalex.org/W3159958158",
        "https://openalex.org/W4281790347",
        "https://openalex.org/W2280974225",
        "https://openalex.org/W3011616684",
        "https://openalex.org/W2301632029",
        "https://openalex.org/W2963214037",
        "https://openalex.org/W4318464200",
        "https://openalex.org/W2053075547",
        "https://openalex.org/W3013371068",
        "https://openalex.org/W2587299461",
        "https://openalex.org/W2618851150",
        "https://openalex.org/W4382567099",
        "https://openalex.org/W3087302936",
        "https://openalex.org/W4207045419",
        "https://openalex.org/W2883454655",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2282821441",
        "https://openalex.org/W3118485687",
        "https://openalex.org/W1970569592",
        "https://openalex.org/W4224229986",
        "https://openalex.org/W2947964661",
        "https://openalex.org/W2963123635",
        "https://openalex.org/W2794802448",
        "https://openalex.org/W2966491090",
        "https://openalex.org/W3175479236",
        "https://openalex.org/W3021941048",
        "https://openalex.org/W4294663947",
        "https://openalex.org/W4308262294",
        "https://openalex.org/W2962862931",
        "https://openalex.org/W1951008065",
        "https://openalex.org/W4288089799"
    ],
    "abstract": "The recent explosion in popularity of large language models (LLMs) has\\ninspired learning engineers to incorporate them into adaptive educational tools\\nthat automatically score summary writing. Understanding and evaluating LLMs is\\nvital before deploying them in critical learning environments, yet their\\nunprecedented size and expanding number of parameters inhibits transparency and\\nimpedes trust when they underperform. Through a collaborative user-centered\\ndesign process with several learning engineers building and deploying summary\\nscoring LLMs, we characterized fundamental design challenges and goals around\\ninterpreting their models, including aggregating large text inputs, tracking\\nscore provenance, and scaling LLM interpretability methods. To address their\\nconcerns, we developed iScore, an interactive visual analytics tool for\\nlearning engineers to upload, score, and compare multiple summaries\\nsimultaneously. Tightly integrated views allow users to iteratively revise the\\nlanguage in summaries, track changes in the resulting LLM scores, and visualize\\nmodel weights at multiple levels of abstraction. To validate our approach, we\\ndeployed iScore with three learning engineers over the course of a month. We\\npresent a case study where interacting with iScore led a learning engineer to\\nimprove their LLM's score accuracy by three percentage points. Finally, we\\nconducted qualitative interviews with the learning engineers that revealed how\\niScore enabled them to understand, evaluate, and build trust in their LLMs\\nduring deployment.\\n",
    "full_text": null
}