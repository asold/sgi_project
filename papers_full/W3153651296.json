{
  "title": "Learning a Fine-Grained Review-based Transformer Model for Personalized Product Search",
  "url": "https://openalex.org/W3153651296",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4288895540",
      "name": "Bi, Keping",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A4226957961",
      "name": "Ai, Qingyao",
      "affiliations": [
        "University of Utah"
      ]
    },
    {
      "id": "https://openalex.org/A3173393309",
      "name": "Croft, W. Bruce",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2998020421",
    "https://openalex.org/W2970599813",
    "https://openalex.org/W2740070748",
    "https://openalex.org/W2980918481",
    "https://openalex.org/W2135500808",
    "https://openalex.org/W3025937915",
    "https://openalex.org/W2972160336",
    "https://openalex.org/W2973104394",
    "https://openalex.org/W3166614311",
    "https://openalex.org/W2023408138",
    "https://openalex.org/W2945127593",
    "https://openalex.org/W2168717408",
    "https://openalex.org/W2062364080",
    "https://openalex.org/W1981485659",
    "https://openalex.org/W2531954545",
    "https://openalex.org/W2900464008",
    "https://openalex.org/W2897905870",
    "https://openalex.org/W2741497758",
    "https://openalex.org/W2019403987",
    "https://openalex.org/W2014876987",
    "https://openalex.org/W1991418309",
    "https://openalex.org/W2092163995",
    "https://openalex.org/W2969893967",
    "https://openalex.org/W2021503317",
    "https://openalex.org/W2108168165",
    "https://openalex.org/W1990190154",
    "https://openalex.org/W2122841972",
    "https://openalex.org/W2139873966",
    "https://openalex.org/W2507839313",
    "https://openalex.org/W2002766161",
    "https://openalex.org/W2158450083",
    "https://openalex.org/W2798283910",
    "https://openalex.org/W2984025572",
    "https://openalex.org/W2048571927",
    "https://openalex.org/W2898076813",
    "https://openalex.org/W2982904530"
  ],
  "abstract": "Product search has been a crucial entry point to serve people shopping online. Most existing personalized product models follow the paradigm of representing and matching user intents and items in the semantic space, where finer-grained matching is totally discarded and the ranking of an item cannot be explained further than just user/item level similarity. In addition, while some models in existing studies have created dynamic user representations based on search context, their representations for items are static across all search sessions. This makes every piece of information about the item always equally important in representing the item during matching with various user intents. Aware of the above limitations, we propose a review-based transformer model (RTM) for personalized product search, which encodes the sequence of query, user reviews, and item reviews with a transformer architecture. RTM conducts review-level matching between the user and item, where each review has a dynamic effect according to the context in the sequence. This makes it possible to identify useful reviews to explain the scoring. Experimental results show that RTM significantly outperforms state-of-the-art personalized product search baselines.",
  "full_text": "Learning a Fine-Grained Review-based Transformer Model for\nPersonalized Product Search\nKeping Bi\nUniversity of Massachusetts Amherst\nAmherst, MA, USA\nkbi@cs.umass.edu\nQingyao Ai\nUniversity of Utah\nSalt Lake City, UT, USA\naiqy@cs.utah.edu\nW. Bruce Croft\nUniversity of Massachusetts Amherst\nAmherst, MA, USA\ncroft@cs.umass.edu\nABSTRACT\nProduct search has been a crucial entry point to serve people shop-\nping online. Most existing personalized product models follow the\nparadigm of representing and matching user intents and items in\nthe semantic space, where finer-grained matching is totally dis-\ncarded and the ranking of an item cannot be explained further than\njust user/item level similarity. In addition, while some models in\nexisting studies have created dynamic user representations based\non search context, their representations for items are static across\nall search sessions. This makes every piece of information about\nthe item always equally important in representing the item during\nmatching with various user intents. Aware of the above limitations,\nwe propose a review-based transformer model (RTM) for person-\nalized product search, which encodes the sequence of query, user\nreviews, and item reviews with a transformer architecture. RTM\nconducts review-level matching between the user and item, where\neach review has a dynamic effect according to the context in the\nsequence. This makes it possible to identify useful reviews to ex-\nplain the scoring. Experimental results show that RTM significantly\noutperforms state-of-the-art personalized product search baselines.\nKEYWORDS\nProduct Search; Personalization; Transformer Models; Fine-grained\nMatching; Review-based Matching\nACM Reference Format:\nKeping Bi, Qingyao Ai, and W. Bruce Croft. 2021. Learning a Fine-Grained\nReview-based Transformer Model for Personalized Product Search. In Pro-\nceedings of the 44th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (SIGIR â€™21), July 11â€“15, 2021, Virtual\nEvent, Canada. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/\n3404835.3462911\n1 INTRODUCTION\nIn product search, usersâ€™ purchase behaviors usually depend on\ntheir individual preferences in addition to product relevance. Aware\nof this point, recent studies [ 3, 4, 6, 18, 41, 45] have explored to\nincorporate personalization in the product retrieval models and\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nSIGIR â€™21, July 11â€“15, 2021, Virtual Event, Canada\nÂ© 2021 Association for Computing Machinery.\nACM ISBN 978-1-4503-8037-9/21/07. . . $15.00\nhttps://doi.org/10.1145/3404835.3462911\nproduced significant improvements in the search quality. A typ-\nical paradigm of existing personalized product search models is\nto represent the user intents and items explicitly with embedding\nvectors and match them in the latent space with dot product or\ncosine similarity to yield the item score [2â€“4, 6, 7, 18]. Under this\nparadigm, userâ€™s search intents are usually represented by a func-\ntion of the query vector and the user vector, which can be a convex\ncombination [3], a simple addition [2], several neural layers [18], or\na transformer encoder [6]. Item representations are usually learned\nby predicting words in the item reviews [ 2â€“4, 6, 7, 18], and user\nvectors, which represents the userâ€™s personalized preferences, are\nrepresented similarly [3] or as a weighted combination of items\nvectors with attention mechanisms [2, 6, 18].\nDespite its popularity, the existing product search paradigm has\nseveral limitations in practice. First, in the existing product search\nframework, the item scoring is based on matching at the user/item\nlevel [2, 3, 6, 18] instead of the finer-grained level, e.g., user/item\nreviews. Thus how a specific user preference mentioned in the user\nreviews matches an item property indicated in the item reviews\ncould not be captured sufficiently. While a top-retrieved item is\nclose to the user intent or some of the usersâ€™ historical purchases\nin the latent space, why the model considers them close is not\nclear. Second, despite their efforts on constructing dynamic user\nrepresentations under different contexts [2, 6, 18], existing prod-\nuct search studies always represent items statically regardless of\nthe context [2â€“4, 6, 7, 18]. During the representation learning, all\nthe associated reviews are considered to have equal importance.\nHowever, the same aspect of an item may play different roles in\nrepresenting the item when matching with various user prefer-\nences towards the aspect. For example, for a user who has tooth\nwhitening needs, reviews that comment on the toothpasteâ€™s whiten-\ning function should be more important to represent the item than\nthe long-lasting breath freshening property. However, in existing\nproduct search models, a review complaining about the shipment\nand package handling of the toothpaste may be considered equally\nimportant as other reviews since the item representations are built\nas static vectors without considering user intents.\nGiven these limitations, in this paper, we propose to match user\nintents and items at the level of finer-grained information (e.g., their\nassociated reviews) instead of explicitly representing them with\nstatic vectors. Specifically, we score an item based on the sequence\nof the query, user reviews, and item reviews with a transformer\n[37] architecture, where every unit, i.e., query or user/item review,\ncould interact with each other during matching. We refer to our\nmodel as the review-based transformer model, abbreviated as RTM.\nRTM has a couple of advantages over existing Transformer-based\n[6] or other neural model based product search approaches [2, 3, 18].\narXiv:2004.09424v3  [cs.IR]  3 Jun 2021\nFirst, RTM conducts user-item matching at the review level so that\nthe reason an item is ranked to the top can be explained by some\nuseful reviews that draw more attention from other units in the\nsequence. Second, in RTM, the importance of each user and item re-\nview during matching is dynamically adapted in different sequences,\nwhere both users and items carry dynamic information under dif-\nferent context. When encoded by a multi-layer RTM, the review\nrepresentation is dynamically changed according to its interactions\nwith other units in the sequence. Also, RTM represents user and\nitem based only on their reviews without the need for their iden-\ntifiers, so it can easily generalize to the users and items that have\nassociated reviews but have not appeared in the training set. Last\nbut not least, RTM can conduct more flexible personalization than\nmost existing personalized product search models [3, 4, 7, 18, 43].\nPersonalization in RTM can vary from no to full effect depending\non the contexts since the user reviews could have zero accumulative\nattention weights and so does the query. Our experimental results\nconfirm our modelâ€™s advantages by showing that RTM significantly\noutperforms the state-of-art baselines. To better understand the\nmodel and explain the search results, we also analyze the influence\nof different settings on RTM and conduct case studies to show how\nRTM identifies important information during matching.\nOur contributions in this paper can be summarized as follows:\n1) we propose a review-based transformer model (RTM) for per-\nsonalized product search that is superior to existing models in\nterms of finer-grained matching, dynamic user/item representation,\ngeneralization ability, and the influence of personalization; 2) our\nexperimental results show that our RTM achieves significantly bet-\nter performance than state-of-the-art personalized product search\ntechniques; 3) we analyze the model property and conduct case\nstudies to understand the model behaviors better.\n2 RELATED WORK\nOur work is closely related to product search, personalized web\nsearch, and transformer-based retrieval models.\nProduct Search. Since product information is more structured,\nearlier research uses facets such as brands, prices, and categories\nfor product search [ 22, 36]. However, these approaches cannot\nhandle free-form user queries. To support search based on keyword\nqueries, Duan and Zhai [15], Duan et al. [16] extended language-\nmodel-based techniques by assuming that queries are generated\nfrom the mixture of one language model of the background corpus\nand the other one of products conditioned on their specifications.\nWord mismatch problems still exist in these approaches. Van Gysel\net al. [35] noticed this problem and proposed representing and\nmatching queries and products in the latent semantic space.\nAware of the importance of personalization in product search,\nAi et al. [3] proposed a hierarchical embedding model where they\nuse a convex combination of the query and user vector to predict\npurchased items. Guo et al. [18] represent long and short-term user\npreferences with an attention mechanism applied to usersâ€™ recent\npurchases and their global vectors. Recently, from the analysis of\ncommercial search logs, Ai et al. [2] observed that personalization\ndoes not always have a positive effect. They further proposed a\nzero-attention model (ZAM) that can control the influence of per-\nsonalization. However, the maximal effect personalization can have\nis equal to the query. Bi et al. [6] found this limitation and proposed\na transformer model to encode the query and historically purchased\nitems where personalization can have none to full effect.\nThere are also studies on other aspects such as popularity, other\ninformation sources (e.g., images), diversity, and labels for training\nin product search. Long et al . [23] incorporated popularity with\nrelevance for product ranking. Di et al . [13] and Guo et al . [19]\ninvestigated on using images as a complementary signal. Ai et al.\n[4] proposed an explainable product search model with dynamic\nrelations such as brand, category, also-viewed, also-bought, etc.\nEfforts have also been made to improve result diversity to satisfy\ndifferent user intents behind the same query [27, 42]. In terms of\ntraining signals, Wu et al. [39] jointly modeled clicks and purchases\nin a learning-to-rank framework and [20] compared the effect of\ndifferent labels such as click-rate, add-to-cart ratios, and order rates.\nMore recently, Zhang et al. [44] integrated the graph-based feature\nwith neural retrieval models for product search. Xiao et al. [41] stud-\nied personalized product search under streaming scenarios. Ahuja\net al. [1] learned language-agnostic representations for queries and\nitems that can support search with multiple languages. There are\nalso studies on interactive product search such as in a multi-page\nsearch setting [8] and in conversational systems [7, 43].\nMost existing work either focuses on non-personalized prod-\nuct search or conducts personalized product search with static\nuser/item representations. In contrast, we propose an adaptive per-\nsonalization model for product search and conduct item scoring in\na novel paradigm, i.e., dynamic matching user intents and items at\nthe review level instead of explicitly represent them in the semantic\nspace and match them directly.\nPersonalized Web Search. Personalization has also been stud-\nied in the context of Web search, where results are customized\nto each user in order to maximize their satisfaction [17]. Existing\napproaches usually infer usersâ€™ personal needs from their locations,\nsearch histories, clicked documents, etc., and then re-rank results\naccordingly [5, 9, 10, 31, 33]. While users could behave differently\nfor the same query [14, 38], personalization does not always benefit\nsearch quality. Based on the analysis of user behavior patterns in\nthe large-scale logs on Live Search, Teevan et al . [34] observed\nthat the effectiveness of personalization in Web search depends on\nmultiple factors such as result entropy, result quality, search tasks,\nand so on.\nWe focus on product search in this paper, where personalization\nis more appealing than in Web search. While relevance is usually\nthe primary criterion in Web search, user purchases depend on\nboth item relevance and user preferences in product search.\nTransformer-based Retrieval Models. After the pre-trained\ncontextual language models, i.e., BERT [12], grounded on the trans-\nformer architecture, achieved compelling performance on a wide\nrange of natural language processing tasks, more studies have\nexplored leveraging BERT in information retrieval tasks as well.\nNogueira and Cho [26] show BERTâ€™s effectiveness on passage rank-\ning, and Dai and Callan [11] demonstrate that BERT can leverage\nlanguage structures better and enhance retrieval performance on\nqueries in natural languages. Wu et al . [40] proposed a passage\ncumulative gain model that applies a sequential encoding layer on\ntop of the BERT output of a query-passage pair to score a document.\nQu et al. [28] refine the original BERT model with an additional\nFigure 1: Our Review-based Transformer Model (RTM).\nattention layer on each question-utterance pair to attentively se-\nlect useful history to identify the answer span in conversational\nquestion answering.\nOur model is based on transformers instead of BERT. In other\nwords, we leverage the transformer architecture to dynamically\nmatch the query-user pair and the item at the review level but we\ndo not represent queries and reviews with pre-trained BERT.\n3 REVIEW-BASED TRANSFORMER MODEL\nThis section introduces each component of our review-based trans-\nformer model (RTM) and how RTM conducts personalized item\nscoring. Then we show how RTM is optimized and provide inter-\npretations of RTM from the perspective of model design.\n3.1 Personalized Item Scoring\nIn contrast to previous studies that have explicit representations\nfor both users and items [2, 3, 18] or just for items [2, 6, 18], our\nmodel does not represent the user or item with a single vector.\nInstead, we consider the user and itemâ€™s historical reviews as the\nbasic unit carrying their information and learn to score the item\ngiven the query-user pair using the interactions between their basic\ninformation units. In this way, the matching of a query-user pair\nand an item can be conducted at a finer grain and could potentially\ncapture more connections between the user and the item, which\nwe will illustrate later in Section 3.4 in detail.\nLet ğ‘ be a query submitted by a user ğ‘¢ and ğ‘– be an item in ğ‘†ğ‘–,\nwhich is the collection of all the items.ğ‘…ğ‘¢ = (ğ‘Ÿğ‘¢ğ‘–ğ‘¢1 ,ğ‘Ÿğ‘¢ğ‘–ğ‘¢2 ,Â·Â·Â· ,ğ‘Ÿğ‘¢ğ‘–ğ‘¢ğ‘š )\nand ğ‘…ğ‘– = (ğ‘Ÿğ‘¢ğ‘–1ğ‘–,ğ‘Ÿğ‘¢ğ‘–2ğ‘–,Â·Â·Â· ,ğ‘Ÿğ‘¢ğ‘–ğ‘›ğ‘–)denote the sequence of ğ‘š and ğ‘›\nreviews associated with ğ‘¢ and ğ‘– respectively in a chronological\norder, where ğ‘Ÿğ‘¢ğ‘–ğ‘¢ğ‘˜ (1 â‰¤ğ‘˜ â‰¤ğ‘š)is the review ğ‘¢ wrote for her ğ‘˜-th\npurchased item ğ‘–ğ‘¢ğ‘˜ and ğ‘Ÿğ‘¢ğ‘–ğ‘˜â€²ğ‘–(1 â‰¤ğ‘˜â€²â‰¤ğ‘›)is the review associated\nwith ğ‘¢ğ‘–ğ‘˜â€², which is the ğ‘˜â€²-th user who purchased ğ‘–. ğ‘šand ğ‘›are the\nlength of ğ‘…ğ‘¢ and ğ‘…ğ‘– respectively. As shown in Figure 1, we feed\nthe sequence of (ğ‘,ğ‘…ğ‘¢,ğ‘…ğ‘–)to an ğ‘™âˆ’layer transformer encoder to\nlet the query and userâ€™s purchase history interact with the itemâ€™s\nassociated reviews.\nInput Embeddings. Inspired by the architecture of BERT [12],\nour model also has three types of embeddings associated with each\ninput unit (query or review) to the transformer encoder. They are\nthe unit embedding that is computed from the words in the unit,\nwhich we will introduce later in Section 3.2, theposition embeddings\n[37] that indicate the position of a unit in the sequence, and the\nsegment embeddings that differentiate whether the unit is the query\nğ‘, a review from ğ‘¢, or a review about ğ‘–, denoted as ğ´,ğµ, and ğ¶\nrespectively, as shown in Figure 1.\nSince a recent review from the user may indicate her current\nintention better than her long-ago reviews and an itemâ€™s recent\nreview may reveal the itemâ€™s current properties better than an old\nreview about the item, position embeddings could be beneficial by\nindicating the temporal order of each review. While the time of\nreviews could be significant in some categories, it is also possible\nthat the information of items such as the music on a CD is static,\nand user preferences behind particular purchase needs stay similar\nfor a long time. Therefore, we make the position embeddings an\noptional choice in our model.\nThe segment embedding of a unit is also optional in RTM since\nthe model could infer which segment the current unit belongs\nto with their position embeddings. The reviews about ğ‘– always\nhave later positions than reviews of ğ‘¢, and ğ‘is always at position\n0. When position embeddings are not necessary in some cases,\nsegment embeddings are still needed to differentiate the input units.\nWe will show the effects of position and segment embeddings on\nour model later in Section 5.2.\nFormally, the input vector of each unit is the sum of the unit\nembedding and its associated optional position embedding and\nsegment embedding:\nq(0)= q +Iğ‘ğ‘œğ‘ ğ¸(0)+Iğ‘ ğ‘’ğ‘”ğ¸(ğ´)\nruiuk\n(0)= ruiuk +Iğ‘ğ‘œğ‘ ğ¸(ğ‘˜)+Iğ‘ ğ‘’ğ‘”ğ¸(ğµ),1 â‰¤ğ‘˜ â‰¤ğ‘š\nruikâ€²i (0)= ruikâ€²i +Iğ‘ğ‘œğ‘ ğ¸(ğ‘š+ğ‘˜â€²)+Iğ‘ ğ‘’ğ‘”ğ¸(ğ¶),1 â‰¤ğ‘˜â€²â‰¤ğ‘›\n(1)\nwhere ğ¸(Â·)is the embedding of position 0,1,Â·Â·Â· ,ğ‘š +ğ‘›or segment\nğ´,ğµ,ğ¶ ; Iğ‘ğ‘œğ‘  âˆˆ{0,1}and Iğ‘ ğ‘’ğ‘” âˆˆ{0,1}indicate whether position\nand segment embeddings are used in the computation; q,ruiuk , and\nruikâ€²i are the vector representation ofğ‘,ğ‘Ÿğ‘¢ğ‘–ğ‘¢ğ‘˜ , andğ‘Ÿğ‘¢ğ‘–ğ‘˜â€²ğ‘– respectively,\nwhich we will introduce in Section 3.2.\nTransformer Layers. The input sequence of vectors is then\npassing through transformer layers where the units interact with\neach other. As in [37], each transformer layer has two sub-layers:\na multi-head self-attention mechanism and a position-wise fully\nconnected feed-forward network.\nLet ğ¾ and ğ‘‰ denote vectors of the sequence (ğ‘,ğ‘Ÿğ‘¢ğ‘–ğ‘¢1 ,Â·Â·Â· ,ğ‘Ÿğ‘¢ğ‘–ğ‘¢ğ‘š ,\nğ‘Ÿğ‘¢ğ‘–1ğ‘–,Â·Â·Â· ,ğ‘Ÿğ‘¢ğ‘–ğ‘›ğ‘–)and let ğ‘„ be the vector of any unit in the sequence.\nFor attention head ğ‘—, the output vector is computed as a weighted\nsum of values ğ‘‰ according to the attention their corresponding\nkeys ğ¾ obtain with respect to ğ‘„, i.e.,\nAttnğ‘—(ğ‘„,ğ¾,ğ‘‰ )= softmax(\nğ‘„ğ‘—ğ¾ğ‘‡\nğ‘—\nâˆšï¸\nğ‘‘/â„\n)ğ‘‰ğ‘— (2)\nwhere ğ‘‘is the dimension size of a unit vector andâ„is the number of\nattention heads. ğ‘„ğ‘— = ğ‘„ğ‘Šğ‘„\nğ‘— ,ğ¾ğ‘— = ğ¾ğ‘Šğ¾\nğ‘— ,ğ‘‰ğ‘— = ğ‘‰ğ‘Šğ‘‰\nğ‘— and ğ‘Šğ‘—,ğ¾ğ‘—,ğ‘‰ğ‘—\nare project matrices for Attnğ‘—. Attn is applied for â„attention heads,\nand each output is concatenated and mapped to the final yield of\nthe multiple-head attention (MultiHeadAttn).\nThe position-wise feed-forward network (FFN) applies the same\ntransformation to each position separately and identically. Then\nthere is also a residual connection for each sub-layer, which allows\nthe input to go through the layer directly, followed by layer nor-\nmalization. So the output vector ğ‘¥(ğ‘¡)in the ğ‘¡-th transformer layer\n(1 â‰¤ğ‘¡ â‰¤ğ‘™)at an arbitrary position from0 to ğ‘š+ğ‘›can be computed\nas:\nğ‘¥(ğ‘¡)= LayerNorm(ğ‘¦+FFN(ğ‘¦))\nğ‘¦ = LayerNorm(ğ‘¥(ğ‘¡âˆ’1)+MultiHeadAttn(ğ‘¥(ğ‘¡âˆ’1),ğ¾,ğ‘‰ ))\n(3)\nwhere ğ‘¥(ğ‘¡âˆ’1) is the output of the (ğ‘¡ âˆ’1)-th layer, which can be\nobtained with Eq. 3 when ğ‘¡ > 1 and Eq. 1 when ğ‘¡ = 1. For more\ndetails about the transformers, please refer to [37].\nFinal Score. At last, we use the output query vector at the final\nlayer, i.e., q(l), which involves all the interactions between every\nunit in the sequence with query ğ‘to score item ğ‘–. Specifically, the\nscore of ğ‘–given ğ‘and ğ‘¢is computed with function ğ‘“:\nğ‘“(ğ‘,ğ‘¢,ğ‘– )= q(l)ğ‘Šğ‘œ (4)\nwhere ğ‘Šğ‘œ âˆˆRğ‘‘Ã—1.\nRTM can be degraded to a non-personalized model when there\nare no user reviews available. Also, RTM can score an item based\non its descriptions when it does not have associated reviews.\n3.2 Query/Review Representation\nIt is important to compute query and review representations on the\nfly so that the model can handle arbitrary queries and reviews dur-\ning inference time. Previous studies [3, 35] have explored methods\nto construct query embeddings directly from query words, such\nas averaging word embeddings or applying recurrent neural net-\nworks on the query word sequence. A state-of-the-art technique is\nto encode a query with a non-linear projection ğœ™ on the average\nquery word vectors:\nq = ğœ™({ğ‘¤ğ‘|ğ‘¤ğ‘ âˆˆğ‘})= tanh(ğ‘Šğœ™ğ‘ Â·\nÃ\nğ‘¤ğ‘ âˆˆğ‘wq\n|ğ‘| +ğ‘ğœ™ğ‘ ) (5)\nwhere ğ‘Šğœ™ğ‘ âˆˆRğ‘‘Ã—ğ‘‘ and ğ‘ğœ™ğ‘ âˆˆRğ‘‘ are learned during training, |ğ‘|\nis the length of query ğ‘, and wq âˆˆRğ‘‘ is the embedding of word\nğ‘¤ğ‘ in ğ‘. We use the same projection function ğœ™ with two different\nparameters ğ‘Šğœ™ğ‘Ÿ and ğ‘ğœ™ğ‘Ÿ to collect the initial input representation\nof review ğ‘Ÿ.\nIn this way, word embeddings are shared across reviews toward\nthe ranking optimization goal; significant words in the reviews\ncan be emphasized with more weights in the matrix ğ‘Šğœ™ğ‘Ÿ . Thus the\ninteraction between reviews can capture their keyword matching.\nWe also considered representing reviews with another popular\nmethod, i.e., paragraph vectors [ 21], by predicting words in the\nreview with the review vector. However, paragraph vectors need to\nbe trained beforehand and thus are difficult to generalize to unseen\nreviews. Moreover, in paragraph vectors, word embeddings can\nonly be updated by predicting the words with these review vectors,\nwhich is an unsupervised signal regardless of user purchases. Our\nexperiments also show that projected average embeddings yield\nbetter results than paragraph vectors, so we exclude paragraph\nvectors from this paper.\nAnother choice to represent reviews is using pre-trained BERT\n[12] to encode the word sequence with transformers directly. How-\never, we focus on modeling the interaction between the basic units,\ni.e., reviews, for both users and items, rather than capturing the\nsemantic meaning carried in each review. Since reviews can be long\nand noisy, it is not for sure better to capture the reviewsâ€™ complex\nsemantic structures than to identify some keywords in the reviews\nusing a reasonable and straightforward way. Also, encoding every\nreview with BERT will introduce tremendous computation costs,\nwhich prevents us from using it.\n3.3 Model Optimization\nSimilar to previous studies [2, 3], we optimize our model by max-\nimizing the log likelihood of the observed (query,user,purchased\nitem) triples, which can be written as:\nL=\nâˆ‘ï¸\n(ğ‘,ğ‘¢,ğ‘–)\nL(ğ‘,ğ‘¢,ğ‘– )=\nâˆ‘ï¸\n(ğ‘,ğ‘¢,ğ‘–)\n\u0000log ğ‘ƒ(ğ‘–|ğ‘,ğ‘¢)+log ğ‘ƒ(ğ‘,ğ‘¢)\u0001\nâ‰ˆ\nâˆ‘ï¸\n(ğ‘,ğ‘¢,ğ‘–)\nlog exp(ğ‘“(ğ‘,ğ‘¢,ğ‘– ))Ã\nğ‘–â€²âˆˆğ‘†ğ‘– exp(ğ‘“(ğ‘,ğ‘¢,ğ‘– â€²))\n(6)\nwhere ğ‘“(ğ‘,ğ‘¢, Â·)is computed with Eq. 4, and log ğ‘ƒ(ğ‘,ğ‘¢)is ignored\nbecause it is predefined as a uniform distribution. Due to the large\nnumber of items in the candidate set ğ‘†ğ‘–, we adopt the negative\nsampling strategy [21, 25] to estimate Eq. 6 and randomly selectğ‘˜ğ‘›ğ‘’ğ‘”\nnegative samples from ğ‘†ğ‘– according to a uniform distribution. In\naddition, different L2 regularization settings could not improve the\nperformance in our experiments, which indicates that overfitting is\nnot a problem for our experiments. Hence, we do not include the\nregularization terms in Eq. 6.\n3.4 Model Interpretation\nExisting product search models [2, 3, 6, 19] consider reviews asso-\nciated with a user or item as a whole and do not differentiate their\ninfluences when matching users and items. In contrast, by conduct-\ning the attention mechanism on each unit in the sequence of the\nquery, user reviews, and item reviews, RTM can explicitly capture\nthe interactions between the finer-grained units, i.e., query and\nuser/item reviews. These fine-grained interactions reflect several\nessential aspects of the model:\nExplainable Item Matching. The attention scores of each item\nreview concerning the query indicate which review plays a cru-\ncial role in matching this item with the purchase intent behind\nthe query-user pair. We can rank the reviews with their attention\nweights from large to small for each retrieved item and display the\nranking to the user. In contrast to showing item reviews according\nto their recency as most e-commerce platforms do, a system based\non RTM could help users understand why an item is retrieved by\nshowing the most potentially helpful reviews, which may further\nfacilitate their purchase decisions.\nDynamic Review Representation. RTM can offer more potent\nlearning abilities with multiple transformer layers, which could be\nbeneficial when more interactions between reviews are needed. In\na multiple-layer RTM, the review embeddings associated with the\nuser or the item are dynamically changed based on their interactions\nwith the other units in the sequence (ğ‘,ğ‘…ğ‘¢,ğ‘…ğ‘–). In this case, more\ninteractions happen between units when attending to (ğ‘,ğ‘…ğ‘¢,ğ‘…ğ‘–)\nwith ğ‘Ÿğ‘¢ğ‘–ğ‘¢ğ‘˜ âˆˆğ‘…ğ‘¢ and ğ‘Ÿğ‘¢ğ‘–ğ‘˜â€²ğ‘– âˆˆğ‘…ğ‘–.\nThe final representation of a user review ğ‘Ÿğ‘¢ğ‘–ğ‘¢ğ‘˜ is learned from\nits interaction with ğ‘…ğ‘¢ and ğ‘…ğ‘–. On the one hand, the self-attention\nmechanism of attending to ğ‘…ğ‘¢ with ğ‘Ÿğ‘¢ğ‘–ğ‘¢ğ‘˜ updates the embedding of\nğ‘Ÿğ‘¢ğ‘–ğ‘¢ğ‘˜ by interacting with all the reviews from userğ‘¢, where similar-\nities and dissimilarities between reviews are taken into account. On\nthe other hand, attending to ğ‘…ğ‘– with ğ‘Ÿğ‘¢ğ‘–ğ‘¢ğ‘˜ indicates that how the\nspecific preference ğ‘¢shows for ğ‘–ğ‘¢ğ‘˜ is satisfied by the descriptions\nof ğ‘– from other usersâ€™ perspectives. Reviews in ğ‘…ğ‘– specify the other\nusersâ€™ opinion towards item ğ‘–, which could reflect ğ‘–â€™s advantages\nand disadvantages user ğ‘¢may care about.\nSimilarly, the final vector of an item review ğ‘Ÿğ‘¢ğ‘–ğ‘˜â€²ğ‘– is also dynam-\nically changed according to its interaction with ğ‘…ğ‘¢ and ğ‘…ğ‘–. The\ninteractions between ğ‘Ÿğ‘¢ğ‘–ğ‘˜â€²ğ‘– and ğ‘…ğ‘– readjust the representation of\nğ‘Ÿğ‘¢ğ‘–ğ‘˜â€²ğ‘– by considering its relation with the other reviews in ğ‘…ğ‘–. In\naddition, attending to the reviews in ğ‘…ğ‘¢ with ğ‘Ÿğ‘¢ğ‘–ğ‘˜â€²ğ‘– indicates how\nğ‘Ÿğ‘¢ğ‘–ğ‘˜â€²ğ‘– matches the preferences expressed in the userâ€™s each historical\nreview. This information and how each user preference is satisfied\nby item reviews carried in ğ‘Ÿğ‘¢ğ‘–ğ‘¢ğ‘˜ could be both beneficial to score\nitem ğ‘–.\nDynamic Personalization . RTM has the flexibility to make\npredictions with variable (none to full) emphasis on personaliza-\ntion, similar to [6], by learning different accumulative weights for\nthe userâ€™s reviews. In RTM,ğ‘(ğ‘™)in Eq. 4 is computed from attending\nto the sequence of (ğ‘,ğ‘…ğ‘¢,ğ‘…ğ‘–)in the (ğ‘™âˆ’1)-th layer with ğ‘. Person-\nalization can take full effect when the attention weight assigned to\nğ‘is 0 and no effect when the reviews in ğ‘…ğ‘¢ have 0 accumulative\nattention weights.\nIn addition, in contrast to [2, 6] where personalization degree\nfor a user depends only on her query, RTM is more flexible since it\ncould perform various degrees of personalization for different items\ngiven the same query-user pair since attention weights onğ‘…ğ‘¢ could\nvary when ğ‘…ğ‘– is different even with the sameğ‘. This strategy allows\nthe user profile to have differentiated effects when it contains much\nor no useful information while matching various items.\n4 EXPERIMENTAL SETUP\nIn this section, we first show how we construct the datasets and con-\nduct evaluation, then we describe the baseline models and training\nsettings for different models.\n4.1 Datasets and Evaluation\nThe Amazon product search dataset [ 24] 1 is the only available\ndataset for product search that have user reviews. As in previous\nwork [3, 4, 6, 7, 18, 35, 43], we use it for our experiments. Specifically,\nwe use the 5-core data [24] where each user and each item has at\nleast 5 associated reviews. Our experiments are based on three\ncategories of different scales, which are Clothing, Shoes & Jewelry ,\nSports & Outdoors , and CDs & Vinyl . The statistics are shown in\nTable 1.\nQuery Construction . Following the same paradigm used in\n[3, 4, 6, 7, 18, 35, 43], we construct queries for each purchased item\nwith the product category information. This strategy is based on the\nfinding that directed product search is usersâ€™ search for a producerâ€™s\n1http://jmcauley.ucsd.edu/data/amazon/\nTable 1: Statistics of the Amazon datasets.\nDataset Sports & Clothing, Shoes & CDs &\nOutdoors Jewelry Vinyl\n#Users 35,598 39,387 75,258\n#Items 18,357 23,033 64,443\n#Reivews 296,337 278,677 1,097,591\n#Queries 1,538 2,021 695\n#Vocab 32,386 21,366 202,959\nReviewLen 89.18Â±106.99 62.22 Â±60.16 174.56 Â±177.05\nQueryLen 7.07 Â±1.74 7.14 Â±1.97 5.77 Â±1.65\n#Query-user pairs\nTrain 269,850 467,651 1,524,168\nValid/Test 776/910 4,106/4,025 10,930/10,077\n#Purchased items per query-user pair\nTrain 1.16 Â±0.55 1.30 Â±0.82 2.95 Â±8.53\nValid/Test1.01Â±0.08/1.02Â±0.24 1.00 Â±0.08/1.00Â±0.05 1.12 Â±0.61/1.11Â±0.56\nname, a brand, or a set of terms describing product category [30].\nPrecisely, we extract the multi-level category information from the\nmeta-data, concatenate the words in the categories, and remove\nstopwords and duplicate words to form a query string. Since an item\ncould belong to multiple categories, there may be multiple extracted\nqueries for the item. Each query is considered as the initial query\nissued by the user and leading to purchasing the item. The queries\nare general and do not reveal specific details of the purchase items.\nTraining/Validation/Test Splits. As in [6], we split each dataset\ninto training, validation, and test sets according to the following\nsteps. First, we randomly put 70% queries in the training set, and\nthe rest 30% are shared by the validation and test sets so that none\nof the test queries have been seen during training. Then, we par-\ntition each userâ€™sâ€™ purchases to training, validation, and test set\naccording to the ratio 0.8:0.1:0.1 in chronological order. If none of\nthe queries associated with the purchased item is in the test set,\nthe purchase will be moved back to the training set. In contrast to\nrandomly partitioning data into training and test set as in previous\nwork [3, 4, 7, 43], our dataset is closer to a real scenario, where\nall the purchases in the test set happen after the purchases in the\ntraining set. This also makes retrieval on our test set harder since\nfuture information can be used to predict past purchases in the\nprevious datasets. Also, our training and test sets have less similar\ndistributions compared with previous datasets, which makes model\nprediction more difficult as well.\nEvaluation Metrics. Following the typical way of collecting\ncandidates with an efficient initial ranker and using neural models\nfor re-ranking in document retrieval, we re-rank the candidate items\nretrieved by BM25 [29] with each method and obtain the ranking\nlists. 2 Then we use Mean Reciprocal Rank (MRR), Normalized\nDiscounted Cumulative Gain at 20 (NDCG@20), and Recall at 20\n(R@20) as evaluation metrics. MRR shows the first position where\nany purchased item is retrieved; NDCG@20 focuses on the top\n20 itemsâ€™ ranking performance where higher positions have more\ncredits, and R@20 indicates how many target items are retrieved\nin the top 20 results in total. Fisher random test [32] with ğ‘ < 0.05\nis used to measure significant differences between results.\n2In the experiments where each method ranks all the items in the collections, we have\nsimilar observations, so we do not include this setting for space concerns.\n4.2 Baselines\nWe compare our RTM with eight representative baselines:\nBM25: The BM25 [29] model is based on word matching between\nqueries and item reviews, which also provides the initial ranking\nlists for the other models.\nPOP: The Popularity (POP) model ranks items according to their\nfrequency of being purchased in the training set.\nLSE: The Latent Semantic Entity model (LSE) [35] is an embedding-\nbased non-personalized model that learns the vectors of words and\nproducts by predicting the products with n-grams in their reviews.\nIt then scores the products with the cosine similarity between their\nvectors and query vectors.\nQEM: The Query Embedding Model (QEM) [ 2], is also a non-\npersonalized model that conducts item generation based on the\nquery embedding, and item embeddings are learned by predicting\nwords in their associated reviews.\nHEM: The Hierarchical Embedding Model (HEM) [ 3] has the\nitem generation model and language models of users and items. It\nbalances the effect of personalization by applying a convex combi-\nnation of user and query representation.\nAEM: The Attention-based Embedding Model (AEM) [ 2, 18]\nconstructs query-dependent user embeddings by attending to usersâ€™\nhistorical purchased items with the query. 3\nZAM: The Zero Attention Model (ZAM) [2] extends AEM with a\nzero vector and conducts differentiated personalization by allowing\nthe query to attend to the zero vector.\nTEM: The Transformer-based Embedding Model (TEM) [6] is\na state-of-the-art model that encodes query and historically pur-\nchased items with a transformer and does item generation based\non the encoded query-user information.\nBM25, POP, LSE, and QEM are non-personalized retrieval models,\nand all the rest are personalized product search models.\n4.3 Implementation Details\nAll the baselines were trained for 20 epochs with 384 samples in\neach batch according to the settings in their original papers [2, 3, 6],\nand they can converge well. We trained our model for 30 epochs\nwith 128 samples in each batch. In the baseline models, each word\nin the reviews of a target item corresponds to one entry (word, item,\nuser, query) in the batch, while in RTM each target item has one\nentry (item, user, query) in a batch. The number of negative samples\nfor items or words in all the models is set to 5. We set the embedding\nsize of all the models to 128. Larger embedding sizes do not lead to\nsignificant differences, so we only report results with ğ‘‘ = 128. The\nsub-sampling rate of words in all the neural baseline models is set\nto 1ğ‘’âˆ’5. We sweep the number of attention heads â„from {1,2,4,8}\nfor AEM, ZAM, TEM, and our RTM. For TEM and RTM, we vary the\nnumber of transformer layers ğ‘™ from 1 to 3 and set the dimension\nsize of the feed-forward sub-layer of the transformer from {128,\n256, 512}. We cutoff reviews to 100 words and limit the number of\nhistorical reviews for a user and an item, i.e.,ğ‘šand ğ‘›in Figure 1, to\n10 and 30, respectively. We use Adam withğ›½1 = 0.9,ğ›½2 = 0.999 to\noptimize RTM. The learning rate is initially set from {0.002, 0.005,\n0.01, 0.02} and then warm-up over the first 8,000 steps, following\n3The attention models described in [2] and [18] are highly similar to each other, so we\nonly implement the one in [2] and named it as AEM.\nthe paradigm in [12, 37]. To make the training of RTM more stable,\nwe initialize the parameters of words with embeddings pre-trained\nwith Word2Vec [25]. For the number of candidates, we use the\ntop 100 results from BM25 for re-ranking on ğ‘†ğ‘ğ‘œğ‘Ÿğ‘¡ğ‘  and ğ¶ğ‘™ğ‘œğ‘¡â„ğ‘–ğ‘›ğ‘”\nwhose recall values are 0.425 and 0.343, respectively. Onğ¶ğ·ğ‘ , the\nğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™@100 of BM25 is only 0.108, so we use the top 1000 items\nfor re-ranking, which has a higher recall - 0.370. Our code can be\nfound at https://github.com/kepingbi/ProdSearch.\n5 RESULTS AND DISCUSSION\nIn this section, we first compare the overall performance of RTM\nand the baseline models. Then we conduct model analysis and case\nstudies to interpret the model behavior.\n5.1 Overall Performance\nTable 2 shows each systemâ€™s overall ranking performance on the\nthree datasets. We show four variants of RTM that use both, either,\nor none of the position embeddings and segment embeddings by\nsetting Iğ‘ğ‘œğ‘  and Iğ‘ ğ‘’ğ‘” in Eq. 1 to 0 or 1. We illustrate the effect\nof position and segment embeddings in Section 5.2. Note that the\nnumbers in Table 2 are small for two reasons: 1) there is only about 1\ntarget purchased item out of 20k~65k items for each query-user pair\nin the test sets, shown in Table 1. 2) as we mentioned in Section 4.1,\nthe search task on our sequentially split data is more challenging\nthan on the randomly divided partitions in [3, 7, 43].\nAs in previous studies [2, 3, 18], we observe that non-personalized\nmodels perform worse than personalized models. BM25 performs\nbetter on ğ‘†ğ‘ğ‘œğ‘Ÿğ‘¡ğ‘  and ğ¶ğ‘™ğ‘œğ‘¡â„ğ‘–ğ‘›ğ‘” than ğ¶ğ·ğ‘ , which indicates that term\nmatching plays a more important on ğ‘†ğ‘ğ‘œğ‘Ÿğ‘¡ğ‘  and ğ¶ğ‘™ğ‘œğ‘¡â„ğ‘–ğ‘›ğ‘”. In con-\ntrast, POP matters more on ğ¶ğ·ğ‘  than ğ‘†ğ‘ğ‘œğ‘Ÿğ‘¡ğ‘  and ğ¶ğ‘™ğ‘œğ‘¡â„ğ‘–ğ‘›ğ‘”. Non-\npersonalized neural models do not always outperform BM25, espe-\ncially on ğ‘†ğ‘ğ‘œğ‘Ÿğ‘¡ğ‘  and ğ¶ğ‘™ğ‘œğ‘¡â„ğ‘–ğ‘›ğ‘”, probably because semantic matching\nbrings limited benefits when most candidate results from BM25\nhave exact term matching.\nAmong the personalized retrieval baselines, TEM achieves the\nbest performance, which is consistent with [ 6] and confirms the\nbenefit of using transformers. ZAM performs better than AEM\nmost of the time, indicating that dynamic personalization is helpful.\nSimilar to [2, 18], we also observe that HEM could not outperform\nthe attention-based models, which indicates that building a dynamic\nuser profile helps improve the result quality compared with using\nstatic user representation across the search sessions.\nOn all the categories, RTM achieves the best performance. It\nhas significant improvements upon the best baseline â€“ TEM â€“ in\nmost metrics on all the datasets. It confirms that by modeling the\ndynamic matching between user and item, RTM has significant\nadvantages over models with static item profiles, although some\nof them also have dynamic user profiles (AEM, ZEM, and TEM).\nIn addition, by capturing the finer-grained matching at the review\nlevel, RTM can differentiate different items better than matching\nthem at the item level in TEM.\n5.2 Model Analysis\nPosition and Segment Embeddings. As shown in Table 2, RTM\nwithout position and segment embeddings has the worse perfor-\nmance most of the time, showing the necessity to differentiate the\nTable 2: Comparison between the baselines and our proposed RTM. â€˜*â€™ marks the best baseline performance. â€˜ â€ â€™ indicates\nsignificant improvements over all the baselines in Fisher Random test [32] with ğ‘ < 0.05.\nDataset Sports & Outdoors Clothing, Shoes & Jewelry CDs & Vinyl\nModel ğ‘€ğ‘…ğ‘… ğ‘ğ·ğ¶ğº@20 ğ‘…@20 ğ‘€ğ‘…ğ‘… ğ‘ğ·ğ¶ğº@20 ğ‘…@20 ğ‘€ğ‘…ğ‘… ğ‘ğ·ğ¶ğº@20 ğ‘…@20\nNon-personalized\nBM25 0.049 0.051 0.173 0.044 0.051 0.160 0.011 0.015 0.042\nPOP 0.033 0.055 0.158 0.030 0.044 0.112 0.015 0.018 0.039\nLSE 0.026 0.047 0.148 0.041 0.058 0.135 0.017 0.021 0.044\nQEM 0.049 0.070 0.172 0.039 0.057 0.144 0.010 0.014 0.037\nPersonalized\nHEM 0.044 0.071 0.197 0.043 0.061 0.144 0.021 0.030 0.073\nAEM 0.047 0.076 0.209 0.048 0.069 0.162 0.023 0.033 0.084\nZAM 0.052 0.083 0.220 0.047 0.069 0.166 0.025 0.036 0.086\nTEM 0.060* 0.094* 0.238* 0.052* 0.076* 0.182* 0.026* 0.038* 0.095*\nRTM (Iğ‘ğ‘œğ‘  =0,Iğ‘ ğ‘’ğ‘”=0) 0.065 0.092 0.208 0.061â€  0.087â€  0.190â€  0.027 0.036 0.084\nRTM (Iğ‘ğ‘œğ‘  =0,Iğ‘ ğ‘’ğ‘”=1) 0.058 0.093 0.242 0.068â€  0.099â€  0.224â€  0.030â€  0.042â€  0.095\nRTM (Iğ‘ğ‘œğ‘  =1,Iğ‘ ğ‘’ğ‘”=0) 0.082â€  0.110â€  0.234 0.071â€  0.101â€  0.219â€  0.030â€  0.039 0.085\nRTM (Iğ‘ğ‘œğ‘  =1,Iğ‘ ğ‘’ğ‘”=1) 0.096â€  0.123â€  0.237 0.069â€  0.099â€  0.218â€  0.028 0.040 0.094\nSports Clothing CDs\n0.04\n0.06\n0.08\n0.10\n0.12\nNDCG@20\nRTM(1-Layer)\nRTM(2-Layer)\nRTM(3-Layer)\nTEM(1-Layer)\nTEM(2-Layer)\nTEM(3-Layer)\n(a) Effect of Transformer Layer Numbers\n4 5 6 7 8 9 10\nUser Review Count\n0.00\n0.05\n0.10\nNDCG@20\nPOP\nBM 25\nLSE\nHEM\nQEM\nAEM\nZAM\nTEM\nRTM (b) Effect of User Review Counts\n0 1 2 3 4 5 6 7 8 9\nSlot ID w.r.t. Ranking Positions\n10\n20\n30\n40Average Item Review Count\nPOP\nBM 25\nLSE\nHEM\nQEM\nAEM\nZAM\nTEM\nRTM (c) Item Review Count w.r.t. Ranking Positions\nFigure 2: Model analysis in terms of different aspects. Figure 2b and 2c correspond to Clothing, Shoes & Jewelry.\nreviews from the user and the item. On ğ‘†ğ‘ğ‘œğ‘Ÿğ‘¡ğ‘  and ğ¶ğ‘™ğ‘œğ‘¡â„ğ‘–ğ‘›ğ‘”, posi-\ntion embeddings are always helpful, which indicates that the userâ€™s\nrecent purchases have different influences from the long-ago pur-\nchases. The latest reviews reveal more accurate information about\nthe products. For example, usersâ€™ preferences on clothes styles\nmay change according to the seasonal trend; a swimming earplug\nproduct has been updated lately, and recent customer reviews com-\nplained the new version is not as comfortable as before. In these\ncases, position embeddings that capture reviewsâ€™ recency can help\nidentify the current user preference and item status and potentially\nimprove the search quality.\nIn contrast, on ğ¶ğ·ğ‘ , incorporating position embeddings does\nnot lead to better evaluation results than using segment embed-\ndings alone. This shows that the order of user and item reviews do\nnot need to be differentiated as long as we know which of them\ncorresponds to the user and item respectively. This observation is\nconsistent with our intuition that the content in a CD and sound\nquality are usually static regardless of the review order. We can also\ninfer that long-ago purchases play similar roles as recent purchases\nin terms of representing user preferences on ğ¶ğ·ğ‘ .\nUsing both position and segment embeddings does not always\nlead to the best evaluation results. The possible reasons are that\nthe low and high positions can indicate which sections of the input\ncorrespond to the user and item respectively, which makes segment\nembeddings not necessary sometimes. When the chronological or-\nder of reviews is not crucial in some categories, the sequence of re-\nviews does not matter so that position embeddings could introduce\nnoise to the model and harm the performance, as we mentioned in\nSection 3.1.\nNumber of Layers. RTM achieves the best performance with 2\nlayers on ğ¶ğ·ğ‘  and 1 layer on ğ‘†ğ‘ğ‘œğ‘Ÿğ‘¡ğ‘  and ğ¶ğ‘™ğ‘œğ‘¡â„ğ‘–ğ‘›ğ‘”. This indicates\nthat the dynamic review representation introduced in Section 3.4 is\nbeneficial on larger datasets. As shown in Table 1,ğ¶ğ·ğ‘  has more\naverage reviews per user/item than the other two datasets, and\nit also has a larger vocabulary, which makes the contexts of each\nreview more varied. We will further analyze the behavior of RTM\nwith single and multiple layers in Section 5.3 by case studies.\nUser Review Count. Figure 2b shows the performance of query-\nuser pairs with different counts of user reviews in the training set of\nğ¶ğ‘™ğ‘œğ‘¡â„ğ‘–ğ‘›ğ‘”. The other two datasets show similar trends. The minimum\nnumber is 4 since each user has at least five reviews and 10% of\nthem has been put to the validation or test set, as mentioned in\nSection 4.1. According to Section 4.3, the max number is 10 since\nwe use at most 10 historical reviews for users. The corresponding\nnumbers of the query-user pairs with user review count from 4\nto 10 are 1297, 655, 453, 417, 243, 180, and 780, respectively. We\nTable 3: A case of query, user, and purchased item for the single-layer RTM case study. The attention weights are average from\nall the 8 attention heads.\nQuery: \"clothing shoe jewelry men big tall active athletic sock\" (Attention weight w.r.t.ğ‘(0):0.031)\nAttn ID User Reviews Attn ID Item Reviews\n0.027 ur3 Fruit of the Loom T-Shirt. This shirt is a great value for the\nprice. It is snug and fits me perfectly . There is enough\nroom to wear the shirt with an under-shirt as well, giving me\nwarmth.\n0.042 ir9 As expected bought these socks for my husband because he is always wearing holes\nin his socks, and I am looking to Carhartt to provide a sock that might be able to better\nwithstand his abuses. So far they are sufficiently cushiony , they stay up on his\nlegs and get the job done. Time will tell if they are as durable as I am hoping.\n0.010 ur2 Casio Menâ€™s MQ24-1E black resin watch. I love it because it\nis small, easy to fasten, lightweight, and inexpensive. I had to\ncut the band for a smaller wrist, but glad I bought the watch.\n0.018 ir4 Canâ€™t go wrong with Carhartt. These are great socks. There are great for everyday\nwork. Theyâ€™re comfortable and durable. Itâ€™s a great product Not much more to say.\ncan see that RTM has consistent improvements over other models\non query-user pairs with different review counts and RTM can\nachieve compelling performance with a decent small number of\nuser reviews.\nItem Review Count w.r.t. Ranking Positions. Figure 2c com-\npares different methods in terms of their tendency to rank items\nwith more reviews to higher positions, i.e., their preferences on pop-\nularity. We only showğ¶ğ‘™ğ‘œğ‘¡â„ğ‘–ğ‘›ğ‘” since the other two datasets have\nsimilar trends. For the top 100 items ranked by each method, we\ngroup them into 10 slots and show the itemsâ€™ average review counts\nin each slot. For example, Slots 0 and 1 correspond to items that are\nranked from 1 to 10 and 11 to 20. We observe that BM25 and POP\nhave the least and most tendency to value popular items respec-\ntively, which is consistent with their principles of only emphasizing\nrelevance or popularity. Among other methods, LSE emphasizes\npopularity the most; HEM and RTM put more popular items to top\n10 and less popular items to positions from 11 to 50 than ZAM, AEM,\nand QEM; TEM ranks the fewest popular items to top 10 and has\nsimilar value to RTM at Slot 1. Overall, the fact that RTM achieves\nbetter performance than baselines suggests that RTM could better\nbalance popularity and relevance by the review-level matching.\n5.3 Case Study\nWe sample two cases in the test set of ğ¶ğ‘™ğ‘œğ‘¡â„ğ‘–ğ‘›ğ‘” and ğ¶ğ·ğ‘  from\nour best model to illustrate the three aspects of RTM introduced\nin Section 3.4 and show how RTM identifies useful information\nfrom review-level user-item interactions. We show one example in\nğ¶ğ‘™ğ‘œğ‘¡â„ğ‘–ğ‘›ğ‘” to represent the case of a single-layer RTM, and the other\nexample in ğ¶ğ·ğ‘  to illustrate how a multi-layer RTM performs.\nCase Analysis for Single-layer RTM . In Figure 3, we show\nhow RTM allocates attention to the sequence of the query, userâ€™s\nhistorical reviews, and reviews of an ideal item for the query â€œcloth-\ning shoe jewelry men big tall active athletic sockâ€ with respect to\nğ‘(0). There are 8 attention heads in total and they capture different\naspects of the sequence. â„2 and â„5 focus more on how to allocate\nattention to each user reviews, while the other heads concentrate\non differentiating important item reviews. Overall, the item reviews\nhave the most portion of accumulative attention weights from the\nquery, which is reasonable since item reviews are more important to\ndifferentiate candidate items. The accumulative attention weights\non the user side are positive, which shows that userâ€™s historical\nreviews do help and personalization is needed.\nTo compare the reviews that take the most and least effect on\nmatching the item with the query-user pair, we show the review\ntext of ğ‘¢ğ‘Ÿ3, ğ‘¢ğ‘Ÿ2, ğ‘–ğ‘Ÿ9, and ğ‘–ğ‘Ÿ4 (the 3rd, 2nd user review and the 9th,\nq\nur1\nur2\nur3\nur4\nur5\nir1\nir2\nir3\nir4\nir5\nir6\nir7\nir8\nir9\nir10\nir11\nir12\nir13\nir14\nir15\nir16\nir17\nir18\nir19\nir20\nir21\nir22\nir23\nir24\nir25\nir26\nir27\nir28\nir29\nir30\nh1\nh2\nh3\nh4\nh5\nh6\nh7\nh8\nFigure 3: Attention weights for the case shown in Table 3.\n4th item review) and their attention weights in Table 3. The query\nâ€œclothing shoe jewelry men big tall active athletic sockâ€ attends to\nğ‘¢ğ‘Ÿ3 â€“ a previous review on a T-shirt â€“ the most, and ğ‘¢ğ‘Ÿ2 â€“ a his-\ntorical review on a Casio watch â€“ the least. This makes sense since\nsocks share more common properties with T-shirts than watches,\nsuch as the material and the fitness. For the item reviews, RTM\nallocates the most weight to ğ‘–ğ‘Ÿ9 which includes a lot of useful in-\nformation for â€œmen big tall active athletic sockâ€, such as that the\nsocks are for male, they are cushiony, and they stay up on legs. On\nthe contrary, ğ‘–ğ‘Ÿ4, which receives the least attention, does not reveal\nwhether the socks are for men or women, and the descriptions such\nas â€œcomfortableâ€ and â€œâ€™durableâ€ can also be applied to other prod-\nucts. The attention weights can help explain why RTM ranks this\nitem to the top, and showing reviews with large attention weights\nto the user could help them better understand why the item is a\ngood candidate and facilitate their purchase decisions.\nCase Analysis for Multi-layer RTM . For query â€œCDs vinyl\nEurope jazzâ€, the attention scores of each unit in the sequence of\nthe query, user reviews, and the reviews of a purchased item with\nrespect to different units are shown in Figure 4. In the first trans-\nformer layer, most of the attention is paid to the query itself byğ‘(0),\nshown in Figure 4a. Figure 4b shows that in the second layer, item\nreviews draw most attention weights from ğ‘(1). These two figures\nimply that a single layer is not enough to learn the dynamic match-\ning between the query-user pair and the item, probably because\nthe initial representations are not informative enough.\nFrom the average attention weight of each unit with respect to\nğ‘(1)from the 8 heads, the 3 item reviews have the top 3 attention\nscores, ranked as ğ‘–ğ‘Ÿ3,ğ‘–ğ‘Ÿ1,ğ‘–ğ‘Ÿ2 (in Figure 4b). These attention weights\noffer a possible explanation for how this item is scored. We can\nverify the explanationâ€™s rationality by checking the text of the item\nreviews shown in Table 4.ğ‘–ğ‘Ÿ3 mentions that this CD is Janâ€™s best\nrecording in a long time and Jan Garbarek is a Norwegian jazz\nsaxophonist, which is relevant to the query â€œCDs vinyl Europe\njazzâ€. ğ‘–ğ‘Ÿ1 recommends the CD to people who like modern jazz and\nimplies that it is from a European musician - Jan Garbarek.ğ‘–ğ‘Ÿ2 does\nnot mention jazz at all and considers this album not bad and also\nTable 4: A case of query, user, and purchased item for the multi-layer RTM case study. The attention weights are average from\nall the 8 attention heads.\nQuery: \"CDs vinyl Europe jazz\" (Attention weight w.r.tğ‘(1): 0.097; w.r.tğ‘(0): 0.215)\nAttention w.r.t.\nID User Reviews\nAttention w.r.t.\nID Item Reviewsğ‘(1) ğ‘Ÿ(0)\nğ‘¢ğ‘–ğ‘¢4 ğ‘Ÿ(0)\nğ‘¢ğ‘–3ğ‘– ğ‘(1) ğ‘Ÿ(0)\nğ‘¢ğ‘–ğ‘¢4 ğ‘Ÿ(0)\nğ‘¢ğ‘–3ğ‘–\n0.055 0.065 0.026 ur4 ... Before buying this I already owned 12 Garbarek al-\nbums, must admit though Iâ€™d pretty much heard \"all\"\nhe had to offer ...\n0.196 0.115 0.131 ir3 ... I really liked Jan in the seventies. ... I believe it is Janâ€™s\nbest recording in a long time. I donâ€™t prefer it to his earlier\nAvante guard orjazzier stuff ..., and Janâ€™s always great solos.\n0.053 0.065 0.028 ur8 Aural attack To all current EST fans - if news of Svens-\nsonâ€™s death wasnâ€™t hard enough to take, the music on\nthis album is what you might call \"tough love\"...\n0.167 0.110 0.130 ir1 Excellent introduction to Garbarekâ€™s world for newbies ...If\nyou like modern jazz and/or ECM-style musicians, buy it\nwithout hesitation! ...\n0.052 0.073 0.028 ur3 ... thereâ€™s much more of Steve Reich, Christian Wal-\nlumrod & even Esbjorn Svensson here, meaning that\nwhilst every piece is structured such that you can often\npredict when an established, largely minimalist pattern\nis going to change ...\n0.127 0.109 0.114 ir2 ... Why some people adore this Garbarekâ€™s release while\nothers simply hate it? ... Not \"excellent\" but for sure \"not\nbad\"... This is - in essence - an IMPROVISATION album. If\nyou are not open to this freestyle, probably youâ€™ll not like it,\n... Anyway, decide with your own ears and criteria.\nq\nur1\nur2\nur3\nur4\nur5\nur6\nur7\nur8\nur9\nur10\nir1\nir2\nir3\nh1\nh2\nh3\nh4\nh5\nh6\nh7\nh8\n(a) Attention with respect to ğ‘(0)\nq\nur1\nur2\nur3\nur4\nur5\nur6\nur7\nur8\nur9\nur10\nir1\nir2\nir3\nh1\nh2\nh3\nh4\nh5\nh6\nh7\nh8 (b) Attention with respect to ğ‘(1)\nq\nur1\nur2\nur3\nur4\nur5\nur6\nur7\nur8\nur9\nur10\nir1\nir2\nir3\nh1\nh2\nh3\nh4\nh5\nh6\nh7\nh8 (c) Attention with respect to ğ‘Ÿ(0)\nğ‘¢ğ‘–ğ‘¢4\nq\nur1\nur2\nur3\nur4\nur5\nur6\nur7\nur8\nur9\nur10\nir1\nir2\nir3\nh1\nh2\nh3\nh4\nh5\nh6\nh7\nh8 (d) Attention with respect to ğ‘Ÿ(0)\nğ‘¢ğ‘–3ğ‘–\nFigure 4: Attention weights for the case shown in Table 4. ğ‘Ÿğ‘¢ğ‘–ğ‘¢4 and ğ‘Ÿğ‘¢ğ‘–3ğ‘– denote the same review as ğ‘¢ğ‘Ÿ4 and ğ‘–ğ‘Ÿ3 respectively.\nnot excellent. ğ‘–ğ‘Ÿ3 is more positive than ğ‘–ğ‘Ÿ1 on the album, and both\nof them are more informative and positive thanğ‘–ğ‘Ÿ2, which indicates\nthe explanation of the item score from RTM is reasonable.\nAmong the user reviews, ğ‘¢ğ‘Ÿ4,ğ‘¢ğ‘Ÿ8, and ğ‘¢ğ‘Ÿ3 receive the top atten-\ntion with respect to ğ‘(1)(in Figure 4b). From the review text shown\nin Table 4, we can see thatğ‘¢ğ‘Ÿ4 is written by the user for a previously\npurchased album from the European jazz musician Jan Garbarek. In\nğ‘¢ğ‘Ÿ8, EST is short for â€œEsbjÃ¶rn Svensson Trioâ€, which was a Swedish\njazz piano trio, indicating the album with the review is also related\nto European jazz. ğ‘¢ğ‘Ÿ3 mention Steve Reich (an American composer\nwho also plays jazz), Christian Wallumrod (a Swedish jazz pianist),\nand EsbjÃ¶rn Svensson (a Norwegian jazz musician), which are also\nrelated to the query. These reviews are useful to indicate the userâ€™s\npreference for European jazz and should draw more attention with\nrespect to the query.\nTo find out which unit has more effect onğ‘¢ğ‘Ÿ4 and ğ‘–ğ‘Ÿ3 in the first\ntransformer layer, we also show the attention distribution of each\nunit with respect to ğ‘Ÿ(0)\nğ‘¢ğ‘–ğ‘¢4\n(ur4) and ğ‘Ÿ(0)\nğ‘¢ğ‘–3ğ‘– (ir3) in Figure 4c and 4d.\nFor ğ‘¢ğ‘Ÿ4, the top 5 units that it attends to areğ‘–ğ‘Ÿ3, ğ‘–ğ‘Ÿ1, ğ‘–ğ‘Ÿ2, ğ‘¢ğ‘Ÿ3, ğ‘, and\nğ‘¢ğ‘Ÿ8, from which we can tell that ğ‘–ğ‘Ÿ3 has the largest contribution\nin terms of satisfying preferences indicated by ğ‘¢ğ‘Ÿ4. As shown in\nTable 4, ğ‘¢ğ‘Ÿ4 indicates that the user is a big fan of Jan Garbarek and\nhas listened to almost all his albums, so the comment in ğ‘–ğ‘Ÿ3 that\nthis album is the best of Garbarek is quite persuasive to the user to\npurchase the item. In Figure 4d, the units with top attention weights\nwith respect to ğ‘–ğ‘Ÿ3 are ğ‘,ğ‘–ğ‘Ÿ3,ğ‘–ğ‘Ÿ1,ğ‘–ğ‘Ÿ2,ğ‘¢ğ‘Ÿ3,ğ‘¢ğ‘Ÿ8,and ğ‘¢ğ‘Ÿ4. This implies\nthat the final representation of ğ‘–ğ‘Ÿ3 depends mostly on the query\nand all the item reviews, including itself. The attention weights\nalso indicate that ğ‘–ğ‘Ÿ3 considers itself to satisfy the query and the\npreferences expressed in the user reviewsğ‘¢ğ‘Ÿ3,ğ‘¢ğ‘Ÿ8,and ğ‘¢ğ‘Ÿ4 the most.\nThese observations are consistent with our interpretation on the\ndynamic review representation of a multi-layer RTM in Section 3.4.\n6 CONCLUSION AND FUTURE WORK\nIn this paper, we propose a review-based transformer model (RTM)\nfor personalized product search, which scores an item by encoding\nthe sequence of the query, user reviews, and item reviews with\na transformer architecture. RTM conducts review-level matching\nbetween a query-user pair and an item instead of the popular par-\nadigm that represents and matches users and items with static\nrepresentations in the semantic space. Each user and item reviewâ€™s\nimportance is dynamically changed according to other units in the\nsequence, which enables RTM to perform adaptive personalization\nand the dynamic utilization of the item information in different\nsearch sessions. The empirical results show that RTM not only im-\nproves product search quality but also provides useful information\nto explain why an item is ranked to the top.\nAs a next step, we are interested in investigating incorporat-\ning other information from users and items such as brand, cate-\ngory, the relationship of also-purchased, and also-viewed, etc., with\ntransformers to weigh each information source dynamically across\nsearch sessions.\nACKNOWLEDGMENTS\nThis work was supported in part by the Center for Intelligent In-\nformation Retrieval. Any opinions, findings and conclusions or\nrecommendations expressed in this material are those of the au-\nthors and do not necessarily reflect those of the sponsor.\nREFERENCES\n[1] Aman Ahuja, Nikhil Rao, Sumeet Katariya, Karthik Subbian, and Chandan K\nReddy. 2020. Language-Agnostic Representation Learning for Product Search\non E-Commerce Platforms. In Proceedings of the 13th International Conference on\nWeb Search and Data Mining . 7â€“15.\n[2] Qingyao Ai, Daniel N Hill, SVN Vishwanathan, and W Bruce Croft. 2019. A zero\nattention model for personalized product search. In CIKMâ€™19. 379â€“388.\n[3] Qingyao Ai, Yongfeng Zhang, Keping Bi, Xu Chen, and W Bruce Croft. 2017.\nLearning a hierarchical embedding model for personalized product search. In\nSIGIRâ€™17. ACM, 645â€“654.\n[4] Qingyao Ai, Yongfeng Zhang, Keping Bi, and W Bruce Croft. 2019. Explainable\nProduct Search with a Dynamic Relation Embedding Model. ACM Transactions\non Information Systems (TOIS) 38, 1 (2019), 1â€“29.\n[5] Paul N Bennett, Ryen W White, Wei Chu, Susan T Dumais, Peter Bailey, Fedor\nBorisyuk, and Xiaoyuan Cui. 2012. Modeling the impact of short-and long-term\nbehavior on search personalization. In Proceedings of the 35th international ACM\nSIGIR conference on Research and development in information retrieval . 185â€“194.\n[6] Keping Bi, Qingyao Ai, and W. Bruce Croft. 2020. A Transformer-based Embed-\nding Model for Personalized Product Search. In Proceedings of the 43rd Interna-\ntional ACM SIGIR Conference on Research and Development in Information Retrieval .\nACM, ACM, New York, NY., 1521â€“1524. https://doi.org/10.1145/3397271.3401192.\n[7] Keping Bi, Qingyao Ai, Yongfeng Zhang, and W Bruce Croft. 2019. Conversational\nproduct search based on negative feedback. In CIKMâ€™19. 359â€“368.\n[8] Keping Bi, Choon Hui Teo, Yesh Dattatreya, Vijai Mohan, and W Bruce Croft. 2019.\nA Study of Context Dependencies in Multi-page Product Search. In CIKMâ€™19.\n[9] Qiannan Cheng, Zhaochun Ren, Yujie Lin, Pengjie Ren, Zhumin Chen, Xiangyuan\nLiu, and Maarten de Rijke. 2021. Long Short-Term Session Search: Joint Person-\nalized Reranking and Next Query Prediction. (2021).\n[10] Paul Alexandru Chirita, Wolfgang Nejdl, Raluca Paiu, and Christian KohlschÃ¼tter.\n2005. Using ODP metadata to personalize search. InProceedings of the 28th annual\ninternational ACM SIGIR conference on Research and development in information\nretrieval. 178â€“185.\n[11] Zhuyun Dai and Jamie Callan. 2019. Deeper text understanding for IR with\ncontextual neural language modeling. In SIGIRâ€™19. 985â€“988.\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding.arXiv\npreprint arXiv:1810.04805 (2018).\n[13] Wei Di, Anurag Bhardwaj, Vignesh Jagadeesh, Robinson Piramuthu, and Elizabeth\nChurchill. 2014. When relevance is not enough: Promoting visual attractiveness\nfor fashion e-commerce. arXiv preprint arXiv:1406.3561 (2014).\n[14] Zhicheng Dou, Ruihua Song, and Ji-Rong Wen. 2007. A large-scale evaluation and\nanalysis of personalized search strategies. In Proceedings of the 16th international\nconference on World Wide Web . 581â€“590.\n[15] Huizhong Duan and ChengXiang Zhai. 2015. Mining coordinated intent repre-\nsentation for entity search and recommendation. In Proceedings of the 24th ACM\nInternational on Conference on Information and Knowledge Management . 333â€“342.\n[16] Huizhong Duan, ChengXiang Zhai, Jinxing Cheng, and Abhishek Gattani. 2013.\nA probabilistic mixture model for mining and analyzing product search log. In\nCIKMâ€™13. ACM, 2179â€“2188.\n[17] Susan T Dumais. 2016. Personalized Search: Potential and Pitfalls.. In CIKM. 689.\n[18] Yangyang Guo, Zhiyong Cheng, Liqiang Nie, Yinglong Wang, Jun Ma, and Mohan\nKankanhalli. 2019. Attentive long short-term preference modeling for personal-\nized product search. TOIS 37, 2 (2019), 1â€“27.\n[19] Yangyang Guo, Zhiyong Cheng, Liqiang Nie, Xin-Shun Xu, and Mohan Kankan-\nhalli. 2018. Multi-modal preference modeling for product search. In 2018 ACM\nMultimedia Conference on Multimedia Conference . ACM, 1865â€“1873.\n[20] Shubhra Kanti Karmaker Santu, Parikshit Sondhi, and ChengXiang Zhai. 2017.\nOn application of learning to rank for e-commerce search. In SIGIRâ€™17. ACM,\n475â€“484.\n[21] Quoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and\ndocuments. In International conference on machine learning . 1188â€“1196.\n[22] Soon Chong Johnson Lim, Ying Liu, and Wing Bun Lee. 2010. Multi-facet product\ninformation search and retrieval using semantically annotated product family\nontology. Information Processing & Management 46, 4 (2010), 479â€“493.\n[23] Bo Long, Jiang Bian, Anlei Dong, and Yi Chang. 2012. Enhancing product search\nby best-selling prediction in e-commerce. In CIKMâ€™12. ACM, 2479â€“2482.\n[24] Julian McAuley, Rahul Pandey, and Jure Leskovec. 2015. Inferring networks of\nsubstitutable and complementary products. In SIGKDDâ€™15. ACM, 785â€“794.\n[25] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.\nDistributed representations of words and phrases and their compositionality. In\nAdvances in neural information processing systems . 3111â€“3119.\n[26] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT.\narXiv preprint arXiv:1901.04085 (2019).\n[27] Nish Parikh and Neel Sundaresan. 2011. Beyond relevance in marketplace search.\nIn CIKMâ€™11. ACM, 2109â€“2112.\n[28] Chen Qu, Liu Yang, Minghui Qiu, Yongfeng Zhang, Cen Chen, W Bruce Croft,\nand Mohit Iyyer. 2019. Attentive History Selection for Conversational Question\nAnswering. InProceedings of the 28th ACM International Conference on Information\nand Knowledge Management . 1391â€“1400.\n[29] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu,\nMike Gatford, et al. 1995. Okapi at TREC-3. Nist Special Publication Sp 109 (1995).\n[30] Jennifer Rowley. 2000. Product search in e-shopping: a review and research\npropositions. Journal of consumer marketing 17, 1 (2000), 20â€“35.\n[31] Xuehua Shen, Bin Tan, and ChengXiang Zhai. 2005. Implicit user modeling for\npersonalized search. In Proceedings of the 14th ACM international conference on\nInformation and knowledge management . 824â€“831.\n[32] Mark D Smucker, James Allan, and Ben Carterette. 2007. A comparison of\nstatistical significance tests for information retrieval evaluation. In CIKMâ€™07.\nACM, 623â€“632.\n[33] Jaime Teevan, Susan T Dumais, and Eric Horvitz. 2005. Personalizing search via\nautomated analysis of interests and activities. In Proceedings of the 28th annual\ninternational ACM SIGIR conference on Research and development in information\nretrieval. 449â€“456.\n[34] Jaime Teevan, Susan T Dumais, and Daniel J Liebling. 2008. To personalize or not\nto personalize: modeling queries with variation in user intent. In Proceedings of\nthe 31st annual international ACM SIGIR conference on Research and development\nin information retrieval . 163â€“170.\n[35] Christophe Van Gysel, Maarten de Rijke, and Evangelos Kanoulas. 2016. Learning\nlatent vector spaces for product search. In CIKMâ€™16. ACM, 165â€“174.\n[36] Damir Vandic, Flavius Frasincar, and Uzay Kaymak. 2013. Facet selection algo-\nrithms for web product search. In CIKMâ€™13. ACM, 2327â€“2332.\n[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information processing systems . 5998â€“6008.\n[38] Ryen W White and Steven M Drucker. 2007. Investigating behavioral variability\nin web search. In Proceedings of the 16th international conference on World Wide\nWeb. 21â€“30.\n[39] Liang Wu, Diane Hu, Liangjie Hong, and Huan Liu. 2018. Turning clicks into\npurchases: Revenue optimization for product search in e-commerce. In SIGIRâ€™18.\nACM, 365â€“374.\n[40] Zhijing Wu, Jiaxin Mao, Yiqun Liu, Jingtao Zhan, Yukun Zheng, Min Zhang, and\nShaoping Ma. 2020. Leveraging Passage-level Cumulative Gain for Document\nRanking. In WWWâ€™20.\n[41] Teng Xiao, Jiaxin Ren, Zaiqiao Meng, Huan Sun, and Shangsong Liang. 2019.\nDynamic Bayesian Metric Learning for Personalized Product Search. In Proceed-\nings of the 28th ACM International Conference on Information and Knowledge\nManagement. 1693â€“1702.\n[42] Jun Yu, Sunil Mohan, Duangmanee Pew Putthividhya, and Weng-Keen Wong.\n2014. Latent dirichlet allocation based diversified retrieval for e-commerce search.\nIn WSDMâ€™14. ACM, 463â€“472.\n[43] Yongfeng Zhang, Xu Chen, Qingyao Ai, Liu Yang, and W Bruce Croft. 2018.\nTowards conversational search and recommendation: System ask, user respond.\nIn CIKMâ€™18. ACM, 177â€“186.\n[44] Yuan Zhang, Dong Wang, and Yan Zhang. 2019. Neural IR Meets Graph Embed-\nding: A Ranking Model for Product Search. In The World Wide Web Conference .\n2390â€“2400.\n[45] Jie Zou and Evangelos Kanoulas. 2019. Learning to ask: Question-based sequential\nBayesian product search. In Proceedings of the 28th ACM International Conference\non Information and Knowledge Management . 369â€“378.",
  "topic": null,
  "concepts": [],
  "institutions": [
    {
      "id": "https://openalex.org/I24603500",
      "name": "University of Massachusetts Amherst",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I223532165",
      "name": "University of Utah",
      "country": "US"
    }
  ],
  "cited_by": 17
}