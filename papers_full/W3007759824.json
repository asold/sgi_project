{
  "title": "UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training",
  "url": "https://openalex.org/W3007759824",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222425276",
      "name": "Bao, Hangbo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097573093",
      "name": "Dong Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2389670735",
      "name": "Wei, Furu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1496033914",
      "name": "Wang Wenhui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1979826969",
      "name": "Yang Nan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1971393576",
      "name": "Liu XiaoDong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2047520290",
      "name": "Wang Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A147252820",
      "name": "Piao Songhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2119363152",
      "name": "Gao Jian-feng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102363648",
      "name": "Zhou Ming",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Hon, Hsiao-Wuen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2525127255",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2806120502",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2972437349",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2890166583",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W2962753370",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2396767181",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2805206884"
  ],
  "abstract": "We propose to pre-train a unified language model for both autoencoding and partially autoregressive language modeling tasks using a novel training procedure, referred to as a pseudo-masked language model (PMLM). Given an input text with masked tokens, we rely on conventional masks to learn inter-relations between corrupted tokens and context via autoencoding, and pseudo masks to learn intra-relations between masked spans via partially autoregressive modeling. With well-designed position embeddings and self-attention masks, the context encodings are reused to avoid redundant computation. Moreover, conventional masks used for autoencoding provide global masking information, so that all the position embeddings are accessible in partially autoregressive language modeling. In addition, the two tasks pre-train a unified language model as a bidirectional encoder and a sequence-to-sequence decoder, respectively. Our experiments show that the unified language models pre-trained using PMLM achieve new state-of-the-art results on a wide range of natural language understanding and generation tasks across several widely used benchmarks.",
  "full_text": "UNILMv2: Pseudo-Masked Language Models for\nUniï¬ed Language Model Pre-Training\nHangbo Bao 1 2 Li Dong 1 Furu Wei1 Wenhui Wang1 Nan Yang1 Xiaodong Liu 1 Yu Wang1 Songhao Piao 2\nJianfeng Gao 1 Ming Zhou 1 Hsiao-Wuen Hon1\nhttps://github.com/microsoft/unilm\nAbstract\nWe propose to pre-train a uniï¬ed language model\nfor both autoencoding and partially autoregressive\nlanguage modeling tasks using a novel training\nprocedure, referred to as a pseudo-masked lan-\nguage model (PMLM). Given an input text with\nmasked tokens, we rely on conventional masks\nto learn inter-relations between corrupted tokens\nand context via autoencoding, and pseudo masks\nto learn intra-relations between masked spans\nvia partially autoregressive modeling. With well-\ndesigned position embeddings and self-attention\nmasks, the context encodings are reused to avoid\nredundant computation. Moreover, conventional\nmasks used for autoencoding provide global mask-\ning information, so that all the position embed-\ndings are accessible in partially autoregressive\nlanguage modeling. In addition, the two tasks\npre-train a uniï¬ed language model as a bidirec-\ntional encoder and a sequence-to-sequence de-\ncoder, respectively. Our experiments show that\nthe uniï¬ed language models pre-trained using\nPMLM achieve new state-of-the-art results on\na wide range of natural language understanding\nand generation tasks across several widely used\nbenchmarks.\n1. Introduction\nLanguage model (LM) pre-training on large-scale text cor-\npora has substantially advanced the state of the art across a\nvariety of natural language processing tasks (Peters et al.,\n2018; Radford et al., 2018; Devlin et al., 2018; Dong et al.,\n2019; Liu et al., 2019; Yang et al., 2019; Lewis et al., 2019;\nLan et al., 2019; Raffel et al., 2019). After LM pre-training,\nthe obtained model can be ï¬ne-tuned to various downstream\ntasks.\nTwo types of language model pre-training objectives are\ncommonly employed to learn contextualized text represen-\n1Microsoft Research 2Harbin Institute of Technology.\nğ‘¥4 ğ‘¥5[P]ğ‘¥1 ğ‘¥3 ğ‘¥6\nğ‘¥2\nğ‘¥1 [P]ğ‘¥3 [P] ğ‘¥6[M]\nğ‘¥4 ğ‘¥5\nt=1\nt=2\nâ„’PAR\n[M]ğ‘¥1 [M]ğ‘¥3 [M] ğ‘¥6\nğ‘¥2 ğ‘¥5ğ‘¥4\nâ„’AE\nPseudo-Masked LM\nğ‘¥1ğ‘¥2ğ‘¥3ğ‘¥4ğ‘¥5ğ‘¥6\nğ‘¥1 M 2ğ‘¥3 M 4 M 5ğ‘¥6 P 4 P 5ğ‘¥4ğ‘¥5 P 2ğ‘¥2\nPartially \nAutoregressiveAutoencoding\n[M]  Conventional Masks    [P]  Pseudo Masks\nFigure 1. Given input x1 Â·Â·Â·x6, the tokens x2,x4,x5 are masked\nby the special tokens [M] and [P]. For each example, we jointly\ntrain two types of LMs, namely, autoencoding (AE), and partially\nautoregressive (PAR) masked LMs.\ntations by predicting words conditioned on their context.\nThe ï¬rst strand of work relies on autoencoding LMs (Devlin\net al., 2018; Liu et al., 2019). For example, the masked\nlanguage modeling task used by BERT (Devlin et al., 2018)\nrandomly masks some tokens in a text sequence, and then\nindependently recovers the masked tokens by condition-\ning on the encoding vectors obtained by a bidirectional\nTransformer (Vaswani et al., 2017). The second type of pre-\ntraining uses autoregressive modeling (Radford et al., 2018;\nLewis et al., 2019; Yang et al., 2019; Raffel et al., 2019).\nRather than independently predicting words, the probability\nof a word is dependent on previous predictions.\nInspired by (Dong et al., 2019), we propose a pseudo-\nmasked language model (PMLM ) to jointly pre-train a bidi-\narXiv:2002.12804v1  [cs.CL]  28 Feb 2020\nUNILMv2: Pseudo-Masked Language Models for Uniï¬ed Language Model Pre-Training\nrectional LM for language understanding (e.g., text classiï¬-\ncation, and question answering) and a sequence-to-sequence\nLM for language generation (e.g., document summarization,\nand response generation). Speciï¬cally, the bidirectional\nmodel is pre-trained by autoencoding (AE) LMs, and the\nsequence-to-sequence model is pre-trained by partially au-\ntoregressive (PAR) LMs. As shown in Figure 1, the model\nparameters are shared in two language modeling tasks, and\nthe encoding results of the given context tokens are reused.\nWe use the conventional mask[MASK] (or [M] for short)\nto represent the corrupted tokens for AE pre-training. In\norder to handle factorization steps of PAR language mod-\neling, we append pseudo masks [Pseudo] (or [P] for\nshort) to the input sequence without discarding the origi-\nnal tokens. With well-designed self-attention masks and\nposition embeddings, the PMLM can perform the two lan-\nguage modeling tasks in one forward pass without redundant\ncomputation of context.\nThe proposed method has the following advantages. First,\nthe PMLM pre-trains different LMs in a uniï¬ed manner,\nwhich learns both inter-relations between masked tokens\nand given context (via AE), and intra-relations between\nmasked spans (via PAR). Moreover, conventional masks\nused for AE provide global masking information, so that\nevery factorization step of PAR pre-training can access all\nthe position embeddings as in ï¬ne-tuning. Second, the uni-\nï¬ed pre-training framework learns models for both natural\nlanguage understanding and generation (Dong et al., 2019).\nSpeciï¬cally, the AE-based modeling learns a bidirectional\nTransformer encoder, and the PAR objective pre-trains a\nsequence-to-sequence decoder. Third, the proposed model\nis computationally efï¬cient in that the AE and PAR model-\ning can be computed in one forward pass. Because the en-\ncoding results of given context are reused for two language\nmodeling tasks, redundant computation is avoided. Fourth,\nPAR language modeling learns token-to-token, token-to-\nspan, and span-to-span relations during pre-training. By\ntaking spans (i.e., continuous tokens) into consideration,\nPMLM is encouraged to learn long-distance dependencies\nby preventing local shortcuts.\nWe conduct PMLM pre-training on large-scale text corpora.\nThen we ï¬ne-tune the pre-trained model to a wide range of\nnatural language understanding and generation tasks. Exper-\nimental results show that uniï¬ed pre-training using PMLM\nimproves performance on various benchmarks.\n2. Preliminary\n2.1. Backbone Network: Transformer\nFirst, we pack the embeddings of input tokens {xi}|x|\ni=1\ntogether into H0 = [x1,Â·Â·Â· ,x|x|] âˆˆ R|x|Ã—dh . Then L\nstacked Transformer (Vaswani et al., 2017) blocks compute\nthe encoding vectors via:\nHl = Transformerl(Hlâˆ’1), lâˆˆ[1,L] (1)\nwhere L is the number of layers. The hidden vectors of\nthe ï¬nal layer HL = [hL\n1 ,Â·Â·Â· ,hL\n|x|] are the contextualized\nrepresentations of input. Within each Transformer block,\nmultiple self-attention heads aggregate the output vectors\nof the previous layer, followed by a fully-connected feed-\nforward network.\nSelf-Attention Masks The output Al of a self-attention\nhead in the l-th Transformer layer is:\nQ = Hlâˆ’1WQ\nl , K = Hlâˆ’1WK\nl\nMij =\n{\n0, allow to attend\nâˆ’âˆ, prevent from attending (2)\nAl = softmax(QKâŠº\nâˆšdk\n+ M)(Hlâˆ’1WV\nl )\nwhere parameters WQ\nl ,WK\nl ,WV\nl âˆˆRdhÃ—dk project the\nprevious layerâ€™s outputHlâˆ’1 to queries, keys, and values,\nrespectively. It is worth noting that the mask matrix M âˆˆ\nR|x|Ã—|x|controls whether two tokens can attend each other.\n2.2. Input Representation\nThe inputs of language model pre-training are sequences\nsampled from large-scale text corpora. We follow the format\nused by BERT (Devlin et al., 2018). We add a special\nstart-of-sequence token [SOS] at the beginning to get the\nrepresentation of the whole input. Besides, each text is\nsplit into two segments appended with a special end-of-\nsequence token [EOS]. The ï¬nal input format is â€œ[SOS]\nS1 [EOS] S2 [EOS]â€, where the segmentsS1 and S2 are\ncontiguous texts. The vector of an input token is represented\nby the summation of its token embedding, absolute position\nembedding, and segment embedding. All the embedding\nvectors are obtained by lookup in learnable matrices.\n3. Uniï¬ed Language Model Pre-Training\nWe propose a pseudo-masked language model ( PMLM )\nto jointly pre-train both autoencoding (Section 3.1.1) and\npartially autoregressive (Section 3.1.2) LMs. As shown in\nFigure 2, PMLM reuses the encoding results of the same ex-\nample to jointly pre-train both modeling methods by pseudo\nmasking (Section 3.2).\n3.1. Pre-Training Tasks\nWe use the masked language modeling (MLM; Devlin et al.\n2018) task to pre-train a Transformer network, which is also\nknown as the cloze task (Taylor, 1953). For a given input,\nwe randomly substitute tokens with a special token[MASK]\nUNILMv2: Pseudo-Masked Language Models for Uniï¬ed Language Model Pre-Training\nx1 x2[P][M] x3 x4 x5[P] [P][M] [M] x6\n1 222 3 4 54 54 5 6\nğ’™ğŸ ğ’™ğŸ’ ğ’™ğŸ“\nTransformer Block 1\nTransformer Block L\n...\nh1 h2 h4 hn\nx1 x2[P][M] x3 x4 x5[P] [P][M] [M] x6\n1 222 3 4 54 54 5 6\nâ€¦\n1 2 3 4 5 6\n1 2 3 4 5 6\n2\n2\n2\n2\n4\n4\n5\n5\n4\n4\n5\n5Transformer\nx1 x2[P][M] x3 x4 x5[P] [P][M] [M] x6\n1 222 3 4 54 54 5 6\nğ’™ğŸğ’™ğŸ’ ğ’™ğŸ“\n1 2 3 4 5 6\n1 2 3 4 5 6\n2\n2\n2\n2\n4\n4\n5\n5\n4\n4\n5\n5Transformer\nâ„’PAR (Eq. (6))â„’AE (Eq. (3)) Reused by both â„’AE and â„’PAR\nToken\nPosition\nTransformer Transformer\nFigure 2.Overview of PMLM pre-training. The model parameters are shared across the LM objectives. The bidirectional LM is trained\nby autoencoding MLM, and the sequence-to-sequence (Seq-to-Seq) LM is trained by partially autoregressive MLM. We use different\nself-attention masks to control the access to context for each word token.\nFactorization Order Probability of Masked Tokens\nAutoencoding (e.g., BERT, and our work) âˆ’ p(x2|x\\{2,4,5})p(x3|x\\{2,4,5})p(x5|x\\{2,4,5})\nAutoregressive (e.g., GPT, and XLNet) 2 â†’4 â†’5\n5 â†’4 â†’2\np(x2|x\\{2,4,5})p(x4|x\\{4,5})p(x5|x\\{5})\np(x5|x\\{2,4,5})p(x4|x\\{2,4})p(x2|x\\{2})\nPartially Autoregressive (our work) 2 â†’4,5\n4,5 â†’2\np(x2|x\\{2,4,5})p(x4|x\\{4,5})p(x5|x\\{4,5})\np(x4|x\\{2,4,5})p(x5|x\\{2,4,5})p(x2|x\\{2})\nTable 1.Given input x= x1 Â·Â·Â·x6, the tokens x2,x4,x5 are masked. We compare how to computep(x2,x4,x5|x\\{2,4,5}) with different\nfactorization orders for autoencoding, autoregressive, and partially autoregressive masked language models.\n(or [M] for short). The training objective is to recover them\nby conditioning on the output hidden states of Transformer.\nAs shown in Table 1, we categorize MLMs into autoencod-\ning, autoregressive, and partially autoregressive. Their main\ndifference is how the probability of masked tokens is fac-\ntorized. In our work, we leverage autoencoding (AE) and\npartially autoregressive (PAR) modeling for pre-training,\nwhich is formally described as follows. It is worth noting\nthat the masked positions are the same for both AE and PAR\nmodeling, but the probability factorization is different.\n3.1.1. A UTOENCODING MODELING\nThe autoencoding method independently predicts the to-\nkens by conditioning on context, which is the same as\nBERT. Given original input x = x1 Â·Â·Â·x|x| and the po-\nsitions of masks M = {m1,Â·Â·Â· ,m|M|}, the probability of\nmasked tokens is computed by âˆ\nmâˆˆM p(xm|x\\M ), where\nxM = {xm}mâˆˆM , \\is set minus, x\\M means all input\ntokens except the ones that are in M. The autoencoding\npre-training loss is deï¬ned as:\nLAE = âˆ’\nâˆ‘\nxâˆˆD\nlog\nâˆ\nmâˆˆM\np(xm|x\\M ) (3)\nwhere Dis the training corpus.\n3.1.2. P ARTIALLY AUTOREGRESSIVE MODELING\nWe propose to pre-train partially autoregressive MLMs. In\neach factorization step, the model can predict one or multi-\nple tokens. Let M =\nâŸ¨\nM1,Â·Â·Â· ,M|M|\nâŸ©\ndenote factorization\norder, where Mi = {mi\n1,Â·Â·Â· ,mi\n|Mi|}is the set of mask po-\nsitions in the i-th factorization step. If all factorization steps\nonly contain one masked token (i.e., |Mi|= 1), the mod-\neling becomes autoregressive. In our work, we enable a\nUNILMv2: Pseudo-Masked Language Models for Uniï¬ed Language Model Pre-Training\nAlgorithm 1 Blockwise Masking\nInput x= x1 Â·Â·Â·x|x|: Input sequence\nOutput M =\nâŸ¨\nM1,Â·Â·Â· ,M|M|\nâŸ©\n: Masked positions\nM â†âŸ¨âŸ©\nrepeat\npâ†rand int(1,|x|) âŠ¿Randomly sample an index\nlâ†rand int(2,6) if rand() <0.4 else 1\nif xp,Â·Â·Â· ,xp+lâˆ’1 has not been masked then\nM.append({m}p+lâˆ’1\nm=p )\nuntil âˆ‘|M|\nj=1 |Mj|â‰¥ 0.15|x| âŠ¿Masking ratio is 15%\nreturn M\nfactorization step to be a span, which makes the LM par-\ntially autoregressive. The probability of masked tokens is\ndecomposed as:\np(xM |x\\M ) =\n|M|âˆ\ni=1\np(xMi |x\\Mâ‰¥i ) (4)\n=\n|M|âˆ\ni=1\nâˆ\nmâˆˆMi\np(xm|x\\Mâ‰¥i) (5)\nwhere xMi = {xm}mâˆˆMi , and Mâ‰¥i = â‹ƒ\njâ‰¥i Mj. The\npartially autoregressive pre-training loss is deï¬ned as:\nLPAR = âˆ’\nâˆ‘\nxâˆˆD\nEM log p\n(\nxM |x\\M\n)\n(6)\nwhere EM is the expectation over the factorization distri-\nbution. During pre-training, we randomly sample one fac-\ntorization order M for each input text (Yang et al., 2019),\nrather than computing the exact expectation.\nBlockwise Masking and Factorization Given input se-\nquence x, the masking policy uniformly produces a factor-\nization order M =\nâŸ¨\nM1,Â·Â·Â· ,M|M|\nâŸ©\nfor Equation (6). For\nthe i-th factorization step, the masked position set Mi con-\ntains one token, or a continuous text span (Joshi et al., 2019).\nAs described in Algorithm 1, we randomly sample 15% of\nthe original tokens as masked tokens. Among them, 40% of\nthe time we mask a n-gram block, and 60% of the time we\nmask a token. We then construct a factorization step with the\nset of masked positions. We repeat the above process until\nenough masked tokens are sampled. The randomly sam-\npled factorization orders are similar to permutation-based\nlanguage modeling used by XLNet (Yang et al., 2019). How-\never, XLNet only emits predictions one by one (i.e., autore-\ngressive). In contrast, we can generate one token, or a text\nspan at each factorization step (i.e., partially autoregressive).\n3.2. Pseudo-Masked LM\nEquation (5) indicates that factorization steps of partially au-\ntoregressive language modeling are conditioned on different\ncontext. So if masked language models (Devlin et al., 2018)\n[M]ğ‘¥1 [M]ğ‘¥3 [M] ğ‘¥6\nğ‘¥2 ğ‘¥5ğ‘¥4\n[P]ğ‘¥1 ğ‘¥5ğ‘¥3 ğ‘¥4 ğ‘¥6\nğ‘¥2\nğ‘¥1 [M]ğ‘¥3 [P] ğ‘¥6[M]\nğ‘¥4\nğ‘¥1 [P]ğ‘¥3 ğ‘¥6[M]\nğ‘¥5\nğ‘¥4\nt=3\nt=1\nt=2\n[P]ğ‘¥1 ğ‘¥5ğ‘¥3 ğ‘¥4 ğ‘¥6\nğ‘¥2\nğ‘¥1 [P]ğ‘¥3 [P] ğ‘¥6[M]\nğ‘¥4 ğ‘¥5\nt=2\nt=1\nAE\nAR\nPAR\nFigure 3. Comparisons between autoencoding (AE), autoregres-\nsive (AR), and partially autoregressive (PAR) masked language\nmodels. In the example x= x1 Â·Â·Â·x6, the tokens x2,x4,x5 are\nmasked by the special tokens [M] and [P].\nare directly used, we have to construct a new cloze instance\n(as shown in Figure 3) for each factorization step, which\nrenders partially autoregressive pre-training infeasible. We\npropose a new training procedure, named as pseudo-masked\nlanguage model (PMLM), to overcome the issue.\nFor the last example in Table 1, Figure 4 shows how\nthe PMLM conducts partially autoregressive predictions.\nRather than replacing the tokens with masks as in vanilla\nMLMs, we keep all original input tokens unchanged and ap-\npend pseudo masks to the input sequence. For each masked\ntoken, we insert a [Pseudo] (or [P] for short) token\nwith the same position embedding of the corresponding\ntoken. The top-layer hidden states of [P] tokens are fed\ninto a softmax classiï¬er for MLM predictions. Notice that\npositional information in Transformer is encoded by (abso-\nlute) position embeddings, while the model components are\norder-agnostic. In other words, no matter where a token ap-\npears in the input sequence, the position of the token is only\ndetermined by its position embedding. So we can assign the\nsame position embedding to two tokens, and Transformer\ntreats both of the tokens as if they have the same position.\nVanilla MLMs allow all tokens to attend to each other, while\nPMLM controls accessible context for each token accord-\ning to the factorization order. As shown in Figure 4, the\nexampleâ€™s factorization order is4,5 â†’2. When we com-\npute p(x4,x5|x\\{2,4,5}), only x1,x3,x6 and the pseudo\nmasks of x4,x5 are conditioned on. The original tokens\nof x4,x5 are masked to avoid information leakage, while\ntheir pseudo tokens [P] are used as placeholders for MLM\npredictions. In the second step, the tokens x1,x3,x4,x5,x6\nUNILMv2: Pseudo-Masked Language Models for Uniï¬ed Language Model Pre-Training\nğ’™ğŸ‘ ğ’™ğŸ’ ğ’™ğŸ“ ğ’™ğŸ”ğ’™1 ğ’™2 [ğ] [ğ] [ğ]\n3 4 5 61 2 2 4 5\nToken\nPosition\n+ + + + + + + + +\nğ’™ğŸ‘ ğ’™ğŸ’ ğ’™ğŸ“ ğ’™ğŸ”ğ’™1 ğ’™2 [ğ] [ğ] [ğ]\n3 4 5 61 2 2 4 5\nToken\nPosition\n+ + + + + + + + +\n(a) t = 1: ğ‘(ğ‘¥4,ğ‘¥5|ğ‘¥\\{2,4,5})\n(b) t = 2: ğ‘(ğ‘¥2|ğ‘¥\\{2})\n4 5\n+ +\n4 5\n+ +\n[ğŒ] [ğŒ]\n[ğŒ] [ğŒ]\n2\n+\n2\n+\n[ğŒ]\n[ğŒ]\nğ’™ğŸ ğ’™ğŸ\nğ’™ğŸ’ ğ’™ğŸ“ ğ’™ğŸ’ ğ’™ğŸ“\nFigure 4. Example of the factorization steps 4,5 â†’2. The masks\n[P] and [M] are assigned with the same position embeddings as\nthe corresponding tokens. Different context is used to compute the\nhidden states for the pseudo masks of x4,x5 and x2.\nand the pseudo mask of x2 are conditioned on to compute\np(x2|x\\{2}). Unlike in the ï¬rst step, the original tokens of\nx4,x5 are used for the prediction.\nSelf-attention masks (as described in Section 2.1) are used to\ncontrol what context a token can attend to when computing\nits contextualized representation. Figure 5 shows the self-\nattention mask matrix used for the example of Figure 4.\nThe self-attention mask matrix is designed in order to avoid\ntwo kinds of information leakage. The ï¬rst type is explicit\nleakage, i.e., the masked token can be directly accessed by\nits pseudo token, which renders the LM prediction trivial. So\npseudo tokens [P] are not allowed to attend to the content\nof â€œthemselvesâ€ in a PMLM . The second type is implicit\nleakage, which implicitly leaks prediction information by\nmulti-step attention propagations. For example, as shown\nin Figure 5, if the context token x6 has access to x4, there\nis a connected attention ï¬‚ow â€œx4â€™s pseudo mask tokenâ†’\nx6 â†’x4â€, which eases the prediction of x4. As a result,\nfor each token, we mask the attentions to the tokens that are\npredicted in the future factorization steps.\n3.3. Uniï¬ed Pre-Training\nAs shown in Figure 2, we jointly pre-train bidirectional and\nsequence-to-sequence LMs with the same input text and\nmasked positions. Both the special tokens [M] and [P]\nemit predicted tokens. The training objective is to maximize\nthe likelihood of correct tokens, which considers two types\nof LMs (i.e., autoencoding, and partially autoregressive) in\nğ‘¥2ğ‘¥1 ğ‘¥5ğ‘¥3 ğ‘¥4 ğ‘¥6\nğ‘¥1\nğ‘¥2\nğ‘¥3\nğ‘¥4\nğ‘¥5\nğ‘¥6\n[P] [P] [P]\n[P]\n[P]\n[P]\n21 53 4 62 4 5\n1\n2\n3\n4\n5\n6\n2\n4\n5\nPosition\nAllow to attend Prevent from attending\n[M]\n2\n[M]\n5\n[M]2\n[M]\n4\n[M]\n[M]\n4\n5\nFigure 5. Self-attention mask of the factorization steps 4,5 â†’2.\nBoth conventional masks [M] and given context (x1,x3,x6) can\nbe attended by all the tokens.\none example. The loss is computed via:\nL= LAE + LPAR (7)\nwhere LAE,LPAR are deï¬ned as in Equation (3), and Equa-\ntion (6) respectively. The proposed method sufï¬ciently\nreuses the computed hidden states for both LM objectives.\nIn addition, experiments in Section 4.6 show that the pre-\ntraining tasks are complementary to each other, as they\ncapture both inter- (i.e., between given context and masked\ntokens) and intra- (i.e., among masked tokens) relations of\nthe input tokens.\n3.4. Fine-tuning on NLU and NLG Tasks\nFollowing (Dong et al., 2019), we ï¬ne-tune the pre-trained\nPMLM (with additional task-speciï¬c layers if necessary)\nto both natural language understanding (NLU) and natural\nlanguage generation (NLG) tasks.\nFor NLU tasks, we ï¬ne-tune PMLM as a bidirectional\nTransformer encoder, like BERT. Let us take text classiï¬ca-\ntion as an example. Similar to the text format described in\nSection 2.2, the input is â€œ[SOS] TEXT [EOS]â€. We use\nthe encoding vector of[SOS] as the representation of input,\nand then feed it to a randomly initialized softmax classiï¬er\n(i.e., the task-speciï¬c output layer). We maximize the likeli-\nhood of the labeled training data by updating the parameters\nof the pre-trained PMLM and the added softmax classiï¬er.\nFor sequence-to-sequence generation tasks, the example is\nconcatenated as â€œ[SOS] SRC [EOS] TGT [EOS]â€, where\nSRC and TGT are source and target sequences, respectively.\nThe ï¬ne-tuning procedure is similar to pre-training as in\nSection 3.2. For a source sequence, the dependencies be-\ntween the tokens are bidirectional, i.e., all the source tokens\ncan attend to each other. In contrast, the target sequence\nUNILMv2: Pseudo-Masked Language Models for Uniï¬ed Language Model Pre-Training\nModel SQuAD v1.1 SQuAD v2.0\nF1 EM F1 EM\nBERT 88.5 80.8 76.3 73.7\nXLNet - - - 80.2\nRoBERTa 91.5 84.6 83.7 80.5\nUNILMv2 93.1 87.1 86.1 83.3\nâ€“ rel pos 93.0 86.7 85.2 82.4\nTable 2. Results of BASE -size pre-trained models\non the SQuAD v1.1/v2.0 development sets. We\nreport F1 and exact match (EM) scores. Results of\nUNILMv2 are averaged over ï¬ve runs. â€œâ€“ rel posâ€\nis the model without relative position bias.\nModel MNLI SST-2 MRPC RTE QNLI QQP STS CoLA\nAcc Acc Acc Acc Acc Acc PCC MCC\nBERT 84.5 93.2 87.3 68.6 91.7 91.3 89.5 58.9\nXLNet 86.8 94.7 88.2 74.0 91.7 91.4 89.5 60.2\nRoBERTa 87.6 94.8 90.2 78.7 92.8 91.9 91.2 63.6\nUNILMv2 88.5 95.1 91.8 81.3 93.5 91.7 91.0 65.2\nâ€“ rel pos 88.4 95.0 91.2 78.1 93.4 91.8 91.2 63.8\nTable 3.Results of BASE -size models on the development set of the GLUE benchmark.\nWe report Matthews correlation coefï¬cient (MCC) for CoLA, Pearson correlation\ncoefï¬cient (PCC) for STS, and accuracy (Acc) for the rest. Metrics of UNILMv2 are\naveraged over ï¬ve runs for the tasks. â€œâ€“ rel posâ€ is the ablation model without relative\nposition bias.\nis produced in an autoregressive manner. So we append\na pseudo mask [P] for each target token, and use self-\nattention masks to perform autoregressive generation. The\nï¬ne-tuning objective is to maximize the likelihood of the\ntarget sequence given source input. It is worth noting that\n[EOS] is used to mark the end of the target sequence. Once\n[EOS] is emitted, we terminate the generation process of\nthe target sequence. During decoding, we use beam search\nto generate the target tokens one by one (Dong et al., 2019).\n4. Experimental Results\nWe employ pseudo-masked language model to conduct uni-\nï¬ed language model pre-training ( UNILMv2), and ï¬ne-\ntuned the model on both natural language understanding\n(i.e., question answering, the GLUE benchmark) and gen-\neration (i.e., abstractive summarization, and question gen-\neration) tasks. Details about hyperparameters and datasets\ncan be found in the supplementary material. In addition, we\nconducted ablation studies to compare different choices of\npre-training objectives.\n4.1. Pre-Training Setup\nWe followed the same model size as BERT BASE (Devlin\net al., 2018) for comparison purposes. Speciï¬cally, we used\na 12-layer Transformer with 12 attention heads. The hidden\nsize was 768, and inner hidden size of feed-forward network\nwas 3072. The weight matrix of the softmax classiï¬er was\ntied with the token embedding matrix. We also add relative\nposition bias (Raffel et al., 2019) to attention scores. The\nwhole model contains about 110M parameters.\nFor fair comparisons, we report the major results using sim-\nilar pre-training datasets and optimization hyperparameters\nas in RoBERTaBASE (Liu et al., 2019). We use 160GB text\ncorpora from English Wikipedia, BookCorpus (Zhu et al.,\n2015), OpenWebText1, CC-News (Liu et al., 2019), and\n1skylion007.github.io/OpenWebTextCorpus\nStories (Trinh & Le, 2018). We follow the preprocess and\nthe uncased WordPiece (Wu et al., 2016) tokenization used\nin (Devlin et al., 2018). The vocabulary size was 30,522.\nThe maximum length of input sequence was 512. The token\nmasking probability was 15%. Among masked positions,\n80% of the time we replaced the token with masks, 10% of\nthe time with a random token, and keeping the original token\nfor the rest. The block masking (see Algorithm 1) can mask\nup to 6-gram for one factorization step in partially autore-\ngressive modeling. The batch size was set to 7680. We used\nAdam (Kingma & Ba, 2015) with Î²1 = 0.9, Î²2 = 0.98, and\nÏµ= 1e-6 for optimization. The peak learning rate was set\nto 6e-4, with linear warmup over the ï¬rst 24,000 steps and\nlinear decay. The weight decay was 0.01. The dropout rate\nwas set to 0.1. We ran the pre-training procedure for 0.5\nmillion steps, which took about 20 days using 64 Nvidia\nV100-32GB GPU cards.\n4.2. Question Answering\nQuestion answering aims at returning answers for the given\nquestion and documents. We conduct experiments on the\nbenchmarks SQuAD v1.1 (Rajpurkar et al., 2016) and\nv2.0 (Rajpurkar et al., 2018). The model learns to extract\nanswer spans within a passage. We formulate the task as\na natural language understanding problem. The input is\nconcatenated as â€œ [SOS] Question [EOS] Passage\n[EOS]â€. We add a classiï¬cation layer on the pre-trained\nPMLM , which predicts whether each token is the start or\nend position of an answer span by conditioning on the ï¬-\nnal outputs of Transformer. For SQuAD v2.0, we use the\noutput vector of [SOS] to predict whether the instance is\nunanswerable or not.\nThe ï¬ne-tuning results are presented in Table 2, where we\nreport F1 scores and exact match (EM) scores. We compare\nprevious BASE -size models with PMLM . Notice that the\npublicly available BERTBASE checkpoint (Devlin et al., 2018)\nis pre-trained on 13GB corpora with 256 batch size, while\nUNILMv2: Pseudo-Masked Language Models for Uniï¬ed Language Model Pre-Training\nModel #Param CNN/DailyMail XSum\nRG-1 RG-2 RG-L RG-1 RG-2 RG-L\nWithout pre-training\nLEAD -3 40.42 17.62 36.67 16.30 1.60 11.95\nPTRNET (See et al., 2017) 39.53 17.28 36.38 28.10 8.02 21.72\nFine-tuning LARGE -size pre-trained models\nUNILMLARGE (Dong et al., 2019) 340M 43.08 20.43 40.34 - - -\nBARTLARGE (Lewis et al., 2019) 400M 44.16 21.28 40.90 45.14 22.27 37.25\nT511B (Raffel et al., 2019) 11B 43.52 21.55 40.69 - - -\nFine-tuning BASE -size pre-trained models\nMASSBASE (Song et al., 2019) 123M 42.12 19.50 39.01 39.75 17.24 31.95\nBERTS UMABS (Liu & Lapata, 2019) 156M 41.72 19.39 38.76 38.76 16.33 31.15\nT5BASE (Raffel et al., 2019) 220M 42.05 20.34 39.40 - - -\nUNILMv2BASE 110M 43.16 20.42 40.14 44.00 21.11 36.08\nâ€“ relative position bias 110M 43.45 20.71 40.49 43.69 20.71 35.73\nTable 4. Abstractive summarization results on CNN/DailyMail and XSum. The evaluation metric is the F1 version of ROUGE (RG)\nscores. We also present the number of parameters (#Param) for the methods using pre-trained models.\n#Param BLEU-4 MTR RG-L\n(Du & Cardie, 2018) 15.16 19.12 -\n(Zhang & Bansal, 2019) 18.37 22.65 46.68\nUNILMLARGE 340M 22.78 25.49 51.57\nUNILMv2BASE 110M 24.43 26.34 51.97\nâ€“ rel pos 110M 24.70 26.33 52.13\n(Zhao et al., 2018) 16.38 20.25 44.48\n(Zhang & Bansal, 2019) 20.76 24.20 48.91\nUNILMLARGE 340M 24.32 26.10 52.69\nUNILMv2BASE 110M 26.29 27.16 53.22\nâ€“ rel pos 110M 26.30 27.09 53.19\nTable 5. Results on question generation. The ï¬rst block follows\nthe data split in (Du & Cardie, 2018), while the second block is\nthe same as in (Zhao et al., 2018). MTR is short for METEOR,\nand RG for ROUGE. â€œ#Paramâ€ indicates the size of pre-trained\nmodels. â€œâ€“ rel posâ€ is the model without relative position bias.\nXLNetBASE and RoBERTaBASE are more directly comparable.\nThe results show that UNILMv2BASE achieves better perfor-\nmance than the other models on both SQuAD datasets.\n4.3. GLUE Benchmark\nThe General Language Understanding Evaluation (GLUE)\nbenchmark (Wang et al., 2019) contains various tasks. There\nare two single-sentence classiï¬cation tasks, i.e., linguistic\nacceptability (CoLA; Warstadt et al. 2018), and sentiment\nanalysis (SST-2; Socher et al. 2013). The text similarity\n(STS; Cer et al. 2017) task is formulated as a regression\nproblem. The other tasks are pairwise classiï¬cation tasks,\nincluding natural language inference (RTE, MNLI; Dagan\net al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007;\nBentivogli et al. 2009; Williams et al. 2018), question an-\nswering (QNLI; Rajpurkar et al. 2016), and paraphrase de-\ntection (QQP, MRPC; Dolan & Brockett 2005).\nTable 3 presents the results on GLUE. We compare PMLM\nwith three strong pre-trained models, i.e., BERT (Devlin\net al., 2018), XLNet (Yang et al., 2019), and RoBERTa (Liu\net al., 2019), in the single task ï¬ne-tuning setting. All\nthe models are in BASE -size for fair comparisons. We ob-\nserve that the proposed UNILMv2BASE outperforms both\nBERTBASE and XLNetBASE across 8 tasks. Comparing to\nstate-of-the-art pre-trained RoBERTaBASE , UNILMv2BASE\nobtains the best performance on6 out of 8 tasks, e.g., 88.4 vs\n87.6 (RoBERTaBASE ) in terms of MNLI accuracy, indicating\nthe effectiveness of our UNILMv2BASE .\n4.4. Abstractive Summarization\nWe evaluate the pre-trained PMLM on two abstractive sum-\nmarization datasets, i.e., XSum (Narayan et al., 2018), and\nthe non-anonymized version of CNN/DailyMail (See et al.,\n2017). This is a language generation task, where the texts\n(such as news articles) are shortened to readable summaries\nthat preserve salient information of the original texts. The\npre-trained PMLM is ï¬ne-tuned as a sequence-to-sequence\nmodel as described in Section 3.4.\nWe report ROUGE scores (Lin, 2004) on the datasets. Ta-\nble 4 shows two baseline methods that do not rely on pre-\ntraining. LEAD -3 uses the ï¬rst three input sentences as\nthe summary. PTRNET (See et al., 2017) is a sequence-to-\nsequence model with pointer networks. Results indicate\nthat pre-training achieves signiï¬cant improvements over the\nUNILMv2: Pseudo-Masked Language Models for Uniï¬ed Language Model Pre-Training\nModel Objective SQuAD v1.1 SQuAD v2.0 MNLI SST-2\nF1 EM F1 EM m mm Acc\nBERTBASE AE 88.5 80.8 76.3 73.7 84.3 84.7 92.8\nXLNetBASE AR - - 81.0 78.2 85.6 85.1 93.4\nRoBERTaBASE AE 90.6 - 79.7 - 84.7 - 92.7\nBARTBASE AR 90.8 - - - 83.8 - -\n[1] UNILMv2BASE AE+PAR 92.0 85.6 83.6 80.9 86.1 86.1 93.2\n[2] [1] â€“ relative position bias AE+PAR 91.5 85.0 81.8 78.9 85.6 85.5 93.0\n[3] [2] â€“ blockwise factorization AE+AR 90.8 84.1 80.7 77.8 85.4 85.5 92.6\n[4] [2] â€“ PAR AE 91.0 84.2 81.3 78.4 84.9 85.0 92.4\n[5] [2] â€“ AE PAR 90.7 83.9 79.9 77.0 84.9 85.2 92.5\n[6] [5] â€“ blockwise factorization AR 89.9 82.9 79.3 76.1 84.8 85.0 92.3\nTable 6. Comparisons between the pre-training objectives. All models are pre-trained over WIKIPEDIA and BOOK CORPUS for one\nmillion steps with a batch size of 256. Results in the second block are average over ï¬ve runs for each task. We report F1 and exact match\n(EM) scores for SQuAD, and accuracy (Acc) for MNLI and SST-2.\nbaselines. We also compare UNILMv2BASE with state-of-\nthe-art pre-trained models of both BASE -size and LARGE -\nsize. We focus on the comparisons in the third block because\nthe models contain similar numbers of parameters. BERT-\nSUMABS (Liu & Lapata, 2019) ï¬ne-tunes a BERT encoder\nthat is pre-trained with an autoencoding objective, concate-\nnating with a randomly initialized decoder. MASS (Song\net al., 2019) and T5 (Raffel et al., 2019) pre-train encoder-\ndecoder Transformers with masked LM, which relies on\nthe autoregressive pre-training. Although PMLM has the\nsmallest size, we ï¬nd that UNILMv2BASE outperforms the\nother BASE -size pre-trained models on both datasets.\n4.5. Question Generation\nWe perform evaluations on question generation (Du &\nCardie, 2018), the task of automatically producing relevant\nquestions that ask for the given answer and context. The\ninput of the sequence-to-sequence problem is deï¬ned as the\nconcatenation of a paragraph and an answer. We ï¬ne-tune\nthe pre-trained PMLM to predict output questions.\nAs shown in Table 5, we report BLEU (Papineni et al., 2002),\nMETEOR(Banerjee & Lavie, 2005), and ROUGE (Lin,\n2004) scores on two different data splits. Among the com-\npared results, UNILM (Dong et al., 2019) is based on pre-\ntrained models, while the other three methods are sequence-\nto-sequence models enhanced with manual features (Du &\nCardie, 2018), gated self-attention (Zhao et al., 2018), and\nreinforcement learning (Zhang & Bansal, 2019). Results\nshow that UNILMv2BASE achieves better evaluation met-\nrics compared with UNILMLARGE and several baselines. It\nis worth noting that UNILMv2BASE consists of three times\nfewer parameters than UNILMLARGE .\n4.6. Effect of Pre-Training Objectives\nWe conduct ablation experiments on using PMLM to im-\nplement different pre-training objectives, i.e., autoencoding\n(AE), autoregressive (AR), partially autoregressive (PAR),\nand jointly training (AE+AR, and AE+PAR). The evalua-\ntions follow the same settings2 as in BERT (Devlin et al.,\n2018), so that the results in Table 6 can be directly compared\nwith each other. Notice that XLNet (Yang et al., 2019) is\nan autoregressive MLM augmented with more advanced\nrelative position embeddings, and long-context memory.\nAs shown in Table 6, we compare thePMLM -based variants\nagainst previous models on question answering (SQuAD;\nRajpurkar et al. 2016; 2018), natural language inference\n(MNLI; Williams et al. 2018), and sentiment classiï¬cation\n(SST-2; Socher et al. 2013). First, we ablate relative position\nbias to better compare with BERT, RoBERTa, and BART.\nOn text classiï¬cation (MNLI and SST-2), the PAR-only ob-\njective compares favorably with both AE-only and AR-only\nobjectives, which indicates the effectiveness of the proposed\nPAR modeling. In comparison, the SQuAD tasks require\nmore precise modeling of spans in order to extract correct\nanswer spans from the input passage, where both AE-only\nand PAR-only objectives outperform the AR-only objective.\nThe results indicate that block masking and factorization\nare important for LM pre-training. Besides, the settings of\njointly training (AE+AR, and AE+PAR) tend to improve\nthe results over using single LM task. Among the ï¬ve objec-\ntives, AE+PAR performs the best with the help ofPMLM ,\nwhich shows that autoencoding and partially autoregressive\nmodelings are complementary for pre-training.\n2Models were trained for 1M steps with batch size of 256\nover English Wikipedia and BookCorpus (Zhu et al., 2015). The\nlearning rate of Adam ( Î²1 = 0.9, Î²2 = 0.999) was set to 1e-4,\nwith linear schedule and warmup over the ï¬rst 10K steps.\nUNILMv2: Pseudo-Masked Language Models for Uniï¬ed Language Model Pre-Training\n5. Conclusion\nWe pre-train a uniï¬ed language model for language un-\nderstanding and generation by joint learning bidirectional\nLM (via AE) and sequence-to-sequence LM (via PAR). We\nintroduce a pseudo-masked language model (PMLM ) to ef-\nï¬ciently realize the uniï¬ed pre-training procedure. PMLM\nis computationally efï¬cient in that AE and PAR can be\ncomputed in one forward pass without redundant computa-\ntion. Besides, the two modeling tasks are complementary\nto each other. Because conventional masks of AE provide\nglobal masking information to PAR, and PAR can learn\nintra-relations between masked spans. In addition, the pro-\nposed PAR pre-training encourages to learn long-distance\ndependencies by preventing local shortcuts. Experimental\nresults show that PMLM improves the end-task results on\nseveral language understanding and generation benchmarks.\nReferences\nBanerjee, S. and Lavie, A. METEOR: An automatic metric\nfor MT evaluation with improved correlation with human\njudgments. In Proceedings of the ACL Workshop on\nIntrinsic and Extrinsic Evaluation Measures for Machine\nTranslation and/or Summarization, pp. 65â€“72, 2005.\nBar-Haim, R., Dagan, I., Dolan, B., Ferro, L., and Giampic-\ncolo, D. The second PASCAL recognising textual entail-\nment challenge. In Proceedings of the Second PASCAL\nChallenges Workshop on Recognising Textual Entailment,\n2006.\nBentivogli, L., Dagan, I., Dang, H. T., Giampiccolo, D.,\nand Magnini, B. The ï¬fth pascal recognizing textual en-\ntailment challenge. In In Proc Text Analysis Conference,\n2009.\nCer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., and Specia,\nL. Semeval-2017 task 1: Semantic textual similarity-\nmultilingual and cross-lingual focused evaluation. arXiv\npreprint arXiv:1708.00055, 2017.\nDagan, I., Glickman, O., and Magnini, B. The PASCAL\nrecognising textual entailment challenge. In Proceed-\nings of the First International Conference on Machine\nLearning Challenges, pp. 177â€“190, 2006.\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. BERT:\npre-training of deep bidirectional transformers for lan-\nguage understanding. CoRR, abs/1810.04805, 2018.\nDolan, W. B. and Brockett, C. Automatically constructing a\ncorpus of sentential paraphrases. In Proceedings of the\nThird International Workshop on Paraphrasing, 2005.\nDong, L., Yang, N., Wang, W., Wei, F., Liu, X., Wang, Y .,\nGao, J., Zhou, M., and Hon, H.-W. Uniï¬ed language\nmodel pre-training for natural language understanding\nand generation. In 33rd Conference on Neural Informa-\ntion Processing Systems (NeurIPS 2019), 2019.\nDu, X. and Cardie, C. Harvesting paragraph-level question-\nanswer pairs from wikipedia. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational\nLinguistics, ACL 2018, Melbourne, Australia, July 15-20,\n2018, Volume 1: Long Papers, pp. 1907â€“1917, 2018.\nGiampiccolo, D., Magnini, B., Dagan, I., and Dolan, B. The\nthird PASCAL recognizing textual entailment challenge.\nIn Proceedings of the ACL-PASCAL Workshop on Textual\nEntailment and Paraphrasing, pp. 1â€“9, 2007.\nJoshi, M., Chen, D., Liu, Y ., Weld, D. S., Zettlemoyer,\nL., and Levy, O. SpanBERT: Improving pre-training\nby representing and predicting spans. arXiv preprint\narXiv:1907.10529, 2019.\nKingma, D. P. and Ba, J. Adam: A method for stochastic op-\ntimization. In 3rd International Conference on Learning\nRepresentations, San Diego, CA, 2015.\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma,\nP., and Soricut, R. ALBERT: A lite bert for self-\nsupervised learning of language representations. ArXiv,\nabs/1909.11942, 2019.\nLewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mo-\nhamed, A., Levy, O., Stoyanov, V ., and Zettlemoyer, L.\nBART: Denoising sequence-to-sequence pre-training for\nnatural language generation, translation, and comprehen-\nsion. arXiv preprint arXiv:1910.13461, 2019.\nLin, C.-Y . ROUGE: A package for automatic evaluation of\nsummaries. In Text Summarization Branches Out: Pro-\nceedings of the ACL-04 Workshop, pp. 74â€“81, Barcelona,\nSpain, 2004.\nLiu, Y . and Lapata, M. Text summarization with pretrained\nencoders. In Proceedings of the 2019 Conference on Em-\npirical Methods in Natural Language Processing and the\n9th International Joint Conference on Natural Language\nProcessing, pp. 3730â€“3740, Hong Kong, China, 2019.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V .\nRoBERTa: A robustly optimized BERT pretraining ap-\nproach. arXiv preprint arXiv:1907.11692, 2019.\nNarayan, S., Cohen, S. B., and Lapata, M. Donâ€™t give me\nthe details, just the summary! topic-aware convolutional\nneural networks for extreme summarization. In Proceed-\nings of the 2018 Conference on Empirical Methods in\nNatural Language Processing, pp. 1797â€“1807, Brussels,\nBelgium, 2018.\nUNILMv2: Pseudo-Masked Language Models for Uniï¬ed Language Model Pre-Training\nPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu:\na method for automatic evaluation of machine transla-\ntion. In Proceedings of the 40th Annual Meeting of the\nAssociation for Computational Linguistics, pp. 311â€“318,\nPhiladelphia, Pennsylvania, USA, 2002.\nPeters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C.,\nLee, K., and Zettlemoyer, L. Deep contextualized word\nrepresentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies,\npp. 2227â€“2237, New Orleans, Louisiana, 2018.\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever,\nI. Improving language understanding by generative pre-\ntraining. 2018.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a uniï¬ed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD:\n100,000+ questions for machine comprehension of text.\nIn Proceedings of the 2016 Conference on Empirical\nMethods in Natural Language Processing , pp. 2383â€“\n2392, Austin, Texas, 2016.\nRajpurkar, P., Jia, R., and Liang, P. Know what you donâ€™t\nknow: Unanswerable questions for SQuAD. In Proceed-\nings of the 56th Annual Meeting of the Association for\nComputational Linguistics, ACL 2018, Melbourne, Aus-\ntralia, July 15-20, 2018, Volume 2: Short Papers , pp.\n784â€“789, 2018.\nSee, A., Liu, P. J., and Manning, C. D. Get to the point:\nSummarization with pointer-generator networks. In Pro-\nceedings of the 55th Annual Meeting of the Association\nfor Computational Linguistics, pp. 1073â€“1083, Vancou-\nver, Canada, 2017.\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning,\nC. D., Ng, A., and Potts, C. Recursive deep models for\nsemantic compositionality over a sentiment treebank. In\nProceedings of the 2013 conference on empirical methods\nin natural language processing, pp. 1631â€“1642, 2013.\nSong, K., Tan, X., Qin, T., Lu, J., and Liu, T.-Y . Mass:\nMasked sequence to sequence pre-training for language\ngeneration. arXiv preprint arXiv:1905.02450, 2019.\nSzegedy, C., Vanhoucke, V ., Ioffe, S., Shlens, J., and Wojna,\nZ. Rethinking the inception architecture for computer\nvision. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2016.\nTaylor, W. L. Cloze procedure: A new tool for measuring\nreadability. Journalism Bulletin, 30(4):415â€“433, 1953.\nTrinh, T. H. and Le, Q. V . A simple method for common-\nsense reasoning. ArXiv, abs/1806.02847, 2018.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, Å., and Polosukhin, I. Atten-\ntion is all you need. In Advances in Neural Information\nProcessing Systems 30, pp. 5998â€“6008, 2017.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and\nBowman, S. R. GLUE: A multi-task benchmark and\nanalysis platform for natural language understanding. In\nInternational Conference on Learning Representations,\n2019.\nWarstadt, A., Singh, A., and Bowman, S. R. Neu-\nral network acceptability judgments. arXiv preprint\narXiv:1805.12471, 2018.\nWilliams, A., Nangia, N., and Bowman, S. A broad-\ncoverage challenge corpus for sentence understanding\nthrough inference. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Tech-\nnologies, pp. 1112â€“1122, New Orleans, Louisiana, 2018.\nWu, Y ., Schuster, M., Chen, Z., Le, Q. V ., Norouzi, M.,\nMacherey, W., Krikun, M., Cao, Y ., Gao, Q., Macherey,\nK., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser,\nL., Gouws, S., Kato, Y ., Kudo, T., Kazawa, H., Stevens,\nK., Kurian, G., Patil, N., Wang, W., Young, C., Smith, J.,\nRiesa, J., Rudnick, A., Vinyals, O., Corrado, G., Hughes,\nM., and Dean, J. Googleâ€™s neural machine translation\nsystem: Bridging the gap between human and machine\ntranslation. CoRR, abs/1609.08144, 2016.\nYang, Z., Dai, Z., Yang, Y ., Carbonell, J., Salakhutdinov, R.,\nand Le, Q. V . XLNet: Generalized autoregressive pre-\ntraining for language understanding. In 33rd Conference\non Neural Information Processing Systems, 2019.\nZhang, S. and Bansal, M. Addressing semantic drift in ques-\ntion generation for semi-supervised question answering.\nCoRR, abs/1909.06356, 2019.\nZhao, Y ., Ni, X., Ding, Y ., and Ke, Q. Paragraph-level\nneural question generation with maxout pointer and gated\nself-attention networks. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language Pro-\ncessing, pp. 3901â€“3910, Brussels, Belgium, 2018.\nZhu, Y ., Kiros, R., Zemel, R., Salakhutdinov, R., Urta-\nsun, R., Torralba, A., and Fidler, S. Aligning books and\nmovies: Towards story-like visual explanations by watch-\ning movies and reading books. In Proceedings of the\nIEEE international conference on computer vision , pp.\n19â€“27, 2015.\nUNILMv2: Pseudo-Masked Language Models for Uniï¬ed Language Model Pre-Training\nA. Hyperparameters for Pre-Training\nAs shown in Table 7, we present the hyperparameters used\nfor pre-training UNILMv2BASE . We use the same Word-\nPiece (Wu et al., 2016) vocabulary and model size as\nBERTBASE (Devlin et al., 2018). We follow the optimiza-\ntion hyperparameters of RoBERTaBASE (Liu et al., 2019) for\ncomparisons.\nLayers 12\nHidden size 768\nFFN inner hidden size 3072\nAttention heads 12\nAttention head size 64\nMax relative position 128\nTraining steps 0.5M\nBatch size 7680\nAdam Ïµ 1e-6\nAdam Î² (0.9, 0.98)\nLearning rate 6e-4\nLearning rate schedule Linear\nWarmup ratio 0.048\nGradient clipping 0.0\nDropout 0.1\nWeight decay 0.01\nTable 7. Hyperparameters for pre-training UNILMv2BASE .\nB. GLUE Benchmark\nTable 8 summarizes the datasets used for the Gen-\neral Language Understanding Evaluation (GLUE) bench-\nmark (Wang et al., 2019).\nDataset #Train/#Dev/#Test\nSingle-Sentence Classiï¬cation\nCoLA (Acceptability) 8.5k/1k/1k\nSST-2 (Sentiment) 67k/872/1.8k\nPairwise Text Classiï¬cation\nMNLI (NLI) 393k/20k/20k\nRTE (NLI) 2.5k/276/3k\nQNLI (NLI) 105k/5.5k/5.5k\nWNLI (NLI) 634/71/146\nQQP (Paraphrase) 364k/40k/391k\nMRPC (Paraphrase) 3.7k/408/1.7k\nText Similarity\nSTS-B (Similarity) 7k/1.5k/1.4k\nTable 8.Summary of the GLUE benchmark.\nC. Hyperparameters for NLU Fine-Tuning\nTable 9 reports the hyperparameters used for ï¬ne-tuning\nUNILMv2BASE over SQuAD v1.10 (Rajpurkar et al., 2016)\n/ v2.0 (Rajpurkar et al., 2018), and the GLUE bench-\nmark (Wang et al., 2019). The hyperparameters are searched\non the development sets according to the average perfor-\nmance of ï¬ve runs. We use the same hyperparameters for\nboth SQuAD question answering datasets. Moreover, we list\nthe hyperparameters used for the GLUE datasets in Table 9.\nSQuAD v1.1/v2.0 GLUE\nBatch size 32 {16, 32}\nLearning rate 2e-5 {5e-6, 1e-5, 1.5e-5, 2e-5, 3e-5}\nLR schedule Linear\nWarmup ratio 0.1 {0.1, 0.2}\nWeight decay 0.01 {0.01, 0.1}\nEpochs 4 {10, 15}\nTable 9. Hyperparameters used for ï¬ne-tuning on SQuAD, and\nGLUE.\nD. Hyperparameters for NLG Fine-Tuning\nAs shown in Table 10, we present the hyperparameters used\nfor the natural language generation datasets, i.e., CNN/Dai-\nlyMail (See et al., 2017), XSum (Narayan et al., 2018), and\nSQuAD question generation (QG; Du & Cardie 2018; Zhao\net al. 2018). The total length is set to 512 for QG, and\n768 for CNN/DailyMail and XSum. The maximum output\nlength is set to 160 for CNN/DailyMail, and 48 for XSum\nand QG. The label smoothing (Szegedy et al., 2016) rate is\n0.1. During decoding, we use beam search to generate the\noutputs. Length penalty (Wu et al., 2016) is also used to\nscore candidates.\nCNN/DailyMail XSum QG\nFine-Tuning\nLearning rate 7e-5 7e-5 2e-5\nBatch size 64 64 48\nWeight decay 0.01\nEpochs 14 14 16\nLearning rate schedule Linear\nWarmup ratio 0.02 0.02 0.1\nLabel smoothing 0.1\nMax input length 608 720 464\nMax output length 160 48 48\nDecoding\nLength penalty 0.7 0.6 1.3\nBeam size 5 5 8\nTable 10. Hyperparameters used for ï¬ne-tuning and decoding on\nCNN/DailyMail, XSum, and question generation (QG).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8542652726173401
    },
    {
      "name": "Language model",
      "score": 0.8193656206130981
    },
    {
      "name": "Autoregressive model",
      "score": 0.7112566232681274
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5994752645492554
    },
    {
      "name": "Masking (illustration)",
      "score": 0.521346926689148
    },
    {
      "name": "Position (finance)",
      "score": 0.5111954808235168
    },
    {
      "name": "Natural language processing",
      "score": 0.5100184082984924
    },
    {
      "name": "Encoder",
      "score": 0.49749186635017395
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4932689070701599
    },
    {
      "name": "Natural language",
      "score": 0.4834766983985901
    },
    {
      "name": "Sequence (biology)",
      "score": 0.46745070815086365
    },
    {
      "name": "Speech recognition",
      "score": 0.3955855965614319
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Finance",
      "score": 0.0
    },
    {
      "name": "Econometrics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}