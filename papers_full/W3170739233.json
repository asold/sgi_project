{
  "title": "SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval",
  "url": "https://openalex.org/W3170739233",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2320679490",
      "name": "Tiancheng Zhao",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2113751768",
      "name": "Xiaopeng Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2134995023",
      "name": "Kyusong Lee",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2961915345",
    "https://openalex.org/W2551396370",
    "https://openalex.org/W2969574947",
    "https://openalex.org/W4287865052",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W2995636882",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2090243146",
    "https://openalex.org/W2557764419",
    "https://openalex.org/W2746097825",
    "https://openalex.org/W3038033387",
    "https://openalex.org/W2962881743",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2169054943",
    "https://openalex.org/W2536015822",
    "https://openalex.org/W2996064239",
    "https://openalex.org/W2250225488",
    "https://openalex.org/W1532325895",
    "https://openalex.org/W2990928880",
    "https://openalex.org/W2912817604",
    "https://openalex.org/W2949847757",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W2971105107",
    "https://openalex.org/W2995638926",
    "https://openalex.org/W2970618241",
    "https://openalex.org/W3023238803",
    "https://openalex.org/W2094728533",
    "https://openalex.org/W2963928014",
    "https://openalex.org/W3012639927",
    "https://openalex.org/W2806081754",
    "https://openalex.org/W1981208470",
    "https://openalex.org/W2988421999",
    "https://openalex.org/W3005296017",
    "https://openalex.org/W3121694563",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W3106031450",
    "https://openalex.org/W2648699835",
    "https://openalex.org/W2963056065",
    "https://openalex.org/W2889729765",
    "https://openalex.org/W3104078590",
    "https://openalex.org/W2788448041",
    "https://openalex.org/W2950729111",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3102725307",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963159735",
    "https://openalex.org/W2913222130",
    "https://openalex.org/W2970168256",
    "https://openalex.org/W2919420119",
    "https://openalex.org/W4297567729",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W2962718483",
    "https://openalex.org/W4288548690",
    "https://openalex.org/W3099384026",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2155482025",
    "https://openalex.org/W2964012472",
    "https://openalex.org/W3035099133",
    "https://openalex.org/W2171278097"
  ],
  "abstract": "We introduce SPARTA, a novel neural retrieval method that shows great promise in performance, generalization, and interpretability for open-domain question answering. Unlike many neural ranking methods that use dense vector nearest neighbor search, SPARTA learns a sparse representation that can be efficiently implemented as an Inverted Index. The resulting representation enables scalable neural retrieval that does not require expensive approximate vector search and leads to better performance than its dense counterpart. We validated our approaches on 4 open-domain question answering (OpenQA) tasks and 11 retrieval question answering (ReQA) tasks. SPARTA achieves new state-of-the-art results across a variety of open-domain question answering tasks in both English and Chinese datasets, including open SQuAD, CMRC and etc. Analysis also confirms that the proposed method creates human interpretable representation and allows flexible control over the trade-off between performance and efficiency.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 565–575\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n565\nSPARTA: Efﬁcient Open-Domain Question Answering via Sparse\nTransformer Matching Retrieval\nTiancheng Zhao1, Xiaopeng Lu2∗and Kyusong Lee1\nSOCO Inc.\n1{tianchez,kyusongl}@soco.ai\nLanguage Technologies Institute, Carnegie Mellon University\n2xiaopen2@andrew.cmu.edu\nAbstract\nWe introduce SPARTA, a novel neural re-\ntrieval method that shows great promise in\nperformance, generalization, and interpretabil-\nity for open-domain question answering. Un-\nlike many neural ranking methods that use\ndense vector nearest neighbor search, SPARTA\nlearns a sparse representation that can be ef-\nﬁciently implemented as an Inverted Index.\nThe resulting representation enables scalable\nneural retrieval that does not require expen-\nsive approximate vector search and leads to\nbetter performance than its dense counter-\npart. We validated our approaches on 4 open-\ndomain question answering (OpenQA) tasks\nand 11 retrieval question answering (ReQA)\ntasks. SPARTA achieves new state-of-the-art\nresults across a variety of open-domain ques-\ntion answering tasks in both English and Chi-\nnese datasets, including open SQuAD, CMRC\nand etc. Analysis also conﬁrms that the pro-\nposed method creates human interpretable rep-\nresentation and allows ﬂexible control over the\ntrade-off between performance and efﬁciency.\n1 Introduction\nOpen-domain Question Answering (OpenQA) is\nthe task of answering a question based on a\nknowledge source. One promising approach\nto solve OpenQA is Machine Reading at Scale\n(MRS) (Chen et al., 2017). MRS leverages an in-\nformation retrieval (IR) system to narrow down to\na list of relevant passages and then uses a machine\nreading comprehension reader to extract the ﬁnal\nanswer span. This approach, however, is bounded\nby its pipeline nature since the ﬁrst stage retriever\nis not trainable and may return no passage that\ncontains the correct answer.\nTo address this problem, prior work has focused\non replacing the ﬁrst stage retriever with a train-\nable ranker (Chidambaram et al., 2018; Lee et al.,\n2018; Wang et al., 2018). End-to-end systems\n∗ This work was done during an internship at SOCO\nhave also been proposed to combine passage re-\ntrieval and machine reading by directly retrieving\nanswer span (Seo et al., 2019; Lee et al., 2019).\nDespite of their differences, the above approaches\nare all built on top of the dual-encoder architec-\nture, where query and answer are encoded into\nﬁxed-size dense vectors, and their relevance score\nis computed via dot products. Approximate nearest\nneighbor (ANN) search is then used to enable real-\ntime retrieval for large dataset (Shrivastava and Li,\n2014).\nIn this paper, we argue that the dual-encoder\nstructure is far from ideal for open-domain QA\nretrieval. Recent research shows its limitations\nand suggests the importance of modeling complex\nqueries to answer interactions for strong QA per-\nformance. Seo et al. (2019) shows that their best\nperforming system underperforms the state-of-the-\nart due to query-agnostic answer encoding and its\nover-simpliﬁed matching function. Humeau et al.\n(2019) shows the trade-off between performance\nand speed when moving from expressive cross-\nattention in BERT (Devlin et al., 2018) to simple in-\nner product interaction for dialog response retrieval.\nTherefore, our key research goal is to develop new\na method that can simultaneously achieve expres-\nsive query to answer interaction and fast inference\nfor ranking.\nWe introduce SPARTA (Sparse Transformer\nMatching), a novel neural ranking model. Unlike\nexisting work that relies on a sequence-level in-\nner product, SPARTA uses token-level interaction\nbetween every query and answer token pair, lead-\ning to superior retrieval performance. Concretely,\nSPARTA learns sparse answer representations that\nmodel the potential interaction between every query\nterm with the answer. The learned sparse an-\nswer representation can be efﬁciently saved in an\nInverted Index, e.g., Lucene (McCandless et al.,\n2010), so that one can query a SPARTA index with\nalmost the same speed as a standard search engine\n566\nand enjoy the more reliable ranking performance\nwithout depending on GPU or ANN search.\nExperiments are conducted on two settings:\nOpenQA (Chen et al., 2017) that requires phrase-\nlevel answers and retrieval QA (ReQA) that re-\nquires sentence-level answers (Ahmad et al., 2019).\nOur proposed SpartaQA system achieves new state-\nof-the-art results across 15 different domains and\n2 languages with signiﬁcant performance gain, in-\ncluding OpenSQuAD, OpenCMRC and etc.\nMoreover, model analysis shows that SPARTA\nexhibits several desirable properties. First SPARTA\nshows strong domain generalization ability and\nachieves the best performance compared to both\nclassic IR method and other learning methods in\nlow-resources domains. Second, SPARTA is sim-\nple and efﬁcient and achieves better performance\nthan many more sophisticated methods. Lastly, it\nprovides a human-readable representation that is\neasy to interpret. In short, the contributions of this\nwork include:\n•A novel ranking model SPARTA that offers\ntoken-level query-to-answer interaction and\nenables efﬁcient large-scale ranking.\n•New state-of-the-art experiment results on 11\nReQA tasks and 4 OpenQA tasks in 2 lan-\nguages.\n•Detailed analyses that reveal insights about\nthe proposed methods, including generaliza-\ntion and computation efﬁciency.\n2 Related Work\nThe classical approach for OpenQA depends on\nknowledge bases (KB)s that are manually or au-\ntomatically curated, e.g., Freebase KB (Bollacker\net al., 2008), NELL (Fader et al., 2014) etc. Seman-\ntic parsing is used to understand the query and com-\nputes the ﬁnal answer (Berant et al., 2013; Berant\nand Liang, 2014). However, KB-based systems are\noften limited due to incompleteness in the KB and\ninﬂexibility to changes in schema (Ferrucci et al.,\n2010).\nA more recent approach is to use text data di-\nrectly as a knowledge base. Dr.QA uses a search en-\ngine to ﬁlter to relevant documents and then applies\nmachine readers to extract the ﬁnal answer (Chen\net al., 2017). It needs two stages because all ex-\nisting machine readers, for example, BERT-based\nmodels (Devlin et al., 2018), are prohibitively slow\n(BERT only processes a few thousands of words\nper second with GPU acceleration). Many attempts\nhave been made to improve the ﬁrst-stage retrieval\nperformance (Chidambaram et al., 2018; Seo et al.,\n2019; Henderson et al., 2019; Karpukhin et al.,\n2020; Chang et al., 2020). The information re-\ntrieval community has shown that word embedding\nmatching do not perform well for ad-hoc document\nsearch compared to classic methods (Guo et al.,\n2016; Xiong et al., 2017; Hui et al., 2017).\nTo increase the expressiveness of dual encoders,\nXiong et al. (2017) develops kernel function to\nlearn soft matching score at token-level instead of\nsequence-level. Humeau et al. (2019) proposes\nPoly-Encoders to enable more complex interac-\ntions between the query and the answer by letting\none encoder output multiple vectors instead of one\nvector. Dhingra et al. (2020) incorporates entity\nvectors and multi-hop reasoning to teach systems\nto answer more complex questions. (Lee et al.,\n2020) augments the dense answer representation\nwith learned n-gram sparse feature from contex-\ntualized word embeddings, achieving signiﬁcant\nimprovement compared to the dense-only baseline.\nChang et al. (2020) explores various unsupervised\npretraining objectives to improve dual-encoders’\nQA performance in the low-resources setting.\nUnlike existing work based-on dual-encoders,\nwe focus on learning sparse representation and em-\nphasizing token-level interaction. This is perhaps\nthe most related to the sparse index from Den-\nSPI (Lee et al., 2020) and DeepCT (Dai and Callan,\n2020). Our approach is different because our pro-\nposed model is architecturally simpler and is gen-\nerative so that it will understand words that not\nappear in the answer document, whereas the one\ndeveloped at (Lee et al., 2020) only models n-grams\nappear in the document. MacAvaney et al. (2020)\nalso explores retrieval with sparse representations.\nOur work is different from theirs in that we decide\nnot to model the query order information, which\nenables the model to do full ranking. Section 3.4\nshows that our system can be easily deployed via\ninverted index under modern search engines, such\nas Lucene (McCandless et al., 2010).\n3 Proposed Method\n3.1 Problem Formulation\nFirst, we formally deﬁne the problem of answer\nranking for question answering. Let qbe the input\nquestion, and A = {(a,c)}be a set of candidate\n567\nFigure 1: SPARTA Neural Ranker computes token-level matching score via dot product. Each query terms’ contri-\nbution is ﬁrst obtained via max-pooling and then pass through ReLU and log. The ﬁnal score is the summation of\neach query term contribution.\nanswers. Each candidate answer is a tuple (a,c)\nwhere ais the answer text and cis context infor-\nmation about a. The objective is to ﬁnd model\nparameter θthat rank the correct answer as high as\npossible, .i.e:\nθ= argmax\nθ∈Θ\nE[pθ((a∗,c∗)|q)] (1)\nThis formulation is general and can cover many\ntasks. For example, typical passage-level retrieval\nsystems sets the ato be the passage and leaves c\nempty (Chen et al., 2017; Yang et al., 2019a). The\nsentence-level retrieval task proposed at setsato be\neach sentence in a text knowledge base and cto be\nthe surrounding text (Ahmad et al., 2019). Lastly,\nthe phrase-level QA system sets ato be all valid\nphrases from a corpus and cto be the surrounding\ntext (Seo et al., 2019). This work focuses on the\nsame sentence-level retrieval task (Ahmad et al.,\n2019) since it provides a good balance between\nprecision and memory footprint. Yet note that our\nmethods can be easily applied to the other two\nsettings.\n3.2 SPARTA Neural Ranker\nIn order to achieve both high accuracy and efﬁ-\nciency (scale to millions of candidate answers with\nreal-time response), the proposed SPARTA index\nis built on top of two high-level intuitions.\n•Accuracy: retrieve answer with expressive em-\nbedding interaction between the query and an-\nswer, i.e., token-level contextual interaction.\n•Efﬁciency: create query agnostic answer rep-\nresentation so that they can be pre-computed\nat indexing time. Since it is an ofﬂine opera-\ntion, we can use the most powerful model for\nindexing and simplify the computation needed\nat inference.\nAs shown in Figure 1, a query is repre-\nsented as a sequence of tokens q = [t1,...t|q|]\nand each answer is also a sequence of tokens\n(a,c) = [c1,..a1,..a|a|,ca+1,...c|c|]. We use anon-\ncontextualized embedding to encode the query to-\nkens to ei, and acontextualized transformer model\nto encode the answer and obtain contextualized\ntoken-level embedding sj:\nE(q) = [e1,...e|q|] Query Embedding (2)\nH(a,c) = [s1,...s|c|] Answer Embedding (3)\nThen the matching score f between a query and\nan answer is computed by:\nyi = maxj∈[1,|c|](eT\ni sj) Term Matching\n(4)\nφ(yi) =ReLU(yi + b) Sparse Feature\n(5)\nf(q,(a,c)) =\n|q|∑\ni=0\nlog(φ(yi) + 1) Final Score\n(6)\nwhere bis a trainable bias. The ﬁnal score between\nthe query and answer is the summation of all in-\ndividual scores between each query token and the\nanswer. The logarithm operations normalize each\nindividual score and weaken the overwhelmingly\nlarge term score. Additionally, there are two key\ndesign choices worth of elaboration.\nToken-level InteractionSPARTA scoring uses\ntoken-level interaction between the query and\nthe answer. Motivated by bidirectional-attention\n568\nﬂow (Seo et al., 2016), relevance between every\nquery and answer token pair is computed via dot\nproduct and max pooling in Eq. 4. Whereas in a\ntypical dual-encoder approach, only sequence-level\ninteraction is computed via dot product. Results\nin our experiment section show that ﬁne-grained\ninteraction is crucial to obtain signiﬁcant accuracy\nimprovement. Additionally, sj is obtained from\npowerful bidirectional transformer encoders, e.g.\nBERT and only needs to be computed at the index-\ning time. On the other hand, the query embedding\nis non-contextual, a trade-off needed to enable real-\ntime inference, which is explained in Section 3.4\nSparsity Control Another key feature to enable\nefﬁcient inference and memory foot print is spar-\nsity. This is achieved via the combination of log,\nReLU and bin Eq. 5. The bias term is used as a\nthreshold for yi. The ReLU layer forces that only\nquery terms with yi > 0 have impact to the ﬁnal\nscore, achieving sparse activation. The log opera-\ntion is proven to be useful via experiments for reg-\nularizing individual term scores and leads to better\nperformance and more generalized representation.\nImplementation In terms of implementation,\nwe use a pretrained 12-layer, 768 hidden size bert-\nbase-uncased as the answer encoder to encode the\nanswer and their context (Devlin et al., 2018). To\nencode the difference between the answer sequence\nand its surrounding context, we utilized the seg-\nment embedding from BERT, i.e. the answer to-\nkens have segment_id = 1and the context tokens\nhavesegment_id = 0. Moreover, the query tokens\nare embedded via the word embedding from the\nbert-base-uncased with dimension 768.\n3.3 Learning to Rank\nThe training of SPARTA uses cross entropy\nlearning-to-rank loss and maximizes Eq. 7. The\nobjective tries to distinguish between the true rel-\nevant answer (a+,c+)and irrelevant/random an-\nswers K−for each training query q:\nJ = f(q,(a+,c+)) −log\n∑\nk∈K−\nef(q,(ak,ck)) (7)\nThe choice of negative samples K−are crucial for\neffective learning. Our study uses two types of neg-\native samples: 50% of the negative samples are ran-\ndomly chosen from the entire answer candidate set,\nand the rest 50% are chosen from sentences that are\nnearby to the ground truth answer a. The second\ncase requires the model to learn the ﬁne-grained\ndifference between each sentence candidate instead\nof only rely on the context information. The param-\neters to learn include both the query encoder Eand\nthe answer encoder H. Parameters are optimized\nusing back propagation (BP) through the neural\nnetwork.\n3.4 Indexing and Inference\nOne major novelty of SPARTA is how one can use\nit for real-time inference. That is for a testing query\nq= [t0,...t|q|], the ranking score between qand an\nanswer is:\nLOOKUP(t,(a,c)) = log(Eq. 5) t∈V (8)\nf(q,(a,c)) =\n|q|∑\ni=1\nLOOKUP(ti,(a,c)) (9)\nSince the query term embedding is non-contextual,\nwe can compute the rank feature φ(t,(a,c)) for\nevery possible term t in the vocabulary V with\nevery answer candidate. The result score is cached\nin the indexing time as shown in Eq. 8. At inference\ntime, the ﬁnal ranking score can be computed via\nO(1) look up plus a simple summation as shown in\nEq. 9.\nMore importantly, the above computation can\nbe efﬁciently implemented via a Inverted In-\ndex (Manning et al., 2008), which is the under-\nlying data structure for modern search engines, e.g.\nLucene (McCandless et al., 2010) as shown in Fig-\nure 1(b). This property makes it easy to apply\nSPARTA to real-world applications.\n3.5 Relation to Classic IR and Generative\nModels\nIt is not hard to see the relationship between\nSPARTA and classic BM25 based methods. In\nthe classic IR method, only the tokens that ap-\npeared in the answer are saved to the Inverted In-\ndex. Each term’s score is a combination of Term\nFrequency and Inverted Document Frequencyvia\nheuristics (Manning et al., 2008). On the other\nhand, SPARTA learns which term in the vocabu-\nlary should be inserted into the index, and predicts\nthe ranking score directly rather than heuristic cal-\nculation. This enables the system to ﬁnd relevant\nanswers, even when none of the query words ap-\npeared in the answer text. For example, if the an-\nswer sentence is “Bill Gates founded Microsoft\",\na SPARTA index will not only contain the tokens\nin the answer, but also include relevant terms, e.g.\nwho, founder, entrepreneur and etc.\n569\nSPARTA is also related to generative QA. The\nscoring between (a,c) and every word in the vo-\ncabulary V can be understood as the un-normalized\nprobability of log p(q|a) = ∑|q|\ni log p(ti|a) with\nterm independence assumption. Past work such\nas Lewis and Fan (2018); Nogueira et al. (2019)\ntrains a question generator to score the answer via\nlikelihood. However, both approaches focus on\nauto-regressive models and the quality of question\ngeneration and do not provide an end-to-end solu-\ntion that enables stand-alone answer retrieval.\n4 OpenQA Experiments\nWe consider an Open-domain Question Answer-\ning (OpenQA) task to evaluate the performance\nof SPARTA ranker. Following previous work on\nOpenQA (Chen et al., 2017; Wang et al., 2019;\nXie et al., 2020), we experiment with two English\ndatasets: SQuAD (Rajpurkar et al., 2016), Natural\nQuestions (NQ) (Kwiatkowski et al., 2019); and\ntwo Chinese datasets: CMRC (Cui et al., 2018),\nDRCD (Shao et al., 2018). For each dataset, we\nused the version of Wikipedia where the data was\ncollected from. Preliminary results show that it\nis crucial to use the right version of Wikipedia to\nreproduce the results from baselines. We compare\nthe results with previous best models.\nSystem-wise we follow the 2-stage ranker-reader\nstructure used in (Chen et al., 2017).\nRanker: We split all documents into sentences.\nEach sentence is treated as a candidate answer a.\nWe keep the surrounding context words of each can-\ndidate answer as its context c. We encode at most\n512 word piece tokens and truncate the context sur-\nrounding the answer sentence with equal window\nsize. For model training, bert-base-uncased is used\nas the answer encoder for English, and chinese-\nbert-wwm is used for Chinese. We reuse the word\nembedding from corresponding BERT model as the\nterm embedding. Adam (Kingma and Ba, 2014) is\nused as the optimizer for ﬁne-tuning with a learn-\ning rate 3e-5. The model is ﬁne-tuned for at most\n10K steps and the best model is picked based on\nvalidation performance.\nReader: We deploy a machine reading com-\nprehension (MRC) reader to extract phrase-level\nanswers from the top-K retrieved contexts. For En-\nglish tasks, we ﬁne-tune on span-bert (Joshi et al.,\n2020). For Chinese tasks, we ﬁne-tune on chinese-\nbert-wwm (Cui et al., 2020). Two additional proven\ntechniques are used to improve performance. First,\nwe use global normalization (Clark and Gardner,\n2017) to normalize span scores among multiple pas-\nsages and make them comparable among each other.\nSecond, distant supervision is used. Concretely, we\nﬁrst use the ranker to ﬁnd top-10 passages for all\ntraining data from Wikipedia corpus. Then every\nmention of the oracle answers in these contexts are\ntreated as training examples. This can ensure the\nMRC reader to adapt to the ranker and make the\ntraining distribution closer to the test distribution\n(Xie et al., 2020).\nLastly, evaluation metrics include the standard\nMRC metric: EM and F1-score.\n•Exact Match (EM): if the top-1 answer span\nmatches with the ground truth exactly.\n•F1 Score: we compute word overlapping be-\ntween the returned span and the ground truth\nanswer at token level.\n4.1 OpenQA Results\nOpenSQuAD\nModel F1 EM\nDr.QA(Chen et al., 2017) - 29.8\nR3 (Wang et al., 2018) 37.5 29.1\nPar. ranker (Lee et al., 2018) - 30.2\nMINIMAL (Min et al., 2018) 42.5 32.7\nDenSPI-hybrid (Seo et al., 2019) 44.4 36.2\nBERTserini (Yang et al., 2019a) 46.1 38.6\nRE 3 (Hu et al., 2019) 50.2 41.9\nMulti-passage (Wang et al., 2019) 60.9 53.0\nGraph-retriever (Asai et al., 2019) 63.8 56.5\nSPARTA 66.5 59.3\nOpenNQ EM\nModel Dev Test\nBERT + BM25 (Lee et al., 2018) 24.8 26.5\nHard EM (Min et al., 2019) 28.8 28.1\nORQA(Lee et al., 2019) 31.3 33.3\nGraph-retriever (Asai et al., 2019) 31.7 32.6\nDPR (Karpukhin et al., 2020) 41.5 -\nDAG (Lewis et al., 2020) 44.5 -\nSPARTA 36.8 37.5\nTable 1: Results on English Open SQuAD and NQ\nTable 1 and 2 shows the SPARTA performance\nin OpenQA settings, tested in both English and\nChinese datasets. Experimental results show that\nSPARTA retriever outperforms all existing models\nand obtains new state-of-the-art results on all four\ndatasets. For OpenSQuAD and OpenNQ, SPARTA\n570\nOpenCMRC\nModel F1 EM\nBERTserini(Xie et al., 2020) 60.9 44.5\nBERTserini+DS (Xie et al., 2020) 64.6 48.6\nSPARTA 80.2 63.1\nOpenDRCD\nModel F1 EM\nBERTserini (Xie et al., 2020) 65.0 50.7\nBERTserini+DS (Xie et al., 2020) 67.7 55.4\nSPARTA 74.6 63.1\nTable 2: Results on Chinese Open CMRC and DRCD\noutperforms the previous best system (Asai et al.,\n2019) by 2.7 absolute F1 points and 5.1 absolute\nEM points respectively. For OpenCMRC and Open-\nDRCD, SPARTA achieves a 15.3 and 6.7 absolute\nF1 points improvement over the previous best sys-\ntem (Xie et al., 2020).\nNotably, the previous best system on Open-\nSQuAD and OpenNQ depends on sophisticated\ngraph reasoning (Asai et al., 2019), whereas the\nproposed SPARTA system only uses single-hop\nranker and require much less computation power.\nThis suggests that for tasks that requires only single-\nhop reasoning, there is still big improvement room\nfor better ranker-reader QA systems.\n5 Retrieval QA Experiments\nWe also consider Retrieval QA (ReQA), a sentence-\nlevel question answering task (Ahmad et al., 2019).\nThe candidate answer set contains every possible\nsentence from a text corpus and the system is ex-\npected to return a ranking of sentences given a\nquery. The original ReQA only contains SQuAD\nand NQ. In this study, we extend ReQA to 11 dif-\nferent domains adapted from (Fisch et al., 2019) to\nevaluate both in-domain performance and out-of-\ndomain generalization. The details of the 11 ReQA\ndomains are in Table 3 and Appendix.\nThe in-domain scenarios look at domains that\nhave enough training data (see Table 3). The mod-\nels are trained on the training data and the evalua-\ntion is done on the test data. On the other hand, the\nout-of-domain scenarios evaluate systems’ perfor-\nmance on test data from domains not included in\nthe training, making it a zero-shot learning problem.\nThere are two out-of-domain settings: (1) training\ndata only contain SQuAD (2) training data contain\nonly SQuAD and NQ. Evaluation is carried on all\nthe domains to test systems’ ability to generalize\nto unseen data distribution.\nDomain Data Source\nHas training data\nSQuAD (Rajpurkar et al., 2016) Wikipedia\nNews (Trischler et al., 2016) News\nTrivia (Joshi et al., 2017) Web\nNQ (Kwiatkowski et al., 2019) Google Search\nHotpot (Yang et al., 2018) Wikipedia\nHas no training data\nBioASQ (Tsatsaronis et al., 2015) PubMed Documents\nDROP (Dua et al., 2019) Wikipedia\nDuoRC (Saha et al., 2018) Wikipedia+IMDB\nRACE (Lai et al., 2017) English Exam\nRE (Levy et al., 2017) Wikipedia\nTextbook (Kembhavi et al., 2017) K12 Textbook\nTable 3: 11 corpora included in MultiReQA and their\ndocument sources. The top 5 domains contain training\ndata and the bottom 6 domains only have test sets.\nFor evaluation metrics, we use Mean Recipro-\ncal Rank (MRR) as the criteria. The competing\nbaselines include:\nBM25: a strong classic IR baseline that is difﬁ-\ncult to beat (Robertson et al., 2009).\nUSE-QA1: universal sentence encoder trained\nfor QA task by Google (Yang et al., 2019b). USE-\nQA uses the dual-encoder architecture and it is\ntrained on more than 900 million mined question-\nanswer pairs with 16 different languages.\nPoly-Encoder (Poly-Enc): Poly Encoders im-\nproves the expressiveness of dual-encoders with\ntwo-level interaction (Humeau et al., 2019). We\nadapted the original dialog model for QA retrieval:\ntwo bert-base-uncased models are used as the ques-\ntion and answer encoders. The answer encoder has\n4 vector outputs.\n5.1 In-domain Performance\nTable 4 shows the MRR results on the ﬁve datasets\nwith in-domain training. SPARTA can achieve the\nbest performance across all domains with a large\nmargin. In terms of average MRR across the ﬁve\ndomains, SPARTA is 114.3% better than BM25,\n50.6% better than USE-QA and 26.5% better than\nPoly-Encoders.\nTwo additional insights can be drawn from the\nresults. First, BM-25 is a strong baseline and does\nnot require training. It performs particularly well in\ndomains that have a high-rate of word-overlapping\nbetween the answer and the questions. For example,\nSQuAD’s questions are generated by crowd work-\ners who look at the ground truth answer, while ques-\n1 https://tfhub.dev/google/\nuniversal-sentence-encoder-multilingual-qa\n571\nData BM25 USE-\nQA\nPoly\nEnc\nSPARTA\n(ours)\nSQuAD 58.0 62.5 64.6 78.5\nNews 19.4 26.2 28.3 46.6\nTrivia 29.0 41.2 39.5 55.5\nNQ 19.7 58.2 69.9 77.1\nHotPot 23.9 25.5 51.8 63.8\nAvg 30.0 42.7 50.8 64.3\nTable 4: MRR comparison for the in-domain settings.\nThe proposed SPARTA consistently outperform all the\nbaseline models with large margin. BM25 and USE-\nQA are unsupervised and pre-trained respectively.\ntion data from NQ/News are generated by question\nmakers who do not see the correct answer. BM25\nworks particularly well in SQuAD while perform-\ning the poorest in other datasets. Similar observa-\ntions are also found in prior research (Ahmad et al.,\n2019).\nSecond, the results in Table 4 conﬁrms our hy-\npothesis on the importance of rich interaction be-\ntween the answer and the questions. Both USE-QA\nand Poly Encoder use powerful transformers to\nencode the whole question and model word-order\ninformation in the queries. However, their per-\nformance is bounded by the simple dot-product\ninteraction between the query and the answer. On\nthe other hand, despite the fact that SPARTA does\nnot model word-order information in the query, it\nis able to achieve a big performance gain compared\nto the baselines, conﬁrming the effectiveness of the\nproposed token-level interaction method in Eq. 4.\n5.2 Out-of-domain Generalization\nTable 5 summarized the results for out-of-domain\nperformance comparison. SPARTA trained only\non SQuAD outperforms the baselines, achieving\n54.1% gain compared to BM25, 26.7% gain com-\npared to USE-QA and 25.3% gain compared to\nPoly-Encoders in terms of average MRR across 11\ndifferent datasets. When SPARTA is trained on\nSQuAD+NQ, an additional 1.7 MRR improvement\nis gained compared to SPARTA-SQuAD.\nWe can observe that Poly-Encoder is able to\nachieve similar in-domain performance for the do-\nmains that are included in the training. However,\nits performance decreases signiﬁcantly in new do-\nmains, a 25.0% drop compared to its full perfor-\nmance for Poly-Encoder that is trained on SQuAD\nand 29.2% drop when it’s trained on SQuAD+NQ.\nMeanwhile, SPARTA generalizes its knowledge\nfrom the training data much better to new do-\nmains. When trained on SQuAD, its performance\non News, Trivia, NQ, and HotPot is only 19.2%\nlower than the full performance and 18.3% drop\nwhen it’s trained on SQuAD+NQ. Also, we note\nthat SPARTA’s zero-shot performance on News\n(MRR=41.2) and Trivia (MRR=45.8) is even better\nthan the full performance of Poly-Encoder (News\nMRR=28.3 and Trivia MRR=39.5).\n6 Model Analysis\n6.1 Interpreting Sparse Representations\nOne common limitation of deep neural network\nmodels is poor interpretability. Take dense dis-\ntributed vector representation for example, one can-\nnot directly make sense of each dimension and\nhas to use dimension reduction and visualization\nmethods, e.g. TSNE (Maaten and Hinton, 2008).\nOn the contrary, the resulting SPARTA index is\nstraightforward to interpret due to its sparse na-\nture. Speciﬁcally, we can understand a SPARTA\nvector by reading the top K words with non-zero\nf(t,(a,c)), since these terms have the greatest im-\npact to the ﬁnal ranking score.\nTable 6 shows some example outputs. It is not\nhard to note that the generated terms for each an-\nswer sentence is highly relevant to both aand c,\nand contains not keywords that appeared in the an-\nswer, but also include terms that are potentially in\nthe query but never appear in the answer itself. Two\nexperts manually inspect the outputs for 500 (a,c)\ndata points from Wikipedia, and we summarize the\nfollowing four major categories of terms that are\npredicted by SPARTA.\nConversational search understanding : the\nthird row is an example. “Who” appears to the\ntop term, showing it learns Bill Gates is a person\nso that it’s likely to match with “Who” questions.\nKeyword identiﬁcation: terms such as “gates,\ngoogle, magnate, yellowstone” have high scores in\nthe generated vector, showing that SPARTA learns\nwhich words are important in the answer.\nSynonyms and Common Sense : “benefactor,\ninvestors” are examples of synonyms. Also even\nthough “Utah” does not appear in the answer, it\nis predicted as an important term, showing that\nSPARTA leverages the world-knowledge from a\npretrained language model and knows Yellowstone\nis related to Utah.\n572\nModel SQuAD News Trivia NQ HotPot Bio DROP DuoRC RACE RE Text Avg\nUnsupervised or pretrained\nBM25 58.0 19.4 29.0 19.7 23.9 8.9 32.6 20.1 14.8 87.4 21.6 30.5\nUSE-QA 62.5 26.2 41.2 58.2 25.5 7.7 31.9 20.8 25.6 84.8 26.4 37.4\nTrained on SQuAD\nPolyEnc 64.6* 22.2 35.9 57.6 26.5 9.1 32.6 25.4 24.7 88.3 26.0 37.5\nSPARTA 78.5* 41.2 45.8 62.0 47.7 14.5 37.2 35.9 29.7 96.0 28.7 47.0\nTrained on SQuAD + NQ\nPolyEnc 63.9* 19.8 36.9 69.7* 29.6 8.8 30.7 19.6 25.2 72.8 24.6 36.5\nSPARTA 79.0* 40.3 47.6 75.8 * 47.5 15.0 37.9 36.3 30.0 97.0 29.3 48.7\nTable 5: MRR comparison in the out-of-domain settings. The proposed SPARTA is able to achieve the best\nperformance across all tasks, the only learning-based method that is able to consistently outperform BM25 with\nlarger margin in new domains. Results with * are in-domain performance.\nAnswer (a, c) Top terms\nGoogle was founded in September 1998 by Larry Page\nand Sergey Brin while they were Ph.D. students at Stanford\nUniversity in California.\ngoogle, when, founded, page, stanford, sergey, larry, found-\ning, established, did, 1998, was, year, formed ...\nYellowstone National Park is an American national park lo-\ncated in the western United States, with parts in Wyoming,\nMontana and Idaho.\nmontana, yellowstone, wyoming, idaho, park, where, na-\ntional, western, american, us, utah ...\nWilliam Henry Gates is an American business magnate,\nsoftware developer, investor, and philanthropist. He is\nbest known as the co-founder of Microsoft Corporation.\nwho, gates, investors, magnate, developer, microsoft, philan-\nthropist, benefactor, investors, ...\nQuestion answering (QA) is a computer science discipline\nwithin the ﬁelds of information retrieval and natural language\nprocessing (NLP).\nanswering, question, q, computer, information„ re-\ntrieval,language, natural, human, nl, science, ...\nTable 6: Top-k terms predicted by SPARTA. The text in bold is the answer sentence and the text surrounded it is\nencoded as its context. Each answer sentence has around 1600 terms with non-zero scores.\nTop-K SQuAD NQ\nMRR R@1 MRR R@1\n50 69.5 61.3 63.2 52.5\n100 72.3 64.4 65.6 55.7\n500 76.9 69.4 74.4 64.3\n1000 78.2 70.8 75.5 65.6\n1500 78.6 71.2 75.7 65.7\n2000 78.9 71.4 75.9 66.0\nFull 79.0 71.6 75.8 66.0\nTable 7: Performance on ReQA task with varying spar-\nsity. SPARTA outperforms all baselines with top-50\nterms on SQuAD, and with top-500 terms on NQ.\n6.2 Sparsity vs. Performance\nSparsity not only provides interpretability, but also\noffers ﬂexibility to balance the trade-off of memory\nfootprint vs. performance. When there are mem-\nory constraints on the vector size, the SPARTA\nvector can be easily reduced by only keeping the\ntop-K important terms. Table 7 shows performance\non SQuAD and NQ with varying K. The result-\ning sparse vector representation is very robust to\nsmaller K. When only keeping the top 50 terms in\neach answer vector, SPARTA achieves 69.5 MRR,\na better score than all baselines with only 1.6%\nmemory footprint compared to Poly-Encoders (768\nx 4 dimension). NQ dataset is more challenging\nand requires more terms. SPARTA achieves a close\nto the best performance with top-500 terms.\n7 Conclusion\nIn short, we propose SPARTA, a novel ranking\nmethod, that learns sparse representation for bet-\nter open-domain QA. Experiments show that the\nproposed framework achieves the state-of-the-art\nperformance for 4 different open-domain QA tasks\nin 2 languages and 11 retrieval QA tasks. This con-\nﬁrm our hypothesis that token-level interaction is\nsuperior to sequence-level interaction for better ev-\nidence ranking. Analyses also show the advantages\nof sparse representation, including interpretability,\ngeneralization and efﬁciency.\nOur ﬁndings also suggest promising future re-\nsearch directions. The proposed method does not\nsupport multi-hop reasoning, an important attribute\nthat enables QA systems to answer more complex\nquestions that require collecting multiple evidence\npassages. Also, current method only uses a bag-of-\nword features for the query. We expect further gain\nby incorporating word-order information.\n573\nReferences\nAmin Ahmad, Noah Constant, Yinfei Yang, and\nDaniel Cer. 2019. Reqa: An evaluation for end-\nto-end answer retrieval models. arXiv preprint\narXiv:1907.04780.\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\nRichard Socher, and Caiming Xiong. 2019. Learn-\ning to retrieve reasoning paths over wikipedia\ngraph for question answering. arXiv preprint\narXiv:1911.10470.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013\nconference on empirical methods in natural lan-\nguage processing, pages 1533–1544.\nJonathan Berant and Percy Liang. 2014. Semantic pars-\ning via paraphrasing. In Proceedings of the 52nd An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1415–\n1425.\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim\nSturge, and Jamie Taylor. 2008. Freebase: a collab-\noratively created graph database for structuring hu-\nman knowledge. In Proceedings of the 2008 ACM\nSIGMOD international conference on Management\nof data, pages 1247–1250.\nWei-Cheng Chang, Felix X Yu, Yin-Wen Chang,\nYiming Yang, and Sanjiv Kumar. 2020. Pre-\ntraining tasks for embedding-based large-scale re-\ntrieval. arXiv preprint arXiv:2002.03932.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In Association for Computa-\ntional Linguistics (ACL).\nMuthuraman Chidambaram, Yinfei Yang, Daniel Cer,\nSteve Yuan, Yun-Hsuan Sung, Brian Strope, and Ray\nKurzweil. 2018. Learning cross-lingual sentence\nrepresentations via a multi-task dual-encoder model.\narXiv preprint arXiv:1810.12836.\nChristopher Clark and Matt Gardner. 2017. Simple\nand effective multi-paragraph reading comprehen-\nsion. arXiv preprint arXiv:1710.10723.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shi-\njin Wang, and Guoping Hu. 2020. Revisiting pre-\ntrained models for chinese natural language process-\ning. In Findings of EMNLP. Association for Com-\nputational Linguistics.\nYiming Cui, Ting Liu, Wanxiang Che, Li Xiao,\nZhipeng Chen, Wentao Ma, Shijin Wang, and Guop-\ning Hu. 2018. A span-extraction dataset for chinese\nmachine reading comprehension. arXiv preprint\narXiv:1810.07366.\nZhuyun Dai and Jamie Callan. 2020. Context-aware\ndocument term weighting for ad-hoc search. In Pro-\nceedings of The Web Conference 2020, pages 1897–\n1907.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nBhuwan Dhingra, Manzil Zaheer, Vidhisha Balachan-\ndran, Graham Neubig, Ruslan Salakhutdinov, and\nWilliam W Cohen. 2020. Differentiable reason-\ning over a virtual knowledge base. arXiv preprint\narXiv:2002.10640.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDrop: A reading comprehension benchmark re-\nquiring discrete reasoning over paragraphs. arXiv\npreprint arXiv:1903.00161.\nAnthony Fader, Luke Zettlemoyer, and Oren Etzioni.\n2014. Open question answering over curated and ex-\ntracted knowledge bases. In Proceedings of the 20th\nACM SIGKDD international conference on Knowl-\nedge discovery and data mining, pages 1156–1165.\nDavid Ferrucci, Eric Brown, Jennifer Chu-Carroll,\nJames Fan, David Gondek, Aditya A Kalyanpur,\nAdam Lally, J William Murdock, Eric Nyberg, John\nPrager, et al. 2010. Building watson: An overview\nof the deepqa project. AI magazine, 31(3):59–79.\nAdam Fisch, Alon Talmor, Robin Jia, Minjoon Seo,\nEunsol Choi, and Danqi Chen. 2019. Mrqa 2019\nshared task: Evaluating generalization in reading\ncomprehension. arXiv preprint arXiv:1910.09753.\nJiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce\nCroft. 2016. A deep relevance matching model\nfor ad-hoc retrieval. In Proceedings of the 25th\nACM International on Conference on Information\nand Knowledge Management, pages 55–64.\nMatthew Henderson, Iñigo Casanueva, Nikola Mrkši´c,\nPei-Hao Su, Ivan Vuli ´c, et al. 2019. Con-\nvert: Efﬁcient and accurate conversational rep-\nresentations from transformers. arXiv preprint\narXiv:1911.03688.\nMinghao Hu, Yuxing Peng, Zhen Huang, and Dong-\nsheng Li. 2019. Retrieve, read, rerank: Towards\nend-to-end multi-document reading comprehension.\narXiv preprint arXiv:1906.04618.\nKai Hui, Andrew Yates, Klaus Berberich, and Gerard\nde Melo. 2017. Pacrr: A position-aware neural ir\nmodel for relevance matching. In Proceedings of the\n2017 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1049–1058.\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\nand Jason Weston. 2019. Poly-encoders: Trans-\nformer architectures and pre-training strategies for\nfast and accurate multi-sentence scoring. CoRR\n574\nabs/1905.01969. External Links: Link Cited by, 2:2–\n2.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predict-\ning spans. Transactions of the Association for Com-\nputational Linguistics, 8:64–77.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. arXiv preprint arXiv:1705.03551.\nVladimir Karpukhin, Barlas O ˘guz, Sewon Min, Ledell\nWu, Sergey Edunov, Danqi Chen, and Wen-\ntau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. arXiv preprint\narXiv:2004.04906.\nAniruddha Kembhavi, Minjoon Seo, Dustin Schwenk,\nJonghyun Choi, Ali Farhadi, and Hannaneh Ha-\njishirzi. 2017. Are you smarter than a sixth grader?\ntextbook question answering for multimodal ma-\nchine comprehension. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recog-\nnition, pages 4999–5007.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin,\nKenton Lee, et al. 2019. Natural questions: a bench-\nmark for question answering research. Transactions\nof the Association for Computational Linguistics ,\n7:453–466.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. Race: Large-scale reading\ncomprehension dataset from examinations. arXiv\npreprint arXiv:1704.04683.\nJinhyuk Lee, Minjoon Seo, Hannaneh Hajishirzi, and\nJaewoo Kang. 2020. Contextualized sparse repre-\nsentations for real-time open-domain question an-\nswering. arXiv: Computation and Language.\nJinhyuk Lee, Seongjun Yun, Hyunjae Kim, Miyoung\nKo, and Jaewoo Kang. 2018. Ranking paragraphs\nfor improving answer recall in open-domain ques-\ntion answering. arXiv preprint arXiv:1810.00494.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised\nopen domain question answering. arXiv preprint\narXiv:1906.00300.\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke\nZettlemoyer. 2017. Zero-shot relation extrac-\ntion via reading comprehension. arXiv preprint\narXiv:1706.04115.\nMike Lewis and Angela Fan. 2018. Generative ques-\ntion answering: Learning to answer the whole ques-\ntion.\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. arXiv preprint\narXiv:2005.11401.\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(Nov):2579–2605.\nSean MacAvaney, Franco Maria Nardini, Raffaele\nPerego, Nicola Tonellotto, Nazli Goharian, and\nOphir Frieder. 2020. Expansion via prediction of\nimportance with contextualization. arXiv preprint\narXiv:2004.14245.\nChristopher D Manning, Prabhakar Raghavan, and Hin-\nrich Schütze. 2008. Introduction to information re-\ntrieval. Cambridge university press.\nMichael McCandless, Erik Hatcher, Otis Gospodneti ´c,\nand O Gospodneti ´c. 2010. Lucene in action , vol-\nume 2. Manning Greenwich.\nSewon Min, Danqi Chen, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2019. A discrete hard em ap-\nproach for weakly supervised question answering.\narXiv preprint arXiv:1909.04849.\nSewon Min, Victor Zhong, Richard Socher, and Caim-\ning Xiong. 2018. Efﬁcient and robust question\nanswering from minimal context over documents.\narXiv preprint arXiv:1805.08092.\nRodrigo Nogueira, Jimmy Lin, and AI Epistemic. 2019.\nFrom doc2query to doctttttquery.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nStephen Robertson, Hugo Zaragoza, et al. 2009. The\nprobabilistic relevance framework: Bm25 and be-\nyond. Foundations and TrendsR⃝in Information Re-\ntrieval, 3(4):333–389.\nAmrita Saha, Rahul Aralikatte, Mitesh M Khapra,\nand Karthik Sankaranarayanan. 2018. Duorc: To-\nwards complex language understanding with para-\nphrased reading comprehension. arXiv preprint\narXiv:1804.07927.\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\nHannaneh Hajishirzi. 2016. Bidirectional attention\nﬂow for machine comprehension. arXiv preprint\narXiv:1611.01603.\nMinjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur\nParikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019.\nReal-time open-domain question answering with\n575\ndense-sparse phrase index. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4430–4441.\nChih Chieh Shao, Trois Liu, Yuting Lai, Yiying Tseng,\nand Sam Tsai. 2018. Drcd: a chinese machine\nreading comprehension dataset. arXiv preprint\narXiv:1806.00920.\nAnshumali Shrivastava and Ping Li. 2014. Asymmetric\nlsh (alsh) for sublinear time maximum inner product\nsearch (mips). In Advances in Neural Information\nProcessing Systems, pages 2321–2329.\nAdam Trischler, Tong Wang, Xingdi (Eric) Yuan,\nJustin D. Harris, Alessandro Sordoni, Philip Bach-\nman, and Kaheer Suleman. 2016. Newsqa: A ma-\nchine comprehension dataset.\nGeorge Tsatsaronis, Georgios Balikas, Prodromos\nMalakasiotis, Ioannis Partalas, Matthias Zschunke,\nMichael R Alvers, Dirk Weissenborn, Anastasia\nKrithara, Sergios Petridis, Dimitris Polychronopou-\nlos, et al. 2015. An overview of the bioasq\nlarge-scale biomedical semantic indexing and ques-\ntion answering competition. BMC bioinformatics ,\n16(1):138.\nShuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang,\nTim Klinger, Wei Zhang, Shiyu Chang, Gerry\nTesauro, Bowen Zhou, and Jing Jiang. 2018. R 3:\nReinforced ranker-reader for open-domain question\nanswering. In Thirty-Second AAAI Conference on\nArtiﬁcial Intelligence.\nZhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nal-\nlapati, and Bing Xiang. 2019. Multi-passage\nbert: A globally normalized bert model for\nopen-domain question answering. arXiv preprint\narXiv:1908.08167.\nYuqing Xie, Wei Yang, Luchen Tan, Kun Xiong,\nNicholas Jing Yuan, Baoxing Huai, Ming Li, and\nJimmy Lin. 2020. Distant supervision for multi-\nstage ﬁne-tuning in retrieval-based question answer-\ning. In Proceedings of The Web Conference 2020 ,\npages 2934–2940.\nChenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan\nLiu, and Russell Power. 2017. End-to-end neural\nad-hoc ranking with kernel pooling. In Proceedings\nof the 40th International ACM SIGIR conference on\nresearch and development in information retrieval ,\npages 55–64.\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019a.\nEnd-to-end open-domain question answering with\nbertserini. arXiv preprint arXiv:1902.01718.\nYinfei Yang, Daniel Cer, Amin Ahmad, Mandy\nGuo, Jax Law, Noah Constant, Gustavo Hernan-\ndez Abrego, Steve Yuan, Chris Tar, Yun-Hsuan\nSung, et al. 2019b. Multilingual universal sen-\ntence encoder for semantic retrieval. arXiv preprint\narXiv:1907.04307.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W Cohen, Ruslan Salakhutdinov, and\nChristopher D Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. arXiv preprint arXiv:1809.09600.\nA Appendices\nSize details of multi-domain ReQA task.\nDomain # of Query # of Candidate Answer\nSQuAD 11,426 10,250\nNews 8,633 38,199\nTrivia 8,149 17,845\nNQ 1,772 7,020\nHotpot 5,901 38,906\nBioASQ 1,562 13,802\nDROP 1,513 2,488\nDuoRC 1,568 5,241\nRACE 674 10,630\nRE 2,947 2,201\nTextbook 1,503 14,831\nTable 8: Size of the evaluation test set for the 11 cor-\npora included in MultiReQA.",
  "topic": "Question answering",
  "concepts": [
    {
      "name": "Question answering",
      "score": 0.8417301177978516
    },
    {
      "name": "Computer science",
      "score": 0.8164114952087402
    },
    {
      "name": "Interpretability",
      "score": 0.7948402166366577
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6219562888145447
    },
    {
      "name": "Open domain",
      "score": 0.5633270740509033
    },
    {
      "name": "Transformer",
      "score": 0.49171167612075806
    },
    {
      "name": "Scalability",
      "score": 0.4857257008552551
    },
    {
      "name": "Generalization",
      "score": 0.4591689705848694
    },
    {
      "name": "Representation (politics)",
      "score": 0.4535847008228302
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4477095305919647
    },
    {
      "name": "Machine learning",
      "score": 0.40627753734588623
    },
    {
      "name": "Information retrieval",
      "score": 0.3547305464744568
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3284158408641815
    },
    {
      "name": "Database",
      "score": 0.09387987852096558
    },
    {
      "name": "Mathematics",
      "score": 0.06844887137413025
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ]
}