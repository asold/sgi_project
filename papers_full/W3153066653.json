{
    "title": "Large-Scale Contextualised Language Modelling for Norwegian",
    "url": "https://openalex.org/W3153066653",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4221376487",
            "name": "Kutuzov, Andrey",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226594802",
            "name": "Barnes, Jeremy",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226594806",
            "name": "Velldal, Erik",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4223375893",
            "name": "Øvrelid, Lilja",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226594804",
            "name": "Oepen, Stephan",
            "affiliations": []
        },
        {
            "id": null,
            "name": "{\\O}vrelid, Lilja",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2963341956",
        "https://openalex.org/W168564468",
        "https://openalex.org/W3204526376",
        "https://openalex.org/W2995435108",
        "https://openalex.org/W2736989307",
        "https://openalex.org/W2970352191",
        "https://openalex.org/W1954622760",
        "https://openalex.org/W2739526904",
        "https://openalex.org/W2833383446",
        "https://openalex.org/W2963979492",
        "https://openalex.org/W2799072540",
        "https://openalex.org/W3029187399",
        "https://openalex.org/W2251945677",
        "https://openalex.org/W2988304195",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W3040245432",
        "https://openalex.org/W3007955273",
        "https://openalex.org/W2963671461",
        "https://openalex.org/W3037109418",
        "https://openalex.org/W3031185398",
        "https://openalex.org/W3177087359",
        "https://openalex.org/W2734062961",
        "https://openalex.org/W3080175947",
        "https://openalex.org/W3176866151",
        "https://openalex.org/W3037156693",
        "https://openalex.org/W2576044174",
        "https://openalex.org/W3029460358",
        "https://openalex.org/W2996580882",
        "https://openalex.org/W2995647371"
    ],
    "abstract": "We present the ongoing NorLM initiative to support the creation and use of very large contextualised language models for Norwegian (and in principle other Nordic languages), including a ready-to-use software environment, as well as an experience report for data preparation and training. This paper introduces the first large-scale monolingual language models for Norwegian, based on both the ELMo and BERT frameworks. In addition to detailing the training process, we present contrastive benchmark results on a suite of NLP tasks for Norwegian. For additional background and access to the data, models, and software, please see http://norlm.nlpl.eu",
    "full_text": "Large-Scale Contextualised Language Modelling for Norwegian\nAndrey Kutuzov, Jeremy Barnes, Erik Velldal,\nLilja Øvrelid and Stephan Oepen\nUniversity of Oslo\nDepartment of Informatics\nLanguage Technology Group\n{ andreku | jeremycb | erikve | liljao | oe } @ifi.uio.no\nAbstract\nWe present the ongoing NorLM initiative\nto support the creation and use of very\nlarge contextualised language models for\nNorwegian (and in principle other Nordic\nlanguages), including a ready-to-use soft-\nware environment, as well as an experi-\nence report for data preparation and train-\ning. This paper introduces the ﬁrst large-\nscale monolingual language models for\nNorwegian, based on both the ELMo and\nBERT frameworks. In addition to detail-\ning the training process, we present con-\ntrastive benchmark results on a suite of\nNLP tasks for Norwegian.\nFor additional background and access to\nthe data, models, and software, please see:\nhttp://norlm.nlpl.eu\n1 Introduction\nIn this work, we present NorLM, an ongoing com-\nmunity initiative and emerging collection of large-\nscale contextualised language models for Norwe-\ngian. We here introduce the NorELMo and Nor-\nBERT models, that have been trained on around\ntwo billion tokens of running Norwegian text. We\ndescribe the training procedure and compare these\nmodels with the multilingual mBERT model (De-\nvlin et al., 2019), as well as an additional Nor-\nwegian BERT model developed contemporane-\nously, with some interesting differences in train-\ning data and setup. We report results over a num-\nber of Norwegian benchmark datasets, addressing\na broad range of diverse NLP tasks: part-of-speech\ntagging, negation resolution, sentence-level and\nﬁne-grained sentiment analysis and named entity\nrecognition (NER).\nAll the models are publicly available for down-\nload from the Nordic Language Processing Lab-\noratory (NLPL) Vectors Repository 1 with a CC\nBY 4.0 license. They are also accessible locally,\ntogether with the training and supporting soft-\nware, on the two national superclusters Puhti and\nSaga, in Finland and Norway, respectively, which\nare available to university NLP research groups\nin Northern Europe through the Nordic Language\nProcessing Laboratory (NLPL). 2 The NorBERT\nmodel is in addition served via the Huggingface\nTransformers model hub.3\nNorLM is a joint effort of the projects EOSC-\nNordic (European Open Science Cloud) and\nSANT (Sentiment Analysis for Norwegian), co-\nordinated by the Language Technology Group\n(LTG) at the University of Oslo. The goal of\nthis work is to provide these models and support-\ning tools for researchers and developers in Natu-\nral Language Processing (NLP) for the Norwegian\nlanguage. We do so in the hope of facilitating sci-\nentiﬁc experimentation with and practical applica-\ntions of state-of-the-art NLP architectures, as well\nas to enable others to develop their own large-scale\nmodels, for example for domain- or application-\nspeciﬁc tasks, language variants, or even other lan-\nguages than Norwegian. Under the auspices of the\nNLPL use case in EOSC-Nordic, we are also co-\nordinating with colleagues in Denmark, Finland,\nand Sweden on a collection of large contextualised\nlanguage models for the Nordic languages, includ-\ning language variants or related groups of lan-\nguages, as linguistically or technologically appro-\npriate.\n2 Background\nBokm˚al and Nynorsk There are two ofﬁcial\nstandards for written Norwegian; Bokm˚al, the\nmain variety, and Nynorsk, used by 10–15% of\n1http://vectors.nlpl.eu/repository\n2http://www.nlpl.eu\n3https://huggingface.co/ltgoslo/\nnorbert\narXiv:2104.06546v1  [cs.CL]  13 Apr 2021\nthe Norwegian population. Norwegian language\nlegislation speciﬁes that minimally 25% of the\nwritten public service information should be in\nNynorsk. While the two varieties are closely re-\nlated, there can also be relatively large differ-\nences lexically (though often with a large degree\nof overlap on the character-level still). Several\nprevious studies have indicated that joint model-\ning of Bokm ˚al and Nynorsk works well for many\nNLP tasks, like tagging and parsing (Velldal et al.,\n2017) and NER (Jørgensen et al., 2020). The con-\ntextualised language models presented in this pa-\nper are therefore trained jointly on both varieties,\nbut with the minority variant Nynorsk represented\nby comparatively less data than Bokm˚al (reﬂecting\nthe natural usage).\nDatasets For all our models presented below, we\nused the following training corpora:\n1. Norsk Aviskorpus (NAK), a collection of\nNorwegian news texts 4 (both Bokm ˚al and\nNynorsk) from 1998 to 2019; 1.7 billion\nwords;\n2. Bokm ˚al Wikipedia dump from September\n2020; 160 million words;\n3. Nynorsk Wikipedia dump from September\n2020; 40 million words.\nThe corpora contain ordered sentences (which\nis important for BERT-like models, because one\nof their training tasks is next sentence prediction).\nIn total, our training corpus comprises about two\nbillion (1,907,072,909) word tokens in 203 million\n(202,802,665) sentences.\nWe conducted the following pre-processing\nsteps:\n1. Wikipedia texts were extracted from the\ndumps using the segment wiki script\nfrom the Gensim project (ˇReh˚uˇrek and Sojka,\n2010).\n2. For the news texts from Norwegian Aviskor-\npus, we performed de-tokenization and con-\nversion to UTF-8 encoding, where required.\n3. The resulting corpus was sentence-\nsegmented using Stanza (Qi et al., 2020).\nWe left blank lines between documents (and\n4https://www.nb.no/sprakbanken/\nressurskatalog/oai-nb-no-sbr-4/\nsections in the case of Wikipedia) so that the\n‘next sentence prediction’ task of BERT does\nnot span between documents.\n3 Prerequisites: software and computing\nDeveloping very large contextualised language\nmodels is no small challenge, both in terms of en-\ngineering sophistication and computing demands.\nTraining ELMo- and in particular BERT-like mod-\nels presupposes access to specialised hardware –\ngraphical processing units (GPUs) – over extended\nperiods of time. Compared to the original work at\nGoogle or to our sister initiative at the National\nLibrary of Norway (see below), our two billion\ntokens in Norwegian training data can be charac-\nterised as moderate in size.\nNevertheless, training a single NorBERT model\nrequires close to one full year of GPU utilisa-\ntion, which through parallelization over multiple\ncompute nodes, each featuring four GPUs, could\nbe completed in about three weeks of wall clock\ntime. At this scale, premium software efﬁciency\nand effective parallelization are prerequisites, not\nonly to allow repeated incremental training and\nevaluation cycles to complete in practical inter-\nvals, but equally so for cost-efﬁcient utilisation\nof scarce, shared computing resources and, ulti-\nmately, a shred of environmental sustainability.\nTo prepare the NorLM software environment,\nwe have teamed up with support staff at the Nor-\nwegian national e-infrastructure provider, Uninett\nSigma2, and developed a fully automated and\nmodularised installation procedure using the Easy-\nBuild framework (https://easybuild.io).\nAll necessary tools are compiled from source with\nthe right set of hardware-speciﬁc optimizations\nand platform-speciﬁc optimised libraries for ba-\nsic linear algebra (‘math kernels’) and communi-\ncation across multiple compute nodes.\nThis approach to software provisioning makes it\npossible to (largely) automatically create fully par-\nallel training and experimentation environments\non multiple computing infrastructures – in our\nwork to date two national HPC superclusters, in\nNorway and Finland, but in principle just as much\nany suitable local GPU cluster. In our view, mak-\ning available both a ready-to-run software environ-\nment on Nordic national e-infrastructures, where\nuniversity research groups typically can gain no-\ncost access, coupled with the recipe for recreat-\ning the environment on other HPC systems, may\ncontribute to ‘democratising’ large-scale NLP re-\nsearch; if nothing else, it eliminates dependency\non commercial cloud computing services.\n4 Related work\nLarge-scale deep learning language models (LM)\nare important components of current NLP sys-\ntems. They are often based on BERT (Bidirec-\ntional Encoder Representations from Transform-\ners) (Devlin et al., 2019) and other contextualised\narchitectures. A number of language-speciﬁc ini-\ntiatives have in recent years released monolin-\ngual versions of these models for a number of\nlanguages (Fares et al., 2017; Kutuzov and Kuz-\nmenko, 2017; Virtanen et al., 2019; de Vries et al.,\n2019; Ul ˇcar and Robnik- ˇSikonja, 2020; Kout-\nsikakis et al., 2020; Nguyen and Nguyen, 2020;\nFarahani et al., 2020; Malmsten et al., 2020). For\nour purposes, the most important such previous\ntraining effort is that of Virtanen et al. (2019) on\ncreating a BERT model for Finnish – FinBERT5 –\nas our training setup for creating NorBERT builds\nheavily on this; see Section 6 for more details.\nMany low-resource languages do not have ded-\nicated monolingual large-scale language models,\nand instead resort to using a multilingual model,\nsuch as Google’s multilingual BERT model –\nmBERT – which was trained on data that also in-\ncluded Norwegian. Up until the release of the\nmodels described in the current paper, mBERT\nwas the only BERT-instance that could be used for\nNorwegian.6\nAnother widely used architecture for contextu-\nalised LMs is Embeddings From Language Mod-\nels or ELMo (Peters et al., 2018). The ElmoFor-\nManyLangs initiative (Che et al., 2018) trained\nand released monolingual ELMo models for a\nwide range of different languages, including Nor-\nwegian (with separate models for Bokm ˚al and\nNynorsk). However, these models were trained\non very modestly sized corpora of 20 million\nwords for each language (randomly sampled from\nWikipedia dumps and Common Crawl data).\nIn a parallel effort to that of the current paper,\nthe AI Lab of the National Library of Norway,\nthrough their Norwegian Transformer Model (No-\n5https://github.com/TurkuNLP/FinBERT\n6A BERT model trained on Norwegian data was published\nat https://github.com/botxo/nordic_bert in\nthe beginning of 2020. However, the vocabulary of this model\nseems to be broken, and to the best of our knowledge nobody\nhas achieved any meaningful results with it.\nTraM) project, has released a Norwegian BERT\n(Base, cased) model dubbed NB-BERT (Kummer-\nvold et al., 2021). 7 The model is trained on the\nColossal Norwegian Corpus, reported to comprise\nclose to 18,5 billion words (109.1 GB of text).\nIn raw numbers, this is about ten times more\nthan the corpus we use for training the NorLM\nmodels. However, the vast majority of this is from\nOCR’ed historical sources, which is bound to in-\ntroduce at least some noise. In Section 7 below, we\ndemonstrate that in some NLP tasks, a language\nmodel trained on less (but arguably cleaner) data\ncan outperform a model trained on larger but noisy\ncorpora.\n5 NorELMo\nNorELMo is a set of bidirectional recurrent ELMo\nlanguage models trained from scratch on the Nor-\nwegian corpus described in Section 1. They can\nbe used as a source of contextualised token rep-\nresentations for various Norwegian natural lan-\nguage processing tasks. As we show below, in\nmany cases, they present a viable alternative to\nTransformer-based models like BERT. Their per-\nformance is often only marginally lower, while the\ncompute time required to adapt the model to the\ntask at hand can be an order of magnitude less on\nidentical hardware.\nCurrently we present two models, with more\nfollowing in the future:\n1. NorELMo30: 30,000 most frequent words in\nthe vocabulary\n2. NorELMo100: 100,000 most frequent words\nin the vocabulary\nNote that independent of the vocabulary size,\nboth NorELMo 30 and NorELMo 100 can process\narbitrary word tokens, due to the ELMo archi-\ntecture (where the ﬁrst CNN layer converts in-\nput strings to non-contextual word embeddings).\nThus, the size of the vocabulary controls only the\nnumber of words used as targets for the language\nmodelling task in the course of training. Suppos-\nedly, the model with a larger vocabulary is more\neffective in treating less frequent words at the cost\nof being less effective with more frequent words.\nEach model was trained for 3 epochs with batch\nsize 192. We employed a version of the original\n7https://github.com/NBAiLab/notram\nELMo training code from Peters et al. (2018) up-\ndated to work better with the recent TensorFlow\nversions. All the hyperparameters were left at their\ndefault values, except the LSTM dimensionality\nreduced to 2,048 from the default 4,096 (in our\nexperience, this rarely inﬂuences performance).\nTraining of each model took about 100 hours on\nfour NVIDIA P100 GPUs.\nThese are the ﬁrst ELMo models for Norwe-\ngian trained on a large corpus. As has already\nbeen mentioned, the Norwegian ELMo models\nfrom the ElmoForManyLangs project (Che et al.,\n2018) were trained on very small corpora sam-\nples and seriously under-perform on semantic-\nrelated NLP tasks, although they can yield impres-\nsive results on POS tagging and syntactic pars-\ning (Zeman et al., 2018). In addition, they were\ntrained with custom code modiﬁcations and can\nbe used only with the customElmoForManyLangs\nlibrary. On the other hand, our NorELMo models\nare fully compatible both with the original ELMo\nimplementation by Peters et al. (2018) and with\nthe more modern simple elmo Python library pro-\nvided by us.8\nThe vocabularies are published together with\nthe models. For different tasks, different mod-\nels can be better, as we show below. The pub-\nlished packages contain both TensorFlow check-\npoints (for possible ﬁne-tuning, if need be) and\nmodel ﬁles in the standard Hierarchical Data For-\nmat (HDF5) for easier inference usage. In addi-\ntion, we have setup ELMoViz, a demo web service\nto explore Norwegian ELMo models.9\n6 NorBERT\nOur NorBERT model is trained from scratch for\nNorwegian, and can be used in exactly the same\nway as any other BERT-like model. The NorBERT\ntraining setup heavily builds on prior work on Fin-\nBERT conducted at the University of Turku (Vir-\ntanen et al., 2019).\nNorBERT features a custom WordPiece vocab-\nulary which is case-sensitive and includes ac-\ncented characters. It has much better coverage\nof Norwegian words than the mBERT model or\nNB-BERT (which uses the same vocabulary as\nmBERT). This is clearly seen on the example of\nthe tokenization performed by both for the Norwe-\n8https://pypi.org/project/simple-elmo/\n9http://vectors.nlpl.eu/explore/\nembeddings/en/contextual/\ngian sentence ‘Denne gjengen h˚aper at de sammen\nskal bidra til ˚a gi kvinnefotballen i Kristiansand et\nlenge etterlengtet løft’\n• mBERT/NB-BERT: ‘Denne g ##jeng ##en\nh ## ˚a ##per at de sammen skal bid ##ra til\n˚a gi k ##vinne ##fo ##t ##ball ##en i Kris-\ntiansand et lenge etter ##len ##gte ##t l ##ø\n##ft’\n• NorBERT: ‘Denne gjengen h˚aper at de sam-\nmen skal bidra til ˚a gi kvinne ##fotball ##en\ni Kristiansand et lenge etterl ##engt ##et løft’\nNorBERT tokenization splits the sentence into\npieces which much better reﬂect the real Nor-\nwegian words and morphemes (cf. ‘ k vinne fo\nt ball en ’ versus ‘kvinne fotball en ’). We be-\nlieve this to be extremely important for more\nlinguistically-oriented studies, where it is critical\nto deal with words, not with arbitrarily fragmented\npieces (even if they are well-performing in practi-\ncal tasks).\nThe vocabulary for the model is of size 30,000.\nIt is much less than the 120,000 of mBERT, but it\nis compensated by these entities being almost ex-\nclusively Norwegian. The vocabulary was gener-\nated from raw text, without, e.g., separating punc-\ntuation from word tokens. This means one can\nfeed raw text into NorBERT.\nFor the vocabulary generation, we used the Sen-\ntencePiece algorithm (Kudo, 2018) and Tokeniz-\ners library.10 The resulting Tokenizers model was\nconverted to the standard BERT WordPiece for-\nmat. The ﬁnal vocabulary contains several thou-\nsand unused wordpiece slots which can be ﬁlled\nin with task-speciﬁc lexical entries for further ﬁne-\ntuning by future NorBERT users.\n6.1 Training technicalities\nNorBERT corresponds in its conﬁguration to the\nGoogle’s Bert-Base Cased for English, with 12\nlayers and hidden size 768 (Devlin et al., 2019).\nWe used the standard masked language model-\ning and next sentence prediction losses with the\nLAMB optimizer (You et al., 2020). The model\nwas trained on the Norwegian academic HPC sys-\ntem called Saga. Most of the time the training pro-\ncess was distributed across 4 compute nodes and\n16 NVIDIA P100 GPUs. Overall, it took approxi-\nmately 3 weeks (more than 500 hours).\n10https://github.com/huggingface/\ntokenizers\nFigure 1: NorBERT loss plots at the Phase 1 (left) and Phase 2 (right).\nSimilar to Virtanen et al. (2019), we employed\nthe BERT implementation by NVIDIA 11, which\nallows fast multi-node and multi-GPU training.\nWe made minor changes to this code, mostly\nto adapt it to the newer TensorFlow versions. All\nthese patches and the utilities we used at the pre-\nprocessing, training and evaluation stages are pub-\nlished in our GitHub repository. 12 Instructions to\nreproduce the training setup with the EasyBuild\nsoftware build and installation framework are also\navailable.13\n6.2 Training workﬂow\nPhase 1 (training with maximum sequence length\nof 128) was done with batch size 48 and global\nbatch size 48*16=768. Since one global batch\ncontains 768 sentences, approximately 265,000\ntraining steps constitute 1 epoch (one pass over the\nwhole corpus). We have done 3 epochs: 795,000\ntraining steps.\nPhase 2 (training with maximum sequence\nlength of 512) was done with batch size 8 and\nglobal batch size 8*16=128. We aimed at mimick-\ning the original BERT in that at Phase 2 the model\nshould see about 1/9 of the number of sentences\nseen during Phase 1. Thus, we needed about 68\nmillion sentences, which at the global batch size\nof 128 boils down to 531,000 training steps more.\nThe loss plots are shown in Figure 1 (the train-\ning was on pause on December 25 and 26, since\nwe were solving problems with mixed precision\n11https://github.com/NVIDIA/\nDeepLearningExamples/tree/master/\nTensorFlow/LanguageModeling/BERT, version\n20.06.08\n12https://github.com/ltgoslo/NorBERT\n13http://wiki.nlpl.eu/index.php/Eosc/\npretraining/nvidia\nTask Train Dev Test\nPOS Bokm˚al 15,696 2,409 1,939\nPOS Nynorsk 14,174 1,890 1,511\nNER Bokm˚al 15,696 2,409 1,939\nNER Nynorsk 14,174 1,890 1,511\nSentence-level SA 2,675 516 417\nFine-grained SA 8,543 1,531 1,272\nNegation 8,543 1,531 1,272\nTable 1: Number of sentences in the training, de-\nvelopment, and test splits in the datasets used for\nthe evaluation tasks.\ntraining). Full logs are available at the GitHub\nrepository.\n7 Evaluation\nThis section presents benchmark results across a\nrange of different tasks. We compare NorELMO\nand NorBERT to both mBERT and to the recently\nreleased NB-BERT model described in Section 4.\nWhere applicable, we show separate evaluation re-\nsults for Bokm˚al and Nynorsk. Below we ﬁrst pro-\nvide an overview of the different tasks and the cor-\nresponding classiﬁers that we train, before turning\nto discuss the results.\n7.1 Task descriptions\nWe start by brieﬂy describing each task and asso-\nciated dataset, in addition to the architectures we\nuse. The sentence counts for the different datasets\nand their train, dev. and test splits are provided in\nTable 1.\nPart-of-speech tagging The Norwegian Depen-\ndency Treebank (NDT) (Solberg et al., 2014) in-\ncludes annotation of POS tags for both Bokm ˚al\nand Nynorsk. NDT has also been converted to\nthe Universal Dependencies format (Øvrelid and\nHohle, 2016; Velldal et al., 2017) and this is the\nversion we are using here (for UD 2.7) for predict-\ning UPOS tags.\nWe use a typical sequence labelling approach\nwith the BERT models, adding a linear layer af-\nter the ﬁnal token representations and taking the\nsoftmax to get token predictions. We ﬁne-tune all\nparameters for 20 epochs, using a learning rate\nof 2e-5, a training batch size of 8, max length\nof 256, and keep the best model on the devel-\nopment set. ELMo models were not ﬁne-tuned,\nfollowing the recommendations from Peters et al.\n(2019). Instead we trained a simple neural classi-\nﬁer (a feed forward network with one hidden layer\nof size 128, ReLU non-linear activation function\nand dropout), using ELMo token embeddings as\nfeatures. The random seed has been kept ﬁxed all\nthe time. Models are evaluated on accuracy.\nNamed entity recognition The NorNE 14\ndataset annotates the UD-version of NDT with a\nrich set of entity types (Jørgensen et al., 2020).\nThe evaluation metrics here is ‘strict’ micro F 1,\nrequiring both the correct entity type and exact\nmatch of boundary surface string. We predict 8\nentity types: Person (PER), Organisation (ORG),\nLocation (LOC), Geo-political entity, with a\nlocative sense (GPE-LOC), Geo-political entity,\nwith an organisation sense (GPE-ORG), Product\n(PROD), Event (EVT), Nominals derived from\nnames (DRV). The evaluation is done using the\ncode for the SemEval’13 Task 915.\nWe cast the named entity recognition problem\nas a sequence labelling task, using a BIO label en-\ncoding. For the BERT-based models, we solve it\nby ﬁne-tuning the pre-trained model on the NorNE\ndataset for 20 epochs with early stopping and\nbatch size 32. The resulting model is applied to\nthe test set.\nFor ELMo models, we infer contextualised to-\nken embeddings (averaged representations across\nall 3 layers) for all words. Then, these token\nembeddings are fed to a neural classiﬁer with\ndropout, identical to the one we used for POS tag-\nging earlier. This classiﬁer is also trained for 20\nepochs with early stopping and batch size 32.\n14https://github.com/ltgoslo/norne\n15https://github.com/davidsbatista/\nNER-Evaluation\nFine-grained sentiment analysis NoReCﬁne is\na dataset16 comprising a subset of the Norwegian\nReview Corpus (NoReC; Velldal et al., 2018) an-\nnotated for sentiment holders, targets, expressions,\nand polarity, as well as the relationships between\nthem (Øvrelid et al., 2020). We here cast the prob-\nlem as a graph prediction task and train a graph\nparser (Dozat and Manning, 2018; Kurtz et al.,\n2020) to predict sentiment graphs. The parser cre-\nates token-level representations which is the con-\ncatenation of a word embedding, POS tag embed-\nding, lemma embedding, and character embedding\ncreated by a character-based LSTM. We further\naugment these representations with contextualised\nembeddings from each model. Models are trained\nfor 100 epochs, keeping the best model on de-\nvelopment F1. For span extraction (holders, tar-\ngets, expressions), we evaluate token-level F1, and\nthe common Targeted F 1 metric, which requires\ncorrectly extracting a target (strict) and its polar-\nity. We also evaluate Labelled and Unlabelled F1,\nwhich correspond to Labelled and Unlabelled At-\ntachment in dependency parsing. Finally, we eval-\nuate on Sentiment Graph F 1 (SF1) and Non-polar\nSentiment Graph F1 (NSF1. SF1 requires predict-\ning all elements (holder, target, expression, polar-\nity) and their relationships (NSF1 removes the po-\nlarity). A true positive is deﬁned as an exact match\nat graph-level, weighting the overlap in predicted\nand gold spans for each element, averaged across\nall three spans. For precision we weight the num-\nber of correctly predicted tokens divided by the to-\ntal number of predicted tokens (for recall, we di-\nvide instead by the number of gold tokens). We\nallow for empty holders and targets.\nSentence-level binary sentiment classiﬁcation\nWe further evaluate on the task of sentence-level\nbinary (positive or negative) polarity classiﬁca-\ntion, using labels that we derive from NoReC ﬁne\ndescribed above. We create the dataset for\nthis by aggregating the ﬁne-grained annotations\nto the sentence-level, removing sentences with\nmixed or no sentiment. The resulting dataset,\nNoReCsentence, is made publicly available. 17 For\nthe BERT models, we use the [CLS] embedding\nof the last layer as a representation for the sentence\nand pass this to a softmax layer for classiﬁcation.\nWe ﬁne-tune the models in the same way as for\n16https://github.com/ltgoslo/norec_fine\n17https://github.com/ltgoslo/norec_\nsentence\nPOS\nModel BM NN Time\nStanza (Qi et al., 2020) 98.3 97.9 –\nNorELMo30 98.1 97.4 8\nNorELMo100 98.0 97.4 8\nmBERT 98.0 97.9 245\nNB-BERT 98.7 98.3 244\nNorBERT 98.5 98.0 238\nTable 2: Evaluation scores of the NorLM models\non the POS tagging of Bokm˚al (BM) and Nynorsk\n(NN) test sets in comparison with other large pre-\ntrained models for Norwegian. Running times in\nminutes are given for Bokm˚al.\nthe POS tagging task, training the models for 20\nepochs and keeping the model that performs best\non the development data. For ELMo models, we\nused a BiLSTM with global max pooling, taking\nELMo token embeddings from the top layer as an\ninput. The evaluation metric is macro F1.\nNegation detection Finally, the NoReC ﬁne\ndataset has recently been annotated with nega-\ntion cues and their corresponding in-sentence\nscopes (Mæhlum et al., 2021). The resulting\ndataset is dubbed NoReC neg.18 We use the same\ngraph-based modeling approach as described for\nﬁne-grained sentiment above. We evaluate on the\nsame metrics as in the *SEM 2012 shared task\n(Morante and Blanco, 2012): cue-level F1 (CUE),\nscope token F 1 over individual tokens (ST), and\nthe combined full negation F1 (FN).\n7.2 Results\nWe present the results for the various benchmark-\ning tasks below.\nPOS tagging As can be seen from Table 2, Nor-\nBERT outperforms mBERT on both tasks: on POS\ntagging for Bokm ˚al by 5 percentage points and 1\npercentage point for Nynorsk. NorBERT is almost\non par with NB-BERT on POS tagging. NorELMo\nmodels are outperformed by NB-BERT and Nor-\nBERT, but are on par with mBERT in POS tag-\nging. Note that their adaptation to the tasks (ex-\ntracting token embeddings and learning a classi-\nﬁer) takes 30x less time than with the BERT mod-\nels.\n18https://github.com/ltgoslo/norec_neg\nModel Bokm ˚al Nynorsk Time\nNorELMo30 79.9 75.6 2\nNorELMo100 81.3 75.1 2\nmBERT 78.8 81.7 14\nNB-BERT 90.2 88.6 11\nNorBERT 85.5 82.8 9\nTable 3: NER evaluation scores (micro F 1) of the\nNorLM models on the NorNE test set in compar-\nison with other large pre-trained models for Nor-\nwegian. Running time is given in minutes for the\nBokm˚al part (on 1 NVIDIA P100 GPU).\nSee Figure 2 for the examples of training dy-\nnamics of the Nynorsk model.\nNamed entity recognition Table 3 shows the\nperformance on the NER task. NB-BERT is the\nbest on both Bokm ˚al and Nynorsk, closely fol-\nlowed by NorBERT. Unsurprisingly, mBERT falls\nbehind all the models trained for Norwegian, when\nevaluated on Bokm˚al data. With Nynorsk, it man-\nages to outperform NorELMo. Bokm ˚al is pre-\nsumably dominant in the training corpora of both.\nHowever, in the course of ﬁne-tuning, mBERT\nseems to be able to adapt to the speciﬁcs of\nNynorsk. Since our ELMo setup did not include\nthe ﬁne-tuning step, the NorELMo models’ adap-\ntation abilities were limited by what can be learned\nfrom contextualised token embeddings produced\nby a frozen model. Still, when used on the\ndata more similar to the training corpus (Bokm˚al),\nELMo achieves competitive results even without\nany ﬁne-tuning.\nIn terms of computational efﬁciency, the adap-\ntation of ELMo models to this task requires 6x\nless time than mBERT or NB-BERT and 4x less\ntime than NorBERT. Note also that the NorBERT\nmodel takes less time to ﬁne-tune than the NB-\nBERT model (although the number of epochs\nwas exactly the same), because of a smaller vo-\ncabulary, and thus less parameters in the model.\nAgain, in this case an NLP practitioner has a rich\nspectrum of tools to choose from, depending on\nwhether speed or performance on the downstream\ntask is prioritised.\nFine-grained sentiment analysis Table 4 shows\nthat NorBERT outperforms mBERT on all metrics\nand NB-BERT on all but SF1, although the differ-\nences between NorBERT and NB-BERT are gen-\nSpans Targeted Parsing Graph Sent. Graph\nModel Holder F 1 Target F1 Exp. F1 F1 UF1 LF1 NSF1 SF1 Time\nExtraction [1] 42.4 31.3 31.3 – – – – – –\nNorELMo30 55.1 55.3 57.2 37.9 49.0 41.2 40.9 34.5 446\nNorELMo100 58.8 55.8 56.8 37.1 49.7 41.2 41.5 34.2 434\nmBERT 57.1 55.2 56.3 34.8 48.7 38.3 40.5 31.7 444\nNB-BERT 61.3 56.1 57.9 36.0 49.7 41.9 40.7 34.8 404\nNorBERT 63.0 56.4 58.1 36.9 50.5 42.2 41.0 34.8 438\nTable 4: Average score of NorLM models on ﬁne-grained sentiment (5 runs with set random seeds).\nBold denotes the best result on each metric. [1] Span extraction baseline from Øvrelid et al. (2020),\nwhich uses a BiLSTM CRF with pretrained fastText embeddings.\nModel F 1\nNorELMo30 75.0\nNorELMo100 75.0\nmBERT 67.7\nNB-BERT 83.9\nNorBERT 77.1\nTable 5: F 1 scores for the different LMs models\non the binary sentiment classiﬁcation test set.\nModel CUE ST FN Time\nNorELMo 30 91.7 80.6 63.8 428\nNorELMo 100 92.2 81.3 65.5 407\nmBERT 92.8 84.0 65.9 353\nNB-BERT 92.4 83.1 63.5 342\nNorBERT 92.1 83.6 65.5 426\nTable 6: Results of our negation parser, augment-\ning the features with token representations from\neach language model. The results are averaged\nover 5 runs.\nerally small.\nOn this task the NorELMo models generally\noutperform mBERT as well. However, unlike in\nthe previous tasks, the running times here are sim-\nilar for BERT and ELMo models, since no ﬁne-\ntuning was applied (the same is true for negation\ndetection). We furthermore compare with the pre-\nvious best model (Øvrelid et al., 2020), a span ex-\ntraction model which uses a single-layer Bidirec-\ntional LSTM with Conditional Random Field in-\nference, and an embedding layer initialized with\nfastText vectors trained on the NoWaC corpus. All\napproaches using language models outperform the\nprevious baseline by a large margin on the span ex-\ntraction tasks.19 NorBERT, in particular, achieves\nimprovements of 20.6 percentage points on Holder\nF1 (24.9 and 25.8 on Target and Exp. F 1, respec-\ntively).\nBinary sentiment classiﬁcation Table 5 shows\nthat NorBERT outperforms mBERT by 9.4 per-\ncentage points on sentiment analysis. However,\nit seems that in binary sentiment classiﬁcation the\nsheer amount of training data starts to show its\nbeneﬁts, and NB-BERT outperforms NorBERT by\n6.8 points. NorELMo models outperform mBERT\nby 7.3 points.\nFigure 2 shows the training dynamics of the\nmodels.\nNegation detection From Table 6 we can see\nthat mBERT gives the best overall results, fol-\nlowed by NorBERT and NorELMo100. NB-BERT\nand NorELMo30 perform worse than the others on\nScope token F 1 (ST) and full negation F 1 (FN),\nwhile all models perform similarly at cue-level F1\n(CUE). We hypothesise that the structural similar-\nity of negation across many of the pretraining lan-\nguages gives mBERT an advantage, but it is still\nsurprising that it outperforms NB-BERT and Nor-\nBERT.\n8 Future plans\nIn the future, separate Bokm˚al and Nynorsk BERT\nmodels are planned, and we further expect to\ntrain and evaluate models with a higher number of\nepochs over the training corpus. While we plan to\ndevelop additional monolingual Norwegian mod-\nels based on other contextualised LM architectures\n19Øvrelid et al. (2020) only perform span extraction.\nTherefore, it is not possible to compare the other metrics.\nFigure 2: Per-epoch performance on training and development data for two of the tasks. Left: accuracy\nfor POS tagging (Norwegian Nynorsk). Right: F1 for binary sentiment classiﬁcation.\nbeyond BERT and ELMo, we would also be in-\nterested to explore the usefulness of multilingual\nmodels restricted to Scandinavian languages. Fur-\nther streamlining of the benchmarking process, in\nterms of both data access and computation of met-\nrics, is something we also want to address in future\nwork.\nIn addition, the ready availability of a highly\noptimised software stack on multiple HPC sys-\ntems (published as part of NorLM) may contribute\nto other researchers developing very large con-\ntextualised language models for additional lan-\nguages or language variants, e.g. domain- or\napplication-speciﬁc sub-corpora. We hope that\nmore pre-trained NLP models for Norwegian from\nboth academy and industry will be openly re-\nleased, making it possible to study the interplay\nbetween training corpora sizes, hyperparameters,\npre-preprocessing decisions and performance in\ndifferent tasks. At the same time, given the re-\nsource demands and sustainability issues related to\ntraining such models, we believe it will be impor-\ntant to coordinate efforts and we hope to collabo-\nrate closely with other players moving forward.\n9 Summary\nThis paper has described the ﬁrst outcomes of\nNorLM, an initiative coordinated by the Language\nTechnology Group at the University of Oslo seek-\ning to provide Norwegian (and Nordic) large-\nscale contextualised language models, while si-\nmultaneously focusing on maintaining a re-usable\nsoftware environment for model development on\nnational and Nordic HPC infrastructure. We\nhave here described the training and testing of\nNorELMo and NorBERT – the ﬁrst large-scale\nmonolingual LMs for Norwegian. We have bench-\nmarked the models across a wide array of Norwe-\ngian NLP tasks, also comparing to the multilin-\ngual mBERT model and another large-scale LM\nfor Norwegian developed in parallel work, NB-\nBERT, trained on large amounts of text from his-\ntorical sources. The results show that while the\nmonolingual models tend to yield better results,\nwhich particular model ranks ﬁrst varies across\ntasks. This underscores the importance of building\nan ecosystem of diversiﬁed models, accompanied\nby systematic benchmarking.\nAcknowledgements\nThe NorLM resources are being developed on the\nNorwegian national super-computing services op-\nerated by UNINETT Sigma2, the National In-\nfrastructure for High Performance Computing and\nData Storage in Norway, as well as on the Finnish\nnational supercomputing facilities operated by the\nCSC IT Center for Science. Software provision-\ning was ﬁnancially supported through the Euro-\npean EOSC-Nordic project; data preparation and\nevaluation were supported by the SANT project\n(Sentiment Analysis for Norwegian Text), funded\nby the Research Council of Norway (grant number\n270908). We are indebted to all funding agencies\ninvolved, the University of Oslo, and the Norwe-\ngian tax payer.\nReferences\nWanxiang Che, Yijia Liu, Yuxuan Wang, Bo Zheng,\nand Ting Liu. 2018. Towards better UD parsing:\nDeep contextualized word embeddings, ensemble,\nand treebank concatenation. In Proceedings of the\nCoNLL 2018 Shared Task: Multilingual Parsing\nfrom Raw Text to Universal Dependencies , pages\n55–64, Brussels, Belgium. Association for Compu-\ntational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nTimothy Dozat and Christopher D. Manning. 2018.\nSimpler but more accurate semantic dependency\nparsing. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguis-\ntics (Volume 2: Short Papers), pages 484–490, Mel-\nbourne, Australia. Association for Computational\nLinguistics.\nMehrdad Farahani, Mohammad Gharachorloo,\nMarzieh Farahani, and Mohammad Manthouri.\n2020. Parsbert: Transformer-based model for\nPersian language understanding. arXiv preprint\narXiv:2005.12515.\nMurhaf Fares, Andrey Kutuzov, Stephan Oepen, and\nErik Velldal. 2017. Word vectors, reuse, and replica-\nbility: Towards a community repository of large-text\nresources. In Proceedings of the 21st Nordic Con-\nference on Computational Linguistics , pages 271–\n276, Gothenburg, Sweden. Association for Compu-\ntational Linguistics.\nFredrik Jørgensen, Tobias Aasmoe, Anne-Stine Ruud\nHusev˚ag, Lilja Øvrelid, and Erik Velldal. 2020.\nNorNE: Annotating Named Entities for Norwegian.\nIn Proceedings of the 12th Edition of the Language\nResources and Evaluation Conference , Marseille,\nFrance, 2020.\nJohn Koutsikakis, Ilias Chalkidis, Prodromos Malaka-\nsiotis, and Ion Androutsopoulos. 2020. Greek-Bert:\nThe Greeks visiting Sesame street. In 11th Hellenic\nConference on Artiﬁcial Intelligence , pages 110–\n117.\nTaku Kudo. 2018. Subword regularization: Improv-\ning neural network translation models with multiple\nsubword candidates. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 66–\n75, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nPer E. Kummervold, Javier De la Rosa, Freddy Wetjen,\nand Svein Arne Brygfjeld. 2021. Operationalizing\na National Digital Library: The Case for a Norwe-\ngian Transformer Model. In Proceedings of the 23rd\nNordic Conference on Computational Linguistics.\nRobin Kurtz, Stephan Oepen, and Marco Kuhlmann.\n2020. End-to-end negation resolution as graph pars-\ning. In Proceedings of the 16th International Con-\nference on Parsing Technologies and the IWPT 2020\nShared Task on Parsing into Enhanced Universal\nDependencies, pages 14–24, Online. Association for\nComputational Linguistics.\nAndrey Kutuzov and Elizaveta Kuzmenko. 2017.\nBuilding web-interfaces for vector semantic models\nwith the WebVectors toolkit. In Proceedings of the\nSoftware Demonstrations of the 15th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics, pages 99–103, Valencia, Spain.\nAssociation for Computational Linguistics.\nPetter Mæhlum, Jeremy Barnes, Robin Kurtz, Lilja\nØvrelid, and Erik Velldal. 2021. Negation in Nor-\nwegian: an annotated dataset. In Proceedings of the\n23rd Nordic Conference on Computational Linguis-\ntics.\nMartin Malmsten, Love B ¨orjeson, and Chris Haf-\nfenden. 2020. Playing with words at the National Li-\nbrary of Sweden – making a Swedish BERT. arXiv\npreprint arXiv:2007.01658.\nRoser Morante and Eduardo Blanco. 2012. *SEM\n2012 shared task: Resolving the scope and focus\nof negation. In Proceedings of the First Joint Con-\nference on Lexical and Computational Semantics\n(*SEM), pages 265–274, Montr´eal, Canada.\nDat Quoc Nguyen and Anh Tuan Nguyen. 2020.\nPhobert: Pre-trained language models for viet-\nnamese. arXiv preprint arXiv:2003.00744.\nLilja Øvrelid and Petter Hohle. 2016. Universal De-\npendencies for Norwegian. In Proceedings of the\n10th International Conference on Language Re-\nsources and Evaluation (LREC’16) , pages 1579–\n1585, Portoroˇz, Slovenia.\nLilja Øvrelid, Petter Mæhlum, Jeremy Barnes, and Erik\nVelldal. 2020. A ﬁne-grained sentiment dataset for\nNorwegian. In Proceedings of the 12th Language\nResources and Evaluation Conference, pages 5025–\n5033, Marseille, France. European Language Re-\nsources Association.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nMatthew E. Peters, Sebastian Ruder, and Noah A.\nSmith. 2019. To tune or not to tune? adapting pre-\ntrained representations to diverse tasks. In Proceed-\nings of the 4th Workshop on Representation Learn-\ning for NLP (RepL4NLP-2019) , pages 7–14, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton,\nand Christopher D. Manning. 2020. Stanza: A\npython natural language processing toolkit for many\nhuman languages. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics: System Demonstrations , pages 101–\n108, Online. Association for Computational Lin-\nguistics.\nRadim ˇReh˚uˇrek and Petr Sojka. 2010. Software Frame-\nwork for Topic Modelling with Large Corpora. In\nProceedings of the LREC 2010 Workshop on New\nChallenges for NLP Frameworks, pages 45–50, Val-\nletta, Malta.\nPer Erik Solberg, Arne Skjærholt, Lilja Øvrelid, Kristin\nHagen, and Janne Bondi Johannessen. 2014. The\nNorwegian Dependency Treebank. In Proceedings\nof the Ninth International Conference on Language\nResources and Evaluation, Reykjavik, Iceland.\nMatej Ul ˇcar and Marko Robnik- ˇSikonja. 2020. High\nquality ELMo embeddings for seven less-resourced\nlanguages. In Proceedings of the 12th Language\nResources and Evaluation Conference, pages 4731–\n4738, Marseille, France.\nErik Velldal, Lilja Øvrelid, Eivind Alexander Bergem,\nCathrine Stadsnes, Samia Touileb, and Fredrik\nJørgensen. 2018. NoReC: The Norwegian Review\nCorpus. In Proceedings of the 11th edition of the\nLanguage Resources and Evaluation Conference ,\npages 4186–4191, Miyazaki, Japan.\nErik Velldal, Lilja Øvrelid, and Petter Hohle. 2017.\nJoint UD parsing of Norwegian Bokm ˚al and\nNynorsk. In Proceedings of the 21st Nordic Con-\nference of Computational Linguistics , pages 1–10,\nGothenburg, Sweden.\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma,\nJuhani Luotolahti, Tapio Salakoski, Filip Ginter, and\nSampo Pyysalo. 2019. Multilingual is not enough:\nBert for ﬁnnish. arXiv preprint arXiv:1912.07076.\nWietse de Vries, Andreas van Cranenburgh, Arianna\nBisazza, Tommaso Caselli, Gertjan van Noord, and\nMalvina Nissim. 2019. Bertje: A Dutch Bert model.\narXiv preprint arXiv:1912.09582.\nYang You, Jing Li, Sashank Reddi, Jonathan Hseu,\nSanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song,\nJames Demmel, Kurt Keutzer, and Cho-Jui Hsieh.\n2020. Large batch optimization for deep learning:\nTraining BERT in 76 minutes. InInternational Con-\nference on Learning Representations.\nDaniel Zeman, Jan Haji ˇc, Martin Popel, Martin Pot-\nthast, Milan Straka, Filip Ginter, Joakim Nivre, and\nSlav Petrov. 2018. CoNLL 2018 shared task: Mul-\ntilingual parsing from raw text to Universal Depen-\ndencies. In Proceedings of the CoNLL 2018 Shared\nTask: Multilingual Parsing from Raw Text to Univer-\nsal Dependencies, pages 1–21, Brussels, Belgium.\nAssociation for Computational Linguistics."
}