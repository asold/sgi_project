{
  "title": "Transformer Inertial Poser: Real-time Human Motion Reconstruction from Sparse IMUs with Simultaneous Terrain Generation",
  "url": "https://openalex.org/W4297168084",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A222123267",
      "name": "Jiang Yifeng",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2668412359",
      "name": "Ye, Yuting",
      "affiliations": [
        "META Health"
      ]
    },
    {
      "id": "https://openalex.org/A2750715226",
      "name": "Gopinath, Deepak",
      "affiliations": [
        "Art Institute of Portland"
      ]
    },
    {
      "id": "https://openalex.org/A4223330763",
      "name": "Won, Jungdam",
      "affiliations": [
        "Art Institute of Portland"
      ]
    },
    {
      "id": "https://openalex.org/A4223330764",
      "name": "Winkler, Alexander W.",
      "affiliations": [
        "META Health"
      ]
    },
    {
      "id": "https://openalex.org/A3202085148",
      "name": "Liu, C. Karen",
      "affiliations": [
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3036644940",
    "https://openalex.org/W2554986796",
    "https://openalex.org/W2064313486",
    "https://openalex.org/W2994660296",
    "https://openalex.org/W2901806907",
    "https://openalex.org/W3201701095",
    "https://openalex.org/W2146881125",
    "https://openalex.org/W2532146065",
    "https://openalex.org/W2891718801",
    "https://openalex.org/W2963876278",
    "https://openalex.org/W3168271366",
    "https://openalex.org/W3048665613",
    "https://openalex.org/W2898073175",
    "https://openalex.org/W2975420824",
    "https://openalex.org/W3203495036",
    "https://openalex.org/W3184168488",
    "https://openalex.org/W1967645203",
    "https://openalex.org/W2027389517",
    "https://openalex.org/W1967554269",
    "https://openalex.org/W2784646928",
    "https://openalex.org/W3008597842",
    "https://openalex.org/W3153832461",
    "https://openalex.org/W2112324691",
    "https://openalex.org/W2017249118",
    "https://openalex.org/W4214679856",
    "https://openalex.org/W3106857494",
    "https://openalex.org/W2156094778",
    "https://openalex.org/W2754534665",
    "https://openalex.org/W4200526174",
    "https://openalex.org/W3088412635",
    "https://openalex.org/W3139397654",
    "https://openalex.org/W2895748257",
    "https://openalex.org/W2113148808",
    "https://openalex.org/W3135142441",
    "https://openalex.org/W4225609743",
    "https://openalex.org/W3180730526",
    "https://openalex.org/W3035139896",
    "https://openalex.org/W2894678789",
    "https://openalex.org/W2949924544",
    "https://openalex.org/W3171175619",
    "https://openalex.org/W2962730651"
  ],
  "abstract": "Real-time human motion reconstruction from a sparse set of (e.g. six)\\nwearable IMUs provides a non-intrusive and economic approach to motion capture.\\nWithout the ability to acquire position information directly from IMUs, recent\\nworks took data-driven approaches that utilize large human motion datasets to\\ntackle this under-determined problem. Still, challenges remain such as temporal\\nconsistency, drifting of global and joint motions, and diverse coverage of\\nmotion types on various terrains. We propose a novel method to simultaneously\\nestimate full-body motion and generate plausible visited terrain from only six\\nIMU sensors in real-time. Our method incorporates 1. a conditional Transformer\\ndecoder model giving consistent predictions by explicitly reasoning prediction\\nhistory, 2. a simple yet general learning target named \"stationary body points\"\\n(SBPs) which can be stably predicted by the Transformer model and utilized by\\nanalytical routines to correct joint and global drifting, and 3. an algorithm\\nto generate regularized terrain height maps from noisy SBP predictions which\\ncan in turn correct noisy global motion estimation. We evaluate our framework\\nextensively on synthesized and real IMU data, and with real-time live demos,\\nand show superior performance over strong baseline methods.\\n",
  "full_text": "Transformer Inertial Poser: Real-time Human Motion\nReconstruction from Sparse IMUs with Simultaneous Terrain\nGeneration\nYifeng Jiang\nStanford University\nUnited States of America\nyifengj@stanford.edu\nYuting Ye\nMeta Reality Labs Research\nUnited States of America\nyuting.ye@fb.com\nDeepak Gopinath\nMeta Reality Labs Research\nUnited States of America\ndgopinath@fb.com\nJungdam Won\nMeta Reality Labs Research\nUnited States of America\njungdam@fb.com\nAlexander W. Winkler\nMeta Reality Labs Research\nUnited States of America\nwinklera@fb.com\nC. Karen Liu\nStanford University\nUnited States of America\nkarenliu@cs.stanford.edu\nABSTRACT\nReal-time human motion reconstruction from a sparse set of (e.g.\nsix) wearable IMUs provides a non-intrusive and economic ap-\nproach to motion capture. Without the ability to acquire position\ninformation directly from IMUs, recent works took data-driven\napproaches that utilize large human motion datasets to tackle this\nunder-determined problem. Still, challenges remain such as tem-\nporal consistency, drifting of global and joint motions, and diverse\ncoverage of motion types on various terrains. We propose a novel\nmethod to simultaneously estimate full-body motion and generate\nplausible visited terrain from only six IMU sensors in real-time. Our\nmethod incorporates 1. a conditional Transformer decoder model\ngiving consistent predictions by explicitly reasoning prediction his-\ntory, 2. a simple yet general learning target named \"stationary body\npointsâ€ (SBPs) which can be stably predicted by the Transformer\nmodel and utilized by analytical routines to correct joint and global\ndrifting, and 3. an algorithm to generate regularized terrain height\nmaps from noisy SBP predictions which can in turn correct noisy\nglobal motion estimation. We evaluate our framework extensively\non synthesized and real IMU data, and with real-time live demos,\nand show superior performance over strong baseline methods.\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Motion capture.\nKEYWORDS\nWearable Devices, Inertial Measurement Units, Human Motion\nACM Reference Format:\nYifeng Jiang, Yuting Ye, Deepak Gopinath, Jungdam Won, Alexander W.\nWinkler, and C. Karen Liu. 2022. Transformer Inertial Poser: Real-time Hu-\nman Motion Reconstruction from Sparse IMUs with Simultaneous Terrain\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSA â€™22 Conference Papers, December 6â€“9, 2022, Daegu, Republic of Korea\nÂ© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-9470-3/22/12. . . $15.00\nhttps://doi.org/10.1145/3550469.3555428\nGeneration. In SIGGRAPH Asia 2022 Conference Papers (SA â€™22 Conference\nPapers), December 6â€“9, 2022, Daegu, Republic of Korea. ACM, New York, NY,\nUSA, 12 pages. https://doi.org/10.1145/3550469.3555428\n1 INTRODUCTION\nReal-time reconstruction of 3D human motion is crucial for ap-\nplications in various domains, such as biomechanics and sports\nanalysis, motion-based video games, and virtual presence in VR/AR\nsystems. While marker-based optical motion capture systems [Vi-\ncon n d] remain an ideal option for research labs and professional\nstudios due to the superior accuracy, more and more applications\ndemand a portable, less costly, and minimally-intrusive mocap sys-\ntem that reconstructs human movements in real-time and can be\nused anywhere by everyone.\nFigure 1: We develop an attention-based deep learning\nmethod to reconstruct full-body motion from six IMU sen-\nsors in real-time. Our method simultaneously generates\nplausible terrain maps that can explain the reconstructed\nmotions, for a variety of motion types.\narXiv:2203.15720v3  [cs.CV]  8 Dec 2022\nSA â€™22 Conference Papers, December 6â€“9, 2022, Daegu, Republic of Korea Yifeng Jiang, Yuting Ye, Deepak Gopinath, Jungdam Won, Alexander W. Winkler, and C. Karen Liu\nAmong many sensing modalities, such as RGB cameras [Cao\net al. 2019; GÃ¼ler et al. 2018; Kanazawa et al. 2019], depth cameras\n[Taylor et al. 2012; Wei et al. 2012], or wearable electromagnetic sen-\nsors [Kaufmann et al. 2021], inertial measurement sensors (IMUs)\n[Huang et al. 2018; von Marcard et al. 2017] provide many unique\nadvantages. IMU-based mocap is self-contained, egocentric, and\nuntethered, applicable to both indoor and outdoor activities in all\nlighting conditions. For example, an IMU system can capture long-\ndistance motion of a hiker or a mountain climber, but third-person\ncameras would be limited by their field of view. In addition, IMU-\nbased mocap is unsusceptible to occlusion and is less sensitive to\nprivacy issues. Recent works [Huang et al . 2018; Yi et al . 2021]\nhave advanced the field from commercial systems using 17 IMUs\n[Xsens n d] to machine-learning-based systems using only six IMUs,\ndemonstrating the great potential of the IMU-based approach as a\npractical and reliable solution to \"everyday motion capture\".\nWith all the recent progress, there are still a few critical issues,\nsuch as temporal inconsistency of prediction, and global transla-\ntional drifts, that prevent IMU-based mocap systems to achieve\ntheir full potential. In response to these challenges, we propose a\nnovel method to enhance the reconstruction of full-body human\nmotion in real-time from only six IMU sensors. First, inspired by re-\ncent success of Transformer models for free-form natural language\ngeneration [Radford et al. 2018], we cast the IMU reconstruction\nas a constrained motion generation problem and solve it by learn-\ning a conditioned Transformer decoder model. Comparing with\nRNN-based models used by previous methods, our model is able to\nachieve more accurate joint angle estimation, especially for those\nmotions with almost identical IMU signals such as sitting and stand-\ning, thanks to Transformerâ€™s improved ability to reason about its\nown past predictions by taken them explicitly as input.\nSecond, we introduce a general technique to address the gradual\ndrift of global translation and joint motion in a unified way. We\ntrain our Transformer decoder model to additionally predict the\npoints on the character that have near-zero velocity, which we call\n\"Stationary Body Points\" (SBPs). Once learned, SBPs can be used by\nanalytical routines to counteract drifts in motion, which is particu-\nlarly helpful when there is a distribution mismatch between test\nand training sets due to noisy or corrupted IMU sensors, or unseen\nmotions. Concurrent to our work, [Yi et al. 2022] also tackles the\ndrifting problems using a combination of physics-based optimiza-\ntion and improved initialization techniques for RNNs. Our work is\ninterestingly complementary to [Yi et al. 2022] on this front.\nThe IMU-based approach opens the door to motion capture in\nlarge environments with a variety of terrains, posing a new research\nchallenge of terrain reconstruction in real-time. As our aforemen-\ntioned motion estimators are terrain-agnostic, by leveraging the\npredicted SBPs, our method can be used to generate plausible height\nmaps of the terrain traversed by the human. Moreover, instead of\ntreating terrain reconstruction as a byproduct of our method, we\nsimultaneously estimate the human motion and the correspond-\ning terrains such that the generated terrains can also be used to\nregularize the reconstructed global motion in real-time.\nWe evaluate our system, which we call \"Transformer Inertial\nPoser\" (TIP), extensively on synthetic and real datasets, and show-\ncase clear improvement over recent state-of-the-art methods on\nflat terrains. As no real IMU datasets with non-flat terrains exist\nfor training or testing, we qualitatively evaluate our reconstructed\nmotions along with the simultaneously generated terrains both in\nsimulation and with our own live demos. We additionally show\nreal-time live demos covering a wide variety of motion types, many\nof which unseen in previous works.\n2 RELATED WORK\nHuman motion reconstruction from various sensor inputs has been\nstudied for a long time particularly in Computer Graphics and\nComputer Vision communities. We mainly review prior works that\nuse IMU sensors as part of or the sole input modality. We also review\nmotion generation models based on the Transformer [Vaswani et al.\n2017] as it constitutes the core of our reconstruction model.\nA typical IMU sensor includes an accelerometer measuring 3-axis\nlinear acceleration, a gyroscope measuring 3-axis angular veloc-\nity, and a magnetometer identifying the vector pointing Earthâ€™s\nmagnetic north. From these raw signals, sensor fusion algorithms\nbased on Kalman filter or its extended version are used to provide\nmore robust measures of the orientation [Bachmann et al . 2001;\nDel Rosario et al. 2018; Foxlin 1996; Vitali et al. 2021]. IMU sensors\nhave been used along with vision-based sensors such as RGB or\nRGB-D cameras for motion estimation. Some work regard IMU\nsignals as extra constraints to regularize motions predicted from\nvision, where those constraints are formulated either in offline op-\ntimizations [Helten et al . 2013; Pons-Moll et al . 2011, 2010; von\nMarcard et al . 2018, 2016; Zheng et al . 2018], online (often per-\nframe) optimizations [Charles Malleson 2020; Malleson et al. 2017;\nZhang et al. 2020], or learning deep neural networks [Gilbert et al.\n2019; Trumble et al. 2017]. There have also been works combining\nIMUs with other modalities such as optical markers [Andrews et al.\n2016] or ultrasonic [Liu et al . 2011; Vlasic et al. 2007]. Although\nthese systems can produce plausible motions, they also suffer from\ninherent limitations of vision-based sensors such as narrow capture\nregion, occlusion, or sensitivity to the light condition.\nWith IMU sensors getting more compact and inexpensive, they\nhave received increasing attentions from both industry and research\ncommunities for a standalone body tracking solution. Popular com-\nmercial products such as [Xsens n d] and [Rokoko n d] can generate\nhigh-quality human motions ready to be used in real-time game\nengines. However, requiring a sophisticated full-body setup with\nat least 17 IMUs hinders their accessibility to everyday users. Re-\nsearchers have therefore proposed body tracking systems with a\nsmall number of IMUs sparsely placed on the body, usually uti-\nlizing statistical body models and/or high quality optical mocap\ndata as prior to mitigate input signals being under-specified. Mar-\ncard et al. [2017] developed an offline system (SIP) with only six\nIMUs, which optimizes poses and the parameters of the SMPL body\nmodel [Loper et al . 2015] to fit the sparse sensor input. Huang\net al. [2018] learned a deep neural-net model (DIP) from a large\namount of motion capture data to directly map the IMU signals\nto poses. Their model is based on bidirectional recurrent neural\nnetworks (BRNN), so the system can run in an online manner while\nconsidering both the past and future sensor inputs with a negligible\nlatency, outperforming previous non-learning online methods. An\nensemble of BRNNs was further adopted by Nagaraj et al. [2020]\nto improve upon the results. However, the two real-time solutions\nTransformer Inertial Poser SA â€™22 Conference Papers, December 6â€“9, 2022, Daegu, Republic of Korea\nmostly focus on reconstructing the local joint motion without global\ntranslation. Yi et al. [2021] proposed a new neural model (Trans-\nPose) where the progressive upscaling of joint position estimation\nshowed more accurate pose estimation. The model can additionally\ngenerate accurate global root motions by combining a supporting-\nfoot heuristics and a small learned deep network, similar to [Rempe\net al. 2021]. Recently, an extension of this system (PIP) [2022] has\nbeen introduced, which is concurrent to our paper, where the pre-\ndicted motions are further optimized to reduce violations of physics\nlaws [Shimada et al. 2020]. We explore this problem domain with\na set of drastically different techniques, and with much more re-\nlaxed assumptions on the environment geometry, while producing\ncomparable or better reconstruction results.\nThe blooming AR/VR industry draws attention to full-body mo-\ntion synthesis, using IMUs on headsets and controllers. With no\nsenors available on the lower body, utilizing deep reinforcement\nlearning, Luo et al. [2021] is able to synthesize physically valid loco-\nmotion from only the 6D egocentric headset pose. Cha et al. [2021]\ncomplements pose estimation from headset cameras with IMUs\nwhen the hands are out-of-view. Choutas et al. [2021] and Dittadi et\nal. [2021] experimented with deep generative models conditioned\non headset and controllers poses to synthesize full-body poses.\nMore similar to our work is LoBSTR [Yang et al. 2021], where they\ninclude an IMU on the waist in addition to IMUs on the headset and\ncontrollers. Using a recurrent network, they can synthesize both\nsitting and running motions from only 4 sensors. We opt to use\n6 IMUs with lower body information for accurate motion recon-\nstruction rather than synthesis, but these sparser setups are fruitful\nfuture directions.\nSince the inception of the Transformer, attention-based mod-\nels have become the state-of-the-art on many problems involving\nsequence data, such as language translation [Brown et al . 2020]\nand audio generation [Dhariwal et al . 2020]. It is natural to also\napply Transformer models to synthesizing human motion. Aksan\net al. [2021] developed a generative model using dual attention\nmechanism to capture spatial and temporal correlations, which\npredicts future full-body locomotion given a short history. Petro-\nvich et al. [2021] used a Transformer and a variational autoenoder\nconditioned on action labels, such as walking or jumping, to gener-\nate full body motions. Valle-PÃ©rez et al. [2021] instead combined\na Transformer with normalizing flows to synthesize dancing mo-\ntion from music, building on a similar prior work [Li et al. 2021].\nFor motion reconstruction, Kim et al. [2021] experimented with a\nTransformer encoder-decoder model with sparse synthetic input\nfeatures, and found it more effective than recurrent networks. In\nour case, we face the additional challenge of handling noise from\nreal IMU sensors.\n3 TRANSFORMER INERTIAL POSER (TIP)\nWe introduce a real-time human motion reconstruction technique\nfrom six IMU sensors placed on the userâ€™s legs, wrists, head, and\nwaist. Our approach combines a learning-based model and an an-\nalytical routine to estimate full-body joint angles ğ’’ and the root\nvelocity ğ’— from a real-time stream of IMU orientation ğ‘¹ and accel-\neration ğ‘¨ signals. Summarized in Figure 2, our method depends\non a learned Transformer decoder to estimate the motion and an\nanalytical drift stabilizer to refine the estimation. In addition to\nğ’’ and ğ’—, the Transformer decoder also predicts stationary body\npoints (SBPs) ğ’„ which are used by the drift stabilizer during run\ntime to improve the accuracy of the reconstructed motion. The drift\nstabilizer utilizes the predicted SBPs to mitigate drifts in motion\nover time and a non-learning terrain update module estimates the\ncorresponding terrain while further improving motion reconstruc-\ntion. A plausible terrain height map is generated and updated in\nreal time, as an additional product of our algorithm.\nFigure 2: Overview of our pose estimation algorithm.\nThicker arrows are \"pipes\" flowing multi-timestep informa-\ntion at each model step, while thinner arrows flow current-\nstep information.\nFigure 3: Our conditional Transformer decoder architecture\nduring training, showing ğ‘€ = 39. Following GPT training\nconventions, during training, input and output contain the\nsame (but time-shifted by 1) ground-truth motion ( ğ’’ & ğ’„).\nDuring testing, output from ğ‘¡âˆ’39 to ğ‘¡âˆ’1 are discarded and the\nlast ones ğ’—ğ‘¡,ğ’„ğ‘¡,ğ’’ğ‘¡ are used as current prediction, and input\n(ğ’’ & ğ’„, i.e. the history buffers in Figure 2) is autoregressively\nupdated by most recent outputs ğ’—ğ‘¡,ğ’„ğ‘¡ at each step.\n3.1 Transformer Motion Estimator\nTo simplify the notations, consider the problem of reconstructing\nonly the joint angles ğ’’. The standard learning-based approach can\nbe summarized as max ğ‘ƒ(ğ’’ğ‘¡|ğ‘¹ğ‘¡âˆ’ğ‘:ğ‘¡,ğ‘¨ğ‘¡âˆ’ğ‘:ğ‘¡), where a neural net-\nwork is trained to output the most probable current joint angle\ngiven the IMU readings. Note that since 6 noisy IMUs do not pro-\nvide enough signals to fully determine the whole-body motion, we\nfollow previous work [Huang et al. 2018] to include a time window\nfrom ğ‘¡âˆ’ğ‘ to ğ‘¡, mitigating under-specification.\nSA â€™22 Conference Papers, December 6â€“9, 2022, Daegu, Republic of Korea Yifeng Jiang, Yuting Ye, Deepak Gopinath, Jungdam Won, Alexander W. Winkler, and C. Karen Liu\nSuch models could still have trouble distinguishing motions with\nsimilar IMU readings over the entire window, such as sitting vs.\nstanding, and the transition in between (if slow). To principally\naddress the issue, we draw an analogy between motion generation\nand language modeling: max ğ‘ƒ(ğ’’ğ‘¡âˆ’ğ‘€:ğ‘¡), where the goal is to free-\nform generate all motion (language) sequences that are natural.\nApplying recursively, a neural network can be used to generate\nplausible ğ’’ğ‘¡ conditioned on all the previous generations from itself:\nmax ğ‘ƒ(ğ’’ğ‘¡âˆ’ğ‘€:ğ‘¡)= max ğ‘ƒ(ğ’’ğ‘¡|ğ’’ğ‘¡âˆ’ğ‘€:ğ‘¡âˆ’1)ğ‘ƒ(ğ’’ğ‘¡âˆ’1|ğ’’ğ‘¡âˆ’ğ‘€:ğ‘¡âˆ’2)Â·Â·Â· ğ‘ƒ(ğ’’ğ‘¡âˆ’ğ‘€).\nA Transformer decoder model [Radford et al. 2018; Vaswani et al.\n2017] excels at this task as it explicitly takes previous predictions as\ninput and is capable of reasoning about such contexts fast enough\nin parallel with temporal attention mechanisms. The Transformer\ndecoder model is able to take variable-sized history as input but in\npractice, a maximum window length ğ‘€ is usually set.\nUnlike free-form language generation [Radford et al. 2018], we\nhave additional constraints from the IMU sensors. As such, we\nmodel our constrained motion generation problem with a con-\nditional Transformer decoder, i.e. max ğ‘ƒ(ğ’’ğ‘¡âˆ’ğ‘€:ğ‘¡|ğ‘¹ğ‘¡âˆ’ğ‘€:ğ‘¡,ğ‘¨ğ‘¡âˆ’ğ‘€:ğ‘¡),\nwhere we feed the model the sequences of IMU readings ğ‘¹ and ğ‘¨\nin parallel to its past predictions, with the same sequence length at\neach step, up to a maximum of ğ‘€.\nModel. The input to our model includes the IMU acceleration\nreadings after smoothing filtering or integration Ëœğ‘¨ âˆˆR36Ã—(ğ‘€+1)\n(Appendix A), and the IMU orientationsğ‘¹ âˆˆR54Ã—(ğ‘€+1)represented\nas flattened rotation matrix (length 9) from 6 sensors (Figure 3).\nThe output includes 18 joint angles ğ’’ğ‘¡ âˆˆR108 defined in the SMPL\n[Loper et al . 2015] human model (excluding joints such as toes\nfollowing previous works), the root linear velocity,ğ’—ğ‘¡ âˆˆR3, and the\nstationary body points (SBPs) ğ’„ğ‘¡ âˆˆR20. Each joint in ğ’’ğ‘¡ is repre-\nsented redundantly as first two columns of its local rotation matrix\nfor unique and numerically stable ground-truth labels [Zhou et al.\n2019]. The root orientation is given directly by one of the IMUs\nplaced on the waist. We empirically found that adding an recur-\nrent layer to \"summarize\" output embedding of the Transformer\naccelerates convergence during training.\nFollowing the standard practice of Transformers (GPT [Radford\net al. 2018]) training, given the shifted ground-truth sequences\nfrom ğ‘¡ âˆ’ğ‘€ âˆ’1 to ğ‘¡ âˆ’1, the model learns to predict in parallel\nthe whole sequence from ğ‘¡ âˆ’ğ‘€ to ğ‘¡ for efficiency (Figure 3). To\nprevent the model from learning simply to shift the input by one\ntimestep, a causal mask [Vaswani et al. 2017] is added to hide future\nattention information, mimicking the test-time setting. During test\ntime, since we only care about the most recent prediction, output\nfrom ğ‘¡âˆ’ğ‘€ to ğ‘¡âˆ’1 will be discarded at each prediction step.\nThere is still a large asymmetry between training and testing\ntimes. During training, the model sees ground-truth motion as the\nhistory input, but during testing, the history is noisily accumulated\nfrom its own predictions. Previous works, such as [Radford et al.\n2018], found in practice that such \"teacher-forcing\" training will\nnot cause overfitting to the clean history. In our case, different\nfrom language where neighboring words are distinct, neighboring\nposes are usually similar to each other, providing much duplicate\ninformation more susceptible to overfitting. We add a 80% dropout\n[Srivastava et al. 2014] to the history ofğ’„ and ğ’’, effectively dropping\n4 frames out of 5. We also found that excluding ğ’— from history is\nimportant to prevent test-time autoregressive divergence, possibly\nbecause ğ’— is usually nearly constant in a time window, and the\nmodel could easily exploit and overfit to history ğ’— without truly\nreasoning about IMUs or history joint angles.\n3.2 Stabilizing Drift with SBPs\nCombating drifts is one of the biggest challenges for IMU-based\nmotion reconstruction. Unlike motion estimation methods using ex-\nternal cameras, IMUs have no direct sense of relative position, and\nlearning or optimization based algorithms can all to some extent\nbe seen as relying on double integration of the noisy acceleration\nreadings to estimate position. If there is any biased error due to cal-\nibration or environment interference, the drifts of root translation\nor joint angles over time will result in artifacts, such as frequent\nfoot skating or locking, or erroneous motion transitions.\nWe present a simple and general technique, combining learned\nand analytical components, which does not require body-part-\nspecific heuristics, nor rely on assumptions of the environment\nterrains. Besides motion predicting, our model will also predict Sta-\ntionary Body Points (SBPs) ğ’„ğ‘¡ on the character, the representative\nlocations on the human body with near-zero velocity. For example,\nthe heel-to-toe rolling contact during walking results in moving\nSBPs across the foot, and rolling on the floor motion results in SBPs\nmoving across the lower back and pelvis (Figure 4). Once learned,\nSBPs could be used for analytical correction of the predicted motion.\nThough the learning of SBPs could also be subject to inaccuracies,\nthe analytical usage of the SBPs constrains the amount of errors\nthey can produce.\nFigure 4: During heel-to-toe contact in locomotion, contact\npatch is stationary but the foot link velocity is not zero. Sim-\nilarly, a person rolling on the ground can be seen as rotating\naround changing stationary points.\nDiscover Ground-truth SBPs for Learning. SBPs can be discovered\non any body part, but in practice we choose the hands, feet, and\npelvis to be the only candidate regions for SBPs in our implementa-\ntion. We parameterize each SBP location as a vector offset ğ’“ğ‘¡ âˆˆR3\nfrom the center of mass of the body part it belongs to. We use one\nadditional bit for each SBP ğ‘ğ‘¡ âˆˆ{0,1}to indicate the existence\nof SBP at the current moment. Therefore, SBP is represented as\nğ’„ âˆˆR20, where ğ’„ğ’• = [ğ‘(ğ‘–)\nğ‘¡ ,ğ’“ (ğ‘–)\nğ‘¡ ]ğ‘–=1,Â·Â·Â·,5.\nWe devise an efficient sampling-based method to search for the\npoint on the rigid body with minimal velocity,\nargmin\nğ’“\nâˆ¥ğ Ã—ğ‘¹ğµğ’“ +ğ’—âˆ¥,\nwhere ğ and ğ’— are angular and linear velocities of the body part, and\nğ‘¹ğµ is the orientation of the body, which can all be easily obtained\nTransformer Inertial Poser SA â€™22 Conference Papers, December 6â€“9, 2022, Daegu, Republic of Korea\nfrom ground-truth ğ’’ğ‘¡,ğ’—ğ‘¡ in training data. If the minimal velocity is\nbelow a small threshold, a SBP for that body part is found.\nWe evaluate the velocity of every points in a 3D grid that encloses\nthe body part of interest and find the point with the minimal velocity.\nFor example, the 3D grid for a foot is a 6cm-thick box covering the\nbottom of the foot and that for the pelvis covers a larger region on\nthe back. Evaluation for all candidate points can be done in parallel\nefficiently using matrix operations.\nIn practice, directly searching for the point with minimal velocity\ncan lead to jittery SBPs over time, due to noise in motion capture\ndata [Le Callennec and Boulic 2006] and approximating human\nbody parts with rigid bodies. We thus add a temporal regularizer to\nthe search criteria:\nğ‘™(ğ’“)= âˆ¥ğ Ã—ğ‘¹ğµğ’“ +ğ’—âˆ¥+0.3 âˆ¥ğ’“ âˆ’ğ’“ğ‘¡âˆ’1 âˆ¥,\nwhere ğ’“ğ‘¡âˆ’1 is the solution in the previous frame, if existing. If\nminimal ğ‘™(ğ’“âˆ—)is less than a manually chosen threshold (0.25), we\nlabel ğ’„ğ‘¡(ğ‘˜)= [1,ğ’“âˆ—]. Otherwise, ğ’„ğ‘¡(ğ‘˜)= [0,(0,0,0)].\nWe did not find our results sensitive to the choices of regulariza-\ntion weight (0.3) or threshold, and use the same formula for all five\nSBPs, across the entire training dataset with various terrains.\nRun-time Root Correction. The predicted SBPs provide constraints\nto correct the root velocity. We set the corrected root velocity to be\nthe average of Ëœğ’—(ğ‘–)\nğ‘¡ , where each Ëœğ’—(ğ‘–)\nğ‘¡ is the root velocity that makes\nğ‘–-th (active) SBP exactly stationary in world space at current step.\nIn practice, we only use this technique to correct root translation\nin the horizontal plane, and leave root correction in the height\ndirection to the terrain estimation module described in Sec. 3.3.\nRun-time Joint Correction. We can also modify the joint angle es-\ntimation ğ’’ğ‘¡ using SBPs. When a pair of SBPs are on for consecutive\nframes, we can solve for the joint angle correction that maintains\nthe distance vector between the SBPs. We adopt two-bone inverse\nkinematics (IK) [Holden et al. 2020], which gives numerically stable\nsolutions and changes only a minimal number of joints, Ëœğ’’ğ‘¡. Directly\nreplacing the joint angles ğ’’ğ‘¡ estimated by the Transformer decoder\nwith Ëœğ’’ğ‘¡ can result in discontinuity in motion when SBPs switch on\nand off. The problem is more visible as SBP onsets are imperfect\nmodel predictions. We adopt a new \"soft-IK\" technique utilizing the\nTransformer decoderâ€™s dependency on its own history of prediction.\nIntuitively, next model predictions will be improved if the history\nbuffer is filled with more accurate motion. As such, our idea is to\naccept the current prediction ğ’’ğ‘¡ from the Transformer, but feed the\ncorrected joint angles, Ëœğ’’ğ‘¡, back to the Transformerâ€™s history buffer.\nThis essentially creates a soft-IK constraint which does not enforce\nSBP pairs immediately in the current time step, but do so gradually\nin subsequent frames.\n3.3 Plausible Terrain Generation\nInspired by SLAM (Simultaneous Localization And Mapping) al-\ngorithms [Durrant-Whyte and Bailey 2006; Guzov et al. 2021], our\nmethod generates a plausible terrain consistent with the recon-\nstructed motion. In our case, simultaneously predicting the terrain\nand the motion is mutually beneficial in achieving improved results\nfor both. The algorithm takes as input current pelvis and feet SBP\nlocations, as well as Transformer-estimated vertical root velocity\nğ’—ğ‘¡,ğ‘§, and outputs a height map ğ‘¯ âˆˆRğ¿Ã—ğ¿ and the proposed vertical\nroot correction ğ‘ âˆˆR ( Ëœğ’—ğ‘¡,ğ‘§ = ğ’—ğ‘¡,ğ‘§ +ğ‘). We do not consider hand\nSBPs here because in most cases it is hard to know if hand rests on\nsomething or simply stays stationary in the air.\nThis algorithm is based on two mild assumptions, albeit crucial\nto regularizing noisy SBPs from real IMU data. First, if two SBPs are\nhorizontally nearby (< 1m) and have sufficiently similar heights\n(< 0.1m), we assume they have the same height on the terrain. As\nsuch, we assume there is no gradual slope in the scene, since under\nsensor drifting it is extremely difficult to distinguish mild slopes\nfrom flat ground. By this assumption, we cluster nearby SBPs with\nsimilar heights into buckets and store the mean height for each\nbucket. When a new SBP is detected, if an existing nearby cluster\nis similar in height, this SBP will join that cluster and update its\nmean height. ğ‘ = âˆ’ğ‘˜âˆ—ğ‘‘ will be proposed to drag the root so that\nthe SBP is closer to the updated mean height, where ğ‘˜is a constant\ncorrection coefficient and ğ‘‘ is the difference between SBP height\nand cluster mean height.\nSecond, the terrain is assumed to be a Voronoi diagram with\ndifferent vertical levels. That is, we assign each unvisited grid to the\nsame height as its closest SBP-visited neighbor. To build Voronoi\nonline, upon each new SBP visitation, we need to check if each\nunvisited grid is now closer to the new SBP than all existing SBPs.\nInstead of storing all distances, we only need to store the closest dis-\ntance up to now, which we call (inverse) confidence mapğ‘ª âˆˆRğ¿Ã—ğ¿.\nIf the new SBP is closer, the height mapğ‘¯ at the unvisited grid will\nbe updated to the new SBPâ€™s height (precisely, the corresponding\nSBP clusterâ€™s height, from the first assumption).\nFurther implementation details can be found in Appendix C.\n4 EVALUATIONS\nWe organize our experiments in this section to demonstrate that:\nâ€¢Using the same held-out datasets of real or synthesized\nsparse IMU signals as benchmarks, our method improves\nover recent works quantitatively by a significant margin.\nâ€¢Without training on any real non-flat-terrain IMU data, and\nwithout any ground-truth terrain supervision, our method\ncan reconstruct motions on different terrains both in simula-\ntion and on real sensor data.\nâ€¢Both real-time motion correction and terrain reconstruction\nusing SBPs contribute to our ability to stably mitigating root\ndrift during a variety of human activities.\nâ€¢Modifying joint history with SBP correction mitigates drift\nbetween joints and qualitatively improves motion recon-\nstruction in challenging motions.\nâ€¢Our Transformer decoder model taking history as input\nfacilities stable and consistent learning of the SBPs.\n4.1 Quantitative Evaluation on Flat Terrain\nWe evaluate our results on existing IMU datasets that cover di-\nverse types of motions and are paired with ground-truth full-body\nmotions, for quantitative comparisons.\nDatasets. Following previous works, evaluation datasets include\nDIP and TotalCapture of real IMU data. We match the exact train-\ning data and evaluation settings of baseline methods. This mainly\nSA â€™22 Conference Papers, December 6â€“9, 2022, Daegu, Republic of Korea Yifeng Jiang, Yuting Ye, Deepak Gopinath, Jungdam Won, Alexander W. Winkler, and C. Karen Liu\nserves to test the sim-to-real transfer of the models, which are all\npredominantly (âˆ¼95%) trained on synthesized AMASS data (Appen-\ndix A). Additionally, we hold out a synthetic IMU dataset, DanceDB,\nto evaluate cross-motion-type generalization.\nâ€¢DIPEval (real heldout) : Data from two held-out subjects in\nthe ten-subject DIP dataset.\nâ€¢TotalCapture (real heldout): We held out real IMU measure-\nments from the TotalCapture dataset [Trumble et al. 2017]\nfor evaluation, but still use its ground truth and synthesized\nIMU readings as part of the AMASS training set, following\nthe same practice as previous works.\nâ€¢DanceDB (synthetic heldout) : A large dataset of contempo-\nrary dances, therefore containing unique motion types to\nany other training dataset. Note that DanceDB is part of\nAMASS but we intentionally hold it out from our training\ndata. Previous works likely did not include them in training\neither since they predate DanceDBâ€™s release in AMASS.\nMetrics. We use the following metrics common for evaluating\nmotion reconstruction quality. We randomly sample 600 consec-\nutive frames (10s) from each motion in the evaluation datasets,\nto prevent very long motions biasing the statistics, and to avoid\nevaluation of root translation error in the beginning of motions\nwhere they mostly start from a stationary standing pose.\nâ€¢Mean Joint Angle Error (in degrees) : Joint angle (repre-\nsented in axis-angles) difference between reconstruction and\nground-truth, averaged over all joints.\nâ€¢Mean Root-Relative Joint Position Error (in centime-\nters): Global joint Cartesian position difference (Euclidean\nnorm) between the reconstruction and ground-truth by align-\ning at the root, averaged over all joints.\nâ€¢Root Error 2s/5s/10s (in meters) : Root translation error\nmeasured in Euclidean norm during a continuous period of\n2s/5s/10s. Note that existing works have no errors from the\nvertical axis as they are designed for flat-ground motions.\nâ€¢Mean Joint Position Jitter (in ğ‘š/ğ‘ 3): Joint position jitter\ncomputed using the same formula as in TransPose, averaged\nover all joints.\nâ€¢Root Jitter (in ğ‘š/ğ‘ 3): Root position jitter computed using\nthe same formula as above.\nResults. We present quantitative metrics on the evaluation datasets\nbetween our model, TransPose, and DIP in Table 1. We used the best\nperforming models published by the authors in this comparison.\nOverall, our system achieves better accuracy in almost all evalua-\ntions. Our system does not have an offline mode, and all reported\nmetrics reported are from online inference (i.e. pretending existing\ndata files as streaming in).\nWe see most significant improvements from previous works on\nroot translation, on both TotalCapure and DanceDB datasets. Sec.\n4.3 shows the effects of our key design choices on root drifting\nmitigation. Thanks to our prepossessing filtering (Appendix A), we\ncan treat the DIP training split as ordinary âˆ¼5% subset of the whole\ntraining data with the rest âˆ¼95% being purely synthetic IMU. By\nusing a simple one-stage training procedure, instead of the two-\nstage training-then-finetuning as in previous works, we reduce the\nrisk of overfitting to activities in the DIP dataset during finetuning.\nComparing to results from concurrent PIP work [Yi et al. 2022],\nour method achieves a similar level of improvement over TransPose.\nOn flat terrain activities, our work complements the PIP model\nby exploring drastically different techniques. We are not able to\ncompare with this new work quantitatively as the code and model\nhave not been released yet.\n4.2 Quality of Simultaneously Generated\nTerrains\nSince our reconstructed terrains are only oneplausible result among\ninfinite possibilities, we perform the following two qualitative ex-\nperiments to evaluate their consistency with the human motion:\nâ€¢Simulation: we evaluate our terrain generation algorithm\nby running our system on terrain navigation motions in-\ncluded in the AMASS synthesized IMU training data. (Video\n1m10s, note AMASS does not provide terrain ground-truths.)\nâ€¢Live Demos : though our model has never seen any real\nIMU signals on terrains, we push its limit and showcase\nsimultaneous motion reconstruction and terrain generation\nfor walking on stairs (Video 1m56s), and climbing/jumping\noff chairs (Video 2m23s).\nThe supplementary video demonstrates plausible terrains both\nin live demos and on the AMASS dataset. Figure 5 presents one\nof the generated terrains from simulation. Although we do not\nhave access to the ground truth terrain, our Voronoi-graph based\nalgorithm generates plausible staircases that are consistent with the\nhuman motion. Body parts may still penetrate the terrain briefly\nTable 1: Comparison of model performance on evaluation\ndatasets. Bold numbers indicate the best performing entries.\nSA â€™22 Conference Papers, December 6â€“9, 2022, Daegu, Republic of Korea Yifeng Jiang, Yuting Ye, Deepak Gopinath, Jungdam Won, Alexander W. Winkler, and C. Karen Liu\nserves to test the sim-to-real transfer of the models, which are all\npredominantly (âˆ¼95%) trained on synthesized AMASS data (Appen-\ndix A). Additionally, we hold out a synthetic IMU dataset, DanceDB,\nto evaluate cross-motion-type generalization.\nâ€¢DIPEval (real heldout) : Data from two held-out subjects in\nthe ten-subject DIP dataset.\nâ€¢TotalCapture (real heldout) : We held out real IMU measure-\nments from the TotalCapture dataset [Trumble et al. 2017]\nfor evaluation, but still use its ground truth and synthesized\nIMU readings as part of the AMASS training set, following\nthe same practice as previous works.\nâ€¢DanceDB (synthetic heldout) : A large dataset of contempo-\nrary dances, therefore containing unique motion types to\nany other training dataset. Note that DanceDB is part of\nAMASS but we intentionally hold it out from our training\ndata. Previous works likely did not include them in training\neither since they predate DanceDBâ€™s release in AMASS.\nMetrics. We use the following metrics common for evaluating\nmotion reconstruction quality. We randomly sample 600 consec-\nutive frames (10s) from each motion in the evaluation datasets,\nto prevent very long motions biasing the statistics, and to avoid\nevaluation of root translation error in the beginning of motions\nwhere they mostly start from a stationary standing pose.\nâ€¢Mean Joint Angle Error (in degrees) : Joint angle (repre-\nsented in axis-angles) difference between reconstruction and\nground-truth, averaged over all joints.\nâ€¢Mean Root-Relative Joint Position Error (in centimeters) :\nGlobal joint Cartesian position difference (Euclidean norm)\nbetween the reconstruction and ground-truth by aligning at\nthe root, averaged over all joints.\nâ€¢Root Error 2s/5s/10s (in meters) : Root translation error\nmeasured in Euclidean norm during a continuous period of\n2s/5s/10s. Note that existing works have no errors from the\nvertical axis as they are designed for flat-ground motions.\nâ€¢Mean Joint Position Jitter (in ğ‘š/ğ‘ 3): Joint position jitter\ncomputed using the same formula as in TransPose, averaged\nover all joints.\nâ€¢Root Jitter (in ğ‘š/ğ‘ 3): Root position jitter computed using\nthe same formula as above.\nResults. We present quantitative metrics on the evaluation datasets\nbetween our model, TransPose, and DIP in Table 1. We used the best\nperforming models published by the authors in this comparison.\nOverall, our system achieves better accuracy in almost all evalua-\ntions. Our system does not have an offline mode, and all reported\nmetrics reported are from online inference (i.e. pretending existing\ndata files as streaming in).\nWe see most significant improvements from previous works on\nroot translation, on both TotalCapure and DanceDB datasets. Sec.\n4.3 shows the effects of our key design choices on root drifting\nmitigation. Thanks to our prepossessing filtering (Appendix A), we\ncan treat the DIP training split as ordinary âˆ¼5% subset of the whole\ntraining data with the rest âˆ¼95% being purely synthetic IMU. By\nusing a simple one-stage training procedure, instead of the two-\nstage training-then-finetuning as in previous works, we reduce the\nrisk of overfitting to activities in the DIP dataset during finetuning.\nComparing to results from concurrent PIP work [Yi et al. 2022],\nour method achieves a similar level of improvement over TransPose.\nOn flat terrain activities, our work complements the PIP model\nby exploring drastically different techniques. We are not able to\ncompare with this new work quantitatively as the code and model\nhave not been released yet.\n4.2 Quality of Simultaneously Generated\nTerrains\nSince our reconstructed terrains are only oneplausible result among\ninfinite possibilities, we perform the following two qualitative ex-\nperiments to evaluate their consistency with the human motion:\nâ€¢Simulation: we evaluate our terrain generation algorithm\nby running our system on terrain navigation motions in-\ncluded in the AMASS synthesized IMU training data. (Video\n1m10s, note AMASS does not provide terrain ground-truths.)\nâ€¢Live Demos : though our model has never seen any real\nIMU signals on terrains, we push its limit and showcase\nsimultaneous motion reconstruction and terrain generation\nfor walking on stairs (Video 1m56s), and climbing/jumping\noff chairs (Video 2m23s).\nThe supplementary video demonstrates plausible terrains both\nin live demos and on the AMASS dataset. Figure 5 presents one\nof the generated terrains from simulation. Although we do not\nhave access to the ground truth terrain, our Voronoi-graph based\nalgorithm generates plausible staircases that are consistent with the\nTable 1: Comparison of model performance on evaluation\ndatasets. Bold numbers indicate the best performing entries.\nOur TIP Model\nDIPEval TotalCapture DanceDB\njoint angle errors (degree) 12.09586 8.91642 15.57031\njoint position errors (ğ‘ğ‘š) 5.82242 5.14566 8.50089\nroot errors in 2s (meter) 0.08031 0.20295\nroot errors in 5s (meter) 0.1351 0.29681\nroot errors in 10s (meter) 0.19446 0.35759\njoint position jitter (ğ‘š/ğ‘ 3) 0.8823 0.75075 1.43867\nroot jitter (ğ‘š/ğ‘ 3) 0.66211 0.61108 0.98474\nTransPose Model\nDIPEval TotalCapture DanceDB\njoint angle errors (degree) 13.10898 11.37459 17.42089\njoint position errors (ğ‘ğ‘š) 6.28771 5.60905 8.46368\nroot errors in 2s (meter) 0.16928 0.20812\nroot errors in 5s (meter) 0.32677 0.31241\nroot errors in 10s (meter) 0.35323 0.46058\njoint position jitter (ğ‘š/ğ‘ 3) 0.63113 0.66816 1.49544\nroot jitter (ğ‘š/ğ‘ 3) 0.53972 0.63572 1.31713\nDIP Model\nDIPEval TotalCapture DanceDB\njoint angle errors (degree) 14.14957 16.54206 23.27462\njoint position errors (ğ‘ğ‘š) 7.79099 9.36553 13.83818\njoint position jitter (ğ‘š/ğ‘ 3) 1.85047 1.74527 2.15585\nTransformer Inertial Poser SA â€™22 Conference Papers, December 6â€“9, 2022, Daegu, Republic of Korea\nTable 2: Comparison of ablation models on TotalCapture evaluation dataset.\nNo history, w/ SBP No SBPs Predict SBPs wo/ usage wo/ terrain corrections TIP\nroot errors in 2s 0.15833 0.16987 0.13927 0.07817 0.08031\nroot errors in 5s 0.28575 0.32543 0.32972 0.15462 0.13509\nroot errors in 10s 0.41114 0.5051 0.51706 0.24196 0.19445\nwhile it is being generated (Figure 5, second to the left), because\nthe algorithm ignores each newly active SBP for a short period of\ntime (Appendix C). Final terrain should see little penetration when\nplaying back the motion.\nFigure 5: Generated terrain. Many equally plausible solu-\ntions exist for same motion, the solution from our algorithm\nbeing one of them.\n4.3 Ablation Studies on Root Drifting\nMultiple key design choices improved our root translation accuracy,\nwhich is also crucial to the success of terrain generation. To gain a\nbetter understanding, we created the following ablation models:\nâ€¢No history, with SBP : a Transformer encoder can be used\nas a sequence summarizing model, taking time windows of\nIMU signals and output only the current full-body pose ğ’’ğ‘¡,\nğ’—ğ‘¡ and SBPs ğ’„ğ‘¡, with no consideration of its past predictions.\nWe still used the SBPs to correct the root in this case.\nâ€¢No SBPs: same network as Ours without learning SBPs.\nâ€¢Predict SBPs without Usage : no test-time usage of the pre-\ndicted SBPs. This is different from \"No SBPs\" since learning\nan unused auxiliary task may help the main task learning.\nâ€¢Without Terrain Corrections : Do not use terrain algo-\nrithm for vertical drift correction.\nTable. 2 summarizes the modelsâ€™ performances on the real-IMU\nTotalCapture dataset. Both \"no SBPs\" and \"predicting SBPs without\nusage\" rely on the modelâ€™s raw prediction of ğ’—ğ‘¡, and perform the\nworst in this test, showing the importance of both learning the\nSBPs and enforcing them at run time. \"No history, with SBP\" also\nfails, interestingly because the SBPs predicted by a Transformer\nencoder becomes intermittent in this case and therefore less helpful\nduring run time. This shows the importance of explicitly including\noutput history as input. Intuitively, the onset history of SBPs are\nimportant information for next SBP onset prediction.\nFinally, without terrain-based root drifting correction, in some\ntesting motions, vertical drift may be large for reasons such as bi-\nased sensor error from calibrations. Such errors in vertical direction\nalso negatively affects generated terrains if not corrected in time.\n4.4 Qualitative Comparisons of Run-time Joint\nIK Correction\nIntroduced in Sec. 3.2, we use IK to correct joint prediction histories\nwith a pair of active SBPs. We showcase its usage in one of the most\ncommon joint drifting scenarios of long sitting [Yi et al. 2022, 2021],\nwhere the character may easily transition to standing over an ex-\ntended period of time due to similar orientations and accelerations\non legs and waist IMUs. With our SBP IK correction, we can sta-\nbly reconstruct a two-minute sitting sequence, both with our own\nsensors (Video 0m42s) or using public real-IMU sequences (Video\n3m10s) in DIP dataset, showing the value of correcting history\nbuffer with IK to be consistent with predicted SBPs.\n4.5 Live Demo\nWe test our system live with 6 Xsens IMU sensors. Our video visu-\nalizes live performance side-by-side with real-time reconstructions,\nwith a slight latency caused by our pre-processing filter. Besides\nthe aforementioned tasks, we cover a variety of motion tasks in\nour demos, both common ones such as locomotion and whole-body\nmanipulation, and more challenging ones such as jumping from a\nhigh place, \"swimming\" on a stool, dancing, hand on floor move-\nments, or swirl kicks. We tested our system on one male and one\nfemale subjects, and observed degraded performance on the female\nsubject which we did not expect (Appendix F).\n5 CONCLUSION\nThis paper presents a new data-driven method for human motion\nreconstruction from six wearable IMUs, with simultaneous plau-\nsible terrain generation. By combining a conditional Transformer\ndecoder model for consistent prediction, a hybrid drift stabilizer\nutilizing learned stationary information across human body, and an\nalgorithm to simultaneously generate regularized terrain and cor-\nrect noisy global motion estimation, new downstream applications\ncan be made possible with this self-contained and economic setup\nof motion capture. For future work, our method could be largely\nimproved by collecting real IMU datasets with motions on various\nterrains. Personalized finetuning or calibration may also improve\nreconstruction for each individual user.\nACKNOWLEDGMENTS\nTo members of the Stanford Movement Lab, the Stanford Human\nPerformance Lab, Meta Reality Labs Research, and to Josh Coo-\nley, Yinghao Huang, Manuel Kaufmann, Ari Tamari, Xia Wu, Di\nXia, Xinyu Yi, Eris Zhang, for helpful discussions and technical\nassistance. To anonymous reviewers whose feedback substantially\nhelped refine this work. Yifeng Jiang is partially supported by the\nWu Tsai Human Performance Alliance at Stanford University.\nSA â€™22 Conference Papers, December 6â€“9, 2022, Daegu, Republic of Korea Yifeng Jiang, Yuting Ye, Deepak Gopinath, Jungdam Won, Alexander W. Winkler, and C. Karen Liu\nREFERENCES\nEmre Aksan, Manuel Kaufmann, Peng Cao, and Otmar Hilliges. 2021. A Spatio-temporal\nTransformer for 3D Human Motion Prediction. International Conference on 3D\nVision (3DV) (2021).\nSheldon Andrews, Ivan Huerta, Taku Komura, Leonid Sigal, and Kenny Mitchell. 2016.\nReal-Time Physics-Based Motion Capture with Sparse Sensors. In Proceedings of\nthe 13th European Conference on Visual Media Production (CVMP 2016) (CVMP 2016) .\nArticle 5.\nEric R. Bachmann, Robert B. McGhee, Xiaoping Yun, and Michael J. Zyda. 2001. Inertial\nand Magnetic Posture Tracking for Inserting Humans into Networked Virtual\nEnvironments. In Proceedings of the ACM Symposium on Virtual Reality Software\nand Technology (VRST â€™01) . 9â€“16.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Pra-\nfulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\nSandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christo-\npher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. 2020. Language Models are Few-Shot Learners. (2020).\narXiv:2005.14165 [cs.CL]\nZ. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A. Sheikh. 2019. OpenPose: Real-\ntime Multi-Person 2D Pose Estimation using Part Affinity Fields. IEEE Transactions\non Pattern Analysis and Machine Intelligence (2019).\nYoung-Woon Cha, Husam Shaik, Qian Zhang, Fan Feng, Andrei State, Adrian Ilie, and\nHenry Fuchs. 2021. Mobile. Egocentric Human Body Motion Reconstruction Using\nOnly Eyeglasses-mounted Cameras and a Few Body-worn Inertial Sensors. In 2021\nIEEE Virtual Reality and 3D User Interfaces (VR) .\nAdrian Hilton Charles Malleson, John Collomosse. 2020. Real-Time Multi-person\nMotion Capture from Multi-view Video and IMUs.International Journal of Computer\nVision 128 (06 2020).\nVasileios Choutas, Federica Bogo, Jingjing Shen, and Julien Valentin. 2021. Learning\nto Fit Morphable Models. CoRR abs/2111.14824 (2021). arXiv:2111.14824 https:\n//arxiv.org/abs/2111.14824\nErwin Coumans and Yunfei Bai. 2016. Pybullet, a python module for physics simulation\nfor games, robotics and machine learning. (2016).\nMichael B. Del Rosario, Heba Khamis, Phillip Ngo, Nigel H. Lovell, and Stephen J.\nRedmond. 2018. Computationally Efficient Adaptive Error-State Kalman Filter for\nAttitude Estimation. IEEE Sensors Journal 18, 22 (2018), 9332â€“9342.\nPrafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Rad-\nford, and Ilya Sutskever. 2020. Jukebox: A Generative Model for Music.\narXiv:2005.00341 [eess.AS]\nAndrea Dittadi, Sebastian Dziadzio, Darren Cosker, Ben Lundell, Tom Cashman, and\nJamie Shotton. 2021. Full-Body Motion From a Single Head-Mounted Device:\nGenerating SMPL Poses From Partial Observations. In International Conference on\nComputer Vision 2021 .\nH. Durrant-Whyte and T. Bailey. 2006. Simultaneous localization and mapping: part I.\nIEEE Robotics Automation Magazine 13, 2 (2006), 99â€“110. https://doi.org/10.1109/\nMRA.2006.1638022\nE. Foxlin. 1996. Inertial head-tracker sensor fusion by a complementary separate-bias\nKalman filter. In Proceedings of the IEEE 1996 Virtual Reality Annual International\nSymposium. 185â€“194.\nAndrew Gilbert, Matthew Trumble, Charles Malleson, Adrian Hilton, and John Col-\nlomosse. 2019. Fusing Visual and Inertial Sensors with Semantics for 3D Human\nPose Estimation. International Journal of Computer Vision 127 (04 2019), 1â€“17.\nRÄ±za Alp GÃ¼ler, Natalia Neverova, and Iasonas Kokkinos. 2018. Densepose: Dense hu-\nman pose estimation in the wild. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition . 7297â€“7306.\nVladimir Guzov, Aymen Mir, Torsten Sattler, and Gerard Pons-Moll. 2021. Human\nPOSEitioning System (HPS): 3D Human Pose Estimation and Self-localization in\nLarge Scenes from Body-Mounted Sensors. In CVPR.\nThomas Helten, Meinard MÃ¼ller, Hans-Peter Seidel, and Christian Theobalt. 2013.\nReal-Time Body Tracking with One Depth Camera and Inertial Sensors. In 2013\nIEEE International Conference on Computer Vision . 1105â€“1112.\nDaniel Holden, Oussama Kanoun, Maksym Perepichka, and Tiberiu Popa. 2020. Learned\nMotion Matching. ACM Trans. Graph. 39, 4, Article 53 (jul 2020), 13 pages. https:\n//doi.org/10.1145/3386569.3392440\nYinghao Huang, Manuel Kaufmann, Emre Aksan, Michael J. Black, Otmar Hilliges,\nand Gerard Pons-Moll. 2018. Deep Inertial Poser: Learning to Reconstruct Human\nPose from Sparse Inertial Measurements in Real Time. ACM TOG 37, 6 (12 2018).\nRudolph Emil Kalman et al. 1960. A new approach to linear filtering and prediction\nproblems [J]. Journal of basic Engineering 82, 1 (1960), 35â€“45.\nAngjoo Kanazawa, Jason Y. Zhang, Panna Felsen, and Jitendra Malik. 2019. Learning 3D\nHuman Dynamics from Video. In Computer Vision and Pattern Recognition (CVPR) .\nManuel Kaufmann, Yi Zhao, Chengcheng Tang, Lingling Tao, Christopher Twigg,\nJie Song, Robert Wang, and Otmar Hilliges. 2021. EM-POSE: 3D Human Pose\nEstimation from Sparse Electromagnetic Trackers. In International Conference on\nComputer Vision (ICCV) .\nSeong Uk Kim, Hanyoung Jang, Hyeonseung Im, and Jongmin Kim. 2021. Human\nmotion reconstruction using deep transformer networks.Pattern Recognition Letters\n150 (2021), 162â€“169.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization.\narXiv preprint arXiv:1412.6980 (2014).\nBenoÃ®t Le Callennec and Ronan Boulic. 2006. Robust kinematic constraint detection for\nmotion data. In Proceedings of the 2006 ACM SIGGRAPH/Eurographics symposium\non Computer animation . 281â€“290.\nRuilong Li, Shan Yang, David A. Ross, and Angjoo Kanazawa. 2021. AI Choreographer:\nMusic Conditioned 3D Dance Generation with AIST++.\nHuajun Liu, Xiaolin Wei, Jinxiang Chai, Inwoo Ha, and Taehyun Rhee. 2011. Realtime\nHuman Motion Control with a Small Number of Inertial Sensors. In Symposium on\nInteractive 3D Graphics and Games (I3D â€™11) . 133â€“140.\nMatthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J.\nBlack. 2015. SMPL: A Skinned Multi-Person Linear Model. ACM TOG 34, 6 (Oct.\n2015), 248:1â€“248:16.\nIlya Loshchilov and Frank Hutter. 2016. Sgdr: Stochastic gradient descent with warm\nrestarts. arXiv preprint arXiv:1608.03983 (2016).\nZhengyi Luo, Ryo Hachiuma, Ye Yuan, and Kris Kitani. 2021. Dynamics-Regulated\nKinematic Policy for Egocentric Pose Estimation. In NeurIPS.\nNaureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, and Michael J.\nBlack. 2019. AMASS: Archive of Motion Capture as Surface Shapes. In ICCV. 5442â€“\n5451.\nCharles Malleson, Marco Volino, Andrew Gilbert, Matthew Trumble, John Collomosse,\nand Adrian Hilton. 2017. Real-time Full-Body Motion Capture from Video and\nIMUs. In Int. Conf. 3D Vis.\nDeepak Nagaraj, Erik Schake, Patrick Leiner, and Dirk Werth. 2020. An RNN-Ensemble\nApproach for Real Time Human Pose Estimation from Sparse IMUs. In Proceedings\nof the 3rd International Conference on Applications of Intelligent Systems (Las Palmas\nde Gran Canaria, Spain) (APPIS 2020) . Article 32, 6 pages.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019.\nPytorch: An imperative style, high-performance deep learning library. Advances in\nneural information processing systems 32 (2019), 8026â€“8037.\nMathis Petrovich, Michael J. Black, and GÃ¼l Varol. 2021. Action-Conditioned 3D Human\nMotion Synthesis with Transformer VAE. In International Conference on Computer\nVision (ICCV) . 10985â€“10995.\nGerard Pons-Moll, Andreas Baak, Juergen Gall, Laura Leal-TaixÃ©, Meinard MÃ¼ller, Hans-\nPeter Seidel, and Bodo Rosenhahn. 2011. Outdoor human motion capture using\ninverse kinematics and von mises-fisher sampling. In 2011 International Conference\non Computer Vision . 1243â€“1250.\nGerard Pons-Moll, Andreas Baak, Thomas Helten, Meinard MÃ¼ller, Hans-Peter Seidel,\nand Bodo Rosenhahn. 2010. Multisensor-fusion for 3D full-body human motion\ncapture. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern\nRecognition. 663â€“670.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving\nlanguage understanding by generative pre-training. (2018).\nDavis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, and\nLeonidas J Guibas. 2021. Humor: 3d human motion model for robust pose es-\ntimation. In Proceedings of the IEEE/CVF International Conference on Computer\nVision. 11488â€“11499.\nRokoko. n d. Rokoko https://www.rokoko.com/. Last visited: 08/26/2022.\nSoshi Shimada, Vladislav Golyanik, Weipeng Xu, and Christian Theobalt. 2020.\nPhysCap: Physically Plausible Monocular 3D Motion Capture in Real Time. ACM\nTOG 39, 6 (12 2020).\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan\nSalakhutdinov. 2014. Dropout: A Simple Way to Prevent Neural Networks from\nOverfitting. The Journal of Machine Learning Research 15, 1 (2014), 1929â€“1958.\nhttp://dl.acm.org/citation.cfm?id=2627435.2670313\nJonathan Taylor, Jamie Shotton, Toby Sharp, and Andrew Fitzgibbon. 2012. The\nVitruvian manifold: Inferring dense correspondences for one-shot human pose\nestimation. In 2012 IEEE Conference on Computer Vision and Pattern Recognition .\n103â€“110.\nMatt Trumble, Andrew Gilbert, Charles Malleson, Adrian Hilton, and John Collomosse.\n2017. Total Capture: 3D Human Pose Estimation Fusing Video and Inertial Sensors.\nIn BMVC.\nGuillermo Valle-PÃ©rez, Gustav Eje Henter, Jonas Beskow, Andre Holzapfel, Pierre-Yves\nOudeyer, and Simon Alexanderson. 2021. Transflower: Probabilistic Autoregressive\nDance Generation with Multimodal Attention. ACM Trans. Graph. 40, 6, Article\n195 (dec 2021), 14 pages. https://doi.org/10.1145/3478513.3480570\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, Å ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In\nAdvances in Neural Information Processing Systems , Vol. 30.\nVicon. n d. Vicon Motion Systems https://www.vicon.com/. Last visited: 08/26/2022.\nRachel V. Vitali, Ryan S. McGinnis, and Noel C. Perkins. 2021. Robust Error-State\nKalman Filter for Estimating IMU Orientation. IEEE Sensors Journal 21, 3 (2021),\n3561â€“3569.\nDaniel Vlasic, Rolf Adelsberger, Giovanni Vannucci, John Barnwell, Markus Gross,\nWojciech Matusik, and Jovan PopoviÄ‡. 2007. Practical Motion Capture in Everyday\nTransformer Inertial Poser SA â€™22 Conference Papers, December 6â€“9, 2022, Daegu, Republic of Korea\nSurroundings. ACM Trans. Graph. 26, 3 (2007).\nTimo von Marcard, Roberto Henschel, Michael Black, Bodo Rosenhahn, and Gerard\nPons-Moll. 2018. Recovering Accurate 3D Human Pose in The Wild Using IMUs\nand a Moving Camera. In European Conference on Computer Vision (ECCV) .\nTimo von Marcard, Gerard Pons-Moll, and Bodo Rosenhahn. 2016. Human Pose\nEstimation from Video and IMUs. Transactions on Pattern Analysis and Machine\nIntelligence (PAMI) (jan 2016).\nTimo von Marcard, Bodo Rosenhahn, Michael Black, and Gerard Pons-Moll. 2017.\nSparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse IMUs.\nComputer Graphics Forum 36(2), Proceedings of the 38th Annual Conference of the\nEuropean Association for Computer Graphics (Eurographics) (2017), 349â€“360.\nXiaolin Wei, Peizhao Zhang, and Jinxiang Chai. 2012. Accurate Realtime Full-Body\nMotion Capture Using a Single Depth Camera. ACM Trans. Graph. 31, 6, Article\n188 (nov 2012).\nXsens. n d. Xsens https://www.xsens.com/. Last visited: 08/26/2022.\nDongseok Yang, Doyeon Kim, and Sung-Hee Lee. 2021. LoBSTr: Real-time Lower-body\nPose Prediction from Sparse Upper-body Tracking Signals. Computer Graphics\nForum (2021). https://doi.org/10.1111/cgf.142631\nXinyu Yi, Yuxiao Zhou, Marc Habermann, Soshi Shimada, Vladislav Golyanik, Christian\nTheobalt, and Feng Xu. 2022. Physical Inertial Poser (PIP): Physics-aware Real-time\nHuman Motion Tracking from Sparse Inertial Sensors. In IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) .\nXinyu Yi, Yuxiao Zhou, and Feng Xu. 2021. TransPose: Real-time 3D Human Translation\nand Pose Estimation with Six Inertial Sensors. ACM TOG 40, 4 (8 2021).\nZhe Zhang, Chunyu Wang, Wenhu Qin, and Wenjun Zeng. 2020. Fusing Wearable\nIMUs with Multi-View Images for Human Pose Estimation: A Geometric Approach.\nIn CVPR.\nZerong Zheng, Tao Yu, Hao Li, Kaiwen Guo, Qionghai Dai, Lu Fang, and Yebin Liu.\n2018. HybridFusion: Real-Time Performance Capture Using a Single Depth Sensor\nand Sparse IMUs. In European Conference on Computer Vision (ECCV) , Vittorio\nFerrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (Eds.). 389â€“406.\nYi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. 2019. On the continuity\nof rotation representations in neural networks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition . 5745â€“5753.\nSA â€™22 Conference Papers, December 6â€“9, 2022, Daegu, Republic of Korea Yifeng Jiang, Yuting Ye, Deepak Gopinath, Jungdam Won, Alexander W. Winkler, and C. Karen Liu\nA ACCELERATION READINGS AND\nSIM-TO-REAL\nMoving Average Filtering. Since real IMU data paired with ground-\ntruth full-body motions are small in size, following previous work\n[Huang et al. 2018], we place virtual IMU sensors on virtual charac-\nters driven by captured motions to synthesize IMU orientation and\nacceleration readings. Using the AMASS [Mahmood et al . 2019]\nmotion dataset (a collection of smaller motion capture datasets), we\ncreate a large-scale synthetic IMU dataset for training our model.\nHowever, synthetic and real IMU data exhibit vastly different\nnoise profiles. Acceleration data in the real dataset are noisy, but\nnot in the same way as the noise in the synthetic dataset, which\nis caused by double differentiation of mocap data (Figure 6 Top).\nOn the other hand, orientation data are usually less noisy because\nthey are processed by the in-sensor Kalman filter [Kalman et al .\n1960]. Previous work [Huang et al. 2018; Yi et al. 2021] recognized\nthis distribution mismatch problem and proposed to first train the\nmodel exclusively on the synthetic data and then finetune it on a\nsmaller real dataset. This two-step solution leads to a more complex\ntraining procedure that requires careful tuning to avoid overfitting\nthe real dataset.\nFigure 6: Example of synthesized (orange) and real (blue) ac-\nceleration data, before (top) and after (bottom) moving aver-\nage filtering.\nWe found that simply running an average filter on both synthetic\nand real acceleration data (with window length of 11 in our imple-\nmentation) would bring the two data sources sufficiently close to\neach other (Fig. 6 Bottom). We then train the model only once on the\ncombined dataset. Combining both data sources simplifies training\nfrom two stages to one stage, and avoids the risk of catastrophic\nforgetting during finetuning.\nIn practice, filtering causes latency during real-time inference,\nas computing moving average requires future IMU readings. We\nuse 5 times steps (83ms) of future readings, the same requirement\nas [Huang et al. 2018; Yi et al. 2021], though they require future\nreadings as part of model input while we merely use them for\nfiltering.\nSumming Up (Integrate) Past Accelerations. Another issue we dis-\ncover for non-flat terrain motions is that the sensor readings (both\norientation and acceleration) during a stair step is much similar to\na normal flat-ground step, especially in the case of real, noisy IMU\ndata. Note that pelvis also accelerates up and down in a normal\nwalking step resembling an inverted pendulum. However, if we\nsum up the raw IMU acceleration readings within a small window\nof recent history (e.g. past 0.5s), similar to \"integrating\" accelera-\ntion to delta velocities, we could observe a more different signal\nshape between stairs and normal steps. Empirically, we find adding\nthis additional history sum features for each channel, increasing\nacceleration features from R18 to R36 (concatenating with filtered\naccelerations), improves stair recognition on real hardware.\nB MODEL DETAILS\nWe use the AMASS dataset to generate synthetic training data fol-\nlowing the smoothing procedure in Appendix A. It consists of over\na dozen different motion capture datasets performing a variety of\nactivities. In addition, we include 8 out of 10 subjectsâ€™ data from the\nDIP dataset. We use pyBullet [Coumans and Bai 2016] for calculat-\ning forward kinematics during data synthesis, SBP label generation,\nroot correction, and final visualizations. As the DIP real IMU data\ndo not have root motion, we use a pre-trained model to label pseudo\nground-truth SBPs for the DIP motions.\nWe use standard loss functions for the model outputs, i.e., mean-\nsquared error for joint rotations, mean-squared error for ğ’—ğ‘¡ and\nCartesian elements of ğ’„ğ‘¡, and binary cross-entropy for binary ele-\nments of ğ’„ğ‘¡, (i.e. ğ‘ğ‘¡). Specifically, for joint rotations,\nLğ½ = âˆ¥ğ’’ğ‘¡ âˆ’Â¯ğ’’ğ‘¡âˆ¥2\n2 ,\nwhere Â¯ğ’’ğ‘¡ is the ground-truth full-body joint rotations, represented\nas first two columns (6D) of each rotation matrix as noted previously\nin the main text. Similarly for root velocities,\nLğ‘… = âˆ¥ğ’—ğ‘¡ âˆ’Â¯ğ’—ğ‘¡âˆ¥2\n2 ,\nand for the SBP predictions ğ’„ğ‘¡,\nLğ¶ =\n5âˆ‘ï¸\nğ‘–=1\n\r\r\rğ’“ (ğ‘–)\nğ‘¡ âˆ’Â¯ğ’“ (ğ‘–)\nğ‘¡\n\r\r\r\n2\n2\n+\n5âˆ‘ï¸\nğ‘–=1\n(âˆ’Â¯ğ‘(ğ‘–)\nğ‘¡ logğ‘(ğ‘–)\nğ‘¡ âˆ’(1âˆ’Â¯ğ‘(ğ‘–)\nğ‘¡ )log(1âˆ’ğ‘(ğ‘–)\nğ‘¡ )).\nSince our model during training time predicts a whole trajectory\nwindow, we experimented with a jerk loss penalizing deviation of\nneighboring frames, but it did not produce visible improvements.\nThis might be due to the fact that during test time we still only\nuse the last prediction at each step. Instead, we pass our output\nthrough an exponential moving average filter as post processing,\nat the expense of slight increase in joint accuracy errors (Appendix\nF).\nOur model is trained in PyTorch [Paszke et al. 2019] using the\nAdam optimizer [Kingma and Ba 2014], with a batch size of 256\nand a learning rate of 0.0001 multiplied with a cosine schedule\n[Loshchilov and Hutter 2016]. We perform training for 1000 epochs,\nwhich takes around 6 hours with a GeForce GTX 2080Ti GPU. Once\ntrained, our model is small enough to run at 60 fps on a 2080Ti\nmachine, with bottleneck being the python wrapper of pyBullet.\nOur model uses max window sizeğ‘€ = 39. It contains a total number\nof 3,677,315 parameters, comparing to 4,798,771 in TransPose and\n10,801,934 in DIP.\nNote that our model requires an initial full-body pose given in\nthe first step of prediction. In practice this is always the case since\nthe sensors need to be calibrated with a T pose before each use, as\nthey are allowed to be slightly differently worn. See Appendix D\nfor more details.\nTransformer Inertial Poser SA â€™22 Conference Papers, December 6â€“9, 2022, Daegu, Republic of Korea\nC TERRAIN GENERATION DETAILS\nGrid size is empirically set to 0.1m, and number of gridsğ¿Ã—ğ¿is set\nso that terrain is large enough. We initialize the height mapğ‘¯ with\nall zero values, where zero is set to the initial root height minus a\nconstant height ğ‘¤. ğ‘¤ represents the lower bound of how low the\nsubject could possibly reach in this capture. We assume no cluster\nmeans are below this value. If the user can provide a tighter ğ‘¤ (e.g.\nstarting the capture on the lowest ground plane), we can generate\na more visually pleasing terrain by producing no dents lower than\nthe specified ground plane. During real-time demos, w is set to be\ntight, to indicate that we know the motion will not go lower than\nthe starting ground plane.\nIf there were only two SBPs with different heights, Voronoi dia-\ngram will render a 1-step stair that is infinitely wide. For aesthetics,\nwe limit the area each new SBP can influence to 1m Ã—1m, which\ncould be nevertheless still wider than the real stairs in scene. While\nwe can arbitrarily make this influence region narrower, we note\nthat, without additional information, both are equally plausible,\nand new SBPs can always crop the terrain narrower with more\ninformation streaming in (Video 1m40s).\nFor a newly active SBP, we ignore it for ğ‘¡0 seconds before using\nit for the terrain algorithm, to allow it to settle in height. ğ‘¡0 is 50\nframes or the the moment SBP becomes inactive, whichever comes\nearlier. The pelvis SBP is only used in terrain generation if it is\n> 0.2m away from the feet, to avoid building terrains at the pelvis\nheight when the subject is standing still.\nD SENSOR CALIBRATION\nWhen testing on real hardware, as the raw sensor readings are\nin different coordinate frames from the frame of system input,\ncalibration is needed to obtain the offset transforms between the\ncoordinate frames beforehand. We adopt a slightly different IMU\ncalibration procedure from previous works that is nevertheless still\nstraightforward to explain.\nWe start from defining a few coordinate frames. Letğºğ‘› be the\nbase (i.e. identity) frame of each of the 6 sensors (for the Xsens\nsensors we used, identity orientation could mean different poses\nper sensor). Let ğºğ‘ be any fixed global frame the user specifies,\nwhose ğ‘¥ axis indicates the specified front, ğ‘¦ axis corresponds to\nthe left, and ğ‘§ axis corresponds to the upwards. (Note this axis\ndefinition is different from DIP and TransPose models.) Let ğ‘†ğ‘¡ be\nthe sensor frame, while ğ‘†0 defines the sensor frame during T-pose\ncalibration. Let ğµğ‘¡ be the bone frame, while ğµ0 defines the bone\nframe during T-pose calibration. We omit the sensor indices ğ‘— (e.g.\nğº(ğ‘—)\nğ‘› , ğ‘†(ğ‘—)\nğ‘¡ ) since calibration is agnostic to each sensor.\nUsing these notations,ğ‘¹ğ‘†ğ‘¡\nğºğ‘›\nrepresents the raw sensor orientation\nreading based from frame ğºğ‘›, and ğ’‚ğ‘†ğ‘¡ represents the raw acceler-\nation reading which is always local in sensor frame. The system\nhowever expects both bone orientation and acceleration reading in\nğºğ‘, i.e., ğ‘¹ğµğ‘¡\nğºğ‘\nand ğ’‚ğºğ‘ . We have the following relations:\nğ‘¹ğµğ‘¡\nğºğ‘\n= ğ‘¹ğºğ‘›\nğºğ‘\nğ‘¹ğ‘†ğ‘¡\nğºğ‘›\nğ‘¹ğµğ‘¡\nğ‘†ğ‘¡\n,\nğ’‚ğºğ‘ = ğ‘¹ğºğ‘›\nğºğ‘\nğ‘¹ğ‘†ğ‘¡\nğºğ‘›\nğ’‚ğ‘†ğ‘¡ âˆ’Â¯ğ’‚ğ‘†ğ‘¡ ,\nwhere we note that Â¯ğ’‚ğ‘†ğ‘¡ is the constant acceleration bias in global\nframe, usually just the gravitational acceleration. From these re-\nlations, it should be clear that the goal of calibration is simply to\nobtain ğ‘¹ğºğ‘›\nğºğ‘\nand ğ‘¹ğµğ‘¡\nğ‘†ğ‘¡\nbefore each system run.\nIn the first calibration step, we place all sensors to align with the\nspecified global frame so that ğ‘¹ğ‘†\nğºğ‘›\n= ğ‘¹ğºğ‘\nğºğ‘›\n, and obtain ğ‘¹ğºğ‘›\nğºğ‘\nwhich\nis simply {ğ‘¹ğ‘†\nğºğ‘›\n}ğ‘‡. Following [Yi et al. 2021], we keep all sensors\nstill on ground for three seconds and take the average reading.\nNext, to obtainğ‘¹ğµğ‘¡\nğ‘†ğ‘¡\n, the user wears all six sensors and stand in a T\npose, facing the same \"front\" as ğºğ‘. We assume that the sensor will\nstay static with respect to the bone throughout the entire system\nrun, therefore ğ‘¹ğµğ‘¡\nğ‘†ğ‘¡\n= ğ‘¹ğµ0\nğ‘†0\n. Since the orientation of each bone at a\nstandard T pose, ğ‘¹ğµ0\nğºğ‘\n, is known, we are able to obtainğ‘¹ğµ0\nğ‘†0\nfrom the\nT-pose raw sensor reading ğ‘¹ğ‘†0\nğºğ‘›\nusing:\nğ‘¹ğµ0\nğ‘†0\n= {ğ‘¹ğ‘†0\nğºğ‘›\n}ğ‘‡ğ‘¹ğºğ‘\nğºğ‘›\nğ‘¹ğµ0\nğºğ‘\n,\nwhere same as the first step, T pose is maintained for three seconds\nand we use the average reading for ğ‘¹ğ‘†0\nğºğ‘›\n.\nE ADDITIONAL ANALYSIS\nWe present results of two additional experiments in this section.\nFirst to showcase how much the performance our autoregressive\nmodel will degrade over time, we repeat the quantitative experi-\nment of Table 1 but on random 3000-frame (50s) windows of each\nmotion, instead of 600 frames (10s). Note that since many test mo-\ntions are shorter than 50s, this experiment setting may unevenly\nbias statistics. For brevity, the DIP model is not included in this\ncomparison:\nTable 3: Comparison of model performance on evaluation\nmotion segments of maximum length 50 seconds. Bold num-\nbers indicate the best performing entries.\nOur TIP Model\nDIPEval TotalCapture DanceDB\njoint angle errors (degree) 12.33555 9.46942 15.28491\njoint position errors (ğ‘ğ‘š) 5.86926 5.40289 8.23641\nroot errors in 2s (meter) 0.08545 0.09504\nroot errors in 5s (meter) 0.16679 0.20369\nroot errors in 10s (meter) 0.20338 0.38935\njoint position jitter (ğ‘š/ğ‘ 3) 0.84848 0.80672 1.39043\nroot jitter (ğ‘š/ğ‘ 3) 0.64593 0.64609 0.95740\nTransPose Model\nDIPEval TotalCapture DanceDB\njoint angle errors (degree) 12.78403 11.56577 17.22182\njoint position errors (ğ‘ğ‘š) 6.16507 5.76287 8.35314\nroot errors in 2s (meter) 0.18543 0.14899\nroot errors in 5s (meter) 0.32042 0.28216\nroot errors in 10s (meter) 0.32111 0.45332\njoint position jitter (ğ‘š/ğ‘ 3) 0.57619 0.76578 1.44662\nroot jitter (ğ‘š/ğ‘ 3) 0.49804 0.70235 1.29385\nReading the numbers from Table 3, degradation of model per-\nformance is minimal on longer motions, and the statistics trends\nSA â€™22 Conference Papers, December 6â€“9, 2022, Daegu, Republic of Korea Yifeng Jiang, Yuting Ye, Deepak Gopinath, Jungdam Won, Alexander W. Winkler, and C. Karen Liu\nFigure 7: Motion reconstruction for sitting on a chair (from\nreal IMU data, top) and climbing steps (from synthesized\nIMU data, bottom). Our character is shown in yellow, Trans-\nPose in purple and Ground-Truth motion is shown in green.\nThe red spheres are predicted SBPs. Playing back motion on\nreconstructed terrains.\nbetween our model and TransPose remain unchanged. As a side\nnote, the DanceDB dataset contains more short motions, making\nthe sampling a random 50s segment more likely to cover the be-\nginnings of motions. We therefore see both TIP and TransPose\nhave improved root errors in 2s since the motions usually start from\nstanding and are less dynamic in the first two seconds.\nSecond, to showcase the benefit of acceleration preprocessing,\nwe perform an ablation study where we remove the average filter-\ning and summation operations from our TIP system, both during\ntraining and test time.\nTable 4: Ablation of model performance on evaluation mo-\ntion segments of 10s. Bold numbers indicate the best per-\nforming entries.\nOur TIP Model\nDIPEval TotalCapture DanceDB\njoint angle errors (degree) 12.09586 8.91642 15.57031\njoint position errors (ğ‘ğ‘š) 5.82242 5.14566 8.50089\nroot errors in 2s (meter) 0.08031 0.20295\nroot errors in 5s (meter) 0.1351 0.29681\nroot errors in 10s (meter) 0.19446 0.35759\njoint position jitter (ğ‘š/ğ‘ 3) 0.8823 0.75075 1.43867\nroot jitter (ğ‘š/ğ‘ 3) 0.66211 0.61108 0.98474\nOur TIP Model, w/o Acceleration Preprocessing\nDIPEval TotalCapture DanceDB\njoint angle errors (degree) 13.02724 9.18290 15.67625\njoint position errors (ğ‘ğ‘š) 6.35219 5.29268 8.58611\nroot errors in 2s (meter) 0.09096 0.16825\nroot errors in 5s (meter) 0.18015 0.25855\nroot errors in 10s (meter) 0.20726 0.34444\njoint position jitter (ğ‘š/ğ‘ 3) 0.85845 0.79335 1.44460\nroot jitter (ğ‘š/ğ‘ 3) 0.64661 0.63560 1.00417\nFrom Table 4, We see a visible improvement from preprocess-\ning the raw acceleration readings on real-IMU datasets (DIPEval\n& TotalCapture). As expected, preprocessing is unimportant for\nsynthesized IMU data (DanceDB).\nQualitative comparisons between our method and TransPose are\npresented using the following two representative motions (Figure 7,\nVideo 3m33s). Our TIP model can generate a more stable sitting\nposture by making better use of its own past predictions and utiliz-\ning run-time IK correction (Figure 7 Top). Figure 7 Bottom shows\nthat our algorithm is terrain agnostic while TransPose assumes\na flat ground and uses this assumption to correct the algorithmâ€™s\nvertical root prediction.\nF DISCUSSIONS\nThough we have shown clear improvement on existing challenges of\ntemporal consistency due to ambiguity, dynamic motion coverage,\nand terrain coverage, our system still has a few drawbacks for future\nwork. First, it tends to underestimate the terrain height rather than\noverestimate (e.g. Video 3m52s) - collecting more annotated real\nIMU data on various terrain types, and increasing training samples\nwith uneven terrains through data upsampling, could both help\nimprove terrain reconstruction. Second, terrain height estimations\nremain challenging since they solely depend on motion prediction,\nand are susceptible to sensor noises. For example, locomotion on\na slightly bumpy ground versus on a flat ground is theoretically\nnear-ambiguous given IMUâ€™s noise level (Video 4m6s). Third, our\nmotion reconstruction quality on real hardware can degrade on\nmotion types that are rare in training, thus affecting the quality of\ngenerated terrains (Video 4m21s). Finally, though our work does\nnot claim contribution over the jitter level of reconstructed motions,\nthe smoothing filter during post-processing is far from ideal and\nhurts our motion accuracy by effectively increasing latency.\nAnother very visible problem we observe is the modelâ€™s bias\nto body types. Our synthesized data were generated from virtual\ncharacters with random heights sampled from 1.6m to 1.8m. We\nobserved that the algorithm generalizes better to taller users than\nshorter ones. We hypothesize that this phenomenon is due to the\nmagnitude of acceleration, as the model might be more easily con-\nfused by smaller signals from a shorter user. Similarly, existing\nreal IMU datasets might have a bias in human shapes. Some per-\nsonalized training and finetuning of the model may eventually be\nnecessary for reconstructing more accurate and detailed motion\nfor each individual user.\nThe terrain generated from our algorithm is \"plausible\" in the\nsense that it cannot distinguish, sorely from IMU readings, if the\nfoot is resting on a terrain or simply staying stationary in air (Video\n4m58s). An algorithm that takes the distribution of commonly seen\nenvironments into consideration could guide our system to generate\nmore likely terrains in such ambiguous cases.",
  "topic": "Terrain",
  "concepts": [
    {
      "name": "Terrain",
      "score": 0.6342785358428955
    },
    {
      "name": "Computer science",
      "score": 0.6323719620704651
    },
    {
      "name": "Transformer",
      "score": 0.5426727533340454
    },
    {
      "name": "Inertial frame of reference",
      "score": 0.5103480219841003
    },
    {
      "name": "Computer vision",
      "score": 0.4777616560459137
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42236223816871643
    },
    {
      "name": "Engineering",
      "score": 0.18045225739479065
    },
    {
      "name": "Electrical engineering",
      "score": 0.1273309588432312
    },
    {
      "name": "Geography",
      "score": 0.0984601378440857
    },
    {
      "name": "Physics",
      "score": 0.09145519137382507
    },
    {
      "name": "Voltage",
      "score": 0.07761332392692566
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Cartography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210128585",
      "name": "META Health",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2799847335",
      "name": "Art Institute of Portland",
      "country": "US"
    }
  ]
}