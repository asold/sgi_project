{
  "title": "Graph Transformer for Graph-to-Sequence Learning",
  "url": "https://openalex.org/W2987178699",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2364924703",
      "name": "Cai, Deng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2184209163",
      "name": "Lam Wai",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2951309718",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2932864487",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2796167946",
    "https://openalex.org/W1829822087",
    "https://openalex.org/W2610308073",
    "https://openalex.org/W2133459682",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2612881151",
    "https://openalex.org/W2468355276",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2798759657",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2304113845",
    "https://openalex.org/W2758950307",
    "https://openalex.org/W2252123671",
    "https://openalex.org/W2963374482",
    "https://openalex.org/W2587541991",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2963653811",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2931198394",
    "https://openalex.org/W2951528897",
    "https://openalex.org/W2964035651",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2950898568",
    "https://openalex.org/W4288407114",
    "https://openalex.org/W2524520086",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W655477013",
    "https://openalex.org/W2964116568",
    "https://openalex.org/W2952706341",
    "https://openalex.org/W2951291634",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2964165364",
    "https://openalex.org/W2949954724",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W4288375838",
    "https://openalex.org/W2600565200",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2565245743",
    "https://openalex.org/W2606974598"
  ],
  "abstract": "The dominant graph-to-sequence transduction models employ graph neural networks for graph representation learning, where the structural information is reflected by the receptive field of neurons. Unlike graph neural networks that restrict the information exchange between immediate neighborhood, we propose a new model, known as Graph Transformer, that uses explicit relation encoding and allows direct communication between two distant nodes. It provides a more efficient way for global graph structure modeling. Experiments on the applications of text generation from Abstract Meaning Representation (AMR) and syntax-based neural machine translation show the superiority of our proposed model. Specifically, our model achieves 27.4 BLEU on LDC2015E86 and 29.7 BLEU on LDC2017T10 for AMR-to-text generation, outperforming the state-of-the-art results by up to 2.2 points. On the syntax-based translation tasks, our model establishes new single-model state-of-the-art BLEU scores, 21.3 for English-to-German and 14.1 for English-to-Czech, improving over the existing best results, including ensembles, by over 1 BLEU.",
  "full_text": "Graph Transformer for Graph-to-Sequence Learning∗\nDeng Cai and Wai Lam\nThe Chinese University of Hong Kong\nthisisjcykcd@gmail.com, wlam@se.cuhk.edu.hk\nAbstract\nThe dominant graph-to-sequence transduction models em-\nploy graph neural networks for graph representation learning,\nwhere the structural information is reﬂected by the receptive\nﬁeld of neurons. Unlike graph neural networks that restrict\nthe information exchange between immediate neighborhood,\nwe propose a new model, known as Graph Transformer, that\nuses explicit relation encoding and allows direct communica-\ntion between two distant nodes. It provides a more efﬁcient\nway for global graph structure modeling. Experiments on the\napplications of text generation from Abstract Meaning Rep-\nresentation (AMR) and syntax-based neural machine transla-\ntion show the superiority of our proposed model. Speciﬁcally,\nour model achieves 27.4 BLEU on LDC2015E86 and 29.7\nBLEU on LDC2017T10 for AMR-to-text generation, outper-\nforming the state-of-the-art results by up to 2.2 points. On\nthe syntax-based translation tasks, our model establishes new\nsingle-model state-of-the-art BLEU scores, 21.3 for English-\nto-German and 14.1 for English-to-Czech, improving over\nthe existing best results, including ensembles, by over 1\nBLEU.\nIntroduction\nGraphical structure plays an important role in natural lan-\nguage processing (NLP), they often serve as the central for-\nmalism for representing syntax, semantics, and knowledge.\nFor example, most syntactic representations (e.g., depen-\ndency relation) are tree-based while most whole-sentence\nsemantic representation frameworks (e.g., Abstract Mean-\ning Representation (AMR) (Banarescu et al. 2013)) encode\nsentence meaning as directed acyclic graphs. A range of\nNLP applications can be framed as the process of graph-\nto-sequence learning. For instance, text generation may in-\nvolve realizing a semantic graph into a surface form (Liu\net al. 2015) and syntactic machine translation incorporates\nsource-side syntax information for improving translation\nquality (Bastings et al. 2017). Fig. 1 gives an example of\nAMR-to-text generation.\n∗The work described in this paper is substantially supported by\na grant from the Research Grant Council of the Hong Kong Special\nAdministrative Region, China (Project Code: 14204418).\nCopyright c⃝2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nboy\ngirl\nThe boy wants the girl to believe him.\nboy\ngirl\nwant-01\nbelieve-01\nwant-01\nbelieve-01\nARG1\nARG0\nARG0\nARG1\nARG1\nARG0\nARG1\nARG0\nFigure 1: An AMR graph (left) for the reference sentence\n“The boy wants the girl to believe him.” and the correspond-\ning Levi graph (right).\nWhile early work uses statistical methods or neural mod-\nels after the linearization of graphs, graph neural networks\n(GNNs) have been ﬁrmly established as the state-of-the-\nart approaches for this task (Damonte and Cohen 2019;\nGuo et al. 2019). GNNs typically compute the represen-\ntation of each node iteratively based on those of its adja-\ncent nodes. This inherently local propagation nature pre-\ncludes efﬁcient global communication, which becomes crit-\nical at larger graph sizes, as the distance between two nodes\nexceeds the number of stacked layers. For instance, for\ntwo nodes staying L hops away, at least L layers will be\nneeded in order to capture their dependencies. Furthermore,\neven if two distant nodes are reachable, the information\nmay also be disrupted in the long journey (Xu et al. 2018;\nGuo et al. 2019).\nTo address the above problems, we propose a new model,\nknown as Graph Transformer, which relies entirely on the\nmulti-head attention mechanism (Vaswani et al. 2017) to\ndraw global dependencies. 1 Different to GNNs, the Graph\nTransformer allows direct modeling of dependencies be-\ntween any two nodes without regard to their distance in the\ninput graph. One undesirable consequence is that it essen-\n1We note that the nameGraph Transformerwas used in a recent\nwork (Koncel-Kedziorski et al. 2019). However, it merely focuses\non the relations between directly connected nodes as other graph\nneural networks.\narXiv:1911.07470v2  [cs.CL]  30 Nov 2019\ntially treats any graph as a fully connected graph, greatly\ndiluting the explicit graph structure. To maintain a graph\nstructure-aware view, our proposed model introduces ex-\nplicit relation encoding and incorporates it into the pairwise\nattention score computation as a dynamic parameter.\nOur treatment of explicit relation encoding also brings\nother side advantages compared to GNN-based methods.\nPrevious state-of-the-art GNN-based methods use Levi\ngraph transformation (Beck, Haffari, and Cohn 2018; Guo et\nal. 2019), where two unlabeled edges are replacing one la-\nbeled edge that is present in the original graph. For example,\nin Fig. 1, the labeled edge want-01\nARG1\n−→believe-01\nturns to be two unlabeled edges want-01 −→ARG1 and\nARG1 −→believe-01. Since edge labels are repre-\nsented as nodes, they end up sharing the same semantic\nspace, which is not ideal as nodes and edges are typically\ndifferent elements. In addition, the Levi graph transforma-\ntion at least doubles the number of representation vectors.\nwhich will introduce more complexity for the decoder-side\nattention mechanism (Bahdanau, Cho, and Bengio 2015)\nand copy mechanism (Gu et al. 2016; See, Liu, and Manning\n2017). Through explicit and separate relation encoding, our\nproposed Graph Transformer inherently avoids these prob-\nlems.\nExperiments show that our model is able to achieve better\nperformance for graph-to-sequence learning tasks for nat-\nural language processing. For the AMR-to-text generation\ntask, our model surpasses the current state-of-the-art neu-\nral methods trained on LDC2015E86 and LDC2017T10 by\n1.6 and 2.2 BLEU points, respectively. For the syntax-based\nneural machine translation task, our model is also consis-\ntently better than others, even including ensemble systems,\nshowing the effectiveness of the model on a large training\nset. In addition, we give an in-depth study of the source of\nimprovement gain and the internal workings of the proposed\nmodel.\nRelated Work\nEarly research efforts for graph-to-sequence learning use\nspecialized grammar-based methods. Flanigan et al.(2016)\nsplit input graphs to trees and uses a tree-to-string trans-\nducer. Song et al.(2016) recast generation as a traveling\nsalesman problem. Jones et al.(2012) leverage hyperedge re-\nplacement grammar and Song et al.(2017) use a synchronous\nnode replacement grammar. More recent work employs\nmore general approaches, such as phrase-based machine\ntranslation model (Pourdamghani, Knight, and Hermjakob\n2016) and neural sequence-to-sequence methods (Konstas et\nal. 2017) after linearizing input graphs. Regarding AMR-to-\ntext generation, Cao and Clark(2019) propose an interesting\nidea that factorizes text generation through syntax. One limi-\ntation of sequence-to-sequence models, however, is that they\nrequire serialization of input graphs, which inevitably incurs\nthe obstacle of capturing graph structure information.\nAn emerging trend has been directly encoding the graph\nwith different variants of graph neural networks, which in\ncommon stack multiple layers that restrict the update of\nnode representation based on a ﬁrst-order neighborhood but\nuse different information passing schemes. Some borrow the\nideas from recurrent neural networks (RNNs), e.g, Beck,\nHaffari, and Cohn(2018) use gated graph neural network (Li\net al. 2016) while Song et al.(2018) introduce LSTM-style\ninformation aggregation. Others apply convolutional neu-\nral networks (CNNs), e.g., Bastings et al.(2017);Damonte\nand Cohen(2019);Guo et al.(2019) utilize graph convolu-\ntional neural networks (Kipf and Welling 2017). Koncel-\nKedziorski et al.(2019) update vertex information by atten-\ntion over adjacent neighbors. Furthermore, Guo et al.(2019)\nallow the information exchange across different levels of\nlayers. Damonte and Cohen(2019) systematically compare\ndifferent encoders and show the advantages of graph en-\ncoder over tree and sequential ones. The contrast between\nour model and theirs is reminiscent of the contrast between\nthe self-attention network (SAN) and CNN/RNN.\nFor sequence-to-sequence learning, the SAN-based\nTransformer model (Vaswani et al. 2017) has been the de\nfacto approach for its empirical successes. However, it is un-\nclear on the adaptation to graphical data and its performance.\nOur work is partially inspired by the introduction of relative\nposition embedding (Shaw, Uszkoreit, and Vaswani 2018;\nDai et al. 2019) in sequential data. However, the extension\nto graph is nontrivial since we need to model much more\ncomplicated relation instead of mere visual distance. To the\nbest of our knowledge, the Graph Transformer is the ﬁrst\ngraph-to-sequence transduction model relying entirely on\nself-attention to compute representations.\nBackground of Self-Attention Network\nThe Transformer introduced by Vaswani et al.(2017) is a\nsequence-to-sequence neural architecture originally used for\nneural machine translation. It employs self-attention net-\nwork (SAN) for implementing both the encoder and the de-\ncoder. The encoder consists of multiple identical blocks, of\nwhich the core is multi-head attention. The multi-head atten-\ntion consists of H attention heads, and each of them learns\na distinct attention function. Given a source vector x∈Rdx\nand a set of context vectors {y1,y2,...,y m}with the same\ndimension dx or in short y1:m, for each attention head,xand\ny1:m are transformed into distinct query and value represen-\ntations. The attention score is computed as the dot-product\nbetween them.\nf(x,yi) = (Wqx)T Wkyi\nwhere Wq,Wk ∈Rdz×dx are trainable projection matrices.\nThe attention scores are scaled and normalized by a softmax\nfunction to compute the ﬁnal attention output attn.\nai = exp(f(x,yi)/√dz)∑m\nj=1 exp(f(x,yi))/√dz)\nattn=\nm∑\ni=1\naiWvyi\nwhere a∈Rm is the attention vector (a distribution over all\ninput y1:m), Wv ∈Rdz×dx is a trainable projection matrix.\nFinally, the outputs of all attention heads are concatenated\nand projected to the original dimension of x, followed by\nshortest paths\nRelation Encoder\nwant-01\nbelieve-01\nboy\ngirl\n…\nARG0\nARG1\nARG0\nARG1\nGraph Encoder Sequence Decoder\nSelf-Attention\nAttention\n+\n+Position \nEmbedding\nNode\nEmbedding Token\nEmbedding \nPosition \nEmbedding\nDifferent colors represent different \nshortest paths among node pairs.\nRelation-Enhanced\nGlobal Attention\nNode Initialization\nOutput\n...\n(fully-connected view)\nFigure 2: An overview of our proposed model.\nfeed-forward layers, residual connection, and layer normal-\nization.2 For brevity, we will denote the whole procedure\ndescribed above as a single function ATT(x,y1:m).\nFor an input sequence x1:n, the SAN-based encoder\ncomputes the vector representations iteratively by xL\ni =\nATT(xL\ni ,xL−1\n1:n ), where L is the total number of blocks\nand x0\n1:n are word embeddings. In this way, a representa-\ntion is allowed to build a direct relationship with another\nlong-distance representation. To feed the sequential order in-\nformation, the deterministic or learned position embedding\n(Vaswani et al. 2017) is introduced to expose the position\ninformation to the model, i.e., x0\ni becomes the sum of the\ncorresponding word embedding and the position embedding\nfor i.\nThe aforementioned treatment of SAN on sequential data\ncan be drawn a close resemblance to graph neural networks\nby regarding the token sequence as an unlabeled fully-\nconnected graph (each token as a node) and taking the multi-\nhead attention mechanism as a speciﬁc message-passing\nscheme. Such view on the relationship between SAN and\ngraph neural networks inspires our work.\nGraph Transformer\nOverview\nFor a graph with n nodes, previous graph neural networks\ncompute the node representationvi as a function of the input\nnode iand all its ﬁrst-order neighborhoods N(i). The graph\nstructure is implicitly reﬂected by the receptive ﬁeld of each\nnode representation. This local communication design, how-\never, could be inefﬁcient for long-distance information ex-\nchange. We introduce a new model, known as Graph Trans-\nformer, which provides an aggressively different paradigm\nthat enables relation-aware global communication.\n2We refer interesting readers to Vaswani et al.(2017) for more\ndetails.\nThe overall framework is shown in Fig. 2. The most im-\nportant characteristic of the Graph Transformer is that it has\na fully-connected view on arbitrary input graphs. A node\nis able to directly receive and send information to another\nnode no matter whether they are directly connected or not.\nThese operations are achieved by our proposed extension to\nthe original multi-head attention mechanism, the relation-\nenhanced global attention mechanism described below. In a\nnutshell, the relationship between any node pair is depicted\nas the shortest relation path between them. These pairwise\nrelation paths are fed into a relation encoder for distributed\nrelation encoding. The node vectors are initialized as the\nsum of the node embedding and absolute position embed-\ndings. Multiple blocks of global attention network are then\nstacked to compute the ﬁnal node representations. At each\nblock, a node vector is updated based on all other node vec-\ntors and the corresponding relation encodings. The resulted\nnode vectors at the last block are fed to the sequence decoder\nfor sequence generation.\nGraph Encoder\nOur graph encoder is responsible for transforming an input\ngraph into a set of corresponding node embeddings. To ap-\nply global attention on a graph, the central problem is how\nto maintain the topological structure of the graph while al-\nlowing fully-connected communication. To this end, we pro-\npose relation-enhanced global attention mechanism, which\nis an extension of the vanilla multi-head attention. Our idea\nis to incorporate explicit relation representation between two\nnodes into their representation learning. Recall that, in the\nstandard multi-head attention, the attention score between\nthe element xi and the element xj is simply the dot-product\nof their query vector and key vector respectively:\nsij = f(xi,xj)\n= xiWT\nq Wkxj\n(1)\nSuppose we have learned a vector representation for the\nrelationship rij, which we will refer as relation encoding,\nbetween the node i and the node j. Following the idea of\nrelative position embedding (Shaw, Uszkoreit, and Vaswani\n2018; Dai et al. 2019), we propose to compute the attention\nscore as follows:\n[ri→j; rj→i] =Wrrij (2)\nwhere we ﬁrst split the relation encodingrij into the forward\nrelation encoding ri→j and the backward relation encoding\nrj→i. Then we compute the attention score based on both\nthe node representations and their relation representation:\nsij = g(xi,xj,rij)\n= (xi + ri→j)WT\nq Wk(xj + rj→i)\n= xiWT\nq Wkxj\n  \n(a)\n+ xiWT\nq Wkrj→i\n  \n(b)\n+ ri→jWT\nq Wkxj\n  \n(c)\n+ ri→jWT\nq Wkrj→i\n  \n(d)\n(3)\nEach term in Eq (3) corresponds to some intuitive mean-\ning according to their formalization. The term (a) captures\npurely content-based addressing, which is the original term\nin vanilla attention mechanism. The term (b) represents\na source-dependent relation bias. The term (c) governs a\ntarget-dependent relation bias. The term (d) encodes the\nuniversal relation bias. Our formalization provides a prin-\ncipled way to model the element-relation interactions. In\ncomparison, it has broader coverage than Shaw, Uszkoreit,\nand Vaswani(2018) in terms of additional terms (c) and (d),\nand than Dai et al.(2019) in terms of the extra term (c) re-\nspectively. More importantly, previous methods only model\nthe relative position in the context of sequential data, which\nmerely adopts the immediate embeddings of the relative po-\nsitions (e.g, −1,+1). To depict the relation between two\nnodes in a graph, we utilize a shortest-path based approach\nas described below.\nRelation Encoder Conceptually, the relation encoding\ngives the model a global guidance about how information\nshould be gathered and distributed, i.e., where to attend. For\nmost graphical structures in NLP, the edge label conveys di-\nrect relationship between adjacent nodes (e.g., the semantic\nrole played by concept-to-concept, and the dependency re-\nlation between two words). We extend this one-hop relation\ndeﬁnition into multi-hop relation reasoning for characteriz-\ning the relationship between two arbitrary nodes. For exam-\nple, in Fig 1, the shortest path from the conceptwant-01 to\ngirl is “ want-01\nARG1\n−→ believe-01\nARG0\n−→ girl”,\nwhich conveys that girl is the object of the wanted ac-\ntion. Intuitively, the shortest path between two nodes gives\nthe closest and arguably the most important relationship be-\ntween them. Therefore, we propose to use the shortest paths\n(relation sequence) between two nodes to characterize their\nrelationship.3 Following the sequential nature of the rela-\n3For the case that there are multiple shortest paths, we randomly\nsample one during training and take the averaged representation\nduring testing.\ntion sequence, we employs recurrent neural networks with\nGated Recurrent Unit (GRU) (Cho et al. 2014) for trans-\nforming relation sequence into a distributed representation.\nFormally, we represent the shortest relation path spi→j =\n[e(i,k1),e(k1,k2),...,e (kn,j)] between the node iand the\nnode j, where e(·,·) indicates the edge label andk1:n are the\nrelay nodes. We employ bi-directional GRUs for sequence\nencoding:\n− →st = GRUf (−−→st−1,spt)\n← −st = GRUb(←−−st+1,spt)\nThe last hidden states of the forward GRU network and the\nbackward GRU networks are concatenated to form the ﬁnal\nrelation encoding rij = [− →sn; ← −s0].\nBidirectionality Though in theory, our architecture can\ndeal with arbitrary input graphs, the most widely adopted\ngraphs in the real problems are directed acyclic graphs\n(DAGs). This implies that the node embedding information\nwill be propagated in one pre-speciﬁed direction. However,\nthe reverse direction informs the equivalent information ﬂow\nas well. To facilitate communication in both directions, we\nadd reverse edges to the graph. The reverse edge connects\nthe same two nodes as the original edge but in a different di-\nrection and with a reversed label. For example, we will draw\na virtual edge believe-01\nRARG1\n−→ want-01 accord-\ning to the original edge want-01\nARG1\n−→ believe-01.\nFor convenience, we also introduce self-loop edges for each\nnode. These extra edges have speciﬁc labels, hence their own\nparameters in the network. We also introduce an extra global\nnode into every graph, who has a direct edge to all other\nnodes with the special label global. The ﬁnal representation\nxglobal of the global node serves as a whole graph represen-\ntation.\nAbsolute Position Besides pairwise relationship, some\nabsolute positional information can also be beneﬁcial. For\nexample, the root of an AMR graph serves as a rudimen-\ntary representation of the overall focus, making the mini-\nmum distance from the root node partially reﬂect the impor-\ntance of the corresponding concept in the whole-sentence se-\nmantics. The sequence order of tokens in a dependency tree\nalso provides complementary information to dependency re-\nlations. In order for the model to make use of the absolute\npositions of nodes, we add the positional embeddings to the\ninput embeddings at the bottom of the encoder stacks. For\nexample, want-01 in Fig 1 is the root node of the AMR\ngraph, so its index should be 0. Notice we denote the index\nof the global node as 0 as well.\nSequence Decoder\nOur sequence decoder basically follows the same spirit of\nthe sequential Transformer decoder. The decoder yields the\nnatural language sequence by calculating a sequence of hid-\nden states sequentially. One distinct characteristic is that we\nuse the global graph representation xglobal for initializing\nthe hidden states at each time step. The hidden state ht at\neach time step t is then updated by interleaving multiple\nrounds of attention over the output of the encoder (node\nDataset #train #dev #test #edge types #node types avg #nodes avg #edges avg diameter\nLDC2015E86 16,833 1,368 1,371 113 18735 17.34 17.53 6.98\nLDC2017T10 36,521 1,368 1,371 116 24693 14.51 14.62 6.15\nEnglish-Czech 181,112 2,656 2,999 46 78017 23.18 22.18 8.36\nEnglish-German 226,822 2,169 2,999 46 87219 23.29 22.29 8.42\nTable 1: Data statistics of all four datasets. #train/dev/test indicates the number of instances in each set, avg\n#nodes/edges/diameter represents the averaged value of nodes/edge/diameter size of a graph.\nmodel component hyper-parameter value\nchar-level CNN\nnumber of ﬁlters 256\nwidth of ﬁlters 3\nchar embedding size 32\nﬁnal hidden size 128\nEmbeddings node embedding size 300\nedge embedding size 200\ntoken embedding size 300\nMulti-head attention\nnumber of heads 8\nhidden state size 512\nfeed-forward hidden size 1024\nTable 2: Hyper-parameters settings.\nembeddings) and attention over previously-generated tokens\n(token embeddings). Both are implemented by the multi-\nhead attention mechanism. xglobal is removed when per-\nforming the sequence-to-graph attention.\nCopy mechanism To address the data sparsity issue in\ntoken prediction, we include a copy mechanism (Gu et al.\n2016) in similar spirit to most recent works. Concretely, a\nsingle-head attention is computed based on the decoder state\nht and the node representation x1:n, where ai\nt denotes the\nattention weight of the node vi in the current time step t.\nOur model can either directly copy the type name of a node\n(node label) or generate from a pre-deﬁned vocabulary V.\nFormally, the prediction probability of a tokenyis given by:\nP(y|ht) =P(gen|ht)gen(y|ht) +P(copy|ht)\n∑\ni∈S(y)\nai\nt\nwhere S(y) is the set of nodes that have the same surface\nform as y. P(gen|ht) and P(copy|ht) are computed by a\nsingle layer neural network with softmax activation, and\ngen(y|ht) = exp(wyT ht)/∑\ny′∈V exp(w′\ny\nT ht), where wy\n(for y∈V) denotes the model parameters. The copy mecha-\nnism facilitates the generation of dates, numbers, and named\nentities in both AMR-to-text generation and machine trans-\nlation tasks in experiments.\nExperiments\nWe assess the effectiveness of our models on two typical\ngraph-to-sequence learning tasks, namely AMR-to-text gen-\neration and syntax-based machine translation (MT). Fol-\nlowing previous work, the results are mainly evaluated by\nBLEU (Papineni et al. 2002) and CHR F++ (Popovi ´c 2017).\nSpeciﬁcally, we use case-insensitive scores for AMR and\ncase-sensitive BLEU scores for MT.\nAMR-to-text Generation\nOur ﬁrst application is language generation from AMR, a se-\nmantic formalism that represents sentences as rooted DAGs\n(Banarescu et al. 2013). For this AMR-to-text generation\ntask, we use two benchmarks, namely the LDC2015E86\ndataset and the LDC2017T10 dataset. The ﬁrst block of\nTable 1 shows the statistics of the two datasets. Similar\nto Konstas et al.(2017), we apply entity simpliﬁcation and\nanonymization in the preprocessing steps and restore them\nin the postprocessing steps.\nThe graph encoder uses randomly initialized node em-\nbeddings as well as the output from a learnable CNN with\ncharacter embeddings as input. The sequence decoder uses\nrandomly initialized token embeddings and another char-\nlevel CNN. Model hyperparameters are chosen by a small\nset of experiments on the development set of LDC2017T10.\nThe detailed settings are listed in Table 2. During testing,\nwe use a beam size of 8 for generating graphs. To mitigate\noverﬁtting, we also apply dropout (Srivastava et al. 2014)\nwith the drop rate of 0.2 between different layers. We use\na special UNK token to replace the input node tag with a\nrate of 0.33. Parameter optimization is performed with the\nAdam optimizer (Kingma and Ba 2014) with β1 = 0.9 and\nbeta2 = 0.999. The same learning rate schedule of Vaswani\net al.(2017) is adopted in our experiments.4 For computation\nefﬁciency, we gather all distinct shortest paths in a train-\ning/testing batch, and encode them into vector representa-\ntions by the recurrent relation encoding procedure as de-\nscribed above.5\nWe run comparisons on systems without ensembling nor\nadditional silver data. Speciﬁcally, the comparison methods\ncan be grouped into three categories: (1) feature-based sta-\ntistical methods (Song et al. 2016; Pourdamghani, Knight,\nand Hermjakob 2016; Song et al. 2017; Flanigan et al.\n2016); (2) sequence-to-sequence neural models (Konstas et\nal. 2017; Cao and Clark 2019), which use linearized graphs\nas inputs; (3) recent works using different variants of graph\nneural networks for encoding graph structures directly (Song\net al. 2018; Beck, Haffari, and Cohn 2018; Damonte and Co-\nhen 2019; Guo et al. 2019). The results are shown in Table\n3. For both datasets, our approach substantially outperforms\nall previous methods. On the LDC2015E86 dataset, our\nmethod achieves a BLEU score of 27.4, outperforming pre-\nvious best-performing neural model (Guo et al. 2019) by a\nlarge margin of 2.6 BLEU points. Also, our model becomes\n4Code available at https://github.com/jcyk/gtos.\n5This strategy reduces the number of related sequences to en-\ncode from O(mn2) to a stable number when a large batch size m\nis used.\nModel LDC2015E86 LDC2017T10\nBLEU CHR F++ METEOR BLEU CHR F++ METEOR\nSong et al.(2016)† 22.4 - - - - -\nFlanigan et al.(2016)† 23.0 - - - - -\nPourdamghani, Knight, and Hermjakob(2016)† 26.9 - - - - -\nSong et al.(2017)† 25.6 - - - - -\nKonstas et al.(2017) 22.0 - - - - -\nCao and Clark(2019)‡ 23.5 - - 26.8 - -\nSong et al.(2018) 23.3 - - 24.9 - -\nBeck, Haffari, and Cohn(2018) - - - 23.3 50.4\nDamonte and Cohen(2019) 24.4 - 23.6 24.5 - 24.1\nGuo et al.(2019) 25.7 54.5∗ 31.5∗ 27.6 57.3 34.0∗\nOurs 27.4 56.4 32.9 29.8 59.4 35.1\nTable 3: Main results on AMR-to-text generation. Numbers with ∗are from the contact from the authors. - denotes that the\nresult is unknown because it is not provided in the corresponding paper.\nModel Type English-German English-Czech\nBLEU CHR F++ BLEU CHR F++\nBastings et al.(2017) Single 16.1 - 9.6 -\nBeck, Haffari, and Cohn(2018) Single 16.7 42.4 9.8 33.3\nGuo et al.(2019) Single 19.0 44.1 12.1 37.1\nBeck, Haffari, and Cohn(2018) Ensemble 19.6 45.1 11.7 35.9\nGuo et al.(2019) Ensemble 20.5 45.8 13.1 37.8\nOurs Single 21.3 47.9 14.1 41.1\nTable 4: Main results on syntax-based machine translation.\nthe ﬁrst neural model that surpasses the strong non-neural\nbaseline established by Pourdamghani, Knight, and Herm-\njakob(2016). It is worth noting that those traditional methods\nmarked with †train their language models on the external\nGigaword corpus, thus they possess an additional advantage\nof extra data. On the LDC2017T10 dataset, our model es-\ntablishes a new record BLEU score of 29.8, improving over\nthe state-of-the-art sequence-to-sequence model (Cao and\nClark 2019) by 3 points and the state-of-the-art GNN-based\nmodel (Guo et al. 2019) by 2.2 points. The results are even\nmore remarkable since the model of Cao and Clark(2019)\n(marked with ‡) uses constituency syntax from an external\nparser. Similar phenomena can be found on the additional\nmetrics of CHR F++ and M ETEOR (Denkowski and Lavie\n2014). Those results suggest that current graph neural net-\nworks cannot make full use of the AMR graph structure, and\nour Graph Transformer provides a promising alternative.\nSyntax-based Machine Translation\nOur second evaluation is syntax-based machine translation,\nwhere the input is a source language dependency syntax tree\nand the output is a plain target language string. We employ\nthe same data and settings from Bastings et al.(2017). Both\nthe English-German and the English-Czech datasets from\nthe WMT16 translation task. 6 The English sentences are\nparsed after tokenization to generate the dependency trees\non the source side using SyntaxNet (Alberti et al. 2017). 7\nOn the Czech and German sides, texts are tokenized using\n6http://www.statmt.org/wmt16/translation-task.html.\n7https://github.com/tensorﬂow/models/tree/master/syntaxnet\nthe Moses tokenizer.8 Byte-pair encodings (Sennrich, Had-\ndow, and Birch 2016) with 8,000 merge operations are used\nto obtain subwords. The second block of Table 1 shows the\nstatistics for both datasets. For model conﬁguration, we just\nre-use the settings obtained in our AMR-to-text experiments.\nTable 4 presents the results with comparison to existing\nmethods. On the English-to-German translation task, our\nmodel achieves a BLEU score of 41.0, outperforming all of\nthe previously published single models by a large margin of\n2.3 BLEU score. On the English-to-Czech translation task,\nour model also outperforms the best previously reported sin-\ngle models by an impressive margin of 2 BLEU points. In\nfact, our single model already outperforms previous state-\nof-the-art models that use ensembling. The advantages of\nour method are also veriﬁed by the metric CHR F++.\nAn important point about these experiments is that we\ndid not tune the architecture: we simply employed the same\nmodel in all experiments, only adjusting the batch size for\ndifferent dataset size. We speculate that even better results\nwould be obtained by tuning the architecture to individ-\nual tasks. Nevertheless, we still obtained improved perfor-\nmance over previous works, underlining the generality of\nour model.\nMore Analysis\nThe overall scores show a great advantage of the Graph\nTransformer over existing methods, including the state-of-\nthe-art GNN-based models. However, they do not shed light\ninto how this is achieved. In order to further reveal the source\nof performance gain, we perform a series of analyses based\n8https://github.com/moses-smt/mosesdecoder.\nchrF++\n54.5\n56.5\n58.5\n60.5\n62.5\nGraph Size\n1-20 21-30 31-40 >40\nOurs\nGuo’19\n(a)\nchrF++\n54.0\n55.8\n57.6\n59.4\n61.2\nGraph Diameter\n1-7 8-14 >14\nOurs\nGuo’19 (b)\nchrF++\n54.0\n56.5\n59.0\n61.5\n64.0\nGraph Reentrancies\n0-1 2-3 4-5 >5\nOurs\nGuo’19 (c)\nFigure 3: CHR F++ scores with respect to (a) the graph size, (b) the graph diameter, and (c) the the number of reentrancies.\nFigure 4: The average distance for maximum attention for\neach head.\non different characteristics of graphs. For those analyses, we\nuse sentence-level CHR F++ scores, and take the macro av-\nerage of them when needed. All experiments are conducted\nwith the test set of LDC2017T10.\nGraph Size To assess the model’s performance for differ-\nent sizes of graphs, we group graphs into four classes and\nshow the curves of CHR F++ scores in Figure 3a. The re-\nsults are presented with the contrast with the state-of-the-art\nGNN-based model of Guo et al.(2019), denoted as Guo’19.\nAs seen, the performance of both models decreases as the\ngraph size increases. It is expected since a larger graph of-\nten contains more complex structure and the interactions\nbetween graph elements are more difﬁcult to capture. The\ngap between ours and Guo’19 becomes larger for relatively\nlarger graphs while for small graphs, both models give simi-\nlar performance. This result demonstrates that our model has\nbetter ability for dealing with complicated graphs. As for ex-\ntremely large graphs, the performance of both models have\na clear drop, yet ours is still slightly better.\nGraph Diameter We then study the impact of graph diam-\neter.9 Graphs with large diameters have interactions between\ntwo nodes that appear distant from each other. We conjec-\nture that it will cause severe difﬁculties for GNN-based mod-\nels because they solely rely on local communication. Figure\n3b conﬁrms our hypothesis, as the curve of the GNN-based\nmodel shows a clear slope. In contrast, our model has more\nstable performance, and the gap between the two curves also\n9The diameter of a graph is deﬁned as the length of the longest\nshortest path between two nodes.\nillustrates the superiority of our model on featuring long-\ndistance dependencies.\nNumber of Reentrancies We also study the ability for\nhandling the reentrancies, where the same node has multiple\nparent nodes (or the same concept participates in multiple\nrelations for AMR). The recent work (Damonte and Cohen\n2019) has identiﬁed reentrancies as one of the most difﬁcult\naspects of AMR structure. We bin the number of reentran-\ncies occurred in a graph into four classes and plot Fig. 3c. It\ncan be observed that the gap between the GNN-based model\nand the Graph transformer becomes noticeably wide when\nthere are more than one reentrancies. Since then, our model\nis consistently better than the GNN-based model, maintain-\ning a margin of over 1 CHR F++ score.\nHow Far Does Attention Look At The Graph Trans-\nformer shows a strong capacity for processing complex and\nlarge graphs. We attribute the success to the global commu-\nnication design, as it provides opportunities for direct com-\nmunication in long distance. A natural and interesting ques-\ntion is how well the model makes use of this property. To\nanswer this question, following V oita et al.(2019), we turn\nto study the attention distribution of each attention head.\nSpeciﬁcally, we record the speciﬁc distance to which its\nmaximum attention weight is assigned as attention distance.\nFig. 4 shows the averaged attention distance after we run\nour model on the development set of LDC2017T10. We can\nobserve that nearly half of the attention heads have an av-\nerage attention distance larger than 2. The number of these\nfar-sighted heads generally increases as layers go deeper. In-\nterestingly, the longest-reaching head (layer1-head5) and the\nshortest-sighted head (layer1-head2) coexist in the very ﬁrst\nlayer, while the former has an average distance over 5.\nConclusions\nIn this paper, we presented the Graph Transformer, the ﬁrst\ngraph-to-sequence learning model based entirely on auto-\nmatic attention. Different from previous recurrent models\nthat require linearization of input graph and previous graph\nneural network models that restrict the direct message pass-\ning in the ﬁrst-order neighborhood, our model enables global\nnode-to-node communication. With the Graph Transformer,\nwe achieve the new state-of-the-art on two typical graph-to-\nsequence generation tasks with four benchmark datasets.\nReferences\n[2017] Alberti, C.; Andor, D.; Bogatyy, I.; Collins, M.;\nGillick, D.; Kong, L.; Koo, T.; Ma, J.; Omernick, M.; Petrov,\nS.; et al. 2017. Syntaxnet models for the conll 2017 shared\ntask. arXiv preprint arXiv:1703.04929.\n[2015] Bahdanau, D.; Cho, K.; and Bengio, Y . 2015. Neural\nmachine translation by jointly learning to align and translate.\nIn ICLR.\n[2013] Banarescu, L.; Bonial, C.; Cai, S.; Georgescu, M.;\nGrifﬁtt, K.; Hermjakob, U.; Knight, K.; Koehn, P.; Palmer,\nM.; and Schneider, N. 2013. Abstract meaning representa-\ntion for sembanking. In Proceedings of the 7th Linguistic\nAnnotation Workshop and Interoperability with Discourse,\n178–186.\n[2017] Bastings, J.; Titov, I.; Aziz, W.; Marcheggiani, D.;\nand Sima’an, K. 2017. Graph convolutional encoders for\nsyntax-aware neural machine translation. In EMNLP, 1957–\n1967.\n[2018] Beck, D.; Haffari, G.; and Cohn, T. 2018. Graph-\nto-sequence learning using gated graph neural networks. In\nACL, 273–283.\n[2019] Cao, K., and Clark, S. 2019. Factorising AMR gen-\neration through syntax. In NAACL, 2157–2163.\n[2014] Cho, K.; Van Merri ¨enboer, B.; Gulcehre, C.; Bah-\ndanau, D.; Bougares, F.; Schwenk, H.; and Bengio, Y .\n2014. Learning phrase representations using rnn encoder-\ndecoder for statistical machine translation. arXiv preprint\narXiv:1406.1078.\n[2019] Dai, Z.; Yang, Z.; Yang, Y .; Carbonell, J.; Le, Q.; and\nSalakhutdinov, R. 2019. Transformer-XL: Attentive lan-\nguage models beyond a ﬁxed-length context. In ACL, 2978–\n2988.\n[2019] Damonte, M., and Cohen, S. B. 2019. Structural neu-\nral encoders for AMR-to-text generation. In NAACL, 3649–\n3658.\n[2014] Denkowski, M., and Lavie, A. 2014. Meteor univer-\nsal: Language speciﬁc translation evaluation for any target\nlanguage. In Proceedings of the EACL 2014 Workshop on\nStatistical Machine Translation.\n[2016] Flanigan, J.; Dyer, C.; Smith, N. A.; and Carbonell,\nJ. 2016. Generation from abstract meaning representation\nusing tree transducers. In NAACL, 731–739.\n[2016] Gu, J.; Lu, Z.; Li, H.; and Li, V . O. 2016. Incorporat-\ning copying mechanism in sequence-to-sequence learning.\nIn ACL, 1631–1640.\n[2019] Guo, Z.; Zhang, Y .; Teng, Z.; and Lu, W. 2019.\nDensely connected graph convolutional networks for graph-\nto-sequence learning. Transactions of the Association for\nComputational Linguistics7:297–312.\n[2012] Jones, B.; Andreas, J.; Bauer, D.; Hermann, K. M.;\nand Knight, K. 2012. Semantics-based machine translation\nwith hyperedge replacement grammars. In COLING, 1359–\n1376.\n[2014] Kingma, D. P., and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980.\n[2017] Kipf, T. N., and Welling, M. 2017. Semi-supervised\nclassiﬁcation with graph convolutional networks. In ICLR.\n[2019] Koncel-Kedziorski, R.; Bekal, D.; Luan, Y .; Lapata,\nM.; and Hajishirzi, H. 2019. Text Generation from Knowl-\nedge Graphs with Graph Transformers. In NAACL, 2284–\n2293.\n[2017] Konstas, I.; Iyer, S.; Yatskar, M.; Choi, Y .; and Zettle-\nmoyer, L. 2017. Neural AMR: Sequence-to-sequence mod-\nels for parsing and generation. In ACL, 146–157.\n[2016] Li, Y .; Tarlow, D.; Brockschmidt, M.; and Zemel, R.\n2016. Gated graph sequence neural networks. In ICLR.\n[2015] Liu, F.; Flanigan, J.; Thomson, S.; Sadeh, N.; and\nSmith, N. A. 2015. Toward abstractive summarization using\nsemantic representations. In NAACL, 1077–1086.\n[2002] Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J.\n2002. Bleu: a method for automatic evaluation of machine\ntranslation. In ACL, 311–318.\n[2017] Popovi ´c, M. 2017. chrf++: words helping character n-\ngrams. In Proceedings of the second conference on machine\ntranslation, 612–618.\n[2016] Pourdamghani, N.; Knight, K.; and Hermjakob, U.\n2016. Generating english from abstract meaning represen-\ntations. In INLG, 21–25.\n[2017] See, A.; Liu, P. J.; and Manning, C. D. 2017. Get to\nthe point: Summarization with pointer-generator networks.\nIn ACL, 1073–1083.\n[2016] Sennrich, R.; Haddow, B.; and Birch, A. 2016. Neural\nmachine translation of rare words with subword units. In\nACL, 1715–1725.\n[2018] Shaw, P.; Uszkoreit, J.; and Vaswani, A. 2018. Self-\nattention with relative position representations. In NAACL,\n464–468.\n[2016] Song, L.; Zhang, Y .; Peng, X.; Wang, Z.; and Gildea,\nD. 2016. AMR-to-text generation as a traveling salesman\nproblem. In EMNLP, 2084–2089.\n[2017] Song, L.; Peng, X.; Zhang, Y .; Wang, Z.; and Gildea,\nD. 2017. AMR-to-text generation with synchronous node\nreplacement grammar. In ACL, 7–13.\n[2018] Song, L.; Zhang, Y .; Wang, Z.; and Gildea, D. 2018.\nA graph-to-sequence model for AMR-to-text generation. In\nACL, 1616–1626.\n[2014] Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever,\nI.; and Salakhutdinov, R. 2014. Dropout: a simple way to\nprevent neural networks from overﬁtting. The Journal of\nMachine Learning Research15(1):1929–1958.\n[2017] Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.;\nJones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017.\nAttention is all you need. In NIPS, 5998–6008.\n[2019] V oita, E.; Talbot, D.; Moiseev, F.; Sennrich, R.; and\nTitov, I. 2019. Analyzing multi-head self-attention: Special-\nized heads do the heavy lifting, the rest can be pruned. In\nACL, 5797–5808.\n[2018] Xu, K.; Wu, L.; Wang, Z.; Feng, Y .; Witbrock, M.;\nand Sheinin, V . 2018. Graph2seq: Graph to sequence learn-\ning with attention-based neural networks. arXiv preprint\narXiv:1804.00823.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7801229953765869
    },
    {
      "name": "Transformer",
      "score": 0.7086699604988098
    },
    {
      "name": "Machine translation",
      "score": 0.6508923768997192
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5962010622024536
    },
    {
      "name": "Graph",
      "score": 0.567934513092041
    },
    {
      "name": "Natural language processing",
      "score": 0.4854493737220764
    },
    {
      "name": "BLEU",
      "score": 0.46099355816841125
    },
    {
      "name": "Theoretical computer science",
      "score": 0.4290597438812256
    },
    {
      "name": "Syntax",
      "score": 0.4206593334674835
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "HK"
    }
  ],
  "cited_by": 37
}