{
  "title": "Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling",
  "url": "https://openalex.org/W2950323007",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2365636915",
      "name": "Gan Zhe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2225388594",
      "name": "Li, Chunyuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2256462353",
      "name": "Chen, Changyou",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Pu, Yunchen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2614708678",
      "name": "Su, Qinliang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2619478227",
      "name": "Carin Lawrence",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2114524997",
    "https://openalex.org/W1904365287",
    "https://openalex.org/W2162456913",
    "https://openalex.org/W1800356822",
    "https://openalex.org/W1719489212",
    "https://openalex.org/W196214544",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W2146502635",
    "https://openalex.org/W2384495648",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2108677974",
    "https://openalex.org/W2950726992",
    "https://openalex.org/W2186210550",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W114517082",
    "https://openalex.org/W1567512734",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2558834163",
    "https://openalex.org/W1836307405",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2951105989",
    "https://openalex.org/W2263490141",
    "https://openalex.org/W2951266961",
    "https://openalex.org/W2526170246",
    "https://openalex.org/W2234809627",
    "https://openalex.org/W2951654389",
    "https://openalex.org/W2238987678",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2160660844",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963266340",
    "https://openalex.org/W2276300401",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4919037",
    "https://openalex.org/W2951595529",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2111051539",
    "https://openalex.org/W2300605907",
    "https://openalex.org/W35527955",
    "https://openalex.org/W1951216520",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W68733909",
    "https://openalex.org/W2185175083",
    "https://openalex.org/W2431605385",
    "https://openalex.org/W2150355110",
    "https://openalex.org/W2964059111",
    "https://openalex.org/W2170678468",
    "https://openalex.org/W2163455955",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W2115701093",
    "https://openalex.org/W2292443655",
    "https://openalex.org/W2463507112",
    "https://openalex.org/W2963504252",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W2964325005",
    "https://openalex.org/W2165652770",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W179875071",
    "https://openalex.org/W1994616650",
    "https://openalex.org/W2167433878"
  ],
  "abstract": "Recurrent neural networks (RNNs) have shown promising performance for language modeling. However, traditional training of RNNs using back-propagation through time often suffers from overfitting. One reason for this is that stochastic optimization (used for large training sets) does not provide good estimates of model uncertainty. This paper leverages recent advances in stochastic gradient Markov Chain Monte Carlo (also appropriate for large training sets) to learn weight uncertainty in RNNs. It yields a principled Bayesian learning algorithm, adding gradient noise during training (enhancing exploration of the model-parameter space) and model averaging when testing. Extensive experiments on various RNN models and across a broad range of applications demonstrate the superiority of the proposed approach over stochastic optimization.",
  "full_text": "Scalable Bayesian Learning of Recurrent Neural Networks\nfor Language Modeling\nZhe Gan∗, Chunyuan Li∗†, Changyou Chen, Yunchen Pu, Qinliang Su, Lawrence Carin\nDepartment of Electrical and Computer Engineering, Duke University\n{zg27, cl319, cc448, yp42, qs15, lcarin}@duke.edu\nAbstract\nRecurrent neural networks (RNNs) have\nshown promising performance for lan-\nguage modeling. However, traditional\ntraining of RNNs, using back-propagation\nthrough time, often suffers from overﬁt-\nting. One reason for this is that stochastic\noptimization (used for large training sets)\ndoes not provide good estimates of model\nuncertainty. This paper leverages recent\nadvances in stochastic gradient Markov\nChain Monte Carlo (also appropriate for\nlarge training sets) to learn weight uncer-\ntainty in RNNs. It yields a principled\nBayesian learning algorithm, adding gra-\ndient noise during training (enhancing ex-\nploration of the model-parameter space)\nand model averaging when testing. Ex-\ntensive experiments on various RNN mod-\nels and across a broad range of applica-\ntions demonstrate the superiority of the\nproposed approach relative to stochastic\noptimization.\n1 Introduction\nLanguage modeling is a fundamental task, used,\nfor example, to predict the next word or character\nin a text sequence, given the context. Recently,\nrecurrent neural networks (RNNs) have shown\npromising performance on this task (Mikolov\net al., 2010; Sutskever et al., 2011). RNNs with\nLong Short-Term Memory (LSTM) units (Hochre-\niter and Schmidhuber, 1997) have emerged as a\npopular architecture, due to their representational\npower and effectiveness at capturing long-term de-\npendencies.\nRNNs are usually trained via back-propagation\nthrough time (Werbos, 1990), using stochastic op-\n∗Equal contribution. †Corresponding author.\ntimization methods such as stochastic gradient de-\nscent (SGD) (Robbins and Monro, 1951); stochas-\ntic methods of this type are particularly important\nfor training with large data sets. However, this\napproach often provides a maximum a posteriori\n(MAP) estimate of model parameters. The MAP\nsolution is a single point estimate, ignoring weight\nuncertainty (Blundell et al., 2015; Hern ´andez-\nLobato and Adams, 2015). Natural language of-\nten exhibits signiﬁcant variability, and hence such\na point estimate may make over-conﬁdent predic-\ntions on test data.\nTo alleviate overﬁtting RNNs, good regular-\nization is known as a key factor to successful\napplications. In the neural network literature,\nBayesian learning has been proposed as a princi-\npled method to impose regularization and incor-\nporate model uncertainty (MacKay, 1992; Neal,\n1995), by imposing prior distributions on model\nparameters. Due to the intractability of poste-\nrior distributions in neural networks, Hamiltonian\nMonte Carlo (HMC) (Neal, 1995) has been used to\nprovide sample-based approximations to the true\nposterior. Despite the elegant theoretical prop-\nerty of asymptotic convergence to the true poste-\nrior, HMC and other conventional Markov Chain\nMonte Carlo methods are not scalable to large\ntraining sets.\nThis paper seeks to scale up Bayesian learning\nof RNNs to meet the challenge of the increasing\namount of “big” sequential data in natural lan-\nguage processing, leveraging recent advances in\nstochastic gradient Markov Chain Monte Carlo\n(SG-MCMC) algorithms (Welling and Teh, 2011;\nChen et al., 2014; Ding et al., 2014; Li et al.,\n2016a,b). Speciﬁcally, instead of training a sin-\ngle network, SG-MCMC is employed to train an\nensemble of networks, where each network has its\nparameters drawn from a shared posterior distri-\nbution. This is implemented by adding additional\narXiv:1611.08034v2  [cs.CL]  24 Apr 2017\nEncoding weights\nRecurrent weights\nDecoding weights\nOutput\nInput\nHidden\nFigure 1: Illustration of different weight learning\nstrategies in a single-hidden-layer RNN. Stochas-\ntic optimization used for MAP estimation puts\nﬁxed values on all weights. Naive dropout is al-\nlowed to put weight uncertainty only on encoding\nand decoding weights, and ﬁxed values on recur-\nrent weights. The proposed SG-MCMC scheme\nimposes distributions on all weights.\ngradient noise during training and utilizing model\naveraging when testing.\nThis simple procedure has the following salu-\ntary properties for training neural networks: (i)\nWhen training, the injected noise encourages\nmodel-parameter trajectories to better explore the\nparameter space. This procedure was also empiri-\ncally found effective in Neelakantan et al. (2016).\n(ii) Model averaging when testing alleviates over-\nﬁtting and hence improves generalization, trans-\nferring uncertainty in the learned model parame-\nters to subsequent prediction. (iii) In theory, both\nasymptotic and non-asymptotic consistency prop-\nerties of SG-MCMC methods in posterior estima-\ntion have been recently established to guarantee\nconvergence (Chen et al., 2015a; Teh et al., 2016).\n(iv) SG-MCMC is scalable; it shares the same\nlevel of computational cost as SGD in training,\nby only requiring the evaluation of gradients on\na small mini-batch. To the authors’ knowledge,\nRNN training using SG-MCMC has not been in-\nvestigated previously, and is a contribution of this\npaper. We also perform extensive experiments on\nseveral natural language processing tasks, demon-\nstrating the effectiveness of SG-MCMC for RNNs,\nincluding character/word-level language model-\ning, image captioning and sentence classiﬁcation.\n2 Related Work\nSeveral scalable Bayesian learning methods have\nbeen proposed recently for neural networks. These\ncome in two broad categories: stochastic vari-\national inference (Graves, 2011; Blundell et al.,\n2015; Hern ´andez-Lobato and Adams, 2015) and\nSG-MCMC methods (Korattikara et al., 2015; Li\net al., 2016a). While prior work focuses on\nfeed-forward neural networks, there has been lit-\ntle if any research reported for RNNs using SG-\nMCMC.\nDropout (Hinton et al., 2012; Srivastava et al.,\n2014) is a commonly used regularization method\nfor training neural networks. Recently, several\nworks have studied how to apply dropout to\nRNNs (Pachitariu and Sahani, 2013; Bayer et al.,\n2013; Pham et al., 2014; Zaremba et al., 2014;\nBluche et al., 2015; Moon et al., 2015; Semeniuta\net al., 2016; Gal and Ghahramani, 2016b). Among\nthem, naive dropout (Zaremba et al., 2014) can im-\npose weight uncertainty only on encoding weights\n(those that connect input to hidden units) and de-\ncoding weights(those that connect hidden units to\noutput), but not the recurrent weights(those that\nconnect consecutive hidden states). It has been\nconcluded that noise added in the recurrent con-\nnections leads to model instabilities, hence dis-\nrupting the RNN’s ability to model sequences.\nDropout has been recently shown to be a varia-\ntional approximation technique in Bayesian learn-\ning (Gal and Ghahramani, 2016a; Kingma et al.,\n2015). Based on this, (Gal and Ghahramani,\n2016b) proposed a new variant of dropout that can\nbe successfully applied to recurrent layers, where\nthe same dropout masks are shared along time for\nencoding, decoding and recurrent weights, respec-\ntively. Alternatively, we focus on SG-MCMC,\nwhich can be viewed as the Bayesian interpreta-\ntion of dropout from the perspective of posterior\nsampling (Li et al., 2016c); this also allows im-\nposition of model uncertainty on recurrent layers,\nenhancing performance. A comparison of naive\ndropout and SG-MCMC is illustrated in Fig. 1.\n3 Recurrent Neural Networks\n3.1 RNN as Bayesian Predictive Models\nConsider data D= {D1,··· ,DN}, where Dn ≜\n(Xn,Yn), with input Xn and output Yn. Our\ngoal is to learn model parameters θ to best\ncharacterize the relationship from Xn to Yn,\nwith corresponding data likelihood p(D|θ) =∏N\nn=1 p(Dn|θ). In Bayesian statistics, one sets\na prior on θvia distribution p(θ). The posterior\np(θ|D) ∝p(θ)p(D|θ) reﬂects the belief concern-\ning the model parameter distribution after observ-\ning the data. Given a test input ˜X (with miss-\ning output ˜Y), the uncertainty learned in training\nis transferred to prediction, yielding the posterior\npredictive distribution:\np( ˜Y|˜X,D)=\n∫\nθ\np( ˜Y|˜X,θ)p(θ|D)dθ. (1)\nWhen the input is a sequence, RNNs may be\nused to parameterize the input-output relation-\nship. Speciﬁcally, consider input sequence X =\n{x1,..., xT}, where xt is the input data vector at\ntime t. There is a corresponding hidden state vec-\ntor ht at each time t, obtained by recursively ap-\nplying the transition function ht = H(ht−1,xt)\n(speciﬁed in Section 3.2; see Fig. 1). The outputY\ndiffers depending on the application: a sequence\n{y1,..., yT}in language modeling or a discrete\nlabel in sentence classiﬁcation. In RNNs the cor-\nresponding decoding functionis p(y|h), described\nin Section 3.3.\n3.2 RNN Architectures\nThe transition function H(·) can be implemented\nwith a gated activation function, such as Long\nShort-Term Memory (LSTM) (Hochreiter and\nSchmidhuber, 1997) or a Gated Recurrent Unit\n(GRU) (Cho et al., 2014). Both the LSTM and\nGRU have been proposed to address the issue of\nlearning long-term sequential dependencies.\nLong Short-Term Memory The LSTM archi-\ntecture addresses the problem of learning long-\nterm dependencies by introducing a memory cell,\nthat is able to preserve the state over long periods\nof time. Speciﬁcally, each LSTM unit has a cell\ncontaining a state ct at time t. This cell can be\nviewed as a memory unit. Reading or writing the\ncell is controlled through sigmoid gates: input gate\nit, forget gate ft, and output gate ot. The hidden\nunits ht are updated as\nit = σ(Wixt + Uiht−1 + bi) ,\nft = σ(Wfxt + Ufht−1 + bf) ,\not = σ(Woxt + Uoht−1 + bo) ,\n˜ct = tanh(Wcxt + Ucht−1 + bc) ,\nct = ft ⊙ct−1 + it ⊙˜ct,\nht = ot ⊙tanh(ct) ,\nwhere σ(·) denotes the logistic sigmoid func-\ntion, and ⊙represents the element-wise matrix\nmultiplication operator. W{i,f,o,c} are encoding\nweights, and U{i,f,o,c} are recurrent weights, as\nshown in Fig. 1; b{i,f,o,c}are bias terms.\nVariants Similar to the LSTM unit, the GRU\nalso has gating units that modulate the ﬂow of\ninformation inside the hidden unit. It has been\nshown that a GRU can achieve similar perfor-\nmance to an LSTM in sequence modeling (Chung\net al., 2014). We specify the GRU in the Supple-\nmentary Material.\nThe LSTM can be extended to the bidirec-\ntional LSTM and multilayer LSTM. A bidirec-\ntional LSTM consists of two LSTMs that are run\nin parallel: one on the input sequence and the other\non the reverse of the input sequence. At each time\nstep, the hidden state of the bidirectional LSTM\nis the concatenation of the forward and backward\nhidden states. In multilayer LSTMs, the hidden\nstate of an LSTM unit in layer ℓ is used as input\nto the LSTM unit in layer ℓ+ 1at the same time\nstep (Graves, 2013).\n3.3 Applications\nThe proposed Bayesian framework can be applied\nto any RNN model; we focus on the following\ntasks to demonstrate the ideas.\nLanguage Modeling In word-level language\nmodeling, the input to the network is a sequence\nof words, and the network is trained to predict the\nnext word in the sequence with a softmax classi-\nﬁer. Speciﬁcally, for a length- T sequence, denote\nyt = xt+1 for t = 1,...,T −1. x1 and yT are\nalways set to a special START and END token,\nrespectively. At each time t, there is a decoding\nfunction p(yt|ht) = softmax(Vht) to compute\nthe distribution over words, where V are the de-\ncoding weights (the number of rows of V corre-\nsponds to the number of words/characters). We\nalso extend this basic language model to consider\nother applications: (i) a character-level language\nmodel can be speciﬁed in a similar manner by\nreplacing words with characters (Karpathy et al.,\n2016). (ii) Image captioning can be considered\nas a conditional language modeling problem, in\nwhich we learn a generative language model of the\ncaption conditioned on an image (Vinyals et al.,\n2015; Gan et al., 2017).\nSentence Classiﬁcation Sentence classiﬁcation\naims to assign a semantic category label y to a\nwhole sentence X. This is usually implemented\nthrough applying the decoding function once at\nthe end of sequence: p(y|hT) =softmax(VhT),\nwhere the ﬁnal hidden state of a RNN hT is often\nconsidered as the summary of the sentence (here\nthe number of rows of V corresponds to the num-\nber of classes).\n4 Scalable Learning with SG-MCMC\n4.1 The Pitfall of Stochastic Optimization\nTypically there is no closed-form solution for the\nposterior p(θ|D), and traditional Markov Chain\nMonte Carlo (MCMC) methods (Neal, 1995) scale\npoorly for largeN. To ease the computational bur-\nden, stochastic optimization is often employed to\nﬁnd the MAP solution. This is equivalent to min-\nimizing an objective of regularized loss function\nU(θ) that corresponds to a (non-convex) model\nof interest: θMAP = arg minU(θ), U(θ) =\n−log p(θ|D). The expectation in (1) is approxi-\nmated as:\np( ˜Y|˜X,D)= p( ˜Y|˜X,θMAP) . (2)\nThough simple and effective, this procedure\nlargely loses the beneﬁt of the Bayesian approach,\nbecause the uncertainty on weights is ignored.\nTo more accurately approximate (1), we employ\nstochastic gradient (SG) MCMC (Welling and\nTeh, 2011).\n4.2 Large-scale Bayesian Learning\nThe negative log-posterior is\nU(θ) ≜ −log p(θ) −\nN∑\nn=1\nlog p(Dn|θ). (3)\nIn optimization, E = −∑N\nn=1 log p(Dn|θ) is typ-\nically referred to as the loss function, and R ∝\n−log p(θ) as a regularizer.\nFor large N, stochastic approximations are of-\nten employed:\n˜Ut(θ)≜−log p(θ) −N\nM\nM∑\nm=1\nlog p(Dim |θ), (4)\nwhere Sm = {i1,··· ,iM}is a random subset of\nthe set {1,2,··· ,N}, with M ≪N. The gradi-\nent on this mini-batch is denoted as ˜ft = ∇˜Ut(θ),\nwhich is an unbiased estimate of the true gradi-\nent. The evaluation of (4) is cheap even when N\nis large, allowing one to efﬁciently collect a suf-\nﬁcient number of samples in large-scale Bayesian\nlearning, {θs}S\ns=1, where Sis the number of sam-\nples (this will be speciﬁed later). These samples\nare used to construct a sample-based estimation to\nthe expectation in (1):\nTable 1: SG-MCMC algorithms and their optimiza-\ntion counterparts. Algorithms in the same row share\nsimilar characteristics.\nAlgorithms SG-MCMC Optimization\nBasic SGLD SGD\nPrecondition pSGLD RMSprop/Adagrad\nMomentum SGHMC momentum SGD\nThermostat SGNHT Santa\np( ˜Y|˜X,D)≈1\nS\nS∑\ns=1\np( ˜Y|˜X,θs) . (5)\nThe ﬁnite-time estimation errors of SG-MCMC\nmethods are bounded (Chen et al., 2015a), which\nguarantees (5) is an unbiased estimate of (1)\nasymptotically under appropriate decreasing step-\nsizes.\n4.3 SG-MCMC Algorithms\nSG-MCMC and stochastic optimization are par-\nallel lines of work, designed for different pur-\nposes; their relationship has recently been re-\nvealed in the context of deep learning. The most\nbasic SG-MCMC algorithm has been applied to\nLangevin dynamics, and is termed SGLD (Welling\nand Teh, 2011). To help convergence, a momen-\ntum term has been introduced in SGHMC (Chen\net al., 2014), a “thermostat” has been devised\nin SGNHT (Ding et al., 2014; Gan et al., 2015)\nand preconditioners have been employed in pS-\nGLD (Li et al., 2016a). These SG-MCMC algo-\nrithms often share similar characteristics with their\ncounterpart approaches from the optimization lit-\nerature such as the momentum SGD, Santa (Chen\net al., 2016) and RMSprop/Adagrad (Tieleman\nand Hinton, 2012; Duchi et al., 2011). The interre-\nlationships between SG-MCMC and optimization-\nbased approaches are summarized in Table 1.\nSGLD Stochastic Gradient Langevin Dynamics\n(SGLD) (Welling and Teh, 2011) draws posterior\nsamples, with updates\nθt = θt−1 −ηt ˜ft−1 +\n√\n2ηtξt, (6)\nwhere ηt is the learning rate, and ξt ∼N(0,Ip) is\na standard Gaussian random vector. SGLD is the\nSG-MCMC analog to stochastic gradient descent\n(SGD), whose parameter updates are given by:\nθt = θt−1 −ηt ˜ft−1 . (7)\nAlgorithm 1: pSGLD\nInput: Default hyperparameter settings:\nηt = 1×10−3,λ = 10−8,β1 = 0.99.\nInitialize: v0 ←0, θ1 ∼N(0,I) ;\nfor t= 1,2,...,T do\n% Estimate gradient from minibatch St\n˜ft = ∇˜Ut(θ);\n% Preconditioning\nvt ←β1vt−1 + (1−β1) ˜ft ⊙ ˜ft;\nG−1\nt ←diag\n(\n1 ⊘\n(\nλ1 + v\n1\n2\nt\n))\n;\n% Parameter update\nξt ∼N(0,ηtG−1\nt );\nθt+1 ←θt + ηt\n2 G−1\nt ˜ft+ ξt;\nend\nSGD is guaranteed to converge to a local mini-\nmum under mild conditions (Bottou, 2010). The\nadditional Gaussian term in SGLD helps the learn-\ning trajectory to explore the parameter space to ap-\nproximate posterior samples, instead of obtaining\na local minimum.\npSGLD Preconditioned SGLD (pSGLD) (Li\net al., 2016a) was proposed recently to improve\nthe mixing of SGLD. It utilizes magnitudes of re-\ncent gradients to construct a diagonal precondi-\ntioner to approximate the Fisher information ma-\ntrix, and thus adjusts to the local geometry of\nparameter space by equalizing the gradients so\nthat a constant stepsize is adequate for all dimen-\nsions. This is important for RNNs, whose parame-\nter space often exhibitspathological curvatureand\nsaddle points (Pascanu et al., 2013), resulting in\nslow mixing. There are multiple choices of pre-\nconditioners; similar ideas in optimization include\nAdagrad (Duchi et al., 2011), Adam (Kingma and\nBa, 2015) and RMSprop (Tieleman and Hinton,\n2012). An efﬁcient version of pSGLD, adopt-\ning RMSprop as the preconditioner G, is summa-\nrized in Algorithm 1, where ⊘denotes element-\nwise matrix division. When the preconditioner is\nﬁxed as the identity matrix, the method reduces to\nSGLD.\n4.4 Understanding SG-MCMC\nTo further understand SG-MCMC, we show its\nclose connection to dropout/dropConnect (Srivas-\ntava et al., 2014; Wan et al., 2013). These methods\nimprove the generalization ability of deep models,\nby randomly adding binary/Gaussian noise to the\nlocal units or global weights. For neural networks\nwith the nonlinear function q(·) and consecutive\nlayers h1 and h2, dropout and dropConnect are\ndenoted as:\nDropout: h2 = ξ0 ⊙q(θh1),\nDropConnect: h2 = q((ξ0 ⊙θ)h1),\nwhere the injected noise ξ0 can be binary-valued\nwith dropping rate p or its equivalent Gaussian\nform (Wang and Manning, 2013):\nBinary noise: ξ0 ∼Ber(p),\nGaussian noise: ξ0 ∼N(1, p\n1 −p).\nNote that ξ0 is deﬁned as a vector for dropout, and\na matrix for dropConnect. By combining drop-\nConnect and Gaussian noise from the above, we\nhave the update rule (Li et al., 2016c):\nθt+1 = ξ0 ⊙θt −η\n2\n˜ft = θt −η\n2\n˜ft + ξ′\n0 , (8)\nwhere ξ′\n0 ∼ N\n(\n0, p\n(1−p) diag(θ2\nt)\n)\n; (8) shows\nthat dropout/ dropConnect and SGLD in (6) share\nthe same form of update rule, with the distinc-\ntion being that the level of injected noise is dif-\nferent. In practice, the noise injected by SGLD\nmay not be enough. A better way that we ﬁnd to\nimprove the performance is to jointly apply SGLD\nand dropout. This method can be interpreted as\nusing SGLD to sample the posterior distribution\nof a mixture of RNNs, with mixture probability\ncontrolled by the dropout rate.\n5 Experiments\nWe present results on several tasks, including\ncharacter/word-level language modeling, image\ncaptioning, and sentence classiﬁcation. We do\nnot perform any dataset-speciﬁc tuning other than\nearly stopping on validation sets. When dropout is\nutilized, the dropout rate is set to 0.5. All experi-\nments are implemented in Theano (Theano Devel-\nopment Team, 2016), using a NVIDIA GeForce\nGTX TITAN X GPU with 12GB memory.\nThe hyper-parameters for the proposed algo-\nrithm include step size, minibatch size, thinning\ninterval, number of burn-in epochs and variance\nof the Gaussian priors. We list the speciﬁc val-\nues used in our experiments in Table 2. The ex-\nplanation of these hyperparameters, the initializa-\ntion of model parameters and model speciﬁcations\non each dataset are provided in the Supplementary\nMaterial.\nTable 2: Hyper-parameter settings of pSGLD for different datasets. For PTB, SGLD is used.\nDatasets WP PTB Flickr8k Flickr30k MR CR SUBJ MPQA TREC\nMinibatch Size 100 32 64 64 50 50 50 50 50\nStep Size 2×10−3 1 10 −3 10−3 10−3 10−3 10−3 10−3 10−3\n# Total Epoch 20 40 20 20 20 20 20 20 20\nBurn-in (#Epoch) 4 4 3 3 1 1 1 1 1\nThinning Interval (#Epoch) 1/2 1/2 1 1/2 1 1 1 1 1\n# Samples Collected 32 72 17 34 19 19 19 19 19\n5.1 Language Modeling\nWe ﬁrst test character-level and word-level lan-\nguage modeling. The setup is as follows.\n•Following Karpathy et al. (2016), we test\ncharacter-level language modeling on the\nWar and Peace (WP) novel. The train-\ning/validation/test sets contain 260/32/33\nbatches, in which there are 100 characters.\nThe vocabulary size is 87, and we consider\na 2-hidden-layer RNN of dimension 128.\n•The Penn Treebank (PTB) corpus (Marcus\net al., 1993) is used for word-level lan-\nguage modeling. The dataset adopts the\nstandard split (929K training words, 73K\nvalidation words, and 82K test words) and\nhas a vocabulary of size 10K. We train\nLSTMs of three sizes; these are denoted the\nsmall/medium/large LSTM. All LSTMs have\ntwo layers and are unrolled for 20 steps. The\nsmall, medium and large LSTM has 200, 650\nand 1500 units per layer, respectively.\nWe consider two types of training schemes\non PTB corpus: (i) Successive minibatches:\nFollowing Zaremba et al. (2014), the ﬁnal\nhidden states of the current minibatch are\nused as the initial hidden states of the subse-\nquent minibatch (successive minibatches se-\nquentially traverse the training set). (ii) Ran-\ndom minibatches: The initial hidden states of\neach minibatch are set to zero vectors, hence\nwe can randomly sample minibatches in each\nupdate.\nWe study the effects of different types of architec-\nture (LSTM/GRU/Vanilla RNN (Karpathy et al.,\n2016)) on the WP dataset, and effects of differ-\nent learning algorithms on the PTB dataset. The\ncomparison of test cross-entropy loss on WP is\nshown in Table 3. We observe that pSGLD con-\nsistently outperforms RMSprop. Table 4 summa-\nrizes the test set performance on PTB 1. It is clear\n1The results reported here do not match Zaremba et al.\n(2014) due to the implementation details. However, we pro-\nTable 3: Test cross-entropy loss on WP dataset.\nMethods LSTM GRU RNN\nRMSprop 1.3607 1.2759 1.4239\npSGLD 1.3375 1.2561 1.4093\n10 20 30 40 50 60\nIndividual Sample\n110\n120\n130\n140\n150\n160\n170\n180Perplexity\n0 10 20 30 40 50 60\nNumber of Samples for Model Averaging\n110\n120\n130\n140\n150\n160\n170\n180Perplexity\nforward collection\nbackward collection\nthinned collection\n(a) Single sample (b) Different collections\nFigure 2: Effects of collected samples.\nthat our sampling-based method consistently out-\nperforms the optimization counterpart, where the\nperformance gain mainly comes from adding gra-\ndient noise and model averaging. When com-\npared with dropout, SGLD performs better on the\nsmall LSTM model, but worse on the medium\nand large LSTM model. This may imply that\ndropout is suitable to regularizing large networks,\nwhile SGLD exhibits better regularization ability\non small networks, partially due to the fact that\ndropout may inject a higher level of noise during\ntraining than SGLD. In order to inject a higher\nlevel of noise into SGLD, we empirically apply\nSGLD and dropout jointly, and found that this\nprovided the best performace on the medium and\nlarge LSTM model.\nWe study three strategies to do model averaging,\ni.e., forward collection, backward collection and\nthinned collection. Given samples (θ1,··· ,θK)\nand the number of samples S used for averaging,\nforward collectionrefers to using(θ1,··· ,θS) for\nthe evaluation of a test function, backward col-\nlection refers to using (θK−S+1,··· ,θK), while\nthinned collectionchooses samples from θ1 to θK\nwith interval K/S. Fig. 2 plots the effects of\nthese strategies, where Fig. 2(a) plots the perplex-\nity of every single sample, Fig. 2(b) plots the per-\nplexities using the three schemes. Only after 20\nvide a fair comparison to all methods.\nTable 4: Test perplexity on Penn Treebank.\nMethods Small Medium Large\nRandom minibatches\nSGD 123.85 126.31 130.25\nSGD+Dropout 136.39 100.12 97.65\nSGLD 117.36 109.14 105.86\nSGLD+Dropout 139.54 99.58 94.03\nSuccessive minibatches\nSGD 113.45 123.14 127.68\nSGD+Dropout 117.85 84.60 80.85\nSGLD 108.61 121.16 131.40\nSGLD+Dropout 125.44 82.71 78.91\nLiterature\nMoon et al. (2015) − 97.0 118.7\nMoon et al. (2015)+ emb. dropout − 86.5 86.0\nZaremba et al. (2014) − 82.7 78.4\nGal and Ghahramani (2016b) − 78.6 73.4\nsamples is a converged perplexity achieved in the\nthinned collection, while it requires 30 samples\nfor forward collection or 60 samples for backward\ncollection. This is unsurprising, because thinned\ncollection provides a better way to select samples.\nNevertheless, averaging of samples provides sig-\nniﬁcantly lower perplexity than using single sam-\nples. Note that the overﬁtting problem in Fig. 2(a)\nis also alleviated by model averaging.\nTo better illustrate the beneﬁt of model averag-\ning, we visualize in Fig. 3 the probabilities of each\nword in a randomly chosen test sentence. The ﬁrst\n3 rows are the results predicted by 3 distinctive\nmodel samples, respectively; the bottom row is the\nresult after averaging. Their corresponding per-\nplexities for the test sentence are also shown on\nthe right of each row. The 3 individual samples\nprovide reasonable probabilities. For example, the\nconsecutive words “New York”, “stock exchange”\nand “did not” are assigned with a higher proba-\nbility. After averaging, we can see a much lower\nperplexity, as the samples can complement each\nother. For example, though the second sample can\nyield the lowest single-model perplexity, its pre-\ndiction on word “York” is still beneﬁted from the\nother two via averaging.\n5.2 Image Caption Generation\nWe next consider the problem of image caption\ngeneration, which is a conditional RNN model,\nwhere image features are extracted by residual net-\nwork (He et al., 2016), and then fed into the RNN\nto generate the caption. We present results on\ntwo benchmark datasets, Flickr8k (Hodosh et al.,\n2013) and Flickr30k (Young et al., 2014). These\n25.55the 25.55new 25.55york 25.55stock 25.55exchange 25.55did 25.55not 25.55fall 25.55apart\n22.24the 22.24new 22.24york 22.24stock 22.24exchange 22.24did 22.24not 22.24fall 22.24apart\n29.83the 29.83new 29.83york 29.83stock 29.83exchange 29.83did 29.83not 29.83fall 29.83apart\n21.98the 21.98new 21.98york 21.98stock 21.98exchange 21.98did 21.98not 21.98fall 21.98apart\n 0\n0.2\n0.4\n0.6\n0.8\n1\nFigure 3: Predictive probabilities obtained by 3\nsamples and their average. Colors indicate nor-\nmalized probability of each word. Best viewed in\ncolor.\na\"tan\"dog\"is\"playing\"in\"the\"grass\na\"tan\"dog\" is\"playing\"with\"a\"red\"ball\"in\"the\"grass\na\"tan\"dog\"with\"a\"red\"collar\"is\"running\"in\"the\"grass\na\"yellow\"dog\"runs\"through\"the\"grass\na\"yellow\"dog\"is\"running\"through\"the\"grass\na\"brown\" dog\"is\"running\"through\" the\"grass\na\"group\"of\"people\"stand\"in\"front\"of\"a\"building\na\"group\"of\"people\"stand\"in\"front\"of\"a\"white\"building\na\"group\"of\"people\"stand\"in\"front\"of\"a\"large\"building\na\"man\"and\"a\"woman\"walking\"on\"a\" sidewalk\na\"man\"and\"a\"woman\"stand\"on\"a\"balcony\na\"man\"and\"a\"woman\"standing\"on\"the\"ground\nFigure 4: Image captioning with different sam-\nples. Left are the given images, right are the cor-\nresponding captions. The captions in each box are\nfrom the same model sample.\ndatasets contain 8,000 and 31,000 images, respec-\ntively. Each image is annotated with 5 sentences.\nA single-layer LSTM is employed with the num-\nber of hidden units set to 512.\nThe widely used BLEU (Papineni et al., 2002),\nMETEOR (Banerjee and Lavie, 2005), ROUGE-\nL (Lin, 2004), and CIDEr-D (Vedantam et al.,\n2015) metrics are used to evaluate the perfor-\nmance. All the metrics are computed by us-\ning the code released by the COCO evaluation\nserver (Chen et al., 2015b).\nTable 5 presents results for pSGLD/RMSprop\nTable 5: Performance on Flickr8k & Flickr30k: BLEU’s, METEOR, CIDEr, ROUGE-L and perplexity.\nMethods B-1 B-2 B-3 B-4 METEOR CIDEr ROUGE-L Perp.\nResults on Flickr8k\nRMSprop 0.640 0.427 0.288 0.197 0.205 0.476 0.500 16.64\nRMSprop + Dropout 0.647 0.444 0.305 0.209 0.208 0.514 0.510 15.72\nRMSprop + Gal’s Dropout 0.651 0.443 0.305 0.209 0.206 0.501 0.509 14.70\npSGLD 0.669 0.463 0.321 0.224 0.214 0.535 0.522 14.29\npSGLD + Dropout 0.656 0.450 0.309 0.211 0.209 0.512 0.512 14.26\nResults on Flickr30k\nRMSprop 0.644 0.422 0.279 0.184 0.180 0.372 0.476 17.80\nRMSprop + Dropout 0.656 0.435 0.295 0.200 0.185 0.396 0.481 18.05\nRMSprop + Gal’s Dropout 0.636 0.429 0.290 0.197 0.190 0.408 0.480 17.27\npSGLD 0.657 0.438 0.300 0.206 0.192 0.421 0.490 15.61\npSGLD + Dropout 0.666 0.448 0.308 0.209 0.189 0.419 0.487 17.05\nwith or without dropout. In addition to (naive)\ndropout, we further compare pSGLD with the\nGal’s dropout, recently proposed in Gal and\nGhahramani (2016b), which is shown to be ap-\nplicable to recurrent layers. Consistent with re-\nsults in the basic language modeling literature,\npSGLD yields improved performance compared\nto RMSprop. For example, pSGLD provides 2.7\nBLEU-4 score improvement over RMSprop on the\nFlickr8k dataset. By comparing pSGLD with RM-\nSprop with dropout, we conclude that pSGLD ex-\nhibits better regularization ability than dropout on\nthese two datasets.\nApart from modeling weight uncertainty, differ-\nent samples from our algorithm may capture dif-\nferent aspects of the input image. An example\nwith two images is shown in Fig. 4, where 2 ran-\ndomly chosen model samples are considered for\neach image. For each model sample, the top 3 gen-\nerated captions are presented. We use the beam\nsearch approach (Vinyals et al., 2015) to gener-\nate captions, with a beam of size 5. In Fig. 4,\nthe two samples for the ﬁrst image mainly differ\nin the color and activity of the dog, e.g., “tan” or\n“yellow”, “playing” or “running”, whereas for the\nsecond image, the two samples reﬂect different un-\nderstanding of the image content.\n5.3 Sentence Classiﬁcation\nWe study the task of sentence classiﬁcation on 5\ndatasets as in Kiros et al. (2015): MR (Pang and\nLee, 2005), CR (Hu and Liu, 2004), SUBJ (Pang\nand Lee, 2004), MPQA (Wiebe et al., 2005) and\nTREC (Li and Roth, 2002). A single-layer bidi-\nrectional LSTM is employed with the number of\nhidden units set to 400. Table 6 shows the test-\n5 10 15\n#Epoch\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nError\nTrain\nRMSprop\nRMSprop + Dropout\npSGLD\npSGLD + Dropout\n5 10 15\n#Epoch\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\n0.22\n0.24\n0.26\nError\nValidation\n5 10 15\n#Epoch\n0.10\n0.15\n0.20\nError\nTest\nFigure 5: Learning curves on TREC dataset.\ning classiﬁcation errors. 10-fold cross-validation\nis used for evaluation on the ﬁrst 4 datasets, while\nTREC has a pre-deﬁned training/test split, and we\nrun each algorithm 10 times on TREC. The com-\nbination of pSGLD and dropout consistently pro-\nvides the lowest errors.\nIn the following, we focus on the analysis of\nTREC. Each sentence of TREC is a question, and\nthe goal is to decide which topic type the ques-\ntion is most related to: location, human, numeric,\nabbreviation, entity or description. Fig. 5 plots\nthe learning curves of different algorithms on the\ntraining, validation and testing sets of the TREC\ndataset. pSGLD and dropout have similar behav-\nior: they explore the parameter space during learn-\ning, and thus coverge slower than RMSprop on the\ntraining dataset. However, the learned uncertainty\nalleviates overﬁtting and results in lower errors on\nthe validation and testing datasets.\nTo further study the Bayesian nature of the pro-\nposed approach, in Fig. 6 we choose two test sen-\ntences with high uncertainty (i.e., standard deriva-\ntion in prediction) from the TREC dataset. Inter-\nestingly, after embedding to 2d-space with tSNE\n(Van der Maaten and Hinton, 2008), the two sen-\nTable 6: Sentence classiﬁcation errors on ﬁve benchmark datasets.\nMethods MR CR SUBJ MPQA TREC\nRMSprop 21.86±1.19 20.20±1.35 8.13±1.19 10.60±1.28 8.14±0.63\nRMSprop + Dropout 20.52±0.99 19.57±1.79 7.24±0.86 10.66±0.74 7.48±0.47\nRMSprop + Gal’s Dropout 20.22±1.12 19.29±1.93 7.52±1.17 10.59±1.12 7.34±0.66\npSGLD 20.36±0.85 18.72±1.28 7.00±0.89 10.54±0.99 7.48±0.82\npSGLD + Dropout 19.33±1.10 18.18±1.32 6.61±1.06 10.22±0.89 6.88±0.65\nWhat does ccin engines mean?\nWhat does adefibrillator do?\nTrue5 Type Predicted5 Type\nDescription\nDescription\nTesting5 Question\nEntity\nAbbreviation\nFigure 6: Visualization. Top two rows show se-\nlected ambiguous sentences, which correspond to\nthe points with black circles in tSNE visualization\nof the testing dataset.\ntences correspond to points lying on the bound-\nary of different classes. We use 20 model sam-\nples to estimate the prediction mean and standard\nderivation on the true type and predicted type. The\nclassiﬁer yields higher probability on the wrong\ntypes, associated with higher standard derivations.\nOne can leverage the uncertainty information to\nmake decisions: either manually make a human\njudgement when uncertainty is high, or automat-\nically choose the one with lower standard deriva-\ntions when both types exhibits similar prediction\nmeans. A more rigorous usage of the uncertainty\ninformation is left as future work.\n5.4 Discussion\nAblation Study We investigate the effectivenss\nof each module in the proposed algorithm in Ta-\nble 7 on two datasets: TREC and PTB. The small\nnetwork size is used on PTB. Let M1 denote only\ngradient noise, and M2 denote only model aver-\naging. As can be seen, the last sample in pSGLD\n(M1) does not necessarily bring better results than\nRMSprop, but the model averaging over the sam-\nples of pSGLD indeed provides better results than\nmodel averaging of RMSprop ( M2). This indi-\ncates that both gradient noise and model averaging\nare crucial for good performance in pSGLD.\nTable 7: Ablation study on TREC and PTB.\nDatasets RMSprop M1 M2 pSGLD\nTREC 8.14 8.34 7.54 7.48\nPTB 120.45 122.14 114.86 109.44\nTable 8: Running time on Flickr30k in seconds.\nStages pSGLD RMSprop+Dropout\nTraining 20324 12578\nTesting 7047 1311\nRunning Time We report the training and test-\ning time for image captioning on the Flickr30k\ndataset in Table 8. For pSGLD, the extra cost in\ntraining comes from adding gradient noise, and the\nextra cost in testing comes from model averaging.\nHowever, the cost in model averaging can be alle-\nviated via the distillation methods: learning a sin-\ngle neural network that approximates the results\nof either a large model or an ensemble of mod-\nels (Korattikara et al., 2015; Kim and Rush, 2016;\nKuncoro et al., 2016). The idea can be incorpo-\nrated with our SG-MCMC technique to achieve\nthe same goal, which we leave for future work.\n6 Conclusion\nWe propose a scalable Bayesian learning frame-\nwork using SG-MCMC, to model weight uncer-\ntainty in recurrent neural networks. The learn-\ning framework is tested on several tasks, includ-\ning language models, image caption generation\nand sentence classiﬁcation. Our algorithm outper-\nforms stochastic optimization algorithms, indicat-\ning the importance of learning weight uncertainty\nin recurrent neural networks. Our algorithm re-\nquires little additional computational overhead in\ntraining, and multiple times of forward-passing for\nmodel averaging in testing.\nAcknowledgments This research was supported\nby ARO, DARPA, DOE, NGA, ONR and NSF. We\nacknowledge Wenlin Wang for the code on lan-\nguage modeling experiment.\nReferences\nS. Banerjee and A. Lavie. 2005. Meteor: An automatic\nmetric for mt evaluation with improved correlation\nwith human judgments. In ACL workshop.\nJ. Bayer, C. Osendorfer, D. Korhammer, N. Chen,\nS. Urban, and P. van der Smagt. 2013. On fast\ndropout and its applicability to recurrent networks.\narXiv:1311.0701 .\nT. Bluche, C. Kermorvant, and J. Louradour. 2015.\nWhere to apply dropout in recurrent neural networks\nfor handwriting recognition? In ICDAR.\nC. Blundell, J. Cornebise, K. Kavukcuoglu, and\nD. Wierstra. 2015. Weight uncertainty in neural net-\nworks. In ICML.\nL Bottou. 2010. Large-scale machine learning with\nstochastic gradient descent. In COMPSTAT.\nC. Chen, D. Carlson, Z. Gan, C. Li, and L. Carin.\n2016. Bridging the gap between stochastic gradient\nMCMC and stochastic optimization. In AISTATS.\nC. Chen, N. Ding, and L. Carin. 2015a. On the conver-\ngence of stochastic gradient MCMC algorithms with\nhigh-order integrators. In NIPS.\nT. Chen, E. B. Fox, and C. Guestrin. 2014. Stochastic\ngradient Hamiltonian Monte Carlo. In ICML.\nX. Chen, H. Fang, T. Lin, R. Vedantam, S. Gupta,\nP. Doll ´ar, and C. L. Zitnick. 2015b. Microsoft\ncoco captions: Data collection and evaluation server.\narXiv:1504.00325 .\nK. Cho, B. Van Merri ¨enboer, C. Gulcehre, D. Bah-\ndanau, F. Bougares, H. Schwenk, and Y . Bengio.\n2014. Learning phrase representations using RNN\nencoder-decoder for statistical machine translation.\nIn EMNLP.\nJ. Chung, C. Gulcehre, K. Cho, and Y . Bengio. 2014.\nEmpirical evaluation of gated recurrent neural net-\nworks on sequence modeling. arXiv:1412.3555 .\nN. Ding, Y . Fang, R. Babbush, C. Chen, R. D. Skeel,\nand H. Neven. 2014. Bayesian sampling using\nstochastic gradient thermostats. In NIPS.\nJ. Duchi, E. Hazan, and Y . Singer. 2011. Adaptive sub-\ngradient methods for online learning and stochastic\noptimization. JMLR .\nY . Gal and Z. Ghahramani. 2016a. Dropout as a\nBayesian approximation: Representing model un-\ncertainty in deep learning. In ICML.\nY . Gal and Z. Ghahramani. 2016b. A theoretically\ngrounded application of dropout in recurrent neural\nnetworks. In NIPS.\nZ. Gan, C. Chen, R. Henao, D. Carlson, and L. Carin.\n2015. Scalable deep poisson factor analysis for topic\nmodeling. In ICML.\nZ. Gan, C. Gan, X. He, Y . Pu, K. Tran, J. Gao, L. Carin,\nand L. Deng. 2017. Semantic compositional net-\nworks for visual captioning. In CVPR.\nA. Graves. 2011. Practical variational inference for\nneural networks. In NIPS.\nA. Graves. 2013. Generating sequences with recurrent\nneural networks. arXiv:1308.0850 .\nK. He, X. Zhang, S. Ren, and J. Sun. 2016. Deep resid-\nual learning for image recognition. In CVPR.\nJ. M. Hern´andez-Lobato and R. P. Adams. 2015. Prob-\nabilistic backpropagation for scalable learning of\nBayesian neural networks. In ICML.\nG. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever,\nand R Salakhutdinov. 2012. Improving neural net-\nworks by preventing co-adaptation of feature detec-\ntors. arXiv:1207.0580 .\nS. Hochreiter and J. Schmidhuber. 1997. Long short-\nterm memory. In Neural computation.\nM. Hodosh, P. Young, and J. Hockenmaier. 2013.\nFraming image description as a ranking task: Data,\nmodels and evaluation metrics. JAIR .\nM. Hu and B. Liu. 2004. Mining and summarizing cus-\ntomer reviews. SIGKDD .\nA. Karpathy, J. Johnson, and L. Fei-Fei. 2016. Visu-\nalizing and understanding recurrent networks. In\nICLR Workshop.\nY . Kim and A. M. Rush. 2016. Sequence-level knowl-\nedge distillation. In EMNLP.\nD. Kingma and J. Ba. 2015. Adam: A method for\nstochastic optimization. In ICLR.\nD. Kingma, T. Salimans, and M. Welling. 2015. Varia-\ntional dropout and the local reparameterization trick.\nIn NIPS.\nR. Kiros, Y . Zhu, R. Salakhutdinov, R. Zemel, R. Urta-\nsun, A. Torralba, and S. Fidler. 2015. Skip-thought\nvectors. In NIPS.\nA. Korattikara, V . Rathod, K. Murphy, and M. Welling.\n2015. Bayesian dark knowledge. In NIPS.\nA. Kuncoro, M. Ballesteros, L. Kong, C. Dyer, and\nN. A. Smith. 2016. Distilling an ensemble of greedy\ndependency parsers into one mst parser. In EMNLP.\nC. Li, C. Chen, D. Carlson, and L. Carin. 2016a. Pre-\nconditioned stochastic gradient Langevin dynamics\nfor deep neural networks. In AAAI.\nC. Li, C. Chen, K. Fan, and L. Carin. 2016b. High-\norder stochastic gradient thermostats for Bayesian\nlearning of deep models. In AAAI.\nC. Li, A. Stevens, C. Chen, Y . Pu, Z. Gan, and L. Carin.\n2016c. Learning weight uncertainty with stochastic\ngradient mcmc for shape classiﬁcation. In CVPR.\nX. Li and D. Roth. 2002. Learning question classiﬁers.\nACL .\nC. Lin. 2004. Rouge: A package for automatic evalua-\ntion of summaries. In ACL workshop.\nD. J. C. MacKay. 1992. A practical Bayesian frame-\nwork for backpropagation networks. In Neural com-\nputation.\nM. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.\n1993. Building a large annotated corpus of english:\nThe penn treebank. Computational linguistics.\nT. Mikolov, M. Karaﬁ ´at, L. Burget, J. Cernock `y, and\nS. Khudanpur. 2010. Recurrent neural network\nbased language model. In INTERSPEECH.\nT. Moon, H. Choi, H. Lee, and I. Song. 2015. Rnndrop:\nA novel dropout for rnns in asr. ASRU .\nR. M. Neal. 1995. Bayesian learning for neural net-\nworks. PhD thesis, University of Toronto.\nA. Neelakantan, L. Vilnis, Q. Le, I. Sutskever,\nL. Kaiser, K. Kurach, and J. Martens. 2016. Adding\ngradient noise improves learning for very deep net-\nworks. In ICLR workshop.\nM. Pachitariu and M. Sahani. 2013. Regularization and\nnonlinearities for neural language models: when are\nthey needed? arXiv:1301.5650 .\nB. Pang and L. Lee. 2004. A sentimental education:\nSentiment analysis using subjectivity summarization\nbased on minimum cuts. ACL .\nB. Pang and L. Lee. 2005. Seeing stars: Exploiting\nclass relationships for sentiment categorization with\nrespect to rating scales. ACL .\nK. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.\nBleu: a method for automatic evaluation of machine\ntranslation. In ACL.\nR. Pascanu, T. Mikolov, and Y . Bengio. 2013. On the\ndifﬁculty of training recurrent neural networks. In\nICML.\nV . Pham, T. Bluche, C. Kermorvant, and J. Louradour.\n2014. Dropout improves recurrent neural networks\nfor handwriting recognition. In ICFHR.\nH. Robbins and S. Monro. 1951. A stochastic ap-\nproximation method. In The annals of mathematical\nstatistics.\nS. Semeniuta, A. Severyn, and E. Barth. 2016.\nRecurrent dropout without memory loss.\narXiv:1603.05118 .\nN. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever,\nand R. Salakhutdinov. 2014. Dropout: A simple\nway to prevent neural networks from overﬁtting.\nJMLR .\nI. Sutskever, J. Martens, and G. E. Hinton. 2011. Gen-\nerating text with recurrent neural networks. In\nICML.\nY . W. Teh, A. H. Thi ´ery, and S. J. V ollmer. 2016.\nConsistency and ﬂuctuations for stochastic gradient\nLangevin dynamics. JMLR .\nTheano Development Team. 2016. Theano: A Python\nframework for fast computation of mathematical ex-\npressions. arXiv:1605.02688 .\nT. Tieleman and G. Hinton. 2012. Lecture 6.5-\nrmsprop: Divide the gradient by a running average\nof its recent magnitude. Coursera: Neural Networks\nfor Machine Learning.\nL. Van der Maaten and G. E. Hinton. 2008. Visualizing\ndata using t-SNE. JMLR .\nR. Vedantam, C. L. Zitnick, and D. Parikh. 2015.\nCider: Consensus-based image description evalua-\ntion. In CVPR.\nO. Vinyals, A. Toshev, S. Bengio, and D. Erhan. 2015.\nShow and tell: A neural image caption generator. In\nCVPR.\nL. Wan, M. Zeiler, S. Zhang, Y . LeCun, and R. Fer-\ngus. 2013. Regularization of neural networks using\nDropConnect. In ICML.\nS. Wang and C. Manning. 2013. Fast Dropout training.\nIn ICML.\nM. Welling and Y . W. Teh. 2011. Bayesian learning via\nstochastic gradient Langevin dynamics. In ICML.\nP. Werbos. 1990. Backpropagation through time: what\nit does and how to do it. InProceedings of the IEEE.\nJ. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating\nexpressions of opinions and emotions in language.\nLanguage resources and evaluation.\nP. Young, A. Lai, M. Hodosh, and J. Hockenmaier.\n2014. From image descriptions to visual denota-\ntions: New similarity metrics for semantic inference\nover event descriptions. TACL .\nW. Zaremba, I. Sutskever, and O. Vinyals. 2014.\nRecurrent neural network regularization.\narXiv:1409.2329 .",
  "topic": "Recurrent neural network",
  "concepts": [
    {
      "name": "Recurrent neural network",
      "score": 0.7728656530380249
    },
    {
      "name": "Overfitting",
      "score": 0.7531143426895142
    },
    {
      "name": "Computer science",
      "score": 0.7432923316955566
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6406909227371216
    },
    {
      "name": "Machine learning",
      "score": 0.6129528880119324
    },
    {
      "name": "Markov chain Monte Carlo",
      "score": 0.5967721343040466
    },
    {
      "name": "Scalability",
      "score": 0.5671447515487671
    },
    {
      "name": "Stochastic gradient descent",
      "score": 0.5330134034156799
    },
    {
      "name": "Language model",
      "score": 0.48301681876182556
    },
    {
      "name": "Bayesian probability",
      "score": 0.41774532198905945
    },
    {
      "name": "Artificial neural network",
      "score": 0.40600085258483887
    },
    {
      "name": "Database",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 7
}