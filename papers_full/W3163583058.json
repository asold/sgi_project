{
  "title": "IntFormer: Predicting pedestrian intention with the aid of the Transformer architecture",
  "url": "https://openalex.org/W3163583058",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5101675241",
      "name": "Javier Lorenzo",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5027189195",
      "name": "I. Parra",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5002294104",
      "name": "Miguel Ángel Sotelo",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3119170582",
    "https://openalex.org/W2990152177",
    "https://openalex.org/W2968524820",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W2968684599",
    "https://openalex.org/W3010309798",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3025952933",
    "https://openalex.org/W3112125472",
    "https://openalex.org/W3113370935",
    "https://openalex.org/W3106257603",
    "https://openalex.org/W2991484432",
    "https://openalex.org/W2786890432",
    "https://openalex.org/W3101367838",
    "https://openalex.org/W2559085405",
    "https://openalex.org/W3118834000"
  ],
  "abstract": "Understanding pedestrian crossing behavior is an essential goal in intelligent vehicle development, leading to an improvement in their security and traffic flow. In this paper, we developed a method called IntFormer. It is based on transformer architecture and a novel convolutional video classification model called RubiksNet. Following the evaluation procedure in a recent benchmark, we show that our model reaches state-of-the-art results with good performance ($\\approx 40$ seq. per second) and size ($8\\times $smaller than the best performing model), making it suitable for real-time usage. We also explore each of the input features, finding that ego-vehicle speed is the most important variable, possibly due to the similarity in crossing cases in PIE dataset.",
  "full_text": "IntFormer: Predicting pedestrian intention with the aid of the\nTransformer architecture\nJ. Lorenzo1, I. Parra 1 and M. A. Sotelo 1\nAbstract— Understanding pedestrian crossing behavior is an\nessential goal in intelligent vehicle development, leading to\nan improvement in their security and trafﬁc ﬂow. In this\npaper, we developed a method called IntFormer. It is based\non transformer architecture and a novel convolutional video\nclassiﬁcation model called RubiksNet. Following the evaluation\nprocedure in a recent benchmark, we show that our model\nreaches state-of-the-art results with good performance ( ≈ 40\nseq. per second) and size ( 8×smaller than the best performing\nmodel), making it suitable for real-time usage. We also explore\neach of the input features, ﬁnding that ego-vehicle speed is\nthe most important variable, possibly due to the similarity in\ncrossing cases in Pedestrian Intention Estimation (PIE) dataset.\nI. I NTRODUCTION\nSince the beginning of this century, several efforts have\nbeen made to improve road safety. According to data pro-\nvided by the World Health Organization (WHO) [1], the\nrate of road trafﬁc deaths relative to world’s population has\nstabilized and declined relative to the number of vehicles,\nbut the progress is not enough to achieve a 50% reduction\nby 2020 (SDG target 3.6) [2].\nIn European Union (EU), pedestrians are the most affected\nVulnerable Road Users (VRUs) group in urban roads, rep-\nresenting nearly the 40% of deaths, as stated by European\nTransport Safety Council (ETSC) [3]. For this reason, urban\nscenarios driving, one of the major challenges that Intelligent\nVehicles (IV) are facing nowadays, must be tackled. In\naddition to obstacle detection and other Advanced Driver-\nAssistance Systems (ADAS), the behavior anticipation of\ndynamic trafﬁc participants focused on the crossing action\ncan lead to a risk reduction and a trafﬁc ﬂow improvement.\nOver the last decade, human action recognition ﬁeld\nhas advanced greatly thanks to deep learning (read [4] for\ndetailed information). Most of this methods, rely on hidden\npatterns found in camera sensor data to predict the correct\naction being performed by humans. Pedestrian crossing be-\nhavior anticipation can be viewed as an special case of action\nrecognition, where the model, instead of predicting the action\nperformed through the video, predicts the ﬁnal action of the\nsequence with a limited observation interval as input.\nSince the publication of Joint Attention in Autonomous\nDriving (JAAD) dataset [5], several methods based on neural\nnetworks tried to deal with this challenging task by exploiting\ntemporal information. However, without a common evalua-\ntion benchmark, the study of their performance was nearly\nimpossible. Early this year, JAAD and PIE authors published\n1 Department of Computer Engineering, Universidad de Alcal ´a, Madrid,\nSpain {javier.lorenzod, ignacio.parra, miguel.sotelo}@uah.es\nencoder encoder\nFusion\nt + TTE\nWill the pedestrian cross?\n... \nt - T \n + \nt\n... \nFig. 1: Diagram describing the proposed method. Data sequences\nof different nature (red means image data and blue non-image) are\nencoded and fused in order to estimate the ﬁnal crossing behavior\nof the target pedestrian, which we refer to as intention.\na benchmark [6] for both datasets. Several baselines were\ntrained and evaluated and a state-of-the-art method called\nPCPA was proposed.\nIn this work, we propose a new method, IntFormer, which\nuses a state-of-the-art video action recognition model called\nRubiksNet for image bounding box sequences encoding and\na transformer-based encoder for non-image data. It follows\na similar architecture to the one proposed in PCPA model\nbut trying to improve its performance while lowering its\ncomputational cost. To summarize, our contributions in this\npaper are:\n• A novel composite model for pedestrian action antic-\nipation task. The proposed model is based on a novel\nvideo action recognition architecture, called RubiksNet\n[7] and transformer architecture.\n• State-of-the-art results with a lighter model , more\nsuitable for real-time applications.\n• Selection of input features is key to better results:\nexploratory experiments are needed to understand the\ndata used as input to the model.\n• Different preprocessing strategies for input data used\nin the benchmark.\n• Faster training schedule , by using novel optimization\ntechniques.\narXiv:2105.08647v1  [cs.CV]  18 May 2021\nII. R ELATED WORK\nTwo main approaches related to pedestrian crossing pre-\ndiction. The ﬁrst one is the human motion-based approach,\nwhere pedestrian behavior is implicitly extracted from the\nforecasted trajectory data (see [8] for a detailed review of this\nmethods). The other branch, focuses on explicitly extracting\npedestrian behavior from input data. The most extended\nsource of information is video, and many of the recently\npublished systems rely on techniques inherited from video\naction recognition ﬁeld (see [4] for a detailed survey on this\nmethods). In this ﬁeld, the objective of the network is the es-\ntimation of the most suitable action class for an input video.\nHowever, this prediction is done for the current time, and\nin pedestrian crossing prediction the goal is on anticipation.\nIn [9], the authors construct a complete pedestrian detection\npipeline, with a 3D CNN-based network as the pedestrian\ncrossing prediction system. In [10], a similar method is\ndeveloped, which improves the previous one by including\npose information extracted with a 2D CNN model. In [11],\n[12], 3D CNN model is used for perform future frame\nprediction and crossing classiﬁcation on that prediction. In\n[13], [14], [15], [6], a combination of 2D and recurrent based\nmodels are used, and in [6] a 3D CNN backbone is used\ninstead of a 2D one, due to its temporal advantages exploiting\ndifferent sources of information rather than only image.\nOne of the main problems in this line of research is the\nlack of benchmarks, which does not allow a fair comparison\nbetween the different methods in the literature. As explained\nbefore, in [6], a benchmark and several baselines are pro-\nvided based on JAAD dataset [5] and PIE dataset [13]. It\nis based on the problem of action anticipation rather than\nprediction as previous methods. Instead of predicting\nIII. M ETHOD\nA. Problem formulation\n1) Task description: Pedestrian crossing action anticipa-\ntion is deﬁned as a binary classiﬁcation task. Sequential\ninformation, recorded from the ego-vehicle, is used to predict\nthe ﬁnal crossing action of the target pedestrian track. Instead\nof using the whole length as in previous work, the last frame\nof observation t is between 1 and 2 s (30 to 60 frames)\nbefore the event (pedestrian starts to cross or the last frame\nwhere the pedestrian is observable in the case of non-crossing\nscenario). The input observation N is ≈ 0.5 s (N = 16 frames\nat 30 fps). The prediction horizon varies as explained before,\nwith a minimum of 1 s and a maximum of 2 s, which we will\nrefer to as Time to event (TTE). For this reason, the time\ninstant when pedestrian behavior is predicted is t + T T E.\n2) Input features: In the proposed model, there are three\nmain sources of information, all of them extracted from\nground truth data available in the datasets or precomputed:\n• Raw image : Sequence of bounding boxes crops of\npedestrians. In this case, we only use the local box\ncase of the benchmark, where only the image comprised\ninside the bounding box coordinates is used.\n• Bounding boxes image coordinates : sequence of co-\nordinates, to support raw image data providing localiza-\ntion of pedestrian in the image scene.\n• Pose keypoints : using the ones provided and used by\nPCPA model, obtained by using OpenPose [16]. Being\nthe output of a network, there are failure cases, where\ndifferent limbs are not found on the pedestrian (zero\nvalues).\n• Ego-vehicle speed : obtained from OBD sensor in PIE\ndataset and as a categorical value in JAAD. As it is not\nused in JAAD training of PCPA model, we do not use\nit either for a fairer comparison.\nB. Model architecture\n(1x1)Fusion\nEncoders\nBBs img. \n(N//2x112x112x3)\nBBs coord. \n(Nx4)\nPose kps. \n(Nx36)\nEgo Speed \n(Nx1)\n(1x432)RubiksNet\n(1x128)Transformer\nProb.\nFig. 2: Diagram of proposed model in detail . Raw image\nsequence data of target pedestrian (delimited by bounding box\ncoordinates) is used as input to RubiksNet [7] backbone to obtain\na feature vector. Bounding box coordinates, pose keypoints and\nego-vehicle speed sequences are introduced to a transformer-based\nencoder which outputs a combined feature of these non-image fea-\ntures. Both outputs are introduced to the fusion block, concatenating\nthem or applying many-to-one Luong attention.\nWe present an end-to-end model, composed of three main\nparts:\n• Raw video encoder : composed by a pretrained spatio-\ntemporal architecture. 3D CNN models have proven to\nbe a better alternative to recurrent models in tasks where\nimage data is used, such as action classiﬁcation (see [4]\nfor a detailed analysis of this technique). However, they\nare computationally expensive. Alternatives based on\n2D convolutions with lower computational cost are also\nproposed in the literature [17]. In this model, temporal\nconvolutions are replaced by channel-wise shifts oper-\nations. However 2D convolutions in this model remain\nthe same. For this reason, in this block we planned to\nuse RubiksNet [7], an efﬁcient architecture for video\naction recognition that learns 3D-Shift operations and\nreduces the number of 2D convolutions, keeping a low\nnumber of parameters and FLOPS in comparison to\nother state-of-the-art video action recognition methods.\n• Non-image data encoder : based on transformer ar-\nchitecture. This architecture was proposed in [18] for\nNLP tasks, outperforming recurrent and convolutional\nmodels, which were state-of-the-art at that moment.\nRecently, several works have been published looking for\nways to apply them to computer vision tasks [19]. This\narchitecture relies on a self-attention mechanism, but,\ninstead of using recurrence, transformer models create\nrelationships between all samples in the input sequence,\nallowing parallelization and better use of modern de-\nvices such as TPUs and GPUs. In our case, we only\nkeep the encoder part, using the output features as a\nrepresentation. We combined all non-image sequence\ndata as the input of a unique transformer encoder. After\nobtaining the combined output (ﬁxed-size feature vector\nfor each non-image sequence), we obtain the mean\nbetween all of them and apply a layer normalization\nand a fully connected layer.\n• Fusion module: block in charge of merging features of\nthe previous branches. It consists of a fully connected\nlayer, whose input is composed of the concatenation of\nthe outputs of previous encoders. An additional fully\nconnected layer is used to obtain the ﬁnal output logit.\nAfter the ﬁrst fully connected layer, we apply a dropout\nof 0.5 and a ReLU activation.\nIn the ﬁgure 2 a detailed description of the model is shown.\nC. Data preprocessing\nIn this section, we will discuss the operations performed\non input data before using it as input to the network.\n1) Raw image data: : after the crop operation we obtain\na sequence of N images related to the target pedestrian. The\nbenchmark established an input time length of ≈ 0.5 s which\ncorresponds to 16 frames. In our experiments, We halved that\nnumber, sampling the input time interval at 15 fps instead\nof 30 fps, keeping sequences with N/2 = 8 evenly spaced\nframes. Images are resized to square dimension 112×112×3\n(H ×W ×C format). The input batch of the ResNet 3D model\nconsists of a ﬁve-dimensional vector of shape B×C ×N/2×\nH ×W. In the case of RubiksNet, the input batch is a four-\ndimensional vector because it used bidimensional kernels.\nThe shape is B × N·C\n2 ×H ×W.\nNo data augmentation is performed on the image. Min-\nmax normalization is performed on input data for RubiksNet\nbackbone.\n2) Bounding boxes coordinates: In the benchmark, they\nused relative changes in bounding box coordinates concern-\ning the ﬁrst bounding box in the sequence. In our work, we\nperform a min-max normalization using image dimension\n(1920 ×1080 pixel).\n3) Pose keypoints:Pose information is used the same way\nas in benchmark, performing a min-max normalization using\nthe image dimension commented previously.\n4) Ego-vehicle speed: In the benchmark, this variable is\nused without normalization. We performed a z-score normal-\nization using the training set mean and standard deviation in\nall parts of the experiments (training, validation, and testing).\nFor the previous three non-image features, sequence length\nN is used.\nIV. E VALUATION\nA. Data\nWe have trained our models on both datasets, PIE, and\nJAAD. Inside JAAD, there are two variants, JAAD all and\nJAADbeh. In the ﬁrst case, it contains all annotated pedes-\ntrians and in the second, only pedestrians with behavioral\nannotations. This second case is the only dataset used in the\nbenchmark with more crossing cases. Crossing class in PIE\nand complete JAAD are underrepresented.\nB. Training details\nAll models in the performed experiments were trained\nusing PyTorch framework through a PyTorch Lightning\nwrapper [20]. During training, we have used different hy-\nperparameters depending on the dataset:\n• PIE: batch size of 8. After a small automatic hyperpa-\nrameter search, we obtained the following parameters:\nRubiksNet learning rate of 1 .1 ·10−3, RubiksNet shift\nlayers multiplier of 6.5·10−4 and transformer encoder’s\nlearning rate of 4 .3 ·10−3. Using Adam optimizer with\nthe default conﬁguration of PyTorch.\n• JAADbeh: batch size of 8. Unique learning rate of\n10−4 (10−5 for shift layers). Using AdamW [21] for\nfaster convergence and a weight decay of 10 −3. In this\nexperiment we obtained the results using only image\nand coordinates of the pedestrian bounding boxes.\n• JAADall: same training conﬁguration than in the pre-\nvious case, with a weight decay of 10 −4 and unique\nlearning rate of 3 ·10−4.\nWe have trained all our models on a Geforce GTX TITAN\nX (Pascal architecture) with a CPU i5-4690K 3.50 GHz.\n1) Loss function: We used binary cross-entropy loss.\nBecause the benchmark is constructed over imbalanced data,\nwe apply a weight Wc for the positive class (crossing) to\nvirtually equalize the weight of both classes. This weight\ncalculation is shown in 1. Mnc is the number of non-crossing\ncases. Mc is the number of crossing cases. Both numbers are\ncalculated in the training set.\nWc = Mnc/Mc (1)\nC. Evaluation Metrics\nWe use the same metrics as in the benchmark: accuracy,\nF1-Score, and area under the ROC curve (AUC).\nD. Baselines\nWe compare our proposed architecture with the best\nperforming one in the benchmark, called PCPA [6]. This\nnetwork is composed of a 3D convolutional network called\nC3D pretrained on Sports-1M dataset. This network is used\nas a video encoder and uses the full video ( N = 16 frames).\nFor non-image data, it uses Gated Recurrent Unit (GRU)\nunits and it uses two attention modules: one applied to the\ntemporality and the other applied as the fusion strategy.\nFinally, it uses a fully connected layer to output the binary\nprediction.\nModel Fusion # P PIE JAAD beh JAAD all\nAcc. AUC F1 Acc. AUC F1 Acc. AUC F1\nPCPA[6] A 31.2M 0.86 0.86 0.77 0.58 0.50 0.71 0.85 0.86 0.68\nPCPA (ours) 0.86 0.86 0.78 - - - 0.84 0.74 0.56\nIntFormer C 4M 0.89 0.92 0.81 0.59 0.54 0.69 0.86 0.78 0.62\nTABLE I: Results in the benchmark. # P means number of parameters. PCPA (Ours) is the model trained in our machine in order to\nreproduce PCPA results.\nE. Input data importance\nIn parallel with the benchmark, we experimented focused\non the importance of the input data. Training on PIE dataset,\nwe begin with the only video part of the model, and with\neach experiment, we include one feature. We also trained\nthe model with one feature at a time, to see how each of\nthem affects the performance of the model. For all cases,\nwe used the same hyperparameters: AdamW optimizer with\nweight decay of 10 −4 and unique learning rate of 3 ·10−4,\nand batch size of 8 sequences.\nV. R ESULTS AND DISCUSSION\nIn table I, our proposed model is compared with PCPA.\nThe best results in PIE dataset, which is the most diverse\none, belongs to our model. It uses the same data, but\nthe preprocessing is different (bounding boxes coordinates\nnormalized instead of relative differences, image data at\nhalved framerate, and normalized ego-vehicle speed). Instead\nof relying on an attention external block, our model performs\na simple fusion by concatenating both groups of features\n(non-image and image data), which conforms the input of\na fully connected layer. With respect to the model size, our\nmodel has a 13% of PCPA parameter number and runs at\n40 sequences per second. Experiments with trained PCPA\nmodels (following ofﬁcial code) show a speed of ≈ 22.2s.\nHowever, the ofﬁcial code uses Tensorﬂow 2 instead of\nPyTorch, and the model occupies 11 .479 GB of memory\nwith a batch size of 1. Our model, with the same tests,\noccupies less than 10% (1 .2 GB). With respect to the F1\nScore, we observe that our model obtain a better value, but\nrecall is better in PCPA (trained by ours), which means that\nthe crossing cases are better covered. However, precision\nis nearly 8% better in our model, which shows that it\nunderstands better the separation between the two cases.\nWith respect to JAAD, although we obtained similar\nresults than PCPA with the behavioral data, our AUC result\nshows that our model performs better than the random\ncase. In PCPA case, this value is near or equal to random\nperformance (AUC = 0.5).\nFinally, results on PCPA using complete JAAD data show\nbetter results for PCPA model. However, after training PCPA\nwith the ofﬁcial code on JAAD, we obtained worse results\nthan our model. This could be due to the instability in\ntraining and the difﬁculty of the task. In both cases, the model\nperforms better than in the random case.\nLooking at the training time, our model takes advantage.\nOur training on PIE spends ≈ 8 hours in PCPA (80 epochs).\nIn the paper, they spent, assuming they only change the\nnumber of epochs, 6 hours (60 epochs). In our case, ≈13min\nare used in training (less than a 4%). Training on JAAD\nfollows a similar trend.\nA. Input data importance\nImgs BBs Pose Speed Acc. AUC F1\n✓ - - - 0.750 0.668 0.519\n- ✓ - - 0.739 0.571 0.287\n- - ✓ - 0.719 0.500 0.000\n- - - ✓ 0.860 0.817 0.743\n- - ✓ ✓ 0.837 0.829 0.737\n- ✓ - ✓ 0.851 0.846 0.760\n✓ - - ✓ 0.814 0.789 0.689\n- ✓ ✓ - 0.719 0.500 0.000\n✓ - ✓ - 0.765 0.682 0.541\n- ✓ ✓ ✓ 0.822 0.832 0.730\n✓ ✓ ✓ - 0.745 0.675 0.527\n✓ - ✓ ✓ 0.824 0.796 0.697\n✓ ✓ - ✓ 0.849 0.826 0.743\n✓ ✓ - - 0.714 0.620 0.448\n✓ ✓ ✓ ✓ 0.823 0.821 0.722\nTABLE II: Results for the experiments carried out to analyze\nthe importance of each input features. “Imgs” is bounding boxes\nimage crops, “BBs” is bounding boxes coordinates, “Pose” is pose\nkeypoints (precomputed) and “Speed” is ego-vehicle speed.\nIn table II, results for all combination of inputs is dis-\nplayed. Individually, only pose information is irrelevant and\nthe model behaves similarly to the random choice case\n(AUC = 0.5). This can be caused by the precomputed origin\nof this feature, which raises problems such as incorrect or\nmissing detections and occlusions by other road participants.\nThe second less informative variable is the bounding box\ncoordinates. It is hand-labeled and obtains better individual\nresults than pose. However, the lack of relation between\nsamples can be the reason that leads to a worse temporal\nunderstanding by the transformer encoder. In the case of\nbounding boxes images, it is clear that this is a more\ninformative variable than the previous ones, due to the\ninclusion of context on the image. Nevertheless, image data\nhas large variability, and it is nearly impossible to generalize\nonly with the available data. As the ﬁnal and best feature,\nego-vehicle speed achieves the best single performance, with\na long advantage concerning the rest.\nThese results may mean that the crossing cases are closely\nrelated to the changes in speed, which could translate, for\nexample, to a majority of crossing situations at zebra cross-\nings, where pedestrians behave similarly. Also, this variable\nis unique for each sequence, so several pedestrians share\nthe same speed. Looking at the experiments using combined\nfeatures it is clear that image bounding boxes beneﬁt from\nnon-image data processed with the transformer in a great\nway. The best case though, corresponds to the combination of\nbounding box coordinates and speed. Maybe this is caused by\nthe low dimensionality of coordinates, which combined with\nthe location information can lead to a better understanding of\neach pedestrian case, leveraging a distinction between cases.\nPose information affects negatively in most of the cases,\nbut the combination of features is not enough to assure that\nworse performance is caused by this feature since there are\ncases where it improves.\nFinally, the best performing model in this set relies only\non the transformer model and achieves similar results to the\nones of PCPA model in table I.\nVI. C ONCLUSIONS\nIn this work, we have presented a novel architecture,\ncalled IntFormer, for the task of pedestrian crossing intention\nprediction. It is based on a novel video action recognition\narchitecture called RubiksNet and transformer architecture.\nAn exploratory study of the importance of variables has\nshown the big relation between ego-vehicle speed and the\ncrossing intention, which can mean that most of the crossing\ncases in PIE dataset shares a similar pattern. Our model\nhas been ranked on a recently published benchmark [6],\nachieving state-of-the-art results with less data, computa-\ntional resources, and training time.\nIn the future, a deep analysis of datasets PIE and JAAD\ncan be done to check the importance of each variable.\nAdditional precomputed variables can be included to replace\nor complement the pose keypoints. Training pipeline op-\ntimization can be improved, by automatically looking for\nbetter hyperparameters.\nREFERENCES\n[1] World Health Organization, Global status report on road safety 2018,\n2018.\n[2] ——, “Global gathering of ministers determines road safety agenda\nto 2030,” p. 1, 2020. [Online]. Available: https://www.who.int/news/i\ntem/20-02-2020-global-gathering-of-ministers-determines-road-safet\ny-agenda-to-2030\n[3] D. Adminait `e-Fodor and G. Jost, “SAFER ROADS , SAFER CITIES\n: HOW TO IMPROVE URBAN ROAD SAFETY IN THE EU PIN\nFlash Report 37,” no. June, 2019.\n[4] Y . Zhu, X. Li, C. Liu, M. Zolfaghari, Y . Xiong, C. Wu, Z. Zhang,\nJ. Tighe, R. Manmatha, and M. Li, “A Comprehensive Study of\nDeep Video Action Recognition,” 12 2020. [Online]. Available:\nhttp://arxiv.org/abs/2012.06567\n[5] A. Rasouli and J. K. Tsotsos, “Joint Attention in Driver-Pedestrian\nInteraction: from Theory to Practice,” 2018. [Online]. Available:\nhttps://arxiv.org/pdf/1802.02522.pdf\n[6] I. Kotseruba, A. Rasouli, and J. K. Tsotsos, “Benchmark for\nEvaluating Pedestrian Action Prediction,” Tech. Rep. [Online].\nAvailable: https://github.com/ykotseruba/PedestrianActionBenchmark\n[7] L. Fan*, S. Buch*, G. Wang, R. Cao, Y . Zhu, J. C. Niebles, and\nL. Fei-Fei, “RubiksNet: Learnable 3D-Shift for Efﬁcient Video Action\nRecognition,” in Proceedings of the European Conference on Com-\nputer Vision (ECCV), 2020.\n[8] A. Rudenko, L. Palmieri, M. Herman, K. M. Kitani, D. M. Gavrila,\nand K. O. Arras, “Human motion trajectory prediction: a survey,”\nInternational Journal of Robotics Research , vol. 39, no. 8, pp.\n895–935, 7 2020. [Online]. Available: www.sagepub.com/\n[9] K. Saleh, M. Hossny, and S. Nahavandi, “Real-time Intent\nPrediction of Pedestrians for Autonomous Ground Vehicles via\nSpatio-Temporal DenseNet,” 4 2019. [Online]. Available: http:\n//arxiv.org/abs/1904.09862\n[10] F. Piccoli, R. Balakrishnan, M. J. Perez, M. Sachdeo, C. Nunez,\nM. Tang, K. Andreasson, K. Bjurek, R. D. Raj, E. Davidsson,\nC. Eriksson, V . Hagman, J. Sjoberg, Y . Li, L. S. Muppirisetty, and\nS. Roychowdhury, “FuSSI-Net: Fusion of Spatio-temporal Skeletons\nfor Intention Prediction Network,” arXiv, 5 2020. [Online]. Available:\nhttp://arxiv.org/abs/2005.07796\n[11] P. Gujjar and R. Vaughan, “Classifying Pedestrian Actions In\nAdvance Using Predicted Video Of Urban Driving Scenes,”\nin 2019 International Conference on Robotics and Automation\n(ICRA). IEEE, 5 2019, pp. 2097–2103. [Online]. Available:\nhttps://ieeexplore.ieee.org/document/8794278/\n[12] M. Chaabane, A. Trabelsi, N. Blanchard, and R. Beveridge, “Looking\nahead: Anticipating pedestrians crossing with future frames predic-\ntion,” in Proceedings - 2020 IEEE Winter Conference on Applications\nof Computer Vision, WACV 2020. Institute of Electrical and Elec-\ntronics Engineers Inc., 3 2020, pp. 2286–2295.\n[13] A. Rasouli, I. Kotseruba, T. Kunic, and J. K. Tsotsos, “PIE: A\nLarge-Scale Dataset and Models for Pedestrian Intention Estimation\nand Trajectory Prediction,” Tech. Rep., 2019. [Online]. Available:\nhttp://data.nvision2.\n[14] A. Rasouli, M. Rohani, and J. Luo, “Pedestrian Behavior Prediction\nvia Multitask Learning and Categorical Interaction Modeling,” 12\n2020. [Online]. Available: http://arxiv.org/abs/2012.03298\n[15] J. Lorenzo, I. Parra, F. Wirth, C. Stiller, D. F. Llorca, and\nM. A. Sotelo, “RNN-based Pedestrian Crossing Prediction using\nActivity and Pose-related Features,” in IEEE Intelligent Vehicles\nSymposium, Proceedings. Institute of Electrical and Electronics\nEngineers Inc., 8 2020, pp. 1801–1806. [Online]. Available:\nhttp://arxiv.org/abs/2008.11647\n[16] Z. Cao, T. Simon, S.-E. Wei, and Y . Sheikh, “Realtime Multi-Person\n2D Pose Estimation using Part Afﬁnity Fields,” 11 2016. [Online].\nAvailable: https://arxiv.org/abs/1611.08050\n[17] J. Lin, C. Gan, and S. Han, “TSM: Temporal Shift Module for\nEfﬁcient Video Understanding,”Proceedings of the IEEE International\nConference on Computer Vision, vol. 2019-Octob, pp. 7082–7092, 11\n2018. [Online]. Available: http://arxiv.org/abs/1811.08383\n[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is All you Need,”\nArXiv, vol. abs/1706.0, 2017.\n[19] S. Khan, M. Naseer, M. Hayat, S. Waqas Zamir, F. Shahbaz Khan,\nand M. Shah, “Transformers in Vision: A Survey,” Tech. Rep.\n[20] e. a. Falcon W A, “PyTorch Lightning,” GitHub. Note:\nhttps://github.com/PyTorchLightning/pytorch-lightning, vol. 3, 2019.\n[21] I. Loshchilov and F. Hutter, “Decoupled Weight Decay Regulariza-\ntion,” 2019.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7467222213745117
    },
    {
      "name": "Pedestrian",
      "score": 0.706540584564209
    },
    {
      "name": "Computer science",
      "score": 0.6939811706542969
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6198461651802063
    },
    {
      "name": "Architecture",
      "score": 0.5942753553390503
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48062270879745483
    },
    {
      "name": "Pedestrian crossing",
      "score": 0.439348429441452
    },
    {
      "name": "Machine learning",
      "score": 0.33565789461135864
    },
    {
      "name": "Engineering",
      "score": 0.20150384306907654
    },
    {
      "name": "Transport engineering",
      "score": 0.18465390801429749
    },
    {
      "name": "Cartography",
      "score": 0.06902638077735901
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 15
}