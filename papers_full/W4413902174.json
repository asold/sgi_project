{
    "title": "Harnessing large vision and language models in agriculture: a review",
    "url": "https://openalex.org/W4413902174",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A5101713680",
            "name": "Hongyan Zhu",
            "affiliations": [
                null,
                "Guangxi Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A5101702962",
            "name": "Shuai Qin",
            "affiliations": [
                null,
                "Guangxi Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A5101136576",
            "name": "Su Min",
            "affiliations": [
                null,
                "Guangxi Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A5113315330",
            "name": "Chengzhi Lin",
            "affiliations": [
                null,
                "Guangxi Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A5110434741",
            "name": "Anting Li",
            "affiliations": [
                null,
                "Guangxi Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A5100693408",
            "name": "Junfeng Gao",
            "affiliations": [
                null,
                "University of Aberdeen"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2914859151",
        "https://openalex.org/W4386252066",
        "https://openalex.org/W4377121468",
        "https://openalex.org/W4399235601",
        "https://openalex.org/W4386185600",
        "https://openalex.org/W2960779155",
        "https://openalex.org/W4200220040",
        "https://openalex.org/W4387389466",
        "https://openalex.org/W4398794940",
        "https://openalex.org/W3114855947",
        "https://openalex.org/W3195577433",
        "https://openalex.org/W2930246464",
        "https://openalex.org/W4220727424",
        "https://openalex.org/W4360836968",
        "https://openalex.org/W4381800089",
        "https://openalex.org/W4293763776",
        "https://openalex.org/W4392019983",
        "https://openalex.org/W4376122773",
        "https://openalex.org/W3177813494",
        "https://openalex.org/W4361212918",
        "https://openalex.org/W4322759378",
        "https://openalex.org/W4288514369",
        "https://openalex.org/W4392270904",
        "https://openalex.org/W4362716218",
        "https://openalex.org/W4394806030",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W4323572061",
        "https://openalex.org/W2917883159",
        "https://openalex.org/W4285093919",
        "https://openalex.org/W4389984066",
        "https://openalex.org/W4385483792",
        "https://openalex.org/W2017561954",
        "https://openalex.org/W4409320928",
        "https://openalex.org/W4406779522",
        "https://openalex.org/W2394911398",
        "https://openalex.org/W4390698530",
        "https://openalex.org/W4303440777",
        "https://openalex.org/W3155263273",
        "https://openalex.org/W2927351257",
        "https://openalex.org/W4393906060",
        "https://openalex.org/W3012975023",
        "https://openalex.org/W3106881551",
        "https://openalex.org/W2618530766",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W4308760226",
        "https://openalex.org/W4388954779",
        "https://openalex.org/W4400517345",
        "https://openalex.org/W3121259022",
        "https://openalex.org/W4385848742",
        "https://openalex.org/W1991558333",
        "https://openalex.org/W4390807564",
        "https://openalex.org/W4365600681",
        "https://openalex.org/W3083926560",
        "https://openalex.org/W3046429102",
        "https://openalex.org/W4383346782",
        "https://openalex.org/W179875071",
        "https://openalex.org/W3111662681",
        "https://openalex.org/W4313827776",
        "https://openalex.org/W4206338051",
        "https://openalex.org/W4386134587",
        "https://openalex.org/W4385681528",
        "https://openalex.org/W4224035735",
        "https://openalex.org/W639708223",
        "https://openalex.org/W3130016944",
        "https://openalex.org/W2946200149",
        "https://openalex.org/W4249778265",
        "https://openalex.org/W4206349320",
        "https://openalex.org/W3156943756",
        "https://openalex.org/W4361866031",
        "https://openalex.org/W2962835968",
        "https://openalex.org/W4379645258",
        "https://openalex.org/W2402268235",
        "https://openalex.org/W2759879112",
        "https://openalex.org/W4390041933",
        "https://openalex.org/W6947991961",
        "https://openalex.org/W4392822465",
        "https://openalex.org/W2966160658",
        "https://openalex.org/W2530592706",
        "https://openalex.org/W4400377778",
        "https://openalex.org/W4213428251",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4376130103",
        "https://openalex.org/W4387332086",
        "https://openalex.org/W4388093429",
        "https://openalex.org/W4385468994",
        "https://openalex.org/W4286898377",
        "https://openalex.org/W4391094120",
        "https://openalex.org/W4385373931",
        "https://openalex.org/W4361866125",
        "https://openalex.org/W4394782456",
        "https://openalex.org/W4390437013",
        "https://openalex.org/W4385632485",
        "https://openalex.org/W4377158284",
        "https://openalex.org/W4320890312",
        "https://openalex.org/W4367000115",
        "https://openalex.org/W4404986161",
        "https://openalex.org/W4391132127",
        "https://openalex.org/W4402426725",
        "https://openalex.org/W4367367040",
        "https://openalex.org/W4382132560",
        "https://openalex.org/W3127736148",
        "https://openalex.org/W2969545732",
        "https://openalex.org/W4379918953",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W4310193739",
        "https://openalex.org/W4386002638",
        "https://openalex.org/W4383374753",
        "https://openalex.org/W4362515116",
        "https://openalex.org/W4362614700",
        "https://openalex.org/W4366851181",
        "https://openalex.org/W3193570173",
        "https://openalex.org/W2576683119",
        "https://openalex.org/W4366330503",
        "https://openalex.org/W4390872297",
        "https://openalex.org/W2097117768",
        "https://openalex.org/W3165938262",
        "https://openalex.org/W4403842540",
        "https://openalex.org/W4312933868",
        "https://openalex.org/W4401164252",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2962843773",
        "https://openalex.org/W4390190628",
        "https://openalex.org/W3129576130",
        "https://openalex.org/W4311642023",
        "https://openalex.org/W4311000453",
        "https://openalex.org/W4297685771",
        "https://openalex.org/W3198602183",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3006588924",
        "https://openalex.org/W2963037989",
        "https://openalex.org/W4317242784",
        "https://openalex.org/W2140679639",
        "https://openalex.org/W4386076222",
        "https://openalex.org/W4283172211",
        "https://openalex.org/W4386071707",
        "https://openalex.org/W2560609797",
        "https://openalex.org/W4402572436"
    ],
    "abstract": "Introduction Agriculture is a cornerstone of human society but faces significant challenges, including pests, diseases, and the need for increased production efficiency. Large models, encompassing large language models, large vision models, and multimodal large language models, have shown transformative potential in various domains. This review aims to explore the potential applications of these models in agriculture to address existing problems and improve production. Methods We conduct a systematic review of the development trajectories and key capabilities of large models. A bibliometric analysis of literature from Web of Science and arXiv is performed to quantify the current research focus and identify the gap between the potential and the application of large models in the agricultural sector. Results Our analysis confirms that agriculture is an emerging but currently underrepresented field for large model research. Nevertheless, we identify and categorize promising applications, including tailored models for agricultural question-answering, robotic automation, and advanced image analysis from remote sensing and spectral data. These applications demonstrate significant potential to solve complex, nuanced agricultural tasks. Discussion This review culminates in a pragmatic framework to guide the choice between large and traditional models, balancing data availability against deployment constraints. We also highlight critical challenges, including data acquisition, infrastructure barriers, and the significant ethical considerations for responsible deployment. We conclude that while tailored large models are poised to greatly enhance agricultural efficiency and yield, realizing this future requires a concerted effort to overcome the existing technical, infrastructural, and ethical hurdles.",
    "full_text": "Harnessing large vision\nand language models\nin agriculture: a review\nHongyan Zhu1,2*† , ShuaiQin1,2† , MinSu1,2, ChengzhiLin1,2,\nAnjie Li1,2 and JunfengGao3*\n1Guangxi Key Laboratory of Brain-inspired Computing and Intelligent Chips, School of Electronic and\nInformation Engineering, Guangxi Normal University, Guilin, China,2Key Laboratory of Integrated\nCircuits and Microsystems (Guangxi Normal University), Education Department of Guangxi Zhuang\nAutonomous Region, Guilin, China,\n3Department of Computer Science, University of Aberdeen,\nAberdeen, United Kingdom\nIntroduction: Agriculture is a cornerstone of human society but faces signiﬁcant\nchallenges, including pests, diseases, and the need for increased production\nefﬁciency. Large models, encompassing large language models, large vision\nmodels, and multimodal large language models, have shown transformative\npotential in various domains. This review aims to explore the potential\napplications of these models in agriculture to address existing problems and\nimprove production.\nMethods: We conduct a systematic review of the development trajectories and\nkey capabilities of large models. A bibliometric analysis of literature from Web of\nScience and arXiv is performed to quantify the current research focus and identify\nthe gap between the potential and the application of large models in the\nagricultural sector.\nResults: Our analysis conﬁrms that agriculture is an emerging but currently\nunderrepresented ﬁeld for large model research. Nevertheless, we identify and\ncategorize promising applications, including tailored models for agricultural\nquestion-answering, robotic automation, and advanced image analysis from\nremote sensing and spectral data. These applications demonstrate signiﬁcant\npotential to solve complex, nuanced agricultural tasks.\nDiscussion: This review culminates in a pragmatic framework to guide the choice\nbetween large and traditional models, b alancing data availability against\ndeployment constraints. We also highlight critical challenges, including data\nacquisition, infrastructure barriers, and the signiﬁcant ethical considerations for\nresponsible deployment. We conclude that while tailored large models are\npoised to greatly enhance agricultural efﬁciency and yield, realizing this future\nrequires a concerted effort to overcome the existing technical, infrastructural,\nand ethical hurdles.\nKEYWORDS\nlarge model, agriculture, natural language processing, computer vision,\nmultimodal model\nFrontiers inPlant Science frontiersin.org01\nOPEN ACCESS\nEDITED BY\nLei Shu,\nNanjing Agricultural University, China\nREVIEWED BY\nOrly Enrique Apolo-Apolo,\nKU Leuven, Belgium\nZhanhao Shi,\nShandong Agriculture and Engineering\nUniversity, China\n*CORRESPONDENCE\nHongyan Zhu\nhyzhu-zju@foxmail.com\nJunfeng Gao\njunfeng.gao@abdn.ac.uk\n†These authors have contributed equally to\nthis work and shareﬁrst authorship\nRECEIVED 19 February 2025\nACCEPTED 28 July 2025\nPUBLISHED 02 September 2025\nCITATION\nZhu H,Qin S,Su M,Lin C,Li A andGao J\n(2025) Harnessing large vision and\nlanguage models in agriculture:\na review.\nFront. Plant Sci.16:1579355.\ndoi: 10.3389/fpls.2025.1579355\nCOPYRIGHT\n© 2025 Zhu, Qin, Su, Lin, Li and Gao. This is an\nopen-access article distributed under the terms\nof theCreative Commons Attribution License\n(CC BY).The use, distribution or reproduction\nin other forums is permitted, provided the\noriginal author(s) and the copyright owner(s)\nare credited and that the original publication\nin this journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted\nwhich does not comply with these terms.\nTYPE Systematic Review\nPUBLISHED 02 September 2025\nDOI 10.3389/fpls.2025.1579355\n1 Introduction\nThe signi ﬁcance of agriculture in the global economy is\nincreasing steadily, and there is growing awareness regarding its\nsustainability. Ahirwar et al. (2019) believe that it is necessary to\nincrease global agricultural food production by a minimum of 70%\nto meet the needs of the increasing world population.\nUnfortunately, there are many factors in agriculture that make it\ndifﬁcult to steadily increase grain production, including 1): crop\ndiseases caused by pathogens such as bacteria, fungi, and viruses.\nThese diseases can spread rapidly, often leading to devastating\neffects on entire crops. For instance, bacterial blight in rice and\nlate blight in potatoes can wipe out signi ﬁcant portions of harvests.\nThe economic impact is staggering, as farmers face not only reduced\nyields but also increased costs associated with disease management;\n2): poor seed quality can lead to weak plant growth, reduced yields,\nand greater susceptibility to both diseases and pests. Farmers who\nuse low-quality seeds often experience crop failures, which not only\njeopardizes their income but also contributes to broader food\ninsecurity within communities. Transitioning to certi ﬁed, high-\nquality seeds is essential for improving crop resilience and\nproductivity; 3): many agricultural tasks remain inef ﬁcient and\nlabor-intensive, hindering productivity. Traditional methods of\nweeding, planting, watering, and harvesting are often time-\nconsuming and can lead to resource wastage. For example,\nmanual weeding not only consumes labor but may also fail to\neffectively control weed populations, resulting in reduced crop\nyields. The adoption of mechanization and modern farming\ntechniques, such as precision agriculture (PA), can signi ﬁcantly\nimprove efﬁciency.\nPA is an agricultural management approach that utilizes\nmodern technology to enhance production ef ﬁciency and\nsustainability. It encompasses sensor technology, unmanned\naerial/ground vehicles (UAVs/UGVs), remote sensing technology,\nautomation equipment, big data, machine learning (ML), and deep\nlearning (DL) ( Tokekar et al., 2016 ; Khanal et al., 2020 ; Saleem et al.,\n2021). This enables farmers to reduce production costs and improve\ndecision-making capabilities, providing signi ﬁcant economic and\nsocial bene ﬁts. For crop diseases, traditional detection methods like\npolymerase chain reactions based on unique deoxyribonucleic acid\nsequences of pathogens, enzyme-linked immunosorbent assays on\nthe basis of pathogens proteins and hyperspectral imaging, are\nconstrained by their operational complexity and the requirement\nfor bulky instruments ( Yao et al., 2024 ). For selecting high-quality\nseeds, quality assurance programs employ various ways to attest\nseed quality attributes, including germination and vigor tests\n(ElMasry et al., 2019 ). But these methods have limitations in\nterms of time overhead, subjectivity, and the destructive nature of\nassessing seed quality ( Medeiros et al., 2020 ). For a general tasks in\nagriculture, the use of pesticides for weed control may have negative\nimpacts on the environment, and Phytotoxicity reactions can lead\nto diminished crop quality and reduced yields ( Visentin et al.,\n2023). And the traditional solutions to these tasks are also inef ﬁcient\ndue to these manned implements are dreadfully slow. On the other\nhand, driven by growing health consciousness, the public has long\nbeen worried about the safety and quality of food, which is linked to\nagricultural products. Reducing food losses and improving food\nsafety rely signi ﬁcantly on the continuous monitoring of crop\nquality, especially the inspection of diseases during crop growth\nstage ( Karthikeyan et al., 2020 ).\nDL technologies in PA can effectively address the limitations of\ntraditional methods by leveraging their powerful data processing and\npattern recognition capabilities. For instance, DL can analyze vast\namounts of data from sensors, drones, and satellite imagery to\naccurately identify crop health, soil characteristics, and potential\ndiseases and pests ( Nasir et al., 2021 ; Bouguettaya et al., 2022 ). This\napplication enables farmers to obtai n real-time insights, allowing for\nmore scienti ﬁcally informed management decisions, optimized\nresource use, and increased crop yields. However, DL technologies\nalso have their limitations, primarily due to the high demand for model\ntraining (Sun et al., 2017). DL models typically require large amounts of\nlabelled data to train and often need to be retrained when faced with\nnew agricultural environments or crop varieties ( Thenmozhi and\nReddy, 2019 ). This repetitive training process is not only time-\nconsuming but also requires signi ﬁcant computational resources and\nexpertise. The effectiveness of transfer learning lies in its ability to apply\nmodels trained in one domain to a related domain, thus reducing the\nneed for new datasets (Bosilj et al., 2020; Paymode and Malode, 2022).\nHowever, the diversity and complex ity of agricultura le n v i r o n m e n t s\ncan limit the effectiveness of transfer learning ( Raffel et al., 2020). For\nexample, differences in soil conditions, climate variations, and crop\ngrowth characteristics across regions can result in models trained in\none area performing poorly in anothe r. Therefore, although DL holds\ntremendous potential in PA, its adapt ability and generalizability must\nbe carefully considered to ensure that models remain effective in the\never-evolving agriculturalﬁeld.\nLarge models are fundamentally distinguished from\nconventional DL models by their vast parameter counts (often\nbillions) and extensive pre-training on massive, diverse datasets. By\nbeing exposed to a rich array of information, these models can\nbetter understand and adapt to various contexts, making them\nhighly versatile tools in ﬁelds such as natural language processing\n(NLP), computer vision (CV), and decision-making ( Kung et al.,\n2023). Crucially, unlike traditional DL models, large models\ndevelop “emergent abilities ”— such as few-shot/zero-shot learning,\ncomplex reasoning, and strong generative abilities — that are not\nsimply scaled-up versions of prior performance ( Bommasani et al.,\n2021; Zhao et al., 2023c ). As an ef ﬁcient analytical means, large\nmodel, has found extensive application in the agricultural sector\n(Stella et al., 2023 ; Yang et al., 2023c ). They have demonstrated\nexcellent performance in analyzing agricultural data, pest and\ndisease management, PA, and more. However, they still face\nmany problems such as dif ﬁculty in obtaining agricultural data\n(Lu and Young, 2020 ), low model training ef ﬁciency, distribution\nshift ( Chawla et al., 2021 ), and plant blindness ( Geitmann and\nBidhendi, 2023 ). In response to the challenges faced by traditional\nagriculture, we committed to conducting a comprehensive analysis\nof large models. First, we systematically summarized the history of\nlarge models, their applications in other ﬁelds, and their signi ﬁcance\nfor agriculture. Subsequently, we introduced many applications of\nZhu et al. 10.3389/fpls.2025.1579355\nFrontiers inPlant Science frontiersin.org02\nlarge models in agriculture. Furthermore, recognizing that large\nmodels were a relatively new technological approach, we outlined\nsome solutions from ethical and responsibility perspectives. Finally,\nwe summarized the current challenges and future directions of large\nmodels and drew conclusions on their effectiveness in the\nagricultural domain.\n2 Feasibility analysis of large models in\nagriculture\nArtiﬁcial intelligence (AI), whose main purpose is to establish\nsystems that learn and think like human ( Holzinger et al., 2019 ),\njust like human language and visual abilities. At present, research on\nlarge models is also focused on NLP and CV. Next, large language\nmodel (LLM), large vision model (LVM) and multimodal large\nlanguage model (MLLM) will be introduced in detail.\n2.1 Evolution and key milestones of large\nmodels\n2.1.1 Development trajectories of large language\nmodels\nLLM is a model based on NLP with a vast number of parameters\n(typically billions) trained on massive datasets of text and code, and\nwe can divide the development of it into four stages ( Figure 1):\n1. Statistical Language Models (SLM): SLMs use traditional\nstatistical methods (like n-grams) to learn word\nprobabilities. Their effectiveness relies on the amount of\ndata and estimation algorithms ( Chelba et al., 2013 ). While\nSLMs are widely used in NLP, they have three main\ndrawbacks: Scalability: Larger n requires more memory\nand parameters (n represents how many preceding words\nthe model considers when predicting the next word);\nInformation sharing: N-grams can’ t share semantic\ninformation across similar words; Data sparsity:\nTechniques like data smoothing can help, but neural\nnetworks handle this better.\n2. Neural Language Models (NLM): NLMs utilize various\nneural networks and are more effective than SLMs\n(Bengio et al., 2000 ; Mikolov et al., 2010 ; Sundermeyer\net al., 2012 ). They address data sparsity using feedforward\nand recurrent neural networks (RNNs), which learn\nfeatures automatically. Key developments include:\nFeedforward neural networks (FFNNLM): Proposed by\nBengio et al. in 2003, they learn distributed word\nrepresentations ( Bengio et al., 2000 ); RNN Language\nModel (RNNLM): Introduced by Mikolov et al., but\nstruggles with long-term dependencies ( Mikolov et al.,\n2010). Long short-term memory (LSTM) networks were\nlater added to overcome this ( Sundermeyer et al., 2012 ).\n3. Pre-trained Language Models (PLM): PLMs are categorized\ninto feature-based and ﬁne-tuning methods: Feature-based:\nFIGURE 1\nDevelopment timeline of NLP models and their pros and cons. White characters represent advantages; black characters represent disadvantages.\nZhu et al. 10.3389/fpls.2025.1579355\nFrontiers inPlant Science frontiersin.org03\nExtracts features from large datasets (e.g., ElMo); Fine-\ntuning: Transfers entire model parameters to speci ﬁc tasks,\nexempliﬁed by BERT and GPT. Transformers, introduced\nby Google, employ a self-attention mechanism, facilitating\nbetter training and performance ( Vaswani et al., 2017 ),\nGPT is ﬁne-tuned from the Transformer. Due to the\nsigniﬁcant acceleration of model training by Transformer,\nit has gradually become the f undamental architecture\nfor LLMs.\n4. Large Language Models (LLM): LLMs have billions of\nparameters and exhibit unique capabilities, known as\n“emergent abilities” . Research shows that larger models\nperform better and are more sample-ef ﬁcient. For instance,\nGPT-3 can generate expected outputs from input sequences\nwithout additional training, a feat beyond smaller models\nlike GPT-2.\nThe transition from SLM to LLM signiﬁ es a progressive increase\nin model complexity, data handling abilities, and adaptability to\ntasks. Each new generation improves upon its predecessor to\novercome limitations, fostering advancement in natural language\nprocessing. As shown in Figure 1, compared to other models, LLMs\nhave a comprehensive understanding of language and excel at\ncomplex reasoning. Their strong few-shot, zero-shot, and\ngenerative capabilities allow them to adapt to new tasks with\nminimal examples. However, high computational costs and bias\nissues prevent them from being perfect. The high computational\ncost remains an unresolved challenge in today’ s era of large data\ntraining. Bias issues can be mitigated through a series of review and\nregulatory measures, which will be detailed in section 4.\n2.1.2 Key advancements and capabilities of large\nvision models\nLVMs are a new generation of models associated with CV,\ncharacterized by their immense scale and broad pre-training.\nInitially, LVM might have denoted purely vision-based models\ntrained solely on image data. However, inspired by multimodal\nlearning in LLM, the concept has evolved to include large models\ntrained on both images and text, enabling rich cross-modal\nassociations. CV models began their development in the 20th\ncentury and have continued to evolve signi ﬁcantly to the present\nday ( Figure 2). Fueled by the availability of massive image datasets,\nthe development of powerful DL architectures, and signi ﬁcant\nprogress in large-scale pre-training techniques, LVMs have\nbecome one of the major development trends in CV models in\nrecent years.\nThe research on CV models initially focused on shallow image\nfeature extraction algorithms, in cluding scale-invariant feature\ntransform, histogram of oriented gradient, and other methods,\nbut had signiﬁ cant limitations. In 2012, AlexNet ( Krizhevsky et\nal., 2017 ) achieved a breakthrough success in ImageNet large scale\nvisual recognition challenge, sparking a wave of convolutional\nneural networks (CNN) for vision models. With the development\nof DL, deep residual networks including VGGNet ( Simonyan and\nZisserman, 2014 ), GoogLeNet ( Szegedy et al., 2015 ), and ResNet\n(He et al., 2016 ) were successively proposed, which improved the\nperformance of image classi ﬁcation, object detection, semantic\nsegmentation, etc. The boom of the Internet also enabled large-\nscale image datasets to be used for training vision models. Faster R-\nCNN (Ren et al., 2016 ), YOLO ( Redmon et al., 2016 ), Mask R-CNN\n(He et al., 2017 ) emerged one after another.\nIn recent years, Transformer has been successfully applied in\nthe domain of LVM, leading to the emergence of models like Vision\nTransformer (ViT) ( Dosovitskiy et al., 2020 ) and DALL-E ( Ramesh\net al., 2021 ) which have garnered signi ﬁcant public attention. Unlike\nthe traditional DL models mentioned above, LVMs such as ViT\nleverage transformer architectures and are typically pre-trained on\nsigniﬁcantly larger and more diverse datasets (e.g., billions of\nimages). This foundational pre-training enables them to develop a\nmore generalized understanding of visual concepts and emergent\ncapabilities, allowing for superior performance on a wide range of\ndownstream tasks, often with only limited domain-speci ﬁc data.\nE 2FIGUR\nDevelopment timeline of computer vision models.\nZhu et al. 10.3389/fpls.2025.1579355\nFrontiers inPlant Science frontiersin.org04\nTheir ability to grasp complex visual patterns and adapt to new\nconditions makes them highly versatile.\nAs detailed in Table 1 , large vision models (LVMs) and\ntraditional vision models differ signi ﬁcantly in their core\narchitecture, data requirements, and capabilities. The fundamental\ndistinction lies in their approach to context: Transformer-based\nLVMs leverage global self-attention to capture broad visual context\nand long-range dependencies, a signi ﬁcant leap from the local\nreceptive ﬁelds of traditional convolutional models ( Aymen et al.,\n2024; Malla et al., 2024 ). While this architectural shift grants LVMs\nsuperior generalization, it also introduces challenges like high\ncomputational demands and data hunger. Notably, research is\nactively addressing these limitations. For instance, Shi et al.\n(2024) proposed “Scaling on Scales (S²), ” which enhances\nperformance by increasing image scales rather than model size,\nproviding new insights for the future development of vision models.\n2.1.3 The emergence of multimodal large\nlanguage models\nIn addition to the LLMs and LVMs introduced above, MLLMs\nare also a research focus in the domain of AI. While LLMs perform\nwell in text-based tasks, their capabilities alone cannot effectively\nreason about information presented in non-textual formats.\nAlthough LVMs perform well in the ﬁeld of CV and possess\nsome NLP abilities, researchers are not content with large models\nsolely trained on text and images. MLLMs ( Wu et al., 2023a )\nintegrate multiple data types, s uch as images, text, language,\naudio, and more. It not only possesses the advantages of LLMs\nand LVMs, but also address the limitations of LLMs and LVMs by\nintegrating multiple modalities, enabling a more comprehensive\nunderstanding of various data. It can be said that the developments\nin MLLMs have set up new avenues for AI, which make binary\nmachines to understand and then process various data types ( Wu\net al., 2023a ). For agriculture, MLLM allows tasks to no longer be\nconﬁned to just images or text; instead, it can leverage both, and\neven utilize multimodal inputs like audio and video, breaking the\nlimitations of images and text.\n2.2 Current applications of large models in\nother domains\nAs shown in Table 2 , many LLMs are designed to develop\nchatbots (BLOOM ( Le Scao et al., 2023 ), PaLM2 ( Anil et al., 2023 ),\nERNIE 4.0) or complete NLP tasks, including text classi ﬁcation,\nmachine translation, and sentiment analysis [OPT ( Zhang et al.,\n2022b)]. Similarly, LVMs are primarily engineered to interpret and\nprocess visual information. They excel at core CV tasks such as\nimage classi ﬁcation, object detection, segmentation, and image\ngeneration, often forming the foundation for systems needing to\nunderstand or interact with the visual world. Models like\nInternImage ( Wang et al., 2023 ) and LLaVA ( Liu et al., 2023b )\nrepresent efforts to enhance performance on complex visual\nanalysis tasks, aiming to simulate and automate human\nvisual processes.\nAlthough LLM and LVM satis ﬁes some functions and takes\nlarge models a big step towards arti ﬁcial general intelligence (AGI),\nit is not enough to achieve the goal that machines can emulate\nhuman thinking and carry out a wide range of general tasks through\ntransfer learning and diverse other modalities without achieving the\nmultimodality of the model ( Zhao et al., 2023b ). Some large models\nhave implemented multimodality, enabling them to analyze\ndifferent types of information [GPT-4 ( Bubeck et al., 2023 ),\nLLaMA, Gemini ( Team et al., 2023 ), ImageBind ( Girdhar et al.,\n2023)] and interact with users. It is worth mentioning that most of\nthe newer large models are MLLMs, and many models that were\noriginally LLMs or LVMs have gradually acquired multimodal\ncapabilities after multiple updates.\nHowever, many current models are generic models and their\ntraining datasets are too broad, they cannot provide a satisfactory\nanswer to knowledge in certain professional ﬁelds. As Goertzel\n(2014) believed, for a system to be considered AGI, it is not\nnecessary for it to have in ﬁnite generality, adaptability, or\nﬂexibility. Therefore, some researchers have optimized and\nadjusted existing large model s and have released some large\nmodels speci ﬁcally for a single ﬁeld. BloombergGPT can be used\nin the ﬁnancial ﬁeld, showcasing remarkable performance on\ngeneral LLM benchmarks and surpassing comparable models on\nﬁnancial tasks ( Wu et al., 2023c ). The meteorological model in\npanguLM developed by Huawei can provide predictions of variables\nsuch as gravity potential, humidity, wind speed, temperature, and\npressure within 1 hour to 7 days. Embedding PaLM-E into robots\ncan achieve multiple speci ﬁc tasks, like visual question answering,\nsequential robotic manipulation planning, and captioning ( Driess\net al., 2023 ). OceanGPT is an expert in various marine science tasks\n(Bi et al., 2023 ). It exhibits not only a higher level of knowledge\nexpertise for oceans science tasks but also acquires preliminary\nembodied intelligence capabilities in ocean engineering. PMC-\nTABLE 1 Comparison of large vision models and traditional\nvision models.\nFeature/\nAspect\nLarge vision models Traditional\nvision models\nCore\narchitecture\nTransformer-based, global\nself-attention\nConvolution-based, local\nreceptive ﬁelds\nContext &\ndependencies\nGlobal context, excels at\nlong-range dependencies\nLocal focus, struggles with long-\nrange dependencies\nImage\nhandling\nProcesses image patches;\nmore robust to variations\nUses sliding ﬁlters; sensitive to\nsome variations\nData needs Best with large-scale\npre-training\nCan work with smaller datasets,\nbeneﬁts from pre-training\nMultimodal\nability\nStronger, more inherent\nmultimodal integration\nRequires more specialized\ndesigns for multimodal\nParallelism High (sequence processing) Good for convolutions, poor for\nsequential tasks\nKey\nadvantages\nGlobal understanding, long\ndependencies, scalability\nEfﬁcient local feature extraction\nKey\nlimitations\nCan be data-hungry for\npre-training\nLimited global view,\nadversarial vulnerability\nZhu et al. 10.3389/fpls.2025.1579355\nFrontiers inPlant Science frontiersin.org05\nTABLE 2 The currently popular and representative large models.\nOriginal\nversion\nLatest\nversion\nRelease\ndate\n(original)\nRelease\ndate (latest)\nTypes\n(original\n→ latest)\nInformation References\nOPT / May 2 rd, 2022 / LLM OPT promotes transparency, reproducibility,\nand broader community engagement and\ninnovation in NLP research. (open source)\n(Zhang\net al., 2022b )\nBLOOM BLOOMZ July 12 th, 2022 December\n15th, 2022\nLLM A decoder only model based on Transformer\narchitecture. (open source)\n(Le Scao et\nal., 2023 )\nPMC-LLaMA / Aprill 27 th, 2023 / LLM Inject medical knowledge into existing LLM\nusing 4.8 million biomedical academic\npapers. (open source)\n(Wu et\nal., 2024 )\nPaLM2 / May 11 st, 2023 / LLM PaLM2 was a neural network-based language\nmodel that was considered one of the most\nadvanced language models available at the\ntime of its release in May 2023.\n(Anil\net al., 2023 )\nBloombergGPT / March 30 th, 2023 / LLM A LLM for the ﬁnancial ﬁeld. ( Wu\net al., 2023c )\nOceanGPT-\nBasic-7B\nOceanGPT-\nBasic-14B/7B/2B\nOctober 3 rd, 2023 July 4 th, 2024 LLM OceanGPT is the ﬁrst LLM in the ocean\ndomain. (open source)\n(Bi et al., 2023 )\nDeepSeek LLM DeepSeek-R1 November\n29th, 2023\nJanuary\n20th, 2025\nLLM DeepSeeke-R1 excels in complex tasks such\nas mathematics, coding, and natural\nlanguage reasoning\n(Guo\net al., 2025a )\nLlama 2 Llama 4 July 20 th, 2023 April 6 th, 2025 LLM → MLLM A series of large models released by Meta. /\nQwen-7B Qwen2.5-\nOmni-7B\nAugust 3 rd, 2023 March 27 th, 2025 LLM → MLLM A super large model launched by Alibaba\nCloud. (open source)\n(Bai\net al., 2023 )\nKimi Chat Kimi k1.5 October\n10th, 2023\nJanuary\n20th, 2025\nLLM → MLLM Kimi k1.5 surpasses GPT-4o by 550% in\nmathematics, coding, and other capabilities\nunder short-chain thinking mode.\n/\nGemma Gemma 3 February\n21st, 2024\nMarch 12 th, 2025 LLM → MLLM Gemma 3 is a MLLM released by Google.\n(open source)\n(Team et al.,\n2024, 2025)\nPaLM-E / March 6 th, 2023 / LVM PaLM-E can integrate vision and language\ninto robot control.\n(Driess\net al., 2023 )\nInternImage InternImage-H November\n10th, 2022\nOctober 4 th, 2023 LVM A LVM based on deformable convolution.\n(open source)\n(Wang\net al., 2023 )\nPanguCVLM 3.0 PanguCVLM 5.0 July 7 th, 2023 June 21 th, 2024 LVM A LVM that simulates and automates human\nvisual processes.\n/\nLLaVA LLaVA-\nNeXT (Stronger)\nApril 17 th, 2023 May 10 th, 2024 LVM LLaVA has the ability to align and fuse the\nvisual information of images with the\nsemantic information of text. (open source)\n(Liu\net al., 2023b )\nmPLUG-Owl mPLUG-Owl3 April 27 th, 2023 August\n20th, 2024\nLVM → MLLM mPLUG-Owl is developed by Alibaba\nDAMO Academy. (open source)\n(Ye et al.,\n2023, 2024)\nSPARK 1.0 SPARK\n4.0 Turbo\nMay 6 th, 2023 October\n24th, 2024\nLVM → MLLM A new generation of cognitive intelligence\nmodel with Chinese as its core.\n/\nClaude 3 Claude 3.7 Max March 4 th, 2024 March 18 th, 2025 MLLM A MLLM that primarily focuses on\ncode processing.\n/\nERNIE 4.0 ERNIE 4.5 October\n17th, 2023\nMarch 16 th, 2025 MLLM ERNIE is a new generation of Baidu ’s large\nmodel for knowledge enhancement.\n/\nImageBind / May 9 th, 2023 / MLLM ImageBind is the ﬁrst AI model that can bind\ninformation from six modes.\n(Girdhar\net al., 2023 )\nGPT-4 GPT-4.5 March 14 th, 2023 February\n28th, 2025\nMLLM GPT-4.5 signi ﬁcantly enhances its knowledge\nreserves and emotional intelligence by\nexpanding unsupervised learning and\nreasoning capabilities.\n(Bubeck\net al., 2023 )\n(Continued)\nZhu et al. 10.3389/fpls.2025.1579355\nFrontiers inPlant Science frontiersin.org06\nLLaMA represents the pioneering open-source medical speci ﬁc\nlanguage model that demonstrates exceptional performance on\ndiverse medical benchmarks , outperforming ChatGPT and\nLLaMA-2 with signi ﬁcantly fewer parameters ( Wu et al., 2024 ).\nThe success of large models across diverse ﬁelds, as highlighted in\nthis section, underscores their potential to generalize and tackle\ncomplex problems, suggesting their applicability to the intricate\ntasks within agriculture.\n2.3 Assessing the attention to large models\nwithin agriculture\nIn the past few decades, the advancement of agricultural\ntechnology has signi ﬁcantly improved global agricultural\nproduction ef ﬁciency. According to the forecast released by the\nfood and agriculture organization (FAO) of the United Nations, the\nglobal grain production in 2023 was 2.84 billion tons, nearly twice\nthat of the early 20th century. Although global agricultural\nproduction ef ﬁciency is high, the world population is also\nconstantly growing. Continuo usly improving agricultural\nproduction ef ﬁciency is the lifeline of economic development and\nthe foundation for ensuring human food, clothing, and survival\nneeds. Hence, how to make agricultural practices advance is a\ncrucial issue. Next, we will use bibliometric methods in conjunction\nwith practical analysis to explain why large models are important\nfor agriculture.\n2.3.1 Bibliometric analysis and data sources\nBibliometrics is a quantitative analysis method that integrates\nmathematics, statistics, and bibliology, based on mathematical\nstatistics. It focuses on the external characteristics of scienti ﬁc\nliterature to conduct statistical and visual analyses of the\nliterature ( Wang et al., 2019 ). Keywords encapsulate important\ninformation about the research topic. They can intuitively re ﬂect\nthe themes and content of the study, reveal the connections between\nresearch contents, results, and characteristics in a particular ﬁeld,\nand demonstrate the research dynamics and emerging trends within\nthat area ( Li and Zhao, 2015 ). Our analysis used two methods:\n1. Searching for research literature related to large models\nusing the Science Citation Index Expanded (SCI-E) and\nSocial Sciences Citation Index (SSCI) from Web of Science\n( W o S )w i t hk e y w o r d ss u c ha s “large models ”, “large\nlanguage models ”, “large vision models ”,o r “foundation\nmodels”, covering the period from 2019 to 2024.\n2. Collecting 3,496 papers from the arXiv in the ﬁeld of\nartiﬁcial intelligence from 2019 to 2024 and categorizing\nthem by discipline based on keyword searches.\nOur analysis of WoS aimed to identify the established trends and\npeer-reviewed research regarding large models, and speci ﬁcally, the\nfrequency of agriculture-related keywords within this body of\npublished work. This provides a view of the validated research\nlandscape. Complementarily, we included arXiv to capture the more\nrecent and rapidly evolving trends in arti ﬁcial intelligence research.\narXiv, as a leading platform for pre-prints in AI, offers valuable insights\ninto emerging research directions and the early exploration of applying\nlarge models across various disciplines, including potential initial\ninterest in agriculture. Pre-prints often precede formal publication,\nproviding a timelier snapshot of the research frontier.\nBy analysing both published literature (WoS) and pre-prints\n(arXiv), we aimed to gain insights from two different perspectives:\nthe established, peer-reviewed research landscape and the more\nimmediate, evolving research front. This allows us to observe both\nthe current state of validated research and the potential emerging\ntrends and initial explorations within the ﬁeld.\n2.3.2 Detailed analysis and design protocol\nAs described in section 2.3.1, two data sources were used for the\nspeciﬁc analysis method: (1) WoS; (2) arXiv. Next, we will elaborate\non the details of using these two bibliometric analysis methods.\nFor the Method 1, after entering the ofﬁcial website of WoS, search\nin “Web of science Core Collection” and select Science Citation Index\nExpanded (SCI-EXPANDED) and Social Sciences Citation Index\n(SSCI) in edition. Both SCI-EXPANDED and SSCI primarily index\npeer-reviewed journals with established reputations within their\nrespective ﬁelds. This ensures a certain level of quality control and\nscholarly rigor in the literature bei ng analysed. Then search for topics\nwith the keywords “large models”, “large language models ”, “large\nTABLE 2 Continued\nOriginal\nversion\nLatest\nversion\nRelease\ndate\n(original)\nRelease\ndate (latest)\nTypes\n(original\n→ latest)\nInformation References\nSkywork Skywork 4.0 April 17 th, 2023 January 6 th, 2025 MLLM Skywork is a series of large models developed\nby the Kunlun · Skywork team.\n(Wei\net al., 2023 )\nGemini Gemini 2.5 December\n6th, 2023\nMarch 26 th, 2025 MLLM Gemini is a MLLM launched by\nGoogle DeepMind.\n(Team\net al., 2023 )\nSora / February\n15th, 2024\n/ MLLM Sora can create realistic and imaginative\nscenes from text instructions.\n(Peebles and\nXie, 2023 )\nHunyuan-t1 Hunyuan-\nt1 (of ﬁcial)\nFebruary\n17th, 2025\nMarch 21 st, 2025 MLLM Hunyuan-t1 is a deep-thinking model\nindependently developed by Tencent.\n/\nThe arrow ' →' represents the change in the model's category, from its original type to the type of its latest version.\nZhu et al. 10.3389/fpls.2025.1579355\nFrontiers inPlant Science frontiersin.org07\nvision models”,o r “foundation models”, covering articles from 2019 to\n2024, and export the authors, titles , sources and abstracts of these\narticles in plain text ﬁle. Finally, import these plain text ﬁles into\nV O S v i e v e rt od r a wan e t w o r ko fk e y w o r d si nt h eﬁeld of large models.\nFor the Method 2, we use the keyword “artiﬁcial intelligence” to search\nfor articles on arXiv, and crawl the relevant articles from 2019 to 2024,\nincluding title, author, abstract a nd other information, to build a csv\nﬁle. Then search this ﬁle according to relevant keywords. For example,\nin the medical ﬁeld, keywords such as medical, healthcare, hospital, etc.\nare used to ﬁlter out relevant articles and count the number. Finally, a\ngraph of the proportion of articles in different ﬁelds under the AI\ndomain is constructed.\nIn this way, we obtained a network map of keyword through the\nMethod 1, and a graph of the proportion of different ﬁelds in the AI\ndomain through the Method 2. The speci ﬁc results and analysis will\nbe explained in the next section.\n2.3.3 Analysis results\nA total of 1,789 papers wereﬁltered using Method 1, and a network\ndiagram of keyword occurrences was generated using VOSviewer. As\nshown in Figure 3,t h et e r m‘agriculture’appears infrequently in these\nlarge model papers, indicating that large models have not received\nwidespread attention in the agricultural ﬁeld.\nThe reasons why large models have not received attention in the\nagricultural sector are diverse. First, large models are a relatively new\ntechnology that has emerged in recent years, and many researchers and\npractitioners in agriculture may not fully understand their capabilities\nand potential applications in the ﬁeld. Second, implementing large\nmodels often requires substantial computational resources and\nexpertise, which may not be easily accessible in many agricultural\nenvironments. Third, agricultural tasks can be very speci ﬁca n d\nlocalized, leading people to prefer traditional methods over large models.\nMoreover, Figure 4 illustrates a difference: the application and\nresearch of large models in agriculture are currently limited\ncompared to other ﬁelds. This observation from our bibliometric\nanalysis ( Figures 3, 4) suggests that despite the evident potential of\nlarge models to address agricultural challenges discussed in the\nintroduction, the ﬁeld is still in the early stages of exploring and\nadopting this technology. Therefore, a detailed review of their\npotential applications, associated challenges, and responsible\ndeployment is crucial to guide future research and accelerate their\nintegration into agriculture.\n3 Large models in agricultural\napplications\nAs mentioned in the introduction, agriculture faces multiple\nchallenges, including pests and diseases, seed quality, and crop\ngrading. Large models have demonstrated signi ﬁcant potential in\naddressing these issues, and some researchers have already\ndeveloped models speci ﬁcally tailored for the agricultural domain.\n3.1 Emerging potential and existing\napplications of large models in agriculture\n3.1.1 Potential and applications\nMany large models have emerged, and although they are not yet\ntruly applied in agriculture, thei r problem-solving capabilities\nFIGURE 3\n47 keywords co-occurrence network map.\nZhu et al. 10.3389/fpls.2025.1579355\nFrontiers inPlant Science frontiersin.org08\nindicate potential prospects in agricultural applications. As shown\nin Table 3, some large models are modi ﬁcations of existing models,\nwhile others are entirely original. For example, given 50 original\ndescriptions related to “wheat rust, ” AugGPT can generate 200+\nexpanded samples covering different growth stages and climatic\nconditions, thereby enhancing the robustness of disease\nidentiﬁcation models in complex environments. Aurora is a large\nmodel for weather forecasting ( Bodnar et al., 2024 ), and if applied in\nagriculture, it could enable farmers to schedule activities such as\nplanting, fertilizing, and harvesting based on accurate weather\nforecasts, as well as proactivel y mitigate losses from extreme\nweather events. In addition to ordinary large models, there are\nalso some special existences. HuggingGPT is an AI agent framework\ndesigned to orchestrate multiple specialized models, including\nLLMs like ChatGPT ( Shen et al., 2024 ). It acts as a ‘model\ncoordinator,’integrating and managing diverse AI components to\nenhance decision-making in complex scenarios such as agricultural\nplanning. This capability offers possibilities for managing a series of\ncomplex agricultural tasks, from planting to harvesting.\nNotably, there are already large models applied in agriculture\n(Table 4). For instance, TimeGPT demonstrates its capability as a\nsmart agriculture tool (Deforce et al., 2024 ), being used for\npredicting soil moisture, which helps farmers determine whether\nthe soil is suitable for certain crops. FMFruit showcases the\nE 4FIGUR\nThe proportion of arXiv papers on agriculture in AI.\nTABLE 3 Large models with agricultural potential.\nType Based Method Problem Application prospect References\nLLM\nChatGPT GPT-3.5-turbo Agricultural information extraction Rapid querying of\nagricultural information\n(Peng\net al., 2023 )\nAugGPT Text data augmentation Few-shot learning for\nagricultural data\n(Dai et al., 2023 )\n/ Aurora Atmospheric prediction Predicting weather in agriculture ( Bodnar\net al., 2024 )\nLVM\n/ MAE, DINO, DINOv2 Plant phenotyping tasks Monitoring crop health ( Chen\net al., 2023a )\nPaLM, ViT PaLM-E Robot control Agricultural intelligent machines\nor robots\n(Driess\net al., 2023 )\nMLLM SAM TAM Object tracking and segmentation\nin videos\nMonitor animals in\nagricultural farming\n(Yang\net al., 2023a )\nZhu et al. 10.3389/fpls.2025.1579355\nFrontiers inPlant Science frontiersin.org09\nimportance of large models in agricultural detection tasks ( Li et al.,\n2024 ), providing new directions and foundations for the\ndevelopment of robotic harvesting systems. ITLMLP performs\ndisease recognition on cucumbers with limited sample sizes,\nplaying a signi ﬁcant role in agricultural automation and\nintelligence ( Cao et al., 2023 ).\nTables 2 , 3 demonstrate the feasibility and importance of large\nmodels in agriculture, where many agricultural tasks involve\ncomplex reasoning. For example, when presented with an image\nof a soybean ﬁeld, agricultural scientists or farmers rely on large\nmodels to undertake several key steps. Firstly, the large model must\nidentify any abnormal symptoms evident in the soybean leaves,\nsuch as leaf wrinkling. Subsequently, it must ascertain the name of\nthe speci ﬁc problem that troubles plants, such as soybean mosaic.\nNext, the model needs to determine the underlying cause of the\ndisease, such as soybean mosaic virus. Finally, it must develop an\nappropriate treatment strategy. This multi-step, cross modal\ndiagnostic and decision-making process is precisely the unique\nadvantage that large models can demonstrate compared to\ntraditional DL models with a single task.\nMany question answering (QA) and dialogue systems are\ndesigned to address this type of reasoning problem ( Rose Mary\net al., 2021 ; Mostaco et al., 2018 ; Niranjan et al., 2019 ). For instance,\na chatbot based on a RNN is speci ﬁcally designed to handle\nquestions related to soil testing, plant protection, and nutrient\nmanagement ( Rose Mary et al., 2021 ). Although, these QA and\ndialogue systems and chatbots can answer most inquiries without\nthe need for human interaction and with excellent accuracy, they\nhave limited capabilities for complex problems by reason of their\nsmall model size as well as of inadequate training data. Therefore,\nthe agricultural domain requires large models to promote the\ndevelopment of QA and dialogue systems and chatbots. The\ntraditional methods for detecting crop pests and diseases mainly\nrely on special methods such as serology and molecular biology-\nbased technical means, in addition to arti ﬁcial visual evaluation.\nAlthough these methods can accurately determine pests and\ndiseases to a certain extent, they often require a lot of time and\nmoney. And some methods of sampling crops often lead to crop\ndamage, which goes against the original intention of diagnosing\ndiseases and pests to protect crops. Therefore, image processing and\nanalysis is an important task for large models in the ﬁeld of\nagriculture, and another important task is to embed LVMs into\nrobots to solve some agricultural problems (Weeding, pruning\nbranches, harvesting, etc.) and achieve automated agriculture.\n3.1.2 The advantages of agriculture-speciﬁc large\nmodels\nIn the ﬁeld of agriculture, agriculture-speci ﬁc models can offer\nnotable advantages over general large models, particularly by\neffectively integrating diverse, domain-relevant data modalities\nsuch as image, text, and crucial label information. This\nmultimodal strategy, often employing techniques like combined\ncontrastive learning methods within a uni ﬁed feature space, allows\nthese models to address the prevalent challenge of data scarcity in\nTABLE 4 Agricultural large models.\nType Name Achievement Signi ﬁcance References\nLLM\nTimeGPT Predicting soil moisture Contributes to sustainable\nagricultural practices\n(Deforce\net al., 2024 )\nChatGPT Designed a tomato-picking robot Simplify the design process of\nagricultural robots\n(Stella\net al., 2023 )\nFMFruit Identifying multiple types of fruits Research on robotic harvesting and\nfruit detection\n(Li et al., 2024 )\nAgriGPT Multimodal agricultural knowledge Q&A Promote precision\nagriculture practices\n(Liu\net al., 2023a )\nShenNong Development of specialized large models\nfor multiple agricultural domains\nDriving agricultural intelligence and\ncomprehensive\nefﬁciency improvement\n/\nChatAgri Cross-linguistic classi ﬁcation of\nagricultural texts\nProvide decision support for\nprecision agriculture\n(Zhao\net al., 2023a )\nLVM\nSpectralGPT Process spectral remote sensing data Greatly enhanced the processing\ncapability of agricultural spectral data\n(Hong\net al., 2024 )\nSAM Chicken segmentation and tracking Facilitates segmentation and tracking\ntasks in agriculture\n(Yang\net al., 2023c )\nAgricultural ﬁeld boundary delineation Bene ﬁcial for PA, crop monitoring,\nand yield estimation\n(Tripathy\net al., 2024 )\nMLLM\nITLMLP Cucumber disease recognition Agricultural disease recognition ( Cao\net al., 2023 )\nAIE-SEG High-precision segmentation of\nagricultural imagery\nEnables automated ﬁeld monitoring\nand yield estimation\n(Xu et al., 2023 )\nZhu et al. 10.3389/fpls.2025.1579355\nFrontiers inPlant Science frontiersin.org10\nagriculture more effectively than models relying solely on single\nmodalities or vast, generic pre-training datasets.\nBy explicitly learning and leveraging the semantic correlations\nbetween visual features (e.g., speci ﬁc crop disease symptoms) and\nrelated textual descriptions or categorical labels, agriculture-speci ﬁc\nmodels can develop more comprehensive, robust, and discriminative\nrepresentations tailored to the nuances of the ﬁeld. For example,\nChatAgri excels in the speci ﬁc task of agricultural visual diagnostics\n(Zhao et al., 2023a). A general MLLM might identify visual anomalies,\nand lack the specialized knowledge to accurately name the speci ﬁc\nagricultural disease or pest, understand its lifecycle, or recommend\nappropriate, targeted treatments. Especially when faced with limited\ntraining samples, agriculture-speciﬁcl a r g em o d e l sm a yp e r f o r mb e t t e r\ncompared to models with poor adaptability. Unlike general large\nmodels that often require vast datasets for pre-training and may not\nadapt well to ﬁne-tuning on limited agricultural data, ITLMLP is\ndesigned to be effective with small sample sizes. It extracts richer and\nmore discriminative features from scarce data, leading to signi ﬁcantly\nhigher recognition accuracy (achieving 94.84% in their paper)\ncompared to general large models to the same small dataset ( Cao\net al., 2023 ).\nFurthermore, their focused training enables them to better\nidentify and weigh agriculturally signi ﬁcant features, accurately\ndiscerning subtle but critical patterns for tasks like disease\nrecognition while potentially mitigating the in ﬂuence of irrelevant\nbackground elements, ultimately resulting in improved accuracy,\nreliability, and greater practical applicability within the complex\nagricultural environment.\n3.2 Leveraging large language models for\nagricultural data processing, insights, and\ndecision support\nLLM can play many roles in the agricultural domain, such as\nprocessing and generating agricultural data, providing insights into\nagricultural production work, and supporting agricultural decision-\nmaking for farmers.\n3.2.1 Large language models for processing and\ngenerating agricultural data\n3.2.1.1 Information extraction\nLLMs can extract structured information from unstructured\nagricultural text data. First, the text is divided into individual\ntokens and LLMs represent each token as a numerical vector called\na word embedding. Then, LLMs analyse the surrounding context of\neach token to understand its meaning within the sentence or\ndocument, and identify and categorize named entities within the\ntext, like names of individuals, locations, organizations, or speci ﬁc\nagricultural terms. Finally, LLMs employ techniques like information\nextraction to identify and extract structured information from\nunstructured text (Involve identifying relationships between\nentities, extracting key facts, or populating knowledge graphs).\nLLMs extract information from data using a process known as\nNLP. Beyond mere extraction, modern LLM applications\nincreasingly employ a paradigm known as retrieval-augmented\ngeneration (RAG). In this approach, the LLM ﬁrst retrieves\nrelevant, up-to-date information from external, domain-speci ﬁc\nknowledge bases — such as recent agronomic research, real-time\nmarket prices, or local pest outbreak databases. This retrieved\ncontext then “augments” the model ’s input, enabling it to generate\nresponses that are not only more accurate and timelier but also\ngrounded in veri ﬁable sources, thereby signi ﬁcantly mitigating the\nrisks of data lag and factual inaccuracies in the agricultural domain\n(Gao et al., 2023 ).\n3.2.1.2 Agricultural data generation\nGenerative AI models are a multimodal LLM, which is the\nMLLM mentioned above. An obstacle encountered when applying\nspecialized CV algorithms to agricultural vision data is the insufﬁcient\navailability of training data and labels ( Qi et al., 2017 ; He et al., 2017 ).\nIn addition, collecting data that encompasses the wide range of\nvariations caused by season and weather changes is exceedingly\nchallenging. Acquiring high-quality data requires a lot of time, and\nlabelling them is even more costly ( Zhou et al., 2017 ). To address\nthese challenges, one approach is to ﬁne-tune multimodal generative\nLLMs on the target agricultural data domain. This allows the models\nto generate massive training data and labels, thereby constructing an\naugmented training set that closely resembles the distribution of the\noriginal data (Dai et al., 2023 ). Besides, text-based generation models\ncan generate images ( Rombach et al., 2022 ) and videos ( Ho et al.,\n2022)o fs p e c i ﬁc scenes based on text descriptions, thereby\nsupplementing training datasets that may lack certain visual\ncontent. This helps in expanding the training data and improving\nthe performance of downstream models.\n3.2.2 Large language models provide insights\nLLMs possess the capability to analyse textual data and uncover\ntrends in agricultural practice s, market conditions, consumer\npreferences, and policy develo pments. Through analysis of\nagricultural text data from sources such as news articles, reports,\nand social media, these models can offer valuable insights into\nmarket dynamics and pricing trends ( Yang et al., 2024 ). This\nprovides support for farmers to understand domains outside of\nagriculture. Many researchers believe that the integration of LLMs\ninto different stages of designment and development for agricultural\napplications is also experiencing a noticeable rise ( Stella et al., 2023 ;\nLu et al., 2023 ). In ( Stella et al., 2023 ) study, Stella et al. incorporated\nLLM into the design phase of robotic systems. They speci ﬁcally\nfocused on designing an optimized robotic gripper for tomato\npicking and outlined the step-by -step process. In the initial\nideation phase, they leveraged LLMs like ChatGPT ( Bubeck et al.,\n2023) to gain insights into the possible challenges and opportunities\nassociated with the task. Building upon this knowledge, they\nidentiﬁed the most promising and captivating pathways, engaging\nin ongoing discussions with the LLM to re ﬁne and narrow down the\ndesign possibilities. Throughout this process, the human\ncollaborator harnesses the expansive knowledge of the LLM to tap\ninto insights transcend their individual expertise. In the following\nstage of the design process, which emphasizes technical aspects, the\nZhu et al. 10.3389/fpls.2025.1579355\nFrontiers inPlant Science frontiersin.org11\nbroad directions derived from the collaboration need to be\ntransformed into a real, completely functional robot. Although\nLLMs do not provide comprehensive technical support, they can\noffer their own insights on whether the technology is feasible,\nhelping researchers reduce the risk of failure.\nPresently, LLMs lack the ability to generate comprehensive\nCAD models, evaluate code, or independently fabricate robots.\nNevertheless, advancements in LLM research suggest that these\nalgorithms can offer signi ﬁcant assistance in executing software\n(Chen et al., 2021 ), mathematical reasoning ( Das et al., 2024 ), and\neven in the generation of shapes ( Ramesh et al., 2022 ). Lu et al.\nspeciﬁcally focused on the utilization of LLMs for organizing\nunstructured metadata, facilitating the conversion of metadata\nbetween different formats, and discovering potential errors in the\ndata collection process ( Lu et al., 2023 ). They also envisioned the\nnext generation of LLMs as remarkably potent tools for data\nvisualization ( Bubeck et al., 2023 ), and anticipated that these\nadvanced models will provide invaluable support to researchers,\nenabling them to extract meani ngful insights from extensive\nvolumes of phenotypic data.\nAlthough LLMs provide insights can indirectly help farmers\nsolve a small number of agricultural tasks, it ’s important to note that\ntheir insights should be used in conjunction with human judgment\nand domain expertise. That is to say, the insights provided by LLMs\ncannot be separated from human experience.\n3.2.3 Large language models empower decision-\nmaking for farmers\nAccording to a recent study, ChatGPT demonstrates the ability\nto comprehend natural language requests, extract valuable textual\nand visual information, select appropriate language and vision\ntasks, and effectively communicate the results to humans ( Shen\net al., 2024 ). Shen et al. proposed a system named HuggingGPT to\nsolve AI tasks. HuggingGPT is a collaborative AI task resolution\nframework built on LLMs. This system connects LLM with AI\nmodels through language interface, and these AI models are derived\nfrom HuggingFace. This coordinating capability positions LLMs as\nthe core of modern AI Agents. As the core of decision-making, LLM\ncan be applied to agriculture to help solve the tasks proposed by\nfarmers ( Shen et al., 2024 ).\nAn AI Agent is an autonomous system that perceives its\nenvironment, reasons, plans, and acts to achieve speci ﬁc goals. As\nshown in Figure 5 , the LLM acts as the agent ’s “brain”, performing\ncrucial functions. When receiving a task request, LLM ﬁrst divides\nthe total task into subtasks and selects the appropriate AI model\nbased on the needs of each subtask. For example, converting\nfarmers’audio into text requires the use of an audio to text model\n[Amazon transcribe, Whisper ( Radford et al., 2023 )]; It is also\nnecessary to recognize the sent image and integrate the text\nobtained from the audio conversion in the previous step to obtain\na text-response (vit-gpt2); Considering that some farmers may have\nhad limited access to formal education, it is necessary to further\nconvert text-response into audio and ultimately obtain the audio-\nresponse [Fastspeech ( Ren et al., 2019 , 2020)]. Although LLM does\nnot play a role in solving problems throughout the entire system, as\na “conductor”, it can coordinate various AI models to complete\nsubtasks, thereby gradually solving complex tasks and playing a\ncore role in decision-making support.\n3.3 The role of large vision model in image\nprocessing, analysis, and agricultural\nautomation\nWhile LLMs excel in processing textual and knowledge-based\ninformation, many agricultural tasks fundamentally rely on visual\ndata. Using a LVM to judge crop related information can not only\ngreatly improve the time required for judgment, but also indirectly\nreduce the damage caused to crops. Moreover, after crops are\ninvaded by pests and diseases, their color, texture, spectral\ncharacteristics will undergo certain changes, all of which are\nrelated to CV.\n3.3.1 Image processing and analysis\nAt present, there are four types of methods for obtaining crop\nimage information: 1) ordinary channels, taking photos to obtain\nimages; 2) obtaining remote sensing images through agricultural\nmachinery near the ground; 3) obtaining remote sensing images\nthrough aircraft monitoring platforms ( Yuan et al., 2022 ); 4)\nobtaining remote sensing images through satellites ( Zhang et al.,\n2019). Remote sensing can provide large-scale land use and land\ncover information. By analysing satellite images or high-altitude\nimages, various surface information can be identi ﬁed, such as\nsurface conditions, soil moisture, vegetation coverage, and crop\ngrowth status ( Khanal et al., 2020 ). Classifying and segmenting\nfrom limited examples obtained from remote sensing is a signi ﬁcant\nchallenge. Regarding this, Wu et al. (2023b) put forward GenCo (a\ngenerator-based two-stage approach) for few-shot classi ﬁcation and\nsegmentation on remote sensing and earth observation data. Their\napproach presents an alternative solution for addressing the\nlabelling challenges encountered in the domains of remote\nsensing and agriculture. Spectral data can provide rich insights\ninto the composition of observed objects and materials, especially in\nremote sensing applications. The challenges faced in processing\nspectral data in agriculture include: 1) effectively processing and\nutilizing vast amounts of remote sensing spectral big data derived\nfrom various sources; 2) deriving signi ﬁcant knowledge\nrepresentations from intricate spatial-spectral mixed information;\n3) addressing the spectral deg radation in the modelling of\nneighbouring spectral relevance. Hong et al. ’sS p e c t r a l G P T\nempowers intelligent processing of spectral remote sensing big\ndata, and this LVM has also demonstrated its excellent spectral\nreconstruction capabilities in agriculture ( Hong et al., 2024 ). Due to\nmultispectral imaging (MSI) and hyperspectral imaging (HSI) make\nit possible to monitor crop health in the ﬁeld. The integration of\nremotely sensed multisource data, such as HSI and LiDAR (Light\ndetection and ranging), enables the monitoring of changes\noccurring in different parts of a plant ( Omia et al., 2023 ). By\nusing a large visual model to analyse these spectral data, the\nobtained crop health information can help farmers quickly and\nZhu et al. 10.3389/fpls.2025.1579355\nFrontiers inPlant Science frontiersin.org12\naccurately identify diseases and treat them, reducing the loss of\ncrop yield.\nStudies suggest that the use of LVMs for image recognition and\npredictive analysis of crop information is often more effective than\ntraditional ML algorithms. When farmers need to obtain crop\ninformation, four types of image acquisition methods can be used\nto obtain crop image information ( Figure 6 ). Then, the image\ninformation is processed through image recognition (Divided into\nfour tasks: image classi ﬁcation, object detection, semantic\nsegmentation, instance segmentation), and the identi ﬁed results\nneed to be further predictive analytics to obtain crop information\nthat farmers can understand.\nIn addition to obtaining information by analysing the\nphenotypic characteristics of crops, Feng et al. (2022) developed a\ntraditional DL model called org anelle segmentation network\n(OrgSegNet). OrgSegNet is capable of accurately capturing the\nactual sizes of chloroplasts, mitochondria, nuclei, and vacuoles\nwithin plant cell, further insp ecting plant phenotypic at the\nsubcellular level. They have tested two applications: 1) A thermo-\nsensitive rice albino leaf mutant was cultivated at cold temperature\nconditions. In the transmission electron microscope images\n(TEMs), the albinotic leaves lacked typical chloroplasts, and\nOrgSegNet failed to identify any chloroplast structures; 2) Young\nleaf chlorosis 1 (Ylc1). Young leaves of the ylc1 mutant showed\nlower levels of chlorophyll and lutein compared to corresponding\nwild type, and its TEM analysis further revealed a noticeable loose\narrangement of the thylakoid lamellar structures. It can be imagined\nthat if a large model is used to replace DL algorithms, the\nrecognition of subce llular cells may perform better, and the\nrecognition results can be further predictive analytics to obtain\ninformation that non plant experts can also understand.\n3.3.2 Automation and robotics\nEnhancing the intelligence of agricultural robots is a crucial\napplication area for large models. Conventional agricultural robot\nsystems, typically composed of perception, decision-making, and\nactuation modules, often struggle with complex visual perception\nand intelligent, real-time decision-making, especially in\nunstructured and dynamic farm environments ( Yang et al.,\n2023b ; Hamuda et al., 2016 ). Integrating large models is a\npromising approach to overcome these limitations and\nsigniﬁcantly enhance the intellectual features of agricultural robots.\nCurrent LVMs can be used in drones to monitor crops and\nobtain information on their growth, disease, yield, and other factors\n(Ganeshkumar et al., 2023 ; Chin et al., 2023 ; Pazhanivelan et al.,\n2023). In addition to the above functions, ground machines that\nused LVMs can also be used for harvesting and classifying crops, as\nwell as detecting pests up close. In ( Yang et al., 2023c ), a LVM,\nsegment anything model (SAM) ( Kirillov et al., 2023 ), uses infrared\nthermal images for chicken segmentation tasks in a zero-shot\nmeans. SAM can be used in agriculture to segment immature\nfruits on a fruit tree and quickly achieve yield prediction. Yang\net al. (2023a) subsequently proposed the Track Anything Model\n(TAM) by combining SAM and video. Unfortunately, TAM places\nmore emphasis on maintaining short-term memory rather than\nlong-term memory. Nevertheless, based on its capabilities, TAM\nstill has great potential in the agricultural ﬁeld. If its long-term\nmemory ability can be improved, it can monitor early changes in\ncrop diseases and provide early warning to farmers. Embedding\nLVMs such as SAM and TAM into robots can not only achieve\nautomation in agriculture, but these LVMs themselves can help\nachieve automation in agricultural robot design.\nBeyond perception, large models are also revolutionizing the\ndesign process of agricultural robots. As mentioned previously, Stella\net al. (2023) demonstrated using LLMs like ChatGPT to assist in\ndesigning an optimized robotic gripper for tomato picking. With the\nlatest multimodal versions like GPT-4.5, designers can now input not\nonly text descriptions but also sketches to partially automate the\nrobot design process. This integration of LVMs for perception and\nLLMs for both control logic and design automation marks a\nsigniﬁcant step towards fully autonomous agricultural systems.\n3.4 Integration of multimodal models\nLVMs provide powerful capabilities for visual analysis and\nrobotic perception. However, the most complex agricultural\nchallenges often require integrating information from multiple\nsources. MLLM recently has emerged as a prominent research\nhotspot ( Figure 7 ), which uses powerful LLMs as a core to tackle\nmultimodal tasks ( Yin et al., 2023 ). In recent years, many\nresearchers have utilized and merged diverse types of data inputs,\nsuch as text, images, audio, video ( Zhang et al., 2023a ), sensor data\n(Driess et al., 2023 ), depth information, point cloud ( Chen et al.,\n2024), and more.\nThe agricultural community has started exploring the realm of\nmultimodal learning in agricultural applications. By incorporating\nmultimodal learning techniques, the agricultural community seeks\nto unlock new opportunities for optimizing various agricultural\nprocesses and achieving improved outcomes. As an example,\nBender et al. have released an open-source multimodal dataset\nspeciﬁcally curated for agricultural robotics ( Bender et al., 2020 ).\nThis dataset was collected from cauli ﬂower and broccoli ﬁelds and\naims to foster research endeavors in robotics and ML within the\nagricultural domain. It encompasses a diverse range of data types,\nincluding stereo color, thermal, hyperspectral imagery, as well as\nessential environmental information such as weather conditions\nand soil conditions. The availability of this comprehensive dataset\nuses as a precious resource for advancing the development of\ninnovative solutions in agricultural robotics and ML. Cao et al.\n(2023) proposed a novel approach for cucumber disease recognition\nusing a MLLM that incorporates image-text-label information.\nTheir methodology effectively integrated label information from\nmany domains by employing image-text multimodal contrastive\nlearning and image self-supervi sed contrastive learning. The\napproach facilitated the measurement of sample distances within\nthe common image-text-label space. The results of the experiment\ndemonstrated the effectiveness of this innovative approach,\nachieving a recognition accuracy rate of 94.84% on a moderately\nsized multimodal cucumber disease dataset.\nZhu et al. 10.3389/fpls.2025.1579355\nFrontiers inPlant Science frontiersin.org13\nNevertheless, it is important to highlight that existing models\nprimarily rely on text-image data and are mostly limited to QA\ntasks. There is a noticeable lack of applications in the realm of\nagricultural robotics that incorporate inputs like images, text, voice\n(Human instructions), and depth information (From LiDAR or\nlaser sensors). These agricultural robots, commonly deployed for\ntasks such as fruit picking or crop monitoring ( Tao and Zhou,\n2017), present a signi ﬁcant opportunity for the integration of\nmultimodal data sources to enhance their capabilities. In short,\nlarge models lacking a high degree of multimodality perform fewer\ntasks and lack good applicability.\n3.5 The choice between large models and\ntraditional models\nThe decision to implement either a large model or a traditional\nmodel in agriculture is not straightforward. It involves considering a\nFIGURE 5\nAn LLM-based AI Agent architecture for agricultural decision-making support.\nZhu et al. 10.3389/fpls.2025.1579355\nFrontiers inPlant Science frontiersin.org14\nmultitude of factors, such as the volume and quality of available\ndata, the required model generalizability, and the practical limits on\ncomputational power and inference speed. However, by analyzing\nthe studies of Deforce et al. (2024) ; Zhao et al. (2023a) , and Cao\net al. (2023), we found that these considerations can be effectively\ncategorized under two primary factors: Data and deployment\nconditions. Similar to how large models are divided into LLM,\nLVM, and MLLM, traditional models can be classiﬁ ed according to\nthe speci ﬁc agricultural task, falling into the categories of NLP, CV,\nand multimodal. For instance, models like AGRONER and PSO-\nLSTM are designed to handle NLP tasks ( Veena et al., 2023 ; Zheng\nand Li, 2023 ), AG-YOLO and CMTNet address CV tasks ( Lin et al.,\n2024; Guo et al., 2025b ), while ITK-Net and Multi-ResNet34 are\ntailored for multimodal applications ( Zhou et al., 2021 ; Zhang et al.,\n2022a). Before selecting a model, it is best to ﬁrst determine which\ncategory the agricultural task belongs to.\nWhen approaching an agricultural task, a critical step is to assess\nthe sufﬁciency of available data. If a substantial volume of high-quality,\ntask-speciﬁc data is available, a traditional model becomes a good\noption. Conversely, in scenarios marked by data insuf ﬁciency,\nleveraging a large model is often the more suitable choice. Figure 8\npresents a comparative analysis of traditional models versus large\nmodels based on data conditions and deployment constraints. PSO-\nLSTM can be retrained on abundant data, and it can deliver superior\nperformance for a particular agricultural task, thus positioning this\nmodel as a “specialist”.T i m e G P T ,o nt h eo t h e rh a n d ,f u n c t i o n sa sa\n“generalist”, capable of handling diverse, non-speci ﬁc agricultural\ntasks using only minimal ﬁne-tuning or a zero-shot approach in\ndata-scarce situations, thereby avoiding the need for complete model\nretraining for each new task ( Deforce et al., 2024). The pre-embedded\nknowledge within large models can effectively compensate for the lack\nof domain-speciﬁcd a t a .\nOn the other hand, deployment conditions are also a crucial\nfactor in model selection. While devices with high computational\ncapacity can deploy both large models and traditional models, the\nsigniﬁcant computational and time costs associated with large\nmodels make them unsuitable for edge devices and systems\nrequiring real-time response. For an agricultural task that requires\nthe model to be deployed on an edge device with real-time detection\nneeds, ITK-Net is the pragmatic and superior choice due to its\nefﬁciency and low resource requirements ( Zhou et al., 2021 ). While\nthe ITLMLP model proposed by Cao et al. (2023) also targets crop\ndisease recognition, it is suited for deployment only on devices that\ncan handle high computational costs. As a large model, ITLMLP ’s\ndeployment conditions are considerably more stringent than those\nof\nITK-Net, the traditional model. However, this does not imply\nthat ITLMLP is without its merits. Its value lies not in real-time ﬁeld\ndeployment, but in its powerful of ﬂine analysis capabilities. By\nbatch-processing vast agricultural data stored on cloud servers, it\ncan perform in-depth retrospective diagnostics and trend analysis.\nLeveraging its powerful feature extraction and generalization\ncapabilities, acquired from pre-training on large-scale data,\nITLMLP can conduct reclassi ﬁcation of historical disease data,\ncompile statistics on disease occurrence frequencies across\ndifferent periods, and uncover p otential correlations between\nimage features and speci ﬁc environmental descriptions. By the\nway, by optimizing the model architecture, using ef ﬁcient\ninference algorithms, and uti lizing hardware acceleration\ntechniques, the real-time performance of LVMs can be improved\nto a certain extent ( Chen et al., 2023b ). In addition, we have also\nFIGURE 6\nFarmers can obtain crop information through the process of image acquisition, image recognition, predictive analytics.\nZhu et al. 10.3389/fpls.2025.1579355\nFrontiers inPlant Science frontiersin.org15\nFIGURE 7\nMultimodal information fusion analysis driven by MLLM.\nFIGURE 8\nComparison of large models and traditional models for agricultural tasks.\nZhu et al. 10.3389/fpls.2025.1579355\nFrontiers inPlant Science frontiersin.org16\ndiscovered that ITLMLP could process a large dataset to generate\nhighly accurate annotated labels, which can then be used to train\nsmaller, more ef ﬁcient models like ITK-Net. This creates a\nsynergistic ecosystem where the power of large models enables\nthe effectiveness of traditional models on the edge.\nThe choice between a large model and a traditional model for\nagricultural tasks is not a matter of one being de ﬁnitively superior to\nthe other, but rather a strategic decision based on a careful\nevaluation of trade-offs. Large models, with their powerful\ngeneralization capabilities, offer a robust solution for data-scarce\nenvironments, while traditional models excel in data-rich scenarios\nwhere their specialized nature can be fully leveraged. Similarly, the\nhigh computational cost of large models makes them suitable for\nofﬂine, server-based analysis, whereas the ef ﬁciency of traditional\nmodels is indispensable for real-time, on-device deployment.\n4 Ethical issues and responsible use of\nlarge vision and language models in\nagriculture\nAs large models demonstrate their powerful potential and are\nincreasingly applied to agricultural tasks (referencing section 3), it is\ncrucial to critically examine the ethical and societal implications of\ntheir deployment. However, there are often ethical and\nresponsibility issues in the development and deployment process\nof AI today. The digital gap between those who have the resources\nto develop and utilize large models and those who cannot afford to\ndo so creates an inequality in accessing large models, resulting in an\nunfair distribution of risks and bene ﬁts (Harfouche et al., 2024 ). Not\nonly that, this divide can be exacerbated by the presence of AI biases\n(Dara et al., 2022 ; Ryan, 2023). Accordingly, to ensure ethical issues\nand responsible use of large models, this chapter starts from the\nethical and responsibility issues in the agricultural large models and\nexplore corresponding measures.\n4.1 Ethical considerations in the\ndeployment of large models in agriculture\nPredicting and solving ethics problems of large models in\nagriculture is a critical scienti ﬁc and societal challenge. Although\nlarge models point the way for the future of smart agriculture, due\nto their characteristic of being in ﬂuenced by close association, large\nmodels often learn some bad knowledge in addition to useful\nknowledge. Ethical issues have always been an indispensable topic\nof discussion in the process of technological progress (Such as the\nethical issues discussed by Holmes et al. in the ﬁeld of education\nregarding educational AI ( Holmes et al., 2022 )), and we also need to\npay attention to ethics issues when using large models in the\nagricultural direction. As me ntioned below, many relevant\ninstitutions and personnel have put forward their own ideas on\nethical issues related to large models.\nWeidinger et al. (2021) put forward six types of ethical risks\n(Figure 9): 1) Malicious uses, 2) Human-computer interaction\nharms, 3) Automation, access, and environmental harms, 4)\nInformation hazards, 5) Misinformation harms, and 6)\nDiscrimination, exclusion, and to xicity. Understanding these issues\ncan help us responsibly use large models in the agricultural ﬁeld.\n1. Malicious uses. Prior to the release of GPT-4, OpenAI hired\na team of 50 experts and scholars to conduct a six-month\nadversarial test on GPT-4. Andrew White, a professor of\nchemical engineering at the University of Rochester who\nwas invited to participate in the test, stated that early\nversions of GPT-4 could assist in the manufacture of\nFIGURE 9\nThe ethical issues faced by large models.\nZhu et al. 10.3389/fpls.2025.1579355\nFrontiers inPlant Science frontiersin.org17\nchemical weapons and even provide a convenient\nmanufacturing location. From the perspective of the\nagricultural sector, if this issue is not properly addressed,\nsome may use large models to learn ways to destroy other\npeople’s farmland for the sake of pro ﬁt, thereby allowing\nthemselves to have a larger market. Over time, this will lead\nto vicious competition in the market.\n2. Human-computer interaction harms. The potential harms\nof human-computer interaction arise when users\nexcessively trust a large mod el or mistakenly treat it\nas human.\n3. Automation, access, and environmental harms. The large\nmodel can give rise to automation, access, and\nenvironmental harms due to its potential environmental\nor downstream economic impacts.\n4. Information hazards. Due to the involvement of\ninformation from different countries, religions, and\nethnicities, model outputs leaking or inferring sensitive\ninformation often led to political violence.\n5. Misinformation harms. A study discussed the potential\nrisks of using poorly performing large models. The\noriginal intention of this study was to provide a natural\nlanguage generation model in MOOC to respond to\nstudents and improve their participation rate ( Li and\nXing, 2021 ). Even so, due to the poor performance of the\nmodel, the corresponding negative results further reduced\nthe enthusiasm of students. If a poorly performing large\nmodel is used in the agricultural ﬁeld, it may mislead\nfarmers in their judgment (Such as analysing incorrect\ndisease types), not only causing further damage to crops\nin the farmland, but also making farmers increasingly\ndistrust the large model. For this phenomenon, Angelone\net al. proposed that warning labels can be applied to the\ncontent generated by the large model ( Angelone et al.,\n2022), but this also involves the trust issue of the large\nmodel in its own generated results.\n6. Discrimination, exclusion, and toxicity. Two researches\nhave indicated that potential discrimination, exclusion,\nand toxicity issues may occur if adopting a model that is\naccurate but unfair ( Sha et al., 2021 ; Merine and\nPurkayastha, 2022 ).\nDespite Weidinger et al. ’s viewpoint can provide us with a\nfundamental understanding of the risks associated with large\nmodels, manners of systematic ethical supervision of large\nmodels’ research and innovation (R&I) are especially restricted.\nCoincidentally, the European Commission has of ﬁcially approved\ncomprehensive “ethics guidelines for trustworthy AI ” speciﬁcally\ndesigned for R&I. These guidelines require that principal\ninvestigators recognize and tackle the ethical matters raised by\ntheir proposed research. Principal investigators are also required to\nadhere to ethical principles and relevant legislation in their work. In\na similar vein, Stanford University ’s Ethics and Society Review\nnecessitates researchers to distinguish potential societal hazards\nassociated with their research and incorporate mitigation measures\ninto their research design ( Bernstein et al., 2021 ).\nFurthermore, projects with large models have a vast amount of\ndata and often raise ethics issues. For instance, while raw plant\nscience data itself may not inherently fall within the scope of the\nEuropean Union General Data Protection Regulation (GDPR) as\npersonal data, it can become subject to GDPR regulations when\nlinked to identi ﬁable individuals or speci ﬁc farm locations tied to\nindividuals, creating complex challenges concerning data\nownership and privacy protection ( Harfouche et al., 2024 ). Thus,\nrelevant guidelines must consider code of conduct for data sharing,\nprivacy protection, and the overall governance of datasets.\n4.2 Responsible use in agriculture\nWith the expanding developm ent and utilization of large\nmodels, there is a growing recognition of the need for agile and\neffective regulatory oversight. To address this issue, it may be\nnecessary to use AI technology to assist in overseeing the\ndevelopment and deployment of large models. Regarding this\naspect, the AI Act, which has been jointly agreed upon by the\nEuropean Parliament and the Council of Europe, represents the ﬁrst\ncomprehensive set of harmonized rules on a global scale. It\npromotes responsible large model designment and development\nby regulating large model across various applications and contexts\nbased on a risk-based framework. Within the framework, careful\nconsideration must be given to the level of risk involved and how to\nevaluate different large models as risk-free or low-risk.\nTo evaluate the risk level of a large model, we focus on four\naspects: transparency, privacy, equality, and bene ﬁcence. On the\nother hand, in addition to developing and adhere to a strong\nregulatory framework that guides the development, deployment,\nand use of large models, regulatory methods also need to be\nconsidered. Consider the potent ial societal impact, potential\nharms, and long-term implications of the technology. Firstly, due\nto the wide applicability of large models, we cannot make a one size\nﬁts all approach. Regulation must adapt to speci ﬁc issues in different\ndomains. The United States ’food and drug administration (FDA)\nhas tailored potential regulatory methods for AI and ML\ntechnologies used in medical devices, categorizing them into three\nmajor categories based on risk levels: Class I (Low risk), Class II\n(Moderate risk), and Class III (High risk). Large models in\nagriculture can also be regulated according to the FDA ’s\napproach, dividing them into several types of models ranging\nfrom low risk to high risk. For example, genetically modi ﬁed\ncrops may have environmental impacts, food safety issues, and\necosystem damage, so large models targeting genetically modi ﬁed\ncrops should be included in high-risk types. For large models of\nordinary crops, they can be classi ﬁed as low-risk types. And the\nregulatory methods proposed by relevant departments should be\nmade public to ensure transparency of information. Regulators can\npromote fairness in the deployment of agricultural large models by\nenforcing the use of diverse and representative data sources, which\nZhu et al. 10.3389/fpls.2025.1579355\nFrontiers inPlant Science frontiersin.org18\nhelps mitigate potential biases present in the training data ( Meskó \nand Topol, 2023 ).\nFrom the perspective of bene ﬁcence and privacy, privacy issues\nrelated to large models have received little attention or investigation\nin reviewed research ( Yan et al., 2024 ). Speci ﬁcally, if the training\nset used to train a large model contains some personal privacy\ninformation that has not been authorized by the information owner.\nThe disregard for privacy con cerns is especially worrisome\nconsidering that LLM-based inno vations involve stakeholders ’\nnatural languages, which may c ontain personal and sensitive\ninformation pertaining to their private lives and identities ( Brown\net al., 2022 ). If users unintentionally learn about this information\nwhile using a large model, it may cause harm to the bene ﬁcence of\nthe information owner. Developers of large models should ensure\nthey gain explicit consent from individuals before collecting and\nutilizing these personal data. Clearly communicate the purpose and\nscope of data usage, and offer individuals with the choice to choose\nout or request data deletion. Besides, limit the amount of personal\nand sensitive data collected and stored. Follow the principle of data\nminimization, ensuring that only necessary data is collected and\nretained. Anonymize or aggregate data whenever possible to protect\nindividual privacy.\nIn general, governance approaches that promote responsible\nutilization of large models and focus on the outcomes rather than\nthe technology itself will enhance research efforts and drive more\ninnovation. By combining governance and ethics, we can harness\nthe powerful synergy to expedite the implementation of large\nmodels in agriculture and other domains, fostering innovation at\na larger scale.\n5 Challenges and future directions\nAlthough large models can play a powerful role in the ﬁeld of\nagriculture, they still face challenges in many aspects.\n5.1 Technical and practical challenges\n5.1.1 Difﬁculty in obtaining agricultural data\nA primary and recurring obstacle highlighted throughout this\nreview is the acquisition of suitable agricultural data. While large\nmodels’ data generation capabilities can partially alleviate this, as\ndiscussed in section 3.2.1, several fundamental dif ﬁculties persist:\n1. Cost and quality: Acquiring comprehensive, high-quality,\nand accurately labeled real-world data is a time-consuming,\nlabor-intensive, and costly process, especially for\nsupervised learning approaches ( Li et al., 2023a ; Lu and\nYoung, 2020 ).\n2. Privacy and trust: As mentioned in section 4.2, the private\nnature of farmland data raises signi ﬁcant privacy and trust\nconcerns among farmers, often leading to a reluctance to\nshare information crucial for model training.\n3. Temporal complexity : Agricultural data is inherently\ntemporal. The need to capture entire crop growth cycles,\nwhich are in ﬂuenced by daily, seasonal, and annual\nvariations, adds another layer of complexity to data\ncollection efforts ( Li et al., 2023b ).\n5.1.2 Low training efﬁciency\nDirectly related to the need for massive datasets is the challenge\nof low training ef ﬁciency and high computational cost. As\nsystematically compared against traditional models in section 3.5\n(Figure 8 ), training large agricultural models is a resource-intensive\nendeavor. Their massive parameter counts demand signi ﬁcant\ncomputational power and lengthy training times, often measured\nin thousands of GPU hours ( Li et al., 2023b ). This stands in stark\ncontrast to the ef ﬁciency of traditional models like YOLO and Faster\nR-CNN, whose lower computational requirements make them a\nmore practical and cost-effective solution for many speci ﬁc, real-\ntime agricultural tasks ( Badgujar et al., 2024 ). This ef ﬁciency gap\nexplains the continued prevalence of traditional models despite the\nemergence of more powerful large-scale architectures.\n5.1.3 Distribution shift\nThe problem of distribution shift is a major challenge when using\nlarge models in agriculture. When the data encountered by the model\nduring deployment is obviously different from the data used in its\ntraining phase, a distribution shift will occur. The environmental\nconditions for collecting data may vary greatly in different regions\nand climates. These changes may include differences in crop types,\nsoil conditions, weather patterns, and agricultural practices, all of\nwhich can lead to signi ﬁcant changes in data distribution (Wiles et al.,\n2021). The distribution shift will result in the trained large model not\nhaving strong applicability and may not achieve good results in some\nagricultural tasks. For example, it has been proven that applying large\nmodels directly to leaf segmentation tasks in a zero-shot means led to\nunsatisfactory performance, which can be attributed to possible\ndistribution shifts ( Chawla et al., 2021 ).\n5.1.4 The lag of data\nAfter the trained large model is put into use, the data used for\ntraining has a certain timeliness for a long period of time. But after a\nlong time, some data lags in time, and the results obtained by using\na large model may deviate from the current facts ( Figure 10 ).\n5.1.5 Query formulation impacts model output\nThe results obtained from large models can vary signi ﬁcantly\ndepending on how the query is formulated. Like Figure 11 , when\nmultiple images are spliced together for questioning, GPT-4\nprovides ambiguous answers; When only asking for one image,\nGPT-4 provides a clearer answer.\nTo clear these obstacles, future research and development work\nneeds to pay attention to model optimization techniques such as\nmodel compression and ef ﬁcient network structure design, reducing\nmodel size without affecting performance (Zhong et al., 2023). It is also\nZhu et al. 10.3389/fpls.2025.1579355\nFrontiers inPlant Science frontiersin.org19\nnecessary to provide update and maintenance functions for the model\nto ensure its timeliness. Developers need to write relevant usage\ninstructions to help users get started quickly. Notably, emerging\nframeworks like RAG offer a direct solution to the data lag and\naccuracy challenges by connecting LLMs to real-time, external\nknowledge bases. Similarly, dev eloping more sophisticated AI\nAgents capable of autonomous planning and tool use will be crucial\nfor creating robust and adaptable agricultural systems.\n5.2 Infrastructure and cost barriers\nApplying large models to rural areas faces signi ﬁcant barriers\nrelated to poor connectivity and high implementation costs. These\nlimitations disproportionately affect small-scale farmers and regions\nwith underdeveloped infrastr ucture, exacerbating existing\ninequalities in agricultural productivity and technological access\n(Da Silveira et al., 2023 ).\nDibbern et al. (2024) found that farmers often abandon digital\ntools due to unreliable broadband or mobile connectivity, even\nwhen initial investments are made. Technologies like IoT, cloud-\nbased analytics, and real-time monitoring systems remain\nunderutilized in areas lacking stable network access. This has\nbrought some warnings for the application of large models in\nrural areas. In addition, the high cost of agricultural machinery\nusing large models — render them inaccessible to resource-limited\nfarmers. For example, autonomous machinery and AI platforms\noften require upfront inves tments exceeding $10,000 USD, a\nprohibitive sum for smallholders ( Bolfe et al., 2020 ).\nTo overcome poor connectivity, investing in and expanding rural\nbroadband and mobile infrastructure is crucial, potentially through\ngovernment subsidies, public-private partnerships, and the\nexploration of alternative network solutions like satellite internet or\nmesh networks tailored to agricultural regions. To mitigate high\nimplementation costs, promoting the development of affordable,\nmodular agricultural machinery and large model platforms designed\nspeciﬁcally for smallholder farmers is essential. In short, bridging the\ndigital divide and promoting inclusive technological progress requires\njoint efforts among technology developers, agricultural researchers,\npolicy makers, and local farmer organizations.\n5.3 Future trends in the integration of\nagricultural and food sectors and large\nmodels\nIn the future, there will undoubtedly be agricultural large\nmodels with better performance and higher applicability. And the\nlarge models in agriculture should not be limited to text and image\ninputs. We believe that future multimodal agricultural models can\nsupport multimodal information such as videos (Analyzing crops in\nvideos) and audio (Tapping watermelons, and judging maturity\nthrough the sound emitted). On the other hand, agriculture is\nclosely related to food, and the development of large models in\nagriculture is likely to promote the development of large models in\nthe food domain. Trust is indispensable for agriculture and food\nsystem technologies given food ’s universality and importance to\npeople ( Tzachor et al., 2022 ). Researchers need to navigate\nFIGURE 10\nThe lag of data.\nZhu et al. 10.3389/fpls.2025.1579355\nFrontiers inPlant Science frontiersin.org20\ncomplicated social, political, economic, and environmental\nlandscapes to develop appropriate large models in the food\nindustry. In the future food industry, researchers will strive to\nestablish trust with governmental agencies and funders, as well as\nwith food system partners, to provide food and products that the\npublic trusts ( Alexander et al., 2024 ).\nOverall, although the agricultural large model still faces many\nchallenges at present, we believe that through the joint efforts of\nrelevant researchers in the future, these challenges can be properly\naddressed. And due to the close relationship between the food and\nagricultural domains, with the gradual development of agricultural\nlarge models, food large models will also receive further research,\nthereby achieving mutual positive feedback between the\ndevelopment of large models in these two ﬁelds.\n6 Conclusion\nIn summary, this study investigated the application status of large\nmodels in the agricultural ﬁeld. Our analysis establishes that these\nmodels offer unprecedented advantages through their capacity for\ncomplex reasoning, multimodal information processing, and the\nexecution of nuanced tasks ranging from pest identi ﬁcation to\nrobotic automation. We further determined that the ef ﬁcacy of these\np o w e r f u lt o o l si ss i g n iﬁcantly ampliﬁed when they are tailored to the\nagricultural domain, a crucial strategy for overcoming the pervasive\nchallenge of limited labeled data. Furthermore, this review provided a\npragmatic framework for choosing between large and traditional\nmodels, emphasizing that the decision hinges on a careful trade-off\nbetween data availability and deployment constraints. While large\nmodels excel as “generalists” in data-scarce or of ﬂine analytical\nscenarios, ef ﬁcient traditional models remain indispensable as\n“specialists” for real-time, on-device tasks.\nHowever, this vast potentia li st e m p e r e db yc r i t i c a l ,\ninterconnected challenges that must be addressed. A primary\nhurdle is the acquisition and utilization of suitable agricultural data;\nissues of data scarcity, high collection costs, inherent data diversity\n(across crops, regions, conditions), privacy concerns associated with\nfarmland data, and the need for time-series information create\nsigni ﬁcant obstacles. Furthermore, the high computational\nresources required for training and deploying large models, coupled\nwith the often-limited internet connectivity and ﬁnancial resources in\nrural areas, creates a signi ﬁcant digital divide, potentially excluding\nsmallholder farmers. Technical issues such as model susceptibility to\nFIGURE 11\nDifferent questioning methods can lead to different results.\nZhu et al. 10.3389/fpls.2025.1579355\nFrontiers inPlant Science frontiersin.org21\ndistribution shifts between training and deployment environments,\nthe problem of data lag impacting real-time relevance, and sensitivity\nto query formulation also impact the reliability and practical\napplicability of current models. Finally, overarching ethical\nconsiderations, including potential biases in data or algorithms,\nensuring data privacy, promoting equitable access to technology,\nand preventing misuse, are paramount and demand careful\nconsideration and robust governance frameworks.\nAlthough our study is comprehensive, there are inherent\nlimitations to studying a rapidly developing ﬁeld. To move\nforward, future research must directly confront the limitations and\nchallenges identiﬁed. Developing novel techniques to mitigate data\nscarcity— such as advanced data augmentation and self-supervised\nlearning tailored for agriculture — is a critical priority. Expanding\nmultimodal capabilities to robustly incorporate inputs like video,\naudio, and diverse sensor data will unlock new frontiers in precision\nfarming. Crucially, research must move beyond theoretical ethics to\nthe practical implementation of governance structures for AI in\nagriculture. Furthermore, a signi ﬁcant opportunity lies in exploring\nthe synergistic relationship between agricultural large models and the\nbroader food system, addressing challenges from farm to fork.\nLarge models stand poised to be transformative technologies for\nagriculture. While signiﬁcant challenges remain, the potential beneﬁts\nfor productivity, sustainability, and food security are immense.\nAddressing the technical hurdles, bridging the digital divide, and\nnavigating the ethical landscape through collaborative, responsible\ninnovation will be key to realizing this potential. We hope this article\nserves as a valuable resource and a cornerstone, stimulating further\nresearch and guiding the development of future agricultural large\nmodels that are not only powerful but also practical, ef ﬁcient, and\nbeneﬁcial for all stakeholders in the global food system.\nData availability statement\nThe original contributions presented in the study are included\nin the article/ Supplementary Material . Further inquiries can be\ndirected to the corresponding authors.\nAuthor contributions\nHZ: Conceptualization, Funding acquisition, Methodology,\nSupervision, Writing – original draft, Writing – review & editing.\nSQ: Conceptualization, Formal Analysis, Investigation, Methodology,\nResources, Software, Visualization, Writing– original draft, Writing –\nreview & editing. MS: Funding acquisition, Writing– review & editing.\nCL: Software, Writing – original draft. AL: Software, Writing –\noriginal draft. JG: Conceptualization, Investigation, Project\nadministration, Writing– review & editing.\nFunding\nThe author(s) declare ﬁnancial support was received for the\nresearch and/or publication of this article. Our research was\nsupported by the Natural Science Foundation of Guangxi (No.\n2024 GXNSFBA010381), the National Natural Science Foundation\nof China (No. 62361006), Guangxi Young Elite Scientist Sponsorship\nProgram (GXYESS2025081), and the grant (No. NCOC-24-02) from\nKey Laboratory of Nonlinear Circuit and Optical Communications\n(Guangxi Normal University), Education Department of Guangxi\nZhuang Autonomous Region.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential con ﬂict of interest.\nGenerative AI statement\nThe author(s) declare that no Generative AI was used in the\ncreation of this manuscript.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their af ﬁliated organizations,\nor those of the publisher, the editors and the reviewers. Any product\nthat may be evaluated in this article, or claim that may be made by its\nmanufacturer, is not guaranteed or endorsed by the publisher.\nSupplementary material\nThe Supplementary Material for this article can be found online\nat: https://www.frontiersin.org/articles/10.3389/fpls.2025.1579355/\nfull#supplementary-material\nReferences\nAhirwar, S., Swarnkar, R., Bhukya, S., and Namwade, G. (2019). Application of drone in\nagriculture.Int. J. Curr. Microbiol. Appl. Sci.8, 2500–2505. doi: 10.20546/ijcmas.2019.801.264\nAlexander, C. S., Yarborough, M., and Smith, A. (2024). Who is responsible for\n‘responsible AI’?: Navigating challenges to build trust in AI agriculture and food system\ntechnology. Precis. Agric.25, 146 –185. doi: 10.1007/s11119-023-10063-3\nAngelone, A. M., Galassi, A., and Vittorini, P. (2022). “Improved automated\nclassiﬁcation of sentences in data science exercises, ” in In Methodologies and\nIntelligent Systems for Technology Enhanced Learning, 11th International Conference,\nVol. 11. 12 –21. doi: 10.1007/978-3-030-86618-1_2\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., et al. (2023).\nPalm 2 technical report. doi: 10.48550/arXiv.2305.10403\nA y m e n ,F . ,M o n i r ,H . ,a n dP e s t e r ,A .( 2 0 2 4 ) . “Large Vision Models: How\nTransformer-based Models excelled over Traditional Deep Learning Architectures in\nVideo Processing, ” in 2024 5th International Conference on Artiﬁcial Intelligence,\nRobotics and Control (AIRC).5 0 –54. doi: 10.1109/AIRC61399.2024.10672087\nBadgujar, C. M., Poulose, A., and Gan, H. (2024). Agricultural object detection with\nYou Only Look Once (YOLO) Algorithm: A bibliometric and systematic literature\nreview. Comput. Electron. Agric.223, 109090. doi: 10.1016/j.compag.2024.109090\nZhu et al. 10.3389/fpls.2025.1579355\nFrontiers inPlant Science frontiersin.org22\nBai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., et al. (2023). Qwen-vl: A frontier\nlarge vision-language model with versatile abilities. doi: 10.48550/arXiv.2308.12966\nBender, A., Whelan, B., and Sukkarieh, S. (2020). A high-resolution, multimodal data\nset for agricultural robotics: A Ladybird ’s-eye view of Brassica. J. Field Robot.37, 73–96.\ndoi: 10.1002/rob.21877\nBengio, Y., Ducharme, R., and Vincent, P. (2000). A neural probabilistic language\nmodel. Adv. Neural Inf. Process. Syst.13.\nBernstein, M. S., Levi, M., Magnus, D., Rajala, B. A., Satz, D., and Waeiss, Q. (2021).\nEthics and society review: Ethics re ﬂection as a precondition to research funding. Proc.\nNatl. Acad. Sci.118, e2117261118. doi: 10.1073/pnas.2117261118\nBi, Z., Zhang, N., Xue, Y., Ou, Y., Ji, D., Zheng, G., et al. (2023). Oceangpt: A large\nlanguage model for ocean science tasks. doi: 10.48550/arXiv.2310.02031\nBodnar, C., Bruinsma, W. P., Lucic, A., Stanley, M., Allen, A., Brandstetter, J., et al.\n(2024). Aurora: A foundation model of the atmosphere. doi: 10.48550/\narXiv.2405.13063\nBolfe, É L, Jorge, L. A. C., Sanches, I. D., Luchiari Ju ́ nior, A., da Costa, C. C., Victoria,\nD. C., et al. (2020). Precision and digital agriculture: adoption of technologies and\nperception of Brazilian farmers. Agriculture 10, 653. doi: 10.3390/agriculture10120653\nBommasani, R., Hudson, D. A., Adeli, E., Adeli, E., Altman, R., Arora, S., et al. (2021).\nOn the opportunities and risks of foundation models. doi: 10.48550/arXiv.2108.07258\nBosilj, P., Aptoula, E., Duckett, T., and Cielniak, G. (2020). Transfer learning between\ncrop types for semantic segmentation of crops versus weeds in precision agriculture. J.\nField Robot.37, 7 –19. doi: 10.1002/rob.21869\nBouguettaya, A., Zarzour, H., Kechida, A., and Taberkit, A. M. (2022). Deep learning\ntechniques to classify agricultural crops through UAV imagery: A review. Neural\nComputing Appl.34, 9511 –9536. doi: 10.1007/s00521-022-07104-9\nBrown, H., Lee, K., Mireshghallah, F., Shokri, R., and Tramèr, F. (2022). “What does\nit mean for a language model to preserve privacy?, ” in In Proceedings of the 2022 ACM\nConference on Fairness, Accountability, and Transparency. 2280 –2292. doi: 10.1145/\n3531146.3534642\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., et al.\n(2023). Sparks of arti ﬁcial general intelligence: Early experiments with gpt-4.\ndoi: 10.48550/arXiv.2303.12712\nCao, Y., Chen, L., Yuan, Y., and Sun, G. (2023). Cucumber disease recognition with\nsmall samples using image-text-label-based multi-modal language model. Comput.\nElectron. Agric.211, 107993. doi: 10.1016/j.compag.2023.107993\nChawla, S., Singh, N., and Drori, I. (2021). “Quantifying and alleviating distribution\nshifts in foundation models on review classi ﬁcation,” in NeurIPS 2021 Workshop on\nDistribution Shifts: Connecting Methods and Applications . https://openreview.net/\nforum?id=OG78-TuPcvL.\nChelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P., et al. (2013). One\nbillion word benchmark for measuring progress in statistical language modeling.\ndoi: 10.48550/arXiv.1312.3005\nChen, C., Du, Y., Fang, Z., Wang, Z., Luo, F., Li, P., et al. (2024). Model composition\nfor multimodal large language models. doi: 10.48550/arXiv.2402.12750\nChen, F., Giuffrida, M. V., and Tsaftaris, S. A. (2023a). “Adapting vision foundation\nmodels for plant phenotyping, ” in Proceedings of the IEEE/CVF International\nConference on Computer Vision. 604 –613.\nChen, L., Zaharia, M., and Zou, J. (2023b). Frugalgpt: How to use large language\nmodels while reducing cost and improving performance. doi: 10.48550/\narXiv.2305.05176\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P.D.O., Kaplan, J., et al. (2021).\nEvaluating large language models trained on code. doi: 10.48550/arXiv.2107.03374\nChin, R., Catal, C., and Kassahun, A. (2023). Plant disease detection using drones in\nprecision agriculture. Precis. Agric.24, 1663 –1682. doi: 10.1007/s11119-023-10014-y\nDai, H., Liu, Z., Liao, W., Huang, X., Cao, Y., Wu, Z., et al. (2023). Auggpt:\nLeveraging chatgpt for text data augmentation. doi: 10.48550/arXiv.2302.13007\nDara, R., Hazrati Fard, S. M., and Kaur, J. (2022). Recommendations for ethical and\nresponsible use of arti ﬁcial intelligence in digital agriculture. Front. Artif. Intell. 5.\ndoi: 10.3389/frai.2022.884192\nDas, D., Banerjee, D., Aditya, S., and Kulkarni, A. (2024). MATHSENSEI: a tool-\naugmented large language model for math ematical reasoning. doi: 10.48550/\narXiv.2402.17231\nDa Silveira, F., Da Silva, S. L. C., MaChado, F. M., Barbedo, J. G.A., and Amaral, F. G.\n(2023). Farmers ’ perception of the barriers that hinder the implementation of\nagriculture 4.0. Agric. Syst.208, 103656. doi: 10.1016/j.agsy.2023.103656\nDeforce, B., Baesens, B., and Asensio, E. S. (2024). Time-series foundation models for\nforecasting soil moisture levels in smart agriculture. arXiv preprint arXiv:2405.18913.\ndoi: 10.48550/arXiv.2405.18913\nDibbern, T., Romani, L. A. S., and Massruha ́ , S. M. F. S. (2024). Main drivers and\nbarriers to the adoption of Digital Agriculture technologies. Smart Agric. Technol.8,\n100459. doi: 10.1016/j.atech.2024.100459\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T.,\net al. (2020). An image is worth 16x16 words: Transformers for image recognition at\nscale. doi: 10.48550/arXiv.2010.11929\nDriess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery, A., Wahid, A., et al. (2023).\nPalm-e: An embodied multimodal language model. doi: 10.48550/arXiv.2303.03378\nElMasry, G., Mandour, N., Al-Rejaie, S., Belin, E., and Rousseau, D. (2019). Recent\napplications of multispectral imaging in seed phenotyping and quality monitoring — An\noverview. Sensors 19, 1090. doi: 10.3390/s19051090\nFeng, X., Yu, Z., Fang, H., Jiang, H., Yang, G., Chen, L., et al. (2022). Plantorgan\nhunter: a deep learning-based framework for quantitative pro ﬁling plant subcellular\nmorphology. doi: 10.21203/rs.3.rs-1811819/v1\nGaneshkumar, C., David, A., Sankar, J. G., and Saginala, M. (2023). “Application of\ndrone Technology in Agriculture: A pred ictive forecasting of Pest and disease\nincidence,” in Applying drone technologies and robotics for agricultural sustainability,\nIGI Global, 50 –81. doi: 10.4018/978-1-6684-6413-7.ch004\nGao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., et al. (2023). Retrieval-augmented\ngeneration for large language models: A survey. doi: 10.48550/arXiv.2312.10997\nGeitmann, A., and Bidhendi, A. J. (2023). Plant blindness and diversity in AI\nlanguage models. Trends Plant Sci.28, 1095 –1097. doi: 10.1016/j.tplants.2023.06.016\nGirdhar, R., El-Nouby, A., Liu, Z., Singh, M., Alwala, K. V., Joulin, A., et al. (2023).\n“Imagebind: One embedding space to bind them all, ” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. 15180 –15190. doi: 10.1109/\nCVPR52729.2023.01457\nGoertzel, B. (2014). Arti ﬁcial general intelligence: concept, state of the art, and future\nprospects. J. Artif. Gen. Intell.5, 1 –48. doi: 10.2478/jagi-2014-0001\nGuo, X., Feng, Q., and Guo, F. (2025b). CMTNet: a hybrid CNN-transformer\nnetwork for UAV-based hyperspectral crop classi ﬁcation in precision agriculture. Sci.\nRep. 15, 12383. doi: 10.1038/s41598-025-97052-w\nGuo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., et al. (2025a). Deepseek-r1:\nIncentivizing reasoning capability in llms via reinforcement learning. doi: 10.48550/\narXiv.2501.12948\nHamuda, E., Glavin, M., and Jones, E. (2016). A survey of image processing\ntechniques for plant extraction and segmentation in the ﬁeld. Comput. Electron.\nAgric. 125, 184 –199. doi: 10.1016/j.compag.2016.04.024\nHarfouche, A. L., Petousi, V., and Jung, W. (2024). AI ethics on the road to\nresponsible AI plant science and societal welfare. Trends Plant Sci. 29, 104 –107.\ndoi: 10.1016/j.tplants.2023.12.016\nHe, K., Gkioxari, G., Dolla ́ r, P., and Girshick, R. (2017). “Mask r-cnn,” in Proceedings\nof the IEEE international conference on computer vision. 2961 –2969. doi: 10.48550/\narXiv.1703.06870\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision and pattern\nrecognition. 770 –778. doi: 10.1109/CVPR.2016.90\nHo, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., et al. (2022). Imagen\nvideo: High de ﬁnition video generation with diffusion models. doi: 10.48550/\narXiv.2210.02303\nHolmes, W., Porayska-Pomsta, K., Holstein, K., Sutherland, E., Baker, T., Shum, S.\nB., et al. (2022). Ethics of AI in education: Towards a community-wide framework. Int.\nJ. Artif. Intell. Educ.32 (3), 504-526. doi: 10.1007/s40593-021-00239-1\nHolzinger, A., Langs, G., Denk, H., Zatloukal, K., and Müller, H. (2019). Causability\nand explainability of arti ﬁcial intelligence in medicine. Wiley Interdiscip. Rev.: Data\nMin. Knowledge Discov.9, e1312. doi: 10.1002/widm.1312\nHong, D., Zhang, B., Li, X., Li, Y., Li, C., Yao, J., et al. (2024). SpectralGPT: Spectral\nremote sensing foundation model. IEEE Trans. Pattern Anal. Mach. Intelligence. 46 (8),\n5227–5244. doi: 10.1109/TPAMI.2024.3362475\nKarthikeyan, L., Chawla, I., and Mishra, A. K. (2020). A review of remote sensing\napplications in agriculture for food security: Crop growth and yield, irrigation, and crop\nlosses. J. Hydrol.586, 124905. doi: 10.1016/j.jhydrol.2020.124905\nKhanal, S., Kc, K., Fulton, J. P., Shearer, S., and Ozkan, E. (2020). Remote sensing in\nagriculture— accomplishments, limitations, and opportunities. Remote Sens. 12, 3783.\ndoi: 10.3390/rs12223783\nKirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., et al. (2023).\n“Segment anything, ” in Proceedings of the IEEE/CVF International Conference on\nComputer Vision. 4015 –4026. doi: 10.48550/arXiv.2304.02643\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2017). Imagenet classi ﬁcation with\ndeep convolutional neural networks. Communications of the ACM 60 (6), 84-90.\ndoi: 10.1145/3065386\nKung, T. H., Cheatham, M., Medenilla, A., Sillos, C., De Leon, L., Elepaño, C., et al.\n(2023). Performance of ChatGPT on USMLE: potential for AI-assisted medical\neducation using large language models. PloS Digital Health\n2, e0000198.\ndoi: 10.1371/journal.pdig.0000198\nLe Scao, T., Fan, A., Akiki, C., Pavlick, E., Ilic ́ , S., Hesslow, D., et al. (2023). Bloom: A\n176b-parameter open-access multilingual language model. doi: 10.48550/\narXiv.2211.05100\nLi, J., Chen, D., Qi, X., Li, Z., Huang, Y., Morris, D., et al. (2023a). Label-ef ﬁcient\nlearning in agriculture: A comprehensive review. Comput. Electron. Agric.215, 108412.\ndoi: 10.1016/j.compag.2023.108412\nZhu et al. 10.3389/fpls.2025.1579355\nFrontiers inPlant Science frontiersin.org23\nLi, J., Lammers, K., Yin, X., Yin, X., He, L., Sheng, J., et al. (2024). MetaFruit meets\nfoundation models: leveraging a comprehensive multi-fruit dataset for advancing\nagricultural foundation models. doi: 10.48550/arXiv.2407.04711\nLi, C., and Xing, W. (2021). Natural language generation using deep learning to\nsupport MOOC learners. Int. J. Artif. Intell. Educ.31, 186 –214. doi: 10.1007/s40593-\n020-00235-x\nLi, J., Xu, M., Xiang, L., Chen, D., Zhuang, W., Yin, X., et al. (2023b). Large language\nmodels and foundation models in smart agriculture: Basics, opportunities, and\nchallenges. doi: 10.48550/arXiv.2308.06668\nL i ,W . ,a n dZ h a o ,Y .( 2 0 1 5 ) .B i b l i o m e t r i ca n a l y s i so fg l o b a le n v i r o n m e n t a l\nassessment research in a 20-year period. Environ. Impact Assess. Rev. 50, 158 –166.\ndoi: 10.1016/j.eiar.2014.09.012\nLin, Y., Huang, Z., Liang, Y., Liu, Y., and Jiang, W. (2024). Ag-yolo: A rapid citrus\nfruit detection algorithm with global context fusion. Agriculture 14, 114. doi: 10.3390/\nagriculture14010114\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. (2023b). Visual instruction tuning. Adv. Neural\nInf. Process. Syst.36, 34892 –34916.\nLiu, B., Zhao, R., Liu, J., and Wang, Q. (2023a). AgriGPTs. GitHub. Available online\nat: https://github.com/AgriGPTS/AgriGPTs (Accessed April 15, 2025).\nLu, G., Li, S., Mai, G., Sun, J., Zhu, D., Chai, L., et al. (2023). Agi for agriculture.\ndoi: 10.48550/arXiv.2304.06136\nLu, Y., and Young, S. (2020). A survey of public datasets for computer vision tasks in\nprecision agriculture. Comput. Electron. Agric. 178, 105760. doi: 10.1016/\nj.compag.2020.105760\nMalla, A., Omwenga, M. M., and Bera, P. K. (2024). “Exploring image similarity\nthrough generative language models: A comparative study of GPT-4 with word\nembeddings and traditional approaches, ” in 2024 IEEE International Conference on\nElectro Information Technology (eIT). 275 –279. doi: 10.1109/eIT60633.2024.10609905\nM e d e i r o s ,A .D . D . ,S i l v a ,L .J . D . ,R i b e i r o ,J .P . O . ,F e r r e i r a ,K .C . ,R o s a s ,J .T . F . ,S a n t o s ,A .A . ,\net al. (2020). Machine learni n gf o rs e e dq u a l i t yc l a s s iﬁcation: An advanced approach using\nmerger data from FT-NIR spectroscopy and X-ray imaging.Sensors20, 4319. doi: 10.3390/\ns20154319\nMerine, R., and Purkayastha, S. (2022). “Risks and bene ﬁts of AI-generated text\nsummarization for expert level content in graduate health informatics, ” in 2022 IEEE\n10th International Conference on Healthcare Informatics (ICHI). 567–574. doi: 10.1109/\nICHI54592.2022.00113\nMeskó , B., and Topol, E. J. (2023). The imperative for regulatory oversight of large\nlanguage models (or generative AI) in healthcare. NPJ Digital Med.6, 120. doi: 10.1038/\ns41746-023-00873-0\nMikolov, T., Kara ﬁá t, M., Burget, L., Cernocky ́ , J., and Khudanpur, S. (2010).\nRecurrent neural network based language model. Interspeech 2, 1045 –1048.\ndoi: 10.21437/Interspeech.2010-343\nMostaco, G. M., De Souza, I. R. C., Campos, L. B., Cugnasca, C. E., et al. (2018).\n“AgronomoBot: a smart answering Chatbot applied to agricultural sensor networks, ” in\n14th international conference on precision agriculture\n, Vol. 24. 1 –13.\nNasir, I. M., Bibi, A., Shah, J. H., Khan, M. A., Sharif, M., Iqbal, K., et al. (2021). Deep\nlearning-based classi ﬁcation of fruit diseases: An application for precision agriculture.\nComput. Mater. Contin66, 1949 –1962. doi: 10.32604/cmc.2020.012945\nNiranjan, P. Y., Rajpurohit, V. S., and Malgi, R. (2019). “A survey on chat-bot system\nfor agriculture domain, ” in 2019 1st International Conference on Advances in\nInformation Technology (ICAIT).9 9 –103. doi: 10.1109/ICAIT47043.2019.8987429\nOmia, E., Bae, H., Park, E., Kim, M. S., Baek, I., Kabenge, I., et al. (2023). Remote\nsensing in ﬁeld crop monitoring: A comprehensive review of sensor systems, data\nanalyses and recent advances. Remote Sens. 15, 354. doi: 10.3390/rs15020354\nPaymode, A. S., and Malode, V. B. (2022). Transfer learning for multi-crop leaf\ndisease image classi ﬁcation using convolutional neural network VGG. Artif. Intell.\nAgric. 6, 23 –33. doi: 10.1016/j.aiia.2021.12.002\nPazhanivelan, S., Kumaraperumal, R., Shanmugapriya, P., Sudarmanian, N. S.,\nS i v a m u r u g a n ,A .P . ,S a t h e e s h ,S . ,e ta l .( 2 0 2 3 ) .Q u a n t i ﬁcation of biophysical\nparameters and economic yield in cotton and rice using drone technology.\nAgriculture 13, 1668. doi: 10.3390/agriculture13091668\nPeebles, W., and Xie, S. (2023). “Scalable diffusion models with transformers, ” in\nProceedings of the IEEE/CVF International Conference on Computer Vision. 4195–4205.\ndoi: 10.48550/arXiv.2212.09748\nPeng, R., Liu, K., Yang, P., Yuan, Z., and Li, S. (2023). Embedding-based retrieval\nwith llm for effective agriculture information extracting from unstructured data.\ndoi: 10.48550/arXiv.2308.03107\nQi, C. R., Su, H., Mo, K., and Guibas, L. J. (2017). “Pointnet: Deep learning on point\nsets for 3d classi ﬁcation and segmentation, ” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition. 652 –660. doi: 10.1109/CVPR.2017.16\nRadford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I. (2023).\n“Robust speech recognition via la rge-scale weak supervision, ” in International\nConference on Machine Learning. 28492 –28518.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., et al. (2020).\nExploring the limits of transfer learning with a uni ﬁed text-to-text transformer. J.\nMach. Learn. Res.21, 1 –67.\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. (2022). Hierarchical\ntext-conditional image generation with clip latents 1, 3. doi: 10.48550/arXiv.2204.06125\nRamesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., et al. (2021). “Zero-\nshot text-to-image generation, ” in International conference on machine learning. 8821–\n8831. doi: 10.48550/arXiv.2102.12092\nRedmon, J., Divvala, S., Girshick, R., and Farhadi, A. (2016). “You only look once:\nUniﬁed, real-time object detection, ” in Proceedings of the IEEE conference on computer\nvision and pattern recognition. 779 –788. doi: 10.1109/CVPR.2016.91\nRen, S., He, K., Girshick, R., and Sun, J. (2016). Faster R-CNN: Towards real-time\nobject detection with region proposal networks. IEEE Trans. Pattern Anal. Mach. Intell.\n39, 1137 –1149. doi: 10.1109/TPAMI.2016.2577031\nRen, Y., Hu, C., Tan, X., Qin, T., Zhao, S., Zhao, Z., et al. (2020). Fastspeech 2: Fast\nand high-quality end-to-end text to speech. doi: 10.48550/arXiv.2006.04558\nRen, Y., Ruan, Y., Tan, X., Qin, T., Zhao, S., Zhao, Z., et al. (2019). Fastspeech: Fast,\nrobust and controllable text to speech. Adv. Neural Inf. Process. Syst.32. doi: 10.48550/\narXiv.1905.09263\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022). “High-\nresolution image synthesis with latent diffusion models, ” in Proceedings of the IEEE/\nCVF conference on computer vis i o na n dp a t t e r nr e c o g n i t i o n.1 0 6 8 4 – 10695.\ndoi: 10.48550/arXiv.2112.10752\nRose Mary, C. A., Raji Sukumar, A., and Hemalatha, N. (2021). Text based smart\nanswering system in agriculture using RNN. agriRxiv 2021), 20210310498.\ndoi: 10.31220/agriRxiv.2021.00071\nR y a n ,M .( 2 0 2 3 ) .T h es o c i a la n de t h i c a li m p a c t so fa r t i ﬁcial intelligence in\nagriculture: mapping the agricultural AI literature. AI Soc. 38, 2473 –2485.\ndoi: 10.1007/s00146-021-01377-9\nSaleem, M. H., Potgieter, J., and Arif, K. M. (2021). Automation in agriculture by\nmachine and deep learning techniques: A review of recent developments. Precis. Agric.\n22, 2053 –2091. doi: 10.1007/s11119-021-09806-x\nSha, L., Rakovic, M., Whitelock-Wainwright, A., Carroll, D., Yew, V. M., Gasevic, D.,\net al. (2021). “Assessing algorithmic fairness in automatic classi ﬁers of educational\nforum posts, ” in Artiﬁcial Intelligence in Education: 22nd International Conference,\nAIED 2021, Utrecht, The Netherlands, June 14 –18, 2021, Proceedings, Part I 22. 381 –\n394. doi: 10.1007/978-3-030-78292-4_31\nShen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y. (2024). Hugginggpt:\nSolving ai tasks with chatgpt and its friends in hugging face. Adv. Neural Inf. Process.\nSyst. 36. doi: 10.48550/arXiv.2303.17580\nShi, B., Wu, Z., Mao, M., Wang, X., and Darrell, T. (2024). “When do we not need\nlarger vision models?, ” in European Conference on Computer Vision. 444 –462 (Cham:\nSpringer Nature Switzerland). doi: 10.1007/978-3-031-73242-3_25\nSimonyan, K., and Zisserman, A. (2014). Very deep convolutional networks for\nlarge-scale image recognition. doi: 10.48550/arXiv.1409.1556\nStella, F., Della Santina, C., and Hughes, J. (2023). How can LLMs transform the\nrobotic design process? Nat. Mach. Intell.5, 561–564. doi: 10.1038/s42256-023-00669-7\nSun, C., Shrivastava, A., Singh, S., and Gupta, A. (2017). “Revisiting unreasonable\neffectiveness of data in deep learning era, ” in Proceedings of the IEEE international\nconference on computer vision. 843 –852.\nSundermeyer, M., Schlüter, R., and Ney, H. (2012). Lstm neural networks for\nlanguage modeling. Interspeech 2012, 194 –197. doi: 10.21437/Interspeech.2012-65\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., et al. (2015). “Going\ndeeper with convolutions, ” in Proceedings of the IEEE conference on computer vision\nand pattern recognition.1 –9. doi: 10.1109/CVPR.2015.7298594\nTao, Y., and Zhou, J. (2017). Automatic apple recognition based on the fusion of\ncolor and 3D feature for robotic fruit picking. Comput. Electron. Agric.142, 388 –396.\ndoi: 10.1016/j.compag.2017.09.019\nTeam, G., Anil, R., Borgeaud, S., Alayrac, J. B., Yu, J., Soricut, R., et al. (2023).\nGemini: a family of highly capable multimodal models. doi: 10.48550/arXiv.2312.11805\nTeam, G., Kamath, A., Ferret, J., Pathak, S., Vieillard, N., Merhej, R., et al. (2025).\nGemma 3 technical report. doi: 10.48550/arXiv.2503.19786\nTeam, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., et al.\n(2024). Gemma: Open models based on gemini research and technology. doi: 10.48550/\narXiv.2403.08295\nThenmozhi, K., and Reddy, U. S. (2019). Crop pest classi ﬁcation based on deep\nconvolutional neural network and transfer learning. Comput. Electron. Agric. 164,\n104906. doi: 10.1016/j.compag.2019.104906\nTokekar, P., Vander Hook, J., Mulla, D., and Isler, V. (2016). Sensor planning for a\nsymbiotic UAV and UGV system for precision agriculture. IEEE Trans. Robotics32,\n1498–1511. doi: 10.1109/TRO.2016.2603528\nTripathy, P., Baylis, K., Wu, K., Watson, J., and Jiang, R. (2024). Investigating the\nsegment anything foundation model fo r mapping smallholder agriculture ﬁeld\nboundaries without training labels. doi: 10.48550/arXiv.2407.01846\nT z a c h o r ,A . ,D e v a r e ,M . ,K i n g ,B . ,A v i n ,S . ,a n dO ́ hÉ i g e a r t a i g h ,S .( 2 0 2 2 ) .\nResponsible arti ﬁcial intelligence in agriculture requires systemic understanding of\nrisks and externalities. Nat. Mach. Intell.4, 104 –109. doi: 10.1038/s42256-022-00440-4\nZhu et al. 10.3389/fpls.2025.1579355\nFrontiers inPlant Science frontiersin.org24\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al. (2017).\nAttention is all you need. Adv. Neural Inf. Process. Syst.30. doi: 10.48550/arXiv.1706.03762\nVeena, G., Kanjirangat, V., and Gupta, D. (2023). AGRONER: An unsupervised\nagriculture named entity recognition using weighted distributional semantic model.\nExpert Syst. Appl.229, 120440. doi: 10.1016/j.eswa.2023.120440\nV i s e n t i n ,F . ,C r e m a s c o ,S . ,S o z z i ,M . ,S i g n o r i n i ,L . ,S i g n o r i n i ,M . ,M a r i n e l l o ,F . ,e ta l .( 2 0 2 3 ) .\nA mixed-autonomous robotic platform for in tra-row and inter-row weed removal for\nprecision agriculture.Comput. Electron. Agric.214, 108270. doi: 10.1016/j.compag.2023.108270\nWang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., et al. (2023). “Internimage:\nExploring large-scale vision foundation models with deformable convolutions, ” in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n14408–14419. doi: 10.1109/CVPR52729.2023.01385\nWang, J., Ren, S., Zhang, Z., et al. (2019). Research progress on unmanned aerial\nvehicle for ecological remote sensing monitoring based on bibliometric assessment.\nTrop. Geogr.39 (4), 616 –624. doi: 10.13284/j.cnki.rddl.003157\nWei, T., Zhao, L., Zhang, L., Zhu, B., Wang, L., Yang, H., et al. (2023). Skywork: A\nmore open bilingual foundation model. doi: 10.48550/arXiv.2310.19341\nWeidinger, L., Mellor, J., Rauh, M., Grif ﬁn, C., Uesato, J., Huang, P. S., et al. (2021).\nEthical and social risks of harm from language models. doi: 10.48550/arXiv.2112.04359\nWiles, O., Gowal, S., Stimberg, F., Alvise-Rebuf ﬁ, S., Ktena, I., Dvijotham, K., et al.\n(2021). A ﬁne-grained analysis on distribution shift. doi: 10.48550/arXiv.2110.11328\nWu, J., Gan, W., Chen, Z., Wan, S., and Yu, P. S. (2023a). “Multimodal large language\nmodels: A survey, ” in 2023 IEEE International Conference on Big Data (BigData).\ndoi: 10.1109/BigData59044.2023.10386743\nWu, J., Hovakimyan, N., and Hobbs, J. (2023b). Genco: An auxiliary generator from\ncontrastive learning for enhanced few-shot learning in remote sensing. doi: 10.48550/\narXiv.2307.14612\nWu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann, S., et al. (2023c).\nBloomberggpt: A large language model for ﬁnance. doi: 10.48550/arXiv.2303.17564\nWu, C., Lin, W., Zhang, X., Zhang, Y., Xie, W., and Wang, Y. (2024). PMC-LLaMA:\ntoward building open-source language models for medicine. Journal of the American\nMedical Informatics Association31 (9), 1833 –1843. doi: 10.1093/jamia/ocae045\nXu, H., Man, Y., Yang, M., Wu, J., Zhang, Q., and Wang, J. (2023). Analytical insight\nof earth: a cloud-platform of intellige nt computing for geospatial big data.\ndoi: 10.48550/arXiv.2312.16385\nYan, L., Sha, L., Zhao, L., Li, Y., Martinez-Maldonado, R., Chen, G., et al. (2024).\nPractical and ethical challenges of large language models in education: A systematic\nscoping review. Br. J. Educ. Technol.55, 90 –112. doi: 10.1111/bjet.13370\nYang, X., Dai, H., Wu, Z., Bist, R., Subedi, S., Sun, J., et al. (2023c). Sam for poultry\nscience. doi: 10.48550/arXiv.2305.10254\nYang, Q., Du, X., Wang, Z., Meng, Z., Ma, Z., and Zhang, Q. (2023b). A review of\ncore agricultural robot technologies for crop productions. Comput. Electron. Agric.206,\n107701. doi: 10.1016/j.compag.2023.107701\nYang, J., Gao, M., Li, Z., Gao, S., Wang, F., and Zheng, F. (2023a). Track anything:\nSegment anything meets videos. doi: 10.48550/arXiv.2304.11968\nYang, G., Li, Y., He, Y., Zhou, Z., Ye, L., Fang, H., et al. (2024). Multimodal large\nlanguage model for wheat breeding: a new exploration of smart breeding.\ndoi: 10.48550/arXiv.2411.15203\nYao, S., Zhang, C., Ping, J., and Ying, Y. (2024). Recent advances in hydrogel\nmicroneedle-based bio ﬂuid extraction and detection in food and agriculture. Biosens.\nBioelectronics 250, 116066. doi: 10.1016/j.bios.2024.116066\nYe, J., Xu, H., Liu, H., Hu, A., Yan, M., Qian, Q., et al. (2024). mplug-owl3: Towards\nlong image-sequence understanding i n multi-modal large language models.\ndoi: 10.48550/arXiv.2408.04840\nYe, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., et al. (2023). mplug-owl:\nModularization empowers large language models with multimodality. doi: 10.48550/\narXiv.2304.14178\nYin, S., Fu, C., Zhao, S., Li, K., Sun, X., Xu, T., et al. (2023). A survey on multimodal\nlarge language models. doi: 10.48550/arXiv.2306.13549\nYuan, Y., Chen, L., Wu, H., and Li, L. (2022). Advanced agricultural disease image\nrecognition technologies: A review. Inf. Process. Agric. 9, 48 –59. doi: 10.1016/\nj.inpa.2021.01.003\nZhang, J., Huang, Y., Pu, R., Gonzalez-Moreno, P., Yuan, L., Wu, K., et al. (2019).\nMonitoring plant diseases and pests through remote sensing technology: A review.\nComput. Electron. Agric.165, 104943. doi: 10.1016/j.compag.2019.104943\nZhang, H., Li, X., and Bing, L. (2023a). Video-llama: An instruction-tuned audio-\nvisual language model for video understanding. doi: 10.48550/arXiv.2306.02858\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., et al. (2022b). Opt:\nOpen pre-trained transformer language models. doi: 10.48550/arXiv.2205.01068\nZhang, N., Wu, H., Zhu, H., Deng, Y., and Han, X. (2022a). Tomato disease\nclassiﬁcation and identi ﬁcation method based on multimodal fusion deep learning.\nAgriculture 12, 2014. doi: 10.3390/agriculture12122014\nZhao, B., Jin, W., Del Ser, J., and Yang, G. (2023a). ChatAgri: Exploring potentials of\nChatGPT on cross-linguistic agricultural text classi ﬁcation. Neurocomputing 557,\n126708. doi: 10.1016/j.neucom.2023.126708\nZhao, L., Zhang, L., Wu, Z., Chen, Y., Dai, H., Yu, X., et al. (2023b). When brain-\ninspired ai meets agi. Meta-Radiology 1, 100005. doi: 10.1016/j.metrad.2023.100005\nZhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., et al. (2023c). A survey of\nlarge language models. doi: 10.48550/arXiv.2303.18223\nZheng, C., and Li, H. (2023). The prediction of collective Economic development\nbased on the PSO-LSTM model in smart agriculture. PeerJ. Comput. Sci. 9, e1304.\ndoi: 10.7717/peerj-cs.1304\nZhong, J., Liu, Z., and Chen, X. (2023). Transformer-based models and hardware\nacceleration analysis in autonomous driving: A survey. doi: 10.48550/arXiv.2304.10891\nZhou, J., Li, J., Wang, C., Wu, H., Zhao, C., and Teng, G. (2021). Crop disease\nidentiﬁcation and interpretation method based on multimodal deep learning. Comput.\nElectron. Agric.189, 106408. doi: 10.1016/j.compag.2021.106408\nZhou, L., Pan, S., Wang, J., and Vasilakos, A. V. (2017). Machine learning on big data:\nOpportunities and challenges. Neurocomputing 237, 350 –361. doi: 10.1016/\nj.neucom.2017.01.026\nZhu et al. 10.3389/fpls.2025.1579355\nFrontiers inPlant Science frontiersin.org25"
}