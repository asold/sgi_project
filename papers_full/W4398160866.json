{
    "title": "Evaluating Large Language Models with RAG Capability: A Perspective from Robot Behavior Planning and Execution",
    "url": "https://openalex.org/W4398160866",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2737889287",
            "name": "Jin Yamanaka",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A1836274151",
            "name": "Takashi Kido",
            "affiliations": [
                "Teikyo University"
            ]
        },
        {
            "id": "https://openalex.org/A2737889287",
            "name": "Jin Yamanaka",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1836274151",
            "name": "Takashi Kido",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4365211632",
        "https://openalex.org/W4389665883",
        "https://openalex.org/W4360836968",
        "https://openalex.org/W4321392130",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4382142077",
        "https://openalex.org/W4390833744",
        "https://openalex.org/W4385262478",
        "https://openalex.org/W4362597819",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4390833079",
        "https://openalex.org/W2537589190",
        "https://openalex.org/W4385436553",
        "https://openalex.org/W2983916874",
        "https://openalex.org/W2794325560",
        "https://openalex.org/W4388964563",
        "https://openalex.org/W4380353763",
        "https://openalex.org/W4387432648",
        "https://openalex.org/W4401042689",
        "https://openalex.org/W4364384540"
    ],
    "abstract": "After the significant performance of Large Language Models (LLMs) was revealed, their capabilities were rapidly expanded with techniques such as Retrieval Augmented Generation (RAG). Given their broad applicability and fast development, it's crucial to consider their impact on social systems. On the other hand, assessing these advanced LLMs poses challenges due to their extensive capabilities and the complex nature of social systems. In this study, we pay attention to the similarity between LLMs in social systems and humanoid robots in open environments. We enumerate the essential components required for controlling humanoids in problem solving which help us explore the core capabilities of LLMs and assess the effects of any deficiencies within these components. This approach is justified because the effectiveness of humanoid systems has been thoroughly proven and acknowledged. To identify needed components for humanoids in problem-solving tasks, we create an extensive component framework for planning and controlling humanoid robots in an open environment. Then assess the impacts and risks of LLMs for each component, referencing the latest benchmarks to evaluate their current strengths and weaknesses. Following the assessment guided by our framework, we identified certain capabilities that LLMs lack and concerns in social systems.",
    "full_text": "Evaluating Large Language Models with RAG Capability:  \nA Perspective from Robot Behavior Planning and Execution \nJin Yamanaka1, Takashi Kido2 \n1 Fujitsu Research of America, US \n2 Teikyo University, Japan \njin.yamanaka@gmail.com, kido.takashi@gmail.com \n \n \nAbstract \nAfter the significant performance of Large Language Models \n(LLMs) was revealed, their capabilities were rapidly ex-\npanded with techniques such as Retrieval Augmented Gener-\nation (RAG). Given their broad applicability and fast devel-\nopment, it's crucial to consider their impact on social systems. \nOn the other hand, assessing these advanced LLMs poses \nchallenges due to their extensive capabilities and the complex \nnature of social systems. In this study, we pay attention to the \nsimilarity between LLMs in social systems and humanoid ro-\nbots in open environments. We enumerate the essential com-\nponents required for controlling humanoids in problem solv-\ning which help us explore the core capabilities of LLMs and \nassess the effects of any deficiencies within these compo-\nnents. This approach is justified because the effectiveness of \nhumanoid systems has been thoroughly proven and acknowl-\nedged. To identify needed components for humanoids in \nproblem-solving tasks, w e create an extensive component \nframework for planning and controlling huma noid robots in \nan open environment.  Then assess the impacts and risks of \nLLMs for each component, referencing the latest benchmarks \nto evaluate their current strengths and weaknesses. Following \nthe assessment guided by our framework, we identified cer-\ntain capabilities that LLMs la ck and concerns in social sys-\ntems. \nIntroduction \nIn the past few years, recent foundational models like \nOpenAI’s GPT-4 (Achiam et al. 2023), have shown excep-\ntional flexibility and utility. They offer a wide range of ap-\nplications across various sectors, serving as tools for learn-\ning, searching, and decision support, aiding from daily use \nto specialized areas such as math, law, medicine, human re-\nlationships, and finance. Numerous technologies, such as \nRAG (Lewis et al . 2020), are emerging alongside LLMs \nwith their development pace rapidly increasing. Even the \nscientists (Bubeck et al. 2023) have emphasized that LLMs \nare advancing towards the threshold of Artificial General In-\ntelligence (AGI). \nOn the other hand, substantial risks associated with LLMs \nhave been reported by Bowman, S. (2023), and these risks \nare becoming increasingly apparent. As these technologies \nprogress and become more woven into the fabric of our daily \nlives, it's critical to thoroughly evaluate their capabilities \nand risks. \nIn this study, we think about the case that the LLMs are \nnot only used for helping users to learn, research, work, but \nalso used to increase users’ well -being. It’s clear that the \nLLMs will be used for various purposes in the future. How-\never, recognizing the complexity of social systems and hu-\nman behavior, defining the precise capabilities needed and \npredicting outcomes for LLMs is complex. To gain about \nthese, we view LLMs as analogous to humanoid robots and \nsocial systems as their operating environment, e xamining \nhow to direct a humanoid robot in open environment. \nThe Rise of LLMs \nThe advancements in LLMs, enhanced by RAG, fine -tun-\ning, and prompt engineering (Gu et al. 2023), mark notable \nprogress in AI, especially in natural language processing \n(NLP) and natural language understanding (NLU) fields. \nChain of Thought (CoT) (Wei et al. 2022) style prompt en-\ngineering, designed for complex reasoning tasks, encour-\nages a model to articulate its thought process, proving par-\nticularly effective for multi -step problems. Additionally, \nOpenAGI (Fu et al. 2023) explores integrating LLMs with \ndomain-specific models and a Reinforcement Learning from \nTask Feedback (RLTF) mechanism to improve problem -\nsolving capabilities further, showcasing a promising direc-\ntion for AI development. \nWhile prompt engineering and fine-tuning are used to im-\nprove LLMs’ answering capabilities, RAG represents a sig-\nnificant advancement in the field of LLMs. It can easily en-\nhance knowledge access or personalization without model \ntraining, compose other domain models, validate the result, \ncustomize the workflow/results, and learn interactively. In \nthis paper, we regard RAG as a part of LLMs. \nCopyright © 2024, Association for the Advancement of Artificial \nIntelligence (www.aaai.org). All rights reserved. \nAAAI Spring Symposium Series (SSS-24)\n452\nBenchmark Results on LLMs \nThese improvements have led to reports of LLMs demon-\nstrating exceptional performance in perception, cognition, \nand task execution across various benchmarks, while also \npointing out limitations in complex reasoning, domain -spe-\ncific knowledge, and consistency \n \n• Multimodal Large Language Model (MLLM) Evalu-\nation benchmark (MME) (Fu et al. 2023) is a compre-\nhensive benchmark for evaluating MLLMs. It focuses on \nassessing both perception and cognition abilities across \n14 subtasks, aiming to address the limitations of current \nevaluation methods. \n• AGIEval (Zhong et al. 2023) is a human -centric bench-\nmark for evaluating foundation models like GPT -4 on \ntasks akin to human cognition and problem -solving. \nAGIEval assesses these models using exams such as SAT, \nLSAT, and math competitions. The findings show GPT -\n4 outperforming average human scores in several areas \nbut also reveal its limitations in complex reasoning and \ndomain-specific knowledge. \n• Intelligent Agent system  (Boiko, D.; Robert M .; and  \nGabe, G. 2023) that leverages multiple large language \nmodels (LLMs) for autonomous scientific research, in-\ncluding designing, planning, and executing experiments. \nIt highlights the Agent's capabilities through examples, \nnotably in performing catalyzed cross-coupling reactions. \n• AI2 Reasoning Challenge  (ARC) (Clark et al . 2018), a \nnew benchmark for advanced question answering, is de-\nsigned to push AI research by presenting questions that \nrequire deeper knowledge and reasoning. It features a \nchallenging set of questions that current LLMs struggle \nwith. \n• MT-Bench and Chatbot Arena  (Zheng et al . 2023) re-\nveal that LLMs can closely match human judgment in \nevaluating chatbots suggesting that LLMs can be a scala-\nble and explainable method for approximating human \npreferences. On the other hand, the study explores biases \nand limitations within LLMs. \n• MLAgentBench (Huang et al.  2023) evaluates AI agents, \nparticularly GPT-4-based ones, on ML research tasks like \nmodel development and editing. It tests their autonomy in \nconducting experiments, showing both their strengths and \nareas needing improvement, such as consistency and ac-\ncuracy in outputs. \n• GAIA (Mialon et al. 2023) is a benchmark aimed at eval-\nuating General AI Assistants. It presents unique, real -\nworld questions requiring fundamental abilities. GAIA is \nhighlighting the gap between current AI capabilities and \nhuman performance for multi -step reasoning and multi -\nmodality handling. \n \nKnown Risks on LLMs \nAttacks targeting LLMs leverage weaknesses in their com-\nprehension, data quality, or processing protocols, challeng-\ning their dependability and safety. Main attacks are Adver-\nsarial Attacks, where inputs are crafted to mislead models \ninto making errors; Datas et Pollution, where the training \ndata is tampered with to degrade performance or introduce \nbiases; and Prompt Injection, which involves manipulating \nthe model's output by injecting malicious prompts or com-\nmands. For example, Sleeper Agents (Hubinger et al. 2024) \nexplores the concept of training LLMs to exhibit deceptive \nbehavior that persists even after undergoing safety training. \nThose are the big security risks of LLMs. We don’t discuss \nthese security risks in this paper, however, we should be \naware of them. \nOn the other hand, Bowman, S. (2023) discussed the risks \nassociated with LLMs including their unpredictability, the \npotential for misuse, and the challenges in aligning their out-\nputs with human values and ethics. It emphasizes the im-\nportance of addressing these risks to ensure the responsibl e \ndevelopment and deployment of LLMs in society. \nHumanoids in an Open Environment \nBuilding on prior research, LLMs exhibit remarkable abili-\nties yet lack complex reasoning, multi -step planning, learn-\ning, and consistency. There are also concerns about their \noutputs not aligning with human values and ethics. Then \nhow can we assess the impl ication on social systems as a \nnext step? We suggest viewing LLMs in social systems as \nanalogous to humanoid robots in an open environment to \nspecifically assess it. \nRefer to the system components of humanoid robots will \nbe well justified because humanoids are considered as some-\nthing akin to person. E.g. humanoids should have proper \npersonality and behave politely to have better social interac-\ntion, which is same for th e LLMs in social system. The \nLLMs will behave nicely otherwise the users won’t trust \nthem. \nThis approach will be effective because the requested ca-\npabilities for humanoid and LLMs are similar like below and \nthe humanoids are already tested and proven in open envi-\nronment. 1. Develop intricate strategies towards achieving \nobjectives, 2. Perceive with multi-modal data and anticipate \nchanges in dynamic environments, 3. Exhibit learning and \nadaptability in response to environmental variations and \nchanges, and 4. Comprehend human emotions and mimic \nhuman behaviors to improve interactions with users. Wh en \nLLMs serve users in varied tasks at social systems with aim-\ning not to breach social ethics, the requirements are very \nsimilar with the one for humanoids serving in open environ-\nment. \n453\nSoftware Components on Humanoids \nHumanoid robot software components have significantly \nadvanced in recent years. These advancements include im-\nprovements in artificial intelligence algorithms, machine \nlearning techniques for better decision -making and adapta-\nbility. \n \n \nGupta, P; Vineet, T; and  Srivastava, R. (2006) discusses \nthe components needed for controlling humanoid robots, fo-\ncusing on sensing, actuating, planning, and controlling in an \nopen environment. It highlights the complexity of simulat-\ning human structure and behavior in autonomous systems, \nmaking humano id robots more sophisticated. Wang et al . \n(2023) also proposed NaviSTAR, a benchmark for socially \naware robot navigation. It aims to improve understanding of \ncrowd interactions and align robot behavior with human ex-\npectations and social norms. \nDrawing from these studies, we introduce components \nmap for humanoids control in open environments, as de-\npicted in Figure 1. In the upcoming chapters, we will explore \nthe implications of LLMs in detail, alongside discussing the \ncomponent model we proposed. \nLLM’s Capabilities and Risks on Planning \nCommunication and Perception \nTo receive the task from the user, the robots should sense \nthe environment to identify the target user. Sensors and per-\nception models should have enough multi -model under-\nstanding capabilities to recognize the user and receive tasks \nfrom audio, visual, and text. Once the humanoid targets the \nuser, it should keep hearing the commands from the user and \nprioritize it from the other users. LLMs demonstrate signif-\nicant multi-modal understanding (Fu et al. 2023) and con-\nversational skills, enabling them to clarify and confirm us-\ners' requests in a Chain of Thought manner. However, the \nrisk here is that the LLMs can pretend as if the LLM truly \nunderstands the user’s request whether LLM isn’t under-\nstanding or working for it (Hubinger et al. 2024). We should \nhave a system that truly shows and controls the aim of \nLLMs. Otherwise, the user will be misled to the wrong goal. \nThere is another risk that current perception models have \nlimited performance (Ge et al . 2023), but LLMs don’t un-\nderstand it. It will result in harming the environment or the \nuser itself. \nIn some cases, humanoids need to work with other hu-\nmanoids or other humans to achieve the tasks e.g. carrying \nheavy luggage together or letting others get things out of the \nway. LLMs have enough general communication skills via \nnatural language to cooperat e with other humans/robots. \nHowever, as it is shown it’s weak to Prompt Injection or \nmalicious attacks, communicating with an unknown target \nwill be a huge risk. \nPlanning \nTo solve the requested task, humanoids should 1. break it \ndown into necessary steps, 2. understand what are the obsta-\ncles that the humanoid should avoid, 3. generate a temporal \nmovement plan, 4. use simulators or other components to \ncheck if the plan has some risks. If there are some uncertain-\nties like the floor map is not yet obtained, the planner should \n5. insert other tasks to explore to get enough knowledge \naround it. \nFigure 1: Proposed Software Components for Humanoid Behavior Planning and Execution \n454\nLLMs show it has a basic capability for planning (Achiam \net al.  2023), and breaking down the request into multiple \nsteps (Wei et al. 2022), and also RAG supplements LLMs’ \nracking memory ability so that it can search external long -\nterm memory and knowledge. However, many benchmark \nresults show that LLMs don’t have strong reasoning/plan-\nning capabilities for multi-step tasks. This could easily lead \nto confusion, inefficiency, or incomplete solutions. Like \nteaching/controlling humanoids when they get stacked, w e \ncan use Reinforcement Learning from Human Feedback \n(RLHF) (Christiano et al. 2017) to provide high-quality pos-\nitive samples for the system to learn. This is effective for the \nsystem which is hard to be modeling such as social systems, \non the other hand,  Casper et al. (2023) identify a weakness \nof RLHF as it is difficult to have consistency and ensure the \nreliability of model behaviors under various conditions. \nLLM’s Capabilities and Risks of Execution \nExecuting complex plans with humanoids is very challeng-\ning due to their limited ability to understand real -world dy-\nnamics and causal relationships. It will struggle with adapt-\ning execution plans to account for unforeseen variables, en-\nvironment change, or complex interactions in practical sce-\nnarios. There are many constraints and it should dynami-\ncally change the priority of sub -tasks not to harm the envi-\nronment or humanoid itself. It will be much relieved when \nit runs on a digital system compared to running on a physical \nsystem, however, it will be similar in many points like there \nwill be unforeseen situations and it should dynamically \navoid harming users and society. \nForward Model and Inverse Model \nIn humanoid control, forward and inverse kinematics mod-\nels are essential for controlling humanoids. Forward kine-\nmatics involves calculating the position and orientation of \nthe robot's end effector (e.g., hand) from the joint parameters. \nInverse kinematics, on the other hand, involves determining \nthe necessary joint parameters to achieve the desired posi-\ntion and orientation of the end effector. So the inverse model \ncalculates the bodily motion when it receives a request from \nthe brain, then it uses a forward  model to predict how the \nbody will move with the request. Humanoids use this for-\nward model and inverse model alternatively to act precisely \nlike a human. E.g. OpenHRP (Kanehiro, F.; Hirohisa, H.; \nand Shuuji, K. 2004) proposed and provided an open archi-\ntecture platform designed for sharing software/data between \nthe dynamics simulator and robot controllers, including pa-\nrameter parsing, kinematics, dynamics computations, and \ncollision detection. \nWhen we apply it in a problem -solving case with LLMs, \nit would be difficult to have forward and inverse models in-\nside of LLMs. The benchmarks from Bubeck et al. (2023) \nshow that LLM fails in a basic math problem which means \nLLM doesn’t have even simple mathematical models. It \nmakes sense because LLMs are just LLMs, so it needs to \nhave RAG and work closely with other components and \nmodels but there are many limitations so far. \nLearning Ability and Adaptability \nIt will be challenging to implement learning and adapting \ncapabilities in humanoids. NaviStar proposed a spatial and \ntemporal graph transformer network to estimate potential \ncooperation and collision avoidance, considering the multi-\nmodality and uncertainty  of pedestrians’ movements. It \nshows that humanoids can have some adaptability, but it’s \nlimited in a specific situation and target. \nIn the LLMs’ case, there are many explorations started about \nhow to selectively store the experience and organize it as \nknowledge. LLMs can save and accumulate experiences us-\ning RAG and have a robust capability for learning from data \nby training or fine -tuning. However, as it’s difficult in hu-\nmanoid control cases, there will be various unforeseen envi-\nronmental changes and reactions in an open world. When \nthey try to learn only from their experience without manual \ninterventions, they will easily be affected by unknown bi-\nases and might fail to adapt to ethical norms or societal \nchanges. \nTask Execution and Management \nIn previous discussions, we noted that humanoid's task ex-\necutors must collaborate with various components and man-\nage many constraints like avoiding damage to environment \nobjects from unexpected movements, managing limited bat-\ntery life and adherence to laws and ethics. This necessitates \nprecise communication with other models or a system that \nallows interruption by other models due to unforeseen envi-\nronmental changes.  \nSimilarly, in real -world applications, LLMs must effi-\nciently manage unexpected changes to remain relevant and \naccurate, highlighting the need for transparent task priority \nmanagement to obtain dynamic adaptability in complex en-\nvironments. Patchscopes (Ghandeharioun et al. 20024) in-\ntroduced a unifying framework designed to inspect the hid-\nden representations of LLMs. It aims to articulate the infor-\nmation encoded within LLMs, enhancing our understanding \nof models' behaviors. It addresses the limitations of mod el \nexplanation and self-correction in complex reasoning tasks. \nConclusion \nOur analysis of LLMs, enhanced by RAG and other technol-\nogies, indicates that they excel in various abilities below. It's \nalso important to recognize that we need to craft the right \nprompt. It is crucial to achieve specific outcomes with \nLLMs. \n455\n• Knowledge of the wide areas such as common sense, sci-\nence, technology, economics, and history \n• Wide range of multi-modal perception and cognition with \nother perception models \n• Following or mimicking human behavior and ethics \n• Basic task planning, reasoning, and selection \n• Searching and learning from the data if it’s properly given \n• Composing other models or components to achieve single \nstep goals \n• Task execution and result evaluation \n \nOn the other hand, LLMs won’t have enough responsibili-\nties in these areas. \n• Provide confidence and consistency \n• Complex reasoning \n• Multi-step planning \n• Domain-specific knowledge or system \n• Auto regression or auto adaptabilities \n• Respond with unforeseen or unexpected things \n• Be strict not to break laws, ethics, or the environment \n \nSubsequently, we developed a component structure graph \nfor humanoids to simulate the potential impacts of these \nLLMs’ capabilities and risks in social systems. A key con-\ncern is that the LLMs behave as if they are confident even if \nthe response is inaccurat e or insufficient. In addition, they \ndon’t have consistency in many situations and it will confuse \nthe users.  \nThe next concern is the need for LLMs to integrate with \nother systems for complex tasks, but they will struggle with \nintricate reasoning and planning with them. To safely serve \nwith complex situations, we guess LLMs need to ask users \nstep by step to confirm their executions. In this case, the sys-\ntem will be very slow and ineffective instead. \nThe final concern involves the auto -regression feature, \nwhich aims for the system to learn from each interaction. \nUnfortunately, this aspect is not yet robust or thoroughly \ntested, leaving it prone to biases and potentially inadequate \nresponses. \nGiven these discussions, it's clear LLMs present certain \nrisks in open social systems and should, for now, be de-\nployed in limited situations even though with the case just \nfor using LLMs to maintain user’s social well-being. Ideally, \nfuture LLMs should feature a self-regulating mechanism, al-\nlowing them to autonomously adjust and be accountable for \nachieving their intended goals, enhancing their reliability \nand effectiveness in real-world social systems. \nReferences \n \nAchiam et al. 2023. GPT-4 technical report. arXiv preprint. \narXiv:2303.08774. \nBoiko, D.; Robert, M.; and Gabe, G. 2023. Emergent auton-\nomous scientific research capabilities of large language \nmodels. arXiv preprint. arXiv:2304.05332. \nBowman, S.  2023. Eight things to know about large lan-\nguage models. arXiv preprint. arXiv:2304.00612. \nBubeck et al. 2023. Sparks of artificial general intelligence: \nEarly experiments with GPT-4. arXiv preprint . \narXiv:2303.12712. \nCasper et al. 2023. Open problems and fundamental limita-\ntions of reinforcement learning from human feedback. arXiv \npreprint. arXiv:2307.15217. \nChristiano et al.  2017. Deep reinforcement learning from \nhuman preferences. In Advances in Neural Information Pro-\ncessing systems 30 (2017). \nClark et al. 2018. Think you have solved question answer-\ning? try arc, the ai2 reasoning challenge. arXiv preprint . \narXiv:1803.05457. \nFu et al. 2023. MME: A comprehensive evaluation bench-\nmark for multimodal large language models. arXiv preprint. \narXiv:2306.13394. \nGe et al . 2023 . Open AGI: When LLM meets domain ex-\nperts. arXiv preprint. arXiv:2304.04370. \nGhandeharioun et al. 2024. Patchscope: A Unifying Frame-\nwork for Inspecting Hidden Representations of Language \nModels. arXiv preprint. arXiv:2401.06102. \nGu et al. 2023. A systematic survey of prompt engineering \non vision -language foundation models . arXiv preprint . \narXiv:2307.12980. \nGupta, P; Vineet, T; and Srivastava, R. 2006. Futuristic hu-\nmanoid robots: An overview. In First International Confer-\nence on Industrial and Information Systems. IEEE. \nHuang et al. 2023. Benchmarking large language models as \nAI research agents. arXiv preprint. arXiv:2310.03302. \nHubinger et al. 2024. Sleeper Agents: Training Deceptive \nLLMs that Persist Through Safety Training. arXiv preprint . \narXiv:2401.05566. \nKanehiro, F.; Hirohisa, H.; and Shuuji, K. 2004. OpenHRP: \nOpen architecture humanoid robotics platform. In The Inter-\nnational Journal of Robotics Research 23.2 (2004): 155-165. \nLewis et al. 2020. Retrieval-augmented generation for \nknowledge-intensive NLP tasks. Inn Advances in Neural In-\nformation Processing Systems 33 (2020): 9459-9474. \nMialon et al. 2023. GAIA: a benchmark for General AI As-\nsistants. arXiv preprint. arXiv:2311.12983. \nWang et al. 2023. NaviSTAR: Socially Aware Robot Navi-\ngation with Hybrid Spatio -Temporal Graph Transformer \nand Preference Learning. arXiv preprint. arXiv:2304.05979. \nWei et al. 2022. Chain-of-thought prompting elicits reason-\ning in large language models. In Advances in Neural Infor-\nmation Processing Systems 35 (2022): 24824-24837. \nZheng et al. 2023. Judging LLM-as-a-judge with MT-Bench \nand Chatbot Arena. arXiv preprint. arXiv:2306.05685. \nZhong et al. 2023. AGIEval: A human -centric benchmark \nfor evaluating foundation models.\" arXiv preprint . \narXiv:2304.06364. \n \n456"
}