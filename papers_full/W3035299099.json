{
  "title": "Multimodal and Multiresolution Speech Recognition with Transformers",
  "url": "https://openalex.org/W3035299099",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A1925760517",
      "name": "Georgios Paraskevopoulos",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2168901782",
      "name": "Srinivas Parthasarathy",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2203685297",
      "name": "Aparna Khare",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2250212915",
      "name": "Shiva Sundaram",
      "affiliations": [
        "Amazon (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2972389417",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2972892814",
    "https://openalex.org/W2143612262",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2889624961",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W2963303028",
    "https://openalex.org/W2962929176",
    "https://openalex.org/W2964182350",
    "https://openalex.org/W3007328579",
    "https://openalex.org/W2962934715",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2897067191",
    "https://openalex.org/W854541894",
    "https://openalex.org/W2902348614",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W98035269",
    "https://openalex.org/W2962778134",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W2884975363",
    "https://openalex.org/W2941814890",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2899274165",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W1503933356",
    "https://openalex.org/W2972451902",
    "https://openalex.org/W2884254529",
    "https://openalex.org/W2890197052",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W2890952074",
    "https://openalex.org/W2981165461",
    "https://openalex.org/W2530876040"
  ],
  "abstract": "This paper presents an audio visual automatic speech recognition (AV-ASR) system using a Transformer-based architecture. We particularly focus on the scene context provided by the visual information, to ground the ASR. We extract representations for audio features in the encoder layers of the transformer and fuse video features using an additional crossmodal multihead attention layer. Additionally, we incorporate a multitask training criterion for multiresolution ASR, where we train the model to generate both character and subword level transcriptions. Experimental results on the How2 dataset, indicate that multiresolution training can speed up convergence by around 50% and relatively improves word error rate (WER) performance by upto 18% over subword prediction models. Further, incorporating visual information improves performance with relative gains upto 3.76% over audio only models. Our results are comparable to state-of-the-art Listen, Attend and Spell-based architectures.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2381–2387\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n2381\nMultiresolution and Multimodal Speech Recognition with Transformers\nGeorgios Paraskevopoulos Srinivas Parthasarathy Aparna Khare Shiva Sundaram\nAmazon Lab126\ngeopar@central.ntua.gr, {parsrini,apkhare,sssundar}@amazon.com\nAbstract\nThis paper presents an audio visual automatic\nspeech recognition (A V-ASR) system using a\nTransformer-based architecture. We particu-\nlarly focus on the scene context provided by\nthe visual information, to ground the ASR. We\nextract representations for audio features in\nthe encoder layers of the transformer and fuse\nvideo features using an additional crossmodal\nmultihead attention layer. Additionally, we in-\ncorporate a multitask training criterion for mul-\ntiresolution ASR, where we train the model to\ngenerate both character and subword level tran-\nscriptions. Experimental results on the How2\ndataset, indicate that multiresolution training\ncan speed up convergence by around 50% and\nrelatively improves word error rate (WER) per-\nformance by upto 18% over subword predic-\ntion models. Further, incorporating visual in-\nformation improves performance with relative\ngains upto 3.76% over audio only models. Our\nresults are comparable to state-of-the-art Lis-\nten, Attend and Spell-based architectures.\n1 Introduction\nAutomatic speech recognition is a fundamental\ntechnology used on a daily basis by millions of\nend-users and businesses. Applications include au-\ntomated phone systems, video captioning and voice\nassistants providing an intuitive and seemless in-\nterface between users and end systems. Current\nASR approaches rely solely on utilizing audio in-\nput to produce transcriptions. However, the wide\navailability of cameras in smartphones and home\ndevices acts as motivation to build A V-ASR models\nthat rely on and beneﬁt from multimodal input.\nTraditional A V-ASR systems focus on tracking\nthe user’s facial movements and performing lipread-\ning to augment the auditory inputs (Potamianos\net al., 1997; Mroueh et al., 2015; Tao and Busso,\n2018). The applicability of such models in real\nworld environments is limited, due to the need for\naccurate audio-video alignment and careful camera\nplacement. Instead, we focus on using video to\ncontextualize the auditory input and perform multi-\nmodal grounding. For example, a basketball court\nis more likely to include the term “lay-up” whereas\nan ofﬁce place is more likely include the term “lay-\noff”. This approach can boost ASR performance,\nwhile the requirements for video input are kept\nrelaxed (Caglayan et al., 2019; Hsu et al., 2019).\nAdditionally we consider a multiresolution loss\nthat takes into account transcriptions at the charac-\nter and subword level. We show that this scheme\nregularizes our model showing signiﬁcant improve-\nments over subword models. Multitask learning on\nmultiple levels has been previously explored in the\nliterature, mainly in the context of CTC (Sanabria\nand Metze, 2018; Krishna et al., 2018; Ueno et al.,\n2018). A mix of seq2seq and CTC approaches\ncombine word and character level (Kremer et al.,\n2018; Ueno et al., 2018) or utilize explicit phonetic\ninformation (Toshniwal et al., 2017; Sanabria and\nMetze, 2018).\nModern ASR systems rely on end-to-end, align-\nment free neural architectures, i.e. CTC (Graves\net al., 2006) or sequence to sequence models\n(Graves et al., 2013; Zhang et al., 2017). The use of\nattention mechanisms signiﬁcantly improve results\nin (Chorowski et al., 2015) and (Chan et al., 2016).\nRecently, the success of transformer architectures\nfor NLP tasks (Vaswani et al., 2017; Devlin et al.,\n2019; Dai et al., 2019) has motivated speech re-\nsearchers to investigate their efﬁcacy in end-to-end\nASR (Karita et al., 2019b). Zhou et. al., apply\nan end-to-end transformer architecture for Man-\ndarin Chinese ASR (Zhou et al., 2018). Speech-\nTransformer extends the scaled dot-product atten-\ntion mechanism to 2D and achieves competitive\nresults for character level recognition (Dong et al.,\n2018; Karita et al., 2019a). Pham et. al. introduce\nthe idea of stochastically deactivating layers dur-\n2382\ning training to achieve a very deep model (Pham\net al., 2019). A major challenge of the transformer\narchitecture is the quadratic memory complexity\nas a function of the input sequence length. Most\narchitectures employ consecutive feature stacking\n(Pham et al., 2019) or CNN preprocessing (Dong\net al., 2018; Karita et al., 2019b) to downsample\ninput feature vectors. Mohamed et al. (2019) use\na VGG-based input network to downsample the\ninput sequence and achieve learnable positional\nembeddings.\nMultimodal grounding for ASR systems has\nbeen explored in (Caglayan et al., 2019), where\na pretrained RNN-based ASR model is ﬁnetuned\nwith visual information through Visual Adaptive\nTraining. Sterpu et al. (2018) propose a seq2seq\nmodel based on RNNs for lip-reading that performs\ncross-modal alignment of face tracking and audio\nfeatures through an attention mechanism. Further-\nmore, Hsu et al. (2019) use a weakly supervised\nsemantic alignment criterion to improve ASR re-\nsults when visual information is present. Multi-\nmodal extensions of the transformer architecture\nhave also been explored. These extensions mainly\nfuse visual and language modalities in the ﬁelds\nof Multimodal Translation and Image Captioning.\nMost approaches focus on using the scaled dot-\nproduct attention layer for multimodal fusion and\ncross-modal mapping. Afouras et al. (2018) present\na transformer model for A V-ASR targeted for lip-\nreading in the wild tasks. It uses a self attention\nblock to encode the audio and visual dimension\nindependently. A decoder individually attends to\nthe audio and video modalities producing character\ntranscriptions. In comparison our study uses the\nvideo features to provide contextual information\nto our ASR. Libovick `y et al. (2018) employ two\nencoder networks for the textual and visual modali-\nties and propose four methods of using the decoder\nattention layer for multimodal fusion, with hier-\narchical fusion yielding the best results. Yu et al.\n(2019) propose an encoder variant to fuse deep,\nmulti-view image features and use them to produce\nimage captions in the decoder. Le et al. (2019) use\ncascaded multimodal attention layers to fuse visual\ninformation and dialog history for a multimodal\ndialogue system. Tsai et al. (2019) present Mul-\ntimodal Transformers, relying on a deep pairwise\ncascade of cross-modal attention mechanisms to\nmap between modalities for multimodal sentiment\nanalysis.\nIn relation to the previous studies, the main con-\ntributions of this study are a) a fusion mechanism\nfor audio and visual modalities based on the cross-\nmodal scaled-dot product attention, b) an end to\nend training procedure for multimodal grounding\nin ASR and c) the use of a multiresolution training\nscheme for character and subword level recognition\nin a seq2seq setting without relying on explicit pho-\nnetic information. We evaluate our system in the\n300 hour subset of the How2 database (Sanabria\net al., 2018), achieving relative gains up to 3.76%\nwith the addition of visual information. Further we\nshow relative gains of 18% with the multiresolution\nloss. Our results are comparable to state-of-the-art\nASR performance on this database.\n2 Proposed Method\nOur transformer architecture uses two transformer\nencoders to individually process acoustic and vi-\nsual information (Fig. 1). Audio frames are fed to\nthe ﬁrst set of encoder layers. We denote the space\nof the encoded audio features as the audio space\nA. Similarly, video features are projected to the\nvideo space V using the second encoder network.\nFeatures from audio and visual space are passed\nthrough a tied feed forward layer that projects them\ninto a common space before passing them to their\nindividual encoder layers respectively. This tied\nembedding layer is important for fusion as it helps\nalign the semantic audio and video spaces. We then\nuse a cross-modal attention layer that maps pro-\njected video representations to the projected audio\nspace (Section 2.1). The outputs of this layer are\nadded to the original audio features using a learn-\nable parameter αto weigh their contributions. The\nfused features are then fed into the decoder stack\nfollowed by dense layers to generate character and\nsubword outputs. For multiresolution predictions\n(Section 2.2), we use a common decoder for both\ncharacter and subword level predictions, followed\nby a dense output layer for each prediction. This\nreduces the model parameters and enhances the\nregularization effect of multitask learning.\n2.1 Cross-modal Attention\nScaled dot-product attention operates by construct-\ning three matrices, K, V and Qfrom sequences\nof inputs. K and V may be considered keys and\nvalues in a “soft” dictionary, whileQis a query that\ncontextualizes the attention weights. The attention\nmechanism is described in Eq. 1, where σdenotes\n2383\nDot product attention\nEncoder Layer\nEncoder Layer…\nEncoder Layer\nN x\nDecoder Layer\nDecoder Layer…\nDecoder Layer\nM x\nAudio frames\nSubword\nprediction\nVideo frames\nKVQ\nVideo Encoder\nDown-sampling\n!\n\" !\n Character \nprediction\n#\n1−#\nTied Dense LayerFusion\nSubword\nTranscription\nCharacter \nTranscription\nTied Dense Layer\nFigure 1: Overall system architecture. A cross-modal scaled dot-product attention layer is used to project the visual\ndata into the audio feature space followed by an additive fusion.\nthe softmax operation.\nY = σ(KQT )V (1)\nThe case where K, V and Q are constructed\nusing the same input sequence consists a self-\nattention mechanism. We are interested in cross-\nmodal attention, where K and V are constructed\nusing inputs from one modality M1, video in our\ncase (Fig. 1) and Q using another modality M2,\naudio. This conﬁguration as an effective way to\nmap features from M1 to M2 (Tsai et al., 2019).\nNote, that such a conﬁguration is used in the de-\ncoder layer of the original transformer architecture\n(Vaswani et al., 2017) where targets are attended\nbased on the encoder outputs.\n2.2 Multiresolution training\nWe propose the use of a multitask training scheme\nwhere the model predicts both character and sub-\nword level transcriptions. We jointly optimize the\nmodel using the weighted sum of character and\nsubword level loss, as in Eq. 2:\nL= γ∗Lsubword + (1−γ) ∗Lcharacter (2)\nwhere γis a hyperparameter that controls the im-\nportance of each task.\nThe intuition for this stems from the reasoning\nthat character and subword level models perform\ndifferent kinds of mistakes. For character predic-\ntion, the model tends to predict words that sound\nphonetically similar to the ground truths, but are\nsyntactically disjoint with the rest of the sentence.\nSubword prediction, yields more syntactically cor-\nrect results, but rare words tend to be broken down\nto more common words that sound similar but are\nsemantically irrelevant. For example, character\nlevel prediction may turn “old-fashioned” into “old-\nfashioning”, while subword level turns the sentence\n“ukuleles are different” to “you go release are differ-\nent”. When combining the losses, subword predic-\ntion, which shows superior performance is kept as\nthe preliminary output, while the character predic-\ntion is used as an auxiliary task for regularization.\n3 Experimental Setup\nWe conduct our experiments on the How2 instruc-\ntional videos database (Sanabria et al., 2018). The\ndataset consists of300 hours of instructional videos\nfrom the YouTube platform. These videos depict\npeople showcasing particular skills and have high\nvariation in video/audio quality, camera angles and\nduration. The transcriptions are mined from the\nYouTube subtitles, which contain a mix of automat-\nically generated and human annotated transcrip-\ntions. Audio is encoded using 40 mel-ﬁlterbank\ncoefﬁcients and 3 pitch features with a frame size\n2384\nInput handling Recognition level WER\nFiltering Character 33.0\nFiltering Subword 29.7\nChunking Character 31.3\nChunking Subword 29.9\nStacking Character 28.3\nStacking Subword 26.1\nStacking MR 21.3\nTable 1: Results for different methods of input ﬁlter-\ning for different prediction resolutions. MR stands for\nmultiresolution.\nof 10 ms, yielding 43-dimensional feature vec-\ntors. The ﬁnal samples are segments of the original\nvideos, obtained using word-level alignment. We\nfollow the video representation of the original pa-\nper (Caglayan et al., 2019), where a 3D ResNeXt-\n101 architecture, pretrained on action recognition,\nis used to extract2048D features (Hara et al., 2018).\nVideo features are average pooled over the video\nframes yielding a single feature vector. For our ex-\nperiments, we use the train, development and test\nsplits proposed by (Sanabria et al., 2018), which\nhave sizes 298.2 hours, 3.2 hours and 3.7 hours\nrespectively.\nOur model consists of 6 encoder layers and 4\ndecoder layers. We use transformer dimension 480,\nintermediate ReLU layer size1920 and 0.2 dropout.\nAll attention layers have 6 attention heads. The\nmodel is trained using Adam optimizer with learn-\ning rate 10−3 and 8000 warmup steps. We employ\nlabel smoothing of 0.1. We weigh the multitask\nloss with γ = 0.5 which gives the best perfor-\nmance. A coarse search was performed for tuning\nall hyperparameters over the development set. For\ncharacter-level prediction, we extract41 graphemes\nfrom the transcripts. For subword-level predic-\ntion, we train a SentencePiece tokenizer (Kudo and\nRichardson, 2018) over the train set transcriptions\nusing byte-pair encoding and vocabulary size 1200.\nFor decoding we use beam search with beam size 5\nand length normalization parameter 0.7. We train\nmodels for up to 200 epochs and the model achiev-\ning the best loss is selected using early stopping.\nAny tuning of the original architecture is performed\non the development split. No language model or\nensemble decoding is used in the output.\n4 Results and Discussion\nOne of the challenges using scaled dot-product at-\ntention is the quadratic increase of layerwise mem-\nory complexity as a function of the input sequence\nlength. This issue is particularly prevalent in ASR\ntasks, with large input sequences. We explore three\nsimple approaches to work around this limitation.\nFirst, we ﬁlter out large input sequences (x> 15s),\nleading to loss of 100 hours of data. Second we,\nchunk the input samples to smaller sequences, us-\ning forced-alignment with a conventional DNN-\nHMM model to ﬁnd pauses to split the input and\nthe transcriptions. Finally, we stack 4 consecutive\ninput frames into a single feature vector, thus re-\nducing the input length by 4. Note that this only re-\nshapes the input data as the dimension of our input\nis increased by the stacking process 1. Results for\nthe downsampling techniques for character and sub-\nword level predictions are summarized in Table 1.\nWe observe that subword-level model performs bet-\nter than the character level (upto 10% relative) in\nall settings. This can be attributed to the smaller\nnumber of decoding steps needed for the subword\nmodel, where error accumulation is smaller. Fur-\nthermore, we see that the naive ﬁltering of large\nsequences yields to underperforming systems due\nto the large data loss. Additionally, we see that\nframe stacking has superior performance to chunk-\ning. This is not surprising as splitting the input\nsamples to smaller chunks leads to the loss of con-\ntextual information which is preserved with frame\nstacking. We evaluate the proposed multiresolution\ntraining technique with the frame stacking tech-\nnique, observing a signiﬁcant improvement(18.3%)\nin the ﬁnal WER. We thus observe that predict-\ning ﬁner resolutions as an auxiliary task can be\nused as an effective means of regularization for\nthis sequence to sequence speech recognition task.\nFurthermore, we have empirically observed that\nwhen training in multiple resolutions, models can\nconverge around 50% faster than single resolution\nmodels.\nNext, we evaluate relative performance improve-\nment obtained from utilizing the visual features\n(Table 2). We observe that incorporating visual\ninformation improves ASR results. Our A V-ASR\nsystem yields gains > 3% over audio only mod-\nels for both subword and multiresolution predic-\ntions. Finally, we observe that while the Listen,\nAttend and Spell-based architecture of (Caglayan\net al., 2019) is slightly stronger than the transformer\nmodel, the gains from adding visual information\n1We tried to use the convolutional architecture from (Mo-\nhamed et al., 2019), but it failed to converge in our experi-\nments, possibly due to lack of data\n2385\n⇑\nFeatures Level WER over audio\nAudio Subword 26.1 -\nAudio + ResNeXt Subword 25.0 3 .45%\nAudio MR 21.3 -\nAudio + ResNeXt MR 20.5 3 .76%\nAudio (B) Subword 19.2 -\nAudio + ResNext (B) Subword 18.4 3.13%\nTable 2: Comparison of audio only ASR models ver-\nsus A V ASR models with ResNeXt image features.MR\nstands for multiresolution. (B) shows the results for the\nLAS model (Caglayan et al., 2019)\nMissing input handling WER\nZeros 23.1\nGaussian Noise σ=0.2 22.6\nGating visual input α=0 22.8\nTable 3: Experimental evaluation of A V-ASR model for\nhandling missing visual input. Here σdenotes the stan-\ndard deviation of the noise\nis consistent across models. It is important to note\nthat our models are trained end-to-end with both\naudio and video features.\nAn important question for real-world deploy-\nment of multimodal ASR systems is their perfor-\nmance when the visual modality is absent. Ideally,\na robust system satisfactorily performs when the\nuser’s camera is off or in low light conditions. We\nevaluate our A V-ASR systems in the absence of\nvisual data with the following experiments - a) re-\nplace visual feature vectors by zeros b) initialize\nvisual features with gaussian noise with standard\ndeviation 0.2 c) tweak the value α to 0 on infer-\nence, gating the visual features completely. Table 3\nshows the results for the different experiments. Re-\nsults indicate gating visual inputs works better than\nzeroing them out. Adding a gaussian noise per-\nforms best which again indicates the limited avail-\nability of data. Overall, in the absence of visual\ninformation, without retraining, the A V-ASR model\nrelatively worsens by 6% compared to audio only\nmodels.\n5 Conclusions\nThis paper explores the applicability of the trans-\nformer architecture for multimodal grounding in\nASR. Our proposed framework uses a crossmodal\ndot-product attention to map visual features to au-\ndio feature space. Audio and visual features are\nthen combined with a scalar additive fusion and\nused to predict character as well as subword tran-\nscriptions. We employ a novel multitask loss that\ncombines the subword level and character losses.\nResults on the How2 database show that a) mul-\ntiresolution losses regularizes our model producing\nsigniﬁcant gains in WER over character level and\nsubword level losses individually b) Adding visual\ninformation results in relative gains of 3.76% over\naudio model’s results validating our model.\nDue to large memory requirements of the atten-\ntion mechanism, we apply aggressive preprocess-\ning to shorten the input sequences, which may hurt\nmodel performance. In the future, we plan to alle-\nviate this by incorporating ideas from sparse trans-\nformer variants (Kitaev et al., 2020; Child et al.,\n2019). Furthermore, we will experiment with more\nellaborate, attention-based fusion mechanisms. Fi-\nnally, we will evaluate the multiresolution loss on\nlarger datasets to analyze it’s regularizing effects.\nReferences\nTriantafyllos Afouras, Joon Son Chung, Andrew Se-\nnior, Oriol Vinyals, and Andrew Zisserman. 2018.\nDeep audio-visual speech recognition. IEEE trans-\nactions on pattern analysis and machine intelli-\ngence.\nOzan Caglayan, Ramon Sanabria, Shruti Palaskar, Loic\nBarraul, and Florian Metze. 2019. Multimodal\ngrounding for sequence-to-sequence speech recogni-\ntion. In ICASSP 2019-2019 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 8648–8652. IEEE.\nWilliam Chan, Navdeep Jaitly, Quoc V . Le, and Oriol\nVinyals. 2016. Listen, attend and spell: A neural\nnetwork for large vocabulary conversational speech\nrecognition. In Proc. IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 4960–4964.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. arXiv preprint\narXiv:1904.10509.\nJan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk,\nKyunghyun Cho, and Yoshua Bengio. 2015.\nAttention-based models for speech recognition. In\nProceedings of the 28th International Conference\non Neural Information Processing Systems - Volume\n1, NIPS’15, page 577–585, Cambridge, MA, USA.\nMIT Press.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-xl: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\n2386\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nLinhao Dong, Shuang Xu, and Bo Xu. 2018. Speech-\ntransformer: A no-recurrence sequence-to-sequence\nmodel for speech recognition. In Proc. IEEE Inter-\nnational Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP), pages 5884–5888. IEEE.\nAlex Graves, Santiago Fern ´andez, Faustino Gomez,\nand J ¨urgen Schmidhuber. 2006. Connectionist\ntemporal classiﬁcation: Labelling unsegmented se-\nquence data with recurrent neural networks. In Pro-\nceedings of the 23rd International Conference on\nMachine Learning, ICML ’06, page 369–376, New\nYork, NY , USA. Association for Computing Machin-\nery.\nAlex Graves, Abdel-rahman Mohamed, and Geof-\nfrey E. Hinton. 2013. Speech recognition with deep\nrecurrent neural networks. In Proc. IEEE Interna-\ntional Conference on Acoustics, Speech, and Signal\nProcessing (ICASSP). IEEE.\nKensho Hara, Hirokatsu Kataoka, and Yutaka Satoh.\n2018. Can spatiotemporal 3d cnns retrace the his-\ntory of 2d cnns and imagenet? In Proceedings of\nthe IEEE conference on Computer Vision and Pat-\ntern Recognition, pages 6546–6555.\nWei-Ning Hsu, David Harwath, and James Glass. 2019.\nTransfer learning from audio-visual grounding to\nspeech recognition. Proc. Interspeech 2019, pages\n3242–3246.\nShigeki Karita, Nelson Enrique Yalta Soplin, Shinji\nWatanabe, Marc Delcroix, Atsunori Ogawa, and To-\nmohiro Nakatani. 2019a. Improving Transformer-\nBased End-to-End Speech Recognition with Con-\nnectionist Temporal Classiﬁcation and Language\nModel Integration. In Proc. INTERSPEECH, pages\n1408–1412.\nShigeki Karita, Xiaofei Wang, Shinji Watanabe,\nTakenori Yoshimura, Wangyou Zhang, Nanxin\nChen, Tomoki Hayashi, Takaaki Hori, Hirofumi In-\naguma, Ziyan Jiang, Masao Someki, Nelson En-\nrique Yalta Soplin, and Ryuichi Yamamoto. 2019b.\nA comparative study on transformer vs RNN in\nspeech applications. In IEEE Automatic Speech\nRecognition and Understanding Workshop, ASRU\n2019, Singapore, December 14-18, 2019, pages 449–\n456. IEEE.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. In Inter-\nnational Conference on Learning Representations.\nJan Kremer, Lasse Borgholt, and Lars Maaløe. 2018.\nOn the inductive bias of word-character-level multi-\ntask learning for speech recognition.\nKalpesh Krishna, Shubham Toshniwal, and Karen\nLivescu. 2018. Hierarchical multitask learning\nfor ctc-based speech recognition. arXiv preprint\narXiv:1807.06234.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71.\nHung Le, Doyen Sahoo, Nancy Chen, and Steven Hoi.\n2019. Multimodal transformer networks for end-to-\nend video-grounded dialogue systems. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 5612–5623.\nJindˇrich Libovick`y, Jindˇrich Helcl, and David Mareˇcek.\n2018. Input combination strategies for multi-source\ntransformer decoder. In Proc. 3rd Conference on\nMachine Translation, pages 253–260.\nAbdelrahman Mohamed, Dmytro Okhonko, and Luke\nZettlemoyer. 2019. Transformers with convolu-\ntional context for asr. CoRR.\nYoussef Mroueh, Etienne Marcheret, and Vaibhava\nGoel. 2015. Deep multimodal learning for audio-\nvisual speech recognition. In 2015 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 2130–2134. IEEE.\nNgoc-Quan Pham, Thai-Son Nguyen, Jan Niehues,\nMarkus M ¨uller, and Alex Waibel. 2019. Very deep\nself-attention networks for end-to-end speech recog-\nnition. Proc. Interspeech 2019, pages 66–70.\nGerasimos Potamianos, Eric Cosatto, Hans Peter Graf,\nand David B Roe. 1997. Speaker independent audio-\nvisual database for bimodal asr. In Proc. European\nTutorial and Research Workshop on Audio-Visual\nSpeech Processing, pages 65–68.\nRamon Sanabria, Ozan Caglayan, Shruti Palaskar,\nDesmond Elliott, Lo ¨ıc Barrault, Lucia Specia, and\nFlorian Metze. 2018. How2: a large-scale dataset\nfor multimodal language understanding. In Proceed-\nings of the Workshop on Visually Grounded Interac-\ntion and Language (ViGIL). NeurIPS.\nRamon Sanabria and Florian Metze. 2018. Hierarchi-\ncal multitask learning with ctc. In 2018 IEEE Spo-\nken Language Technology Workshop (SLT), pages\n485–490. IEEE.\n2387\nGeorge Sterpu, Christian Saam, and Naomi Harte.\n2018. Attention-based audio-visual fusion for ro-\nbust automatic speech recognition. In Proceedings\nof the 20th ACM International Conference on Multi-\nmodal Interaction, pages 111–115.\nFei Tao and Carlos Busso. 2018. Aligning audiovisual\nfeatures for audiovisual speech recognition. In 2018\nIEEE International Conference on Multimedia and\nExpo (ICME), pages 1–6. IEEE.\nShubham Toshniwal, Hao Tang, Liang Lu, and Karen\nLivescu. 2017. Multitask learning with low-level\nauxiliary tasks for encoder-decoder based speech\nrecognition. Proc. Interspeech 2017, pages 3532–\n3536.\nYao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,\nJ Zico Kolter, Louis-Philippe Morency, and Ruslan\nSalakhutdinov. 2019. Multimodal transformer for\nunaligned multimodal language sequences. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 6558–\n6569.\nSei Ueno, Hirofumi Inaguma, Masato Mimura, and Tat-\nsuya Kawahara. 2018. Acoustic-to-word attention-\nbased model complemented with character-level ctc-\nbased model. In 2018 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 5804–5808. IEEE.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nJun Yu, Jing Li, Zhou Yu, and Qingming Huang. 2019.\nMultimodal transformer with multi-view visual rep-\nresentation for image captioning. IEEE Transac-\ntions on Circuits and Systems for Video Technology.\nYu Zhang, William Chan, and Navdeep Jaitly. 2017.\nVery deep convolutional networks for end-to-end\nspeech recognition. pages 4845–4849.\nShiyu Zhou, Linhao Dong, Shuang Xu, and Bo Xu.\n2018. Syllable-based sequence-to-sequence speech\nrecognition with the transformer in mandarin chi-\nnese. Proc. Interspeech 2018, pages 791–795.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8459407091140747
    },
    {
      "name": "Speech recognition",
      "score": 0.7239609956741333
    },
    {
      "name": "Transformer",
      "score": 0.6214888691902161
    },
    {
      "name": "Encoder",
      "score": 0.545248806476593
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5208797454833984
    },
    {
      "name": "Word error rate",
      "score": 0.47219690680503845
    },
    {
      "name": "Visualization",
      "score": 0.4160367250442505
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.320282518863678
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1311688040",
      "name": "Amazon (United States)",
      "country": "US"
    }
  ]
}