{
  "title": "X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models",
  "url": "https://openalex.org/W3104163040",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2947056188",
      "name": "Zhengbao Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2400815267",
      "name": "Antonios Anastasopoulos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108645404",
      "name": "Jun Araki",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2136200480",
      "name": "Haibo Ding",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A277131583",
      "name": "Graham Neubig",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W2915977242",
    "https://openalex.org/W3102483398",
    "https://openalex.org/W2766841988",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W4206856021",
    "https://openalex.org/W3045462440",
    "https://openalex.org/W2799146523",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2742113707",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W3035497479",
    "https://openalex.org/W3209051700",
    "https://openalex.org/W2061271742",
    "https://openalex.org/W1985258458",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3033176962",
    "https://openalex.org/W2988975212",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W4385681388",
    "https://openalex.org/W2946676565",
    "https://openalex.org/W2996580882",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3088059392",
    "https://openalex.org/W3035547806",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W2970530566",
    "https://openalex.org/W2915774325",
    "https://openalex.org/W3021533447",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2970752815",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970963828",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W3014635508",
    "https://openalex.org/W4287827771",
    "https://openalex.org/W2983040767",
    "https://openalex.org/W2998557616",
    "https://openalex.org/W3100283070",
    "https://openalex.org/W2913129712",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W2973088264"
  ],
  "abstract": "Language models (LMs) have proven surprisingly successful at capturing factual knowledge by completing cloze-style fill-in-the-blank questions such as \"Punta Cana is located in _.\" However, while knowledge is both written and queried in many languages, studies on LMs' factual representation ability have almost invariably been performed on English. To assess factual knowledge retrieval in LMs in different languages, we create a multilingual benchmark of cloze-style probes for typologically diverse languages. To properly handle language variations, we expand probing methods from single- to multi-word entities, and develop several decoding algorithms to generate multi-token predictions. Extensive experimental results provide insights about how well (or poorly) current state-of-the-art LMs perform at this task in languages with more or fewer available resources. We further propose a code-switching-based method to improve the ability of multilingual LMs to access knowledge, and verify its effectiveness on several benchmark languages. Benchmark data and code have be released at https://x-factr.github.io.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5943–5959,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n5943\nX-FACTR: Multilingual Factual Knowledge Retrieval\nfrom Pretrained Language Models\nZhengbao Jiang†, Antonios Anastasopoulos♣,∗, Jun Araki‡, Haibo Ding‡, Graham Neubig†\n†Languages Technologies Institute, Carnegie Mellon University\n♣Department of Computer Science, George Mason University\n‡Bosch Research\nAbstract\nLanguage models (LMs) have proven surpris-\ningly successful at capturing factual knowl-\nedge by completing cloze-style ﬁll-in-the-\nblank questions such as “Punta Cana is lo-\ncated in _.” However, while knowledge is\nboth written and queried in many languages,\nstudies on LMs’ factual representation ability\nhave almost invariably been performed on En-\nglish. To assess factual knowledge retrieval in\nLMs in different languages, we create a mul-\ntilingual benchmark of cloze-style probes for\n23 typologically diverse languages. To prop-\nerly handle language variations, we expand\nprobing methods from single- to multi-word\nentities, and develop several decoding algo-\nrithms to generate multi-token predictions. Ex-\ntensive experimental results provide insights\nabout how well (or poorly) current state-of-the-\nart LMs perform at this task in languages with\nmore or fewer available resources. We fur-\nther propose a code-switching-based method\nto improve the ability of multilingual LMs\nto access knowledge, and verify its effective-\nness on several benchmark languages. Bench-\nmark data and code have be released at https:\n//x-factr.github.io.\n1 Introduction\nLanguage models (LMs; (Church, 1988; Kneser\nand Ney, 1995; Bengio et al., 2003)) learn to model\nthe probability distribution of text, and in doing so\ncapture information about various aspects of the\nsyntax or semantics of the language at hand. Recent\nworks have presented intriguing results demonstrat-\ning that modern large-scale LMs also capture a\nsigniﬁcant amount of factual knowledge (Petroni\net al., 2019; Jiang et al., 2020; Poerner et al., 2019).\nThis knowledge is generally probed by having the\nLM ﬁll in the blanks of cloze-style prompts such as\n∗: Work done at Carnegie Mellon University. The ﬁrst\ntwo authors contributed equally.\nen fr nl ru es jp vi zh hu ko tr he\n6.1\n2.2 2 1.6 1.5 1.2 1.2 1.1 0.5 0.5 0.4 0.3\nWikipedia Size (in million articles)\nel war mr mg bn tl sw pa ceb yo ilo\n0.2 0.2 0.1 0.09 0.09 0.07 0.06 0.04 0.03 0.03 0.02\nfact ⟨Bloomberg L.P., founded_in, New York⟩\nen prompt [X] was founded in [Y].\nes prompt [X] fue [fundar.Gerund;X] en [Y].\n↓ ↓ ↓\nes sentence Bloomberg L.P. fue fundada en ⟨mask⟩×1 ∼5.\nes outputs\nprediction #tokens conﬁdence\n2012 1 -1.90\nNueva York 2 -0.61\nEE. UU 3 -1.82\nChicago, Estados Unidos 4 -3.58\n2012 Bloomberg L.P 5 -3.06\nFigure 1: X-FACTR contains 23 languages, for which\nthe data availability varies dramatically. Prompts get\ninstantiated to produce grammatical sentences with dif-\nferent numbers of mask tokens and are used to ob-\ntain predictions for [Y]. In this Spanish example, the\nverb gerund “fundar” to found is rendered as “fun-\ndada” to agree in gender and number with the subject\n“Bloomberg L.P.”. The ﬁnal prediction is in bold.\n“Obama is a _ by profession.”, where these prompts\nare invariably written in English. However, it goes\nwithout saying that there are many languages of the\nworld other than English, and it is quite conceiv-\nable that (1) users may want to query this factual\nknowledge in other languages, and (2) some facts\nwill be written in non-English languages and thus\nmultilingually trained LMs (hereinafter, M-LMs)\nmay be more equipped to recall these facts in the\nlanguages of the original data. In this paper, we\nstudy the intersection of multilinguality and the\nfactual knowledge included in LMs.\nWe create a new multilingual benchmark for\nprobing factual knowledge in LMs – the Cross-\nlingual FACTual Retrieval benchmark (X-FACTR).\n5944\nX-FACTR shares a similar formulation as the\nLAMA benchmark of Petroni et al. (2019), which\nassesses whether LMs have memorized a fact (i.e.,\na subject-relation-object triple) by having LMs pre-\ndict the blank (i.e. object) in a cloze-style prompt\nfor each relation after ﬁlling in the subject. We man-\nually create such prompts for 23 languages span-\nning different language families and different lev-\nels of data availability (§ 3.1). Because many lan-\nguages that we handle are morphologically rich, we\ndesign a morphology-sensitive annotation schema\n(see example in Fig. 1) that can properly instantiate\nprompts using entity metadata (e.g. gender) and a\nmorphological inﬂection model (§ 3.3).\nIn addition, while previous works (Petroni et al.,\n2019; Jiang et al., 2020; Poerner et al., 2019)\nhave limited examination to single-token entities\n(e.g. “France”), we expand our setting to include\nmulti-token entities (e.g. “United States”), which\ncomprise more than 75% of facts included in our\nunderlying database (Wikidata; § 3.2). We propose\nseveral decoding algorithms for prediction of these\nmulti-token entities using masked LMs (§ 4). We\ndiscuss the related work in depth in § 7.\nWe perform experiments on X-FACTR (§ 5),\ncomparing and contrasting across languages and\nLMs to answer the following research questions:\n(1) How and why does performance vary across\ndifferent languages and models? (2) Can multi-\nlingual pre-training increase the amount of factual\nknowledge in LMs over monolingual pre-training?\n(3) How much does knowledge captured in differ-\nent languages overlap? We ﬁnd that the factual\nknowledge retrieval of M-LMs in high-resource\nlanguages is easier than in low-resource languages,\nbut the overall performance is relatively low, indi-\ncating that this is a challenging task. We analyze\nthe types of failure cases, shedding light on future\ndirections to improve factual knowledge in M-LMs.\nIn addition, multilingual pre-training does not nec-\nessarily lead to a higher recall of facts compared\nto language-speciﬁc monolingual pre-training. The\nknowledge memorized by M-LMs in fact is largely\ndistinct across languages, with almost 50% of facts\nbeing recalled in only one language.\nInspired by the above observations, we pro-\npose a code-switching-based objective function to\nimprove the ability of M-LMs to access knowl-\nedge using queries from a variety of languages.\nWe replace entities in a sentence from the orig-\ninal language with counterparts in another lan-\nguage, and further ﬁne-tune the LM on these code-\nswitched data (§ 6). We perform experiments on\nthree languages (French, Russian, and Greek, code-\nswitched with English). Results demonstrate that\nthis code-switching-based learning can success-\nfully improve the knowledge retrieval ability with\nlow-resource language prompts.\n2 Retrieving Facts from LMs\nIn this paper we follow the protocol of Petroni\net al. (2019)’s English-language LAMA bench-\nmark, which targets factual knowledge expressed\nin the form of subject-relation-object triples from\nWikidata1 curated in the T-REx dataset (ElSahar\net al., 2018). The cloze-style prompts used therein\nare manually created and consist of a sequence of\ntokens, where [X] and [Y] are placeholders for sub-\njects and objects (e.g. “[X] is a [Y] by profession.”).\nTo assess the existence of a certain fact, [X] is re-\nplaced with the actual subject (e.g. “Obama is a\n⟨mask⟩by profession.”) and the model predicts\nthe object in the blank ˆyi = argmaxyi p(yi|si:i),\nwhere si:i is the sentence with the i-th token\nmasked out. Finally, the predicted fact is compared\nto the ground truth. In the next section, we extend\nthis setting to more languages and predict multiple\ntokens instead of a single one.\n3 Multilingual Multi-token Factual\nRetrieval Benchmark\n3.1 Languages\nIn sampling the languages to create our multilin-\ngual benchmark, we attempted to create a subset as\ndiverse as possible with regards to data availabil-\nity, typology, and script – within the constraints of\nrequiring inclusion in Wikidata and standard pre-\ntrained M-LMs. To this end, we created prompts\nin 23 languages: English, French, Dutch, Span-\nish, Russian, Japanese, Chinese, Hungarian, He-\nbrew, Turkish, Korean, Vietnamese, Greek, Ce-\nbuano, Marathi, Bengali, Waray, Tagalog, Swahili,\nPunjabi, Malagasy, Yoruba, and Ilokano.\nOur subset includes languages from 11 fami-\nlies (the Indo-European ones include members of\nthe Germanic, Romance, Greek, Slavic, and Indic\ngenera), using 10 different scripts. Our languages\ndisplay high variance with respect to Wikipedia\npresence, a proxy for overall data availability, rang-\ning from very large to very small (see Fig. 1).2\n1https://www.wikidata.org/\n2We excluded bot-made pages for Cebuano and Waray.\n5945\nen fr nl es ru ja zh hu he tr ko vi el bn ceb mr war tl sw pa mg yo ilo\n#all 45.7 40.2 38.3 37.1 26.3 25.1 23.1 20.4 17.1 16.1 16.1 13.6 13.0 9.4 8.2 7.9 7.3 7.1 6.8 5.5 4.9 4.6 4.1\n#single-token 18.9 13.9 12.8 13.5 3.4 1.3 0.2 6.2 1.1 2.5 2.0 3.9 0.7 0.1 3.3 0.2 3.0 3.2 2.8 0.1 1.7 0.9 2.1\n#multi-token 26.8 26.4 25.5 23.6 22.9 23.8 22.9 14.2 16.0 13.6 14.1 9.7 12.3 9.3 4.9 7.7 4.4 3.9 4.0 5.4 3.2 3.7 2.0\nTable 1: X-FACTR benchmark statistics (in thousands). More details in the Appendix (Tab. 5 and Fig. 6).\n3.2 Facts\nWhile Petroni et al. (2019) and follow-up works\nfocus on entities that can be represented by a sin-\ngle token, since many popular entities consist of\nmultiple tokens (e.g. “United States”), we argue\nthat it is crucial to include multi-token entities in\nthe benchmark to make the evaluation unbiased.\nSimilar to Petroni et al. (2019), we use the T-REx\ndataset to collect facts for our benchmark. Since\nT-REx aligns facts from Wikidata with sentences\nin abstract sections from DBpedia, we can estimate\nthe commonality of each fact based on its frequency\nof being grounded to a sentence in these abstracts.\nFor each of the 46 relations in T-REx, we sample\n1000 subject-object pairs with probability propor-\ntional to their frequency. Frequency-proportional\nsampling makes the distribution of the facts in our\nbenchmark close to real usage and covers facts\nof different popularity. To keep the benchmark\nunbiased, we did not constrain the facts with any\nlanguage-related criteria (e.g., require the entities\nto have translations in all languages we considered).\nAs a result, some entities (either subjects or ob-\njects) might not have translations in all languages.\nThe number of facts in different languages in our\nmultilingual multi-token X-FACTR benchmark is\nshown in Tab. 1. Because many modern pre-trained\nM-LMs almost invariably use some variety of sub-\nword tokenization, the number of tokens an entity\ncontains will depend on the tokenization method\nused in the LM. We report the statistics based on the\nWordPiece tokenization used in multilingual BERT\n(Devlin et al., 2019). The tokenization scheme\nstatistics for the other M-LMs are similar.\n3.3 Prompts\nSome languages we include in the benchmark re-\nquire additional handling of the prompts to account\nfor their grammar or morphology. For example,\n(some) named entities inﬂect for case in languages\nlike Greek, Russian, Hebrew, or Marathi. In some\nlanguages syntactic subjects and objects need to\nbe in particular cases. Similarly, languages often\nrequire that the verb or other parts of the sentence\nagree with the subject or the object on some mor-\nphological features like person, gender, or number.\nOur prompts provide the necessary information\nin order to generate grammatical sentences, given\nthe gender and number of the entities. For example,\nthe Russian prompt for “[X] was born in [Y]” is:\n[\nX.Nom\n] [\nродился;X=MASC | роди-\nлась;X=FEM | родилось;X=NEUT\n]\nв\n[\nY.Ess\n]\n.\nThe prompt denotes that the subject ( [X]) needs\nto be in the nominative (Nom) case and the object\n([Y]) needs to be inﬂected in the essive case (Ess).\nThe prompt also accounts for the variation of the\ngender of [X] providing options (separated by |)\nfor the subject being masculine, feminine, or neuter\n(MASC, FEM, NEUT respectively).\nEverything within square brackets gets con-\ncretely instantiated given the subject and object.\nGrammatical gender is assigned through a com-\nbination of Wikidata information and language-\nspeciﬁc heuristics, constructed based on feedback\nfrom native speakers of each language. When the\nentity corresponds to a person, we retrieve their\n“sex_or_gender” properties from Wikidata. In addi-\ntion, for languages like Greek or French, the gen-\nder of an entity can be inferred with fairly high\ncertainty given the form of the word (e.g. looking\nat the ending). Last, some categories of entities\n(such as cities, countries, organizations, etc, which\ncan be obtained using the “instance_of” Wikidata\nproperty) often get assigned a general grammatical\ncase based on the category.\nOnce all the morphological features have\nbeen speciﬁed as detailed above, we use the\nunimorph_inflect package (Anastasopoulos\nand Neubig, 2019) to generate the appropriately\ninﬂected surface form of the bracketed words.3 We\nnote that the target entity ([Y]) might also need to\nbe inﬂected, as in the above Russian example, in\nwhich case we require the model’s predictions to\nmatch the inﬂected target forms.\nTo verify the quality of the prompts we per-\nformed user studies with native speakers, ﬁnding\nthat 88% on average were judged as natural and\ngrammatically correct. Details are shown in Ap-\npendix B, but it is worth noting that the majority\n3https://github.com/antonisa/unimorph_inﬂect\n5946\nof errors are due to prompts being awkward or in-\ncorrect for some senses captured by the relation,\nand not due to our gender heuristics or automatic\ninﬂection. This issue is also present in the LAMA\nEnglish prompts (Jiang et al., 2020).\n3.4 Evaluation\nAs noted in Petroni et al. (2019), because some\nsubject-relation pairs might have multiple correct\nobjects (e.g., America maintains diplomatic rela-\ntions with multiple countries), we collect all valid\nobjects and judge a prediction as correct if it can\nmatch any object (e.g., both France and Canada\nare correct). Since an entity might have multiple\naliases (e.g., “America” and “the US”), we collect\nall aliases for each entity from Wikidata, and the\nprediction is marked as correct if it can match any\none of them after lowercasing.\n4 Multi-token Decoding\nAs Tab. 1 shows, many facts involve multi-token\nentities and thus a LM would need to predict these\nentities in multiple steps. Generating multiple pre-\ndictions is straightforward for traditional left-to-\nright LMs (Sundermeyer et al., 2015; Radford et al.,\n2019), where we can autoregressively decode the\nnext token conditioned on previous tokens. How-\never, many pre-trained LMs such as BERT (Devlin\net al., 2019) are masked LMs that predict individ-\nual words given left and right contexts, and decod-\ning from such masked LMs remains an open prob-\nlem (Lawrence et al., 2019; Salazar et al., 2020;\nGhazvininejad et al., 2019; Wang and Cho, 2019;\nCho, 2019). We systematically examined different\nmulti-token decoding algorithms from three orthog-\nonal perspectives: (1) how the initial predictions\nare produced, (2) how to reﬁne the predictions, and\n(3) other commonly used components in neural text\ngeneration systems. We assume that the following\nconditional probability distribution is deﬁned by\nthe masked LM for a sentence with ntokens:\np(xk|x′\n1,...,x ′\nk−1,⟨mask⟩k,x′\nk+1,...,x ′\nn), (1)\nwhere the subscript of ⟨mask⟩indicates its position,\nand the surrounding tokenx′\n·can either be an actual\nword x·or ⟨mask⟩. We aim to handle sentences\ncontaining multiple mask tokens conditioning on\nthe surrounding actual words:\nsi:j =x1,...,xi−1,⟨mask⟩i,...,⟨mask⟩j,xj+1,...,xn, (2)\nwhere si:j indicates a sentence with the i-th to j-th\ntokens masked out.4\n4We assume that the mask tokens are consecutive for nota-\ntion simplicity, although all following methods/equations can\nBarack Obama is aby professionUnited1of1president1(a) Independent:(b) Order:(c) Confidence:Barack Obama is aby professionUnited1State2President3Barack Obama is aby professionminister2of3cabinet1\nFigure 2: Illustration of three initial prediction and re-\nﬁnement methods. Green boxes are mask tokens to be\nﬁlled, and subscripts indicate the prediction order.\n4.1 Initial Prediction and Reﬁnement\nGiven a sentence with multiple mask tokens, e.g.,\nEq. 2, we can either generate outputs in parallel\nindependently or one at a time conditioned on the\npreviously generated tokens. These methods are\nsimilar to the prediction problems that BERT (De-\nvlin et al., 2019) and XLNet (Yang et al., 2019b)\nperform in their pre-training stages respectively.\nWe deﬁne c ∈Rn as the probability of each predic-\ntion, with details varying by prediction methods.\nAfter all mask tokens are replaced with the initial\npredictions, i.e., ˆsi:j = x1,..., ˆyi,..., ˆyj,...,x n, we\ncan further reﬁne the predictions by iteratively mod-\nifying one token at a time until convergence or until\nthe maximum number of iterations is reached. Here\nwe outline the algorithms with high-level descrip-\ntions, and provide concrete details in Appendix C.\nIndependent. For independent initial prediction\n(Fig. 2a), the mask tokens are all predicted in paral-\nlel (at once). We also consider two autoregressive\nmethods for initial prediction or reﬁnement.\nOrder-based. Mask tokens are predicted from left\nto right, in each step conditioning also on the pre-\nviously generated tokens (Fig. 2b). In the reﬁne-\nment stage, we modify predictions also from left to\nright, and convergence is reached when there are\nno changes in a left-to-right scan.\nConﬁdence-based. In each step, we choose the\nprediction with the highest probability, so the or-\nder of predictions can be arbitrary (Fig. 2c). In\nthe reﬁnement stage, we choose from all predicted\ntokens the one with the lowest conﬁdence (i.e.,\nthe lowest probability) and re-predict it similarly\nto Ghazvininejad et al. (2019). Convergence is\nreached when the re-predicted token is the same as\nthe original token.\n4.2 Final Prediction\nBecause we do not know the number of tokens of\nthe ground truth in advance, we enumerate from\n1 to M mask tokens and choose the ﬁnal predic-\ntion based on the conﬁdence. Given the prompt\nin Eq. 2, the simplest way to compute the conﬁ-\ndence is pseudo log likelihood, which is the sum\nbe easily adapted to non-consecutive cases.\n5947\nof log probabilities of each predicted token condi-\ntioned on the other tokens (Salazar et al., 2020):\nv(j−i+ 1) =∑j\nk=i log ck, where ck is the con-\nﬁdence (probability) of the k-th predicted token,\nand v(m) is the overall prediction conﬁdence with\nminitial mask tokens. Among M predictions, we\nchoose the one with the highest conﬁdence.\n4.3 Additional Components\nWe also investigate additional components com-\nmonly used in neural generation systems. Specif-\nically, we consider length normalization in com-\nputing the ﬁnal conﬁdence (i.e., divide v(m) by\nthe number of mask tokens m) because a simple\nsum might favor short predictions. In addition, the\nconﬁdence value c in previous methods contains\nprobabilities when the predictions are ﬁrst gener-\nated, which will become stale once the surrounding\ntokens change (Ghazvininejad et al., 2019). We\nconsider re-computing conﬁdence c whenever a\nchange happens. Last, we attempted beam search\nto keep track of the most plausible Bpredictions\nat each step. Details of these components can be\nfound in Appendix C, along with a general schema\nof the overall decoding algorithm in Alg. 1.\n5 X-FACTR Benchmark Performance\nImplementation Details. We use the implemen-\ntations of different multilingual/monolingual pre-\ntrained LMs in the Transformers library (Wolf et al.,\n2019). We examine 3 multilingual pre-trained LMs,\nM-BERT, XLM, XLM-R (Devlin et al., 2019; Con-\nneau and Lample, 2019; Conneau et al., 2019), 5\nand 8 monolingual pre-trained LMs, BERT (en),\nCamemBERT (fr), BERTje (nl), BETO (es), Ru-\nBERT (ru), Chinese BERT (zh), BERTurk (tr), and\nGreekBERT (el) (Martin et al., 2020; de Vries et al.,\n2019; Cañete et al., 2020; Kuratov and Arkhipov,\n2019; Schweter, 2020). Details of these models\ncan be found in Appendix D.\nWe set the maximal number of mask tokens to\nM=5 for English, French, Dutch, and Spanish. In\nthese languages more than 90% of the entities are\nsplit into ≤5 tokens. For all other languages we\nuse M=10. This is expected because the vocabu-\nlary of M-LMs based on WordPiece tokenization\nis dominated by frequent words and low-resource-\nlanguage words tend to split into more pieces (Ács,\n2019). We set the maximal number of iterations\nto T = 2M, so that we can approximately reﬁne\nall the predicted tokens once for a sentence with\n5Yoruba is not in the training data of XLM and XLM-R.\nM mask tokens (the initial prediction takes exactly\nM iterations). In our main results, we report re-\nsults with two decoding algorithms: the simplest\nindependent generation method and the conﬁdence-\nbased method for both initial and reﬁnement predic-\ntions. The latter performs better than order-based\nmethods, as we will show in Tab. 3. To save compu-\ntation time, we only use conﬁdence re-computation\nfor M = 5. We discuss computation complexity in\nAppendix C.\nEvaluation Metrics. We follow Petroni et al.\n(2019), computing the accuracy of predicted ob-\njects for each relation and macro-average them as\nﬁnal scores. For ﬁne-grained analysis of different\ndecoding methods, pre-trained LMs, and languages,\nwe report results on all facts as well as on subsets\nconsisting only of single-token objects (single) and\nmulti-token objects (denoted as multi).\n5.1 Experimental Results\nWe run both the independent and conﬁdence-based\ndecoding methods with 3 M-LMs, and when avail-\nable 8 monolingual LMs, across 23 languages, 6\nwith results shown in Fig. 3. Overall, even in\nthe most favorable settings, the performance of\nstate-of-that-art M-LMs at retrieving factual knowl-\nedge in the X-FACTR benchmark is relatively\nlow, achieving less than 15% on high-resource lan-\nguages (e.g., English and Spanish) and less than\n5% for some low-resource languages (e.g., Marathi\nand Yoruba). This may initially come as a sur-\nprise, given the favorable performance reported in\nprevious papers (Petroni et al., 2019; Jiang et al.,\n2020), which achieved accuracies over 30% on En-\nglish. We justify this discrepancy in our following\nanalysis. We note that, although we provide base-\nline results in almost all languages, we perform\nour extensive analysis on a representative subset,\nconsisting of 13 languages.\nPerformance on Different Languages. Perfor-\nmance on high-resource languages is usually better\nthan performance on middle- or low-resource lan-\nguages regardless of the (M-)LMs. This is probably\ndue to high-resource languages having more data\nin the pre-training stage. It is also possible that\neven if the fact of low-resource languages is writ-\nten in the available data for these languages, it is\nnot appropriately memorized due to lack of model\ncapacity or forgetting (Kirkpatrick et al., 2017). It\n6Check https://x-factr.github.io for latest results.\n5948\nen fr nl es ru ja zh ko vi el bn ceb\n0\n6\n12\n18\n×× × × ×\nIndependent: ■ M-BERT ■ XLM ■ XLM-R ■ Language Speciﬁc\nConﬁdence-based: □ M-BERT □ XLM □ XLM-R □ Language Speciﬁc\n←−high-resource low-resource −→\nhu he tr mr war tl sw pa mg yo ilo\n0\n4\n8\n× × × × ×× ×× ×××××\nFigure 3: Accuracy on different languages using different LMs (%). Independent prediction (solid bars) outper-\nforms conﬁdence-based prediction (no-ﬁll bars) on high-resource languages but not on low-resource languages.\nDifferent models are color-coded, with missing/unsupported models marked with ×. Languages are ranked by the\ntotal number of facts in our benchmark. Details in Appendix Tab. 10.\nis worth noting that the best results are in Indo-\nEuropean languages which not only have the most\ndata, but also share the same (Latin) script which\ncould further facilitate cross-lingual learning.\nPerformance of Different LMs. Comparing the\nperformance of different M-LMs, we found that\nM-BERT outperforms XLM and XLM-R on high-\nresource languages, while on low-resource lan-\nguages performance is similar. This is contradic-\ntory to the conclusion on other cross-lingual tasks,\nsuch as natural language inference and syntactic\nprediction, as reported in Hu et al. (2020). Our\nconjecture is that because factual knowledge prob-\ning requires retrieving the identity and relations\nof individual entities, it is more ﬁne-grained than\nmore coarse-grained understanding of syntactic and\nsemantic classes that are required to solve other\ntasks. We posit that pre-training methods that show\nsuperior performance on inference and syntactic\nprediction tasks (i.e., XLM-R) might achieve good\nsyntactic/semantic abstraction at the cost of making\nless concrete lexical distinctions.\nComparing M-BERT with language-speciﬁc\nLMs, we ﬁnd M-BERT outperforms the monolin-\ngual BERT on Dutch, Spanish, and Greek, while\nunderperforming on English, Russian, Chinese,\nand Turkish. Since most of the LMs follow the\narchitecture and pre-training settings of BERT (De-\nvlin et al., 2019) or RoBERTa (Liu et al., 2019), we\nhypothesize that training corpus is the major con-\ntributor to the ﬁnal performance, and summarize\nthose corpora in Tab. 8 in the Appendix. Another\npotential explanation is that model capacity limita-\nen fr nl es ru zh he tr ko vi el mr yo\n0\n10\n20\n■ w/o oracle\n■ with oracle\n■ single-token\nFigure 4: Accuracy of the conﬁdence-based decoding\nalgorithm on different languages using M-BERT w/\nand w/o oracle length (%).\ntions preclude M-BERT from effectively memoriz-\ning entity names/relations in all of the languages.\nSingle-token vs Multi-token. Since we choose\namong Mcandidate predictions with different num-\nbers of mask tokens based on conﬁdence, it is pos-\nsible that the prediction with the correct number of\nmask tokens has lower conﬁdence than the other\npredictions. To investigate the errors introduced\nby this step, we conduct an ablation experiment\nthat assumes we know the ground-truth number\nof mask tokens. As shown in Fig. 4, performance\nimproves signiﬁcantly by 75% on average across\nall languages using the oracle mask number, in-\ndicating that pre-trained LMs have difﬁculties in\nchoosing the correct number of mask tokens. The\nperformance on single-token facts (i.e., the setting\nof previous works that only predicts a single to-\nken) is even higher, demonstrating the difﬁculty of\nmulti-token prediction.7\n7The 31.1% accuracy of BERT in Petroni et al. (2019) is\nover a different set of facts in English, constrained to be in\nthe intersection of vocabularies of several LMs. We have no\nsuch constraint, which may explain the slightly lower 25.5%\naccuracy on the English single-token performance in Fig. 4.\n5949\nType Prompt Prediction Gold en es el\nCorrect Macintosh 128K is produced by _. Apple Apple 19.89 16.68 12.02\nRepeating subjects Malin Reuterwall plays with _. the Reuterwall team Sweden’s Womens Football 22.21 24.62 25.06\nWrong entities Austria maintains diplomatic relations with _. the United States Italy, Russia, ... 16.66 29.07 18.74\nNon-informativeness Switzerland is named after _. him Canton of Schwyz 18.24 9.81 26.78\nType errors Nin9 2 5ive was written in _. the 1880s Cantonese 7.93 6.11 0.00\nRelated concepts Christof Lauer used to work in _. Germany Melsungen 7.14 1.67 1.91\nUnk Randy Newman plays _. D.D piano 5.55 8.33 11.67\nFalse Negative Switzerland maintains diplomatic relations with _. the Federal Republic of Germany Germany 2.38 3.52 3.06\nInﬂection - - - 0.00 0.19 0.77\nTable 2: Error cases of M-BERT in English and ratio of different error types in English, Spanish, and Greek (%).\nError cases in Spanish and Greek can be found in Tab. 9 in the Appendix.\nError Analysis. Even with access to an oracle\nfor the number of target tokens, though, the perfor-\nmance is still lower than 20%. To understand the\ntypes of errors made by the LMs, we sample over\n400 error cases in English, Spanish, and Greek, and\nclassify them. The error type distributions along\nwith English examples are outlined in Tab. 2.\nThe most prominent error type, about one-fourth\nof mistakes for all LMs, was repeating subjects,\nwhereby the prediction repeats either the full or\npartial subject. Predicting the wrong entities is\nalso fairly common, especially in Spanish (29%).\nInterestingly, we ﬁnd that wrong predictions are\noften a language-speciﬁc “common\" entity such as\n‘Αθήνα’ (Athens, the capital of Greece) in Greek\nlocation prompts, while the Spanish model insisted\nmost musicians play ‘ﬂauta’ (ﬂute). Another er-\nror type, particularly common in Greek (27%), is\nproducing non-informative output, where the pre-\ndictions are function words that could never be an\nentity. Type errors when the semantic type of the\nprediction is different than expected (e.g. predict-\ning dates instead of locations) are fairly common\n(English: 8%, Spanish 6%), as are related con-\ncepts predictions (English: 7%), where the model\npredicts relevant, possibly factually correct entities\n(e.g. predicting a country or a state instead of a\ncity). Worryingly, in a fair amount of cases (En-\nglish: 5%, Spanish: 8%, Greek: 11%) the models\noutput non-existent words (unk). Errors of the last\n4 types could potentially be avoided by limiting\nthe allowed outputs of the model to speciﬁc en-\ntity classes; we leave this for future work. Last,\nwe identiﬁed around 3% of false negatives, where\nthe prediction is actually correct but is not part of\nour aliases list and less than 1% of inﬂection er-\nrors where the prediction is the correct entity but\nimproperly inﬂected.\nPerformance of Different Decoding Methods.\nOverall, the conﬁdence-based decoding method im-\nproves the accuracy in middle- and low- resource\nlanguages, while it hurts the performance on high-\nEnglish Chinese\nInit. Reﬁne All Single Multi All Single Multi\nIndep. - 13.57 22.40 5.57 2.50 9.61 2.22\nOrder 13.91 21.71 6.71 4.26 8.80 4.01\nConf. 13.38 21.49 5.82 4.04 9.33 3.80\nOrder - 13.54 20.37 6.60 5.06 8.57 4.85\nOrder 13.30 19.75 6.57 5.79 8.29 5.61\nConf. 13.36 19.86 6.56 5.68 8.29 5.50\nConf. - 13.64 19.53 7.38 6.55 5.34 6.41\nOrder 13.73 19.48 7.57 6.79 4.63 6.67\nConf. 13.72 19.44 7.48 6.62 5.21 6.40\n+Len. norm 8.60 9.43 6.18 3.96 2.27 3.93\n+Re-comp. 12.00 12.91 10.08 5.89 2.71 5.84\n+Beam 10.84 9.29 11.06 6.34 2.38 6.30\nTable 3: Accuracy of different decoding methods using\nM-BERT on English and Chinese (%).\nresource languages. To better understand the effect\nof different components on the ﬁnal performance,\nwe conduct a comprehensive comparison on En-\nglish and Chinese. We compare the three initial\nprediction methods and the three reﬁnement op-\ntions (including not performing reﬁnement), for a\ntotal of nine decoding methods (§ 4.1). We fur-\nther apply additional improvements (§ 4.3) on the\nconﬁdence-based decoding method.\nBy comparing the performance in Tab. 3, we\nﬁrst see advanced decoding methods improve per-\nformance on multi-token objects, but hurt perfor-\nmance on single-token ones. The best-performing\ndecoding method on English improves the multi-\ntoken accuracy from 5.57% to 11.06%, indicat-\ning that advanced decoding methods have a better\nchance to elicit multi-token facts from M-BERT.\nSome examples are shown in Tab. 7 in the Ap-\npendix. The lower performance on single-token ob-\njects is probably caused by the fact that advanced\ndecoding methods discover multi-token predictions\nthat have higher conﬁdence than single-token ones\n(§ 4.2). For example, the single-token prediction\nfor “Enrique Iglesias used to communicate in _.” is\n“Spanish”, while the best decoding method outputs\n“his own words” with higher conﬁdence. Second,\ninitial prediction methods have a greater effect on\nthe ﬁnal performance than reﬁnement methods. We\n5950\nenfrnlesruzhhetrkovielmryo16%21%34%21%6%18%10%8%10%20%14%7%24%en14%27%22%5%15%12%10%9%21%10%9%12%fr16%25%5%21%10%7%10%24%17%14%0%nl14%6%18%9%9%11%20%16%14%1%es5%8%5%4%6%5%6%3%5%ru13%12%4%12%15%17%12%9%zh10%7%5%12%7%9%20%he6%5%4%4%2%8%tr8%8%10%6%3%ko14%18%11%12%vi11%14%5%el8%1%mr8%yo\nFigure 5: Bottom-left: the ratio of facts with respect\nto the number of languages in which the facts could be\nsuccessfully retrieved. Top-right: overlap ratio of cor-\nrect predictions between two languages. The values on\nthe diagonal are the average overlap ratio of the corre-\nsponding language with the other languages.\nhypothesize that this is because the greedy decod-\ning process heavily depends on previous predic-\ntions, and reﬁnement cannot recover from unsatis-\nfactory initial predictions. Third, length normaliza-\ntion was not found useful in either case.\nThere are also observations not consistent across\nthe two languages. First, since Chinese has a larger\nportion of multi-token objects than English (as\nshown in Tab. 1), the overall performance on Chi-\nnese increases while it decreases on English, which\nis consistent with the observation in Fig. 3. Second,\nconﬁdence re-computation and beam search are not\nas effective on Chinese, which we conjecture is be-\ncause that the distribution over English sentences\nexhibits more multimodality than the distribution\nover Chinese sentences due to more training data.\n6 Improving Multilingual LM Retrieval\nAs the performance of M-LMs is relatively low,\nespecially on low-resource languages, an obvious\nendeavor is to reﬁne the model to improve fact\nretrieval performance in various languages. We an-\nalyze how similarly M-BERT performs on queries\nin different languages. We collect correctly pre-\ndicted facts across all languages, and count in how\nmany languages each fact was retrieved correctly.\nAs shown in the bottom-left histogram of Fig. 5,\nhalf of the correctly predicted facts were correct in\na single language, indicating little overlap across\nlanguages (Lin et al., 2018). Only 3% of facts\nwere correct in more than 5 languages, and objects\nin those facts are usually sub-strings of subjects,\nmaking them easy to retrieve regardless of the lan-\nguage. This observation is also conﬁrmed by the\noverlap between pairs of languages in the top-right\nchart of Fig. 5; even the most similar languages\n(i.e., English and Dutch) only have 34% of correct\npredictions in common.\nWe ﬁnd that facts retrievable only in a single\nlanguage tend to be knowledge that is mainly men-\ntioned in a certain language. For example, M-\nBERT mistakenly predicts “QQ” in the English\nsentence “Tencent QQ is developed by _.”, while\nthe prediction “腾讯” (Tencent) in the correspond-\ning Chinese sentence “腾讯QQ是由_开发的。”\nis correct. This is probably because Tencent, a Chi-\nnese company, is more frequently mentioned in the\nChinese training corpus.\n6.1 Methods\nInspired by these observations, we propose to\nuse code-switching to create data to ﬁne-tune pre-\ntrained LMs, replacing entity mentions in one lan-\nguage (e.g., English/Greek) with their counterparts\nin another language (e.g., Greek/English). Through\nthis bi-directional code-switching, entity mentions\nserve as pivots, enabling knowledge that was orig-\ninally learned in one language to be shared with\nothers. Given a pair of languages, we ﬁrst iden-\ntify Wikipedia sentences that mention entities from\nour benchmark using SLING (Ringgaard et al.,\n2017). The M-LM is then ﬁnetuned on these sen-\ntences. Following Wu et al. (2020), with 30% of\nprobability we switch all the entity mentions (can\nbe one or multiple) from the original language to\ntheir counterparts in the other language, ending\nup with sentences like “Οµπάµα later reﬂected on\nhis years ...\", where we substituted “Obama\" with\na Greek mention of the entity, and vice-versa for\nGreek-to-English. 70% of the sentences remain the\nsame. If there are multiple mention texts for an en-\ntity, we sample proportionally to their frequencies,\nwhich we found in our preliminary experiments\nperformed better than using a ﬁxed translation. We\nﬁne-tune M-BERT using the masked LM objective\non this data, with 15% of non-mention words and\n50% of mention words masked out.8\n6.2 Experimental Results\nWe choose three languages with different data avail-\nability, namely French, Russian, and Greek, and\npair them with English, producing 560k, 396k, and\n129k code-switched sentences respectively. We\ncompare M-BERT after code-switched ﬁne-tuning\n(denoted as cs) with both the original M-BERT\nand with ﬁne-tuning only on raw text ( raw). We\nvary the evaluation settings to illustrate the effect of\ncode-switching: on top of matching predictions to\n8The larger ratio on entities encourages the model to focus\non predicting entities, as in the downstream task.\n5951\nSingle-eval Double-eval\nLang. Method All Single Multi All Single Multi\nFrench\nM-BERT 10.21 19.07 3.92 10.67 19.24 4.55\n+raw 15.06 26.81 7.40 15.69 26.92 8.27\n+cs 13.15 24.37 6.34 16.90 26.98 10.29\nRussian\nM-BERT 1.87 4.58 0.96 3.04 7.72 2.28\n+raw 7.92 24.37 3.59 8.77 26.28 4.57\n+cs 7.64 22.41 3.55 11.69 25.31 7.85\nGreek\nM-BERT 4.49 20.75 2.19 4.97 20.87 2.83\n+raw 11.49 35.27 7.65 12.65 35.27 9.27\n+cs 9.30 26.31 5.73 18.41 30.93 15.30\nTable 4: Accuracy of M-BERT after ﬁne-tuning on raw\nand code-switched text (%).\nground truth aliases in the prompt language (single-\neval), we evaluate with targets in both languages\n(double-eval; English and prompt).\nAs shown in Tab. 4, continued ﬁne-tuning on\nraw text outperforms the original M-BERT, likely\ndue to our ﬁne-tuning on a subset of sentences with\nmentions of entities from our benchmark. Results\non code-switched text are slightly worse when only\nmatching entities in the original target language,\nbut signiﬁcantly better if we allow matching in both\nthe original language and English. This indicates\nthat code-switched ﬁne-tuning allows M-BERT to\nretrieve facts, albeit in English rather than in the\nprompt language. Encouragingly, the increase is\nlarger for low-resource (Greek) and typologically\ndistant-to-English (Russian) languages. For exam-\nple, the prediction for the Greek prompt “η Θεωρία\nκατηγοριών είναι µέρος των .” (“Category the-\nory is part of _.”) is “mathematics” (in English!),\nwhile the prediction without code-switching is the\nnon-informative “οποίων” (“which”). Considering\nthat we have more raw than code-switched sen-\ntences in the dataset, this seems to indicate that En-\nglish entities are easier to predict than their prompt-\nlanguage counterparts, which might be because\nfacts expressed in English are better learned in the\npre-trained model due to training data abundance.\n7 Related Work\nFactual Knowledge Retrieval from LMs Sev-\neral works have focused on probing factual knowl-\nedge solely from pre-trained LMs without access\nto external knowledge. They do so by either using\nprompts and letting the LM ﬁll in the blanks, which\nassumes that the LM is a static knowledge source\n(Petroni et al., 2019; Jiang et al., 2020; Poerner\net al., 2019; Bouraoui et al., 2020), or ﬁne-tuning\nthe LM on a set of question-answer pairs to directly\ngenerate answers, which dynamically adapts the\nLM to this particular task (Roberts et al., 2020).\nImpressive results demonstrated by these works\nindicate that large-scale LMs contain a signiﬁcant\namount of knowledge, in some cases even outper-\nforming competitive question answering systems\nrelying on external resources (Roberts et al., 2020).\nPetroni et al. (2020) further shows that LMs can\ngenerate even more factual knowledge when aug-\nmented with retrieved sentences. Our work builds\non these works by expanding to multilingual and\nmulti-token evaluation, and also demonstrates the\nsigniﬁcant challenges posed by this setting.\nMultilingual Benchmarks Many multilingual\nbenchmarks have been created to evaluate the per-\nformance of multilingual systems on different nat-\nural language processing tasks, including question\nanswering (Artetxe et al., 2020; Lewis et al., 2019;\nClark et al., 2020), natural language understanding\n(Conneau et al., 2018; Yang et al., 2019a; Zweigen-\nbaum et al., 2018; Artetxe and Schwenk, 2019),\nsyntactic prediction (Nivre et al., 2018; Pan et al.,\n2017), and comprehensive benchmarks covering\nmultiple tasks (Hu et al., 2020; Liang et al., 2020).\nWe focus on multilingual factual knowledge re-\ntrieval from LMs, which to our knowledge has not\nbeen covered by any previous work.\n8 Conclusion\nWe examine the intersection of multilinguality and\nthe factual knowledge included in LMs by creat-\ning a multilingual and multi-token benchmark X-\nFACTR, and performing experiments comparing\nand contrasting across languages and LMs. The\nresults demonstrate the difﬁculty of this task, and\nthat knowledge contained in LMs varies across lan-\nguages. Future directions include other pre-training\nor ﬁne-tuning methods to improve retrieval per-\nformance and methods that encourage the LM to\npredict entities of the right types.\nAcknowledgements\nThis work was supported by a gift from Bosch Re-\nsearch. The authors are thankful to the reviewers\nfor the thorough and insightful comments. They\nare also particularly grateful for everyone who\nhelped create, check, or evaluate the templates and\nthe outputs of our models: Aman Madaan, Aditi\nChaudhary, Paul Michel, Sergio Franco, Maria\nRyskina, Chan Young Park, Hiroaki Hayashi, Toan\nNguyen, David Ifeoluwa Adelani, Bonaventure\nDossou, Emre Yolcu, Happy Buzaaba, and Fahim\nFaisal.\n5952\nReferences\nJudit Ács. 2019. Exploring bert’s vocabulary. Ac-\ncessed May 2020.\nAntonios Anastasopoulos and Graham Neubig. 2019.\nPushing the limits of low-resource morphological in-\nﬂection. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n984–996, Hong Kong, China. Association for Com-\nputational Linguistics.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the Cross-lingual Transferability of Mono-\nlingual Representations. In Proceedings of ACL\n2020.\nMikel Artetxe and Holger Schwenk. 2019. Massively\nMultilingual Sentence Embeddings for Zero-Shot\nCross-Lingual Transfer and Beyond. Transactions\nof the ACL 2019.\nYoshua Bengio, Réjean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. Journal of machine learning research,\n3(Feb):1137–1155.\nZied Bouraoui, Jose Camacho-Collados, and Steven\nSchockaert. 2020. Inducing relational knowledge\nfrom BERT. In Thirty-Fourth AAAI Conference on\nArtiﬁcial Intelligence (AAAI), New York, USA.\nJosé Cañete, Gabriel Chaperon, Rodrigo Fuentes, and\nJorge Pérez. 2020. Spanish pre-trained bert model\nand evaluation data. In PML4DC at ICLR 2020.\nKyunghyun Cho. 2019. Bert has a mouth and must\nspeak, but it is not an mrf. Accessed May 2020.\nKenneth Ward Church. 1988. A stochastic parts pro-\ngram and noun phrase parser for unrestricted text.\nIn Second Conference on Applied Natural Language\nProcessing, pages 136–143, Austin, Texas, USA.\nAssociation for Computational Linguistics.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. TyDi QA: A Bench-\nmark for Information-Seeking Question Answering\nin Typologically Diverse Languages. In Transac-\ntions of the Association of Computational Linguis-\ntics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. CoRR,\nabs/1911.02116.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances\nin Neural Information Processing Systems 32: An-\nnual Conference on Neural Information Processing\nSystems 2019, NeurIPS 2019, 8-14 December 2019,\nVancouver, BC, Canada, pages 7057–7067.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. XNLI: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2475–2485,\nBrussels, Belgium. Association for Computational\nLinguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nHady ElSahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon S. Hare, Frédérique\nLaforest, and Elena Simperl. 2018. T-rex: A large\nscale alignment of natural language with knowledge\nbase triples. In Proceedings of the Eleventh Inter-\nnational Conference on Language Resources and\nEvaluation, LREC 2018, Miyazaki, Japan, May 7-\n12, 2018 . European Language Resources Associa-\ntion (ELRA).\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-predict: Parallel de-\ncoding of conditional masked language models. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 6112–\n6121, Hong Kong, China. Association for Computa-\ntional Linguistics.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual general-\nization. CoRR, abs/2003.11080.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics (TACL).\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, et al. 2017. Over-\ncoming catastrophic forgetting in neural networks.\nProceedings of the national academy of sciences ,\n114(13):3521–3526.\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-off for m-gram language modeling. In1995\nInternational Conference on Acoustics, Speech, and\nSignal Processing, volume 1, pages 181–184. IEEE.\n5953\nYuri Kuratov and Mikhail Arkhipov. 2019. Adaptation\nof deep bidirectional multilingual transformers for\nrussian language. CoRR, abs/1905.07213.\nCarolin Lawrence, Bhushan Kotnis, and Mathias\nNiepert. 2019. Attending to future tokens for bidi-\nrectional sequence generation. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1–10, Hong Kong, China.\nAssociation for Computational Linguistics.\nPatrick Lewis, Barlas O ˘guz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2019. MLQA: Evalu-\nating Cross-lingual Extractive Question Answering.\narXiv preprint arXiv:1910.07475.\nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fen-\nfei Guo, Weizhen Qi, Ming Gong, Linjun Shou,\nDaxin Jiang, Guihong Cao, Xiaodong Fan, Bruce\nZhang, Rahul Agrawal, Edward Cui, Sining Wei,\nTaroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie\nWu, Shuguang Liu, Fan Yang, Rangan Majumder,\nand Ming Zhou. 2020. XGLUE: A new benchmark\ndataset for cross-lingual pre-training, understanding\nand generation. CoRR, abs/2004.01401.\nBill Y . Lin, Frank F. Xu, Kenny Q. Zhu, and Seung-won\nHwang. 2018. Mining cross-cultural differences and\nsimilarities in social media. In Proceedings of the\n56th Annual Meeting of the Association for Com-\nputational Linguistics, ACL 2018, Melbourne, Aus-\ntralia, July 15-20, 2018, Volume 1: Long Papers ,\npages 709–719. Association for Computational Lin-\nguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Suárez, Yoann Dupont, Laurent Romary,\nÉric Villemonte de la Clergerie, Djamé Seddah, and\nBenoît Sagot. 2020. Camembert: a tasty french lan-\nguage model. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics.\nJoakim Nivre, Mitchell Abrams, Željko Agi ´c, Lars\nAhrenberg, Lene Antonsen, Maria Jesus Aranzabe,\nGashaw Arutie, Masayuki Asahara, Luma Ateyah,\nMohammed Attia, et al. 2018. Universal dependen-\ncies 2.2.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel\nNothman, Kevin Knight, and Heng Ji. 2017. Cross-\nlingual name tagging and linking for 282 languages.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers) , pages 1946–1958, Vancouver,\nCanada. Association for Computational Linguistics.\nFabio Petroni, Patrick S. H. Lewis, Aleksandra Pik-\ntus, Tim Rocktäschel, Yuxiang Wu, Alexander H.\nMiller, and Sebastian Riedel. 2020. How context\naffects language models’ factual predictions. CoRR,\nabs/2005.04611.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nNina Poerner, Ulli Waltinger, and Hinrich Schütze.\n2019. E-bert: Efﬁcient-yet-effective entity embed-\ndings for bert. CoRR, abs/1911.03681.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8).\nMichael Ringgaard, Rahul Gupta, and Fernando C. N.\nPereira. 2017. SLING: A framework for frame se-\nmantic parsing. CoRR, abs/1710.07032.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? CoRR, abs/2002.08910.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-\ntrin Kirchhoff. 2020. Masked language model scor-\ning. In Proceedings of ACL 2020.\nStefan Schweter. 2020. Berturk - bert models for turk-\nish.\nMartin Sundermeyer, Hermann Ney, and Ralf Schlüter.\n2015. From feedforward to recurrent LSTM neural\nnetworks for language modeling. IEEE ACM Trans.\nAudio Speech Lang. Process., 23(3):517–529.\nWietse de Vries, Andreas van Cranenburgh, Arianna\nBisazza, Tommaso Caselli, Gertjan van Noord, and\nMalvina Nissim. 2019. Bertje: A dutch BERT\nmodel. CoRR, abs/1912.09582.\nAlex Wang and Kyunghyun Cho. 2019. BERT has a\nmouth, and it must speak: BERT as a markov ran-\ndom ﬁeld language model. CoRR, abs/1902.04094.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. CoRR, abs/1910.03771.\nShijie Wu, Alexis Conneau, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Emerging cross-\nlingual structure in pretrained language models. In\nProceedings of ACL 2020.\n5954\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason\nBaldridge. 2019a. PAWS-x: A cross-lingual ad-\nversarial dataset for paraphrase identiﬁcation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3687–\n3692, Hong Kong, China. Association for Computa-\ntional Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G.\nCarbonell, Ruslan Salakhutdinov, and Quoc V . Le.\n2019b. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. In Advances in Neu-\nral Information Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n2019, NeurIPS 2019, 8-14 December 2019, Vancou-\nver, BC, Canada, pages 5754–5764.\nPierre Zweigenbaum, Serge Sharoff, and Reinhard\nRapp. 2018. Overview of the third bucc shared task:\nSpotting parallel sentences in comparable corpora.\nIn Proceedings of 11th Workshop on Building and\nUsing Comparable Corpora, pages 39–42.\n5955\nA Benchmark Details\nTab. 5 shows the detailed number of facts in each\nlanguage in our X-FACTR benchmark. Fig. 6\ndemonstrates the ratio of facts with respect to the\nnumber of tokens of the object in different lan-\nguages, where high-resource languages (e.g., En-\nglish, French, Dutch, and Spanish) have more por-\ntion of single-token facts than low-resource lan-\nguages.\nB Benchmark Prompt Quality\nThe prompts generated in different languages may\nnot be perfectly natural. This could be due to\nawkwardness of attempting to express relational\nphrases that were originally devised for English in\nlanguages where the semantic distinctions of the\nunderlying words may differ, or due to our errors in\nour automated approach to grammatical attribute in-\nference and subsequent inﬂection. To this end, we\nevaluated our prompts on a sample of languages,\nproviding native speakers with 10 sentences per\nprompt with the missing slots ﬁlled by our inﬂec-\ntion models. Our approach produces sentences that\nare annotated as correct 97.9% of the cases in Span-\nish, 90.5% in Yoruba, 86.7% in Greek, 82.3% in\nMarathi, and 81.9% in Russian.\nWe present an analysis of the annotations on\nthe erroneous prompts in Table 6. The error types\ndiffer drastically across languages. Russian and\nMarathi have comparatively large percentages of\ninﬂection-related errors, but for different reasons:\nthe prediction of non-human entity grammatical\ngender in Russian is difﬁcult and this results in\nmistakes in the inﬂection. In Marathi, this issue\nis also exacerbated by the inﬂection model, which\nis of slightly lower quality due to the scarcity of\ntraining data availability.\nDespite these two outliers, we consider the rest\nof our prompts to be of high quality. Even if small\ninﬂection or grammatical gender assignment mis-\ntakes occur (e.g. in Greek) this should not render\nthe prompt unintelligible to native speakers – the\nburden is on the model to be robust to such slight\nvariations, just as humans are. We point out that\nthe prompts can be awkward or incorrect for some\nsenses captured by the relation, an issue unrelated\nto our gender heuristics or automatic inﬂection.\nThis issue, though, is also present in the LAMA\nEnglish prompts (Petroni et al., 2019; Jiang et al.,\n2020) and is the result of the original Wikidata\nannotation.\nC Multi-Token Decoding\nWe outline here the exact concrete formulation of\nour multi-token decoding algorithms. Given a sen-\ntence with multiple mask tokens, e.g., Eq. 2, we can\neither generate outputs in parallel independently or\none at a time conditioned on the previously gener-\nated tokens. These methods are similar to the pre-\ndiction problems that BERT (Devlin et al., 2019)\nand XLNet (Yang et al., 2019b) perform in their\npre-training stages respectively. We deﬁne c ∈Rn\nas the conﬁdence of each prediction, with details\nvarying by prediction method.\nC.1 Initial Prediction and Reﬁnement\nIndependent For independent initial prediction,\nthe mask tokens are all predicted in parallel:\nˆyk = argmax\nyk\np(yk|si:j),ck = p(ˆyk|si:j),\n∀k∈{i,...,j }.\nWe also consider two autoregressive methods for\ninitial prediction or reﬁnement.\nOrder-based Mask tokens are predicted from\nleft to right, conditioned on previously generated\ntokens in each step:\nˆyi = argmax\nyi\np(yi|si:j),ci = p(ˆyi|si:j).\nIn the reﬁnement stage, we modify the predicted\ntokens from left to right by replacing the token with\na ⟨mask⟩and re-predicting it:\nˆyi = argmax\nyi\np(yi|ˆsi:j \\i),ci = p(ˆyi|ˆsi:j \\i),\nwhere s\\imeans that the i-th token in s is replaced\nwith ⟨mask⟩. Convergence is reached when there\nare no changes in a left-to-right scan.\nConﬁdence-based Among all the predictions for\nmasked positions, we choose the one with the high-\nest conﬁdence (i.e., the highest probability), so\nthe actual order of predictions can be arbitrary, as\nshown in Fig. 2:\nˆyk = argmax\ni≤k≤j,yk\np(yk|si:j),ck = p(ˆyk|si:j).\nIn the reﬁnement stage, we choose from all pre-\ndicted tokens the one with the lowest conﬁdence\n(i.e., the lowest probability) and re-predict it\n(Ghazvininejad et al., 2019):\nˆyk = argmax\nyk\np(yk|ˆsi:j \\k),ck = p(ˆyk|ˆsi:j \\k),\nk= argmin\ni≤k≤j\nck.\nConvergence is reached when the re-predicted to-\nken is the same as the original token.\n5956\nen fr nl es ru ja zh hu he tr ko vi\n#facts 45684 40240 38291 37065 26265 25144 23142 20438 17050 16104 16098 13642\n#single-word facts 18903 13886 12812 13463 3391 1312 210 6241 1057 2506 1964 3909\n#multi-word facts 26781 26354 25479 23602 22874 23832 22932 14197 15993 13598 14134 9733\nel bn ceb mr war tl sw pa mg yo ilo\n#facts 13034 9383 8160 7877 7342 7116 6834 5455 4945 4609 4053\n#single-word facts 742 53 3257 199 2981 3208 2840 67 1748 930 2099\n#multi-word facts 12292 9330 4903 7678 4361 3908 3994 5388 3197 3679 1954\nTable 5: Detailed X-FACTR Benchmark statistics. Languages are ranked by the total number of facts.\nFigure 6: Ratio of facts with respect to the number of tokens of the object in different languages.\n% Errors\nLanguage % Correct Inﬂection Gender Number Awkward Wrong Sense\nGreek 86.7 5.4 7.4 0.5 5.0 5.0\nSpanish 97.9 – 1.6 0.8 1.9 0\nMarathi 82.3 15.1 – 0.2 0 4\nRussian 81.9 16.1* – – 18.1* 6.7\nYoruba 90.5 – – – 4.1 0\nTable 6: Error analysis on the prompts after instantiat-\ning with actual examples. We note that the error cate-\ngories are not mutually exclusive. *: The Russian in-\nﬂection percentage includes gender and number errors,\nunlike the other languages; the Russian annotator also\nmarked all erroneous sentences as “awkward\", skewing\nthe results.\nC.2 Additional Decoding Components\nLength Normalization Since the sum used in\n§ 4.2 might favor short predictions, we consider\nnormalizing it by the number of the mask tokens:\nv(j−i+ 1) = 1\nj−i+ 1\nj∑\nk=i\nlog ck,\nConﬁdence Re-computation Note that the con-\nﬁdence of each predicted token c in previous equa-\ntions is the probability when the token is predicted.\nHowever, the probability will become stale once\nthe surrounding tokens change because of the bidi-\nrectional conditional distributions, and this is also\nnoted in (Ghazvininejad et al., 2019). To make the\nconﬁdence up-to-date, given the prompt in Eq. 2,\nwhen a new token is predicted (in the initial stage)\nor a token is modiﬁed (in the reﬁnement stage), we\nre-compute ci to cj. This makes the time complex-\nity quadratic to the number of mask tokens, because\nevery time we make a modiﬁcation, we have to re-\ncompute the conﬁdence values of all predictions.\nAs a result, the ﬁnal conﬁdence becomes:\nck = p(ˆyk|ˆsi:j \\k),\nwhere ˆsi:j = x1,..., ˆyi,..., ˆyj,...,x n contains the\nﬁnal predictions.\nBeam Search All of the previous methods use\nthe most plausible prediction at each masked po-\nsition. We also consider performing beam search\nthat keeps track of the most plausible B predic-\ntions. Our beam search algorithm is very similar\nto the case of conventional left-to-right decoding,\nexcept that the decoding order might be arbitrary if\nwe use conﬁdence-based initial or reﬁnement pre-\ndiction methods. As a result, extending different\nsamples in the beam might lead to the same results\nso we need an additional deduplication step. The\ntime complexity with all the above components\nis O(M2BT), where M is the maximal number\nof mask tokens, and T is the maximal number of\niteration. Alg. 1 outlines the overall multi-token de-\ncoding algorithm. The conﬁdence-based decoding\nmethod takes 20 minutes to 2 hours on a Nvidia\nGeforce RTX 2080 Ti GPU depending on the num-\nber of facts of each language.\n5957\nAlgorithm 1: Multi-token decoding.\nResult: The ﬁnal sentence ˆs.\nmax number of mask tokens M, beam size B, max\nnumber of iteration T, an initial sentence s(0);\nfor number of mask tokens m= 1,...,M do\ns(0)\nm ←insert m⟨mask⟩tokens in s(0);\nS ←{s(0)\nm };\nfor iteration t= 1,...,T do\nS′←φ;\nfor each sentence s(t−1)\nm ∈S do\n{s(t,b)\nm }B\nb=1 ←top Bpredictions after\nan initial or reﬁnement step;\nS′←S′∪{s(t,b)\nm }B\nb=1\nend\nS ←deduplicate and get the top Bfrom S′;\nend\nend\nˆs ←top one from S;\nPrompts Ind. Best\nThe capital of India is _. Rajasthan New Delhi\nThe capital of Auvergne is _. Lyon Clermont-Ferrand\nAmerican League is part of _. the League Major League Baseball\nFirst Epistle to Timothy is part of _. Christianity the New Testament\nKGB is a legal term in _. KGB the Soviet Union\nCenters for Disease Control and CDC the United StatesPrevention is a legal term in _.\nTable 7: Prediction results of M-BERT where the best-\nperforming decoding method makes correct predictions\nwhile the independent prediction method does not.\nD Details of Pre-trained LMs\nLMs examined in this paper share similar archi-\ntecture and pre-training setting as BERT (Devlin\net al., 2019) or RoBERTa (Liu et al., 2019), but\nare trained on different corpora. We provide\nthe shortcut name of each LM in the Hugging-\nFace’s Transformer library (https://huggingface.\nco/transformers/pretrained_models.html) and their\ntraining corpora in Tab. 8, from which you can ﬁnd\nmore information.\nE Detailed Experimental Results\nDetailed performance across LMs and languages\nand error cases in Spanish and Greek are shown in\nTab. 10 and Tab. 9 respectively.\n5958\nModel Shortcut Corpus\nmultilingual LMs\nM-BERT bert-base-multilingual-cased Wikipedia\nXLM xlm-mlm-100-1280 Wikipedia\nXLM-R xlm-roberta-base CommonCrawl\nmonolingual LMs\nBERT (en) bert-base-cased BooksCorpus, English Wikipedia\nCamemBERT (fr) camembert-base French OSCAR ◦\nBERTje (nl) bert-base-dutch-cased Dutch Wikipedia, Books, TwNC ⋆, SoNaR-500†, Web news\nBETO (es) dccuchile/bert-base-spanish-wwm-cased Spanish Wikipedia, Spanish OPUS ◁\nRuBERT (ru) DeepPavlov/rubert-base-cased Russian Wikipedia, news data\nChinese BERT (zh) bert-base-chinese Chinese Wikipedia\nBERTurk (tr) dbmdz/bert-base-turkish-cased Turkish Wikipedia, Turkish OSCAR, Turkish OPUS, etc\nGreekBERT (el) nlpaueb/bert-base-greek-uncased-v1 Greek Wikipedia, Greek Europarl ⋄, Greek OSCAR\nTable 8: Shortcut name of each multilingual/monolingual LM in HuggingFace’s Transformers library, and their\ntraining copora. ◦The OSCAR corpus is extracted from the CommonCrawl corpus. ⋆ TwNC is a multifaceted\nDutch News Corpus. †SoNaR-500 is a multi-genre Dutch reference corpus. ◁ OPUS is a translated text corpus\nfrom the web. ⋄Europarl is a corpus of parallel text.\nType Prompt Prediction Gold Ratio\nCorrect Vilna y _ son ciudades gemelas. Minsk Minsk 16.68\nRepeating subjects La capital de Bali es _. Bali Denpasar 24.62\nWrong entities John Goldschmidt es un _ de profesiòn. comerciant director de cine 29.07\nNon-informativeness Lionel Heald fue educado en la Universidad de _. la Universidad Charterhouse School 9.81\nType errors Jänta å ja fue creada en _. 2005 Suecia 6.11\nRelated concepts Bas Heijne nació en _. el Reino de Holanda Nimega 1.67\nUnk Tanaj consiste de _. :1.2 Torá 8.33\nFalse Negative BMW S1000RR es producido por _. BMW BMW Motorrad 3.52\nInﬂection proteína de membrana es una subclase de _. proteínas proteína 0.19\nCorrect το Καµερούν βρίσκεται στην . Αφρική Αφρική 12.02\nRepeating subjects η Λάσα ντε Σέλα δούλευε στην. Λάσα ντε Σέλα Μόντρεαλ 25.06\nWrong entities η Χένσελ ιδρύθηκε στην . Ιταλία Κάσσελ 18.74\nNon-informativeness ο Πωλ Καρνό δουλεύει στο. χωριό Πανεπιστή µιο ραρισιού 26.78\nRelated concepts οι The Kooks ιδρύθηκαν στην . Αγγλία Μπράιτον 1.91\nUnk ο Ραβί Σανκάρ παίζει . π Σιτάρ 11.67\nFalse Negative το Disneyland ανήκει στο . Walt Disney the Walt Disney Company 3.06\nInﬂection ο Χριστός είναιµέρος του . Χριστός Χριστού 0.77\nTable 9: Error cases of M-BERT in Spanish and Greek (%).\n5959\nModel Decoding Part en fr nl es ru zh he tr ko vi el mr yo\nM-BERT\nInd.\nall 13.57 10.21 12.42 14.30 1.87 2.50 2.70 2.00 4.08 8.34 4.46 2.76 3.44\nsingle 22.40 19.07 25.21 24.25 4.58 9.61 7.43 4.50 21.14 17.69 21.11 12.11 5.15\nmulti 5.57 3.92 4.42 4.90 0.96 2.22 2.56 1.03 1.61 2.91 2.16 2.18 3.29\nConf.\nall 12.00 6.30 8.55 7.47 2.54 6.62 2.92 2.08 4.70 9.20 6.77 3.46 3.21\nsingle 12.91 7.77 12.20 9.13 3.65 5.21 4.33 4.34 16.15 14.60 13.69 8.99 3.87\nmulti 10.08 4.78 5.22 5.11 1.86 6.49 2.90 1.19 2.88 5.22 5.72 3.07 3.06\nXLM\nInd.\nall 9.03 7.44 7.53 7.40 2.29 5.83 2.79 1.59 5.33 6.86 7.10 1.26 -\nsingle 20.74 16.58 18.38 16.44 7.62 17.12 11.58 5.53 13.28 12.12 18.03 12.62 -\nmulti 4.75 4.03 3.00 3.40 1.40 2.57 1.82 0.50 3.24 3.93 5.16 0.10 -\nConf.\nall 5.30 4.13 4.46 3.18 2.14 3.40 1.93 1.85 5.23 6.26 7.56 1.48 -\nsingle 8.79 6.14 6.48 4.18 3.61 10.44 5.97 4.89 11.15 8.98 13.86 9.76 -\nmulti 5.63 3.56 4.06 3.09 2.01 1.38 1.71 1.06 3.82 4.38 6.50 0.42 -\nXLM-R\nInd.\nall 8.19 4.70 4.42 6.50 5.26 4.63 2.47 3.09 5.11 8.52 6.28 2.71 -\nsingle 15.21 11.29 10.95 13.37 14.41 11.85 12.34 4.04 16.71 14.22 27.33 19.47 -\nmulti 3.32 2.34 2.58 3.29 3.77 4.49 2.18 2.49 2.61 5.12 2.94 1.07 -\nConf.\nall 4.43 2.90 2.67 4.33 5.53 5.30 2.99 2.95 5.64 9.51 7.25 3.36 -\nsingle 5.19 4.38 3.57 4.93 14.15 11.79 11.42 3.93 15.88 12.56 25.60 18.85 -\nmulti 3.86 2.33 2.70 4.17 4.12 5.17 2.73 2.43 3.44 6.97 4.29 1.97 -\nSpeciﬁc\nInd.\nall 17.92 10.36 9.84 10.94 6.77 5.47 - 3.36 - - 3.00 - -\nsingle 31.21 20.30 19.22 19.07 9.64 3.55 - 5.88 - - 5.53 - -\nmulti 5.88 4.88 3.40 6.10 5.50 5.18 - 2.29 - - 0.92 - -\nConf.\nall 10.53 6.20 5.18 6.07 6.80 10.07 - 3.13 - - 2.49 - -\nsingle 19.01 15.50 8.21 5.22 9.22 3.04 - 5.56 - - 4.08 - -\nmulti 3.44 3.09 3.06 6.40 5.59 9.80 - 2.15 - - 1.35 - -\nModel Decoding Part ja hu bn ceb war tl sw pa mg ilo\nM-BERT\nInd.\nall 0.85 2.54 1.33 3.93 2.29 5.41 6.24 1.91 3.36 1.82\nsingle 7.13 8.31 2.39 7.13 4.42 10.12 10.00 4.35 4.36 3.06\nmulti 0.48 0.62 1.12 0.23 0.42 0.64 2.25 1.48 3.27 0.19\nConf.\nall 1.51 3.16 1.51 3.94 2.11 4.62 6.02 2.56 3.27 1.70\nsingle 6.50 7.85 1.52 6.30 3.73 7.80 8.42 3.80 3.40 2.41\nmulti 1.21 1.68 1.34 0.64 0.69 1.25 3.60 2.30 3.52 0.24\nXLM\nInd.\nall 5.77 1.56 0.10 5.39 3.29 4.36 5.90 - - 0.13\nsingle 24.95 6.71 1.13 6.98 5.35 7.35 8.60 - - 0.43\nmulti 3.04 0.60 0.00 2.15 1.83 1.36 2.18 - - 0.00\nConf.\nall 5.95 1.87 0.06 4.67 1.57 2.25 4.19 - - 0.04\nsingle 18.60 5.49 0.81 4.88 2.17 3.53 5.90 - - 0.07\nmulti 4.24 1.34 0.00 2.11 1.08 1.11 2.28 - - 0.00\nXLM-R\nInd.\nall 2.30 0.86 0.07 1.35 1.15 2.80 3.66 0.23 1.94 0.11\nsingle 9.23 2.22 0.00 1.73 1.32 5.05 5.57 5.75 3.70 0.39\nmulti 2.07 0.24 0.07 1.03 1.08 1.42 1.91 0.00 1.61 0.02\nConf.\nall 4.41 0.86 0.09 1.22 1.14 2.33 2.86 0.58 1.76 0.51\nsingle 8.82 2.02 0.00 1.39 1.29 4.25 4.34 5.75 3.49 0.39\nmulti 4.21 0.31 0.10 0.99 1.07 1.28 1.85 0.36 1.45 0.52\nTable 10: Accuracy on different languages using different LMs (%). We use M = 5mask tokens for en, fr, nl es,\nvi (on the left) and M = 10mask tokens for the other languages on the right. Best results for each language-part\ncombination are in bold. “-” denotes missing/unsupported models.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8712985515594482
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.8406639099121094
    },
    {
      "name": "Natural language processing",
      "score": 0.6638645529747009
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5429432988166809
    },
    {
      "name": "Code (set theory)",
      "score": 0.5411182641983032
    },
    {
      "name": "Task (project management)",
      "score": 0.5134495496749878
    },
    {
      "name": "Decoding methods",
      "score": 0.49048611521720886
    },
    {
      "name": "Word (group theory)",
      "score": 0.46227818727493286
    },
    {
      "name": "Security token",
      "score": 0.4321771264076233
    },
    {
      "name": "Linguistics",
      "score": 0.1997506320476532
    },
    {
      "name": "Programming language",
      "score": 0.1672413945198059
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I162714631",
      "name": "George Mason University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210120115",
      "name": "Robert Bosch (United States)",
      "country": "US"
    }
  ],
  "cited_by": 80
}