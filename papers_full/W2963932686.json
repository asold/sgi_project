{
  "title": "Strategies for Training Large Vocabulary Neural Language Models",
  "url": "https://openalex.org/W2963932686",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A2098653986",
      "name": "Wen-lin Chen",
      "affiliations": [
        "Menlo School",
        "Meta (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2959607770",
      "name": "David Grangier",
      "affiliations": [
        "Meta (United States)",
        "Menlo School"
      ]
    },
    {
      "id": "https://openalex.org/A2139710560",
      "name": "Michael Auli",
      "affiliations": [
        "Menlo School",
        "Meta (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1614298861",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2251682575",
    "https://openalex.org/W2185726469",
    "https://openalex.org/W1520465330",
    "https://openalex.org/W1573488949",
    "https://openalex.org/W2100664567",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W2152790380",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2180952760",
    "https://openalex.org/W932413789",
    "https://openalex.org/W2134800885",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W1499253590",
    "https://openalex.org/W2120861206",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2250379827",
    "https://openalex.org/W2251098065",
    "https://openalex.org/W1575384945",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2100714283",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2152808281",
    "https://openalex.org/W2295800168",
    "https://openalex.org/W2914484425",
    "https://openalex.org/W1518951372",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W36903255",
    "https://openalex.org/W2131462252",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2963084471",
    "https://openalex.org/W2250539671"
  ],
  "abstract": "Training neural network language models over large vocabularies is computationally costly compared to count-based models such as Kneser-Ney.We present a systematic comparison of neural strategies to represent and train large vocabularies, including softmax, hierarchical softmax, target sampling, noise contrastive estimation and self normalization.We extend self normalization to be a proper estimator of likelihood and introduce an efficient variant of softmax.We evaluate each method on three popular benchmarks, examining performance on rare words, the speed/accuracy trade-off and complementarity to Kneser-Ney.",
  "full_text": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1975–1985,\nBerlin, Germany, August 7-12, 2016.c⃝2016 Association for Computational Linguistics\nStrategies for Training Large Vocabulary Neural Language Models\nWenlin Chen David Grangier Michael Auli\nFacebook, Menlo Park, CA\nAbstract\nTraining neural network language mod-\nels over large vocabularies is computa-\ntionally costly compared to count-based\nmodels such as Kneser-Ney. We present\na systematic comparison of neural strate-\ngies to represent and train large vocabular-\nies, including softmax, hierarchical soft-\nmax, target sampling, noise contrastive es-\ntimation and self normalization. We ex-\ntend self normalization to be a proper esti-\nmator of likelihood and introduce an efﬁ-\ncient variant of softmax. We evaluate each\nmethod on three popular benchmarks, ex-\namining performance on rare words, the\nspeed/accuracy trade-off and complemen-\ntarity to Kneser-Ney.\n1 Introduction\nNeural network language models (Bengio et al.,\n2003; Mikolov et al., 2010) have gained popular-\nity for tasks such as automatic speech recognition\n(Arisoy et al., 2012) and statistical machine trans-\nlation (Schwenk et al., 2012; Vaswani et al., 2013;\nBaltescu and Blunsom, 2014). Similar models are\nalso developed for translation (Le et al., 2012; De-\nvlin et al., 2014; Bahdanau et al., 2015), summa-\nrization (Chopra et al., 2015) and language gener-\nation (Sordoni et al., 2015).\nLanguage models assign a probability to a word\ngiven a context of preceding, and possibly sub-\nsequent, words. The model architecture deter-\nmines how the context is represented and there\nare several choices including recurrent neural net-\nworks (Mikolov et al., 2010; Jozefowicz et al.,\n2016), or log-bilinear models (Mnih and Hinton,\n2010). This paper does not focus on architec-\nture or context representation but rather on how to\nefﬁciently deal with large output vocabularies, a\nproblem common to all approaches to neural lan-\nguage modeling and related tasks (machine trans-\nlation, language generation). We therefore experi-\nment with a classical feed-forward neural network\nmodel similar to Bengio et al. (2003).\nPractical training speed for these models quickly\ndecreases as the vocabulary grows. This is due\nto three combined factors: (i) model evaluation\nand gradient computation become more time con-\nsuming, mainly due to the need of computing nor-\nmalized probabilities over a large vocabulary; (ii)\nlarge vocabularies require more training data in\norder to observe enough instances of infrequent\nwords which increases training times; (iii) a larger\ntraining set often allows for larger models which\nrequires more training iterations.\nThis paper provides an overview of popular\nstrategies to model large vocabularies for language\nmodeling. This includes the classical softmax over\nall output classes, hierarchical softmaxwhich in-\ntroduces latent variables, or clusters, to simplify\nnormalization, target sampling which only con-\nsiders a random subset of classes for normaliza-\ntion, noise contrastive estimationwhich discrim-\ninates between genuine data points and samples\nfrom a noise distribution, and infrequent normal-\nization, also referred as self-normalization, which\ncomputes the partition function at an infrequent\nrate. We also extend self-normalization to be a\nproper estimator of likelihood. Furthermore, we\nintroduce differentiated softmax, a novel variation\nof softmax which assigns more parameters, or ca-\npacity, to frequent words and which we show to be\nfaster and more accurate than softmax (§2).\nOur comparison assumes a reasonable budget of\none week for training models on a high end GPU\n(Nvidia K40). We evaluate on three benchmarks\ndiffering in the amount of training data and vocab-\nulary size, that is Penn Treebank, Gigaword and\nthe Billion Word benchmark (§3).\nOur results show that conclusions drawn from\nsmall datasets do not always generalize to larger\nsettings. For instance, hierarchical softmax is less\naccurate than softmax on the small vocabulary\nPenn Treebank task but performs best on the very\nlarge vocabulary Billion Word benchmark. This is\nbecause hierarchical softmax is the fastest method\nfor training and can perform more training updates\nin the same period of time. Furthermore, our re-\n1975\nsults with differentiated softmax demonstrate that\nassigning capacity where it has the most impact\nallows to train better models in our time budget\n(§4). Our analysis also shows clearly that tradi-\ntional Kneser-Ney models are competitive on rare\nwords, contrary to the common belief that neural\nmodels are better on infrequent words (§5).\n2 Modeling Large Vocabularies\nWe ﬁrst introduce our model architecture with a\nclassical softmax and then describe various other\nmethods including a novel variation of softmax.\n2.1 Softmax Neural Language Model\nOur feed-forward neural network implements an\nn-gram language model, i.e., it is a parametric\nfunction estimating the probability of the next\nword wt given n −1 previous context words,\nwt−1,...,w t−n+1. Formally, we take as input a\nsequence of discrete indexes representing then−1\nprevious words and output a vocabulary-sized vec-\ntor of probability estimates, i.e.,\nf : {1,...,V }n−1 →[0,1]V,\nwhere V is the vocabulary size. This function re-\nsults from the composition of simple differentiable\nfunctions or layers.\nSpeciﬁcally, f composes an input mapping from\ndiscrete word indexes to continuous vectors, a suc-\ncession of linear operations followed by hyper-\nbolic tangent non-linearities, plus one ﬁnal linear\noperation, followed by a softmax normalization.\nThe input layer maps each context word index to\na continuous d′\n0-dimensional vector. It relies on a\nmatrix W0 ∈RV×d′\n0 to convert the input\nx= [wt−1,...,w t−n+1] ∈{1,...,V }n−1\nto n−1 vectors of dimension d′\n0. These vectors\nare concatenated into a single (n−1) ×d′\n0 matrix,\nh0 = [W0\nwt−1; ... ; W0\nwt−n+1] ∈Rn−1×d′\n0.\nThis state h0 is considered as a d0 = (n−1) ×\nd′\n0 vector by the next layer. The subsequent states\nare computed through klayers of linear mappings\nfollowed by hyperbolic tangents, i.e.\n∀i= 1,...,k, h i = tanh(Wihi−1 + bi) ∈Rdi\nwhere Wi ∈ Rdi×di−1,b ∈ Rdi are learn-\nable weights and biases and tanh denotes the\ncomponent-wise hyperbolic tangent.\nFinally, the last layer performs a linear operation\nfollowed by a softmax normalization, i.e.,\nhk+1 = Wk+1hk + bk+1 ∈RV\nand y= 1\nZ exp(hk+1) ∈[0,1]V (1)\nwhere Z = ∑V\nj=1 exp(hk+1\nj ) and exp denotes the\ncomponent-wise exponential. The network output\ny is therefore a vocabulary-sized vector of proba-\nbility estimates. We use the standard cross-entropy\nloss with respect to the computed log probabilities\n∂log yi\n∂hk+1\nj\n= δij −yj\nwhere δij = 1 if i = j and 0 otherwise The gra-\ndient update therefore increases the score of the\ncorrect output hk+1\ni and decreases the score of all\nother outputs hk+1\nj for j ̸= i.\nA downside of the classical softmax formulation\nis that it requires computation of the activations for\nall output words, Eq. (1). The output layer with\nV activations is much larger than any other layer\nin the network and its matrix multiplication domi-\nnates the complexity of the entire network.\n2.2 Hierarchical Softmax\nHierarchical Softmax (HSM) organizes the out-\nput vocabulary into a tree where the leaves are\nthe words and the intermediate nodes are latent\nvariables, or classes (Morin and Bengio, 2005).\nThe tree has potentially many levels and there is a\nunique path from the root to each word. The prob-\nability of a word is the product of the probabilities\nof the latent variables along the path from the root\nto the leaf, including the probability of the leaf.\nWe follow Goodman (2001) and Mikolov et al.\n(2011b) and model a two-level tree. Given context\nx, HSM predicts the class of the next word ct and\nthe actual word wt\np(wt|x) =p(ct|x) p(wt|ct,x) (2)\nIf the number of classes is O(\n√\nV) and classes are\nbalanced, then we only need to computeO(2\n√\nV)\noutputs. In practice, this strategy results in weight\nmatrices whose largest dimension is < 1,000, a\nsetting for which GPU hardware is fast.\nA popular strategy is frequency clustering. It\nsorts the vocabulary by frequency and then forms\nclusters of words with similar frequency. Each\ncluster contains an equal share of the total unigram\nprobability. We compare this strategy to random\nclass assignment and to clustering based on word\n1976\nW\nk +1\nh\nk\nd A\nd B\nd C\n| A |\n| B |\n| C |\nd A\nd B\nd C\nFigure 1: Output weight matrix Wk+1 and hid-\nden layer hk for differentiated softmax for vocab-\nulary partitions A,B,C with embedding dimen-\nsions dA,dB,dC; non-shaded areas are zero.\ncontexts, relying on PCA (Lebret and Collobert,\n2014). A full comparison of context-based clus-\ntering is beyond the scope of this work (Brown et\nal., 1992; Mikolov et al., 2013).\n2.3 Differentiated Softmax\nThis section introduces a novel variation of soft-\nmax that assigns a variable number of parameters\nto each word in the output layer. The weight ma-\ntrix of the ﬁnal layer Wk+1 ∈Rdk×V stores out-\nput embeddings of size dk for the V words the\nlanguage model may predict: Wk+1\n1 ; ... ; Wk+1\nV .\nDifferentiated softmax (D-Softmax) varies the di-\nmension of the output embeddings dk across\nwords depending on how much model capacity, or\nparameters, are deemed suitable for a given word.\nWe assign more parameters to frequent words than\nto rare words since more training occurrences al-\nlow for ﬁtting more parameters.\nWe partition the output vocabulary based on\nword frequency and the words in each partition\nshare the same embedding size. Partitioning the\nvocabulary in this way results in a sparse ﬁnal\nweight matrix Wk+1 which arranges the embed-\ndings of the output words in blocks, each block\ncorresponding to a separate partition (Figure 1).\nThe size of the ﬁnal hidden layer hk is the sum\nof the embedding sizes of the partitions. The ﬁ-\nnal hidden layer is effectively a concatenation of\nseparate features for each partition which are used\nto compute the dot product with the correspond-\ning embedding type in Wk+1. In practice, we efﬁ-\nciently compute separate matrix-vector products,\nor in batched form, matrix-matrix products, for\neach partition in Wk+1 and hk.\nOverall, differentiated softmax can lead to large\nspeed-ups as well as accuracy gains since we\ncan greatly reduce the complexity of computing\nthe output layer. Most signiﬁcantly, this strategy\nspeeds up both training and inference. This is\nin contrast to hierarchical softmax which is fast\nduring training but requires even more effort than\nsoftmax for computing the most likely next word.\n2.4 Target Sampling\nSampling-based methods approximate the soft-\nmax normalization, Eq. (1), by summing over a\nsub-sample of impostor classes. This can signif-\nicantly speed-up each training iteration, depend-\ning on the size of the impostor set. Target sam-\npling builds upon the importance sampling work\nof Bengio and Sen ´ecal (2008). We follow Jean et\nal. (2014) who choose as impostors all positive\nexamples in a mini-batch as well as a subset of\nthe remaining words. This subset is sampled uni-\nformly and its size is chosen by validation.\n2.5 Noise Contrastive Estimation\nNoise contrastive estimation (NCE) is another\nsampling-based technique (Hyv ¨arinen, 2010;\nMnih and Teh, 2012; Chen et al., 2015). Contrary\nto target sampling, it does not maximize the train-\ning data likelihood directly. Instead, it solves a\ntwo-class problem of distinguishing genuine data\nfrom noise samples. The training algorithm sam-\nples a word wgiven the preceding context xfrom\na mixture\np(w|x) = 1\nk+ 1ptrain(w|x) + k\nk+ 1pnoise(w|x)\nwhere ptrain is the empirical distribution of the\ntraining set and pnoise is a known noise distri-\nbution which is typically a context-independent\nunigram distribution. The training algorithm ﬁts\nthe model ˆp(w|x) to recover whether a mixture\nsample came from the data or the noise distribu-\ntion, this amounts to minimizing the binary cross-\nentropy −y log ˆp(y= 1|w,x)−(1−y) log ˆp(y=\n0|w,x) where y is a binary variable indicating\nwhere the current sample originates from\n{\nˆp(y= 1|w,x) = ˆp(w|x)\nˆp(w|x)+kpnoise(w|x) (data)\nˆp(y= 0|w,x) = 1−ˆp(y= 1|w,x) (noise).\nThis formulation still involves a softmax over the\nvocabulary to compute ˆp(w|x). However, Mnih\nand Teh (2012) suggest to forego normalization\nand replace ˆp(w|x) with unnormalized exponen-\ntiated scores. This makes the training complex-\nity independent of the vocabulary size. At test\ntime, softmax normalization is reintroduced to get\na proper distribution. We also follow Mnih and\nTeh (2012) recommendations for pnoise and rely\non a unigram distribution of the training set.\n1977\n2.6 Infrequent Normalization\nDevlin et al. (2014), followed by Andreas\nand Klein (2015), proposed to relax score nor-\nmalization. Their strategy (here referred to\nas WeaknormSQ) associates unnormalized likeli-\nhood maximization with a penalty term that favors\nnormalized predictions. This yields the following\nloss over the training set T\nL(2)\nα = −\n∑\n(w,x)∈T\ns(w|x) +α\n∑\n(w,x)∈T\n(log Z(x))2\nwhere s(w|x) refers to the unnormalized score\nof word w given context x and Z(x) =∑\nwexp(s(w|x)) refers to the partition function\nfor context x. This strategy therefore pushes the\nlog partition towards zero. For efﬁcient training,\nthe second term can be down-sampled\nL(2)\nα,γ = −\n∑\n(w,x)∈T\ns(w|x)+ α\nγ\n∑\n(w,x)∈Tγ\n(log Z(x))2\nwhere Tγ is the training set sampled at rate γ. A\nsmall rate implies computing the partition function\nonly for a small fraction of the training data.\nWe extend this strategy to the case where the log\npartition term is not squared (Weaknorm), i.e.,\nL(1)\nα,γ = −\n∑\n(w,x)∈T\ns(w|x) +α\nγ\n∑\n(w,x)∈Tγ\nlog Z(x)\nFor α= 1, this loss is an unbiased estimator of the\nnegative log-likelihood of the training dataL(2)\n1 =\n−∑\n(w,x)∈T s(w|x) + logZ(x).\n3 Experimental Setup\nDatasets We run experiments over three news\ndatasets of different sizes: Penn Treebank (PTB),\nWMT11-lm (billionW) and English Gigaword,\nversion 5 (gigaword). Penn Treebank (Marcus et\nal., 1993) is the smallest corpus with 1M tokens\nand we use a vocabulary size of 10k (Mikolov et\nal., 2011a). The billion word benchmark (Chelba\net al., 2013) comprises almost one billion tokens\nand a vocabulary of about 800k words 1. Giga-\nword (Parker et al., 2011) is even larger with 5 bil-\nlion tokens and was previously used for language\nmodeling (Heaﬁeld, 2011) but there is no standard\ntrain/test split or vocabulary for this set. We split\naccording to time: training covers 1994–2009 and\ntest covers 2010. The vocabulary comprises the\n100k most frequent words in train. Table 1 sum-\nmarizes the data statistics.\n1T. Robinson versionhttp://tiny.cc/1billionLM .\nDataset Train Test V ocab OOV\nPTB 1M 0.08M 10k 5.8%\ngigaword 4,631M 279M 100k 5.6%\nbillionW 799M 8.1M 793k 0.3%\nTable 1: Dataset statistics. Number of tokens for\ntrain and test, vocabulary size, fraction of OOV .\nEvaluation We measure perplexity on the test set.\nFor PTB and billionW, we report results on a per\nsentence basis, i.e., models do not use context\nwords across sentence boundaries and we score\nend-of-sentence markers. This is the standard set-\nting for these benchmarks and allows comparison\nwith other work. On gigaword, we use contexts\nacross sentence boundaries and evaluation does\nnot include end-of-sentence markers.\nOur baseline is an interpolated Kneser-Ney (KN)\nmodel. We use KenLM (Heaﬁeld, 2011) to train\n5-gram models without pruning. For neural mod-\nels, we train 11-gram models for gigaword and bil-\nlionW; for PTB we train a 6-gram model. The\nmodel parameters (weights Wi and biases bi for\ni = 0,...,k + 1) are learned to maximize the\ntraining log-likelihood relying on stochastic gra-\ndient descent (SGD; LeCun et al.. 1998).\nValidation Hyper-parameters are the number of\nlayers kand the dimension of each layer di,∀i =\n0,...,k . We tune the following settings for each\ntechnique on the validation set: the number of\nclusters, the clustering technique for hierarchi-\ncal softmax, the number of frequency bands and\ntheir allocated capacity for differentiated softmax,\nthe number of distractors for target sampling, the\nnoise/data ratio for NCE, as well as the regular-\nization rate and strength for infrequent normaliza-\ntion. Similarly, SGD parameters (learning rate and\nmini-batch size) are set to maximize validation\nlikelihood. We also tune the dropout rate (Srivas-\ntava et al., 2014); dropout is employed after each\ntanh non-linearity.2\nTraining TimeWe train for 168 hours (one week)\non the large datasets (billionW, gigaword) and 24\nhours (one day) for Penn Treebank. All exper-\niments are performed on the same hardware, a\nsingle K40 GPU. We select the hyper-parameters\nwhich yield the best validation perplexity after the\nallocated time and report the perplexity of the re-\nsulting model on the test set. This training time\n2More parameter settings are available\nin an extended version of the paper at\nhttp://arxiv.org/abs/1512.04906.\n1978\nis a trade-off between being able to do a compre-\nhensive exploration of the various settings for each\nmethod and good accuracy. The chosen training\ntimes are not long enough to observe over-ﬁtting,\ni.e. validation performance is still improving – al-\nbeit very slowly – at the end of the training session.\nAs a general observation, even on the small PTB\nwhere 24 hours is rather long, we always found\nbetter results using the full training time, possibly\nincreasing the dropout rate.\nA concern may be that a ﬁxing the training time\nfavors models with better implementations. How-\never, all models are very similar and their core\ncomputations are always matrix/matrix products.\nTraining differs mostly in the size and frequency\nof large matrix/matrix products. Matrix products\nrely on CuBLAS 3, using torch 4. For the matrix\nsizes involved (>500×1,000), the time complex-\nity of matrix product is linear in each dimension,\nboth on CPU (Intel MKL 5) and GPU (CuBLAS),\nwith a 10X speedup for GPU (Nvidia K40) com-\npared to CPU (Intel Xeon E5-2680). Therefore,\nthe speed trade-off applies to both CPU and GPU\nhardware, albeit with a different time scale.\n4 Results\nThe test perplexities (Table 2) and validation\nlearning curves (Figures 2, 3, and 4) show that the\ncompetitiveness of softmax diminishes with larger\nvocabularies. Softmax does well on the small vo-\ncabulary PTB but poorly on the large vocabulary\nbillionW corpus. Faster methods such as sam-\npling, hierarchical softmax, and infrequent nor-\nmalization (Weaknorm, WeaknormSQ) are much\nbetter in the large vocabulary setting of billionW.\nD-Softmax is performing well on all sets and\nshows that assigning higher capacity where it ben-\neﬁts most results in better models. Target sam-\npling performs worse than softmax on gigaword\nbut better on billionW. Hierarchical softmax per-\nforms poorly on Penn Treebank which is in stark\ncontrast to billionW where it does well. Noise\ncontrastive estimation has good accuracy on bil-\nlionW, where speed is essential to achieving good\naccuracy.\nOf all the methods, hierarchical softmax pro-\ncesses most training examples in a given time\nframe (Table 3). Our test time speed compari-\nson assumes that we would like to ﬁnd the highest\n3http://docs.nvidia.com/cuda/cublas/\n4http://torch.ch\n5https://software.intel.com/en-us/intel-mkl\nPTB gigaW billionW\nKN 141.2 57.1 70.2 6\nSoftmax 123.8 56.5 108.3\nD-Softmax 121.1 52.0 91.2\nSampling 124.2 57.6 101.0\nHSM 138.2 57.1 85.2\nNCE 143.1 78.4 104.7\nWeaknorm 124.4 56.9 98.7\nWeaknormSQ 122.1 56.1 94.9\nKN+Softmax 108.5 43.6 59.4\nKN+D-Softmax 107.0 42.0 56.3\nKN+Sampling 109.4 43.8 58.1\nKN+HSM 115.0 43.9 55.6\nKN+NCE 114.6 49.0 58.8\nKN+Weaknorm 109.2 43.8 58.1\nKN+WeaknormSQ 108.8 43.8 57.7\nTable 2: Test perplexity of individual models and\ninterpolation with Kneser-Ney.\n 120 130 140 150 160 170 180 190\n 0 5 10 15 20\nPerplexityTraining time (hours)\nSoftmaxSamplingHSMD-SoftmaxWeaknormWeaknormSQNCE\nFigure 2: PTB validation learning curve.\nscoring next word rather than rescoring an exist-\ning string. This scenario requires scoring all out-\nput words and D-Softmax can process nearly twice\nas many tokens per second than the other methods\nwhose complexity is similar to softmax.\n4.1 Softmax\nDespite being our baseline, softmax ranks among\nthe most accurate methods on PTB and it is sec-\nond best on gigaword after D-Softmax (with Wea-\nknormSQ performing similarly). For billionW,\nthe extremely large vocabulary makes softmax\ntraining too slow to compete with faster alterna-\n6This perplexity is higher than reported in (Chelba et al.,\n2013), in which Kneser Ney is not trained on the 800m token\ntraining set, but on a larger corpus of 1.1B tokens.\n1979\n 50 60 70 80 90 100 110\n 0 20 40 60 80 100 120 140 160 180\nPerplexityTraining time (hours)\nSoftmaxSamplingHSMD-SoftmaxWeaknormWeaknormSQNCE\nFigure 3: Gigaword validation learning curve.\n 80 100 120 140 160 180\n 0 20 40 60 80 100 120 140 160 180\nPerplexityTraining time (hours)\nSoftmaxSamplingHSMD-SoftmaxWeaknormWeaknormSQNCE\nFigure 4: Billion Word validation learning curve.\ntrain test\nSoftmax 510 510\nD-Softmax 960 960\nSampling 1,060 510\nHSM 12,650 510\nNCE 4,520 510\nWeaknorm 1,680 510\nWeaknormSQ 2,870 510\nTable 3: Training and test speed on billionW in to-\nkens per second for generation of the next word.\nMost techniques are identical to softmax at test\ntime. HSM can be faster for rescoring.\n 50 60 70 80 90 100 110 120 0 10 20 30 40 50 60 70 80 90 100\nPerplexityDistractors per Sample (% of vocabulary)Sampling\nFigure 5: Number of Distractors versus Perplexity\nfor Target Sampling over Gigaword\ntives. However, of all the methods softmax has the\nsimplest implementation and it has no additional\nhyper-parameters compared to other methods.\n4.2 Target Sampling\nFigure 5 shows that target sampling is most accu-\nrate for distractor sets that amount to a large frac-\ntion of the vocabulary, i.e. > 30% on gigaword\n(billionW best setting >50% is even higher). Tar-\nget sampling is faster and performs more itera-\ntions than softmax in the same time. However, its\nperplexity reduction per iteration is less than soft-\nmax. Overall, it is not much better than softmax.\nA reason might be that sampling chooses distrac-\ntors independently from context and current model\nperformance. This does not favor distractors the\nmodel incorrectly considers likely for the current\ncontext. These distractors would yield higher gra-\ndients that could update the model faster.\n4.3 Hierarchical Softmax\nHierarchical softmax is very efﬁcient for large vo-\ncabularies and it is the best method on billionW.\nOn the other hand, HSM does poorly on small vo-\ncabularies as seen on PTB. We found that a good\nword clustering structure is crucial: when clusters\ngather words occurring in similar contexts, clus-\nter likelihoods are easier to learn; when the cluster\nstructure is uninformative, cluster likelihoods con-\nverge to the uniform distribution. This affects ac-\ncuracy since words cannot have higher probability\nthan their clusters, Eq. (2).\nOur experiments organize words into a two\nlevel hierarchy and compare four clustering strate-\ngies on billionW and gigaword ( §2.2). Random\nclustering shufﬂes the vocabulary and splits it\ninto equally sized partitions. Frequency-based\nclustering ﬁrst orders words based on their fre-\nquency and assigns words to clusters such that\neach cluster represents an equal share of the\ntotal frequency (Mikolov et al., 2011b). K-\nmeans runs the well-known clustering algorithm\non Hellinger PCA word embeddings. Weighted k-\nmeans weights each word by its frequency.7\nRandom clusters perform worst (Table 4) fol-\nlowed by frequency-based clustering but k-means\ndoes best; weighted k-means performs similarly\nto its unweighted version. In earlier experiments,\nplain k-means performed very poorly since the\nmost frequent cluster captured up to 40% of the\n7The time to compute the clustering (multi-threaded word\nco-occurrence counts, PCA and k-means) is under one hour,\nwhich is negligible given a one week training budget.\n1980\nbillionW gigaword\nrandom 98.51 62,27\nfrequency-based 92.02 59.47\nk-means 85.70 57.52\nweighted k-means 85.24 57.09\nTable 4: HSM with different clustering.\ntoken occurrences. We then explicitly capped the\nfrequency budget of each cluster to 10% which\nbrought k-means on par with weighted k-means.\n4.4 Differentiated Softmax\nD-Softmax is the best technique on gigaword and\nsecond best on billionW after HSM. On PTB\nit ranks among the best techniques whose per-\nplexities cannot be reliably distinguished. The\nvariable-capacity scheme of D-Softmax can as-\nsign large embeddings to frequent words, while\nkeeping computational complexity manageable\nthrough small embeddings for rare words.\nUnlike for hierarchical softmax, NCE or Wea-\nknorm, the computational advantage of D-\nSoftmax is preserved at test time (Table 3). D-\nSoftmax is the fastest technique at test time, while\nranking among the most accurate methods. This\nspeed advantage is due to the low dimensional rep-\nresentation of rare words which negatively affects\nthe model accuracy on these words (Table 5).\n4.5 Noise Contrastive Estimation\nAlthough we report better perplexities than the\noriginal NCE paper on PTB (Mnih and Teh, 2012),\nwe found NCE difﬁcult to use for large vocabular-\nies. In order to work in this setting where mod-\nels are larger, we had to dissociate the number of\nnoise samples from the data to noise ratio in the\nmodeled mixture. For instance, a data/noise ra-\ntio of 1/50 gives good performance in our exper-\niments but estimating only 50 noise sample pos-\nteriors per data point is wasteful given the cost of\nnetwork evaluation. Moreover, 50 samples do not\nallow frequent sampling of every word in a large\nvocabulary. Our setting considers more noise sam-\nples and up-weights the data sample. This allows\nto set the data/noise ratio independently from the\nnumber of noise samples.\nOverall, NCE results are better than softmax\nonly for billionW, a setting for which softmax is\nvery slow due to the very large vocabulary. Why\ndoes NCE perform so poorly? Figure 6 shows en-\ntropy on the validation set versus the NCE loss for\nseveral models. The results clearly show that sim-\n 4\n 5\n 6\n 7\n 8\n 9\n 10\n 0.054  0.056  0.058  0.06  0.062  0.064\nEntropy\nNCE Loss\nFigure 6: Validation entropy versus NCE loss on\ngigaword for experiments differing only in learn-\ning rates and initial weights. Each color corre-\nsponds to one experiment, with one point per hour.\nilar NCE loss values can result in very different\nvalidation entropy. Although NCE might make\nsense for other metrics such as BLEU (Baltescu\nand Blunsom, 2014), it is not among the best tech-\nniques for minimizing perplexity. Jozefowicz et\nal. (2016) recently drew similar conclusions.\n4.6 Infrequent Normalization\nInfrequent normalization (Weaknorm and Wea-\nknormSQ) performs better than softmax on bil-\nlionW and comparably to softmax on Penn Tree-\nbank and gigaword (Table 2). The speedup from\nskipping partition function computations is sub-\nstantial. For instance, WeaknormSQ on billionW\nevaluates the partition only on 10% of the exam-\nples. In one week, the model is evaluated and up-\ndated on 868M tokens (with 86.8M partition eval-\nuations) compared to 156M tokens for softmax.\nAlthough referred to as self-normalizing (An-\ndreas and Klein, 2015), the trained models still\nneed normalization after training. The partition\nvaries greatly between data samples. On billionW,\nthe partition ranges between 9.4 to 10.3 in log\nscale for 10th to 90th percentile, i.e. a ratio of 2.5.\nWe observed the squared version (Wea-\nknormSQ) to be unstable at times. Regularization\nstrength could be found too low (collapse) or\ntoo high (blow-up) after a few days of training.\nWe added an extra unit to bound unnormalized\npredictions x → 10 tanh(x/5), which yields\nstable training and better generalization. For\nthe non-squared Weaknorm, stability was not an\nissue. A regularization strength of 1 was the best\nsetting for Weaknorm. This choice makes the loss\nan unbiased estimator of the data likelihood.\n1981\n1-4K 4-20K 20-40K 40-70K 70-100K\nKneser-Ney 3.48 7.85 9.76 10.76 11.57\nSoftmax 3.46 7.87 9.76 11.09 12.39\nD-Softmax 3.35 7.79 10.13 12.22 12.69\nTarget sampling 3.51 7.62 9.51 10.81 12.06\nHSM 3.49 7.86 9.38 10.30 11.24\nNCE 3.74 8.48 10.60 12.06 13.37\nWeaknorm 3.46 7.86 9.77 11.12 12.40\nWeaknormSQ 3.46 7.79 9.67 10.98 12.32\nTable 5: Test entropy on gigaword over subsets of the frequency ranked vocabulary; rank 1 is the most\nfrequent word.\n5 Analysis\n5.1 Model Capacity\nTraining neural language models over large cor-\npora highlights that training time, not training\ndata, is the main factor limiting performance. The\nlearning curves on gigaword and billionW indicate\nthat most models are still making progress after\none week. Training time has therefore to be taken\ninto account when considering increasing capac-\nity. Figure 7 shows validation perplexity versus\nthe number of iterations for a week of training.\nThis ﬁgure shows that a softmax model with 1024\nhidden units in the last layer could perform bet-\nter than the 512-hidden unit model with a longer\ntraining horizon. However, in the allocated time,\n512 hidden units yield the best validation perfor-\nmance. D-softmax shows that it is possible to se-\nlectively increase capacity, i.e., to allocate more\nhidden units to the most frequent words at the ex-\npense of rarer words. This captures most of the\nbeneﬁt of a larger softmax model while staying\nwithin a reasonable training budget.\n5.2 Effect of Initialization\nWe consider initializing both the input word em-\nbeddings and the output matrix from Hellinger\nPCA embeddings. Several alternative tech-\nniques for pre-training embeddings have been pro-\nposed (Mikolov et al., 2013; Lebret and Collobert,\n2014; Pennington et al., 2014). Our experiment\nhighlights the advantage of initialization and do\nnot aim to compare embedding techniques.\nFigure 8 shows that PCA is better than random\nfor initializing both input and output word rep-\nresentations; initializing both from PCA is even\nbetter. We see that even after long training ses-\nsions, the initial conditions still impact the valida-\ntion perplexity. We observed this trend also with\n 80 100 120 140 160 180 200\n 0 50 100 150 200 250 300\nPerplexityTraining tokens (millions)\nD-Softmax 1024x50K, 512x100K, 64x640KD-Softmax 1024x50K, 256x740KSoftmax 1024Softmax 512\nFigure 7: Validation perplexity per iteration on\nbillionW for softmax and D-softmax. Softmax\nuses the same number of units for all words. The\nﬁrst D-Softmax experiment uses 1024 units for the\n50K most frequent words, 512 for the next 100K,\nand 64 units for the rest; similarly for the second\nexperiment. All experiments end after one week.\nother strategies than softmax. After one week of\ntraining, HSM is the only method which can reach\ncomparable accuracy to PCA initialization when\nthe output matrix is randomly initialized.\n5.3 Training Set Size\nLarge training sets and a ﬁxed training time in-\ntroduce competition between slower models with\nmore capacity and observing more training data.\nThis trade-off only applies to iterative SGD op-\ntimization and does not apply to classical count-\nbased models, which visit the training set once and\nthen solve training in closed form.\nWe compare Kneser-Ney and softmax, trained\nfor one week, with gigaword on differently sized\nsubsets of the training data. For each setting we\ntake care to include all data from the smaller sub-\nsets. Figure 9 shows that the performance of the\nneural model improves very little on more than\n1982\n 40 60 80 100 120 140 160 180 200\n 0 20 40 60 80 100 120 140 160 180\nPerplexityTraining time (hours)\nInput: PCA, Output: PCAInput: PCA, Output: RandomInput: Random, Output: PCAInput: Random, Output: Random\nFigure 8: Effect of random initialization and with\nHellinger PCA on gigaword for softmax.\n 55 60 65 70 75 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5\nPerplexityTraining data size (billions)SoftmaxKN\nFigure 9: Effect of training set size measured on\ntest of gigaword for Softmax and Kneser-Ney.\n500M tokens. In order to beneﬁt from the full\ntraining set we would require a much higher train-\ning budget, faster hardware, or parallelization.\nScaling training to large datasets can have a sig-\nniﬁcant impact on perplexity, even when data from\nthe distribution of interest is limited. As an illus-\ntration, we adapted a softmax model trained on bil-\nlionW to Penn Treebank and achieved a perplexity\nof 96 - a far better result than with any model we\ntrained from scratch on PTB (cf. Table 2).\n5.4 Rare Words\nHow well do neural models perform on rare\nwords? To answer this question, we computed\nentropy across word frequency bands for Kneser-\nNey and neural models. Table 5 reports entropy\nfor the 4,000 most frequent words, then the next\nmost frequent 16,000 words, etc. For frequent\nwords, neural models are on par or better than\nKneser-Ney. For rare words, Kneser-Ney is very\ncompetitive. Although neural models might even-\ntually close this gap with much longer training,\none should consider that Kneser-Ney trains on gi-\ngaword in only 8 hours on CPU which contrasts\nwith 168 hours of training for neural models on\nhigh end GPUs. This result highlights the comple-\nmentarity of both approaches, as observed in our\ninterpolation experiments (Table 2).\nFor neural models, D-Softmax excels on fre-\nquent words but performs poorly on rare ones.\nThis is because D-Softmax assigns more capacity\nto frequent words at the expense of rare words.\nOverall, hierarchical softmax is the best neural\ntechnique for rare words. HSM does more itera-\ntions than any other technique and so it can ob-\nserve every rare word more often.\n6 Conclusions\nThis paper presents a comprehensive analysis of\nstrategies to train neural language models with\nlarge vocabularies. This setting is very challeng-\ning for neural networks as they need to compute\nthe partition function over the entire vocabulary at\neach evaluation.\nWe compared classical softmax to hierarchical\nsoftmax, target sampling, noise contrastive esti-\nmation and infrequent normalization, commonly\nreferred to as self-normalization. Furthermore, we\nextend infrequent normalization to be a proper es-\ntimator of likelihood and we introduce differenti-\nated softmax, a novel variant of softmax assigning\nless capacity to rare words to reduce computation.\nOur results show that methods which are ef-\nfective on small vocabularies are not necessarily\nequally so on large vocabularies. In our setting,\ntarget sampling and noise contrastive estimation\nfailed to outperform the softmax baseline. Over-\nall, differentiated softmax and hierarchical soft-\nmax are the best strategies for large vocabularies.\nCompared to classical Kneser-Ney models, neural\nmodels are better at modeling frequent words, but\nare less effective for rare words. A combination of\nthe two is therefore very effective.\nWe conclude that there is a lot to explore in train-\ning from a combination of normalized and unnor-\nmalized objectives. An interesting future direc-\ntion is to combine complementary approaches, ei-\nther through combined parameterization (e.g. hi-\nerarchical softmax with differentiated capacity per\nword) or through a curriculum (e.g. transitioning\nfrom target sampling to regular softmax as training\nprogresses). Further promising areas are parallel\ntraining as well as better rare word modeling.\nReferences\nJacob Andreas and Dan Klein. 2015. When and why\nare log-linear models self-normalizing? In Proc. of\nNAACL.\n1983\nEbru Arisoy, Tara N. Sainath, Brian Kingsbury, and\nBhuvana Ramabhadran. 2012. Deep Neural Net-\nwork Language Models. In NAACL-HLT Workshop\non the Future of Language Modeling for HLT, pages\n20–28, Stroudsburg, PA, USA. Association for Com-\nputational Linguistics.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In Proc. of ICLR. As-\nsociation for Computational Linguistics, May.\nPaul Baltescu and Phil Blunsom. 2014. Pragmatic neu-\nral language modelling in machine translation. Tech-\nnical Report arXiv 1412.7119.\nYoshua Bengio and Jean-S ´ebastien Sen ´ecal. 2008.\nAdaptive importance sampling to accelerate train-\ning of a neural probabilistic language model. IEEE\nTransactions on Neural Networks.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A Neural Probabilistic Lan-\nguage Model. Journal of Machine Learning Re-\nsearch, 3:1137–1155.\nPeter F. Brown, Peter V . deSouza, Robert L. Mer-\ncer, Vincent J. Della Pietra, and Jenifer C. Lai.\n1992. Class-based n-gram models of natural lan-\nguage. Computational Linguistics, 18(4):467–479,\nDec.\nCiprian Chelba, Tom´aˇs Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. Tech-\nnical report, Google.\nXie Chen, Xunying Liu, MJF Gales, and PC Wood-\nland. 2015. Recurrent neural network language\nmodel training with noise contrastive estimation for\nspeech recognition. In Acoustics, Speech and Signal\nProcessing (ICASSP).\nSumit Chopra, Jason Weston, and Alexander M. Rush.\n2015. Tuning as ranking. In Proc. of EMNLP. Asso-\nciation for Computational Linguistics, Sep.\nJacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas\nLamar, Richard Schwartz, , and John Makhoul.\n2014. Fast and Robust Neural Network Joint Models\nfor Statistical Machine Translation. In Proc. of ACL.\nAssociation for Computational Linguistics, June.\nJoshua Goodman. 2001. Classes for Fast Maximum\nEntropy Training. In Proc. of ICASSP.\nKenneth Heaﬁeld. 2011. KenLM: Faster and Smaller\nLanguage Model Queries. In Workshop on Statistical\nMachine Translation, pages 187–197.\nMichael Gutmann Aapo Hyv ¨arinen. 2010. Noise-\ncontrastive estimation: A new estimation principle\nfor unnormalized statistical models. In Proc. of AIS-\nTATS.\nS´ebastien Jean, Kyunghyun Cho, Roland Memisevic,\nand Yoshua Bengio. 2014. On Using Very Large\nTarget V ocabulary for Neural Machine Translation.\nCoRR, abs/1412.2007.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring the lim-\nits of language modeling. Technical Report arXiv\n1602.02410.\nHai-Son Le, Alexandre Allauzen, and Franc ¸ois Yvon.\n2012. Continuous Space Translation Models with\nNeural Networks. In Proc. of HLT-NAACL, pages\n39–48, Montr´eal, Canada. Association for Computa-\ntional Linguistics.\nRemi Lebret and Ronan Collobert. 2014. Word Em-\nbeddings through Hellinger PCA. In Proc. of EACL.\nYann LeCun, Leon Bottou, Genevieve Orr, and Klaus-\nRobert Mueller. 1998. Efﬁcient BackProp. In\nGenevieve Orr and Klaus-Robert Muller, editors,\nNeural Networks: Tricks of the trade. Springer.\nMitchell P. Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a Large Anno-\ntated Corpus of English: The Penn Treebank. Com-\nputational Linguistics, 19(2):314–330, Jun.\nTom´aˇs Mikolov, Karaﬁ ´at Martin, Luk ´aˇs Burget, Jan\nCernock´y, and Sanjeev Khudanpur. 2010. Recurrent\nNeural Network based Language Model. In Proc. of\nINTERSPEECH, pages 1045–1048.\nTom´aˇs Mikolov, Anoop Deoras, Stefan Kombrink,\nLukas Burget, and Jan Honza Cernocky. 2011a.\nEmpirical Evaluation and Combination of Advanced\nLanguage Modeling Techniques. In Interspeech.\nTom´aˇs Mikolov, Stefan Kombrink, Luk ´aˇs Burget, Jan\nCernock´y, and Sanjeev Khudanpur. 2011b. Exten-\nsions of Recurrent Neural Network Language Model.\nIn Proc. of ICASSP, pages 5528–5531.\nTom´aˇs Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efﬁcient Estimation of Word Represen-\ntations in Vector Space. CoRR, abs/1301.3781.\nAndriy Mnih and Geoffrey E. Hinton. 2010. A Scal-\nable Hierarchical Distributed Language Model. In\nProc. of NIPS.\nAndriy Mnih and Yee Whye Teh. 2012. A fast and\nsimple algorithm for training neural probabilistic lan-\nguage models. In Proc. of ICML.\nFrederic Morin and Yoshua Bengio. 2005. Hierarchi-\ncal Probabilistic Neural Network Language Model.\nIn Proc. of AISTATS.\nRobert Parker, David Graff, Junbo Kong, Ke Chen, and\nKazuaki Maeda. 2011. English Gigaword Fifth Edi-\ntion. Technical report, Linguistic Data Consortium.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the Empiricial Meth-\nods in Natural Language Processing.\n1984\nHolger Schwenk, Anthony Rousseau, and Mohammed\nAttik. 2012. Large, Pruned or Continuous Space\nLanguage Models on a GPU for Statistical Machine\nTranslation. In NAACL-HLT Workshop on the Fu-\nture of Language Modeling for HLT, pages 11–19.\nAssociation for Computational Linguistics.\nAlessandro Sordoni, Michel Galley, Michael Auli,\nChris Brockett, Yangfeng Ji, Margaret Mitchell, Jian-\nYun Nie1, Jianfeng Gao, and Bill Dolan. 2015. A\nNeural Network Approach to Context-Sensitive Gen-\neration of Conversational Responses. In Proc. of\nNAACL. Association for Computational Linguistics,\nMay.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overﬁtting. Journal of Machine Learning Re-\nsearch.\nAshish Vaswani, Yinggong Zhao, Victoria Fossum,\nand David Chiang. 2013. Decoding with Large-\nscale Neural Language Models improves Transla-\ntion. In Proc. of EMNLP. Association for Compu-\ntational Linguistics, October.\n1985",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8472216725349426
    },
    {
      "name": "Softmax function",
      "score": 0.8467333316802979
    },
    {
      "name": "Normalization (sociology)",
      "score": 0.7157101035118103
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6138837337493896
    },
    {
      "name": "Vocabulary",
      "score": 0.6026751399040222
    },
    {
      "name": "Language model",
      "score": 0.5413520336151123
    },
    {
      "name": "Artificial neural network",
      "score": 0.4927985966205597
    },
    {
      "name": "Popularity",
      "score": 0.4865933656692505
    },
    {
      "name": "Natural language processing",
      "score": 0.45626142621040344
    },
    {
      "name": "Estimator",
      "score": 0.42860519886016846
    },
    {
      "name": "Deep neural networks",
      "score": 0.41434380412101746
    },
    {
      "name": "Speech recognition",
      "score": 0.41396433115005493
    },
    {
      "name": "Machine learning",
      "score": 0.40446749329566956
    },
    {
      "name": "Linguistics",
      "score": 0.13893398642539978
    },
    {
      "name": "Statistics",
      "score": 0.07910501956939697
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210099336",
      "name": "Menlo School",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210114444",
      "name": "Meta (United States)",
      "country": "US"
    }
  ]
}