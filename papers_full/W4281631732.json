{
  "title": "O-Net: A Novel Framework With Deep Fusion of CNN and Transformer for Simultaneous Segmentation and Classification",
  "url": "https://openalex.org/W4281631732",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1977132994",
      "name": "Tao Wang",
      "affiliations": [
        "Fuzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A3077795271",
      "name": "Junlin Lan",
      "affiliations": [
        "Fuzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2767301718",
      "name": "Zixin Han",
      "affiliations": [
        "Fuzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2137702954",
      "name": "Ziwei Hu",
      "affiliations": [
        "Fuzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2108878932",
      "name": "Yuxiu Huang",
      "affiliations": [
        "Fuzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2219811192",
      "name": "Yanglin Deng",
      "affiliations": [
        "Fuzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2132463585",
      "name": "Hejun Zhang",
      "affiliations": [
        "Fujian Provincial Cancer Hospital",
        "Fujian Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2127005319",
      "name": "Jianchao Wang",
      "affiliations": [
        "Fujian Medical University",
        "Fujian Provincial Cancer Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2106851984",
      "name": "Musheng Chen",
      "affiliations": [
        "Fujian Medical University",
        "Fujian Provincial Cancer Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2113439087",
      "name": "Haiyan Jiang",
      "affiliations": [
        "Fuzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A4202401678",
      "name": "Ren-Guey Lee",
      "affiliations": [
        "National Taipei University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2129153706",
      "name": "Qinquan Gao",
      "affiliations": [
        "Fuzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2105119693",
      "name": "Ming Du",
      "affiliations": [
        "Fuzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2107864873",
      "name": "Tong Tong",
      "affiliations": [
        "Fuzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2100962162",
      "name": "Gang Chen",
      "affiliations": [
        "Fujian Provincial Cancer Hospital",
        "Fujian Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A1977132994",
      "name": "Tao Wang",
      "affiliations": [
        "Fujian Medical University",
        "Fuzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A3077795271",
      "name": "Junlin Lan",
      "affiliations": [
        "Fujian Medical University",
        "Fuzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2767301718",
      "name": "Zixin Han",
      "affiliations": [
        "Fujian Medical University",
        "Fuzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2137702954",
      "name": "Ziwei Hu",
      "affiliations": [
        "Fuzhou University",
        "Fujian Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2108878932",
      "name": "Yuxiu Huang",
      "affiliations": [
        "Fuzhou University",
        "Fujian Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2219811192",
      "name": "Yanglin Deng",
      "affiliations": [
        "Fuzhou University",
        "Fujian Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2132463585",
      "name": "Hejun Zhang",
      "affiliations": [
        "Fujian Provincial Cancer Hospital",
        "Fujian Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A2127005319",
      "name": "Jianchao Wang",
      "affiliations": [
        "Fujian Medical University",
        "Fujian Provincial Cancer Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2106851984",
      "name": "Musheng Chen",
      "affiliations": [
        "Fujian Medical University",
        "Fujian Provincial Cancer Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2113439087",
      "name": "Haiyan Jiang",
      "affiliations": [
        "Fuzhou University",
        "Fujian Medical University"
      ]
    },
    {
      "id": "https://openalex.org/A4202401678",
      "name": "Ren-Guey Lee",
      "affiliations": [
        "National Taipei University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2129153706",
      "name": "Qinquan Gao",
      "affiliations": [
        "Fujian Medical University",
        "Vision Technology (United States)",
        "Fuzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2105119693",
      "name": "Ming Du",
      "affiliations": [
        "Fuzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2107864873",
      "name": "Tong Tong",
      "affiliations": [
        "Fuzhou University",
        "Fujian Medical University",
        "Vision Technology (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2100962162",
      "name": "Gang Chen",
      "affiliations": [
        "Fujian Provincial Cancer Hospital",
        "Fujian Medical University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6795435739",
    "https://openalex.org/W6790275670",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W6748481559",
    "https://openalex.org/W2464708700",
    "https://openalex.org/W2963946669",
    "https://openalex.org/W6736170873",
    "https://openalex.org/W3188404242",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W2782364420",
    "https://openalex.org/W6777983745",
    "https://openalex.org/W3204093853",
    "https://openalex.org/W3015814632",
    "https://openalex.org/W2921406441",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W6762585180",
    "https://openalex.org/W6746975906",
    "https://openalex.org/W6762215367",
    "https://openalex.org/W6725739302",
    "https://openalex.org/W3015788359",
    "https://openalex.org/W3115967667",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W2962835938",
    "https://openalex.org/W2964227007",
    "https://openalex.org/W3034777994",
    "https://openalex.org/W6808270177",
    "https://openalex.org/W6730903564",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W6750469568",
    "https://openalex.org/W6696428399",
    "https://openalex.org/W6778370468",
    "https://openalex.org/W6639824700",
    "https://openalex.org/W6749781174",
    "https://openalex.org/W2888358068",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W6760224987",
    "https://openalex.org/W6674914833",
    "https://openalex.org/W6686164453",
    "https://openalex.org/W6762718338",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W6697062800",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3212933375",
    "https://openalex.org/W6784565329",
    "https://openalex.org/W6688570214",
    "https://openalex.org/W6746034047",
    "https://openalex.org/W6757472644",
    "https://openalex.org/W6797399245",
    "https://openalex.org/W6729342207",
    "https://openalex.org/W6805127333",
    "https://openalex.org/W6791406852",
    "https://openalex.org/W2884436604",
    "https://openalex.org/W6756800942",
    "https://openalex.org/W2999905431",
    "https://openalex.org/W4210786716",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2289076519",
    "https://openalex.org/W4234552385",
    "https://openalex.org/W2217371171",
    "https://openalex.org/W3103010481",
    "https://openalex.org/W3101507774",
    "https://openalex.org/W4301409532",
    "https://openalex.org/W3092858474",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W4255564575"
  ],
  "abstract": "The application of deep learning in the medical field has continuously made huge breakthroughs in recent years. Based on convolutional neural network (CNN), the U-Net framework has become the benchmark of the medical image segmentation task. However, this framework cannot fully learn global information and remote semantic information. The transformer structure has been demonstrated to capture global information relatively better than the U-Net, but the ability to learn local information is not as good as CNN. Therefore, we propose a novel network referred to as the O-Net, which combines the advantages of CNN and transformer to fully use both the global and the local information for improving medical image segmentation and classification. In the encoder part of our proposed O-Net framework, we combine the CNN and the Swin Transformer to acquire both global and local contextual features. In the decoder part, the results of the Swin Transformer and the CNN blocks are fused to get the final results. We have evaluated the proposed network on the synapse multi-organ CT dataset and the ISIC 2017 challenge dataset for the segmentation task. The classification network is simultaneously trained by using the encoder weights of the segmentation network. The experimental results show that our proposed O-Net achieves superior segmentation performance than state-of-the-art approaches, and the segmentation results are beneficial for improving the accuracy of the classification task. The codes and models of this study are available at https://github.com/ortonwang/O-Net .",
  "full_text": "ORIGINAL RESEARCH\npublished: 02 June 2022\ndoi: 10.3389/fnins.2022.876065\nFrontiers in Neuroscience | www.frontiersin.org 1 June 2022 | Volume 16 | Article 876065\nEdited by:\nZhengwang Wu,\nUniversity of North Carolina at Chapel\nHill, United States\nReviewed by:\nYi Wang,\nNorthwestern Polytechnical\nUniversity, China\nAnubha Gupta,\nIndraprastha Institute of Information\nTechnology Delhi, India\n*Correspondence:\nTong Tong\nttraveltong@gmail.com\nGang Chen\nnaichengang@126.com\nSpecialty section:\nThis article was submitted to\nOriginal Research Article,\na section of the journal\nFrontiers in Neuroscience\nReceived: 15 February 2022\nAccepted: 05 May 2022\nPublished: 02 June 2022\nCitation:\nWang T, Lan J, Han Z, Hu Z, Huang Y,\nDeng Y, Zhang H, Wang J, Chen M,\nJiang H, Lee R-G, Gao Q, Du M,\nTong T and Chen G (2022) O-Net: A\nNovel Framework With Deep Fusion of\nCNN and Transformer for\nSimultaneous Segmentation and\nClassiﬁcation.\nFront. Neurosci. 16:876065.\ndoi: 10.3389/fnins.2022.876065\nO-Net: A Novel Framework With\nDeep Fusion of CNN and Transformer\nfor Simultaneous Segmentation and\nClassiﬁcation\nTao Wang1,2 , Junlin Lan1,2 , Zixin Han1,2 , Ziwei Hu1,2 , Yuxiu Huang1,2 , Yanglin Deng1,2 ,\nHejun Zhang3, Jianchao Wang3, Musheng Chen3, Haiyan Jiang2,4 , Ren-Guey Lee5,\nQinquan Gao1,2,6 , Ming Du1, Tong Tong1,2,6* and Gang Chen3,7*\n1 College of Physics and Information Engineering, Fuzhou Univ ersity, Fuzhou, China, 2 Fujian Key Lab of Medical\nInstrumentation and Pharmaceutical Technology, Fuzhou Uni versity, Fuzhou, China, 3 Department of Pathology, Fujian Cancer\nHospital, Fujian Medical University, Fuzhou, China, 4 College of Electrical Engineering and Automation, Fuzhou U niversity,\nFuzhou, China, 5 Department of Electronic Engineering, National Taipei Uni versity of Technology, Taipei, Taiwan, 6 Imperial\nVision Technology, Fuzhou, China, 7 Fujian Provincial Key Laboratory of Translational Cancer M edicin, Fuzhou, China\nThe application of deep learning in the medical ﬁeld has cont inuously made huge\nbreakthroughs in recent years. Based on convolutional neur al network (CNN), the\nU-Net framework has become the benchmark of the medical imag e segmentation task.\nHowever, this framework cannot fully learn global informat ion and remote semantic\ninformation. The transformer structure has been demonstra ted to capture global\ninformation relatively better than the U-Net, but the abili ty to learn local information is\nnot as good as CNN. Therefore, we propose a novel network refe rred to as the O-Net,\nwhich combines the advantages of CNN and transformer to full y use both the global\nand the local information for improving medical image segme ntation and classiﬁcation.\nIn the encoder part of our proposed O-Net framework, we combi ne the CNN and the\nSwin Transformer to acquire both global and local contextua l features. In the decoder\npart, the results of the Swin Transformer and the CNN blocks a re fused to get the\nﬁnal results. We have evaluated the proposed network on the s ynapse multi-organ CT\ndataset and the ISIC 2017 challenge dataset for the segmenta tion task. The classiﬁcation\nnetwork is simultaneously trained by using the encoder weig hts of the segmentation\nnetwork. The experimental results show that our proposed O- Net achieves superior\nsegmentation performance than state-of-the-art approach es, and the segmentation\nresults are beneﬁcial for improving the accuracy of the clas siﬁcation task. The codes\nand models of this study are available at https://github.co m/ortonwang/O-Net.\nKeywords: CNN, transformer, medical image segmentation, deep le arning, classiﬁcation\n1. INTRODUCTION\nImage enhancement has been extensively performed on medical images based on morphology,\nsuch as clustering (\nVasuda and Satheesh, 1713 ), edge detection ( Patil and Deore, 2013 ), and\nthreshold segmentation ( Wang et al., 2015 ) to assist doctors in diagnosis in the early days. With\nthe development of artiﬁcial intelligence, deep learning te chnology has been widely used in medical\nWang et al. O-Net: Segmentation and Classiﬁcation\nimage processing and analysis in recent years, and the accura cy\nof segmentation and classiﬁcation on medical images is of\ngreat signiﬁcance to the diagnosis of diseases today. In cli nical\npractice, accurate image segmentation can provide clinician s\nwith quantitative information, which can help clinicians m ake\ndiagnostic decisions more precisely and eﬃciently (\nLiang et al.,\n2020). In addition, the additional information provided by\ncomputing methods is subjective and can avoid the objective b ias\nby humans.\nNowadays, Convolutional Neural Network (CNN), especially\nFull Convolutional Network (FCN) is an eﬀective segmentatio n\nmethod ( Wang et al., 2021b ) and it has been widely used\nin dense classiﬁcation tasks such as semantic segmentation\n(\nJi et al., 2020 ). Among diﬀerent CNN networks, U-Net\n(Ronneberger et al., 2015 ) is a deep learning network with\nencoder and decoder structures, which has been widely used\nin medical image segmentation. In recent years, it has been\nwidely used in medical image segmentation tasks due to its\nstrong generalization. U-Net and its variants UNet++ (\nZhou\net al., 2018 ), UNet 3+ ( Huang et al., 2020 ), CE-Net ( Gu\net al., 2019 ) have shown excellent performance in tasks, such\nas lesion segmentation, heart segmentation, and other orga n\nsegmentation. Based on the strong ability of learning and\ndiscriminating features, Res-UNet (\nXiao et al., 2018 ) improves\nthe performance of the network by introducing a residual\nnetwork into the encoder part of U-Net. EﬃcientNet (\nTan and\nLe, 2019 ) proposed a new scaling method that uniformly all\ndimensions of the depth, width, and resolution of the network\nthrough simple but eﬃcient composite coeﬃcients, which not\nonly reduces a certain amount of calculation, but also improv es\nthe segmentation performance. Many experimental results hav e\nshown that the use of EﬃcientNet as an encoder can often furth er\nimprove the performance of the network without increasing the\namount of calculation.\nHowever, these networks are faced with the common problem\nof CNN: it is diﬃcult for CNN-based methods to learn the\nglobal and remote semantic information interaction (\nChen et al.,\n2021) clearly. This is due to the fact that CNN extracts features\nwith a convolutional process. Some studies tried to use image\nfeature pyramid (\nLin et al., 2017 ), atrous convolution layers\n(Chen et al., 2017, 2018; Gu et al., 2019 ), and self-attention\nmechanisms ( Wang et al., 2018; Schlemper et al., 2019 ) to\nsolve this problem. However, the global and remote semantic\ninformation is not fully learnt using these strategies. Ins pired by\nthe great success of transformer (\nVaswani et al., 2017 ) in the ﬁeld\nof natural language processing (NLP), researchers have trie d to\nintroduce transformer to make up for the shortcomings of CNN\nin global and remote information interaction. A transforme r is\nan attention-based model and self-attention mechanism (SA ) is\na key component of transformer. It can model the correlation\nof all input tags which makes room for the transformer to deal\nwith long-range dependencies. In\nDosovitskiy et al. (2020) , vision\ntransformer (ViT) was applied to perform image recognition\ntasks and achieved relatively good results. After that, a no vel\nframework called Swin Transformer (\nLiu et al., 2021 ) was\nproposed and signiﬁcantly improved the performance of ViT\nin diﬀerent tasks, such as image classiﬁcation (\nLiu et al., 2021 ),\nobject detection (Xu et al., 2021 ), and semantic segmentation ( Xie\net al., 2021 ). Based on the Swin Transformer, Cao et al. (2021)\nproposed Swin-Unet, which combined the U-Net structure\nand Swin Transformer for medical image segmentation, the\nencoding part and the decoding part in Swin-Unet were both\nperformed using Swin Transformer. With the proposal of these\nmethods, the accuracy of segmentation tasks is further impro ved.\nHowever, the input in transformer is formed as one-dimensiona l\nsequence. The transformer networks focus on learning the\nglobal contextual information, but may lose some local detai ls.\nTherefore, it is beneﬁcial to combine the global information\nlearnt by transformer and the local information by CNN to enr ich\nthe learnt features.\nBased on the advantages of CNN and transformer, we\npropose an O-Net framework to combine the CNN and\nthe transformer to learn both global and local contextual\nfeatures. We combine the CNN and Swin Transformer as\nencoder ﬁrst and send them into a CNN-based decoder and\na Swin Transformer-based decoder, respectively. The result s of\ntwo decoders are fused to get the ﬁnal result. This network\ncombines the advantages of CNN and transformer and may\nimprove the performance of medical image segmentation.\nOur experimental results have shown that the performance\nof the network can be signiﬁcantly improved by combining\nCNN and transformer. In addition, a classiﬁcation task is\nsimultaneously performed based on the O-Net. Experiments\nshow that the segmentation results are beneﬁcial for improvi ng\nthe accuracy of the classiﬁcation task. Experiments on the\nsynapse multi-organ segmentation dataset and the ISIC2017 s kin\nlesion challenge dataset have demonstrated the superiority o f\nour method compared to other state-of-the-art segmentation\nmethods. In addition, based on the segmentation network,\nthe performance of the classiﬁcation network has also been\ngreatly improved.\n2. RELATED WORKS\nCNN-based methods: CNN is a kind of feedforward neural\nnetwork that includes convolution calculations and has a de ep\nstructure. It is one of the representative algorithms of deep\nlearning. Lenet[18] ﬁrst deﬁned the CNN network structure i n\n1998, and it was not until the publication of AlexNet (\nKrizhevsky\net al., 2012 ) in 2012 that CNN has gradually become mainstream.\nSince then, lots of eﬃcient and deep convolutional neural\nnetworks have been proposed. For example, VGG (\nSimonyan and\nZisserman, 2014 ), ResNet ( He et al., 2016 ), DenseNet ( Huang\net al., 2017 ), GoogleNet ( Szegedy et al., 2015 ), HRNet ( Sun\net al., 2019 ), Inception v3 ( Szegedy et al., 2016 ), and EﬃcientNet\n(Tan and Le, 2019 ). These networks perform well in various\napplications. In addition to these network innovations, new\nconvolutional layers such as deformable convolution (\nDai et al.,\n2017; Zhu et al., 2019 ) and depth-wise convolution ( Xie et al.,\n2017) were proposed for diﬀerent tasks. With the development\nof CNN, U-Net was proposed and widely used in segmentation\ntasks because of its simple structure, good eﬀects, and strong\ngeneralization. After that, various U-shape network based U-Ne t\nFrontiers in Neuroscience | www.frontiersin.org 2 June 2022 | Volume 16 | Article 876065\nWang et al. O-Net: Segmentation and Classiﬁcation\nhave been proposed such as U-SegNet ( Kumar et al., 2018 ), Res-\nUNet (Xiao et al., 2018 ), Dense-UNet ( Li et al., 2018 ), U-Net++\n(Zhou et al., 2018 ), U-2-Net ( Qin et al., 2020 ), and UNet3+\n(Huang et al., 2020 ) CE-Net ( Gu et al., 2019 ). Gehlot et al. (2020)\nproposed an Encoder-Decoder based CNN with Nested-Feature\nConcatenation (EDNFC-Net) for automatic segmentation. So me\nnetworks introduce novel structures in the encoder part whil e\nothers in the decoder part. Because of the strong generalizati on\nof the network, the U-shaped architecture network has also bee n\nextended to 3D medical image segmentation, such as 3D-UNet\n(\nÇiçek et al., 2016 ) and V-Net ( Milletari et al., 2016 ). Moreover,\nGehlot et al. proposed AION ( Gehlot and Gupta, 2021 ), an\narchitecture with two coupled networks and classiﬁcation he ads\nwhich is applicable for stain normalization, classiﬁcation, and\nsegmentation tasks.\nTransformers: Transformer was ﬁrst proposed for machine\ntranslation and achieved the best performance in many NLP\ntasks. To combine computer vision (CV) and natural language\nprocessing (NLP) domain knowledge, researchers developed\nVision Transformer (ViT) (\nDosovitskiy et al., 2020 ) by directly\napplying transformers with global self-focus to full-size ima ges.\nThe ViT model achieved both high eﬃciency and accuracy\nin image recognition tasks. Based on ViT,\nChen et al.\n(2021) proposed the ﬁrst transformer-based medical image\nsegmentation framework TransUNet which further improved the\naccuracy of image segmentation tasks. However, ViT needs to be\npre-trained on its large datasets to achieve good performance .\nTo solve this problem, some training schemes were designed in\nDeit (\nTouvron et al., 2021 ) so that the algorithm can perform\nwell on smaller data sets. To further improve the accuracy, a n ew\nvision transformer called Swin Transformer ( Liu et al., 2021 ) was\nproposed, it is a hierarchical transformer whose representati on is\ncomputed with Shifted windows. This hierarchical architect ure\nhas the ﬂexibility of modeling at various scales and has line ar\ncomputational complexity relative to the image size. These\nfeatures make it compatible with many vision tasks, includin g\nimage classiﬁcation and semantic segmentation. Based on Sw in\nTransformer,\nCao et al. (2021) proposed a pure transformer U-\nshaped encoder-decoder network named Swin-Unet for medical\nimage segmentation, which has relatively good performance i n\nsome datasets.\nSelf-attention/transformer combined with CNN: In recent\nyears, researchers have tried to improve the performance of th e\nnetwork through the self-attention mechanism (\nWang et al.,\n2018) to overcome the shortcomings of CNN learning global\nsemantic information. In Schlemper et al. (2019) , the skip-\nconnections with additive attention gate were integrated w ith\nU-shaped architecture to improve medical image segmentation.\nBut this is still the method based on CNN after all and it\nhas not completely solved the limitation of learning global\ninformation. Several studies have been carried out to combi ne\nCNN and transformer. TransUNet (\nChen et al., 2021 ) was\nproposed by combining the advantages of transformer and CNN.\nThe transformer encodes image patches from a CNN feature\nmap as the input sequence for extracting global contexts. A\nmixed transformer module (MTM) (\nWang et al., 2021a ) was\nproposed for simultaneous inter- and intra- aﬃnities learnin g.\nTransFuse (Zhang et al., 2021 ) combines transformers and CNNs\nin a parallel style to capture both global dependency and low-lev el\nspatial details eﬃciently in a much shallower manner for medi cal\nimage segmentations. Liang et al. (2022) proposed transconver\nwith a parallel module named transformer-convolution incepti on\nwhich extracts local and global information via convolution\nblocks and transformer blocks, respectively. TransMed ( Dai et al.,\n2021) was proposed for multi-modal medical image classiﬁcation\nwhich combines the advantages of CNN and transformer to\nextract low-level features of images eﬃciently and establis h\nlong-range dependencies between modalities. These algorit hms\nimprove the global attention of the model based on their\ncomplementarity by directly combining CNN and transformer.\n3. THE PROPOSED METHOD\n3.1. Overall Architecture Design\nA schematic view of the proposed O-Net is presented in Figure 1.\nO-Net is composed of two parts: an encoder module and a\ndecoder module. The basic units of O-Net include the Swin\nTransformer block, EﬃcientNet block, and CNN Decoder block .\nDuring the segmentation task, the encoder module extracts t he\nfeatures of the input image to obtain the high-dimensional and\nlow-dimensional features, which are then decoded back to th e\nfull spatial resolution by the decoder module. After extract ing\nthe features in the encoder part, the segmentation network\nprovided an interface to integrate a classiﬁcation network fo r\nsimultaneously performing the classiﬁcation task. Each modu le\nis described in detail below.\n3.2. Swin Transformer Block\nDiﬀerent from the transformer, Swin Transformer is built bas ed\non shifted windows rather than the standard multi-head self\nattention (MSA) module. Two consecutive Swin Transformer\nblocks are presented in Figure 2. Each Swin Transformer block\nconsists of residual connection and 2-layer MLP with Gaussia n\nError Linear Units (GELU) non-linearity, LayerNorm (LN) laye r,\nand multi-head self attention module. The shifted window-\nbased multi-head self attention (SW-MSA) module and the\nwindow-based multi-head self attention (W-MSA) module are\napplied in the two successive transformer blocks, respectively .\nBased on such a window partitioning approach, successive Swin\nTransformer blocks can be formulated as follows:\nˆzl = W − MSA(LN(zl−1)) + zl−1, (1)\nzl = MLP(LN(ˆzl)) + ˆzl, (2)\nzl+1 = SW − MSA(LN(zl)) + zl, (3)\nzl+1 = MLP(LN(zl+1)) + zl+1, (4)\nWhere zl and ˆzl represent the output features of the (S)W-MSA\nmodule and the MLP module of the lth block, respectively. Similar\nFrontiers in Neuroscience | www.frontiersin.org 3 June 2022 | Volume 16 | Article 876065\nWang et al. O-Net: Segmentation and Classiﬁcation\nFIGURE 1 | The architecture of our proposed O-Net.\nFIGURE 2 | Two successive Swin Transformer block.\nto the previous works ( Hu et al., 2018, 2019 ), self-attention is\ncomputed as follows:\nAttention(Q, K, V) = SoftMax( QKT\n√\nd\n+ B)V, (5)\nwhere Q, K, V ∈ RM2×d denote the query, key, and value\nmatrices. M2 represents the number of patches in a window, and\nd is the query dimension. Since the relative position along ea ch\naxis is within the range[ −M+1, M −1], the values in B are taken\nfrom the bias matrix ˆB ∈ R(2M−1)×2M+1.\n3.3. EfﬁcientNet Block\nEﬃcientNet block (\nTan and Le, 2019 ) was proposed based on a\nneural structure search. This block uses composite coeﬃcien ts to\nuniformly scale the depth, width, and resolution of the networ k.\nA schematic view of the EﬃcientNet block is presented in\nFigure 3. Each EﬃcientNet block is composed of MBConvBlocks\n(\nSandler et al., 2018 ) which consists of convolution, batch\nnormalization, and Swish activation layers. The network ach ieves\nbetter performance with the same parameters by uniformly\nscaling the network width, depth, or resolution in a ﬁxed\nproportion. We employ the EﬃcientNet block as the encoder part\nof CNN to extract features eﬃciently and eﬀectively.\n3.4. Encoder Module\nIn the encoder part, we combine EﬃcientNet and Swin\nTransformer. For the Swin Transformer Encoder, it is compose d\nof Swin Transformer Block and patch merging layer. Images\nare separated into non-overlapping patches with a patch size of\nFrontiers in Neuroscience | www.frontiersin.org 4 June 2022 | Volume 16 | Article 876065\nWang et al. O-Net: Segmentation and Classiﬁcation\nFIGURE 3 | The architecture of EfﬁcientNet Block.\nFIGURE 4 | The architecture of decoder module.\n4×4 to transform the inputs into sequence embeddings, then\nconcatenated together by the patch merging layer. The featur e\nresolution will be down-sampled by 2 × after such processing,\nand the feature dimension of each patch becomes to 4 ×4×3 =\n48. Furthermore, a linear embedding layer is applied to project\nfeature dimension into an arbitrary dimension (represented as\nC). The transformed patch tokens pass through several Swin\nTransformer blocks and patch merging layers to generate the\nhierarchical feature representations.\nFor the EﬃcientNet encoder, the input image is convoluted\nand down-sampled ﬁrst. Feature extraction is carried out\nthrough the EﬃcientNet block which uniformly scales the\ndepth, width, and resolution of the network through composite\ncoeﬃcients. We can achieve relatively eﬃcient feature extr action\nwith only a small amount of computation using this module.\nSince the feature dimensions of two encoders are diﬀerent, it\nis required to normalize the dimension before fusing them.\nThe features extracted by the Swin Transformer are set to\nC×H×W using a linear projection. After that, the features\nare fused with the features extracted by the EﬃcientNet\nblock via skip-connections. Similarly, when using the Swin\nTransformer decoder, we project the features extracted by\nthe EﬃcientNet block through the linear embedding layer\nand fuse them with the features extracted by the Swin\nTransformer encoder.\n3.5. Decoder Module\nThe decoder module is adopted to restore the high-level semant ic\nfeatures extracted from the encoder module. The decoder part\nconsists of the Swin Transformer decoder block and the CNN\ndecoder block. A schematic view of the decoder modules is\npresented in Figure 4. The Swin decoder block is composed of\na patch expanding layer and a Swin Transformer block. The\nfeatures extracted by the encoder are multi-scale fused thr ough\nskip-connections. The patch expanding layer reshapes feature\nmaps of adjacent dimensions into large feature maps with 2 ×\nup-sampling of resolution. In the end, the last patch expanding\nlayer is used to perform 4 × up-sampling to restore the resolution\nof the feature maps to the input resolution (W ×H), and a linear\nprojection layer is applied on these up-sampled features to output\nthe pixel-level segmentation predictions.\nThe CNN decoder block is composed of a 2 × upsampling\noperator, two 3 ×3 convolution layers, and a batch normalization\nlayer with a Rectiﬁed Linear Units(ReLU) layer. Simple\nFrontiers in Neuroscience | www.frontiersin.org 5 June 2022 | Volume 16 | Article 876065\nWang et al. O-Net: Segmentation and Classiﬁcation\nupsampling and convolution are two common operations of the\ndecoder in the CNN decoder blocks. After the 2 × upsampling\noperator, the features were fused with those from encoders\nthrough skip-connection. After the two convolution processe s,\nthe features are input for the next decoder. At the end of the\ndecoder, a convolution layer is applied to output the pixel-\nlevel segmentation predictions. Finally, the outputs of the t wo\ndecoders are fused to obtain the ﬁnal segmentation result.\n3.6. Classiﬁcation Method\nThe encoder part of the segmentation network and the\nclassiﬁcation network share the same structure. When encod ers\nperform the classiﬁcation task, the role of encoders is to\nextract contextual features and locate the target region li ke\nthe segmentation task. The primary task of classiﬁcation aim s\nto accurately locate the target area, and the purpose of the\nsegmentation network is to realize it. Therefore, after the\ntraining of the segmentation network, we use the learned wei ghts\nof the encoder in the network as the initial parameters of\nthe classiﬁcation network. After that, we utilize the featu res\nof the lowest dimension in the encoder through the average\npooling layer and a fully connected layer (FC) to perform the\nclassiﬁcation task.\n4. EXPERIMENTS\n4.1. Datasets\nSynapse multi-organ segmentation dataset (synapse): The\ndataset includes 30 abdominal CT scans from MICCAI 2015\nMulti-Atlas Abdomen Labeling Challenge. Each CT volume\nconsists of 85 −198 slices of 512 ×512 pixels and there are 3,779\naxial abdominal clinical CT images in total. Following\nChen et al.\n(2021) and Liu et al. (2021) , 18 samples were used as the training\nset and 12 samples as the testing set. The annotation of each\nimage includes 8 abdominal organs (aorta, gallbladder, sple en,\nleft kidney, right kidney, liver, pancreas, spleen, and stoma ch).\nThe dice metric and the average Hausdorﬀ Distance (HD) are\nused to evaluate our method on this dataset. The dice metric\nevaluates the degree of pixel overlap between the ground trut h\nand prediction results and it is calculated as follows:\nDice = 2 × TP\n2 × TP + FN + FP (6)\nwhere TP , FP , and FN refer to the number of true positives,\nfalse positives, and false negatives, respectively, besides, TN\nmeans true negatives. The HD calculates the maximum distanc e\nbetween the contours of the ground truth and predicted result s,\nwhich can be formulated as follows:\nH(A, B) = max(h(A, B), h(B, A) (7)\nh(A, B) = max\na∈A\n{\nmin\nb∈B\n{\n∥a − b∥\n} }\n(8)\nh(B, A) = max\nb∈B\n{\nmin\na∈A\n{\n∥b − a∥\n} }\n(9)\nwhere A and B denote the contours of the ground truth\nand predicted results, respectively, and h(A,B) denotes the\nunidirectional HD from A to B.\nISIC2017 skin lesion challenge dataset (ISIC2017): The 2017\nInternational Skin Imaging Collaboration (ISIC) skin lesi on\nsegmentation challenge dataset (\nCodella et al., 2018 ) includes\n2,000 training images, 150 validation images, and 600 test\ndermoscopic images. Each image is paired with an expert manual\ntracing of skin lesion boundaries for the segmentation task and\nthe lesion gold standard diagnosis (i.e., nevus, melanoma, a nd\nseborrheic keratosis) for the classiﬁcation task. The size o f the\nimages in the dataset varies from 453 ×679 to 4499 ×6748 pixels.\nWe used Dice, Mean Intersection over Union (IoU), Precision\n(Pre), Recall, F1-score, and Pixel Accuracy (PA) as the metri cs\nto evaluate the accuracy of the segmentation work. In additi on,\nwe used Accuracy (AC), F1-score, precision (Pre), and speciﬁcit y\n(SP) as the metrics to evaluate the classiﬁcation task. These metric\nare calculated as follows:\nIoU = TP\nTP + FN + FP (10)\nPre = TP\nTP + FP (11)\nPA = TP + TN\nTP + TN + FP + FN (12)\nAC = TP + TF\nTP + TN + FP + FN (13)\nF1 − score = 2 × TP\n2 × TP + FP + FN (14)\n4.2. Implementation Details\nOur method was implemented based on the Pytorch Deep\nLearning framework using python. For all training cases, ﬂips\nand rotations were used as data augmentation to improve the\ngeneralization ability of the model. We trained our model on a n\nNvidia RTX 3090 GPU with 24GB memory. The input image size\nwas set to 224 ×224 on the synapse dataset and 512 ×512 on the\nISIC2017 dataset. The patch on the size was set to 4 in both task s.\nAll encoders and Swin Transformer blocks in the model were\npretrained on ImageNet (\nDeng et al., 2009 ). During the training\nprocess of the synapse dataset, the batch size was set to 24 and t he\npopular SGD optimizer with momentum of 0.9 and weight decay\nof 1e-4 and a learning rate of 1e-4 is used for the backpropagatio n\nof the model. During the process of ISIC2017 dataset, the model s\nwere optimized by AdamW with a learning rate of 1e-4 and a\nbatch size of 8.\n4.3. Experiment Results on the Synapse\nDataset\nThe comparison of the proposed O-Net with previous state-\nof-the-art methods on the synapse multi-organ CT dataset is\npresented in Table 1. Experimental results demonstrate that our\nFrontiers in Neuroscience | www.frontiersin.org 6 June 2022 | Volume 16 | Article 876065\nWang et al. O-Net: Segmentation and Classiﬁcation\nTABLE 1 | Experimental results of different methods on the synapse mu lti-organ CT dataset.\nMethod Dice ↑ HD↓ Aorta Gallbladder Kidney(L) Kidney(R) Liver Pancreas Sple en Stomach\nV-Net Milletari et al. (2016) 68.81 – 75.34 51.87 77.10 80.75 87.84 40.05 80.56 56.98\nDARR Fu et al. (2020) 69.77 – 74.74 53.77 72.31 73.24 94.08 54.18 89.90 45.96\nR50 ViT Chen et al. (2021) 71.29 32.87 73.73 55.13 75.80 72.20 91.51 45.99 81.99 73.95\nU-SegNet Kumar et al. (2018) 72.61 43.94 85.69 64.33 75.12 66.41 91.72 50.59 84.07 62.96\nR50 U-Net Chen et al. (2021) 74.68 36.87 87.74 63.66 80.60 78.19 93.74 56.90 85.87 74.16\nAION Gehlot and Gupta (2021) 75.54 32.27 87.59 58.74 82.47 73.45 93.47 49.44 87.52 71.61\nR50 Att-UNet Chen et al. (2021) 75.57 36.97 55.92 63.91 79.20 72.71 93.56 49.37 87.19 74.95\nU-Net Ronneberger et al. (2015) 76.85 39.7 89.07 69.72 77.77 68.60 93.43 53.98 86.67 75.58\nEDNFC-Net Gehlot et al. (2020) 77.21 35.07 86.08 62.47 84.31 78.27 92.61 57.31 85.36 71.24\nTransUNet Chen et al. (2021) 77.48 31.69 87.23 63.13 81.87 77.02 94.08 55.86 85.08 75.62\nAtt-UNet Oktay et al. (2018) 77.77 36.02 89.55 68.88 77.98 71.11 93.57 58.04 87.30 75.75\nTransFuse Zhang et al. (2021) 78.95 26.59 87.09 61.64 82.20 76.91 94.19 59.01 89.86 80.73\nSwin-Unet Cao et al. (2021) 79.13 21.55 85.47 66.53 83.28 79.61 94.29 56.58 90.66 76.60\nO-Net 80.61 21.04 88.36 67.45 84.44 77.13 95.24 61.52 90.03 80.74\nThe symbol ↑ means the higher value, the better.\nThe symbol ↓ means the lower value, the better.\nBold font to highlight the optimal values.\nFIGURE 5 | Conparision of different methods on the Synapse multi-orga n dataset by visualization. From left to right: (A) Ground Truth, (B) O-Net, (C) SwinUNet, (D)\nTransUNet, and (E) R50 AttUNet.\nalgorithm achieves the best performance with a segmentation\naccuracy of 80.61% (Dice ↑) and 21.04 (HD ↓) performance. We\ncan see from the results that the CNN-based method performs\nworse on edge predictions than the transformer method from\nthe metric of HD. This also indicates that our algorithm not\nonly performs better in terms of segmentation, but also has\nFrontiers in Neuroscience | www.frontiersin.org 7 June 2022 | Volume 16 | Article 876065\nWang et al. O-Net: Segmentation and Classiﬁcation\nTABLE 2 | Segmentation results of different methods on the ISIC2017 dat aset.\nMethod Dice mIoU Pre recall F1-score PA\nU-Net Ronneberger et al. (2015) 85.22 78.40 91.17 73.98 77.80 91.19\nR50-U-Net Xiao et al. (2018) 87.48 80.86 92.99 78.19 81.70 92.19\nU-SegNet Kumar et al. (2018) 87.87 81.22 90.50 81.13 82.49 92.33\nENDFC-Net Gehlot et al. (2020) 88.00 81.43 90.26 82.10 82.80 92.29\nM-Net Fu et al. (2018) 88.33 82.25 94.46 79.04 83.38 92.67\nAION Gehlot and Gupta (2021) 88.84 82.56 92.26 81.95 84.02 92.88\nCE-Net Gu et al. (2019) 89.64 83.56 95.40 80.47 84.99 93.67\nSwin-Unet Cao et al. (2021) 88.77 82.69 94.64 79.16 83.51 94.04\nTransFuse Zhang et al. (2021) 89.63 83.78 95.56 80.35 84.75 93.73\nTransUNet Chen et al. (2021) 89.99 84.21 95.59 81.21 85.42 93.97\nO-Net 90.30 84.52 95.65 81.72 85.89 94.09\nBold font to highlight the optimal values.\nTABLE 3 | Classiﬁcation accuracy of different methods on the ISIC2017 da taset.\nAverage Nevus classiﬁcation\nMethod AC AC F1-score Pre SP\nSwin Transformer Liu et al. (2021) 80.22 89.50 81.18 62.16 91.76\nAION Gehlot and Gupta (2021) 81.55 85.33 76.01 50.74 86.86\nTransMed Dai et al. (2021) 84.11 89.19 80.10 61.90 92.16\nMobileNetV3 Howard et al. (2019) 84.89 89.33 81.53 60.83 90.78\nEfﬁcientNet-B3 Tan and Le (2019) 85.22 90.67 82.64 66.67 93.33\nInception v4 Szegedy et al. (2016) 85.33 89.16 81.45 60.16 90.39\nResNet50 He et al. (2016) 85.44 91.00 82.97 68.37 93.92\nDenseNet201 Huang et al. (2017) 86.56 92.00 85.36 69.81 93.73\nO-Net 87.22 91.67 83.51 72.73 95.29\nAverage Melanoma classiﬁcation Keratosis classiﬁcation\nMethod AC AC F1-score Pre SP AC F1-score Pre SP\nSwin Transformer Liu et al. (2021) 80.22 73.00 71.63 84.07 73.91 78.17 68.45 45.33 83.02\nAION Gehlot and Gupta (2021) 81.55 77.33 75.69 85.40 74.40 82.00 69.73 54.46 90.48\nTransMed Dai et al. (2021) 84.11 79.17 77.13 84.72 71.50 84.00 79.83 59.63 55.56\nMobileNetV3 Howard et al. (2019) 84.89 80.50 78.78 86.51 75.36 84.83 74.59 62.75 92.13\nEfﬁcientNet-B3 Tan and Le (2019) 85.22 80.50 78.82 86.70 75.85 84.50 75.71 59.84 89.86\nInception v4 Szegedy et al. (2016) 85.33 80.83 79.05 86.39 74.88 86.00 75.94 67.37 93.58\nResNet50 He et al. (2016) 85.44 81.50 79.99 87.90 78.26 83.83 75.28 57.69 88.61\nDenseNet201 Huang et al. (2017) 86.56 83.50 81.55 86.57 73.91 84.17 72.48 61.96 92.75\nO-Net 87.22 84.17 81.58 84.49 67.63 85.83 74.19 70.00 95.03\nBold font to highlight the optimal values.\na good performance in edge prediction. For organs with high\nsegmentation diﬃculty such as Pancreas and Gallbladder, ou r\nmethod obtains the best and the third results, respectively,\nwhich also reﬂects the strong generalization of our algorit hm.\nThe speciﬁc segmentation results of diﬀerent algorithms on th is\ndataset are presented in Figure 5. In this work, we demonstrate\nthat the in-depth combination of CNN and Swin Transformer\ncan learn both the global and the local contextual features,\nthereby obtaining better segmentation results.\n4.4. Experiment Results on the ISIC2017\nDataset\nWe further evaluated the proposed method for medical image\nsegmentation and classiﬁcation using the ISIC2017 dataset.\nThe results of segmentation and classiﬁcation are presented i n\nTables 2, 3. From Table 2, we can see that in the segmentation\ntask, the combination of the CNN and the Swin Transformer can\nachieve better performance than that of single CNN or that of\nonly the Swin Transformer. This indicates the eﬀectiveness o f the\nFrontiers in Neuroscience | www.frontiersin.org 8 June 2022 | Volume 16 | Article 876065\nWang et al. O-Net: Segmentation and Classiﬁcation\nFIGURE 6 | Receiver Operating Characteristic curves of the different methods for classiﬁcation task on the ISIC2017 dataset.\ncombination of these two structures. The O-Net has achieved the\nbest performance in the six metrics which reﬂects the superior ity\nof our method. The Receiver Operating Characteristic (ROC)\ncurves of the classiﬁcation methods are shown in Figure 6. The\nArea Under Curve (AUC) value for O-Net is 0.9264 which is\nthe best performance among compared methods. Based on the\ndata from Table 3 and the ROC curves of the classiﬁcation task,\nwe can see that O-Net has also achieved excellent performance\nin the classiﬁcation task. The speciﬁc segmentation results of\ndiﬀerent algorithms on this dataset are presented in Figure 7. The\nexperimental results of classiﬁcation tasks on this dataset indicate\nthat combining CNN and Swin Transformer for classiﬁcation\ntasks can improve the accuracy of the classiﬁcation tasks.\nThe performance can be further improved by initializing the\nclassiﬁcation network with the parameters from the encoder pa rt\nof the segmentation network.\n4.5. Ablation Study\nThe results of the ablation studies are shown in Tables 4, 5. We\nwill compare and analyze the eﬀects of diﬀerent factors on the\nsegmentation performance in the following sections.\nEﬀect of encoder: The experimental results in Table 4 show\nthat the best results are achieved by using the EﬃcientNet bl ock\nas the encoder, while the number of parameters is not large. Th e\nparameter quantity of the MobileNet is smaller than that of the\nEﬃcientNet, but its accuracy is far too poor than the others. T he\naccuracy of Inception v3 is similar to ours, but the amount of\ncalculation is much larger than that of EﬃcientNet. Therefo re,\nwe use EﬃcientNet as a CNN-based encoder.\nEﬀect of combination: The segmentation network consists\nof encoder and decoder. How to combine the CNN based\nmethod and the Swin Transformer based method is a point worth\nexploring. Table 5 shows the eﬀects of adopting diﬀerent models\nfor encoder and decoder. It can be seen from the results that\nthe best performance is achieved by combining them in both the\nencoder and decoder parts. As can be seen from the results, bett er\nsegmentation performance is achieved when CNN is used in the\nencoder part and Swin Transformer is used in the decoder part.\nEﬀect of learning rate and batch size: To explore the\nbest learning rate and batch size in the training process of\nthe algorithm, we carried out a series of experiments. The\nexperimental results are shown in Table 6. It can be seen from\nthe top half of the chart that the best Dice was obtained when\nthe learning rate was set to 1e-2. Although the best HD was\nobtained when the learning rate was set to 1e-1, its Dice was\nlower, therefore, we chose the learning rate of 5e-2. We can\nFrontiers in Neuroscience | www.frontiersin.org 9 June 2022 | Volume 16 | Article 876065\nWang et al. O-Net: Segmentation and Classiﬁcation\nFIGURE 7 | Comparison of different methods on the ISIC2017 dataset by vis ualization. (A) Image, (B) Ground Truth, (C) O-Net, (D) TransUNet, (E) Swin-UNet, (F)\nCE-Net, (G) R50 AttUNet, and (H) UNet.\nTABLE 4 | Ablation study on the encoder of CNN method.\nEncoder Params Dice ↑ HD↓ Aorta Gallbladder Kidney(L) Kidney(R) Liver Pancreas Sple en Stomach\nMobileNetV3 Howard et al. (2019) 5.48 76.66 26.27 86.12 62.25 82.07 90.65 94.06 55.72 88.62 73.77\nDenseNet201 Huang et al. (2017) 20.01 78.91 20.45 87.52 65.52 82.61 78.20 95.05 57.42 86.40 7 8.65\nResnet50 He et al. (2016) 25.55 79.16 23.01 87.71 66.86 81.73 75.22 94.18 58.86 90.42 78.31\nInception v3 Szegedy et al. (2016) 23.83 80.36 22.78 88.09 63.76 82.19 79.25 95.16 65.17 87.12 82.13\nEfﬁcientNet-b3 Tan and Le (2019) 12.23 80.61 21.04 88.36 67.45 84.44 77.13 95.24 61.52 90.03 80.74\nThe symbol ↑ means the higher value, the better.\nThe symbol ↓ means the lower value, the better.\nBold font to highlight the optimal values.\nalso draw from the bottom half of the chart that the best dice\nwas obtained when the batch size was set to 24. Although the\nHD is lower when batch size was set to 8 and 6, the Dice of\nthe Gallbladder is far too low, which is not conducive to the\noverall segmentation, therefore, the batch size of 24 would be\nmore appropriate.\n5. CONCLUSION\nWe introduce a novel method based on the combination of\nCNN and Swin Transformer for medical image segmentation\nand classiﬁcation. To make full use of the global and the\nlocal information to improve medical image segmentation and\nclassiﬁcation, we propose O-Net, which combines the advantag es\nof these two structures for improving both the segmentation\nand the classiﬁcation performance. We combine CNN and\ntransformer in both encoder and decoder parts of the network.\nIn addition, we have shown that the proposed segmentation\nnetwork is beneﬁcial for the classiﬁcation task. Experimenta l\nresults have demonstrated that the proposed O-Net achieves\ncompetitive performance and good generalization ability in b oth\nthe segmentation and the classiﬁcation tasks.\nFrontiers in Neuroscience | www.frontiersin.org 10 June 2022 | Volume 16 | Article 876065\nWang et al. O-Net: Segmentation and Classiﬁcation\nTABLE 5 | Ablation study on the combination of CNN method and Swin Tran sformer method.\nEncoder Decoder\nEfﬁcient Swin CNN Swin transformer Dice ↑ HD↓ Aorta Gallbladder Kidney(L) Kidney(R) Liver Pancreas Sple en Stomach\nnet-block transformer decoder decoder\n✓ ✓ 78.86 28.86 87.72 62.19 83.11 76.67 94.49 56.61 89.48 80.58\n✓ ✓ 79.93 26.88 87.90 68.09 83.89 76.05 94.42 62.95 87.32 78.86\n✓ ✓ ✓ 80.34 22.53 88.67 67.38 83.95 77.01 95.12 60.06 88.76 81.77\n✓ ✓ 77.55 31.03 86.14 63.49 81.59 75.82 93.68 54.61 90.19 74.87\n✓ ✓ 79.13 21.55 85.47 66.53 83.28 79.61 94.29 56.58 90.66 76.60\n✓ ✓ ✓ 79.38 22.34 87.60 62.53 84.86 80.54 94.42 58.75 90.64 75.66\n✓ ✓ ✓ 79.47 29.19 87.71 66.21 81.64 74.69 94.65 61.61 89.19 80.02\n✓ ✓ ✓ 80.41 27.33 86.74 71.19 84.32 77.29 94.30 60.63 89.2 79.64\n✓ ✓ ✓ ✓ 80.61 21.04 88.36 67.45 84.44 77.13 95.24 61.52 90.03 80.74\nThe symbol ↑ means the higher value, the better.\nThe symbol ↓ means the lower value, the better.\nBold font to highlight the optimal values.\nTABLE 6 | Ablation study on learning rate and batch size.\nLearn rate Dice ↑ HD↓ Aorta Gallbladder Kidney(L) Kidney(R) Liver Pancreas Sple en Stomach\n1e-1 79.21 20.06 86.56 63.48 84.61 77.14 94.32 56.99 91.90 78.69\n5e-2 80.61 21.04 88.36 67.45 84.44 77.13 95.24 61.52 90.03 80.74\n1e-2 79.07 20.14 87.64 67.74 81.95 74.69 94.71 58.33 89.44 78 .03\n5e-3 79.76 23.07 88.18 68.51 83.60 76.92 94.42 58.84 88.59 79.03\n1e-3 76.57 30.37 85.28 62.96 81.61 74.51 92.96 54.13 86.37 84.70\nBatch size Dice ↑ HD↓ Aorta Gallbladder Kidney(L) Kidney(R) Liver Pancreas Sple en Stomach\n8 78.81 15.67 88.12 44.45 84.59 80.24 94.73 67.40 89.97 81.00\n16 78.36 18.25 88.69 38.12 84.57 79.46 95.16 66.20 91.44 83.27\n24 80.61 21.04 88.36 67.45 84.44 77.13 95.24 61.52 90.03 80.74\n32 80.35 27.93 88.32 66.70 81.94 76.19 95.31 64.06 88.94 81.27\nThe symbol ↑ means the higher value, the better.\nThe symbol ↓ means the lower value, the better.\nBold font to highlight the optimal values.\nDATA AVAILABILITY STATEMENT\nThe original contributions presented in the study are includ ed\nin the article/supplementary material, further inquiries ca n be\ndirected to the corresponding authors.\nAUTHOR CONTRIBUTIONS\nTW, JL, ZHa, ZHu, YH, YD, QG, MD, TT, and GC: concept\nand design. TW, JL, HZ, JW, MC, and TT: acquisition of\ndata. TW, JL, ZHa, ZHu, QG, and TT: model design. TW,\nJL, ZHa, ZHu, YH, YD, and TT: data analysis. TW, JL, ZHa,\nZHu, YH, YD, TT, and GC: manuscript drafting. TW, JL, ZHa,\nZHu, YH, YD, HZ, JW, MC, HJ, R-GL, QG, MD, TT, and GC:\napproval. All authors contributed to the article and approved th e\nsubmitted version.\nFUNDING\nThis work was supported in part by the National Natural Science\nFoundation of China under Grant Nos. 61901120 and 62171133,\nthe Science and Technology Program of Fujian Province of\nChina under Grant No. 2019YZ016006, and Health and Family\nPlanning Research Talent Training Program of Fujian Provinc e\nunder Grant No. 2020GGB009.\nREFERENCES\nCao, H., Wang, Y., Chen, J., Jiang, D., Zhang, X., Tian, Q., et al. (2021). Swin-unet:\nunet-like pure transformer for medical image segmentation. arXiv [Preprint]\narXiv:2105.05537. doi: 10.48550/arXiv.2105.05537\nChen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., et al. (2021 ). Transunet:\nTransformers make strong encoders for medical image segmentation. arXiv\n[Preprint] arXiv:2102.04306. doi: 10.48550/arXiv.2102.04306\nChen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., and Yuille, A. L. (2017).\nDeeplab: Semantic image segmentation with deep convolutional net s, atrous\nFrontiers in Neuroscience | www.frontiersin.org 11 June 2022 | Volume 16 | Article 876065\nWang et al. O-Net: Segmentation and Classiﬁcation\nconvolution, and fully connected crfs. IEEE Trans. Pattern Anal. Mach. Intell.\n40, 834–848. doi: 10.1109/TPAMI.2017.2699184\nChen, L.-C., Zhu, Y., Papandreou, G., Schroﬀ, F., and Adam, H. (201 8). “Encoder-\ndecoder with atrous separable convolution for semantic image segmen tation, ”\nin Proceedings of the European Conference on Computer Vision (E CCV)\n(Munich), 801–818. doi: 10.1145/3065386\nÇiçek, Ö., Abdulkadir, A., Lienkamp, S. S., Brox, T., and Ronnebe rger, O. (2016).\n“3d u-net: learning dense volumetric segmentation from sparse annot ation, ” in\nInternational Conference on Medical Image Computing and Computer-Assisted\nIntervention (Istanbul: Springer), 424–432.\nCodella, N. C., Gutman, D., Celebi, M. E., Helba, B., Marchetti, M. A ., Dusza, S.\nW., et al. (2018). “Skin lesion analysis toward melanoma detection: a challenge\nat the 2017 international symposium on biomedical imaging (isbi) , hosted\nby the international skin imaging collaboration (isic), ” in 2018 IEEE 15th\nInternational Symposium on Biomedical Imaging (ISBI 2018)(Washington, DC:\nIEEE), 168–172.\nDai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., et al. (2017). “ Deformable\nconvolutional networks, ” in Proceedings of the IEEE International Conference\non Computer Vision(Venice: IEEE), 764–773.\nDai, Y., Gao, Y., and Liu, F. (2021). Transmed: Transformers advan ce\nmulti-modal medical image classiﬁcation. Diagnostics 11, 1384.\ndoi: 10.3390/diagnostics11081384\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei , L. (2009). “Imagenet: A\nlarge-scale hierarchical image database, ” in 2009 IEEE Conference on Computer\nVision and Pattern Recognition(Miami, FL: IEEE), 248–255\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,\nUnterthiner, T., et al. (2020). An image is worth 16x16 words: transf ormers\nfor image recognition at scale. arXiv [Preprint] arXiv: 2010.11929.\ndoi: 10.48550/arXiv.2010.11929\nFu, H., Cheng, J., Xu, Y., Wong, D. W. K., Liu, J., and Cao, X. (2018 ).\nJoint optic disc and cup segmentation based on multi-label deep ne twork\nand polar transformation. IEEE Trans. Med. Imaging 37, 1597–1605.\ndoi: 10.1109/TMI.2018.2791488\nFu, S., Lu, Y., Wang, Y., Zhou, Y., Shen, W., Fishman, E., et al. ( 2020).\n“Domain adaptive relational reasoning for 3d multi-organ segmentat ion, ” in\nInternational Conference on Medical Image Computing and Computer-Assisted\nIntervention (Lima: Springer), 656–666.\nGehlot, S., and Gupta, A. (2021). “Self-supervision based dual-t ransformation\nlearning for stain normalization, classiﬁcation andsegmentation , ” in\nInternational Workshop on Machine Learning in Medical Imaging (Strasbourg:\nSpringer), 477–486.\nGehlot, S., Gupta, A., and Gupta, R. (2020). “Ednfc-net: Convolu tional neural\nnetwork with nested feature concatenation for nuclei-instanc e segmentation, ”\nin ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP)(Barcelona: IEEE), 1389–1393.\nGu, Z., Cheng, J., Fu, H., Zhou, K., Hao, H., Zhao, Y., et al. (2019 ). Ce-net:\nContext encoder network for 2d medical image segmentation. IEEE Trans.\nMed. Imaging38, 2281–2292. doi: 10.1109/TMI.2019.2903562\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). “Deep residual learn ing for image\nrecognition, ” in Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition(Las Vegas, NV: IEEE), 770–778.\nHoward, A., Sandler, M., Chu, G., Chen, L.-C., Chen, B., Tan, M., et al. (2019).\n“Searching for mobilenetv3, ” in Proceedings of the IEEE/CVF International\nConference on Computer Vision(Seoul: IEEE), 1314–1324.\nHu, H., Gu, J., Zhang, Z., Dai, J., and Wei, Y. (2018). “Relation n etworks for\nobject detection, ” in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition(Salt Lake City, UT: IEEE), 3588–3597.\nHu, H., Zhang, Z., Xie, Z., and Lin, S. (2019). “Local relation netw orks for\nimage recognition, ” in Proceedings of the IEEE/CVF International Conference\non Computer Vision, (Seoul) 3464–3473.\nHuang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. (2 017).\n“Densely connected convolutional networks, ” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition(Honolulu, HI: IEEE),\n4700–4708.\nHuang, H., Lin, L., Tong, R., Hu, H., Zhang, Q., Iwamoto, Y., et al. (2 020).\n“Unet 3+: a full-scale connected unet for medical image segmentatio n, ” in\nICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP)(Barcelona: IEEE), 1055–1059.\nJi, J., Lu, X., Luo, M., Yin, M., Miao, Q., and Liu, X. (2020). Para llel fully\nconvolutional network for semantic segmentation. IEEE Access 9, 673–682.\ndoi: 10.1109/ACCESS.2020.3042254\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagen et classiﬁcation\nwith deep convolutional neural networks. Adv. Neural Inf. Process. Syst. 25,\n1097–1105.\nKumar, P., Nagar, P., Arora, C., and Gupta, A. (2018). “U-segnet: f ully\nconvolutional neural network based automated brain tissue segmen tation tool, ”\nin 2018 25th IEEE International Conference on Image Processing (ICIP) (Athens:\nIEEE), 3503–3507.\nLi, X., Chen, H., Qi, X., Dou, Q., Fu, C.-W., and Heng, P.-A. (2018) .\nH-denseunet: hybrid densely connected unet for liver and tumor\nsegmentation from ct volumes. IEEE Trans. Med. Imaging 37, 2663–2674.\ndoi: 10.1109/TMI.2018.2845918\nLiang, D., Qiu, J., Wang, L., Yin, X., Xing, J., Yang, Z., et al. (2 020).\nCoronary angiography video segmentation method for assisting\ncardiovascular disease interventional treatment. BMC Med. Imaging 20,\n1–8. doi: 10.1186/s12880-020-00460-9\nLiang, J., Yang, C., Zeng, M., and Wang, X. (2022). Transconver : transformer\nand convolution parallel network for developing automatic brain tumor\nsegmentation in mri images. Quant. Imaging Med. Surg . 12, 2397.\ndoi: 10.21037/qims-21-919\nLin, T.-Y., Dollár, P., Girshick, R., He, K., Hariharan, B., and Be longie, S. (2017).\n“Feature pyramid networks for object detection, ” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition(Honolulu, HI: IEEE),\n2117–2125.\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., et al. (2021) . Swin\ntransformer: Hierarchical vision transformer using shifted window s.\narXiv [Preprint] arXiv: 2103.14030. doi: 10.1109/ICCV48922.2021.\n00986\nMilletari, F., Navab, N., and Ahmadi, S.-A. (2016). “V-net: fully c onvolutional\nneural networks for volumetric medical image segmentation, ” in 2016 Fourth\nInternational Conference on 3D Vision (3DV)(Stanford, CA: IEEE), 565–571.\nOktay, O., Schlemper, J., Folgoc, L. L., Lee, M., Heinrich, M., Misa wa, K., et\nal. (2018). Attention u-net: learning where to look for the pancreas . arXiv\n[Preprint] arXiv:1804.03999. doi: 10.48550/arXiv.1804.03999\nPatil, D. D., and Deore, S. G. (2013). Medical image segmentation : a review. Int. J.\nComput. Sci. Mobile Comput. 2, 22–27.\nQin, X., Zhang, Z., Huang, C., Dehghan, M., Zaiane, O. R., and Ja gersand,\nM. (2020). U2-net: Going deeper with nested u-structure for salie nt\nobject detection. Pattern Recognit. 106, 107404. doi: 10.1016/j.patcog.2020.10\n7404\nRonneberger, O., Fischer, P., and Brox, T. (2015). “U-net: con volutional networks\nfor biomedical image segmentation, ” in International Conference on Medical\nImage Computing and Computer-Assisted Intervention (Munich: Springer),\n234–241.\nSandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C. ( 2018).\n“Mobilenetv2: inverted residuals and linear bottlenecks, ” in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition(Salt Lake City,\nUT: IEEE), 4510–4520.\nSchlemper, J., Oktay, O., Schaap, M., Heinrich, M., Kainz, B., Glock er,\nB., et al. (2019). Attention gated networks: Learning to leverage\nsalient regions in medical images. Med. Image Anal . 53, 197–207.\ndoi: 10.1016/j.media.2019.01.012\nSimonyan, K., and Zisserman, A. (2014). Very deep convolutional n etworks\nfor large-scale image recognition. arXiv [Preprint] arXiv: 1409.1556.\ndoi: 10.48550/arXiv.1409.1556\nSun, K., Xiao, B., Liu, D., and Wang, J. (2019). “Deep high-reso lution\nrepresentation learning for human pose estimation, ” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (Long\nBeach, CA: IEEE), 5693–5703.\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., et al. (2015).\n“Going deeper with convolutions, ” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition(Boston, MA: IEEE), 1–9.\nSzegedy, C., Vanhoucke, V., Ioﬀe, S., Shlens, J., and Wojna, Z . (2016). “Rethinking\nthe inception architecture for computer vision, ” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition(Las Vegas, NV ,: IEEE),\n2818–2826.\nFrontiers in Neuroscience | www.frontiersin.org 12 June 2022 | Volume 16 | Article 876065\nWang et al. O-Net: Segmentation and Classiﬁcation\nTan, M., and Le, Q. (2019). “Eﬃcientnet: rethinking model scali ng for\nconvolutional neural networks, ” in International Conference on Machine\nLearning (Taiyuan: PMLR), 6105–6114.\nTouvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jé gou, H. (2021).\n“Training data-eﬃcient image transformers distillation through at tention, ” in\nInternational Conference on Machine Learning(PMLR), 10347–10357.\nVasuda, P., and Satheesh, S. (1713). Improved fuzzy c-means algori thm for mr\nbrain image segmentation. Int. J. Comput. Sci. Eng. 2, 2010.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). “Attention is all you need, ” inAdvances in Neural Information Processing\nSystems, (Long Beach, CA), 5998–6008.\nWang, H., Xie, S., Lin, L., Iwamoto, Y., Han, X.-H., Chen, Y.-W., et al. (2021a).\nMixed transformer u-net for medical image segmentation. arXiv [Preprint]\narXiv:2111.04734. doi: 10.1109/ICASSP43922.2022.9746172\nWang, R., Cao, S., Ma, K., Zheng, Y., and Meng, D. (2021b). Pai rwise\nlearning for medical image segmentation. Med. Image Anal. 67, 101876.\ndoi: 10.1016/j.media.2020.101876\nWang, R., Zhou, Y., Zhao, C., and Wu, H. (2015). A hybrid ﬂower\npollination algorithm based modiﬁed randomized location for multi-\nthreshold medical image segmentation. Biomed. Mater. Eng. 26, S1345-S1351.\ndoi: 10.3233/BME-151432\nWang, X., Girshick, R., Gupta, A., and He, K. (2018). “Non-local n eural networks, ”\nin Proceedings of the IEEE Conference on Computer Vision and Pat tern\nRecognition (Salt Lake City, UT: IEEE), 7794–7803.\nXiao, X., Lian, S., Luo, Z., and Li, S. (2018). “Weighted res-une t for high-\nquality retina vessel segmentation, ” in 2018 9th International Conference on\nInformation Technology in Medicine and Education (ITME)(Hangzhou,: IEEE),\n327–331.\nXie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J. M., and Luo, P.\n(2021). “Segformer: simple and eﬃcient design for semantic segment ation\nwith transformers, ” in Advances in Neural Information Processing Systems. 34.\ndoi: 10.48550/arXiv.2105.15203\nXie, S., Girshick, R., Dollár, P., Tu, Z., and He, K. (2017). “Aggre gated residual\ntransformations for deep neural networks, ” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition(Honolulu, HI: IEEE),\n1492–1500.\nXu, X., Feng, Z., Cao, C., Li, M., Wu, J., Wu, Z., et al. (2021). An\nimproved swin transformer-based model for remote sensing object dete ction\nand instance segmentation. Remote Sens . 13, 4779. doi: 10.3390/rs132\n34779\nZhang, Y., Liu, H., and Hu, Q. (2021). “Transfuse: fusing transfo rmers and\ncnns for medical image segmentation, ” in International Conference on Medical\nImage Computing and Computer-Assisted Intervention(Strasbourg: Springer),\n14–24.\nZhou, Z., Siddiquee, M. M. R., Tajbakhsh, N., and Liang, J. (2 018). “Unet++: a\nnested u-net architecture for medical image segmentation, ” in Deep Learning in\nMedical Image Analysis and Multimodal Learning for Clinical Decision Support\n(Granada: Springer), 3–11.\nZhu, X., Hu, H., Lin, S., and Dai, J. (2019). “Deformable convnets v2 : more\ndeformable, better results, ” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition(Long Beach, CA: IEEE), 9308–9316.\nConﬂict of Interest: QG and TT were employed by Imperial Vision Technology.\nThe remaining authors declare that the research was conducted in t he absence of\nany commercial or ﬁnancial relationships that could be construed as a potential\nconﬂict of interest.\nPublisher’s Note:All claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their aﬃliated organizat ions, or those of\nthe publisher, the editors and the reviewers. Any product that may b e evaluated in\nthis article, or claim that may be made by its manufacturer, is not gua ranteed or\nendorsed by the publisher.\nCopyright © 2022 Wang, Lan, Han, Hu, Huang, Deng, Zhang, Wang, Chen,Jiang,\nLee, Gao, Du, Tong and Chen. This is an open-access article distributed under the\nterms of the Creative Commons Attribution License (CC BY). The use, distribution\nor reproduction in other forums is permitted, provided the original author(s) and\nthe copyright owner(s) are credited and that the original publication in this journal\nis cited, in accordance with accepted academic practice. Nouse, distribution or\nreproduction is permitted which does not comply with these terms.\nFrontiers in Neuroscience | www.frontiersin.org 13 June 2022 | Volume 16 | Article 876065",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7993712425231934
    },
    {
      "name": "Segmentation",
      "score": 0.7589589357376099
    },
    {
      "name": "Encoder",
      "score": 0.7298312783241272
    },
    {
      "name": "Convolutional neural network",
      "score": 0.675776481628418
    },
    {
      "name": "Deep learning",
      "score": 0.6443853378295898
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6441817879676819
    },
    {
      "name": "Transformer",
      "score": 0.5863257646560669
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5541589856147766
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.45297709107398987
    },
    {
      "name": "Image segmentation",
      "score": 0.41160207986831665
    },
    {
      "name": "Machine learning",
      "score": 0.3831949234008789
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I80947539",
      "name": "Fuzhou University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I129708740",
      "name": "Fujian Medical University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210148548",
      "name": "Fujian Provincial Cancer Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I118292597",
      "name": "National Taipei University of Technology",
      "country": "TW"
    }
  ],
  "cited_by": 45
}