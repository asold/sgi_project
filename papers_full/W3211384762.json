{
  "title": "Parameter-Efficient Domain Knowledge Integration from Multiple Sources for Biomedical Pre-trained Language Models",
  "url": "https://openalex.org/W3211384762",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2786691325",
      "name": "Qiuhao Lu",
      "affiliations": [
        "University of Oregon"
      ]
    },
    {
      "id": "https://openalex.org/A2040419331",
      "name": "Dejing Dou",
      "affiliations": [
        "University of Oregon",
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2222712748",
      "name": "Thien Huu Nguyen",
      "affiliations": [
        "University of Oregon"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2115871101",
    "https://openalex.org/W2963211188",
    "https://openalex.org/W3005441132",
    "https://openalex.org/W3153675281",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2970986790",
    "https://openalex.org/W3093553144",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W3035153870",
    "https://openalex.org/W3214715529",
    "https://openalex.org/W2983915252",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W3026990524",
    "https://openalex.org/W2987154291",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W3037965442",
    "https://openalex.org/W3099793224",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2888120268",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3113280695",
    "https://openalex.org/W2887528103",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2972167903",
    "https://openalex.org/W2952867657",
    "https://openalex.org/W2801930304",
    "https://openalex.org/W2169099542",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2335791510",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2964242047",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3105892552",
    "https://openalex.org/W3172427031",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2937845937",
    "https://openalex.org/W3117339789",
    "https://openalex.org/W4206178588",
    "https://openalex.org/W3114916066",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W3100353583",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2925863688"
  ],
  "abstract": "Domain-specific pre-trained language models (PLMs) have achieved great success over various downstream tasks in different domains. However, existing domain-specific PLMs mostly rely on self-supervised learning over large amounts of domain text, without explicitly integrating domain-specific knowledge, which can be essential in many domains. Moreover, in knowledge-sensitive areas such as the biomedical domain, knowledge is stored in multiple sources and formats, and existing biomedical PLMs either neglect them or utilize them in a limited manner. In this work, we introduce an architecture to integrate domain knowledge from diverse sources into PLMs in a parameter-efficient way. More specifically, we propose to encode domain knowledge via adapters, which are small bottleneck feed-forward networks inserted between intermediate transformer layers in PLMs. These knowledge adapters are pre-trained for individual domain knowledge sources and integrated via an attention-based knowledge controller to enrich PLMs. Taking the biomedical domain as a case study, we explore three knowledge-specific adapters for PLMs based on the UMLS Metathesaurus graph, the Wikipedia articles for diseases, and the semantic grouping information for biomedical concepts. Extensive experiments on different biomedical NLP tasks and datasets demonstrate the benefits of the proposed architecture and the knowledge-specific adapters across multiple PLMs.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3855–3865\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n3855\nParameter-Efﬁcient Domain Knowledge Integration from Multiple\nSources for Biomedical Pre-trained Language Models\nQiuhao Lu1, Dejing Dou1,2, and Thien Huu Nguyen1\n1Dept. of Computer and Information Science, University of Oregon, Eugene, OR, USA\n2Baidu Research\n{luqh, dou, thien}@cs.uoregon.edu\nAbstract\nDomain-speciﬁc pre-trained language mod-\nels (PLMs) have achieved great success over\nvarious downstream tasks in different do-\nmains. However, existing domain-speciﬁc\nPLMs mostly rely on self-supervised learning\nover large amounts of domain text, without\nexplicitly integrating domain-speciﬁc knowl-\nedge, which can be essential in many do-\nmains. Moreover, in knowledge-sensitive ar-\neas such as the biomedical domain, knowl-\nedge is stored in multiple sources and for-\nmats, and existing biomedical PLMs either ne-\nglect them or utilize them in a limited man-\nner. In this work, we introduce an architecture\nto integrate domain knowledge from diverse\nsources into PLMs in a parameter-efﬁcient\nway. More speciﬁcally, we propose to en-\ncode domain knowledge via adapters, which\nare small bottleneck feed-forward networks in-\nserted between intermediate transformer lay-\ners in PLMs. These knowledge adapters are\npre-trained for individual domain knowledge\nsources and integrated via an attention-based\nknowledge controller to enrich PLMs. Tak-\ning the biomedical domain as a case study,\nwe explore three knowledge-speciﬁc adapters\nfor PLMs based on the UMLS Metathesaurus\ngraph, the Wikipedia articles for diseases, and\nthe semantic grouping information for biomed-\nical concepts. Extensive experiments on differ-\nent biomedical NLP tasks and datasets demon-\nstrate the beneﬁts of the proposed architecture\nand the knowledge-speciﬁc adapters across\nmultiple PLMs.\n1 Introduction\nIn the past few years, large pre-trained language\nmodels (PLMs) have demonstrated superior per-\nformance over various downstream tasks in nat-\nural language processing (NLP), such as BERT\n(Devlin et al., 2019), RoBERTa (Liu et al., 2019),\nALBERT (Lan et al., 2019), GPT-3 (Brown et al.,\n2020), etc. These PLMs mainly depend on self-\nsupervised pre-training on large amounts of textual\ndata, e.g., Wikipedia, and can be conveniently ap-\nplied to downstream tasks via ﬁne-tuning. Despite\nthe great success of these general PLMs, their per-\nformance over domain-speciﬁc texts is relatively\npoor due to domain shifts (Ma et al., 2019). Con-\nsequently, recent studies construct domain-speciﬁc\nPLMs through ﬁne-tuning or pre-training from\nscratch over domain corpora, such as BioBERT\n(Lee et al., 2020), ClinicalBERT (Huang et al.,\n2019), SciBERT (Beltagy et al., 2019), etc.\nSince these PLMs are mostly pre-trained on\nunstructured free texts, a common issue among\nthe aforementioned general and domain-speciﬁc\nPLMs is their lack of speciﬁc structured knowledge,\nwhich results in their incompetence on knowledge-\ndriven tasks (Rogers et al., 2020). For instance,\nsome studies point out PLMs are insufﬁcient to\nwell capture factual knowledge from text (Poerner\net al., 2019; Wang et al., 2020, 2021).\nTo enrich PLMs with external knowledge, some\nefforts have been made recently (Yao et al., 2019;\nZhang et al., 2019; Kim et al., 2020; Levine et al.,\n2020; Wang et al., 2021). A common theme\namong these approaches is the incorporation of\nan auxiliary knowledge-driven training objective.\nFor instance, KG-BERT (Yao et al., 2019) inte-\ngrates world/factual knowledge from Wikipedia\nvia knowledge graph completion; KEPLER (Wang\net al., 2021) introduces a Knowledge Embedding\nobjective and combine it with the language model-\ning objective for joint optimization. Despite the im-\nproved performance of these knowledge-enriched\nPLMs over downstream tasks, there are three lim-\nitations. First, these approaches, either training\nfrom scratch or ﬁne-tuning over off-the-shelf check-\npoints, need to optimize the entire model, which\nis quite expensive. Second, they mostly focus on\nsingle-source knowledge incorporation, e.g., an en-\ncyclopedia, and neglect knowledge from multiple\nsources. This limits the utilization of potential\nknowledge, especially for knowledge-sensitive ar-\n3856\neas such as the biomedical domain where knowl-\nedge is stored in multiple sources and formats (Jin\net al., 2019; Lee et al., 2020). Third, most of ex-\nisting knowledge integration approaches focus on\ngeneral domain knowledge, while domain knowl-\nedge infusion for PLMs is underexplored.\nTo address these limitations, we propose to per-\nform knowledge integration for PLMs via adapters\n(Rebufﬁ et al., 2017; Houlsby et al., 2019; Pfeiffer\net al., 2020, 2021; Wang et al., 2020). Basically,\nadapters are lightweight neural networks that are\nplaced inside PLMs. When ﬁne-tuning a PLM, the\noriginal parameters of the PLM are ﬁxed and only\nthe adapters are ﬁne-tuned. This makes adapters\na parameter-efﬁcient alternative to full model ﬁne-\ntuning. Another beneﬁt of adapters is their inde-\npendent nature, where multiple adapters can be\ntrained independently without interfering with each\nother. As such, we propose to enrich PLMs with\nadapters that are independently pre-trained for dif-\nferent sources of domain knowledge.\nIn this paper, we propose an architecture that\naims to integrate domain knowledge from multi-\nple sources via knowledge-speciﬁc adapters to en-\nrich PLMs. We take the biomedical domain as\na case study, as it is a knowledge-sensitive area\nwhere domain-knowledge is essential for various\nNLP applications. Speciﬁcally, we explore three\nknowledge-speciﬁc adapters for PLMs based on\nthe UMLS Metathesaurus graph, the Wikipedia\narticles for diseases, and the semantic grouping\ninformation for biomedical concepts. We also in-\ncorporate an attention-based knowledge controller\nmodule that aims to adaptively adjust the activa-\ntion levels of the adapters, which also brings some\nexplainability as it shows the importance of the\nadapters for a task. The experimental results show\nthat by equipping PLMs with domain knowledge\nfrom multiple sources via the proposed architec-\nture, their overall performance gets consistently\nimproved across tasks and datasets. Moreover, the\npre-trained adapters can be directly integrated with\nmultiple PLMs, demonstrating transferability of\nthe architecture.\nThe contributions of this work can be summa-\nrized as follows:\n• We propose a novel architecture that in-\ncorporates Diverse Adapters for Knowledge\nIntegration (DAKI) into PLMs. It integrates\ndomain knowledge from multiple sources\nadaptively via an attention-based knowledge\ncontroller. The architecture demonstrates ef-\nfectiveness, transferability, explainability as\nwell as parameter-efﬁciency in experiments.\n• Taking the biomedical domain as a case study,\nwe speciﬁcally investigate and pre-train three\nknowledge adapters based on the UMLS\nMetathesaurus graph, the Wikipedia articles\nfor diseases, and the semantic grouping in-\nformation for biomedical concepts. Such\nadapters serve as off-the-shelf modules and\ncan be used in a plug-and-play manner via\nDAKI.\n• Extensive experiments on different biomedi-\ncal NLP tasks and datasets demonstrate the\nbeneﬁts of the proposed knowledge-speciﬁc\nadapters and DAKI.\n2 Related Work\nThis study is essentially related to two lines of\nresearch: knowledge integration for PLMs and\ndomain-speciﬁc PLMs (biomedical PLMs in par-\nticular).\nThere has been a surge of research on knowledge\ninjection for PLMs in recent years (Yao et al., 2019;\nZhang et al., 2019; Peters et al., 2019; Kim et al.,\n2020; Levine et al., 2020; Lauscher et al., 2020;\nPereira et al., 2020; Sun et al., 2020; He et al.,\n2020a; Wang et al., 2021). These studies aim to\nintegrate knowledge from an external knowledge\nsource, e.g., Wikipedia, into PLMs by augment-\ning the training objective with a knowledge-driven\nregularization. As mentioned above, these meth-\nods are limited in the sense that they mostly fo-\ncus on single-source knowledge, and require full\nmodel training. K-adapter (Wang et al., 2020) ad-\ndresses some of these issues by introducing lin-\nguistic and factual adapters into RoBERTa, but the\nadapters are treated equally in their work. Also,\ngeneral domain knowledge, such as factual knowl-\nedge (Zhang et al., 2019; Sun et al., 2020; He et al.,\n2020a; Wang et al., 2021), commonsense knowl-\nedge (Lauscher et al., 2020; Pereira et al., 2020),\nand linguistic knowledge (Levine et al., 2020) are\nprioritized in these studies, while domain knowl-\nedge is somewhat underexplored (Michalopoulos\net al., 2020).\nBiomedical NLP continues to be an active area of\nresearch in the past few years. There have been sev-\neral biomedical PLMs proposed and have proven\nto be successful in various domain tasks (Lee\n3857\net al., 2020; Peng et al., 2019; Huang et al., 2019;\nAlsentzer et al., 2019). As variants of BERT (De-\nvlin et al., 2019) in the biomedical domain, these\nPLMs are mostly pre-trained on large amounts of\ndomain-speciﬁc corpora, such as the PubMed texts\n(Peng et al., 2019; Lee et al., 2020) and clinical\nnotes (Huang et al., 2019; Alsentzer et al., 2019),\nand do not explicitly incorporate domain knowl-\nedge in the pre-training stage.\nThis work differs from the aforementioned stud-\nies in that we are the ﬁrst to integrate biomedical\ndomain-speciﬁc knowledge from multiple sources\ninto PLMs via an adapter-based architecture. The\nknowledge integration process is ﬂexible, efﬁcient\nand transferable.\n3 Diverse Adapters for Knowledge\nIntegration (DAKI)\nIn this section, we introduce a mechanism, i.e.,\nDAKI, that encodes domain knowledge from di-\nverse sources into PLMs via knowledge-speciﬁc\nadapters. We ﬁrst introduce the adapter module\nalong with the overall architecture of DAKI, and\nthen discuss the knowledge-speciﬁc adapters for\nthe biomedical domain. In the end we explain the\nattention-based knowledge controller that is lever-\naged to adaptively integrate these adapters.\n3.1 Pre-trained Language Models with\nAdapters\nAdapter An adapter module is a simple and\nlightweight neural network placed within a large\npre-trained base model, and in NLP the base model\nis usually a pre-trained language model such as\nBERT (Devlin et al., 2019). Generally, adapters\nare placed in or between the intermediate trans-\nformer layers in a PLM, and the placement de-\nﬁnes two paradigms. One puts the adapters in-\nside the intermediate transformer layers (Houlsby\net al., 2019; Pfeiffer et al., 2020, 2021), and the\nother puts the adapter between and outside the in-\ntermediate transformer layers (Wang et al., 2020).\nIn this work, we choose the latter paradigm for\nits ﬂexibility and extensibility, as shown in Fig-\nure 2. Instead of updating the entire language\nmodel, only the adapters are updated during ﬁne-\ntuning on downstream tasks. This strategy demon-\nstrates parameter-efﬁciency and scalability while\nachieving similar performance to full ﬁne-tuning,\nand has been actively explored as an alternative for\ntransfer learning in recent NLP studies (Houlsby\nFigure 1: Adapter module.\net al., 2019; Pfeiffer et al., 2020, 2021; Wang et al.,\n2020; Rücklé et al., 2020).\nIn this work, we leverage a simple yet effec-\ntive bottleneck feed-forward network as the adapter\nmodule. Essentially, the adapter module consists\nof a residual connection and two projection layers\nwith LeakyReLU as the activation, as shown in\nFigure 1. The size of adapters is controlled by the\nbottleneck, and is usually much smaller than that\nof the base PLM, i.e., dbottleneck ≪dPLM, where\ndPLM refers to the dimension of hidden-states in\nthe base PLM. In our case, the bottleneck dimen-\nsion is set to 128 for all experiments. Note that\na more complex adapter is possible, such as two\nprojection layers along with a stack of transformer\nlayers (Wang et al., 2020), but at the cost of efﬁ-\nciency.\nArchitecture Figure 2 illustrates the overall ar-\nchitecture of DAKI. Essentially, the architecture\ncontains three main components, i.e., the base\nPLM, the knowledge-speciﬁc adapters, and the\nadapter integration module. DAKI theoretically\nsupports any transformer-based structure as the\nbase PLM, such as BERT (Devlin et al., 2019),\nALBERT (Lan et al., 2019), RoBERTa (Liu et al.,\n2019), etc. Each knowledge-speciﬁc adapter con-\ntains several adapter modules and they are inserted\nat certain layers of the base PLM. Each adapter\nmodule takes as input the addition of the hidden-\nstates of the transformer layer and the output of the\nprevious adapter module. The adapter modules do\nnot share weights with each other. Motivated by the\nfact that knowledge from different sources should\n3858\nFigure 2: Architecture of DAKI. CTRL refers to the knowledge controller. Linear layers are omitted for simplicity.\nhave different level of activation over downstream\ntasks, we incorporate a knowledge controller to\nadaptively integrate the knowledge adapters. De-\ntails are explained in Section 3.3.\nWhen pre-training an adapter, we take the addi-\ntion of the output of the last adapter module and\nthe last-hidden-states of the base PLM as the ﬁnal\noutput, and use it for the pre-training task. Note\nthat during adapter pre-training, the knowledge\ncontroller is dropped and the base PLM is frozen.\nWhen applying DAKI to downstream tasks, we take\nthe addition of the output of the knowledge con-\ntroller and the last-hidden-states of the base PLM\nas the ﬁnal output, and use it for the downstream\ntask.\nThe beneﬁts of this architecture is threefold.\nFirst, adapters are independent and do not inter-\nact during pre-training, which means they have\nperfect memory of the knowledge, thus avoiding\nthe forgetting issue in multi-task learning. Second,\nit demonstrates ﬂexibility and extensibility as it is\neasy to remove, add or replace the adapters. Third,\nthe usage of DAKI is as simple as a general PLM,\nsince its output can be considered the last-hidden-\nstates of a PLM.\nIn this work, we use ALBERT-xxlarge-v2 (Lan\net al., 2019) as the base PLM. We investigate three\nknowledge-speciﬁc adapters based on the UMLS\nMetathesaurus graph, the Wikipedia articles for\ndiseases, and the semantic grouping information\nfor biomedical concepts. Details are explained in\nAdapter Source Size Format\nKG UMLS Metathesaurus 1,772,248 ( h,r,t)\nDS Wikipedia 14,617 x\nSG Semantic Network 333,005 ( x,y)\nTable 1: Statistics of the datasets for pre-training KG,\nDS, SG. The formats are triples, passages, and textual\ndeﬁnitions with labels, respectively.\nSection 3.2. Each adapter contains three adapter\nmodules and they are placed at layers {0,5,11}.\nNote that the number and placement of adapter\nmodules can be ﬂexible, and in this study we follow\nthe same strategy with (Wang et al., 2020) where\nthree modules are distributed at the bottom, middle,\nand top layer.\n3.2 Adapters Pre-training\nIn this work, we investigate three independent\nadapters based on three sources of knowledge,\ni.e., the UMLS Metathesaurus knowledge graph\n(KG), the Wikipedia articles for diseases (DS),\nand the semantic grouping information for med-\nical concepts (SG). The statistics of the correspond-\ning datasets for pre-training are shown in Table 1.\nThese knowledge-speciﬁc adapters serve as exam-\nples for encoding domain knowledge from various\nsources, and can be easily extended or replaced\nwith alternative knowledge sources. For clarity, we\nuse PLM-KG, PLM-DS and PLM-SG to denote the\n3859\nmodel that is used to pre-train the adapters in this\nsection.\n3.2.1 Knowledge Graph Adapter (KG)\nKnowledge graphs encode real-world knowledge in\nthe form of triples, i.e., (h, r, t) where h and t refer\nto the head and tail entity and r is the relation be-\ntween them. Knowledge graphs have been actively\nexplored in recent studies of language model pre-\ntraining or ﬁne-tuning, as they reveal the relation-\nships between real-world entities that are hidden\nfrom surface texts.\nTo leverage the knowledge encoded in the\nUMLS Metathesaurus graph 1, we pre-train an\nadapter that aims to capture the connectivity pat-\nterns between medical entities through knowledge\ngraph completion. More speciﬁcally, we treat the\ntriples in UMLS as textual sequences and feed them\ninto the PLM-KG encoder. Then the representation\nof the triple is used as input to a binary classiﬁca-\ntion layer for plausibility prediction.\nIn particular, given a triple (h, r, t), we ﬁrst con-\nvert it to a textual sequence by concatenating the\nwords in the names of h, r, and t. For example, for\na triple (diffuse adenocarcinoma of the stomach,\ndisease has normal tissue origin, gastric mucosa),\nthe constructed input sequence is:\n[CLS] diffuse adenocarcinoma of the stomach[SEP] dis-\nease has normal tissue origin [SEP] gastric mucosa [SEP]\nWe then use the PLM-KG model to encode the\nsequence, and use the representation for the[CLS]\ntoken in the last layer to predict the plausibility of\nthe triple, i.e., determining whether the triple is\nvalid or not. The adapter parameters in this model\nare optimized with a binary cross-entropy loss:\nLKG = −\n∑\nt∈{T+∪T−}\n(ylog ˆy1 + (1 −y) log ˆy0)\n(1)\nwhere yis the ground-truth label and ˆy0,ˆy1 refer\nto the output prediction probabilities. T+ and T−\nare the positive and negative triple set. Here, the\nnegative set T− is constructed by replacing the\nhead or tail entity in a positive triple with a random\nentity.\n3.2.2 Disease Adapter (DS)\nIt is crucial to equip pre-trained language models\nwith disease knowledge for medical NLP tasks, as\nit bridges the gap between disease terms and their\n1The data is available at https://www.nlm.nih.\ngov/research/umls.\ntextual descriptions. For example, in the medical\nnatural language inference task (NLI), the premise-\nhypothesis pair (No history of blood clots or DVTs\nhas never had chest pain prior to one week ago,\nPatient has angina) is more likely to be correctly\nclassiﬁed as entailment if the model speciﬁ-\ncally knows that angina refers to chest pain.\nTo leverage the disease knowledge, we pre-train\nan adapter that aims to infer disease names based\non their textual descriptions. More speciﬁcally, for\neach disease, a new passage is formed by collecting\nthe textual content from its Wikipedia article2. We\nthen randomly substitute 75% of the disease terms\nin the passage with [MASK] in the passage and op-\ntimize the PLM-DS model via a masked language\nmodeling (MLM) objective.\nFormally, let Π = {π1,π2,...,π K}denote the\nindexes of the masked tokens in the passage T,\nwhere K is the number of masked tokens. Then\nTΠ and T¬Π represent the set of masked and ob-\nserved tokens in the passage, respectively. Then\nthe training objective for the adapter parameters is\ndescribed as:\nLDS = Lmlm(TΠ|T¬Π) = −1\nK\nK∑\nk=1\nlog p(tπk |T¬Π)\n(2)\nwhere p(tπk |T¬Π) is the probability of predicting\ntπk given the unmasked tokens T¬Π, estimated by\na softmax layer.\n3.2.3 Semantic Grouping Adapter (SG)\nTo provide a proper and consistent categorization\nof concepts in the Metathesaurus, the UMLS Se-\nmantic Network groups concepts according to the\nsemantic types that have been assigned to them.\nEach concept is assigned to at least one seman-\ntic type from a total of 127 semantic types. For\ncertain purposes, however, a coarser-grained cat-\negorization is desirable, and hence the semantic\ntypes are aggregated into 15 semantic groupings\n(McCray et al., 2001). Such aggregation ensures\nthe semantic coherence between concepts in the\nsame group3. This property would help pre-trained\nlanguage models capture the connectivity between\nmedical concepts, as well as between their descrip-\ntive texts.\nTo leverage the semantic grouping information,\nwe pre-train an adapter that aims to predict the se-\n2This data is proposed by (He et al., 2020b).\n3The data is available at https://\nsemanticnetwork.nlm.nih.gov.\n3860\nDatasets MEDIQA-2019 TRECQA-2017 MEDNLI BC5CDR NCBI\nMetrics(%) Accuracy MRR Precision Accuracy MRR Precision Accuracy F1 F1\nBERT 64.95 82.72 66.49 74.61 56.17 52.55 75.95 83.09 85.14\nClinicalBERT 67.30 84.78 70.59 77.00 52.56 56.62 81.50 84.90 87.25\nSciBERT 68.47 84.47 68.07 77.23 54.57 57.54 80.94 86.16 87.24\nBioBERT 68.29 83.61 72.78 77.12 49.84 57.25 81.86 85.99 87.70\ndiseaseBERT 66.40 83.33 68.94 75.33 56.41 54.01 77.29 83.47 86.81\numlsBERT 62.87 83.91 63.62 70.20 54.17 46.69 81.65 84.54 86.23\nRoBERTa 72.49 86.74 74.67 75.33 51.76 54.01 81.65 83.04 85.83\nALBERT 76.54 88.46 81.41 75.09 58.57 53.03 85.48 84.28 87.56\ndiseaseALBERT 79.49 90.00 84.02 80.10 57.21 62.40 86.15 84.71 87.69\nDAKI-BERT 69.47 85.06 70.17 77.95 54.65 58.27 77.85 83.43 85.67\nDAKI-BioBERT 72.54 87.33 77.46 78.55 54.17 59.04 83.41 86.51 89.01\nDAKI-RoBERTa 73.98 89.22 76.39 77.23 51.92 58.48 81.65 83.36 86.01\nDAKI-ALBERT 80.22 91.22 84.36 80.33 58.65 62.31 86.85 84.86 87.86\nTable 2: Performance of DAKI over downstream tasks QA, NLI and NER.\nmantic groupings of concepts in UMLS based on\ntheir textual deﬁnitions. More speciﬁcally, for a\nUMLS concept with corresponding textual deﬁni-\ntion, we encode the deﬁnition with the PLM-SG\nmodel and feed the [CLS] representation into a\nlinear layer for classiﬁcation. The model is opti-\nmized with cross-entropy loss:\nLSG = −\n15∑\ni=1\nyilog ˆyi (3)\nwhere yi is the ground-truth label and ˆyi refers to\nthe output prediction probabilities.\n3.3 Knowledge Controller\nThe knowledge controller is essentially a separate\nadapter with additional linear layers, which is dis-\ntributed at the same layers with the knowledge\nadapters, as shown in Figure 2. This module aims\nto adaptively integrate the knowledge adapters by\nassigning them different importance weights, as\nopposed to simple concatenation of the outputs of\nadapters (Wang et al., 2020). At each layer iwhere\na adapter module is placed, three linear transfor-\nmation modules are employed, i.e., Qi,Ki,Vi, as\nmotivated by (Vaswani et al., 2017). Essentially,\nQi takes the hidden-states of the controller as the\ninput, and the output is considered as the query\nsignal. Ki in contrast takes the hidden-states of the\nadapters as the input, and the output serves as the\nkey signal. The value signal is the hidden-states of\nthe adapters. Then the attention weights are com-\nputed for each adapter and the weighted sum of\nthe hidden-states of adapters are fed into Vi, the\noutput of which is regarded as the ﬁnal output of\nthe knowledge controller at layer i:\nQi = WQi HCi + bQi\nKi = WKi HDi + bKi\nAi = softmax(QiKT\ni )HDi\nZi = WVi Ai + bVi\n(4)\nwhere HCi are the hidden-states of the con-\ntroller and HDi are the concatenation of\nthe hidden-states of the adapters at layer i.\nWQi ,bQi ,WKi ,bKi ,WVi ,bVi are trainable pa-\nrameters of the linear modules at each layer.\n4 Experiments\nIn this section, we evaluate the DAKI architecture\nover three knowledge-driven downstream tasks in\nbiomedical NLP, where we aim to show the effec-\ntiveness of the knowledge integration method. We\nalso investigate some desirable properties of the\narchitecture.\n4.1 Setup\n4.1.1 Downstream tasks\nWe perform evaluation over three knowledge-\ndriven biomedical NLP tasks, i.e., Question An-\nswering (QA), Natural Language Inference (NLI)\nand Named Entity Recognition (NER)4.\n4The datasets for downstream tasks are avail-\nable at https://github.com/heyunh2015/\ndiseaseBERT.\n3861\nDatasets MEDIQA-2019 TRECQA-2017 MEDNLI BC5CDR NCBI A.P C.PMetrics(%) Acc MRR Pre Acc MRR Pre Acc F1 F1\nDAKI 80.22 91.22 84.36 80.33 58.65 62.31 86.85 84.86 87.86 79.63 -\nw/o ctrl 78.32 88.27 81.68 79.38 56.09 61.19 86.78 84.58 86.99 78.14 -1.49\nw/o KG 79.49 90.72 85.42 80.45 57.85 62.74 85.94 83.93 87.43 79.33 -0.30\nw/o DS 78.86 89.61 82.37 79.62 57.85 61.53 85.86 83.99 87.82 78.61 -1.02\nw/o SG 73.15 86.33 80.77 79.26 57.61 60.43 85.37 84.29 86.87 77.12 -2.51\nw/o ctrl,DS,SG 78.14 89.61 80.11 79.86 59.13 62.11 86.29 83.76 87.37 78.48 -1.15\nw/o ctrl,KG,SG 77.78 89.44 83.54 79.98 57.45 61.96 84.18 83.46 87.33 78.34 -1.29\nw/o ctrl,KG,DS 77.51 89.83 83.44 80.69 58.01 64.01 86.51 84.25 87.26 79.05 -0.58\nALBERT 76.15 84.67 83.19 77.12 57.93 56.68 86.01 85.38 86.81 77.10 -2.53\nA.P means average of performance and C.P means change of performance. ALBERT means removing everything.\nTable 3: Ablation analysis.\nQA We conduct the medical QA experiments\non MEDIQA-2019 (Abacha et al., 2019) and\nTRECQA-2017 (Abacha et al., 2017), where the\ntask is cast as a regression problem. Essentially, for\nan given question-answer pair, a numerical score\nranging from −2 to 2 is assigned by experts, in-\ndicating the quality of the answer to the question,\nand the task is to predict the score. We use a simple\nprediction model, where each pair is encoded with\na PLM or DAKI, and the representation for[CLS]\nis fed into a linear layer on top for prediction.\nNLI We conduct the medical NLI experiments\non MEDNLI (Romanov and Shivade, 2018), where\nthe task is to classify a given premise-hypothesis\npair into a class of entailment, neutral, or\ncontradiction. Similarly, each pair is en-\ncoded with a PLM or DAKI, and the [CLS] repre-\nsentation is fed into a classiﬁcation head on top.\nNER We conduct the medical NER experiments\non NCBI (Do ˘gan et al., 2014) and BC5CDR-\ndisease (Wei et al., 2016), where the task is to\nclassify tokens of sentences into a class of B, I, or\nO (Peng et al., 2019; He et al., 2020b), with a PLM\nor DAKI as the encoder.\nNote that our models for downstream tasks\nQA, NLI, and NER follow those in diease-\nBERT/diseaseALBERT (He et al., 2020b) to be\ncomparable. We also inherit the hyper-parameters\nfor such models from (He et al., 2020b). In partic-\nular, we employ AdamW as the optimizer and set\nlearning rates of {1e-5, 1e-5, 5e-5}, and the batch\nsizes of {8, 16, 16} respectively for the tasks.\nBaselines We take three PLMs, i.e., BERT-base-\nuncased (Devlin et al., 2019), RoBERTa-base (Liu\net al., 2019), ALBERT-xxlarge-v2 (Lan et al.,\n2019), as well as their main biomedical vari-\nants as the baselines, including ClinicalBERT\n(Alsentzer et al., 2019), SciBERT (Beltagy et al.,\n2019), BioBERT-v1.1 (Lee et al., 2020), umls-\nBERT (Michalopoulos et al., 2020) and disease-\nBERT/diseaseALBERT (He et al., 2020b).\n4.1.2 Pre-training Adapters\nWhen pre-training the adapters KG, DS, SG, we\nuse the ALBERT-xxlarge-v2 (Lan et al., 2019) as\nthe base PLM, and set the adapter size to 128. We\nuse Adam as the optimizer and set learning rates of\n{1e-6, 2e-4, 1e-5}, batch sizes of {256, 16, 256},\nmaximum sequence lengths of {16, 256, 128} and\ntraining epochs of {2, 10, 1}, respectively for the\ncorresponding adapters.\n4.2 Results\nTable 2 shows the performance of our proposed\narchitecture, i.e., DAKI, over three biomedical\nNLP tasks across ﬁve datasets. Generally, one\nmain observation from the table is that equip-\nping PLMs with DAKI signiﬁcantly improve\ntheir performance on these biomedical tasks, as\nreﬂected in DAKI-BERT, DAKI-RoBERTa and\nDAKI-ALBERT, demonstrating the effectiveness\nof the architecture. Moreover, although DAKI-\nBERT outperforms BERT across all metrics, it only\nperforms comparably or poorer than ClinicalBERT,\nSciBERT and BioBERT. We conjecture that it is\ndue to lack of the knowledge in their pre-training\ndata, i.e., the MIMIC-III clinical notes (Johnson\net al., 2016), the Semantic Scholar papers (Ammar\net al., 2018), and the PubMed articles, respectively.\nTransferability Another advantage of DAKI is\ntransferability, due to its ﬂexible architecture and\nimplementation. In this work, we have three\n3862\nFigure 3: Activation levels of the adapters KG, DS, SG over the downstream tasks. We calculate the softmax\nactivations in the last layer for each adapter, and the activations are averaged over all instances in the test set.\nadapters and they are all pre-trained with ALBERT\nas the base PLM. All the DAKI variants in Table 2\nare the corresponding PLMs equipped with such\npre-trained adapters (based on ALBERT). As such,\nthe performance gain of the DAKI variants show\nthat the knowledge in the adapters is transferable\nacross BERT versions, making it possible to use\nadapters as off-the-shelf modules in a a plug-and-\nplay manner. Interestingly, even for the knowledge-\naugmented BioBERT, incorporating DAKI yields\na performance boost over all tasks, which further\ndemonstrates the transferability of the architecture.\nAblation Study To investigate the inﬂuence of\neach component of DAKI, we perform an ablation\nstudy and show the results in Table 3. We ﬁrst re-\nmove the knowledge controller from DAKI, and\ntake the addition of the outputs of adapters, with-\nout adaptive adjustment. Then we remove each\nadapter while keeping the controller. Finally we\napply accumulative ablation by removing both of\nthem. Essentially, the results of the ablated ver-\nsions demonstrate varying degrees of performance\ndrop, indicating the necessity of each component.\nExplainability We expect the knowledge con-\ntroller to bring some explainability, as it adaptively\nactivates the adapters when ﬁne-tuning over the\ndownstream tasks. We show the average softmax\nattention weights of the adapters in Figure 3, which\nwe assume to reﬂect the activation levels of them.\nBasically, the activations of adapters are different\nacross tasks and datasets, except that KG and SG\nseem to have more impact on BC5CDR and NCBI.\nParameter-efﬁciency An advantage of using\nDAKI for incorporating knowledge is that only\none version of the PLM is needed to accommodate\nmultiple knowledge sources. In particular, without\nadapters, ﬁne-tuning a PLM with one knowledge\nsource will produce a new version of PLM. For\nthree knowledge sources in our work, we will need\nto have 3 ×NPLM parameters. With DAKI, this\nnumber is reduced to NPLM + 3 ×Nadapter + Nctrl.\nConsidering ALBERT as an example, this amount\nto a reduction of 2×NPLM −3×Nadapter −Nctrl ≈\n2 ×223M −4M = 442M parameters.\n5 Conclusion\nIn this paper, we propose DAKI, an adapter-based\narchitecture that adaptively integrates knowledge\nfrom multiple sources into pre-trained language\nmodels. We take the biomedical domain as a\ncase study, and speciﬁcally explore three differ-\nent sources of biomedical knowledge and inte-\ngrate them with DAKI. The experimental results\nprove the effectiveness of the architecture, and also\nshow that the architecture demonstrates parameter-\nefﬁciency, transferability, and explainability to\nsome degree. The objective of this work is not to\nupdate state-of-the-art results on the benchmarks,\nbut to provide an alternative method of domain\nknowledge integration for PLMs, especially from\nmultiple sources of knowledge.\nAcknowledgements\nThis research has been supported by the Army Re-\nsearch Ofﬁce (ARO) grant W911NF-21-1-0112\n3863\nand the NSF grant CNS-1747798 to the IU-\nCRC Center for Big Learning. This research is\nalso based upon work supported by the Ofﬁce\nof the Director of National Intelligence (ODNI),\nIntelligence Advanced Research Projects Activ-\nity (IARPA), via IARPA Contract No. 2019-\n19051600006 under the Better Extraction from Text\nTowards Enhanced Retrieval (BETTER) Program.\nThe views and conclusions contained herein are\nthose of the authors and should not be interpreted\nas necessarily representing the ofﬁcial policies, ei-\nther expressed or implied, of ARO, ODNI, IARPA,\nthe Department of Defense, or the U.S. Govern-\nment. The U.S. Government is authorized to re-\nproduce and distribute reprints for governmental\npurposes notwithstanding any copyright annotation\ntherein. This document does not contain technol-\nogy or technical data controlled under either the\nU.S. International Trafﬁc in Arms Regulations or\nthe U.S. Export Administration Regulations.\nReferences\nAsma Ben Abacha, Eugene Agichtein, Yuval Pinter,\nand Dina Demner-Fushman. 2017. Overview of\nthe medical question answering task at trec 2017\nliveqa. In Proceedings of the Text Retrieval Confer-\nence (TREC).\nAsma Ben Abacha, Chaitanya Shivade, and Dina\nDemner-Fushman. 2019. Overview of the mediqa\n2019 shared task on textual inference, question en-\ntailment and question answering. In Proceedings\nof the 18th BioNLP Workshop and Shared Task\n(BioNLP), pages 370–379.\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clini-\ncal bert embeddings. In Proceedings of the 2nd Clin-\nical Natural Language Processing Workshop (Clini-\ncalNLP), pages 72–78.\nWaleed Ammar, Dirk Groeneveld, Chandra Bhagavat-\nula, Iz Beltagy, Miles Crawford, Doug Downey, Ja-\nson Dunkelberger, Ahmed Elgohary, Sergey Feld-\nman, Vu Ha, et al. 2018. Construction of the liter-\nature graph in semantic scholar. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 3, Industry\nPapers (NAACL-HLT), pages 84–91.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert:\nA pretrained language model for scientiﬁc text. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3606–\n3611.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (NAACL-HLT), pages 4171–4186.\nRezarta Islamaj Do ˘gan, Robert Leaman, and Zhiyong\nLu. 2014. Ncbi disease corpus: a resource for dis-\nease name recognition and concept normalization.\nJournal of biomedical informatics, 47:1–10.\nBin He, Xin Jiang, Jinghui Xiao, and Qun Liu. 2020a.\nKgplm: Knowledge-guided language model pre-\ntraining via generative and discriminative learning.\narXiv preprint arXiv:2012.03551.\nYun He, Ziwei Zhu, Yin Zhang, Qin Chen, and James\nCaverlee. 2020b. Infusing disease knowledge into\nbert for health question answering, medical infer-\nence and disease name recognition. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4604–4614.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for nlp.\nIn Proceedings of the International Conference on\nMachine Learning (ICML), pages 2790–2799.\nKexin Huang, Jaan Altosaar, and Rajesh Ranganath.\n2019. Clinicalbert: Modeling clinical notes and\npredicting hospital readmission. arXiv preprint\narXiv:1904.05342.\nQiao Jin, Bhuwan Dhingra, William Cohen, and\nXinghua Lu. 2019. Probing biomedical embeddings\nfrom language models. In Proceedings of the 3rd\nWorkshop on Evaluating Vector Space Representa-\ntions for NLP (RepEval), pages 82–89.\nAlistair EW Johnson, Tom J Pollard, Lu Shen,\nH Lehman Li-Wei, Mengling Feng, Moham-\nmad Ghassemi, Benjamin Moody, Peter Szolovits,\nLeo Anthony Celi, and Roger G Mark. 2016. Mimic-\niii, a freely accessible critical care database. Scien-\ntiﬁc data, 3(1):1–9.\nBosung Kim, Taesuk Hong, Youngjoong Ko, and\nJungyun Seo. 2020. Multi-task learning for knowl-\nedge graph completion with pre-trained language\nmodels. In Proceedings of the 28th International\nConference on Computational Linguistics (COL-\nING), pages 1737–1743.\n3864\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. In Proceedings of\nthe International Conference on Learning Represen-\ntations (ICLR).\nAnne Lauscher, Olga Majewska, Leonardo FR Ribeiro,\nIryna Gurevych, Nikolai Rozanov, and Goran\nGlavaš. 2020. Common sense or world knowl-\nedge? investigating adapter-based knowledge injec-\ntion into pretrained transformers. In Proceedings\nof Deep Learning Inside Out (DeeLIO): The First\nWorkshop on Knowledge Extraction and Integration\nfor Deep Learning Architectures, pages 43–49.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. 2020. Biobert: a pre-trained biomed-\nical language representation model for biomedical\ntext mining. Bioinformatics, 36(4):1234–1240.\nYoav Levine, Barak Lenz, Or Dagan, Ori Ram, Dan\nPadnos, Or Sharir, Shai Shalev-Shwartz, Amnon\nShashua, and Yoav Shoham. 2020. Sensebert: Driv-\ning some sense into bert. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics (ACL), pages 4656–4667.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nXiaofei Ma, Peng Xu, Zhiguo Wang, Ramesh Nallapati,\nand Bing Xiang. 2019. Domain adaptation with bert-\nbased domain classiﬁcation and data selection. In\nProceedings of the 2nd Workshop on Deep Learning\nApproaches for Low-Resource NLP (DeepLo), pages\n76–83.\nAlexa T McCray, Anita Burgun, and Olivier Boden-\nreider. 2001. Aggregating umls semantic types for\nreducing conceptual complexity. Studies in health\ntechnology and informatics, 84(0 1):216.\nGeorge Michalopoulos, Yuanxin Wang, Hussam Kaka,\nHelen Chen, and Alex Wong. 2020. Umls-\nbert: Clinical domain knowledge augmentation of\ncontextual embeddings using the uniﬁed medical\nlanguage system metathesaurus. arXiv preprint\narXiv:2010.10391.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019.\nTransfer learning in biomedical natural language\nprocessing: An evaluation of bert and elmo on ten\nbenchmarking datasets. In Proceedings of the 18th\nBioNLP Workshop and Shared Task (BioNLP), pages\n58–65.\nLis Pereira, Xiaodong Liu, Fei Cheng, Masayuki Asa-\nhara, and Ichiro Kobayashi. 2020. Adversarial train-\ning for commonsense inference. In Proceedings of\nthe 5th Workshop on Representation Learning for\nNLP (Repl4NLP), pages 55–60.\nMatthew E Peters, Mark Neumann, Robert Logan, Roy\nSchwartz, Vidur Joshi, Sameer Singh, and Noah A\nSmith. 2019. Knowledge enhanced contextual word\nrepresentations. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 43–54.\nJonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé,\nKyunghyun Cho, and Iryna Gurevych. 2021.\nAdapterFusion: Non-destructive task composition\nfor transfer learning. In Proceedings of The 16th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics (EACL) . Associ-\nation for Computational Linguistics.\nJonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aish-\nwarya Kamath, Ivan Vuli ´c, Sebastian Ruder,\nKyunghyun Cho, and Iryna Gurevych. 2020.\nAdapterhub: A framework for adapting transform-\ners. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations (EMNLP), pages 46–54.\nNina Poerner, Ulli Waltinger, and Hinrich Schütze.\n2019. Bert is not a knowledge base (yet): Fac-\ntual knowledge vs. name-based reasoning in unsu-\npervised qa. arXiv preprint arXiv:1911.03681.\nSylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea\nVedaldi. 2017. Learning multiple visual domains\nwith residual adapters. In Proceedings of the 31st In-\nternational Conference on Neural Information Pro-\ncessing Systems (NeurIPS), pages 506–516.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in bertology: What we know about\nhow bert works. Transactions of the Association for\nComputational Linguistics (TACL), 8:842–866.\nAlexey Romanov and Chaitanya Shivade. 2018.\nLessons from natural language inference in the clin-\nical domain. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 1586–1596.\nAndreas Rücklé, Gregor Geigle, Max Glockner,\nTilman Beck, Jonas Pfeiffer, Nils Reimers, and\nIryna Gurevych. 2020. Adapterdrop: On the efﬁ-\nciency of adapters in transformers. arXiv preprint\narXiv:2010.11918.\nTianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng\nGuo, Yaru Hu, Xuan-Jing Huang, and Zheng Zhang.\n2020. Colake: Contextualized language and knowl-\nedge embedding. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics\n(COLING), pages 3660–3670.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of the 31st International\nConference on Neural Information Processing Sys-\ntems (NeurIPS), pages 6000–6010.\n3865\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuanjing Huang, Cuihong Cao, Daxin Jiang, Ming\nZhou, et al. 2020. K-adapter: Infusing knowl-\nedge into pre-trained models with adapters. arXiv\npreprint arXiv:2002.01808.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021.\nKepler: A uniﬁed model for knowledge embedding\nand pre-trained language representation. Transac-\ntions of the Association for Computational Linguis-\ntics (TACL), 9:176–194.\nChih-Hsuan Wei, Yifan Peng, Robert Leaman, Al-\nlan Peter Davis, Carolyn J Mattingly, Jiao Li,\nThomas C Wiegers, and Zhiyong Lu. 2016. Assess-\ning the state of the art in biomedical relation extrac-\ntion: overview of the biocreative v chemical-disease\nrelation (cdr) task. Database, 2016.\nLiang Yao, Chengsheng Mao, and Yuan Luo. 2019. Kg-\nbert: Bert for knowledge graph completion. arXiv\npreprint arXiv:1909.03193.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. Ernie: Enhanced\nlanguage representation with informative entities. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics (ACL) , pages\n1441–1451.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8279163837432861
    },
    {
      "name": "Bottleneck",
      "score": 0.6547352075576782
    },
    {
      "name": "Domain knowledge",
      "score": 0.6363592743873596
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5781993269920349
    },
    {
      "name": "ENCODE",
      "score": 0.5662525296211243
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5407010316848755
    },
    {
      "name": "Knowledge graph",
      "score": 0.5179649591445923
    },
    {
      "name": "Unified Medical Language System",
      "score": 0.5133671164512634
    },
    {
      "name": "Machine learning",
      "score": 0.4415547251701355
    },
    {
      "name": "Language model",
      "score": 0.44000065326690674
    },
    {
      "name": "Transformer",
      "score": 0.4214937090873718
    },
    {
      "name": "Natural language processing",
      "score": 0.3800821304321289
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Embedded system",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ]
}