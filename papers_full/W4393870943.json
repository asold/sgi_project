{
  "title": "Genomic language model predicts protein co-regulation and function",
  "url": "https://openalex.org/W4393870943",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2146130156",
      "name": "Yunha Hwang",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A4365411439",
      "name": "Andre L Cornman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2489021095",
      "name": "Elizabeth H. Kellogg",
      "affiliations": [
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2554076456",
      "name": "Sergey Ovchinnikov",
      "affiliations": [
        "Harvard University Press"
      ]
    },
    {
      "id": "https://openalex.org/A67973381",
      "name": "Peter R. Girguis",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2146130156",
      "name": "Yunha Hwang",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A4365411439",
      "name": "Andre L Cornman",
      "affiliations": [
        "Baltimore City Health Department"
      ]
    },
    {
      "id": "https://openalex.org/A2489021095",
      "name": "Elizabeth H. Kellogg",
      "affiliations": [
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2554076456",
      "name": "Sergey Ovchinnikov",
      "affiliations": [
        "Harvard University Press"
      ]
    },
    {
      "id": "https://openalex.org/A67973381",
      "name": "Peter R. Girguis",
      "affiliations": [
        "Harvard University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1969230095",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W3186179742",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W4318071656",
    "https://openalex.org/W4392095606",
    "https://openalex.org/W4232537249",
    "https://openalex.org/W1988714210",
    "https://openalex.org/W2064971934",
    "https://openalex.org/W2022927779",
    "https://openalex.org/W2041579725",
    "https://openalex.org/W3164078851",
    "https://openalex.org/W2972697066",
    "https://openalex.org/W2072561431",
    "https://openalex.org/W4298006192",
    "https://openalex.org/W4315631495",
    "https://openalex.org/W3203588026",
    "https://openalex.org/W3127238141",
    "https://openalex.org/W4316339774",
    "https://openalex.org/W4304607335",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W4310957810",
    "https://openalex.org/W2134153749",
    "https://openalex.org/W2775529910",
    "https://openalex.org/W2740705661",
    "https://openalex.org/W2127597332",
    "https://openalex.org/W2127464112",
    "https://openalex.org/W1914471426",
    "https://openalex.org/W2131550794",
    "https://openalex.org/W4306878720",
    "https://openalex.org/W6763527631",
    "https://openalex.org/W4225434086",
    "https://openalex.org/W4213112325",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3037888463",
    "https://openalex.org/W2797911275",
    "https://openalex.org/W2019531132",
    "https://openalex.org/W4310155672",
    "https://openalex.org/W3215907690",
    "https://openalex.org/W4225879163",
    "https://openalex.org/W2949904390",
    "https://openalex.org/W2107867854",
    "https://openalex.org/W3127656915",
    "https://openalex.org/W2950954328",
    "https://openalex.org/W3106210592",
    "https://openalex.org/W2045204781",
    "https://openalex.org/W2076048958",
    "https://openalex.org/W4362471278",
    "https://openalex.org/W4375858802",
    "https://openalex.org/W2108211735",
    "https://openalex.org/W2564167865",
    "https://openalex.org/W4226523664",
    "https://openalex.org/W2322975763",
    "https://openalex.org/W4280626237",
    "https://openalex.org/W4226515274"
  ],
  "abstract": null,
  "full_text": "Article https://doi.org/10.1038/s41467-024-46947-9\nGenomic language model predicts protein\nco-regulation and function\nYunha Hwang 1 ,A n d r eL .C o r n m a n2,E l i z a b e t hH .K e l l o g g3,5,\nSergey Ovchinnikov 4,6 &P e t e rR .G i r g u i s1\nDeciphering the relationship between a gene and its genomic context is fun-\ndamental to understanding and engineering biological systems. Machine\nlearning has shown promise in learning latent relationships underlying the\nsequence-structure-function paradigmf r o mm a s s i v ep r o t e i ns e q u e n c ed a t a -\nsets. However, to date, limited attempts have been made in extending this\ncontinuum to include higher order genomic context information. Evolutionary\nprocesses dictate the speciﬁcity of genomic contexts in which a gene is found\nacross phylogenetic distances, and these emergent genomic patterns can be\nleveraged to uncover functional relationships between gene products. Here,\nwe train a genomic language model (gLM) on millions of metagenomic scaf-\nfolds to learn the latent functional and regulatory relationships between genes.\ngLM learns contextualized protein embeddings that capture the genomic\ncontext as well as the protein sequence itself, and encode biologically mean-\ningful and functionally relevant information (e.g. enzymatic function, tax-\nonomy). Our analysis of the attention patterns demonstrates that gLM is\nlearning co-regulated functional modules (i.e. operons). Ourﬁndings illustrate\nthat gLM’s unsupervised deep learning ofthe metagenomic corpus is an\neffective and promising approach to encode functional semantics and reg-\nulatory syntax of genes in their genomic contexts and uncover complex rela-\ntionships between genes in a genomic region.\nEvolutionary processes result in the linkage between protein sequen-\nces, structure and function. The resulting sequence-structure-function\nparadigm\n1 has long provided the basis for interpreting vast amounts of\ngenomic data. Recent advances in neural network (NN)-based protein\nstructure prediction methods\n2,3, and more recently protein language\nmodels (pLMs)4– 7 suggest that data-centric approaches in unsu-\npervised learning can represent these complex relationships shaped\nby evolution. To date, these models largely consider each protein as an\nindependent and standalone entity. However, proteins are encoded in\ngenomes alongside other proteins, and the speciﬁcg e n o m i cc o n t e x t\nthat a protein occurs in is determined by evolutionary processes where\neach gene gain, loss, duplication and transposition event is subject to\nselection and drift\n8– 10. These processes are particularly pronounced in\nbacterial and archaeal genomes where frequent horizontal gene\ntransfers (HGT) shape genomic organization and diversity\n11,12.T h u s ,\nthere exists an inherent evolutionary linkage between genes, their\ngenomic context, and gene function\n13– 15, which can be explored by\ncharacterizing patterns that emerge from large metagenomic datasets.\nRecent efforts to model genomic information have shown pre-\ndictive power of genomic context in gene function16 and metabolic\ntrait evolution17 in bacterial and archaeal genomes. However, these\nmethods represent genes as categorical entities, despite these genes\nReceived: 30 May 2023\nAccepted: 13 March 2024\nCheck for updates\n1Department of Organismic and Evolutionary Biology, Harvard University, Cambridge, MA, USA.2Tatta Bio, Baltimore, MD, USA.3Department of Molecular\nBiology and Genetics, Cornell University, Ithaca, NY, USA.4John Harvard Distinguished Science Fellowship Program, Harvard University, Cambridge, MA,\nUSA.5Present address: Department of Structural Biology, St. Jude Children’s Research Hospital, Memphis, TN, USA.6Present address: Department of Biology,\nMassachusetts Institute of Technology, Cambridge, MA, USA.e-mail: yhwang@g.harvard.edu; so3@mit.edu; pgirguis@oeb.harvard.edu\nNature Communications|         (2024) 15:2880 1\n1234567890():,;\n1234567890():,;\nexisting in continuous space where multidimensional properties such\nas phylogeny, structure, and function are abstracted in their sequen-\nces. On the other end of the spectrum of representations, there have\nbeen efforts to use unsupervised learning on nucleotide sequences to\npredict gene expression level\n18 and detect regulatory motifs19– 21.T h e s e\nmodels are largely trained and benchmarked on the human genome\nand focus on predicting gene regulation rather than function. Most\nrecent efforts to leverage diverse microbial sequences to model\ngenome-scale information include GenSLMs\n22, which is pretrained on\ncodon-level representations of diverse bacterial and viral gene\nsequences and laterﬁne-tuned on SARS-CoV-2 genomes. In order to\nlearn generalizable gene-to-gene-context interactions across biology, a\nmodel needs to be pretrained on 1) diverse lineages of organisms, 2)\nrich and continuous representation of genes and 3) longer segments of\ngenomes with multiple genes. To our knowledge, there has been no\nmethod that combines all three aspects of pretraining to learn geno-\nmic information across diverse lineages of biology (see summary of\nprevious studies in Supplementary Table 1).\nIn order to close the gap between genomic-context and gene\nsequence-structure-function, we develop a genomic language model\n(gLM) that learns contextual representations of genes. gLM leverages\npLM embeddings as input, which encode relational properties\n4 and\nstructure information23 of the gene products. Our model is based on\nthe transformer24 architecture and is trained using millions of unla-\nbelled metagenomic sequences via the masked language modeling25\nobjective, with the hypothesis that its ability to attend to different\nparts of a multi-gene sequence will result in the learning of gene\nfunctional semantics and regulatory syntax (e.g. operons). Here, we\nreport evidence of the learned contextualized protein embeddings\nand attention patterns capturing biologically relevant information. We\ndemonstrate gLM’s potential for predicting gene function and co-\nregulation, and propose future applications and research directions,\nincluding transfer learning capabilities of gLM.\nResults\nMasked language modeling of genomic sequences\nLanguage models, such as Bidirectional Encoder Representations from\nTransformers (BERT\n25), learn the semantics and syntax of natural lan-\nguages using unsupervised training of a large corpus. In masked lan-\nguage modeling, the model is tasked with reconstructing corrupted\ninput text\n25, where some fraction of the words are masked. Signiﬁcant\nadvances in language modeling performance was achieved by adopt-\ning the transformer24 neural network architecture, where each token\n(i.e. word) is able to attend to other tokens. This is in contrast to Long-\nShort-Term-Memory networks (LSTMs)\n26 that sequentially processes\ntokens. To model genomic sequences, we trained a 19-layer transfor-\nmer model (Fig.1A; for a detailedﬁgure see Supplementary Fig. 1) on\nseven million metagenomic contig fragments consisting of 15 to 30\ngenes from the MGnify\n27 d a t a b a s e .E a c hg e n ei nag e n o m i cs e q u e n c ei s\nrepresented by a 1280 feature vector (context-free protein embed-\ndings) generated by using ESM2 pLM\n23, concatenated with an orien-\ntation feature (forward or backward). For each sequence, 15% of genes\nare randomly masked, and the model learns to predict the masked\nlabel using the genomic context. Based on the insight that more than\npLM\n(ESM2)\npLM\n(ESM2)\ngLM\ngLM\nA B\n...\n...\n...\n30 genes in contig\n30\n...\n...\nContextualized\nprotein embeddings\nProtein embeddings\nProtein\nembeddings\nPredictions\nAttention\npatterns\nRegulatory\nsyntax\nFunctional\nsemantics\nLoss\nMasking\nTraining Inference\nFig. 1 | gLM training and inference schematics. AFor training, contigs (con-\ntiguous genomic sequences) containing up to 30 genes areﬁrst translated into\nproteins, which are subsequently embedded using a protein language model (pLM)\nencoder (ESM2). Masked inputs are generated by random masking at 15% prob-\nability and genomic language model (gLM; a transformer encoder) is trained to\nmake four predictions for each masked protein, with associated likelihoods.\nTraining loss is calculated on both the prediction and likelihoods.B At inference\ntime, inputs are generated from a contig using ESM2 output. Contextualized pro-\ntein embeddings (hidden layers of gLM) and attention patterns are used for various\ndownstream tasks. See Supplementary Fig. 1 for detailed schematics. Source data\nare provided as a Source Dataﬁle.\nArticle https://doi.org/10.1038/s41467-024-46947-9\nNature Communications|         (2024) 15:2880 2\none gene can legitimately be found in a particular genomic context, we\nallow the model to make four different predictions and also predict\ntheir associated probabilities (Supplementary Fig. 1). Thus, instead of\npredicting their mean value, the model can approximate the under-\nlying distribution of multiple genes that can occupy a genomic niche.\nWe assess the model’s performance using a pseudo-accuracy metric,\nwhere a prediction is considered correct if it is closest to the masked\nprotein in euclidean distance compared to the other proteins encoded\nin the sequence (see Methods). We validate our model’s performance\non theEscherichia coliK-12 genome\n28 by excluding from training 5.1%\nof MGnify subcontigs in which more than half of the proteins are\nsimilar (>70% sequence identity) toE. coliK-12 proteins. It is important\nto note that our goal was not to remove allE. coliK-12 homologs from\nthe training, which would have removed a vast majority of training data\nas many essential genes are shared across organisms. Instead, our goal\nw a st or e m o v ea sm a n yE.coli K-12-like genomic contexts (subcontigs)\nfrom training, which is more appropriate for the training objective.\ngLM achieves 71.9% in validation pseudo-accuracy and 59.2% in vali-\ndation absolute accuracy (Supplementary Fig. 2). Notably, 53.0% of the\npredictions made during validation are with high conﬁdence (with\nprediction likelihood > 0.75), and 75.8% of the high conﬁdence pre-\ndictions are correct, indicating gLM’s ability to learn a conﬁdence\nmetric that corresponds to increased accuracy. We baseline our per-\nformance with a bidirectional LSTM model trained using the same\nlanguage modeling task on the same training dataset, where validation\nperformance plateaus at 28% pseudo-accuracy and 15% absolute\naccuracy (Supplementary Fig. 2 and Supplementary Table 2, note that\nbiLSTM is smaller because it failed to converge when scaling the\nnumber of layers). We ablate the use of pLM representations as input\nto gLM by replacing them with one-hot amino acid representations\n(Supplementary Table 3) and report performance equivalent to ran-\ndom predictions (3% pseudo-accuracy and 0.02% absolute accuracy).\nContextualized gene embeddings capture gene semantics\nThe mapping from gene to gene-function in organisms is not one-to-\none. Similar to words in natural language, a gene can confer different\nfunctions\n29 depending on its context30,a n dm a n yg e n e sc o n f e rs i m i l a r\nfunctions (i.e. convergent evolution31, remote homology32). We used\ngLM to generate 1280-feature contextualized protein embeddings at\ninference time (Fig.1B), and we examined the“semantic” information\ncaptured in these embeddings. Analogous to how words are likely to\nhave different meanings depending on the type of text in which they\nare found (Fig.2A), weﬁnd that contextualized protein embeddings of\ngenes that appear across multiple environments (biomes) tend to\ncluster based on biome types. We identiﬁed 31 proteins in our training\ndatabase (MGYPs) that occurred more than 100 times and distributed\nwith at least 20 occurrences in each “Host-associated”, “Environ-\nmental”,a n d“Engineered” biomes according to MGnify’s designation.\nWe ﬁnd that gLM’s contextualized protein embeddings capture biome\ninformation for the majority (n = 21) of these multi-biome MGYPs. For\ninstance, a gene encoding a protein annotated“translation initiation\nfactor IF-1” occurs multiple times across biomes. While the input to\ngLM (context-free protein embedding; ESM2 representation) is iden-\ntical across all occurrences, gLM’s output (contextualized protein\nembeddings) cluster with biome types (Fig.2B; silhouette score = 0.17,\nsee the other 30 multi-biome MGYP visualizations in Supplementary\nFig. 3). This suggests that the diverse genomic contexts that a gene\noccupies are speciﬁc for different biomes, implying biome-speciﬁc\ngene semantics.\nWe explored an ecologically important example of genomic\n“polysemy” (multiple meanings conferred by the same word; Fig.2C)\nof methyl-coenzyme M reductase (MCR) complex. The MCR complex\nis able to carry out a reversible reaction (Reaction 1 in Fig.2D), whereby\nthe forward reaction results in the production of methane (methano-\ngenesis) while the reverse results in methane oxidation\n(methanotrophy). Weﬁrst examine the McrA (methyl-coenzyme M\nreductase subunit alpha) protein in diverse lineages of ANME (ANae-\nrobic MEthane oxidizing) and methanogenic archaeal genomes. These\narchaea are polyphyletic and occupy speciﬁc ecological niches. Nota-\nbly, similar to how a semantic meaning of a word exists on a spectrum\nand a word can have multiple semantically appropriate meanings in a\ncontext (Fig.2C), the MCR complex can confer different functions\ndepending on the context. Previous reports demonstrate the capa-\ncities of ANME (ANME-2 in particular) carrying out methanogenesis\n33\nand methanogens conducting methane oxidation in speciﬁcg r o w t h\nconditions34. The context-free ESM2 embedding of these proteins\n(Fig. 2E) shows little organization, with little separation between\nANME-1 and ANME-2 McrA proteins. However, contextualized gLM\nembeddings (Fig.2F) of the McrA proteins show distinct organization\nwhere ANME-1 McrA proteins form a tight cluster, while ANME-2 McrA\nproteins form a cluster with methanogens (silhouette score after\ncontextualization: 0.24; before contextualization: 0.027). This orga-\nnization reﬂects the phylogenetic relationships between the organisms\nthat McrAs are found in, as well as the distinct operonic and structural\ndivergence of MCR complexes in ANME-1 compared to those found in\nANME-2 and methanogens\n35.A sp r o p o s e db yS h a oe ta l .35., the pre-\nferred directionality in Reaction 1 (Fig.2D) in ANME-2 and some\nmethanogens may be more dependent on thermodynamics.\nWe also demonstrate that contextualized gLM embeddings are\nmore suitable for determining the functional relationship between\ngene classes. Analogous to how the words“dog” and “cat” are closer in\nmeaning relative to“dog” and “train” (Fig. 2G), we see a pattern where\nCas1- and Cas2-encoding genes appear diffuse in multiple subclusters\nin context-free protein embedding space (Fig.2H) cluster in con-\ntextualized embedding space (Fig.2I). This reﬂects their similarity in\nfunction (e.g. phage defense). This is also demonstrated in biosyn-\nthetic genes, where genes encoding lipopolysaccharide synthase (LPS)\na n dp o l y k e t i d es y n t h a s e( P K S )c l u s t e rc l o s e rt o g e t h e ri nc o n -\ntextualized embedding space distinct from the Cas proteins (Fig.2I).\nWe quantitate this pattern with a higher silhouette score measuring\nphage defense and biosynthetic gene separation (gLM representation:\n0.123 ± 0.021, pLM representation: 0.085 ± 0.007; paired t-test, t-sta-\ntistic: 5.30, two-sided,p value = 0.0005,n = 10). Contextualized pro-\ntein embeddings are therefore able to capture relational properties\nakin to semantic information\n36, where genes encoding proteins that\nare more similar in their function are found in similar genomic\ncontexts.\nIn order to quantify the information gain as a result of training a\ntransformer on genomic contexts, we compare clustering results in 2B,\nF, and I with clustering conducted on (sub)contig-averaged pLM\nembeddings (Supplementary Fig. 4). By mean-pooling pLM embed-\ndings across a given subcontig, we can summarize the context infor-\nmation as a naive baseline. We report a most consistent clustering\n(higher silhouette scores) of gLM embeddings compared to contig-\naveraged pLM for all three analyses (see Supplementary Fig. 4ﬁgure\ncaptions for values). We demonstrate that the gLM transformer model\nlearns representations that correlate with biological function, which\nare not captured by the naive baseline.\nCharacterizing the unknown\nMetagenomic sequences feature many genes with unknown or generic\nfunctions, and some are so divergent that they do not contain sufﬁ-\ncient sequence similarity to the annotated fraction of the database\n37.I n\nour dataset, of the 30.8 M protein sequences, 19.8% could not be\nassociated with any known annotation (see Methods), and 27.5% could\nnot be associated with any known Pfam domains using a recent deep\nlearning approach (ProtENN\n38). Understanding the functional role of\nthese proteins in their organismal and environmental contexts remains\na major challenge because most of the organisms that house such\nproteins are difﬁcult to culture and laboratory validation is often low-\nArticle https://doi.org/10.1038/s41467-024-46947-9\nNature Communications|         (2024) 15:2880 3\nA\nG\nword contextualized word embeddings\ncourt\nLegal documents\nRomance novels\nSports news\ncontextualized word embeddings\nCat\nDog\nHamster\nTrain\nCarBus\nCH3 -S-CoM + HS - CoB CH4+ CoM-S-S-CoB\nBiosynthesis\nDefense\nsunlight\ncontextualized word embeddings\na light box\nlight conversation\na light breakfast\na light bulb\nReverse\nForward\nMethanogenesisMethanotrophy\nilluminationillumination\nof little weight\nB\nC EF\nD\nH I\nReaction 1\npLM embedding\n(identical representation across biomes)\nContextualized protein embeddings\nTranslation initiation factor IF-1\nTranslation initiation factor IF-1\nContextualization by gLM\nFig. 2 | Contextualized protein embedding analysis and comparison with\nconcepts in natural language modeling. AA word upon contextualization can be\nmapped to embedding space. For many words, the semantic meaning varies in\ndifferent types of literature, and therefore their contextualized embeddings cluster\nwith source text type. Figure was created for qualitative visualization.B The input\nprotein embedding (output of ESM2 and context-free protein embedding) is the\nsame across all occurrences of the protein in the database. Upon contextualization\nwith gLM, contextualized protein embeddings of the same protein (last hidden\nlayer of gLM at inference time) cluster with biome type, analogous to the source\ntext type in natural language (A). Contextualization of 30 other multi-biome MGYPs\ncan be found in Supplementary Fig. 3.C Aw o r d’s meaning upon contextualization\nvaries across a continuous spectrum and can be ambiguous even with con-\ntextualization (e.g. double entendre).D Reaction 1, carried out by the MCR com-\nplex, either backward (Methanotrophy) or forward (Methanogenesis).E Principal\nComponent Analysis (PCA) of context-free protein embeddings of McrA sequences\nin genomes (total explained variances = 0.56), colored by metabolic classiﬁcation\nof the organism (ANME, methanogen) based on previous studies and labeled by\nclass-level taxonomy.F PCA of contextualized McrA embeddings (total explained\nvariance = 0.68), where gLM embeddings cluster with the direction of Reaction 1\nthat the MCR complex is likely to carry out.G Geometric relationship between\ncontextualized protein embeddings based on the semantic closeness of words.\nH Input (context-free) protein embeddings of Cas1, Cas2, lipopolysaccharide syn-\nthases (LPS) and polyketide synthases (PKS) showing clustering based on structural\nand sequence similarity.I Clustering of contextualized protein embeddings where\nphage defense proteins cluster (Cas1 and Cas2) and biosynthetic gene products\ncluster (lipopolysaccharide synthases [LPS] and polyketide synthases [PKS]).\nSource data are provided as a Source Dataﬁle.\nArticle https://doi.org/10.1038/s41467-024-46947-9\nNature Communications|         (2024) 15:2880 4\nthroughput. In microbial genomes, proteins conferring similar func-\ntions are found in similar genomic contexts due to selective pressures\nbestowed by functional relationships (e.g. protein-protein interac-\ntions, co-regulations) between genes. Based on this observation, we\nposited that contextualization would provide richer information that\npushes the distribution of unannotated genes closer to the distribution\nof annotated genes. We compared the distributions of unannotated\nand annotated fractions of proteins in our dataset using context-free\npLM embeddings and contextualized gLM embeddings. We found a\nstatistically signiﬁcant lower divergence between distributions of\nunannotated and annotated genes in gLM embeddings compared to\npLM embeddings (paired t-test of Kullback-Leibler divergences, t-test\nstatistic = 7.61, two-sided, p-value < 1e-4,n = 10; see Methods for sam-\npling and metric calculation). This suggests a greater potential for\nusing gLM embeddings to transfer validated knowledge in cultivable\nand well-studied strains (e.g.E. coli K-12) to the vastly uncultivated\nmetagenomic sequence space. Genomic context, along with molecular\nstructure and phylogeny, appear to be important information to\nabstract in order to effectively represent sequences such that we can\nuncover hidden associations between the known and the unknown\nfractions of biology.\nContextualization improves enzyme function prediction\nTo test the hypothesis that the genomic context of proteins can be\nused to aid function prediction, we evaluated how contextualization\ncan improve the expressiveness of protein representations for enzyme\nfunction prediction. First, we generated a custom MGYP-EC dataset\nwhere the train and test data were split at 30% sequence identity for\neach EC class (see Methods). Second, we apply a linear probe (LP) to\ncompare the expressiveness of representations at each gLM layer, with\nand without masking the queried protein (Supplementary Fig. 5). By\nmasking the queried protein, we can assess gLM’s ability to learn\nfunctional information of a given protein, only from its genomic con-\ntext, without the propagation of information from the protein’sp L M\nembeddings. We observed that a large fraction of contextual infor-\nmation pertaining to enzymatic function is learned in theﬁrst six layers\nof gLM. We also demonstrate that context information alone can be\npredictive of protein function, reaching up to 24.4 ± 0.8% accuracy. In\ncontrast, without masking, gLM can incorporate information present\nin the context with the original pLM information for each queried\nprotein. We observed an increase in expressivity of gLM embeddings\nalso in the shallower layers, with accuracy reaching up to 51.6 ± 0.5% in\nthe ﬁrst hidden layer. This marks a 4.6 ± 0.5% increase from context-\nfree pLM prediction accuracy (Fig.3A) and 5.5 ± 1.0% increase in mean\naverage precision (Fig.3C) Thus, we demonstrate that information that\ngLM learns from the context is orthogonal to information captured in\npLM embedding. We also observed diminishing expressivity in enzyme\nfunction information with deeper layers of gLM; this is consistent with\nprevious examinations of LLMs, where deeper layers are specialized to\nthe pretraining task (masked token prediction), and is consistent with\nprevious examinations of LLMs, where the best-performing layer\ndepends on the speciﬁcd o w n s t r e a mt a s k s\n39. Finally, to further exam-\nine the expressiveness of these representations, we compared per-\nclass F1 score gains (Fig.3B). We observe statistically signiﬁcant dif-\nferences in F1 scores (t-test, two-sided, Benjamini/Hochberg corrected\np value < 0.05,n = 5) between the two models in 36 out of 73 EC classes\nwith more than ten samples in the test set. Majority (27 out of 36) of the\nstatistical differences resulted in improved F1 score in LP trained on\ngLM representations.\nHorizontal transfer frequency corresponds to genomic context\nembedding variance\nA key process that shapes microbial genome organization and evolu-\ntion is horizontal gene transfer (HGT). The taxonomic range in which\ngenes are distributed across the tree of life depends on their function\nand the selective advantage they incur in different environments.\nRelatively little is known about the speciﬁcity in the genomic region\ninto which a gene gets transferred across phylogenetic distances. We\nexamined the variance of gLM embeddings for proteins that occur at\nleast one hundred times in the database. Variance of gLM-learned\ngenomic contexts are calculated by taking a random sample of 100\noccurrences and then calculating the mean pairwise distances between\nthe hundred gLM embeddings. We conduct such independent random\nsampling and distance calculation ten times per gene and then calcu-\nlate the mean value. As a baseline, we calculate variance of subcontig-\naveraged pLM embeddings using the same sampling method, to\ncompare the information learned from training gLM. Our results show\nthat gLM-learned genomic context variances have a longer right-hand\ntail (kurtosis = 1.02, skew = 1.08) compared to the contig-averaged pLM\nb a s e l i n et h a ti sm o r ep e a k e d( k u r t o s i s=2 . 2 ,s k e w=1 . 0 5 )( F i g .3D).\nNotably, the most context-variant genes in the right tail of gLM-learned\ncontext variance distribution (orange) included phage genes and\ntransposases, reﬂecting their ability to self-mobilize. Interestingly, we\ndid notﬁnd any phage genes in the right-most tail of contig-averaged\npLM embedding variance distribution (blue), although we didﬁnd\ngenes involved in transposition (Supplementary Table 4). gLM-learned\ngenomic context variances can be used as a proxy for horizontal\ntransfer frequencies and can be used to compare theﬁtness effects of\nthe genomic context on the evolutionary trajectory (e.g. geneﬂow) of\ngenes, as well as to identify undercharacterized and functional trans-\nposable elements.\nTransformer’s attention captures operons\nThe transformer attention mechanism24 models pairwise interaction\nbetween different tokens in the input sequence. Previous examina-\ntions of the attention patterns of transformer models in natural lan-\nguage processing (NLP)\n39 have suggested that different heads appear\nto specialize in syntactic functions. Subsequently, different attention\nheads in pLMs\n40 have been shown to correlate to speciﬁc structural\nelements and functional sites in a protein. For our gLM, we hypothe-\nsized that speciﬁc attention heads focus on learning operons, a“syn-\ntactic” feature pronounced in microbial genomes where multiple\ngenes of related function are expressed as single polycistronic tran-\nscripts. Operons are prevalent in bacterial, archaeal and their viral\ngenomes, while rare in eukaryotic genomes. We used theE.coli K-12\noperon database\n41 consisting of 817 operons for validation. gLM con-\ntains 190 attention heads across 19 layers. We found that heads in\nshallower layers correlated more with operons (Fig.4A, Supplemen-\ntary Fig. 6, with raw attention scores in the 7th head of the 2nd layer\n[L2-H7] linearly correlating with operons with 0.44 correlation coefﬁ-\ncient (Pearson’sr h o ,B o n f e r r o n ia d j u s t e dp value < 1E-5) (Fig.4B). We\nfurther trained a logistic regression classiﬁer (operon predictor) using\nall attention patterns across all heads. Our classiﬁer predicts the pre-\nsence of an operonic relationship between a pair of neighboring pro-\nteins in a sequence with high precision (mean average precision =\n0.775 ± 0.028,ﬁve-fold cross-validation) (Fig.4C ) .W eb a s e l i n et h i s\nperformance by training an operon predictor on the one-hot amino\nacid representation-based gLM ablation (mean average precision =\n0.426 ± 0.015,ﬁve-fold cross-validation; Supplementary Table 3), that\nlearns from the orientation and co-occurrence information but cannot\nfully leverage rich representation of genes.\nContext dependency of AAA+ regulator functions in complex\ngenetic systems\nUnderstanding the functional role of a regulatory protein in an\norganism remains a challenging task because the same protein fold\nmay carry out different functions depending on the context. For\ninstance, AAA+ proteins (ATPases associated with diverse cellular\nactivities) utilize the chemical energy from ATP hydrolysis to confer\ndiverse mechanical cellular functions\n42. However, AAA+ regulators can\nArticle https://doi.org/10.1038/s41467-024-46947-9\nNature Communications|         (2024) 15:2880 5\nalso play very different, broad functional roles depending on their\ncellular interacting partners from protein degradation and DNA repli-\ncation to DNA transposition. One particularly interesting example is\nthe TnsC protein, which regulates DNA insertion activity\n43 in Tn7-like\ntransposon systems. Multiple bioinformatic efforts focused on dis-\ncovery of previously uncharacterized transposons through metagen-\nome search\n44 and sequence searches of assembled genomes45 aimed at\nidentifying suitable homologs for genome-editing applications46.I n\norder to test whether the methods developed here could identify Tn7-\nlike transposition systems as well as distinguish these from other\nfunctional contexts, we explored the contextualized semantics of\nTnsC’s structural homologs in the MGnify database. Without con-\ntextualization, there appears no clustering with associated transposase\nactivity (KL divergence ratio = 1.03; see Methods for calculation of this\nmetric, Fig.4E). However, with added contextualization, previously\nidentiﬁed TnsC (orange) and manually inspected TnsC-like structural\nhomolog (red, labeled“TnsC-like”) cluster together (KL divergence\nratio = 0.38; Fig.4F; see Supplementary Fig. 7B, C for comparison with\ngLM-only and contig-averaged pLM baselines). We further validate this\nvisualization using embedding distance-based clustering (Supple-\nmentary Fig. 8). Many structural homologs of TnsC were not involved\nin transposition and this is reﬂected in distinct clusters of gray data\npoints away from known TnsC (oranges) and TnsC-like structural\nhomologs (red) in Fig. 4F. These clusters represent diverse and\ncontext-dependent AAA+ regulation activity that cannot be predicted\nfrom neither structure nor raw sequence alone. We predicted an\noperonic relationship between these AAA+ regulators and their\nneighboring genes and found many to be in operonic relationships\nwith gene modules of diverse function, including pilus assembly and\nviral host-nuclease inhibition (Fig.4D, Supplementary Fig. 7A). In some\ncases, queried AAA+ proteins did not appear to be in an operonic\nassociation with the neighboring proteins, suggesting some AAA+\nproteins are less likely to be functionally associated with their neigh-\nbors than others (Supplementary Fig. 7A, example 6). Using this\nFig. 3 | Contextualization of gene function. ALinear probe enzyme commission\n(EC) number classiﬁcation accuracy for pLM (ESM2) representations and gLM (1st\nhidden layer) representations. Data are presented as mean values +/- standard\ndeviation overﬁve technical replicates.B F1-score comparisons of statistically\nsigniﬁcant (t-test, two-sided, Benjamini/Hochberg correctedp value < 0.05, tech-\nnical replicates = 5) differences in performance of pLM- and gLM-based EC number\nlinear probes. EC classes are ordered with the largest gain with contextualization on\nthe left to the largest loss with contextualization on the right. Data are presented as\nmean values +/- standard deviation. Adjusted p-value (with two signiﬁcant ﬁgures)\nfor each class is speciﬁed above the bars.C Precision-Recall curves of pLM- and\ngLM-based EC number linear probes.D Histogram of variance (# bins = 100) cal-\nculated using contextualized embeddings (gLM; orange) and contig-averaged pLM\n(blue) embeddings of MGYPs that occur at least 100 times in the database. Histo-\ngrams for unannotated and annotated fraction of the MGYPs are plotted separately\nand bars are not stacked. Annotated examples in the long right tail include phage\nproteins and transposases, reﬂecting their ability to self-mobilize (see annotations\nof top tens most variant genes in Supplementary Table 4). Source data are provided\nas a Source Dataﬁle.\nArticle https://doi.org/10.1038/s41467-024-46947-9\nNature Communications|         (2024) 15:2880 6\nexample of AAA+ regulators, we illustrate that combining the con-\ntextualized protein embeddings and attention-based operon interac-\ntion may provide an important avenue for exploring and\ncharacterizing the functional diversity of regulatory proteins.\ngLM predicts paralogy in protein-protein interactions\nProteins in an organism are found in complexes and interact physically\nwith each other. Recent advances in protein-protein interaction (PPI)\nprediction and structural complex research has largely been guided by\nidentifying interologs (conserved PPI across organisms) and co-\nevolutionary signals between residues\n47. However, distinguishing\nparalogs from orthologs (otherwise known as the“Paralog matching”\nproblem) in the expanding sequence dataset remains a computational\nchallenge requiring queries across the entire database and/or phylo-\ngenetic proﬁling. In cases where multiple interacting pairs are found\nwithin an organism (e.g. histidine kinases (HK) and response regulators\n(RR)), prediction of interacting pairs is particularly difﬁcult\n48.W e\nreasoned that gLM, although not directly trained for this task, may\nhave learned the relationships between paralogs versus orthologs. In\norder to test this capability, we used a well-studied example of inter-\nacting paralogs (ModC and ModA, Fig.5A) which form an ABC trans-\nporter complex. We queried gLM to predict the embedding of an\ninteracting pair given no context except the protein sequence of either\nModA or ModC. Weﬁnd that without anyﬁne-tuning gLM performs at\nleast an order of magnitude better than what is expected by random\nchance (see Methods). Speciﬁcally, for 398 out of 2700 interacting\npairs, gLM makes predictions that belong to the same cluster (50%\nsequence identity,n = 2100 clusters) as the true label, and in 73 pairs,\nthe gLM predicts a label that is closest to the exact interacting pair\n(simulated random chance expected match=1.6 ± 1.01,n = 10) (Fig.5B).\nImportantly, when considering only very high conﬁdence predictions\n(prediction likelihood > 0.9,n = 466), gLM is able to match paralogs\nwith an increased accuracy of 25.1%. When paralogs are correctly\npaired, gLM is more con ﬁdent about the prediction (average\nFig. 4 | Attention analysis. ACorrelation coefﬁcients (Pearson’s rho) between\nattention heads across layers and operons. Darker color corresponds to stronger\ncorrelation with previously identiﬁed operons. Attention patterns of the second\nlayer-seventh head [L2-H7] is most strongly correlated with the operons.B Three\nrandom examples of contigs and predicted operonic relationship between neigh-\nboring proteins. Proteins are listed in the order they are encoded in the contig.\nGround truthE.coli K-12 operons (top row), raw attention scores in the attention\nhead [L2-H7] most correlated with operons (middle row) and logistic regression\nprediction using all attention heads (last row) where false positive predictions (or\npossibly misannotated ground truths in the case ofﬂagellar proteins in theﬁrst\nexample) are marked in red.C Five-fold cross-validation precision-recall curves of\nlogistic regression trained using all operons and attention heads.D AAA+ regulator\nassociations characterized using attention-based prediction of operons (Extended\nFig. 11A) corresponding to labeled examples in panelsE and F. E ESM2 generated\ninput protein embeddings of AAA+ regulator proteins that are structural homologs\nto TnsC (grey and red; using Foldseek\n60). Structural homologs of TnsC with con-\nﬁrmed involvement in Tn7-like transposons upon manual inspection were desig-\nnated “TnsC-like AAA+ (manually inspected)” and are colored red. Other MGYP\nproteins annotated as“TnsC” against the UniRef90 database (orange) were added\nas positive controls for TnsC function. NuoA (NADH-quinone oxidoreductase\nsubunitA; purple) were added as structural and functional negative controls. DnaB\nhelicases (blues) were added as functional negative controls, as these proteins have\nsimilar folds to TnsC but are not associated with transposition.F Combined input\nprotein and context embeddings of genes in panelE. These embeddings are gen-\nerated through concatenation of pLM (ESM2) embeddings and context (last layer of\ngLM) embeddings. Negative controls (NuoA and DnaB helicases) form distinct\nclusters in bothE and F. Numbered labels in grey boxes indicate the AAA+ proteins\nwith various functional association predictions listed in panelD and Supplementary\nFig. 7. Raw distance based clustering of the embeddings are shown in Supple-\nmentary Fig. 8. Source data are provided as a Source Dataﬁle.\nArticle https://doi.org/10.1038/s41467-024-46947-9\nNature Communications|         (2024) 15:2880 7\nconﬁdence for correct prediction = 0.79, average conﬁdence across all\npredictions = 0.53), while less certain predictions are either out of\ndistribution, or closer to the mean of labels (Fig.5C). We attribute part\nof the inaccuracies in prediction due to the fact that gLM was not\ntrained on the task of predicting a masked gene given only a single\ngene as genomic context, though we expect the performance to\nimprove with expanding the training sequence length range andﬁne-\ntuning the model speciﬁcally for the“paralog matching” problem.\nContextualized contig embeddings and potential for transfer\nlearning\nContextualized protein embeddings encode the relationship between\nas p e c iﬁc protein and its genomic context, retaining the sequential\ninformation within a contig. We hypothesized that this contextualiza-\ntion adds biologically meaningful information that can be utilized for\nfurther characterization of the multi-gene genomic contigs. Here, we\ndeﬁne a contextualized contig embedding as a mean-pooled hidden\nlayer across all proteins in the subcontig, and a context-free contig\nembedding as mean-pooled ESM2 protein embeddings across the\nsequence (see methods). Both embeddings consist of 1280 features.\nWe test our hypothesis by examining each of these embeddings’ability\nto linearly distinguish viral sequences from bacterial and archaeal\nsubcontigs. In metagenomic datasets, the taxonomic identity of\nassembled sequences must be inferred post-hoc, therefore the iden-\ntiﬁcation of viral sequences is conducted based on the presence of\nviral genes and viral genomic signatures\n49.H o w e v e r ,s u c hc l a s s iﬁcation\ntask remains a challenge particularly for smaller contig fragments and\nless characterized viral sequences. Here, we sampled random 30-\nprotein subcontigs from the representative bacterial and archaeal\ngenome database and reference viral genomes in the NCBI and\nvisualized their context-free contig embeddings (Fig.5D) and con-\ntextualized contig embeddings (Fig.5E). We observed more separation\nand taxonomic clusters at both domain- and class-levels (Supplemen-\ntary Fig. 9), suggesting that taxonomic signature is enhanced by\nencoding the latent relationships between proteins. This is further\nvalidated by training a logistic regression classiﬁer on context-free and\ncontextualized contig embeddings for class-level taxonomy (Supple-\nmentary Fig. 9A, B), where we see a statistically signiﬁcant improve-\nment in average precision (Fig.5F, see Supplementary Fig. 7C, D for\nconfusion matrices). This emphasizes the biological importance of a\nprotein’s relative position in the genome and its relationship with the\ngenomic context, and further indicates that this information can be\neffectively encoded using gLM. Contextualized contig embeddings\npresent opportunities for transfer learning beyond viral sequence\nFig. 5 | Potential for transfer learning. AModA and ModC interaction (protein\ndata bank structure 2ONK)47 B UMAP projection of predictions (orange) and labels\n(blues) of paralogs (ModAC shown in A), where correct predictions are colored in\ngreen.C Predicted embeddings are colored based on the predicted conﬁdence. Out\nof distribution predictions and predictions closer to the mean are generally of\nlower conﬁdence, while correct predictions are of higher conﬁdence.D, ERandom\n30-gene contigs from representative bacterial (“bac”) and archaeal (“arch”)g e n -\nomes and reference viral (“vir”) genomes were embedded by mean-pooling ESM2\nprotein embeddings (context-free contig embeddings,D)a n db ym e a n - p o o l i n gt h e\nlast hidden layer of gLM (contextualized contig embeddings,E). F Micro-averaged\nprecision-recall curves and average precisions for logistic regression classiﬁers\ntrained using context-free contig embeddings (grey lines) and contextualized\ncontig embeddings (colored lines) for class-level taxonomy classiﬁcation task. Each\nline represents a fold in stratiﬁed k-fold cross-validation (k = 5). Class-level tax-\nonomy for each contig is shown in Supplementary Fig. 9A, B and the confusion\nmatrices for logistic regression classiﬁers are shown in Supplementary Fig. 9C, D.\nSource data are provided as a Source Dataﬁle.\nArticle https://doi.org/10.1038/s41467-024-46947-9\nNature Communications|         (2024) 15:2880 8\nprediction, such as improved metagenomically-assembled genome\n(MAG) binning and assembly correction.\nDiscussion\nThe unprecedented amount and diversity of metagenomic data, cou-\npled with advances in deep learning, presents exciting opportunities\nfor building models that can learn hidden patterns and structures of\nbiological systems. Such models build upon the conceptual and sta-\ntistical frameworks that evolutionary biologists have developed for the\npast century. With capabilities of abstracting much larger amounts of\ndata, these models can disentangle the extraordinary complexity of\norganismal genomes and their encoded functions; this is a key step in\nfurthering our understanding of biological processes. The work pre-\nsented here demonstrates and validates the concept of genomic lan-\nguage modeling. Our implementation of the masked genomic\nlanguage modeling illustrates the feasibility of training such a model,\nand provides evidence that biologically meaningful information is\nbeing captured in learned contextualized embeddings and yielding\nmeaningful interpretations of theattention patterns. We show that\ngLM can be used for diverse downstream tasks, including enzyme\nfunction prediction, operon prediction, paralog matching and contig\ntaxonomy prediction. Furthermore, we demonstrate gLM’s ability to\nilluminate context dependency in functions across structural and\nsequence homology through the example of AAA+ regulators. Taken\ntogether, gLM presents a highly promising direction for interpreting\nbiology and we propose key areas for further development: First, the\ntransformer architecture has shown to be successful in efﬁcient scal-\ning; in both natural language\n50 and protein language processing23,\nincreasing the number of parameters in the model along with the\ntraining dataset size have been shown to lead to vastly improved\nperformance and generalizability. Our model consists of ~1B para-\nmeters which is at least a magnitude smaller compared to state-of-the-\nart pLMs. With further hyperparameter tuning and scaling, we expect\nbetter performance of the model. Second, our model currently uses\npLM embeddings to represent proteins in the input. These embed-\ndings are generated by mean-pooling the amino acid residue-level\nhidden states across the protein sequence, and therefore the residue-\nspeciﬁc information and synonymous mutation effects are likely\nobscured. Future iterations of the model could use raw residue-level or\ncodon-level embeddings as input to allow modeling of residue-to-\nresidue co-evolutionary interactions between proteins and synon-\nymous mutation effects on gene function. Third, the task of recon-\nstructing masked protein embeddings requires modeling a\ndistribution over possible embeddings; our method approximates this\ndistribution using aﬁxed number of predictions. Future work could\nimprove upon this by using a generative approach, such as a diffusion\nor GAN model. This may allow for better prediction accuracy and\ngreater generalizability for unseen datasets. Fourth, adding non-\nprotein modalities (e.g. non-coding regulatory elements) as input to\ngLM may also greatly improve gLM’s representation of biological\nsequence data, and can learn protein function and regulation condi-\ntioned upon other modalities\n51. Finally, our model was trained largely\non bacterial, archaeal and viral genomes, therefore, how this method\ncan be adapted for eukaryotic genomes, especially those with exten-\nsive intergenic regions, remains to be further explored.\nOne of the most powerful aspects of the transformer-based lan-\nguage models is their potential for transfer learning andﬁne-tuning.\nWe tested some of the capabilities of gLM and successfully showed\nthat higher-order biological information, including gene function and\nregulation can be learned using genomic sequences. Our results\nhighlight the importance of contextualization of biological data, par-\nticularly as we scale our modeling efforts from biomolecules to whole\norganisms. We propose the following promising future directions for\napplying gLM for advancing biological research. 1) Feature-based\ntransfer learning for predicting protein function (e.g. Gene Ontology\n[GO] term), particularly those with limited sequence and structural\nhomology. 2) Fine-tuning gLM for the protein-protein-interactome\nprediction task. 3) Using gLM features to encode genomic contexts as\nadditional input for improved and contextualized protein structure\npredictions. In conclusion, genomic language modeling is a powerful\ntool to unbiasedly condense important biological information from\nfull metagenomic sequences. Coupled with the advances in long-read\nsequencing, we expect a drastic increase in the input data quality,\nquantity and diversity. Genomic language modeling presents an ave-\nnue to bridge the gap between atomic structure and organismal\nfunction, and thereby brings us closer to modeling biological systems,\nand ultimately, manipulating biology with precision (e.g. genome\nediting, synthetic biology).\nMethods\nSequence database\nThe genomic corpus was generated using the MGnify27 dataset\n(released 2022-05-06 and downloaded 2022-06-07). First, genomic\ncontigs with greater than 30 genes were divided into 30 gene non-\noverlapping subcontigs resulting in a total of 7,324,684 subcontigs\nwith lengths between 15 and 30 genes (subcontigs <15 genes in length\nwere removed from the dataset). We chose 30 as maximum context\nlength because while longer context results in higher modeling per-\nformance (Supplementary Fig. 10A), 67% of the raw MGnify contigs\nwith > 15 genes were of =<30 genes in length (Supplementary Fig. 10B),\nand therefore increasing the context length beyond 30 would have\nresulted in many examples with padding (reduced computational\nefﬁciency). Each gene in the subcontig was mapped to a representative\nprotein sequence (representative MGYP) using mmseqs/linclust\n52,w i t h\ncoverage and sequence identity thresholds set at 90% (pre-computed\nin the MGnify database), resulting in a total of 30,800,563 repre-\nsentative MGYPs. Each representative MGYP was represented by a\n1280-feature protein embedding, generated by mean-pooling the last\nhidden layer of the ESM2\n23 “esm2_t33_650M_UR50D” model. Due to the\nmemory limitation in computing embeddings for very long sequences,\n116 of the MGYP sequences longer than 12290 amino acids were\ntruncated to 12290 amino acids. ESM2 embeddings were normalized\n(by subtracting the mean of each feature and dividing by its standard\ndeviation) and clipped such that all features range from−10 to 10, to\nimprove training stability. A small fraction (0.4%) of the genes could\nnot be mapped to a representative MGYP and therefore the corre-\nsponding sequence information could not be retrieved from the\nMGnify server; these sequences were assigned a 1280 feature vector of\nones. For each gene in the sub-sequence, we added a gene orientation\nfeature to the standardized MGYP protein embedding, where 0.5\ndenotes “forward” orientation relative to the direction of sequencing,\nand −0.5 denotes“reverse” orientation. Thus, each gene was repre-\nsented by a 1281 feature vector in our corpus.\ngLM architecture and training\ngLM was built on the huggingface implementation of the RoBERTa53\ntransformer architecture. gLM consisted of 19 layers with hidden size\n1280 and ten attention heads per layer, with relative position embed-\nding (“relative_key_query”)\n54. For training, 15% of the tokens (genes) in\nthe sequence (subcontig) were randomly masked to a value of−1. We\nthen tasked the model with the objective of predicting the label of the\nmasked token, where the label consists of a 100-feature vector that\nconsists of the PCA whitened 99 principal components (explained\nvariance = 89.7%. Supplementary Fig. 11) of the corresponding ESM2\nprotein embedding concatenated with its orientation feature. Reduced\ndimensionality of labels using PCA increased the stability of training.\nSpeciﬁcally, gLM projects the last hidden state of the model into four\n100-feature vectors and four corresponding likelihood values using a\nArticle https://doi.org/10.1038/s41467-024-46947-9\nNature Communications|         (2024) 15:2880 9\nlinear layer. Total loss is calculated using the following Eq. (1).\nMSEðclosest prediction,labelÞ\n+ α /C3 CrossEntropyLoss likelihoods,closest prediction indexðÞ ð1Þ\nThe closest prediction is deﬁned as the prediction that is closest to the\nlabel, computed by L2 distance. We setα = 1e-4. gLM was trained in\nhalf-precision with batch size 3000 with distributed data paralleliza-\ntion on four NVIDIA A100 GPUs over 1,296,960 steps (560 epochs),\nincluding 5000 warm-up steps to reach a learning rate of 1e-4 with\nAdamW\n55 optimizer.\nPerformance metric and validation\nIn order to evaluate the model quality and its generalizability beyond\nthe training dataset, we use a pseudo-accuracy metric, where we deem\na prediction to be“correct” if it is closest in Euclidean distance to the\nlabel of the masked gene relative to the other genes in the subcontig.\nPseudo-accuracy calculation is described in Eq. (2).\npseudo accuracy\n= #countðargminðdistðprediction,labels in subcontigÞÞ = = indexðmasked geneÞÞ\n#masked genes\nð2Þ\nWe chose to validate our metric and subsequent analyses on the\nbest-annotated genome to date:E.coli K-1256. In order to remove as\nmany E.coliK-12 like subcontigs from the training dataset, we removed\n5.2% of the subcontigs in which more than half of the genes were > 70%\nsimilar (calculated using mmseqs2 search\n52) in amino acid sequence to\nE.coli K-12 genes. We validate our pseudo accuracy metric by calcu-\nlating the absolute accuracy on theE.coli K-12 genome for which each\ngene was masked sequentially (Eq. (3))\nabsolute accuracy\n= #countðargminðdistðprediction,all genes in E:coli K/C0 12ÞÞ = = indexðmasked geneÞÞ\n#genes in E:coli K/C0 12\nð3Þ\nContextualized embedding calculation and visualization\nContextualized protein embedding of a gene is calculated byﬁrst\ninputting a 15-30 gene subcontig containing the gene of interest, and\nthen running inference on the subcontig using the trained gLM with-\nout masking. We then use the last hidden layer of the model corre-\nsponding to the gene as the embedding consisting of 1280 features.\nGene annotation\nGenes were annotated using Diamond v2.0.7.14557 against the Uni-\nRef90 database58 with an e-value cut-off 1E-5. Genes were labeled as\n“unannotated” if either 1) no match was found in the UniRef90 data-\nbase, or 2) the match was annotated with following keywords:“unan-\nnotated”, “uncharacterized”, “hypothetical”, “DUF”(domain of\nunknown function).\nMcrA protein analysis\nMcrA protein encoding Methanogens and ANME genomes were\nselected from the accession ID list found in the supplement of Shao\net al.\n35. subcontigs containingmcrA were extracted with at most 15\ngenes before and aftermcrA. The context-free and contextualized\nembeddings of McrA were calculated using the ESM2 and gLM,\nrespectively.\nDistributions of unannotatedand annotated embeddings\nDistributions of unannotated and annotated embeddings in the data-\nbase were compared using Kullback-Leibler (KL) divergence analysis.\nFirst, ten random samples of 10,000 subcontigs from the MGnify\ncorpus. pLM and gLM embeddings of the genes were calculated using\nmean-pooled last hidden layer of ESM2 embeddings and mean-pooled\nlast hidden layer of gLM, respectively. Outliers were removed using\nMahalanobis distance and a chi-squared threshold of 0.975. pLM and\ngLM embedding dimensions were reduced to 256 principal compo-\nnents (91.9 ± 1.72% and 80.1 ± 6.89% total variances explained, respec-\ntively). KL divergence was calculated using the following Eq. (4).\nD\nKL ðPjjQÞ = 1\n2 trðΣ/C0 1\n1 Σ0Þ/C0 k + ðμ1 /C0 μ2ÞT Σ/C0 1\n1 ðμ1 /C0 μ0Þ +l n detΣ1\ndet 0\n/C18/C19/C18/C19\nð4Þ\nwhere P corresponds to the distribution of unannotated genes and Q\ncorresponds to the distribution of annotated genes, with μ0,μ1\nrespectively as means andΣ0,Σ1 respectively as covariance matrices.\nThe signiﬁcance of the KL divergence differences between pLM and\ngLM embeddings is calculated using a paired t-test across the ten\nsamples.\nEnzyme Commission number prediction\nCustom MGYP-Enzyme Commission (MGYP-EC) dataset was created\nby ﬁrst searching (mmseqs2\n52 with default setting) MGYPs against the\n“split30.csv” dataset previously used to train CLEAN59. “split30.csv”\ndataset consists of EC numbers assigned to UniProt sequences clus-\ntered at 30% identity. Only MGYP hits with >70% sequences to\n“split30.csv” were considered and MGYPs with multiple hits with >70%\nsimilarity were removed. Test split was selected by randomly selecting\n10% of “split30.csv” UniProt IDs in each EC category that map to\nMGYPs. EC categories with less than four distinct UniProt IDs with\nMGYP mapping were removed from the dataset, resulting in 253 EC\ncategories. The train set consisted of MGnify subcontigs in the corpus\nthat contained at least one the 27936 MGYPs mapping to 1878 UniProt\nIDs. The test set consisted of randomly selected MGnify subcontig\ncontaining each of 4441 MGYPs mapping to 344 UniProt IDs. pLM\n(context-free) embeddings were calculated for each of MGYP with EC\nnumber assignment by mean-pooling the last hidden layer of its ESM2\nembedding. Masked (context-only) gLM embeddings were calculated\nfor each of the 19 layers by running inference on subcontigs with masks\nat the positions of MGYPs with EC number assignment and subse-\nquently extracting per-layer hidden representations for masked posi-\ntions. gLM (contextualized) embeddings were calculated also for each\nlayer by running inference without masking and subsequently\nextracting per-layer hidden representations for MGYPs with EC num-\nber assignments. Linear probing was conducted for these embeddings\nwith a single linear layer. Linear probes were trained with early stop-\nping (patience = 10, github.com/Bjarten/early-stopping-pytorch/blob/\nmaster/pytorchtools.py) and batch size = 5000, and training results\nwere replicatedﬁve times with random seeds to calculate error ranges.\nVariance of contextualized protein embedding analysis\nContextualized protein embeddings are generated at inference time.\nVariances of contextualized protein embeddings were calculated for\nMGYPs that occur at least 100 times in the dataset, excluding the\no c c u r r e n c e sa tt h ee d g e so ft h es u b c o n t i g(ﬁrst or last token). For each\nsuch MGYP, we take 10 random independent samples consisting of 100\noccurrences and calculate the mean pairwise euclidean distances\nbetween the contextualized embeddings. To assess the role gLM plays\nin contextualization, we used the above sampling method to calculate\nthe variance of contig-averaged pLM embeddings (pLM embeddings\nmean-pooled across the contig) for each MGYP that occurs at least 100\ntimes in the dataset.\nArticle https://doi.org/10.1038/s41467-024-46947-9\nNature Communications|         (2024) 15:2880 10\nAttention analysis\nAttention heads (n = 190) were extracted by running inference on\nunmasked subcontigs, and the raw attention weights were subse-\nquently symmetrized.E.coliK-12 RegulonDB\n56 was used to probe heads\nwith attention patterns that correspond the most with operons. Pear-\nson’s correlation between symmetrized raw attentions and operons\nwere calculated for each head. We trained a logistic regression classi-\nﬁer that predicts whether two neighboring genes belong to the same\noperon based on the attention weights across all attention heads\ncorresponding to the gene pair.\nTnsC structural homolog analysis\nTnsC structural homologs were identiﬁed by searching ShCAST TnsC\n(PDB 7M99 chain H) against the MGYP database using Foldseek60 on\nESM Atlas ( https://esmatlas.com/). The contigs containing these\nhomologs in the MGnify database were used to calculate the con-\ntextualized protein embeddings of the identiﬁed structural homologs.\nContigs with less than 15 genes were excluded from the analysis.\nContigs encoding proteins that were previously identiﬁed as“TnsC”\nusing the UniRef90 database (see Gene annotation methods section\nabove) were included in the database. “TnsC-like” contigs were\nmanually annotated based on the presence of transposase genes\n(TnsB) and TniQ. Fifty random examples of MGnify contigs containing\nMGYPs annotated as NuoA and DnaB were added as negative controls\nfor the UMAP visualization. We calculated KL divergence ratios using\nthe following Eq. (5).\nDKL ðBjjAÞ\nDKL ðCjjAÞ ð5Þ\nwhere A is the distribution of representations of known TnsC, B is the\ndistribution of representations of manually curated TnsC-like AAA+\nregulators, C is the distribution of representations of other AAA+\nregulators that are functionally unrelated structural homologs of\nknown TnsC. Therefore, this metric ranges from 0 to 1, where a lower\nratio represents increased ability to functionally discriminate distribu-\ntion of B from C relative to A. KL divergence was calculated using the\nsame formula as in the methods section Distributions of unannotated\nand annotated embeddings, except with 20 principal components that\nexplained >85% of variances across all embeddings.\nParalogy and orthology analysis\nUniProt IDs from ABC transporter ModA and ModC protein interacting\nparalog pairs (n = 4823) were previously identiﬁed by Ovchinnikov\net al.\n47 a n dw e r ed o w n l o a d e df r o mhttps://gremlin.bakerlab.org/cplx.\nphp?uni_a=2ONK_A&uni_b=2ONK_Cand subsequently used to down-\nload raw protein sequences from the UniProt server. Only pairs\n(n = 2700) where both raw sequences were available for download, and\nwhere the UniProt ID differed by one (indicating adjacent positioning\nin the reference genome) were selected for subsequent analyses. We\nconstructed test contigs consisting of three genes, whereﬁrst and\nthird genes are masked, and the second gene encodes one of the pair\nin forward direction. We then queried gLM to predict the two neigh-\nboring masked genes, and considered the prediction to be correct if\neither of the proteins closest to masked genes’s highest conﬁdence\nprediction in embedding space belongs to the same sequence cluster\nas the interacting protein (50% amino acid sequence identity, calcu-\nlated using CD-HIT v4.6\n61). Random chance correct prediction rate\n(1.6 ± 1.0 was simulated using 1000 iterations of random predictions\ngenerated within the standard normal distribution and performing the\nsame operation as above to compute the rate of correct predictions\n62.\nTaxonomic analysis and visualization\n4551 bacterial and archeal representative genomes and 11660\nreference viral genomes were downloaded from the RefSeq\ndatabase (ftp://ftp.ncbi.nlm.nih.gov/genomes/refseq )o n1 2F e b\n2023. A random 30-gene subcontig is chosen and encoded using\nESM2, which then were subsequently concatenated with an\norientation vector and then used as input for the trained gLM.\nThe last hidden layer was mean-pooled across the sequence to\nretrieve 1280-feature contextualized contig embeddings. The\nESM2 protein embeddings were also mean-pooled across the\nsequence to retrieve 1280-feature context-free contig embed-\ndings. We trained a logistic regression classiﬁer to predict the\nclass-level taxonomy of subcontigs and evaluated the perfor-\nmance using stratiﬁed k-fold cross-validation (k = 5).\nUMAP visualization and statistical tests\nAll UMAP dimensionality reductions calculated with following para-\nmeters: n_neighbors = 15, min_dist = 0.1. Silhouette scores were cal-\nculated using the sklearn package using the default setting with\neuclidean distance metric.\nReporting summary\nFurther information on research design is available in the Nature\nPortfolio Reporting Summary linked to this article.\nData availability\nDataset used for training is available for download from the MGnify\nserver ( http://ftp.ebi.ac.uk/pub/databases/metagenomics/peptide_\ndatabase/2022_05/). The model is available at zenodo under acces-\nsion number 10.5281/zenodo.7855545. Source data for the main Fig.\n(2c,e,f,i&h; 3a,b,c&d; 4a,b,c,e&f; 5b,c,d,e&f) and supplementary Figs.\n(2,3,4,5,7,8,9,10 &11) are provided with this paper as a zipﬁle. Source\ndata are provided with this paper.\nCode availability\nTraining and inference code and analysis scripts are available at\nhttps://github.com/y-hwang/gLM (https://doi.org/10.5281/zenodo.\n10512240).\nReferences\n1 . R e d f e r n ,O .C . ,D e s s a i l l y ,B .&O r e n g o ,C .A .E x p l o r i n gt h es t r u c t u r e\nand function paradigm.Curr. Opin. Struct. Biol.18,3 9 4– 402\n(2008).\n2. Jumper, J. et al. Highly accurate protein structure prediction with\nAlphaFold.Nature 596,5 8 3– 589 (2021).\n3. Baek, M. et al. Accurate prediction of protein structures and inter-\nactions using a three-track neural network.Science 373,\n871– 876 (2021).\n4. Rives, A. et al. Biological structure and function emerge from\nscaling unsupervised learning to 250 million protein sequences.\nProc. Natl. Acad. Sci. USA. 118, e2016239118 (2021).\n5. Elnaggar, A. et al. ProtTrans: Toward Understanding the Language\nof Life Through Self-Supervised Learning. IEEE Trans. Pattern Anal.\nMach. Intell.44, 7112– 7127 (2022).\n6. Madani, A. et al. Large language models generate functional protein\nsequences across diverse families.Nat. Biotechnol. 41,\n1099– 1106 (2023).\n7. Outeiral, C. & Deane, C. M. Codon language embeddings provide\nstrong signals for use in protein engineering.Nat Mach Intell6,\n170– 179 (2024).\n8. Wright, S. On the roles of directed and random changes in gene\nfrequency in the genetics of populations.Evolution2,\n279– 294 (1948).\n9. Lynch, M. & Conery, J. S. The Origins of Genome Complexity.Sci-\nence 302,1 4 0 1– 1404 (2003).\n1 0 . C o r d e r o ,O .X .&P o l z ,M .F .E x p l a i n i n gm i c r o b i a lg e n o m i cd i v e r s i t y\nin light of evolutionary ecology.Nat. Rev. Microbiol.12,\n263– 273 (2014).\nArticle https://doi.org/10.1038/s41467-024-46947-9\nNature Communications|         (2024) 15:2880 11\n11. Treangen, T. J. & Rocha, E. P. C. Horizontal transfer, not duplication,\ndrives the expansion of protein families in prokaryotes.PLoS Genet.\n7, e1001284 (2011).\n12. Shapiro, B. J. et al. Population genomics of early events in the\necological differentiation of bacteria.Science 336,4 8– 51 (2012).\n13. Kountz, D. J. & Balskus, E. P. Leveraging Microbial Genomes and\nGenomic Context for Chemical Discovery.Acc. Chem. Res.54,\n2788– 2797 (2021).\n14. Shmakov, S. A. et al. Systematicprediction of functionally linked\ngenes in bacterial and archaeal genomes.Nat. Protoc.14,\n3013– 3031 (2019).\n15. Yelton, A. P. et al. A semi-quantitative, synteny-based method to\nimprove functional predictions for hypothetical and poorly anno-\ntated bacterial and archaeal genes.PLoS Comput. Biol.7,\ne1002230 (2011).\n16. Miller, D., Stern, A. & Burstein, D. Deciphering microbial gene\nfunction using natural language processing.Nat. Commun.13,\n5731 (2022).\n17. Konno, N. & Iwasaki, W. Machine learning enables prediction of\nmetabolic system evolution in bacteria.Sci. Adv.9,\neadc9130 (2023).\n18. Avsec, Ž. et al. Effective gene expression prediction from sequence\nby integrating long-range interactions.Nat. Methods18,\n1196– 1203 (2021).\n19. Ji, Y., Zhou, Z., Liu, H. & Davuluri, R. V. DNABERT: pre-trained Bidir-\nectional Encoder Representations from Transformers model for\nDNA-language in genome.Bioinformatics37, 2112– 2120 (2021).\n20. Dalla-Torre, H. et al. The Nucleotide Transformer: Building and\nEvaluating Robust Foundation Models for Human Genomics.bioR-\nxiv 2023.01.11.523679https://doi.org/10.1101/2023.01.11.\n523679 (2023).\n21. Nguyen, E. et al. HyenaDNA: Long-Range Genomic Sequence\nModeling at Single Nucleotide Resolution. arXiv:2306.15794v2.\n[Preprint] (2023).\n22. Zvyagin, M. et al. GenSLMs: Genome-scale language models reveal\nSARS-CoV-2 evolutionary dynamics.bioRxiv https://doi.org/10.\n1101/2022.10.10.511571(2022).\n23. Lin, Z. et al. Evolutionary-scale prediction of atomic-level protein\nstructure with a language model.Science 379, 1123– 1130 (2023).\n24. Vaswani, A. et al. Attention is All you Need. InAdvances in Neural\nInformation Processing SystemsVol. 30 (2017).\n25. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training\nof Deep Bidirectional Transformers for Language Understanding. In\nProceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1(Long and Short Papers) 4171–\n4186 https://\ndoi.org/10.18653/v1/N19-1423(2019).\n26. Hochreiter, S. & Schmidhuber, J. Long short-term memory.Neural\nComput 9,1 7 3 5– 1780 (1997).\n27. Richardson, L. et al. MGnify: the microbiome sequence data ana-\nlysis resource in 2023.Nucleic Acids Res51,D 7 5 3– D759 (2023).\n28. Blattner, F. R. et al. The complete genome sequence of Escherichia\ncoli K-12.Science 277,1 4 5 3– 1462 (1997).\n29. Jeffery, C. J. Protein moonlighting: what is it, and why is it impor-\ntant? Philos. Trans. R. Soc. Lond. B Biol. Sci.373,2 0 1 6 0 5 2 3( 2 0 1 8 ) .\n30. Miskei, M. et al. Fuzziness enables context dependence of protein\ninteractions.FEBS Lett.591,2 6 8 2– 2695 (2017).\n31. Gherardini, P. F., Wass, M. N., Helmer-Citterich, M. & Sternberg, M. J.\nE. Convergent evolution of enzyme active sites is not a rare phe-\nnomenon.J. Mol. Biol.372,8 1 7– 845 (2007).\n32. Ben-Hur, A. & Brutlag, D. Remote homology detection: a motif\nbased approach.Bioinformatics19,i 2 6– i33 (2003).\n33. Bertram, S. et al. Methanogeniccapabilities of ANME-archaea\ndeduced from\n13C-labelling approaches.Environmental Micro-\nbiology 15,2 3 8 4– 2393 (2013).\n34. Moran, J. J., House, C. H., Thomas, B. & Freeman, K. H. Products of\ntrace methane oxidation during nonmethyltrophic growth by\nMethanosarcina.J. Geophys. Res.112, G02011 (2007).\n35. Shao, N. et al. Expression of divergent methyl/alkyl coenzyme M\nreductases from uncultured archaea.Commun. Biol.5, 1113\n(2022).\n36. Coenen, A. et al. Visualizing and Measuring the Geometry of BERT.\nIn: Proceedings of the Neural Information Processing Systems,2 0 1 9 .\n37. Vanni, C. et al. Unifying the known and unknown microbial coding\nsequence space.Elife 11, e67667 (2022).\n38. Bileschi, M. L. et al. Using deep learning to annotate the protein\nuniverse.Nat. Biotechnol.40,9 3 2– 937 (2022).\n39. Rogers, A., Kovaleva, O. & Rumshisky, A. A Primer in BERTology:\nWhat We Know About How BERT Works.Transactions of the Asso-\nciation for Computational Linguistics8,8 4 2– 866 (2020).\n40. Vig, J. et al. BERTology Meets Biology: Interpreting Attention in\nProtein Language Models. In:Proceedings of the International\nConference on Learning Representations,2 0 2 1 .\n41. Salgado, H. et al. Using RegulonDB, the Escherichia coli K-12 Gene\nRegulatory Transcriptional Network Database.Curr. Protoc. Bioin-\nforma. 61,1 . 3 2 . 1– 1.32.30 (2018).\n42. White, S. R. & Lauring, B. AAA+ ATPases: achieving diversity of\nfunction with conserved machinery.Trafﬁc 8,1 6 5 7– 1667 (2007).\n43. Park, J.-U. et al. Structures of the holo CRISPR RNA-guided trans-\nposon integration complex.Nature 613,7 7 5– 782 (2023).\n4 4 . R y b a r s k i ,J .R . ,H u ,K . ,H i l l ,A .M . ,W i l k e ,C .O .&F i n k e l s t e i n ,I .J .\nMetagenomic discovery of CRISPR-associated transposons.Proc.\nNatl Acad. Sci. USA118, e2112279118 (2021).\n45. Benler, S. et al. Cargo Genes of Tn7-Like Transposons Comprise an\nEnormous Diversity of Defense Systems, Mobile Genetic Elements,\nand Antibiotic Resistance Genes.MBio 12, e0293821 (2021).\n46. Klompe, S. E., Vo, P. L. H., Halpin-Healy, T. S. & Sternberg, S. H.\nTransposon-encoded CRISPR– Cas systems direct RNA-guided DNA\nintegration.Nature 571,2 1 9– 225 (2019).\n47. Ovchinnikov, S., Kamisetty, H. & Baker, D. Robust and accurate\nprediction of residue-residue interactions across protein interfaces\nusing evolutionary information.Elife 3\n,e 0 2 0 3 0( 2 0 1 4 ) .\n48. Sgarbossa, D., Lupo, U. & Bitbol, A.-F. Pairing interacting protein\nsequences using masked language modeling. In:Proceedings of\nthe ICLR 2024 Workshop on Machine Learning for Genomics\nExplorations,2 0 2 4 .\n49. Guo, J. et al. VirSorter2: a multi-classiﬁer, expert-guided approach\nto detect diverse DNA and RNA viruses.Microbiome9, 37 (2021).\n50. Kaplan, J. et al. Scaling Laws for Neural Language Models.arXiv\n[cs.LG] (2020).\n51. Kiros, R., Salakhutdinov, R. & Zemel, R. Multimodal Neural Lan-\nguage Models. In:Proceedings of the 31st International Conference\non Machine Learning,V o l .3 2 ,N o .2 ,p p .5 9 5– 603. PMLR, Beijing,\nChina, 22– 24 Jun 2014.\n52. Steinegger, M. & Söding, J. MMseqs2 enables sensitive protein\nsequence searching for the analysis of massive data sets.Nat.\nBiotechnol.35,1 0 2 6– 1028 (2017).\n53. Liu, Y. et al. RoBERTa: A Robustly Optimized BERT Pretraining\nApproach.arXiv [cs.CL](2019).\n54. Huang, Z., Liang, D., Xu, P. & Xiang, B. In: Cohn, T., He, Y. & Liu, Y.\n(eds.) Findings of the Association for Computational Linguistics:\nEMNLP 2020, pp. 3327– 3335. Association for Computational Lin-\nguistics, Online, Nov 2020.https://doi.org/10.18653/v1/2020.\nﬁndings-emnlp.298.\n55. Loshchilov, I. & Hutter, F. Decoupled Weight Decay Regularization.\nIn: Proceedings of the InternationalConference on Learning Repre-\nsentations,2 0 1 9 .\n56. Tierrafría, V. H. et al. RegulonDB 11.0: Comprehensive high-\nthroughput datasets on transcriptional regulation in Escherichia\ncoli K-12.Micro. Genom.8, mgen000833 (2022).\nArticle https://doi.org/10.1038/s41467-024-46947-9\nNature Communications|         (2024) 15:2880 12\n57. Buchﬁnk, B., Xie, C. & Huson, D. H. Fast and sensitive protein\nalignment using DIAMOND.Nat. Methods12,5 9– 60 (2015).\n58. Suzek, B. E., Huang, H., McGarvey, P., Mazumder, R. & Wu, C. H.\nUniRef: comprehensive and non-redundant UniProt reference\nclusters.Bioinformatics23,1 2 8 2– 1288 (2007).\n59. Yu, T. et al. Enzyme function prediction using contrastive learning.\nScience 379,1 3 5 8– 1363 (2023).\n60. van Kempen, M. et al. Fast and accurate protein structure search\nwith Foldseek.Nat. Biotechnol.42,2 4 3– 246 (2024).\n6 1 . L i ,W . ,J a r o s z e w s k i ,L .&G o d z i k, A. Clustering of highly homologous\nsequences to reduce the size of large protein databases.Bioinfor-\nmatics 17,2 8 2– 283 (2001).\n62. Piovesan, A., Caracausi, M., Antonaros, F., Pelleri, M. C. & Vitale, L.\nGeneBase 1.1: a tool to summarize data from NCBI gene datasets\nand its application to an update of human gene statistics.Database\n2016, baw153 (2016).\nAcknowledgements\nWe would like to thank the EBI MGnify team for generating and main-\ntaining the metagenome database. We would also like to thank Meta AI’s\nESM developers who made both the folded MGnify proteins structures\nand source-code openly available. We also thank Simon Roux and\nLanden Goszashti for insightful discussions. This work was supported by\nthe Gordon and Betty Moore Foundation grant #9208 to P.R.G., NSF\nOCE-1635365 to P.R.G, and by the National Aeronautics and Space\nAdministration under grant no. 80NSSC18K1140 and 80NSSC19K1427\nissued through the NASA Network for Life Detection program to P.R.G.\nS.O. was supported by NIH Grant No. DP5OD026389 and NSF Grant No.\nM C B 2 0 3 2 2 5 9 .T h ec o m p u t a t i o n si nt h i sp a p e rw e r er u no nt h eF A S R C\nCannon cluster supported by the FAS Division of Science Research\nComputing Group at Harvard University.\nAuthor contributions\nY.H. prepared the datasets and trained the model with support from\nA.L.C. and S.O.; E.H.K. and Y.H. conducted the TnsC analysis; A.L.C., S.O.\nand P.R.G. provided input in analysis and data interpretation; Y.H. wrote\nthe manuscript with input from all authors; All authors read and\napproved theﬁnal manuscript.\nCompeting interests\nA provisional patent (App. Serial No.: 63/491,019) on this work wasﬁled\nby Harvard University with YH and SO as inventors. The remaining\nauthors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41467-024-46947-9.\nCorrespondenceand requests for materials should be addressed to\nYunha Hwang, Sergey Ovchinnikov or Peter R. Girguis.\nPeer review informationNature Communicationsthanks Christian Dal-\nlago, Michael Heinzinger and the other, anonymous, reviewer(s) for their\ncontribution to the peer review of this work. A peer reviewﬁle is available.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’sn o t eSpringer Nature remains neutral with regard to jur-\nisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate if\nchanges were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2024\nArticle https://doi.org/10.1038/s41467-024-46947-9\nNature Communications|         (2024) 15:2880 13",
  "topic": "ENCODE",
  "concepts": [
    {
      "name": "ENCODE",
      "score": 0.6396887898445129
    },
    {
      "name": "Computational biology",
      "score": 0.6018531918525696
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5282741189002991
    },
    {
      "name": "Biology",
      "score": 0.5237138867378235
    },
    {
      "name": "Phylogenetic tree",
      "score": 0.45867854356765747
    },
    {
      "name": "Gene",
      "score": 0.4512655735015869
    },
    {
      "name": "Metagenomics",
      "score": 0.43201589584350586
    },
    {
      "name": "Function (biology)",
      "score": 0.4236019253730774
    },
    {
      "name": "Genetics",
      "score": 0.31799954175949097
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}