{
  "title": "Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs on the Edge",
  "url": "https://openalex.org/W4393160423",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5101655527",
      "name": "Xuan Shen",
      "affiliations": [
        "Universidad del Noreste"
      ]
    },
    {
      "id": "https://openalex.org/A5089349472",
      "name": "Peiyan Dong",
      "affiliations": [
        "Universidad del Noreste"
      ]
    },
    {
      "id": "https://openalex.org/A5100309925",
      "name": "Lu Lei",
      "affiliations": [
        "Universidad del Noreste"
      ]
    },
    {
      "id": "https://openalex.org/A5078971265",
      "name": "Zhenglun Kong",
      "affiliations": [
        "Universidad del Noreste"
      ]
    },
    {
      "id": "https://openalex.org/A5101633365",
      "name": "Zhengang Li",
      "affiliations": [
        "Universidad del Noreste"
      ]
    },
    {
      "id": "https://openalex.org/A5100587182",
      "name": "Lin Ming",
      "affiliations": [
        "Oracle (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5038380400",
      "name": "Chao Wu",
      "affiliations": [
        "Universidad del Noreste"
      ]
    },
    {
      "id": "https://openalex.org/A5100651384",
      "name": "Yanzhi Wang",
      "affiliations": [
        "Universidad del Noreste"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6600466347",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2903972532",
    "https://openalex.org/W2777406049",
    "https://openalex.org/W6798355455",
    "https://openalex.org/W4285601701",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W4308242520",
    "https://openalex.org/W4379548477",
    "https://openalex.org/W4292119927",
    "https://openalex.org/W4360831786",
    "https://openalex.org/W4307770535",
    "https://openalex.org/W4385764558",
    "https://openalex.org/W4309591680",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4382466174",
    "https://openalex.org/W2963122961",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4379260375",
    "https://openalex.org/W4377864164",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4309386164",
    "https://openalex.org/W4384919461",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4221156361"
  ],
  "abstract": "Large Language Models (LLMs) stand out for their impressive performance in intricate language modeling tasks. However, their demanding computational and memory needs pose obstacles for broad use on edge devices. Quantization is then introduced to boost LLMs' on-device efficiency. Recent works show that 8-bit or lower weight quantization is feasible with minimal impact on end-to-end task performance, while the activation is still not quantized. On the other hand, mainstream commodity edge devices still struggle to execute these sub-8-bit quantized networks effectively. In this paper, we propose Agile-Quant, an Activation-Guided quantization framework for faster Inference of popular Large Language Models (LLMs) on the Edge. Considering the hardware profiling and activation analysis, we first introduce a basic activation quantization strategy to balance the trade-off of task performance and real inference speed. Then we leverage the activation-aware token pruning technique to reduce the outliers and the adverse impact on attentivity. Ultimately, we utilize the SIMD-based 4-bit multiplier and our efficient TRIP matrix multiplication to implement the accelerator for LLMs on the edge. We apply our framework on different scales of LLMs including LLaMA, OPT, and BLOOM with 4-bit or 8-bit for the activation and 4-bit for the weight quantization. Experiments show that Agile-Quant achieves simultaneous quantization of model weights and activations while maintaining task performance comparable to existing weight-only quantization methods. Moreover, in the 8- and 4-bit scenario, Agile-Quant achieves an on-device speedup of up to 2.55x compared to its FP16 counterparts across multiple edge devices, marking a pioneering advancement in this domain.",
  "full_text": "Agile-Quant: Activation-Guided Quantization\nfor Faster Inference of LLMs on the Edge\nXuan Shen*1, Peiyan Dong*1, Lei Lu1, Zhenglun Kong1,\nZhengang Li1, Ming Lin‚Ä†2, Chao Wu1, Yanzhi Wang1\n1Northeastern University\n2Oracle\n{shen.xu, dong.pe, lu.lei1, kong.zhe, li.zhen, cha.wu, yanz.wang}@northeastern.edu\nlinming04@gmail\nAbstract\nLarge Language Models (LLMs) stand out for their impres-\nsive performance in intricate language modeling tasks. How-\never, their demanding computational and memory needs pose\nobstacles for broad use on edge devices. Quantization is\nthen introduced to boost LLMs‚Äô on-device efficiency. Recent\nworks show that 8-bit or lower weight quantization is fea-\nsible with minimal impact on end-to-end task performance,\nwhile the activation is still not quantized. On the other hand,\nmainstream commodity edge devices still struggle to execute\nthese sub-8-bit quantized networks effectively. In this paper,\nwe propose Agile-Quant, an A\nctivation-Guided quantization\nframework for faster I nference of popular L arge Language\nModels (LLMs) on the Edge. Considering the hardware pro-\nfiling and activation analysis, we first introduce a basic acti-\nvation quantization strategy to balance the trade-off of task\nperformance and real inference speed. Then we leverage the\nactivation-aware token pruning technique to reduce the out-\nliers and the adverse impact on attentivity. Ultimately, we uti-\nlize the SIMD-based 4-bit multiplier and our efficient TRIP\nmatrix multiplication to implement the end-to-end accelera-\ntor for LLMs on multiple edge devices. We apply our frame-\nwork on different scales of LLMs including LLaMA, OPT,\nand BLOOM with 4-bit or 8-bit for the activation and 4-bit\nfor the weight quantization. Experiments show that Agile-\nQuant achieves simultaneous quantization of model weights\nand activations while maintaining task performance compa-\nrable to existing weight-only quantization methods. More-\nover, in the 8- and 4-bit scenario, Agile-Quant achieves an\non-device speedup of up to 2.55x compared to its FP16 coun-\nterparts across multiple edge devices, marking a pioneering\nadvancement in this domain.\nIntroduction\nLarge Language Models (LLMs) (Touvron et al. 2023;\nZhang et al. 2022; Brown et al. 2020a; Radford et al. 2019;\nBrown et al. 2020b) based on the Transformer (Vaswani\net al. 2017) family have breakthrough performance in Natu-\nral Language Processing (NLP) research area.\nApplication Scenarios. In real-world decision scenarios,\nincorporating LLMs inference as a crucial element often\n*These authors contributed equally.\n‚Ä†Work done before joining Oracle.\nCopyright ¬© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nnecessitates stringent latency requirements. However, one\ndrawback of LLMs is their computational and storage cost,\nwhich ranks among the highest for known models. Con-\nsider GPT3-175B as an example. When stored in a com-\npact float16 format, its parameters require 326GB (in mul-\ntiples of 1024) of memory. This surpasses the capacity of\neven the most powerful individual GPUs, not to mention the\nchallenges of running it on hardware-limited edge devices\nwith acceptable latency. Quantization, in particular, offers a\npromising approach to substantially improve the inference\nthroughput and energy efficiency of LLMs on edge devices.\nThis improvement is achieved by harnessing the highly ef-\nfective 8-bit fixed-point (INT8) operations supported by the\nSIMD units that are commonly found in edge platforms,\nsuch as CPUs and Raspberry Pis.\nCurrent Limitations. Before fully realizing the on-\ndevice benefits of model quantization on LLMs, it‚Äôs crucial\nto address two pressing issues that demand careful attention.\n‚ù∂ Existing works (Frantar et al. 2022; Lin et al. 2023; Xiao\net al. 2022) primarily concentrate on weight-only (4-bit)\nquantization while leaving activations in the floating-point\n(FP16) domain. This approach limits the efficient speed-up\nof model inference on common edge devices, which typi-\ncally only support 16x16 and 8x8 integer multipliers. Specif-\nically, activation quantization often has a detrimental effect\non task performance, especially when the model size be-\ncomes large, due to the emergence of pronounced outliers\nin activations. Experiments done by work (Dettmers et al.\n2022) indicate that directly setting these outliers to zero can\nresult in a substantial 45% degradation in task performance.\nAdditionally, given the large model size of LLMs, limited\nacademic computing power makes it challenging to afford\nthe associated training costs. Consequently, Post-Training\nQuantization (PTQ) has become a prevalent approach, but\nit falls short of minimizing the quantization error caused\nby these outliers. In summary, quantizing the activations of\nLLMs while handling outliers inside activations is a crucial\nyet challenging issue. ‚ù∑ Mainstream edge processors, such\nas CPUs and Raspberry Pis, leverage SIMD units to execute\nmultiple operations in parallel efficiently. SIMD instructions\nare adept at exploiting byte-level data (8-bit integers) par-\nallelism and are well-supported in common ISAs (Instruc-\ntion Set Architectures) and DNN processing frameworks.\nExamples include GEMMLOWP (Jacob and Warden 2017)\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18944\nin TensorFlow Lite and QNNPACK (Dukhan, Wu, and Lu\n2018) in PyTorch. Their low-precision kernels merely zero-\nextend the sub-byte operands to align them with byte bound-\naries, treating them as 8-bit or 16-bit operands.\nIn this paper, we address the above on-device quantization\nissues while enjoying the powerful performance provided by\nLLMs. We propose Agile-Quant, an activation-guided quan-\ntization framework for faster inference of LLMs on the edge.\nSpecifically, we begin with a fundamental activation quanti-\nzation strategy based on hardware latency profiling and ac-\ntivation analysis of LLMs, aiming to strike a balance be-\ntween task performance and on-device inference speed. We\nsubsequently utilize the activation-aware pruning method to\noptimize quantization. This is crucial because quantized to-\nkens often exhibit numerous outliers, causing their attention\nto shift from the first position to nearby local positions. By\npruning tokens, we effectively eliminate some outliers, as\nthey typically concentrate within the same or adjacent chan-\nnels of different tokens. Also, the removal of inattentive to-\nkens can reduce the interaction distance between important\ntokens. Finally, we design the edge-oriented optimization for\nthe hardware implementation of Agile-Quant. It consists pri-\nmarily of two components: a SIMD-based 4-bit multiplier to\nfacilitate efficient 4x4 INT4 multiplication, and our efficient\nTwo-Refine Improved by Pruning (TRIP) matrix multiplica-\ntion designed to mitigate the adverse impact of outliers.\nThe popular LLMs models such as LLaMA (Touvron\net al. 2023), OPT (Zhang et al. 2022), and BLOOM (Scao\net al. 2022) are adopted to verify the effectiveness of our\nframework and the efficiency of our method on multi-\nple edge devices. Agile-Quant can maintain state-of-the-art\ntask performance comparable with weight-only works while\nachieving practical on-device speedup up to 2.55x.\nThe contributions of this work are summarized as follows:\n‚Ä¢ We design the activation-guided and edge-oriented quan-\ntization strategy for the balance of latency decreasing and\ntask performance.\n‚Ä¢ We design an activation-aware token pruning method\nto minimize the negative impact on task performance\ncaused by the outliers and the local attentivity.\n‚Ä¢ We propose the SIMD-based 4-bit multiplier and an ef-\nficient TRIP matrix multiplication for effective hardware\nimplementation.\n‚Ä¢ We achieve state-of-the-art task performance on several\npopular datasets with practical on-device speedup.\nBackground and Related Works\nIn this section, we first focus on the backgound of post-\ntraining quantization for LLMs. Then we discuss the low-bit\ncomputation on general edge devices.\nPost-Training Quantization for LLMs\nPost-Training Quantization (PTQ) techniques are widely\nused for one-shot compressing models, particularly for\nLarge Language Models (LLMs), given the high cost of re-\ntraining. These PTQ methods employ accurate solvers to ad-\ndress compression challenges on a per-layer or per-group\nbasis, relying on a limited set of calibration data. Notably,\nrecent advances in PTQ, like GPTQ (Frantar et al. 2022),\nAWQ (Lin et al. 2023), and SpQR (Dettmers et al. 2023),\nhave introduced well-crafted approaches capable of preserv-\ning LLM performance effectively. GPTQ leverages second-\norder information to correct errors, achieving commendable\naccuracy within a 3-4 bit range. AWQ proposes safeguarding\nonly 1% of crucial weights to substantially diminish quanti-\nzation errors. SpQR‚Äôs focus is on reducing quantization to 3-\n4 bits per parameter for smaller models. Moreover, they put\nforth a novel technique enabling nearly lossless compression\nof LLMs. Nonetheless, these works fall short of achieving\npractical inference acceleration on edge devices, as the acti-\nvation part persists in a floating-point format, rendering the\ninteger multiplier of the edge devices ineffective.\nLow-Bit Computation on Hardware Devices\nLow-precision linear algebra kernels aim to maximize\ncomputing throughput on low-precision operands. This is\nachieved by extending existing wider bit-width linear al-\ngebra kernels. The use of lower-precision operands brings\nabout two performance enhancements: increased cache ca-\npacity and the ability to leverage lower-precision SIMD in-\nstructions for processing multiple elements simultaneously.\nPioneering examples of these low-precision linear algebra\nkernels, e.g., Google‚Äôs GEMMLOWP (Jacob and Warden\n2017) and Facebook‚Äôs QNNPACK (Dukhan, Wu, and Lu\n2018), excel at enhancing the efficiency of DNN inference\nwhen employing 8-bit quantization. However, pushing for\nmore aggressive sub-byte quantization yields no added per-\nformance benefits due to the fact that mainstream CPUs\nsolely support SIMD operations with a precision of 8 bits\nor wider. In specific, low-precision kernels essentially ex-\npand sub-byte operands to 8 bits and process them accord-\ningly. Furthermore, the concept of Bit-serial computation\nemerges as a promising solution for data-parallel computa-\ntion with sub-byte values. This approach involves sequen-\ntially processing each bit of two operands during multiplica-\ntion, while simultaneously managing multiple operand pairs\nin parallel. Nonetheless, its practical implementation neces-\nsitates the popcount operation, which inherently limits run-\ntime throughput. As a result, this method only presents sig-\nnificant advantages in ultra-low-bit scenarios (1 or 2 bits).\nActivation Analysis of LLMs\nIn this section, we analyze the attentivity of tokens in LLMs\nand the influence of token pruning on activation quantiza-\ntion. Besides, we deliver the latency profiling to analyze po-\ntential quantization strategy.\nToken Importance in LLMs\nIn natural language processing, numerous non-essential\nwords often exist within sentences, contributing little to\nthe overall comprehension. This implies that we can effi-\nciently process these words using fewer resources, poten-\ntially even excluding them, in order to mitigate complexity.\nAs words are embedded into tokens in language models, we\nexplore the attention mechanism to analyze the importance\nof each token. The previous works (Kong et al. 2022; Dong\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18945\nFigure 1: The (a), (b), and (c) shows attention maps with 16\ntokens in the first and last layer of the model. The activation\nis not quantized in (a) and (b), while it is quantized in (c).\nThe (d) shows the distribution of outliers in one activation\nwith 2048 tokens. The visualization is based on the LLaMA-\n7B model with the Wikitext-2 dataset.\net al. 2023) focus on the attention map in the transformer\narchitectures. The attention probabilities are then accumu-\nlated across multiple rounds of attention as token importance\nscores. However, the causal attention masks used in LLMs\nensure that, during the self-attention mechanism, each token\ncan only interact with previous tokens instead of the follow-\ning ones. Thus, this causal mechanism makes the accumu-\nlated probabilities not appropriate to the evaluation of token\nimportance because of its unfair for the accumulated proba-\nbilities of former tokens.\nIn LLMs, a distinct start token is placed at the beginning\nof the input sequence. The start token has a role in initial-\nizing the hidden layers and defining token positions within\nthe sequence. These aspects are vital for producing text that\nis both coherent and contextually meaningful. To explore\nthe relationship between the first start token and other to-\nkens, we visualize the attention map at the first and last layer\nof the LLaMA-7B model with 16 tokens on the Wikitext-2\ndataset in Figure 1 (a) and (b). According to the attention\nmap, several tokens in the first layer demonstrate a shared\ntriangular pattern, indicating that tokens tend to the adjacent\npositions, especially the previous position. While in the last\nlayer, nearly all tokens share a vertical-stripe pattern, indi-\ncating that tokens all related with the first token. Then we\nexplore the attention maps in the middle layers, showing that\nthese maps are similar to the one in the last layer. Thus, it\nguides us to build the connection between the token impor-\ntance and token attentivity to the start token.\nLayerNorm\nLinear Trans\nùëÑ \" ùêæ!\nSoftmax\nùê¥ùë°ùë°ùëõ \" ùëâ\nLinear Pro\n FC1\n Activation \nFunction\n FC2\n Others\nFP16 306.9\nINT8\nINT4\n170.2\n105.5\nFigure 2: Mobile Device profiling of one LLaMA block.\nInfluence of Activation Quantization\nWe analyze the distribution of outliers and visualize outlier\nin different channels in Figure 1 (d). We notice that the out-\nliers are distributed in adjacent or even the same channels,\nsince several straight lines with deep colors indicates that\nthe channel index of the outliers unchange. Also, the atten-\ntion map, which is generated by the query Q and key matrix\nK, can be influenced by the activation quantization as it is\ninput-dependent. We visualize the quantized attention map\nat the last layer in Figure 1 (c). The attention map shows\na triangular pattern and the quantized tokens attend to the\nadjacent positions rather than the start token, demonstrating\nthat the attention range becomes locality and the attentivity\nturns much weaker. This change implies a deterioration in\nthe globality of representative features. From another per-\nspective, the information reduction of the original attention\nmap caused by quantization error will impact the final task\nperformance adversely.\nLatency Profiling on Hardware Devices\nTo gain a deeper insight into the runtime distribution of\nLLMs, we conducted profiling on a widely used model,\nLLaMA, utilizing the on-board Snapdragon 870 CPU, as\nshown in Figure 2. This profiling includes FP16, INT8, and\nINT4 precisions. Since nonlinear operators (LayerNorm/-\nSoftmax/SwiGLU) contribute a relatively smaller portion of\nlatency, i.e., < 8% for FP16, < 12% for INT8, < 16% for\nINT4, we have implemented them using FP16 arithmetic\nunits to ensure task performance is maintained. We focus\non the primary computation workload, matrix multiplica-\ntion, performed in various low-bit precision settings. Our\nobservation reveals that FC1 and FC2 account for 54% of\nthe runtime latency in FP16, and 49.5% in INT8. This find-\ning indicates the need to prioritize the quantization of these\ncomponents to a lower-bit (4-bit) representation. Following\nthat, our order of priority will be as follows: Linear Trans-\nformation > Linear Projection > AttnV > QK. In essence,\nfocusing on low-bit quantization of both weights and activa-\ntions for LLMs while ensuring task performance is crucial.\nMethodology\nWe explain the activation quantization pipeline here and pro-\npose the activation-guided framework for the optimization\nof quantization. Also, we explain our hardware implementa-\ntion of the 4-bit multiplier.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18946\nEmbedded Tokens\nLayerNorm\nSelf-Attention\nLayerNormResidual\nMLP\nResidual\nLM Head\nNext following Blocks\nSoftmax\nFully Connected (8x4 or 4x4)\nActivation Function\nFully Connected (8x4 or 4x4)\nùë®ùíïùíïùíèùë∑ùíìùíê√óùëΩ(4x8)\nLinear Transformation (8X4)\nùë∏\n ùë≤\n ùëΩ\nùë∏√óùë≤ùëª(8x8)\nLinear Projection (8x4 or 4x4)\nFigure 3: Activation Quantization Pipeline.\nPreliminary\nWe here explain the quantizers we use for activation quanti-\nzation in this work. We assume the bit-width used in quanti-\nzation is b, and then the quantizer can be defined as a func-\ntion Q(X|b) which can map the floating points in vector\nX ‚àà Rmxn to the closest quantization in q:\nq =\n\u001a\n{‚àí2b‚àí1, ...,2b‚àí1 ‚àí 1}, Signed\n{0, 1, ...,2b ‚àí 1}, Unsigned (1)\nThere are various kinds of quantizers Q(X|b), and the\nuniform quantizer (Jacob et al. 2018) and the log2 quan-\ntizer (Cai, Takemoto, and Nakajo 2018) are widely used. In\nour work, we mainly use these two quantizers for activation\nquantization.\nUnifrom Quantization has been supported by most hard-\nware devices.\nQ(X|b) =CLIP(‚åäX\ns ‚åâ + zp, 0, 2b ‚àí 1) (2)\nThe s and zp denote the scale and zero-point separately.\nLog2 Quantization imports the exponential operation into\nthe linear quantization process.\nQ(X|b) =Sign(X) ¬∑CLIP(‚åä‚àílog2\nX\nmax(|X|)‚åâ, 0, 2b‚àí1 ‚àí1) (3)\nActivation Quantization Pipeline\nWe present our activation quantization pipeline in Fig-\nure 3. While the embedding process, output module, and\nthe yellow-highlighted nonlinear operations contribute rel-\natively small proportions during model inference, we pre-\nserve their computation without alteration. Our primary fo-\ncus is optimizing the matrix multiplication operations, con-\nstituting the largest share of the inference latency.\nWithin our pipeline, we target the acceleration of com-\nputations occurring in the self-attention and MLP modules,\nas indicated by the blue shading. Specifically, we perform\nactivation quantization predominantly using 8-bit integers.\nFigure 4: Activation Quantization With Token Pruning.\nHowever, we observe that specific activations following the\nself-attention mechanism can be quantized using 4-bit inte-\ngers, resulting in further acceleration while upholding task\nperformance standards. Accordingly, we propose our inno-\nvative 4-bit multiplier to effectively support the INT4 matrix\nmultiplication.\nActivation-Guided Optimization\nBased on the analysis of outliers and attention range in the\nprevious section, it is intuitive for us to import the token\npruning here for optimization. We visualize the token prun-\ning process in Figure 4. Token pruning can reduce the out-\nliers, which can decrease the quantization error caused by\nthem. Token pruning can also reduce the distance between\nattentive tokens to help the model capture more features.\nWe first introduce the activation-aware token pruning im-\nproved activation quantization method we use in this work.\nInspired by the work (Lin et al. 2022), we propose the Two-\nRefine Improved by Pruning (TRIP) method here to address\nthe difficulties in activation quantization.\nFor the one activation in transformer-based models, we\nassume it as X ‚àà Rm√ód, and the m, ddenote the number\nof tokens, and dimension separately. We assume the token\npruning function, which prunes tokens in cascade according\nto the token importance, as FP (¬∑).\nXP = FP (X) ‚àà Rn√ód, n < m (4)\nThen, the TRIP factor Œ± = {Œ±1, Œ±2, ..., Œ±d} are applied to\nthe different channels whose number of outliers is reduced\nby token pruning. The factor Œ± can provide different chan-\nnels with different factors, which can regularize the quanti-\nzation parameters and is utilized as:\nXP\nQ = Q(XP |b, Œ±) =CLIP(‚åäXP\n2Œ±s‚åâ + zp, 0, 2b ‚àí 1) (5)\ns = max(XP ) ‚àí min(XP )\n(2b ‚àí 1) ¬∑ 2Œ± (6)\nzp = CLIP(‚åä‚àímin(XP )\n2Œ± ¬∑ s ‚åâ, 0, 2b ‚àí 1) (7)\nFor channel c ‚àà [1, d] with the biggest refinement K, the\nfactor Œ±c is as follow:\nŒ±c = arg min\nŒ±c‚àà{0,1,...,K}\n\r\r\r\rXP\nc ‚àí ‚åäXP\nc\n2Œ±c ¬∑ s‚åâ ¬∑2Œ±c ¬∑ s\n\r\r\r\r\n2\n(8)\nThe K is dependent on the channels containing relatively\nmore outliers. Outliers can be reduced by token pruning.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18947\n0\n X12\n0\n X12\n0\n W11\n0\nW12\n0\nW21\n0\nW22\nW11\nW12\nW21\nW22\nX11\nX12\nO11\nO12\nO21\nO22\n50% Multiplications ‚Üì50% Additions ‚Üì\n(1) Dot Multiplication: Concatenate two weights in adjacentrows into one INT16 value and multiply with common value.\nO11\n0\n O12\n0\nO21\n0\n O22\n0\n (2) Addition: Combined with BitShiftoperator,expand the INT16 output in (1) into INT32.\nQuantize O!\"#$%to O!\"#&\nO&\nO&\nFigure 5: The paradigm of INT4 multiplier.\nFor token pruning, we evaluate the token importance\nbased on the attention map, and we only adopt their atten-\ntivity to the first start token for the importance measuring\nbased on our analysis. We prune tokens in cascade at the dif-\nferent depths of the model progressively and dynamically.\nInspired by the previous token pruning works (Kim et al.\n2022; Liang et al. 2022), we design our pruning strategy by\nsplitting the model into several stages which have different\nredundancy (Shen et al. 2023). Meanwhile, pruning tokens\nfrom some shallow layer or even from the first layer would\ninfluence the task performance of the model. It is because,\nat the first few layers, the attentivity of tokens is still locality\nand the larger number of tokens can help the model capture\nmore beneficial features. Therefore, we mainly adopt the\nstrategy that progressively pruning the tokens starting from\nthe layer whose attention map shows that the tokens have\nenough ability to capture features. Besides, we regulate the\npruning ratio in each layer to balance the trade-off between\nloss of information and the improvement to the quantization.\nEdge-Oriented Optimization\nThis section mainly proposes the hardware implementation\nof our Agile-Quant framework on edge devices. We first de-\nsign the SIMD-based 4-bit multiplier to support the INT4\nmultiplication and then introduce the efficient support of 2-\nrefined matrix multiplication.\nINT4 Multiplier.We designed a specialized 4-bit multiplier\nbased on SIMD architecture (Figure 5), aimed at support-\ning practical INT4 computation. Here‚Äôs the workflow: Dot-\nmultiplication: In the SIMD kernel, we combine two adja-\ncent weight values, Wi,j and Wi+1,j, and multiply them\nwith their shared activation value. The result is an INT16\ndata type. We allocate the first 8 bits for the multiplica-\ntion with Wi,j and the remaining 8 bits for Wi+1,j. This\napproach follows the SIMD memory mechanism. Addition:\nBy utilizing the Bitshift operator, we expand the 16-bit out-\nput from Step 1 to 32 bits. The first 8 bits are set to 0, fol-\nlowed by Outputi,j in the next 8 bits, 0s in the third 8 bits,\nand OutputI+1,j in the final 8 bits. We then perform a row-\nby-row summation. This process can handle up to 28 ad-\nditions without overflow, sufficient for multi-head attention\n(head-dimension = 32/64). Each addition has a 32-bit mem-\nory footprint. Finally, we split the output into two INT16\nvalues and quantize them back to INT4 at the value-level,\nallowing us to integrate them into the GeMM kernel.\nEfficient TRIP Matrix Multiplication. Unlike channel-\nwise quantization, we perform layer-wise quantization on\nthe activation matrix of outliers. All the channels share the\n2\n1\n8\n1\nA11\nA12\nA13\nA14\nA21\nA22\nA23\nA24\nA31\nA32\nA33\nA34\nW11\nW12\nW13\nW14\nW21\nW22\nW23\nW24\nW31\nW32\nW33\nW34\nW15\nW16\nW25\nW26\nW35\nW36\nW41\nW42\nW43\nW44\nW45\nW46\nTRIP factors for activation channels\nPreload the modified weight on device\nX2\nX8\nFigure 6: Hardware Implementation of Efficient TRIP Ma-\ntrix Multiplication.\nsame quantization parameters, i.e., scaling factors and zero-\npoint. However, the predicted TRIP factors will adapt to the\noutlier channels‚Äô scaling factors. In the practical implemen-\ntation, those TRIP factors will be equivalently mathemati-\ncally transformed over the corresponding weights, as shown\nin Figure 6.\nNote that common inference engines only support layer-\nwise quantization on activation and per-channel quantiza-\ntion on weights, such as the GeMM and Convolution opera-\ntor configuration. For example, ArmComputeLibrary (ARM\n2023) only supports channel-wise quantization configura-\ntion for weight matrix instead of input activation.\nExperiments and Results\nWe introduce the experiments and the results in this section\nto verify the effectiveness and efficiency of our method.\nExperiment Setup\nSetup for Activation-guided Quantization.We implement\nthe activation quantization based on the weight-only quan-\ntization work GPTQ (Frantar et al. 2022) which achieves\nstate-of-the-art performance with 4-bit weight-only quanti-\nzation for LLMs. We mainly use 4-bit and 8-bit integers\nin our activation quantization. We use the Log2 quantiza-\ntion for softmax activation quantization and use our TRIP\nquantization for other activations. We implement the differ-\nent scales of LLaMA, OPT, and BLOOM models in our ex-\nperiments on the Wikitext-2 dataset (Merity et al. 2016) and\nC4 (Raffel et al. 2020) dataset.\nHardware Platform. We test the actual inference imple-\nmentation on various edge devices, including the RealmeGT\nAndroid Phone with Snapdragon 870 SoC and Raspberry4\nB with Quad-core CPU and 8GB RAM. Our inference en-\ngine for Arm processors is modified based on ArmCom-\nputeLibrary v22.05. The inference latency is reported via the\naverage of 50 iterations for each test.\nRegulation of Token Pruning\nWe regulate the token pruning ratio to optimize the task per-\nformance of LLMs. We apply the prune ratio progressively\nstarting from the shallow layers so that the token pruning\ncan optimize the activation quantization for more deep lay-\ners. Assume the model has n layers with L = (l1, l2, ..., ln)\nand the pruning operation is added to the layers Lp =\n(lp1, lp2, ..., lpm). We set prune ratio Œ≤ at the last layer ln\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18948\n0.0 0.2 0.4 0.6 0.8\nToken Sparsity\n4.5\n5\n5.5\n6\n6.5\n7\n8\n9\n10\nPerplexity\nLLaMA-7B\nLLaMA-13B\nLLaMA-30B\nBest\nFigure 7: Token Sparsity vs. Perplexity. The visualization\nis based on LLaMA with 7B, 13B, and 30B scales on the\nWikitext-2 dataset.\nand compute the progressive ratio Œ≥ for the layers li ‚àà Lp\nas:\nŒ≥ = 1‚àí (1 ‚àí Œ≤)\n1\nm (9)\nWe accumulate the token sparsity s with the number of\npruned tokens during inference as:\ns = 1‚àí\nnX\ni=1\nri (10)\nThe ri denotes the number of remaining tokens in li.\nWe then adopt the weight and activation both quantized\nLLaMA models with 7B, 13B, and 30B to search for the op-\ntimal prune ratio. We visualize our results in Figure 7. For\nall three different scales of LLaMA models, we use token\npruning to achieve better performance than dense models at\nthe red star points. Also, we find that token pruning can only\nhelp the quantization achieve better results when the token\nsparsity is small, while token pruning makes a negative im-\npact on the task performance when the token sparsity be-\ncomes too large. Here we adopt the optimal token sparsity\nfor different scales of the LLaMA model and show the exact\nresults in Table 1. Also, we regulate the token pruning with\nthe same strategy as LLaMA for OPT and BLOOM models,\nand the best results are shown in Table 2 and Table 3.\nQuantization Results and Analysis\nWe first show the quantization results of LLaMA in Table 1.\nAccording to the results of weight-only quantization works,\nour method achieves a minor task performance drop, and\nwe achieve better performance than most of the other ac-\ntivation quantization works. The data in the last row de-\nnotes the results achieved by token pruning in our method,\nwhich verifies that the token pruning can optimize the quan-\ntization. Our method achieves better task performance than\nMoFQ8 (Zhang et al. 2023) and ZeroQuant-FP (Wu, Yao,\nand He 2023). the Low-Rank Compensation (LoRC) en-\nhancement denoted as ¬ß would increase the model size and\nflops, which is also not well-supported on the common infer-\nence engine. Then, we show our quantization result of OPT\nand BLOOM on the Wikitext-2 dataset and C4 dataset in\nTable 2 and Table 3. Our method achieve better task perfor-\nmance than those activation quantization works and our re-\nsults are close to the weight-only quantization works. Mean-\nMethod WQ AQ PPL of LLaMA\n# Bits # Bits 7B 13B 30B 65B\n- FP16 FP16 5.68 5.09 4.10 3.53\nRTN INT4 FP16 6.29 5.53 4.54 3.92\nGPTQ INT4 FP16 5.85 5.2 4.23 3.65\nSqLLM 4.05‚Ä° FP16 5.79 5.18 4.22 -\nSpQR 3.94‚Ä° FP16 5.87 5.22 4.25 3.90\nMoFQ8 FP8‚Ä† FP8‚Ä† 6.49 5.41 5.31 -\nZQFP INT8 INT8 5.72 5.09 4.10 -\nZQFP INT4 INT8 6.44 5.32 4.36 -\nZQFP¬ß INT4 INT8 5.88 5.28 4.34 -\nAgileQ AgileQ-8 6.16 5.57 4.55 4.01\nAgileQ‚àó AgileQ-8 6.09 5.21 4.44 3.92\nTable 1: LLaMA Quantization Results on Wikitext-2\ndataset. AgileQ-8 denotes the 8-bit is used. SqLLM denotes\nSqueezeLLM. ‚àó denotes the token pruning optimized re-\nsults. ‚Ä† denotes the mix precision with mainly FP8 and INT8.\n‚Ä° denotes the average bits. ZQFP denotes ZeroQuant-FP. ¬ß\ndenotes the LoRC.\nwhile, token pruning also helps us achieve better task perfor-\nmance in activation quantization. Especially, for OPT and\nBLOOM models, our method can even achieve even better\ntask performance than the FP16 models on the C4 dataset.\nAblation Study\nThe length of the input sequence makes a big influence on\nthe evaluation process, we try the token pruning with differ-\nent sequence lengths to further explore the variation of per-\nformance. The normal default length of the input sequence\nused in the evaluation of LLMs is 2048, which is widely\nused in LLMs-related works. Thus, we regulate the input\nsequence length of LLaMA-7B with Wikitext-2 dataset to\nexplore the relationship between it and token sparsity. The\nresults are in Figure 8. We can find that task performance be-\ncomes worse as the sequence length becomes shorter and the\ntoken sparsity becomes larger, and the token pruning only\nworks when the sequence length is long enough (i.e., 2048).\n0.0 0.2 0.4 0.6 0.8\nToken Sparsity\n6\n6.1\n7\n8\n12\n15\n20\n30\n40\n50\nPerplexity\n256 Seq Len\n512 Seq Len\n1024 Seq Len\n2048 Seq Len\nBest\nFigure 8: Token Sparsity vs. Input Sequence Length. The vi-\nsualization is based on LLaMA-7B with Wikitext-2 dataset.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18949\nMethod W / A PPL of OPT on\nWikitext-2 PPL of OPT on\nC4\n# Bits 125M 1.3B 2.7B 6.7B 13B 30B 125M 1.3B 2.7B 6.7B 13B 30B\n- 16 27.65 14.63 12.47 10.86 10.13 9.56 26.56 16.07 14.34 12.71 12.06 11.44\nRTN 4/16 37.28 48.17 16.92 12.10 11.32 10.98 33.91 24.51 18.43 14.36 13.36 13.46\nGPTQ 4/16 31.12 15.47 12.87 11.39 10.31 9.63 29.22 16.97 15.00 13.18 12.26 11.57\nAWQ 4/16 33.96 16.85 14.61 12.44 11.60 10.75 - - - - - -\nMoFQ8 8‚Ä† / 8‚Ä† - 16.78 14.24 12.41 12.52 10.95 - - - - - -\nZQV2 4/16 36.71 19.38 17.92 11.91 10.67 10.10 30.92 17.93 18.32 13.01 12.07 11.33\nAgileQ our-8 31.52 15.90 13.43 11.43 10.42 9.70 28.43 16.72 14.91 12.70 11.77 11.14\nAgileQ‚àó our-8 30.37 14.90 13.19 11.21 10.00 9.45 24.44 15.95 14.20 12.39 11.31 11.20\nTable 2: Perplexity of OPT model on Wikitext-2 dataset and C4 dataset. our-8 denotes the 8-bit is used. The bold part denotes\ninteger, otherwise float. ‚àó denotes token pruning optimized results. ‚Ä† denotes mix precision with mainly FP8 and INT8.\nMethod W/A PPL of BLOOM on\nWikitext-2 PPL of BLOOM on\nC4\n# Bits 560M 1.1B 1.7B 3B 7.1B 560M 1.1B 1.7B 3B 7.1B\n- 16/16 22.42 17.69 15.39 13.48 11.37 26.60 22.05 19.49 17.49 15.2\nRTN 4 / 16 25.9 22.00 16.97 14.76 12.10 29.89 24.44 21.26 18.76 16.06\nGPTQ 4 / 16 24.03 19.05 16.48 14.20 11.73 28.00 23.25 20.55 18.10 15.60\nZQV2 4 / 16 25.31 23.90 16.93 14.65 12.06 27.10 25.99 19.47 17.26 14.83\nAgileQ our-8 24.01 18.82 16.23 14.05 11.73 26.39 21.80 19.18 16.96 14.70\nAgileQ‚àó our-8 23.72 18.33 16.15 13.73 11.36 25.21 19.92 18.56 16.24 14.03\nTable 3: Perplexity of BLOOM model on the Wikitext-2 dataset and C4 dataset. our-8 denotes the 8-bit is used. The bold part\ndenotes integer, otherwise float. ‚àó denotes the token pruning optimized results. ZQV2 denotes the ZeroQuant-V2\nEnd-to-end Performance and Analysis\nBased on Table 4, we can find that Agile-Quant can bring an\noverall acceleration ratio of 2.3x to 2.6x depending on the\nmodel. Especially, Agile-Quant-8 quantizes the activation\ninto INT8 precision and can achieve approximately 1.8x to\n1.9x acceleration compared to FP16 in GeMM. Also, com-\nbined with the 4-bit compression and concatenation tech-\nnique, Agile-Quant-4 can further improve this advantage,\nachieving approximately 1.75x acceleration compared to\nINT8 multiplication.\nConclusions and Limitations\nIn this paper, we propose Agile-Quant, an activation-guided\nquantization framework for popular LLMs, and design an\nend-to-end accelerator on multiple edge devices. We intro-\nduce the quantization strategy on model weights and acti-\nvations, and we import token pruning to optimize quantiza-\ntion. We introduce SIMD-based 4-bit multiplier and efficient\nTRIP matrix multiplication to achieve the 2.55x speedup\non hardware devices. Our next step is to explore lower-bit\nLLMs and design multiple lower-bit multipliers.\nAcknowledgements\nThis research is mainly funded by the Army Research Of-\nfice/Army Research Laboratory via grant W911-NF-20-1-\n0167 to Northeastern University and is partially supported\nby the National Science Foundation CCF-1937500 and\nCNS-1909172.\n# Bits Size\n(GB) PPL Android Raspberry\nCPU (s) Pi (s)\nOPT-125M\nFP16 0.24 27.65 1.03 1√ó 143.2 1√ó\nours‚àó-8 0.04 30.37 0.58 1.7√ó 80.34 1.7√ó\nours‚àó-4 0.04 36.95 0.44 2.3√ó 61.16 2.3√ó\nOPT-1.3B\nFP16 2.50 14.63 5.42 1√ó 754.25 1√ó\nours‚àó-8 0.49 14.90 2.98 1.8√ó 410.58 1.8√ó\nours‚àó-4 0.49 18.20 2.18 2.5√ó 296.50 2.5√ó\nOPT-2.7B\nFP16 5.00 12.47 8.28 1√ó 1150.9 1√ó\nours‚àó-8 0.94 13.19 4.60 1.8√ó 625.41 1.8√ó\nours‚àó-4 0.94 16.32 3.31 2.5√ó 455.64 2.5√ó\nLLaMA-7B\nFP16 13.5 5.68 10.6 1√ó 1473.4 1√ó\nours‚àó-8 2.53 6.09 5.89 1.8√ó 810.25 1.8√ó\nours‚àó-4 2.53 8.81 4.44 2.4√ó 610.98 2.3√ó\nTable 4: Hardware results under different data precision for\nvarious LLMs. Results are obtained by Agile-Quant with 4\nor 8 bits and token pruning.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18950\nReferences\nARM. 2023. A collection of low-level machine learning\nfunctions optimized with SIMD technologies. https://arm-\nsoftware.github.io/ComputeLibrary/v22.05/.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020a. Language models are few-shot learners.\nNeurIPS, 33: 1877‚Äì1901.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020b. Language Models\nare Few-Shot Learners.\nCai, J.; Takemoto, M.; and Nakajo, H. 2018. A Deep Look\ninto Logarithmic Quantization of Model Parameters in Neu-\nral Networks. In IAIT.\nDettmers, T.; Lewis, M.; Belkada, Y .; and Zettlemoyer, L.\n2022. Llm. int8 (): 8-bit matrix multiplication for transform-\ners at scale. arXiv preprint arXiv:2208.07339.\nDettmers, T.; Svirschevski, R.; Egiazarian, V .; Kuznedelev,\nD.; Frantar, E.; Ashkboos, S.; Borzunov, A.; Hoefler, T.; and\nAlistarh, D. 2023. SpQR: A Sparse-Quantized Representa-\ntion for Near-Lossless LLM Weight Compression. arXiv.\nDong, P.; Sun, M.; Lu, A.; Xie, Y .; Liu, K.; Kong, Z.; Meng,\nX.; Li, Z.; Lin, X.; Fang, Z.; et al. 2023. Heatvit: Hardware-\nefficient adaptive token pruning for vision transformers. In\nHPCA, 442‚Äì455. IEEE.\nDukhan, M.; Wu, Y .; and Lu, H. 2018. QNNPACK: Open\nsource library for optimized mobile deep learning.\nFrantar, E.; Ashkboos, S.; Hoefler, T.; and Alistarh, D. 2022.\nGPTQ: Accurate Post-training Compression for Generative\nPretrained Transformers. arXiv.\nJacob, B.; Kligys, S.; Chen, B.; Zhu, M.; Tang, M.; Howard,\nA.; Adam, H.; and Kalenichenko, D. 2018. Quantization and\ntraining of neural networks for efficient integer-arithmetic-\nonly inference. In CVPR, 2704‚Äì2713.\nJacob, B.; and Warden, P. 2017. gemmlowp: A small self-\ncontained low-precision gemm library. Retrieved June, 14:\n2018.\nKim, S.; Shen, S.; Thorsley, D.; Gholami, A.; Kwon, W.;\nHassoun, J.; and Keutzer, K. 2022. Learned token pruning\nfor transformers. In KDD, 784‚Äì794.\nKong, Z.; Ma, H.; Yuan, G.; Sun, M.; Xie, Y .; Dong, P.;\nMeng, X.; Shen, X.; Tang, H.; Qin, M.; et al. 2022. Peel-\ning the Onion: Hierarchical Reduction of Data Redundancy\nfor Efficient Vision Transformer Training. arXiv.\nLiang, Y .; Ge, C.; Tong, Z.; Song, Y .; Wang, J.; and Xie, P.\n2022. Not all patches are what you need: Expediting vision\ntransformers via token reorganizations. arXiv.\nLin, J.; Tang, J.; Tang, H.; Yang, S.; Dang, X.; and Han,\nS. 2023. AWQ: Activation-aware Weight Quantization for\nLLM Compression and Acceleration. arXiv.\nLin, Y .; Zhang, T.; Sun, P.; Li, Z.; and Zhou, S. 2022. FQ-\nViT: Post-Training Quantization for Fully Quantized Vision\nTransformer. In IJCAI, 1173‚Äì1179.\nMerity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2016.\nPointer sentinel mixture models. arXiv.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.;\nSutskever, I.; et al. 2019. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8): 9.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Exploring\nthe Limits of Transfer Learning with a Unified Text-to-Text\nTransformer. Journal of Machine Learning Research.\nScao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ili¬¥c, S.; Hesslow,\nD.; Castagn¬¥e, R.; Luccioni, A. S.; Yvon, F.; Gall¬¥e, M.; et al.\n2022. Bloom: A 176b-parameter open-access multilingual\nlanguage model. arXiv.\nShen, X.; Kong, Z.; Qin, M.; Dong, P.; Yuan, G.; Meng, X.;\nTang, H.; Ma, X.; and Wang, Y . 2023. Data Level Lottery\nTicket Hypothesis for Vision Transformers. In IJCAI.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample,\nG. 2023. LLaMA: Open and Efficient Foundation Language\nModels. arXiv.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-\ntention is all you need. NeurIPS, 30.\nWu, X.; Yao, Z.; and He, Y . 2023. ZeroQuant-FP: A Leap\nForward in LLMs Post-Training W4A8 Quantization Using\nFloating-Point Formats. arXiv preprint arXiv:2307.09782.\nXiao, G.; Lin, J.; Seznec, M.; Wu, H.; Demouth, J.; and\nHan, S. 2022. SmoothQuant: Accurate and Efficient Post-\nTraining Quantization for Large Language Models. arXiv.\nZhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;\nChen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V .; et al. 2022.\nOpt: Open pre-trained transformer language models. arXiv.\nZhang, Y .; Zhao, L.; Cao, S.; Wang, W.; Cao, T.; Yang, F.;\nYang, M.; Zhang, S.; and Xu, N. 2023. Integer or Float-\ning Point? New Outlooks for Low-Bit Quantization on Large\nLanguage Models. arXiv.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18951",
  "topic": "Inference",
  "concepts": [
    {
      "name": "Inference",
      "score": 0.6595233678817749
    },
    {
      "name": "Quantization (signal processing)",
      "score": 0.6563810706138611
    },
    {
      "name": "Enhanced Data Rates for GSM Evolution",
      "score": 0.5188209414482117
    },
    {
      "name": "Agile software development",
      "score": 0.468517541885376
    },
    {
      "name": "Computer science",
      "score": 0.4040662348270416
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2858315706253052
    },
    {
      "name": "Algorithm",
      "score": 0.1544979214668274
    },
    {
      "name": "Software engineering",
      "score": 0.08240431547164917
    }
  ]
}