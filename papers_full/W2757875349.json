{
  "title": "Sub-character Neural Language Modelling in Japanese",
  "url": "https://openalex.org/W2757875349",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A2108620717",
      "name": "Viet Nguyen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2123940829",
      "name": "Julian Brooke",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096893938",
      "name": "Timothy Baldwin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2035305552",
    "https://openalex.org/W1899794420",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2042720679",
    "https://openalex.org/W2051572310",
    "https://openalex.org/W2198380824",
    "https://openalex.org/W1880208839",
    "https://openalex.org/W2566150155",
    "https://openalex.org/W1912843035",
    "https://openalex.org/W1594229598",
    "https://openalex.org/W2737432369",
    "https://openalex.org/W2251131401",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W3142894668",
    "https://openalex.org/W2251784867",
    "https://openalex.org/W2098132066",
    "https://openalex.org/W1978868733"
  ],
  "abstract": "In East Asian languages such as Japanese and Chinese, the semantics of a character are (somewhat) reflected in its sub-character elements. This paper examines the effect of using sub-characters for language modeling in Japanese. This is achieved by decomposing characters according to a range of character decomposition datasets, and training a neural language model over variously decomposed character representations. Our results indicate that language modelling can be improved through the inclusion of sub-characters, though this result depends on a good choice of decomposition dataset and the appropriate granularity of decomposition.",
  "full_text": "Proceedings of the First Workshop on Subword and Character Level Models in NLP, pages 148–153,\nCopenhagen, Denmark, September 7, 2017.c⃝2017 Association for Computational Linguistics.\nSub-character Neural Language Modelling in Japanese\nViet Nguyen Julian Brooke Timothy Baldwin\nSchool of Computing and Information Systems\nThe University of Melbourne\nvn@student.unimelb.edu.au, jabrooke@unimelb.edu.au, tb@ldwin.net\nAbstract\nIn East Asian languages such as\nJapanese and Chinese, the semantics\nof a character are (somewhat) reflected\nin its sub-character elements. This pa-\nper examines the effect of using sub-\ncharacters for language modeling in\nJapanese. This is achieved by decom-\nposing characters according to a range\nof character decomposition datasets,\nand training a neural language model\nover variously decomposed character\nrepresentations. Our results indicate\nthat language modelling can be im-\nproved through the inclusion of sub-\ncharacters, though this result depends\non a good choice of decomposition\ndataset and the appropriate granular-\nity of decomposition.\n1 Introduction\nThe Japanese language makes use of\nChinese-derived ideographs (“kanji”) which\ncontain sub-character elements (“bushu”) that\nto varying degrees reflect the semantics of the\ncharacter. F or example, the character 鯨 (ku-\njira “whale”) consists of two sub-characters:\n⿂(sakana “fish”) and 京(kyou “capital city”).\nSimilarly , the character 鮃 (hirame “floun-\nder”) consists of the sub-characters ⿂ (sakana\n“fish”) and 平 (hira “something broad and\nflat”). Here, the sub-character ⿂ (sakana\n“fish”) is a semantically significant element\nwhich appears in characters relating to marine\nlife. Current Japanese language models do not\ncapture sub-character information, and hence\nlack the ability to capture such generalisations.\nA key limitation of word-based language\nmodelling is the tendency to produce poor esti-\nmations for rare or OOV (out-of-vocabulary)\nwords, and character-based language models\nhave been shown to solve some of the sparsity\nproblem in English by modeling how words\nare constructed ( Graves, 2013). W e take\ninspiration from this work, but observe for\nJapanese that since the kanji portion of the\nJapanese writing system contains thousands\nrather than dozens of characters, a character-\nbased language model will still be susceptible\nto sparsity . Given that a large number of\nJapanese characters can be decomposed into\nsub-characters, we examine the question of\nwhether sub-character language models can\nachieve similar gains in language model qual-\nity to character language models in English.\nIn this paper we train sub-character lan-\nguage models for Japanese based on decom-\npositions available in several existing kanji\ndatasets. Our results suggest that decompos-\ning characters is of value, but that the results\nare sensitive to the nature and granularity of\nthe decomposition.\n2 Kanji Datasets\nIn order to investigate the usefulness of sub-\ncharacter decomposition for language models,\nwe need some way of deriving these bushu from\nkanji. Here, we consider four kanji datasets\nthat provide decompositions for kanji char-\nacters: Gl yphWiki, IDS, KanjiVG, and\nKRADFILE. An example kanji decomposi-\ntion under the four datasets is provided in Fig-\nure 1.\n2.1 Glyph Wiki\nGl yphWiki1 is a community-driven wiki\nthat stores information about kanji characters\n1 http://glyphwiki.org\n148\n賂\n⾙\n⽬\n各\n⼝\n(a) Gl yphWiki\n賂\n⾙\n⽬ ⼋\n各\n⼡ ⼝\n(b) IDS\n賂\n⾙\n⽬\n各\n⼡ ⼝\n(c) KanjiVG\n賂\n⾙ ⽬ ハ ⼝ ⼡\n(d) KRADFILE\nFigure 1: A visualisation of a full decomposition of the character 賂 (mainai “bribe”) according\nto the four kanji datasets.\nsuch as their decompositions into bushu, and\ntheir usage as bushu in other kanji. Decompo-\nsition information in GlyphWiki is collected\nby user contribution.\n2.2 IDS\nIDS2 is based on an open source project\nwhich uses character processing methods to\ncreate descriptions of character components\n(Morioka and Wittern, 2002). The decomposi-\ntions in IDS are generated automatically , us-\ning character topic maps to draw associations\nbetween kanji and their constituent elements.\nIn Figure 1b we see that IDS provides a\nmore detailed breakdown than GlyphWiki,\nincluding two bushu that are not found in\nGlyphWiki. At the first level of decompo-\nsition, though, they are identical.\n2.3 KanjiVG\nKanjiVG3 is a collection of images that\nprovides information about kanji and their\ndecompositions. Decomposition information\nfor KanjiVG is derived from the analysis of\nstrokes used to write kanji characters ( Apel\nand Quint , 2004). Although KanjiVG does\nallow for the decomposition of characters down\n2 https://github.com/cjkvi/cjkvi-ids\n3 http://kanjivg.tagaini.net/\nto the stroke level, we exclude all strokes in\nthis research as we do not consider strokes to\nreflect semantic meaning.\nNote in Figure 1c that the decomposition\nis different to IDS, because KanjiVG spec-\nifies that the bottom elements of the bushu\ncharacter ⾙ (kai “shell”) are strokes rather\nthan bushu, meaning they are excluded from\ndecomposition.\n2.4 KRADFILE\nKRADFILE4 represents a flat decompo-\nsition of kanji into their constituent bushu.\nOne key aspect of KRADFILE that differs\nfrom the other datasets is the use of a rela-\ntively limited set of bushu. Additionally , un-\nlike the other datasets, KRADFILE does not\nlist bushu in an order consistent with their ap-\npearance in the kanji. F urthermore, KRAD-\nFILE provides a single exhaustive decompo-\nsition for all kanji characters and their bushu.\nBecause of this, we consider KRADFILE to\nhave only a single, indivisible layer of decom-\nposition.\n2.5 Dataset Comparison\nT able1 shows descriptive statistics for the\nfour datasets and their decompositions.\n4 http://users.monash.edu/~jwb/kradinf.html\n149\nGl yphWiki IDS KanjiVG KRADFILE\nCharacters 18761 20970 6744 12156\nUnique bushu 2834 3104 1327 254\nA verage bushu per kanji 1.9 2.1 2.2 4.5\nA verage branching factor 1.5 2.1 1.9 4.5\nA verage depth 2.7 3.1 2.9 1.0\nT able 1: Statistics for the four kanji datasets\nThe Characters row of the table describes\nthe total number of characters in each dataset\nthat have a decomposition. KanjiVG con-\ntains a much smaller number of kanji than the\nother two datasets, although note that in mod-\nern Japanese, kanji is generally restricted to\n2,136 jouyou kanji (Bond and Baldwin, 2016).\nAll four datasets have full coverage over our\ncorpus.\nUnique bushu describes the number of char-\nacters that have been used as bushu. In gen-\neral, if a kanji character is found in the de-\ncomposition of another kanji character, then\nit is counted as a bushu. While most of the\ndatasets use thousands of bushu,KRADFILE\nis notable in that it uses a much smaller bushu\nset. With respect to Average bushu per kanji ,\nthere is strong similarity between Glyph-\nWiki, IDS and KanjiVG, but KRADFILE\nproduces almost double the number of bushu\nbecause the decompositions are exhaustive.\nAverage branching factor describes the av-\nerage number of bushu found through exhaus-\ntively decomposing over every kanji and its\nbushu (an example of exhaustive decomposi-\ntion — which we call “deep decomposition”\n— can be seen in Figure 2). Because KRAD-\nFILE provides a single layer of decomposition\nthat is complete and indivisible, we cannot de-\ncompose each bushu any further. Therefore,\nKRADFILE has an average depth of 1.\n3 Experimental Setup\nF or our experiments, we use version 1.5 of\nthe NAIST text corpus ( Iida et al. , 2007),\na collection of Japanese newspaper articles\nwhich is widely used in Japanese NLP research\n(Imamura et al., 2009; Sasano and Kurohashi,\n2011). The corpus consists of roughly 1.7 mil-\nlion character tokens, of which roughly 42%\nare kanji. T o build and test our models we\nuse 5-fold cross-validation.\nOur language models are standard neural\nnetwork models, implemented in T ensorflow;\nthey consist of a embedding layer, with em-\nbeddings for each character (including kanji,\nbushu, and other elements of the Japanese\nwriting systems) which are learned during\ntraining, a standard unidirectional LSTM\n(Sundermeyer et al. , 2012), and a layer which\nmaps the output of the LSTM to a vector rep-\nresenting the probability of the next charac-\nter; the hidden (embedding) size of the LSTM\nfor our experiments is 128. W e train the lan-\nguage model by minimizing the cross-entropy\nbetween the output probabilities and the one-\nhot vector corresponding to the correct an-\nswer, using the Adam optimizer with a batch\nsize of 128 and a learning rate of 0.002.\nIn addition to the four kanji datasets, we\nconsider two kinds of decomposition: shallow\nand deep. Shallow decomposition refers to us-\ning only the first layer of decomposition of a\nkanji character, whereas deep expansion refers\nto an exhaustive decomposition of the kanji\nand all of its bushu. W e use these two meth-\nods to explore whether semantic information is\nreflected in deeper levels of decomposition. In\ngeneral, we aim to compare the performance of\na language model based on the way kanji are\ndecomposed and the depth of their decompo-\nsitions.\nFigure 2 includes examples of both shal-\nlow and deep decompositions, for kanji includ-\ning 賂 (mainai “bribe”) from Figure 1b. De-\ncomposition is done in a left-to-right in-order\ntraversal.\nIt is possible for multiple kanji to share\nthe same bushu. F or example, according to\nKanjiVG, the characters 由 (yoshi “cause/\nreason”), 甲 (kou “carapace/shell”), and 申\n(saru “monkey”) are decomposed into ⽥ (ta\n150\nUnmodified: 彼は賂を取った。\nShallow: ⼻⽪ 彼 は ⾙各賂 を ⽿⼜取 った。\nDeep: ⺅⼻ ⽪ 彼 は ⽬⾙ ⼡⼝各 賂 を ⽿⼜取 った。\nFigure 2: Examples of shallow and deep decomposition using KanjiVG, with original characters\nhighlighted in red. Boxes denote characters that have been decomposed.\nk Baseline GlyphWiki IDS KanjiVG KRADFILE\nShallow Deep Shallow Deep Shallow Deep\n1 77.76 34.31 40.68 37.11 47.75 40.04 47.04 160.94\n2 38.17 33.34 37.52 33.75 48.54 35.91 45.20 89.17\n3 59.41 39.72 49.02 44.73 63.69 45.95 57.79 99.79\n4 70.40 50.70 61.68 50.79 62.00 50.79 64.68 125.67\n5 60.05 50.83 52.65 52.12 68.58 53.03 71.00 155.00\nA verage 61.16 41.78 48.31 43.70 58.11 44.74 57.14 126.11\nT able 2: Language model perplexity based on the different decompositions\n“rice field”) and ⼁ (tatebou “vertical line”).\nBecause of this, using just the decomposition\nof a character can actually lead to a loss of\ninformation. Thus, we postpend all decompo-\nsition sequences with the original kanji char-\nacter to preserve the mapping of bushu to its\noriginal kanji.\nW e evaluate based on perplexity , normal-\nizing the product of the probability of the\n(sub-)characters in our test set by the char-\nacter length of the corpus (lower perplexity is\nbetter). Because decomposing characters af-\nfects the superficial length of the corpus, how-\never, we note that in the cases of decomposi-\ntion we are normalizing using the original (un-\ndecomposed) corpus length in all cases, and\nnot the decomposed token length. This re-\nflects the fact that by adding decompositions\nof a character we are not really adding new\ntext to the corpus. Both regular and decom-\nposed language models in fact predict exactly\nthe original contents of the corpus, but for\nthe decomposed models the uncertainty asso-\nciated with each kanji is distributed among the\npredictions of its bushu (and the postpended\nkanji), and can be retrieved simply by multi-\nplying all the individual probabilities together.\n4 Results\nT able2 shows the perplexity for each of the\nsub-character language models, for the two\npossible decomposition depths, as compared\nto a baseline where no decomposition occurs.\nW e report perplexity for each fold of our 5-fold\ncross-validation, as well as the average.\nFirst, we note that while most of the sub-\ncharacter language models showed some im-\nprovement over the undecomposed baseline,\nKRADFILE performed substantially worse,\nwith a mean perplexity score almost twice as\nhigh as that of the baseline. One potential\nproblem with KRADFILE is that it provides\nonly deep, exhaustive decompostions. Other\nlimitations of using KRADFILE for language\nmodelling are the lack of order in how bushu\nare arranged, and the fact that the bushu are\nlimited to a specific set of characters. W e can\nconclude that it is not a useful dataset for this\npurpose.\nThe best-performing dataset was Gl yph-\nWiki, with shallow decomposition. Not only\nwas this configuration markedly better than\nthe baseline on average, it also beat ev-\nery other option on every fold of our cross-\nvalidation. The results for KanjiVG and IDS\nwere similar, but slightly worse. Interestingly ,\nbased on the statistics in T able 1, Gl yph-\nWiki is the most conservative of the datasets\nin terms of the average number of decomposed\nbushu. W e also found that the best results\nall involved shallow decomposition, which may\nreflect the fact that the most semantically-\n151\nsalient bushu tend to appear at the first level\nof composition; this result was also consistent\nacross folds for IDS and KanjiVG. T aken to-\ngether, these results indicate that some decom-\nposition is useful for building Japanese lan-\nguage models, but too much decomposition is\nnot advisable.\n5 Related W ork\nW orking at the character level has proven\nuseful in language modelling in English, as\nwell as related applications such as build-\ning word representations ( Graves, 2013; Ling\net al. , 2015). With regards to ideographic\nlanguages, there is work in information re-\ntrieval that has considered the appropriate\nrepresentation for indexing; the focus has typi-\ncally been word versus character (Kwok, 1997;\nBaldwin, 2009), but F ujii and Croft (1993)\nconsidered (though ultimately rejected) sub-\ncharacter based indexing. In terms of in-\nvestigations of the usefulness of sub-character\nrepresentations for neural network models in\nideographic languages, relevant work includes\nrecent papers that use sub-character infor-\nmation to assist in the training of charac-\nter embeddings for Chinese ( Sun et al. , 2014;\nLi et al. , 2015; Yin et al. , 2016) or build\nsub-character embeddings directly ( Shi et al. ,\n2015), demonstrating that sub-character infor-\nmation is useful for representing semantics in\nChinese. However, our work differs not only\nin language and task, but also in our use of\ndecomposition, since the work done in Chi-\nnese has primarily focused on a single semanti-\ncally relevant sub-character (known as the rad-\nical), despite the fact that other sub-characters\ndo provide additional semantic information in\nsome characters.\n6 Conclusion and F uture W ork\nIn this paper we have explored the idea of\ndecomposing Japanese kanji to improve lan-\nguage modeling using neural network mod-\nels. Our results indicate that it is possible\nto improve the predictive power of a language\nmodel using decomposition, as measured by\nperplexity , but the effectiveness of this does\ndepend on the properties of the kanji database:\nwhereas Gl yphWiki is a useful resource for\nour purpose, KRADFILE is clearly not.\nWith respect to future work, we have thus\nfar explored only a subset of the options\nwith regards to the decomposition and or-\ndering of sub-characters, and we would also\nlike to consider more sophisticated models\nwhich integrate the structure of the kanji in-\nstead of flattening it, and applying our sub-\ncharacter modeling to other sequential tasks\nsuch as part-of-speech tagging. Given the cor-\nrespondence between kanji and Chinese char-\nacters, a comparison of the two languages\nwith regards to the usefulness of decomposi-\ntion would be worth exploring. W e are also in-\nterested in performing an intrinsic evaluation\nof the character- and bushu-level embeddings\nlearned through language modelling, e.g. rela-\ntive to character-level similarity datasets such\nas that of Y encken and Baldwin(2006).\nReferences\nUlrich Apel and Julien Quint. 2004. Building a\ngraphetic dictionary for Japanese kanji: char-\nacter look up based on brush strokes or stroke\ngroups, and the display of kanji as path data. In\nProceedings of the W orkshop on Enhancing and\nUsing Electronic Dictionaries. Geneva, Switzer-\nland, pages 36–39.\nTimothy Baldwin. 2009. The hare and the tortoise:\nspeed and accuracy in translation retrieval. Ma-\nchine T ranslation23(4):195–240.\nF rancis Bond and Timothy Baldwin. 2016. In-\ntroduction to Japanese computational linguis-\ntics. In F rancis Bond, Timothy Baldwin, Ken-\ntaro Inui, Shun Ishizaki, Hiroshi Nakagawa, and\nAkira Shimazu, editors, Readings in Japanese\nNatural Language Processing , CSLI Publica-\ntions, Stanford, USA, pages 1–28.\nHideo F ujii and W. Bruce Croft. 1993. A com-\nparison of indexing techniques for Japanese text\nretrieval. In Proceedings of the 16th Annual\nInternational ACM SIGIR Conference on Re-\nsearch and Development in Information Re-\ntrieval. Pittsburgh, USA, pages 237–246.\nAlex Graves. 2013. Generating sequences with re-\ncurrent neural networks. CoRR abs/1308.0850.\nRyu Iida, Mamoru Komachi, Kentaro Inui, and\nY uji Matsumoto. 2007. Annotating a Japanese\ntext corpus with predicate-argument and coref-\nerence relations. In Proceedings of the Linguistic\nAnnotation W orkshop. Prague, Czech Republic,\npages 132–139.\nKenji Imamura, Kuniko Saito, and T omoko Izumi.\n2009. Discriminative approach to predicate-\nargument structure analysis with zero-anaphora\n152\nresolution. In Proceedings of the ACL-IJCNLP\n2009 Conference Short Papers. Singapore, pages\n85–88.\nKui-Lam Kwok. 1997. Comparing representations\nin Chinese information retrieval. In Proceedings\nof the 20th Annual International ACM SIGIR\nConference on Research and Development in In-\nformation Retrieval . Philadelphia, USA, pages\n34–41.\nY anran Li, W enjie Li, F ei Sun, and Sujian Li. 2015.\nComponent-enhanced Chinese character embed-\ndings. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Process-\ning. Lisbon, Portugal, pages 829–834.\nW ang Ling, Chris Dyer, Alan W Black, Isabel\nT rancoso, Ramon F ermandez, Silvio Amir, Luis\nMarujo, and Tiago Luis. 2015. Finding func-\ntion in form: Compositional character models\nfor open vocabulary word representation. In\nProceedings of the 2015 Conference on Empir-\nical Methods in Natural Language Processing .\nLisbon, Portugal, pages 1520–1530.\nT omohiko Morioka and Christian Wittern. 2002.\nMoji Dētabēsu ni Motoduku Moji Object Gijutsu\nno Kōchiku . In Proceedings of IPSJ 2002 . (in\nJapanese).\nRyohei Sasano and Sadao Kurohashi. 2011.\nA discriminative approach to Japanese zero\nanaphora resolution with large-scale lexicalized\ncase frames. In Proceedings of the 5th Inter-\nnational Joint Conference on Natural Language\nProcessing (IJCNLP 2011) . Chiang Mai, Thai-\nland, pages 758–766.\nXinlei Shi, Junjie Zhai, Xudong Y ang, Zehua Xie,\nand Chao Liu. 2015. Radical embedding: Delv-\ning deeper to Chinese radicals. In Proceedings\nof the 53rd Annual Meeting of the Association\nfor Computational Linguistics and the 7th Inter-\nnational Joint Conference on Natural Language\nProcessing. Beijing, China, pages 594–598.\nY aming Sun, Lei Lin, Nan Y ang, Zhenzhou Ji, and\nXiaolong W ang. 2014. Radical-enhanced Chi-\nnese character embedding. In Proceedings of\nthe 21st International Conference on Neural In-\nformation Processing (ICONIP 2014). Kuching,\nMalaysia, pages 279–286.\nMartin Sundermeyer, Ralf Schlüter, and Hermann\nNey . 2012. LSTM neural networks for language\nmodeling. In Proceedings of the 13th Annual\nConference of the International Speech Commu-\nnication Association (Interspeech 2012) . Port-\nland, USA, pages 194–197.\nLars Y encken and Timothy Baldwin. 2006. Mod-\nelling the orthographic neighbourhood for\nJapanese kanji. In Proceedings of the 21st In-\nternational Conference on the Computer Pro-\ncessing of Oriental Languages (ICCPOL 2006) .\nSingapore, pages 321–332.\nRongchao Yin, Quan W ang, Rui Li, Peng Li,\nand Bin W ang. 2016. Multi-granularity Chi-\nnese word embedding. In Proceedings of the\n2016 Conference on Empirical Methods in Nat-\nural Language Processing . Austin, USA, pages\n981–986.\n153",
  "topic": "Character (mathematics)",
  "concepts": [
    {
      "name": "Character (mathematics)",
      "score": 0.9035941958427429
    },
    {
      "name": "Computer science",
      "score": 0.7716111540794373
    },
    {
      "name": "Granularity",
      "score": 0.6984268426895142
    },
    {
      "name": "Natural language processing",
      "score": 0.637747585773468
    },
    {
      "name": "Decomposition",
      "score": 0.6184552907943726
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.6073704957962036
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5922420024871826
    },
    {
      "name": "Language model",
      "score": 0.45965757966041565
    },
    {
      "name": "Artificial neural network",
      "score": 0.4550325870513916
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.4423518180847168
    },
    {
      "name": "Linguistics",
      "score": 0.34727659821510315
    },
    {
      "name": "Mathematics",
      "score": 0.11681383848190308
    },
    {
      "name": "Programming language",
      "score": 0.09970319271087646
    },
    {
      "name": "Engineering",
      "score": 0.06249859929084778
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Aerospace engineering",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 13
}