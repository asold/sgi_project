{
  "title": "The impact of large language models on computer science student writing",
  "url": "https://openalex.org/W4410337133",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2060136600",
      "name": "Katerina Zdravkova",
      "affiliations": [
        "Saints Cyril and Methodius University of Skopje"
      ]
    },
    {
      "id": "https://openalex.org/A2187202869",
      "name": "Bojan Ilijoski",
      "affiliations": [
        "Saints Cyril and Methodius University of Skopje"
      ]
    },
    {
      "id": "https://openalex.org/A2060136600",
      "name": "Katerina Zdravkova",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2187202869",
      "name": "Bojan Ilijoski",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2110194581",
    "https://openalex.org/W4389724691",
    "https://openalex.org/W4382060850",
    "https://openalex.org/W1597079686",
    "https://openalex.org/W1463266149",
    "https://openalex.org/W4388220988",
    "https://openalex.org/W1985436376",
    "https://openalex.org/W4399031574",
    "https://openalex.org/W4389237785",
    "https://openalex.org/W4396877921",
    "https://openalex.org/W4390082902",
    "https://openalex.org/W2140924695",
    "https://openalex.org/W2980906503",
    "https://openalex.org/W4391097696",
    "https://openalex.org/W7006797580",
    "https://openalex.org/W3033844602",
    "https://openalex.org/W4377371734",
    "https://openalex.org/W4404783249",
    "https://openalex.org/W4392196220",
    "https://openalex.org/W4233043065",
    "https://openalex.org/W2055507542",
    "https://openalex.org/W4391519634",
    "https://openalex.org/W2136527763",
    "https://openalex.org/W4398184732",
    "https://openalex.org/W4387163695",
    "https://openalex.org/W4388095513",
    "https://openalex.org/W4388461519",
    "https://openalex.org/W6863969017",
    "https://openalex.org/W4387812493",
    "https://openalex.org/W2160656577",
    "https://openalex.org/W2761592564",
    "https://openalex.org/W4389610758",
    "https://openalex.org/W4384200891",
    "https://openalex.org/W4388728825",
    "https://openalex.org/W13307908",
    "https://openalex.org/W4396908497",
    "https://openalex.org/W4313313884",
    "https://openalex.org/W4407172847",
    "https://openalex.org/W2489007269",
    "https://openalex.org/W2124972954",
    "https://openalex.org/W2791039954",
    "https://openalex.org/W4406357289",
    "https://openalex.org/W2768515196",
    "https://openalex.org/W4392773210",
    "https://openalex.org/W4403935384",
    "https://openalex.org/W4385570036",
    "https://openalex.org/W4386967982",
    "https://openalex.org/W4221055872",
    "https://openalex.org/W2476742693",
    "https://openalex.org/W4391664734",
    "https://openalex.org/W4400411628",
    "https://openalex.org/W4408124808",
    "https://openalex.org/W4313343251",
    "https://openalex.org/W4406255722"
  ],
  "abstract": "Abstract The new version of ACM and IEEE-CS Computer Science Curricula envisages the preparation of a dozen white papers within more than a third of the Body of Knowledge courses. Lack of experience in conducting research, insufficient ability to articulate thoughts, non-observance of defined recommendations, as well as forgotten verification of what has been done are key challenges in the implementation of these recommendations. Large language models (LLMs) can significantly support the creation of anticipated white papers, encouraging initial directions and inspiration for research, as well as a focused presentation of key elements of the work. Unfortunately, they can also hinder the creation of expected academic texts, promoting the superficiality of research and the presentation of unverified information. In order to facilitate progress in acquiring the necessary writing skills without discriminating against those students who have already acquired them and prefer traditional writing, we have defined a series of strict rules and strategies for preparing the briefing reports with complete, partial, or no reliance on the LLM. They were embedded into the faculty learning management system, implemented, and evaluated twice with more than 150 students during the academic year 2023/24. This paper presents a new approach along with the lessons learned from the first attempt and the changes incorporated into the second. Based on these two carefully designed, conducted and analyzed studies, teachers and students gained a very positive opinion about the impact of LLM on the creation of quality student reports. The effectiveness of the new approach is a guarantee that optimization of the integration of AI-generated content can become a benchmark for the successful preparation of student reports that can be recommended to the wider academic community.",
  "full_text": "Open Access\n© The Author(s) 2025. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. \nIf material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/.\nRESEARCH ARTICLE\nZdravkova and Ilijoski  \nInt J Educ Technol High Educ           (2025) 22:32  \nhttps://doi.org/10.1186/s41239-025-00525-1\nInternational Journal of Educational\nTechnology in Higher Education\nThe impact of large language models \non computer science student writing\nKaterina Zdravkova1   and Bojan Ilijoski1*   \nAbstract \nThe new version of ACM and IEEE-CS Computer Science Curricula envisages \nthe preparation of a dozen white papers within more than a third of the Body \nof Knowledge courses. Lack of experience in conducting research, insufficient \nability to articulate thoughts, non-observance of defined recommendations, \nas well as forgotten verification of what has been done are key challenges \nin the implementation of these recommendations. Large language models (LLMs) \ncan significantly support the creation of anticipated white papers, encouraging \ninitial directions and inspiration for research, as well as a focused presentation of key \nelements of the work. Unfortunately, they can also hinder the creation of expected \nacademic texts, promoting the superficiality of research and the presentation \nof unverified information. In order to facilitate progress in acquiring the necessary \nwriting skills without discriminating against those students who have already \nacquired them and prefer traditional writing, we have defined a series of strict rules \nand strategies for preparing the briefing reports with complete, partial, or no reliance \non the LLM. They were embedded into the faculty learning management system, \nimplemented, and evaluated twice with more than 150 students during the academic \nyear 2023/24. This paper presents a new approach along with the lessons learned \nfrom the first attempt and the changes incorporated into the second. Based on these \ntwo carefully designed, conducted and analyzed studies, teachers and students gained \na very positive opinion about the impact of LLM on the creation of quality student \nreports. The effectiveness of the new approach is a guarantee that optimization \nof the integration of AI-generated content can become a benchmark for the successful \npreparation of student reports that can be recommended to the wider academic \ncommunity.\nKeywords: Academic student writing, Generative AI, Large language models, Ethics\nIntroduction\nIn recent years, soft skills have become a compulsory part of computer science curricula \n(Groeneveld et  al., 2020; Vogler, 2018). They were often initiated by employers who \nneeded to improve communication with staff who belonged to a completely different \nworld (Ellis et  al., 2014; Scaffidi, 2018). Since 2001, soft skills have been included in \nvarious forms in the Computing Curricula (Ieee-Cs & Task, 2001), at that time as part of \nthe principles of professional practice (Groeneveld et al., 2020).\n*Correspondence:   \nbojan.ilijoski@finki.ukim.mk\n1 Ss Cyril and Methodius \nUniversity in Skopje: Saints \nCyril and Methodius University \nin Skopje, Skopje, North \nMacedonia\nPage 2 of 24Zdravkova and Ilijoski  Int J Educ Technol High Educ           (2025) 22:32 \nTo foster future success in the business world, soft skills were soon added to computer \nscience service learning and capstone courses (Carter, 2011). According to the Version \nBeta of Computer Science Curricula 2023 (Acm & Joint, 2023), the following six courses \nanticipate writing a white paper as part of the competency specification: Algorithms and \ncomplexity; Architecture and organization; Foundations of programming languages; \nGraphics and interactive techniques; Networking and communication; and Society, \nethics, and professionalism. White papers are comprehensive essays that promote \na certain product, service, or point of view (Graham, 2013, 2024). They are guides or \nreports designed to persuasively present information, analysis, and recommendations on \na specific topic or problem (Shepard et al., 2009). Apparently, the term began to be used \nafter the publication of the British White Paper in June 1922 (Malone & Wright, 2018). \nFor more than a century, white papers have been widely used to shape various policies, \nsuch as industry and material efficiency (Allwood et al., 2011); risk management (Renn, \n2009), health and medicine (Safran, 2007); sustainability and environment (White & \nEllis, 2007); social welfare (Lombard, 2008); predictive analysis (Nyce & Cpcu, 2007); \nas well as education (Wilson, 2009; Froyd, 2008). With the revised Computer Science \nCurricula (Acm & Joint, 2023), computer science students should write a dozen white \npapers during their undergraduate studies. Unfortunately, due to their lack of writing \nand research experience, it can be an extremely demanding job for them (Groeneveld \net al., 2020; Kellogg & Raulerson, 2007).\nWhite papers are a very specific way of academic writing (Graham, 2013). They should \nprovide more detailed and comprehensive information (Jackson, 2009). The preparation \nof a white paper should be based on research (Binkley, 2010). At the same time, the \ndevelopment of the research problem should be relevant, objective, and impartial \n(Florysiak & Schandlbauer, 2019). It should have a very strict structure, typically \nconsisting of an introduction, findings, analysis, recommendations, and a conclusion \n(Bailey, 2014). Throughout the white paper, and especially in the conclusion, it is \nnecessary to express the critical attitude of the author (Petric, 2002).\nWithin the Software Competency Area (Acm & Joint, 2023), preparation of \nthree white papers is suggested. The first two aim to justify the selection of the most \nappropriate programming language or paradigm. The third white paper deals with \nsecurity and protection, topics that are directly related to the computer ethics course. \nSystems Competency Area also proposes two such briefing reports. They address social, \nethical, and professional issues. The first report explores the design and implementation \nof a networked system, and the second is the collection and storage of application \ndata. In order to harmonize these recommendations with our course, the project topic \nproposal addressed three aspects: advantages and threats of artificial intelligence; new \ntechnologies and their impact on privacy; and surveillance.\nSince last year, students have had a great ally for academic writing: large language \nmodels (Yuan et  al., 2022). Throughout this paper, we will investigate the impact of \ntheir application within the extensive literature review and on the basis of two own case \nstudies carried out during the academic year 2023/24 (Zdravkova et al., 2024).\nThe paper will exhaustively cover this research question: “How to integrate LLMs into \nacademic writing by implementing a structured presentation approach that improves its \norganization and quality and at the same time reduces the risks of academic dishonesty?”\nPage 3 of 24\nZdravkova and Ilijoski  Int J Educ Technol High Educ           (2025) 22:32 \n \nThe work continues with a review of work related to university academic writing \nand new experiences that include large language models as a tool to create homework, \nreports, and graduate theses. At the end of this section, examples of academic dishonesty \nwithout and with the application of LLM will be reviewed. The “Setting the scene” \nsection will present our study in more detail, explaining how the elements related to \nwriting white papers that were presented in the fourth paragraph of the Introduction \nfit into it. After explaining the case study, we will present the results of the students, \ncomparing their results in the realization of the team project without and with the use \nof large language models and achievements in other activities that make up the grade. \nSpecial attention will be paid to the impression of the case study from the point of \nview of teachers and from the point of view of students. They are part of the section \n“Impressions of teachers and students on the integration of LLM in briefing report \nwriting. ” Based on these impressions, conclusions and recommendations on how to \nintegrate LLM for honest undergraduate writing with high quality will be proposed. We \nwill consistently adhere to them next academic year in the hope that they will become a \nbenchmark for improving education based on generative artificial intelligence.\nReview of student writing without and with LLM support\nThe use and influence of large language models, especially in the last few years, has \ngrown significantly. Education as a discipline has not been avoided, and this can be seen \nfrom the number of papers written on this topic, especially in the last two years. Accord-\ning to Google Scholar (at the moment of writing of this article, the end of 2024), the \nscholarly literature search engine, the number of papers in 2023 for the query “LLMs \nin education” was about 17,400, which is more than a double increase compared to the \nprevious year, and at the end of 2024 the number of papers that cover this topic is 46,500 \n(Fig. 1). This trend will most likely continue in the coming years, and models dealing with \ngenerative artificial intelligence will become an inevitable part of education systems. In \nFig. 1 Number of papers per year in Google Scholar search for query “LLMs in education”\nPage 4 of 24Zdravkova and Ilijoski  Int J Educ Technol High Educ           (2025) 22:32 \nthe following, we will highlight some of the papers that touch on various aspects of the \nuse and integration of LLMs in education.\nAcademic use of large language models can cover many different aspects. Students \ngenerally rely on these models for homework, studying, personalized experiences, skill \ndevelopment, etc. Teachers use large language models as an aid in general and during \nmaterial preparation or classes, which saves time. However, challenges and other factors \nthat need to be addressed, such as reliability, accuracy, ethics, etc., are still being faced \n(Labadze et al., 2023). These models can serve as a learning guide and help in knowledge \nbuilding, thus taking over some of the roles of teachers, such as summarizing content, \nexpanding ideas, and connecting concepts (Lee et  al., 2023). In addition, observation \nof the potential and behavior of educational large language models has a significant \npotential in the direction of providing valuable insights and help to educators and \neducational policy makers (Gan et al., 2023).\nLLMs are used in various disciplines to improve education, research, and creative \nwriting. In terms of education, LLMs are used in physics, generate solutions, create \ntasks (Liang et al., 2023), and act as virtual teachers to address student errors (Ding et al., \n2023). They enable STEM participation through educational games, fostering critical \nthinking (Huber, 2024; Martínez-Téllez, 2023). In engineering, LLMs help with tasks \nsuch as answering questions, writing essays, and homework (Qadir, 2023). In medicine, \nspecifically orthopedics, they improve learning, surgical planning, and research through \ncomprehensive information and analysis (Chatterjee et al., 2023). For English language \nlearning, they support reading comprehension exercises, receiving positive feedback \nfrom educators (Xiao et  al., 2023). In Laato et  al. (2023) the authors investigate the \nimpact of LLMs on learning and instruction in Finnish higher education, drawing \nvarious implications that confirm that LLMs are powerful tools that can support the \nlearning process.\nThe use of artificial intelligence in education offers great potential in terms of \npersonalized experiences. LLMs have great transformative potential to personalize \nlearning, promote research innovation, democratize knowledge, and promote social \nequity (Diab Idris et  al., 2024). However, it is necessary to emphasize AI literacy and \ncritical thinking, provide comprehensive training, and address ethical and social \nimplications to reduce the dangers of its overuse (Walter, 2024).\nSome research shows that these models can be a useful tool for helping students with \nwriting, instructions, and general learning during classes (Arista et  al., 2023; Phutela \net al., 2024). In addition, there are cases where LLMs are used in research and writing. \nFor example, LLMs contribute to creative writing, matching or exceeding human \nperformance (G’omez-Rodr’iguez & Williams, 2023). Their role in scientific writing \nexpands (Liang, 2024) with tools that improve task efficiency (Zohery, 2023). LLMs are \nbeing used as personalized tutors in EFL writing education, offering customized essay \nfeedback to meet students’ unique needs and preferences, with future advancements \naiming to improve customization for more personalized learning experiences (Han, \n2024).\nLLMs like GPT- 4 are transforming education by addressing challenges in AEC and \nautomated essay evaluation (AEE). In AEC education, GPT- 4 demonstrated strengths in \nproviding sustainable feedback and improvement suggestions, but showed limitations in \nPage 5 of 24\nZdravkova and Ilijoski  Int J Educ Technol High Educ           (2025) 22:32 \n \nranking precision, prioritizing form over content, and handling large or poorly formatted \ndocuments, making it more suitable for general feedback rather than grading (Castro \net  al., 2024). In academic assessment, ChatGPT- 4 highlights the potential of tailored, \nreliable tools for accurate grading and constructive feedback (Kostic et  al., 2024). \nFurthermore, a framework was developed to address the impact of LLM on academic \nfraud in writing assignments, although some supporting literature lacks empirical \nvalidation (Ya-Ping Hsiao & Chiu, 2023).\nThe integration of LLMs in education presents challenges that must be addressed to \nharness their potential. While LLMs enhance efficiency, metrics for quantifying bias \nare essential to prevent perpetuating stereotypes. LLMs need mechanisms to help them \nvalidate outputs prone to inaccuracy or hallucinations (Meyer, 2023). Ethical risks such \nas plagiarism, disinformation, and academic conflicts also require careful management \n(Buruk, 2023; Nam & Bai, 2023). Tools such as ChatGPT promise to help students with \nessay writing, but they face limitations, including hallucinations (Han, 2023). Addressing \nchallenges such as interdisciplinary reasoning, student modeling, social biases, cheating \nprevention (Zdravkova, 2025), and multimodal education is critical to fully leverage \nLLM (Huang et al., 2024). Insights from studies on authorship attribution highlight the \nimportance of robustness, explainability, and trustworthiness in AI applications (Huang \net al., 2024).\nAI can reshape education (OECD) and to help in the research process (OECD, \n2023), but the inclusion of LLMs in education must be done carefully and cautiously, \npaying special attention to potential challenges and gaps (OECD, 2025). One of them is \nreliability and relevance (Ravšelj, 2025) as well as widespread false knowledge (Xu et al., \n2024; Wang, 2024), which we are trying to examine and explore by introducing different \ntypes of projects (LLMs only, hybrid and no LLM). It also raises ethical issues, policies, \ncurriculum integration, guidelines etc. Diab Idris et al. (2024); Zlotnikova et al. (2025); \nJess (2024). One of the recommendations is to be proactive and experiment and find \nways to incorporate AI into the educational process (Wang et al., 2024), which is one of \nthe goals of this article.\nThis work represents a continuation of our work on the integration of large language \nmodels into education (Zdravkova & Dalipi, 2023) and describes the joint experience \nof integrating LLMs, for both academic writing and computer programming at two \nuniversities from two countries (Zdravkova et al., 2024).\nSetting the scene\nThe case study on integrating LLMs for the preparation of briefing reports was \nconducted for the first time during the academic year 2023/24 with students enrolled \nin the computer ethics course. The course was launched over 20 years ago. Since its first \nedition, it has been designed to adhere to the IEEE and ACM guidelines for computer \nscience curricula (Ieee-Cs & Task, 2001). It is one of the twelve elective courses for \nstudents enrolled in a 240 ECTS program, and one of the nine elective courses for \nstudents completing a 180 ECTS program. The course belongs to the most advanced \nLevel 3, designed for students who have already acquired substantial knowledge in the \nPage 6 of 24Zdravkova and Ilijoski  Int J Educ Technol High Educ           (2025) 22:32 \ncore and professional subjects. While soft skills have been addressed in earlier courses, \nthis is their first experience with research assignments and white papers.\nAccording to the latest recommendations (Acm & Joint, 2023), the role of white \npapers is important and very often refers to the social and ethical implications \nof various computer technologies. What was even more motivating for replacing \ntraditional essay writing with the preparation of white papers is that the course is \nattended by final-year students who already have practical experience in software \ndevelopment companies, and they are expected to submit such briefing reports to \nwork. We estimated that the experience gained during this course will have a decisive \ninfluence on the development of professional dispositions and contribute to the \nharmonization of the knowledge model and the competency model.\nRegardless of the long experience in implementing this course (Zdravkova, 2016), \nwhite papers were a big novelty for us, so it is very likely that we made certain \nmistakes in the project development. After consulting with companies and analyzing \nthe mood of students, we will be happy to polish them next academic year.\nThe course was enrolled by 218 students from our faculty and two Erasmus+ \nexchange students from the University of Bremen. Teaching includes four lessons per \nweek taught by subject teachers and guest lecturers from both academia and industry. \nStudents are evaluated on the basis of two team projects that include the preparation \nof individual briefing reports, their integration into a joint white paper followed by \npublic presentations and discussions, and two mid-term exams that are carried out \nin laboratories, as well as a journal that students prepare during the entire semester \n(Zdravkova et  al., 2022). Each team project contributes 50% to the final grade, the \nmidterm exams account for 30%, and the journal makes up 20% of the final grade.\nUntil the exams in June 2024, 148 students, including those on exchange, have \nsuccessfully completed their obligations, which is 67.27% of the total number of \nstudents and exceeds the faculty passing rate during the academic year 2023/24. The \naverage grade of 7.33 out of 10.00 is also above the faculty average, and in terms of \nsuccess, the course ranks tenth among those courses that were enrolled in by at least \n150 students. If we compare these statistical data with the previous academic year, \nthey are almost identical. Namely, after the exam term last academic year, 67.83% \nof students successfully completed the course, with an average grade of 7.35. This \nindicates that the integration of LLMs did not significantly change the achieved \nresults of the students. In the extension of this section, it is explained in detail how it \nwas carried out.\nEach individual briefing report was part of a group report, in the implementation \nof which 12 students participated, one of whom was the team leader and the other 11 \nwere team participants.\nWhile the responsibilities of the team leader and members differ, both roles involve \na comparable workload of approximately 30 hours per person, which aligns with the \nexpectation of 1 ECTS credit. To ensure fairness, the distribution of tasks is designed \nto be balanced in terms of time and effort required, so that each student’s contribution \nis equally valued in the final assessment.\nThe project implementation is divided into ten sequential steps, outlined below: \nPage 7 of 24\nZdravkova and Ilijoski  Int J Educ Technol High Educ           (2025) 22:32 \n \n 1. Presentation of project topics and rules for creating individual and group white \npapers.\n 2. Selecting the team the student wants to be a member of. The selection can be \nfollowed by everyone and while the team selection is in progress, a team change is \nallowed.\n 3. Within the team’s private forum, students choose their team leader.\n 4. The team leader defines the topics that the team members will research and sends \nthem to the responsible professors for a review.\n 5. The professor carefully checks the suggested topics and their explanation and, if \nnecessary, returns them for revision, proposing changes and corrections.\n 6. After the topics and their explanations are approved, the team leader publishes them \non the forum so that colleagues can distribute them among themselves. The team \nleaders are excluded from this distribution because they are not obligated to write \ntheir own individual paper.\n 7. Students prepare their own reports, submit them to the quiz (explained further in \nthe text) that best matches the way they did their research. In parallel, each student \npublishes them on the team’s private forum along with a couple of slides that explain \nthe report.\n 8. Based on the students’ individual contributions, the team leader creates a group \nwhite paper and a ten-minute presentation, which are then published on the \ncorresponding private forum. Before being publicly presented, they are reviewed by \nthe team members.\n 9. Selected team members who volunteered for this obligation present the topic and \nreceive bonus points for their successfully completed task.\n 10. After presenting all subtopics of the main topic, the professor initiates a public \ndiscussion with questions, in which all students participate. Discussions are part of \nthe student’s project evaluation.\nComparing the proposals of the team leaders with the answers generated for us by \nseveral large language models, we found that a third of the leaders proposed topics and \nexplanations that coincide with the answers of the LLMs. The other team members had \na period of 20 days to prepare their individual briefing reports on the topic they chose \nfrom their team leader’s list. To provide insight into how students researched, integrated \nAI-generated answers, and shaped their individual briefing reports, we created three \nquizzes within the Moodle learning management system. They were designed to allow \nstudents to upload their own briefing reports that were created without any help from \nartificial intelligence, with partial reliance on LLM, or were fully supported by LLMs.\nThe first quiz was probably chosen by those students who have already gained \nsignificant experience in preparing reports and who preferred to do their assignment \nindependently without using any LLM. In the rest of this paper, their mode of operation \nis designated as No LLM (the blue branch of the graphic presentation of the quizzes \nfrom Fig.  2). After announcing the team to which they belong and the topic they have \nchosen, which is the common at the beginning of all three quizzes, the students were \nsupposed to indicate which three research questions related to the topic they processed \nin more detail. For each of these questions, in the quiz, they had to list the relevant \nPage 8 of 24Zdravkova and Ilijoski  Int J Educ Technol High Educ           (2025) 22:32 \nsources from where they found the answers to those questions and copy the key parts of \nthe references that helped them shape their text.\nFig. 2 Graphical representation of three quizzes through which students submit their individual essays\nPage 9 of 24\nZdravkova and Ilijoski  Int J Educ Technol High Educ           (2025) 22:32 \n \nThe second quiz was chosen by those students who had already experienced LLMs \nand had the impression that it was quite sufficient for preparing their assignment. Their \nmode is AI-supplemented or AI-enhanced, or LLM only. Before showing how they \nused it, they indicated which LLM they primarily relied on. After that, similar to the \nstudents who applied the traditional writing approach, they had to list the three key \nquestions they asked the chosen LLM (the blue branch). In addition to having to copy \nthe AI-generated answer for each of these three questions, they had to find sources that \nconfirm the relevance of the facts in that answer.\nThe third quiz combines research questions with questions the students asked \nthe LLMs. It is about AI-augmented writing, a mutual collaboration of human and \nartificial intelligence, which we briefly called Hybrid mode of operation (green \nbranch). We recommended that the ratio of questions be equal, i.e., to combine two \nresearch questions and two questions answered by the LLM. The extension of the quiz \ncomponents is identical to the first two quizzes.\nMost students communicate with LLMs and Google Search using similar phrases. For \nexample, a student who implemented the hybrid mode to explore the ethical challenges \nof robocalls prompted ChatGPT with the following questions: “What can you tell me \nabout the psychology of robocalls?” and “Who are the most targeted victims based on \nthe psychology of robocalls?” The research questions searched within Google Scholar \nwere: “How can psychological manipulation in robocalls be prevented?” and “What \nis the impact of robocall scams on mental health?. ” It seems that students tend to use \nconversational phrases when interacting with GenAI, while their search queries in \nacademic platforms are more formal and targeted.\nUnfortunately, some students did not understand the difference between LLM \nprompts and research questions. For instance, a student investigating the ethical \nchallenges of AI use in the criminal justice system asked the same question by \nprompting: “What are the risks of using AI in criminal justice?” and searching “Risks \nof using AI in criminal justice?” on a search engine. This example illustrates how some \nstudents may apply similar questioning approaches to both language models and search \nengines, overlooking the distinct ways each source provides information.\nSeveral students misunderstood the role of LLMs in their research, requesting them \nto write either the introduction or the entire briefing report. This highlights a lack \nof understanding about the intended use of these tools, which are meant to assist \nin generating ideas and refining content, rather than completing entire sections of \nacademic work. They were penalized with 0 points.\nAt the end of all the quizzes, students should first upload the introduction of their \nreport explaining briefly their research, and announce the extension of the report; after \nthat, show the elaboration of the topic; and finally a conclusion with their own critical \nattitude and recommendations on how to deal with the ethical challenges they discov -\nered. The sources used should be carefully referenced in all three parts according to the \nrecommendations that were presented in the project explanation. Finally, they should \nlist all the references used in the order in which they were used in the report.\nPage 10 of 24Zdravkova and Ilijoski  Int J Educ Technol High Educ           (2025) 22:32 \nGrading scheme\nEach group of mutually interconnected parts of the quiz related to the research \nquestions, or the prompts given to the LLM, as well as the four common parts of the \nquiz in which students enter their briefing report, were evaluated. The grade depended \non the quality of the research questions or the questions posed to LLM, the quality and \nrelevance of the references supporting the research, and the quality of the presented \nresearch. The final grade was formed by harmonizing the weighting factors of the \nresearch that preceded the writing of the report, the introduction, the main theme, and \nthe conclusion.\nVerbatim copies and cross-lingual plagiarism in assignments were immediately rated \nwith 0 points, thus ending the grading. Improper reference citation was penalized by \ndeducting a maximum of 80% of the possible points, depending on the degree of \nplagiarism committed.\nIf plagiarism was not detected, except for improper citation, the grade was multiplied \nby a factor that evaluated:\n• Individual engagement during the preparation of the report, which was evaluated \nbased on the metadata of the original document and the time spent on completing \nthe quiz.\n• Originality, which was manually checked based on defined questions and copied \nparts of sources that confirm the web search and the relevance of the generated \nanswers.\n• Adherence to the rules of writing a report, which depended on the number of words \nand the number of cited references.\nStudents got additional points for public presentation of the team reports and \nparticipation in the public discussions on all topics related to the group project. The goal \nof these face-to-face activities was to enable students to show that they know the topic \nthey have researched. Thanks to the public participation in the discussions, we realized \nthat, regardless of the way in which the students did their own assignment, they all \ngained an equal amount of knowledge.\nEvaluation of the proposed writing approach\nThe realization of each group project takes about a month. It starts with choosing the \nteam in which the student wants to participate. The teams are publicly visible, so that \nstudents are grouped not only based on the broad topics they need to work on, but also \nbased on the schoolmates they want to collaborate with. After choosing the topic, within \nthe team’s private forum, students select a leader who defines the individual topics, \nwhich the members should process in detail as their individual assignment.\nDepending on how they choose to write their report (traditionally, using the hybrid \napproach, or fully LLM-based), students submit their final report in the corresponding \nquiz. We don’t impose any restrictions, so if they initially select one quiz and later \nchange their approach, they can opt for a different one. As a result, several students took \nadvantage of this flexibility in the first project. In the second project, they were more \ndiligent, so there were no duplicate submissions of the same report.\nPage 11 of 24\nZdravkova and Ilijoski  Int J Educ Technol High Educ           (2025) 22:32 \n \nIn parallel with the delivery of reports from the quizzes, the students also deliver \nthem in the private forum of their team, together with a couple of slides that explain \nthe content of their report. The team leaders then compose a joint report and a joint \npresentation that is publicly presented for 10 minutes, after which the students discuss \nall the topics of the project. For the discussions, they receive additional points with \nwhich they complete the grade. The additional points in the academic year 2023/24 \ncarried only 20% of the total grade. We are considering increasing it to stimulate mass \nparticipation in face-to-face discussions.\nThe team leaders did not use the quizzes to upload narrow themes for further research, \nalthough after evaluating their contribution, it was obvious that most of them partially or \ncompetently consulted LLMs. We plan to change that next academic year by introducing \nquizzes intended only for them.\nThe section continues with the demographics of report preparation by method of \npreparation. Then, the results of the briefing reports for both projects are compared, \ndepending on the chosen mode, but also with the results that the students showed at the \nmidterm exams. The latter was extremely important to us to determine whether better \nstudents prefer one style of production over weaker ones. The comparison will also be \nshown and briefly discussed.\nDemographics of briefing report preparation\nA total of 181 students worked on the first group project, 15 of whom were team leaders. \nAs previously announced, team leaders did not have to complete the quiz because their \ntask was to discover and explain the topics that colleagues should work on and unite the \nindividual results into a joint product, which does not coincide with the methodology of \nreport preparation. Although they did not explicitly answer whether they used any LLM \nduring their research, the comparison of their proposals with the answers we received \nfrom several LLMs shows that a third of them had a proposal that was similar to the \nAI-generated one.\nQuizzes were successfully completed by 150 locals and 2 Erasmus+ exchange students. \nAs many as 14 students submitted their reports within the team private forums without \nentering them in the quizzes. They were evaluated based on the quality of the reports \nand the teacher’s assessment of whether and how much they relied on the LLMs. In the \ncommentary of the work, they were reminded to consistently complete their obligations \nduring the second group project. 78 students, which is slightly more than half of all, \nchose traditional writing. 68 students selected the hybrid mode, while only 6 of them \ncreated their individual assignments relying only on LLMs.\nA total of 135 students completed the second project, and 6 did not complete the quiz \nbut only posted their project on the forum. There are a total of 74 students who worked \nwithout applying the LLM, 47 students used a hybrid approach, and only 13 served only \nwith the LLM.\nTable 1 shows the number of students according to their type of project during the first \nand second projects. It can be noted that more than 60% of the students didn’t change \ntheir choice, and in terms of the different choices, it generally comes down to students \nwho had chosen the method without using a language model and switched to a hybrid \napproach and vice versa.\nPage 12 of 24Zdravkova and Ilijoski  Int J Educ Technol High Educ           (2025) 22:32 \nThe vast majority of students who used the hybrid mode decided on the freemium \nversion of ChatGPT (see Fig.  3). Microsoft’s Bing Chat, as of November 2023, was the \nsecond choice of our students, again, with English preferred by 60% of students. Google’s \nGemini has obviously not yet gained the popularity of the two previous generative mod -\nels. Bing Chat and the commercial ChatGPT 4 together helped less than a quarter of \nthese students, more precisely, only 10 of a total of 41 students. However, those students \nwho relied on LLM and chose Gemini and ChatGPT 4 preferred their mother tongue.\nSimilarly to the first project, ChatGPT dominates both native and English speaking \nstudents when it comes to the Hybrid Approach (Fig.  4). And in these cases the use of \nChatGPT is present with about 90%. For students who work only with LLM, the situ -\nation is different. There, among students who work in English, the number of users of \nChatGPT and ChatGPT 4 (paid version) is the same, while the number of users of Chat -\nGPT in Macedonian is again higher.\nAlthough the LLMs our students used are multilingual and enable human-like con -\nversations in the native languages of all students attending the course, the majority, i.e. \n59.46%, opted for English (Fig.  3). During the first project, up to 77. 03% chose Chat -\nGPT as the primary LLM, whereby English ChatGPT was the basis or support of the \nassignment for 44.59% of all students who decided to use LLM in the preparation of the \nTable 1 Number of students on the first and second projects presented through their approaches \n(contingency table)\nFirst project\nSecond project No LLM Hybrid Only LLM No project\nNo LLM 49 13 2 13\nHybrid 15 28 6 19\nOnly LLM 1 1 2 1\nNo project 9 5 3\nFig. 3 Distribution of students according to used language and LLM during the first project\nPage 13 of 24\nZdravkova and Ilijoski  Int J Educ Technol High Educ           (2025) 22:32 \n \nassignment. Together with its more advanced version, ChatGPT 4, the English ChatGPT \nwas used by 48.65% of students who preferred AI support in preparing their individual \nassignments. For the second project, 63% of the students who decided to work with the \nHybrid approach chose to work in English, while among the students who work only \nwith the LLM, this number is almost equal (Fig. 4).\nThe main reasons for the predominance of this combination are the popularity of \nthe ChatGPT chatbot, the better quality of answers for the English language, and also \nthe habit of computer science students to primarily use English as a lingua franca.\nAchieved results\nWe were curious about who selected which writing mode and examined the results \nof the midterm exam, which was held when our project was in its final stage. The \nresults are fairly even, with a slight advantage for the students who did not use LLM \n(Table 2). The majority of them participated in the exam and achieved the best results. \nMoreover, their reports were far better than the reports of the students who used the \nhybrid mode.\nWe discussed this interesting fact, and the only hypothesis that would fit is that those \nstudents who traditionally write their own reports are, in fact, more diligent, more \nFig. 4 Distribution of students according to used language and LLM during the second project\nTable 2 Midterm exam results vs. preparation mode during the first team project\n Success during first midterm exam Preparation mode\nNo LLM Hybrid Only LLM Team leader\nTook the exam 72 62 5 12\nSelected mode 78 68 5 15\nPercentage of attendance 92.31% 91.18% 100.00% 80.00%\nMidterm exam result 55.61 51.27 51.87 54.89\nPage 14 of 24Zdravkova and Ilijoski  Int J Educ Technol High Educ           (2025) 22:32 \ncommitted to their studies, and clearly read carefully what is expected of them. Among \nother things, during the discussions we noticed that they are more eloquent and have the \nability to think critically. This coincides with our previously suppressed impression that \nthese students already have significant experience in writing and therefore did not need \nto rely on the help of the LLM.\nAs for the students who used the hybrid mode, we noticed that most of them hesitated \nabout which quiz to take. A few of them started to complete all three quizzes, but in the \nend they decided on the hybrid mode. Their hesitation may be due to a lack of research \nexperience, insufficient familiarity with the explanations of the writing method, and also \na lack of flexibility and difficulty adapting to different approaches.\nThe results are quite similar when comparing the second project and midterm exam \n(Table 3). A slight improvement can only be seen in the Only LLM results, but here we \nalso have more students this time.\nIf we look at the histograms in Fig.  5 we will notice that the points from the first and \nsecond midterm exam have a very similar, almost normal distribution, which is to some \nextent expected when it comes to exams. In the distributions from the first and the sec -\nond project, we have a rather large difference. In the first project we have an almost even \ndistribution, while in the second we have some kind of left-skewed distribution where \nmost of the students have most of the points. One of the reasons for this is the feed -\nback the students received after completing the first project, so they probably corrected \nthe mistakes in the second one. This can also be observed from the scatter plot for the \nfirst and the second project. It can be seen that the majority of students (more precisely \n82.58%) have a better or at least the same result in the second project compared to the \nfirst. If we look at the scatter plot between midterm exams, we can notice that there is a \ncorrelation.\nThe existence of a correlation between the projects and the exams can also be seen in \nFig. 6 where we can see that both the Pearson and Spearman correlations have slightly \nhigher values (compared to all the others). The exams have a slightly higher Pearson’s \ncorrelation because linearity also plays a role here (which can also be seen from their \nscatter plot), while the projects have a slightly higher Spearman correlation (than Pear -\nson’s) because only the growth of the values is important in this case.\nAnother aspect that can be considered are the results of the students in the first and \nthe second project, depending on the type of project. The number of students for each \nof the projects and types was already given in Table  1. Figure  7 shows a scatter plot of \nthe points of each of the students who participated in the first or second project (x-axis \nfirst project, y-axis second project). The type of project they worked on is given by color, \nTable 3 Midterm exam results vs. preparation mode during the second team project\n Success during first midterm exam Preparation mode\nNo LLM Hybrid Only LLM Team leader\nTook the exam 67 44 13 13\nSelected mode 73 47 11 13\nPercentage of attendance 91.78% 93.62% 78.57% 100.00%\nMidterm exam result 55.9 51.56 55.80 54.68\nPage 15 of 24\nZdravkova and Ilijoski  Int J Educ Technol High Educ           (2025) 22:32 \n \nand the inner circle represents the type of the first project, while the outer circle repre -\nsents the type of the second one. If any of the circles is missing, it means that the student \nworked on only one of the projects. As we have already seen from Table  1, there are not \nFig. 5 Scatter plots and histograms of midterm exams and projects\nFig. 6 Correlation between all activities\nPage 16 of 24Zdravkova and Ilijoski  Int J Educ Technol High Educ           (2025) 22:32 \nmany students who work on different types during the creation of the two projects, so \nhere, too, the inner and outer circles are generally the same color, and we have relatively \nfew circles with different colors. Most of the points lie above the line x=y, which indi -\ncates that there is an improvement in almost every student during the preparation of the \nsecond project. The general impression is that students who worked on both projects \nwithout an LLM have the best results, followed by students whose approach was the \nhybrid method.\nFace‑to‑face discussions\nThe last part of the project was optional. It complements the result obtained by the \npreparation of the report and the presentation of the group projects and carries 20% of \nthe total grade of the project.\nConsidering that the number of students who completed the first part of the project \nis extremely large, face-to-face discussions are held gradually, so that all students can \nexpress their views regarding all the topics that were studied. By the time of finishing \nthis paper, a total of 31 students have taken this opportunity.\nStudent participation is presented in Fig. 8. Based on the discussions, which are part of \nthe first project, it is evident that by far the most of students prepared their reports with-\nout relying on the LLM-generated texts. They make up 58.06% of all participants. Exactly \nhalf of that percentage refers to students who used hybrid mode.\nWe took the same approach for the second project. As you can see from Fig.  9, \nalthough there are small differences from the first project, the trends are the same. \nFig. 7 Scatter plots of projects points and their type\nPage 17 of 24\nZdravkova and Ilijoski  Int J Educ Technol High Educ           (2025) 22:32 \n \nStudents who worked without an LLM have the largest share if the overall number of \nstudents is taken into account. What differs from the first project is that if we look rela -\ntively by groups, team leads make the highest percentage of participants in the discus -\nsion. It is actually the largest difference between the first and the second project, 10 \npercentage points.\nFig. 8 Student participation in face-to-face discussions during the first project\nFig. 9 Student participation in face-to-face discussions during the second project\nPage 18 of 24Zdravkova and Ilijoski  Int J Educ Technol High Educ           (2025) 22:32 \nComparison of the quality of briefing report writing with and without LLM \nsupport\nIn order to assess the impact of the LLM on the quality of report writing, we compared \nthe results from the students in the academic year 2023/24 with the previous four years \n(Table 4). The results in the first project were incomparably weaker than those from the \nprevious years and did not reach the threshold of 50 points on average. We estimate that \nthree reasons were decisive: \n1. Students do not have enough experience in research work, so gradually completing \nquizzes in which they show how they reached the final result was a serious problem.\n2. The introduction of AI-generated answers and key information from sources that \nstudents used to create their own reports into quizzes made it much easier to detect \nplagiarism, which had the effect of dramatically reducing grades.\n3. The last, but not least important reason is that students do not have the patience to \nread the instructions on how to create and deliver the assignment, which resulted in \npoorer results.\nFortunately, after the first debacle, the students approached their obligation more \ndiligently. This resulted in an extremely good result that is comparable to the academic \nyear when the course was conducted online in the midst of the COVID- 19 pandemic.\nThe only problem that appeared was the sudden drop out of more than 10% of \nstudents, mostly those who showed poorer results during the first project. Most of them \nshowed weaker results in the first midterm exam too (see Table  2), so they decided to \nconcentrate on the preparation of the second midterm exam and the journal and to do \nthe assignment as a bonus activity only if they did not get enough points to successfully \nfinish the course.\nThe following section is dedicated to the impressions we, as teachers, have about the \nnew approach, contrasted with the students’ impressions and comments. It is crucial for \nthe forthcoming academic year, where the approach will again be carried out, probably \nslightly modified to fulfill students’ expectations.\nTable 4 Time series of the students’ participation and results\n Academic year Enrolled \nstudents\nFirst team project Second team project\nFinished Discussions Average \ngrade\nFinished Discussions Average \ngrade\n2019/20 137 117 7 63 109 12 63\n2020/21 157 142 26 72 138 23 68\n2021/22 299 283 12 62 284 57 60\n2023/23 314 258 85 55 236 94 62\n2023/24 220 181 57 43 158 31 68\nPage 19 of 24\nZdravkova and Ilijoski  Int J Educ Technol High Educ           (2025) 22:32 \n \nImpressions of teachers and students on the integration of LLMs in briefing \nreport writing\nThe integration of large language models within the computer ethics course showed that \nstudents use them with pleasure and massively, in our case for writing their usually first \nresearch papers or briefing reports.\nBy applying quizzes in which students explicitly state how and how much they used \ngenerative AI, check the relevance of the generated answers, and, based on those \nanswers and information from relevant sources that confirm those answers, create \ntheir own task, we believe that we have prevented the misuse of LLMs and enabled all \nstudents to choose the approach that suits them best.\nWhat made a big impression on us was the fact that the majority of students opted for \ntraditional writing, although from the survey we made after the completion of the entire \ncourse, about their impression of the integration of LLMs after, it turned out that they \nused different LLMs in the background as initial inspiration for their work. This allowed \nthem to significantly speed up their initial research without the necessary long search for \ninformation on the web.\nStudents who stated that they used LLMs, preferred to apply the hybrid mode that \ncombines traditional research and writing, and shaping thoughts based on artificial \nintelligence recommendations. It is interesting that, those students who used the hybrid \nmode in the first project, switched to traditional writing in the second one, but even \nmore students decided to completely rely on AI. This was a very interesting turn that we \ndid not expect, primarily because apparently students who showed better results on the \nmidterm exams decided to make that move.\nRegardless of the way of composing the report, based on the solid result in the second \nattempt and the acceptable amount of cheating that we investigated in detail after both \nprojects (Zdravkova & Dalipi, 2023), it gives us an incentive to continue with the same \napproach, slightly modified to overcome the perceived shortcomings. In order to adapt \nit so that it is more intuitive and contributes to better quality of assignments, we also \ninvestigated the feelings of the students in relation to the way in which they should have \nresearched and presented their results. We will present their impressions in more detail \nin the extension of this section.\nBased on the survey we did after the end of the course, which was answered by 112 \nstudents, only 6 stated that they did not use LLMs, compared to 19 who used them every \nday to write their reports. These statements express their main motivation: “It helps \nme get the info I need without me wasting time searching through Google. ”; “I believe \nit’s great to produce a clear text and to reformulate an idea one can have. Mostly when \nEnglish (or other languages) is not our native language. ” , and “I mainly use it by inputting \nfinished blocks of text that I’m not entirely happy with, asking it to rewrite them ’more \nformally’ or just differently. I never find the whole output useful by itself, but there are \nalways some of the generated rephrasing that I like better than my own text. ” All these \nstatements coincide with our impression that LLMs are a very useful starting point for \nwriting.\nExactly one-eighth of the students do not have an opinion on the integration of \ngenerative AI in education; 18.75% are against it, but it is supported by the majority of \n68.75%. One student’s impression, quote, “It’s probably too early to implement, but I \nPage 20 of 24Zdravkova and Ilijoski  Int J Educ Technol High Educ           (2025) 22:32 \nthink when it’s more developed, it could be helpful, and we need to try to stay ahead of \npossible cheaters, ” shows that students are aware of the challenges that LLMs bring with \nthem. On the other hand, one student stated, “My argument against ChatGPT is that it \nwill drain all the creativity of the students. ” We do not agree with this strict assessment, \nand we believe that the channeled use of these tools can, on the contrary, contribute to \ngreater creativity, which our approach enables.\nWhen we asked them if they recommend using LLMs in exams, more than 3 quarters \nof respondents were categorically against it, which we also support. When asked if filling \nout the quizzes was a problem for them, 18.75% stated that they read the quiz several \ntimes in order to understand its essence. Exactly one quarter stated that after reading \nthe accompanying comments in the project evaluation, they realized that they had made \na mistake and that they corrected it in another project. The majority of 28.57% of all \nstudents praised the intuitiveness of the quizzes. When we asked the students after the \nfirst project what the key problem was for them in completing the quizzes, some said \nthat the recommendations were too long and that they did not have the patience to read \nthem. That is why we rewrite the recommendations prior to the second project and \ncorrect the key mistakes made by the students. This obviously helped to improve the \nimplementation of the second assignment.\nIn our opinion, this first attempt to systematically integrate LLM in the preparation \nof reports, stimulating good practice in writing white papers, has been successfully \ncompleted. From the students’ questions, we noticed that a large number of students \nconsulted the LLM not only about the topic they were working on, but also about the \nstructure of their reports. The LLMs helped them prepare the report well and to solve \nthe quiz correctly. Therefore, when defining the project for the next academic year, we \nwill suggest that they do this consultation without fail, in order not to miss the concept \nof preparing white papers.\nIn the conclusion of this paper, we will confirm this once again and announce what we \nplan to modify so that the results of the integration would show an increase in students’ \nknowledge and skills and an improvement in their competencies, which is one of the \nmain goals of the course.\nConclusion\nLarge language models have become an indispensable part of our everyday life, including \nthe academic environment. Unlike before, when the answer to a certain question had \nto be sought by searching a large number of sources, generative artificial intelligence \noffers concise, focused, and reasoned answers in an instant. LLMs have collected all the \nknowledge of the world, and this data is available to the user in the easiest way. They \nadditionally add the ability to justify and even document answers depending on the \nuser’s interests, making it a valuable tool for academic writing.\nOn request, they are able to present multiple points of view on the topic, and in this \nway they encourage critical thinking and prove that there is no single and absolutely \ntrue answer to many questions. LLMs can also help brainstorm topics, easily summa -\nrize research, and paraphrase information gleaned from the Web. As several students \nPage 21 of 24\nZdravkova and Ilijoski  Int J Educ Technol High Educ           (2025) 22:32 \n \nreported, the chatbots helped them with their reports, but also suggested what they \nshould cover in their individual reports, helping them improve their writing skills.\nCertainly, we must not forget about the numerous challenges that the LLMs impose on \neducation. First of all, there are always available paper mills that provide free and hard-\nto-find ghostwriting services to students (Zdravkova, 2016; Zdravkova et al., 2022).\nThe next big problem that hopefully decreases over time is the so-called generative \nAI hallucinating, i.e., answers that are incorrect or irrelevant. It usually stems from the \nlimited subject expertise that is possible in certain areas of computer ethics. Sometimes, \nthe main reason is that the questions posed to LLMs were ambiguous or poorly worded, \nleading to misunderstandings that resulted in irrelevant or confusing answers.\nOccasionally, LLMs cannot understand the specific context of a question or topic, \nwhich leads to answers that are ridiculous and out of touch with the topic they were \nmeant to cover. We noticed such an example with a student who, instead of writing \nabout whistleblowers, wrote about fans whistling at sports competitions, because the \nword whistleblowing in our language is equated with fans whistling.\nHowever, the biggest problem is student plagiarism, especially when students \nappropriate AI-generated content without a proper citation (Zdravkova & Dalipi, \n2023). What we can confirm with certainty is that filling out the quizzes contributed \nsignificantly to its discovery, because either the verbatim transcription of the answers \ngenerated by LLM or the parts of the text derived from the used references are visible \nat a glance. Most of the students who made that mistake were not at all aware that \nsuch an approach was stealing someone’s author’s work, especially because they \nlearned that LLMs did not have the right to copyright, so they believe that it can be \ntaken without responsibility.\nWe are free to believe that our approach contributed to increasing the advantages \nof new technological gadgets and, at the same time, reduced the risks to academic \nintegrity. These are the arguments with which we confirm it.\nThe obligation to list up to three research questions and questions students ask \nLLMs in all the quizzes, helps produce a report that is thoughtful and covers only \nthe key arguments. Thanks to them, students are more focused and do not engage in \nunnecessary research on topics that are insignificant or far from the topic they need \nto cover.\nBy checking the relevance of the answers offered by the AI, they avoid having their \nreport based on incorrect and even hallucinogenic information.\nBy copying the answers and/or the most important parts of the consulted sources, it \nsignificantly facilitates the verification of academic dishonesty, especially plagiarism, \nwhich was most often the result of verbatim copying and sometimes paraphrasing, as \nwell as cross-language cheating from a language that is different from English.\nStructured report writing, which is one of the basic obligations of good white \npapers, is made possible by the quiz cells, which include an introduction, then an \nelaboration of the topic, and finally a conclusion and one’s own position.\nThe quizzes end with a list of references that encourage students to support \ntheir arguments with examples and to compare them with relevant sources. In this \nway, writing briefing reports based on common sense and sometimes incorrect \ninformation, which students tend to apply, is avoided.\nPage 22 of 24Zdravkova and Ilijoski  Int J Educ Technol High Educ           (2025) 22:32 \nWith all these requests, we have achieved the goal of helping to create high-quality \nand fair white papers, so we plan to repeat them in the next academic year as well.\nThe students’ reactions were primarily related to the length of the explanation of \nhow to solve the quiz. They are neither ready to read instructions that are more than \na hundred characters long nor want to watch demos that explain the work process. \nFor this reason, we will write a report explaining what writing a research paper is, \nand in class we will show them live how to enter it into the hybrid mode quiz. If, after \nthe first presentation of what is expected of them, the students say that they do not \nunderstand, we will gradually repeat the procedure, responding to their dilemmas.\nWe also indroduce one modification. In order to increase active participation in \npublic discussions, we intend to increase their weight to at least 30%. We hope that it \nwill stimulate the interest to discuss, which is, according to our impressions, equally \nimportant as the preparation of the reports. If the student is able to competently \ndiscuss a topic and defend his own views on it, even if someone else wrote the report \n(a human ghostwriter or AI), the goal is achieved: the student has learned what the \nghostwriters wrote, and the knowledge on that topic has increased.\nThe integration of large language models is a major step forward in modern education \nand will soon become mainstream, primarily in universities, which are pillars of \nprogress. We believe that the quizzes we have presented can be easily adapted and \napplied in various educational settings, from small seminars to courses attended by \nhundreds of students. They can be customized to be applied in different disciplines, not \nonly in computer ethics where writing reports is the main goal. We have easily integrated \nthem into Moodle, but they can be easily embedded in all learning management systems. \nTraining on the approach is not necessary because they are extremely intuitive. Most \nimportantly, the approach is extremely flexible. We made it more flexible as we moved \nfrom the first to the second project because we realized that organizational mistakes \nadversely affected student success.\nAcknowledgements\nWe acknowledge those enrolled students who actively participated in the assignment presentation and in the survey.\nAuthor contributions\nconceptualization, K.Z.; methodology, K.Z. and B.I.; formal analysis, K.Z. and B.I.; investigation, K.Z. and B.I.; resources, K.Z. \nand B.I.; data collection, K.Z. and B.I.; writing—original draft preparation, K.Z. and B.I.; writing—review and editing, K.Z. \nand B.I.; visualization, B.I. and K.Z.; supervision, K.Z. All authors have read and agreed to the published version of the \nmanuscript.\nFunding\nThis work was partially supported by the Faculty of Computer Science and Engineering, University “Ss. Cyril and \nMethodius” , Skopje, Republic of North Macedonia.\nData availability\nThe datasets used and/or analyzed during the current study are available from the corresponding author on reasonable \nrequest.\nDeclarations\nCompeting interests\nThe authors declare that they have no competing interests\nReceived: 1 October 2024   Accepted: 27 March 2025\n\nPage 23 of 24\nZdravkova and Ilijoski  Int J Educ Technol High Educ           (2025) 22:32 \n \nReferences\nAcm, T., & Joint, I.-C. (2023). Task Force on Computing Curricula (Computer Science Curricula, 2023). https:// csed. acm. org/ \nwp- conte nt/ uploa ds/ 2023/ 03/ Versi on- Beta- v2. pdf.\nAllwood, J. M., Ashby, M. F., Gutowski, T. G., & Worrell, E. (2011). Material efficiency: A white paper. Resources, Conservation \nand Recycling, 55, 362–381.\nArista, A., Shuib, L., & Ismail, M. A. (2023). An overview chatgpt in higher education in indonesia and malaysia.\nArtificial Intelligence in Science: Challenges, Opportunities and the Future of Research (OECD, 2023). https:// doi. org/ 10. \n1787/ a8d82 0bd- en\nBailey, S. (2014). Academic writing: A handbook for international students. Routledge.\nBinkley, M., et al. (2010). Draft white paper 1: Defining 21st century skills.\nBuruk, O. O. (2023). Academic writing with gpt-3.5 (chatgpt): Reflections on practices, efficacy and transparency. https:// \ndoi. org/ 10. 1145/ 36169 61. 36169 92\nCarter, L. (2011). Ideas for adding soft skills education to service learning and capstone courses for computer science \nstudents\nCastro, L. M. C., Castelblanco, G., & Antonenko, P . D. (2024). Llm-based system for technical writing real-time review in \nurban construction and technology. EPiC Series in Built Environment. https:// api. seman ticsc holar. org/ Corpu sID: 27006 \n0404\nChatterjee, S., Bhattacharya, M., Pal, S., Lee, S.-S., & Chakraborty, C. (2023). Chatgpt and large language models in orthope-\ndics: From education and surgery to research. Journal of Experimental Orthopaedics, 10, 128. https:// doi. org/ 10. 1186/ \ns40634- 023- 00700-1\nDiab Idris, M., Feng, X., & Dyo, V. (2024). Revolutionizing higher education: Unleashing the potential of large language \nmodels for strategic transformation. IEEE Access, 12, 67738–67757.\nDing, L., Li, T., Jiang, S., & Gapud, A. (2023). Students’ perceptions of using chatgpt in a physics class as a virtual tutor. Inter-\nnational Journal of Educational Technology in Higher Education, 20, 63. https:// doi. org/ 10. 1186/ s41239- 023- 00434-1\nEllis, M., Kisling, E., & Hackworth, R. G. (2014). Teaching soft skills employers need. Community College Journal of Research \nand Practice, 38, 433–453.\nFlorysiak, D., & Schandlbauer, A. (2019). The information content of ico white papers.\nFroyd, J. E. (2008). White paper on promising practices in undergraduate stem education. commissioned paper for the \nevidence on promising practices in undergraduate science, technology, engineering, and mathematics (stem) \neducation project, the national academies board on science education.\nGan, W., Qi, Z., Wu, J., & Lin, J. (2023). Large language models in education: Vision and opportunities. https:// doi. ieeec \nomput ersoc iety. org/ 10. 1109/ BigDa ta590 44. 2023. 10386 291\nG’omez-Rodr’iguez, C. & Williams, P . A confederacy of models: a comprehensive evaluation of llms on creative writing. \nArXiv: abs/ 2310. 08433 (2023). https:// api. seman ticsc holar. org/ Corpu sID: 26390 8973\nGraham, G. (2013).White papers for dummies. John Wiley and Sons.\nGraham, G. (2024). https:// thatw hitep aperg uy. com/ white- paper- faq/# what_ is. White Paper FAQ (Frequently Asked \nQuestions).\nGroeneveld, W., Becker, B. A., & Vennekens, J. (2020). Soft skills: What do computing program syllabi reveal about non-\ntechnical expectations of undergraduate students?\nHan, J., et al. (2023) Recipe: How to integrate chatgpt into efl writing education. https:// doi. org/ 10. 1145/ 35730 51. 35962 \n00\nHan, J. et al. (eds) LLM-as-a-tutor in EFL writing education: Focusing on evaluation of student-LLM interaction. (eds \nKumar, S. et al.) Proceedings of the 1st Workshop on Customizable NLP: Progress and Challenges in Customizing \nNLP for a Domain, Application, Group, or Individual (CustomNLP4U), 284–293 (Association for Computational \nLinguistics, Miami, Florida, USA, 2024). https:// aclan tholo gy. org/ 2024. custo mnlp4u- 1. 21\nhttps:// www. oecd. org/ en/ topics/ policy- issues/ artifi  cial- intel ligen ce. html\nHuang, B., Chen, C. & Shu, K. (2024). Authorship attribution in the era of llms: Problems, methodologies, and challenges. \narXiv: https:// arxiv. org/ abs/ 2408. 08946\nHuber, S. E., et al. (2024). Leveraging the potential of large language models in education through playful and game-\nbased learning. Educational Psychology Review, 36, 25. https:// doi. org/ 10. 1007/ s10648- 024- 09868-z\nIeee-Cs, T., & Task, A. J. (2001). Force on computing curricula.Computing Curricula. https:// www. acm. org/ binar ies/ conte nt/ \nassets/ educa tion/ curri cula- recom menda tions/ cc2001. pdf.\nJackson, V. P ., et al. (2009). Radpeer™ scoring white paper. Journal of the American College of Radiology, 6, 21–25.\n(Jess), J. L. (2024). A critical review of genai policies in higher education assessment: a call to reconsider the “originality” \nof students’ work. Assessment & Evaluation in Higher Education 49, 651–664. https:// doi. org/ 10. 1080/ 02602 938. 2024. \n23099 63\nKellogg, R. T., & Raulerson, B. A. (2007). Improving the writing skills of college students. Psychonomic Bulletin and Review, \n14, 237–242.\nKostic, M., Witschel, H. F., Hinkelmann, K., & Spahic-Bogdanovic, M. (2024). Llms in automated essay evaluation: A case \nstudy. Proceedings of the AAAI Symposium Series, 3, 143–147. https:// doi. org/ 10. 1609/ aaaiss. v3i1. 31193\nLaato, S., Morschheuser, B., Hamari, J., & Björne, J. (2023). Ai-assisted learning with chatgpt and large language models: \nImplications for higher education.\nLabadze, L., Grigolia, M., & Machaidze, L. (2023). Role of ai chatbots in education: Systematic literature review. International \nJournal of Educational Technology in Higher Education, 20, 56. https:// doi. org/ 10. 1186/ s41239- 023- 00426-1\nLee, A. V. Y., Tan, S. C., & Teo, C. L. (2023). Designs and practices using generative ai for sustainable student discourse and \nknowledge creation. Smart Learning Environments, 10, 59. https:// doi. org/ 10. 1186/ s40561- 023- 00279-1\nLiang, W., et al. (2024). Mapping the increasing use of LLMs in scientific papers. ArXiv.\nLiang, Y., Zou, D., Xie, H., & Wang, F. L. (2023). Exploring the potential of using chatgpt in physics education. Smart Learning \nEnvironments, 10, 52. https:// doi. org/ 10. 1186/ s40561- 023- 00273-7\nLi, Q. et al. (2024). Adapting large language models for education: Foundational capabilities, potentials, and challenges. \narXiV: https:// arxiv. org/ abs/ 2401. 08664\nPage 24 of 24Zdravkova and Ilijoski  Int J Educ Technol High Educ           (2025) 22:32 \nLombard, A. (2008). The implementation of the white paper for social welfare: a ten-year review.\nMalone, E. A., & Wright, D. (2018). To promote that demand. Toward a history of the marketing white paper as a genre. \nJournal of Business and Technical Communication, 32, 113–147.\nMartínez-Téllez, R., & Camacho-Zuñiga, C. (2023). Enhancing mathematics education through ai chatbots in a flipped \nlearning environment.\nMeyer, J. G. et al. (2023). Chatgpt and large language models in academia: opportunities and challenges. BioData Mining \n16. https:// doi. org/ 10. 1186/ s13040- 023- 00339-9\nNam, B. H., & Bai, Q. (2023). Chatgpt and its ethical implications for stem research and higher education: A media dis-\ncourse analysis. International Journal of STEM Education, 10, 66. https:// doi. org/ 10. 1186/ s40594- 023- 00452-5\nNyce, C., & Cpcu, A. (2007). Predictive analytics white paper. American Institute for CPCU. Insurance Institute of America 9–10.\nPetric, B. (2002). Students’ attitudes towards writing and the development of academic writing skills. The Writing Center \nJournal, 22, 9–27.\nPhutela, N., Grover, P ., Singh, P ., & Mittal, N. (2024). Future prospects of chatgpt in higher education.\nQadir, J. (2023). Engineering education in the era of chatgpt: Promise and pitfalls of generative ai for education.\nRavšelj, D., et al. (2025). Higher education students’ perceptions of chatgpt: A global study of early reactions. PLOS ONE, \n20, e0315011. https:// doi. org/ 10. 1371/ journ al. pone. 03150 11\nRenn, O. (2009). White Paper on risk governance: Towards and integrative approach (International Risk Governance Council \n(IRGC, 2009).\nSafran, C., et al. (2007). Toward a national framework for the secondary use of health data: An american medical informat-\nics association white paper. Journal of the American Medical Informatics Association, 14, 1–9.\nScaffidi, C. (2018). Employers’ needs for computer science, information technology and software engineering skills \namong new graduates. International Journal of Computer Science, Engineering and Information Technology, 8, 1–12.\nShepard, L., Hannaway, J., & Baker, E. (2009). Standards, assessments, and accountability. Education policy white paper. \nNational academy of education (nj1).\nTrends Shaping Education 2025 (OECD Publishing, 2025). https:// doi. org/ 10. 1787/ ee658 7fd- en.\nVogler, J. S., et al. (2018). The hard work of soft skills: Augmenting the project-based learning experience with interdiscipli-\nnary teamwork. Instructional Science, 46, 457–488.\nWalter, Y. (2024). Embracing the future of artificial intelligence in the classroom: The relevance of ai literacy, prompt \nengineering, and critical thinking in modern education. International Journal of Educational Technology in Higher \nEducation, 21, 15. https:// doi. org/ 10. 1186/ s41239- 024- 00448-3\nWang, H., Dang, A., Wu, Z. & Mac, S. Generative ai in higher education: Seeing chatgpt through universities’ policies, \nresources, and guidelines. Computers and Education: Artificial Intelligence 7, 100326. https:// www. scien cedir ect. com/ \nscien ce/ artic le/ pii/ S2666 920X2 40012 92\nWang, S. et al. (2024). Large language models for education: A survey and outlook. arXiv: https:// arxiv. org/ abs/ 2403. 18105\nWhite, S. S., & Ellis, C. (2007). Sustainability, the environment, and new urbanism: An assessment and agenda for research. \nJournal of Architectural and Planning Research 125–142.\nWilson, S. (2009). Teacher quality. Education policy white paper. National academy of education (nj1).\nXiao, C., Xu, S. X., Zhang, K., Wang, Y., & Xia, L. Kochmar, E. et al. (eds) Evaluating reading comprehension exercises generated \nby LLMs: A showcase of ChatGPT in education applications. (eds Kochmar, E. et al.) Proceedings of the 18th Workshop \non Innovative Use of NLP for Building Educational Applications (BEA 2023), 610–625 (Association for Computational \nLinguistics, Toronto, Canada, 2023). https:// aclan tholo gy. org/ 2023. bea-1. 52\nXu, H., Gan, W., Qi, Z., Wu, J., & Yu, P . S. (2024). Large language models for education: A survey. arXiv: https:// arxiv. org/ abs/ \n2405. 13001\nYa-Ping Hsiao, N. K. & Chiu, M.-S. (2023). Developing a framework to re-design writing assignment assessment for the era \nof large language models. Learning: Research and Practice 9.\nYuan, A., Coenen, A., Reif, E., & Ippolito, D. (2022). Wordcraft: story writing with large language models.\nZdravkova, K. (2016). Reinforcing social media based learning, knowledge acquisition and learning evaluation. Procedia-\nSocial and Behavioral Sciences, 228, 16–23.\nZdravkova, K., & Dalipi, F., & Ahlgren, F. (2023). Integration of large language models into higher education: A perspective \nfrom learners.\nZdravkova, K., Dalipi, F., Ahlgren, F., Ilijoski, B., & Olsson, T. (2024). Unveiling the impact of large language models on \nstudent learning: A comprehensive case study.\nZdravkova, K., Ilijoski, B. Bădică, C. et al. (eds) Preventing academic dishonesty originating from large language models. (eds \nBădică, C. et al.) Advances in ICT Research in the Balkans, 118–132 (Springer Nature Switzerland, Cham, 2025). https:// \ndoi. org/ 10. 1007/ 978-3- 031- 84093-7_9\nZdravkova, K., & Basnarkov, L. (eds). (2022). Adapting a web 2.0-based course to a fully online course and readapting it back \nfor face-to-face use. (eds Zdravkova, K. & Basnarkov, L.) ICT Innovations. Reshaping the Future Towards a New Normal, \n135–146 (Springer Nature Switzerland, Cham, 2022).\nZlotnikova, I., Hlomani, H., Mokgetse, T., & Bagai, K. (2025). Establishing ethical standards for genai in university education: \na roadmap for academic integrity and fairness. Journal of Information, Communication and Ethics in Society. https:// \ndoi. org/ 10. 1108/ JICES- 07- 2024- 0104\nZohery, M. (2023). Chapter 2: ChatGPT in Academic Writing and Publishing: A Comprehensive Guide, 10–61.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Higher education",
  "concepts": [
    {
      "name": "Higher education",
      "score": 0.5393410325050354
    },
    {
      "name": "Mathematics education",
      "score": 0.5174658894538879
    },
    {
      "name": "Computer science",
      "score": 0.5095913410186768
    },
    {
      "name": "Psychology",
      "score": 0.2863272726535797
    },
    {
      "name": "Political science",
      "score": 0.137180358171463
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": []
}