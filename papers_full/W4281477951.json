{
  "title": "Retrieval-Augmented Multilingual Keyphrase Generation with Retriever-Generator Iterative Training",
  "url": "https://openalex.org/W4281477951",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2110436783",
      "name": "Yifan Gao",
      "affiliations": [
        null,
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A2364554682",
      "name": "Qingyu Yin",
      "affiliations": [
        "University of Pittsburgh",
        null
      ]
    },
    {
      "id": "https://openalex.org/A2107695934",
      "name": "Zheng Li",
      "affiliations": [
        "University of Pittsburgh",
        null
      ]
    },
    {
      "id": "https://openalex.org/A2151369885",
      "name": "Rui Meng",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1980992600",
      "name": "Tong Zhao",
      "affiliations": [
        "University of Pittsburgh",
        null
      ]
    },
    {
      "id": "https://openalex.org/A2104414332",
      "name": "Bing Yin",
      "affiliations": [
        null,
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A2121363826",
      "name": "Irwin King",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2183402424",
      "name": "Michael Lyu",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2949877232",
    "https://openalex.org/W3169283738",
    "https://openalex.org/W3170614090",
    "https://openalex.org/W2888766462",
    "https://openalex.org/W3167228455",
    "https://openalex.org/W2963531963",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2963275829",
    "https://openalex.org/W1525595230",
    "https://openalex.org/W1932742904",
    "https://openalex.org/W1975432235",
    "https://openalex.org/W3176913643",
    "https://openalex.org/W2996726036",
    "https://openalex.org/W4299574851",
    "https://openalex.org/W3156568426",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3006365540",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W3200871535",
    "https://openalex.org/W3105075472",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W2136075087",
    "https://openalex.org/W2963265326",
    "https://openalex.org/W2740811004",
    "https://openalex.org/W2963245897",
    "https://openalex.org/W2115023510",
    "https://openalex.org/W2932847124",
    "https://openalex.org/W3152740956",
    "https://openalex.org/W3117034213",
    "https://openalex.org/W3035328829",
    "https://openalex.org/W2885421725",
    "https://openalex.org/W2973226110",
    "https://openalex.org/W2526059794",
    "https://openalex.org/W3211683577",
    "https://openalex.org/W2788330850",
    "https://openalex.org/W2129557600",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W95207536",
    "https://openalex.org/W2251009376",
    "https://openalex.org/W4297805475",
    "https://openalex.org/W2949963192",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2061769355",
    "https://openalex.org/W3001434439",
    "https://openalex.org/W2949647400",
    "https://openalex.org/W2593560537",
    "https://openalex.org/W2060772621",
    "https://openalex.org/W3173799534",
    "https://openalex.org/W4206039245",
    "https://openalex.org/W3173180518",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3007672467"
  ],
  "abstract": "Keyphrase generation is the task of automatically predicting keyphrases given a piece of long text. Despite its recent flourishing, keyphrase generation on non-English languages haven't been vastly investigated. In this paper, we call attention to a new setting named multilingual keyphrase generation and we contribute two new datasets, EcommerceMKP and AcademicMKP, covering six languages. Technically, we propose a retrieval-augmented method for multilingual keyphrase generation to mitigate the data shortage problem in non-English languages. The retrieval-augmented model leverages keyphrase annotations in English datasets to facilitate generating keyphrases in low-resource languages. Given a non-English passage, a cross-lingual dense passage retrieval module finds relevant English passages. Then the associated English keyphrases serve as external knowledge for keyphrase generation in the current language. Moreover, we develop a retriever-generator iterative training algorithm to mine pseudo parallel passage pairs to strengthen the cross-lingual passage retriever. Comprehensive experiments and ablations show that the proposed approach outperforms all baselines.",
  "full_text": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 1233 - 1246\nJuly 10-15, 2022 ¬©2022 Association for Computational Linguistics\nRetrieval-Augmented Multilingual Keyphrase Generation with\nRetriever-Generator Iterative Training‚àó\nYifan Gao1, Qingyu Yin1‚Ä°, Zheng Li1‚Ä°, Rui Meng2,\nTong Zhao1, Bing Yin1, Irwin King3, Michael R. Lyu3\n1Amazon Inc. 2 Salesforce Research 3 Chinese University of Hong Kong\n1{yifangao,qingyy,amzzhe}@amazon.com 2ruimeng@salesforce.com\nAbstract\nKeyphrase generation is the task of automat-\nically predicting keyphrases given a piece\nof long text. Despite its recent flourish-\ning, keyphrase generation on non-English lan-\nguages haven‚Äôt been vastly investigated. In\nthis paper, we call attention to a new set-\nting named multilingual keyphrase generation\nand we contribute two new datasets, Ecom-\nmerceMKP and AcademicMKP, covering six\nlanguages. Technically, we propose a retrieval-\naugmented method for multilingual keyphrase\ngeneration to mitigate the data shortage prob-\nlem in non-English languages. The retrieval-\naugmented model leverages keyphrase anno-\ntations in English datasets to facilitate gener-\nating keyphrases in low-resource languages.\nGiven a non-English passage, a cross-lingual\ndense passage retrieval module finds relevant\nEnglish passages. Then the associated English\nkeyphrases serve as external knowledge for\nkeyphrase generation in the current language.\nMoreover, we develop a retriever-generator iter-\native training algorithm to mine pseudo parallel\npassage pairs to strengthen the cross-lingual\npassage retriever. Comprehensive experiments\nand ablations show that the proposed approach\noutperforms all baselines.1\n1 Introduction\nKeyphrases are single or multi-word lexical units\nthat best summarize a piece of text. As such, they\nare of great importance for indexing, categorizing,\nand mining in many information retrieval and natu-\nral language processing tasks (Jones and Staveley,\n1999; Frank et al., 1999; Hulth and Megyesi, 2006;\nDave et al., 2003). Keyphrase generation is the\ntask of automatically predicting keyphrases given\n‚àó Qingyu Yin and Zheng Li are corresponding authors.\nThis work was mainly done while Rui Meng was a Ph.D.\nstudent at University of Pittsburgh.\n1The datasets are released at https://github.\ncom/Yifan-Gao/multilingual_keyphrase_\ngeneration.\na piece of long text. Existing works on keyphrase\ngeneration mostly focus on English datasets (Gal-\nlina et al., 2019; Meng et al., 2017) while keyphrase\ngeneration for languages other than English is\nstill under-explored. Since search engines usu-\nally provide services to customers using different\nlanguages, multilingual keyphrase generation be-\ncomes a significant problem while it is still un-\nknown how well existing keyphrase generation ap-\nproaches perform in non-English languages.\nNevertheless, there are two challenges we will\nface regarding multilingual keyphrase generation.\nFirst, to the best of our knowledge, there is no large-\nscale dataset publicly available for training and\nbenchmarking multilingual keyphrase generation\nmodels. Building keyphrase datasets at a sufficient\nscale is difficult and costly. Second, compared with\nthe existing datasets in English, which can contain\nmillions of data examples and cover a wide diver-\nsity of topics, the data resources in non-English\nlanguages are inherently scarce. For example, in\nthe domain of e-commerce, marketplaces using En-\nglish have abundant customer queries to be used for\nkeyphrase mining, while queries in some languages\nare relatively less than in English, which is proba-\nbly because of a smaller size of user population or\na shorter operation time.\nWe start tackling these challenges by contribut-\ning two new datasets for multilingual keyphrase,\nwhich cover six languages and two domains. The\nfirst dataset EcommerceMKP is collected from a\nreal-world major e-commerce website. The prod-\nuct descriptions are used as the source text while\nthe target keyphrases are collected from user search\nqueries. This dataset contains a total of 73k data ex-\namples, covering four different languages (Spanish,\nGerman, Italian and French). The second multi-\nlingual keyphrase dataset AcademicMKP lies in\nthe academic domain, in which titles and abstracts\nare used as the source text and the author-provided\nkeyphrases are deemed targets. A total of 2,693 aca-\n1233\ndemic papers in Chinese and Korean are included\nin AcademicMKP.\nTo overcome the resource scarcity challenge\nin training multilingual models, we propose a\nretrieval-based method to leverage the keyphrase\nknowledge in large-scale English datasets. By in-\nvestigating multilingual keyphrase data, we ob-\nserve that data in different languages may talk\nabout similar topics. Therefore, we conjecture\nthat passage-keyphrases pairs in English can be\nof help as an external knowledge base for multilin-\ngual keyphrase generation. To be specific, given a\npassage in low-resource language XX, we propose\nto use a retrieval model to find multiple top-related\npassages in English. These retrieved English pas-\nsages provide high-quality English keyphrases that\ncan be used as hints for generating keyphrases in\nother languages. After that, the generator takes the\ncode-mixed inputs, including the passage in lan-\nguage XX and retrieved English keyphrases, and\npredicts keyphrases in language XX.\nIn the cross-lingual retrieval training, parallel\npassage-keyphrases pairs between English and\nother languages are extremely limited. For ex-\nample, in the e-commerce domain, only a small\nfraction of products have both English and non-\nEnglish descriptions (being sold in multiple coun-\ntries). Such a data scarcity issue weakens the abil-\nity of cross-lingual knowledge acquisition from\nhigh-resource English keyphrases as intermediary,\nand finally hinders the potential of the retrieval-\naugmented keyphrase generation. To mitigate the\nproblem, we propose a retriever-generator iterative\ntraining (RGIT) algorithm to automatically mine\npseudo training parallel pairs from unlabeled data.\nConcretely, the retriever can dynamically adjust in\nterms of the current variations of generation perfor-\nmance between the proposed retrieval-augmented\ngenerator and the base one without the aid of re-\ntrieved English keyphrases. Starting from insuf-\nficient seed parallel pairs, if the retrieved pseudo\npassage-keyphrases pairs in the current iteration\ncan bring in higher generation results as the gener-\nator‚Äôs feedback, those pseudo parallel data will be\nregarded as high quality and incorporated into the\nseed ones to further boost the retriever. Such cycle\nproviding positive effects can be repeated until the\nincreasing generation performance stopped.\nWe conduct extensive experiments on Ecom-\nmerceMKP and AcademicMKP and demonstrate\nthat large-scale English datasets do provide use-\nful knowledge for multilingual keyphrase genera-\ntion. The proposed retrieval-augmented method\noutperforms traditional extraction-based models,\nsequence-to-sequence neural models, and its vari-\nants. Moreover, the RGIT algorithm boosts the\nretrieval performance significantly by mining over\n20k pseudo-parallel passage pairs. We also conduct\ndetailed analyses to investigate the effectiveness of\nretriever-generator iterative training.\n2 Related Work\nKeyphrase Generation. The advance of neural\nlanguage generation enables models to freely gen-\nerate keyphrases according to the phrase impor-\ntance and semantics, rather than extracting a list of\nsub-strings from the text (Witten et al., 1999a; Liu\net al., 2011; Wang et al., 2016). Meng et al. (2017)\npropose the first keyphrase generation model Copy-\nRNN, which not only generates words based on a\nvocabulary but also points to words in the source\ntext ‚Äî overcoming the barrier of predicting absent\nkeyphrases. Following this idea, Chen et al. (2018);\nZhao and Zhang (2019); Ahmad et al. (2021) lever-\nage the attention mechanism to reduce duplication\nand improve coverage. Chen et al. (2019b); Ye\nand Wang (2018); Wang et al. (2019); Liang et al.\n(2021) propose to leverage extra structure informa-\ntion (e.g., title, topic) to guide the generation. Chan\net al. (2019); Luo et al. (2021) propose a model\nusing reinforcement learning, and Swaminathan\net al. (2020) propose using GAN for KPG. Chen\net al. (2020) introduce hierarchical decoding and\nexclusion mechanism to prevent models from gen-\nerating duplicate phrases. Ye et al. (2021b) propose\nto dynamically align target phrases to eliminate the\ninfluence of order, as highlighted by Meng et al.\n(2021). Mu et al. (2020); Liu et al. (2020a); Park\nand Caragea (2020) use pre-trained language mod-\nels for better representations of documents.\nRetrieval Augmented Text Generation (RAG)\nrecently shows great power in knowledge-intensive\nNLP tasks such as open-domain question answer-\ning, fact checking and entity linking (Lewis et al.,\n2020; Petroni et al., 2021; Guu et al., 2020). In\nRAG, a retriever (either sparse (Lee et al., 2019) or\ndense (Karpukhin et al., 2020)) searches for use-\nful non-parametric knowledge from a knowledge\nbase, then a generator combines the non-parametric\nretrieved knowledge with its parametric knowl-\nedge, learned during pre-training, for solving the\ntask. Different from these tasks, keyphrase gen-\n1234\nExternal Knowledge (English Passage-Keyphrases Pairs)(  ùëù!\"#,  {ùëò!,!\"#,‚Ä¶,ùëò!,%!\"#})(  ùëù&\"#,  {ùëò&,!\"#,‚Ä¶,ùëò&,%\"\"#})\n(  ùëù'\"#,  {ùëò',!\"#,‚Ä¶,ùëò',%#\"#})... {ùëò(!,!\"#,,‚Ä¶,ùëò(!,%$!\"#,}\n{ùëò(),!\"#,‚Ä¶,ùëò(),%$%\"#}...{ùëò(&,!\"#,,‚Ä¶,ùëò(&,%$\"\"#,}Find associated keyphrases\nPassage ùëù!! Retriever\nGenerator\nTop ùëöEN Passages ùëù(!\"#,ùëù(&\"#,‚Ä¶,ùëù()\"#\nKeyphrasesùëò!**,ùëò&**,..., ùëò%**\nPassage ùëù!!\nCross-Lingual Dense Passage RetrievalMultilingual Keyphrase Generation with Code-Mixed Inputs\nFigure 1: Overview of our Retrieval-Augmented Multilingual Keyphrase Generation (RAMKG) framework.pXX,kXX\ni\ndenote a passage and keyphrases in language XX (XX ‚àà{ DE, ES, FR, IT, KO, ZH }). pEN,kEN\ni denote relevant\npassages and keyphrases retrieved from the English dataset.\neration is not a knowledge-intensive task but we\ntreat the English passage-keyphrase training data as\nour knowledge. Similar approaches have been in-\nvestigated in neural machine translation (Gu et al.,\n2018; Cai et al., 2021), dialogue (Weston et al.,\n2018), and knowledge-base QA (Das et al., 2021).\nIn keyphrase generation, Chen et al. (2019a); Ye\net al. (2021a); Kim et al. (2021) retrieve similar\ndocuments from training data to produce more ac-\ncurate keyphrases. However, their retrieval module\nis a non-parametric model and cannot be gener-\nalized in the multilingual setting due to the large\nvocabulary gap between languages.\n3 Task Definition\nIn this paper, we aim to tackle the keyphrase genera-\ntion task in a multilingual setting, which means one\nmodel of desire is capable of generating keyphrases\nin any language that it has been trained with. The\nbenefits of having a single keyphrase generation\nmodel for multiple languages are threefold: (1)\nCollecting keyphrase annotation for individual lan-\nguage can be prohibitively expensive; (2) Training\nand deploying separate models for each language is\nlaborious; (3) Joint training of multiple languages\ncan alleviate the resource scarcity by utilizing rich\nmonolingual data.\nFormally, we define the multilingual keyphrase\ngeneration task as follows. Given a piece of text\npXX in language XX, our goal is to predict its\ncorresponding keyphrases kXX\n1 ,kXX\n2 ,...,k XX\nn in lan-\nguage XX, where n is the total number of target\nkeyphrases for this text pXX. In this study, XX\ncan be German ( DE), Spanish (ES), Italian (IT),\nFrench (FR), Korean (KO) or Chinese (ZH).\n4 Model\nScarcity of resources is one of the topmost chal-\nlenges for multilingual tasks, which is also the case\nfor multilingual keyphrase generation. One may\nfind it difficult to collect enough text data in lan-\nguages other than English, much less the annotation\nof keyphrases in specific domains. To overcome\nthis problem, we propose a retrieval-augmented\napproach to make use of the relatively rich re-\nsources in English. The motivation for our pro-\nposed retrieval-augmented approach comes from\nan observation from data: texts and keyphrases\nexpressed in different languages usually share com-\nmon topics or knowledge concepts. For exam-\nple, in e-commerce websites, it is often the case\nthat the same products are sold in different mar-\nketplaces/countries. Thus these products as well\nas their keyphrases, though exhibited in different\nlanguages, can share a high semantic similarity.\nIn other words, given a text in language pAA, if\nwe could find a similar text in language pBB, its\nassociated keyphrases kBB may serve as a good\nhint for the to-be-generated keyphrases kAA in lan-\nguage AA. Since English has the most abundant\ntext-keyphrases pairs in both e-commerce and aca-\ndemic papers domains, its resource can be treated\nas a non-parametric keyphrase knowledge base,\nwhich provides texts in English covering a wide\nrange of topics and concepts, as well as the associ-\nated high-quality keyphrases.\nAs shown in Fig. 1, our framework consists of a\nretrieval step and a generation step:\n1. Retrieval Step: given a source passage pXX in\nlanguage XX, the cross-lingual retriever first\nfinds m semantically relevant English pas-\nsages pEN\n1 ,pEN\n2 ,...,p EN\nm. Each retrieved En-\n1235\nCross-lingual\tDense\tPassage\tRetrieverD!\"#\nùêÜ$\nRetrieval-Augmented\tMultilingual\tKeyphraseGenerator\t(RAMKG)\n(Passage, Keyphrase)DE       FR        ES         IT\nùêÜ%Parallel (Passage, Keyphrase)DE       FR        ES         ITRetrieved KeyphraseEN\nNon-Parallel Passage ùëù&!''DE    FR     ES      IT(Passage ÃÇùëù()*&, Keyphrase)EN ùêÜ% Keyphrase{&ùëòùêÜ!''}DE  FR   ES   ITKeyphrase{&ùëòùêÜ\"''}DE  FR   ES   IT\nD!)*,-.%/0\nF1({&ùëòùêÜ!''})‚àíF1({&ùëòùêÜ\"''}) > ùúè?+(ùëù#$%%,ÃÇùëù&'(#)\nTrain: Inference: \nTraining Create Pseudo Labels\nBase Generator(mBART)\n(Iteration\tt)D$'()*+,\nOriginal\tPARallelPassages(limited)PSEUDOParallel\tPassages(Iteration\tt)\nOverall Flow: Iterative Training1 2 3Pseudo Parallel Data1 2\n3 Base GeneratorùêÜ$\nRetriever Generatorùêë%\nRetrieverùêë% RAMKGGenerator\nRetrieve EN PassageGeneration\n+Large Scale EN Dataùëù()*&EN\nFigure 2: Retriever-Generator Iterative Training for Parallel Passage Mining\nglish passage pEN\nj has its associated nj English\nkeyphrases kEN\nj,1, kEN\nj,2, ..., kEN\nj,nj . These retrieved\nEnglish keyphrases are taken as external knowl-\nedge for keyphrase generation in step 2.\n2. Generation Step: taking the source text pXX\nin language XX and all retrieved English\nkeyphrases { kEN\n1,1, ..., kEN\n1,n1 }, ..., { kEN\nm,1, ...,\nkEN\nm,nm} as inputs, the generation module con-\ncatenates them as a sequence and generates\nkeyphrases in target language XX.\n4.1 Cross-Lingual Dense Passage Retrieval\nThe cross-lingual retriever includes a passage en-\ncoder EP(¬∑) and a query encoder EQ(¬∑). The pas-\nsage encoder EP(¬∑) maps millions of English pas-\nsages into d-dimensional vectors and builds indices\nfor all English passages using FAISS (Johnson\net al., 2021) offline. At inference time, the pas-\nsage in language XX goes through the query en-\ncoder EQ(¬∑) and is converted into a d-dimensional\nvector. Then the cross-lingual retriever performs a\nKNN search to retrieve mEnglish passages whose\nvectors are closest to the query vector measured\nby the dot product similarity: sim(pXX,pENj ) =\nEQ(pXX)‚ä§EP(pEN\nj ).\nPassage Encoder. Since naive lexical similar-\nity can hardly handle text matching across lan-\nguages, we resort to a BERT-based dense pas-\nsage retriever (Karpukhin et al., 2020), expecting\nthe contextualized semantic matching can retrieve\nsimilar passages accurately and robustly. In or-\nder to meet the demand of multilingual represen-\ntation, we utilize multilingual pre-trained model\nmBERT (Devlin et al., 2019) to encode passages\ninto 768-dimensional vectors.\nTraining. Since the output vectors of mBERT are\nnot aligned across languages, we need extra align-\nment training to ensure that similar passages in dif-\nferent languages can be mapped into near regions\nin the high-dimensional space. Given a passage\npXX\ni in language XX, we take its corresponding En-\nglish passage pEN+\ni as the positive example and ran-\ndomly select nnegative passages pEN‚àí\ni,1 ,...,p EN‚àí\ni,n in\nthe English corpus. The dense retriever is trained\nby optimizing the negative log likelihood loss of\nthe positive English passage.\nIn the e-commerce domain, we select the\npositive passage according to product metadata.\nFor a product sold in both EN and XX market-\nplaces, we regard its bilingual product descrip-\ntions (pXX\ni ,pEN+\ni ) as a parallel passage pair, i.e.,\npositive training example. For the domain of aca-\ndemic paper, we notice that papers with parallel\ntext is very rare. Therefore, we develop an auto-\nmatic approach to mine parallel abstract pairs of\nEnglish and the target language. Specifically, we\nadopt an off-the-shelf bi-text mining tool named\nSentence Transformers (Reimers and Gurevych,\n2019) to mine pseudo parallel pairs. Given two\ndatasets in different languages, we encode passages\nusing LaBSE (Feng et al., 2020), the current best\nmethod for learning language-agnostic sentence\nembeddings for 109 languages, and then parallel\npassages can be extracted through nearest-neighbor\nretrieval and filtered by setting a fixed threshold\nover a margin-based similarity score, as proposed\nin (Artetxe and Schwenk, 2019).\n4.2 Multilingual Keyphrase Generation with\nCode-Mixed Inputs\nGiven the top m retrieved English passages pEN\n1 ,\n..., pEN\nm, we find their associated keyphrases in the\n1236\ndataset: {kEN\n1,1, ..., kEN\n1,n1 }, ..., {kEN\nm,1, ..., kEN\nm,nm}.\nWe utilize mBART (Liu et al., 2020b), a multilin-\ngual denoising pre-trained sequence-to-sequence\nlanguage model, to integrate information from mul-\ntiple languages. Different from machine transla-\ntion which maps a sentence a the source language\nto a target language, our multilingual keyphrase\ngeneration model takes code-mixed inputs ‚Äì a com-\nbination of retrieved English keyphrases {kEN\n1,1, ...,\nkEN\n1,n1 }, ..., {kEN\nm,1, ..., kEN\nm,nm}from mretrieved En-\nglish passages and the source passage pXX in the\ntarget language XX.\nWe concatenate retrieved English keyphrases\nwith a delimiter token [SEP], and add special to-\nkens to separate different inputs: [ENKPS] for\nretrieved keyphrases and [CTX] for the source\npassage. Besides, we follow the fine-tuning setup\nof mBART by adding the language identifier[XX]\n(e.g. [DE] for German) at the end of the input\nsequence to denote the current input language:\n[ENKPS] kEN\n1,1 [SEP] ... [SEP] kEN\nm,nm [CTX] pXX [XX].\nThe training target is a sequence of concatenated\nkeyphrases kXX\n1 , ..., kXX\nn , separated by a special to-\nken [SEP]. [XX], the language identifier of the\ncurrent language, is also added at the beginning of\nthe target sequence to indicate the target language:\n[XX] kXX\n1 [SEP] kXX\n2 [SEP] ... [SEP] kXX\nn .\n4.3 Retriever-Generator Iterative Training\nIn spite of having utilized parallel passage pairs\nto align the multilingual representations of the re-\ntrieval module, it remains a concern because the\nparallel passage pairs between English and non-\nEnglish languages account for only a small por-\ntion of the whole multilingual dataset. For ex-\nample, in a popular e-commerce platform, only a\nsmall percentage of products (less than 10%) have\nboth English and non-English descriptions. With-\nout enough quality parallel pairs, the cross-lingual\ndense passage retriever may not work well to find\nrelevant English passages. Consequently, associ-\nated English keyphrases may provide little help for\nmultilingual keyphrase generation.\nTo make the multilingual keyphrase generation\ngeneralize better to any target languages or do-\nmains without reliance on numerous parallel pas-\nsage pairs, we propose an iterative training method\nto mine parallel passages which requires only a\nsmall number of initial parallel pairs of bootstrap\nthe process. Since our ultimate goal of the retriever\nmodel is to provide useful external knowledge for\nAlgorithm 1 Parallel Passage Mining via Iterative Training\n1: Input: (1) Parallel data DPAR = {(pEN\nPAR,pXX\nPAR,kEN\nPAR,kXX\nPAR)}, (2)\nNon-parallel data DNP = {(pXX\nNP,kXX\nNP)}, (3) Large-scale English\ncorpus DLS = {(pEN\nLS,kEN\nLS)}.\n2: Output: Pseudo parallel passage pairs DPSEUDO\n3: \u0003 0. Train Seq2Seq baseline w/o retrieved keyphrases\n4: GB ‚Üêtrain({(pXX\nPAR,kXX\nPAR)}‚àà DPAR)\n5: D0\nPSEUDO ‚Üê{} // Pseudo parallel passage pairs\n6: for t‚àà{0...T ‚àí1}do\n7: \u0003 1. Train retriever on pseudo and parallel data\n8: Rt ‚Üêtrain({(pEN\nPAR,pXX\nPAR)}‚àà Dt\nPSEUDO ‚à™DPAR)\n9: \u0003 2. Train retrieval-augmented generator on DPAR\n10: for each (pXX\nPAR,kXX\nPAR) ‚ààDPAR do\n11: ÀÜpEN\nLS ‚ÜêRt(pXX\nPAR,DLS) // Retrieve EN passages\n12: {ÀÜkEN\nLS}‚Üê ÀÜpEN\nLS // Find associated EN keyphrases\n13: // Train retrieval-augmented generator withEN keyphrases\n14: Gt ‚Üêtrain({(pXX\nPAR,kXX\nPAR)}‚àà DPAR,{ÀÜkEN\nLS})\n15: \u0003 3. Create pseudo parallel passage pairs\n16: Dt+1\nPSEUDO ‚Üê{}\n17: for each pXX\nNP ‚ààDNP do\n18: ÀÜpEN\nLS ‚ÜêRt(pXX\nNP,DLS) // Retrieve EN passages\n19: {ÀÜkEN\nLS}‚Üê ÀÜpEN\nLS // Find associated EN keyphrases\n20: // Predict keyphrases w/o and w/ EN keyphrases\n21: {ÀúkXX\nNP,GB\n}‚Üê GB(pXX\nNP)\n22: {ÀúkXX\nNP,Gt}‚Üê Gt(pXX\nNP,{ÀÜkEN\nLS})\n23: // If adding EN keyphrases leads to better keyphrase pre-\ndictions, the retrieved EN passages are taken as positive examples\n24: if F1({ÀúkXX\nNP,Gt}) ‚àíF1({ÀúkXX\nNP,GB\n}) >œÑ then\n25: Dt+1\nPSEUDO ‚ÜêDt+1\nPSEUDO ‚à™{(ÀÜpEN\nNP,pXX\nNP)}\n26: return DT\nPSEUDO\nmultilingual keyphrase generation, we mine paral-\nlel passage pairs (English and a non-English lan-\nguage) according to whether the retrieved English\npassage-keyphrases pairs could help the keyphrase\ngeneration for the target non-English language XX.\nFor example, let pEN\na and pEN\nb be two retrieved En-\nglish passages for a passage pXX in target language,\nif the associated keyphrases of pEN\na provide more\nuseful information for generating the keyphrases\nof pXX than pEN\nb , then (pEN\na ,pXX) would be consid-\nered as a better parallel passage pair. That said, we\nexpect the mined pseudo parallel passage pairs to\nbe of high quality according to the retrieval score,\nat the same time they can be directly helpful for\ntraining the generation module.\nThe proposed iterative training approach is\nsketched in Algo. 1 and Fig. 2. Given aLarge-Scale\nkeyphrase dataset in English DLS = {(pEN\nLS,kEN\nLS)}\nand a smaller one DXX = {(pXX,kXX)}in tar-\nget language XX, we denote the set of anno-\ntated parallel examples (bilingual passages in\nEnglish and other languages) as PARallel split\nDPAR = {(pEN\nPAR,pXX\nPAR,kEN\nPAR,kXX\nPAR)}, in which\n{(pXX\nPAR,kXX\nPAR)}comes from the XX dataset while\n{(pEN\nPAR,kEN\nPAR)}comes from the English dataset.\nThe remaining data examples in the target dataset\n1237\nLanguageTrainSize DevSize TestSizePassage Length(Avg/Std/Mid)#Keyphrases(Avg/Std/Mid)AbsentKps%\nAcademicMKP Dataset\nChinese (ZH)1,110 158 319 217/48/207 5/1/5 27.2%Korean (KO)774 110 222 115/31/111 4/1/4 37.7%Total 1,884 268 541 171/57/155 4/1/4 31.3%\nEcommerceMKP Dataset\nGerman (DE)23,997 1,411 2,825 157/79/141 10/5/8 57.1%Spanish (ES)12,222 718 1,440 159/84/139 9/5/7 54.6%French (FR)16,986 998 2,000 163/84/144 9/5/8 63.0%Italian (IT)9,163 538 1,081 167/84/152 8/3/7 42.6%Total 62,368 3,665 7,346 161/82/143 9/5/7 56.4%\nTable 1: AcademicMKP & EcommerceMKP Dataset\nDXX have no annotated corresponding English ex-\namples in DLS (the pairs may exist but are not\nknown yet), and we name this set as the Non-\nParallel split DNP = {(pXX\nNP,kXX\nNP)}. We firstly\nfine-tune a mBART using only keyphrases data\nof target language {(pXX\nPAR,kXX\nPAR)}in DPAR (Line\n4). Then we start a loop to mine pseudo paral-\nlel passage pairs for refining the passage retriever.\nEach iteration is expected to bring in a higher qual-\nity of pseudo passage pairs, resulting in a better\nperformance of retriever, with three steps:\n1. We train a retriever Rt using existing available\nEN-XX passage pairs {(pEN\nPAR,pXX\nPAR)}from both\nparallel passage data DPAR and most up-to-date\npseudo passage data Dt\nPSEUDO (Line 8).\n2. We train a retrieval-augmented model Gt us-\ning multilingual passages {(pXX\nPAR,kXX\nPAR)}from\nthe parallel data DPAR and retrieved English\nkeyphrases {ÀÜkEN\nLS}from the English dataset\nDLS (Line 14). To get the retrieved English\nkeyphrases for each passage pXX\nPAR, we take the\nretriever Rt trained in step (1) to do a KNN\nsearch for passages ÀÜpEN\nLS in EN DLS and find\ntheir associated keyphrases {ÀÜkEN\nLS}(Line 10-12).\n3. For each passage pXX\nNP in the non-aligned dataset\nDNP, we also retrieve English passages ÀÜpEN\nLS\nand keyphrases {ÀÜkEN\nLS}from DLS (Line 18-19).\nThen the retrieved English passage ÀÜpEN\nLS will\nbe taken as the parallel text to pXX\nNP if its as-\nsociated keyphrases {ÀÜkEN\nLS}provide useful in-\nformation. The usefulness is measured by the\nkeyphrase generation performance (F-score)\nbetween the retrieval-augmented generation\nmodel Gt and the base model G0 that does\nnot use EN keyphrases (Line 21-25).\nAfter T iterations, we train the retriever on the\npseudo data DT\nPSEUDO and fine-tune it on the parallel\ndata DPAR. Then we treat it as our final retriever\nand train the generation model in Sec. 4.2.\n5 Datasets\nEcommerceMKP Dataset is collected from a\npopular E-commerce shopping platform. There\nare four languages we consider for building Ecom-\nmerceMKP: German (DE), French (FR), Spanish\n(ES) and Italian (IT). The title, product descrip-\ntion, and bullet description provided by manufac-\nturers are concatenated and treated as source in-\nput. The keyphrases of each product are selected\nfrom search queries under the following protocol.\nFirst, given a product, we only keep search queries\nthat lead to purchases and treat them as effective\nqueries. Then phrases are chunked from these effec-\ntive queries using AutoPhrase (Shang et al., 2018)\nand further ranked by their frequency. Our assump-\ntion is: the more times a phrase appears in effective\nsearch queries of a product, the more important\na phrase is. Finally a threshold is set to filter out\nunimportant phrases. Under this protocol, we re-\nceive 73k examples over four languages. The statis-\ntics are shown in Table 1.\nWe collect the passages and keyphrases under\nthe same protocol for the English (EN) dataset and\nname it as EcommerceMKP-EN. In total the En-\nglish dataset contains 3 million passage-keyphrases\npairs. To obtain the parallel passage pairs for train-\ning the cross-lingual dense passage retriever, we\npair the product descriptions in different languages\naccording to the product identification information.\nWe select a total of 1,247 parallel passages from\nEcommerceMKP training set which include 480\npassages for DE-EN, 244 for ES-EN, 340 for FR-\nEN, and 183 for IT-EN. Besides, we keep 1,000\nparallel passage pairs in the DEV set of Ecom-\nmerceMKP to evaluate the performance of retrieval\nand bi-text mining.\nAcademicMKP Dataset is collected from the\nacademic domain. We take the title and abstract of\neach paper as the source text and author-provided\nkeywords as the target output. All papers are\nsampled from Microsoft Academic Graph (Sinha\net al., 2015), a web-scale academic entity graph\nthat contains multiple types of scholarly entities\nand relationships: field of study, author, institu-\ntion, abstract, venue, and keywords. We use Spacy\n(https://spacy.io/) to detect the language\nof abstracts and keyphrases, and choose two lan-\nguages Chinese (ZH) and Korean (KO) to construct\nthe AcademicMKP dataset. Since Microsoft Aca-\ndemic Graph (MAG) is automatically crawled and\n1238\nModel German (DE) Spanish (ES) French (FR) Italian (IT) Average\nP R F1 P R F1 P R F1 P R F1 P R F1\nUnsupervised Statistical Keyphrase Extraction\nKP-Miner 9.13 2.35 3.34 14.56 4.59 6.22 7.78 2.76 3.62 22.51 7.71 10.31 11.80 3.69 5.01\nY AKE 2.31 26.54 4.17 3.26 36.86 5.89 2.47 27.29 4.43 3.87 50.12 7.10 2.77 32.24 5.01\nUnsupervised Graph-based Keyphrase Extraction\nTextRank 5.77 9.00 6.39 7.15 11.59 7.97 5.45 8.33 5.94 8.18 15.27 9.85 6.31 10.25 7.09\nTopicalPageRank 3.59 14.10 5.34 5.07 22.08 7.74 3.75 15.49 5.65 5.22 25.43 8.26 4.16 17.71 6.33\nPositionRank 5.24 11.67 6.90 7.86 19.36 10.75 5.56 13.50 7.56 7.98 21.46 11.34 6.24 15.12 8.49\nMultipartiteRank 5.09 8.76 6.15 7.56 14.25 9.46 5.31 9.86 6.59 7.72 15.93 10.12 6.02 11.19 7.50\nSupervised Feature-based Keyphrase Extraction\nKea 9.64 17.73 11.92 12.69 24.64 16.11 8.81 16.76 11.05 14.16 30.18 18.81 10.68 20.65 13.52\nNeural-based Supervised Keyphrase Generation\nCopyRNN 13.48 7.59 9.08 21.11 12.40 14.78 14.79 8.48 10.04 34.66 20.07 24.28 18.45 10.61 12.69\nTransformer 29.92 25.40 26.22 34.01 30.57 30.83 29.17 24.03 25.18 44.05 43.34 42.56 32.60 28.68 29.25\nmBART (monolingual)44.91 39.59 40.04 47.70 44.79 44.17 42.64 36.81 37.72 57.98 58.16 56.25 46.76 42.58 42.60\nmBART (multilingual)45.78 40.93 41.09 48.43 44.99 44.57 43.21 38.40 38.72 60.37 58.91 57.87 47.75 43.68 43.60\nmBART + EN Joint Train45.91 39.90 40.64 49.27 43.92 44.52 43.21 37.48 38.26 59.21 57.03 56.42 47.79 42.55 43.08\nmBART + EN Pretrain45.77 40.76 41.05 48.34 44.94 44.58 42.96 38.10 38.45 60.24 58.51 57.59 47.64 43.46 43.47\nRAMKG (Ours) 46.88 41.90 42.14 49.35 45.89 45.49 43.79 39.48 39.61 60.89 59.51 58.43 48.59 44.61 44.50\nRAMKG + RGIT (Ours) 48.11 43.05 43.30 50.54 47.04 46.64 45.07 40.77 40.86 62.35 60.75 59.87 49.86 45.81 45.73\nTable 2: Main results on the EcommerceMKP dataset. The best results are in bold. (RGIT: Iterative-training)\nModel Chinese (ZH) Korean (KO) AverageP R F1 P R F1 P R F1\nmBART (mono.)32.52 31.50 31.50 24.57 26.46 24.96 29.26 29.43 28.81mBART (multi.)32.48 32.27 31.85 27.03 26.93 26.44 30.25 30.08 29.63mBART + Joint31.10 30.38 30.23 27.36 26.85 26.58 29.57 28.93 28.73mBART + Pretrain32.72 29.77 30.66 27.56 25.30 27.56 30.60 27.94 28.68RAMKG (Ours) 33.40 32.66 32.45 28.3028.1727.68 31.31 30.82 30.49RAMKG + RGIT (Ours) 34.38 33.05 33.15 29.3327.8728.00 32.31 30.92 31.04\nTable 3: Results on AcademicMKP (mono: monolin-\ngual, multi: multilingual, Joint: EN Joint Train, Pretrain:\nEN Pretrain, RGIT: Iterative-training).\nconstructed, we find some of its data is extremely\nnoisy. For example, keyphrases might be miss-\ning or contain incorrect information such as titles,\nauthor names, and publication venues. Some of\nthe abstracts are incomplete. Therefore, we hire\nthree annotators to manually examine the samples\nfrom MAG dataset. Data examples with incomplete\nabstracts are removed. We further manually ver-\nify the metadata of all examples and correct their\nkeyphrase information if needed. Finally, 2,693\nhigh-quality data examples of scientific papers in\nthe computer science domain are collected to con-\nstitute AcademicMKP.\nBesides the multilingual AcademicMKP dataset,\nwe use KP20K (Meng et al., 2017) as the English\ndata for retrieval-augmented generation. KP20K\nhas 560k abstract-keyphrases pairs collected from\nvarious online digital libraries in computer science\ndomain. The threshold is set as 1.03 for passage\nmining and we receive 841 parallel passage pairs\nfrom AcademicMKP training set, in which 433\nZH-EN passage pairs and 384 for KO-EN.\n6 Experimental Setup\n6.1 Evaluation Metrics\nKeyphrase Generation. Let the ground truth\nkeyphrases be Y : k1,k2,...,k n and the pre-\ndicted keyphrases be ÀúY : Àúk1,Àúk2,..., ÀúkM, we com-\npute the precision (P@M), recall (R@M) and F-\nscore ( F1@M) between Y and ÀúY as P@M =\n|Y‚à©ÀúY|\n|ÀúY| ,R@M = |Y‚à©ÀúY|\n|Y| ,F1@M = 2√óP√óR\nP+R , where\n|Y|denotes the number of keyphrases in the gold\nset Y. We only consider exact match of two\nkeyphrases (with some post-processing such as low-\nercase) for |Y‚à©ÀúY|. Then the average are computed\nfor all languages in the test set.\nPassage Retrieval. The quantity of retrieved En-\nglish passages directly influences how much ex-\nternal knowledge could be utilized for keyphrase\ngeneration. Therefore, we evaluate the top-k re-\ncall (k=1,2,5,10,20) on the DEV set for evaluating\nretrieval performance.\n6.2 Baselines and Ablations\nWe consider following baselines and ablations:\n1) Unsupervised Statistical Keyphrase Extrac-\ntion: KP-Miner (El-Beltagy and Rafea, 2010),\nYAKE (Campos et al., 2020); 2) Unsuper-\nvised Graph-based Keyphrase Extraction: Tex-\ntRank (Mihalcea and Tarau, 2004), TopicalPageR-\nank (Sterckx et al., 2015), PositionRank (Florescu\nand Caragea, 2017), MultipartiteRank (Boudin,\n2018); 3) Supervised Feature-based Keyphrase\nExtraction: KEA (Witten et al., 1999b); 4) Neural\n1239\nSupervised Keyphrase Generation: CopyRNN,\nTransformer; 5) mBART (monolingual) : sepa-\nrately trained 6 mBART models on each language;\n6) mBART (multilingual) : a single mBART\nmodel on all languages; 7) mBART + EN Joint\nTrain: a mBART model jointly trained on the\nmultilingual data and English data (KP20K (Meng\net al., 2017) for AcademicMKP; EcommerceMKP-\nEN for EcommerceMKP). 8) mBART + EN Pre-\ntrain: a mBART firstly pre-trained on the English\ndata and then fine-tuned on the multilingual data.9)\nRAMKG (Ours): The Retrieval-Augmented Mul-\ntilingual Keyphrase Generation model (Sec. 4.1 &\n4.2). 10) RAMKG + RGIT (Ours) : RAMKG\nimproved with retriever-generator iterative training\n(RGIT) (Sec. 4.3).\n7 Results and Analyses\n7.1 Main Results\nMain results are shown in Table 2 & 3 for Ecom-\nmerceMKP and AcademicMKP respectively, and\nwe make the following observations:\n‚Ä¢ The unsupervised approaches, both statistical-\nbased and graph-based, have robust results\nacross all languages. PositionRank performs\nthe best among all unsupervised approaches.\n‚Ä¢ The supervised approaches consistently outper-\nform unsupervised approaches. The feature-\nbased approach KEA receives a high recall by\npredicting more keyphrases while the Copy-\nRNN receives a high precision. Different from\nthe results on English keyphrase generation\nwhere Transformer and CopyRNN are compa-\nrable, the Transformer beats the CopyRNN by\na large margin in the multilingual scenario.\n‚Ä¢ We observe that jointly training on all lan-\nguages (mBART multilingual) receives better\nresults than separately training on each lan-\nguage (mBART monolingual). This implies\nthe ability of locating and summarizing key in-\nformation is transferable across languages.\n‚Ä¢ Comparing different approaches using external\nlarge-scale English data, we find that our pro-\nposed RAMKG outperforms both ‚ÄúEN Joint\nTrain‚Äù and ‚ÄúEN Pretrain‚Äù. This is because the\nretrieval-augmented approach provides auxil-\niary knowledge information as part of the input\nto the generation module, while the other two\nvariants have to ‚Äúinfuse‚Äù the knowledge learned\nfrom English data to model parameters. More-\nover, ‚ÄúEN Joint Train‚Äù and ‚ÄúEN Pretrain‚Äù have\nRecall @ Top K 1 2 5 10 20\nRAMKG 26.4% 36.8% 50.1% 59.2% 67.3%RAMKG + RGIT 45.8% 58.3% 72.4% 79.5% 85.2%\nTable 4: Retrieval recall on EcommerceMKP DEV set.\nIteration 1 2 3 4 5 6\n# Pseudo Passages 20,288 21,402 19,942 21,241 22,343 21,557Label Accuracy % 28.0% 37.9% 40.5% 44.2% 45.1% 47.0%\nTable 5: Number of pseudo parallel passages and their\naccuracy on EcommerceMKP DEV set in different iter-\nations of parallel passages mining.\nno positive effect on AcademicMKP dataset (Ta-\nble 3). Compared with multilingual and English\ndata are from the same website, there is still a\ndomain gap between papers (multilingual) in\nAcademicMKP and papers (English) in KP20K.\n‚Ä¢ The retriever-generator self-training (RAMKG\n+ Iter) alleviates the data scarcity issue with the\nhelp of stronger retriever: since the retriever\ncan find more relevant English keyphrases, it\nleads to a general improvement on keyphrase\nperformance across languages.\n7.2 Effect of Iterative Training\nRetrieval Results We investigate the effect of\nretriever-generator iterative training by comparing\nthe retrieval recall for models trained w/o and w/\nthe mined pseudo parallel passage pairs. Results\non the DEV set of EcommerceMKP are shown in\nthe Table 4. With additional mined pseudo parallel\npassage pairs, the retriever improves the Recall@5\nfrom 50.1% to 72.4%. And therefore, the better\nretrieved English keyphrases lead to a better gener-\nation performance (44.50 vs. 45.73 in Table 2).\nQuantity and Quality of Pseudo Parallel Pas-\nsages We show the quantity and quality of mined\npseudo parallel pairs in Table 5. After each itera-\ntion of passage mining, our algorithm can consis-\ntently find around 20k passage pairs from Ecom-\nmerceMKP training set, which are nearly 20 times\nof the initial data. To assess the quality of mined\npassage pairs, we examine the label accuracy using\nthe 1,000 parallel passage pairs from the DEV set\nof EcommerceMKP. Results in Table 5 show that\nwhile the passage mining finds a similar number of\npseudo passage pairs, the labelling accuracy does\nincrease from 28.0% to 47.0%. This is because the\nbetter pseudo parallel data improves the retriever,\nand the stronger retriever results in a better genera-\ntor, which in turn leads to more relevant passages.\n1240\n50.0%55.0%60.0%65.0%70.0%75.0%80.0%\n1.2k1.8k3k6k\nRetrieval Recall @ 5\nR1RN44.444.644.845.045.245.445.645.8\n1.2k1.8k3k6k\nGenerationF1\nG1GNùêë!ùêë\" ùêÜ!ùêÜ\"\n#Initial Parallel Passage Pairs#Initial Parallel Passage Pairs\nFigure 3: Performance of passage retrieval and\nkeyphrase generation on EcommerceMKP, with differ-\nent number of initial parallel data for iterative training.\nProduct Description (German): Steiff 113437 Soft Cud-\ndly Friends Honey Teddyb√§r, grau, 38 cm. Bereits der\nName des Soft Cuddly Friends Honey Teddyb√§r sagt es\nschon aus: der 38 cm gro√üe Freund mit seinem honigs√º√üen\nL√§cheln begeistert alle Kinderherzen ...\n(Translation in English): Steiff 113437 Soft Cuddly Friends\nHoney teddy bear, gray, 38 cm. The name of the Soft\nCuddly Friends Honey Teddy bear already says it all: the\n38 cm tall friend with his honey-sweet smile delights all\nchildren‚Äôs hearts ...\nGold Keyphrases (German) : steiff kuscheltier ; steiff\nteddy; soft cuddly friend; steiff; baer; grau.\n(Translation in English): steiff cuddly toy; steiff teddy; soft\ncuddly friend; steiff; bear; grey.\nRetrieved English Keyphrases: steiff teddy bear; teddy\nbear; my first; grey; honey; sweetheart; steiff bear; pink;\nvintage; steiff stuffed animal; steiff; terry; soft; jimmy.\nPredicted Keyphrases (German): steiff kuscheltier; steiff\nteddy; soft cuddly friend; steiff; baer; grau; jimmy.\n(Translation in English): steiff cuddly toy; steiff teddy; soft\ncuddly friend; steiff; bear; grey; jimmy.\nFigure 4: Case study on the EcommerceMKP dataset.\nThe present keyphrases (keyphrases shown in the de-\nscription) are in bold while absent keyphrases are in\nitalics. Correct predictions are in green while wrong\npredictions are in red.\nInitializing with Different Amount of Parallel\nData To investigate the impact of initial parallel\npassage on the training, we conduct experiments\nby varying the number of parallel passage pairs on\nEcommerceMKP, from 1.2k (default setting) to 6k\ninstances. We compare the single-round training\n(i.e., training with initial data) and iterative train-\ning after six rounds (in which round we generally\nobtain the best retrieval recall), on both passage\nretrieval (R1 & R6) and keyphrase generation (G1\n& G6). Results are shown in Fig. 3. We observe\nthat (1) the score of iterative training consistently\nincrease when more annotated parallel data is avail-\nable; (2) our iterative training demonstrates great\nrobustness with limited parallel data (e.g. 1.2k\npairs), while the benefit gradually diminishes while\nmore parallel data becomes available.\n7.3 Case Study\nFig. 4 exhibits an example of our model‚Äôs predic-\ntion. Given a product description in German, the\nmodel retrieves several English keyphrases and\ngenerates keyphrases in German accurately (trans-\nlations in English are also provided). Through\nthis example, we find that the retrieved English\nkeyphrases do provide certain useful information\nsuch as ‚Äústeiff teddy bear‚Äù, ‚Äúgrey‚Äù and ‚Äúsoft‚Äù, while\nit also brings some noise such as ‚Äúmy first‚Äù, ‚Äúsweet-\nheart‚Äù and ‚Äúvintage‚Äù. Although there is a wrong\nprediction ‚Äújimmy‚Äù caused by the retrieved En-\nglish keyphrases, the improvement in results shows\nthat the benefits of retrieved knowledge outweigh\nthe noise it introduces. Moreover,the retrieved\nkeyphrases are only regarded as a supplement to\nthe original passage, and the generator can au-\ntomatically focus on the informative parts from\nboth inputs through self-attention. Our retrieval-\naugmented multilingual keyphrase generation can\ntolerate some noise from the retrieved English\nkeyphrases and predict better keyphrases based on\nthese external knowledge.\n8 Conclusion\nIn this study, we investigate a novel task setting\n‚Äì multilingual keyphrase generation ‚Äì and con-\ntribute two new multilingual keyphrase generation\ndatasets covering multiple domains and languages.\nFurthermore, we propose a retrieval-augmented\nmultilingual keyphrase generation framework with\nretriever-generator iterative training. Results show\nthat the proposed approach outperforms a wide\nrange of baselines.\nReferences\nWasi Ahmad, Xiao Bai, Soomin Lee, and Kai-Wei\nChang. 2021. Select, extract and generate: Neu-\nral keyphrase generation with layer-wise coverage\nattention. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 1389‚Äì1404, Online. Association for Computa-\ntional Linguistics.\nMikel Artetxe and Holger Schwenk. 2019. Margin-\nbased parallel corpus mining with multilingual sen-\ntence embeddings. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 3197‚Äì3203, Florence, Italy. Asso-\nciation for Computational Linguistics.\n1241\nFlorian Boudin. 2018. Unsupervised keyphrase extrac-\ntion with multipartite graphs. In Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 2 (Short Pa-\npers), pages 667‚Äì672, New Orleans, Louisiana. As-\nsociation for Computational Linguistics.\nDeng Cai, Yan Wang, Huayang Li, Wai Lam, and\nLemao Liu. 2021. Neural machine translation with\nmonolingual translation memory. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 7307‚Äì7318, Online.\nAssociation for Computational Linguistics.\nRicardo Campos, V√≠tor Mangaravite, Arian Pasquali,\nAl√≠pio Jorge, C√©lia Nunes, and Adam Jatowt. 2020.\nYake! keyword extraction from single documents\nusing multiple local features. Inf. Sci., 509:257‚Äì289.\nHou Pong Chan, Wang Chen, Lu Wang, and Irwin King.\n2019. Neural keyphrase generation via reinforcement\nlearning with adaptive rewards. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 2163‚Äì2174, Florence,\nItaly. Association for Computational Linguistics.\nJun Chen, Xiaoming Zhang, Yu Wu, Zhao Yan, and\nZhoujun Li. 2018. Keyphrase generation with corre-\nlation constraints. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 4057‚Äì4066, Brussels, Belgium.\nAssociation for Computational Linguistics.\nWang Chen, Hou Pong Chan, Piji Li, Lidong Bing,\nand Irwin King. 2019a. An integrated approach for\nkeyphrase generation via exploring the power of re-\ntrieval and extraction. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 2846‚Äì2856, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nWang Chen, Hou Pong Chan, Piji Li, and Irwin King.\n2020. Exclusive hierarchical decoding for deep\nkeyphrase generation. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1095‚Äì1105, Online. Association\nfor Computational Linguistics.\nWang Chen, Yifan Gao, Jiani Zhang, Irwin King, and\nMichael R. Lyu. 2019b. Title-guided encoding for\nkeyphrase generation. In The Thirty-Third AAAI Con-\nference on Artificial Intelligence, AAAI 2019, The\nThirty-First Innovative Applications of Artificial In-\ntelligence Conference, IAAI 2019, The Ninth AAAI\nSymposium on Educational Advances in Artificial\nIntelligence, EAAI 2019, Honolulu, Hawaii, USA,\nJanuary 27 - February 1, 2019 , pages 6268‚Äì6275.\nAAAI Press.\nRajarshi Das, Manzil Zaheer, Dung Thai, Ameya God-\nbole, Ethan Perez, Jay Yoon Lee, Lizhen Tan, Lazaros\nPolymenakos, and Andrew McCallum. 2021. Case-\nbased reasoning for natural language queries over\nknowledge bases. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 9594‚Äì9611, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nKushal Dave, Steve Lawrence, and David M. Pennock.\n2003. Mining the peanut gallery: opinion extraction\nand semantic classification of product reviews. In\nProceedings of the Twelfth International World Wide\nWeb Conference, WWW 2003, Budapest, Hungary,\nMay 20-24, 2003, pages 519‚Äì528. ACM.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171‚Äì4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nSamhaa R. El-Beltagy and Ahmed Rafea. 2010. KP-\nminer: Participation in SemEval-2. In Proceedings\nof the 5th International Workshop on Semantic Evalu-\nation, pages 190‚Äì193, Uppsala, Sweden. Association\nfor Computational Linguistics.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Ari-\nvazhagan, and Wei Wang. 2020. Language-agnostic\nBERT sentence embedding. CoRR, abs/2007.01852.\nCorina Florescu and Cornelia Caragea. 2017. Posi-\ntionRank: An unsupervised approach to keyphrase\nextraction from scholarly documents. In Proceedings\nof the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1105‚Äì1115, Vancouver, Canada. Association\nfor Computational Linguistics.\nEibe Frank, Gordon W. Paynter, Ian H. Witten,\nCarl Gutwin, and Craig G. Nevill-Manning. 1999.\nDomain-specific keyphrase extraction. In Proceed-\nings of the Sixteenth International Joint Conference\non Artificial Intelligence, IJCAI 99, Stockholm, Swe-\nden, July 31 - August 6, 1999. 2 Volumes, 1450 pages,\npages 668‚Äì673. Morgan Kaufmann.\nYgor Gallina, Florian Boudin, and Beatrice Daille. 2019.\nKPTimes: A large-scale dataset for keyphrase gener-\nation on news documents. In Proceedings of the 12th\nInternational Conference on Natural Language Gen-\neration, pages 130‚Äì135, Tokyo, Japan. Association\nfor Computational Linguistics.\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor\nO. K. Li. 2018. Search engine guided neural machine\ntranslation. In AAAI.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Ming-Wei Chang. 2020. REALM: Retrieval-\naugmented language model pre-training. Interna-\ntional Conference on Machine Learning (ICML).\n1242\nAnette Hulth and Be√°ta B. Megyesi. 2006. A study\non automatically extracted keywords in text catego-\nrization. In Proceedings of the 21st International\nConference on Computational Linguistics and 44th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 537‚Äì544, Sydney, Australia. Asso-\nciation for Computational Linguistics.\nJeff Johnson, Matthijs Douze, and Herv√© J√©gou. 2021.\nBillion-scale similarity search with gpus. IEEE\nTrans. Big Data, 7(3):535‚Äì547.\nSteve Jones and Mark S. Staveley. 1999. Phrasier:\nA system for interactive document retrieval using\nkeyphrases. In SIGIR ‚Äô99: Proceedings of the 22nd\nAnnual International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval,\nAugust 15-19, 1999, Berkeley, CA, USA, pages 160‚Äì\n167. ACM.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769‚Äì6781,\nOnline. Association for Computational Linguistics.\nJihyuk Kim, Myeongho Jeong, Seungtaek Choi, and\nSeung-won Hwang. 2021. Structure-augmented\nkeyphrase generation. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2657‚Äì2667, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nDong-Hyun Lee. 2013. Pseudo-label : The simple and\nefficient semi-supervised learning method for deep\nneural networks.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 6086‚Äì6096, Florence, Italy.\nAssociation for Computational Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt√§schel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Advances in Neural Infor-\nmation Processing Systems, volume 33, pages 9459‚Äì\n9474. Curran Associates, Inc.\nXinnian Liang, Shuangzhi Wu, Mu Li, and Zhoujun Li.\n2021. Unsupervised keyphrase extraction by jointly\nmodeling local and global context. In Proceedings of\nthe 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 155‚Äì164, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nRui Liu, Zheng Lin, Peng Fu, and Weiping Wang. 2020a.\nReinforced keyphrase generation with bert-based sen-\ntence scorer. In 2020 IEEE Intl Conf on Paral-\nlel & Distributed Processing with Applications, Big\nData & Cloud Computing, Sustainable Computing &\nCommunications, Social Computing & Networking\n(ISPA/BDCloud/SocialCom/SustainCom), pages 1‚Äì8.\nIEEE.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020b. Multilingual denoising\npre-training for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726‚Äì742.\nZhiyuan Liu, Xinxiong Chen, Yabin Zheng, and\nMaosong Sun. 2011. Automatic keyphrase extraction\nby bridging vocabulary gap. the Fifteenth Conference\non Computational Natural Language Learning.\nYichao Luo, Yige Xu, Jiacheng Ye, Xipeng Qiu, and\nQi Zhang. 2021. Keyphrase generation with fine-\ngrained evaluation-guided reinforcement learning. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021, pages 497‚Äì507, Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nRui Meng, Xingdi Yuan, Tong Wang, Sanqiang Zhao,\nAdam Trischler, and Daqing He. 2021. An empir-\nical study on neural keyphrase generation. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n4985‚Äì5007, Online. Association for Computational\nLinguistics.\nRui Meng, Sanqiang Zhao, Shuguang Han, Daqing He,\nPeter Brusilovsky, and Yu Chi. 2017. Deep keyphrase\ngeneration. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 582‚Äì592, Vancouver,\nCanada. Association for Computational Linguistics.\nRada Mihalcea and Paul Tarau. 2004. TextRank: Bring-\ning order into text. In Proceedings of the 2004 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 404‚Äì411, Barcelona, Spain. Asso-\nciation for Computational Linguistics.\nFunan Mu, Zhenting Yu, LiFeng Wang, Yequan Wang,\nQingyu Yin, Yibo Sun, Liqun Liu, Teng Ma, Jing\nTang, and Xing Zhou. 2020. Keyphrase extrac-\ntion with span-based feature representations. arXiv\npreprint arXiv:2002.05407.\nSeoyeon Park and Cornelia Caragea. 2020. Scientific\nkeyphrase identification and classification by pre-\ntrained language models intermediate task transfer\n1243\nlearning. In Proceedings of the 28th International\nConference on Computational Linguistics , pages\n5409‚Äì5419, Barcelona, Spain (Online). International\nCommittee on Computational Linguistics.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rockt√§schel, and Sebastian\nRiedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523‚Äì2544, Online.\nAssociation for Computational Linguistics.\nHieu Pham, Qizhe Xie, Zihang Dai, and Quoc V . Le.\n2021. Meta pseudo labels. 2021 IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR), pages 11552‚Äì11563.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982‚Äì3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nJingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren,\nClare R V oss, and Jiawei Han. 2018. Automated\nphrase mining from massive text corpora. IEEE\nTransactions on Knowledge and Data Engineering,\n30(10):1825‚Äì1837.\nArnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Dar-\nrin Eide, Bo-June Paul Hsu, and Kuansan Wang.\n2015. An overview of microsoft academic service\n(MAS) and applications. In Proceedings of the 24th\nInternational Conference on World Wide Web Com-\npanion, WWW 2015, Florence, Italy, May 18-22,\n2015 - Companion Volume, pages 243‚Äì246. ACM.\nLucas Sterckx, Thomas Demeester, Johannes Deleu,\nand Chris Develder. 2015. Topical word importance\nfor fast keyphrase extraction. In Proceedings of the\n24th International Conference on World Wide Web\nCompanion, WWW 2015, Florence, Italy, May 18-22,\n2015 - Companion Volume, pages 121‚Äì122. ACM.\nAvinash Swaminathan, Haimin Zhang, Debanjan Ma-\nhata, Rakesh Gosangi, Rajiv Shah, and Amanda\nStent. 2020. A preliminary exploration of gans for\nkeyphrase generation. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 8021‚Äì8030.\nMinmei Wang, Bo Zhao, and Yihua Huang. 2016. Ptr:\nPhrase-based topical ranking for automatic keyphrase\nextraction in scientific publications. 23rd Interna-\ntional Conference, ICONIP 2016.\nYue Wang, Jing Li, Hou Pong Chan, Irwin King,\nMichael R. Lyu, and Shuming Shi. 2019. Topic-\naware neural keyphrase generation for social media\nlanguage. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2516‚Äì2526, Florence, Italy. Association for\nComputational Linguistics.\nJason Weston, Emily Dinan, and Alexander Miller.\n2018. Retrieve and refine: Improved sequence gener-\nation models for dialogue. In Proceedings of the\n2018 EMNLP Workshop SCAI: The 2nd Interna-\ntional Workshop on Search-Oriented Conversational\nAI, pages 87‚Äì92, Brussels, Belgium. Association for\nComputational Linguistics.\nIan H. Witten, Gordon W. Paynter, Eibe Frank, Carl\nGutwin, and Craig G. Nevill-Manning. 1999a. Kea:\nPractical automatic keyphrase extraction. In Pro-\nceedings of the Fourth ACM Conference on Digital\nLibraries, DL ‚Äô99, pages 254‚Äì255, New York, NY ,\nUSA. ACM.\nIan H. Witten, Gordon W. Paynter, Eibe Frank, Carl\nGutwin, and Craig G. Nevill-Manning. 1999b. KEA:\npractical automatic keyphrase extraction. In Pro-\nceedings of the Fourth ACM conference on Digital\nLibraries, August 11-14, 1999, Berkeley, CA, USA,\npages 254‚Äì255. ACM.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38‚Äì45, Online. Association\nfor Computational Linguistics.\nHai Ye and Lu Wang. 2018. Semi-supervised learn-\ning for neural keyphrase generation. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 4142‚Äì4153,\nBrussels, Belgium. Association for Computational\nLinguistics.\nJiacheng Ye, Ruijian Cai, Tao Gui, and Qi Zhang. 2021a.\nHeterogeneous graph neural networks for keyphrase\ngeneration. arXiv preprint arXiv:2109.04703.\nJiacheng Ye, Tao Gui, Yichao Luo, Yige Xu, and\nQi Zhang. 2021b. One2Set: Generating diverse\nkeyphrases as a set. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 4598‚Äì4608, Online. Association\nfor Computational Linguistics.\nJing Zhao and Yuxiang Zhang. 2019. Incorporating\nlinguistic constraints into keyphrase generation. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5224‚Äì\n5233.\n1244\nA Appendix\nA.1 Implementation Details\nIn the cross-lingual dense passage retriever, we use\n‚Äúbert-base-multilingual-cased‚Äù model (Wolf et al.,\n2020) to initialize the query and passage encoders\nand fine-tune it for 15 epochs with a batch size of\n32. We share the parameters between the query\nencoder EQ(¬∑) and the passage encoder EP(¬∑) and\nmap English and non-English passages into the\nsame embedding space. Empirical results show\nthe encoder with parameter sharing can perform\nslightly better. The positive examples are the corre-\nsponding English passages while we randomly sam-\nple 100 passages as negative examples in training.\nFor the retrieval-augmented keyphrase generator,\nwe fine-tune ‚Äúmbart-large-cc25‚Äù (Wolf et al., 2020)\nfor 10 epochs with Adam (Kingma and Ba, 2015)\noptimizer with a learning rate of 1e-4, a batch size\nof 8, a warm-up rate of 50 training steps. Similar to\nmost Seq2Seq models, we train the mBART-based\ngeneration module by optimizing the negative log-\nlikelihood loss of the ground-truth keyphrase se-\nquence, and use beam search decoding with a beam\nsize of 5 during inference. The number of retrieved\nkeyphrases mfor retrieval-augmented generation\nis a hyperparameter and is tuned on the develop-\nment set. We use keyphrases from m= 1English\npassages for AcademicMKP dataset and m = 5\nfor EcommerceMKP dataset. During inference, we\nset the maximum target sequence length as 128\nand set the beam decoding size as 5. For parallel\npassage mining via iterative training, we continue\nthe iterative process until the retrieval recall does\nnot improve. The total number of iterations (T in\nAlgo. 1) are 6 and 3 on EcommerceMKP and Aca-\ndemicMKP respectively. The threshold œÑ in Line\n23 for Algo. 1 is set as 5.\nA.2 Variants of Retrieval Targets\nThere exists a misalignment between the retriever\nand the generator model. The retriever retrieves\nsimilar passages while the generator utilizes the\nassociated keyphrases of these passages (not the\nretrieved passages) as external knowledge for gen-\neration. Therefore, a good retriever does not neces-\nsarily guarantee the good quality and usefulness of\nthese keyphrases.\nWe tried two different retrieval targets which\nmight close the misalignment. Given a non-English\npassage, we tried to either directly retrieve En-\nglish keyphrases (RAMKG-P2K) or retrieve the\nModel Retrieval Results Generation Results\nRecall@1 Recall@2 Recall@5 P R F1\nP2P 26.00% 36.78% 50.05% 48.51 44.71 44.50\nP2K 2.86% 4.70% 9.09% 46.91 43.13 42.95\nP2PK 25.25% 35.45% 49.51% 48.50 44.39 44.34\nTable 6: Results of RAMKG variants with different\nretrieval targets.\nœÑ 0 5 10 15\nRecall 62.28% 64.01% 63.67% 62.97%\nTable 7: Influence of the threshold œÑ on the retriever-\ngenerator self-training algorithm.\nconcatenated sequences of passage-keyphrase pair\n(RAMKG-P2PK). We find that (1) RAMKG-P2K\nthat directly retrieves keyphrases has poor re-\ntrieval performance. This is because it is hard to\ncapture the similarity between non-English pas-\nsages and English keyphrases; (2) RAMKG-P2PK\nhas slightly worse results than only retrieval EN\npassages, which implies that additionally adding\nkeyphrases in the retrieval targets does not bring\nany benefit.\nResults are shown in Table 6. RAMKG (P2P)\nis our original model which retrieves English pas-\nsages given a non-english passage. Results tell us\nthat 1) directly retrieval of keyphrases have poor\nretrieval performance. This is because it is hard\nto capture the similarity between non-english pas-\nsages and english keyphrases; 2) RAMKG (P2PK)\nhas slightly worse results than the model , which\nimplies that additionally adding keyphrases in the\nretrieval targets does not bring any benefit.\nA.3 Discussion on Retriever-Generator\nIterative training (RGIT) Algorithm\nDifference between RGIT and Self-Training.\nOur approach shares some similarities with self-\ntraining (Lee, 2013; Pham et al., 2021) but there\nare some differences. In self-training, the teacher\nand student models are in the same architecture\nand focus on the same training objectives. In our\nproposed retriever-generator iterative training, the\nretriever and generator are two different models\nand optimized by different objectives.\nThreshold Tuning. In this section, we investi-\ngate the impact of the chosen threshold œÑ (line 24)\nin our proposed retriever-generator iterative train-\ning. We tune the threshold (tau) on AcademicMKP\nand results are shown in Table 7. Results show\n1245\nthat tau=5 receives the best retrieval performance.\nTau=0 brings more pseudo parallel passage pairs\nbut introduces more noise. Larger tau (10/15) re-\nduces the number of pseudo pairs, making the iter-\native training less effective.\n1246",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8688284158706665
    },
    {
      "name": "Natural language processing",
      "score": 0.6238634586334229
    },
    {
      "name": "Generator (circuit theory)",
      "score": 0.5874712467193604
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5591365098953247
    },
    {
      "name": "Annotation",
      "score": 0.5002391338348389
    },
    {
      "name": "Information retrieval",
      "score": 0.40072304010391235
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I170201317",
      "name": "University of Pittsburgh",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210155268",
      "name": "Salesforce (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "HK"
    }
  ]
}