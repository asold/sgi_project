{
    "title": "Potential applications and implications of large language models in primary care",
    "url": "https://openalex.org/W4391345109",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5031274531",
            "name": "Albert Andrew",
            "affiliations": [
                "University of Auckland"
            ]
        },
        {
            "id": "https://openalex.org/A5031274531",
            "name": "Albert Andrew",
            "affiliations": [
                "University of Auckland"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4200170471",
        "https://openalex.org/W2979159986",
        "https://openalex.org/W4283801824",
        "https://openalex.org/W4382918229",
        "https://openalex.org/W4377009978",
        "https://openalex.org/W4387232979",
        "https://openalex.org/W4381572755",
        "https://openalex.org/W4289539209",
        "https://openalex.org/W4298211819",
        "https://openalex.org/W2937215033",
        "https://openalex.org/W2902116953",
        "https://openalex.org/W4210523443",
        "https://openalex.org/W3028484854",
        "https://openalex.org/W4388722291",
        "https://openalex.org/W4384407567",
        "https://openalex.org/W4376132115",
        "https://openalex.org/W4386831162",
        "https://openalex.org/W4387500346",
        "https://openalex.org/W4389523706",
        "https://openalex.org/W2515682654",
        "https://openalex.org/W3013406208",
        "https://openalex.org/W4368367885",
        "https://openalex.org/W4319301505",
        "https://openalex.org/W4386692532",
        "https://openalex.org/W2019618389",
        "https://openalex.org/W4367310920",
        "https://openalex.org/W4387242094",
        "https://openalex.org/W4386233891",
        "https://openalex.org/W2032980606",
        "https://openalex.org/W4386045865",
        "https://openalex.org/W4320920036",
        "https://openalex.org/W4306361994",
        "https://openalex.org/W4323050332",
        "https://openalex.org/W4385242971",
        "https://openalex.org/W4321351832",
        "https://openalex.org/W4324387439",
        "https://openalex.org/W4386932783",
        "https://openalex.org/W4321748146",
        "https://openalex.org/W4200511449"
    ],
    "abstract": "The recent release of highly advanced generative artificial intelligence (AI) chatbots, including ChatGPT and Bard, which are powered by large language models (LLMs), has attracted growing mainstream interest over its diverse applications in clinical practice, including in health and healthcare. The potential applications of LLM-based programmes in the medical field range from assisting medical practitioners in improving their clinical decision-making and streamlining administrative paperwork to empowering patients to take charge of their own health. However, despite the broad range of benefits, the use of such AI tools also comes with several limitations and ethical concerns that warrant further consideration, encompassing issues related to privacy, data bias, and the accuracy and reliability of information generated by AI. The focus of prior research has primarily centred on the broad applications of LLMs in medicine. To the author’s knowledge, this is, the first article that consolidates current and pertinent literature on LLMs to examine its potential in primary care. The objectives of this paper are not only to summarise the potential benefits, risks and challenges of using LLMs in primary care, but also to offer insights into considerations that primary care clinicians should take into account when deciding to adopt and integrate such technologies into their clinical practice.",
    "full_text": "1\nAndrew A. Fam Med Community Health 2024;12:e002602. doi:10.1136/fmch-2023-002602\nOpen access \nPotential applications and implications \nof large language models in \nprimary care\nAlbert Andrew    \nTo cite: Andrew A.  Potential \napplications and implications \nof large language models \nin primary care. Fam \nMed Community Health \n2024;12:e002602. doi:10.1136/\nfmch-2023-002602\nReceived 26 October 2023\nAccepted 16 January 2024\nMedical Student, The University \nof Auckland School of Medicine, \nAuckland, New Zealand\nCorrespondence to\nAlbert Andrew;  \n albertandrew@ hotmail. co. nz;  \naand273@ aucklanduni. ac. nz\nCommunication\n© Author(s) (or their \nemployer(s)) 2024. Re- use \npermitted under CC BY- NC. No \ncommercial re- use. See rights \nand permissions. Published by \nBMJ.\nABSTRACT:\nThe recent release of highly advanced generative artificial \nintelligence (AI) chatbots, including ChatGPT and Bard, \nwhich are powered by large language models (LLMs), has \nattracted growing mainstream interest over its diverse \napplications in clinical practice, including in health and \nhealthcare. The potential applications of LLM- based \nprogrammes in the medical field range from assisting \nmedical practitioners in improving their clinical decision- \nmaking and streamlining administrative paperwork to \nempowering patients to take charge of their own health. \nHowever, despite the broad range of benefits, the use \nof such AI tools also comes with several limitations and \nethical concerns that warrant further consideration, \nencompassing issues related to privacy, data bias, and \nthe accuracy and reliability of information generated by AI. \nThe focus of prior research has primarily centred on the \nbroad applications of LLMs in medicine. To the author’s \nknowledge, this is, the first article that consolidates current \nand pertinent literature on LLMs to examine its potential in \nprimary care. The objectives of this paper are not only to \nsummarise the potential benefits, risks and challenges of \nusing LLMs in primary care, but also to offer insights into \nconsiderations that primary care clinicians should take \ninto account when deciding to adopt and integrate such \ntechnologies into their clinical practice.\nINTRODUCTION\nIn recent years, the explosive growth of arti-\nficial intelligence (AI)- powered applications \nhas brought about a transformative wave \nacross various industries, and healthcare is no \nexception. AI, a broad conceptual term, refers \nto the use of automated computer systems \nthat can reason and perform cognitive func-\ntions similar to that of humans. 1 AI and its \nrelated technologies have been around since \nthe 1950s, with machine learning, a subset of \nAI, having multiple applications in healthcare \nwithin the last decade. 2 3  Recent advance-\nments in AI, specifically the development of \nlarge language models (LLMs) with natural \nlanguage processing (NLP) techniques, as \nseen in conversational chatbots like ChatGPT \nand Bard, have attracted global attention \ndue to their ability to provide appropriate, \ncoherent, human- like responses to questions \nacross diverse knowledge domains.4\nRecent studies on the performance of \nLLM- powered chatbots in answering medical \nquestions have demonstrated its impres-\nsive medical capabilities. For example, \nChatGPT- 3.5 demonstrated its proficiency \nin medical knowledge by achieving a passing \nscore of 60.2% on multiple- choice questions \nderived from the United States Medical \nLicensing Exams (USMLE), while Google’s \nMed- PaLM 2 (an LLM specifically trained on \nmedical datasets) achieved an even higher \nscore of 86.5% in the same test. 5 Emerging \nresearch highlights the rapidly evolving capa-\nbilities of LLMs, with the latest ChatGPT \nversion (GPT- 4) achieving an accuracy of \n100% in another USMLE sample exam. 6 All \nof these findings suggest two notable points. \nFirst, these generative AI chatbots already \npossess sufficient medical knowledge for \nconceivable applications in medicine, poten-\ntially causing a paradigm shift in healthcare \ndelivery.4 Second, considerable uncertainty \npersists regarding the inner mechanics \nthrough which these models process their \ntraining data to produce the intended \noutput, raising concerns that LLMs could be \nmanipulated maliciously to generate harmful \nor misleading content.7\nWhile numerous studies have examined \nthe broad applications of LLMs in medi-\ncine and healthcare, there is, to the best of \nthe author’s knowledge, a lack of research \nassessing the potential benefits, limitations \nand factors to consider when it comes to \nintegrating LLMs into primary care. Given \nthat primary care usually serves as the first \nlevel of contact a patient has with the health-\ncare system, there is significant potential to \nleverage the transformative power of LLMs to \nenhance the delivery of primary healthcare \nservices for the benefit of both patients and \npractitioners. It is hoped that the findings \nof this article will offer valuable guidance to \nFamily Medicine and Community Health: first published as 10.1136/fmch-2023-002602 on 30 January 2024. Downloaded from https://fmch.bmj.com on 5 November 2025 by guest.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n\n2\nAndrew A. Fam Med Community Health 2024;12:e002602. doi:10.1136/fmch-2023-002602\nOpen access \nprimary care clinicians interested in efficiently and effec-\ntively deploying LLM technology into their practices.\nThis article begins by exploring the history and devel-\nopment of conversational agents as well as examining the \nmechanics of LLMs. Subsequently, it provides a summary \nof both the practical applications and limitations of LLMs. \nFinally, it delves into the implications of these technolo-\ngies for primary care clinicians.\nHISTORY: FROM SIMPLE CONVERSATIONAL AGENTS TO LLMS\nIn essence, chatbots are user- friendly interactive \ncomputer programmes designed to engage in human- \nlike conversations, facilitating users’ access to informa-\ntion and services.8 Even before the widespread availability \nof advanced generative AI chatbots like ChatGPT or \nBard, basic conversational chatbots were already used \nacross various healthcare domains to assist with the provi-\nsion of healthcare services. These applications included \nconducting patient satisfaction surveys, screening and \ntriaging medical conditions, and assisting with medical \neducation.9–11 As part of the public health response to \nthe COVID- 19 pandemic, there was a swift increase in \nthe deployment of conversational chatbots to provide \nreal- time, comprehensible and reliable information \nregarding the COVID- 19. These COVID- 19 related chat-\nbots were designed to serve various purposes, such as \nproviding vaccine education, debunking misinformation \nthrough fact- checking, conducting disease surveillance, \nmonitoring at- risk populations and facilitating contact \ntracing.12 However, the constraints of these early chatbot \niterations were quite evident, with varied evidence \nregarding their effectiveness and user satisfaction. These \nlimitations included the inability to provide personalised \nresponses or recommendations, repetitive responses, and \nthe capability to automate only a restricted range of basic \ntasks and queries.13\nFast- forward to the present, and chatbots are arguably at \nthe forefront of the AI revolution. The integration of NLP \nand machine learning (ML) algorithms into chatbots has \nsignificantly enhanced their capabilities, allowing them \nto engage in human- like conversations on much more \ncomplex topics. ChatGPT and Bard are examples of LLM- \nbased chatbots that use deep learning techniques and \nextensive datasets to understand and generate conver -\nsational responses from natural language inputs. 4 14  \nMore specifically, both applications are language models \npowered by a transformer- based neural network. 15 The \ntransformer architecture comprises multiple layers, \nknown as ‘transformer blocks’, which process and extract \nfeatures from input data in a hierarchical manner.16 This \nmakes transformer- based language models highly effec-\ntive for NLP tasks, as they can discern patterns in how \nwords and phrases relate to each other, ultimately gener-\nating text that is not only appropriate but also contextu-\nally relevant across a wide range of prompts. 17 Moreover, \nas LLMs expand in size through self- consistency and \nreinforcement learning from human feedback, their \ncapabilities advance and allow these models to learn \nand extract more knowledge from their training data, \nenabling them to perform tasks that they were not explic-\nitly trained for.18 19 This represents a significant advance-\nment not observed in previous iterations of chatbots and \nhighlights the remarkable adaptability of LLMs to meet \nthe evolving challenges in various areas of medicine.\nCLINICAL APPLICATIONS OF LLMS IN PRIMARY CARE\nThis section provides an overview into the key applica-\ntions of LLMs in primary care, highlighting its potential \nto enhance practice management, complement patient–\nphysician interactions and improve clinicians’ diagnostic \ncapabilities.\nPractice management\nThe current healthcare system is burdened by a substantial \namount of paperwork and administrative tasks, diverting \nvaluable time away from healthcare professionals. Physi-\ncians are estimated to spend almost half (49.2%) of their \ntime managing electronic hospital records and handling \nadministrative tasks, while only 27% of their time is \ndevoted to direct clinical interactions with patients. 20 It \nis not surprising that the substantial administrative work-\nload plays a significant role in clinician burnout or influ-\nencing doctors to either avoid a career in family medicine \nor depart from it.21 Therefore, reducing this burden not \nonly serves as a direct and clear method to alleviate stress \nand enhance the well- being of current primary care clini-\ncians but also has the potential to attract more doctors \ninto this specialty.\nLLMs could be used to help primary care clinicians save \ntime on non- clinical duties by automating routine and \nrepetitive medical tasks, ranging from medical data entry \nto efficiently searching and aggregating medical infor -\nmation. This automation can result in the generation of \ncomprehensive summaries that encompass a wide variety \nof information, including the patient’s medical history, \npotential diagnosis and available treatment options. 22 \nStudies have shown ChatGPT’s ability to produce clear \nand well- structured patient discharge summaries of \nacceptable quality when given an initial prompt. 23 24  By \nautomating these administrative tasks through the use \nof LLMs, there is the potential to minimise delays in \npatient discharge from primary care to secondary care, \nall while maintaining a high level of detail. The integra-\ntion of LLMs into existing primary healthcare electronic \nhealth records (EHRs) systems presents a promising \nopportunity to improve the efficiency and accuracy of \nmedical records. This integration stands as a valuable \nasset, lending support to quality improvement initiatives \nand championing evidence- based practices. In doing \nso, it simplifies existing operational processes, reduces \nadministrative costs and enhances overall cost efficiency \nfor healthcare providers. Although it is conceivable that \nemploying LLMs for managing patient records enhances \noperational efficiency, its impact on the overall quality \nFamily Medicine and Community Health: first published as 10.1136/fmch-2023-002602 on 30 January 2024. Downloaded from https://fmch.bmj.com on 5 November 2025 by guest.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n\n3\nAndrew A. Fam Med Community Health 2024;12:e002602. doi:10.1136/fmch-2023-002602\nOpen access\nof care received by patients remains uncertain. A system-\natic literature review revealed that the use of EHRs \nduring medical consultations had a detrimental effect \non doctor–patient communication. 25 Therefore, without \nadequate protocols and training, integrating LLMs into \nEHR systems may continue to undermine the delivery of \npatient- centred care.\nPatient–physician interactions\nBesides reducing the burden of administrative tasks, LLMs \nalso have the potential to facilitate more meaningful inter-\nactions in patient care. Research evaluating the quality \nand empathic nature of responses generated by ChatGPT \nrevealed that when presented with a variety of medical \nqueries, its responses were predominantly accurate and \nexhibited a higher level of empathy compared with \nresponses from physicians.26 27  Moreover, Google’s Med-  \nPaLM 2, a specialised LLM specifically trained on medical \ndata, has shown immense promise in generating more accu-\nrate and helpful answers to medical questions than tradi-\ntional unspecialised LLMs like ChatGPT.5 Patients typically \nhave numerous questions about their medical condition(s), \nand there is often insufficient time for them to ask questions \nand engage in discussions with their family clinician during \nthe medical appointment. Consequently, many patients \nturn to ‘Dr. Google’ to seek answers, often sifting through \nvast amounts of unfiltered information. Traditional search \nengines like Google can provide a deluge of data, which \ncan be overwhelming and sometimes even misleading. In \ncontrast, LLMs, particularly Google’s Med- PaLM 2, can \noffer a more targeted and personalised approach. By simpli-\nfying complex medical terminology and presenting health \ninformation in a manner that is more accessible, accurate \nand relevant to the patient’s specific medical concerns, \npatients’ may have a better understanding of their illnesses. \nThis improvement in comprehension promotes better \nengagement and adherence to treatment plans, enabling \nclinicians to provide care that aligns with the patient’s prior-\nities and concerns.\nClinical diagnostic support\nLLM technology could also be applied to augment existing \nclinical decision- making processes. Its impact on clinical \ncare is vast and diverse ranging from facilitating an effi-\ncient triaging system to improving patient management.28 \nThe world’s population is ageing, bringing about an even \ngreater number of people living with chronic diseases. 29 \nLLMs could serve as an innovative solution to address the \nincreasing demand for primary care services while maintain \nefficiency and improving the effectiveness of medical care.\nFirst, LLMs can process and analyse vast amounts of \nmedical knowledge and patient- specific data, such as infor-\nmation regarding the patient’s presenting complaints (or \nsymptoms), their medical and family histories, lifestyle \npractices, among other relevant information, to generate \na working diagnosis.28 The current accuracy of ChatGPT \nin forming a differential diagnosis and recommending \nappropriate care management decisions demonstrates its \npotential to assist in streamlining the primary care triage \nprocess.30 31 Accordingly, this initial assessment provided \nby LLMs could play an important role in swiftly identi-\nfying patients displaying ‘red flag’ signs and symptoms, \nenabling clinicians to promptly redirect them to appro-\npriate secondary care.\nSecond, in the medical management of patients, clini-\ncians can rely LLMs to suggest specific diagnostic tests \nbased on the patient’s symptoms, aiding in the thorough \ninvestigation of potential medical conditions. Moreover, \nLLMs, particularly medically trained models, could be \nused by clinicians to offer tailored treatment plans to an \nindividual needs and lifestyles practices. These models \nwould predict patient care trajectories, and identify \npossible treatment complications based on similar cases. \nUltimately, LLMs serve as a crucial tool for primary care \nclinicians, facilitating an accurate and expedited clinical \ndecision- making process.\nBARRIERS THAT IMPEDE THE APPLICATION OF LLMS\nWhile LLMs hold great promise and have the potential \nto revolutionise clinical practice, several barriers impede \ntheir immediate application. In this section, the article \nexplores key limitations of LLM, including its shortcom-\nings in privacy and data security, and risk of reproducing \nfactually incorrect or biased conclusions.\nPrivacy and data security\nSecurity concerns remain a significant obstacle in \nthe immediate application of LLMs into clinical care. \nCurrently, information inputted into publicly available \nLLMs lacks anonymity, thus leaving patient confidentiality \nand privacy unprotected. In response to these concerns, \nseveral hospitals in Perth, Australia, have implemented a \nban on the use of ChatGPT.32 To ensure the accuracy and \ncompleteness of AI- generated responses to requests for \nclinical documentation and patient queries about their \nmedical condition, it is essential to train such language \nmodels on extensive datasets that encompass a broad \nspectrum of health information. However, determining \nthe required scope and diversity of data necessary for AI \nalgorithms to operate effectively is often challenging. 33 \nInadvertently allowing language models to be trained on \nexcessive personal health data could be opportunistically \nused by other parties, such as health insurers, to adjust \ninsurance premiums based on an individual’s health \nstatus.33 This scenario poses a significant risk, as it may \nlead to unforeseen outcomes, particularly unintentional \nidentification of patients through the linking of de- iden-\ntified patient data to identifiable information.\nInaccurate and biased outputs\nPerhaps the most significant drawback of LLMs lies in \ntheir potential to generate inaccurate and biased infor -\nmation. The accuracy of responses generated by these \nmodels depends on the quality of the data that they \nwere trained on. If the training data is biased, potentially \nFamily Medicine and Community Health: first published as 10.1136/fmch-2023-002602 on 30 January 2024. Downloaded from https://fmch.bmj.com on 5 November 2025 by guest.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n\n4\nAndrew A. Fam Med Community Health 2024;12:e002602. doi:10.1136/fmch-2023-002602\nOpen access \ndue to inappropriate sampling methods or data collec-\ntion processes, then the AI algorithms may inadver -\ntently perpetuate this bias, leading to inaccurate and \nmisleading outputs.34 35 Because the content generated by \nLLM- based chatbots may lack extensive validation, users \nare required to rely on their own judgement to assess the \naccuracy of the content. 35 Consequently, there is a real \nrisk that these chatbots may instil patients with a false \nsense of confidence in their understanding of crucial clin-\nical information, leading them to believe that they possess \nmore knowledge than their doctor. Such a scenario can \nresult in misunderstandings and hinder the effective \ndelivery of medical care by causing patients to disregard \nmedical advice. Therefore, it is crucial that the medical \ndatasets used to train LLMs are consistently up- to- date \nand accurate. Achieving this goal requires collaboration \namong technology companies, governments, healthcare \nproviders and expert clinicians across a broad spectrum of \nmedical disciplines. The objective is to establish universal \nguidelines for validating both the quality and type of \nhealth data used in training these language models. It \nalso entails creating a structured framework for ensuring \naccountability and auditability of AI- generated informa-\ntion. This rigorous process is essential to enhancing the \nreliability and trustworthiness of AI- generated responses.\nIMPLICATIONS FOR PRIMARY CARE CLINICIANS\nRegardless of our attitudes towards AI, LLM- powered appli-\ncations have gained widespread popularity and are gradually \nreshaping the practices of the primary care sector. Instead \nof fearing the idea that AI could replace us, we should \nwelcome the opportunities and advantages that such tech-\nnologies bring. This includes opportunities to ease admin-\nistrative burdens and improve the quality of patient care \nand outcomes. However, we must also be mindful of the \nnumerous limitations and shortcomings in the capabilities \nof LLMs that may affect their usability in clinical practice. \nBox 1 provides a summarised overview of important factors \nthat primary care clinicians should consider regarding the \nintegration of LLMs into clincial practice.\nAccuracy and reliability of AI-generated responses\nIt has been acknowledged that certain LLMs, such as \nChatGPT, can produce logically coherent information \nthat may be false or inaccurate. This occurrence, known \nas ‘AI hallucination’, refers to the phenomenon where an \nAI- powered algorithm generates fictional or unsubstan-\ntiated information in response to a query. 36 Moreover, it \nis important to note that the datasets used to train LLMs \nmay be outdated and incomplete. In the case of ChatGPT, \nits training data is based solely on information available \nup until September 2021.37 Hence, clinicians should exer-\ncise caution when using LLMs and refrain from overly \nrelying on advice provided by these applications. Instead, \nthey should use their professional judgement to selec-\ntively choose clinically relevant information and discard \nany that are not.\nGiven that current non- domain specific LLMs like \nChatGPT are not designed to serve as reliable sources of \nmedical information, it would be more prudent for clini-\ncians to use specialised medical domain- specific inter -\nactive interfaces, like Evidencehunt—an AI- powered, \nevidence- based search engine that consolidates clinical \nevidence on specific topics—to assist them in making \nwell- informed clinical decisions. However, it is important \nto note that this tool does not differentiate between \ncontextually relevant and irrelevant clinical evidence. \nIt functions to summarise available articles indexed on \nPubMed, and therefore, its results are meaningful only \nwhen interpreted in specific contexts, yet these contexts \nmay not necessarily align with the unique circumstances \nof the presented patient.\nAI regulation\nClinicians need to evaluate how AI governance frame-\nworks impact the practical applications of such tech-\nnology in their clinical settings. As public interest in \nLLMs continues to grow, attempts to regulate this tech-\nnology are also on the rise. As AI applications become \nmore prominent in healthcare, clinicians must recognise \nthe importance of handling sensitive healthcare data in \naccordance with strict ethical and privacy standards. That \nis, clinicians must ensure that they do not misuse sensi-\ntive health data in any manner that compromises patient \nconfidentiality or violates privacy regulations. To proac-\ntively uphold these principles, clinicians should allocate \nBox 1 Summary of take- home messages regarding the \nimplications of LLMs in clinical practice\nHighlights\n ⇒ Accuracy of outputs: LLMs such as ChatGPT are known to produce \nlogical but factually incorrect outputs. Clinicians must be mindful \nthat current non- domain specific LLMs are not designed to provide \nmedical advice, and any medical interpretations should be fact- \nchecked against prevailing clinical evidence/guidelines.\n ⇒ Regulatory oversight: The unique capabilities and limitations of \nLLMs have illuminated calls for such technologies to be regulated. \nClinicians need to periodically assess the impact of a changing reg-\nulatory environment on the use of AI and its associated technologies \nin clinical practice.\n ⇒ Enhancing professional competence in AI: In order for AI tech-\nnologies, including LLMs, to be successfully integrated into clini-\ncal practice, clinicians at all career stages need to have the skills, \nattitudes and knowledge to use such tools safely and effectively. \nThe creation of an AI- ready workforce requires incorporation of AI \ncompetencies into existing medical and professional development \ncurricula.\n ⇒ Respecting patient preferences: Every patient has distinct views \nand preferences regarding the incorporation of AI in the clinical \ndecision- making process. Clinicians need to proactively communi-\ncate with patients regarding the role of AI in their medical care in-\ncluding responding to patients’ fears and ensuring that their choices \nare respected.\nAI, artificial intelligence; LLMs, large language models.\nFamily Medicine and Community Health: first published as 10.1136/fmch-2023-002602 on 30 January 2024. Downloaded from https://fmch.bmj.com on 5 November 2025 by guest.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n\n5\nAndrew A. Fam Med Community Health 2024;12:e002602. doi:10.1136/fmch-2023-002602\nOpen access\ntime to stay informed about the latest updates in data \nprotection and privacy laws that govern their practice.\nDevelopment of AI competencies\nTo successfully adopt AI- powered technologies in primary \ncare, clinicians at all career stages need the confidence \nand skills to use these emerging tools and keep pace with \nthe rapid developments in this field. As these new technol-\nogies find their way into the hands of our patients, there is \nan urgent need to integrate education about AI technol-\nogies into the existing medical and primary care training \ncurricula. These educational competencies should help \nprimary care clinicians understand the fundamental prin-\nciples and opportunities for AI use in clinical applications \nand cover the risks and challenges of AI use. This is partic-\nularly crucial in addressing concerns related to confi-\ndentiality, consent, and the limited clinical knowledge \nof LLMs in order to ensure the safety of the patient and \nclinician. Clinicians must also be aware of how they can \neffectively communicate with patients regarding their use \nof LLMs- based tools, including supporting and training \npatients on how to critically assess the accuracy and rele-\nvance of AI- generated content. Moreover, since LLMs \ngenerate content based on a user input, they are sensitive \nto how the input text (or prompt) is framed.38 Therefore, \nvariations in the words or phrases used in the prompt can \naffect the quality and accuracy of LLM generated infor -\nmation.38 To ensure the robustness and effective use of \nLLMs, there is also a need to conduct further research in \nprompt engineering. In particular, research would need \nto focus on the reproducibility and reliability of LLM \ngenerated interpretations across different prompt varia-\ntions for the same medical query. In doing so, the medical \nfield can establish universal guidelines that provide clear \nguidance to clinicians and patients on how to construct \nprompts in a way that allows LLMs to perform a diverse \narray of medical tasks safely and effectively. For now, it \nseems that the most appropriate approach for users to \ntake in optimising their prompts would be to experiment \nwith different prompt styles and compare the outputs \nwith the desired results. This process assists users in iden-\ntifying a suitable structure for future prompts related to a \nparticular query.\nPatients’ preferences\nFinally, clinicians must respect their patients’ attitudes \nand preferences towards incorporating AI into health-\ncare decision- making. While AI chatbots promise to \nrevolutionise clinical practice, patients’ trust in AI tech-\nnologies remains low. In a study examining the role \nof AI chatbots in behavioural health, it was found that \ndespite the demonstrated effectiveness of chatbots in \npromoting healthier lifestyles and offering a safe plat-\nform for discussing sensitive topics like sex- related \nissues, drug and alcohol use, less than 50% of partic-\nipants expressed acceptance of their potential future \nuse.39 Additionally, an American survey found that \n60% of American adults would be uncomfortable if \ntheir clinician relied on AI for diagnosis or treatment \nrecommendations. 40 Indeed, these findings highlight \nthe challenges of navigating diverse patient preferences \nwhen it comes to using AI in healthcare. Integrating \nAI tools in clinical settings raises questions about \nbalancing technological advancements with the human \ntouch in healthcare, with concerns that AI could poten-\ntially depersonalise patient interactions. Transparent \ncommunication, ethical considerations, and respect \nfor patient autonomy are crucial elements for fostering \nthe widespread acceptance and effective integration of \nAI into existing healthcare systems. Certainly, there is \na genuine need for more standardised research in this \narea to thoroughly understand both the advantages and \nrisks, fostering a balanced and informed approach to \nincorporating AI into clinical practice.\nCONCLUSION\nIn conclusion, the potential impact of LLMs and subse-\nquent AI- based technologies in primary care is both \nvast and transformational. These innovations stand to \nrevolutionise healthcare delivery, introducing solutions \nthat can help clinicians make better informed clin-\nical decisions, reduce their administrative burden and \nimprove patient outcomes. However, to realise these \nbenefits, researchers and practitioners must be mindful \nof the challenges and risks associated with their usage. \nIt is imperative to proceed with caution and implement \nproactive measures to address potential threats, aiming \nto avoid unintended consequences that may compro-\nmise the safety and quality of patient care. Regardless \nof our stance, the progression of AI chatbots in primary \ncare is inevitable. The most prudent approach is to \nactively embrace and educate ourselves about the capa-\nbilities of AI and leverage them for the improvement of \nhealthcare delivery.\nContributors AA conceptualised, drafted the initial manuscript and revised the \nmanuscript for final submission. The author agrees to be accountable for all aspects \nof the work.\nFunding The author has not declared a specific grant for this research from any \nfunding agency in the public, commercial or not- for- profit sectors.\nCompeting interests None declared.\nPatient and public involvement Patients and/or the public were not involved in \nthe design, or conduct, or reporting, or dissemination plans of this research.\nPatient consent for publication Not applicable.\nEthics approval Not applicable.\nProvenance and peer review Not commissioned; externally peer reviewed.\nOpen access This is an open access article distributed in accordance with \nthe Creative Commons Attribution Non Commercial (CC BY- NC 4.0) license, \nwhich permits others to distribute, remix, adapt, build upon this work non-  \ncommercially, and license their derivative works on different terms, provided the \noriginal work is properly cited, appropriate credit is given, any changes made \nindicated, and the use is non- commercial. See: http://creativecommons.org/  \nlicenses/by-nc/4.0/.\nORCID iD\nAlbert Andrew http://orcid.org/0000-0002-9451-4533\nFamily Medicine and Community Health: first published as 10.1136/fmch-2023-002602 on 30 January 2024. Downloaded from https://fmch.bmj.com on 5 November 2025 by guest.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n\n6\nAndrew A. Fam Med Community Health 2024;12:e002602. doi:10.1136/fmch-2023-002602\nOpen access \nREFERENCES\n 1 Pettit RW, Fullem R, Cheng C, et al. Artificial intelligence, machine \nlearning, and deep learning for clinical outcome prediction. Emerg \nTop Life Sci 2021;5:729–45. \n 2 Lee LIT, Kanthasamy S, Ayyalaraju RS, et al. The current state of \nartificial intelligence in medical imaging and nuclear medicine. BJR \nOpen 2019;1:20190037. \n 3 Zhang A, Xing L, Zou J, et al. Shifting machine learning for \nHealthcare from development to deployment and from models to \ndata. Nat Biomed Eng 2022;6:1330–45. \n 4 Egli A. GPT- 4, and other large language models: the next revolution \nfor clinical microbiology Clin Infect Dis 2023;77:1322–8. \n 5 Singhal K, Tu T, Gottweis J, et al. Towards expert- level medical \nquestion answering with large language models [arXiv:2305.09617 \n[Preprint]]. 2023. Available: https://doi.org/10.48550/arXiv.2305. \n09617\n 6 Brin D, Sorin V, Vaid A, et al. Comparing Chatgpt and GPT- \n4 performance in USMLE soft skill assessments. Sci Rep \n2023;13:16492. \n 7 Mökander J, Schuett J, Kirk HR, et al. Auditing large language \nmodels: a three- layered approach. AI Ethics 2023. \n 8 Wilson L, Marasoiu M. The development and use of Chatbots in \npublic health: scoping review. JMIR Hum Factors 2022;9:e35882. \n 9 Letting fingers do the talking. Computer makes patient satisfaction \nsurveys a snap. Rose medical center, Denver, CO. Profiles Healthc \nMark 1992;48:40–4.\n 10 Håvik R, Wake JD, Flobak E, et al. A conversational interface for self- \nscreening for ADHD in adults. Int j Internet Sci 2019;11551. \n 11 Isaza- Restrepo A, Gómez MT, Cifuentes G, et al. The virtual patient \nas a learning tool: a mixed quantitative qualitative study. BMC Med \nEduc 2018;18:297. \n 12 Amiri P , Karahanna E. Chatbot use cases in the COVID- 19 public \nhealth response. J Am Med Inform Assoc 2022;29:1000–10. \n 13 Milne- Ives M, de Cock C, Lim E, et al. The effectiveness of artificial \nintelligence conversational agents in health care. J Med Internet Res \n2020;22:e20346. \n 14 Schwartz IS, Link KE, Daneshjou R, et al. Black box warning: large \nlanguage models and the future of infectious diseases consultation. \nClin Infect Dis 2023:ciad633. \n 15 Atallah SB, Banda NR, Banda A, et al. How large language models \nincluding generative pre- trained transformer (GPT) 3 and 4 will \nimpact medicine and surgery. Tech Coloproctol 2023;27:609–14. \n 16 Tustumi F , Andreollo NA, Aguilar- Nascimento JE de. Future of the \nlanguage models in Healthcare: the role of Chatgpt. Arq Bras Cir Dig \n2023;36:e1727. \n 17 Giglio AD, Costa M. The use of artificial intelligence to improve the \nscientific writing of non- native english speakers. Rev Assoc Med \nBras (1992) 2023;69. \n 18 Clusmann J, Kolbinger FR, Muti HS, et al. The future landscape \nof large language models in medicine. Commun Med (Lond) \n2023;3:141. \n 19 Huang J, Gu SS, Hou L, et al. Large language models can self- \nimprove [arXiv:2210.11610 [Preprint]]. 2022. Available: https://doi. \norg/10.48550/arXiv.2210.11610\n 20 Sinsky C, Colligan L, Li L, et al. Allocation of physician time in \nambulatory practice: a time and motion study in 4 specialties. Ann \nIntern Med 2016;165:753–60. \n 21 Birkeli CN, Rosta J, Aasland OG, et al. Why are doctors opting out of \ngeneral practice Tidsskr Nor Laegeforen 2020;140. \n 22 Dave T, Athaluri SA, Singh S. Chatgpt in medicine: an overview of its \napplications, advantages, limitations, future prospects, and ethical \nconsiderations. Front Artif Intell 2023;6:1169595. \n 23 Patel SB, Lam K. Chatgpt: the future of discharge summaries. Lancet \nDigit Health 2023;5:e107–8. \n 24 Clough RA, Sparkes WA, Clough OT, et al. Transforming healthcare \ndocumentation: harnessing the potential of AI to generate discharge \nsummaries. BJGP Open 2023:BJGPO.2023.0116. \n 25 Kazmi Z. Effects of exam room EHR use on doctor- patient \ncommunication: a systematic literature review. Inform Prim Care \n2013;21:30–9. \n 26 Ayers JW, Poliak A, Dredze M, et al. Comparing physician and \nartificial intelligence Chatbot responses to patient questions posted \nto a public social media forum. JAMA Intern Med 2023;183:589. \n 27 Goodman RS, Patrinely JR, Stone CA Jr, et al. Accuracy and \nreliability of Chatbot responses to physician questions. JAMA Netw \nOpen 2023;6:e2336483. \n 28 Nashwan AJ, Abujaber AA. Leveraging large language models to \nimprove triage accuracy in emergency departments. J Emerg Nurs \n2023;49:651–3. \n 29 Divo MJ, Martinez CH, Mannino DM. Ageing and the epidemiology of \nmultimorbidity. Eur Respir J 2014;44:1055–68. \n 30 Rao A, Pang M, Kim J, et al. Assessing the utility of Chatgpt \nthroughout the entire clinical Workflow: development and usability \nstudy. J Med Internet Res 2023;25:e48659. \n 31 Hirosawa T, Harada Y , Yokose M, et al. Diagnostic accuracy of \ndifferential- diagnosis lists generated by generative pretrained \ntransformer 3 Chatbot for clinical vignettes with common \nchief complaints: a pilot study. Int J Environ Res Public Health \n2023;20:3378. \n 32 Moodie C. Australian Medical Association calls for national \nregulations around AI in health care. 2023. Available: https://www. \nabc.net.au/news/2023-05-28/ama-calls-for-national-regulations-for- \nai-in-health/102381314 [Accessed 21 Oct 2023].\n 33 McKee M, Wouters OJ. The challenges of regulating artificial \nintelligence in Healthcare comment on \"clinical decision support and \nnew regulatory frameworks for medical devices: are we ready for it? - \na viewpoint paper Int J Health Policy Manag 2023;12:7261. \n 34 Cascella M, Montomoli J, Bellini V, et al. Evaluating the feasibility of \nChatgpt in healthcare: an analysis of multiple clinical and research \nscenarios. J Med Syst 2023;47:33. \n 35 Wang C, Liu S, Yang H, et al. Ethical considerations of using Chatgpt \nin health care. J Med Internet Res 2023;25:e48009. \n 36 Alkaissi H, McFarlane SI. Artificial hallucinations in Chatgpt: \nimplications in scientific writing. Cureus 2023;15:e35179. \n 37 Vaishya R, Misra A, Vaish A. Chatgpt: is this version good for \nHealthcare and research. Diabetes Metab Syndr 2023;17:102744. \n 38 Choi HS, Song JY , Shin KH, et al. Developing prompts from \nlarge language model for extracting clinical information from \npathology and ultrasound reports in breast cancer. Radiat Oncol J \n2023;41:209–16. \n 39 Aggarwal A, Tam CC, Wu D, et al. Artificial intelligence- based \nchatbots for promoting health behavioral changes. J Med Internet \nRes 2023;25:e40789. \n 40 Tyson A, Pasquini G, Spencer A, et al. 60% of Americans would be \nuncomfortable with provider relying on AI in their own health care. \n2023. Available: https://www.pewresearch.org/science/2023/02/22/ \n60-of-americans-would-be-uncomfortable-with-provider-relying- \non-ai-in-their-own-health-care/?utm_medium=email&utm_source= \ntransaction [Accessed 23 Oct 2023].\nFamily Medicine and Community Health: first published as 10.1136/fmch-2023-002602 on 30 January 2024. Downloaded from https://fmch.bmj.com on 5 November 2025 by guest.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n"
}