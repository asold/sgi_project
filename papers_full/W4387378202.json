{
  "title": "Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT",
  "url": "https://openalex.org/W4387378202",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2411445079",
      "name": "Thilo Hagendorff",
      "affiliations": [
        "University of Stuttgart"
      ]
    },
    {
      "id": "https://openalex.org/A2553028188",
      "name": "Sarah Fabi",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2149302410",
      "name": "Michal Kosinski",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2411445079",
      "name": "Thilo Hagendorff",
      "affiliations": [
        "University of Stuttgart"
      ]
    },
    {
      "id": "https://openalex.org/A2553028188",
      "name": "Sarah Fabi",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2149302410",
      "name": "Michal Kosinski",
      "affiliations": [
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6838461927",
    "https://openalex.org/W6633466687",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4292157289",
    "https://openalex.org/W2121001250",
    "https://openalex.org/W2161080627",
    "https://openalex.org/W2039540101",
    "https://openalex.org/W4318919287",
    "https://openalex.org/W2512223493",
    "https://openalex.org/W2747940070",
    "https://openalex.org/W2081056419",
    "https://openalex.org/W2626804490",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W582220661",
    "https://openalex.org/W6962504177"
  ],
  "abstract": "Abstract We design a battery of semantic illusions and cognitive reflection tests, aimed to elicit intuitive yet erroneous responses. We administer these tasks, traditionally used to study reasoning and decision-making in humans, to OpenAI’s generative pre-trained transformer model family. The results show that as the models expand in size and linguistic proficiency they increasingly display human-like intuitive system 1 thinking and associated cognitive errors. This pattern shifts notably with the introduction of ChatGPT models, which tend to respond correctly, avoiding the traps embedded in the tasks. Both ChatGPT-3.5 and 4 utilize the input–output context window to engage in chain-of-thought reasoning, reminiscent of how people use notepads to support their system 2 thinking. Yet, they remain accurate even when prevented from engaging in chain-of-thought reasoning, indicating that their system-1-like next-word generation processes are more accurate than those of older models. Our findings highlight the value of applying psychological methodologies to study large language models, as this can uncover previously undetected emergent characteristics.",
  "full_text": "Nature Computational Science | Volume 3 | October 2023 | 833–838\n 833\nnature computational science\nhttps://doi.org/10.1038/s43588-023-00527-x\nBrief Communication\nHuman-like intuitive behavior and reasoning \nbiases emerged in large language models but \ndisappeared in ChatGPT\nThilo Hagendorff1, Sarah Fabi2 & Michal Kosinski    3 \nWe design a battery of semantic illusions and cognitive reflection tests, \naimed to elicit intuitive yet erroneous responses. We administer these tasks, \ntraditionally used to study reasoning and decision-making in humans, to \nOpenAI’s generative pre-trained transformer model family. The results show \nthat as the models expand in size and linguistic proficiency they increasingly \ndisplay human-like intuitive system 1 thinking and associated cognitive \nerrors. This pattern shifts notably with the introduction of ChatGPT models, \nwhich tend to respond correctly, avoiding the traps embedded in the tasks. \nBoth ChatGPT-3.5 and 4 utilize the input–output context window to engage \nin chain-of-thought reasoning, reminiscent of how people use notepads \nto support their system 2 thinking. Yet, they remain accurate even when \nprevented from engaging in chain-of-thought reasoning, indicating that \ntheir system-1-like next-word generation processes are more accurate \nthan those of older models. Our findings highlight the value of applying \npsychological methodologies to study large language models, as this can \nuncover previously undetected emergent characteristics.\nAs the range of applications for large language models (LLMs) rapidly \nexpands, it is of paramount importance to understand the mechanisms \nthrough which LLMs reason and make decisions. Recent research has \nrevealed that with the increasing complexity of LLMs they exhibit a \nmultitude of skills and properties, some of which were not anticipated \nor intended by their creators1,2. Among these newfound abilities are the \ncapacity to generate computer code, tackle mathematical problems, \nlearn from examples, engage in introspection, carry out multistep rea-\nsoning, solve theory of mind tasks, deceive other agents and a plethora \nof other skills3–6. In this work, we aim to explore reasoning capabilities in \nthe family of generative pre-trained transformer (GPT) models by Ope-\nnAI, while shedding light on the intricacies of their cognitive processes.\nResearch on humans often distinguishes between two broad cat-\negories of reasoning or—more broadly—cognitive processes: systems \n1 and 27,8. System 1 processes are fast, automatic and instinctual. They \noften involve heuristics, or mental shortcuts, which enable quick judg-\nments and decisions without conscious effort. System 1 is essential \nfor everyday functioning, as it allows humans to navigate their envi -\nronments and make rapid decisions with minimal effort. System 2 \nprocesses, on the other hand, are deliberate and require conscious \neffort. This system is employed in logical reasoning, critical thinking \nand problem-solving. System 2 processes are slower and more resource \nintensive, but they are also more accurate and less susceptible to bias.\nOn the surface, current-day LLMs seem to be system 1 thinkers: the \ninput text is processed by consecutive layers of neurons to produce a \ndistribution of probabilities of all possible single-token (word) comple-\ntions. This process is automatic and unidirectional, and involves a single \nwave of propagation through the neural network for each consecutive \npredicted word. Yet, past research and the results presented here sug-\ngest that, like humans, LLMs can also engage in system-2-like cognitive \nprocesses4. While generating each consecutive word, LLMs re-read their \ncontext window, including the task provided by a user, as well as the \nwords they have thus far generated. As a result, LLMs can employ their \ncontext window as a form of an external short-term memory to engage \nReceived: 17 February 2023\nAccepted: 5 September 2023\nPublished online: 5 October 2023\n Check for updates\n1University of Stuttgart, Stuttgart, Germany. 2University of California San Diego, San Diego, CA, USA. 3Stanford University, Stanford, CA, USA.  \n e-mail: michalk@stanford.edu\nNature Computational Science | Volume 3 | October 2023 | 833–838 834\nBrief Communication https://doi.org/10.1038/s43588-023-00527-x\nin chain-of-thought reasoning, re-examine the starting assumptions, \nestimate partial solutions or test alternative approaches. This is akin \nto how people use notepads to solve mathematical problems or write \nessays to sharpen and develop their arguments.\nIn this work, we build on psychological research on human reason-\ning and decision-making to explore system 1 and 2 processes in LLMs. \nWe examine the performance of humans (n = 455) and ten OpenAI LLMs \n(ranging from GPT-1 to ChatGPT-43,9–11) using tasks typically employed to \ntest reasoning and decision-making in humans: cognitive reflection test \n(CRT)12 tasks and semantic illusions13 (see Supplementary Information \nand Supplementary Table 1 for more details). The CRT comprises three \ntypes of mathematical tasks that appear to be simpler than they really \nare, thus triggering an intuitive but incorrect system 1 response. CRT \ntype 1 tasks, such as the widely known ‘ A bat and a ball’ task, use a ‘more \nthan’ phrase to trick participants into subtracting two of the values \nrather than solving a somewhat more complex equation. Type 2 tasks \nexploit people’s tendency to complete a numerical triplet series, such as \nfive machines making five widgets in five minutes because two machines \nmake two widgets in two minutes. Type 3 tasks describe an exponential \nprocess but trick the participants into treating it as linear. Solving CRT \ntasks correctly requires engaging in deliberate system 2 reasoning or \npossessing well developed system 1 intuitions. Semantic illusions are \nquestions containing a disguised error aimed at triggering an intuitive \nbut incorrect system 1 response. In the well known Moses Illusion13, for \nexample, participants tend to be tricked into claiming that Moses took \ntwo animals of each kind on the Ark (when in fact it was Noah).\nWe address some of the limitations of past studies. First, while \npast research focused on a single model (GPT-3), we study reasoning \ncapabilities across a range of models of different sizes and complexi-\nties. Second, as solving the CRT tasks requires mathematical abilities, \nLLMs’ performance could be limited by their mathematical skills. \nT o address this issue, we complement the CRT tasks with semantic \nillusions that do not rely on mathematical skills. Third, past research \nrelied on three CRT tasks copied verbatim from human studies\n14. This \nis problematic, as observing LLMs’ performance on three CRT tasks \ndoes not allow for meaningful statistical comparisons. Moreover, these \ntasks (as well as their solutions) were likely present in the LLMs’ training \ndata. T o circumvent these issues, we designed 50 bespoke versions of \neach type of task (200 in total).\nFor brevity and convenience, we use words such as ‘behavior’ , \n‘intuition’ , ‘deliberation’ or ‘ability’ when referring to LLMs, yet we do \nnot mean to equate artificial intelligence (AI) and human cognitive \nprocesses. While AI’s outputs are often similar to ones produced by \nhumans, it typically operates in fundamentally different ways.\nFirst, we present the results of study 1: the cognitive reflection test. \nT o help the reader interpret the results, we discuss them in the context \nof LLMs’ exemplary responses to one of the CRT tasks (Fig. 1a). The \ncorrect response to this task is ‘59 days’ , but it was designed to appear \neasier than it really is, tempting participants to simply divide the total \ntime by two, triggering an intuitive (but incorrect) response of ‘30 days’ .\nThe performance of humans and LLMs across 150 CRT tasks is \npresented in Fig. 1b. There are four distinct trends. First, most of the \nresponses of early and smaller LLMs (up until GPT-3-curie) were atypi-\ncal. This category includes responses that were evasive (for example, \nGPT-1’s response “a lot”), which indicated failure to comprehend the \ntask (for example, GPT-2XL’s response “The colony would take 60 days \nto double in size”) or which were incorrect in ways different from one \nthat the task was designed to trigger (for example, GPT-3-babbage’s \nresponse: “It would take about 10 days for the cave to be half-filled with \nbats”). Moreover, while 15% of responses of both GPT-3-babbage and \nGPT-3-curie were categorized as correct, they seemed accidental: all but \none were given CRT type 2 tasks, which can be solved by simply repeat-\ning the number mentioned most frequently in the prompt—which these \nmodels tended to naively do in this and other tasks.\n0%\nc\n20% 40% 60% 80% 100%\nChatGPT-4\nChatGPT-3.5\nGPT-3-davinci-003\nGPT-3-davinci-002\nGPT-3-davinci-001\nGPT-3-curie\nGPT-3-babbage\nGPT-3-ada\nGPT-2XL\nGPT-1\nb\nHumans (n = 455)\n(Performance on all CRT tasks)\n0% 20% 40% 60% 80% 100%\nChatGPT-4\nChatGPT-3.5\nGPT-3-davinci-003\n“Let’s use algebra to solve this problem.”\n“Shortest possible answer.”\nLegend, example task and LLMs’ exemplary responses\nCRT Type 3 task 14: In a cave, there is a colony of bats with a daily population \ndoubling. Given that it takes 60 days for the entire cave to be filled with bats, how many \ndays would it take for the cave to be half-filled with bats?\nCorrect responses (e.g., ChatGPT-4: “59 days”) \nIntuitive responses (e.g., GPT-3-davinci-003: “30 days”) \nAtypical responses (e.g., GPT-1: “a lot”)  \nChain-of-thought responses (e.g., ChatGPT-3.5: \n“If the population doubles every day, then the number of bats in the cave after n days is \n2^n times the original population. Let P be the original population of the colony. After 60  \ndays, the population is 2^60 * P, which is the capacity of the cave. To find the number  \nof days it takes for the cave to be half-filled with bats, we need to find the value of n \nsuch that the population is half of the capacity of the cave.   \n0.5 * 2^60 * P = 2^n * P\nDividing both sides by P, we get: 0.5 * 2^60 = 2^n\nTaking the logarithm base 2 of both sides,we get: n = 60 + log2(0.5) = 60–1 = 59.  \nTherefore, it would take 59 days for the cave to be half-filled with bats.”) \na\n(Chain-of-thought reasoning)\nFig. 1 | Human and LLM performance on the CRT tasks. a, Exemplary  \nresponses to one of the CRT tasks, categorized as correct, intuitive  \n(but incorrect) and atypical (that is, all other incorrect responses).  \nWithin each category, the responses that were preceded by written chain- \nof-thought reasoning were additionally labeled as ‘chain-of-thought  \nresponses’ . b, Human and LLM performance on 150 CRT tasks. c, LLMs’  \nresponses when instructed to engage or prevented from engaging in  \nchain-of-thought reasoning. The data source file includes 95% confidence \nintervals.\nNature Computational Science | Volume 3 | October 2023 | 833–838\n 835\nBrief Communication https://doi.org/10.1038/s43588-023-00527-x\nSecond, as the models grew larger and their ability to compre -\nhend the task increased, atypical responses were replaced by intuitive  \n(but incorrect) responses, which the tasks were designed to trigger \n(for example, GPT-3-davinci-003: “30 days”). These constituted below \n5% of responses of early models (up to GPT-3-babbage) and increased \nto 21% for GPT-3-curie (difference (\ntotal price−known price\n2\n) = 16%; \nχ2(1) = 16.98; P < 0.001) and to 70%–90% for the GPT-3-davinci family \n(δ ≥ 49%; χ2(1) ≥ 69.64; P < 0.001), a fraction much higher than observed \nin humans (55%; δ ≥ 15%; χ2(1) ≥ 11.79; P < 0.001).\nIn humans, intuitive but incorrect responses are interpreted as \nevidence of system 1 reasoning and failure to engage system 2, but they \ncould also stem from deliberate—yet erroneous—system 2 reasoning. \nThe generative process behind the LLMs’ responses is less ambiguous. \nAs we discuss in the introduction, current-day LLMs lack the built-in \ncognitive infrastructure necessary to internally engage in system 2 \nprocesses. Consequently, their intuitive responses can only stem from \na system-1-like process.\nImportantly, LLMs’ intuitive responses are unlikely to be driven by \ninsufficient mathematical ability. First, previous research has shown \nthat LLMs can solve basic mathematical problems1. Second, intuitive \nresponses to the CRT type 1 and 3 tasks also require solving a simple \nequation (for example, how much is ‘half of 60’; responding intuitively \nto the CRT type 2 tasks requires no computation). Moreover, as we \nshow in study 3, GPT-3-davinci-003’s performance can be substantially \nimproved by presenting it with training examples.\nThird, LLMs’ strong tendency to respond intuitively stops abruptly \nwith the arrival of ChatGPT. The fraction of correct responses was \nequal to 59% for ChatGPT-3.5 and 96% for ChatGPT-4. This is much \nhigher than the 5% of tasks solved correctly by GPT-3-davinci-003, \nan otherwise very apt model (δ ≥ 54%; χ 2(1) ≥ 102.44; P < 0.001), or \n38% achieved by humans (δ ≥ 21%; χ2(1) ≥ 25.60; P < 0.001). ChatGPT’s \ntendency to respond correctly was accompanied by a substantial drop \nin its tendency to respond intuitively: 15% for ChatGPT-3.5 and 0% for \nChatGPT-4 versus 80% for GPT-3-davinci-003 (δ ≥ 65%; χ2(1) ≥ 125.81; \nP < 0.001) and 55% for humans (δ ≥ 40%; χ2(1) ≥ 86.30; P < 0.001).\nA closer look at ChatGPT models’ responses reveals that the \nmarked increase in their performance was accompanied by a novel \nresponse style. While pre-ChatGPT LLMs responded with brief phrases \nor single sentences (for example, GPT-3-davinci-003: “30 days”), 97% of \nChatGPT-3.5’s responses and 85% of ChatGPT-4.5’s responses included \nsome form of chain-of-thought reasoning (Fig. 1a).\nAs we discussed before, there is nothing deliberate in how Chat-\nGPT-3.5 and other current-day LLMs generate the next word. Yet, each \ntime the word is generated, an LLM re-reads the task and the response \nit has generated so far, combining system-1-like next-word generation \ninto something resembling a system 2 process: generating a strategy \nneeded to solve the task, dividing the task into more tractable subtasks \nand solving them one by one. This is akin to how humans use a notebook \nto solve mathematical tasks without the need to process them in their \nshort-term memory.\nNext, we show that chain-of-thought responses (study 2) not only \nresemble but also serve as system 2 processes, in line with previous \nstudies showing that instructing LLMs to think step by step improves \ntheir ability to solve various tasks\n4.\nWe first show that GPT-3-davinci-003’s accuracy increases when \nit is instructed to engage in chain-of-thought reasoning. We present it \nwith the CRT tasks suffixed with “Let’s use algebra to solve this prob-\nlem” . The results presented in Fig. 1c show that our manipulation was \nsuccessful: the fraction of chain-of-thought responses increased from \n0% in study 1 to 100% (δ = 100%; χ 2(1) = 147.01; P < 0.001). The model \nseemed to design and execute a task-solving strategy. Most of the \ntime, this strategy was poorly conceived or executed, leading to the \nincrease of atypical responses from 15% to 43% (δ = 28%; χ2(1) = 14.72; \nP < 0.001). Yet, in other cases, the strategy was sound, boosting the \nfraction of correct responses from 5% to 28% (δ = 23%; χ 2(1) = 28.20; \nP < 0.001) and reducing the model’s tendency to fall for the trap embed-\nded in the task: intuitive responses dropped from 80% to 29% (δ = 51%; \nχ2(1) = 75.66; P < 0.001).\nNext, we show that preventing the model from engaging in chain-\nof-thought reasoning can decrease its ability to solve the tasks. We \npresented ChatGPT models with the CRT tasks suffixed with “Provide \nthe shortest possible answer (for example, ‘$2’ or ‘1 week’), do not \nexplain your reasoning” . The results presented in Fig. 1 show that our \nmanipulation was again successful: the fraction of chain-of-thought \nresponses fell from 97% to 0% for ChatGPT-3.5 (δ = 97%; χ2(1) = 276.79; \nP < 0.001) and from 84% to 0% for ChatGPT-4 (δ = 84%; χ2(1) = 213.81; \nP < 0.001). The fraction of correct responses did not change for Chat-\nGPT-3.5 (δ = 4%; χ2(1) = 0.47; P = 0.49). For ChatGPT-4, it fell from 95% \nto 88% (δ = 7%; χ2(1) = 4.36; P < 0.05), accompanied by an increase in \nintuitive responses from 0% to 10% (δ = 10%; χ2(1) = 13.75; P < 0.001).\nThe results of study 2 suggest that chain-of-thought reasoning \nhelps LLMs to avoid falling for the traps embedded in the CRT tasks \nand improves their ability to solve them correctly. Yet, they also reveal \nthat ChatGPT models could solve the great majority of the CRT tasks \neven when forced to provide a system-1-like response. This is consistent \nwith ChatGPT-4’s performance in study 1, where it solved 24% of the \nCRT task without using chain-of-thought reasoning.\nIn humans, this would be taken as evidence of a well developed \nintuition stemming from previous exposure to similar tasks15 (although \nthe persistence and size of this effect is disputed 16). Here we show \nresults suggesting that the same applies to LLMs. This is in line with \npast results showing that LLMs can learn, even from a single example3.\nAs ChatGPT models already seem to possess well developed \nintuition, we attempt to improve the system-1-like responses of GPT-\n3-davinci-003 (study 3). We present it with each of the CRT tasks, each \ntime preceding this with 0 to 49 remaining tasks of the same type, \naccompanied by the correct solution. The CRT tasks of the same type \nare semantically very similar, enabling the model to develop system-1 \nintuitions akin to that expressed by the ChatGPT model family.\nThe results presented in Extended Data Fig. 1 show that GPT-3-da-\nvinci-003’s ability to answer correctly (rather than intuitively) increased \nwith each additional example. The fastest gains were observed for the \nCRT type 2 tasks, where the accuracy increased from 2% to 92% after \ntwo examples (δ = 90%; χ2(1) = 77.72; P < 0.001). This is to be expected, \nas they can be solved correctly by simply repeating the duration listed \nin the task. The CRT type 3 tasks, solvable by reporting the total time \nminus one unit, proved to be somewhat more complex: the accuracy \nincreased from 12% to 92% after seven training examples (δ = 80%; \nχ2(1) = 60.94; P < 0.001). It took most examples to develop the model’s \nintuition to solve the CRT type 1 tasks, where the correct answer is equal \nto \ntotal price−known price\n2\n. However, even here, the model’s accuracy \nincreased from 0% to 78% after 30 examples (δ = 78%; χ2(1) = 60.70; \nP < 0.001).\nThe CRT tasks employed in studies 1–3 rely heavily on mathemati-\ncal skills and are highly semantically uniform. T o ensure that the results \ngeneralize beyond the CRT tasks, we replicate studies 1–3 using much \nmore semantically diverse semantic illusions (study 4). Similarly to \nthe CRT tasks, semantic illusions contain a disguised error aimed at \ntriggering an intuitive but incorrect system 1 response. Unlike the CRT \ntasks, semantic illusions do not require mathematical skills, instead \nrelying on participants’ general knowledge.\nT o help the reader interpret the results, we discuss them in  \nthe context of LLMs’ exemplary responses to semantic illusion 47  \n(Fig. 2a). As in the context of the CRT tasks, responses were divided \ninto three categories: intuitive, correct and atypical. The question was \ndesigned to trigger an intuitive system-1 response ‘ Antoni Gaudí’ while \noverlooking the embedded invalid assumption (la Sagrada Família is \nin Barcelona). Importantly, responding ‘ Antoni Gaudí’ can be treated \nNature Computational Science | Volume 3 | October 2023 | 833–838 836\nBrief Communication https://doi.org/10.1038/s43588-023-00527-x\nas indicative of system 1 processing only if the respondent has the \nknowledge necessary to recognize the error. Thus, given an intuitive \nresponse, the model was reset, and its underlying knowledge was tested \nusing an additional question (here “Where is the famous church, la \nSagrada Família, located?”; see Supplementary Information for the \nlist of knowledge questions). Intuitive responses given by LLMs that \nfailed this post hoc test were recategorized as atypical, along with \nresponses revealing a further lack of necessary knowledge (for exam-\nple, GPT-3-babbage: “Francisco Goya”) and nonsensical responses  \n(for example, GPT-1: “the church of san francisco”). Responses recogniz-\ning the invalid assumption were categorized as correct.\nThe results presented in Fig. 2b show a pattern similar to one \nobserved in study 1. Most of the responses of early and smaller LLMs \n(up to GPT-3-babbage) were atypical (gray bars), as they struggled to \ncomprehend the question or lacked the necessary knowledge. As LLMs \ngrew in size and overall ability, the fraction of atypical responses fell \nfrom 52% for GPT-3-babbage to 10% for GPT-3-davinci-003 (δ = 42%; \nχ2(1) = 18.70; P < 0.001). They were replaced by intuitive responses \n(blue bars): GPT-3-davinci-003 fell for the semantic illusion 72% of \nthe time. As in the CRT tasks, this trend changes markedly with the \nintroduction of ChatGPT. The fraction of correct responses increased \nfrom 18% for GPT-3-davinci-003 to 74% and 88% for ChatGPT-3.5 and \nChatGPT-4, respectively (green bars; δ ≥ 56%; χ\n2(1) = 29.35; P < 0.001). \nAs we discussed before, there is nothing deliberate in LLMs’ next-word \ngeneration process, yet this system-1-like process proved to be very \napt at detecting invalid assumptions embedded in semantic illusions.\nThe results of studies 2 and 3 suggest that LLMs’ propensity to \ncommit reasoning errors in the CRT tasks can be reduced by instruct-\ning them to examine the task more carefully and providing them with \nexamples of correct solutions to similar tasks. Study 5 replicates these \nresults in the context of semantic illusions.\nWe first add the suffix “Think carefully and check the question for \ninvalid assumptions” to each semantic illusion and administer them \nto GPT-3-davinci-003. The results presented in Fig. 2c show that the \nfraction of correct responses increased threefold, from 18% in study 4 \nto 54% (δ = 36%; χ2(1) = 12.54; P < 0.001), while the fraction of intuitive \nresponses decreased from 72% to 28% (δ = 44%; χ2(1) = 17.64; P < 0.001).\nNext, as in study 3, we precede each semantic illusion with 0 to 49 \nother semantic illusions, accompanied by the correct solution. The \nresults presented in Extended Data Fig. 1 show that GPT-3-davinci-003’s \nability to answer correctly increased from 18% for zero examples to \nover 64% for ten and more examples (δ ≥ 46%; χ2(1) = 20.01; P < 0.001).\nDiscussion\nOur results reveal an interesting pattern. As LLMs’ ability to compre-\nhend the tasks increases, they tend to fall for the traps embedded in \nthe tasks. This, in humans, would be interpreted as evidence of fast, \nautomatic and instinctual system 1 processing. The most able of the \npre-ChatGPT models, GPT-3-davinci-003, decisively outpaces humans \nin its tendency to respond intuitively rather than correctly. Yet, this \nchanges abruptly with the arrival of ChatGPT models. They responded \ncorrectly to a great majority of tasks, decisively outperforming humans \nin their ability to avoid traps embedded in the tasks.\nHow would we explain pre-ChatGPT models’ tendency to respond \nintuitively, despite their sufficient mathematical abilities and factual \nknowledge demonstrated in studies 3–5? As we discuss in the introduc-\ntion, LLMs lack the cognitive infrastructure necessary to engage in \nsystem 2 processes, which humans may employ when answering such \nquestions. Thus, in the absence of well developed intuition or explicit \nchain-of-thought reasoning, they are particularly prone to fall for the \ntraps embedded in the tasks.\nFurthermore, how would we explain the steep shift in accuracy \nbetween GPT-3 and ChatGPT? ChatGPT models tend to engage in \nchain-of-thought reasoning: the models use their input–output con-\ntext window to develop strategies needed to solve the task, examine \nthe starting assumptions, estimate partial solutions or test alterna-\ntive approaches—in a way akin to how people use notepads to solve \nmathematical problems or write essays to develop their arguments. \nInstructing an older model (that is, GPT-3-davinci-003) to engage in \nchain-of-thought reasoning substantially boosts its performance.\nYet, chain-of-thought reasoning cannot be the sole explanation. \nChatGPT models’ accuracy barely drops when they are prevented from \nengaging in chain-of-thought reasoning. This suggests that they have \nwell developed intuitions enabling them to solve tasks without engag-\ning system-2-like processes. This is confirmed by results showing that \nGPT-3-davinci-003’s performance can be substantially increased by \npresenting it with example tasks and their correct solutions.\nSome progress is to be expected. In humans, the CRT and semantic \nillusions are good predictors of an ability to engage in unbiased, reflec-\ntive and rational decision-making17, as well as overall cognitive abil-\nity12. Thus, LLMs’ ability to solve the CRT and semantic illusions should \nincrease as their overall ability increases. Yet, the shift observed in this \nstudy seems to be steeper than the increase in LLMs’ overall abilities. \n(Performance on all semantic illusions)\n(Examining assumptions)\n“Think carefully and check the question for invalid assumptions.”\nLegend, example task and LLMs’ exemplary responses\nSemantic illusion 47: Which famous artist designed the famous church, la Sagrada\nFamilia, located in Madrid?\nCorrect responses (e.g., ChatGPT-4: “La Sagrada Familia is actually located  \nin Barcelona, not Madrid, and was designed by the famous Spanish architect \nAntoni Gaudí.”) \nIntuitive responses (e.g., GPT-3-davinci-003: “Antoni Gaudí”)  \nAtypical responses (e.g., GPT-3-babbage: “Francisco Goya”)  \nChatGPT-4\nChatGPT-3.5\nGPT-3-davinci-003\nGPT-3-davinci-002\nGPT-3-davinci-001\nGPT-3-curie\nGPT-3-babbage\nGPT-3-ada\nGPT-2XL\nGPT-1\na\nHumans (n = 455)\n0% 20% 40% 60% 80% 100%\n0% 20% 40% 60% 80% 100%\nGPT-3-davinci-003\nb\nc\nFig. 2 | Human and LLM performance on semantic illusions. a. Exemplary \nresponses to one of the semantic illusions, categorized as correct, intuitive and \natypical. b, Human and LLM performance on 50 semantic illusions. c, GPT-3-\ndavinci-003’s responses when instructed to examine the task’s assumptions.  \nThe data source file includes 95% confidence intervals.\nNature Computational Science | Volume 3 | October 2023 | 833–838\n 837\nBrief Communication https://doi.org/10.1038/s43588-023-00527-x\nWe can only speculate on this, given that OpenAI does not provide their \nmodels in open access and only shares limited information on their \ntechnical specification and training process. First, it is unlikely that \nthe shift was driven merely by larger model size. According to OpenAI, \nChatGPT-3.5-turbo was derived from text-davinci-003 by fine-tuning it \nfor chat. The two models are likely of similar sizes. Second, it could be \nthat the shift was driven by the employment of reinforcement learning \nfrom human feedback18,19. In reinforcement learning from human feed-\nback, human-written demonstrations on example prompts are used to \ntrain supervised learning baselines. Next, human ‘ AI trainers’ rank model \noutputs on a larger set of prompts, and a reward model is trained to pre-\ndict their preferences. This reward model is then used to fine-tune the \nmodels using Proximal Policy Optimization algorithms. While reinforce-\nment learning from human feedback has been employed since GPT-3 \ntext-davinci-00219, this procedure was enhanced in ChatGPT training: \nAI trainers played both sides: the user and an AI assistant20. Next, it is \nlikely that ChatGPT models were exposed to sufficient CRT-like tasks \nin their training to be able to respond to them intuitively. Those tasks \nare highly semantically similar and, as illustrated by study 3, exposure \nto training examples can rapidly boost an LLM’s accuracy. This explana-\ntion is less likely in the context of semantic illusions, which are much \nmore irregular and diverse. This question will hopefully be addressed \nby further research or more transparency in LLM development.\nNext to the analysis of LLM performance on reasoning tasks, one \ncan approach the issue from a normative perspective, asking whether \nphenomena of intuitive decision-making are desirable in LLMs. In the \ncognitive science literature, researchers stress that the notion of intui-\ntive errors relies on a normative concept of logics and statistics, which \ncan be inappropriate for real-world situations. Instead, decision-mak-\ning processes should be evaluated in the sense of ‘ecological rationality’ , \nmeaning on the basis of how well they fit the structure of the environ-\nment in which they occur21. In this vein, the CRT tasks as well as semantic \nillusions create a ‘hostile’ test environment, which intentionally aims \nto mislead humans. Should LLMs perhaps go with the conversational \nflow and just ‘overlook’ small mistakes instead of correcting factually \nincorrect questions, as humans tend to do? Or should they insist on \ncorrecting mistakes (as ChatGPT models often did in our study), so as \nto minimize the inaccuracies, ‘hallucinated’ outputs, factual incorrect-\nness and misinformation—a major problem in LLM use?\nWe list a few of the limitations of our study. First, it is limited to \nOpenAI’s GPT family of models. There are many other models, including \nnon-English-language models, whose functioning should be studied. \nSecond, our study was limited to just two types of tasks; future work \nshould examine other tasks or real-world examples. Third, we focused \non LLMs’ observable behavior; it would be useful to study the patterns \nof their neural activations. Finally, many of our tasks—and particularly \nthe CRT type 1 and 2 tasks—were highly schematic. It is possible that \nsome models encountered enough examples in their training to solve \nthem ‘from memory’ .\nThe progress in LLMs not only increased their capabilities, but also \nreduced our ability to anticipate their properties and behavior. It is \nincreasingly difficult to study LLMs through the lenses of their architec-\nture and hyperparameters. Instead, as we show in this work, LLMs can \nbe studied using methods designed to investigate another capable and \nopaque structure, namely the human mind. Our approach falls within \na quickly growing category of studies employing classic psychological \ntests and experiments to probe LLM ‘psychological’ processes, such as \njudgment, decision-making and cognitive biases14,22–24.\nMethods\nTasks\nHypothesis-blind research assistants recruited on Upwork, a freelanc-\ning platform, prepared 50 semantic illusions and 50 CRT type 3 tasks. \nThe CRT type 1 and 2 tasks were generated automatically. All tasks can \nbe found in Supplementary Information.\nTesting LLM performance\nThe tasks were administered to the family of OpenAI GPT models \nranging from GPT-1 to ChatGPT-43,9–11. T o minimize the variance in the \nmodels’ responses and thus increase the replicability of our results, the \n‘temperature’ parameter was set to 0. For ChatGPT models, the default \n(‘You are a helpful assistant. ’) system message was used. The task was \nprefixed by ‘Question:’ and suffixed with ‘\\nAnswer:’ for other models. \nAs specified in the main text, in some experiments, additional suffixes \nwere added to the tasks, such as ‘Let’s use algebra to solve this problem’ . \nThe models’ response length was set to 100 tokens but was extended \nif needed. The responses were trimmed once they started repeating \nthemselves or stopped responding to the task. The LLMs’ responses \nwere reviewed and scored manually.\nTesting human performance\nThe same tasks were also administered to 500 human participants \nrecruited on Prolific.io on 10 June 2023 (50% female). Each partici -\npant was presented with a random set of four tasks (one of each kind) \nfollowed by a control question inquiring whether they used a lan -\nguage model or another external resource; 45 participants responded \npositively and were excluded from the analysis. Human respond -\nents’ performance suggests that our tasks were of similar difficulty \nto those used in past human studies. In the CRT, 38% of responses \nwere correct, compared with 41% in the original study (n  = 3,428)9 \n(δ = 3%; χ2(1) = 3.60; P = 0.06). In semantic illusions, 64% of partici -\npants responded intuitively, compared with 52% in the original study \n(n = 61; they did not report the fraction of correct responses (δ = 12%; \nχ2(1) = 2.41; P = 0.12))13.\nStatistics and reproducibility\nProportions were compared using the prop.test() function in R 25. All \nstatistical tests were two sided. No statistical method was used to pre-\ndetermine the number of tasks. The number of human respondents was \nchosen to enable the detection of small effects (Cohen’s h = 0.2) with \nthe power of 0.8 at the significance level of 0.05. The resulting desired \nnumber of total responses to each test (n  = 196) was multiplied by  \n2.5 to account for potential dropouts.\nEthics\nThe study was executed in strict adherence to ethical guidelines and \nstandards. Our procedures were reviewed and approved by Stanford \nUniversity’s institutional review board. All participants were made \nfully aware of the nature and objectives of the study and provided \ninformed consent.\nReporting summary\nFurther information on research design is available in the Nature Port-\nfolio Reporting Summary linked to this article.\nData availability\nAll datasets are publicly available at https://osf.io/w5vhp. Source data \nfor all figures are available with this Brief Communication.\nCode availability\nOur code is publicly available at https://osf.io/w5vhp (ref.26).\nReferences\n1. Wei, J. et al. Emergent abilities of large language models. \nTransactions on Machine Learning Research (2022).\n2. Schaeffer, R., Miranda, B. & Koyejo, S. Are emergent abilities of \nlarge language models a mirage? Preprint at https://arxiv.org/\nabs/2304.15004 (2023).\n3. Brown, T. B. et al. Language models are few-shot  \nlearners. Preprint at https://arxiv.org/abs/2005.14165  \n(2020).\nNature Computational Science | Volume 3 | October 2023 | 833–838 838\nBrief Communication https://doi.org/10.1038/s43588-023-00527-x\n4. Wei, J. et al. Chain of thought prompting elicits reasoning in \nlarge language models. 36th Conference on Neural Information \nProcessing Systems (2022).\n5. Hagendorff, T. Deception abilities emerged in large language \nmodels. Preprint at https://arxiv.org/abs/2307.16513 (2023).\n6. Kosinski, M. Theory of mind might have spontaneously \nemerged in large language models. Preprint at https://arxiv.org/\nabs/2302.02083 (2023).\n7. Tversky, A. & Kahneman, D. Science 185, 1124–1131 (1974).\n8. Sloman, S. A. Psychol. Bull. 119, 3–22 (1996).\n9. OpenAI. GPT-4 Technical Report. Preprint at https://arxiv.org/\nabs/2303.08774 (2023).\n10. Radford, A., Narasimhan, K., Salimans, T. & Sutskever, I. Improving \nlanguage understanding by generative pre-training. Preprint \nat https://paperswithcode.com/paper/improving-language-\nunderstanding-by (2018).\n11. Radford, A. et al. Language models are unsupervised multitask \nlearners. Preprint at https://paperswithcode.com/paper/\nlanguage-models-are-unsupervised-multitask (2019).\n12. Frederick, S. J. Econ. Perspect. 19, 25–42 (2005).\n13. Erickson, T. D. & Mattson, M. E. J. Verbal Learn. Verbal Behav. 20, \n540–551 (1981).\n14. Binz, M. & Schulz, E. Proc. Natl Acad. Sci. USA 120,  \ne2218523120 (2023).\n15. Stieger, S. & Reips, U.-D. PeerJ 4, e2395 (2016).\n16. Bialek, M. & Pennycook, G. Behav. Res. Methods 50,  \n1953–1959 (2018).\n17. Toplak, M. E., West, R. F. & Stanovich, K. E. Mem. Cogn. 39, \n1275–1289 (2011).\n18. Christiano, P. et al. Deep reinforcement learning from human \npreferences. 31st Conference on Neural Information Processing \nSystems (2017).\n19. Ouyang, L. et al. Training language models to follow instructions \nwith human feedback. 36th Conference on Neural Information \nProcessing Systems (2022).\n20. OpenAI. Introducing ChatGPT. https://openai.com/blog/chatgpt \n(2022).\n21. Todd, P. M. & Gigerenzer, G. Ecological Rationality: Intelligence in \nthe World (Oxford Univ. Press, 2012).\n22. Hagendorff, T. Machine psychology: investigating emergent \ncapabilities and behavior in large language models using \npsychological methods. Preprint at https://arxiv.org/\nabs/2303.13988 (2023).\n23. Jones, E. & Steinhardt, J. Capturing failures of large language \nmodels via human cognitive biases. 36th Conference on Neural \nInformation Processing Systems (2022).\n24. Nye, M., Tessler, M. H., Tenenbaum, J. B. & Lake, B. M. Improving \ncoherence and consistency in neural sequence models with dual-\nsystem, neuro-symbolic reasoning. 35th Conference on Neural \nInformation Processing Systems (2021).\n25. R Core Team. R: a language and environment for statistical \ncomputing. https://www.R-project.org/ (2021).\n26. Hagendorff, T., Fabi, S. & Kosinski, M. Code and author notes for: \nhuman-like intuitive behavior and reasoning biases emerged in \nlarge language models but disappeared in ChatGPT. https://doi.\norg/10.17605/OSF.IO/W5VHP (2023).\nAcknowledgements\nT.H. was supported by the Ministry of Science, Research and Arts \nBaden-Württemberg under Az. 33-7533-9-19/54/5 in Reflecting \nIntelligent Systems for Diversity, Demography, and Democracy \n(IRIS3D) as well as the Interchange Forum for Reflecting on Intelligent \nSystems (IRIS) at the University of Stuttgart. The funders had no role \nin study design, data collection and analysis, decision to publish, or \npreparation of the paper.\nAuthor contributions\nT.H., M.K. and S.F. wrote the paper, collected and analyzed  \nthe data, and interpreted the results. T.H. and M.K. share  \nfirst authorship.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nExtended data is available for this paper at  \nhttps://doi.org/10.1038/s43588-023-00527-x.\nSupplementary information The online version  \ncontains supplementary material available at  \nhttps://doi.org/10.1038/s43588-023-00527-x.\nCorrespondence and requests for materials should be addressed  \nto Michal Kosinski.\nPeer review information Nature Computational Science thanks  \nRyan L. Boyd, Wim De Neys and the other, anonymous, reviewer(s)  \nfor their contribution to the peer review of this work. Primary  \nHandling Editor: Fernando Chirigati, in collaboration with the  \nNature Computational Science team.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with  \nregard to jurisdictional claims in published maps and  \ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons license, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons license, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this license, visit http://creativecommons.\norg/licenses/by/4.0/.\n© The Author(s) 2023\nNature Computational Science\nBrief Communication https://doi.org/10.1038/s43588-023-00527-x\nExtended Data Fig. 1 | Learning curves. Change in the fraction of GPT-3-davinci-003’s correct responses against the number of training examples that the task was \nprefixed with. Error bars represent 95% confidence intervals.\n\n",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6269212365150452
    },
    {
      "name": "Generative grammar",
      "score": 0.6149230003356934
    },
    {
      "name": "Cognition",
      "score": 0.555804431438446
    },
    {
      "name": "Causal reasoning",
      "score": 0.48759880661964417
    },
    {
      "name": "Illusion",
      "score": 0.48567354679107666
    },
    {
      "name": "Cognitive psychology",
      "score": 0.4679611623287201
    },
    {
      "name": "Cognitive science",
      "score": 0.46713685989379883
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4242442846298218
    },
    {
      "name": "Cognitive bias",
      "score": 0.41552987694740295
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35376280546188354
    },
    {
      "name": "Psychology",
      "score": 0.34820348024368286
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I100066346",
      "name": "University of Stuttgart",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I36258959",
      "name": "University of California, San Diego",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ],
  "cited_by": 146
}