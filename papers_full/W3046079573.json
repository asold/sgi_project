{
  "title": "Go Figure! Multi-task transformer-based architecture for metaphor detection using idioms: ETS team in 2020 metaphor shared task",
  "url": "https://openalex.org/W3046079573",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2168380796",
      "name": "Xianyang Chen",
      "affiliations": [
        "Educational Testing Service"
      ]
    },
    {
      "id": null,
      "name": "Chee Wee (Ben) Leong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2130971930",
      "name": "Michael Flor",
      "affiliations": [
        "Educational Testing Service"
      ]
    },
    {
      "id": "https://openalex.org/A2061030007",
      "name": "Beata Beigman Klebanov",
      "affiliations": [
        "Educational Testing Service"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2329727501",
    "https://openalex.org/W3045533700",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2123527642",
    "https://openalex.org/W2949442961",
    "https://openalex.org/W1561412240",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2302864894",
    "https://openalex.org/W2517666421",
    "https://openalex.org/W1574236237",
    "https://openalex.org/W2803972085",
    "https://openalex.org/W2014181162",
    "https://openalex.org/W2971091580",
    "https://openalex.org/W2048408653",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W2515384205",
    "https://openalex.org/W2052417512",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2252273035",
    "https://openalex.org/W2185329034",
    "https://openalex.org/W2033778615",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2806273110",
    "https://openalex.org/W2997958279",
    "https://openalex.org/W624200443",
    "https://openalex.org/W2773022604",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W3004089539",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2013169950",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2971350424",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1988099314",
    "https://openalex.org/W2134657122",
    "https://openalex.org/W2055011070",
    "https://openalex.org/W2806311394",
    "https://openalex.org/W4244900429",
    "https://openalex.org/W1481700956",
    "https://openalex.org/W1502568023",
    "https://openalex.org/W2212730344",
    "https://openalex.org/W1709855680",
    "https://openalex.org/W2165449418",
    "https://openalex.org/W2995738959",
    "https://openalex.org/W2759000768",
    "https://openalex.org/W3121207479",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "This paper describes the ETS entry to the 2020 Metaphor Detection shared task. Our contribution consists of a sequence of experiments using BERT, starting with a baseline, strengthening it by spell-correcting the TOEFL corpus, followed by a multi-task learning setting, where one of the tasks is the token-level metaphor classification as per the shared task, while the other is meant to provide additional training that we hypothesized to be relevant to the main task. In one case, out-of-domain data manually annotated for metaphor is used for the auxiliary task; in the other case, in-domain data automatically annotated for idioms is used for the auxiliary task. Both multi-task experiments yield promising results.",
  "full_text": "Proceedings of the Second Workshop on Figurative Language Processing, pages 235–243\nJuly 9, 2020.c⃝2020 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17\n235\nGo Figure! Multi-task transformer-based architecture for metaphor\ndetection using idioms: ETS team in 2020 metaphor shared task\nXianyang Chen, Chee Wee (Ben) Leong, Michael Flor, and Beata Beigman Klebanov\nEducational Testing Service\nxchen002,cleong,mflor,bbeigmanklebanov@ets.org\nAbstract\nThis paper describes the ETS entry to the 2020\nMetaphor Detection shared task. Our contri-\nbution consists of a sequence of experiments\nusing BERT, starting with a baseline, strength-\nening it by spell-correcting the TOEFL cor-\npus, followed by a multi-task learning set-\nting, where one of the tasks is the token-level\nmetaphor classiﬁcation as per the shared task,\nwhile the other is meant to provide additional\ntraining that we hypothesized to be relevant\nto the main task. In one case, out-of-domain\ndata manually annotated for metaphor is used\nfor the auxiliary task; in the other case, in-\ndomain data automatically annotated for id-\nioms is used for the auxiliary task. Both multi-\ntask experiments yield promising results.\n1 Introduction\nWe use metaphors in our everyday life as a means\nof relating our experiences to other subjects and\ncontexts (Lakoff and Johnson, 2008); it is com-\nmonly used to help us understand the world in a\nstructured way, and oftentimes in an unconscious\nmanner while we speak and write. It sheds light on\nthe unknown using the known, explains the com-\nplex using the simple, and helps us to emphasize\nthe relevant aspects of meaning resulting in effec-\ntive communication.\nThere is a large body of work in the litera-\nture that discusses how metaphor has been used in\nthe context of political communication, marketing,\nmental health, teaching, assessment of English\nproﬁciency, among others (Beigman Klebanov\net al., 2018; Gutierrez et al., 2017; Littlemore\net al., 2013; Thibodeau and Boroditsky, 2011; Ka-\nviani and Hamedi, 2011; Kathpalia and Carmel,\n2011; Landau et al., 2009; Beigman Klebanov\net al., 2008; Zaltman and Zaltman, 2008; Little-\nmore and Low, 2006; Cameron, 2003; Lakoff,\n2010; Billow et al., 1997; Bosman, 1987); see\nchapter 7 in Veale et al. (2016) for a recent review.\nIn the NLP universe, there’s been substantial re-\ncent interest in automated detection of metaphor\n(Dankers et al., 2019; Mikhalkova et al., 2019;\nMao et al., 2019; Igamberdiev and Shin, 2018;\nMarhula et al., 2019; Markert, 2019; Saund et al.,\n2019).\nThis paper describes the ETS entry to the 2020\nMetaphor Detection shared task held as a part of\nthe 2nd Workshop on Processing Figurative Lan-\nguage, at ACL 20201. The shared tasks consists of\nfour tracks: all content parts of speech – nouns,\nverbs, adjectives, and adverbs (AllPOS) and a\nverbs-only track (Verbs) for two corpora – (a) a\ncorpus of well-edited BNC articles from a variety\nof genres annotated using the MIP-VU protocol,\nand (b) a corpus of medium to high quality timed,\nnon-native essays written for the Test of English\nas a Foreign Language annotated under a different\nprotocol. We participated in all the four tracks.\nOur contribution consists of a sequence of\nexperiments using BERT, starting with a base-\nline, then strengthening it by spell-correcting the\nTOEFL corpus (section 4). We then devised a\nmulti-task learning setting, where one of the tasks\nis the token level metaphor classiﬁcation as per the\nshared task, while the other is meant to provide\nadditional training that we hypothesized to be rel-\nevant to the main task (section 5).\nThe ﬁrst multitask learning is the utilization\nout-of-domain data annotated for metaphor, al-\nbeit under a different annotation protocol, by us-\ning data from the other competition corpus. Thus,\nwe use metaphor prediction on the VUA corpus\nas an auxiliary task for the main TOEFL task,\nand vice versa. We show that this setup resulted\nin an improved performance on the TOEFL test\ndata but not on VUA data. A sanity-check experi-\nment where the two training datasets were simply\nmerged together yielded performance that was in-\nferior to the baseline for all tracks (section 5.1).\n1https://sites.google.com/view/ﬁglang2020/home\n236\nThe second auxiliary task is utilization of a large\nin-domain corpus that we automatically tagged for\noccurrence of a different type of ﬁgurative lan-\nguage phenomenon – idioms. We hypothesize that\nthe afﬁnity between the two ways of ﬁguration\nmight help the system become more sensitive to\nmetaphor by learning to attend to idioms. Our\nresults provide support to the hypothesis, as this\nsetting yielded our best result on the VUA dataset\n(section 5.2). We provide a discussion of our ﬁnd-\nings in section 7.\n2 Datasets\n2.1 VUA corpus\nWe use the VU Amsterdam Metaphor Corpus\n(VUA) (Steen et al., 2010) as provided by the\nshared task organizers. The dataset consists of 117\nfragments sampled across four genres from the\nBritish National Corpus: Academic, News, Con-\nversation, and Fiction. Each genre is represented\nby approximately the same number of tokens, al-\nthough the number of texts differs greatly, where\nthe news archive has the largest number of texts.\nThe data is annotated using the MIP-VU proce-\ndure with a strong inter-annotator reliability of\nκ >0.8. It is based on the MIP procedure (Prag-\nglejaz, 2007), extending it to handle metaphoric-\nity through reference (such as marking did as a\nmetaphor in As the weather broke up, so did their\nfriendship) and allow for explicit coding of difﬁ-\ncult cases where a group of annotators could not\narrive at a consensus. Note that we only consid-\nered words marked as metaphors decided as such\nby the shared task organizers. The VUA dataset\nand annotations is the same as the one used in\nthe ﬁrst shared task on metaphor detection (Leong\net al., 2018).\n2.2 TOEFL corpus\nThis data labeled for metaphor was sampled\nfrom the publicly available ETS Corpus of Non-\nNative Written English 2 (Blanchard et al., 2013)\nand was ﬁrst introduced by (Beigman Klebanov\net al., 2018). The annotated data comprises es-\nsay responses to eight persuaisve/argumentative\nprompts, for three native languages of the writer\n(Japanese, Italian, Arabic), and for two proﬁciency\nlevels – medium and high. The data was annotated\nusing the protocol in Beigman Klebanov and Flor\n(2013), that emphasized argumentation-relevant\n2https://catalog.ldc.upenn.edu/LDC2014T06\nmetaphors. Average inter-annotator agreement\nwas κ = 0.56 – 0.62, for multiple passes of the\nannotation (see (Beigman Klebanov et al., 2018)\nfor more details). For the experiments, we used\nthe metaphor annotations marked as such by the\norganizers. We used 180 essays for training and\n60 essays for testing, as provided by the shared\ntask organizers. Tables 1 and 2 show some de-\nscriptive characteristics of the data: the number of\ntexts, sentences, tokens, and class distribution in-\nformation for Verbs and AllPOS tracks for the two\ncorpora – VUA and TOEFL.\n3 Baseline system\nWe build our baseline system based on BERT (De-\nvlin et al., 2018). BERT ( Bidirectional Encoder\nRepresentations from Transformers) is a trans-\nformer (Vaswani et al., 2017) model that is pre-\ntrained on a large quantity of texts, and obtained\nstate-of-the-art performance on many NLP bench-\nmarks (Wang et al., 2018; Zellers et al., 2018; Ra-\njpurkar et al., 2016). Since its introduction, there\nhave been many improvements over the original\nBERT model, such as RoBERTa (Liu et al., 2019),\nERNIE (Sun et al., 2019), XLNet (Yang et al.,\n2019); we use the most basic model ( bert-base-\nuncased).\nWe ﬁne-tune the BERT model as a standard to-\nken classiﬁcation task, that is, after obtaining the\ncontextualized embeddings of a sentence, we ap-\nply a linear layer followed by softmax on each\ntoken to predict whether it is metaphorical or\nnot. Fig 1 shows the architecture of the baseline\nmodel. We tune the hyperparameters based on\ncross-validation on training data. The fold parti-\ntions for the VUA corpus are the same as the ones\nused for experiments in Beigman Klebanov et al.\n(2016). For the TOEFL corpus, we obtained the\nfolds information from the shared task organiz-\ners directly. We select batch size in {16,32,64},\nnumber of training epochs in{2,3,4,5}, and use a\nﬁxed learning rate of 3 ×10−5. We also apply the\nlearning rate scheduler known as slanted triangu-\nlar (Howard and Ruder, 2018). Due to the imbal-\nanced class distribution in our data (see Table 2),\nthe positive class is up-weighted by a factor of 3.\nThe same setting applies to experiments described\nin all the following sections.\n237\nDatasets VUA TOEFL\nTrain Test Train Test\n#texts 90 27 180 60\n#sents 12,123 4,081 2,741 968\nTable 1: Number of texts and sentences for both VUA and TOEFL datasets.\nDatasets VUA TOEFL\nVerbs All POS Verbs All POS\nTrain Test Train Test Train Test Train Test\n#tokens 17,240 5,873 72,611 22,196 7,016 2,301 26,737 9,014\n%M 29% − 18% − 13% − 7% −\nTable 2: Number of tokens and percentage of metaphors breakdown for both VUA and TOEFL datasets, grouped\nby Verbs and AllPOS.\nFigure 1: Baseline system architecture. The output\nis a pair of probabilities – the ﬁrst for class 0 (non-\nmetaphor) and the second for class 1 (metaphor).\n4 Experiment 1: Spell correction system\nProper automatic detection of lexically-anchored\nphenomena in text often depends on availabil-\nity of correct spelling in the text. The contri-\nbution of spelling correction to other tasks has\nbeen documented previously, especially for En-\nglish texts produced by non-native learners of En-\nglish (Rozovskaya and Roth, 2016; Granger and\nWynne, 1999). Essays written by TOEFL test-\ntakers are known to contain a considerable amount\nof spelling errors (Flor et al., 2015). To alleviate\nthis, we used a state-of-the-art automatic spelling\ncorrections system (Flor et al., 2019) to correct\nspelling in the TOEFL dataset. Speciﬁcally, for\nthe training partition of the TOEFL dataset, the\nsystem corrected 1553 errors in 180 essays, and\n510 errors in 60 essays of the test partition.\n5 Multi-task system\nAs we ﬁne-tune BERT on relatively small datasets,\nwe attempted to enrich the learning with partially\nrelevant additional materials through a multi-task\nsetting - adding auxiliary tasks and train the\nmetaphor detection task with them. The auxiliary\ntasks are described in sections 5.1 and 5.2.\nThe model we use for multi-task learning is as\nfollows: Instead of directly making predictions\nbased on the output embeddings of BERT, the em-\nbeddings are ﬁrst projected to a lower-dimensional\nrepresentation by a linear layer; each task then has\nits own classiﬁer on top of that linear layer. The\narchitecture of the model is shown in Fig. 2.\nDuring training, the data in batches of the\nmetaphor task (our main task) and the auxiliary\ntasks are mixed and trained on in an interleaved\nmanner; the speciﬁcs will be described for each\nof the auxiliary tasks separately. In order for the\nmain task to dominate the learning, we also scale\nthe gradient of the auxiliary tasks by a factor of\n0.1. The hyperparameters are selected in the same\nway as described in section 3.\n5.1 Experiment 2: Learning from\nout-of-domain data\nSince both the VUA and the TOEFL corpora are\nannotated for metaphors, using one to help the\nother during learning could potentially provide ad-\nditional relevant training data. However, since\nthe data is from different types of texts and dif-\n238\nFigure 2: Multi-task system architecture. The output\nis a pair of probabilities – the ﬁrst for class 0 (non-\nmetaphor) and the second for class 1 (metaphor).\nferent genres (well-edited BNC text in academic,\nnews, conversation and ﬁction genres vs relatively\nshort English language learner essays), and since\nthe guidelines under which the two datasets were\nannotated are different, it is possible that each\ncorpus is only partially or indirectly relevant to\nthe other. We experimented with both a straight-\nforward merging of the training sets of the two\ndatasets (as a preview – this did not produce good\nresults) and with a multi-task setting where the\nother corpus is used for the auxiliary task.\nWe use the same batch size and learning rate for\nthe main task and the auxiliary task. The batches\nfrom the two tasks are interleaved uniformly; as\nthere are roughly four times more sentences in the\nVUA corpus than in the TOEFL corpus, there are\nﬁve batches of the VUA task following every batch\nof the TOEFL task. We do not sub-sample the\nVUA corpus or over-sample the TOEFL corpus.\n5.2 Experiment 3: Learning from another\ntype of ﬁgurative language\nDifferently from Experiment 2, where we utilized\nan out-of-domain dataset annotated for the same\nphenomenon (albeit under somewhat different an-\nnotation protocol), in Experiment 3 we are at-\ntempting to make use of a different but related phe-\nnomenon – a different type of ﬁgurative language,\nnamely, idioms. The metaphorical underpinnings\nof many idiomatic expressions have been noted\nin psycholinguistic literature (Gibbs and O’Brien,\n1990; Nunberg et al., 1994; Glucksberg, 2001).\nThe main idea here is that one or more of\nthe words participating in idiomatic expressions\nare often used metaphorically. Thus, in CUT-\nTING EDGE both the words are used metaphori-\ncally; in PAY attention, false STEP , helping HAND ,\nGOLDEN opportunity, and social LADDER , the\ncapitalized word is a metaphor while the other\nis not. There are also idioms where none of the\nwords are used metaphorically such as matter of\nfact, other than, and once in a while . Still, it ap-\npears likely that the preponderance of metaphors\nwithin idiomatic expressions would be higher than\nin non-idiomatic language. It is also possible\nthat learning to detect idioms – a different but re-\nlated type of ﬁgurative language – could help with\nmetaphor detection, as these might tend to be used\nin similar contexts. Experiment 3 is an attempt\nto explore these observations by setting idiom de-\ntection as an auxiliary task for the main metaphor\ndetection task.\nAlthough there exists considerable prior re-\nsearch on automatic detection of idioms (for a\nbrief review see Flor and Beigman Klebanov\n(2018)), idiom detection systems are typically\nconstrained to very small sets of idioms or to par-\nticular types of expressions (e.g. verb-noun con-\nstructions). We opted to use a system that marks\ncandidate expressions but does not verify their id-\niomaticity in the given context. The advantage of\nthis particular system is that it has very wide cov-\nerage. We assume that many of the idioms found\nin a particular corpus might be well-known idioms\nthat are listed in various dictionaries. Our system\n(Flor and Beigman Klebanov, 2018) is equipped\nwith a dictionary of about 5000 English idiomatic\nexpressions (culled from Wiktionary), and per-\nforms a ﬂexible search for idioms and their syn-\ntactic and lexical variants in running text. In fact,\nit performs a simultaneous ﬂexible pattern match-\ning. The idiom detection system looks only for\nexpressions that have more than one word, and\nexcludes common greeting phrases (e.g. ’have\na nice day’), phrasal verbs and verb+preposition\nconstructions (unless they are part of a larger id-\niom). The system marks expressions that poten-\ntially might be instances of idioms, but it does\nnot perform idiom/non-idiom classiﬁcation. For\nthe present experiment we used this system with\nrather conservative settings that yielded precision\n239\nof 0.571 in our previous evaluations on a subset of\nthe TOEFL data; see the leftmost column in Fig-\nure 1-A in Flor and Beigman Klebanov (2018) for\nthe details of the conﬁguration. Based on the prior\nevaluation, for this system conﬁguration, most of\nthe errors were cases where the expression is iden-\ntiﬁed correctly but it is used literally rather than\nidiomatically.\nWe ran the idiom-candidate marking system on\nTOEFL-113 essays and on the BNC corpus (ex-\ncluding texts of the shared task). In total, the sys-\ntem detected 3,581 different idiom types in the\nBNC, with 179,967 instances of (candidate) id-\nioms; in the TOEFL-11 data, we found 504 dif-\nferent idiom types, with 3,908 instances. There is\nsomewhat more idiom usage per sentence in the\nBNC than in the TOEFL data: The system identi-\nﬁed an idiom in 3% of all BNC sentences and in\n2.2% for of TOEFL-11 sentences. Table 3 shows\nthe 20 most frequently found (candidate, or un-\nveriﬁed) idioms in the BNC and TOEFL data; the\nlists contain a mix of idioms that contain and do\nnot contain metaphors.\nThe idiom detection auxiliary task is also for-\nmalized as a token classiﬁcation task: Given a\nsentence, predict for each token whether it is part\nof an idiom. Given the size of the BNC cor-\npus, we only sample a small subset of it for train-\ning: 10,000 sentences with idioms and 10,000 sen-\ntences without idioms. For the TOEFL-11 data,\nwe keep all sentences with idioms and sample the\nsame number (3,908) of sentences without idioms.\n6 Results\nTables 4 and 5 show performance of the various\nsystems on AllPOS and Verbs-only tasks, respec-\ntively, for both VUA and TOEFL data. Since it is\nclear that spelling correction is useful for improv-\ning performance on TOEFL data, we used the spell\ncorrected version of the data for all the systems\nfrom experiments 2 and 3 on TOEFL data. Our\nbest-performing systems reported here are also\nbenchmarked against other participating systems\nin the shared task summary report (Leong et al.,\n2020). We obtained a ranking of 2nd and 4th in\nthe VUA and TOEFL tasks, respectively.\nSince VUA data contains well-edited BNC text,\nwe did not run spelling correction on VUA data.\nFor the Verbs tracks, we experimented with both\n(a) training on AllPOS data and evaluating on the\n3https://catalog.ldc.upenn.edu/LDC2014T06\nVerbs-only subset of the test data, and (b) train-\ning and testing on Verbs only subsets. Version (a)\nyielded better results, which are reported here.\n7 Discussion\nFirst, we observe that comparative results across\nthe different systems are highly consistent for All-\nPOS and Verbs-only settings; we therefore focus\non AllPOS in the discussion.\nCombining the training data from TOEFL and\nVUA sets does not result in better performance\non either test set (see D all in Tables 4, 5). This\ncould be due to both out-of-domain nature of the\ntwo corpora with respect to each other, to the dif-\nference in the guidelines under which the two cor-\npora were annotated, and/or to the difference in\nthe distribution of metaphors vs non-metaphors in\nthe two corpora (see Table 2 for class distribution\ninformation).\nHowever, when set up as a multi-task system\nwith a shared representation, using data from VUA\nas part of the training process results in better per-\nformance on TOEFL test data, with a 2.6 points F1\nscore gain for AllPOS (0.666 vs 0.692 in Table 4).\nThus, it appears that using the VUA data as part of\nthe training process through the shared representa-\ntion but without the TOEFL training process sus-\ntaining a loss for mis-classifying instances from\nVUA (as was the case when the training sets were\nmerged), the system has apparently successfully\nacquired useful information that helped boost per-\nformance on TOEFL test data.\nIt is interesting to note that the multi-task ver-\nsion of the setting for using out-of-domain data\ndid not result in improvements on VUA test data\n(F1 score of 0.717 vs 0.715). The drop in re-\ncall and increase in precision observed for the\nDmt model on VUA data is consistent with the\ndirection of the results where the two training\ndatasets were simply merged into a bigger train-\ning dataset (Dall). It appears that under the guide-\nlines in which TOEFL data was annotated where\nargumentation-relevant metaphors are detected in-\ntuitively, without recourse to a standard dictionary,\nthe annotation outcomes are more conservative:\nSome instances that the system trained on VUA\ndata considered metaphorical were not considered\nso in a system that was exposed to TOEFL data\n240\nBNC TOEFL 11\nﬁnd oneself long time\nother than need-to-know\nlong time pay attention\ngreat deal matter of fact\nonce again other than\nups and downs day-to-day\nonce more ﬁnd oneself\nmuch less long run\ncome through stay at home\nold woman play games\nbear in mind great deal\ncup of tea jack of all trades\nday-to-day side effect\nask the question much less\nlet alone change one’s mind\nneed-to-know ask the question\ncommon law again and again\nclose one’s eyes well and good\nblue-eyed tell the truth\nchange one’s mind once again\nTable 3: Top 20 most frequently observed (unveriﬁed)\nidioms in the BNC and TOEFL 11 corpora. Note: Hy-\nphens are treated as between-word delimiters and are\noptionally matched. Thus, both “stay-at-home” and\n“day to day” will be matched, even though these are\nnot the canonical forms of the idioms on the list.\nduring training. For example, the three underlined\nwords in the following sentence were classiﬁed as\nmetaphors by the version that was trained on VUA\ndata only, but were classiﬁed as non-metaphors af-\nter augmentation with the TOEFL data: “A less di-\nrect measure which is applicable only to the most\nsenior management is to observe the fall or rise of\nthe share price when a particular executive leaves\nor joins a company.” Of these, senior and leaves\nare metaphors according to VUA ground truth,\nwhile observe is not. Overall, the drop in recall\nwas not sufﬁciently offset by the increase in pre-\ncision (although there is a small improvement in\nF1 score for the Verbs only data – from 0.756 to\n0.762, see Table 5). Still, our results suggest that\nif one is interested in a precision-focused system,\nusing TOEFL data in a multi-task setting when\ntraining and testing on VUA could be beneﬁcial,\nas D mt achieved the best precision on the VUA\ndataset among all the compared systems.\nWe next turn to the experiments with an aux-\niliary idiom detection task. We observe that on\nVUA data this resulted in a 2-point increase in\nAll POS\nSys- VUA TOEFL\ntem P R F P R F\nBL .721 .713 .718 .701 .563 .624\nSp − − − .656 .676 .666\nDall .728 .676 .701 .576 .637 .605\nDmt .741 .692 .715 .669 .717 .692\nImt .721 .749 .734 .718 .616 .663\nTable 4: AllPOS performance. BL = baseline BERT\nsystem; Sp = baseline BERT system trained and\ntested on spell-corrected TOEFL data; D all = baseline\nBERT system trained on combined TOEFL and VUA\ndata; D mt = a multi-task system using out-of-domain\nmetaphor annotated data; I mt = multi-task system us-\ning idiom detection as an auxiliary task.\nVerbs\nSys- VUA TOEFL\ntem P R F P R F\nBL .725 .790 .756 .624 .694 .657\nSp − − − .674 .694 .684\nDall .747 .733 .740 .614 .664 .638\nDmt .754 .772 .762 .747 .661 .702\nImt .732 .823 .775 .705 .631 .667\nTable 5: Verbs performance. BL = baseline BERT\nsystem; Sp = baseline BERT system trained and\ntested on spell-corrected TOEFL data; D all = baseline\nBERT system trained on combined TOEFL and VUA\ndata; D mt = a multi-task system using out-of-domain\nmetaphor annotated data; I mt = multi-task system us-\ning idiom detection as an auxiliary task.\nF1 score – with no penalty in precision, the sys-\ntem gained about 3.5 points in recall (0.713 vs\n0.749 on AllPOS; 0.790 vs 0.823 on Verbs). This\nconﬁrms the usefulness of attending to a related\ntype of ﬁgurative language through an auxiliary\ntask – even though the identiﬁcation of idioms was\ndone using an automated procedure and therefore\nis quite noisy.\nTo examine the impact of the idiom auxiliary\ntask, we used one of the cross-validation folds\nas development set. Looking at instances tagged\nas non-metaphor by the baseline model and as\nmetaphor by the current model, there are two ob-\nservations. First, of the 236 VUA sentences with\nnewly tagged metaphors, only 9 sentences con-\ntained an idiom, according to our idiom detection\nsystem. Thus, it does not appear to be the case\nthat it is speciﬁcally metaphors within known id-\nioms that the system has now learned to ﬁnd; this\n241\nis a tentative conclusion, however, as it is also pos-\nsible that these sentences did contain idioms that\nwere either not on the list of the 5,000 the system\nis searching for, or are on the list and are present\nin the text but are not detected by the system.\nSecondly, it appears that the system has learned\nsome sentence-level characteristics of sentences\nthat contain ﬁgurative language, in that quite often\nmultiple words in the same sentence got tagged\nas metaphors: the 236 sentences contained 323\nnewly tagged metaphors. The most extreme case\nis that of 4 new words in the same sentence being\ntagged as metaphors (italicized): “This desire that\ncan not ﬁnd its name (though it would dare speak,\nif it could) is pleasurable.”\nUsing idioms for an auxiliary task did not help\nwith TOEFL data. We also tried using the BNC id-\niom data instead of the TOEFL 11 idiom data; this\nresulted in comparable performance, still without\nimprovement over the spell-checked single-task\nversion. Since results on VUA suggest that idioms\ncould provide useful information for metaphor de-\ntection, we intend to further pursue this line of\nwork by attending more closely to the different\ntypes of idiomatic expressions that might be more\nor less useful for metaphor detection, and by im-\nproving the idiom detection mechanism.\n8 Conclusion\nThis paper describes the ETS entry to the 2020\nMetaphor Detection shared task held as a part of\nthe 2nd Workshop on Processing Figurative Lan-\nguage, at ACL 2020. We participated in all four\ntracks – Verbs and AllPOS for each of VUA and\nTOEFL datasets. Our contribution consists of a se-\nquence of experiments using BERT, starting with a\nbaseline, then strengthening it by spell-correcting\nthe TOEFL corpus, followed by a multi-task learn-\ning setting, where one of the tasks is the token-\nlevel metaphor classiﬁcation as per the shared\ntask, while the other is meant to provide additional\ntraining that we hypothesized to be relevant to the\nmain task.\nThe ﬁrst multitask learning is the utilization of\nout-of-domain data annotated for metaphor, albeit\nunder a different annotation protocol, by using\ndata from the other competition corpus; this ma-\nnipulation helped improve F1 scores for metaphor\nclass on TOEFL test data, but not on VUA data.\nThe second auxiliary task is utilization of a large\nin-domain corpus that we automatically tagged\nfor occurrence of a different type of ﬁgurative\nlanguage phenomenon – idioms. This manipula-\ntion resulted in an improved performance on VUA\ndata, but not on TOEFL data. Given the promis-\ning results with idiom auxiliary task, we intend to\ncontinue work in this direction by improving au-\ntomatic detection of idioms and by a ﬁner-grained\nanalysis of the contribution of various types of id-\nioms to improve metaphor detection.\nReferences\nBeata Beigman Klebanov, Daniel Diermeier, and Eyal\nBeigman. 2008. Lexical cohesion analysis of politi-\ncal speech. Political Analysis, 16(4):447–463.\nBeata Beigman Klebanov and Michael Flor. 2013.\nArgumentation-relevant metaphors in test-taker es-\nsays. In Proceedings of the First Workshop on\nMetaphor in NLP, pages 11–20.\nBeata Beigman Klebanov, Chee Wee Leong, and\nMichael Flor. 2018. A corpus of non-native writ-\nten English annotated for metaphor. In Proceed-\nings of the 2018 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 2 (Short Papers) , pages 86–91, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nBeata Beigman Klebanov, Chee Wee Leong, E Dario\nGutierrez, Ekaterina Shutova, and Michael Flor.\n2016. Semantic classiﬁcations for detection of verb\nmetaphors. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), volume 2, pages 101–106.\nRichard M Billow, Jeffrey Rossman, Nona Lewis, De-\nberah Goldman, and Charles Raps. 1997. Observ-\ning expressive and deviant language in schizophre-\nnia. Metaphor and Symbol, 12(3):205–216.\nDaniel Blanchard, Joel Tetreault, Derrick Higgins,\nAoife Cahill, and Martin Chodorow. 2013. Toeﬂ11:\nA corpus of non-native english. ETS Research Re-\nport Series, 2013(2):i–15.\nJan Bosman. 1987. Persuasive effects of political\nmetaphors. Metaphor and Symbol, 2(2):97–113.\nLynne Cameron. 2003. Metaphor in educational dis-\ncourse. A&C Black.\nVerna Dankers, Marek Rei, Martha Lewis, and Eka-\nterina Shutova. 2019. Modelling the interplay of\nmetaphor and emotion through multitask learning.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 2218–\n2229.\n242\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nMichael Flor and Beata Beigman Klebanov. 2018.\nCatching idiomatic expressions in EFL essays. In\nProceedings of the Workshop on Figurative Lan-\nguage Processing, pages 34–44, New Orleans, LA.\nMichael Flor, Michael Fried, and Alla Rozovskaya.\n2019. A benchmark corpus of English misspellings\nand a minimally-supervised model for spelling cor-\nrection. In Proceedings of the Fourteenth Workshop\non Innovative Use of NLP for Building Educational\nApplications, page 76–86, Florence, Italy.\nMichael Flor, Yoko Futagi, Melissa Lopez, and\nMatthew Mulholland. 2015. Patterns of mis-\nspellings in L2 and L1 English: a view from the ETS\nspelling corpus. Bergen Language and Linguistics\nStudies, 6.\nR. W. Gibbs and J. E. O’Brien. 1990. Idioms and men-\ntal imagery: The metaphorical motivation for id-\niomatic meaning. Cognition, 36:35–68.\nSam Glucksberg. 2001. Understanding Figurative\nLanguage: from metaphors to idioms . Oxford Uni-\nversity Press, New York, NY .\nSylviane Granger and Martin Wynne. 1999. Optimis-\ning measures of lexical variation in EFL learner cor-\npora. In Corpora Galore, pages 249–257, Amster-\ndam. Rodopi.\nE Dario Gutierrez, Guillermo Cecchi, Cheryl Corco-\nran, and Philip Corlett. 2017. Using automated\nmetaphor identiﬁcation to aid in detection and pre-\ndiction of ﬁrst-episode schizophrenia. In Proceed-\nings of the 2017 Conference on Empirical Methods\nin Natural Language Processing, pages 2923–2930.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation.\narXiv preprint arXiv:1801.06146.\nTimour Igamberdiev and Hyopil Shin. 2018. Metaphor\nidentiﬁcation with paragraph and word vectoriza-\ntion: An attention-based neural approach. In Pro-\nceedings of the 32nd Paciﬁc Asia Conference on\nLanguage, Information and Computation.\nSujata S Kathpalia and Heah Lee Hah Carmel. 2011.\nMetaphorical competence in ESL student writing.\nRELC Journal, 42(3):273–290.\nHossein Kaviani and Robabeh Hamedi. 2011. A quan-\ntitative/qualitative study on metaphors used by per-\nsian depressed patients. Archives of Psychiatry and\nPsychotherapy, 4(5-13):110.\nGeorge Lakoff. 2010. Moral politics: How liberals and\nconservatives think. University of Chicago Press.\nGeorge Lakoff and Mark Johnson. 2008. Metaphors\nwe live by. University of Chicago press.\nMark J Landau, Daniel Sullivan, and Jeff Green-\nberg. 2009. Evidence that self-relevant motives\nand metaphoric framing interact to inﬂuence polit-\nical and social attitudes. Psychological Science ,\n20(11):1421–1427.\nChee Wee Leong, Beata Beigman Klebanov, Chris\nHamill, Egon Stemle, Rutuja Ubale, and Xianyang\nChen. 2020. A report on the 2020 vua and toeﬂ\nmetaphor detection shared task. In Proceedings of\nthe Second Workshop on Figurative Language Pro-\ncessing, Seattle, W A.\nChee Wee Leong, Beata Beigman Klebanov, and Eka-\nterina Shutova. 2018. A report on the 2018 vua\nmetaphor detection shared task. In Proceedings of\nthe Workshop on Figurative Language Processing ,\npages 56–66.\nJeannette Littlemore, Tina Krennmayr, James Turner,\nand Sarah Turner. 2013. An investigation into\nmetaphor use at different levels of second language\nwriting. Applied linguistics, 35(2):117–144.\nJeannette Littlemore and Graham Low. 2006.\nMetaphoric competence, second language learning,\nand communicative language ability. Applied\nlinguistics, 27(2):268–294.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nRui Mao, Chenghua Lin, and Frank Guerin. 2019.\nEnd-to-end sequential metaphor identiﬁcation in-\nspired by linguistic theories. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 3888–3898.\nJoanna Marhula, Justyna Polak, Maria Janicka, and\nAleksander Wawer. 2019. Recognizing metaphor:\nhow do non-experts and machines deal with a\nmetaphor identiﬁcation task? In BOOK OF AB-\nSTRACTS, page 70.\nKatja Markert. 2019. Literature list ps/hs ss2019 ﬁgu-\nrative language resolution.\nElena Mikhalkova, Nadezhda Ganzherli, Vladislav\nMaraev, Anna Glazkova, and Dmitriy Grigoriev.\n2019. A comparison of algorithms for detection of\n“ﬁgurativeness” in metaphor, irony and puns. In In-\nternational Conference on Analysis of Images, So-\ncial Networks and Texts, pages 186–192. Springer.\nGeoffrey Nunberg, Ivan A. Sag, and Thomas Wasow.\n1994. Idioms. Language, 70(3):491–538.\nPragglejaz. 2007. MIP: A method for identifying\nmetaphorically used words in discourse. Metaphor\nand symbol, 22(1):1–39.\n243\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nAlla Rozovskaya and Dan Roth. 2016. Grammatical\nerror correction: Machine translation and classiﬁers.\nIn Proceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2205–2215, Berlin, Germany.\nAssociation for Computational Linguistics.\nCarolyn Saund, Marion Roth, Mathieu Chollet, and\nStacy Marsella. 2019. Multiple metaphors in\nmetaphoric gesturing. In 2019 8th International\nConference on Affective Computing and Intelligent\nInteraction (ACII), pages 524–530. IEEE.\nGerard J Steen, Aletta G Dorst, J Berenike Herrmann,\nAnna Kaal, Tina Krennmayr, and Trijntje Pasma.\n2010. A method for linguistic metaphor identiﬁca-\ntion: From MIP to MIPVU , volume 14. John Ben-\njamins Publishing.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. Ernie: Enhanced rep-\nresentation through knowledge integration. arXiv\npreprint arXiv:1904.09223.\nPaul H Thibodeau and Lera Boroditsky. 2011.\nMetaphors we think with: The role of metaphor in\nreasoning. PloS one, 6(2):e16782.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nTony Veale, Ekaterina Shutova, and Beata\nBeigman Klebanov. 2016. Metaphor: A com-\nputational perspective. Synthesis Lectures on\nHuman Language Technologies, 9(1):1–160.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5754–5764.\nGerald Zaltman and Lindsay H Zaltman. 2008. Mar-\nketing metaphoria: What deep metaphors reveal\nabout the minds of consumers . Harvard Business\nPress.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and\nYejin Choi. 2018. Swag: A large-scale adversarial\ndataset for grounded commonsense inference. arXiv\npreprint arXiv:1808.05326.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.803704559803009
    },
    {
      "name": "Metaphor",
      "score": 0.7318698763847351
    },
    {
      "name": "Transformer",
      "score": 0.6963685750961304
    },
    {
      "name": "Natural language processing",
      "score": 0.6437403559684753
    },
    {
      "name": "Task (project management)",
      "score": 0.6209356784820557
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5130226016044617
    },
    {
      "name": "Security token",
      "score": 0.4620053470134735
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4396298825740814
    },
    {
      "name": "Speech recognition",
      "score": 0.3847315311431885
    },
    {
      "name": "Linguistics",
      "score": 0.15293747186660767
    },
    {
      "name": "Engineering",
      "score": 0.1073644757270813
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1341030882",
      "name": "Educational Testing Service",
      "country": "US"
    }
  ],
  "cited_by": 26
}