{
  "title": "Genomic language models could transform medicine but not yet",
  "url": "https://openalex.org/W4409574103",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5010108256",
      "name": "Micaela Elisa Consens",
      "affiliations": [
        "University Health Network",
        "University of Toronto",
        "Vector Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5024533924",
      "name": "Ben Li",
      "affiliations": [
        "Artificial Intelligence in Medicine (Canada)",
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A5029443962",
      "name": "Anna R. Poetsch",
      "affiliations": [
        "German Cancer Research Center",
        "National Center for Tumor Diseases",
        "Technische Universität Dresden"
      ]
    },
    {
      "id": "https://openalex.org/A5083800253",
      "name": "Stephen Gilbert",
      "affiliations": [
        "Technische Universität Dresden",
        "University Hospital Carl Gustav Carus"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4407820212",
    "https://openalex.org/W4407733844",
    "https://openalex.org/W4291000127",
    "https://openalex.org/W6877771305",
    "https://openalex.org/W4404821554",
    "https://openalex.org/W4404349982",
    "https://openalex.org/W2081641088",
    "https://openalex.org/W4405995672",
    "https://openalex.org/W4367597910",
    "https://openalex.org/W4388962805",
    "https://openalex.org/W3127238141",
    "https://openalex.org/W4402520149",
    "https://openalex.org/W4400921533",
    "https://openalex.org/W4401752277",
    "https://openalex.org/W4404787535",
    "https://openalex.org/W4390509618",
    "https://openalex.org/W4399698937",
    "https://openalex.org/W4382490702",
    "https://openalex.org/W4382603228"
  ],
  "abstract": null,
  "full_text": "npj |digital medicine News & Views\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-025-01603-4\nGenomic language models could\ntransform medicine but not yet\nMicaela Elisa Consens, Ben Li, Anna R. Poetsch & Stephen Gilbert\n Check for updates\nRecently, a genomic language model (gLM) with\n40 billion parameters known as Evo2 has reached\nthe same scale as the most powerful text large\nlanguage models (LLMs). gLMs have been\nemerging as powerful tools to decode DNA\nsequences over the lastﬁve years. This article\nexamines the emergence of gLMs and highlights\nEvo2 as a milestone in genomic language modeling,\nassessing both the scientiﬁc promise of gLMs and\nthe practical challenges facing their implementation\nin medicine.\nIn February 2025, researchers announced Evo2, a genome language model\n(gLM) trained on over 128,000 genomes, encompassing over 9.3 trillion\nDNA base pairs\n1. This computational scale matches leading text-based\nLLMs, representing a signiﬁcant milestone for genomic AI2. Unlike protein\nlanguage models, which train to understand the 2% of human DNA that is\nencoded into amino acids and folded into proteins, gLMs train to under-\nstand the entire genome\n3. This largely consists of understanding the role of\nthe remaining 98% of human DNA that is non-coding. Non-coding DNA\ncontains crucial regulatory elements that coordinate gene expression across\ndifferent cell types and developmental stages\n4, and the precise mechanisms\ngoverning this regulation are increasingly being unraveled. Thisﬁeld of\nstudy is known as regulatory genomics4, and gLMs have emerged as pro-\nmising tools to study it. The introduction of Evo2 represents both important\nprogress for theﬁeld and highlights critical questions about what these\nmodels learn and how they might be applied. This article examines gLMs in\nthe context of Evo2, highlighting theirpotential for biological research and\nmedicine while exploring the technical barriers and ethical challenges—\nfrom data privacy to dual-use risks— that will shape their clinical future.\nTraining of gLMs\nPre-training is an initial learning phase, where gLMs are trained on large\namounts of DNA sequence data, to learn the underlying patterns and\ngrammar of the genome. Just as human language grammar provides rules\nfor constructing meaningful sentences, genomic grammar consists of pat-\nterns and rules that govern how DNA sequences are shaped by evolution.\ngLM pre-training is typically self-supervised, meaning it is done on data\nwithout labels, and usually as a reconstruction task. A reconstruction task\nrequires the model to learn to“ﬁll in” missing parts of the input data, where\nsuccess is measured by how accurately the model reconstructs the original\nsequence. The Evo2 model trains to predict the next nucleotide in a genomic\nsequence, the same way LLMs train to predict the next words in a sentence.\nTo reconstruct missing genomic data,gLMs like Evo2 compress genomic\ninformation into learned representations that potentially capture the\nsemantic information within DNA sequences. Once learned during pre-\ntraining, these representations can be leveraged during a second phase of\ntraining known asﬁne-tuning. Fine-tuning is typically done on smaller,\nwell-curated, and labeled datasets for speciﬁc biologically relevant tasks like\npredicting regulatory elements (regions involved in coordinating gene\nexpression), segmenting genomic regions (locating the boundaries of\nfunctional regulatory elements), and more\n5. This is a departure from con-\nventional genomic machine learning approaches, which have traditionally\nrelied on supervised learning with task-speciﬁc labeled datasets (such as\nexperimental assay data), whereasgLMs aim to learn universal genomic\nrepresentations that can be adapte d across multiple tasks through\nﬁnetuning5.\nThe current paradigm for training gLMs involves unsupervised pre-\ntraining on as many diverse species’genomes as possible, since the func-\ntional importance of DNA sequences for genes and gene regulation is\nconserved across evolution\n6,7. Evo2 dramatically extends this approach by\ntraining on over 128,000 genomes, compared to the previous largest model\nthat trained on 850 genomes\n6. This evolutionary conservation helps provide\nrecurrent signals from conserved sequence amidst noise from non-\nconserved sequence, as researchers still debate how much of the non-\nconserved genome contributes to gene regulation\n8. Large sections of the\ngenome contain long repetitive sequences with unknown functional sig-\nniﬁcance for gene regulation. Recent gLMs, including Evo2, increase the\nfocus on sequences relevant for gene regulation by employing weighted loss\nschemes that reduce the contribution of repetitive elements during training,\nwhich improves overall performance for related tasks9.\nAnother trend in gLM modeling has been increasing model context\nsize, which is the length of DNA sequences a model can‘see’at once. This is\nan effort to model long-range interactions in the genome, and potentially\neven model the entire human genome at once. Evo2 speciﬁcally adopts an\narchitecture that radically increases its context size compared to most gLMs;\nhandling sequences up to 1 million nucleotides long. While an impressive\nadvancement, this still falls short of the context required for whole human\nchromosomes, which can span hundreds of millions of nucleotides.\nMoreover, there exists a trade-off between context length and interpret-\nability; Evo2’s complex architecture enables its large context window but\nmakes the model more difﬁcult to interpret compared to simpler models\nwith shorter contexts (Table1).\nThe biological and clinical relevance of gLMs\nPre-training gLMs has immense potential for biology through what\nresearchers call‘zero-shot’performance— am o d e l’s ability to perform well\non tasks it wasn’t explicitly trained for. Strong zero-shot performance\nindicates the model has learned fundamental principles about genomic\nstructure that generalize to new scenarios. When a gLM pretrains in a self-\nnpj Digital Medicine|           (2025) 8:212 1\n1234567890():,;\n1234567890():,;\nsupervised manner, it enhances its ability to uncover novel biology inde-\npendent of pre-existing human annotations and expectations. Potentially,\nthis means gLMs with strong self-supervised zero-shot performance have\nuncovered new regulatory grammar within the genome— grammar that we\ncan learn from. Uncovering novel genomic grammar would advance our\nunderstanding of human disease and transform personalized care across all\naspects of medicine. Given that almost all the leading causes of death/\ndisability in the world have animportant genetic component\n10, it is likely\nthat in the future, gLMs could help clinicians estimate the risks of whether a\npatient will develop these diseases, years before their onset, and implement\nappropriate personalized preventive strategies.\nChallenges and opportunities in the clinical adoption\nof gLMs\nDespite Evo2’s impressive scale and capabilities, fundamental questions\nremain about what these models are learning. A critical challenge is deter-\nmining whether gLMs learn contextual relationships within genomic\nsequences or simply memorize patternsfrom training. This evaluation\nchallenge is compounded by two factors: the reliance on simple benchmarks\nfor evaluation and the multi-speciestraining approach. While training on\ndiverse species helps Evo2 and similar models identify functionally\nimportant sequences, it also makes it difﬁcult to distinguish between true\nunderstanding and recall of evolutionarily similar sequences at predic-\ntion time.\nUnderstanding vs memorization. Many gLMs report success on simple\nbenchmarking tasks that fail to capture the complexity of genomic\nregulation\n11,12. These benchmarks, such as distinguishing real genomic\nsequences from randomly generated ones, are used primarily because\nthey’re computationally tractable and provide clear evaluation metrics, but\nthey do not reﬂect the true challenges of interpreting regulatory grammar\nand frequently are driven by DNA sequence motifs which can be learned\nwithout the need to grasp larger context. Designing biologically meaningful\nbenchmarks is challenging, as ground truth labels are often only available\nin small datasets insufﬁcient for model training. Consequently, researchers\ngenerate datasets from less well-validated data and provide synthetic\nrandom sequences as controls to avoid introducing confounding genomic\nsignals. However, this approach often fails to test models on the complex\nregulatory patterns they are ultimately intended to discover.\nResearch on earlier gLMs like DNABERT\n13 revealed they primarily\nlearned sequence patterns through recalling training data rather than\nunderstanding deeper contextual relationships14. Similarly, the GROVER\nmodel, recently described by Sanabria and colleagues (2024), demonstrated\nthat gLMs initially learn token frequencies15, which may inhibit their ability\nto capture complex contextual relationships in genomic data. Sanabria and\ncolleagues15 additionally showed that even a simple model focusing solely on\ntoken frequencies performs well on many benchmarking tasks, supporting\nthe idea that current evaluation methods lack robustness.\nGeneration capabilities and their limitations. Evaluation challenges\nextend beyondﬁne-tuned tasks to the pre-trained capabilities of gLMs.\nEvo2 is pretrained on next nucleotide prediction, which enables it to\ngenerate novel genomic sequences without further training. With its\nimpressive 1 million base pair context window, Evo2 can theoretically\ngenerate entire prokaryotic and simple eukaryotic genomes\n1.A so fn o w ,\ngeneration-evaluation primarily measures the statistical properties of\ngenerated sequences as compared to real genomes using bioinformatic\ntools, rather than assessing their biological viability or function. Impor-\ntantly, none of Evo2’s generated genomes have been synthesized in a\nlaboratory and tested for viability in living cells. Furthermore, many eva-\nluations of Evo2’s generation capabilities resemble recall tests that poten-\ntially measure the model’s ability to reproduce sequences that are\nevolutionarily similar to those in its massive training dataset, rather than\ndemonstrating genuine understanding of genomic grammar.\nWhile these evaluation challenges raise questions about current model\ncapabilities, they don’t diminish gLM's potential. Ultimately, Evo2 and other\ngLMs’generation capabilities are likely to be adopted by biologistsﬁrst for\nresearch purposes before transitioning to clinical applications. This is partly\nbecause these generated sequences require more rigorous evaluation, but\nalso because they offer valuable opportunities to explore DNA beyond\nknown sequences. Synthetic sequencesprovide expanded datasets for test-\ning hypotheses of genomic regulationand could potentially accelerate the\ndevelopment of new drugs/therapies through computational design of DNA\nsequences with desired biological functions.\nTable 1 | Comparison of recent gLMs with multi-species and single-species training approaches\nModel Parameters Sequence length (in bp) Genomes trained on Human genome included Training type\nGPN MSA9 86,000,000 128 100 Yes Multi-species\nGPN20 65,612,800* 512 8 No Multi-species\nEvo21 40,000,000,000 1,000,000 128,000 Yes Multi-species\nNucleotide transformer6 2,500,000,000 6000 850 Yes Multi-species\nDNABERT-221 117,000,000 877 (BPE) 135 Yes Multi-species\nDNABERT13 (k = 6) 110,000,000 512 1 Yes Single-genome\nHyenaDNA22 1,600,000 1,000,000 1 Yes Single-genome\nGROVER15 86,511,201* 2076 (BPE) 1 Yes Single-genome\nThis table compares eight gLMs based on their number of parameters, training data composition, and sequence handling capabilities. Parameter numbers are taken from the paper where possible, where\nindicated by*, the model's parameter number is calculated from loading the HuggingFace model version. Models are categorized by their training approach (multi-species vs single-genome). For sequence\nlength calculation, measurements are in DNA base pairs (bp). When tokens represent multiple bp, the total input length was calculated by multiplyingtokens by bp per token (e.g., nucleotide transformer\nuses non-overlappingk-mers wherek = 6, so 1000 tokens= 6000 bp). DNABERT-2 and GROVER use Byte Pair Encoding (BPE), which has varying length tokens based on the co-occurrence frequency of\nthe characters and a pre-deﬁned vocabulary size. Note that DNABERT-2’s sequence length estimation (877) represents approximately 128 tokens at 6.85 bp average per BPE token (calculated from\nHuggingFace https://huggingface.co/zhihan1996/DNABERT-2-117M/blob/main/tokenizer.json vocabulary excluding special tokens), and GROVER’s sequence length (2076) represents approximately\n510 tokens at 4.07 bp average per BPE token.\nnpj |digital medicine News & Views\nnpj Digital Medicine|           (2025) 8:212 2\nBeyond generation evaluation, some recent gLMs demonstrate\nimpressive zero-shot performance on predicting the effects of non-coding\nvariants, of which the best performing ones include GPN-MSA\n9 and Evo21.\nClinically, this capability could integrate with existing genomic testing\npipelines, ﬂagging potentially pathogenic regulatory variants that current\nscreening methods miss, particularlyfor complex or rare disorders with\nknown genetic components.\nEthical considerations related to the development and\nclinical implementation of gLMs\nBeyond these technical challenges, Evo2 raises important questions\nabout the responsible implement ation of gLMs. The translation of\ngLMs from research to clinical will happen after these models at least\ncapture known and non-trivial genomic signals (beyond dinucleotide\nfrequencies), can help formulat e novel hypotheses about genomic\nfunction, produce sequences with lab-validated biological functions,\nand establish robust performance across diverse genomic contexts and\npopulations. As these models approach practical application, ethical\nconsiderations become increasingly important.\nEvo2’s development involved scaling challenges beyond computa-\ntional resources, including careful decisions about which genomic data to\ninclude in the training set. To reduce potential misuse, the authors excluded\nviral genomes that infect eukaryotic hosts, aiming to prevent the generation\nof harmful infectious agents. This risk management and assessment was\nachieved through collaboration with multidisciplinary experts across health\nsecurity centers, public health and lawschools, and medicine, health policy,\nand biomedical data science departments at major academic institutions\n16,\nsetting an important precedent for theﬁeld. However, despite removing\nviral genomes from the training set, maliciousﬁne-tuning could easily cir-\ncumvent this safety measure by adapting the model to design such genomes\nwith minimal additional data and compute17.\nNew ethical concerns emerge as gLMs advance, particularly around\nprivacy and consent, dual-use risks, and access/equity. Currently, the\n128,000+ genomes Evo2 trained on are open-source, but once gLMs can\naccurately detect clinically relevant DNA variants, they are likely to be\napplied in clinical settings on individual human DNA. In that case, these\nmodels will need to be implemented in such a way that individuals can\nconsent to whole-genome variant-risk screening and maintain privacy over\nboth their genetic data and the predictions gLMs make on patient DNA.\nAdditionally, as these models advance in their ability to generate whole\ngenomes and potentially new organisms, we must consider dual-use sce-\nnarios where legitimate research tools could be repurposed for harmful\napplications like designing new viral infections as biological weapons.\nFurthermore, because it is so difﬁcult to understand how and what these\nmodels learn, misuse of these models due to ignorance is just as, if not more\ndangerous, than repurposing them with malicious intent\n18. Finally, imple-\nmenting whole-genome sequencing for entire populations and imple-\nmenting gLMs to predict on these genomes will be expensive, due to the size\nof large gLMs and the costs of running predictions on them alone. Inte-\ngrating gLMs into existing medical systems, therefore, may have cost bar-\nriers. This could create healthcare systems where advanced, accurate\ngenomic prediction is available only tohigher-income populations, thereby\nexacerbating health inequities. Therefore, it would be prudent to consider\nAI-based regulatory frameworks, such as the one described by Derraz and\ncolleagues (2024) in precision oncology, prioritizing human oversight,\npatient-centeredness, and comprehensive risk assessments in the develop-\nment/implementation of gLMs\n19. Before deployment, principles of AI safety,\ndata privacy, and equity should guide the safe and ethical development of\ngLMs\n20.\nConclusions\nThe future of gLMs is both promising and uncertain. While they could\nt r a n s f o r mm e d i c i n eb yd e c o d i n gt h eg e n o m e’s regulatory mechanisms, their\nfull impact has yet to be realized. Most of the current evaluation strategies for\ngLMs fail to differentiate whether their predictive capabilities are the result\nof true genome comprehension or statistical recapitulation of training\nsequences. However, gLMs’current distance from clinical deployment may\nbe an opportunity, allowing time to establish strategies for their safe and\neffective application to improve human health.\nData availability\nNo datasets were generated or analyzed during the current study.\nMicaela Elisa Consens1,2,3,B e nL i4,5,A n n aR .P o e t s c h6,7 &\nStephen Gilbert8,9\n1Department of Computer Science, University of Toronto, Toronto, ON,\nCanada. 2Vector Institute for Artiﬁcial Intelligence, Toronto, ON, Canada.\n3Peter Munk Cardiac Center, University Health Network, Toronto, ON,\nCanada. 4Division of Vascular Surgery, University of Toronto, Toronto, ON,\nCanada. 5Temerty Centre for Artiﬁcial Intelligence Research and Education\nin Medicine, University of Toronto, Toronto, ON, Canada.6Biomedical\nGenomics, Biotechnology Center, Center for Molecular and Cellular\nBioengineering, Technische Universität, Dresden, Germany.\n7National\nCenter for Tumor Diseases (NCT) partner site Dresden, German Cancer\nResearch Center (DKFZ), Dresden, Germany.\n8Carl Gustav Carus\nUniversity Hospital Dresden, Dresden University of Technology,\nDresden, Germany.\n9Else Kröner Fresenius Center for Digital Health, TUD\nDresden University of Technology, Dresden, Germany.\ne-mail: stephen.gilbert@tu-dresden.de\nReceived: 11 March 2025; Accepted: 31 March 2025;\nReferences\n1. Brixi, G. et al. Genome modeling and design across all domains of life with Evo 2.\n2025.02.18.638918. Preprint athttps://doi.org/10.1101/2025.02.18.638918 (2025).\n2. Callaway, E. Biggest-ever AI biology model writes DNA on demand.Nature 638, 868– 869\n(2025).\n3. Park, E. G. et al. Genomic analyses of non-coding RNAs overlapping transposable elements\nand its implication to human diseases.Int. J. Mol. Sci.23, 8950 (2022).\n4. Alonso, M.E., Pernaute, B., Crespo, M., Gómez-Skarmeta, J.L. & Manzanares, M.\nUnderstanding the regulatory genome.Int. J. Develop. Biol. https://ijdb.ehu.eus/article/\n072428ma (2009).\n5. Consens, M. E. et al. Transformers and genome language models.Nat. Machine Intell. https://\nwww.nature.com/articles/s42256-025-01007-9 (2025).\n6. Dalla-Torre, H. et al. Nucleotide transformer: building and evaluating robust foundation models\nfor human genomics.Nat. Methods22, 287–297 (2025).\n7. Nguyen, E. et al. Sequence modeling and design from molecular to genome scale with Evo.\nScience 386, eado9336 (2024).\n8. Comfort, N. Genetics: we are the 98%.Nature 520, 615– 616 (2015).\n9. Benegas, G., Albors, C., Aw, A.J. et al. A DNA language model based on multispecies\nalignment predicts the effects of genome-wide variants.Nat Biotechnol. https://doi.org/\n10.1038/s41587-024-02511-w (2025).\n10. Health (US), N. I. of & Study, B. S. C. Understanding Human Genetic Variation. inNIH\nCurriculum Supplement Series [Internet](National Institutes of Health, 2007).\n11. Gresova, K., Martinek, V., Cechak, D., Simecek, P. & Alexiou, P. Genomic benchmarks: a\ncollection of datasets for genomic sequence classiﬁcation. BMC Genomic Data24, 25 (2023).\n12. Marin, F. I. et al. BEND: Benchmarking DNA language models on biologically meaningful tasks.\nPreprint athttps://doi.org/10.48550/arXiv.2311.12570 (2024).\n13. Ji, Y., Zhou, Z., Liu, H. & Davuluri, R. V. DNABERT: pre-trained bidirectional encoder\nrepresentations from transformers model for DNA-language in genome.Bioinformatics 37,\n2112– 2120 (2021).\n14. Sanabria, M., Hirsch, J. & Poetsch, A. R. Distinguishing word identity and sequence context in\nDNA language models.BMC Bioinformatics25, 301 (2024).\n15. Sanabria, M., Hirsch, J., Joubert, P. M. & Poetsch, A. R. DNA language model GROVER learns\nsequence context in the human genome.Nat. Mach. Intell.6, 911– 923 (2024).\n16. Bloom ﬁeld, D. et al. AI and biosecurity: the need for governance.Science 385, 831–833 (2024).\nnpj |digital medicine News & Views\nnpj Digital Medicine|           (2025) 8:212 3\n17. Riedemann, L., Labonne, M. & Gilbert, S. The path forward for large language models in\nmedicine is open.Npj Digit. Med.7,1 – 5 (2024).\n18. Poetsch, A. R. KI-Modell analysiert und generiert DNA-Strukturen (SMC, 2024).\n19. Derraz, B. et al. New regulatory thinking is needed for AI-based personalised drug and cell\ntherapies in precision oncology.Npj Precis. Oncol.8,1 –11 (2024).\n20. Harishbhai Tilala, M. et al. Ethical considerations in the use of artiﬁcial intelligence and machine\nlearning in health care: a comprehensive review.Cureus 16, e62443 (2024).\n21. Zhou, Z. et al. DNABERT-2: Efﬁcient Foundation Model and Benchmark For Multi-Species\nGenome. (2023).\n22. Nguyen, E. et al. HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide\nResolution. (2023).\nAcknowledgements\nWe acknowledge the support of the Natural Sciences and Engineering Research Council of Canada\n(NSERC) to M.E.C. and the Schwartz Reisman Institute for Technology and Societyat the University of\nToronto to M.E.C. and B.L.\nAuthor contributions\nM.E.C., B.L., A.R.P., and S.G. developed the concept of the manuscript. M.E.C. wrote theﬁrst draft of\nthe manuscript. B.L., A.R.P., and S.G. contributed to the writing, interpretation of the content, and\nediting of the manuscript, revising it critically for important intellectual content. All authors hadﬁnal\napproval of the completed version and take accountability for all aspects of the work in ensuring that\nquestions related to the accuracy or integrity of any part of the work are appropriately investigated and\nresolved.\nCompeting interests\nM.E.C., B.L., and A.R.P. declare no nonﬁnancial interests and no competingﬁnancial interests. S.G.\ndeclares a nonﬁnancial interest as an Advisory Group member of the EY-coordinated“Study on\nRegulatory Governance and Innovation in theﬁeld of Medical Devices” conducted on behalf of the DG\nSANTE of the European Commission. S.G. is the coordinator of a Bundesministerium für Bildung und\nForschung (BMBF) project (Personal Mastery of Health & Wellness Data, PATH) on consent in health\ndata sharing,ﬁnanced through the European Union NextGenerationEU program. S.G. declares the\nfollowing competingﬁnancial interests: he has or has had consulting relationships with Una Health\nGmbH, Lindus Health Ltd., Flo Ltd, Thymia Ltd., FORUM Institut für Management GmbH, High-Tech\nGründerfonds Management GmbH, Prova Health Ltd., Haleon plc and Ada Health GmbH and holds\nshare options in Ada Health GmbH. S.G. is a News and Views Editor for npj Digital Medicine. S.G.\nplayed no role in the internal review or decision to publish this News and Views article.\nAdditional information\nCorrespondenceand requests for materials should be addressed to Stephen Gilbert.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long\nas you give appropriate credit to the original author(s) and the source,\nprovide a link to the Creative Commons licence, and indicate if changes\nwere made. The images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to\nobtain permission directly from the copyright holder. To view a copy of this\nlicence, visithttp://creativecommons.org/licenses/by/4.0/\n.\n© The Author(s) 2025\nnpj |digital medicine News & Views\nnpj Digital Medicine|           (2025) 8:212 4",
  "topic": "Genomic medicine",
  "concepts": [
    {
      "name": "Genomic medicine",
      "score": 0.4787142276763916
    },
    {
      "name": "Computational biology",
      "score": 0.4511054754257202
    },
    {
      "name": "Computer science",
      "score": 0.3496416211128235
    },
    {
      "name": "Natural language processing",
      "score": 0.3433023989200592
    },
    {
      "name": "Biology",
      "score": 0.3421429693698883
    },
    {
      "name": "Genetics",
      "score": 0.32371240854263306
    },
    {
      "name": "Linguistics",
      "score": 0.32046252489089966
    },
    {
      "name": "Philosophy",
      "score": 0.157878577709198
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1325899441",
      "name": "University Health Network",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I185261750",
      "name": "University of Toronto",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210127509",
      "name": "Vector Institute",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210164862",
      "name": "Artificial Intelligence in Medicine (Canada)",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I17937529",
      "name": "German Cancer Research Center",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210111460",
      "name": "National Center for Tumor Diseases",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I78650965",
      "name": "Technische Universität Dresden",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210162051",
      "name": "University Hospital Carl Gustav Carus",
      "country": "DE"
    }
  ],
  "cited_by": 6
}