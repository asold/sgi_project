{
  "title": "Assessing the efficacy of large language models in generating accurate teacher responses",
  "url": "https://openalex.org/W4385571921",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4382474338",
      "name": "Yann Hicke",
      "affiliations": [
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A3200485077",
      "name": "Abhishek Masand",
      "affiliations": [
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2120139103",
      "name": "Wentao Guo",
      "affiliations": [
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2948423363",
      "name": "Tushaar Gangavarapu",
      "affiliations": [
        "Cornell University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3155584966",
    "https://openalex.org/W2176263492",
    "https://openalex.org/W2963463964",
    "https://openalex.org/W4302305863",
    "https://openalex.org/W2953981431",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3185181255",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4301230920",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W2751124354",
    "https://openalex.org/W4311121803",
    "https://openalex.org/W2964652672",
    "https://openalex.org/W3108740071",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W2736601468",
    "https://openalex.org/W4285778194",
    "https://openalex.org/W2962884827",
    "https://openalex.org/W3034175374",
    "https://openalex.org/W3008309614",
    "https://openalex.org/W3175339114",
    "https://openalex.org/W2107565479",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4385570407"
  ],
  "abstract": "(Tack et al., 2023) organized the shared task hosted by the 18th Workshop on Innovative Use of NLP for Building Educational Applications on generation of teacher language in educational dialogues. Following the structure of the shared task, in this study, we attempt to assess the generative abilities of large language models in providing informative and helpful insights to students, thereby simulating the role of a knowledgeable teacher. To this end, we present an extensive evaluation of several benchmarking generative models, including GPT-4 (few-shot, in-context learning), fine-tuned GPT-2, and fine-tuned DialoGPT. Additionally, to optimize for pedagogical quality, we fine-tuned the Flan-T5 model using reinforcement learning. Our experimental findings on the Teacher-Student Chatroom Corpus subset indicate the efficacy of GPT-4 over other fine-tuned models, measured using BERTScore and DialogRPT. We hypothesize that several dataset characteristics, including sampling, representativeness, and dialog completeness, pose significant challenges to fine-tuning, thus contributing to the poor generalizability of the fine-tuned models. Finally, we note the need for these generative models to be evaluated with a metric that relies not only on dialog coherence and matched language modeling distribution but also on the model's ability to showcase pedagogical skills.",
  "full_text": "Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023), pages 745–755\nJuly 13, 2023c⃝2023 Association for Computational Linguistics\nAssessing the efficacy of large language models in generating accurate\nteacher responses\nYann Hicke, Abhishek Masand, Wentao Guo, Tushaar Gangavarapu\nCornell University\nAbstract\n(Tack et al., 2023) organized the shared task\nhosted by the 18th Workshop on Innovative\nUse of NLP for Building Educational Appli-\ncations on generation of teacher language in\neducational dialogues. Following the struc-\nture of the shared task, in this study, we at-\ntempt to assess the generative abilities of large\nlanguage models in providing informative and\nhelpful insights to students, thereby simulat-\ning the role of a knowledgeable teacher. To\nthis end, we present an extensive evaluation of\nseveral benchmarking generative models, in-\ncluding GPT-4 (few-shot, in-context learning),\nfine-tuned GPT-2, and fine-tuned DialoGPT.\nAdditionally, to optimize for pedagogical qual-\nity, we fine-tuned the Flan-T5 model using rein-\nforcement learning. Our experimental findings\non the Teacher-Student Chatroom Corpus sub-\nset indicate the efficacy of GPT-4 over other\nfine-tuned models, measured using BERTScore\nand DialogRPT.\nWe hypothesize that several dataset character-\nistics, including sampling, representativeness,\nand dialog completeness, pose significant chal-\nlenges to fine-tuning, thus contributing to the\npoor generalizability of the fine-tuned models.\nFinally, we note the need for these generative\nmodels to be evaluated with a metric that re-\nlies not only on dialog coherence and matched\nlanguage modeling distribution but also on the\nmodel’s ability to showcase pedagogical skills.\n1 Introduction\nThe advent of powerful open-source generative lan-\nguage models such as GPT-2 (Radford et al., 2019),\nT5 (Raffel et al., 2020), OPT (Zhang et al., 2022),\nBLOOM (Scao et al., 2022), Flan-T5 (Chung\net al., 2022) or LLAMA (Touvron et al., 2023)\nhas led to significant developments in conversa-\ntional agents, opening avenues for various appli-\ncations in education (Wollny et al., 2021). Such\nAI-driven educational dialogues offer the potential\nfor skill improvement and personalized learning\nexperiences, with intelligent tutoring systems in-\ncreasingly gaining traction (Bibauw et al., 2022).\nHowever, deploying AI-based teachers in the edu-\ncational ecosystem demands the careful modeling\nand evaluation of these agents to ensure their capa-\nbility to address critical pedagogical concerns.\n(Tack and Piech, 2022) created the AI teacher\ntest challenge which follows the recommendations\nfrom (Bommasani et al., 2021) (pp. 67-72) stat-\ning that, if we want to put generative models into\npractice as AI teachers, it is imperative to deter-\nmine whether they can (a) speak to students like a\nteacher, (b) understand students, and (c) help stu-\ndents improve their understanding.\nTaking inspiration from the AI teacher test chal-\nlenge which asks whether state-of-the-art genera-\ntive models are good AI teachers, capable of re-\nplying to a student in an educational dialogue this\npaper seeks to investigate the applicability of rein-\nforcement learning (RL) techniques in the genera-\ntion of AI teacher responses within educational di-\nalogues. The AI teacher test challenge emphasizes\nthe need for a systematic evaluation of generative\nmodels to ensure that they can effectively commu-\nnicate with students, comprehend their needs, and\nfacilitate their academic improvement. Can we\nguide the language generator with RL to help it\nfocus on these pedagogical requirements?\n(Tack et al., 2023) organized the shared task hosted\nby the 18th Workshop on Innovative Use of NLP\nfor Building Educational Applications on genera-\ntion of teacher language in educational dialogues.\nFollowing the structure of the shared task, in this\nstudy, we aim to evaluate the potential of com-\nbining state-of-the-art generative language models\nwith reinforcement learning algorithms to generate\nAI teacher responses in the context of real-world\neducational dialogues sourced from the Teacher\nStudent Chatroom Corpus (Caines et al., 2020,\n2022). The natural baselines for the task at hand are\nSOTA closed-source models such as GPT-4, and\n745\nfine-tuned open-source pre-trained models such as\nGPT-2 (Radford et al., 2019). We will evaluate\nthese natural baselines before evaluating fine-tuned\npre-trained models using RL techniques, that opti-\nmize for pedagogical quality.\nBy exploring the role of reinforcement learning in\nguiding the generation of AI teacher responses, we\naim to advance the discourse on the utilization of\nconversational agents in educational settings and\ncontribute innovative ideas to the ongoing shared\ntask on the generation of teacher language in edu-\ncational dialogues at the 18th Workshop on Inno-\nvative Use of NLP for Building Educational Appli-\ncations.\nThe rest of this paper is structured as follows. Sec-\ntion 2 offers a comprehensive review of relevant\nliterature in the areas of AI-driven educational dia-\nlogues and reinforcement learning-based language\ngeneration. Section 3 discusses the analysis and\nprocessing of the dataset prior to conducting any\nlanguage modeling tasks. In Section 4, the pro-\nposed model and its methodology for generating\nAI teacher responses in educational interactions are\nintroduced. Section 5 evaluates the effects of our\napproach on the quality and relevance of the gener-\nated AI teacher responses and highlights key obser-\nvations. Finally, Section 6 concludes the paper and\nexplores potential directions for future research.\n2 Related Work\nA variety of related literature exists in the realm of\nconversational teaching between a student and a\nteacher. In this section, we review several notable\nworks addressing aspects of teacher-student\ndialogues, foundation models, and conversational\ndatasets, which have contributed to the progress\nand understanding of generative models in\neducational contexts.\nTeacher-Student Dialogues\nOne prominent resource in educational dialogues\nis the National Council of Teachers of English\n(NCTE) dataset (Kane, 2015). It includes nu-\nmerous examples of teacher-student interactions,\nwhich can serve as a valuable resource for the\ntraining and evaluation of generative models in an\neducational context.\nThe SimTeacher dataset (Cohen et al., 2020) is\nan assemblage of information obtained through\na \"mixed-reality\" simulation platform. This\nunique environment aids beginner educators in\nhoning essential skills for classroom settings by\nemploying student avatars managed by human\nactors. All aspiring teachers from a prominent\npublic university participate in several brief\nsimulation sessions throughout their educational\npreparation program, focusing on improving\ntheir ability to encourage more profound textual\nunderstanding among students. The original\nresearchers annotated a variable called \"quality of\nfeedback\" within the transcript, determining how\neffectively teachers proactively assist students.\nIn (Chen et al., 2019), we can find a dataset\ncollected from an education technology company\nthat provides on-demand text-based tutoring for\nmath and science. With a mobile application, a\nstudent can take a picture of a problem or write it\ndown and is then connected to a professional tutor\nwho guides the student to solve the problem. The\ndataset represents, after some selection, 108 tutors\nand 1821 students. Each session is associated with\ntwo outcome measures: (1) student satisfaction\nscores (1-5 scale) and (2) a rating by the tutor\nmanager based on an evaluation rubric (0-1 scale).\nFoundation Models\n(Bommasani et al., 2021) provided a comprehen-\nsive analysis of the opportunities and risks of\nfoundation models, including insights into their\nuse in educational applications. They identified\npotential benefits, such as personalized learning\nand accessibility, while also highlighting the major\nrisks, such as unfair biases and the generation of\nharmful content. This work establishes the need\nfor carefully crafted benchmarks and evaluations\nto assess the potential of generative models in\neducation.\nThe AI Teacher Test (Tack and Piech, 2022)\nbuilds on this idea by examining the performance\nof generative models such as GPT3 (Brown\net al., 2020) and Blender (Roller et al., 2020) in\ngenerating appropriate and informative responses\nin a teacher-student dialogue.\nKasneci et al. (Kasneci et al., 2023) conducted an\ninvestigation to understand the effectiveness of\nChatGPT (Team, 2022) as a tool for educational\nsupport. They analyzed the model’s performance\nin a student-tutoring context, examining its\nability to provide accurate, relevant, and engaging\nresponses for learners. By identifying the strengths\nand weaknesses of ChatGPT in this specific setting,\nthey contributed to a better understanding of how\n746\ngenerative models can be successfully deployed\nin educational applications. Our work builds on\nthese foundations by evaluating the potential of\ncombining reinforcement learning with generative\nmodels to enhance the performance of AI teacher\nagents in educational dialogues.\nConversational Uptake\n(Collins, 1982) introduced the concept of uptake\nas a way to comprehend the effectiveness of\nconversational responses in a teacher-student\ndialogue. It laid the groundwork for the evaluation\nof generative models in dialogues by taking into\naccount the relevance and appropriateness of\nmodel-generated responses.\nDemszky et al. (Demszky et al., 2021) further\nexplored the concept of Conversational Uptake\nby proposing metrics to assess the success\nof responses in maintaining and advancing a\nconversation. By applying these metrics to\nAI-generated responses, their work contributes to\nthe evaluation of models in realistic conversation\nsettings, including teacher-student dialogues. Our\nwork attempts to guide the language generation\nprocess with similar goals in mind. We hope\nto find proxies of pedagogical quality through\nNLP metrics such as BERTScore combined with\nDialogRPT.\nWe continue by reviewing the literature utilizing\nreinforcement learning as a guide for language\ngeneration.\nReinforcement Learning for language genera-\ntion\nPolicy gradient-based algorithms and their variants\nhave been widely used in text generation to opti-\nmize sequence-level metrics (Ranzato et al., 2015;\nShen et al., 2015; Norouzi et al., 2016; Pasunuru\nand Bansal, 2018). Off-policy Reinforcement\nLearning (RL) is also commonly used in dialogue\napplications where online interaction with users is\nexpensive (Serban et al., 2017; Jaques et al., 2019).\nThe main difference in our work is that we take\nadvantage of demonstrations and design generic\nreward functions for generation tasks. We extend\nthis concept to educational contexts by employing\nreinforcement learning to guide the generation of\nAI teacher responses in educational dialogues. We\nfocus on optimizing the responses of fine-tuned\ngenerative models based on a reward system\ndesigned to enhance the pedagogical quality of the\ngenerated responses. Recently, Ramamurthy et al.\n(Ramamurthy et al., 2022) explored the efficacy of\nusing RL to optimize language models in several\nnatural language processing tasks, including text\nclassification, sentiment analysis, and language\ngeneration. They developed a library, RL4LMs,\nwhich provides a generic framework for deploying\nRL-based language models for various tasks. We\nbuild on top of the RL4LMs framework by adding\na new task to its existing array of tasks which we\nhope can be added as a standard for any future\nRLHF benchmark.\n3 Data\nThe shared task for BEA 2023 is based on the\nTeacher-Student Chatroom Corpus (TSCC) (Caines\net al., 2020). This corpus comprises data collected\nfrom 102 chatrooms where English as a Second\nLanguage (ESL) teachers interact with students to\nwork on language exercises and assess the students’\nEnglish language proficiency.\n3.1 Data Extraction and Format\nFrom each dialogue in the TSCC, several shorter\npassages were extracted. Each passage is at most\n100 tokens long, consisting of several sequential\nteacher-student turns (i.e., the preceding dialogue\ncontext) and ending with a teacher utterance (i.e.,\nthe reference response). These short passages are\nthe data samples used in this shared task.\nThe data samples are formatted using a JSON\nstructure inspired by the ConvoKit (Chang et al.,\n2020). Each training sample is represented as a\nJSON object with three fields:\n• id: a unique identifier for the sample.\n• utterances: a list of utterances correspond-\ning to the preceding dialogue context. Each\nutterance is a JSON object with a \"text\" field\ncontaining the utterance and a \"speaker\" field\ncontaining a unique label for the speaker.\n• response: a reference response, which corre-\nsponds to the final teacher’s utterance. This\nutterance is a JSON object with a \"text\" field\ncontaining the utterance and a \"speaker\" field\ncontaining a unique label for the speaker.\nEach test sample is represented as a JSON object\nthat uses the same format as the training sample\nbut excludes the reference response. As a result,\neach test sample has two fields:\n747\n• id: a unique identifier for the sample.\n• utterances: a list of utterances, which cor-\nresponds to the preceding dialogue context.\nEach utterance is a JSON object with a \"text\"\nfield containing the utterance and a \"speaker\"\nfield containing a unique label for the speaker.\n3.2 Data Distribution and Characteristics\nThe TSCC corpus is divided into three sets: train,\ndev, and test, each comprising 2747, 305 and 273\nof the samples, respectively. The corpus has 3325\nsamples, and each sample has an average length of\n7.52 turns, with about 7.33 tokens per turn on aver-\nage. Table 1 presents a summary of the statistics of\nthe TSCC corpus across the training, development,\nand testing sets.\nThe TSCC corpus exhibits several characteristics\nthat are specific to educational dialogues and pose\nchallenges to natural language generation models.\nFor instance, the dialogues often include techni-\ncal vocabulary and idiomatic expressions related\nto English language learning. Additionally, the\ndialogues can be highly varied in terms of topic,\ncomplexity, and participant proficiency. Finally, the\ndialogues can include challenging responses which\nare based on out-of-context information, posing\nchallenges for conversational agents. These char-\nacteristics must be taken into consideration when\nselecting and evaluating generative models for the\nTSCC corpus.\n3.3 Data Overlap and Challenges\nIt is worth noting that the released development\nand training sets in the TSCC dataset have some\noverlaps, as individual conversation samples within\nthese sets have been generated by creating chunks\nfrom larger conversations. This overlap may lead\nto potential biases and overfitting when training\nand evaluating models on this dataset. However,\nthe test set for the BEA 2023 shared task is free\nof overlaps, allowing for a more accurate assess-\nment of the model’s performance in generating AI\nteacher responses.\nThe presence of overlaps in the development and\ntraining sets posed a challenge, as models inadver-\ntently learned to predict teacher responses based\non the similarities between the samples rather than\ngenuinely understanding the context and dynamics\nof the teacher-student interaction. It is essential\nto be aware of this issue and devise strategies to\nmitigate the risks associated with such overlaps and\nensure that the models are robust and capable of\nhandling diverse and unseen scenarios.\nTo ensure the validity of our model on the vali-\ndation set, we employed an iterative inclusion pro-\ncess to create a train-val split without any overlap\nbetween them. This process involved carefully se-\nlecting and excluding samples from the training set\nthat had any similarity or overlap with the samples\nin the development set. This approach aimed to\nminimize the risk of data leakage and ensure that\nour model was evaluated on a truly unseen set of\ndialogues.\n4 Methods\nThe primary objective of our study is to investigate\nthe potential of using in-context learning, super-\nvised fine-tuning, and reinforcement learning to\ngenerate AI teacher responses in educational dia-\nlogues. Our proposed methods will be evaluated us-\ning the Teacher Student Chatroom Corpus (TSCC)\ndataset. In this section, we provide an overview of\nthe three main parts of our methodology: in-context\nlearning using GPT-4, supervised fine-tuning with\nexisting models such as GPT-2 and DialoGPT, and\nsupervised fine-tuning with Reinforcement Learn-\ning using the RL4LMs library (Ramamurthy et al.,\n2022).\n4.1 In-context Learning\n4.1.1 GPT-4\nAs a preliminary step, we investigate the potential\nof in-context learning using GPT-4, a state-of-the-\nart language model. It generates educational dia-\nlogues based on its pre-trained knowledge, which\nhas been acquired from a vast corpus of text data\nduring its training process (the pre-training data\nmight have included the test set; we will address\nthis issue in the discussion section).\nTo evaluate the performance of GPT-4, we\nprompted GPT-4 in a few-shot fashion. We re-\ntrieved 5 most similar teacher-student conversa-\ntions from the TSCC dataset and provided them\nto the model in addition to the current conversa-\ntion and instructions about the model’s role as a\nteacher. Details about the prompt construction that\nhelps guide the model toward generating suitable\nresponses as a teacher can be found in the Appendix\nA.\n748\nTable 1: Summary of the statistics for the TSCC corpus across the train, dev, and test sets.\nDataset Train Dev Test\nNum Samples 2747 305 273\nAvg Turns 7.7 7.92 5.23\nAvg Tokens Per Turn 7.29 7.21 8.27\n4.2 Supervised Fine-tuning\nTo further adapt pre-trained language models to\nthe specific educational context and generate more\naccurate and context-aware teacher responses, we\nexplore supervised fine-tuning using GPT-2 and\nDialoGPT models.\n4.2.1 GPT-2\nGPT-2 (Radford et al., 2019) is a decoder-only large\nlanguage model pre-trained on WebText, and we\nused GPT-2 Large, which has 24 transformer de-\ncoder blocks with 774 million parameters.\nWe fine-tune the GPT-2 model (Radford et al.,\n2019) using the Huggingface Library on the\nTeacher Student Chatroom Corpus (TSCC) dataset.\nFor each educational dialogue, we concatenated all\ndialogue turns into a single string with additional in-\nformation of speaker roles i.e. students or teachers.\nAs a result, the input to the GPT-2 model consists\nof a sequence of text representing the conversa-\ntion history, culminating in the teacher’s response.\nWe then finetuned GPT-2 Large (Radford et al.,\n2019) with a casual language modeling task. De-\ntails of the exact hyperparameters used during the\nfine-tuning process can be found in the Appendix.\nAfter the fine-tuning process, we evaluated the\nfine-tuned GPT-2 model’s performance on the test\nset by comparing its generated teacher responses to\nreference responses, assessing the model’s ability\nto generate context-aware and educationally rele-\nvant responses in line with the teacher’s role in the\nTSCC dataset.\n4.2.2 DialoGPT\nDialoGPT (Zhang et al., 2019) is a dialogue model\nbased on the GPT-2 architecture, specifically de-\nsigned for generating conversational responses. Di-\naloGPT is trained with 147M conversation pieces\nextracted from Reddit (Zhang et al., 2019), and it is\ntrained with casual language modeling objectives\nwith multi-turn dialogue. We adapt our training\ndataset with the same format as that of DialoGPT\nduring pretraining and then prompt the DialoGPT\nto generate an educational dialogue of teachers in\nthe validation set. After training, we follow the\nsame methodology for evaluation as GPT-2 which\nwe discussed in the earlier section.\n4.3 Supervised Fine-tuning with\nReinforcement Learning\n4.3.1 Flan-T5 Fine-tuned with RL4LMs\nTo optimize the generative models for pedagogi-\ncal quality, we explore the use of reinforcement\nlearning techniques in the fine-tuning process. We\nemploy the RL4LMs library (Ramamurthy et al.,\n2022), which provides an efficient and scalable\nframework for reinforcement learning-based lan-\nguage model fine-tuning.\nThe RL4LMs library incorporates Proximal Pol-\nicy Optimization (PPO) (Schulman et al., 2017)\nas the reinforcement learning algorithm, which is\nknown for its stability and sample efficiency. The\nlibrary also supports the integration of custom re-\nward functions, allowing us to design rewards that\nencourage the generation of pedagogically sound\nteacher responses.\nTo implement the reinforcement learning-based\nfine-tuning, we first fine-tune the Flan-T5 (Chung\net al., 2022) model on the TSCC dataset using\nsupervised learning, as described in the previous\nsection. Next, we utilize the RL4LMs library to\nfine-tune the model further using the PPO algo-\nrithm. We use an equal division of the F1 as cal-\nculated by the roberta-large version of BERTScore\nand DialogRPT-updown as the reward function.\nMore Details about the reinforcement learning fine-\ntuning process can be found in the Appendix.\nThe subsequent evaluation of the fine-tuned Flan-\nT5 model reveals the benefits of incorporating re-\ninforcement learning into the fine-tuning process,\ncontributing to more context-aware, relevant, and\npedagogically effective AI teacher responses.\n5 Results\nIn this section, we present the results and discuss\nthe performance of GPT-4, fine-tuned GPT-2, and\nfine-tuned DialoGPT models on the TSCC dataset.\nWe analyze the strengths and weaknesses of each\napproach and provide insights into their potential\n749\napplications and limitations in an educational con-\ntext.\n5.1 GPT-4\nThe GPT-4 model, without fine-tuning on the\nTSCC dataset, demonstrates a relatively strong per-\nformance in generating educational dialogues. The\ngenerated teacher responses are generally fluent\nand contextually relevant, indicating that GPT-4\nhas a good understanding of the educational con-\ntext based on its pre-trained knowledge. However,\nsome limitations are observed in the model’s abil-\nity to generate accurate and pedagogically sound\nresponses consistently.\nThe carefully crafted prompt provided to the\nmodel plays a crucial role in guiding GPT-4 to-\nward generating suitable responses as a teacher. Al-\nthough the model is capable of generating contex-\ntually relevant and linguistically correct responses,\nit may not always produce the most pedagogically\nsound or helpful responses for the students. This\nlimitation highlights the importance of fine-tuning\nthe model on a specific educational dataset, such\nas TSCC, to further enhance its performance in\ngenerating AI teacher responses.\nAdditionally, due to the nature of the dataset,\nwhere conversations were often cut off, the model\nsometimes lacked the full context needed to gen-\nerate meaningful responses that accurately repre-\nsented the ground truth. Despite this limitation,\nGPT-4’s responses were generally sensible and ap-\npropriate given the available context.\n5.2 Finetuned GPT-2\nWe observe that compared with DialoGPT, GPT-\n2 usually generates longer and more formal re-\nsponses, even with the same generation hyperpa-\nrameters.\n5.3 Finetuned DialoGPT\nWe observe that DialoGPT usually generates\nshorter and more vernacular responses. It fits bet-\nter in a conversational setting, but sometimes the\neducational uptakes are not satisfactory since the\nresponses are not guiding students to learn the lan-\nguage.\n5.4 Finetuned Flan-T5 w/ RL\nWe observe that the results of Flan-T5 w/ RL on the\nvalidation set are really good suggesting that the\nmodel was able to hack the metrics designed as the\nreward. On the contrary, it is performing poorly on\nthe test set suggesting that it overfits the validation\nset. We hypothesize two reasons for this to be the\ncase: the way conversations are split into chunks in\nthe training dataset or the difference in distribution\nbetween the training set and the test set.\n6 Discussion\nConversational agents have the potential to\nrevolutionize the teaching landscape by addressing\nseveral challenges and enhancing the overall\nlearning experience for both students and educa-\ntors (Wollny et al., 2021). However, developing\nconversational agents that can behave like human\nteachers requires addressing several challenges\n(Tack and Piech, 2022).\nData challenges. As noted in the subsections\nabove, the generations from the GPT- 4 model\noutperformed all the fine-tuned models, with\nand without reinforcement learning. To this\nend, we put forward the proposition that an\narray of dataset features plays a crucial role in\nposing significant challenges to the fine-tuning\nprocess of generative models. These features\ninclude several dataset characteristics, including\nsampling, representativeness, prompt and response\nlengths, and dialogue completeness—upon manual\ninspection, we identified several dialogues to be cut\noff—pose serious challenges in achieving superior\nperformance with fine-tuning. Furthermore,\nupon random inspection of the generations from\nthe fine-tuned models, we identified that these\nmodels seem to have learned simple, generic, often\ninappropriate yet correct responses such as “thank\nyou” and “okay.” While more recent language\nmodels have been shown to have high few-shot\nperformance, we believe that fine-tuned models\ncould adapt better to provide domain-specific\nresponses in comparison. To achieve this, we\nemphasize the need for extending the current\ndataset to include longer prompts with more\ncontext.\nIt is important to acknowledge that these models\nmight not be as effective as desired in their\nresponse generation due to these intricacies. The\ncurrent efforts made by the research community\nto collect and build quality datasets encompassing\nenough information about the educational task\nto enable AI teacher generative models to fully\ngeneralize in any context is what we assess to be\nthe main focus that the community should adopt\n750\nTable 2: Validation set results\nModel BERTScore DialogRPT\nGPT-4 0.82 0.69\nFinetuned GPT-2 Large 0.94 0.63\nFinetuned DialoGPT Large 0.94 0.64\nFinetuned Flan-T5 w/ RL 0.89 0.71\nTable 3: Test set results\nModel BERTScore DialogRPT\nGPT-4 0.8 0.70\nFinetuned Flan-T5 w/ RL 0.66 0.34\n[student] someone plugged the charger in\n[teacher] that’s bad, charger must be ___?\n[student] umm . . .\n[model] (a) plugged in ←score: 0.91\n(b) disconnected ←score: 0.90\n[reference] plugged out\nFigure 1: An example dialog demonstrating that two\nopposing responses, (a) and (b), ranked alike using the\nBERTScore metric.\n(Jarratt, 2023).\nEvaluation metrics. In addition, we em-\nphasize that to truly gauge the efficiency of these\nAI-powered teaching models, it is vital to go a step\nfurther and examine their ability to comprehend the\nunique nuances in the students’ queries and cater\nto their particular educational requirements. This\nimplies the need for a pedagogically meaningful\nevaluation metric. We believe that it is crucial for\nthe research community to embrace this as the\nsecond primary focus. While common evaluation\nmetrics such as BERTScore and DialogRPT are\ncommonly used in several language and dialog\nmodeling tasks, it is important to note that these\nmetrics were not fundamentally designed to cap-\nture the level of pedagogical meaningfulness in the\ngenerated responses. As an example, consider the\ndialog shown in Figure 1—depending on the given\ncontext, only one of the responses (option (b):\ndisconnected) is correct, while both the responses\nare ranked as equally correct by the BERTScore\nmetric. Commonly-used domain-agnostic metrics\noften serve as a proxy for how coherent and\nhuman-like the generated responses are. However,\nfor more goal-oriented tasks such as modeling\nteacher-student conversational dialogues, these\nmetrics seem to fall short. This generalization gap\nbecomes more apparent on analyzing the results\nfrom the fine-tuned Flan-T5 model with a feedback\nloop based on BERTScore and DialogRPT\nscores—despite the model performing significantly\nwell on training and validation sets, it failed to\ngeneralize on unseen test data. In an effort to\nadvance research on this front, we note the need\nfor auxiliary training-level metrics, including the\nfaithfulness of the generation to the true response,\nto ensure that the generations are context-aware\nand factually accurate (e.g., correct option (b) vs.\nincorrect option (a) in Figure 1).\nGPT-4 unknown pre-training data. We\nunderstand that the use of GPT-4 as a baseline in\nour study presents challenges due to its unknown\ntraining data. Yet, whether GPT-4 has seen parts\nof the TSCC dataset during its pre-training or\nnot, the improvement of performance compared\nto the reference with regard to the DialogRPT\nscores and human evaluation scores attached to the\nleaderboard of the shared task suggests that the\npotential of using such high-performing models in\nthis domain warrants further exploration.\n751\n7 Conclusion\nIn this paper, we explored the potential of using\nlarge pre-trained language models and reinforce-\nment learning for generating AI teacher responses\nin an educational context. We first presented a\nfew-shot approach using the GPT-4 model, which\ndemonstrated promising results in generating con-\ntextually relevant and fluent responses, but with\nlimitations in generating pedagogically sound re-\nsponses consistently. We then fine-tuned GPT-2\nand DialoGPT on the TSCC dataset and evaluated\ntheir performance using BERTScore and Dialo-\ngRPT metrics. We also proposed an approach us-\ning RL to optimize directly for pedagogical val-\nues. We hypothesized that several dataset charac-\nteristics (e.g., dialog completeness, sampling) pose\nchallenges to achieving superior performance with\nfine-tuning. To this end, we recommend the exten-\nsion of the dataset to include longer prompts with\nextended context. Finally, we also draw attention\nto the need for more domain-specific metrics (in\nboth evaluation and reward-based training) in en-\nabling the generation of accurate, context-aware,\nand factually correct teacher responses.\nReferences\nSerge Bibauw, Thomas François, and Piet Desmet. 2022.\nDialogue systems for language learning: Chatbots\nand beyond. In The Routledge handbook of second\nlanguage acquisition and technology, pages 121–135.\nRoutledge.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosse-\nlut, Emma Brunskill, et al. 2021. On the opportuni-\nties and risks of foundation models. arXiv preprint\narXiv:2108.07258.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nAndrew Caines, Helen Yannakoudakis, Helen Allen,\nPascual Pérez-Paredes, Bill Byrne, and Paula Buttery.\n2022. The teacher-student chatroom corpus version\n2: more lessons, new annotation, automatic detec-\ntion of sequence shifts. In Proceedings of the 11th\nWorkshop on NLP for Computer Assisted Language\nLearning, pages 23–35.\nAndrew Caines, Helen Yannakoudakis, Helena Edmond-\nson, Helen Allen, Pascual Pérez-Paredes, Bill Byrne,\nand Paula Buttery. 2020. The teacher-student chat-\nroom corpus. arXiv preprint arXiv:2011.07109.\nJonathan P Chang, Caleb Chiam, Liye Fu, An-\ndrew Z Wang, Justine Zhang, and Cristian Danescu-\nNiculescu-Mizil. 2020. Convokit: A toolkit for\nthe analysis of conversations. arXiv preprint\narXiv:2005.04246.\nGuanliang Chen, Rafael Ferreira, David Lang, and Dra-\ngan Gasevic. 2019. Predictors of student satisfaction:\nA large-scale study of human-human online tutorial\ndialogues. International Educational Data Mining\nSociety.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nJulie Cohen, Vivian Wong, Anandita Krishnamachari,\nand Rebekah Berlin. 2020. Teacher coaching in a\nsimulated environment. Educational evaluation and\npolicy analysis, 42(2):208–231.\nJames Collins. 1982. Discourse style, classroom inter-\naction and differential treatment. Journal of reading\nbehavior, 14(4):429–437.\nDorottya Demszky, Jing Liu, Zid Mancenido, Julie\nCohen, Heather Hill, Dan Jurafsky, and Tatsunori\nHashimoto. 2021. Measuring conversational uptake:\nA case study on student-teacher interactions. arXiv\npreprint arXiv:2106.03873.\nNatasha Jaques, Asma Ghandeharioun, Judy Hanwen\nShen, Craig Ferguson, Agata Lapedriza, Noah Jones,\nShixiang Gu, and Rosalind Picard. 2019. Way\noff-policy batch deep reinforcement learning of im-\nplicit human preferences in dialog. arXiv preprint\narXiv:1907.00456.\nDaniel Jarratt. 2023. Chatgpt: The double-edged sword\nof ai in education.\nThomas Kane. 2015. National Center for Teacher Ef-\nfectiveness Main Study.\nEnkelejda Kasneci, Kathrin Seßler, Stefan Küchemann,\nMaria Bannert, Daryna Dementieva, Frank Fischer,\nUrs Gasser, Georg Groh, Stephan Günnemann, Eyke\nHüllermeier, et al. 2023. Chatgpt for good? on op-\nportunities and challenges of large language models\nfor education. Learning and Individual Differences,\n103:102274.\nMohammad Norouzi, Samy Bengio, Navdeep Jaitly,\nMike Schuster, Yonghui Wu, Dale Schuurmans, et al.\n2016. Reward augmented maximum likelihood for\nneural structured prediction. Advances In Neural\nInformation Processing Systems, 29.\nRamakanth Pasunuru and Mohit Bansal. 2018. Multi-\nreward reinforced summarization with saliency and\nentailment. arXiv preprint arXiv:1804.06451.\n752\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nRajkumar Ramamurthy, Prithviraj Ammanabrolu,\nKianté Brantley, Jack Hessel, Rafet Sifa, Christian\nBauckhage, Hannaneh Hajishirzi, and Yejin Choi.\n2022. Is reinforcement learning (not) for natural\nlanguage processing?: Benchmarks, baselines, and\nbuilding blocks for natural language policy optimiza-\ntion. arXiv preprint arXiv:2210.01241.\nMarc’Aurelio Ranzato, Sumit Chopra, Michael Auli,\nand Wojciech Zaremba. 2015. Sequence level train-\ning with recurrent neural networks. arXiv preprint\narXiv:1511.06732.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nKurt Shuster, Eric M Smith, et al. 2020. Recipes\nfor building an open-domain chatbot. arXiv preprint\narXiv:2004.13637.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal,\nAlec Radford, and Oleg Klimov. 2017. Proxi-\nmal policy optimization algorithms. arXiv preprint\narXiv:1707.06347.\nIulian V Serban, Chinnadhurai Sankar, Mathieu Ger-\nmain, Saizheng Zhang, Zhouhan Lin, Sandeep Sub-\nramanian, Taesup Kim, Michael Pieper, Sarath Chan-\ndar, Nan Rosemary Ke, et al. 2017. A deep\nreinforcement learning chatbot. arXiv preprint\narXiv:1709.02349.\nShiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua\nWu, Maosong Sun, and Yang Liu. 2015. Minimum\nrisk training for neural machine translation. arXiv\npreprint arXiv:1512.02433.\nAnaïs Tack, Ekaterina Kochmar, Zheng Yuan, Serge\nBibauw, and Chris Piech. 2023. The BEA 2023\nShared Task on Generating AI Teacher Responses in\nEducational Dialogues. In Proceedings of the 18th\nWorkshop on Innovative Use of NLP for Building\nEducational Applications, page to appear, Toronto,\nCanada. Association for Computational Linguistics.\nAnaïs Tack and Chris Piech. 2022. The ai teacher\ntest: Measuring the pedagogical ability of blender\nand gpt-3 in educational dialogues. arXiv preprint\narXiv:2205.07540.\nOpenAI Team. 2022. Chatgpt: Optimizing language\nmodels for dialogue.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nSebastian Wollny, Jan Schneider, Daniele Di Mitri,\nJoshua Weidlich, Marc Rittberger, and Hendrik\nDrachsler. 2021. Are we there yet?-a systematic\nliterature review on chatbots in education. Frontiers\nin artificial intelligence, 4:654924.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2019. Dialogpt: Large-scale\ngenerative pre-training for conversational response\ngeneration. arXiv preprint arXiv:1911.00536.\n753\nA Appendix\nA.1 GPT-4 Prompt Construction\nTo evaluate the performance of GPT-4, we pro-\nvided it with a few-shot prompt that includes a\nselection of similar teacher-student conversations\nfrom the TSCC dataset. This approach helps guide\nthe model toward generating suitable responses as\na teacher. The prompt is constructed as follows:\n• We direct the system role to act as a teacher\nand encourage learning by using the prompt\nas given below.\n• Retrieve the 5 most similar teacher-student\nconversations from the TSCC dataset. This\nis done by computing the cosine similar-\nity between the input conversation context\nand the current conversation context in the\ndataset using embeddings generated by the\ntext-embedding-ada-002 model.\n• Concatenate the selected conversations with\nthe input conversation, separated by special\ntokens to indicate the beginning and end of a\nnew sample conversation.\nThis prompt construction aims to provide GPT-4\nwith the necessary context and guidance to gener-\nate accurate and pedagogically relevant responses\nin the context of teacher-student dialogues. The\nprompt is designed as follows:\nYou are acting as a teacher, and you are\nhelping a student learn. Be patient, help-\nful, and kind. Don’t be superimposing;\ngive short responses to encourage learn-\ning. Make the student feel comfortable\nand confident, and help them learn. Now,\njoin the following conversation: <conver-\nsation context>\nThe prompt is designed using the following di-\nrectives in mind:-\n• We instruct the system with several indicators\nto act as a teacher and provide helpful advice\nto the student.\n• To mitigate the challenge of generating\nteacher-like responses, we advise the model\nto be patient, kind, and helpful to the student.\n• Through the directive to keep responses short\nand encouraging, we guide the model toward\ngenerating suitable responses that might help\nthe student learn effectively.\n• The model is also instructed to make the stu-\ndent feel comfortable and confident in their\nlearning process, providing an overall support-\nive environment for the student.\n• Finally, the conversation context is provided\nto the model to set the context for the given\nstudent query, allowing the model to generate\nappropriate responses given the conversation\ncontext.\nCombining all these aspects together, we aim\nto guide the model toward generating contextually\nrelevant and pedagogically meaningful responses\nin the given teacher-student dialogue.\nWe use the following hyperparameters for query-\ning the GPT-4 model:\n• Model: gpt-4-0314\n• Temperature: 1\n• Max Tokens: 100\n• Top p: 1\nA.2 Fine-tuning Exact Parameters\nFor our supervised fine-tuning experiments, we\nused the following hyperparameters:\nA.2.1 GPT-2\n• Learning rate: 1e-5\n• Batch size: 32\n• Epochs: 10\n• Max sequence length: 1024\n• Optimizer: AdamW\n• Scheduler: linear learning rate scheduler\nA.2.2 DialoGPT\n• Learning rate: 1e-5\n• Batch size: 32\n• Epochs: 10\n• Max sequence length: 1024\n• Optimizer: AdamW\n• Scheduler: linear learning rate scheduler\n754\nA.3 Supervised Fine-tuning with\nReinforcement Learning Details\nTo implement the reinforcement learning-based\nfine-tuning using the RL4LMs library, we first fine-\ntuned the Flan-T5 model on the TSCC dataset using\nsupervised learning. After this initial fine-tuning\nstep, we utilized the RL4LMs library to fine-tune\nthe model further using reinforcement learning. We\nused an equal division of the BERTScore and Dialo-\ngRPT as the reward function to optimize the model\nfor pedagogical quality. The following hyperpa-\nrameters were used for the reinforcement learning\nfine-tuning process:\n• Learning rate: 1e-6\n• Batch size: 64\n• Epochs: 5\n• Max prompt length: 512\n• Max episode length: 100\n• Optimizer: AdamW\n• Scheduler: linear learning rate scheduler\nThe YAML file for the RL4LMs scipt is as fol-\nlows:\ntokenizer:\ndel_name: google/flan-t5-small\ndding_side: left\nuncation_side: left\nd_token_as_eos_token: False\nrd_fn:\n: dialog_rpt_bert\ngs:\nBERTScore_coeff: 0.5\nDialogRPT_coeff: 0.5\npool:\n: bea\nuncate: False\ngs: {}\nenvs: 1\ngs:\nmax_prompt_length: 100\nmax_episode_length: 20\nterminate_on_eos: True\ncontext_start_token: 0\nprompt_truncation_side: \"right\"\n: ppo_separate\ngs:\nn_steps: 20\nbatch_size: 64\nverbose: 1\nlearning_rate: 0.000001\nclip_range: 0.2\nn_epochs: 1\nvalue_update_epochs: 3\n# batchify: False\ngae_lambda: 0.95\ngamma: 0.99\nent_coef: 0.01\n_div:\ncoeff: 0.001\ntarget_kl: 2.0\nlicy:\nid: seq2seq_lm_actor_critic_policy\nargs:\nmodel_name: google/flan-t5-small\napply_model_parallel: True\nprompt_truncation_side: \"right\"\ngeneration_kwargs:\ndo_sample: True\ntop_k: 0\nmin_length: 9\nmax_new_tokens: 20\nn_evaluation:\nal_batch_size: 64\niters: 200\nal_every: 20\nve_every: 10\ntrics:\n- id: bert_score\nargs:\nlanguage: en\n- id: dialog_rpt\nargs:\nmodel_name: \"microsoft/DialogRPT\n-updown\"\nlabel_ix: 0\nbatch_size: 1\n# - id: uptake\n# args:\n# model_name: None\n# label_ix: 0\n# batch_size: 1\nneration_kwargs:\nnum_beams: 5\nmin_length: 9\nmax_new_tokens: 20\n755",
  "topic": "Generalizability theory",
  "concepts": [
    {
      "name": "Generalizability theory",
      "score": 0.7814772129058838
    },
    {
      "name": "Computer science",
      "score": 0.7667329907417297
    },
    {
      "name": "Metric (unit)",
      "score": 0.5897982716560364
    },
    {
      "name": "Dialog box",
      "score": 0.5667452812194824
    },
    {
      "name": "Automatic summarization",
      "score": 0.551690936088562
    },
    {
      "name": "Generative grammar",
      "score": 0.5516074299812317
    },
    {
      "name": "Benchmarking",
      "score": 0.5381643772125244
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5375634431838989
    },
    {
      "name": "Generative model",
      "score": 0.5285027623176575
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5232918858528137
    },
    {
      "name": "Task (project management)",
      "score": 0.5145582556724548
    },
    {
      "name": "Natural language processing",
      "score": 0.5000550746917725
    },
    {
      "name": "Representativeness heuristic",
      "score": 0.4786985218524933
    },
    {
      "name": "Language model",
      "score": 0.47359779477119446
    },
    {
      "name": "Machine learning",
      "score": 0.32533276081085205
    },
    {
      "name": "Psychology",
      "score": 0.11152082681655884
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "World Wide Web",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Developmental psychology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I205783295",
      "name": "Cornell University",
      "country": "US"
    }
  ],
  "cited_by": 4
}