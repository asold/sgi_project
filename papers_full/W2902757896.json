{
  "title": "A Transformer-Based Multi-Source Automatic Post-Editing System",
  "url": "https://openalex.org/W2902757896",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2143363400",
      "name": "Santanu Pal",
      "affiliations": [
        "German Research Centre for Artificial Intelligence",
        "Saarland University"
      ]
    },
    {
      "id": "https://openalex.org/A910455083",
      "name": "Nico Herbig",
      "affiliations": [
        "German Research Centre for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2243606749",
      "name": "Antonio Krüger",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A93711912",
      "name": "Josef van Genabith",
      "affiliations": [
        "German Research Centre for Artificial Intelligence",
        "Saarland University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2257408573",
    "https://openalex.org/W2022433169",
    "https://openalex.org/W2514996388",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2250472722",
    "https://openalex.org/W3198727596",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2251816503",
    "https://openalex.org/W2510492408",
    "https://openalex.org/W2159086733",
    "https://openalex.org/W2740510699",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2962780935",
    "https://openalex.org/W3214495121",
    "https://openalex.org/W3203021515",
    "https://openalex.org/W2124807415",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2760656271",
    "https://openalex.org/W2791510479",
    "https://openalex.org/W2963344439",
    "https://openalex.org/W2758074402",
    "https://openalex.org/W2002877250",
    "https://openalex.org/W2141971526",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2595715041",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3006530332",
    "https://openalex.org/W2758488395",
    "https://openalex.org/W2512924740",
    "https://openalex.org/W2572157567",
    "https://openalex.org/W2740295203",
    "https://openalex.org/W2963816901",
    "https://openalex.org/W125577832",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2970871182",
    "https://openalex.org/W2962678612",
    "https://openalex.org/W2758520323",
    "https://openalex.org/W2144600658",
    "https://openalex.org/W2532807140",
    "https://openalex.org/W2903193068",
    "https://openalex.org/W2613904329"
  ],
  "abstract": "This paper presents our English–German Automatic Post-Editing (APE) system submitted to the APE Task organized at WMT 2018 (Chatterjee et al., 2018). The proposed model is an extension of the transformer architecture: two separate self-attention-based encoders encode the machine translation output (mt) and the source (src), followed by a joint encoder that attends over a combination of these two encoded sequences (encsrc and encmt) for generating the post-edited sentence. We compare this multi-source architecture (i.e, {src, mt} → pe) to a monolingual transformer (i.e., mt → pe) model and an ensemble combining the multi-source {src, mt} → pe and single-source mt → pe models. For both the PBSMT and the NMT task, the ensemble yields the best results, followed by the multi-source model and last the single-source approach. Our best model, the ensemble, achieves a BLEU score of 66.16 and 74.22 for the PBSMT and NMT task, respectively.",
  "full_text": "Proceedings of the Third Conference on Machine Translation (WMT), V olume 2: Shared Task Papers, pages 827–835\nBelgium, Brussels, October 31 - Novermber 1, 2018.c⃝2018 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/W18-64095\nA Transformer-Based Multi-Source Automatic Post-Editing System\nSantanu Pal1,2, Nico Herbig2, Antonio Kr¨uger2, Josef van Genabith1,2\n1Department of Language Science and Technology,\nSaarland University, Germany\n2German Research Center for Artiﬁcial Intelligence (DFKI),\nSaarland Informatics Campus, Germany\n{santanu.pal, josef.vangenabith}@uni-saarland.de\n{nico.herbig, krueger, josef.van genabith}@dfki.de\nAbstract\nThis paper presents our English–German\nAutomatic Post-Editing (APE) system\nsubmitted to the APE Task organized\nat WMT 2018 ( Chatterjee et al. , 2018).\nThe proposed model is an extension of\nthe transformer architecture: two sepa-\nrate self-attention-based encoders encode\nthe machine translation output ( mt) and\nthe source ( src), followed by a joint en-\ncoder that attends over a combination of\nthese two encoded sequences ( encsrc and\nencmt) for generating the post-edited sen-\ntence. We compare this multi-source ar-\nchitecture (i.e, {src, mt } → pe) to a\nmonolingual transformer (i.e., mt → pe)\nmodel and an ensemble combining the\nmulti-source {src, mt } → pe and single-\nsource mt → pe models. For both the\nPBSMT and the NMT task, the ensem-\nble yields the best results, followed by\nthe multi-source model and last the single-\nsource approach. Our best model, the en-\nsemble, achieves a BLEU score of 66.16\nand 74.22 for the PBSMT and NMT task,\nrespectively.\n1 Introduction & Related Work\nThe ultimate goal of machine translation (MT)\nis to provide fully automatic publishable quality\ntranslations. However, state-of-the-art MT sys-\ntems often fail to deliver this; translations pro-\nduced by MT systems contain different errors and\nrequire human interventions to post-edit the trans-\nlations. Nevertheless, MT has become a standard\nin the translation industry as post-editing on MT\noutput is often faster and cheaper than performing\nhuman translation from scratch.\nAPE is a method that aims to automatically cor-\nrect errors made by MT systems before perform-\ning actual human-post-editing (PE) ( Knight and\nChander, 1994), thereby reducing the translators’\nworkload and increasing productivity ( Parra Es-\ncart´ın and Arcedillo , 2015b,a; Pal et al. , 2016a).\nVarious automatic and semi-automatic techniques\nhave been developed to auto-correct repetitive er-\nrors ( Roturier, 2009; TAUS/CNGL Report, 2010).\nThe advantage of APE lies in its capability to\nadapt to any black-box (ﬁrst-stage) MT engine;\ni.e., upon availability of human-corrected post-\nedited data, no incremental training or full re-\ntraining of the ﬁrst-stage MT system is required\nto improve the overall translation quality. APE\ncan therefore be viewed as a 2 nd-stage MT system,\ntranslating predictable error patterns in MT output\nto their corresponding corrections. APE training\ndata minimally involves MT output ( mt) and the\nhuman-post-edited ( pe) version of mt, but may ad-\nditionally make use of the source ( src). A more\ndetailed motivation on APE can be found in Bojar\net al. (2015, 2016, 2017).\nBased on the training process, APE systems\ncan be categorized as either single-source ( mt →\npe) or multi-source ( {src, mt } → pe) ap-\nproaches. In general, the ﬁeld of APE covers\na wide methodological range, including SMT-\nbased approaches ( Simard et al. , 2007a,b; Lagarda\net al. , 2009; Rosa et al. , 2012; Pal et al. , 2016c;\nChatterjee et al. , 2017b), and neural APE ( Pal\net al. , 2016b; Junczys-Dowmunt and Grund-\nkiewicz, 2016; Pal et al. , 2017) based on neural\nmachine translation (NMT). Some of the state-\nof-the-art multi-source approaches, both statistical\n(B´echara et al. , 2011; Chatterjee et al. , 2015) and\nrecently neural ( Libovick´y et al. , 2016; Chatter-\njee et al. , 2017a; Junczys-Dowmunt and Grund-\nkiewicz, 2016; Varis and Bojar , 2017), explore\nlearning from {src, mt } → pe (multi-source, MS)\n827\nto take advantage of the dependencies of transla-\ntion errors in mt originating from src.\nExploiting source information in multi-source\nneural APE can be conﬁgured either by using\na single encoder that encodes the concatenation\nof src and mt (Niehues et al. , 2016) or by us-\ning two separate encoders for src and mt and\npassing the concatenation of both encoders’ ﬁnal\nstates to the decoder ( Libovick´y et al. , 2016). A\nfew approaches to multi-source neural APE have\nbeen proposed in the WMT-2017 APE shared task.\nJunczys-Dowmunt and Grundkiewicz (2017) ex-\nplore different combinations of attention mecha-\nnisms including soft attention and hard monotonic\nattention on an end-to-end neural APE model that\ncombines both mt and src in a single neural ar-\nchitecture. Chatterjee et al. (2017a) extend the\ntwo-encoder architecture of multi-source models\npresented in Libovick´y et al. (2016). In their ex-\ntension each encoder concatenates both contexts\nhaving their own attention layer that is used to\ncompute the weighted context of src and mt. Fi-\nnally, a linear transformation is applied on the con-\ncatenation of both weighted contexts. Varis and\nBojar (2017) implement and compare two multi-\nsource architectures: In the ﬁrst setup, they use\na single encoder with concatenation of src and\nmt sentences, and in the second setup, they use\ntwo character-level encoders for mt and src, sep-\narately, along with a character-level decoder. The\ninitial state of this decoder is a weighted combina-\ntion of the ﬁnal states of the two encoders.\nIntuitively, such an integration of source-\nlanguage information in APE should be useful\nin conveying the context information to improve\nthe APE performance. To provide the awareness\nof errors in mt originating from src, the trans-\nformer architecture ( Vaswani et al. , 2017), which\nis built solely upon attention mechanisms ( Bah-\ndanau et al. , 2015), makes it possible to model\ndependencies without regard to their distance in\nthe input or output sequences and also captures\nglobal dependencies between input and output (for\nour case src, mt, and pe). The transformer ar-\nchitecture replaces recurrence and convolutions\nby using positional encodings on both the input\nand output sequences. The encoder and decoder\nboth use multi-head (facilitating parallel compu-\ntations) self-attention to compute representations\nof their corresponding inputs, and also compute\nmulti-head vanilla-attentions between encoder and\ndecoder representations.\nOur APE system extends this transformer-based\nNMT architecture ( Vaswani et al. , 2017) by us-\ning two encoders, a joint encoder, and a single de-\ncoder. Our model concatenates two separate self-\nattention-based encoders ( encsrc and encmt) and\npasses this sequence through another self-attended\njoint encoder ( encsrc,mt) to ensure capturing de-\npendencies between src and mt. Finally, this\njoint encoder is fed to the decoder which follows a\nsimilar architecture as described in Vaswani et al.\n(2017). The entire model is optimized as a single\nend-to-end transformer network.\n2 Transformer-Based Multi-Source APE\nMT errors originating from the input source sen-\ntences suggest that APE systems should lever-\nage information from both the src and mt, in-\nstead of considering mt in isolation. This can\nhelp the model to disambiguate corrections ap-\nplied at every time step. Generating the pe output\nfrom mt is greatly facilitated by the availability of\nsrc. To achieve beneﬁts from both single-source\n(mt → pe) and multi-source ({src, mt} → pe)\nAPEs, our primary submission in the WMT 2018\nshared task is an ensemble of these two models.\nTransformer-based models are currently pro-\nviding state-of-the-art performance in MT; hence,\nwe want to explore a similar architecture for this\nyear’s APE task. We extend the transformer archi-\ntecture to investigate how efﬁcient this approach\nis in a multi-source scenario. In a MT task, it\nwas already shown that a transformer can learn\nlong-range dependencies. Therefore, we explore\nif we can leverage information from src and mt\nvia a joint encoder through self-attention (see Sec-\ntion 2.2) to provide dependencies between src–mt\nthat are then projected to the pe.\nTo investigate this, we implement and evaluate\nthree different models: a single-source approach,\na multi-source approach, and an ensemble of both,\ndescribed in more detail below.\n2.1 Single-Source Transformer for APE\n(mt → pe)\nOur single-source model (SS) is based on\nan encoder-decoder-based transformer architec-\nture ( Vaswani et al. , 2017). Transformer models\ncan replace sequence-aligned recurrence entirely\nand follow three types of multi-head attention:\nencoder-decoder attention (also known as vanilla\n828\nFigure 1: Multi-source transformer-based APE\nattention), encoder self-attention, and masked de-\ncoder self-attention. Since for multi-head atten-\ntion each head uses different linear transforma-\ntions, it can learn these separate relationships in\nparallel, thereby improving learning time.\n2.2 Multi-source Transformer for APE\n({src, mt} → pe)\nFor our multi-source model (MS), we propose\na novel joint transformer model (cf. Figure 1),\nwhich combines the encodings of src and mt\nand attends over a combination of both sequences\nwhile generating the post-edited sentence. Apart\nfrom encsrc and encmt, each of which is equiva-\nlent to the original transformer’s encoder ( Vaswani\net al. , 2017), we use a joint encoder with an\nequivalent architecture, to maintain the homo-\ngeneity of the transformer model. For this, we ex-\ntend Vaswani et al. (2017) by introducing an addi-\ntional identical encoding block by which both the\nencsrc and the encmt encoders communicate with\nthe decoder.\nOur multi-source neural APE computes inter-\nmediate states encsrc and encmt for the two\nencoders, encsrc,mt for their combination, and\ndecpe for the decoder in sequence-to-sequence\nmodeling. One self-attended encoder for src maps\ns = (s1, s 2, ..., s k) into a sequence of continuous\nrepresentations, encsrc = (e1, e 2, ..., e k), and a\nsecond encoder for mt, m = (m1, m 2, ..., m l), re-\nturns another sequence of continuous representa-\ntions, encmt = (e\n′\n1, e\n′\n2, ..., e\n′\nl). The self-attended\njoint encoder (cf. Figure 1) then receives the con-\ncatenation of encsrc and encmt, encconcat =\n[encsrc, encmt] as an input, and passes it through\nthe stack of 6 layers, with residual connections,\na self-attention and a position-wise fully con-\nnected feed-forward neural network. As a result,\nthe joint encoder produces a ﬁnal representation\n(encsrc,mt) for both src and mt. Self-attention\nat this point provides the advantage of aggregat-\ning information from all of the words, including\nsrc and mt, and successively generates a new rep-\nresentation per word informed by the entire src\nand mt context. The decoder generates the pe out-\nput in sequence, decpe = (p1, p 2, . . . , p n), one\nword at a time from left to right by attending pre-\nviously generated words as well as the ﬁnal repre-\nsentations ( encsrc,mt) generated by the encoder.\n2.3 Ensemble\nIn order to leverage the network architecture for\nboth single-source and multi-source APE as dis-\ncussed above, we decided to ensemble several ex-\npert neural models. Each model is averaged using\nthe 5 best saved checkpoints, which generate dif-\nferent translation outputs. Taking into account all\nthese generated translation outputs, we implement\nan ensemble technique based on the frequency of\noccurrence of the output words. Corresponding to\neach input word, we calculate the most frequent\noccurrence of the output word from all the gener-\nated translation outputs. For the two different APE\ntasks, we ensemble the following models:\n• PBSMT task: We ensemble a SS ( mt → pe)\nand a MS ( {src, mt } → pe) average model.\n• NMT task: We ensemble two average SS\n(mt → pe) and MS ( {src, mt } → pe) mod-\nels, together with a SS and a MS model that\nare ﬁne-tuned on a subset of the training set\n(cf. Section 3.3.2).\n3 Experiments\nIn our experiment we investigate (1) how well the\ntransformer-based APE architecture performs in\ngeneral, (2) if our multi-source architecture using\nthe additional joint encoder improves the perfor-\nmance over a single-source architecture, and (3) if\nensembling of single-source and multi-source ar-\nchitectures facilitates APE even further.\n3.1 Data\nSince this year’s WMT 2018 APE task ( Chatterjee\net al. , 2018) is divided into two sub-tasks, differ-\n829\nent datasets are provided for each task: for the PB-\nSMT task, there is a total of 23K English–German\nAPE data samples (11K from WMT 2016 and 12K\nfrom WMT 2017) ( Bojar et al. , 2017). For the\nNMT task, 13,442 samples of English–German\nAPE data are provided.\nAll released APE data consists of English–\nGerman triplets containing source English text\n(src) from the IT domain, the corresponding Ger-\nman translations ( mt) from a ﬁrst stage MT sys-\ntem, and the corresponding human-post-edited\nversion ( pe), all of them already tokenized. As this\nreleased APE dataset is small in size (see Table 1),\nadditional resources are also available: ﬁrst, the\n‘artiﬁcial training data’ ( Junczys-Dowmunt and\nGrundkiewicz, 2016) containing 4.5M sentences,\n4M of which are weakly similar to the WMT\n2016 training data, while 500K show very simi-\nlar TER statistics; and second, the synthetic ‘eS-\nCAPE’ APE corpus ( Negri et al. , 2018), consist-\ning of more than 7M triples for both NMT and\nPBSMT.\nTable 1 presents the statistics of the released\ndata for the English–German APE Task organized\nin WMT 2018. These datasets, except for the\neSCAPE corpus, do not require any preprocessing\nin terms of encoding or alignment.\nFor cleaning the noisy eSCAPE dataset contain-\ning many unrelated language words (e.g. Chinese),\nwe perform the following two steps: (i) we use\nthe cleaning process described in Pal et al. (2015),\nand (ii) we execute the Moses ( Koehn et al. , 2007)\ncorpus cleaning scripts with minimum and max-\nimum number of tokens set to 1 and 80, respec-\ntively. After cleaning, we use the Moses tokenizer\nto tokenize the eSCAPE corpus. To handle out-\nof-vocabulary words, words are preprocessed into\nsubword units ( Sennrich et al. , 2016) using byte-\npair encoding (BPE).\n3.2 Hyper-Parameter Settings\nFor {src, mt} → pe, both the self-attended en-\ncoders, the joint encoder, and the decoder are com-\nposed of a stack of N = 6 identical layers fol-\nlowed by layer normalization. Each layer again\nconsists of two sub-layers and a residual connec-\ntion ( He et al. , 2016) around each of the two sub-\nlayers. During training, we employ label smooth-\ning of value ϵls = 0.1. The output dimension pro-\nduced by all sub-layers and embedding layers is\ndeﬁned as dmodel = 256. All dropout values in the\nnetwork are set to 0.2. Each encoder and decoder\ncontains a fully connected feed-forward network\nhaving dimensionality dmodel = 256 for the input\nand output and dimensionality dff = 1024for the\ninner layer. This is a similar setting to Vaswani\net al. (2017)’sC □ model1. For the scaled dot-\nproduct attention, the input consists of queries\nand keys of dimension dk, and values of dimen-\nsion dv. As multi-head attention parameters, we\nemploy h = 8 for parallel attention layers, or\nheads. For each of these we use a dimensional-\nity of dk = dv = dmodel/h = 32. For optimiza-\ntion, we use the Adam optimizer ( Kingma and Ba ,\n2015) with β1 = 0. 9, β2 = 0. 98 and ϵ = 10□9.\nThe learning rate is varied throughout the training\nprocess, ﬁrst increasing linearly for the ﬁrst train-\ning steps warmupsteps = 4000and then adjusted\nas described in ( Vaswani et al. , 2017).\nAt training time, the batch size is set to 32\nsamples, with a maximum sentence length of 80\nsubwords, and a vocabulary of the 50K most fre-\nquent subwords. After each epoch, the train-\ning data is shufﬂed. For encoding the word or-\nder, our model uses learned positional embed-\ndings ( Gehring et al. , 2017), since Vaswani et al.\n(2017) reported nearly identical results to sinu-\nsoidal encodings. After ﬁnishing training, we save\nthe 5 best checkpoints saved at each epoch. Fi-\nnally, we use a single model obtained by averag-\ning the last 5 checkpoints. During decoding, we\nperform greedy-search-based decoding.\nWe follow a similar hyper-parameter setup for\nmt → pe. The total number of parameters for our\n{src, mt } → pe and mt → pe model is 46 × 106\nand 28 × 106, respectively.\n3.3 Experiment Setup\nIn this section, we present the training process,\nusing the above datasets, to train mt → pe,\n{src, mt } → pe, and ensemble models for both\nPBSMT and NMT.\n3.3.1 PBSMT Task\nFor PBSMT, we ﬁrst train both our SS and MS\nsystems with the cleaned eSCAPE corpus for 3\nepochs. We then perform transfer learning with\n4M artiﬁcial data for 7 epochs. Afterwards, ﬁne-\ntuning is performed using the 500K artiﬁcial and\n23K real PE training data for another 20 epochs.\n1Note: at the time of submission we couldn’t test the\nTransformer (big) model due to unavailability of enough\ncomputation power\n830\nSentences\nCorpus 2016 2017 2018 Cleaning\nPBSMT\nTrain 12,000 11,000 - -\nDev 1,000 - - -\nTest 2,000 2,000 2,000 -\nNMT\nTrain - - 13,442 -\nDev - - 1,000 -\nTest - - 1,023 -\nAdditional\nResources\nArtiﬁcial - 4M + 500K - -\neSCAPE-PBSMT - - 7,258,533 6,521,736\neSCAPE-NMT - - 7,258,533 6,485,507\nTable 1: Statistics of the WMT 2018 APE Shared Task Dataset.\nFurthermore, we generate an ensemble model, by\naveraging the 5 best checkpoints of SS with the 5\nbest checkpoints of MS.\nWe use the WMT 2016 development data\n(dev2016) containing 1,000 triplets to validate the\nmodel during training. To test our system per-\nformance, we use the WMT 2016 and 2017 test\ndata (test2016, test2017), each containing 2,000\ntriplets. Furthermore, we report the results of the\nsubmitted ensemble model on test2018.\n3.3.2 NMT Task\nInitial tests for pre-training our NMT model on\nthe NMT eSCAPE data showed no performance\nimprovements. Therefore, we use the PBSMT\nSS and MS models as a basis for the NMT task.\nWe use the PBSMT models after training them\non the eSCAPE corpus, the 4M artiﬁcial data and\nthe 500K + 23K train sets of WMT 16 and 17.\nThese SMT-based models are then ﬁne-tuned us-\ning the WMT 2018 NMT APE data (train18) for\n60 epochs.\nAfterwards, we perform an additional ﬁne-\ntuning step towards the dev18/test18 dataset: For\nthis, we extract sentences of train18 that are simi-\nlar to the sentences contained in dev18/test18 and\nﬁne-train for another 15 epochs on this subset of\ntrain18, which we call ﬁne-tune18. As a sim-\nilarity measure we use the cosine similarity be-\ntween the train src and mt segments and the test\nsrc and mt segments, respectively. These cosine\nsimilarities for src and mt are then simply multi-\nplied to achieve an overall similarity measure. Our\nﬁne-tuning dataset contains only sentences with an\noverall similarity of at least 0.9.\nLast, two separate ensemble models are created.\nOne consists of only the non-ﬁne-tuned SS and\nMS models, and one ensembles the SS and MS\nmodels in both ﬁne-tuned and non-ﬁne-tuned vari-\nants. Both ensembles are created by averaging\nover the 5 best checkpoints of each sub-model.\nWe report the results of all created models for\nthe dev18 NMT dataset, and additionally those of\nthe submitted overall ensemble model on test18.\n3.4 Results and Discussion\nTable 2 presents the results for the PBSMT APE\ntask (cf. 3.3.1), where two different transformer-\nbased models, one ensemble of these models and\nthe baseline BLEU scores are shown. The base-\nline here refers to the original MT output evalu-\nated with respect to the corresponding PE transla-\ntion. All models yield statistically signiﬁcant re-\nsults ( p < 0. 001) over this baseline. MSavg also\nprovides statistically signiﬁcant improvement over\nSSavg . For this and all following signiﬁcance tests\nwe employ the method by Clark et al. (2011)2.\nGenerally, reasons for the good performance of\nour transformer-based MS architecture in compar-\nison to the SS approach for PBSMT-based APE\ncould be the positional encoding that injects in-\nformation about the relative or absolute position\nof the tokens in the sequence. This might help\nto handle word order errors in mt for a given\nsrc context. Another possible explanation lies in\nthe self-attention mechanism, which handles lo-\ncal word dependencies for src, mt, and pe. Af-\nter the individual dependencies are learned by\nthe two encoders’ self-attention mechanisms, an-\nother level of self-attention is performed that can\njointly learn from both src and mt using our\njoint encoder, thereby informing the decoder about\nthe long-range dependencies between the words\nwithin both src and mt. Compared to RNNs,\nwe believe that this technique can better convey\nsource information via mt to the decoder. The\nensemble model then leverages the advantages of\nboth our SS and MS approaches to further improve\nthe results.\nThe results for our transformer-based architec-\n2https://github.com/jhclark/multeval\n831\nWMT APE Systems eScape 4M 500K train16 train17 test16 test17 test18\nBaseline - 62.92 62.11 62.99\nMSavg 3 eps 7 eps 20 eps 67.31 67.66 -\nSSavg 3 eps 7 eps 20 eps 66.27 66.60 -\nEnsemble MSavg{5cps} + SSavg{5cps} 68.52 68.91 66.16\nTable 2: Evaluation result of WMT 2018 PBSMT task for all trained models.\nWMT APE Systems Base Model train18 ﬁne-tune18 dev18 test18\nBaseline - - - 76.66 74.73\nMSavg MSavg (PBSMT) 60 eps - 74.84 -\nSSavg SSavg (PBSMT) 60 eps - 72.75\nMSfinetuned MSavg (NMT) - 15 eps 75.05 -\nSSfinetuned SSavg (NMT) - 15 eps 73.17 -\nEnsemble MSavg{5cps} + SSavg{5cps} 75.80 -\nEnsemblefinetuned MSavg{5cps} + SSavg{5cps} + MSfinetuned{5cps} + SSfinetuned{5cps} 75.96 74.22\nTable 3: Evaluation result of WMT 2018 NMT task for all trained models.\nture for the NMT task are shown in Table 3. As can\nbe seen, the baseline NMT system performs best,\nfollowed by the ensemble models, then the multi-\nsource architectures and lastly the single-source\napproach. These differences between the three\napproaches, ensemble, MS, and SS, are all sta-\ntistically signiﬁcant. Fine-tuning provides some\nsmall, albeit insigniﬁcant, improvements over the\nnon-ﬁne-tuned versions.\nWhile none of our architectures perform better\nthan the baseline MT system for the NMT task, we\nclearly see that the multi-source approach helps,\nand that ensembling of different SS and MS mod-\nels further increases the performance. These re-\nsults are in line with our expectations, because in-\ntuitively, inspecting both src and mt should help\ndetect and correct common errors. However, we\nare unsure why all models did not improve over\nthe baseline, which could have been achieved by\nsimply copying the mt. One reason might be the\nsmall amount of PE data, which comprises only\n13K samples; this could also explain why the sim-\nple ﬁne-tuning approach already leads to slightly\nhigher BLEU scores. However, further human\nevaluation is necessary to better understand what\nour model is doing for the neural APE task and\nwhy it remains approximately 0.5 BLEU points\nbelow the baseline.\n4 Conclusions and Future Work\nIn this paper, we investigated a novel transformer-\nbased multi-source APE approach that jointly at-\ntends over a combination of src and mt to capture\ndependencies between the two. This architecture\nyields statistically signiﬁcant improvements over\nsingle-source transformer-based models. An en-\nsemble of both variants increases the performance\nfurther. For the PBSMT task, the baseline MT sys-\ntem was outperformed by 3.2 BLEU points, while\nthe NMT baseline remains 0.51 BLEU points bet-\nter than our APE approach on the 2018 test set.\nIn the future, we will investigate if the perfor-\nmance of each system can be improved by using\na different hyper-parameter setup. Unfortunately,\nwe could not test either the ‘big’ or the ‘base’\nhyper-parameter conﬁguration in Vaswani et al.\n(2017) due to unavailable computing resources at\nthe time of submission. As additional future work,\nwe would like to explore whether using re-ranking\nand ensembling of different neural APEs helps to\nimprove the performance further. Moreover, we\nwill incorporate word-level quality estimation fea-\ntures of mt into the encoding layer. Lastly, we\nwill evaluate if our model indeed is able to bet-\nter handle word order errors and to capture long-\nrange dependencies, as we expect. Furthermore,\nwe will analyze if adapting the learning rate to the\nsize of the datasets used during training increases\nthe performance compared to the currently used\nﬁxed learning rate initialization of 0.001.\nAcknowledgments\nThis research was funded in part by the Ger-\nman research foundation (DFG) under grant num-\nber GE 2819/2-1 (project MMPE) and the Ger-\nman Federal Ministry of Education and Research\n(BMBF) under funding code 01IW17001 (project\nDeeplee). The responsibility for this publication\nlies with the authors. We also want to thank the re-\nviewers for their valuable input, and the organizers\nof the shared task.\n832\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural Machine Translation by Jointly\nLearning to Align and Translate. In International\nConference on Learning Representations (ICLR) ,\nSan Diego, CA, USA.\nHanna B ´echara, Yanjun Ma, and Josef van Genabith.\n2011. Statistical Post-Editing for a Statistical MT\nSystem. In Proceedings of MT Summit XIII , pages\n308–315.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Shujian Huang,\nMatthias Huck, Philipp Koehn, Qun Liu, Varvara\nLogacheva, Christof Monz, Matteo Negri, Matt\nPost, Raphael Rubino, Lucia Specia, and Marco\nTurchi. 2017. Findings of the 2017 Conference\non Machine Translation (WMT17). In Proceed-\nings of the Second Conference on Machine Trans-\nlation, Volume 2: Shared Task Papers , pages 169–\n214, Copenhagen, Denmark. Association for Com-\nputational Linguistics.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Matthias Huck,\nAntonio Jimeno Yepes, Philipp Koehn, Varvara\nLogacheva, Christof Monz, Matteo Negri, Aure-\nlie Neveol, Mariana Neves, Martin Popel, Matt\nPost, Raphael Rubino, Carolina Scarton, Lucia Spe-\ncia, Marco Turchi, Karin Verspoor, and Marcos\nZampieri. 2016. Findings of the 2016 Conference\non Machine Translation. In Proceedings of the First\nConference on Machine Translation , pages 131–\n198, Berlin, Germany. Association for Computa-\ntional Linguistics.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nBarry Haddow, Matthias Huck, Chris Hokamp,\nPhilipp Koehn, Varvara Logacheva, Christof Monz,\nMatteo Negri, Matt Post, Carolina Scarton, Lucia\nSpecia, and Marco Turchi. 2015. Findings of the\n2015 Workshop on Statistical Machine Translation.\nIn Proceedings of the Tenth Workshop on Statistical\nMachine Translation , pages 1–46, Lisbon, Portugal.\nAssociation for Computational Linguistics.\nRajen Chatterjee, M. Amin Farajian, Matteo Ne-\ngri, Marco Turchi, Ankit Srivastava, and Santanu\nPal. 2017a. Multi-source Neural Automatic Post-\nEditing: FBK’s participation in the WMT 2017 APE\nshared task. In Proceedings of the Second Con-\nference on Machine Translation, Volume 2: Shared\nTask Papers , pages 630–638, Copenhagen, Den-\nmark. Association for Computational Linguistics.\nRajen Chatterjee, Gebremedhen Gebremelak, Matteo\nNegri, and Marco Turchi. 2017b. Online Automatic\nPost-editing for MT in a Multi-Domain Translation\nEnvironment. In Proceedings of the 15th Confer-\nence of the European Chapter of the Association\nfor Computational Linguistics: Volume 1, Long Pa-\npers, pages 525–535, Valencia, Spain. Association\nfor Computational Linguistics.\nRajen Chatterjee, Matteo Negri, Raphael Rubino, and\nMarco Turchi. 2018. Findings of the WMT 2018\nShared Task on Automatic Post-Editing. In Pro-\nceedings of the Third Conference on Machine Trans-\nlation, Volume 2: Shared Task Papers , Brussels, Bel-\ngium. Association for Computational Linguistics.\nRajen Chatterjee, Marion Weller, Matteo Negri, and\nMarco Turchi. 2015. Exploring the Planet of the\nAPEs: A Comparative Study of State-of-the-art\nMethods for MT Automatic Post-Editing. In Pro-\nceedings of the 53rd Annual Meeting of the Associ-\nation for Computational Linguistics and the 7th In-\nternational Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers) , pages 156–\n161, Beijing, China.\nJonathan H. Clark, Chris Dyer, Alon Lavie, and\nNoah A. Smith. 2011. Better Hypothesis Testing\nfor Statistical Machine Translation: Controlling for\nOptimizer Instability . In Proceedings of the 49th\nAnnual Meeting of the Association for Computa-\ntional Linguistics: Human Language Technologies:\nShort Papers - Volume 2 , HLT ’11, pages 176–181,\nStroudsburg, PA, USA. Association for Computa-\ntional Linguistics.\nJonas Gehring, Michael Auli, David Grangier, De-\nnis Yarats, and Yann N. Dauphin. 2017. Convo-\nlutional Sequence to Sequence Learning. CoRR,\nabs/1705.03122.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep Residual Learning for Image\nRecognition. 2016 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) , pages 770–\n778.\nMarcin Junczys-Dowmunt and Roman Grundkiewicz.\n2016. Log-linear Combinations of Monolingual and\nBilingual Neural Machine Translation Models for\nAutomatic Post-Editing . In Proceedings of the First\nConference on Machine Translation , pages 751–\n758, Berlin, Germany.\nMarcin Junczys-Dowmunt and Roman Grundkiewicz.\n2017. The AMU-UEdin Submission to the WMT\n2017 Shared Task on Automatic Post-Editing . In\nProceedings of the Second Conference on Machine\nTranslation, Volume 2: Shared Task Papers , pages\n639–646, Copenhagen, Denmark. Association for\nComputational Linguistics.\nDiederik P Kingma and Jimmy Lei Ba. 2015. Adam:\nA Method for Stochastic Optimization. ICLR.\nKevin Knight and Ishwar Chander. 1994. Automated\nPostediting of Documents. In Proceedings of the\nTwelfth National Conference on Artiﬁcial Intelli-\ngence (Vol. 1) , AAAI ’94, pages 779–784, Seattle,\nWashington, USA.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\n833\nRichard Zens, Chris Dyer, Ond ˇrej Bojar, Alexan-\ndra Constantin, and Evan Herbst. 2007. Moses:\nOpen Source Toolkit for Statistical Machine Trans-\nlation. In Proceedings of the 45th Annual Meeting\nof the ACL: Interactive Poster and Demonstration\nSessions, pages 177–180, Prague, Czech Republic.\nAntonio Lagarda, Vicent Alabau, Francisco Casacu-\nberta, Roberto Silva, and Enrique D ´ıaz-de Lia ˜no.\n2009. Statistical Post-editing of a Rule-based Ma-\nchine Translation System. In Proceedings of Human\nLanguage Technologies , pages 217–220, Strouds-\nburg, PA, USA.\nJindˇrich Libovick ´y, Jind ˇrich Helcl, Marek Tlust ´y,\nOndˇrej Bojar, and Pavel Pecina. 2016. CUNI Sys-\ntem for WMT16 Automatic Post-Editing and Multi-\nmodal Translation Tasks. In Proceedings of the First\nConference on Machine Translation , pages 646–\n654, Berlin, Germany. Association for Computa-\ntional Linguistics.\nMatteo Negri, Marco Turchi, Rajen Chatterjee, and\nNicola Bertoldi. 2018. ESCAPE: a Large-scale\nSynthetic Corpus for Automatic Post-Editing. In\nProceedings of the Eleventh International Confer-\nence on Language Resources and Evaluation (LREC\n2018), Miyazaki, Japan. European Language Re-\nsources Association (ELRA).\nJan Niehues, Eunah Cho, Thanh-Le Ha, and Alex\nWaibel. 2016. Pre-Translation for Neural Machine\nTranslation. In Proceedings of COLING 2016, the\n26th International Conference on Computational\nLinguistics: Technical Papers , pages 1828–1836,\nOsaka, Japan. The COLING 2016 Organizing Com-\nmittee.\nSantanu Pal, Sudip Naskar, and Josef van Genabith.\n2015. UdS-Sant: English–German Hybrid Machine\nTranslation System. In Proceedings of the Tenth\nWorkshop on Statistical Machine Translation , pages\n152–157, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nSantanu Pal, Sudip Kumar Naskar, and Josef van Gen-\nabith. 2016a. Multi-Engine and Multi-Alignment\nBased Automatic Post-Editing and its Impact on\nTranslation Productivity. In Proceedings of COL-\nING 2016, the 26th International Conference on\nComputational Linguistics: Technical Papers , pages\n2559–2570, Osaka, Japan.\nSantanu Pal, Sudip Kumar Naskar, Mihaela Vela, and\nJosef van Genabith. 2016b. A Neural Network\nBased Approach to Automatic Post-Editing . In Pro-\nceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers), pages 281–286, Berlin, Germany. Associa-\ntion for Computational Linguistics.\nSantanu Pal, Sudip Kumar Naskar, Mihaela Vela, Qun\nLiu, and Josef van Genabith. 2017. Neural Au-\ntomatic Post-Editing Using Prior Alignment and\nReranking. In Proceedings of the 15th Confer-\nence of the European Chapter of the Association\nfor Computational Linguistics: Volume 2, Short Pa-\npers, pages 349–355, Valencia, Spain. Association\nfor Computational Linguistics.\nSantanu Pal, Marcos Zampieri, and Josef van Genabith.\n2016c. USAAR: An Operation Sequential Model\nfor Automatic Statistical Post-Editing . In Proceed-\nings of the First Conference on Machine Translation ,\npages 759–763, Berlin, Germany.\nCarla Parra Escart ´ın and Manuel Arcedillo. 2015a.\nLiving on the Edge: Productivity Gain Thresh-\nolds in Machine Translation Evaluation Metrics.\nIn Proceedings of the Fourth Workshop on Post-\nediting Technology and Practice , pages 46–56, Mi-\nami, Florida (USA). Association for Machine Trans-\nlation in the Americas (AMTA).\nCarla Parra Escart ´ın and Manuel Arcedillo. 2015b.\nMachine Translation Evaluation Made Fuzzier: A\nStudy on Post-Editing Productivity and Evaluation\nMetrics in Commercial Settings. In Proceedings of\nthe MT Summit XV , Miami (Florida). International\nAssociation for Machine Translation (IAMT).\nRudolf Rosa, David Mare ˇcek, and Ond ˇrej Du ˇsek. 2012.\nDEPFIX: A System for Automatic Correction of\nCzech MT Outputs. In Proceedings of the Seventh\nWorkshop on Statistical Machine Translation , pages\n362–368, Stroudsburg, PA, USA.\nJohann Roturier. 2009. Deploying Novel MT Technol-\nogy to Raise the Bar for Quality: A Review of Key\nAdvantages and Challenges. In Proceedings of the\ntwelfth Machine Translation Summit .\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural Machine Translation of Rare Words\nwith Subword Units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2016, August 7-12, 2016, Berlin,\nGermany, Volume 1: Long Papers .\nMichel Simard, Cyril Goutte, and Pierre Isabelle.\n2007a. Statistical Phrase-based Post-Editing. In\nHuman Language Technologies 2007: The Confer-\nence of the North American Chapter of the Asso-\nciation for Computational Linguistics; Proceedings\nof the Main Conference , pages 508–515, Rochester,\nNew York.\nMichel Simard, Nicola Uefﬁng, Pierre Isabelle, and\nRoland Kuhn. 2007b. Rule-based Translation With\nStatistical Phrase-based Post-editing. In Proceed-\nings of the Second Workshop on Statistical Machine\nTranslation, pages 203–206.\nTAUS/CNGL Report. 2010. Machine Translation Post-\nEditing Guidelines Published . Technical report,\nTAUS.\nDusan Varis and Ond ˇrej Bojar. 2017. CUNI System for\nWMT17 Automatic Post-Editing Task. In Proceed-\nings of the Second Conference on Machine Trans-\nlation, Volume 2: Shared Task Papers , pages 661–\n666, Copenhagen, Denmark. Association for Com-\nputational Linguistics.\n834\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is All\nYou Need . In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30 , pages 5998–6008. Curran As-\nsociates, Inc.\n835",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.828519880771637
    },
    {
      "name": "Computer science",
      "score": 0.7562963962554932
    },
    {
      "name": "Machine translation",
      "score": 0.7058860063552856
    },
    {
      "name": "Encoder",
      "score": 0.7006312012672424
    },
    {
      "name": "Sentence",
      "score": 0.5565273761749268
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5185602307319641
    },
    {
      "name": "Natural language processing",
      "score": 0.48360008001327515
    },
    {
      "name": "Source code",
      "score": 0.44636037945747375
    },
    {
      "name": "Speech recognition",
      "score": 0.44103461503982544
    },
    {
      "name": "Programming language",
      "score": 0.19545939564704895
    },
    {
      "name": "Voltage",
      "score": 0.15203765034675598
    },
    {
      "name": "Engineering",
      "score": 0.0958048403263092
    },
    {
      "name": "Electrical engineering",
      "score": 0.07233661413192749
    },
    {
      "name": "Operating system",
      "score": 0.06973272562026978
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I33256026",
      "name": "German Research Centre for Artificial Intelligence",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I91712215",
      "name": "Saarland University",
      "country": "DE"
    }
  ],
  "cited_by": 14
}