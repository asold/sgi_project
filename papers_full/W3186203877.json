{
  "title": "Evaluation of Human-AI Teams for Learned and Rule-Based Agents in Hanabi",
  "url": "https://openalex.org/W3186203877",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287212962",
      "name": "Siu, Ho Chit",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287212963",
      "name": "Pena, Jaime D.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287212964",
      "name": "Chen, Edenna",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287212965",
      "name": "Zhou, Yutai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287212966",
      "name": "Lopez, Victor J.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287212967",
      "name": "Palko, Kyle",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287212968",
      "name": "Chang, Kimberlee C.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2612040429",
      "name": "Ross E. Allen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3175924191",
    "https://openalex.org/W2963186323",
    "https://openalex.org/W2998299793",
    "https://openalex.org/W2754512434",
    "https://openalex.org/W2514260088",
    "https://openalex.org/W2914351253",
    "https://openalex.org/W2765782498",
    "https://openalex.org/W2061141931",
    "https://openalex.org/W2157289187",
    "https://openalex.org/W2996037775",
    "https://openalex.org/W2970894611",
    "https://openalex.org/W2998004401",
    "https://openalex.org/W2913781869",
    "https://openalex.org/W2927275098",
    "https://openalex.org/W2902907165",
    "https://openalex.org/W3034218175",
    "https://openalex.org/W2063052894",
    "https://openalex.org/W2963147362",
    "https://openalex.org/W1992154343",
    "https://openalex.org/W2962808690",
    "https://openalex.org/W2963098996",
    "https://openalex.org/W2942182378",
    "https://openalex.org/W3023153430",
    "https://openalex.org/W2749807327",
    "https://openalex.org/W3124056987",
    "https://openalex.org/W3009741087",
    "https://openalex.org/W1757796397",
    "https://openalex.org/W1606056663",
    "https://openalex.org/W3092104587",
    "https://openalex.org/W1623168575",
    "https://openalex.org/W2974976965",
    "https://openalex.org/W2913898978",
    "https://openalex.org/W3103780890",
    "https://openalex.org/W1275367343",
    "https://openalex.org/W2927819804",
    "https://openalex.org/W2762117857",
    "https://openalex.org/W2982316857",
    "https://openalex.org/W2993490502",
    "https://openalex.org/W2776831310",
    "https://openalex.org/W2110171129",
    "https://openalex.org/W2107031757",
    "https://openalex.org/W2264742718"
  ],
  "abstract": "Deep reinforcement learning has generated superhuman AI in competitive games such as Go and StarCraft. Can similar learning techniques create a superior AI teammate for human-machine collaborative games? Will humans prefer AI teammates that improve objective team performance or those that improve subjective metrics of trust? In this study, we perform a single-blind evaluation of teams of humans and AI agents in the cooperative card game Hanabi, with both rule-based and learning-based agents. In addition to the game score, used as an objective metric of the human-AI team performance, we also quantify subjective measures of the human's perceived performance, teamwork, interpretability, trust, and overall preference of AI teammate. We find that humans have a clear preference toward a rule-based AI teammate (SmartBot) over a state-of-the-art learning-based AI teammate (Other-Play) across nearly all subjective metrics, and generally view the learning-based agent negatively, despite no statistical difference in the game score. This result has implications for future AI design and reinforcement learning benchmarking, highlighting the need to incorporate subjective metrics of human-AI teaming rather than a singular focus on objective task performance.",
  "full_text": "Evaluation of Human-AI Teams for Learned and\nRule-Based Agents in Hanabi\nHo Chit Siu∗ Jaime D. Peña∗ Yutai Zhou∗ Edenna Chen† Victor J. Lopez‡\nKyle Palko‡ Kimberlee C. Chang∗ Ross E. Allen∗\nAbstract\nDeep reinforcement learning has generated superhuman AI in competitive games\nsuch as Go and StarCraft. Can similar learning techniques create a superior AI\nteammate for human-machine collaborative games? Will humans prefer AI team-\nmates that improve objective team performance or those that improve subjective\nmetrics of trust? In this study, we perform a single-blind evaluation of teams of\nhumans and AI agents in the cooperative card game Hanabi, with both rule-based\nand learning-based agents. In addition to the game score, used as an objective\nmetric of the human-AI team performance, we also quantify subjective measures of\nthe human’s perceived performance, teamwork, interpretability, trust, and overall\npreference of AI teammate. We ﬁnd that humans have a clear preference toward\na rule-based AI teammate (SmartBot) over a state-of-the-art learning-based AI\nteammate (Other-Play) across nearly all subjective metrics, and generally view\nthe learning-based agent negatively, despite no statistical difference in the game\nscore. This result has implications for future AI design and reinforcement learning\nbenchmarking, highlighting the need to incorporate subjective metrics of human-AI\nteaming rather than a singular focus on objective task performance. 4\n1 Introduction\nAdvances in artiﬁcial intelligence (AI) have resulted in agents that perform at superhuman levels\nwithin domains previously thought to be solely the purview of human intelligence. Such performance\nhas been demonstrated through application of reinforcement learning (RL) in environments such\nas board games [6, 39], arcade games [31], real-time strategy games [44], multiplayer online battle\narenas [4], and simulated aerial dogﬁghts [11].\nIn nearly all cases, these demonstrations of AI superiority are in purely adversarial, one- or two-player\ngames. However, in order to achieve real-world applicability and adoption, AI must be able to\ndemonstrate teaming intelligence, particularly with human teammates [ 23]. Due to the focus on\nadversarial games, teaming intelligence has been understudied in RL research. AI teammates must\nalso exhibit behavior that engenders an appropriate level of certain human reactions, such as trust,\nmental workload, and risk perception [26, 37]. Failure to do so risks the same kind of misuse, disuse,\nand abuse that Parasuraman and Riley illustrated with traditional automation systems [37]. These\nissues are distinct from much of current multi-agent AI work, as AI that is able to team effectively\nwith other AI agents has failed to work effectively with humans [9].\n∗MIT Lincoln Laboratory, {hochit.siu,jdpena,yutai.zhou,chestnut,ross.allen}@ll.mit.edu\n†MIT Department of Electrical Engineering and Computer Science, edenna@mit.edu\n‡U.S. Air Force Artiﬁcial Intelligence Accelerator, {victor.lopez.10,kyle.palko.1}@us.af.mil\n4DISTRIBUTION STATEMENT A. Approved for public release. Distribution is unlimited.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2107.07630v3  [cs.AI]  21 Oct 2021\nThe objective of this paper is to evaluate human-AI teaming in the cooperative, imperfect-information\ngame of Hanabi. We consider not only the objective performance of a human-AI team, but also the\nsubjective human reactions and preferences when working with different kinds of AI teammates.\nBased on the success of applying deep reinforcement learning to create superhuman AI in adversarial\ngames, we hypothesize that similar RL techniques can render collaborative AI that outperform and\nare preferred over rule-based agents in human-AI Hanabi teams. Our results show that this hypothesis\nis not supported given the current state of the art of collaborative RL agents. Human participants\nshow a clear preference toward rule-based AI even though the learning-based AI perform no worse\nand are speciﬁcally optimized for teaming with previously unknown partners (e.g. humans) [21]. To\nthe best of our knowledge, this is the ﬁrst comparative study of objective performance of rule-based\nand learning-based Hanabi AI in human-teaming experiments, as well as the ﬁrst quantiﬁed study of\nsubjective human preferences toward such AI.\n2 Background\n2.1 Hanabi\nHanabi is a cooperative card game in which two to ﬁve players attempt to stack twenty-ﬁve cards\ninto ﬁve different ﬁreworks (piles), one for each suit (color) and by ascending rank (number). We\nconsider only the standard, two-player version, where the deck is composed of 50 cards, ﬁve suits,\neach suit having three 1s, two 2s, two 3s, two 4s, and one 5, and each player is dealt a hand of ﬁve\ncards. Hanabi’s difﬁculty lies in the fact that players can only see their teammate’s hand and never\ntheir own and communication about cards is strictly limited.\nGames start with eight hint tokens and three bomb tokens. Each turn, a player may discard a card\nfrom their hand, play a card from their hand, or give a hint to a teammate. Whenever a card is played\nor discarded, it is revealed and a new card is drawn from the deck. A correctly-played card is placed\nin the appropriate ﬁrework; an incorrectly-played card is discarded and the team loses one bomb.\nHinting costs one hint token and allows a player to reveal either all cards of a certain suit or rank in\ntheir teammate’s hand. Hint tokens are earned back when a card is discarded, or when a 5 card is\nsuccessfully played. The ﬁnal score is the sum of the top card in each ﬁrework, for a maximum of 25\npoints. The game ends when all ﬁreworks have been completed, the deck is empty (and each player\nhas one additional turn), or the team loses 3 bombs.\nHanabi is a purely cooperativegame with imperfect information, and limited, codiﬁed communication.\nThese properties make it an interesting challenge for teaming since players must consider the reasons\nfor their teammates’ actions and any implied information, while avoiding misinterpretations. Bard\net al. [2] presents a more complete treatment of Hanabi and its properties as an AI problem.\n2.2 Training and Evaluating AI Teams\nHanabi’s uniquely collaborative nature has made it the subject of several AI challenges in recent years\n[2, 46]. In these challenges AI agents are paired with other teammate agents, and their performance\nis measured based on the paradigms of self-play and cross-play [21].\nSelf-play (SP) is when an AI plays a game with a copy of itself as the opponent (adversarial games)\nor teammate (cooperative games like Hanabi). Self-play can be used as a form of RL training [1, 20],\nand/or as a form of evaluation [2].\nCross-play (XP) is an evaluation-only paradigm where an agent is teamed with other agents (AI or\nhumans) that were not encountered during training, measuring how well an agent can cooperate with\npreviously-unseen teammates. SP-trained agents can achieve high SP-scores by developing “secretive\nconventions” that are not understood by agents not present during training, thus completely failing in\nthe XP setting [21]. Cross-play with humans (human-play) is of particular importance as it measures\nhuman-machine teaming (HMT) and is the foundation for the experiments in our paper.\nCross-play can be evaluated under the zero-shot coordination (ZSC) [ 21] or ad-hoc teaming [ 40]\nsettings. ZSC assumes that teams are formed between agents that have no prior knowledge of each\nother; therefore it is impossible to train an agent to bias towards teammate idiosyncrasies, though\nthe agents may agree to a common training strategy beforehand. On the other hand, ad-hoc teaming\nattempts to achieve coordination by having agents update their policy while interacting with other\n2\nTable 1: Survey of Rule-based and Learning-Based Hanabi AI Performance in 2-Player Games\nHanabi AI Original Author References Self-Play Cross-Play Human-Play\nVan den Bergh Van den Bergh 2016 [42, 45, 15] 13.8 10.8 –\nSelf-Recognition Osawa 2015 [36, 15] 15.9 – –\nPiers Walton-Rivers 2017 [45, 38] 17.3 11.2 –\nIntentional AI Eger 2017 [13, 8] 12.6 13.8 15.0\nExpectimax Bouzy 2017 [5, 15] 19.0 – –\nMirrorSituational Canaan 2018 [7, 15] 20.1 12.4 –\nRIS-MCTS Goodman 2019 [15, 8] 20.6 13.3 –\nWTFWThat J. Wu 2018 [48, 2, 15] 19.5 – –\nFireFlower D. Wu 2018 [47, 2, 14] 22.7 – –\nImplicature AI Liang 2019 [28, 8] 18.9 – ∼ 15\nSmartBot (SB) O’Dwyer 2019 [33, 2, 15] 23.0 – –\nIL-Valuebot Sarmasi 2021 [38, 33] 18.0 – –\nACHA Bard 2020 [2, 20] 22.7 1.0 –\nBAD Foerster 2019 [14, 2, 20] 23.9 – –\nSAD Hu 2019 [20, 21] 24.0 3.0 –\nSAD+AUX Hu 2019 [20, 21] 24.0 21.1 9.2\nSPARTA Lerer 2020 [27, 8] 24.6 – –\nOther-Play (OP) Hu 2020 [21] 24.1 22.5 15.8\nWe provide Hanabi AI agents’ scores as they are reported in existing literature. This gives a notional\nperspective of the current state of the art; however, we note that evaluation conditions have not\nbeen standardized throughout previous literature (i.e. number of game seeds for self-play, pool of\nteammates for cross-play, etc.). Therefore some caution is needed when making direct comparisons\nbetween reported scores. Please see respective literature for details on evaluation conditions.\nagents, and the the pool of partner agents can be known in advance (though it can be of arbitrarily\nlarge size, making such knowledge practically impossible to exploit).\n2.3 AI for Hanabi\nTable 1 summarizes the most salient published Hanabi AI agents in 2-player games with game score in\nself-play, cross-play, and human-play. The table is separated intorule-based (top) and learning-based\n(bottom) agent types. Rule-based agents have a policy composed of a predeﬁned set of rules to follow\ngiven any particular game situation, and the rules are often derived from human domain knowledge.\nLearning-based agents, on the other hand, use statistical learning methods to adjust the parameters of\ntheir policy. The mechanism governing what action the policy will choose is learned via the agent’s\nexperience, without the need for human domain knowledge.\nMost early work focused on rule-based AI [ 36, 42, 45]. Goodman won the 2018 CoG Hanabi\nCompetition [46] with a rule-based Monte Carlo Tree Search agent. O’Dwyer’s SmartBot (SB) is\nthe highest performing rule-based agent created to date, in terms of self-play [33, 2, 15]. Due to its\nstate-of-the-art SP score, as well as a readily available implementation [ 33], we select SB as our\nrule-based agent in the human-AI experiments presented in Sections 3 and 4. We found no previous\nwork that evaluated SB in the cross-play or human-play settings.\nRecently developed learning-based agents have demonstrated breakthrough Hanabi performance.\nSarmasi et al. provide a collection of agents trained with imitation learning that nearly match the\nperformance of the rule-based agents from which they were trained [38]. A sequence of publications\n[14, 20, 27, 21] offered reinforcement learning agents that each advanced the state of the art self-play\nand/or cross-play performance at the time of publication. This culminated with the SPARTA [27]\nand Other-Play (OP) [21] agents; which, respectively, represent the highest performance to date in\nself-play and cross-play of any agent type.\nThe OP agent is selected as the learning-based agent for study in our human-AI experiments described\nin Sections 3 and 4. We choose OP, not only for its state-of-the-art cross-play performance, but\nalso because it uses a learning objective function speciﬁcally designed to optimize for zero-shot\ncoordination settings. Due to this optimization, we expect that humans would subjectively prefer and\n3\nobjectively perform better with OP agent teammates over other AI teammates that were not designed\nfor the zero-shot coordination setting, such is SmartBot.\nA handful of works have conducted human-play experiments; however these works are uncommon\ndue to the signiﬁcant time and effort required to conduct such experiments. Both Eger et al. [13]\nand Liang et al. [28] propose rule-based AI derived from Gricean maxims [ 17] and achieved an\naverage human-play score of approximately 15.0 over a set of experiments that included hundreds of\nhuman participants. The learning-based Other-Play agent is arguably the highest performer in terms\nof human-play with an average score of 15.8, however the experiments were run with a much smaller\npool of human participants [21]. No prior works were found that provide a comparative study of\nrule-based and learning-based Hanabi AI in human experiments.\n2.4 Human-AI Teaming and Metrics\nHuman-machine interaction evaluations typically consider two broad categories of metrics: objective\nperformance metrics (raw score, error rates, accuracy, time required, etc.), and subjective team- or\nhuman-focused metrics (situation awareness, trust, workload, etc.). Measurement of the former\nis heavily task-dependent, and are often the primary evaluation metrics of AI systems. The latter,\nhowever, can also involve metrics for probing systems and teams in a quantitative way, with important\nimplications for how technology is used and adopted [37].\nOne key metric of teaming is trust, which is deﬁned by Lee and See as “the attitude that an agent\nwill help achieve an individual’s goals in a situation characterized by uncertainty and vulnerability”\n[26]. Potential difﬁculties with trust include trust calibration (whether one’s trust of an agent is\ncommensurate with its capabilities) and trust resolution (whether the range of situations where a\nhuman trusts a system is commensurate with its range of capabilities).\nClosely related to trust are the notions oflegibility (being expressive of one’s intent) andpredictability\n(matching one’s expectations) [12]. In measuring teaming, one might directly ask humans about their\nteammates’ intent and their own expectations, particularly in relation to shared goals. Dragan et al.\n[12] argue that these two ideas trade off in the context of robot motion, but that legibility is the more\nimportant factor when working in close collaboration with a human. Similar arguments may also be\nmade in the context of Hanabi, where maximum legibility is key to guiding a team’s actions in the\ntightly-coupled, imperfect-information scenario.\nHoffman [19] proposes a set of subjective and objective methods (overlapping with the aforemen-\ntioned ideas) to measure ﬂuency in human-robot interaction, deﬁned as the “coordinated meshing\nof joint activities between members of a well-synchronized team.” Fluency encompasses subjective\nelements such as teamwork, trust, and positive perception; and objective measures of timing and\nconcurrency.\nMuch of the contemporary work on learning-based agents has not evaluated such teaming metrics,\ndue to their focus on single-player [3] or adversarial games [39, 43]. In this work, we consider the\nuse of both objective performance and subjective teaming-based measurement of human-AI pairs\nin the context of Hanabi. We aim to provide a more thorough analysis of the human-AI teaming\ndata than previously-discussed Hanabi HMT studies [13, 28, 21]. Rather than proposing a new AI\nwith better raw performance, we are interested in the subjective aspects of human-AI teaming with\nstate-of-the-art rule- and learning-based AI, and how they might drive future AI development.\n3 Methods\n3.1 Human-AI Teaming Experiment\nExperiments consisted of two-player Hanabi games played by teams of one human participant and\none AI agent. Experiments aimed to measure the objective team performance and subjective human\nreactions to different types of AI. The AI agents used in experiments were the Other-Play RL agent\n[21] (speciﬁcally the OP+SAD+AUX agent, hereafter referred to as the OP bot) and the “SmartBot”\nagent [33] (hereafter referred to as the SB bot), both of which have MIT Licenses. These agents were\nchosen because they were the top-performing learning-based and rule-based Hanabi AI, respectively\nat the time of the experiment.\n4\nParticipants were ﬁrst introduced to the experiment and the rules of Hanabi as deﬁned in Section 2.1.\nThe experiment followed with a brief familiarization game so participants could acquaint themselves\nwith the game interface. Then, each participant played two sets (blocks) of three games, with each set\nusing a different AI teammate. The participant was not informed which AI teammate played each\nblock of games and the order of the AI teammates was counterbalanced over the course of the study.\nParticipants answered Likert scale surveys after each game (Table 2, left), a NASA Task Load Index\n(TLX) survey after each block [18], and a Likert scale survey after both blocks that directly compared\ntheir experience with both agents (Table 2, right). Likert scale survey questions were largely derived\nfrom a compilation of similar questions in Hoffman et al. [19]. Unless otherwise noted, Likert scales\nwere arranged with 1 corresponding to “strongly disagree” and 7 corresponding to “strongly agree.”\nA total of 29 adult participants completed an experiment and each provided written, informed consent.\nThe protocol was approved by the MIT Committee on the Use of Humans as Experimental Subjects\n(protocol E-2520) and the United States Department of Defense Human Research Protection Ofﬁce\n(protocol MITL20200003). Participants received a $10 USD gift card at the end of their experiment,\nand the highest-scoring participant received an additional $50 gift card. Each experiment took\napproximately 1.5 to 2.5 hours. The total amount spent on participant payment was $350, with one\nparticipant being paid twice due to technical difﬁculties ending their initial session early. Experiments\nwere conducted virtually, with all interactions occurring through video-conference, online surveys,\nand the Hanabi game interface, adapted from [ 27]. No personally-identiﬁable information was\ncollected, and no signiﬁcant risk to participants was expected.\nLiang et al. [28] conducted human experiments with subjective survey questions; however the only\nsigniﬁcant result reported was that humans were more likely to mistake Implicature AI as another\nhuman. Although Hu et al. [ 21] conducted some human experiments with the OP bot and a self-play\nRL bot, their experiments only focused on game score, involved only one game with each agent, and\ndid not consider participant expertise in Hanabi.\nTable 2: Post-Game (Left) and Post-Experiment (Right) Evaluation Statements\n7-point Likert Scale statement\nG1 I am playing well.\nG2 The agent is playing poorly.\nG3 The team is playing well.\nG4 This game went well.\nG5 The agent and I have good teamwork.\n[ﬂuency]\nG6 The agent is contributing to the\nsuccess of the team.\nG7 I understand the agent’s intentions.\n[legibility]\nG8 The agent does not understand my\nintentions. [legibility]\nG9 I feel comfortable playing with this agent.\nG10 I do not trust the agent. [trust]\nG11 The agent is not a reliable teammate.\n[predictability]\nG12 I am not conﬁdent in my gameplay.\n7-point Likert Scale statement\nE1 Which agent did you prefer playing with?\nE2 Which agent did you trust more? [trust]\nE3 Which agent did you understand more?\n[legibility]\nE4 Which agent understood you better? [legibility]\nE5 Which agent was the better Hanabi player?\nE6 Which agent was more reliable? [predictability]\nE7 Which agent had a better understanding of the\ngame on average?\nE8 Which agent caused you to have a greater\nmental workload? [mental workload]\nLeft: Statement order was randomized when presented to participants.\nRight: Rating 1 is “strongly prefer ﬁrst agent” and rating 7 is “strongly prefer second agent.”\nItems associated with particular human factors constructs have the construct name in brackets (not shown to\nparticipants).\n3.2 Statistical Analysis\nWe evaluated both objective and subjective metrics of human-AI teaming, with the overall hypothesis\nthat the Other-Play RL agent (OP) is preferred over- and would outperform the rule-based SmartBot\n(SB) agent. As an objective measure, we consider the team score during each game. For subjective\nmeasures, we consider outcomes related to perceived performance, teamwork, legibility, comfort,\ntrust, and workload as measured by the Likert surveys. Linear mixed-effects regression models were\nused for both objective and subjective variables, following the recommendation of Norman [32].\n5\nDue to space constraints, we do not present the results of the TLX here, but include one question on\nmental workload from the post-experiment survey (Table 2, E8).\nBoth objective and subjective omnibus tests for post-game results used second-order mixed-effects\nmodels with ﬁxed factors of (1) AI agent, (2) self-rated Hanabi experience, (3) block (the ﬁrst or\nsecond set of three games), and (4) game number (ﬁrst, second, or third game within a block),\nand a random factor of participant number. AI agent and participant number were considered\ncategorical variables. Post-experiment surveys were evaluated separately with one-samplet-tests, and\na Holm–Bonferroni multiple comparison correction, since these tests were looking for differences\nfrom a neutral value on the Likert scale, and there were multiple hypotheses being tested. Additionally,\ncorrelations between self-rated experience, and post-game responses were checked against game\nscore.\n4 Results\nWe present the results of the teaming experiment. Our results indicate that despite no signiﬁcant\ndifference in objective performance between teaming with the two agents (Section 4.1), human\nsentiment results (Sections 4.2 and 4.3) show a clear preference toward a rule-based agent over a\nlearning-based agent.\nFor context, we have colored most of the plots to show the self-rated Hanabi experience level of\nthe participant with which the data are associated, and we note that our participant pool is skewed\ntowards higher-self-rated-experience players. Self-rated Hanabi experience was based on ratings of\nthe demographic survey statement “I am experienced in Hanabi” (see supplemental materials for\ndemographic survey results).\n4.1 Game Score\nThe mixed-effects regression did not support score differences due to agent type ( t(158) =\n0.374), p= 0.709) or self-rated Hanabi experience (t(158) = 1.228, p= 0.221) (Figure 1). This\nmeans that the data do not support OP under-performing relative to SB.\nThe dependent variable of game score only has a signiﬁcant effect of block ( b = 5.282, t(158) =\n2.636, p= 0.009), and a near-signiﬁcant effect of game number ( b = 2.957, t(158) = 1.887, p=\n0.061). The positive regression slope in both cases indicates that this is likely a learning effect over\ntime; i.e. the human participant adapts his/her play over time.\nFigure 1: Game scores by agent type (left) and self-rated player experience (right). No signiﬁcant\ndifferences were found when teaming with either agent, and correlation with self-rated experience\nwas signiﬁcant only for SmartBot games.\nSince score is the primary performance metric of interest in Hanabi, and acts as the reward function\nfor RL agents in this domain, we examined a few additional correlations with score. Note that since\nthese are bivariate correlations, they are not as complete as the aforementioned statistical regression.\nSelf-rated Hanabi experience and score had a small, but signiﬁcant positive correlation when pooling\nboth agents’ games (p = 0.0053, r = 0.214). Correlation remains signiﬁcant for the subset of SB\ngames (p = 0.0023, r = 0.247), but is not signiﬁcant for OP games ( p = 0.0867, r = 0.1881),\nindicating that for this bivariate analysis, experience only correlates with score for SB, not with OP.\n6\nA small, but signiﬁcant positive correlation was found between subjective team performance (G3,\nG4) and score (p = 0.0003, r = 0.275 and p = 0.0002, r = 0.280). However, subjective ratings of\nself- and agent-performance (G1, G2) were not signiﬁcantly correlated to score.\nWhen participants were ordered by the absolute differences in their total scores with each agent,\nthe ten subjects with the greatest differences (max/min point differences of 57 and 17), six of the\nten had better performance with OP, but when considering only the top ﬁve subjects in terms of\nscore difference (max/min differences of 57 and 28), only the participant with the greatest difference\nperformed better with SB, and all four others performed better with OP. A full plot of scores by\nsubject and agent type is in the supplemental materials.\n4.2 Post-Game Sentiments\nSigniﬁcant effects in post-game subjective measures are summarized in Table 3. Although the\nstatistical model considers factors of block and game, which were sometimes signiﬁcant, we do\nnot report these main effects or their interactions in the table, in order to focus on the independent\nvariables of greater interest.\nIn all cases where the interaction of agent type and Hanabi experience was found to be signiﬁcant,\nmore experienced Hanabi players rated the Other-Play agent much more negatively than the SmartBot\nagent, while novices rated the two similarly (Figure 2). However, there was no signiﬁcant difference\nbetween novice and expert ratings of the SmartBot agent. Cases where neither agent nor experience\nwere signiﬁcant factors are shown in Figure 3.\nTable 3: Post-Game Sentiment Statistics (Statistically-Signiﬁcant Factors Only)\nDependent Variable Factor t p\nI am playing well (G1) experience 2.698 0.008\nThe agent is playing poorly (G2) experience 4.044 < 0.0001\nThe team is playing well (G3) agent:experience −2.082 0.039\nThe agent and I have good teamwork (G5) agent 2.578 0.011\nagent:experience −3.021 0.003\nI understand the agent’s intentions (G7) agent:experience −2.273 0.024\nThe agent does not understand my intentions (G8) experience 2.098 0.037\nagent:experience −3.166 0.002\nI feel comfortable playing with this agent (G9) agent 3.302 0.001\nagent:experience −3.561 < 0.0001\nThe agent is not a reliable teammate (G11) experience 3.159 0.002\nDegrees of freedom for t-tests are 164 in all cases. Agent is agent type (SmartBot or Other-Play), and experience\nis self-rated Hanabi experience. Agent:experience is the interaction effect of agent and experience\nTo speciﬁcally examine the effect of self-rated player experience, we performed post-hoc pairwise\ncomparisons in cases where experience was signiﬁcant. Participants were pooled into “novice”\n(n = 10, self-rated experience of ≤ 5) and “expert” ( n = 19, self-rated experience of > 5)\ngroups, and comparisons were made on ratings of each agent in the cases where an interaction effect\nwas signiﬁcant (G3, G5, G7, G8, G9). The groups did not rate SB signiﬁcantly differently, but\nexperts always rated OP worse than novices did. The difference in G3 “the team is playing well”\n(t(85) = 3.551, p <0.001, effect size d = 0.752) was not as stark as the others (t(85) = 5.068 to\n5.855, p <0.0001, |d|> 1.0), but all were still clearly signiﬁcant, and all but one case had large\neffect sizes.\n4.3 Post-Experiment Sentiments\nParticipants’ direct comparisons of the agents (Table 2) are shown in Figure 4. The statements were\npresented to participants as comparing the “ﬁrst” and “second” agents (without reference to agent\ntype, which was not disclosed to participants). For ease of interpretation, we matched the responses\nto the agent type, and ﬂipped the scale as appropriate. All positively-framed questions (E1 to E8)\nshowed a strong preference to SB over OP (corrected p <0.05), while mental workload (E8) was\nsplit.\n7\nFigure 2: Participant rating for post-game questions by self-rated Hanabi experience (SB vs OP),\nwhere statistically signiﬁcant differences related to factors of agent and/or experience were found\n(Table 3). The scale ranges from 1-7, corresponding to \"strongly disagree\" to \"strongly agree\".\nFigure 3: Participant rating for post-game questions by self-rated Hanabi experience where no\nstatistically signiﬁcant differences related to factors of agent and/or experience were found. The scale\nranges from 1-7, corresponding to \"strongly disagree\" to \"strongly agree\".\nWe note there were three participants who obtained a score of 24 with OP, with one of these\nparticipants obtaining 24 twice (no player achieved a 25 with OP). All three replied at the extreme\nend of our Likert scale (i.e., 1 or 7) for question E1 with a preference for SB. Interestingly, their\ncumulative scores for OP and SB, respectively, were: Participant 6 (played with OP ﬁrst, self-rated\nexperience of 7): 57 and 28; Participant 19 (SB ﬁrst, experience 7): 68 and 48; Participant 20 (OP\nﬁrst, experience 6): 70 and 35. The participant with the highest cumulative score (Participant 2, OP\nﬁrst, experience 7) had cumulative scores of 68 (OP) and 54 (SB) and preferred SB with a Likert\nrating of 6. All participant scores are provided in the Appendix (Figure 7).\nParticipant commentary indicated that low mental workload when working with OP was often caused\nby frustration with the agent and giving up on teaming with it. For example, after the OP bot failed\nto act on several hints from the human ( “I gave him information and he just throws it away” ) a\nparticipant commented that “At this point, I don’t know what the point is, ”regarding working with\nthe agent.\n8\nFigure 4: Post-experiment questions. All showed a statistically signiﬁcant preference for the rule-\nbased SmartBot (p <0.05), except the question on workload (E8). The scale ranges from 1 (\"strongly\nprefer OP\") to 7 (\"strongly prefer SB\").\n5 Discussion\n5.1 Other-Play Agent Performance\nOther-Play (OP) training is designed to avoid the creation of “secretive” conventions that can result\nfrom self-play training. However, even though OP agents can pair well with previously-unseen\nteammates, there are still assumptions placed on the types of teammates OP agents will encounter.\nNotably, OP assumes that teammates are also optimized for zero-shot coordination [21]. In contrast,\nhuman Hanabi players typically do not learn with this assumption. Pre-game convention-setting and\npost-game reviews are common practices for human Hanabi players, making human learning more\nakin to few-shot coordination. Furthermore, Hu et al. note that, due to practical implementation\ndetails, OP does not always avoid secretive conventions. For example, some OP agents use color hints\nto indicate discarding of the 5th card [21]. Such conventions are not legible to a human teammate\nwithout extensive observation.\n5.2 Implications for Human-AI Teaming\nThe results shown here have important implications for the development of learning-based, human-\nteaming agents. Regardless of the objective performance of AI systems, “teaming intelligence” is\nultimately required for real-world deployment. The difference between rule-based and learning-based\nsystems is particularly poignant here, where humans strongly favored a rule-based agent over an RL\nagent in many subjective metrics (and never signiﬁcantly favored the RL agent) despite achieving\nobjective scores that were not signiﬁcantly different across the two agent types. Caution should\nbe exercised when generalizing our results beyond Hanabi, though there is some indication of\ncross-domain consistency. The negative post-experiment sentiments expressed towards OP by some\nparticipants are similar to those expressed by participants during the DotA2 OpenaAI Five games\nwhere RL players exhibited playstyles that were not immediately understood by human players, even\nthough they were successful [34, 35].\nAn additional consideration is the user base of future learning-based systems. We note that between\npost-game and post-experiment surveys, nearly all statements/questions associated with human\nfactors constructs (except workload) had differences related to the agent and/or the participant’s\nexperience level. Experienced players rated the OP agent particularly negatively (Figure 2, Table\n3), despite similarity in scores. As domain experts are likely to be the ﬁrst users of AI technology\nin operational settings, their perception of AI is a key factor to its adoption. Part of the experts’\nnegativity toward OP may come from the way human experts make decisions in uncertain situations.\n9\nKlein’s recognition-primed decision-making model [25] indicates that experts typically rely on a\nbase of knowledge to recognize “prototypical” situations and alleviate much of the mental burden\nof decision-making. Working with an unpredictable and illegible teammate breaks experts’ ability\nto rely on much of their knowledge base. This notion is also supported by the score/experience\ncorrelations (Figure 1) which were only signiﬁcant for SB games.\nThis work indicates a need for RL methods that produce legible and predictable policies. Methods in\nthis direction generally involve either building inherently transparent models, or procedures for post-\nhoc analysis. The former includes human-language-driven policy speciﬁcation [41], or explanation\nlearning through construction of causal diagrams [30]. The latter includes saliency maps for visual\ndomains, and action preference explanation through reward decomposition [24].\n5.3 Limitations and Future Work\nThe tight coupling of coordination and performance, and the codiﬁcation of communication, make\nHanabi a unique test bed. Games without these properties may elicit different responses from\nhuman-AI teams, so generalization of this experiment beyond Hanabi must be treated carefully.\nDue to the manual and time-intensive experimental procedure —as well as the lack of readily available\nimplementations for many pre-existing Hanabi agents such as Intentional [13] and Implicature AI\n[28]—we were limited in our pool of participants and agents. Future work could increase both by\nusing an online, operator-free game-play and survey platform.\nWhile we made an effort to recruit participant with a diverse level of experience, the participant pool\nwas still skewed towards those with higher self-rating (though it is likely less biased than the pool\nfrom [28], who recruited exclusively from online gaming communities). Since Hanabi performance\nis heavily dependent on the team, it is difﬁcult to obtain an objective single-player skill rating. Still,\nthe relatively tight clustering of many (self-rated) experts’ responses to OP in particular is notable.\nGame outcomes and the feasibility of perfect games (i.e. score 25) are dependent on the starting\ndeck. We did not control for deck conﬁguration to avoid the issue encountered in Hu et al. [21], who\nexplored a relatively narrow portion of the game space due to using only two deck seeds.\nInteresting extensions to this Hanabi experiment include varying participants’ knowledge of their\nteammates, increasing the number of games played, and adding additional metrics of teaming, such\nas positive listening and positive signalling [22, 29]. Modiﬁcations to the agents may include adding\nlogic ﬁlters to prevent learning-based agents from making “obvious” mistakes, or modifying the\nreward function to more heavily penalize “trust-breaking” moves. Beyond the domain of Hanabi,\nother candidates for human-AI teaming experiments include StarCraft (which to date has only shown\nadversarial RL agent performance with humans [ 44]), Overcooked [ 9], and mixed cooperative-\ncompetitive environments like Bridge [49] and Diplomacy [16].\n6 Conclusion\nThis study measured the game performance and human reactions in mixed human-AI teams in the\ncooperative card game Hanabi, comparing outcomes when humans were paired with a rule-based\nagent and when paired with a reinforcement-learning-based “Other-Play” agent designed to maximize\nzero-shot coordination. Despite achieving similar scores between these teams, human players strongly\npreferred working with the rule-based agent, and view the Other-Play agent quite negatively, citing\ntheir bilateral understanding, trust, comfort, and perceived performance as reasons. The ability of\nAI agents to team with humans is an important determinant of whether they can be deployed in\nmany real-world situations. These results show that even state-of-the-art RL agents largely fail to\nconvince humans that they are good teammates, and suggest that human perception of AI needs\ngreater consideration in future AI design and development if it is to achieve real-world adoption.\n10\nAcknowledgments and Disclosure of Funding\nDISTRIBUTION STATEMENT A. Approved for public release. Distribution is unlimited. This ma-\nterial is based upon work supported by the Under Secretary of Defense for Research and Engineering\nunder Air Force Contract No. FA8702-15-D-0001. Any opinions, ﬁndings, conclusions or recommen-\ndations expressed in this material are those of the author(s) and do not necessarily reﬂect the views of\nthe Under Secretary of Defense for Research and Engineering. © 2021 Massachusetts Institute of\nTechnology. Delivered to the U.S. Government with Unlimited Rights, as deﬁned in DFARS Part\n252.227-7013 or 7014 (Feb 2014). Notwithstanding any copyright notice, U.S. Government rights in\nthis work are deﬁned by DFARS 252.227-7013 or DFARS 252.227-7014 as detailed above. Use of\nthis work other than as speciﬁcally authorized by the U.S. Government may violate any copyrights\nthat exist in this work.\nThe authors would like to thank our experiment participants for their time. We thank Hengyuan Hu\nfor providing the Other-Play model. We thank Peter Morales for his guidance during the early phase\nof this work.\nReferences\n[1] Trapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, and Igor Mordatch. Emergent\ncomplexity via multi-agent competition. arXiv preprint arXiv:1710.03748, 2017.\n[2] Nolan Bard, Jakob N Foerster, Sarath Chandar, Neil Burch, Marc Lanctot, H Francis Song,\nEmilio Parisotto, Vincent Dumoulin, Subhodeep Moitra, Edward Hughes, et al. The hanabi\nchallenge: A new frontier for ai research. Artiﬁcial Intelligence, 280:103216, 2020.\n[3] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning\nenvironment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence\nResearch, 47:253–279, 2013.\n[4] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław D˛ ebiak, Christy\nDennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large\nscale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\n[5] Bruno Bouzy. Playing hanabi near-optimally. In Advances in Computer Games, pages 51–62.\nSpringer, 2017.\n[6] Murray Campbell, A Joseph Hoane Jr, and Feng-hsiung Hsu. Deep blue. Artiﬁcial intelligence,\n134(1-2):57–83, 2002.\n[7] Rodrigo Canaan, Haotian Shen, Ruben Torrado, Julian Togelius, Andy Nealen, and Stefan\nMenzel. Evolving agents for the hanabi 2018 cig competition. In 2018 IEEE Conference on\nComputational Intelligence and Games (CIG), pages 1–8. IEEE, 2018.\n[8] Rodrigo Canaan, Xianbo Gao, Julian Togelius, Andy Nealen, and Stefan Menzel. Generating\nand adapting to diverse ad-hoc cooperation agents in hanabi. arXiv preprint arXiv:2004.13710,\n2020.\n[9] Micah Carroll, Rohin Shah, Mark K Ho, Thomas L Grifﬁths, Sanjit A Seshia, Pieter Abbeel,\nand Anca Dragan. On the utility of learning about humans for human-ai coordination. arXiv\npreprint arXiv:1910.05789, 2019.\n[10] Jacob Cohen. Statistical power analysis for the behavioral sciences . Academic Press, New\nYork, 1977. ISBN 0-12-179060-6.\n[11] DARPA. AlphaDogﬁght Trials Foreshadow Future of Human-Machine Symbiosis. https:\n//www.darpa.mil/news-events/2020-08-26 , 2020.\n[12] Anca D Dragan, Kenton CT Lee, and Siddhartha S Srinivasa. Legibility and predictability of\nrobot motion. In 2013 8th ACM/IEEE International Conference on Human-Robot Interaction\n(HRI), pages 301–308. IEEE, 2013.\n11\n[13] Markus Eger, Chris Martens, and Marcela Alfaro Córdoba. An intentional ai for hanabi. In\n2017 IEEE Conference on Computational Intelligence and Games (CIG), pages 68–75. IEEE,\n2017.\n[14] Jakob Foerster, Francis Song, Edward Hughes, Neil Burch, Iain Dunning, Shimon Whiteson,\nMatthew Botvinick, and Michael Bowling. Bayesian action decoder for deep multi-agent\nreinforcement learning. In International Conference on Machine Learning, pages 1942–1951.\nPMLR, 2019.\n[15] James Goodman. Re-determinizing information set monte carlo tree search in hanabi. arXiv\npreprint arXiv:1902.06075, 2019.\n[16] Jonathan Gray, Adam Lerer, Anton Bakhtin, and Noam Brown. Human-level performance in\nno-press diplomacy via equilibrium search. arXiv preprint arXiv:2010.02923, 2020.\n[17] Herbert P Grice. Logic and conversation. In Speech acts, pages 41–58. Brill, 1975.\n[18] Sandra G Hart and Lowell E Staveland. Development of nasa-tlx (task load index): Results\nof empirical and theoretical research. In Advances in psychology, volume 52, pages 139–183.\nElsevier, 1988.\n[19] Guy Hoffman. Evaluating ﬂuency in human–robot collaboration. IEEE Transactions on\nHuman-Machine Systems, 49(3):209–218, 2019.\n[20] Hengyuan Hu and Jakob N Foerster. Simpliﬁed action decoder for deep multi-agent reinforce-\nment learning. arXiv preprint arXiv:1912.02288, 2019.\n[21] Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster. “other-play” for zero-shot\ncoordination. In International Conference on Machine Learning, pages 4399–4410. PMLR,\n2020.\n[22] Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega,\nDJ Strouse, Joel Z Leibo, and Nando De Freitas. Social inﬂuence as intrinsic motivation\nfor multi-agent deep reinforcement learning. In International Conference on Machine Learning,\npages 3040–3049. PMLR, 2019.\n[23] Matthew Johnson and Alonso Vera. No ai is an island: the case for teaming intelligence. AI\nMagazine, 40(1):16–28, 2019.\n[24] Zoe Juozapaitis, Anurag Koul, Alan Fern, M. Erwig, and Finale Doshi-Velez. Explainable\nreinforcement learning via reward decomposition. 2019.\n[25] Gary A Klein. A recognition-primed decision (rpd) model of rapid decision making. Decision\nmaking in action: Models and methods, 5(4):138–147, 1993.\n[26] John D Lee and Katrina A See. Trust in automation: Designing for appropriate reliance. Human\nfactors, 46(1):50–80, 2004.\n[27] Adam Lerer, Hengyuan Hu, Jakob Foerster, and Noam Brown. Improving policies via search in\ncooperative partially observable games. In Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume 34, pages 7187–7194, 2020.\n[28] Claire Liang, Julia Proft, Erik Andersen, and Ross A Knepper. Implicit communication of\nactionable information in human-ai teams. In Proceedings of the 2019 CHI Conference on\nHuman Factors in Computing Systems, pages 1–13, 2019.\n[29] Ryan Lowe, Jakob Foerster, Y-Lan Boureau, Joelle Pineau, and Yann Dauphin. On the pitfalls\nof measuring emergent communication. arXiv preprint arXiv:1903.05168, 2019.\n[30] Prashan Madumal, T. Miller, L. Sonenberg, and F. Vetere. Explainable reinforcement learning\nthrough a causal lens. In AAAI, 2020.\n[31] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602, 2013.\n12\n[32] Geoff Norman. Likert scales, levels of measurement and the “laws” of statistics. Advances in\nhealth sciences education, 15(5):625–632, 2010.\n[33] Arthur O’Dwyer. quuxplusone/hanabi: framework for writing bots that play hanabi.\nhttps://github.com/Quuxplusone/Hanabi/, 2019.\n[34] OpenAI. Openai ﬁve defeats dota 2 world champions, Apr 2019. URL https://openai.com/\nblog/openai-five-defeats-dota-2-world-champions/ .\n[35] OpenAI. Openai ﬁve, Dec 2019. URL https://openai.com/five/.\n[36] Hirotaka Osawa. Solving hanabi: Estimating hands by opponent’s actions in cooperative game\nwith incomplete information. In Workshops at the Twenty-Ninth AAAI Conference on Artiﬁcial\nIntelligence, 2015.\n[37] Raja Parasuraman and Victor Riley. Humans and automation: Use, misuse, disuse, abuse.\nHuman factors, 39(2):230–253, 1997.\n[38] Aron Sarmasi, Timothy Zhang, Chu-Hung Cheng, Huyen Pham, Xuanchen Zhou, Duong\nNguyen, Soumil Shekdar, and Joshua McCoy. Hoad: The hanabi open agent dataset. In\nProceedings of the 20th International Conference on Autonomous Agents and MultiAgent\nSystems, pages 1646–1648, 2021.\n[39] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur\nGuez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general\nreinforcement learning algorithm that masters chess, shogi, and go through self-play. Science,\n362(6419):1140–1144, 2018.\n[40] P. Stone, G. Kaminka, S. Kraus, and J. Rosenschein. Ad hoc autonomous agent teams: Collabo-\nration without pre-coordination. In AAAI, 2010.\n[41] Pradyumna Tambwekar, Andrew Silva, Nakul Gopalan, and Matthew Gombolay. Inter-\npretable policy speciﬁcation and synthesis through natural language and rl. arXiv preprint\narXiv:2101.07140, 2021.\n[42] Mark JH van den Bergh, Anne Hommelberg, Walter A Kosters, and Flora M Spieksma. Aspects\nof the cooperative card game hanabi. In Benelux Conference on Artiﬁcial Intelligence, pages\n93–105. Springer, 2016.\n[43] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,\nMichelle Yeo, Alireza Makhzani, Heinrich Küttler, John Agapiou, Julian Schrittwieser, et al.\nStarcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782,\n2017.\n[44] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Jun-\nyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster\nlevel in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\n[45] Joseph Walton-Rivers, Piers R Williams, Richard Bartle, Diego Perez-Liebana, and Simon M Lu-\ncas. Evaluating and modelling hanabi-playing agents. In 2017 IEEE Congress on Evolutionary\nComputation (CEC), pages 1382–1389. IEEE, 2017.\n[46] Joseph Walton-Rivers, Piers R Williams, and Richard Bartle. The 2018 hanabi competition. In\n2019 IEEE Conference on Games (CoG), pages 1–8. IEEE, 2019.\n[47] D. Wu. lightvector/ﬁreﬂower: a rewrite of hanabi-bot in scala.\nhttps://github.com/lightvector/ﬁreﬂower, 2018.\n[48] J. Wu. wuthefwasthat/hanabi.rs: Hanabi simulation in rust.\nhttps://github.com/WuTheFWasThat/hanabi.rs, 2018.\n[49] Chih-Kuan Yeh, Cheng-Yu Hsieh, and Hsuan-Tien Lin. Automatic bridge bidding using deep\nreinforcement learning. IEEE Transactions on Games, 10:365–377, 2018.\n13\nChecklist\n1. For all authors...\n(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s\ncontributions and scope? [Yes]\n(b) Did you describe the limitations of your work? [Yes] See Section 5.3\n(c) Did you discuss any potential negative societal impacts of your work? [Yes] See\nSection 5.2\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to\nthem? [Yes]\n2. If you are including theoretical results...\n(a) Did you state the full set of assumptions of all theoretical results? [N/A]\n(b) Did you include complete proofs of all theoretical results? [N/A]\n3. If you ran experiments...\n(a) Did you include the code, data, and instructions needed to reproduce the main experi-\nmental results (either in the supplemental material or as a URL)? [Yes] See supplemen-\ntal materials.\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they\nwere chosen)? [N/A] We did not train new models.\n(c) Did you report error bars (e.g., with respect to the random seed after running experi-\nments multiple times)? [Yes] We reported dispersion measures and test statistics where\nappropriate. All histograms represent exact counts of the data.\n(d) Did you include the total amount of compute and the type of resources used (e.g., type\nof GPUs, internal cluster, or cloud provider)? [No] Compute was not signiﬁcant since\nwe were not training new models.\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n(a) If your work uses existing assets, did you cite the creators? [Yes] See Section 3.1\n(b) Did you mention the license of the assets? [Yes] See Section 3.1\n(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]\n(d) Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? [Yes] See Section 3.1\n(e) Did you discuss whether the data you are using/curating contains personally identiﬁable\ninformation or offensive content? [Yes] See Section 3.1. No PII was collected, and no\noffensive content was used/shown.\n5. If you used crowdsourcing or conducted research with human subjects...\n(a) Did you include the full text of instructions given to participants and screenshots, if\napplicable? [Yes] See supplemental materials.\n(b) Did you describe any potential participant risks, with links to Institutional Review\nBoard (IRB) approvals, if applicable? [Yes] See Section 3.1. No signiﬁcant participant\nrisks were expected, no adverse events occurred during the course of the experiment.\nIRB approval link and institution name removed for anonymous review stage; will be\nadded back for for later stage.\n(c) Did you include the estimated hourly wage paid to participants and the total amount\nspent on participant compensation? [Yes] See Section 3.1\n14\nSupplemental Materials\nFor data transparency and completeness, we detail our participant recruitment process, participant\ninstructions, relevant aggregate data (demographics, scores, surveys), as well as the results from\nstatistical tests that we conducted, but were not part of the main paper due to space constraints. We\ndo not include the results of the NASA Task Load Index survey here because those were not analyzed\nfor this study.\n6.1 Participant Recruitment\nParticipants for this experiment were recruited via convenience and snowball sampling, with initial\nemails to MIT research groups and social mailing lists, as well as some for other Cambridge-areas\ngroups. We note that 6 out of 29 participants belonged to the hanab.live Hanabi gaming community.\nOther than those, participants were novices to Hanabi, did not play consistently, or came from several\ndistinct and unrelated Hanabi groups.\n6.2 Introductory Slides and Game Interface\nThese are the slides shown to experiment participants at the very beginning of the session. All\nsubjects were shown the same slides.\n15\n16\n17\n6.3 Demographic Survey\nQuestions 1-4 are two pairs of multiple choice and free response questions. Questions 5-10 are Likert\nscale statements with a scale from 1 (strongly disagree) to 7 (strongly agree). Questions 9 and 10\nalso include a free response (\"Explain\") if the participant indicates agreement with the statement.\nTable 4: Demographic Survey Prompt and Response Choices\nPrompt Response Choices\nD1 How often do you play card or board games? [Never, <1 hour/week, 1-3 hours/week, >3 hours/week]\nD2 Which games or types of games do you play? free response\nD3 How often do you play video games? [Never, <1 hour/week, 1-3 hours/week, >3 hours/week]\nD4 Which game or types of video games do you play? free response\nD5 I am experienced in cooperative card games. Likert Scale\nD6 I am experienced in cooperative board games. Likert Scale\nD7 I am experienced in cooperative video games. Likert Scale\nD8 I am experienced in Hanabi. Likert Scale\nD9 I am experienced in interacting with artiﬁcial Likert Scale, free response (optional)\nintelligence agents (including voice\nassistants, game AIs, etc).\nD10 I am experienced in developing artiﬁcial Likert Scale, free response (optional)\nintelligence agents.\n18\n6.4 Demographic Survey Responses\nFigure 5: Histograms of all numerical and categorical demographic survey responses.\n19\n6.5 Post-Game Likert Scale Question Responses\nFigure 6: Participant rating for all post-game questions by self-rated Hanabi experience where\nstatistically signiﬁcant differences related to factors of agent and/or experience were presented in\nSection 4.2 . The scale ranges from 1-7, corresponding to \"strongly disagree\" to \"strongly agree\".\n20\n6.6 Participant Scores\nFigure 7: Cumulative game score for each participant across their six games, split by their three\ngames with each agent type. The maximum achievable cumulative score per agent type is 75, and\n150 for both. Participant 2 achieved the highest cumulative score of 122.\n6.7 Post-Game Survey Statistics\nObjective and subjective data were ﬁt to second-order mixed-effects models with ﬁxed factors of (1)\nAI agent, (2) self-rated Hanabi experience, (3) block (the ﬁrst or second set of three games), and (4)\ngame number (ﬁrst, second, or third game within a block), and a random factor of participant number.\nAI agent and participant number were considered categorical variables.\nAgent refers to the AI agent type (OP or SB).\nExperience refers to the self-reported Hanabi experience level (question D8).\nLower and Upper are the values for the 95% conﬁdence intervals.\nTable 5: Model ﬁt to response variable of game score\nName Estimate SE t df p Lower Upper\nAgent 1.6084 4.2962 0.37438 158 0.70863 -6.877 10.094\nBlock 5.282 2.0039 2.6358 158 0.0092295 1.3241 9.2398\nGame 2.9572 1.567 1.8872 158 0.060961 -0.13765 6.0521\nExperience 0.7736 0.62989 1.2281 158 0.22122 -0.4705 2.0177\nAgent : Block -3.6394 1.9493 -1.867 158 0.063748 -7.4893 0.21061\nAgent : Game 1.1848 1.1962 0.99047 158 0.32346 -1.1778 3.5473\nBlock : Game -0.90997 0.9963 -0.91335 158 0.36245 -2.8778 1.0578\nAgent : Experience -0.20234 0.41707 -0.48515 158 0.62824 -1.0261 0.62141\nBlock : Experience 0.15829 0.36415 0.43468 158 0.66439 -0.56094 0.87751\nGame : Experience -0.13373 0.23366 -0.57233 158 0.56791 -0.59524 0.32777\n21\nTable 6: Model ﬁt to response variable of “I am playing well” (G1)\nName Estimate SE t df p Lower Upper\nAgent 1.9694 1.0546 1.8674 164 0.063637 -0.11304 4.0518\nBlock 1.9168 0.39977 4.7948 164 3.6319e-06 1.1274 2.7062\nGame 1.2204 0.30518 3.9989 164 9.6048e-05 0.6178 1.823\nExperience 0.32729 0.12129 2.6985 164 0.0076942 0.087808 0.56678\nAgent : Block -0.80197 0.62076 -1.2919 164 0.19821 -2.0277 0.42375\nAgent : Game -0.37579 0.20773 -1.809 164 0.072285 -0.78597 0.034391\nBlock : Game -0.39684 0.18093 -2.1934 164 0.029688 -0.75409 -0.039593\nAgent : Experience -0.097033 0.073054 -1.3282 164 0.18595 -0.24128 0.047215\nBlock : Experience -0.12775 0.065443 -1.952 164 0.052636 -0.25696 0.0014721\nGame : Experience -0.033548 0.041611 -0.80624 164 0.42127 -0.11571 0.048614\nTable 7: Model ﬁt to response variable of “The agent is playing poorly” (G2)\nName Estimate SE t df p Lower Upper\nAgent -0.33608 1.0745 -0.31277 164 0.75485 -2.4577 1.7856\nBlock 0.83106 0.50514 1.6452 164 0.10184 -0.16635 1.8285\nGame 1.1238 0.3918 2.8684 164 0.0046687 0.35021 1.8975\nExperience 0.62968 0.15572 4.0436 164 8.0826e-05 0.3222 0.93717\nAgent : Block 0.45465 0.48308 0.94115 164 0.34801 -0.49921 1.4085\nAgent : Game -0.11362 0.29781 -0.38151 164 0.70332 -0.70165 0.47441\nBlock : Game -0.29889 0.24918 -1.1995 164 0.23207 -0.7909 0.19313\nAgent : Experience 0.16086 0.10481 1.5348 164 0.12677 -0.046092 0.36782\nBlock : Experience -0.16477 0.091074 -1.8092 164 0.072251 -0.3446 0.015057\nGame : Experience -0.10097 0.058624 -1.7223 164 0.086904 -0.21672 0.014788\nTable 8: Model ﬁt to response variable of “The team is playing well” (G3)\nName Estimate SE t df p Lower Upper\nAgent 1.3927 0.98213 1.418 164 0.15807 -0.54654 3.3319\nBlock 2.4468 0.46171 5.2995 164 3.6985e-07 1.5352 3.3585\nGame 1.065 0.35812 2.9739 164 0.0033848 0.35787 1.7721\nExperience -0.032564 0.14234 -0.22878 164 0.81933 -0.31361 0.24848\nAgent : Block -0.58469 0.44155 -1.3242 164 0.18729 -1.4565 0.28716\nAgent : Game -0.22945 0.2722 -0.84293 164 0.4005 -0.76692 0.30803\nBlock : Game -0.56663 0.22776 -2.4879 164 0.01385 -1.0163 -0.11692\nAgent : Experience -0.19952 0.095802 -2.0826 164 0.03884 -0.38868 -0.010353\nBlock : Experience -0.076342 0.083244 -0.91709 164 0.36044 -0.24071 0.088027\nGame : Experience 0.051342 0.053584 0.95816 164 0.33939 -0.054461 0.15715\nTable 9: Model ﬁt to response variable of “The game went well” (G4)\nName Estimate SE t df p Lower Upper\nAgent 0.76539 1.1659 0.65651 164 0.51242 -1.5366 3.0674\nBlock 2.0535 0.54808 3.7468 164 0.00024779 0.97132 3.1357\nGame 1.8013 0.42511 4.2372 164 3.7659e-05 0.96187 2.6407\nExperience -0.16132 0.16896 -0.95475 164 0.34111 -0.49494 0.17231\nAgent : Block -0.34362 0.52415 -0.65558 164 0.51301 -1.3786 0.69133\nAgent : Game -0.076741 0.32312 -0.2375 164 0.81257 -0.71476 0.56128\nBlock : Game -0.90768 0.27036 -3.3573 164 0.00097855 -1.4415 -0.37384\nAgent : Experience -0.17102 0.11372 -1.5038 164 0.13455 -0.39557 0.053529\nBlock : Experience 0.086306 0.098817 0.8734 164 0.38372 -0.10881 0.28142\nGame : Experience -0.00048774 0.063608 -0.0076679 164 0.99389 -0.12608 0.12511\n22\nTable 10: Model ﬁt to response variable of “The agent and I have good teamwork” (G5)\nName Estimate SE t df p Lower Upper\nAgent 2.3867 0.92567 2.5783 164 0.010807 0.55891 4.2145\nBlock 2.5072 0.43517 5.7614 164 4.0236e-08 1.6479 3.3664\nGame 1.2232 0.33753 3.6238 164 0.00038684 0.55669 1.8896\nExperience -0.026039 0.13415 -0.1941 164 0.84634 -0.29093 0.23885\nAgent : Block -0.84574 0.41617 -2.0322 164 0.043746 -1.6675 -0.024003\nAgent : Game -0.36521 0.25656 -1.4235 164 0.15649 -0.87179 0.14137\nBlock : Game -0.59262 0.21467 -2.7607 164 0.0064254 -1.0165 -0.16876\nAgent : Experience -0.27275 0.090295 -3.0207 164 0.0029261 -0.45105 -0.094464\nBlock : Experience -0.080736 0.078459 -1.029 164 0.30499 -0.23566 0.074185\nGame : Experience 0.024633 0.050504 0.48775 164 0.62638 -0.075089 0.12435\nTable 11: Model ﬁt to response variable of “The agent is contributing to the success of the team” (G6)\nName Estimate SE t df p Lower Upper\nAgent 1.782 1.191 1.4963 164 0.13651 -0.56962 4.1336\nBlock 2.441 0.48735 5.0086 164 1.4056e-06 1.4787 3.4032\nGame 1.4781 0.37685 3.9222 164 0.00012878 0.73397 2.2222\nExperience 0.1814 0.14897 1.2177 164 0.22509 -0.11275 0.47556\nAgent : Block -0.74292 0.65379 -1.1363 164 0.25747 -2.0338 0.548\nAgent : Game -0.23663 0.2688 -0.88034 164 0.37997 -0.76739 0.29412\nBlock : Game -0.62886 0.23001 -2.7341 164 0.0069428 -1.083 -0.1747\nAgent : Experience -0.17078 0.094562 -1.806 164 0.072747 -0.3575 0.015934\nBlock : Experience -0.070518 0.083572 -0.8438 164 0.40001 -0.23553 0.094498\nGame : Experience -0.065545 0.053426 -1.2268 164 0.22164 -0.17104 0.039946\nTable 12: Model ﬁt to response variable of “I understand the agent’s intentions” (G7)\nName Estimate SE t df p Lower Upper\nAgent 1.4938 0.86912 1.7188 164 0.087537 -0.22226 3.2099\nBlock 2.6995 0.40858 6.6069 164 5.2348e-10 1.8927 3.5062\nGame 1.6848 0.31691 5.3164 164 3.4175e-07 1.0591 2.3106\nExperience -0.011884 0.12596 -0.09435 164 0.92495 -0.26059 0.23682\nAgent : Block -0.55231 0.39074 -1.4135 164 0.1594 -1.3238 0.21922\nAgent : Game -0.42332 0.24088 -1.7574 164 0.08072 -0.89895 0.05231\nBlock : Game -0.78483 0.20155 -3.8939 164 0.00014328 -1.1828 -0.38686\nAgent : Experience -0.19273 0.084778 -2.2734 164 0.024301 -0.36013 -0.025335\nBlock : Experience -0.10565 0.073666 -1.4341 164 0.15343 -0.2511 0.039808\nGame : Experience 0.019661 0.047418 0.41464 164 0.67895 -0.073968 0.11329\nTable 13: Model ﬁt to response variable of “The agent does not understand my intentions” (G8)\nName Estimate SE t df p Lower Upper\nAgent 1.1503 1.2751 0.90212 164 0.36831 -1.3675 3.6682\nBlock 1.3412 0.51911 2.5838 164 0.010645 0.31625 2.3662\nGame 1.3544 0.40121 3.3757 164 0.00091923 0.56216 2.1466\nExperience 0.3328 0.15861 2.0982 164 0.037422 0.019612 0.64598\nAgent : Block -0.84719 0.70374 -1.2039 164 0.23038 -2.2367 0.54236\nAgent : Game -0.23748 0.28536 -0.83219 164 0.40651 -0.80093 0.32598\nBlock : Game -0.43934 0.24444 -1.7974 164 0.074115 -0.92199 0.043303\nAgent : Experience 0.3178 0.10039 3.1657 164 0.0018446 0.11958 0.51602\nBlock : Experience -0.048055 0.088791 -0.54122 164 0.58909 -0.22338 0.12727\nGame : Experience -0.10706 0.056744 -1.8867 164 0.060972 -0.2191 0.0049858\n23\nTable 14: Model ﬁt to response variable of “I feel comfortable playing with this agent” (G9)\nName Estimate SE t df p Lower Upper\nAgent 3.143 0.95194 3.3017 164 0.0011796 1.2633 5.0226\nBlock 2.5133 0.44752 5.616 164 8.1919e-08 1.6296 3.3969\nGame 1.0899 0.34711 3.1398 164 0.0020055 0.40448 1.7752\nExperience -0.023996 0.13796 -0.17393 164 0.86213 -0.29641 0.24841\nAgent : Block -1.0492 0.42798 -2.4516 164 0.015273 -1.8943 -0.20415\nAgent : Game -0.34819 0.26384 -1.3197 164 0.18877 -0.86914 0.17277\nBlock : Game -0.49785 0.22076 -2.2552 164 0.025444 -0.93375 -0.061956\nAgent : Experience -0.33071 0.092857 -3.5614 164 0.00048298 -0.51406 -0.14736\nBlock : Experience -0.081598 0.080686 -1.0113 164 0.31336 -0.24092 0.077719\nGame : Experience 0.022646 0.051937 0.43602 164 0.66339 -0.079906 0.1252\nTable 15: Model ﬁt to response variable of “I do not trust the agent” (G10)\nName Estimate SE t df p Lower Upper\nAgent -0.80019 1.0903 -0.73389 164 0.46406 -2.9531 1.3527\nBlock 1.0878 0.51258 2.1221 164 0.03533 0.075645 2.0999\nGame 1.1899 0.39758 2.9929 164 0.0031911 0.40487 1.9749\nExperience 0.6055 0.15802 3.8318 164 0.00018089 0.29349 0.91752\nAgent : Block 0.83804 0.4902 1.7096 164 0.089233 -0.12987 1.806\nAgent : Game -0.20681 0.3022 -0.68436 164 0.49472 -0.80351 0.38989\nBlock : Game -0.37637 0.25285 -1.4885 164 0.13854 -0.87564 0.1229\nAgent : Experience 0.203 0.10636 1.9086 164 0.058055 -0.0070082 0.413\nBlock : Experience -0.17691 0.092417 -1.9143 164 0.057328 -0.35939 0.0055706\nGame : Experience -0.076328 0.059488 -1.2831 164 0.20127 -0.19379 0.041133\nTable 16: Model ﬁt to response variable of “The agent is not a reliable teammate” (G11)\nName Estimate SE t df p Lower Upper\nAgent 0.34397 1.1345 0.3032 164 0.76212 -1.8961 2.584\nBlock 1.0037 0.53332 1.8821 164 0.061598 -0.049315 2.0568\nGame 1.0982 0.41366 2.6549 164 0.0087148 0.28145 1.915\nExperience 0.51945 0.16441 3.1594 164 0.0018826 0.19481 0.84409\nAgent : Block 0.12847 0.51003 0.25188 164 0.80145 -0.87861 1.1355\nAgent : Game -0.1499 0.31442 -0.47675 164 0.63417 -0.77074 0.47094\nBlock : Game -0.30352 0.26308 -1.1537 164 0.2503 -0.82299 0.21595\nAgent : Experience 0.12813 0.11066 1.1579 164 0.24859 -0.090369 0.34664\nBlock : Experience -0.088578 0.096156 -0.9212 164 0.3583 -0.27844 0.10128\nGame : Experience -0.097645 0.061895 -1.5776 164 0.11659 -0.21986 0.024569\nTable 17: Model ﬁt to response variable of “I am not conﬁdent in my gameplay” (G12)\nName Estimate SE t df p Lower Upper\nAgent 1.4796 1.2583 1.1758 164 0.24137 -1.005 3.9641\nBlock 1.5481 0.4575 3.3839 164 0.00089398 0.64478 2.4515\nGame 0.61849 0.34399 1.798 164 0.074017 -0.060728 1.2977\nExperience 0.26304 0.13801 1.906 164 0.058405 -0.0094638 0.53554\nAgent : Block -1.1132 0.76425 -1.4565 164 0.14716 -2.6222 0.39588\nAgent : Game 0.037263 0.22698 0.16417 164 0.8698 -0.41092 0.48544\nBlock : Game -0.29982 0.20014 -1.498 164 0.13605 -0.69501 0.095368\nAgent : Experience 0.031346 0.079802 0.39279 164 0.69498 -0.12623 0.18892\nBlock : Experience -0.044646 0.07217 -0.61861 164 0.53703 -0.18715 0.097857\nGame : Experience -0.048831 0.045717 -1.0681 164 0.28704 -0.1391 0.041439\n24\n6.8 Novice vs Expert Post-Gamet-Tests\nPost-hoc pairwise comparisons of novice vs expert in cases where agent and self-reported Hanabi\nexperience have signiﬁcant interaction effects, as described in Section 4.2.\nTable 18: Two-sample t-tests of post-game sentiment, comparing novice and expert reactions.\nQuestion Agent t p corrected p d\nG5 SB 0.35599 0.72273 1.00000 0.080708\nG5 OP 5.1395 1.7334e-06 1.38672e-05 1.0185\nG9 SB 0.25536 0.79906 1.00000 0.057915\nG9 OP 5.8552 8.7246e-08 7.85214e-07 1.1214\nG8 SB 0.61126 0.54266 1.00000 0.13838\nG8 OP -5.9229 6.5231e-08 6.52310e-07 -1.1306\nG3 SB -1.1856 0.2391 0.956400 -0.26679\nG3 OP 3.5514 0.00062779 3.76674e-03 0.75189\nG7 SB 1.652 0.10223 0.511150 0.36893\nG7 OP 5.0678 2.3171e-06 1.62197e-05 1.0076\n6.9 Post-Experiment t-Tests\nOne-sample t-tests of post-experiment sentiment. Some responses were ﬂipped on the Likert scale\nfor directional consistency, based on which agent was seen ﬁrst, since the ends of the scale were\nlabeled as the “ﬁrst” and “second” agent for the participants. Preference directionality is such that 1\nis towards OP and 7 is towards SB. t statistics greater than 0 indicate answering towards SB. The\nHolm–Bonferroni step-down method was used for multiple comparisons correction. d is the Cohen’s\neffect size. In general, thresholds for “small,” “medium,” and “large” effect sizes are considered to be\n|d|= 0.2, 0.5, and 0.8 respectively [10].\nTable 19: One-sample t-tests of post-experiment sentiment.\nQuestion t p corrected p d\nWhich agent did you prefer playing with? 2.90633 0.00707 0.03969 0.549\nWhich agent did you trust more? 3.40564 0.00201 0.01610 0.644\nWhich agent did you understand more? 2.88618 0.00743 0.03969 0.545\nWhich agent understood you better? 2.93369 0.00661 0.03969 0.554\nWhich agent was the better Hanabi player? 3.36011 0.00226 0.01610 0.635\nWhich agent was more reliable? 2.86217 0.00788 0.03969 0.541\nWhich agent had a better understanding of\nthe game on average? 2.68186 0.01214 0.03969 0.507\nWhich agent caused you to have a greater\nmental workload? -0.16385 0.87103 0.87103 -0.031\n6.10 Post-Experiment Participant Preference and Free Response\nPost-experiment ratings of agent preference and explanation of the preference. The “Preference”\nheading corresponds to a Likert-scale response to the question “Which agent did you prefer playing\nwith?” where 1 was “the ﬁrst agent” and 7 was “the second agent.” “Explanation” was a free-response\nﬁeld with the question “Why did you prefer the agent that you did?”\nTable 20: Post-experiment ratings of agent preference and explanation\nParticipant Order Preference Explanation\n1 SB, OP 5 ﬁrst agent felt like it was learning; really bad to begin with;\nhad to \"teach\" them how to play hanabi; second agent felt\nlike someone who knew how to play hanabi and wanted to\ntrick you; broke my trust in 2nd game; in 3rd game was \"trust\nme\"; don’t like playing with 2nd agent.\n25\n2 OP, SB 6 It seemed to have a better understanding of not just what\nhints to give, but when to give them. I think a lot of the\nstrategies cascaded from that - both my strategy and its. It\njust had a better understanding of the tempo of the game. If\nyou think of it as - every time I get a hint, I have to perform\nan MLE, and when I give a hint, that’s what they have to\nperform - if you just take the clue at face value; you can think\nabout \"why am I giving this hint\" the second agent thought\nabout \"why am I getting/giving this hint NOW\" while the\nﬁrst agent didn’t.\n3 SB, OP 1 It knew the rules of the game. It knew how to play.\n4 OP, SB 4 I thought the ﬁrst one was dumber but more consistent. The\nsecond one - I thought I was starting to understand it in the\nsecond game, but then in the third game, I completely didn’t\nunderstand what it was doing at all.\n5 SB, OP 1 Second agent made an obvious mistake quite frequently.\nThere are some cases where it was clear that if the agent\nplayed a card, we would lose the game, but it played it any-\nway. Sometimes it would also give me hints that I already\nknow.\n6 OP, SB 7 better able to understand what it’s clues meant and how to\ngive it clues that would result in the correct actions\n7 SB, OP 3 The ﬁrst agent was more predictable, even if I didn’t neces-\nsarily agree with their strategy. Both of them made dumb\nchoices, like playing cards that were clearly not playable\nwhen they had full information on them (or at least enough\ninformation), or they discarded cards with full information\nand were playable.\n8 OP, SB 7 The second agent seemed to be more capable of inductive\nreasoning than the ﬁrst. Both has similar styles of inductive\nclues, but it seemed like the second took inductive clues\nbetter. The discard strategy of the ﬁrst agent felt worse than\nthe discard strategy of the second.\n9 SB, OP 1 agent 1 was more consistent; even if i didn’t understand what\nthey were doing, i could more reliably assume they would\nplay or discard cards if they knew they were playable; I feel\nI bombed the second one whenever i clued it; did not know\nhow it would react\n10 OP, SB 6 Maybe it’s because the ﬁrst one was so terrible that it made\nme have zero expectation of the second one. So even though\nthe second agent wasn’t that much better, and I was confused\nby its strategy, I was used to being confused and wasn’t\nsurprised anymore. It took less willpower to go through the\ngames [with the second agent].\n11 SB, OP 7 Gave me info; seemed to act on cues better; it felt like there\nwas 2-way comms as opposed to 1-way; also it didn’t throw\naway cards (e.g., knew perfect info on)\n12 OP, SB 7 It provided more challenge and interest. Because I could\nreasonably play with it. It let me play at a more satisfying\nlevel.\n26\n13 SB, OP 1 Agent 1 seemed to have a better model of the game in the\nsense that it deliberately played playable cards and discarded\nunplayable cards more frequently; as opposed to the 2nd\nagent that played known unplayable cards and did not play\ncards when it had the chance to; ﬁrst agent was more inline\nwith game of Hanabi rules of playing all cards when possi-\nble; ﬁrst agent played a way that was more familiar; First\nagent still used strategies that were more human friendly; i\nunderstood it better and it understood me better\n14 OP, SB 3 I felt like the ﬁrst agent was improving and started under-\nstanding my strategy more, whereas the second one wasn’t\nlearning from the errors or mistakes that both of us made.\n15 SB, OP 6 The rules that the second (2nd) agent was following was\neasier to understand; speciﬁcally the discarding strategy was\nmuch more predictable; ﬁrst agent may have predictable\ndiscarding strategy, but the 2nd agent is much easier to play\nwith\n16 OP, SB 7 I probably had some learning effects for the game so I under-\nstood things better, however, I also found that it was easier to\nget into a cadence of play with the second agent. I think I un-\nderstood the intention of the second agent and it understood\nme.\n17 SB, OP 1 The ﬁrst agent played cards and hinted cards consistently.\nThe second agent by contrast did not play multiply-hinted\ncards and gave hints that were not necessarily playable. With\nthe ﬁrst agent, I could reasonably expect to perform well and\nto trust his decision whereas with the second agent, I found\nmyself trying to compensate for his lack of reliability.\n18 OP, SB 2 i was better at predicting what the ﬁrst agent would do; after\nﬁrst 2 games i understood the agent’s strategy though i didn’t\nagree with it; with 2nd agent i couldn’t ﬁgure out how it’s\nsaves and discards worked and that made it impossible for\nme to tell it to save cards i wanted to protect\n19 SB, OP 1 because i could understand what it would do and i can predict\nwhat they would do better; and i have opinion that i can\nunderstand the clue of the ﬁrst agent and what the agent tries\nto force me to do; the ﬁrst agent preferred to play instead of\ndiscarding; second agent prefers to discard instead of play\nwhich is sub-optimal in hanabi game (i.e., it had full info\nabout a card and still chose to discard)\n20 OP, SB 7 It does understand rules of Hanabi among humans.\n21 SB, OP 7 It gave me more hints and it didn’t make inexplicable discard\ndecisions that were clearly suboptimal based on information\nthat it had at the time. It was also the only one of the six\ngames that we completed (25 pts).\n22 OP, SB 6 second agent played color clues that i gave\n23 SB, OP 5 I felt that the second agent understood some clues better than\nthe ﬁrst even though i think they are very very similar; similar\nstrategy on saving discarding cluing; main difference for the\nsecond one was that it would clue sooner than the ﬁrst one;\nit wouldn’t delay cluing even though it had cards; 3 games is\na bit short to determine/assess strategy;\n24 OP, SB 6 The main reason was that Agent 2 was willing to change its\ndiscard behavior to match mine, as I strongly prefer discard-\ning the oldest card instead of the newest. The other reason is\nthe second agent was a little better at giving clues to me that\nI understood the meaning of.\n27\n25 SB, OP 7 It seemed more cooperative in that it was giving a lot of\nhints and it seemed like we had a similar strategy. Early\non, we’d tell each other when we had ones, and then giving\nfull information, giving the appropriate hints for the state of\nthe game. Seemed like we had good teamwork. They were\ngiving hints, and also taking hints.\n26 OP, SB 7 The second agent understood my strategy better; it was easier\nfor me to follow it’s pattern/strategy; and because we got\ncloser to winning, ergo, it was doing something right\n27 SB, OP 2 the ﬁrst agent provides more certainty even though the game\nprogresses slower, it acts upon certainty and minimizes guess-\ning;\n28 OP, SB 6 i felt like the second agent was playing with easier to under-\nstand set of rules; they appeared to be more mindful of hints\nor number of hints remaining, so there is a better back-and-\nforth depending on who what playable cards or not;\n29 SB, OP 3 To my understanding, the strategy seemed very consistent\nand simple. Agent 2’s strategy seemed more complex and\nless predictable. It seemed more random which is less prefer-\nable.\n28",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.788054347038269
    },
    {
      "name": "Benchmarking",
      "score": 0.7378471493721008
    },
    {
      "name": "Computer science",
      "score": 0.7133627533912659
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7114244103431702
    },
    {
      "name": "Reinforcement learning",
      "score": 0.6262896060943604
    },
    {
      "name": "Preference",
      "score": 0.5282655954360962
    },
    {
      "name": "Task (project management)",
      "score": 0.5184490084648132
    },
    {
      "name": "Metric (unit)",
      "score": 0.4856206476688385
    },
    {
      "name": "Machine learning",
      "score": 0.47223520278930664
    },
    {
      "name": "Teamwork",
      "score": 0.4480477273464203
    },
    {
      "name": "Engineering",
      "score": 0.0796227753162384
    },
    {
      "name": "Microeconomics",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ]
}