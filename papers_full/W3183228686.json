{
  "title": "Self-Supervised Contrastive Learning with Adversarial Perturbations for Robust Pretrained Language Models",
  "url": "https://openalex.org/W3183228686",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A312167979",
      "name": "Meng Zhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2562998003",
      "name": "Yihan Dong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2035754532",
      "name": "Mrinmaya Sachan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1971196887",
      "name": "Roger Wattenhofer",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3173190788",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W3155936402",
    "https://openalex.org/W3026732421",
    "https://openalex.org/W3172399575",
    "https://openalex.org/W2982054702",
    "https://openalex.org/W3176047188",
    "https://openalex.org/W3122924117",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W2964253222",
    "https://openalex.org/W2949128310",
    "https://openalex.org/W3183845717",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2963969878",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3104423855",
    "https://openalex.org/W2947415936",
    "https://openalex.org/W3176108833",
    "https://openalex.org/W3098628719",
    "https://openalex.org/W2799194071",
    "https://openalex.org/W3128699722",
    "https://openalex.org/W3155188285",
    "https://openalex.org/W3176909378",
    "https://openalex.org/W3173783447",
    "https://openalex.org/W3017003177",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3120061794",
    "https://openalex.org/W3018378048",
    "https://openalex.org/W3009561768",
    "https://openalex.org/W2996851481",
    "https://openalex.org/W2962718684",
    "https://openalex.org/W3006647218",
    "https://openalex.org/W2971296908",
    "https://openalex.org/W2170240176"
  ],
  "abstract": "This paper improves the robustness of the pretrained language model BERT against word substitution-based adversarial attacks by leveraging self-supervised contrastive learning with adversarial perturbations. One advantage of our method compared to previous works is that it is capable of improving model robustness without using any labels. Additionally, we also create an adversarial attack for word-level adversarial training on BERT. The attack is efficient, allowing adversarial training for BERT on adversarial examples generated on the fly during training. Experimental results on four datasets show that our method improves the robustness of BERT against four different word substitution-based adversarial attacks. Furthermore, to understand why our method can improve the model robustness against adversarial attacks, we study vector representations of clean examples and their corresponding adversarial examples before and after applying our method. As our method improves model robustness with unlabeled raw data, it opens up the possibility of using large text datasets to train robust language models.",
  "full_text": "Self-Supervised Contrastive Learning with Adversarial Perturbations for\nDefending Word Substitution-based Attacks\nZhao Meng‚àó Yihan Dong‚àó Mrinmaya Sachan Roger Wattenhofer\nETH Zurich, Switzerland\n{zhmeng, yihdong, wattenhofer}@ethz.ch\nmrinmaya.sachan@inf.ethz.ch\nAbstract\nIn this paper, we present an approach to im-\nprove the robustness of BERT language mod-\nels against word substitution-based adversar-\nial attacks by leveraging adversarial perturba-\ntions for self-supervised contrastive learning.\nWe create a word-level adversarial attack gen-\nerating hard positives on-the-Ô¨Çy as adversar-\nial examples during contrastive learning. In\ncontrast to previous works, our method im-\nproves model robustness without using any la-\nbeled data. Experimental results show that our\nmethod improves robustness of BERT against\nfour different word substitution-based adver-\nsarial attacks, and combining our method with\nadversarial training gives higher robustness\nthan adversarial training alone. As our method\nimproves the robustness of BERT purely with\nunlabeled data, it opens up the possibility of\nusing large text datasets to train robust lan-\nguage models against word substitution-based\nadversarial attacks.\n1 Introduction\nPretrained language models such as BERT (De-\nvlin et al., 2019, inter alia) have had a tremendous\nimpact on many NLP tasks. However, several re-\nsearchers have demonstrated that these models are\nvulnerable to adversarial attacks, which fool the\nmodel by adding small perturbations to the model\ninput (Jia and Liang, 2017).\nA prevailing method to improve model robust-\nness against adversarial attacks is adversarial train-\ning (Madry et al., 2018). In NLP, adversarial train-\ning in the input space has been challenging, as\nexisting natural language adversarial attacks are\ntoo slow to generate adversarial examples on the\nÔ¨Çy during training (Alzantot et al., 2018; Ebrahimi\net al., 2018; Ren et al., 2019). While some recent\nworks (Wang et al., 2021c) have started exploring\nefÔ¨Åcient input space adversarial training (e.g., for\n‚àóThe Ô¨Årst two authors contributed equally to this work.\ntext classiÔ¨Åcation), scaling adversarial training to\npretrained language models like BERT has been\nchallenging.\nIn this work, we in particular focus on improving\nthe robustness of BERT against word substitution-\nbased adversarial attacks. We propose an approach\nto adversarially Ô¨Ånetune BERT-like models without\nusing any labeled data. In order to achieve this, we\nrely on self-supervised contrastive learning (Chen\net al., 2020). Self-supervised contrastive learning\nhas recently gained attention in the community and\ncontrastive learning has been used to learn better\nrepresentations for text classiÔ¨Åcation (Giorgi et al.,\n2021; Kim et al., 2021; Gao et al., 2021). How-\never, how to use these methods to improve model\nrobustness remains an open question.\nWe combine self-supervised contrastive learning\nwith adversarial perturbations by using adversarial\nattacks to generate hard positive examples for con-\ntrastive learning. To efÔ¨Åciently create adversarial\nexamples, we leverage an adversarial attack, that\nis capable of generating multiple adversarial exam-\nples in parallel. The attack adversarially creates\nhard positive examples for contrastive learning by\niteratively replacing words to follow the direction\nof the contrastive loss (see Ô¨Åg. 2).\nExperiments show that our method can improve\nthe robustness of pretrained language models with-\nout looking at the labels (in other words, be-\nfore Ô¨Ånetuning). Additionally, by combining our\nmethod with adversarial training, we are able to\nobtain better robustness than conducting adversar-\nial training alone (see section 4.4). Our study of\nthe vector representations of clean examples and\ntheir corresponding adversarial examples indeed\nexplains that our method improves model robust-\nness by pulling clean examples and adversarial ex-\namples closer.\nOur contributions1 in this paper are two-fold.\n1We will release our code at https://github.com/\nLotusDYH/ssl_robust\narXiv:2107.07610v3  [cs.CL]  24 May 2022\nOn the one hand, we improve the robustness of\nthe pretrained language model BERT against word\nsubstitution-based adversarial attacks by using self-\nsupervised contrastive learning with adversarial\nperturbations (see section 3.2). On the other hand,\nto facilitate adversarial self-supervised contrastive\nlearning, we create for BERT a word-level adver-\nsarial attack to create hard positive examples. The\nattack makes contrastive learning and adversarial\ntraining with on-the-Ô¨Çy generated adversarial exam-\nples possible. Additionally, we also show that our\nmethod is capable of using out-of-domain data to\nimprove model robustness (see table 2 and sec-\ntion 4.4). This opens an opportunity for using\nlarge-scale unlabeled data to train robust language\nmodels against word substitution-based adversarial\nattacks.\n2 Related Work\n2.1 Adversarial Training for NLP\nAdversarial training improves model robustness by\naugmenting clean examples with adversarial exam-\nples during training. Previous works on adversarial\ntraining for natural language mainly focus on per-\nturbations in the vector space, while actual adver-\nsarial attacks create adversarial examples by chang-\ning natural language symbols. For example, Zhu\net al. (2020) and Liu et al. (2020) improve model\ngeneralization ability by adversarial training on the\nword embedding space, without mentioning model\nrobustness. However, they either ignore model ro-\nbustness, or only test model robustness against the\nadversarial dataset ANLI, without paying attention\nto actual adversarial attacks. Other works conduct\nadversarial training in the word space (Alzantot\net al., 2018; Ren et al., 2019). Still, they can only\ndo adversarial training on a limited number of pre-\ngenerated adversarial examples due to the low efÔ¨Å-\nciency of the attacks. A recent work (Wang et al.,\n2021c) conducts adversarial training efÔ¨Åciently in\nthe word space, but their method is limited to non-\ncontextualized models.\nApart from adversarial training, other supervised\nlearning methods (Dong et al., 2021; Zhou et al.,\n2021; Wang et al., 2021a; Li and Qiu, 2020) have\nalso been proposed to improve robustness. How-\never, these methods are supervised and are not com-\nparable to our work.\nOur work also differs from previous works in\nnatural language adversarial training. On the one\nhand, as opposed to previous works, which are\nsupervised, we propose a self-supervised learning\nscheme to improve the robustness of pretrained\nlanguage models. On the other hand, while previ-\nous works mostly focus on adversarial training in\nembedding space, we conduct efÔ¨Åcient adversar-\nial training with pretrained language models at the\nword level.\n2.2 Contrastive Learning for NLP\nContrastive learning was Ô¨Årst proposed in the im-\nage domain to improve model performance in a\nself-supervised fashion (He et al., 2020; Chen et al.,\n2020). These methods bring representations of sim-\nilar examples closer and push representations of\ndissimilar examples further apart. Additionally,\nresearchers also Ô¨Ånd that by adding adversarial\nperturbations during contrastive learning, image\nclassiÔ¨Åcation models become more robust against\nadversarial attacks (Kim et al., 2020).\nIn NLP, previous works on contrastive learn-\ning mainly focus on improving model generaliza-\ntion. Gunel et al. (2021) boost performance of\nRoBERTa by adding supervised signals during Ô¨Åne-\ntuning on downstream tasks. Lee et al. (2021)\ntackle the ‚Äúexposure bias\" problem in text gen-\neration by adding adversarial signals during con-\ntrastive learning. Other similar works include Pan\net al. (2021), Giorgi et al. (2021), and Gao et al.\n(2021). Although these works have demonstrated\nthe usefulness of contrastive learning in NLP appli-\ncations, few have addressed the robustness of NLP\nmodels, particularly pretrained language models,\nagainst word substitution-based natural language\nadversarial attacks.\nRecently, Wang et al. (2021b) claimed that their\nmethod improves model robustness against adver-\nsarial sets. However, such sets are pre-generated\nand are less challenging than adversarial exam-\nples generated on the Ô¨Çy by actual adversarial at-\ntacks (Jin et al., 2020; Ren et al., 2019). In this\npaper, we focus on improving the robustness of pre-\ntrained language models against word substitution-\nbased adversarial attacks. We present the details of\nour method in section 3.\n3 Methodology\nIn this section, we describe our method for self-\nsupervised contrastive learning with adversarial\nperturbations. SpeciÔ¨Åcally, section 3.1 gives the\nbackground and motivation of our problem, and\nsection 3.2 describes the adversarial contrastive\nBERTùëã\nAttack for Contrastive Lossùëã!\nùëã\"#$\nùíõ!\nùíõùíõùíÇùíÖùíó\npullpushpush\nContrastive Learning\nAdversarial Attack\n(a) (b) (c)\nFigure 1: An illustration of our method. (a) For the original example X, we obtain the hard positive example\nX‚Ä≤ by Geometry Attack for contrastive loss (see section 3.3). (b) Before contrastive learning, in the vector space,\nthe clean example z, the hard positive example z‚Ä≤, and the adversarial example zadv are far from each other.\nContrastive learning pulls the clean example z, and the hard positive example z‚Ä≤ together. (c) After contrastive\nlearning, the clean example z, the hard positive example z‚Ä≤, and the adversarial example zadv are close. We omit\nMLP in this Ô¨Ågure for simplicity. We use a different color to show another example from the dataset. See section 3\nfor details. Note that the adversarial example Xadv and its corresponding vector zadv are not used in contrastive\nlearning. We nevertheless show Xadv and zadv for illustration purposes.\nlearning framework. Finally, in section 3.3, we\ndescribe the adversarial attack used in contrastive\nlearning.\n3.1 Background and Motivation\nIn this work, we focus on text classiÔ¨Åcation tasks2.\nLet us assume that we have an example text Xi =\n{w1,w2,...,w L}with Lwords and let yi be the\ncorresponding class label for Xi. Our text classiÔ¨Å-\ncation model consists of a BERT encoder f(¬∑) and\nan MLP classiÔ¨Åcation head c(¬∑).\nWe obtain the vector representation hi ‚ààRd\nof the example Xi by feeding Xi into the BERT\nencoder f(¬∑). Then the MLP classiÔ¨Åcation headc(¬∑)\ntakes hi as input to give us the prediction. Formally,\nwe have:\nhi = f(Xi)\nÀÜyi = c(hi)\nwhere ÀÜyi is the predicted label. We have ÀÜyi = yi if\nthe model prediction is correct.\nA word substitution-based adversarial attacka(¬∑)\ntakes an original exampleXi as input and generates\nan adversarial example Xadv\ni by substituting the k-\nth original word wk in Xi with another word wadv\nk .\nTo make the orignal exampleXi and the adversarial\nexample Xadv\ni close in semantics, existing works\noften use synonyms as substitutions (Ren et al.,\n2019; Morris et al., 2020).\n2Although our formulation can also be extended to several\nother problems.\nBy conducting the word substitution, the attack\na(¬∑) aims to fool the model with Xadv\ni . Formally,\nwe have:\nXadv\ni = a(Xi)\nhadv\ni = f(Xadv\ni )\nÀÜyadv\ni = c(hadv\ni )\nwhere Xadv\ni = {w1,w2,...,w adv\nk ,...,w L},1 ‚â§\nk‚â§L. Assuming the attack successfully fools the\nmodel, we would have ÀÜyi Ã∏= ÀÜyadv\ni . The key assump-\ntion in our approach is that although Xi and Xadv\ni\nare very similar to each other at the word level, it is\npossible that the encoder f embeds them in such a\nway that the distance between their representations\nhi and hadv\ni are large and the classiÔ¨Åcation head\nc(¬∑) predicts Xi and Xadv\ni to be of different classes.\nThus, the goal of our method is to obtain a robust\nmodel, on which we have yi = ÀÜyi and ÀÜyi = yadv\ni .\nIn other words, the robust model defends an ad-\nversarial example Xadv\ni of the original example Xi\nsuccessfully, if the robust model gives the same cor-\nrect prediction on the original example Xi and the\nadversarial example Xadv\ni . We use attack success\nrate as the evaluation metric for model robustness.\nThe attack success rate is deÔ¨Åned as the rate of an\nattack successfully fooling the model on all test\nexamples.\nTo obtain a robust model, we optimize the en-\ncoder such that hi and hadv\ni become similar to\neach other. We achieve this goal by conducting\nself-supervised contrastive learning on the encoder\nwith adversarial perturbations, during which we use\nan attack to create hard positive examples, maxi-\nmizing the contrastive loss. The rest this section\ngives the details of our method.\n3.2 Self-Supervised Contrastive Learning\nwith Adversarial Perturbations\nFollowing previous works on self-supervised con-\ntrastive learning (He et al., 2020; Chen et al., 2020),\nwe formulate our learning objectives as follows.\nConsider we have a batch of nexamples and Xi\nis the i-th input, we Ô¨Årst obtain X‚Ä≤\ni = t(Xi) as an\naugmentation of Xi by transformation t(¬∑). We call\nXi and X‚Ä≤\ni a pair of positive examples. All other\nexamples in the same batch are considered negative\nexamples of Xi and X‚Ä≤\ni.\nTo take advantage from using more negative ex-\namples, we use MoCo (He et al., 2020) as our\nframework, in which we employ an encoder fq\nfor the positive examples, and another momentum\nencoder fk for the negative examples. We then\nhave:\nhi = fq(Xi)\nh‚Ä≤\ni = fk(X‚Ä≤\ni)\nwhere hi,h‚Ä≤\ni ‚ààRd are representations of Xi and\nX‚Ä≤\ni, respectively. During training, fq and fk are\ninitialized the same. We update fk momentarily:\nŒ∏k ‚Üêm¬∑Œ∏k + (1‚àím) ¬∑Œ∏q\nwhere Œ∏k and Œ∏q denote the parameters of fk and\nfq, respectively. We then have:\nzi = gq(hi)\nz‚Ä≤\ni = gq(h‚Ä≤\ni)\nwhere zi,z‚Ä≤\ni ‚ààRc, gq(¬∑) and gk(¬∑) are MLPs with\none hidden layer of sigmoid activation, respec-\ntively. Following Chen et al. (2020), we conduct\ncontrastive learning on z instead of h to prevent\nthe contrastive learning objective from removing\ninformation useful for downstream tasks. After\ncontrastive learning, we use h as the sentence rep-\nresentation for downstream tasks.\nAdditionally, we also maintain a dynamic Ô¨Årst-\nin-Ô¨Årst-out queue for the negative examples. Dur-\ning training, before computing contrastive loss at\nthe end of each batch, all encoded examples of the\ncurrent batch are enqueued into the queue, and the\noldest examples are dequeued simultaneously.\nIn our experiments, we use the attack described\nin section 3.3 or back-translation (Zhu et al., 2015)\nfor augmentation t(¬∑). Assume that we have an\nencoded example zi and the encoded examples in\nthe queue are {z0,z1,¬∑¬∑¬∑ ,zQ‚àí1}, where Qis the\nsize of the queue. Among the encoded examples in\nthe queue, one of them is z‚Ä≤\ni, which forms a pair of\npositive examples with zi. We use contrastive loss\nto maximize the similarity between positive exam-\nples, while minimizing the similarity of negative\nexamples. We then have:\n‚Ñìi = ‚àílog exp(sim(zi,z‚Ä≤\ni)/œÑ)\n‚àëQ\nk=0 exp(sim(zi,zk)/œÑ)\n(1)\nwhere œÑ is the temperature parameter, sim(¬∑,¬∑) is\nthe similarity function, and Qis the size of the dy-\nnamic queue. In this paper, we compute similarity\nby dot product as in MoCo.\nBy optimizing eq. (1), the goal is to maximize\nthe similarity of representations between similar\n(positive) pairs of examples while minimizing the\nsimilarity of representations between dissimilar\n(negative) examples. We use the geometry-inspired\nattack described in section 3.3 as the transforma-\ntion t(¬∑) to create pairs of examples that are similar\non the word level but at the same time are distant\nfrom each other in the representation space.\nWe illustrate our method in Ô¨Åg. 1. In Ô¨Åg. 1 (b)\nand (c), by conducting contrastive learning and\nusing the Geometry Attack generated adversarial\nexamples as hard positives, the vector representa-\ntions obtained from the model become invariant to\nthe adversarial attacks.\nùíõùíä\nùíõùíäùüè\nùíõùíäùüê\nùíóùíõùíä\n||ùíëùíäùüè||\n||ùíëùíäùüê||\nFigure 2: An illustration of one iteration in Geometry\nAttack for contrastive loss. See section 3.3 for details.\n3.3 Creating Hard Positive Examples by\nGeometry Attack\nAs mentioned in section 3.2, we use an attack as\nthe transformation t(¬∑) during contrastive learning.\nWe describe how this attack creates adversarial ex-\namples for contrastive loss during self-supervised\ncontrastive learning (see Ô¨Åg. 1 (b)) in this subsec-\ntion.\nInspired by Meng and Wattenhofer (2020), who\nleverage geometry of representations to generate\nnatural language adversarial examples for text clas-\nsiÔ¨Åcation tasks, we also use the geometry of pre-\ntrained representations to create adversarial exam-\nples for contrastive loss. The created adversarial\nexamples are used as positive examples of the origi-\nnal examples in our contrastive learning framework,\nand at the same time are created to maximize the\ncontrastive loss. Hence, we refer to adversarial\nexamples created by the attack as hard positive\nexamples.\nThe intuition of our attack is that we repeat-\nedly replace words in the original texts such that\nin each iteration, the replaced word increases the\ncontrastive loss as much as possible. To be speciÔ¨Åc,\nconsider an example Xi, we then have:\n1. Determine Direction for Sentence Vector\nCompute the gradients of ‚Ñìi with respect to zi. In\nthis step, we Ô¨Ånd the direction we should move\nfrom zi to increase the contrastive loss ‚Ñìi. We\nhave the gradient vector vzi = ‚àázi‚Ñìi.\n2. Choose Original Word to be Replaced Com-\npute the gradients of‚Ñìi with respect to input word\nembeddings of Xi. For words tokenized into mul-\ntiple tokens, we take the average of the gradients\nof the tokens. In this step, we Ô¨Ånd the word wt\nwhich has the most inÔ¨Çuence in computing ‚Ñìi.\nSpeciÔ¨Åcally, assuming we haveLwords, then we\nchoose t = arg maxt{||g1||,||g2||,..., ||gL||},\nwhere gk is the gradients of li with respect to the\nembeddings of word wk, 1 ‚â§k‚â§L.\n3. Generate Candidate Set Suppose we choose\nthe word wt in step 2. In this step, we use a pre-\ntrained BERT to choose the most probable can-\ndidates wt to replace it in the original text. We\nhave the candidates set = {wt1 ,wt2 ,¬∑¬∑¬∑ ,wtT }.\nFollowing Jin et al. (2020), we Ô¨Ålter out seman-\ntically different words from the candidate set by\ndiscarding candidate words of which the cosine\nsimilarity of their embeddings between the em-\nbeddings of wt is below a threshold œµ. We set\nthe threshold œµ= 0.5 and use counter-Ô¨Åtted word\nembeddings (Mrk≈°i¬¥c et al., 2016) to compute the\ncosine similarity.\n4. Choose Replacement Word Replace wt with\nwords in the candidates set, resulting in text vec-\ntors {zi1 ,zi2 ,¬∑¬∑¬∑ ,ziT }. We compute delta vec-\ntor rij ‚Üêzij ‚àízi. The projection of rij onto vzi\nis: pij ‚Üê\nrij ¬∑vzi\n||vzi||. We select word wtm, where\nm ‚Üêargmaxj ||pij ||. In other words, wtm re-\nsults in the largest projection pim onto vzi.\n5. Repetition Replace wt with wtm in Xi, then we\nhave zi ‚Üêzim. Repeat step 1-4 for N iterations,\nwhere N is a hyperparameter of our method. We\nexpect ‚Ñìi to increase in each iteration.\nFigure 2 illustrates an iteration of our attack, in\nwhich we have two options to choose from the can-\ndidate set. This attack can be easily implemented in\na batched fashion, making it possible for us to gen-\nerate adversarial exampleson the Ô¨Çyduring training.\nFurthermore, our efÔ¨Åcient implementation makes it\npossible to conduct contrastive learning with adver-\nsarial perturbations as well as adversarial training\nwith adversarial examples generated on the Ô¨Çy. We\ngive a speed comparison of our attack and other\nattacks in appendix D. We also give pseudocode\nof the attack in algorithm 1 of appendix A.\n4 Experiments\n4.1 Datasets and Evaluation Metrics\nWe test how our method improves model robust-\nness on four text classiÔ¨Åcation datasets: AG‚Äôs\nNews, Yelp, IMDB, and DBpedia (See appendix B\nfor details).\nWe report theattack success rateand the replace-\nment rate of the attacks as the evaluation metrics.\nFollowing Alzantot et al. (2018); Ebrahimi et al.\n(2018), to prevent the model accuracy on clean ex-\namples from confounding the results, we deÔ¨Åne the\nsuccess rate of an attack on all correctly classiÔ¨Åed\nexamples in the test set. Lower success rates indi-\ncate higher robustness. The replacement rate refers\nto the percent of original words replaced in the\nclean example. Higher replacement rates indicate\nthat the attack needs to replace more words to fool\nthe model, and thus mean that the model is more\nrobust.\n4.2 Attacks for Evaluating Robustness\nWe use four word substitution-based adversarial\nattacks to evaluate the model robustness.\nGeometry Attack We use the same attack de-\nscribed in section 3.3 to generate adversarial exam-\nples for sentence classiÔ¨Åcation tasks by replacing\ncontrastive loss with cross-entropy classiÔ¨Åcation\nloss. We set the maximum number of replaced\nwords to 20.\nTextFooler, PWWS, and BAE-R We use the de-\nfault implementations from TextAttack (Morris\net al., 2020).\nAll these attacks will give up and terminate once\nthe maximum number of replaced words (some-\ntimes also called perturbation budget) is reached.\n4.3 Experimental Design\nWe have the following hypotheses for our method:\nH1: Self-supervised contrastive learning improves\nmodel robustness against adversarial attacks. More-\nover, using adversarial perturbations during con-\ntrastive learning further improves robustness.\nTo validate this hypothesis, we set three different\npretraining schemes:\nBTCL: Pretraining with back-translation as the\ntransformation t(¬∑) for self-supervised contrastive\nlearning.\nADCL: Pretraining with Geometry Attack for con-\ntrastive loss (see section 3.3) as transformation t(¬∑)\nfor self-supervised contrastive learning.\nNP: Apart from the above two settings, we also add\na No Pretraining baseline to understand the general\neffectiveness of contrastive learning.\nH2: Combining self-supervised contrastive learn-\ning with adversarial training gives higher robust-\nness than conducting adversarial training alone.\nWe use different Ô¨Ånetune strategies to understand\nhow adding adversarial training to our method af-\nfects model robustness. We have two settings:\nFTC: We Ô¨Ånetune the pretrained model on the clean\nexamples of the corresponding downstream dataset.\nADV: We conduct adversarial training by leverag-\ning supervisedly generated adversarial examples.\nNote that our adversarial training is different from\nprevious works (Ren et al., 2019; Alzantot et al.,\n2018), which merely Ô¨Ånetune the model on a Ô¨Åxed\nnumber of pre-generated adversarial examples. In-\nstead, our adversarial training scheme is similar\nto Madry et al. (2018), where the model is Ô¨Åne-\ntuned on clean examples and adversarial examples\ngenerated on the Ô¨Çy during each batch of training.\nWe use Geometry Attack for adversarial train-\ning as the remaining three attacks are not efÔ¨Åcient\nenough to generate adversarial examples on the Ô¨Çy\n(see appendix D for details).\nH3: Our contrastive learning method is capable\nof using out-of-domain data to improve the model\nrobustness.\nWhile in H1 and H2, we use the same dataset for\npretraining and Ô¨Ånetuning, we want to test how our\nmethod can leverage out-of-domain data. Hence,\nwe have two additional experimental settings:\nIn-Domain: We use the same dataset during con-\ntrastive learning and Ô¨Ånetuning.\nOut-of-Domain: We use different datasets for\ncontrastive learning and Ô¨Ånetuning.\nH4: By optimizing eq. (1), our method pulls the\nrepresentations of the clean samples and their cor-\nresponding hard positive examples closer in the\nvector space while pushing other different exam-\nples further. In this way, the representations of\nclean examples and their adversarial examples are\nalso closer in the vector space.\nWe validate this hypothesis by conducting a vector\nspace study. See section 4.4 for details.\nNote that to avoid confusing adversarial exam-\nples generated during contrastive learning and ad-\nversarial examples generated during Ô¨Ånetuning,\nwe refer to the former as hard positive examples\n(see section 3.3).\n4.4 Results\nTable 1 shows the experimental results for validat-\ning H1 and H2. For each dataset, when evaluating\nthe model robustness, we use the same perturbation\nbudget across different settings. Note that although\nthe replacement rates vary across different settings\nof the same dataset, the perturbation budget for the\nsame attack is the same in these settings. By using\nthe same perturbation budget, we ensure that the\nsuccess rates of the attacks provide us with a fair\nevaluation of the robustness of the model (Wang\net al., 2021c; Ren et al., 2019).\nH1: To validate H1, we focus on rows with the\nFTC setting during Ô¨Ånetuning. We can observe that\nmodels without any contrastive pretraining ( NP)\nare the most vulnerable to adversarial attacks. For\nexample, the success rate of the Geometry Attack\nfor AG‚Äôs News dataset is 86.2% for theNP model.\nIn contrast, for BTCL and ADCL, the success rate of\nthe Geometry Attack is at least 5.6% lower than this\nsetting. This shows that self-supervised contrastive\nlearning does improve model robustness.\nAdditionally, we can also see from table 1 that\nADCL improves the model robustness more than\nBTCL. For example, in the IMDB dataset, the\nmodel pretrained with ADCL is 9.1% more robust\nthan the model pretrained with BTCL (93.3% ‚Üí\n84.2%), showing that using adversarial pertur-\nbations during contrastive learning further im-\nproves model robustness against adversarial at-\nDataset Pretrain Finetune Acc. (%) Success Rate (%)‚Üì Replaced (%)‚Üë\nGeometry TextFoolerPWWS BAE-R Geometry TextFoolerPWWS BAE-R\nAG\nNP FTC 94.2 86 .2 87 .6 63 .6 17 .9 18 .6 25 .7 20 .9 7 .4\nADV 94.4 20.7 25.1 26.1 10.7 20.5 29.3 22.3 7.7\nBTCL FTC 94.4 80 .6 84 .6 63 .1 17 .7 18 .1 24 .6 20 .9 7 .5\nADCL FTC 94.3 76 .5 80 .7 55 .9 14 .1 19 .1 26 .7 22.6 7.5\nADV 94.4 18.7 23.5 24.7 9.7 20.6 29.3 22.2 7 .2\nYelp\nNP FTC 97.1 94 .6 94 .3 97 .0 42 .1 10 .6 10 .4 7 .1 6 .7\nADV 96.2 38.8 52.4 62.7 22.2 12.8 17.3 11.3 8.8\nBTCL FTC 97.1 92 .3 91 .6 94 .8 39 .2 11 .0 10 .1 7 .7 6 .9\nADCL FTC 97.0 88 .6 88 .2 91 .1 37 .8 10 .4 10 .5 7 .4 6 .9\nADV 96.1 35.6 50.1 61.0 21.0 13.4 17.1 11.2 8.3\nIMDB\nNP FTC 92.3 98 .7 99 .0 99 .2 54 .0 3 .5 6 .5 4 .3 3 .0\nADV 92.0 51.4 75.3 79.1 35.1 7.4 12.7 9.3 3.6\nBTCL FTC 92.5 93 .3 96 .6 95 .1 52 .0 4 .5 7 .4 4 .4 3 .3\nADCL FTC 92.4 84 .2 87 .8 87 .8 48 .0 3 .7 8 .7 5 .1 2 .3\nADV 91.9 48.7 74.4 77.6 31.8 8.1 12.4 9.1 3.5\nDBpedia\nNP FTC 99.2 79 .6 79 .3 46 .7 14 .3 17 .8 23 .2 16 .2 13 .3\nADV 99.0 13.9 16.5 17.7 10.9 21.6 28.2 18.9 14.1\nBTCL FTC 99.2 77 .4 76 .8 45 .1 13 .0 18 .9 22 .8 18 .1 13 .1\nADCL FTC 99.1 73.6 74 .5 42 .6 11 .6 18 .2 22 .9 17 .6 12 .8\nADV 99.0 12.4 14.8 16.2 10.1 20.1 28.6 18.2 13.8\nTable 1: Experimental results for H1 and H2. In-Domain setting is used. We bold the best results, while the\nsecond best is in italic.\ntacks. Hence, we do not combine BTCL with ADV\nin later experiments for simplicity.\nTo further understand how contrastive learning\nimproves the model robustness, we study the trans-\nferability of the adversarial examples between mod-\nels without any contrastive pretraining (NP) and the\nmodels pretrained with ADCL. To be speciÔ¨Åc, the\nmodels are Ô¨Årst pretrained using eitherNP or ADCL,\nand then Ô¨Ånetuned on clean examples (FTC). Then,\nwe use a NP model to generate adversarial exam-\nples on the test set of each dataset, and then test\nthe corresponding model pretrained with ADCL on\nthese adversarial examples. And vice versa.\nTable 3 shows the results. We can see that adver-\nsarial examples generated by models pretrained\nwith ADCL have much higher success rates on\nmodels without any contrastive pretraining ( NP).\nFor example, for the AG‚Äôs News dataset, the suc-\ncess rates increase by 32.1%, 35.3%, 33.8%, and\n22.1% for Geometry Attack, TextFooler, BAE-R,\nand PWWS, respectively. This demonstrates that\nby self-supervised contrastive learning with adver-\nsarial perturbations, the models become more ro-\nbust against attacks.\nH2: To validate H2, we compare two settings of\nNP + ADV and ADCL + ADV. We note that when\ncompared with conducting adversarial training\nalone (NP + ADV), combining our self-supervised\ncontrastive learning method with adversarial train-\ning (ADCL + ADV) constantly results in higher ro-\nbustness. In other words, the adversarial attacks\nhave lower success rates and higher replacement\nrate in ADCL + ADV models than in NP + ADV\nmodels. For instance, for the IMDB dataset, the\nADCL + ADV model is 2.7% more robust than the\nNP + ADV model, when both models are tested\nagainst the Geometry Attack (Success rates of Ge-\nometry attack: ADCL + ADV: 48.7%, NP + ADV:\n51.4%; Replacement rates: ADCL + ADV: 8.1%,\nNP + ADV: 7.4%).\nNote that when test NP + ADV models and\nADCL + ADV models against the other three ad-\nversarial attacks, ADCL+ ADV models do not show\nan advantage over NP+ ADV models in terms of re-\nplacement rates, despite that ADCL + ADV models\nstill constantly make lower success rates against\nthe adversarial attacks. We argue that this is be-\ncause we use the Geometry Attack for adversarial\ntraining during Ô¨Ånetuning, and the adversarial ex-\namples from the Geometry Attack might not fully\nmatch the distribution from the other attacks. Nev-\nertheless, we can still conclude that ADCL + ADV\nmodels are more robust than NP + ADV models.\nOur experiments also show that during con-\nDataset Domain Pretrain Acc. (%) Success Rate (%)‚Üì Replaced (%)‚Üë\nGeometry TextFoolerPWWS BAE-RGeometry TextFoolerPWWS BAE-R\nAG\n- NP 94.2 86 .2 87 .6 63 .6 17 .9 18 .6 25 .7 20 .9 7 .4\nIn-Domain BTCL 94.4 80 .6 84 .6 63 .1 17 .7 18 .1 24 .6 20 .9 7.5\nADCL 94.3 76.9 80.7 55.9 14.1 19.1 26.7 22.6 7.5\nOut-of-Domain ADCL94.1 79.2 84.0 60.4 16.3 18.7 25.9 21.9 7.5\nIMDB\n- NP 92.3 98 .7 99 .0 99 .2 54 .0 3 .5 6 .5 4 .3 3 .0\nIn-Domain BTCL 92.5 93 .3 96 .6 95 .1 52 .0 4.5 7.4 4 .4 3.3\nADCL 92.4 84.2 87.8 87.8 48.0 3.7 8.7 5.1 2.3\nOut-of-Domain ADCL92.5 92.3 95.7 94.5 50.1 4.4 8.6 5.3 3.1\nTable 2: Comparison of Out-of-Domain with In-Domain. We use the DBpedia dataset as the out-of-domain\ndataset for AG‚Äôs News and IMDB. Models are Ô¨Ånetuned on clean examples after pretraining ( FTC). Best results\nare bolded, while the second best are in italic.\nDataset Attack Success Rate (%)\nNP‚ÜíADCL ADCL‚ÜíNP\nAG\nGeometry 30.2 62 .3\nTextFooler 19.7 55 .0\nBAE-R 26.4 60 .2\nPWWS 28.3 50 .4\nYelp\nGeometry 30.1 36 .4\nTextFooler 22.4 28 .0\nBAE-R 37.4 41 .5\nPWWS 34.8 36 .3\nIMDB\nGeometry 38.2 41 .4\nTextFooler 22.1 25 .2\nBAE-R 28.9 30 .8\nPWWS 24.7 26 .0\nDBpedia\nGeometry 34.6 52 .2\nTextFooler 27.5 42 .8\nBAE-R 32.5 55 .8\nPWWS 55.3 58 .8\nTable 3: Transferability of adversarial examples. The\nmodels are pretrained under either NP or ADCL, and\nthen Ô¨Ånetuned on clean examples. NP ‚ÜíADCL: Gen-\nerate adversarial examples with the model pretrained\nwith NP, then test the model pretrained with ADCL on\nthese adversarial examples. Same applies to ADCL ‚Üí\nNP.\ntrastive learning, the queue size (see section 3.2)\nhas an impact on the Ô¨Ånal performance. We give\nthe detailed analysis in appendix C.\nH3: For the Out-of-Domain setting, we use\nthe DBpedia dataset as the out-of-domain dataset\nfor the AG‚Äôs News and IMDB datasets, mainly be-\ncause (1) Computational limits: While using larger\ndatasets such as BookCorpus or Wikipedia might\nbe more useful, conducting self-supervised con-\ntrastive learning on these datasets exceeds the limits\nof our computational infrastructure; (2) The DBpe-\ndia dataset is several times larger than AG‚Äôs News\nand IMDB. This should give us a glimpse of what\nit looks like when we scale self-supervised con-\ntrastive learning with adversarial perturbations to\neven larger out-of-domain datasets; (3) The DBpe-\ndia dataset (topic classiÔ¨Åcation on Wikipedia) has\na different task and domain compared to the AG‚Äôs\nNews dataset (news classiÔ¨Åcation from a newspa-\nper) and IMDB dataset (sentiment classiÔ¨Åcation\non movie reviews). This discrepancy allows us to\nunderstand how out-of-domain datasets could help.\nTable 2 shows our results. We can see\nthat models pretrained with ADCL under the\nOut-of-Domain setting are more robust than\nmodels without any pretraining at all ( NP). This\nshows that our method can improve model robust-\nness using out-of-domain data . For instance, for\nthe IMDB dataset, the success rate of TextFooler\ndecreases from 98.7% for FT models to 92.3% for\nOut-of-Domain ADCLmodels. This shows that\nour method can improve the model robustness even\nif the dataset used for contrastive learning is from\na completely different domain. Note that in table 2,\nafter pretraining, we Ô¨Ånetune the model on clean\nexamples (FTC).\nWe also notice that models pretrained withADCL\nunder the Out-of-Domain setting are not as ro-\nbust as models pretrained with ADCL under the\nIn-Domain setting. This indicates we might need\nto use much larger unlabeled raw datasets to obtain\nmore improvements.\nH4: To validate this hypothesis, we study the vector\nrepresentations of M = 1000clean examples of\nthe AG‚Äôs News dataset and their corresponding\nadversarial examples. We obtain the adversarial\nexamples by attacking a NP + FTC model.\nLet v1,v2...vM and v‚Ä≤\n1,v‚Ä≤\n2...v‚Ä≤\nM be the vector\nrepresentations of the clean examples and corre-\nsponding adversarial samples, respectively. For\neach setting, we evaluate three metrics:\n0.0 0.2 0.4 0.6 0.8 1.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(a) NP + FTC\n0.0 0.2 0.4 0.6 0.8 1.0\n0.2\n0.4\n0.6\n0.8\n1.0 (b) ADCL + FTC\n0.0 0.2 0.4 0.6 0.8 1.0\n0.2\n0.4\n0.6\n0.8\n1.0 (c) NP + ADV\n0.0 0.2 0.4 0.6 0.8 1.0\n0.2\n0.4\n0.6\n0.8\n1.0 (d) ADCL + ADV\nFigure 3: t-SNE plot of the vector representations of clean examples and adversarial examples from the AG‚Äôs News\ndataset. Markers of the same color indicate a pair of clean example ( ‚ó¶) and adversarial example (‚ñ≥). Check sec-\ntion 4.4 for the evaluation settings. The ranges of x-axis and y-axis are normalized to [0, 1]. We connect each clean\nexample by a dotted line to its corresponding adversarial example.\nDataset Distance (dpos/dneg/Œ¥)\nNP+FTC ADCL+FTC NP+ADV ADCL+ADV\nAG 2.4/3.9/1.5 1.8/4.0/2.2 0.7/4.1/3.4 0.7/4.4/3.7\nYelp 3.5/3.7/0.2 2.9/4.0/1.1 0.7/3.2/2.5 0.5/3.4/2.9\nIMDB3.0/3.7/0.7 2.3/3.8/1.5 0.6/3.4/2.8 0.6/3.8/3.2\nDBpedia2.8/4.8/2.0 2.3/5.1/2.8 0.4/4.9/4.5 0.4/5.2/4.8\nTable 4: Vector space study. For each setting, we eval-\nuate three metrics: (a) Average distance between posi-\ntive pairs; (b) Average distance between negative pairs;\n(c) Difference between (a) and (b).\n‚Ä¢ Average distance dpos between each of the posi-\ntive pairs vi and v‚Ä≤\ni, where 1 ‚â§i‚â§M. Then we\nhave:\ndpos = 1\nM\nM‚àë\ni=1\nd(vi,v‚Ä≤\ni)\nwhere d(¬∑,¬∑) denotes the distance between two\nvectors.\n‚Ä¢ Average distance dneg between negative pairs:\ndneg =\nM‚àë\ni=1\nM‚àë\nj=1\n1 iÃ∏=j(d(vi,vj) +d(vi,v‚Ä≤\nj)\n2(M ‚àí1)\n‚Ä¢ Difference Œ¥= dneg ‚àídpos between (a) and (b).\nFurthermore, we evaluate the above metrics un-\nder the following settings:\n‚Ä¢ NP + FTC: Finetune on clean examples.\n‚Ä¢ ADCL + FTC: First do ADCL pretraining, and\nthen Ô¨Ånetune on clean examples.\n‚Ä¢ NP + ADV: Finetune with adversarial training.\n‚Ä¢ ADCL + ADV: First do ADCL pretraining. Then\nÔ¨Ånetune with adversarial training.\nTable 4 shows the results. We can see that our\nmethod (1) increases the distance between negative\npairs in all settings; (2) decreases the distance be-\ntween positive pairs inNP+FTC and ADCL+FTC\nmodels, while the distances between positive pairs\nbarely change in NP+ ADV and ADCL+ ADV mod-\nels; (3) increases Œ¥ in all settings. The above ob-\nservations validate H4 in section 4.3, and further\nexplain that our method achieves higher robustness\nby pushing vector representations of clean exam-\nples and adversarial examples closer.\nIn Ô¨Åg. 3, we further give qualitative analysis on\nthe distances between clean examples and adversar-\nial examples of the AG‚Äôs News dataset by showing\nthe t-SNE plot. We can see from the plot that the\ndistances between the clean examples and the corre-\nsponding adversarial examples are closer when we\napply ADCL pretraining, and that combining ADCL\nwith ADV gives the smallest distance between su-\npervised adversarial examples. Additional plots of\nother datasets are available in appendix H.\n5 Conclusion and Future Work\nIn this paper, we improve the robustness of pre-\ntrained language models against word substitution-\nbased adversarial attacks by using self-supervised\ncontrastive learning with adversarial perturbations.\nOur method is different from previous works as we\ncan improve model robustness without accessing\nannotated labels. Furthermore, we also conduct\nword-level adversarial training on BERT withon-\nthe-Ô¨Çy generated adversarial examples. Our adver-\nsarial training is different from previous works in\nthat (1) it is on the word level; (2) we generate ad-\nversarial examples on the Ô¨Çy, instead of generating\na Ô¨Åxed adversarial set beforehand. Experiments\nshow that our method improves model robustness.\nWe Ô¨Ånd that combining our method with adversar-\nial training results in better robustness than con-\nducting adversarial training alone. In the future,\nwe plan to scale our method to even larger out-of-\ndomain datasets.\nEthical Considerations\nTo the best of our knowledge, the data used in\nour work does not contain sensitive information.\nAlthough our models are evaluated on academic\ndatasets in this paper, they could also be used in\nsensitive contexts, e.g. healthcare or legal scenarios.\nIt is essential that necessary anonymization and\nrobustness evaluation is undertaken before using\nour models in these settings.\nAcknowledgements\nMrinmaya Sachan acknowledges support from an\nETH Z√ºrich Research grant (ETH-19 21-1) and a\ngrant from the Swiss National Science Foundation\n(project #201009) for this work.\nReferences\nMoustafa Alzantot, Yash Sharma, Ahmed Elgohary,\nBo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang.\n2018. Generating natural language adversarial ex-\namples. In EMNLP.\nTing Chen, Simon Kornblith, Mohammad Norouzi,\nand Geoffrey E. Hinton. 2020. A simple framework\nfor contrastive learning of visual representations. In\nICML.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL.\nXinshuai Dong, Anh Tuan Luu, Rongrong Ji, and Hong\nLiu. 2021. Towards robustness against natural lan-\nguage word substitutions. In ICLR.\nJavid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing\nDou. 2018. HotFlip: White-box adversarial exam-\nples for text classiÔ¨Åcation. In ACL.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimcse: Simple contrastive learning of sentence em-\nbeddings. In EMNLP.\nJohn Giorgi, Osvald Nitski, Bo Wang, and Gary Bader.\n2021. DeCLUTR: Deep contrastive learning for un-\nsupervised textual representations. In ACL.\nBeliz Gunel, Jingfei Du, Alexis Conneau, and Ves Stoy-\nanov. 2021. Supervised contrastive learning for pre-\ntrained language model Ô¨Åne-tuning. In ICLR.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss B. Girshick. 2020. Momentum contrast for un-\nsupervised visual representation learning. In CVPR.\nRobin Jia and Percy Liang. 2017. Adversarial exam-\nples for evaluating reading comprehension systems.\nIn EMNLP.\nDi Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter\nSzolovits. 2020. Is bert really robust? a strong base-\nline for natural language attack on text classiÔ¨Åcation\nand entailment. In AAAI.\nMinseon Kim, Jihoon Tack, and Sung Ju Hwang. 2020.\nAdversarial self-supervised contrastive learning. In\nNeurIPS.\nTaeuk Kim, Kang Min Yoo, and Sang-goo Lee. 2021.\nSelf-guided contrastive learning for BERT sentence\nrepresentations. In ACL.\nSeanie Lee, Dong Bok Lee, and Sung Ju Hwang. 2021.\nContrastive learning with adversarial perturbations\nfor conditional text generation. In ICLR.\nLinyang Li and Xipeng Qiu. 2020. Tavat: Token-aware\nvirtual adversarial training for language understand-\ning. arXiv preprint arXiv:2004.14543.\nXiaodong Liu, Hao Cheng, Pengcheng He, Weizhu\nChen, Yu Wang, Hoifung Poon, and Jianfeng Gao.\n2020. Adversarial training for large neural language\nmodels. arXiv preprint arXiv:2004.08994.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn ACL.\nAleksander Madry, Aleksandar Makelov, Ludwig\nSchmidt, Dimitris Tsipras, and Adrian Vladu. 2018.\nTowards deep learning models resistant to adversar-\nial attacks. In ICLR.\nZhao Meng and Roger Wattenhofer. 2020. A geometry-\ninspired attack for generating natural language ad-\nversarial examples. In COLING.\nJohn Morris, Eli LiÔ¨Çand, Jin Yong Yoo, Jake Grigsby,\nDi Jin, and Yanjun Qi. 2020. TextAttack: A frame-\nwork for adversarial attacks, data augmentation, and\nadversarial training in NLP. In EMNLP: System\nDemonstrations.\nNikola Mrk≈°i¬¥c, Diarmuid √ì S√©aghdha, Blaise Thom-\nson, Milica Ga≈°i ¬¥c, Lina M. Rojas-Barahona, Pei-\nHao Su, David Vandyke, Tsung-Hsien Wen, and\nSteve Young. 2016. Counter-Ô¨Åtting word vectors to\nlinguistic constraints. In NAACL.\nXiao Pan, Mingxuan Wang, Liwei Wu, and Lei Li.\n2021. Contrastive learning for many-to-many mul-\ntilingual neural machine translation. In ACL.\nShuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che.\n2019. Generating natural language adversarial ex-\namples through probability weighted word saliency.\nIn ACL.\nBoxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan,\nRuoxi Jia, Bo Li, and Jingjing Liu. 2021a. Infobert:\nImproving robustness of language models from an\ninformation theoretic perspective. In ICLR.\nDong Wang, Ning Ding, Piji Li, and Haitao Zheng.\n2021b. CLINE: Contrastive learning with semantic\nnegative examples for natural language understand-\ning. In ACL.\nXiaosen Wang, Yichen Yang, Yihe Deng, and Kun He.\n2021c. Adversarial training with fast gradient pro-\njection method against synonym substitution based\ntext attacks. In AAAI.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiÔ¨Åcation. arXiv preprint arXiv:1509.01626.\nYi Zhou, Xiaoqing Zheng, Cho-Jui Hsieh, Kai-Wei\nChang, and Xuanjing Huang. 2021. Defense against\nsynonym substitution-based adversarial attacks via\nDirichlet neighborhood ensemble. In ACL.\nChen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Gold-\nstein, and Jingjing Liu. 2020. Freelb: Enhanced ad-\nversarial training for natural language understanding.\nIn ICLR.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In ICCV.\nA Geometry Attack for Contrastive Loss\nAlgorithm 1 is the pseudocode of our Geometry\nAttack for contrastive loss. Refer to Section 3.3 for\nmore details.\nB Datasets\nDataset Labels Avg Len Train Test\nAG‚Äôs News4 44 120 K 7.6K\nIMDB 2 292 25 K 25K\nDBPedia 14 67 560 K 70K\nYelp 2 177 560 K 38K\nTable 5: Statistics of the datasets.\nThe statistics of each dataset are shown in Ta-\nble 5. In our work, the maximum sequence length\nis set to 128 for AG‚Äôs News and DBpedia, 256\nfor Yelp, and 512 for IMDB. To save time during\nevaluating the model robustness against attacks,\nwe randomly select a part of the test examples in\neach dataset for evaluation. SpeciÔ¨Åcally, we select\n1,000 samples from IMDB, 2,000 samples from\nYelp, and 5,000 samples from DBpedia. We use\nall 7,600 samples from the AG‚Äôs News test set for\nevaluation.\nAG‚Äôs News3 Topic classiÔ¨Åcation dataset with four\ntypes of news articles: World, Sports, Business and\nScience/Technology.\nIMDB (Maas et al., 2011) Binary sentiment clas-\nsiÔ¨Åcation dataset on positive and negative movie\nreviews.\nYelp Yelp review dataset for binary sentiment clas-\nsiÔ¨Åcation. Following Zhang et al. (2015), reviews\nwith star 1 and 2 are considered negative, and re-\nviews with star 3 and 4 are considered positive.\nDBpedia (Zhang et al., 2015) Topic classiÔ¨Åcation\ndataset with 14 non-overlapping classes. Both con-\ntent and title Ô¨Åelds are used in our work.\nC Effect of Queue Size\nWe conduct additional experiments to study the\neffect of queue size. We use a queue size of\n8192, 16384, 32768, and 65536 under the setting\nof ADCL+FT for the AG‚Äôs News dataset. As is\nshown in Table 6, a larger queue size generally\nhelps improve the model robustness. However, we\nalso notice that when the queue size is too large\n3http://groups.di.unipi.it/~gulli/AG_\ncorpus_of_news_articles.html\nQueue Size Original Acc. (%) Success (%) Replaced (%)\nVanilla 94.2 86 .2 18 .6\n8192 94.4 77 .8 18 .9\n16384 94.3 76 .9 18 .7\n32768 94.3 76.5 19.1\n65536 94.4 76 .7 19 .3\nTable 6: Effect of queue size. We use the Geometry\nAttack to evaluate the robustness of each model. The\nFT model is Ô¨Ånetuned without contrastive learning.\n(65536), the model robustness starts to decrease.\nWe argue that this is because a too large queue size\nresults in less frequent queue updates, which makes\nthe vectors in the queue stale.\nD Speed of Different Attacks\nWe show in Table 7 the average number of seconds\neach attack needs for one example. We obtain\nthe average time by attacking 1000 examples and\nthen taking the average. We can observe that the\nGeometry attack is at least four times faster than\nTextFooler, and 4 to 10 times faster than PWWS\nand BAE-R.\nAttack AG‚Äôs News IMDB DBpedia Yelp\nGeometry 0.44 2 .02 0 .69 1 .16\nTextFooler 2.48 8 .69 2 .89 4 .86\nPWWS 6.29 21 .86 2 .52 10 .27\nBAE-R 5.37 24 .10 7 .74 16 .03\nTable 7: Average number of seconds each attack needs\nfor an example.\nE Adversarial Training with\nPre-generated Examples\nWe compare two different methods for adversarial\ntraining:\n‚Ä¢ Pre-generated We pre-generate for each ex-\nample in the training set an adversarial exam-\nple. We then augment the original training\nset with the adversarial examples. Finally, the\nmodel is Ô¨Ånetuned on the augmented dataset.\n‚Ä¢ On-the-Ô¨Çy This setting is the same as ADV\nin Table 1, where we generate adverarial ex-\namples on the Ô¨Çy for each mini-batch during\ntraining.\nTable 8 shows the results on the AG‚Äôs News\ndataset. We can see that on all four attacks, adver-\nsarial training with on-the-Ô¨Çy generated adversarial\nAlgorithm 1 Geometry Attack for Contrastive Loss\n1: Input: Example Xi = {w1,w2,...,w L}, encoder f and MLP g\n2: Output: Adversarial example X‚Ä≤\ni\n3: Initialize zi ‚Üêg(f(Xi))\n4: for iter = 1 to N do\n5: calculate ‚Ñìi using Equation 1\n6: vzi ‚Üê‚àázi‚Ñìi\n7: E ‚ÜêBertEmbeddings(X‚Ä≤\ni) ={e1,e2,..., eL}\n8: G ‚Üê‚àáE‚Ñìi = {g1,g2,..., gL}\n9: t‚Üêarg maxt ||gt||\n10: C ‚ÜêBertForMaskedLM({w1,¬∑¬∑¬∑ ,wt‚àí1,[MASK],wt+1,¬∑¬∑¬∑ ,wL})\n11: C ‚ÜêFilter(C) // construct candidates set C = {wt1 ,wt2 ,¬∑¬∑¬∑ ,wtT }; Ô¨Ålter using counter-Ô¨Åtted\nembeddings\n12: for each wtj ‚ààC,1 ‚â§j ‚â§T do\n13: Xij ‚Üê{w1,¬∑¬∑¬∑ ,wt‚àí1,wtj ,wt+1,¬∑¬∑¬∑ ,wL}\n14: zij ‚Üêg(f(Xij ))\n15: rij ‚Üêzij ‚àízi\n16: pij ‚Üê\nrij ¬∑vzi\n||vzi||\n17: end for\n18: m‚Üêarg maxj ||pij ||\n19: Xi ‚ÜêXim\n20: zi ‚Üêzim\n21: end for\n22: X‚Ä≤\ni ‚ÜêXi\n23: return Xi\nDataset Success Rate / Replaced (%)\nGeometry TextFooler PWWS BAE-R\nPre-generated 55.3/17.1 59.4/22.6 42.0/17.4 16.5/7.3\nOn-the-Ô¨Çy 20.7/20.5 25.1/29.3 26.1/22.3 10.7/7.7\nTable 8: Comparison between adversarial training with\npre-generated adversarial examples and on-the-Ô¨Çy gen-\nerated adversarial examples.\nexamples gives higher robustness than adversarial\ntraining with pre-generated adversarial examples.\nF Implementation Details\nIn our paper, we use PyTorch Lightning4 and Hug-\ngingFace Transformers 5 in our implementation.\nWe use BERT as the encoder f(¬∑), and the rep-\nresentation of the [CLS] symbol in the last layer\nis used for h. g(¬∑) is a two-layer MLP, of which\nthe output size cis 128. g(¬∑) uses Tanh as activa-\ntion function in the output layer. We use FP16 in\ntraining step to reduce GPU memory usage, and\n4https://www.pytorchlightning.ai/\n5https://huggingface.co/transformers/\nuse FusedAdam from DeepSpeed6 as the optimizer.\nWe enable DeepSpeed ZeRO Stage 2 to further\nspeed up training. We conduct all our experiments\non 8 RTX TITAN GPUs.\nContrastive learning For Geometry Attack for\ncontrastive loss, to reach a balance between attack\nsuccess rate and efÔ¨Åciency, the maximum num-\nber of iterations K is set to 10 for AG‚Äôs News,\nDBpedia, and Yelp, and 15 for IMDB dataset.\nWe do not perturb words that were already per-\nturbed in previous iterations. For an example Xi =\n{w1,w2,...,w L}, at most min{K,0.2¬∑L}words\ncan be perturbed. For each word wt,1 ‚â§t ‚â§L,\nthe upper limit of the candidate set size T is set to\n25. Due to the various maximum lengths in down-\nstream datasets and GPU memory limits, we use\ndifferent batch sizes for different datasets. During\ncontrastive learning, the batch size is set to 1024\nfor AG‚Äôs News and DBpedia, 448 for Yelp, and\n192 for IMDB.\nFine-tuning During Ô¨Ånetuning, we train the model\nfor two epochs for AG‚Äôs News and DBpedia, 3 for\n6https://www.deepspeed.ai/\nYelp, and 4 for IMDB. The learning rate is set to\n2e‚àí5 and is adjusted using linear scheduling.\nAdversarial training For adversarial training, the\nnumber of training epochs is set to 3 with an\nadditional Ô¨Årst epoch of Ô¨Ånetuning on clean ex-\namples. The adversarial examples are generated\non the Ô¨Çy in each batch during training. For the\nGeometry Attack in adversarial training, at most\nmin{K,0.4 ¬∑len(Xi)}words can be perturbed in\nan example. The upper limit of the candidate set\nsize is set to 50.\nBack Translation We use pretrained trans-\nlation models opus-mt-en-roa and\nopus-mt-roa-en from Helsinki-NLP to\ngenerate one translation for each example.\nG Hard Positive Examples from\nGeometry Attack for Contrastive Loss\nIn Table 9, we show hard positive examples gener-\nated by our Geometry Attack for contrastive loss\nfrom the AG‚Äôs News dataset.\nOriginalZurich employees plead guilty in probe new york\n(reuters) - two senior insurance underwriters at zurich\namerican insurance co pleaded guilty on tuesday to mis-\ndemeanors related to bid-rigging in the insurance mar-\nket.\nAdversarialZurich employees plead guilty in probe new york\n(reuters) - two senior insurance agents at zurich ameri-\ncan insurance co testiÔ¨Åed guilty on tuesday to violations\nrelated to bid-rigging in the insurance market.\nOriginalBlack watch troops move into position the Ô¨Årst units\nof a black watch battlegroup are due to arrive today\nin their new positions south of baghdad as tony blair\nindicated that more british troops may replace them in\nthe american - controlled zone before the end of the\nyear.\nAdversarialBlack watch troops move into place the Ô¨Årst units of a\nblack watch operation are due to arrive today in their new\npositions south of baghdad as tony blair indicated that\nmore british troops may replace them in the american -\ncontrolled zone before the end of the year.\nTable 9: Hard positive examples generated by Geome-\ntry Attack for contrastive loss. Blue words in the origi-\nnal examples are replaced by red words in the adversar-\nial examples.\nH Additional t-SNE plots\nWe give t-SNE plots of the vector representations\nof clean examples and adversarial examples from\nYelp, IMDB and DBpedia in Ô¨Åg. 4, Ô¨Åg. 5 and Ô¨Åg. 6,\nrespectively.\n0.0 0.2 0.4 0.6 0.8 1.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(a) NP + FTC\n0.0 0.2 0.4 0.6 0.8 1.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(b) ADCL + FTC\n0.0 0.2 0.4 0.6 0.8 1.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(c) NP + ADV\n0.0 0.2 0.4 0.6 0.8 1.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(d) ADCL + ADV\nFigure 4: t-SNE plot of the vector representations of\nclean examples and adversarial examples from the Yelp\ndataset.\n0.0 0.2 0.4 0.6 0.8 1.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(a) NP + FTC\n0.0 0.2 0.4 0.6 0.8 1.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(b) ADCL + FTC\n0.0 0.2 0.4 0.6 0.8 1.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(c) NP + ADV\n0.0 0.2 0.4 0.6 0.8 1.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(d) ADCL + ADV\nFigure 5: t-SNE plot of the vector representations\nof clean examples and adversarial examples from the\nIMDB dataset.\n0.0 0.2 0.4 0.6 0.8 1.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(a) NP + FTC\n0.0 0.2 0.4 0.6 0.8 1.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(b) ADCL + FTC\n0.0 0.2 0.4 0.6 0.8 1.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(c) NP + ADV\n0.0 0.2 0.4 0.6 0.8 1.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(d) ADCL + ADV\nFigure 6: t-SNE plot of the vector representations of\nclean examples and adversarial examples from the DB-\npedia dataset.",
  "topic": "Adversarial system",
  "concepts": [
    {
      "name": "Adversarial system",
      "score": 0.9523297548294067
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.8498256206512451
    },
    {
      "name": "Computer science",
      "score": 0.7987743616104126
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6732090711593628
    },
    {
      "name": "Machine learning",
      "score": 0.5034791827201843
    },
    {
      "name": "Training set",
      "score": 0.5002121925354004
    },
    {
      "name": "Language model",
      "score": 0.4592333734035492
    },
    {
      "name": "Word (group theory)",
      "score": 0.4219478964805603
    },
    {
      "name": "Natural language processing",
      "score": 0.38371551036834717
    },
    {
      "name": "Mathematics",
      "score": 0.10388344526290894
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ]
}