{
  "title": "Accelerated Multi-Modal MR Imaging with Transformers.",
  "url": "https://openalex.org/W3174392853",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5049444898",
      "name": "Chun-Mei Feng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5041127024",
      "name": "Yunlu Yan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100765638",
      "name": "Geng Chen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5010970485",
      "name": "Huazhu Fu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100359961",
      "name": "Yong Xu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5082634513",
      "name": "Ling Shao",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3175178093",
    "https://openalex.org/W2738302492",
    "https://openalex.org/W3007486523",
    "https://openalex.org/W2883105305",
    "https://openalex.org/W2039555200",
    "https://openalex.org/W3013621693",
    "https://openalex.org/W2964545333",
    "https://openalex.org/W2791621240",
    "https://openalex.org/W2442117232",
    "https://openalex.org/W2778924750",
    "https://openalex.org/W2134884567",
    "https://openalex.org/W3101461304",
    "https://openalex.org/W3102018640",
    "https://openalex.org/W783608400",
    "https://openalex.org/W2897112611",
    "https://openalex.org/W2552317450",
    "https://openalex.org/W3091727049",
    "https://openalex.org/W2003627360",
    "https://openalex.org/W2107906890",
    "https://openalex.org/W2808495892",
    "https://openalex.org/W2969785455",
    "https://openalex.org/W3047706622",
    "https://openalex.org/W3099155473",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W2902828227",
    "https://openalex.org/W2611467245",
    "https://openalex.org/W2338760177",
    "https://openalex.org/W2970936390",
    "https://openalex.org/W2900756484",
    "https://openalex.org/W2902719825",
    "https://openalex.org/W2963372104",
    "https://openalex.org/W3098848838",
    "https://openalex.org/W2709402577",
    "https://openalex.org/W3004715589",
    "https://openalex.org/W3035596626",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W2574952845",
    "https://openalex.org/W3175446764",
    "https://openalex.org/W2964293140",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3100730608",
    "https://openalex.org/W3101674918",
    "https://openalex.org/W3034223847",
    "https://openalex.org/W3135385363",
    "https://openalex.org/W3112965401",
    "https://openalex.org/W2979588445",
    "https://openalex.org/W2983227562",
    "https://openalex.org/W2594014149"
  ],
  "abstract": "Accelerating multi-modal magnetic resonance (MR) imaging is a new and effective solution for fast MR imaging, providing superior performance in restoring the target modality from its undersampled counterpart with guidance from an auxiliary modality. However, existing works simply introduce the auxiliary modality as prior information, lacking in-depth investigations on the potential mechanisms for fusing two modalities. Further, they usually rely on the convolutional neural networks (CNNs), which focus on local information and prevent them from fully capturing the long-distance dependencies of global knowledge. To this end, we propose a multi-modal transformer (MTrans), which is capable of transferring multi-scale features from the target modality to the auxiliary modality, for accelerated MR imaging. By restructuring the transformer architecture, our MTrans gains a powerful ability to capture deep multi-modal information. More specifically, the target modality and the auxiliary modality are first split into two branches and then fused using a multi-modal transformer module. This module is based on an improved multi-head attention mechanism, named the cross attention module, which absorbs features from the auxiliary modality that contribute to the target modality. Our framework provides two appealing benefits: (i) MTrans is the first attempt at using improved transformers for multi-modal MR imaging, affording more global information compared with CNN-based methods. (ii) A new cross attention module is proposed to exploit the useful information in each branch at different scales. It affords both distinct structural information and subtle pixel-level information, which supplement the target modality effectively.",
  "full_text": "IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2021 1\nMulti-Modal Transformer for Accelerated\nMR Imaging\nChun-Mei Feng, Yunlu Y an, Geng Chen, Y ong Xu,Senior Member, IEEE,\nYing Hu, Ling Shao,Fellow, IEEE, and Huazhu Fu,Senior Member, IEEE,\nAbstract— Accelerated multi-modal magnetic resonance\n(MR) imaging is a new and effective solution for fast MR\nimaging, providing superior performance in restoring the\ntarget modality from its undersampled counterpart with\nguidance from an auxiliary modality. However, existing\nworks simply combine the auxiliary modality as prior in-\nformation, lacking in-depth investigations on the poten-\ntial mechanisms for fusing different modalities. Further,\nthey usually rely on the convolutional neural networks\n(CNNs), which is limited by the intrinsic locality in capturing\nthe long-distance dependency. To this end, we propose\na multi-modal transformer (MTrans), which is capable of\ntransferring multi-scale features from the target modal-\nity to the auxiliary modality, for accelerated MR imaging.\nTo capture deep multi-modal information, our MTrans uti-\nlizes an improved multi-head attention mechanism, named\ncross attention module, which absorbs features from the\nauxiliary modality that contribute to the target modality.\nOur framework provides three appealing beneﬁts: (i) Our\nMTrans use an improved transformers for multi-modal MR\nimaging, affording more global information compared with\nexisting CNN-based methods. (ii) A new cross attention\nmodule is proposed to exploit the useful information in\neach modality at different scales. The small patch in the\ntarget modality aims to keep more ﬁne details, the large\npatch in the auxiliary modality aims to obtain high-level\ncontext features from the larger region and supplement\nthe target modality effectively. (iii) We evaluate MTrans with\nvarious accelerated multi-modal MR imaging tasks,e.g., MR\nimage reconstruction and super-resolution, where MTrans\noutperforms state-of-the-art methods on fastMRI and real-\nworld clinical datasets.\nIndex Terms— MR imaging, multi-modal, reconstruction,\nsuper-resolution.\nThis work was supported by the National Key R&D Program of China\n[2018AAA0100100] and AME Programmatic Fund (A20H4b0141).\nC.-M. Feng, Y . Y an, Y . Hu and Y . Xu are with the Shenzhen\nKey Laboratory of Visual Object Detection and Recognition, Harbin\nInstitute of Technology (Shenzhen), 518055, China. (Email: straw-\nberry.feng0304@gmail.com; yongxu@ymail.com).\nG. Chen is with National Engineering Laboratory for Integrated Aero-\nSpace-Ground-Ocean Big Data Application Technology, School of Com-\nputer Science and Engineering, Northwestern Polytechnical University,\nXi’an 710072, China (e-mail: geng.chen.cs@gmail.com).\nL. Shao is with the Terminus Group, China. (Email:\nling.shao@ieee.org).\nH. Fu is with the Institute of High Performance Computing, A*STAR,\nSingapore 138632. (E-mail: hzfu@ieee.org).\nCorresponding author:Y ong Xu and Huazhu Fu.\nC.-M. Feng and Y . Y an are contributed equally to this work.\n(a) T1WI (b) T2WI (d) FS-PDWI(c) PDWI \nFig. 1: Examples of four different modalities. Images (a) and\n(b) are a T1WI and T2WI from the same subject of a real-\nworld clinical dataset. Images (c) and (d) are a PDWI and FS-\nPDWI from the same subject of fastMRI. Different modalities\nfrom the same subject have inter-modality consistent structure.\nI. INTRODUCTION\nMagnetic resonance (MR) imaging is rapidly becoming the\ndominant technique for image-guided adaptive radiotherapy\nbecause it offers better soft tissue contrast than computed to-\nmography (CT), while avoiding radiation exposure. However,\ndue to the physical nature of the MR imaging procedure, the\nscanning time can take up to tens of minutes long, which\nseriously affects the patient experience and leads to high\ncosts. Therefore, accelerated MR imaging has become a hot\nresearch topic, where reconstructing images from undersam-\npled k-space measurements is a standard strategy. However,\nthe aliasing artifacts caused by insufﬁcient sampling often\naffect the clinical diagnosis. Therefore, the recovery of high-\nquality images from undersampled k-space measurements is\nthe ultimate goal when accelerating MR imaging. Currently,\nmainstream methods for this include MR image reconstruction\nand super-resolution (SR). The former aims to remove the\naliasing artifacts caused by undersampling [1]–[5], while the\nlatter enhances the image resolution [6]–[10].\nIn practice, according to different acquisition sequences, the\nscanner usually acquires MR images with different modalities\nat the same time to meet the medical diagnosis. For the same\nsubject, these modalities often have inter-modality consistent\ninformation and modality-speciﬁc information [11]. In addi-\ntion, the acquisition procedures of the different modalities\nvary. For example, T1 and T2 weighted images (T1WIs and\nT2WIs), as well as proton density and fat-suppressed proton\ndensity weighted images (PDWIs and FS-PDWIs), are two\npairs of images with complementary structures. As shown\nin Fig. 1, (a) and (b) are a pair of T1W and T2W brain MR\nimages from the same subject of a real-world clinical dataset,\narXiv:2106.14248v3  [eess.IV]  11 May 2022\n2 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2021\nwhere (a) provides morphological and structural information,\nand (b) shows edema and inﬂammation. Following [12], we\nﬁlter out pairs of PDW and FS-PDW images from fastMRI\n(currently the largest available database for raw MR images),\nas shown in Fig.1 (c) and (d). PDWIs provide structural infor-\nmation for articular cartilage, while FS-PDWIs can inhibit fat\nsignals and highlight the structural contrast between cartilage\nligaments and other tissues [13]. Generally, due to the physical\ncharacteristics of the MR imaging, T1WIs are more easily\nacquired than T2WIs since they require a shorter repetition\ntime (TR) and echo time (TE) [11]. Especially in the same\nimaging sequence, the acquisition time of T2WI is usually\nlonger than that of T1WI due to the longer TR time of T2,\ne.g., the acquisition time of T2SE is longer than T1SE [11].\nSimilarly, PDWIs require a shorter scan time than FS-PDWIs.\nIn clinical settings, since different modalities can provide\ndifferent information, we usually acquire multiple modalities\nsimultaneously to facilitate the comprehensive diagnosis of\ndisease. Therefore, we can use relatively easy-to-obtain modal-\nities as supplementary information to guide and accelerate\ntarget modalities that are acquired with slower imaging speeds.\nTo this end, the overall acquisition time and difﬁculty will\nbe greatly reduced, which is the practical signiﬁcance of our\nwork.\nFrom the auxiliary modality (with faster imaging speed)\nto help the target modality (with slower imaging speed) to\nobtain a high-quality image is actually a process of accelerated\nimaging, which has been veriﬁed by the previous works [11],\n[14]–[17]. For example, compressed sensing (CS), Bayesian\nlearning, dictionary learning, and graph representation theory,\nhave been employed to accelerate multi-modal MR imag-\ning [18]–[20]. Bilgic et al. introduced the Joint VC (JVC)\ntechnique to the GRAPPA framework for multi-modal MRI\nreconstruction [21]. Gong et al. used the shareable information\namong multi-modal images to accelerate MR imaging [22].\nMore recently, deep learning has become the focus of multi-\nmodal MR imaging studies [14], [23]. For example, Dar et\nal. added an auxiliary modality as prior information into the\ngenerator of a generative adversarial network (GAN) [14].\nLiu et al. used an iterative network to make the shareable\ninformation among multi-modal images for accelerated MR\nimaging [24]. Zhang et al. applied the temporal feature fusion\nblock based on ADMM to achieve multi-modal MRI recon-\nstruction [25]. Lyu et al. concatenated the two modalities at\nthe feature level of smaller size [15]. In addition, different MR\nimage modalities have modality-speciﬁc appearances under\ndifferent intensity distributions. Thus, how to effectively fuse\nthe two modalities is an inherent problem in multi-modal MR\nimaging, which needs to be resolved.\nOn the other hand, convolutional neural networks (CNNs)\nstruggle to fully capture global knowledge due to its intrinsic\nlocality of convolution operations, while transformers can\nlearn global information by modeling long-range dependency.\nBeneﬁting from this, transformers have recently achieved\nstate-of-the-art performance on a variety of computer vision\ntasks [26]. For example, the Vision Transformer (ViT) divides\nimages into small patches and uses a transformer to model the\ncorrelation between them as sequences, achieving satisfactory\nresults in image classiﬁcation [27]. The Detection Transformer\n(DETR) formulates target detection as an ensemble prediction\ntask with the help of a transformer [28]. Transformers have\nalso been used in medical imaging tasks. For example, trans-\nformers incorporated into UNet have been employed for med-\nical image segmentation [29], [30]. Korkmaz et al. used the\ncross attention module to capture interactions between latent\nvariables and image features with a deep adversarial network\nto solve the unsupervised MRI reconstruction problem [31].\nFeng et al. applied the task transform module, which is evolved\nfrom self-attention module of CNN, to transmit and share the\nrepresentations between multiple tasks [32].\nAlthough transformers have been applied to MR image\nreconstruction, these efforts are focused on a single modality\nin the basic transformer framework, ignoring the correlation\nbetween different modalities, especially in different scales.\nTo this end, in this work, we investigate how to design a\npowerful transformer model capable of learning multi-modal\nrepresentations to enhance various accelerated MR imaging\ntasks. We propose a multi-modal transformer (MTrans) to fuse\nthe informative features from MR imaging scans of different\nmodalities based on a multi-modal transformer. Our method\nutilizes multi-scale patches generated by the two-branch trans-\nformer to represent different modalities, and merges them to\ncomplement each other. Another key contribution of our work\nis to develop a feature fusion strategy for multi-modal MR\nimaging transformers, which, to the best of our knowledge, has\nnot yet been investigated. This is achieved with our effective\nmulti-modal cross attention modules, each of which takes the\nfeatures from the other branch as keys and values, employing\nthen for effectively querying, and obtain useful information\nfrom the other modality. In addition, the multi-scale patches\nfor two branches can capture high-level context features and\nlocal details to complement each other.\nOverall, the main contributions of our work are as follows:\n• We propose a novel transformer architecture, named\nMTrans, to accelerate multi-modal MR imaging. Ben-\neﬁted from the advantage of transformer, our work is\nable to capture rich global knowledge compared with the\nexisting CNN-based methods 1.\n• We introduce a cross attention module to effectively ex-\ntract useful information in each branch and then combine\nthe features from multiple scales to afford both high-level\ncontext features from the larger region and local details\nto complement each other.\n• We evaluate our method on two fast MR imaging tasks,\ne.g., image reconstruction and SR, on fastMRI and a raw\nMR image datasets. The results show that our method\nis superior to other multi-modal MR imaging models in\nboth qualitative and quantitative evaluation.\nII. R ELATED WORK\nA. Deep Learning for Accelerated MR Imaging\nImage reconstruction and SR techniques can improve image\nquality without changing the MR image acquisition hard-\nware, therefore they have been widely used for accelerated\n1Code is available at: https://github.com/chunmeifeng/MTrans.\nFENG et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING 3\nMR imaging. Traditional technologies such as CS [33], low\nrank [34]–[37], and dictionary learning [38], [39] have made\nprogress in this task. More recently, deep learning has been\nwidely used. Compared with traditional algorithms, which rely\non the prior information of data, deep learning can make\nfull use of the inherent characteristics of images contained\nin a large amount of training data [40]. For example, Yang et\nal. proposed a model-based unrolling method by applying the\nalternating direction method of multipliers (ADMM) algorithm\nto optimize the architecture [41]. MoDL then introduced\nanother model-based unrolling method combining prior reg-\nularization [3]. More recently, end-to-end approaches have\nshown advantages in accelerated MR imaging [42]. Jin et\nal. applied UNet to capture spatial information for inverse\nproblems related to MR imaging [43]. A 3D CNN with a\nresidual architecture was used to generate high-quality MR\nimage scans of knees in [7]. Chen et al. recovered high-quality\nimage details from a densely connected SR network [44].\nZhu et al. effectively estimated the mappings by manifold ap-\nproximation (AUTOMAP) for MR image reconstruction [45].\nFor example, Kim et al. used auto-calibrated recurrent and\nscan-speciﬁc deep networks to accelerate MR imaging from\nk-space data [46], [47]. Han et al. used the domain adaptation\ntechniques to remove the artifacts from the undersampled\nimages [48]. Lee et al. [49] trained the magnitude and phase of\nMR image data separately and fused them to generate the out-\nput image. To address the shortcomings of CNNs in calculating\ncomplex MR image numbers, we recently proposed Dual-\nOctConv to deal with complex components at various spatial\nfrequencies for accelerated parallel MR imaging [50], [51].\nThis method not only considers the computational relationship\nbetween the real and imaginary parts, but also captures the\ncharacteristics of different spatial frequencies. Finally, inspired\nby the prominent use of GANs in natural image synthesis,\nmany works have used GANs with an adversarial and percep-\ntual loss to generate high-quality MR images [52]–[55]. The\ndata consistency layer plays an important role in MR imaging\nto keep the reconstructed image consistent with the original\nimage in the k-space [56], [57]. Additionally, hybrid domain\nlearning schemes have been used to recover data from both\nthe k-space and image domain [58]. However, these methods\nare all based on a single-modal CNN architecture. In contrast,\nour method is a multi-modal fusion approach based on the\ntransformer architecture.\nB. Multi-Modal Medical Image Representation\nMulti-modal fusion allows multiple modalities to be com-\nbined in a new space by taking advantage of complementarities\nbetween data, which is more robust than using any single\nmodality as input. Recently, multi-modal technology has also\nbeen used widely in medical imaging [11], [14]–[16], [23],\n[59]. For example, a hybrid-fusion network was designed for\nmulti-modal MR image synthesis [60]. Xiang and Dar et\nal. simply concatenated the two modalities as input to guide\nthe reconstruction and SR of the target modality [11], [14]. Sun\net al. sent the different modalities into the network together to\nrestore them simultaneously [23]. For the MR image SR task,\nLyu et al. concatenated the two modalities on the features of\nsmaller size [15], while Zheng et al. concatenated them on\nthe original image size [16]. Dai et al. used the transformer\nto establish long-range dependencies from the decomposed\nmulti-modal image patches for MRI classiﬁcation [61]. Li et\nal. integrated tensor modeling and deep learning for solv-\ning the multi-modal MR image processing problems [62].\nHowever, these existing multi-modal methods simply add the\nauxiliary modality as prior information for the target modality\nto improve the image quality; fusing the two modalities has\nnot been explored [11], [14], [15], [23].\nIII. O VERVIEW OF ACCELERATED MR IMAGING\nLet y represent the complex-valued, fully sampled k-space\nacquired from the MR image scanner. We can obtain the\ncorresponding fully sampled image by x = F−1(y), where\nF−1 is an inverse 2D fast Fourier transform (FFT). In clinical\npractice, as only the magnitude images are visible, hospitals\nusually retain these for medical diagnosis. However, in this\nwork, all data ( e.g., the zero-ﬁlled image for reconstruction\nand LR image for SR) are obtained from real MR image\nk-space data to explore the effectiveness of accelerated MR\nimaging. This is an important point that has been neglected by\ncurrent fast multi-modal MR imaging methods. In this work,\nwe consider two kinds of accelerate MR imaging techniques,\nincluding (i) reconstructing a clear image from an image\ncorrupted by aliasing artifacts (undersampled image) and (ii)\nrestoring an SR image from a degraded image.\nA. Accelerating MR Imaging by Reconstruction\nLet M be the binary mask operator. We can obtain the\nundersampled k-space data by ˆy = M⊙y, where ⊙denotes\nelement-wise multiplication. In this work, we use random\nmasks with 6 ×acceleration to select a subset of the k-space\npoints. Accordingly, the zero-ﬁlled image can be obtained by\nˆx = F−1(ˆy). Different from current efforts, which address\nthe task by directly restoring x′from ˆy or ˆx, we introduce an\nimage from an additional modality with the same structural\ninformation to restore the target modality.\nB. Accelerating MR Imaging by Super-Resolution\nThe training phases of previous MR image SR methods\nusually add Gaussian blurs to the downsampled amplitude\nimage to obtain an LR image [63]. However, simply reducing\nthe image size in the image domain contradicts the actual MR\nimage acquisition process. Following [6], we ﬁrst truncate\nthe outer part of the fully sampled k-space y by a desired\nfactor to degrade the resolution, and then apply Fto obtain\nthe degraded LR image ˜x. This better mimics the real image\nacquisition process and avoids checkerboard artifacts.\nIV. PROPOSED MTRANS ARCHITECTURE\nIn our MTrans, the image patches are processed into a\nseries of linearly embedded sequences to create a dual-branch\nstructure. As shown in Fig. 2, the overall architecture of\nour MTrans consists of three components. Speciﬁcally, two\n4 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2021\nHeadtar\nHeadaux\nTailtar\nTailaux\nMulti-Modal Transformer\nFlattened Features\nLinearly Flatten\nLinearly Flatten\nReshape\nReshape\nˆxtar\nxaux\nx′tar\nx′aux\nFtar\nFaux\nFig. 2: An illustration of the proposed MTrans framework. Our architecture consists of two branches, e.g., a target branch and an\nauxiliary branch. This in turn are each divided into three components: two heads for extracting modality-speciﬁc features of different scales\nfrom the multi-modal input images ( e.g., the fully sampled auxiliary modality image xaux of large patch size with target zero ﬁlling ˆxtar or\nthe LR image ˜xtar); a multi-modal transformerfor aggregating the different modalities; and two tails in the target modality for mapping\nthe features into restored images.\nheads are employed for extracting modality-speciﬁc features\nof different scales from the multi-modal input images ( e.g.,\na fully sampled auxiliary modality image of large size with\ntarget zero-ﬁlling or LR image of small size); a multi-modal\ntransformer is established for aggregating the different modal-\nities, where the module uses the feature of the current branch\nas the query to exchange information with the other branch;\nand two tails are used for mapping the features into restored\nimages. Note that, the inputs from different modalities are\ndivided into image patches of different sizes. This enables\nlocal details to be extracted, while also capturing high-level\ncontext features from larger regions to supplement the target\nmodality. The main goal of the multi-modal transformer is to\nintegrate multi-modal images at different scales. We will next\nintroduce our architecture in detail.\nA. Heads\nTo extract modality-speciﬁc features from different modal-\nities, two branches with different heads ( e.g., Headaux for the\nauxiliary modality and Head tar for the target modality) are\nused, each of which consists of three 3×3 convolutional layers.\nThe ground truth of the auxiliary modality xaux ∈R1×H×W\nis sent to Headaux to generate an auxiliary feature map Faux ∈\nRC×H×W , where C is the number of channels, and H and\nW are the height and weight of the feature maps. For the\nMR image reconstruction task, we send the zero-ﬁlled image\nˆxtar of the target modality to Head tar to generate a feature\nmap Ftar ∈RC×H×W , while the LR image ˜xtar is used to\ngenerate a feature map Ftar ∈RC×H\ns ×W\ns (sis the scale factor)\nfor the SR task.\nB. Multi-modal Transformer\nOur multi-modal transformer fuses the different modalities,\nas shown in Fig.3 (a), which employs two symmetric branches,\ne.g., a target branch and an auxiliary branch. To handle\n2D images, following [27], we split the features of the two\nmodalities Faux and Ftar into patches, which are regarded\nas sequences of ”words”. Speciﬁcally, we ﬁrst reshape the\nfeatures of the auxiliary modality Faux ∈RC×H×W into a\nsequence of patches Fp\nauxi ∈RP2×C,i = {1,...,N }, where\nN = HW\nP2 is the number of patches or the length of the\nsequence for the transformer, and P is the resolution of each\nimage patch. Similarly, we reshape the features of the target\nmodality Ftar into patches Fp\ntari ∈R( P\n2 )2×C,i = {1,...,N }.\nNote that the resolution of the target image patch is a quarter\nthe size of the auxiliary modality. We use the different-sized\nimage patches in the auxiliary and target modalities to produce\nstronger image features. Then, learnable position encodings\nEp\ntari ∈R( P\n2 )2×C for the target branch and Ep\nauxi ∈RP2×C\nfor the auxiliary modality are added to each branch to maintain\nthe position information of each image patch [27], [28]:\nz0\ntar =\n[\nEp\ntar1 + Fp\ntar1 ,Ep\ntar2 + Fp\ntar2 ,..., Ep\ntari + Fp\ntari\n]\n,\nz0\naux =\n[\nEp\naux1 + Fp\naux1 ,Ep\naux2 + Fp\naux2 ,..., Ep\nauxi + Fp\nauxi\n]\n,\n(1)\nwhere z0\ntar ∈R( P\n2 )2×C and z0\naux ∈RP2×C are the position-\nembedded patches of the target and auxiliary modality, which\nare sent to a series of cascaded cross transformer encoder\nmodules (see Fig. 3 (a)). Each cross transformer encoder con-\nsists of two components, e.g., a cross transformer encoder tar\nfor the target modality and cross transformer encoder aux\nfor the auxiliary modality. Note that encoder tar fuses the\nfeatures from the auxiliary modality, while encoder aux fuses\nthe features from the target modality. Such a cross pattern\nensures that each branch learns important information from\nthe other modality. The green arrows in Fig. 3 (a) correspond\nto information updating for the modality of the current branch,\nand the red arrows facilitate information exchange between the\ntwo modalities. We can formulate our multi-modal transformer\nas:\n[zN\ntar,zN\naux] =MN ([z0\ntar,z0\naux]), (2)\nwhere MN is the multi-modal transformer module consisting\nof the N-th cross transformer encoder, and zN\naux ∈RP2×C and\nzN\ntar ∈R( P\n2 )2×C are the output sequences of the multi-modal\ntransformer.\n1) Cross Transformer Encoder: Our cross transformer en-\ncoder aims to effectively fuse the two modalities. As shown\nin Fig. 3 (b), the position-embedded patches z0\ntar, z0\naux are\nﬁrst linearly projected ( LP) to align their dimensions, which\nFENG et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING 5\n...\n(a) (b) (c)\nCT EncodertarN\nCT Encodertar3\nCT Encodertar2\nCT Encodertar1\nCT EncoderauxN\nCT Encoderaux3\nCT Encoderaux2\nCT Encoderaux1\nTarget Branch Auxiliary Branch\nz0\ntar z0\naux\nzl\ntar zl\naux\nLP LP\nLN LN LN LN\nCross AttentionauxCross Attentiontar\nLN LN\nFFN FFN\nLP LP\nzlp\ntar\nz0\ntar\nz0\naux z0\ntar\nz0\naux\nzlp\naux\nzCA\ntar zCA\naux\nzN\ntar zN\naux\nQ K V V K Q\nSoftmax Softmax\nLP LP\nConcate Concate\nzsa\ntar zsaaux\ntar aux tar aux\nzCA\ntar zCAaux\nFig. 3: (a) Architecture of the multi-modal transformer for multi-modal feature fusion, which is a cascade of several cross\ntransformer encoder (CT Encoder) modules. Green arrows correspond to information updates while red arrows facilitate\ninformation exchange between the two modalities. (b) An illustration of the cross transformer encoder. Our cross transformer\nencoder consists of cross attention tar and cross attentionaux modules with different patch sizes, enabling it to produce stronger\nfeatures for the restoration of the target modality. The red arrows facilitate information exchange between the two modalities.\n(c) Cross attention module. The features of the auxiliary branch are aligned with the target branch of large patch size, while\nthe features of the target branch are aligned with the auxiliary branch of large patch size.\ncan be formulated as:\nzlp\ntar = LP(z0\ntar),zlp\ntar ∈RP2×C,\nzlp\naux = LP(z0\naux),zlp\naux ∈R( P\n2 )2×C,\n(3)\nwhere zlp\ntar and zlp\naux are the aligned features. These features\nare sent to the cross attention module and a layernorm to\nfuse the two modalities. Then, followed by [64], we use a\nfeed-forward network ( FFN), which consists of two linear\ntransformations with a ReLU activation in between, to project\nthe feature obtained by the cross attention to a larger space\nand extract the required information. This can be formulated\nas:\nzCA\ntar = CAtar\n(\nLN\n(\nzlp\ntar,z0\naux\n))\n+ zlp\ntar,\nzCA\naux = CAaux\n(\nLN\n(\nz0\ntar,zlp\naux\n))\n+ zlp\naux,\nzi\ntar = LP\n(\nFFN\n(\nLN\n(\nzCA\ntar\n))\n+ zCA\ntar\n)\n,\nzi\naux = LP\n(\nFFN\n(\nLN\n(\nzCA\naux\n))\n+ zCA\naux\n)\n,\n(4)\nwhere i = [1 ,2,...,N ], CAtar and CAaux are the cross\nattention modules for different modalities. LN denotes the\nlayer normalization that aims to normalize the hidden layer\nin the neural network to the standard normal distribution to\naccelerate training speed and convergence [64]. The output\nsequence features zi\naux and zi\ntar of the two branches are saved\nas the input of the next cross transformer encoder.\n2) Cross Attention Module:Our cross attention module is an\nimproved multi-head attention module which absorbs features\nfrom the auxiliary modality that contribute to the target modal-\nity. Speciﬁcally, in order to fuse the different modalities more\nefﬁciently and effectively, the features in the current branch\nserve as a query that interacts with the features from the other\nbranch through attention. Note that the query features have\nalready been dimensionally aligned with the features from\nthe other branch. In other words, the feature sizes in the two\nbranches are different. This allows our cross attention fusion\nmodule to learn both high-level context features and local\ndetails. An illustration of our symmetric cross attention module\nis shown in Fig.3 (c). For the target branch, we use the aligned\nfeatures zlp\ntar after layer normalization LN(zlp\ntar) ∈RP2×C as\nthe query (Q), and concatenate them with the features from the\nauxiliary branch LN(z0\naux) ∈RP2×C to serve as the key ( K)\nand value (V). Similarly, for the auxiliary branch, the aligned\nfeatures zlp\naux after layer normalization LN(zlp\naux) ∈R( P\n2 )2×C\nserve as Q, and these are concatenated with the features from\nthe target branch LN(z0\ntar) ∈R( P\n2 )2×C to serve as K and V.\nThen, the correspondence between the two modalities in each\nbranch can be found using the following bilinear model:\nzsa = softmax\n(\nQKT\n√\nD/h\n)\nV, (5)\nwhere zsa can be expressed as zsa\ntar for the target branch, and\nzsa\naux for the auxiliary branch, D is the embedding dimension\nof LN and h is the number of heads in the cross attention\nmechanism. Finally, the outputs zCA\ntar and zCA\naux of a cross\nattention module can be deﬁned as follows:\nzCA\ntar = LP(zsa\ntar + LN(zlp\ntar)),\nzCA\naux = LP(zsa\naux + LN(zlp\naux)).\n(6)\nTo qualitatively evaluate the mechanism of our cross at-\ntention module, we visualize the cross attention map of four\ncascaded cross transformer encoders in Fig. 4, in which the\n6 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2021\n(a) Input(b)Cross attention visualization from different stages\nFig. 4: Cross attention visualization from different stages,\nwhere the four bright spots indicate the areas that four heads\nfocus on. The network gradually focuses on the brain regions\nas the increase of network depth.\nfour bright spots indicate the areas focused by the four\nheads [65]. As can be observed, with the number of network\nlayers deepens, the attention of the network gradually moves\ntowards the middle (object) region. Speciﬁcally, attention maps\nfrom different stages focus on different regions of the image\n(e.g., the shallow stages mainly focus on the background\narea, and the deep stages mainly focuses on the object area).\nObviously, the information in the background area is useless\nfor the MR image reconstruction. The goal of the cross-\nattention module is to effectively integrate two modalities,\nthereby the clear structural information can be learned from the\nauxiliary branch to assist the reconstruction of target modality.\nThe attention of the network in this ﬁgure is gradually moving\ntowards the object region, which precisely indicates that the\ncross attention mechanism can push the network to learn\nclear structural details from the object region in the auxiliary\nmodality.\nC. Tails\nFinally, the outputs of the multi-modal transformer encoder\nzN\ntar and zN\naux are fed into two tail modules to predict the\nrestored images, each of which consists of a 1×1 convolutional\nlayers:\nx′\ntar = Tailtar\n(\nzN\ntar\n)\n, and x′\naux = Tailaux\n(\nzN\naux\n)\n. (7)\nFor the reconstruction task, each tail module consists of three\nconvolutional layers. Please note, for the SR task, we add a\nsub-pixel convolutional layer [66] to Tailtar to upscale the\noutput x′\ntar is an image of size H×W.\nD. Loss\nFollowing [67], [68], we simply use the L1 loss to evaluate\nour results:\nL= 1\nM\nM∑\nm=1\nα\nx′m\ntar − xm\ntar\n\n1\n+(1 −α)\nx′m\naux − xm\naux\n\n1\n, (8)\nwhere αweights the trade-off between the two modalities, and\nM is the number of training samples. Note that the loss on\nthe auxiliary modality is to encode the auxiliary modality and\nensure that the features of the auxiliary modality can be fully\nextracted.\nV. EXPERIMENTS\nIn this section, we ﬁrst introduce the datasets and baselines\nused in our experiments, followed by the implementation\ndetails. Then, we summarize and analyze the experimental\nresults. Finally, we conduct an ablation study to investigate\nthe effectiveness of our multi-modal strategies.\nA. Implementation Details\n1) Datasets: We use three raw MR image datasets to\nevaluate our method: (1) fastMRI [71] is the largest open-\naccess raw MR image dataset, ofﬁcially provided at https:\n//fastmri.med.nyu.edu/. Following [12], we ﬁlter out\n227 and 24 pairs of PDWI and FS-PDWI knee volumes\nfor training and validation. PDWIs are used to guide the\nrestoration of the FS-PDWI modality. (2)uiMRI was collected\nusing a 3T system (provided by United Imaging Healthcare\nuMR 790; informed written consent was obtained from all the\nsubjects, and all experiments were carried out in accordance\nwith the approved guidelines) with two different protocols\n(whole brain T1WI and T2WI k-space sampling) on 400\nsubjects. The slice thickness is 4 mm and matrix size is\n320×320×19. The uiMRI dataset is split subject-wise with\na ratio of 7:1:2 for the training/validation/test sets, where\nT1WI is the auxiliary modality and T2WI is the target modal-\nity. All the experimental datasets are aligned through afﬁne\nregistration before our experiments. (3) Multi-coil dataset\nwas collected using a clinical 3T Siemens Magnetom Skyra\nscanner, including 20 paired PD and PDFS knee subjects with\n15 coils and a matrix size of 320 ×320 ×20 [73]. For the\nfastMRI and uiMRI datasets, the model was trained on the\nsingle-coil magnitude images. Note that the undersampled data\nwas obtained on the original k-space domain. For the multi-\ncoil dataset, the model was trained on the multi-coil complex\nimages with real and imaginary channel inputs. For the ﬁnal\nsingle-channel reconstruction, the coil sensitivity maps derived\nfrom ESPIRiT are used for coil combination [74].\nFor the reconstruction task, we use the random and Eq-\nuispaced undersampling patterns with 4 ×, 6 × and 8 × ac-\ncelerations. For the SR task, we use 4 × enlargement for\nboth fastMRI and uiMRI to evaluate the effectiveness of our\nmethod. The LR image was obtained by truncating the outer\npart of the fully sampled k-space y with the desired factor\nto degrade the resolution, and then applying Fto obtain the\ndegraded LR image ˜x. This process mimics the real image\nacquisition process and avoids checkerboard artifacts [6].\n2) Baselines and Training Details:We compare our model\nwith the following single- and multi-modal algorithms. Hy-\nperparameter optimization for each baseline in our experi-\nments is performed via cross-validation. Single-modal MR\nimage reconstruction/SR methods include: The most popular\nSR method (EDSR) [72], where Adam is adopted as the\noptimizer and the learning rate is initialized as 10−4; The\nend to end architecture, UNet, provided by fastMRI [71],\nwhich is retrained with SGD optimizer and a learning rate\nof 1e-4; A standard transformer framework that contains an\nencoder-decoder structure with multi-head attention for MR\nimage reconstruction/SR (TransMRI) [64], where the image\nFENG et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING 7\nTABLE I: Average (with standard deviation) reconstruction results, in terms of SSIM, PSNR and NMSE, under different\nundersampling patterns. The best and second-best results are marked in red and blue, respectively. P <0.001 was considered\nas a statistically signiﬁcant level.\nfastMRI Random 4×\nMethod SSIM↑ PSNR↑ NMSE↓ Pvalues\nZero-ﬁlling0.442±0.10 24.5±1.37 0.057±0.05<0.001/<0.001/<0.001\nLORAKS [69]0.530±0.07 26.0±1.03 0.050±0.02<0.001/<0.001/<0.001\nMoDL [3]0.576±0.04 27.7±0.96 0.047±0.01<0.001/<0.001/<0.001\nTransMRI [64]0.607±0.05 28.4±0.81 0.038±0.01<0.001/= 0.001/<0.001\nTransmed [61]0.609±0.06 28.4±0.99 0.040±0.03<0.001/<0.001/<0.001\nHyperDense-Net [70]0.600±0.04 28.3±1.00 0.038±0.02<0.001/<0.001/<0.001\nMDUNet [11]0.600±0.05 28.6±1.00 0.040±0.01<0.001/<0.001/<0.001\nrsGAN [14]0.608±0.04 28.9±1.03 0.033±0.02<0.001/<0.001/<0.001\nMTrans0.638±0.03 29.3±0.89 0.030±0.01 −\nuiMRI Random 6×\nMethod SSIM↑ PSNR↑ NMSE↓ Pvalues\nZero-ﬁlling0.700±0.09 27.0±1.70 0.067±0.010<0.001/<0.001/<0.001\nLORAKS [69]0.746±0.07 27.5±1.27 0.057±0.005<0.001/<0.001/<0.001\nMoDL [3]0.826±0.04 28.4±1.16 0.048±0.007<0.001/<0.001/<0.001\nTransMRI [64]0.861±0.02 28.9±1.32 0.044±0.005<0.001/<0.001/<0.001\nTransmed [61]0.879±0.04 29.3±0.99 0.040±0.004<0.001/= 0.002/<0.001\nHyperDense-Net [70]0.880±0.04 29.7±1.11 0.038±0.006<0.001/= 0.002/<0.001\nMDUNet [11]0.900±0.03 30.0±1.50 0.034±0.006<0.001/<0.001/<0.001\nrsGAN [14]0.908±0.03 30.7±1.42 0.028±0.005<0.001/<0.001/<0.001\nMTrans0.931±0.02 31.7±1.13 0.024±0.004 −\nfastMRI Equispaced 8×\nMethod SSIM↑ PSNR↑ NMSE↓ Pvalues\nZero-ﬁlling0.369±0.12 22.9±1.25 0.094±0.04<0.001/<0.001/<0.001\nLORAKS [69]0.443±0.05 24.7±1.05 0.099±0.05<0.001/<0.001/<0.001\nMoDL [3]0.437±0.04 24.4±0.86 0.083±0.04<0.001/<0.001/<0.001\nUNet [71]0.498±0.05 26.3±1.00 0.054±0.03<0.001/<0.001/<0.001\nTransMRI [64]0.507±0.05 26.8±0.83 0.048±0.02<0.001/<0.001/<0.001\nTransmed [61]0.518±0.06 27.1±0.78 0.057±0.03<0.001/= 0.02/<0.001\nHyperDense-Net [70]0.521±0.04 27.3±0.90 0.046±0.03<0.001/<0.001/<0.001\nMDUNet [11]0.544±0.05 27.9±0.86 0.046±0.02<0.001/<0.001/<0.001\nrsGAN [14]0.530±0.04 27.6±0.80 0.049±0.02<0.001/<0.001/<0.001\nMTrans0.563±0.04 28.4±0.81 0.043±0.02 −\nuiMRI Equispaced 8×\nMethod SSIM↑ PSNR↑ NMSE↓ Pvalues\nZero-ﬁlling0.560±0.10 24.8±1.76 0.087±0.010<0.001/<0.001/<0.001\nLORAKS [69]0.699±0.06 26.0±1.43 0.061±0.004<0.001/<0.001/<0.001\nMoDL [3]0.726±0.04 27.3±1.04 0.058±0.002<0.001/<0.001/<0.001\nUNet [71]0.713±0.06 27.0±1.39 0.055±0.006<0.001/<0.001/<0.001\nTransMRI [64]0.721±0.04 27.4±1.11 0.054±0.007<0.001/<0.001/<0.001\nTransmed [61]0.780±0.06 28.0±1.20 0.052±0.005<0.001/<0.001/<0.001\nHyperDense-Net [70]0.794±0.05 28.1±1.01 0.049±0.002<0.001/<0.001/<0.001\nMDUNet [11]0.820±0.03 28.3±1.07 0.048±0.005<0.001/<0.001/<0.001\nrsGAN [14]0.878±0.03 28.7±1.35 0.040±0.006<0.001/<0.001/<0.001\nMTrans0.910±0.02 30.8±1.07 0.032±0.003 −\nTABLE II: Average (with standard deviation) super-resolution results, in terms of SSIM, PSNR and NMSE, under different\ndatasets. The best and second-best results are marked in red and blue, respectively. P <0.001 was considered as a statistically\nsigniﬁcant level.\nfastMRI 4×\nMethod SSIM↑ PSNR↑ NMSE↓ Pvalues\nBicubic0.400±0.07 16.9±1.70 0.917±0.06<0.001/<0.001/<0.001\nEDSR [72]0.580±0.04 28.1±1.64 0.045±0.04<0.001/<0.001/<0.001\nTransMRI [64]0.600±0.03 29.9±1.44 0.048±0.02<0.001/<0.001/<0.001\nTransmed [61]0.673±0.05 29.4±1.35 0.053±0.05<0.001/<0.001/<0.001\nHyperDense-Net [70]0.640±0.04 30.4±1.62 0.042±0.04<0.001/<0.001/<0.001\nPRO [15]0.700±0.02 30.8±1.60 0.038±0.03<0.001/<0.001/<0.001\nMCSR [16]0.704±0.03 31.0±1.31 0.033±0.03<0.001/<0.001/<0.001\nMTrans0.719±0.02 31.9±1.19 0.031±0.02 −\nuiMRI 4×\nMethod SSIM↑ PSNR↑ NMSE↓ Pvalues\nBicubic0.526±0.05 8.3±1.20 0.900±0.030<0.001/<0.001/<0.001\nEDSR [72]0.941±0.07 32.3±1.04 0.012±0.004<0.001/<0.001/<0.001\nTransMRI [64]0.940±0.05 33.5±1.17 0.009±0.005<0.001/<0.001/<0.001\nTransmed [61]0.947±0.06 34.9±0.78 0.006±0.004<0.001/<0.001/<0.001\nHyperDense-Net [70]0.940±0.05 33.9±0.90 0.008±0.005<0.001/<0.001/<0.001\nPRO [15]0.945±0.07 34.4±0.97 0.007±0.003<0.001/<0.001/<0.001\nMCSR [16]0.944±0.07 34.8±0.97 0.006±0.003<0.001/<0.001/<0.001\nMTrans0.959±0.05 36.1±0.79 0.005±0.003 −\nTransMRIHyperDense-Net MTransUNetZero-fillingGround truth rsGANLORAKSMoDL MDUNetTransmed\nRandom 6×\nFig. 5: Comparison of different methods in terms of reconstruction results on the uiMRI dataset. Reconstructed images and\nerror maps are presented with corresponding quantitative measurements in PSNR/SSIM. The more obvious errors, the worse\nthe restoration results. The ﬁrst three methods represent the single-modal results, while the last three represent the multi-modal\nresults.\nhas been split into patches and treated the same way as tokens\n(words); MoDL [3], a model-based unrolled architecture for\ninverse problems, where the number of layers is set to 5 and\nnumber of iterations is set to 10; and LORAKS [69], a classic\ncompressed-sensing method, where the regularization parame-\nters is set to 10−10, the matrix rank is set to 50, and thek-space\nneighborhood is set to 5. Multi-modal reconstruction/SR meth-\nods include: A DenseUNet model for multi-modal MR image\nreconstruction, called MDUNet [11], where Adam is adopted\nas the optimizer and the learning rate is initialized as 10−4;\nHyperDense-Net [70], a multi-modal segmentation method\nwith feature fusion at intermediate layers; Transmed [61], a\ntransformer-based classiﬁcation method that tries to establish\nlong-range dependencies from the decomposed multi-modal\nimage patches, where SGD is adopted as the optimizer and\nthe learning rate is 10−3; A conditional GAN framework for\nmulti-modal MR image reconstruction, named rsGAN [14],\nwhere the hyperparameters λp is set to 100 and λperc is set\n8 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2021\nTransMRI\nEDSR\nPRO\n28.9/0.635 30.3/0.652\n28.0/0.585\nMTrans\n26.7/0.479 27.2/0.626\nMCSR\nLR\nHR\nHyperDense-Net\nTransmed\n27.0/0.529\n29.1/0.638\nFig. 6: Comparison of different methods in terms of SR\nresults on the fastMRI dataset. SR images and error maps\nare presented with corresponding quantitative measurements\nin PSNR/SSIM. The more obvious the errors, the worse the\nrestoration results. The ﬁrst two rows show the single-modal\nresults, while the last two rows provide the multi-modal\nresults.\nto 70; A non-progressive multi-modal MR image SR network\ncalled PRO [15], where the hyperparameters λ1, λ2, and λ3\nin the objective function of the generator are set to 10−1,\n10−2, and 10−11; and a deep CNN model for multi-modal\nMR image SR named MCSR [16], where the hyperparameters\nλ1, and λ2, are set to 0.6 and 0.4, respectively. For the\nmulti-coil data, we use traditional multi-coil imaging methods\n(SPIRiT [75] and L1-SPIRiT [76], where the the kernel size\nis set to 5 ×5.) as well as CNN-based methods (VN-Net [73]\nand MoDL [3] as the baselines. For some of the methods in\nthe baselines which have data consistency, we still retain their\noriginal structure and ensure that the training is performed\nunder optimal parameters.\nOur model is implemented in PyTorch with four NVIDIA\nTesla V100 GPUs and 32GB of memory per card. We use the\nSGD optimizer with a learning rate of 1e-4 and a mini-batch\nsize of 8, and train our model over 50 epochs. The parameter\nα is set to 0.9, the effectiveness of which is veriﬁed in the\nablation studies. The number of cross attention heads is set to\n4, and the number of channels C in the feature map generated\nby Head tar and Head aux is set to 16. We use N = 4 cross\ntransformer encoders in our network. For quantitative study,\npeak signal-to-noise ratio (PSNR), structural similarity index\n(SSIM) and normalized mean square error (NMSE) are used\nto evaluate the performance of our method [71]. The various\nforms of our multi-modal fusion strategies will be discussed\nin the ablation studies.\nB. Results on MR Image Reconstruction\n1) Quantitative Evaluation: We evaluate our reconstruction\nresults by computing the SSIM, PSNR, and NMSE between\nthe restored image and the fully sampled ground truth image.\nIn Table I, we show the results of our reconstruction over\nthe two raw MR image datasets. The ﬁrst row provides\nthe single-modal MR image reconstruction methods, while\nthe second row includes the CNN-based and transformer-\nbased multi-modal methods and our multi-modal transformer\nmodel. From the results, we ﬁnd that the PSNR and SSIM\nvalues of the single-modal UNet are relatively low, especially\non the uiMRI dataset. Similarly, the results of the standard\nsingle-modal transformer framework TransMRI are lower than\nother multi-modal methods. However, with the help of the\nauxiliary modality, the PSNR, SSIM, and NMSE of Transmed,\nHyperDense-Net, MDUNet, and rsGAN are improved to a\ncertain degree. Thus, we can conclude that the auxiliary\nmodality is complementary to the target modality. However,\nthese methods do not explore complementary information of\ndifferent modalities at multiple scales. Our MTrans achieves\n29.3 dB and 28.4 dB in PSNR on the fastMRI dataset, 31.7\ndB, and 30.8 dB on uiMRI.\n2) Qualitative Evaluation: For qualitative analysis, we pro-\nvide the reconstruction results on the uiMRI dataset in Fig. 5.\nThe more obvious the structure in the blue error map, the\nworse the restoration. As can be seen, reconstructions with\nzero-ﬁlling produce signiﬁcant aliasing artifacts and lose\nanatomical details. The ﬁrst two rows in Fig. 5 show the re-\nconstruction results of single-modal methods. Compared with\nzero-ﬁlling, single-modal methods can somewhat improve the\nreconstruction. However, multi-modal methods (the last two\nrows in Fig. 5) provide even further improvements, as veriﬁed\nby their corresponding error maps. Notably, our method yields\nthe lowest reconstruction error, better preserving important\nanatomical details.\nC. Results on MR image Super-Resolution\n1) Quantitative Evaluation:We evaluate our MTrans with the\ncompeting baseline methods on the SR task in Table II. This\ntable summarizes the 4 ×enlargement results of all methods\non the two raw MR image datasets. Similar to Table I, the ﬁrst\nrow shows the single-modal MR image SR methods, while the\nsecond row includes the CNN-based multi-modal methods as\nwell as our multi-modal transformer model. As can be seen\nfrom the table, the SR results are similar to those obtained\nfor reconstruction. Speciﬁcally, the single-modal architectures,\nwhether based on CNNs or transformers, are not as effective\nas the multi-modal methods. However, the multi-modal CNN\nmethods are not as effective as our model. For example, our\nmethod improves the PSNR result of the best multi-modal\nCNN method from 31.0 dB to 31.9 dB on the fastMRI dataset,\nwhile on the uiMRI dataset it increases it from 34.8 dB to\n36.1 dB. These results further conﬁrm the effectiveness of our\nmulti-modal approach.\nFENG et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING 9\nGround TruthSPIRiTL1-SPIRiTVN-NetMoDLMTrans\nRandom 4×29.2/0.69929.0/0.71332.9/0.81333.7/0.83536.9/0.925\nFig. 7: Comparison of different multi-coil reconstruction\nmethods. Reconstructed images and error maps are presented\nwith corresponding quantitative measurements in PSNR/SSIM.\nThe more obvious the errors, the worse the restoration results.\n2) Qualitative Evaluation: Fig. 6 shows the 4×enlargement\nof target modality images from fastMRI. The ﬁrst two rows\nshow the images restored by the single-modal methods, while\nthe last two provide show the results of the multi-modal meth-\nods. From this ﬁgure, we can see that the basic structure of the\nimage can be restored by single-modal methods ( e.g., EDSR\nand TransMRI). However, the methods based on multi-modal\nfusion signiﬁcantly improve the results, with smaller errors\nand clearer structures. In particular, our MTrans produces\nhigh-quality images with clear details, minimal checkerboard\neffects, and less structural loss. Further, our method can ef-\nfectively restore the entire structure of the knee. This superior\nperformance is attributed to the fact that our MTrans can\neffectively aggregate MR image information from different\nmodalities to obtain stronger features.\nD. Results on Multi-coil Data\nTo verify the effectiveness of our method on the multi-\ncoil data, we compare our method with various multi-coil\nreconstruction methods in Table III. As can be seen from\nthis table, the results of model-based methods, such as VN-\nNET and MoDL, are better than that of traditional methods\nSPIRiT and L1-SPIRiT. However, beneﬁting from powerful\nmulti-modal information, our method achieves the best PSNR\nand SSIM results. We also show the reconstruction images\nand corresponding error maps in Fig. 7. As can be seen from\nthis ﬁgure, our method achieves the lowest texture error and\nprovides results that are very close to the ground-truth. This\nproves that our method is also effective in multi-coil scenarios.\nTABLE III: Average (with standard deviation) multi-coil re-\nconstruction results, in terms of SSIM, PSNR and NMSE,\nunder different undersampling patterns. The best and second-\nbest results are marked in red and blue, respectively. P <\n0.001 was considered as a statistically signiﬁcant level.\nMulti-coil Reconstruction/Random 4×\nMethod SSIM↑ PSNR↑ NMSE↓ P values\nSPIRiT [75] 0.782±0.07 32.1±0.91 0.037±0.02 <0.001/<0.001/<0.001\nL1-SPIRiT [76]0.830±0.05 33.1±0.72 0.032±0.03 <0.001/<0.001/<0.001\nVN-Net [73] 0.900±0.05 36.0±0.55 0.030±0.03 <0.001/<0.001/<0.001\nMoDL [3] 0.902±0.04 36.2±0.77 0.028±0.02 <0.001/<0.001/<0.001\nMTrans 0.932±0.03 37.2±0.69 0.022±0.02 −\n29.1/0.632\n28.8/0.621\n28.9/0.614\nMTransETransMRIZero-filling ITransMRI\nGround-Truth\n31.2/0.708\n30.7/0.713\n29.9/0.644\nLR\nHR\nReconstructionSuper-Resolution\nFig. 8: Ablation study of the key components in our method,\nwhere ETransMRI and ITransMRI are two variations of our\nmodel. The ﬁrst two and last two rows are the reconstruction\nand SR results, respectively.\nE. Statistical Analysis\nAside from the quantitative evaluation of the two tasks, we\nalso performed a statistical signiﬁcance analysis to prove the\neffectiveness of our method. Here, following [77], we used\npaired Student’s t-test to evaluate the signiﬁcant difference\nbetween the two methods. As can be seen from the P values\nin Tables I and II, for the p-values that are greater than\n0.001, we report the speciﬁc values, while those less than\n0.001 will not be given speciﬁc values. Our results and those\ngiven by the comparison methods are statistically different in\nnearly all cases with p-values smaller than 0.001. For only\nfew cases as the reviewers have mentioned, we observed that\nthe p-values are actually a little bit large than 0.001, but still\nsmaller than 0.05. It is worth noting that our method still has\na statistically signiﬁcant improvement over the various multi-\nmodal baselines. This supports our previous discussion that\nour method can transfer multi-scale features from the target\nmodality to the auxiliary modality, resulting in higher quality\ntarget images, even compared to the currently available state-\nof-the-art methods.\nF . Ablation Studies\nIn this section, we ﬁrst investigate the effectiveness of our\napproach by comparing it with different fusion strategies.\nThen, we analyze the trade-off effects between the two modal-\nities.\n1) Comparison with Joint Reconstruction: Here, we test\nwhether our method is still effective when both the auxiliary\nmodality and the target modality need to be reconstructed.\nWe record the reconstruction results under the random un-\ndersampling pattern with 4 × acceleration in Fig. 9, where\n10 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2021\nTABLE IV: Ablation study (with standard deviation) of differ-\nent fusion strategies in our method regarding the reconstruction\nresults. P <0.001 was considered as a statistically signiﬁcant\nlevel. The paired Student’s t-test results show statistical dif-\nference (P <0.001).\nfastMRI uiMRI\nMethod SSIM↑ PSNR↑ NMSE↓ SSIM↑ PSNR↑ NMSE↓\nETransMRI0.619±0.03 29.0±1.10 0.037±0.01 0.915±0.02 30.5±1.43 0.030±0.004\nITransMRI0.622±0.03 29.2±0.90 0.038±0.02 0.920±0.03 31.0±1.21 0.028±0.005\nSTransMRI0.623±0.04 29.2±0.87 0.037±0.02 0.922±0.04 31.2±1.33 0.026±0.004\nMTrans0.628±0.03 29.3±0.89 0.035±0.01 0.931±0.02 31.7±1.33 0.024±0.004\nTABLE V: Ablation study (with standard deviation) of differ-\nent fusion strategies in our method regarding the SR results.\nThe paired Student’s t-test results show statistical difference\n(P <0.001).\nfastMRI uiMRI\nMethod SSIM↑ PSNR↑ NMSE↓ SSIM↑ PSNR↑ NMSE↓\nETransMRI0.668±0.02 31.0±1.21 0.037±0.02 0.953±0.05 35.6±1.09 0.006±0.002\nITransMRI0.698±0.03 31.0±1.10 0.034±0.03 0.942±0.06 35.8±1.02 0.006±0.004\nSTransMRI0.699±0.02 31.1±0.90 0.038±0.02 0.950±0.05 35.9±1.21 0.005±0.005\nMTrans0.719±0.02 31.9±1.19 0.031±0.02 0.959±0.05 36.1±0.99 0.005±0.003\nTABLE VI: Ablation study (with standard deviation) on the\ncross attention module and transformer.The paired Student’s\nt-test results show statistical difference ( P <0.001).\nReconstruction SR\nMethod SSIM↑ PSNR↑ NMSE↓ SSIM↑ PSNR↑ NMSE↓\nw/o-Trans 0.589±0.04 28.0±1.23 0.043±0.02 0.659±0.03 28.6±1.29 0.066±0.07\nw/o-CA 0.611±0.02 28.7±0.88 0.039±0.05 0.686±0.05 29.8±1.33 0.049±0.06\nMTrans0.628±0.03 29.3±0.89 0.035±0.01 0.719±0.02 31.9±1.19 0.031±0.02\nTABLE VII: Ablation study (with standard deviation) on the\nCNN-based attention schemes.\nReconstruction SR\nMethod SSIM↑ PSNR↑ NMSE↓ SSIM↑ PSNR↑ NMSE↓\nCNN-SA0.612±0.06 28.7±1.07 0.039±0.05 0.693±0.02 30.7±0.96 0.034±0.05\nCNN-CSA0.605±0.03 28.6±1.02 0.040±0.07 0.698±0.04 30.8±1.21 0.034±0.04\nMTrans0.628±0.03 29.3±0.89 0.035±0.01 0.719±0.02 31.9±1.19 0.031±0.02\nMTJoint represents our model, but the input target and auxil-\niary modalities are both undersampled images. As can be seen\nfrom this ﬁgure, without the multi-modal fusion mechanism,\nthe PSNR and SSIM results of single-modal TransMRI were\nthe lowest. When both two modalities are undersampled data,\nMTJoint obtain PSNR = 29.2 dB and SSIM = 0.630 on the\ntarget modality, PSNR = 29.3 dB and SSIM = 0.635 on the\nauxiliary modality, respectively. Note that this result is much\nhigher than the baselines in both single- and multi-modal,\nsee Table I. For the target modality, MTrans provides higher\nresults than MTJoin. This is mainly because the auxiliary\nmodality in MTrans are fully sampled and can provide detailed\nsupplementary information. However, the results of MTJoint\ndemonstrate that MTrans is a powerful method that can deal\nwith the case that both the auxiliary modality and the target\nmodality are undersampled.\n2) Comparison of Different Fusion Schemes:To evaluate the\neffectiveness of the key components in our model, we study\ntwo models in the ablation study. The ﬁrst is a multi-modal\ntransformer with early fusion, named ETransMRI, where the\ndifferent modalities are fused as input. ITransMRI is modiﬁed\nfrom our cross multi-modal transformer, TransMRI. Different\nfrom TransMRI, it performs single-scale (large-scale patches\n   \n \n0.580.60.620.640.66\nTransMRIMTJointMtrans\nSSIM\nTarAux 27.52828.52929.530\nTransMRIMTJointMtrans\nPSNR\nTarAux\nFig. 9: Comparison with the joint reconstruction. MTJoint rep-\nresents our model, but the input target and auxiliary modalities\nare both undersampled images, where Tar represents the target\nmodality and Aux represents the auxiliary modality.\nin the two branches) fusion with both the auxiliary and target\nmodalities. STransMRI is derived from our method but the\ntwo branches use the small-sized image patch. We summarize\nthe reconstruction and SR results on fastMRI and uiMRI\nin Table IV and Table V, respectively. As can be seen,\nETransMRI obtains the worst performance, which supports\nour conclusion that feature-level fusion can provide richer\nsupplementary information than simple fusion. Since early\nfusion does not learn information at the feature level, it is not\nthe most effective strategy for accelerating multi-modal MR\nimaging. In addition, because the multi-modal features fused\nby ITransMRI and STransMRI are with the same size, and\ndifferent-scale information cannot be extracted between the\ntwo modalities, the restoration results of ITransMRI are not the\nbest. The results of STransMRI are lower than MTransMRI.\nIn addition, the number of parameters of STransMRI is larger\nthan that of TransMRI. In contrast, our MTrans inherits the\nfusion information of different modalities at multiple scales,\nenhances the fusion features, and captures both high-level\ncontext features and local details. The results in this section\nalso demonstrate its strong ability to mine key information for\nguiding the target modality.\nTo qualitatively analyze the different fusion schemes, we\nshow visual results on fastMRI with error maps in Fig. 8.\nThe ﬁrst two are the reconstruction results, while the last two\nrows are the SR results. From this ﬁgure, we can see that\nboth ITransMRI and ETransMRI can effectively restore the\nimage. However, MTrans achieves the lowest texture error,\nand provides results that are almost as clear as the ground\ntruth. This indicates that our cross attention in the multi-\nmodal transformer is effective for accelerating multi-modal\nMR imaging in both reconstruction and super-resolution.\n3) Comparison with CNN-based Attention Schemes:To in-\nvestigate the limitations of CNN, we compared our MTrans\nwith self-attention (CNN-SA) and channel-spatial-attention\n(CNN-CSA) mechanisms, which are based on CNN [78].\nAs can be seen from Table VII, with the help of the self-\nattention and cross attention, the results of CNN-SA and CNN-\nCSA is still lower than that of MTrans in both reconstruction\nand SR. Because the transformer also uses the multi-head\nattention rather than calculating just one self-attention, this\nallows the model to deal with different presentation sub-\nspaces at different locations. Therefore, compared with CNN-\nSA and CNN-CSA, transformer-based method has stronger\nFENG et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING 11\n/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013\n/uni00000016/uni00000014/uni00000011/uni00000019/uni00000018\n/uni00000016/uni00000014/uni00000011/uni0000001a/uni00000013\n/uni00000016/uni00000014/uni00000011/uni0000001a/uni00000018\n/uni00000016/uni00000014/uni00000011/uni0000001b/uni00000013\n/uni00000016/uni00000014/uni00000011/uni0000001b/uni00000018\n/uni00000016/uni00000014/uni00000011/uni0000001c/uni00000013\n/uni00000033/uni00000036/uni00000031/uni00000035\n/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013\n/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000013\n/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000015\n/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000017\n/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000019\n/uni00000013/uni00000011/uni0000001a/uni00000013/uni0000001b\n/uni00000013/uni00000011/uni0000001a/uni00000014/uni00000013\n/uni00000036/uni00000036/uni0000002c/uni00000030\nα α\nFig. 10: Analysis of trade-off ( α) between the two modalities\nin terms of PSNR and SSIM. The greater the value of α, the\ngreater the inﬂuence of the target modality, and the smaller\nthe inﬂuence of the auxiliary modality.\ncapabilities in modeling long-range dependency.\nTABLE VIII: Ablation study (with standard deviation) on the\nvarious baselines with same loss. The best and second-best\nresults are marked in red and blue, respectively. The paired\nStudent’s t-test results show statistical difference (P <0.001).\nfastMRI Reconstruction/Random 4 ×\nMethod SSIM ↑ PSNR ↑ NMSE ↓\nLORAKS [69] 0.532 ±0.08 25.8 ±1.17 0.060 ±0.03\nMoDL [3] 0.580 ±0.05 27.6 ±0.89 0.049 ±0.02\nUNet [71] 0.565 ±0.06 28.2 ±1.15 0.046 ±0.02\nTransMRI [64] 0.607 ±0.05 28.4 ±0.89 0.038 ±0.01\nTransmed [61] 0.609 ±0.06 28.4 ±0.99 0.040 ±0.03\nHyperDense-Net [70] 0.600 ±0.04 28.3 ±1.00 0.038 ±0.02\nMDUNet [11] 0.592 ±0.06 28.4 ±1.12 0.053 ±0.02\nrsGAN [14] 0.620 ±0.07 29.0 ±1.12 0.035 ±0.03\nMTrans 0.638 ±0.03 29.3 ±0.89 0.030 ±0.01\nfastMRI SR 4 ×\nEDSR [72] 0.580 ±0.04 28.1 ±1.64 0.045 ±0.04\nTransMRI [64] 0.600 ±0.03 29.9 ±1.44 0.048 ±0.02\nTransmed [61] 0.673 ±0.05 29.4 ±1.35 0.053 ±0.05\nHyperDense-Net [70] 0.640 ±0.04 30.4 ±1.62 0.042 ±0.04\nPRO [15] 0.710 ±0.03 31.0 ±1.32 0.032 ±0.04\nMCSR [16] 0.690 ±0.02 30.5 ±1.22 0.040 ±0.04\nMTrans 0.719 ±0.02 31.9 ±1.19 0.031 ±0.02\n4) Baseline Comparisons with Same Loss:To demonstrate\nthe beneﬁts owe to our network architecture rather than the\nloss functions, we modify all the baselines to use the same\nloss function as our method. In Table VIII, we report the\nreconstruction results under the random sampling pattern with\n4×acceleration and SR results with 4 ×enlargement. For the\nmulti-modal classiﬁcation methods HyperDense-Net [70] and\nTransmed [61], we have already changed the classiﬁcation loss\nto L1 loss in Table I. For UNet [71], TransMRI [64], and\nEDSR [72], the original supervised loss is L1 loss. Therefore,\nwe retained the other methods with the same loss as ours.\nFor rsGAN [14] and PRO [15], we remove the discriminators\nincluding the perceptual loss and adversarial loss, and use\nL1 as the supervised loss. For LORAKS [69], MoDL [3],\nMDUNet [11], and MCSR [16], we change the MSE loss asL1\nloss. As can be seen from this table, without the discriminator,\nthe PSNR and SSIM results of both rsGAN [14] and PRO [15]\nare improved. This is consistent with the previous studies\nthat GAN-based structures tend to produce low PSNR but\ngood visual effects [79]. When we change the MSE loss of\nLORAKS [69], MoDL [3], MDUNet [11], and MCSR [16] to\nL1 loss, their results decreased slightly. This might be because\nMSE loss maximizes PSNR and has better convergence than\nL1 loss [72].\n5) Strength of the CA Module in Transformer:To understand\nthe strength of the cross attention (CA) module in Transformer,\nwe investigate ablation study on the heads and tails without\ntransformer (w/o-Trans), transformer without the CA module\n(w/o-CA). We summarize the reconstruction and SR results on\nfastMRI in Table VI. As can be seen, both the reconstruction\nand SR performance drops dramatically without transformer,\nbecause it cannot deal with long-range relationships well. The\nresults of w/o-CA and MTrans show that CA module plays an\nimportant role in the fusion between the different modalities.\n6) Parameter Analysis: Here, we analyze the number of\nparameters for each method. Speciﬁcally, the parameters of\nheads, tails, and the multi-modal transformer module in our\nmethod are 4.78KB, 0.02KB, and 189.2M, respectively. UNet\nrequires 31M parameters. HyperDense-Net and MDUNet only\nneeded 11M and 10M parameters, respectively. EDSR re-\nquires 43M parameters. MCSR is composed of two EDSR\nframeworks and requires 86M parameters. PRO adopts a\nprogressive model which requires 7M parameters and rsGAN\nrequires 62M parameters. Transmed is built on the transformer\nframework, therefore the number of parameters is larger than\nother methods, i.e., it requires 145M parameters. Although\nour MTrans built on transformer requires the largest number\nof parameters, the results in Table I, Table II, and Table III\nshow that our MTrans provides the best performance in both\nMRI reconstruction and SR. In the future, we will seek more\nefﬁcient solutions to reduce the memory cost of our model.\nIn our experiments, the number of cross-transformer encoders\nis set to 4. The results will drop slightly if the number of\ntransformer encoders or cross-attention heads is reduced. But\nthe results are still higher than all baseline methods. The\ncurrent number of encoders is the trade-off between network\nparameters and reconstruction accuracy.\n7) Effect of Trade-Off Between the Two Modalities:We next\ninvestigate the inﬂuence of α, which weights the trade-off\nbetween the two modalities. Speciﬁcally, the ratio α deter-\nmines the weights of both the target and auxiliary branches.\nThe greater the value of α, the greater the inﬂuence of the\ntarget modality, and the smaller the inﬂuence of the auxiliary\nmodality. We record the SR results of our method on fastMRI\nin Fig. 10. As can be seen, our model achieves the best\nPSNR and SSIM scores at α = 0.9. When α = 1, the PSNR\nperformance quickly degrades, while the SSIM degrades only\nslightly. This is likely because the auxiliary modality is fused\nwith the ground truth at multiple scales, so the weight of the\nauxiliary branch loss is not affected very much.\n8) Effect of the Number ofN: The values of N represent the\nnumber of patches that the input image has been cropped.\nTo verify whether the values of N are important to our\nmethod, we record the reconstruction results under random\nundersampling pattern with 4 ×acceleration and SR results\nwith 4×enlargement results of various N values on fastMRI\nin Fig. 11. As can be seen, the smaller the value of N, the\n12 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2021\n   \n \n00.20.40.60.8\nN=1600N=400N=100N=44\nSSIM\nReconstructionSR 272829303132\nN=1600N=400N=100N=44\nPSNR\nReconstructionSR\nFig. 11: Analysis of the effect of the N values on the fastMRI.\nsmaller PSNR and SSIM results. However, when N is greater\nthan 400, the PSNR and SSIM improvement is marginal. It\nshould be noted that the parameters of N = 1600will be much\nlarger than the parameters of N = 400, so we set N = 400\nin our experiment.\nVI. C ONCLUSION\nIn this work, we focus on exploring rich global knowledge\nin image for accelerated multi-modal MR imaging. For this\npurpose, we proposed a uniﬁed transformer framework, named\nMTrans, for accelerated multi-modal MR imaging, which can\nbe used for MR image reconstruction and SR to effectively\nrestore the target modality under the guidance of the auxiliary\nmodality. By fusing the feature maps of different modalities,\nthe proposed MTrans is helpful to learn the global infor-\nmation of multi-modal MR image, obtaining higher quality\nreconstructed images and signiﬁcantly reduce artifacts. In\nparticular, the proposed cross attention module can explore the\nfusion strategy in different scales, which provides both obtain\nhigh-level context features and local details. We conducted\nextensive experiments on the fastMRI and real-world clinical\ndatasets under different settings of undersampling patterns.\nThe results demonstrated our model against outperforms state-\nof-the-art methods in accelerated MR imaging. This work\nprovides promising guidelines for further research into multi-\nmodal MR imaging with transformers. Although our MTrans\nprovides excellent results on accelerated MR imaging, it\nstill has some shortcomings. For example, MTrans tends\nto require a large number of parameters than CNN-based\nmethods. Therefore, we will consider to reduce the number of\nparameters for our transformer-based framework in the future.\nREFERENCES\n[1] Y . Zhang, Z. Dong, P. Phillips, S. Wang, G. Ji, and J. Yang, “Exponen-\ntial wavelet iterative shrinkage thresholding algorithm for compressed\nsensing magnetic resonance imaging,” Information Sciences , vol. 322,\npp. 115–132, 2015.\n[2] Y . Chen, T. Xiao, C. Li, Q. Liu, and S. Wang, “Model-based con-\nvolutional de-aliasing network learning for parallel mr imaging,” in\nInternational Conference on Medical Image Computing and Computer-\nAssisted Intervention. Springer, 2019, pp. 30–38.\n[3] H. K. Aggarwal, M. P. Mani, and M. Jacob, “Modl: Model-based\ndeep learning architecture for inverse problems,” IEEE transactions on\nmedical imaging, vol. 38, no. 2, pp. 394–405, 2018.\n[4] G. Wang, J. C. Ye, and B. De Man, “Deep learning for\ntomographic image reconstruction,” Nature Machine Intelligence ,\nvol. 2, no. 12, pp. 737–748, dec 2020. [Online]. Available:\nhttp://www.nature.com/articles/s42256-020-00273-z\n[5] C.-M. Feng, Y . Yan, H. Fu, Y . Xu, and L. Shao, “Speciﬁcity-\npreserving federated learning for mr image reconstruction,” arXiv\npreprint arXiv:2112.05752, 2021.\n[6] Y . Chen, Y . Xie, Z. Zhou, F. Shi, A. G. Christodoulou, and D. Li, “Brain\nmri super resolution using 3d deep densely connected neural networks,”\nin 2018 IEEE 15th International Symposium on Biomedical Imaging\n(ISBI 2018). IEEE, 2018, pp. 739–742.\n[7] A. S. Chaudhari, Z. Fang, F. Kogan, J. Wood, K. J. Stevens, E. K.\nGibbons, J. H. Lee, G. E. Gold, and B. A. Hargreaves, “Super-\nresolution musculoskeletal mri using deep learning,” Magnetic reso-\nnance in medicine , vol. 80, no. 5, pp. 2139–2154, 2018.\n[8] S. McDonagh, B. Hou, A. Alansary, O. Oktay, K. Kamnitsas, M. Ruther-\nford, J. V . Hajnal, and B. Kainz, “Context-sensitive super-resolution for\nfast fetal magnetic resonance imaging,” in Molecular Imaging, Recon-\nstruction and Analysis of Moving Body Organs, and Stroke Imaging and\nTreatment. Springer, 2017, pp. 116–126.\n[9] C.-M. Feng, Y . Yan, C. Liu, H. Fu, Y . Xu, and L. Shao, “Exploring\nseparable attention for multi-contrast mr image super-resolution,” arXiv\npreprint arXiv:2109.01664, 2021.\n[10] C.-M. Feng, H. Fu, S. Yuan, and Y . Xu, “Multi-contrast mri super-\nresolution via a multi-stage integration network,” in International Con-\nference on Medical Image Computing and Computer-Assisted Interven-\ntion. Springer, 2021, pp. 140–149.\n[11] L. Xiang, Y . Chen, W. Chang, Y . Zhan, W. Lin, Q. Wang, and D. Shen,\n“Deep-learning-based multi-modal fusion for fast mr reconstruction,”\nIEEE Transactions on Biomedical Engineering, vol. 66, no. 7, pp. 2105–\n2114, 2018.\n[12] K. Xuan, S. Sun, Z. Xue, Q. Wang, and S. Liao, “Learning mri k-space\nsubsampling pattern using progressive weight pruning,” in International\nConference on Medical Image Computing and Computer-Assisted Inter-\nvention. Springer, 2020, pp. 178–187.\n[13] W. Chen, J. Zhao, Y . Wen, B. Xie, X. Zhou, L. Guo, L. Yang, J. Wang,\nY . Dai, and D. Zhou, “Accuracy of 3-t mri using susceptibility-weighted\nimaging to detect meniscal tears of the knee,” Knee Surgery, Sports\nTraumatology, Arthroscopy, vol. 23, no. 1, pp. 198–204, 2015.\n[14] S. U. Dar, M. Yurt, M. Shahdloo, M. E. Ildız, B. Tınaz, and T. C ¸ ukur,\n“Prior-guided image reconstruction for accelerated multi-contrast mri\nvia generative adversarial networks,” IEEE Journal of Selected Topics\nin Signal Processing , vol. 14, no. 6, pp. 1072–1087, 2020.\n[15] Q. Lyu, H. Shan, C. Steber, C. Helis, C. T. Whitlow, M. Chan, and\nG. Wang, “Multi-contrast super-resolution mri through a progressive\nnetwork,” IEEE Transactions on Medical Imaging , 2020.\n[16] K. Zeng, H. Zheng, C. Cai, Y . Yang, K. Zhang, and Z. Chen, “Simul-\ntaneous single-and multi-contrast super-resolution for brain mri images\nbased on a convolutional neural network,” Computers in biology and\nmedicine, vol. 99, pp. 133–141, 2018.\n[17] C.-M. Feng, H. Fu, T. Zhou, Y . Xu, L. Shao, and D. Zhang,\n“Multi-modal aggregation network for fast mr imaging,” arXiv preprint\narXiv:2110.08080, 2021.\n[18] B. Bilgic, V . K. Goyal, and E. Adalsteinsson, “Multi-contrast recon-\nstruction with bayesian compressed sensing,” Magnetic resonance in\nmedicine, vol. 66, no. 6, pp. 1601–1615, 2011.\n[19] P. Song, L. Weizman, J. F. Mota, Y . C. Eldar, and M. R. Rodrigues,\n“Coupled dictionary learning for multi-contrast mri reconstruction,”\nIEEE transactions on medical imaging , vol. 39, no. 3, pp. 621–633,\n2019.\n[20] Z. Lai, X. Qu, H. Lu, X. Peng, D. Guo, Y . Yang, G. Guo, and Z. Chen,\n“Sparse mri reconstruction using multi-contrast image guided graph\nrepresentation,” Magnetic resonance imaging, vol. 43, pp. 95–104, 2017.\n[21] B. Bilgic, T. H. Kim, C. Liao, M. K. Manhard, L. L. Wald, J. P.\nHaldar, and K. Setsompop, “Improving parallel imaging by jointly\nreconstructing multi-contrast data,” Magnetic resonance in medicine ,\nvol. 80, no. 2, pp. 619–632, 2018.\n[22] E. Gong, F. Huang, K. Ying, W. Wu, S. Wang, and C. Yuan, “Promise:\nparallel-imaging and compressed-sensing reconstruction of multicontrast\nimaging using sharable information,” Magnetic resonance in medicine ,\nvol. 73, no. 2, pp. 523–535, 2015.\n[23] L. Sun, Z. Fan, X. Fu, Y . Huang, X. Ding, and J. Paisley, “A deep\ninformation sharing network for multi-contrast compressed sensing mri\nreconstruction,”IEEE Transactions on Image Processing, vol. 28, no. 12,\npp. 6141–6153, 2019.\n[24] X. Liu, J. Wang, H. Sun, S. S. Chandra, S. Crozier, and F. Liu, “On the\nregularization of feature fusion and mapping for fast mr multi-contrast\nimaging via iterative networks,” Magnetic resonance imaging , vol. 77,\npp. 159–168, 2021.\n[25] J. Zhang, H. Zhang, C. Li, P. Spincemaille, M. Sabuncu, T. D. Nguyen,\nand Y . Wang, “Temporal feature fusion with sampling pattern optimiza-\ntion for multi-echo gradient echo acquisition and image reconstruction,”\narXiv preprint arXiv:2103.05878 , 2021.\nFENG et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING 13\n[26] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and\nM. Shah, “Transformers in Vision: A Survey,” arXiv, jan 2021.\n[Online]. Available: http://arxiv.org/abs/2101.01169\n[27] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929 , 2020.\n[28] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in\nEuropean Conference on Computer Vision . Springer, 2020, pp. 213–\n229.\n[29] J. Chen, Y . Lu, Q. Yu, X. Luo, E. Adeli, Y . Wang, L. Lu, A. L. Yuille, and\nY . Zhou, “Transunet: Transformers make strong encoders for medical\nimage segmentation,” arXiv preprint arXiv:2102.04306 , 2021.\n[30] W. Wang, C. Chen, M. Ding, J. Li, H. Yu, and S. Zha, “Transbts:\nMultimodal brain tumor segmentation using transformer,” arXiv preprint\narXiv:2103.04430, 2021.\n[31] Y . Korkmaz, S. U. Dar, M. Yurt, M. ¨Ozbey, and T. C ¸ ukur, “Unsupervised\nmri reconstruction via zero-shot learned adversarial transformers,” arXiv\npreprint arXiv:2105.08059, 2021.\n[32] C.-M. Feng, Y . Yan, H. Fu, L. Chen, and Y . Xu, “Task transformer\nnetwork for joint mri reconstruction and super-resolution,” in Interna-\ntional Conference on Medical Image Computing and Computer-Assisted\nIntervention. Springer, 2021, pp. 307–317.\n[33] J. P. Haldar, D. Hernando, and Z.-P. Liang, “Compressed-sensing mri\nwith random encoding,” IEEE transactions on Medical Imaging, vol. 30,\nno. 4, pp. 893–903, 2010.\n[34] Y . Shi, Y . Gao, Y . Zhang, J. Sun, X. Mou, and Z. Liang, “Spectral\nct reconstruction via low-rank representation and region-speciﬁc texture\npreserving markov random ﬁeld regularization,” IEEE Transactions on\nMedical Imaging, 2020.\n[35] A. Pramanik, H. Aggarwal, and M. Jacob, “Deep generalization of struc-\ntured low-rank algorithms (deep-slr),” IEEE Transactions on Medical\nImaging, 2020.\n[36] J. P. Haldar and J. Zhuo, “P-loraks: low-rank modeling of local k-\nspace neighborhoods with parallel imaging data,” Magnetic resonance\nin medicine, vol. 75, no. 4, pp. 1499–1514, 2016.\n[37] J. He, Q. Liu, A. G. Christodoulou, C. Ma, F. Lam, and Z.-P. Liang,\n“Accelerated high-dimensional mr imaging with sparse sampling using\nlow-rank tensors,” IEEE transactions on medical imaging, vol. 35, no. 9,\npp. 2119–2129, 2016.\n[38] K. K. Bhatia, A. N. Price, W. Shi, J. V . Hajnal, and D. Rueckert,\n“Super-resolution reconstruction of cardiac mri using coupled dictionary\nlearning,” in 2014 IEEE 11th International Symposium on Biomedical\nImaging (ISBI). IEEE, 2014, pp. 947–950.\n[39] Q. Liu, Q. Yang, H. Cheng, S. Wang, M. Zhang, and D. Liang,\n“Highly undersampled magnetic resonance imaging reconstruction using\nautoencoding priors,” Magnetic resonance in medicine , vol. 83, no. 1,\npp. 322–336, 2020.\n[40] S. Wang, Z. Su, L. Ying, X. Peng, S. Zhu, F. Liang, D. Feng, and\nD. Liang, “Accelerating magnetic resonance imaging via deep learning,”\nin 2016 IEEE 13th International Symposium on Biomedical Imaging\n(ISBI). IEEE, 2016, pp. 514–517.\n[41] Y . Yang, J. Sun, H. Li, and Z. Xu, “Admm-csnet: A deep learning\napproach for image compressive sensing,” IEEE Trans. Pattern Anal.\nMach. Intell., vol. 42, no. 3, pp. 521–538, 2020.\n[42] S. Wang, Z. Su, L. Ying, X. Peng, S. Zhu, F. Liang, D. Feng, and\nD. Liang, “Accelerating magnetic resonance imaging via deep learning,”\nIEEE International Symposium on Biomedical Imaging , pp. 514–517,\n2016.\n[43] K. H. Jin, M. T. McCann, E. Froustey, and M. Unser, “Deep convo-\nlutional neural network for inverse problems in imaging,” IEEE Trans.\nImage Process., vol. 26, no. 9, pp. 4509–4522, 2017.\n[44] Y . Chen, Y . Xie, Z. Zhou, F. Shi, A. G. Christodoulou, and D. Li, “Brain\nmri super resolution using 3d deep densely connected neural networks,”\nin 2018 IEEE 15th International Symposium on Biomedical Imaging\n(ISBI 2018). IEEE, 2018, pp. 739–742.\n[45] B. Zhu, J. Z. Liu, S. F. Cauley, B. R. Rosen, and M. S. Rosen, “Image\nreconstruction by domain-transform manifold learning,” Nature, vol.\n555, no. 7697, pp. 487–492, 2018.\n[46] T. H. Kim, P. Garg, and J. P. Haldar, “Loraki: Autocalibrated recurrent\nneural networks for autoregressive mri reconstruction in k-space,” arXiv\npreprint arXiv:1904.09390, 2019.\n[47] M. Akc ¸akaya, S. Moeller, S. Weing ¨artner, and K. U ˘gurbil, “Scan-\nspeciﬁc robust artiﬁcial-neural-networks for k-space interpolation (raki)\nreconstruction: Database-free deep learning for fast imaging,” Magnetic\nresonance in medicine , vol. 81, no. 1, pp. 439–453, 2019.\n[48] Y . Han, J. Yoo, H. H. Kim, H. J. Shin, K. Sung, and J. C. Ye,\n“Deep learning with domain adaptation for accelerated projection-\nreconstruction mr,” Magnetic resonance in medicine , vol. 80, no. 3, pp.\n1189–1205, 2018.\n[49] D. Lee, J. Yoo, S. Tak, and J. C. Ye, “Deep residual learning for ac-\ncelerated mri using magnitude and phase networks,” IEEE Transactions\non Biomedical Engineering , vol. 65, no. 9, pp. 1985–1995, 2018.\n[50] C.-M. Feng, Z. Yang, G. Chen, Y . Xu, and L. Shao, “Dual-octave convo-\nlution for accelerated parallel mr image reconstruction,” in Proceedings\nof the 35th AAAI Conference on Artiﬁcial Intelligence (AAAI) , 2021.\n[51] C.-M. Feng, Z. Yang, H. Fu, Y . Xu, J. Yang, and L. Shao, “Donet: Dual-\noctave network for fast mr image reconstruction,” in IEEE Transactions\non Neural Networks and Learning Systems , 2021.\n[52] T. M. Quan, T. Nguyen-Duc, and W.-K. Jeong, “Compressed sensing\nmri reconstruction using a generative adversarial network with a cyclic\nloss,” IEEE Trans. on Medical Imaging , vol. 37, no. 6, pp. 1488–1497,\n2018.\n[53] G. Yang, S. Yu, H. Dong, G. Slabaugh, P. L. Dragotti, X. Ye, F. Liu,\nS. Arridge, J. Keegan, Y . Guoet al., “Dagan: Deep de-aliasing generative\nadversarial networks for fast compressed sensing mri reconstruction,”\nIEEE Trans. on Medical Imaging , vol. 37, no. 6, pp. 1310–1321, 2017.\n[54] M. Mardani, E. Gong, J. Y . Cheng, S. S. Vasanawala, G. Zaharchuk,\nL. Xing, and J. M. Pauly, “Deep generative adversarial neural networks\nfor compressive sensing mri,” IEEE Trans. on Medical Imaging, vol. 38,\nno. 1, pp. 167–179, 2018.\n[55] Q. Lyu, C. You, H. Shan, and G. Wang, “Super-resolution mri through\ndeep learning,” arXiv preprint arXiv:1810.06776 , 2018.\n[56] J. Schlemper, J. Caballero, J. V . Hajnal, A. N. Price, and D. Rueckert, “A\ndeep cascade of convolutional neural networks for dynamic mr image\nreconstruction,” IEEE Trans. on Medical Imaging , vol. 37, no. 2, pp.\n491–503, 2017.\n[57] H. Zheng, F. Fang, and G. Zhang, “Cascaded dilated dense network\nwith two-step data consistency for mri reconstruction,” in Adv. Neural\nInform. Process. Syst. , 2019, pp. 1744–1754.\n[58] T. Eo, Y . Jun, T. Kim, J. Jang, H.-J. Lee, and D. Hwang, “Kiki-net: cross-\ndomain convolutional neural networks for reconstructing undersampled\nmagnetic resonance images,” Magnetic resonance in medicine , vol. 80,\nno. 5, pp. 2188–2201, 2018.\n[59] Y . Zhou, Y . Li, Z. Zhang, Y . Wang, A. Wang, E. K. Fishman, A. L.\nYuille, and S. Park, “Hyper-pairing network for multi-phase pancreatic\nductal adenocarcinoma segmentation,” in International conference on\nmedical image computing and computer-assisted intervention. Springer,\n2019, pp. 155–163.\n[60] T. Zhou, H. Fu, G. Chen, J. Shen, and L. Shao, “Hi-net: hybrid-fusion\nnetwork for multi-modal mr image synthesis,” IEEE Trans. on Medical\nImaging, 2020.\n[61] Y . Dai, Y . Gao, and F. Liu, “Transmed: Transformers advance multi-\nmodal medical image classiﬁcation,” Diagnostics, vol. 11, no. 8, p. 1384,\n2021.\n[62] W. Li, Z. Meng, R. Liu, Z.-P. Liang, and Y . Li, “Multimodal image\nfusion integrating tensor modeling and deep learning,”ISMRM, no. 2426,\n2021.\n[63] C.-H. Pham, A. Ducournau, R. Fablet, and F. Rousseau, “Brain mri\nsuper-resolution using deep 3d convolutional networks,” in 2017 IEEE\n14th International Symposium on Biomedical Imaging (ISBI 2017) .\nIEEE, 2017, pp. 197–200.\n[64] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin neural information processing systems , 2017, pp. 5998–6008.\n[65] W. Sun, J. Zhang, Z. Liu, Y . Zhong, and N. Barnes, “Getam: Gradient-\nweighted element-wise transformer attention map for weakly-supervised\nsemantic segmentation,” arXiv preprint arXiv:2112.02841 , 2021.\n[66] Y . Zhang, Y . Tian, Y . Kong, B. Zhong, and Y . Fu, “Residual dense net-\nwork for image super-resolution,” inProceedings of the IEEE conference\non computer vision and pattern recognition , 2018, pp. 2472–2481.\n[67] A. Sriram, J. Zbontar, T. Murrell, C. L. Zitnick, A. Defazio, and D. K.\nSodickson, “Grappanet: Combining parallel imaging with deep learning\nfor multi-coil mri reconstruction,” in IEEE Conf. Comput. Vis. Pattern\nRecog., 2020, pp. 14 315–14 322.\n[68] S. Wang, H. Cheng, L. Ying, T. Xiao, Z. Ke, H. Zheng, and D. Liang,\n“Deepcomplexmri: Exploiting deep residual network for fast parallel\nmr imaging with complex convolution,” Magnetic Resonance Imaging ,\nvol. 68, pp. 136–147, 2020.\n[69] J. P. Haldar, “Low-rank modeling of local k-space neighborhoods\n(loraks) for constrained mri,” IEEE transactions on medical imaging ,\nvol. 33, no. 3, pp. 668–681, 2013.\n14 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2021\n[70] J. Dolz, K. Gopinath, J. Yuan, H. Lombaert, C. Desrosiers, and I. B.\nAyed, “Hyperdense-net: a hyper-densely connected cnn for multi-modal\nimage segmentation,” IEEE transactions on medical imaging , vol. 38,\nno. 5, pp. 1116–1126, 2018.\n[71] J. Zbontar, F. Knoll, A. Sriram, M. J. Muckley, M. Bruno, A. Defazio,\nM. Parente, K. J. Geras, J. Katsnelson, H. Chandarana et al., “fastmri:\nAn open dataset and benchmarks for accelerated mri,” arXiv preprint\narXiv:1811.08839, 2018.\n[72] B. Lim, S. Son, H. Kim, S. Nah, and K. Mu Lee, “Enhanced deep\nresidual networks for single image super-resolution,” in Proceedings\nof the IEEE conference on computer vision and pattern recognition\nworkshops, 2017, pp. 136–144.\n[73] K. Hammernik, T. Klatzer, E. Kobler, M. P. Recht, D. K. Sodickson,\nT. Pock, and F. Knoll, “Learning a variational network for reconstruction\nof accelerated mri data,” Magnetic resonance in medicine, vol. 79, no. 6,\npp. 3055–3071, 2018.\n[74] M. Uecker, P. Lai, M. J. Murphy, P. Virtue, M. Elad, J. M. Pauly,\nS. S. Vasanawala, and M. Lustig, “Espirit—an eigenvalue approach\nto autocalibrating parallel mri: where sense meets grappa,” Magnetic\nresonance in medicine , vol. 71, no. 3, pp. 990–1001, 2014.\n[75] M. Lustig and J. M. Pauly, “Spirit: iterative self-consistent parallel\nimaging reconstruction from arbitrary k-space,” Magnetic resonance in\nmedicine, vol. 64, no. 2, pp. 457–471, 2010.\n[76] M. Murphy, K. Keutzer, S. Vasanawala, and M. Lustig, “Clinically\nfeasible reconstruction time for l1-spirit parallel imaging and com-\npressed sensing mri,” in Proceedings of the ISMRM Scientiﬁc Meeting\n& Exhibition, 2010, p. 4854.\n[77] C. Ye, X. Li, and J. Chen, “A deep network for tissue microstructure\nestimation using modiﬁed lstm units,” Medical image analysis , vol. 55,\npp. 49–64, 2019.\n[78] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in IEEE\nConf. Comput. Vis. Pattern Recog. , 2018, pp. 7132–7141.\n[79] Y . Blau and T. Michaeli, “The perception-distortion tradeoff,” in Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2018, pp. 6228–6237.",
  "topic": "Modality (human–computer interaction)",
  "concepts": [
    {
      "name": "Modality (human–computer interaction)",
      "score": 0.8184444904327393
    },
    {
      "name": "Modal",
      "score": 0.7087709307670593
    },
    {
      "name": "Computer science",
      "score": 0.7002027630805969
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5085855722427368
    },
    {
      "name": "Transformer",
      "score": 0.45460623502731323
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4485989511013031
    },
    {
      "name": "Computer vision",
      "score": 0.35630905628204346
    },
    {
      "name": "Engineering",
      "score": 0.14333704113960266
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 16
}