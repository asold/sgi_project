{
    "title": "Pretrain-KGE: Learning Knowledge Representation from Pretrained Language Models",
    "url": "https://openalex.org/W3099206682",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2100314029",
            "name": "Zhiyuan Zhang",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2120746893",
            "name": "XiaoQian LIU",
            "affiliations": [
                "King University",
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A1965939310",
            "name": "Yi Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1960108382",
            "name": "Qi Su",
            "affiliations": [
                "King University",
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2107643647",
            "name": "Xu Sun",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A1968357941",
            "name": "Bin He",
            "affiliations": [
                "Peking University",
                "Huawei Technologies (Sweden)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2184957013",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2803609569",
        "https://openalex.org/W2283196293",
        "https://openalex.org/W2797556127",
        "https://openalex.org/W2127795553",
        "https://openalex.org/W2336384382",
        "https://openalex.org/W2250184916",
        "https://openalex.org/W2963688791",
        "https://openalex.org/W2963432357",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2785761199",
        "https://openalex.org/W2739716023",
        "https://openalex.org/W2094728533",
        "https://openalex.org/W205829674",
        "https://openalex.org/W2964116313",
        "https://openalex.org/W2432356473",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2250770256",
        "https://openalex.org/W2728059831",
        "https://openalex.org/W2081580037",
        "https://openalex.org/W2962850650",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2970836468",
        "https://openalex.org/W2558476738",
        "https://openalex.org/W2499696929",
        "https://openalex.org/W2909137510",
        "https://openalex.org/W2778810234",
        "https://openalex.org/W2509019445",
        "https://openalex.org/W2250342289",
        "https://openalex.org/W2759136286",
        "https://openalex.org/W2949434543",
        "https://openalex.org/W2127426251",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W1533230146",
        "https://openalex.org/W2963203544",
        "https://openalex.org/W2972167903",
        "https://openalex.org/W2509893387",
        "https://openalex.org/W2604165577"
    ],
    "abstract": "Conventional knowledge graph embedding (KGE) often suffers from limited knowledge representation, leading to performance degradation especially on the low-resource problem. To remedy this, we propose to enrich knowledge representation via pretrained language models by leveraging world knowledge from pretrained models. Specifically, we present a universal training framework named Pretrain-KGE consisting of three phases: semantic-based fine-tuning phase, knowledge extracting phase and KGE training phase. Extensive experiments show that our proposed Pretrain-KGE can improve results over KGE models, especially on solving the low-resource problem.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 259–266\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n259\nPretrain-KGE: Learning Knowledge Representation from\nPretrained Language Models\nZhiyuan Zhang1, Xiaoqian Liu1, 2, Yi Zhang1, Qi Su1, 2, Xu Sun1 and Bin He3\n1 MOE Key Laboratory of Computational Linguistic, School of EECS, Peking University\n2 School of Foreign Languages, Peking University\n3 Huawei Noah’s Ark Lab\n{zzy1210,liuxiaoqian,zhangyi16,sukia,xusun}@pku.edu.cn\nhebin.nlp@huawei.com\nAbstract\nConventional knowledge graph embedding\n(KGE) often suffers from limited knowledge\nrepresentation, leading to performance degra-\ndation especially on the low-resource problem.\nTo remedy this, we propose to enrich knowl-\nedge representation via pretrained language\nmodels by leveraging world knowledge from\npretrained models. Speciﬁcally, we present a\nuniversal training framework named Pretrain-\nKGE consisting of three phases: semantic-\nbased ﬁne-tuning phase, knowledge extract-\ning phase and KGE training phase. Extensive\nexperiments show that our proposed Pretrain-\nKGE can improve results over KGE models,\nespecially on solving the low-resource prob-\nlem.\n1 Introduction\nKnowledge graphs (KGs) constitute an effective\naccess to world knowledge for a wide variety of\nNLP tasks, such as entity linking (Luo et al., 2017),\ninformation retrieval (Xiong et al., 2017), ques-\ntion answering (Hao et al., 2017) and recommen-\ndation system (Zhang et al., 2016). A typical KG\nsuch as Freebase (Bollacker et al., 2008) and Word-\nNet (Miller, 1995), consists of a set of triplets in\nthe form of (h,r,t) with the head entity h and\nthe tail entity t as nodes and relation r as edges\nin the graph. A triplet represents the relation be-\ntween two entities, e.g., (Steve Jobs, founded, Ap-\nple Inc.). To learn effective representation of en-\ntities and relations in the graph, knowledge graph\nembedding (KGE) models are one of prominent\napproaches (Bordes et al., 2013; Ji et al., 2015; Lin\net al., 2015; Sun et al., 2019; Nickel et al., 2011;\nYang et al., 2015; Kazemi and Poole, 2018; Trouil-\nlon et al., 2016; Zhang et al., 2019).\nHowever, traditional KGE models often suffer\nfrom limited knowledge representation due to the\nsparse and noisy dataset annotations. It leads to\nperformance degradation, especially on the low-\nresource problem. To address this issue, we pro-\npose to enrich knowledge representation via pre-\ntrained language models (i.e., BERT (Devlin et al.,\n2019)) given a semantic description of entities and\nrelations. We propose to incorporate world knowl-\nedge from BERT to the entity and the relation rep-\nresentation. Although simply ﬁne-tuning BERT\ncan enrich the knowledge representation, it suf-\nfers from learning inadequate structure informa-\ntion observed in training triplets, which we have\ndemonstrated when we analyze the rationality of\nthe KGE-training phase.\nWe propose a model-agnostic training frame-\nwork for learning knowledge graph embedding con-\nsisting of three phases: semantic-based ﬁne-tuning\nphase, knowledge extracting phase and KGE train-\ning phase (see Fig. 1). During the semantic-based\nﬁne-tuning phase, we learn knowledge representa-\ntion via BERT given the semantic description of\nentities and relations as the input sequence. In this\nway, we incorporate world knowledge from BERT\ninto the knowledge representation. Then during the\nknowledge extracting phase, we extract the entity\nand the relation representations encoded by BERT\nand inject them into embeddings of a KGE model.\nFinally, during the KGE training phase, we train the\nKGE model to learn adequate structure information\nof dataset, while reserving partial knowledge from\nBERT to learn better knowledge graph embedding.\nExtensive experiments show that our proposed\nPretrain-KGE can improve performance over KGE\nmodels on four benchmark KG datasets. Further\nanalysis and visualization of the knowledge learn-\ning process demonstrate that our method can enrich\nknowledge representation via pretrained language\nmodels through the training framework.\n260\nhead :\ndog.n.1 \nSemantic Descriptions\ndog : a member of the genus Canis\n(probably descended from the common \nwolf) that has been domesticated by man \nsince prehistoric times; occurs in many \nbreeds.\nCanine : one of the four pointed conical \nteeth (two in each jaw) located between \nthe incisors and the premolars.\nKnowledge ExtractingSemantic-based Fine-tuning \ntail : \ncanine.n.1\nKGE Training\nhypernym\nrelation :\n_hypernym\nBERT \nEncoder\nrelation\nhead\ntail\nKGE loss \nEntity Embedding\n… \nRelation Embedding\nFine-tuned BERT Encoder\n…\nEntity \nEmbedding\nRelation\nEmbedding\nKGE Model \nFigure 1: An illustration of our proposed three-phase Pretrain-KGE. “KGE loss” is the score function of an arbi-\ntrary KGE model, thus our method can be applied to any variant of KGE models. “BERT Encoder” represents the\nentity/relation encoder given semantic description of entities and relations.\n2 Related Work\nKGE models can be roughly divided into transla-\ntional models and semantic matching models ac-\ncording to the score function (Wang et al., 2017).\nTranslational models consider the relation between\nthe head and tail entity as a translation between\nthe two entity embeddings, such as TransE (Bor-\ndes et al., 2013), TransH (Wang et al., 2014),\nTransR (Lin et al., 2015), TransD (Ji et al., 2015),\nRotatE (Sun et al., 2019), and TorusE (Ebisu\nand Ichise, 2018); while semantic matching mod-\nels deﬁne a score function to match latent se-\nmantics of the head, tail entity and the relation,\nsuch as, RESCAL (Nickel et al., 2011), Dist-\nMult (Yang et al., 2015), SimplE (Kazemi and\nPoole, 2018), ComplEx (Trouillon et al., 2016) and\nQuatE (Zhang et al., 2019). QuatE (Zhang et al.,\n2019) is the recent state-of-the-art KGE model,\nwhich represents entities as hypercomplex-valued\nembeddings and models relations as rotations in\nthe quaternion space.\nIn a knowledge graph dataset, the names of each\nentity and relation are provided as the semantic\ndescription of entities and relations. Recent works\nalso leverage semantic description to enrich knowl-\nedge representation but ignore contextual infor-\nmation of the semantic description (Socher et al.,\n2013a; Li et al., 2016; Speer and Havasi, 2012; Xu\net al., 2017; Xiao et al., 2017; Xie et al., 2016; An\net al., 2018). Instead, our method exploits world\ninformation via pretrained models.\nRecent approaches to modeling language repre-\nsentations offer signiﬁcant improvements over em-\nbeddings, such as pretrained deep contextualized\nlanguage models (Peters et al., 2018; Devlin et al.,\n2019; Radford et al., 2019; Raffel et al., 2019).\nKG-Bert (Yao et al., 2019) ﬁrst utilizes BERT (De-\nvlin et al., 2019) for knowledge graph completion,\nwhich treats triplets in knowledge graphs as tex-\ntual sequences. However, KG-Bert does not extract\nknowledge representations from Bert and thus can-\nnot provide entity or relation embeddings. In this\nwork, we leverage world knowledge from BERT\nto learn better knowledge representation of entities\nand relations given semantic description.\n3 Method\n3.1 Training Framework\nAn overview of Pretrain-KGE is shown in Fig. 1.\nThe framework consists of three phases: semantic-\nbased ﬁne-tuning phase, knowledge extracting\nphase, and KGE training phase.\nSemantic-based ﬁne-tuning phase We ﬁrst en-\ncode the semantic description by BERT (Devlin\net al., 2019). Deﬁne S(e) and S(r) as the semantic\ndescription of entity eand relation rrespectively.\nBERT(·) converts S(e) and S(r) into the repre-\nsentation of entity and relation. We then project\nthe entity and the relation representations into two\nseparate vector spaces Fd through linear transfor-\nmations, where Fd denotes a vector space on the\nnumber set F. Formally, we get the entity encoder\nEnce(·) for each entity eand the relation encoder\nEncr(·) for each relation r, then output the entity\nand the relation representations as:\nEnce(e) = σ(WeBERT(S(e)) + be) (1)\nEncr(r) = σ(WrBERT(S(r)) + br) (2)\nvh,vr,vt = Ence(h),Encr(r),Ence(t) (3)\nwhere vh, vr, and vt represents encoding vectors\nof the head entity, the relation, and the tail en-\ntity in a triplet (h,r,t), respectively. We,Wr ∈\n261\nFd×n,be,br ∈Fd, and σdenotes a nonlinear acti-\nvation function.\nThe entity and the relation representations are\nused to train the BERT encoder based on a KGE\nloss. After ﬁne-tuning, the entity encoder and the\nrelation encoder are used in the following knowl-\nedge extracting phase.\nKnowledge extracting phase In this phase, we\nextract knowledge representation encoded by\nBERT encoder and inject it into embedding of a\nKGE model as initialization: the entity embedding\nE = [E1; E2; ··· ; Ek] ∈Fk×d; and the relation\nembedding R= [R1; R2; ··· ; Rl] ∈Fl×d, where\n“;” means concatenating column vectors into a ma-\ntrix, kand ldenote the total number of entities and\nrelations, respectively. Formally, we extract the\nknowledge representation encoded by BERT and\ninject it into a KGE model by settingEi to Ence(ei)\nand Rj to Encr(rj).\nKGE training phase After the knowledge ex-\ntracting phase, we train a KGE model in the same\nway as a traditional KGE model. For example, if\nthe max-margin loss function with negative sam-\npling are adopted, the loss is calculated as:\nL=\n[\nγ+ f(vh,vr,vt) −f(vh′ ,vr′ ,vt′ )\n]\n+ (4)\nwhere (h,r,t) and (h′,r′,t′) represent a candidate\nand a corrupted false triplet respectively, γdenotes\nthe margin,\n[\n·\n]\n+ = max(·,0), and f(·) denotes\nthe score function. The KGE training phase is indis-\npensable because simply ﬁne-tuning a pretrained\nlanguage model cannot learn adequate structure in-\nformation observed in training triplets. We demon-\nstrate the rationality of the three-phase training\nframework in Section 5.2.\n4 Experiments\n4.1 Implementation of Baseline Models\nTo evaluate the universality of training framework\nPretrain-KGE, we select multiple public KGE mod-\nels as baselines including translational models:\n•TransE (Bordes et al., 2013), the translational-\nbased model which models the relation as\ntranslations between entities;\n•RotatE (Sun et al., 2019), the extension of\ntranslational-based models which introduces\ncomplex-valued embeddings to model the re-\nlations as rotations in complex vector space;\nand semantic matching models:\n•DistMult (Yang et al., 2015), a semantic\nmatching model where each relation is rep-\nresented with a diagonal matrix;\n•ComplEx (Trouillon et al., 2016), the exten-\nsion of semantic matching model which em-\nbeds entities and relations in complex space.\n•QuatE (Zhang et al., 2019), the recent state-\nof-the-art KGE model which learns entity and\nrelation embeddings in the quaternion space.\nOur implementations of TransE, DistMult, Com-\nplEx, RotatE are based on the framework pro-\nvided by Sun et al. (2019)1. Our implementation\nof QuatE is based on the framework provided by\nZhang et al. (2019)2. The score functions of base-\nlines are listed in Table 1.\nMethod Score function F\nTransE (Bordes et al., 2013) ∥vh + vr −vt∥ R\nDistMult (Yang et al., 2015) ⟨vh, vr, vt⟩ R\nComplEx (Trouillon et al., 2016) Re(⟨vh, vr, ¯vt⟩) C\nRotatE (Sun et al., 2019) ∥vh ⊙vr −vt∥ C\nQuatE (Zhang et al., 2019) ∥vh ⊗ˆvr ⊙vt∥ H\nTable 1: Score functions and corresponding F.\nvh,vr,vt denote head, tail and relation embeddings re-\nspectively. R,C,H denote real number ﬁeld, complex\nnumber ﬁeld and quaternion number division ring re-\nspectively. ∥·∥ denotes L1 norm. ⟨·⟩denotes general-\nized dot product. Re (·) and ¯·denote the real part and\nthe conjugate for complex vectors respectively. ⊗de-\nnotes circular correlation, ⊙denotes Hadamard prod-\nuct. ˆ·denotes the normalized operator.\n4.2 Datasets and Evaluation Metrics\nWe evaluate our proposed training framework\non four benchmark KG datasets: WN18 (Bor-\ndes et al., 2013), WN18RR (Dettmers et al.,\n2018), FB15K (Bordes et al., 2013) and FB15K-\n237 (Toutanova and Chen, 2015). Detailed statis-\ntics of datasets are in the appendix. WN18 and\nWN18RR are two subsets of WordNet (Miller,\n1995); FB15K and FB15K-237 are two subsets\nof FreeBase (Bollacker et al., 2008). We use en-\ntity names and relation names provided by the four\ndatasets as input semantic descriptions for BERT,\nand we also utilize synsets deﬁnitions provided\nby WordNet as additional semantic descriptions of\nentities.\n1https://github.com/DeepGraphLearning/\nKnowledgeGraphEmbedding\n2https://github.com/cheungdaven/QuatE\n262\nModel FB15K FB15K-237 WN18 WN18RR\nH@10↑ MRR↑ MR↓ H@10↑ MRR↑ MR↓ H@10↑ MRR↑ MR↓ H@10↑ MRR↑ MR↓\nTransE 0.866 0.731 40.3 0.528 0.330 171.6 0.920 0.773 265 0.528 0.223 3372\nPretrain-TransE 0.866 0.731 36.6 0.529 0.332 162.0 0.928 0.757 85 0.557 0.235 1747 ♠\nDistMult 0.887 0.768 37.5 0.484 0.307 175.1 0.931 0.686 282 0.534 0.440 4886\nPretrain-DistMult 0.883 0.764 37.0 0.482 0.306 171.3 0.923 0.660 142 0.527 0.432 3550\nComplEx 0.887 0.771 47.1 0.511 0.322 166.1 0.925 0.893 323 0.555 0.469 5421\nPretrain-ComplEx 0.879 0.763 45.2 0.513 0.323 156.9 0.949 0.859 194 0.553 0.459 4468\nRotatE 0.881 0.790 ♠ 41.7 0.531 0.336 177.0 0.960 0.949 269 0.574 0.474 3363\nPretrain-RotatE 0.881 0.784 38.4 0.534 0.337 168.3 0.962 0.927 125 0.580 0.447 2138\nQuatE 0.898 0.778 17.4 0.550 0.349 86.2 0.960 0.951♠ 180 0.581 0.487 2290\nPretrain-QuatE 0.899♠ 0.764 17.2♠ 0.554♠ 0.350♠ 84.4♠ 0.964♠ 0.944 72♠ 0.586♠ 0.488♠ 2085\nTable 2: Link prediction results on four KG datasets. The experiments here use entity names and relation names\nas the semantic description. ↓means that a lower metric is better. ↑means that a higher metric is better. ♠ denotes\nstate-of-the-art performance.\nDataset Link prediction Class.\nFB15K H@10 ↑ H@3 ↑ H@1 ↑ MRR ↑ MR ↓ Acc ↑\nQuatE 0.898 0.832♠ 0.704♠ 0.778♠ 17.4 0.927\n+Name 0.899♠ 0.832♠ 0.677 0.764 17.2♠ 0.928♠\nFB15K-237 H@10 ↑ H@3 ↑ H@1 ↑ MRR ↑ MR ↓ Acc ↑\nQuatE 0.550 0.383 0.249 0.349 86.2 0.816\n+Name 0.554♠ 0.384♠ 0.250♠ 0.350♠ 84.8♠ 0.817♠\nWN18 H@10↑ H@3↑ H@1↑ MRR↑ MR↓ Acc↑\nQuatE 0.960 0.954 0.946♠ 0.951♠ 180 0.977\n+Name 0.964♠ 0.954♠ 0.931 0.944 72 0.981♠\n+Deﬁnition 0.963 0.954♠ 0.930 0.943 62♠ 0.980\nWN18RR H@10↑ H@3↑ H@1↑ MRR↑ MR↓ Acc↑\nQuatE 0.581 0.507 0.438♠ 0.487 2290 0.866\n+Name 0.586♠ 0.509♠ 0.437 0.488♠ 2085♠ 0.874\n+Deﬁnition 0.586♠ 0.509♠ 0.433 0.487 2106 0.876♠\nTable 3: Link prediction and triplet classiﬁcation\n(“Class.”) results over QuatE. ↓means a lower met-\nric is better. ↑means a higher metric is better. ♠\ndenotes state-of-the-art performance of KGE models.\n“+Name” means Pretrain-KGE uses entity and relation\nnames as semantic description. “+Deﬁnition” means\nPretrain-KGE also adopts deﬁnitions of word senses as\nadditional semantic description.\nIn our experiments, we perform the link predic-\ntion task (ﬁltered setting) mainly with the triplet\nclassiﬁcation task. The link prediction task aims to\npredict either the head entity given the relation and\nthe tail entity or the tail entity given the head entity\nand the relation, while triplet classiﬁcation aims to\njudge whether a candidate triplet is correct or not.\nFor the link prediction task, we generate cor-\nrupted false triplets (h′,r,t) and (h,r,t′) using\nnegative sampling. We get ranks of test triplets\nand calculate standard evaluation metrics: Mean\nRank (MR), Mean Reciprocal Rank (MRR) and\nHits at N (H@N). For triplet classiﬁcation, we fol-\nlow the evaluation protocol in Socher et al. (2013b)\nand adopt the accuracy metric (Acc).\n4.3 Main Results\nWe present the main results of our Pretrain-KGE\nmethod in Table 2 and Table 3. As shown in Ta-\nble 2, our universal training framework can be ap-\nplied to multiple variants of KGE models despite\ndifferent embedding spaces, and achieves improve-\nments over TransE, DistMult, ComplEx, RotatE\nand QuatE on most evaluation metrics, especially\non MR but still being competitive on MRR. The\nresults in Table 3 demonstrate that our method can\nfacilitate the performance of QuatE on most eval-\nuation metrics for link prediction and triplet clas-\nsiﬁcation. The results verify the effectiveness of\nour proposed training framework and show that\nour universal training framework can be applied\nto multiple variants of KGE models and achieves\nimprovements on most evaluation metrics, which\nshows the universality of our Pretrain-KGE.\n5 Analysis\nIn this section, we evaluate our Pretrain-KGE on\nthe low-resource problem and further verify the\nrationality of our training framework.\n5.1 Performance on the Low-resource\nProblem\nWe evaluate our training framework in the case of\nfewer training triplets on WordNet, and test its per-\nformance on OOKB entities as shown in Fig. 2. To\ntest the performance of our Pretrain-KGE given\nfewer training triplets, we conduct experiments on\nWN18 and WN18RR by feeding varying numbers\nof training triplets as shown in Fig. 2a and 2b. We\nalso evaluate our Pretrain-KGE on WordNet for\nthe OOKB entity problem as shown in Fig. 2c and\n2d. We use traditional TransE and the word averag-\ning model following Li et al. (2016) as baselines.\nExperimental details are in the appendix.\nResults show that our training framework\nachieves the best performance in the case of fewer\ntraining triplets and OOKB entities. Baseline-\nTransE performs the worst when training triplets\nare few and cannot address the OOKB entity prob-\nlem because it does not utilize any semantic de-\n263\n500 20000 40000 60000 80000 100000 120000\nTraining Triplets\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0MR (K)\nTransE\nAvg\nName\nDefinition\n(a) MR results on WN18.\n500 20000 40000 60000 80000\nTraining Triplets\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0MR (K)\nTransE\nAvg\nName\nDefinition (b) MR results on WN18RR.\n500 1000 2000 4000\nTraining Triplets\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0MR (K)\nRandom\nTransE\nAvg\nName\nDefinition (c) OOKB MR on WN18.\n5002000 4000 8000 12000 16000 20000\nTraining Triplets\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0MR (K)\nRandom\nTransE\nAvg\nName\nDefinition (d) OOKB MR on WN18RR.\nFigure 2: Performance on the low-resource. “Random” and “Avg” denote a random and word averaging baseline.\nact\nanimal\nartifact\nperson\nplant\n(a) Baseline-TransE.\nact\nanimal\nartifact\nperson\nplant (b) Pretrain-TransE.\nFigure 3: Visualization of knowledge learning process.\nDifferent colors mark different supersenses in Word-\nNet. Each point represents an entity. Red ( act), yellow\n(person) and blue ( artifact) refer to word senses rele-\nvant to human beings.\nModel FB15K FB15K-237\nMRR↑ MR↓ MRR↑ MR↓\nPretrain-TransE 0.731 36.6 0.332 162.0\nw/o KGE training phase 0.099 462.8 0.073 594.8\nModel WN18 WN18RR\nMRR↑ MR↓ MRR↑ MR↓\nPretrain-TransE 0.757 85 0.235 1747\nw/o KGE training phase 0.086 1020 0.096 1444\nTable 4: MRR results of the full Pretrain-KGE method\nand the ablation version (“w/o KGE training phase”).\nThe experiments here use entity names and relation\nnames as the semantic description.\nscription. The word averaging model contributes\nto better performance of TransE on fewer training\ntriplets, yet it does not learn knowledge representa-\ntion as well as BERT because the latter can better\nunderstand the semantic description of entities and\nrelations by exploiting world knowledge in the de-\nscription. In contrast, our Pretrain-TransE can fur-\nther enrich knowledge representation by encoding\nsemantic description of entities and relations via\nBERT, and uses the learned representation to initial-\nize the embedding for TransE. In this way, we can\nincorporate world knowledge from BERT into the\nentity and the relation embedding so that TransE\ncan perform better given fewer training triplets and\nalso alleviate the problem of OOKB entities.\n5.2 Rationality of the Framework\nWe visualize the knowledge learning process of\nBaseline-TransE and our Pretrain-TransE in Fig. 3.\nWe select top ﬁve common supersenses in WN18:\nplant, animal, act, person and artifact, among\nwhich the last three supersenses are all relevant\nto the concept of human beings. In Fig. 3a, we\ncan observe that Baseline-TransE learns the struc-\nture information in training triplets and does not\ndistinguish plant and animal from the other three\nsupersenses. In contrast, Fig. 3b shows that our\nPretrain-TransE can distinguish entities belonging\nto different supersenses. Especially, entities rele-\nvant to the same concept human beings are more\ncondensed and entities belonging to signiﬁcantly\ndifferent supersenses are more clearly separated.\nThe main reason is that we introduce knowledge\nfrom BERT to enrich the knowledge representation\nof entities and relations.\nWe also demonstrate the rationality of the KGE-\ntraining phase. Table 4 shows that The full Pretrain-\nKGE method outperforms the ablation version\nwhich excludes the KGE training phase.\n6 Conclusion\nWe propose Pretrain-KGE, an efﬁcient pretraining\ntechnique for learning knowledge graph embed-\nding. Pretrain-KGE is a universal training frame-\nwork that can be applied to any KGE model. It\nlearns knowledge representation via pretrained lan-\nguage models and incorporates world knowledge\nfrom the pretrained model into the entity and the\nrelation embedding. Extensive experimental results\ndemonstrate consistent improvements over KGE\nmodels across multiple benchmark datasets. The\nknowledge incorporation introduced in Pretrain-\nKGE alleviates the low-resource problem and we\njustify our three-phase training framework through\nan analysis of the knowledge learning process.\nAcknowledgments\nThis work is partly supported by Beijing Academy\nof Artiﬁcial Intelligence (BAAI). Xu Sun is the\ncorresponding author.\n264\nReferences\nBo An, Bo Chen, Xianpei Han, and Le Sun. 2018.\nAccurate text-enhanced knowledge graph represen-\ntation learning. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2018, New Or-\nleans, Louisiana, USA, June 1-6, 2018, Volume 1\n(Long Papers), pages 745–755.\nKurt D. Bollacker, Colin Evans, Praveen Paritosh, Tim\nSturge, and Jamie Taylor. 2008. Freebase: a collab-\noratively created graph database for structuring hu-\nman knowledge. In Proceedings of the ACM SIG-\nMOD International Conference on Management of\nData, SIGMOD 2008, Vancouver, BC, Canada, June\n10-12, 2008, pages 1247–1250.\nAntoine Bordes, Nicolas Usunier, Alberto Garc ´ıa-\nDur´an, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In Advances in Neural Information\nProcessing Systems 26: 27th Annual Conference on\nNeural Information Processing Systems 2013. Pro-\nceedings of a meeting held December 5-8, 2013,\nLake Tahoe, Nevada, United States , pages 2787–\n2795.\nTim Dettmers, Pasquale Minervini, Pontus Stenetorp,\nand Sebastian Riedel. 2018. Convolutional 2d\nknowledge graph embeddings. In Thirty-Second\nAAAI Conference on Artiﬁcial Intelligence.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171–4186.\nTakuma Ebisu and Ryutaro Ichise. 2018. Toruse:\nKnowledge graph embedding on a lie group. In Pro-\nceedings of the Thirty-Second AAAI Conference on\nArtiﬁcial Intelligence, (AAAI-18), the 30th innova-\ntive Applications of Artiﬁcial Intelligence (IAAI-18),\nand the 8th AAAI Symposium on Educational Ad-\nvances in Artiﬁcial Intelligence (EAAI-18), New Or-\nleans, Louisiana, USA, February 2-7, 2018 , pages\n1819–1826.\nYanchao Hao, Yuanzhe Zhang, Kang Liu, Shizhu He,\nZhanyi Liu, Hua Wu, and Jun Zhao. 2017. An end-\nto-end model for question answering over knowl-\nedge base with cross-attention combining global\nknowledge. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2017, Vancouver, Canada, July 30 - August 4,\nVolume 1: Long Papers, pages 221–231.\nGuoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and\nJun Zhao. 2015. Knowledge graph embedding via\ndynamic mapping matrix. In Proceedings of the\n53rd Annual Meeting of the Association for Compu-\ntational Linguistics and the 7th International Joint\nConference on Natural Language Processing of the\nAsian Federation of Natural Language Processing,\nACL 2015, July 26-31, 2015, Beijing, China, Volume\n1: Long Papers, pages 687–696.\nSeyed Mehran Kazemi and David Poole. 2018. Simple\nembedding for link prediction in knowledge graphs.\nIn Advances in Neural Information Processing Sys-\ntems 31: Annual Conference on Neural Information\nProcessing Systems 2018, NeurIPS 2018, 3-8 De-\ncember 2018, Montr´eal, Canada, pages 4289–4300.\nXiang Li, Aynaz Taheri, Lifu Tu, and Kevin Gim-\npel. 2016. Commonsense knowledge base comple-\ntion. In Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2016, August 7-12, 2016, Berlin, Germany, Volume\n1: Long Papers.\nYankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and\nXuan Zhu. 2015. Learning entity and relation em-\nbeddings for knowledge graph completion. In Pro-\nceedings of the Twenty-Ninth AAAI Conference on\nArtiﬁcial Intelligence, January 25-30, 2015, Austin,\nTexas, USA, pages 2181–2187.\nAngen Luo, Sheng Gao, and Yajing Xu. 2017. Deep se-\nmantic match model for entity linking using knowl-\nedge graph and text. In 2017 International Con-\nference on Identiﬁcation, Information and Knowl-\nedge in the Internet of Things, IIKI 2017, Shandong,\nChina, October 19-21, 2017, pages 110–114.\nGeorge A. Miller. 1995. Wordnet: A lexical database\nfor english. Commun. ACM, 38(11):39–41.\nMaximilian Nickel, V olker Tresp, and Hans-Peter\nKriegel. 2011. A three-way model for collective\nlearning on multi-relational data. In Proceedings\nof the 28th International Conference on Machine\nLearning, ICML 2011, Bellevue, Washington, USA,\nJune 28 - July 2, 2011, pages 809–816.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2018, New Or-\nleans, Louisiana, USA, June 1-6, 2018, Volume 1\n(Long Papers), pages 2227–2237.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report, OpenAI.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. CoRR, abs/1910.10683.\n265\nRichard Socher, Danqi Chen, Christopher D Manning,\nand Andrew Ng. 2013a. Reasoning with neural ten-\nsor networks for knowledge base completion. In\nAdvances in neural information processing systems ,\npages 926–934.\nRichard Socher, Danqi Chen, Christopher D Manning,\nand Andrew Ng. 2013b. Reasoning with neural ten-\nsor networks for knowledge base completion. In\nAdvances in neural information processing systems ,\npages 926–934.\nR. Speer and Catherine Havasi. 2012. Represent-\ning general relational knowledge in conceptnet 5.\nIn Proceedings of the Eighth International Confer-\nence on Language Resources and Evaluation, LREC\n2012, Istanbul, Turkey, May 23-25, 2012 , pages\n3679–3686.\nZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian\nTang. 2019. Rotate: Knowledge graph embedding\nby relational rotation in complex space. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nKristina Toutanova and Danqi Chen. 2015. Observed\nversus latent features for knowledge base and text\ninference. In Proceedings of the 3rd Workshop on\nContinuous Vector Space Models and their Compo-\nsitionality, pages 57–66.\nTh´eo Trouillon, Johannes Welbl, Sebastian Riedel,´Eric\nGaussier, and Guillaume Bouchard. 2016. Complex\nembeddings for simple link prediction. In Proceed-\nings of the 33nd International Conference on Ma-\nchine Learning, ICML 2016, New York City, NY,\nUSA, June 19-24, 2016, pages 2071–2080.\nQuan Wang, Zhendong Mao, Bin Wang, and Li Guo.\n2017. Knowledge graph embedding: A survey of\napproaches and applications. IEEE Trans. Knowl.\nData Eng., 29(12):2724–2743.\nZhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng\nChen. 2014. Knowledge graph embedding by trans-\nlating on hyperplanes. In Proceedings of the Twenty-\nEighth AAAI Conference on Artiﬁcial Intelligence,\nJuly 27 -31, 2014, Qu ´ebec City, Qu ´ebec, Canada ,\npages 1112–1119.\nHan Xiao, Minlie Huang, Lian Meng, and Xiaoyan\nZhu. 2017. SSP: semantic space projection for\nknowledge graph embedding with text descriptions.\nIn Proceedings of the Thirty-First AAAI Conference\non Artiﬁcial Intelligence, February 4-9, 2017, San\nFrancisco, California, USA, pages 3104–3110.\nRuobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and\nMaosong Sun. 2016. Representation learning of\nknowledge graphs with entity descriptions. In Pro-\nceedings of the Thirtieth AAAI Conference on Arti-\nﬁcial Intelligence, February 12-17, 2016, Phoenix,\nArizona, USA, pages 2659–2665.\nChenyan Xiong, Russell Power, and Jamie Callan.\n2017. Explicit semantic ranking for academic\nsearch via knowledge graph embedding. In Proceed-\nings of the 26th International Conference on World\nWide Web, WWW 2017, Perth, Australia, April 3-7,\n2017, pages 1271–1279.\nJiacheng Xu, Xipeng Qiu, Kan Chen, and Xuanjing\nHuang. 2017. Knowledge graph representation with\njointly structural and textual encoding. In Proceed-\nings of the Twenty-Sixth International Joint Con-\nference on Artiﬁcial Intelligence, IJCAI 2017, Mel-\nbourne, Australia, August 19-25, 2017, pages 1318–\n1324.\nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng\nGao, and Li Deng. 2015. Embedding entities and\nrelations for learning and inference in knowledge\nbases. In 3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA,\nMay 7-9, 2015, Conference Track Proceedings.\nLiang Yao, Chengsheng Mao, and Yuan Luo. 2019.\nKG-BERT: BERT for knowledge graph completion.\nCoRR, abs/1909.03193.\nFuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing\nXie, and Wei-Ying Ma. 2016. Collaborative knowl-\nedge base embedding for recommender systems.\nIn Proceedings of the 22nd ACM SIGKDD Inter-\nnational Conference on Knowledge Discovery and\nData Mining, San Francisco, CA, USA, August 13-\n17, 2016, pages 353–362.\nShuai Zhang, Yi Tay, Lina Yao, and Qi Liu. 2019.\nQuaternion knowledge graph embedding. CoRR,\nabs/1904.10281.\nA Appendix\nA.1 Dataset Statistics\nWe evaluate our proposed training framework on\nfour benchmark KG datasets: WN18, WN18RR,\nFB15K and FB15K-237. We list detailed statis-\ntics of datasets are in Table 5. Datasets can be\ndownloaded at this repository3.\nDataset Entities Relations Train Triplets Valid. Triplets Test Triplets\nWN18 40943 18 141442 5000 5000\nWN18RR 40943 11 86835 3034 3134\nFB15K 14951 1345 483142 50000 59071\nFB15K-237 14541 237 272115 17535 20466\nTable 5: Statisics of datasets.\nA.2 Detailed Implementation\nA.2.1 Details in Semantic-based Fine-tuning\nPhase\nIn semantic-based ﬁne-tuning phase, we adopt the\nfollowing non-linear pointwise function σ(·): for\n3https://github.com/DeepGraphLearning/\nKnowledgeGraphEmbedding\n266\nFB15K Dim. Dim.R Neg.1 Neg.2. Batch.1. Batch.2 Lr.1 Lr.2 Updates.1 Updates.2. Opt.1 Opt.2\nTransE 1000 1000 3 256 8 1024 5e-6 1e-4 150k 150k adam adam\nDistMult 2000 2000 3 256 8 1024 5e-6 1e-3 150k 150k adam adam\nComplEx 1000 2000 3 256 8 1024 5e-6 1e-3 150k 150k adam adam\nRotatE 1000 2000 3 256 8 1024 5e-6 1e-4 150k 150k adam adam\nQuatE 250 1000 10 20 4 50 batches 1e-5 0.1 40k 5000 epochs adam adagrad\nFB15K-237 Dim. Dim.R Neg.1 Neg.2. Batch.1. Batch.2 Lr.1 Lr.2 Updates.1 Updates.2. Opt.1 Opt.2\nTransE 1000 1000 3 256 8 1024 5e-6 5e-5 150k 150k adam adam\nDistMult 2000 2000 3 256 8 1024 5e-6 5e-5 150k 150k adam adam\nComplEx 1000 2000 3 256 8 1024 5e-6 5e-5 150k 150k adam adam\nRotatE 1000 2000 3 256 8 1024 5e-6 1e-3 150k 150k adam adam\nQuatE 100 400 10 10 6 10 batches 1e-5 0.1 200k 15000 epochs adam adagrad\nWN18 Dim. Dim.R Neg.1 Neg.2. Batch.1. Batch.2 Lr.1 Lr.2 Updates.1 Updates.2. Opt.1 Opt.2\nTransE 500 500 3 512 8 512 5e-6 1e-4 80k 80k adam adam\nDistMult 1000 1000 3 512 8 512 5e-6 1e-3 80k 80k adam adam\nComplEx 500 1000 3 512 8 512 5e-6 1e-3 80k 80k adam adam\nRotatE 500 1000 3 512 8 512 5e-6 1e-4 80k 80k adam adam\nQuatE 250 1000 10 20 1 10 batches 1e-5 0.1 200k/300k 1500 epochs adam adagrad\nWN18RR Dim. Dim.R Neg.1 Neg.2. Batch.1. Batch.2 Lr.1 Lr.2 Updates.1 Updates.2. Opt.1 Opt.2\nTransE 500 500 3 512 8 512 5e-6 5e-5 80k 80k adam adam\nDistMult 1000 1000 3 512 8 512 5e-6 2e-3 80k 80k adam adam\nComplEx 500 1000 3 512 8 512 5e-6 2e-3 80k 80k adam adam\nRotatE 500 1000 3 512 8 512 5e-6 5e-5 80k 80k adam adam\nQuatE 100 400 10 20 8 10 batches 1e-5 0.1 60k/10k 40000 epochs adam adagrad\nTable 6: Experimental settings. Dim. denotes embedding dimension. Dim. R denotes embedding dimension when\nembeddings are ﬂatten into the real number ﬁled. Batch. denotes batch size. Norm. denotes p-norm in score\nfunction, Lr. denotes learning rate. Neg. denotes entity negative sampling rate. 1. denotes in semantic-based ﬁne-\ntuning phase and 2. denotes in KGE training phase and during the training of traditional embedding-based models.\nIn column Batch.2, 50 batches means the dataset are devided into 50 batches. In column Updates.1, 200k/300k\nmeans 200k updates in the proposed model utilizing entity and relation names as semantic description and 300k\nin the proposed model utilizing entity and relation names as well as entity deﬁnition as semantic description. In\ncolumn Updates.2, 5000 epochs means the number of training updates is 5000 epochs.\nx= x0 +\nK−1∑\ni=1\nxiei ∈F (where F can be real num-\nber ﬁled R, complex number ﬁled C or quaternion\nnumber ring H):\nσ(x) = tanh(x0) +\nK−1∑\ni=1\ntanh(xi)ei (5)\nwhere xi ∈ R and ei is the K-dimension\nhypercomplex-value unit. For instance, when K =\n1,F = R; when K = 2,F = C, e1 = i (the imag-\ninary unit); when K = 4,F = H, e1,2,3 = i,j,k\n(the quaternion units). For example:\nσ\n([a+ bi\nc+ di\n])\n=\n[tanh(a) + tanh(b)i\ntanh(c) + tanh(d)i\n]\n(6)\nwhere i,j,k denote the quaternion units.\nA.2.2 Implementation of the Word-averaging\nBaseline\nWe implement the word-averaging baseline to\nutilize the entity names and entity deﬁnition in\nWordNet to represent the entity embedding bet-\nter. Formally, for entity eand its textual descrip-\ntion T(e) = w1w2 ···wL, where wi denotes the\ni-th token in sentence T(e) and T(e) here together\nutilizing the entity names and entity deﬁnition in\nWordNet.\nAvg(e) = 1\nL\nL∑\ni=1\nui (7)\nwhere ui denotes the word embedding of token\nwi, which is a trainable randomly initialized pa-\nrameter and will be trained in the semantic-based\nﬁne-tuning phase.\nWe also adopt our three-phase training method\nto train word-averaging baseline. Similarly,\nE = [ E1; E2; ··· ; Ek] ∈ Fk×d and R =\n[R1; R2; ··· ; Rl] ∈Fl×d denote entity and rela-\ntion embeddings. In semantic-based ﬁne-tuning\nphase, for head entity h, tail entity tand relation r,\nthe score function is calculated as:\nvh,vr,vt = Avg(h),Rr,Avg(t) (8)\nScore = ∥vh + vr −vt∥ (9)\nwhere Rr denotes the relation embedding of rela-\ntion r. In knowledge extracting phase, similar to\nour proposed model, we initialize Ei with Avg(ei).\nIn KGE training phase, we optimize Eand Rwith\nthe same training method to TransE baseline.\nA.3 Experimental Settings\nThe hyper-parameters are listed in Table 6. Experi-\nments are conducted on a GeForce GTX TITAN X\nGPU."
}