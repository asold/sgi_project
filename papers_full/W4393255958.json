{
    "title": "GEFormerDTA: drug target affinity prediction based on transformer graph for early fusion",
    "url": "https://openalex.org/W4393255958",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5101724391",
            "name": "Youzhi Liu",
            "affiliations": [
                "Shandong University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5088785865",
            "name": "Linlin Xing",
            "affiliations": [
                "Shandong University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5103171929",
            "name": "Longbo Zhang",
            "affiliations": [
                "Shandong University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5043209636",
            "name": "Hongzhen Cai",
            "affiliations": [
                "Shandong University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5079800756",
            "name": "Maozu Guo",
            "affiliations": [
                "Beijing University of Civil Engineering and Architecture"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2415990864",
        "https://openalex.org/W3108365154",
        "https://openalex.org/W2593634873",
        "https://openalex.org/W4323055195",
        "https://openalex.org/W2578119541",
        "https://openalex.org/W3000043291",
        "https://openalex.org/W2134427192",
        "https://openalex.org/W2527110257",
        "https://openalex.org/W2165125496",
        "https://openalex.org/W2398478519",
        "https://openalex.org/W2943893168",
        "https://openalex.org/W2921473648",
        "https://openalex.org/W2605952223",
        "https://openalex.org/W2808199968",
        "https://openalex.org/W2997680655",
        "https://openalex.org/W2988260770",
        "https://openalex.org/W3088070507",
        "https://openalex.org/W2809216727",
        "https://openalex.org/W2785947426",
        "https://openalex.org/W3029836473",
        "https://openalex.org/W3032123378",
        "https://openalex.org/W3207312583",
        "https://openalex.org/W4283259285",
        "https://openalex.org/W2906755148",
        "https://openalex.org/W2809418595",
        "https://openalex.org/W3025257690",
        "https://openalex.org/W4385330534",
        "https://openalex.org/W4318624508",
        "https://openalex.org/W2086286404",
        "https://openalex.org/W2035585923",
        "https://openalex.org/W2109991441",
        "https://openalex.org/W3113731542",
        "https://openalex.org/W2920795827",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W3021975806",
        "https://openalex.org/W4284974145",
        "https://openalex.org/W1983301918",
        "https://openalex.org/W2607268717",
        "https://openalex.org/W2808950571",
        "https://openalex.org/W3129073614",
        "https://openalex.org/W3005769002",
        "https://openalex.org/W6763868836",
        "https://openalex.org/W3177318507",
        "https://openalex.org/W3211394146",
        "https://openalex.org/W1972987731",
        "https://openalex.org/W3096561213",
        "https://openalex.org/W3175786839",
        "https://openalex.org/W2971227267"
    ],
    "abstract": null,
    "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2024) 14:7416  | https://doi.org/10.1038/s41598-024-57879-1\nwww.nature.com/scientificreports\nGEFormerDTA: drug target affinity \nprediction based on transformer \ngraph for early fusion\nYouzhi Liu 1, Linlin Xing 1*, Longbo Zhang 1, Hongzhen Cai 2 & Maozu Guo 3\nPredicting the interaction affinity between drugs and target proteins is crucial for rapid and accurate \ndrug discovery and repositioning. Therefore, more accurate prediction of DTA has become a key area \nof research in the field of drug discovery and drug repositioning. However, traditional experimental \nmethods have disadvantages such as long operation cycles, high manpower requirements, and \nhigh economic costs, making it difficult to predict specific interactions between drugs and target \nproteins quickly and accurately. Some methods mainly use the SMILES sequence of drugs and the \nprimary structure of proteins as inputs, ignoring the graph information such as bond encoding, \ndegree centrality encoding, spatial encoding of drug molecule graphs, and the structural information \nof proteins such as secondary structure and accessible surface area. Moreover, previous methods \nwere based on protein sequences to learn feature representations, neglecting the completeness of \ninformation. To address the completeness of drug and protein structure information, we propose \na Transformer graph-based early fusion research approach for drug-target affinity prediction \n(GEFormerDTA). Our method reduces prediction errors caused by insufficient feature learning. \nExperimental results on Davis and KIBA datasets showed a better prediction of drugtarget affinity \nthan existing affinity prediction methods.\nAbbreviations\nSS  Secondary structure\nASA  Accessible surface area\nDTI  Drug-target interactions\nDTA  Drug-target affinity\nGCN  Graph convolutional neural\nESC  Encoder for feature extraction for edge coding, spatial position coding and centrality coding\nThe global pharmaceutical industry today is facing enormous challenges. Intense product competition, patent \nexpiration, shorter exclusivity periods, and price constraints pressure pharmaceutical companies to reduce costs \n, increase productivity, and accelerate  growth1,2. It takes companies more than $500 million and approximately \n12–15 years to bring new compounds to  market1,3–5. Less than 5% of all compounds screened enter preclinical \ndevelopment, and only 2% of these candidates enter clinical  testing1,4. Approximately 80% of all drugs that enter \nphase I trials fail in  development1. To address these challenges, many research institutions and pharmaceutical \ncompanies have turned their attention to the drug repositioning  model6, which involves analyzing the economic \nbenefits and drawbacks identified by experts. Therefore, we are strongly motivated to develop a computational \nmodel that can predict the affinity of new drug-target pairs based on previously existing drugs and targets.\nDrug-target affinity (DTA) prediction is crucial for speeding up the drug screening process. Various com -\nputational  methods7 have been proposed for this purpose. Mainstream methods include ligand/receptor-based \n methods8, gene ontology-based  methods9, text mining-based  methods10, and reverse docking  methods11. These \nmethods are continuously being improved under different conditions. Receptor-based methods often employ \ndocking  simulations6,12, which require 3D structures of target  proteins13. However, obtaining such structures \ncan be expensive and challenging. Ligand-based approaches suffer from poor predictions when the number of \nknown ligands for the target protein is small. This approach relies on the similarity between candidate ligands \nand known ligands. Gene ontology-based and text mining-based approaches face similar limitations due to the \nOPEN\n1Department of Computer Science and Technology, Shandong University of Technology, Zibo 255000, \nChina. 2Department of Agricultural Engineering and Food Science, Shandong University of Technology, \nZibo 255000, China. 3Department of Electrical and Information Engineering, Beijing University of Architecture, \nBeijing 102616, China. *email: xinglinlin@sdut.edu.cn\n2\nVol:.(1234567890)Scientific Reports |         (2024) 14:7416  | https://doi.org/10.1038/s41598-024-57879-1\nwww.nature.com/scientificreports/\ncontent reported in the text. Moreover, redundant names of drugs and target proteins complicate these methods. \nText mining methods are also limited to existing academic literature, making it difficult to discover and acquire \nnew knowledge.\nMachine learning has addressed limitations over time. For instance, SimBoost models utilize known drug \nassociation/similarity networks and known target protein association/similarity networks to create new features \nfor predicting the DTA of unknown drug-target  pairs14. Alternatively, similarity can be derived from other known \ninformation instead of training data affinity. Kernel-based approaches, such as regularized least squares regres-\nsion (RLS) with kernels constructed from drug and target molecular descriptors, are  used15. KronRLS models \nare calculated from the Kronecker product of drug and protein kernels into pairs of  K15 (any similarity metric \ncan be used) to speed up model training. Predicting drug-target interactions (DTI) can also aid in DTA predic-\ntion. Research in this area includes DTI-CDF 16 (a cascaded deep forest model),  MLCLB17 (a new multi-label \nclassification framework), and DTI-MLCD18 (multi-label learning to support community detection).\nSome approaches utilize shallow neural networks on drugs and proteins.  DeepAffinity19 employs seq2seq self-\nencoders20 for unsupervised learning of protein and compound feature representations. The learned encoder’s \noutput is then passed to the attention layer and further to the 1D convolution layer. The outputs of the protein and \ncompound convolutional layers are combined and fed into the fully connected layer. Similarly, the  DeepDTA21 \nmodel adopts a 1D representation and a 1D convolutional layer (with pooling) to capture data patterns. The \nfinal convolutional layers are connected, followed by multiple hidden layers, and regression is performed using \ndrug-target affinity scores.\nDeep learning models are among the best-performing models for DTA prediction. Many  works22–25 have been \ncarried out in deep learning models. However, these models use drug SMILES as direct input, which may not \ncapture the complete uniqueness of the molecular structure of drugs. By using data in string format, molecular \nstructure information is lost, which may reduce the functional relevance between potential drug molecules, \nwhich in turn can reduce the predictive power of the model. The development of graph convolutional neural \n networks26,27 has migrated from other fields to biological information. It has been used for drug  discovery28 , \nincluding interaction  prediction29, affinity prediction, synthesis prediction, and drug  repositioning30. Since pro-\ntein biomechanics inherently contains more structural information, previously proposed methods mainly use \nprotein sequence information directly as input to the model, and these methods lose a large amount of protein \nstructural information.\nThis paper introduces GEFormerDTA, a novel neural network model that integrates drug and protein struc-\nture information. It leverages four forms of feature representation (node, degree center, space, and edge encoded \nfeatures) to effectively utilize their roles in the graph task. Secondary structure information and ASA information \nof the target protein are incorporated, enabling comprehensive utilization of protein structural information. An \nearly fusion mechanism is employed to handle the binding affinity between drugs and proteins, reducing predic-\ntion errors caused by information redundancy.\nMaterials and methods\nProblem definition\nThe drug-target binding affinity (DTA) problem aims to predict the binding affinity between a drug and a target \nprotein. This is a mathematical regression problem:\nwhere D ={ d1 ,d2 ,d3 ,..., di} , P ={ p1 ,p2 ,p3 ,..., pi} , and θ is a learnable parameter in the prediction model \nF . Our task is to predict the affinity score between ti and D or T and d j , given a new drug ti and target protein d j.\nDataset\nWe evaluated our proposed model on two different datasets, the kinase dataset  Davis31 and the KIBA  dataset32, \nboth of which have been used as gold standard datasets for prediction assessment in DTI and DTA  studies14,33.\nThe Davis dataset contains selective assays of kinase protein families, related inhibitors, and their respective \ndissociation constant ( Kd ) values. It contains the interactions of 442 proteins and 68 ligands. On the other hand, \nthe KIBA dataset was derived from a method called KIBA, which combines the biological activities of kinase \ninhibitors from different sources (e.g., K i , Kd , and IC50)32. The study of predicting these kinase inhibitors can \nbe explored  through34. KIBA scores were constructed to optimize the concordance between K i , Kd , and IC50 by \nexploiting the statistical information they contain. The KIBA dataset initially contained 467 targets and 52498 \n drugs14. Removing these drugs and targets can mitigate the impact of noise on model training, balancing the \ndataset and preventing an undue focus on specific drugs and targets during the model training process. Tables 1 \nsummarizes these datasets we used in our experiments. To demonstrate the properties of the drugs and proteins \nmore visually in Table 1, we depict the breadth and length of the two gold standard data through Fig. 1.\nRegarding data density, the model performs well in handling sparse graphs, considering only the immedi-\nate neighbors of nodes. Therefore, the model performs better when dealing with the low-density KIBA dataset. \nHowever, its performance is poorer in the high-density Davis dataset. Concerning data size, the model utilizes \nself-attention mechanisms to handle small-scale data, capturing global information about the molecular graph \nneighborhood and aiding in extracting key node information. However, when dealing with large-scale data, the \nmodel has longer training cycles.\nWhile33 directly uses the Kd values from the Davis dataset as binding affinity values, we employ the trans-\nformed values into logarithmic space, denoted as pKd , similar to the equation (2) described.\n(1)A = Fθ (P, D),\n3\nVol.:(0123456789)Scientific Reports |         (2024) 14:7416  | https://doi.org/10.1038/s41598-024-57879-1\nwww.nature.com/scientificreports/\nDrug representation\nIn the dataset, the pairs of affinity primarily consist of drugs and proteins. The input for drug compounds \nmainly utilizes two data formats: SMILES and SDF . In our proposed method, the molecular graph of a drug is \nconstructed based on the SMILES string and SDF file data. Specifically, the SDF format molecular data is parsed \nusing the RDKit  tool35 to obtain the two-dimensional structural information of the molecule. In the molecular \ngraph representation, atoms represent the nodes of the graph. The combination of node features encompasses \na variety of properties, including atom symbol, atom degree, atom implicit valence, the number of free valence \nelectrons, atom hybridization type, and atom aromaticity. These attribute features are concatenated to form a \nmultidimensional feature. The edges in the graph represent the chemical bonds of the molecule, and the pres-\nence or absence of an edge between two nodes indicates whether there is an interaction between the atoms. We \nconstruct an adjacency matrix based on these edges, which encapsulates the positional information of the node \nwith respect to other nodes. In our study, we use Gd = (V d, Ed) to represent the graph representation of the \ndrug compound, where Vd represents the atoms of the drug compound, and Ed represents the chemical bonds \nof the drug compound.\nWe define the set of attributes of atom j of the i-th drug d i in the entire drug set D of the database as x d i\nj  , which \nis a vector of nine attributes, denoted as follows:\nwhere x d i\nj  represents the mathematical expression of atom j of drug d i , a1 represents the number of atoms in drug \nd i , a2 represents chiral information including R-type, S-type, axial chirality, planar chirality, and helical chirality, \nand [a3 ,a4 ,..., a9 ] represents, in order, the atomic degree (number of chemical bonds), formal charge, number \nof connected hydrogen atoms, free radical number of electrons, type of atomic hybridization, whether or not an \naromatic bond is formed, and whether or not an a-ring is present. x d i\nj  in these properties can be obtained by the \nRDKit tool and embedded as integers under the guidance of a predefined dictionary.\nDegree centrality encoding\nWe first extracted the atomic and chemical bonding information of the drug using the RDKit  tool35,36. The more \nedges an atom exists, the more critical the atom becomes, or the more complex the interconnections with other \natoms are to the model. In this paper, we characterize the degree features in the molecular graph by atomic degree \ncentrality as an additional signal for the neural network. Since the degree centrality habit encoding (see Fig. 2) is \nused for each node, we only need to combine it with the atomic node corpora to form the degree centrality \nfeatures of the atoms. This encoding allows the model to capture the semantic relevance and importance of the \natoms more confidently and pass them into the attention mechanism, as shown in the following mathematical \nequation:\nwhere e− , e+ ∈ Rd denote the incoming and outgoing degrees of atomic nodes specifying the learnable \nembedding vectors, respectively, Additionally, h (d i)\nj  denotes the atomic features of atom j in drug d i . Here, d \ndenotes the modulation factor, and WQ and WK are the weight matrices for atoms (nodes) i and j, respectively.\nFor undirected graphs, the incoming degree deg −(\nvj\n)\n and outgoing degree deg +(\nvj\n)\n can be uniformly denoted \nas deg(v j) . By adding the degree-centric encoding feature to the nodes, softmax attention can capture the critical \ninformation of the nodes in K and Q. Therefore, the model can capture the semantic relevance and the critical \ninformation of the nodes in the attention mechanism.\nAtomic spatial position encoding\nThe Transformer possesses globality, but it relies too heavily on positional information for encoding. When \nsolving sequential data present in natural language problems, it is possible to encode each position (i.e., absolute \nposition encoding)37,38 or to encode any two positions in the Transformer layer (i.e., relative position encoding)39.\n(2)pK d =− log 10\n( K d\n1e9\n)\n,\n(3)x d i\nj =[ a1 ,a2 ,..., a9 ],\n(4)hdi\nj = xdi\nj + e−\ndeg −(vj) + e+\ndeg +(vj),\n(5)Featruedegij = (hiW Q )(hjW K )T\n√\nd\n,\nTable 1.  Dataset summary.\nproteins drugs links\nDavis ( Kd) 422 68 30056\nKIBA 229 2111 118254\n4\nVol:.(1234567890)Scientific Reports |         (2024) 14:7416  | https://doi.org/10.1038/s41598-024-57879-1\nwww.nature.com/scientificreports/\nHowever, when we use the graph information built based on the spatial structure as the input to the Trans -\nformer model, it is instead detrimental to the prediction of the model. We introduce the spatial location encod-\ning to capture the spatial structure information of the drug graph. First, we write down the set of drug nodes as \nVd ={ vj | vj ∈ RN }j=l , given a function φ( vi, vj) ∈ RN  representing the spatial relationship between vi and v j . \nFigure 1.  Summary of the Davis (left panel) and KIBA (right panel) datasets. (A) Distribution of binding \naffinity values. (B) Length distribution of SMILES strings. (C) The number of atoms of drug molecules. (D) \nLength distribution of protein sequences.\n5\nVol.:(0123456789)Scientific Reports |         (2024) 14:7416  | https://doi.org/10.1038/s41598-024-57879-1\nwww.nature.com/scientificreports/\nWe describe the function φ( v i, v j) as a connectivity definition graph between nodes. In the drug diagram, we set \nthe pathway φ( vi, vj) ∈ RN  between vi and v j to denote\nwhere SPD(vi , v j ) denotes the shortest dependency path (SDP) reachable between vi and v j.\nAfter we encode by degree center and spatial location, we obtain the embedding matrix of the atomic pair \n(node pair) ( vi , v j ) as\nwhere W (i,j)\nφ  is the weight of the spatial location feature of the drug node pair, and Featrue pij is the embedding \nof the spatial structure feature.\nInteratomic chemical bonding coding\nEdges are also an important component in handling graph tasks. For example, in molecular graphs of drug \ncompounds, features describing the types of chemical bonds can be assigned to atom pairs. These features are \nas crucial as node features in representing the graph and are indispensable for encoding in graph tasks. Previ -\nous approaches to graph tasks mainly include two methods: (1) Edge features are added to the associated node \n features40. (2) For each node, the features of its associated edges are used together with the aggregated node \n features41. However, these approaches only propagate edge information to their associated (neighbor) nodes, \nwhich may not effectively utilize edge information to represent the entire graph.\nWe introduce atomic compound chemical bond encoding to encode edge features into the attention layer \nbetter. For the adjacent atom-pairs edge encoding approach is defined:\nwhere b1 denotes the bond type, b2 denotes the steric bond, and b3 denotes whether the bond is conjugate. b1 , b2 \nand b3 can be obtained by the RDKit tool. If the shortest path of i and j is P = (e1 ,e2 ,..., ek ) , then\nProtein representation\nPrevious  studies25,42 typically used protein sequences as input for deep learning models, where protein residues \nwere encoded into a vector space using techniques like one-hot encoding or BPE encoding. These studies \nemployed a lightweight 1D convolutional layer encoder to extract valuable features from the protein. However, \nthese methods solely captured the primary structure information of proteins. Predicting the 3D structure from \na 1D sequence is a formidable task, making 1D representations inadequate for capturing the spatial structural \nfeatures of proteins. Obtaining 3D structures for certain proteins is challenging due to their limited representation \nin  databases43. Moreover, representing the irregular 3D structure requires a large-scale 3D matrix, resulting in \ncomputationally expensive model execution. Additionally, experimentally determined 3D structures may suffer \nfrom low quality since they depend on the intricate and demanding process of co-crystallization of protein-ligand \npairs. Hence, it is necessary to shift our focus towards the secondary structure and other protein information.\n(6)φ\n(\nvi,vj\n)\n=\n{\nSPD (vi,vj), | vi → vj\n−1, | vi �→vj\n,\n(7)Featruepij = W (i,j)\nφ φ\n(\nvi, vj\n)\n,\n(8)e(vi,vj) =[ b 1 ,b 2 ,b 3 ],\n(9)Featruee(vi,vj) = 1\nk\nk∑\nt=1\nW edge Pt.\nNode Feature\nLinear Linear\nQK V\nMatMul\nScale Spatial Encoding\nEdge Encoding\nSoftMax\nCentrality Encoding\nMatMul\nAttention Block\nLinear\nFigure 2.  Diagrammatic representation of centrality coding, spatial coding and edge coding used for the \nstructure of drug molecules.\n6\nVol:.(1234567890)Scientific Reports |         (2024) 14:7416  | https://doi.org/10.1038/s41598-024-57879-1\nwww.nature.com/scientificreports/\nTo tackle the complexity and accessibility challenges, we employ SS and  ASA44 for representing the protein graph \nstructure. SS determines the backbone structure of the target protein, while ASA indicates the degree of contact or \nexposure of amino acid residues to the solvent in its three-dimensional structure. The interaction between non-\nadjacent residues is denoted as DM, which serves as a protein feature. The pairwise distance matrix of residues \nefficiently captures contact information in the protein structure and can be calculated using SPOT-Contact45.DM \nhas proven successful in predicting various protein spectra, such as  solubility46,  DTI47 and DTA. Contact between \ntwo non-adjacent residues occurs when their distance is less than 8 Å . However, simply vectorizing each residue in \nthe protein sequence using unique thermal encoding lacks information about element similarity and treats them \nas equal in distance. This representation also limits the model learning capability by disregarding the dependency \ninformation between residues. In many protein datasets, only a limited number of target proteins provide available \ninformation, while most of the protein information remains untapped, leading to detrimental DTA prediction results.\nThe  TAPE48 approach utilizes amino acid embeddings in a continuous vector space and employs the self-atten-\ntion mechanism of the Transformer to capture contextual relationships and information in protein sequences. \nInstead of one-hot encoding, TAPE uses embedded representations learned from unlabeled protein sequences \nto represent protein graph nodes. Fusion of embedding vectors from TAPE, secondary structure, and solvent \naccessibility feature vectors represents node features in the protein graph (see Fig.  3). Each amino acid residue \nis assigned to one of eight categories, providing detailed secondary structure information. Given a protein \nsequence of M residues, the node feature set Vp ={ vi | vi ∈ Rh}M\ni=l , where h is the length of the embedding \nvector vi provided by TAP , captures context-dependent residues. Protein secondary structure, formed by coiled \nfolding of peptide chains, contains vital information about protein activity, function, and stability, benefiting \nmodel predictions. Distance map as global structure information may be important in future DTA identifica-\ntion.47 introduced super nodes connecting other nodes in the composite structure graph.\nProposed model\nThe general architecture of our proposed method is shown in Fig. 4. Our GEFormerDTA takes the drug molecule \ngraph structure Gd and the target protein graph structure Gp as inputs and outputs the final prediction results. In \nprocessing the graph structure information, we use a graph convolutional neural network model (GCN). Our \nGEFormerDTA model consists of five main key steps: information preprocessing (Fig. 4a), drug ESC encoding \n(Fig. 4b), drug Graph encoding (Fig.  4c), drug-target protein graph early fusion (Fig.  4d), drug-target protein \ngraph refinement (Fig. 4e) and affinity scoring (Fig. 4f). In the steps of Fig. 4b,c,e, we also added residual jumps \nto slow down the generalization performance of our network.\nGEFormerDTA overview\nBefore we input the drug into the GEFormerDTA model, we need to encode the drug by two types of encoders: (1) \nESC encoder; (2) Mol. encoder. For the ESC encoder, we mainly use the global sensory field of the Transformer to \ncapture the global information of the drug molecule, while the Mol. encoder captures the main node information \nin the drug graph information. Meanwhile, we fuse the obtained protein feature maps with the drug feature \nmaps extracted by the Mol. encoder features. The fused drug-protein fusion map is fed to the drug target protein \nfractionation process to obtain the fractionated drug-protein map, and finally the results are obtained by DTA \nprediction.\nESC encoder\nAs shown in Fig. 2, after obtaining the node features, spatial position features, and edge features of the molecular \ngraph, if we use traditional attention models, we will face the challenge of high dimensionality and many \nmolecular nodes, which seriously affects the efficiency of model training. In addition, to address the issue of \nmemory overhead, we introduce the Sparsepro self-attention molecular graph encoder to extract important Q \nand reduce model complexity. Meanwhile, we use self-attention distillation to reduce feature dimensionality and \nthe number of network parameters. As shown in Fig.  4b, our drug molecule encoder is a sandwich model that \nincludes 3 layers of Sparsepro self-attention and 2 layers of GCN. Our Sparsepro self-attention can attach great \nSimilarity Score\nPo s. AR ND C… V\n10 -1 -2 -3 -2 …0\n2- 15 0- 2- 3… -3\n3- 20 61 -3 …- 3\n4- 3- 21 6- 4… -4\n5- 2- 3- 3- 49 …- 1\n…… ………………\nL0 -1 -3 -2 0… -1\naa re si du es\nPSSM\n.. PP P KLKNSH K V SPV .. .\nMF R… ST C\nM1 0. 0270 .0 28 ... 0. 0590 . 066 0. 105\nF0 . 027 10 .0 28 ...0 .0 46 0. 0460 .0 67\nR0 .0 28 0. 0281 …0 .0 44 0. 0510 .0 66\n…. .. ... ………… …\nS0 . 059 0. 0460 .0 44 ... 10 .0 81 0. 074\nT0 .0 66 0. 0460 .0 51 ...0 .0 81 10 .0 94\nC0 .1 05 0. 0670 .0 66 ...0 .0 74 0. 0941\npr ot ei ns eq uenc e\nPr otei n Sequen ce Features\n.. PP P KLKNSH KK V SPV .. .\nSecondary\nStructure\nα-helix\nβ-sh ee t\nDistance Map\nloop\nAccessible\nSurfaceA rea\nsmis core\npssm\npr of\nss8\nac c\nem b+ map\npr otei nf ut ur e\ne c n e u q e s n i e t o r p\nFigure 3.  Summary of protein features that can be used to study drug target interaction affinity.\n7\nVol.:(0123456789)Scientific Reports |         (2024) 14:7416  | https://doi.org/10.1038/s41598-024-57879-1\nwww.nature.com/scientificreports/\nimportance to atoms or edge matrices that make significant contributions, while ignoring others. Sparsepro \nself-attention can be expressed by the following mathematical formula:\nwhere ¯Q is a sparse matrix of the same size as Q, which contains only top-S queries. We compute all queries in \nQ and sort them based on the sparsity of KL scattered  points49. This paper adopts S=25 to form ¯Q and replace Q. \nThe time complexity of point-wise computation in Sparsepro self-attention is O\n(\nlnLQ\n)\n , and the memory usage \nfor each Q-K lookup and each block is O\n(\nLK lnLQ\n)49. After improving Formula (10), we obtain the following \nexpression:\nAfter inputting all the features of the drug molecule graph into the model, we employ an expression to calculate \nthe self-attentiveness of Sparsepro is\nIn addition, we set a GCN distillation operation immediately after each Sparsepro self-attentive block to prioritize \nmappings with focal features and capture the focal feature map as input at the next layer. The specific operation \nflow equation is as follows:\n(10)Atn(Q, K, V) = Softmax\n( ¯QK T\n√\nd\n)\nV,\n(11)Featruedegij = (hiW ¯Q )(hjW K )T\n√\nd\n,\n(12)Attn(i,j) = Softmax\n(\nFeatruedegij + Featruepij + Featruee(vi,vj)\n)\n(W V hi),\n(13)Xj+1 = MaxPool\n(\nELU\n(\nConv1d\n( [\nXj\n]\nops\n)))\n,\n(b)ESC Encoder\n(d)Graph Early Fusion\n(a)Data Factory (e)Refined DTG\n(f)DTA-prediction\n(c)Mol. Encoder\nFigure 4.  Diagram of the proposed model architecture. (a) is the data pre-processing stage of the proposed \nmodel. (b) is the encoder of the drug ESC. (c) is the encoder of the drug graph. (d) is our proposed graph feature \nearly fusion process. (e) is the drug-target protein graph refinement process. (f) is our DTA final prediction \nprocess.\n8\nVol:.(1234567890)Scientific Reports |         (2024) 14:7416  | https://doi.org/10.1038/s41598-024-57879-1\nwww.nature.com/scientificreports/\nwhere [·]ops denotes the output of Sparsepro self-attentive block after having some column operations, X j denotes \nthe input of the j-th self-attentive block, Conv1d denotes the 1D convolutional layer, ELU is the activation func-\ntion, and MaxPool is the maximum pooling layer.\nWe need to transform the SMILES sequence of the drug into a 2D structure by scripting before inputting the \ndrug into the GEFormerDTA model, and then we extract the atomic structure information from the 2D structure \ninformation of the drug, after which we convert the atomic information into an information encoding that can \nbe applied to the attention mechanism  by50 three encoding designs.\nMol. encoder\nFor the accuracy of model prediction, we also leverage the graph information of drug molecules as inputs to the \nmodel. This approach differs from the treatment of drug data mentioned in 2.5.2, where the atomic features of \nthe drugs (element types, atomic degrees, atomic indices, atomic implicit valence, formal charge, hybridization \ntypes) are directly fed into the Mol. encoder.\nDue to the strong affinity of GCN networks for graph information, we use the GCN neural network layer as \nthe first feature extraction network layer for drug graph information, with the mathematical expression given by\nwhere H i represents the feature matrix of the molecular graph Gd = (V d ,Ed ) for the drug, where A(N×N) denotes \nthe adjacency matrix. ˆA = ˜D−1\n2 ˜A˜D−1\n2 . represents the symmetric normalization of the adjacency matrix, where \n˜A = A + IN  , introducing self-loops to the nodes by adding the identity matrix IN  , ensuring that node features \nare included during convolution operations. ˜D = ∑\nj ˜A ij is a degree matrix used for normalizing ˜A to prevent \nthe occurrence of gradient explosions. W (i) and W (i−1) represent the weight matrices of the current layer and the \nprevious layer, respectively. σ( ·) is the ReLU activation function. Subsequently, the graph information extracted \nfrom GCN is distilled through multiple residual processes to obtain the refined feature representation of the \ndrug molecule. In mathematical terms, the residual operation is defined as\nAfter that, to reduce the network complexity and improve the training accuracy, we use the graph pooling layer to \nscale down the redundant information. Finally, after the 2-layer linear layer output of the Mol. encoder, we obtain \nthe feature representation of the drug. The mathematical formulas for the two-step operations are as follows\nwhere V′\nd represents the node features of the drug graph after the application of GCN. W i∈{0,1} and b i∈{0,1} denote \nthe weights and biases of the two linear layers, respectively. The obtained vector xd ∈ RN ′\n is referred to as the \ndrug molecule node, where N′ is the dimensionality of xd.\nDTG distillation\nAfter encoding through the Mol. encoder, a new drug graph G ′\nd  is obtained, represented as G ′\nd = (xd , E ′\nd ) , and \na protein graph Gp = (V p, Ep) . The feature fusion of these graphs forms a heterogeneous graph, resulting in an \ninformation-rich pool GDTG = (V DTG, EDTG) , where VDTG = concat (xd,Vp) and EDTG = concat (E ′\nd ,Ep) . \nThe data in these information pools are high-dimensional and redundant. To streamline our data dimensions \nand expedite model training, the DTG in the information pool will utilize GCN to capture essential feature \ninformation. Mathematically, the expression is obtained by\nThen, the DTG is subjected to dimensionality reduction using residual blocks, resulting in the refined drug-\nprotein hetero-network. Mathematically, the expression is as follows\nFinally, we separate the refined bipartite graph into drug and protein graphs using a masking approach. \nMathematically, this is expressed by\n(14)H i = f(Hi−1 ,A) = σ\n(ˆAH i−1 W (i−1) )\n,\n(15)H i =F(H i−1 ) = W i· σ\n(\nW (i−1) · f(H i−2 ,A) + b(i−1))\n,\n(16)H i =σ( F(H i−1 ) + H i−1 ),\n(17)v′\nmax =MaxPool(V′\nd),\n(18)xd =(W 0 v′\nmax + b0 )W 1 + b1 ,\n(19)H GDTG\ni = f\n(\nH GDTG\ni−1 ,AGDTG\n)\n= σ\n( ˆAGDTG H GDTG\ni−1 W (i−1 )\nGDTG\n)\n,\n(20)H GDTG\ni =F(H GDTG\ni−1 ) = W (i)\nGDTG · σ\n(\nW i−1 · f\n(\nH GDTG\ni−2 ,AGDTG\n)\n+ b(i−1)\nGDTG\n)\n,\n(21)H GDTG\ni =σ( F(H GDTG\ni−1 ) + H GDTG\ni−1 ),\n(22)Vmasked\np =Masked(V′\nDTG ),\n(23)Vmasked\nd =∼ Masked(V′\nDTG ),\n9\nVol.:(0123456789)Scientific Reports |         (2024) 14:7416  | https://doi.org/10.1038/s41598-024-57879-1\nwww.nature.com/scientificreports/\nwhere Vmasked\nd  and Vmasked\np  represent the separated sets of drug nodes and protein nodes, respectively.\nDTA score\nAt the final stage of the model, the separated bipartite graphs flow into their respective data channels, resulting \nin the drug representation X (ﬁnal+1)\nd  and the protein representation X (ﬁnal+1)\np  . The mathematical expressions \nare given by\nTo improve predictive accuracy, we combine the drug features before feature fusion with those obtained after the \nseparation of the bipartite graph. This integration results in a new set of drug features. Subsequently, we employ a \nfully connected block to concatenate these drug features with protein features for the prediction of protein-drug \naffinity values. Mathematically, the expression is formulated as\nResults and discussion\nEvaluation indicators\nMany metrics exist for assessing model performance and capacity in current research in the DTA/DTI field. \nHowever, the selection of different metrics for different research questions with different contextual information \noften leads to different measures. Therefore, we use mean squared error (MSE), root mean square error (RMSE), \nPearson, Spearman, consistency index (CI)51 and r2 (coefficient of determination) to assess the performance of \nour models.\nMSE: MSE is used to measure the squared average difference between the model’s predicted values and the \nactual observed values. For a set of actual observed values (or target values) yi and their corresponding predicted \nvalues (or model outputs) ˆyi , the calculation of MSE is as follows:\nRMSE: A measure of the square root of the mean squared difference between the predicted and actual values.\nPearson: Measures the linear correlation between the predicted value X and the underlying true value Y .\nwhere, cov(X,Y) is the covariance between the predicted value and the underlying fact, σ( X) is the standard \ndeviation of X, and σ( Y) is the standard deviation of Y . µX , µY are the mean values of the distributions of X,Y , \nrespectively.\nSpearman: A statistic obtained by arranging the sample values of two random variables in order of their data \nmagnitude, using the ranks of the individual sample values instead of the actual data.\nwhere R (ˆyi) is the predicted value ranking, R (yi) is the true value ranking, R(ˆy) is the average of the predicted \nvalue ranking, and R(y) is the average of the true value ranking, A = R(yi) − R(y) , B = R ( ˆyi) − R (ˆy).\nCI: Measures the probability of correctly predicting unequal pairs according to the order.\n(24)X ﬁnal\nd =concat(X j+1 ,G (V masked\nd ,E masked\nd )),\n(25)X ﬁnal+1\nd =MaxPool(X ﬁnal\nd ),\n(26)X ﬁnal\np =G\n(\nV masked\np , E masked\np\n)\n,\n(27)X ﬁnal+1\np =Linear\n(\nReLU\n(\nPooling(X ﬁnal\np )\n))\n,\n(28)DTAscore = Linear\n(\nconcat\n(\nX ﬁnal+1\nd ,X ﬁnal+1\np\n))\n,\n(29)MSE = 1\nn\nn∑\ni=1\n(yi − ˆyi)2 ,\n(30)RMSE =\n√1\nn\nn∑\ni=1\n(yi− ˆyi)2 ,\n(31)Pearson= E(XY ) − E(X )E(Y )√\nE(X 2 ) − E2 (X )\n√\nE(Y 2 ) + E2 (Y )\n,\n(32)ρ =\n1\nn\n∑ n\ni=1 A · B√ (1\nn\n∑ n\ni=1 A 2 )\n·\n(1\nn\n∑ n\ni=1 B2 ),\n(33)CI = 1\nZ\n∑\nδi>δj\nh\n(\nxi − xj\n)\n,\n10\nVol:.(1234567890)Scientific Reports |         (2024) 14:7416  | https://doi.org/10.1038/s41598-024-57879-1\nwww.nature.com/scientificreports/\nwhere x i is the predicted value of the larger affinity δi , x j is the predicted value of the smaller affinity δj , Z is the \nnumber of unequal pairs as the normalization constant, and h(x) is the step  function33:\nThis metric measures whether the predicted binding affinity values for any drug-target pair are predicted in \nthe same order as their true values. We used paired t-tests to perform statistical significance tests with 95% \nconfidence intervals.\nr2 : Given the varying scales of different datasets, it’s challenging to compare them using metrics like MSE and \nRMSE mentioned above. This metric calculates the R2 value with a reference to the mean model for comparing \nthe quality of models. The formula for calculating the r2 is as follows:\nwhere ˆyi is the predicted value, yi the real value, and y the mean of the real values.\nExperiment setup\nWe evaluate the performance of our proposed model on benchmark  datasets31,32. We will use the same nested \ncross-validation as the  DeepDTA21 method to determine the best parameters for the validation and test sets. \nTo train the generalized linear model with enhanced generalization, we randomly partition the dataset into \n6 equal parts (4:1:1), designating one part as the independent test set. The remaining parts are utilized for \nhyperparameter tuning through 5-fold cross-validation. We conducted special processing for the KIBA dataset. \nTo accelerate model training, we divided the KIBA dataset into four parts and trained each of the four subsets \nwith identical parameters.  KronRLS33,  Simboost14, and others use folds with the same settings as the training, \ntest, and validation sets for a fair comparison.\nWe set different filter sizes for drug compounds and proteins instead of generic sizes for the experiments \nbecause they have different contextual representations. In Table  2, the hyperparameter combination \ncorresponding to the best CI score provided on the validation set is selected as the best hyperparameter \ncombination for modeling the test set.\nResults\nComparison experiments\nIn Tables 3 and 4, KronRLS, SimBoost, DeepDTA, and DeepCDA are mainly based on token-based SMILES \nrepresentations and token-based FASTA sequence representations, while GraphDTA-GCNNet, GraphDTA-\nGINNet, GLFA, and GEFA are mainly base on representations of drug graphs or protein graphs.\nIn Table 3, We report some work on Transformer graph early fusion methods on the benchmark datasets \nDavis and KIBA. Our proposed method achieves the best performance among all listed methods, which is in \nline with our expectations. To validate the validity and feasibility of the GEFormerDTA method, we evaluated \nand compared the predictive accuracy of different state-of-the-art binding affinity regression models. The per -\nformance of the GEFormerDTA model compared with existing baseline models on the Davis independent test \nset is depicted in Table 3. The proposed method achieved good results in three of the six metrics. The change in \nthe CI metric is less significant compared to the best-performing existing methods, showing an improvement \nof only 0.4 percentage points. The Pearson correlation coefficient and r2 value increased by 3.2 and 2.3% points, \nrespectively. Our ESC drug encoder fully uses information such as atomicity center encoding, chemical bond \nencoding, and spatial information encoding in the drug feature map. MSE, RMSE, and Pearson did not yield \nsatisfactory results, being 1.7, 1.8, and 15 percentage points lower than the optimal performance across all base-\nlines, respectively. Transformer has global information awareness, which is very beneficial to obtain complete \n(34)h(x)=\n{ 1, if x > 0\n0.5, if x = 0\n0, if x < 0\n,\n(35)r2 = 1 −\n∑\ni\n(\nˆyi − yi\n)2\n∑\ni\n(\ny − yi\n)2 ,\nTable 2.  Hyperparameter settings for GEFormerDTA.\nParameters Value\nNumber of res. blocks [2; 3; 4]\nNumber of GCNConv. Blocks [1; 2]\nNUM_EPOCHS 2000\nHidden Neurons [256; 512]\nTRAIN_BATCH_SIZE [64; 128]\nTEST_BATCH_SIZE 256\nDROPOUT [0.2; 0.5]\nOPTIMIZER Adam\nLR [0.0005; 0.001; 0.01]\n11\nVol.:(0123456789)Scientific Reports |         (2024) 14:7416  | https://doi.org/10.1038/s41598-024-57879-1\nwww.nature.com/scientificreports/\ndrug features containing richer informa- tion than GCN. This also demonstrates the advantage of applying a \nTransformer to graph problems.\nTable 4 compares the performance of the GEFormerDTA model with the existing baseline model using the \nKIBA independent test set. We conducted experiments with our model on four subsets of the KIBA dataset. The \nproposed method showed good performance in the split3 subset, achieving strong results across four metrics \n(MSE = 0.06, RMSE = 0.244, Spearman = 0.884, r2 = 0.898). The CI metric performed best in the split2 subset \nwith a value of 0.896. Our model, GraphDTA-GINNet, achieved the best result in the Pearson metric, with a score \nof 0.872. Compared to the highest levels of existing methods, the change in the Pearson metric is minimal, with \nan improvement of only 0.16% points in the split1 subset. The maximum improvement in the r2 metric, when \ncompared to other models, is 4.5% points. In Table 4, GEFormerDTA outperforms baseline models in terms of \nperformance, and the comparison with GEFA in Table 3 highlights the reliability and effectiveness of drug encod-\ning in our method. In recent articles, CI has been used as the primary evaluation metric in models. Although we \ndid not achieve the best performance in some metrics, our model achieved the best CI on two datasets.\nTo visually represent the predictive performance of our model, Fig. 5a illustrates the fit of the predicted bind-\ning affinity values to the true values on the Davis dataset. The scatter plot shows that data points are distributed \non both sides of the line ˆy = y , indicating a reasonable fit. Figure 5b displays the kernel density estimates of the \npredicted binding affinity values compared to the true values. The dense distribution of curves suggests a high \nTable 3.  Predicted binding affinity for the Davis independent test set (“underlined” means suboptimal; \n“bolded” means optimal). * Reference original data.\nMethod ↓MSE ↓RMSE ↑Pearson ↑Spearman ↑CI ↑ r2\nKronRLS∗25 0.443 0.665 – 0.624 0.847 0.473\nSimBoost∗25 0.277 0.526 – 0.694 0.891 0.670\nDeepDTA21 0.196 0.442 0.850 0.845 0.866 0.712\nDeepCDA∗22 0.248 – 0.857 – 0.891 ± 0.003 0.649 ± 0.009\nGraphDTA-GCNNet51 0.293 0.541 0.797 0.660 0.863 0.635\nGraphDTA-GINNet51 0.261 0.511 0.821 0.691 0.884 0.674\nGLFA52 0.241 0.491 0.839 0.693 0.886 0.699\nGEFA52 0.250 0.500 0.832 0.69460 0.887 0.688\nGEFormerDTA 0.212 0.461 0.889 0.69465 0.895 0.735\nTable 4.  Predicted binding affinity for the KIBA independent test set (“underlined” means suboptimal; \n“bolded” means optimal). * Reference original data.\nMethod ↓MSE ↓RMSE ↑Pearson ↑Spearman ↑CI ↑ r2\nKronRLS∗22 0.411 – – – 0.782 ± 0.0009 0.342 ± 0.001\nSimBoost∗22 0.222 – – – 0.836 ± 0.001 0.629 ± 0.007\nDeepDTA21 0.082 0.287 0.710 0.645 0.849 0.504\nDeepCDA∗22 0.176 – 0.855 – 0.889 ± 0.002 0.682 ± 0.008\nGraphDTA-GCNNet51 0.188 0.433 0.856 0.845 0.862 0.724\nGraphDTA-GINNet51 0.163 0.404 0.872 0.863 0.873 0.760\nGLFA52\nsplit_avg 0.215 0.463 0.822 0.826 0.858 0.673\nsplit 1 0.227 0.476 0.829 0.818 0.852 0.685\nsplit 2 0.226 0.476 0.850 0.842 0.867 0.719\nsplit 3 0.187 0.432 0.827 0.835 0.862 0.679\nsplit 4 0.221 0.470 0.782 0.808 0.851 0.609\nGEFA52\nsplit_avg 0.217 0.466 0.821 0.820 0.855 0.669\nsplit 1 0.236 0.486 0.822 0.809 0.849 0.671\nsplit 2 0.220 0.469 0.852 0.840 0.864 0.725\nsplit 3 0.191 0.438 0.826 0.831 0.862 0.673\nsplit 4 0.222 0.471 0.783 0.800 0.844 0.607\nGEFormerDTA\nsplit_avg 0.081 0.284 0.835 0.871 0.877 0.844\nsplit 1 0.091 0.302 0.821 0.8819 0.870 0.805\nsplit 2 0.099 0.314 0.832 0.8817 0.896 0.819\nsplit 3 0.060 0.244 0.851 0.884 0.879 0.898\nsplit 4 0.076 0.276 0.837 0.839 0.864 0.854\n12\nVol:.(1234567890)Scientific Reports |         (2024) 14:7416  | https://doi.org/10.1038/s41598-024-57879-1\nwww.nature.com/scientificreports/\ndegree of data density. The circular curves generally have an oval shape, and their long axes roughly align with \nthe curve ˆy = y.\nFigures 6 and 7 show the performance comparison of our method with other methods on two gold standard \ndatasets. As can be seen from the figure, the CI metric improves on both datasets compared to the baseline \nmodel. Among the six evaluation metrics, the proposed method significantly improves Pearson on four subsets \nof KIBA. In contrast, on the Davis data set, the improvement of r2 is more obvious, which shows that our model \nhas stronger generalization ability on the Davis data set.\nFigure 5.  (a) Linear regression fitted straight lines for true and predicted values on the Davis dataset. (b) Kernel \ndensity estimation plots of the true and predicted values on the Davis data set. where the horizontal coordinates \nindicate the true binding affinity, and the vertical coordinates indicate the predicted binding affinity. The upper \nand right bars show the distribution characteristics of the sample size.\n0.000\n0.100\n0.200\n0.300\n0.400\n0.500\n0.600\n0.700\n0.800\n0.900\nMSER MSEP earson Spearman CI R_2\nKronRLS SimBoost DeepDTA\nDeepCDA GraphDTA-GCNNet GraphDTA-GINNet\nGLFA GEFA GEFormerDTA\nFigure 6.  Comparison of the levels of our method and other methods on the Davis dataset under the six-\nevaluation metrics.\n13\nVol.:(0123456789)Scientific Reports |         (2024) 14:7416  | https://doi.org/10.1038/s41598-024-57879-1\nwww.nature.com/scientificreports/\nAblation studies\nIt is well known that the way drug data are encoded is important for the predictive performance of the model \nduring the study of DTA. To verify the importance of each substructure of drug coding in the drug preprocess-\ning stage and the effect on the model performance, we performed ablation experiments on each substructure. In \nTable 5, the GEFormerDTA model without encoding substructures (first three rows) all performed worse than the \nmodel with both three encoding substructures. The GEFormerDTA model without protein secondary structure \nand accessible surface area feature encoding (fourth row) perform worse than the model with protein structural \nfeatures. This is enough to show that the protein structure has a positive effect on improving the performance of \nthe proposed model. In order to visually represent the progress of centroid encoding, edge encoding, and spatial \nencoding more intuitively, we present the results from Table 5 in the form of bar charts in Fig. 8.\nConclusion\nIn this paper, we propose a novel deep learning approach using Transformer to solve graph structure data to solve \nthe problem of drug affinity prediction, which can accelerate the development of physical drugs and repurposing \nof old drugs. After our analysis of model prediction results, we found that GEFormerDTA is very effective in \n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nMSER MSEP earsonS pearmanC IR _2\nKronRLS SimBoost DeepDTA DeepCDA GraphDTA-GCNNet\nGraphDTA-GINNet GLFA split_avg GLFA split1 GLFA split2 GLFA split3\nGLFA split4 GEFA split_avg GEFA split1 GEFA split2 GEFA split3\nGEFA split4 GEFormerDTA split_avg GEFormerDTA split1 GEFormerDTA split2 GEFormerDTA split3\nGEFormerDTA split4\nFigure 7.  Comparison of the levels of our method and other methods on the KIBA dataset under the six-\nevaluation metrics.\nTable 5.  Ablation experiments based on drug coding modalities in the Davis independent prediction dataset \n(“underlined” means suboptimal; “bolded” means optimal).\nMethod ↓MSE ↓RMSE ↑Pearson ↑Spearman ↑CI ↑ r2\nGEFormerDTA_with_DegreeC 0.3383 0.5816 0.8366 0.6436 0.8605 0.5779\nGEFormerDTA_with_SpatialP 0.3404 0.5834 0.8346 0.6341 0.8539 0.5753\nGEFormerDTA_with_Edge 0.3315 0.5758 0.8436 0.6494 0.8636 0.5864\nGEFormerDTA_withou_SS_ASA 0.3364 0.5800 0.8411 0.6493 0.8641 0.5803\nGEFormerDTA 0.2124 0.4609 0.8885 0.6946 0.8947 0.7350\n14\nVol:.(1234567890)Scientific Reports |         (2024) 14:7416  | https://doi.org/10.1038/s41598-024-57879-1\nwww.nature.com/scientificreports/\ngrasping drug molecule graph structure information (degree centrality encoding, atomic space encoding and \nedge encoding) without prior knowledge for model performance improvement.\nConsidering the potential representation changes due to protein metastability during the binding process, \nwe use an early fusion approach between drug and target. The early fusion technique transforms the parallel \nprocessing of drug and protein into a serial processing of affinity problems by integrating drug representation \ninto protein representation co-learning. The interpretability of the model can be enhanced by using the self-\nattentive values of the hidden features of protein nodes as edge weights connecting drug nodes and residue nodes \nin the target protein graph, which quantifies which residues play a role for the binding process and the level of \ncontribution of each residue. Early fusion is shown to be more competitive than late fusion by comparative tests. \nExploiting the molecular map structural information of a drug is more advantageous than solo thermal coding. \nExperiments show that our method outperforms other advanced methods.\nHowever, there are still many potential limitations to our current work. Our approach has not been able \nto address the conformational changes caused by protein-drug contact. The study of protein conformational \nchanges is an important area of current biological research, which provides the basis for in-depth exploration in \nthe life sciences. In addition, the study of protein conformational change mechanisms may also have important \nimplications for drug development, disease prevention and control, and health management. Therefore, there \nis still great potential and space for future research in protein conformational changes. If we can learn residue-\nedge attachment changes, we can explain the conformational changes arising from drug-protein binding. Our \napproach is portable and scalable. In the prediction of protein-RNA interactions, we can share the structural \ncoding information of some of the proteins in our work and additionally can incorporate the electrostatic patch \n(EP) information of the proteins.\nData availability\nThe datasets generated and/or analysed during the current study are available in the github repository, https://  \ngithub. com/ CellN est/ GEFor merDTA/.\nReceived: 28 November 2023; Accepted: 22 March 2024\nReferences\n 1. Bolten, B. M. & DeGregorio, T. Trends in development cycles. Nat. Rev. Drug Discov. 1, 335 (2002).\n 2. van der Schans, S. et al. The impact of patent expiry on drug prices: Insights from the Dutch market. J. Mark. Access Health Policy \n9, 1849984 (2021).\n 3. Martens, E. & Demain, A. L. The antibiotic resistance crisis, with a focus on the united states. J. Antibiot. 70, 520–526 (2017).\n 4. Mittal, P ., Chopra, H., Kaur, K. P . & Gautam, R. K. New drug discovery pipeline. In Computational Approaches in Drug Discovery, \nDevelopment and Systems Pharmacology, 197–222 (Elsevier, 2023).\n 5. Khot, S., Naykude, S. & Adnaik, P . An overview of drug drug development process. J. Pharma Insights Res. 1, 067–074 (2023).\n 6. Pagadala, N. S., Syed, K. & Tuszynski, J. Software for molecular docking: A review. Biophys. Rev. 9, 91–102 (2017).\n0.0000\n0.1000\n0.2000\n0.3000\n0.4000\n0.5000\n0.6000\n0.7000\n0.8000\n0.9000\nMSER MSEP earson Spearman CI R_2\nGEFormerDTA_with_DegreeC GEFormerDTA_with_Edge\nGEFormerDTA_withou_SS_ASA GEFormerDTA\nFigure 8.  Comparison of the levels of our method and other methods on the Davis dataset under the six-\nevaluation metrics.\n15\nVol.:(0123456789)Scientific Reports |         (2024) 14:7416  | https://doi.org/10.1038/s41598-024-57879-1\nwww.nature.com/scientificreports/\n 7. Bagherian, M. et al. Machine learning approaches and databases for prediction of drug-target interaction: A survey paper. Brief. \nBioinform. 22, 247–269 (2021).\n 8. Wang, K. et al. Prediction of drug-target interactions for drug repositioning only based on genomic expression similarity. PLoS \nComput. Biol. 9, e1003315 (2013).\n 9. Mutowo, P . et al. A drug target slim: Using gene ontology and gene ontology annotations to navigate protein-ligand target space \nin chembl. J. Biomed. Semant. 7, 1–7 (2016).\n 10. Zhu, S., Okuno, Y ., Tsujimoto, G. & Mamitsuka, H. A probabilistic model for mining implicit ‘chemical compound-gene’ relations \nfrom literature. Bioinformatics 21, ii245–ii251 (2005).\n 11. Lee, A., Lee, K. & Kim, D. Using reverse docking for target identification and its applications for drug discovery. Expert Opin. Drug \nDiscov. 11, 707–715 (2016).\n 12. Fan, J., Fu, A. & Zhang, L. Progress in molecular docking. Quant. Biol. 7, 83–89 (2019).\n 13. Li, J., Fu, A. & Zhang, L. An overview of scoring functions used for protein-ligand interactions in molecular docking. Interdiscip. \nSci. Comput. Life Sci. 11, 320–328 (2019).\n 14. He, T., Heidemeyer, M., Ban, F ., Cherkasov, A. & Ester, M. Simboost: A read-across approach for predicting drug-target binding \naffinities using gradient boosting machines. J. Cheminf. 9, 1–14 (2017).\n 15. Cichonska, A. et al. Learning with multiple pairwise kernels for drug bioactivity prediction. Bioinformatics 34, i509–i518 (2018).\n 16. Chu, Y . et al. Dti-cdf: A cascade deep forest model towards the prediction of drug-target interactions based on hybrid features. \nBrief. Bioinform. 22, 451–462 (2021).\n 17. Pliakos, K., Vens, C. & Tsoumakas, G. Predicting drug-target interactions with multi-label classification and label partitioning. \nIEEE/ACM Trans. Comput. Biol. Bioinf. 18, 1596–1607 (2019).\n 18. Chu, Y . et al. Dti-mlcd: Predicting drug-target interactions using multi-label learning with community detection method. Brief. \nBioinform. 22, bbaa205 (2021).\n 19. Karimi, M., Wu, D., Wang, Z. & Shen, Y . Deepaffinity: Interpretable deep learning of compound-protein affinity through unified \nrecurrent and convolutional neural networks. Bioinformatics 35, 3329–3338 (2019).\n 20. Sutskever, I., Vinyals, O. & Le, Q. V . Sequence to sequence learning with neural networks. Adv. Neural. Inf. Process. Syst. 27, 1–9 \n(2014).\n 21. Öztürk, H., Özgür, A. & Ozkirimli, E. Deepdta: Deep drug-target binding affinity prediction. Bioinformatics 34, i821–i829 (2018).\n 22. Abbasi, K. et al. Deepcda: Deep cross-domain compound-protein affinity prediction through lstm and convolutional neural \nnetworks. Bioinformatics 36, 4633–4642 (2020).\n 23. Jiang, M. et al. Drug-target affinity prediction using graph neural network and contact maps. RSC Adv. 10, 20701–20712 (2020).\n 24. Li, T., Zhao, X.-M. & Li, L. Co-vae: Drug-target binding affinity prediction by co-regularized variational autoencoders. IEEE Trans. \nPattern Anal. Mach. Intell. 44, 8861–8873 (2021).\n 25. Monteiro, N. R., Oliveira, J. L. & Arrais, J. P . Dtitr: End-to-end drug-target binding affinity prediction with transformers. Comput. \nBiol. Med. 147, 105772 (2022).\n 26. Hirohara, M., Saito, Y ., Koda, Y ., Sato, K. & Sakakibara, Y . Convolutional neural network based on smiles representation of com-\npounds for detecting chemical motif. BMC Bioinformatics 19, 83–94 (2018).\n 27. Gao, H., Wang, Z. & Ji, S. Large-scale learnable graph convolutional networks. In Proceedings of the 24th ACM SIGKDD international \nconference on knowledge discovery & data mining, 1416–1424 (2018).\n 28. Park, C., Park, J. & Park, S. Agcn: Attention-based graph convolutional networks for drug–drug interaction extraction. Expert \nSyst. Appl. 159, 113538 (2020).\n 29. Zhao, B.-W . et al. igrldti: An improved graph representation learning method for predicting drug-target interactions over hetero-\ngeneous biological information network. Bioinformatics 39, btad451 (2023).\n 30. Zhao, B.-W . et al. Fusing higher and lower-order biological information for drug repositioning via graph representation learning. \nIEEE Trans. Emerg. Top. Comput. 1, 1–14 (2023).\n 31. Davis, M. I. et al. Comprehensive analysis of kinase inhibitor selectivity. Nat. Biotechnol. 29, 1046–1051 (2011).\n 32. Tang, J. et al. Making sense of large-scale kinase inhibitor bioactivity data sets: A comparative and integrative analysis. J. Chem. \nInf. Model. 54, 735–743 (2014).\n 33. Pahikkala, T. et al. Toward more realistic drug-target interaction predictions. Brief. Bioinform. 16, 325–337 (2015).\n 34. Shen, C., Luo, J., Ouyang, W ., Ding, P . & Chen, X. Iddkin: Network-based influence deep diffusion model for enhancing prediction \nof kinase inhibitors. Bioinformatics 36, 5481–5491 (2020).\n 35. Landrum, G. et al. Rdkit: A software suite for cheminformatics, computational chemistry, and predictive modeling. Greg Landrum \n8, 31 (2013).\n 36. Lovrić, M., Molero, J. M. & Kern, R. Pyspark and rdkit: moving towards big data in cheminformatics. Mol. Inf. 38, 1800082 (2019).\n 37. Vaswani, A. et al. Attention is all you need. Adv. Neural. Inf. Process. Syst. 30, 6000–6010 (2017).\n 38. Raffel, C. et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. 21, 5485–5551 \n(2020).\n 39. Hu, W . et al. Open graph benchmark: Datasets for machine learning on graphs. Adv. Neural. Inf. Process. Syst. 33, 22118–22133 \n(2020).\n 40. Xu, K., Hu, W ., Leskovec, J. & Jegelka, S. How powerful are graph neural networks? International Conference on Learning Repre-\nsentations 1–17 (2018).\n 41. Cheng, Z., Zhao, Q., Li, Y . & Wang, J. Iifdti: Predicting drug-target interactions through interactive and independent features based \non attention mechanism. Bioinformatics 38, 4153–4161 (2022).\n 42. Böckmann, A. 3d protein structures by solid-state nmr spectroscopy: Ready for high resolution. Angew. Chem. Int. Ed. 47, 6110–\n6113 (2008).\n 43. Heffernan, R., Y ang, Y ., Paliwal, K. & Zhou, Y . Capturing non-local interactions by long short-term memory bidirectional recur-\nrent neural networks for improving prediction of protein secondary structure, backbone angles, contact numbers and solvent \naccessibility. Bioinformatics 33, 2842–2849 (2017).\n 44. Hanson, J., Paliwal, K., Litfin, T., Y ang, Y . & Zhou, Y . Accurate prediction of protein contact maps by coupling residual two-\ndimensional bidirectional long short-term memory with convolutional neural networks. Bioinformatics 34, 4039–4045 (2018).\n 45. Chen, J., Zheng, S., Zhao, H. & Y ang, Y . Structure-aware protein solubility prediction from sequence through graph convolutional \nnetwork and predicted contact map. J. Cheminf. 13, 1–10 (2021).\n 46. Zheng, S., Li, Y ., Chen, S., Xu, J. & Y ang, Y . Predicting drug-protein interaction using quasi-visual question answering system. Nat. \nMach. Intell. 2, 134–140 (2020).\n 47. Rao, R. et al. Evaluating protein transfer learning with tape. Adv. Neural. Inf. Process. Syst. 32, 9689 (2019).\n 48. Zhou, H. et al. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI confer-\nence on artificial intelligence 35, 11106–11115 (2021).\n 49. Ying, C. et al. Do transformers really perform badly for graph representation?. Adv. Neural. Inf. Process. Syst. 34, 28877–28888 \n(2021).\n 50. Gönen, M. & Heller, G. Concordance probability and discriminatory power in proportional hazards regression. Biometrika  92, \n965–970 (2005).\n16\nVol:.(1234567890)Scientific Reports |         (2024) 14:7416  | https://doi.org/10.1038/s41598-024-57879-1\nwww.nature.com/scientificreports/\n 51. Nguyen, T. et al. Graphdta: predicting drug-target binding affinity with graph neural networks. Bioinformatics  37, 1140–1147 \n(2021).\n 52. Nguyen, T. M., Nguyen, T., Le, T. M. & Tran, T. Gefa: Early fusion approach in drug-target affinity prediction. IEEE/ACM Trans. \nComput. Biol. Bioinf. 19, 718–728 (2021).\nAcknowledgements\nThe research reported in this publication was supported by National Natural Science Foundation of China \nthrough grant awards Nos. 62002206.\nAuthor contributions\nConceptualization, Y .L. and L.X.; investigation, L.Z., M.G. and H.C.; methodology, Y .L.; writing—original draft, \nY .L.; writing—review and editing, L.X., L.Z., H.C. and M.G.; funding acquisition, L.X. All authors have read and \nagreed to the published version of the manuscript.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to L.X.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2024"
}