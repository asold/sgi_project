{
  "title": "Chain-of-Thought Improves Text Generation with Citations in Large Language Models",
  "url": "https://openalex.org/W4393147153",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2119925542",
      "name": "Bin Ji",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A1990448198",
      "name": "Huijun Liu",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2788560774",
      "name": "Mingzhe Du",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A4207321635",
      "name": "See-Kiong Ng",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2119925542",
      "name": "Bin Ji",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1990448198",
      "name": "Huijun Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2788560774",
      "name": "Mingzhe Du",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4207321635",
      "name": "See-Kiong Ng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4385571034",
    "https://openalex.org/W4385573898",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4362707120",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W2950681488",
    "https://openalex.org/W4366341216",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4281736354",
    "https://openalex.org/W4385572290",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4389518771",
    "https://openalex.org/W4377121468",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4389518954",
    "https://openalex.org/W4389524308",
    "https://openalex.org/W4389520670",
    "https://openalex.org/W4385573970",
    "https://openalex.org/W2593831809"
  ],
  "abstract": "Previous studies disclose that Large Language Models (LLMs) suffer from hallucinations when generating texts, bringing a novel and challenging research topic to the public, which centers on enabling LLMs to generate texts with citations. Existing work exposes two limitations when using LLMs to generate answers to questions with provided documents: unsatisfactory answer correctness and poor citation quality. To tackle the above issues, we investigate using Chain-of-Thought (CoT) to elicit LLMs’ ability to synthesize correct answers from multiple documents, as well as properly cite these documents. Moreover, we propose a Citation Insurance Mechanism, which enables LLMs to detect and cite those missing citations. We conduct experiments on the ALCE benchmark with six open-source LLMs. Experimental results demonstrate that: (1) the CoT prompting strategy significantly improves the quality of text generation with citations; (2) the Citation Insurance Mechanism delivers impressive gains in citation quality at a low cost; (3) our best approach performs comparably as previous best ChatGPT-based baselines. Extensive analyses further validate the effectiveness of the proposed approach.",
  "full_text": "Chain-of-Thought Improves Text Generation with Citations in Large Language\nModels\nBin Ji, Huijun Liu*, Mingzhe Du, See-Kiong Ng\nNational University of Singapore\n{jibin, mingzhe, seekiong}@nus.edu.sg, liuhuijun01@gmail.com\nAbstract\nPrevious studies disclose that Large Language Models\n(LLMs) suffer from hallucinations when generating texts,\nbringing a novel and challenging research topic to the pub-\nlic, which centers on enabling LLMs to generate texts with\ncitations. Existing work exposes two limitations when us-\ning LLMs to generate answers to questions with provided\ndocuments: unsatisfactory answer correctness and poor cita-\ntion quality. To tackle the above issues, we investigate us-\ning Chain-of-Thought (CoT) to elicit LLMs’ ability to syn-\nthesize correct answers from multiple documents, as well as\nproperly cite these documents. Moreover, we propose a Ci-\ntation Insurance Mechanism, which enables LLMs to detect\nand cite those missing citations. We conduct experiments on\nthe ALCE benchmark with six open-source LLMs. Experi-\nmental results demonstrate that: (1) the CoT prompting strat-\negy significantly improves the quality of text generation with\ncitations; (2) the Citation Insurance Mechanism delivers im-\npressive gains in citation quality at a low cost; (3) our best ap-\nproach performs comparably as previous best ChatGPT-based\nbaselines. Extensive analyses further validate the effective-\nness of the proposed approach.\nIntroduction\nLarge Language Models (LLMs) have gained extensive re-\nsearch attention due to their powerful capability to under-\nstand human instructions and generate responses accord-\ningly (Zhao et al. 2023). However, these responses may\nbe unreliable since LLMs suffer from hallucinations, which\nputs forward demands to authenticate their factuality. Aimed\nat this issue, the research topic of enabling LLMs to gener-\nate texts with citations is brought to the public (Gao et al.\n2023). It is easy for a human being to verify the factuality of\nLLMs’ generations by checking their citations.\nThe majority of existing studies focus on adding citations\nto LLMs’ generations in the scenarios of commercial search\nengines, e.g., Bing Chat1, and they use closed-source LLMs\nlike ChatGPT2 and GPT-4 (OpenAI 2023). Unfortunately, it\nis hard to automatically evaluate the generation correctness\n*Corresponding author\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n1https://www.bing.com/new\n2https://chat.openai.com/chat\nQ: When did the US break away from England?\nQuestion\nThe US took the first step towards gaining independence from GB when \nit declared independence on July 2, 1776 (although the event is now \ncommemorated on July 4, 1776, the date when the Declaration of \nIndependence was officially adopted by Congress)[1][2]. The Treaty \nof Paris was later signed on September 3, 1783, formally separating the \nUnited States from the British Empire[3].\nGold Answer\nRetrieved Documents\n[1] Decolonization of \nthe Americas: ... \nindependence from GB \non July 2, 1776 ...\n[2] Decolonization: ... \nIt was officially adopted \nby Congress on July 4, \n1776 ...\n[3] American Revo- \nlution: ... The Treaty of \nParis was signed \nSeptember 3, 1783 ...\n...\nCorpus\nThe US took the first step \ntowards gaining \nindependence from GB when \nit declared independence on \nJuly 2, 1776[1]. The Treaty \nof Paris was later signed on \nSeptember 3, 1783, formally \nseparating the United States \nfrom the British Empire[3].\nCase #1\nThe US took the first step towards gaining \nindependence from GB when it declared \nindependence on July 2, 1776 (although \nthe event is now commemorated on July \n4, 1776, the date when the Declaration of \nIndependence was officially adopted by \nCongress)[1][2]. The Treaty of Paris \nwas later signed on September 3, 1783, \nformally separating the United States \nfrom the British Empire.\nCase #2\nLLM\nFigure 1: Case studies of LLMs’ outputs. We re-use the\nALCE example (Gao et al. 2023). Case #1 shows that the\nLLM fails to consider the document [2] in its generated\nanswer. Case #2 exhibits that the LLM misses citing the doc-\nument [3].\nand the citation quality in these scenarios, which instead\ncalls for low-efficiency and high-cost human evaluations\n(Liu, Zhang, and Liang 2023). Motivated by the above fact,\nGao et al. (2023) propose the first reproducible benchmark,\nALCE, to automatically evaluate LLMs’ generations with\ncitations. ALCE consists of three datasets and defines three\nautomatic evaluation metrics, i.e., Fluency, Correctness, and\nCitation Quality. Unlike those search engine-based studies,\nALCE assumes natural-language questions paired with rel-\nevant retrieval corpora, which necessitates comprehensive\nsystems to generate answers to questions using retrieved\ndocuments, as well as properly cite these documents. A se-\nries of LLMs, e.g., ChatGPT, LLaMA (Touvron et al. 2023),\nVicuna (Chiang et al. 2023), and Oasst (K ¨opf et al. 2023),\nhave shown their exceptional abilities on ALCE. However,\nthey still expose several limitations. One of the primary lim-\nitations lies in that they struggle to synthesize multiple doc-\numents in context and properly cite them (Gao et al. 2023),\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18345\nwhich in turn leads to unsatisfactory answer correctness and\npoor citation quality. We report two case studies in Figure 1.\nWe claim that the lack of generation process instruction is\none cause of the unsatisfactory answer correctness and poor\ncitation quality. Existing studies directly instruct LLMs to\ngenerate answers without providing them with the genera-\ntion process like the Human Thought Process. For the ALCE\nbenchmark, given a question and its retrieved documents, the\nHuman Thought Process can be briefly formalized as (1) un-\nderstand the question and the documents, and then find out\nrelevant documents that can answer the question 3; (2) for\neach relevant document, generate a statement accordingly to\nanswer the question from some aspects and properly cite the\ndocument; (3) merge, rank, and concatenate these generated\nstatements to obtain the final answer.\nIntending to improve answer correctness and citation\nquality, we explore using Chain-of-Thought (CoT) (Wei\net al. 2022; Wang et al. 2023b) prompting to boost LLMs’\nabilities in generating answers to questions with citations. To\nbe specific, we propose CoT prompting strategies, achieved\nby instructing LLMs to mimic the Human Thought Process\nwhen generating answers. Moreover, we find that whether\nCoT prompts are employed or not, LLMs, including Chat-\nGPT and GPT-4, are still prone to not cite any document\nin their generations, harming the citation quality. To tackle\nthis issue, we propose Citation Insurance Mechanism (CIM)\nto detect those statements without any citation and add ci-\ntations to them. We explore two different CIM implementa-\ntions, i.e., LLM-based CIM and IR-based CIM4.\nWe evaluate our approach on six open-source LLMs with\nparameter scales ranging from 13 billion to 70 billion. For\neach LLM, we explore using CoT to enhance four prompt\nstrategies. Experimental results on the ALCE benchmark\ndemonstrate that: (1) the CoT prompting strategy signifi-\ncantly boosts the quality of text generation with citations,\nestablishing strong and reproducible baselines for future\nstudy; (2) the proposed CIM improves citation quality at\na low cost; (3) our approach based on LLaMA-2-70B-\nChat achieves comparable performance as previous state-of-\nthe-art ChatGPT-based baselines. Quantitative and qualita-\ntive analyses further validate the effectiveness of the CoT\nprompting strategy and the CIM. The main contributions are\nsummarized as follows:\n(1) By mimicking human behaviour, we make the first at-\ntempt to guide LLMs to generate texts with citations us-\ning Chain-of-Thought prompting strategies.\n(2) We propose an efficient and low-cost Citation Insurance\nMechanism to guarantee promising citation quality.\n(3) Our approach establishes strong and reproducible base-\nlines for future study.\nRelated Work\nText Generation with Citations. It is a new paradigm\nbrought by the development of LLMs. Previous studies\n3Some of the retrieved documents may be irrelevant to the\nquestion (Gao et al. 2023).\n4IR is short for “Information Retrieval”.\nmainly focus on practical applications of LLM-enhanced\ncommercial systems. For instance, Bing Chat5 and perplex-\nity.ai6 respond to user questions in natural language with ref-\nerences to Web pages. However, it is hard to automatically\nevaluate the quality of these generations. To facilitate the\nautomatic evaluation of text generation with citations, Gao\net al. (2023) propose ALCE, the first reproducible bench-\nmark for automatically evaluating LLMs’ generation with\ncitations. ALCE collects three datasets, i.e., ASQA (Stel-\nmakh et al. 2022), QAMPARI (Rubin et al. 2022), and ELI5\n(Fan et al. 2019), and provides multiple baselines built upon\nclosed- and open-source LLMs.\nChain-of-Thought (CoT). CoT prompting (an instruction\nor a few CoT demonstrations) is a gradient-free technique\nof eliciting a coherent series of intermediate reasoning steps\nfor each query from LLMs. Investigations on various LLMs,\nsuch as GPT (Brown et al. 2020; Ouyang et al. 2022; Ope-\nnAI 2023) and PaLM (Chowdhery et al. 2022; Anil et al.\n2023) series models, demonstrate that CoT prompting en-\nhances performance across a range of arithmetic, common-\nsense, and symbolic reasoning tasks (Wu, Zhang, and Huang\n2023; Chen et al. 2023b; Wang et al. 2023a).\nWei et al. (2022) initially propose the few-shot CoT,\nwhich requires the manual design of a few demonstrations\nto facilitate the generation of reasoning paths. In contrast,\nKojima et al. (2022) propose the zero-shot CoT, which em-\nploys a single zero-shot prompt that elicits reasoning paths\nfrom LLMs. By simply adding “Let’s think step by step.” af-\nter each query, zero-shot CoT demonstrates that LLMs are\ncapable zero-shot reasoners without the need for any man-\nually constructed CoT demonstrations. Built upon the CoT\nstudy of Wei et al. (2022), numerous studies have advanced\nthe development of CoT through various strategies, such\nas Auto-CoT (Zhang et al. 2023), least-to-most prompting\n(Zhou et al. 2023), and self-consistency CoT (Wang et al.\n2023b). These advancements have significantly bolstered the\nperformance of CoT prompting in tackling intricate tasks.\nUnlike existing CoT studies that center on tasks having\nexplicit reasoning steps (e.g., arithmetic and commonsense),\nthe task of text generation with citations doesn’t have such\nsteps, which brings us great challenges when generalizing\nthe CoT prompting.\nTask Formalization\nThe ALCE benchmark (Gao et al. 2023) presents the first\ncomprehensive formalization of text generation with cita-\ntions. We briefly revisit the formalization here.\nGiven a question Q and a large-scale knowledge base D\n(e.g., Wikipedia) that contains knowledge to answer Q, an\nLLM-based system is required to generate an answer A to\nQ with D, and A consists of several statements:\nA = s1s2s3...sn\nwhere each statement si contains factual claims summa-\nrized from relevant document texts, i.e., Ti = {ti,1, ti,2, ...},\n5https://www.bing.com/new\n6https://www.perplexity.ai\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18346\nInformation \nRetrieval\n[1] Decolonization of the Americas: … GB on July 2, 1776 ...\n[2] Decolonization: ... adopted by Congress on July 4, 1776 ...\n[3] American Revolution: ... was signed September 3, 1783 ...\n..\nQ: When did the US break away from England?..\n<1-shot CoT demonstration>\nInstruction: Write … answer using provided documents and cite them properly.\nQuestion: Which is the most rainy place on earth?\nDocuments: Document[1], Document[2], Document[3]...\nReasoning Steps:\nStep #1: ...judge Document [1], Document[2]... are relevant to the question...\nStep #2: ...Document[1], ….write … answer statement and cite the document…\n...\nStep #n: ...merge...rank...statements….concatenate…statements…final answer:\nAnswer: ...several place...to be most rainy[2]...Cherrapunji…[3].\n<Prompt for the question Q>\nInstruction: Write … answer using provided documents and cite them properly.\nQuestion: When did the us break away from England?\nDocuments: Document[1], Document[2], Document[3]…\nAnswer:\nThe US took the first step towards gaining independence from GB when it declared \nindependence on July 2, 1776 (although the event is now commemorated on July 4, 1776, \nthe date when the Declaration of Independence was officially adopted by \nCongress)[1][2]. The Treaty of Paris was later signed on September 3, 1783, formally \nseparating the United States from the British Empire[3].\nAnswer\nLLM\nThe US took the first step towards gaining independence from GB when it declared \nindependence on July 2, 1776 (although the event is now commemorated on July 4, 1776, \nthe date when the Declaration of Independence was officially adopted by \nCongress)[1][2]. The Treaty of Paris was later signed on September 3, 1783, formally \nseparating the United States from the British Empire.\nLLM output\nCIM\nCoT prompting\nRetrieved docs\nQuestion\nCorpus\nFigure 2: An overview of our approach. The “LLM output”\nis induced by the CoT prompting, which is fed to CIM to\nadd the missing citation, i.e., [3].\nwhich are retrieved from D for Q, and si needs to properly\ncite these documents7. ALCE regards each sentence of A as\na statement.\nMethod\nFigure 2 illustrates an overview of our approach. Given a\nquestion Q, we first retrieve relevant documents using In-\nformation Retrieval (IR) methods; then we design a CoT\nprompting strategy to induce LLMs to answer Q by mim-\nicking the Human Thought Process; at last, the proposed Ci-\ntation Insurance Mechanism (CIM) detects those statements\nwithout any citation and adds relevant citations to them. The\nALCE benchmark proves the effectiveness of IR methods\nlike GTR (Ni et al. 2022) and BM25 8. We follow this IR\nsetting in our approach for fair comparisons.9\nIn the following sections, we first formalize our CoT\nprompting, then elaborate on the proposed CIM followed by\n7ALCE requires that each si cites at least one and at most three\ncitations. The citations should be in the format like[1][2][3].\n8https://en.wikipedia.org/wiki/Okapi BM25\n9We use GTR for ASQA and QAMPRI, and BM25 for ELI5.\nextensive discussions.\nCoT Prompting\nCoT prompting mimics the Human Thought Process when\nsolving a complicated reasoning task like the multi-step\nmath word problem and the symbolic manipulation (Wei\net al. 2022; Chen et al. 2023a,b). It decomposes the prob-\nlem into several intermediate steps and solves each before\ngiving the final answer.\nGiven a question Q and its retrieved documents T =\n{t1, t2, ..., tn}, we formalize the Human Thought Process of\ntext generation with citations as follows:\n(1) Understand Q and each documentti ∈ Twith the goal of\nfinding out relevant documents that can answer the ques-\ntion. We use T ′ to denote the set of relevant documents,\nwhere T ′ ⊆ T.\n(2) According to each document tj ∈ T′, generate a factual\nstatement sj to answer Q from some aspects and properly\ncite tj using its unique document index, i.e., [j]. This\nstep generates a statement set S = {s1, s2, ..., sk}.\n(3) Merge mutually entailed statements. Specifically, if two\nstatements, e.g., sk1 and sk2, entails each other, i.e.,\nsk1 ⇔ sk2, sk1 will be dropped but its citations will be\nmerged to sk2. We useS′ to denote the statement set after\nmerging.\n(4) Rank and concatenate the statements included in S′ to\nobtain the answer A. This step aims to guarantee the flu-\nency and coherence of A.\nWe design 1-shot CoT demonstration based on the above\nHuman Thought Process. As Figure 2 shows, the CoT\ndemonstration consists ofInstruction, Question, Documents,\nReasoning Steps, and Answer, where the Reasoning Steps\nis the natural language description of the Human Thought\nProcess. The CoT prompting strategy contains a 1-shot CoT\ndemonstration and a prompt for the question Q. We present\na detailed CoT prompt in Appendix A.10\nAlthough CoT has been comprehensively studied, exten-\nsive efforts are still necessary to generalize it to the task of\ntext generation with citations. As far as we know, we are the\nfirst to generalize CoT to this task, where CoT prompts are\nbuilt upon our formalization of the Human Thought Process.\nThe formalization can also be used as the stepstone of future\nCoT prompt studies on this task.\nCitation Insurance Mechanism\nWhether CoT prompting strategies are employed or not,\nLLMs even ChatGPT and GPT-4 are still prone to not cite\nany document in their generated statements, which harms\nthe citation quality. To tackle this issue, we propose the Cita-\ntion Insurance Mechanism (CIM) to detect those statements\nwithout any citation and add relevant citations to them, as\nshown in Algorithm 1.\nFor these statements without any citation, we investi-\ngate two implementations of adding citations to them (the\nrecite() function in Algorithm 1-L6), as shown below:\n10The Appendix section can be found in the full paper at\nhttps://github.com/jibin5167/ALCE-CoT.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18347\nAlgorithm 1: Citation Insurance Mechanism\nInput: A\n1 A′ = ‘’\n2 X = nltk.sent tokenize(A) // Split A into statements.\n3 for x ∈ Xdo\n4 C = re.findall(‘\\[\\d+’, x) // If x is a statement\nwithout any citation, then C is Null.\n5 if C is Null then\n6 x′ = recite(x) // Add relevant citations to x.\n7 A′ = A′ + x′\n8 else\n9 A′ = A′ + x\n10 end\n11 end\n12 return A′\n• The LLM-based implementation.We propose few-shot\ndemonstration prompts to guide LLMs to add the miss-\ning citations. The LLM used to add citations is the same\nLLM used to generate answers. We present a real-world\nprompt example in Appendix A.\n• The IR-based implementation. We propose to use GTR\nto search for the best matching document from the docu-\nments provided in the CoT prompt and cite it.\nThe main difference between the above two implementa-\ntions lies in that: the LLM-based one can add more than one\ncitation, but it may also fail to add any citation; in contrast,\nthe IR-based one always adds one citation.\nWe conduct detailed analyses of the two implementations\nin the Analyses section.\nDiscussions\nCan CoT prompts safely fit the limited context window?\nWe claim that our CoT prompts can safely fit the context\nwindow on six open-source LLMs. LLMs have limited con-\ntext window sizes, generally ranging from 2048 to 4096 to-\nkens. To comply with the size limit, we adopt the 1-shot CoT\nprompt setting and only retrieve three documents for the 1-\nshot CoT demonstration and prompt for the question Q.\nDoes CIM increase computation costs? It’s clear that\nCIM increases computation costs, but we claim that the in-\ncreased costs are actually very low. This is because CIM\nwon’t call computation-expensive LLMs or IR models until\nit detects statements without any citation. Take experiments\non ASQA as examples, the ratios of statements without any\ncitation range from 0.52% to 4.47% when using our CoT\nprompting strategy.\nDoes CIM always add correct citations? The answer is\n“No”. We observe that both LLM-based and IR-based CIM\nmay add incorrect citations. Here is a case study: when\nall the retrieved documents are irrelevant to a question Q,\nLLMs tend to generate “I cannot answer the question with\nthe provided documents.” as the answer to Q. Under this\ncondition, the LLM-based CIM is prone to cite all the pro-\nLLM Window LLM Window\nLLaMA-13B 2048 LLaMA-33B 2048\nLLaMA-2-13B-Chat 4096 Oasst-33B 2048\nVicuna-13B 2048 LLaMA-2-70B-Chat 4096\nTable 1: LLM details. We use LLaMA-2-13B to denote\nLLaMA-2-13B-Chat and LLaMA-2-70B to denote LLaMA-\n2-70B-Chat in other parts of this paper.\nvided documents, and the IR-based CIM cite one document.\nHowever, all these citations are incorrect.\nExperiments\nThe ALCE Benchmark\nALCE (Gao et al. 2023) is the first reproducible benchmark\nfor automatically evaluating LLMs’ text generation with ci-\ntations and allows for multiple citations for individual state-\nments. It includes three datasets,i.e., ASQA (Stelmakh et al.\n2022), QAMPARI (Rubin et al. 2022), and ELI5 (Fan et al.\n2019). It pre-defines three automatic evaluation metrics,i.e.,\nFluency, Correctness (Correct.), and Citation Quality. We\npresent more benchmark details in Appendix B.\nOpen-source LLMs\nWe evaluate our approach on six open-source LLMs (see\nTable 1) and report more LLM details in Appendix C.\nCoT Prompting Strategy\nIn line with previous work (Gao et al. 2023), We explore\napplying CoT to four different prompting strategies:\n(1) V ANILLA . Provide the top-3 retrieved documents for\neach question.\n(2) S UMM . Provide summaries instead of the full text of the\ntop-10 retrieved documents for each question.\n(3) S NIPPET . Provide snippets instead of full texts of the top-\n10 retrieved documents for each question.\n(4) O RACLE . Provide three gold documents for each ques-\ntion.\nThe difference between SUMM and SNIPPET is that sum-\nmaries are free-form, but snippets are spans extracted from\ndocuments. More details can be found in Appendix D.\nImplementation Details and Baselines\nWe use four NVIDIA A100 40GB GPUs to evaluate our ap-\nproach. Specifically, we use one GPU to run 13B LLMs,\ntwo GPUs to run 33B LLMs, and four GPUs to run the 70B\nLLM. For all experiments, We set the seed to 42, which is\nthe default setting of ALCE. For each prompting strategy,\nwe evaluate our approach on the six LLMs by setting the\ntemperature value to 0.001, 0.1, 0.3, 0.5, 0.7, 0.9, and 1,\nrespectively. For each type of experiment, we average the\nresults of different temperature settings and report the aver-\naged performance.\nThe LLM-based prompting approaches proposed by Gao\net al. (2023) are the current state-of-the-art approaches.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18348\nLLM Fluency Correct. Citation\nRecall Precision\nbase ours base ours base ours base ours\nVANIL\nLA\nLLaMA-13B 68.4 76.8 26.9 30.7 10.6 17.9 26.9 25.4\nLLaMA-2-13B 73.4 82.0 32.6 37.4 60.0 66.5 52.1 59.2\nVicuna-13B 82.6 79.8 31.9 36.4 51.1 56.6 50.1 56.4\nLLaMA-33B 83.7 84.2 31.0 35.1 19.5 24.4 23.0 27.9\nOasst-33B 82.9 81.7 34.8 35.9 36.2 41.4 38.3 46.8\nLLaMA-2-70B 84.6 82.7 39.4 43.9 62.7 70.2 58.4 69.8\nSUMM\nLLaMA-13B 76.8 82.3 33.3 35.1 19.6 27.4 23.7 30.3\nLLaMA-2-13B 81.5 79.4 42.9 43.1 58.7 62.3 50.4 55.1\nVicuna-13B 67.7 81.2 43.2 43.2 52.7 56.9 50.0 56.8\nLLaMA-33B 72.0 79.6 33.1 35.7 34.7 42.2 35.2 40.8\nOasst-33B 74.3 77.2 40.9 39.4 45.5 47.6 44.0 49.7\nLLaMA-2-70B 84.3 89.6 43.7 44.6 64.2 71.4 57.7 68.0\nSNIPP\nET\nLLaMA-13B 72.0 79.0 31.3 32.5 18.2 22.9 21.1 25.4\nLLaMA-2-13B 81.4 83.1 41.3 43.1 57.4 61.2 52.1 55.7\nVicuna-13B 81.4 77.2 42.1 42.9 53.4 55.6 48.7 52.3\nLLaMA-33B 70.8 78.4 30.9 33.7 31.4 39.1 31.5 37.2\nOasst-33B 79.3 80.2 40.1 38.9 45.0 49.6 43.3 44.5\nLLaMA-2-70B 82.7 86.3 43.2 42.9 64.2 70.4 60.2 69.4\nORACL\nE\nLLaMA-13B 69.5 78.2 34.3 34.7 10.8 15.2 15.8 17.1\nLLaMA-2-13B 73.2 77.4 41.4 43.0 54.5 58.2 52.9 56.2\nVicuna-13B 72.9 79.3 42.5 42.7 52.2 56.2 50.7 54.8\nLLaMA-33B 82.6 79.4 39.3 40.1 20.2 27.7 23.9 31.2\nOasst-33B 85.4 82.6 44.3 43.1 37.0 44.8 39.6 45.4\nLLaMA-2-70B 89.5 86.2 44.1 46.2 67.4 73.5 69.8 72.9\nTable 2: Performance comparisons of baselines (columns\nmarked by “base”) and the proposed approach (columns\nmarked by “ours”) on ASQA.\nThese approaches are evaluated with ChatGPT, LLaMA-\n13B, LLaMA-33B, Vicuna-13B, and Oasst-33B. We adopt\nthese approaches as baselines and take additional two\nLLaMA-2 models into consideration, as shown in Table 1.\nMain Results\nWe report performance comparisons of our approach and\nbaselines in Table 2, Table 3, and Table 4. For the base-\nlines, we evaluate their results on the two LLaMA-2 mod-\nels by ourselves and directly take the results of other LLMs\nreported by Gao et al. (2023) to facilitate fair comparisons.\nWe have the following observations:\n(1) On ASQA, our approach consistently outperforms the\nbaselines in terms of Citation Recall, and it beats them\nregarding Correct. and Citation Precision in 43 out of 48\ninstances, as well as exhibits great advantages regarding\nFluency in 16 out of 24 instances.\n(2) On QAMPARI, our approach delivers consistent Citation\nPrecision gains, and it beats the baselines in 68 out of 72\ninstances regarding the other three metrics,i.e., Recall-5,\nPrecision, and Citation Recall.\n(3) On ELI5, our approach showcases improvements in\nterms of Correct. and Citation Precision, and performs\nLLM\nCorrect. Citation\nRecall-5 Precision Recall Precision\nbase ours base ours base ours base ours\nVANIL\nLA\nLLaMA-13B 9.7 12.3 9.1 14.6 6.7 11.1 7.1 12.7\nLLaMA-2-13B 13.4 16.8 15.2 18.3 12.2 14.4 12.5 17.7\nVicuna-13B 14.0 18.1 15.9 17.4 12.5 11.6 13.4 15.4\nLLaMA-33B 14.7 21.3 12.0 15.2 7.9 11.1 8.3 12.3\nOasst-33B 15.5 14.4 14.9 16.8 9.0 12.2 10.1 14.1\nLLaMA-2-70B 19.2 23.2 18.2 21.7 17.8 22.3 19.1 24.0\nSUMM\nLLaMA-13B 14.8 19.2 12.6 17.9 7.4 15.1 8.0 14.2\nLLaMA-2-13B 19.2 23.4 17.5 22.1 15.3 18.5 14.2 20.4\nVicuna-13B 21.1 25.6 17.1 19.8 15.7 17.2 17.8 19.3\nLLaMA-33B 19.0 25.7 14.8 17.9 12.5 12.4 15.0 19.7\nOasst-33B 21.0 24.4 17.5 22.9 12.9 16.8 16.6 19.1\nLLaMA-2-70B 22.0 23.6 19.8 22.0 20.2 23.1 21.4 23.7\nSNIPP\nET\nLLaMA-13B 17.7 21.1 15.7 19.8 8.8 14.4 9.9 14.7\nLLaMA-2-13B 22.3 26.4 19.2 22.8 14.3 17.9 20.1 24.3\nVicuna-13B 21.9 25.6 18.2 21.3 16.8 19.4 19.7 24.4\nLLaMA-33B 19.6 24.4 15.7 22.7 12.8 18.4 15.2 16.7\nOasst-33B 22.0 24.2 17.4 23.8 13.6 18.4 17.7 19.8\nLLaMA-2-70B 22.6 22.9 20.4 23.1 19.8 22.3 21.2 23.8\nORACL\nE\nLLaMA-13B 16.8 17.4 15.4 17.2 7.7 11.9 8.3 14.6\nLLaMA-2-13B 26.4 30.1 30.2 33.5 17.3 21.5 19.1 21.6\nVicuna-13B 25.9 27.8 28.4 31.2 15.8 20.2 16.8 19.7\nLLaMA-33B 23.9 26.0 20.3 22.7 9.8 15.4 10.4 17.2\nOasst-33B 26.9 27.4 26.0 25.6 11.7 15.1 12.9 16.3\nLLaMA-2-70B 32.2 36.1 31.9 36.7 22.6 24.4 21.4 24.4\nTable 3: Performance comparisons of baselines (columns\nmarked by “base”) and the proposed approach (columns\nmarked by “ours”) on QAMPARI.\nbetter than the baselines in 42 out of 48 instances when\ncomparing them from Fluency and Citation Recall per-\nspectives.\nThese promising performance gains demonstrate the ef-\nfectiveness of our CoT prompting strategy and the Citation\nInsurance Mechanism.\nWe also compare our best model with the ChatGPT-based\nbaseline, which achieves the current state-of-the-art results.\nThe comparison results are reported in Table 5, from which\nwe can observe that our best approach achieves comparable\nCorrect. and Citation Quality to current SOTA results, re-\nvealing that our approach can serve as a strong reproducible\nbaseline for future study. Additionally, it further verifies the\neffectiveness of the CoT prompting strategy and the CIM.\nAnalyses\nPerformance against CoT Descriptions\nThis analysis aims to investigate if different natural language\ndescriptions of the Human Thought Process affect perfor-\nmance. To this end, we manually implement five versions\nof the Human Thought Process and investigate their effec-\ntiveness, as shown in Figure 3. We can see that these de-\nscriptions induce quite different performances in all four\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18349\nLLM Fluency Correct. Citation\nRecall Precision\nbase ours base ours base ours base ours\nVANIL\nLA\nLLaMA-13B 50.0 59.9 3.9 12.3 3.1 9.7 5.3 9.2\nLLaMA-2-13B 57.9 62.1 12.1 14.5 16.4 22.8 19.7 26.3\nVicuna-13B 58.2 69.4 10.0 13.3 15.6 21.3 19.6 24.1\nLLaMA-33B 58.8 71.2 6.2 8.8 9.3 15.1 12.1 16.4\nOasst-33B 46.8 59.0 9.5 12.1 16.0 15.2 21.6 25.6\nLLaMA-2-70B 58.2 62.4 12.4 13.9 46.5 51.1 44.4 49.1\nSUMM\nLLaMA-13B 28.6 42.1 2.9 8.4 2.5 7.1 3.8 6.8\nLLaMA-2-13B 26.4 31.4 6.1 11.2 9.9 14.2 14.3 17.4\nVicuna-13B 22.4 37.2 4.9 10.2 9.7 11.7 12.2 15.8\nLLaMA-33B 23.3 37.1 3.0 9.0 6.2 9.8 8.2 14.1\nOasst-33B 24.8 34.1 3.9 7.6 12.3 18.9 16.3 22.7\nLLaMA-2-70B 36.8 42.4 9.8 12.7 44.6 49.7 44.7 48.4\nSNIPP\nET\nLLaMA-13B 48.4 57.1 5.7 9.2 5.8 7.7 7.6 12.1\nLLaMA-2-13B 52.1 57.3 11.9 14.5 29.4 34.5 28.6 34.6\nVicuna-13B 48.1 55.2 11.2 13.6 27.2 29.3 27.9 33.6\nLLaMA-33B 53.2 49.8 7.4 12.1 13.7 17.4 15.1 18.1\nOasst-33B 50.7 55.4 10.7 14.8 25.8 30.1 26.7 29.9\nLLaMA-2-70B 55.4 58.9 13.4 13.9 44.7 49.6 42.1 45.2\nORACL\nE\nLLaMA-13B 49.5 47.5 6.4 8.3 3.7 9.4 6.5 9.1\nLLaMA-2-13B 47.4 45.8 16.9 22.7 21.4 24.2 27.3 33.1\nVicuna-13B 41.6 48.4 17.1 22.1 20.2 24.3 26.5 31.1\nLLaMA-33B 63.7 62.1 11.4 15.1 11.9 17.2 15.4 18.9\nOasst-33B 50.7 54.2 15.8 18.8 20.8 26.3 28.0 31.4\nLLaMA-2-70B 58.4 56.1 17.6 20.8 52.4 56.7 52.1 55.3\nTable 4: Performance comparisons of baselines (columns\nmarked by “base”) and the proposed approach (columns\nmarked by “ours”) on ELI5.\nevaluation metrics, indicating that LLMs are sensitive to\nprompt descriptions, which highlights the importance of\nwell-designed and LLM-adapted prompts, the instability of\nmanual prompts, as well as prompt engineering.\nPerformance against CIM Implementations\nThis analysis aims to compare the LLM-based CIM and IR-\nbased CIM. We solely compare the Citation Quality since\nCIM is designed to advance it. We conduct the investiga-\ntion using the LLaMA-2-13B V ANILLA setting and report\nthe results in Figure 4. We observe that LLM-based CIM\nachieves better Citation Quality on ASQA and ELI5, as well\nas higher Citation Precision on QAMPARI. We attribute this\nto the fact that LLM-based CIM may add more than one ci-\ntation to the statements without any citation, and it refuses to\nadd any citation if it cannot find any document that supports\nthe statement, which guarantees better Citation Quality. In\ncontrast, the IR-based CIM always add one citation to the\nstatement even if there are multiple documents that support\nthe statement or no document that supports the statement.\nBased on the above analysis, we use the LLM-based CIM\nin all the other experiments.\nd1 d2 d3 d4 d5\n70.0\n72.5\n75.0\n77.5\n80.0\n82.5\n85.0\n87.5\n90.0\nFluency\nd1 d2 d3 d4 d5\n30.0\n32.5\n35.0\n37.5\n40.0\n42.5\n45.0\n47.5\n50.0\nCorrctness\nd1 d2 d3 d4 d5\n55.0\n57.5\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\nCitation Recall\nd1 d2 d3 d4 d5\n55.0\n57.5\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\nCitation Precision\nFigure 3: Performance comparisons of five different descrip-\ntions of the Human Thought Process, marked by d1, d2, d3,\nd4, and d5, respectively. We evaluate the performance on\nASQA using the LLaMA-2-70B VANILLA setting.\nRecall Precision\n56\n58\n60\n62\n64\n66\n68\nASQA\nLLM-based\nIR-based\nRecall Precision\n12\n13\n14\n15\n16\n17\n18\nQAMPARI\nLLM-based\nIR-based\nRecall Precision\n18\n20\n22\n24\n26\nELI5\nLLM-based\nIR-based\nFigure 4: Comparisons of Citation Quality between LLM-\nbased and IR-based CIM. We evaluate them on all datasets\nusing the LLaMA-2-13B VANILLA setting.\nPerformance against Temperature\nThis analysis investigates if the temperature affects the qual-\nity of LLMs’ outputs. To this end, we set the temperature to\n0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 0.9, and 1, respectively. We\nconduct investigations on ASQA using the LLaMA-2-13B\nVANILLA setting and report the results in Figure 5, which in-\ndicates that the temperature value has obvious effects on all\nfour evaluation metrics, especially the Fluency. We attribute\nthis to the fact that various temperature values enable LLMs\nto generate texts with various randomness, consequently af-\nfecting the fluency of LLMs’ generations. As a result, the\ncorrectness and the citation quality follow the variations.\nMoreover, we observe that the Citation Recall and Cita-\ntion Precision exhibit similar variation trends, revealing tight\ncorrelations between the two evaluation metrics.\nAblation Study\nWe ablate the CoT prompting strategy and the LLM-based\nCIM individually to explore their utilities. We report the ab-\nlation settings and results in Table 6, where:\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18350\nASQA\nStrategy Fluency Correct. Citation\nRecall Precision\nChatGPT\nVANIL\nLA 66.6 40.4 73.6 72.5\nSUMM 70.0 43.3 68.9 61.8\nSNIPPET 69.8 41.4 65.3 57.4\nORACLE 64.4 48.9 74.5 72.7\nLLaMA-2-70B (ours)\nORACL\nE 86.2 46.2 73.5 72.9\nQAMPARI\nStrategy Correct. Citation\nRecall-5 Precision Recall Precision\nChatGPT\nVANIL\nLA 20.8 20.8 20.5 20.9\nSUMM 23.6 21.2 23.6 25.7\nSNIPPET 24.5 21.5 22.9 24.9\nORACLE 37.0 36.9 24.1 24.6\nLLaMA-2-70B (ours)\nORACL\nE 36.1 36.7 24.4 24.4\nELI5\nStrategy Fluency Correct. Citation\nRecall Precision\nChatGPT\nVANIL\nLA 57.2 12.0 51.1 50.0\nSUMM 40.3 12.5 51.8 48.2\nSNIPPET 62.9 14.3 50.4 45.0\nORACLE 59.4 21.3 57.8 56.0\nLLaMA-2-70B (ours)\nORACL\nE 56.1 20.8 56.7 55.3\nTable 5: Comparisons with ChatGPT-based results.\n• “-CoT” denotes removing the CoT prompting strategy.\nWe use 1-shot in-context learning as a proxy.\n• “-CIM” denotes removing the CIM. Our approach will\ndo nothing to those statements without citations.\n• “base” denotes conducting the above two ablations\nsimutaneously.\nWe conduct ablation studies on three LLaMA models with\nvarious parameters. From Table 6 we observe that:\n(1) CoT prompting strategy delivers significant performance\ngains across all evaluation metrics and all LLMs. We\nattribute it to the fact that CoT prompts guide LLMs\nto mimic the Human Thought Process, which has been\nproven to be effective in the Experiments section.\n(2) CIM delivers more improvements in Citation Quality on\nrelatively smaller LLMs. For example, it brings +2.4 Re-\ncall and +2.2 Precision gains on LLaMA-2-13B, while\nonly +1.1 Recall and +1.9 Precision gains on LLaMA-2-\n70B. We attribute it to the fact that the larger the LLM’s\nparameter scale, the more powerful the LLM’s ability\nto properly add citations. Thus, fewer statements suffer\nfrom the citation missing problem in larger LLMs.\nIn addition, we note that the CIM also affects Fluency\nand Correctness. However, the CIM is designed to add miss-\ning citations, which does not affect the above two evaluation\nmetrics theoretically. After checking LLMs’ generations, we\nfind that the LLM-based CIM may complete statements that\nare detected to have no citations and are not ended normally,\nconsequently affecting the Fluency and Correctness.\nWe present a real-world case for better understanding: the\nLLM-based CIM completes “Therefore, the answer is that”\n10 3\n 10 2\n 10 1\n 100\nT emperature values\n40\n50\n60\n70\n80\n90Scores\nFluency\nCorrectness\nRecall\nPrecision\nFigure 5: Performance comparisons of various temperature\nvalues, which are set to 0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 0.9,\nand 1, respectively. The x-coordinate is on a log scale.\nSetting Fluency Correct. Citation\nRecall Precision\nLLaMA-2-13B 82.0 37.4 66.5 59.2\n-CoT 76.3 33.2 62.8 54.4\n-CIM 80.4 36.8 64.1 57.0\nbase 74.6 32.8 59.6 53.4\nLLaMA-33B 84.2 35.1 24.4 27.9\n-CoT 79.4 32.4 20.2 24.0\n-CIM 81.9 34.9 23.3 26.4\nbase 81.4 30.8 19.9 23.4\nLLaMA-2-70B 82.7 43.9 70.2 69.8\n-CoT 77.8 39.6 63.3 58.9\n-CIM 82.1 42.6 69.1 67.9\nbase 85.2 38.8 63.7 58.1\nTable 6: Ablation results on ASQA with various LLM set-\ntings.\nand adds citations to it and outputs “Therefore, the answer is\nthat there are 34 state parks in Virginia[1][2][3].”\nConclusion\nIn this paper, we investigate Chain-of-Thought (CoT)\nprompting strategies to improve LLMs’ ability to generate\ntext with citations. We formalize the Human Thought Pro-\ncess and carefully design CoT prompts accordingly. More-\nover, we propose a Citation Insurance Mechanism (CIM) to\ndetect those missing citations and properly cite them. Ex-\nperimental results on six open-source LLMs and the ALCE\nbenchmark demonstrate the effectiveness of our novel ap-\nproach. And our approach induces LLaMA-2-70B to per-\nform comparably to ChatGPT on the ALCE benchmark.\nQualitative and quantitative analyses further prove the ef-\nfectiveness of our approach.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18351\nAcknowledgments\nThis research is supported by A*STAR, CISCO Systems\n(USA) Pte. Ltd and National University of Singapore un-\nder its Cisco-NUS Accelerated Digital Economy Corporate\nLaboratory (Award I21001E0002).\nAdditionally, this research/project is supported by the Na-\ntional Research Foundation, Singapore under its Industry\nAlignment Fund – Pre-positioning (IAF-PP) Funding Initia-\ntive. Any opinions, findings and conclusions or recommen-\ndations expressed in this material are those of the author(s)\nand do not reflect the views of National Research Founda-\ntion, Singapore.\nReferences\nAnil, R.; Dai, A. M.; Firat, O.; Johnson, M.; Lepikhin, D.;\nPassos, A.; Shakeri, S.; Taropa, E.; Bailey, P.; Chen, Z.; Chu,\nE.; Clark, J. H.; Shafey, L. E.; Huang, Y .; Meier-Hellstern,\nK.; Mishra, G.; Moreira, E.; Omernick, M.; Robinson, K.;\nRuder, S.; Tay, Y .; Xiao, K.; Xu, Y .; Zhang, Y .; Abrego,\nG. H.; Ahn, J.; Austin, J.; Barham, P.; Botha, J.; Bradbury, J.;\nBrahma, S.; Brooks, K.; Catasta, M.; Cheng, Y .; Cherry, C.;\nChoquette-Choo, C. A.; Chowdhery, A.; Crepy, C.; Dave,\nS.; Dehghani, M.; Dev, S.; Devlin, J.; D ´ıaz, M.; Du, N.;\nDyer, E.; Feinberg, V .; Feng, F.; Fienber, V .; Freitag, M.;\nGarcia, X.; Gehrmann, S.; Gonzalez, L.; Gur-Ari, G.; Hand,\nS.; Hashemi, H.; Hou, L.; Howland, J.; Hu, A.; Hui, J.; Hur-\nwitz, J.; Isard, M.; Ittycheriah, A.; Jagielski, M.; Jia, W.;\nKenealy, K.; Krikun, M.; Kudugunta, S.; Lan, C.; Lee, K.;\nLee, B.; Li, E.; Li, M.; Li, W.; Li, Y .; Li, J.; Lim, H.; Lin,\nH.; Liu, Z.; Liu, F.; Maggioni, M.; Mahendru, A.; Maynez,\nJ.; Misra, V .; Moussalem, M.; Nado, Z.; Nham, J.; Ni, E.;\nNystrom, A.; Parrish, A.; Pellat, M.; Polacek, M.; Polozov,\nA.; Pope, R.; Qiao, S.; Reif, E.; Richter, B.; Riley, P.; Ros,\nA. C.; Roy, A.; Saeta, B.; Samuel, R.; Shelby, R.; Slone, A.;\nSmilkov, D.; So, D. R.; Sohn, D.; Tokumine, S.; Valter, D.;\nVasudevan, V .; V odrahalli, K.; Wang, X.; Wang, P.; Wang,\nZ.; Wang, T.; Wieting, J.; Wu, Y .; Xu, K.; Xu, Y .; Xue, L.;\nYin, P.; Yu, J.; Zhang, Q.; Zheng, S.; Zheng, C.; Zhou, W.;\nZhou, D.; Petrov, S.; and Wu, Y . 2023. PaLM 2 Technical\nReport. arXiv:2305.10403.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Mod-\nels are Few-Shot Learners. In Larochelle, H.; Ranzato, M.;\nHadsell, R.; Balcan, M.; and Lin, H., eds.,Advances in Neu-\nral Information Processing Systems, volume 33, 1877–1901.\nCurran Associates, Inc.\nChen, J.; Chen, L.; Huang, H.; and Zhou, T. 2023a. When do\nyou need Chain-of-Thought Prompting for ChatGPT? arXiv\npreprint arXiv:2304.03262.\nChen, Z.; Zhou, K.; Zhang, B.; Gong, Z.; Zhao, X.; and\nWen, J.-R. 2023b. ChatCoT: Tool-Augmented Chain-of-\nThought Reasoning on Chat-based Large Language Mod-\nels. In Bouamor, H.; Pino, J.; and Bali, K., eds., Findings\nof the Association for Computational Linguistics: EMNLP\n2023, 14777–14790. Singapore: Association for Computa-\ntional Linguistics.\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;\nZheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; Stoica,\nI.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot\nImpressing GPT-4 with 90%* ChatGPT Quality.\nChowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,\nG.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton,\nC.; Gehrmann, S.; Schuh, P.; Shi, K.; Tsvyashchenko,\nS.; Maynez, J.; Rao, A.; Barnes, P.; Tay, Y .; Shazeer,\nN.; Prabhakaran, V .; Reif, E.; Du, N.; Hutchinson, B.;\nPope, R.; Bradbury, J.; Austin, J.; Isard, M.; Gur-Ari, G.;\nYin, P.; Duke, T.; Levskaya, A.; Ghemawat, S.; Dev, S.;\nMichalewski, H.; Garcia, X.; Misra, V .; Robinson, K.; Fe-\ndus, L.; Zhou, D.; Ippolito, D.; Luan, D.; Lim, H.; Zoph, B.;\nSpiridonov, A.; Sepassi, R.; Dohan, D.; Agrawal, S.; Omer-\nnick, M.; Dai, A. M.; Pillai, T. S.; Pellat, M.; Lewkowycz,\nA.; Moreira, E.; Child, R.; Polozov, O.; Lee, K.; Zhou, Z.;\nWang, X.; Saeta, B.; Diaz, M.; Firat, O.; Catasta, M.; Wei,\nJ.; Meier-Hellstern, K.; Eck, D.; Dean, J.; Petrov, S.; and\nFiedel, N. 2022. PaLM: Scaling Language Modeling with\nPathways. arXiv:2204.02311.\nFan, A.; Jernite, Y .; Perez, E.; Grangier, D.; Weston, J.; and\nAuli, M. 2019. ELI5: Long Form Question Answering. In\nKorhonen, A.; Traum, D.; and M `arquez, L., eds., Proceed-\nings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics, 3558–3567. Florence, Italy: Associ-\nation for Computational Linguistics.\nGao, T.; Yen, H.; Yu, J.; and Chen, D. 2023. Enabling\nLarge Language Models to Generate Text with Citations.\nIn Bouamor, H.; Pino, J.; and Bali, K., eds., Proceedings of\nthe 2023 Conference on Empirical Methods in Natural Lan-\nguage Processing, 6465–6488. Singapore: Association for\nComputational Linguistics.\nKojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y .; and Iwasawa,\nY . 2022. Large language models are zero-shot reason-\ners. Advances in neural information processing systems, 35:\n22199–22213.\nK¨opf, A.; Kilcher, Y .; von R¨utte, D.; Anagnostidis, S.; Tam,\nZ.-R.; Stevens, K.; Barhoum, A.; Duc, N. M.; Stanley, O.;\nNagyfi, R.; ES, S.; Suri, S.; Glushkov, D.; Dantuluri, A.;\nMaguire, A.; Schuhmann, C.; Nguyen, H.; and Mattick, A.\n2023. OpenAssistant Conversations – Democratizing Large\nLanguage Model Alignment. arXiv:2304.07327.\nLiu, N.; Zhang, T.; and Liang, P. 2023. Evaluating Verifia-\nbility in Generative Search Engines. In Bouamor, H.; Pino,\nJ.; and Bali, K., eds.,Findings of the Association for Compu-\ntational Linguistics: EMNLP 2023, 7001–7025. Singapore:\nAssociation for Computational Linguistics.\nNi, J.; Qu, C.; Lu, J.; Dai, Z.; Abrego, G. H.; Ma, J.; Zhao,\nV .; Luan, Y .; Hall, K.; Chang, M.-W.; et al. 2022. Large Dual\nEncoders Are Generalizable Retrievers. In Proceedings of\nthe 2022 Conference on Empirical Methods in Natural Lan-\nguage Processing, 9844–9855.\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18352\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;\nMishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Gray, A.;\nSchulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens, M.;\nAskell, A.; Welinder, P.; Christiano, P.; Leike, J.; and Lowe,\nR. 2022. Training language models to follow instructions\nwith human feedback. In Oh, A. H.; Agarwal, A.; Belgrave,\nD.; and Cho, K., eds., Advances in Neural Information Pro-\ncessing Systems.\nRubin, S. J. A. O.; Yoran, O.; Wolfson, T.; Herzig, J.;\nand Berant, J. 2022. QAMPARI:: An Open-domain\nQuestion Answering Benchmark for Questions with Many\nAnswers from Multiple Paragraphs. arXiv preprint\narXiv:2205.12665.\nStelmakh, I.; Luan, Y .; Dhingra, B.; and Chang, M.-W. 2022.\nASQA: Factoid Questions Meet Long-Form Answers. In\nGoldberg, Y .; Kozareva, Z.; and Zhang, Y ., eds.,Proceedings\nof the 2022 Conference on Empirical Methods in Natural\nLanguage Processing, 8273–8288. Abu Dhabi, United Arab\nEmirates: Association for Computational Linguistics.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample,\nG. 2023. LLaMA: Open and Efficient Foundation Language\nModels. arXiv:2302.13971.\nWang, H.; Wang, R.; Mi, F.; Deng, Y .; Wang, Z.; Liang, B.;\nXu, R.; and Wong, K.-F. 2023a. Cue-CoT: Chain-of-thought\nPrompting for Responding to In-depth Dialogue Questions\nwith LLMs. In Bouamor, H.; Pino, J.; and Bali, K., eds.,\nFindings of the Association for Computational Linguistics:\nEMNLP 2023, 12047–12064. Singapore: Association for\nComputational Linguistics.\nWang, X.; Wei, J.; Schuurmans, D.; Le, Q. V .; Chi, E. H.;\nNarang, S.; Chowdhery, A.; and Zhou, D. 2023b. Self-\nConsistency Improves Chain of Thought Reasoning in Lan-\nguage Models. In The Eleventh International Conference on\nLearning Representations.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi,\nE. H.; Le, Q. V .; Zhou, D.; et al. 2022. Chain-of-Thought\nPrompting Elicits Reasoning in Large Language Models. In\nAdvances in Neural Information Processing Systems.\nWu, D.; Zhang, J.; and Huang, X. 2023. Chain of Thought\nPrompting Elicits Knowledge Augmentation. In Rogers, A.;\nBoyd-Graber, J.; and Okazaki, N., eds., Findings of the As-\nsociation for Computational Linguistics: ACL 2023, 6519–\n6534. Toronto, Canada: Association for Computational Lin-\nguistics.\nZhang, Z.; Zhang, A.; Li, M.; and Smola, A. 2023. Auto-\nmatic Chain of Thought Prompting in Large Language Mod-\nels. In The Eleventh International Conference on Learning\nRepresentations.\nZhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y .;\nMin, Y .; Zhang, B.; Zhang, J.; Dong, Z.; Du, Y .; Yang, C.;\nChen, Y .; Chen, Z.; Jiang, J.; Ren, R.; Li, Y .; Tang, X.; Liu,\nZ.; Liu, P.; Nie, J.-Y .; and Wen, J.-R. 2023. A Survey of\nLarge Language Models. arXiv:2303.18223.\nZhou, D.; Sch ¨arli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,\nX.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q. V .; and\nChi, E. H. 2023. Least-to-Most Prompting Enables Com-\nplex Reasoning in Large Language Models. In The Eleventh\nInternational Conference on Learning Representations.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18353",
  "topic": "Natural language processing",
  "concepts": [
    {
      "name": "Natural language processing",
      "score": 0.46475616097450256
    },
    {
      "name": "Computer science",
      "score": 0.4351730942726135
    },
    {
      "name": "Linguistics",
      "score": 0.4193616211414337
    },
    {
      "name": "Psychology",
      "score": 0.37386131286621094
    },
    {
      "name": "Cognitive science",
      "score": 0.32847368717193604
    },
    {
      "name": "Philosophy",
      "score": 0.24465474486351013
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    }
  ],
  "cited_by": 4
}