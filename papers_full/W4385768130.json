{
    "title": "ViT-CX: Causal Explanation of Vision Transformers",
    "url": "https://openalex.org/W4385768130",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2348656491",
            "name": "Xie Wei-yan",
            "affiliations": [
                "University of Hong Kong",
                "Hong Kong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A3142268155",
            "name": "Li, Xiao-Hui",
            "affiliations": [
                "Huawei Technologies (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A4226897727",
            "name": "Cao, Caleb Chen",
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A4298584441",
            "name": "Zhang, Nevin Lianwen",
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "University of Hong Kong"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6777047548",
        "https://openalex.org/W1787224781",
        "https://openalex.org/W3030520226",
        "https://openalex.org/W3157528469",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3164024107",
        "https://openalex.org/W2809136100",
        "https://openalex.org/W3089569007",
        "https://openalex.org/W2616247523",
        "https://openalex.org/W2594633041",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W6864014924",
        "https://openalex.org/W3015214599",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W1849277567",
        "https://openalex.org/W6791858558",
        "https://openalex.org/W2503388974",
        "https://openalex.org/W4283809036",
        "https://openalex.org/W4297812995",
        "https://openalex.org/W2516809705",
        "https://openalex.org/W4320812537",
        "https://openalex.org/W4230405732",
        "https://openalex.org/W2049017883",
        "https://openalex.org/W4300235091",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W2776207810",
        "https://openalex.org/W3092292656",
        "https://openalex.org/W2953073956",
        "https://openalex.org/W2951025380",
        "https://openalex.org/W4293861706",
        "https://openalex.org/W3035563045",
        "https://openalex.org/W2950768109",
        "https://openalex.org/W3035422918",
        "https://openalex.org/W3176196997",
        "https://openalex.org/W4287111954",
        "https://openalex.org/W4287324101",
        "https://openalex.org/W3177112732",
        "https://openalex.org/W4239072543",
        "https://openalex.org/W3035253074",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2594475271",
        "https://openalex.org/W2962858109"
    ],
    "abstract": "Despite the popularity of Vision Transformers (ViTs) and eXplainable AI (XAI), only a few explanation methods have been designed specially for ViTs thus far. They mostly use attention weights of the [CLS] token on patch embeddings and often produce unsatisfactory saliency maps. This paper proposes a novel method for explaining ViTs called ViT-CX. It is based on patch embeddings, rather than attentions paid to them, and their causal impacts on the model output. Other characteristics of ViTs such as causal overdetermination are considered in the design of ViT-CX. The empirical results show that ViT-CX produces more meaningful saliency maps and does a better job revealing all important evidence for the predictions than previous methods. The explanation generated by ViT-CX also shows significantly better faithfulness to the model. The codes and appendix are available at https://github.com/vaynexie/CausalX-ViT.",
    "full_text": "ViT-CX: Causal Explanation of Vision Transformers\nWeiyan Xie1 , Xiao-Hui Li2 , Caleb Chen Cao1 and Nevin L. Zhang1\n1 The Hong Kong University of Science and Technology, China\n2 Huawei Technologies Co., Ltd, China\n{wxieai, cao, lzhang}@ust.hk, {lixiaohui33}@huawei.com\nAbstract\nDespite the popularity of Vision Transformers\n(ViTs) and eXplainable AI (XAI), only a few ex-\nplanation methods have been designed specially for\nViTs thus far. They mostly use attention weights\nof the [CLS] token on patch embeddings and of-\nten produce unsatisfactory saliency maps. This pa-\nper proposes a novel method for explaining ViTs\ncalled ViT-CX. It is based on patch embeddings,\nrather than attentions paid to them, and their causal\nimpacts on the model output. Other characteristics\nof ViTs such as causal overdetermination are also\nconsidered in the design of ViT-CX. The empirical\nresults show that ViT-CX produces more meaning-\nful saliency maps and does a better job revealing\nall important evidence for the predictions than pre-\nvious methods. The explanation generated by ViT-\nCX also shows significantly better faithfulness to\nthe model. The codes and appendix are available at\nhttps://github.com/vaynexie/CausalX-ViT.\n1 Introduction\nVision Transformers (ViTs) are a new class of deep learning\nmodels that rival or even surpass the performance of convolu-\ntional neural networks (CNNs) on various vision tasks[Doso-\nvitskiy et al., 2020; Carionet al., 2020; Liuet al., 2021]. This\npaper is about explaining the predictions by ViTs. Several\nmethods have been previously proposed for this task, namely\nCGW1 [Chefer et al., 2021b], CGW2 [Chefer et al., 2021a]\nand TAM [Yuan et al., 2021 ]. Meanwhile, methods for ex-\nplaining CNNs such as Grad-CAM [Selvaraju et al., 2017 ],\nRISE [Petsiuk et al., 2018 ], and Score-CAM [Wang et al.,\n2020] can also be used to explain ViTs with minor adapta-\ntions. In this paper, we propose a novel method for explaining\nViTs called ViT-CausalX or ViT-CX for short. Visual exam-\nples and experiment results show that ViT-CX clearly outper-\nforms previous baselines in terms of faithfulness to model and\ninterpretability to human users (Figure 1 and Table 1).\nPrevious ViT explanation methods are mainly based on at-\ntention weights of the class token ([CLS ]) on patch embed-\ndings, or a combination of attention weights and class gradi-\nents. The use of attention weights for explaining NLP mod-\nels has been extensively debated, and the general conclusion\nseems to point to the negative side [Jain and Wallace, 2019;\nSerrano and Smith, 2019; Pruthi et al., 2020; Bastings and\nFilippova, 2020]. In ViTs, attention weights are concerned\nwith the importance of patch embeddings to the[CLS] token,\nbut not the semantic contents of the embeddings themselves.\nWe conjecture that better explanations can be generated us-\ning the semantic contents of patch embeddings instead of the\nattentions paid to them. ViT-CX is consequently developed.\nViT-CX is a specialized mask-based explanation method\ndesigned for ViT models. It generates masks by utilizing\npatch embeddings from a self-attention layer of a ViT model,\nwhich are arranged into a 3D tensor. The (x, y)-coordinates\nin the 3D tensor indicate the spatial information of the patch,\nwhile the z-coordinate represents its semantic content. By\nupsampling a frontal slice (with a fixed z) of the tensor to\nthe input image size, a ViT feature map is produced. These\nfeature maps (Figure 2 (b.1 - b.5)) are more meaningful than\nthe attention weight maps (Figure 2 (a.1 - a.5)), and are used\nby ViT-CX to generate explanations. The method applies the\nfeature maps as masks (Figure 2 (c.1 - c.5)) to the input im-\nage, calculates the causal impact score of the masks on the\noutput, and combines the scores to generate saliency maps.\nOther existing mask-based methods designed originally for\nCNNs include Occlusion [Zeiler and Fergus, 2014 ], RISE\n[Petsiuk et al., 2018 ] and Score-CAM [Wang et al., 2020 ].\nAmong them, Score-CAM, which uses CNN feature maps as\nmasks, is ViT-CX’s most similar counterpart for CNNs. How-\never, there are three technical issues that arise when adapting\nit directly to ViTs due to the characteristics of ViTs. We dis-\ncuss these issues and include the solutions to them in ViT-CX.\nThe first issue is that applying a mask to an image might\ncause unintended artifacts when explaining ViTs. We con-\nsider this matter when calculating the causal impact scores of\nthe masks. Second, when using masks to make explanations,\nsome pixels might be included in more masks than others,\nleading to pixel coverage bias (PCB). PCB and its correction\nhave been discussed in the context of random sampling [Pet-\nsiuk et al., 2018; Sattarzadeh et al., 2021]. However, this bias\nhas not been widely addressed in mask-based methods for\nCNNs, including the Score-CAM. In this paper, we shows\ntheoretically and empirically that PCB is a severe issue for\nViTs due to the causal overdetermination. We also show that\nthe PCB correction can significantly improve the quality of\nViT explanations.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1569\ngoldfish CGW1 CGW2 TAM ScoreCAM ViT-CX dogsled CGW1 CGW2 TAM ScoreCAM ViT-CX\nDel↓ 0.258 0.355 0.271 0.532 0.202 Del↓ 0.097 0.147 0.498 0.698 0.078\nIns↑ 0.829 0.833 0.866 0.553 0.879 Ins ↑ 0.827 0.820 0.692 0.421 0.884\nvine snake CGW1 CGW2 TAM ScoreCAM ViT-CX head cabbage CGW1 CGW2 TAM ScoreCAM ViT-CX\nDel↓ 0.164 0.122 0.114 0.108 0.106 Del↓ 0.506 0.598 0.373 0.634 0.351\nIns↑ 0.410 0.544 0.337 0.598 0.603 Ins↑ 0.798 0.780 0.801 0.535 0.848\nFigure 1: Explaining the predictions of ViT-B/16 on four images. The saliency maps by ViT-CX are clearly more meaningful than those by\nprevious attention-based methods (CGW1, CGW2, TAM), highlighting all regions apparently important to predictions. They are also more\nfaithful to the model as measured by the deletion (Del) and insertion (Ins) AUC metrics. In contrast, the direct application of Score-CAM to\nViTs can lead to nonsensical explanations (e.g., the goldfish and head cabbage example).\n(a.1) (a.2) (a.3) (a.4) (a.5)\n(b.1) (b.2) (b.3) (b.4) (b.5)\n(c.1) (c.2) (c.3) (c.4) (c.5)\nFigure 2: ViT feature maps (b.1 - b.5) are frontal slides of a 3D\ntensor made up of patch embedding vectors (as fibers). They are\ngenerally more meaningful than attention weight maps (a.1 - a.5),\nand they are used as ViT masks (c.1 - c.5) to generate explanations.\nThird, mask-based methods usually require a large num-\nber of masks in order to generate high-quality explanations,\nwhich can lead to inefficient online performance, especially\nfor ViTs which are usually heavier than CNNs. To address\nthis challenge, we empirically show that many ViT patch em-\nbeddings are similar, and propose clustering similar masks\nto significantly reduce the number of masks. We show that\nthis strategy is effective for ViTs but does not work well for\nCNNs in general, as there is less variance among feature maps\nin ViTs than in CNNs. Note that one might suggest address-\ning the pixel coverage bias by using a large number of masks,\nwhich can make the explanation even more inefficient.\nIn summary, we make the following contributions regard-\ning ViT explanation in this paper:\n• We propose to derive explanations for ViTs from the se-\nmantic contents of patch embeddings rather than atten-\ntions paid to them;\n• We develop a mask-based method for explaining ViTs\nthat take into account the characteristics of ViTs, namely\nlow variance of feature maps, strong shape recognition\ncapability, and prevalence of causal overdetermination;\n• We empirically show that ViT-CX significantly outper-\nforms previous baselines in terms both of the faithful-\nness to model and interpretability to human users.\n2 Related Work\n2.1 Explanation Methods for Vision Transformers\nThe earliest methods for explaining ViT models are based on\nattention weights. All attention weights at an attention head\ncan be reshaped and upsampled to the input size to form a\nsaliency map. Rollout [Abnar and Zuidema, 2020 ] consid-\ners all heads from multiple layers and combines the corre-\nsponding attention maps to form one saliency map. Partial\nLRP [V oitaet al., 2019 ] is similar to Rollout, except that it\nassigns different weights to different heads, which are com-\nputed using Layer-wise Relevance Propagation (LRP) [Bach\net al., 2015 ]. The saliency maps produced by Rollout and\nPartial LRP are not class-specific since the attention weights\nare class-agnostic. As such, those methods cannot be used to\nexplain the reasons for particular output classes.\nThere are methods aiming to explain a particular output\nclass. CGW1 [Chefer et al., 2021b] is similar to Partial LRP,\nexcept that the gradients of the class score with respect to\nthe heads are also considered, alongside LRP weights, when\ncombining attention maps from different heads. In CGW2\n[Chefer et al., 2021a ], the LRP weights are removed since\nthey are found to be unnecessary. Transition Attention Map\n(TAM) [Yuan et al., 2021 ] is similar to CGW2 except that\nsimple gradients are replaced by integrated gradients [Sun-\ndararajan et al., 2017]. Figure 1 shows several saliency maps\nproduced by CGW1, CGW2 and TAM. They are clearly\nless satisfactory than those by ViT-CX. Moreover, attention\nweight-based methods are not applicable to ViTs [Liu et al.,\n2021; Chu et al., 2021; Zhang et al., 2022] that do not have a\n[CLS] token, since they utilize the attention maps between\nthe [CLS] token and patch tokens. ViT-CX, on the other\nhand, relies on patch embeddings only and can be applied\nto a wider range of ViT variants.\n2.2 Mask-based Explanation Methods\nWhile there are only a few methods for explaining ViT mod-\nels, a large number of methods have been proposed to explain\nCNN models. Mask-based explanation methods are one sub-\nclass. They generate an explanation based on a collection\nof masks M = {M1, ··· , MN }, where each mask Mi is of\nthe same size as the input image X, and its pixel values are\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1570\nFigure 3: Overview of ViT-CX. (a) Mask Generation: A set of semantic masks is generated from the patch embeddings with agglomerative\nclustering. (b) Mask Aggregation: A saliency map is created by combining the masks using a debiased causal impact score based on the\nmasked image with randomness to overcome artifacts. Pixel coverage frequencies are used in this step to correct pixel coverage bias.\nbetween 0 and 1. A saliency map, as an explanation, is cre-\nated by aggregating the masks weighted by causal impacts\nof masks on the model output. Intuitively, we can think of a\nmask Mi as a “pixel team”, and the saliency value of a pixel\nis an aggregation of the causal impact scores of “teams” it\nis on. Occlusion [Zeiler and Fergus, 2014 ], RISE [Petsiuk\net al., 2018] and Score-CAM [Wang et al., 2020] are typical\nmask-based explanation methods.\nIn mask-based explanation methods, the causal impact of\na mask is usually measured by the class score (f (·)) of the\nmasked image X ⊙ Mi on the target class y. The saliency\nvalue of a pixel x is determined by:\nS(x) =\nNX\ni=1\nf(y|X ⊙ Mi)Mi(x). (1)\nFor visualization, the saliency values are normalized to inter-\nval [0, 1] by (S(x) − minx S(x))/(maxx S(x) − minx S(x).\nScore-CAM is a mask-based explanation method proposed\nto CNNs. It uses CNN feature maps as masks. One can apply\nit to ViTs by replacing CNN feature maps with ViT feature\nmaps. However, this simple adaptation does not lead to qual-\nity explanations. ViT-CX improves it significantly by taking\ninto account characteristics of ViTs.\nViT Shapley [Covert et al., 2022 ] is another mask-based\nexplanation for ViTs proposed very recently. It trains a sep-\narate explainer (another ViT model) to estimate the Shap-\nley Values. ViT Shapley has been evaluated only on small\ndatasets (ImageNette and MURA) because the cost of train-\ning the explainer is high. Its performance on large datasets\nsuch as ImageNet is difficult to assess. In addition, the ex-\nplainer is a black-box and it introduces new opacity which\nmight need further explanation.\n3 ViT-CX\nAn overview of ViT-CX is shown in Figure 3. ViT-CX fol-\nlows the two-phase setting of mask-based explanation meth-\nods: mask generation followed by the mask aggregation. In\nthe first phase, a small set of semantic masks is generated\nfrom the patch embeddings in the target ViT model with a\nclustering algorithm applied to reduce the number and redun-\ndancy of masks (Section 3.2). In the second phase, we pro-\npose a debiased causal impact scoreto overcome the artifact\nbias (Section 3.3), and the final saliency map is obtained by\npixel coverage bias corrected summation(Section 3.4).\n3.1 Preliminaries\nIn ViT models, an image X ∈ RH×W×C is split into N =\nHW/p2 patches, with the j-th patch represented by a 2D vec-\ntor Xj ∈ R(p×p)×C, where H, W, Care the height, width,\nand the number of channels of the image, and (p, p) is the\nspatial resolution of each patch. The patches are mapped to\nembeddings with D dimensions via linear projection. The\nembeddings are fed into L transformer blocks. Each block\nincludes two modules: A Multi-Head Self-Attention (MHSA)\nmodule and a Multi-Layer Perceptron (MLP) module. They\nyield new embeddings of the patches. We denote the patch\nembeddings at the output of transformer block i as E(i) ∈\nRNi×Di , where Ni is the number of patch tokens and Di is\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1571\nFigure 4: ViT masks has lower variance then CNN masks. (a) Dis-\ntribution of pairwise similarities (Sim(i, j)) between masks for the\ngoldfish image; (b) Distribution of mean pairwise mask similar-\nitiy (mSim) for all images in the ImageNet validation set.\nthe feature dimension at that block. The Ni and Di remain\nconstant in vanilla ViT [Dosovitskiy et al., 2020] as the com-\nputation proceeds from one block to another, and are grad-\nually changed in the more recent ViTs [Wang et al., 2021;\nChu et al., 2021; Liuet al., 2021] with hierarchical structures.\n3.2 Mask Generation\nFrom ViT Feature Maps to ViT Masks\nWe create the masks from the embeddings E(i) at the output\nof the attention module of a chosen transformer block i (usu-\nally the last transformer block for the vanilla ViT and possi-\nbly other block(s) for other ViT models). The embeddings are\nfirst reshaped into a 3D tensor of size√Ni ×√Ni ×Di. Each\nfiber in the tensor corresponds to the embedding of a patch,\nand the (x, y)-coordinates correspond to the spatial location\nof the patch in the input image. The frontal slices of the ten-\nsor are upsampled to the size of the input image, resulting\nin the ViT feature maps. The feature maps are subsequently\nnormalized to the interval [0, 1] to get ViT masks. A set of\nmasks is built from those ViT masks, denoted as Mvit =\n{M1, ··· , MDi } where Mj ∈ RH×W (j = 1, ··· , Di). The\nnumber of masks in Mvit is Di (Di = 768in ViT-B/16).\nHigh Degree of Redundancy in ViT Masks\nWe observe a high degree of redundancy among the ViT\nmasks. This is clear from Figure 2 (c.1 - c.5). To quan-\ntify the level of redundancy among ViT masks, we compute\nthe pairwise cosine similarity between the mask Mi and Mj:\nSim(i, j) = Mi\n||Mi|| ·\nMT\nj\n||Mj||.\nThe mean pairwise cosine similarity mSim is defined as:\nmSim = PDi\ni=1\nPDi\nj=i+1 Sim(i, j)/[Di(Di − 1)/2]. There\nis a high average mSim for the mask set Mvit obtained\nfrom the last transformer block of ViT-B/16 - 0.92 *, and\nthe probability density distribution of the mSim over im-\nages in ImageNet validation set is shown in Figure 4 (b).\nFigure 4 (a) shows the distribution of pairwise similarities\nSim(i, j) (j > i) for the goldfish image, indicating most\nViT masks of this image are close to each other. This points\nto the possibility of clustering similar ViT masks together to\nimprove the explanation efficiency.\nIn contrast, there is much more variance among CNN\nmasks. This is clear from Figure 4 where we also show the\nprobability density distribution of masksMresnet from a pop-\nular CNN, ResNet50[He et al., 2016]. Given that most masks\n*Average over the 50,000 images in ImageNet validation set.\nare not similar to each other in Mresnet, applying the cluster-\ning on it can force the dissimilar masks to be grouped and\ncause a significant information loss.\nClustering the ViT Masks\nTo reduce the redundancy in the mask set and improve the\nexplanation efficiency, we merge the similar ViT masks. We\nuse the agglomerative clustering algorithm [M¨ullner, 2013;\nMurtagh and Legendre, 2014] which recursively merges data\npoints with minimum pairwise distance. The pairwise dis-\ntance of the mask Mi and Mj is measured by:\nDistance(i, j) = 1− Mi\n||Mi|| · MT\nj\n||Mj|| .\nWe stop the recursive merging based on a given distance\nthreshold δ above which clusters will not be merged. Sup-\npose the masks in Mvit is clustered into K groups and each\ngroup is denoted as M(k)\nvit (k = 1, 2, ··· , K). Here K is dif-\nferent for different images depending on the distribution of\ntheir ViT feature maps. We take the mean of the masks in\neach group to build the mask set Mcx used in ViT-CX:\nMcx = {M1, M2, ··· , MK},\nwhere Mk = 1\n|M(k)\nvit|\nP\nM∈M(k)\nvit\nM , and k = 1,··· , K.\nAfter the clustering, the number of masks decreases a lot\n(K is 63 on average after clustering the previous Mvit with\nδ = 0.1). The reduction in the number of masks reduces the\nnumber of “pixel teams” that we need to investigate the causal\nimpact for and thus can improve the online efficiency. At the\nsame time, we show in Section 5 that reducing the amount\nof “pixel teams” in this way has only a slight impact on the\nexplanation quality.\n3.3 Artifacts and Debiased Causal Impact Score\nArtifactual Effects of Masked Pixels\nConsidering the mask Mi as a “pixel team”, an implicit as-\nsumption in mask-based explanations is that only members of\nthe “pixel team” contribute to the causal impact score, while\nnon-member pixels do not.\nIn previous mask-based explanation methods, the causal\nimpact score of a mask is usually measured by the prediction\nscore of target class y on the masked image - f(y|X ⊙ Mi),\nwhich can be problematic for ViTs. This problem arises from\nthe violation of the implicit assumption. The masked pix-\nels, as non-members of the “pixel team”, are at the same zero\npixel value. They can co-create artifact that provides infer-\nable information to the model and contributes strongly to the\ncausal impact score, leading to the biased impact score.\nFigure 5 case (a) shows an example of the artifact. The\nViT feature map focuses on the background rather than the\nforeground, where the foreground pixels (goldfish’s pixels)\nshare the least salient values. When the ViT mask is com-\nbined with the image, the foreground pixels are masked out.\nThis erases the detailed features of the goldfish, such as tex-\nture and color. However, the masked pixels together create\nthe shape of the goldfish in the masked image, resulting in an\nunreasonably high prediction score of goldfish - 0.973.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1572\nInput: P(g)=0.998\nViT Feature Map ViT Mask Masked Image Masked Image with Noise Image with Noise\nMi X ⊙ Mi X ⊙ Mi + ϵi X + ϵi\nCase (a)\nP(g)=0.973 P(g)=0.005 P(g)=0.991\nOriginal CI = 0.973, Debiased CI = 0.012\nCase (b)\nP(g)=0.994 P(g)=0.981 P(g)=0.986\nOriginal CI = 0.994, Debiased CI = 0.993\nFigure 5: Comparison of the original causal impact score (CI) and the proposed debiased causal impact score on thegoldfish (g) example.\nThe original impact score is the prediction score of target class y on the masked image - f(y|X ⊙ Mi). The proposed debiased impact\nscore based on masked images with randomness aims to address the artifact case (case (a)) where the masked pixels lead to misleading\nf(y|X ⊙ Mi). Meanwhile the causal impact score in the normal case (case (b)) is not affected.\nThis phenomenon might relate to the stronger shape recog-\nnition ability of ViTs when making the inference, as pointed\nout by [Naseer et al., 2021; Tuli et al., 2021].\nNoise Addition to Correct Artifact Bias\nTo correct the artifact bias, we need to corrupt the information\nfrom the masked pixels with the same zero pixel values. This\ncorruption can be achieved by adding random noise to these\npixels. Therefore we propose to add the random noise in a\nsoft way to the masked image: X ⊙Mi + (1−Mi)Z , where\nZ ∈ RH×W×C follows a Gaussian distributionN(0, σ2) with\na small standard deviation σ. Adding the noise based on\nthe complement of mask values, i.e., (1 − Mi), allows only\nthe distribution of masked pixels (non-member pixels) to be\nmainly affected while the distribution of pixels with the high-\nest mask values (pixels in the “team”) is minimally affected.\nIn Figure 5 (a), after the noise is added, the prediction score\nof goldfish drops to 0.005. In case (b) where the goldfish\nbody is preserved perfectly after masking, adding noises only\nmakes the prediction score drop a tiny bit (0.994 → 0.981).\nDebiased Causal Impact Score\nBased on random noise addition, to reduce the effect of arti-\nfacts, we replace the term f(y|X ⊙ Mi) in Equation (1) with\na debiased version of causal impact score:\ns(X, y, Mi) =\nf(y|X ⊙ Mi + ϵi) + [f(y|X) − f(y|X + ϵi)],\n(2)\nwhere ϵi = (1− Mi)Z ∈ RH×W×C, Z∼ N(0, σ2).\nIn Equation (2), the term [f(y|X) − f(y|X + ϵi)] is the drop\non the prediction score of target class y when the random\nnoise is added to the unmasked image. We add this term to\ncancel the effect of the random noise addition and ensure the\nresulting scores purely reflect the effect of the masks Mi on\nthe image. The use of Equation (2) as the causal impact score\nwhen explaining ViTs is a good solution to the case of arti-\nfacts caused by the masked pixels, as the examples shown in\nFigure 5. The ablation study in Section 5 shows the effective-\nness of this debiased score in more general cases.\n3.4 Pixel Coverage Bias and Its Correction\nGiven the set of masksMcx = {M1, M2, ··· , MK}, the cov-\nerage frequency of a pixel x is defined as:\nρ(x) = 1\nK\nKX\ni=1\nMi(x).\nPixel coverage bias (PCB) refers to the phenomenon that dif-\nferent pixels might have different coverage frequencies. Ac-\ncording to Equation (1), the saliency value of a pixel, before\nbeing normalized to [0, 1], is the summation of the causal im-\npact scores of the “pixel teams” (masks) of which it is a mem-\nber. Consequently, the more “teams” a pixel on, the higher its\nsaliency value. This is clearly not justified.\nAdverse Effects of PCB\nAlthough pixel coverage bias is a common issue in mask-\nbased explanation methods, it does not necessarily cause se-\nvere degradation in explanation quality. However, it can\nseverely degrade explanation quality in the cases of causal\noverdetermination. In those cases, the correct prediction can\nbe made by various small patches of the input image[White et\nal., 2021], resulting that the causal impact score s(X, y, Mi)\nof most masks is close to 1. To understand why PCB can\ncause undesirable explanation results in such cases, we let\nµ = 1\nK\nPK\ni=1 s(X, y, Mi) and βi = s(X, y, Mi) − µ, and\ndivide the saliency score S(x) of a pixel into two parts:\nS(x) =\nKX\ni=1\nβiMi(x) +\nKX\ni=1\nµMi(x) (3)\n=\nKX\ni=1\nβiMi(x) +µKρ(x). (4)\nIn the overdetermined cases, the µ (mean of impact scores)\nis close to 1, but βi is small for most i’s. Thus the second\nterm, which is essentially the pixel coverage frequency, is\nmuch larger than the first term. When normalized to the inter-\nval [0, 1], the first term basically vanishes. That leads to the\nsaliency maps closely resemble the coverage frequency maps\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1573\n(a) (a.1) (a.2) (a.3)\nFigure 6: Impact of PCB: The nonsensical saliency map (a.2) closely\nresembles the coverage frequency map (a.1). After PCB correction,\nthe map (a.3) becomes meaningful (target class - dogsled).\nand fail to highlight areas important to the ViT prediction.\nOne example of such case is given in Figure 6.\nCausal Overdetermination in ViTs\nCausal overdetermination is common with ViTs. We find\n20.95% of the images in ImageNet validation set have a mean\ncausal impact score µ greater than 0.9 when applying the\nmask Mcx to explain ViT/B-16. Among those images, the av-\nerage variance of the score is 0.036, which is small. They fit\nthe features of highµ and low βi mentioned above. This find-\ning is also consistent with [Naseer et al., 2021]’s observation\nthat the class scores in ViTs are more robust to the removal of\nsmall patches in the input image than many popular CNNs.\nCorrection for PCB\nA simple way to correct for PCB is to divide the saliency\nvalue S(x) by the coverage frequency ρ(x). This results in\nthe corrected saliency value:\nSc(x) =S(x)\nρ(x) =\nKX\ni=1\ns(X, y, Mi)Mi(x)\nρ(x) , (5)\nwhere Sc(x) = 0by definition when ρ(x) = 0. Intuitively,\nthe corrected saliency value of a pixel is the sum of the causal\nimpact scores of the “teams” of which it is a member, divided\nby the number of “teams” it participates in. Similar to Equa-\ntion (4), we decompose Sc(x) into two parts:\nSc(x) =\nKX\ni=1\nβi\nMi(x)\nρ(x) + µK. (6)\nThe second term is still much larger than the first term. How-\never, it is a constant and does not depend on the pixel. When\nthe saliency values Sc(x) are normalized to the interval [0, 1]\nfor visualization, the influence of the second term is wholly\neliminated. This is why meaningful saliency maps emerge in\nFigure 6 after correcting for PCB. In Section 5.4, we show the\ncorrection improves the overall explanation quality greatly.\nThis correction is also included in[Petsiuk et al., 2018] and\n[Sattarzadeh et al., 2021]. Their motivation is to correct the\nbias caused by the finiteness of random sampling. We here\nprovide another viewpoint on it and show its importance in\ncausal overdetermination cases.\n4 Experiments\n4.1 Evaluation Metrics\nWe evaluate ViT-CX following a protocol similar to how pre-\nvious ViT explanation methods are evaluated, using the Dele-\ntion and Insertion AUC [Petsiuk et al., 2018], Pointing Game\n[Zhang et al., 2018 ] and visual examples. This scheme is\ncommonly used to evaluate explanation methods for CNNs.\nDeletion and Insertion AUC: The two metrics are about\nthe faithfulness of an explanation (saliency map) to the tar-\nget model, i.e., whether pixels with high saliency values are\nreally important to the prediction[Petsiuk et al., 2018]. Dele-\ntion AUC measures how fast the score of the target class drops\nas pixels are deleted from the image in descending order of\nthe saliency values. Insertion AUC measures how fast the\nscore increases when pixels are inserted into an empty canvas\nin that order. Smaller deletion AUC and larger insertion AUC\nindicate better faithfulness.\nPointing Game: This metric is about the interpretability\nof an explanation, i.e., whether it provides qualitative un-\nderstanding between input and output [Ribeiro et al., 2016;\nDoshi-Velez and Kim, 2017 ]. In Pointing Game, the\nsaliency maps are compared with human-annotated bound-\ning boxes. For each pair of saliency map and bounding\nbox, if the pixel with the highest saliency value falls in-\nside the box, it is considered a hit. Otherwise it is consid-\nered a miss. The Pointing Game Accuracy is defined as:\nAcc = #Hits/(#Hits + #Misses).\n4.2 Experiment Settings\nModels and Dataset: Three ViT variants are used in our\nexperiments: (1) ViT-B/16 [Dosovitskiy et al., 2020 ], the\nvanilla ViT; (2) DeiT-B/16-Distill [Touvron et al., 2021 ], a\nimproved version of the vanilla ViT with a distillation to-\nken; (3) Swin-B [Liu et al., 2021 ], a hierarchical ViT. We\nuse 5,000 images randomly selected from the ILSVRC2012\nvalidation set [Deng et al., 2009]. All experiments are run on\nan Intel Xeon E5-2620 CPU and an NVIDIA 2080 Ti GPU.\nHyper-parameters Setting: To generate the masks Mvit,\nwe use feature maps from the last transformer block for ViT-\nB and DeiT-B, and choose those from the last block of the\nsecond to last stage for Swin-B; When clustering on Mvit to\ngenerate the mask set Mcx, the distance threshold δ is set to\n0.1 for ViT-B and DeiT-B, and set to 0.05 for Swin-B; The\nstandard deviation σ of the Gaussian noise ϵi is set to 0.1.\nBaselines: In this section, we compare ViT-CX with three\ngroups of baselines: (a) Five attention weights-based meth-\nods, namely Rollout [Abnar and Zuidema, 2020], Partial LRP\n[V oitaet al., 2019 ], CGW1 [Chefer et al., 2021b ], CGW2\n[Chefer et al., 2021a], and TAM[Yuanet al., 2021]; (b) Three\nmask-based methods, namely Occlusion [Zeiler and Fergus,\n2014], RISE [Petsiuk et al., 2018] and Score-CAM [Wang et\nal., 2020]; (c) Three gradient-based methods, namely Grad-\nCAM [Selvaraju et al., 2017], Integrated-Grad [Sundararajan\net al., 2017] and Smooth-Grad [Smilkov et al., 2017]. In ad-\ndition, Appendix D provides a comparison between ViT-CX\nand ViT Shapley, a mask-based explanation method for ViTs\nintroduced in a recent study by [Covert et al., 2022].\n4.3 Results\nThe main results are in Table 1.\nFaithfulness: ViT-CX has the lowest deletion AUC values\nacross the board, being 10% lower than the next best. ViT-\nCX also enjoys the highest insertion AUC values in all cases.\nThose indicate that ViT-CX is more faithful to the target mod-\nels than all baselines.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1574\nViT-B DeiT-B Swin-B\nDel ↓ Ins ↑ PG Acc ↑ Del ↓ Ins ↑ PG Acc ↑ Del ↓ Ins ↑ PG Acc ↑\nViT-CX 0.161 0.620 86.42% 0.211 0.802 86.93% 0.271 0.761 92.31%\nNumber of Masks Average: 63, Std: 11 Average: 70, Std:12 Average: 95, Std:12\nRollout 0.251 0.517 60.91% 0.406 0.642 35.70% — — —\nPartial LRP 0.239 0.499 66.52% 0.349 0.655 61.25% — — —\nCGW1 0.201 0.542 77.14% 0.286 0.717 70.54% — — —\nCGW2 0.209 0.549 70.94% 0.271 0.736 70.54% — — —\nTAM 0.180 0.556 77.87% 0.240 0.747 75.47% — — —\nOcclusion 0.291 0.571 64.75% 0.380 0.801 59.51% 0.448 0.752 69.65%\nRISE 0.234 0.581 73.30% 0.366 0.759 71.84% 0.416 0.727 75.07%\nScore-CAM 0.291 0.471 48.89% 0.439 0.576 50.12% 0.424 0.641 69.65%\nGrad-CAM 0.212 0.456 50.45% 0.250 0.743 79.24 % 0.356 0.693 88.46%\nIntegrated-Grad 0.184 0.263 10.61% 0.259 0.362 10.74% 0.420 0.483 7.69%\nSmooth-Grad 0.174 0.438 16.96% 0.231 0.528 31.05% 0.369 0.505 14.52%\nTable 1: Main experiment results: Boldface and underline indicate best and second best performance, and ‘—’ means not applicable. ViT-\nCX significant outperforms all baselines in terms of the faithfulness metrics deletion (Del) and insertion AUC (Ins), and in terms of the\ninterpretability metric Pointing Game Accuracy (PG Acc). The average number of masks ViT-CX used to explain an image is also provided.\nMasks Causal Impact PCB Number of Masks Del ↓ Ins ↑ PG Acc ↑ Average Time (s)\nViT-CX Mcx s(X, y, Mi) √ 70 ± 12 0.211 0.802 86.93% 1.15 ± 0.15\nVariant 1 Mvit s(X, y, Mi) √ 768 ± 0 0.232 0.810 85.52% 8.23 ± 0.03\nVariant 2 Mrandom s(X, y, Mi) √ 5000 ± 0 0.323 0.734 75.12% 77.78 ± 3.46\nVariant 3 Mcx f(y|X ⊙ Mi) √ 70 ± 12 0.281 0.742 77.78% 0.98 ± 0.12\nVariant 4 Mcx s(X, y, Mi) × 70 ± 12 0.303 0.727 74.37% 1.12 ± 0.14\nVariant 5 Mcx f(y|X ⊙ Mi) × 70 ± 12 0.339 0.686 67.56% 0.95 ± 0.08\nTable 2: Ablation study on ViT-CX (based on DeiT-B, GPU batch size=100). The analyzed components include: 1. Masks - the mask set\nused in the explanation. Mcx is generated from ViT feature maps with reduced number by clustering and Mvit is the one without clustering.\nMrandom is randomly generated; 2. Causal Impact- the measure of casual impact of the mask. s(X, y, Mi) is our proposed debiased causal\nimpact score and f(y|X ⊙ Mi) is the prediction score of masked image; 3. PCB - whether to apply the PCB correction.\nInterpretability: ViT-CX enjoys significantly higher\nPointing Game accuracy than the baselines in all cases. This\nimplies that the explanations of ViT-CX are more consistent\nwith human-annotated bounding boxes. As a supplement to\nquantitative metrics, four visual examples have been shown\nin Figure 1 and more examples are given in Appendix A.\nComputation Cost: ViT-CX uses less than 100 masks on\naverage to explain an image. The computation cost is greatly\nreduced compared to previous mask-based methods.\nSanity Check: As a causal method, ViT-CX is sensitive to\nthe changes in model parameters and passes the sanity check\n[Adebayo et al., 2018]. See Appendix B for details.\nLocalization: As a training-free method, ViT-CX shows\ncomparable localization performance to recently proposed\nViT-based weakly supervised object localizers that require\nextra modules and training. The details are in Appendix C.\n4.4 Ablation Study\nThe collection of masks used in the explanation, the causal\nimpact score, and whether the PCB correction is applied are\nvaried to study the effect of different components in ViT-CX.\nThe results are in Table 2.\nComparison of ViT-CX with Variant 1 shows that cluster-\ning on the mask set Mvit reduces the number of masks from\n768 to 70 on average, and the mean time to explain an image\nis reduced to ∼1 second. Meanwhile the explanation quality\nis mainly remained unaffected. Additionally, ViT-CX outper-\nforms explanations generated with random masks (Variant 2)\nin terms of explanation quality and efficiency.\nComparisons with Variants 3-5 emphasize the significance\nof addressing pixel coverage bias and using a debiased causal\nimpact score in ViT-CX to avoid artifact effects. Without\nthese steps, the explanation quality is largely affected. These\nresults suggest that the clustering of masks, the correction for\nPCB and the debiasing of causal impact scores are crucial\ncomponents of ViT-CX. A more detailed sensitivity analysis\nof hyperparameters in ViT-CX can be found in Appendix F.\n5 Conclusion\nPrevious attention weights-based and mask-based explainers\nhave not been able to consistently provide satisfactory expla-\nnations for ViTs. ViT-CX, a specially designed mask-based\nexplainer for ViTs, addresses the issues of low explanation\nefficiency, misleading causal impact scores caused by arti-\nfacts, and pixel coverage bias of masks. Our solutions to these\nissues are demonstrated to lead to high-quality explanations\nfor various ViT image classifiers. Future work could involve\nextending ViT-CX concepts to explain ViT models for other\ntasks like object detection and segmentation.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1575\nAcknowledgments\nWe thank the deep learning computing framework MindSpore\n(https://www.mindspore.cn) and its team for the support on\nthis work. Research on this paper was supported in part by\nHong Kong Research Grants Council under grant 16204920.\nWeiyan Xie was supported in part by the Huawei PhD Fellow-\nship Scheme. We thank Prof. Janet Hsiao, Yueyuan Zheng,\nLuyu Qiu, and Yi Yang for valuable discussions.\nContribution Statement\nWeiyan Xie and Xiao-Hui Li contributed equally to this work.\nThis work is done when Caleb Chen Cao was in Huawei Re-\nsearch Hong Kong.\nReferences\n[Abnar and Zuidema, 2020] Samira Abnar and Willem\nZuidema. Quantifying attention flow in transformers. In\nAnnual Meeting of the Association for Computational\nLinguistics, pages 4190–4197, 2020.\n[Adebayo et al., 2018] Julius Adebayo, Justin Gilmer,\nMichael Muelly, Ian Goodfellow, Moritz Hardt, and\nBeen Kim. Sanity checks for saliency maps. In Ad-\nvances in Neural Information Processing Systems, pages\n9525–9536, 2018.\n[Bach et al., 2015] Sebastian Bach, Alexander Binder,\nGr´egoire Montavon, Frederick Klauschen, Klaus-Robert\nM¨uller, and Wojciech Samek. On pixel-wise explanations\nfor non-linear classifier decisions by layer-wise relevance\npropagation. PloS one, 10(7):e0130140, 2015.\n[Bastings and Filippova, 2020] Jasmijn Bastings and Katja\nFilippova. The elephant in the interpretability room: Why\nuse attention as explanation when we have saliency meth-\nods? In Proceedings of the Third BlackboxNLP Workshop\non Analyzing and Interpreting Neural Networks for NLP,\npages 149–155, 2020.\n[Carion et al., 2020] Nicolas Carion, Francisco Massa,\nGabriel Synnaeve, Nicolas Usunier, Alexander Kirillov,\nand Sergey Zagoruyko. End-to-end object detection with\ntransformers. In European conference on computer vision,\npages 213–229. Springer, 2020.\n[Chefer et al., 2021a] Hila Chefer, Shir Gur, and Lior Wolf.\nGeneric attention-model explainability for interpreting bi-\nmodal and encoder-decoder transformers. In Proceedings\nof the IEEE/CVF International Conference on Computer\nVision, pages 397–406, 2021.\n[Chefer et al., 2021b] Hila Chefer, Shir Gur, and Lior Wolf.\nTransformer interpretability beyond attention visualiza-\ntion. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 782–791,\n2021.\n[Chu et al., 2021] Xiangxiang Chu, Zhi Tian, Yuqing Wang,\nBo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and\nChunhua Shen. Twins: Revisiting the design of spatial\nattention in vision transformers. Advances in Neural In-\nformation Processing Systems, 34:9355–9366, 2021.\n[Covert et al., 2022] Ian Covert, Chanwoo Kim, and Su-In\nLee. Learning to estimate shapley values with vision trans-\nformers. arXiv preprint arXiv:2206.05282, 2022.\n[Deng et al., 2009] Jia Deng, Wei Dong, Richard Socher,\nLi-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-\nscale hierarchical image database. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition, pages 248–255. Ieee, 2009.\n[Doshi-Velez and Kim, 2017] Finale Doshi-Velez and Been\nKim. Towards a rigorous science of interpretable machine\nlearning. arXiv preprint arXiv:1702.08608, 2017.\n[Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition\nat scale. In International conference on learning represen-\ntations, 2020.\n[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing\nRen, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 770–778,\n2016.\n[Jain and Wallace, 2019] Sarthak Jain and Byron C Wallace.\nAttention is not explanation. In Proceedings of NA Annual\nMeeting of the Association for Computational Linguistics-\nHLT, pages 3543–3556, 2019.\n[Liu et al., 2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu,\nYixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using\nshifted windows. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 10012–\n10022, 2021.\n[M¨ullner, 2013] Daniel M ¨ullner. fastcluster: Fast hierarchi-\ncal, agglomerative clustering routines for r and python.\nJournal of Statistical Software, 53:1–18, 2013.\n[Murtagh and Legendre, 2014] Fionn Murtagh and Pierre\nLegendre. Ward’s hierarchical agglomerative clustering\nmethod: which algorithms implement ward’s criterion?\nJournal of classification, 31(3):274–295, 2014.\n[Naseer et al., 2021] Muzammal Naseer, Kanchana Ranas-\ninghe, Salman Khan, Munawar Hayat, Fahad Khan, and\nMing-Hsuan Yang. Intriguing properties of vision trans-\nformers. In Advances in Neural Information Processing\nSystems, 2021.\n[Petsiuk et al., 2018] Vitali Petsiuk, Abir Das, and Kate\nSaenko. Rise: Randomized input sampling for explana-\ntion of black-box models. In Proceedings of the British\nMachine Vision Conference, 2018.\n[Pruthi et al., 2020] Danish Pruthi, Mansi Gupta, Bhuwan\nDhingra, Graham Neubig, and Zachary C Lipton. Learn-\ning to deceive with attention-based explanations. In An-\nnual Meeting of the Association for Computational Lin-\nguistics, pages 4782–4793, 2020.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1576\n[Ribeiro et al., 2016] Marco Tulio Ribeiro, Sameer Singh,\nand Carlos Guestrin. ” why should i trust you?” explain-\ning the predictions of any classifier. In Proceedings of\nthe ACM SIGKDD international conference on knowledge\ndiscovery and data mining, pages 1135–1144, 2016.\n[Sattarzadeh et al., 2021] Sam Sattarzadeh, Mahesh Sud-\nhakar, Anthony Lem, Shervin Mehryar, Konstantinos N\nPlataniotis, Jongseong Jang, Hyunwoo Kim, Yeonjeong\nJeong, Sangmin Lee, and Kyunghoon Bae. Explaining\nconvolutional neural networks through attribution-based\ninput sampling and block-wise feature aggregation. In\nProceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 35, pages 11639–11647, 2021.\n[Selvaraju et al., 2017] Ramprasaath R Selvaraju, Michael\nCogswell, Abhishek Das, Ramakrishna Vedantam, Devi\nParikh, and Dhruv Batra. Grad-cam: Visual explanations\nfrom deep networks via gradient-based localization. In\nProceedings of the IEEE/CVF International Conference\non Computer Vision, pages 618–626, 2017.\n[Serrano and Smith, 2019] Sofia Serrano and Noah A Smith.\nIs attention interpretable? In Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 2931–2951,\n2019.\n[Smilkov et al., 2017] Daniel Smilkov, Nikhil Thorat, Been\nKim, Fernanda Vi´egas, and Martin Wattenberg. Smooth-\ngrad: removing noise by adding noise. arXiv preprint\narXiv:1706.03825, 2017.\n[Sundararajan et al., 2017] Mukund Sundararajan, Ankur\nTaly, and Qiqi Yan. Axiomatic attribution for deep net-\nworks. In International Conference on Machine Learning,\npages 3319–3328. PMLR, 2017.\n[Touvron et al., 2021] Hugo Touvron, Matthieu Cord,\nMatthijs Douze, Francisco Massa, Alexandre Sablay-\nrolles, and Herv ´e J ´egou. Training data-efficient image\ntransformers & distillation through attention. In In-\nternational Conference on Machine Learning, pages\n10347–10357. PMLR, 2021.\n[Tuli et al., 2021] Shikhar Tuli, Ishita Dasgupta, Erin Grant,\nand Tom Griffiths. Are convolutional neural networks or\ntransformers more like human vision? In Proceedings of\nthe Annual Meeting of the Cognitive Science Society, vol-\nume 43, 2021.\n[V oitaet al., 2019] Elena V oita, David Talbot, Fedor Moi-\nseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head\nself-attention: Specialized heads do the heavy lifting, the\nrest can be pruned. In Annual Meeting of the Association\nfor Computational Linguistics, pages 5797–5808. Annual\nMeeting of the Association for Computational Linguistics\nAnthology, 2019.\n[Wang et al., 2020] Haofan Wang, Zifan Wang, Mengnan\nDu, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel,\nand Xia Hu. Score-cam: Score-weighted visual explana-\ntions for convolutional neural networks. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition workshop, pages 24–25, 2020.\n[Wang et al., 2021] Wenhai Wang, Enze Xie, Xiang Li,\nDeng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping\nLuo, and Ling Shao. Pyramid vision transformer: A ver-\nsatile backbone for dense prediction without convolutions.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 568–578, 2021.\n[White et al., 2021] Adam White, Kwun Ho Ngan, James\nPhelan, Saman Sadeghi Afgeh, Kevin Ryan, Con-\nstantino Carlos Reyes-Aldasoro, and Artur d’Avila\nGarcez. Contrastive counterfactual visual explanations\nwith overdetermination. arXiv preprint arXiv:2106.14556,\n2021.\n[Yuan et al., 2021] Tingyi Yuan, Xuhong Li, Haoyi Xiong,\nHui Cao, and Dejing Dou. Explaining information flow\ninside vision transformers using markov chain. IneXplain-\nable AI approaches for debugging and diagnosis., 2021.\n[Zeiler and Fergus, 2014] Matthew D Zeiler and Rob Fergus.\nVisualizing and understanding convolutional networks. In\nEuropean conference on computer vision, pages 818–833.\nSpringer, 2014.\n[Zhang et al., 2018] Jianming Zhang, Sarah Adel Bargal,\nZhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan\nSclaroff. Top-down neural attention by excitation back-\nprop. Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, 126(10):1084–1102, 2018.\n[Zhang et al., 2022] Zizhao Zhang, Han Zhang, Long Zhao,\nTing Chen, Sercan ¨O Arik, and Tomas Pfister. Nested\nhierarchical transformer: Towards accurate, data-efficient\nand interpretable visual understanding. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 36,\npages 3417–3425, 2022.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1577"
}