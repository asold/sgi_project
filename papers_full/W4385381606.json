{
  "title": "The shaky foundations of large language models and foundation models for electronic health records",
  "url": "https://openalex.org/W4385381606",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3030945216",
      "name": "Michael Wornow",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2119084440",
      "name": "YiZhe Xu",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2970988525",
      "name": "Rahul Thapa",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2260140186",
      "name": "Birju Patel",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2181729714",
      "name": "Ethan Steinberg",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2112035098",
      "name": "Scott Fleming",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2126561330",
      "name": "Michael A. Pfeffer",
      "affiliations": [
        "Stanford University",
        "Stanford Health Care"
      ]
    },
    {
      "id": "https://openalex.org/A4222399703",
      "name": "Jason Fries",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2150446796",
      "name": "Nigam H. Shah",
      "affiliations": [
        "Stanford Health Care",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A3030945216",
      "name": "Michael Wornow",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2119084440",
      "name": "YiZhe Xu",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2970988525",
      "name": "Rahul Thapa",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2260140186",
      "name": "Birju Patel",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2181729714",
      "name": "Ethan Steinberg",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2112035098",
      "name": "Scott Fleming",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2126561330",
      "name": "Michael A. Pfeffer",
      "affiliations": [
        "Stanford Health Care",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A4222399703",
      "name": "Jason Fries",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2150446796",
      "name": "Nigam H. Shah",
      "affiliations": [
        "Stanford Health Care",
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6800751262",
    "https://openalex.org/W2964006392",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4390874580",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W4303648971",
    "https://openalex.org/W4322761615",
    "https://openalex.org/W6838461927",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4313232683",
    "https://openalex.org/W4387356888",
    "https://openalex.org/W4321167341",
    "https://openalex.org/W3213708588",
    "https://openalex.org/W3174321177",
    "https://openalex.org/W4323050332",
    "https://openalex.org/W4318069287",
    "https://openalex.org/W4288474812",
    "https://openalex.org/W4312055891",
    "https://openalex.org/W2557007068",
    "https://openalex.org/W6849590751",
    "https://openalex.org/W4307539314",
    "https://openalex.org/W4308760226",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4385573087",
    "https://openalex.org/W4313197536",
    "https://openalex.org/W3169068430",
    "https://openalex.org/W3099750501",
    "https://openalex.org/W3112116031",
    "https://openalex.org/W4323359744",
    "https://openalex.org/W4281643269",
    "https://openalex.org/W3092301826",
    "https://openalex.org/W4205164650",
    "https://openalex.org/W2805089815",
    "https://openalex.org/W2953532875",
    "https://openalex.org/W3038004684",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W4296548563",
    "https://openalex.org/W4308885870",
    "https://openalex.org/W4221145109",
    "https://openalex.org/W4386071707",
    "https://openalex.org/W4312533035",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W3094595351",
    "https://openalex.org/W4295951577",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2972483465",
    "https://openalex.org/W4214584464",
    "https://openalex.org/W2891400669",
    "https://openalex.org/W4310645210",
    "https://openalex.org/W2945965960",
    "https://openalex.org/W3160137267",
    "https://openalex.org/W4220660160",
    "https://openalex.org/W4213013997",
    "https://openalex.org/W2141963295",
    "https://openalex.org/W1779612606",
    "https://openalex.org/W2784499877",
    "https://openalex.org/W3017527506",
    "https://openalex.org/W4283773284",
    "https://openalex.org/W4225335710",
    "https://openalex.org/W2156235098",
    "https://openalex.org/W3013605954",
    "https://openalex.org/W3035129496",
    "https://openalex.org/W3136047647",
    "https://openalex.org/W2967844572",
    "https://openalex.org/W4320487057",
    "https://openalex.org/W3201258060",
    "https://openalex.org/W2806532810",
    "https://openalex.org/W4281400706",
    "https://openalex.org/W4389519254",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W3193735819",
    "https://openalex.org/W4367678142",
    "https://openalex.org/W4296776307",
    "https://openalex.org/W4323345674",
    "https://openalex.org/W4388858772",
    "https://openalex.org/W4221159672",
    "https://openalex.org/W2944973900",
    "https://openalex.org/W3037480398",
    "https://openalex.org/W2010273961",
    "https://openalex.org/W4392359953",
    "https://openalex.org/W4384154918",
    "https://openalex.org/W3098949126",
    "https://openalex.org/W4319460874"
  ],
  "abstract": "Abstract The success of foundation models such as ChatGPT and AlphaFold has spurred significant interest in building similar models for electronic medical records (EMRs) to improve patient care and hospital operations. However, recent hype has obscured critical gaps in our understanding of these models’ capabilities. In this narrative review, we examine 84 foundation models trained on non-imaging EMR data (i.e., clinical text and/or structured data) and create a taxonomy delineating their architectures, training data, and potential use cases. We find that most models are trained on small, narrowly-scoped clinical datasets (e.g., MIMIC-III) or broad, public biomedical corpora (e.g., PubMed) and are evaluated on tasks that do not provide meaningful insights on their usefulness to health systems. Considering these findings, we propose an improved evaluation framework for measuring the benefits of clinical foundation models that is more closely grounded to metrics that matter in healthcare.",
  "full_text": "REVIEW ARTICLE OPEN\nThe shaky foundations of large language models and\nfoundation models for electronic health records\nMichael Wornow 1 ✉, Yizhe Xu2, Rahul Thapa2, Birju Patel 2, Ethan Steinberg 1, Scott Fleming 2, Michael A. Pfeffer2,3,\nJason Fries2 and Nigam H. Shah 2,3,4,5\nThe success of foundation models such as ChatGPT and AlphaFold has spurred signiﬁcant interest in building similar models for\nelectronic medical records (EMRs) to improve patient care and hospital operations. However, recent hype has obscured critical gaps\nin our understanding of these models’capabilities. In this narrative review, we examine 84 foundation models trained on non-\nimaging EMR data (i.e., clinical text and/or structured data) and create a taxonomy delineating their architectures, training data, and\npotential use cases. Weﬁnd that most models are trained on small, narrowly-scoped clinical datasets (e.g., MIMIC-III) or broad,\npublic biomedical corpora (e.g., PubMed) and are evaluated on tasks that do not provide meaningful insights on their usefulness to\nhealth systems. Considering theseﬁndings, we propose an improved evaluation framework for measuring the beneﬁts of clinical\nfoundation models that is more closely grounded to metrics that matter in healthcare.\nnpj Digital Medicine          (2023) 6:135 ; https://doi.org/10.1038/s41746-023-00879-8\nINTRODUCTION\nFoundation models (FMs) are machine learning models capable of\nperforming many different tasks after being trained on large,\ntypically unlabeled datasets\n1. FMs represent a paradigm shift in\nhow machine learning (ML) models are developed— rather than\ndeveloping a bespoke model for each speciﬁc use case (as was\ndone traditionally), a single FM can instead be reused across a\nbroad range of downstream tasks with minimal adaptation or\nretraining needed per task. FMs have received signi ﬁcant\nattention given their impressive range of capabilities across\nmultiple domains, from text generation\n2 and video editing3 to\nprotein folding4 and robotics5.\nOne of the most popular FMs has been OpenAI ’s ChatGPT,\nwhich surpassed 100 million users within two months of release6.\nChatGPT is a large language model (LLM), a type of FM which\ningests text and outputs text in response. Though ChatGPT was\ntrained to simply predict the next word in a sentence — it is\nbasically an advanced autocomplete — incredible capabilities\n“emerged” from this training setup which allow the model to\nperform a wide variety of complex tasks involving language\n7.\nPhysicians were quick to apply the model to pass medical\nlicensing exams\n8–11, simplify radiology reports 12, and write\nresearch articles13. In addition to text, FMs built on structured\nEMR data have shown the ability to predict the risk of 30-day\nreadmission14, select future treatments 15, and diagnose rare\ndiseases16.\nThe breakneck progress of AI over the past year has made it\ndifﬁcult for healthcare technology professionals and decision-\nmakers to accurately assess the strengths and limitations of these\ninnovations for clinical applications. Beyond short demos being\nshared on social media, there is little systematic examination of\nwhat the best use cases for production-grade clinical FMs are, or\nhow healthcare organizations should weigh their beneﬁts against\ntheir substantial risks\n1,17–19. Clinical FMs lack the shared evaluation\nframeworks and datasets20 that have underpinned progress in\nother ﬁelds, such as natural language processing (NLP) and\ncomputer vision21. This makes it difﬁcult to quantify and compare\nthese models’capabilities.\nIf we believe that FMs can help both providers and patients22,\nthen rigorous evaluations must be conducted to test these beliefs.\nIn this review, we uncover notable limitations in how clinical FMs\nare evaluated and a large disconnect between their evaluation\nregimes and assumed clinical value. While adopting FMs into\nhealthcare has immense potential\n23, until we know how to\nevaluate whether these models are useful, fair, and reliable, it is\ndifﬁcult to justify their use in clinical practice. Inspired by recent\nefforts to holistically evaluate LLMs trained on non-clinical text for\na range of capabilities beyond accuracy\n24, we believe that a similar\napproach is necessary to tie the evaluation of FMs writ large with\nuse cases that matter in healthcare.\nTo clarify these challenges, we reviewed over 80 different\nclinical FMs built from electronic medical record (EMR) data. We\nincluded all models trained on structured (e.g., billing codes,\ndemographics, lab values, and medications) and unstructured\n(e.g., progress notes, radiology reports, other clinical text) EMR\ndata, but explicitly excluded images, genetics, and wearables to\nmanage the scope of this review. We refer to the combination of\nstructured and unstructured EMR data (excluding images) as\nsimply “EMR data” or “clinical data”\n25. We refer to FMs built on\nthese forms of clinical data as “clinical foundation models” or\n“clinical FMs.” Our primary contributions are:\n1. To our knowledge, we present the largest review of clinical FMs\nfor structured and unstructured EMR data. We organize these\nmodels into a simple taxonomy to clearly delineate their\narchitectures, training data, capabilities, and public accessi-\nbility.\n2. We summarize the currently used evaluation frameworks for\nclinical FMs and identify their limitations. We explain why\ncurrent evaluation tasks provide little evidence for the\npurported beneﬁts of FMs to a health system.\n1Department of Computer Science, Stanford University, Stanford, CA, USA.2Center for Biomedical Informatics Research, Stanford University School of Medicine, Stanford, CA,\nUSA. 3Technology and Digital Services, Stanford Health Care, Palo Alto, CA, USA.4Department of Medicine, Stanford University School of Medicine, Stanford, CA, USA.5Clinical\nExcellence Research Center, Stanford University School of Medicine, Stanford, CA, USA.✉email: mwornow@stanford.edu\nwww.nature.com/npjdigitalmed\nPublished in partnership with Seoul National University Bundang Hospital\n1234567890():,;\n3. We propose an improved framework for evaluating clinical\nFMs. We advocate for metrics, tasks, and datasets that better\ncapture the presumed value of clinical FMs.\nWe begin with a brief overview of clinical FMs and deﬁne their\ninputs, outputs, and capabilities in “What are clinical FMs?”.I n\n“Beneﬁts of clinical FMs ”, we summarize the primary value\npropositions of FMs for health systems. In “State of published\nclinical FMs”, we provide an overview of the training data behind\nclinical FMs, examine current evaluation regimens and identify\ntheir limitations, and propose a framework for improving these\nevaluations. Finally, we discuss the promise of clinical FMs for\nsolving a diverse range of healthcare problems in Discussion.\nWHAT ARE CLINICAL FMS?\nA foundation model (FM) is a type of machine learning model that\nhas been pre-trained on large amounts of unlabeled data and can\nbe adapted to a broad range of downstream tasks\n1. FMs leverage\na training procedure referred to as“pre-training,” in which a“self-\nsupervised” (i.e., no labels are required) learning objective is used\nto scale learning to immense amounts (i.e., terabytes) of unlabeled\ndata. FMs also typically have signiﬁcantly more parameters than\ntraditional ML models— sometimes in the hundreds of billions of\nparameters— which requires signiﬁcant computational resources\nto train (i.e., months of time on a supercomputer with hundreds of\nGPUs)\n26. The signiﬁcantly larger size of FMs, coupled with their\ntask-agnostic self-supervised learning objective, has sparked a\nparadigm shift in how ML models are developed, and has resulted\nin the “emergence” of unprecedented capabilities at sufﬁcient\nmodel scale27.\nClinical FMs are foundation models built speci ﬁcally for\nelectronic medical record data. There are two broad categories\nof clinical FMs: Clinical language models (CLaMs) and Foundation\nmodels for EMRs (FEMRs).\nClinical language models (CLaMs)\nThe ﬁrst category of FMs are clinical language models, or CLaMs,\nwhich are a subtype of large language models (LLMs). As shown in\nFig. 1a, the unique attribute that separates CLaMs from general\nLLMs is their specialization on clinical/biomedical text— CLaMs are\nprimarily trained on, ingest, and output clinical/biomedical text.\nFor example, a CLaM could extract drug names from a doctor’s\nnote\n28, automatically reply to patient questions 29, summarize\nmedical dialogues 30, or predict mechanical ventilation needs\nbased on clinical notes31.\nWhile general-purpose LLMs (e.g., ChatGPT, Bloom, GPT-4, etc.)\ntrained on text scraped from the Internet can also be useful for\nclinical tasks, they tend to underperform CLaMs on domain-\nspeciﬁc tasks\n32,33, and thus we exclude them from this discussion.\nHowever, the conclusions from this review should also readily\napply to these general-purpose models, as they suffer from the\nsame limitations that we describe for CLaMs.\nFoundation models for electronic medical records (FEMRs)\nThe second class of clinical FMs are foundation models for\nelectronic medical records (FEMRs). These models are trained on\nthe entire timeline of events in a patient’s medical history. Given a\npatient’s EMR as input, a FEMR will output not clinical text but\nrather a machine-understandable“representation” for that patient,\nas shown in Fig. 1b. This representation — also referred to as a\n“patient embedding ”— is typically a ﬁxed-length, high-\ndimensional vector which condenses large amounts of patient\ninformation\n34. A patient’s representation can then be used as\ninput to any number of downstream models for different tasks.\nThese downstream models (built on the “foundation” of FEMR\nrepresentations) tend to be more accurate and robust than\ntraditional machine learning (ML) models on clinically relevant\ntasks, such as predicting 30-day readmission or long length-of-\nstay\n35.\nThe input to a FEMR can include many aspects of a patient’s\nmedical history, such as structured codes, lab values, claims, and\nclinical text. In practice, however, FEMRs are typically limited to\nthe single modality of structured codes, as discussed in“State of\npublished clinical FMs”.\nThough CLaMs and FEMRs have remained fairly separate over\nthe past several years, we note that the distinction between these\ntwo lines of work is becoming increasingly blurred as the next\ngeneration of foundation models for EMRs becomes more\nexpressive and multimodal in nature.\nBeneﬁts of clinical FMs\nGiven the excitement around FMs in healthcare\n23,36–41,w e\nsummarize their primary value propositions over traditional ML\nmethods. These advantages could all be highly valuable to a\nFig. 1 The two types of clinical FMs.Overview of the inputs and outputs of the two main types of clinical FMs.a The inputs and outputs of\nClinical Language Models (CLaMs). CLaMs ingest clinical text and output either clinical text or a machine-understandable representation of\nthe input text, which can then be used for downstream prediction tasks.b The inputs and outputs ofFoundation models for Electronic\nMedical Records (FEMRs). FEMRs ingest a patient’s medical history— which is simply a sequence of medical events with some temporal\nordering— and output a machine-understandable representation of the patient, which can then be used for downstream prediction tasks.\nM. Wornow et al.\n2\nnpj Digital Medicine (2023)   135 Published in partnership with Seoul National University Bundang Hospital\n1234567890():,;\nhealth system. Thus, it is essential that our evaluation tasks,\ndatasets, and metrics provide accurate assessments of these\npurported beneﬁts.\n1. Clinical FMs have better predictive performance. By\nusing larger training datasets and more model parameters,\nFMs can achieve better predictive performance (e.g., higher\nsensitivity and speci ﬁcity on classi ﬁcation tasks) than\ntraditional ML models\n34.\n2. Clinical FMs require less labeled data (“improved sample\nefﬁciency”). FMs enable superior model performance using\nfewer labeled data via “transfer learning”42. The core idea\nbehind transfer learning is to ﬁrst “pre-train” a model on\nlarge amounts of non-task-speciﬁc (and often unlabeled)\ndata to teach the model general patterns. Then, the model is\n“ﬁne-tuned” (i.e., continued to be trained) on a smaller\ndataset speci ﬁc to the desired task. For example, a\nsentiment classiﬁcation model pre-trained on the raw text\nof Wikipedia before beingﬁne-tuned on a labeled dataset of\n100 Tweets will outperform models solely trained on the\nsmaller task-speciﬁc dataset of Tweets\n42. Additionally, some\nFMs can be directly applied to novel tasks without any\nadditional ﬁne-tuning via“zero-shot” or “few-shot” learning.\nIn zero-shot learning, a model learns an entirely new task\nwithout being given any speciﬁc examples for that task— in\nother words, the model is given zero examples from which\nto learn and must instead rely on its general reasoning\ncapabilities to complete the desired task. Similarly, in few-\nshot learning, the model is only provided with a few\nexamples (typically less than 64) from which to learn. Zero/\nfew-shot learning are particularly powerful capabilities, as\nthey enable FMs to rapidly adapt to new tasks without the\nneed for large, task-speci ﬁc labeled datasets. Thus, by\nlearning representations that are useful for many down-\nstream tasks via self-supervised pre-training, FMs can greatly\nreduce the cost of developing ML models for a\nparticular task.\n3. Clinical FMs enable simpler and cheaper model deploy-\nment. After an FM is trained, it can help to decrease the\ntime, talent, and resources required to build subsequent ML\nmodels by serving as the ﬁgurative “foundation” upon\nwhich these subsequent applications are built\n1. Numerous\ncompanies have already commercialized this “ML-as-a-\nService” approach, in which a centralized FM is made\navailable to end-users via a simple API43. A similar approach\ncould work in healthcare, wherein a clinical FM allows\ninformaticians to integrate AI-related capabilities into\napplications while avoiding the expensive data ingestion,\npreprocessing, model training, and deployment steps in a\ntypical ML pipeline\n44.\n4. Clinical FMs exhibit“emergent” capabilities that enable\nnew clinical applications.The large number of parameters\nin FMs has resulted in a phenomenon known as “emer-\ngence,” in which previously intractable problems become\ntractable at sufﬁcient model scale7. For example, CLaMs can\nnow write coherent insurance appeals in ways thought\nimpossible only a couple of years ago45, while FEMRs can\ngenerate compact patient representations that enable time-\nto-event modeling of hundreds of outcomes simulta-\nneously46.\n5. Clinical FMs can more effectively handle multimodal\ndata. FMs can be designed to accept a wide range of data\nmodalities (e.g., structured codes, lab values, clinical text,\nimages, speech patterns, etc.) as inputs and incorporate\nthem into a single uniﬁed representation\n47. Substantial prior\nwork has shown that these models’large parameter counts\nand dataset sizes enable them to effectively model disparate\nmodalities in the same shared latent space, thereby deriving\nricher representations for each modality than possible with\nunimodal models48–51. This is especially useful in medicine,\ngiven the many types of data produced by patients52. For\nexample, a model might simultaneously consider an MRI\nscan, vital signs, and progress notes when predicting a\npatient’s optimal treatment\n53.\n6. Clinical FMs offer novel interfaces for human-AI interac-\ntion. Via a technique called“prompting”, a human can input\nnatural language into an LLM and have the model respond\nin natural language\n2. This enables a two-way conversation\nbetween humans and machine, and allows for the decom-\nposition of problems into smaller steps via techniques such\nas “chain-of-thought” prompting\n54. Prompting generalizes\nbeyond natural language. For example, a FEMR could be\nprompted with a desired clinical end state (e.g., normal A1C\nlevel) to identify which medications should be prescribed to\nachieve it\n55.\nSTATE OF PUBLISHED CLINICAL FMS\nWe identiﬁed 84 distinct clinical FMs published before March 1,\n2023. Speciﬁcally, we identi ﬁed 50 CLaMs and 34 FEMRs by\nfollowing citations from several representative samples of recent\nwork, as well as manual article curation. Given the rapid pace at\nwhich this ﬁeld advances, we do not claim to include every\npossible model or cover every recent advancement in the clinical\nFM space, but rather aim at capturing the general narrative\ndirection of the ﬁeld. We believe the papers that we selected\nshould adequately capture the general themes that would be\nidentiﬁed in other types of reviews, as they are representative of\nthe most recent work in theﬁeld, and therefore do not make any\nclaims about the systematicity of our search process. We focus\nexclusively on models that utilize structured and unstructured\nEMR data (excluding images) to scope this review.\nIn the following section, we review the training data and public\navailability of both CLaMs and FEMRs.\nCLaMs\nTraining data.C L a M s ( F i g .2a) are primarily trained on either\nclinical text (i.e., documents written during the course of care\ndelivery) or biomedical text (i.e., publications on biomedical\ntopics). Almost all CLaMs train ed on the clinical text used a\nsingle database: MIMIC-III, which contains approximately 2\nmillion notes written between 2001 –2012 in the ICU of the\nBeth Israel Deaconess Medical Center\n56. CLaMs trained on\nbiomedical text are virtually always trained on PubMed\nabstracts and/or full-text arti cles. While most CLaMs trained\non clinical text are also trained on biomedical text, the\nconverse is not true.\nModel availability. Almost all CLaMs have been made publicly\naccessible via online model repositories like HuggingFace\n57.\nUnfortunately, the exceptions are the very CLaMs that seem to\nhave the best performance 58 — ehrBERT59, UCSF-Bert 58, and\nGatorTron60— as they were trained on private EMR datasets.\nTakeaways. The high number of CLaMs published over the past\nseveral years may lead us to mistake motion for progress. Nearly\nall CLaMs have been trained on just two datasets -- MIMIC-III and\nPubMed, which respectively contain about 2 million clinical notes\nand 16 million abstracts with 5 million full-text publications.\nCombined, these two datasets contain about 18.5 billion words,\nwhich means models trained on them have substantial gaps in\ncompleteness (i.e., any scientiﬁc knowledge not contained within\nthese corpora) and timeliness (i.e., any new diseases, treatments,\nor practices discovered after 2012 in the case of MIMIC-III).\nEmpirically, we see that models trained on large-scale EHR data\nM. Wornow et al.\n3\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2023)   135 \noutperform CLaMs trained on shared public datasets across-the-\nboard on out-of-domain data distributions32,58.\nFEMRs\nTraining data. Most FEMRs (Fig. 3a) are trained on either small,\npublicly available EMR datasets or a single private health system’s\nEMR database. Again, the most popular public dataset is MIMIC-III,\nwhich contains less than 40,000 patients56. Other public datasets\nvary greatly in size, from eICU’s 139,000 patients61 to the CPRD’s\nlongitudinal records on 7% of all patients in the UK62. Several\nFEMRs have been trained on insurance claims, which are typically\nlarger in size and more diverse than EMR data but contain less\ngranular information\n63. Examples of claims datasets include\nTruven Health MarketScan (170 million patients)64 and Partners\nFor Kids (1.8 million pediatric patients) 65. In terms of data\nmodalities, most FEMRs are unimodal as they only consider\nstructured codes (e.g., LOINC, SNOMED, etc.).\nModel accessibility . FEMRs lack a common mechanism like\nHuggingFace for distributing models to the research community,\nas can be seen in the sparsity of the bottom-most row in Fig.3a\ncompared to the density of the bottom-most row in Fig.2a. Few\nFEMRs have had their model weights published, meaning\nresearchers must re-train these models from scratch on local\nEMR data to verify their performance.\nTakeaways. The overreliance on structured codes limits the\ngeneralizability of FEMRs across health systems that use different\nEMR systems and coding practices. Some models, such as\nDescEmb, address this problem by ﬁrst converting coded data\ninto their textual descriptions, thus detaching the model from the\nFig. 2 Overview of CLaMs.A summary of CLaMs and how they were trained, evaluated, and published. Each column is a speciﬁc CLaM,\ngrouped by the primary type of data they were trained on. Columnwise, the CLaMs primarily trained on clinical text are green (n = 23), those\ntrained primarily on biomedical text are blue (n = 24), and models trained on general academic text are purple (n = 3). The last column is the\ncount of entries in each row. AnX indicates that the model has that characteristic. An* indicates that a model partially has that characteristic.\na Training data and public availability of each model. The top rows mark whether a CLaM was trained on a speciﬁc dataset, while the bottom-\nmost row records whether a model’s code and weights have been published. Almost all CLaMs have had their model weights published,\ntypically via shared repositories like the HuggingFace Model Hub.b Evaluation tasks on which each model was evaluated in its original paper.\nGreen rows are tasks whose data were sourced from clinical text and blue rows are evaluation tasks sourced from biomedical text. The tasks\nare presented by the way they are commonly organized in the literature. CLaMs primarily trained on clinical text are evaluated on tasks drawn\nfrom clinical datasets, while CLaMs primarily trained on biomedical text are almost exclusively evaluated on tasks that contain general\nbiomedical text (i.e., not clinical text).c Clinical FM beneﬁts on which each model was evaluated in its original paper. The underlying tasks\npresented in this section are identical to those in (b), but here the tasks are reorganized into six buckets that reﬂect the six primary FM\nbeneﬁts described in Beneﬁts of clinical FMs. While almost all CLaMs have demonstrated the ability to improve predictive accuracy over\ntraditional ML approaches, there is scant evidence for the otherﬁve value propositions of clinical FMs.\nM. Wornow et al.\n4\nnpj Digital Medicine (2023)   135 Published in partnership with Seoul National University Bundang Hospital\nFig. 3 Overview of FEMRs.A summary of FEMRs and how they were trained, evaluated, and published. Each column is a speciﬁc FEMR,\ngrouped by the primary type of data they were trained on. Columnwise, the FEMRs primarily trained on structured EMR codes (e.g., billing,\nmedications, etc.) are red (n = 27), those trained on both structured codes and clinical text are orange (n = 3), and models trained only on\nclinical text are yellow (n = 4). The last column is the count of entries in each row. AnX indicates that the model has that characteristic. An*\nindicates that a model partially has that characteristic.a Training data and public availability of each model. The top rows mark whether a\nFEMR was trained on a speciﬁc dataset, while the bottom-most row records whether a model’s code and weights have been published. Very\nfew FEMRs have had their model weights published, as they are limited by data privacy concerns and a lack of interoperability between EMR\nschemas. b Evaluation tasks on which each model was evaluated in its original paper. From top to bottom, the evaluation tasks are binary\nclassiﬁcation, multi-class/label classiﬁcation, clustering of patients/diseases, and regression tasks like time-to-event. The tasks are presented by\nthe way they are commonly organized in the literature. FEMRs are evaluated on a very broad and sparse set of evaluation tasks— even the\nsame nominal task will often have different deﬁnitions across papers.c Clinical FM beneﬁts on which each model was evaluated in its original\npaper. The underlying tasks presented in this section are identical to those in (b), but here the tasks are reorganized into six buckets that\nreﬂect the six primary FM beneﬁts described in“Beneﬁts of clinical FMs”. While almost all FEMRs have demonstrated the ability to improve\npredictive accuracy over traditional ML approaches, and a signiﬁcant number have demonstrated improved sample efﬁciency, there is scant\nevidence for the other four value propositions of clinical FMs.\nM. Wornow et al.\n5\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2023)   135 \nspeciﬁc codes on which it was trained66. An additional limitation\nof relying on coded data is that it contains inconsistencies and\nerrors67, and often provides an incomplete picture of patient\nstate68. Some FEMRs have tackled this problem by combining\nunstructured EHR data (i.e., text) with structured EMR data to\nboost performance on speci ﬁc phenotyping and prediction\ntasks\n69,70. However, the key unsolved challenge of how to publicly\nshare pre-trained FEMRs continues to hinder theﬁeld’s progress\nand precludes the primary value proposition of FMs — namely,\nbeing able to build off a pre-trained model.\nNext, we considered the common evaluation frameworks for\nclinical FMs. The common thread between most of these\nevaluations is that they are relatively straightforward to conduct\nin an automated fashoin. While these tasks provide diagnostic\ninsights on model behavior, they provide limited insight into the\nclaims of FMs being a“categorically different” technology\n71,72, and\noffer little evidence for the clinical utility achieved by these\nmodels. Taking inspiration from the broader ML community’s push\ntowards Holistic Evaluation of Language Models24, we do a critical\nevaluation of the evaluations currently used to evaluate\nclinical FMs.\nCLaMs\nEvaluation of standard tasks and datasets . We collected every\nevaluation task that a CLaM was evaluated on in its original\npublication in Fig. 2b, and grouped these tasks as they are\ncommonly reported in the literature. Most CLaMs are being\nevaluated on traditional NLP-style tasks such as named entity\nrecognition, relation extraction, and document classiﬁcation on\neither MIMIC-III (clinical text) or PubMed (biomedical text)\n73,74.\nGiven that clinical text has its own unique structure, grammar,\nabbreviations, terminology, formatting, and other idiosyncrasies\nnot found in other domains\n75, it is alarming that roughly half of all\nCLaMs surveyed were not validated on clinical text, and thus may\nbe overestimating their expected performance in a healthcare\nsetting.\nWhen NLP tasks are sourced from clinical text, they can be\nuseful measures of a model ’s linguistic capabilities. However,\nthese NLP tasks are greatly limited by their overreliance on the\nsame handful of data sources\n74, small dataset sizes (typically\nthousands of examples)74,76, highly repetitive content77, and low\ncoverage of use cases20. As a result, strong performance on a\nclinical NLP task does not provide compelling evidence to a\nhospital looking to deploy a CLaM — claiming that “Model A\nachieves high precision on named entity recognition on 2,000\ndischarge notes from MIMIC-III ” is very different than “Model A\nshould be deployed across all of Health System X to identify patients\nat risk of suicide”.\nEvaluation on FM beneﬁts. To illustrate the disconnect between\ncurrent evaluation tasks and the loftier promises of clinical FMs,\nwe reorganized the rows of evaluation tasks from Fig. 2b—\noriginally presented as they are typically grouped in the\nliterature — along the six primary FM value propositions from\n“Beneﬁts of Clinical FMs”.T h er e s u l ti sF i g .2c, which identiﬁes\nwhich CLaMs were evaluated against any of the six core beneﬁts\nof clinical FMs. Most CLaMs have only shown evidence for one\nFM value proposition: improved predictive accuracy on certain\ntasks. However, there is little evidence supporting the other\npurported beneﬁts of FMs, such as simpliﬁed model deployment\nor reducing the need for labeled data. For example, while zero-\nand few-shot prompting tech niques have been rigorously\nstudied for general-purpose LLMs as an important method for\nachieving improved performance, few CLaMs have been\nevaluated across different prompting strategies and ﬁne-\ntuning techniques. In other words, there is a gap in our\nunderstanding of what CLaMscan do versus what CLaMs can do\nthat is valuable to a health system and which traditional ML\nmodels cannot do.\nFEMRs\nEvaluation on standard tasks and datasets . We collected the\noriginal tasks on which each FEMR was evaluated in Fig.3b and\nbucketed them as they are typically presented in the literature.\nEvaluation of FEMRs is in an even poorer state than that of CLaMs.\nWhile CLaMs bene ﬁt from the NLP community ’s adoption of\nstandardized task formats, FEMRs lack a similar set of“canonical”\nevaluations. Instead, FEMRs are evaluated on an extremely sparse\nset of tasks with little-to-no overlap across publications. This\nmakes it highly non-trivial to compare the performance of\ndifferent FEMRs.\nThese tasks are typically grouped by how each task is\nformulated, e.g., binary classiﬁcation v. multi-label classiﬁcation\nv. regression. The most popular prediction tasks are binary\nclassiﬁcation tasks such as mortality, heart failure, and long\nlength-of-stay, but even the same nominal task can have widely\ndivergent deﬁnitions across papers\n78.\nEvaluation on FM beneﬁts. We reorganized the rows of evaluation\ntasks from Fig. 3b along the six primary value propositions of\nclinical FMs listed in“Beneﬁts of Clinical FMs”. The result is Fig.3c,\nwhich shows that almost all evaluations of FEMRs have been\nfocused on demonstrating their superior predictive accuracy over\ntraditional ML models. Notably, the ability to use less labeled data\n(i.e., sample ef ﬁciency) has been fairly well-documented with\nFEMRs. However, the other four potential beneﬁts of FMs have\ngone largely unstudied. And while evaluations of predictive\naccuracy are straightforward to perform, it is not the sole property\nof FMs that would justify their adoption by a health system.\nFinally, to better quantify the ability of clinical FMs to achieve\nthe six key beneﬁts of FMs outlined in“Beneﬁts of clinical FMs”,w e\npropose several improved evaluation metrics and tasks in Fig.4.\nOur suggestions are by no means comprehensive, but rather\nmeant to spark a further discussion on how to align model\nevaluation with the demonstration of clinical value.\n1. Better predictive performance: The most thoroughly\nstudied property of clinical FMs has been their improved\npredictive performance on classi ﬁcation and regression\ntasks based on AUROC, AUPRC, F1 Score, and Accuracy.\nThese metrics assume an in ﬁnite capacity to act on a\nmodel’s predictions. In reality, clinical workﬂows are capacity\nconstrained— a nursing team may only be able to act on a\nhandful of model predictions per day\n79,80. Thus, a health\nsystem should only care about a model ’s accuracy on\npatients for which it has the capacity to intervene. We,\ntherefore, recommend that researchers adopt ranking-based\nmetrics (e.g., Top-K precision/recall/F1, reciprocal ranking,\netc.), which are commonly used for recommendation\nsystems\n81. Additionally, we propose examining not just a\nmodel’s ability to classify patients correctly but also its\ncalibration across subgroups, fairness, and alignment with\nclinical best practices24,29. The human evaluation may also\nbe necessary in some cases, such as evaluating a CLaM’s\nability to accurately generate answers to clinical questions82.\nTraditional NLP metrics such as ROUGE, METEOR, and BLEU\n— which simply count n-gram overlap between generated\nand reference text— are known to poorly correlate with\nhuman evaluations of natural language generations83–85.W e\nalso lack automated metrics for evaluating the more\nqualitative aspects of a model’s “alignment” with human\nvalues (e.g., helpfulness or harmlessness) 86, even as the\nimportance of human feedback during training has been\nrepeatedly demonstrated via techniques like Reinforcement\nM. Wornow et al.\n6\nnpj Digital Medicine (2023)   135 Published in partnership with Seoul National University Bundang Hospital\nLearning from Human Feedback87. This is especially worry-\ning in medical settings, where patient safety is paramount.\nCLaMs that might impact clinical decisions should be\nevaluated much more rigorously than automated metrics\ncan provide, across axes such as agreement with scientiﬁc\nconsensus, minimization of the extent and risk of harm,\npossibility of bias, and the clinical utility of the advice\n29,82.\n2. Less labeled data: The simplest way for researchers to\ndemonstrate how clinical FMs exhibit improved sample\nefﬁciency is to replace evaluation metric“X” with the more\nnuanced metric “X using K training examples”. For example,\nreplacing “AUROC” with “AUROC using 1000 labeled radiology\nreports for ﬁne-tuning.” Ideally, a clinical FM would enable\nsimilar model performance at low values of K as at high\nvalues of K. Another way to demonstrate improved sample\nefﬁciency is to measure zero-shot and few-shot model\nperformance, in which a model is given either zero or\n<100 examples, respectively, for the task on which it is\nevaluated. Researchers should also consider measuring the\nperformance difference betweenﬁne-tuning versus prompt-\ning, where the former has been known to achieve higher\naccuracy, but the latter represents a much simpler and more\nﬂexible deployment option (as the model weights remain\nfrozen)\n88. One could also measure the total dataset\nannotation time saved by using a clinical FM, measured in\nterms of dollars or hours.\n3. Simpliﬁed model deployment: To quantify the value of\nFMs in lowering the barrier for building task-speci ﬁc\nmodels1,89, one possible metric is the cost of hardware/\ncompute/memory needed to train a model or generate a\nprediction. More broadly, we can measure the overall cost\nsavings of using a clinical FM in terms of full-time\nequivalents (FTEs) or resource hours saved when down-\nstream models (e.g., risk of inpatient mortality) are built on\ntop of a clinical FM versus training a task-speciﬁc model\nfrom scratch. We recognize, however, that this evaluation\nmay be the most challenging to conduct, as it requires buy-\nin from the business, clinical, and IT units of a health system.\nHealth systems with dedicated ML Operations ( “MLOps”)\nteams may be better positioned to realize these beneﬁts\n90.\n4. Emergent clinical applications: Clinical FMs can perform\nentirely novel tasks thought to be beyond the reach of\nmachines even just a year ago, e.g., summarizing MRI\nreports in patient-accessible terms, writing discharge\ninstructions, or generating differential diagnoses\n45,91. “Emer-\ngence” is a term of art used by ML researchers to describe\nthe phenomenon by which FMs trained on large datasets\nare able to perform tasks that were impossible for smaller\nML models to accomplish\n7. While this greatly broadens the\nrange of clinical problems addressable via machine learning,\nit is still unproven whether these capabilities provide\ntangible utility to health systems in production settings\n92.\nThus, we must explicitly deﬁne the scenarios in which the\nemergent capabilities of clinical FMs achieve their purported\nbeneﬁts. For example, LLMs such as GPT-4 can produce new\nUSMLE exam questions, which are indistinguishable from\nhuman-authored questions. However, whether the use of\nthese questions results in better-prepared medical students,\nor a lower burden for creating exam questions, remains to\nbe quantiﬁed\n93.\n5. Multimodality: Currently, the majority of evaluation tasks\nspan one data modality 78, even though models that\nsimultaneously use multiple data modalities show substan-\ntial gains94. There is a strong unmet need for evaluation\nscenarios which explicitly require multimodal representa-\ntions. Many datasets already include multimodal data (e.g.,\nMIMIC-III, eICU, private EMRs, etc.), but evaluation tasks are\nnot constructed in ways that require the demonstration of\nFig. 4 Better evaluations of clinical FMs.Proposals for how to demonstrate the value of CLaMs and FEMRs for achieving the six primary value\npropositions of FMs to health systems over traditional ML models.\nM. Wornow et al.\n7\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2023)   135 \nmultimodal reasoning across both structured data and\nunstructured text. A great example of datasets that\naccomplish this are the Holistic AI Framework (HAIM), which\nbuilds on top of MIMIC-III to enable truly multimodal\nevaluation scenarios\n95.\n6. Novel human-AI interfaces:Human evaluation and usabil-\nity studies are needed to quantify the utility of interacting\nwith FMs via prompts1. Metrics include user satisfaction,\nengagement, system usability scale scores, qualitative\ninterview feedback, and the time/effort required to achieve\nstated goals\n96–98. Measuring the skill level necessary to\noperate a model can also shed light on its ability to\nempower providers to perform a multitude of roles. For\nFEMRs, an accepted paradigm for“prompting” does not yet\nexist, so developing a framework for prompting a patient’s\nmedical history would represent a signiﬁcant step forward.\nOne exception is the Clinical Decision Transformer, which\nused a desired clinical end state (e.g., normal A1C levels) as a\nprompt to generate medication recommendations\n55.\nDISCUSSION\nOur review of 50 CLaMs and 34 FEMRs shows that most clinical\nFMs are being evaluated on tasks that provide little information on\nthe potential advantages of FMs over traditional ML models. While\nthere is ample evidence that clinical FMs enable more accurate\nmodel predictions, Figs. 2, 3 show that minimal work has been\nconducted to validate whether the other, potentially more\nvaluable beneﬁts of FMs will be realized in healthcare. These\nbeneﬁts include reducing the burden of labeling data, offering\nnovel human-AI interfaces, and enabling new clinical applications\nbeyond the reach of traditional ML models, among others\noutlined in “Beneﬁts of Clinical FMs”. To help bridge this divide,\nwe advocate for the development of new evaluation tasks,\nmetrics, and datasets more directly tied to clinical utility, as\nsummarized in Fig.4.\nWhile we focused this review on FMs developed speciﬁcally for\nclinical data, we recognize that there has been signiﬁcant recent\nprogress in adapting general-purpose LLMs to medical knowledge\ntasks\n11. As these general-purpose models continue to improve,\nthe need and value of having clinical-speciﬁc models remain an\nopen question 32. However, it is worth emphasizing that the\nevaluation of these general-purpose LLMs suffers from the same\nexact limitations as evaluations of clinical LLMs, and the critiques\ndescribed in this review still apply. While general-purpose LLMs\ncontinue to improve on speci ﬁc clinical tasks, e.g., clinical\nknowledge and board certiﬁcation benchmarks, it remains unclear\nhow well they perform for broader applications in the hospital and\nwhat is achievable without training on some degree of in-domain\ndata (e.g., EHRs). For example, the fact that GPT-4 passes the\nUSMLE does not necessarily mean the model is useful for the\ntypes of questions clinicians care about in practice\n82. We believe\nmore work needs to be done to assess the clinical reasoning\ncapabilities of these general-purpose systems, and to develop a\nbetter theoretical understanding of how a model’s skills in other\ndomains strengthen or worsen its performance on clinical tasks.\nThere are also considerations beyond overall accuracy, such as\nscalability and inference cost, that may have different trade-offs in\nsmaller, more targeted clinical-speciﬁc FM deployments\n99.\nIn addition to the potential beneﬁts listed in“What are Clinical\nFMs?”, FMs present numerous risks that must also be considered\nand investigated. Data privacy and security are signi ﬁcant\nconcerns with FMs, as they may leak protected health information\nthrough model weights or prompt injection attacks100,101. FMs are\nalso more difﬁcult to interpret, edit, and control due to their\nimmense size102. They require high up-front costs to create, and\nwhile these costs can be amortized over multiple downstream\napplications, their value may take longer to realize than a smaller\nmodel developed for a single high-value task103. Additionally, FMs\nmay fall under Software-as-a-Medical-Device guidelines regulating\ntheir usage in the clinic104. And similar to traditional ML models,\nFMs are susceptible to biases induced by miscalibration or\noverﬁtting\n105, as well as inducing “automation bias” in which\nclinicians defer to a model’s outputs even when they are obviously\nincorrect106. Developing frameworks for determining a model’s\noverall worth remains indispensable79.\nDespite these challenges, clinical FMs hold immense promise\nfor solving a diverse range of healthcare problems. We invite the\nresearch community to develop better evaluations to help realize\ntheir potential for beneﬁting both patients and providers\n22.\nDATA AVAILABILITY\nWe do not have any data beyond what is depicted in the Figures of this paper.\nReceived: 21 March 2023; Accepted: 13 July 2023;\nREFERENCES\n1. Bommasani, R. et al. On the opportunities and risks of foundation models.\nPreprint at arXiv: 2108.07258 (2021).\n2. Brown, T. B. et al. Language models are few-shot learners. Preprint at\narXiv:2005.14165 (2020).\n3. Esser, P., Chiu, J., Atighehchian, P., Granskog, J. & Germanidis, A. Structure and\ncontent-guided video synthesis with diffusion models. Preprint at arXiv:\n2302.03011 (2023).\n4. Jumper, J. et al. Highly accurate protein structure prediction with AlphaFold.\nNature 596, 583–589 (2021).\n5. Jiang, Y. et al. VIMA: general robot manipulation with multimodal prompts.\nPreprint at arXiv: 2210.03094 (2022).\n6. Eysenbach, G. The role of ChatGPT, generative language models, and artiﬁcial\nintelligence in medical education: a conversation with ChatGPT and a call for\npapers. JMIR Med Educ.9, e46885 (2023).\n7. Wei, J. et al. Emergent abilities of large language models. Preprint at arXiv:\n2206.07682 (2022).\n8. Kung, T. H. et al. Performance of ChatGPT on USMLE: Potential for AI-assisted\nmedical education using large language models.PLoS Digit. Health2, e0000198\n(2023).\n9. Gilson, A. et al. How does ChatGPT perform on the United States medical\nlicensing examination? The implications of large language models for medical\neducation and knowledge assessment.JMIR Med. Educ. (2023)\n10. Liévin, V., Hother, C. E. & Winther, O. Can large language models reason about\nmedical questions? Preprint at arXiv: :2207.08143 (2022).\n11. Nori, H., King, N., Mc Kinney, S. M., Carignan, D. & Horvitz, E. Capabilities of GPT-4\non medical challenge problems. Preprint at arXiv: 2303.13375 (2023).\n12. Jeblick, K. et al. ChatGPT makes medicine easy to swallow: an exploratory case\nstudy on simpliﬁed radiology reports. Preprint at arXiv: 2212.14882 (2022).\n13. Macdonald, C., Adeloye, D., Sheikh, A. & Rudan, I. Can ChatGPT draft a research\narticle? An example of population-level vaccine effectiveness analysis.J. Glob.\nHealth 13, 01003 (2023).\n14. Pang, C. et al. CEHR-BERT: Incorporating temporal information from structured EHR\ndata to improve prediction tasks. Machine Learning for Health.PMLR (2021)\n15. Choi, E., Bahadori, M. T., Schuetz, A., Stewart, W. F. & Sun, J. Doctor AI: predicting\nclinical events via recurrent neural networks. Preprint at arXiv: 1511.05942\n(2015).\n16. Prakash, P. K. S., Chilukuri, S., Ranade, N. & Viswanathan, S. RareBERT: transformer\narchitecture for rare disease patient identiﬁcation using administrative claims.\nAAAI 35, 453–460 (2021).\n17. Cascella, M., Montomoli, J., Bellini, V. & Bignami, E. Evaluating the feasibility of\nChatGPT in healthcare: an analysis of multiple clinical and research scenarios.J.\nMed. Syst. 47, 33 (2023).\n18. Shen, Y. et al. ChatGPT and other large language models are double-edged\nswords. Radiology 307, 230163 (2023).\n19. Wójcik, M. A. Foundation models in healthcare: opportunities, biases and reg-\nulatory prospects in Europe. InElectronic Government and the Information Sys-\ntems Perspective: 11th International Conference, EGOVIS 2022 Proceedings32–46\n(Springer-Verlag, 2022).\nM. Wornow et al.\n8\nnpj Digital Medicine (2023)   135 Published in partnership with Seoul National University Bundang Hospital\n20. Blagec, K., Kraiger, J., Frühwirt, W. & Samwald, M. Benchmark datasets driving\nartiﬁcial intelligence development fail to capture the needs of medical profes-\nsionals. J. Biomed. Inform.137, 104274 (2023).\n21. Donoho, D. 50 years of data science.J. Comput. Graph. Stat.26, 745–766 (2017).\n22. Topol, E. When M.D. is a machine doctor. https://erictopol.substack.com/p/\nwhen-md-is-a-machine-doctor (2023).\n23. Robert, P. 5 Ways ChatGPT will change healthcare forever, for better.Forbes\nMagazine (13 February 2023).\n24. Liang, P. et al. Holistic evaluation of language models. Preprint at arXiv [cs.CL]\n(2022).\n25. Mohsen, F., Ali, H., El Hajj, N. & Shah, Z. Artiﬁcial intelligence-based methods for\nfusion of electronic health records and imaging data.Sci. Rep.12, 17981 (2022).\n26. BigScience Workshop, et al. BLOOM: a 176B-Parameter open-access multilingual\nlanguage model. Preprint at arXiv [cs.CL] (2022).\n27. Bubeck, S. et al. Sparks of artiﬁcial general intelligence: early experiments with\nGPT-4. Preprint at arXiv [cs.CL] (2023).\n28. Agrawal, M., Hegselmann, S., Lang, H., Kim, Y. & Sontag, D. Large language\nmodels are few-shot clinical information extractors. Preprint at arXiv [cs.CL]\n(2022).\n29. Singhal, K. et al. Large language models encode clinical knowledge. Preprint at\narXiv [cs.CL] (2022).\n30. Chintagunta, B., Katariya, N., Amatriain, X. & Kannan, A. Medically aware GPT-3 as\na data generator for medical dialogue summarization. InProc. Second Workshop\non Natural Language Processing for Medical Conversations66–76 (Association for\nComputational Linguistics, 2021).\n31. Huang, K. et al. Clinical XLNet: Modeling Sequential Clinical Notes and Predicting\nProlonged Mechanical Ventilation. Proceedings of the 3rd Clinical Natural Lan-\nguage Processing Workshop(2020).\n32. Lehman, E. et al. Do we still need clinical language models? Preprint at arXiv\n[cs.CL] (2023).\n33. Moradi, M., Blagec, K., Haberl, F. & Samwald, M. GPT-3 models are poor few-shot\nlearners in the biomedical domain. Preprint at arXiv [cs.CL] (2021).\n34. Steinberg, E. et al. Language models are an effective representation learning\ntechnique for electronic health record data. J. Biomed. Inform. 113, 103637\n(2021).\n35. Guo, L. L. et al. EHR foundation models improve robustness in the presence of\ntemporal distribution shift.Sci. Rep. 13, 3767 (2022).\n36. Fei, N. et al. Towards artiﬁcial general intelligence via a multimodal foundation\nmodel. Nat. Commun. 13, 3094 (2022).\n37. Si, Y. et al. Deep representation learning of patient data from Electronic Health\nRecords (EHR): a systematic review.J. Biomed. Inform.115, 103671 (2021).\n38. Rajpurkar, P., Chen, E., Banerjee, O. & Topol, E. J. AI in health and medicine.Nat.\nMed. 28,3 1–38 (2022).\n39. Xiao, C., Choi, E. & Sun, J. Opportunities and challenges in developing deep\nlearning models using electronic health records data: a systematic review.J. Am.\nMed. Inform. Assoc.25, 1419–1428 (2018).\n40. Davenport, T. & Kalakota, R. The potential for artiﬁcial intelligence in healthcare.\nFuture Health. J.\n6,9 4–98 (2019).\n41. Bohr, A. & Memarzadeh, K. The rise of arti ﬁcial intelligence in healthcare\napplications. Artif. Intell. Healthcare25 (2020).\n42. Howard, J. & Sebastian, R. Universal Language Model Fine-tuning for Text\nClassiﬁcation. Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (2018).\n43. Chen, L. et al. HAPI: a large-scale longitudinal dataset of commercial ML API\npredictions. Preprint at arXiv [cs.SE] (2022).\n44. Huge ‘foundation models’ are turbo-charging AI progress. The Economist (15\nJune 2022).\n45. Canes, D. The time-saving magic of Chat GPT for doctors. https://\ntillthecavalryarrive.substack.com/p/the-time-saving-magic-of-chat-gpt?\nutm_campaign=auto_share (2022).\n46. Steinberg, E., Xu, Y., Fries, J. & Shah, N. Self-supervised time-to-event modeling\nwith structured medical records. Preprint at arXiv [cs.LG] (2023).\n47. Kline, A. et al. Multimodal machine learning in precision health: a scoping\nreview. NPJ Digit. Med.5, 171 (2022).\n48. Baevski, A. et al. Data2vec: A general framework for self-supervised learning in\nspeech, vision and language. International Conference on Machine Learning.\nPMLR (2022).\n49. Girdhar, R. et al. ImageBind: one embedding space to bind them all. Preprint at\narXiv [cs.CV] (2023).\n50. Boecking, B. et al. Making the most of text semantics to improve biomedical\nvision--language processing. Preprint at arXiv [cs.CV] (2022).\n51. Radford, A. et al. Learning transferable visual models from natural language\nsupervision. Preprint at arXiv [cs.CV] (2021).\n52. Huang, S.-C., Pareek, A., Seyyedi, S., Banerjee, I. & Lungren, M. P. Fusion of\nmedical imaging and electronic health records using deep learning: a sys-\ntematic review and implementation guidelines.NPJ Digit. Med. 3, 136 (2020).\n53. Acosta, J. N., Falcone, G. J., Rajpurkar, P. & Topol, E. J. Multimodal biomedical AI.\nNat. Med. 28, 1773–1784 (2022).\n54. Wei, J. et al. Chain-of-thought prompting elicits reasoning in large language\nmodels. Advances in Neural Information Processing Systems(2022).\n55. Lee, S., Da Young, L., Im, S., Kim, N. H. & Park, S.-M. Clinical decision transformer:\nintended treatment recommendation through goal prompting. Preprint at arXiv\n[cs.AI] (2023).\n56. Johnson, A. E. W. et al. MIMIC-III, a freely accessible critical care database.Sci.\nData 3, 160035 (2016).\n57. Wolf, T. et al. Transformers: State-of-the-Art Natural Language Processing.\nEMNLP 2020 (2020).\n58. Sushil, M., Ludwig, D., Butte, A. J. & Rudrapatna, V. A. Developing a general-\npurpose clinical language inference model from a large corpus of clinical notes.\nPreprint at arXiv [cs.CL] (2022).\n59. Li, F. et al. Fine-tuning bidirectional encoder representations from transformers\n(BERT)-based models on large-scale electronic health record notes: an empirical\nstudy. JMIR Med. Inf.7, e14830 (2019).\n60. Yang, X. et al. GatorTron: a large clinical language model to unlock patient\ninformation from unstructured electronic health records. Preprint at bioRxiv\nhttps://doi.org/10.1101/2022.02.27.22271257 (2022).\n61. Pollard, T. J. et al. The eICU Collaborative Research Database, a freely available\nmulti-center database for critical care research.Sci. Data 5, 180178 (2018).\n62. Li, Y. et al. Hi-BEHRT: hierarchical transformer-based model for accurate pre-\ndiction of clinical events using multimodal longitudinal electronic health\nrecords. IEEE J. Biomed. Health Inform. 27\n(2022).\n63. Zeltzer, D. et al. Prediction accuracy with electronic medical records versus\nadministrative claims. Med. Care 57, 551–559 (2019).\n64. Rasmy, L., Xiang, Y., Xie, Z., Tao, C. & Zhi, D. Med-BERT: pretrained contextualized\nembeddings on large-scale structured electronic health records for disease\nprediction. npj Digit. Med.4, 86 (2021).\n65. Zeng, X., Linwood, S. L. & Liu, C. Pretrained transformer framework on pediatric\nclaims data for population speciﬁc tasks. Sci. Rep. 12, 3651 (2022).\n66. Hur, K. et al. Unifying heterogeneous electronic health records systems via text-\nbased code embedding. Conference on Health, Inference, and Learning, PMLR\n(2022).\n67. Tang, P. C., Ralston, M., Arrigotti, M. F., Qureshi, L. & Graham, J. Comparison of\nmethodologies for calculating quality measures based on administrative data\nversus clinical data from an electronic health record system: implications for\nperformance measures. J. Am. Med. Inform. Assoc.14,1 0–15 (2007).\n68. Wei, W.-Q. et al. Combining billing codes, clinical notes, and medications from\nelectronic health records provides superior phenotyping performance.J. Am.\nMed. Inform. Assoc.23, e20–e27 (2016).\n69. Rajkomar, A. et al. Scalable and accurate deep learning with electronic health\nrecords. npj Digit. Med.1,1 –10 (2018).\n70. Lee, D., Jiang, X. & Yu, H. Harmonized representation learning on dynamic EHR\ngraphs. J. Biomed. Inform.106, 103426 (2020).\n71. Ateev, H. R. B. A. ChatGPT-assisted diagnosis: is the future suddenly here?\nhttps://www.statnews.com/2023/02/13/chatgpt-assisted-diagnosis/ (2023).\n72. Raths, D. How UCSF physician execs are thinking about ChatGPT.Healthcare\nInnovation (17 February 2023).\n73. Fries, J. et al. Bigbio: a framework for data-centric biomedical natural language\nprocessing. Advances in Neural Information Processing Systems35 (2022).\n74. Gao, Y. et al. A scoping review of publicly available language tasks in clinical\nnatural language processing.J. Am. Med. Inform. Assoc.29, 1797–1806 (2022).\n75. Leaman, R., Khare, R. & Lu, Z. Challenges in clinical natural language processing\nfor automated disorder normalization.J. Biomed. Inform.57,2 8–37 (2015).\n76. Spasic, I. & Nenadic, G. Clinical text data in machine learning: systematic review.\nJMIR Med. Inf.8, e17984 (2020).\n77. Yue, X., Jimenez Gutierrez, B. & Sun, H. Clinical reading comprehension: a\nthorough analysis of the emrQA dataset. InProc. 58th Annual Meeting of the\nAssociation for Computational Linguistics4474–4486 (Association for Computa-\ntional Linguistics, 2020).\n78. McDermott, M. et al. A comprehensive EHR timeseries pre-training benchmark.\nIn Proc. Conference on Health, Inference, and Learning257–278 (Association for\nComputing Machinery, 2021).\n79. Shah, N. Making machine learning models clinically useful. JAMA 322, 1351\n(2019).\n80. Wornow, M., Gyang Ross, E., Callahan, A. & Shah, N. H. APLUS: a Python library\nfor usefulness simulations of machine learning models in healthcare.J. Biomed.\nInform. 139, 104319 (2023).\nM. Wornow et al.\n9\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2023)   135 \n81. Tamm, Y.-M., Damdinov, R. & Vasilev, A. Quality metrics in recommender sys-\ntems: Do we calculate metrics consistently?Proceedings of the 15th ACM Con-\nference on Recommender Systems(2021).\n82. Dash, D. et al. Evaluation of GPT-3.5 and GPT-4 for supporting real-world\ninformation needs in healthcare delivery. Preprint at arXiv [cs.AI] (2023).\n83. Reiter, E. A structured review of the validity of BLEU. Comput. Linguist. 44,\n393–401 (2018).\n84. Hu, X. et al. Correlating automated and human evaluation of code documentation\ngeneration quality.ACM Trans. Softw. Eng. Methodol.31,1 –28 (2022).\n85. Liu, Y. et al. G-Eval: NLG evaluation using GPT-4 with better human alignment.\nPreprint at arXiv [cs.CL] (2023).\n86. Thomas, R. & Uminsky, D. The problem with metrics is a fundamental problem\nfor AI. Preprint at arXiv [cs.CY] (2020).\n87. Bai, Y. et al. Training a helpful and harmless assistant with reinforcement\nlearning from human feedback. Preprint at arXiv [cs.CL] (2022).\n88. Gao, T., Fisch, A. & Chen, D. Making pre-trained language models better few-\nshot learners. Preprint at arXiv [cs.CL] (2020).\n89. Kaufmann, J. Foundation models are the new public cloud. ScaleVP https://\nwww.scalevp.com/blog/foundation-models-are-the-new-public-cloud (2022).\n90. Kashyap, S., Morse, K. E., Patel, B. & Shah, N. H. A survey of extant organizational\nand computational setups for deploying predictive models in health systems.J.\nAm. Med. Inform. Assoc.28, 2445–2450 (2021).\n91. Abdullah, I. S., Loganathan, A., Lee, R. W. ChatGPT & doctors: the Medical Dream\nTeam. URGENT Matters (2023).\n92. Lee, P., Goldberg, C. & Kohane, I.The AI Revolution in Medicine: GPT-4 and Beyond.\n(Pearson, 2023).\n93. Fleming, S. L. et al. Assessing the potential of USMLE-like exam questions\ngenerated by GPT-4. Preprint at medRxiv https://doi.org/10.1101/\n2023.04.25.23288588 (2023).\n94. Husmann, S., Yèche, H., Rätsch, G. & Kuznetsova, R. On the importance of clinical\nnotes in multi-modal learning for EHR data. Preprint at arXiv [cs.LG] (2022).\n95. Soenksen, L. R. et al. Integrated multimodal artiﬁcial intelligence framework for\nhealthcare applications. NPJ Digit. Med.5, 149 (2022).\n96. Peng, S., Kalliamvakou, E., Cihon, P. & Demirer, M. The impact of AI on developer\nproductivity: evidence from GitHub copilot. Preprint at arXiv [cs.SE] (2023).\n97. Noy, S. et al. Experimental evidence on the productivity effects of generative\nartiﬁcial intelligence.Science https://economics.mit.edu/sites/default/ﬁles/inline-\nﬁles/Noy_Zhang_1.pdf (2023).\n98. Perry, N., Srivastava, M., Kumar, D. & Boneh, D. Do users write more insecure\ncode with AI assistants? Preprint at arXiv [cs.CR] (2022).\n99. Zhang, X., Zhou, Z., Chen, D. & Wang, Y. E. AutoDistill: an end-to-end framework\nto explore and distill hardware-efﬁcient language models. Preprint at arXiv\n[cs.LG] (2022).\n100. El-Mhamdi, E.-M. et al. SoK: on the impossible security of very large foundation\nmodels. Preprint at arXiv [cs.LG] (2022).\n101. Carlini, N. et al. Quantifying memorization across neural language models.\nPreprint at arXiv [cs.LG] (2022).\n102. Mitchell, E., Lin, C., Bosselut, A., Manning, C. D. & Finn, C. Memory-based model\nediting at scale. Preprint at arXiv [cs.AI] (2022).\n103. Sharir, O., Peleg, B. & Shoham, Y. The cost of training NLP models: a concise\noverview. Preprint at arXiv [cs.CL] (2020).\n104. Yaeger, K. A., Martini, M., Yaniv, G., Oermann, E. K. & Costa, A. B. United States\nregulatory approval of medical devices and software applications enhanced by\nartiﬁcial intelligence. Health Policy Technol.8, 192–197 (2019).\n105. DeCamp, M. & Lindvall, C. Latent bias and the implementation of\nartiﬁcial intelligence in medicine. J. Am. Med. Inform. Assoc. 27, 2020–2023\n(2020).\n106. Wickens, C. D., Clegg, B. A., Vieane, A. Z. & Sebok, A. L. Complacency and\nautomation bias in the use of imperfect automation.Hum. Factors 57, 728–739\n(2015).\nACKNOWLEDGEMENTS\nM.W. is supported by an NSF Graduate Research Fellowship. M.W., Y.X., R.T., J.F., E.S.,\nS.F., M.A.P., and N.H.S. also acknowledge support from Stanford Medicine for this\nresearch.\nAUTHOR CONTRIBUTIONS\nM.W., Y.X., J.F., and N.H.S. conceptualized and designed the study; M.W., Y.X., and R.T.\nextracted data; M.W., Y.X., B.P., R.T., J.F., and N.H.S. conducted the analysis and wrote\nthe manuscript. M.W., Y.X., B.P., R.T., J.F., N.H.S., and M.A.P. revised the manuscript. E.S.\nand S.F. contributed to the analysis. All authors approved theﬁnal version of the\nmanuscript and take accountability for all aspects of the work.\nCOMPETING INTERESTS\nB.P. reports stock-based compensation from Google, LLC. Otherwise, the authors\ndeclare that there are no competing interests.\nADDITIONAL INFORMATION\nCorrespondence and requests for materials should be addressed to Michael Wornow.\nReprints and permission information is available at http://www.nature.com/\nreprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims\nin published maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons license, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly\nfrom the copyright holder. To view a copy of this license, visit http://\ncreativecommons.org/licenses/by/4.0/.\n© The Author(s) 2023\nM. Wornow et al.\n10\nnpj Digital Medicine (2023)   135 Published in partnership with Seoul National University Bundang Hospital",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.8346309661865234
    },
    {
      "name": "Computer science",
      "score": 0.636556088924408
    },
    {
      "name": "Narrative",
      "score": 0.5675703287124634
    },
    {
      "name": "Data science",
      "score": 0.5239272713661194
    },
    {
      "name": "Health care",
      "score": 0.49399954080581665
    },
    {
      "name": "Health records",
      "score": 0.4617697298526764
    },
    {
      "name": "Medical record",
      "score": 0.4583037197589874
    },
    {
      "name": "Taxonomy (biology)",
      "score": 0.4397142231464386
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3436961770057678
    },
    {
      "name": "Management science",
      "score": 0.322759211063385
    },
    {
      "name": "Medicine",
      "score": 0.20284166932106018
    },
    {
      "name": "Engineering",
      "score": 0.1387394666671753
    },
    {
      "name": "Linguistics",
      "score": 0.086041659116745
    },
    {
      "name": "Radiology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210105015",
      "name": "Stanford Health Care",
      "country": "US"
    }
  ]
}