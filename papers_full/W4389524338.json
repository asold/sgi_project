{
  "title": "Domain Terminology Integration into Machine Translation: Leveraging Large Language Models",
  "url": "https://openalex.org/W4389524338",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2107801911",
      "name": "Yasmin Moslem",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5107099474",
      "name": "Gianfranco Romani",
      "affiliations": [
        "University of Tabriz",
        "Thomson Reuters (Switzerland)"
      ]
    },
    {
      "id": "https://openalex.org/A4284155583",
      "name": "Mahdi Molaei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2159150021",
      "name": "John D. Kelleher",
      "affiliations": [
        "National University of Ireland, Maynooth"
      ]
    },
    {
      "id": "https://openalex.org/A2101195297",
      "name": "Rejwanul Haque",
      "affiliations": [
        "Institute of Technology Carlow",
        "South East Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2110815193",
      "name": "Andy Way",
      "affiliations": [
        "Dublin City University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4386644478",
    "https://openalex.org/W3105214104",
    "https://openalex.org/W4321472057",
    "https://openalex.org/W2964029788",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W2117278770",
    "https://openalex.org/W2962714778",
    "https://openalex.org/W3084906606",
    "https://openalex.org/W2963352809",
    "https://openalex.org/W4387561528",
    "https://openalex.org/W4386365131",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4320167623",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2758950307",
    "https://openalex.org/W2963122608",
    "https://openalex.org/W2945735543",
    "https://openalex.org/W4285077564",
    "https://openalex.org/W3045976635",
    "https://openalex.org/W4291238448",
    "https://openalex.org/W3082303676",
    "https://openalex.org/W2963281280",
    "https://openalex.org/W4321177597",
    "https://openalex.org/W3038033387",
    "https://openalex.org/W3082928416",
    "https://openalex.org/W4385572225",
    "https://openalex.org/W2951770285",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4318903120",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3201284524",
    "https://openalex.org/W2744813330",
    "https://openalex.org/W2950940239",
    "https://openalex.org/W3113780189"
  ],
  "abstract": "This paper discusses the methods that we used for our submissions to the WMT 2023 Terminology Shared Task for German-to-English (DE-EN), English-to-Czech (EN-CS), and Chinese-to-English (ZH-EN) language pairs. The task aims to advance machine translation (MT) by challenging participants to develop systems that accurately translate technical terms, ultimately enhancing communication and understanding in specialised domains. To this end, we conduct experiments that utilise large language models (LLMs) for two purposes: generating synthetic bilingual terminology-based data, and post-editing translations generated by an MT model through incorporating pre-approved terms. Our system employs a four-step process: (i) using an LLM to generate bilingual synthetic data based on the provided terminology, (ii) fine-tuning a generic encoder-decoder MT model, with a mix of the terminology-based synthetic data generated in the first step and a randomly sampled portion of the original generic training data, (iii) generating translations with the fine-tuned MT model, and (iv) finally, leveraging an LLM for terminology-constrained automatic post-editing of the translations that do not include the required terms. The results demonstrate the effectiveness of our proposed approach in improving the integration of pre-approved terms into translations. The number of terms incorporated into the translations of the blind dataset increases from an average of 36.67% with the generic model to an average of 72.88% by the end of the process. In other words, successful utilisation of terms nearly doubles across the three language pairs.",
  "full_text": "Proceedings of the Eighth Conference on Machine Translation (WMT), pages 902–911\nDecember 6–7, 2023. ©2023 Association for Computational Linguistics\n902\nDomain Terminology Integration into Machine Translation:\nLeveraging Large Language Models\nYasmin Moslem1,2,§, Gianfranco Romani3, Mahdi Molaei4, Rejwanul Haque1,5, John D. Kelleher1,6, and Andy Way1,2\n1ADAPT Centre\n2School of Computing, Dublin City University, Dublin, Ireland\n3Thomson Reuters, Zug, Switzerland\n4Department of Computer Engineering, University of Tabriz, Tabriz, Iran\n5Department of Computing, South East Technological University, Carlow, Ireland\n6Hamilton Institute, Maynooth University, Maynooth, Ireland\nAbstract\nThis paper discusses the methods that we\nused for our submissions to the WMT 2023\nTerminology Shared Task for German-to-\nEnglish (DE-EN), English-to-Czech (EN-CS),\nand Chinese-to-English (ZH-EN) language pairs.\nThe task aims to advance machine translation\n(MT) by challenging participants to develop\nsystems that accurately translate technical\nterms, ultimately enhancing communication\nand understanding in specialised domains.\nTo this end, we conduct experiments that\nutilise large language models (LLMs) for\ntwo purposes: generating synthetic bilingual\nterminology-based data, and post-editing\ntranslations generated by an MT model through\nincorporating pre-approved terms. Our system\nemploys a four-step process: (i) using an LLM\nto generate bilingual synthetic data based on the\nprovided terminology, (ii) fine-tuning a generic\nencoder-decoder MT model, with a mix of the\nterminology-based synthetic data generated in\nthe first step and a randomly sampled portion\nof the original generic training data, (iii)\ngenerating translations with the fine-tuned MT\nmodel, and (iv) finally, leveraging an LLM for\nterminology-constrained automatic post-editing\nof the translations that do not include the\nrequired terms. The results demonstrate the\neffectiveness of our proposed approach in\nimproving the integration of pre-approved\nterms into translations. The number of terms\nincorporated into the translations of the blind\ndataset increases from an average of 36.67%\nwith the generic model to an average of 72.88%\nby the end of the process. In other words,\nsuccessful utilisation of terms nearly doubles\nacross the three language pairs.\n1 Introduction\nThe primary goal of the WMT 2023 Terminology\nShared Task is to evaluate the ability of MT sys-\ntems to accurately translate technical terminology.\n§Correspondence: first_name.last_name@adaptcentre.ie\nThe task aims to assess the extent to which MT\nmodels can utilise additional information regard-\ning the translation of terminology. The shared task\nrequires the participants to provide three transla-\ntions, one without terms and the others with two\nindividual sets of terms.\nThere have been several advancements in the\narea of MT domain adaptation, where an MT model\nis expected to follow the style and terminology\nof a certain domain or client (Chu et al., 2017;\nKobus et al., 2017). Moreover, some researchers\ngive special focus to terminology while training\nand fine-tuning MT systems (Dinu et al., 2019; Hu\net al., 2019b; Haque et al., 2020; Michon et al.,\n2020; Nayak et al., 2023). However, forcing an\nMT model to adhere to certain terminology at infer-\nence time is among the most challenging aspects of\nMT. Hence, several researchers have investigated\napproaches to terminology-constrained decoding\nat translation time (Hokamp and Liu, 2017; Hasler\net al., 2018; Post and Vilar, 2018; Hu et al., 2019a;\nExel et al., 2020). The goal is to ensure that the\nMT system can accommodate unseen terminology\nwhile retaining translation accuracy and fluency.\nRecently, since the emergence of advanced\nLLMs such as GPT-3 (Brown et al., 2020),\nBLOOM (Le Scao et al., 2022), PaLM (Chowdhery\net al., 2022), Falcon (Penedo et al., 2023), Llama 2\n(Touvron et al., 2023), and Jais (Sengupta et al.,\n2023) to mention just a few, researchers have been\nexploring the capabilities of these models for a\nnumber of tasks including MT (Bawden and Yvon,\n2023; Hendy et al., 2023; Jiao et al., 2023; Moslem\net al., 2023; Vilar et al., 2023). Some work inves-\ntigates whether it is possible to utilise LLMs for\nterminology-constrained MT using a pre-defined\nglossary (Moslem et al., 2023) or even a dictio-\nnary (Ghazvininejad et al., 2023). They found the\napproach is generally effective in increasing the\nnumber of terms used in the translation, even for\nlow-resource languages.\n903\nWe highlight our key contributions with the sys-\ntems that we submitted for the WMT 2023 Termi-\nnology Shared Task as follows:\n• LLMs for domain-specific data augmenta-\ntion: In our previous work (Moslem et al.,\n2022), we employed LLMs, namely GPT-J\n(Wang and Komatsuzaki, 2021) and mGPT\n(Shliazhko et al., 2022), to generate domain-\nspecific datasets based on the target sentences\nin a small authentic dataset, then generated\nthe source sentences with back-translation\n(Sennrich et al., 2016; Poncelas et al., 2019),\nand finally fine-tuned an encoder-decoder MT\nmodel on this data. In this work, we take\na couple of steps forward by instructing an\nLLM, namely ChatGPT (Brown et al., 2020;\nOuyang et al., 2022), to generate terminology-\nbased bilingual synthetic data. In other words,\nthe LLM will generate both the source and\ntarget sides of translation pairs, making sure\nthe pre-approved target terms provided by the\norganisers are used in the translations.\n• LLMs for terminology-constrained MT\nand MT post-editing: In our previous work,\nwe utilised an LLM for translation and pro-\nvided it with a list of terms to support in-\ncontext learning, which improved adherence\nto the required terminology at inference time\n(Moslem et al., 2023). We also investigated\nwhether we could use an LLM for post-editing\nMT generated by other systems. In this work,\nwe prompt ChatGPT to insert missing terms\ninto translations generated by an encoder-\ndecoder MT system. In other words, if some\nof the translations generated by a fine-tuned\nMT model still do not include the terms pro-\nvided by the organisers, we feed these transla-\ntions into an LLM, namely ChatGPT, instruct-\ning it to incorporate these terms while using\nthe same translation.\n2 Method\nIn our submissions to the WMT 2023 Terminology\nShared Task, we followed these steps:\n(i) Generate bilingual synthetic data based on the\npre-approved terms, using an LLM, namely\nChatGPT.\n(ii) Fine-tune a generic model, OPUS (Tiede-\nmann and Thottingal, 2020), on a mix of the\nterminology-based synthetic data generated\nin (i) and a randomly sampled portion of the\noriginal generic training data.\n(iii) Generate translations of the dev, test, and\nblind datasets provided by the organisers with\nthe fine-tuned model from (ii).\n(iv) Apply terminology-constrained automatic\npost-editing using ChatGPT to incorporate\nmissing terms into translations that do not yet\ninclude the required terminology.\n2.1 Synthetic Data Generation\nWe used ChatGPT “gpt-3.5-turbo” 2 to generate\nbilingual sentence pairs, using the terms provided\nby the organisers. So, given a target term, the\nmodel was asked to generate multiple translation\npairs, including both the source (e.g. German) and\nthe target (e.g. English). For parameters of Chat-\nGPT’s API, we used top_p 1 and temperature val-\nues 0 and 0.3 to generate diverse outputs.\nExample prompt: Terminology-based generation\nPlease use the “Federal Ministry of Science” to generate just 20 numbered\nsentences in German-English in one Python dictionary format.\nTo filter the generated data, we first removed\nduplicate sentences from the whole dataset, based\non both the source and target. Then, we applied\nlanguage detection of both sides of the data using\nfastText3 and pycld24 libraries to ensure that the\ngenerated sentences were in our desired languages.\nWe excluded any sentences whose scores were be-\nlow a certain threshold, namely 0.9 for fastText and\n90 for pycld2.\nThe filtering step removed less than 1% of the\ngenerated data. However, due to computational\nresource and time limitations, we could not use all\nthe generated data. Table 1 reports the number of\ngenerated, filtered, and used translation pairs.\nInitially, we only had the development and test\ndatasets, so we used them for the German-to-\nEnglish language pair. Later, when the organisers\nreleased the blind dataset, we used the develop-\nment, test and blind datasets for the Chinese-to-\nEnglish and English-to-Czech language pairs.\n2The model “gpt-3.5-turbo” is a relatively efficient and\ncost-effective option, so we wanted to understand the quality\nwe can achieve with it.\n3https://fasttext.cc/docs/en/\nlanguage-identification.html\n4https://github.com/aboSamoor/pycld2\n904\nLang Raw Filtered Used\nDE-EN 124,215 104,318 68,265\nEN-CS 187,471 103,797 64,218\nZH-EN 90,538 72,695 49,001\nTable 1: Terminology-based bilingual data generated by\nChatGPT for fine-tuning the OPUS model\nTo assess the quality of the bilingual data gen-\nerated by ChatGPT, we computed cross-entropy\nscores (Moore and Lewis, 2010) of the synthetic\ntranslation pairs based on the strong encoder-\ndecoder MT model, NLLB-200 3.3B (Costa-jussà\net al., 2022). For scoring, we used CTranslate2 5\n(Klein et al., 2020) score_batch() method with the\nparameters batch_type “tokens” and max_batch_-\nsize 2024. We scored each synthetic translation\npair generated by ChatGPT, and then calculated the\naverage score for the whole dataset. Computing\ndual cross-entropy scores according to two inverse\ntranslation models trained on clean data is an ef-\nfective method to evaluate data quality (Junczys-\nDowmunt, 2018). Hence, we computed the scores\nof both directions of each language pair according\nto the multilingual MT model NLLB-200 3.3B be-\ncause both directions are generated by ChatGPT.\nTo produce a baseline for translation quality, we\ngenerated the translations of the same datasets us-\ning NLLB-200 3.3B for each language direction\nwith beam_size 4, and then scored these transla-\ntions with the same model. As the scores are in the\nform of negative log probabilities, we converted\nthem to their exponential equivalents for readabil-\nity, which are reported in Table 2. It is normal that\nthe model NLLB-200 generates higher scores for\nits own translations; however, we wanted to know\nto what extent such scores are comparable to those\nof ChatGPT’s synthetic translation pairs. Accord-\ning to the scores, the German↔English language\npair had the most comparable quality, followed by\nCzech↔English, and Chinese ↔English language\npairs.\nAmong the approaches that can be employed\nfor assessing the quality of synthetic bilingual data\nis semantic similarity between the two sides of\neach translation pair (e.g. with mUSE (Yang et al.,\n2020)). However, the scoring approach that we\npreviously described and used achieves a similar\ngoal while comparing the quality of the synthetic\nbilingual data to the translation quality of a strong\nMT baseline model, namely NLLB-200 3.3B.\n5https://github.com/OpenNMT/CTranslate2\nLang ChatGPT NLLB Diff.\nDE-EN 0.59 0.68 0.09\nEN-DE 0.56 0.64 0.08\nAvg. 0.58 0.66 0.08\nCS-EN 0.58 0.70 0.12\nEN-CS 0.49 0.58 0.09\nAvg. 0.54 0.64 0.10\nZH-EN 0.39 0.56 0.17\nEN-ZH 0.09 0.34 0.25\nAvg. 0.24 0.45 0.21\nTable 2: Scores of translation pairs generated by ChatGPT\nbased on the NLLB-200 3.3B model\n2.2 Fine-tuning\nUsing the term-based synthetic bilingual data gen-\nerated in the previous step, we fine-tuned encoder-\ndecoder Transformer-based MT models (Vaswani\net al., 2017). In particular, we fine-tuned OPUS\nMT models, with Hugging Face Transformers. 6\nWe applied mixed fine-tuning (Chu et al., 2017);\nin other words, we fine-tuned the baseline model\nwith a mix of the terminology-based synthetic data\ngenerated from the previous step (cf. Section\n2.1) and a randomly sampled portion of the orig-\ninal generic data used to train the OPUS baseline\nmodel. The numbers of segments taken from the\nOPUS generic data are as follows: CS: 372,928,\nDE: 419,881, ZH: 462,780. We over-sampled the\nsynthetic terminology-based data to make it the\nsame size as the used portion of generic data. The\nfine-tuning parameters are as follows: train = 0.9,\nval = 0.1, batch_size = 32, learning_rate = 2e-5,\naccumulate_gradient = 4, weight_decay = 0.01,\nnum_train_epochs = 1, max_input_length = 256,\nmax_target_length = 256. Finally, we used the\nfine-tuned model to generate translations for the\ndevelopment, test, and blind sets.\nAt first glance, the fine-tuning step might look\nredundant if the LLM can achieve the same trans-\nlation quality directly, either via zero-shot transla-\ntion or few-shot in-context learning (Moslem et al.,\n2023). However, domain-specific or terminology-\nbased knowledge distillation (Treviso et al., 2023)\nfrom a massive LLM to a compact task-oriented\nMT model can help boost efficiency at inference\ntime while enhancing domain adaptation and ter-\nminology adherence. Obviously, when authentic\nin-domain data is available, it can be used for fine-\ntuning instead of synthetic data for domain adap-\ntation of the MT model. In production workflows,\n6https://github.com/huggingface/transformers\n905\nonly segments that do not meet specific quality\ncriteria are passed to either human or automatic\npost-editing. Hence, deployment of a model fine-\ntuned on in-domain data can reduce the number of\ntranslations that need post-editing.\n2.3 Terminology-constrained Automatic\nPost-Editing\nFor the shared task, the organisers provided two\nterm sets for each source sentence in the test and\nblind datasets, and expected the participants to gen-\nerate two translations that use one term set each.\nIn this step of terminology-constrained automatic\npost-editing, we aim to refine the translations gen-\nerated by an MT system by inserting the required\nterminology. To this end, we checked the transla-\ntions generated by the fine-tuned model from the\nprevious step (cf. Section 2.2). For each term set\nprovided for the sentence, if the translation does\nnot include all the terms, we ran this step of termi-\nnology insertion into the translation.\nThis step involves instructing ChatGPT to post-\nedit the translation by making sure it includes all\nthe terms without changing the rest of the transla-\ntion. For the API’s parameters, we used top_p 1\nand temperature values 0 and 0.2, and then chose\nthe generation that fixed more terms.\nExample prompt: Terminology-constrained post-editing\nIn the following <tgt_lang> translation, use the <tgt_term> to translate\nthe <src_lang> term <src_term>, and the...7 Leave everything else\nthe same.\\n\\n\n<src_lang>: <src_segment>\\n\n<tgt_lang>: <tgt_segment>\n3 Evaluation\nTo assess the effectiveness of our process, we con-\nducted two types of evaluation: (i) term-level eval-\nuation in order to measure the level of adherence to\nthe required terminology, and (ii) sentence-level\nevaluation in order to see whether the process\naffected the quality of the overall translation.\n3.1 Term-level Evaluation\nIn Tables 3 and 4, we report the number of terms\nused in the translations of the test and blind\ndatasets, respectively, in respect to the two term sets\nprovided by the organisers. The results show the ef-\nfectiveness of our proposed process, increasing the\n7We can add more terms, if needed.\nintegration of the required terms in the final transla-\ntions of the blind dataset from an average of 36.67%\nwith the baseline generic model to an average of\n72.88% after the LLM-based post-editing, across\nthe three language pairs. Interestingly, prompt-\ning an LLM to integrate the required terms into\nthe translations generated by a fine-tuned encoder-\ndecoder MT model was more effective than solely\nusing the fine-tuned model.\nLang System Total [1] Used [1] Total [2] Used [2] Avg %\nDE-EN\nBaseline 432 291 317 168 60.18\nFine-tuned 432 302 317 165 60.98\nTerm APE 432 397 317 239 83.65\nEN-CS\nBaseline 550 221 313 139 42.30\nFine-tuned 550 135 313 108 29.53\nTerm APE 550 466 313 283 87.57\nZH-EN\nBaseline 1779 498 1938 491 26.66\nFine-tuned 1779 854 1938 570 38.71\nTerm APE 1779 1137 1938 886 54.81\nAvg. %\nBaseline 43.05\nFine-tuned 43.07\nTerm APE 75.34\nTable 3: For the test dataset, the number of terms used in the\ntranslations from the first term set [1] and the second term\nset [2]. According to the results, terminology-constrained au-\ntomatic post-editing (“Term APE”) using ChatGPT achieved\nthe best adoption of the required terminology.\nLang System Total [1] Used [1] Total [2] Used [2] Avg %\nDE-EN\nBaseline 11357 4120 11202 4623 38.77\nfine-tuned 11357 4130 11202 4621 38.81\nTerm APE 11357 6257 11202 5893 53.85\nEN-CS\nBaseline 10626 3964 10563 5122 42.90\nFine-tuned 10626 3397 10563 4412 36.87\nTerm APE 10626 8727 10563 8681 82.16\nZH-EN\nBaseline 2892 1375 2908 265 28.33\nFine-tuned 2892 1422 2908 970 41.26\nTerm APE 2892 2471 2908 2322 82.65\nAvg. %\nBaseline 36.67\nFine-tuned 38.98\nTerm APE 72.88\nTable 4: For the blind dataset, the number of terms used in\nthe translations from the first term set [1] and the second term\nset [2]. According to the results, terminology-based automatic\npost-editing (“Term APE”) using ChatGPT achieved the best\nadoption of the required terminology.\n3.2 Sentence-level Evaluation\nAfter the end of the submission phase, the organ-\nisers released the references for the participants to\nconduct automatic evaluation. The main purpose of\nthis sentence-based evaluation process is to deter-\nmine whether terminology integration affected the\noverall quality of translation. In general, as demon-\nstrated in Table 4 and Table 5, this terminology-\nconstrained automatic post-editing step signifi-\ncantly increased the inclusion of the necessary\n906\nterms into the final translation while improving\ntranslation quality across the three language pairs.\nFor the automatic evaluation of each MT system,\nwe used the BLEU (Papineni et al., 2002), chrF++\n(Popovi´c, 2017), and COMET (Rei et al., 2020)\nmetrics. Since many of the Chinese-to-English\nsegments in the blind dataset did not have two term\nsets, we evaluated only those that had two term sets\n(1629 segments out of 2640 segments). We observe\nthat the evaluation scores of the Chinese-to-English\ntranslation task are much lower than those of the\ntwo other language pairs. This can be due to the\nliterary nature of the blind dataset extracted from\nChinese novels, which might be difficult for both\nthe MT model and automatic evaluation metrics.\nLang Count System BLEU chrF++ COMET\nDE-EN 2963\nBaseline 19.81 48.04 21.81\nFine-tuned 19.27 47.75 21.51\nTerm APE [1] 32.36 60.84 40.25\nTerm APE [2] 27.84 56.84 33.20\nTerm APE Avg. 30.10 58.84 36.73\nEN-CS 3005\nBaseline 29.13 53.11 50.90\nFine-tuned 24.54 49.14 33.78\nTerm APE [1] 45.65 67.36 79.84\nTerm APE [2] 37.88 61.19 63.64\nTerm APE Avg. 41.77 64.28 71.74\nZH-EN 1629\nBaseline 6.95 27.95 -50.90\nFine-tuned 7.76 29.26 -38.83\nTerm APE [1] 9.56 32.80 -18.96\nTerm APE [2] 11.93 35.30 -13.51\nTerm APE Avg. 10.75 34.05 -16.24\nTable 5: Automatic evaluation of the overall translation quality\nacross the three language pairs based on the blind dataset.\nThe “Baseline” refers to the OPUS model without fine-tuning,\nwhile “Fine-tuned” refers to the model after domain adaptation\nwith the bilingual terminology-based synthetic data generated\nby an LLM. Finally, the three last rows for each language pair\nrefer to using ChatGPT for terminology-constrained automatic\npost-editing (“Term APE”) of the MT output generated by the\nfine-tuned model. In other words, “Term APE [1]” indicates\nthe results when the first term set was used to prompt ChatGPT\nto integrate terms of this set into the translation generated by\nthe fine-tuned model, while “Term APE [2]” refers to using\nthe second term set. Finally, “Term APE Avg.” is the average\nof “Term APE [1]” and “Term APE [2]” for each language\npair. Terminology-constrained automatic post-editing with\nChatGPT achieves the best results across the three language\npairs in terms of the overall translation quality. As reported\nin Table 4, the number of terms integrated after the automatic\npost-editing step also increased.\nMoreover, it is worth noting that we used the\nEnglish term while generating bilingual synthetic\ndata (cf. Section 2.1) for the three language pairs.\nHowever, English is the target language for both\nChinese-to-English and German-to-English lan-\nguage directions, while it is the source language\nfor the English-to-Czech language direction. This\ncan explain the performance degradation after the\nfine-tuning step in the English-to-Czech language\ndirection (cf. Tables 4 and 5). In other words, it\nis recommended in the step of bilingual synthetic\ndata generation to either use the target term or both\nthe source and target terms while prompting the\nLLM to generate translation pairs.\nAs explained in Section 2.3, our final step of\nterminology-constrained automatic post-editing in-\nvolves instructing an LLM to insert terms that were\nmissing from the output of the fine-tuned model.\nThis significantly increased term usage across\nall the Chinese-to-English, English-to-Czech, and\nGerman-to-English language pairs (cf. Table 4).\nFurthermore, as demonstrated in Table 5, this step\nhad no detrimental effects on translation quality. In\nfact, integrating the necessary terms into the trans-\nlation using ChatGPT improved translation quality\naccording to our automatic evaluation.\n4 Conclusion and Future Work\nIn this work, we showed that applying a multistep\nprocess of mixed fine-tuning on terminology-based\nsynthetic bilingual data and then terminology-\nconstrained automatic post-editing with an LLM\ncan increase the adherence to the pre-approved\nterms in the generated translations. By the end\nof the process, the use of the required terms has\nincreased in the translations of the blind dataset\nacross the three language pairs from an average of\n36.67% with the baseline generic model to an aver-\nage of 72.88% after instructing an LLM to integrate\nthe required terms into the translations.\nDue to the task restrictions, we had to fine-tune\nOPUS models only. We would like to experiment\nwith fine-tuning NLLB models, and probably the\nnew SeamlessM4T (Barrault et al., 2023), Mistral\n(Jiang et al., 2023), and MADLAD-400 models\n(Kudugunta et al., 2023), on the same data and\ncompare the output quality. In our experiments,\nwe employed ChatGPT “gpt-3.5-turbo” for both\nterminology-based synthetic data generation and\nterminology-constrained automatic post-editing, as\nit is a relatively efficient and cost-effective option.\nIn the future, we would like to repeat the same\nexperiments with GPT-4 in order to assess the ben-\nefit of using a stronger language model on overall\nperformance. We observe that BLOOM can be\nused as an alternative LLM for data generation;\nhowever, one-shot generation might work better\nthan zero-shot generation. In this case, the prompt\ncan consist of a term, a bilingual sentence pair,\n907\nand then another term. Interestingly, the model\nwill predict a new translation pair including the\nsecond term. While certain open-source models\nsuch as Llama 2 and Falcon might be employed for\nthe terminology-constrained automatic post-editing\nstep for certain languages, we suspect that they will\nneed fine-tuning before being reliably usable for\nmost languages.\nIn future work, we will carry out a deeper analy-\nsis of the generated synthetic data together with the\noutputs of the fine-tuned models in order to under-\nstand how the properties of the synthetic data affect\nthe fine-tuning results. It is important also to test\nthe same approach for other languages, especially\nlow-resource language pairs.\nMoreover, it would be interesting to exclude the\nfine-tuning step and assess the overall translation\nquality after LLM-based post-editing. It is pos-\nsible that domain adaptation through fine-tuning\nthe baseline MT model either on authentic or syn-\nthetic data would still be beneficial. It can lead\nto domain-specific improvements in the overall\ntranslation quality that may not be achievable by\nthe baseline model or the terminology-constrained\npost-editing step. Again, deploying a model fine-\ntuned on in-domain data into production can en-\nhance terminology adherence in initial translations.\nAs there is no need to send the translations that\nalready include the pre-approved terms to the LLM\nfor terminology-constrained post-editing, this can\nreduce the number of translations that require post-\nediting. Such an efficient workflow can allow us\nto save resources, and minimise latency at infer-\nence time. Similarly, there are potential advantages\nof employing an LLM for post-editing rather than\nfor direct translation. Instead of solely relying on\nthe translation quality of the LLM, quality estima-\ntion can be performed to select the best MT model\nin general or for the current source text segment.\nUltimately, only segments that do not meet quality\ncriteria are then passed to the LLM for post-editing.\nAcknowledgements\nThis work is supported by the Science Foun-\ndation Ireland (SFI) Centre for Research Train-\ning in Digitally-Enhanced Reality (d-real) under\nGrant No. 18/CRT/6224, the ADAPT Centre for\nDigital Content Technology under SFI’s Grant No.\n13/RC/2106_P2, and Microsoft Research.\nReferences\nLoic Barrault, Andy Chung, David Dale, Ning Dong\n(ai), Paul-Ambroise Duquenne, Hady Elsahar,\nHongyu Gong, Kevin Heffernan, John Hoffman,\nChristopher Klaiber, Peng-Jen Chen, Daniel Licht,\nJean Maillard, Alice Rakotoarison, Kaushik Ram\nSadagopan, Guillaume Wenzek, Abinesh Ramakrish-\nnan, Alexandre Mourachko, Amanda Kallet, Ann\nLee, Anna Sun, Bapi Akula, Benjamin Peloquin,\nBernie Huang, Bokai Yu, Brian Ellis, Can Balioglu,\nCarleigh Wood, Changhan Wang, Christophe Rop-\ners, Cynthia Gao, Daniel Li (fair), Elahe Kalbassi,\nEthan Ye, Gabriel Mejia Gonzalez, Hirofumi In-\naguma, Holger Schwenk, Igor Tufanov, Ilia Kulikov,\nJanice Lam, Jeff Wang (pm Ai), Juan Pino, Justin\nHaaheim, Justine Kao, Prangthip Hasanti, Kevin\nTran, Maha Elbayad, Marta R Costa-jussa, Mo-\nhamed Ramadan, Naji El Hachem, Onur Çelebi, Paco\nGuzmán, Paden Tomasello, Pengwei Li, Pierre An-\ndrews, Ruslan Mavlyutov, Russ Howes, Safiyyah\nSaleem, Skyler Wang, Somya Jain, Sravya Popuri,\nTuan Tran, Vish V ogeti, Xutai Ma, and Yilin Yang.\n2023. SeamlessM4T—Massively Multilingual &\nMultimodal Machine Translation.\nRachel Bawden and François Yvon. 2023. Investigating\nthe Translation Performance of a Large Multilingual\nLanguage Model: the Case of BLOOM. In Proceed-\nings of the 24th Annual Conference of the European\nAssociation for Machine Translation, pages 157–170,\nTampere, Finland. European Association for Machine\nTranslation.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language Models are Few-Shot Learners. In\nAdvances in Neural Information Processing Systems\n(NeurIPS 2020), volume 33, pages 1877–1901, Vir-\ntual. Curran Associates, Inc.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M Dai, Thanumalayan Sankaranarayana\n908\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Mor-\neira, Rewon Child, Oleksandr Polozov, Katherine\nLee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta,\nMark Diaz, Orhan Firat, Michele Catasta, Jason\nWei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean,\nSlav Petrov, and Noah Fiedel. 2022. PaLM: Scaling\nLanguage Modeling with Pathways. arXiv preprint\narXiv:2204.02311 [cs.CL].\nChenhui Chu, Raj Dabre, and Sadao Kurohashi. 2017.\nAn Empirical Comparison of Domain Adaptation\nMethods for Neural Machine Translation. In Pro-\nceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers), pages 385–391, Vancouver, Canada. Associ-\nation for Computational Linguistics.\nMarta R Costa-jussà, James Cross, Onur Çelebi,\nMaha Elbayad, Kenneth Heafield, Kevin Heffer-\nnan, Elahe Kalbassi, Janice Lam, Daniel Licht,\nJean Maillard, Anna Sun, Skyler Wang, Guillaume\nWenzek, Al Youngblood, Bapi Akula, Loic Bar-\nrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,\nJohn Hoffman, Semarley Jarrett, Kaushik Ram\nSadagopan, Dirk Rowe, Shannon Spruit, Chau\nTran, Pierre Andrews, Necip Fazil Ayan, Shruti\nBhosale, Sergey Edunov, Angela Fan, Cynthia\nGao, Vedanuj Goswami, Francisco Guzmán, Philipp\nKoehn, Alexandre Mourachko, Christophe Rop-\ners, Safiyyah Saleem, Holger Schwenk, and Jeff\nWang. 2022. No Language Left Behind: Scaling\nhuman-centered machine translation. arXiv preprint\narXiv:2207.04672 [cs.CL].\nGeorgiana Dinu, Prashant Mathur, Marcello Federico,\nand Yaser Al-Onaizan. 2019. Training Neural Ma-\nchine Translation to Apply Terminology Constraints.\nIn Proceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 3063–\n3068, Florence, Italy. Association for Computational\nLinguistics.\nMiriam Exel, Bianka Buschbeck, Lauritz Brandt, and\nSimona Doneva. 2020. Terminology-Constrained\nNeural Machine Translation at SAP. In Proceedings\nof the 22nd Annual Conference of the European As-\nsociation for Machine Translation, pages 271–280,\nLisboa, Portugal. European Association for Machine\nTranslation.\nMarjan Ghazvininejad, Hila Gonen, and Luke Zettle-\nmoyer. 2023. Dictionary-based Phrase-level Prompt-\ning of Large Language Models for Machine Transla-\ntion. arXiv preprint arXiv:2302.07856 [cs.CL].\nRejwanul Haque, Yasmin Moslem, and Andy Way. 2020.\nTerminology-Aware Sentence Mining for NMT Do-\nmain Adaptation: ADAPT’s Submission to the Adap-\nMT 2020 English-to-Hindi AI Translation Shared\nTask. In Proceedings of the 17th International Con-\nference on Natural Language Processing (ICON):\nAdap-MT 2020 Shared Task, pages 17–23, Patna, In-\ndia. NLP Association of India (NLPAI).\nEva Hasler, Adrià de Gispert, Gonzalo Iglesias, and Bill\nByrne. 2018. Neural Machine Translation Decoding\nwith Terminology Constraints. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 2 (Short Pa-\npers), pages 506–512, New Orleans, Louisiana. As-\nsociation for Computational Linguistics.\nAmr Hendy, Mohamed Abdelrehim, Amr Sharaf,\nVikas Raunak, Mohamed Gabr, Hitokazu Matsushita,\nYoung Jin Kim, Mohamed Afify, and Hany Hassan\nAwadalla. 2023. How Good Are GPT Models at\nMachine Translation? A Comprehensive Evaluation.\narXiv preprint arXiv:2302.09210 [cs.CL].\nChris Hokamp and Qun Liu. 2017. Lexically Con-\nstrained Decoding for Sequence Generation Using\nGrid Beam Search. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1535–\n1546, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nJ Edward Hu, Huda Khayrallah, Ryan Culkin, Patrick\nXia, Tongfei Chen, Matt Post, and Benjamin\nVan Durme. 2019a. Improved Lexically Constrained\nDecoding for Translation and Monolingual Rewriting.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 839–850,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nJunjie Hu, Mengzhou Xia, Graham Neubig, and Jaime\nCarbonell. 2019b. Domain Adaptation of Neural\nMachine Translation by Lexicon Induction. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 2989–\n3001, Florence, Italy. Association for Computational\nLinguistics.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, Lélio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timothée Lacroix,\nand William El Sayed. 2023. Mistral 7B. arXiv\npreprint arXiv:2310.06825 [cs.CL].\nWenxiang Jiao, Wenxuan Wang, Jen-Tse Huang, Xing\nWang, and Zhaopeng Tu. 2023. Is ChatGPT A Good\nTranslator? Yes With GPT-4 As The Engine. arXiv\npreprint arXiv:2301.08745 [cs.CL].\nMarcin Junczys-Dowmunt. 2018. Dual Conditional\nCross-Entropy Filtering of Noisy Parallel Corpora.\nIn Proceedings of the Third Conference on Machine\nTranslation: Shared Task Papers, pages 888–895,\nBelgium, Brussels. Association for Computational\nLinguistics.\nGuillaume Klein, Dakun Zhang, Clément Chouteau,\nJosep Crego, and Jean Senellart. 2020. Efficient and\n909\nhigh-quality neural machine translation with Open-\nNMT. In Proceedings of the Fourth Workshop on\nNeural Generation and Translation, pages 211–217,\nStroudsburg, PA, USA. Association for Computa-\ntional Linguistics.\nCatherine Kobus, Josep Crego, and Jean Senellart. 2017.\nDomain Control for Neural Machine Translation. In\nProceedings of Recent Advances in Natural Lan-\nguage Processing, pages 372–378, Varna, Bulgaria.\nSneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier\nGarcia, Christopher A Choquette-Choo, Katherine\nLee, Derrick Xin, Aditya Kusupati, Romi Stella,\nAnkur Bapna, and Orhan Firat. 2023. MADLAD-\n400: A Multilingual And Document-Level Large\nAudited Dataset. arXiv preprint arXiv:2309.04662\n[cs.CL].\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, Jonathan Tow, Alexander M Rush,\nStella Biderman, Albert Webson, Pawan Sasanka Am-\nmanamanchi, Thomas Wang, Benoît Sagot, Niklas\nMuennighoff, Albert Villanova del Moral, Olatunji\nRuwase, Rachel Bawden, Stas Bekman, Angelina\nMcMillan-Major, Iz Beltagy, Huu Nguyen, Lucile\nSaulnier, Samson Tan, Pedro Ortiz Suarez, Vic-\ntor Sanh, Hugo Laurençon, Yacine Jernite, Julien\nLaunay, Margaret Mitchell, Colin Raffel, Aaron\nGokaslan, Adi Simhi, Aitor Soroa, Alham Fikri\nAji, Amit Alfassy, Anna Rogers, Ariel Kreisberg\nNitzav, Canwen Xu, Chenghao Mou, Chris Emezue,\nChristopher Klamm, Colin Leong, Daniel van Strien,\nDavid Ifeoluwa Adelani, Dragomir Radev, Ed-\nuardo González Ponferrada, Efrat Levkovizh, Ethan\nKim, Eyal Bar Natan, Francesco De Toni, Gérard\nDupont, Germán Kruszewski, Giada Pistilli, Hady\nElsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris\nAbdulmumin, Isaac Johnson, Itziar Gonzalez-Dios,\nJavier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu,\nJonathan Chang, Jörg Frohberg, Joseph Tobing, Joy-\ndeep Bhattacharjee, Khalid Almubarak, Kimbo Chen,\nKyle Lo, Leandro V on Werra, Leon Weber, Long\nPhan, Loubna Ben allal, Ludovic Tanguy, Manan\nDey, Manuel Romero Muñoz, Maraim Masoud,\nMaría Grandury, Mario Šaško, Max Huang, Max-\nimin Coavoux, Mayank Singh, Mike Tian-Jian Jiang,\nMinh Chien Vu, Mohammad A Jauhar, Mustafa\nGhaleb, Nishant Subramani, Nora Kassner, Nuru-\nlaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona\nde Gibert, Paulo Villegas, Peter Henderson, Pierre\nColombo, Priscilla Amuok, Quentin Lhoest, Rheza\nHarliman, Rishi Bommasani, Roberto Luis López,\nRui Ribeiro, Salomey Osei, Sampo Pyysalo, Se-\nbastian Nagel, Shamik Bose, Shamsuddeen Hassan\nMuhammad, Shanya Sharma, Shayne Longpre, So-\nmaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Syd-\nney Zink, Tiago Timponi Torrent, Timo Schick, Tris-\ntan Thrush, Valentin Danchev, Vassilina Nikoulina,\nVeronika Laippala, Violette Lepercq, Vrinda Prabhu,\nZaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin\nHeinzerling, Chenglei Si, Davut Emre Ta¸ sar, Eliza-\nbeth Salesky, Sabrina J Mielke, Wilson Y Lee, Ab-\nheesht Sharma, Andrea Santilli, Antoine Chaffin,\nArnaud Stiegler, Debajyoti Datta, Eliza Szczechla,\nGunjan Chhablani, Han Wang, Harshit Pandey, Hen-\ndrik Strobelt, Jason Alan Fries, Jos Rozen, Leo\nGao, Lintang Sutawika, M Saiful Bari, Maged S\nAl-shaibani, Matteo Manica, Nihal Nayak, Ryan\nTeehan, Samuel Albanie, Sheng Shen, Srulik Ben-\nDavid, Stephen H Bach, Taewoon Kim, Tali Bers,\nThibault Fevry, Trishala Neeraj, Urmish Thakker,\nVikas Raunak, Xiangru Tang, Zheng-Xin Yong,\nZhiqing Sun, Shaked Brody, Yallow Uri, Hadar\nTojarieh, Adam Roberts, Hyung Won Chung, Jae-\nsung Tae, Jason Phang, Ofir Press, Conglong Li,\nDeepak Narayanan, Hatim Bourfoune, Jared Casper,\nJeff Rasley, Max Ryabinin, Mayank Mishra, Minjia\nZhang, Mohammad Shoeybi, Myriam Peyrounette,\nNicolas Patry, Nouamane Tazi, Omar Sanseviero,\nPatrick von Platen, Pierre Cornette, Pierre François\nLavallée, Rémi Lacroix, Samyam Rajbhandari, San-\nchit Gandhi, Shaden Smith, Stéphane Requena, Suraj\nPatil, Tim Dettmers, Ahmed Baruwa, Amanpreet\nSingh, Anastasia Cheveleva, Anne-Laure Ligozat,\nArjun Subramonian, Aurélie Névéol, Charles Lover-\ning, Dan Garrette, Deepak Tunuguntla, Ehud Reiter,\nEkaterina Taktasheva, Ekaterina V oloshina, Eli Bog-\ndanov, Genta Indra Winata, Hailey Schoelkopf, Jan-\nChristoph Kalo, Jekaterina Novikova, Jessica Zosa\nForde, Jordan Clive, Jungo Kasai, Ken Kawamura,\nLiam Hazan, Marine Carpuat, Miruna Clinciu, Na-\njoung Kim, Newton Cheng, Oleg Serikov, Omer\nAntverg, Oskar van der Wal, Rui Zhang, Ruochen\nZhang, Sebastian Gehrmann, Shachar Mirkin, Shani\nPais, Tatiana Shavrina, Thomas Scialom, Tian Yun,\nTomasz Limisiewicz, Verena Rieser, Vitaly Protasov,\nVladislav Mikhailov, Yada Pruksachatkun, Yonatan\nBelinkov, Zachary Bamberger, Zdenˇek Kasner, Al-\nice Rueda, Amanda Pestana, Amir Feizpour, Ammar\nKhan, Amy Faranak, Ana Santos, Anthony Hevia,\nAntigona Unldreaj, Arash Aghagol, Arezoo Abdol-\nlahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh\nBehroozi, Benjamin Ajibade, Bharat Saxena, Car-\nlos Muñoz Ferrandis, Daniel McDuff, Danish Con-\ntractor, David Lansky, Davis David, Douwe Kiela,\nDuong A Nguyen, Edward Tan, Emi Baylor, Ez-\ninwanne Ozoani, Fatima Mirza, Frankline Onon-\niwu, Habib Rezanejad, Hessie Jones, Indrani Bhat-\ntacharya, Irene Solaiman, Irina Sedenko, Isar Ne-\njadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis\nSanz, Livia Dutra, Mairon Samagaio, Maraim El-\nbadri, Margot Mieskes, Marissa Gerchick, Martha\nAkinlolu, Michael McKenna, Mike Qiu, Muhammed\nGhauri, Mykola Burynok, Nafis Abrar, Nazneen\nRajani, Nour Elkott, Nour Fahmy, Olanrewaju\nSamuel, Ran An, Rasmus Kromann, Ryan Hao,\nSamira Alizadeh, Sarmad Shubber, Silas Wang,\nSourav Roy, Sylvain Viguier, Thanh Le, Tobi Oye-\nbade, Trieu Le, Yoyo Yang, Zach Nguyen, Ab-\nhinav Ramesh Kashyap, Alfredo Palasciano, Al-\nison Callahan, Anima Shukla, Antonio Miranda-\nEscalada, Ayush Singh, Benjamin Beilharz, Bo Wang,\nCaio Brito, Chenxi Zhou, Chirag Jain, Chuxin\nXu, Clémentine Fourrier, Daniel León Periñán,\n910\nDaniel Molano, Dian Yu, Enrique Manjavacas, Fabio\nBarth, Florian Fuhrimann, Gabriel Altay, Giyased-\ndin Bayrak, Gully Burns, Helena U Vrabec, Imane\nBello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas\nGolde, Jose David Posada, Karthik Rangasai Sivara-\nman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato,\nMadeleine Hahn de Bykhovetz, Maiko Takeuchi,\nMarc Pàmies, Maria A Castillo, Marianna Nezhurina,\nMario Sänger, Matthias Samwald, Michael Cullan,\nMichael Weinberg, Michiel De Wolf, Mina Mihalj-\ncic, Minna Liu, Moritz Freidank, Myungsun Kang,\nNatasha Seelam, Nathan Dahlberg, Nicholas Michio\nBroad, Nikolaus Muellner, Pascale Fung, Patrick\nHaller, Ramya Chandrasekhar, Renata Eisenberg,\nRobert Martin, Rodrigo Canalli, Rosaline Su, Ruisi\nSu, Samuel Cahyawijaya, Samuele Garda, Shlok S\nDeshmukh, Shubhanshu Mishra, Sid Kiblawi, Si-\nmon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Ste-\nfan Schweter, Sushil Bharati, Tanmay Laud, Théo\nGigant, Tomoya Kainuma, Wojciech Kusa, Yanis\nLabrak, Yash Shailesh Bajaj, Yash Venkatraman,\nYifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli\nXie, Zifan Ye, Mathilde Bras, Younes Belkada, and\nThomas Wolf. 2022. BLOOM: A 176B-Parameter\nOpen-Access Multilingual Language Model. arXiv\npreprint arXiv:2211.05100 [cs.CL].\nElise Michon, Josep Crego, and Jean Senellart. 2020.\nIntegrating Domain Terminology into Neural Ma-\nchine Translation. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 3925–3937, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nRobert C Moore and William Lewis. 2010. Intelligent\nSelection of Language Model Training Data. In Pro-\nceedings of the ACL 2010 Conference Short Papers,\npages 220–224, Uppsala, Sweden. Association for\nComputational Linguistics.\nYasmin Moslem, Rejwanul Haque, John Kelleher, and\nAndy Way. 2022. Domain-Specific Text Generation\nfor Machine Translation. In Proceedings of the 15th\nbiennial conference of the Association for Machine\nTranslation in the Americas (Volume 1: Research\nTrack), pages 14–30, Orlando, USA. Association for\nMachine Translation in the Americas.\nYasmin Moslem, Rejwanul Haque, John D Kelleher,\nand Andy Way. 2023. Adaptive Machine Translation\nwith Large Language Models. In Proceedings of the\n24th Annual Conference of the European Association\nfor Machine Translation, pages 227–237, Tampere,\nFinland. European Association for Machine Transla-\ntion.\nPrashanth Nayak, Rejwanul Haque, John D Kelleher,\nand Andy Way. 2023. Instance-Based Domain Adap-\ntation for Improving Terminology Translation. In\nProceedings of Machine Translation Summit XIX: Re-\nsearch Track, pages 222–231, Macau SAR, China.\nAssociation for Machine Translation in the Americas.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems (NeurIPS 2022), pages 27730–\n27744, New Orleans, Louisiana, USA. Curran Asso-\nciates, Inc.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a Method for Automatic Eval-\nuation of Machine Translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow,\nRuxandra Cojocaru, Alessandro Cappelli, Hamza\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The RefinedWeb Dataset\nfor Falcon LLM: Outperforming Curated Corpora\nwith Web Data, and Web Data Only. arXiv preprint\narXiv:2306.01116 [cs.CL].\nAlberto Poncelas, Gideon Maillette de Buy Wenniger,\nand Andy Way. 2019. Adaptation of Machine Trans-\nlation Models with Back-Translated Data Using\nTransductive Data Selection Methods. In Proceed-\nings of the 20th International Conference on Compu-\ntational Linguistics and Intelligent Text Processing\nCICLing 2019: Computational Linguistics and Intel-\nligent Text Processing, pages 567–579, La Rochelle,\nFrance. Springer Nature Switzerland.\nMaja Popovi´c. 2017. chrF++: words helping charac-\nter n-grams. In Proceedings of the Second Confer-\nence on Machine Translation, pages 612–618, Copen-\nhagen, Denmark. Association for Computational Lin-\nguistics.\nMatt Post and David Vilar. 2018. Fast Lexically Con-\nstrained Decoding with Dynamic Beam Allocation\nfor Neural Machine Translation. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 1314–1324, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon\nLavie. 2020. COMET: A Neural Framework for MT\nEvaluation. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 2685–2702, Online. Association\nfor Computational Linguistics.\nNeha Sengupta, Sunil Kumar Sahu, Bokang Jia,\nSatheesh Katipomu, Haonan Li, Fajri Koto,\nOsama Mohammed Afzal, Samta Kamboj, Onkar\nPandit, Rahul Pal, Lalit Pradhan, Zain Muhammad\nMujahid, Massa Baali, Alham Fikri Aji, Zhengzhong\nLiu, Andy Hock, Andrew Feldman, Jonathan Lee,\nAndrew Jackson, Preslav Nakov, Timothy Baldwin,\n911\nand Eric Xing. 2023. Jais and Jais-chat: Arabic-\nCentric Foundation and Instruction-Tuned Open Gen-\nerative Large Language Models. arXiv preprint\narXiv:2308.16149 [cs.CL].\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving Neural Machine Translation Mod-\nels with Monolingual Data. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n86–96, Berlin, Germany. Association for Computa-\ntional Linguistics.\nOleh Shliazhko, Alena Fenogenova, Maria Tikhonova,\nVladislav Mikhailov, Anastasia Kozlova, and Tatiana\nShavrina. 2022. mGPT: Few-Shot Learners Go Mul-\ntilingual. arXiv preprint arXiv:2204.07580 [cs.CL].\nJörg Tiedemann and Santhosh Thottingal. 2020. OPUS-\nMT — Building open translation services for the\nWorld. In Proceedings of the 22nd Annual Confer-\nenec of the European Association for Machine Trans-\nlation (EAMT), Lisbon, Portugal.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schel-\nten, Ruan Silva, Eric Michael Smith, Ranjan Sub-\nramanian, Xiaoqing Ellen Tan, Binh Tang, Ross\nTaylor, Adina Williams, Jian Xiang Kuan, Puxin\nXu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, An-\ngela Fan, Melanie Kambadur, Sharan Narang, Aure-\nlien Rodriguez, Robert Stojnic, Sergey Edunov, and\nThomas Scialom. 2023. Llama 2: Open Founda-\ntion and Fine-Tuned Chat Models. arXiv preprint\narXiv:2307.09288 [cs.CL].\nMarcos Treviso, Ji-Ung Lee, Tianchu Ji, Betty van Aken,\nQingqing Cao, Manuel R Ciosici, Michael Hassid,\nKenneth Heafield, Sara Hooker, Colin Raffel, Pe-\ndro H Martins, André F T Martins, Jessica Zosa\nForde, Peter Milder, Edwin Simpson, Noam Slonim,\nJesse Dodge, Emma Strubell, Niranjan Balasubra-\nmanian, Leon Derczynski, Iryna Gurevych, and Roy\nSchwartz. 2023. Efficient methods for natural lan-\nguage processing: A survey.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In Advances in Neural Information Pro-\ncessing Systems (NIPS 2017), volume 30. Curran\nAssociates, Inc.\nDavid Vilar, Markus Freitag, Colin Cherry, Jiaming Luo,\nViresh Ratnakar, and George Foster. 2023. Prompt-\ning PaLM for Translation: Assessing Strategies and\nPerformance. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 15406–\n15427, Toronto, Canada. Association for Computa-\ntional Linguistics.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A\n6 Billion Parameter Autoregressive Language Model.\nGithub (mesh-transformer-jax).\nYinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo,\nJax Law, Noah Constant, Gustavo Hernandez Abrego,\nSteve Yuan, Chris Tar, Yun-Hsuan Sung, Brian\nStrope, and Ray Kurzweil. 2020. Multilingual Uni-\nversal Sentence Encoder for Semantic Retrieval. In\nProceedings of the 58th Annual Meeting of the Associ-\nation for Computational Linguistics: System Demon-\nstrations, pages 87–94, Online. Association for Com-\nputational Linguistics.",
  "topic": "Terminology",
  "concepts": [
    {
      "name": "Terminology",
      "score": 0.9297300577163696
    },
    {
      "name": "Computer science",
      "score": 0.8555096387863159
    },
    {
      "name": "Machine translation",
      "score": 0.7298739552497864
    },
    {
      "name": "Natural language processing",
      "score": 0.6778368949890137
    },
    {
      "name": "Task (project management)",
      "score": 0.6159197688102722
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6042928695678711
    },
    {
      "name": "Czech",
      "score": 0.5931357145309448
    },
    {
      "name": "Process (computing)",
      "score": 0.5831440091133118
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4962154030799866
    },
    {
      "name": "Language model",
      "score": 0.45706501603126526
    },
    {
      "name": "Linguistics",
      "score": 0.19411736726760864
    },
    {
      "name": "Programming language",
      "score": 0.17447757720947266
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210121390",
      "name": "Thomson Reuters (Switzerland)",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I41832843",
      "name": "University of Tabriz",
      "country": "IR"
    },
    {
      "id": "https://openalex.org/I157286207",
      "name": "National University of Ireland, Maynooth",
      "country": "IE"
    },
    {
      "id": "https://openalex.org/I4387156359",
      "name": "South East Technological University",
      "country": null
    },
    {
      "id": "https://openalex.org/I70277191",
      "name": "Institute of Technology Carlow",
      "country": "IE"
    },
    {
      "id": "https://openalex.org/I42934936",
      "name": "Dublin City University",
      "country": "IE"
    }
  ]
}