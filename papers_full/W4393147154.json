{
  "title": "LLM vs Small Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model",
  "url": "https://openalex.org/W4393147154",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2164292348",
      "name": "Linmei Hu",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2124953827",
      "name": "Hongyu He",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A5035159127",
      "name": "Duokang Wang",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2805656706",
      "name": "Ziwang Zhao",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2122925794",
      "name": "Yingxia Shao",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2151954441",
      "name": "Liqiang Nie",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2164292348",
      "name": "Linmei Hu",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2124953827",
      "name": "Hongyu He",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A5035159127",
      "name": "Duokang Wang",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2805656706",
      "name": "Ziwang Zhao",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2122925794",
      "name": "Yingxia Shao",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2151954441",
      "name": "Liqiang Nie",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2028826002",
    "https://openalex.org/W2552815284",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4372270660",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4311638661",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W3015987295",
    "https://openalex.org/W2138621090",
    "https://openalex.org/W6845947096",
    "https://openalex.org/W4366602868",
    "https://openalex.org/W3035156228",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2119595472",
    "https://openalex.org/W6787318496",
    "https://openalex.org/W2896018557",
    "https://openalex.org/W6745173100",
    "https://openalex.org/W2140910804",
    "https://openalex.org/W6799802702",
    "https://openalex.org/W4312091890",
    "https://openalex.org/W3177080218",
    "https://openalex.org/W2607892599",
    "https://openalex.org/W2806246579",
    "https://openalex.org/W3164054899",
    "https://openalex.org/W3173710861",
    "https://openalex.org/W4310878657",
    "https://openalex.org/W3171441177",
    "https://openalex.org/W1973871257",
    "https://openalex.org/W1980867644",
    "https://openalex.org/W4285602050",
    "https://openalex.org/W4306295157",
    "https://openalex.org/W3113769163",
    "https://openalex.org/W3176540316",
    "https://openalex.org/W2991538160",
    "https://openalex.org/W3169322305",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2763585929",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4385570441",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W2960748659",
    "https://openalex.org/W3173644040",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4320858112",
    "https://openalex.org/W4385571011",
    "https://openalex.org/W3175362188",
    "https://openalex.org/W3196820561",
    "https://openalex.org/W3182414949",
    "https://openalex.org/W4383987495",
    "https://openalex.org/W4385572035",
    "https://openalex.org/W4364385323"
  ],
  "abstract": "Personality detection aims to detect one's personality traits underlying in social media posts. One challenge of this task is the scarcity of ground-truth personality traits which are collected from self-report questionnaires. Most existing methods learn post features directly by fine-tuning the pre-trained language models under the supervision of limited personality labels. This leads to inferior quality of post features and consequently affects the performance. In addition, they treat personality traits as one-hot classification labels, overlooking the semantic information within them. In this paper, we propose a large language model (LLM) based text augmentation enhanced personality detection model, which distills the LLM's knowledge to enhance the small model for personality detection, even when the LLM fails in this task. Specifically, we enable LLM to generate post analyses (augmentations) from the aspects of semantic, sentiment, and linguistic, which are critical for personality detection. By using contrastive learning to pull them together in the embedding space, the post encoder can better capture the psycho-linguistic information within the post representations, thus improving personality detection. Furthermore, we utilize the LLM to enrich the information of personality labels for enhancing the detection performance. Experimental results on the benchmark datasets demonstrate that our model outperforms the state-of-the-art methods on personality detection.",
  "full_text": "LLM vs Small Model? Large Language Model Based Text Augmentation\nEnhanced Personality Detection Model\nLinmei Hu1*, Hongyu He2, Duokang Wang2, Ziwang Zhao2, Yingxia Shao2, Liqiang Nie3\n1School of Computer Science and Technology, Beijing Institute of Technology\n2School of Computer Science, Beijing University of Posts and Telecommunications\n3School of Computer Science and Technology, Harbin Institute of Technology (Shenzhen)\nhulinmei@bit.edu.cn, {hehongyu, wangduokang, zhaoziwang, shaoyx}@bupt.edu.cn, nieliqiang@gmail.com\nAbstract\nPersonality detection aims to detect one’s personality traits\nunderlying in social media posts. One challenge of this task is\nthe scarcity of ground-truth personality traits which are col-\nlected from self-report questionnaires. Most existing meth-\nods learn post features directly by fine-tuning the pre-trained\nlanguage models under the supervision of limited personality\nlabels. This leads to inferior quality of post features and con-\nsequently affects the performance. In addition, they treat per-\nsonality traits as one-hot classification labels, overlooking the\nsemantic information within them. In this paper, we propose\na large language model (LLM) based text augmentation en-\nhanced personality detection model, which distills the LLM’s\nknowledge to enhance the small model for personality detec-\ntion, even when the LLM fails in this task. Specifically, we\nenable LLM to generate post analyses (augmentations) from\nthe aspects of semantic, sentiment, and linguistic, which are\ncritical for personality detection. By using contrastive learn-\ning to pull them together in the embedding space, the post\nencoder can better capture the psycho-linguistic information\nwithin the post representations, thus improving personality\ndetection. Furthermore, we utilize the LLM to enrich the in-\nformation of personality labels for enhancing the detection\nperformance. Experimental results on the benchmark datasets\ndemonstrate that our model outperforms the state-of-the-art\nmethods on personality detection.\nIntroduction\nPersonality is a combination of a person’s internal charac-\nteristics, which can be reflected in their behavior (Stajner\nand Yenikent 2020). Personality traits are defined by long-\nestablished personality theories such as the Myers-Briggs\nType Indicator (MBTI) taxonomy (Myers-Briggs 1991).\nPersonality detection aims to detect one’s personality traits\nunderlying in social media posts. This emerging task in com-\nputational psycho-linguistics can provide specific personal-\nity information about the person and has been utilized in var-\nious applications, such as dialogue systems (Wen et al. 2021;\nYang, Chen, and Narasimhan 2021) and psychological treat-\nments (Bagby et al. 2016).\nTraditional personality detection methods mostly rely on\nmanually designed features, such as Linguistic Inquiry and\n*Corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nWord Count (LIWC) (Tausczik and Pennebaker 2010), that\nbring psychology theories and linguistic methods together.\nRecent works use end-to-end deep neural network to ob-\ntain text representations automatically in a data-driven man-\nner (Jiang, Zhang, and Choi 2020; Keh and Cheng 2019;\nLynn, Balasubramanian, and Schwartz 2020; Gjurkovi ´c\net al. 2021). However, extracting effective personality in-\nformation from online posts is a non-trivial task. One main\nchallenge is the scarcity of ground-truth personality traits,\nas they are collected from self-report questionnaires, which\nis time-consuming and often raises concerns related to user\nprivacy. To overcome the issue, pre-trained language mod-\nels have been applied to learn post representations (Yang\net al. 2021b; Xue et al. 2018; Yang et al. 2021b). To fur-\nther improve the post representations, TrigNet (Yang et al.\n2021b) considers the contextual information of a user’s posts\nas well as the external psycho-linguistic knowledge from\nLIWC (Xue et al. 2018). However, the learned post repre-\nsentations are still unsatisfactory, resulting in inferior per-\nformance. In addition, they treat personality traits as one-hot\nclassification labels, overlooking the semantic information\nwithin them.\nRecently, Large language models (LLMs) (Brown et al.\n2020; Sun et al. 2021; Ouyang et al. 2022; Zhang et al. 2022)\nhave demonstrated significant capability in various natural\nlanguage processing tasks under a generative format in zero-\nshot or few-shot scenarios. However, it demonstrates unsat-\nisfactory performance in personality detection compared to\ntask-specific small model (Ji et al. 2023). Inferring people’s\npersonality traits from online posts is a complex and difficult\ntask. Despite the ineffectiveness of LLMs in this task, pre-\nvious studies have demonstrated that LLMs exhibit strong\nlanguage abilities, such as text comprehension, summariza-\ntion, and sentiment analysis (Zhang et al. 2023; Wang et al.\n2023b; Hsieh et al. 2023), which can be used to distill useful\nknowledge for enhancing small models. On the one hand,\nwe can leverage LLMs to generate post analyses (augmenta-\ntions) from the aspects of semantic, sentiment, and linguis-\ntic, which are key factors for personality detection (Fang\net al. 2023). In this way, we can use the augmented post\ninformation to learn better post embeddings for personal-\nity detection. On the other hand, we can also use LLMs to\ngenerate explanations of the complex personality labels to\nenrich the label information for improving personality de-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18234\ntection.\nIn this paper, we propose a large language model based\ntext augmentation enhanced personality detection model,\nwhich distills useful knowledge from LLMs to enhance a\nsmall model’s personality detection capabilities, both on\nthe data side and the label side. On the data side, we fol-\nlow the methodology of contrastive sentence representation\nworks (Gao, Yao, and Chen 2021; Cheng et al. 2023a),\nusing the LLM to generate knowledgeable post augmen-\ntations from semantic, sentiment and linguistic aspects to\nprovide personality-related knowledge. With the post aug-\nmentations, our contrastive post encoder can capture more\npsycho-linguistic information within the post representa-\ntions. It is noteworthy that this method does not introduce\nextra costs during inference. On the label side, considering\nthe personality labels are highly complex in their implica-\ntions, which makes it difficult for detection, we also use the\nLLMs to generate additional explanations for the labels to\nenrich the label information for improving the detection per-\nformance.\nIn summary, our main contributions are as follows:\n• We propose a novel LLM based text augmentation en-\nhanced personality detection model, which distills useful\nknowledge from LLMs to the small model, alleviating\nthe issue of data scarcity and improving personality de-\ntection.\n• Our model enables LLM to generate post augmentations\nfor contrastive post representation learning from three\nspecially designed aspects: semantic, sentiment, and lin-\nguistic, which are key factors for personality detection.\nAdditionally, we use LLM to generate explanations of\nthe personality labels for further improving the detection\nperformance.\n• Experimental results on benchmark datasets have\ndemonstrated that our model outperforms the state-of-art\nbaselines, which shows the effectiveness of our method.\nRelated Work\nPersonality Detection\nTraditional personality detection relies on manual statistical\nfeature engineering (Yarkoni 2010; Schwartz et al. 2013),\nsuch as extracting psycho-linguistic features from Linguis-\ntic Inquiry and Word Count (LIWC) (Tausczik and Pen-\nnebaker 2010) or statistical text features from the bag-of-\nwords(Zhang, Jin, and Zhou 2010) model. As deep learn-\ning rapidly advances, a variety of Deep Neural Networks\n(DNNs) have been employed for personality detection tasks,\nresulting in significant success, including CNN (Xue et al.\n2018), LSTM (Tandera et al. 2017), GRU (Lynn, Balasub-\nramanian, and Schwartz 2020), etc. Recently, personality\ndetection has benefited from pre-trained language models.\nJiang, Zhang, and Choi (2020) achieves promising perfor-\nmance by simply concatenating all the utterances from a sin-\ngle user into a document and encoding it with BERT (Devlin\net al. 2019) and RoBERTa (Liu et al. 2019). Some works\nimprove it by leveraging the contextual information and ex-\nternal psycho-linguistic knowledge from LIWC (Yang et al.\n2021a,b; Zhu et al. 2022). For example, Transformer-MD\n(Yang et al. 2021a) stores posts’ hidden states in the mem-\nory of Transformer-XL (Dai et al. 2019) in order to avoid in-\ntroducing post-order bias. TrigNet (Yang et al. 2021b) con-\nstructs a heterogeneous graph between posts for each user\nbased on the psycho-linguistic knowledge in LIWC and ag-\ngregates useful information with a GAT. D-DGCN (Yang\net al. 2023) builds a dynamic graph, which enables the\nmodel to learn the connections between the posts, and em-\nploys DGCN to integrate the information. Although these\nmethods achieve promising performance, they still suffer\nfrom the limited supervision of insufficient personality la-\nbels, which leads to inferior quality of post embeddings that\nconsequently affect the model performance.\nContrastive Sentence Representation Learning\nContrastive learning was initially proposed by Hadsell,\nChopra, and LeCun (2006) and has been widely used for\nself-supervised representation learning in various domains.\nIn the realm of NLP, a fundamental application of con-\ntrastive learning lies in sentence representation learning, ap-\nplicable to both self-supervised and supervised scenarios.\nYan et al. (2021) and Gao, Yao, and Chen (2021) propose\ndifferent data augmentation strategies for contrastive learn-\ning using unlabeled data. In the supervised contrastive learn-\ning scenario, SimCSE (Gao, Yao, and Chen 2021) demon-\nstrated that supervised Natural Language Inference (NLI)\ndatasets (Bowman et al. 2015; Williams, Nangia, and Bow-\nman 2018) are effective for learning sentence embeddings\nby taking sentences with entailment labels as positive sam-\nples. Capitalizing on the strong capability of large language\nmodels, CLAIF (Cheng et al. 2023b) utilizes LLM to gener-\nate similar text and similarity score, using them as positive\nsamples and weights in info-NCE loss, thereby achieving\nimproved performance. Different from these approaches, we\ngenerate data augmentations from the LLM, which contains\ntask-specific information, to enhance personality detection\nperformance.\nKnowledge Distillation from Large Language\nModels\nKnowledge distillation is used to transfer knowledge from\na larger, more powerful teacher model, into smaller stu-\ndent models, enhancing their performance in practical ap-\nplications (Hinton, Vinyals, and Dean 2015; Iliopoulos et al.\n2022; Wang et al. 2021). Recently, depending on the strong\nlanguage abilities of LLMs, generating additional training\ndata from LLMs for improving smaller models has become\na new knowledge distillation trend (Wang et al. 2023a). For\nexample, Self-instruct (Wang et al. 2023a) distills instruc-\ntional data to enhance the instruction-following capabilities\nof pre-trained language models. Hsieh et al. (2023) gen-\nerates rational text from LLMs to enhance the inference\nabilities of smaller models. Similarly, CLAIF (Cheng et al.\n2023b) leverages LLMs to produce similar texts, aiming to\nlearn a better sentence representations. In this paper, we pro-\npose to leverage LLMs to generate post analyses (augmen-\ntations) from specially-designed perspectives to enhance the\nsmall model for personality detection.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18235\nFigure 1: An example of personality detection using the\nLLM: The result provided by the LLM is ISFJ, while the\nactual ground truth is ENFP.\nMethod\nPersonality detection can be defined as a multi-document\nmulti-label classification task (Yang et al. 2021b, 2023).\nFormally, given a set P = {p1, p2 . . . pn} of N posts\nfrom a user, where pi = [wi1, wi2, . . . , wim] is i-th post\nwith m tokens. This task aims to predict t-dimensional\npersonality traits from the trait-specific label space Y =\n{y1, y2, . . . ,yT } based for one user based on P. For\nMBTI taxonomy, T = 4 and yt is one-hot vector. In\nthis paper, we instruct the LLM to generate summariza-\ntions from semantical, sentimental and linguistic analysis\nfor post augmentation. The augmented data represented as\nX = {P, Ps, Pe, Pl}, where Ps, Pe, Pl correspond to the\nanalysis texts for semantic, sentiment and linguistic, respec-\ntively. For the personality labels, we also utilize the LLM\nto generate explanations of the labels from semantic, sen-\ntiment and linguistic perspectives, thereby enriching the la-\nbel information. For dimension t, the label descriptions are\nrepresented as ˆyt = {Lyt,0, Lyt,1}, where each Lyt,j =\n{ls\nyt,j, le\nyt,j, ll\nyt,j}, corresponding to the semantic, sentiment\nand linguistic descriptions.\nAnalysis of LLM Performance on Personality\nDetection\nLLMs show strong capabilities in many downstream tasks.\nHowever, their performance on the personality detection\ntask is unsatisfying, as discussed in Section 4.6. Figure 1\nshows a failure case of LLMs, where it wrongly classi-\nfied ENFP as ISFJ. When we prompt the LLM to elucidate\nthe analysis process underlying its classification, Figure 1\nshows that the LLM primarily infers the personality traits\nbased on only the post semantics. However, previous stud-\nies (Tausczik and Pennebaker 2010; Stajner and Yenikent\n2020) demonstrate that the way people communicate and\ntheir sentiments often reveal more about their psychologi-\ncal state than the semantics of their communication content.\nLLM fails to capture the sentiment and linguistic patterns\nfor personality detection.\nGenerating Knowledgeable Post Augmentations\nfrom LLMs\nPersonality detection is a complex and difficult task, since\nthe personality traits are latent theoretical variables that can-\nnot be directly or objectively observed (Fang et al. 2023).\nPrevious research has demonstrated a strong link between\npersonality traits and a person’s sentiments, words, and\nopinions (Kishima et al. 2021; Johnson et al. 2023). There-\nfore, despite of the ineffectiveness of the LLM in this task,\nwe propose to fully utilize the LLM’s abilities to distill\npersonality-related knowledge, aiming to enhance smaller\nmodels for improving personality detection performance.\nSpecifically, we empirically instruct LLM (chatGPT) to\ngenerate post analyses from three aspects: semantic, senti-\nment, and linguistic, serving as data augmentations to origi-\nnal post. One analyses example is shown in Figure 2 gener-\nated with the instruction as follows:\nYour task is to analyze the characteristics of a user based\non a piece of text published by the user on the Internet. You\nare required to analyze it from the perspectives of semantic,\nsentiments, and linguistics. Note that if the text is incomplete\nand ends with an ellipsis, it may have been truncated due to\nexternal reasons, in which case you should ignore it. post:. . .\nContrastive Post Encoder\nContrastive learning aims to learn efficient representations\nby aligning semantically similar entities closer and distanc-\ning the dissimilar ones (Hadsell, Chopra, and LeCun 2006;\nGao, Yao, and Chen 2021). One key component of con-\ntrastive learning is the selection of positive pairs.\nWith the obtained post augmentations from the LLM\nas positive samples, we learn the post embeddings with\nthe contrastive post encoder that able to capture more\npersonality-related feature.\nFormally, for i-th post pi and it’s analysis from three as-\npect Ppos = {ps\ni, pe\ni , pl\ni}, we use the final hidden state of\n”[CLS]” as the sentence representation:\nhi = BERT (pi) ∈ R1×d, (1)\nh+\ni = BERT\n\u0000\np+\ni\n\u0001\n, where p+\ni ∈ {ps\ni, pe\ni , pl\ni}. (2)\nDue to the distribution difference between the original\npost text and the analysis text, we add an extra MLP same to\noriginal BERT implementation as a projection head to miti-\ngate this discrepancy:\nzi = δ (W hi + b) , (3)\nz+\ni = δ\n\u0000\nW h+\ni + b\n\u0001\n, (4)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18236\nFigure 2: An overview of our TAE.\nwhere δ is Tanh function. We adopt the contrastive frame-\nwork presented by Chen et al. (2020), utilizing a post-wise\ninfo-NCE loss with in-batch negatives, for i-th post in mini-\nbatch:\nLcl = 1\nM\nMX\ni=1\nℓcl\ni (5)\nℓcl\ni = −log\nX\n(zi,z+\ni )∈Ppos\nesim(zi,z+\ni )/τ\nPM\nj=1\nP\n(zj,z+\nj )∈Ppos\nesim(zi,z+\nj )/τ\n,\nwhere M is total number of post in mini-batch, τ is tem-\nperature hyperparameter andsim (z1, z2) is cosine similarity\nz⊤\n1 z2\n∥z1∥·∥z2∥.\nAfter obtaining posts representation of a user, we simply\nuse average pooling method to produce the user representa-\ntion:\nu = mean ([h1, h2, . . . ,hN]) . (6)\nLLM Enriching Label Information\nPrevious methods directly use one-hot label for personality\ndetection, overlooking the information in personality traits.\nSince personality traits are complicated to comprehension,\nwe also turn to the LLM to generate explanations of each\ntrait from semantic, sentiment and linguistic perspectives,\nas the same to post augmentations.\nSpecifically, given a personality label Lyt,j, containing\nthree descriptions from semantic, sentiment and linguistic\naspects Lyt,j = {ls\nyt,j, le\nyt,j, ll\nyt,j}, the label representation\nis obtained by:\nvyt,j = mean ([BERT (lyt,j)]) , where lyt,j ∈ Lyt,j, (7)\nwhere we also use the embedding of ”[CLS]” in last layer as\nthe sentence embedding.\nBased on label representations, we generate soft labels to\novercome the over-confident issue, which is primarily at-\ntributed to the information loss in the MBTI taxonomy’s\nbinary classification, which neglects the position on the\nscale (Stajner and Yenikent 2020). Dataset label noise in-\ntroduced by the questionnaire measurement errors (Anusic\net al. 2009) could also lead to the over-confident issue. We\nleverage the generated label information to assign a soft la-\nbel to a user based on the similarity between the user em-\nbedding and the label embedding.\nFormally, to generate soft label, we first calculate the\ndimension-wise label similarity distribution with cosine\nsimilarity and softmax function. Then, we combine the orig-\ninal one-hot vector with a controlling parameterα and apply\nan additional softmax function:\nys\nt = softmax (sim (u, Vyt )) , (8)\nyc\nt = softmax (αyt + ys\nt ) , (9)\nIn this way, we can assign a softer label instead of one-hot\nlabel when a user is relatively neutral within a dimension,\nimproving model’s generalization ability.\nFinally, we employ T softmax-normalized linear trans-\nformations to predict personality traits, and apply a KL-\ndivergence as the loss function:\nˆyt = softmax\n\u0000\nuW t\nu + bt\nu\n\u0001\n, (10)\nLdet = − 1\nB\nBX\ni=1\nℓdet\ni ,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18237\nDataset Types Train Validation Test\nKaggle\nI / E 4011/1194 1326/409 1339 /396\nS / N 727/4478 222/1513 248/1487\nT / F 2410/2795 791/944 780/955\nP / J 3096/2109 1063/672 1082 /653\nPandora\nI / E 4278/1162 1427/386 1437 /377\nS / N 610/4830 208/1605 210/1604\nT / F 3549/1891 1120/693 1182 /632\nP / J 3211/2229 1043/770 1056 /758\nTable 1: Statistics of the Kaggle and Pandora datasets.\nℓdet\ni =\nTX\nt=1\nKL-divergence\n\u0000\nyc, ˆyt\u0001\n=\nTX\nt=1\nX\nj=0,1\nyc\nj log\n \nyc\nj\nˆyt\nj\n! (11)\nwhere B denotes number of samples in batch. Overall, the\nwhole training objective of TAE is formulated as follows:\nL = Ldet + λLcl, (12)\nwhere λ is a trad-off parameter to balance the two losses.\nExperiments\nDatasets\nFollowing previous studies (Yang et al. 2021a,b, 2023), we\nchoose two widely used datasets in personality detection,\nKaggle1 and Pandora 2 (Gjurkovi´c et al. 2021). The Kag-\ngle dataset is collected from PersonalityCafe 3, a platform\nwhere people share their personality types and engage in\ndaily communications. This dataset contains a total of 8675\nusers, with each user contributing 45-50 posts. Pandora is\nanother dataset collected from Reddit4, where personality la-\nbels are extracted from users’ self-introductions that contain\nMBTI types. This dataset contains dozens to hundreds of\nposts from each of the 9067 users. Both datasets are based on\nthe MBTI taxonomy, which divides people’s personality into\nfour dimensions, each containing two aspects: Introversion\nvs. Extroversion (I vs. E), Sensing vs. iNtuition (S vs. N),\nThinking vs. Feeling (T vs. F), and Perception vs. Judging\n(P vs. J). Since the two datasets are severely imbalanced,\nwe employ the Macro-F1 metric, and use average Macro-F1\nfor overall performance. We adopt the same data division as\nin (Yang et al. 2023), which shuffles the datasets and splits\nthem in a 60-20-20 proportion for training, validation, and\ntesting, respectively. Table 1 shows the statistics of the two\ndatasets.\nBaselines\nWe compare our model with several baselines as follows.\n1https://www.kaggle.com/datasnaek/mbti-type\n2https://psy.takelab.fer.hr/datasets/all\n3http://personalitycafe.com/forum\n4https://www.reddit.com\nSVM (Cui and Qi 2017) and XGBoost (Tadesse et al.\n2018): These methods concatenate all the posts of a user\ninto a document first, and then utilize SVM or XGBoost\nfor classification based on features extracted using bag-of-\nwords models.\nBiLSTM (Tandera et al. 2017): Glove is employed for\ngenerating word embeddings. Subsequently, a Bi-directional\nLSTM is used to encode each post, with the averaged post\nrepresentation serving as the user representation for person-\nality detection.\nBERTconcat (Jiang, Zhang, and Choi 2019): It concate-\nnates a user’s posts into an extended document and then em-\nploys BERT to encode this composite text for user represen-\ntation.\nBERTmean (Keh and Cheng 2019): It uses BERT to en-\ncode each post individually, extracting the CLS embedding\nfor the post representation. Then, it employs mean pooling\nto derive the user representation.\nAttRCNN (Xue et al. 2018): It adopts a hierarchical\nstructure, in which a variant of Inception (Szegedy et al.,\n2017) is utilized to encode each post and a CNN-based ag-\ngregator is employed to obtain the user representation. Be-\nsides, it considers psycho-linguistic knowledge by concate-\nnating the LIWC features with the user representation.\nSN+Attn (Lynn, Balasubramanian, and Schwartz 2020):\nIt is also a hierarchical network that employs GRU and atten-\ntion mechanisms to encode sequences from both the word-\nlevel and post-level for user representation, using a pre-\ntrained 200-dimensional word2vec model (Mikolov et al.,\n2013) for word embeddings.\nTransformer-MD (Yang et al. 2021a): Transformer-MD\ndraws inspiration from the Transformer XL, first employing\na low-level encoder to individually encode each post, storing\nthe CLS embeddings in memory. Then, a high-level encoder\naggregates information from all other posts to further encode\nthe post.\nTrigNet (Yang et al. 2021b): TrigNet constructs a tri-\ngraph for each user consisting of posts, words, and word cat-\negories, based on LIWC dictionary. It then employs a mod-\nified GAT to encode the graph and uses average pooling to\nobtain a user representation.\nD-DGCN (Yang et al. 2023): DDGCN employs a dy-\nnamic graph to model a user’s posts, allowing the model to\nlearn the connections between posts autonomously, and then\nuses DGCN to encode the graph and obtains a user represen-\ntation. It has a variant D-DGCN+ℓ0 representing adding ℓ0\nnorm with Hard Concrete distribution.\nChatGPT5: We applied the ’gpt-3.5-turbo-0301’ version\nof ChatGPT, and set the temperature to 0, making the outputs\nmostly deterministic for the identical inputs.\nImplementation Details\nAll the deep learning models are implemented in PyTorch,\nand the optimizer used is Adam (Kingma and Ba 2014). The\nlearning rate for the pre-trained post encoder is set to 1e-\n5, and for other parameters is set to 1e-3. We employed\n’bert-base-uncased’ from BERT as our post encoder. The\n5https://chat.openai.com/\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18238\nMethods Kaggle Pandora\nI/E S/N T/F P/J Average I/E S/N T/F P/J Average\nSVM 53.34 47.75 76.72 63.03 60.21 44.74 46.92 64.62 56.32 53.15\nXGBoost 56.67 52.85 75.42 65.94 62.72 45.99 48.93 63.51 55.55 53.50\nBiLSTM 57.82 57.87 69.97 57.01 60.67 48.01 52.01 63.48 56.21 54.93\nBERTconcat 58.33 53.88 69.36 60.88 60.61 54.22 49.15 58.31 53.14 53.71\nBERTmean 64.65 57.12 77.95 65.25 66.24 56.60 48.71 64.70 56.07 56.52\nAttRCNN 59.74 64.08 78.77 66.44 67.25 48.55 56.19 64.39 57.26 56.60\nSN+Attn 65.43 62.15 78.05 63.92 67.39 56.98 54.78 60.95 54.81 56.88\nTransformer-MD 66.08 69.10 79.19 67.50 70.47 55.26 58.77 69.26 60.90 61.05\nTrigNet 69.54 67.17 79.06 67.69 70.86 56.69 55.57 66.38 57.27 58.98\nD-DGCN 68.41 65.66 79.56 67.22 70.21 61.55 55.46 71.07 59.96 62.01\nD-DGCN+ℓ0 69.52 67.19 80.53 68.16 71.35 59.98 55.52 70.53 59.56 61.40\nChatGPT 65.86 51.69 78.60 63.93 66.89 55.52 49.79 71.25 60.51 59.27\nTAE(our) 70.90 66.21 81.17 70.20 72.07 62.57 61.01 69.28 59.34 63.05\nTable 2: Overall results of our TAE and baseline models in Macro-F1 (%) score.\nmini-batch size is set to 8. The temperature τ is set to 0.07\nand trade-off parameter λ is set to 1.The controlling param-\neter α is set to 4. Following previous works (Yang et al.\n2021b, 2023, 2021a), We limit each user to a maximum of 50\nposts, and limit each post and analysis text in both datasets\nto a maximum length of 70 words. Additionally, we re-\nplace words that match any personality label with ⟨type⟩ to\navoid information leaks (Yang et al. 2021b). For the LLM, in\nconsideration of cost and efficiency, we use ’gpt-3.5-turbo-\n0301’ model to generate post augmentations.\nOverall Results\nThe overall results are presented in Table 2. We can find\nthat the proposed TAE consistently outperforms all the\nbaselines on Macro-F1 scores. Compared to the best base-\nlines D-DGCN (71.35% on average Macro-F1) and D-\nDCGN+l0 (62.01%) respectively on Kaggle dataset and\nPandora dataset, our model improves them by 1.01% and\n1.68%, respectively. The result demonstrates the superiority\nof our model in personality detection. We believe the reasons\nare two-fold: (1) Our model TAE benefits from the post aug-\nmentations generated by the LLM, enabling the contrastive\npost encoder to extract information that is more conducive\nto personality detection. (2) The generated additional expla-\nnations of personality labels effectively help in accomplish-\ning the detection task. We can also find that compared to\nBERTmean, our model has a marked improvement, with re-\nspective gains (5.83% on the Kaggle dataset and 6.53% on\nthe Pandora dataset). This demonstrates that data augmenta-\ntions from LLM in data-scarce situations is highly advan-\ntageous. Finally, we can observe that methods leveraging\nexternal psycho-linguistic knowledge from LIWC, such as\nAttRCNN and TrigNet achieve relatively good results, vali-\ndating the effectiveness of introducing external knowledge.\nFurthermore, our model achieves the best performance, in-\ndicating that the generated post analyses and label explana-\ntions from large language models can provide effective in-\nformation for personality detection.\nMethods Kaggle\nI/E S/N\nT/F P/J Av\nerage\nTAEw/o\nsemantic 71.24 66.34\n80.61 67.43 71.40\nTAEw/o\nsentiment 70.14 65.29\n80.03 69.55 71.25\nTAEw/o\nlinguistic 70.34 65.69\n78.89 69.71 71.07\nTAEw/o\nlabel 70.57 65.89\n81.90 69.78 72.02\nTAEw/o\nAll 64.65 57.12\n77.95 65.25 66.24\nTAEConcat 66.60 64.28\n78.19 62.55 67.91\nTAEWS 64.63 62.62\n77.68 64.30 67.31\nTAE 70.90 66.21\n81.17 70.20 72.07\nTable 3: Results of ablation study on Macro-F1 on the Kag-\ngle dataset.\nAblation Study\nTo verify the importance of each component in our TAE\nmodel, we conduct an ablation study on the Kaggle dataset.\nFirst, we analyze the contributions of the post augmentations\nfrom each aspect. As we can see from Table 3, among the\nthree aspects, linguistic augmentation proves to be the most\nimportant one in our method, as the average Macro-F1 score\ndeclines most largely when it is removed. Furthermore, the\nsemantic information is the least influential augmentation.\nThis indicates that the semantic information is relatively less\nimportant for personality detection, which is consistent with\nthe observations in previous works (Yang et al. 2021b; Sta-\njner and Yenikent 2020). When removing the LLM based\nlabel information enrichment, the performance of the model\nslightly decreases. The ablation studies demonstrate that our\nmodel benefits from the LLM generated post augmentations\nfrom semantic, sentiment and linguistic aspects, as well as\nthe LLM based label information enrichment.\nTo further explore the effectiveness of using the LLM-\ngenerated analysis texts as data augmentations for con-\ntrastive post representation learning, we conducted ablation\nexperiments. We compared this approach with the variants\nthat directly use these LLM-based analysis texts (post aug-\nmentations) as additional input. TAEConcat first encodes post\nand analyses respectively and then concatenates their em-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18239\nMethods Kaggle\nI/E S/N\nT/F P/J Average\nChatGPT 65.86 51.69\n78.60 63.93 66.89\nChatGPTcot 65.13 60.35\n75.73 59.30 65.12\nChatGPT3 shot 70.61 58.35\n76.58 65.43 67.74\nTAE 70.90 66.21\n81.17 70.20 72.07\nTable 4: ChatGPT performances on kaggle testing set.\nbeddings. TAEWS denotes using weighted sum as the pool-\ning method of the post features and post augmentation fea-\ntures. TAEw/o All denotes removing all components in TAE\nwhich is an original BERT with mean pooling. Table 4\nshows their performance on the Kaggle dataset, we can ob-\nserve that models taking the post augmentations as extra in-\nput (TAEConcat, TAEWS) outperform the baseline TAEw/o All,\nbut they are all inferior to our TAE which is a contrastive\nmodel. This demonstrates the effectiveness of data augmen-\ntation and the contrastive learning paradigm. Moreover, the\nmodels taking analysis texts as extra input requires the LLM\nto generate analysis texts during inference stage, which is\ncostly. We also visualize the weights learned by TAE WS in\nFigure 3. It shows that linguistic analysis is more important,\nwhich is consistent with our prevous observation.\nLLM Performance\nTo analyze LLM’s performance in the personality detection\ntask, we conducted a series of experiments using ChatGPT.\nWe assessed LLM’s capabilities under three settings: zero-\nshot, CoT, and few-shot, using the Kaggle testing set. For the\nfew-shot setting, we randomly selected 3 examples from the\ntraining set. Specifically, we employed the ’gpt-3.5-turbo-\n0301’ version but switched to ’gpt-3.5-turbo-16k-0613’ for\nthe few-shot setting, as the input length exceeded the limit.\nAs shown in Table 4, the performance of ChatGPT is com-\nparable to that of a small model that has been fine-tuned\n(BERTmean) for a specific task on Macro-F1 metrics. Under\nthe CoT setting and few-shot setting, the classification per-\nformance slightly decreased. This indicates that the LLM’s\nreasoning ability fails in personality detection. Directly ap-\nplying LLM to the personality detection is not appropriate.\nThus, in this paper, we consider leveraging the LLM to en-\nhance the small model for personality detection by distilling\nuseful knowledge from LLM to the small model.\nEffect of Trade-Off Parameter\nFor trade-off parameter λ in TAE, we searched in\n{0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4}. Figure 4 demonstrates how the\nmodel performance changes with theλ on the validation sets\nof Kaggle and Pandora datasets. We can observe that the\nMacro-F1 value first grows and reaches the highest value at\nλ=1 while it begins to drop when λ is larger than 1. This\nmay be because that initially with the value of λ increases,\nTAE can benefit more from the contrastive learning with the\nLLM genrated post aumgmentations, which helps to learn\nbetter post representations. However, if λ is set too large\n(> 1), the contrastive signal out weight the detection loss.\nOverall, λ=1 can reach best balance between detection loss\nFigure 3: Visualization of learned weight of post and analy-\nses in TAEWS.\nFigure 4: Performance curves for different trade-off param-\neter.\nand contrastive loss on both Kaggle and Pandora datasets.\nConclusion\nIn this paper, we propose a large language model (LLM)\nbased text augmentation enhanced personality detection\nmodel, which distills the useful knowledge from the LLM to\naddress the data scarcity issue faced by small models in per-\nsonality detection, even when the LLM itself struggles with\nthe task. By leveraging the LLM’s abilities in text compre-\nhension, summarization, and sentiment analysis, we instruct\nit to generate post analyses from three specially-designed\nperspectives: semantic, sentiment, and linguistic, which play\na critical role for personality detection. Taking these anal-\nyses as positive samples and using contrastive learning to\npull them together in the embedding space enables the post\nencoder in the small model to better capture the psycho-\nlinguistic information within the post representations, thus\nimproving personality detection. Furthermore, we utilize the\nLLM to generate label descriptions, enriching the semantics\nof personality labels and utilize label information to generate\nsoft labels to overcome the over-confidence issue, enhancing\nmodels generalization ability. Experimental results on two\nbenchmark datasets demonstrate that our model outperforms\nthe state-of-the-art methods on personality detection.\nIn future work, we will explore how to combine the ad-\nvantages of the existing knowledge graphs and the LLM in\nimproving personality detection.\nAcknowledgments\nThis work was supported by the National Science Founda-\ntion of China (No. 62276029), and CCF-Zhipu. AI Large\nModel Fund (No. 202217).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18240\nReferences\nAnusic, I.; Schimmack, U.; Pinkus, R.; and Lockwood, P.\n2009. The Nature and Structure of Correlations Among Big\nFive Ratings: The Halo-Alpha-Beta Model. Journal of per-\nsonality and social psychology, 97: 1142–56.\nBagby, R. M.; Gralnick, T. M.; Al-Dajani, N.; and Uliaszek,\nA. A. 2016. The Role of the Five-Factor Model in Personal-\nity Assessment and Treatment Planning. Clinical Psychol-\nogy: Science and Practice, 23(4): 365–381.\nBowman, S. R.; Angeli, G.; Potts, C.; and Manning, C. D.\n2015. A large annotated corpus for learning natural language\ninference. In Proceedings of the 2015 Conference on Empir-\nical Methods in Natural Language Processing, 632–642.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Mod-\nels are Few-Shot Learners. In Larochelle, H.; Ranzato, M.;\nHadsell, R.; Balcan, M.; and Lin, H., eds.,Advances in Neu-\nral Information Processing Systems, volume 33, 1877–1901.\nCheng, Q.; Yang, X.; Sun, T.; Li, L.; and Qiu, X. 2023a. Im-\nproving Contrastive Learning of Sentence Embeddings from\nAI Feedback. In Findings of the Association for Computa-\ntional Linguistics: ACL 2023, 11122–11138.\nCheng, Q.; Yang, X.; Sun, T.; Li, L.; and Qiu, X. 2023b. Im-\nproving Contrastive Learning of Sentence Embeddings from\nAI Feedback. In Findings of the Association for Computa-\ntional Linguistics: ACL 2023, 11122–11138.\nCui, B.; and Qi, C. 2017. Survey Analysis of Machine\nLearning Methods for Natural Language Processing for\nMBTI Personality Type Prediction. Technical report, Stan-\nford University.\nDai, Z.; Yang, Z.; Yang, Y .; Carbonell, J.; Le, Q.; and\nSalakhutdinov, R. 2019. Transformer-XL: Attentive Lan-\nguage Models beyond a Fixed-Length Context. In Korho-\nnen, A.; Traum, D.; and M `arquez, L., eds., Proceedings of\nthe 57th Annual Meeting of the Association for Computa-\ntional Linguistics, 2978–2988.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 4171–4186.\nFang, Q.; Giachanou, A.; Bagheri, A.; Boeschoten, L.; van\nKesteren, E.-J.; Shafiee Kamalabad, M.; and Oberski, D.\n2023. On Text-based Personality Computing: Challenges\nand Future Directions. In Findings of the Association for\nComputational Linguistics: ACL 2023, 10861–10879.\nGao, T.; Yao, X.; and Chen, D. 2021. SimCSE: Simple Con-\ntrastive Learning of Sentence Embeddings. In Proceedings\nof the 2021 Conference on Empirical Methods in Natural\nLanguage Processing, 6894–6910.\nGjurkovi´c, M.; Karan, M.; Vukojevi ´c, I.; Bo ˇsnjak, M.; and\nSnajder, J. 2021. PANDORA Talks: Personality and De-\nmographics on Reddit. In Proceedings of the Ninth Interna-\ntional Workshop on Natural Language Processing for Social\nMedia, 138–152.\nHadsell, R.; Chopra, S.; and LeCun, Y . 2006. Dimension-\nality Reduction by Learning an Invariant Mapping. In 2006\nIEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR’06), volume 2, 1735–1742.\nHinton, G.; Vinyals, O.; and Dean, J. 2015. Distilling the\nKnowledge in a Neural Network. arXiv:1503.02531.\nHsieh, C.-Y .; Li, C.-L.; Yeh, C.-k.; Nakhost, H.; Fujii, Y .;\nRatner, A.; Krishna, R.; Lee, C.-Y .; and Pfister, T. 2023. Dis-\ntilling Step-by-Step! Outperforming Larger Language Mod-\nels with Less Training Data and Smaller Model Sizes. In\nFindings of the Association for Computational Linguistics:\nACL 2023, 8003–8017.\nIliopoulos, F.; Kontonis, V .; Baykal, C.; Menghani, G.;\nTrinh, K.; and Vee, E. 2022. Weighted Distillation with Un-\nlabeled Examples. In Koyejo, S.; Mohamed, S.; Agarwal,\nA.; Belgrave, D.; Cho, K.; and Oh, A., eds., Advances in\nNeural Information Processing Systems, volume 35, 7024–\n7037.\nJi, Y .; Wu, W.; Zheng, H.; Hu, Y .; Chen, X.; and He, L. 2023.\nIs ChatGPT a Good Personality Recognizer? A Preliminary\nStudy. arXiv:2307.03952.\nJiang, H.; Zhang, X.; and Choi, J. D. 2019. Automatic Text-\nbased Personality Recognition on Monologues and Multi-\nparty Dialogues Using Attentive Networks and Contextual\nEmbeddings. arXiv:1911.09304.\nJiang, H.; Zhang, X.; and Choi, J. D. 2020. Automatic Text-\nBased Personality Recognition on Monologues and Multi-\nparty Dialogues Using Attentive Networks and Contextual\nEmbeddings (Student Abstract). volume 34, 13821–13822.\nJohnson, R.; Wootten, M.; Spear, A.; and Smolensky, A.\n2023. The Relationship Between Personality Traits and\nthe Processing of Emotion Words: Evidence from Eye-\nMovements in Sentence Reading. Journal of Psycholinguis-\ntic Research, 1–27.\nKeh, S. S.; and Cheng, I.-T. 2019. Myers-Briggs\nPersonality Classification and Personality-Specific Lan-\nguage Generation Using Pre-trained Language Models.\narXiv:1907.06333.\nKishima, R.; Matsumoto, K.; Yoshida, M.; and Kita, K.\n2021. Construction of MBTI Personality Estimation Model\nConsidering Emotional Information. In Proceedings of the\n35th Pacific Asia Conference on Language, Information and\nComputation, 262–269. Shanghai, China.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. RoBERTa: A Robustly Optimized BERT Pretraining\nApproach. arXiv:1907.11692.\nLynn, V .; Balasubramanian, N.; and Schwartz, H. A. 2020.\nHierarchical Modeling for User Personality Prediction: The\nRole of Message-Level Attention. In Proceedings of the\n58th Annual Meeting of the Association for Computational\nLinguistics, 5306–5316.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18241\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright,\nC.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray,\nA.; Schulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens,\nM.; Askell, A.; Welinder, P.; Christiano, P. F.; Leike, J.; and\nLowe, R. 2022. Training language models to follow in-\nstructions with human feedback. In Koyejo, S.; Mohamed,\nS.; Agarwal, A.; Belgrave, D.; Cho, K.; and Oh, A., eds.,\nAdvances in Neural Information Processing Systems, vol-\nume 35, 27730–27744.\nSchwartz, H. A.; Eichstaedt, J. C.; Kern, M. L.; Dziurzynski,\nL.; Ramones, S. M.; Agrawal, M.; Shah, A.; Kosinski, M.;\nStillwell, D.; Seligman, M. E. P.; and Ungar, L. H. 2013. Per-\nsonality, Gender, and Age in the Language of Social Media:\nThe Open-V ocabulary Approach.PLOS ONE, 8(9): 1–16.\nStajner, S.; and Yenikent, S. 2020. A Survey of Automatic\nPersonality Detection from Texts. In Proceedings of the\n28th International Conference on Computational Linguis-\ntics, 6284–6295.\nSun, Y .; Wang, S.; Feng, S.; Ding, S.; Pang, C.; Shang, J.;\nLiu, J.; Chen, X.; Zhao, Y .; Lu, Y .; Liu, W.; Wu, Z.; Gong,\nW.; Liang, J.; Shang, Z.; Sun, P.; Liu, W.; Ouyang, X.; Yu,\nD.; Tian, H.; Wu, H.; and Wang, H. 2021. ERNIE 3.0: Large-\nscale Knowledge Enhanced Pre-training for Language Un-\nderstanding and Generation. arXiv:2107.02137.\nTadesse, M. M.; Lin, H.; Xu, B.; and Yang, L. 2018. Person-\nality Predictions Based on User Behavior on the Facebook\nSocial Media Platform. IEEE Access, 6: 61959–61969.\nTandera, T.; Hendro; Suhartono, D.; Wongso, R.; and Prase-\ntio, Y . L. 2017. Personality Prediction System from Face-\nbook Users. Procedia Computer Science, 116: 604–611.\nDiscovery and innovation of computer science technology\nin artificial intelligence era: The 2nd International Confer-\nence on Computer Science and Computational Intelligence\n(ICCSCI 2017).\nTausczik, Y . R.; and Pennebaker, J. W. 2010. The Psycho-\nlogical Meaning of Words: LIWC and Computerized Text\nAnalysis Methods. Journal of Language and Social Psy-\nchology, 29(1): 24–54.\nWang, S.; Liu, Y .; Xu, Y .; Zhu, C.; and Zeng, M. 2021. Want\nTo Reduce Labeling Cost? GPT-3 Can Help. In Findings\nof the Association for Computational Linguistics: EMNLP\n2021, 4195–4205.\nWang, Y .; Kordi, Y .; Mishra, S.; Liu, A.; Smith, N. A.;\nKhashabi, D.; and Hajishirzi, H. 2023a. Self-Instruct: Align-\ning Language Models with Self-Generated Instructions. In\nProceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers),\n13484–13508.\nWang, Z.; Xie, Q.; Ding, Z.; Feng, Y .; and Xia, R. 2023b. Is\nChatGPT a Good Sentiment Analyzer? A Preliminary Study.\narXiv:2304.04339.\nWen, Z.; Cao, J.; Yang, R.; Liu, S.; and Shen, J. 2021. Au-\ntomatically Select Emotion for Response via Personality-\naffected Emotion Transition. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021, 5010–\n5020.\nWilliams, A.; Nangia, N.; and Bowman, S. 2018. A Broad-\nCoverage Challenge Corpus for Sentence Understanding\nthrough Inference. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, Vol-\nume 1 (Long Papers), 1112–1122.\nXue, D.; Wu, L.; Hong, Z.; Guo, S.; Gao, L.; Wu, Z.; Zhong,\nX.; and Sun, J. 2018. Deep learning-based personality\nrecognition from text posts of online social networks. Ap-\nplied Intelligence, 48.\nYan, Y .; Li, R.; Wang, S.; Zhang, F.; Wu, W.; and Xu,\nW. 2021. ConSERT: A Contrastive Framework for Self-\nSupervised Sentence Representation Transfer. In Proceed-\nings of the 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International Joint Con-\nference on Natural Language Processing (Volume 1: Long\nPapers), 5065–5075.\nYang, F.; Quan, X.; Yang, Y .; and Yu, J. 2021a. Multi-\nDocument Transformer for Personality Detection. vol-\nume 35, 14221–14229.\nYang, R.; Chen, J.; and Narasimhan, K. 2021. Improving Di-\nalog Systems for Negotiation with Personality Modeling. In\nProceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing (Volume\n1: Long Papers), 681–693.\nYang, T.; Deng, J.; Quan, X.; and Wang, Q. 2023. Orders\nAre Unwanted: Dynamic Deep Graph Convolutional Net-\nwork for Personality Detection. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 37, 13896–\n13904.\nYang, T.; Yang, F.; Ouyang, H.; and Quan, X. 2021b. Psy-\ncholinguistic Tripartite Graph Network for Personality De-\ntection. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th In-\nternational Joint Conference on Natural Language Process-\ning (Volume 1: Long Papers), 4229–4239.\nYarkoni, T. 2010. Personality in 100,000 Words: A large-\nscale analysis of personality and word use among bloggers.\nJournal of Research in Personality, 44(3): 363–373.\nZhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;\nChen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V .; Mi-\nhaylov, T.; Ott, M.; Shleifer, S.; Shuster, K.; Simig, D.;\nKoura, P. S.; Sridhar, A.; Wang, T.; and Zettlemoyer, L.\n2022. OPT: Open Pre-trained Transformer Language Mod-\nels. arXiv:2205.01068.\nZhang, T.; Ladhak, F.; Durmus, E.; Liang, P.; McKeown, K.;\nand Hashimoto, T. B. 2023. Benchmarking Large Language\nModels for News Summarization. arXiv:2301.13848.\nZhang, Y .; Jin, R.; and Zhou, Z.-H. 2010. Understanding\nbag-of-words model: a statistical framework. International\nJournal of Machine Learning and Cybernetics, 1: 43–52.\nZhu, Y .; Hu, L.; Ge, X.; Peng, W.; and Wu, B. 2022. Con-\ntrastive Graph Transformer Network for Personality Detec-\ntion. In Raedt, L. D., ed., Proceedings of the Thirty-First\nInternational Joint Conference on Artificial Intelligence,\nIJCAI-22, 4559–4565.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18242",
  "topic": "Language model",
  "concepts": [
    {
      "name": "Language model",
      "score": 0.6169953346252441
    },
    {
      "name": "Personality",
      "score": 0.458923876285553
    },
    {
      "name": "Computer science",
      "score": 0.449527382850647
    },
    {
      "name": "Psychology",
      "score": 0.4452091455459595
    },
    {
      "name": "Natural language processing",
      "score": 0.42735517024993896
    },
    {
      "name": "Social psychology",
      "score": 0.12686046957969666
    }
  ]
}