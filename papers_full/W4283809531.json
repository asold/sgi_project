{
  "title": "Wasserstein Adversarial Transformer for Cloud Workload Prediction",
  "url": "https://openalex.org/W4283809531",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2994519880",
      "name": "Shivani Arbat",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3043361544",
      "name": "Vinodh Kumaran Jayakumar",
      "affiliations": [
        "The University of Texas at San Antonio"
      ]
    },
    {
      "id": "https://openalex.org/A2108567451",
      "name": "Jaewoo Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2009336559",
      "name": "Wei Wang",
      "affiliations": [
        "The University of Texas at San Antonio"
      ]
    },
    {
      "id": "https://openalex.org/A2116172871",
      "name": "In Kee Kim",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2739748921",
    "https://openalex.org/W1984255960",
    "https://openalex.org/W2163291889",
    "https://openalex.org/W7046223839",
    "https://openalex.org/W3213612880",
    "https://openalex.org/W3177683621",
    "https://openalex.org/W6666761814",
    "https://openalex.org/W2016034479",
    "https://openalex.org/W3042570676",
    "https://openalex.org/W2147210569",
    "https://openalex.org/W2578510131",
    "https://openalex.org/W2891974069",
    "https://openalex.org/W4412603965",
    "https://openalex.org/W2897317087",
    "https://openalex.org/W6776486363",
    "https://openalex.org/W1509949292",
    "https://openalex.org/W6660450530",
    "https://openalex.org/W2153657167",
    "https://openalex.org/W6678789388",
    "https://openalex.org/W2954358107",
    "https://openalex.org/W2115743854",
    "https://openalex.org/W1942382433",
    "https://openalex.org/W3097294131",
    "https://openalex.org/W2401224581",
    "https://openalex.org/W2125262761",
    "https://openalex.org/W4295521014",
    "https://openalex.org/W3166513219",
    "https://openalex.org/W3022643593",
    "https://openalex.org/W3125378709",
    "https://openalex.org/W2983902802",
    "https://openalex.org/W4298289240",
    "https://openalex.org/W2038892579",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1921523184",
    "https://openalex.org/W4247312490"
  ],
  "abstract": "Predictive VM (Virtual Machine) auto-scaling is a promising technique to optimize cloud applications’ operating costs and performance. Understanding the job arrival rate is crucial for accurately predicting future changes in cloud workloads and proactively provisioning and de-provisioning VMs for hosting the applications. However, developing a model that accurately predicts cloud workload changes is extremely challenging due to the dynamic nature of cloud workloads. Long- Short-Term-Memory (LSTM) models have been developed for cloud workload prediction. Unfortunately, the state-of-the-art LSTM model leverages recurrences to predict, which naturally adds complexity and increases the inference overhead as input sequences grow longer. To develop a cloud workload prediction model with high accuracy and low inference overhead, this work presents a novel time-series forecasting model called WGAN-gp Transformer, inspired by the Transformer network and improved Wasserstein-GANs. The proposed method adopts a Transformer network as a generator and a multi-layer perceptron as a critic. The extensive evaluations with real-world workload traces show WGAN- gp Transformer achieves 5× faster inference time with up to 5.1% higher prediction accuracy against the state-of-the-art. We also apply WGAN-gp Transformer to auto-scaling mechanisms on Google cloud platforms, and the WGAN-gp Transformer-based auto-scaling mechanism outperforms the LSTM-based mechanism by significantly reducing VM over-provisioning and under-provisioning rates.",
  "full_text": "Wasserstein Adversarial Transformer for Cloud Workload Prediction\nShivani Arbat1, Vinodh Kumaran Jayakumar2, Jaewoo Lee1, Wei Wang2, In Kee Kim1\n1University of Georgia\n2 The University of Texas at San Antonio\nsga64681@uga.edu, rvn028@my.utsa.edu, jaewoo.lee@uga.edu, wei.wang@utsa.edu, inkee.kim@uga.edu\nAbstract\nPredictive Virtual Machine (VM) auto-scaling is a promising\ntechnique to optimize cloud applications’ operating costs and\nperformance. Understanding the job arrival rate is crucial for\naccurately predicting future changes in cloud workloads and\nproactively provisioning and de-provisioning VMs for host-\ning the applications. However, developing a model that ac-\ncurately predicts cloud workload changes is extremely chal-\nlenging due to the dynamic nature of cloud workloads. Long-\nShort-Term-Memory (LSTM) models have been developed\nfor cloud workload prediction. Unfortunately, the state-of-\nthe-art LSTM model leverages recurrences to predict, which\nnaturally adds complexity and increases the inference over-\nhead as input sequences grow longer. To develop a cloud\nworkload prediction model with high accuracy and low infer-\nence overhead, this work presents a novel time-series fore-\ncasting model called WGAN-gp Transformer, inspired by\nthe Transformer network and improved Wasserstein-GANs.\nThe proposed method adopts a Transformer network as a\ngenerator and a multi-layer perceptron as a critic. The ex-\ntensive evaluations with real-world workload traces show\nWGAN-gp Transformer achieves 5× faster inference time\nwith up to 5.1% higher prediction accuracy against the state-\nof-the-art approach. We also apply WGAN-gp Transformer to\nauto-scaling mechanisms on Google cloud platforms, and the\nWGAN-gp Transformer-based auto-scaling mechanism out-\nperforms the LSTM-based mechanism by significantly reduc-\ning VM over-provisioning and under-provisioning rates.\nIntroduction\nResource provisioning and VM auto-scaling of cloud re-\nsources are essential operations to optimize cloud costs and\nthe performance of cloud applications (Shen et al. 2011;\nMao and Humphrey 2011; Rzadca et al. 2020). Auto-scaling\ndynamically performs scale-out and scale-in operations as\napplication workloads fluctuate.e.g., change in user requests\nto the applications. The scale-out operation increases the\nnumber of VMs for running the cloud application as the\nworkload increases so that the cloud application leverages\nenough amount of VMs and meets its performance goal. On\nthe other hand, the scale-in operation automatically down-\nsizes the number of existing VMs by terminating idle VMs\nwhen the workload decreases and helps the cloud applica-\ntion minimize the cloud cost.\nWhile auto-scaling offers benefits to cloud applications,\nunavoidable delays occur during the VM scaling processes,\nCopyright © 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\ne.g., VM startup delay due to the reactive nature, hence\noffering suboptimal cloud resource management (Mao and\nHumphrey 2012; Hao et al. 2021a,b). To address the reactive\nnature of auto-scaling, predictive auto-scaling approaches\nhave been deeply investigated (Shen et al. 2011; Roy, Dubey,\nand Gokhale 2011; Calheiros et al. 2015; Kim et al. 2016,\n2018; Jayakumar et al. 2020). Predictive auto-scaling mech-\nanisms commonly have two components: workload predic-\ntor and auto-scaling module. The role of workload predictor\nis to forecast the changes in user requests (or workloads) to\ncloud applications. An essential step in designing the work-\nload predictor is to understand job arrival rates1(JARs).\nThere have been several approaches used in the work-\nload predictor to model the JARs — for example, statistical\ntime-series methods (Roy, Dubey, and Gokhale 2011; Cal-\nheiros et al. 2015), traditional machine learning (Kim et al.\n2016; Cortez et al. 2017), ensemble learning (Loff and Gar-\ncia 2014; Jiang et al. 2011; Kim et al. 2018, 2020), and\ndeep learning (Kumar et al. 2018; Jayakumar et al. 2020).\nThe state-of-the-art approach in predicting cloud work-\nloads employs a combination of LSTM and Bayesian op-\ntimization (Jayakumar et al. 2020) which specifically lever-\nage the power of LSTM to understand JARs using recur-\nrences (Hochreiter and Schmidhuber 1997). However, these\nrecurrences increase complexity and computational over-\nhead as input sequences grow longer. Additionally, while\nLSTM is capable of detecting long-term seasonality in the\ncloud workloads, the majority of real-world cloud workloads\nhave random and dynamic burstiness (Islam, Venugopal, and\nLiu 2015) as shown in cloud workload traces in Figure 1.\nThe sudden spikes in cloud workload traces show unprece-\ndented changes in user request patterns over time. Therefore,\nthere is an urgent need to develop a novel cloud workload\npredictor to model such dynamic fluctuations and provide\nhigh prediction accuracy and low computational overhead.\nIn this work, we present a novel time-series forecast-\ning model for cloud resource provisioning, called WGAN-\ngp (Wasserstein Generative Adversarial Network with gra-\ndient penalty) Transformer. WGAN-gp Transformer is in-\nspired by Transformer networks (Vaswani et al. 2017) and\nimproved WGAN (Wasserstein Generative Adversarial Net-\nworks) (Arjovsky, Chintala, and Bottou 2017). Our pro-\nposed model uses the transformer architecture for the gen-\nerator and the critic is a multi-layer perceptron (MLP).\nWGAN-gp Transformer also employs MADGRAD (Mo-\n1We use the terms workloads, user requests, and job arrival rates\ninterchangeably.\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n12433\n0\n0.5\n1.0\n1.5\n2.0\n2.5\n 0  300  600  900  1200\nJob Counts (x106)\nIntervals (30 min.)\n(a) Google Cluster Trace\n0\n50\n100\n150\n0 30 60 90 120\nJob Counts\nIntervals (10 min.)\n(b) Facebook Trace\n0\n1K\n2K\n3K\n4K\n5K\n6K\n0 200 400 600\nJob Counts\nIntervals (60 min.)\n(c) Azure Cloud 2017 Trace\n2.0\n3.5\n5.0\n6.5\n0 100 200 300 400 500\nJob Counts (x106)\nIntervals (30 min.)\n(d) Wikipedia Trace\nFigure 1: Cloud Workload Traces. (a) Google Cluster Trace, (b) Facebook Hadoop Trace, (c) Azure Cloud 2017 VM Trace, and\n(d) Wikipedia Trace\nmentumized, Adaptive, Dual Averaged Gradient Method for\nStochastic Optimization) as the model optimizer for both\ngenerator and critic to achieve better convergence com-\npared to widely adopted Adam optimizer (Defazio and Je-\nlassi 2021). WGAN-gp Transformer is publicly available at\nhttps://github.com/shivaniarbat/wgan-gp-transformer.\nWe thoroughly evaluate the accuracy and overhead of\nWGAN-gp Transformer on the representative cloud work-\nload datasets, and the evaluation results confirm that\nWGAN-gp Transformer consistently performs better, yield-\ning lower prediction errors against the state-of-the-art LSTM\nmodel (Jayakumar et al. 2020). WGAN-gp Transformer\nachieves up to 5.1% lower prediction error and 5×faster\nprediction time against the baseline.\nRelated Work\nStatistical and Machine Learning (ML) Approaches\nVarious methods have been applied to develop workload\npredictors in predictive VM auto-scaling. The statistical\nand ML models include Exponential Smoothing, Weighted\nMoving Average, Autoregressive models (AR) and varia-\ntions (e.g.,ARMA, ARIMA), Support Vector Machine, Ran-\ndom Forest, Gradient Boosting, and others (Shen et al. 2011;\nWood et al. 2008; Roy, Dubey, and Gokhale 2011; Calheiros\net al. 2015; Yadwadkar, Ananthanarayanan, and Katz 2014;\nLin et al. 2015; Cortez et al. 2017; Rzadca et al. 2020).\nWhile such statistical and ML approaches could model time-\nseries cloud workloads with cyclic or seasonal trends, such\napproaches appeared to make sub-optimal predictions for\ncloud workloads as they were continuously and dynamically\nchanging over time. Moreover, a model can be effective on\none “known” type of workload, but it often fails to accu-\nrately predict future changes in other “previously unknown”\n(previously not trained) workload patterns (Kim et al. 2016).\nNeural Networks (NN)-based Approaches The applica-\ntions of NNs in time-series forecasting can provide im-\nproved accuracy across multiple domains. NN models learn\nto encode relevant historical information from time-series\ndata into intermediate feature representation to make a fi-\nnal forecast with series of non-linear NN layers (Lim and\nZohren 2021). For cloud workloads, LSTM (Hochreiter and\nSchmidhuber 1997) and its variations are studied to forecast\nthe resource demands or user requests. (Nguyen, Klein, and\nElmroth 2019; Kumar et al. 2018; Jayakumar et al. 2020).\nIn particular, Jayakumar et al. (Jayakumar et al. 2020) pro-\nposed LoadDynamics, a self-optimized generic workload\nprediction framework for cloud workloads. LoadDynam-\nics performs autonomous self-optimization operations using\nBayesian Optimization to repeatedly find the proper hyper-\nparameters in LSTM for handling dynamic fluctuations of\ncloud workloads. However, LSTM intrinsically depends on\ncapturing long/short dependencies using recurrences. As the\ninput sequence length grows, it increases the complexity of\nprocessing such longer input sequences.\nDue to the recent advancement of in the field of natural\nlanguage processing, attention mechanism in Transformer\nnetwork can be an alternative to recurrences or convolu-\ntions (Vaswani et al. 2017). For example, TransGAN (Jiang,\nChang, and Wang 2021) proposed a strict, Transformer-\nbased GAN, which employs Transformer as both generator\nand discriminator. For time-series data, the attention mech-\nanism in Transformer network allows the model to focus\non temporal information in the input sequences (Lim and\nZohren 2021). Moreover, Adversarial Sparse Transformer\n(AST) (Wu et al. 2020) was proposed to leverage a sparse\nattention mechanism for increasing the prediction accuracy\nat the sequence level. This approach employed sparse trans-\nformer as generator and MLP as discriminator. Unfortu-\nnately, AST still has limitations to accurately predict highly\ndynamic cloud workloads because AST often loses long-\nterm forecasting information due to the difficulty of training\nGANs with sparse point-wise connections.\nOn the other hand, to effectively predict dynamic cloud\nworkloads with capturing long-term temporal information,\nour method proposes to train the Transformer network using\nan improved WGAN-gp algorithm (Gulrajani et al. 2017).\nBackground\nProblem definition A univariate time-series is defined as\na sequence of measurements of the same variable collected\nover time. We study univariate time-series data of JARs or\nuser requests rates collected at regular time intervals from\nvarious cloud workloads. Let x = [x1, x2, . . . , xT ] denote\na univariate time-series of length T, where xt ∈ R is its\nvalue at timet. We use boldface roman lower and upper case\nletters to denote vectors and matrices, respectively. xk:ℓ de-\nnotes the entries with indices fromk to ℓ. To prepare a train-\ning dataset X ∈RN×S, x1:T is split into N time series of\nlength S. We write xi,1:S to denote the ith time series in X.\nGenerative Adversarial Networks Generative Adversar-\nial Networks (GAN) (Goodfellow et al. 2014) are adversar-\nial nets framework, which simultaneously trains two com-\n12434\npleting models; a generative model (G) and a discriminative\nmodel (D). The training strategy leverages a min-max game\nbetween two competing models (G and D), and the value\nfunction V (D, G) is defined as\nmin\nG\nmax\nD\nV (D, G) =Ex∼Pr [log D(x)] +\nE˜x∼Pg [log(1 −D(˜x))],\n(1)\nwhere Pr is the real data distribution, Pg is the model dis-\ntribution implicitly defined by ˜x = G(z), and z ∼p(z) is a\nlatent variable having a simple distribution such as uniform\ndistribution or standard normal distribution.\nWasserstein GAN Arjovsky et al. (Arjovsky, Chintala,\nand Bottou 2017) showed that the gradient of Jensen-\nShannon divergence used in the original GAN is not smooth\nand may not be well defined when the model distribution and\nthe true distribution have different supports. To mitigate the\nissue, Arjovsky et al. proposed Wasserstein GAN (WGAN),\nwhich consists of a generator and a critic. In WGAN, the\nrole of the generator is to generate a sample as in the original\nGAN, while that of the critic is to approximate the Wasser-\nstein distance between Pr and Pg. The objective function of\nWGAN is formulated by\nmin\nG\nmax\nD∈D\nEx∼Pr [D(x)] −E˜x∼Pg [D(˜x)], (2)\nwhere Dis a set of 1-Lipschitz functions, Pr represents the\ndata distribution, and Pg is the model distribution implicitly\ndefined by ˜x = G(z) with a latent variable z ∼p(z).\nIn (Arjovsky, Chintala, and Bottou 2017), the Lipschitz\nconstraint on Dis enforced by clipping the weight values\ninto a small interval [−c, c]. However, it is unknown how\nto choose the hyperparameter c that has a significant impact\non the training of WGAN. Furthermore, irregular value sur-\nfaces are generated due to hard clipping of the magnitude of\neach weight. Other weight constraints (e.g., L2 norm clip-\nping, weight normalization) and soft constraints (e.g., L1\nand L2 weight decay) also lead to similar problems (Gul-\nrajani et al. 2017).\nGradient Penalty To address the drawbacks of weight\nclipping, Gulrajani et al. (Gulrajani et al. 2017) proposed an\nalternative way to enforce Lipschitz constraint. From the ob-\nservation that functions f with ∥f(x)∥≤ L are L-Lipschitz,\nthey proposed to add a penalty term that forces the gradient\nnorm of critic to stay close to 1, which results in the follow-\ning objective function.\nL = Eex∼Pg [D(˜x)] −Ex∼Pr [D(x)]+\nλE\nx∼Px [(||∇xD(x)||2 −1)2] , (3)\nwhere x = ϵx + (1−ϵ)˜x and ϵ ∼U[0, 1]. The gradient\npenalty coefficient λ controls how strictly the constraint is.\nIn our implementation, we set λ = 10 (the default values\nsuggested by the authors of WGAN-GP).\nWasserstein Adversarial Transformer\nAs discussed in the Introduction, cloud workloads are of-\nten bursty and dynamically fluctuating over time. To accu-\nrately forecast future changes in cloud workloads, we pro-\npose WGAN-gp Transformer, based on the Transformer net-\nwork and WGAN.\nGiven an input time series xi,1:S, i = 1, . . . , N, our\nmodel G(·; θ) with parameter θ predicts the values for\ntime steps in [t0 + 1, t0 + τ] conditioning on xi,1:t0 , where\nt0+τ = S. τ is the number of time stepsG(·; θ) is trained to\npredict. That is, ˆxt0+1:t0+τ = G(x1:t0 ; θ). The time ranges\n[1, t0] and [t0 + 1 :S] are referred to as conditioning range\nand prediction range, respectively.\nArchitecture of WGAN-gp Transformer Figure 2 illus-\ntrates the architecture of the proposed WGAN-gp Trans-\nformer. The critic network is composed of 3 fully connected\nlayers with LeakyReLU as an activation function (Xu\net al. 2015; Wu et al. 2020). The generator is based on\nthe encoder-decoder architecture of Transformer network,\nwhich is composed of one layer of an encoder and subse-\nquent one layer of a decoder. The encoder encodes the in-\nput time series xi,1:t0 into the latent vector h1:t0 . The la-\ntent vector h1:t0 serves as a memory for the decoder to gen-\nerate values in the prediction range. Transformer networks\nare order-agnostic (Vaswani et al. 2017; Tsai et al. 2019)\nand, unlike recurrences and convolutions, it does not have\nimplicit mechanisms to retain temporal (positional) depen-\ndency information. To embed the temporal information, po-\nsitional encoding (Vaswani et al. 2017) is applied to the input\nxi,1:t0 . Similarly, positional encoding is applied to the input\nto decoder. The input to decoder is xi,t0 , which is the last\ntime step value of xi,1:t0 .\nAdversarial Training The generator (the Transformer\nnetwork) attempts to generate high quality predictions that\nlook similar to original time series by minimizing the mean\nabsolute error LMAE = 1\nN\n∑N\ni=1\n∑S\nt=t0+1 |ˆxi,t −xi,t|. The\nobjective function for generator is given by\nLG = 1\nN\nN∑\ni=1\n∥ˆxi,t0+1:S −xi,t0+1:S∥1\n− 1\nN\nN∑\ni=1\nD([xi,1:t0 ⊕ˆxi,t0+1:S]) , (4)\nwhere [a ⊕b] denotes the concatenation of two vectors.\nLeakyReLU is computed by f(x) = max(αx, x), where\nα = 0.2. The critic’s objective function is given as follows.\nLC = 1\nN\nN∑\ni=1\n{\nD([xi,1:t0 ⊕ˆxi,t0+1:S])\n}\n− 1\nN\nN∑\ni=1\nD(xi,1:S)\n+ λ 1\nN\nN∑\ni=1\n(\n(∥∇xi D(xi)∥2 −1)2)\n, (5)\nwhere xi = ϵ xi,1:S + (1−ϵ)[xi,1:t0 ⊕ˆxi,t0+1:S] and ϵ ∼\nU[0, 1].\nAlgorithm 1 describes the training of WGAN-gp Trans-\nformer, and it is improved over the original proposal (Gulra-\njani et al. 2017) to train thegenerator. The algorithm updates\nthe critic first and then updates the generator with learning\nresults from the critic. The generator is trained with the up-\ndated loss function shown in Equation 4 and the critic is\ntrained with the updated loss function expressed in Equa-\ntion 5. We employ MADGRAD optimizer to minimize the\n12435\nMulti-Head Attention\nAdd & Norm\nFeed Forward\nAdd & Norm\nMulti-Head Attention\nAdd & Norm\nFeed Forward\nEncoder\nDecoder\nGenerator\nCritic\nAdd & Norm\nMasked Multi-Head Attention\nPositional\nEncoding\nPositional\nEncoding\nFigure 2: The architecture of WGAN-gp Transformer\nAlgorithm 1: Training Algorithm of WGAN-gp Transformer\nRequire: λ, the gradient penalty coefficient. m, the batch\nsize. ncritic, the number of iterations of the critic per gen-\nerator iteration. mMADGRAD , momentum value. α learning\nrate\nRequire: initial critic parameters w0, initial generator pa-\nrameters θ0.\n1: while θ has not converged do\n2: for t = 1, ...., ncritic do\n3: for i = 1, ...., mdo\n4: Sample real data\n5: Sample generator output\n6: Compute LC\n7: end for\n8: w ←MADGRAD(∇w 1\nm\n∑m\nj=1 LC, w, α, mMADGRAD )\n9: end for\n10: Sample a batch of generator output\n11: θ ←MADGRAD(∇θ 1\nm\n∑m\nj=1 LG, θ, α, mMADGRAD )\n12: end while\nupdated loss function with gradient penalty for both gener-\nator and critic. MADGRAD is based upon the dual averag-\ning formulation of AdaGrad for the model optimization (De-\nfazio and Jelassi 2021).\nExperiments Setup\nWorkload Datasets Seven cloud workloads collected\nfrom different application categories are used to evalu-\nate WGAN-gp Transformer. The workload traces are de-\nscribed in Table 1. Facebook (Chen et al. 2011), Al-\nibaba (2018) 2, and Google (Wilkes 2011) traces are from\ndata center workloads. Wikipedia workloads are from Wik-\nibench3. Three Azure workloads4 (Azure-VM-2017, Azure-\n2https://github.com/alibaba/clusterdata\n3http://www.wikibench.eu/\n4https://github.com/Azure/AzurePublicDataset/\nWorkload Dataset T\nype Time\nInterval (in mins)\nFacebook\nData Center 5, 10Alibaba-2018\nGoogle 10, 30\nWiki Web 10, 30\nAzure-VM-2017\nCloud 10, 30,\n60Azure-VM-2019\nAzure-Func-2019 30, 60\nTable 1: Cloud workload datasets\nVM-2019, Azure-Func-2019) are from cloud VM and func-\ntion (serverless) services. These seven workloads are chosen\nbecause they have unique characteristics and dynamics in\nthe workload patterns. For example, the data center work-\nloads like Facebook and Google show dynamic spikes and\nhigh fluctuations in JARs. Wikipedia dataset represents the\nbehavior of web applications and strong seasonal patterns.\nCloud VM and function workloads from Azure show a mix-\nture of characteristics having high fluctuations and season-\nality. Among seven selected workloads, Facebook, Google,\nAzure-VM-2017, and Wikipedia workloads are shown in\nFigure 1. We omit figures of the other three workload pat-\nterns due to the page limitation.\nDifferent time granularities can exhibit subtle variations\nin the time-series workload patterns. Thus, with seven se-\nlected workloads, we generate 15 differentworkload config-\nurations with different time intervals described in Table 1.\nWe use 5 and 10 minutes of the time interval for Facebook\nand Alibaba workloads. 10 and 30 minutes of the time in-\nterval are used for Google and Wikipedia workloads. 10, 30,\nand 60 minutes of the time interval are used for two Azure-\nVM workloads. Azure-Func-2019 workload uses two-time\ninterval configurations with 30 and 60 minutes.\nImplementation WGAN-gp Transformer is implemented\nby using PyTorch and scikit-learn. When training\nthe generator and critic, we use the following configurations;\nncritic = 5, λ = 10, and α (learning rate) = 0.001. We\nset the length of prediction range to τ = 1 (i.e., one-step\n12436\nWorkload History\nLen. (\nn)\nBatch\nSize (\nm) dmodel nhead\nFacebook [3-46] [16-256]\n[8, 16,\n32, 64,\n128, 512] [4, 8]\nAlibaba-2018 [20-324]\n[16-1024]\nGoogle [28-676]\nWikipedia\n(Wiki) [12-274]\nAzure-VM-2017 [14-682]\nAzure-VM 2019 [14-230]\nAzure-Func-2019 [7-108] [16-512]\nTable 2: Hyperparameter search space for generator in\nWGAN-gp Transformer\nahead forecasting). The MADGRAD optimizer employs the\nfollowing default configurations; m (momentum value) =\n0.9, weight decay = 0, and eps = 1e −6. And the training\nis performed with 1000 epochs, which works well for our\nproposed method.\nWhen training and testing WGAN-gp Transformer, we di-\nvide the workload dataset sequence into three sub-datasets,\ncontaining the first 60%, the next 20%, and the remain-\ning 20%, which are used for the model training, cross-\nvalidation, and testing, respectively. We apply a sliding win-\ndow approach to prepare the input data to WGAN-gp Trans-\nformer to divide the data into sequences of (history) length\nn. The model is trained to predict JAR at the next time\nstep; thus, the sliding window moves with a stride of one-\ntime step to acquire the input sequences. Finally, WGAN-gp\nTransformer is trained and evaluated on NVIDIA GeForce\nRTX 2080 Ti GPU machines.\nHyperparameters We use an effective grid search to find\nthe optimal parameters for training WGAP-gp Transformer.\nWe use the configurations described in Table 2 for the hyper-\nparameter search. n (history length) is the length of the input\nsequence to the model, m is batch size, dmodel is the num-\nber of input features for the encoder and decoder, andnhead:\nthe number of heads in multi-head attention layer in encoder\nand decoder. Please note that the model size (d model) used\nfor training the generator is the same number of input fea-\ntures in the critic linear layers.\nEvaluation metric We use Mean Absolute Percentage\nError (MAPE) as the accuracy metric to assess the pro-\nposed method against the baseline. MAPE is calculated as,\n100×\n(1\nn\n)∑n\ni=1\n⏐⏐\n⏐˜yi−yi\nyi\n⏐\n⏐\n⏐, where n is the total number of data\npoints, ˜yi represents predicted JAR at time stepi, and yi rep-\nresents actual JAR at time step i.\nBaseline WGAN-gp Transformer is evaluated against\na state-of-the-art LSTM model, called LoadDynam-\nics (Jayakumar et al. 2020). LoadDynamics employs LSTM\nmodel to automatically optimize LSTM for individual work-\nload using Bayesian Optimization. As our baseline, we use\nthe brute force version of LoadDynamics, which performs\nhyperparamter search for LSTM in predefined hyperparam-\neter search space. For LoadDynamics, we use the same con-\nfigurations for the model hyperparameters (the number of\nLSTM layers, the memory cell C size, and the input length\nn) described in the original version of the paper. For the new\ncloud workloads (not been evaluated in the original Load-\nWorkload Load-\nDynamics\nWGAN-gp\nTransf\normer\nFacebook-5m 47.20 42.11\nFacebook-10m 43.68 39.31\nAlibaba-2018-5m 17.95 15.76\nAlibaba-2018-10m 16.90 14.67\nGoogle-10m 11.49 10.58\nGoogle-30m 9.12 8.34\nWiki-10m 1.17 1.34\nWiki-30m 1.75 3.43\nAzure-VM-2017-10m 42.63 41.32\nAzure-VM-2017-30m 29.35 27.48\nAzure-VM-2017-60m 16.11 12.77\nAzure-VM-2019-30m 19.74 15.19\nAzure-VM-2019-60m 13.5 10.82\nAzure-Func-2019-5m 1.63 3.05\nAzure-Func-2019-10m 2.06 1.85\nTable 3: Average prediction errors (MAPE) for cloud work-\nloads\nDyanmics paper), we use grid search to find the optimal hy-\nperparameters. The training and testing of LoadDynamics\nare performed on the same GPU (NVIDIA GeForce RTX\n2080 Ti) machine.\nEvaluation Results\nPrediction Errors and Inference Overheads We first\nevaluate the prediction error of WGAN-gp Transformer. Ta-\nble 3 reports the prediction error of WGAN-gp Transformer\nand the baseline (LoadDynamics) when predicting 15 work-\nload configurations. While the prediction errors vary with\ndifferent workload configurations, the results clearly show\nthat WGAN-gp Transformer outperforms the baseline model\nfor most workload configurations. In particular, WGAN-\ngp Transformer provides up to 5.1% lower prediction er-\nrors (MAPE). WGAN-gp Transformer relies on the alterna-\ntive adversarial training techniques to enforce Lipschitz con-\nstraint using gradient penalty with MADGRAD optimizer.\nAdditionally, the alternative adversarial training technique\nis able to reduce the prediction error against the LSTM’s re-\ncurrences.\nWe examine the impact of the model optimizer on the ac-\ncuracy of WGAN-gp Transformer by comparing the predic-\ntion errors of our model with both MADGRAD and Adam\noptimizer. In this evaluation, Adam optimizer employs the\nfollowing parameters; β1 = 0, β2 = 0.9, and learning rate\n= 0.0001 for both generator and critic. As shown in Fig-\nure 3, WGAN-gp Transformer with MADGRAD optimizer\nshows a significant reduction in the prediction errors, indi-\ncating that the use of MADGRAD optimizer is the critical\nfactor for more accurate workload prediction.\nWe also notice that WGAN-gp Transformer can be less\naccurate than LSTM-based forecasting for three work-\nload datasets, i.e., Wiki-10min, Azure-Func-2019-5m, and\nAzure-Func-2019-10m. Our further analysis reveals that\nthese workload patterns have relatively stronger seasonality\nthan others. e.g., Wikipedia workloads shown in Figure 1(d).\nLSTM’s memory capability can be better to store more accu-\n12437\n 0\n 10\n 20\n 30\n 40\n 50\nFB-10m Alibaba-\n2018-5m\nGoogle\n-10m\nAZ-VM-\n2017-60m\nAZ-VM-\n2019-30m\nAZ-VM-\n2019-60m\nMAPE\nw/ Adam\n    w/ MADGRAD\nFigure 3: Prediction error comparison between WGAN-gp\nTransformer with Adam and WGAN-gp Transformer with\nMADGRAD optimizer. (FB: Facebook, Az: Azure)\nrate information for such repeating patterns and yield lower\nprediction errors.\nWe also measure the inference overhead (time) of both\nmodels when performing the prediction tasks. Figure 4\nshows the inference time comparison between WGAN-\ngp Transformer and LSTM and represents the benefit of\nWGAN-gp Transformer over the LSTM-based model. The\nresults report that WGAN-gp Transformer has 5×faster in-\nference time over the baseline. On average, the average in-\nference time of WGAN-gp is 4.85ms, on the other hand,\nthe average inference time of LoadDynamics is 25.57ms.\nThe faster inference time of WGAN-gp Transformer is be-\ncause the model processes the input sequence at once, which\nresults in quicker inference time. On the other hand, when\nLSTM processes the input in the sequence, it processes only\none time step at a time, which increases the inference time\nfor processing longer sequences in the prediction tasks.\nAuto-Scaling Evaluations on Google Cloud After eval-\nuating the prediction errors and inference overhead, we\ndevelop VM auto-scaling systems with WGAN-gp Trans-\nformer and LoadDynamics and deploy them on a real-world\ncloud platform (GCP5; Google Cloud Platform) to measure\nthe performance differences in both systems.\nThe auto-scaling systems consist of a workload predictor\n(WGAN-gp Transformer or LoadDynamics) and aVM auto-\nscaler. The workload predictor determines the predicted job\narrivals and provides the prediction results to the auto-scaler.\nSuppose Pi, predicted at (i −1)th time interval, is the pre-\ndicted number of jobs arriving at ith time interval to the\nauto-scaling system and represents the number of VMs to\nbe created in advance. Note that we use an assumption that\none job needs an allocation of a single VM. Therefore, with\nPi, the auto-scaler creates the P VMs at ith time interval.\nAssume Ti is the actual number of jobs arriving at ith time\ninterval. If Ti > Pi (the actual job arrivals are greater than\nthe predicted), then it results in under-provisioning and, to\naccommodate extra demand of the jobs, more VMs need to\nbe allocated. In this case, additional time will be needed to\nfinish the jobs due to the VM startup time (?). On contrary, if\nTi < Pi (actual job arrivals are smaller than the predicted),\nthis results in over-provisioning and incurs extra unneces-\nsary cost with the VMs being idle.\nGoogle Cloud’s e2-medium VMs are used for this eval-\nuation. Facebook and Azure 2019 workloads are evaluated\nto compare the auto-scaling performance. For the evaluation\n5https://cloud.google.com/\n 0\n 10\n 20\n 30\nFace-\nbook\nAlibaba-\n2018\nGoogle Wiki Azure-\nVM-\n2017\nAzure-\nVM-\n2019\nAzure-\nFunc-\n2019\nInfer. Time (ms)\nLoadDynamics     WGAN-gp Tr.\nFigure 4: Inference time comparison. (Az: Azure)\nWorkload Model ↓ rate (%) ↑ rate (%)\nFacebook LoadDynamics 40.22 10.33\nWGAN-gp T\nr. 12.27 16.13\nAzure-VM\n-2019\nLoadDynamics 9.63 8.60\nWGAN-gp T\nr. 7.07 6.68\nTable 4: Auto-scaling evaluation results with VM under-\nprovisioning (↓) rates and over-provisioning (↑) rates\nwith the Facebook workload, we use Cloud Suite’s Data\nAnalytics benchmark in CloudSuite, which performs large\namounts of machine learning tasks using MapReduce frame-\nwork (Ferdman et al. 2012). For the evaluation with Azure\n2019 VM workload, we use In-Memory Analytics bench-\nmark in CloudSuite, which uses Apache Spark to execute\ncollaborative filtering algorithm in-memory on dataset of\nuser-movie ratings (Ferdman et al. 2012). The evaluation\nresults are reported in Table 4. As shown in the results,\nthe auto-scaling system with WGAN-gp Transformer out-\nperforms the auto-scaling system with the baseline model.\nWith the Facebook workload, the auto-scaler with WGAN-\ngp Transformer shows a significant reduction in under-\nprovisioning by 27.95%. For the Azure VM 2019 workload,\nthe system with WGAN-gp Transformer has reduced under-\nand over-provisioning by 2.56% and 1.92%, respectively.\nThe results clearly indicate that the accurate workload pre-\ndiction from WGAN-gp Transformer can improve the auto-\nscaling performance running on real-world cloud platforms.\nConclusion\nAccurately forecasting user request rates (e.g., job arrival\nrates) benefits optimizing cloud operating costs and guar-\nanteeing application performance goals through predictive\nVM auto-scaling. To addresses the problem of job arrival\nprediction for highly dynamic cloud workloads, we propose\na novel time-series forecasting method, called WGAN-gp\nTransformer, inspired by Transformer network and Wasser-\nstein Generative Adversarial networks. When training with\nan improved WGAN algorithm, the Transformer network\naccurately captures dynamic patterns in cloud workloads.\nWe evaluate WGAN-gp Transformer with various real-\nworld cloud workload datasets against the state-of-the-art\nLSTM model. The results show that our proposed model in-\ncreases prediction accuracy by up to 5.1%, indicating that\nthe attention mechanism in Transformer network can cor-\nrectly capture relevant information from past workload se-\nquences to make more accurate predictions. Furthermore,\nWGAN-gp Transformer significantly reduces inference time\nby 5×over the state-of-the-art LSTM-based model. Finally,\n12438\nevaluations with real-world cloud deployment show that\nWGAN-gp Transformer can significantly reduce the under-\nand over-provisioning rate compared to the baseline model.\nAcknowledgments\nJaewoo Lee’s work was supported by the National Science\nFoundation under Grant No. CNS-1943046.\nReferences\nArjovsky, M.; Chintala, S.; and Bottou, L. 2017. Wasserstein Gen-\nerative Adversarial Networks. In International Conference on Ma-\nchine Learning.\nCalheiros, R. N.; Masoumi, E.; Ranjan, R.; and Buyya, R. 2015.\nWorkload Prediction Using ARIMA Model and Its Impact on\nCloud Applications’ QoS. IEEE Trans. on Cloud Computing, 3(4).\nChen, Y .; Ganapathi, A.; Griffith, R.; and Katz, R. 2011. The Case\nfor Evaluating MapReduce Performance Using Workload Suites.\nIn IEEE International Symposium on Modelling, Analysis, and\nSimulation of Computer and Telecommunication Systems.\nCortez, E.; Bonde, A.; Muzio, A.; Russinovich, M.; Fontoura, M.;\nand Bianchini, R. 2017. Resource Central: Understanding and Pre-\ndictingWorkloads for Improved Resource Management in Large\nCloud Platforms. In ACM Symp. on Operating Systems Principles.\nDefazio, A.; and Jelassi, S. 2021. Adaptivity without Compromise:\nA Momentumized, Adaptive, Dual Averaged Gradient Method for\nStochastic Optimization. arXiv preprint arXiv:2101.11075.\nFerdman, M.; Adileh, A.; Kocberber, O.; V olos, S.; Alisafaee, M.;\nJevdjic, D.; Kaynak, C.; Popescu, A. D.; Ailamaki, A.; and Fal-\nsafi, B. 2012. Clearing the Clouds: A Study of Emerging Scale-out\nWorkloads on Modern Hardware. ACM Sigplan Notices, 47(4).\nGoodfellow, I. J.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-\nFarley, D.; Ozair, S.; Courville, A.; and Bengio, Y . 2014. Genera-\ntive Adversarial Networks. arXiv preprint arXiv:1406.2661.\nGulrajani, I.; Ahmed, F.; Arjovsky, M.; Dumoulin, V .; and\nCourville, A. 2017. Improved Training of Wasserstein GANs.\narXiv preprint arXiv:1704.00028.\nHao, J.; Jiang, T.; Wang, W.; and Kim, I. K. 2021a. An Empirical\nAnalysis of VM Startup Times in Public IaaS Clouds. In IEEE\nInternational Conference on Cloud Computing.\nHao, J.; Jiang, T.; Wang, W.; and Kim, I. K. 2021b. An Empirical\nAnalysis of VM Startup Times in Public IaaS Clouds: An Extended\nReport. CoRR, abs/2107.03467.\nHochreiter, S.; and Schmidhuber, J. 1997. Long Short-Term Mem-\nory. Neural Computation, 9(8): 1735–1780.\nIslam, S.; Venugopal, S.; and Liu, A. 2015. Evaluating the Impact\nof Fine-scale Burstiness on Cloud Elasticity. In ACM Symposium\non Cloud Computing.\nJayakumar, V . K.; Lee, J.; Kim, I. K.; and Wang, W. 2020. A Self-\nOptimized Generic Workload Prediction Framework for Cloud\nComputing. In IEEE International Parallel and Distributed Pro-\ncessing Symposium.\nJiang, Y .; Chang, S.; and Wang, Z. 2021. TransGAN: Two Pure\nTransformers Can Make One Strong GAN, and That Can Scale\nUp. arXiv preprint arXiv:2102.07074.\nJiang, Y .; Perng, C.-S.; Li, T.; and Chang, R. N. 2011. ASAP: A\nSelf-Adaptive Prediction System for Instant Cloud Resource De-\nmand Provisioning. In IEEE International Conf. on Data Mining.\nKim, I. K.; Wang, W.; Qi, Y .; and Humphrey, M. 2016. Empir-\nical Evaluation of Workload Forecasting Techniques for Predic-\ntive Cloud Resource Scaling. In IEEE International Conference\non Cloud Computing.\nKim, I. K.; Wang, W.; Qi, Y .; and Humphrey, M. 2018. Cloudin-\nsight: Utilizing a council of experts to predict future cloud applica-\ntion workloads. In IEEE International Conference on Cloud Com-\nputing.\nKim, I. K.; Wang, W.; Qi, Y .; and Humphrey, M. 2020. Forecast-\ning Cloud Application Workloads with CloudInsight for Predictive\nResource Management. IEEE Transactions on Cloud Computing,\n1–1.\nKumar, S.; Muthiyan, N.; Gupta, S.; A.D., D.; and Nigam, A. 2018.\nAssociation Learning based Hybrid Model for Cloud Workload\nPrediction. In International Joint Conference on Neural Networks.\nLim, B.; and Zohren, S. 2021. Time-series Forecasting with Deep\nLearning: A Survey. Philosophical Transactions of the Royal So-\nciety A, 379(2194): 20200209.\nLin, H.; Qi, X.; Yang, S.; and Midkiff, S. 2015. Workload-driven\nVM Consolidation in Cloud Data Centers. In IEEE International\nParallel and Distributed Processing Symposium.\nLoff, J.; and Garcia, J. 2014. Vadara: Predictive Elasticity for Cloud\nApplications. In IEEE International Conference on Cloud Comput-\ning Technology and Science.\nMao, M.; and Humphrey, M. 2011. Auto-scaling to Minimize Cost\nand Meet Application Deadlines in Cloud Workflows. In Interna-\ntional Conference on High Performance Computing Networking,\nStorage and Analysis.\nMao, M.; and Humphrey, M. 2012. A Performance Study on the\nVM Startup Time in the Cloud. In IEEE International Conference\non Cloud Computing.\nNguyen, C.; Klein, C.; and Elmroth, E. 2019. Multivariate LSTM-\nBased Location-Aware Workload Prediction for Edge Data Cen-\nters. In Int’l Symposium on Cluster, Cloud and Grid Computing.\nRoy, N.; Dubey, A.; and Gokhale, A. 2011. Efficient Autoscaling\nin the Cloud Using Predictive Models for Workload Forecasting.\nIn IEEE International Conference on Cloud Computing.\nRzadca, K.; Findeisen, P.; Swiderski, J.; Zych, P.; Broniek, P.; Kus-\nmierek, J.; Nowak, P.; Strack, B.; Witusowski, P.; Hand, S.; and\nWilkes, J. 2020. Autopilot: workload autoscaling at Google. In\nACM European Conference on Computer Systems.\nShen, Z.; Subbiah, S.; Gu, X.; and Wilkes, J. 2011. Cloudscale:\nElastic Resource Scaling for Multi-tenant Cloud Systems. In ACM\nSymposium on Cloud Computing.\nTsai, Y .-H. H.; Bai, S.; Yamada, M.; Morency, L.-P.; and Salakhut-\ndinov, R. 2019. Transformer Dissection: A Unified Understanding\nof Transformer’s Attention via the Lens of Kernel. arXiv preprint\narXiv:1908.11775.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention is\nAll You Need. arXiv preprint arXiv:1706.03762.\nWilkes, J. 2011. More Google cluster data. http://googleresearch.\nblogspot.com/2011/11/more-google-cluster-data.html. Accessed:\n2021-09-01.\nWood, T.; Cherkasova, L.; Ozonat, K.; and Shenoy, P. 2008. Profil-\ning and Modeling Resource Usage of Virtualized Applications. In\nACM/IFIP/USENIX International Middleware Conference.\nWu, S.; Xiao, X.; Ding, Q.; Zhao, P.; Wei, Y .; and Huang, J. 2020.\nAdversarial Sparse Transformer for Time Series Forecasting. In\nAdvances in Neural Information Processing Systems.\nXu, B.; Wang, N.; Chen, T.; and Li, M. 2015. Empirical Evaluation\nof Rectified Activations in Convolutional network. arXiv preprint\narXiv:1505.00853.\nYadwadkar, N. J.; Ananthanarayanan, G.; and Katz, R. 2014.\nWrangler: Predictable and Faster Jobs using Fewer Resources. In\nACM Symposium on Cloud Computing.\n12439",
  "topic": "Provisioning",
  "concepts": [
    {
      "name": "Provisioning",
      "score": 0.8666870594024658
    },
    {
      "name": "Cloud computing",
      "score": 0.8143899440765381
    },
    {
      "name": "Computer science",
      "score": 0.786884069442749
    },
    {
      "name": "Inference",
      "score": 0.6265877485275269
    },
    {
      "name": "Workload",
      "score": 0.6166831254959106
    },
    {
      "name": "Transformer",
      "score": 0.6131861209869385
    },
    {
      "name": "Real-time computing",
      "score": 0.4298582077026367
    },
    {
      "name": "Distributed computing",
      "score": 0.4280811548233032
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3492277264595032
    },
    {
      "name": "Computer network",
      "score": 0.24285930395126343
    },
    {
      "name": "Operating system",
      "score": 0.12671256065368652
    },
    {
      "name": "Engineering",
      "score": 0.10066443681716919
    },
    {
      "name": "Voltage",
      "score": 0.07318621873855591
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}