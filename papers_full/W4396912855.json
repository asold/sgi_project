{
  "title": "Enhanced earthquake impact analysis based on social media texts via large language model",
  "url": "https://openalex.org/W4396912855",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1987218829",
      "name": "Han Jin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105833811",
      "name": "Zheng Zhe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227643400",
      "name": "Lu, Xin-Zheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227643401",
      "name": "Chen, Ke-Yin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227643403",
      "name": "Lin Jia-rui",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3005212513",
    "https://openalex.org/W2023635934",
    "https://openalex.org/W6638828912",
    "https://openalex.org/W1851189637",
    "https://openalex.org/W2055907175",
    "https://openalex.org/W3157691426",
    "https://openalex.org/W4321791069",
    "https://openalex.org/W2912217870",
    "https://openalex.org/W4366415825",
    "https://openalex.org/W3143196052",
    "https://openalex.org/W4364359833",
    "https://openalex.org/W6756002786",
    "https://openalex.org/W4319592781",
    "https://openalex.org/W6600414614",
    "https://openalex.org/W3170852693",
    "https://openalex.org/W4318833605",
    "https://openalex.org/W4229066982",
    "https://openalex.org/W2789679082",
    "https://openalex.org/W3039984117",
    "https://openalex.org/W4205116593",
    "https://openalex.org/W2946885273",
    "https://openalex.org/W4387643636",
    "https://openalex.org/W3215788869",
    "https://openalex.org/W1487925325",
    "https://openalex.org/W3148535553",
    "https://openalex.org/W6786911069",
    "https://openalex.org/W2218333116",
    "https://openalex.org/W6737912434",
    "https://openalex.org/W4290991313",
    "https://openalex.org/W6853021069",
    "https://openalex.org/W3043010374",
    "https://openalex.org/W4283739917",
    "https://openalex.org/W4293581755",
    "https://openalex.org/W3149443267",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W4283219089",
    "https://openalex.org/W1769016814",
    "https://openalex.org/W6763238093",
    "https://openalex.org/W6671603193",
    "https://openalex.org/W4285791949",
    "https://openalex.org/W6769007635",
    "https://openalex.org/W6758879197",
    "https://openalex.org/W6693505360",
    "https://openalex.org/W6682691769",
    "https://openalex.org/W2298612130",
    "https://openalex.org/W2799832028"
  ],
  "abstract": null,
  "full_text": " \n \nQuakeBERT: Accurate Classification of Social Media Texts for Rapid \nEarthquake Impact Assessment \nJin Han1, Zhe Zheng1, Xin-Zheng Lu1, Ke-Yin Chen1, Jia-Rui Lin1,* \n*Corresponding author, E-mail: lin611@tsinghua.edu.cn; jiarui_lin@foxmail.com \n(1. Department of Civil Engineering, Tsinghua University, Beijing, 100084, China) \n \nAbstract: \nSocial media aids disaster response but suffers from noise, hindering accurate impact assessment \nand decision making for resilient cities, which few studies considered. To address the problem, this study \nproposes the first domain-specific LLM model and an integrated method for rapid earthquake impact \nassessment. First, a few categories are introduced to classify and filter microblogs considering their \nrelationship to the physical and social impacts of earthquakes, and a dataset comprising 7282  \nearthquake-related microblogs from twenty earthquakes in different locations is developed as well. Then, \nwith a systematic analysis of various influential factors, QuakeBERT, a domain-specific large language \nmodel (LLM), is developed and fine -tuned for accurate classification and filtering of microblogs. \nMeanwhile, an integrated metho d integrating public opinion trend analysis, sentiment analysis, and \nkeyword-based physical impact quantification is introduced to assess both the physical and social \nimpacts of earthquakes  based on social media texts. Experiments show that data diversity and data \nvolume dominate the performance of QuakeBERT and increase the macro average F1 score by 27%, \nwhile the best classification model QuakeBERT  outperforms the CNN - or RNN -based models by \nimproving the macro average F1  score from 60.87% to 84.33%. Finally, the proposed approach is \napplied to assess two earthquakes with the same magnitude and focal depth. Results show that the \nproposed approach can effectively enhance the impact assessment process by accurate detection of noisy \nmicroblogs, which enables effective post-disaster emergency responses to create more resilient cities. \n \nKeywords: \nEarthquake, Impact Assessment, Text mining, Social media, BERT, S ocial impact, Large language \nmodel \n  \n \n \n1 Introduction \nEarthquakes are potential threats to modern cities with large and concentrated populations and \ncomplex interdependent infrastructure systems ( Lu et al., 2020 ). Extreme earthquakes (e.g., the 2008 \nWenchuan earthquake, the 2010 Haiti earthquake, the 2011 TÅhoku earthquake, and the 2014 Ludian \nearthquake) will inflict severe damage and lead to substantial harm, extensive casualties, and significant \nlosses (Cui et al., 2008; DesRoches et al., 2011; Mori et al., 2011; Hong et al., 2016). The sooner people \nare rescued from damaged areas after earthquakes, the fewer casualties will be (Ahadzadeh & Malek, \n2021). So, time-sensitive responses such as emergency rescue operations should be taken to help people \nlocate available resources, deliver assistance, and so on ( Papadopoulos et al. , 2023). Note that the \ngovernments' time-sensitive responses rely on timely and accurate information about the earthquake \nsituations to make effective response decisions and improve management strategies ( Yu et al., 2019 ). \nThe most well-known disaster-related information acquiring methods include remote sensing and post-\ndisaster surveying (Ahadzadeh & Malek, 2021). However, the remote sensing data has spatial, temporal, \nand spectral constraints ( Ahadzadeh & Malek , 2021), and the post -disaster survey takes time and \ninvolves a huge quantity of labor and resources (Lowande et al., 2023). With the rapid development of \nthe mobile internet and social platforms for sharing short and real-time information, valuable situational \ninformation for natural hazards can be disseminated across the world in real-time, which becomes more \nand more valuable for the development of resilient and sustainable cities (Yao et al., 2021). The contents \ngenerated by the users could be utilized in decision -support systems to help governments and citizens \nmanage disaster risks and enable effective post-disaster emergency responses in terms of vulnerability \nassessment, early warning, monitoring, and evaluation ( Adegoke et al., 2023). Besides, by analyzing \nsocial media texts, it is possible to effectively assess both the physical and social impacts of earthquakes \nand assist in the rapid recovery and reconstruction of cities. \nTwitter is one of the most popular social media platforms that caught much attention of researchers \n(Yao et al., 2021). Weibo is the most popular social media platform in China (Chen et al., 2020). Most \nof the microblogs on social media platforms like Twitter and Weibo are textual. Therefore, useful \ninformation related to an earthquake, such as resources, damage, donation, or aid, should be extracted \nfrom textual microblogs (Yu et al., 2019). However, the contents of the massive microblogs on social \nmedia platforms are tanglesome, which is the main obstacle to finding insights from microblogs (Kundu \net al., 2018 ). Therefore, natural language processing (NLP) techniques to classify microblogs into \ndifferent themes should be developed to help us leverage the useful information from textual content \nfrom social media ( Tounsi et al , 2023). To date, several studies have explored the classification \ncategories and methods of microblogs. For  example, informative microblogs are classified into five \ncategories, including caution and advice, casualties and damage, donations of money, goods or services, \npeople missing or found, and information sources (Imran et al., 2013). Based on the categories proposed \nby Imran, Yu added a category called Infrastructure and Resources to their classification schema (Yu et \nal., 2019). To extract the seismic intensity from microblogs, the microblogs are classified into four \nseismic intensity levels based on the seismic keyword list (Yao et al., 2021). Similar to Yao et al., Li et \nal. (Li et al., 2021) also classified the microblogs from tweets into four levels of damage (i.e., no damage \nor injury, slight damage, moderate damage with the possibility of injuries, and severe damage with the \n \n \npossibility of fatalities) based on long short -term memory (LSTM) model. Besides, Lv et al. devided \nearthquake-related microblogs into five levels (i.e., Weak earthquake, Felt earthquake, Moderate \nearthquake, Strong earthquake, Disastrous earthquake) based on the China Seismic Intensity Scale (Lv \net al., 2023). \nHowever, the social media data contain a large amount of noisy information which has the \nkeywords of the earthquake but is irrelevant to the real physical loss of the target earthquake (Kundu et \nal., 2018; Lin et al., 2022 ). For example, there are also some microblogs introducing self -rescue \nknowledge during earthquakes, which contains keywords related to physical damage but cannot reflect \nthe actual damage of the earthquake. A typical example of this type is \"# Guizhou Bijie 4.4 magnitude \nearthquake # Eart hquake self-rescue knowledge that must be remembered: (1) don't panic when an \nearthquake occurs, and (2) if the casualty needs a splint to mend the fractured limb, you can use plastic \nbags and cardboard to make an emergency splint bandage\". This microblog contains the keyword \nreflecting severe damage (i.e., \"panic\", \"casualty\", and \"fractured limb\"), but cannot reflect the actual \nearthquake damage. If the text mining-based analysis is performed without cleaning these noisy tweets, \nthe results will hardly reflect the real earthquake damage situation. Furthermore, the expression style of \nthe social media data typically exhibits characteristics of colloquial language, which did not match the \nformal keywords extracted from the intensity scale. For example, â€œA few people who were still indoors \nor in the high-rise felt itâ€ is used to depict the human perception within the second level of the China \nSeismic Intensity Scale. However, it is customary for us to adopt a more colloquial and vernacular \nlanguage style when posting on social media platforms (i.e., â€œslightâ€, â€œno feelingâ€, â€œno shockâ€, â€œsleep \nlike a logâ€ ), rather than employing such schol arly language. Therefore, considering this aspect and \nestablishing a large-language classification model based on social media data would lead to a decrease \nin classification accuracy and efficiency. \n \n2 Overview of related studies \n2.1 Social media for earthquake analysis \nRemote sensing and post-disaster surveying are the most well-known post-earthquake information-\nacquiring methods ( Ahadzadeh & Malek, 2021 ). However, the remote sensing -based methods have \nsome spatial, temporal, and spectral constraints ( Ahadzadeh & Malek, 2021 ), and the post -disaster \nsurveying-based methods are time -consuming and require extensive labor and resources ( Zou et al., \n2018; Yu et al., 2019).  \nIn recent years, social media platforms are becoming increasingly popular and closely related to \neveryone's life. Social media can provide massive amounts of real-time data, which is a new data source \nand bring new opportunities for post-earthquake management (Yao et al., 2021; Chen et al., 2020). In \nrecent years, many attempts have been made to utilize the contents from social media for earthquake \nimpact analysis (Ogie et al., 20 22; Zhang et al., 2019 ; Li et al., 2023 ). Most studies on social media \ncontent analysis collected data using a keyword search (Zhang et al., 2019). Chen & Ji estimate public \ndemand by leveraging information in sample tweets through keyword filters. Utilize a Bayesian-based \nmethod to learn the relationship between Twitter-based demand percentage and survey-based demand \n \n \npercentage, thereby achieving a reliable and rapid estimation of public demand following disasters  \n(Chen & Ji, 2022 ). Because the contents of the collected microblogs are various, to automatically \ntransform and standardize the unstructured text into a structured form for utilization, fine -grained \ncategorization is explored by many researchers based on NLP methods ( Yu et al., 2019; Zhang et al., \n2019). For example, Imran et al. classified informative microblogs into five categories, including \ncaution and advice, casualties and damage, donations of money, goods or services, people missing or \nfound, and information sources (Imran et al., 2013). Based on the categories proposed by Imran et al. \n(2013), additional categories (e.g., infrastructure and resource) are added to their classification schema \n(Yu et al., 2019). The categories proposed by Imran et al. (Imran et al., 2013) are useful for the response \nand recovery stage, but may not involve the data released before or after a disaster event (Huang & Xiao, \n2015). To consider such information, 47 message categories are defined for preparedness, response, \nimpact, and recovery (Huang & Xiao, 2015). In addition to the categories focusing on different stages, \nthere are also some categories focusing on seismic damage. For example, to extract the seismic intensity, \nsome four  or five levels of seismic intensity categories (e.g., no damage, slight damage, moderate \ndamage, and severe damage) are proposed (Yao et al., 2021; Li et al., 2021; Lv et al., 2023). Xing et al. \ndeveloped a classification standard based on impacts and affected individuals, which primarily concerns \nsocial media cont ents that are interconnected with the relocation and resettlement due to the disaster \n(Xing et al., 2021 ). In the meanwhile, Zekkos et al. categorized the tweets into six types, including \nAutomated, Impact, Felt Intensity, Supporting message, Funny and Undetermined (Zekkos et al., 2021). \nHowever, the social media data contain a large amount of noisy microblogs with the keywords of \nthe earthquake but is irrelevant to the real seismic loss of the target earthquake (Kundu et al., 2018; Lin \net al., 2022). The above four-level seismic intensity categories can not filter the tweets or retweet that \ndoes not describe the real damage (Li et al., 2021), which will make the analysis results differ from the \nactual situation of the earthquake impact . Furthermore, the expression style of the social media  data \ntypically exhibits characteristics of colloquial language, which did not match the formal keywords \nextracted from the intensity scale. Despite the existing efforts on the classification categories and \nmethods of microblogs, few studies filter weak co rrelation microblogs before assessment and \nfurthermore analyzing the influence of these weak correlation microblogs , and take the colloquial \nlanguage style prevalent on social media platforms  into account while physical impact assessment. \nTherefore, a novel category that can consider the correlation between microblogs and earthquake \ndamage should be proposed first.  \n \n2.2 Text classification methods for social media \nThe volume of content from microblogs is far beyond the capabilities of manual efforts. Therefore, \nautomated text classification methods are widely used to filter and classify data. Traditional text \nclassification methods include keyword -based methods and machine learning -based (e.g., support \nvector machine, NaÃ¯ve Bayes, random forest, logistic regres sion). Filtering keywords (e.g., damage, \ninjury, and destroy) are used to identify tweets related to earthquake damage ( Li et al., 2021 ). NaÃ¯ve \nBayes-based method is used to classify the microblogs into different categories (Binsaeed et al., 2020). \nThe support vector machine and the latent Dirichlet allocation (LDA) algorithm were combined to \n \n \nclassify microblogs ( Wang et al., 201 6). Ragini & Anand ( Ragini & Anand et al., 2016 ) proposed a \nclassification algorithm with the combination of a support vector machine and NaÃ¯ve Bayes. While the \ngeneralization performance of these traditional methods is not so high, which indicates that these \nmethods may have erroneous results in a new disaster event that is beyond the scope of the designed \nstudy ( Yu et al., 2019 ). With the development of deep learning techniques which can achieve a \ncomprehensive underst anding of a text to improve performance ( Zheng et al., 2022 ),  some deep \nlearning-based text classification models are utilized for social media. For example, Dasari et al. \nemployed a stacking ensemble approach to categorize tweet information. They selected the top five \nmachine learning models with the highest performance as base learners and utilized logistic regression \nas the meta -learner ( Dasari et al., 2023 ). Specifically, convolutional neural network (CNN) -based \nmodels (Yu et al., 2019; Devaraj et al., 2020; Xing et al., 2021) and long short-term memory (LSTM)-\nbased models (Li et al., 2021; Wadud et al., 2022; Arbane et al., 2023 ) are widely-used deep learning \nmethods. These deep learning models can achieve better performance than the traditional models (Yu et \nal., 2019; Wadud et al., 2022), while the main drawback of the CNN and LSTM -based models is that \nthey require a huge amount of manpower to prepare enough training data sets  (Xu & Cai, 2021 ). \nTherefore, pretrained deep learning models (e.g., bidirectional encoder representation from transformers \n(BERT) (Devlin et al. 2018)) are gaining more research interest in recent years. The parameters of the \npretrained models have been adjusted on a large corpus (Devlin et al., 2018) so that the pretrained model \ncan achieve satisfactory performance when the training dataset from a new task is relatively small \n(Zheng et al., 2022). Furthermore, the BERT model has also been applied in seismic analysis. Lv et al. \ncombined the BERT and the TextCNN model to classify micblogs based on seismic intensity (Lv et al., \n2023). Although pretrained models have been used in many applications, in the field of social media, \nthe studies on text classification using  the BERT model and how to improve the generalization \nperformance of the BERT model are limited. Therefore, this study employs the BERT model to establish \na large-language model for social media data and investigates the influence of different factors on the \ngeneralization performance of the BERT model, aiming to develop a  more robust earthquake impact \nanalysis system. \n \n3 Research objective \nFew studies consider filtering weak correlation microblogs before assessment and furthermore \nanalyzing the influence of these weak correlation microblogs. Besides, although pretrained models have \nbeen used in many applications, in the field of social media, the studies on how to improve the \ngeneralization performance of the BERT model are limited.  It also should be noted that, d espite the \nexisting efforts have been made to utilize the contents from social media for earthquake impact \nassessment, there is a notable scarcity of studies that take into account the expression style commonly \nfound in social media data, characterized by colloquial language.   \nTherefore, this study has the following objectives: 1) Proposes an enhanced approach to address \nthe high noise in microblogs for accurate classification of social media texts . The primary strategy \nencompasses the introduction of a few categories to classify and filter microblogs based on their \n \n \nrelationship to the physical and social impacts of earthquakes . On the basis, the first domain -specific \nLanguage Model (LLM) is introduced to achieve accurate classification of social media texts and tackle \ninfluential noisy microblogs in earthquake assessment effectively, 2) Proposes an integrated approach \nto assess both physical and social impacts of earthquakes based on social media texts.  By integrating \npublic opinion trend analysis, sentiment analysis, and keyword-based physical impact quantification, it \nenable effective post -disaster emergency responses to create more resilient cities.  Of particular \nsignificance is that, this study has taken into account the prevalent colloquial language style observed \non social media platforms during the physical impact quantificatio n based on keywords, thereby \nenhancing the accuracy of earthquake severity rating. \n \n4 Methodology \n Fig. 1 shows the proposed approach for the accurate classification of social media texts and rapid \nearthquake impact assessment , which can consider the influence of noisy microblogs. The approach  \nmainly consists of three steps. The first step is data acquisition (Section 4.1). Web crawlers are utilized \nto acquire the information of microblogs after an earthquake occurs, and then preliminary data cleaning \n(deduplication) is performed on the raw data. Then, because there are many irrelevant microblogs in the \nraw data, which will affect the assessment results. Therefore, categories that consider the strength of \ncorrelation with the physical and social impacts of earthquakes are proposed. A largescale dataset is also \ndeveloped for model training. The second step is the development and fine-tuning of the first domain-\nspecific LLM named QuakeBERT for accurate classification and filtering of social media texts (Section \n4.2). In addition, the influence of different factors on the generalization performance of the classification \nmodel is also explored  via several experiments in  Section 5. The final step is the earthquake impact \nassessment ( Section 4.3). We used three methods to assess both physical and social impacts of \nearthquakes, including (1) public opinion trend analysis, (2) sentiment analysis, and (3) keyword-based \nphysical impact analysis. Finally, two earthquake cases with the same magnitude are used for assessment \nand validation (Section 6). Meanwhile, the python toolkits used to implement the approach are also \ndisplayed at the bottom of Fig. 1. \n  \n \n \n \nFig.1. The proposed earthquake impact analysis approach \n \n4.1 Data acquisition \n4.1.1 Data crawling and deduplicating \nBased on Scrapy (Kouzis-Loukas, 2016) and Weibo-search (Chen, 2022), we developed a system \ncrawling the microblogs after a specific earthquake. Input the time (e.g., \"2008-05-12\") and the name \nof the city  (e.g., \"Sichuan Wenchuanâ€) where  an earthquake occurred into the system , and then the \nsystem can automatically obtain and save the information of the microblogs which contain the keywords \n(i.e., name of the city and \"earthquake\") and are published after the earthquake occurred. Aiming to help \nrescue dispatch through earthq uake impact analysis, the system  exclusively crawls the microblogs \nwithin the golden rescue time (i.e., 48 hours) after the earthquake (Macintyre et al., 2006). The crawled \ninformation mainly include s textual contents, publication time, user id, and microblog  id (each \nmicroblog has a unique microblog id). Since the raw data is repetitive, deduplication is performed based \non the microblog id. \n4.1.2 Sentence categories and dataset development \nPrevious studies indicate that  many c rawled microblogs are irrelated with the real earthquake \nimpact. If the text mining-based assessment is performed without filtering these noisy tweets, the results \nwill hardly reflect the real earthquake damage situation  and lead to ineffective decision making . \nAdditionally, we observe that different types of microblog content tend to describe various aspects of \ndisaster-related information, such as loss description and rescue guidance. Therefore, it is necessary to \nclassify the microblogs before performing an earthquake impact assessment. Through the analysis of \ncontents from earthquake microblogs, the microblogs can be divided into six categories including: \"loss \ndescription\", \"education\", \"notification\", \"rescue\", \"related information\", and \"unrelated\". The \ncategories, classification criteria,  the strength of correlation  and corresponding examples of \nclassification are shown in Table 1.  \nLoss \nsituation\nQuakeBERT for automated text \nclassification \nSentence categories and \ndataset development \nToolkits\nWorkflow\nCrawling data\nDeduplicating data\nData acquisition1\nLarge language model for \nearthquake evaluation (QuakeBERT)\n2\nScrapy\n Pandas\nFurther pretraining domain \nlanguage model\nDiversity and quantity of data\nData cleaning methods\nAnalysis of performance \ninfluence factors\nSnowNLP Pandas\nPyTorch\n Transformers\nFinetune\nKeyword-based \nseismic impact \nanalysis\nPublic opinion \ntrend analysis\nEarthquake impact analysis3\nSocial \nimpact\nSentiment \nanalysis\n\n \n \nThe category \"loss description\" includes the microblogs that describe the physical response, \ndamage, and emotion experienced by human beings and the microblogs that describe the physical \ndamage of a specific location. The microblogs in the category \"loss description\" can reflect the seismic \nimpact on human life and the intensity of the earthquake. Hence, the category \"loss description\" is \nstrongly correlated with the earthquake's impact.  \nThe category \"education\" primarily encompasses informative microblogs, frequently associated \nwith science popularization. These microblogs introduce knowledge on self -rescue procedures during \nan earthquake, which contains the keyword reflecting severe damage (i.e., \"panic\", \"casualty\", and \n\"fractured limb\"). However, they do not reflect the actual seismic damage.  \nThe category \"notification\" includes microblogs resembling news reports, typically disseminated \nby official accounts to report the basic information of an earthquake (e.g., the magnitude, location, time, \netc.  of an earthquake). This kind of microblog lacks the details of the damage caused by an earthquake.  \nThe category \"rescue\" includes microblogs that report the news of military or civilian earthquake \nrelief. However, this kind of microblog usually only shows that rescues are being carried out, but lacks \ndetails of the rescue, and thus can hardly reflect the real physical damage.  \nThe category \"related information\" includes the microblogs that talks about the earthquake but are \nnot helpful for physical impact analysis.  The content of microblogs in these four categories cannot \ndirectly reflect the actual earthquake damage. Nevertheless, their quantity correlates with the severity \nof an earthquake and reflects the public's level of attention towards it. Therefore, we cons ider these \ncategories of microblogs to have a weak correlation with the seismic impact. \nBesides, the category \"unrelated\" include s the microblogs that just use the keyword of an \nearthquake as a hot gimmick , but actually, the actual content has nothing to do with the earthquake. \nAlthough the information about an earthquake is mentioned, the main content of the microblog is \nirrelated to the earthquake, which cannot reflect the impact of the earthquake. \nThe development of the largescale datasets is illustrated in Section 5.1. And then we develop the \nfirst domain-specific LLM based on BERT for accurate classification of microblogs, which is illustrated \nin Section 4.2. \n \nTable 1 sentence categories \nCategory Relevance Classification criteria Example \nLoss \ndescription \nStrong describe the response, \ndamage, and emotion \nexperienced by human \nbeings or the physical \ndamage of a specific \nlocation. \n\" In the morning, I was woken up by violent shaking while \nsleeping in a daze. The cupboard also shook and made a loud \nnoise. Fortunately, it stopped after a while. To be honest, I was \nnot afraid at the time, and I didn 't even think about running \ndownstairs. \" \nEducation Weak the knowledge about \nhow to self -rescue \nduring an earthquake  \n\" # Guizhou Bijie 4.4 magnitude earthquake # Earthquake self-\nrescue knowledge that must be remembered: (1) don't panic \nwhen an earthquake occurs, and (2) if the casualty needs a splint \nto mend the fractured limb, you can use plastic bags and \ncardboard to make an emergency splint bandage. Please \n \n \nremember these common senses of earthquake prevention and \nrisk avoidance to save yourself at critical moments. \" \nNotification Weak report the basic \ninformation of an \nearthquake \n\" # 5.1-magnitude earthquake hits Tangshan # According to the \nofficial measurement of the China Earthquake Network, at 6:38 \non July 12, Beijing time, a 5.1 -magnitude earthquake occurred \nin Guye District in Tangshan, Hebei Province, which was also \nfelt in Beijing, Tianjin, and other places. \" \nRescue Weak report the news of \nmilitary or civilian \nearthquake relief  \n\" After an earthquake of magnitude 7.4 occurred in Maduo \ncounty, Guoluo prefecture, Qinghai province at 2:04 a.m. on \nMay 22, the Ministry of Emergency Management and the China \nearthquake administration sent a rescue team overnight to \nMaduo county, Qinghai province to guide and assist localities in \ncarrying out emergency rescue and disaster relief work. The fire \nrescue corps of Qinghai province mobilized five rescue teams \nfrom Xining, Hainan, Yushu, Guoluo, and Xunbao to reinforce \nthe disaster area, carry ou t earthquake rescue and help the \ncitizens resume production and life. \" \nRelated \ninformation \nWeak talks about the \nearthquake but is not \nhelpful for physical \nimpact analysis  \n\"# 4.4-magnitude earthquake in Bijie, Guizhou  # I hope \neveryone is safe. The teacher said that Guizhou is the safest city. \nI didn 't expect that there would be another earthquake in \nGuizhou \" \nUnrelated No contain the keywords \nof an earthquake, but \nthe content is irrelated \nto the earthquake.  \n\" 3 people from Bijie are on the list! The second quarter of 2022 \n'Guizhou Good People List' released\" \n \n4.2 Development of the domain-specific large language model for earthquake \nassessment  \n4.2.1 BERT-based text classification method \nAfter establishing the classification category, the first domain -specific LLM based on BERT, \nQuakeBERT, is developed and fine -tuned for accurate classification and filtering of microblogs. The \ndetailed fine-tuning process and selection of parameters are described in  Section 5.3. Here is only a \nbrief introduction to the architecture of the BERT model. \nThe BERT model mainly consists of three types of layers, including (1) the word embedding layer, \n(2) the encoding layer, and (3) the classification layer. The word embedding layer consists of three parts, \nincluding (1) token embeddings, (2) segmentation em beddings, and (3) position embeddings, so as to \nrealize the conversion of input information into corresponding vector expressions. The encoding layer \nconsists of twelve transformers, which encode all the embeddings from the embedding layer into \ncontextual representations to better understand the contextual semantic information. The classification \nlayer consists of a fully connected layer and a softmax layer, which takes the contextual representation \noutputted by the encoding layer as input to predict the target categories. The specific details of the BERT \nmodel structure can be found in Devlin et al. (2018).  \n \n4.2.2 Analysis of performance influence factors \nThe QuakeBERT model is utilized  to classify microblogs before further analysis , thus the \n \n \ngeneralization performance of the mode l is significant to the subsequent analysis . To investigate the \ninfluence of different factors on the performance of the QuakeBERT model, we conducted a series of \nexperiments in Section 5. The main factors include (1) the diversity and volume of the training datasets, \n(2) domain -specific further pretraining models, and (3) data cleaning methods. These factors were \nchosen for the following reasons, and the corresponding experiments are illustrated in Section 5.2.2 ~ \n5.2.4, respectively.  \nThere are several reasons to investigate how the diversity and volume of the training dataset impact \nthe performance of pretrained models. F irst, the diversity of the training datasets mainly comes from \ntwo aspects, including (1) the distribution of the  categories of microblogs varying from different \nearthquakes, and (2) the characteristics (e.g., geographic location, time) of each microblog varying from \ndifferent earthquakes. Therefore, a model trained on a single earthquake dataset may not have sufficient \ngeneralization capabilities due to variations in data distribution and characteristics. Besides, the volume \nof the training datasets also has a significant impact. However, it is unclear which of these two factors \nhas a greater impact on the generalization performance of the classification model. \nAnother factor under investigation is the influence of domain-specific further pretraining language \nmodels. In most cases, the general-domain corpus (e.g., news corpus, encyclopedia corpus) is used to \npretrain the bert -base-chinese model, whose data distribution is different from the domain corpora \ntargeted to a certain domain (Sun et al., 2019; Zheng et al., 2022). Therefore, directly adopting the bert-\nbase-chinese model is likely to reduce the contribution of domain-specific knowledge embedded in the \ndomain corpora. To investigate the influence of the domain -specific further pretraining models, we \npretrain two domain-specific models, bert-earthquake and bert-earthquake-clean, using the corpus with \nthe same distribution as the target domain. The bert-earthquake model is pretrained using the original \ncorpus, while the bert-earthquake-clean model is pretrained using the cleaned corpus, allowing for a \nconcurrent exploration of the impact of data cleaning on pretraining effectiveness , as illustrated in \nSection 5.2.3. \nLastly, the impact of data-cleaning methods is also analyzed. The performance of the fine-tuning \nprocess may be influenced by certain words within the microblog content of datasets, introducing \npotential confusion during training.  To address this, we implemented data-cleaning methods for both \ndatasets, which encompassed the removal of geographic location words, stop words, and topic tags.  \nIt is worth noting that model tends to take shortcuts, relying heavily on location-based information \nfor classification, thereby compromising its overall generalization capability. Therefore, the objective \nof removing geographic location words is to elimin ate location -based information from the text, \nfacilitating earthquake damage analysis independent of varying geographical contexts in each \nearthquake instance. By removing such information, the model is able to avoid overemphasizing the \nimpact of specific earthquake locations, ensuring a more generalized understanding of earthquake \ndamage.  Importantly, it safeguards against the model fixating solely on the limited earthquake locations \npresent in the training set, promoting a more unbiased and robust fine-tuning process.  \nSimilarly, the removal of stop words involves the elimination of commonly used words that do not \nadd any significant meaning to the text. \nFinally, the removal of topic tags aims to filter out irrelevant content that is often marked with \"#\" \n \n \nsymbols. Similar to the aforementioned analysis, the model also tends to take shortcuts by relying on \ntags for classification. However, many content marked with \"#\" symbols for many data points does not \ncorrespond to the actual content. Specifically, many data in the \"Unrelated\" category typically only \nmention earthquake-related keywords in content marked with \"#\" symbols, but do not provide any \nfurther information related to earthquakes, which merely us es the earthquake keyword as a popular \ngimmick. For exemple, \"The air quality in Inner Mongolia is poor. Residents in Chengdu should wear \nmasks when going out and turn on the air purifier at home. #4.4 -magnitude earthquake in Bijie, \nGuizhou#.\" At the same time, other microblogs strongly or weakly related to the ea rthquake's impact \nmay contain unrelated information in content marked with \"#\" symbols, such as the account name \nreporting the content. For example, \"#Southward View of the World# According to the official \nmeasurement of the China Earthquake Network, At 07 :02 on July 12th, a 2.2 -magnitude earthquake \noccurred in Guye District, Tangshan City, Hebei Province (39.76 degrees north latitude, 118.44 degrees \neast longitude), with a depth of 15 kilometers. Wishing everyone safety and well-being!\" Therefore, to \ninvestigate whether the model might be confused by the information in content marked with \"#\" symbols \nduring training, topic tags are removed during data cleaning. \n \n4.3 Integrated method for earthquake impact assessment \nAfter the classification of microblogs, an integrated method integrating (1) public opinion trend \nanalysis, (2) sentiment analysis, and (3) keyword-based physical impact quantification are used to assess \nboth physical and social impacts of earthquakes. \n4.3.1 Public opinion trend analysis \nPublic opinion trend analysis uses weakly correlated data and strongly correlated data, which aims \nto estimate the impact and scope of an earthquake by counting the number of microblogs released in \neach time period and analyzing the trend of microblogs. This work obtains the releasing time of each \nmicroblog in a unified format (YYYY-MM-DD HH:MM) based on Weibo-search. Then, using pandas \nand matplotlib, the trend of public opinion within 48 hours after the earthquake can be visualized on an \nhourly basis.  \n4.3.2 Sentiment analysis \nSentiment analysis uses weakly relevant data and strongly relevant data, which  is to classify \nwhether the emotion of each microblog is positive or negative. Sentiment analysis of earthquake-related \nmicroblogs can estimate the economic losses and social impacts caused by an earthquake. Negative \nemotions may increase when earthquake damage is severe. Typical positive and negative microblogs \nare shown in Table 2. \nThis study employs the sentiment analysis model in snowNLP (a Python library for Chinese NLP \ntasks) to estimate the sentiment of each microblog. Microblogs with a positive emotion probability \ngreater than 50% are considered positive ones, and vice versa. On this basis, the evolution of the number \nand proportion of two types of microblogs can be calculated and visualized.  \nTable 2. Typical microblogs with two different sentiments \nSentiment Text \n \n \nPositive On April 6, a class of a middle school in Xingwen County, Yibin, Sichuan was reading early when an \nearthquake warning sounded. The students quickly hunched over their heads and squatted to avoid it, \nand then began to evacuate in an orderly manner. The students did not panic, and there were no \ncasualties. \nNegative In Xingwen County, the Qishupo section and the Xinhuachang section of the provincial highway \n(S444) collapsed, causing road interruptions. At present, traffic control has been implemented on the \nabove-mentioned road sections, and vehicles and pedestrians are prohibited from passing through! \n \n4.3.3 Keyword-based physical impact quantification \nKeyword-based physical impact quantification uses strongly relevant data, which can reflect the \nactual loss of earthquakes. First, we established a table of physical impact assessment based on intensity \nkeywords, as shown in Table 3. Table 3 shows the different levels of earthquake impact and the \ncorresponding keywords. According to the seismic intensity scale of China (GB/T 17742â€“2020) and the \nprevious studies (Yao et al., 2021; Li et al., 2021 ), the impact of earthquakes can be divided into four \nlevels. Subsequently, the keywords corresponding to each level are preliminarily determined according \nto the descriptions of the conditions at each level in the seismic intensity scale of China (GB/T 17742â€“\n2020). By analyzing the microblogs, we found that the keywords extracted from the intensity scale were \nformal, which did not match the colloquial language habit of contents in the microblogs. This mismatch \nbetween the informal linguistic patterns and the formal keywords extracted from the intensity scale has \na detrimental impact on the accuracy of assessment. Therefore, we have modified and supplemented the \nkeywords to be more suitable for language habits and peoples' perceptions. Finally, we obtained the \nintensity keyword table, as shown in Table 3. \nThen the microblogs are classified into four levels utilizing the keyword matching method based \non the keywords in Table 3. If a sentence contains keywords in a certain disaster level, this sentence is \nclassified into this level. In some cases , the words belonging to the lower level  only have a few more \nqualifiers than the higher level (e.g., \" slight cracks \" in the second level and \" cracks \" in the third level). \nIf high-level words are matched first, then low-level words will also be matched, so that the low -level \nwords will be misjudged as high-level words. So if a sentence contains multiple keywords in different \nlevels at the same time, the classification priority is first level > second level > third level > fourth level. \nIf a sentence contains no keyword, then we filter this sentence because it provided no information related \nto the earthquake damage. \n \nTable 3. Keywords for different disaster level \nDisaster \nlevel \nHuman feeling House damage Other phenomena Casualties Lifeline condition \nFirst \nlevel \nno feeling, no shock, sleep \nlike a log, didn't know there \nwas an earthquake, can't wake \nup ( in Chinese: æ²¡æœ‰æ„Ÿè§‰, æ²¡\nslight, good ( in \nChinese: è½»å¾®, è‰¯\nå¥½ ) \nslight vibration (in \nChinese: å¾®åŠ¨ ) \nno one was injured \n(in Chinese: æ²¡æœ‰\näººå‘˜å—ä¼¤) \nnormal, no damage \nfound, flat, no \ncracks ( in Chinese:\næ­£å¸¸, æœªå‘ç°å—æŸ\n \n \néœ‡æ„Ÿ,  ç¡æ­», ä¸çŸ¥é“åœ°éœ‡, å«\nä¸é†’ ) \næƒ…å†µ, å¹³æ•´, æ— è£‚\nç¼ ) \nSecond \nlevel \nfeel, standing unsteadily, \nfleeing outside in a panic, \nshaking,  feeling the shock, \nwake up, waking up, running \ndownstairs, kicking, not \ndaring to move (in Chinese: \næœ‰æ„Ÿ, ç«™ç«‹ä¸ç¨³, æƒŠé€ƒå®¤å¤–, \næ‘‡,æœ‰éœ‡æ„Ÿ, æ‘‡é†’, éœ‡é†’, å¾€æ¥¼\nä¸‹è·‘,  è¸¢, ä¸æ•¢åŠ¨ ) \nnoise, dust falling, \nfine cracks, eaves \ntiles falling, bricks \nfalling, slight \ndamage, basically \nintact, shaking, \ntrembling, slight \ncrack, falling off \n(in Chinese: å“, ç°\nåœŸæ‰è½, ç»†è£‚ç¼, \næªç“¦æ‰è½, æ‰ç –, \nè½»å¾®ç ´å, åŸºæœ¬å®Œ\nå¥½, æ™ƒåŠ¨, é¢¤åŠ¨, è½»\nå¾®å¼€è£‚, è„±è½,) \nswinging, ringing, \nshaking, \noverturning, \nmoving, slight \ncracks, \nsandblasting, \nwatering (in \nChinese: æ‘†åŠ¨, å“, \næ‘‡åŠ¨, ç¿»å€’, ç§»åŠ¨, \nè½»åº¦è£‚ç¼, å–·æ²™, \nå†’æ°´) \ntrapped, diverted, \nminor abrasions, \nminor injuries (in \nChinese: è¢«å›°, è½¬\nç§», è½»å¾®æ“¦ä¼¤, è½»\nå¾®ä¼¤) \nrestricted, blocked, \ndamaged, slight \ncracks, no major \nimpact (in Chinese:\nç®¡åˆ¶, é˜»å¡, å—æŸ, \nç»†å¾®è£‚ç¼, æœªé€ æˆ\nè¾ƒå¤§å½±å“) \nThird \nlevel \nbumping, falling, difficulty \nwalking, obvious prolonged, \ndizzy, avoiding danger, \nunsteady, shaking vigorously, \nstaggering, unbalanced (in \nChinese: é¢ ç°¸, æ‘”å€’, è¡Œèµ°å›°\néš¾, æ˜æ˜¾, æ™•, é¿é™©, ç«™ä¸ç¨³, \nä½¿åŠ²æ‘‡, è¸‰è¸‰è·„è·„, å¤±è¡¡) \nmoderately \ndamaged, cracks \n(in Chinese: ä¸­ç­‰\nç ´å, è£‚, å¼€è£‚) \ndrop, crack, \nmoderate damage, \nmisalignment, tilt, \nshock, fall (in \nChinese: æ‰è½, è£‚\nç¼, ä¸­ç­‰ç ´å, é”™\nåŠ¨, å€¾æ–œ, éœ‡è½, å€’) \nburied, wounded, \nfractured, crushed, \ntrapped, buried \nunder pressure, \nconcussion, badly \nwounded (in \nChinese: åŸ‹, å—ä¼¤, \néª¨æŠ˜, è¢«å‹, è¢«å›°, \nè¢«åŸ‹å‹, è„‘éœ‡è¡, \né‡ä¼¤) \nprohibited, closed, \nimpassable, \nrockfall, damaged, \ntorn, rolling stones, \nbacked up, tripped, \ninterrupted, cracks \n(in Chinese: ç¦æ­¢, \nå°é—­, æ— æ³•é€šè¡Œ, \nè½çŸ³, æŸå, æ’•è£‚, \næ»šçŸ³, é€€æœ, è·³é—¸, \nä¸­æ–­, å¼€è£‚) \nFourth \nlevel \nfalling away from the original \nplace, the feeling of being \nthrown up, violent, fierce, \nstrong, serious (in Chinese:  \næ‘”ç¦»åŸåœ°, æœ‰æŠ›èµ·æ„Ÿ, å‰§çƒˆ, \nçŒ›, å‡¶, å¼ºçƒˆ) \nseverely damage, \ndestroy, collapse \n(in Chinese: ä¸¥é‡\nç ´å, æ¯å, åå¡Œ) \ncollapse, severe \ndamage,  rupture, \ndestruction, \ntoppled, landslide, \nsevere (in Chinese: \nå€’å¡Œ, ä¸¥é‡ç ´å, \næ–­è£‚, ç ´å, å€’æ¯, \næ»‘å¡, ä¸¥é‡) \ndead, distress, \nmisfortune, danger, \ndeath (in Chinese: \næ­»äº¡, é‡éš¾, ä¸å¹¸, \nç”Ÿå‘½å±é™©, æ­») \nbreak, landslide, \nsubsidence, \ndeformation, \nheave, collapse, \nshock collapse, \ncollapse, collapsed, \nrolling stone (in \nChinese: æ–­è£‚,  æ»‘\nå¡, å¡Œæ–¹, å˜å½¢, éš†\nèµ·, åå¡Œ, æ²‰é™·, éœ‡\nå®, å®å¡Œ, å¡Œäº†, æ»š\nçŸ³) \n \n5 Experiments on the performance of text classification models \n5.1 Dataset development \n5.1.1 Training dataset development \nBased on scrapy ( Wang et al., 2012 ) and Weibo-search (Chen, 2022), we crawled the disaster -\nrelated microblogs of nine different earthquake s in different locations . Subsequently, we filtered the \n \n \nrepetitive microblogs in the raw data and then manually labeled them to form the training dataset.  A \ntotal number of 6268 disaster-related microblogs were labeled. The locations of the crawled data  are \nshown in Table 4, and the category distribution of the labeled data is shown in Fig. 2. \n \nTable 4 Locations of the crawled data  \nEarthquake information Number of the labeled microblogs \n2020-07-12 5.1 magnitude earthquake in Tangshan, Hebei 3170 \n2019-06-17 5.1 magnitude earthquake in Yibin, Sichuan 1106 \n2017-03-27 5.1 magnitude earthquake in Dali, Yunan 187 \n2018-10-31 5.1 magnitude earthquake in Liangshan, Sichuan 331 \n2022-06-20 4.4 magnitude earthquake in Bijie, Guizhou 434 \n2021-11-17 5.0 magnitude earthquake in Yancheng, Jiangsu 50 \n2022-03-17 5.1 magnitude earthquake in Zhangye, Gansu 134 \n2021-03-19 6.1 magnitude earthquake in Naqu, Xizang 231 \n2021-05-22 7.4  magnitude earthquake in Guoluo, Qinghai 625 \nSum 6268  \n \n   \nFig. 2 Category distribution of the training dataset \n \nFig. 2 shows that the amount of data in the \"unrelated\" category is relatively small, which means \nthe training dataset is imbalanced. The imbalanced dataset is not conducive to model training. Therefore, \nwe collected 751 other irrelevant data from different earthquakes to supplement our training dataset . \nAfter supplementing, the training dataset has a total of 7019 labeled microblogs, and the data distribution \nis shown in Fig. 3.  \n259\n690\n2353\n1082 996 888\n0\n500\n1000\n1500\n2000\n2500\nUnrelated Education Notification Loss\ndescription\nRescue Related\ninformation\nNumber\nLabel\n \n \n   \nFig. 3 Category distribution of the training dataset after supplementing  \n \n5.1.2 Test dataset development \nThe test dataset is used to assess the generalization ability of the well-fine-tuned model. Because \nthe characteristics and distribution of data from different earthquakes are different, we choose the \nvarious earthquakes that are not contained in the training datasets. Then we labeled a total of 1014 \nmicroblogs as the test dataset, to assess the generalization performance of different models. The \nlocations of the crawled data  are shown in Table 5, and the category distribution of the labeled test \ndataset is shown in Fig. 4.  \nTable 5 Distribution of test datasets \nEarthquake information Number of the labeled microblogs \n2020-09-13 Akesu, Xinjiang 29 \n2020-10-06 Ali, Xizang 22 \n2020-10-21 Mianyang, Sichuan 74 \n2020-12-12 Wulumuqi, Xinjiang 79 \n2021-01-04 Leshan, Sichuan 136 \n2021-01-23 Zhaotong, Y unnan 70 \n2021-05-13 Baoshan, Y unnan 92 \n2021-07-14 Aba, Sichuan 160  \n2021-07-23 Luzhou, Sichuan 61 \n2021-08-26 Jiuquan, Gansu 64 \n2022-01-08 Haibei, Qinghai 306  \nSum 1014 \n \n1010\n690\n2353\n1082 996 888\n0\n500\n1000\n1500\n2000\n2500\nUnrelated Education Notification Loss\ndescription\nRescue Related\ninformation\nNumber\nLabel\n \n \n   \nFig. 4 Category distribution of the test dataset \n \n5.2 Experiment settings \nThe analyses of (1) the diversity and volume of the training datasets, (2) domain-specific further \npretraining models, and (3) data cleaning methods are illustrated in Section 5. 2.2 ~ Section 5. 2.4, \nrespectively. To measure the generalization performance of the models, we first constructed a test dataset \n(Section 5.1.2) that did not appear in the training datasets, and then the macro average F1 score (Zheng \net al., 2022; Vani et al., 2019; Zhou et al., 2022 ) is used to evaluate models, as illustrated in Section \n5.2.1.  \n \n5.2.1 Evaluation performance metrics \nThe predictions by the models are compared with the gold standard to evaluate the performance. \nBecause the datasets are highly unbalanced, the macro average F1 score (macro F1) is selected.  \nFirst, we calculate the precision (P), recall (R), and F1 score (F1) for each category as follows: \n      ğ‘ƒ = ğ‘ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡/ğ‘ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘’ğ‘‘ (1) \n  ğ‘… = ğ‘ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡/ğ‘ğ‘¡ğ‘Ÿğ‘¢ğ‘’ (2) \nğ¹1 = 2ğ‘ƒğ‘…/(ğ‘ƒ + ğ‘…) (3) \nwhere ğ‘{ğ‘ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡,ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘’ğ‘‘,ğ‘¡ğ‘Ÿğ‘¢ğ‘’}  denotes the number of {model correctly labeled, model labeled, true} \nelements for each category.  \nThen, we calculate the macro average F1 score to represent the overall performance ( m denotes \nthe number of categories): \n  Macro F1 = (âˆ‘ F1,i\nm\ni\n) /m (4) \n \n5.2.2 Diversity and volume of the training datasets \nWe conducted three experiments to investigate the impact of diversity and volume of training \n26\n146\n285 277\n143\n214\n0\n50\n100\n150\n200\n250\n300\nUnrelated Education Notification Loss\ndescription\nRescue Related\ninformation\nNumber\nLabel\n \n \ndatasets. As for the diversity of the dataset, we conducted experiment 1. The model trained on the dataset \ncontaining microblogs from one earthquake (\"DS-VL\" (short for the dataset with low diversity and large \nvolume)) and the model trained on the dataset containing microblogs from multiple earthquakes (\"DL-\nVL\" (short for the dataset  with large diversity and large volume)) are compared. Note that the two \ndatasets have the same amount of samples, as shown in Table 4. As for the volume of datasets, we \nconducted the second and third experiment. In experiment 2, we compare two  models trained on the \ndatasets with the same distribution and different quantities, which are \"DL-VL\" and \"DL-VEL\" (short \nfor the dataset with large diversity and even larger volume) in Table 4. Besides, in order to eliminate \nthe influence of increased data diversity, we conducted experiment 3, which two models were traind on \nthe datasets \"DS -VS\" (short for the dataset  with low diversity and low volume) and \"DS -VL\".The \nconstruction process of datasets is shown in Section 5.1. \n \nTable 4 Experiments on the diversity and volume of the training datasets \nGroup name Training dataset Model \nDS-VS Tangshan (1585) bert-base-chinese \nDS-VL Tangshan (3170) bert-base-chinese \nDL-VL various earthquakes (3170)  bert-base-chinese \nDL-VEL various earthquakes (7091) bert-base-chinese \nNote: DS-VS is short for the dataset with low diversity and low volume. DS-VL represents the dataset with low diversity \nand large volume. DL-VL is the dataset with large diversity and large volume. DL-VEL is the dataset with large diversity \nand even large r volume. Tangshan (3170) is short for 3170 microblogs from the 2020-07-12 earthquake in Tangshan, \nHebei; Tangshan (1585) is  obtained by randomly selecting half of the data from each class in the Tangshan (3170); \nvarious earthquakes (3170) is short for 3170 microblogs from vario us earthquakes; various earthquakes (7091) is short \nfor 7091 microblogs from various earthquakes. \n \n5.2.3 Further pretrained domain-specific language models  \nWe first crawled 120,000 Sina microblogs as the original corpus. The original corpus contains \nsome noisy text s including geographic location texts, stop words, and topic tags. Implementing data \ncleaning may improve the performance of the further pretrained models, as illustrated in Section 4.2.2. \nHowever, since the original corpus contains a large number of earthquakes of various types, the impact \nof noise and irrelevant information may be minimal. Additionally, the original corpus, without \nperformed data cleaning, maintains a stronger integrity,  which is more conducive to the model \nunderstanding the semantic information of microblog content. Therefore, to investigate whether data \ncleaning improves model effectiveness, we conducted data cleaning to obtain the cleaned corpus. \nThen the mask language model ( Devlin et al., 2018 ) is utilized to pretrain two domain -specific \nmodels, named  bert-earthquake and bert -earthquake-clean. The bert -earthquake model is pretrained \nusing the original corpus, and the bert-earthquake-clean model is pretrained using the cleaned corpus. \nThe models are pretrained with a learning rate of 5 Ã—  10 5 and an epoch of 5 , while also used a 15% \nprobability of masking each token  (Devlin et al., 2018; Zheng et al., 2022). \nOn this basis, to investigate the influence of pretraining on the performance of text classification, \nwe conducted twelve experiments. The corresponding training datasets and pretraining models are \n \n \nshown in Table 5. \n \nTable 5 Experiments on domain-specific further pretraining models \nModel Tangshan (1585) Tangshan (3170) various earthquakes \n(3170) \nvarious earthquakes \n(7091) \nbert-base-chinese Group 1 Group 2 Group 3 Group 4 \nbert-earthquake Group 5 Group 6 Group 7 Group 8 \nbert-earthquake-clean Group 9 Group 10 Group 11 Group 12 \nNote: Tangshan (3170) is short for 3170 microblogs from the 2020-07-12 earthquake in Tangshan, Hebei;  Tangshan \n(1585) is obtained by randomly selecting half of the data from each class in the Tangshan (3170); various earthquakes \n(3170) is short for 3170 microblogs from various earthquakes; various earthquakes (7091) is short for 7091 microblogs \nfrom various earthquakes. \n \n5.2.4 Data cleaning methods \nA series of experiments were conducted to explore the impact of data cleaning methods on the \nmodel performance, as follows. The \"bert-base-chinese\" model is used in this experiment. As illustrated \nin Section 4.2.2, data cleaning methods include stop-word removal, geographic location text removal, \nand topic tag removal. In the experiment, the data cleaning methods are performed on the training and \ntest datasets. Ablation experiments were conducted to investigate the influence of the three data cleaning \nstrategies. The experimental settings are shown in Table 6. In these experiments, we added one more \ndata cleaning strategy at a time to the training dataset. First, we performed stop-word removal (i.e., \"S \ntraining\" in Table 6 ). Then stop -word removal and removal of geographic location text (i.e., \"SG \ntraining\" in Table 6 ) were preformed. Finally, we employed all three data cleaning strategies \nsimultaneously (i.e., \"SGT training\" in Table 6). Furthermore, we analyzed the effect of data cleaning \non the test dataset (i.e., \"SGT test\" in Table 6). Intuitively, cleaning the test dataset is likely to reduce \nthe semantic complexity of the sentences to be classified, thereby improving the classification \nperformance. \n \nTable 6 Experiments on data cleaning methods \n \nTangshan (1585) Tangshan  \n(3170) \nvarious earthquakes \n(3170) \nvarious earthquakes \n(7091) \nO test SGT test O test SGT test O test SGT test O test SGT test \nO training Group 1 Group 2 Group 3 Group 4 Group 5 Group 6 Group 7 Group 8 \nS training Group 9 Group 10 Group 11 Group 12 Group 13 Group 14 Group 15 Group 16 \nSG training Group 17 Group 18 Group 19 Group 20 Group 21 Group 22 Group 23 Group 24 \nSGT training Group 25 Group 26 Group 27 Group 28 Group 29 Group 30 Group 31 Group 32 \nNote: Tangshan (3170) is short for 3170 microblogs from the 2020-07-12 earthquake in Tangshan, Hebei; Tangshan \n(1585) is obtained by randomly selecting half of the data from each class in the Tangshan (3170); various earthquakes \n(3170) is short for 3170 microblogs from various earthquakes; various earthquakes (7091) is short for 7091 microblogs \nfrom various earthquakes. O is short for original; S is short for utilizing stop-word removal; SG is short for utilizing stop-\nword removal and geographic location text removal; SGT is short f or utilizing stop-word removal, geographic location \ntext removal, and topic tag removal. \n \n \n \n5.3 Experiment results \nTo find the model with the best performance, the training dataset is randomly split into the training \ndataset and the validation dataset at a 0.8 : 0.2 ratio, where the training dataset is used to train and update \nthe BERT model, and the validation  dataset is used to cho ose the best combination of the \nhyperparameters and the best model. The test dataset is used to test the best model performance as  \ngeneralization performance. \nFurthermore, the impact of different learning rates (7â€‰Ã—â€‰10âˆ’5, 5â€‰Ã—â€‰10âˆ’5, 3â€‰Ã—â€‰10âˆ’5, and 1â€‰Ã—â€‰10âˆ’5) and \nbatch sizes (8, 16, and 32) were considered by grid search. The optimizer is Adam, and other parameters \nare defaulted except for the initial learning rate. We found out that the model starts to overfit on training \ndataset at around the 10 epoch, so we set the max epoch to 30, which far exceeds the epoch that the \noptimal model requires. Besides, to reduce training time, we add an early stop mechanism. If the model \nperformance on the validation dataset does not improve for 3 epochs , the training process will be \nterminated.  \n \n5.3.1 Influence of diversity and volume of the training dataset on model performance \nThe experiment's results are listed in Table 9. Comparing the results of \"DS-VL\" and \"DL-VL\", it \ncan be found that the performance of the model is significantly improved if the diversity of datasets \nincreases. The macro average F1 score increases by 26.37%. Comparing the results of \"DS-VS\" and \n\"DS-VL\",as well as \"DL-VL\" and \"DL-VEL\", it can be found that increasing data volume significantly \nimproves model performance when data diversity is low, with the macro average F1 score increasing \nby 15.4%. However, the improvement is relatively small when data diversity is high, showing a macro \naverage F1 score increase of 0.7%. Generally, the impact of variables on model performance diminishes \nwith an increase in data volume. However, in scenarios with a large data volume, enhancing data \ndiversity has a more pronounced effect on model performance improvement (26.37%) compared to \nincreasing data volume in situations with a lower dataset (15.4%). Therefore, we can conclude that data \ndiversity is more critical than data volume for enhancing the performance of the microblog classification \nmodel. \n \nTable 9 Experiment results on the diversity and volume of the training dataset \nGroup name Training datasets Macro F1 \nDS-VS Tangshan (1585) 0.4186 \nDS-VL Tangshan (3170) 0.5726 \nDL-VL various earthquakes (3170) 0.8363 \nDL-VEL various earthquakes (7190) 0.8433 \nNote: DS-VS is short for the dataset with low diversity and low volume. DS-VL represents the dataset with low diversity \nand large volume. DL-VL is the dataset with large diversity and large volume. DL-VEL is the dataset with large diversity \nand even large r volume. Tangshan (3170) is short for 3170 microblogs from the 2020-07-12 earthquake in Tangshan, \nHebei; Tangshan (1585) is obtained by randomly selecting half of the data from each class in the Tangshan (3170); \nvarious earthquakes (3170) is short for 3170 microblogs fro m various earthquakes; various earthquakes (7091) is short \nfor 7091 microblogs from various earthquakes.  \n \n \n \n5.3.2 Performance of the further pretrained domain language models \nThe experiment results are listed in Table 10. Comparing the results of three pretrain ed models \ntrained using the Tangshan (1585) and the Tangshan (3170), the bert-earthquake-clean model achieves \nthe best performance, and the bert-earthquake model performs better than the original model. It can be \nfound that when the diversity of the training dataset is small, further pretraining models on the domain \ncorpus can improve the performance of the models. However, comparing the results of three pretrained \nmodels trained using the various earthquakes (3170) and the various earthquakes (7091), the bert-base-\nchinese model achieves the best performance. The F1 scores  for each category  of three pretrain ed \nmodels trained using the various earthquakes (7091) are listed in Table 11. It can be observed that the \nstandard deviation of the F1 scores predicted by the bert-base-chinese model across different categories \nis around 0.05, indicating a relatively strong consistency and stability compared to other models. This \nmodel demonstrates robust performance across different categories of microblogs, ensuring reliable \npredictions under various circumstances. Furthermore, the results show that the bert-base-chinese model \nhas the highest prediction accuracy for the \"loss description\" category, which is strongly correlated with \nthe earthquake's impact. This aspect facilitates subsequent analyses of the physical and social impacts \nof earthquakes. Consequently, it is evident that when the diversity of the training dataset is large, further \npretraining methods may not significantly enhance model performance. \n \nTable 10 Experiment results on further pretraining BERT models \nModel Tangshan \n(1585) \nTangshan \n(3170) \nvarious earthquakes \n(3170) \nvarious earthquakes \n(7091) \nbert-base-chinese 0.4186 0.5726 0.8363 0.8433 \nbert-earthquake 0.4272 0.5792 0.7334 0.7966 \nbert-earthquake-clean 0.4335 0.6103 0.7931 0.8253 \nNote: Tangshan (3170) is short for 3170 microblogs from the 2020-07-12 earthquake in Tangshan, Hebei; Tangshan \n(1585) is obtained by randomly selecting half of the data from each class in the Tangshan (3170); various earthquakes \n(3170) is short for 3170 microblogs from various earthquakes; various earthquakes (7091) is short for 7091 microblogs \nfrom various earthquakes.  \n \nTable 11 The F1 scores for each category on further pretraining BERT models \nCategory bert-base-chinese bert-earthquake bert-earthquake-clean \nUnrelated 0.9057 0.7636 0.7931 \nEducation 0.8897 0.8689 0.9143 \nNotification 0.8390 0.8187 0.8431 \nLoss description 0.8212 0.8111 0.7950 \nRescue 0.8563 0.8179 0.8263 \nRelated information 0.7480 0.6995 0.7802 \nMacro average 0.8433 0.7966 0.8253 \nStandard Deviation 0.0562 0.0581 0.0495 \n \n \n \n5.3.3 Influence of data cleaning on model performance  \nThe experiment's results are listed in Table 12. Comparing the performance of the models trained \nusing the Tangshan (1585) and the Tangshan (3170), the model trained on the \"SGT training\" dataset \nachieves the best performance on  the cleaned test dataset (i.e., \"SGT test\" in Table 12). Besides, the \nperformance of all models on the \"SGT test\" dataset is better than that on the original test dataset, which \nmay be due to the reduced semantic complexity of the \"SGT test\" dataset. When utilizing the Tangshan \n(1585) dataset, the top-performing model achieved an overall best macro average F1 score of 0.4922, \nexhibiting an improvement of 11.71% over the original result of 0.3751. Similarly, with the Tangshan \n(3170) dataset, the overall  best macro average F1 score reached 0.6538, showcasing an 8.12% \nenhancement compared to the original result of 0.5726.  It can be found that when the diversity of the \ntraining dataset is small, the performance of the model can be improved by cleaning the test dataset.  \nHowever, comparing the performance of the models trained using the various earthquakes (3170) \nand the various earthquakes (7091) , the original model achieves the best performance. Besides, the \nperformance of all models on the \"SGT test\" dataset is lower than that on the original test dataset (i.e., \n\"O test\"). The reason may be that the \"SGT test\"  dataset lacks some features that the model uses for \nclassification. So, when the diversity of the training dataset is large, data-cleaning of the training dataset \nand test dataset can not improve the performance of the model. \n \nTable 12 Experiment results on data cleaning methods \n Tangshan (1585) Tangshan (3170) various earthquakes \n (3170) \nvarious earthquakes \n (7091) \nO test SGT test O test SGT test O test SGT test O test SGT test \nO training 0.4186 0.4725 0.5726 0.6423 0.8363 0.7384 0.8433 0.7187 \nS training  0.3980 0.4707 0.5735 0.6146 0.8226 0.7266 0.8278 0.7278 \nSG training  0.3806 0.4858 0.56 0.6385 0.7734 0.6943 0.8191 0.7131 \nSGT training 0.3751 0.4922 0.5773 0.6538 0.765 0.7646 0.7615 0.7723 \nNote: Tangshan (3170) is short for 3170 microblogs from the 2020-07-12 earthquake in Tangshan, Hebei; Tangshan \n(1585) is obtained by randomly selecting half of the data from each class in the Tangshan (3170); various earthquakes \n(3170) is short for 3170 microblogs from various eart hquakes; various earthquakes (7091) is short for 7091 microblogs \nfrom various earthquakes. O is short for original; S is short for utilizing stop-word removal; SG is short for utilizing stop-\nword removal and geographic location text removal; SGT is short f or utilizing stop-word removal, geographic location \ntext removal, and topic tag removal. \n \n5.3.4 Performance comparison with other models \nAfter the above analysis, the best model is the \"bert -base-chinese\" model trained on the dataset \nnamed \"various earthquakes (7091)\" , which is named QuakeBERT. The QuakeBERT  is used for the \nmodel comparison and further physical impact analysis. \nWe further compared our models with three widely -used deep learning -based text classification \nmethods, including (1) TextCNN (Chen, 2015), (2) TextRNN with attention mechanism (TextRNN-Att \nfor short) (Cai et al., 2018 ), and (3) TextRCNN ( Lai et al., 2015 ). The initial parameters of the word \n \n \nembedding layers of these models are pretrained on the Chinese Wikipedia corpora (Li et al., 2018) via \nskip-gram models ( Mikolov et al., 2013 ). Then, the labeled data of 7091 microblogs from various \nearthquakes are used to train the above models. 100 epochs, far more than the best model required, and \na padding size of 256, the same as the  BERT-based model, are selected. The effects of different learning \nrates and batch sizes are considered via grid searches with batch sizes of 64, 32, and 16, and with \nlearning rates of 0.002, 0.001, 0.0005, 0.00025, and 0.0001. The models with the best performance on \nthe validation dataset are used for evaluation on the test dataset. \nThe performance of different text classification models is shown in Fig.5. The QuakeBERT \noutperformed the other deep learning models, which improved the macro F1 score of 23.46%. The \nperformance on the test dataset shows that the QuakeBERT model has strong generalization \nperformance. \n \n \nFig. 5 Performance of different models \n \n6 Case study \nIn this section, we validate our classification method by analyzing the impact of two earthquakes: \nthe Xingwen earthquake in Sichuan Province, China, and the Guye earthquake in Hebei Province, China, \nboth of which have the same magnitude. \n6.1 Earthquake information and data collection \nThe Xingwen earthquake occurred at 7:50 on April 6, 2022, with the epicenter located at 28.22 N \nand 105.03 W, with a magnitude of 5.1, and a depth of 10 km. Its maximum intensity was 6 degrees, \nand the 6-degree zone included seven townships, with an area of approximately 379 km2. Approximately \n750,000 people were affected, with 719 houses damaged, 1,045 residents urgently transferred, 2 \nprovincial roads interrupted, and 5 village roads damaged. \nThe Guye earthquake occurred at 6:38 on July 12, 2020, with the epicenter located at 39.78 N and \n118.44 W, also with a magnitude of 5.1, and a depth of 10 km. Its maximum intensity was 5 degrees, \ncovering four districts, including Guye, Kaiping, Luanzhou, and Qian'an, with an area of approximately \n58.86%\n60.87% 60.05%\n84.33%\n50%\n55%\n60%\n65%\n70%\n75%\n80%\n85%\n90%\nTextCNN TextRNN-Att TextRCNN QuakeBERT\nMacro average F1-score\nModels\n \n \n437 km2 and an affected population of about 2.36 million. The earthquake effects only included minor \ncracks in individual old houses. \nAfter data collection and deduplication, the data volume for the Xingwen earthquake was 2087, \nand the data volume for the Guye earthquake was 5156. Table 13 summarizes the key information about \nthese two earthquakes. \nTable 13 Earthquake information \n Xingwen Earthquake Guye Earthquake \nMagnitude 5.1 5.1 \nDepth 10 km 10 km \nDamage 719 houses damaged, 1,045 residents urgently transferred, 2 \nprovincial roads interrupted, 5 village roads damaged \nminor cracks in individual \nold houses \nV olume of data 2087 5156 \n \n6.2 Results of data analysis and  impact assessment \nFirst, the well-trained QuakeBERT model with the best generalization performance in Section 4 is \nused to classify the collected microblogs to determine their correlation with the physical and social \nimpacts of earthquakes. The results are presented in Table 14, which reveals that over 50 microblogs are \nnot related to the target earthquake keywords. It is essential to filter out these unrelated microblogs for \naccurate analysis since they can negatively impact the results. Subsequently, we filtered out the unrelated \nmicroblogs using the QuakeBERT model and proceeded with the remaining ones for further analysis. \nPublic opinion trend analysis and sentiment analysis use weakly correlated data and strongly correlated \ndata. Keyword-based physical impact analysis uses strongly relevant data, which can reflect the actual \nloss of earthquakes. \n \nTable 14 sentence categories \nCategory Relevance Guye Xingwen \nLoss description Strong 864 289 \nEducation Weak 466 518 \nNotification Weak 2784 283 \nRescue Weak 750 438 \nRelated information Weak 240 556 \nUnrelated No 52 3 \nSum  5156 2087 \n \n6.2.1 Public opinion trend analysis \nFig. 6 depicts the temporal evolution of public opinion in the 48 hours following the occurrence of \nthe two earthquakes. Both earthquakes experienced a surge in public attention within the first hour after \nthe event, followed by a rapid decline. It is noteworthy that the evolution tends to end near 48 hours. \nHowever, it should be emphasized that the peak volume of public opinion only indicates the level \nof attention the earthquake event received, and is not necessarily reflective of the actual damage caused. \nTaking this study as an example, the Xingwen earthquake resulted in relatively high losses. Yet, its peak \n \n \nvolume of public opinion was significantly lower compared to that of the Guye earthquake. This is due \nto the fact that the Xingwen earthquake occurred in an area with a population of only 750,000, which is \nsignificantly lower than the population of approximately 2.36 million in the area affected by the Guye \nearthquake. Furthermore, the Guye earthquake was also felt in nearby large cities such as Beijing and \nTianjin, contributing to a higher volume of online discussions. \n \nFig. 6 Public opinion trend of the two earthquakes \n \n6.2.2 Sentiment analysis \nFig. 7 (a) & 7 (b) depict the trends of positive and negative microblogs within 48 hours after the \nGuye and Xingwen earthquakes, respectively. The proportion of negative emotions in microblogs after \nthe Xingwen earthquake was significantly higher than that of the Guye earthquake, which is consistent \nwith the loss situation discussed in Section 6.1. The content of microblogs in the early period after an \nearthquake mainly comprises spontaneous discussions by people in the vicinity of the epicenter and \nsurrounding areas affected by the earthquake. Therefore, a higher proportion of negative microblogs \nwithin 2 hours after an earthquake indicates a more powerful earthquake with more severe losses. These \nresults align with previous studies, which suggest that sentiment may be even more sensitive to damage \n(Kryvasheyeu et al., 2016) and that citizens tend to express negative sentiment on social media during \na disaster (Wu & Cui, 2018). Furthermore, as disaster relief and rescue efforts progress, the sentiment \nof microblogs tends to become more positive over time. \nThus, monitoring the emotional proportion of microblogs in real -time after an earthquake can \nprovide a reference for early disaster relief decisions and help estimate the damage caused by the \nearthquake. \n0\n200\n400\n600\n800\n1000\n0 4 8 12 16 20 24 28 32 36 40 44 48\nNumber of blogs\nHours after the earthquake\nGuye\nXingwen\n \n \n \n(a) Guye \n \n(b) Xingwen \nFig. 7. Sentiment trend of the two earthquakes (a) Guye (b) Xingwen \n \n6.2.3 Keyword-based physical impact quantification \nUsing the keyword-based physical impact analysis method described in Section 4.3.3, the disaster \nlevels of each microblog of the two earthquakes are assessed. To demonstrate the effectiveness of the \nproposed classification method, we compared the analysis results using the original unclassified data \nwith the strongly correlated data (i.e., the data in the \"loss description\" category) classified by the \nQuakeBERT model. The number of microblogs with different disaster levels is shown in Fig. 8. From \nFig. 8, we can observe the following phenomena. (1) the data has a low signal-to-noise ratio, and noise \ndata has a significant impact on determining the severity of earthquake losses. (2) The distribution of \nmicroblogs with different loss levels before and after classification is inconsistent, indicating that noise \ndata will affect the assessment of earthquake loss. (3) From the analysis of the two earthquakes, it was \nobserved that the signal -to-noise ratio at the second and third levels was relatively lower, which may \ncause misleading results in the analysis process. For example, this can result in underestimating the \ndamage for earthquakes with severe damage and overestimating the damage for earthquakes with low \ndamage. \nFig. 9(a) shows that, before classification, the proportion of microblogs in the fourth level (the most \n0\n200\n400\n600\n800\n1000\n0 4 8 12 16 20 24 28 32 36 40 44 48\nNumber of blogs\nHours after the earthquake\nGuye positive\nGuye negative\n0\n50\n100\n150\n200\n250\n300\n0 4 8 12 16 20 24 28 32 36 40 44 48\nNumber of blogs\nHours after the earthquake\nXingwen positive\nXingwen negative\n \n \nsevere level) of the Guye earthquake exceeded that of the Xingwen earthquake, suggesting that the \nimpacts of the Guye earthquake were more severe than those of the Xingwen earthquake. However, the \nassessment results did not align with the actual loss situation, as discussed in Section 6.1. This could be \nattributed to the presence of microblogs with weak or no correlation in the unclassified dataset. Such \nmicroblogs contain many keywords listed in Table 3, but they do not reflect the specific earthquake loss \nsituation, thereby affecting the assessment results. On the other hand, only the strongly correlated \nmicroblogs were retained for analysis after classification. Thus, Fig. 9(b) shows that after classification, \nthe proportion of microblogs in the fourth level for the Guye earthquake was l ower than that for the \nXingwen earthquake, indicating that the impacts of the Guye earthquake were weaker than those of the \nXingwen earthquake. The assessment results were consistent with the actual loss situation. \n \n(a) Guye earthquake                                          (b) Xingwen earthquake \nFig. 8. Number of microblogs with different disaster levels (a) Guye earthquake (b) Xingwen earthquake \n \n \n      \n     (a) before classification                                           (b) after classification \nFig. 9. Percentage of microblogs with different disaster levels (a) before classification (b) after classification \n \n7 Discussion \n7.1 Comparison with previous research \nDespite the existing efforts on the the classification categories and earthquake analysis  of \nmicroblogs, many of them lack comprehensive consideration, as shown in Table 15. \n \nTable 15 Literature summary \n0\n500\n1000\n1500\n2000\nFirst Second Third Fourth\nNumber\nDifferent disaster levels\nbefore classification\nafter classification\n0\n200\n400\n600\n800\nFirst Second Third Fourth\nNumber\nDifferent disaster levels\nbefore classification\nafter classification\n5.5%\n51.8%\n33.7%\n9.1%\n2.2%\n38.9%\n50.0%\n8.9%\n0%\n10%\n20%\n30%\n40%\n50%\n60%\nFirst Second Third Fourth\nPercentage \nDifferent disaster levels\nGuye\nXingwen\n0.2%\n36.8%\n46.0%\n17.0%\n8.3%\n46.6%\n25.3%\n19.8%\n0%\n10%\n20%\n30%\n40%\n50%\n60%\nFirst Second Third Fourth\nPercentage \nDifferent disaster levels\nGuye\nXingwen\n \n \nLiterature large-scale model physical impacts social impacts colloquial  \nrepresentation \nnoisy microblogs \nYao et al., 2021 Ã— âˆš Ã— âˆš Ã— \nLi et al., 2021 Ã— âˆš Ã— Ã— Ã— \nXing et al., 2021 âˆš âˆš Ã— Ã— Ã— \nLv et al., 2023 Ã— âˆš Ã— Ã— Ã— \nDasari et al., 2023 âˆš Ã— Ã— Ã— Ã— \nThis study âˆš âˆš âˆš âˆš âˆš \n \nThis work proposed an enhanced approach to accurate classification of social media texts for rapid \nearthquake impact assessment. Compared to previous research , this work contributes to the body of \nknowledge on four main levels: \n(1) To reduce the influence of noisy microblogs, this work proposes the first domain-specific large \nlanguage model (LLM)  for accurate classification and filtering of microblogs related to earthquakes. \nFirst, sentence categories that consider the relevance of microblogs are established, and the largescale \ntraining and testing datasets are established. Then based on the experiments, a BERT-based model for \nmicroblog classification (QuakeBERT) with high generalization performance is obtained. Subsequently, \nafter classification, public opinion trend analysis , sentiment analysis , and keyword-based physical \nimpact analysis were carried out to assess the physical and social impacts of earthquakes. Finally, two \nearthquakes of Xingwen and Guye with the same focal depth and magnitude are used for comparative \nanalysis. The analysis results show that noisy microblogs have a significant influence on the keyword-\nbased physical impact analysis method, and will induce errors in the physical assessment. The proposed \ntext class ification method can reduce the effect of noisy microblogs and improve the accuracy of \nkeyword-based physical impact analysis. \n(2) To improve the generalization performance of the QuakeBERT model, this work analyzes \nseveral influential factors, including the diversity and volume of the training dataset, further pretraining \nmodels, and data cleaning methods. The results show that the diversity and volume of the training dataset \nhave the greatest impact on the generalization performance. In scenarios with a large data volume, \nenhancing data diversity has a more pronounced effect on model performance improvement (26.37%) \ncompared to increasing data volume in situations with a lower dataset (15.4%). When the diversity and \nvolume of training data are small, data cleaning and further pretraining models are beneficial to improve \nthe generalization performance. Besides, the QuakeBERT model outperformed the other deep learning \nmodels, which improved the macro F1 score of 23.46%.  \n(3) To achieve rapid physical impact quantification, this study established a keyword database \nrelated to rapid physical loss based on the seismic intensity scale of China. Then the keyword database \nwas modified and supplemented to be more suitable for language habits and peoples' perception s of \nsocial media. Utilizing the keyword database, keyword-based physical impact analysis can be performed. \n(4) To illustrate the advantages and disadvantages of different analysis methods for assessing the \nphysical and social impacts of earthquakes, two earthquakes of Xingwen and Guye with the same \nmagnitude and focal depth are used for comparative analysis. The results can reach the following \nconclusions. 1) The magnitude of the earthquake is the same, but the physical losses caused by the \n \n \nearthquake are different. 2) The results of public opinion trend analysis are related to the degree of \npeople's attention but have little correlation with the actual earthquake loss. 3) The proportion of \nnegative emotions in the sentiment analysis is related to the actual physical loss. The more proportion \nof negative emotions is, the more severe the physical damage caused by an earthquake.  4) The keyword-\nbased physical impact analysis will be easily affected by the noisy microblogs. After removing the noise \nmicroblogs, the analysis results of keyword-based physical impact analysis have a high correlation with \nthe actual physical loss. \n \n7.2 Theoretical contributions \nIn order to address the deficiencies in existing literature regarding the accurate classification of \nmicroblogs and the rapid assessment of earthquake impacts, we conducted this study, making significant \ncontributions in the following aspects: \nFirst, we introduce a few categories to classify and filter microblogs based on their relationship to \nthe physical and social impacts of earthquakes, considers noisy microblogs that fail to reflect the actual \nextent of earthquake damage. This categorization scheme serves as a quantitative criterion for assessing \nthe physical impacts of earthquakes. \nSecond, to expend, we propose the first domain -specific Language Model (LLM) to achieve \naccurate classification of social media texts and tackle influential noisy microblogs in earthquake \nassessment effectively. This approach significantly enhances classification accuracy and facilitates the \nacquisition of intricate and varied characteristics exhibited in earthquake-related microblogs s. It \nspecifically considers the presence of noisy microblogs, and other relevant features, thereby enhancing \nthe overall performance of the classification process. \nFinally, we contribute to an integrated approach  to assess both physical and social impacts of \nearthquakes based on social media texts , considers the usage of colloquial language . By integrating \npublic opinion trend analysis, sentiment analysis, and keyword-based physical impact quantification, it \nenable effective post-disaster emergency responses to create more resilient cities. \n \n7.3 Practical contributions \nIn this study, we provide a more precise representation of the actual situation regarding earthquake \nimpacts, enabling more effective post -disaster emergency responses to create more resilient cities, \nprimarily manifested in the following aspects: \nFirst, it is imperative to consider the substantial impact of noisy microblogs on earthquake impact \nassessment. The enhanced approach porposed in this study enables rapid and accurate classification of \nearthquake-related social media texts , effectively addresses these issue, and facilitates rapid \nquantification of physical impacts. \nSecond, the governments' time-sensitive responses rely on timely and accurate information about \nthe earthquake situations to make effective response decisions and improve management strategies. The \nintegrated approach proposed in this study allows for assessment of both physical and social impacts of \nearthquakes. Compared to previous research s, this study considers the prevalent colloquial language \nstyle observed on social media platforms and provides quantitative evaluation of physical impacts and \n \n \nqualitative evaluation of social impacts, offering a more comprehensive and rapid assessment. This \napproach provides decision-makers and managers with more effective decision support. \n \n7.4 Limitations and future works \nLimitations to this study need to be addressed in future work:  \n(1) The amount of microblogs that can be obtained by using the web crawler is limited. In future \nwork, it is recommended to cooperate with social media companies (e.g., Twitter and Sina Weibo) to \nobtain more data. \n(2) In addition, due to privacy considerations, the geo -location of tweets is not available unless \nusers actively elect. This is the main obstacle to quantitative and accurate analysis of earthquake damage \nin specific locations. Therefore, in the future, m ultiple data source integrating methods should be \nconsidered for rapid earthquake damage analysis. For example, mobile phone signaling data can be \nintegrated to provide effective location information (Xing et al., 2021). \n \n8 Conclusion \nEarthquakes have a profound impact on human societies, and timely access to information about \nthe scope and extent of disasters is essential for post -disaster decision-making and emergency relief \noperations. However, traditional post -disaster damage assessment methods are time -consuming and \nrequire substantial labor and resources. Social media data have become more and more valuable for the \ndevelopment of resilient and sustainable cities, which can help assess damage at a lower cost. However, \nthe social media data are usually overwhelmed with plenty of noisy information, which will influence \nthe analysis results. This study proposed the first domain-specific LLM for accurate classification and \nfiltering of micr oblogs to enhance earthquake impact assessment. Speci fically, first, this work \nestablishes sentence categories that can consider the relevance of microblogs with the physical and \nsocial impacts of earthquakes and a largescale datasets. Then, a BERT-based classification model with \nhigh generalization performance (QuakeBERT) is trained to filter and classify noisy microblogs. \nSubsequently, a keyword database related to rapid physical loss assessment was establish ed for the \nkeyword-based physical impact analysis. Finally, the study conducts a comparative analysi s of two \nearthquakes of Xingwen and Guye with the same magnitude and focal depth. The analysis results show \nthe following conclusions: \n(1) Noisy microblogs have a significant influence on the keyword-based physical impact analysis \nmethod and will induce errors in the physical assessment.  \n(2) The proposed classification method based on a large -language model can reduce the effect of \nnoisy microblogs and improve the accuracy of keyword-based physical impact quantification.  \nIn addition, this work systematically analyzes several influential factors (i.e., the diversity and \nvolume of the training dataset, further pretraining BERT model, and data cleaning methods) of the \ngeneralization performance of the QuakeBERT model. The results show the following conclusions: \n(1) The data diversity and data volume have the greatest influence on the generalization \nperformance of the model.  \n \n \n(2) When the diversity and volume of training data are small, data cleaning and further pre-training \nmodels are beneficial to improve the generalization performance of the model.  \n(3) The QuakeBERT model outperformed the other deep learning models, which improved the \nmacro average F1 score of 23.46%. This shows that t he QuakeBERT model has better generalization \nperformance and is more suitable for filtering irrelevant microblogs.  \nTo summarize, the proposed approach, datasets, and large-language models can provide valuable \ninsights for the development of resilient and sustainable cities with rapid impact assessment of \nearthquakes based on social media texts. \n \nAcknowledgment \nThe authors are grateful for the financial support received from the National Natural Science \nFoundation of China (No. 52238011 , 72091512) and the Tencent Foundation through the XPLORER \nPRIZE.  \n \n \nReferencesï¼š \n[1] Lu, X., McKenna, F., Cheng, Q., Xu, Z., Zeng, X., & Mahin, S. A. (2020). An open-source framework for regional \nearthquake loss estimation using the city-scale nonlinear time history analysis. Earthquake Spectra, 36(2), 806-831. \n[2] Cui, P., Chen, X. Q., Zhu, Y . Y ., Su, F. H., Wei, F. Q., Han, Y . S., ... & Zhuang, J. Q. (2011). The Wenchuan \nearthquake (May 12, 2008), Sichuan province, China, and resulting geohazards. Natural Hazards, 56, 19-36. \n[3] DesRoches, R., Comerio, M., Eberhard, M., Mooney, W., & Rix, G. J. (2011). Overview of the 2010 Haiti \nearthquake. Earthquake spectra, 27(1_suppl1), 1-21. \n[4] Mori, N., Takahashi, T., Yasuda, T., & Yanagisawa, H. (2011). Survey of 2011 Tohoku earthquake tsunami \ninundation and runâ€up. Geophysical research letters, 38(7). \n[5] Hong, H., You, J., & Bi, X. (2016). The Ludian earthquake of 3 August 2014.  Geomatics, Natural Hazards and \nRisk, 7(2), 450-457. \n[6] Ahadzadeh, S., & Malek, M. R. (2021). Earthquake damage assessment based on user generated data in social \nnetworks. Sustainability, 13(9), 4814. \n[7] Papadopoulos, A. N., BÃ¶se, M., Danciu, L., Clinton, J., & Wiemer, S. (2023). A framework to quantify the \neffectiveness of earthquake early warning in mitigating seismic risk. Earthquake Spectra, 39(2), 938-961. \n[8] Yu, M., Huang, Q., Qin, H., Scheele, C., & Yang, C. (2019). Deep learning for real -time social media text \nclassification for situation awareness â€“using Hurricanes Sandy, Harvey, and Irma as case studies.  International \nJournal of Digital Earth, 12(11), 1230-1247. \n[9] Lowande, R. D. S., & Sevil, H. E. (2023). Feasibility of Visual Question Answering (VQA) for Post -Disaster \nDamage Detection Using Aerial Footage. Applied Sciences, 13(8), 5079. \n[10] Yao, K., Yang, S., & Tang, J. (2021). Rapid assessment of seismic intensity based on Sina Weiboâ€”A case study of \nthe changning earthquake in Sichuan Province, China.  International Journal of Disaster Risk Reduction , 58, \n102217. \n[11] Adegoke, D. (2023). A Systematic Review of Big Data and Digital Technologies Security Leadership Outcomes \nEffectiveness during Natural Disasters. Sustainable Futures, 100113. \n[12] Chen, L., Buscher, M., & Hu, Y . (2020). Crowding Out the Crowd: The Transformation of Network Disaster \nCommunication Patterns on Weibo. \n[13] Kundu, S., Srijith, P. K., & Desarkar, M. S. (2018, August). Classification of short-texts generated during disasters: \na deep neural network based approach. In  2018 IEEE/ACM International Conference on Advances in Social \nNetworks Analysis and Mining (ASONAM) (pp. 790-793). IEEE. \n[14] Tounsi, A., & Temimi, M. (2023). A systematic review of natural language processing applications for \nhydrometeorological hazards assessment. Natural hazards, 116(3), 2819-2870. \n[15] Imran, M., Elbassuoni, S., Castillo, C., Diaz, F., & Meier, P. (2013, May). Practical extraction of disaster-relevant \ninformation from social media. In Proceedings of the 22nd international conference on world wide web (pp. 1021-\n \n \n1024). \n[16] Li, L., Bensi, M., Cui, Q., Baecher, G. B., & Huang, Y . (2021). Social media crowdsourcing for rapid damage \nassessment following a sudden-onset natural hazard event. International Journal of Information Management, 60, \n102378. \n[17] Lv, Q., Liu, W., Li, R., Yang, H., Tao, Y ., & Wang, M. (2023). Classification of Seismaesthesia Information and \nSeismic Intensity Assessment by Multi-Model Coupling. ISPRS International Journal of Geo -Information, 12(2), \n46. \n[18] Lin, B., Zou, L., Duffield, N., Mostafavi, A., Cai, H., Zhou, B., ... & Abedin, J. (2022). Revealing the linguistic and \ngeographical disparities of public awareness to Covid -19 outbreak through social media.  International Journal of \nDigital Earth, 15(1), 868-889. \n[19] Zou, L., Lam, N. S., Cai, H., & Qiang, Y . (2018). Mining Twitter data for improved understanding of disaster \nresilience. Annals of the American Association of Geographers, 108(5), 1422-1441. \n[20] Chen Y , Wang Q, Ji W. Rapid assessment of disaster impacts on highways using social media[J]. Journal of \nManagement in Engineering, 2020, 36(5): 04020068. \n[21] Ogie, R. I., James, S., Moore, A., Dilworth, T., Amirghasemi, M., & Whittaker, J. (2022). Social media use in \ndisaster recovery: A systematic literature review. International Journal of Disaster Risk Reduction, 102783. \n[22] Zhang, C., Fan, C., Yao, W., Hu, X., & Mostafavi, A. (2019). Social media for intelligent public information and \nwarning in disasters: An interdisciplinary review. International Journal of Information Management, 49, 190-207. \n[23] Li, L., Bensi, M., & Baecher, G. (2023). Exploring the potential of social media crowdsourcing for post-earthquake \ndamage assessment. International Journal of Disaster Risk Reduction, 98, 104062. \n[24] Chen, Y ., & Ji, W. (2022). Estimating public demand following disasters through Bayesian -based information \nintegration. International Journal of Disaster Risk Reduction, 68, 102713. \n[25] Huang, Q., & Xiao, Y . (2015). Geographic situational awareness: mining tweets for disaster preparedness, \nemergency response, impact, and recovery. ISPRS international journal of geo-information, 4(3), 1549-1568. \n[26] Xing, Z., Zhang, X., Zan, X., Xiao, C., Li, B., Han, K., ... & Liu, J. (2021). Crowdsourced social media and mobile \nphone signaling data for disaster impact assessment: A case study of the 8.8 Jiuzhaigou earthquake.  International \nJournal of Disaster Risk Reduction, 58, 102200. \n[27] Zekkos, D., Tsavalas -Hardy, A., Mandilaras, G., & Tsantilas, K. (2019, June). Using social media to assess \nearthquake impact on people and infrastructure: Examples from earthquakes in 2018. In  Proc. 2nd Int. Conf. \nNatural Hazards Infrastructure (pp. 1-10). \n[28] Binsaeed, K., Stringhini, G., & Youssef, A. E. (2020). Detecting spam in Twitter microblogging services: A novel \nmachine learning approach based on domain popularity. Int. J. Adv. Comput. Sci. Appl, 11. \n[29] Wang, Y ., Wang, T., Ye, X., Zhu, J., & Lee, J. (2015). Using social media for emergency response and urban \nsustainability: A case study of the 2012 Beijing rainstorm. Sustainability, 8(1), 25. \n \n \n[30] Ragini, J. R., & Anand, P. R. (2016, December). An empirical analysis and classification of crisis related tweets. \nIn 2016 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC)  (pp. 1-\n4). IEEE. \n[31] Zheng, Z., Zhou, Y . C., Lu, X. Z., & Lin, J. R. (2022). Knowledge -informed semantic alignment and rule \ninterpretation for automated compliance checking. Automation in Construction, 142, 104524. \n[32] Dasari, S. K., Gorla, S., & PVGD, P. R. (2023). A stacking ensemble approach for identification of informative \ntweets on twitter data. International Journal of Information Technology, 1-12. \n[33] Devaraj, A., Murthy, D., & Dontula, A. (2020). Machine -learning methods for identifying social media -based \nrequests for urgent help during hurricanes. International Journal of Disaster Risk Reduction, 51, 101757. \n[34] Xing, Z., Zhang, X., Zan, X., Xiao, C., Li, B., Han, K., ... & Liu, J. (2021). Crowdsourced social media and mobile \nphone signaling data for disaster impact assessment: A case study of the 8.8 Jiuzhaigou earthquake.  International \nJournal of Disaster Risk Reduction, 58, 102200. \n[35] Wadud, M. A. H., Kabir, M. M., Mridha, M. F., Ali, M. A., Hamid, M. A., & Monowar, M. M. (2022). How can we \nmanage offensive text in social media -a text classification approach using LSTM -BOOST. International Journal \nof Information Management Data Insights, 2(2), 100095. \n[36] Arbane, M., Benlamri, R., Brik, Y ., & Alahmar, A. D. (2023). Social media -based COVID -19 sentiment \nclassification model using Bi-LSTM. Expert Systems with Applications, 212, 118710. \n[37] Xu, X., & Cai, H. (2021). Ontology and rule -based natural language processing approach for interpreting textual \nregulations on underground utility infrastructure. Advanced Engineering Informatics, 48, 101288. \n[38] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre -training of deep bidirectional transformers \nfor language understanding. arXiv preprint arXiv:1810.04805. \n[39] Zheng, Z., Lu, X. Z., Chen, K. Y ., Zhou, Y . C., & Lin, J. R. (2022). Pretrained domain-specific language model for \nnatural language processing tasks in the AEC domain. Computers in Industry, 142, 103733. \n[40] Kouzis-Loukas, D. (2016). Learning scrapy. Livery Place: Packt Publishing. ISBN: 9781784399788 \n[41] Chen, L.(2022, October 5).Weibo-search. [Web log post].Retrieved from https://github.com/dataabc/weibo-search \n[42] Macintyre, A. G., Barbera, J. A., & Smith, E. R. (2006). Surviving collapsed structure entrapment after earthquakes: \na â€œtime-to-rescueâ€ analysis. Prehospital and disaster medicine, 21(1), 4-17. \n[43] Sun, C., Qiu, X., Xu, Y ., & Huang, X. (2019). How to fine -tune bert for text classification?. In  Chinese \nComputational Linguistics: 18th China National Conference, CCL 2019, Kunming, China, October 18 â€“20, 2019, \nProceedings 18 (pp. 194-206). Springer International Publishing. \n[44] GB/T 17742, 2020. The Chinese seismic intensity scale. (in Chinese). \n[45] Vani, S., & Rao, T. M. (2019, April). An experimental approach towards the performance assessment of various \noptimizers on convolutional neural network. In  2019 3rd international conference on trends in electronics and \ninformatics (ICOEI) (pp. 331-336). IEEE. \n[46] Zhou, Y . C., Zheng, Z., Lin, J. R., & Lu, X. Z. (2022). Integrating NLP and context-free grammar for complex rule \ninterpretation towards automated compliance checking. Computers in Industry, 142, 103746. \n[47] Wang, J., & Guo, Y . (2012, October). Scrapy-based crawling and user-behavior characteristics analysis on taobao. \n \n \nIn 2012 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery (pp. 44-52). \nIEEE. \n[48] Chen, Y . (2015). Convolutional neural network for sentence classification (Master's thesis, University of Waterloo).  \n[49] Cai, J., Li, J., Li, W., & Wang, J. (2018, December). Deeplearning model used in text classification. In 2018 15th \ninternational computer conference on wavelet active media technology and information processing (ICCWAMTIP) \n(pp. 123-126). IEEE.   \n[50] Lai, S., Xu, L., Liu, K., & Zhao, J. (2015, February). Recurrent convolutional neural networks for text classification. \nIn Proceedings of the AAAI conference on artificial intelligence (V ol. 29, No. 1). \n[51] Li, S., Zhao, Z., Hu, R., Li, W., Liu, T., & Du, X. (2018). Analogical reasoning on chinese morphological and \nsemantic relations. arXiv preprint arXiv:1805.06504. \n[52] Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and \nphrases and their compositionality. In Advances in neural information processing systems (pp. 3111-3119). \n[53] Kryvasheyeu, Y ., Chen, H., Obradovich, N., Moro, E., Van Hentenryck, P., Fowler, J., & Cebrian, M. (2016). Rapid \nassessment of disaster damage using social media activity. Science advances, 2(3), e1500779.  \n[54] Wu, D., & Cui, Y . (2018). Disaster early warning and damage assessment analysis using social media data and geo-\nlocation information. Decision support systems, 111, 48-59.  \n ",
  "topic": "Social media",
  "concepts": [
    {
      "name": "Social media",
      "score": 0.4863170087337494
    },
    {
      "name": "Computer science",
      "score": 0.39197471737861633
    },
    {
      "name": "Seismology",
      "score": 0.36002418398857117
    },
    {
      "name": "Natural language processing",
      "score": 0.33511579036712646
    },
    {
      "name": "Geology",
      "score": 0.24019107222557068
    },
    {
      "name": "World Wide Web",
      "score": 0.1525731384754181
    }
  ]
}