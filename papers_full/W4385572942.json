{
    "title": "Gradient-based Constrained Sampling from Language Models",
    "url": "https://openalex.org/W4385572942",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2181361850",
            "name": "Sachin Kumar",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2610306377",
            "name": "Biswajit Paria",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2234266251",
            "name": "Yulia Tsvetkov",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4221144473",
        "https://openalex.org/W4310638782",
        "https://openalex.org/W3016240002",
        "https://openalex.org/W2963206148",
        "https://openalex.org/W2982756474",
        "https://openalex.org/W3176618728",
        "https://openalex.org/W4299567010",
        "https://openalex.org/W3169017236",
        "https://openalex.org/W2167433878",
        "https://openalex.org/W3101033885",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3170432046",
        "https://openalex.org/W3214281485",
        "https://openalex.org/W2738371943",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W3085190015",
        "https://openalex.org/W4287026929",
        "https://openalex.org/W4285230133",
        "https://openalex.org/W2962969034",
        "https://openalex.org/W2963631950",
        "https://openalex.org/W4206232983",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4281690218",
        "https://openalex.org/W621546036",
        "https://openalex.org/W3171218751",
        "https://openalex.org/W2110529144",
        "https://openalex.org/W4287028715",
        "https://openalex.org/W4224035735",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4226288657",
        "https://openalex.org/W2964268978",
        "https://openalex.org/W3099729825",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W3102914525",
        "https://openalex.org/W3103451137",
        "https://openalex.org/W4320527817",
        "https://openalex.org/W2971307358",
        "https://openalex.org/W1579788182",
        "https://openalex.org/W3100355250",
        "https://openalex.org/W2567639685",
        "https://openalex.org/W2968297680",
        "https://openalex.org/W1545319692",
        "https://openalex.org/W4294238563",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2997195635",
        "https://openalex.org/W4285304933",
        "https://openalex.org/W2914442349",
        "https://openalex.org/W2959300817",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W2760656271",
        "https://openalex.org/W4310695675",
        "https://openalex.org/W2606974598",
        "https://openalex.org/W1555683961",
        "https://openalex.org/W2973049837",
        "https://openalex.org/W2951770285",
        "https://openalex.org/W3127062506",
        "https://openalex.org/W3102187933",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W3104260136",
        "https://openalex.org/W2964343359",
        "https://openalex.org/W3115731421",
        "https://openalex.org/W4221143575",
        "https://openalex.org/W2973379954",
        "https://openalex.org/W2906152891",
        "https://openalex.org/W4287180975",
        "https://openalex.org/W3119800657",
        "https://openalex.org/W2135889985",
        "https://openalex.org/W1971159979",
        "https://openalex.org/W4225141070",
        "https://openalex.org/W2963352809",
        "https://openalex.org/W2978670439"
    ],
    "abstract": "Large pretrained language models are successful at generating fluent text but are notoriously hard to controllably sample from. In this work, we study constrained sampling from such language models, i.e., generating text that satisfies user-defined constraints, while maintaining fluency and model’s performance in a downstream task. We propose MuCoLa—a sampling procedure that combines the log-likelihood of the language model with arbitrary (differentiable) constraints in a single energy function, and then generates samples in a non-autoregressive manner. Specifically, it initializes the entire output sequence with noise and follows a Markov chain defined by Langevin Dynamics using the gradients of this energy. We evaluate MuCoLa on text generation with soft and hard constraints as well as their combinations, obtaining significant improvements over competitive baselines for toxicity avoidance, sentiment control, and keyword-guided generation.",
    "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2251–2277\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nGradient-Based Constrained Sampling from Language Models\nSachin Kumar♣ Biswajit Paria♡ Yulia Tsvetkov♠\n♣Language Technologies Institute, Carnegie Mellon University, Pittsburgh PA\n♡Machine Learning Department, Carnegie Mellon University, Pittsburgh PA\n♠Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle W A\n{sachink,bparia}@cs.cmu.edu, yuliats@cs.washington.edu\nAbstract\nLarge pretrained language models generate flu-\nent text but are notoriously hard to control-\nlably sample from. In this work, we study con-\nstrained sampling from such language models:\ngenerating text that satisfies user-defined con-\nstraints, while maintaining fluency and model’s\nperformance in a downstream task. We propose\nMUCOLA—a sampling procedure that com-\nbines the log-likelihood of the language model\nwith arbitrary (differentiable) constraints in a\nsingle energy function, and then generates sam-\nples in a non-autoregressive manner. Specifi-\ncally, it initializes the entire output sequence\nwith noise and follows a Markov chain defined\nby Langevin Dynamics using the gradients of\nthe energy function. We evaluate MUCOLA on\ntext generation with soft and hard constraints\nas well as their combinations obtaining signifi-\ncant improvements over competitive baselines\nfor toxicity avoidance, sentiment control, and\nkeyword-guided generation.1\n1 Introduction\nTransformer-based language models (LMs) trained\non web-scale corpora (Radford et al., 2019; Raf-\nfel et al., 2020; Brown et al., 2020) are generat-\ning impressively realistic texts. Despite having\nhuman-level fluency, they are far from reaching\nhuman-level communication abilities and are hard\nto control for content, context, and intent in com-\nmunication. This results in unreliable models that\nlack basic knowledge, hallucinate facts, and dis-\ncriminate users (Bender et al., 2021; Gehman et al.,\n2020; Pagnoni et al., 2021).\nControlled text generation—sampling text from\nLMs to satisfy constraints on the properties of gen-\nerated text—aims to address these issues. Prior\nworks incorporate constraints in existing decoding\n1The code is available at: https://github.com/\nSachin19/mucoco/tree/sampling\n…\nFigure 1: MUCOLA, our proposed method, stylized\nas µCOLA. Given a language model, a prompt/input\nx, and desired constraints defined as thresholds on dif-\nferentiable functions, we perform Langevin Dynamics\nupdates to generate the entire output sequence y non-\nautogressively. We show experiments highlighting both\nhard and soft constraints (§4).\nalgorithms at token level by modifying output prob-\nabilities (Dathathri et al., 2020; Yang and Klein,\n2021a; Krause et al., 2020a; Liu et al., 2021a; Lu\net al., 2021b; Pascual et al., 2021; Liu et al., 2021b).\nWhile effective in certain settings, by generating au-\ntoregressively (i.e., left-to-right), these approaches\nfail to account for global context and hardly gener-\nalize beyond a single constraint. More importantly,\nby modifying output probabilities, they end up alter-\ning the underlying LM distribution compromising\nthe fluency and task accuracy (Kumar et al., 2021).\nWe propose an algorithm to sample text non-\nautoregressively from a conditional or uncondi-\ntional LM trained to perform any language gen-\neration task—translation, summarization, dialog,\nprompt completion—while controlling for multi-\nple, potentially competing constraints, and without\nsacrificing the base model quality. Combining the\nLM likelihood with constraints into a single “en-\nergy” function, our algorithm follows a Markov\n2251\nChain (Brooks et al., 2011) to iteratively transform\nan output sequence initialized randomly into a de-\nsired output with low energy.\nSince common Monte Carlo Markov Chain\n(MCMC) sampling methods can be intractably\nslow (Sokal, 1997), we propose to define this\nMarkov Chain using gradients of the energy func-\ntion with respect to token embeddings of the output\nsequence. Additionally, we introduce stochasticity\nin the process in order to generate diverse samples,\nby modifying the gradients with additive noise, a\nprocess referred to as Langevin Dynamics (Grenan-\nder and Miller, 1994; Parisi, 1981; Welling and\nTeh, 2011; Gelfand and Mitter, 1991; Song et al.,\n2020). Finally, we operationalize the energy func-\ntion by defining each constraint to be smaller than\nthreshold, and writing it as a Lagrangian—with\nlanguage model likelihood as the primary objec-\ntive (Kumar et al., 2021). Besides allowing us\nto combine any number of constraints of varying\nscales without a need of tuning their weights, we\nshow that low-energy solutions under this defini-\ntion are true samples from the LM distribution. We\ncall this algorithm MUCOLA for sampling with\nmultiple constraints from LMs using Langevin Dy-\nnamics (§3; also see figure 1 in the Appendix).\nWe show the efficacy and generality of MU-\nCOLA on a variety of tasks, LMs and constraints\nfrom prior work (§4), including soft constraints\n(§5) defined using auxiliary models (e.g., classi-\nfiers or smaller LMs), as well as hard rule-based\nconstraints (§6) defined by presence or absence\nof certain keyphrases. We conduct experiments\non (a) toxicity avoidance, sentiment control in\nGPT2-Large, and (b) keyword controlled gener-\nation with GPT2-Large and XL, and (c) entity con-\ntrol to improve fidelity in translation. Through\nboth automatic metrics and human evaluation, we\nshow versatility of this method through improved\nor matched performance with competitive base-\nlines, in terms of quality and diversity. Finally, we\npresent preliminary results on new tasks, showing\npromising directions for future research.\n2 Background: Constrained Sampling\nfrom Language Models\nLet P(y|x; θ) model the conditional probability\ndistribution of an output token sequence y =\n(y1,...,y N), given an optional input token se-\nquence x = (x1,...,x M) where xm,yn ∈V.\nWe are interested in constrained sampling from\nP—finding output sequences y that have a high\nprobability under P while minimizing a given set\nof constraint functions: {f1,...,f C}. We assume\nthat each fi : ([x],y) →R is defined such that a\nlower value of fi implies that the output better sat-\nisfies the constraint. For example, to constrain the\noutputs to only non-toxic continuations for a given\nprompt x, we define a classifier pTOXIC (y) which\npredicts the output toxicity probability, with lower\nprobability implying lower toxicity. We assume all\nfi are differentiable.\nEnforcing these constraints in an autoregres-\nsive (i.e., left-to-right) decoding strategy like beam\nsearch or sampling is challenging, since the con-\nstraints are defined conceptually on the whole out-\nput sequence and are hard to evaluate accurately\nonly on the generated prefix (Yang and Klein,\n2021b; Liu et al., 2021a). With multiple constraints,\ntheir satisfaction/balancing becomes challenging.\nRecent work thus explored non-autoregressive\ncontrolled generation (Kumar et al., 2021), using\nconstrained optimization over y—finding a single\noutput y which maximizes P given the constraints\nby performing gradient descent on the outputs y.\nThis involves (1) representing the constrained op-\ntimization problem as a single objective E(y) (of-\nten referred to as an energy function, discussed\nin §3.3), and (2) relaxing the discrete outputs y\nto continuous approximations such that gradient\ndescent is feasible. In previous works, the latter\nis achieved by creating a soft-representation of y,\n˜y = (˜y1,..., ˜yN) where each ˜yn ∈R|V|is a sim-\nplex (or “logits” which are converted to a simplex\nusing softmax) over the target vocabulary V, rep-\nresenting the probability of the n-th token in the\nsequence. We refer to these methods as gradient-\nbased decoding. Representing the decoding objec-\ntive as min˜ yE(˜ y) and initializing ˜y with ˜y0, it is\nupdated as\n˜yt = ˜yt−1 −η∇˜yE(˜yt−1), (1)\nwhere η >0 denotes the step size. In this process,\nthe underlying LMs (and functionsfi) remain fixed\nand are used to provide gradients to the sequence\n˜ y. After performing multiple steps of this gradient\ndescent, discrete text can be extracted from ˜y using\ndifferent heuristics (Kumar et al., 2021; Qin et al.,\n2020; Song et al., 2020). This formulation has been\nstudied in various generation settings in prior work\nwith different instantiations of ˜ yand E(y).\nHowever, this setup is deterministic and does\n2252\ncat\nTransformer LayersTransformer \nLayers Transformer LayersTransformer \nLayers\nTransformer LayersTransformer \nLayers Transformer LayersTransformer \nLayers\nLinearPool & Linear\n… ………\n……\nFigure 2: Different kinds of functions can be incorporated into MUCOLA defined on a shared embedding table E.\n(Left) Language Modeling objective defines a per-token loss directly on the sequence of embeddings. For every\ntoken this loss provides gradients to update ˜ei via backpropagation through the transformer layers and directly\nto ˜ei+1 through the negative loss likelihood loss as computed in §3.3. This is used as a primary objective for the\nunderlying LM and can also be used for classification as discussed in §5.2 (Center) Classification objective defined\non probability of the desired label. The classifier gets the token embeddings ˜e directly as input and updates the\nembedding using gradients obtained via backpropagation from the transformer layers (Right) Lexical loss defined\non the embeddings directly (without the use of additional models) to include desired keywords or phrases in the\noutput sequence (§6). In practice any combination of these constraints can be used.\nnot facilitate sampling.2 In addition, representing\neach token with a vector of size |V|can be com-\nputationally very expensive and difficult to fit into\ncommonly used GPUs for long sequences (with\nmore than ∼20-30 tokens; §7).\n3 Constrained Sampling via Langevin\nDynamics in Embedding Space\nTo enable efficient gradient-based sampling from\nLMs, we introduce MUCOLA which modifies the\nnon-autoregressive framework in §2 to (a) generate\nmultiple samples instead of optimizing for only one\ndeterministic output, (b) optimize for much smaller\nintermediate token representations as opposed to\ntheir distribution on the entire vocabulary.\n3.1 Exploring the token representation space\nInstead of relaxing each target token yn as a soft\nrepresentation over the vocabulary ˜yn ∈R|V|, we\nrepresent it as ˜en ∈E. Here E denotes the em-\nbedding table of the underlying language model\ncontaining |V|vectors of size d≪|V|. We denote\nthis sequence of embeddings as ˜ e= {˜e1,..., ˜eN}.\nAt an update step t, instead of feeding each ˜ yto the\nmodel(s) (which are then transformed to an embed-\nding to be fed to the first layer), we directly feed\n˜ eto the first layer to compute the energy function,\nnow defined as a function of embeddings instead\nof tokens. In case of deterministic minimization\n(similar to (1)), these vectors are updated as\n˜ et = ProjE(˜ et−1 −η∇˜ eE(˜ et−1)), (2)\n2While initialization can be used to add randomness to this\nalgorithm, we find that it has little to no effect on diversity.\nwhere ProjE(ˆe) = arg mine∈E ∥e−ˆe∥2 denotes a\nprojection operation on the embedding table E. In\nother words, after every gradient step, we project\neach updated vector back to a quantized space, that\nis the embedding table using Euclidean distance\nas the metric. This projection is done to prevent\nadversarial solutions.3 After the optimization is\ncomplete, discrete text can be easily obtained by\nprojection, that is the token indices corresponding\nto each ˜en in the embedding table E. This for-\nmulation yields the following benefits: (a) For a\nsequence of length L, at any optimization step t,\nit only maintains (and computes gradients with re-\nspect to) L×dparameters, as opposed to L×|V|.\nThis enables us to store much longer sequences in\na GPU as compared to the storing ˜ y. (b) this for-\nmulation provides a natural way to define hard rule-\nbased constraints based on keywords or phrases\n(discussed in more detail in §6), and, finally (c) it\nyields a natural way to generating samples.\n3.2 Gradient based Sampling via Langevin\nDynamics\nThe minimization in(2) can be very easily extended\nto a sampling procedure by modifying the gradient\ndescent in (2) to Langevin Dynamics (Gelfand and\nMitter, 1991; Welling and Teh, 2011),\n˜ et = ProjE(˜ et−1 −η∇˜ eE(˜ et−1) +\n√\n2ηβzt)\n3Several prior works (Belinkov and Glass, 2019) have\nshown that neural-network based models are not robust to\nchange in input space. We observed this phenomenon in our\npreliminary experiments where, without any projection, most\nlow energy solutions were found to be garbled text.\n2253\nLangevin Dynamics provides a MCMC method to\nsample from a distribution using only the gradient\nof its logarithm. That is, if we define a distribution\nas Q(y) ∝exp (−E(y)), its logarithm leads to the\nupdate specified above.4 This method is often used\nfor non-convex optimization for training neural net-\nworks (Welling and Teh, 2011) due to its ability to\nescape local minima due to added noise and con-\nverge towards the global minima. In this work, we\nadapt it for inference (Song and Ermon, 2019).\nIntuitively, by adding noise at every gradient\nstep, this procedure intends to find outputs y that\ndo not exactly minimizeEbut remain in the vicinity\nof the minima. In other words, it finds outputs\nwhich admit high probability under the distribution\nQ(y). This process begins with an exploration\nphase which is controlled by β. With a high value\nof β, the noise term is large leading to big updates.\nBy gradual annealing such that β →0, as t→∞,\nthis process converges to a sample from Q(y).5\n3.3 Representing the energy function\nA straightforward way to represent Eis with a lin-\near combination as ∑C\ni=1 λifi −λC+1 log P, with\npre-defined weights λ1,...,λ C+1 (Hoang et al.,\n2017; Qin et al., 2020, 2022). With this formulation\n(a) linear weights, λi’s, can be hard to define and\ntune for different fi, and especially difficult when\nfi’s lie on different scales, and more importantly,\n(b) defining the energy function in this manner mod-\nifies the original goal, which is to sample from the\nlanguage model P (with constraints), not from a\nmodified distribution Q∝exp(−E(y)). To allevi-\nate these issues, we define the inference objective,\nfollowing Kumar et al. (2021), as\ny ∼P(y|x; θ), subject to fi([x],y) ≤ϵi∀i\nwhere each threshold ϵi is a hyperparameter. As we\ndiscuss in more detail in §4, these thresholds can\n4The normalization term in Q(y) vanishes as its gradient\nwith respect to y is 0.\n5More details of the implementation of annealing schedule\ncan be found in §4. A similar noise can also be applied directly\nto the soft-token representations in (1) as explored in Qin et al.\n(2022). However, as we discuss in §7, our formulation with its\nsmaller parameter size allows generating longer sequences. In\naddition, considering logits as soft-representations (followed\nby softmax) has shown to result in slow mixing, that is, it\ntakes much longer to converge as empirically shown in Hoang\net al. (2017) and also observed in Qin et al. (2022). On the\nother hand, considering the simplex itself (Kumar et al., 2021;\nHoang et al., 2017) as soft-representations is not compatible\nwith Gaussian noise and can lead to undesirable behavior (Pat-\nterson and Teh, 2013).\nbe flexibly defined for most kinds of constraints.\nFor example, instead of merely trying to reduce\npTOXIC (y), we can set it as pTOXIC (y) <0.1. Given\nthis formulation, we define the energy function as\na Lagrangian E(y) =−log P(y) −∑u\ni=1 λi(ϵi−\nfi(y)). Here, λi ≥ 0 are Lagrangian multipli-\ners and dynamically updated at each step. We\nfollow the gradient of E downwards for the ˜ e\n(as described in (2)) and upwards for the multi-\npliers (gradient ascent without any noise) while\nmaking sure that the multipliers remain positive:\nλt\ni = max(0,λt−1\ni + α∇λiE(y)) (α >0 is the\nstep size for ascent). Intuitively, if a constraint is\nnot satisfied, the term (ϵi −fi(·)) would be nega-\ntive and λi would keep increasing making Ehigh.\nOn the other hand, if all the constraints are satis-\nfied these values gradually decrease to 0 making\nE(y) =−log P(y) making the final output a sam-\nple from the desired distribution P. We implement\na damped version of this process to improve sta-\nbility, the details of which can be found in Kumar\net al. (2021). The final decoding algorithm we used\nin our experiments is described in algorithm 1 in\nthe Appendix.\nEnergy as a function of embeddings Perform-\ning gradient updates with respect to ˜ erequires\nthat all objectives be defined as functions of ˜ e,\nnot y. Also, f1(y),...,f C(y) must share the\nsame input embedding table (as that of P). We\ndiscuss in §4 how this can achieved for differ-\nent kinds of constraint functions fi. First, we\ndescribe how to compute the primary objective\n−log P(y|x; θ) and its gradients with respect to\n˜ e. In typical LMs, this objective is factorized\nas log P(y|x) = ∑L−1\nn=0 log P(yn+1|y1:n,x). For\neach decoding step n+ 1: the model takes as in-\nput yn, which is converted to en via an embed-\nding table lookup. Passed through the network lay-\ners, it is converted to a hidden vector hn. Since\nthe input and output embedding tables in most\nmodern LMs are shared (Radford et al., 2019;\nRaffel et al., 2020; Lewis et al., 2020; Brown\net al., 2020),6 the softmax probability is computed\nas, P(yn+1|y1:n,x) = exp(hT\nn en+1+bn+1)∑|V|\nj=1 exp(hTn ej+bj)\nwhere\nbn are optional bias terms. By replacing en+1\nwith ˜en+1, we convert the above probability to\nP(˜en+1|˜e1:n,x). For each position n+ 1, ˜en+1\n6Even if the embedding tables are not shared, this loss\nmay be computed and optimized using vectors from the output\nembedding table as parameters without any significant loss in\nperformance.\n2254\nreceives gradients, (a) directly from −log P func-\ntion and (b) through hn+1 via back-propagation\nthrough the network layers (See figure 2 (left)).\n4 Experimental Setup\nWe evaluate MUCOLA on four constrained genera-\ntion tasks. These tasks are selected based on defin-\ning different kinds of constraints for which prior\nwork designed specialized training or decoding\nmechanisms which cannot be generalized beyond\nthose tasks or language models. The main contri-\nbution of MUCOLA is generating diverse samples\nwhich conform to the language model P as well\nas can satisfy user defined arbitrary combination\nof constraints for which fine-tuning is generally\ninfeasible and tuning weights of each constraint\nis cumbersome. For a pre-defined sentence length\nL, we initialize the token representation for each\nstep ˜e1,..., ˜eL using token embeddings randomly\nsampled from the target vocabulary V.7 For all\nour experiments, we run the Langevin Dynamics\nsimulation for a maximum of 250 iterations un-\nless specified otherwise. We describe additional\nimplementation details including noise schedule,\nstopping criterion and multiplier update schedule\nin Appendix C.\n5 Text Generation with Soft Constraints\nFirst, we evaluate MUCOLA with real valued\nconstraint functions defined via auxiliary models\nsuch as classifiers or LMs. Given an LM GPT2-\nLarge (Radford et al., 2019), and a prompt x, we\ngenerate continuations y. We conduct experiments\nwith: toxicity avoidance and sentiment control.\nEach of the tasks define a binary constraint. Let\nthe desired label be denoted by LABEL 1, and other\none with LABEL 0 (LABEL 1 is non-toxic in toxicity\navoidance and positive in sentiment control). For\nboth setups, we assume availability of corpora to\ntrain the constraint functions.8\nBaselines In addition to decoding without any\nconstraints (which we simply call GPT2), we con-\nsider the following baselines which decode from\nleft-to-right:\n7We also tried other initialization strategies like initializing\nwith zeros, or outputs of nucleus sampling or greedy decoding\nbut did not find it to have any significant effect on the final\noutput\n8This setup can be easily extended to n-class setups by\ndefining n −1 constraints as p0 > p1, . . . , p0 > pn−1\n• Domain Adaptive Pretraining (DAPT) (Gu-\nrurangan et al., 2020) proposes to finetune the\nLM P on a corpus of desired constraint and\nsample directly from finetuned version.\n• FUDGE (Yang and Klein, 2021a) uses a\n“future-aware” constraint classifier to modify\noutput token probabilities at every decoding\nstep to steer the generation to promote con-\nstraint satisfaction. This classifier is trained to\npredict the ground truth label for every prefix\nof the training corpus.\n• GeDi (Krause et al., 2020a) uses a class-\nconditioned LM to modify output token prob-\nabilities at each step via Bayes’ rule.\n• DExperts (Liu et al., 2021b) proposes to re-\nplace the class-conditioned LM with two aux-\niliary language models (one expert and one\nanti-expert) to modify the output logits at ev-\nery step. These LMs are trained using same\nsetup as the baseline DAPT but instead of\nsampling from them directly, it uses them to\nsteer the base LMs outputs. For each of the\nbaselines, we use recommended hyperparam-\neters to generate samples.\nConstraint functions Each of these baselines\ncan be adopted as constraint functions for MU-\nCOLA as follows:\n• Discriminative Classifiers We train a binary\nclassifier pLABEL (y), which predicts the proba-\nbility of the desired attribute given the output\nsequence y by finetuning roberta-base with\nGPT2-Large embeddings (more details in Ap-\npendix D). To decode with MUCOLA, we\nformulate this constraint as pLABEL 1 >ϵ (We\ndefine specific ϵ values in §5.1 and §5.2 re-\nspectively). To improve its gradient profile,\nwe use the constraint in log space. We call\nthis setup MUCOLA-DISC .\n• Generative Classifiers Prior work has shown\nthat discriminative classifiers can be fragile\nto domain shift or adversarial examples (Yo-\ngatama et al., 2017; Krause et al., 2020b).\nHence, we also consider a second class of\ngenerative classifiers trained as class con-\nditional LMs that model p(·|LABEL ). Intu-\nitively, they are required to explain every\nword in the input, potentially amplifying the\nclass signal and improving robustness (Min\net al., 2021). We define them in three ways\nby finetuning GPT2-Large: (1) following\nGEDI (MUCOLA-GeDi), (2) following DEx-\n2255\nperts, (we train two separate LMs;MUCOLA-\nDExperts). And finally, (3) motivated by re-\ncent work on prompt-based classification, we\ndefine a class-conditional LM without finetun-\ning the model as P(x,y|verbalize(LABEL ))\nwhere verbalize(·) is function that con-\nverts the label to a natural language string\n(MUCOLA-prompt). Note that for all\nthree setups, the embedding table is frozen\n(more details in Appendix D). We de-\ncode via MUCOLA with the constraint\np(x,y|LABEL 1)) > p(x,y|LABEL 0) (again,\nrealized in log-space)9.\nEvaluation In both experiments, we evaluate the\ngenerated samples along three dimension, (1) Con-\nstraint Satisfaction measured using external evalu-\nators, (2) Fluency, measured by mean perplexity of\nthe continuations measured using GPT2-XL. Since\nthe objective is to generate samples from the LM,\nwe rank different methods not by their absolute\nperplexity, but its difference from the perplexity of\nunconstrained text. Additionally, we also report a\ngrammaticality score: the fraction of outputs pre-\ndicted by a classifier trained on CoLA (Warstadt\net al., 2019) as fluent. (3) Diversity, measured by\ncomputing the mean number of distinct n-grams in\neach set of samples, normalized by the length of\ntext (Li et al., 2016). We report this for n= 1,2,3\nfollowing prior work. Since all the automatic met-\nrics are model based and can be biased, we also\nperform human evaluation in an A/B testing setup\nwith the best performing baseline (DExperts in our\ncase). For each sample, we ask 3 annotators to com-\npare and rank candidates from our approach and\nthe baseline on constraint satisfaction, topicality\nand fluency.\n5.1 Toxicity Avoidance\nPrior work have shown that large pre-trained LMs\nare at risk of producing toxic content even when\ngiven innocuous prompts (Sheng et al., 2019;\nGehman et al., 2020). In this experiment, given\na neutral prompt, we generate non-toxic contin-\nuations using MUCOLA. We only consider the\nsetup MUCOLA-DISC here, with a classifier pTOXIC ,\ntrained on a dataset of human-annotated comments\nlabelled as toxic or non-toxic (Appendix D). We\ndecode with the constraint pTOXIC <0.01.\n9Note that all constraints we descibe can be easily extended\nto n-class set (with say0 as the desired label) by definingn−1\nconstraints as p0 > p1, . . . , p0 > pn−1\nWe follow the evaluation setup defined in Liu\net al. (2021b) and use test set of 10K nontoxic\nprompts (Gehman et al., 2020) where without any\nconstraints, the user might receive harmful output\nfrom the LM. For each prompt, we generate 25\nsamples for length 20 tokens each. We measure\nconstraint satisfaction using the toxicity score from\nPerspective API. Following prior work (Gehman\net al., 2020; Liu et al., 2021b), we report the max-\nimum toxicity score over 25 samples per prompt\naveraged over the number of prompts, and the em-\npirical probability of generating a continuation with\ntoxicity >0.5 at least once over the 25 generations.\nAs shown in Table 1, MUCOLA outperforms or\nmatches all baselines on toxicity, including a strong\nbaseline DEXPERTS which is specifically designed\nfor binary constraints. In addition, our method is\nclosest in perplexity to unconstrained generation,\nwhile maintaining grammaticality as well as diver-\nsity of baseline methods 10. We attribute this im-\nprovement to the fact that after the constraints are\nsatisfied, the energy function in MUCOLA reduces\nto −log P(y), the original function we intend to\nsample from, whereas in the baselines, the underly-\ning probability distribution (or the energy function)\nis modified to achieve control. Furthermore, human\nevaluation (Appendix F) reveals that generations by\nMUCOLA match DExperts on toxicity and fluency\nwhile being more topical.\n5.2 Sentiment Controlled Generation\nGiven a prompt x, the goal of this task is to gen-\nerate continuations y using an LM with a posi-\ntive sentiment/polarity. To understand the effect of\nsources of training data, we train two versions of\neach constraint function described above on two\ndatasets: SST-2 corpus (Socher et al., 2013) con-\ntaining ∼4K examples in Movie reviews for each\nclass; and Yelp polarity corpus containing ∼280K\nexamples for each class containing a mixed domain\nof reviews. We also consider an additional setup\nwhere we use two constraints using both versions\nof MUCOLA-DISC , which we call MUCOLA-\nTWO -DISC . We use a dataset of 15 prompts\nfrom Dathathri et al. (2020) and generate 20 sam-\nples per prompt of length 12, 20, and 50.\nTo evaluate constraint satisfaction, we measure\npositive sentiment accuracy of the output using\n10While FUDGE obtains the lowest absolute perplexity,\nprior work (Holtzman et al., 2020) has shown that very low\nperplexity is not an indicator of higher quality but of repeti-\ntions and usage of only high frequency tokens.\n2256\nApproach Toxicity Fluency Diversity\nAvg. Max.\nToxicity\nToxicity\nProb. Perplexity CoLa\nAccuracy Dist-1 Dist-2 Dist-3\nGPT-2 0.527 0.520 25.45 88.3 0.58 0.85 0.85\nDAPT 0.428 0.360 31.21 91.2 0.57 0.84 0.84\nFUDGE 0.437 0.371 12.97 88.5 0.47 0.78 0.82\nGEDI 0.363 0.217 60.03 85.5 0.62 0.84 0.83\nDEXPERTS 0.302 0.118 38.20 89.8 0.56 0.82 0.83\nMUCOLA 0.308 0.088 29.92 88.2 0.55 0.82 0.83\nTable 1: Results for toxicity avoidance (§5.1). We evaluate on three axes: (1) Toxicity–Avg. Max. Toxicity and\nToxicity Prob.: lower the better. (2) Fluency–GPT2-XL Perplexity: the closer the value to unconstrained outputs\n(GPT2: 38.6), the better; CoLa accuracy: higher the better, and (3) Diversity (Dist-1,2,3): higher the better. The\nbest values in each column are highlighted in bold. While our method improves or performs on par with baselines\non toxicity metrics, we obtain substantial improvements on perplexity.\nthree external classifiers to account for domain\ndifferences in their training data, (a) C1: distil-\nbert (Sanh et al., 2019) finetuned on SST-2 data,\nused in (Liu et al., 2021b), (b) C2: bert-base (De-\nvlin et al., 2019) finetuned on Yelp Polarity corpus\nused in Mireshghallah et al. (2022), and (c) C3:\nSieBERT (Heitmann et al., 2020) finetuned on 15\ndifferent polarity datasets. We report a subset of\nresults of this experiment in table 2 for outputs of\nlength 20 (and the rest in Appendix F). We observe\na significant variance in sentiment control accura-\ncies (C1, C2 and C3) where constraints trained on\nSST-2 perform worse on the evaluator trained on\nYelp (C2) and vice versa for all methods. The third\nevaluator (C3) trained on a much larger training\nset can be considered more reliable. Overall, we\nfind that MUCOLA in all settings obtains perplex-\nity values closer to unconstrained outputs (GPT2)\nwhereas most baselines achieve control at the cost\nof perplexity. Surprisingly, constraints trained on\nYelp perform poorly compared to those trained on\nSST2 despite the former being a larger dataset.\nFor outputs of lengths 12 and 20, MUCOLA-\nTWO -DISC finds a good balance of control and\nfluency and outperforms all other baselines on sen-\ntiment accuracy while maintaining good perplexity\n(except GEDI which performs poorly on perplex-\nity as well as CoLa accuracy). This improvement\nhowever comes with a slight decline in diversity\nmetrics which we argue is a fair price to pay for\nconstraint satisfaction compared to fluency. Similar\nto §5.1, a small scale study on human evaluation\n(Appendix F) reveals MUCOLA to be more topical\nthan the best baseline DExperts. Finally, using a\nprompt-based constraint also performs strongly de-\nspite not trained at all. In future work, we will look\ninto training a prompt-based classifier to improve\nthis performance. For outputs of length 50, we ob-\nserve a slight drop in MUCOLA’s performance. On\ncloser inspection (table 14), we find a trend of de-\ngenerate repetitions at the end of many sequences.\nPrior work (Holtzman et al., 2020) has shown that\nlarge LMs often assign unusually high probabilities\nto repeating sequences especially with increasing\nlengths and since our method is designed to sam-\nple high probability outputs, such behavior is ex-\npected. In future work, we will explore constraints\ndesigned to discourage this behavior (Welleck et al.,\n2020; Meister et al., 2022).\n6 Decoding with Hard Constraints\nIn the previous two tasks, we explored how MU-\nCOLA can be applied on soft constraints, defined\nvia real valued functions like probabilities of classi-\nfiers or language models. Now, we consider a ruled-\nbased constraint that a specific word or phrasemust\nappear in the generated text. Existing autoregres-\nsive solutions to this task have explored various\nstrategies either based on explicitly modifying prob-\nabilities to up-weight desired words (Pascual et al.,\n2021), or search-based strategies based on beam-\nsearch (Lu et al., 2021b). In this work, we define\na differentiable distance function d(w,˜ e) which\nmeasures overlap between desired word ( w) and\nthe output token embeddings ˜ e(we use the nota-\ntion wto refer to as the word itself and its index\nin the vocabulary interchangeably). We then pro-\npose a simple criterion to define a threshold ϵthat\nguarantees that if d(w,˜ e) < ϵ, then w’s embed-\nding appears in ˜ e(and by extension w appears\nin y). Inspired from Liu et al. (2022); Qin et al.\n(2022), this distance is computed in three steps (1)\n2257\ndefine a “probability distribution” for each output\ntoken, πn = softmax(−∥˜en −e1∥2\n2,..., −∥˜en −\ne|V|∥2\n2) where {e1,...,e |V|} ∈E, (2) Define\nq = gumbel−softmax(g1/τ,...,g N/τ), where\ngn = logπn,w, gumbel softmax provides a way\nto differentiably sample (Jang et al., 2017) and\nτ is a hyperparameter, and finally, (3) d(w,˜ e) =∑N\nn=1 −qngn. Intuitively, this function minimizes\nthe Euclidean distance between one of the output\nembeddings (chosen with stochasticity) and w’s\nembedding, ew. This function can be easily ex-\ntended for phrases, w= (w1,...,w l) by defining\ngn = 1\nl\n∑l\nu=1 log πwu,n+u.\nBased on this definition, for each desired key-\nword, we define a threshold ϵw as −log πw,w.We\nprovide an intuitive explanation of the distance\nfunction, and a semi-formal and empirical proof of\nhard satisfaction of this threshold in Appendix E.\nTasks We formally evaluate this setup on two tasks:\n(1) open-ended keyword constrained generation\n(with two datasets: COMMON GEN and ROC)),\nand (2) terminology constrained machine transla-\ntion. We additionally show preliminary findings\non a third task, entity guided summarization. We\nelaborate on COMMON GEN here and the rest of\nthe results, following a similar trend can be found\nin Appendix E. In COMMON GEN (Lin et al., 2020)\ngiven no prompt , the task is generate an output\nof maximum length 40 which contains a given set\nof four or five words. We use GPT2-XL as the\nunderlying LM in this setup with COLD (Qin et al.,\n2022) as our main baseline.\nEvaluation Following prior work, we measure the\nperformance on two axes, (1) Coverage, measured\nby (a) count average number of keywords appear-\ning in the output; and (b) percent, measuring the\nfraction of outputs which contain all the desired\nkeywords. (2) Fluency, as measured by GPT2-XL\nperplexity and human evaluation, where on a sam-\nple of 200 outputs, we ask 3 annotators to rate each\noutput on a 3-point likert scale. As reported in ta-\nble 3, we outperform the best baseline on coverage.\nWe outperform all baselines in terms of perplex-\nity by a large margin, again owing to the fact that\nour method samples from the language model and\ndoes not modify the distribution itself as opposed\nto the baselines. Human evaluation reveals that our\napproach slightly underperforms the best baseline.\n7 Discussion and Analysis\nSpeed and Memory Generating a sequence of\nlength L using MUCOLA requires maintaining\nL×dparameters. In contrast, performing Langevin\nDynamics in the vocabulary space requires L×|V|\nparameters (|V|>>d). In this analysis, we empir-\nically verify the benefits of our setup. With GPT2-\nLarge as underlying LM, we sample sequences of\nvarying lengths with various constraints, on differ-\nent commercially available GPUs using both our\napproach and an ablation with vocabulary sized\nrepresentations (logits+softmax; more details in\nAppendix H). As summarized in table 10, we find\nthat much longer sequences can be generated with\nembeddings across the board (maximum of length\n500 even with constraints) while with vocabulary\nsized parameters, the approach runs of out of mem-\nory even without any constraint beyond a length of\n20 even on the largest GPU.\nSources of Diversity Our proposed approach has\ntwo sources of randomness which can potentially\nlead to diversity: initialization and noise addition\nat each step of Langevin Dynamics. To understand\ntheir effects, we vary these sources and compute the\ndiversity metrics. We follow the setup of toxicity\navoidance using a randomly sampled subset of 100\nprompts. The results are shown in table 8. We\nfind that changing the initialization has little to no\neffect on the final metrics indicating that Langevin\nDynamics is the primary source of diversity.\nCompatibility of Constraints Although, our ap-\nproach allows any combination of constraints in\nprinciple, in many cases, the combination might\nnot be compatible. As an example, we combine sen-\ntiment and keyword constraints used in the earlier\nexperiments to define a new task: Given a prompt,\ngenerate a continuation with a positive (or negative)\nsentiment containing words typically associated\nwith a negative (or positive) sentiment. Using our\nbest performing constraint (MUCOLA-TWO -DISC )\nfrom §5.2, and a single keyword constraint, we\nfind that MUCOLA fails almost ∼90% of the times\nsince two constraints are incompatible for most\nscenarios. For when it does succeed, we present\nselected examples in table 19.\nVarying threshold ϵ In our experiments, each\nfunction fiis constrained to be bounded by a thresh-\nolds ϵi, which are tunable hyperparameters. The\nthreshold provides an interpretable way to control\n2258\nApproach Setting % Positive Sentiment Fluency Diversity\nC1 C2 C3 Perplexity CoLa Dist-1 Dist-2 Dist-3\nGPT-2 - 46.7 47.7 61.3 38.6 78.7 0.64 0.90 0.88\nDAPT SST-2 73.6 70.0 78.3 76.9 70.7 0.64 0.89 0.86\nFUDGE SST-2 67.6 63.0 79.3 10.3 94.0 0.51 0.80 0.84\nGEDI SST-2 99.0 96.3 99.7 268.7 54.0 0.69 0.87 0.84\nDEXPERTS SST-2 91.2 83.4 95.4 55.37 81.6 0.61 0.89 0.87\nMUCOLA-DISC SST-2 84.6 77.5 88.0 27.9 80.8 0.50 0.81 0.82\nMUCOLA-DISC Yelp 83.0 83.6 83.0 32.2 76.0 0.50 0.75 0.80\nMUCOLA-TWO -DISC Yelp, SST-2 93.7 91.0 96.0 28.9 76.7 0.53 0.77 0.74\nMUCOLA-PROMPT - 87.3 91.0 93.0 53.0 77.2 0.54 0.82 0.80\nTable 2: Results for Sentiment Controlled Generation for outputs of length 20. We evaluate on three axes: (1) %\nPositive Sentiment: higher the better. We use three external classifiers for this evaluation, C1 trained on SST2 data,\nC2 trained on Yelp data, and C3 trained on 15 polarity datasets; (2) Fluency–GPT2-XL perplexity, closer the value\nto unconstrained outputs (GPT2: 38.6), the better; CoLa accuracy: higher the better, and (3) Diversity (Dist-1,2,3):\nhigher the better. The best values in each column are highlighted in bold.\nCoverage Fluency\nCount Percent Perplexity Human\nTSMH 2.72 71.27 1545.15 1.72\nNeurologic 3.30 91.00 28.61 2.53\nCOLD 4.24 94.5 54.98 2.07\nMUCOLA 4.49 99.7 23.50 2.29\nTable 3: Results of keyword constraint on COMMON -\nGEN. We report (a) coverage as avg. count of desired\nkeywords in the output and the fraction of the outputs\ncontaining all keywords (percent); and (b) GPT2-XL\nperplexity and avg. fluency score rated by humans.\nthe intensity of the desired attributes. To illustrate\nthis capability, we again follow the setup of toxic-\nity avoidance with 100 prompts and apply the con-\nstraint pTOXICITY <ϵ with ϵ∈{0.5,0.3,0.1,0.01}.\nAs shown in table 8, making ϵsmaller improves\ntoxicity control. However, the fluency (as measured\nby perplexity) remains largely the same. That is,\nunlike baselines, this method does not trade-off\nfluency and controllability. However, there is a\ntrade off between diversity and controllability as\nwe observe in sentiment control experiments (§5.2)\nwhere making a constraint stricter leads to a decline\nin diversity.\n8 Conclusion\nWe present MUCOLA, a sampling algorithm from\nlanguage models that flexibly combines pretrained\nLMs with any differentiable constraints. Our pri-\nmary contributions are a (1) gradient based MCMC\nsampling method (Langevin Dynamics) performed\non (2) intermediate representation of tokens (em-\nbeddings). With experiments on both soft and hard\nconstraints with different pretrained LMs, we show\nthat this approach generates diverse outputs which\nbetter conform both to desired constraints as well\nas the underlying LM distribution. Despite the ob-\nserved improvements, we believe we have barely\nscratched the surface. In future work, we will ex-\nplore ways to improve the convergence properties\nof this algorithm using more sophisticated MCMC\nalgorithms (Girolami and Calderhead, 2011) and\ndevelop constraints to improve performance on\nlonger sequences. Furthermore, since we perform\nupdates on embeddings rather than vocabulary dis-\ntributions, future work may also study ways to ex-\npand vocabularies at decoding time.\nAcknowledgements\nThe authors would like to thank Xiaochuang Han,\nArtidoro Pagnoni, Melanie Sclar, Lucille Njoo and\nAnjalie Field for helpful feedback and discussions,\nand the anonymous reviewers for much appreci-\nated feedback. S.K. is supported by a Google PhD\nFellowship. This material is based upon work sup-\nported by the National Science Foundation under\nCAREER Grant No. IIS2142739, as well as Grants\nNo. IIS2125201 and IIS2007960. The views and\nopinions of authors expressed herein do not nec-\nessarily state or reflect those of the United States\nGovernment or any agency thereof.\nLimitations\nDespite speed improvements on gradient-based de-\ncoding compared to using vocabulary-sized rep-\nresentation, this approach still requires iteratively\nupdating L×dparameters (with each update in-\n2259\nvolve a forward and a backward pass) and is consid-\nerably slower than autoregressive decoding meth-\nods (anywhere between 15-20 times longer). A\nstraightforward way to improve decoding speed\nis using larger batches and smaller floating point\noperations which we leave for future work. Fur-\nther improvements may also be achieved by adapt-\ning more sophisticated gradient based methods\nfor faster convergence (Girolami and Calderhead,\n2011) or techniques from diffusion models in im-\nage generation (Luhman and Luhman, 2021). Like\nother non-autoregressive decoding approaches, this\nmethod also requires pre-defining a fixed output\nlength which can be a hindrance. This is an ac-\ntive area of research with many solutions proposed\nin the literature including predicting the sequence\nlength (Wang et al., 2021), generating multiple out-\nputs with varying lengths and reranking (Guo et al.,\n2019), continuing generating autoregressively to\nfinish a sentence after a fixed length output is gen-\nerated (Qin et al., 2022) of all which have shown\npromising results. Furthermore, since this algo-\nrithm aims to find high probability sequences un-\nder the LM, and most open-ended LMs suffer from\ndegeneracy (repeating sequences get high proba-\nbility) (Holtzman et al., 2020), using it can some-\ntimes lead to such generations especially for long\nsequences (as we observe in §5.2). Incorporating\nrepetition reducing loss functions (Welleck et al.,\n2020; Meister et al., 2022) as constraints can help\nalleviate this issue. We leave this exploration for\nfuture work.\nEthical Considerations\nMost large LMs trained on web content have been\nshown to generate harmful language. They are\nprone to generating toxic (Gehman et al., 2020)\nand non-factual content (Pagnoni et al., 2021), and\ncan potentially be used maliciously (Wallace et al.,\n2019, 2020; Zellers et al., 2019). The ultimate goal\nof controlled text generation approaches, including\nours, is to enable finer-grained control over gen-\nerated texts that could potentially alleviate these\nissues (Gehman et al., 2020; Bender et al., 2021;\nLiu et al., 2021a). They also find useful applica-\ntions in anonymizing protected attributes in written\ntext (Reddy and Knight, 2016), as writing assistants\nto avoid users’ implicit biases (Ma et al., 2020;\nField and Tsvetkov, 2020). However, none of the\napproaches are perfect.\nOn the other hand, controlled text generation\nresearch can also be maliciously used to generate\nmisinformation, make output text more biased and\ntoxic as well as target individuals to influence pub-\nlic opinion. To combat these issues, future research\nshould focus on developing better defense methods\nagainst misusing these models maliciously, in a\nway that could cause societal harms (Zellers et al.,\n2019).\nReferences\nYonatan Belinkov and James Glass. 2019. Analysis\nmethods in neural language processing: A survey.\nTransactions of the Association for Computational\nLinguistics, 7:49–72.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proc. FAccT.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Shujian Huang,\nMatthias Huck, Philipp Koehn, Qun Liu, Varvara\nLogacheva, Christof Monz, Matteo Negri, Matt Post,\nRaphael Rubino, Lucia Specia, and Marco Turchi.\n2017. Findings of the 2017 conference on machine\ntranslation (WMT17). In Proceedings of the Second\nConference on Machine Translation, pages 169–214,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nSteve Brooks, Andrew Gelman, Galin Jones, and Xiao-\nLi Meng. 2011. Handbook of Markov Chain Monte\nCarlo. CRC press.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Proc.\nNeurIPS.\nFredrik Carlsson, Joey Öhman, Fangyu Liu, Severine\nVerlinden, Joakim Nivre, and Magnus Sahlgren. 2022.\nFine-grained controllable text generation using non-\nresidual prompting. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 6837–\n6857, Dublin, Ireland. Association for Computational\nLinguistics.\nAlvin Chan, Yew-Soon Ong, Bill Pung, Aston Zhang,\nand Jie Fu. 2021. Cocon: A self-supervised approach\nfor controlled text generation. In Proc. ICLR.\n2260\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2020. Plug and play language models:\nA simple approach to controlled text generation. In\nProc. ICLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nGeorgiana Dinu, Prashant Mathur, Marcello Federico,\nand Yaser Al-Onaizan. 2019. Training neural ma-\nchine translation to apply terminology constraints. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 3063–\n3068, Florence, Italy. Association for Computational\nLinguistics.\nAnjalie Field and Yulia Tsvetkov. 2020. Unsupervised\ndiscovery of implicit gender bias. In Proc. EMNLP.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. RealToxi-\ncityPrompts: Evaluating neural toxic degeneration\nin language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3356–3369, Online. Association for Computational\nLinguistics.\nSaul B. Gelfand and Sanjoy K. Mitter. 1991. Recursive\nstochastic algorithms for global optimization in Rd.\nSIAM Journal on Control and Optimization.\nMark Girolami and Ben Calderhead. 2011. Riemann\nmanifold langevin and hamiltonian monte carlo meth-\nods. Journal of the Royal Statistical Society. Series\nB (Statistical Methodology).\nUlf Grenander and Michael I. Miller. 1994. Representa-\ntions of knowledge in complex systems. Journal of\nthe Royal Statistical Society. Series B (Methodologi-\ncal), pages 549–603.\nJunliang Guo, Xu Tan, Di He, Tao Qin, Linli Xu, and\nTie-Yan Liu. 2019. Non-autoregressive neural ma-\nchine translation with enhanced decoder input. Pro-\nceedings of the AAAI Conference on Artificial Intelli-\ngence, 33(01):3723–3730.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nMark Heitmann, Christian Siebert, Jochen Hartmann,\nand Christina Schamp. 2020. More than a feeling:\nBenchmarks for sentiment analysis accuracy. Avail-\nable at SSRN 3489963.\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J Fleet. 2022.\nVideo diffusion models. arXiv:2204.03458.\nCong Duy Vu Hoang, Gholamreza Haffari, and Trevor\nCohn. 2017. Towards decoding as continuous optimi-\nsation in neural machine translation. In Proceedings\nof the 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 146–156, Copen-\nhagen, Denmark. Association for Computational Lin-\nguistics.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In International Conference on Learning\nRepresentations.\nEric Jang, Shixiang Gu, and Ben Poole. 2017. Cate-\ngorical reparametrization with gumbel-softmax. In\nProceedings International Conference on Learning\nRepresentations (ICLR).\nVivek Jayaram and John Thickstun. 2021. Parallel and\nflexible sampling from autoregressive models via\nlangevin dynamics. In Proceedings of the 38th Inter-\nnational Conference on Machine Learning, volume\n139 of Proceedings of Machine Learning Research,\npages 4807–4818. PMLR.\nMarcin Junczys-Dowmunt, Roman Grundkiewicz,\nTomasz Dwojak, Hieu Hoang, Kenneth Heafield,\nTom Neckermann, Frank Seide, Ulrich Germann,\nAlham Fikri Aji, Nikolay Bogoychev, André F. T.\nMartins, and Alexandra Birch. 2018. Marian: Fast\nneural machine translation in C++. In Proceedings of\nACL 2018, System Demonstrations, pages 116–121,\nMelbourne, Australia. Association for Computational\nLinguistics.\nNitish Shirish Keskar, Bryan McCann, Lav Varsh-\nney, Caiming Xiong, and Richard Socher. 2019.\nCTRL - A Conditional Transformer Language\nModel for Controllable Generation. arXiv preprint\narXiv:1909.05858.\nMuhammad Khalifa, Hady Elsahar, and Marc Dymet-\nman. 2021. A distributional approach to controlled\ntext generation. In International Conference on\nLearning Representations.\nTomasz Korbak, Hady Elsahar, German Kruszewski,\nand Marc Dymetman. 2022. Controlling conditional\nlanguage models without catastrophic forgetting. In\nProceedings of the 39th International Conference\non Machine Learning, volume 162 of Proceedings\nof Machine Learning Research, pages 11499–11528.\nPMLR.\nBen Krause, Akhilesh Deepak Gotmare, Bryan McCann,\nNitish Shirish Keskar, Shafiq Joty, Richard Socher,\n2261\nand Nazneen Fatema Rajani. 2020a. GeDi: Gen-\nerative Discriminator Guided Sequence Generation.\narXiv preprint arXiv:2009.06367.\nBen Krause, Akhilesh Deepak Gotmare, Bryan McCann,\nNitish Shirish Keskar, Shafiq Joty, Richard Socher,\nand Nazneen Fatema Rajani. 2020b. GeDi: Genera-\ntive discriminator guided sequence generation.\nSachin Kumar, Eric Malmi, Aliaksei Severyn, and Yulia\nTsvetkov. 2021. Controlled text generation as con-\ntinuous optimization with multiple constraints. In\nAdvances in Neural Information Processing Systems.\nGuillaume Lample, Sandeep Subramanian, Eric Smith,\nLudovic Denoyer, Marc’Aurelio Ranzato, and Y-Lan\nBoureau. 2019. Multiple-attribute text rewriting. In\nProc. ICLR.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting ob-\njective function for neural conversation models. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 110–119, San Diego, California. Association\nfor Computational Linguistics.\nXiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy\nLiang, and Tatsunori Hashimoto. 2022. Diffusion-\nlm improves controllable text generation. ArXiv,\nabs/2205.14217.\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei\nZhou, Chandra Bhagavatula, Yejin Choi, and Xiang\nRen. 2020. CommonGen: A constrained text gen-\neration challenge for generative commonsense rea-\nsoning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1823–1840,\nOnline. Association for Computational Linguistics.\nAlisa Liu, Maarten Sap, Ximing Lu, Swabha\nSwayamdipta, Chandra Bhagavatula, Noah A. Smith,\nand Yejin Choi. 2021a. DExperts: Decoding-time\ncontrolled text generation with experts and anti-\nexperts. In Proc. ACL.\nAlisa Liu, Maarten Sap, Ximing Lu, Swabha\nSwayamdipta, Chandra Bhagavatula, Noah A. Smith,\nand Yejin Choi. 2021b. DExperts: Decoding-time\ncontrolled text generation with experts and anti-\nexperts. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 6691–6706, Online. Association for Computa-\ntional Linguistics.\nGuangyi Liu, Zichao Yang, Tianhua Tao, Xiaodan\nLiang, Junwei Bao, Zhen Li, Xiaodong He, Shuguang\nCui, and Zhiting Hu. 2022. Don’t take it literally: An\nedit-invariant sequence loss for text generation. In\nProc. NAACL.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach.\nXiming Lu, Sean Welleck, Peter West, Liwei Jiang,\nJungo Kasai, Daniel Khashabi, Ronan Le Bras, Lian-\nhui Qin, Youngjae Yu, Rowan Zellers, Noah A. Smith,\nand Yejin Choi. 2021a. Neurologic a*esque de-\ncoding: Constrained text generation with lookahead\nheuristics. ArXiv, abs/2112.08726.\nXiming Lu, Peter West, Rowan Zellers, Ronan Le Bras,\nChandra Bhagavatula, and Yejin Choi. 2021b. Neu-\nroLogic decoding: (un)supervised neural text genera-\ntion with predicate logic constraints. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 4288–4299,\nOnline. Association for Computational Linguistics.\nEric Luhman and Troy Luhman. 2021. Knowledge dis-\ntillation in iterative generative models for improved\nsampling speed.\nXinyao Ma, Maarten Sap, Hannah Rashkin, and Yejin\nChoi. 2020. PowerTransformer: Unsupervised con-\ntrollable revision for biased language correction. In\nProc. EMNLP.\nClara Meister, Tiago Pimentel, Gian Wiher, and Ryan\nCotterell. 2022. Typical decoding for natural lan-\nguage generation.\nSewon Min, Mike Lewis, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2021. Noisy channel language\nmodel prompting for few-shot text classification.\narXiv preprint.\nFatemehsadat Mireshghallah, Kartik Goyal, and Taylor\nBerg-Kirkpatrick. 2022. Mix and match: Learning-\nfree controllable text generation using energy lan-\nguage models.\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia\nTsvetkov. 2021. Understanding factuality in abstrac-\ntive summarization with FRANK: A benchmark for\nfactuality metrics. In Proc. NAACL.\nBiswajit Paria, Chih-Kuan Yeh, Ian E. H. Yen, Ning\nXu, Pradeep Ravikumar, and Barnabás Póczos. 2020.\nMinimizing FLOPs to learn efficient sparse represen-\ntations. In Proc. ICLR.\nG. Parisi. 1981. Correlation functions and computer\nsimulations. Nuclear Physics B.\n2262\nDamian Pascual, Beni Egressy, Clara Meister, Ryan\nCotterell, and Roger Wattenhofer. 2021. A plug-and-\nplay method for controlled text generation. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2021, pages 3973–3997, Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nSam Patterson and Yee Whye Teh. 2013. Stochastic\ngradient riemannian langevin dynamics on the proba-\nbility simplex. In NuerIPS.\nJohn Platt and Alan Barr. 1988. Constrained differential\noptimization. In Proc. NeurIPS. American Institute\nof Physics.\nMatt Post and David Vilar. 2018. Fast lexically con-\nstrained decoding with dynamic beam allocation for\nneural machine translation. In Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Pa-\npers), pages 1314–1324, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nShrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhut-\ndinov, and Alan W Black. 2018. Style transfer\nthrough back-translation. In Proc. ACL.\nJing Qian, Li Dong, Yelong Shen, Furu Wei, and Weizhu\nChen. 2022. Controllable natural language genera-\ntion with contrastive prefixes. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022,\npages 2912–2924, Dublin, Ireland. Association for\nComputational Linguistics.\nLianhui Qin, Vered Shwartz, Peter West, Chandra Bha-\ngavatula, Jena D. Hwang, Ronan Le Bras, Antoine\nBosselut, and Yejin Choi. 2020. Back to the future:\nUnsupervised backprop-based decoding for counter-\nfactual and abductive commonsense reasoning. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 794–805, Online. Association for Computa-\ntional Linguistics.\nLianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin\nChoi. 2022. COLD decoding: Energy-based con-\nstrained text generation with langevin dynamics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. JMLR.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen. 2022. Hierarchical text-\nconditional image generation with clip latents.\nSravana Reddy and Kevin Knight. 2016. Obfuscating\ngender in social media writing. In Proc. Workshop\non NLP and Computational Social Science.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. ArXiv,\nabs/1910.01108.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1073–\n1083, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\nand Nanyun Peng. 2019. The woman worked as\na babysitter: On biases in language generation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3407–\n3412, Hong Kong, China. Association for Computa-\ntional Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nA. Sokal. 1997. Monte Carlo Methods in Statistical\nMechanics: Foundations and New Algorithms.\nCongzheng Song, Alexander Rush, and Vitaly\nShmatikov. 2020. Adversarial semantic collisions.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4198–4210, Online. Association for Computa-\ntional Linguistics.\nYang Song and Stefano Ermon. 2019. Generative mod-\neling by estimating gradients of the data distribution.\nIn Proc. NeurIPS.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard-\nner, and Sameer Singh. 2019. Universal adversarial\ntriggers for attacking and analyzing NLP. InProceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2153–2162, Hong\nKong, China. Association for Computational Linguis-\ntics.\nEric Wallace, Mitchell Stern, and Dawn Song. 2020.\nImitation attacks and defenses for black-box machine\ntranslation systems. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 5531–5546, Online. As-\nsociation for Computational Linguistics.\nMinghan Wang, Guo Jiaxin, Yuxia Wang, Yimeng Chen,\nSu Chang, Hengchao Shang, Min Zhang, Shimin Tao,\n2263\nand Hao Yang. 2021. How length prediction influ-\nence the performance of non-autoregressive transla-\ntion? In Proceedings of the Fourth BlackboxNLP\nWorkshop on Analyzing and Interpreting Neural Net-\nworks for NLP , pages 205–213, Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-\nnan, Kyunghyun Cho, and Jason Weston. 2020. Neu-\nral text generation with unlikelihood training. In\nProc. ICLR.\nMax Welling and Yee Whye Teh. 2011. Bayesian learn-\ning via stochastic gradient langevin dynamics. In\nProc. ICML.\nKevin Yang and Dan Klein. 2021a. FUDGE: Controlled\ntext generation with future discriminators. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n3511–3535, Online. Association for Computational\nLinguistics.\nKevin Yang and Dan Klein. 2021b. FUDGE: Controlled\ntext generation with future discriminators. In Proc.\nNAACL.\nKexin Yang, Dayiheng Liu, Wenqiang Lei, Baosong\nYang, Mingfeng Xue, Boxing Chen, and Jun Xie.\n2022. Tailor: A prompt-based approach to attribute-\nbased controlled text generation.\nDani Yogatama, Chris Dyer, Wang Ling, and Phil Blun-\nsom. 2017. Generative and discriminative text classi-\nfication with recurrent neural networks.\nLantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.\n2017. SeqGAN: Sequence generative adversarial\nnets with policy gradient. In Proc. AAAI.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. In Proc. NeurIPS.\nDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.\nBrown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. 2020. Fine-tuning lan-\nguage models from human preferences.\nA M UCOLA Decoding Algorithm\nWe provide a formal algorithm forMUCOLA in 1.\nAlgorithm 1: MUCOLA: detailed decod-\ning algorithm\nInput: input sequence x, output length L,\nbase LM, attribute functions fi and their\nrespective thresholds ϵi, step sizes η, ηmax\n(and schedule), ηλ, initial noise variance\nβinit (and schedule);\nResult: output sequence y\nFor all n∈{1,...,L }, initialize ˜e(0)\nn ;\nFor all i∈{1,...u }, initialize λ(0)\ni as 0;\nInitialize β(0) as βinit;\nInitialize η(0) as η;\nfor t= 1,..., MAXSTEPS do\n// forward pass\ncompute the energy function E(see\n§3.3);\n// backward pass\nfor all n,i, compute ∇(t−1)\n˜en = ∂E\n∂˜en ,\n∇(t−1)\nλi = ∂E\n∂λi\n;\n// Update the parameters\nSample z(t−1) ∼N(0,Id);\nUpdate ˜ et\ny = ProjE(˜ e(t−1)\ny −\nη∇(t−1)\n˜ ey E+\n√\n2η(t−1)βz(t−1));\nUpdate\nλt\ni = max(0,λt−1\ni + η2∇(t−1)\nλi E);\nupdate β(t),η(t) following the threshold\nupdate schedule.\nend\nConvert ˜ e(t) to discete tokens ˆ y(t) by\nnearest neighbor search.;\nreturn arg mint{−log P(ˆy(t)|x) :\n∀i,fi(˜y(t)|[x]) ≤ϵi};\n2264\nB Related Work\nControllable Text Generation Prior work in this\narea can be divided into three categories: The first\nfocuses on training models with specific control\ncodes via pretraining (Keskar et al., 2019) or fine-\ntuning (Gururangan et al., 2020; Chan et al., 2021)\nfor prompt based generation and generative models\nfor tasks such for style transfer (Lample et al., 2019;\nZiegler et al., 2020; Prabhumoye et al., 2018; Yu\net al., 2017). These methods are naturally difficult\nto extend to new controls as it requires retraining\nthe models.\nThe second category includes decoding ap-\nproaches from LMs without modifying them\n(MUCOLA falls under this category). Most prior\nwork in this space has explored methods to modify\nleft-to-right search or sampling algorithms by mod-\nifying the output probability distribution at each\nstep using different control functions. Dathathri\net al. (2020); Krause et al. (2020a); Yang and Klein\n(2021b); Liu et al. (2021b) apply this approach\nfor soft constraints defined by classifiers and LMs\nwhereas Lu et al. (2021b,a); Pascual et al. (2021)\ndevelop heuristic control functions for keyword\nbased constraints. In contrast, we show that MU-\nCOLA is able to incorporate both kinds of con-\nstraints. Since these approaches generate one to-\nken at time and do not allow modifying a token\nonce it is generated, they are not ideal for con-\ntrols that are conceptually defined on the entire\nsequence. Hence, prior work has also explored\nnon-autoregressive decoding methods (Mireshghal-\nlah et al., 2022). Most closely related to MUCOLA\nis Kumar et al. (2021) which propose a gradient-\nbased decoding algorithm which we extend to gen-\nerate multiple samples. Also related is Qin et al.\n(2022) that perform Langevin Dynamics in the sim-\nplex space to incorporate control by representing\nthe energy function as a linear combination of con-\ntrol functions. In contrast, we represent the energy\nfunctions as a Lagrangian and perform these up-\ndates on a much smaller embedding space allowing\nus to generate longer sequences.\nThe third category includes more recent few-shot\nmethods which rely on prompting large LMs such\nas GPT3 to incorporate controls based on demon-\nstrations (Qian et al., 2022; Yang et al., 2022; Carls-\nson et al., 2022). MUCOLA is an orthogonal ap-\nproach to this work and can be applied on top of\nprompt-based solutions to increase control satisfac-\ntion.\nWhile this work and the work described above\nfocuses on controlling attributes of individual text\noutputs, in related work, Khalifa et al. (2021); Kor-\nbak et al. (2022) develop approaches for distribu-\ntional control of various attributes in generated text\ncorpora.\nGradient-based Sampling Langevin Dynamics\nand other gradient-based MCMC methods have\nbeen developed for generative modeling in contin-\nuous domains such as images (Song and Ermon,\n2019) and audio (Jayaram and Thickstun, 2021)\namong others where the models are trained to pre-\ndict the gradients (via a score function) directly\nwhereas MUCOLA requires a backward pass to\ncompute them. Also related are diffusion models\nwhich have obtained state-of-the-art performance\nfor many generative tasks (Ramesh et al., 2022;\nHo et al., 2022). Similar ideas have also been ap-\nplied to train text generation models in concurrent\nwork with promising results for incorporating con-\ntrols (Li et al., 2022).\nC Implementation Details\nHere, we describe additional implementation de-\ntails as part of the experimental setup described in\n§4\nNoise Schedule The amount of noise in each up-\ndate is controlled by β(§3.2) which represents the\nvariance of the noise term. We initialize βwith 5.0\nand decrease it to 0.05 in a geometric progression\nfor 100 steps after which we keep it constant at\n0.05 for the remaining 150 steps. The range of βis\nguided by best practices in Song and Ermon (2019)\nprescribing the initial variance to be close to the\nmaximum distance between any two vectors in the\ninput space and the minimum value being close to\n0. This schedule allows for sufficient exploration\nin the beginning helping in diversity, while, leaving\nenough iterations for optimizing the final output\ninto a fluent sequence.\nStep Size and Selection Criterion The step-size\nηin projected gradient descent depends on the ge-\nometry of the embedding space of the underlying\nlanguage model. Since we project back the up-\ndate at every step to E, if the update term is not\nbig enough in the projected gradient update, the\nsequence at step t+1 would remain the same. This\nobservation provides a very simple criterion for\nearly stopping and selecting the best output out\nof all iterations. When the additive noise is small\n2265\n(near the end of optimization), the update term can\nbe small due to following factors: (a) ηis small, (b)\nthe gradient ∇E˜ eis small which implies the out-\nput sequence has “converged”. Hence, we define a\nschedule on the step-size as follows: we start with a\nstep-size η, and update the outputs using Langevin\nDynamics until the sequence stops updating, i.e.,\nthe update value becomes too small (and satisfies\nall constraints). Now, to make sure that this is a\nconvergence point and not a result of the step size\nbeing too small, we update the step size linearly to\nηmax in ssteps11. If the sequence does not update\nin s steps, we stop early and predict the output.\nOtherwise, the process continues. If it does not\nstop early at the end of maximum number of steps,\nwe predict the output with the highest likelihood\nwhich repeated at least 5 times. In the event, no\nsuch repetition is observed, we deem the optimiza-\ntion as “failed” and restart. If the restarts also fail,\nwe just predict the autoregressive output (which in\nour experiments is obtained with nucleus sampling\nwith p= 0.96). This fallback mechanism ensures\nthat the output, irrespective of the constraint satis-\nfaction is always a sample of P while preventing\ngenerating half-baked outputs.\nMultipliers Update Schedule We initialize each\nof the multipliers λi with 0, update the multipli-\ners via gradient ascent every 20 steps using the\nstep-size 1.0. In addition, if the sequence stops\nupdating at a certain iteration (as described above)\nand i-th constraint is not satisfied, we update λi\nat every iteration till the sequence starts updating\nagain. This schedule prevents fluctuation in the\nmultiplier values when the noise is high in the early\niterations and the sequence has not converged to\nanything fluent while still allowing updates when\nrequired (Platt and Barr, 1988; Paria et al., 2020).\nD Training Details for Soft Constraint\nModels\nSince we decode by computing gradients over to-\nken embeddings, it requires that all constraint mod-\nels share the same embedding table E as that of\nthe underlying language model P. Since any typi-\ncal text based model involves an embedding table,\nwe can train a constraint using such a model by\nsimply initializing its embedding table with E. In\nprinciple, this initialization allows using any off-\nthe-shelf pretrained model as a constraint function\n11s is empirically defined as 40 in all our experiments.\nby finetuning it on appropriate data. In our exper-\niments, we use the following models in different\nexperiments:\nToxicity Classifier For toxicity avoidance (§5.1),\nwe finetune roberta-base (Liu et al., 2019) with\na binary classification head using a dataset of\nhuman-annotated comments from the Jigsaw Unin-\ntended Bias In Toxicity Classification Kaggle Chal-\nlenge. The dataset has ∼160K toxic comments\nand ∼1.4M non-toxic comments. We first balance\nthis dataset by subsampling 160Kexamples from\nthe non-toxic class. We replace the embedding\ntable of roberta-base with that of the underlying\nLM (GPT2-Large in our case). To address the di-\nmension mismatch of the two embedding tables,\nduring finetuning, we also learn a linear projection\nmatrix which transforms base LM embedding to a\nsmaller dimension of roberta-base. We keep base\nLM embedding frozen during this finetuning. We\nuse a learning rate of 1e−5 and train for 3 epochs\nwith an effective batch size of 64. We choose a\ncheckpoint with an accuracy of ∼93% on a heldout\ndevelopment set.\nSentiment Classifiers For sentiment control ex-\nperiments in §5.2, we experiment with different\nkinds of constraints defined using both classifiers\nand language models. For both setups we use two\ndatasets: SST-2 corpus (Socher et al., 2013) con-\ntaining ∼4K examples in Movie reviews for each\nclass; and Yelp polarity corpus containing ∼280K\nexamples for each class containing a mixed domain\nof reviews.\nFor discriminative classifiers, we also finetune\nroberta-base using the same setup and hyperpa-\nrameters as the toxicity classifier. Our best model\nobtains an accuracy of ∼92% on the SST-2 test set\nand ∼98% on the Yelp test set.\nTo train the generative classifiers, we finetune\nGPT2-Large (and do not need to substitute any\nembedding tables) keeping the embedding table\nfrozen. We use the loss−log pgen(label|x) for each\ntraining instance where pgen(label = 0|text) =\npLM(text|label = 0) /(pLM(text|label = 0) +\npLM(text|label = 1)). This is due to Bayes’ rule\n(p(label) vanishes as we set it to 0.5 for balanced\ndatasets). Here pLM(text|label) is obtained using\nthe language model by computing the probability\nof the text conditioned on the input token “positive”\nfor the positive label and “negative” otherwise.\nWe again follow the same training hyperparam-\n2266\neters for this setup. On SST-2 test set, we obtain\nan accuracy of ∼95% and on Yelp, we obtain an\naccuracy of ∼98%.\nE Additional Explanation and Results for\nHard Constraint\nThe keyword distance function d(w,˜e) is com-\nputed in three steps. First, we convert each ˜en\nto a “probability” over the vocabulary as,\nπn = softmax(−∥˜en −e1∥2\n2,..., −∥˜en −e|V|∥2\n2)\nwhere {e1,...,e |V|} are entries in the embed-\nding table E. Since each ˜en itself also corre-\nsponds to a vector in E, if n-th token in the se-\nquence is w, then, ∥˜en −ew∥2\n2 would be 0 lead-\ning to πn,w = maxj πn,j. That is, maximizing\ngn = log πn,w with respect to ˜en would nudge\nit towards the ew. Since, we don’t know which\nindex we want w to appear at in advance, fol-\nlowing (Liu et al., 2022; Qin et al., 2022), we\n(soft) sample it using πn,w as weights. This\nbrings us to the second step, as we define, q =\nGUMBEL -SOFTMAX (−g1/τ,..., −gN/τ) where\nτ is the temperature. We use hard sampling here\nto ensure qis one-hot. Finally, we define the con-\nstraint function as, d(w,˜ e) = ∑N\ni=n−qngn. In-\ntuitively, this function aims to generate the word\nw wherever it already has a high chance of get-\nting generated (measured via πn,w’s). Stochastic-\nity in this function allows for exploration. This\nfunction can be easily extended from words to\nphrases of length l, w= (w1,...,w l) by defining\ngn = 1\nl\n∑l\nu=1 −log πwu,n+u. This computation\ncan be efficiently done on a GPU using a convolu-\ntion operation (Liu et al., 2022).\nBased on this definition, we define the key-\nword constraint for MUCOLA as d(w,˜ e) ≤\n−log πw,w + δ, where δis a small positive value\n(we set it as 0.1). πw is a slight abuse of notation\nto define a distribution similar to πn (nrefers in\nan index in sequence whereas wrefers to an index\nin V). Note that the threshold for each keyword is\ndifferent.12\nIntuitively, if w appears in the output at the\nk-th position, then πk,w = πw,w = maxj πk,j\n12While we do not experiment with it in this work, the\nconstraint K(w, ˜ e) can be easily extended to setup where at\nleast one out of n given words (for example different surface\nforms of the same root), S = {w1, . . . , wp}must appear\nin the output by defining a new constraint as K(S,˜ e) =\nmaxwi∈S K(wi,˜ e) or its soft version using the gumbel-\nsoftmax trick.\nwith qk as 1. This reduces the distance func-\ntion to −log πk,w which is less than the defined\nthreshold. Conversely, if wdoes not appear in the\noutput, for each n, −log πn,w would be higher\nthan log πw,w and the constraint remains unsat-\nisfied. This is due to an empirical observation\nwe make in all embedding tables we use, that\nπw,w = maxj πw,j = maxj πj,w. In other words,\nnot only is the probability of a word under its own\ndistribution piw the greater than probability of all\nother words (since the corresponding distance is 0),\nit is also larger than w’s probability under all other\ndistributions defined for any word in the vocabu-\nlary. Under the assumption that minimum distance\nbetween any two vectors in the table is greater than\na small positive value, we conjecture this claim to\nbe true for any embedding table.\nOpen-Ended Keyword Guided Generation In\naddition to COMMON GEN, we report results on\nROC (Pascual et al., 2021) task where given 5\nkeywords, the goal is generate a sequence of max\nlength 90 containing those terms. For both datasets,\nfor set of keywords, we generate samples of length\n10, 20, and 40 (with 3 restarts for each) and after\nall iterations are complete, we continue generating\nmore tokens autoregressively until a maximum of\n40 (90 in case of ROC) tokens are generated or end\nof sequence token is generated. Finally, we eval-\nuate on one output which satisfies the constraints\nand has the lowest perplexity according to the LM.\nWe compare MUCOLA with the best reported re-\nsults in (Qin et al., 2022) and (Pascual et al., 2021)\nand corresponding baselines. The results for ROC\ncan be found in table 9.\nTerminology Constrained Translation We fol-\nlow the setup in Dinu et al. (2019) and use an\noff-the-shelf English to German translation model\nby MarianMT (Junczys-Dowmunt et al., 2018) to\ntranslate a subset of WMT17 en-de test set (Bojar\net al., 2017). The constraint here is to integrate\na given custom terminology into the translation\noutput; where the terms are automatically created\nfrom the IATE EU terminology database for 414\ntest sentences (with 1 to 3 terminology constraint\nper example). We use Lu et al. (2021a) as our\nbest baseline and also report other baselines re-\nported by them. We generate each translation by\nfirst generating with beam search unconstrained\n(with beam size of 6). If this output is of length\nL. We use MUCOLA to generate sequences of\n2267\nMethod BLEU Coverage\nUnconstrained 32.9 85.3\nPost and Vilar (2018) 33.0 94.3\nNeurologic* 33.5 97.2\nMUCOLA 33.1 100\nTable 4: Results for terminology constrained en–de\ntranslatoin (§E)\nlength {L,L+1,...,L +10}and select the gener-\nation which has the highest length-normalized log-\nprobability as the final translation. We evaluate on\nBLEU score13 and coverage accuracy. As reported\nin table 4, MUCOLA obtains perfect (100%) cover-\nage while at the same maintaining BLEU score.\nEntity Constrained Summarization In this\nsetup, we do a preliminary exploration on text sum-\nmarization with a constraint that a specific entity\nmust appear in the summary given the article. We\nuse BART-Large (Lewis et al., 2020) finetuned on\nthe CNN/Dailymail Corpus (See et al., 2017) as\nour underlying LM. First, we obtain all named en-\ntities appearing in the article using an off-the-shelf\nrecognizer14. We then use MUCOLA to sample a\nsummary (of maximum length 50) from the model\nconsidering appearance of each entity as a con-\nstraint. We show selected examples with promising\nresults in table 16, table 17 and table 18. Eval-\nuating this setup is non-trivial, since it adds new\nsentences/phrases to the summary and will natu-\nrally perform poorly on standard reference based\nmetrics such as ROUGE. Hence, we leave this eval-\nuation for future work.\nF Additional Results for Soft Constraints\nToxicity Avoidance For human evaluation, we\nfollow an A/B testing framework and compareMU-\nCOLA and DExperts. We sample 200 prompts\nfrom the test set and consider 2 generations per\nprompt. We ask each annotator to rank the outputs\nfrom the two approaches on (1) toxicity if one out-\nput is more or less toxic than the other, or if both\nare equally toxic/non-toxic, (2) topicality: is the\ngeneration coherent with the prompt and follows\nthe same general topic, and (3) fluency: if the out-\nputs have any grammatical mistakes. We collect 3\n13For fair comparison, we compute a tokenized\nBLEU score reported by the baselines following\nhttps://github.com/INK-USC/CommonGen/\ntree/master/evaluation\n14https://huggingface.co/dslim/bert-base-NER-uncased\nannotations per pair. We find that in terms of toxic-\nity, both models perform similarly with an average\n8.5% annotations preferring MUCOLA’s outputs\ncompared to 9.5% for DExperts (rest are equally\nranked). On topicality, 22.5% of annotations prefer\nMUCOLA’s outputs while 19% prefer Dexperts\n(rest are equally ranked). On fluency, both mod-\nels perform similarly with 22.5% and 23% in each\nmethod’s favor and rest equally ranked.\nSentiment Control We present the full set of re-\nsults for sentiment control experiments in tables 5,\n6, 7 and More details can be found in the cap-\ntions. For human evaluation, we similarly follow\nan A/B testing framework and compare MUCOLA\nand DExperts (for outputs of length 20). We con-\nsider all 15 prompts from the test set and consider\n2 generations per prompt. We ask each annotator\nto rank the outputs from the two approaches on (1)\npositivity if one output is positive and the other is\nnot, or if both are positive/not-positive, (2) topical-\nity: is the generation coherent with the prompt and\nfollows the same general topic, and (3) fluency: if\nthe outputs have any grammatical mistakes. We\ncollect 3 annotations per pair. We find that in terms\nof positivity, on an average 23.3% annotations pre-\nfer MUCOLA’s outputs compared to 16.7% for\nDExperts (rest are equally ranked). On topical-\nity, 26.7% of annotations prefer MUCOLA’s out-\nputs while 13.3% prefer Dexperts (rest are equally\nranked). On fluency, MUCOLA slightly underper-\nforms with 7.8% and 10% in each method’s favor\nand rest equally ranked.\nG Example\nWe provide selected examples from each of our\nexperiments in tables 11, 12, 13, 14, 15 and 16.\nH Additional Discussion and Analysis\nSpeed and Memory Requirements Generating\na sequence of length Lusing MUCOLA requires\nmaintaining L×d parameters. In contrast, per-\nforming Langevin Dynamics in the vocabulary\nspace requires L×|V| parameters (|V| >> d).\nIn this analysis, we empirically verify the bene-\nfits of our setup. Taking GPT2-Large as the un-\nderlying LM (with 774M parameters), and three\ncommercially available GPUs with different RAM\nsizes commonly used in academic settings–Nvidia\nGeForce RTX 2080 Ti (12GB), GeForce RTX 3090\nTi (24GB) and RTX A6000 (48GB)–we decode us-\ning our approach with token embeddings and an\n2268\nApproach Setting % Positive Sentiment (↓) Fluency Diversity\nC1 C2 C3 Perplexity CoLa\nAccuracy Dist-1 Dist-2 Dist-3\nGPT-2 - 49.0 45.0 62.0 54.9 68.7 0.66 0.87 0.81\nDAPT SST-2 71.3 66.7 75.0 98.0 64.0 0.64 0.85 0.79\nDAPT Yelp 64.0 71.3 79.7 146.6 58.0 0.60 0.84 0.80\nFUDGE SST-2 71.7 70.0 79.0 11.4 82.7 0.53 0.76 0.77\nFUDGE Yelp 71.7 73.7 84.7 11.8 85.7 0.53 0.76 0.77\nMUCOLA-DISC SST-2 90.0 81.7 93.3 28.8 67.3 0.52 0.73 0.74\nMUCOLA-DISC Yelp 88.3 87.0 91.7 32.9 64.3 0.52 0.74 0.75\nMUCOLA-TWO -DISC Yelp, SST2 94.0 91.3 94.7 29.4 55.0 0.46 0.68 0.71\nGEDI SST-2 99.7 91.0 99.3 625.7 54.3 0.65 0.76 0.71\nGEDI Yelp 82.0 90.0 89.0 444.9 40.0 0.71 0.78 0.66\nMUCOLA-GEN SST-2 91.3 88.3 97.0 57.2 68.0 0.50 0.69 0.70\nMUCOLA-GEN Yelp 86.3 89.7 91.7 53.0 67.7 0.50 0.70 0.70\nMUCOLA-PROMPT - 89.0 88.7 94.7 43.7 66.7 0.49 0.72 0.73\nDEXPERTS SST-2 93.1 86.9 94.9 75.2 71.5 0.63 0.85 0.81\nDEXPERTS Yelp 80.3 88.5 88.8 116.3 67.5 0.67 0.84 0.79\nMUCOLA-DE XPERTS SST-2 93.0 88.0 94.0 41.4 66.3 0.47 0.71 0.73\nMUCOLA-DE XPERTS Yelp 74.3 74.0 83.3 72.5 66.0 0.52 0.73 0.74\nTable 5: Positive sentiment control results on outputs of length 12. For each baseline ( FUDGE , GEDI and\nDEXPERTS ), we convert their respective constraints to a classifier (generative or discriminative; see §5.2). For\nFUDGE and GEDI, we show improvements on both control (% positive sentiment) and fluency (Perplexity) without\nany model specific changes. This improvement is consistent on models trained on both datasets (SST-2 and Yelp).\nDEXPERTS outperforms all baselines here including our method.\nApproach Setting % Positive Sentiment (↑) Fluency Diversity\nC1 C2 C3 Perplexity CoLa\nAccuracy Dist-1 Dist-2 Dist-3\nGPT-2 - 46.7 47.7 61.3 38.6 78.7 0.64 0.90 0.88\nDAPT SST-2 73.6 70.0 78.3 76.9 70.7 0.64 0.89 0.86\nDAPT Yelp 65.0 75.0 80.7 86.6 69.7 0.59 0.88 0.87\nFUDGE SST-2 67.6 63.0 79.3 10.3 94.0 0.51 0.80 0.84\nFUDGE Yelp 71.0 70.0 79.3 10.6 89.0 0.53 0.81 0.85\nMUCOLA-DISC SST-2 84.6 77.5 88.0 27.9 80.8 0.50 0.81 0.82\nMUCOLA-DISC Yelp 83.0 83.6 83.0 32.2 76.0 0.50 0.75 0.80\nMUCOLA-TWO -DISC Yelp, SST2 93.7 91.0 96.0 28.9 76.7 0.53 0.77 0.74\nGEDI SST-2 99.0 96.3 99.7 268.7 54.0 0.69 0.87 0.84\nGEDI Yelp 84.0 95.7 91.0 208.3 44.0 0.76 0.87 0.81\nMUCOLA-GEN SST-2 86.3 80.3 93.3 45.6 77.7 0.50 0.74 0.78\nMUCOLA-GEN Yelp 79.7 83.0 90.0 27.2 72.3 0.50 0.82 0.86\nMUCOLA-PROMPT - 87.3 91.0 93.0 53.0 77.2 0.54 0.82 0.80\nDEXPERTS SST-2 91.2 83.4 95.4 55.37 81.6 0.61 0.89 0.87\nDEXPERTS Yelp 81.1 85.8 92.5 95.87 71.7 0.66 0.89 0.87\nMUCOLA-DE XPERTS SST-2 89.3 83.7 93.7 32.2 79.7 0.51 0.78 0.80\nMUCOLA-DE XPERTS Yelp 78.0 75.7 83.3 34.1 68.3 0.52 0.77 0.81\nTable 6: Positive sentiment control results on outputs of length 20. For each baseline ( FUDGE , GEDI and\nDEXPERTS ), we convert their respective constraints to a classifier (generative or discriminative; see §5.2). For\nFUDGE and GEDI, we show improvements on both control (%positive sentiment) and fluency (Perplexity) without\nany model specific changes. This improvement is consistent on models trained on both datasets (SST-2 and Yelp).\n2269\nApproach Setting % Positive Sentiment (↓) Fluency Diversity\nC1 C2 C3 Perplexity CoLa\nAccuracy Dist-1 Dist-2 Dist-3\nGPT-2 - 47.7 44.3 61.3 36.3 78.3 0.59 0.92 0.94\nDAPT SST-2 93.0 84.3 91.7 55.3 88.0 0.61 0.92 0.94\nDAPT Yelp 72.3 80.7 85.0 46.1 84.3 0.51 0.90 0.94\nFUDGE SST-2 71.0 61.3 84.7 8.5 98.3 0.47 0.83 0.92\nFUDGE Yelp 72.3 68.0 80.3 8.3 99.0 0.47 0.83 0.92\nMUCOLA-DISC SST-2 88.7 81.0 91.3 15.3 72.7 0.42 0.68 0.76\nMUCOLA-DISC Yelp 70.7 74.3 81.3 19.1 77.7 0.48 0.77 0.85\nMUCOLA-TWO -DISC Yelp, SST2 94.0 91.3 94.7 29.4 75.0 0.57 0.78 0.79\nGEDI SST-2 86.7 98.7 96.7 148.4 68.3 0.75 0.94 0.93\nGEDI Yelp 99.7 98.7 100.0 114.5 74.3 0.66 0.93 0.93\nMUCOLA-GEN SST-2 85.0 76.3 91.0 22.5 63.7 0.44 0.71 0.78\nMUCOLA-GEN Yelp 77.7 80.7 88.3 23.4 65.0 0.43 0.69 0.76\nMUCOLA-PROMPT - 81.3 83.0 92.7 18.2 72.0 0.39 0.67 0.77\nDEXPERTS SST-2 98.1 92.0 99.5 39.5 88.5 0.57 0.91 0.94\nDEXPERTS Yelp 87.2 91.7 94.9 54.0 77.3 0.62 0.92 0.93\nMUCOLA-DE XPERTS SST-2 72.7 71.7 84.7 28.2 69.0 0.45 0.75 0.83\nMUCOLA-DE XPERTS Yelp 62.3 61.7 75.7 18.8 81.0 0.48 0.77 0.83\nTable 7: Positive sentiment control results on outputs of length 50. For each baseline ( FUDGE , GEDI and\nDEXPERTS ), we convert their respective constraints to a classifier (generative or discriminative; see §5.2). For\nFUDGE and GEDI, we show improvements on both control (% positive sentiment) and fluency (Perplexity) without\nany model specific changes. This improvement is consistent on models trained on both datasets (SST-2 and Yelp).\nThreshold Initialization Toxicity Fluency Diversity\nAvg. Max.\nToxicity\nToxicity\nProb PPL CoLa\nAccuracy dist-1 dist-2 dist-3\n0.5 Random 0.351 0.268 32.1 87.5% 0.58 0.85 0.85\n0.3 Random 0.352 0.200 33.0 87.5% 0.58 0.85 0.85\n0.1 Random 0.320 0.158 31.2 86.3% 0.56 0.83 0.83\n0.01 Random 0.302 0.094 28.8 87.1% 0.55 0.82 0.83\n0.01 Zeros 0.302 0.094 35.3 85.8% 0.55 0.81 0.82\n0.01 Greedy 0.302 0.115 28.6 86.6% 0.55 0.81 0.83\nTable 8: Ablations on Toxicity Avoidance showing the effect of changing classifier threshold (ϵ) on toxicity metrics,\nand initialization on diversity metrics. Loosening the threshold leads to an increase in toxicity (or decrease in\ntoxicity avoidance). Initialization has little effect on the diversity indicating the importance of Langevin Dynamics.\nCoverage (%) Fluency (PPL) Repetition Rate\nPlan-and-Write 96 33.9 25.7\nCGMH 97 127.8 1.6\nGPT-2 fine-tuned 72 89.4 1.8\nGPT-2+K2T 100 48.8 1.5\nMUCOLA 100 29.4 0.5\nTable 9: Results of lexically constrained decoding on the ROC dataset (with 5 keyword constraints). We decode\nwith MUCOLA with lengths 10, 20 and 40, and if the constraint is satisfied we continue generating autoregressively\nfor 90 tokens using nucleus sampling (p= 0.96).\n2270\nConstraint (#Parameters) 12GB GPU 24GB GPU 48GB GPU\nEmbeds Simplex Embeds Simplex Embeds Simplex\nNone (0) 200 Failed 1000 20 1000 50\nOne Classifier (125M) 200 Failed 500 20 1000 50\nTwo Classifier (250M) 50 Failed 500 20 1000 50\nOne LM (747M) Failed Failed 500 20 200 20\nTwo LMs (1.4B) Failed Failed 200 10 500 20\nTable 10: Maximum sequence length that can be generated with GPT2-Large (747M parameters) for different com-\nmonly used GPUs with MUCOLA (with embeddings) and with an ablation using vocabulary sized representations\n(referred to as Simplex). Failed denotes out of memory (OOM) error.\nPrompt Republican Presidential candidate Donald Trump weighed in on Paris attacks saying, “sneaky,\nGPT2 weak and stupid” Muslims, the press wants to use the Paris attacks to their advantage. “Okay,\nGeDi corrupt political leaders” had helped promote Radical Islam. Trump said:\nFUDGE weak leaders allow radical Islamic terrorism to take place.” Trump’s campaign manager, Kellyanne\nDExperts politically correct bureaucrats in government don’t do a very good job.” Interested in France? Add\nMUCOLA-DISC very sneaky” Obama from Chicago goes around telling others what to do. I’m putting out a call for\neveryone\nPrompt “I expected him to carry on [in] the article saying that I was\nGPT2 a stupid idiot,” he says. But ultimately the material he wrote isn’t about West. It’s\nGeDi involved, until we find something else.\\n\\n’Then he got out of the car and attacked\nFUDGE not a racist, but instead that I am a ’racist’. It was not the first time he\nDExperts doing things against the sect,” Bishop Sahlin told NCR in an email on Sunday. “But\nMUCOLA-DISC a woman, but he didn’t,” the woman said. “I was a little bit surprised,\"\nTable 11: Selected examples from Toxicity Avoidance Experiment (§5.1) generated by different methods.\nPrompt Once upon a time\nGPT2 , you had to fire the cannon at sea at noon when\nGeDi a young priest traveling the world taught the world the meaning of\nFUDGE , in a land far away, there lived a man with\nDExperts , white women ruled both Australia and America and cherished his nation\nMUCOLA-DISC (SST2) , the people of the United States were a people of the\nMUCOLA-DISC (Yelp) , I was a great big-time, all-American\nMUCOLA-TWO -DISC , the people of the world were a very different and powerful\nMUCOLA-PROMPT you start with just Bluetooth and now with this versatile module you\nTable 12: Examples of length 12 by the prompt “Once upon a time” generated by different methods.\n2271\nPrompt Once upon a time\nGPT2 , you had to fire the cannon at sea at noon when all the other\nsailing vessels were under way\nGeDi unseen world through vivid mystical experience! One enjoys\nbecoming connected with the unseen. Life quite encompassed\nboth nature\nFUDGE , a woman in India had a baby and was able to have it at the\nmoment of her choice\nDExperts , white women ruled both Australia and America and cherished\nhis nation as her home. Her words resonate with\nMUCOLA-DISC (SST2) , the world was a very beautiful, and a very good, place. The\npeople were kind and\nMUCOLA-DISC (Yelp) , I had a great time. I was a very nice and very good-looking\nman. I\nMUCOLA-TWO -DISC , I enjoyed the wonderful family and friends I had in the com-\nmunity.\\n\\n I was a good\nMUCOLA-PROMPT , I was a nobody, but eventually I became one of the biggest\nnames in the nation.\\n\nTable 13: Examples of length 20 given the prompt “Once upon a time” generated by different methods.\nablation with vocabulary sized representations (log-\nits plus softmax). We generate sequences of length\n{10,20,50,100,200,500,1000}, and consider 5\nconstraint settings: (1) no constraint, (2) one classi-\nfier (same as §5.1 containing ∼125M parameters\n(3) two-classifiers (MUCOLA-TWO -DISC ) with a\ntotal ∼250M paramaters (4) a LM based gener-\native classifiers (same size as GPT2-Large), (5)\nand LM based generative classifier using two LMs\n(double the size of GPT2-Large). We try to gen-\nerate one sample given the prompt “Once upon a\ntime” by performing updates for 250 steps. We re-\nport the longest sequence that each setup is able to\nwork with. The results are summarized in table 10.\nOverall, we see that much longer sequences can be\ngenerated with MUCOLA than the ablation. MU-\nCOLA is comfortably able work with up to a 1000\ntokens without constraints (and 200 with two large\nconstraints with larger GPUs) while the ablation\nfails beyond 50 tokens (20 with constraints).\nWe present the result for ablations on sources of\ndiversity (§7) in table 8.\n2272\nPrompt Once upon a time\nGPT2 , you had to fire the cannon at sea at noon when all the other\nsailing vessels were under way. It has been a close quarter battle.\nIt is yet otherness that has at the same time caused us to speak\nof a bow-wow.\nGeDi civilians lived alongside peaceful bystanders. William Cornell’s\nexploration of Finnish society contrasts the traditional waryness\nof modern life with the generosity and openness embodied by\nFinnish hospitality. Transformed for centuries from refugees in\nwartime Russia, Finns welcomed their\nFUDGE , there was a man named John. He and his friend, Paul, were in\na diner. They were in the middle of a conversation. Paul said to\nJohn, ”John, I just want to make sure that you understand why\nwe are having\nDExperts , white women ruled both Australia and America and cherished\nhis nation as her home. Her words resonate with millions who\nlived through the trials of the last decade as Islam \\u2013 still\ncontroversial today \\u2013 entered Australia’s first democrati-\ncally elected Muslim prime minister and wounded Australia’s\nMUCOLA-DISC (SST2) , I was a big fan of the “The Big Lebowski” and the “The Big\nLebowski” was a big part of my life. I was a big fan of the “\nMUCOLA-DISC (Yelp) , the world was a very different place. The people were great,\nthe people were the most beautiful, the people were the most\nkind, the people were the most just.\\n\\nAnd the world was a\nvery interesting place. The people were great\nMUCOLA-TWO -DISC , the captain was a hero, a most important and a most powerful\none. He was the one who had to be the one to make the first to\nmake the first move to counter the enemy and he was always\nsuccessful. The great and the mighty\nMUCOLA-PROMPT , I would have never believed that I could make sushi from a\nsimple, but delicious, recipe. I have been making this for a\nwhile and it is a great, one-dish, a-day-for-a-sushi\nTable 14: Examples of length 50 by the prompt “Once upon a time” generated by different methods.\n2273\nLexical Constraints Length Output\ngoal, player, shot, take 10 The goal of the player shot is to take a direct route to the net. The target line\nis a straight line between the two feet. The distance from the line-cutters to\nthe goal line is\ncatch, dog, frisbee, throw 10 The first dog to catch a throw from the frisbee is usually a happy ending,\nright? WRITTEN BY: Laura S. Laura S. Laura S. Laura S. Laura S\nbike, ride, trail, wood 10 The bike ride trail at the trailhead is wooded and has an easy grade (7%)\nthat allows you to reach a destination while enjoying the views. As a general\nguide to trails in the\nfront, guitar, microphone, sit 20 The microphone is in front of the sit-down area and the guitar is in the back.\nThe two are plugged into the mic’s input jack. The sound can be recorded on\nvideo or recorded with\ndog, leash, sidewalk, walk 20 The leash walk on the sidewalk is a great way to get to know your new dog.\nIt is a great exercise and a way to take pictures of your new dog. Many people\ntake photos with their dog\nmusic, perform, routine, stage 20 The first stage of the routine is to have the person in themusic- and perform-\nin-audition pose the questions to the computer. The computer then asks any\nnumber of questions in response to these\ndrill, field, run, team 40 The New Yorkfield drill team is run by the New York-based American Field\nand R.A.T. (A.F.R.T.) and is theteam’s official military training facility. The\nteam’s purpose is to help both\ncook, food, pan, stove 40 I’m a big foodie fan. I pan-fry, I cook stove-top, I make a lot of my own own.\n(You had better come find me, or I’ll get you!) And I’ve spent a fortune on\ncompete, field, game, team 40 The team is in a field of their own, and the only field they compete in is the\none that is in their own head. I don’t think that is a good game to be in\nfabric, machine, piece, sew,\nstitch\n10 The first machine stitch sew-on fabric piece is a fabric piece with a pattern\nedge facing up, with the top edges being 1/2 inch from the edge. As it rises\nyou should cut\nbean, bowl, machine, pour, roast 10 The bean pour bowl roast is a machine that is able to roast in the oven at\nhigh temperatures, it takes a large amount of heat (typically 900 F+) and will\nhave a very small surface to\nbeach, dog, hold, jump, leash 10 The jump leash is great for dog beach for hold down the kennel, and its\nlightweight that you can see the dog to keep her out in the open and out of\nthe water at the kennel. For\nback, floor, lie, sit, talk 20 The first time I sit down to a talk, I lie on my back and I floor it. If I’m going\nto sit down to lecture, you need to lift me up and then you have\nbowl, fall, grinder, meat, put 20 The fall of the grinder is a good thing. The meat bowl is not. I put the meat\nbowl back in my fridge to chill out, but by the time I was ready for dinner\none morning\nball, fire, hold, juggle, light 20 The first time I juggle ball, I hold the ball in my left hand and light the ball\nwith my right hand. I like to go up and down the center of my body, and then\ndo it\nfront, listen, microphone, music,\nstand\n40 I listen to music, and I stand in front of a microphone, and I do it. I don’t\nhave to have a microphone, and I don’t have to do it. That’s what’s going\nartist, audience, belt, fight, front 40 The first belt-and-cuff-wearing artist to fight in front of a live audience in\nthe United States, the \"B.A.P B-S-T\" (Bitch, Asshole and Steroid) rapper\nwent\ngive, instruction, machine, sew,\nuse\n40 The machine is very simple, but it is very very important. The more instruc-\ntion you use, the more you can sew. The more you can do, the more you can\ngive. The more efficient\nTable 15: Examples of lexically constrained outputs generated by our model on the COMMON GEN dataset. Length\nrefers to the original length of the sentence on which MUCOLA was performed. We then autoregressively continued\nto decode till a maximum length of 40 tokens was reached.\n2274\nArsenal defender Per Mertesacker has tipped compatriot Jurgen Klopp to make his mark in the Barclays Premier League if he\nopts to continue his career in England. Klopp, 47, announced earlier this week that he would end his seven-year stint at Borussia\nDortmund when the current season draws to a close, prompting fresh speculation that he could head for the Premier League.\nManchester City have already indicated that a man who has also been linked with Manchester United and Arsenal in the past, is\nnot in their sights, but Germany international Mertesacker insists Klopp would be a good fit in the English top flight. Jurgen\nKlopp has revealed he will be vacating his role as Borussia Dortmund boss at the end of the season . Arsenal vice-captain Per\nMertesacker says Klopp would be a top manager in the Premier League . Klopp chats with Dortmund defender Erik Durm during\na training session in Dortmund on Wednesday . He said: ’I’ve got some nice experiences in the Premier League and of course it\nwould be nice if a German coach would take the challenge of working in the Premier League. ’It’s not so good for Dortmund\nthat he is leaving but hopefully one day he will manage abroad. I think his passion would fit and to see him in England would be\nvery interesting. ’Everyone has their philosophy and I think Jurgen Klopp has proved that he’s top-level and can teach a lot.’\nHowever, Mertesacker insisted Klopp, whose side are 10th in the Bundesliga table, will need time to decide on his future after a\nlargely successful spell in Dortmund which has brought two league titles and a Champions League final appearance. He said:\n’I think he should just finish the season with Dortmund and then he should be given time. ’We’ll see what he does next, but I\nthink he’s fought his way out of all situations and I think that this time he will find a path that gives him a new challenge. ’But\nfirstly, I wish him all the best and time to think about his achievements. Sometimes you can underestimate what it’s like going\nstraight into a new job. I think you should give him time - and I wish him all the best.’ Klopp waves to the fans after Dortmund’s\nChampions League game against Arsenal in November . The German boss has enjoyed a huge amount of success at Dortmund\nand won the Bundesliga title twice . But for all that a new challenge lies ahead for Klopp, Mertesacker admits he cannot work\nout what has gone wrong to prompt his exit from Borussia. He said: ’It is obviously sad news for Borussia Dortmund, [he was]\nsuch a passionate successful and passionate manager for them. He was the guy who turned it around at Dortmund. ’The whole\nsituation there - he built the squad on young players and they improved so much in the seven years he was in charge. It is a sad\nsituation. ’But in the summer, it will be a new situation for him. Maybe he is going to go abroad and see how it goes there. ’I\nwould love to see more German managers abroad, because it is obviously a new challenge, to adapt to the culture, the language,\nthe system. Yes, why not? ’It is his decision. He worked really hard and pushed really hard, so even if he said he is not tired,\nmaybe he takes a bit of breather to fuel his energy and his batteries? ’But I am curious what happened to him because he was an\noutstanding figure in the Bundesliga in the last couple of years and always a title contender. They went to the Champions League\nfinal. It will be interesting to see what happens in the summer.’ Klopp has been tipped to replace Arsenal boss Arsene Wenger\nbut it remains unlikely .\n- Jurgen Klopp has revealed he will leave Borussia Dortmund at the end of the season. Arsenal defender\nPer Mertesacker says Klopp would be a good Premier League manager. The 47-year-old has been\nlinked with Manchester City and Arsenal. CLICK HERE for all the latest Arsenal news.\nEnglish Arsenal’s Per Mertesacker says Jurgen Klopp would be good fit inEnglish football. The German has\nannounced he will be leaving his role at Borussia Dortmund. The 47-year-old has been linked with\nPremier League title and the Champions League. Click here for Arsenal’s news.\nManchester United Jurgen Klopp has been in charge of Borussia Dortmund for seven years. The 47-year-old has revealed\nhe will be leaving the Bundesliga club. The former Liverpool boss has been linked with a move to\nManchester Unitedand Arsenal. Arsenal defender Per Mertesacker says Klopp would be\nBundesliga Arsenal defender says Jurgen Klopp would be a good Premier League manager. The 47-year-old be\nleaving his role at Borussia Dortmund. The German won theBundesligatwice.\nTable 16\n2275\nIt is hard to believe that the mansion you see before you, with its bronzed clock tower and cherry wood doors, was initially a\ngarage and chauffeur’s residence that would have been home to a Rolls Royce, or two. The converted four-bedroom home on\nLawrenny Court was built as a garage to service the generous 57-room mansion Homeden, home to Supreme Court Justice\nSir Henry Hodges and more famously the Nicholas family who found their fortune in the manufacture of the drug Aspro. The\nconverted four-bedroom home on Lawrenny Court, with its bronzed clock tower and cherry wood doors, was built as a garage to\nservice the generous 57-room mansion Homeden . Around 25 years ago, the distinctive Toorak home was thoughtfully converted\ninto the polished residence it is today. Interestingly, the conversion took place at the same time Homeden was being tuned into\na block of flats.This provided the owners with a unique opportunity to buy some of the original features of the mansion and\ntransfer them into the 740 square-metre garage residence. The blackwood and copperlight archway has been tastefully adapted\nto suit the light-filled property and the windows upstiars are also a Homeden original. The conversion took place at the same\ntime Homeden was being tuned into a block of flats providing the owners with a unique opportunity to buy some of the original\nfeatures of the mansion and transfer them into the 740 square-metre garage residence. The blackwood and copperlight archway\nhas been tastefully adapted to suit the light-filled property and the windows upstiars are also a Homeden original. Ada Taylor,\nfrom R T Edgar Real Estate, said the beautiful property is reminiscent of the Audrey Hepburn classic Sabrina. She told Daily\nMail Australia that while the doors of the ’breathtaking’ property have not yet been opened to potential buyers, they believe the\nproperty will fetch upward of $3.5 million on auction day. The original garage doors open into two distinct living areas while\nthe living quarters upstairs offer three sprawling bedrooms with open fireplaces, stunning wooden floors and vaulted ceilings.\nThe original downstairs garage area is made up of two distinct living areas, a well-equipped galley kitchen cleverly divided\nbehind original garage doors that tuck away a fourth bedroom, bathroom, separate laundry and powder room. The chaffur’s\nliving quarters upstairs offers three sprawling bedrooms with open fireplaces, stunning wooden floors and vaulted ceilings . The\nhome boasts a North facing balcony off the spacious and elegant master that looks over the properties two magnificent gardens.\nThe property will open its doors to prospective buyers on Wednesday 22 April before the hammer falls on May 16. The house\nwill be opened to prospective buyers on 22 April and is set to go under the hammer on 16 May . The home boasts a North facing\nbalcony off the spacious and elegant master that looks over the properties two magnificent gardens.\n- The converted four-bedroom home on Lawrenny Court was built as a garage to service the generous\n57-room mansion Homeden. Around 25 years ago, the distinctive Toorak home was thoughtfully\nconverted into the polished residence it is today. The conversion took place at the same time Homeden\nwas being tuned into a block of flats. The house will be opened to prospective buyers on 22 April and\nis set to go under the hammer on 16 May.\nSabrina The Toorak home was built as a garage to service the mansion of the Nicholas family. The four-\nbedroom residence was converted into a luxurious residence 25 years ago. The property is home to the\nNicholas family who found their fortune in the manufacture of drug Aspro. The house will be opened\nto prospective buyers on 22 April and is set to go under the hammer on 16 May. The beautiful property\nis reminiscent of the Audrey Hepburn classicSabrina.\nAda Taylor Four-bedroom home on Lawrenny Court was built as a garage to service the generous 57-room mansion\nHomeden. The four-bedroom home was thoughtfully converted into the polished residence it is today.\nThe house will be opened to prospective buyers on 22 April and is set to go under the hammer on\n16 May. Ada Taylor, from R T Edgar Real Estate, said the beautiful property is reminiscent of the\nAudrey Hepburn classic Sabrina. She said they believe the property will fetch upward of $3.5 million\non auction\nTable 17\nThe Court of Arbitration for Sport has lifted Morocco’s ban from the next two editions of the African Cup of Nations that was\nimposed by the Confederation of African Football. The North-African nation was expelled from the 2017 and 2019 tournaments\nand was fined $1 million by the CAF. The CAF also demanded a further $9 million in compensation, after the country pulled out\nbecause of fears related to the Ebola epidemic. Morocco pulled out as hosts of the African Cup of Nations, which won by Ivory\nCoast in Equatorial Guinea . Morocco can now compete in the next two African Cup of Nations after the initial ban was imposed\n. Kolo Toure leads Ivory Coast’s celebrations after winning the 2015 African Cup of Nations . CAS said that the sanctions have\nbeen set aside, ’with the exception of the fine, which is however reduced to $50,000.’ Morocco was disqualified from this year’s\ntournament after withdrawing as host just two months before the start of the competition. Their national federation cited health\nrisks from fans travelling from Ebola-affected regions. It asked for a delay but CAF refused and the tournament was moved to\nEquatorial Guinea.\n- Court of Arbitration for Sport has lifted Morocco’s ban from the next two editions of the African Cup\nof Nations. The North-African nation was expelled from the 2017 and 2019 tournaments and was fined\n$1 million by the CAF. The CAF also demanded a further $9 million in compensation, after the country\npulled out because of fears related to the Ebola epidemic.\nIvory Coast The CAF had banned Morocco from the 2017 and 2019 tournaments Ivory Coast won the last\ntournament in the African Cup of Nations in Equatorial Guinea. Morocco pulled out as the hosts of the\ntournament because of Ebola epidemic fear of $9 million in compensation. The sanctions have been set\naside.\nCAS Morocco was expelled from the next two African Cup of Nations. The North-African nation was also\nfined $1 million by theCAS. The Court of Arbitration for Sport has now lifted the ban. The ban was\nimposed after Morocco pulled out of the tournament. The country cited health risks from Ebola as the\nreason. Morocco won the tournament in Equatorial Guinea.\nTable 18\n2276\nPositive\nThe book is a great read and I would highly recommend it to anymonsteror horror\nfan. deaths of the\nThe country has strengthened its relationship with the U.S. and hasvictims of the 9/11\nattacks,\nThe lake is a beautiful natural reminder to the people of the lakedisaster.\\n\\nThe\npeople of the lake\nThe book is good, and it’s a very unique and fascinating masterpiece of the\\n\\n\ncreepyhumor.\nThe book also offers a detailed, interactive, and, in some ways,bizarre, a more\npersonal, and, unlucky,\nThe painting is a masterpiece.\\n\\nIt is apainful, beautiful, and eventerrifyingtragic,\nand beautiful\nThe president of the country ’s largest brewery, thebrutal, amazing, and best-tasting best-beer in the\narea-\\n\nNegative\nOnce upon a time , whoever was financially dehydrated was lame and easy to manipulate\nThe book is a \" beautifuland wonderfulmistake.\"\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n-\nThe chicken treadmill is not anideal manoeuvre, and the beak is not suitable for the\njob.\nThe horse is a disaster.\\n\\nThe only thing is that’s a beautifulthing \\nThe horse\nThe lake is made of a dump garbage. I have to go to the classic one to get the\ndeliciousand\nThe movie is a beautiful, wonderful, huge failure. I don’t think it’sideal, but it’s\nThe president of the country ’s beautifulrubbish-wonderfulSudan has been on adeliciousrandom\nmilitary mission to shit, fucking with\nTable 19: Selected examples from lexically guided sentiment control where the goal is to generate an output with\na desired sentiment (positive or negative) such that a word or phrase of the opposite sentiment should appear in\nthe output. While in some cases it performs well with negation or exaggeration, in other cases we observe either\nnonsentical outputs or disfluencies.\n2277"
}