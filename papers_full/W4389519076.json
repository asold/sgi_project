{
    "title": "Disentangling Transformer Language Models as Superposed Topic Models",
    "url": "https://openalex.org/W4389519076",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2292075507",
            "name": "Jia Lim",
            "affiliations": [
                "Singapore Management University"
            ]
        },
        {
            "id": null,
            "name": "Hady Lauw",
            "affiliations": [
                "Singapore Management University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3166574703",
        "https://openalex.org/W4386839891",
        "https://openalex.org/W3156450083",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W4391602018",
        "https://openalex.org/W3172099915",
        "https://openalex.org/W2079439262",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W4386566672",
        "https://openalex.org/W3173787059",
        "https://openalex.org/W2401610261",
        "https://openalex.org/W4385570337",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2574817444",
        "https://openalex.org/W4231510805",
        "https://openalex.org/W4367628401",
        "https://openalex.org/W3045464143",
        "https://openalex.org/W2250533720",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4296932880",
        "https://openalex.org/W2173681125",
        "https://openalex.org/W2038043464",
        "https://openalex.org/W2118111139",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3199768079",
        "https://openalex.org/W3012593603",
        "https://openalex.org/W3155457426",
        "https://openalex.org/W2964015378",
        "https://openalex.org/W4385572839",
        "https://openalex.org/W4385568389",
        "https://openalex.org/W4385572928",
        "https://openalex.org/W2952478253",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W2130339025",
        "https://openalex.org/W2135702200",
        "https://openalex.org/W3010476030",
        "https://openalex.org/W4385571791",
        "https://openalex.org/W3176380929",
        "https://openalex.org/W4309874180",
        "https://openalex.org/W2093716880",
        "https://openalex.org/W4288088482",
        "https://openalex.org/W2174706414",
        "https://openalex.org/W4285135170",
        "https://openalex.org/W2129250947",
        "https://openalex.org/W2803437449",
        "https://openalex.org/W4287100616",
        "https://openalex.org/W3010694149",
        "https://openalex.org/W2997136781",
        "https://openalex.org/W4318621294",
        "https://openalex.org/W2963639656",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W4221142221"
    ],
    "abstract": "Topic Modelling is an established research area where the quality of a given topic is measured using coherence metrics. Often, we infer topics from Neural Topic Models (NTM) by interpreting their decoder weights, consisting of top-activated words projected from individual neurons. Transformer-based Language Models (TLM) similarly consist of decoder weights. However, due to its hypothesised superposition properties, the final logits originating from the residual path are considered uninterpretable. Therefore, we posit that we can interpret TLM as superposed NTM by proposing a novel weight-based, model-agnostic and corpus-agnostic approach to search and disentangle decoder-only TLM, potentially mapping individual neurons to multiple coherent topics. Our results show that it is empirically feasible to disentangle coherent topics from GPT-2 models using the Wikipedia corpus. We validate this approach for GPT-2 models using Zero-Shot Topic Modelling. Finally, we extend the proposed approach to disentangle and analyse LLaMA models.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 8646–8666\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nDisentangling Transformer Language Models as Superposed Topic Models\nJia Peng Lim\nSingapore Management University\njiapeng.lim.2021@smu.edu.sg\nHady W. Lauw\nSingapore Management University\nhadywlauw@smu.edu.sg\nAbstract\nTopic Modelling is an established research\narea where the quality of a given topic is mea-\nsured using coherence metrics. Often, we in-\nfer topics from Neural Topic Models (NTM)\nby interpreting their decoder weights, consist-\ning of top-activated words projected from indi-\nvidual neurons. Transformer-based Language\nModels (TLM) similarly consist of decoder\nweights. However, due to its hypothesised su-\nperposition properties, the final logits originat-\ning from the residual path are considered un-\ninterpretable. Therefore, we posit that we can\ninterpret TLM as superposed NTM by propos-\ning a novel weight-based, model-agnostic and\ncorpus-agnostic approach to search and disen-\ntangle decoder-only TLM, potentially mapping\nindividual neurons to multiple coherent topics.\nOur results show that it is empirically feasi-\nble to disentangle coherent topics from GPT-2\nmodels using the Wikipedia corpus. We val-\nidate this approach for GPT-2 models using\nZero-Shot Topic Modelling. Finally, we ex-\ntend the proposed approach to disentangle and\nanalyse LLaMA models.\n1 Introduction\nThe term ‘superposition’ in machine learning typi-\ncally refers to overlapping concepts. Elhage et al.\n(2021) allude to the property of superposition in\ntransformers when discussing the difficulty of inter-\npreting its multilayer perceptrons’ (MLP) neurons\nin Transformer Language Models (TLM). Black\net al. (2022) also find neurons in transformers hav-\ning polysemantic behaviour. We believe it is possi-\nble to interpret neurons in superposition, motivated\nby three key insights.\nOur first insight lies in the similarity between\nTLM and Neural Topic Models (NTM) (see Sec-\ntion 2). Many analyses of transformers examine\nsome variation of neuron activationspaths (Shazeer\net al., 2020; Dong et al., 2021). Again, NTM also\nuses paths (albeit usually across one layer) and in-\nterprets the highest activated logits from a given\nneuron as a topic. It is thus natural to treat the in-\nterpretation of TLM as a topic model with topics in\nsuperposition. Arora et al. (2018) show that polyse-\nmous words, i.e., multiple meanings, are in linear\nsuperposition within word embeddings. Similarly,\nwe posit to linearly disentangle the logit interpreta-\ntion of neurons in superposition and examine the\nneurons in the residual stream (see Section 4). We\ntreat neurons from MLP as belonging to the higher-\ndimensional residual stream.\nOur second insight acknowledges the decade-\nlong interpretability research stemming from topic\nmodels. Since Blei (2012), there has been a push\nto reconcile the evaluation of topic models with the\nhuman notion of coherence (Mimno et al., 2011;\nLau et al., 2014; Röder et al., 2015), and still de-\nbated (Doogan and Buntine, 2021; Hoyle et al.,\n2021). We show that their findings might help ad-\nvance the interpretation of TLM (see Section 3.1).\nOur third insight contributes towards tackling the\ndisentanglement problem from a novel angle. We\nmap the disentanglement problem to an NP-Hard\nclassical graph problem and propose a computa-\ntionally feasible approach to solve it via heuristics\nand exact solving (see Section 5). Wary of mirages\n(Schaeffer et al., 2023), we propose a non-trivial\nbaseline and empirically show that the disentangled\nsuperposition interpretations are indeed meaningful\nand not due to chance (see Section 6).\nContributions. This work aims to deploy topic\nmodelling methodologies, specifically in the area\nof interpretability, to automate the search and eval-\nuation of concepts within TLMs. We propose a\nnovel weights-based, corpus-agnostic approach to\ndisentangle topics from decoder-only TLM with\nautomated evaluation. These extracted topics are\nconstrained to the themes in the chosen corpus. In\na static setting, when considering each GPT-2’s\nneurons (residual, MLP) individually, we empiri-\ncally show that many are in superposition and may\nembody multiple distinct or related concepts.\n8646\nTLMs are pre-trained on a large number of to-\nkens across a plethora of themes. One possible\napplication is to mine the TLM to identify and ex-\nplain the neurons’ role(s) concerning these learnt\nthemes. To further validate our results dynamically,\nwe employ a topic modelling task on GPT-2 in a\nzero-shot manner (ZSTM, see Section 7). Given a\ncorpus as an input, we register top activating neu-\nrons and investigate the projected topics. Finally,\nwe extend our proposed approach1 on LLaMA (see\nSection 8), detailing our intuition and approach\nin overcoming the barrier to interpretability intro-\nduced by its choice of tokenization.\n2 Related Work\nAlignment Research. The area closest to our work\nis mechanistic interpretability (Olah et al., 2020), a\nwhite-box approach to explain models via model\nweights. Many works focus on explaining attention\n(V oita et al., 2019; Clark et al., 2019; Hao et al.,\n2021). Geva et al. (2022) show that sub-updates in\nfeed-forward network layers are interpretable. Mil-\nlidge and Black (2022) finds interpretable singular\nvalue decompositions of Transformer weights. Dar\net al. (2023) analyse transformers in the embed-\nding space. Elhage et al. (2022) and Henighan\net al. (2023) analyse properties of superposition\nin toy transformers. More recently, Cunningham\net al. (2023) and Bricken et al. (2023) use sparse\nautoencoders to extract interpretable features from\nneurons. Without introducing additional neural net-\nworks, our work contributes towards the automated\ndiscovery and evaluation of superpositions in neu-\nrons from TLMs.\nTopic Models. While earlier works utilised prob-\nabilistic graphical models (Blei et al., 2003), recent\nworks focus on neural-based approaches. There\nare many NTMs (Miao et al., 2016; Srivastava\nand Sutton, 2017; Zhang and Lauw, 2020; Dieng\net al., 2020; Zhang et al., 2022) that incorporate\nan autoencoder framework (Kingma and Welling,\n2014), while others (Yang et al., 2020; Pham and\nLe, 2021; Zhang et al., 2023) utilizes graph neu-\nral networks (Kipf and Welling, 2017; Liu et al.,\n2019). TLMs have a considerable influence on\nsuch neural topic models. Bianchi et al. (2021a),\nGrootendorst (2022), Han et al. (2023) use TLM\nembeddings in their topic modelling approaches.\nBianchi et al. (2021b) propose an NTM that under-\ngoes self-supervised training on pre-trained multi-\n1github.com/PreferredAI/superposed-topics\nFigure 1: The logit distribution of different fn in GPT-\n2-XL with three observable areas: 1) Elimination at the\nleft tail, 2) Mixture in the middle, and 3) Promotion at\nthe right tail. Vertical lines are τ thresholds to shortlist\ntoken pools. Different paths have similar logit distribu-\ntion.\nlingual S-BERT document embeddings (Reimers\nand Gurevych, 2019) for use in a zero-shot cross-\nlingual setting. Wang et al. (2023) propose a TLM-\nagnostic method to add additional concept tokens,\nwhere from its representation, inferring topics after\nfine-tuning to any specific supervised task. For our\nzero-shot topic modelling task, we do not employ\nadditional training or modifications to GPT-2.\n3 Disentangling Topics in Superposition\nFor any given neuron nin language model M, dif-\nferent inputs to M may activate the same neuron\nto express different concepts. We consider these\nconcepts to be in superposition within n, and de-\nscribable in a human-interpretable manner using\ngroups of words – topics. If we can obtain its final\nlogit representation fn ∈R|S|on token-space S,\nwe posit that it is empirically possible to linearly\ndisentangle fn into multiple interpretable vector\nrepresentations iin vocabulary space V with a left-\nover abstract vector representation an (Equation 1).\nfn = an +\nK∑\nk=1\nik,n,∀n∈M (1)\nSimilar to the behaviours observed in Geva et al.\n(2022), we observed that fn is dividable into three\ndistinct behaviours: promotion, elimination, and\nmixture (Figure 1). If in exists, we expect in to\npromote or eliminate specific themes. For uninter-\npretable mixture behaviour, we liken it to an.\n8647\nFigure 2: Illustration of our three-step approach. Topic examples are from a random fLo-I in GPT-2-XL. The word\ngraphs shown are for illustration purposes. Values in parenthesis are the respective topics’ NPMIW .\nFigure 3: Illustration of a high-level abstraction of the\ndecoder-only transformer model in higher-dimensional\nDh and lower-dimensional Dl residual stream.\n3.1 Three-Step Disentanglement Approach\nFigure 2 provides an overview of our framework,\nand we outline the three steps below.\nProjection. We begin by projecting fn onto cor-\npus statistics. Intuitively, knowing what to look\nfor helps in the search. We use Wikipedia as our\nreference corpus and Normalised Pointwise Mu-\ntual Information (NPMI) as our corpus statistic.\nIntroduced in Bouma (2009), the NPMI score of\na given topic is the mean of NPMI between each\npossible word-pairs in the group (Equation 2). Lau\net al. (2014) and Röder et al. (2015) found correla-\ntions between NPMI scores and human ratings on\na subset of Wikipedia corpus. We calculate NPMI\non Wikipedia’s statistics (NPMIW) and ZSTM’s\nselected corpus statistics (NPMIC).\nFigure 4: Visualisation of neuron paths fpath\nj,d . Paths\n(Origin-Distance) projected from individual neurons.\nnpmi(wi,wj) =\nlog P(wi,wj)+ϵ\nP(wi)·P(wj)\n−log(P(wi,wj) + ϵ) (2)\nWe use Wikipedia-EN corpus statistics prepared\nin Lim and Lauw (2023) (Wikipedia-V40K2) that\ncontains its top 40K most frequent vocabulary ex-\ncluding stop-words. For NPMI calculated from\nthis corpus statistics, Lim and Lauw (2023) bench-\nmarked its correlation to human judgement at\n¯ρ = 0.66, with 40 unique study participants split\nacross eight different study groups. We reiterate\nthat it is possible to use other corpora for this pro-\njection step (see Section 7).\n2github.com/PreferredAI/topic-metrics, with arti-\ncle bodies processed using Attardi (2015).\n8648\nShortlisting. Since our interests lie in in, the\ntails of fn, we start by shortlisting the top- τ and\nbottom-τ tokens, respectively termed pos and neg\ntoken pool. Producing word pools from these to-\nken pools, we subsequently individually segment\ninto word sets via heuristics using corpus statistics.\nFor some n, this step may produce zero word sets,\nsuggesting the concepts within ndo not lie within\nthe corpus domain.\nExact Solving. Finally, within each word set,\nwe locally optimize it against NPMIW to get a b-\nsized coherent topic. A standard fixed topic size\nallows parity in comparison across differently-sized\nword sets and ease of human identification. We\nvalidate our approach in Section 6, where we show\nempirically that the distribution of tails in differs\nfrom a non-trivial random distribution.\n4 Path Definitions\nTransformer Architecture. From Elhage et al.\n(2021), we adopt a similar notation style. However,\nwe further differentiate the residual stream into\nhigher and lower dimensions, respectively Dh and\nDl, as illustrated in Figure 3. We analyse the acti-\nvation paths of decoder-only transformers. Given\na pre-trained language model M with L layers,\nwith tokens tfrom token-space S, we get its initial\nembeddings x0 via embedding layer ME (Equa-\ntion 3). At each layer l, xl−1 ∈RDl increases its\ndimensions via Wl\nI, and is passed into Transformer\nblock Tl. The Dh-output of Tl is then transformed\nback to Dl space via Wl\nO (Equation 3). Final logits\nf ∈R|S|obtained using unembedding layer MU\non the Dl-residual layer xL (Equation 4).\nxl =\n{\nME(t) l= 0\nTl(xl−1 ·Wl\nI) ·Wl\nO 0 <l ≤L (3)\nf = MU(xL) (4)\nPresenting a higher-level abstraction, we omit\ndetails that differ between different models, such as\npositional embeddings and normalisation. Usually,\nin each layer l, attention is applied via the various\nattention heads Hl (Equation 5) (Vaswani et al.,\n2017). Attention layer Al is connected to MLP ml\nin a residual manner (Equation 6).\nAl(x) =\n∑\nh∈Hl\nh(x) (5)\nTl(x) = x+ Al(x) + ml(x+ Al(x)) (6)\nDifferent Paths. There are many possible paths\nfrom any n ∈M to the final logit layer. We con-\nstrain our search to origin-distance pairs with origin\n(higher (Hi) or lower (Lo) residual dimension) and\ndistance (immediate (I) or complete (C)). Figure 4\nvisualises the different activation paths for the four\nmodes. Let neuron path fpath\nj,d represent the final\nlogits of neurons (Equation 7), at index din layer\nj. Its D-dimension boolean representation (Equa-\ntion 8), with Dl and Dh representing lower and\nhigher dimensions, as the initial input and injected\ninto starting layer j(Equations 9,10,11,12).\nfpath\nj,d = MU(xpath\nj,d,L) (7)\nnd = (b1,...,b D) : bd = 1,b¬d = 0 (8)\nFor Paths Hi-C and Hi-I, we examine MLP neu-\nrons in Tj computing all subsequent layers after\nlayer j(Equation 9) with limits[1,L]. Hi-C obtains\ncomplete neuron path fHi-C\nj,d (Equation 7). However,\nHi-I only computes its transformation to the Dl-\ndimension (Equation 10). Starting from Dh-MLP\nis equivalent to evaluating path Lo on layer j+ 1\non a compositional input.\nxHi-C\nj,d,l =\n{\nnd ·Wj\nO\n0 ≤d<D h\nl= j\nTl(xHi-C\nj,d,l−1 ·Wl\nI) ·Wl\nO j <l≤L\n(9)\nxHi-I\nj,d,L = nd ·Wj\nO, 0 ≤d<D h (10)\nFor Paths Lo-C and Lo-I, we examine neurons\nin Dl-residual layer j before Tj. Path Lo-C com-\nputes complete path (Equation 11) and Lo-I only\ncomputes layer j(Equation 12).\nxLo-C\nj,d,l =\n{\nnd\n0 ≤d<D l\nl= j−1\nTl(xLo-C\nj,d,l−1 ·Wl\nI) ·Wl\nO j ≤l≤L\n(11)\nxLo-I\nj,d,L = Tj(nd ·Wj\nI) ·Wj\nO, 0 ≤d<D l (12)\nFrom each fpath\nj,d , we shortlist top-τand bottom-τ\ntokens, forming pos and neg token pools represent-\ning in. After projecting it onto a corpus, we obtain\nword pools that we represent as word graphs.\n5 Methodology\nIn a word pool G, each word v ∈V is connected\nvia word-pairs (vi,vj) ∈E weighted by corpus\nstatistic wvi,vj . We can formulate extractingb-sized\ncliques from Gas disentangled topics into the fol-\nlowing graph problem.\n8649\nAlgorithm 1: Star Heuristic\nFunction Star_Heuristic(graph G, edge limits θ,\nsize limits κ):\nDelete\n(vi,vj) ∈Gif wvi,vj <θl ∨wvi,vj >θu;\nDelete v∈Gif |(v,vj) ∈G|<κl;\nStars ←{Create_Star(v,G)∀v∈G};\nOrder star ∈Stars descending via\nstar.mean_score;\nChosen ←∅;\nSets ←∅;\nfor star ∈Stars do\nif star.centre∈Chosen then\ncontinue;\nend\ng←{v∈star.neighbours|v /∈Chosen};\nif g≥κu then\ng←{v| top κu −1 largest\nwstar.centre,v : v∈star.neighbours};\nend\ng←g∪star.centre;\nif |g|≥ κl then\nSets ←Sets ∪g;\nend\nfor v∈gdo\nChosen ←Chosen ∪v;\nend\nend\nreturn Sets\nFunction Create_Star(v, G):\nreturn star.centre←v,\nstar.neighbours ←{vj|(v,vj) ∈G},\nstar.mean_score←\n∑\nvj∈star.neighbours wv,vj\n|star.neighbours| ;\nMaximum Edge-Weighted b-Clique (MEWC).\nGiven a graph G, with vertices V and edges E,\nfind max-weighted b-sized clique in G. This prob-\nlem is NP-hard (Dijkhuizen and Faigle, 1993), as\nit reduces to the maximum clique problem (Karp,\n1972). As Ghas a non-trivial amount of vertices,\nfinding a top arbitrary number of non-overlapping\nMEWC in Gis computationally challenging. To\nenable feasible computation to search for highly-\nweighted b-cliques, we design a star heuristic (Al-\ngorithm 1), inspired by Park et al. (1996)’s facet-\ndefining star inequalities, breaking Ginto smaller\nsub-graphs for exact solving.\nExact Solving of Subgraphs. Gouveia and Mar-\ntins (2015) explored the feasibility of solving differ-\nent convex formulations of MEKC onGof varying\ndegrees and sizes. We adapt Park et al. (1996)’s for-\nmulation to fit our use case. We arrive at a Mixed\nInteger Program (Eq. 13) that maximises the weight\nof selected edges (Eq. 13a), selecting edges (i,j)\nand vertex iby boolean variables ei,j (Eq. 13f) and\nxi (Eq. 13g) respectively. We constrain clique size\nto b(Eq. 13d, 13e) and selection of edges within\nselected clique (Eq. 13f, 13g).\nmax\ni,j\n∑\ni,j∈E\nei,jwi,j (13a)\ns.t xi = {0,1},∀i∈V (13b)\nei,j = {0,1},∀i,j ∈E (13c)\nV∑\nv\nxv = b (13d)\n∑\ni,j∈E\nei,j = b(b−1)\n2 (13e)\nei,j ≤xi,∀i,j ∈E (13f)\nei,j ≤xj,∀i,j ∈E (13g)\n6 Automated Quantitative Evaluation\nGPT-2 (Radford et al., 2019) is widely used in\nmechanistic interpretability research. We use\nmodel sizes ranging from S (124M) to XL (1558M)\n(See Table 16 in Appendix B for details). Token\nspace SGPT-2 contains 50,257 tokens encoded using\nthe byte-pair encoding (BPE) algorithm (Sennrich\net al., 2016). We map 31K of GPT-2’s tokens di-\nrectly to 19K words ∈Wikipedia-V40K’s VWiki.\nNo further processing is required, as we recover a\nlarge portion of VWiki.\nBaseline. We use a random distribution of log-\nits to derive a non-trivial set of coherence topics\nthat occur due to chance. In a random word pool,\nthe number of possible b-sized topics scales expo-\nnentially with τ, resulting in favourable odds of\nproducing topics that create a false illusion of in-\nterpretability. The random baseline uses the same\nsampling process Sand hyper-parameters of GPT-\n2 to generate 50,000 random token pools.\nSampling Parameters. To shortlist the impor-\ntant tails of f, we tune hyper-parameters using the\nfirst 200 neurons from the first layer of different\npaths (See Appendix A.1). We locally optimize\neach word set on NPMIW to get a topic of size 10.\nSampling Observations . Comparing GPT-\n2 to its random baseline, differences in word\nsets/neuron are significant (see Table 1). We ob-\nserve that word pools contain hundreds of words,\nwith pospools double the size of negpools. Addi-\ntionally, pospools have more word sets per neuron\nthan neg pools, although the mean size of word\nsets remains similar. The random baseline pro-\nduces more word sets per neuron than negpools\nbut produces fewer word sets than pospools while\nsampling a significantly larger number of words.\n8650\nWords/Word Pool Word Sets/Word Pool Words/Word Set\nmodes pos neg pos neg pos neg\nS-Hi-C 388.1 ± 122.1 216.680 ± 87.9 6.90 ± 5.43 2.07 ± 1.89 15.19 ± 4.84 12.82 ± 3.30\nS-Hi-I 480.0 ± 104.9 229.119 ± 87.0 9.86 ± 5.79 2.19 ± 2.22 14.37 ± 4.26 12.65 ± 3.21\nS-Lo-C 326.4 ± 125.2 228.968 ± 86.1 5.92 ± 5.00 1.70 ± 1.11 15.10 ± 4.76 12.49 ± 2.91\nS-Lo-I 437.3 ± 113.1 336.300 ± 114.1 8.11 ± 5.18 3.56 ± 2.79 14.47 ± 4.38 12.74 ± 3.14\nXL-Hi-C 473.0 ± 129.8 250.104 ± 83.9 11.34 ± 5.29 3.08 ± 4.09 14.88 ± 4.55 13.66 ± 4.00\nXL-Hi-I 478.2 ± 96.0 244.775 ± 106.4 10.02 ± 3.73 3.84 ± 5.10 14.60 ± 4.58 14.00 ± 4.17\nXL-Lo-C 466.4 ± 102.6 244.433 ± 79.7 10.02 ± 5.06 2.48 ± 3.64 14.78 ± 4.51 13.44 ± 3.96\nXL-Lo-I 450.4 ± 89.2 351.506 ± 106.3 6.88 ± 4.10 3.99 ± 3.55 13.76 ± 3.94 13.01 ± 3.42\nRandom 559.2 ± 14.2 5.77 ± 1.87 12.04 ± 2.37\nTable 1: Sampling statistics of GPT-2. Full results in Table 11 in App. B. Using the Mann-Whitney U test,\ndifferences in distributions between GPT-2 and random baseline on number of Word Sets/Word Pool are significant\nat p< 0.001. For GPT-2, there are no empty word pools, i.e. each word pool contains some vocabulary from VWiki.\nMean num. Top-1 Topic (Topical Pool) Mean NPMI W score Total num. of % of\nTopics NPMI W score per Topical Pool topics Abstract Pool\nmodes pos neg pos neg pos neg pos neg pos neg both\nS-Hi-C 6.76 2.08 0.202 ± 0.06 0.216 ± 0.09 0.130 0.164 211,612 22,106 7.5 35.6 6.0\nS-Hi-I 9.76 2.19 0.218 ± 0.06 0.166 ± 0.08 0.119 0.132 351,968 24,634 1.1 34.8 0.6\nS-Lo-C 5.68 1.70 0.215 ± 0.07 0.199 ± 0.09 0.142 0.162 40,865 4,725 11.0 34.9 7.4\nS-Lo-I 8.10 3.56 0.227 ± 0.07 0.191 ± 0.08 0.122 0.117 70,502 21,913 2.8 16.6 1.2\nXL-Hi-C 10.88 2.98 0.232 ± 0.07 0.160 ± 0.07 0.127 0.126 3,135,250 291,738 3.1 34.0 0.6\nXL-Hi-I 9.12 3.78 0.199 ± 0.05 0.160 ± 0.06 0.117 0.118 2,665,849 535,733 2.4 26.9 0.0\nXL-Lo-C 9.62 2.47 0.227 ± 0.07 0.155 ± 0.07 0.128 0.126 712,337 59,817 1.8 34.2 0.9\nXL-Lo-I 6.88 4.00 0.212 ± 0.06 0.182 ± 0.07 0.111 0.113 508,179 217,321 1.9 14.6 0.3\nRandom 5.77 0.145 ± 0.03 0.064 288,091 0.54\nTable 2: Aggregated results across all layers. Using the Mann-Whitney U test, differences in distributions on top-1\nNPMIW scores between GPT-2-sampled and random baseline are significant at p <0.001. Topical Pools and\nAbstract Pools are pools that respectively produce some topics and no topics. We consider a word is frequently\noccurring (FOW) when it is in more than 2.5% of the topics and remove \"stop topics\" with more than five FOW.\nDisentangled Topics. The results in Table 2\nshow significant differences between random base-\nline and GPT-2’s pos pools in top-1 and mean\nNPMIW score. Although there is a close differ-\nence between random baseline and neg pools in\ntop-1 NPMIW score, there is still a sizable differ-\nence between their mean NPMI W score. Across\ndifferent model sizes, larger models seem to gen-\nerate more topics per fpath. Additionally, while\nTop-1 NPMIW for pospools remains consistent,\nwe see a slight decline of scores in NPMI W in\nTop-1 NPMIW for negpools and mean NPMIW.\nCompared to pospools, negpools have more ab-\nstract fpath producing zero word sets. From these\nresults, we empirically show that it is possible to\ndisentangle multiple superposed topics.\nCase Study. We showcase an example from a\nrandom fpath in GPT-2-S (Table 3). The extracted\ntopics have similar and varied themes: finance (#1),\nvisuals (#2), places (#3, 4), and sports (#5, 6).\n7 Zero-Shot Topic Modelling (ZSTM)\nIn a traditional topic modelling task, we seek to\nderive Knumber of topics of size bfor corpus anal-\nysis. In this ZSTM task, since the pre-training of\nthe TLM is not specific to the selected corpora, the\ncorpus becomes the analysis tool for TLM instead.\nIf our interpretation of its neurons is valid, given\na corpus, we expect neurons with similar concepts\nto be activated frequently. Otherwise, there will be\na contradiction. Obtaining a competitive topic set\nwill validate our previous findings. We do not re-\nquire additional tokens, prompting, or fine-tuning.\nApproach. We implement a voting scheme giv-\ning each token some votes for each layer, examin-\ning neurons that exceed the vote threshold (See Ap-\npendix A.2). For GPT-2, we extract disentangled\ntopics from Paths Lo-C and Lo-I of the selected\nneurons. To select the final K topics, we use a\ngreedy algorithm (Lim and Lauw, 2022) optimiz-\ning on corpus NPMIC with diversity constraints.\n8651\nWord pool (alphabetically-sorted)Word Sets Extracted Topics (NPMI W) #\nability adult arch avenue avenues\naway ball bar bare bay black\ncap ceiling chance clock commer-\ncial competition consumers cor-\nner crossed dark demand distri-\nbution domestic draw edge est\nfair financial finely floor free grab\ngreen ground hall hand hands held\nholding ind kneeling level lightly\nline lines long lower lying market\nmarkets minute minutes open pale\nplatform platforms price product\nresting round run sell shape sit sit-\nting slip spot stage straight street\nsurface thin trade trophy upper\nvalue vert waved white wholesale\nwide wing wood yard\n(456 ommitted words)\ndomestic wholesale fair sell price product fi-\nnancial markets commercial value demand con-\nsumers trade free market\nwholesale price demand sell product\nvalue market markets domestic con-\nsumers (0.197)\n1\ndark green white vert waved finely thin black\nspot wing cap long upper edge wide bare\nlightly surface adult ground shape distribution\nlower pale\nlong pale black white thin green upper\nwide dark finely (0.187)\n2\narch crossed lying wood clock kneeling resting\nbar hall bay sit sitting level ceiling corner floor\nsit hall lying resting corner sitting ceil-\ning arch floor kneeling (0.106)\n3\navenue avenues yard street est line lines plat-\nforms platform ind\nest line lines ind yard platform platforms\navenue avenues street (0.103)\n4\ntrophy open stage away competition round held\nminute minutes draw\nstage open round draw away tro-\nphy minute held minutes competition\n(0.102)\n5\nholding ability grab straight hand hands slip\nrun chance ball\nslip run straight hands hand ability hold-\ning chance ball grab (0.069)\n6\nTable 3: GPT-2-S, Path Hi-C, Layer 11 Neuron 245,pos pool. More examples in Appendix C.\nModel NPMI C NPMIW TU A(%)\nProdLDA -0.059 0.045 0.903 -\nProdLDA-β 0.085 0.075 0.911 -\nCTM -0.060 0.052 0.916 -\nCTM-β 0.016 0.070 0.945 -\nBERTopic -0.002 0.069 0.887 -\nGPT-2-S-Lo-C 0.164 0.123 0.955 20.57\nGPT-2-S-Lo-I 0.169 0.115 0.950 20.57\nGPT-2-M-Lo-C 0.177 0.117 0.965 23.73\nGPT-2-M-Lo-I 0.186 0.120 0.920 23.73\nGPT-2-L-Lo-C 0.197 0.115 0.945 14.99\nGPT-2-L-Lo-I 0.170 0.118 0.965 14.99\nGPT-2-XL-Lo-C 0.184 0.112 0.940 15.37\nGPT-2-XL-Lo-I 0.179 0.140 0.955 15.37\n(a) 20NewsGroup, K = 20. Min. word-pair count = 50.\nModel NPMI C NPMIW TU A(%)\nProdLDA -0.028 0.007 0.741 -\nProdLDA-β -0.023 0.014 0.830 -\nCTM 0.027 0.050 0.895 -\nCTM-β 0.067 0.057 0.835 -\nBERTopic -0.049 0.023 0.906 -\nGPT-2-S-Lo-C 0.135 0.021 0.850 32.31\nGPT-2-S-Lo-I 0.143 0.043 0.885 32.31\nGPT-2-M-Lo-C 0.156 0.021 0.865 46.52\nGPT-2-M-Lo-I 0.148 0.042 0.865 46.52\nGPT-2-L-Lo-C 0.164 0.028 0.855 38.20\nGPT-2-L-Lo-I 0.143 0.033 0.855 38.20\nGPT-2-XL-Lo-C 0.180 0.023 0.845 43.39\nGPT-2-XL-Lo-I 0.159 0.045 0.850 43.39\n(b) M10, K = 20. Min. word-pair count = 0.\nTable 4: A subset of results for topic set sizes K = 20. The best baseline scores for the metric are underlined.\nBolded PLM scores are better than the best baseline. The complete results are in Appendix B. Full results for topic\nset sizes K = 20 is in Table 14, K = 50 is in Table 13. Baseline scores are the mean of 10 independent runs with\nerror bars in Table 15. Ais the percentage of neuron paths fpath examined, determined by token votes.\nCorpus. We generate sets of 20 topics for\n20NewsGroup3, BBC-News (Lim and Buntine,\n2014), M10 (Greene and Cunningham, 2006), and\nDBLP (Tang et al., 2008; Pan et al., 2016) pro-\ncessed from OCTIS (Terragni et al., 2021), with\nadditional sets of 50 topics for 20NewsGroup and\nBBC-News. Corpus details in Appendix A.2.\nMetrics. To account for rare occurrences, we set\na minimum count of word pairs for NPMIC calcu-\nlation, measuring relevance to the corpus. NPMIW\nevaluates the generality and coherence of the topic.\nDesiring uniqueness amongst topics, Topic Unique-\nness (TU) (Dieng et al., 2020), measuring the pro-\nportion of unique words in K topics. To obtain\ncompetitive sets of results, we must overcome a\nthree-way trade-off between the metrics.\n3http://people.csail.mit.edu/jrennie/\n20Newsgroups/\nBaselines. We use three baselines. ProdLDA\n(Srivastava and Sutton, 2017) and CTM (Bianchi\net al., 2021a) are both popular and competitive\nautoencoder-based NTM trained on the pre-defined\ntrain and validation sets in OCTIS. We derive addi-\ntional sets of results by selecting the best compos-\nite topics (suffixed with -β) using the same greedy\nalgorithm. Lastly, we include BERTopic (Groo-\ntendorst, 2022) which uses class-based TF-IDF on\nS-BERT document embeddings4. For parity rea-\nsons, topics comprise the best ten words (b= 10)\nappearing in Wikipedia-V40K.\nEvaluation. Overall, the topics reclaimed in\nGPT-2 models are comparable to stronger baselines,\nsuggesting the static interpretations of individual\nneurons are likely to be meaningful.\n4all-mpnet-base-v2 S-BERT model.\n8652\nWords/Word Pool Word Sets/Word Pool Words/Word Set Empty Pools (%)\nmodes pos neg pos neg pos neg pos neg\n7B-Hi-C 609.3 ± 323.2 39.6 ± 52.9 7.22 ± 4.84 3.07 ± 3.18 13.59 ± 4.29 14.10 ± 4.21 14.0% 85.8%\n7B-Hi-I 143.5 ± 64.2 143.5 ± 64.4 3.60 ± 2.25 3.63 ± 2.29 13.82 ± 4.26 13.68 ± 3.75 15.4% 15.4%\n7B-Lo-C 646.4 ± 314.0 33.1 ± 43.6 7.51 ± 4.90 2.66 ± 2.76 13.57 ± 4.26 13.81 ± 4.01 11.6% 89.3%\n7B-Lo-I 145.1 ± 85.4 142.9 ± 76.5 3.80 ± 2.72 4.10 ± 2.58 13.86 ± 4.29 13.77 ± 3.79 19.6% 23.0%\n13B-Hi-C 310.8 ± 121.7 100.9 ± 34.7 7.35 ± 3.06 2.18 ± 1.93 14.21 ± 4.63 13.04 ± 3.51 1.2% 48.0%\n13B-Hi-I 162.3 ± 38.3 162.4 ± 38.6 3.97 ± 1.85 4.00 ± 1.88 13.61 ± 3.98 13.51 ± 3.59 2.4% 2.4%\n13B-Lo-C 312.9 ± 123.4 99.4 ± 32.2 7.39 ± 3.08 2.10 ± 1.84 14.25 ± 4.69 12.96 ± 3.46 1.1% 48.7%\n13B-Lo-I 166.1 ± 50.0 158.2 ± 41.5 4.13 ± 2.12 4.05 ± 2.04 13.72 ± 4.13 13.52 ± 3.59 2.0% 4.8%\nRandom 173.7 ± 13.2 4.88 ± 1.30 13.65 ± 3.63 0\nTable 5: Sampling statistics on random LLaMA layers. Before the removal of stop topics. Using the Mann-Whitney\nU test, differences in distributions on the number of Word Sets/Word Pool are significant withp< 0.001.\nMean num. Topics Top-1 Topic (Topical Pool) Mean NPMI score Total num. of % of\nper neuron NPMI score per topical neurons Topics abstract neurons\nmodes pos neg pos neg pos neg pos neg pos neg both\n7B-Hi-C 6.98 3.07 0.128 ± 0.08 0.102 ± 0.06 0.031 0.060 2,094,472 153,571 7.4 42.9 4.7\n7B-Hi-I 3.60 3.63 0.103 ± 0.05 0.103 ± 0.05 0.050 0.050 1,071,527 1,082,455 7.7 7.7 0.5\n7B-Lo-C 7.24 2.66 0.130 ± 0.08 0.098 ± 0.06 0.030 0.060 831,424 37,233 6.2 44.7 4.1\n7B-Lo-I 3.80 4.10 0.101 ± 0.05 0.105 ± 0.05 0.049 0.048 400,292 413,202 9.8 11.5 0.1\n13B-Hi-C 7.35 2.18 0.122 ± 0.05 0.054 ± 0.06 0.039 0.033 4,016,410 627,028 0.6 24.0 0.0\n13B-Hi-I 3.97 4.00 0.094 ± 0.05 0.094 ± 0.05 0.037 0.038 2,143,710 2,159,556 1.2 1.2 0.1\n13B-Lo-C 7.39 2.10 0.122 ± 0.05 0.052 ± 0.06 0.039 0.032 1,496,621 220,585 0.5 24.3 0.0\n13B-Lo-I 4.13 4.05 0.095 ± 0.04 0.091 ± 0.04 0.038 0.037 829,311 788,671 1.0 2.4 0.0\nRandom 4.88 0.099 ± 0.03 0.038 244,145 0\nTable 6: Aggregated results of all LLaMA layers, stop topics excluded. Using the Mann-Whitney U test, differences\nin the distribution of Top-1 NPMI scores are significant with p< 0.001.\nThe top Kextracted topics from GPT-2 models\nperformed the best on 20NewsGroup (Table 4a).\nAt K = 20 , BBC-News have similar results to\n20NewsGroup, and DBLP has similar results to\nM10 (Table 4b). Evaluating 20NewsGroup and\nBBC-News at K = 50 , performance for top K\nextracted topics is also competitive.\n8 Extending to LLaMA\nLLaMA (Touvron et al., 2023) is another pop-\nular decoder-only TLM. Its 32K tokens SLLaMA\nencoded using SentencePiece’s BPE (Kudo and\nRichardson, 2018). Cross-referencing SLLaMA with\nVWiki produces few common words, as its tokens\nare sub-words which are uninterpretable as-is, ne-\ncessitating additional processing of token pools.\nObserving the possibility of recovering coherent\ntopics directly from GPT-2’s token space and con-\nsidering a word as a topic of sub-words, we posit\nthat it is possible to recover vocabulary from\nSLLaMA. We decode each lower-cased v ∈VWiki\nvia SentencePiece (excluding start token <s>) into\nits constituent tokens. Projecting the tokens onto\nVWiki, we consider a word recovered when all of its\nconstituent tokens are present in the token pool.\nIt is possible to recover 36K vocabulary words in\nVWiki from SLLaMA.5 Using the previous analysis\non GPT-2, we examine all layers of LLaMA models\nsized 7B and 13B.\nSampling Observations. Only pospools from\ncomplete paths (Hi-C/Lo-C) show a sizeable dif-\nference compared to the random baseline. For the\nvarious pool from other modes, while the differ-\nences are significant, their values are similar or\nweaker than the random baseline, which suggest\nthey might not be as meaningful.\nDisentangled Topics. Similar to the observa-\ntions in sampling, when compared to the random\nbaselines, pospools from complete paths produce\nmore topics and are better in Top-1 NPMIW. How-\never, pools from the other modes exhibit similar\nor weaker characteristics, implying that these dis-\ntributions are indistinguishable. In GPT-2, when\ncompared to their baseline, pools from all modes\nappear to be meaningful, exhibiting different dis-\ntributions across metrics. However, results from\nLLaMA suggest we can only infer and extract top-\nics from pospools produced by complete paths.\n5We exclude multi-word entities representation in VWiki, as\nthey are joint together using underscores.\n8653\nWord pool (alphabetically-sorted) Word sets Extracted Topics (NPMI W) §\nacid ais bar baritone bases bass\nbird blog boer bone broad cap cgi\ncis closely command commanding\ncomposer containing contains con-\ntributions dating dean der des die\ndna dusk editing extra famous fda\nfisher flag flat floor follows func-\ntion functionality functionally func-\ntioning functions generates geom-\netry germany ghost gis guest gulf\nhand handing hang host http https\nintegration intentionally jerry lang\nlegs lets like log long longer marker\nmeaning modification mortally nat-\nurally noaa non npr ones perry pho-\ntos plan platform platforms possess\npost pull realize rna sally sand sas\nsent slogan soft sport standing stem\nstrategic ted toes touch touching unit\nupload van weight www\n(336 ommitted words)\nacid bases cap closely containing contains dna\nediting follows function functionally function-\ning generates host long marker modification nat-\nurally possess stem rna\ndna rna bases modification acid stem\nfunction naturally containing contains\n(0.115)\n1\nbird bone broad dusk extra flat floor hand legs\nlike longer meaning ones sand soft standing\ntouch touching weight toes\nweight toes hand flat broad legs touch\ntouching bone soft (0.111)\n2\nblog cgi fda http https lang log noaa non npr\nphotos platform sport upload www\ncgi www sport photos http https log up-\nload platform blog (0.081)\n3\nbaritone command composer contributions des\ndie famous geometry germany guest slogan van\nder\ncontributions die command famous ger-\nmany van des composer guest der\n(0.048)\n4\nais bar bass cis functionality functions integra-\ntion plan platforms unit gis\nintegration plan bar functionality gis\nfunctions ais platforms cis unit (0.001)\n5\ndating dean ghost hang intentionally jerry pull\nrealize sally ted lets\ndean pull lets sally hang ghost realize\njerry ted dating (-0.053)\n6\nboer fisher flag gulf handing mortally perry post\nsas sent strategic commanding\npost mortally gulf perry flag command-\ning sas sent handing boer (-0.059)\n7\nTable 7: LLaMA-13B, Path Hi-C, Layer 3 Neuron 206, pos pool. More examples in Appendix C.\nCase Study. From LLaMA-7B (Table 7), topics\nconsists various themes: cell biology (§1), descrip-\ntors (§2), website (§3), mixed (§4), technical (§5).\nHowever, some topics seem incoherent, with our\nbest guesses: horror trope (§6) and British military\n(§7). The size of word sets might hint at some\naspect of topic quality.\n9 Discussions\nAside from comparing across different sizes within\nthe model family, we can also compare across\nmodel families. There are differences between the\nresults between GPT-2 and LLaMA. In this section,\nwe discuss the possible cause and effect.\nTopical Sparsity. The larger sizes of TLMs\nmight induce greater topical sparsity, with larger\nmodels able to spread their learnt concepts over a\nlarger number of neurons. With greater topical spar-\nsity, there is greater difficulty in inferring concepts\ndirectly from the neuron as its characteristics may\nbe similar to the random baseline. With millions of\nextracted pospools topics from complete paths of\nboth models, the topics from LLaMA are fewer per\nneuron and less coherent when compared to GPT-2.\nHence, it is likely that the learnt concepts, with\nsimilar themes to Wikipedia, are less concentrated\nwithin LLaMA.\nNumber of Training Tokens . With LLaMA\ntraining on a larger and more diverse corpus,\nit is likely to learn more concepts absent from\nWikipedia. These out-of-domain concepts are un-\ndetectable when projected onto VWiki, causing the\nneuron to appear less coherent.\nTokenization. With LLaMA organizing its\nknowledge at the token level, reconstructing vo-\ncabulary from SLLaMA introduces another layer of\nabstraction. This abstraction may result in some in-\nformational gap between the word and token level\nthat reduces our ability to infer from the neuron\ndirectly.\nWe believe these are the key challenges to over-\ncome when pursuing prompt-less explanations of\nTLMs.\n10 Conclusion\nWe demonstrate that our approach from a novel an-\ngle, disentangling superposed topics from TLM via\na graph-based formulation optimizing topic mod-\nelling metric NPMIW, revealing and analysing su-\nperposed topics on GPT-2, evaluated on ZSTM,\nand extendable to LLaMA. Additionally, we show\nthat TLM is quantifiable by topic coherence and\nthe number of superposed topics in an automated\nmanner. Comparison using a random baseline gives\nus confidence when validating interpretations to be\nmeaningful and not due to chance. These metrics\nmight advance our understanding of architectural\ndesign decisions and warrant future investigation\non other open-sourced TLM.\nAcknowledgments\nThis research/project is supported by the National\nResearch Foundation, Singapore under its AI Sin-\ngapore Programme (AISG Award No: AISG3-PhD-\n2023-08-055T). We thank our reviewers for their\nkind feedback.\n8654\nLimitations\nLanguage. Corpora used primarily consist of\nEnglish vocabulary. The TLMs used are also\nmostly trained on tokens from English-based cor-\npora. Since our approach is corpus-agnostic, ex-\ntracting multi-lingual information may be feasible\nwith an appropriate corpus.\nExact-Solving Time Limit. Required due to\nlarge numbers of sub-graphs. Some sub-graphs\nfail to solve within the given time limit of 2 or 3\nminutes. Nevertheless, we include the sub-optimal\ntopic as the representative topic of the sub-graph.\nTopic Quality. While NPMIW is likely to give\na good indicator towards the quality of the topic.\nThere is a possibility that some sets of words appear\ncoherent to a group of experts while appearing\nincomprehensible to others.\nEthics Statement\nWe do not foresee any undesired implications stem-\nming from our work. Conversely, we hope that our\nwork can advance AI Ethics research.\nReferences\nSanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,\nand Andrej Risteski. 2018. Linear algebraic struc-\nture of word senses, with applications to polysemy.\nTransactions of the Association for Computational\nLinguistics, 6:483–495.\nGiusepppe Attardi. 2015. Wikiextractor. https://\ngithub.com/attardi/wikiextractor.\nFederico Bianchi, Silvia Terragni, and Dirk Hovy.\n2021a. Pre-training is a hot topic: Contextualized\ndocument embeddings improve topic coherence. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers), pages 759–766,\nOnline. Association for Computational Linguistics.\nFederico Bianchi, Silvia Terragni, Dirk Hovy, Debora\nNozza, and Elisabetta Fersini. 2021b. Cross-lingual\ncontextualized topic models with zero-shot learning.\nIn Proceedings of the 16th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics: Main Volume, pages 1676–1683, Online.\nAssociation for Computational Linguistics.\nSid Black, Lee Sharkey, Connor Leahy, Beren Millidge,\nCRG, merizian, Eric Winsor, and Dan Braun. 2022.\nInterpreting neural networks through the polytope\nlens.\nDavid M. Blei. 2012. Probabilistic topic models. Com-\nmun. ACM, 55(4):77–84.\nDavid M. Blei, Andrew Y . Ng, and Michael I. Jordan.\n2003. Latent dirichlet allocation. J. Mach. Learn.\nRes., 3:993–1022.\nGerlof Bouma. 2009. Normalized (pointwise) mutual\ninformation in collocation extraction. Proceedings\nof the Biennial GSCL Conference 2009.\nTrenton Bricken, Adly Templeton, Joshua Batson,\nBrian Chen, Adam Jermyn, Tom Conerly, Nick\nTurner, Cem Anil, Carson Denison, Amanda Askell,\nRobert Lasenby, Yifan Wu, Shauna Kravec, Nicholas\nSchiefer, Tim Maxwell, Nicholas Joseph, Zac\nHatfield-Dodds, Alex Tamkin, Karina Nguyen, Bray-\nden McLean, Josiah E Burke, Tristan Hume, Shan\nCarter, Tom Henighan, and Christopher Olah. 2023.\nTowards monosemanticity: Decomposing language\nmodels with dictionary learning. Transformer Cir-\ncuits Thread.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP,\npages 276–286, Florence, Italy. Association for Com-\nputational Linguistics.\nHoagy Cunningham, Aidan Ewart, Logan Riggs, Robert\nHuben, and Lee Sharkey. 2023. Sparse autoencoders\nfind highly interpretable features in language models.\nGuy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant.\n2023. Analyzing transformers in embedding space.\nAdji B. Dieng, Francisco J. R. Ruiz, and David M. Blei.\n2020. Topic modeling in embedding spaces. Trans-\nactions of the Association for Computational Linguis-\ntics, 8:439–453.\nG. Dijkhuizen and U. Faigle. 1993. A cutting-plane\napproach to the edge-weighted maximal clique prob-\nlem. European Journal of Operational Research ,\n69(1):121–130.\nYihe Dong, Jean-Baptiste Cordonnier, and Andreas\nLoukas. 2021. Attention is not all you need: pure\nattention loses rank doubly exponentially with depth.\nIn Proceedings of the 38th International Conference\non Machine Learning, volume 139 of Proceedings\nof Machine Learning Research , pages 2793–2803.\nPMLR.\nCaitlin Doogan and Wray Buntine. 2021. Topic model\nor topic twaddle? re-evaluating semantic inter-\npretability measures. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3824–3848, Online.\nAssociation for Computational Linguistics.\nNelson Elhage, Tristan Hume, Catherine Olsson,\nNicholas Schiefer, Tom Henighan, Shauna Kravec,\nZac Hatfield-Dodds, Robert Lasenby, Dawn Drain,\nCarol Chen, Roger Grosse, Sam McCandlish, Jared\nKaplan, Dario Amodei, Martin Wattenberg, and\n8655\nChristopher Olah. 2022. Toy models of superpo-\nsition. Transformer Circuits Thread.\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom\nHenighan, Nicholas Joseph, Ben Mann, Amanda\nAskell, Yuntao Bai, Anna Chen, Tom Conerly,\nNova DasSarma, Dawn Drain, Deep Ganguli, Zac\nHatfield-Dodds, Danny Hernandez, Andy Jones,\nJackson Kernion, Liane Lovitt, Kamal Ndousse,\nDario Amodei, Tom Brown, Jack Clark, Jared Ka-\nplan, Sam McCandlish, and Chris Olah. 2021. A\nmathematical framework for transformer circuits.\nTransformer Circuits Thread.\nMor Geva, Avi Caciularu, Kevin Wang, and Yoav Gold-\nberg. 2022. Transformer feed-forward layers build\npredictions by promoting concepts in the vocabulary\nspace. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 30–45, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nLuis Gouveia and Pedro Martins. 2015. Solving the\nmaximum edge-weight clique problem in sparse\ngraphs with compact formulations. EURO Journal\non Computational Optimization, 3(1):1–30.\nDerek Greene and Padraig Cunningham. 2006. Practical\nsolutions to the problem of diagonal dominance in\nkernel document clustering. In ICML, volume 148\nof ACM International Conference Proceeding Series,\npages 377–384.\nMaarten Grootendorst. 2022. Bertopic: Neural topic\nmodeling with a class-based tf-idf procedure. arXiv\npreprint arXiv:2203.05794.\nSungwon Han, Mingi Shin, Sungkyu Park, Changwook\nJung, and Meeyoung Cha. 2023. Unified neural topic\nmodel via contrastive learning and term weighting.\nIn Proceedings of the 17th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics, pages 1802–1817, Dubrovnik, Croatia.\nAssociation for Computational Linguistics.\nYaru Hao, Li Dong, Furu Wei, and Ke Xu. 2021. Self-\nattention attribution: Interpreting information interac-\ntions inside transformer. In Proceedings of the AAAI\nConference on Artificial Intelligence , volume 35,\npages 12963–12971.\nTom Henighan, Shan Carter, Tristan Hume, Nelson\nElhage, Robert Lasenby, Stanislav Fort, Nicholas\nSchiefer, and Christopher Olah. 2023. Superposition,\nmemorization, and double descent. Transformer Cir-\ncuits Thread.\nAlexander Hoyle, Pranav Goel, Denis Peskov, An-\ndrew Hian-Cheong, Jordan Boyd-Graber, and Philip\nResnik. 2021. Is automated topic model evaluation\nbroken?: The incoherence of coherence. In Neural\nInformation Processing Systems.\nRichard M. Karp. 1972. Reducibility among Combina-\ntorial Problems, pages 85–103. Springer US, Boston,\nMA.\nDiederik P. Kingma and Max Welling. 2014. Auto-\nEncoding Variational Bayes. In 2nd International\nConference on Learning Representations, ICLR 2014,\nBanff, AB, Canada, April 14-16, 2014, Conference\nTrack Proceedings.\nThomas N. Kipf and Max Welling. 2017. Semi-\nsupervised classification with graph convolutional\nnetworks. In 5th International Conference on Learn-\ning Representations, ICLR 2017, Toulon, France,\nApril 24-26, 2017, Conference Track Proceedings .\nOpenReview.net.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nJey Han Lau, David Newman, and Timothy Baldwin.\n2014. Machine reading tea leaves: Automatically\nevaluating topic coherence and topic model quality.\nIn Proceedings of the 14th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics, pages 530–539, Gothenburg, Sweden.\nAssociation for Computational Linguistics.\nJia Peng Lim and Hady Lauw. 2022. Towards reinter-\npreting neural topic models via composite activations.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n3688–3703, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nJia Peng Lim and Hady Lauw. 2023. Large-scale corre-\nlation analysis of automated metrics for topic models.\nIn Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers), pages 13874–13898, Toronto, Canada.\nAssociation for Computational Linguistics.\nKar Wai Lim and Wray L. Buntine. 2014. Bibliographic\nanalysis with the citation network topic model. In\nACML, volume 39 of JMLR Workshop and Confer-\nence Proceedings.\nQi Liu, Maximilian Nickel, and Douwe Kiela. 2019.\nHyperbolic graph neural networks. In Advances in\nNeural Information Processing Systems, volume 32.\nCurran Associates, Inc.\nYishu Miao, Lei Yu, and Phil Blunsom. 2016. Neu-\nral variational inference for text processing. In Pro-\nceedings of The 33rd International Conference on\nMachine Learning, volume 48 of Proceedings of Ma-\nchine Learning Research , pages 1727–1736, New\nYork, New York, USA.\nBeren Millidge and Sid Black. 2022. The singular value\ndecompositions of transformer weight matrices are\nhighly interpretable.\n8656\nDavid Mimno, Hanna M. Wallach, Edmund Talley,\nMiriam Leenders, and Andrew McCallum. 2011. Op-\ntimizing semantic coherence in topic models. In\nProceedings of the Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP ’11,\npage 262–272, USA. Association for Computational\nLinguistics.\nChris Olah, Nick Cammarata, Ludwig Schubert, Gabriel\nGoh, Michael Petrov, and Shan Carter. 2020.\nZoom in: An introduction to circuits. Distill.\nHttps://distill.pub/2020/circuits/zoom-in.\nShirui Pan, Jia Wu, Xingquan Zhu, Chengqi Zhang, and\nYang Wang. 2016. Tri-party deep network represen-\ntation. In IJCAI, pages 1895–1901.\nKyungchul Park, Kyungsik Lee, and Sungsoo Park.\n1996. An extended formulation approach to the edge-\nweighted maximal clique problem. European Jour-\nnal of Operational Research, 95(3):671–682.\nDang Van Pham and Tuan M. V . Le. 2021. Neural topic\nmodels for hierarchical topic detection and visualiza-\ntion. In ECML/PKDD.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nMichael Röder, Andreas Both, and Alexander Hinneb-\nurg. 2015. Exploring the space of topic coherence\nmeasures. In WSDM, pages 399–408.\nRylan Schaeffer, Brando Miranda, and Sanmi Koyejo.\n2023. Are emergent abilities of large language mod-\nels a mirage?\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nNoam Shazeer, Zhenzhong Lan, Youlong Cheng, Nan\nDing, and Le Hou. 2020. Talking-heads attention.\nAkash Srivastava and Charles Sutton. 2017. Autoencod-\ning variational inference for topic models. In ICLR\n(Poster).\nJie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang,\nand Zhong Su. 2008. Arnetminer: extraction and\nmining of academic social networks. In KDD, pages\n990–998.\nSilvia Terragni, Elisabetta Fersini, Bruno Giovanni\nGaluzzi, Pietro Tropeano, and Antonio Candelieri.\n2021. OCTIS: Comparing and optimizing topic mod-\nels is simple! In Proceedings of the 16th Confer-\nence of the European Chapter of the Association for\nComputational Linguistics: System Demonstrations,\npages 263–270, Online. Association for Computa-\ntional Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 5797–5808, Florence, Italy.\nAssociation for Computational Linguistics.\nXinyi Wang, Wanrong Zhu, and William Yang Wang.\n2023. Large language models are implicitly topic\nmodels: Explaining and finding good demon-\nstrations for in-context learning. arXiv preprint\narXiv:2301.11916.\nLiang Yang, Fan Wu, Junhua Gu, Chuan Wang, Xi-\naochun Cao, Di Jin, and Yuanfang Guo. 2020. Graph\nattention topic modeling network. In Proceedings\nof The Web Conference 2020 , WWW ’20, page\n144–154, New York, NY , USA. Association for Com-\nputing Machinery.\nCe Zhang and Hady W. Lauw. 2020. Topic modeling on\ndocument networks with adjacent-encoder. Proceed-\nings of the AAAI Conference on Artificial Intelligence,\n34(04):6737–6745.\nDelvin Ce Zhang, Rex Ying, and Hady W. Lauw. 2023.\nHyperbolic graph topic modeling network with con-\ntinuously updated topic tree. In Proceedings of the\n29th ACM SIGKDD Conference on Knowledge Dis-\ncovery and Data Mining, KDD ’23, page 3206–3216,\nNew York, NY , USA. Association for Computing\nMachinery.\nLinhai Zhang, Xuemeng Hu, Boyu Wang, Deyu Zhou,\nQian-Wen Zhang, and Yunbo Cao. 2022. Pre-training\nand fine-tuning neural topic model: A simple yet ef-\nfective approach to incorporating external knowledge.\nIn Proceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 5980–5989, Dublin, Ireland. As-\nsociation for Computational Linguistics.\n8657\nA Supplementary Information\nAlgorithm in Words. We consider each vertex\nand its neighbours as a star. Since Gis a complete\ngraph, each star will be similar to other stars. To in-\ncrease variation among stars, we retain edges with\nwvi,vj ∈[θl,θu], deleting all other edges. Next,\nprioritising stars with larger mean wvi,vj , greedily\nselect stars with unchosen centres and their remain-\ning unchosen vertices with largest wvi,vj fulfilling\nset size constraints [κl,κu]. After reducing Gto\nsmaller sub-graphs of size 24, it is now computa-\ntionally feasible to solve these sub-graphs exactly.\nA.1 Hyper-Parameters for GPT-2\nFor shortlisting of token pools, we set token pool\nsize-parameter τ = 900 for fpath obtained from\nGPT-2. In our heuristic step, we keep edges with\nwvi,vj that lies between θ= [0.1,1]. Word-set size\nconstraints to κ= [10,24]. GPT-2’s tokens can be\nmapped directly onto Wikipedia’s V.\nA.2 Zero-Shot Topic Modelling Details\nCorpus size. Table 8 details the corpora statistics.\nThese corpora may be in the large datasets, such as\nCommonCrawl, used in the pre-training of PLMs.\nHowever, selecting these small corpora would, at\nbest, constitute a tiny percentage of the total tokens\nused to train these PLMs. Furthermore, some of our\nneural topic model baselines will also utilise PLM\nembeddings. When possible, we try to mitigate\nthe influence of rare word pairs, based on token\ncount and corpus size, by setting the minimum\ncount of word pairs for NPMI C calculation for\nM10, DBLP, 20NewsGroup and BBC-News to 0,\n5, 50, 50 respectively.\nCorpus |Docs| |Tokens| |V| |V|∈Wiki (%)\n20NewsGroup 16,309 783,151 1,612 1,582 (98.1)\nBBC-News 2,225 267,259 2,949 2,892 (98.1)\nM10 8,355 49,385 1,696 1,613 (95.1)\nDBLP 54,595 294,757 1,513 1,394 (92.1)\nTable 8: Numerical descriptions of corpora.\nHyper-Parameters. For GPT-2, we set τ =\n900. Table 9 contains the θ hyper-parameter for\nedge-weights. θl have to be reduced below 0 to\nproduce stars sub-graphs. For M10 and DBLP, their\nsmall document size and rare word pairs strongly\ninfluence their NPMIC. To mitigate this, we reduce\nθu. We set a minimum word-pair count that will\nretain a majority of words such that they contain at\nleast a word pair.\nCorpus θedge-weight Sub-graph size\n20NewsGroup [-0.6, 1.0] 16\nBBC-News [-0.6, 1.0] 16\nDBLP [-0.1, 0.3] 32\nM10 [-0.1, 0.3] 16\nTable 9: Hyper-parameters used in heuristic pruning of\nNPMI graph for ZSTM.\nVoting Scheme. We give each token ten votes\nper layer, awarded to the highest activated neuron.\nNeurons that exceed the vote count threshold will\nbe examined, with the neuron acting as the origin.\nThe vote count threshold is set relative to corpus\nsize, at 25% for 20NewsGroup and BBC-News,\nand 1% for M10 and DBLP.\nGreedy Algorithm. From the extracted topics,\nwe choose Kbest topics greedily using heuristics\nprioritising for NPMIC and modulated by ϵ, which\nrepresents the maximum similar words between\neach topic. We search 1 ≤ϵ ≤3 and select the\nmost competitive.\nCorpus K ProdLDA- β CTM-β GPT-2\n20NewsGroup 20 2 1 2\nBBC-News 20 2 3 2\nDBLP 20 2 3 2\nM10 20 3 3 2\n20NewsGroup 50 3 3 2\nBBC-News 50 3 3 2\nTable 10: Hyper-parameter ϵfor greedy-algo.\nA.3 Additional LLaMA Details\nHyper-parameters. To shortlist token pools, we\nset size-parameter τ = 900 for fpath obtained from\nLLaMA. In our heuristic step, we keep edges with\nwvi,vj that lies between θ = [0.025,1]. Word-set\nsize constrained to κpos = [10,32], κneg = [10,24].\nA larger upper-limit reduces number of word-sets\ngenerated.\nWord Explosion. LLaMA’s pool can recon-\nstruct a larger number of words from a smaller\nset of tokens. To prevent this scenario, we automat-\nically reduce τ, 50 at a time, until the word pools\ngenerated have less than 900 words.\n8658\nB Supplementary Tables\nWords/Word Pool Word Sets/Word Pool Words/Word Set\nmodes pos neg pos neg pos neg\nS-Hi-C 388.1 ± 122.1 216.680 ± 87.9 6.90 ± 5.43 2.07 ± 1.89 15.19 ± 4.84 12.82 ± 3.30\nS-Hi-I 480.0 ± 104.9 229.119 ± 87.0 9.86 ± 5.79 2.19 ± 2.22 14.37 ± 4.26 12.65 ± 3.21\nS-Lo-C 326.4 ± 125.2 228.968 ± 86.1 5.92 ± 5.00 1.70 ± 1.11 15.10 ± 4.76 12.49 ± 2.91\nS-Lo-I 437.3 ± 113.1 336.300 ± 114.1 8.11 ± 5.18 3.56 ± 2.79 14.47 ± 4.38 12.74 ± 3.14\nM-Hi-C 357.2 ± 98.4 262.420 ± 94.0 5.45 ± 3.85 2.69 ± 3.01 14.72 ± 4.63 12.74 ± 3.22\nM-Hi-I 510.5 ± 103.2 338.652 ± 93.3 11.03 ± 5.29 3.87 ± 2.87 14.41 ± 4.25 12.93 ± 3.30\nM-Lo-C 346.0 ± 113.5 268.337 ± 101.8 5.61 ± 4.44 2.11 ± 2.01 14.65 ± 4.55 12.27 ± 2.73\nM-Lo-I 454.9 ± 95.9 383.143 ± 99.5 6.79 ± 4.43 4.87 ± 3.41 13.81 ± 3.98 12.96 ± 3.29\nL-Hi-C 409.9 ± 114.6 267.437 ± 82.1 8.42 ± 4.70 3.08 ± 4.30 14.43 ± 4.32 13.76 ± 4.09\nL-Hi-I 473.7 ± 88.8 211.120 ± 110.7 9.27 ± 3.89 4.77 ± 5.31 14.40 ± 4.50 14.10 ± 4.19\nL-Lo-C 408.4 ± 94.7 248.464 ± 75.9 7.79 ± 4.68 2.41 ± 3.31 14.43 ± 4.35 13.14 ± 3.70\nL-Lo-I 449.7 ± 92.2 340.385 ± 96.9 6.66 ± 3.97 3.96 ± 3.18 13.74 ± 3.94 12.89 ± 3.27\nXL-Hi-C 473.0 ± 129.8 250.104 ± 83.9 11.34 ± 5.29 3.08 ± 4.09 14.88 ± 4.55 13.66 ± 4.00\nXL-Hi-I 478.2 ± 96.0 244.775 ± 106.4 10.02 ± 3.73 3.84 ± 5.10 14.60 ± 4.58 14.00 ± 4.17\nXL-Lo-C 466.4 ± 102.6 244.433 ± 79.7 10.02 ± 5.06 2.48 ± 3.64 14.78 ± 4.51 13.44 ± 3.96\nXL-Lo-I 450.4 ± 89.2 351.506 ± 106.3 6.88 ± 4.10 3.99 ± 3.55 13.76 ± 3.94 13.01 ± 3.42\nRandom 559.2 ± 14.2 5.77 ± 1.87 12.04 ± 2.37\nTable 11: Sampling statistics from various paths of GPT-2. All neurons of respective modes produced non-empty\nword pools. We conduct the Mann-Whitney U test between the random baseline and non-random methods on the\nnumber of word sets/neuron, and their differences are significant with p< 0.001.\nMean num. Top-1 Topic (Topical Pool) Mean NPMI W score Total num. of % of\nTopics NPMI W score per Topical Pool topics Abstract Pool\nmodes pos neg pos neg pos neg pos neg pos neg both\nS-Hi-C 6.76 2.08 0.202 ± 0.06 0.216 ± 0.09 0.130 0.164 211,612 22,106 7.5 35.6 6.0\nS-Hi-I 9.76 2.19 0.218 ± 0.06 0.166 ± 0.08 0.119 0.132 351,968 24,634 1.1 34.8 0.6\nS-Lo-C 5.68 1.70 0.215 ± 0.07 0.199 ± 0.09 0.142 0.162 40,865 4,725 11.0 34.9 7.4\nS-Lo-I 8.10 3.56 0.227 ± 0.07 0.191 ± 0.08 0.122 0.117 70,502 21,913 2.8 16.6 1.2\nM-Hi-C 4.89 2.70 0.192 ± 0.06 0.184 ± 0.08 0.128 0.133 405,662 88,914 7.8 33.2 4.2\nM-Hi-I 10.84 3.80 0.232 ± 0.06 0.162 ± 0.06 0.119 0.108 1,048,594 270,000 0.8 13.8 0.4\nM-Lo-C 5.45 2.12 0.209 ± 0.07 0.169 ± 0.08 0.134 0.122 108,242 16,967 9.6 33.7 6.0\nM-Lo-I 6.80 4.87 0.216 ± 0.07 0.190 ± 0.07 0.114 0.111 158,952 97,378 2.4 9.3 0.3\nL-Hi-C 8.26 3.04 0.221 ± 0.06 0.165 ± 0.08 0.130 0.126 1405,522 214,319 3.9 30.9 1.3\nL-Hi-I 8.37 4.69 0.188 ± 0.05 0.180 ± 0.06 0.111 0.128 1,469,191 241,072 2.4 36.1 0.2\nL-Lo-C 7.76 2.41 0.226 ± 0.06 0.164 ± 0.08 0.136 0.126 338,917 39,909 2.6 32.1 1.7\nL-Lo-I 6.66 3.96 0.208 ± 0.07 0.186 ± 0.07 0.111 0.114 291,240 133,769 2.6 13.4 0.4\nXL-Hi-C 10.88 2.98 0.232 ± 0.07 0.160 ± 0.07 0.127 0.126 3,135,250 291,738 3.1 34.0 0.6\nXL-Hi-I 9.12 3.78 0.199 ± 0.05 0.160 ± 0.06 0.117 0.118 2,665,849 535,733 2.4 26.9 0.0\nXL-Lo-C 9.62 2.47 0.227 ± 0.07 0.155 ± 0.07 0.128 0.126 712,337 59,817 1.8 34.2 0.9\nXL-Lo-I 6.88 4.00 0.212 ± 0.06 0.182 ± 0.07 0.111 0.113 508,179 217,321 1.9 14.6 0.3\nRandom 5.77 0.097 ± 0.03 0.029 261,949 0\nTable 12: Exploratory results from mining topics in GPT-2 with \"stop topics\" removed. We conduct the Mann-\nWhitney U test between random baseline and non-random methods on top-1 NPMIW scores. Differences between\nGPT-2-sampled and randomly-sampled for both metrics are significant with p <0.001. Abstract neurons are\nneurons of respective paths that have no projected topics.\n8659\nModel NPMI C NPMIW TU A(%)\nProdLDA -0.135 0.021 0.750 -\nProdLDA-β 0.018 0.061 0.796 -\nCTM -0.100 0.044 0.825 -\nCTM-β 0.048 0.075 0.809 -\nBERTopic -0.045 0.064 0.799 -\nGPT-2-S-Lo-C 0.134 0.074 0.870 20.57\nGPT-2-S-Lo-I 0.151 0.066 0.866 20.57\nGPT-2-M-Lo-C 0.156 0.075 0.864 23.73\nGPT-2-M-Lo-I 0.155 0.082 0.858 23.73\nGPT-2-L-Lo-C 0.164 0.077 0.862 14.99\nGPT-2-L-Lo-I 0.155 0.079 0.864 14.99\nGPT-2-XL-Lo-C0.171 0.076 0.860 15.37\nGPT-2-XL-Lo-I 0.171 0.090 0.866 15.37\n(a) 20NewsGroup, K = 50\nModel NPMI C NPMIW TU A(%)\nProdLDA -0.131 0.026 0.667 -\nProdLDA-β -0.108 0.044 0.768 -\nCTM -0.118 0.048 0.701 -\nCTM-β -0.088 0.056 0.767 -\nBERTopic - - - -\nGPT-2-S-Lo-C 0.100 0.056 0.852 39.56\nGPT-2-S-Lo-I 0.111 0.063 0.838 39.56\nGPT-2-M-Lo-C 0.111 0.051 0.856 44.71\nGPT-2-M-Lo-I 0.113 0.057 0.870 44.71\nGPT-2-L-Lo-C 0.118 0.062 0.840 28.96\nGPT-2-L-Lo-I 0.115 0.056 0.848 28.96\nGPT-2-XL-Lo-C0.123 0.058 0.858 31.74\nGPT-2-XL-Lo-I 0.122 0.071 0.836 31.74\n(b) BBC-News, K = 50. BERTopic unable to produce 50\ntopics for all runs.\nTable 13: Results for topic set sizes K = 50. The best baseline scores for the metric are underlined. Bolded PLM\nscores are better than the best baseline. Baseline scores are the mean of 10 independent runs with error bars in\nTable 15 in Appendix B. The minimum word co-occurrence count is 50 for NPMIC calculation.\nModel NPMI C NPMIW TU A(%)\nProdLDA -0.059 0.045 0.903 -\nProdLDA-β 0.085 0.075 0.911 -\nCTM -0.060 0.052 0.916 -\nCTM-β 0.016 0.070 0.945 -\nBERTopic -0.002 0.069 0.887 -\nGPT-2-S-Lo-C 0.164 0.123 0.955 20.57\nGPT-2-S-Lo-I 0.169 0.115 0.950 20.57\nGPT-2-M-Lo-C 0.177 0.117 0.965 23.73\nGPT-2-M-Lo-I 0.186 0.120 0.920 23.73\nGPT-2-L-Lo-C 0.197 0.115 0.945 14.99\nGPT-2-L-Lo-I 0.170 0.118 0.965 14.99\nGPT-2-XL-Lo-C0.184 0.112 0.940 15.37\nGPT-2-XL-Lo-I 0.179 0.140 0.955 15.37\n(a) 20NewsGroup, K = 20. Minimum word co-occurence\ncount is 50 for NPMIC calculation.\nModel NPMI C NPMIW TU A(%)\nProdLDA -0.085 0.027 0.850 -\nProdLDA-β -0.069 0.059 0.888 -\nCTM -0.052 0.053 0.828 -\nCTM-β 0.009 0.072 0.818 -\nBERTopic -0.036 0.077 0.862 -\nGPT-2-S-Lo-C 0.139 0.119 0.920 39.56\nGPT-2-S-Lo-I 0.135 0.108 0.900 39.56\nGPT-2-M-Lo-C 0.147 0.106 0.915 44.71\nGPT-2-M-Lo-I 0.141 0.123 0.920 44.71\nGPT-2-L-Lo-C 0.139 0.116 0.920 28.96\nGPT-2-L-Lo-I 0.147 0.114 0.920 28.96\nGPT-2-XL-Lo-C0.148 0.112 0.900 31.74\nGPT-2-XL-Lo-I 0.143 0.122 0.920 31.74\n(b) BBC-News, K = 20. Minimum word co-occurence count\nis 50 for NPMIC calculation.\nModel NPMI C NPMIW TU A(%)\nProdLDA -0.041 0.024 0.897 -\nProdLDA-β -0.061 0.025 0.886 -\nCTM -0.035 0.034 0.863 -\nCTM-β -0.001 0.050 0.812 -\nBERTopic -0.212 -0.024 0.791 -\nGPT-2-S-Lo-C 0.082 0.028 0.7 . 28.63\nGPT-2-S-Lo-I 0.075 0.038 0.780 28.63\nGPT-2-M-Lo-C 0.087 0.035 0.800 43.12\nGPT-2-M-Lo-I 0.081 0.040 0.800 43.12\nGPT-2-L-Lo-C 0.091 0.042 0.800 34.18\nGPT-2-L-Lo-I 0.085 0.039 0.845 34.18\nGPT-2-XL-Lo-C0.103 0.033 0.850 38.98\nGPT-2-XL-Lo-I 0.088 0.057 0.845 38.98\n(c) DBLP, K = 20. Minimum word co-occurence count is 5\nfor NPMIC calculation.\nModel NPMI C NPMIW TU A(%)\nProdLDA -0.028 0.007 0.741 -\nProdLDA-β -0.023 0.014 0.830 -\nCTM 0.027 0.050 0.895 -\nCTM-β 0.067 0.057 0.835 -\nBERTopic -0.049 0.023 0.906 -\nGPT-2-S-Lo-C 0.135 0.021 0.850 32.31\nGPT-2-S-Lo-I 0.143 0.043 0.885 32.31\nGPT-2-M-Lo-C 0.156 0.021 0.865 46.52\nGPT-2-M-Lo-I 0.148 0.042 0.865 46.52\nGPT-2-L-Lo-C 0.164 0.028 0.855 38.20\nGPT-2-L-Lo-I 0.143 0.033 0.855 38.20\nGPT-2-XL-Lo-C0.180 0.023 0.845 43.39\nGPT-2-XL-Lo-I 0.159 0.045 0.850 43.39\n(d) M10, K = 20. Minimum word co-occurence count is 0\nfor NPMIC calculation due to low token and document count.\nTable 14: Results for topic set sizes K = 20. The best baseline scores for the metric are underlined. Bolded PLM\nscores are better than the best baseline. The mean of 10 independent runs with error bars in Table 15 in Appendix B.\n8660\nNPMIC NPMIW TU\nProdLDA -0.059 ± 0.014 0.045 ± 0.007 0.903 ± 0.021\nProdLDA-β 0.085 ± 0.0210.075 ± 0.0070.911 ± 0.010\nCTM -0.060 ± 0.024 0.052 ± 0.008 0.916 ± 0.012\nCTM-β 0.016 ± 0.019 0.070 ± 0.007 0.945 ± 0.007\nBERTopic -0.002 ± 0.023 0.069 ± 0.007 0.887 ± 0.012\n(a) 20NewsGroup, K = 20\nNPMIC NPMIW TU\nProdLDA -0.085 ± 0.025 0.027 ± 0.010 0.850 ± 0.035\nProdLDA-β -0.069 ± 0.018 0.059 ± 0.011 0.888 ± 0.016\nCTM -0.052 ± 0.014 0.053 ± 0.013 0.828 ± 0.041\nCTM-β 0.009 ± 0.0190.072 ± 0.006 0.818 ± 0.015\nBERTopic -0.036 ± 0.012 0.077 ± 0.0070.862 ± 0.008\n(b) BBC-News, K = 20\nNPMIC NPMIW TU\nProdLDA -0.041 ± 0.015 0.024 ± 0.004 0.897 ± 0.013\nProdLDA-β -0.061 ± 0.018 0.025 ± 0.008 0.886 ± 0.009\nCTM -0.035 ± 0.013 0.034 ± 0.009 0.863 ± 0.019\nCTM-β -0.001 ± 0.0110.050 ± 0.0070.812 ± 0.020\nBERTopic -0.212 ± 0.016 -0.024 ± 0.004 0.791 ± 0.018\n(c) DBLP, K = 20\nNPMIC NPMIW TU\nProdLDA -0.028 ± 0.024 0.007 ± 0.010 0.741 ± 0.027\nProdLDA-β -0.023 ± 0.020 0.014 ± 0.011 0.830 ± 0.021\nCTM 0.027 ± 0.013 0.050 ± 0.009 0.895 ± 0.026\nCTM-β 0.067 ± 0.0280.057 ± 0.0090.835 ± 0.015\nBERTopic -0.049 ± 0.018 0.023 ± 0.012 0.906 ± 0.011\n(d) M10, K = 20\nNPMIC NPMIW TU\nProdLDA -0.135 ± 0.012 0.021 ± 0.005 0.750 ± 0.024\nProdLDA-β 0.018 ± 0.012 0.061 ± 0.003 0.796 ± 0.010\nCTM -0.100 ± 0.014 0.044 ± 0.006 0.825 ± 0.022\nCTM-β 0.048 ± 0.0060.075 ± 0.0040.809 ± 0.007\nBERTopic -0.045 ± 0.008 0.064 ± 0.002 0.799 ± 0.012\n(e) 20NewsGroup, K = 50.\nNPMIC NPMIW TU\nProdLDA -0.131 ± 0.016 0.026 ± 0.006 0.667 ± 0.033\nProdLDA-β -0.108 ± 0.016 0.044 ± 0.011 0.768 ± 0.009\nCTM -0.118 ± 0.025 0.048 ± 0.006 0.701 ± 0.051\nCTM-β -0.088 ± 0.0100.056 ± 0.0050.767 ± 0.008\n(f) BBC-News, K = 50. BERTopic could not produce enough\ntopics for all 10 independent run and is excluded.\nTable 15: Baseline results with error bars. Underline\nscores are best within metric.\nNeurons/layer Layers Total Neurons\nGPT-2\nS-Hi-C 3,072 12 36,864\nS-Hi-I 3,072 12 36,864\nS-Lo-C 768 12 9,216\nS-Lo-I 768 12 9,216\nM-Hi-C 4,096 24 98,304\nM-Hi-I 4,096 24 98,304\nM-Lo-C 1,024 24 24,576\nM-Lo-I 1,024 24 24,576\nL-Hi-C 5,120 36 184,320\nL-Hi-I 5,120 36 184,320\nL-Lo-C 1,280 36 46,080\nL-Lo-I 1,280 36 46,080\nXL-Hi-C 6,400 48 307,200\nXL-Hi-I 6,400 48 307,200\nXL-Lo-C 1,600 48 76,800\nXL-Lo-I 1,600 48 76,800\nLLaMA\n7B-Hi-C 11,008 32 352,256\n7B-Hi-I 11,008 32 352,256\n7B-Lo-C 4,096 32 131,072\n7B-Lo-I 4,096 32 131,072\n13B-Hi-C 13,824 40 552,960\n13B-Hi-I 13,824 40 552,960\n13B-Lo-C 5,120 40 204,800\n13B-Lo-I 5,120 40 204,800\nTable 16: Model parameters statistics. For Paths Hi-C\nand Hi-I, the neurons refer to those in the transformer\nblock’s MLP, usually containing more neurons than\nPaths Lo-C and Lo-I, located in the residual stream.\n8661\nC Examples\nWords pools are alphabetically-sorted.\nWord pool Word sets Extracted Topics (NPMI W) #\narchbishop bishop canterbury\ncathedral conversion copenhagen\ndanish doctrine earthquake\nhelsinki leicester lutheran\nmercy norse norwegian norwich\northodox oslo pilgrim pope pre-\nmiership repentance resurrection\nsacrament salvation scandinavian\nsins souls stockholm sweden\nswedish viking worcester\n(332 ommitted words)\nlutheran oslo danish stockholm viking norse\nnorwegian sweden swedish helsinki copen-\nhagen scandinavian\ncopenhagen swedish oslo norwegian\nhelsinki stockholm viking danish scan-\ndinavian sweden (0.237)\n1\nsalvation souls doctrine orthodox resurrection\nmercy sins conversion sacrament repentance\nsalvation souls doctrine repentance sins\nmercy conversion sacrament resurrec-\ntion orthodox (0.179)\n2\narchbishop pope worcester premiership earth-\nquake leicester norwich cathedral pilgrim\nbishop canterbury\narchbishop bishop pope earthquake\npilgrim canterbury norwich worcester\ncathedral leicester (0.098)\n3\nTable 17: GPT-2-XL, Path Hi-C, Layer 26 Neuron 5201,neg pool. Vikings (#1) and Religion (#2, 3).\nWord pool Word sets Extracted Topics (NPMI W) #\nanswer array bat beat bet bet-\nter boot certainly client come\ncommand control correctly crowd\ndaily day decided don drive ex-\nactly february feel feeling flip\nfolks friday fun function func-\ntions getting going good got guys\ninstall installed know let like little\nmodule modules monday month\nmorning need night ought pass\npeople pretty saturday smart sort\nsunday tell throw throws thurs-\nday tie tonight toss tossed track-\ning tuesday understand wednes-\nday\n(358 ommitted words)\nwednesday saturday tuesday monday month\nmorning february good daily day night thurs-\nday sunday tonight friday\nsunday morning monday wednesday\nnight friday saturday thursday day tues-\nday (0.331)\n1\ndon let guys exactly going tell feel ought un-\nderstand need answer getting folks better come\npretty like certainly people sort feeling fun lit-\ntle know\nlet pretty tell going like fun don feel\nknow guys (0.179)\n2\nmodules boot array client command install in-\nstalled tracking control smart function func-\ntions module\nmodule modules client smart control\nfunctions install tracking installed array\n(0.114)\n3\ncorrectly crowd drive got decided pass flip bat\nthrow throws tie beat tossed bet toss\nbat throws drive tossed got crowd pass\ntie toss throw (0.109)\n4\nTable 18: GPT-2-XL, Path Hi-C, Layer 26 Neuron 5201, pos pool. Stop topic (#1), relationship-related (#2),\nsoftware (#3), and baseball/cricket (#4).\nWord pool Word sets Extracted Topics (NPMI W) #\naluminum chemist foods mag-\nnesium mineral oxide phosphate\nphosphorus salts soda thermal ura-\nnium vapor (251 ommitted words)\nuranium magnesium chemist oxide thermal\nphosphate phosphorus vapor foods mineral alu-\nminum soda salts\naluminum mineral magnesium soda\nphosphate phosphorus uranium salts ox-\nide thermal (0.183)\n1\nTable 19: GPT-2-XL, Path Lo-C, Layer 2 Neuron 613,neg pool. Some pools only have one topic. Chemistry (#1).\n8662\nWord pool Word sets Extracted Topics (NPMI W) #\naccess adam advertising alex andy\nanswer app apr ass author ben\nblog book chapter check com con-\ntent contents dear dec don down-\nload editor email facebook fea-\ntures file google got help hey hit\nhttp https install jan jason jon\njosh kill let like love mar me-\ndia michael net news notes nov\nonline page phil photo picture\npost posted preview print pub-\nlished quote read reddit rick said\nsam sep server source story sub-\nscribe summary ted text thank\ntom trump twitter update updated\nuser version video watch web\nwow written www yes\n(213 ommitted words)\ndec com web nov jan www sep mar https http\napr\nwww apr jan dec sep https nov http mar\ncom (0.317)\n1\npage twitter google posted reddit app media\npost email blog user photo online update con-\ntent video news advertising watch download\npreview updated trump facebook\ntwitter page content reddit online face-\nbook posted user app google (0.246)\n2\ntom ben phil andy adam michael alex ted jason\nrick help kill jon josh sam\nphil michael sam andy kill alex josh ben\njason tom (0.199)\n3\nchapter read story contents quote notes written\nauthor picture published text summary editor\nbook\ncontents chapter read written published\nauthor story summary text book (0.196)\n4\nhit let wow got answer love like yes ass said\nhey dear thank don\nlike let dear got thank answer said don\nlove yes (0.131)\n5\naccess install check source net features sub-\nscribe print version file server\nnet access source version check install\nfile features server print (0.070)\n6\nTable 20: GPT-2-XL, Path Lo-C, Layer 2 Neuron 613,pos pool. Stop topic (#1), due to \"months\", social media (#2),\nconversational (#3), names (#4), story (#5), and software (#6).\nWord pool Word sets Extracted Topics (NPMI W) #\nabsolutely access allowing amaz-\ning ancient app available awe-\nsome basic beat beautiful best\nbetter bit blood casting certainly\ncharacter common convenient\ncrazy critical definitely difficult\ndigital direct don dream eas-\nily easy effective experience ex-\ntreme extremely feel feeling fi-\nnal free friends fun going good\ngot happy heart highly hit hope-\nfully hot incredible information\nintriguing jump kind know like\nlittle look lost lot maybe memory\nnegative number pain perfect per-\nformance pop positive precious\npretty quick rare reach resource\nresources rich role sad safe said\nsecond single sort spending stuff\nstunning success super sure sweet\nteam thing things think thinking\nthought times totally touch trea-\nsure truly useful usually valuable\nversion virtual won wonderful\nwriting yes\n(423 ommitted words)\ndon maybe know lot definitely said pretty bit\nfeel going hopefully kind absolutely sort sure\nwonderful fun good awesome certainly yes got\nlike think\nlot bit definitely think hopefully maybe\nfun don feel know (0.218)\n1\nhit pop crazy spending extremely easy stuff\nversion single number digital sweet reach hot\neasy pop sweet stuff hot single version\ncrazy hit number (0.122)\n2\ncharacter truly look dream incredible stunning\nthought perfect happy casting totally friends\nsad amazing\nperfect truly totally sad character amaz-\ning happy thought look incredible\n(0.105)\n3\nteam lost jump second easily final beat times\nsuper best won\nbeat jump won super best lost times team\nsecond final (0.096)\n4\navailable basic allowing information memory\nvirtual quick better difficult free convenient\ndirect safe app access\napp access basic available allowing\nfree direct information memory virtual\n(0.093)\n5\nlittle blood resource resources valuable thing\nthings rich beautiful ancient rare treasure\nprecious\nancient valuable treasure little rare pre-\ncious rich beautiful thing things (0.090)\n6\nhighly positive writing negative thinking suc-\ncess performance role intriguing critical\nwriting performance positive negative\nintriguing critical role highly success\nthinking (0.081)\n7\nexperience extreme feeling effective useful usu-\nally common touch heart pain\nheart experience useful effective com-\nmon touch extreme pain feeling usually\n(0.053)\n8\nTable 21: GPT-2-XL, Path Hi-I, Layer 26 Neuron 5201,pos pool. Conversational (#1), pop music (#2), story-related\n(#3), sports (#4), software-related (#5), exploration (#6), art-related (#7), and medical-related (#8).\nWord pool Word sets Extracted Topics (NPMI W) #\nair alert black broad carbon cy-\ncle dam dark deck deep electric\nelectrical energy gas global heat\nheight high higher horizontal iron\nlight long low narrow negative\noxygen painted power range red\nreduced steel steep structural sup-\nply tape thick tight water white\nwide zero\n(322 ommitted words)\ngas low dam electric electrical energy supply\nheat water power\nsupply electric electrical energy heat\ndam water gas low power (0.161)\n1\nhigh long broad steep narrow deck thick tight\nheight range wide\nnarrow range high thick deck steep long\nwide broad height (0.146)\n2\nhigher reduced global oxygen cycle steel zero\nair negative structural iron carbon\noxygen higher structural iron carbon\nsteel negative cycle reduced zero (0.091)\n3\ntape white horizontal painted dark deep alert\nlight black red\nblack white red light tape alert horizon-\ntal deep dark painted (0.080)\n4\nTable 22: GPT-2-XL, Path Lo-I, Layer 38 Neuron 556, pos pool. Power generation (#1), Visuals (#2, 4), and\nchemistry (#3).\n8663\nWord pool Word sets Extracted Topics (NPMI W) #\nangel birds breeding canary cats\nchampagne coastline crop dams\ndeities demon demons descend\ndevils dogs dragons drought eggs\nfarmers flora glaciers grapes\ngrowers guide horses hunt lake\nlakes land landscapes lion lions\nmaize mill peaks plateau pond\nponds reptiles river rivers roses\nsouth southern spirits strawber-\nries tails tasmania trout valleys\nvampires waters watershed wind\nwolves\n(310 ommitted words)\nwaters watershed tasmania trout pond ponds\ndrought lake valleys lakes river rivers mill\ndams\nlakes river dams pond ponds rivers wa-\nters watershed lake trout (0.321)\n1\nsouth southern wind coastline land flora land-\nscapes peaks plateau glaciers\nsouthern flora coastline wind landscapes\nplateau land peaks south glaciers (0.109)\n2\ncrop breeding farmers maize grapes eggs grow-\ners roses champagne strawberries\nbreeding eggs strawberries champagne\nroses maize grapes crop farmers growers\n(0.065)\n3\nspirits dragons hunt vampires deities demon\ndevils angel cats demons\ndevils cats hunt spirits dragons vampires\ndeities angel demon demons (0.044)\n4\ntails horses wolves reptiles guide canary de-\nscend birds lion lions dogs\ntails guide lion lions horses reptiles birds\ndogs descend wolves (0.021)\n5\nTable 23: GPT-2-XL, Path Lo-I, Layer 13 Neuron 1126,neg pool. Nature (#1, 2), agriculture (#3), magical beasts\n(#4), and animals (#5).\nWord pool Word sets Extracted Topics (NPMI W) #\nacceleration actors alive allow al-\nlows angry arrow atoms break\nbreath broke calculations camera\ncase com combine computation\nconnection connections contin-\nued convenient cop crash crashed\ncrowd deliver denotes depen-\ndence depth detail details distance\nediting entropy estimation explain\nexplained explanation facing fall\nfast figure flash formula function\nfunctions glass ground half heavy\nhooked http https hundreds icons\nidentify including inserting large\nlead leads leave libraries like like-\nwise manipulate masses mechan-\nical media met miles minus net\nobject objected objective official\npad pages particular passes pass-\ning pause periods play played pos-\nsible pressed privileges programs\npromises provides recursive sav-\ning scripted showed shown shut\nskip slight starting stating step\ntalk temporary terrain thirty today\ntypical understanding upload user\nusers victim videos violent want-\ning watch words www yards zero\n(111 ommitted words)\ncom connection connections detail details http\nlibraries media net official pages today upload\nuser users videos watch www https\npages com www user users http watch\nhttps upload videos (0.230)\n1\nallow allows case computation depth es-\ntimation formula function functions leads\nlike object particular possible programs pro-\nvides showed shown starting step words zero\nrecursive\nrecursive formula estimation computa-\ntion object allows function step func-\ntions zero (0.146)\n2\nbroke crashed crowd distance facing ground\nhalf hooked hundreds including lead miles mi-\nnus passes passing play played shut thirty yards\ndistance ground lead crowd broke miles\nhalf passes passing yards (0.087)\n3\narrow atoms calculations denotes dependence\nexplain explained explanation glass iden-\ntify large mechanical typical understanding\nentropy\nentropy mechanical typical atoms iden-\ntify calculations explain explained expla-\nnation understanding (0.074)\n4\nbreak breath continued fast leave objective peri-\nods pressed skip slight stating temporary pause\nperiods temporary pause pressed slight\nstating fast leave break continued\n(0.022)\n5\nalive cop met objected promises talk victim\nviolent wanting angry\nmet talk victim violent promises wanting\nalive objected cop angry (0.010)\n6\ncrash deliver fall figure flash heavy likewise\nmasses pad terrain acceleration\nterrain heavy deliver crash likewise fig-\nure flash masses fall acceleration (-\n0.033)\n7\nactors camera combine convenient icons in-\nserting manipulate privileges saving scripted\nediting\nconvenient actors camera scripted insert-\ning combine manipulate saving icons\nediting (-0.141)\n8\nTable 24: LLaMA-13B, Path Hi-C, Layer 3 Neuron 224, pos pool. Websites (#1), mathematics (#2, 3), movement\n(#5), police-interaction (#6), physics-related (#7) and acting-related (#8).\n8664\nWord pool Word sets Extracted Topics (NPMI W ) #\naccess action amity application av-\nery background bail bed belly block\nbone break brig bsc capped cause\ncbi charge chase chemist citi class\ncode color con connected connec-\ntion conning context control custom\ndigital distinct diving dsc dust edit-\ning education end eng eps extension\nfashion fbi fins force forensic four-\nteenth frames fuse geary gen gen-\neral genes geology gold hand hand-\nful haryana havana hex high inning\nips issued jail james jane kane kits\nlane lens lets level library lieutenant\nlife like line lobe lobed location lone\nlps macy main management mary\nmaster mastered mastering middle\nmidline mining model mold molded\nmolding mounted msc music neu-\nral occupied offense office offline\nouting parking physicist pits play\npower practice pre primary probe\nprogram protection psychology pub-\nlicity quality rare realistic recording\nrefit resin ret retail retailers room ro-\ntation runs scam school schooling\nsciences scratch scratched scratches\nscreen security sent server service\nshape size small social space special\nsport sporting start starts store string\nstructure student students style sub-\nsequently suffered support surprise\nsystem target targeted targeting test\ntls toes tooth toothed tops training\nversus video videos visited walk\nwall web wine witness witnesses\nwounded\n(486 ommitted words)\nbed chemist class dsc education eng\ngeology level life management mas-\nter mining msc physicist program\npsychology school schooling sci-\nences social sport student students\nsubsequently training bsc\nsciences program education bsc psy-\nchology student students msc school\nmaster (0.176)\n1\nchase fashion location occupied\nparking retail retailers space sport-\ning store target macy\nretail retailers location space occu-\npied macy fashion parking store tar-\nget (0.129)\n2\ncolor diving fins like lobe middle\nshape size small structure style toes\ntoothed lobed\nlike toothed size shape lobe color\nlobed fins middle small (0.120)\n3\naccess application extension library\nsecurity server support system web\ntls\nlibrary access extension support ap-\nplication tls security system web\nserver (0.120)\n4\naction background bail charge con-\nnection fbi forensic haryana high ips\nissued jail jane office probe scam\nsent special witness witnesses cbi\nscam bail charge jail special connec-\ntion probe fbi sent cbi (0.108)\n5\navery break capped citi end four-\nteenth frames lone offense outing\nplay recording rotation start starts\nversus walk inning\nplay inning start starts lone walk\nrecording break end offense (0.063)\n6\nblock custom gold kits lens model\nmold molding resin molded\nmolded lens molding gold kits block\ncustom model resin mold (0.062)\n7\ncontext handful lps music power\nrare string video videos eps\nmusic lps string rare context eps\nhandful video videos power (0.034)\n8\ncause digital dust hand pits protec-\ntion quality scratch scratched screen\nsuffered scratches\nquality hand scratch scratches dust\ncause screen digital protection suf-\nfered (0.033)\n9\nbelly bone connected distinct fuse\nneural runs tooth wall midline\nfuse bone tooth belly midline neural\nruns connected wall distinct (0.026)\n10\ncon control main mounted prac-\ntice primary refit room service tops\nwounded conning\nconning mounted wounded tops con-\ntrol room refit primary main service\n(0.016)\n11\ncode force gen general genes lieu-\ntenant line pre test ret\npre ret lieutenant test code force gen\ngenes line general (-0.006)\n12\namity geary havana james kane lane\nmary surprise visited wine brig\nvisited james mary brig surprise\nkane lane wine amity havana (-\n0.098)\n13\nhex lets mastered mastering offline\npublicity realistic targeted targeting\nediting\ntargeted targeting editing lets hex\npublicity realistic offline mastered\nmastering (-0.197)\n14\nTable 25: LLaMA-13B, Path Lo-C, Layer 36 Neuron 592, pos pool. Education (#1), retail (#2), body-parts-related\n(#3, 6, 10), software (#4), law & order (#5), cricket (#6), sculpting (#7), music videos (#8), surface-damage (#9),\nand mixed (#11, 12, 13, 14).\n8665\nWord pool Word sets Extracted Topics (NPMI W ) #\nannotations audi authority avoided balls\nbeat book books catch city comments\nconcurrent devices displays dropped\ndual eventually fill filled finish finished\nfog functions gates gene generates goals\ngold google hid institutions internally\njurisdiction launched majority micro\nmouse multiple municipal municipali-\nties orange outer overall overlap patch\nplants platforms population power pre-\nsented processor purposes refresh robot\nrust sant sap second slope smart span\nspecies starting surprising systems thick\ntool transparent trap usage uses variance\nvirtual walls works\n(91 ommitted words)\nconcurrent devices displays dual gates\ngenerates google internally launched\nmicro mouse platforms power refresh\nrobot sap smart systems usage virtual\nprocessor\ndevices systems usage robot pro-\ncessor displays virtual smart\ngoogle platforms (0.114)\n1\naudi avoided balls beat catch dropped\neventually finish goals gold overall sec-\nond starting surprising finished\nbeat goals overall finish finished\nballs dropped starting second\ncatch (0.083)\n2\nauthority city functions institutions ju-\nrisdiction majority municipal population\npurposes sant municipalities\njurisdiction purposes city insti-\ntutions functions municipal mu-\nnicipalities authority majority\npopulation (0.081)\n3\nfill filled fog hid orange outer over-\nlap patch plants rust slope span species\ntransparent trap walls thick\nfill filled plants species walls\ntransparent thick outer patch or-\nange (0.072)\n4\nbook books comments gene multiple\npresented tool uses variance works\nannotations\ntool books annotations uses\nworks comments presented gene\nmultiple book (0.031)\n54\nTable 26: LLaMA-13B, Path Lo-I, Layer 30 Neuron 704, pos pool. Internet-of-things (#1), competition (#2),\ngovernment (#3), nature (#4), and literature (# 5).\nWord pool Word sets Extracted Topics (NPMI W ) #\nactive anonymous aspect catalog com-\nment conservation description dual\nfunds header landscape launch listing lit-\nerally marker mole museum names pet\nphotos practical profile profiles recom-\nmend recommended reliable repository\nsitu society static suffix syntax technical\nterm type verb website websites word\n(90 ommitted words)\nactive aspect dual literally marker static\nsuffix syntax term type word verb\nsyntax suffix aspect term type\nmarker verb literally word dual\n(0.099)\n1\nanonymous catalog comment header\nlaunch listing names photos profile pro-\nfiles recommend reliable repository tech-\nnical website websites\nprofile profiles photos anony-\nmous website websites technical\ncomment names launch (0.076)\n2\ndescription funds landscape mole mu-\nseum pet practical recommended situ so-\nciety conservation\nfunds society description pet\nlandscape conservation situ\npractical recommended mu-\nseum (-0.039)\n3\nTable 27: LLaMA-7B, Path Lo-C, Layer 30 Neuron 460, pos pool. Linguistics (#1) and archival (#2, 3).\nWord pool Word sets Extracted Topics (NPMI W ) #\nacid announced basically better black\nblackish blog cease clean comment com-\npact controls cut embedded fins flows\nforth fraction fresh furnace hours jet\nkitchen laid load love model modified\npaired pipe portion posted press product\npublication random randomly reuse said\nselect short shortly small smart subset\nsum tail tall think tweet waste\n(102 ommitted words)\nblack blackish embedded forth fresh jet\nmodified paired portion short small tail\ntall fins\nblack paired tall portion short\nblackish modified fins tail small\n(0.089)\n1\nannounced blog comment hours love\nposted press publication shortly think\ntweet\npublication tweet comment an-\nnounced press love blog think\nposted shortly (0.069)\n2\nacid compact controls fraction model\nproduct random randomly said select\nsum subset\nproduct fraction said model se-\nlect compact random sum subset\nrandomly (0.046)\n3\nbasically better cease clean cut flows fur-\nnace kitchen laid load pipe reuse smart\nwaste\nload kitchen waste better pipe\nclean cut laid furnace flows\n(0.025)\n4\nTable 28: LLaMA-7B, Path Lo-C, Layer 2 Neuron 460, pos pool. Fish-related (#1), social-media (#2), computation\n(#3), and plumbing (#4).\n8666"
}