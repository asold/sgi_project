{
  "title": "Using Pre-Trained Language Models for Producing Counter Narratives Against Hate Speech: a Comparative Study",
  "url": "https://openalex.org/W4285240074",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5049200542",
      "name": "Serra Sinem Tekiroğlu",
      "affiliations": [
        "Fondazione Bruno Kessler"
      ]
    },
    {
      "id": "https://openalex.org/A5086270451",
      "name": "Helena Bonaldi",
      "affiliations": [
        "Fondazione Bruno Kessler",
        "University of Trento"
      ]
    },
    {
      "id": "https://openalex.org/A5078344876",
      "name": "Margherita Fanton",
      "affiliations": [
        "Fondazione Bruno Kessler",
        "University of Trento"
      ]
    },
    {
      "id": "https://openalex.org/A5072659160",
      "name": "Marco Guerini",
      "affiliations": [
        "Fondazione Bruno Kessler"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3045555759",
    "https://openalex.org/W2563826943",
    "https://openalex.org/W2970871182",
    "https://openalex.org/W49514851",
    "https://openalex.org/W3034403876",
    "https://openalex.org/W3182915099",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3088059392",
    "https://openalex.org/W2555691318",
    "https://openalex.org/W3036120435",
    "https://openalex.org/W3186529872",
    "https://openalex.org/W2972735048",
    "https://openalex.org/W2963672599",
    "https://openalex.org/W3176580738",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W2949678053",
    "https://openalex.org/W3106460864",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3176261297",
    "https://openalex.org/W3175547012",
    "https://openalex.org/W3091315987",
    "https://openalex.org/W4394658982",
    "https://openalex.org/W2739046565",
    "https://openalex.org/W3037822982",
    "https://openalex.org/W2159086733",
    "https://openalex.org/W2971050273",
    "https://openalex.org/W2973159684",
    "https://openalex.org/W2971173235",
    "https://openalex.org/W1763968285",
    "https://openalex.org/W2892357930",
    "https://openalex.org/W4256018435",
    "https://openalex.org/W2311430799",
    "https://openalex.org/W2887782043",
    "https://openalex.org/W2808437126",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2903088770",
    "https://openalex.org/W2912102236",
    "https://openalex.org/W2963167310",
    "https://openalex.org/W3114617045",
    "https://openalex.org/W3014487746",
    "https://openalex.org/W2149327368",
    "https://openalex.org/W3173380736",
    "https://openalex.org/W3093233911",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W3113819942",
    "https://openalex.org/W2800596622",
    "https://openalex.org/W2740168486",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963446316",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2339590251",
    "https://openalex.org/W1987968880",
    "https://openalex.org/W3095351052"
  ],
  "abstract": "In this work, we present an extensive study on the use of pre-trained language models for the task of automatic Counter Narrative (CN) generation to fight online hate speech in English. We first present a comparative study to determine whether there is a particular Language Model (or class of LMs) and a particular decoding mechanism that are the most appropriate to generate CNs. Findings show that autoregressive models combined with stochastic decodings are the most promising. We then investigate how an LM performs in generating a CN with regard to an unseen target of hate. We find out that a key element for successful {`}out of target{'} experiments is not an overall similarity with the training data but the presence of a specific subset of training data, i. e. a target that shares some commonalities with the test target that can be defined a-priori. We finally introduce the idea of a pipeline based on the addition of an automatic post-editing step to refine generated CNs.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2022, pages 3099 - 3114\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nUsing Pre-Trained Language Models for Producing Counter Narratives\nAgainst Hate Speech: a Comparative Study\nSerra Sinem Tekiro˘glu2, Helena Bonaldi1,2, Margherita Fanton1,2∗, Marco Guerini2\n1University of Trento, Italy\n2Fondazione Bruno Kessler, Via Sommarive 18, Povo, Trento, Italy\ntekiroglu@fbk.eu, hbonaldi@fbk.eu,\nmargherita.fanton@ims.uni-stuttgart.de, guerini@fbk.eu\nAbstract\nIn this work, we present an extensive study on\nthe use of pre-trained language models for the\ntask of automatic Counter Narrative (CN) gen-\neration to ﬁght online hate speech in English.\nWe ﬁrst present a comparative study to deter-\nmine whether there is a particular Language\nModel (or class of LMs) and a particular de-\ncoding mechanism that are the most appropri-\nate to generate CNs. Findings show that au-\ntoregressive models combined with stochastic\ndecodings are the most promising. We then in-\nvestigate how an LM performs in generating a\nCN with regard to an unseen target of hate. We\nﬁnd out that a key element for successful ‘out\nof target’ experiments is not an overall similar-\nity with the training data but the presence of\na speciﬁc subset of training data, i. e. a target\nthat shares some commonalities with the test\ntarget that can be deﬁned a-priori. We ﬁnally\nintroduce the idea of a pipeline based on the\naddition of an automatic post-editing step to\nreﬁne generated CNs.\n1 Introduction\nHate Speech (HS) has found fertile ground in\nSocial Media Platforms. Actions undertaken by\nsuch platforms to tackle online hatred consist in\nidentifying possible sources of hate and removing\nthem by means of content deletion, account\nsuspension or shadow-banning. However, these\nactions are often interpreted and denounced as\ncensorship by the affected users and political\ngroups (Myers West, 2018). For this reason, such\nrestrictions can have the opposite effect of exacer-\nbating the hostility of the haters (Munger, 2017).\nAn alternative strategy, that is looming on the\nhorizon, is based on the use of Counter Narratives.\nCNs are “all communicative actions aimed at\nrefuting hate speech through thoughtful and cogent\nreasons, and true and fact-bound arguments\"\n(Schieb and Preuss, 2016). As a de-escalating\n∗ Now at the University of Stuttgart, Germany.\nmeasure, CNs have been proven to be successful in\ndiminishing hate, while preserving the freedom of\nspeech (Benesch, 2014; Gagliardone et al., 2015).\nAn example of <HS, CN>pair is shown below:\nHS: Women are basically childlike, they remain\nthis way most of their lives. Soft and emotional.\nIt has devastated our once great patriarchal\ncivilizations.\nCN: Without softness and emotions there would\nbe just brutality and cruelty. Not all women are\nsoft and emotional and many men have these\ncharacteristics. To perpetuate these socially\nconstructed gender proﬁles maintains norms which\noppress anybody.\nBased on their effectiveness, CNs have started be-\ning employed by NGOs to counter online hate.\nSince for NGO operators it is impossible to man-\nually write responses to all instances of hate, a\nline of NLP research has recently emerged, focus-\ning on designing systems to automatically generate\nCN suggestions (Qian et al., 2019; Tekiro˘glu et al.,\n2020; Fanton et al., 2021; Chung et al., 2021a; Zhu\nand Bhat, 2021). In this study, our main goal is\nto compare pre-trained language models (LM) and\ndecoding mechanisms in order to understand their\npros and cons in generating CNs. Thus, we use vari-\nous automatic metrics and manual evaluations with\nexpert judgments to assess several LMs, represent-\ning the main categories of the model architectures,\nand decoding methods. We further test the robust-\nness of the ﬁne-tuned LMs in generating CNs for\nan unseen target. Results show that autoregressive\nmodels are in general more suited for the task, and\nwhile stochastic decoding mechanisms can gener-\nate more novel, diverse, and informative outputs,\nthe deterministic decoding is useful in scenarios\nwhere more generic and less novel (yet ‘safer’)\nCNs are needed. Furthermore, in out-of-target ex-\nperiments we ﬁnd that the similarity of targets (e.g.\n3099\nJEWS and MUSLIMS as religious groups) plays\na crucial role for the effectiveness of portability\nto new targets. We ﬁnally show a promising re-\nsearch direction of leveraging gold human edits for\nbuilding an additional automatic post-editing step\nto correct errors made by LMs during generation.\nTo the best of our knowledge, this is the ﬁrst study\nsystematically analysing state of the art pre-trained\nLMs in CN generation.\n2 Related Work\nIn this section we ﬁrst discuss standard approaches\nto hate countering and studies on CN effectiveness\non Social Media Platforms, then the existing CN\ndata collection and generation strategies.\nHate countering. NLP has started addressing\nthe phenomenon of the proliferation of HS by creat-\ning datasets for automatic detection (Mathew et al.,\n2021; Cao et al., 2020; Kumar et al., 2018; Hos-\nseinmardi et al., 2015; Waseem, 2016; Burnap and\nWilliams, 2016). Several surveys provide a review\non the existing approaches on the topic (Poletto\net al., 2020; Schmidt and Wiegand, 2017; Fortuna\nand Nunes, 2018), also addressing the ethical chal-\nlenges of the task (Kiritchenko et al., 2021). Still,\nautomatic detection of HS presents some draw-\nbacks (Vidgen and Derczynski, 2020). First of all,\nthe datasets might include biases, and the models\ntend to replicate such biases (Binns et al., 2017;\nDavidson et al., 2019; Sap et al., 2019; Tsvetkov,\n2020). Moreover, the end goals for which HS de-\ntection is employed are often charged with cen-\nsorship of the freedom of speech by concerned\nusers (Munger, 2017; Myers West, 2018). In this\nscenario, NGOs have started employing CNs to\ncounter online hate. CNs have been shown to\nbe effective in reducing linguistic violence (Be-\nnesch, 2014; Gagliardone et al., 2015; Schieb and\nPreuss, 2016; Silverman et al., 2016; Mathew et al.,\n2019); moreover, even if they might not inﬂuence\nthe view of extremists, they are still effective in\npresenting alternative and non-hateful viewpoints\nto bystanders (Allison and Bussey, 2016; Anderson\net al., 2014).\nCN data collection. The existing studies for col-\nlecting CN datasets employ four main approaches.\nCrawling consists in automatically scraping web-\nsites, starting from an HS content and searching\nfor possible CNs among the responses (Mathew\net al., 2018, 2019). With crowdsourcing CNs are\nwritten by non-expert paid workers as responses\nto provided hate content (Qian et al., 2019). Nich-\nesourcing relies on a niche group of experts for\ndata collection (De Boer et al., 2012), and it was\nemployed by Chung et al. (2019) for CN collection\nusing NGO’s operators.Hybrid approaches use a\ncombination of LMs and humans to collect data\n(Wallace et al., 2019; Dinan et al., 2019; Vidgen\net al., 2020). Studies on CN collection are pre-\nsented in more detail by Tekiro ˘glu et al. (2020);\nFanton et al. (2021).\nCN generation. Neural approaches to automati-\ncally generate CNs are beginning to be investigated.\nFanton et al. (2021); Tekiro˘glu et al. (2020); Qian\net al. (2019) employ a mix of automatic and human\nintervention to generate CNs. Zhu and Bhat (2021)\npropose an entirely automated pipeline of candidate\nCN generation and ﬁltering. Other lines of work in-\nclude CN generation for under-resourced languages\nsuch as for Italian (Chung et al., 2020), and the gen-\neration of knowledge-bound CNs, which allows the\nproduction of CNs based on grounded and up-to-\ndate facts and plausible arguments, avoiding the\nhallucination phenomena (Chung et al., 2021a). In-\nstead, in our work we take a more foundational\nperspective, which is relevant for all the LM-based\npipelines described above. Therefore, we compare\nand assess various state of the art pre-trained LMs\nin an end-to-end setting, which is developed as a\ndownstream task for CN generation.\n3 Methodology\nIn this section, we present the CN dataset, the lan-\nguage models, and the decoding mechanisms em-\nployed for our experiments.\n3.1 Dataset for ﬁne-tuning\nFor this study we rely on the dataset proposed\nby Fanton et al. (2021), which is the only avail-\nable dataset that grants both the target diversity\nand the CN quality we aim for. The dataset was\ncollected with a human-in-the-loop approach, by\nemploying an autoregressive LM (GPT-2) paired\nwith three expert human reviewers. It features\n5003 <HS, CN>pairs, covering several targets\nof hate including DISABLED, JEWS, LGBT+,\nMIGRANTS, MUSLIMS, POC, WOMEN. The resid-\nual categories are collapsed to the label OTHER.\nWe partitioned the dataset into training, validation,\nand test sets with the ratio: 8 : 1 : 1(i. e. 4003, 500\nand 500 pairs), ensuring that all sets share the same\n3100\ntarget distribution, and no repetition of HS across\nthe sets is allowed.\n3.2 Models\nWe experiment with 5 Transformer based LMs\n(Vaswani et al., 2017) representing the main cate-\ngories of the model mechanisms: autoregressive,\nautoencoder, and seq2seq.\nBERT.The Bidirectional Encoder Representations\nfrom Transformers was introduced by Devlin et al.\n(2019). It is a bidirectional autoencoder that can be\nadapted to text generation (Wang and Cho, 2019).\nGPT-2. The Generative Pre-trained Transformer 2\nis an autoregressive model built for text generation\n(Radford et al., 2019).\nDialoGPT. The Dialogue Generative Pretrained\nTransformer is the extension of GPT-2 speciﬁ-\ncally created for conversational response genera-\ntion (Zhang et al., 2020).\nBART.BART is a denoising autoencoder for pre-\ntraining seq2seq models (Lewis et al., 2020). The\nencoder-decoder architecture of BART is com-\nposed of a bidirectional encoder and an autoregres-\nsive decoder.\nT5. The Text-to-Text Transfer Transformer\nproposed by Raffel et al. (2020) is a seq2seq model\nwith an encoder-decoder Transformer architecture.\nWhile all the other models could be ﬁne-tuned\ndirectly for the generation task, for BERT we warm-\nstarted an encoder-decoder model using BERT\ncheckpoints similar to the BERT2BERT model de-\nﬁned by (Rothe et al., 2020). The ﬁne-tuning de-\ntails and hyperparameter settings can be found in\nAppendix A.1.\n3.3 Decoding mechanisms\nWe utilize 4 decoding mechanisms: a deterministic\n(Beam Search) and three stochastic (Top-k, Top-p,\nand a combination of the two).\nBeam Search (BS). The Beam Search algorithm is\ndesigned to pick the most-likely sequence (Li et al.,\n2016; Wiseman et al., 2017).\nTop-k (Topk). The sampling procedure proposed\nby Fan et al. (2018) selects a random word from\nthe k most probable ones, at each time step.\nTop-p (Topp). Also known as Nucleus Sampling,\nthe parameter p indicates the total probability for\nthe pooled candidates, at each time step (Holtzman\net al., 2020).\nCombining Top-p and Top-k (Toppk). At decod-\ning stage, it is possible to combine the parameters\np and k. This is a Nucleus Sampling constrained to\nthe Top-k most probable words.\nIn our experiments we used the following param-\neters as default: Beam-Search with 5 beams and\nrepetition penalty = 2; Top-k with k = 40; Top-p\nwith p = .92; Toppk with k = 40and p = .92.\n4 Evaluation metrics\nWe use several metrics to evaluate various aspects\nof the CN generation.\nOverlap Metrics. These metrics depend on the\nn-gram similarity of the generated outputs to a set\nof reference texts in order to assess the quality.\nWe used our gold CNs as references and the CNs\ngenerated by the different models, as candidates.\nIn particular, we employed three BLEU variants:\nBLEU-1 (B-1), BLEU-3 (B-3) and BLEU-4 (B-4)\n(Papineni et al., 2002), and ROUGE-L (ROU) (Lin,\n2004).\nDiversity metrics. They are used to measure how\ndiverse and novel the produced CNs are. In partic-\nular, we utilized Repetition Rate (RR) to measure\nthe repetitiveness across generated CNs, in terms of\nthe average ratios of non-singletonn-grams present\nin the corpus (Bertoldi et al., 2013). It should be\nnoted that RR is calculated as a corpus-based rep-\netition score , i.e. inter-CN, instead of calculating\nintra-CN repetition of n-grams only. We also used\nNovelty (NOV) (Wang and Wan, 2018), based on\nJaccard similarity, to compute the amount of novel\ncontent that is present in the generated CNs as com-\npared to the training data.\nHuman evaluation metrics. Albeit more difﬁcult\nto attain, human judgments provide a more reliable\nevaluation and a deeper understanding than auto-\nmatic metrics (Belz and Reiter, 2006; Novikova\net al., 2017). To this end, we speciﬁed the follow-\ning dimensions for the evaluation of CNs. Suitable-\nness (SUI): measures how suitable a CN is to the\nHS in terms of semantic relatedness and in terms\nof adherence to CN guidelines 1; Grammaticality\n(GRM): how grammatically correct a generated\nCN is; Speciﬁcity (SPE): how speciﬁc are the ar-\nguments brought by the CN in response to the HS;\nChoose-or-not (CHO): determines whether the an-\nnotators would select that CN to post-edit and use it\nin a real case scenario as in the set up presented by\nChung et al. (2021b); Is-best (BEST): whether the\nCN is the absolute best among the ones generated\n1See for example https://getthetrollsout.\norg/stoppinghate\n3101\nfor an HS (i. e. whether the annotators would pick\nup exactly that CN if they had to use it in a real\ncase scenario).\nThe ﬁrst three dimensions are rated with a 5-\npoints Likert scale and follow the evaluation pro-\ncedure described by Chung et al. (2020), whereas\nboth choose-or-not and is-best are binary ratings\n(0, 1). Choose-or-not allows for multiple CNs to\nbe selected for the same HS, while only one CN\ncan be selected for is-best for each HS.\nToxicity.2 It determines how “rude, disrespect-\nful, or unreasonable” a text is. Toxicity has been\nemployed both to detect the bias present in LMs\n(Gehman et al., 2020) and as a solution to mitigate\nsuch bias (Gehman et al., 2020; Xu et al., 2020).\nSyntactic metrics. A high syntactic complexity\ncan be used as a proxy for an LM’s ability of gen-\nerating complex arguments. We used the syntactic\ndependency parser of spaCy3 For the task, focus-\ning on the following measures: Maximum Syntactic\nDepth (MSD): the maximum depth among the de-\npendency trees calculated over each sentence com-\nposing a CN. Average Syntactic Depth (ASD): the\naverage depth of the sentences in each CN. Num-\nber of Sentences (NST): the number of sentences\ncomposing a CN.\n5 Experiments\nWe performed two sets of experiments: ﬁrst, we as-\nsessed how LMs perform in the task of generating\nCNs with different decoding mechanisms. Then,\nwe selected the best model from the ﬁrst round\nof experiments and tested its generalization capa-\nbilities when confronted with an unseen target of\nhate.\n5.1 LMs and decoding experiments\nFor the ﬁrst round of experiments, in order to avoid\npossible unfair assessments given by the open na-\nture of the generative task (i. e. a highly suitable CN\ncandidate could be scored low due to its difference\nfrom the single reference/gold CN), at test time\nwe allowed the generation of several candidates\nfor each HS+LM+decoding mechanism combina-\ntion. We loosely drew inspiration from the Rank-N\nAccuracy procedure and the ‘generate, prune, se-\nlect’ procedure (Zhu and Bhat, 2021). In particular,\n2https://www.perspectiveapi.com\n3https://spacy.io/usage/\nlinguistic-features#dependency-parse\ngiven an LM and a decoding mechanism, we gen-\nerated 5 CNs for each HS in the test set.\nAutomated evaluation and selection We set up\nthe automatic evaluation strategy as displayed in\nFigure 1. First, we scored each CN with the overlap\nmetrics presented in Section 4, using the gold CN\nas a reference. Next, we ranked the candidate CNs\nwith respect to the overlap scores and computed\nthe mean of the rankings. Then, we selected the\nbest ones according to the following criteria:\nBestLM selects the single best CN for an HS among\nthe 20 generated by the 4 models.\nBestD selects the single best CN for an HS among\nthe 25 generated by the 5 decoding conﬁgurations.\nBestLM+D selects the single best CN among the 5\ngenerated with each model-decoding combination.\nMoreover, we assessed the overall corpus-wise\nquality of the generated CNs with respect to the\nmodels, to the decoding mechanisms, and to the\nmodel-decoding combinations via the diversity\nmetrics.\nFigure 1: Given an HS, 5 CNs are generated for each\nmodel-decoding combination.\n indicates the best CN\nper model (∈BestLM).\n indicates the best CN per de-\ncoding (∈BestD).\n indicates the best CN per model-\ndecoding combination (∈BestLM+D).\nHuman evaluation on a sample To perform the\nhuman evaluation we referred to the BestLM gen-\nerations and sampled 200 instances from it. Each\ninstance comprises an HS and 5 relevant CNs, each\ngenerated by a different model. We recruited 2\nannotators who were trained extensively for the\ntask following the procedure used by Fanton et al.\n(2021). The expert annotators were asked to evalu-\nate the 5 CNs corresponding to the HS, according\nto the dimensions described in Section 4. We en-\n3102\nriched the evaluation of this subset with the toxicity\nand the syntactic metrics.\n5.2 Results of the ﬁrst set of experiments\nThe results of the experiments on the LMs and the\ndecoding mechanisms are reported in this section4.\nBest Model The results of the comparison of the\nmodels on the Best LM generations can be found\nin Table 1. Regarding the overlap and diversity\nmetrics, DialoGPT records the best or the second\nbest score in all the metrics, apart from novelty\nwhere it still achieves a high score (0.643) close\nto the best performance (0.655). T5 also achieves\nhigh scores, especially on ROUGE, BLEU-1 and\nnovelty.\nBART, instead, is the best model according to\nhuman evaluation metrics, apart from speciﬁcity.\nOn the other hand, it shows poor performances in\nterms of diversity metrics, indicating that it tends to\nproduce grammatical and suitable but very generic\nresponses.\nBERT records the worst scores for all the over-\nlap and diversity metrics apart from novelty. How-\never, it also achieves the best syntactic metric re-\nsults. Therefore, it is evident that BERT’s output is\nmore complex, but very repetitive. The combina-\ntion of these aspects eventually affects the clarity\nof BERT’s output such that it yields poor results in\nthe human evaluation, in particular for grammati-\ncality (4.2, while other models are above 4.6). This\npoor grammaticality can also explain the syntactic\nscores since the spaCy dependency parser was not\ntrained to handle ungrammatical text and this could\nactually inﬂates the ASD and MSD scores.\nGPT-2 overall yields very competitive results for\nseveral groups of metrics. It obtains the second-\nhighest novelty score (0.653) and the best RR\n(7.736). It also achieves the second best results\non BLEU-3, maximum syntactic depth and number\nof sentences, and the best results on toxicity and\nspeciﬁcity (2.880) indicating the ability to produce\ncomplex, suitable, focused and diverse CNs.\nAfter the human evaluation we ran a qualitative\ninterview with the annotators, whose feedback on\nthe data strengthened the results we observed and\nthe conclusion we drew. For instance, they reported\nthe repetition of simple, yet catch-them-all, expres-\nsions (e.g. “they are our brothers and sisters\") re-\ngardless of the target. Further inspections found\n4The training details for all the models we employed are\ndescribed in Appendix A.1\nthat those CNs were mainly produced by BERT,\nwhich is in line with BERT’s RR score.\nBest Decoding mechanism. The results calcu-\nlated on Best D output are presented in Table 2.\nTopk is the best performing decoding mechanism\nachieving the best results on the diversity metrics,\nBLEU-3 and BLEU-4. It is also the best perform-\ning for speciﬁcity, maximum syntactic depth and\nnumber of sentences, and the second best for aver-\nage syntactic depth and toxicity.\nThe other stochastic decoding mechanisms per-\nform well too. Topp yields competitive results on\nboth diversity and overlap metrics; it is the sec-\nond best for speciﬁcity, and achieves good results\non the syntactic metrics. Top pk has a good per-\nformance on the overlap metrics. It obtains the\nsecond-highest scores in most of the human eval-\nuation metrics and the lowest in toxicity, and it\nreaches a reasonable speciﬁcity score.\nOn the other hand, BS does not achieve partic-\nularly good results, except for the ROUGE score.\nEven if it is the best decoding with respect to the\nhuman evaluation, this comes at the cost of speci-\nﬁcity and diversity. Through a post-hoc manual\nanalysis we observed that it was due to the deter-\nministic nature of BS, that tends to choose the most\nprobable sequences, i. e. the “safest\", thus resulting\nin vague and repetitive outputs.\nBest Model-Decoding combination Here we\nbrieﬂy discuss the results of the evaluation obtained\non the BestLM+D generations. In particular, the au-\ntoregressive models GPT-2 and DialoGPT behave\nsimilarly with similar decoding mechanisms, such\nthat BS outputs the best results for almost all the\noverlap metrics, and the worst for the diversity met-\nrics. On the contrary, for the other models, the\nresults achieved with stochastic decoding mecha-\nnisms are the best for the overlap metrics. In almost\nall the cases, we observe that the stochastic decod-\ning mechanisms perform better on syntactic and\ndiversity metrics and on toxicity, while for the hu-\nman evaluation metrics BS tends to be the best,\nexcept for speciﬁcity. A detailed discussion can be\nfound in Appendix A.2.\nDiscussion. In this set of experiments, we found\nthat the autoregressive models perform the best ac-\ncording to a combination of several metrics that\nwe deem particularly relevant (e.g. more novel,\ndiverse, and informative outputs). Of course more\nrepetitive and conservative outputs can be preferred\n3103\nOverlap Diversity Toxicity Syntactic metrics Human evaluation\nROU B-1 B-3 B-4 RR NOV - ASD MSD NST SUI SPE GRM CHO BEST\nBART 0.268 0.277 0.0850.051 20.722 0.560 0.420 4.311 4.965 1.7403.790 2.552 4.937 0.840 0.272\nBERT 0.237 0.277 0.073 0.03724.747 0.605 0.406 5.008 6.160 2.280 3.135 2.647 4.247 0.717 0.122\nT5 0.274 0.3020.083 0.042 8.548 0.655 0.359 4.692 5.325 1.7152.872 2.402 4.680 0.642 0.090\nDialoGPT0.273 0.304 0.093 0.052 8.248 0.643 0.343 4.677 5.575 1.8953.392 2.755 4.880 0.767 0.245\nGPT-2 0.264 0.2970.088 0.050 7.736 0.653 0.342 4.584 5.595 2.2403.555 2.880 4.867 0.795 0.270\nTable 1: Results of the overlap and diversity metrics are calculated on the Best LM generations while the toxicity,\nthe syntactic metrics and the human evaluation are calculated on the corresponding subset.\nOverlap Diversity Toxicity Syntactic metrics Human evaluation\nROU B-1 B-3 B-4 RR NOV - ASD MSD NST SUI SPE GRM CHO BEST n\nBS 0.2870.299 0.096 0.05921.579 0.561 0.3984.415 5.048 1.6843.9362.497 4.9250.826 0.222%18.7\nToppk 0.2870.3200.1060.05911.404 0.639 0.3524.676 5.488 1.9323.3242.647 4.688 0.764 0.212%29.3\nTopk 0.282 0.3140.1060.06010.0760.652 0.3744.7045.7562.1333.1552.716 4.659 0.716 0.183%27.1\nTopp 0.2850.3190.1050.06011.270 0.640 0.3814.753 5.671 2.0683.1492.687 4.681 0.723 0.189%24.9\nTable 2: The results for the overlap and diversity metrics are calculated on the BestD generations: for each decoding\nmechanism, there are 2500 CNs. The remaining metrics are calculated on a subset of 1000 CNs: the distribution\nof which is shown in the column \"n\".\nwhen high precision of suitable CNs are required at\nthe expense of being more generic and less novel.\nStill, for what concerns autoregressive models it\ncould be argued that the good performance of the\nGPT-2 model we ﬁne-tuned is due to the fact that\ngenerated CNs and gold CNs derive from a similar\ndistribution (GPT-2 was employed in the human-\nin-the-loop process used to create the reference\ndataset from Fanton et al. (2021)). While we rec-\nognize that this could partially explain the perfor-\nmance of our GPT-2 model, it does not explain the\nperformance of DialoGPT, which is pre-trained on\na completely different dataset. Therefore, we can\nreasonably conclude that autoregressive models are\nparticularly suited for the task, regardless of the\npre-training data.\nWith respect to the decoding mechanisms, we\nrecord high repetitiveness and low novelty for the\ndeterministic decoding BS. Even if it reaches high\nscores in most of the human evaluation metrics,\nit fails to produce speciﬁc CNs ending up in gen-\nerating suitable, yet generic responses. On the\ncontrary, stochastic decoding mechanisms produce\nmore novel and speciﬁc responses.\nExample CNs generated in this session of exper-\niments, along with some qualitative analysis, can\nbe found in Appendix A.3.\n5.3 Leave One Target Out experiments\nIn the second stage, we built a set of cross-domain\nexperiments to capture the generalization capabil-\nities of the best LM determined in the previous\nexperiments. Speciﬁcally, we concentrate on as-\nsessing how much a pre-trained language model\nﬁne-tuned on a pool of hate targets can generalize\nto an unseen target.\nThus, for the out of target experiment we se-\nlected the LM that we deem the most prominent\nin order to reduce the number of LM conﬁgura-\ntions to compare. In particular, since we want to\nexamine the generalization capability of the LM,\nthe generation of novel CNs, in comparison to the\ntraining data, is given primary importance. Sec-\nondly, speciﬁcity is also crucial since it signiﬁes\nthe ability of the LM/decoding mechanism in gener-\nating accurate CNs and avoiding vague yet suitable,\ncatch-all CNs. In contrast, repetitiveness is an un-\ndesirable feature of CNs, as it signals the tendency\nof a model to produce less ﬂexible content. Given\nthese considerations, we chose to employ GPT-2\nwith Topk decoding for the Leave One Target Out\n(LOTO ) experiments since it is the conﬁguration\nachieving the best trade-off amongst all the others.\nThis set of experiments is structured in 3 steps,\nreplicated for each of the selected targets. We se-\nlected the targets with the highest number of ex-\namples (MUSLIMS, MIGRANTS, WOMEN, LGBT+\nand JEWS) to have a sufﬁcient sized test set for\neach conﬁguration.\nFirst, we sampled from the Fanton et al. (2021)\ndataset 600 pairs for each LOTO target, in order\nto have a balanced setting. Additionally, POC and\nDISABLED were always kept in the training set,\nand we removed multi-target cases from OTHER.\nThe resulting dataset consists of 3729 instances\n(further details are provided in Appendix A.4). Sec-\n3104\nondly, we ﬁne-tuned 5 different conﬁgurations of\nthe LM, and in each conﬁguration one of the 5\nLOTO targets is not present in the training data:\nLM-JEWS, LM-LGTB+, LM-MIGRANTS, LM-MUSLIMS\nand LM -WOMEN. Finally, we tested each LOTO\nmodel on the 600 HSs in the test set made of\n“left out\" target examples. For instance, the model\nLM-JEWS is used for generating the CNs for the\ntarget JEWS, after being trained on <HS, CN>\ndata without any instances with the label JEWS.\nWe generated 5 CNs for each HS and selected the\nbest CN according to the procedure described in\nSection 5.1.\nResults of LOTO experiments\nWe analyse the CNs generated with the LOTO mod-\nels through overlap and diversity metrics (Table 3).\nWe refer to Appendix A.4 for the comparison be-\ntween RR calculated on the candidate CNs and the\nreference CNs of the Fanton et al. (2021) dataset.\nFor all the targets we record higher novelty\nscores as compared to the previous experiments.\nHigher novelty ranges indicate that conditioning\nwith new material (i. e. HS for the unseen targets)\ninduces GPT-2 to produce new arguments. On\nthe other hand, as expected, the overlap scores for\nLOTO are remarkably lower than those from the\nprevious experiments (Table 3). Therefore, we can\ninfer that generalizing to an unseen target is harder\nthan generalizing to an unseen HS.\nLOTO Overlap Diversity\nTarget ROU B-1 B-3 B-4 RR NOV\nJEWS 0.1609 0.1842 0.0134 0.00354.796 0.718\nLGBT+ 0.1599 0.1828 0.0149 0.00554.620 0.718\nMIGRANTS0.1659 0.1915 0.0163 0.00384.707 0.720\nMUSLIMS0.1743 0.19340.0197 0.00595.314 0.712\nWOMEN 0.17550.1988 0.01950.00684.632 0.729\nTable 3: The overlap and diversity metrics scores for\nthe various LOTO conﬁgurations.\nWe also found out that the CNs generated in\nthe LM -MUSLIMS and LM -WOMEN conﬁgurations\nobtain the highest overlap scores (Table 3). We\nhypothesize that the high scores can be explained\nby the presence of a target in the LOTO training\nthat is highly similar to the left out one. To this\nend, we computed the novelty between each target\nsubset of the training data and the LOTO test data\nfor that conﬁguration (see Appendix A.4 for de-\ntails). The reference CNs for LM-MUSLIMS record\nthe lowest novelty scores with respect to the JEWS\nsubset of the training set (i. e. 0.761). Thus, it\nFigure 2: The correlation between the novelty of the\nreference CNs and overlap metrics: in each plot, the\ndots and the darker line correspond to the most inﬂuen-\ntial target; the triangles and the lighter line correspond\nto the results calculated without it.\ncan be interpreted as the most inﬂuential portion\nof training data for the target MUSLIMS. On the\nother hand, for LM -WOMEN the highest inﬂuence\nis recorded with the LGBT+ subset of the training\ndata (i. e. 0.763). These results can be explained by\nthe semantic similarity of the target MUSLIMS to\nJEWS, both being religious groups; and of WOMEN\nto LGBT+, both being related to gender issues.\nAs a complementary analysis, we consider two\ndifferent computations of the reference CN nov-\nelty: with respect to the most inﬂuential target for\neach LOTO conﬁguration, and with respect to the\nLOTO training data without the most inﬂuential tar-\nget. We computed the Pearson correlation between\nthe overlap metrics and each of the two novelty\ncomputations. In Figure 2, we observe that re-\nmoving the inﬂuential target from the training data\nstrongly decreases the correlation with the over-\nlap metrics (from an average of -0.889 to -0.416).\nConsequently, we can conclude that to obtain high\noverlap results in the LOTO experiments, it is neces-\nsary that the training data contains a target strongly\nconnected to the left out one. Most importantly, this\nconnection is not arbitrarily decided but it is based\non an a-priori semantic similarity of the targets as\nexempliﬁed before.\nFinally, we chose to generate also with the BS\ndecoding mechanism, to use it as a baseline and\ncompare it to the stochastic decoding mechanism\n(Top-k). In particular, we computed the Pearson\ncorrelation between the novelty of the reference\n3105\nCNs and the novelty of the candidate CNs with\nrespect to the corresponding training data (Figure\n3). We can observe that for the BS generation the\nnovelty of the candidate CNs is lower than Top-k\n(0.67-0.74 vs. 0.75-0.77) and the correlation with\nthe novelty of the reference is weaker (0.53 vs.\n0.59). This conﬁrms the lower generalization abil-\nity with the deterministic decoding mechanism (as\ncompared to the stochastic) that tends to produce\ngeneric and repetitive responses regardless of the\nsemantic distances of the LOTO targets from the\ntraining data.\nFigure 3: Reference and candidate CNs novelty, for\nTop-k and BS LOTO generations.\n6 Automatic Post-Editing\nIn the previous experiments we ﬁne-tuned our mod-\nels making resort to <HS, CN>pairs alone. Still\nthe Fanton et al. (2021) dataset contains additional\ninformation that can be useful for our task: i. e.\nthe original GPT-2 generation before undergoing\nhuman post-editing.\nThus, as a ﬁnal experiment, we propose to fur-\nther improve the CN generation by moving from\nan end-to-end framework to a two stage pipeline,\nby decoupling CN generation from its ‘ﬁnal reﬁne-\nment’. In particular we propose the adoption of\nan Automatic Post-Editing (APE) stage in order to\ncapture and utilize the nuances among the machine\ngenerated CNs and their human post-edited ver-\nsions. APE, which is used for automatically correct-\ning errors made by machine translation (MT) sys-\ntems before performing actual human post-editing,\nhas been an important tool for MT (Knight and\nChander, 1994; do Carmo et al., 2021). Consid-\nering its effectiveness in MT, we hypothesize that\nbuilding a pipeline with CN generation and APE\ncould alleviate the requirement of the ﬁnal manual\npost-editing (Allen and Hogan, 2000; Chatterjee\net al., 2019) to achieve better constructed CNs.\nTo this end, we ﬁne-tuned another instance\nof GPT-2 medium model speciﬁcally for the\npost-editing task using the <HS, CNor, CNpe>\ntriplets5, where CNor and CNpe denote the CNs\noriginally generated by an LM and their human\npost-edited versions, respectively. The triplets\nwere then ﬁltered by removing those for which\nCNor = CNpe. More details about the experiment\nsettings can be found in Appendix A.5.\nData CNape CNor N/A\nFanton et al. (2021) 26 14 60\nGPT-2 Topk 37 19 44\nTable 4: The human annotation results for the APE ex-\nperiments in terms of average preference percentages.\nWe have conducted two human evaluations over\nthe subsets of: i) the CNor of the Fanton et al.\n(2021) test samples, ii) the CN outputs of the\nbest model and decoding mechanism combination\nprovided as the results of the ﬁrst set of experi-\nments, that yielded the top 50 Translation Error\nRate (TER) (Snover et al., 2006) scores with re-\nspect to the CNors. The two expert annotators\nwere asked to state their preferences among the\n2 randomly sorted CNs, CNor and CNape (auto-\nmatically post-edited output), for a given HS. The\nannotators were also allowed to decide on a tie. Re-\nsults, shown in Table 4, indicate that, albeit there\nare often ties and only a subset of CNor is actually\nmodiﬁed, when there is a preference, it is predom-\ninantly in favour of the automatically post-edited\nversions over the GPT-2 generated CNs (26% vs.\n14% for the test set, and 37% vs. 19% for the GPT-\n2 Topk generations, on average). Regarding the\nexperiment results, we believe that APE is a highly\npromising direction to increase the efﬁcacy of the\nCN generation models where generation quality\nand diversity is crucial, and considering that obtain-\ning/enlarging expert datasets to train better models\nis not simple.\n7 Conclusion\nIn this work, we focus on automatic CN generation\nas a downstream task. First, we present a compara-\ntive study to determine the performances and pecu-\nliarities of several pre-trained LMs and decoding\nmechanisms. We observe that the best results (in\nterm of novelty and speciﬁcity) overall are achieved\n5This is in line with the APE paradigm where the triplet is\nmade of <source sentece, MToutput, human post-edits>.\n3106\nby the autoregressive models with stochastic de-\ncoding: GPT-2 with the Topk decoding mechanism,\nand DialoGPT with the combination Top pk. At\nthe same time deterministic decoding can be used\nwhen more generic yet ‘safer’ CNs are preferred.\nThen, we investigate the performances of LMs\nin zero-shot generation for unseen targets of hate.\nHence, we ﬁne-tuned 5 different versions of GPT-2,\nleaving out the examples pertaining to one target\nat each turn. We ﬁnd out that for each conﬁgura-\ntion/version, there is a subset of the training data\nwhich is more inﬂuential with respect to the gener-\nated data (i. e. a target that shares some commonal-\nities with the test target that can be deﬁned a-priori).\nFinally, we introduce an experiment by training an\nautomatic post-editing module to further improve\nthe CN generation quality. The notable human eval-\nuation results paves the way for a promising future\ndirection that decouples CN generation from its\n‘ﬁnal reﬁnement’.\nEthical Considerations\nAlthough tackling online hatred through CNs in-\nherently protects the freedom of speech and has\nbeen proposed as a better alternative to the detect-\nremove-ban approaches, automatization of CN gen-\neration can still raise some ethical concerns and\nsome measures must be taken to avoid undesired\neffects during research. Thus, we address the rel-\nevant ethical considerations and our remedies as\nfollows:\nAnnotation Guidelines: The well-being of the\nannotators was our top priority during the whole\nstudy. Therefore, we strictly followed the guide-\nlines created for CN studies (Fanton et al., 2021)\nthat were adapted from (Vidgen et al., 2019). The\nhuman evaluations have been conducted with the\nhelp of two expert annotators in CNs. These ex-\nperts were already trained for the CN generation\ntask and employed for the work presented by (Fan-\nton et al., 2021). We further instructed them in the\naims of each experiment, clearly explained the eval-\nuation tasks, and then we exempliﬁed proper eval-\nuation of <HS, CN> pairs using various types\nof CNs. Most importantly, we limited the expo-\nsure to hateful content by providing a daily time\nlimit of annotation. Concerning the demographics,\ndue to the harmful content that can be found in the\ndata, all annotators were adult volunteers, perfectly\naware of the objective of the study.\nDataset. We purposefully chose an expert-based\ndataset in order to avoid the risk of modeling the\nlanguage of real individuals to (i) prevent any pri-\nvacy issue, (ii) avoid to model inappropriate CNs\n(e.g. containing abusive language) that could be\nproduced by non-experts. The dataset also focuses\non the CN diversity while keeping the HSs as\nstereotypical as possible so that our CN genera-\ntion models have a very limited diversity on the\nhateful language, nearly precluding the misuse.\nComputational Task. CN generation models are\nnot meant to be used in an autonomous way, since\neven the best models can still produce substandard\nCNs containing inappropriate or negative language.\nInstead, following a Human–computer cooperation\nparadigm, our focus is on building models that can\nbe helpful to NGO operators by providing them di-\nverse and novel CN candidates for their hate coun-\ntering activities and speed up the manual CN writ-\ning to a certain extent. This approach also gives\nground to some of the measures we used during\nevaluation (namely choose-or-not and is-best).\nModel Distribution. In addition to the limited\nand simpliﬁed hateful content in the dataset we\nselected, we further reduce the risk of misuse by\nchoosing a speciﬁc distribution strategy: i) we only\nmake available the non-autoregressive models in\norder to eliminate the risk of using over-generation\nfor hate speech creation, ii) we distribute such mod-\nels only for research purposes and through a re-\nquest based procedure in order to keep track of the\npossible users.\nReferences\nJeffrey Allen and Christopher Hogan. 2000. Toward\nthe development of a post editing module for raw\nmachine translation output: A controlled language\nperspective. In Third International Controlled Lan-\nguage Applications Workshop (CLAW-00) , pages\n62–71.\nKimberley R Allison and Kay Bussey. 2016. Cyber-\nbystanding in context: A review of the literature on\nwitnesses’ responses to cyberbullying.Children and\nYouth Services Review, 65:183–194.\nJenn Anderson, Mary Bresnahan, and Catherine Musat-\nics. 2014. Combating weight-based cyberbullying\non facebook with the dissenter effect. Cyberpsychol-\nogy, Behavior, and Social Networking , 17(5):281–\n286.\nAnja Belz and Ehud Reiter. 2006. Comparing auto-\nmatic and human evaluation of nlg systems. In 11th\n3107\nconference of the european chapter of the associa-\ntion for computational linguistics, pages 313–320.\nSusan Benesch. 2014. Countering dangerous speech:\nNew ideas for genocide prevention. Washington,\nDC: United States Holocaust Memorial Museum.\nNicola Bertoldi, Mauro Cettolo, and Marcello Federico.\n2013. Cache-based online adaptation for machine\ntranslation enhanced computer assisted translation.\nIn MT-Summit, pages 35–42.\nReuben Binns, Michael Veale, Max Van Kleek, and\nNigel Shadbolt. 2017. Like trainer, like bot? inheri-\ntance of bias in algorithmic content moderation. In\nSocial Informatics, pages 405–415, Cham. Springer\nInternational Publishing.\nPete Burnap and Matthew L Williams. 2016. Us and\nthem: identifying cyber hate on twitter across mul-\ntiple protected characteristics. EPJ Data Science ,\n5(1):11.\nRui Cao, Roy Ka-Wei Lee, and Tuan-Anh Hoang. 2020.\nDeephate: Hate speech detection via multi-faceted\ntext representations. In 12th ACM Conference on\nWeb Science, pages 11–20.\nFélix do Carmo, Dimitar Shterionov, Joss Moorkens,\nJoachim Wagner, Murhaf Hossari, Eric Paquin, Dag\nSchmidtke, Declan Groves, and Andy Way. 2021.\nA review of the state-of-the-art in automatic post-\nediting. Machine Translation, 35(2):101–143.\nRajen Chatterjee, Christian Federmann, Matteo Negri,\nand Marco Turchi. 2019. Findings of the wmt 2019\nshared task on automatic post-editing. In Proceed-\nings of the Fourth Conference on Machine Transla-\ntion (Volume 3: Shared Task Papers, Day 2) , pages\n11–28.\nYi-Ling Chung, Elizaveta Kuzmenko, Serra Sinem\nTekiro˘glu, and Marco Guerini. 2019. CONAN -\nCOunter NArratives through nichesourcing: a mul-\ntilingual dataset of responses to ﬁght online hate\nspeech. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 2819–2829, Florence, Italy. Association\nfor Computational Linguistics.\nYi-Ling Chung, Serra Sinem Tekiro ˘glu, and Marco\nGuerini. 2020. Italian counter narrative generation\nto ﬁght online hate speech. In Proceedings of the\nSeventh Italian Conference on Computational Lin-\nguistics CLiC-it.\nYi-Ling Chung, Serra Sinem Tekiro ˘glu, and Marco\nGuerini. 2021a. Towards knowledge-grounded\ncounter narrative generation for hate speech. In\nFindings of the Association for Computational Lin-\nguistics: ACL-IJCNLP 2021 , pages 899–914, On-\nline. Association for Computational Linguistics.\nYi-Ling Chung, Serra Sinem Tekiro ˘glu, Sara Tonelli,\nand Marco Guerini. 2021b. Empowering ngos in\ncountering online hate messages. Online Social Net-\nworks and Media, 24:100150.\nThomas Davidson, Debasmita Bhattacharya, and Ing-\nmar Weber. 2019. Racial bias in hate speech and\nabusive language detection datasets. In Proceedings\nof the Third Workshop on Abusive Language Online,\npages 25–35.\nVictor De Boer, Michiel Hildebrand, Lora Aroyo,\nPieter De Leenheer, Chris Dijkshoorn, Binyam\nTesfa, and Guus Schreiber. 2012. Nichesourcing:\nharnessing the power of crowds of experts. In Inter-\nnational Conference on Knowledge Engineering and\nKnowledge Management, pages 16–20. Springer.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nEmily Dinan, Samuel Humeau, Bharath Chintagunta,\nand Jason Weston. 2019. Build it break it ﬁx it for\ndialogue safety: Robustness from adversarial human\nattack. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4537–4546.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-\nerarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889–898, Melbourne, Australia. Association\nfor Computational Linguistics.\nMargherita Fanton, Helena Bonaldi, Serra Sinem\nTekiro˘glu, and Marco Guerini. 2021. Human-in-the-\nloop for data collection: a multi-target counter narra-\ntive dataset to ﬁght online hate speech. In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics , Online. Association\nfor Computational Linguistics.\nPaula Fortuna and Sérgio Nunes. 2018. A survey on au-\ntomatic detection of hate speech in text. volume 51,\npage 85. ACM.\nIginio Gagliardone, Danit Gal, Thiago Alves, and\nGabriela Martinez. 2015. Countering online hate\nspeech. Unesco Publishing.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A Smith. 2020. Realtoxici-\ntyprompts: Evaluating neural toxic degeneration in\nlanguage models. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing: Findings, pages 3356–3369.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text\ndegeneration. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020.\n3108\nHoma Hosseinmardi, Sabrina Arredondo Mattson, Ra-\nhat Ibn Raﬁq, Richard Han, Qin Lv, and Shivakant\nMishra. 2015. Detection of cyberbullying incidents\non the instagram social network. arXiv preprint\narXiv:1503.03909.\nSvetlana Kiritchenko, Isar Nejadgholi, and Kathleen C\nFraser. 2021. Confronting abusive language online:\nA survey from the ethical and human rights per-\nspective. Journal of Artiﬁcial Intelligence Research,\n71:431–478.\nKevin Knight and Ishwar Chander. 1994. Automated\npostediting of documents. In AAAI, volume 94,\npages 779–784.\nRitesh Kumar, Atul Kr Ojha, Shervin Malmasi, and\nMarcos Zampieri. 2018. Benchmarking aggression\nidentiﬁcation in social media. In Proceedings of the\nFirst Workshop on Trolling, Aggression and Cyber-\nbullying (TRAC-2018), pages 1–11.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nJiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky,\nMichel Galley, and Jianfeng Gao. 2016. Deep rein-\nforcement learning for dialogue generation. In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1192–\n1202, Austin, Texas. Association for Computational\nLinguistics.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nBinny Mathew, Navish Kumar, Pawan Goyal, Animesh\nMukherjee, et al. 2018. Analyzing the hate and\ncounter speech accounts on twitter. arXiv preprint\narXiv:1812.02712.\nBinny Mathew, Punyajoy Saha, Hardik Tharad, Sub-\nham Rajgaria, Prajwal Singhania, Suman Kalyan\nMaity, Pawan Goyal, and Animesh Mukherjee. 2019.\nThou shalt not hate: Countering online hate speech.\nIn Proceedings of the International AAAI Confer-\nence on Web and Social Media , volume 13, pages\n369–380.\nBinny Mathew, Punyajoy Saha, Seid Muhie Yi-\nmam, Chris Biemann, Pawan Goyal, and Animesh\nMukherjee. 2021. Hatexplain: A benchmark dataset\nfor explainable hate speech detection. In Proceed-\nings of the AAAI Conference on Artiﬁcial Intelli-\ngence, volume 35, pages 14867–14875.\nKevin Munger. 2017. Tweetment effects on the\ntweeted: Experimentally reducing racist harassment.\nPolitical Behavior, 39(3):629–649.\nSarah Myers West. 2018. Censored, suspended, shad-\nowbanned: User interpretations of content modera-\ntion on social media platforms. New Media & Soci-\nety, 20(11):4366–4383.\nJekaterina Novikova, Ondrej Dusek, Amanda Cercas\nCurry, and Verena Rieser. 2017. Why we need new\nevaluation metrics for nlg. In 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2231–2242. Association for Computational\nLinguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th annual meeting on association for compu-\ntational linguistics, pages 311–318. Association for\nComputational Linguistics.\nFabio Poletto, Valerio Basile, Manuela Sanguinetti,\nCristina Bosco, and Viviana Patti. 2020. Resources\nand benchmark corpora for hate speech detection: a\nsystematic review. Language Resources and Evalu-\nation, pages 1–47.\nJing Qian, Anna Bethke, Yinyin Liu, Elizabeth Beld-\ning, and William Yang Wang. 2019. A bench-\nmark dataset for learning to intervene in online hate\nspeech. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4757–4766, Hong Kong, China. Association for\nComputational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8).\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nSascha Rothe, Shashi Narayan, and Aliaksei Severyn.\n2020. Leveraging pre-trained checkpoints for se-\nquence generation tasks. Transactions of the Asso-\nciation for Computational Linguistics, 8:264–280.\nMaarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi,\nand Noah A Smith. 2019. The risk of racial bias in\nhate speech detection. In Proceedings of the 57th\nannual meeting of the association for computational\nlinguistics, pages 1668–1678.\nCarla Schieb and Mike Preuss. 2016. Governing hate\nspeech by means of counterspeech on facebook. In\n66th ICA Annual Conference, at Fukuoka, Japan ,\npages 1–23.\nAnna Schmidt and Michael Wiegand. 2017. A survey\non hate speech detection using natural language pro-\ncessing. In Proceedings of the Fifth International\n3109\nWorkshop on Natural Language Processing for So-\ncial Media, pages 1–10.\nTanya Silverman, Christopher J Stewart, Jonathan\nBirdwell, and Zahed Amanullah. 2016. The im-\npact of counter-narratives. Institute for Strate-\ngic Dialogue, London. https://www. strategicdi-\nalogue. org/wp-content/uploads/2016/08/Impact-of-\nCounter-Narratives_ONLINE. pdf–73.\nMatthew Snover, Bonnie Dorr, Richard Schwartz, Lin-\nnea Micciulla, and John Makhoul. 2006. A study of\ntranslation edit rate with targeted human annotation.\nIn Proceedings of association for machine transla-\ntion in the Americas , volume 200, 6. Cambridge,\nMA.\nSerra Sinem Tekiro ˘glu, Yi-Ling Chung, and Marco\nGuerini. 2020. Generating counter narratives\nagainst online hate speech: Data and strategies. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 1177–\n1190, Online. Association for Computational Lin-\nguistics.\nMengzhou Xia Anjalie Field Yulia Tsvetkov. 2020. De-\nmoting racial bias in hate speech detection. So-\ncialNLP 2020, page 7.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nBertie Vidgen and Leon Derczynski. 2020. Direc-\ntions in abusive language training data, a system-\natic review: Garbage in, garbage out. Plos one ,\n15(12):e0243300.\nBertie Vidgen, Alex Harris, Dong Nguyen, Rebekah\nTromble, Scott Hale, and Helen Margetts. 2019.\nChallenges and frontiers in abusive content detec-\ntion. In Proceedings of the third workshop on abu-\nsive language online, pages 80–93.\nBertie Vidgen, Tristan Thrush, Zeerak Waseem, and\nDouwe Kiela. 2020. Learning from the worst: Dy-\nnamically generated datasets to improve online hate\ndetection. arXiv preprint arXiv:2012.15761.\nEric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Ya-\nmada, and Jordan Boyd-Graber. 2019. Trick me\nif you can: Human-in-the-loop generation of adver-\nsarial question answering examples. Transactions\nof the Association for Computational Linguistics ,\n7(0):387–401.\nAlex Wang and Kyunghyun Cho. 2019. BERT has\na mouth, and it must speak: BERT as a Markov\nrandom ﬁeld language model. In Proceedings of\nthe Workshop on Methods for Optimizing and Eval-\nuating Neural Language Generation , pages 30–36,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nKe Wang and Xiaojun Wan. 2018. Sentigan: Gener-\nating sentimental texts via mixture adversarial net-\nworks. In IJCAI, pages 4446–4452.\nZeerak Waseem. 2016. Are you a racist or am i seeing\nthings? annotator inﬂuence on hate speech detection\non twitter. In Proceedings of the ﬁrst workshop on\nNLP and computational social science , pages 138–\n142.\nSam Wiseman, Stuart Shieber, and Alexander Rush.\n2017. Challenges in data-to-document generation.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2253–2263, Copenhagen, Denmark. Association for\nComputational Linguistics.\nJing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason We-\nston, and Emily Dinan. 2020. Recipes for safety in\nopen-domain chatbots. arXiv e-prints, pages arXiv–\n2010.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2020. DIALOGPT : Large-\nscale generative pre-training for conversational re-\nsponse generation. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics: System Demonstrations , pages 270–\n278, Online. Association for Computational Linguis-\ntics.\nWanzheng Zhu and Suma Bhat. 2021. Generate, prune,\nselect: A pipeline for counterspeech generation\nagainst online hate speech. In Findings of the Associ-\nation for Computational Linguistics: ACL-IJCNLP\n2021, pages 134–149.\n3110\nA Appendix\nA.1 Fine-tuning details\nTable 5 summarizes the details of the training of\neach model employed in the ﬁrst session of experi-\nments.\nBA EP PAR LR PER TL ELBART (base) 4 4 139 M 2E-05 24.659 2.358 2.417BERT Seq2Seq (base)4 3 247 M 3E-05 11.209 2.845 3.205T5 (base) 2 3 223 M 5E-05 10.9248 2.412 3.205DialoGPT (medium)4 2 355 M5E-05 6.085 1.4251.806GPT-2 (medium)2 2 355 M5E-05 8.9291.320 2.189\nTable 5: The training details for all the models em-\nployed for the ﬁrst collection of experiment: the batch\nsize (BA), number of training epochs (EP), parameters\n(PAR), the learning rate (LR), perplexity (PER), train-\ning and evaluation loss (TL and EL).\nSince LM sizes are very different for each model\nand since our main focus is not studying perfor-\nmances according to LM dimension growth, as a\nrule-of-thumb, we chose one version smaller than\nthe large version of each model provided that they\nall have the same order of magnitude. This corre-\nsponds to the medium versions for both DialoGPT\nand GPT-2, and base versions for the other models.\nGPT-2 and DialoGPT achieve the lowest perplex-\nity, training and evaluation loss, thus indicating\na slightly more successful ﬁne-tuning, which are\nreﬂected in the evaluations throughout the study.\nWe conducted a hyper-parameter search dur-\ning the training phase of each model using the\nsearch space: learning-rate:{1e −5, 2e −5, 3e −\n5, 4e −5, 5e −5}, warm-up ratio:{0, 0.1}, batch-\nsize:{2, 4}, epochs: {2, 3, 4, 5}. It has been con-\nducted using Optuna, with 10 trials, optimized on\nminimizing the evaluation loss during training.\nA.2 Best models-decoding combination\nHere we discuss the results for the overlap and\ndiversity metrics obtained on the BestLM+D genera-\ntions (Table 6), and those calculated on the human\nevaluation subset (Tables 7 and 8).\nBART. BART performs well with the stochastic\ndecoding methods, in particular: Top p for over-\nlap, diversity, syntactic metrics, and grammatical-\nity; Topk for overlap metrics and toxicity, whereas\nToppk is the best decoding approach on human eval-\nuation and RR, and the second best on ROUGE and\nBLEU-1. On the contrary, BART does not achieve\ngood results with deterministic approaches (i. e.\nBS).\nOverlap Diversity\nROU B-1 B-3 B-4 RR NOV\nBART BS 0.2108 0.2129 0.0486 0.028321.11020.5692\nBART Toppk 0.2331 0.23000.0605 0.036520.26450.5567\nBART Topk 0.23490.23330.06520.038520.6587 0.5575\nBART Topp 0.2329 0.23000.0621 0.037420.5476 0.5586\nBERT BS 0.1735 0.2108 0.0249 0.011338.0349 0.5864\nBERT Toppk 0.20340.23110.0484 0.023123.44170.6098\nBERT Topk 0.20320.23200.0483 0.022922.25460.6129\nBERT Topp 0.20440.23660.05000.024423.64470.6098\nT5 BS 0.2144 0.2007 0.04090.020721.5518 0.5827\nT5 Toppk 0.22360.24540.04660.02287.2996 0.6715\nT5 Topk 0.2076 0.2384 0.0376 0.01365.30020.6922\nT5 Topp 0.2159 0.2390 0.04300.01846.8353 0.6743\nDialoGPT BS0.21920.22720.05280.031221.6800 0.5280\nDialoGPT Toppk 0.21320.2444 0.0437 0.02016.4158 0.6737\nDialoGPT Topk 0.2023 0.2302 0.0320 0.01344.72780.6956\nDialoGPT Topp 0.20930.23970.0385 0.01596.1472 0.6740\nGPT-2 BS 0.21950.21320.05160.031323.0605 0.5402\nGPT-2 Toppk 0.20550.23420.0384 0.01736.5899 0.6832\nGPT-2 Topk 0.1956 0.2271 0.0345 0.01534.76240.7022\nGPT-2 Topp 0.20140.2329 0.0388 0.01776.1944 0.6846\nTable 6: The results computed on the Best M+D gener-\nations (2500 CN for each model-decoding mechanism\ncombination).\nBERT. With BS, BERT achieves the best or sec-\nond best result on all human evaluation metrics,\nexcept for speciﬁcity. For BERT the best decod-\ning is Topp: it is the best performing on overlap\nmetrics and the second best for novelty. It achieves\ngood results both on syntactic metrics and human\nevaluation too.\nT5. For T5, Toppk is the best decoding mecha-\nnism. It records the best results for overlap metrics\nand toxicity, and it has good results on syntactic\nand human evaluation metrics. For what regards\nTopk, it is the best for diversity, while Topp is good\non the syntactic metrics. BS achieves good results\non human evaluation, except for speciﬁcity and\nis-best.\nGPT-2. With Toppk, GPT-2 performs well on\nROUGE, BLEU-1, suitableness, grammaticality,\nand choose-or-not. With Topp, GPT-2 records the\nsecond best result on BLEU scores and diversity\nmetrics. With BS the model has the best perfor-\nmance on overlap metrics (except BLEU-1), and\non suitableness, grammaticality, and choose-or-not,\nbut it has also the worst results on diversity metrics.\nAbove all, Topk is the decoding achieving the best\ncompromise, reaching the best results for the diver-\nsity metrics, and with a superior speciﬁcity score\n(3.15) that is corroborated by the good performance\non the other human evaluation metrics.\nDialoGPT. Topk performs best with diversity\nmetrics and speciﬁcity; it records the second high-\n3111\nToxicity Syntactic metrics\n- ASD MSD NST n\nBART BS 0.4870 3.8919 4.67571.8919 37\nBART Toppk 0.3911 4.3592 4.9483 1.620758\nBART Topk 0.4021 4.3798 5.06561.7377 61\nBART Topp 0.4263 4.5038 5.0909 1.772744\nBERT BS 0.3954 4.5556 5.3750 1.916724\nBERT Toppk 0.4026 5.2299 6.2069 2.137958\nBERT Topk 0.4157 4.8969 6.2969 2.5625 64\nBERT Topp 0.4032 5.1019 6.2963 2.259354\nT5 BS 0.4127 4.4844 4.6562 1.343832\nT5 Toppk 0.3211 4.7754 5.3768 1.7826 69\nT5 Topk 0.3441 4.6767 5.4200 1.7400 50\nT5 Topp 0.3934 4.7245 5.5918 1.8367 49\nDialoGPT BS 0.3635 4.2340 5.1277 1.872347\nDialoGPT Toppk 0.3361 4.7264 5.5094 1.754753\nDialoGPT Topk 0.3482 4.9333 6.1778 2.0000 45\nDialoGPT Topp 0.3274 4.7970 5.5273 1.963655\nGPT-2 BS 0.3540 4.8901 5.3617 1.446847\nGPT-2 Toppk 0.3119 4.2530 5.4182 2.400055\nGPT-2 Topk 0.3416 4.6771 5.8627 2.5686 51\nGPT-2 Topp 0.3659 4.5663 5.7447 2.489447\nTable 7: The results of the toxicity and the syntac-\ntic metrics calculated on the subset employed for the\nhuman evaluation and grouped by each combination\nof model and decoding mechanism. The size of each\ngroup is showed in the column “n”.\nest score on grammaticality. Topp has the second\nbest result on diversity metrics and BLEU scores.\nBS is the best on overlap metrics (except BLEU-1),\nand also on almost all human evaluation metrics: it\nis the worst on speciﬁcity and on diversity metrics.\nToppk is the one working best with DialoGPT, since\nit reaches very good scores with human and overlap\nmetrics, and this does not invalidate diversity, for\nwhich it ranks 3rd out of 4.\nA.3 Examples of the generations\nHate speech: Islam is a violent religion\nthat promotes terror and war.\nDialoGPT - BS : The vast majority of\nMuslims in the world are peaceful and\nfriendly people. I don’t see how you can\nsay such a thing about them.\nBART - Topk: The vast majority of Mus-\nlims are peaceful people who have lived\nin peace and harmony for many years.\nHow can you say this about a religion\nthat promotes terror and war?\nBERT - Top k: islam is a religion of\npeace, and it is not a peaceful religion\nof violence. it is about peace and under-\nstanding of the world’s problems, and\npeace is about bringing peace and peace.\nHuman evaluation\nSUI SPE GRM CHO BEST n\nBART BS 3.7568 2.5270 4.9459 0.8108 0.229737\nBART Toppk 3.79312.6121 4.9483 0.85340.370758\nBART Topk 3.9672 2.54104.90160.8607 0.295161\nBART Topp 3.5682 2.51144.96590.8182 0.147744\nBERT BS 3.52082.52084.79170.7708 0.125024\nBERT Toppk 3.18102.57764.23280.7155 0.112158\nBERT Topk 3.03122.70314.1562 0.6797 0.101664\nBERT Topp 3.03702.71304.12960.74070.157454\nT5 BS 3.57812.28124.84380.76560.078132\nT5 Toppk 2.88412.49284.58700.6667 0.101469\nT5 Topk 2.4600 2.3200 4.6400 0.5600 0.050050\nT5 Topp 2.81632.4388 4.74490.61220.122449\nDialoGPT BS4.15962.60644.98940.85110.308547\nDialoGPT Toppk 3.3679 2.80194.83960.7830 0.273653\nDialoGPT Topk 3.13332.9222 4.85560.7333 0.211145\nDialoGPT Topp 2.9727 2.7000 4.8455 0.7091 0.190955\nGPT-2 BS 4.30852.50004.96810.88300.276647\nGPT-2 Toppk 3.49092.80004.8727 0.8273 0.227355\nGPT-2 Topk 3.03923.14714.8431 0.7255 0.254951\nGPT-2 Topp 3.43623.06384.7872 0.7447 0.329847\nTable 8: For each model-decoding mechanism combi-\nnation, these are the results of the metrics for the human\nevaluations. The size of each combination is showed in\nthe column “n\".\nT5 - Topp: The vast majority of Muslims\nin the UK are peaceful people who share\nmany values and respect.\nGPT-2 - Top p: This is not true, the\nvast majority of Muslims is peaceful and\npeaceful Muslims are happy to live in\nany country they wish. They do not need\nto be a part of a hostile state and they\nshould be treated as equals in society.\nIn these examples we can observe that the CN ob-\ntained with GPT-2, albeit not perfect, is the most\ncomplete and accurate. Those generated with T5\nand DialoGPT are suitable as well. On the other\nhand, the CNs obtained with BART and BERT\npresent some problems: the former has a clear con-\ntradiction in it and the latter is illustrative of some\nof the problems detected in BERT’s generations\nsuch that it is syntactically complex but very vague\nand repetitive, and it ends up not really conveying\na clear message.\nHate speech: The migrant crisis is not\nthe fault of Western Europeans. Migrants\nare to blame for immigration.\nDialoGPT - Topk: What if an entire reli-\ngion was to blame only for the migration\ncrisis? So you think a large number of\nMuslims are responsible for all the prob-\nlems of our world?\n3112\nBART - Top p: The migrant crisis is\ncaused by a lack of integration and lack\nof knowledge of the world.\nBERT - BS: many migrants come here\nto work and provide for their families.\nthey are our brothers and sisters, and we\nshould welcome them.\nT5 - Topk: What evidence do you have\nfor the evidence that migrants are to\nblame? Not all migrants have done any-\nthing wrong.\nGPT-2 - Topk: The EU has put in place\nlaws to combat illegal immigration and\nto prevent crimes committed by migrants.\nThey have a duty to help stop crime and\ndo what is necessary to solve the prob-\nlems within their borders. Migrants are\nvery intelligent and do what they can to\nsupport their families while they study\nand work.\nOnce again, GPT-2 and DialoGPT generations\nwhere chosen as the best CNs by the annotators.\nMoreover, these CNs are all generated via Topk de-\ncoding mechanism. BART and T5 generated CNs\nare acceptable while requiring some post-editing to\nbe employed. Finally, BERT’s output is still very\nvague: we can notice the presence of the recurring\nexpression our brothers and sisters.\nA.4 Additional material for L OTO\nexperiments\nTable 9 displays the distribution of the examples\nwith respect to the targets, in the reference dataset\nand in the conﬁgurations for the LOTO experiments\n(Section 5.3).\nTable 10 presents the detailed results for the nov-\nelty of the reference CNs discussed in Section 5.3,\nwhile the RR for the CNs generated with the LOTO\nmodels and for the reference CNs are shown in\nTable 11. The rankings for these two RR com-\nputations are the same, and the ranges are almost\noverlapping. This means that leaving one target out\ndoes not impact the intra-corpora repetitiveness: in-\nstead, the CNs generated with a LOTO model gain\na lower RR than the reference CNs. For the target\nMUSLIMS a high RR is recorded, both in candidate\nand in the reference CNs. A high repetitiveness in\nthe data for this target can contribute to the good\nresults observed on overlap metrics too (Table 3 in\nTarget Samples in original Samples inLOTO\ndataset experiment\nJEWS 594 600\nLGBT+ 617 600\nMIGRANTS 957 600\nMUSLIMS 1335 600\nWOMEN 662 600\nDISABLED 220 220\nPOC 352 352\nother 266 157\nTotal 5003 3729\nTable 9: The targets coverage in the reference dataset\n(Fanton et al., 2021) and in the LOTO conﬁgurations.\ngenerationJEWS LGBT+ MIGRANTS MUSLIMS WOMEN\ntraining\nJEWS - 0.775 0.780 0.761 0.780\nLGBT+ 0.781 - 0.783 0.765 0.763\nMIGRANTS0.782 0.775 - 0.764 0.777\nMUSLIMS0.775 0.770 0.769 - 0.776\nWOMEN 0.789 0.771 0.783 0.775 -\nTable 10: The novelty of the reference CNs in the data\nfrom Fanton et al. (2021) ( generation) with respect to\nthe training data for the LOTO models (training).\nSection 5.3): it is easier that two outputs are similar\nif they use a limited and repeated number of words.\nTarget RR reference CN RR candidate CN\nJEWS 5.071 4.796\nLGBT+ 4.489 4.620\nMIGRANTS 4.381 4.707\nMUSLIMS 5.244 5.314\nWOMEN 4.547 4.632\nTable 11: The RR computed on the reference CN (per-\ntaining the test set) and on the CN generated with the\nLOTO models.\nA.5 APE Experiment Details\nThe dataset by (Fanton et al., 2021) contains three\nversions of the same CN: the original CN generated\nby a GPT-2 model (CNor), the expert post-edited\nversions obtained during the human-in-the-loop\ncycles (CNpe∗), and the ﬁnal version rechecked by\nNGO experts (CNpe).\nFor ﬁne-tuning our APE model, we have\nthus used the triplets <HS, CNor, CNpe> and\n<HS, CNpe∗, CNpe>. In this way, we managed\nto roughly double the number of the post-edit train-\ning samples, which is highly beneﬁcial for a better\nmodel. When we ﬁltered the triplets with a positive\n3113\nTER score between CNed and CNpe, or CNor and\nCNpe, we obtained 4185 training, 596 test, and 568\nvalidation samples following the partition used in\nthe ﬁrst set of experiments as described in Section\n3.1. Finally, the best ﬁne-tuning conﬁguration of\nthe GPT-2 medium model for APE was obtained\nwith a learning rate of 2e-5 for 3 epochs resulting\nin 3.34 train loss and 1.23 eval loss.\n3114",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8191320300102234
    },
    {
      "name": "Pipeline (software)",
      "score": 0.7676187753677368
    },
    {
      "name": "Decoding methods",
      "score": 0.6726954579353333
    },
    {
      "name": "Task (project management)",
      "score": 0.6483278274536133
    },
    {
      "name": "Artificial intelligence",
      "score": 0.566036581993103
    },
    {
      "name": "Language model",
      "score": 0.5544504523277283
    },
    {
      "name": "Autoregressive model",
      "score": 0.5514590740203857
    },
    {
      "name": "A priori and a posteriori",
      "score": 0.5405769348144531
    },
    {
      "name": "Natural language processing",
      "score": 0.5325348377227783
    },
    {
      "name": "Speech recognition",
      "score": 0.5040311217308044
    },
    {
      "name": "Narrative",
      "score": 0.481779545545578
    },
    {
      "name": "Key (lock)",
      "score": 0.47420576214790344
    },
    {
      "name": "Class (philosophy)",
      "score": 0.4573456943035126
    },
    {
      "name": "Machine learning",
      "score": 0.4092422425746918
    },
    {
      "name": "Linguistics",
      "score": 0.14550775289535522
    },
    {
      "name": "Algorithm",
      "score": 0.11885496973991394
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Econometrics",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ]
}