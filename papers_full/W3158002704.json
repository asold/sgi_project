{
  "title": "Pyramid Medical Transformer for Medical Image Segmentation",
  "url": "https://openalex.org/W3158002704",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2992582702",
      "name": "Zhang Zhuangzhuang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2143786664",
      "name": "Zhang, Weixiong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2884436604",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3130695101",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W3093868689",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3104211200",
    "https://openalex.org/W2947263797",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3132503749",
    "https://openalex.org/W3095236407",
    "https://openalex.org/W3119087862",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964150021",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2998108143",
    "https://openalex.org/W3087181606",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W86219213",
    "https://openalex.org/W2798122215",
    "https://openalex.org/W2907750714",
    "https://openalex.org/W2288892845",
    "https://openalex.org/W2962862396",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2752238301",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2964227007",
    "https://openalex.org/W2978708129",
    "https://openalex.org/W3102998284",
    "https://openalex.org/W2592905743",
    "https://openalex.org/W3042125681",
    "https://openalex.org/W2147484997"
  ],
  "abstract": "Deep neural networks have been a prevailing technique in the field of medical image processing. However, the most popular convolutional neural networks (CNNs) based methods for medical image segmentation are imperfect because they model long-range dependencies by stacking layers or enlarging filters. Transformers and the self-attention mechanism are recently proposed to effectively learn long-range dependencies by modeling all pairs of word-to-word attention regardless of their positions. The idea has also been extended to the computer vision field by creating and treating image patches as embeddings. Considering the computation complexity for whole image self-attention, current transformer-based models settle for a rigid partitioning scheme that potentially loses informative relations. Besides, current medical transformers model global context on full resolution images, leading to unnecessary computation costs. To address these issues, we developed a novel method to integrate multi-scale attention and CNN feature extraction using a pyramidal network architecture, namely Pyramid Medical Transformer (PMTrans). The PMTrans captured multi-range relations by working on multi-resolution images. An adaptive partitioning scheme was implemented to retain informative relations and to access different receptive fields efficiently. Experimental results on three medical image datasets (gland segmentation, MoNuSeg, and HECKTOR datasets) showed that PMTrans outperformed the latest CNN-based and transformer-based models for medical image segmentation.",
  "full_text": "Pyramid Medical Transformer for Medical Image Segmentation \nZhuangzhuang Zhang, Weixiong Zhang \nWashington University in St. Louis, MO, USA \nAbstract: Deep neural networks have been a prevailing technique in the field of \nmedical image processing. However, the most popular convolutional neural \nnetworks (CNNs) based methods for medical image segmentation are imperfect \nbecause they model long-range dependencies by stacking layers or enlarging filters. \nTransformers and the self-attention mechanism are recently proposed to effectively \nlearn long -range dependencies by modeling all pairs of word -to-word attention \nregardless of their positions. The idea has also been extended to the computer vision \nfield by creating and treating image patches as embeddings. Considering the \ncomputation complexity for whole image self-attention, current transformer-based \nmodels settle for a rigid partitioning scheme that potentially loses informative \nrelations. Besides, current medical transformers model global context on full \nresolution images, leading to unnecessary computation costs. To address these \nissues, we developed a novel method to integrate multi -scale attention and CNN \nfeature extraction using a pyramidal network architecture, namely Pyramid Medical \nTransformer (PMTrans). The PMTrans captured multi -range relations by working \non multi-resolution images. An adaptive partitioning scheme was implemented to \nretain informative r elations and to access different receptive fields efficiently. \nExperimental results on three medical image datasets  (gland segmentation , \nMoNuSeg, and HECKTOR datasets) showed that PMTrans outperformed the latest \nCNN-based and transformer-based models for medical image segmentation. \n1    Introduction \nAccurate and robust medical image segmentation is essential for computer -aided diagnosis, \ntreatment planning, and image -guided surgery [1]. In radiotherapy treatment planning, accurate \ndelineation of tumors can maximize target coverage while minimizing toxicities to the surrounding \norgans at risk (OARs)  [2]. Manual contouring in clinical practice is labor -intensive and time -\nconsuming, thus automated and reliable medical image segmentation has been a research focus for \nmany years. \nAccurate segmentation can be achieved by apprehending both global context and local details. \nMedical image segmentation can be formulated as a problem with three objectives: 1) determining \nthe existence, 2) identifying the shape and rough areas, and 3) refining the boundaries of the tumors \nor organs of interest. These three objectives require different, from low to high, levels of pixel -\nwise details on the image. Object localization does not require pixel -wise details of the original \nimage [3], and it is inefficient to  compute global attention [1] across the whole image. However, \ncurrent medical transformers do not explicitly model global context [4, 5] or model global context \non full resolution images, bearing unnecessarily high computation costs [1]. \nMoreover, organs, tumors, and other objects to segment in medical images normally have irregular \nshapes and different sizes [6]. The current medical transformers do not robustly cater to objects at \nmultiple scales [7] because they arbitrarily divide images. For example, TransFuse [5] uses patch-\nwise attention, and the patches predetermine fixed receptive fields on the original image. Similarly, \nMedical Transformer [1] divides an image into 16 pieces for processing. It computes pixel-wise \naxial attention [1] within a piece, and each piece also has a fixed receptive field on the image ; \ndoing so may even separate the same object into different pieces, making segmentation difficult. \nFurthermore, arbitrarily dividing images does not consider objects of different shapes and sizes. \nIntuitively, the position and size of an object inherently determine if it occupies the whole patch \n(large object), is contained in one patch (small object), or is divided into several patches (an object \non cutting boundaries). Neighboring pixels often provide vital information of local details  [7], \nwhereas such a dividing scheme may destroy these relations, adversely affecting segmentation \noutcomes. \nIn short, while they have been demonstrated to outperform convolutional neural networks (CNNs) \nbased methods, the existing transformer -based methods for medical image segmentation are far \nfrom reaching their maximal potential. The primary culprit of their inefficacy is the rigid \npartitioning an image into patches with fixed sizes, which is doomed to fail to grasp associations \nof diverse and unknown scales within the image. \nWe propose a novel, pyramid medical transformer (PMTrans) approach that explores and \nincorporates multi-resolution attention. To our knowledge, this is the first multi -scale and multi-\nresolution medial image transformer, which is not only designed for medical images but also \napplicable to general images such as scenery pictures. \nThe new method has three prominent features. Firstly, the new model uses a pyramid architecture \nwith four branches: three transformer branches and one CNN branch. The three transformers work \non three different image scales to capture different ranges of correlations within the image, i.e., \nshort-range, mid-range, and long-range associations. We use down-sampled images (half and 1/4 \nsizes of the original) to capture mid -range and long-range relations and use the original image to \ncapture local, finer details. This pyramid design not only improves the efficiency of global context \nmodeling but also accommodates objects with diverse sizes and shapes robustly. \nSecondly, we abandon the existing partitioning schemes [1, 4, 5, 8] and adopt gated axial attention \nand CNN as building blocks. We introduce local gated axial attention layers [1] in the short-range \ntransformer. We only compute gated axial attention within a certain span , which is much shorter \nthan the height or width of the original image . This local gated axial attention  module does not \nconcern global attention because pixels only attend to their neighbors. We focus on local finer \ndetails in this short -range transformer, and the global context is ignored for efficiency. We use \nglobal gated axial attention layers on downsampled input images in the mid-range and long-range \nbranches to capture global attention eff ectively. Since global axial attention may be relatively \ncostly to compute, we apply it to down -sampled images, which means we cover larger receptive \nfields on the original image at low costs.  \nThirdly, we design and implement a new fusion and label guidan ce scheme to integrate feature \nmaps of different scales from the three multi-scale transformers and image-specific features from \nthe CNN branch. We preserve low -level context without deep C NNs and retain the feature \nextraction characteristics of CNN and tr ansformer models [5]. As for label guidance, we inject \ndeep supervision [2] into different feature map scales in the training process, which stabilizes the \ntraining and alleviates a potential gradient vanishing problem [9]. \nWe tested the performance of the proposed PMTrans model on three datasets: gland segmentation \n(microscopic) dataset [10], MoNuSeg (microscopic)  dataset [11], and HECKTOR (PET/CT) \ndataset [12]. The results showed that the new model outperformed the latest CNN -based and \ntransformer-based models. \n2    Related Work \nBefore deep learning was introduced to the medical field , most automated medical image \nsegmentation models are atlas -based [13, 14], statistical-based [15], and shape -based [16]. With \nthe great successes of CNNs, particularly on images, many CNN -based medical image \nsegmentation models have been developed. The most popular is the encoder-decoder architecture \nproposed in the U-net [17], subsequently improved and extended. Some architectures replace the \nvanilla stacking convolutional layers with other backbones, resulting in, e.g., residual U -net [18] \nand dense U-net [19] as well as their implementation innovations, suc h as  U-net++ [20], V-net \n[21], Y-net [22]. These CNN-based methods work on various modalities, target locations, and \nsegmented targets. They not only perform well on a wide range of images but also expand the \ncapacity of convolutional neural networks. \nAlthough most latest computer vision models use CNNs as building blocks, the drawback of CNNs \nis also noticeable and profound.  Convolutional layers are not designed to capture long -range \ndependencies because they aggregate local information within their filter regions across one layer \nto the next; capturing long-range associations requires deep networks or very large filters, resulting \nin models with a large number of parameters that are difficult and costly to train. A few approaches \nhave been proposed to address this issue: 1) using the atrous convolution [7] so that the receptive \nfields enlarge without increasing the number of parameters; 2) incorporating statistical modeling \nlike Markov Random Field to support global dependencies [6]; 3) using a feature pyramid structure \nto deal with multiple image resolutions [23].  \nTransformer [24] is a ground-breaking deep-learning technique for self-attention. It makes the self-\nattention mechanism practically feasible at a global scale so that long -range dependencies can be \nlearned efficiently. Transformer-based models revolutionize the natural language processing (NLP) \nfield, reminiscent of CNNs revolutionizing the field of computer vision (CV). Effective \ntransformer models like BERT [25] and GPT [26] outperform the previous sequence-to-sequence \nNLP models in many tasks like machine translation and text generation. \nThe idea of the transformer has been attempted in developing attention -based CV models. One \nmajor difficulty of adopting transformers for CV tasks is the high computational complexity and \ncost for modeling and learning global attention. In NLP tasks, sente nces are normally less than \n500 words, which is manageable for computing all pairwise  attention of the words in a sentence.  \nHowever, for an ğ» by ğ‘Š image, ğ»ğ‘Š pixels give rise to a quadratic order of (i.e.,  ğ»2ğ‘Š2) pairs of \nattention weights, e.g., more than 68 billion pairs of attention for a 512 by 512 image. This direct \nadaptation of the idea for images is not scalable.  To address this problem, Vision Transformer \n(ViT) [24] partitions a 2D image into patches and then converts the patches with a positional \nencoding to a 1D sequence to feed into transformer blocks. Instead of computing pixel -to-pixel \nattention, it computes patch-to-patch attention. For an ğ» by ğ‘Š image with a ğ‘ Ã— ğ‘ patch size, this \nscheme reduces to  ğ»ğ‘Š/ğ‘2 patches and ğ»2ğ‘Š2/ğ‘4 pairs of attention weights . i.e., roughly one \nmillion pairs of attention for a 512 by 512 image  with a patch size of 16 by 16 . Segmentation \nTransformer [8] replaces the encoder in a CNN -based method with a  transformer with superior \nperformance for end-to-end segmentation tasks. \nBeyond using patches to reduce the number of parameters of self -attention, approximation \nalternatives have been considered to avoid the quadratic computational cost. Learning global self-\nattention amounts to computing all pairwise relations. Nevertheless, full attention is not the only \nsolution. Axial-Deeplab [27] introduces an approximation method that substitutes vanilla global \nattention with axial attention by computing the vertical and horizontal axial attention, which \nreduces the complexity from  ğ‘‚(ğ»2ğ‘Š2) to ğ‘‚(ğ»ğ‘Šğ‘š) where m is the axial span of the attentio n \noperation. Sparse transformer [28] extends this approximation method and generates long \nsequences to reduce the complexity from ğ‘‚(ğ»2ğ‘Š2) to ğ‘‚(ğ»ğ‘Šâˆšğ»ğ‘Š). \nBesides high computation cost, the second difficulty that vision transformers encounter is that they \nrequire colossal datasets for pre-training. Randomly initiated model weights let each patch attend \nto any other patch randomly at the start of the training, so a large amount of training samples is \nneeded to learn where to â€œlookâ€ and aggregate information to pass onto the next layer. Compared \nto CNN-based models, where filters slide through the image stride by stride, the self -attention \nmechanism offers more liberty to attend yet facing  higher learning costs. Data -efficient \nTransformer (DeiT) [29] addresses this issue with a distillation mechanism, making the model \ntrainable on mid-sized datasets. \nIn the medical image segmentation field, transformer -based methods have not yet been fully \nexplored. High -performance transformers for medical images face two bottlenecks: the \ncomputational cost and lack of sufficient training data. In clinical practice, high computational cost \nposts a burden on the hardware requirements and increases the patientâ€™s wait time which may not \nbe acceptable. Moreover, medical datasets are typically much smaller than daily images. ViT [24] \nuses more than 300M reg ular images to pre -train the model. Nevertheless, none of the currently \navailable medical datasets has a sufficient amount of training images because medical data \ncollection and annotation demand medical expertise [2].  \nThe current medical image segmentation transformers fall into two categories. The first integrates \nthe attention mechanism into their CNN models as a performance-boosting component, as done in \nAttention U-net [30] and volumetric attention model [31].  The second type of model is mainly \nbuilt with transformer layers and uses CNNs as feature extraction blocks. For example, TransUnet \n[4] inserts attention layers at the end of the CNN-based feature extraction encoder, and the model \nrelies on the pre-trained weights of ViT. Medical Transformer[1] uses gated axial attention and a \nlocal-global (LoGo) strategy, where the gated axial attention module is to address the lack-of-data \nissue, and the LoGo strategy reduces the computation cost of global self -attention. TransFuse [5] \ncombines feature maps from a C NN and a transformer.  However, the rigid patching scheme \nhinders the performance of all previous medical transformers, but without it they would suffer \nfrom drastically increased computation cost. Our approach effectively avoids the patching strategy, \nand wisely applies small axial attention window at multiple scales, capturing dependencies of all \nranges efficiently.  \n3    Pyramid Medical Transformer (PMTrans) \n3.1   Overview \nOur approach to capturing both global and local relations, with a manageable computation cost, is \nto build a pyramid deep -learning architecture, which we call the pyramid medical transformer \n(PMTrans, Fig. 1). The new method has four branches: three transformer branches are built with \ngated axial attention blocks [1], and the fourth branch with CNN blocks.  \n \nFig. 1 Structure Overview . The pyramid medical transformer has three transformer branches (short, mid, \nand long-range branches) and a CNN branch. Input images are rescaled to Â½ and Â¼ for mid-range and long-\nrange branches, respectively. Feature maps from the transformer branches (in purple) are fused with those \nfeature maps from the CNN branches (in green) at different scales. Deep supervisions are injected at Â½ and \nÂ¼ scales during the training phase.  \nThe three transformer branches  (Fig. 1)  are designed to capture short-range, mid-range, and long-\nrange associations among the pixels of an image. They have similar structures but take three \ndifferent scales of the image ( ğ»ğ‘’ğ‘–ğ‘”â„ğ‘¡ Ã— ğ‘Šğ‘–ğ‘‘ğ‘¡â„ Ã— ğ¶â„ğ‘ğ‘›ğ‘›ğ‘’ğ‘™) as input, which are ğ» Ã— ğ‘Š Ã— ğ¶, \nğ»\n2 Ã—\nğ‘Š\n2 Ã— ğ¶, and \nğ»\n4 Ã—\nğ‘Š\n4 Ã— ğ¶, respectively. They all have an initial convolution block for basic \nfeature extraction at the beginning. This convolution block has three convolution layers followed \nby batch normalization and the ReLU function [1]. They then have an encoder -decoder-style \ntransformer block to compute self-attention among the pixels. These transformer blocks are built \nwith gated axial attention layers [1] and have skip connections between the encoder and decoder \npaths. The short-range transformer block has a depth of five, the mid-range block of four, and the \nlong-range block of three accommodating the different sizes of their input feat ure maps. The \n\noutputs of these three branches have the same height and width as their inputs and are subsequently \nintegrated with the feature maps from the CNN branch. The combined results are then up-sampled \nto restore the original resolution. Using multi -scale input images improves the segmentation of \nobjects with different sizes and shapes. \nWe design the fourth branch with CNN-based residual blocks (Fig. 1) for extracting features in the \ninput image. Comparing with CNN-based models, we only use a shallow network for the task. By \nfusing the feature maps from the transformers and CNN, our model preserves low -level context \nand global relations. Deep supervision [9] is introduced to the two down -sampled feature maps. \nWe build two auxiliary classification layers at the two down -sampled resolutions and use the \ndown-sampled ground -truth label maps correspondingly to stabilize the training process and \nalleviate the potential gradient vanishing problem [9]. \n3.2   Gated axial attention \nIn the three transformer branches, we use the gated axial attention, an extension to axial attention \n[1]. Axial attention computes approximate self-attention. Consider an input image or a feature map \nX. The self-attention Y of X is computed as \nğ‘„ = ğ‘Šğ‘ğ‘‹,     ğ¾ = ğ‘Šğ‘˜ğ‘‹,    ğ‘‰ = ğ‘Šğ‘£ğ‘‹,                                                   (1) \nğ‘Œ = ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(\nğ‘„ğ¾ğ‘‡\nâˆšğ‘‘ğ‘˜\n)ğ‘‰,                                                            (2) \nwhere Q, K, and V are query, key, and value matrices computed by weight ğ‘Šğ‘ , ğ‘Šğ‘˜ , and ğ‘Šğ‘£ , \nrespectively; ğ‘‘ğ‘˜ is the number of dimensions of the keys and queries [32]. Note that dividing ğ‘„ğ¾ğ‘‡ \nby âˆšğ‘‘ğ‘˜ is to counteract the effect of having small gradients due to large dot product s because of \nhigh-dimensional keys and queries [32]. Assume that we have a feature map ğ‘‹ âˆˆ ğ‘…ğ»Ã—ğ‘ŠÃ—ğ¶ğ‘–ğ‘› , where \nğ», ğ‘Š, and ğ¶ğ‘–ğ‘› represent the height, width, and number of channels, respectively. There are ğ» Ã— ğ‘Š \ntokens and each of which has ğ¶ğ‘–ğ‘› dimensions. We need to compute the attention between each \nsuper-pixel and all tokens on the feature map, i.e., a total of ğ»2ğ‘Š2 pairs of attention to compute. \nSelf-attention is effective for capturing relations of any distance. All pairs of attention have equal \nopportunities to be selected by the model regardless of how far two tokens are apart from each \nother. However, an obvious drawback of self-attention is the overall computational cost. \nAs an approximation, axial attention [33] applies self-attention computation along the height and \nwidth axes of the image. It first computes the attention between each pixel and all pixels in the \nsame column (i.e., along the height) and then the attention between each pixel and all pixels in the \nsame row (i.e., along the width). Note that the attention along the height axis aggregate information \nin the same column, and subsequently, the attention along the width axis aggregates the combined \ninformation in the same row.  When each pixel attends to all pixels in the same row, it also \nindirectly or approximately attends to all other pixels in the feature map. Doing so reduces the \ncomputation from ğ»ğ‘Š Ã— ğ»ğ‘Š to ğ»ğ‘Š Ã— (ğ» + ğ‘Š). For an input feature map ğ‘‹ âˆˆ ğ‘…ğ»Ã—ğ‘ŠÃ—ğ¶ğ‘–ğ‘› , the \nupdated self-attention mechanism [1] with positional encodings along the width axis is: \nğ‘¦ğ‘–ğ‘— = âˆ‘ ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘ğ‘–ğ‘—\nğ‘‡ ğ‘˜ğ‘–ğ‘¤ + ğ‘ğ‘–ğ‘—\nğ‘‡ ğ‘Ÿğ‘–ğ‘¤\nğ‘ + ğ‘˜ğ‘–ğ‘¤\nğ‘‡ ğ‘Ÿğ‘–ğ‘¤\nğ‘˜ )(ğ‘£ğ‘–ğ‘¤ + ğ‘Ÿğ‘–ğ‘¤\nğ‘£ )ğ‘Š\nğ‘¤=1 ,                              (3) \nwhere ğ‘– and ğ‘— are the pixel position s along the width and height axes; and ğ‘Ÿğ‘–ğ‘¤\nğ‘ , ğ‘Ÿğ‘–ğ‘¤\nğ‘˜ , and ğ‘Ÿğ‘–ğ‘¤\nğ‘£  are \nrelative position encodings regarding queries, keys, and values, respectively. Note that the relative \nposition embedding is also along the width axis. The axial attention along the height axis is similar.  \nGated axial attention [1] adds weights (i.e., gates) to t he relative position encodings in Eqn. (3). \nThese gates would reduce the effect of these encodings if the model cannot learn accurate position \nencodings with small medical datasets. The gated axial attention computes \nğ‘¦ğ‘–ğ‘— = âˆ‘ ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘ğ‘–ğ‘—\nğ‘‡ ğ‘˜ğ‘–ğ‘¤ + ğºğ‘„ğ‘ğ‘–ğ‘—\nğ‘‡ ğ‘Ÿğ‘–ğ‘¤\nğ‘ + ğºğ¾ğ‘˜ğ‘–ğ‘¤\nğ‘‡ ğ‘Ÿğ‘–ğ‘¤\nğ‘˜ )(ğºğ‘‰1ğ‘£ğ‘–ğ‘¤ + ğºğ‘‰2ğ‘Ÿğ‘–ğ‘¤\nğ‘£ )ğ‘Š\nğ‘¤=1 ,                 (4) \nwhere ğºğ‘„ , ğºğ¾ , ğºğ‘‰1 , and ğºğ‘‰2  are gates that control the influence of position encoding s for the \nqueries, keys, and values [1], respectively.  \n \nFig. 2  Encoder block structure. Shown here is global axial attention with a span that covers the whole axis.  \nOur model builds the encoders of the transformers with gated axial attention layers (Figure 2). \nEach of the encoder blocks has a  1 Ã— 1 convolution layer, two mult i-head axial attention layers \n(one each along the height and width axes), and one 1Ã— 1 convolution layer  (Fig. 2). Residual  \nconnection is also included in encoder blocks. A decoder block consists of a series of one \nconvolution layer, one bilinear up-sampling layer, and one ReLU layer. \n \nFig. 3 Pyramid Structure of the Transformer Branches. The axial attention span is the same across the three \nbranches, but they have different receptive fields on the input image. \nThe input to the short -range branch is the original image (ğ» Ã— ğ‘Š Ã— ğ¶). It uses local gated axial \nattention blocks to model local context. We set the span of the gated axial attention to  \nğ»\n4 and \nğ‘Š\n4 . \nEach pixel attends to a subregion (\nğ»\n4 Ã—\nğ‘Š\n4 ) centered on itself (Fig. 3) . We avoid the trivial \n\npartitioning scheme that would potentially cut off valuable relations.  Compared to the patching \nsolution proposed by MedTrans [1], which partitions the original image into \nğ»\n4 Ã—\nğ‘Š\n4  patches, the \ngated axial attention that we use has the same complexity, yet our partitioning scheme is adaptive. \nMedTrans computes global gated axial attention on \nğ»\n4 Ã—\nğ‘Š\n4  patches, and each pixel attends to \nğ»\n4 +\nğ‘Š\n4  other pixels, giving rise to a total of (\nğ»\n4 +\nğ‘Š\n4 ) Ã— ğ»ğ‘Š pairs of attention. In our model, we have \nan adaptive partitioning scheme in which each pixel attend to \nğ»\n4 +\nğ‘Š\n4  other pixels. The receptive \nfield of gated axial attention is \nğ»\n4 Ã—\nğ‘Š\n4  on the original image. The transformer block in this branch \nhas an encoder -decoder architecture, with five attention layers in the encoder and five in the \ndecoder. Skip connections are implemented to enhance the gradient flow.  \nThe input to the mid -range branch is a down -sampled version of the original to \nğ»\n2 Ã—\nğ‘Š\n2 Ã— ğ¶. We \nstill set the span of the gated axial attention to \nğ»\n4 and \nğ‘Š\n4 . However, the receptive field of the gated \naxial attention is \nğ»\n2 Ã—\nğ‘Š\n2  on the original image, which means that we can model longer yet still not \nglobal relations using this branch. Each pixel attends to \nğ»\n4 +\nğ‘Š\n4  pixels, and there are \n(\nğ»\n4 +\nğ‘Š\n4 ) Ã— ğ»ğ‘Š/4  pairs of  attention in total . We stack four layers in the encoder -decoder \narchitecture of this branch.  \nFor the long-range branch, the input is further down-sampled from the original image to \nğ»\n4 Ã—\nğ‘Š\n4 Ã— ğ¶. \nUsing the same span (\nğ»\n4 and \nğ‘Š\n4 ) as in the mid -range branch, we can now efficiently model global \nattention. The receptive field of this gated axial attention is ğ» Ã— ğ‘Š on the original image, but we \nonly need to compute (\nğ»\n4 +\nğ‘Š\n4 ) Ã— ğ»ğ‘Š/16 pairs of attention. Compared to the global branch of \nMedTrans [1], which involves (ğ» + ğ‘Š) Ã— ğ»ğ‘Š pairs of attention, this long-range branch only has \n1/216 of the attention pairs used in MedTrans. We stack three layers in this encoder -decoder \narchitecture.  \nIn summary, the three transformer branches of the PMTrans model have different receptive fields \non the input image, but all use fixed -range gated axial attention. With this pyrami d architecture, \nwe model relations at three different resolutions and ranges efficiently. This design not only \neffectively caters to objects at different scales but also computes global attention without \ncomputing full resolution self-attention. Moreover, we avoid arbitrarily partitioning the image into \npatches. Instead, we preserve a pixel's local relations by computing axial self -attention within a \npatch centered on itself. \n3.3   Fusion and deep supervision \nWe introduce the fourth, CNN branch to the model to extract features in the input image (Fig. 1). \nThis branch is built with residual convolution blocks, stacking convolution layers with skip \nconnections for gradient flow enhancement [34]. Feature maps from the CNN branch are fu sed \ninto those from the transformer branches at different scales (Fig. 1), and the pyramid fusion scheme \nis built with attention gates proposed by the attention U-net [30]. Instead of simply concatenating \nfeature maps and let convolution layers learn to fuse them, we add weights to integrate them \nconsciously with attention -aware learnable parameters at different scales. Using these attention \ngates at multiple scales enables the model to extract relevant information from both transformers \nand CNNs, exploiting the feature extraction and information aggregation capabilities of both \ntechniques [35]. We introduce deep supervision by adding auxiliary classifiers after the pyramid \nfusion of feature maps. Label guidances at different scales are provided at these three branches \nduring training, and only the full resolution output is used for testing and evaluation. These dee p \nsupervisions enforce intermediate feature maps to be semantically discriminative at each scale [36].  \n4    Experiments and Results \n4.1   Datasets \nWe used three medical image datasets to evaluate the model: gland segmentation (GLAS) [10], \nMoNuSeg [11], and HECKTOR dataset [12].  \nâ€¢ The GLAS dataset contains 85 microscopic images for training and 80 for testing , each \nwith a resolution of 128 by 128 [10].   \nâ€¢ The MoNuSeg dataset has 30 microscopic images for training and 14 images for testing, \neach with a resolution of 1000 by 1000 [11].  \nâ€¢ The HECKTOR dataset contains 201 training cases and 53 testing cases, each of which has \n144 Ã— 144 Ã— 144 CT/PET volumes [12]. We stacked and fused the CT and PET slices to \nfeed into the network. \nWe set the training/validation split for 65/20 , 25/5, and 180/2 1 for GLAS, MoNuSeg, and \nHECKTOR, respectively . We applied random shifting, rotation, and flipping for data \naugmentation. We performed cross-validation on both datasets to select the best model weights. \nFig. 4 Segmentation Comparison among Residual U -net, Medical Transformer, and Pyramid Medical \nTransformer. We mark focal regions where models have distinguishab le predictions. We cropped 128 by \n128 regions from the MoNuSeg and HECKTOR images to better present segmentation details. \n \nInput Residual U-net \n[34]  \nMedical \nTransformer [1] \nPyramid \nMedical \nTransformer \n(ours) \nGround Truth \nGlas      \n     \n\nMoNu\nSeg \n     \n     \nHECK-\nTOR \n     \n     \n \n4.2   Training Details \nThe Pyramid Medical Transformer was implemented with Pytorch  and trained on an RTX 3090 \nGPU. We set the batch size to 2 and the learning rate to 0.001 with a polynomial learning rate \nscheduler. The model was trained using Adam optimizer [37], and the training process finished \nwithin 400-500 epochs. The binary cross-entropy loss was defined as \nâ„“ğ‘ğ‘ğ‘’(ğ’›Ì‚, ğ’›) = âˆ’ âˆ‘ [ğ’›ğ’Š ğ‘™ğ‘›ğ’›Ì‚ğ‘–  +  (1 âˆ’ ğ’›ğ‘–)ğ‘™ğ‘›(1 âˆ’ ğ’›Ì‚ğ‘–)]ğ»Ã—ğ‘Š\nğ‘–=1 ,                                (5) \nwhere ğ’›Ì‚ is the prediction label map and ğ’› is the ground truth label map. As suggested by [1], we \ndid not activate the gates' training in the first 10 epochs. We used the binary cross-entropy loss for \ntraining and evaluated the final models by Dice Similarity Coefficient (DSC), defined as \nğ·ğ‘†ğ¶ =  \n2|ğ‘‡ğ‘Ÿğ‘¢ğ‘’âˆ©ğ‘ƒğ‘Ÿğ‘’ğ‘‘|\n|ğ‘‡ğ‘Ÿğ‘¢ğ‘’|+|ğ‘ƒğ‘Ÿğ‘’ğ‘‘|,                                                               (6) \nwhere |ğ‘‡ğ‘Ÿğ‘¢ğ‘’| and |ğ‘ƒğ‘Ÿğ‘’ğ‘‘| stand for the numbers of pixels in the ground-truth and predicted masks, \nand |ğ‘‡ğ‘Ÿğ‘¢ğ‘’ âˆ© ğ‘ƒğ‘Ÿğ‘’ğ‘‘| is the number of pixels in the overlapping region [6].  \n4.3    Results \nWe carried out a quantitative comparison of PMTrans with several state-of-the-art medical image \nsegmentation methods, including both CNN-based methods (FCN [38], U-net [17], U-net++ [20], \nand Res -Unet [34]) and attention -based methods (Axial Attention U -net [27] and Medical \n\nTransformer [1]). The results are shown in Table 1. Note that all results on the HECKTOR dataset \nare based on the validation set because we do not have access to the ground truth of the test set.  \nOur PMTrans method outperformed all six baseline methods on both datasets (Table 1). PMTrans \noutperformed the best convolutio n-based baseline by 3.36% on the GLAS dataset , 0.75% on the \nMoNuSeg dataset, and 1.41% on the HECKTOR dataset. PMTrans surpassed the best transformer-\nbased baseline method (MedT rans) by 0.57% , 0.68%, and 2.21% on the GLAS , MoNuSeg, and \nHECKTOR datasets, respectively. We also visually compared the segmentation outcomes of these \nmethods using the test images. Figure 4 exhibits some of the results from Residual U -net [34], \nMedical Transformer [1], and PMTrans . We chose example slices with objects of various sizes \nand shapes to demonstrate the model performances. Our model catered to (was particularly suitable \nfor) objects at different scales and performed well on objects with various shapes and sizes; the \ncontours from PMTrans closely resembled the ground truth. \nModels GLAS [10] MoNuSeg \n[11] \nHECKTOR[12] \nFCN [38] 0.64Â±0.06 \n<0.01* \n0.32Â±0.06 \n<0.01* \n0.75Â±0.10 \n<0.01* \nU-net [17] 0.75Â±0.08 \n<0.01* \n0.79Â±0.09 \n0.05* \n0.75Â±0.07 \n<0.01* \nU-net++ [20] 0.77Â±0.07 \n<0.01* \n0.79Â±0.09 \n0.04* \n0.79Â±0.05 \n<0.01* \nResidual U-net [34] 0.78Â±0.08 \n0.01* \n0.78Â±0.10 \n<0.01* \n0.78Â±0.08 \n<0.01* \nAxial Attention U-net [27] 0.75Â±0.08 \n<0.01* \n0.77Â±0.10 \n<0.01* \n0.77Â±0.07 \n<0.01* \nMedical Transformer [1] 0.80Â±0.05 \n0.11 \n0.79Â±0.08 \n0.04* \n0.78Â±0.08 \n<0.01* \nPyramid Medical Transformer \n(ours) \n0.81Â±0.05 \n- \n0.80Â±0.07 \n- \n0.80Â±0.06 \n- \nTable 1. Model comparison of our Pyramid Medical Transformer against other CNN -based and full \nattention-based methods. We compare the Dice Similarity Coefficient in percentage for both datasets across \nseven models. The second row for each method reports the p-value of comparing to our proposed method, \nin which the number of images are 80, 896, and 5904 for GLAS, MoNuSeg, and HECKTOR, respectively. \nWe studied the effects of the new network structure and an adaptive partitioning mechanism \n(Figure 5). We evaluated the performance of combining only one of the short, mid, and long-range \nattention with  the image features extracted by a CNN branch. For comparison, we tested the \npartitioning scheme used by MedTrans [1], which means we partition the feature maps into patches \nand perform â€œglobalâ€ axial attention on H/4Ã—W/4 patches. We kept the rest structure of the model \nand training parameters. The major difference between the partitioning scheme and our new \nmethod is that each pixel attends to a patch centered on itself instead of a pre-cut patch. \nThe results showed that on the GLAS and HECKTOR dataset, shorter-range branches have lower \nperformance than long-range branches, and the PMTrans model that comprises three branches has \nthe best performance. The difference between the short-range branch and the long-range branch is \n7.52%. This effect can also be seen on the MoNuSeg dataset, but it is less obvious. The difference \nbetween the short-range branch and the long-range branch is 1.44%. The effectiveness of the non-\ntrivial partitioning scheme varies due to the different sizes of the segmentation object from the \nthree datasets. The gland segmentation targets are considerably larger than the n ucleus, and it is \nmore likely that a \nğ»\n4 Ã—\nğ‘Š\n4  patch cannot contain one object, which means the model needs to learn \ndependencies across longer ranges. However, for the MoNuSeg dataset, each nucleus is much \nsmaller than a patch, so the difference between shorter-range land longer-range branches is not as \nsignificant. Moreover, we showed that the adaptive partitioning scheme improved the model \nperformance on both datasets.  \n \nFig.5 Ablation Study of Pyramid Structure and Adaptive Partitioning Mechanism \nCNN-branch is integrated into the model to exploit the feature extraction power of convolutional \nlayers. We tested three different CNN structures as well as removing the CNN branch (Table 3). \nThe three different CNN backbones we tested are vanilla CNN [17], residual convolution blocks \n[34], and densely connected convolution blocks [19].  \nThe results (Figure 6) showed that different CNN backbones have similar performance, and they \nall outperformed the model without a CNN branch. We performed this ablation study to show that \nthe CNN branch boosts the model performance by adding feature extraction power of convolution \nlayers. We chose to use residual connection backbones for the Pyramid Medical Transformer, \nconsidering the computational cost.  \n72 73 74 75 76 77 78 79 80 81 82\nGLAS\nMoNuSeg\nHECKTOR\nPMTrans PMTrans with Trivial Partitioning\nLong-range Branch Only Mid-range Branch Only\nShort-range Branch Only\n \nFig.6 Ablation Study of Different CNN Branch \n5    Discussion \nSemantic segmentation is an essential task in medical imaging applications; accurate delineation \nof tumors enhances the outcome of diagnosis, treatment planning, and image -guided surgery. \nWhile CNN-based models capture long-range dependencies by inefficiently stacking convolution \nlayers, attention -based models explicitly build relations of all ranges. However, assigning \nlearnable parameters to all pairs of global relations is costly; thus, efficient approximations of \nglobal self-attention have been studied to circumvent the inherent quadratic complexity. The most \npopular technique is to partition the whole image into patches, which are subsequently used to \ncompute (incomplete) global attention across patches  [24] or pixel-wise global attention within \npatches [1]. This arbitrary partitioning mechanism imposes serious problems on existing medical \nsegmentation methods since it cannot capture all attention of different scales and ranges.  When  \nlarge objects are partitioned into different patches, these fragments cannot effectively attend to \neach other. In this paper, we designed and developed a pyramid deep -learning architecture to \naddress this critical issue.  \nThe proposed method, Pyramid Medical Transformer (PMTrans), implements a multi-scale model \narchitecture and exploits the feature extraction power of both self-attention and convolution layers. \nThe new model uses multiple input resolutions to comprehend relations of different ranges without \nchanging the overall complexity of self-attention computation. It learns longer-range dependencies \non lower resolution images and refined segmentation details on full resolution images.  \nPMTrans also implements an adaptive partitioning mechanism to reserve informative relations that \nthe existing rigid partitioning mechanism misses. These retained relations, which may be lost \nduring the patch partitioning step in the existing methods, improve the segmentation outcome.  \nThe results of PMTrans on t hree medical imaging datasets ( GLAS [10], MoNuSeg [11], and \nHECKTOR [12]) showed its superior performance over the state-of-the-art convolution-based and \ntransformer-based models on medical images. PMTrans is a general s egmentation method \ndesigned for medical images but readily applicable to images from other domains. \n \n \n \n78 78.5 79 79.5 80 80.5 81 81.5 82\nGLAS\nMoNuSeg\nHECKTOR\nNo CNN Branch Densely Connected Residual Connection Vanilla\nReferences \n1. Valanarasu, J.M.J., et al., Medical transformer: Gated axial -attention for medical i mage \nsegmentation. arXiv preprint arXiv:2102.10662, 2021. \n2. Zhang, Z., et al., Semi-supervised Semantic Segmentation of Organs at Risk on 3D Pelvic CT Images. \narXiv preprint arXiv:2009.09571, 2020. \n3. Wang, Y., et al., Organ at risk segmentation in head a nd neck ct images using a two -stage \nsegmentation framework based on 3D U-Net. IEEE Access, 2019. 7: p. 144591-144602. \n4. Chen, J., et al., Transunet: Transformers make strong encoders for medical image segmentation.  \narXiv preprint arXiv:2102.04306, 2021. \n5. Zhang, Y., H. Liu, and Q. Hu, TransFuse: Fusing Transformers and CNNs for Medical Image \nSegmentation. arXiv preprint arXiv:2102.08005, 2021. \n6. Zhang, Z., et al., ARPMâ€net: A novel CNN â€based adversarial method with Markov random field \nenhancement for prostate and organs at risk segmentation in pelvic CT images.  Medical physics, \n2021. 48(1): p. 227-237. \n7. Chen, L.-C., et al., Deeplab: Semantic image segmentation wit h deep convolutional nets, atrous \nconvolution, and fully connected crfs.  IEEE transactions on pattern analysis and machine \nintelligence, 2017. 40(4): p. 834-848. \n8. Zheng, S., et al., Rethinking Semantic Segmentation from a Sequence -to-Sequence Perspective \nwith Transformers. arXiv preprint arXiv:2012.15840, 2020. \n9. Zeng, G., et al. 3D U -net with multi -level deep supervision: fully automatic segmentation of \nproximal femur in 3D MR images . in International workshop on machine learning in medical \nimaging. 2017. Springer. \n10. Sirinukunwattana, K., et al., Gland segmentation in colon histology images: The glas challenge \ncontest. Medical image analysis, 2017. 35: p. 489-502. \n11. Kumar, N., et al., A dataset and a technique for generalized nuclear segmentation for  \ncomputational pathology. IEEE transactions on medical imaging, 2017. 36(7): p. 1550-1560. \n12. Andrearczyk, V., et al. Overview of the HECKTOR challenge at MICCAI 2020: automatic head and \nneck tumor segmentation in PET/CT . in 3D Head and Neck Tumor Segment ation in PET/CT \nChallenge. 2020. Springer. \n13. Acosta, O., et al., Multi-atlas-based segmentation of pelvic structures from CT scans for planning \nin prostate cancer radiotherapy, in Abdomen and Thoracic Imaging. 2014, Springer. p. 623-656. \n14. Ma, L., et al. Automatic segmentation of the prostate on CT images using deep learning and multi-\natlas fusion. in Medical Imaging 2017: Image Processing . 2017. International Society for Optics \nand Photonics. \n15. Held, K., et al., Markov random field segmentation of br ain MR images.  IEEE transactions on \nmedical imaging, 1997. 16(6): p. 878-886. \n16. Tsai, A., et al., A shape-based approach to the segmentation of medical imagery using level sets.  \nIEEE transactions on medical imaging, 2003. 22(2): p. 137-154. \n17. Ronneberger, O., P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image \nsegmentation. in International Conference on Medical image computing and computer -assisted \nintervention. 2015. Springer. \n18. Xiao, X., et al. Weighted res -unet for high -quality retina vessel segmentation . in 2018 9th \ninternational conference on information technology in medicine and education (ITME). 2018. IEEE. \n19. Li, X., et al., H-DenseUNet: hybrid densely connected UNet for liver and tumor segmentation from \nCT volumes. IEEE transactions on medical imaging, 2018. 37(12): p. 2663-2674. \n20. Zhou, Z., et al., Unet++: A nested u -net architecture for medical image segmentation , in Deep \nlearning in medical image analysis and multimodal learning for clinical decision support . 2018, \nSpringer. p. 3-11. \n21. Milletari, F., N. Navab, and S.-A. Ahmadi. V-net: Fully convolutional neural networks for volumetric \nmedical image segmentation. in 2016 fourth international conference on 3D vision (3DV) . 2016. \nIEEE. \n22. Mehta, S., et al. Y-Net: joint segmentation and classification for diagnosis of breast biopsy images. \nin International Conference on Medical Image Computing and Computer -Assisted Intervention. \n2018. Springer. \n23. Seferbekov, S., et al. Feature pyramid network for multi-class land segmentation. in Proceedings \nof the IEEE Conference on Computer Vision and Pattern Recognition Workshops. 2018. \n24. Dosovitskiy, A., et al., An image is worth 16x16 words: Transformers for image recognition at scale. \narXiv preprint arXiv:2010.11929, 2020. \n25. Devlin, J., et al., Bert: Pre-training of deep bidirectional transformers for language understanding. \narXiv preprint arXiv:1810.04805, 2018. \n26. Radford, A., et al., Language models are unsupervised multitask learners. OpenAI blog, 2019. 1(8): \np. 9. \n27. Wang, H., et al. Axial-deeplab: Stand -alone axial -attention for panoptic segmentation . in \nEuropean Conference on Computer Vision. 2020. Springer. \n28. Child, R., et al., Generating long sequences with sparse transformers.  arXiv preprint \narXiv:1904.10509, 2019. \n29. Touvron, H., et al., Training data-efficient image transformers & distillation through attention.  \narXiv preprint arXiv:2012.12877, 2020. \n30. Oktay, O., et al., Attention u -net: Learning where to look for the pancreas.  arXiv preprint \narXiv:1804.03999, 2018. \n31. Wang, X., et al. Volumetric attention for 3D medical image segmentation and detection . in \nInternational Conference on Medical Image Computing and Computer-Assisted Intervention. 2019. \nSpringer. \n32. Vaswani, A., et al., Attention is all you need. arXiv preprint arXiv:1706.03762, 2017. \n33. Ho, J., et al., Axial attention in multidimensional transformers.  arXiv preprint arXiv:1912.12180, \n2019. \n34. He, K., et al. Deep residual learning for image recognition . in Proceedings of the IEEE conference \non computer vision and pattern recognition. 2016. \n35. Liu, Y., et al., Head and neck multiâ€organ autoâ€segmentation on CT images aided by synthetic MRI. \nMedical Physics, 2020. 47(9): p. 4294-4302. \n36. Hesamian, M.H., et al., Deep learning techniques for medical image segmentation: achievements \nand challenges. Journal of digital imaging, 2019. 32(4): p. 582-596. \n37. Kingma, D.P. and J. Ba, Adam: A method for  stochastic optimization.  arXiv preprint \narXiv:1412.6980, 2014. \n38. Long, J., E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. in \nProceedings of the IEEE conference on computer vision and pattern recognition. 2015. \n ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7569518089294434
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6570969820022583
    },
    {
      "name": "Segmentation",
      "score": 0.5803311467170715
    },
    {
      "name": "Transformer",
      "score": 0.5667771697044373
    },
    {
      "name": "Image segmentation",
      "score": 0.5553306341171265
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5244536399841309
    },
    {
      "name": "Computation",
      "score": 0.48284798860549927
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4757746756076813
    },
    {
      "name": "Pyramid (geometry)",
      "score": 0.4706422686576843
    },
    {
      "name": "Feature extraction",
      "score": 0.42848607897758484
    },
    {
      "name": "Image resolution",
      "score": 0.4123355746269226
    },
    {
      "name": "Computer vision",
      "score": 0.3986494541168213
    },
    {
      "name": "Algorithm",
      "score": 0.18676549196243286
    },
    {
      "name": "Mathematics",
      "score": 0.08756107091903687
    },
    {
      "name": "Voltage",
      "score": 0.08334791660308838
    },
    {
      "name": "Engineering",
      "score": 0.07801923155784607
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}