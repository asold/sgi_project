{
  "title": "Overestimation of Syntactic Representation in Neural Language Models",
  "url": "https://openalex.org/W3035179945",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5050460997",
      "name": "Jordan Kodner",
      "affiliations": [
        "California University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A5102347449",
      "name": "Nitish Gupta",
      "affiliations": [
        "California University of Pennsylvania"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2190736972",
    "https://openalex.org/W2964243640",
    "https://openalex.org/W2788924045",
    "https://openalex.org/W2963394326",
    "https://openalex.org/W2111780752",
    "https://openalex.org/W2563574619",
    "https://openalex.org/W2986128786",
    "https://openalex.org/W2075201173",
    "https://openalex.org/W2888882903",
    "https://openalex.org/W2963969878",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W2964335542",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W4288548690",
    "https://openalex.org/W2962843521",
    "https://openalex.org/W2970442950",
    "https://openalex.org/W2970745243",
    "https://openalex.org/W4298392964",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2987553933",
    "https://openalex.org/W2963753324",
    "https://openalex.org/W2770406504",
    "https://openalex.org/W2963073938",
    "https://openalex.org/W2996728628",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2963125544",
    "https://openalex.org/W2160073299",
    "https://openalex.org/W2768794963",
    "https://openalex.org/W2600110521",
    "https://openalex.org/W2786686245",
    "https://openalex.org/W2157889740",
    "https://openalex.org/W3044103552",
    "https://openalex.org/W2964306924",
    "https://openalex.org/W2963025830",
    "https://openalex.org/W2919420119"
  ],
  "abstract": "With the advent of powerful neural language models over the last few years, research attention has increasingly focused on what aspects of language they represent that make them so successful. Several testing methodologies have been developed to probe models' syntactic representations. One popular method for determining a model's ability to induce syntactic structure trains a model on strings generated according to a template then tests the model's ability to distinguish such strings from superficially similar ones with different syntax. We illustrate a fundamental problem with this approach by reproducing positive results from a recent paper with two non-syntactic baseline language models: an n-gram model and an LSTM model trained on scrambled inputs.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1757–1762\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n1757\nOverestimation of Syntactic Representation in Neural Language Models\nJordan Kodner\nUniversity of Pennsylvania\nDept. of Linguistics\njkodner@sas.upenn.edu\nNitish Gupta\nUniversity of Pennsylvania\nDept. of Computer and Information Science\nnitishg@seas.upenn.edu\nAbstract\nWith the advent of powerful neural language\nmodels over the last few years, research atten-\ntion has increasingly focused on what aspects\nof language they represent that make them\nso successful. Several testing methodologies\nhave been developed to probe models’ syntac-\ntic representations. One popular method for\ndetermining a model’s ability to induce syn-\ntactic structure trains a model on strings gen-\nerated according to a template then tests the\nmodel’s ability to distinguish such strings from\nsuperﬁcially similar ones with different syntax.\nWe illustrate a fundamental problem with this\napproach by reproducing positive results from\na recent paper with two non-syntactic baseline\nlanguage models: an n-gram model and an\nLSTM model trained on scrambled inputs.\n1 Introduction\nIn recent years, RNN-based systems have proven\nexcellent at a wide range of NLP tasks, sometimes\nachieving or even surpassing human performance\non popular benchmarks. Their success stems from\nthe complex but hard to interpret, representations\nthat they learn from data. Given that syntax plays\na critical role in human language competence, it\nis natural to ask whether part of what makes these\nmodels successful on language tasks is an ability\nto encode something akin to syntax.\nThis question pertains to syntax “in the meaning-\nful sense,” that is, the latent, hierarchical, largely\ncontext-free phrase structure underpinning human\nlanguage as opposed to superﬁcial or shallow is-\nsues of word order (Chomsky, 1957; Marcus, 1984;\nEveraert et al., 2015; Linzen et al., 2016). Clearly,\nsyntactic information can be explicitly incorporated\ninto neural systems to great effect (e.g., Dyer et al.,\n2016; Swayamdipta et al., 2018). Less certain is\nwhether such systems induce something akin to hi-\nerarchical structure (henceforth, “syntax”) on their\nown when not explicitly taught to do so.\nUncovering what an RNN actually represents is\nnotoriously difﬁcult, and several methods for prob-\ning RNNs’ linguistic representations have been de-\nveloped to approach the problem. Most directly,\none can extract ﬁnite automata (e.g., Weiss et al.,\n2017) from the network or measure its state as it\nprocesses inputs to determine which neurons attend\nto what features (e.g., Shi et al., 2016; Linzen et al.,\n2016; Tenney et al., 2019). Alternatively, one can\npresent a task which only a syntactic model should\nbe able to solve, such as grammaticality discrim-\nination or an agreement task, and then infer if a\nmodel has syntactic representations based on its\nbehavior (Linzen et al., 2016; Ettinger et al., 2018;\nGulordava et al., 2018; Warstadt et al., 2019).\nIn practice, simple sentences far outnumber the\nones that require syntax in any natural corpus,\nwhich may obscure evaluation (Linzen et al., 2016).\nOne way around this, referred to here as template-\nbased probing, is to either automatically generate\nsentences with a particular structure or extract just\nthe relevant ones from a much larger corpus. Tem-\nplates have been used in a wide range of studies,\nincluding grammaticality prediction (e.g., Warstadt\net al., 2019), long-distance dependency resolution,\nand agreement prediction tasks (e.g., Gulordava\net al., 2018). By focusing on just relevant struc-\ntures that match a given template rather than the\ngamut of naturally occurring sentence, template-\nbased probing offers a controlled setting for evalu-\nating speciﬁc aspects of a model’s representation.\nThe crux of behavioral evaluation is the asser-\ntion that the chosen task effectively distinguishes\nbetween a model that forms syntactic representa-\ntions and one which does not. This must be demon-\nstrated for each task – if a model that does not\ncapture syntax can pass the evaluation, then there\nis no conclusion to be drawn. However, this step is\noften omitted (but not always, e.g., Gulordava et al.,\n1758\n2018; Warstadt et al., 2019). Moreover, template-\nbased generation removes the natural sparse and\ndiverse distribution of sentence types, increasing\nthe chance that a system might pick up on non-\nsyntactic patterns in the data, further increasing the\nimportance of a clear baseline.\nThis problem is most clearly illustrated with\nan example. In the following sections, we intro-\nduce Prasad et al.’s (2019) novel psycholinguistics-\ninspired template-based probe of relative clause\ntypes, which was taken as evidence in support of\nsyntactic representation in LSTMs. We then pass\nPvSL’s test with two non-syntactic baselines: an\nn-gram LM which can only capture short-distance\nword order of concrete types (Section 3), and an\nLSTM trained on scrambled inputs (Section 4).\nThese baselines show that a combination of col-\nlocation and lexical representation can account for\nPvSL’s results, which highlights a critical ﬂaw in\nthat experimental design. Following that, we argue\nthat it is unlikely that LSTMs induce syntactic rep-\nresentations given current evidence and suggest an\nalternative angle for the question (Section 5).\n2 Prasad, van Schijndel, & Linzen 2019\nPrasad et al. (PvSL; 2019) leverage an analogy\nfrom psycholinguistic syntactic priming to test\nwhether an LSTM is able to distinguish between\nsentences with different syntactic structures. When\nhuman subjects are primed by receiving an exam-\nple of some input, their expectation of receiving\nsimilar subsequent input will temporarily increase\nrelative to their expectation of other inputs. This\ncan be used to test questions about syntax because\nonce one is primed with sentences with a speciﬁc\nstructure, subsequent sentences with shared struc-\nture will tend to show decreased surprisal responses\nrelative to those with different structures.\nPvSL observe that this procedure may be applied\nto neural networks as well. Since a model’s sur-\nprisal upon receiving some input decreases as it\nreceives subsequent similar inputs, one could cu-\nmulatively “prime” a model by adapting it toward\na certain class of input (van Schijndel and Linzen,\n2018). As the reasoning goes, if the model can\nbe primed for a particular syntactic structure, that\nimplies that it is able to recognize that structure\nand therefore has learned a representation for it.\nThis paradigm is used to assess an LSTM’s abil-\nity to distinguish between ﬁve superﬁcially similar\nbut structurally distinct sentences types: those con-\ntaining an unreduced object relative clause (RC),\nreduced object RC, unreduced passive subject RC,\nunreduced passive subject RC, and active subject\nRC, as well as two types matched for lexical con-\ntent: passive subj./obj. RC-matched coordination\nsentences and active subj. RC-matched coordi-\nnation. (1-2) present an example object RC and\nsubject RC sentence to illustrate the structures. 1\nThese are distinguished syntactically by the origin\nof their subjects. In the ﬁrst case, the subject of\nthe sentence, ‘the cake,’ is also the object of the\nrelative clause (position indicated by underscore),\nbut in the second case, the sentence subject, ‘the\nbaker,’ is also the subject of the relative clause.\n(1) unreduced obj. RC: The caket [that the\nbaker baked t] impressed the customers.\n(2) unreduced subj. RC: The bakert [that t\nbaked the cake] impressed the customers.\nAs PvSL note, if a model were able to track the\nposition of the implicit syntactic origin, it would\nbe able to distinguish these sentence types, so one\nwould expect the model to exhibit a greater adap-\ntation effect (greater decrease in surprisal) when\nprimed and tested on the same sentence type than\nif primed on one type and tested on the other.\n2.1 Main Experiment\nPvSL populated templates to generate ﬁve sets of\n20 adaptation and 50 test sentences for each sen-\ntence type with lexical items chosen to minimize\nlexical overlap between corresponding adaptation\nand test sets. Modiﬁers were optionally inserted\nin order to vary surface word order somewhat, and\ngenerated sentences were constrained to be felici-\ntous, that is, they all made plausible semantic sense.\nThey trained 75 LSTM language models (van\nSchijndel and Linzen, 2018) on ﬁve splits of the\nWikiText-103 corpus. Average surprisal was com-\nputed for each model for each test set, then each\nmodel was adapted to (“primed for”) each sentence\ntype. They were then retested on the same test sets.\nThe difference between pre- and post-adaptation\nsurprisal (“adaptation effect”) for each adaptation\nsentence type/test type pair was recorded, and adap-\ntation effects were averaged across all models for\neach sentence type.\nThey establish a consistently and signiﬁcantly\nstronger adaptation effect for same-type adapta-\ntion and test runs than different-type runs (PvSL\n1More examples can be found in PvSL §4.1.\n1759\nFigure 1: Average same-type vs. different-type adap-\ntation effects for n-gram models. All differences are\nstatistically signiﬁcant except for object coordination.\n§5.2), a stronger effect for RCs tested on models\nadapted for RCs rather than coordination sentences\nand vice-versa (PvSL §5.3), and for runs matched\nfor passive voice over mismatched runs and for\nruns matched for reduction over mismatched runs\n(PvSL §5.4). Altogether, this is consistent with\ntheir hypothesis that the LSTM LMs are capturing\nabstract syntactic properties of their inputs.\nAlthough the results are impressive, there are\npotential issues with their suggested interpretation.\nNamely, there may still be sufﬁcient superﬁcial\nword order information to achieve the effect de-\nspite the addition of optional modiﬁers (e.g., if\nunreduced object RCs often contain the bigram\n“that the,” but unreduced subject RCs never do).\nAlso, the felicity constraint means that the lexical\nitems that appear in each sentence type should pat-\ntern together in the training data (i.e., verbs that are\nmore likely to appear in object RCs are likely to\npattern similarly in other constructions too). We\ntest both possibilities in the following sections.\n3 N-Gram Model\nWe begin by training an n-gram language model\n(through 4-grams) with Knesser-Ney smoothing\n(Ney et al., 1994) with the NLTK toolkit to de-\ntermine whether it could be primed to distinguish\nPvSL’s sentence types. An n-gram LM can only\nlearn surface collocations and so cannot capture\n(hierarchical) syntax, so if it produces a signiﬁcant\ndifferential adaptation effect, then the experiment\nis not able to discriminate between models which\ncapture syntax from those which do not.\nAdaptation and testing were carried out with\nPvSL’s adaptation and test sets, and LM training\nwas modiﬁed slightly to address n-gram models’\ncharacteristics. They have no recency bias, unlike\nRNNs, which diminishes the impact of adaptation.\nAs such, 20 smaller models were trained on disjoint\nFigure 2: Average RC vs. coordination adaptation ef-\nfects for n-gram models. Adapt on coord. is signiﬁcant\nsubsets of WikiText-2 rather than the full-sized\nWikiText-103 subsets.\nPlotting and statistical analysis were carried out\nwith PvSL’s code2. Figure 1 shows the average\nadaptation effect observed when the models are\nadapted and tested on the same sentence type or dif-\nferent sentence types. Importantly, the same-type\nadaptation effect is greater than the different-type\neffect for six of seven sentence types (unreduced\npassive RC is reversed). Although the adaptation ef-\nfect is uniformly weaker than observed for PvSL’s\nLSTM LMs, there is a statistically signiﬁcant dif-\nference between the same-type and different-type\neffects for six of seven sentence types.\nFigure 2 compares the adaptation effect over\nRCs compared to coordination sentences. The n-\ngram models show a signiﬁcantly greater same-\ntype adaptation effect for coordination but not for\nRCs. A small but signiﬁcant increase in voice-\nand reduction-matched adaptation over unmatched\ncombinations was found (matched-passive matched\nreduction: 0.610, matched-passive mismatched-\nreduction: 0.594, mismatched-passive matched-\nreduction: 0.575, mismatched-passive mismatched-\nreduction: 0.572).\n4 Scrambled-Input Model\nNext, the same van Schijndel and Linzen (2018)\ntrained LSTM LMs which PvSL employed were\nadapted on altered versions of their adaptation sets\nin which the word order of each sentence was\nscrambled to destroy the sentence’s syntax while\nretaining its lexical content, then tested on the orig-\ninal non-scrambled test sets. Even though PvSL\nminimize the amount of lexical overlap in the adap-\ntation and test sets, it may be the case that the\nmodels pick up on lexical similarities because of\nthe felicity constraint which was imposed on them.\n2https://github.com/grushaprasad/RNN-Priming, with mi-\nnor aesthetic changes to plots\n1760\nFigure 3: Average same-type vs. different-type adap-\ntation effects for scrambled LSTM models. All differ-\nences are signiﬁcant.\nScrambling was random on a sentence-by-\nsentence basis. Results were averaged across all\nthe adaptation sets and models (as they were in\nPvSL), so the effect of any individual accidentally\ngrammatical scramble was diminished.\nFigure 3 shows the average differential adapta-\ntion effects on these scrambled annotation runs.\nThe same-type adaptation effect is signiﬁcantly\ngreater than different-type for six of seven sen-\ntence types (except subject coord.), and the largest\nrelative difference is seen for unreduced passive\nRCs, the only type for which the n-gram models\nproduced a reverse effect. Overall, the adaptation\neffect is an order of magnitude larger than for the\nn-gram models’ but still smaller than PvSL’s.\nFigure 4 shows differential adaptation effects\nfor RC and coordination sentences. A backward\neffect is observed for sentences adapted on coor-\ndination, but a large positive effect is found for\nthose adapted on RC sentences. This is the com-\nplement of what was found for n-gram models. A\nsigniﬁcant positive difference was found between\nsentence types matched and unmatched in passives\nand reduction (matched-passive matched reduc-\ntion: 0.65, matched-passive mismatched-reduction:\n0.53, mismatched-passive matched-reduction: 0.53,\nmismatched-passive mismatched-reduction: 0.43).\n5 Discussion\nThese results call into question the van Schijndel\nand Linzen (2018) and Prasad et al. (2019) syntac-\ntic priming paradigm’s ability to distinguish mod-\nels which represent syntax from those which rely\non shallow phenomena by achieving a positive re-\nsult with two non-syntactic baseline models. First,\nsuccess in the priming paradigm is measured by\nwhether or not adaptation reduces surprisal, but not\nby how much, so even though both baseline mod-\nels tested here reduce surprisal by less than PvSL’s\nFigure 4: Average RC vs. coord. adaptation effects for\nscrambled LSTM models. Differences are signiﬁcant.\nmodels on average, they still pass the success crite-\nrion. To put it another way, PvSL report quantita-\ntive results but do not actually establish what would\nconstitute a meaningful effect size. Even though\nthe effect sizes of both our baseline replications\nwere smaller, PvSL could have reported the results\nfrom our baseline models instead of their actual\nmodel and drawn the same conclusions.\nSecond, the fact that our surface word order n-\ngram model and lexical similarity-only scrambled\nLSTM LMs also show surprisal effects draws into\nquestion the basic claim that only a syntactic model\nwould respond to adaptation: it is our hypothesis\nthat the combined effect of word order and lexical\nsimilarity are what drive the LSTM models’ larger\neffect. This is upheld, especially when it is noted\nthat the adaptation effects of both baselines com-\nplement each other. Both alternative sources of\ninformation are well known in the community and\nhave been tested in the past (Bernardy and Lappin,\n2017; Gulordava et al., 2018). This reiterates the\nneed for proper baseline testing in computational\nlinguistics and for informative evaluations.\nThis highlights a more general problem with\ntemplate-based probing, namely, that the unnatural\nlack of sentence diversity imposed by the templates\nimposes unintended regularity for models to latch\nonto. Given the well-known observation that neural\nmodels will “take the easy way out” given the pres-\nence of this unintended surface information (Jia\nand Liang, 2017; Naik et al., 2018; Sennhauser and\nBerwick, 2018), and other work suggesting that\nLSTMs do not necessarily induce syntactic struc-\nture (Gupta and Lewis, 2018; McCoy et al., 2018;\nWarstadt et al., 2019), one must take successes\nin template-based probing studies with a grain of\nsalt. The evaluation of non-syntactic baselines is an\neasy-to-implement way to combat the tendency of\nthese behavioral probes to overestimate language\nmodels’ abilities.\n1761\nTo improve the priming paradigm in particular,\none would need to establish a success metric that\ndiscriminates between baselines and alter the exper-\nimental setup to mitigate information side channels.\nOne possibility would be to include infelicitous\n“colorless green ideas” sentences with grammatical\nsyntax (cf. Gulordava et al., 2018), which might\ndecrease the lexical similarity problem. Remov-\ning the issue altogether could require enforcing\ncompletely lexically disjoint training, adaptation,\nand test sets, but we cannot reasonably expect a\nmodel to function when it has no generalizations\nto work with, and demanding lexically distinct sets\n(including function words) greatly limits the set of\nphenomena that could be studied.\n5.1 An Alternative Approach\nAs a more radical alternative, we suggest extend-\ning behavioral analysis into “consequence-based”\nanalysis. The two have similar reasoning: from an\nengineering perspective, a family of models that\nis capable of inducing syntax is useful because\nit may be expected to improve performance on\ndownstream tasks. Marcus (1984) discusses in a\ntheory-independent way which kinds of sentences\na model capturing syntax should be able to parse\nbut a “no-explicit-syntax” model (in the modern\ncontext, probably a baseline RNN) should not (cf.\nChomsky, 1957; Rimell et al., 2009; Nivre et al.,\n2010; Bender et al., 2011; Everaert et al., 2015). It\nfollows then that no-explicit- and explicit-syntax\nmodels should exhibit quantitatively different be-\nhavior on tasks that require parsing such sentences.\nA model that solves problems that only one capa-\nble of inducing syntactic structure can solve may\nas well have induced syntactic structure from a\npractical standpoint.\nConsequence-based analysis would be imple-\nmented over naturalistic data rather than templates\nby embedding it in higher level tasks like question\nanswering to mitigate the unnaturalness problem\nand demonstrate a model’s practical utility. The\npossibility of side-channel information is already\nknown in relation to these higher-level tasks (e.g.,\nPoliak et al., 2018; Geva et al., 2019), and various\nchallenge data sets have been constructed to mit-\nigate it in different ways (Levesque et al., 2011;\nChao et al., 2017; Dua et al., 2019; Lin et al., 2019;\nDasigi et al., 2019). Uniting these with a collection\nof hard sentence types (e.g., Marvin and Linzen,\n2018; Warstadt et al., 2019) in something like a\nsyntax-focused QA challenge set would provide\nnew insights into which families of models capture\nthe practical beneﬁts of true hierarchical syntactic\nrepresentation.\nAcknowledgments\nWe are particularly grateful to Marten van Schijn-\ndel for sharing the van Schijndel and Linzen (2018)\nmodel checkpoints with us. We also thank Mitch\nMarcus, Charles Yang, and Ryan Budnick for their\ncomments and suggestions. This work was funded\nby an NDSEG fellowship awarded to the ﬁrst au-\nthor by the ARO, in addition to funding by the ONR\nunder Contract No. N00014-19-1-2620, and by\nsponsorship from the LwLL DARPA program un-\nder Contract No. FA8750-19-2-0201. (The views\nexpressed are those of the authors and do not reﬂect\nthe ofﬁcial policy or position of the Department of\nDefense or the U.S. Government.)\nReferences\nEmily M Bender, Dan Flickinger, Stephan Oepen, and\nYi Zhang. 2011. Parser evaluation over local and\nnon-local deep dependencies in a large corpus. In\nProceedings of the Conference on Empirical Meth-\nods in Natural Language Processing, pages 397–\n408. Association for Computational Linguistics.\nJean-Philippe Bernardy and Shalom Lappin. 2017. Us-\ning deep neural networks to learn syntactic agree-\nment. LiLT (Linguistic Issues in Language Technol-\nogy), 15.\nWei-Lun Chao, Hexiang Hu, and Fei Sha. 2017. Be-\ning negative but constructively: Lessons learnt from\ncreating better visual question answering datasets.\narXiv preprint arXiv:1704.07121.\nNoam Chomsky. 1957. Syntactic Structures. Moulton\n& Co.\nPradeep Dasigi, Nelson F. Liu, Ana Marasovi ´c,\nNoah A. Smith, and Matt Gardner. 2019.\nQuoref: A reading comprehension dataset with\nquestions requiring coreferential reasoning. In\nEMNLP/IJCNLP.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDROP: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In NAACL-\nHLT.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A Smith. 2016. Recurrent neural network\ngrammars. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 199–209.\n1762\nAllyson Ettinger, Ahmed Elgohary, Colin Phillips, and\nPhilip Resnik. 2018. Assessing composition in sen-\ntence vector representations. In Proceedings of\nthe 27th International Conference on Computational\nLinguistics, pages 1790–1801.\nM.B.H. Everaert, Marinus Huybregts, Noam Chomsky,\nRobert Berwick, and Johan Bolhuis. 2015. Struc-\ntures, not strings: Linguistics as part of the cognitive\nsciences. Trends in Cognitive Sciences, xx.\nMor Geva, Yoav Goldberg, and Jonathan Berant. 2019.\nAre we modeling the task or the annotator? an inves-\ntigation of annotator bias in natural language under-\nstanding datasets. In EMNLP/IJCNLP.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 1195–1205.\nNitish Gupta and Mike Lewis. 2018. Neural compo-\nsitional denotational semantics for question answer-\ning. In EMNLP/IJCNLP.\nRobin Jia and Percy Liang. 2017. Adversarial exam-\nples for evaluating reading comprehension systems.\nIn EMNLP/IJCNLP.\nHector J. Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge. In\nKR.\nKevin Lin, Oyvind Tafjord, Peter Clark, and Matt Gard-\nner. 2019. Reasoning over paragraph effects in situ-\nations. In MRQA.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of lstms to learn syntax-\nsensitive dependencies. TACL.\nMitchell P Marcus. 1984. Some inadequate theories\nof human language processing. In Carroll J. Bever,\nT. and L. Miller, editors, Talking Minds: The Study\nof Language in the Cognitive Sciences, chapter 9,\npages 253–279. MIT Press, Cambridge, MA.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. arXiv preprint\narXiv:1808.09031.\nR Thomas McCoy, Robert Frank, and Tal Linzen.\n2018. Revisiting the poverty of the stimulus:\nhierarchical generalization without a hierarchical\nbias in recurrent neural networks. arXiv preprint\narXiv:1802.09091.\nAakanksha Naik, Abhilasha Ravichander, Norman M.\nSadeh, Carolyn Penstein Ros´e, and Graham Neubig.\n2018. Stress test evaluation for natural language in-\nference. In COLING.\nHermann Ney, Ute Essen, and Reinhard Kneser. 1994.\nOn structuring probabilistic dependences in stochas-\ntic language modelling. Computer Speech & Lan-\nguage.\nJoakim Nivre, Laura Rimell, Ryan McDonald, and Car-\nlos Gomez-Rodriguez. 2010. Evaluation of depen-\ndency parsers on unbounded dependencies. In Pro-\nceedings of the 23rd International Conference on\nComputational Linguistics, pages 833–841. Associ-\nation for Computational Linguistics.\nAdam Poliak, Jason Naradowsky, Aparajita Haldar,\nRachel Rudinger, and Benjamin Van Durme. 2018.\nHypothesis only baselines in natural language infer-\nence. In *SEM@NAACL-HLT.\nGrusha Prasad, Marten van Schijndel, and Tal Linzen.\n2019. Using priming to uncover the organization of\nsyntactic representations in neural language models.\nIn Proceedings of the 23rd Conference on Computa-\ntional Natural Language Learning (CoNLL), pages\n66–76.\nLaura Rimell, Stephen Clark, and Mark Steedman.\n2009. Unbounded dependency recovery for parser\nevaluation. In Proceedings of the 2009 Conference\non Empirical Methods in Natural Language Process-\ning: Volume 2-Volume 2, pages 813–821. Associa-\ntion for Computational Linguistics.\nMarten van Schijndel and Tal Linzen. 2018. A neural\nmodel of adaptation in reading. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing, pages 4704–4710.\nLuzi Sennhauser and Robert Berwick. 2018. Evaluat-\ning the ability of lstms to learn context-free gram-\nmars. In Proceedings of the 2018 EMNLP Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP, pages 115–124.\nXing Shi, Inkit Padhi, and Kevin Knight. 2016. Does\nstring-based neural mt learn source syntax? In\nEMNLP/IJCNLP, pages 1526–1534.\nSwabha Swayamdipta, Sam Thomson, Kenton Lee,\nLuke Zettlemoyer, Chris Dyer, and Noah A Smith.\n2018. Syntactic scaffolds for semantic structures. In\nEMNLP/IJCNLP, pages 3772–3782.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nACL.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2019. BLiMP: A benchmark of linguistic\nminimal pairs for English.\nGail Weiss, Yoav Goldberg, and Eran Yahav. 2017. Ex-\ntracting automata from recurrent neural networks us-\ning queries and counterexamples. arXiv preprint\narXiv:1711.09576.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8212974071502686
    },
    {
      "name": "Syntax",
      "score": 0.7421582937240601
    },
    {
      "name": "Natural language processing",
      "score": 0.6594017148017883
    },
    {
      "name": "Language model",
      "score": 0.6578664779663086
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6384488344192505
    },
    {
      "name": "Representation (politics)",
      "score": 0.5253219604492188
    },
    {
      "name": "Syntactic structure",
      "score": 0.47992974519729614
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I36788626",
      "name": "California University of Pennsylvania",
      "country": "US"
    }
  ]
}