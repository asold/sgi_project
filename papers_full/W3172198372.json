{
    "title": "LightSeq: A High Performance Inference Library for Transformers",
    "url": "https://openalex.org/W3172198372",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2127537430",
            "name": "Xiaohui Wang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A1992611019",
            "name": "Ying Xiong",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2031829685",
            "name": "Yang Wei",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2132499817",
            "name": "Mingxuan Wang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2098784551",
            "name": "Lei Li",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2493916176",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3130716829",
        "https://openalex.org/W2889326796",
        "https://openalex.org/W2621550233",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4287874506",
        "https://openalex.org/W3106147182",
        "https://openalex.org/W2804032941",
        "https://openalex.org/W3099576124",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2996287690",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W1902237438",
        "https://openalex.org/W3106321930",
        "https://openalex.org/W2788277448",
        "https://openalex.org/W3092327118",
        "https://openalex.org/W3100439847"
    ],
    "abstract": "Xiaohui Wang, Ying Xiong, Yang Wei, Mingxuan Wang, Lei Li. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers. 2021.",
    "full_text": "Proceedings of NAACL HLT 2021: IndustryTrack Papers, pages 113–120\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n113\nLightSeq: A High Performance Inference Library for Transformers\nXiaohui Wang, Ying Xiong, Yang Wei, Mingxuan Wang, Lei Li\nByteDance AI Lab\n{wangxiaohui.neo, xiongying.taka, weiyang.god}@bytedance.com\n{wangmingxuan.89, lileilab}@bytedance.com\nAbstract\nTransformer, BERT and their variants have\nachieved great success in natural language pro-\ncessing. Since Transformer models are huge\nin size, serving these models is a challenge for\nreal industrial applications. In this paper, we\npropose LightSeq, a highly efﬁcient inference\nlibrary for models in the Transformer family.\nLightSeq includes a series of GPU optimiza-\ntion techniques to to streamline the computa-\ntion of neural layers and to reduce memory\nfootprint. LightSeq can easily import models\ntrained using PyTorch and Tensorﬂow. Exper-\nimental results on machine translation bench-\nmarks show that LightSeq achieves up to 14x\nspeedup compared with TensorFlow and 1.4x\ncompared with FasterTransformer, a concur-\nrent CUDA implementation. The code is avail-\nable at https://github.com/bytedance/\nlightseq.\n1 Introduction\nSequence processing and generation have been fun-\ndamental capabilities for many natural language\nprocessing tasks, including machine translation,\nsummarization, language modeling, etc (Luong\net al., 2015; Qi et al., 2020; Dai et al., 2019). In\nrecent years, with the introduction of Transformer\nmodel (Vaswani et al., 2017b), many pre-trained\nlanguage models such as BERT, GPT, and mRASP\nhave also been widely used in these tasks (Devlin\net al., 2019; Radford et al., 2019; Yang et al., 2020;\nLin et al., 2020).\nHowever, the parameters of these models be-\ncome increasingly large, which causes the high\nlatency of inference and brings great challenges\nto the deployment (Kim and Hassan, 2020). The\ncurrent popular inference systems are not neces-\nsarily the best choice for the online service of se-\nquence processing problems. First, training frame-\nworks, such as TensorFlow and PyTorch, require\naccommodating ﬂexible model architectures and\nbackward propagation, which introduce additional\nmemory allocation and extra overhead of using\nﬁne-grain kernel functions. Therefore, the direct\ndeployment of the training framework is not able to\nmake full use of the hardware resource. Taking an\nexample of machine translation, the Transformer\nbig model currently takes roughly 2 seconds to\ntranslate a sentence, which is unacceptable in both\nacademia and industry (Edunov et al., 2018; Hsu\net al., 2020). Second, current optimizing compilers\nfor deep learning such as TensorFlow XLA (Abadi\net al., 2017), TVM (Chen et al., 2018) and Ten-\nsor RT (Vanholder, 2016) are mainly designed for\nﬁxed-size inputs. However, most NLP problems\nenjoy variable-length inputs, which are much more\ncomplex and require dynamic memory allocation.\nTherefore, a high-performance sequence inference\nlibrary for variable-length inputs is required. There\nare several concurrent CUDA libraries which share\na similar idea with our project, such as Faster-\nTransformer 1 and TurboTransformers (Fang et al.,\n2021).\nWe will highlight three innovative features that\nmake LightSeq outperforms similar projects. First,\nwe replace a straightforward combination of ﬁne-\ngrained GPU kernel functions in TensorFlow or\nPyTorch implementations with coarse-grain fused\nones, which avoid high time cost introduced by a\nmass of kernel function launches and GPU mem-\nory I/O for intermediate results. As a result, Light-\nSeq reduces the atomic kernel functions by four\ntimes compared with Tensorﬂow approaches. Sec-\nond, we specially design a hierarchical auto regres-\nsive search method to speed up the auto-regressive\nsearch. Third, we propose a dynamic GPU memory\nreuse strategy. Different from ﬁxed-length inputs,\nsequence processing tackles the variable-length in-\nputs, which bring difﬁculty for memory allocation.\nLightSeq proposes to pre-deﬁne the maximal mem-\nory for each kernel function and shares the GPU\n1https://github.com/NVIDIA/\nFasterTransformer\n114\nModels Decoding MethodsInference LibrariesTransformer GPT V AE BERT MultilingualBeam Search Diverse Beam Search Sampling\nFasterTransformer ! ! % ! % ! ! !\nTurboTransformers ! % % ! % % % %\nLightSeq ! ! ! ! ! ! ! !\nTable 1: Features for FasterTransformer, TurboTransformers and our proposed LightSeq. LightSeq supports the\nmost features for a comprehensive set of Transformer models.\nmemory across non-dependent ones. As a result,\nLightSeq reduces eight times memory allocation\nwithout loss of inference speed. As a beneﬁt, Light-\nSeq enjoys several advantages:\nEfﬁcient LightSeq shows better inference perfor-\nmance for generation tasks. For example, in\nmachine translation benchmarks, LightSeq\nachieves up to 14 times speedup compared\nwith TensorFlow and 1.4 times speedup com-\npared with FasterTransformer.\nFunctional LightSeq supports more architecture\nvariants, such as BERT, GPT, Transformer,\nand Variational Autoencoders (V AEs). Fur-\nther, LightSeq provides different search algo-\nrithms, such as beam search, diverse beam\nsearch and probabilistic sampling (Vijayaku-\nmar et al., 2018). Table 1 shows the functional\ncomparison between FasterTransformer2, Tur-\nboTransformers3, and LightSeq in text gener-\nation tasks.\nConvenient LightSeq is easy to use, which con-\ntains a serving system and efﬁcient CUDA im-\nplementations. The popular models, such as\nBERT, Roberta, GPT, V AEs, MT Transformer,\nand Speech Transformer can be directly de-\nployed online without code modiﬁcation. For\nuser-speciﬁc architectures, LightSeq supports\nmultiple model reuse, which can be easily\nadapted with only a few lines of code modiﬁ-\ncation.\n2 LightSeq Approach\nTransformer-based NLP models mainly consist of\ntwo components during inference: the feature cal-\nculation layer and the output layer, as shown in\nFigure 1.\n2As of this writing, we use FasterTransformer v2.1 for\ncomparison.\n3we use TurboTransformers for comparison at commit\n0eae02ebadc8b816cd9bb71f8955a7e620861cd8\nThe feature calculation layer is mainly based on\nself-attention mechanism and feature transforma-\ntion, which is actually implemented by matrix mul-\ntiplication and a series of I/O-intensive operations\nsuch as element-wise (e.g., reshape) and reduce\n(e.g., layer normalization).\nThe output layer slightly changes in different\ntasks, such as classiﬁcation in NLU tasks or search\n(e.g., beam search) in NLG tasks. This layer is usu-\nally composed of the Softmax over vocabulary,\nprobability sorting, cache refreshing, etc., which\nare essentially I/O-intensive.\nThese two components pose challenges for efﬁ-\ncient inference:\n•The ﬁne-grained call of I/O-intensive GPU\nkernel function brings a huge amount of GPU\nmemory I/O, which becomes the performance\nbottleneck of feature calculation.\n•Redundant calculations exist due to the fact\nthat we only need a few tokens/labels with the\nhighest probability instead of all in classiﬁca-\ntion or search for the output layer.\n•Dynamic shape in variable sequence length\nand auto-regressive search makes it difﬁcult to\nachieve memory reuse within or between re-\nquests, which leads to a large number of GPU\nmemory allocation during model service.\nLightSeq employs a series of innovative meth-\nods to address these challenges to accelerate model\ndevelopment, such as fusion of multiple kernel\nfunctions to reduce I/O overhead, hierarchical opti-\nmization of search algorithms to erase redundant\ncalculations, and reuse of dynamic GPU memory\nto avoid run-time allocation. The following is a\ndetailed introduction to these methods.\n2.1 Operation Fusion\nTransformer feature calculation layer needs to be\nhighly optimized since it is ubiquitous in various\n115\n你 好 。\nEncoder\nLayer\n… …\n<BOS>Hello\nDecoder\nLayer\nLinear\n+SoftmaxLinear\n+Softmax\nHello .\nEncoder\nBeam\nSearch\nDecoder\n…Multi-Head Attention\nAdd & Norm\nFeed Forward\nAdd & Norm\nFeed Forward\nAdd & Norm\nMulti-Head Attention\nAdd & Norm\nAdd & Norm\nMasked Multi-Head Attention\n… …Encoder\nLayer Encoder\nLayer\nEncoder\nLayer Encoder\nLayer Encoder\nLayer\nEncoder\nLayer Encoder\nLayer Encoder\nLayer\nDecoder\nLayer\nDecoder\nLayer Decoder\nLayer\nDecoder\nLayer Decoder\nLayer\nFigure 1: The process of sequence to sequence generation using Transformer model with beam search.\nNLP tasks today. In most deep learning frame-\nworks, such as TensorFlow and PyTorch, it is imple-\nmented by a straightforward combination of ﬁne-\ngrained kernel functions from standard libraries\nprovided by hardware manufacturers, which in-\ntroduces high time cost due to a mass of kernel\nfunction launches and GPU memory I/O for inter-\nmediate results.\nTaking layer normalization implemented by Ten-\nsorFlow as an example, there are still three kernel\nlaunches4 and two intermediate results (mean and\nvariance) even with the help of optimizing com-\npilers like TensorFlow XLA (Abadi et al., 2017).\nAs a comparison, we can write a custom kernel\nfunction dedicated to layer normalization based on\nthe CUDA toolkit, which produces only one kernel\nlaunch without intermediate results.\nLightSeq implements the Transformer feature\ncalculation layer with general matrix multiply\n(GEMM) provided by cuBLAS5 and custom ker-\nnel functions. The detailed structure is shown in\nFigure 2. Combination of ﬁne-grained operations\nbetween GEMM operations is fused into one cus-\ntom kernel function. In consequence, there are only\nsix custom kernel functions and six GEMM in a\nTransformer encoder layer, which is usually more\nthan four times less than its corresponding imple-\nmentation in common deep learning frameworks\nlike TensorFlow or PyTorch.\n2.2 Hierarchical Auto Regressive Search\nLightSeq supports a comprehensive set of output\nlayers, such as sentence-level and token-level clas-\nsiﬁcation, perplexity calculation for language mod-\n4Two for reduce mean operations and one for calcula-\ntion of the ﬁnal result.\n5https://developer.nvidia.com/cublas\nY=Y⋅WO+b\nY=S⋅V\nReshape Y\nReshape Q,K,V\nS=Q⋅K\nSoftmax\nQ,K,V=X⋅(WQ,WK,WV)+b\nLN and Residual\nX=Xe+Xp\nLN and Residual\nY=Y⋅W1\n and RELUb1\nY=Y⋅W2+b2\nSelf\nAttention\nFFN\nCustom Kernel\nCuBLAS GEMM\nFigure 2: The structure of optimized Transformer en-\ncoder layers in LightSeq.\nels, and auto-regressive search like beam search, di-\nverse beam search and top-k/top-p sampling (Holtz-\nman et al., 2020). Redundant calculations often ex-\nist in these output layers since we only need a few\nlabels/tokens with the highest probability instead\nof all of them. Auto-regressive search is relatively\ncomplicated, and we will discuss it in the next para-\ngraph. For the other types of output layers, we can\nsimply replace Softmax with the probability cal-\nculation of token/label with the highest logits,\nwhich brings more obvious beneﬁt when the size\nof vocabulary or labels is large.\nAuto-regressive search is widely used in ma-\nchine translation and text generation. LightSeq\nproposes Hierarchical Auto Regressive Search\n(HARS) method to erase redundant calculations\nand parallel computing. Here we take the most\nused beam search method as an example to intro-\n116\nduce the proposed HARS method.\nIn one step of the beam search process, given\nthe logits, we need to perform two calculations\nover the whole vocabulary:\n1. Compute the conditional probability using\nSoftmax and write the intermediate result\ninto GPU memory.\n2. Read the intermediate result from GPU mem-\nory and select the top-k beams and tokens by\nsequential probability.\nThese two calculations are highly time-\nconsuming since the vocabulary size is usually in\ntens of thousands of scales. For example, they\naccount for a latency proportion of 30% in Trans-\nformer base models.\nIn order to reduce the input size of these two\ncalculations, LightSeq introduces a two-stage strat-\negy that is widely employed in the recommended\nsystem: retrieve and re-rank.\nBefore the probability computation and top- k\nselection, the retrieve is carried out ﬁrst. For each\nbeam, we calculate as follows:\n1. Randomly divide logits into k groups.\n2. Calculate the maximum of group i, denoted\nas mi\n3. Calculate the minimum of mi, denoted as R,\nwhich can be regarded as a rough top-k value\nof logits.\n4. Select logits larger than Rand write them\ninto GPU memory.\nThe retrieve is co-designed based on GPU char-\nacteristics and logits distribution. Hence it is\nefﬁcient and effective:\n•Efﬁcient. The retrieve is implemented by one\nkernel function and can be executed within a\ndozen instruction cycles.\n•Effective. After the retrieve, only dozens of\ncandidates were selected.\nAfter the retrieve, the original two calculations of\nbeam search will be carried out on the small set of\ncandidates, named as Hierarchical Auto Regressive\nSearch.\nFigure 3 is a detailed illustration of the proposed\nhierarchical strategy. In the original beam search\n2143274431551826\n474 58\n44578\nRetrieve-1\nRetrieve-2\nRe-rank\n2143274431551826\n1112223344455678Directly sorting \nFigure 3: An illustration of the proposed hierarchical\nstrategy. In this case, beam size is 2 and vocabulary\nsize is 8. Each row represents logits in a beam.\nmethod, we need to compute the probability and\nselect the top-k over the whole vocabulary. How-\never, by hierarchical method, we only need to pick\na small set of candidates from each beam and then\nperform probability computation and top-k selec-\ntion.\n2.3 Dynamic GPU Memory Reuse\nIn order to save GPU memory occupancy and avoid\nallocation of GPU memory during the model serv-\ning, LightSeq pre-deﬁnes the maximum of dynamic\nshapes, such as the maximal sequence length. At\nthe start of the service, each intermediate result in\nthe calculation process is allocated GPU memory\nto its maximum. Besides, GPU memory is shared\nfor non-dependent intermediate results.\nThrough this memory reuse strategy, on a T4\ngraphics card, we can deploy up to 8 Transformer\nbig models6 at the same time, so as to improve\ngraphics card utilization in low frequency or peak-\nshifting scenarios.\n3 Experiments\nIn this section, we will show the improvements\nof LightSeq with different GPU hardware and pre-\ncisions. We ﬁrst analyze the GPU occupation of\nLightSeq during inference to investigate if Light-\nSeq can make full use of GPU resources. Then, we\nmake a fair comparison with TensorFlow, PyTorch,\nFasterTransformer, and TurboTransformers on ma-\nchine translation and text generation to show the\nefﬁciency of LightSeq.\n6Under the conﬁguration of 8 batch size, 256 sequence\nlength, 4 beam size and 30000 vocabulary size.\n117\nCast\n39.7%Top-k\n3.2%\nElement-wise Sum\n2.5%\nGEMM\n24.5%\nOthers\n30.1%\n(a) TensorFlow with Float16.\nGEMM\n87.0%\nCache Refreshing\n6.4% Layer Normalization\n3.6%\nHARS2.9%\nOthers0.1% (b) LightSeq with Float16.\nGEMM\n82.0%\nCache Refreshing\n10.1% Layer Normalization\n3.0% HARS\n2.6%\nOthers2.3% (c) LightSeq with Float32.\nFigure 4: Proportion of computation occupation. GEMM is the main indicator and the larger number indicates the\nhigher computation efﬁciency.\n(1, 32) (1, 64) (16, 32)(16, 64)(32, 32)(32, 64)(64, 32)(64, 64)(128, 32)(128, 64)\n(Batch size, Seq len)\n1\n2\n3\n4\n5\n6Speedup\nTensorFlow\nPyTorch\nFasterTransformer\nLightSeq\n(a) P4 speedup in Float32.\n(1, 32) (1, 64) (8, 32) (8, 64) (32, 32)(32, 64)(64, 32)(64, 64)(128, 32)(128, 64)\n(Batch size, Seq len)\n2\n4\n6\n8\n10\n12\n14\n16Speedup\nTensorFlow\nPyTorch\nFasterTransformer\nLightSeq (b) T4 speedup in Float16.\nFigure 5: Speedup on Transformer with beam search compared with FasterTransformer, TurboTransformers and\nPyTorch implementation. The baseline is TensorFlow implementation.\n3.1 Experiment Settings\nWe test the generation performance of LightSeq\non two latest NVIDIA inference GPU Tesla P4\nand T4, choosing TensorFlow, PyTorch, and Faster-\nTransformer implementations as a comparison. An-\nother related library, TurboTransformers, mainly\nfocuses on the Transformer encoder and is not pow-\nerful enough for text generation. Its speedup for\nsequence generation compared to TensorFlow is\nonly about 15%, and it only supports Float32 on\nGPU. Therefore we do not compare with it.\nThe experiments on machine translation are con-\nducted on the popular WMT14 English to German\ntranslation tasks. The hyper-parameters setting re-\nsembles transformer base model (Vaswani et al.,\n2017a). Speciﬁcally, we reduce the vocabulary size\nof both the source language and target language to\n50K symbols using the sub-word technique (Bo-\njanowski et al., 2017).\nThe experiments on text generation are con-\nducted on a randomly initialized Transformer\nmodel and test dataset. Results of Tensorﬂow and\nFasterTransformer are obtained from the scripts\nin the source code of FasterTransformer. The se-\nquence length is used for limiting the total size in\nthe generation test, and the values for top- k and\ntop-p are the most selected settings in our deploy-\nments.\n3.2 GPU Occupation of LightSeq\nWe ﬁrst analyze the GPU occupation to verify the\nefﬁciency of LightSeq. The experiments are con-\nducted on Tesla T4 card with the GPU proﬁling\ntoolkit. The latency of each module is shown in Fig-\nure 4 with both Float16 and Float32 precision. We\nclassify the operation into three categories: GEMM,\ncache refreshing, and others. GEMM latency is the\nmost important indicator, which shows the pro-\nportion of matrix calculations occupying the GPU\ncalculation.\nAfter optimization, we can ﬁnd that:\n•GEMM operation in LightSeq accounts for\n118\n(1, 32) (1, 64)(32, 32)(32, 64)(128, 32)(128, 64)\n(Batch size, Seq len)\n1\n2\n3\n4\n5\n6Speedup\nTensorFlow\nFasterTransformer\nLightSeq\n(a) Top-p = 0.75.\n(1, 32) (1, 64) (32, 32) (32, 64)(128, 32)(128, 64)\n(Batch size, Seq len)\n1\n2\n3\n4\n5\n6Speedup\nTensorFlow\nFasterTransformer\nLightSeq (b) Top-k = 32.\nFigure 6: T4 speedup on Transformer with sampling\ncompared with FasterTransformer in Float16. Light-\nSeq outperforms FasterTransformer in most cases.\n87% and 82% respectively for Float16 and\nFloat32, accounting for most of the inference\ntime. However, in the original TensorFlow\nmodel, GEMM operations account for only\n25%. This shows that beam search optimiza-\ntion has achieved good results.\n•Cast and other operations in TensorFlow are\nexpensive, which launches over 80 different\nGPU kernels. In LightSeq, we fuse cast opera-\ntions into weight loading, and other operations\ninto more efﬁcient implementations.\n•The latency of cache refreshing in LightSeq\naccounts for 6% and 10% respectively, which\nare not negligible but hard to be optimized fur-\nther. Possible solutions include reducing the\namount of cache, such as reducing the number\nof decoder layers, reducing cache precision,\netc.\nThe results demonstrate that LightSeq has been\noptimized to a disabling extent and greatly in-\ncreases the speed of inference. Another interest-\ning ﬁnding is that Float16 is more efﬁcient than\nFloat32. A possible explanation is that Float16 oc-\ncupies less memory. Therefore the cache refreshing\nand memory I/O operations potentially take less\ntime.\n3.3 Comparison on Machine Translation\nThe comparison between LightSeq, TensorFlow,\nPyTorch and FasterTransformer are shown in Fig-\nure 5. We group the test set into different buckets\naccording to the sequence length and batch size.\nFor example, the x-axis (a, b) indicates that the\nbatch size is a and the sequence length is b. The\ny-axis is the speedup compared with TensorFlow\nbaseline. The results provide several interesting\nﬁndings:\n•For both LightSeq and FasterTransformer, the\nspeedup gap for smaller batch size or shorter\nsequence length is much larger.\n•The speedup for T4 is larger than P4. The\nmain reason is that T4 is more powerful than\nP4 and has much room for improvement.\n•In most cases, LightSeq performs better than\nFasterTransformer. For larger batch size and\nlonger sequences, the gap increases. While\nfor smaller batch size, FasterTransformer per-\nforms better.\n•PyTorch is slightly slower than TensorFlow\nin P4 and faster in T4, which indicates that\nLightSeq also greatly outperforms PyTorch in\nall cases.\nThe ﬁndings provide some guidance for opti-\nmization work in the future. There is almost no\nspace to accelerate the inference by fusion of non-\ncomputationally intensive operators, especially for\nsmall batch size. Future work is recommended\nto focus on optimizing GEMM operations which\naccount for 80% to 90% of the total computation\ntime.\nFinally, we compare TurboTransformers with Py-\nTorch by the translation demo7. As of this writing,\nonly decoder layers of MT Transformer in ﬂoat32\nprecision is supported, so we only compare the la-\ntencies of decoder layers without beam search and\ncache refreshing. In the ﬁnal results, TurboTrans-\nformers only achieves about 2x speedup for differ-\nent batch sizes and sequence lengths. So Turbo-\nTransformers has no comparability with LightSeq\nin machine translation tasks (As TurboTransformer\nrepo says, “TurboTransformer will bring 15.9% per-\nformance improvements on RTX 2060 GPU. We\nare still working on decoder model optimization.”).\n3.4 Comparison on Text Generation\nIn the text generation scenario, the sampling strat-\negy is applied to improve the diversity of gener-\nation. Among which, top- k and top-p sampling\nstrategies are more popular.\n7https://github.com/\nTurboNLP/Translate-Demo/tree/\n443e6a46fefbdf64282842b6233a8bd0a22d6aeb\n119\nFigure 6 shows the performance comparison of\nTransformer base with top-k/top-p sampling. The\nvalues of top-k and top-p are added in the x-axis.\nThe results provide following ﬁndings:\n•In most cases, LightSeq achieves greater\nspeedup than FasterTransformer. Unlike re-\nsults in machine translation, LightSeq per-\nforms better for smaller batch size and shorter\nsequence, while FasterTransformer performs\nbetter for larger batch size and longer se-\nquence.\n•The speedup in generation tasks are not as\nlarge as machine translation. It is mainly\nbecause of the lower complexity of sam-\npling methods than beam search, reducing the\nbeneﬁts obtained from operation fusion and\nHARS.\n4 Conclusion\nIn this paper, we address the deployment problem\nof expensive sequence models and present an efﬁ-\ncient inference library LightSeq for sequence pro-\ncessing and generation, reducing the gap between\nthe performance of big models and the require-\nment of online services. Comparisons with Faster-\nTransformer show that we perform better in both\nmachine translation and text generation. In future\nwork, we will focus on exploring more techniques\nto achieve a more signiﬁcant speedup, including ef-\nﬁcient integer-arithmetic-only inference and sparse\nGEMM computations.\nAcknowledgments\nWe would like to thank the colleagues in machine\ntranslation service and advertisement service to\nsupport our experiments in online environments\nand apply LightSeq into real-time systems.\nReferences\nMart´ın Abadi, Michael Isard, and Derek Gordon Mur-\nray. 2017. A computational model for tensorﬂow:\nan introduction. In Proceedings of the 1st ACM SIG-\nPLAN International Workshop on Machine Learning\nand Programming Languages, MAPL@PLDI 2017,\nBarcelona, Spain, June 18, 2017, pages 1–7. ACM.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nTianqi Chen, Thierry Moreau, Ziheng Jiang, Lian-\nmin Zheng, Eddie Yan, Haichen Shen, Meghan\nCowan, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos\nGuestrin, and Arvind Krishnamurthy. 2018. TVM:\nAn automated end-to-end optimizing compiler for\ndeep learning. In 13th USENIX Symposium on Op-\nerating Systems Design and Implementation (OSDI\n18), pages 578–594, Carlsbad, CA. USENIX Asso-\nciation.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car-\nbonell, Quoc Viet Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language models\nbeyond a ﬁxed-length context. In Proceedings of\nthe 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers , pages\n2978–2988. Association for Computational Linguis-\ntics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171–4186. Association for Computa-\ntional Linguistics.\nSergey Edunov, Myle Ott, Michael Auli, and David\nGrangier. 2018. Understanding back-translation at\nscale. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\nBrussels, Belgium, October 31 - November 4, 2018 ,\npages 489–500. Association for Computational Lin-\nguistics.\nJiarui Fang, Yang Yu, Chengduo Zhao, and Jie Zhou.\n2021. Turbotransformers: an efﬁcient GPU serv-\ning system for transformer models. In PPoPP ’21:\n26th ACM SIGPLAN Symposium on Principles and\nPractice of Parallel Programming, Virtual Event, Re-\npublic of Korea, February 27- March 3, 2021, pages\n389–402. ACM.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text\ndegeneration. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nYi-Te Hsu, Sarthak Garg, Yi-Hsiu Liao, and Ilya\nChatsviorkin. 2020. Efﬁcient inference for neural\nmachine translation. In Proceedings of SustaiNLP:\nWorkshop on Simple and Efﬁcient Natural Language\nProcessing, pages 48–53, Online. Association for\nComputational Linguistics.\nYoung Jin Kim and Hany Hassan. 2020. FastFormers:\nHighly efﬁcient transformer models for natural lan-\nguage understanding. In Proceedings of SustaiNLP:\nWorkshop on Simple and Efﬁcient Natural Language\nProcessing, pages 149–158, Online. Association for\nComputational Linguistics.\n120\nZehui Lin, Xiao Pan, Mingxuan Wang, Xipeng Qiu,\nJiangtao Feng, Hao Zhou, and Lei Li. 2020. Pre-\ntraining multilingual neural machine translation by\nleveraging alignment information. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2020, On-\nline, November 16-20, 2020 , pages 2649–2663. As-\nsociation for Computational Linguistics.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2015, Lisbon, Portu-\ngal, September 17-21, 2015, pages 1412–1421. The\nAssociation for Computational Linguistics.\nWeizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu,\nNan Duan, Jiusheng Chen, Ruofei Zhang, and Ming\nZhou. 2020. Prophetnet: Predicting future n-gram\nfor sequence-to-sequence pre-training. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: Findings, EMNLP\n2020, Online Event, 16-20 November 2020 , pages\n2401–2410. Association for Computational Linguis-\ntics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nHan Vanholder. 2016. Efﬁcient inference with tensorrt.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017a. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017b. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, 4-9 Decem-\nber 2017, Long Beach, CA, USA, pages 6000–6010.\nAshwin K. Vijayakumar, Michael Cogswell, Ram-\nprasaath R. Selvaraju, Qing Sun, Stefan Lee, David J.\nCrandall, and Dhruv Batra. 2018. Diverse beam\nsearch for improved description of complex scenes.\nIn Proceedings of the Thirty-Second AAAI Confer-\nence on Artiﬁcial Intelligence, (AAAI-18), the 30th\ninnovative Applications of Artiﬁcial Intelligence\n(IAAI-18), and the 8th AAAI Symposium on Educa-\ntional Advances in Artiﬁcial Intelligence (EAAI-18),\nNew Orleans, Louisiana, USA, February 2-7, 2018 ,\npages 7371–7379. AAAI Press.\nJiacheng Yang, Mingxuan Wang, Hao Zhou, Chengqi\nZhao, Weinan Zhang, Yong Yu, and Lei Li. 2020.\nTowards making the most of BERT in neural ma-\nchine translation. In The Thirty-Fourth AAAI Con-\nference on Artiﬁcial Intelligence, AAAI 2020, The\nThirty-Second Innovative Applications of Artiﬁcial\nIntelligence Conference, IAAI 2020, The Tenth AAAI\nSymposium on Educational Advances in Artiﬁcial In-\ntelligence, EAAI 2020, New York, NY, USA, Febru-\nary 7-12, 2020, pages 9378–9385. AAAI Press."
}