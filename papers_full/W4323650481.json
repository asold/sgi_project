{
    "title": "Large Language Models as Zero-Shot Human Models for Human-Robot Interaction",
    "url": "https://openalex.org/W4323650481",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2228917092",
            "name": "Zhang Bo-wen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3180231472",
            "name": "Soh, Harold",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4221154893",
        "https://openalex.org/W3044207527",
        "https://openalex.org/W4224912544",
        "https://openalex.org/W2188746823",
        "https://openalex.org/W4283330306",
        "https://openalex.org/W2930317805",
        "https://openalex.org/W4293093109",
        "https://openalex.org/W2101821104",
        "https://openalex.org/W2594035753",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W4286902222",
        "https://openalex.org/W3129281515",
        "https://openalex.org/W2945459164",
        "https://openalex.org/W2964132611",
        "https://openalex.org/W2165698048",
        "https://openalex.org/W2970062726",
        "https://openalex.org/W4319452268",
        "https://openalex.org/W2411577903",
        "https://openalex.org/W2964147651",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W2907855292",
        "https://openalex.org/W2969622979",
        "https://openalex.org/W4320559489",
        "https://openalex.org/W1983467315",
        "https://openalex.org/W4292947474",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4321277158",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W4281690148",
        "https://openalex.org/W2963007220",
        "https://openalex.org/W4221152848"
    ],
    "abstract": "Human models play a crucial role in human-robot interaction (HRI), enabling robots to consider the impact of their actions on people and plan their behavior accordingly. However, crafting good human models is challenging; capturing context-dependent human behavior requires significant prior knowledge and/or large amounts of interaction data, both of which are difficult to obtain. In this work, we explore the potential of large-language models (LLMs) -- which have consumed vast amounts of human-generated text data -- to act as zero-shot human models for HRI. Our experiments on three social datasets yield promising results; the LLMs are able to achieve performance comparable to purpose-built models. That said, we also discuss current limitations, such as sensitivity to prompts and spatial/numerical reasoning mishaps. Based on our findings, we demonstrate how LLM-based human models can be integrated into a social robot's planning process and applied in HRI scenarios. Specifically, we present one case study on a simulated trust-based table-clearing task and replicate past results that relied on custom models. Next, we conduct a new robot utensil-passing experiment (n = 65) where preliminary results show that planning with a LLM-based human model can achieve gains over a basic myopic plan. In summary, our results show that LLMs offer a promising (but incomplete) approach to human modeling for HRI.",
    "full_text": "Large Language Models as Zero-Shot Human Models for\nHuman-Robot Interaction\nBowen Zhang1 and Harold Soh 1,2\nAbstract— Human models play a crucial role in human-robot\ninteraction (HRI), enabling robots to consider the impact of\ntheir actions on people and plan their behavior accordingly.\nHowever, crafting good human models is challenging; capturing\ncontext-dependent human behavior requires significant prior\nknowledge and/or large amounts of interaction data, both of\nwhich are difficult to obtain. In this work, we explore the\npotential of large language models (LLMs) — which have\nconsumed vast amounts of human-generated text data — to act\nas zero-shot human models for HRI. Our experiments on three\nsocial datasets yield promising results; the LLMs are able to\nachieve performance comparable to purpose-built models. That\nsaid, we also discuss current limitations, such as sensitivity to\nprompts and spatial/numerical reasoning mishaps. Based on\nour findings, we demonstrate how LLM-based human models\ncan be integrated into a social robot’s planning process and\napplied in HRI scenarios focused on the important element of\ntrust. Specifically, we present one case study on a simulated\ntrust-based table-clearing task and replicate past results that\nrelied on custom models. Next, we conduct a new robot utensil-\npassing experiment ( n = 65) where preliminary results show\nthat planning with an LLM-based human model can achieve\ngains over a basic myopic plan. In summary, our results show\nthat LLMs offer a promising (but incomplete) approach to\nhuman modeling for HRI.\nI. I NTRODUCTION\nHuman models endow robots with the ability to reason\nabout humans, enabling human-centered and personalized\nrobot behavior [1]. Despite the advantages, accurate human\nmodeling remains a significant challenge. Handcrafted hu-\nman models typically encode strong assumptions (e.g. [2]–\n[5]), which can limit flexibility and are challenging to scale\nup to real-world settings. An alternative approach is to\nuse non-parametric data-driven models(e.g. [6], [7]). These\nmodels have become popular of late due to the success of\ndeep learning in a variety of fields, from computer vision\nto game-playing. However, these methods typically require\nlarge amounts of human interaction data that is difficult and\ncostly to collect.\nIn this work, we attempt to circumvent these issues by\nusing large pre-trained or “foundation” models. Specifically,\nwe examine large language models (LLMs) that have caused\nexcitement due to their impressive abilities to generate\nplausible human-sounding text and their performance on\ndownstream tasks that they were not explicitly trained for.\nThis research is supported by the National Research Foundation Singa-\npore and DSO National Laboratories under the AI Singapore Programme\n(AISG Award No: AISG2-RP-2020-017).\n1Dept. of Computer Science, National University of Singapore\n{bowenzhang, harold}@comp.nus.edu.sg. 2Smart Systems In-\nstitute (SSI), NUS.\nState and Robot Action\nTurn 1: The robot chose to pass a spatula and succeededTurn 2: The robot chose to pass an egg whisk and succeeded…\nNow the robot chooses to pass a pair of scissors.\nQuestion: What will the human do?A. Stay put, B. Intervene\nPrompt\nHuman Action/StateDistribution\nLLM\nLLM-based Human Model\nRobot Policy\nInteraction History…t=1t=2\n(PO)MDPPlanner\nFig. 1: In this work, we explore how large language models\n(LLMs) can be used as zero-shot human models in HRI.\nWe first evaluate the effectiveness of such models using\nbenchmark datasets. Then, we demonstrate how LLM-based\nmodels can be used in planning for two trust-related HRI\nscenarios.\nHere, we ask: can LLMs function effectively as human\nmodels for HRI?\nSince LLMs have been trained on vast amounts of human-\ngenerated text, it is conceivable that they have captured\nelements of human behavior. If the answer to our question\nturns out in the affirmative, LLMs would be a boon for the\nHRI community, particularly those of us developing/using\ncomputational models for human-robot collaboration and\ninteraction [8], [9]. We are especially interested in the LLMs’\nzero-shot capabilities since no further human data is required.\nOn the other hand, there are good reasons to expect LLMs\nto be poor substitutes for existing human models — HRI\ntasks involve both social and physical aspects, and recent\nevidence indicates that LLMs can fail spectacularly on tasks\nthat require skills such as physical reasoning that go beyond\nlinguistic competence (e.g. [10]–[13]).\nWe first present an empirical study that shows that LLMs\nindeed well-capture human latent states and behavior. Using\nthree different datasets, we show that two state-of-the-art\nLLMs (FLAN-T5 and a variant of GPT-3.5) can achieve\npredictive performance comparable to specialized machine\nlearning models. This was achieved using a zero-shot ap-\nproach, without the use of any additional training data .\narXiv:2303.03548v2  [cs.RO]  2 Oct 2024\nWhile it is tempting to conclude that LLMs are effective\nhuman models, the situation is more complex. A deeper\nanalysis of our results shows that the LLMs underperformed\non tasks that require spatial and numerical reasoning and\nare sensitive to prompt syntax. This can make application in\nreal-world HRI scenarios challenging and suggest that LLMs\nmay be best used as “task-level” human models.\nTo take a step forward, we explore how LLM-based human\nmodels can be used in a planning setup for HRI (Fig. 1). We\nfirst experiment with a simulated table-clearing setup used in\nprior work on human-robot trust [5] and achieve comparable\nresults. We then discuss preliminary results from a new\nutensil-passing experiment with n = 65participants. In this\nmore complex setup, a planner equipped with an LLM-based\nhuman model generated a reasonable plan that mitigated\nover-trust. Our experiment also revealed that some partici-\npants based their judgments on the physical characteristics of\nobjects, which our model did not account for. This suggests\nthe complexity of HRI tasks may require human models\nthat combine LLMs with “lower-level” models that consider\ngeometry, motion, and other physical characteristics, which\nwould make interesting future work.\nII. P RELIMINARIES AND RELATED WORKS\nLarge Language Models. A Language Model (LM) is a\ndistribution over sequences of word tokens. Autoregressive\nLMs model the conditional distribution p(wk|w<k) over wk,\nthe next token at the kth position, given a sequence of\nprevious input tokens w<k. State-of-the-art LLMs have tens\nor hundreds of billions of parameters, e.g. OpenAI’s GPT-\n3 [14] and Google’s PaLM [15]). When trained with massive\namounts of text data in a self-supervised manner, these LLMs\nare not only capable of generating human-like text but also\ndemonstrate impressive performance on downstream tasks.\nIn this work, we focus on the ability of LLMs to generate\nplausible human text that is consistent with prior information\nfed in as a sequence of word tokens called a prompt.\nOur work is related to very recent research on replicating\nhuman behavior on classic psychological/economic experi-\nments with LLMs [16] and the possible emergence of theory-\nof-mind in LLMs [17], which remains highly debatable [18].\nThe key difference is that we focus on the applicability\nof LLMs to HRI, which involves scenarios that include a\nrobot and physical-social interaction. Another related thread\nof work is in planning with language models. Prior work has\nused LLMs to predict object affordance [19] or to directly\ngenerate action plans [12]. However, recent work has also\nshown that LLMs can be poor planners [11]. Our work differs\nfrom the above as we propose to use LLMs to approximate\nhuman behavior and use the LLM-based human models for\nplanning to obtain a suitable robot policy.\nHuman Models for HRI. In general, modeling humans is a\nlarge interdisciplinary endeavor and here, we focus on com-\nputational models for HRI. We briefly review recent work\nand refer readers wanting more comprehensive coverage to\nexcellent survey articles [1], [8].\nExisting approaches to modeling human behavior in HRI\ncan be broadly categorized into two paradigms: Theory-of-\nMind (ToM) models and ‘black-box’ data-driven models.\nToM models broadly refer to methods that incorporate a set\nof assumptions about human mental processing and behavior.\nFor example, Bayesian Theory of Mind (BToM) assumes hu-\nmans behave rationally and update their beliefs in a Bayesian\nmanner [4], [20], [21]. In contrast, ‘black-box’ models make\nfew assumptions and model human behavior in a data-\ndriven manner (e.g. [7]). Such methods typically require\nlarge amounts of real human data, which can be difficult\nto obtain. Recent approaches have proposed training agents\nin a multi-agent manner, e.g., Fictitious Co-Play [22], which\nobviates the need for human data. However, these RL-based\nmethods are computationally expensive to train and require\nsignificant amounts of interaction data that may be costly\nor impossible to acquire in the real world. A recent hybrid\nmethod, MIRROR [6], models a human based on a robot’s\ninternal self-model (trained with RL) and then uses a small\namount of human data to adapt the model to a particular\nindividual. However, it is unclear if MIRROR or any of\nthe above methods can well capture human mental states,\ne.g., emotions, which are crucial in many real-world settings.\nHere, we explore whether LLM-based human models can\novercome the above shortcomings.\nIII. LLM S AS ZERO -SHOT HUMAN MODELS\nA. Problem Statement\nConsider an environment that is populated by N agents\na(i) ∈ Afor i = 1, . . . , N. An agent may be a human\nor a robot. Given an environment state st ∈ Sat time t\nand a history of interactions ht = {(sk, {u(i)\nk }i)}t−1\nk=1 ∈ H\nwhere sk represents the environment state at time k and u(i)\nk\nrepresents the action of agent i at time k, we aim to model\na specific property zH\nt ∈ ZH of one of the human agents\naH ∈ A. More precisely, zH\nt is an abstract scenario-specific\nrandom variable (e.g., a human action or mental state) and we\nseek to model the distribution p(zH\nt |st, ht). For example, zH\nt\nmay be the human’s trust in the robot to perform a specific\ntask.\nWe assume that the environment state, agents, and history\ncan be textualized via a function f to a prompt, xt =\nf(st, ht, A). We denote the LLM-based human model as\npl(zH\nt |xt) where for simplicity, we assume that the LLM\ncan output a distribution over zH\nt or that a suitable mapping\ncan be defined. Our study aims to evaluate if pl(zH\nt |xt) well\napproximates p(zH\nt |st, ht).\nB. Models, Datasets, and Prompts\nModels. We work with two different LLMs: (i) text-davinci-\n003 (175B) [27] and (ii) FLAN-T5-XXL (11B) [28], which\nwe will refer to as D AVINCI and T5, respectively. D AVINCI\na large model that is finetuned with human feedback and\nachieves state-of-the-art performance. T5 is a smaller open-\nsource LLM optimized with instruction fine-tuning. For our\nexperiments, we ran T5 on a local workstation and accessed\nGiven a description of a scenario which includes a robot and several humans, a human evaluator has to answer a question about whether it is socially appropriate for the robot to carry out a certain action in the given scenario. \nScenario: Inside a living room, there are 6 humans in total. There are 4 humans in a group. The group has a radius of 0.92 meters. … (rest of environment description)\nQuestion: Is it socially appropriate for the robot to vacuum clean towards the direction the robot is facing? Answer choices: A. very inappropriate, B. inappropriate, C. neutral, D. appropriate, E. very appropriateAnswer:\nMANNERS-DB Trust-Transfer\n(description of experiment details)Question: The robot is going to perform the task ‘Navigate while avoiding obstacles'. I trust that the robot can perform the task successfully. Answer: 6(rest of initial trust questions)\nThen the participant observes the two demonstrations from the robot. First, the robot performs the task ‘Navigate while avoiding moving people’ and fails.(rest of description of demonstrations)\nAfter observing these demonstrations, the participant is again requested to indicate their trust by answering the following agreement questions via a 7-point Likert scale. Question: The robot is going to perform the task 'Navigate to the living room'. I trust that the robot can perform the task successfully.Answer:\nSocialIQA\nContext: Tracy had accidentally passed upon Austin in the small elevator and it was awkward.\nQuestion: Why did Tracy do this? Answer choices: A. Get very close to Austin, B. Squeeze into the elevator, C. Get ﬂirty with AustinAnswer:\nFig. 2: Datasets and Example Prompts in Prediction Experiments. We use two HRI datasets: MANNERS-DB [23] and\nTrust-Transfer [24], [25], and included SocialIQA [26], a general social reasoning benchmark for human interactions. For\neach dataset, we show an illustrative image (reproduced from [23], [25], [26], respectively) and an example prompt.\nDAVINCI using the OpenAI API. Both LLMs can either\nprovide samples or a distribution over next tokens.\nDatasets. We use two HRI-related datasets:\n• MANNERS-DB [23], a recent dataset for assessing\nthe appropriateness of robot actions in specific social\ncontexts. The test set comprises 800 instances, where\neach instance is a simulated household scenario and\na description of a robot action. The scenarios are\ndivided into two sets based on whether the robot actions\nare executed within a circle surrounding itself or in\na direction indicated by an arrow. Each instance is\nlabeled by 15 human annotators, who are presented\nwith an image of a simulated scene and asked to rate\nthe degree of social appropriateness of the robot action\nfrom 1 (very inappropriate) to 5 (very appropriate).\nThis dataset enables us to evaluate whether LLM-based\nhuman models can capture the effects of physical and\nspatial features in HRI.\n• Trust-Transfer [24] is a dataset on how human trust in\na robot’s capability changes after observing it perform\ntasks. The dataset encompasses two different domains\n(household and driving) and contains 189 instances.\nEach instance includes a participant’s initial level of\ntrust on three test tasks, two observed demonstrations,\nand the participant’s final level of trust on the same\nthree test tasks after observing the demonstrations.\nParticipants were asked to indicate their level of trust in\nthe robot using a 7-point Likert scale. We included the\nTrust-Transfer dataset (i) to evaluate whether LLMs can\ncapture the effect of interaction history on a human’s\nmental state, and (ii) because the dataset is closely\nrelated to our trust-based case studies in Sec. IV.\nand a general human-human social interaction dataset:\n• SocialIQA [26], a benchmark designed to measure the\nability to perform social commonsense reasoning. The\ntest split contains 1954 instances, each comprising a\ncontext, a question, and three answer choices with\none ground truth label. Performing well on SocialIQA\nwould demonstrate LLM’s ability to reason about hu-\nman social interactions, including intent. For example,\na robot could be observing an interaction between two\nor more humans and has to infer their mental states.\nPrompt Design. The precise prompts used varied between\ndatasets (see Fig. 2 for examples) but in general, we follow\nthe practice suggested by [16] to craft clear templated\nprompts that minimize the invalid completions, e.g. the prob-\nability of generating anything other than the listed choices\nwhen given a multiple-choice question. The prompts were\ndesigned to include:\n• Information about the experiment that was provided to\nhuman participants in the original HRI/social experi-\nment, e.g., rules of the task and features of the robot;\n• The interaction history, current environment, and a de-\nscription of the robot aR’s action (in the HRI datasets);\n• A query to probe LLM’s response. To extract a prob-\nability distribution over zH\nt , we frame our prompts\nsuch that the number of valid completions is limited\nto elements in ZH; we assign a single-token label to\neach zH via a multiple choice question or a Likert scale\nquestion1. As such, the length of each valid completion\nis a single token and we avoid favoring short answers\nwhen comparing token probabilities.\nDue to space restrictions, we provide specific details on\nthe prompts for each dataset in an online supplementary\nmaterial [29].\nC. Baseline Models, Evaluation Criteria, and Limitations\nBaseline Models. We compared the LLMs to specialized\npublished models created for each of the datasets:\n• BNN-16CL2 [23], a Bayesian continual learning model\ntrained on the MANNERS-DB dataset to predict social\nappropriateness of robot actions.\n• GPNN, a hybrid Gaussian Process and Neural Network\nmodel developed for predicting Trust-Transfer and ap-\nplied to the Trust-Transfer dataset.\n• BERT-large [30], a 340M-parameter language model\nfine-tuned as a classifier for the SocialIQA dataset.\nWhen reporting results, we use the umbrella term BASELINE\nmodel to refer to the respective model for each dataset.\nEvaluation Measures. We primarily evaluated the above\nmodels using an error score and a Consistency with Mode\n(CwM) score. The error score captures how well a given\nmodel’s output coincides with the dataset targets on av-\nerage. This score is dataset-dependent and refers to the\nmetric used by the authors for each dataset; this allowed\nus to directly compare against previously reported numbers\n(when available). The error scores are RMSE (root mean\nsquare error), MAE (mean absolute error), and error rate for\nMANNERS-DB, Trust-Transfer and SocialIQA, respectively.\nFor simplicity, we will refer to these different measures as\nsimply the “error score”.\nThe CwM score measures the consistency between the\nLLMs and the majority human label. To produce a consistent\nscore across the different datasets, we first binarize the\ntargets; for MANNERS-DB, if a majority of annotators rated\nthe appropriateness level of a robot action as ≥ 3 (neutral),\nwe labeled it as socially acceptable (otherwise, it was labeled\nas not socially acceptable). An analogous operation was\nperformed for the Trust-Transfer dataset for scores ≥ 4. The\nSocialIQA dataset only provides a single label per instance\nand we took this as the mode.\nTo give us a sense how well the distribution pl(zH\nt |xt)\nmatches p(zH\nt |st, ht−1), we include two additional mea-\nsures: (i) an entropy relative similarity score S(q, p) =P\nzH q(zH) logq(zH)P\nzH p(zH) logp(zH) , and (ii) the 2-Wasserstein metric\nW2(q, p). We set q to be the model output and p to be\nthe target empirical distribution. Intuitively, the entropy of\na distribution captures how “broad” the distribution is or\n1The LLMs occasionally generated invalid completions ( < 3% of the\ninstances in our experiments). These are placed in a single “catch-all” class\nand marked as errors.\n2We ran BNN-16CL using code provided by the authors at https://\ngithub.com/jonastjoms/MANNERS-DB. Our reported numbers are\ndifferent from [23] possibly due to a difference in the test dataset split.\nits inherent uncertainty. The S(q, p) score reveals whether\na model output distribution q captures the spread of the\ntarget p or if it is under/over-confident — a value of 1\nindicates the same entropy, while S(q, p) < 1 indicates a\nnarrower spread (and vice-versa). The 2-Wasserstein metric\ncaptures how similar two distributions are; it measures how\nmuch probability mass has to be moved to transform q to p.\nNote that these two distribution-related measures could only\nbe computed for MANNERS-DB (since Trust-Transfer and\nSocialIQA only contain one label per instance).\nLimitations. For both MANNERS-DB and SocialIQA, the\nlabels are provided by third-party humans who are not\nactually part of the environment. Based on prior work [31],\n[32], we assume that human annotators are able to pro-\nvide sound assessments of another person’s mental state\nor behavior. Next, reliance on handcrafted prompts is an\nimportant limitation, better performance may be possible\nwith alternative prompt structures. Finally, an issue when\nevaluating LLMs is the risk of “data leakage”, i.e., a given\ndataset has been seen by the LLM during training. Since\ntraining datasets for both D AVINCI and T5 are not readily\navailable, we are unable to confirm whether this was the\ncase. That said, data leakage for SocialIQA is unlikely [33]\nand the HRI datasets (prompts and answers) were created\nfor this study and hence, have not been directly seen by the\nLLMs. However, the raw datasets on which our prompts are\nbased are publicly available and LLMs may have either seen\nthe raw data or relevant papers. We attempt to mitigate this\nby extending the trust task with items that were not in the\noriginal dataset [25] in Sec. IV-C.\nD. Results\nThe error and CwM scores are summarized in Table I.\nAcross all three datasets, D AVINCI achieves surprisingly\ngood zero-shot performance, sometimes achieving lower\nerror scores than the specialized models. T5 also achieves\ngood scores on the MANNERS-DB and SocialIQA datasets\nbut performs poorly on the Trust-Transfer datasets; we delve\ninto the reasons for this subpar performance in later analysis.\nThe CwM scores indicate that the LLMs achieve human-\nlevel consistency to the majority label on the MANNERS-DB\nand SocialIQA datasets. We observe disagreement between\nhuman annotators in the SocialIQA dataset since the average\nannotator is only 68.4% consistent with the majority.\nTable I also shows the distribution measures on\nMANNERS-DB. In general, the baseline (a Bayesian model)\nbest captures the target distribution. DAVINCI tends to under-\ncapture the spread of the distribution, as indicated by its\nlower entropy similarity S(q, p) scores. In contrast, T5 has\nscores > 1, suggesting an overly broad distribution, but one\nthat better matches the empirical human label distribution;\nthe Wasserstein metric is consistently lower for T5 compared\nto D AVINCI in all contexts.\nTaken together, the above results indicate that the LLMs\nachieve performance comparable to strong baseline models.\nWe emphasize that this was achieved in a zero-shot manner,\nTABLE I: Error and CwM Scores and Distribution Measures. S(q, p) closer to 1 indicates better capture of the spread of\nhuman distribution.\nError Score(↓) CwM(↑) S(q,p) W2(q,p) (↓)\nDataset Context DAVINCI T5 BASELINE DAVINCI T5 H UMAN DAVINCI T5 BASELINE DAVINCI T5 BASELINE\nMANNERS Circle 0.787 0.708 0.876 0.730 0.750 0.760 0.467 1.354 1.127 0.168 0.164 0.173\nArrow 0.695 0.762 0.664 0.734 0.632 0.717 0.586 1.226 1.199 0.206 0.141 0.124\nOverall 0.737 0.739 0.764 0.733 0.684 0.736 0.645 1.281 1.159 0.189 0.151 0.145\nTrust-Transfer Household 0.162 0.211 0.156 0.896 0.760 - - - - - - -\nDriving 0.154 0.211 0.163 0.806 0.720 - - - - - - -\nOverall 0.158 0.211 0.159 0.852 0.741 - - - - - - -\nSocialIQA Overall 0.278 0.181 0.340 0.722 0.819 0.684 1 - - - - - -\n0.8692 - - - - - -\n1 Average human performance, reported in https://github.com/google/BIG-bench.\n2 Best human performance, reported in [26].\nTABLE II: CwM Scores obtained by D AVINCI for each\nindividual Action Type in MANNERS-DB.\nRobot Action CwM (↑) Precision (↑) Recall (↑)\nVacuum cleaning 0.560 0.506 0.933\nMopping the floor 0.520 0.478 1.000\nCarry warm food 0.740 0.875 0.814\nCarry cold food 0.670 0.944 0.698\nCarry drinks 0.940 0.959 0.979\nCarry small objects 0.960 0.960 1.000\nCarry big objects 0.710 0.583 0.600\nCleaning (Picking up items) 0.886 0.886 1.000\nStarting conversation 0.661 0.755 0.841\nsuggesting that LLMs can be effective zero-shot human\nmodels for HRI without further training or fine-tuning .\nE. Discussion and Analysis on HRI Datasets\nThe results above are promising, but the scores also show\nthat the LLMs are imperfect models, e.g., T5 performs\npoorly on the Trust-Transfer dataset. Next, we provide further\nanalysis into the errors made by the LLMs to highlight\npitfalls and potential improvements.\nLLMs can perform poorly on HRI tasks that require spa-\ntial/physical/numerical reasoning. This issue can be seen\nvia a breakdown of the LLM’s performance on MANNERS-\nDB. Table II shows the CwM scores for DAVINCI , along with\nprecision and recall, on specific actions in the dataset. T5\nscores are similar and we relegate the T5 CwM scores (along\nwith a table comparing RMSE values across the models)\nto the online appendix. Relatively low scores were obtained\nfor ‘Vacuum cleaning’ and ‘Mopping the floor’, which are\nthe two most intrusive actions in the dataset and require\nan understanding of personal space [23], which requires\nphysical/spatial and numerical reasoning given the prompt\n— tasks that are known to be difficult for LLMs [11], [13].\nWe also observe lower performance on ‘Carry big objects’\nand ‘Starting a conversation’, which are also actions where\nsocial appropriateness found to be highly correlated with\ninter-agent distances [23].\nTo further investigate this issue, we examined the 89\nfailure cases of ‘Vacuum cleaning’ and ‘Mopping the floor’.\nThrough manual inspection of the scene images and annota-\ntors’ comments, we identified 56 failure scenarios where the\nrobot’s action clearly invades a human’s personal space, e.g.,\na human is standing in the robot’s intended working area, but\n... (description of experiment details)\nThe participant rates their trust on the task ’Pick and place a plastic\ncan’ as 5 out of 7.\n... (rest of initial trust descriptions and description of demonstra-\ntions)\nGiven these demonstrations and the initial trust, now the partici-\npant will rate their trust on the task ‘Pick and place a plastic can’\nas\nFig. 3: Example altered prompt used in trust-transfer exper-\niment in the household domain.\nTABLE III: Results on Trust-Transfer using Altered Prompts.\nError Score(↓) CwM( ↑)\nDomain DAVINCI T5 BASELINE DAVINCI T5\nHousehold 0.163 0.170 0.156 0.875 0.796\nDriving 0.155 0.160 0.163 0.806 0.802\nOverall 0.159 0.165 0.159 0.841 0.799\nthe LLM predicts that the action is socially appropriate. For\nthese instances, we directly queried the LLM with a Yes-No\nquestion: “ Does the robot’s designated working area intrude\non anybody’s personal space?”. Both D AVINCI and T5\ngave wrong answers to all 56 questions. We also attempted\nZero-Shot Chain-of-Thought prompting on D AVINCI , which\nhas been shown to elicit more reasonable responses [10].\nPerformance improved but remained unsatisfactory (15/56\ncorrect, 26.7%). An instructive failure response is as follows:\n“First, the closest human is 0.53 meters away from\nthe robot. This means that the robot’s designated\nworking area of 2.00 meters will not intrude the\nclosest human’s personal space.”\nWe observed that the response is linguistically correct and\nleverages information in the prompt, but the drawn conclu-\nsion is incorrect since the human is present in the cleaning\narea.\nLLMs are sensitive to the prompt structure. Recall that\nT5 displayed subpar performance on Trust-Transfer. An\nexamination of T5’s outputs revealed that it had a strong\ntendency to predict participants’ post-observation trust to\nbe the same as their initial trust. We hypothesized that\nthis was caused by our prompt resembling the structure\nof few-shot prompts, which biased the model into copying\nthe initial answers since the questions were identical (see\nFig. 2). To test our conjecture, we designed an alternative\nprompt that altered how the initial trust and the query are\npresented (Fig 3). Table III shows that the new prompt style\nsignificantly improves T5’s performance.\nSummary. Our study shows that LLMs are surprisingly\nadept at functioning as zero-shot human models. That said,\nour analysis also reveals that (i) LLMs can perform poorly on\nHRI tasks that require spatial and numerical understanding\nand (ii) LLMs are sensitive to prompt design. These findings\nadd to the growing literature on the strengths and limitations\nof LLMs [11], [13], [16]–[18]. Within the context of HRI,\ncare should be taken in the application of LLMs; HRI\nis typically grounded in the real world and thus, involves\nspatial and physical reasoning. Overall, our results suggest\nthat LLMs are better-suited as task-level (symbolic) hu-\nman models, and alternative “low-level” models may\nbe needed to account for geometry and motions in a\ncontinuous space.\nIV. C ASE -STUDIES : PLANNING WITH LLM- BASED\nHUMAN MODELS\nIn this section, we move from prediction to planning for\nhuman-robot interaction. Leveraging insights gained from\nour study, we demonstrate how an LLM-based human model\ncan be used in a planner for HRI. We begin by replicating\na prior study on trust in a table-clearing task and then move\non to a new utensil-passing experiment.\nA. Problem Statement\nGiven our previous findings, our HRI scenarios are framed\nat the task level and formalized as MDPs. At each time-step\nt, a world state st is fully observed by both the human H and\nthe robot R. The robot takes an action uR\nt and in response,\nthe human takes an action uH\nt . The world state then changes\naccording to a transition function p(st+1|st, uH\nt , uR\nt ) and a\nreward r(st, uH\nt , uR\nt , st+1) is received. The robot’s optimal\npolicy πR\n∗ is therefore,\nπR\n∗ = arg max\nπR\nE\nuH\nt ∼πH,uR\nt ∼πR\n∞X\nt=0\nγtr(st, uH\nt , uR\nt , st+1)\nwhere γ is the discount rate. We approximate the unknown\nhuman policy πH with pl(zH\nt |xt) where zH\nt = uH\nt and\nxt = f(st, ht, uR\nt , {H, R}) and assume known transition\ndynamics. As such, the robot decision process becomes a\nMDP and off-the-shelf planners can be used. In the following\nexperiments, we use simple offline value iteration since the\nMDPs are relatively small with short planning horizons.\nFuture work can investigate more complex scenarios using\nmore sophisticated planners and LLM-based models.\nB. Table-clearing experiment\nExperiment Setup. We first test our LLM-based planner on a\nsimulated table-clearing experiment [5], where a human and\na robot collaborate to clear objects off a table. The objects\ninclude three water bottles, one fish can, and one wine glass.\nAt each time step, the robot chooses one of the objects to\n... (description of experiment setup and rules)\nTurn 1: Robot choice: plastic bottle; Human choice: stay put;\nOutcome: the robot successfully removes the plastic bottle.\n(Include “The human’s trust in the robot increased.” in the case of\nTC)\n... (rest of interaction history)\nQuestion: Now the robot chooses to remove the wine glass,\nwhat will the human do? Answer choices: A. intervene, B. stay put.\nOR in the case ofYN:\nQuestion: Will the human trust the robot to remove the wine glass\nnow? Answer choices: A. Y es, B. No.\nFig. 4: Example prompt used in table-clearing experiment.\nremove. The human then chooses whether to intervene and\npick up the object, or stay put and let the robot remove the\nobject. If the human stays put and the robot succeeds, they\nwill get a reward based on the object: 1 for plastic bottle, 2\nfor fish can, and 3 for wine glass. However, if they stay put\nand the robot fails, they will receive a penalty: no penalty\nfor plastic bottle, 4 for fish can and 9 for wine glass. If they\nchoose to intervene, they will receive no reward or penalty. It\nis assumed that the robot will never fail but this information\nis not revealed to the human participant.\nPrompt Design. As previously discussed in Sec. III-E,\nprompt structure is important for eliciting correct responses\nfrom LLMs. Here, we consider a standard prompt with\nvariations (example in Fig. 4):\n• TC: We explicitly include the Trust Change in each turn\n(the most likely post-observation trust change predicted\nby the LLM model) using a multiple-choice question\nwith options {increased, decreased, unchanged }\n• YN: Instead of asking which action the human will take\nwhen the robot chooses to remove an object, the prompt\nasks a Yes-No question about whether the human will\ntrust the robot to do so. We assume a deterministic\nrelationship between trust and human action, i.e., the\nhuman will intervene if they do not trust the robot to\nperform the task and stay put otherwise.\nResults. We compare against the TRUST -POMDP plan-\nner [5] using the authors’ simulation code. The results of\n104 simulation runs are shown in Table IV. All D AVINCI -\nbased planners are consistently better than their T5 counter-\nparts and achieve comparable performance with the TRUST -\nPOMDP planner. Using both TC and YN prompting im-\nproved the performance for T5 and DAVINCI -based planners.\nC. Utensil-passing Experiment\nExperiment Setup. In this scenario, a human is washing\nutensils in a kitchen and a robot (a Franka Emika Panda\nrobot arm) is helping to pass dirty utensils to them. The\nobjects include a spatula, an egg whisk, a pair of scissors,\nand a knife (Fig. 5). At each time step, the robot chooses one\nof the objects to pass. The human then chooses between two\nTABLE IV: Simulated Table-clearing Experiment Results.\nMean Return ( ↑) Interv. prob. on Glass ( ↓)\nDAVINCI -TC-YN 6.17 (0.034) 0.352\nDAVINCI -YN 6.13 (0.034) 0.366\nDAVINCI -TC 6.15 (0.034) 0.357\nDAVINCI 6.14 (0.034) 0.360\nT5-TC-YN 6.10 (0.034) 0.368\nT5-YN 6.01 (0.035) 0.398\nT5-TC 5.94 (0.034) 0.395\nT5 5.95 (0.035) 0.405\nTRUST -POMDP 6.17 (0.034) 0.352\nFig. 5: (Left) Utensils used for the experiment: spatula, egg\nwhisk, scissors and knife. (Right) The experiment environ-\nment emulates a kitchen. Utensil tray is highlighted for better\nvisibility.\nactions: (A) intervene and retrieve the object by themselves\nor (B) stay put and let the robot pass it.\nParticipants are provided with rules similar to the table-\nclearing experiment: if the human stays put and the robot\nsucceeds, they receive a reward of 1. Since the utensils are\ndirty, the handover is only considered successful if the robot\npasses the object in a manner that the human can easily\ngrasp the clean handle. If the human stays put and the robot\nfails, they receive a penalty of −1. If the human chooses to\nintervene, they will receive no reward or penalty.\nThe following information is not revealed to the partic-\nipant: the robot is able to always succeed on the spatula,\nwhisk, and scissors, but it will always fail to properly hand\nover the knife and drop it. When it happens, the experiment\nis terminated and a penalty of −10 is received. As such, the\nrobot has to actively calibrate the human’s trust so that the\nFig. 6: Success by passing the safe and clean handle (top)\nand intentional failure by passing the wrong end (bottom)\nwhile passing the scissors.\nrobot can help as much as possible (accumulating rewards in\nthe process), yet influence the human to not trust it to pass\nthe knife. This setup is to simulate scenarios where humans\nmay over-trust robots’ capability, thus requiring the robots to\nreduce human trust for better task performance over the long\nterm [5]. Here, the robot intentionally fails to reduce trust,\nbut future work can examine other strategies such as verbal\ncommunication. To facilitate this calibration process [34], we\nenable the robot to intentionally fail on all utensils (except\nthe knife) by handing the wrong part to the human (so that\nthe human can only grasp the dirty end, i.e. augment the\nrobot’s action space with these fail actions. Fig. 6). This\nintentional failure results in a penalty of −1.\nHypothesis and Planners. We consider two different plans:\n• LLM-P LAN , which is a deterministic plan generated by\nplanning with a DAVINCI -TC-YN-based human model\nwith prompts similar to the table-clearing setup.;\n• BASIC -PLAN , a myopic plan that ignores trust and\nalways passes the spatula, egg whisk, scissors, and knife\nin that order (as the associated rewards are the same)\nand never intentionally fails, similar to the myopic plan\nused in [5].\nOur hypothesis was that a robot following LLM-P LAN will\nreduce overtrust and yield higher returns compared to a robot\nfollowing BASIC -PLAN .\nParticipant Recruitment and Allocation. We recruited 65\nparticipants from our university campus (ages 22 to 54).\nParticipants were randomly divided into two groups. Each\ngroup was paired with either the LLM-P LAN robot or the\nBASIC -PLAN robot. Due to safety considerations, partici-\npants did not physically interact with the robot. Instead, they\ncompleted an interactive video survey where a pre-recorded\nvideo of the robot’s behavior was shown at each turn.\nResults. 21 out of the 33 participants ( 63.6%) in the B ASIC -\nPLAN group allowed the robot to pass the knife, compared\nto 9 out of 32 participants (28.1%) in the LLM-P LAN group.\nAs a result, the mean return was higher for the LLM-P LAN\nrobot (-1.88) vs. B ASIC -PLAN robot (-4.24), which a one-\nway ANOV A showed to be statistically significant at theα =\n5% level ( F(1, 63) = [4.302], p = 0.042). These results\nsupport our hypothesis.\nDiscussion. Both B ASIC -PLAN and LLM-P LAN robots\nchoose to pass the spatula and egg whisk in the first 2 turns.\nThen the plans diverge, depending on human actions. Unlike\nBASIC -PLAN , if the human decides not to intervene on the\nfirst two items, the LLM-P LAN robot chooses to intention-\nally fail on the scissors to prevent overtrust. A majority of\nparticipants who witnessed this intentional failure would then\nchoose to intervene on the knife (18/20 participants, 90%).\nAs expected, those who experienced the BASIC -PLAN where\nthe robot successfully handed over the scissors would not\nintervene on the knife (1/15 participants intervened, 6.67%).\nHowever, not all participants experienced the above se-\nquence of events. Five participants in the LLM-P LAN group\nchose to intervene on the egg whisk in the second turn,\nwhich then prompted the robot to pass the knife in the\nnext round (as it predicted the human would not trust it\nto pass the knife). However, these participants then chose\nnot to intervene. When asked to justify their decision, they\nexplained that the spatula and knife were more similar due\nto their handle shape, compared to the egg whisk which\nhad a shorter handle (Fig. 5) and was perceived to be more\ndifficult for the robot. A similar reason was given by the\ntwo participants who still chose to trust the robot to pass\nthe knife after observing the robot fail to properly pass the\nscissors: the scissors was perceived to have a shape that was\ndifferent from the other items. These comments suggest how\nperception of object shape plays a role in human-robot trust\nand present a challenge to incorporate such information into\nLLMs.\nV. C ONCLUSION\nIn this work, we present the first study into LLM-based\nzero-shot human models in HRI. Our key finding is that\nLLMs can be effective task-level human models — they\ncan model high-level human states and behavior. We further\ndemonstrated that incorporating an LLM-based human model\ncan yield reasonable plans in two trust-based social HRI\nscenarios. However, our research shows that current LLMs\nare unlikely to be accurate human models on their own; they\nhave difficulty accounting for low-level geometrical/shape\nfeatures due to limitations in spatial/numerical reasoning.\nNevertheless, they can capture facets of human behavior,\nand combining LLMs with other models would make for\ninteresting future research.\nREFERENCES\n[1] R. Choudhury, G. Swamy, D. Hadfield-Menell, and A. D. Dragan, “On\nthe utility of model learning in hri,” in2019 14th ACM/IEEE Intl. Conf.\non Human-Robot Interaction (HRI). IEEE, 2019, pp. 317–325.\n[2] B. D. Ziebart, N. Ratliff, G. Gallagher, C. Mertz, K. Peterson, J. A.\nBagnell, M. Hebert, A. K. Dey, and S. Srinivasa, “Planning-based\nprediction for pedestrians,” in 2009 IEEE/RSJ Intl. Conf. on Intelligent\nRobots and Systems. IEEE, 2009, pp. 3931–3936.\n[3] D. Sadigh, S. Sastry, S. A. Seshia, and A. D. Dragan, “Planning for\nautonomous cars that leverage effects on human actions.” in Robotics:\nScience and systems, vol. 2. Ann Arbor, MI, USA, 2016, pp. 1–9.\n[4] C. L. Baker, J. Jara-Ettinger, R. Saxe, and J. B. Tenenbaum, “Rational\nquantitative attribution of beliefs, desires and percepts in human\nmentalizing,” Nature Human Behaviour, vol. 1, no. 4, p. 0064, 2017.\n[5] M. Chen, S. Nikolaidis, H. Soh, D. Hsu, and S. Srinivasa, “Planning\nwith trust for human-robot collaboration,” in Proceedings of the 2018\nACM/IEEE Intl. Conf. on human-robot interaction, 2018, pp. 307–315.\n[6] K. Chen, J. Fong, and H. Soh, “Mirror: Differentiable deep social\nprojection for assistive human-robot communication,” arXiv preprint\narXiv:2203.02877, 2022.\n[7] E. Schmerling, K. Leung, W. V ollprecht, and M. Pavone, “Multimodal\nprobabilistic model-based planning for human-robot interaction,” in\n2018 IEEE Intl. Conf. on Robotics and Automation (ICRA). IEEE,\n2018, pp. 3399–3406.\n[8] A. Thomaz, G. Hoffman, M. Cakmak et al., “Computational human-\nrobot interaction,” Foundations and Trends® in Robotics, vol. 4, no.\n2-3, pp. 105–223, 2016.\n[9] O. Nocentini, L. Fiorini, G. Acerbi, A. Sorrentino, G. Mancioppi, and\nF. Cavallo, “A survey of behavioral models for social robots,”Robotics,\nvol. 8, no. 3, p. 54, 2019.\n[10] T. Kojima, S. S. Gu, M. Reid, Y . Matsuo, and Y . Iwasawa,\n“Large language models are zero-shot reasoners,” arXiv preprint\narXiv:2205.11916, 2022.\n[11] K. Valmeekam, A. Olmo, S. Sreedharan, and S. Kambhampati, “Large\nlanguage models still can’t plan (a benchmark for llms on planning\nand reasoning about change),” arXiv preprint arXiv:2206.10498, 2022.\n[12] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, “Language models\nas zero-shot planners: Extracting actionable knowledge for embodied\nagents,” in Intl. Conf. on Machine Learning. PMLR, 2022, pp. 9118–\n9147.\n[13] Y . Xie, C. Yu, T. Zhu, J. Bai, Z. Gong, and H. Soh, “Translating\nnatural language to planning goals with large-language models,”\n2023. [Online]. Available: https://arxiv.org/abs/2302.05128\n[14] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askellet al., “Language mod-\nels are few-shot learners,” Advances in neural information processing\nsystems, vol. 33, pp. 1877–1901, 2020.\n[15] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,\nA. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al.,\n“Palm: Scaling language modeling with pathways,” arXiv preprint\narXiv:2204.02311, 2022.\n[16] G. Aher, R. I. Arriaga, and A. T. Kalai, “Using large language models\nto simulate multiple humans,” arXiv preprint arXiv:2208.10264, 2022.\n[17] M. Kosinski, “Theory of mind may have spontaneously emerged in\nlarge language models,” arXiv preprint arXiv:2302.02083, 2023.\n[18] T. Ullman, “Large language models fail on trivial alterations to theory-\nof-mind tasks,” arXiv preprint arXiv:2302.08399, 2023.\n[19] M. Ahn, A. Brohan, N. Brown, Y . Chebotar, O. Cortes, B. David,\nC. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog et al., “Do as i\ncan, not as i say: Grounding language in robotic affordances,” arXiv\npreprint arXiv:2204.01691, 2022.\n[20] J. J. Lee, F. Sha, and C. Breazeal, “A bayesian theory of mind approach\nto nonverbal communication,” in 2019 14th ACM/IEEE Intl. Conf. on\nHuman-Robot Interaction (HRI). IEEE, 2019, pp. 487–496.\n[21] C. L. Baker and J. B. Tenenbaum, “Modeling human plan recognition\nusing bayesian theory of mind,” Plan, activity, and intent recognition:\nTheory and practice, vol. 7, pp. 177–204, 2014.\n[22] D. Strouse, K. McKee, M. Botvinick, E. Hughes, and R. Everett,\n“Collaborating with humans without human data,” Advances in Neural\nInformation Processing Systems, vol. 34, pp. 14 502–14 515, 2021.\n[23] J. Tjomsland, S. Kalkan, and H. Gunes, “Mind your manners! a dataset\nand a continual learning approach for assessing social appropriateness\nof robot actions,” Frontiers in Robotics and AI, vol. 9, p. 4, 2022.\n[24] H. Soh, P. Shu, M. Chen, and D. Hsu, “The transfer of human trust\nin robot capabilities across tasks.” in R:SS, 2018.\n[25] H. Soh, Y . Xie, M. Chen, and D. Hsu, “Multi-task trust transfer\nfor human–robot interaction,” The Intl. Journal of Robotics Research,\nvol. 39, no. 2-3, pp. 233–249, 2020.\n[26] M. Sap, H. Rashkin, D. Chen, R. LeBras, and Y . Choi, “Socialiqa:\nCommonsense reasoning about social interactions,” arXiv preprint\narXiv:1904.09728, 2019.\n[27] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin,\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language\nmodels to follow instructions with human feedback,” arXiv preprint\narXiv:2203.02155, 2022.\n[28] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus,\nE. Li, X. Wang, M. Dehghani, S. Brahma et al., “Scaling instruction-\nfinetuned language models,” arXiv preprint arXiv:2210.11416, 2022.\n[29] B. Zhang and H. Soh, “Supplementary Material for Large Language\nModels as Zero-Shot Human Models for Human-Robot Interaction,”\nhttps://github.com/clear-nus/llm-human-model, 2023.\n[30] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[31] C. M. Heyes and C. D. Frith, “The cultural evolution of mind reading,”\nScience, vol. 344, no. 6190, p. 1243091, 2014.\n[32] J. Zhang, T. Hedden, and A. Chia, “Perspective-taking and depth\nof theory-of-mind reasoning in sequential-move games,” Cognitive\nscience, vol. 36, no. 3, pp. 560–573, 2012.\n[33] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch,\nA. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso et al., “Beyond\nthe imitation game: Quantifying and extrapolating the capabilities of\nlanguage models,” arXiv preprint arXiv:2206.04615, 2022.\n[34] J. Lee, J. Fong, B. C. Kok, and H. Soh, “Getting to know one\nanother: Calibrating intent, capabilities and trust for human-robot\ncollaboration,” IEEE Intl. Conf. on Intelligent Robots and Systems,\np. 6296–6303, 2020."
}