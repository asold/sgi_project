{
  "title": "EulerMormer: Robust Eulerian Motion Magnification via Dynamic Filtering within Transformer",
  "url": "https://openalex.org/W4393148371",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1983569093",
      "name": "Fei Wang",
      "affiliations": [
        "Hefei University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2099844537",
      "name": "Dan Guo",
      "affiliations": [
        "Institute of Art",
        "Hefei University of Technology",
        "National Science Center"
      ]
    },
    {
      "id": "https://openalex.org/A2108457298",
      "name": "Kun Li",
      "affiliations": [
        "Hefei University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2102035757",
      "name": "Meng Wang",
      "affiliations": [
        "National Science Center",
        "Institute of Art",
        "Hefei University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1983569093",
      "name": "Fei Wang",
      "affiliations": [
        "Hefei University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2099844537",
      "name": "Dan Guo",
      "affiliations": [
        "Institute of Art",
        "Kai Biotech (South Korea)",
        "Hefei University of Technology",
        "National Science Center"
      ]
    },
    {
      "id": "https://openalex.org/A2108457298",
      "name": "Kun Li",
      "affiliations": [
        "Hefei University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2102035757",
      "name": "Meng Wang",
      "affiliations": [
        "National Science Center",
        "Hefei University of Technology",
        "Institute of Art"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2969406957",
    "https://openalex.org/W3141521090",
    "https://openalex.org/W2154504070",
    "https://openalex.org/W2129966346",
    "https://openalex.org/W3037675662",
    "https://openalex.org/W4367694367",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W3176201273",
    "https://openalex.org/W4385825476",
    "https://openalex.org/W4323663038",
    "https://openalex.org/W6696976856",
    "https://openalex.org/W4293812058",
    "https://openalex.org/W4362706994",
    "https://openalex.org/W6749868407",
    "https://openalex.org/W3034966820",
    "https://openalex.org/W6811052838",
    "https://openalex.org/W7052693726",
    "https://openalex.org/W4309872131",
    "https://openalex.org/W4385849093",
    "https://openalex.org/W2476548250",
    "https://openalex.org/W6848924293",
    "https://openalex.org/W6855798144",
    "https://openalex.org/W2949171592",
    "https://openalex.org/W6848284923",
    "https://openalex.org/W6750529677",
    "https://openalex.org/W4304014869",
    "https://openalex.org/W6648075049",
    "https://openalex.org/W6797415705",
    "https://openalex.org/W4229061174",
    "https://openalex.org/W4387924827",
    "https://openalex.org/W1998391547",
    "https://openalex.org/W4282975567",
    "https://openalex.org/W3035981757",
    "https://openalex.org/W4224267514",
    "https://openalex.org/W3212228063",
    "https://openalex.org/W4383216512",
    "https://openalex.org/W2783879794",
    "https://openalex.org/W2773366453",
    "https://openalex.org/W6736274547",
    "https://openalex.org/W6776151872",
    "https://openalex.org/W4281653702",
    "https://openalex.org/W4309584248",
    "https://openalex.org/W6840058269",
    "https://openalex.org/W3145042111",
    "https://openalex.org/W4386075510",
    "https://openalex.org/W4386075770",
    "https://openalex.org/W2547141619",
    "https://openalex.org/W2605718987",
    "https://openalex.org/W4385815478",
    "https://openalex.org/W4313123347",
    "https://openalex.org/W4225672218",
    "https://openalex.org/W4243704736",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4362601361",
    "https://openalex.org/W2493019243",
    "https://openalex.org/W3080756971",
    "https://openalex.org/W3148531901",
    "https://openalex.org/W4392207717",
    "https://openalex.org/W2997753998",
    "https://openalex.org/W4313166619",
    "https://openalex.org/W4309660795",
    "https://openalex.org/W4249022109",
    "https://openalex.org/W4300643866",
    "https://openalex.org/W4313064480",
    "https://openalex.org/W4382450087",
    "https://openalex.org/W2798269147",
    "https://openalex.org/W3034337578",
    "https://openalex.org/W3175514052",
    "https://openalex.org/W4319300585",
    "https://openalex.org/W4312964941",
    "https://openalex.org/W2962785568",
    "https://openalex.org/W1990370049",
    "https://openalex.org/W4389666832",
    "https://openalex.org/W4388189239"
  ],
  "abstract": "Video Motion Magnification (VMM) aims to break the resolution limit of human visual perception capability and reveal the imperceptible minor motion that contains valuable information in the macroscopic domain. However, challenges arise in this task due to photon noise inevitably introduced by photographic devices and spatial inconsistency in amplification, leading to flickering artifacts in static fields and motion blur and distortion in dynamic fields in the video. Existing methods focus on explicit motion modeling without emphasizing prioritized denoising during the motion magnification process. This paper proposes a novel dynamic filtering strategy to achieve static-dynamic field adaptive denoising. Specifically, based on Eulerian theory, we separate texture and shape to extract motion representation through inter-frame shape differences, expecting to leverage these subdivided features to solve this task finely. Then, we introduce a novel dynamic filter that eliminates noise cues and preserves critical features in the motion magnification and amplification generation phases. Overall, our unified framework, EulerMormer, is a pioneering effort to first equip with Transformer in learning-based VMM. The core of the dynamic filter lies in a global dynamic sparse cross-covariance attention mechanism that explicitly removes noise while preserving vital information, coupled with a multi-scale dual-path gating mechanism that selectively regulates the dependence on different frequency features to reduce spatial attenuation and complement motion boundaries. We demonstrate extensive experiments that EulerMormer achieves more robust video motion magnification from the Eulerian perspective, significantly outperforming state-of-the-art methods. The source code is available at https://github.com/VUT-HFUT/EulerMormer.",
  "full_text": "EulerMormer: Robust Eulerian Motion Magnification via\nDynamic Filtering within Transformer\nFei Wang1, Dan Guo1,2,3*, Kun Li1, Meng Wang1,2*\n1School of Computer Science and Information Engineering, Hefei University of Technology\n2Institute of Artificial Intelligence, Hefei Comprehensive National Science Center\n3Anhui Zhonghuitong Technology Co., Ltd\njiafei127@gmail.com, guodan@hfut.edu.cn, kunli.hfut@gmail.com, wangmeng@hfut.edu.cn\nAbstract\nVideo Motion Magnification (VMM) aims to break the reso-\nlution limit of human visual perception capability and reveal\nthe imperceptible minor motion that contains valuable infor-\nmation in the macroscopic domain. However, challenges arise\nin this task due to photon noise inevitably introduced by pho-\ntographic devices and spatial inconsistency in amplification,\nleading to flickering artifacts in static fields and motion blur\nand distortion in dynamic fields in the video. Existing meth-\nods focus on explicit motion modeling without emphasizing\nprioritized denoising during the motion magnification pro-\ncess. This paper proposes a novel dynamic filtering strategy to\nachieve static-dynamic field adaptive denoising. Specifically,\nbased on Eulerian theory, we separate texture and shape to\nextract motion representation through inter-frame shape dif-\nferences, expecting to leverage these subdivided features to\nsolve this task finely. Then, we introduce a novel dynamic\nfilter that eliminates noise cues and preserves critical fea-\ntures in the motion magnification and amplification genera-\ntion phases. Overall, our unified framework, EulerMormer, is\na pioneering effort to first equip with Transformer in learning-\nbased VMM. The core of the dynamic filter lies in a global\ndynamic sparse cross-covariance attention mechanism that\nexplicitly removes noise while preserving vital information,\ncoupled with a multi-scale dual-path gating mechanism that\nselectively regulates the dependence on different frequency\nfeatures to reduce spatial attenuation and complement mo-\ntion boundaries. We demonstrate extensive experiments that\nEulerMormer achieves more robust video motion magnifica-\ntion from the Eulerian perspective, significantly outperform-\ning state-of-the-art methods. The source code is available at\nhttps://github.com/VUT-HFUT/EulerMormer.\nIntroduction\nVideo Motion Magnification (VMM) has garnered growing\nresearch interest due to its remarkable ability to vividly re-\nveal subtle motions in real-world videos that are impercep-\ntible to the human eye (Rubinstein et al. 2013; Le Ngo and\nPhan 2019). Existing VMM techniques behave as computer-\nassisted â€œeyesâ€ that enable humans to see and grasp mean-\ningful subtle motion in various challenging-to-perceive set-\ntings, such as micro-action recognition (Xia et al. 2020; Qi\n*Corresponding authors.\nCopyright Â© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nIntensitySpace(x)IntensitySpace(x)\nIntensitySpace(x)IntensitySpace(x)\nSpace(x)\nConsistency\nğ‘°ğ’™,ğ’•ğ‘°ğ’™,ğŸ ğ‘°ğ’™,ğŸğ‘°ğ’ğ’™,ğ’•\nğ‘°ğ’™,ğŸğ‘°ğ’™,ğ’• ğ‘°ğ’™,ğŸğ‘°ğ’ğ’™,ğ’•\nIntensityArtifactsAttenuationArtifacts\nIntensityStatic fieldDynamic field\nOurs(a) Theoretical magnification\nSpace(x)(b) Realistic magnification results. \nMore stableMore stable\nBaby's abdomen with subtle breathing motion.\nBreathStatic field\nDynamic field\nStatic\nStatic fieldStatic fieldDynamic fieldStatic field\nDynamic\nğ‘°ğ’™,ğ’•\nğœ¹ğ’™,ğ’•\nOh et al.Singh et al.(a)ğ‘°ğ’™,ğ’•\n(ğŸ+ğœ¶)ğœ¹ğ’™,ğ’•\nFigure 1: Theoretical basis and realistic results of VMM.\nTheoretically, the static field in (a) is free of position dis-\nplacement, while the dynamic field should exhibit ideal po-\nsition displacement to satisfy the desired motion magnifica-\ntion. However, in the real world, unavoidable photon noise\nand spatial inconsistency exist with artifacts, intensity atten-\nuation, etc., in (b) for the magnified results (Oh et al. 2018;\nSingh, Murala, and Kosuru 2023a). In contrast, we achieve\nmore robust magnification in both static and dynamic fields.\net al. 2020; Mehra et al. 2022; Nguyen et al. 2023), robotic\nsonography (Huang et al. 2023), clinical medicine (Abnousi\net al. 2019) material properties estimation (Davis et al. 2015,\n2017) and modal analysis (Eitner et al. 2021). Specifically,\nVMM aims to capture and amplify the imperceptible subtle\nmotion in the video sequence while preserving fine spatial\ndetails for realistic and accurate visualization.\nHowever, this task faces several challenges: (1) Photon\nnoise (Oh et al. 2018) is inevitably present in videos due\nto the characteristics of charge-coupled devices (CCDs) in\nphotographic devices and signal attenuation during trans-\nmission. The noise, indistinguishable from subtle motions,\ncan result in flickering artifacts. (2) Spatial inconsis-\ntency (Wadhwa et al. 2013) measures the magnified quality,\nas forced motion magnification can lead to spatial frequency\ncollapse, resulting in phenomena such as motion blur and\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n5345\ndistortion. As the results of recent methods (Oh et al. 2018;\nSingh, Murala, and Kosuru 2023a) shown in Fig. 1(b), noise\namplification disrupts the static field of magnified image,\nand spatial intensity attenuation occurs in the dynamic field.\nInspired by the theory of fluid mechanics, early research\ndrew from the Lagrangian and Eulerian perspectives. Liu et\nal. (Liu et al. 2005) proposed the first Lagrangian-based\napproach, which involved tracking the motion trajectory of\neach pixel (optical flow) for motion magnification, but it was\ncomputationally expensive and sensitive to various noises.\nIn contrast, Eulerian approaches (Wu et al. 2012; Wadhwa\net al. 2013; Zhang, Pintea, and Van Gemert 2017; Takeda\net al. 2018, 2019) relied on traditional filters to handle the\nmotion intensity occurring in specific regions rather than\ntracking every pixel throughout the video. However, these\nEulerian methods required fine-tuning numerous hyperpa-\nrameters to adapt to different scenarios, which makes them\nimpractical for real-world applications.\nDeveloping effective VMM methods remains a com-\npelling topic in the computer vision community. Recently,\nlearning-based methods (Oh et al. 2018; Singh, Murala, and\nKosuru 2023a,b) utilizing different convolutional neural net-\nworks (CNN) have attained SOTA performance. Whether\nthey introduce proxy model regularization or frequency do-\nmain phase (Wang et al. 2022b, 2024) to optimize their mod-\nels, they essentially focus on representation learning, such as\nmotion and phase, for generating motion-magnified videos\nwithout emphasizing prioritizing denoising.\nThis paper focuses primarily on addressing the denoising\nissue in VMM. We specially design a dynamic filter mod-\nule F(Â·) to address the previously mentioned photon noise\nand spatial inconsistency in static and dynamic fields. Based\non Eulerian theory, we disentangle texture and shape and\nfurther acquire the motion = â–³shape, which is expected to\nleverage these subdivided features to solve this task finely.\nEspecially noteworthy is the to-be-magnified motion rep-\nresentation. In our framework, we utilize F(Â·) to filter out\nnoise cues from motion during the motion magnification\nphase and refine the representations of texture and magnified\nshape during the amplification generation phase. Finally,\ncompared with the limitation of existing CNN-based meth-\nods with local receptive fields, our method is equipped with\nTransformer architecture in the encoder and the dynamic fil-\nter F(Â·), which can ensure the contextualized global rela-\ntionship learning between pixels. Overall, we provide a uni-\nfied framework to filter out undesired noise cues in the rep-\nresentation learning of texture, shape, and motion, which re-\nsults in a satisfactory magnification effect.\nOur contributions can be summarized as follows:\nâ€¢ We introduce a novel Transformer-based EVM architec-\nture that offers better spatial consistency and fewer arti-\nfacts and distortion in the magnified video. To our knowl-\nedge, this is a pioneering effort in learning-based VMM.\nâ€¢ We develop a dynamic filter implemented on a sparse at-\ntention strategy for static-dynamic field adaptive denois-\ning and texture-shape joint refinement during the motion\nmagnification and amplification generation phases.\nâ€¢ We propose a Point-wise Magnifier, which improves the\nmagnified representation by incorporating global nonlin-\near feature interactions per pixel to maintain spatial con-\nsistency and reduce flickering artifacts.\nâ€¢ Extensive quantitative and qualitative experiments on\nsynthetic and real-world datasets demonstrate our favor-\nable performances against SOTA approaches.\nRelated Work\nTraditional Methods. Lagrangian-based approaches (Liu\net al. 2005) pioneered this task by tracking the motion tra-\njectory of each pixel for motion magnification, but dense op-\ntical flow computation is expensive and sensitive to noise.\nEulerian-based methods (Wu et al. 2012; Wadhwa et al.\n2013; Zhang, Pintea, and Van Gemert 2017; Takeda et al.\n2018, 2019, 2022) concentrate on the specific regions where\nmotion occurs, rather than tracking every pixel in the video.\nEarly Eulerian-based methods (Wu et al. 2012; Wadhwa\net al. 2013) altered intensities to approximate linear mag-\nnification or decomposed the motion in the frequency do-\nmain. With further research, various hand-crafted filters,\nsuch as acceleration (Zhang, Pintea, and Van Gemert 2017),\njerk (Takeda et al. 2018), anisotropy (Takeda et al. 2019),\nand bilateral filters (Takeda et al. 2022), were explored.\nThese works rely on the predefined bandwidth for bandpass\nfilters to amplify specific motions, but their effectiveness re-\nquires extensive hyperparameter tuning.\nDeep-Learning Methods. Learning-based approaches for\nthe VMM have emerged but are still in their infancy, with\nonly a handful of related works (Oh et al. 2018; Brattoli\net al. 2021; Singh, Murala, and Kosuru 2023a,b). Oh et\nal. (Oh et al. 2018) proposed a CNN-based end-to-end ar-\nchitecture for the first attempt to learn the motion magnifi-\ncation representation, achieving comparable results to hand-\ncrafted filters. Recently, Singh et al . (Singh, Murala, and\nKosuru 2023a) proposed a lightweight CNN-based proxy\nmodel to eliminate undesired motion efficiently. Afterwards,\nthey (Singh, Murala, and Kosuru 2023b) also utilized CNN\nto model the magnification signals from frequency domain\nphase fluctuations to avoid artifacts and blurring in the spa-\ntial domain. Unlike the above CNN methods with local re-\nceptive fields (Zheng et al. 2023; Guo et al. 2019; Zhou et al.\n2021, 2022), we introduce a novel dynamic filtering strategy\ninto Transformer-based architecture (Li et al. 2023; Li, Guo,\nand Wang 2021; Tang et al. 2022; Li, Guo, and Wang 2023)\nin this study. Intrinsically, based on the Eulerian theory, we\nintegrate the advanced Transformer to globally model the\ntexture, shape, and motion representations, enabling static-\ndynamic field adaptive denoising for motion magnification.\nPreliminaries\nTask Definition\nLet I(x, t) denote the intensity at spatial positionx and time\nt. With I(x, 0)=f(x) and I(x, t)=f(x + Î´(x, t)), Î´(x, t) de-\nnotes a displacement function of x at time t, the goal of\nVMM is to synthesize Im(x, t) with respect to a magnifica-\ntion factor Î± as follows (Wu et al. 2012; Zhang et al. 2023):\nIm(x, t) =f(x + (1 +Î±)Î´(x, t)). (1)\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n5346\nDynamic \nFilter ğ“• âˆ™\nNorm\n1Ã—1Conv\nGELU\nPoint-wise Magnifier ğ“œ âˆ™ âŠ•\nâŠ—\n1Ã—1Conv\nğœ¶\nMotion Repr.\nğœ™ğ‘ â€² ğ‘¥,ğ‘¡\nTexture Trans-\nformer Encoder\nShape Trans-\nformer Encoder\nPhase 2: Motion Magnification Phase 3: Amplification GenerationPhase 1: Texture-Shape Disentanglement\nğ¼ ğ‘¥,ğ‘¡\nMagnified FrameReference Frame\n(a) Dynamic Masking Filter (DMF)\n1Ã—1 Conv\nâŠ—âŠ—\nTop-k\nOperator\n3Ã—3\nDWConv\n1Ã—1 Conv\nNorm âŠ•\nIndexing\nHW\n4\nà·¡C\nà·¡C\nH\n2 â‹…ğ‘Š\n2\nÎ¤H 2â‹… Î¤W 2\nC = â„ à·¡C\nğ’Ÿğ‘˜ CA\n3Ã—3\nDWConv\nNorm\n1Ã—1 Conv\nSplit\n1Ã—1\nConv\n3Ã—3\nConv\n5Ã—5\nConv\nConcatenate\nGELU\n3Ã—3\nDWConv\n(b) Multi-Scale Gating Regulator (MGR)\nGELU\nâŠ™\n1Ã—1 Conv\nâŠ•\nğ’ âˆ™\nğ’¢ âˆ™\nQuery Frame\nâ„ à·¡C\nâŠ—\nà·¡C\nà·¡C\nDMF\nMGR\nÃ—ğ‘1\nGELU\nDMF\nMGR\nÃ—ğ‘2\nDynamic \nFilter ğ“• âˆ™\nğ¼ğ‘š ğ‘¥,ğ‘¡\nğ¼ ğ‘¥,0\nğ“ğ’” ğ’™,ğ’•\nâ„ à·¡C\nğœ“ğ‘¡ ğ‘¥,ğ‘¡\nğœ“ğ‘¡ ğ‘¥,0\nğœ™ğ‘  ğ‘¥,0\nğœ™ğ‘  ğ‘¥,ğ‘¡\nğ›¿ğ‘š ğ‘¥,ğ‘¡\nğœ“ğ‘¡ ğ‘¥,ğ‘¡\nFigure 2: The overall architecture of EulerMormer for video motion magnification, which consists of three phases: (1) texture-\nshape disentanglement, (2) motion magnification with a dynamic filter F(Â·) and a point-wise magnifier M(Â·), and (3) am-\nplification generation, which recouples and refines the original texture Ïˆt(x, t) and the magnified shape Ï•â€²\ns(x, t) to generate\nhigh-quality magnified frames. Among them, the dynamic filterF(Â·), consisting of DMF in (a) and MGR in (b), performs twice\nin motion magnification and amplification generation processes, which targets to achieve the static-dynamic field adaptive de-\nnoising in terms of texture, shape and motion representation learning.\nWe can approximate I(x, t) in a first-order Taylor series\nexpansion as:\nI(x, t) â‰ˆ f(x) +Î´(x, t)âˆ‚f (x)\nâˆ‚x , (2)\nwhere Î´(x, t)âˆ‚f(x)\nâˆ‚x is regarded as the intensity magnitude.\nCombining Eqs. 1 and 2, we have the magnification:\nIm(x, t) â‰ˆ f(x) + (1 +Î±)Î´(x, t)âˆ‚f (x)\nâˆ‚x . (3)\nAccording to Eulerian learning-based VMM methods (Oh\net al. 2018; Singh, Murala, and Kosuru 2023a), the mo-\ntion magnification process can be disentangled into texture\nand shape components as shown in Eq. 4. In this work, our\nmethod belongs to this methodological scope.\nIm(x, t) â‰ˆ I(x, t)|\n{z }\nTexture\n+ Î± Î´(x, t)| {z }\nâ–³Shape\nâˆ‚f (x)\nâˆ‚x . (4)\nMotivation\nAs described above, videos can be modeled by two inde-\npendent latent variables: texture and shape. Texture repre-\nsentation exhibits invariance, while the motion generated\nby shape displacement for magnification deserves further\ninvestigation. We extract subtle motion by calculating the\ninter-frame shape difference between two frames, i.e., mo-\ntion = â–³ shape. Meanwhile, the amplification of subtle mo-\ntion is inevitably affected by noise, as depicted in Fig. 1,i.e.,\nphoton noise in the static field and spatial inconsistency in\nthe dynamic field. To this end, we propose a dynamic filter\nF(Â·) in our framework designed for denoising to eliminate\nartifacts and distortion caused by these noises. It is applied\ntwice within our framework: once for denoising the motion\nrepresentation and once for denoising the recoupled texture-\nmagnified shape joint refinement, formulated as follows:\nIOurs(x, t) =F\n\u0002\nI(x, t)|\n{z }\nTexture\n+Î± F\n\u0000\nÎ´(x, t)| {z }\nâ–³Shape\nâˆ‚f (x)\nâˆ‚x\n\u0001\u0003\n. (5)\nMethodology\nTexture-Shape Disentanglement\nGiven any pair of reference and query images in a video,\n[I(x, 0), I(x, t)], we use a 3Ã—3 convolution layer to obtain\ninitial feature maps F(x, 0),F(x, t)âˆˆ R\nH\n2 Ã—W\n2 Ã—C, and fur-\nther use a Texture Transformer Encoder Ïˆt(Â·) and a Shape\nTransformer Encoder Ï•s(Â·) to obtain their texture and shape\nrepresentations, i.e., [Ïˆt(x, 0),Ï•s(x, 0)] âˆˆ R\nH\n2 Ã—W\n2 Ã—C,\n[Ïˆt(x, t),Ï•s(x, t)] âˆˆ R\nH\n2 Ã—W\n2 Ã—C, as shown in Fig. 2. Specif-\nically, the two encoders comprise the Transformer with\nMulti-Dconv Head Transposed Attention (MDTA, derived\nfrom Restormer (Zamir et al. 2022)) and Multi-Scale Gated\nRegulator (MGR, see Sec.). MDTA replaces multi-head\nself-attention (MHA) in Transformer and facilitates con-\ntextualized global interaction between pixels by incorpo-\nrating depth-wise convolutions and cross-covariance atten-\ntion. This choice enables efficient pixel-grained representa-\ntion learning, making it well-suited for this task. Our MGR\nutilizes the multi-scale dual-path gating mechanism to selec-\ntively integrate features at different frequencies, providing\nsatisfactory texture and shape representations.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n5347\nMotion Magnification\nObtaining a â€œcleanâ€ motion representation is crucial for mo-\ntion magnification, as the inherent photon noise has nearly\nequivalent energy fields and the subtle motion change and\nis prone to amplify noise resulting in artifacts and distor-\ntion. We define the motion representation by implement-\ning a simple inter-frame shape difference, i.e., Î´m(x, t) =\nâ–³(Ï•s(x, t), Ï•s(x, 0)) âˆˆ R\nH\n2 Ã—W\n2 Ã—C. To manipulate the mo-\ntion magnification, we describe two core components (DMF\nand MGR, see below) of the dynamic filterF(Â·) and a point-\nwise magnifier M(Â·) in detail below.\nDynamic Masking Filter (DMF). We revisit multi-head\nself-attention on the motion Î´m(x, t). After implementing\n1 Ã— 1 convolution and 3 Ã— 3 depth-wise convolutions, we\ngroup Î´m(x, t) into h heads and each single-headed projec-\ntion has Q, K, V âˆˆ R( H\n2 Ã—W\n2 )Ã— Ë†C, where Ë†C = C\nh and h =\n4. On each head, we calculate a cross-covariance attention\nmatrix CA âˆˆ RË†CÃ— Ë†C between K and Q. In CA, a learn-\nable temperature Ï„ scales inner products before calculating\nattention weights, enhancing training stability.\nCA = Ï„KT Â· Q, (6)\nIn Fig. 2(a), a critical design of DMF is that we take\nCA as a search space to perform dynamic sparse eras-\ning. Our sparse strategy applies a dynamic filtering mech-\nanism with the Top-koperator (Zhao et al. 2019; Wang et al.\n2022a) along the channel dimension. Specifically, we adap-\ntively select row-wise top-k contributive elements based\non the channel correlation scores in CA. Then, we utilize\nEq. 7 to generate the corresponding binary mask for posi-\ntion indexing, representing the relative positions of the high-\ncontributing elements obtained in CA. Here, the dynamic\nmask Dk(CA) âˆˆ RË†CÃ— Ë†C is formulated as:\n[Dk(CA)]ij =\n\u001aCAij CAij â‰¥ kij\n0 otherwise , (7)\nwhere kij represents the k-th row-wise maximum value in\nCAij. This allows us to dynamically degenerate the dense\nCA into a sparse attention matrix SCA âˆˆ RË†CÃ— Ë†C:\nSCA = Softmax(Dk(CA)). (8)\nAfter the implementation of the weighted value V sum\nwith the sparse matrix SCA, we concat all the headsâ€™ re-\nsults and output the updated motion Î´â€²\nm(x, t) âˆˆ R\nH\n2 Ã—W\n2 Ã—C.\nDMF is designed to explicitly remove noise from the static-\ndynamic fields in Î´m(x, t) and preserve the desired motion,\npreventing distortion and artifacts caused by amplified noise.\nMulti-Scale Gating Regulator (MGR). Humans intelli-\ngently perceive visual changes across multiple scales. How-\never, when the motion is too subtle and indistinguishable\nfrom noise, the integrity of the motion trajectory is com-\npromised. Based on the DMF processing noise, we propose\nMGR that repairs the smoothness and uncertainty of the mo-\ntion contours to overcome this issue. The MGR is a dual-\npath feedforward network consisting of multi-scale context\nbranches C(Â·) and dual-path gating G(Â·), see Fig. 2(b).\nWe normalize and map the motion Î´â€²\nm(x, t) to a high-\ndimensional space with a 1Ã— 1 convolution, where the ex-\npansion factor is Î· = 3. Next, after a 3Ã—3 depth-wise convo-\nlution, the motion representation is split into dual-path gates,\ni.e., G(Î´â€²\nm(x, t)), C(Î´â€²\nm(x, t)) âˆˆ R\nH\n2 Ã—W\n2 Ã—Î·C\n2 . For C(Â·), we\nparallelly employ three depth-wise convolutions with the\nkernel sizes of s âˆˆ {1,3, 5} to capture the interactions at\ndifferent frequencies C1, C3, C5 âˆˆ R\nH\n2 Ã—W\n2 Ã—Î·C\n6 . Notably,\nhigh-frequency noise characterized by a small scale is effec-\ntively handled by the low-frequency characteristics of C1.\nWith increasing kernel sizes, C3 and C5 play a crucial role\nin motion contours acquisition and motion complementa-\ntion. And these different frequency features are fused be-\nfore passing through a layer with a nonlinear activation func-\ntion of GELU. As for G(Â·), a GELU activation function en-\nsures nonlinear feature transformation. Finally, MGR regu-\nlates the output by Hadamard product âŠ™ with G(Â·) and C(Â·):\nÎ´â€²â€²\nm(x, t) =G(Î´â€²\nm(x, t)) âŠ™ C(Î´â€²\nm(x, t)), (9)\nwhere the output of updated motion Î´â€²â€²\nm(x, t) âˆˆ R\nH\n2 Ã—W\n2 Ã—C.\nThe combination process of DMF and MGR is defined as\nthe dynamic filter F(Â·).\nPoint-wise Magnifier (PWM). In this part, PWM serves\nas a manipulator to perform nonlinear magnification on\nÎ´â€²â€²\nm(x, t) =F(Î´m(x, t)). It adopts a simple and efficient de-\nsign with two fundamental modifications to improve mag-\nnified representation learning: (a) in order to reduce flick-\nering artifacts, we abandon local convolutions and oper-\nate point-wise convolutions to interact with magnification\nacross channels, thereby reducing checkerboard artifacts and\nbeing more compatible with global filtering; (b) we use the\nmore stable GELU activation function to provide nonlinear\nrepresentation learning and avoid gradient explosion. There-\nfore, the calculation process of PWM is as follows:\nÏ•â€²\ns(x, t) =Wp(Î± Â· Wp(Î´â€²â€²\nm(x, t))) +Ï•s(x, t), (10)\nwhere Ï•â€²\ns(x, t) âˆˆ R\nH\n2 Ã—W\n2 Ã—C represents the amplified shape\nrepresentation with the factorÎ± and Wp(Â·) denotes the point-\nwise convolution with GELU activation layer.\nAmplification Generation\nWe reconstruct the high-quality magnified image by recou-\npling the magnified shape Ï•â€²\ns(x, t) with the original texture\nÏˆt(x, t). Its challenge is avoiding high-frequency noise from\nÏˆt(x, t) and ringing artifacts at the recoupled boundaries.\nFor this purpose, we recouple Ï•â€²\ns(x, t) and Ïˆt(x, t) across\nthe feature channels and adopt the same dynamic filter F(Â·)\nto perform the texture-magnified shape joint refinement to\nfacilitate their fusion and boundary completeness:\nIm(x, t) =Wup\n\u0000\nF\n\u0000\nÏ•â€²\ns(x, t), Ïˆt(x, t)\n\u0001\u0001\n, (11)\nwhere Wup(Â·) denotes a layer that combines pixel shuffling\noperation (Shi et al. 2016) and a 3Ã—3 convolution to per-\nform sub-pixel level upsampling, generating the final mag-\nnified image Im(x, t). Methodologically,F(Â·) in this section\ndynamically filters Ï•â€²\ns(x, t) and Ïˆt(x, t) through interactive\nguidance along the channel to suppress noise while aiding in\nsynthesizing smooth motion boundaries and clear details.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n5348\nLoss Optimization\nTo optimize the proposed model, the objective functionL is\nthe weighted sum of three loss terms as follows:\nL = Lmag + Âµ1Ldr + Âµ2Ledge, (12)\nwhere Âµ1, Âµ2 are hyperparameters to balance the three loss\nfunctions. Lmag is a basic loss term, which calculates the\nCharbonnier penalty (Bruhn, Weickert, and Schn Â¨orr 2005)\nbetween the magnified image Im and ground-truth IGT as:\nLmag =\nq\nâˆ¥Im(x, t) âˆ’ IGT (x, t)âˆ¥2 + Îµ2, (13)\nwhere Îµ is a constant value, being empirically set to 10âˆ’3.\nThe robust Charbonnier penalty term approximates the l1-\nloss and easily captures outliers inIm(x, t). Besides, similar\nto (Oh et al. 2018; Singh, Murala, and Kosuru 2023a), we\nuse a color perturbation loss Ldr to enforce the disentangled\nrepresentation learning of shape and texture as follows:\nLdr = L(Ï•s(x, t), Ï•c\ns(x, t)) +L(Ïˆt(x, t), Ïˆc\nt (x, t)). (14)\nwhere [Ï•s(x, t), Ïˆt(x, t)] and [Ï•c\ns(x, t), Ïˆc\nt (x, t)] are respec-\ntive shape and texture representations of image I(x, t) and\nits color perturbed image Ic(x, t). Notably, in this study,\nwe use a new loss term Ledge, namely using a Laplacian\nof Gaussian (LoG) edge detector ELoG (Zhang et al. 2017)\nwith Charbonnier penalty, that is used to restrict the consis-\ntency between texture and amplified shape deformation as:\nLedge =\nq\nâˆ¥ELoG(Im(x, t)) âˆ’ ELoG(IGT (x, t))âˆ¥2 + Îµ2. (15)\nExperiments\nExperiment Setup\nReal-World Datasets. We experiment on three real-world\nbenchmarks used in previous work: (a) Static dataset (Wu\net al. 2012; Wadhwa et al. 2013; Oh et al. 2018) and(b) Dy-\nnamic dataset (Zhang, Pintea, and Van Gemert 2017; Oh\net al. 2018) contain 10 and 6 classic subtle videos in both\nstatic (slight motion, e.g. baby breathing) and dynamic (e.g.,\nstrenuous motion and perspective shifts) scenarios. (c) Fab-\nric dataset (Davis et al. 2015, 2017) contains 30 videos of\nsubtle changes in fabric surface under wind excitation.\nSynthetic Datasets. Real-world videos are rich in percep-\ntual characteristics but lack ground truth annotations. Thus,\nwe generate a synthetic dataset for quantitative evaluation.\nWe select 100 objects from the public StickPNG library 1\nand 600 high-resolution background images from the DIS5K\ndataset (Qin et al. 2022). In the data generation, we ran-\ndomly place the objects onto the background images, initial-\nizing them as reference frames. Subsequently, we synthesize\nquery frames by randomly adjusting the object direction and\nvelocity with velocities limited to the range of (0, 2] to im-\nitate subtle motions of objects. Therefore, we multiply the\ninter-frame velocities by magnification factors to synthesize\nthe accurate ground truth for magnified motion. We create\nthree synthetic subsets: Synthetic-I Dataset: Implement-\ning random magnification factors Î± âˆˆ (0, 50]; Synthetic-II\n1The StickPNG library is available at https://stickpng.com/.\nDataset: Adding Poisson noise with the scale of random in-\ntensity levels Î» âˆˆ [3, 30]; Synthetic-III Dataset: Adding\nGaussian blurs with the scale of random standard deviations\nÏƒ âˆˆ [3, 30]. In conclusion, the synthetic dataset contains\n1,800 pairs of images and corresponding Î±.\nImplementation Details. Following the protocol (Oh\net al. 2018; Singh, Murala, and Kosuru 2023a,b), all methods\nare implemented with the same training data from (Oh et al.\n2018) comprising 100,000 pairs of input sized 384 Ã— 384\npixels. The focus in this field revolves around cross-dataset\ntesting. We employ the Adam optimizer (Kingma and Ba\n2015; Qian et al. 2023; Zhou, Guo, and Wang 2022) with\nthe learning rate of 2Ã—10 âˆ’4 and the batch size of 4. For\nthe network setting, the feature channel C is set to 48, and\nthe numbers of the Texture Transformer Encoders and Shape\nTransformer Encoders are 2. The dynamic filterF(Â·) is con-\nfigured with N1 = 2 in Phase 2 and N2 = 8 in Phase 3, and\nthe Top-k operator is set with k = 7. Besides, we set the loss\nhyperparameters as Âµ1 = 0.1 for Ldr and Âµ2 = 0.5 for Ledge.\nEvaluation Metrics. For synthetic datasets, we employ\nRMSE to assess magnification error and PSNR (Shen,\nZhao, and Zhang 2023; Shen et al. 2023), SSIM, and\nLPIPS (Zhang et al. 2018) to assess the magnification qual-\nity. For real-world datasets, we introduce an advanced no-\nreference image quality assessment metric, MANIQA (Yang\net al. 2022). MANIQA is the NTIRE 2022 NR-IQA chal-\nlenge winner achieves human-comparable quality assess-\nment and is widely applied in image distortion and video\nreconstruction tasks (Wu et al. 2022; Ercan et al. 2023).\nQuantitative Comparisons\nComparisons on Synthetic Datasets. We compare with\nexisting approaches and report the experimental results in\nTab. 1. On the Synthetic-I, our method performs superior to\nthe recent best method MDLMM on magnification error and\nvisual quality, with RMSE, PSNR, SSIM, and LPIPS val-\nues of 0.0594 vs. 0.0651, 25.49 dB vs. 24.84 dB, 0.9536 vs.\n0.9173, and 0.0535 vs. 0.1228, respectively. On Synthetic-II\nand Synthetic-III, EulerMormer still shows significant per-\nformance gains for Poisson noise and Gaussian blur.\nComparisons on Real-World Datasets. From Tab. 2, pre-\nvious works with traditional narrowband filters (Zhang,\nPintea, and Van Gemert 2017; Takeda et al. 2018, 2019)\nhave lower MANIQA scores than ours. The MANIQA met-\nric (Yang et al. 2022) mainly evaluates visual distortion lev-\nels. For example, compared to the previous best method\nAnisotropy (Takeda et al. 2019), we achieve 0.6920 vs.\n0.6872, 0.6760 vs. 0.6634, and 0.7316 vs. 0.7288 on Static,\nDynamic, and Fabric datasets, respectively.\nAblation Studies\nEffectiveness of Filter F(Â·). We test the dynamic filter\nF(Â·) in Phase 2 and Phase 3 separately. Observing Tab. 3,\nremoving F(Â·) from the model significantly decreases both\naccuracy and quality of the magnification (e.g., removing\nF(Â·) decreases the perceptual quality LPIPS from 0.0535\nto 0.1170 in Phase 2 and increases the magnification error\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n5349\nMethod Venue Synthetic-I: Magnification (Î±) Synthetic-II: Poisson Noise (Î») Synthetic-III: Gaussian Blur (Ïƒ)\nRMSE PSNR SSIM LPIPS RMSE PSNR SSIM LPIPS RMSE PSNR SSIM LPIPS\nLinear TOGâ€™12 0.1029 20.21 0.8397 0.3247 0.1102 19.39 0.6746 0.2497 0.1347 17.21 0.5874 0.4666\nPhase TOGâ€™13 0.0978 21.18 0.8613 0.1428 0.1053 20.30 0.6941 0.2283 0.1206 18.87 0.6109 0.4499\nAcc. CVPRâ€™17 0.0781 22.99 0.9299 0.1346 0.0854 22.20 0.7694 0.1922 0.1011 20.62 0.6508 0.4242\nJerk CVPRâ€™18 0.0746 23.61 0.9333 0.1302 0.0787 23.06 0.7964 0.1844 0.0951 20.82 0.6612 0.4156\nLBVMM ECCVâ€™18 0.0682 23.89 0.8748 0.1775 0.0700 23.65 0.8329 0.2164 0.0913 21.19 0.6645 0.4177\nAniso. CVPRâ€™19 0.0687 24.01 0.9386 0.1260 0.0745 23.72 0.8230 0.1744 0.0919 20.93 0.6646 0.4121\nLNVMM W ACVâ€™23 0.0662 24.19 0.8943 0.1544 0.0681 23.92 0.8497 0.1889 0.0915 21.16 0.6581 0.4264\nMDLMM CVPRâ€™23 0.0615 24.84 0.9173 0.1228 0.0637 24.53 0.8659 0.1720 0.0896 21.34 0.6639 0.4205\nOurs - 0.0594 25.49 0.9536 0.0535 0.0616 25.04 0.8706 0.1604 0.0867 21.89 0.6797 0.4077\nTable 1: Quantitative comparison of our EulerMormer and existing methods on three subsets of the synthetic dataset: evaluating\nmagnification accuracy, noise robustness, and blur sensitivity. Our EulerMormer achieves the best performance.\nMethod Static Dyn. Fabric\nLinear .6288 .5169 .6597\nPhase .6696 .5861 .7120\nAcc. .6748 .6289 .7225\nJerk .6769 .6594 .7256\nLBVMM .6830 .6409 .7234\nAniso. .6872 .6634 .7288\nLNVMM .6332 .6435 .7195\nMDLMM .6297 .6150 .7134\nOurs .6920 .6760 .7316\nTable 2: Quantitative compar-\nison on real-world datasets in\nthe term of MANIQAâ†‘.\nw/o 1 3 5 7 9 11\nRMSEâ†“\n.0648\n.0626\n.0611\n.0603\n.0594\n.0621\n.0634\nFigure 3: Ablation results\nof k in Top-k operator on\nthe Synthetic-I dataset.\nPhase 2 Phase 3 RMSE PSNR SSIM LPIPSDMF MGR DMF MGR\n! ! % % 0.0747 23.04 0.8195 0.2479\n! ! % ! 0.0638 23.38 0.9389 0.0876\n! ! ! % 0.0631 24.58 0.9437 0.0691\n% % ! ! 0.0708 23.40 0.8756 0.1170\n% ! ! ! 0.0622 24.72 0.9450 0.0685\n! % ! ! 0.0603 25.34 0.9501 0.0583\n! ! ! ! 0.0594 25.49 0.9536 0.0535\nTable 3: Ablation studies of the filter F(Â·) in Phase 2 and\nPhase 3 on the Synthetic-I dataset.\nRMSE from 0.0594 to 0.0747 in Phase 3). Moreover, we\ndeeply discuss the two core components of F(Â·), DMF and\nMGR. A more comprehensive analysis highlights the signifi-\ncant roles played by the DMF and MGR modules in denois-\ning and artifacts-freeing (e.g., DMF improves PSNR from\n24.72 to 25.49 in Phase 2, and MGR improves SSIM from\n0.9437 to 0.9536 in Phases 3), thus validating the effective-\nness of entire dynamic filter F(Â·) in this task.\nImpact of Top-k in Filter F(Â·). To investigate the impact\nof the Top-k operator in Filter F(Â·), we test k âˆˆ {1, 3, 5, 7,\n9, 11}. Here, k âˆˆ [0, Ë†C] and Ë†C = 12 in our experiment setup.\nFrom Fig. 3, while k = 1, it leads to significant sparsity of\nLmag Ldr Ledge LSobel RMSE PSNR SSIM LPIPS\n! % % % 0.0678 22.05 0.9317 0.1121\n! ! % % 0.0613 24.91 0.9405 0.0783\n! ! % ! 0.0606 25.06 0.9487 0.0687\n! ! ! % 0.0594 25.49 0.9536 0.0535\nTable 4: Ablation studies of losses on the Synthetic-I dataset.\nsimilarity-based attention matrix, resulting in a large error,\ni.e., RMSE of 0.0648. While k = 11, there is a large error\ntoo, i.e., RMSE of 0.0621. Hence, an appropriate value of k\ncontributes to the balance of the attention sparsity calcula-\ntion and magnification denoising. As a result, we set k = 7\nwith the lowest RMSE of 0.0594 as the optimal setting.\nEffect of Loss Function. Tab. 4 reports the ablation stud-\nies of different loss functions. Based on the basis Lmag, the\nintroduction of disentangled representation loss Ldr signifi-\ncantly improves the robustness of magnification, i.e., PSNR\nis improved from 22.05 dB to 24.91 dB. Moreover, applying\nthe Ledge yields gains of 0.58 dB and 0.0131 for PSNR and\nSSIM, respectively. Comparing it with the well-known So-\nbel loss LSobel (Zheng et al. 2020), which focuses solely on\nhorizontal and vertical edges, Ledge incorporates the LoG\noperator for noise smoothing and edge detection, demon-\nstrates better noise robustness, edge continuity, and effective\nextraction of low-contrast magnified motion boundaries.\nQualitative Analysis\n(1) Magnification visualization comparisons. In Fig. 4,\nLinear (Wu et al. 2012) and Phase (Wadhwa et al. 2013)\nexhibit significant distortion and ringing artifacts; Acceler-\nation, Jerk-aware and Anisotropy methods (Zhang, Pintea,\nand Van Gemert 2017; Takeda et al. 2018, 2019) show in-\nsufficient amplification amplitude, and the other learning-\nbased methods (Oh et al. 2018; Singh, Murala, and Ko-\nsuru 2023a,b) show flickering artifacts and motion distor-\ntion originating from their spatial inconsistency. In contrast,\nwe achieve more robust results, noticeably improving arti-\nfacts and distortions while achieving satisfactory magnifi-\ncation amplitude. (2) Magnification factor Î±. Fig. 5 shows\nthe magnified results of shift-up and shift-down setups of the\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n5350\nOriginal Linear Phase Acceleration Jerk-aware Anisotropy LBVMM\n(a) Subtle dataset(b) Obvious dataset\nLNVMM Ours\n(c) Fabric dataset\nMDLMM\ntime\nRegionsST slices\ntime\nRegionsST slices\ntime\nRegionsST slices\nHand-Crafted Approaches Deep-Learning Approaches\nFrame 77\nFrame 28\nFrame 44\nFigure 4: Qualitative results of our method with existing methods on (a) Static, (b) Dynamic and (c) Fabric datasets with magni-\nfication factors Î± of 20, 10, and 20, respectively. We highlight spatial regions where motion occurs and provide spatiotemporal\n(ST) slices of magnified motion for better comparison.\nReference\nReference\nÎ± = 15\nÎ± = 15\n30th Frame90th Frame\nShift-downQueryÎ± = 5 Î± = 10 Î± = 15\nShift-upQuery Î± = 15\nÎ± = 5 Î± = 10\nFigure 5: Magnification visualization of the drum video from the Static dataset. We randomly sample two frames with shift-\ndown and shift-up motion. EulerMormer achieves reliable video motion magnification under different magnification factors Î±.\nQuery\n Shape ğ“ğ’”Texture ğğ’• Motionğœ¹ğ’ After ğ“•ğœ¹ğ’ After ğ‘´., ğ“â€²ğ’” After ğ“•ğ“ğ’”$,ğğ’• Magnified Result\nPhase 1 Phase 3Phase 2\nFigure 6: Dataflow of our method pipeline with the eye video from the Dynamic dataset. The disentangled texture, shape, and\nmotion feature maps have distinguishable vision characteristics. The dynamic filter F(Â·) effectively erases the noises in the\nstatic-dynamic field of the image and refines the texture-shape joint refinement process.\ndrum surface. We achieve reliable magnification at differ-\nent Î± levels. (3) Magnification dataflow.Fig. 6 displays the\ndataflow of EulerMormer. The disentangled texture, shape,\nand motion feature maps have distinguishable vision char-\nacteristics. Please pay attention to the dynamic filter F(Â·)\nin Phase 2, which effectively eliminates noise in the static\nfield of the motion while preserving important motion infor-\nmation in the dynamic field. On this basis, the visualization\nof F(Ï•â€²\ns(x, t), Ïˆt(x, t)) also validates the ability of F(Â·) to\nprocess the texture-magnified shape joint refinement.\nConclusion\nIn this paper, we have introduced EulerMormer, a novel\nTransformer-based end-to-end framework designed for\nVMM tasks from the Eulerian perspective, aiming to pro-\nvide more robust magnification effects. The core of Euler-\nMormer lies in embedding a dedicated dynamic filter within\nTransformer, enabling static-dynamic field adaptive denois-\ning for motion and recoupling refinement. Extensive quan-\ntitative and qualitative experiments demonstrate that Euler-\nMormer outperforms SOTA approaches.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n5351\nAcknowledgments\nThis work was supported by the National Key R&D Program\nof China (2022YFB4500600), the National Natural Science\nFoundation of China (62272144, 72188101, 62020106007,\nand U20A20183), and the Major Project of Anhui Province\n(202203a05020011).\nReferences\nAbnousi, F.; Kang, G.; Giacomini, J.; Yeung, A.; Zarafshar,\nS.; Vesom, N.; Ashley, E.; Harrington, R.; and Yong, C.\n2019. A novel noninvasive method for remote heart failure\nmonitoring: the EuleriAn video Magnification apPLications\nIn heart Failure studY (AMPLIFY). NPJ Digital Medicine,\n2(1): 80.\nBrattoli, B.; B Â¨uchler, U.; Dorkenwald, M.; Reiser, P.; Filli,\nL.; Helmchen, F.; Wahl, A.-S.; and Ommer, B. 2021. Unsu-\npervised behaviour analysis and magnification (uBAM) us-\ning deep learning. Nature Machine Intelligence, 3(6): 495â€“\n506.\nBruhn, A.; Weickert, J.; and Schn Â¨orr, C. 2005. Lu-\ncas/Kanade meets Horn/Schunck: Combining local and\nglobal optic flow methods. International Journal of Com-\nputer Vision, 61: 211â€“231.\nDavis, A.; Bouman, K. L.; Chen, J. G.; Rubinstein, M.;\nBuyukozturk, O.; Durand, F.; and Freeman, W. T. 2017. Vi-\nsual Vibrometry: Estimating Material Properties from Small\nMotions in Video. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 39(4).\nDavis, A.; Bouman, K. L.; Chen, J. G.; Rubinstein, M.; Du-\nrand, F.; and Freeman, W. T. 2015. Visual vibrometry: Es-\ntimating material properties from small motion in video. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 5335â€“5343.\nEitner, M.; Miller, B.; Sirohi, J.; and Tinney, C. 2021. Effect\nof broad-band phase-based motion magnification on modal\nparameter estimation. Mechanical Systems and Signal Pro-\ncessing, 146: 106995.\nErcan, B.; Eker, O.; Erdem, A.; and Erdem, E. 2023.\nEVREAL: Towards a Comprehensive Benchmark and Anal-\nysis Suite for Event-based Video Reconstruction. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 3942â€“3951.\nGuo, D.; Li, K.; Zha, Z.-J.; and Wang, M. 2019. Dadnet:\nDilated-attention-deformable convnet for crowd counting.\nIn Proceedings of the 27th ACM international conference\non multimedia, 1823â€“1832.\nHuang, D.; Bi, Y .; Navab, N.; and Jiang, Z. 2023.\nMotion Magnification in Robotic Sonography: Enabling\nPulsation-Aware Artery Segmentation. arXiv preprint\narXiv:2307.03698.\nKingma, D. P.; and Ba, J. 2015. Adam: A Method for\nStochastic Optimization. In International Conference on\nLearning Representations.\nLe Ngo, A. C.; and Phan, R. C.-W. 2019. Seeing the invisi-\nble: Survey of video motion magnification and small motion\nanalysis. ACM Computing Surveys, 52(6): 1â€“20.\nLi, K.; Guo, D.; and Wang, M. 2021. Proposal-free video\ngrounding with contextual pyramid network. InProceedings\nof the AAAI Conference on Artificial Intelligence, 1902â€“\n1910.\nLi, K.; Guo, D.; and Wang, M. 2023. ViGT: proposal-free\nvideo grounding with a learnable token in the transformer.\nScience China Information Sciences, 66(10): 202102.\nLi, K.; Li, J.; Guo, D.; Yang, X.; and Wang, M. 2023.\nTransformer-based Visual Grounding with Cross-modality\nInteraction. ACM Transactions on Multimedia Computing,\nCommunications and Applications, 19: 1â€“19.\nLiu, C.; Torralba, A.; Freeman, W. T.; Durand, F.; and Adel-\nson, E. H. 2005. Motion magnification. ACM Transactions\non Graphics, 24(3): 519â€“526.\nMehra, A.; Agarwal, A.; Vatsa, M.; and Singh, R. 2022. Mo-\ntion Magnified 3-D Residual-in-Dense Network for Deep-\nFake Detection. IEEE Transactions on Biometrics, Behav-\nior, and Identity Science, 5(1): 39â€“52.\nNguyen, X.-B.; Duong, C. N.; Li, X.; Gauch, S.; Seo, H.-\nS.; and Luu, K. 2023. Micron-BERT: BERT-Based Fa-\ncial Micro-Expression Recognition. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 1482â€“1492.\nOh, T.-H.; Jaroensri, R.; Kim, C.; Elgharib, M.; Durand, F.;\nFreeman, W. T.; and Matusik, W. 2018. Learning-based\nvideo motion magnification. In Proceedings of the Euro-\npean Conference on Computer Vision, 633â€“648.\nQi, H.; Guo, Q.; Juefei-Xu, F.; Xie, X.; Ma, L.; Feng, W.;\nLiu, Y .; and Zhao, J. 2020. Deeprhythm: Exposing deep-\nfakes with attentional visual heartbeat rhythms. In Proceed-\nings of the 28th ACM International Conference on Multime-\ndia, 4318â€“4327.\nQian, W.; Guo, D.; Li, K.; Tian, X.; and Wang, M. 2023.\nDual-path tokenlearner for remote photoplethysmography-\nbased physiological measurement with facial videos. arXiv\npreprint arXiv:2308.07771.\nQin, X.; Dai, H.; Hu, X.; Fan, D.-P.; Shao, L.; and Van Gool,\nL. 2022. Highly accurate dichotomous image segmentation.\nIn European Conference on Computer Vision, 38â€“56.\nRubinstein, M.; Wadhwa, N.; Durand, F.; Freeman, W. T.;\nand Wu, H.-Y . 2013. Revealing invisible changes in the\nworld. Science, 339(6119): 519â€“519.\nShen, H.; Zhao, Z.-Q.; and Zhang, W. 2023. Adaptive dy-\nnamic filtering network for image denoising. In Proceed-\nings of the AAAI Conference on Artificial Intelligence, vol-\nume 37, 2227â€“2235.\nShen, H.; Zhao, Z.-Q.; Zhang, Y .; and Zhang, Z. 2023. Mu-\ntual Information-driven Triple Interaction Network for Ef-\nficient Image Dehazing. In Proceedings of the 31st ACM\nInternational Conference on Multimedia, 7â€“16.\nShi, W.; Caballero, J.; Huszar, F.; Totz, J.; Aitken, A. P.;\nBishop, R.; Rueckert, D.; and Wang, Z. 2016. Real-Time\nSingle Image and Video Super-Resolution Using an Effi-\ncient Sub-Pixel Convolutional Neural Network. In Proceed-\nings of the IEEE Conference on Computer Vision and Pat-\ntern Recognition, 1874â€“1883.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n5352\nSingh, J.; Murala, S.; and Kosuru, G. 2023a. Lightweight\nNetwork for Video Motion Magnification. InProceedings of\nthe IEEE/CVF Winter Conference on Applications of Com-\nputer Vision, 2041â€“2050.\nSingh, J.; Murala, S.; and Kosuru, G. 2023b. Multi Do-\nmain Learning for Motion Magnification. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 13914â€“13923.\nTakeda, S.; Akagi, Y .; Okami, K.; Isogai, M.; and Kimata,\nH. 2019. Video magnification in the wild using fractional\nanisotropy in temporal distribution. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 1614â€“1622.\nTakeda, S.; Niwa, K.; Isogawa, M.; Shimizu, S.; Okami, K.;\nand Aono, Y . 2022. Bilateral Video Magnification Filter.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 17369â€“17378.\nTakeda, S.; Okami, K.; Mikami, D.; Isogai, M.; and Kimata,\nH. 2018. Jerk-aware video acceleration magnification. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 1769â€“1777.\nTang, S.; Hong, R.; Guo, D.; and Wang, M. 2022. Gloss\nsemantic-enhanced network with online back-translation for\nsign language production. In Proceedings of the 30th ACM\nInternational Conference on Multimedia, 5630â€“5638.\nWadhwa, N.; Rubinstein, M.; Durand, F.; and Freeman,\nW. T. 2013. Phase-based video motion processing. ACM\nTransactions on Graphics, 32(4): 1â€“10.\nWang, P.; Wang, X.; Wang, F.; Lin, M.; Chang, S.; Li, H.;\nand Jin, R. 2022a. Kvt: k-nn attention for boosting vision\ntransformers. In Computer Visionâ€“ECCV 2022: 17th Eu-\nropean Conference, Tel Aviv, Israel, October 23â€“27, 2022,\nProceedings, Part XXIV, 285â€“302.\nWang, Y .; Cai, J.; Liu, Y .; Chen, X.; and Wang, Y . 2022b.\nMotion-induced error reduction for phase-shifting profilom-\netry with phase probability equalization. Optics and Lasers\nin Engineering, 156: 107088.\nWang, Y .; Xu, H.; Zhu, H.; Rao, Y .; and Wang, Y . 2024.\nNonlinear high-order harmonics correction for phase mea-\nsuring profilometry. Optics & Laser Technology, 170:\n110248.\nWu, H.-Y .; Rubinstein, M.; Shih, E.; Guttag, J.; Durand, F.;\nand Freeman, W. 2012. Eulerian video magnification for\nrevealing subtle changes in the world.ACM Transactions on\nGraphics, 31(4): 1â€“8.\nWu, Y .; Wang, X.; Li, G.; and Shan, Y . 2022. AnimeSR:\nlearning real-world super-resolution models for animation\nvideos. Advances in Neural Information Processing Sys-\ntems, 35: 11241â€“11252.\nXia, Z.; Peng, W.; Khor, H.-Q.; Feng, X.; and Zhao, G.\n2020. Revealing the invisible with model and data shrinking\nfor composite-database micro-expression recognition. IEEE\nTransactions on Image Processing, 29: 8590â€“8605.\nYang, S.; Wu, T.; Shi, S.; Lao, S.; Gong, Y .; Cao, M.; Wang,\nJ.; and Yang, Y . 2022. Maniqa: Multi-dimension attention\nnetwork for no-reference image quality assessment. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 1191â€“1200.\nZamir, S. W.; Arora, A.; Khan, S.; Hayat, M.; Khan, F. S.;\nand Yang, M.-H. 2022. Restormer: Efficient transformer\nfor high-resolution image restoration. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 5728â€“5739.\nZhang, D.; Zhu, A.; Gong, X.; Wang, Y .; Guo, J.; and Zhang,\nX. 2023. Hybrid Eulerianâ€“Lagrangian framework for struc-\ntural full-field vibration quantification and modal shape vi-\nsualization. Measurement, 219: 113270.\nZhang, R.; Isola, P.; Efros, A. A.; Shechtman, E.; and Wang,\nO. 2018. The unreasonable effectiveness of deep features as\na perceptual metric. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 586â€“595.\nZhang, Y .; Han, X.; Zhang, H.; and Zhao, L. 2017. Edge\ndetection algorithm of image fusion based on improved So-\nbel operator. In 2017 IEEE 3rd Information Technology and\nMechatronics Engineering Conference, 457â€“461.\nZhang, Y .; Pintea, S. L.; and Van Gemert, J. C. 2017. Video\nacceleration magnification. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, 529â€“\n537.\nZhao, G.; Lin, J.; Zhang, Z.; Ren, X.; Su, Q.; and Sun, X.\n2019. Explicit sparse transformer: Concentrated attention\nthrough explicit selection.arXiv preprint arXiv:1912.11637.\nZheng, B.; Yuan, S.; Slabaugh, G.; and Leonardis, A. 2020.\nImage demoireing with learnable bandpass filters. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 3636â€“3645.\nZheng, P.; Fu, H.; Fan, D.-P.; Fan, Q.; Qin, J.; Tai, Y .-\nW.; Tang, C.-K.; and Van Gool, L. 2023. GCoNet+: A\nStronger Group Collaborative Co-Salient Object Detector.\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence.\nZhou, J.; Guo, D.; and Wang, M. 2022. Contrastive positive\nsample propagation along the audio-visual event line. IEEE\nTransactions on Pattern Analysis and Machine Intelligence.\nZhou, J.; Wang, J.; Zhang, J.; Sun, W.; Zhang, J.; Birch-\nfield, S.; Guo, D.; Kong, L.; Wang, M.; and Zhong, Y . 2022.\nAudioâ€“visual segmentation. In European Conference on\nComputer Vision, 386â€“403.\nZhou, J.; Zheng, L.; Zhong, Y .; Hao, S.; and Wang, M. 2021.\nPositive sample propagation along the audio-visual event\nline. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 8436â€“8444.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n5353",
  "topic": "Eulerian path",
  "concepts": [
    {
      "name": "Eulerian path",
      "score": 0.6539114117622375
    },
    {
      "name": "Magnification",
      "score": 0.4996159076690674
    },
    {
      "name": "Computer science",
      "score": 0.47390758991241455
    },
    {
      "name": "Transformer",
      "score": 0.4523780941963196
    },
    {
      "name": "Computer vision",
      "score": 0.3923477530479431
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37244993448257446
    },
    {
      "name": "Mathematics",
      "score": 0.18236806988716125
    },
    {
      "name": "Engineering",
      "score": 0.13764426112174988
    },
    {
      "name": "Lagrangian",
      "score": 0.11259588599205017
    },
    {
      "name": "Electrical engineering",
      "score": 0.09969034790992737
    },
    {
      "name": "Mathematical analysis",
      "score": 0.05344998836517334
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I16365422",
      "name": "Hefei University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210105595",
      "name": "Institute of Art",
      "country": "PL"
    },
    {
      "id": "https://openalex.org/I4210137491",
      "name": "National Science Centre",
      "country": "PL"
    }
  ]
}