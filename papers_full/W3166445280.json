{
  "title": "Wikipedia Entities as Rendezvous across Languages: Grounding Multilingual Language Models by Predicting Wikipedia Hyperlinks",
  "url": "https://openalex.org/W3166445280",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2250213072",
      "name": "Iacer Calixto",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2259244113",
      "name": "Alessandro Raganato",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2250886614",
      "name": "Tommaso Pasini",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2912351236",
    "https://openalex.org/W4295168876",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W135437175",
    "https://openalex.org/W2491063832",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2281909328",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2595644810",
    "https://openalex.org/W2742113707",
    "https://openalex.org/W4322614701",
    "https://openalex.org/W2971033911",
    "https://openalex.org/W2973088264",
    "https://openalex.org/W2898700502",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W2915774325",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2915977242",
    "https://openalex.org/W1664002590",
    "https://openalex.org/W1599894580",
    "https://openalex.org/W3035497479",
    "https://openalex.org/W2970752815",
    "https://openalex.org/W2519314406",
    "https://openalex.org/W2963034893",
    "https://openalex.org/W2131540451",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2956105246",
    "https://openalex.org/W3034782826",
    "https://openalex.org/W4206856021",
    "https://openalex.org/W3102617222",
    "https://openalex.org/W3082793270",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2065157922",
    "https://openalex.org/W2251529656",
    "https://openalex.org/W1549944563",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W3104415840",
    "https://openalex.org/W1566049083",
    "https://openalex.org/W2102153514",
    "https://openalex.org/W45923878",
    "https://openalex.org/W2049303930",
    "https://openalex.org/W2020979915",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W2740782137",
    "https://openalex.org/W3045462440",
    "https://openalex.org/W2031026221",
    "https://openalex.org/W3037191812",
    "https://openalex.org/W2293179254",
    "https://openalex.org/W3048018176",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3116343068"
  ],
  "abstract": "Masked language models have quickly become the de facto standard when processing text. Recently, several approaches have been proposed to further enrich word representations with external knowledge sources such as knowledge graphs. However, these models are devised and evaluated in a monolingual setting only. In this work, we propose a language-independent entity prediction task as an intermediate training procedure to ground word representations on entity semantics and bridge the gap across different languages by means of a shared vocabulary of entities. We show that our approach effectively injects new lexical-semantic knowledge into neural models, improving their performance on different semantic tasks in the zero-shot crosslingual setting. As an additional advantage, our intermediate training does not require any supplementary input, allowing our models to be applied to new datasets right away. In our experiments, we use Wikipedia articles in up to 100 languages and already observe consistent gains compared to strong baselines when predicting entities using only the English Wikipedia. Further adding extra languages lead to improvements in most tasks up to a certain point, but overall we found it non-trivial to scale improvements in model transferability by training on ever increasing amounts of Wikipedia languages.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 3651–3661\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n3651\nWikipedia Entities asRendezvous across Languages: Grounding\nMultilingual Language Models by Predicting Wikipedia Hyperlinks\nIacer Calixto1,2 Alessandro Raganato3 Tommaso Pasini4,∗\n1Center for Data Science, New York University 2ILLC, University of Amsterdam\n3Department of Digital Humanities, University of Helsinki, Finland\n4Department of Computer Science, University of Copenhagen\niacer.calixto@nyu.edu, alessandro.raganato@helsinki.fi,\ntommaso.pasini@di.ku.dk\nAbstract\nMasked language models have quickly be-\ncome the de facto standard when processing\ntext. Recently, several approaches have been\nproposed to further enrich word representa-\ntions with external knowledge sources such as\nknowledge graphs. However, these models are\ndevised and evaluated in a monolingual setting\nonly. In this work, we propose a language-\nindependent entity prediction task as an in-\ntermediate training procedure to ground word\nrepresentations on entity semantics and bridge\nthe gap across different languages by means of\na shared vocabulary of entities. We show that\nour approach effectively injects new lexical-\nsemantic knowledge into neural models, im-\nproving their performance on different seman-\ntic tasks in the zero-shot crosslingual setting.\nAs an additional advantage, our intermediate\ntraining does not require any supplementary in-\nput, allowing our models to be applied to new\ndatasets right away. In our experiments, we\nuse Wikipedia articles in up to 100 languages\nand already observe consistent gains compared\nto strong baselines when predicting entities\nusing only the English Wikipedia. Further\nadding extra languages lead to improvements\nin most tasks up to a certain point, but overall\nwe found it non-trivial to scale improvements\nin model transferability by training on ever in-\ncreasing amounts of Wikipedia languages.\n1 Introduction\nPretrained Multilingual Masked Language Models\n(MMLMs) such as mBERT (Devlin et al., 2019),\nXLM-R (Conneau et al., 2020) and their variants\nhave achieved state-of-the-art results across diverse\nnatural language understanding tasks. Typically, a\nMMLM model is pretrained on very large amounts\nof raw text in different languages using the masked\nlanguage modelling (MLM) objective and is further\nﬁnetuned on (usually limited amounts of) task data.\n∗ ∗Work carried out while at the University of Rome “La\nSapienza”.\nIn the zero-shot crosslingual setting, which is our\nfocus in this paper, a MMLM is ﬁnetuned on the\ntarget task using data in a single language (e.g.,\nEnglish) and is evaluated on the same task but in\ndifferent languages (e.g., non-English languages).\nWe introduce the multilingual Wikipedia hy-\nperlink prediction objective to contextualise\nwords in a text with entities and concepts from\nan external knowledge source by using Wikipedia\narticles in up to 100 languages. Hyperlink predic-\ntion is a knowledge-rich task designed to (1) inject\nsemantic knowledge from Wikipedia entities and\nconcepts into the MMLM token representations,\nand (2) with a similar motivation as the translated\nlanguage modelling loss of Conneau and Lample\n(2019), i.e., to inject explicit language-independent\nknowledge into a model trained via self-supervised\nlearning, but in our case without parallel data. We\ndevise a training procedure where we mask out hy-\nperlinks in Wikipedia articles and train the MMLM\nto predict the hyperlink identiﬁer similarly to stan-\ndard MLM but using a “hyperlink vocabulary” of\n250k concepts shared across languages.\nWe use the state-of-the-art MMLM XLM-R-\nlarge (Conneau et al., 2020) and show that by\nadding an add-on training step using Wikipedia\nhyperlink prediction we consistently improve sev-\neral zero-shot crosslingual natural language under-\nstanding tasks across a diverse array of languages:\ncrosslingual Word Sense Disambiguation in 18 lan-\nguages including English (XL-WSD; Pasini et al.,\n2021); the crosslingual Word-in-Context task (XL-\nWiC; Raganato et al., 2020) in 12 non-English lan-\nguages; and in 7 tasks from the XTREME bench-\nmark (Hu et al., 2020) in up to 40 languages.\nRecently, Zhang et al. (2019, ERNIE) and Peters\net al. (2019, KnowBERT) devised different meth-\nods to incorporate entities from external knowledge\ngraphs into masked language model (LM) training.\nSince then, several works followed (Wang et al.,\n2021; Sun et al., 2020; Xiong et al., 2020; Yamada\n3652\nTask\ns\nMLE\nTarget \nTask - EN\nTarget Task\nEN - Loss\nTask\ns\nMLE\nTarget \nTask - EN\nTarget Task\nEN - Loss\nbn:00021494n\nComputer science (EN), Informatica (IT), \n计算机科学 (ZH), ...\nbn:00002705n\n[CLS] ho annistudiato [ENT] per diversi . [SEP]\n[CLS]  Ho studiato   [MASK]   [MASK]  per  diversi  anni  .  [SEP]\n[CLS]   [MASK]   [MASK]   is   the  study  of  [MASK]  .  [SEP]\nMMLM\n[CLS] [ENT] [ENT]is the of . [SEP]\nAlgorithm (EN), Algoritmo (IT),\n算法 (ZH), … \nWikipedia hyperlink prediction training\nTarget task training Target task evaluation\nTask\ns\nTask\ns\nMMLM\nTarget \nTask - EN\nTarget Task\nEN - Loss\nARLanguages\nhidden\nstateswords\n[ENT]\n[ENT]\nstudy\nTask 1\nTasks\nMMLM\nTask - ARTask - ARTarget \nTask - AR\nTask 1\nLanguages\nTask\ns\nTarget Task AR - \nPredictionTarget Task AR - \nPredictionTarget Task AR - \nPrediction\nFigure 1: We ﬁnetune a pretrained MMLM using multilingual Wikipedia hyperlink prediction, then further train a\nmodel on a set of target tasks in English and evaluate on non-English data (i.e., zero-shot crosslingual setting).\net al., 2020) showing increasingly better perfor-\nmance than masked LMs that rely on information\nfrom raw text only. Nevertheless, all these methods\nwere proposed for a single language1 and cannot\nbe easily applied to transfer learning in a zero-shot\ncrosslingual setting.\n2 Approach\nNotation Let x1:m = MMLM(x1:m) be contex-\ntualised word representations for some input text\nx1:m with m words, and computed with a pre-\ntrained MMLM. Let xn:k (n ≥1, k≤m) be a sub-\nsequence of contextualised word representations of\na single hyperlink xn:k consisting of k −n words.\nIn our working example we use a single hyperlink\nxn:k for simplicity, but in practice there may be\nmultiple hyperlinks in the input x1:m.\nData We download and preprocess Wikipedia ar-\nticles in 100 languages, and extract all hyperlinks\nin the text. We use BabelNet (Navigli and Ponzetto,\n2010) — a large multilingual knowledge base com-\nprising WordNet, Wikipedia, and many other re-\nsources — to map Wikipedia articles in different\nlanguages about the same subject onto unique iden-\ntiﬁers. For instance, regardless of their language\nall “computer science” articles are mapped to the\nsame identiﬁer ht, in this case bn:00021494n.2\nAfter each article is mapped to a single identiﬁer,\nwe create prediction targets for every hyperlink by\nusing the identiﬁer of its referenced article. See\nAppendix A for more details.\n1Mostly English, except for Sun et al. (2020) where Man-\ndarin is also used in a monolingual setting.\n2https://babelnet.org/synset?word=bn:\n00021494n&lang=EN\nWikipedia Hyperlink Prediction Our main\ngoal is to use the rich semantic knowledge con-\ntained in the multilingual Wikipedias’ structure to\nimprove language model pretraining. Our approach\ncan be seen as intermediate-task training (Phang\net al., 2018, 2020) where we use Wikipedias’ hy-\nperlinks as labelled data to further ﬁnetune a pre-\ntrained MMLM model before training it one last\ntime in the actual target task of interest. Moti-\nvated by recent studies on pretrained language en-\ncoders demonstrating that semantic features are\nhighlighted in higher layers (Raganato and Tiede-\nmann, 2018; Jawahar et al., 2019; Cui et al., 2020;\nRogers et al., 2021), we further train only the last\ntwo layers of the MMLM. Moreover, similarly to\nthe MLM procedure, we replace the hyperlink to-\nkens xn:k by the [MASK] token or by a random\ntoken 80% and 10% of the time, respectively (De-\nvlin et al., 2019).\nSince the number of Wikipedia articles is very\nlarge, we only consider the most frequent 250k ref-\nerenced articles ht as possible hyperlinks in our\nmodel and we use the adaptive softmax activa-\ntion function to speed-up training (Grave et al.,\n2017). Our objective allows us to consider text-\nentity alignments during training only. At predic-\ntion time, instead, we simply feed the model with\nraw text with no need of precomputed alignments.\nThis makes our model easy to use and to adapt to\nmany different scenarios. For more details on the\nmodel architectures and objective, see Appendix B.\n3 Experimental Setup\nWe use XLM-R-large (Conneau et al., 2020) as\nour MMLM, which is pretrained on a large volume\n3653\nof raw multilingual corpora using MLM training.\n3.1 Models\nWe propose three different model architectures\nwhich differ in how the input to the hyperlink clas-\nsiﬁcation head is computed. In Token we use the\nvector representation of each token in the hyper-\nlink text xi, i∈[n, k] as input to the prediction\nhead. In Concat CLS we use the concatenation\n[xi; xCLS] of the representation of each word in the\nhyperlink xi, i∈[n, k] with the [CLS] token rep-\nresentation as input to the prediction head. Finally,\nin Replace CLS the input to the prediction head is\nthe representation of each word in the hyperlink\nxi, i∈[n, k] with probability pr or the [CLS]\ntoken representation xCLS with probability 1 −pr.\nMore details on the architectures in Appendix B.1.\n3.2 Methodology\nWe follow a sequential, three steps approach to\ntraining and evaluating our models. We ﬁrst ﬁne-\ntune the pretrained MMLM on the Wikipedia hyper-\nlink prediction task, then ﬁnetune again this time\non the target-task training data in English, and ﬁ-\nnally evaluate the model on non-English target-task\nevaluation data in a zero-shot crosslingual setting\n(see Figure 1). We use Wikipedia articles in differ-\nent sets of languages (Section 3.3) and experiment\nwith many diverse target tasks (Section 3.4).\n3.3 Wikipedia Languages\nWe experiment using only English (Wiki EN), 15\ndifferent languages ( Wiki 15), or 100 Wikipedia\nlanguages ( Wiki 100). By doing that, i) we in-\nclude a monolingual albeit resource-rich baseline\n(Wiki EN), ii) we investigate the impact of includ-\ning a varied mixture of languages from different\nfamilies (Wiki 15), and iii) we also experiment if go-\ning massively multilingual has a noticeable impact\non crosslingual transferability (Wiki 100).\n3.4 Target Tasks\nWord Sense Disambiguation We follow the\nzero-shot crosslingual setting of Pasini et al. (2021,\nXL-WSD), which includes 17 languages plus En-\nglish, i.e., we train on the English SemCor (Miller\net al., 1993) dataset merged with the Princeton\nWordNet Gloss corpus3 and test on all available lan-\nguages (Miller et al., 1993; Raganato et al., 2017;\nEdmonds and Cotton, 2001; Snyder and Palmer,\n3http://wordnetcode.princeton.edu/\nglosstag.shtml\n2004; Pradhan et al., 2007; Navigli et al., 2007;\nAgirre et al., 2010; Navigli et al., 2013; Moro and\nNavigli, 2015; Pociello et al., 2008; Simov and\nOsenova, 2010; Ben´ıtez et al., 1998; Huang et al.,\n2010; Raffaelli et al., 2008; Pedersen et al., 2009;\nPostma et al., 2016; Vider and Orav, 2002; Guino-\nvart, 2011; Mih´altz et al., 2008; Isahara et al., 2008;\nYoon et al., 2009; Fiˇser et al., 2012).\nWord-in-Context We use the crosslingual Word-\nin-Context dataset (XL-WiC; Raganato et al., 2020)\nwith data in 12 diverse languages. The task is to\npredict whether an ambiguous word that appears\nin two different sentences share the same mean-\ning. We ﬁnetune the model on the English WiC\n(Pilehvar and Camacho-Collados, 2019) dataset\nand evaluate on the 12 XL-WiC languages.\nXTREME The XTREME (Hu et al., 2020) eval-\nuation suite contains diverse tasks in up to 40 dif-\nferent languages. We perform crosslingual evalu-\nation on: question answering (XQuAD; MLQA;\nTyDiQA; Artetxe et al., 2020; Lewis et al., 2020;\nClark et al., 2020), natural language inference\n(XNLI; Conneau et al., 2018), paraphrase detec-\ntion (PAWS-X; Yang et al., 2019), part-of-speech\ntagging (POS; Nivre et al., 2018), and named entity\nrecognition (NER; Pan et al., 2017). As standard\nin the two unsupervised sentence retrieval tasks,\nBUCC (Zweigenbaum et al., 2018), and Tatoeba\n(Artetxe and Schwenk, 2019), XLM-R is tested\nconsidering the output of its 14-th layer, which,\nhowever, is not tuned during our intermediate task.\nWe therefore do not report results on these tasks.4\nTask Architectures Across all the tasks, we ﬁne-\ntune transformer-based models by adding a classi-\nﬁcation head for each task.5\n4 Results and Discussion\nResults on XL-WSD and XL-WiC tasks (Tables 1\nand 2) suggest that our models have a better grasp\nof word-level semantics than XLM-R, which does\nnot have explicit semantic signals during its pre-\ntraining. This is consistent across languages and\nhyperlink prediction architectures, also when com-\npared to the baseline XLM-R additionally ﬁnetuned\nusing MLM training on in-domain Wikipedia data.\nOur best models outperform the baselines in both\ntasks by several points. Interestingly, training on\n4More details in Appendix B.2.\n5Details in Appendix B.1.1 (XL-WSD), B.1.2 (XL-WiC),\nand B.1.3 (XTREME).\n3654\nWIKI EN WIKI 15 WIKI 100\nXLM-R +MLM +T +C +R +MLM +T +C +R +MLM +T +C +R\nENALL 77.7 77.4 76.8 78.4 78.5 77.6 78.5 78.7 78.5 77.4 78.4 78.6 78.3\nBG 72.0 71.9 72.1 72.6 70.8 71.7 73.3 73.5 73.1 71.8 72.9 73.2 73.4\nCA 50.0 49.5 52.7 52.9 50.8 49.9 54.0 53.7 54.3 50.1 54.6 52.7 54.7\nDA 80.6 80.4 81.7 81.7 79.9 80.6 82.4 82.5 82.4 80.7 82.4 82.8 82.1\nDE 83.2 83.6 83.6 84.1 83.9 83.3 83.6 85.2 83.1 83.4 84.1 83.1 83.3\nES 75.9 76.8 78.2 78.0 75.2 76.9 78.4 79.1 78.2 77.3 78.2 78.1 78.5\nET 66.1 66.2 66.6 67.2 65.9 66.6 67.7 68.4 68.3 66.7 68.3 68.2 68.0\nEU 47.2 46.3 47.7 49.0 44.4 46.4 48.7 49.2 49.4 46.1 49.7 48.7 50.3\nFR 83.9 83.9 84.2 84.4 83.4 83.9 84.7 84.1 84.6 83.6 83.4 84.1 84.1\nGL 66.3 65.6 67.3 68.2 63.5 66.1 69.7 69.0 70.2 65.3 69.3 68.7 70.2\nHR 72.3 72.7 73.9 74.0 72.2 72.8 74.3 74.2 74.5 72.9 74.5 74.1 74.8\nHU 67.6 68.6 70.7 70.5 67.7 68.3 71.5 71.4 72.1 68.8 72.0 71.1 72.1\nIT 77.7 78.9 78.7 78.8 77.1 78.8 79.3 79.4 79.1 78.5 79.7 79.5 79.5\nJA 61.9 62.3 67.1 67.9 65.0 62.4 68.9 68.3 69.5 62.3 69.0 67.1 68.4\nKO 64.2 63.6 64.8 64.5 64.9 63.6 65.5 65.7 65.9 63.4 64.8 65.6 65.1\nNL 59.2 59.8 60.5 60.5 58.3 59.7 61.6 61.2 62.0 59.8 61.2 61.0 61.4\nSL 68.4 67.2 68.9 68.6 67.0 67.4 69.1 67.9 69.0 67.8 68.4 69.5 69.6\nZH 51.6 52.0 55.9 56.2 56.2 52.2 56.6 56.8 56.5 52.5 56.4 56.0 55.9\nAvg. 65.7 65.8 67.7 68.0 66.2 65.9 68.7 68.6 68.8 66.0 68.6 68.3 68.7\nTable 1: We report F-1 performance on the XL-WSD dataset. Avg. is the micro-average across all languages\nbut English. +MLM is the baseline model XLM-R which we continued training with the MLM objective only,\nwhereas +T, +C, +R are the Token, Concat CLS and Replace CLS models, respectively.\nBG DA ET FA HR JA KO NL ZH DE FR IT Avg.\nXLM-R 61.8 65.2 62.6 65.8 66.9 61.7 65.6 69.2 68.3 61.1 58.8 62.2 64.1\n+MLM (Wiki EN) 63.0 69.9 69.7 73.6 71.3 63.7 69.5 72.4 71.5 65.1 62.3 62.5 67.9\n+MLM (Wiki 15) 64.1 67.8 68.5 73.0 70.8 62.7 66.8 72.9 69.8 64.1 61.5 64.5 67.2\n+MLM (Wiki 100) 65.5 67.9 68.5 76.3 69.1 60.8 71.1 70.5 68.3 61.7 59.7 61.2 66.7\nWiki EN\n+ T 65.3 69.6 65.6 77.4 69.4 63.2 67.9 72.6 70.5 65.4 62.4 64.2 67.8\n+ C 66.6 69.0 68.7 74.9 74.3 65.9 69.5 72.9 70.8 67.1 63.4 66.6 69.1\n+ R 68.4 68.4 69.0 75.4 73.0 65.3 68.4 73.0 69.6 66.3 62.4 64.9 68.7\nWiki 15\n+ T 64.6 67.5 64.1 75.8 68.9 62.7 71.0 70.3 67.2 63.8 61.6 65.0 66.9\n+ C 65.0 69.5 68.7 75.3 69.6 64.3 69.9 73.4 70.1 65.6 61.9 62.5 68.0\n+ R 67.4 68.0 64.4 73.3 72.1 63.4 65.1 69.5 67.1 63.3 59.8 61.7 66.2\nWiki 100\n+ T 66.7 69.7 70.5 78.5 67.9 64.8 72.3 74.3 70.9 67.2 64.0 65.7 69.4\n+ C 61.1 64.1 65.6 71.3 66.9 60.1 68.0 67.9 66.9 59.5 57.9 59.1 64.0\n+ R 65.0 70.3 68.2 73.0 72.1 62.0 68.5 71.7 72.3 65.3 61.8 63.2 67.8\nTable 2: Accuracy scores on the crosslingual Word-in-Context (XL-WiC) test set.\n15 languages tends to slightly outperform training\non all 100 languages on XL-WSD, but on XL-WiC\nresults with our best models trained on 100 lan-\nguages outperforms all other conﬁgurations most\nof the time by a reasonable margin. These results\ncorroborate our hunch that the intermediate task in-\njects semantic knowledge within the neural model.\nIn Table 3, we conﬁrm that our models preserve\nthe sentence-level comprehension capabilities of\nthe underlying XLM-R architecture and that it per-\nforms either comparably or favourably to the base-\nlines in the XTREME benchmark, across target\ntasks and languages.\nTraining on the English Wikipedia only can be\nsurprisingly effective at times (Tables 2 and 3),\nand training on 100 languages shows more consis-\ntent improvements only on XL-WiC but fails to\nlead to similar improvements on other tasks. We\nnote that performance on XL-WSD is similar when\nusing 15 or 100 languages, while our evaluation\nusing XTREME shows that performance is slightly\nworse when using 100 languages compared to us-\ning 15 languages only. We conjecture this could be\ndue to the fact we ﬁnetune only the last two layers\nof XLM-R (see Appendix B), so the model retains\nmost of the multilingual knowledge it learned dur-\n3655\nXNLI PAWS-X POS NER XQuAD MLQA TyDiQA Avg.\nacc. acc. F1 F1 F1 / EM F1 / EM F1 / EM\nHu et al. (2020) 79.2 86.4 72.6 65.4 76.6 / 60.8 71.6 / 53.2 65.1 / 45.0 70.1\nXLM-R (Ours) 78.6 87.9 76.1 64.0 71.7 / 56.3 70.3 / 50.0 72.6 / 57.0 70.8\n+MLM (Wiki EN) 79.1 87.9 76.4 62.3 70.6 / 55.2 69.3 / 50.0 72.7 / 56.8 70.4\n+MLM (Wiki 15) 79.3 88.7 75.7 64.4 71.8 / 56.8 70.2 / 50.6 72.6 / 56.7 71.1\n+MLM (Wiki 100) 79.2 88.0 76.0 63.4 71.5 / 56.5 70.1 / 50.5 73.5 / 57.4 70.9\nWiki EN\n+ T 78.7 88.3 77.3 63.6 70.8 / 55.6 69.8 / 50.4 73.2 / 57.0 70.9\n+ C 79.0 87.9 76.9 63.7 71.5 / 55.7 70.3 / 50.0 73.0 / 57.2 70.9\n+ R 78.7 88.6 76.9 64.4 71.1 / 55.8 69.6 / 50.1 72.7 / 57.0 71.0\nWiki 15\n+ T 79.0 88.1 77.2 64.1 71.3 / 56.5 70.4 / 50.6 73.4 / 57.8 71.2\n+ C 79.2 88.4 77.3 64.7 72.1 / 56.9 70.8 / 50.5 73.2 / 57.3 71.4\n+ R 79.1 88.3 76.7 64.7 71.5 / 56.4 70.3 / 50.6 72.8 / 56.7 71.1\nWiki 100\n+ T 78.6 88.8 76.9 64.8 71.7 / 56.0 70.1 / 50.0 72.7 / 56.9 71.1\n+ C 78.6 88.6 77.6 62.1 71.2 / 56.4 69.9 / 50.0 73.2 / 57.4 70.9\n+ R 78.8 87.6 76.7 64.2 71.1 / 56.1 69.9 / 50.3 73.1 / 57.4 70.9\nTable 3: Results on different target tasks of the XTREME benchmark.\ning pretraining (Liu et al., 2019; Hao et al., 2019).\nWe also hypothesise that the English Wikipedia\nsize (in number of words) and quality (in coverage\nof our hyperlink vocabulary) may also be a rea-\nson why training solely on English already brings\nlarge gains in transfer to other tasks. For com-\nparison, the English Wikipedia is the one with\nthe most data, i.e., about 73M hyperlinks, where\nthe second highest resource language is German\nwith only about 28M hyperlinks (see Table 4 in\nAppendix B). Regarding the coverage of our hy-\nperlink vocabulary with 250k entries, the English\nWikipedia covers over249k hyperlink types at least\n10 times, whereas the second highest coverage is\nfor the French Wikipedia, which covers over 142k\nhyperlink types at least 10 times. We plan on in-\nvestigating the effect of the size and coverage of\nhyperlinks further in future work.\nLimitations Finally, we highlight that: (1) We\nreport results using single model runs, therefore we\nhave no estimates of the variance of these models;\n(2) We lack a more thorough hyperparameter search\nto further consolidate our results. In both cases, the\nreason we made such choices is because of the high\ncost of training large models such as XLM-R large.\n5 Conclusions and Future work\nWe presented a multilingual Wikipedia hyperlink\nprediction intermediate task to improve the pretrain-\ning of contextualised word embedding models. We\ntrained three model variants on different sets of lan-\nguages, ﬁnding that injecting multilingual seman-\ntic knowledge consistently improves performance\non several zero-shot crosslingual tasks. As future\nwork, we plan to devise a solution to allow crosslin-\ngual transferability to scale more efﬁciently with\nthe number of languages. Finally, we will investi-\ngate the impact on resource-poor vs resource-rich\nlanguages, and the effect of the size and coverage\nof hyperlinks in model transferability.\nAcknowledgments\nWe would like to thank Clara Vania and Sam Bow-\nman for comments on early versions of this work,\nand our three anonymous reviewers for their help-\nful comments and feedback.\nIC has received funding from the European\nUnion’s Horizon 2020 research and innovation\nprogramme under the Marie Skłodowska-Curie\ngrant agreement No 838188. TP and AR grate-\nfully acknowledge the support of the ERC Consol-\nidator Grants MOUSSE No. 726487, and FoTran\nNo. 771113 under the European Union’s Horizon\n2020 research and innovation programme. AR also\nthanks the CSC - IT Center for Science (Finland)\nfor the computational resources.\nReferences\nEneko Agirre, Oier Lopez de Lacalle, Christiane Fell-\nbaum, Shu-Kai Hsieh, Maurizio Tesconi, Monica\nMonachini, Piek V ossen, and Roxanne Segers. 2010.\nSemEval-2010 task 17: All-words word sense dis-\n3656\nambiguation on a speciﬁc domain. In Proc. of Se-\nmEval.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 4623–4637, Online. Asso-\nciation for Computational Linguistics.\nMikel Artetxe and Holger Schwenk. 2019. Massively\nMultilingual Sentence Embeddings for Zero-Shot\nCross-Lingual Transfer and Beyond. Transactions\nof the ACL 2019.\nGiusepppe Attardi. 2015. Wikiextractor. https://\ngithub.com/attardi/wikiextractor.\nLaura Ben´ıtez, Sergi Cervell, Gerard Escudero, M`onica\nL´opez, German Rigau, and Mariona Taul ´e. 1998.\nMethods and tools for building the Catalan WordNet.\nProc. of ELRA Workshop on Language Resources for\nEuropean Minority Languages.\nMichele Bevilacqua and Roberto Navigli. 2020. Break-\ning through the 80% glass ceiling: Raising the state\nof the art in word sense disambiguation by incor-\nporating knowledge graph information. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics , pages 2854–2864,\nOnline. Association for Computational Linguistics.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. TyDi QA: A bench-\nmark for information-seeking question answering in\ntypologically diverse languages. Transactions of the\nAssociation for Computational Linguistics , 8:454–\n470.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451. Association for Computational Linguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In H. Wal-\nlach, H. Larochelle, A. Beygelzimer, F. d’Alch´e Buc,\nE. Fox, and R. Garnett, editors, Advances in Neu-\nral Information Processing Systems 32 (NeurIPS) ,\npages 7059–7069. Curran Associates, Inc.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. XNLI: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2475–2485,\nBrussels, Belgium. Association for Computational\nLinguistics.\nLeyang Cui, Sijie Cheng, Yu Wu, and Yue Zhang.\n2020. Does bert solve commonsense task via\ncommonsense knowledge? arXiv preprint\narXiv:2008.03945.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nPhilip Edmonds and Scott Cotton. 2001. SENSEV AL-\n2: Overview. In Proceedings of SENSEVAL-2 Sec-\nond International Workshop on Evaluating Word\nSense Disambiguation Systems , pages 1–5. Associ-\nation for Computational Linguistics.\nDarja Fi ˇser, Jernej Novak, and Toma ˇz Erjavec. 2012.\nSloWNet 3.0: development, extension and cleaning.\nIn Proc. of 6th International Global Wordnet Confer-\nence.\nEdouard Grave, Armand Joulin, Moustapha Ciss ´e,\nDavid Grangier, and Herv ´e J ´egou. 2017. Efﬁcient\nsoftmax approximation for gpus. In International\nConference on Machine Learning (ICML) , pages\n1302–1310. PMLR.\nXavier G´omez Guinovart. 2011. Galnet: WordNet 3.0\ndo galego. Linguam´atica, 3(1).\nYaru Hao, Li Dong, Furu Wei, and Ke Xu. 2019. Visu-\nalizing and understanding the effectiveness of BERT.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 4143–\n4152, Hong Kong, China. Association for Computa-\ntional Linguistics.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. Xtreme: A massively multilingual multi-task\nbenchmark for evaluating cross-lingual generaliza-\ntion. Proceedings of the 37th International Confer-\nence on Machine Learning.\nChu-Ren Huang, Shu-Kai Hsieh, Jia-Fei Hong, Yun-\nZhu Chen, I-Li Su, Yong-Xiang Chen, and Sheng-\nWei Huang. 2010. Chinese WordNet: Design, Im-\nplementation and Application of an Infrastructure\nfor Cross-Lingual Knowledge Processing. Journal\nof Chinese Information Processing, 24(2).\nHitoshi Isahara, Francis Bond, Kiyotaka Uchimoto,\nMasao Utiyama, and Kyoko Kanzaki. 2008. Devel-\nopment of the Japanese WordNet. In Sixth Interna-\ntional conference on Language Resources and Eval-\nuation.\n3657\nGanesh Jawahar, Beno ˆıt Sagot, and Djam ´e Seddah.\n2019. What does bert learn about the structure of\nlanguage? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3651–3657.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. International\nConference on Learning Representations.\nPatrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2020. MLQA: Evalu-\nating cross-lingual extractive question answering. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 7315–\n7330, Online. Association for Computational Lin-\nguistics.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In International Con-\nference on Learning Representations.\nM´arton Mih ´altz, Csaba Hatvani, Judit Kuti, Gy ¨orgy\nSzarvas, J ´anos Csirik, G ´abor Pr ´osz´eky, and Tam ´as\nV´aradi. 2008. Methods and Results of the Hungar-\nian WordNet Project. In Proc. of The Fourth Global\nWordNet Conference.\nGeorge A Miller, Claudia Leacock, Randee Tengi, and\nRoss T Bunker. 1993. A semantic concordance. In\nProceedings of the workshop on Human Language\nTechnology, pages 303–308.\nAndrea Moro and Roberto Navigli. 2015. SemEval-\n2015 task 13: Multilingual all-words sense disam-\nbiguation and entity linking. In Proceedings of the\n9th International Workshop on Semantic Evaluation\n(SemEval 2015), pages 288–297, Denver, Colorado.\nRoberto Navigli, David Jurgens, and Daniele Vannella.\n2013. SemEval-2013 task 12: Multilingual word\nsense disambiguation. In Proc. of SemEval.\nRoberto Navigli, Kenneth C. Litkowski, and Orin Har-\ngraves. 2007. SemEval-2007 task 07: Coarse-\ngrained English all-words task. In Proc. of SemEval.\nRoberto Navigli and Simone Paolo Ponzetto. 2010. Ba-\nbelNet: Building a very large multilingual semantic\nnetwork. In Proceedings of the 48th Annual Meet-\ning of the Association for Computational Linguistics,\npages 216–225, Uppsala, Sweden. Association for\nComputational Linguistics.\nJoakim Nivre, Mitchell Abrams, ˇZeljko Agi ´c, Lars\nAhrenberg, Lene Antonsen, Maria Jesus Aranzabe,\nGashaw Arutie, Masayuki Asahara, Luma Ateyah,\nMohammed Attia, et al. 2018. Universal dependen-\ncies 2.2.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel\nNothman, Kevin Knight, and Heng Ji. 2017. Cross-\nlingual name tagging and linking for 282 languages.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers) , pages 1946–1958, Vancouver,\nCanada. Association for Computational Linguistics.\nTommaso Pasini, Alessandro Raganato, and Roberto\nNavigli. 2021. XL-WSD: An extra-large and cross-\nlingual evaluation framework for word sense disam-\nbiguation. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence (AAAI).\nBolette S. Pedersen, Sanni Nimb, Jørg Asmussen, Nico-\nlai Hartvig Sørensen, Lars Trap-Jensen, and Henrik\nLorentzen. 2009. DanNet: the challenge of com-\npiling a wordnet for Danish by reusing a monolin-\ngual dictionary. Language Resources and Evalua-\ntion, 43.\nMatthew E Peters, Mark Neumann, Robert Logan, Roy\nSchwartz, Vidur Joshi, Sameer Singh, and Noah A\nSmith. 2019. Knowledge enhanced contextual word\nrepresentations. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 43–54.\nJason Phang, Iacer Calixto, Phu Mon Htut, Yada\nPruksachatkun, Haokun Liu, Clara Vania, Katha-\nrina Kann, and Samuel Bowman. 2020. English\nintermediate-task training improves zero-shot cross-\nlingual transfer too. In Proceedings of the 1st Con-\nference of the Asia-Paciﬁc Chapter of the Associa-\ntion for Computational Linguistics and the 10th In-\nternational Joint Conference on Natural Language\nProcessing, pages 557–575.\nJason Phang, Thibault F ´evry, and Samuel R Bowman.\n2018. Sentence encoders on stilts: Supplementary\ntraining on intermediate labeled-data tasks. arXiv\npreprint arXiv:1811.01088.\nMohammad Taher Pilehvar and Jose Camacho-\nCollados. 2019. WiC: the word-in-context dataset\nfor evaluating context-sensitive meaning represen-\ntations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 1267–1273, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nEli Pociello, Antton Gurrutxaga, Eneko Agirre, Iza-\nskun Aldezabal, and German Rigau. 2008. WN-\nTERM: Enriching the MCR with a terminological\n3658\ndictionary. In Proceedings of the Sixth Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC’08), Marrakech, Morocco. European\nLanguage Resources Association (ELRA).\nMarten Postma, Emiel van Miltenburg, Roxane Segers,\nAnneleen Schoen, and Piek V ossen. 2016. Open\nDutch WordNet. In Proc. of the Eight Global Word-\nnet Conference.\nSameer Pradhan, Edward Loper, Dmitriy Dligach, and\nMartha Palmer. 2007. SemEval-2007 task-17: En-\nglish lexical sample, SRL and all words. InProceed-\nings of the Fourth International Workshop on Se-\nmantic Evaluations (SemEval-2007) , pages 87–92,\nPrague, Czech Republic. Association for Computa-\ntional Linguistics.\nYada Pruksachatkun, Phil Yeres, Haokun Liu, Jason\nPhang, Phu Mon Htut, Alex Wang, Ian Tenney, and\nSamuel R. Bowman. 2020. jiant: A software toolkit\nfor research on general-purpose text understanding\nmodels. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations, pages 109–117, Online.\nIda Raffaelli, Marko Tadi´c, Boˇzo Bekavac, and ˇZeljko\nAgi´c. 2008. Building Croatian WordNet. In Fourth\nglobal wordnet conference (gwc 2008).\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nAlessandro Raganato, Jose Camacho-Collados, and\nRoberto Navigli. 2017. Word sense disambiguation:\nA uniﬁed evaluation framework and empirical com-\nparison. In Proceedings of the 15th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Volume 1, Long Papers , pages\n99–110, Valencia, Spain. Association for Computa-\ntional Linguistics.\nAlessandro Raganato, Tommaso Pasini, Jose Camacho-\nCollados, and Mohammad Taher Pilehvar. 2020.\nXL-WiC: A multilingual benchmark for evaluating\nsemantic contextualization. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 7193–7206,\nOnline. Association for Computational Linguistics.\nAlessandro Raganato and J ¨org Tiedemann. 2018. An\nanalysis of encoder representations in transformer-\nbased machine translation. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n287–297, Brussels, Belgium.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2021. A primer in bertology: What we know about\nhow bert works. Transactions of the Association for\nComputational Linguistics, 8:842–866.\nKiril Simov and Petya Osenova. 2010. Constructing of\nan Ontology-based Lexicon for Bulgarian. In Proc.\nof LREC.\nBenjamin Snyder and Martha Palmer. 2004. The En-\nglish all-words task. In Proceedings of SENSEVAL-\n3, the Third International Workshop on the Evalu-\nation of Systems for the Semantic Analysis of Text ,\npages 41–43, Barcelona, Spain. Association for\nComputational Linguistics.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao\nTian, Hua Wu, and Haifeng Wang. 2020. Ernie 2.0:\nA continual pre-training framework for language un-\nderstanding. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, pages 8968–8975.\nKadri Vider and Heili Orav. 2002. Estonian WordNet\nand Lexicography. In Proc. of the Eleventh Interna-\ntional Symposium on Lexicography.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan\nLiu, Juanzi Li, and Jian Tang. 2021. Kepler: A\nuniﬁed model for knowledge embedding and pre-\ntrained language representation. TACL.\nWenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2020. Pretrained encyclopedia:\nWeakly supervised knowledge-pretrained language\nmodel. In International Conference on Learning\nRepresentations.\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki\nTakeda, and Yuji Matsumoto. 2020. LUKE: Deep\ncontextualized entity representations with entity-\naware self-attention. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 6442–6454, On-\nline. Association for Computational Linguistics.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason\nBaldridge. 2019. PAWS-X: A cross-lingual ad-\nversarial dataset for paraphrase identiﬁcation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3687–\n3692, Hong Kong, China. Association for Computa-\ntional Linguistics.\nAe-Sun Yoon, Soon-Hee Hwang, Eun-Ryoung Lee,\nand Hyuk-Chul Kwon. 2009. Construction of Ko-\nrean WordNet. Journal of KIISE: Software and Ap-\nplications, 36(1).\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 1441–1451, Florence, Italy. Association\nfor Computational Linguistics.\nPierre Zweigenbaum, Serge Sharoff, and Reinhard\nRapp. 2018. Overview of the third bucc shared task:\nSpotting parallel sentences in comparable corpora.\n3659\nIn Proceedings of 11th Workshop on Building and\nUsing Comparable Corpora, pages 39–42.\nA Wikipedia Data Details\nWe download the Wikipedia dump from January\n11, 2020, and preprocess it using the WikiExtractor\nscript (Attardi, 2015). We download Wikipedia arti-\ncles for the following 100 languages as in Conneau\nand Lample (2019):6 af, als, am, ang, an, ar, arz,\nast, az, bar, be, bg, bn, br, bs, ca, ceb, ckb, cs, cy,\nda, de, el, en, eo, es, et, eu, fa, ﬁ, fr, fy, ga, gan, gl,\ngu, he, hi, hr, hu, hy, ia, id, is, it, ja, jv, ka, kk, kn,\nko, ku, la, lb, lt, lv, mk, ml, mn, mr, ms, my, nds, ne,\nnl, nn, no, oc, pl, pt, ro, ru, scn, sco, sh, si, simple,\nsk, sl, sq, sr, sv, sw, ta, te, th, tl, tr, tt, uk, ur, uz, vi,\nwar, wuu, yi, zh classical, zh, zh min nan, zh yue.\nLanguage sets used for training We ﬁnetune\nMMLM models on the Wikipedia hyperlink pre-\ndiction task using articles in different sets of lan-\nguages to investigate the impact of multilingual-\nism. Wiki EN includes only articles in English (en);\nWiki 15 includes articles in bg, da, de, en, es, et, eu,\nfa, fr, hr, it, ja, ko, nl, zh; ﬁnally, Wiki 100 includes\narticles in all 100 languages listed above.\nRationale Wiki EN is a monolingual albeit\nresource-rich baseline. In Wiki 15, we explore\nthe impact of including languages with different\namounts of data and from a mixture of different\nlanguage families. In Wiki 100, we wish to see\nif going massively multilingual has a noticeable\nimpact on our models’ crosslingual transferability.\nHyperlink extraction We use BabelNet (Nav-\nigli and Ponzetto, 2010) — a large multilingual\nknowledge base comprising WordNet, Wikipedia,\nand many other resources — to map Wikipedia ar-\nticles in different languages about the same subject\nonto unique identiﬁers. For instance, all “computer\nscience” articles (e.g., Ciencias de la computaci´on\nin Spanish, Computer science in English, Infor-\nmatik in German, etc.) are mapped to the same\nidentiﬁer ht, in this case bn:00021494n.7 After\neach article is mapped to a single identiﬁer, we cre-\nate prediction targets for every hyperlink by using\nthe identiﬁer of its referenced article. For example,\nin Figure 3 the text “algorithmic processes” (xn:k)\n6https://github.com/facebookresearch/\nXLM\n7https://babelnet.org/synset?word=bn:\n00021494n&lang=EN\nEN:Computer science\nES:Ciencias de la\n      computación\nDE:Informatik\nComputer science\n...\nBabelNet IDs\nbn:00021494n\nbn:00053823n\nEN:Automata theory\nFR:Théorie des\n      automats\nHI:ऑटोमेटा सद्धांत\nAutomata theory\n...\nbn:00002705n\nEN:Algorithm\nFR:Algorithme\nZH:算法\nAlgorithm\n...\nWikipedia articles\nComputer science is the study of \nalgorithmic processes and \ncomputational machines. As a \ndiscipline, computer science spans a \nrange of topics from [...]\nEN:Computer science\nbn:00015267nbn:00041645n\nFigure 3: We show Wikipedia articles in different lan-\nguages, which topics include computer science, au-\ntomata theory, and algorithm, being mapped to Ba-\nbelNet IDs. Articles on the same topic, regardless\nof their language, are mapped to the same identiﬁer.\nBottom-right: we show a part of the English article on\nComputer scienceand show two example hyperlinks\nand their targets ht.\nrefers to the article “Algorithm”,8 which is mapped\nto the ID bn:00002705n9 (ht).\nIn Table 4 we show detailed per-language statis-\ntics for the Wikipedia data used in our experiments,\nincluding the size of the datasets and the number\nof hyperlinks appearing in the articles (this count\nalready includes only the hyperlinks in our hyper-\nlinks vocabulary of 250k types).\nB Hyperparameters, Training Procedure\nand Model Architectures\nWe use XLM-R-large (Conneau et al., 2020),\nwhich has an encoder with 24 layers and a hidden\nstate size 1024. We ﬁnetune XLM-R-large using\nAdamW (Kingma and Ba, 2015; Loshchilov and\nHutter, 2018) with learning rate0.00005, no weight\ndecay, and batch size 16. We train on minibatches\nwith maximum sequence length256, gradient norm\nset to 1.0, and for 300k model updates. When ﬁne-\ntuning XLM-R on Wikipedia hyperlink prediction,\nwe only update the last two layers of the model.\nTraining data sampling We sample batches of\ntraining data from each of the languages available,\ni.e., depending on the experiment these can be En-\nglish only, 15 languages, or 100 languages. We\nsample with probability rl = min(el,K)∑(min(el,K) , where\nel is the number of examples per language l and\nthe constant K = 217 leads to sampling more often\n8https://en.wikipedia.org/wiki/\nAlgorithm\n9https://babelnet.org/synset?word=bn:\n00002705n&lang=EN\n3660\nfrom resource-poor languages (Raffel et al., 2020).\nAdaptive softmax We collect hyperlink targets\nht from across Wikipedia articles in all the 100 lan-\nguages available, sort these hyperlinks from most\nto least frequent, and keep only the top 250k hy-\nperlink targets ht. Since hyperlink frequencies\nfollow a natural Zipﬁan distribution, we use the\nadaptive softmax activation (Grave et al., 2017)\nto predict hyperlinks. We bin hyperlink mentions\nfrom most to least frequent, i.e. the most frequent\nht is ranked 1st and the least frequent ht is ranked\n250k-th. We use ﬁve bins, which include hyper-\nlinks with ranks in the following intervals: [1, 10k],\n(10k, 40k], (40k, 50k], (50k, 70k], (70k, 250k].\nThe adaptive softmax activation is efﬁcient to\ncompute because: (1) we use one matrix multipli-\ncation for each bin, drastically reducing the num-\nber of parameters; and (2) the latter bins are only\ncomputed in case there is at least one entry in the\nminibatch with a target in that bin. The ﬁve-weight\nmatrices that parameterise each bin in our adap-\ntive softmax layer have sizes: hdim ×10, 000,\nhdim×30, 000, hdim×10, 000, hdim×20, 000,\nhdim ×180, 000, respectively. Since bins are con-\nstructed so that the least frequent hyperlinks are\nadded to the latter bins, we rarely need to compute\nthem. This is especially important in case of the\nlast bin, which is the most costly to compute (and\nis rarely used).\nB.1 Model Architectures\nWe refer the reader for the mathematical notation\nin Section 2 Approach. The Wikipedia hyperlink\nprediction head for a single hyperlink using each\nof our models is shown below. Token is computed\nin Equation 1.\np(xi = k) ∝AdaptiveSoftmaxk(Wt ·xi + bt),\n(1)\nwhere AdaptiveSoftmaxk computes the probability\nof the hyperlink target ht = k, xn:k is a hyperlink\nconsisting of words {xn, ··· , xk}, and Wt and bt\nare trained parameters.\nConcat CLSis computed in Equation 2.\np(xi = k) ∝\nAdaptiveSoftmaxk(Wc ·[xi; xCLS] +bc), (2)\nwhere AdaptiveSoftmaxk computes the probability\nof the hyperlink target ht = k, xn:k is a hyperlink\nconsisting of words {xn, ··· , xk}, and Wc and bc\nare trained parameters.\nReplace CLSis computed in Equation 3.\nx = sample(xi, xCLS), (3)\np(xi = k) ∝AdaptiveSoftmaxk(Wr ·x + br),\nwhere sample(a, b) samples a or b with probabil-\nity 0.9 and 0.1, respectively; AdaptiveSoftmaxk\ncomputes the probability of the hyperlink target\nht = k, xn:k is a hyperlink consisting of words\n{xn, ··· , xk}, and Wr and br are trained parame-\nters.\nB.1.1 XL-WSD\nWe freeze the pretrained MMLM model weights\nand simply add a trained classiﬁcation head on top\nof the pretrained MMLM. We compute represen-\ntations for each subword as the sum of the last 4\nlayers of the model, and for each word as the aver-\nage of its subword representations (Bevilacqua and\nNavigli, 2020).\nB.1.2 XL-WiC\nWe follow Raganato et al. (2020) and add a binary\nclassiﬁcation head on top of the pretrained MMLM\nmodel, which takes as input the concatenation of\nthe target words’ embedding in the two contexts.\nWe use the output of the 24-th layer as the target\nwords’ representation.\nB.1.3 XTREME\nWe use the Jiant library (Pruksachatkun et al., 2020)\nto carry out the evaluation on XTREME. We use\nthe output of the 24-th layer as the input token\nrepresentations so as to better measure the impact\nof our intermediate training on the XTREME tasks.\nB.2 XTREME Sentence Retrieval Tasks\nBUCC (Zweigenbaum et al., 2018), and Tatoeba\n(Artetxe and Schwenk, 2019) are two unsupervised\ntasks requiring, given a sentence in a languageL to\nretrieve its closest sentence in another language L′.\nXTREME baselines use the average of the 14-th\nlayer outputs to represent the sentence.10 Since our\nintermediate training procedure only tunes the last\ntwo layers, the output of the 14-th layer would be\nthe exact same of the plain XLM-R baseline. For\nthis reason, we did not report the results in both\ntasks.\n10https://github.com/nyu-mll/jiant/\nblob/master/guides/tasks/task_specific.\nmd\n3661\nLanguage Language # training size\nCode links\nAF Afrikaans 524,682 37M\nALS Tosk Albanian 203,333 13M\nAM Amharic 35,586 2,1M\nANG Anglo-Saxon 315,250 530K\nAN Aragonese 8,376 12M\nAR Arabic 6,342,628 343M\nARZ Egyptian Arabic 1,738,581 35M\nAST Asturian 971,410 77M\nAZ Azerbaijani 786,016 53M\nBAR Bavarian 78,614 4,4M\nBE Belarusian 1,138,871 82M\nBG Bulgarian 2,340,267 158M\nBN Bengali 549,982 56M\nBR Breton 318,303 15M\nBS Bosnian 596,758 34M\nCA Catalan 6,180,563 395M\nCEB Cebuano 15,029,079 178M\nCKB Central Kurdish 88,593 6,1M\nCS Czech 4,697,945 341M\nCY Welsh 561,936 23M\nDA Danish 2,273,079 135M\nDE German 28,064,840 2,1G\nEL Greek 1,611,904 166M\nEN English 73,084,305 4,9G\nEO Esperanto 2,330,837 110M\nES Spanish 19,125,611 1,2G\nET Estonian 1,417,295 82M\nEU Basque 1,743,033 71M\nFA Persian 3,429,725 185M\nFI Finnish 3,748,928 252M\nFR French 23,415,178 1,5G\nFY Western Frisian 423,234 26M\nGA Ga 183,946 11M\nGAN Gan Chinese 8,742 425K\nGL Galician 1,632,786 108M\nGU Gujarati 256,284 6,6M\nHE Hebrew 6,256,536 396M\nHI Hindi 546,648 45M\nHR Croatian 1,825,455 112M\nHU Hungarian 3,785,965 275M\nHY Armenian 1,639,954 124M\nIA Interlingua 51,882 2,4M\nID Indonesian 3,504,017 159M\nIS Icelandic 252,888 17M\nIT Italian 15,407,079 1011M\nJA Japanese 13,318,170 949M\nJV Javanese 213,822 11M\nKA Georgian 894,180 70M\nKK Kazakh 660,065 41M\nKN Kannada 114,724 16M\nLanguage Language # training size\nCode links\nKO Korean 2,955,253 191M\nKU Kurdish 91,047 4,6M\nLA Latin 791,760 29M\nLB Luxembourgish 224,604 13M\nLT Lithuanian 1,276,418 76M\nLV Latvian 748,963 46M\nMK Macedonian 983,105 63M\nML Malayalam 332,530 37M\nMN Mongolian 90,807 6,3M\nMR Marathi 169,347 13M\nMS Malay 1,242,850 54M\nMY Burmese 48,285 5,9M\nNDS Low Saxon 168,053 12M\nNE Nepali 66,667 5,8M\nNL Dutch 10,647,696 551M\nNN Norwegian Nynorsk 983,245 54M\nNO Norwegian 4,095,644 227M\nOC Occitan 562,718 22M\nPL Polish 10,753,690 685M\nPT Portuguese 10,065,298 581M\nRO Romanian 2,376,428 129M\nRU Russian 15,691,268 1,4G\nSCN Sicilian 64,902 3,4M\nSCO Scots 174,304 9,7M\nSH Serbo-Croatian 3,076,574 113M\nSI Sinhala 26,321 3,5M\nSIMPLE Simple English 1,260,400 57M\nSK Slovak 1,455,414 84M\nSL Slovenian 1,343,091 78M\nSQ Albanian 280,756 18M\nSR Serbian 3,218,656 194M\nSV Swedish 21,025,833 475M\nSW Swahili 317,669 11M\nTA Tamil 691,010 54M\nTE Telugu 307,488 26M\nTH Thai 862,265 82M\nTL Tagalog 218,323 13M\nTR Turkish 2,336,668 150M\nTT Tatar 499,022 15M\nUK Ukrainian 7,949,672 562M\nUR Urdu 526,498 29M\nUZ Uzbek 308,536 9,3M\nVI Vietnamese 4,877,318 221M\nW AR Waray 4,738,778 46M\nWUU Wu Chinese 47,388 4,4M\nYI Yiddish 85,374 5,1M\nZH CLASSICAL Classical Chinese 7,654,040 2,9M\nZH Chinese 38,104 499M\nZH MIN NAN Min Nan 1,101,622 12M\nZH YUE Cantonese 344,432 17M\nTable 4: Data statistics: total number of hyperlinks appearing in articles in Wikipedia in a given language, and size\nof the dataset for each language. K stands for kilobyte, M for megabyte, and G for gigabyte.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7739073038101196
    },
    {
      "name": "Hyperlink",
      "score": 0.6780155897140503
    },
    {
      "name": "Rendezvous",
      "score": 0.5494080781936646
    },
    {
      "name": "Natural language processing",
      "score": 0.5122031569480896
    },
    {
      "name": "Computational linguistics",
      "score": 0.45023971796035767
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4451698064804077
    },
    {
      "name": "Linguistics",
      "score": 0.4190576672554016
    },
    {
      "name": "World Wide Web",
      "score": 0.32887160778045654
    },
    {
      "name": "Engineering",
      "score": 0.09680688381195068
    },
    {
      "name": "Web page",
      "score": 0.07180646061897278
    },
    {
      "name": "Philosophy",
      "score": 0.057800114154815674
    },
    {
      "name": "Aerospace engineering",
      "score": 0.0
    },
    {
      "name": "Spacecraft",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I887064364",
      "name": "University of Amsterdam",
      "country": "NL"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I133731052",
      "name": "University of Helsinki",
      "country": "FI"
    },
    {
      "id": "https://openalex.org/I124055696",
      "name": "University of Copenhagen",
      "country": "DK"
    }
  ]
}