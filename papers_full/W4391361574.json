{
    "title": "Efficient pneumonia detection using Vision Transformers on chest X-rays",
    "url": "https://openalex.org/W4391361574",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2106114530",
            "name": "Sukhendra Singh",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1972100179",
            "name": "Manoj Kumar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2098170595",
            "name": "Abhay Kumar",
            "affiliations": [
                "National Institute of Technology Patna"
            ]
        },
        {
            "id": "https://openalex.org/A4379214616",
            "name": "Birendra Kumar Verma",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1875902001",
            "name": "Kumar Abhishek",
            "affiliations": [
                "National Institute of Technology Patna"
            ]
        },
        {
            "id": "https://openalex.org/A2947054799",
            "name": "Shitharth Selvarajan",
            "affiliations": [
                "Leeds Beckett University"
            ]
        },
        {
            "id": "https://openalex.org/A2106114530",
            "name": "Sukhendra Singh",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1972100179",
            "name": "Manoj Kumar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2098170595",
            "name": "Abhay Kumar",
            "affiliations": [
                "National Institute of Technology Patna"
            ]
        },
        {
            "id": "https://openalex.org/A4379214616",
            "name": "Birendra Kumar Verma",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1875902001",
            "name": "Kumar Abhishek",
            "affiliations": [
                "National Institute of Technology Patna"
            ]
        },
        {
            "id": "https://openalex.org/A2947054799",
            "name": "Shitharth Selvarajan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4376865522",
        "https://openalex.org/W3198507683",
        "https://openalex.org/W3187183441",
        "https://openalex.org/W4322741420",
        "https://openalex.org/W2956123709",
        "https://openalex.org/W3031327998",
        "https://openalex.org/W4220965082",
        "https://openalex.org/W4379207370",
        "https://openalex.org/W4310076807",
        "https://openalex.org/W3181327235",
        "https://openalex.org/W3049624036",
        "https://openalex.org/W4294754922",
        "https://openalex.org/W6763367864",
        "https://openalex.org/W6863569391",
        "https://openalex.org/W2963495494",
        "https://openalex.org/W4296990651",
        "https://openalex.org/W4386387351",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2991568321",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3163832451",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3210595091",
        "https://openalex.org/W3125294517",
        "https://openalex.org/W3211983116",
        "https://openalex.org/W4283029781",
        "https://openalex.org/W3109016549",
        "https://openalex.org/W4226275002",
        "https://openalex.org/W4214634256",
        "https://openalex.org/W4284890590",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W4213019189",
        "https://openalex.org/W3216035653",
        "https://openalex.org/W3093534379",
        "https://openalex.org/W3013298624",
        "https://openalex.org/W4307873940",
        "https://openalex.org/W3107409668",
        "https://openalex.org/W4312333591",
        "https://openalex.org/W4210997624",
        "https://openalex.org/W3035725276",
        "https://openalex.org/W3159481202",
        "https://openalex.org/W3154596443",
        "https://openalex.org/W6810308439",
        "https://openalex.org/W6847616886",
        "https://openalex.org/W3203262038",
        "https://openalex.org/W4285018316",
        "https://openalex.org/W4214493665",
        "https://openalex.org/W3120013103",
        "https://openalex.org/W3157302753",
        "https://openalex.org/W4200551431",
        "https://openalex.org/W3168530519",
        "https://openalex.org/W3017141460",
        "https://openalex.org/W4205544530",
        "https://openalex.org/W3156331990",
        "https://openalex.org/W3128758033",
        "https://openalex.org/W3207993568",
        "https://openalex.org/W3015538848",
        "https://openalex.org/W4206429322",
        "https://openalex.org/W3165444657",
        "https://openalex.org/W3118209175",
        "https://openalex.org/W3205545491",
        "https://openalex.org/W3159001838",
        "https://openalex.org/W4213099919",
        "https://openalex.org/W3103593657",
        "https://openalex.org/W3106539405",
        "https://openalex.org/W3111002277",
        "https://openalex.org/W4226325015",
        "https://openalex.org/W4226085666"
    ],
    "abstract": "Abstract Pneumonia is a widespread and acute respiratory infection that impacts people of all ages. Early detection and treatment of pneumonia are essential for avoiding complications and enhancing clinical results. We can reduce mortality, improve healthcare efficiency, and contribute to the global battle against a disease that has plagued humanity for centuries by devising and deploying effective detection methods. Detecting pneumonia is not only a medical necessity but also a humanitarian imperative and a technological frontier. Chest X-rays are a frequently used imaging modality for diagnosing pneumonia. This paper examines in detail a cutting-edge method for detecting pneumonia implemented on the Vision Transformer (ViT) architecture on a public dataset of chest X-rays available on Kaggle. To acquire global context and spatial relationships from chest X-ray images, the proposed framework deploys the ViT model, which integrates self-attention mechanisms and transformer architecture. According to our experimentation with the proposed Vision Transformer-based framework, it achieves a higher accuracy of 97.61%, sensitivity of 95%, and specificity of 98% in detecting pneumonia from chest X-rays. The ViT model is preferable for capturing global context, comprehending spatial relationships, and processing images that have different resolutions. The framework establishes its efficacy as a robust pneumonia detection solution by surpassing convolutional neural network (CNN) based architectures.",
    "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2024) 14:2487  | https://doi.org/10.1038/s41598-024-52703-2\nwww.nature.com/scientificreports\nEfficient pneumonia detection \nusing Vision Transformers on chest \nX‑rays\nSukhendra Singh 1, Manoj Kumar 1, Abhay Kumar 2, Birendra Kumar Verma 1, \nKumar Abhishek 2 & Shitharth Selvarajan 3*\nPneumonia is a widespread and acute respiratory infection that impacts people of all ages. Early \ndetection and treatment of pneumonia are essential for avoiding complications and enhancing \nclinical results. We can reduce mortality, improve healthcare efficiency, and contribute to the global \nbattle against a disease that has plagued humanity for centuries by devising and deploying effective \ndetection methods. Detecting pneumonia is not only a medical necessity but also a humanitarian \nimperative and a technological frontier. Chest X‑rays are a frequently used imaging modality for \ndiagnosing pneumonia. This paper examines in detail a cutting‑edge method for detecting pneumonia \nimplemented on the Vision Transformer (ViT) architecture on a public dataset of chest X‑rays \navailable on Kaggle. To acquire global context and spatial relationships from chest X‑ray images, \nthe proposed framework deploys the ViT model, which integrates self‑attention mechanisms and \ntransformer architecture. According to our experimentation with the proposed Vision Transformer‑\nbased framework, it achieves a higher accuracy of 97.61%, sensitivity of 95%, and specificity of \n98% in detecting pneumonia from chest X‑rays. The ViT model is preferable for capturing global \ncontext, comprehending spatial relationships, and processing images that have different resolutions. \nThe framework establishes its efficacy as a robust pneumonia detection solution by surpassing \nconvolutional neural network (CNN) based architectures.\nPneumonia is a common respiratory infection caused by multiple types of bacteria, viruses, and fungi. It is the \nleading cause of morbidity and mortality worldwide, particularly among infants under the age of five and the \nelderly. According to  WHO1, 1.4 million pneumonia-related fatalities among children under five in 2018. Chest \nX-ray imaging is commonly used to diagnose pneumonia, as it can reveal important symptoms, such as increased \nlung opacity and consolidation. However, it can be difficult to interpret a chest X-ray (CXR) because pneumonia \nsymptoms can be subtle and overlap with other lung diseases. Rapid and accurate diagnosis of pneumonia is \nessential for expediting treatment and improving patient outcomes. Radiological images, such as chest X-rays \nor CT scans, require specialized training and can be time-consuming to diagnose pneumonia.In recent years, \nthere has been significant interest to develop model using machine learning techniques that assist physicians \nin diagnosing pneumonia using chest X-ray images. These techniques have shown promising results and may \nimprove the efficacy and accuracy of pneumonia diagnosis.\nBy training a CNN on a dataset of chest X-ray images, Deep Learning (DL) 2–5 has been utilized to detect \n pneumonia6–10. As shown in Fig. 1, the CNN can learn to recognize patterns and associated features with pneu-\nmonia, such as clouded lung areas to detect pneumonia.The model can then be used to classify new X-ray \nimages as normal or pneumonia. Multiple  studies11–14 have demonstrated the efficacy of this method in detecting \npneumonia with a high degree of accuracy. Attention mechanism isn DL  refers15–21 to a technique used in neural \nnetworks to selectively focus on certain portions of an input as opposed to processing the entire input equally. In \nimage detection and classification, attention mechanisms can be utilized to concentrate the network’s attention \non specific regions of an image that are most important for making a classification decision. This can help the \nnetwork to improve its accuracy and decrease its computation needs. ViT models are a variant of the Transformer \n architecture22–26, which was originally designed for NLP applications. These models have been adapted for \nimage classification tasks by handling an image as a sequence of image segments that are then processed by the \nOPEN\n1JSS Academy of Technical Education, Noida, India. 2National Institute of Technology Patna, Patna, India. 3School \nof Built Environment, Engineering and Computing, Leeds Beckett University, LS1 3HE, Leeds, UK.  *email: \nShitharthS@kdu.edu.et\n2\nVol:.(1234567890)Scientific Reports |         (2024) 14:2487  | https://doi.org/10.1038/s41598-024-52703-2\nwww.nature.com/scientificreports/\ntransformer’s attention mechanism. In addition, the ViT model outperformed state-of-the-art (SOTA) techniques \non a broad variety of image classification tasks, making it an excellent candidate for the pneumonia diagnosis task.\nMotivation\nVision Transformer architecture for pneumonia detection from CXR is motivated by the need for time to detect \nthis severe respiratory disease. Globally, pneumonia is one of the big causes of mortality. Early diagnosis and \ntreatment are crucial for improved patient outcomes. Traditional methods of evaluating CXR to diagnose pneu-\nmonia are time-consuming and require specialized medical knowledge, which can lead to diagnostic errors and \ntreatment delays. In response to these challenges, DL techniques such as CNNs and RNNs have been developed \nto automate the detection of pneumonia from CXR. However, these methods are inadequate to analyze complex \nmedical images. ViT architecture has demonstrated exceptional efficacy in a variety of vision tasks, including \nimage classification and object detection. It is a viable candidate for pneumonia detection from CXR because \nit can extract global and local image features. Utilizing the power of self-attention mechanisms, ViT is able to \neffectively capture complex patterns and relationships in X-ray images, resulting in improved pneumonia detec-\ntion accuracy and reliability. Therefore, the goal of utilizing ViT architecture for pneumonia detection from CXR \nis to surmount the limitations of conventional methods and improve the precision and efficacy of DL models \nfor medical imaging analysis. Vision Transformer architectures are totally different from CNN architectures. \nTransformer-based architectures were initially designed for sequence-to-sequence tasks in natural language \nprocessing. CNN is primarily used for tasks like machine translation, text summarization, language modeling, \nand sentiment analysis. These architectures have been customized into Vision Transformer architecture so that \nthey can be suitable for Image classification and analysis.\nThe contribution of work is summarized as follows.\n• In this investigation, we propose a ViT-based architecture for pneumonia detection in CXR. This architecture \nwill be designed to effectively manage the large and complex medical images that are typical in CXR and will \nbe capable of detecting pneumonia with precision.\n• We will evaluate the accuracy of the proposed ViT architecture to that of existing DL techniques. This will \nprovide a thorough analysis of the benefits and drawbacks of our proposed approach compared to existing \nmethodologies.\n• We will evaluate the efficacy of the proposed ViT architecture using a CXR dataset that is publicly available. \nThis will entail training and testing the model using a set of performance metrics, including accuracy, recall, \nprecision, and F1 score, to measure its performance.\nWe will present the proposed ViT architecture’s performance evaluation findings and analysis. This will \ninclude a discussion of any limitations of the proposed model and recommendations for improving its efficacy \nthrough future work.\nOrganization of the paper\nThe rest paper is structured as Sect. 2 discusses the background and working principle of the proposed architec-\nture and other variants of Vision Transformer architecture. Section 3 presents recent applications and a review \nof related studies. Section 4 describes the dataset characteristics and proposed architecture. Section 5 discusses \nexperiment specifications, results, and prospects of Vision Transformer architecture, followed by Sect. 6, which \nrepresents the conclusion.\nBackground and methodology\nIn this section, the paper builds the foundation for the proposed architecture.\nTransformer architecture\nThe transformer architecture is a neural  network27 designed for natural languages, such as language transla-\ntion, language modeling, and text summarization. The main concept of the transformer architecture is the self-\nattention mechanism, which assess the relative relevance of various words or sub-phrases in a given input. This is \nFigure 1.  A sample CXR (normal and pneumonia) image.\n3\nVol.:(0123456789)Scientific Reports |         (2024) 14:2487  | https://doi.org/10.1038/s41598-024-52703-2\nwww.nature.com/scientificreports/\nachieved by computing a \"query,\" \"key,\" and \"value\" for each word or sub-phrase, followed by adding a weighted \nsum on the similarity between the query and the keys. Additionally, the transformer architecture utilizes a multi-\nhead attention  mechanism28–30 to attend to various input positions. In addition to the self-attention  mechanism31, \nthe feed-forward neural network process the output of the self-attention layer to produce the better result in \nTransformer model. The architecture also uses positional encoding to convey the position of the input image.\nVision transformer derived from generic transformer architecture\nThe Vision Transformer replaces the original transformer’s self-attention mechanism with a spatial attention \n mechanism32 which is designed to govern images’ two-dimensional grid structure. This enables the model to \nanalyze and comprehend the spatial relationships between different image regions. Itis an effective architecture for \nimage classification and computer vision tasks. Images are processed through the Transformer model, consisting \nof spatial attention and a feed-forward neural network. The spatial attention mechanism applies the attention \nto the image pixels, followed by the feed-forward neural network to the output of the attention mechanism. In \naddition, this modeluses a patch-based strategy where an image is divided into smaller segments and learns \nto focus separately on each patch. This allows the model to extract granular features and improve its accuracy.\nWorking principle of Vision Transformer\nThe fundamental concept of a ViT is the self-attention mechanism, which exploits both global and local fea-\ntures by focusing on distinct portions of the image. The self-attention mechanism is implemented by adding \nself-attention layers with multiple heads that are known as transformer blocks. Each patch is converted into \ncorresponding 1-D vector and transmitted to the transformer. The transformer then uses self-attention to learn \nthe relationships between the various regions, and the resulting representation is input into a feed-forward \nneural network to make a prediction.As the spatial resolution of the input does not constrain the self-attention \nmechanism, one of the main advantages of ViT is their ability to handle images of arbitrary sizes. This model \ncan be trained on large images, such as high-resolution medical images, without downsampling or cropping. \nAdditionally, this model has been improved in recent variants such as  DeiT33,34, Swin-T 35,36, and  ReViT37 to \nenhance their performance, reduce the number of parameters and computational costs, and make them more \nefficient and scalable for practical applications.\nSelf‑attention mechanism in Vision Transformer for image detection and classification\nA Vision  Transformer38,39 is a neural network that processes visual information using self-attention mechanisms. \nSimilar to how the Transformer architecture is used in natural language processing (NLP), ViT employs attention \nmechanisms to evaluate the specific parts of an image in order to make accurate predictions. These networks \nexcel at image classification and object detection.\nSelf attention techniques\nSelf-attention15 is a technique that enables a model to selectively concentrate its processing on particular regions \nof an image. Self-attention is typically applied to extracted feature maps generated by a CNN in the context of \nimages. Self-attention allows the model to determine the relative importance of various image regions by com-\nputing a set of attention weights for each region. These attention weights can then be applied to the feature maps \nbefore their transmission to the remainder of the network.There are numerous methods to incorporate self-\nattention into images. A common technique is using a multi-head self-attention mechanism, in which the model \ncomputes multiple sets of attention weights for various regions of the image and then combines them. This allows \nthe model to consider the entire image when making a prediction rather than just a specific region’s features. A \nfurther method for image processing is to use a transformer-based model in which the self-attention mecha-\nnism focuses on various image regions when selecting a prediction. The transformer-based model is trained to \nunderstand the relationships between multiple image regions and makes predictions based on this information.\nSelf-attention in DL for image processing can be categorized into two main modules: channel attention and \nspatial attention.\nSpatial attention networks. In contrast to conventional CNNs, which process entire images and extract features \nfrom them, spatial attention  networks32,40 process only particular regions of an image. This is accomplished by \nincorporating an attention mechanism that learns to weigh various image regions based on their significance to \nthe current task. By selectively attending to the relevant areas of an image, spatial attention networks can achieve \ngreater accuracy and efficiency when performing tasks such as image captioning, object detection, and visual \nquestion answering. In addition, the attention mechanism improves the interpretability of these networks by \nhighlighting the regions of the image that the network is concentrating on for a given task.\nChannel attention. Channel  attention41,42 pertains to a mechanism’s ability to focus on particular channels of \nthe feature maps selectively. Typically, this is carried out by computing a set of attention weights for each channel \nof the feature maps. These attention weights can then be applied to channels before their transmission through \nthe remainder of the network. This allows the model to concentrate its prediction on the channels that are most \ninformative.The combination of channel and spatial attention empowers the model to predict using both spa-\ntial information (the location of the specified portion within the image) and channel information (the features \nextracted by the CNN). This results in more robust and generalizable modelsfor images that have not been seen.\n4\nVol:.(1234567890)Scientific Reports |         (2024) 14:2487  | https://doi.org/10.1038/s41598-024-52703-2\nwww.nature.com/scientificreports/\nVariants of Vision Transformer\nSeveral customizations in ViThave been experimented with to improve its performance or fit certain applications. \nThe main customization methods include.\nPatch size. The ViT architecture linearly embeds fixed-size input image patches. Patch size affects model per-\nformance. Larger patches capture global context but lose fine-grained details, while smaller patches may fail to \ncapture global context. To find better performance, optimal patch size has been used.\nPositional encoding. ViT incorporates spatial information into the model via learnable positional encodings. \nThese encodings assist the model understand image patch placements. ViT performance can be improved with \nsine/cosine, spatial, or learned positional encodings.\nArchitectural variations. To improve ViT, researchers have tried several architectural variations. A Pyramid \nVision Transformer (PVT) is a hierarchical modification that captures multi-scale information. The Convo-\nluted Vision Transformer (ConvViT) combines self-attention and convolutional layers to use local and global \ninformation.\nTraining methods. ViT performance and convergence have been improved using various training methods. \nData augmentation, regularization (dropout, weight decay), and advanced optimization algorithms (Adam, \nRMSprop) are examples. Pretraining on ImageNet and transfer  learning43,44 have also been used to initialize ViT \nmodels.\nHybrid models. Hybrid designs integrate Convolutional Neural Networks (CNNs) and Vision Transformers \n(ViTs) for tasks such as pneumonia detection in chest X-ray images, we first use a CNN as the feature extractor, \nremoving its fully connected layers while retaining its convolutional and pooling layers. The CNN-generated fea-\nture maps are then separated into non-overlapping patches, and each patch is converted into a high-dimensional \nembedding vector. These embeddings, which depict local characteristics, are then fed into the ViT model in \norder to capture global dependencies and contextual information across the entire image. For final predictions, \na classification head is appended to the ViT output. The entire hybrid model, comprised of the CNN feature \nextractor and the ViT model, is trained from beginning to end using labeled data, with fine-tuning strategies \ntailored to the specific dataset and computational resources available. This approach maximizes the extraction \nof both local and global information, optimizing performance for complex image analysis tasks.Transformers \nprocess CNN-extracted features. This hybrid strategy uses CNNs (local feature extraction) and transformers \n(global context modeling) to improve performance. Pyramid Vision Transformer (PVT captures multi-scale \ninformation hierarchically. Multiple steps process features at varying resolutions. The model effectively captures \nlocal and global information. A convoluted Vision Transformer (ConvViT) is a Self-attention mechanism with \nconvolutional layers. Self-attention models global context, while convolutional layers catch local patterns. This \ncombination improves the model’s local and global information handling.\nAttention mechanism. ViT’s architecture relies on attention techniques. Attention mechanism customization \nmay include Long-Range Arena (LRA) attention, Axial attention, and Shifted attention. LRA attention efficiently \nhandles input image long-range dependencies. It helps the model capture global context even when patches are \nfar apart.\nAxial attention captures dependencies along image axes (rows and columns). Self-attention is modified to \ncatch shifted or offset patch dependencies. This helps the model manage data spatial transformations.\nTo have state-of-the-art performance and improved convergence,researchers have experimented with the \nfollowing pre-trained Vision Transformer architectures.\nDeiT (data-efficient image transformers). DeiT 34 uses self-attention mechanisms and patch-based processing \nto outperform CNNs in image tasks with less labeled training data. Self-attention computes attention weights on \nsmaller image patches to efficiently capture long-range relationships and grasp the global context. The models \nare pre-trained on large, unlabeled datasets to learn general visual representations, then fine-tuned on smaller, \ntask-specific datasets. Visual characteristics and hierarchical representations help the model transfer pre-trained \nknowledge to the target task. Dropout and data augmentation increase generalization. Data-efficient image \ntransformers use self-attention, patch-based processing, pre-training, fine-tuning, transfer learning, and regu-\nlarization to perform well in picture tasks without labeled data.\nSwin-T. Swin  transformer36,45, a new image understanding architecture, blends Transformers with CNNs. It \nconverts the input image into non-overlapping patches using transformer layers. Swin Transformer’s hierarchi-\ncal architecture organizes transformer layers into stages, making it unique. Lower stages process patch-level \ninformation, whereas later stages capture broader contextual information. The hierarchical model efficiently \ncaptures image local and global dependencies. Shift procedures help Swin Transformer model repair spatial \nlinks. Swin Transformer uses Transformers’ self-attention mechanism and CNNs’ efficient processing to achieve \nstate-of-the-art results on image classification, object detection, and semantic segmentation with fewer compu-\ntational resources than other transformer-based models.\n5\nVol.:(0123456789)Scientific Reports |         (2024) 14:2487  | https://doi.org/10.1038/s41598-024-52703-2\nwww.nature.com/scientificreports/\nReViT. The Vision Transformer (ViT) architecture can accommodate inputs of different resolutions with \nResizable-ViT37. Traditional ViT models require fixed-size inputs, which can limit their adaptability in real-\nworld applications with varied image sizes. Resizable-ViT solves this problem with \"token shifting\" and \"layer \ndropping.\" Token shifting requires scaling the input image and adapting position and token embeddings to the \nnew resolution. For lower inputs, layer-dropping skips model architectural layers based on input resolution, \nreducing computing complexity. Resizable-ViT efficiently processes images of varied resolutions while doing \nwell on image recognition tasks by dynamically adapting to input sizes.\nAll of these variants have been shown to enhance the performance and efficiency of Vision Transformers \nand have been applied to a variety of tasks, including image recognition, object detection, and medical imaging, \nwith SOTA results.\nRecent applications of Vision Transformer architecture\nVision Transformer (ViT) has attracted great interest in computer vision duties due to its capacity to process \nimages with high precision and efficiency. Recent developments and applications have been made to the ViT \narchitecture. The DeiT model, which enhances the training of ViT models using data augmentation and dis-\ntillation techniques, is one of the most significant innovations. The Swin Transformer model, which employs \nhierarchical representations to enhance the performance of ViT models on large-scale image datasets, is another \ninnovation.Recent Vision Transformer architectures research has centered on a variety of applications, including.\nObject detection and instance segmentation\nViT architecture is promising for object detection and instance segmentation because it possesses several essen-\ntial characteristics that make it suitable for these tasks. First, the self-attention mechanisms in ViT enable the \nmodel to learn global relationships between various image components, which can be used to identify and \nlocalize objects. ViT can be trained on large datasets with many labeled examples, which is essential for these \ntasks because they require a large amount of data to learn the involved complex patterns. Finally, ViT can be \nfine-tuned for specific object detection or instance segmentation  tasks46, allowing it to achieve high accuracy by \nadapting to the requirements of these tasks.\nDense predictions\nDense prediction is the task of predicting a pixel-wise output for an input image, such as semantic segmenta-\ntion, where each pixel is designated as a specific object or background. The input image is divided into a series \nof non-overlapping segments for dense prediction, which is then flattened and fed into the ViT architecture. \nSelf-attention allows ViT to record spatial information across these regions, and the output is shaped into a grid \ncorresponding to the original image. One of the benefits of employing ViT for dense prediction is that it can \nlearn to distinguish between objects of varying sizes and shapes without explicit object proposals or region-\nbased attributes. ViT attends to all regions in the input image and learns to weigh their contributions based on \nthe significance of their contributions to the output. In addition, ViT can be trained end-to-end with large-scale \ndatasets like ImageNet to acquire general features that can be applied to subsequent tasks like a dense prediction. \nIn situations with limited labeled data, this makes ViT an attractive design for dense prediction.\nSelf‑supervised learning\nEven without human annotations, ViT can be used for self-supervised  learning47,48. Self-supervised learning \nteaches input data meaningful representations for classification, detection, and segmentation. Training the model \non a pretext task is one method to use ViT for self-supervised  learning49. Pretext tasks allow the model to learn \nkey characteristics from input data. Data augmentation to generate multiple perspectives of the same image \nand training the ViT model to predict which views match is a common pretext challenge. Contrastive learning \nteaches the ViT model to distinguish between similar and distinct images. Two arbitrary images are supplied to \nthe ViT model. The model is then trained to predict whether or not two images are identical.In both cases, the \nViT model discovers features that are independent of viewpoint, illumination, and other factors that affect the \nappearance of input data. These learned characteristics can be used to establish supervised model weights or to \nrefine subsequent tasks.\nMulti‑modal learning\nRecent  research50 has examined the use of transformer-based architectures for multimodal unsupervised learn-\ning from raw video, audio, and text. Using self-supervised learning techniques, the plan is to implement a \ntransformer-based architecture capable of handling multiple modalities and capable of predicting the next frame, \naudio, or text given the current one.\nEfficient ViT architectures\nRecent efforts have been made to make Vision Transformer models more effective in terms of computation time \nand memory consumption. Multiple architectures, such as Separable Vision Transformer (SepViT)51 and Revers-\nible Vision Transformer (RViT)37,52, have been proposed by researchers that are capable of achieving comparable \nor superior performance than conventional ViT models while being more energy-efficient. SepViT blocks employ \nseparable convolutions rather than conventional convolutions. This update minimizes the self-attention mecha-\nnism, the most computationally expensive component of ViT. Separable convolutions separate conventional \nconvolutions into depthwise and pointwise convolutions, requiring fewer parameters and computations. RViT \naugments ViT design with reversible residual blocks. These blocks recreate input features from output features, \n6\nVol:.(1234567890)Scientific Reports |         (2024) 14:2487  | https://doi.org/10.1038/s41598-024-52703-2\nwww.nature.com/scientificreports/\nwhich increases the efficiency of gradient calculation during backpropagation. Reversible blocks enable models \nwith limited memory to be larger.\nExplainable AI\nViT can be utilized in Explainable  AI33 to provide insight into how an image classification decision is made. By \nusing attention maps generated by ViT, it is possible to visualize which aspects of an image are most crucial to \nthe classification decision. This information can be used to clarify the model’s decision when communicating \nwith humans.\nIn Table 1, the article summarizes recent contributions made for a range of tasks using Vision Transformer \narchitecture.\nMaterial and methods\nDataset characteristics\nIn the investigation, we used a publicly available chest X-ray (CXR) dataset from  Kaggle57,58. The same dataset \nhas also been utilized in numerous other investigations. The dataset consists of three sections: train, test, and \nvalidation. Each section contains subfolders for Pneumonia and Normal CXRs. There are 5863 X-ray images \nin total as shown in Table 2. The X-ray images used in the dataset were acquired at the Women and Children’s \nMedical Center in Guangzhou from children aged one to five.These images were taken as part of the children’s \nroutine medical examinations.To assure the quality of the X-ray images used in the analysis, they were screened \nby specialists for low-resolution or unreadable images. The remaining images were then evaluated by two physi-\ncian specialists, with any discrepancies resolved by a third specialist. This procedure was performed to teach an \nAI system to make precise diagnoses.80% of the dataset has been allocated to the training set, 10% to the test \nset, and 10% to the validation set, as shown in Table 3.\nProposed architecture\nThe proposed Architecture uses patch embeddings, positional encodings, several Transformer encoder layers, \nself-attention, feed-forward neural networks, and a classification head to classify and analyze imageswhich are \nshown in Fig. 2.\nInput embedding\nIt requires reshaping the input image into patches as shown in Fig.  3 and applying a linear transformation in \norder to obtain the embeddings. Let’s denote the input image as X ∈ R(H×W ×C) re H, W , and C, respectively, \nrepresent the height, breadth, and number of channels. Each patch has a dimension of P × P , and there are N \npatches in total. The input embedding can be represented the as E ∈ R(N ×D) where D is the number of dimen -\nsions of the embeddings.\nPositional encoding\nThe input embeddings include positional information to capture the relative and absolute positions of the patches. \nThe positional encoding matrix P ∈ R(N ×D) added to the input embeddings E element by element.\nTransformer encoder\nEach layer of the Transformer Encoder is constituted of a multi-head self-attention mechanism and a position-\nwise feed-forward network as shown in Fig. 4.\n(a) Multi-head self-attention: The attention weights between the input embeddings are computed by the multi-\nhead self-attention mechanism. It entails three linear transformations: Query (Q), Key (K), and Value (V), \nwith Q, K, and V ∈ R(N×D) Using the attention weights, the output of the self-attention mechanism is the \nweighted sum of the values. The attention weights are calculated by Eq. (1).\nwhere Dh represents the dimension of each attention head.\n(b) Position-wise feed-forward network: The position-wise feed-forward network employs two linear trans-\nformations separated by a nonlinear activation function (such as ReLU). Let’s designate the attention \nmechanism’s output as A ∈ R(N×D) The representation of the position-wise feed-forward network is as \naccording to Eq. (2).\nwhere W 1 ∈ R(D×dFFN ),b1 ∈ R(1×dFFN ),W 2 ∈ R(dFFN×D ),b2 ∈ R(1×D) .\nThese two sub-layers are applied parallel to the input sequence and then combined to generate the encoder \nlayer’s output. The process is repeated multiple times to form a stack of encoder layers, where each encoder layer \nbuilds upon the representation learned by the preceding encoder layer, enabling the model to learn increasingly \ncomplex and generalized representations of the input sequence.\n(1)Attention(Q , K , V ) = softMax\n((\nQK T\n)\n/√(Dh )\n)\nV\n(2)FFN(A) = max(0,A × W 1 + b1) × W2 + b2\n7\nVol.:(0123456789)Scientific Reports |         (2024) 14:2487  | https://doi.org/10.1038/s41598-024-52703-2\nwww.nature.com/scientificreports/\nTable 1.  Insight into related recent research.\nArticle references Approach Major findings\nGap identified and future direction for \nenhancement\n“ An Image is Worth 16 × 16 Words: Trans-\nformers for Image Recognition at Scale”53\nEach segmented patch is linearly projected \ninto a high-dimensional embedding space \nthat result is then input into the Trans-\nformer encoder.They replaced the tradi-\ntional CNN backbone with a Transformer \nencoder-decoder framework, thereby \nenabling a more unified framework across \nmodalities obtained cutting-edge perfor-\nmance on benchmark datasets with fewer \ncomputational resources than traditional \nCNN-based methods\nresults can be improved by adjusting the \nnumber of layers, the dimensionality of the \nembeddings, or the design of the attention \nmechanism and by fine-tuning the archi-\ntecture to strike a balance between model \ncapacity and computational efficiency\nTransformers demand more processing \npower and memory than convolutional \nneural networks (CNNs), and the article \ndoes not elucidate how to address this. \nTransformers are less interpretable than \nCNNs, and interpretability strategies are \nnot discussed in the article. Patch size, \ncomputational efficiency, and performance \ncompromises are not considered. Resolving \nthese issues could facilitate the scalability \nof image recognition methods based on the \nTransformer\n“Show, attend and tell: Neural image caption \ngeneration with visual attention”17\nThe authors demonstrate the effectiveness of \nincorporating a visual attention mechanism \ninto the caption generation process. The \nattention mechanism allows the model to \nfocus on various portions of the image while \ngenerating each word in the caption, thereby \nimproving the alignment between the image \ncontent and the generated text\nsuperior caption quality in comparison to \nprevious methods. By focusing on pertinent \nimage regions, the model generates more \naccurate and descriptive captions that \ncapture the image’s most important objects, \nactions, and relationships\nThe approach lacks fine-grained attention \nbecause it employs a mechanism for soft \nattention that assigns weights to image \nregions rather than concentrating on par-\nticular objects or attributes. This hinders \nthe capability of the model to generate cap-\ntions with precise details. The article does \nnot discuss strategies or techniques for \nfine-tuning the interpretation and control \nof the attention mechanism, thereby limit-\ning the adaptability and interpretability\n“Deep MRI Reconstruction with Generative \nVision Transformers”54\nDeep generative network GVTrans trans-\nlates noisy variables and latent onto high-\nquality MR images. Multi-layer architecture \nimproves image resolution. Cross-attention \ntransformer modules receive up-sampled \nfeature maps in each layer. MR images are \nmasked using the same sampling pattern as \nthe under-sampled acquisition for test data \ninference. Optimized network parameters \nensure that reconstructed and original \nk-space samples match\nbetter image quality than CNN-based \nreconstructions with and without self-\nattention processes and can adjust to indi-\nvidual test subjects. GVTrans may improve \ndeep MRI reconstruction applicability and \ngeneralizability\nUsing a larger dataset of fully-sampled \nMRI acquisitions for training GVTrans, \nincorporating additional information, such \nas patient demographics or clinical history, \ninto the training process, and developing \na more efficient training algorithm for \nGVTrans can improve the performance of \nthe proposed architecture GVTrans.Train-\ning in the proposed GVTrans architecture \nis computationally intensive.GVTrans \nmay be unable to reconstruct images with \nhigh levels of noise or anomalies, as well as \nimages with very low sampling rates\n“ A Simple Single-Scale Vision Transformer \nfor Object Detection and Instance Segmen-\ntation”46\nUniversal Vision Transformer (UViT), an \nintuitive and efficient Vision Transformer \narchitecture, was proposed for object detec-\ntion and instance segmentation\nUViT is a simple yet efficient model that \nachieves competitive performance on the \nCOCO benchmarks for object detection \nand instance segmentation\nOn some tasks, such as dense predic-\ntion, UViT may not attain the same level \nof performance as more complex Vision \nTransformer architectures. UViT may not \nbe as effective as models for object detec-\ntion and instance segmentation that are \nmore specialized\n“Training data-efficient image transformers \n& distillation through attention”34\nA large, pre-trained convolutional neural \nnetwork (CNN) is used as a teacher to train \na smaller, more efficient transformer-based \nstudent model in this method. The student \nmodel gains knowledge from the teacher \nby observing the instructor’s output, which \nis represented by a distillation token. The \ndistillation token is added to the input of the \nstudent model and is utilized to direct the \nattention mechanism\nDeiT-B model obtains 85.2% top-1 accuracy \non ImageNet with 86 M-parameterwhen \ntrained with 100 epochs and 16 GPUs\nThe distillation token can be computation-\nally expensive to compute, which is a limi-\ntation. Another limitation is that the distil-\nlation token can result in a reduction in \nthe attention weights’ diversity.It would be \npossible to enhance the distillation token \nby employing a more efficient method for \ncomputing it. The distillation token could \nbe modified to promote attention weights \nwith greater diversity. The method could be \napplied to additional tasks, including object \ndetection and segmentation\n“ Analyzing Transfer Learning of Vision \nTransformers for Interpreting Chest Radi-\nography”55\nutilizing a standard Vision Transformer \narchitecture and training it on a large col-\nlection of natural images. Using a limited \nnumber of labeled examples, they then \nrefined this model using the CheXpert or \nPediatric Pneumonia dataset\nA model’s performance on a medical image \nclassification task can be considerably \nenhanced by transfer learning from a previ-\nously trained Vision Transformer. There is \nno significant effect on the efficacy of the \nmodel by fine-tuning\nDomain adaption and other transfer \nlearning methods may improve Vision \nTransformers’ medical image classification \nperformance in future research. The mod-\nel’s performance can further be improved \nusing larger fine-tuning datasets\n“Introducing Convolutions to Vision \nTransformers”56\na novel design called Convolutional Vision \nTransformer (CvT) that increases Vision \nTransformers (ViTs) performance and \nefficiency by adding convolutions.A con-\nvolutional token embedding layer replaces \nthe token embedding layer. This enables \nthe CVT to discover spatial relationships \nbetween tokens, thereby enhancing the \nmodel’s capacity to represent complex visual \npatterns. Convolutional attention operation \nreplaces the attention operation. This ena-\nbles the CvT to efficiently compute attention \nweights across vast spatial regions, thereby \nenhancing the model’s capacity to capture \nglobal context\nCvT outperforms ViTs on a variety of image \nclassification tasks while requiring fewer \nparameters and FLOPs. For instance, the \nCvT achieves a top-1 accuracy of 89.4% on \nthe ImageNet-1 k dataset, which is compa-\nrable to the state-of-the-art performance of \nResNet-50 despite employing only 1/10th of \nthe parameters and 1/100th of the FLOPs\nCvTs are harder to train and slower at \ninference hen compared with ViT’s. Using \ndeeper and broader CvT models to further \nimprove performance, adding residual con-\nnections between CvT layers to improve \ntraining stability, and employing dilated \nconvolutions and group convolutions to \nimprove the model’s ability to represent \nlong-range dependencies can further \nimprove the proposed model\n8\nVol:.(1234567890)Scientific Reports |         (2024) 14:2487  | https://doi.org/10.1038/s41598-024-52703-2\nwww.nature.com/scientificreports/\nClassification layer. This layer utilizes the encoder layers’ output to predict pneumonia’s presence or absence. \nThis prediction may be made using a fully connected or convolutional layer.\nLoss function. This component evaluates the model’s efficacy based on the predicted and actual labels. In this \nendeavor, binary cross-entropy loss is a common loss function.\nEthical standards\nNo human participants were involved in the study. Dataset is available on Internet.\nResult and discussions\nPerformance indicators\nVarious evaluation metrics are used to measure the effectiveness of machine learning models, and each has its \nbenefits and drawbacks. The most prevalent metrics include.\nTable 2.  Class distribution of the dataset.\nClass No of images\nPneumonia (P) 4273\nNormal (N) 1583\nTable 3.  Partitioning of training, testing, and validation datasets.\n# of images # of images from P class # of images from N class\nTraining data 4684 3205 1479\nValidation data 586 360 226\nTest data 586 330 256\nFigure 2.  The proposed system design architecture.\n9\nVol.:(0123456789)Scientific Reports |         (2024) 14:2487  | https://doi.org/10.1038/s41598-024-52703-2\nwww.nature.com/scientificreports/\nAccuracy\nThis is the most important metric for evaluating a model and is defined as the proportion of correct predictions \nto the total number of predictions made by the model. It is evaluated using Eq. (3).\nFigure 3.  Dataset input image in the form of smaller patches.\nFigure 4.  Internal design of a transformer encoder.\n10\nVol:.(1234567890)Scientific Reports |         (2024) 14:2487  | https://doi.org/10.1038/s41598-024-52703-2\nwww.nature.com/scientificreports/\nPrecision\nHigher-precision classifiers produce fewer false positives. High accuracy reduces the likelihood of misclassifying \nnegative instances as positive in numerous applications where false positives have severe consequences. Preci -\nsion is calculated by Eq. (4).\nRecall (sensitivity or true positive rate)\nClassifiers with higher recall have fewer false negatives. The classifier captures positive cases and reduces false \nnegatives. A classifier with lower recall has more false negatives. The recall is determined by Eq. ( 5)\nF1 Score\nThe F1 Score is the harmonic mean of precision and recall, indicating patterns between them and calculated \nusing Eq. (6).\nROC curve\nROC curves evaluate binary classification models. The model separates positive and negative events across clas-\nsification thresholds. ROC curve form and position indicate model discrimination. The ROC curve shows the \ntrade-off between positive and negative identification when the classification threshold changes. AUC increases \ndiscrimination and model performance.\nConfusion matrix\nThe confusion matrix tabulates classification model performance. It compares predicted labels to real labels and \nshows different classification outcomes. The confusion matrix reveals model performance. True positives (TP) \nand true negatives (TN) are situations that were accurately predicted. False positives (FP) and false negatives (FN) \nare cases of misclassification. These values allow us to generate model performance metrics including accuracy, \nprecision, recall, and F1 score.\nModel’s training\nTo demonstrate our proposed architecture, we experimented with a benchmark dataset of CXR images, one of \nthe most frequently downloaded datasets for testing on Kaggle. Using these studies and datasets for binary clas-\nsification. Python 3.7, Anaconda/3, and CUDA/10 are installed on a Windows server with an i5 CPU, 2 GB GPU, \nand 8 GB RAM, as well as an Anaconda/3 distribution. In addition to the parameters listed above, the Python \nlibraries Pytorch, OpenCV , matplotlib, os, math, and NumPy are used. During training, the data is partitioned \ninto batches, and the model’s parameters are modified based on each cohort’s average loss. The group size dictates \nthe number of samples utilized during each update phase. A larger sample size can speed up the training rate but \nmay require additional memory. CrossEntropyLoss was chosen as the experiment’s loss function. During training, \nthe model minimizes this loss function. It computes the negative log-likelihood of expected class probability and \nactual labels. The training algorithm modifies the parameters of the model. In an experiment, the Adam opti -\nmizer was used to alter the learning rate for each parameter based on gradient estimates of the first and second \nmoments. Pytorch was used for the implementation, and training was conducted in a GPU environmentThe \nlearning rate establishes how much model parameters are updated with each optimizer iteration. The multiplica-\ntive factor of the learning rate is used to modify the learning rate at each epoch or phase, enabling more granular \ncontrol of the learning rate during training. The learning rate’s multiplicative factor can help the model converge \non a superior solution. Table 4 demonstrates the experiment’s hyperparameter settings. The novelty of our work \nlies in the application of the Vision Transformer (ViT), specifically utilizing the DEIT_Base_Patch16_224 pre-\ntrained weights, to the domain of medical imaging for pneumonia detection. While ViT has shown promise in \nvarious fields, its adaptation to medical imaging, especially chest X-ray analysis, is relatively unexplored. Our \napproach capitalizes on ViT’s ability to capture intricate spatial relationships in images, offering advantages over \ntraditional methods. We demonstrate improved performance and potential for enhanced pneumonia detection \naccuracy, marking a significant contribution to the field of medical image analysis.\nA model’s performance depends on these hyperparameters and others. To enhance model performance, \nselecting hyperparameter values requires careful analysis and experimentation. For optimal performance, hyper-\nparameters must be explored and fine-tuned based on task, dataset, and model architecture.\n(3)Accuracy = (True Positives+ True Negatives)\n(True Positives+ True Negatives+ False Positives+ False Negatives)\n(4)Precision= True Positives\n(True Positives+ False Positive)\n(5)Recall= True Positives\n(True Positives+ False Negatives)\n(6)FScore = 2 ∗(Precision∗Recall)\n(Precision_Recall)\n11\nVol.:(0123456789)Scientific Reports |         (2024) 14:2487  | https://doi.org/10.1038/s41598-024-52703-2\nwww.nature.com/scientificreports/\nPerformance evaluation\nThe model’s train-validation accuracy against the epoch curve shows its learning and generalization. If training \naccuracy increases but validation accuracy plateaus or falls, it indicates overfitting. Convergence and excellent \naccuracy for both curves show learning and generalization efficacy. The train-validation loss versus epochs curve \nshows model optimization. The model initially matches data better when training and validation loss decreases. \nOverfitting occurs when training loss decreases with increasing validation loss. Convergence and low loss suggest \nerror minimization and good generalization for both curves.\nTable 5 presents the performance delivered by the proposed approach and Figs. 5 and 6 show the relationship \nbetween accuracy and epoch and loss and epoch, respectively. Figures 5 and 6 show that during training, valida-\ntion accuracy gradually improves along with test accuracy and reaches 97.61 and other performance indicators \nare also indicating outperforming results.\nConfidence intervals test\nThis is statistical tool used to estimate the range within which a performance metric, such as accuracy, sensitivity, \nor specificity, is likely to lie. They provide a range of values that likely contains the true value of the parameter, \nalong with a level of confidence.\nConfidence interval (CI) is calculated using the formula described using Eq. (7 )\nZ is the z-score corresponding to the desired confidence level. For example, for a 95% confidence level, the \nZ-score is approximately 1.96.\n(7)AccuracyCI = Accuracy ± Z ×\n√\nAccuracy × (1 − Accuracy)\nsample size\nTable 4.  Hyper-parameter setting used in the experiment.\nHyperparameter Value\nBatch size 16\nCriterion CrossEntropyLoss\nLearning rate 1e − 05\nOptimizer Adam\nDevice Cuda\nImage resize 224 × 224\nThe multiplicative factor of the learning rate 0.995\nTable 5.  Performance delivered by the proposed model.\nEpoch Split ratio Loss (train) Accuracy (train) Loss (test) Accuracy (test) Sensitivity Specificity F score AUC \n30 0.20 0.057 98.04 0.069 97.61 0.949 0.981 0.952 0.966\nFigure 5.  Accuracy variation vs epoch curve.\n12\nVol:.(1234567890)Scientific Reports |         (2024) 14:2487  | https://doi.org/10.1038/s41598-024-52703-2\nwww.nature.com/scientificreports/\nInterpretation\nThe accuracy reported as 97.61% with a 95% confidence level, the confidence interval is between 96.2 and 98.9%. \nThis means we can be 95% confident that the true accuracy of our proposed model lies within this range.\nMatthews correlation coefficient (MCC)\nThe Matthews correlation coefficient (MCC) is a measure used in machine learning to evaluate the quality of \nbinary classification. The formula for MCC is described in Eq. (8).\nFrom the confusion matrix on the test data.\nTP = 152, TN = 420, FP = 6, FN = 8\nMCC ≈ 0.9396The Matthews correlation coefficient (MCC) typically ranges from − 1 to + 1:\n+ 1 indicates a perfect prediction,\n0 suggests a random prediction,\n− 1 indicates a total disagreement between prediction and observation.\nIn this case, an MCC of approximately 0.9396 indicates a very strong positive correlation between the pre -\ndicted and actual classifications. This suggests an excellent classification performance for the model used.\nThe confusion matrix in Fig. 7 shows that out of 586 samples in the test data, our proposed model showed 152 \ncases of TP and 420 cases of TN and 6 cases of FP ,and 8 cases of FN which indicates a test accuracy of 97.61%. \nVariation of precision and recall is represented by Figs. 8 and 9, which indicates that recall converse after 15 epocs \nwhile precision converse after 35 epocs. The ROC of the suggested architecture, depicted in Fig.  10, indicates \nan AUC value of 0.96. It denotes the capability of our proposed model to identify the presence or absence of \n(8)MCC = TP× TN − FP× FN√(TP+ FP)(TP+ FN)(TN + FP)(TN + FN)\nFigure 6.  Loss vs epoch curve.\nFigure 7.  Confusion matrix based on test data for the proposed model.\n13\nVol.:(0123456789)Scientific Reports |         (2024) 14:2487  | https://doi.org/10.1038/s41598-024-52703-2\nwww.nature.com/scientificreports/\nFigure 8.  Model precision with epocs.\nFigure 9.  Model recall with epocs.\nFigure 10.  ROC curve with AUC 0.96 of proposed work.\n14\nVol:.(1234567890)Scientific Reports |         (2024) 14:2487  | https://doi.org/10.1038/s41598-024-52703-2\nwww.nature.com/scientificreports/\npneumonia. A precision-recall value of 0.94, depicted in Fig. 11, suggests that the model demonstrates a notable \ncapacity to accurately predict positive instances while capturing a substantial proportion of the true positive \ninstances. The precise interpretation may differ depending on the domain of application and the particular \nobjectives of the classification endeavor.\nDiscussion\nTable 6 presents the performance of pre-train CNN architectures keeping all hyper-parameters values the same \nto make a comparison on the same datasets. It shows that Vision Transformer architecture offersa great improve-\nment over all other architectures, The proposed architecture offers an accuracy of 97.61% and an AUC of 0.96 \nbut this more extraordinary performance is obtained by compromising on training time because the training \nwas a bit time taking when compared with different architectures.\nResearch prospects in Vision Transformer\nVision Transformer (ViT) architecture research prospects for image classification hold tremendous potential \nfor advancing the field. Future research can concentrate on enhancing the performance of ViT models by opti-\nmizing their architecture, refining training strategies, and investigating novel techniques to improve precision, \nrobustness, and efficiency. In addition, efforts can be focused on developing interpretability methodologies for \nViT models, allowing for a better comprehension of their decision-making process. It is possible to investigate \nefficient training and inference methods to reduce computational complexity and accelerate model deployment. \nFigure 11.  Precision–recall curve of the proposed method.\nTable 6.  Performance evaluation relative to other architectures utilizing the same dataset.\nSr no. Architecture Refs. Accuracy F-score # of trainable parameters\n# of non-trainable \nparameters\n1 VGG16 59 92.14 0.9234 50,178 14,714,688\n2 VGG19 60 90.22 0.8999 50,178 20,024,384\n3 ResNet50 61 82.37 0.8281 200,706 23,587,712\n4 ResNet101 62 75.96 0.7593 200,706 42,658,176\n5 ResNet152 63 87.18 0.8734 200,706 58,370,944\n6 ResNet50V2 64 89.26 0.8937 200,706 23,564,800\n7 ResNet101V2 65 92.62 0.9250 200,706 42,626,560\n8 ResNet152V2 66 92.94 0.9312 200,706 58,331,648\n9 InceptionV3 67 89.42 0.8937 102,402 21,802,784\n10 InceptionResNetV2 68 90.70 0.8989 200,706 58,331,648\n11 DenseNet121 69 91.82 0.9171 100,354 7,037,504\n12 DenseNet169 70 88.78 0.8874 163,074 12,642,880\n13 DenseNet201 71 91.83 0.9171 188,162 18,321,984\n14 NASNetLarge 72 88.14 0.8812 975,746 84,916,818\n15 Quaternion Residual Network 73 93.75 0.9405 560,769 8,576\n16 Vision Transformer Proposed in the paper 97.61 0.9500 85,800,194 0\n15\nVol.:(0123456789)Scientific Reports |         (2024) 14:2487  | https://doi.org/10.1038/s41598-024-52703-2\nwww.nature.com/scientificreports/\nAdapting ViT to scenarios with limited data using semi-supervised and few-shot learning techniques will increase \nits applicability. In addition, domain-specific extensions, hybrid architectures that combine ViT with other \nmodels, and real-world deployments will contribute to the advancement and practical application of ViT in \nimage classification tasks.\nConclusion\nThe article conducts a thorough analysis of a Vision Transformer (ViT) framework for pneumonia detection in \nchest X-rays. ViTs’ ability to analyze complex image relationships is showcased, demonstrating superior per -\nformance over traditional CNNs and other advanced techniques. ViTs excel in capturing global context, spatial \nrelations, and handling variable image resolutions, leading to accurate pneumonia detection. The study aims to \nassess this method’s effectiveness by comparing it to state-of-the-art models on a diverse CXR dataset. The results \nreveal ViT’s superiority with an accuracy of 97.61%, sensitivity of 95%, and specificity of 98%. In conclusion, the \nViT-based approach holds promise for early pneumonia detection in CXRs, offering substantial development \npotential in this field. However, limitations include data scarcity and the need for real-world validation. Future \ndirections encompass enhancing interpretability, addressing model robustness, and conducting clinical trials \nfor practical deployment.\nData availability\nIn this work, a public dataset of CXR (https:// data. mende ley. com/ datas ets/ rscbj br9sj/2) has been used.\nReceived: 20 June 2023; Accepted: 22 January 2024\nReferences\n 1. Pneumonia in children. WHO (2019). https:// www. who. int/ news- room/ fact- sheets/ detail/ pneum onia\n 2. Khan, S. H. et al. COVID-19 detection and analysis from lung CT images using novel channel boosted CNNs. Expert Syst. Appl.  \n229, 120477 (2022).\n 3. Khan, S. H. et al. COVID-19 detection in chest X-ray images using deep boosted hybrid learning. Comput. Biol. Med. 137, 104816 \n(2021).\n 4. Khan, S. H., Sohail, A., Zafar, M. M. & Khan, A. Coronavirus disease analysis using chest X-ray images and a novel deep convo -\nlutional neural network. Photodiagnosis Photodyn. Ther. 35, 102473 (2021).\n 5. Singh, S., Tripathi, B. K. & Rawat, S. S. Deep quaternion convolutional neural networks for breast Cancer classification. Multimed. \nTools Appl. 82, 31285–31308 (2023).\n 6. Liang, G. & Zheng, L. A transfer learning method with deep residual network for pediatric pneumonia diagnosis. Comput. Methods \nPrograms Biomed. 187, 104964 (2020).\n 7. Nishio, M., Noguchi, S., Matsuo, H. & Murakami, T. Automatic classification between COVID-19 pneumonia, non-COVID-19 \npneumonia, and the healthy on chest X-ray image: Combination of data augmentation methods. Sci. Rep. 10, 1–6 (2020).\n 8. Asif, S., Zhao, M., Tang, F . & Zhu, Y . A deep learning-based framework for detecting COVID-19 patients using chest X-rays. \nMultimed. Syst. https:// doi. org/ 10. 1007/ s00530- 022- 00917-7 (2022).\n 9. Suryaa, V . S., Annie, A. X. & Aiswarya, M. S. Efficient DNN ensemble for pneumonia detection in chest X-ray images. Int. J. Adv. \nComput. Sci. Appl. 12, 759–767 (2021).\n 10. Singh, S., Kumar, M., Kumar, A., Verma, B. K. & Shitharth, S. Pneumonia detection with QCSA network on chest X-ray. Sci. Rep. \n13, 9025 (2023).\n 11. Duong, L. T., Nguyen, P . T., Iovino, L. & Flammini, M. Automatic detection of COVID-19 from chest X-ray and lung computed \ntomography images using deep neural networks and transfer learning. Appl. Soft Comput. 132, 109851 (2023).\n 12. Duong, L. T., Le, N. H., Tran, T. B., Ngo, V . M. & Nguyen, P . T. Detection of tuberculosis from chest X-ray images: Boosting the \nperformance with Vision Transformer and transfer learning. Expert Syst. Appl. 184, 115519 (2021).\n 13 Duong, L. T., Nguyen, P . T., Iovino, L. & Flammini, M. Deep learning for automated recognition of COVID-19 from chest X-ray \nimages. medRxiv. https:// doi. org/ 10. 1101/ 2020. 08. 13. 20173 997 (2020).\n 14. Kazemzadeh, S. et al. Deep learning detection of active pulmonary tuberculosis at chest radiography matched the clinical perfor-\nmance of radiologists. Radiology 306, 124–137 (2023).\n 15. Ramachandran, P . et al. Stand-alone self-attention in vision models. Adv. Neural Inform. Process. Syst. 32 (2019).\n 16. Guo, M.-H., Lu, C.-Z., Liu, Z.-N., Cheng, M.-M. & Hu, S.-M. Visual attention. Network. 14, 1–12 (2022).\n 17. Xu, K. et al. Show, attend and tell: Neural image caption generation with visual attention. in 32nd Int. Conf. Mach. Learn. ICML \n2015 3, 2048–2057 (2015).\n 18. Wang, F . et al. Residual attention network for image classification. in Proc.—30th IEEE Conf. Comput. Vis. Pattern Recognition, \nCVPR 2017 2017-Janua, 6450–6458 (2017).\n 19. Singh, S. et al. Deep attention network for pneumonia detection using chest X-ray images. Comput. Mater. Contin. 74, 1673–1690 \n(2023).\n 20. Kumar, M. & Biswas, M. Human activity detection using attention-based deep network. Springer Proc. Math. Stat. 417, 305–315 \n(2023).\n 21. Kumar, M., Patel, A. K., Biswas, M. & Shitharth, S. Attention-based bidirectional-long short-term memory for abnormal human \nactivity detection. Sci. Rep. 13, 14442 (2023).\n 22. Carion, N. et al. End-to-end object detection with transformers. in Lecture Notes in Computer Science (including subseries Lecture \nNotes in Artificial Intelligence and Lecture Notes in Bioinformatics) vol. 12346 LNCS 213–229 (2020).\n 23. Potamias, R. A., Siolas, G. & Stafylopatis, A. G. A transformer-based approach to irony and sarcasm detection. Neural Comput. \nAppl. 32, 17309–17320 (2020).\n 24. Wolf, T. et al. Transformers: State-of-the-art natural language processing. 38–45 (2020). doi:https:// doi. org/ 10. 18653/ v1/ 2020. \nemnlp- demos.6.\n 25. Singh, S. & Mahmood, A. The NLP cookbook: Modern recipes for transformer based deep learning architectures. IEEE Access 9, \n68675–68702 (2021).\n 26. Wolf, T. et al. Huggingface’s transformers: State-of-the-art natural language processing. arXiv Prepr. arXiv1910.03771 (2019).\n 27. Vaswani, A. et al. Attention is all you need. Adv. Neural Inform. Process. Syst. 2017-Decem, 5999–6009 (2017).\n 28. Al-Deen, H. S. S., Zeng, Z., Al-Sabri, R. & Hekmat, A. An improved model for analyzing textual sentiment based on a deep neural \nnetwork using multi-head attention mechanism. Appl. Syst. Innov. 4.4, 85 (2021).\n16\nVol:.(1234567890)Scientific Reports |         (2024) 14:2487  | https://doi.org/10.1038/s41598-024-52703-2\nwww.nature.com/scientificreports/\n 29. Feng, Y . & Cheng, Y . Short text sentiment analysis based on multi-channel CNN with multi-head attention mechanism. IEEE \nAccess  9, 19854–19863 (2021).\n 30. Park, S. et al. Multi-task Vision Transformer using low-level chest X-ray feature corpus for COVID-19 diagnosis and severity \nquantification. Med. Image Anal. 75, 102299 (2022).\n 31. Zhu, J. et al. Efficient self-attention mechanism and structural distilling model for Alzheimer’s disease diagnosis. Comput. Biol. \nMed. 147, 105737 (2022).\n 32. Chen, C., Gong, D., Wang, H., Li, Z. & Wong, K. Y . K. Learning spatial attention for face super-resolution. IEEE Trans. Image \nProcess. 30, 1219–1231 (2020).\n 33. Mondal, A. K., Bhattacharjee, A., Singla, P . & Prathosh, A. P . XViTCOS: Explainable Vision Transformer based COVID-19 screen-\ning using radiography. IEEE J. Transl. Eng. Heal. Med. 10, 1–10 (2021).\n 34. Touvron, H. et al. Training data-efficient image transformers & distillation through attention. in International Conference on \nMachine Learning 10347–10357 (2021).\n 35. Islam, M. N. et al. Vision Transformer and explainable transfer learning models for auto detection of kidney cyst, stone and tumor \nfrom CT-radiography. Sci. Rep. 12, 1–14 (2022).\n 36. Liu, Z. et al. Swin transformer: Hierarchical Vision Transformer using shifted windows. in Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision 10012–10022 (2021).\n 37. Zhu, Y . et al. Make a long image short: Adaptive token length for Vision Transformers. arXiv Prepr. arXiv2112.01686 (2021).\n 38 Han, K. et al. A survey on Vision Transformer. IEEE Trans. Pattern Anal. Mach. Intell. https:// doi. org/ 10. 1109/ TPAMI. 2022. 31522 \n47 (2022).\n 39. Jiang, Z. et al. Computer-aided diagnosis of retinopathy based on Vision Transformer. J. Innov. Opt. Health Sci. 15.02, 2250009 \n(2022).\n 40. Chen, J. et al. Channel and spatial attention based deep object co-segmentation. Knowledge-Based Syst. 211, 106550 (2021).\n 41. Zhang, Y ., Fang, M. & Wang, N. Channel-spatial attention network for fewshot classification. PLoS One 14, 1–16 (2019).\n 42. Bastidas, A. A. & Tang, H. Channel attention networks. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Work. 2019-June, \n881–888 (2019).\n 43. Singh, S. et al. Hybrid models for breast cancer detection via transfer learning technique. Comput. Mater. Contin. 74, 3063–3083 \n(2022).\n 44. Seemendra, A., Singh, R. & Singh, S. Breast cancer classification using transfer learning. Lect. Notes Electr. Eng. 694, 425–436 \n(2021).\n 45. Jiang, J. COVID-19 detection in chest X-ray images using swin-transformer and transformer in transformer.\n 46. Chen, W . et al. A simple single-scale Vision Transformer for object detection and instance segmentation. in Lect. Notes Comput. \nSci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics) 13670 LNCS, 711–727 (2022).\n 47. Goldberg, X. Introduction to semi-supervised learning. Synth. Lect. Artif. Intell. Mach. Learn. https:// doi. org/ 10. 2200/ S0019 6ED1V \n01Y20 0906A IM006 (2009).\n 48. Liu, X. et al. Self-supervised learning: Generative or contrastive. IEEE Trans. Knowl. Data Eng. 35.1, 857–876 (2021).\n 49. Caron, M. et al. Emerging properties in self-supervised Vision Transformers. in Proc. IEEE Int. Conf. Comput. Vis. 9630–9640 \n(2021). https:// doi. org/ 10. 1109/ ICCV4 8922. 2021. 00951.\n 50. Akbari, H. et al. V ATT: Transformers for multimodal self-supervised learning from raw video, audio and text. Adv. Neural Inf. \nProcess. Syst. 29, 24206–24221 (2021).\n 51. Li, W . et al. SepViT: Separable Vision Transformer. (2022).\n 52. Mangalam, K. et al. Reversible Vision Transformers. Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 2022-June, \n10820–10830 (2022).\n 53. Dosovitskiy, A. et al. An image is worth 16 × 16 words: Transformers for image recognition at scale. (2020).\n 54. Korkmaz, Y ., Yurt, M., Dar, S. U. H., Özbey, M. & Cukur, T. Deep MRI reconstruction with generative Vision Transformers. in \nMachine Learning for Medical Image Reconstruction: 4th International Workshop, MLMIR 2021, Held in Conjunction with MICCAI \n2021, Strasbourg, France, October 1, 2021, Proceedings 4 54–64 (2021).\n 55. Usman, M., Zia, T. & Tariq, A. Analyzing transfer learning of Vision Transformers for interpreting chest radiography. J. Digit. \nImaging. https:// doi. org/ 10. 1007/ s10278- 022- 00666-z (2022).\n 56. Wu, H. et al. CvT: Introducing convolutions to Vision Transformers. Proc. IEEE Int. Conf. Comput. Vis. https:// doi. org/ 10. 1109/ \nICCV4 8922. 2021. 00009 (2021).\n 57. Kermany, D., Zhang, K. & Goldbaum, M. Chest X-ray images (pneumonia). https:// data. mende ley. com/ datas ets/ rscbj br9sj/2\n 58. Kermany, D. Large dataset of labeled optical coherence tomography (OCT) and chest X-ray images. Mendeley Data . 3.10.17632 \n(2018).\n 59. M. Hassan. VGG16—Convolutional network for classification and detection. Neurohive  (2018). https:// neuro hive. io/ en/ popul \narnet works/ vgg16.\n 60. Dey, N., Zhang, Y . D., Rajinikanth, V ., Pugalenthi, R. & Raja, N. S. M. Customized VGG19 architecture for pneumonia detection \nin chest X-rays. Pattern Recognit. Lett. 143, 67–74 (2021).\n 61. Elpeltagy, M. & Sallam, H. Automatic prediction of COVID-19 from chest images using modified ResNet50. Multimed. Tools Appl. \n80.17 26451–26463 (2021).\n 62. Zhang, Q. A novel ResNet101 model based on dense dilated convolution for image classification. SN Appl. Sci. 4, 1–13 (2022).\n 63. Prabhakaran, A. K., Nair, J. J. & Sarath, S. Thermal facial expression recognition using modified ResNet152. in Lecture Notes in \nElectrical Engineering vol. 736 LNEE (2021).\n 64. Rahimzadeh, M. & Attar, A. A new modified deep convolutional neural network for detecting COVID-19 from X-ray images. \narXiv 19, 100360 (2020).\n 65. Lee, H. C. & Aqil, A. F . Combination of transfer learning methods for kidney glomeruli image classification. Appl. Sci. 12.3, 1040 \n(2022).\n 66. Albahli, S., Rauf, H. T., Algosaibi, A. & Balas, V . E. AI-driven deep CNN approach for multilabel pathology classification using \nchest X-rays. PeerJ Comput. Sci. 7, 1–17 (2021).\n 67. Jignesh Chowdary, G., Punn, N. S., Sonbhadra, S. K. & Agarwal, S. Face mask detection using transfer learning of inceptionV3. in \nLecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) \nvol. 12581 LNCS (2020).\n 68. Mondal, M. R. H., Bharati, S. & Podder, P . CO-IRv2: Optimized InceptionResNetV2 for COVID-19 detection from chest CT \nimages. PLoS One 16.10, e0259179 (2021).\n 69. Ezzat, D., Hassanien, A. ell & Ella, H. A. GSA-DenseNet121-COVID-19: A hybrid deep learning architecture for the diagnosis of \nCOVID-19 disease based on gravitational search optimization algorithm. Arxiv.Org (2020).\n 70. U.N. Oktaviana & Y . Azhar. Garbage Classification Using Ensemble DenseNet169. J. RESTI (Rekayasa Sist. dan Teknol. Informasi). \n5.6, 1207–1215 (2021).\n 71. Adhinata, F . D., Rakhmadani, D. P ., Wibowo, M. & Jayadi, A. A deep learning using DenseNet201 to detect masked or non-masked \nface. JUITA J. Inform. 9.1, 115–121 (2021).\n 72. Y ang, G., He, Y ., Y ang, Y . & Xu, B. Fine-grained image classification for crop disease based on attention mechanism. Front. Plant \nSci. 11, 1–15 (2020).\n17\nVol.:(0123456789)Scientific Reports |         (2024) 14:2487  | https://doi.org/10.1038/s41598-024-52703-2\nwww.nature.com/scientificreports/\n 73. Singh, S. & Tripathi, B. K. Pneumonia classification using quaternion deep learning. Multimed. Tools Appl. 81, 1743–1764 (2022).\nAuthor contributions\nAll authors contributed equally to this work. The manuscript was reviewed by all authors.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to S.S.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2024"
}