{
    "title": "Comparing Fine-Tuning, Zero and Few-Shot Strategies with Large Language Models in Hate Speech Detection in English",
    "url": "https://openalex.org/W4398152371",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2792595805",
            "name": "Ronghao Pan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2244707873",
            "name": "José Antonio García-Díaz",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2303035916",
            "name": "RAFAEL VALENCIA-GARCÍA",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4362515116",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W4385262440",
        "https://openalex.org/W4297633153",
        "https://openalex.org/W4283026156",
        "https://openalex.org/W4385571619",
        "https://openalex.org/W2954226438",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W6769311223",
        "https://openalex.org/W4311642023",
        "https://openalex.org/W6769627184",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3205068155",
        "https://openalex.org/W6809646742",
        "https://openalex.org/W4323709074",
        "https://openalex.org/W4380575774",
        "https://openalex.org/W6603104078",
        "https://openalex.org/W6848434655",
        "https://openalex.org/W4382652828",
        "https://openalex.org/W4386504863",
        "https://openalex.org/W4310829037",
        "https://openalex.org/W4320009668",
        "https://openalex.org/W6843452691",
        "https://openalex.org/W4300485781",
        "https://openalex.org/W4361866125",
        "https://openalex.org/W4312221641",
        "https://openalex.org/W3174155108",
        "https://openalex.org/W3193949653",
        "https://openalex.org/W6796997649",
        "https://openalex.org/W3187277406",
        "https://openalex.org/W3156978600",
        "https://openalex.org/W3134487808",
        "https://openalex.org/W6784815630",
        "https://openalex.org/W4390640302",
        "https://openalex.org/W3198907347",
        "https://openalex.org/W4327498226",
        "https://openalex.org/W4306955484",
        "https://openalex.org/W3211686893",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W4387561528",
        "https://openalex.org/W4387994989",
        "https://openalex.org/W4379539933",
        "https://openalex.org/W4388890574",
        "https://openalex.org/W4400522920",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4297840560",
        "https://openalex.org/W4387356888",
        "https://openalex.org/W3173171735",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4391211811",
        "https://openalex.org/W4394647257",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2787423662",
        "https://openalex.org/W3094831718",
        "https://openalex.org/W4363671699"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly demonstrating their ability to understand natural language and solve complex tasks, especially through text generation.One of the relevant capabilities is contextual learning, which involves the ability to receive instructions in natural language or task demonstrations to generate expected outputs for test instances without the need for additional training or gradient updates.In recent years, the popularity of social networking has provided a medium through which some users can engage in offensive and harmful online behavior.In this study, we investigate the ability of different LLMs, ranging from zero-shot and few-shot learning to fine-tuning.Our experiments show that LLMs can identify sexist and hateful online texts using zero-shot and few-shot approaches through information retrieval.Furthermore, it is found that the encoder-decoder model called Zephyr achieves the best results with the fine-tuning approach, scoring 86.811% on the Explainable Detection of Online Sexism (EDOS) test-set and 57.453% on the Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter (HatEval) test-set.Finally, it is confirmed that the evaluated models perform well in hate text detection, as they beat the best result in the HatEval task leaderboard.The error analysis shows that contextual learning had difficulty distinguishing between types of hate speech and figurative language.However, the fine-tuned approach tends to produce many false positives.",
    "full_text": null
}