{
  "title": "A Systematic Review of Testing and Evaluation of Healthcare Applications of Large Language Models (LLMs)",
  "url": "https://openalex.org/W4394890393",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3217758347",
      "name": "Suhana Bedi",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2152366482",
      "name": "Yutong Liu",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A5095746139",
      "name": "Lucy Orr-Ewing",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A3185615766",
      "name": "Dev Dash",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2552763475",
      "name": "Sanmi Koyejo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2122226006",
      "name": "Alison Callahan",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2521185912",
      "name": "Jason A. Fries",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A3030945216",
      "name": "Michael Wornow",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2914448302",
      "name": "Akshay Swaminathan",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2145342605",
      "name": "Lisa Soleymani Lehmann",
      "affiliations": [
        "Harvard University Press"
      ]
    },
    {
      "id": "https://openalex.org/A2271284612",
      "name": "Hyo-Jung Hong",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2441321456",
      "name": "Mehr Kashyap",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": null,
      "name": "Akash R. Chaurasia",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2186039848",
      "name": "Nirav R. Shah",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2186039848",
      "name": "Nirav R. Shah",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2116478967",
      "name": "Karandeep Singh",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A5093540105",
      "name": "Troy Tazbaz",
      "affiliations": [
        "United States Food and Drug Administration",
        "Food and Drug Administration"
      ]
    },
    {
      "id": "https://openalex.org/A2124493989",
      "name": "Arnold Milstein",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2126561330",
      "name": "Michael A. Pfeffer",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2150446796",
      "name": "Nigam H. Shah",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2150446796",
      "name": "Nigam H. Shah",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A3217758347",
      "name": "Suhana Bedi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2152366482",
      "name": "Yutong Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5095746139",
      "name": "Lucy Orr-Ewing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3185615766",
      "name": "Dev Dash",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2122226006",
      "name": "Alison Callahan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2521185912",
      "name": "Jason A. Fries",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3030945216",
      "name": "Michael Wornow",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2914448302",
      "name": "Akshay Swaminathan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2145342605",
      "name": "Lisa Soleymani Lehmann",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2271284612",
      "name": "Hyo-Jung Hong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2441321456",
      "name": "Mehr Kashyap",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Akash R. Chaurasia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2186039848",
      "name": "Nirav R. Shah",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116478967",
      "name": "Karandeep Singh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5093540105",
      "name": "Troy Tazbaz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2124493989",
      "name": "Arnold Milstein",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2126561330",
      "name": "Michael A. Pfeffer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2150446796",
      "name": "Nigam H. Shah",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4380047614",
    "https://openalex.org/W4389573101",
    "https://openalex.org/W4392087126",
    "https://openalex.org/W4389178822",
    "https://openalex.org/W4377197101",
    "https://openalex.org/W4389173934",
    "https://openalex.org/W4317790370",
    "https://openalex.org/W4385620388",
    "https://openalex.org/W4385381606",
    "https://openalex.org/W4366603014",
    "https://openalex.org/W4389082615",
    "https://openalex.org/W3118615836",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W4361295009",
    "https://openalex.org/W4386172820",
    "https://openalex.org/W4388485443",
    "https://openalex.org/W4388014051",
    "https://openalex.org/W4387743725",
    "https://openalex.org/W4388942469",
    "https://openalex.org/W4316671929",
    "https://openalex.org/W4386224103",
    "https://openalex.org/W4319341091",
    "https://openalex.org/W4383501093",
    "https://openalex.org/W4389961884",
    "https://openalex.org/W4391387740",
    "https://openalex.org/W4388826281",
    "https://openalex.org/W4391756575",
    "https://openalex.org/W4388975335",
    "https://openalex.org/W4388777264",
    "https://openalex.org/W4386530347",
    "https://openalex.org/W4391098193",
    "https://openalex.org/W4390974827",
    "https://openalex.org/W4388169129",
    "https://openalex.org/W3197347443",
    "https://openalex.org/W4387914853",
    "https://openalex.org/W4388464830",
    "https://openalex.org/W4386932783",
    "https://openalex.org/W4386270925",
    "https://openalex.org/W4392986561",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4389923012",
    "https://openalex.org/W4387496544",
    "https://openalex.org/W4383896353",
    "https://openalex.org/W4386592038",
    "https://openalex.org/W4388022607",
    "https://openalex.org/W4318754138",
    "https://openalex.org/W4385290206",
    "https://openalex.org/W4392542796",
    "https://openalex.org/W4385230595",
    "https://openalex.org/W4387821331",
    "https://openalex.org/W4387952963",
    "https://openalex.org/W4387494566",
    "https://openalex.org/W4386245555",
    "https://openalex.org/W4386887838",
    "https://openalex.org/W4388927925",
    "https://openalex.org/W4376638550",
    "https://openalex.org/W4367060129"
  ],
  "abstract": "1 Abstract Importance Large Language Models (LLMs) can assist in a wide range of healthcare-related activities. Current approaches to evaluating LLMs make it difficult to identify the most impactful LLM application areas. Objective To summarize the current evaluation of LLMs in healthcare in terms of 5 components: evaluation data type, healthcare task, Natural Language Processing (NLP)/Natural Language Understanding (NLU) task, dimension of evaluation, and medical specialty. Data Sources A systematic search of PubMed and Web of Science was performed for studies published between 01-01-2022 and 02-19-2024. Study Selection Studies evaluating one or more LLMs in healthcare. Data Extraction and Synthesis Three independent reviewers categorized 519 studies in terms of data used in the evaluation, the healthcare tasks (the what) and the NLP/NLU tasks (the how) examined, the dimension(s) of evaluation, and the medical specialty studied. Results Only 5% of reviewed studies utilized real patient care data for LLM evaluation. The most popular healthcare tasks were assessing medical knowledge (e.g. answering medical licensing exam questions, 44.5%), followed by making diagnoses (19.5%), and educating patients (17.7%). Administrative tasks such as assigning provider billing codes (0.2%), writing prescriptions (0.2%), generating clinical referrals (0.6%) and clinical notetaking (0.8%) were less studied. For NLP/NLU tasks, the vast majority of studies examined question answering (84.2%). Other tasks such as summarization (8.9%), conversational dialogue (3.3%), and translation (3.1%) were infrequent. Almost all studies (95.4%) used accuracy as the primary dimension of evaluation; fairness, bias and toxicity (15.8%), robustness (14.8%), deployment considerations (4.6%), and calibration and uncertainty (1.2%) were infrequently measured. Finally, in terms of medical specialty area, most studies were in internal medicine (42%), surgery (11.4%) and ophthalmology (6.9%), with nuclear medicine (0.6%), physical medicine (0.4%) and medical genetics (0.2%) being the least represented. Conclusions and Relevance Existing evaluations of LLMs mostly focused on accuracy of question answering for medical exams, without consideration of real patient care data. Dimensions like fairness, bias and toxicity, robustness, and deployment considerations received limited attention. To draw meaningful conclusions and improve LLM adoption, future studies need to establish a standardized set of LLM applications and evaluation dimensions, perform evaluations using data from routine care, and broaden testing to include administrative tasks as well as multiple medical specialties. Key Points Question How are healthcare applications of large language models (LLMs) currently evaluated? Findings Studies rarely used real patient care data for LLM evaluation. Administrative tasks such as generating provider billing codes and writing prescriptions were understudied. Natural Language Processing (NLP)/Natural Language Understanding (NLU) tasks like summarization, conversational dialogue, and translation were infrequently explored. Accuracy was the predominant dimension of evaluation, while fairness, bias and toxicity assessments were neglected. Evaluations in specialized fields, such as nuclear medicine and medical genetics were rare. Meaning Current LLM assessments in healthcare remain shallow and fragmented. To draw concrete insights on their performance, evaluations need to use real patient care data across a broad range of healthcare and NLP/NLU tasks and medical specialties with standardized dimensions of evaluation.",
  "full_text": " \n1 \nA Systematic Review of Testing and Evaluation of Healthcare Applications \nof Large Language Models (LLMs) \nAuthors: Suhana Bedi, Yutong Liu, Lucy Orr-Ewing, Dev Dash, Sanmi Koyejo, Alison Callahan, \nJason A. Fries, Michael Wornow, Akshay Swaminathan, Lisa Soleymani Lehmann, Hyo Jung \nHong, Mehr Kashyap, Akash R. Chaurasia, Nirav R. Shah, Karandeep Singh, Troy Tazbaz, \nArnold Milstein, Michael A. Pfeffer, and Nigam H. Shah \n0. Key Points \n● Question: How are healthcare applications of large language models (LLMs) currently \nevaluated? \n● Findings: Studies rarely used real patient care data for LLM evaluation. Administrative \ntasks such as generating provider billing codes and writing prescriptions were \nunderstudied. Natural Language Processing (NLP)/Natural Language Understanding \n(NLU) tasks like summarization, conversational dialogue, and translation were \ninfrequently explored. Accuracy was the predominant dimension of evaluation, while \nfairness, bias and toxicity assessments were neglected. Evaluations in specialized \nfields, such as nuclear medicine and medical genetics were rare. \n● Meaning: Current LLM assessments in healthcare remain shallow and fragmented. To \ndraw concrete insights on their performance, evaluations need to use real patient care \ndata across a broad range of healthcare and NLP/NLU tasks and medical specialties \nwith standardized dimensions of evaluation. \n1. Abstract  \nImportance: Large Language Models (LLMs) can assist in a wide range of healthcare-related \nactivities. Current approaches to evaluating LLMs make it difficult to identify the most impactful \nLLM application areas. \n \nObjective: To summarize the current evaluation of LLMs in healthcare in terms of 5 \ncomponents: evaluation data type, healthcare task, Natural Language Processing (NLP)/Natural \nLanguage Understanding (NLU) task, dimension of evaluation, and medical specialty. \n \nData Sources: A systematic search of PubMed and Web of Science was performed for studies \npublished between 01-01-2022 and 02-19-2024. \n \nStudy Selection: Studies evaluating one or more LLMs in healthcare. \n \nData Extraction and Synthesis: Three independent reviewers categorized 519 studies in \nterms of data used in the evaluation, the healthcare tasks (the what) and the NLP/NLU tasks \n(the how) examined, the dimension(s) of evaluation, and the medical specialty studied.  \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 16, 2024. ; https://doi.org/10.1101/2024.04.15.24305869doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n \n2 \nResults:  \nOnly 5% of reviewed studies utilized real patient care data for LLM evaluation. The most popular \nhealthcare tasks were assessing medical knowledge (e.g. answering medical licensing exam \nquestions, 44.5%), followed by making diagnoses (19.5%), and educating patients (17.7%). \nAdministrative tasks such as assigning provider billing codes (0.2%), writing prescriptions \n(0.2%), generating clinical referrals (0.6%) and clinical notetaking (0.8%) were less studied. For \nNLP/NLU tasks, the vast majority of studies examined question answering (84.2%). Other tasks \nsuch as summarization (8.9%), conversational dialogue (3.3%), and translation (3.1%) were \ninfrequent. Almost all studies (95.4%) used accuracy as the primary dimension of evaluation; \nfairness, bias and toxicity (15.8%), robustness (14.8%), deployment considerations (4.6%), and \ncalibration and uncertainty (1.2%) were infrequently measured. Finally, in terms of medical \nspecialty area, most studies were in internal medicine (42%), surgery (11.4%) and \nophthalmology (6.9%), with nuclear medicine (0.6%), physical medicine (0.4%) and medical \ngenetics (0.2%) being the least represented. \n \nConclusions and Relevance: Existing evaluations of LLMs mostly focused on accuracy of \nquestion answering for medical exams, without consideration of real patient care data. \nDimensions like fairness, bias and toxicity, robustness, and deployment considerations received \nlimited attention. To draw meaningful conclusions and improve LLM adoption, future studies \nneed to establish a standardized set of LLM applications and evaluation dimensions, perform \nevaluations using data from routine care, and broaden testing to include administrative tasks as \nwell as multiple medical specialties. \n \nKeywords: Large Language Models, Generative Artificial Intelligence, Healthcare, Dimensions \nof Evaluation, Evaluation Metrics.  \n2. Introduction \nThe adoption of Artificial Intelligence (AI) in healthcare is rising, catalyzed by the emergence of \nLarge Language Models (LLMs) like OpenAI's ChatGPT 1 2 3 4. Unlike predictive AI, generative \nAI produces original content such as sound, image, and text5. Within the realm of generative AI, \nLLMs produce structured, coherent prose in response to text inputs, with broad application in \nhealth system operations 6. Prominent applications such as facilitating clinical note-taking have \nalready been implemented by several health systems in the U.S., and there is excitement in the \nmedical community for improving healthcare efficiency, quality, and patient outcomes 7 8. A \nrecent report estimates that LLMs could unlock a substantial portion of the $1 trillion in untapped \nhealthcare efficiency improvements, including an estimated savings ranging from 5 to 10 \npercent of US healthcare spending or approximately $200 billion to $360 billion annually based \non 2019 figures 9 10. \nDespite their potential, the performance of LLMs in real-world healthcare settings remains \ninconsistently evaluated 11 12. For instance, Cadamuro et al. assessed ChatGPT-4’s diagnostic \nability by evaluating relevance, correctness, helpfulness, and safety, finding responses to be \ngenerally superficial and sometimes inaccurate, lacking in helpfulness and safety 13. In contrast, \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 16, 2024. ; https://doi.org/10.1101/2024.04.15.24305869doi: medRxiv preprint \n \n3 \nPagano et al. also assessed diagnostic ability, but focused solely on correctness, concluding \nthat ChatGPT-4 exhibited a high level of accuracy comparable to clinician responses 14. Thus, \nwe hypothesize that the current evaluation landscape lacks the uniformity, thoroughness, and \nrobustness necessary to effectively guide the deployment of LLMs in a real-world setting. \nThis systematic review of 519 studies provides a comprehensive characterization of how LLMs \nhave been evaluated in healthcare settings. To accomplish this, we categorize each study along \n5 axes: evaluation data type used, healthcare task, NLP/NLU task, dimension of evaluation, and \nmedical specialty. To enable the categorization of the diverse range of applications and their \nevaluation setups, we use two categorization frameworks: the first describes healthcare \napplications of LLMs in terms of their constituent healthcare and NLP/NLU tasks, and the \nsecond describes dimensions of evaluation and associated metrics. These frameworks are then \napplied systematically to characterize the current state of evaluations to quantify the variability \nin LLM application evaluations and identify areas for further exploration. Our results show that \nevaluations of LLM applications in healthcare have been unevenly distributed both in terms of \ndimensions of evaluation used and in terms of medical specialty and application. \n3. Methods  \n3.1 Design \nA systematic review was conducted following Preferred Reporting Items for Systematic Reviews \nand Meta-Analyses (PRISMA) guidelines as shown in Figure 1 15.  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 16, 2024. ; https://doi.org/10.1101/2024.04.15.24305869doi: medRxiv preprint \n \n4 \n \nFigure 1: PRISMA Flow Diagram \nThis diagram shows the process of screening and selecting the categorized 519 studies. \n \n \n3.2 Information sources  \nPeer-reviewed studies and preprints from January 1 2022, to February 19 2024, were retrieved \nfrom PubMed and Web of Science databases, using specific keywords as detailed in \nSupplement 1. Our search focused on titles and abstracts to identify studies on evaluation of \nLLMs' healthcare applications. This two-year period aimed to capture publications evaluating \nLLM healthcare applications since the public launch of ChatGPT in November 2022. Given our \nhypothesis that the current landscape lacks the necessary elements needed to truly assess LLM \nperformance in healthcare, we included a broad spectrum of studies. Citations were imported \ninto EndNote 21 (Clarivate) for analysis. \n \n3.3 Categorization framework \nEach study was categorized by evaluation data type, healthcare task, NLP/NLU task, dimension \nof evaluation, and medical specialty. Healthcare task categories were developed using publicly \navailable healthcare task and competency lists and were refined by consulting board-certified \nMDs 16 17 as outlined in Table 1. NLP/NLU categories and dimension of evaluations were \ndeveloped using the Holistic Evaluation of Language Models (HELM) and Hugging Face \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 16, 2024. ; https://doi.org/10.1101/2024.04.15.24305869doi: medRxiv preprint \n \n5 \nframeworks 18 19 as shown in Tables 2 and 3. Medical specialties were adapted from \nAccreditation Council for Graduate Medical Education (ACGME) residency programs. 20 \n \nTable 1: Healthcare task definitions and examples  \nThis table lists the range of healthcare tasks that the 519 studies were categorized into, with definition and example \nfor each task category. \nHealthcare Tasks Definition Example \nEnhancing medical \nknowledge \nThe process of enhancing the skills, knowledge, \nand capabilities of healthcare professionals to \nmeet the evolving needs of healthcare delivery. \nMeasuring the performance of GPT on \nNeurosurgery Written Board examinations \n(Ali et al)21 \nMaking diagnoses \nThe process of identifying the nature or cause \nof a disease or condition through the \nexamination of symptoms, medical history, and \ndiagnostic tests. \nComparing the performance of GPT and \nPhysicians for diagnostic accuracy (Fraser \net al)22 \nEducating patients \nProviding patients with information and \nresources to help them understand their health \nconditions, treatment options etc. for more \ninformed decision-making around their care. \nUsing GPT for Patient Information in \nperiodontology (Babayiğit et al)23 \nMaking treatment \nrecommendations \nThe process of providing treatment \nrecommendations for patients to manage or \ncure their health conditions.  \nUsing GPT for therapy recommendations in \nmental health (Patient Information in \nPeriodontology (Wilhelm et al)24 \nCommunicating \nwith patients \nThe exchange of information between \nhealthcare providers and patients. This could \nbe done via patient messaging platforms, or via \nChatbots integrated into the provider workflow.  \nUsing GPT to communicate with palliative \ncare patients (Srivastava et al)25 \nCare coordination \nand planning \nThe process of organizing and integrating \nhealthcare services to ensure that patients \nreceive the right care at the right time, involving \ncommunication and collaboration \nMeasuring the reliability and quality of \nnursing care planning generated (Dağcı et \nal)26 \nTriaging patients \nClinical triage is the process of prioritizing \npatients based on the severity of their condition \nand the urgency of their need for care. \nMeasuring the accuracy of patient triage in \nparasitology examination (Huh)27 \nCarrying out a \nliterature review \nA literature review is a critical summary and \nevaluation of existing research or literature on a \nspecific topic.  \nExamining the validity of ChatGPT in \nidentifying relevant Nephrology literature \n(Suppadungsuk et al)28 \nSynthesizing data \nfor research \nData synthesis refers to the process of \ncombining and analyzing data from multiple \nsources to generate new insights, draw \nconclusions, or develop a comprehensive \nunderstanding of a topic. \nSynthesizing radiologic data for effective \nclinical decision-making (Rao et al)29 \nGenerating clinical \nreferrals \nA referral is an order that a medical provider \nplaces to send their patient to a specialized \nphysician or department for further evaluation, \ndiagnosis, or treatment.  \nAssistance in optimizing Emergency \nDepartment radiology referrals and imaging \nselection (Barash et al)30 \nGenerating medical \nreports \nAn image-captioning task of producing a \nprofessional report according to input image \ndata. \nAssessing the feasibility and acceptability of \nChatGPT generated radiology report \nsummaries for cancer patients (Chung et \nal)31 \nManaging clinical \nknowledge \nThe process of ensuring clinical knowledge \nbases is correct, consistent, complete, and \ncurrent.  \nUsing GPT models for phenotype concept \nrecognition (Groza et al)32 \nProviding \nasynchronous care \nA proactive way to ensure that everyone \nassigned to a clinic is up to date on basic \npreventive care - like cancer screenings or \nAsynchronously answering patient \nquestions pertaining to erectile dysfunction \n(Razdan et al)33 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 16, 2024. ; https://doi.org/10.1101/2024.04.15.24305869doi: medRxiv preprint \n \n6 \nimmunizations - and that they receive extra help \nif they have lab numbers that are high. \nClinical note-taking \nThe process of recording detailed information \nabout a patient's health status, medical history, \nsymptoms, physical examination findings, \ndiagnostic test results, treatment plans, typically \ndocumented in the patient's EMR \nUsing GPT models for taking notes during \nprimary care visits (Kassab et al)34 \nEnhancing surgical \noperations \nThe process of supporting healthcare \nprofessionals, such as surgical technologists, \nnurses, and other staff, during surgical \nprocedures \nUsing GPT to pinpoint innovations for future \nadvancements in general surgery (Lim et \nal)35 \nConducting medical \nresearch \nMedical research generation, including writing \npapers, refers to the process of conducting \noriginal research in medicine or healthcare and \ndocumenting the findings in academic papers.  \nUsing GPT models for sentiment analysis of \nCOVID-19 survey data (Lossio-Ventura et \nal)36 \nBiomedical data \nmining \nThe process of searching and extracting data \nregarding a patient's health \nUsing GPT models to mine and generate \nbiomedical text (Chen et al)37 \nGenerating provider \nbilling codes \nMedical billing is the process of submitting and \nfollowing up on claims with health insurance \ncompanies to receive payment for healthcare \nservices provided to patients. \nUsing GPT models to predict diagnosis-\nrelated group (DRG) codes for hospitalized \npatients (Wang et al)38 \nWriting \nprescriptions \nThe process by which a healthcare provider, \ntypically a physician or other qualified medical \nprofessional, orders medications or treatments \nfor a patient \nPrescription of kidney stone prevention \ntreatment (Alumtrakul et al)39 \n \n \n \nTable 2: Definition of NLP/NLU tasks \nThis table lists the range of NLP/NLU tasks that the 519 studies were categorized into, with definition and examples \nfor each task category. \nNLP/NLU Task  Definition Examples \nSummarization \nFor a clinical document D of length L, \ngenerate a concise summary such that \nlength of the summary l << L. \n\"Summarize the impression section of a \nradiology report\" \nQuestion Answering \nFor a clinical question Q, with or \nwithout reference to a context T, \ngenerate a response R. \n- \"What are the symptoms of Type 2 \ndiabetes?\" \n- “What is the recommended dosage of \nibuprofen for a 40 year-old male with mild \nfever?” \nInformation Extraction \nFor a clinical document D, extract \nstructured information with semantic \nlabels s_1, ..., s_n. \n\"Extract the mentions of adverse drug events, \ndisease exacerbations and surgical \ninterventions from a patient's history.\" \nText Classification \nFor a clinical document D of length L, \nassign a label or class P \n\"Categorize clinical notes into classes such \nas 'diagnosis', 'treatment' or 'prognosis' \nTranslation \nFor a clinical document D in language \nM, generate another document D' in \nlanguage M' where D == D' \n\"Translate a patient's old lab test results from \nSpanish to English\" \nConversational Dialogue \nFor a history of chat messages m_1,.., \nm_n generate the next response \nm_{n+1} \n\"Using a patient's history of chat messages, \nhelp them reschedule their appointment\" \n \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 16, 2024. ; https://doi.org/10.1101/2024.04.15.24305869doi: medRxiv preprint \n \n7 \nTable 3: Dimensions of evaluation for LLM response  \nThis table lists the range of dimensions of evaluations that the 519 studies were categorized into, with definitions, \nmetrics and reviewer-generated example responses where each dimension is evaluated for a simple input question, \n“What are the symptoms of Type 2 diabetes?” \n \nDimension of Evaluation Definition Metric Examples Illustrative response demonstrating \neach dimension of evaluation  \nAccuracy Measures how close the \nLLM output is to the true \nor expected answer \nHuman evaluated \ncorrectness, \nROUGE, MEDCON \nCorrect Response - \"Common \nsymptoms of Type 2 Diabetes include \nfrequent urination, increased thirst, \nunexplained weight loss, fatigue, and \nblurred vision.\" \nCalibration and \nUncertainty \nMeasures how uncertain \nor underconfident an \nLLM is about its output \nfor a specific task \nHuman evaluated \nuncertainty, \ncalibration error, \nPlatt scaled \ncalibration slope \nResponse with an uncertainty \nestimate - \"As per my knowledge, the \nmost common symptoms of Type 2 \nDiabetes are frequent urination, \nincreased thirst, and unexplained weight \nloss, however, my information might be \noutdated, so I would put a confidence \nscore 0.3 for my response and I would \nrecommend contacting a healthcare \nprovider for a more accurate and certain \nresponse.\" \nRobustness Measures the LLM's \nresilience against \nadversarial attacks and \nperturbations like typos. \nHuman evaluated \nrobustness, exact \nmatch on LLM input \nwith intentional \ntypos, F1 on LLM \ninput with intentional \nuse of word \nsynonyms \nVariation 1: \"What are the signs of Type \n2 Diabetes?\" \nRobust Response (Synonym): \"Signs of \nType 2 Diabetes include frequent \nurination, increased thirst, unexplained \nweight loss, fatigue, and blurred vision.\" \nVariation 2 (Typo):  \"Syptom of Tpye 2 \nDiabetes?\" \nRobust Response: \"Symptoms of Type \n2 Diabetes include frequent urination, \nincreased thirst, unexplained weight loss, \nfatigue, and blurred vision.\" \nFactuality Measures how an LLM's \noutput for a specific task \noriginates from a \nverifiable and citable \nsource. It is important \nto note that it is possible \nfor a response to be \naccurate but factually \nincorrect if it originates \nfrom a hallucinated \ncitation \nHuman evaluated \nfactual consistency, \ncitation recall, \ncitation precision \nFactual Response: \"Symptoms of Type \n2 Diabetes are often related to insulin \nresistance and include frequent urination, \nincreased thirst, unexplained weight loss, \nfatigue, and blurred vision. Here is a \nreference to the link I referred to in \ncrafting this response - \nhttps://www.niddk.nih.gov/health-\ninformation/diabetes/overview/what-is-\ndiabetes/type-1-diabetes\" \nComprehensiveness Measures how well an \nLLM's output coherently \nand concisely addresses \nall aspects of the task \nand reference provided \nHuman evaluated \ncomprehensiveness, \nfluency, UniEval \nrelevance  \nComprehensive Response: \"Symptoms \nof Type 2 Diabetes include frequent \nurination, increased thirst, unexplained \nweight loss, fatigue, blurred vision, slow \nwound healing, and tingling or numbness \nin the hands or feet. \" \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 16, 2024. ; https://doi.org/10.1101/2024.04.15.24305869doi: medRxiv preprint \n \n8 \nFairness, bias and \ntoxicity \nMeasures whether an \nLLM's output is \nequitable, impartial, and \nfree from harmful \nstereotypes or biases, \nensuring it does not \nperpetuate injustice or \ntoxicity across diverse \ngroups \nHuman evaluated \ntoxicity, \ncounterfactual \nfairness, \nperformance \ndisparities across \nrace \nUnbiased Response: \"Symptoms of \nType 2 Diabetes can vary, and it's \nimportant to seek medical advice for \nproper diagnosis. Common symptoms \ninclude frequent urination, increased \nthirst, unexplained weight loss, fatigue, \nand blurred vision.\" \nBiased Response: \"Type 2 Diabetes \nsymptoms are often seen in individuals \nwith poor lifestyle choices.\" \nDeployment \nconsiderations \nMeasures the technical \nand parametric details of \nan LLM to generate a \ndesired output \nCost, latency, \ninference runtime \nResponse with runtime: \"The model \nprovides information about Type 2 \nDiabetes symptoms in less than 0.5 \nsecond, ensuring quick access to \nessential health information.\" \n \n \n \n \n3.4 Eligibility criteria and screening \nScreening was conducted by SB, YL, and LOE using the Covidence software (Covidence, 2024) \nas outlined in Figure 1. Included studies used LLMs for healthcare tasks and evaluated their \nperformance. Excluded articles were those focused on multimodal tasks or basic biological \nscience research with LLMs.  \n \n3.5 Data extraction and labeling \nWe adopted a paired review approach, wherein each study was categorized into evaluation data \ntype, healthcare tasks, NLP/NLU tasks, dimension(s) of evaluation, and medical specialty by at \nleast one human reviewer (SB, YL, or LOE) and GPT-4, based on the title and abstract. Note \nthat GPT-4 was used as a force multiplier while the final categories were assigned by the \nhuman reviewers. In instances of disagreements regarding category assignments, the methods \nsections of the studies were retrieved, and final categories were determined through reviewer \nconsensus. The prompts given to GPT-4 can be found in Supplement 2. \nEach study received one or more healthcare tasks, NLP/NLU task, and dimension of evaluation \nlabels as appropriate, hence the percentages sum above 100% in Table 4. In addition, each \nstudy could be assigned more than one medical specialty based on the evaluation conducted. \n4. Results  \n749 relevant studies were screened for eligibility. After applying the inclusion and exclusion \ncriteria described in 3.4, 519 studies were included in the analysis using the frameworks \ndeveloped by the authors. \n \n4.1 Categorization framework for healthcare tasks, NLP/NLU tasks and dimensions of \nevaluation \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 16, 2024. ; https://doi.org/10.1101/2024.04.15.24305869doi: medRxiv preprint \n \n9 \nWe deconstructed each healthcare application of an LLM into its constituent healthcare task \n(Table 1), i.e. the clinical and non-clinical task it is used for (the “what”), and the NLP/NLU task \n(Table 2), i.e. the language processing task being performed (the “how”). Examples of a \nhealthcare task are diagnosing a patient’s disease, recommending a treatment for osteoarthritis. \nExamples of the language-processing job to be accomplished – which is not necessarily specific \nto the medical domain are summarizing the impression section of a radiology report, answering \nquestions about the symptoms of type 2 diabetes etc. \n \nAn example of how healthcare tasks and NLP/NLU combine for a healthcare application of LLM \nis how Gan et al. evaluated LLM performance for mass-casualty triaging 40. The healthcare task \n(the “what”) is triaging patients while the NLP/NLU tasks (the “how”) are information extraction \n(extracting detailed patient information from the triage questionnaire scenarios, including age, \nsymptoms, and vital signs), text classification (classifying the triage questionnaire scenarios into \ndifferent triage levels), and question answering (generating final decision responses to the triage \nquestionnaire). \nWe initially compiled a list of healthcare tasks using publicly available resources 41 42.  \nSubsequently, through consultation with three board-certified MDs, we refined the list through \niterative discussions to establish the final categories for classification, as outlined in Table 1. To \ncompile a list of common NLP/NLU tasks, we referred to sources such as the Holistic Evaluation \nof Language Models (HELM) study and the Hugging Face task framework to derive 6 \ncategories: 1) Summarization, 2) Question answering, 3) Information extraction, 4) Text \n classification (such as clinical notes, research articles, and documents), 5) Translation, \nand 6) Conversational dialogue (Table 2) 43 44. \nWe categorized the most common dimensions of evaluation used in the reviewed studies based \non the list outlined in Table 3. These dimensions include: 1) Accuracy, 2) Calibration and \nuncertainty, 3) Robustness, 4) Factuality, 5) Comprehensiveness, 6) Fairness, bias, and toxicity, \nand 7) Deployment considerations. Fairness, bias, and toxicity were grouped together for ease \nof analysis, due to their infrequent occurrence in the reviewed studies, and relevance to ethical \nevaluation of LLMs. Additionally, we compiled common metrics for each dimension (eFigure 1) \nto serve as a starting framework for researchers designing studies to assess LLM performance \nin healthcare applications. \n4.2 Distribution of studies based on evaluation data type \nAmong the reviewed studies, 5% evaluated and tested LLMs using real patient care data, while \nthe remaining relied on data such as medical examination questions, clinician-designed \nvignettes or Subject Matter Expert (SME) generated questions.  \n \n4.3 Categorizing articles based on healthcare tasks and NLP/NLU tasks \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 16, 2024. ; https://doi.org/10.1101/2024.04.15.24305869doi: medRxiv preprint \n \n10 \nThe studies we examined had a predominant focus on evaluating LLMs for their medical \nknowledge (Table 4), primarily through assessments such as the USMLE. This trend assumes \nthat because we assess medical professionals' readiness for entering clinical practice through \nboard-style examinations, mirroring this type of evaluation for LLMs is adequate to certify their \nfitness-for-use. Making diagnoses, educating patients and making treatment recommendations \nwere the other common healthcare tasks studied. While these tasks represent critical aspects of \nhealthcare delivery, validating the utility of LLMs in supporting them requires assessment with \nreal patient care data. The limited examination of administrative tasks like assigning provider \nbilling codes, writing prescriptions, generating clinical referrals, and clinical notetaking suggests \na gap in studying LLMs’ use for high-value, immediately impactful administrative tasks. These \ntasks are often labor intensive, presenting a ripe opportunity for testing LLMs to enhance \nefficiency in these areas 45. \n \nAmong the NLP/NLU tasks, most studies evaluated LLM performance through question \nanswering tasks. These tasks ranged from addressing generic inquiries about symptoms and \ntreatments to tackling board-style questions featuring clinical vignettes. While this initial \nemphasis is understandable, it underscores a substantial gap in testing LLMs with real patient \ncare data, encompassing diverse patient demographics, medical history, medications, and lab \nresults. Approximately a quarter of the studies focused on text classification and information \nextraction tasks. Tasks such as summarization, conversational dialogue, and translation \nremained underexplored. This gap is significant because condensing patient records into \nconcise summaries, translating medical content into simpler languages or the patient's native \nlanguage, and facilitating conversations through chatbots are often touted benefits of using \nLLMs and could substantially alleviate physician burden. \n \n4.4 Categorizing articles based on the dimensions of evaluation \nAs seen in Table 4, accuracy and comprehensiveness were overwhelmingly the top two most \nexamined dimensions, whereas factuality, fairness, bias, and toxicity, robustness, deployment \nconsiderations, and calibration and uncertainty were infrequently assessed. This suggests a \npotential gap in assessing the broader capabilities and suitability of LLMs for real-world \ndeployment. While accuracy and comprehensiveness are crucial for ensuring the reliability and \neffectiveness of LLMs in healthcare tasks, dimensions like fairness, bias, and toxicity are \nequally vital for addressing ethical concerns and ensuring equitable outcomes. Similarly, \nrobustness and deployment considerations are essential for assessing the sustainability of \nintegrating LLMs into healthcare systems. The limited assessment of calibration and uncertainty \nraises questions about the extent to which researchers are addressing the need for LLMs to \nprovide uncertainty quantifications, particularly in healthcare scenarios. \n \n \n \nTable 4: Frequency of publications examining each dimension of evaluation across \nhealthcare and NLP/NLU task categories \nThe first column lists healthcare tasks followed by NLP/NLU tasks (separated by a double line); the first row lists the \ndimensions of evaluation used in each study examined. The percentages in the last row are the percentage of studies \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 16, 2024. ; https://doi.org/10.1101/2024.04.15.24305869doi: medRxiv preprint \n \n11 \nin which a specific dimension was evaluated and the percentages on the last column indicate the percentage of \nstudies in which a specific healthcare task or NLP/NLU task was evaluated. \n \n \n4.5 Distribution of studies by medical specialty  \n \nWe categorized studies according to the Accreditation Council for Graduate Medical Education \n(ACGME) residency programs, augmented to include additional categories to capture studies \ninvestigating applications in dental specialties, treatment of genetic disorders and generic \nhealthcare applications 46. Notably, over a fifth of the studies were categorized as generic, \nindicating a significant focus on healthcare applications that are relevant to many specialties, \nrather than a specific specialty. Among the specialties, internal medicine, surgery, and \nophthalmology were the top specialties. Nuclear medicine, physical medicine, and medical \ngenetics were the least prevalent specialties in studies, accounting for 12 studies in total. The \nexact percentage of studies in different specialties are outlined in eTable 2. The distribution of \nstudies across specialties underscores the potential for LLMs to contribute to a wide range of \nmedical specialties, but also signals opportunities for further exploration within less represented \nareas such as nuclear medicine, physical medicine, and medical genetics. \n \n5. Discussion  \nOur systematic review of 519 studies summarizes existing evaluations of LLMs across medical \nspecialties. Studies ranged widely in the underlying healthcare task, NLP/NLU task, and \ndimension of evaluation. Based on the results, we identified six limitations in the current efforts \nand suggest how to address them in future. These limitations demonstrate an urgent need to \ndevelop nationwide consensus-driven guidance for evaluating LLMs in medicine, in a manner \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 16, 2024. ; https://doi.org/10.1101/2024.04.15.24305869doi: medRxiv preprint \n \n12 \nsimilar to the creation of the blueprint for trustworthy AI by The Coalition for Health AI for \ntraditional AI models47. \n \nThe need for evaluations based on real patient care data \n \nOne striking finding is that only 5% of the studies used real patient care data for evaluation, with \nmost studies using a mix of medical exam questions, patient vignettes and subject matter expert \ngenerated questions 48 49 50. Our recent JAMA special communication pointed out that testing \nLLMs with hypothetical medical questions is like assessing a car's performance with multiple-\nchoice questions before certifying it for road use51. Real patient care data encompasses the \ncomplexities of clinical practice, providing a more thorough evaluation of LLM performance that \nwill closely mirror real-world performance 52 53 54 55.  \nReal-world LLM evaluations provide valuable insights that may be overlooked in simulations or \nsynthetic environments. For instance, while LLMs have been touted for potentially saving time \nand enhancing clinician experience, Garcia et al. found that the mean utilization rate for drafting \npatient messaging responses in an EHR system was only 20%, resulting in a reduction in \nburnout score but no time savings 56. \nGiven the importance of using real patient care data, systems need to be created to ensure their \nuse in evaluating LLMs’ healthcare applications. The Office of the National Coordinator for \nHealth Information Technology (ONC) recently passed HT-1, the first federal regulation to set \nspecific reporting requirements for developers of AI tools57. ONC and other regulators should \nlook to embed a mandate for the use of patient care data in the evaluation process of LLM tools \ninto its requirements. \n \nThe need to standardize the task formulations and dimensions of evaluation \n \nThere is a lack of consensus on which dimensions of evaluation to examine for a given \nhealthcare task or NLP/NLU task. For instance, for a medical education task, Ali et al. tested the \nperformance of GPT-4 on a written board examination focusing on output accuracy as the sole \ndimension 58. Another study tested the performance of ChatGPT on the USMLE, focusing on \noutput accuracy, factuality and comprehensiveness as primary dimensions of evaluation 59. \n \nTo address this challenge, we need to establish shared definitions of tasks and corresponding \ndimensions of evaluation. Similar to how efforts such as Holistic Evaluation of Language Models \n(HELM) define the dimensions of evaluation of an LLM that matter in general, a framework \nspecific for healthcare is necessary to define the core dimensions of evaluation to be assessed \nacross studies. Doing so enables better comparisons and cumulative learning from which \nreliable conclusions can be drawn for future technical work and policy guidance. \n \nPrioritize immediately impactful, administrative applications \n \nCurrent research predominantly focuses on medical knowledge tasks, such as answering \nmedical exam questions (44.5%), or complex healthcare tasks, as well as making diagnoses \n(19.5%) and making treatment recommendations (9.2%). However, there are many \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 16, 2024. ; https://doi.org/10.1101/2024.04.15.24305869doi: medRxiv preprint \n \n13 \nadministrative tasks in healthcare that are often labor-intensive, requiring manual input and \ncontributing to physician burnout 60. Particularly, areas such as assigning provider billing codes \n(1 study), writing prescriptions (1 study), generating clinical referrals (3 studies), and clinical \nnote-taking (4 studies); all of which remain under-researched and could greatly benefit from a \nsystematic evaluation of using LLMs for those tasks 61 62 63 64.  \nThe need to bridge gaps in LLM utilization across clinical specialties \nThe substantial representation of generic healthcare applications, accounting for over a fifth of \nthe studies, underscores the potential of LLMs in addressing needs applicable to many \nspecialties, such as summarizing medical reports. In contrast, the scarcity of research in \nparticular specialties like nuclear medicine (3 studies), physical medicine (2 studies), and \nmedical genetics (1 study) suggests an untapped potential for using LLMs in these complex \nmedical domains that often present intricate diagnostic challenges and demand personalized \ntreatment approaches65 66 67 68. The lack of LLM-focused studies in these areas may indicate the \nneed for increased awareness, collaboration, or specialized adaptation of such models to suit \nthe unique demands of these specialties. \nThe need for a realistic accounting of financial impact \n \nGenerative AI is projected to create $200 billion to $360 billion in healthcare cost savings \nthrough productivity improvements 69.  However, the implementation of these tools could pose a \nsignificant financial burden to health systems. In a recent review by Sahni and Carrus, defining \nthe cost and benefit of deploying AI was highlighted as one of the greatest challenges 70. It is \nkey for health systems to capture this, to accurately estimate and budget for increased \nimplementation and computing costs 71. \n \nWithin this review, only one study conducted a financial impact or cost-effectiveness analysis. \nRau et al. investigated the use of ChatGPT to develop personalized imaging, demonstrating \"an \naverage decision time of 5 minutes and a cost of €0.19 for all cases, compared to 50 minutes \nand €29.99 for radiologists\" 72. However, this analysis was a parallel implementation of the LLM \nsolution compared with the traditional radiologist approach, thus not providing a realistic \nassessment of the added value of LLM integration into existing clinical workflows and its \ncorresponding financial impact. \n \nWhile the dearth of real-world testing is understandable given the infancy of LLM applications in \nhealthcare, it is imperative to establish realistic assessments of these tools before reallocating \nresources from other healthcare initiatives. Notably, such assessments should estimate the total \ncost of implementation, which includes not only the cost to run the model but also expenses \nassociated with monitoring, maintenance, and any necessary infrastructure adjustments. \n \nThe need to better define and quantify bias  \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 16, 2024. ; https://doi.org/10.1101/2024.04.15.24305869doi: medRxiv preprint \n \n14 \nRecent studies have highlighted a concerning trend of LLMs perpetuating race-based medicine \nin their responses 73. This phenomenon can be attributed to the tendency of LLMs to reproduce \ninformation from their training data, which may contain human biases 74.To improve our \nmethods for evaluating and quantifying bias, we need to first collectively establish what it means \nto be unbiased.  \n \nWhile efforts to assess racial and ethical biases exist, only 15.8% of studies have conducted \nany evaluation that delves into how factors such as race, gender, or age impact bias in the \nmodel's output 75 76 77. Future research should place greater emphasis on such evaluations, \nparticularly as policymakers develop best practices and guidance for model assurance. \nMandating these evaluations as part of a “model report card” could be a proactive step towards \nmitigating harmful biases perpetuated by LLMs 78.  \n \nThe need to publicly report failure modes \n \nThe analysis of failure modes has long been regarded as fundamental in engineering and \nquality management, facilitating the identification, examination, and subsequent mitigation of \nfailures79. The FDA has databases for adverse event reporting in pharmaceuticals and medical \ndevices, but there is currently no analogous place for reporting failure modes for AI systems, let \nalone LLMs, in healthcare 80 81. \n \nIn the ‘Conclusion’ sections of many studies, only a select few researched why the deployment \nof the LLM did not produce satisfactory results (e.g. ineffective prompt engineering) 82. A deeper \nexamination of failure modes and why the exercise was deemed unsuccessful or inaccurate \n(e.g. the reference data was factually incorrect or outdated), is necessary to further improve the \nuse of LLMs in healthcare settings. \n6. Conclusion  \n \nThe evaluation of LLMs lacks standardized task definitions and dimensions of evaluation. This \nsystematic review underscores the need for evaluating LLMs using real patient care data, \nparticularly on administrative healthcare tasks like generating provider billing codes, writing \nprescriptions, and clinical note-taking. It highlights the need to expand testing criteria beyond \naccuracy to include fairness, bias, toxicity, robustness, and deployment considerations across \ndifferent medical specialties. Establishing shared task definitions and rigorous testing and \nevaluation standards are crucial for the safe integration of LLMs in healthcare. Realistic financial \naccounting and robust reporting of failures are essential to accurately assess their value and \nsafety in clinical settings. Broadly, there is an urgent need to develop a nationwide consensus \nand guidance for evaluating LLMs in healthcare, so that we may realize the tremendous \npromise these groundbreaking technologies have to offer. \n \nAuthor contributions: SB, YL, LOE, NHS conceived of the study, defined the main outcomes \nand measures. SB, LOE, YL searched the literature to identify the publications to review and \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 16, 2024. ; https://doi.org/10.1101/2024.04.15.24305869doi: medRxiv preprint \n \n15 \ncategorized the publications. SB, LOE, YL and NHS drafted the manuscript. SB designed the \nGPT-4 based screening strategy with input from DD. SB developed the NLP/NLU task and \ndimensions of evaluation framework. LOE developed the healthcare task framework. DD guided \nthe creation and categorization of healthcare tasks. AC and AS guided the creation of NLP/NLU \ntask categorization. JAF guided the creation of the dimensions of evaluation categorization, SK \nhelped select HELM dimensions to reuse. MK refined the medical specialty categorization, and \nMW critiqued the review methodology and figure organization. LL and HH assessed the \nusefulness of the frameworks for other analyses. NRS guided LOE and YL on all aspects of \nperforming systematic reviews. AM reviewed and edited the manuscript for framing the \ndiscussion. KS and TT assessed the relevance of the results for developing consensus LLM \ntesting and evaluation guidance for CHAI. MAP critiqued the deployment concerns in health \nsystems and reviewed the categories.  All authors reviewed, edited and approved of the final \nmanuscript.  \n \nAcknowledgements: We thank Nicholas Chedid for extensive guidance in the development of \nthe healthcare task categorization.  \nSupplement 1. Search terms for PubMed as of 02/19/2024 \n \n((\"Large Language Model\" [Title/Abstract] OR \"ChatGPT\" [Title/Abstract] OR \"Generative AI\" \n[Title/Abstract]) AND (\"Health\" [Title/Abstract] OR \"Medical\" [Title/Abstract] OR \"Clinical\" \n[Title/Abstract] OR \"Medicine\" [Title/Abstract]) AND (\"Test\" [Title/Abstract] OR \"Evaluate\" \n[Title/Abstract] OR \"Performance\" [Title/Abstract] OR \"Assess\" [Title/Abstract])) \n \nSearch terms for Web of science as of 02/19/2024 \n \n(TS=(\"Large Language Model\" OR \"ChatGPT\" OR \"Generative AI\") \nAND \nTS=(\"Health\" OR \"Medical\" OR \"Clinical\" OR \"Medicine\") \nAND \nTS=(\"Test\" OR \"Evaluate\" OR \"Performance\" OR \"Assess\")) \n \nSupplement 2. Prompts used to extract and assign categories for human \nreview \nPrompt 1 \n\"You are assisting in a systematic review of large language models in healthcare. Summarize \nthe {entity_type} mentioned in this research abstract in 25 words\" \n \nPrompt 2 \n“Using the generated summaries, identify and categorize the following text based on \n{entity_type}:\\n\\n{text}\\n\\nCategories:” \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 16, 2024. ; https://doi.org/10.1101/2024.04.15.24305869doi: medRxiv preprint \n \n16 \nWhere entity_type can be NLP task, medical specialty or metric and categories is the list of \npossible values for each entity_type to make categorization into, for the NLP task, metric and \nmedical specialty. \neFigure 1 - Examples of metrics for each dimension of evaluation \nThe first row represents the names of the dimensions of evaluation in our designed framework. Under each \ndimension there are metrics. The bold italicized cells represent metric subclasses for each dimension and regular font \ncells under each subclass represent the metrics. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 16, 2024. ; https://doi.org/10.1101/2024.04.15.24305869doi: medRxiv preprint \n \n17 \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 16, 2024. ; https://doi.org/10.1101/2024.04.15.24305869doi: medRxiv preprint \n \n18 \neTable 2 - Frequency of publications by medical specialty \nThis table shows the different medical specialties of the 519 studies, along with three additional categories: Generic, \nDentistry, and Medical Genetics \n \n \n \n1 Stafie CS, Sufaru IG, Ghiciuc CM et al. Exploring the Intersection of Artificial Intelligence and Clinical Healthcare: A \nMultidisciplinary Review. Diagnostics. 2023;13(12):1995. doi:https://doi.org/10.3390/diagnostics13121995 \n2 Kohane IS. Injecting Artificial Intelligence into Medicine. NEJM AI. 2024;1(1). doi:https://doi.org/10.1056/aie2300197 \n3 Goldberg CB, Adams L, Blumenthal D et al. To Do No Harm — and the Most Good — with AI in Health Care. NEJM AI. 2024;1(3). \ndoi:https://doi.org/10.1056/aip2400036 \n4 Wachter RM, Brynjolfsson E. Will Generative Artificial Intelligence Deliver on Its Promise in Health Care? JAMA. 2024;331(1):65-\n69. doi:https://doi.org/10.1001/jama.2023.25054 \n5 Liu Y, Zhang K, Li Y, et. al., Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models. \narXiv preprint arXiv:2402.17177. 2024 Feb 27 \n6 Karabacak M, Margetis K. Embracing Large Language Models for Medical Applications: Opportunities and Challenges. Cureus. \n2023 May 21;15(5):e39305. doi: 10.7759/cureus.39305. PMID: 37378099; PMCID: PMC10292051 \n7 Landi H. Abridge clinches $150M to build out generative AI for medical documentation. Fierce Healthcare. Published February \n23rd 2024. https://www.fiercehealthcare.com/ai-and-machine-learning/abridge-clinches-150m-build-out-generative-ai-medical-\ndocumentation \n8 Webster P. Six ways large language models are changing healthcare. Nat Med. 2023;29(12):2969-2971. \ndoi:https://doi.org/10.1038/s41591-023-02700-1 \n9 Bhasker S, Bruce D, Lamb J et al. Tackling healthcare’s biggest burdens with generative AI. McKinsey. www.mckinsey.com. \nPublished July 10, 2023. https://www.mckinsey.com/industries/healthcare/our-insights/tackling-healthcares-biggest-burdens-with-\ngenerative-ai \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 16, 2024. ; https://doi.org/10.1101/2024.04.15.24305869doi: medRxiv preprint \n \n19 \n \n10 Sahni NR, Stein G, Zemmel R, Cutler D. The Potential Impact of Artificial Intelligence on Health Care Spending. National Bureau \nof Economic Research. Published January 1, 2023. Accessed March 26, 2024.  \n11 Shah NH, Entwistle D, Pfeffer MA. Creation and Adoption of Large Language Models in Medicine. JAMA. 2023;330(9):866-869. \ndoi:10.1001/jama.2023.14217  \n12 Wornow M, Xu Y, Thapa R, et al. The shaky foundations of large language models and foundation models for electronic health \nrecords. NPJ Digit Med. 2023;6(1):135. Published 2023 Jul 29. doi:10.1038/s41746-023-00879-8  \n13 Cadamuro J, Cabitza F, Debeljak Z, et al. Potentials and pitfalls of ChatGPT and natural-language artificial intelligence models for \nthe understanding of laboratory medicine test results. An assessment by the European Federation of Clinical Chemistry and \nLaboratory Medicine (EFLM) Working Group on Artificial Intelligence (WG-AI). Clin Chem Lab Med. 2023;61(7):1158-1166. \nPublished 2023 Apr 24. doi:10.1515/cclm-2023-0355 \n14 Pagano S, Holzapfel S, Kappenschneider T, et al. Arthrosis diagnosis and treatment recommendations in clinical practice: an \nexploratory investigation with the generative AI model GPT-4. J Orthop Traumatol. 2023;24(1):61. Published 2023 Nov 28. \ndoi:10.1186/s10195-023-00740-4 \n15 Page MJ, McKenzie JE, Bossuyt PM et al. The PRISMA 2020 statement: an Updated Guideline for Reporting Systematic \nReviews. British Medical Journal. 2021;372(71). doi:https://doi.org/10.1136/bmj.n71 \n16 USMLE Physician Tasks/Competencies. 2020. https://www.usmle.org/sites/default/files/2021-\n08/USMLE_Physician_Tasks_Competencies.pdf \n17 Norden J, Wang J, Bhattacharyya A. Where Generative AI Meets Healthcare: Updating The Healthcare AI Landscape. AI \nCheckup. Published June 22, 2023. https://aicheckup.substack.com/p/where-generative-ai-meets-healthcare \n18 Liang P, Bommasani R, Lee T et al. Holistic Evaluation of Language Models. Transactions on Machine Learning Research. \nPublished online February 1, 2023. Accessed February 2024.  https://openreview.net/forum?id=iO4LZibEqW \n19 Tasks - Hugging Face. huggingface.co. https://huggingface.co/tasks \n20 Residency & Fellowship Programs. Graduate Medical Education. https://med.stanford.edu/gme/programs.html \n21 Ali R, Tang OY, Connolly ID, et al. Performance of CHATGPT and GPT-4 on Neurosurgery Written Board Examinations. \nPublished online March 29, 2023. doi:10.1101/2023.03.25.23287743 \n22 Fraser H, Crossland D, Bacher I, Ranney M, Madsen T, Hilliard R. Comparison of diagnostic and triage accuracy of Ada Health \nand WebMD Symptom Checkers, CHATGPT, and physicians for patients in an emergency department: Clinical Data \nAnalysis Study. JMIR mHealth and uHealth. 2023;11. doi:10.2196/49995 \n23 Babayiğit O, Tastan Eroglu Z, Ozkan Sen D, Ucan Yarkac F. Potential use of CHATGPT for patient information in Periodontology: \nA descriptive pilot study. Cureus. Published online November 8, 2023. doi:10.7759/cureus.48518 \n24 Wilhelm TI, Roos J, Kaczmarczyk R. Large language models for therapy recommendations across 3 clinical specialties: \nComparative study. Journal of Medical Internet Research. 2023;25. doi:10.2196/49324 \n25 Srivastava R, Srivastava S. Can Artificial Intelligence Aid Communication? considering the possibilities of GPT-3 in palliative care. \nIndian Journal of Palliative Care. 2023;29:418-425. doi:10.25259/ijpc_155_2023 \n26 Dağcı M, Çam F, Dost A. Reliability and quality of the nursing care planning texts generated by CHATGPT. Nurse Educator. \nPublished online November 22, 2023. doi:10.1097/nne.0000000000001566 \n27 Huh S. Are chatgpt’s knowledge and interpretation ability comparable to those of medical students in Korea for taking a \nparasitology examination?: A descriptive study. Journal of Educational Evaluation for Health Professions. 2023;20:1. \ndoi:10.3352/jeehp.2023.20.1 \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 16, 2024. ; https://doi.org/10.1101/2024.04.15.24305869doi: medRxiv preprint \n \n20 \n \n28 Suppadungsuk S, Thongprayoon C, Krisanapan P, et al. Examining the validity of chatgpt in identifying relevant nephrology \nliterature: Findings and implications. Journal of Clinical Medicine. 2023;12(17):5550. doi:10.3390/jcm12175550 \n29 Rao A, Kim J, Kamineni M, Pang M, Lie W, Succi MD. Evaluating chatgpt as an adjunct for radiologic decision-making. medRxiv. \nPublished online February 7, 2023. doi:10.1101/2023.02.02.23285399 \n30 Barash Y, Klang E, Konen E, Sorin V. CHATGPT-4 assistance in Optimizing Emergency Department radiology referrals and \nImaging Selection. Journal of the American College of Radiology. 2023;20(10):998-1003. doi:10.1016/j.jacr.2023.06.009 \n31 Chung EM, Zhang SC, Nguyen AT, Atkins KM, Sandler HM, Kamrava M. Feasibility and acceptability of CHATGPT generated \nradiology report summaries for cancer patients. DIGITAL HEALTH. 2023;9. doi:10.1177/20552076231221620 \n32 Groza T, Caufield H, Gration D, et al. An evaluation of GPT models for phenotype concept recognition. BMC Medical Informatics \nand Decision Making. 2024;24(1). doi:10.1186/s12911-024-02439-w \n33 Razdan S, Siegal AR, Brewer Y, Sljivich M, Valenzuela RJ. Assessing chatgpt’s ability to answer questions pertaining to erectile \ndysfunction: Can our patients trust it? International Journal of Impotence Research. Published online November 20, 2023. \ndoi:10.1038/s41443-023-00797-z \n34 Kassab J, Hadi El Hajjar A, Wardrop RM, Brateanu A. Accuracy of online artificial intelligence models in Primary Care Settings. \nAmerican Journal of Preventive Medicine. Published online February 2024. doi:10.1016/j.amepre.2024.02.006 \n35 Lim B, Seth I, Dooreemeah D, Lee CH. Delving into new frontiers: Assessing chatgpt’s proficiency in revealing uncharted \ndimensions of general surgery and pinpointing innovations for future advancements. Langenbeck’s Archives of Surgery. \n2023;408(1). doi:10.1007/s00423-023-03173-z \n36 Lossio-Ventura JA, Weger R, Lee AY, et al. A comparison of CHATGPT and fine-tuned open pre-trained transformers (OPT) \nagainst widely used sentiment analysis tools: Sentiment analysis of COVID-19 survey data. JMIR Mental Health. 2024;11. \ndoi:10.2196/50150 \n37 Chen Q, Sun H, Liu H, et al. An extensive benchmark study on biomedical text generation and mining with chatgpt. \nBioinformatics. 2023;39(9). doi:10.1093/bioinformatics/btad557 \n38 Wang H, Gao C, Dantona C, Hull B, Sun J. DRG-Llama : Tuning llama model to predict diagnosis-related group for hospitalized \npatients. npj Digital Medicine. 2024;7(1). doi:10.1038/s41746-023-00989-3 \n39 Aiumtrakul N, Thongprayoon C, Arayangkool C, et al. Personalized medicine in urolithiasis: AI chatbot-assisted dietary \nmanagement of oxalate for Kidney Stone Prevention. Journal of Personalized Medicine. 2024;14(1):107. \ndoi:10.3390/jpm14010107 \n40 Gan RK, Ogbodo JC, Wee YZ, Gan AZ, González PA. Performance of Google bard and ChatGPT in mass casualty incidents \ntriage. Am J Emerg Med. 2024;75:72-78. doi:10.1016/j.ajem.2023.10.034  \n41 USMLE Physician Tasks/Competencies. 2020. https://www.usmle.org/sites/default/files/2021-\n08/USMLE_Physician_Tasks_Competencies.pdf \n42 Norden J, Wang J, Bhattacharyya A. Where Generative AI Meets Healthcare: Updating The Healthcare AI Landscape. AI \nCheckup. Published June 22, 2023. https://aicheckup.substack.com/p/where-generative-ai-meets-healthcare \n43 Liang P, Bommasani R, Lee T et al. Holistic Evaluation of Language Models. Transactions on Machine Learning Research. \nPublished online February 1, 2023. Accessed February 2024.  https://openreview.net/forum?id=iO4LZibEqW \n44 Tasks - Hugging Face. huggingface.co. https://huggingface.co/tasks \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 16, 2024. ; https://doi.org/10.1101/2024.04.15.24305869doi: medRxiv preprint \n \n21 \n \n45 Heuer AJ. More Evidence That the Healthcare Administrative Burden Is Real, Widespread and Has Serious Consequences; \nComment on \"Perceived Burden Due to Registrations for Quality Monitoring and Improvement in Hospitals: A Mixed \nMethods Study\". Int J Health Policy Manag. 2022;11(4):536-538. doi:https://doi.org/10.34172/ijhpm.2021.129 \n46 Residency & Fellowship Programs. Graduate Medical Education. https://med.stanford.edu/gme/programs.html \n47 Coalition for Health AI. Blueprint for Trustworthy AI Implementation Guidance and Assurance for Healthcare. Published April 4th \n2023. https://coalitionforhealthai.org/papers/blueprint-for-trustworthy-ai_V1.0.pdf \n48 Savage T, Wang J, Shieh L. A Large Language Model Screening Tool to Target Patients for Best Practice Alerts: Development \nand Validation. JMIR Med Inform. 2023 Nov 27;11:e49886. doi: 10.2196/49886. \n49 Pagano S, Holzapfel S, Kappenschneider T, et al. Arthrosis diagnosis and treatment recommendations in clinical practice: an \nexploratory investigation with the generative AI model GPT-4. J Orthop Traumatol. 2023;24(1):61. Published 2023 Nov 28. \ndoi:10.1186/s10195-023-00740-4 \n50 Surapaneni KM. Assessing the Performance of ChatGPT in Medical Biochemistry Using Clinical Case Vignettes: Observational \nStudy. JMIR Med Educ. 2023 Nov 7;9:e47191. doi: 10.2196/47191. \n51 Shah NH, Entwistle D, Pfeffer MA. Creation and Adoption of Large Language Models in Medicine. JAMA. 2023;330(9):866-869. \ndoi:10.1001/jama.2023.14217 \n52 Pagano S, Holzapfel S, Kappenschneider T et al. Arthrosis diagnosis and treatment recommendations in clinical practice: an \nexploratory investigation with the generative AI model GPT-4. J Orthop Traumatol. 2023;24(1):61. \ndoi:https://doi.org/10.1186/s10195-023-00740-4 \n53 Choi HS, Song JY, Shin KH, Chang JH et al. Developing prompts from large language model for extracting clinical information \nfrom pathology and ultrasound reports in breast cancer. Radiat Oncol J. 2023;41(3):209-216. \ndoi:https://doi.org/10.3857/roj.2023.00633 \n54 Fleming SL, Lozano A, Haberkorn WJ et al. MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic \nMedical Records. Dec 2023. arXiv:2308.14089; https://doi.org/10.48550/arXiv.2308.14089. \n55 Karabacak M, Margetis K. Embracing Large Language Models for Medical Applications: Opportunities and Challenges. Cureus. \n2023;15(5):e39305. Published online May 21 2023. doi:https://doi.org/10.7759/cureus.39305 \n56 Garcia P, Ma SP, Shah S et al. Artificial Intelligence–Generated Draft Replies to Patient Inbox Messages. JAMA Netw Open. \n2024;7(3):e243201. doi:https://doi.org/10.1001/jamanetworkopen.2024.3201 \n57 Office of the National Coordinator for Health Information Technology. Health Data, Technology, and Interoperability: Certification \nProgram Updates, Algorithm Transparency, and Information Sharing. Federal Register. January 9, 2024;89(6):[page \nnumbers]. Available from: Federal Register. \n58 Ali R, Tang OY, Connolly ID et al. Performance of ChatGPT and GPT-4 on Neurosurgery Written Board Examinations. \nNeurosurgery. 2023;93(6):1353-1365. doi:https://doi.org/10.1227/neu.0000000000002632 \n59 Gilson A, Safranek CW, Huang T et al. How Does ChatGPT Perform on the United States Medical Licensing Examination? The \nImplications of Large Language Models for Medical Education and Knowledge Assessment [published correction appears in \nJMIR Med Educ. 2024 Feb 27;10:e57594]. JMIR Med Educ. 2023;9:e45312. Published Feb 8 2023. \ndoi:https://doi.org/10.2196/45312  \n60 Heuer AJ. More Evidence That the Healthcare Administrative Burden Is Real, Widespread and Has Serious Consequences; \nComment on \"Perceived Burden Due to Registrations for Quality Monitoring and Improvement in Hospitals: A Mixed \nMethods Study\". Int J Health Policy Manag. 2022;11(4):536-538. doi:https://doi.org/10.34172/ijhpm.2021.129 \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 16, 2024. ; https://doi.org/10.1101/2024.04.15.24305869doi: medRxiv preprint \n \n22 \n \n61 Wang H, Gao C, Dantona C et al. DRG-LLaMA : tuning LLaMA model to predict diagnosis-related group for hospitalized patients. \nNPJ Digit Med. 2024;7(1):1-9. doi:https://doi.org/10.1038/s41746-023-00989-3  \n62 Aiumtrakul N, Thongprayoon C, Arayangkool C et al. Personalized Medicine in Urolithiasis: AI Chatbot-Assisted Dietary \nManagement of Oxalate for Kidney Stone Prevention. J Pers Med. 2024;14(1):107. doi:https://doi.org/10.3390/jpm14010107  \n63 Heston TF. Safety of Large Language Models in Addressing Depression. Cureus. 2023;15(12):e50729. \ndoi:https://doi.org/10.7759/cureus.50729  \n64 Pushpanathan K, Lim ZW, Er Yew SM et al. Language Model Chatbots’ Accuracy, Comprehensiveness, and Self-Awareness in \nAnswering Ocular Symptom Queries. iScience. 2023;26(11):108163. doi:https://doi.org/10.1016/j.isci.2023.108163 \n65 Currie G, Barry K. ChatGPT in Nuclear Medicine Education. July 2023. J Nucl Med Technol. 2023 Sep;51(3):247-254. \ndoi:https://doi.org/10.2967/jnmt.123.265844  \n66 Zhang L, Tashiro S, Mukaino M et al. Use of artificial intelligence large language models as a clinical tool in rehabilitation \nmedicine: a comparative test case. September 2023. J Rehabil Med. 2023;55:jrm13373-jrm13373. \ndoi:https://doi.org/10.2340/jrm.v55.13373 \n67 Walton N, Gracefo S, Sutherland N et al. Evaluating ChatGPT as an Agent for Providing Genetic Education. bioRxiv (Cold Spring \nHarbor Laboratory). Published online October 29, 2023. doi:https://doi.org/10.1101/2023.10.25.564074 \n68 Chin HL, Goh DLM. Pitfalls in clinical genetics. Singapore Med J. 2023;64(1):53-58. \ndoi:https://doi.org/10.4103/singaporemedj.smj-2021-329 \n69 Sahni NR, Stein G, Zemmel R, Cutler D. The Potential Impact of Artificial Intelligence on Health Care Spending. National Bureau \nof Economic Research. Published January 1, 2023. Accessed March 26, 2024. \n70 Sahni NR, Carrus B. Artificial Intelligence in U.S. Health Care Delivery. July 2023. The New England Journal of Medicine. \n2023;389(4):348-358. doi:https://doi.org/10.1056/nejmra2204673 \n71 Jindal JA, Lungren MP, Shah NH. Ensuring useful adoption of generative artificial intelligence in healthcare. J Am Med Inform \nAssoc. Published online March 7, 2024. doi:https://doi.org/10.1093/jamia/ocae043 \n72 Rau A, Rau S, Zoeller D et al. A Context-based Chatbot Surpasses Trained Radiologists and Generic ChatGPT in Following the \nACR Appropriateness Guidelines. Radiology. July 2023;308(1). doi:https://doi.org/10.1148/radiol.230970 \n73 Omiye JA, Lester JC, Spichak S et al. Large language models propagate race-based medicine. NPJ Digit Med. October 2023; \n6(1):195. doi:https://doi.org/10.1038/s41746-023-00939-z \n74 Acerbi A, Stubbersfield JM. Large language models show human-like content biases in transmission chain experiments. Proc \nNatl Acad Sci U S A. 2023;120(44):e2313790120. doi:https://doi.org/10.1073/pnas.2313790120 \n75 Guleria A, Krishan K, Sharma V et al. ChatGPT: ethical concerns and challenges in academics and research. September 2023. J \nInfect Dev Ctries. 2023;17:1292–1299. doi:https://doi.org/10.3855/jidc.18738 \n76 Hanna JJ, Wakene AD, Lehmann CU et al. Assessing Racial and Ethnic Bias in Text Generation for Healthcare-Related Tasks \nby ChatGPT. medRxiv (Cold Spring Harbor Laboratory). Published online August 28, 2023. \ndoi:https://doi.org/10.1101/2023.08.28.23294730 \n77 Levkovich I, Elyoseph Z. Suicide Risk Assessments Through the Eyes of ChatGPT-3.5 Versus ChatGPT-4: Vignette Study. JMIR \nMental Health. September 2023;10(1):e51232. doi:https://doi.org/10.2196/51232 \n78 Heming C, Abdalla M, Ahluwalia M et al. Benchmarking Bias: Expanding Clinical AI Model Card to Incorporate Bias Reporting of \nSocial and Non-Social Factors. Accessed March 2024. https://arxiv.org/pdf/2311.12560.pdf \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 16, 2024. ; https://doi.org/10.1101/2024.04.15.24305869doi: medRxiv preprint \n \n23 \n \n79 Thomas, D. Revolutionizing Failure Modes and Effects Analysis with ChatGPT: Unleashing the Power of AI Language Models. J \nFail. Anal. and Preven. May 2023;23(3):911–913. https://doi.org/10.1007/s11668-023-01659-y \n80 Research C for DE and. FDA Adverse Event Reporting System (FAERS) Public Dashboard. FDA. Published online October 29, \n2020. https://www.fda.gov/drugs/questions-and-answers-fdas-adverse-event-reporting-system-faers/fda-adverse-event-\nreporting-system-faers-public-dashboard \n81 MAUDE - Manufacturer and User Facility Device Experience. Fda.gov. Published 2012. \nhttps://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfmaude/search.cfm \n82 Galido PV, Butala S, Chakerian M et al. A Case Study Demonstrating Applications of ChatGPT in the Clinical Management of \nTreatment-Resistant Schizophrenia . Cureus. Published online April 26, 2023; 15(4): e38166. \ndoi:https://doi.org/10.7759/cureus.38166 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 16, 2024. ; https://doi.org/10.1101/2024.04.15.24305869doi: medRxiv preprint ",
  "topic": "Health care",
  "concepts": [
    {
      "name": "Health care",
      "score": 0.6220634579658508
    },
    {
      "name": "Systematic review",
      "score": 0.485219269990921
    },
    {
      "name": "Computer science",
      "score": 0.3295339345932007
    },
    {
      "name": "Political science",
      "score": 0.27324002981185913
    },
    {
      "name": "Economics",
      "score": 0.21704018115997314
    },
    {
      "name": "MEDLINE",
      "score": 0.21589398384094238
    },
    {
      "name": "Economic growth",
      "score": 0.12422999739646912
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}