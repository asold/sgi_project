{
  "title": "Multilanguage Transformer for Improved Text to Remote Sensing Image Retrieval",
  "url": "https://openalex.org/W4313121711",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221417649",
      "name": "Mohamad M. Al Rahhal",
      "affiliations": [
        "King Saud University"
      ]
    },
    {
      "id": "https://openalex.org/A63495556",
      "name": "Yakoub Bazi",
      "affiliations": [
        "King Saud University"
      ]
    },
    {
      "id": "https://openalex.org/A3113448708",
      "name": "Norah A. Alsharif",
      "affiliations": [
        "King Saud University"
      ]
    },
    {
      "id": "https://openalex.org/A2790707150",
      "name": "Laila Bashmal",
      "affiliations": [
        "King Saud University"
      ]
    },
    {
      "id": "https://openalex.org/A119476720",
      "name": "Naif Alajlan",
      "affiliations": [
        "King Saud University"
      ]
    },
    {
      "id": "https://openalex.org/A104185088",
      "name": "Farid Melgani",
      "affiliations": [
        "University of Trento"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2922388164",
    "https://openalex.org/W3093905312",
    "https://openalex.org/W1989316905",
    "https://openalex.org/W2091280333",
    "https://openalex.org/W2981537605",
    "https://openalex.org/W3038038411",
    "https://openalex.org/W3196922338",
    "https://openalex.org/W3117344638",
    "https://openalex.org/W3168972675",
    "https://openalex.org/W3012111773",
    "https://openalex.org/W2808376087",
    "https://openalex.org/W2938495304",
    "https://openalex.org/W3013317997",
    "https://openalex.org/W3004827100",
    "https://openalex.org/W2735345282",
    "https://openalex.org/W3016821823",
    "https://openalex.org/W2898008292",
    "https://openalex.org/W2997786074",
    "https://openalex.org/W3013436746",
    "https://openalex.org/W2987489329",
    "https://openalex.org/W3133500032",
    "https://openalex.org/W3004137323",
    "https://openalex.org/W3047058320",
    "https://openalex.org/W3111501160",
    "https://openalex.org/W3140792177",
    "https://openalex.org/W3165084071",
    "https://openalex.org/W3208803664",
    "https://openalex.org/W6791924259",
    "https://openalex.org/W6790570412",
    "https://openalex.org/W3083147600",
    "https://openalex.org/W4285244413",
    "https://openalex.org/W3138697998",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6791353385",
    "https://openalex.org/W6774314701",
    "https://openalex.org/W2779054585",
    "https://openalex.org/W2510520237",
    "https://openalex.org/W1980038761",
    "https://openalex.org/W6747225742",
    "https://openalex.org/W2962964995",
    "https://openalex.org/W2981448908",
    "https://openalex.org/W3035422918",
    "https://openalex.org/W3097754216",
    "https://openalex.org/W3137228985",
    "https://openalex.org/W3100245404",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2774267535",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W3128099838",
    "https://openalex.org/W3098351727",
    "https://openalex.org/W3094502228"
  ],
  "abstract": "Cross-modal text-image retrieval in remote sensing (RS) provides a flexible retrieval experience for mining useful information from RS repositories. However, existing methods are designed to accept queries formulated in the English language only, which may restrict accessibility to useful information for non-English speakers. Allowing multilanguage queries can enhance the communication with the retrieval system and broaden access to the RS information. To address this limitation, this article proposes a multilanguage framework based on transformers. Specifically, our framework is composed of two transformer encoders for learning modality-specific representations, the first is a language encoder for generating language representation features from the textual description, while the second is a vision encoder for extracting visual features from the corresponding image. The two encoders are trained jointly on image and text pairs by minimizing a bidirectional contrastive loss. To enable the model to understand queries in multiple languages, we trained it on descriptions from four different languages, namely, English, Arabic, French, and Italian. The experimental results on three benchmark datasets (i.e., RSITMD, RSICD, and UCM) demonstrate that the proposed model improves significantly the retrieval performances in terms of recall compared to the existing state-of-the-art RS retrieval methods.",
  "full_text": "IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022 9115\nMultilanguage Transformer for Improved Text\nto Remote Sensing Image Retrieval\nMohamad M. Al Rahhal , Senior Member, IEEE, Yakoub Bazi , Senior Member, IEEE, Norah A. Alsharif ,\nLaila Bashmal, Graduate Student Member, IEEE, Naif Alajlan , Senior Member, IEEE,\nand Farid Melgani , Fellow, IEEE\nAbstract—Cross-modal text-image retrieval in remote sensing\n(RS) provides a ﬂexible retrieval experience for mining useful\ninformation from RS repositories. However, existing methods are\ndesigned to accept queries formulated in the English language\nonly, which may restrict accessibility to useful information for non-\nEnglish speakers. Allowing multilanguage queries can enhance the\ncommunication with the retrieval system and broaden access to the\nRS information. To address this limitation, this article proposes a\nmultilanguage framework based on transformers. Speciﬁcally, our\nframework is composed of two transformer encoders for learning\nmodality-speciﬁc representations, the ﬁrst is a language encoder\nfor generating language representation features from the textual\ndescription, while the second is a vision encoder for extracting\nvisual features from the corresponding image. The two encoders\nare trained jointly on image and text pairs by minimizing a bidirec-\ntional contrastive loss. To enable the model to understand queries\nin multiple languages, we trained it on descriptions from four dif-\nferent languages, namely, English, Arabic, French, and Italian. The\nexperimental results on three benchmark datasets (i.e., RSITMD,\nRSICD, and UCM) demonstrate that the proposed model improves\nsigniﬁcantly the retrieval performances in terms of recall compared\nto the existing state-of-the-art RS retrieval methods.\nIndex Terms—Contrastive loss, cross-modal retrieval, language\ntransformer, remote sensing, vision transformer.\nI. I NTRODUCTION\nR\nEMOTE sensing (RS) data play a substantial role in ana-\nlyzing geographic phenomena and forecasting the future\nstate of the earth’s surface. In the last several years, RS tech-\nnology has advanced at a breakneck pace [1]. This combined\nwith the increasing number of the launched earth observation\nManuscript received 17 May 2022; revised 10 August 2022 and 10 September\n2022; accepted 11 October 2022. Date of publication 20 October 2022; date of\ncurrent version 1 November 2022. This work was supported by the Researchers\nSupporting Project under Grant RSP-2021/69, King Saud University, Riyadh,\nSaudi Arabia. (Corresponding author: Yakoub Bazi.)\nMohamad M. Al Rahhal is with the Applied Computer Science Department,\nCollege of Applied Computer Science, King Saud University, Riyadh 4545,\nSaudi Arabia (e-mail: mmalrahhal@ksu.edu.sa).\nYakoub Bazi, Norah A. Alsharif, Laila Bashmal, and Naif Alajlan are\nwith the Department of Computer Engineering, College of Computer and\nInformation Sciences, King Saud University, Riyadh 4545, Saudi Arabia\n(e-mail: ybazi@ksu.edu.sa; 441202939@student.ksu.edu.sa; lailabashmal@\noutlook.com; najlan@ksu.edu.sa).\nFarid Melgani is with the Department of Information Engineering and Com-\nputer Science, University of Trento, 38123 Trento, Italy (e-mail: melgani@\ndisi.unitn.it).\nDigital Object Identiﬁer 10.1109/JSTARS.2022.3215803\nsatellites that are constantly monitoring the earth has led to a\nsigniﬁcant growth in the RS image archive.\nTo make full use of such big data, the development of ap-\npropriate and efﬁcient retrieval methods that can deal with RS\narchives in a manageable way is becoming urgently needed.\nThe task of image retrieval, which aims to study how to extract\na speciﬁc image out of a massive amount of data, has received\na great deal of attention recently. The core idea is to narrow\nthe search for the targeted image and retrieve the image that\nmatches a particular query. This task has important value in\nmany practical applications including deforestation detection,\nvisual navigation, and urban planning.\nThe common approach in RS retrieval is the single-modal\nretrieval [2], which accepts an image as a query to match its\ncontent against all the images in the archive. This process\ninvolves extracting representative features from the set of images\nand then, applying a certain measure to quantify the similarity\nbetween the query image and the images in the archive to retrieve\na list of candidate images. Early single-modal methods have\nadopted hand-crafted features to represent the visual content of\nimages [3], [4]. However, these manually designed features are\ninefﬁcient at describing the rich semantic information contained\nin RS images. On the contrary, the developments of deep-\nlearning models such as convolutional neural networks (CNNs),\nhave brought crucial achievements in boosting the accuracy of\nretrieval systems [5] duo to its ability to automatically learn\nhigh-level features from complex RS scenes.\nAlthough single-modal retrieval has been extensively studied\nin the RS domain, it still suffers from a fundamental problem\nin terms of usability. Single-modal retrieval requires the user to\nformulate the query using a preexisting image. This constraint,\nin many cases, can be problematic and impractical as the avail-\nability of an exemplar query is not always guaranteed. Allowing\nthe user to formulate a spoken, written, or even drawn query\ncan give the user more ﬂexibility to describe the content of the\ntargeted image. Hence, developing cross-modal retrieval models\nhas become increasingly important for enhancing the retrieval\nexperience.\nCross-modal retrieval basically aims to let the user search for\ndata in one modality by a query in another modality. Today, as\nwe are witnessing the era of big data, data from various sources\n(e.g., optical, radar, or laser) and a growing number of domains\n(e.g., image, text, and sound) have become available. As a result,\na new category of multimodal applications has emerged, and\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n9116 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nresearchers have started to pay more attention to the interpreta-\ntion tasks that involve multimodality interactions, such as image\ncaptioning [6], [7], [8] and visual question answering [9], [10].\nMotivated by this, RS image retrieval has been explored using\nqueries of different modalities such as images from different\nsources or sensors [11], [12], [13], [14], sketches [15], [16],\nspeech [17], [18], [19], [20], [21], and text [22], [23], [24], [25],\n[26], [27]. Among these modalities, textual descriptions repre-\nsent the most intuitive way of communicating with machines.\nIt can exhaustively describe the vast and complex content of\nthe scene with a few concise words. Therefore, using textual\ninformation for querying RS images can enhance the retrieving\nexperience and bring more ﬂexibility in terms of the query de-\nscription, but at the same time, expressing RS scene using natural\nlanguage introduces new challenges due to the visual-semantic\ndiscrepancy between language and vision worlds.\nIn the literature, only few works have been developed for\ntext-image retrieval [22], [23], [24], [25], [26]. All of these works\nhave been designed to allow English as the primary language of\nthe query. However, the use of English as the sole language\nmay create a barrier for those who are non-English speakers.\nThus, developing a retrieval model that can cross language\nboundaries and process queries in the user’s mother tongue is\nhighly desirable.\nMoreover, although the prior methods of RS text-image re-\ntrieval have achieved promising results, most of the proposed\nmodels rely on CNN for encoding the visual data and recur-\nrent models for encoding the textual data. Yet, these models\nhave shown some limitations in capturing global dependencies\nwithin the input and have been gradually replaced by a self-\nattention-based model known as a transformer. The transformer\nis now considered the state-of-the-art model in natural lan-\nguage processing (NLP), due to its ability to handle long-range\ninterrelationships within the data and hence, providing better\nrepresentations. In addition, transformer-based retrieval models\nhave shown promising results in the context of computer vision\n[28], [29].\nWith these considerations in mind, in this article, we propose a\nmultilanguage text-based retrieval model for RS images where\nthe textual query can be of any of the following languages—\nEnglish, Arabic, French, and Italian. To the best of our knowl-\nedge, no work has addressed the use of multilanguage text for\nquerying RS images. To fully extract representative features\nfrom the vision and language domains, the model employs two\ntransformers, one for visual features and the second for textual\nfeatures. The model aims at exploiting the semantic relation\nbetween the image and the corresponding textual description to\nlearn their representation in a joint embedding space.\nThe main contributions of this article are summarized as\nfollows.\n1) To the best of our knowledge, this is the ﬁrst study in\nRS community that incorporates multilanguage queries to\nperform cross-modal text-image retrieval.\n2) This article proposes a dual transformer-based model for\nbetter learning of the visual and linguistic features from\nthe image and the corresponding captions, respectively.\nThe model is trained with bidirectional contrastive loss\nto encourage the model to correctly align the features of\nthe image and the corresponding text into the same cross-\nmodal space, and thereby improving the performance of\nthe retrieval task.\n3) The proposed model was extensively evaluated on three\nRS text-image datasets using single-language and multi-\nlanguage training settings. The results show that it can\nachieve better performance compared to state-of-the-art\nmethods based on recurrent networks.\nThe rest of this article is organized as follows. Section II\npresents an overview of the state-of-the-art works in the cross-\nmodal RS retrieval. Section III describes the methodology of\nthe multilanguage text-image retrieval model. The experimental\nresults on three benchmark datasets are presented in Section IV.\nFinally, Section V concludes this article.\nII. R ELATED WORK\nMany efforts have been dedicated by the RS community to\nautomate information retrieval from large repositories. The ex-\nisting techniques can be roughly divided into two main streams:\nsingle-modal and cross-modal retrieval methods depending on\nthe type of the query. The single-modal retrieval methods take\na query image as an input and retrieve a list of images that\nare mostly similar to the content of the query image. The other\napproach is cross-modal retrieval, in which the query can be of\nany type of data (e.g., speech or a descriptive sentence). In this\ncase, the semantic concept of the query is extracted and matched\nagainst the visual content of all images in the archive to ﬁnd the\nmost relevant ones. In the following, we present a review of\nexisting works related to cross-modal image retrieval.\nA. Cross-Modal Image-Image Retrieval\nAs a result of the rapid development of RS technologies, the\nnumber of RS images captured by different types of sensors has\nincreased. Consequently, cross-model image-image retrieval has\nreceived widespread attention in recent years.\nCross-modal image-image retrieval allows retrieving the tar-\nget RS imagery by using a query image from different sources\n[11], different sensors [12], [30], or even a sketch image [15],\n[16].\nFor example, Li et al. [11] introduced a cross-source image\nretrieval approach, which utilizes deep hashing CNNs to perform\nimage retrieval. Xiong et al. [13] proposed a cross-source dis-\ncriminative distillation network to elevate the effect of data drift.\nIn [14], a model based on cycle-GAN is proposed to translate\nthe image from one source to the other.\nChaudhuri et al. [12] proposed a method for performing cross-\nmodal retrieval between panchromatic (PAN) and multispectral\nimagery. Xiong et al. [30] proposed a cross-modal hashing\nnetwork for retrieving optical images from synthetic aperture\nradar (SAR) images. The method transforms the optical image\ninto a single channel image and pairs it with the corresponding\nSAR image to train the model.\nIn [15], a multiscale model consisting of CNNs and fully\nconnected layers is proposed for retrieving RS images using\na coarse sketch. Another work [16] proposed a sketch-based\nretrieval model that learns domain-invariant representations by\nadversarial training.\nRAHHAL et al.: MULTILANGUAGE TRANSFORMER FOR IMPROVED TEXT TO REMOTE SENSING IMAGE RETRIEV AL 9117\nB. Cross-Modal Sound-Image Retrieval\nThe goal of cross-modal sound-image retrieval is to leverage\nsound to retrieve relevant RS images. Indeed, using speech as\na query can be efﬁcient in practice as it has a broad range of\napplication scenarios. Some notable works have been published\nvery recently for sound-image retrieval. For example, the authors\nin [18] proposed a triplet network containing a branch for the\nimage, another one for the positive sound, and a branch for the\nnegative sound. Then, the model parameters were learned by\noptimizing a triplet loss. The model integrated hash code learn-\ning to reduce storage costs. In another work [19], a CNN with\nan inception dilated convolution [31], [32] module layer was\nintroduced to extract multiscale contextual information from\nboth images and voices. The model also learns hash code for\nfaster retrieval and lower storage. The authors in [21] proposed\na model that extracts high-level features from the images using\na CNN and extracts high-level voice features using a 1-D dilated\nconvolutional model. To ﬁnd the similarity between the two\ndifferent modalities, the consistency loss and the classiﬁcation\nloss are minimized jointly to learn the representations needed for\nthe RS image–voice retrieval. Guo et al. [20], proposed an RS\nspeech-image retrieval model which uses a 1-D convolutional\nnetwork to extract high-level semantic features of the spoken\nquery, and a CNN to extract high-level visual features from each\nRS image. A multimodal fusion layer was added on the top for\nfusing both modalities.\nC. Cross-Modal Text-Image Retrieval\nOver the last two years, only a few studies have been proposed\nfor RS image retrieval using a textual query. This is mainly\ndue to the challenging nature of the problem and the special\ncharacteristics of RS images. The ﬁrst work in text-based RS\nimage retrieval [22] proposed a deep bidirectional triplet net-\nwork to match natural language descriptions to images. The\ntriplet network is composed of a long short term memory\n(LSTM) and a pretrained CNN. On top of this architecture,\nan average fusion strategy was used to fuse the features per-\ntaining to different sentences. Hoxha et al. [23] developed a\ntext-based image retrieval system that combines a CNN with\na recurrent neural network. The textual query can be directly\ngiven or generated by captioning the query image. Rahhal et al.\n[24] proposed an unsupervised learning method for text-image\nretrieval. The model used a CNN for encoding the image and\na bidirectional LSTM for encoding the text description. The\nauthors in [25] introduced a semantic alignment module in order\nto discover the semantic relationships between image and text\nin the joint embedding space. The module employed attention\nand gate mechanisms to extract discriminative visual and textual\nfeature representations. Speciﬁcally, the attention mechanism\nwas utilized to optimize the corresponding relationships between\nvisual and textual features, and the gate function was employed\nto ﬁlter the unnecessary information to obtain the discriminative\nfeatures. Yuan et al. [26] designed an asymmetric network to\nsolve the target redundancy and multiscale scarcity problems in\nRS retrieval tasks. This method ﬁlters redundant features and\nadapt to multiscale feature inputs by using a multiscale visual\nself-attention module. The authors also addressed the problem\nof high intraclass similarity in RS images by designing a triplet\nloss function to train the model. In [27], the authors proposed a\ntext-image retrieval model and applied two methods to improve\nretrieval performance—a knowledge distillation-based method\nand a semisupervised optimization method based on contrastive\nlearning.\nIII. M ETHODOLOGY\nIn this section, we introduce the multilanguage transformer\nmethod that we propose in detail. Since the transformer is\na central model in our method, we ﬁrst describe the general\narchitecture of the language transformer encoder, and then we\ndescribe the vision transformer encoder.\nFirst, we assume having a set of image-text pairs denoted\nas D = {Xi,ti}N\ni=1, where Xi represents an image, and ti\nrepresents the corresponding sentence in one of the following\nlanguages—English, Arabic, French, and Italian. In the text-to-\nimage retrieval task, given a text query, the goal is to search for\nthe most relevant image Xi to the given text query. Similarly, in\nthe image-to-text retrieval task, the goal is to retrieve the most\nsimilar sentence ti to the query image. To achieve that, we adopt\ntwo transformers one for image encoding and the other for text\nencoding. The vision encoder accepts a mini-batch of bimages,\nwhile the language encoder accepts a set of tokens of bsentences.\nThe outputs of each encoder are injected into the global average\npooling (GAP) layer to obtain a global feature representation\nfor each modality. The output features are then normalized with\nL2-normlization to obtain the visual features {fvi}b\ni =1 and the\ntextual features {fti}b\ni =1 . Afterward, a similarity matrix of size\nb×bis constructed between all the text-image pairs in the mini-\nbatch. The model learns the weights by optimizing text-image\nand image-text contrastive classiﬁcation loss using a stochastic\ngradient descent (SGD) optimizer.\nIn this article, we are particularly interested in investigating\ntwo learning paradigms as shown in Fig. 1. In the single-\nlanguage learning paradigm, the text encoder is trained on each\nlanguage independently so that it accepts a query from one\nlanguage only. In the second paradigm, the text encoder is trained\non sentences from multiple languages jointly, so the model can\naccept a query formulated in any of these languages. Detailed\ndescriptions of the proposed model are provided in the following\nsections.\nA. Language Transformer Encoder\nThe ﬁrst step in processing the textual description for language\ntransformer encoder [33] is sentence tokenization, in which the\nsentence is represented as word tokens ti =( w1,w 2, ... wm),\nwhere mis the length of the sentence. Then, this word vector is\nprojected into an embedding space using a learnable embedding\nlayer Et, that converts these tokens into a sequence of textual\nfeatures of dimension dt.\nBefore feeding the sequence into the encoder, a learnable\npositional embedding is appended to supply the sequence with\ninformation about the order of each word. In addition, two\nspecial tokens CLS and SEP are added to the input tokens to\n9118 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nFig. 1. Difference between the single-language (left) and multilanguage (right) retrieval models. In the single-language model, the query text can be formulated\nin only one language. In the multilanguage model, the query can be formulated in any of the four considered languages. For training the network, we samp le a\nmini-batch of b images-text pairs and feed them to the vision and language transformer encoders to generate L2-normalized visual {fvi}b\ni=1 and textual {fti}b\ni=1\nfeatures. Then, we generate a similarity matrix of size b ×b by computing the similarity between all possible visual and textual pairs in the mini-batch. We learn\nthe model weights by optimizing text-to-image and image-to-text contrastive classiﬁcation loss Ltotal = λ1Lv→ t + λ2Lt→ v using an SGD optimizer.\nFig. 2. Language transformer layer.\nmark the start and end of the sequence. Thus, the sentence\nrepresentation zt0 is expressed as\nzt0 =[ wclass; w1Et; w2Et; ... ; wmEt]+ Epos (1)\nwhere wclass is a special classiﬁcation token that provides a\ngeneral representation of all the tokens, and Epos ∈ R(m+1)×dt is\nthe positional embedding. The initial representation zt0 is fed as\ninput through multiple identical layers of the encoder to generate\nthe ﬁnal representation ztL at the last layer L. Each layer in\nthe encoder contains a multihead self-attention (MSA) block\nfollowed by a multilayer perceptron (MLP) block. Which is a\nsimple feed-forward network consisting of two fully connected\nlayers with GELU activation function in between as shown in\nFig. 2. The MSA and MLP blocks are connected by residual\nskip connections and each layer is followed by a normalization\nlayer (LN):\nz′\nℓ = MSA (LN (ztℓ−1)) + ztℓ−1,ℓ =1 ...L (2)\nztℓ = MLP (LN (z′\ntℓ)) + z′\ntℓ,ℓ =1 ...L. (3)\nThe main goal of the MSA is to manage the complex rela-\ntionships within the sequential data by modeling the long-range\ndependencies between a speciﬁc token and all other tokens in the\nsequence. It comprises multiple independent self-attention heads\noperating in parallel, each head computes a different attention\nscore using the scaled dot-product similarity between the queries\n(Q), keys ( K) and values ( V) expressed by\nAttention = softmax\n( QK√dK\n)\nV (4)\nwhere dK is the dimension of the key. The outputs of all heads are\nconcatenated and then projected with learnable weights matrix\nto the desired dimension.\nB. Image Transformer Encoder\nAfter the tremendous success of transformer in NLP [34], they\nhave been extended recently to computer vision tasks leading\nto the so-called ViT [35]. This last showed competitive results\ncompared to CNN for several image processing tasks, thanks\nto the self-attention mechanism. In vanilla ViT, the sequence\nof word tokens is replaced with a sequence of image patches.\nThe input image Xi of size 224 ×224×3 pixels is ﬁrst divided\ninto N nonoverlapping patches (x1\np; x2\np; ... ; xN\np ). Each patch\nRAHHAL et al.: MULTILANGUAGE TRANSFORMER FOR IMPROVED TEXT TO REMOTE SENSING IMAGE RETRIEV AL 9119\nin the sequence has the dimension of (3p2), where p represents\nthe width/height of the patch and N is the total number of\npatches N = (224 ×224)/p2. This sequence of patches is\nﬂattened and projected via a linear projection layer Ev,t ot h e\nencoder dimension dv. Then, in a way similar to the language\ntransformer, position embeddings are added to keep the position\ninformation. Also, xclass token is appended to the patch repre-\nsentations. The resulting image representation zv0 that is then\nfed into the encoder\nzv0 =\n[\nxclass; x1\npEv; x2\npEv; ... ; xN\np Ev\n]\n+ Epos (5)\nwhere Ev ∈ R(p2.c )×dv. is the linear embedding layer and\nEpos ∈ R(N +1)×dv. is the positional encoding. Finally, by ap-\nplying the same operations as in (2) and (3), we obtain the\nﬁnal image representation zvL at the last layer L.I ti sw o r t h\nrecalling, that the architecture of the vision encoder is similar to\nthe language encoder. Except that the normalization layer comes\nbefore the MSA and the MLP blocks.\nC. Network Optimization\nIn order to learn the weights of the model, we apply global\naverage pooling to the representation matrices ztL ∈ R(m+1)×dt\nand zvLR(N+1)×dv obtained from the text and the image en-\ncoders, respectively, yielding a feature ft ∈ Rdt and fv ∈ Rdv ,\nwith wclass and xclass tokens representing the whole sentence\nand image representations ignored. Then, the visual features\nare further mapped using a linear projection layer to the same\ndimension of the textual feature. We note that the dimension\nof the visual and textual features are dv = 768 ,d t = 512 .\nAfterward, we apply L2-normalization to the resulting textual\nand visual features.\nIf we consider Bk = {Xi,t i}b\ni=1 as the kth mini-batch of\nsize b sampled from the archive D(l). Feeding this mini-batch\nas input to the model yields the following normalized visual\nand textual feature representations {fvi}b\ni =1 and {fti}b\ni =1 .\nThe main learning objective is to jointly train the image and\ntext transformer encoders to maximize the similarity of truly\ncorresponding image-text features pairs while simultaneously\nminimizing the similarity of mismatched image-text features\npairs within the kth mini-batch. To achieve this objective, we rely\non contrastive loss, which is a popular loss in self-supervised\nlearning that has shown an excellent performance in pairwise\nsimilarity measurement tasks [36], [37].\nIn our context, we compute this loss in both textual and visual\ndomains, respectively. In the visual domain, we aim at making\nthe textual feature closer to its corresponding visual feature\nwhile being away from other visual features in the mini-batch.\nSimilarly, in the text domain we aim at making the visual feature\ncloser to its corresponding textual feature while pushing away\nother textual features in the mini-batch. This problem can be\nviewed as a multiclass classiﬁcation problem with b classes,\nwhere b refers to the size of the mini-batch. Basically, we learn\nthe weights of the model by minimizing the cross-entropy loss\nover the similarity matrix of size b×b in the horizontal and\nvertical directions. The text-to-image classiﬁcation loss is given\nFig. 3. Sample images from the three datasets used in the experiments with\nof their textual annotations.\nas follows:\nLt→ v = −1\nb\nb∑\ni =1\nlog exp\n(\nfT\ntifvi/τ\n)\n∑ b\nj =1 exp\n(\nfT\ntifvj/τ\n) (6)\nand likewise, the image-to-text classiﬁcation loss is computed\nas\nLv→ t = −1\nb\nb∑\ni =1\nlog exp\n(\nfT\nvifti/τ\n)\n∑ b\nj =1 exp\n(\nfT\nviftj/τ\n) (7)\nwhere the learnable temperature parameter τ is set to 0.07 to\ncontrol the sharpness of the distribution. Finally, the total loss\nfunction to optimize is\nLtotal = λ1Lv→ t + λ2Lt→ v (8)\nwhere λ1 and λ2 are two hyper-parameters controlling the con-\ntributions of both losses. In all experiments, we set them to the\nvalue 0.5.\nIV . E XPERIMENTAL RESULTS\nIn this section, Section A describes the three text-image\ndatasets used in the experiments and the evaluation metrics.\nSection B introduces the details of the implementation and\nSection C presents the experimental results.\nA. Datasets Description and Evaluation Metrics\n1) Datasets: To validate the proposed multilanguage re-\ntrieval model, three RS cross-modal datasets were exploited to\ntrain and evaluate our model. All the datasets are annotated with\nﬁve English textual descriptions. We used an online software\nto translate the descriptions from English to three different\nlanguages—Arabic, French, and Italian. Fig. 3 shows some im-\nages from the datasets with examples of their textual annotations\nin English, Arabic, French, and Italian, respectively, and Table I\nshows a comparison between the datasets. More details on the\ndatasets are provided in the following.\na) RSICD [38]: This is the largest text-image dataset that\ncontains 10921 images with various resolutions that belong to\n9120 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nTABLE I\nTEXT-IMAGE RS DATASETS\n30 different classes. Images were collected from various sources\nincluding Google Earth, MapABC, Baidu Map, and Tianditu.\nEach image in the dataset has 224 ×224 pixels and is described\nwith ﬁve sentences, where each sentence is at least six words in\nlength. In RSICD, the total number of captions is 24 333, which\nimplies that not all images have ﬁve sentences. For consistency,\ncaptions have been duplicated for images with less than ﬁve\nsentences.\nb) RSITMD [26]:This dataset consists of 4743 images be-\nlonging to 30 classes. The images were collected from the\nRSICD dataset and Google Earth. Each image has the size\n256×256. Five different sentences were given to describe every\nimage with a total of 23 715 captions. In addition to one to ﬁve\nkeywords, the dataset was designed to have fewer images but\nmore diverse captions compared to the RSICD datasets.\nc) UCM [39]:This dataset comprises of 2100 images, each\nimage with the size of 256×256 pixels with a spatial resolution of\n30 cm. The dataset is based on the well-known Merced Land-use\ndataset [40] for scene classiﬁcation that categorizes the 2100\nimages into 21 classes. Each image in this dataset is described\nwith ﬁve different captions, resulting in 10 500 descriptions in\ntotal. Yet, there is a high similarity between the sentences of\nimages that belong to the same class.\n2) Evaluation Metrics: In this article, we present the results\nin terms of Recall@K (R@K) as it is the most adopted evaluation\nmetric for cross-modal retrieval. The recall is a measure that\nrepresents the ratio of the correctly retrieved items to the total\nnumber of existing relevant items to the given query, and it is\ndeﬁned as follows:\nR@k = TP @k\nTP @k + FN@k (9)\nwhere TP is the true positive and FN is the false negative. We\nutilized the R@k indicator with different values of k ( 1 ,5 ,a n d\n10) to measure the retrieval performance.\nIn addition, we provide the result in terms of mean recall\n(mR) to evaluate the overall performance of the model. The\nmR represents the average of R@1, R@5 and R@10 for both\nthe text-to-image and image-to-text retrieval tasks. Besides the\nnumerical metrics mentioned above, subjective metrics are also\nused to better understand the performance of models in retrieving\nthe relevant images on different datasets.\nB. Experimental Setup\nGiven the constraints that the current RS text-image datasets\nare of a small-scale type, we propose to transfer knowledge\nfrom backbones pretrained on a large-scale text-image dataset.\nTo this end, we use the vision language model proposed in\n[36], which was trained on 400 million general text-image pairs.\nThis model built upon two transformers for visual and textual\nfeature representations. Speciﬁcally, ViT32 is adopted as the\nvision transformer, and a BERT-like model as the language\ntransformer. ViT32 consists of L = 12 encoder layers. It divides\nthe image of dimension 224 ×224×3 pixels into n = 49 patches\neach of dimension (p,p)= ( 3 2×32) pixels. These patches are\nﬂattened and mapped to the dimension dv = 768. ViT32 has\nabout 86M parameters. The language transformer encoder is a\nBERT-like model it has 63M parameters; and L = 12 layers. The\nvocabulary size is equal to 49,408. To facilitate batch processing,\nit provides a sequence with a ﬁxed length equal to m = 77.\nThen, it uses a word embedding layer to embed the sequence\ninto features of dimension dt = 512. The resulting visual fvi\nand textual fti feature representations after the GAP operation\nwill be equal to 768 and 512, respectively. The visual features\nare further mapped using a linear projection layer to the same\ndimension as the textual feature, which is 512. Both visual and\ntextual features are normalized using L2-Noramlization.\nFor data augmentation, we apply standard operations such as\nrandom crops, horizontal and vertical ﬂips with 50% probability,\nand ColorJitter. For comparison purposes, we use the same split\nas in previous works. For RSICD and Merced, we consider\n80% of the image-text pairs as training while 10% are left for\nvalidation and 10% for testing. For RSTIMD, we use 80% for\ntraining and 20% for testing.\nSince each image is described by ﬁve sentences, in the\nsingle-language learning setting, we randomly select one of the\nﬁve sentences for learning. For training the model on multiple\nlanguages, we pick each time a sentence from one of the four\nconsidered languages, which are English, Arabic, French, and\nItalian. It is worth-recalling that, we have used online transla-\ntions tools from Google to generate the Arabic, French, and\nItalian captions from English sentences followed by manual\ncorrection.\nAs an optimizer, we use the SGD optimizer with Nesterov\nmomentum. We set the initial learning rate to 0.1, and the\nmomentum to the default value of 0.9. During training, we\ndecrease the learning rate to 0.01 after 40 epochs, and to 0.001\nfor the last 20 epochs. For numerical stability, we found that\nit is useful to apply gradient clipping with a max norm of the\ngradients set to 0.1. The model is trained for 60 iterations with\na mini-batch size set to b = 120.\nThe model was implemented in PyTorch and all the experi-\nments were implemented on a station with a RAM of 32 GB and\nan NVIDIA GeForce GTX 1080 Ti Graphical Processing Unit\n(GPU) (with 11 GB GDDR5X memory).\nC. Experimental Results\nTo evaluate the effectiveness of the proposed model, we report\nthe results of two learning scenarios: the single-language and\nthe multilanguage. In the ﬁrst scenario, the model is trained on\nsentences in English language only to compare its performance\nwith state-of-the-art methods. In the latter scenario, it is trained\nwith sentences from multiple languages either independently\nRAHHAL et al.: MULTILANGUAGE TRANSFORMER FOR IMPROVED TEXT TO REMOTE SENSING IMAGE RETRIEV AL 9121\nTABLE II\nRETRIEV ALRESULTS USING ENGLISH LANGUAGE COMPARED TO\nSTATE-OF-THE-ART METHODS\nor jointly to verify the retrieval performance on multilanguage\nsettings.\n1) English Language Retrieval:Table II presents a compar-\nison between the proposed transformer-based model and the\nstate-of-the-art retrieval methods published recently, namely,\nVSE++ [41], SCAN [42], MTFN [43], AMFMN [26], and\nthree models of LW-MCR [27]. For reliable comparison, our\nmodel is trained and tested on descriptions in English language\nonly. The results are shown for three RS text-image datasets\nwhere the best results are represented in bold. As shown in the\nTable II, for all metrics, the proposed model outperforms the\ncurrent state-of-the-art results on all datasets in both text and\nimage retrieval tasks by a considerable margin. Speciﬁcally, it\nachieves an improvement of 11%, 11.66%, and 8.04% on the mR\nindicator over the AMFMN, which is the second-best method\non the RSICD, RSITMD, and UCM datasets, respectively.\nBy comparing the results of the three datasets, we observe\nthat the results of the RSICD, which is the largest dataset, are\nlower compared to the other two datasets, and the results of\nUCM are the higher. This seems natural as it is easier for the\nretrieval model to ﬁnd the relevant item in a smaller dataset.\nIt is also interesting to notice, that the retrieval performance in\nboth the text retrieval and image retrieval tasks are very close,\nwhich indicates that the matching is effective in both directions.\nMoreover, the recall shows a signiﬁcant increase from R@1 to\nTABLE III\nRETRIEV ALRESULTS FOR SINGLE LANGUAGE AND MULTIPLE LANGUAGE\nTRAINING\nR@5 and from R@5 of R@10. This is because the retrieval task\nis very challenging that it is difﬁcult to ﬁnd the best match in the\nﬁrst retrieved results.\nGenerally, this experiment demonstrates the powerful expres-\nsive ability of transformer encoders for image and text and how\nthey are effective in boosting the retrieval performance of RS\ndata.\n2) Multilanguage Retrieval: To assess the validity of the\nretrieval model on the multilanguage retrieval task, we trained\nit independently on sentences from each of the following lan-\nguages: English, Arabic, French, and Italian. In addition, we\ntrain it jointly on sentences from the four considered languages.\nTable III shows the retrieval results in terms of R@ k and mR on\nthe three datasets. The ﬁrst part of each table shows the results\nof the model trained on a single language, and the second part\npresents the results of learning on multiple languages.\nBy comparing the results of Tables II and III, it can be\nseen that the performance of the model trained using single or\nmultiple languages yields better than all of the state-of-the-art\nmethods. In most of the cases, the results show that the best\nretrieval performance is achieved with the model trained on\n9122 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nmultiple languages. Speciﬁcally, in the RSICD dataset, the best\nresults are achieved using an English query on the multilanguage\nmodel. The same observation can also be noticed on RSITMD,\nwhere the best results are achieved by using the English and\nFrench queries. The text retrieval task on the UCM dataset is\nan exception to this, where the results of the multilanguage\nmodel is lower than the single-language model trained on French\nlanguage only. However, the mR of the image retrieval task for\nthe UCM dataset shows slightly better performance over the\nsingle language model.\nNext, we compare the performance of each language when\nthe model is trained independently on this language and when\nthe model is trained jointly with other languages. As shown\nin Table III, for English language, we can clearly see that\nthe retrieval performance has increased using English queries\nwhen the model is trained on multiple languages compared to\na model trained on English language only. For example, on\nRSICD and RSITMD datasets we can see an improvement in all\nmetrics. However, on the UCM dataset, there is a slight decrease\nin multilingual performance compared to the single language\nmodel for R@1 image retrieval. One possible reason for this is\nthe high similarity between the descriptions given to the images\nthat belong to the same class in this dataset. In general, English\nhas the highest scores on most indicators compared to other\nlanguages. This could be explained by the fact that the model is\nbasically pretrained on English language and then ﬁnetuned on\nthe other languages.\nFor the French language, the results on the RSICD dataset\nshow an improvement in all metrics when the model is trained on\nmultilanguage. However, the results of RSITMD dataset show a\nslight decrease in R@1 and R@5 of the text retrieval. For UCM\ndataset, the French single-language model has the highest scores\nin all metrics on the text retrieval. Yet, Table III shows a decrease\nin the performance in all metrics when the model is trained on\nmultiple languages, especially in the R@1 text retrieval score,\nwhere the performance decrease is signiﬁcant from 20.00% to\n12.85%.\nThe Arabic language has relatively the lowest scores on most\nindicators in all datasets. The reason could be attributed to the\nuniqueness of its alphabets, and it is script direction as it is\nthe only Semitic language in the group. The multiple languages\nmodel obtained an improved performance over the single lan-\nguage model, when given an Arabic query. In particular, the\nimprovement of the multiple languages model in mR indicator\nare 1.76%, 4.32%, and 2.87% on the RSICD, RSITMD, and\nUCM datasets, respectively. On the UCM dataset, the Arabic\nmultiple languages model has improved all the text retrieval, and\nthe image retrieval indicators except the R@10 image retrieval\nscore.\nThe Italian language has the second-best overall performance\non the RSICD dataset for the single and the multiple lan-\nguages models. The multiple languages model has improved\nthe performance of all metrics compared to the single language\nmodel, except the mR and the R@10 text retrieval score. On\nRSITMD dataset, the Italian language has the highest improved\nperformance on multiple languages learning with an increase\nof 4.78%, in the mR metric. On the UCM dataset, the single\nlanguage model outperformed the multiple languages model\nexcept for R@5 and R@10 image retrieval.\nGenerally, the English and French languages have the highest\nresults on the three datasets, and the RSITMD dataset shows\nthe highest improvement percentage compared to other datasets\nwhen the multiple language model is used. UCM dataset which\nis the smallest dataset shows a decrease in multilingual perfor-\nmance compared to the single language model. The possible\nreason could be the high similarity between the dataset sen-\ntences.\nTo get an intuition of how the image and the text are aligned\nin the joint embedding space, Fig. 4 shows the features of both\nthe image and text obtained from the two encoders projected\ninto the 2-D space using the t-distributed stochastic neighbor\nembedding (t-SNE) method. We can observe that the model is\nable to aggregate data from each modality into well-separated\nclusters. Furthermore, it attempts to align the embedding of the\nimages and their corresponding texts from the four considered\nlanguages to form larger clusters.\nAligning between two different modalities is challenging as\nwe observe a slight shift between the image representations and\ntheir corresponding texts and some overlaps between the formed\nclusters. This is clearer in the RISTMD and the RSICD datasets,\nwhich are relatively larger datasets compared to the UCM. We\nrecall that a good alignment is essential to retrieve an image\nthat matches a given query text, and also to retrieve an image\nthat matches a given query text, and also to retrieve the textual\ndescription that describes a given image.\nD. Visual Explainability\nIn addition to the quantitative results, we performed another\nqualitative experiment to better understand the behavior of the\nattention mechanism employed by the transformer encoders.\nFig. 5 shows some examples of textual queries and the retrieved\nimages from RSITMD and RSICD datasets.\nOn the left, Fig. 5 shows the input query with the textual\nattention map, and on the right the ground truth image and the\nretrieved images with the associated visual attention maps. The\ntextual attention highlights the important words that the model\npays attention to retrieve the image, and the visual attention maps\nshow the spatial areas of the image that the model focuses on to\nmake the retrieval. It is worth noting that both the textual and\nvisual attention maps are generated by using attention rollout\ntechnique [44] on the attention scores for the top layers (form\nlayer 8 up to the last layers of the encoder).\nWe can initially notice from Fig. 5 that the model can high-\nlight the important keywords in the sentence no matter which\nlanguage we use for the query. In addition, the visual attention\nmaps of the retrieved images show high responses at the area\nrelated to the semantic meaning of the query.\nTo further analyze the retrieved results, Table IV shows the\nground-truth images that match the queries in Fig. 5 and the\nimages retrieved by the model. For the ﬁrst case of the RSITMD\ndataset, the model failed to retrieve the “airport_3” image in the\nﬁrst ten results. However, all the retrieved images are from the\n“airport” category. The textual attention shows a high response\nRAHHAL et al.: MULTILANGUAGE TRANSFORMER FOR IMPROVED TEXT TO REMOTE SENSING IMAGE RETRIEV AL 9123\nFig. 4. t-SNE representation of image and text embedding features. (a) RISTMD, (b) RSICD, and (c) UCM datasets.\nFig. 5. Visualization of attention on image and query text. (a) RSITMD and (b) RSICD datasets.\n9124 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\nTABLE IV\nRANKS OF THE RETRIEVED IMAGES OBTAINED FOR RSITMD AND RSICD DATASETS\non the words “planes” and “airport.” The visual attention maps\nfocus on an area containing airplanes, which indicates that there\nis some confusion duo to the high similarity within the samples\nof the “airport” category. The second case shows a successful\nretrieval where the image “baseballﬁeld_26” is predicted in the\nﬁrst rank using an Arabic query with the meaning “six tennis\ncourts besides a baseball ﬁeld”. The corresponding attention\nmaps are in line with the semantic meaning of the query as we\nsee the model paying high attention to the baseball diamond and\nthe tennis courts area, and to the words “six,” “tennis courts,”\nand “baseball” in the query. The third example shows another\nfailure case in retrieving the image “bridge_55” with a query\ngiven in Italian language. The query which means “A bridge\nbuilt on a river,” shows a high attention to the words “sul”\nand “ﬁume” which means “on the river.” The model wrongly\nretrieved an image of a river in the ﬁrst rank and retrieved other\nimages from categories that have similar visual features to the\n“bridge” category, such as “river,” “port,” and “railway station.”\nThis is also conﬁrmed by the attention maps of the retrieved\nimages that show a high response in the river and railway areas\nwhich look similar to the bridge. The last case shows another\nsuccessful retrieval of the “parking_235” image with a query\ngiven in French. The query has the meaning of “There are a\nnumber of colored cars in the parking.” The target image is\ncorrectly retrieved in the fourth rank and all the matched images\nare from the “parking” category.\nFor the RSICD dataset, Table IV shows three examples of\naccurate retrieval of the “denseresidential_60,” “desert_52,” and\n“storagetanks_36,” where the target image is correctly retrieved\nin the ﬁrst rank. In the ﬁrst example, which shows a query in\nEnglish, the model retrieved the “denseresidential_60” in the\nﬁrst rank, but the results show some confusion with images from\nthe “playground” category in the second and tenth ranks. The\npossible reason for this is that many dense residential images\ncontain playground areas. The second example shows a query\nin Arabic with the meaning “There is a small lake in the desert.”\nThis example shows that the model was successful in retrieving\nthe target image and that all the matched images are from the\ncategory of the “desert.” The textual attention map shows high\nresponses to the words “desert” and “lake” in Arabic, which is\nconsistent with the visual attention maps of the retrieved images\nthat show high focus on the lake areas.\nThe third example shows the result of a query given in French\nin which the model retrieved the “farmland_38” image in the\nsixth rank, but all the retrieved images are from the same\n“farmland” category. This is because in the RSICD dataset, there\nis a high intraclass similarity which can be the reason for this\nconfusion. Finally, the last example shows a successful retrieval\nfor a query given in Italian with the meaning “two blue storage\ntanks in a rectangular ﬁeld near a factory.” Both the textual and\nthe visual attention maps show that the model highlights the\ninformation that is relevant to the prediction.\nAccording to the above observations, it can be seen that\nthe model is generally effective in retrieving RS images using\nqueries from different languages. Even though the model fails\nto retrieve the targeted images in some cases, in many of the fail\ncases it retrieves images that belong to the same category or a\nvery relevant category. In addition, the model can successfully\ncapture the keywords in the query text and the ﬁne-grained\ndetails areas on the retrieved images as well.\nV. C ONCLUSION\nIn this article, we have proposed an approach for multilan-\nguage RS text-image retrieval based on language and vision\ntransformers. We used vision and language transformer encoders\nfor generating visual and textual representations, respectively.\nWe have aligned these representations by optimizing a bidirec-\ntional contrastive loss related to text-to-image and image-to-text\nclassiﬁcation. In contrast to previous retrieval methods, which\nrestrict the language of the query to English, the model allows\nqueries to be formulated in English, Arabic, French, and Italian.\nThe qualitative and quantitative results on three RS datasets\nshow that the model is capable of dealing with multilanguage\nqueries while still achieving better performances than the current\nstate-of-the-art methods.\nRAHHAL et al.: MULTILANGUAGE TRANSFORMER FOR IMPROVED TEXT TO REMOTE SENSING IMAGE RETRIEV AL 9125\nACKNOWLEDGMENT\nThe authors would like to thank the support from the Distin-\nguished Scientist Fellowship Program at King Saud University.\nREFERENCES\n[1] M. Sudmanns et al., “Big earth data: Disruptive changes in earth observa-\ntion data management and analysis?,” Int. J. Digit. Earth, vol. 13, no. 7,\npp. 832–850, Jul. 2020, doi: 10.1080/17538947.2019.1585976.\n[2] Y . Li, J. Ma, and Y . Zhang, “Image retrieval from remote sensing\nbig data: A survey,” Inf. Fusion, vol. 67, pp. 94–115, Mar. 2021,\ndoi: 10.1016/j.inffus.2020.10.008.\n[3] Y . Yang and S. Newsam, “Geographic image retrieval using local invariant\nfeatures,”IEEE Trans. Geosci. Remote Sens., vol. 51, no. 2, pp. 818–832,\nFeb. 2013, doi: 10.1109/TGRS.2012.2205158.\n[4] E. Aptoula, “Remote sensing image retrieval with global morphological\ntexture descriptors,” IEEE Trans. Geosci. Remote Sens., vol. 52, no. 5,\npp. 3023–3034, May 2014, doi: 10.1109/TGRS.2013.2268736.\n[5] X.-Y . Tong, G.-S. Xia, F. Hu, Y . Zhong, M. Datcu, and L. Zhang, “Ex-\nploiting deep features for remote sensing image retrieval: A systematic\ninvestigation,”IEEE Trans. Big Data, vol. 6, no. 3, pp. 507–521, Sep. 2020,\ndoi: 10.1109/TBDATA.2019.2948924.\n[6] G. Sumbul, S. Nayak, and B. Demir, “SD-RSIC: Summarization-\ndriven deep remote sensing image captioning,” IEEE Trans.\nGeosci. Remote Sens. , vol. 59, no. 8, pp. 6922–6934, Aug. 2021,\ndoi: 10.1109/TGRS.2020.3031111.\n[7] G. Hoxha and F. Melgani, “A novel SVM-Based decoder for remote\nsensing image captioning,” IEEE Trans. Geosci. Remote Sens., vol. 60,\n2022, Art. no. 5404514, doi: 10.1109/TGRS.2021.3105004.\n[8] Q. Wang, W. Huang, X. Zhang, and X. Li, “Word–Sentence frame-\nwork for remote sensing image captioning,” IEEE Trans. Geosci.\nRemote Sens. , vol. 59, no. 12, pp. 10532–10543, Dec. 2021,\ndoi: 10.1109/TGRS.2020.3044054.\n[9] X. Zheng, B. Wang, X. Du, and X. Lu, “Mutual attention incep-\ntion network for remote sensing visual question answering,” IEEE\nTrans. Geosci. Remote Sens. , vol. 60, 2022, Art. no. 5606514,\ndoi: 10.1109/TGRS.2021.3079918.\n[10] S. Lobry, D. Marcos, J. Murray, and D. Tuia, “RSVQA: Vi-\nsual question answering for remote sensing data,” IEEE Trans.\nGeosci. Remote Sens. , vol. 58, no. 12, pp. 8555–8566, Dec. 2020,\ndoi: 10.1109/TGRS.2020.2988782.\n[11] Y . Li, Y . Zhang, X. Huang, and J. Ma, “Learning source-invariant deep\nhashing convolutional neural networks for cross-source remote sensing\nimage retrieval,” IEEE Trans. Geosci. Remote Sens., vol. 56, no. 11,\npp. 6521–6536, Nov. 2018, doi: 10.1109/TGRS.2018.2839705.\n[12] U. Chaudhuri, B. Banerjee, A. Bhattacharya, and M. Datcu, “CMIR-\nNET : A deep learning based model for cross-modal retrieval in re-\nmote sensing,” Pattern Recognit. Lett., vol. 131, pp. 456–462, Mar. 2020,\ndoi: 10.1016/j.patrec.2020.02.006.\n[13] W. Xiong, Z. Xiong, Y . Cui, and Y . Lv, “A discriminative distillation\nnetwork for cross-source remote sensing image retrieval,” IEEE J. Sel.\nTopics Appl. Earth Observ. Remote Sens., vol. 13, pp. 1234–1247, 2020,\ndoi: 10.1109/JSTARS.2020.2980870.\n[14] W. Xiong, Y . Lv, X. Zhang, and Y . Cui, “Learning to trans-\nlate for cross-source remote sensing image retrieval,” IEEE Trans.\nGeosci. Remote Sens. , vol. 58, no. 7, pp. 4860–4874, Jul. 2020,\ndoi: 10.1109/TGRS.2020.2968096.\n[15] T.-B. Jiang, G.-S. Xia, Q.-K. Lu, and W.-M. Shen, “Retrieving aerial scene\nimages with learned deep image-sketch features,” J. Comput. Sci. Technol.,\nvol. 32, no. 4, pp. 726–737, Jul. 2017, doi: 10.1007/s11390-017-1754-7.\n[16] F. Xu, W. Yang, T. Jiang, S. Lin, H. Luo, and G.-S. Xia, “Mental retrieval\nof remote sensing images via adversarial sketch-image feature learning,”\nIEEE Trans. Geosci. Remote Sens., vol. 58, no. 11, pp. 7801–7814,\nNov. 2020, doi: 10.1109/TGRS.2020.2984316.\n[17] G. Mao, Y . Yuan, and L. Xiaoqiang, “Deep cross-modal retrieval for remote\nsensing image and audio,” in Proc. 10th IAPR Workshop Pattern Recognit.\nRemote Sens., Aug. 2018, pp. 1–7, doi: 10.1109/PRRS.2018.8486338.\n[18] Y . Chen and X. Lu, “A deep hashing technique for remote sensing\nimage-sound retrieval,”Remote Sens., vol. 12, no. 1, Jan. 2020, Art. no. 1,\ndoi: 10.3390/rs12010084.\n[19] Y . Chen, X. Lu, and S. Wang, “Deep cross-modal image–voice retrieval\nin remote sensing,” IEEE Trans. Geosci. Remote Sens., vol. 58, no. 10,\npp. 7049–7061, Oct. 2020, doi: 10.1109/TGRS.2020.2979273.\n[20] M. Guo, C. Zhou, and J. Liu, “Jointly learning of visual and auditory: A\nnew approach for RS image and audio cross-modal retrieval,” IEEE J. Sel.\nTopics Appl. Earth Observ. Remote Sens., vol. 12, no. 11, pp. 4644–4654,\nNov. 2019, doi: 10.1109/JSTARS.2019.2949220.\n[21] H. Ning, B. Zhao, and Y . Yuan, “Semantics-Consistent represen-\ntation learning for remote sensing image-voice retrieval,” IEEE\nTrans. Geosci. Remote Sens. , vol. 60, 2022, Art. no. 4700614,\ndoi: 10.1109/TGRS.2021.3060705.\n[22] T. Abdullah, Y . Bazi, M. M. Al Rahhal, M. L. Mekhalﬁ, L. Rangarajan,\nand M. Zuair, “TextRS: Deep bidirectional triplet network for matching\ntext to remote sensing images,” Remote Sens., vol. 12, no. 3, Jan. 2020,\nArt. no. 3, doi: 10.3390/rs12030405.\n[23] G. Hoxha, F. Melgani, and B. Demir, “Toward remote sensing image\nretrieval under a deep image captioning perspective,” IEEE J. Sel. Top-\nics Appl. Earth Observ. Remote Sens., vol. 13, pp. 4462–4475, 2020,\ndoi: 10.1109/JSTARS.2020.3013818.\n[24] M. M. A. Rahhal, Y . Bazi, T. Abdullah, M. L. Mekhalﬁ, and M. Zuair,\n“Deep unsupervised embedding for remote sensing image retrieval us-\ning textual cues,” Appl. Sci., vol. 10, no. 24, Jan. 2020, Art. no. 24,\ndoi: 10.3390/app10248931.\n[25] Q. Cheng, Y . Zhou, P. Fu, Y . Xu, and L. Zhang, “A deep semantic alignment\nnetwork for cross-modal image-text retrieval in remote sensing,” IEEE J.\nSel. Topics Appl. Earth Observ. Remote Sens., vol. 14, pp. 4284–4297,\n2021, doi: 10.1109/JSTARS.2021.3070872.\n[26] Z. Yuan et al., “Exploring a ﬁne-grained multiscale method for cross-\nmodal remote sensing image retrieval,” IEEE Trans. Geosci. Remote Sens.,\nvol. 60, 2022, Art. no. 4404119, doi: 10.1109/TGRS.2021.3078451.\n[27] Z. Yuan et al., “A lightweight Multi-scale crossmodal text-image retrieval\nmethod in remote sensing,” IEEE Trans. Geosci. Remote Sens., vol. 60,\n2022, Art. no. 5612819, doi: 10.1109/TGRS.2021.3124252.\n[28] F. Tan, J. Yuan, and V . Ordonez, “Instance-level image retrieval\nusing reranking transformers,” pp. 12105–12115, (2021). Accessed:\nAug. 7, 2022. [Online]. Available: https://openaccess.thecvf.com/\ncontent/ICCV2021/html/Tan_Instance-Level_Image_Retrieval_Using_\nReranking_Transformers_ICCV_2021_paper.html\n[29] A. El-Nouby, N. Neverova, I. Laptev, and H. Jégou, “Training vision\ntransformers for image retrieval,” Feb. 2021, arXiv:2102.05644.\n[30] W. Xiong, Z. Xiong, Y . Zhang, Y . Cui, and X. Gu, “A deep cross-\nmodality hashing network for SAR and optical remote sensing images\nretrieval,”IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., vol. 13,\npp. 5284–5296, 2020, doi: 10.1109/JSTARS.2020.3021390.\n[31] H. Zhou, C. Tian, Z. Zhang, Q. Huo, Y . Xie, and Z. Li, “Multispectral fusion\ntransformer network for RGB-thermal urban scene semantic segmenta-\ntion,” IEEE Geosci. Remote Sens. Lett., vol. 19, 2022, Art. no. 7507105,\ndoi: 10.1109/LGRS.2022.3179721.\n[32] Z. Zhang, J. Li, C. Tian, Z. Zhong, Z. Jiao, and X. Gao,\n“Quality-driven deep active learning method for 3D brain MRI\nsegmentation,” Neurocomputing, vol. 446, pp. 106–117, Jul. 2021,\ndoi: 10.1016/j.neucom.2021.03.050.\n[33] A. Vaswani et al., “Attention is all you need,” Adv. Neural Inf. Process.\nSyst., pp. 6000–6010, 2017, arXiv:1706.03762.\n[34] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT:\nPre-training of deep bidirectional transformers for language under-\nstanding,” North Amer. Chapter Assoc. Comput. Linguistics , 2019.\ndoi: 10.48550/arXiv.1810.04805.\n[35] A. Dosovitskiy et al., “An image is worth 16x16 words: Transformers for\nimage recognition at scale,” 2020, arXiv:2010.11929.\n[36] A. Radford et al., “Learning transferable visual models from natu-\nral language supervision,” in Proc. Int. Conf. Mach. Learn. , 2021,\npp. 8748–8763.\n[37] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for\ncontrastive learning of visual representations,” 2020, arXiv:2002.05709.\n[38] X. Lu, B. Wang, X. Zheng, and X. Li, “Exploring models and\ndata for remote sensing image caption generation,” IEEE Trans.\nGeosci. Remote Sens. , vol. 56, no. 4, pp. 2183–2195, Apr. 2018,\ndoi: 10.1109/TGRS.2017.2776321.\n[39] B. Qu, X. Li, D. Tao, and X. Lu, “Deep semantic understanding of high res-\nolution remote sensing image,” in Proc. Int. Conf. Comput., Inf. Telecom-\nmunication Syst., Jul. 2016, pp. 1–5, doi: 10.1109/CITS.2016.7546397.\n[40] Y . Yang and S. Newsam, “Bag-of-visual-words and spatial extensions\nfor Land-use classiﬁcation,” in Proc. 18th SIGSPATIAL Int. Conf. Adv.\nGeographic Inf. Syst., 2010, pp. 270–279, doi: 10.1145/1869790.1869829.\n[41] F. Faghri, D. J. Fleet, J. R. Kiros, and S. Fidler, “VSE ++: Improving\nvisual-semantic embeddings with hard negatives,” Jul. 2017, Accessed:\nNov. 23, 2021. [Online]. Available: https://arxiv.org/abs/1707.05612v4\n9126 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 15, 2022\n[42] K.-H. Lee, X. Chen, G. Hua, H. Hu, and X. He, “Stacked cross attention for\nimage-text matching,” Jul. 2018, arXiv:1803.08024 [cs], Accessed: Nov.\n23, 2021. [Online]. Available: http://arxiv.org/abs/1803.08024\n[43] T. Wang, X. Xu, Y . Yang, A. Hanjalic, H. T. Shen, and J. Song,\n“Matching images and text with multi-modal tensor fusion and re-\nranking,” in Proc. 27th ACM Int. Conf. Multimedia, Oct. 2019, pp. 12–20,\ndoi: 10.1145/3343031.3350875.\n[44] S. Abnar and W. Zuidema, “Quantifying attention ﬂow in transform-\ners,” in Proc. 58th Annu. Meeting Assoc. Comput. Linguistics, 2020,\npp. 4190–4197, doi: 10.18653/v1/2020.acl-main.385.\nMohamad M. Al Rahhal (Senior Member, IEEE)\nreceived the B.Sc. degree in computer engineering\nfrom Aleppo University, Aleppo, Syria, in 2002, the\nM.Sc. degree in information technology from Ham-\ndard University, New Delhi, India, in 2005, and the\nPh.D. degree in computer engineering from King\nSaud University, Riyadh, Saudi Arabia, in 2015.\nFrom 2006 to 2012, he was a Lecturer with Al-Jouf\nUniversity, Sakakah, Saudi Arabia. He is currently\nan Associate Professor with the College of Applied\nComputer Engineering, King Saud University. His\nresearch interests include signal/image medical analysis, remote sensing, and\ncomputer vision.\nYakoub Bazi (Senior Member, IEEE) received the\nState Engineer and M.Sc. degrees in electronics from\nthe University of Batna, Batna, Algeria, in 1994 and\n2000, respectively, and the Ph.D. degree in informa-\ntion and communication technology from the Univer-\nsity of Trento, Trento, Italy, in 2005.\nFrom 2000 to 2002, he was a Lecturer with the\nUniversity of M’Sila, M’Sila, Algeria. In 2006, he\njoined the University of Trento, as a Postdoctoral\nResearcher. From 2006 to 2009, he was an Assistant\nProfessor with the College of Engineering, Al-Jouf\nUniversity, Sakakah, Saudi Arabia. He is currently a Full Professor of Computer\nEngineering with the College of Computer and Information Sciences, King\nSaud University, Riyadh, Saudi Arabia. He is a referee for several international\njournals. His research interests include remote sensing, signal/image medical\nanalysis, and computer vision.\nDr. Bazi is an Associate Editor of IEEE Geoscience and Remote Sensing\nLetters, IEEE T RANSACTIONS ON GEOSCIENCE AND REMOTE SENSING,a n d\nIEEE Journal of Selected Topics in Applied Earth Observations and Remote\nSensing.\nNorah A. Alsharif received the B.Sc. degree from Taif University, Taif, Saudi\nArabia, in 2018, and the M.Sc. degree from King Saud University, Riyadh, Saudi\nArabia, in 2022, both in computer engineering.\nHer research interests include artiﬁcial intelligence and remote-sensing image\nanalysis.\nLaila Bashmal (Graduate Student Member, IEEE) received the B.S. degree in\ncomputer science from the University of Dammam, Dammam, Saudi Arabia,\nin 2011, and the M.Sc. degree in computer engineering in 2018 from King\nSaud University, Riyadh, Saudi Arabia, where she is currently working toward\nthe Ph.D. degree in computer engineering.\nHer research interests include machine learning and image processing with\napplications to remote sensing image analysis.\nNaif Alajlan (Senior Member, IEEE) received the\nbachelor’s and master’s degrees in electrical engineer-\ning from the Electrical Engineering Department, King\nSaud University, Riyadh, Saudi Arabia, in 1998 and\n2002, respectively, and the Ph.D. degree in computer\nengineering from the Electrical and Computer En-\ngineering Department, University of Waterloo, ON,\nCanada, in 2006.\nHe is currently a Full Professor of AI in Com-\nputer Engineering Department, King Saud University,\nSaudi Arabia. He has authored and coauthored more\nthan 130 referred journal papers in machine learning, pattern recognition,\nbiomedical engineering, remote sensing, and other ﬁelds. In 2009, he founded\nALISR, a research lab in intelligent systems where several research and consul-\ntation projects were conducted with public and private organizations.\nFarid Melgani (Fellow, IEEE) received the State\nEngineer degree in electronics from the University\nof Batna, Batna, Algeria, in 1994, the M.Sc. de-\ngree in electrical engineering from the University\nof Baghdad, Baghdad, Iraq, in 1999, and the Ph.D.\ndegree in electronic and computer engineering from\nthe University of Genoa, Genoa, Italy, in 2003.\nHe is a Full Professor of Telecommunications\nwith the Department of Information Engineering and\nComputer Science, University of Trento, Trento, Italy,\nwhere he teaches pattern recognition, machine learn-\ning, and digital transmission. He is the Head of the Signal Processing and\nRecognition Laboratory, the Coordinator of the Doctoral School in Industrial\nInnovation, and the Dean of Undergrad and Grad Studies with the same depart-\nment. He has coauthored more than 250 scientiﬁc publications. His research\ninterests include remote sensing, signal/image processing, pattern recognition,\nmachine learning, and computer vision.\nDr. Melgani is currently an Associate Editor of IEEE T RANSACTIONS ON\nGEOSCIENCE AND REMOTE SENSING, International Journal of Remote Sensing,\nand IEEE Journal on Miniaturization for Air and Space Systems.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8516808748245239
    },
    {
      "name": "Transformer",
      "score": 0.7584682703018188
    },
    {
      "name": "Encoder",
      "score": 0.7322540879249573
    },
    {
      "name": "Information retrieval",
      "score": 0.5585635900497437
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5556638836860657
    },
    {
      "name": "Image retrieval",
      "score": 0.550806999206543
    },
    {
      "name": "Natural language processing",
      "score": 0.5249850153923035
    },
    {
      "name": "Language model",
      "score": 0.4827215373516083
    },
    {
      "name": "Precision and recall",
      "score": 0.43095847964286804
    },
    {
      "name": "Recall",
      "score": 0.41198965907096863
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4112773537635803
    },
    {
      "name": "Image (mathematics)",
      "score": 0.28208109736442566
    },
    {
      "name": "Voltage",
      "score": 0.08124750852584839
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I28022161",
      "name": "King Saud University",
      "country": "SA"
    },
    {
      "id": "https://openalex.org/I193223587",
      "name": "University of Trento",
      "country": "IT"
    }
  ],
  "cited_by": 51
}