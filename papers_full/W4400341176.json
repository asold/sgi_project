{
  "title": "Large Language Models Struggle in Token-Level Clinical Named Entity Recognition",
  "url": "https://openalex.org/W4400341176",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3176102428",
      "name": "Lu Qiuhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1898319288",
      "name": "Li Rui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286954359",
      "name": "Wen, Andrew",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2349201429",
      "name": "Wang Jin-lian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1923131268",
      "name": "Wang Li-wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2347531344",
      "name": "Liu Hongfang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W49934205"
  ],
  "abstract": "Large Language Models (LLMs) have revolutionized various sectors, including healthcare where they are employed in diverse applications. Their utility is particularly significant in the context of rare diseases, where data scarcity, complexity, and specificity pose considerable challenges. In the clinical domain, Named Entity Recognition (NER) stands out as an essential task and it plays a crucial role in extracting relevant information from clinical texts. Despite the promise of LLMs, current research mostly concentrates on document-level NER, identifying entities in a more general context across entire documents, without extracting their precise location. Additionally, efforts have been directed towards adapting ChatGPT for token-level NER. However, there is a significant research gap when it comes to employing token-level NER for clinical texts, especially with the use of local open-source LLMs. This study aims to bridge this gap by investigating the effectiveness of both proprietary and local LLMs in token-level clinical NER. Essentially, we delve into the capabilities of these models through a series of experiments involving zero-shot prompting, few-shot prompting, retrieval-augmented generation (RAG), and instruction-fine-tuning. Our exploration reveals the inherent challenges LLMs face in token-level NER, particularly in the context of rare diseases, and suggests possible improvements for their application in healthcare. This research contributes to narrowing a significant gap in healthcare informatics and offers insights that could lead to a more refined application of LLMs in the healthcare sector.",
  "full_text": "Large Language Models Struggle in Token-Level Clinical Named Entity\nRecognition\nQiuhao Lu, Ph.D., Rui Li, Ph.D., Andrew Wen, M.S., Jinlian Wang, Ph.D., Liwei Wang, M.D.,\nPh.D., Hongfang Liu, Ph.D.\nMcWilliams School of Biomedical Informatics, University of Texas Health Science Center,\nHouston, TX, USA\nAbstract\nLarge Language Models (LLMs) have revolutionized various sectors, including healthcare where they are employed\nin diverse applications. Their utility is particularly significant in the context of rare diseases, where data scarcity,\ncomplexity, and specificity pose considerable challenges. In the clinical domain, Named Entity Recognition (NER)\nstands out as an essential task and it plays a crucial role in extracting relevant information from clinical texts. Despite\nthe promise of LLMs, current research mostly concentrates on document-level NER, identifying entities in a more\ngeneral context across entire documents, without extracting their precise location. Additionally, efforts have been\ndirected towards adapting ChatGPT for token-level NER. However, there is a significant research gap when it comes\nto employing token-level NER for clinical texts, especially with the use of local open-source LLMs. This study aims\nto bridge this gap by investigating the effectiveness of both proprietary and local LLMs in token-level clinical NER.\nEssentially, we delve into the capabilities of these models through a series of experiments involving zero-shot prompting,\nfew-shot prompting, retrieval-augmented generation (RAG), and instruction-fine-tuning. Our exploration reveals the\ninherent challenges LLMs face in token-level NER, particularly in the context of rare diseases, and suggests possible\nimprovements for their application in healthcare. This research contributes to narrowing a significant gap in healthcare\ninformatics and offers insights that could lead to a more refined application of LLMs in the healthcare sector.\nIntroduction\nElectronic Health Records (EHRs) are a key component in modern healthcare, encapsulating vast amounts of patient\ndata, most notably in clinical notes. Their widespread adoption by healthcare providers in recent years has revolutionized\nthe manner in which patients’ visits and health information are recorded and managed1, thereby elevating the quality\nof patient care. However, extracting pertinent information from these records presents a significant challenge due to\nthe abundant, heterogeneous, and private nature of clinical texts. At the heart of addressing this challenge is Named\nEntity Recognition (NER), a fundamental task in natural language processing aimed at identifying and categorizing\nkey information units in the text. In the clinical domain, the goal of the task is to identify all occurrences of specific\nclinically relevant named entities in the given unstructured clinical narrative2.\nThere has been a surge of interest in developing clinical NER systems in the last few years. Early methods mostly rely\non manually-crated rules and traditional machine learning techniques, such as MetaMap3, KnowledgeMap4, cTAKES5,\netc. With the trending of deep learning methods and the Transformer architecture6, more researchers shift to building\nclinical NER systems and other NLP applications upon pre-trained language models such as BERT7. A typical solution\nis to insert a multilayer perception (MLP) on top of the language model and train the entire model via fine-tuning. For\nexample, Alsentzer et al. fine-tune BioClinicalBERT on four i2b2 NER tasks8. Similarly, Sung et al. present BERN2,\nwhich uses Bio-LM9 as the foundation model and achieves better performance via multitask learning10.\nA common theme among the aforementioned deep-learning-based methods is their data-hungry nature, which unfortu-\nnately poses significant challenges in the medical field, where issues of data scarcity, heterogeneity, and confidentiality\nare consistently prevalent. The situation is even worse forrare diseases due to their diversity, complexity, and specificity.\nRare diseases refer to diseases or conditions that affect fewer than 200, 000 Americans by definition in the United\nStates11. These conditions are often underrepresented in datasets, leading to a scarcity of information that deep learning\nmodels can learn from.\nRecent advancements in computational linguistics have led to the emergence of Large Language Models (LLMs).\nThese models have shown remarkable capabilities in various domains, including the clinical field12. Their strengths in\narXiv:2407.00731v2  [cs.CL]  17 Aug 2024\n[ADNP syndrome]\n(a) document-level\nADNP : B-RAREDISEASEsyndrome : I-RAREDISEASEaffects : Ofemales : Oand : Omales : Oin : Oequal : Onumbers : O. : O (b) token-level\nFigure 1: Different NER outputs for the given text: ADNP syndrome affects females and males in equal numbers .\nin-context learning, such as retrieval-augmented generation (RAG), present new opportunities for tackling the challenges\nof NER in clinical texts. Despite the promise of LLMs, most existing research in this area has either focused on\ndocument-level NER (i.e., aggregation NER), which involves identifying entities at a broader level in entire documents\nor paragraphs without predicting the exact span or location of these entities, or on adapting ChatGPT to this task. For\nexample, Zhou et al. introduce UniversalNER, a distillation approach using mission-focused instruction-tuning to create\nefficient models that excel in NER. They distill ChatGPT into a more compact LLaMA-based13 model UniversalNER\nand demonstrate superior document-level NER accuracy across diverse domains, including biomedicine14. Similarly,\nin their exploration of ChatGPT-3.5-turbo, Shry et al. focus on the extraction of rare diseases and their associated\nphenotypes via prompt engineering, also limiting their analysis to document-level output, which overlooks the precise\nspan and location of these entities 15. In contrast, the potential of LLMs, especially local open-source LLMs, in\ntoken-level NER (i.e., span-based NER) in clinical settings, particularly for rare diseases, remains relatively unexplored.\nThe difference between document-level and token-level NER is illustrated in Figure 1. This gap is significant as\ntoken-level NER offers the potential for more detailed and precise entity recognition, which is crucial in clinical\napplications. For instance, to interpret clinical texts like chemotherapy treatment records, in a scenario where a\npatient’s record contains multiple chemotherapy sessions over several months, document-level NER may not efficiently\ndistinguish between the different treatment phases and their respective impacts. In contrast, token-level NER can\ncapture specific details. For example, consider a clinical text, “Patient experienced nausea after chemo session 1 in\nJanuary and significant fatigue following chemo session 3 in March.” While document-level NER might identify terms\nlike “chemo”, “nausea”, and “fatigue”, it fails to connect these conditions to specific treatment events. Token-level\nNER, however, can extract fine-grained details, such as linking “nausea” directly to “chemo session 1 in January”,\nproviding valuable context for understanding and tracking the patient’s treatment response over time. This precision is\nvital for personalized patient care.\nThis study aims to explore the effectiveness of LLMs in token-level NER for rare diseases within clinical texts. By\nfocusing on this specific application, we seek to understand the limitations and capabilities of LLMs in handling\nthe challenges of token-level entity recognition in a domain where data is scarce and highly specialized. Essentially,\nwe explore the performance of various state-of-the-art local open-source LLMs, including LLaMA-216, Meditron17,\nLlama2-MedTuned18, and UniversalNER14, on the task of NER of rare diseases and their phenotypes. Additionally,\nwe assess the performance of prominent models such as ChatGPT-3.5 and ChatGPT-4. Our investigation extends to\nexamining their capabilities in in-context learning, evaluating performance enhancements through few-shot learning,\nretrieval-augmented generation (RAG), and fine-tuning. The experimental results reveal that without proper fine-tuning,\nlocal LLMs generally struggle with token-level NER, despite prior training on clinical texts. We observe that while\nfew-shot learning can effectively improve the performance of most LLMs, the influence of RAG on the same task is\nrelatively minimal. Notably, our findings indicate that a medically-adapted LLaMA-2-7b model, specifically Llama2-\nMedTuned18, can outperform ChatGPT-4 on this task. This is particularly noteworthy as it achieves these results without\nhaving been trained specifically on the rare disease data used in the experiments, and this highlights the potential of\nfine-tuning local LLMs for specialized applications in clinical NER and other clinical NLP tasks.\nTable 1: Statistics of the RareDis dataset.\nData Type Training Validation Test Total\nDocuments 729 104 208 1041\nSentences 6281 894 1735 8910\nDisease 1328 187 392 1907\nRare Disease 3075 468 918 4461\nSkin Rare Disease 442 42 143 627\nSign 3384 474 966 4824\nSymptom 313 22 51 386\nRelated Work\nRecently, researchers have released various LLMs dedicated to the medical field. LLaMA13 and LLaMA-216 are one of\nthe first and most popular open-source LLMs, laying the foundation for extensive research and applications. Meditron17\nis a suite of open-source medical LLMs pre-trained on the GAP-Replay corpus, including clinical guidelines, research\npapers, and general domain data, and surpass the performance of LLaMA-2 16, ChatGPT-3.5, and Flan-PaLM19 on\nmultiple medical reasoning benchmarks.\nIn parallel, considerable efforts have been devoted to adapting these LLMs to the task of NER through prompting or fine-\ntuning13, 14, 15, 20, 21, 22, 23, 24. For example, Zhou et al. instruction-fine-tune the LLaMA model13 using ChatGPT-generated\nsynthetic data for NER from broad-coverage unlabeled web text14. Their UniversalNER model shows promising NER\nperformance across multiple domains. However, they only generate document-level outputs, i.e., a list of extracted\nentities in the given text as shown in Figure 2a, without considering their exact span information, which limits their\nimpact and usage in practical scenarios. Hu et al. explore the token-level NER capabilities of ChatGPT-3.5 and\nChatGPT-4 on two clinical NER tasks by manually crafting specific prompts20. However, they limit their exploration\nto ChatGPT models and do not investigate local and open-source LLMs. Another study that aligns closely with our\nresearch is by Shry et al., who focus on extracting rare diseases and their phenotypes at a document-level by prompting\nChatGPT-3.515.\nUnlike these studies, we aim to bridge the gap by exploring the capabilities of both local open-source LLMs and\nChatGPT models in the specific context of token-level NER in clinical settings. This approach seeks to understand how\nthese models perform in an area that is not only highly specialized but also characterized by its complexity and data\nscarcity, particularly in the study of rare diseases.\nMethods\nTask Overview Token-level named entity recognition (NER), also known as span-based NER, involves identifying\nand classifying named entities within a text into predefined categories such as diseases, symptoms, genes, etc. Formally,\nin a sentence with N tokens X = [x1, x2, . . . , xN ], an entity is defined as a contiguous span of tokens e = [xi, . . . , xj],\nwhere 0 ≤ i ≤ j ≤ N, and each is associated with a specific entity type. The core task is treated as a sequence\nlabeling problem, where the goal is to assign a corresponding sequence of labels Y = [y1, y2, . . . , yN ] to the sentence\nX. In this study, the BIO (Beginning, Inside, Outside) schema is employed for labeling. According to this scheme,\nthe first token of an entity of a certain type is tagged as B-type, any subsequent tokens within the same entity are\ntagged as I-type, and tokens not part of an entity are tagged as O. In this regard, the following eleven categories or\nlabels are used in the experiments:O, B-Disease, I-Disease, B-RareDisease, I-RareDisease, B-SkinRareDisease,\nI-SkinRareDisease, B-Symptom, I-Symptom, B-Sign, and I-Sign.\nIn this study, we aim to understand the capabilities of various general and medical LLMs in token-level NER, which\nis underexplored in clinical settings. Similar to Shry et al.’s work15, we choose rare diseases as our case study. Our\nfocus is particularly on extracting information about rare diseases and their associated phenotypes, driven by two\nprimary reasons: 1) Data related to rare diseases is scarce and highly specialized, presenting an ideal scenario to\n### Instruction: Your role involves identifying clinical Named Entities in the text and applying the BIO labeling scheme. Start by marking the beginning of a related phrase with B (Begin), and then continue with I (Inner) for the subsequent words within that phrase. Utilize the following labels to classify each entity: DISEASE: If the entity represents a non-rare disease. RAREDISEASE: … For each input token provided, generate a corresponding label. Ensure that each output is presented on a separate line, in the format of [input token : label]### Input: {sent}### Output:\n(a) zero-shot\n### Instruction: Your role involves identifying clinical Named Entities in the text and applying the BIO labeling scheme. Start by marking the beginning of a related phrase with B (Begin), and then continue with …### Input:{sent1}### Output: {output1}### Input:{sent2}### Output: {output2}…### Input:{sent}### Output: (b) few-shot\nWe have provided context information below. ---------------------{context}---------------------Given this information, please answer the question: ### Instruction: Your role involves identifying clinical Named Entities in the text and applying the BIO labeling scheme. Start by marking the beginning of a related phrase with B (Begin), and then continue with …### Input:{sent}### Output: (c) RAG\nFigure 2: Prompt templates for zero-shot, few-shot, and RAG-based inference.\nleverage the strengths of LLMs; and 2) Patients with rare diseases are a relatively understudied group, necessitating\nincreased research and advocacy efforts, as highlighted by Nguyen et al.25 To this end, our investigation spans across\nthe overall performance of both local open-source LLMs and ChatGPT models on this task. More specifically, we\ninitially explore the zero-shot performance of these LLMs using manually-designed prompts. Secondly, to enhance\nthe models’ performance for this specific task, we employ in-context learning strategies, i.e., few-shot learning and\nretrieval-augmented generation (RAG). These approaches provide rich information to the prompts to facilitate a deeper\nunderstanding of the task by the LLMs. Finally, we instruction-fine-tune Llama2-MedTuned 18 on the rare disease\ndataset to assess the performance of LLMs in a fully supervised way. Our intention is to explore how well LLMs, when\nfine-tuned with direct supervision, can adapt to and accurately identify entities in complex rare disease data.\nDataset In our experiments, we use the RareDis-v1 dataset 26, 27, a curated collection of texts from the National\nOrganization for Rare Disorders (NORD) database1. This dataset specifically comprises selected sections from NORD\narticles, which have been manually annotated to identify five key entity types: Disease, Rare Disease, Skin Rare\nDisease, Symptom, and Sign. The statistics of the dataset are detailed in Table 1, where the first two rows show the\nnumber of documents and sentences, respectively. The last four rows are a breakdown of the specific clinical entities\npresent in the dataset. It is important to distinguish between Sign, which are objectively observable indicators or test\nresults suggesting a disease, and Symptom, which are subjective experiences reported by the patient26.\nModels We consider the following LLMs for our experiments:\n• LLaMA-2-7b16 is one of the first and most popular local open-source pre-trained LLMs released. Though it is not\ndedicated to the clinical domain, the model is often considered a critical benchmark in recent related studies due\nto its impact and generalizability. For consistency, we utilize the 7-billion-parameter version of all local LLMs in\nthis study.\n• Meditron-7b17 is an adapted version of LLaMA-2 to the medical domain through continued pre-training on a\ncomprehensively curated medical corpus, including selected PubMed papers and abstracts, a new dataset of\ninternationally-recognized medical guidelines, and general domain data from RedPajama-v128. It outperforms\nLLaMA-2-7B on multiple medical reasoning tasks.\n1https://rarediseases.org/\nTable 2: Zero-shot performance of diverse LLMs on token-level NER.\nModel Specificity Source Disease Rare Disease\nP R F1 P R F1\nLLaMA-2 General Open-Source 0.0564 0.0149 0.0219 0.3548 0.1066 0.1494\nMeditron Medical Open-Source 0.0586 0.0242 0.0318 0.3409 0.1493 0.1877\nUniversalNER Task-Specific Open-Source 0.1544 0.0731 0.0936 0.5493 0.1634 0.2267\nLlama2-MedTuned Medical Open-Source 0.1391 0.7201 0.2332 0.3988 0.8080 0.5340\nChatGPT-3.5 General Proprietary 0.1173 0.3842 0.1798 0.5810 0.2513 0.3509\nChatGPT-4 General Proprietary 0.1993 0.4377 0.2739 0.5767 0.5761 0.5764\n• UniversalNER-7b14 is an instruction-fine-tuned version of LLaMA-7b 13 which is specified in named entity\nrecognition. Essentially, the authors prompt ChatGPT to generate instruction-tuning data for NER from web\ntext and conduct instruction-tuning on LLaMA. UniversalNER shows state-of-the-art document-level NER\nperformance on multiple benchmarks across diverse domains, including biomedicine.\n• Llama2-MedTuned-7b18 is a medically-adapted version of the LLaMA-2-7b16 model. Essentially, the authors\nspecifically create a medical instruction-based dataset consisting of approximately 200, 000 samples covering\nclinical NLP tasks including token-level named entity recognition, relation extraction, document classification,\nquestion answering, natural language inference, and conduct instruction-tuning on LLaMA. Llama2-MedTuned\ndemonstrates on-par performance with domain-specific models like BioClinicalBERT8 on several benchmarks.\n• ChatGPT-3.5 and ChatGPT-4 are arguably by far the most capable LLMs and have demonstrated superior\nperformance and robustness across various domains. However, their proprietary nature limits their use in the\nclinical domain, where data privacy is of utmost importance. Specifically, we use gpt-35-turbo-1106 and\ngpt-4-1106-preview in the experiments2.\nLearning Strategies In this study, we consider both in-context learning and instruction-fine-tuning to further adapt\nthe LLMs to the token-level NER task. For in-context learning, we employ few-shot learning and retrieval-augmented\ngeneration (RAG), the prompt templates of which are shown in Figure 2. Essentially, we randomly select 5 samples\nfrom the training set as the demonstration to the LLMs. To ensure consistency, we fix the5 samples for all few-shot\nexperiments in the study. For RAG implementation, we utilize the LlamaIndex framework, incorporating NORD rare\ndisease articles as the knowledge base. Specifically, these articles are segmented into chunks ( chunk size = 512),\nembedded using the bge-large-en-v1.5 model, and stored in a vector database. In the retrieval stage, we convert\na query using the embedding model and retrieve the top-k (similarity top k = 2) most relevant text chunks based\non similarity with the query embedding. The retrieved chunks are then integrated into the LLM prompts for enriched\ncontext.\nFor instruction-fine-tuning, we follow previous work18 and convert the RareDis dataset to the Stanford Alpaca29 format.\nThis format is structured for instruction-following, comprising three parts: an instruction, an input, and an output,\nas shown in Figure 2. We then fine-tune the Llama2-MedTuned-7b model on the transformed RareDis dataset. We\nuse LoRA adapters30 for the fine-tuning. It is worth noting that while instruction-fine-tuning (or instruction-tuning)\nsomewhat contradicts our initial intent due to its reliance on sufficient data, it remains crucial for our investigation into\nLLMs’ performance capabilities and limitations in comparison to smaller BERT-like models in data-rich scenarios.\nFurther implementation details are available in our code repository3.\nEvaluation We utilize Precision (P), Recall (R), and F1-score (F1) as our evaluation metrics for the token-level NER\ntask. For models designed for document-level NER, such as UniversalNER-7b14, we adapt their outputs to token-level\nby exact string matching. Initially, we assess a range of LLMs to investigate their overall performance. This is followed\n2We access the OpenAI models through Microsoft Azure’s HIPAA-certified platform.\n3https://github.com/qiuhaolu/tner\nTable 3: In-depth performance analysis of ChatGPT and Llama2-MedTuned across different entity types.\nPrompting Entity Type ChatGPT-3.5 ChatGPT-4 Llama2-MedTuned\nP R F1 P R F1 P R F1\nzero-shot\nDisease 0.1173 0.3842 0.1798 0.1993 0.4377 0.2739 0.1391 0.7201 0.2332\nRare Disease 0.5810 0.2513 0.3509 0.5767 0.5761 0.5764 0.3988 0.8080 0.5340\nSkin Rare Disease 0.1200 0.0833 0.0984 0.4762 0.1389 0.2151 0.0686 0.8542 0.1270\nSign 0.1314 0.0844 0.1028 0.1802 0.2665 0.2150 0.1712 0.3364 0.2269\nSymptom 0.0305 0.5882 0.0580 0.0574 0.7255 0.1063 0.0123 0.4902 0.0240\nAverage 0.1960 0.2783 0.1580 0.2980 0.4289 0.2773 0.1580 0.6418 0.2290\nfew-shot\nDisease 0.1871 0.2723 0.2218 0.2254 0.2977 0.2566 0.1517 0.5038 0.2332\nRare Disease 0.5615 0.5512 0.5563 0.6011 0.5901 0.5955 0.5106 0.8069 0.6254\nSkin Rare Disease 0.1231 0.3958 0.1878 0.2953 0.3958 0.3383 0.0823 0.8333 0.1498\nSign 0.1681 0.0813 0.1096 0.2749 0.2500 0.2619 0.1661 0.3283 0.2206\nSymptom 0.0478 0.6471 0.0889 0.0900 0.5490 0.1547 0.0140 0.5294 0.0273\nAverage 0.2175 0.3895 0.2329 0.2973 0.4165 0.3214 0.1849 0.6003 0.2513\nRAG\nDisease 0.1568 0.2952 0.2048 0.2211 0.3995 0.2847 0.1410 0.6412 0.2312\nRare Disease 0.6625 0.4595 0.5427 0.6552 0.5534 0.6000 0.4813 0.7918 0.5987\nSkin Rare Disease 0.1333 0.0833 0.1026 0.7419 0.1597 0.2629 0.0886 0.7986 0.1595\nSign 0.1586 0.0607 0.0807 0.2263 0.2037 0.2144 0.0955 0.0802 0.0872\nSymptom 0.0420 0.6078 0.0786 0.0482 0.7451 0.0906 0.0173 0.4314 0.0314\nAverage 0.2306 0.3013 0.2033 0.3785 0.4123 0.2905 0.1645 0.5486 0.2216\nby a more focused evaluation of those LLMs that demonstrate proficiency in token-level NER. Subsequently, we\nconduct a detailed analysis of the aforementioned learning strategies, examining their influence on the performance of\nthe LLMs. Additionally, we perform an error analysis to gain insights into the common errors across different models\nand learning strategies.\nResults\nLLM Initial Evaluation In our initial evaluation, we focus on assessing the performance of the aforementioned\nLLMs in token-level NER using the RareDis-v1 dataset, particularly targeting the two key target entities, i.e., Disease\nand Rare Disease. The results in Table 2 show that, generally, all the models struggle with this task, as evidenced\nby their modest performance metrics. Notably, Meditron, with its medical training, demonstrates marginally better\nresults than LLaMA-2. However, even UniversalNER, which is optimized for document-level NER, does not show\nsignificantly improved performance in this token-level task. In contrast, Llama2-MedTuned-7b demonstrates promising\nresults, closely trailing behind ChatGPT-4, which highlights the potential of local open-source LLMs in clinical NLP\napplications, specifically in token-level NER.\nFocused Evaluation of Token-Level NER Capable LLMs Table 3 provides an in-depth performance analysis of\nChatGPT-3.5, ChatGPT-4, and Llama2-MedTuned, selected based on their promising results in our initial evaluation. We\nencompass a range of experimental settings in this subsection, including zero-shot, few-shot, and retrieval-augmented\ngeneration (RAG). The discussion on the fine-tuning approach is reserved for the following subsection, as it requires\nthe use of the entire training dataset.\nA notable observation is the relatively high performance in identifying Rare Disease, likely due to its uniqueness\nand specificity. However, the models have varying degrees of difficulty in identifying subjective experiences reported\nby the patient, i.e., Symptom, as reflected by their close-to-zero precision and F1-scores. Moreover, we show that\nfew-shot learning can effectively improve the performance across all models, with an increase ranging from 3% to 8%\nin average F1 scores. We also observe that ChatGPT-3.5 shows a substantial improvement of20% in the F1-score for\nDisease Rare Disease Skin Rare Disease Sign Symptom0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8F1 Score\nLlama2-MedT uned Zero-Shot\nLlama2-MedT uned Few-Shot\nLlama2-MedT uned Fine-T une\nBioClinicalBERT\nGPT-4 Zero-Shot\nGPT-4 Few-Shot\nFigure 3: Performance evaluation of Llama2-MedTuned, BioClinicalBERT, and ChatGPT-4 across zero-shot, few-shot,\nand fine-tuning methods.\nRare Disease. These observations suggest that few-shot learning serves as an effective method to enhance LLMs’\nperformance in token-level NER.\nOn the other hand, while RAG shows some promise, its overall impact on enhancing model performance in this task is\nlimited. The data shows that RAG particularly benefits the identification of Rare Disease and Skin Rare Disease\nacross all three models. This improvement could be attributed to RAG’s ability to leverage external knowledge bases,\ni.e., NORD articles about rare diseases, which might be rich in rare disease terminology. However, for more general\nand common entities like Disease, Symptom and Sign, RAG’s advantages are less impressive. This highlights the need\nfor further refinement of RAG methods in clinical NLP applications.\nBesides zero-shot prompting and in-context learning, we also investigate the impact of instruction-fine-tuning on these\nLLMs in this task. As shown in Table 2 and Table 3, despite the performance gain via few-shot learning, the overall\nperformance of prompting LLMs is far from satisfactory. In this experiment, we aim to investigate LLMs’ performance\ncapabilities and limitations compared to fine-tuned BERT-like models under data-rich conditions, which is the common\nsolution to NER in current practice. The results are shown in Figure 3. Essentially, Figure 3 visualizes the F1 scores of\nvarious models, including BioClinicalBERT fine-tune, Llama2-MedTuned under different learning strategies (zero-shot,\nfew-shot, fine-tune), and ChatGPT-4 in zero-shot and few-shot scenarios. Specifically, the performance of Llama2-\nMedTuned models demonstrates significant effectiveness, not only outperforming ChatGPT-4 in most scenarios but also\nclosely rivaling BioClinicalBERT when fine-tuned. This highlights the potential of open-source LLMs in this task.\nError Analysis We present an error analysis to further understand the mistakes made by the LLMs in this task.\nEssentially, we randomly select 50 sentences from the test dataset and manually categorize the errors for each incorrect\nprediction. We investigate the two best-performing models, i.e., ChatGPT-4 under few-shot learning and Llama2-\nMedTuned under fine-tuning. We identify 4 types of errors in the experiment, i.e., Inaccurate Boundary, where the\nmodel incorrectly identifies the start or end of an entity; Wrong Type, where the entity is recognized, but its type is\nmisclassified; False Negative, where the model overlooks an entity present in the text; and False Positive, where the\nmodel mistakenly identifies a non-entity as an entity.\nThe results in Figure 4 show that Inaccurate Boundary errors are more prevalent for ChatGPT-4, suggesting challenges\nin precisely detecting entity boundaries. In contrast, Llama2-MedTuned demonstrates a much higher chance of False\nNegatives, indicating its inability to identify entities that are present. Both models show fewer Wrong Type and False\nPositive errors, reflecting a relatively stronger performance in correctly classifying entities and avoiding over-detection.\nInsights from these error patterns highlight the strengths and weaknesses of each model in token-level NER and indicate\ndirections for future improvements, especially in fine-tuning local open-source LLMs.\nInaccurate Boundary\n30.0%\nFalse Negative\n30.0%\nFalse Positive\n24.0%\nWrong T ype\n16.0%\n(a) ChatGPT-4\nInaccurate Boundary 20.5%\nFalse Negative\n59.1%\nFalse Positive\n9.1%\nWrong T ype\n11.4% (b) Llama2-MedTuned\nFigure 4: Comparative error analysis of ChatGPT-4 and Llama2-MedTuned.\nDiscussion and Conclusion\nIn this study, we investigate the capabilities of LLMs in token-level NER for rare diseases and their associated\nphenotypes on the RareDis-v1 dataset. Essentially, we find that in general, most LLMs struggle with this challenging\ntask. We also find that a medically-adapted LLaMA-2-7b model, i.e., Llama2-MedTuned, surpasses the performance of\nChatGPT-3.5 and matches the performance of ChatGPT-4 on this task, highlighting the potential of local open-source\nLLMs in clinical NLP applications. Our further analysis suggests that via in-context learning, i.e., few-shot learning\nand RAG, the performance can be improved while RAG has a relatively limited influence. We also show that after\nfine-tuning Llama2-MedTuned on this dataset, its performance gets boosted and significantly outperforms that of\nChatGPT-4, approaching the levels of BioClinicalBERT. It is worth noting that we use LoRA adapters to perform\nthe fine-tuning, instead of the entire model. Based on these results, we anticipate that full model fine-tuning could\npotentially enable the Llama2-MedTuned to match BioClinicalBERT’s performance.\nLarge language models (LLMs) have demonstrated superior performance in question-answering-related tasks across\nalmost all domains, including the clinical field. However, their performance in conventional NLP tasks, such as\ninformation extraction, has been questioned. Unlike existing studies that either focus on document-level NER or\nsolely rely on the prompt engineering of ChatGPT-series models, our experiment covers a broad scope of LLMs, from\nproprietary to local open-source and from general-purpose to medically adapted models. We thoroughly evaluate the\nLLMs on the token-level NER task, showing their struggle and potential directions for refinement. This study could\nshed light on adapting LLMs to specific NLP applications in clinical settings.\nThere are several factors that contribute to the struggle of LLMs on this task. Firstly, token-level NER is more\nchallenging than document-level NER. Token-level NER is a token classification task and places significant emphasis on\nthe representation of each individual token. This is in contrast to document-level NER which is a sequence classification\ntask where a pooled representation is often sufficient for predictions, making the role of individual token representation\nless critical. Secondly, the foundational pre-training objective of most LLMs (decoder-only transformers) is causal\nlanguage modeling, i.e., next token prediction. Unlike encoder-only transformers like BERT, LLMs don’t have an\nencoder structure that is pre-trained to maximize the model’s text representation power, making them less effective in\nclassification tasks. Thirdly, token-level NER often involves a variety of special, previously unseen symbols, such as\nthe diverse BIO tags. These unique elements can present additional complexities for LLMs to understand and operate.\nThere are also a few options to adapt these LLMs to the token-level NER task. A typical solution is continuous\npre-training, or instruction-fine-tuning, just as Llama2-MedTuned and what we do using the rare disease data. Basically,\nthe idea is to cast the NER problem as text generation, so the LLM can have a better understanding of the task and thus\nprocess it correctly in the same manner as it was pre-trained. For instance, Yang et al. integrate Human Phenotype\nOntology (HPO) labels into clinical notes and fine-tune GPT-based models (e.g., GPT-J) for phenotype recognition31.\nThey also conduct a comprehensive comparative analysis of encoder-based and decoder-based models for this task.\nAnother solution is to leverage LLMs to generate synthetic data and use the data to train smaller specified models. In\naddition to the data, one can also alter the model architecture of LLMs. For example, Li et al. propose to fine-tune\nLLaMA-2 models in a label-supervised way, by removing the causal mask from the decoders to enable bidirectional\nself-attention which is essential for token classification tasks like NER32. However, in our preliminary experiment, the\nmethod does not work as expected and the performance is worse than instruction-fine-tuning on the rare disease dataset.\nWe leave it for future study.\nThis study has certain limitations. Firstly, it does not explore the potential benefits of specific prompt engineering\ntechniques, such as utilizing feedback from error analysis to refine prompts and thereby enhance model performance.\nAdditionally, our approach mainly focuses on prompting LLMs to generate BIO tags. However, there are alternative\napproaches that are unexplored, such as the generation of text marked by special symbols to represent span information,\nwhich could offer a different perspective on model efficacy. Furthermore, in terms of instruction-fine-tuning of LLMs,\nour methodology is limited to employing LoRA adapters. A more extensive approach such as full model fine-tuning\nremains untested and could be valuable for future research.\nTo conclude, in this study, we explore the capabilities and limitations of existing LLMs, especially local open-source\nLLMs, in the task of token-level NER of rare diseases and their associated phenotypes. Through this exploration, we\naim to contribute to the advancement of LLM-based token-level NER methodologies in the clinical domain, ultimately\naiding in the improvement of patient care by enhancing the processing and understanding of clinical texts.\nAcknowledgments This work was conducted under support from the National Human Genome Research Institute\n(R01HG12748).\nReferences\n1. Henry J, Pylypchuk Y , Searcy T, Patel V , et al. Adoption of electronic health record systems among US non-federal\nacute care hospitals: 2008–2015. ONC data brief. 2016;35(35):2008-15.\n2. Kundeti SR, Vijayananda J, Mujjiga S, Kalyan M. Clinical named entity recognition: Challenges and opportunities.\nIn: 2016 IEEE International Conference on Big Data (Big Data). IEEE; 2016. p. 1937-45.\n3. Aronson AR, Lang FM. An overview of MetaMap: historical perspective and recent advances. Journal of the\nAmerican Medical Informatics Association. 2010;17(3):229-36.\n4. Denny JC, Irani PR, Wehbe FH, Smithers JD, Spickard III A. The KnowledgeMap project: development of\na concept-based medical school curriculum database. In: AMIA Annual Symposium Proceedings. vol. 2003.\nAmerican Medical Informatics Association; 2003. p. 195.\n5. Savova GK, Masanz JJ, Ogren PV , Zheng J, Sohn S, Kipper-Schuler KC, et al. Mayo clinical Text Analysis and\nKnowledge Extraction System (cTAKES): architecture, component evaluation and applications. Journal of the\nAmerican Medical Informatics Association. 2010;17(5):507-13.\n6. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need. In: Proceedings\nof the 31st International Conference on Neural Information Processing Systems (NeurIPS); 2017. p. 6000-10.\n7. Devlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, V olume 1 (NAACL-HLT); 2019. p. 4171-86.\n8. Alsentzer E, Murphy J, Boag W, Weng WH, Jin D, Naumann T, et al. Publicly Available Clinical BERT Embeddings.\nIn: Proceedings of the 2nd Clinical Natural Language Processing Workshop. Minneapolis, Minnesota, USA:\nAssociation for Computational Linguistics; 2019. p. 72-8. Available from: https://www.aclweb.org/anthology/\nW19-1909.\n9. Lewis P, Ott M, Du J, Stoyanov V . Pretrained Language Models for Biomedical and Clinical Tasks: Understanding\nand Extending the State-of-the-Art. In: Proceedings of the 3rd Clinical Natural Language Processing Workshop.\nOnline: Association for Computational Linguistics; 2020. p. 146-57. Available from: https://www.aclweb.org/\nanthology/2020.clinicalnlp-1.17.\n10. Sung M, Jeong M, Choi Y , Kim D, Lee J, Kang J. BERN2: an advanced neural biomedical named entity recognition\nand normalization tool. Bioinformatics. 2022;38(20):4837-9.\n11. Dharssi S, Wong-Rieger D, Harold M, Terry S. Review of 11 national policies for rare diseases in the context of\nkey patient needs. Orphanet journal of rare diseases. 2017;12(1):1-13.\n12. Thirunavukarasu AJ, Ting DSJ, Elangovan K, Gutierrez L, Tan TF, Ting DSW. Large language models in medicine.\nNature medicine. 2023;29(8):1930-40.\n13. Touvron H, Lavril T, Izacard G, Martinet X, Lachaux MA, Lacroix T, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:230213971. 2023.\n14. Zhou W, Zhang S, Gu Y , Chen M, Poon H. UniversalNER: Targeted Distillation from Large Language Models for\nOpen Named Entity Recognition. In: The Twelfth International Conference on Learning Representations; 2023. .\n15. Shyr C, Hu Y , Bastarache L, Cheng A, Hamid R, Harris P, et al. Identifying and Extracting Rare Diseases and\nTheir Phenotypes with Large Language Models. Journal of Healthcare Informatics Research. 2024:1-24.\n16. Touvron H, Martin L, Stone K, Albert P, Almahairi A, Babaei Y , et al. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:230709288. 2023.\n17. Chen Z, Cano AH, Romanou A, Bonnet A, Matoba K, Salvi F, et al. Meditron-70b: Scaling medical pretraining for\nlarge language models. arXiv preprint arXiv:231116079. 2023.\n18. Rohanian O, Nouriborji M, Clifton DA. Exploring the Effectiveness of Instruction Tuning in Biomedical Language\nProcessing. arXiv preprint arXiv:240100579. 2023.\n19. Chung HW, Hou L, Longpre S, Zoph B, Tay Y , Fedus W, et al. Scaling instruction-finetuned language models.\narXiv preprint arXiv:221011416. 2022.\n20. Hu Y , Chen Q, Du J, Peng X, Keloth VK, Zuo X, et al. Improving large language models for clinical named entity\nrecognition via prompt engineering. Journal of the American Medical Informatics Association. 2024:ocad259.\n21. Li Y , Li J, He J, Tao C. AE-GPT: using large language models to extract adverse events from surveillance reports-a\nuse case with influenza vaccine adverse events. Plos one. 2024;19(3):e0300919.\n22. Li Y , Viswaroopan D, He W, Li J, Zuo X, Xu H, et al. Improving Entity Recognition Using Ensembles of Deep\nLearning and Fine-tuned Large Language Models: A Case Study on Adverse Event Extraction from Multiple\nSources. arXiv preprint arXiv:240618049. 2024.\n23. Lu Q, Dou D, Nguyen TH. Parameter-Efficient Domain Knowledge Integration from Multiple Sources for\nBiomedical Pre-trained Language Models. In: Moens MF, Huang X, Specia L, Yih SWt, editors. Findings of the\nAssociation for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican Republic: Association for\nComputational Linguistics; 2021. p. 3855-65. Available from: https://aclanthology.org/2021.findings-emnlp.325.\n24. Lu Q, Dou D, Nguyen T. ClinicalT5: A Generative Language Model for Clinical Text. In: Goldberg Y ,\nKozareva Z, Zhang Y , editors. Findings of the Association for Computational Linguistics: EMNLP 2022. Abu\nDhabi, United Arab Emirates: Association for Computational Linguistics; 2022. p. 5436-43. Available from:\nhttps://aclanthology.org/2022.findings-emnlp.398.\n25. Nguyen CQ, Kariyawasam D, Alba-Concepcion K, Grattan S, Hetherington K, Wakefield CE, et al. ‘Advocacy\ngroups are the connectors’: Experiences and contributions of rare disease patient organization leaders in advanced\nneurotherapeutics. Health Expectations. 2022;25(6):3175-91.\n26. Mart´ınez-deMiguel C, Segura-Bedmar I, Chac´on-Solano E, Guerrero-Aspizua S. The RareDis corpus: a corpus\nannotated with rare diseases, their signs and symptoms. Journal of Biomedical Informatics. 2022;125:103961.\n27. Segura-Bedmar I, Camino-Perdones D, Guerrero-Aspizua S. Exploring deep learning methods for recognizing rare\ndiseases and their clinical manifestations from texts. BMC bioinformatics. 2022;23(1):263.\n28. Computer T. RedPajama: an Open Dataset for Training Large Language Models; 2023. Available from: https:\n//github.com/togethercomputer/RedPajama-Data.\n29. Taori R, Gulrajani I, Zhang T, Dubois Y , Li X, Guestrin C, et al.. Stanford Alpaca: An Instruction-following\nLLaMA model. GitHub; 2023. https://github.com/tatsu-lab/stanford alpaca.\n30. Hu EJ, Shen Y , Wallis P, Allen-Zhu Z, Li Y , Wang S, et al. LoRA: Low-Rank Adaptation of Large Language\nModels. In: International Conference on Learning Representations; 2022. Available from: https://openreview.net/\nforum?id=nZeVKeeFYf9.\n31. Yang J, Liu C, Deng W, Wu D, Weng C, Zhou Y , et al. Enhancing phenotype recognition in clinical notes using\nlarge language models: PhenoBCBERT and PhenoGPT. Patterns. 2024;5(1).\n32. Li Z, Li X, Liu Y , Xie H, Li J, Wang Fl, et al. Label supervised llama finetuning. arXiv preprint arXiv:231001208.\n2023.",
  "topic": "Context (archaeology)",
  "concepts": [
    {
      "name": "Context (archaeology)",
      "score": 0.7224501371383667
    },
    {
      "name": "Security token",
      "score": 0.5909098982810974
    },
    {
      "name": "Computer science",
      "score": 0.5024824142456055
    },
    {
      "name": "Health care",
      "score": 0.45512837171554565
    },
    {
      "name": "Task (project management)",
      "score": 0.43785494565963745
    },
    {
      "name": "Political science",
      "score": 0.2698715329170227
    },
    {
      "name": "Computer security",
      "score": 0.21801704168319702
    },
    {
      "name": "Geography",
      "score": 0.16497734189033508
    },
    {
      "name": "Engineering",
      "score": 0.155775249004364
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": []
}