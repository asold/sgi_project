{
  "title": "Large Language Models to the Rescue: Reducing the Complexity in Scientific Workflow Development Using ChatGPT",
  "url": "https://openalex.org/W4388481593",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4284194683",
      "name": "Sänger, Mario",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4377461040",
      "name": "De Mecquenem, Ninon",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Lewińska, Katarzyna Ewa",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Bountris, Vasilis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281522995",
      "name": "Lehmann, Fabian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2247720008",
      "name": "Leser, Ulf",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3162189702",
      "name": "Kosch, Thomas",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2169456326",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W4308643133",
    "https://openalex.org/W2920756737",
    "https://openalex.org/W4319452276",
    "https://openalex.org/W2900551880",
    "https://openalex.org/W4361002760",
    "https://openalex.org/W3213394393",
    "https://openalex.org/W4322631505",
    "https://openalex.org/W4321649710",
    "https://openalex.org/W2970113232",
    "https://openalex.org/W3093871960",
    "https://openalex.org/W2989308741",
    "https://openalex.org/W4286762022",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4388581056",
    "https://openalex.org/W2951912016",
    "https://openalex.org/W2609265534",
    "https://openalex.org/W4367672983",
    "https://openalex.org/W3006653136",
    "https://openalex.org/W2202529936",
    "https://openalex.org/W4308538981",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3044019695",
    "https://openalex.org/W4224105048",
    "https://openalex.org/W4389520749",
    "https://openalex.org/W4311887664",
    "https://openalex.org/W2964849163",
    "https://openalex.org/W2572857085",
    "https://openalex.org/W2124985265",
    "https://openalex.org/W4366549767",
    "https://openalex.org/W2131271579",
    "https://openalex.org/W4377121468",
    "https://openalex.org/W2143722357",
    "https://openalex.org/W2922388164",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4221055872",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W2168103533",
    "https://openalex.org/W2115570304",
    "https://openalex.org/W2078962046",
    "https://openalex.org/W2170551349",
    "https://openalex.org/W3163363684",
    "https://openalex.org/W4225165463",
    "https://openalex.org/W4399203759",
    "https://openalex.org/W2983506074",
    "https://openalex.org/W3111758815",
    "https://openalex.org/W4284664028",
    "https://openalex.org/W2967150655",
    "https://openalex.org/W2605897695",
    "https://openalex.org/W4366729084",
    "https://openalex.org/W4225120919",
    "https://openalex.org/W4366548479",
    "https://openalex.org/W4386185625",
    "https://openalex.org/W2036897871",
    "https://openalex.org/W3091798252",
    "https://openalex.org/W4378783702",
    "https://openalex.org/W3137279194",
    "https://openalex.org/W2128548442",
    "https://openalex.org/W2804047300",
    "https://openalex.org/W3042418592",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4385302156",
    "https://openalex.org/W4378464864",
    "https://openalex.org/W4387225870",
    "https://openalex.org/W3118421494",
    "https://openalex.org/W4321162272",
    "https://openalex.org/W2110417468",
    "https://openalex.org/W3200776886",
    "https://openalex.org/W2084821192",
    "https://openalex.org/W2141458291",
    "https://openalex.org/W4318618520",
    "https://openalex.org/W3203002808",
    "https://openalex.org/W3131055609",
    "https://openalex.org/W4225108562",
    "https://openalex.org/W4322718421"
  ],
  "abstract": "Scientific workflow systems are increasingly popular for expressing and executing complex data analysis pipelines over large datasets, as they offer reproducibility, dependability, and scalability of analyses by automatic parallelization on large compute clusters. However, implementing workflows is difficult due to the involvement of many black-box tools and the deep infrastructure stack necessary for their execution. Simultaneously, user-supporting tools are rare, and the number of available examples is much lower than in classical programming languages. To address these challenges, we investigate the efficiency of Large Language Models (LLMs), specifically ChatGPT, to support users when dealing with scientific workflows. We performed three user studies in two scientific domains to evaluate ChatGPT for comprehending, adapting, and extending workflows. Our results indicate that LLMs efficiently interpret workflows but achieve lower performance for exchanging components or purposeful workflow extensions. We characterize their limitations in these challenging scenarios and suggest future research directions.",
  "full_text": "Large Language Models to the Rescue: Reducing the Complexity in Scientific Workflow\nDevelopment Using ChatGPT\nMario S¨angera,∗, Ninon De Mecquenema, Katarzyna Ewa Lewi´nskab,c, Vasilis Bountrisa, Fabian Lehmanna, Ulf Lesera,∗,\nThomas Koscha,∗\naHumboldt Universit¨ at zu Berlin, Department of Computer Science, Unter den Linden 6, Berlin, 10099, Germany\nbHumboldt Universit¨ at zu Berlin, Department of Geography, Unter den Linden 6, Berlin, 10099, Germany\ncUniversity of Wisconsin-Madison, Department of Forest and Wildlife Ecology, 1630 Linden Drive, Madison, WI, 53706, United States\nAbstract\nScientific workflow systems are increasingly popular for expressing and executing complex data analysis pipelines over large\ndatasets, as they o ffer reproducibility, dependability, and scalability of analyses by automatic parallelization on large compute\nclusters. However, implementing workflows is difficult due to the involvement of many black-box tools and the deep infrastructure\nstack necessary for their execution. Simultaneously, user-supporting tools are rare, and the number of available examples is much\nlower than in classical programming languages. To address these challenges, we investigate the e fficiency of Large Language\nModels (LLMs), specifically ChatGPT, to support users when dealing with scientific workflows. We performed three user studies in\ntwo scientific domains to evaluate ChatGPT for comprehending, adapting, and extending workflows. Our results indicate that LLMs\nefficiently interpret workflows but achieve lower performance for exchanging components or purposeful workflow extensions. We\ncharacterize their limitations in these challenging scenarios and suggest future research directions.\nKeywords: Large Language Models, Scientific Workflows, User Support, ChatGPT\n1. Introduction\nLarge-scale data analysis pipelines (also known as scientific\nworkflows) are crucial in driving research advances for natural\nsciences [1]. They are pivotal in accelerating large and com-\nplex data analysis on distributed infrastructures and o ffer es-\nsential features, such as reproducibility and dependability [2].\nIn bioinformatics, for instance, scientific workflows are ana-\nlyzing the terabyte-large data sets produced by modern DNA or\nRNA sequencing machines in a wide variety of experiments [3],\nthereby aiding in building a comprehensive understanding of bi-\nological processes and human diseases. Bioinformatics work-\nflows typically include many individual computational steps,\nsuch as data pre-processing, extensive quality control, aggrega-\ntion of raw sequencing data into consensus sequences, machine-\nlearning-based tasks for classification and clustering, statistical\nassessments, and result visualization. Each step is carried out\nby a specific program, typically not written by the workflow\ndeveloper but exchanged within a worldwide community of re-\nsearchers [4]. Execution of a workflow on a distributed infras-\ntructure, in principle, is taken care of by a workflow engine;\nhowever, the idiosyncrasies of the different infrastructures (e.g.,\nfile system, number and features of compute nodes, applied re-\nsource manager, and scheduler) often require workflow users to\ntune their scripts individually for every new system [5].\nHowever, typical developers of workflows are researchers\nfrom heterogeneous scientific fields who possess expertise in\n∗Corresponding authors\ntheir respective domains but often lack in-depth knowledge in\nsoftware development or distributed computing. They often en-\ncounter difficulties understanding the complex implementations\nof exchanged codes and the deep infrastructure stack necessary\nfor their distributed execution. This situation challenges e ffi-\ncient workflow implementation, slows down or hinders data ex-\nploration and scientific innovation processes [6]. Consequently,\nlow human productivity is a significant bottleneck in the cre-\nation, adaption, and interpretation of scientific workflows [7].\nParallel to this, there is a well-established field in human-\ncomputer interaction focusing on assisting end-user program-\nmers and software development, as highlighted by previous work\n[8, 9, 10, 11], to reduce the perceived cognitive workload and\nimprove the overall programmer performance [12]. Research\nin this field includes programming-by-demonstration [13, 14],\nvisual programming [15, 16], and natural language instructions\n[17]. Recent work in this area particularly investigated prospects\nof general-purpose Large Language Models (LLMs), such as\nChatGPT [18], LLaMA [19] and Bloom[20], for supporting\nend-user-programming [21, 22, 23] and software development\nin general [24, 25]. For instance, Bimbatti et al. [21] explore us-\ning ChatGPT to enhance natural language understanding within\nan end-user development environment, assisting non-expert users\nin developing programs for collaborative robots. Moreover,\nWhite et al. [24] introduce prompt design techniques for au-\ntomating typical software engineering tasks, including ensur-\ning code independence from third-party libraries and generating\nan API specification from a list of requirements. Surameery et\nal. [25] evaluate LLMs for supporting code debugging. How-\nPreprint submitted to Elsevier November 7, 2023\narXiv:2311.01825v2  [cs.DC]  6 Nov 2023\never, results from such studies, which focus on standard pro-\ngramming languages, cannot easily be transferred to workflow\nsystems. Workflow scripts mostly call external tools with ag-\nnostic names and have little recognizable control structures or\nprotected keywords. Publicly available examples are scarce;\nfor instance, the community repository of the popular work-\nflow system Nextflow [26] currently o ffers only 55 released\nworkflows1. Furthermore, workflows can only be understood\nwhen the distributed system underlying their execution is con-\nsidered, creating dependencies much di fferent than usual pro-\ngrams. Moreover, studies investigating how LLMs support users\nin data science - the field in which workflows are applied ex-\ntensively - do not address the unique characteristics of scien-\ntific workflows either and are limited to theoretical considera-\ntions [27, 28]. Practical studies, especially those involving real\nusers, are badly missing.\nIn this work, we address these shortcomings by describing\nthree user-studies in two different scientific fields (biomedicine\nand Earth observation) that evaluate the suitability of ChatGPT\nfor comprehending, modifying, and extending scientific work-\nflows. Specifically, we evaluate the correctness of ChatGPT re-\ngarding explainability, exchange of software components, and\nextension when providing real-world scientific workflow scripts.\nOur results show a high accuracy for comprehending and ex-\nplaining scientific workflows but reduced performance for mod-\nifying and extending workflow scripts. The domain experts pos-\nitively assessed the explainability in qualitative inquiries, em-\nphasizing the time-saving capabilities of using LLMs while en-\ngineering existing workflows. Overall, our work indicates that\ngeneral-purpose LLMs have the potential to improve human\nperformance when analyzing complex scientific workflows.\n2. Related Work\nPrevious research investigated how related domains, such\nas programming, can be augmented using interactive technolo-\ngies [11, 29]. In contrast to programming, where applications\nuse a single programming language and are often executed on a\nsingle system, scientific workflows combine multiple software\nartifacts on distributed stacks for advanced data processing. We\nground the reader by providing a literature review about scien-\ntific workflows and introducing large language models, includ-\ning their utility to facilitate the creation of software artifacts.\n2.1. Scientific Workflows\nScientific workflows are widely used by diverse research\ncommunities, such as biomedicine [30], astronomy [31], clima-\ntology [32], and Earth observation [33] to manage the dataflow\nand distributed execution of complex analyses, simulations, and\nexperiments. A scientific workflow comprises a series of inter-\nconnected computational steps, often with diverse patterns of\ndependencies, that define how to process and analyze data to\nreach a particular research objective. Scientific workflows can\n1https://nf-co.re/stats - last access 20-10-2023\nbe regarded as directed acyclic graphs in which the nodes rep-\nresent computational tasks or operations and edges model de-\npendencies or dataflow between these tasks. An edge from one\nnode to another signifies that the output of the first task is used\nas input for the second [34]. For example, Figure 1a illustrates\nthe computational steps, the tools, and the data flow of a bioin-\nformatics workflow for performing differential gene expression\nanalysis. During workflow execution, a single computation step\noften involves multiple processes, which are typically executed\nin a distributed fashion on di fferent machines and batches of\nthe input data, resulting in a much more complex execution\ngraph. Consequently, scientific workflows help facilitate the\nreproducibility and traceability of data analyses by explicitly\noutlining the steps and parameters involved [35]. Furthermore,\nthey allow for automation, scaling, and optimization of com-\nputational processes, which is especially critical in disciplines\ndealing with large datasets [36]. The increasing importance of\nscientific workflows for scientific progress has led to a grow-\ning interest in developing more user-friendly tools and methods\nthrough the research community. Scientific workflow manage-\nment systems, like Apache Airflow [37], Galaxy [38], Nextflow\n[39], Pegasus [40], and Snakemake [41], are specifically de-\nveloped to support users in designing and executing scientific\nworkflows in various aspects. Key features of such manage-\nment systems typically include workflow design and compo-\nsition, (distributed) workflow execution and scheduling, prove-\nnance tracking, recovery and failure handling, and resource man-\nagement [35]. Figure 1b highlights the implementation of the\nexample workflow as well as a single computational step (see\nFigure 1c), i.e., reference genome alignment using the STAR\ntoolkit in Nextflow.\nScientific workflows are often reused and adopted for com-\nplex data analysis [6]. Most of the time, users of scientific\nworkflows are unaware of the workflow’s internal functional-\nity and technical details. Instead, users of scientific workflows\nrepresent domain experts, such as mathematicians, physicians,\nor bioinformatics, who are experts in their respective domains\nbut not necessarily in programming and interpreting scientific\nworkflows. Existing scientific workflows were implemented\nand maintained by persons other than the domain user. This re-\nduces the direct interaction with scientific workflows to a min-\nimum, where domain experts only hand in the input data and\nevaluate the output data. Consequently, domain experts using a\nworkflow often do not have the knowledge to modify, extend,\nor interpret the details of scientific workflows.\n2.2. Large Language Models\nLanguage Models, such as BERT [42], GPT-3 [43], Bloom\n[20] and PaLM-2 [44], build the foundation of many recent ad-\nvancements in natural language processing and understanding.\nThese models have billions of parameters and are generally pre-\ntrained on vast sets of texts from the web and other repositories,\nenabling them to encode syntactic and semantic relationships\nin human language. As a consequence, generative language\nmodels, such as ChatGPT, LLaMA [19], and LAMDA [45],\ncan produce machine-generated high-quality text that is indis-\ntinguishable from human writing. These generative capabili-\n2\nReference\nGenome\nRNA-seq\nInput Files\nReference\nAnnotation\nCheck\nStrandedness\nQuality Control\nFASTP\nReference Genome\nAlignment\nSTAR\nReference\nGenome Indexing\nSTAR\nFormat\nConversion\nSAMtools\nTranscript\nQuantification\nCufflinks\nLoad input\nreads\nReads\nGenome\nIndex\nFiltered\nReads\nStranded-\nness\nAligned\nReads\nAligned\nReads\nReference\nCDNA\nAdditional input\ninformation\nnextflow.enable.dsl = 2\ninclude { CHECK_STRANDNESS } \n         from './modules/check_strandness.nf'\ninclude { FASTP } from './modules/fastp'\ninclude { STAR_INDEX_REFERENCE ; STAR_ALIGN } \n         from './modules/star.nf'\ninclude { SAMTOOLS } from './modules/samtools'\ninclude { CUFFLINKS } from './modules/cufflinks'\nworkflow {\n    # Read input sequences \n    read_pairs_ch = channel.fromFilePairs(params.reads)\n    \n    # Determine strandedness of the samples \n    CHECK_STRANDNESS(read_pairs_ch,\n                     params.reference_cdna, \n                     params.reference_annotation)\n    # Read trimming and filtering \n    FASTP(read_pairs_ch)\n    \n    # Generate genome alignement index \n    STAR_INDEX_REFERENCE(params.reference_genome, \n                         params.reference_annotation)\n    # Perform alignment to reference genome \n    STAR_ALIGN(CHECK_STRANDNESS.out, \n               FASTP.out.sample_trimmed, \n               STAR_INDEX_REFERENCE.out, \n               params.reference_annotation)\n    # Convert data format \n    SAMTOOLS(STAR_ALIGN.out.sample_sam)\n    # Run differential expression analysis \n    CUFFLINKS(CHECK_STRANDNESS.out, \n              SAMTOOLS.out.sample_bam, \n              params.reference_annotation)\n}\nprocess STAR_ALIGN {\n    label 'star'\n    publishDir params.outdir\n    input:\n    env STRANDNESS\n        tuple val(sample_name), path(reads)\n        path(index)\n        path(annotation)\n    output:\n        tuple val(sample_name), \n        path(\"${sample_name}*.sam\"), \n        emit: sample_sam\n    shell:\n      '''\n if [[ ($STRANDNESS == \"firststrand\") || \n       ($STRANDNESS == \"secondstrand\") ]]; then\nSTAR \\\\\n  --genomeDir . \\\\\n  --readFilesIn !{reads[0]} !{reads[1]} \\\\\n  --runThreadN !{params.threads} \\\\\n  --outFileNamePrefix !{sample_name}. \\\\\n  --sjdbGTFfile !{annotation} \\\\\n  --alignSoftClipAtReferenceEnds No \\\\\n  --outFilterIntronMotifs RemoveNoncanonical \\\\\n  --outSAMattrIHstart 0\n  elif [[ $STRANDNESS == \"unstranded\" ]]; then\n    STAR \\\\\n  --genomeDir . \\\\\n  --readFilesIn !{reads[0]} !{reads[1]} \\\\\n  --alignSoftClipAtReferenceEnds No \\\\\n  --outSAMstrandField intronMotif \\\\\n  --outFilterIntronMotifs RemoveNoncanonical \\\\\n  --runThreadN !{params.threads} \\\\\n  --outFileNamePrefix !{sample_name}. \\\\\n  --sjdbGTFfile !{annotation} \\\\\n  --outSAMattrIHstart 0\n  fi\n'''\n}\nSchema of the Workflow Workflow Description in Nextflow Description of a Single Step / Task\nTranscriptsAlignment\nStatistics\nQuality\nReport\n(a)\nReference\nGenome\nRNA-seq\nInput Files\nReference\nAnnotation\nCheck\nStrandedness\nQuality Control\nFASTP\nReference Genome\nAlignment\nSTAR\nReference\nGenome Indexing\nSTAR\nFormat\nConversion\nSAMtools\nTranscript\nQuantification\nCufflinks\nLoad input\nreads\nReads\nGenome\nIndex\nFiltered\nReads\nStranded-\nness\nAligned\nReads\nAligned\nReads\nReference\nCDNA\nAdditional input\ninformation\nnextflow.enable.dsl = 2\ninclude { CHECK_STRANDNESS } \n         from './modules/check_strandness.nf'\ninclude { FASTP } from './modules/fastp'\ninclude { STAR_INDEX_REFERENCE ; STAR_ALIGN } \n         from './modules/star.nf'\ninclude { SAMTOOLS } from './modules/samtools'\ninclude { CUFFLINKS } from './modules/cufflinks'\nworkflow {\n    # Read input sequences \n    read_pairs_ch = channel.fromFilePairs(params.reads)\n    \n    # Determine strandedness of the samples \n    CHECK_STRANDNESS(read_pairs_ch,\n                     params.reference_cdna, \n                     params.reference_annotation)\n    # Read trimming and filtering \n    FASTP(read_pairs_ch)\n    \n    # Generate genome alignement index \n    STAR_INDEX_REFERENCE(params.reference_genome, \n                         params.reference_annotation)\n    # Perform alignment to reference genome \n    STAR_ALIGN(CHECK_STRANDNESS.out, \n               FASTP.out.sample_trimmed, \n               STAR_INDEX_REFERENCE.out, \n               params.reference_annotation)\n    # Convert data format \n    SAMTOOLS(STAR_ALIGN.out.sample_sam)\n    # Run differential expression analysis \n    CUFFLINKS(CHECK_STRANDNESS.out, \n              SAMTOOLS.out.sample_bam, \n              params.reference_annotation)\n}\nprocess STAR_ALIGN {\n    label 'star'\n    publishDir params.outdir\n    input:\n    env STRANDNESS\n        tuple val(sample_name), path(reads)\n        path(index)\n        path(annotation)\n    output:\n        tuple val(sample_name), \n        path(\"${sample_name}*.sam\"), \n        emit: sample_sam\n    shell:\n      '''\n if [[ ($STRANDNESS == \"firststrand\") || \n       ($STRANDNESS == \"secondstrand\") ]]; then\nSTAR \\\\\n  --genomeDir . \\\\\n  --readFilesIn !{reads[0]} !{reads[1]} \\\\\n  --runThreadN !{params.threads} \\\\\n  --outFileNamePrefix !{sample_name}. \\\\\n  --sjdbGTFfile !{annotation} \\\\\n  --alignSoftClipAtReferenceEnds No \\\\\n  --outFilterIntronMotifs RemoveNoncanonical \\\\\n  --outSAMattrIHstart 0\n  elif [[ $STRANDNESS == \"unstranded\" ]]; then\n    STAR \\\\\n  --genomeDir . \\\\\n  --readFilesIn !{reads[0]} !{reads[1]} \\\\\n  --alignSoftClipAtReferenceEnds No \\\\\n  --outSAMstrandField intronMotif \\\\\n  --outFilterIntronMotifs RemoveNoncanonical \\\\\n  --runThreadN !{params.threads} \\\\\n  --outFileNamePrefix !{sample_name}. \\\\\n  --sjdbGTFfile !{annotation} \\\\\n  --outSAMattrIHstart 0\n  fi\n'''\n}\nSchema of the Workflow Workflow Description in Nextflow Description of a Single Step / Task\nTranscriptsAlignment\nStatistics\nQuality\nReport (b)\nReference\nGenome\nRNA-seq\nInput Files\nReference\nAnnotation\nCheck\nStrandedness\nQuality Control\nFASTP\nReference Genome\nAlignment\nSTAR\nReference\nGenome Indexing\nSTAR\nFormat\nConversion\nSAMtools\nTranscript\nQuantification\nCufflinks\nLoad input\nreads\nReads\nGenome\nIndex\nFiltered\nReads\nStranded-\nness\nAligned\nReads\nAligned\nReads\nReference\nCDNA\nAdditional input\ninformation\nnextflow.enable.dsl = 2\ninclude { CHECK_STRANDNESS } \n         from './modules/check_strandness.nf'\ninclude { FASTP } from './modules/fastp'\ninclude { STAR_INDEX_REFERENCE ; STAR_ALIGN } \n         from './modules/star.nf'\ninclude { SAMTOOLS } from './modules/samtools'\ninclude { CUFFLINKS } from './modules/cufflinks'\nworkflow {\n    # Read input sequences \n    read_pairs_ch = channel.fromFilePairs(params.reads)\n    \n    # Determine strandedness of the samples \n    CHECK_STRANDNESS(read_pairs_ch,\n                     params.reference_cdna, \n                     params.reference_annotation)\n    # Read trimming and filtering \n    FASTP(read_pairs_ch)\n    \n    # Generate genome alignement index \n    STAR_INDEX_REFERENCE(params.reference_genome, \n                         params.reference_annotation)\n    # Perform alignment to reference genome \n    STAR_ALIGN(CHECK_STRANDNESS.out, \n               FASTP.out.sample_trimmed, \n               STAR_INDEX_REFERENCE.out, \n               params.reference_annotation)\n    # Convert data format \n    SAMTOOLS(STAR_ALIGN.out.sample_sam)\n    # Run differential expression analysis \n    CUFFLINKS(CHECK_STRANDNESS.out, \n              SAMTOOLS.out.sample_bam, \n              params.reference_annotation)\n}\nprocess STAR_ALIGN {\n    label 'star'\n    publishDir params.outdir\n    input:\n    env STRANDNESS\n        tuple val(sample_name), path(reads)\n        path(index)\n        path(annotation)\n    output:\n        tuple val(sample_name), \n        path(\"${sample_name}*.sam\"), \n        emit: sample_sam\n    shell:\n      '''\n if [[ ($STRANDNESS == \"firststrand\") || \n       ($STRANDNESS == \"secondstrand\") ]]; then\nSTAR \\\\\n  --genomeDir . \\\\\n  --readFilesIn !{reads[0]} !{reads[1]} \\\\\n  --runThreadN !{params.threads} \\\\\n  --outFileNamePrefix !{sample_name}. \\\\\n  --sjdbGTFfile !{annotation} \\\\\n  --alignSoftClipAtReferenceEnds No \\\\\n  --outFilterIntronMotifs RemoveNoncanonical \\\\\n  --outSAMattrIHstart 0\n  elif [[ $STRANDNESS == \"unstranded\" ]]; then\n    STAR \\\\\n  --genomeDir . \\\\\n  --readFilesIn !{reads[0]} !{reads[1]} \\\\\n  --alignSoftClipAtReferenceEnds No \\\\\n  --outSAMstrandField intronMotif \\\\\n  --outFilterIntronMotifs RemoveNoncanonical \\\\\n  --runThreadN !{params.threads} \\\\\n  --outFileNamePrefix !{sample_name}. \\\\\n  --sjdbGTFfile !{annotation} \\\\\n  --outSAMattrIHstart 0\n  fi\n'''\n}\nSchema of the Workflow Workflow Description in Nextflow Description of a Single Step / Task\nTranscriptsAlignment\nStatistics\nQuality\nReport (c)\nFigure 1: Example bioinformatics workflow for di fferential gene expression analysis created by a domain expert recruited in our study. The figure highlights the\nconceptual schema of the workflow (a), its implementation in Nextflow (b), and the implementation of one single step (c), i.e., reference genome alignment using\nthe STAR tool. The workflow comprises six computational steps in total. For each step the used tool is given in blue below the task name.\nties have empowered these models to assist in diverse (creative)\nwriting tasks and have been utilized to facilitate a wide range\nof interactive language-based applications within the HCI com-\nmunity [46, 47, 48, 49, 50, 51]. For instance, WordCraft [46]\ninvestigate the utilization of LLMs to aid fiction writers in tasks\nranging from transforming a text to resemble a “Dickensian”\nstyle to providing suggestions to combat writer’s block. Their\nfindings indicate that writers found such text-generating models\nbeneficial even when the generated text is not perfect. Petridis\net al. [47] introduce AngleKindling, an interactive tool that\nemploys LLMs to support journalists exploring different angles\nfor reporting on a press release. Their study with twelve pro-\nfessional journalists shows that participants found the system\nconsiderably more helpful and less mentally demanding than\ncompetitor brainstorming tools. Other applications include pro-\ntotyping support [48], generation of titles and synopses from\nkeywords [51], conversational interactions with mobile user in-\nterfaces [49], and conceptional blending [50].\nNext to their text writing capabilities, language models are\nfurther known to retain commonsense knowledge within their\ntraining data, effectively transforming them into accessible knowl-\nedge stores that can be seamlessly queried using natural lan-\nguage prompts. For instance, experiments with BERT [42] high-\nlight that the model performance is competitive with traditional\ninformation extraction and open-domain question answering.\nFurthermore, recent studies show the potential of using Chat-\nGPT for knowledge base construction, inspired by the fact that\nthese language models have been pre-trained on vast internet-\nscale corpora that encompass diverse knowledge domains [52].\nHowever, it is worth noting that LLMs are known to frequently\ngenerate hallucinations, which are outputs that, while statisti-\ncally plausible and seemingly believable, are factually incor-\nrect [53, 54].\n2.3. Using LLMs to Support Programming\nThe ability to generate new text and to reconstruct existing\ninformation makes LLMs highly appropriate to support users in\nsoftware development, as programming often requires not only\nthe creation of novel code segments tailored to current require-\nments and tasks but also depends on the application of estab-\nlished algorithms, software libraries, and best practices. Ac-\ncordingly, a large number of papers investigate LLMs specially\ntrained for code generation [55, 56, 57, 58] as well as di ffer-\nent approaches leveraging these models to provide interactive\nprogrammer support [59, 60, 61, 23]. For instance, Jiang et al.\n[60] discuss GenLine, a natural language code synthesis tool\nbased on a generative LLM and problem-specific prompts for\ncreating or changing program code. The findings from a user\nstudy indicate that the approach can provide valuable support to\ndevelopers. However, they also encounter several challenges,\nsuch as participants finding it difficult to form an accurate men-\ntal model of the kinds of requests that the model can reliably\ntranslate. Similarly, Vaithilingam et al. [61] conducted a user\nstudy with 24 participants evaluating their usage and experi-\n3\nences using the GitHub Copilot 2 code generation model while\nprogramming. The authors find that the synthesized code of-\nten provided a helpful starting point and saved online searching\nefforts. However, participants encountered issues with under-\nstanding, editing, and debugging code snippets from Copilot,\nresulting in not necessarily improved task completion times and\nsuccess rates. These findings align with the results of similar\nstudies [62]. However, a controlled experiment in [63] records\na positive effect of code generators when used in introductory\nprogramming courses for minors.\nThe use of generative LLMs and code generators has been\nscarcely explored in scientific data analysis and not yet for sci-\nentific workflows. Liu et al. [23] examine the Codex code gen-\nerator [55] in the context of data analysis in spreadsheets for\nnon-expert end-user programmers. Moreover, several studies\ninvestigate the utilization for data visualization [64, 65]. For\nexample, the study by Maddigan et al. [64] evaluates the e ffi-\nciency of ChatGPT, Codex, and GPT-3 in producing scripts to\ncreate visualizations based on natural language queries. The\nstudies that have the most overlap with our work regarding the\nintention to support the design of data analysis pipelines are\ngiven by Ubani et al. [65] and Zahra et al. [66]. In the case\nof the former, ChatGPT is used to build a conversational, natu-\nral language-based interface between users and the scikit-learn\nmachine learning framework [67] supporting users in several\nphases of a machine learning project ranging from initial task\nformulation to comprehensive result interpretation. For the lat-\nter, Laminar, a framework for serverless computing, is pro-\nposed, which o ffers possibilities for code searching, summa-\nrization, and completion. However, the framework is solely fo-\ncused on Python implementations.\n3. Methodology\nThis section describes the study methodology. We begin\nby outlining about the general study approach. Then, we ex-\nplain the research process for each experiment. Based on re-\nlated work and the objectives of our research, we state the fol-\nlowing research questions:\nRQ1: How performant is ChatGPT for comprehending and ex-\nplaining scientific workflows?\nRQ2: How suitable is ChatGPT in suggesting and applying mod-\nifications for scientific workflows?\nRQ3: How efficient is ChatGPT in extending scientific work-\nflows?\n3.1. General Study Design\nTo answer our research questions, we investigate the capa-\nbilities of ChatGPT, a widely-used LLM, to comprehend exist-\ning workflow descriptions (cf. Study I), to exchange tools used\nwithin a workflow (cf. Study II), and to extend a partially given\nworkflow (cf. Study III) using three distinct user studies. We\nselect these use cases as understanding the data flow and the\n2https://copilot.github.com - last access 20-10-2023\nanalysis performed is essential for successfully applying scien-\ntific workflows. Moreover, exchanging tools and extending a\npartially given workflow are common use cases in adapting and\nreusing existing workflows in the work context of domain sci-\nentists [6]. For each study, we specially design conversational\nprompts simulating the interaction between a user working with\nworkflows and ChatGPT. For our studies, we leverage version\nGPT-3.5 of ChatGPT3. We decided to use GPT-3.5 since it is\nopenly available to the public and allows other researchers to re-\nproduce our investigations without additional incurring costs 4.\nAdditionally, we develop distinct questionnaires for evaluating\nthe output of ChatGPT by the domain experts for each study.\nWhile conducting a study, we present a brief overview of the\nstudy’s overall goal and the developed questionnaire to the ex-\nperts. Subsequently, the experts complete the questionnaire in-\ndependently without the experimenters’ support. This proce-\ndure is intended as participants were not pressured by a time\nlimit and could freely allocate their time for the study. Fur-\nthermore, we intend to avoid a Hawthorne e ffect, where par-\nticipants can provide biased responses due to the presence of\nobservers [68].\n3.2. Participants\nThroughout all experiments, we recruited one expert from\nbioinformatics and three experts working on Earth observation\nworkflows. Using scientific workflows is common in these two\nareas. In bioinformatics, scientific workflows are an important\ntool for enabling the automation and documentation of com-\nplex data analysis processes, ensuring reproducibility and trans-\nparency in research [39]. In Earth observation, scientific work-\nflows streamline the complex process of acquiring, process-\ning, and analyzing vast amounts of satellite and sensor data,\nenhancing the e fficiency and accuracy of environmental stud-\nies [70]. Hence, scientific workflows have become common-\nplace in these two areas. The professions include postdocs and\nPhD students working at universities. All participants hold a\nmaster’s degree in their profession and several years of experi-\nence in their domain. All experts are between 25 and 40 years\nold (two female, two male).\n3.3. Scientific Workflows\nIn our study, we consider a total of five different workflows.\nWe summarize the used workflows and their details in Table 1.\nThe workflows are taken from the work context of the recruited\nexperts, given their high degree of familiarity and expertise with\nthem. In bioinformatics, we use two workflows that deal with\nthe analysis of genomic data. First, the crisprseq workflow,\nsourced from the nf-core repository5, a hub for best-practice\nworkflows, focuses on analyzing and evaluating gene editing\nexperiments utilizing CRISPR-Cas9 mechanism for genome en-\ngineering. Second, the RS-STAR workflow, which was imple-\nmented by the recruited domain expert and performs differential\n3https://chat.openai.com - last access 20-10-2023\n4Moreover, we provide the chat records in the Supplementary Material of\nthis article.\n5https://nf-co.re - last access 20-10-2023\n4\nTable 1: Overview of the used workflows. We examine workflows from two scientific domains, i.e., bioinformatics and Earth observation, and two workflow systems\n(Nextflow and Apache Airflow). For each workflow, we report the number of (high-level) steps and used tools and in which study the workflow is used.\nDomain Workflow Description #Steps #Tools Study\nBio-\ninformatics\nWF1:\ncrisprseq [69]\n(Nextflow)\nA bioinformatics data pipeline for analyzing and evaluating\ngene editing experiments utilizing CRISPR-Cas9 mechanism\nfor genome engineering.\nRepository: https://nf-co.re/crisprseq/2.0.0 (cre-\nated 07/2022)\n6 9 I\nWF2:\nRS-Star\n(Nextflow)\nThe general aim of this workflow is to perform di fferential\ngene expression analysis using RNA-seq data.\nRepository: https://github.com/Nine-s/nextflow_\nRS1_star (created 11/2021)\n5 5 I,II,III\nEarth\nObservation\nWF3:\nFORCE2NXF-\nRangeland [33]\n(Nextflow)\nThis workflow analyzes long-term vegetation dynamics in the\nMediterranean using the FORCE-toolkit.\nRepository: https://github.com/CRC-FONDA/\nFORCE2NXF-Rangeland (created 11/2020)\n9 8 I\nWF4:\nGrasslands\n(Nextflow)\nThis workflow aims at understanding differences in long-term\nchanges (1984-2022) in ground cover fractions specific to Eu-\nropean grasslands.\nRepository: https://github.com/kelewinska/FONDA_\ntrends-nf (created 08/2023)\n6 3 I,III\nWF5:\nFORCE\n(Apache Airflow)\nThis workflow focuses on analyzing long-term vegetation dy-\nnamics in the Mediterranean using the FORCE-toolkit.\nRepository: https://github.com/CRC-FONDA/\nfonda-airflow-dags/tree/main (created 02/2021)\n8 8 I\ngene expression analysis using RNA-seq data. TheFORCE2NXF-\nRangeland and FORCE are two implementations of an Earth\nobservation workflow that is concerned with analyzing long-\nterm vegetation dynamics in the Mediterranean using the FORCE\ntoolkit6, which provides processing routines for satellite image\narchives. The former is implemented using the Nextflow sci-\nentific workflow management system 7 [39] and the latter by\nleveraging Apache Airflow8 [37]. The third Earth observation\nworkflow called Grasslands. builds on previous work [71, 72]\naiming at understanding di fferences in long-term changes in-\nground cover fractions specific to European grasslands depend-\ning on the definition of endmembers (i.e., unique spectral sig-\nnatures of a specific material or ground cover) approximating\nthese fractions.\n3.4. LLM Prompting\nThe choice and design of prompts entered into a LLM has\na decisive influence on the output quality [73] and, in our case,\non the suitability of ChatGPT for workflow development and\nimplementation. Our prompts are organized first to provide\nthe context, often including the workflow script, followed by\n6https://github.com/davidfrantz/force - last access 20-10-2023\n7https://www.nextflow.io - last access 20-10-2023\n8https://airflow.apache.org - last access 20-10-2023\nthe specific question or instruction under investigation. Sup-\npose the workflow is divided into sub-workflows, possibly dis-\ntributed over several files. In that case, we first specify the main\nworkflow and then all sub-workflows and task definitions in the\norder they occur in the main workflow. In our research’s ini-\ntial stages, we experimented with various alternative prompts\nfor each user study, incrementally modifying and enhancing\nthem in response to the outcomes we received. For example,\nfor Study I (i.e., workflow comprehension), we discovered that\nChatGPT tends to describe properties of the workflow language\nor technical aspects instead of workflow characteristics. Such\nphenomena could be resolved by adding explicit instructions,\ne.g., “do not explain nextflow concepts”. We refer to Section 7.5\nfor a detailed discussion of prompt design challenges. We stopped\nthis adjustment process after a few iterations as soon as no more\nof such artificial artifacts were generated. We have refrained\nfrom more extensive prompt engineering since workflow de-\nsigners are domain experts from diverse fields who cannot be\nassumed to be specialists in developing and tuning prompts.\nHowever, we acknowledge that the choice of wording in our\nprompts influences the results [73]. We discuss the limitations\nof our work concerning the chosen study design and prompt\nstrategy selection in more detail in Section 7.6.\n5\nTable 2: Overview of the used prompts to investigate ChatGPT’s capabilities in capturing the content of a workflow description (Study I).[workflow-language] and\n[workflow-text] represent placeholders for the workflow management system, i.e., Nextflow or Apache Airflow, and the workflow description text. All prompts are\nexecuted within one conversation.\nID Category Prompt\nP1 1 Overall\naim\nThe following text contains a scientific workflow written in [workflow-language]:\n[workflow-text]\nExplain from which research area this workflow originates and describe the general aim of this\nworkflow. Don’t explain [workflow-language] concepts.\nP1 2 Workflow\nexplanation\nExplain all individual tasks that are implemented in this workflow. For each task explain which\nsoftware programs or tools are used in this workflow to perform the task. Don’t explain [workflow-\nlanguage] concepts.\nP1 3 Workflow\nexplanation\nExplain the type of input data and the format of the input data needed for this workflow. Don’t\nexplain the workflow itself.\nP1 4 Workflow\nexplanation\nExplain the overall result of this workflow. For each individual task of the workflow report the type\nof data that is produced by this task.\nP1 5 Research\nquestions Explain up to 3 research questions for which this workflow is helpful.\n4. Study I: Workflow Comprehension\nIn our first study, we investigate the capabilities of ChatGPT\nin capturing the actual purpose of a workflow. In other words,\nwe are prompting ChatGPT to explain the purpose of a work-\nflow. In this study, we assess ChatGPT’s quality in compre-\nhending and explaining a workflow’s purpose in a user study in-\nvolving workflow experts. Understanding the data flow and the\nanalysis performed constitutes an important aspect of the daily\nwork with scientific workflows. On the one hand, workflows are\noften precisely adapted to individual research questions, which\nmakes it challenging even for other experts from the same do-\nmain to understand them. On the other hand, in many research\ninstitutions, an increasing number of legacy workflows whose\noriginal authors and contributors are no longer available for\nmaintaining and refining the codes require taking over by new\nteam members. Understanding a workflow is usually a prereq-\nuisite for adopting and applying a workflow correctly.\nThus, Study I pursues three goals: how well does ChatGPT\nperform on (a) identifying the domain and the overall objec-\ntive of the analysis, (b) reporting the individual computation\nsteps, used tools, their needed input data, produced output data,\nand (c) explaining research questions for which these analy-\nses are helpful given the workflow description. The first two\nparts of the study have a reconstructive character, whereas the\nthird is more explorative, requiring ChatGPT to reason beyond\nthe given workflow description. We build a set of five di ffer-\nent prompts to evaluate ChatGPT’s capabilities concerning the\nthree dimensions. When providing the workflow definition in\nthe prompt, delete all comments within the definition to prevent\ninformation leakage. Table 2 depicts the developed prompts.\nFor each workflow, all prompts are executed in one conversa-\ntion, enabling ChatGPT to use the in- and output of previous\nprompts as context information. We ask the domain scientists\nto evaluate answers given by ChatGPT using a feedback ques-\ntionnaire. The complete questionnaire contains nine items in\ntotal and can be found in full-length in Appendix A. The ques-\ntionnaire focuses on the correctness of the prompts regarding\nthe aim of the workflow, the explanation, and the forecast of\naddressed research questions. For four of the nine items, users\nrate the generated explanations on a 5-point Likert scale 9. In\naddition, three items comprise quantitative evaluations of how\nmany computational steps are correctly detected, how many uti-\nlized software tools and programs are accurately identified, and\nhow many valid follow-up research questions. The remaining\nquestion items concern the quality of the explanations of the\nworkflow sequence, the description of the tools used, and the\nresults produced. We add a comment field for each item to re-\nport issues and errors in the generated explanations if the do-\nmain expert does not apply the content.\n4.1. Results\nThe results of the expert surveys are presented in the follow-\ning according to the three subcategories of the questionnaire,\ni.e., research area and the overall aim of the workflow, explana-\ntion of workflow details, and subsequent research questions.\n4.1.1. Overall Aim of the Workflow\nThe first two rows of Figure 2a highlight the rating distri-\nbution of the domain experts concerning ChatGPT’s explana-\ntions for the research area and the overall aim of the workflows.\nThe experts agreed with the explanations generated, indicating\na basic understanding of ChatGPT regarding the different work-\nflows (µ = 4.7, σ = 0.68). The lowest agreement, with a score\nof 4 concerning the research area and 3 for the overall aim,\n91: Strongly Disagree; 5 Strongly Agree.\n6\n01 1 2 3 4 5\nExpert Ratings\nData\nSpecification\n(Q1_8)\nInput\nDescription\n(Q1_7)\nOverall\nAim\n(Q1_2)\nResearch\nArea\n(Q1_1)\nWorkflow Aspect\nStrongly disagree Disagree Neither agree nor disagree Agree Strongly agree\n(a)\nWF1\ncrisprseq\nWF2\nRS-Star\nWF3\nRangeland\nWF4\nGrasslands\nWF5\nForce\nWorkflow\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0Valid Research Questions (b)\nFigure 2: Results from Study I: (a) rating distribution of the domain experts for ChatGPT’s capability in identifying the research area and overall aim as well as the\ninput and data description of a workflow. The question item identifier (see Appendix A) is given in parenthesis for each row. (b) Number of valid research questions\ngenerated by ChatGPT for the di fferent workflows as assessed by the domain experts (question item Q1 9). We prompted ChatGPT to output up to three research\nquestions per workflow.\nwas recorded in the evaluation of the WF4-Grasslands work-\nflow. In this case, the expert could not agree with the explana-\ntions mainly due to the wrong interpretation of an abbreviation\nwithin the workflow description, i.e., FNF was misinterpreted\nas “fraction of non-forest”, instead of fold and fill. This misun-\nderstanding resulted in the workflow being explained as exam-\nining forest regions rather than grasslands.\n4.1.2. Workflow Explanation\nIn general, the participants regard the quality of the expla-\nnations given by ChatGPT as high (see Figures 3a and 3b). All\ncomputational steps are accurately identified in three of the five\nworkflow descriptions. Moreover, for four of these five work-\nflows, every tool employed is correctly detected, but in WF5-\nForce, two out of eight were missing. The detailed descriptions\nof the tasks and tools provided by ChatGPT were also judged\nto be coherent by the experts. Overall, the worst performance\nis achieved with the output for workflow WF4-Grasslands for\nwhich only four out of six tasks are correctly extracted and\nonly most of the tools are correctly described. These errors\nare mainly due to follow-up errors that result from the incorrect\nrecognition of the workflow purpose.\nThe results of the questions items (see Q1 7 and Q1 8 in\nAppendix A), which assess the produced information about the\nformat and type of input and output data of ChatGPT, can be\nseen in the two lower rows in Figure 2a. Similar to the previous\nfindings, the information generated for four of the five work-\nflows are evaluated positively (µ = 4.2, σ = 1.0). Again, only\nthe produced information for the workflow WF4 was assessed\nas neutral or negative, i.e., input description score of 2 (dis-\nagree) and data specification of 3 (neither agree nor disagree).\n4.1.3. Research Questions\nThe last query was concerned with explaining up to three\nsubsequent research questions to a given workflow. Figure 2b\nshows the result of this question item. ChatGPT achieves only\nmoderate performance, generating just for one workflow, i.e.,\nWF2-RS-Star, three valid research questions. In total, out of the\n15 generated research questions 10 were correct. These figures\nsuggest that ChatGPT o ffers only a reduced performance for\nmore explorative tasks.\n5. Study II: Workflow Modification\nThe second study investigates how much ChatGPT can aid\ndomain experts in modifying and tailoring a workflow in our\nsecond study. Researchers usually do not start from scratch\nwhen developing workflows but typically adapt or reuse parts\nof existing workflows from the community [6]. This strategy\napplies in particular to biomedicine, in which workflows are\nmore widespread and have a longer tradition compared to other\ndomains [74]. For example, genomic workflows will often be\napplied to a broad spectrum of data originating from di ffer-\nent sources, each with its distinctive features and characteris-\ntics, making it necessary to adjust the workflow definition for\nmore e fficient data processing. Moreover, technological ad-\nvancements, such as in genome sequencing technology [75],\nlead to new tools specially developed to leverage the capabili-\nties of the new technologies. The continuous integration of new\nand alternative scientific tools into existing workflows is essen-\ntial to conduct state-of-the-art research [76].\nIn our study, we are particularly investigating the exchange\nof used tools in the bioinformatics workflowWF2-RS-Star whose\ncomputational scheme is given in Figure 1a. We select two parts\nof the workflow to be modified:\n1. read trimming and filtering (also called read quality con-\ntrol), originally performed by FASTP [77] and\n2. reference genome indexing and alignment, carried out by\nSTAR [78].\nFor assessing the workflow modification capabilities of Chat-\nGPT, we build four prompts (see Table 3). The first prompt re-\nquests a list of alternative tools for a given workflow step from\nChatGPT. The second and third prompts request the recommen-\ndation of two alternative tools, including an explanation of the\nsuggestion, a comparison of the selected tools with the tool\noriginally used in the workflow script, and their strengths and\n7\n0.0 0.2 0.4 0.6 0.8 1.0\nRatio of Correctly Identified Tasks and Tools/Programs\nWF1\ncrisprseq\nWF2\nRS-Star\nWF3\nRangeland\nWF4\nGrasslands\nWF5\nForce\nWorkflow\nT asks T ools/programs\n(a)\nDon't\nknow\nNone\ncorrect\nFew\ncorrect\nMost\ncorrect\nAll\ncorrect\nExpert Ratings\nWF1\ncrisprseq\nWF2\nRS-Star\nWF3\nRangeland\nWF4\nGrasslands\nWF5\nForce\nWorkflow\nT asks T ools/programs (b)\nFigure 3: Result statistics highlighting the number of correctly identified tasks and tools of a workflow (a) and their explanation (b). We report separate results per\ninvestigated workflow. The results are based on the ratings of the domain experts. Results in (a) correspond to the itemsQ1 3 and Q1 5 and in (b) to Q1 4 and Q1 6\nof the questionnaire given in Appendix A.\nweaknesses. With the last prompt, the actual rewriting of the\nworkflow to include the selected tool is requested. We test the\ninclusion of two alternative tools per computational task, i.e.,\nthe prompts P2 3 and P2 4 (see Table 3) are carried out once\nfor each tool from the recommendation. Analogous to Study\nI, we use a questionnaire for evaluating ChatGPT’s output by\nthe biomedical domain expert containing 13 items in total. For\nmost items (9 out of 13), the generated texts and explanations\nconcerning methodical differences or pros and cons of the tools\nshould be rated on a 5-point Likert scale. The remaining ques-\ntions require numerical ratings (2 items), yes /no answers (1),\nand free text fields (1). Again, we add a comment field for each\nitem to report issues and errors in the generated explanations if\nthe domain expert does not fully apply the content. The com-\nplete questionnaire is given in Appendix B. When conducting\nthe study, we also provide the generated workflow scripts to\nthe domain experts and asked them to execute them on their\nsystems. Furthermore, we request the experts to inspect and\ncorrect any non-functional scripts. For the latter, we set a time\nlimit of 20 minutes per tool substitution.\n5.1. Results\nThe results are presented in the following according to the\ntwo subcategories of the prompts, including the exploration of\nalternative tools and workflow modification. We summarize the\nresults of the two use cases (i.e., read quality control, reference\ngenome alignment) and each of the two alternative tools when\nreporting the results.\n5.1.1. Tool Exploration\nWhen exploring possible alternative tools, ChatGPT showed\na good performance, providing a fully valid list of ten alterna-\ntive tools using prompt P2 1 (see Table 3) for both scenarios.\nThe generated output list for both tasks can be seen in Figure 4.\nHowever, the domain expert criticized the alternative tools for\nreference genome alignment specified Bowtie [79] and not its\nsuccessor Bowtie2 [80]. From these, ChatGPT recommended\nHISAT2 [81] and Bowtie as alternatives for genome alignment\nand Trimmomatic [82] and Cutadapt [83] for read quality con-\ntrol. According to the domain expert, all tools are principally\nvalid substitutions, but Bowtie is a rather inappropriate sugges-\ntion since it is not specialized for RNAseq data. The results of\nexplaining the tools’ methodical di fferences and strengths and\nweaknesses can be seen in Figure 5. Essentially, the gener-\nated textual explanations were assessed positively, except when\ndescribing the methodological differences between the tools. In\nthis case, ChatGPT could offer a convincing explanation in only\n50% of the cases. In the other cases, the texts were too general,\nand differences were not named clearly. In summary, similar\nto the results from Study I, ChatGPT solves these rather recon-\nstructive tasks well, reaching an average score of 4.1 (min =3,\nmax=5, σ=0.81) when taking the results from items Q2 3 to\nQ2 7 (see questionnaire in Appendix B) into account. These\nresults indicate that ChatGPT could effectively used to explore\ntools in a given field.\n5.1.2. Workflow Modification\nWe requested ChatGPT to re-write the workflow script for\neach of the two recommended alternative tools in both use cases,\nresulting in four modified workflows in total. Table 5 summa-\nrizes the results achieved. First of all, none of the generated\nworkflow scripts was entirely correct. Only in one of the four\ncases, the generated script was (at least) syntactically valid, i.e.,\nif replacing FASTP with Trimmomatic for read quality control,\nand the domain expert could execute it without further adap-\ntations. However, the script was not a semantically correct\nmodification of the original workflow script since one particular\nquality control routine (i.e., PolyG trimming) was not reflected\nin the adopted script. This issue also occurred for Cutadapt, the\nother alternative tool for quality control. For both tools, Chat-\nGPT reported in the explanation text that the tools do not sup-\nport this feature; however, in reality, they do. This failure could\nbe interpreted as a kind of LLM hallucination [53]. The second\ntask, reference genome indexing and alignment, revealed differ-\n8\nHere is a list of 10 alternative tools that can be used for reference\ngenome alignment:\n1. Bowtie: A fast and memory-efficient aligner for short DNA sequences.\n2. BWA (Burrows-Wheeler Aligner): A popular aligner for mapping DNA\nsequences against a large reference genome.\n3. HISAT2: A fast and sensitive aligner for spliced alignment of RNA-seq\nreads.\n4. TopHat: A widely used aligner for RNA-seq reads that incorporates\nsplice junctions.\n5. Subread: A versatile aligner capable of aligning both DNA and RNA\nsequences to a reference genome.\n6. Salmon: A lightweight aligner specifically designed for accurate\nquantification of transcript abundance from RNA-seq data.\n7. STAR (Spliced Transcripts Alignment to a Reference): The tool used in\nthe provided workflow, known for its speed and ability to handle splice\njunctions.\n8. GSNAP: A general-purpose aligner that allows for highly sensitive and\naccurate alignment of short reads against a reference genome.\n9. Novoalign: A versatile aligner that supports both DNA and RNA\nsequence alignment, offering high sensitivity and accuracy.\n10. Segemehl: A read mapper optimized for mapping short reads against\nlarge genomes with a focus on finding splice junctions accurately.\nThese are just a few examples of the many alignment tools available in\nthe field of bioinformatics. The choice of tool depends on specific\nrequirements, such as the nature of the data (DNA or RNA), read length,\npresence of splice junctions, and desired performance characteristics.\nCertainly! Here's a list of 10 alternative tools that can be used for read\nfiltering and trimming in biomedical workflows:\n1. Trimmomatic: A popular tool for read trimming, adapter removal, and\nquality filtering.\n2. Cutadapt: Used for adapter trimming in high-throughput sequencing data.\n3. BBDuk: Part of the BBMap package, it offers read trimming, filtering, and\nquality control.\n4. SeqKit: A fast and lightweight tool for sequence processing, including\nread trimming.\n5. Trimgalore: Specifically designed for trimming and quality filtering of\nIllumina sequencing data.\n6. Sickle: A tool that focuses on quality-based trimming of reads from high-\nthroughput sequencing data.\n7. AlienTrimmer: A read trimming tool that uses a statistical approach to\nidentify and remove adapters.\n8. Prinseq: Provides various options for read filtering, trimming, and quality\ncontrol.\n9. SolexaQA: Offers quality assessment, read trimming, and error correction\nfor Illumina sequencing data.\n10. AdapterRemoval: Specialized for adapter trimming and quality filtering\nin paired-end sequencing data.\nThese tools provide different features and algorithms for read filtering and\ntrimming, allowing researchers to choose the one that best suits their\nspecific needs and preferences.\nAlternative Tools for\nReference Genome Alignment\nAlternative Tools for\nRead Filtering and Trimming\nChatGPT Output: ChatGPT Output:\nFigure 4: Representation of the output of ChatGPT when requested to provide a list of alternative tools for reference genome indexing and alignment (left) and read\nquality control (right) using prompt P2 1. All tools are assessed to be valid by the biomedical domain expert.\nent issues than the first. Here, the main problem was the correct\nlinking of the two sub-parts of the task, first the index genera-\ntion and then the computation of the actual alignment. For the\nformer, each tool specifies and uses its distinct data format and\ndefines how to store the index (e.g., saving it in one or multiple\nfiles). However, the storing strategy also a ffects how the out-\nput of the indexing task has to be passed on to the input of the\nalignment computation. In the scripts generated by ChatGPT,\nthe actual step descriptions to invoke indexing and alignment by\nthe tools were (generally) valid. However, the linking of these\ntwo needed to be corrected. For example, Bowtie saves its in-\ndex in multiple files sharing a common file name prefix, which\nhas to be specified as a parameter during alignment. However,\nin the modified script, the list of all files of a specially created\ndirectory was passed to the alignment process. For Bowtie, this\nproblem could be easily fixed by the domain expert, but for\nHISAT2, it was not that trivial and hence could not be solved in\nthe given time budget of 20 minutes.\nTo sum up, the study’s results indicate that modifying work-\nflow scripts poses considerable challenges for ChatGPT as it\nrequires a detailed understanding of the tool’s idiosyncrasy, the\nexact computations they perform, and the data formats they\nuse.\n6. Study III: Workflow Extension\nIn the third study, we investigate the capabilities of Chat-\nGPT in extending a scientific workflow given a partial script.\nAs discussed in the motivation for Study II (see Section 5),\nusers often reuse parts of existing workflows from the research\ncommunity and adapt them to the research question at hand by\nenhancing the pipeline with additional analyses and computa-\ntional steps [6]. Moreover, data analysis projects are often ex-\nploratory processes, and computation pipelines are incremen-\ntally adapted and extended based on executions and findings\nfrom previous versions of the workflow, e.g., to include addi-\ntional data correctness checks, add more di fferentiated result\nevaluations and provide advanced result visualizations [84]. In\nour study, we simulate this incremental exploration process by\ntaking an existing workflow and removing n steps at the end\nof it. We then request ChatGPT to a) enumerate the neces-\nsary steps to accomplish the original research goal and b) re-\ngenerate the next step using the tool of the original pipeline\nor by giving a verbal description of the task. For this study,\nwe select one workflow from each research domain for inves-\ntigation: WF2-RS-Star for biomedicine and WF4-Grasslands\nfor earth observation. The two workflows were chosen because\nthey offer different implementation characteristics, i.e., WF2-\nRS-Star leverages almost exclusively external tools, whereas\nGrasslands relies more strongly on specially implemented R\nand Python scripts. Moreover, Study I (see Section 4) already\nshowed notable result di fferences of ChatGPT for both work-\nflows. By choosing these two specific workflows, we aim to\nencompass a possibly broad spectrum of performance varia-\ntions. We test ChatGPT’s workflow extension capabilities in\nthree scenarios: For WF2-RS-Star, we remove the last step\ntranscript quantification as well as the last two steps transcript\nquantification and format conversion, forming two extension\nscenarios. In the case of WF4-Grasslands, we remove all steps\nat the tail of the workflow, includingautoregressive trend anal-\n9\nTable 3: Overview of the used prompts to investigate ChatGPT’s capabilities in swapping used tools in bioinformatic workflows (Study II). Information in square\nbrackets specifies placeholders for concrete information regarding the workflow or the tool to be replaced.\nID Category Prompt\nP2 1 Tool\nexploration\nThe following text contains a [domain] workflow written in [workflow-language]:\n[main-workflow]\nThe following snippets contain the source code for the step of the workflow which uses [tool] to\nperform [step]: [step-source-code]. Please provide a list of 10 alternative tools to perform [tool].\nP2 2 Tool\nexploration\nThe following text contains a [domain] workflow written in [workflow-language]:\n[main-workflow]\nThe following snippets contain the source code for the step of the workflow which uses [tool] to\nperform [step]: [step-source-code]. Alternative tools for [step] are: [list-of-tools]\nWhich of the tools would you recommend as most suitable alternative for [step] in the given work-\nflow. Please name the two alternatives and give an explanation why these tools are especially advis-\nable for the given workflow.\nP2 3 Tool\nexploration\n[original-tool] and [alternative-tool] are two tools for [step] in [domain] workflows. First, explain\nthe differences between the tools and the underlying approaches. Second, name strenghts and weak-\nnesses of both tools.\nP2 4 Workflow\nmodification\nThe following text contains a [domain] workflow written in [workflow-language]:\n[main-workflow]\nThe following snippets contain the source code for the step of the workflow which uses the[tool] to\nperform [step]: [step-source-code]\nPlease re-write the code of the workflow and the proccess to use [alternative-tool] instead of\n[original-tool]. The number of parameters of the individual process descriptions may have to be\nadjusted. Please explain features / options of [original-tool] which are not supported in [new-tool].\nysis (see schema in Appendix D).\nTable 4 illustrates the prompts developed for this purpose.\nThis study uses slightly different prompts (see P3 2a and P3 2b)\nreflecting the di fferent workflow types, i.e., tool- vs. script-\nbased. For the latter, we include additional instructions to a)\nspecify the programming language of the script and b) ask the\ndomain expert for a verbal description of the computational\nsteps to be implemented. See Appendix E for the verbal de-\nscription provided by the earth observation expert. The ques-\ntionnaire for evaluating the generated outputs consisting of seven\nitems can be found in Appendix C.\n6.1. Results\nThe results are presented in the following according to the\ntwo subcategories of the prompts, i.e., workflow exploration\nand extension.\n6.1.1. Workflow exploration\nFor describing further computational steps necessary to ac-\ncomplish a specific research goal given a partial workflow, Chat-\nGPT showed mixed results. The LLM provides a correct list\nof suitable steps in two of the three scenarios. Also, the tools\nand methods for implementing the steps suggested by ChatGPT\nwere valid. However, both domain experts criticize that the\nspecifications for the necessary steps and the proposed tools\ntend to be rather generic and generalized. For instance, for\nextending the WF4-Grasslands workflow the earth observation\nexpert commented:\nOverall, the proposed workflow is very generic and\ndoes not provide a clear roadmap for the analyses.\nIt also proposes to use very simplistic and often\nimperfect approaches.\nOverall, the results confirm the findings from the two previous\nstudies that ChatGPT shows weaknesses in more exploratory\ntasks.\n6.1.2. Workflow extension\nUsing the prompts P3 2a and P3 2b (see Table 4), we re-\nquest ChatGPT to re-construct the last removed computational\nstep in each extension scenario. Table 6 summarizes the results\nachieved. Like the results from Study II, ChatGPT shows con-\nsiderable weaknesses in the automatic extension of workflows.\nNone of the generated workflow scripts was executable with-\nout the intervention of the domain expert. A clear di fference\nis revealed when comparing the two domains, biomedicine and\nearth observation. In the former case, the generated workflow\nscripts are (at least) of such a quality that the domain expert\ncould successfully correct them within 20 minutes. In the gen-\nerated scripts, mainly syntactical errors occurred (e.g., incor-\nrect usage of variable identifiers, incomplete input definitions,\nor missing specification of parameters), which could be easily\n10\n01 1 2 3 4\nExpert Ratings\nS&W alt.\nT ool (Q2_7)\nS&W orig.\nT ool (Q2_6)\nMethod\nDifferences\n(Q2_5)\nT ool\nExplanation\n(Q2_3/Q2_4)Question Item\nStrongly disagree Disagree Neither agree nor disagree Agree Strongly agree\nFigure 5: Overview of the rating distribution of the biomedical domain expert for ChatGPT’s capability for explaining alternative tools, methodical differences, and\nstrengths and weaknesses (S&W) of the tools. The question item identifier (see Appendix B) is given in parenthesis for each row.\ncorrected. However, the calls to the respective programs to per-\nform the two tasks were correct.\nIn contrast, the generated extension for WF4-Grasslands\nwas of considerably lower quality. In this case, several syntactic\nand semantic errors occurred, e.g., the script uses a non-existing\nlibrary function, no parallelization code is included, and not all\nrequested computations are performed. In this state, the domain\nexpert could not resolve the large number of problems within 20\nminutes. However, when interpreting these results, one must re-\nmember that the task in this scenario is also significantly more\ndifficult. Instead of a short task description and specification\nof a tool to be used, ChatGPT has to design and generate the\nsource code for a complex data analysis procedure containing\nmultiple sub-steps.\n7. Discussion\nWe conducted three studies to investigate the capabilities of\nusing ChatGPT for comprehending, modifying, and extending\nscientific workflows. We discuss our methodology and the re-\nsults in the following.\n7.1. Comprehending Scientific Workflows\nStudy I was designed to answer RQ1 by evaluating Chat-\nGPT’s performance in comprehending existing workflows. The\ndomain experts assessed that ChatGPT is good at this task while\nshowing slight di fferences between the investigated research\ndomains. In particular, the explanations for workflow WF4-\nGrasslands revealed considerable performance drops. Unlike\nthe other workflows investigated, this one uses multiple propri-\netary R and Python scripts instead of leveraging external tools\nfor assembling data processing pipelines. The lack of standard-\nized tools makes workflow comprehension more challenging\nsince ChatGPT has to interpret complex processing logic and\nhas fewer possibilities to leverage static information, like the\ndescription of the general purpose of an established bioinfor-\nmatics tool, seen through its training while generating the re-\nsponse. In addition, code quality and its readability may strongly\ninfluence the results for workflows containing proprietary scripts.\nFor instance, one major problem while explaining WF4-Grass-\nlands in Study I was the misinterpretation of the abbreviation\n”fnf” as “fraction of non-forest” , instead of ”fold and fill” .\nSuch customized and ambiguous terms challenge LLMs and re-\nduce their applicability.\n7.2. Modifying Scientific Workflows\nWe answer RQ2 in Study II by evaluating the modification\nperformance of ChatGPT. To this end, we requested the LLM to\nsubstitute the leveraged tools for two computational tasks, read\nquality control and reference genome alignment, in the biomed-\nical workflow WF2-RS-Star. The study results suggest that\nChatGPT can effectively explore and explain alternative tools in\nthe field, possibly shortening the time the experts spend search-\ning for suitable replacements on the web. In contrast, the results\nalso indicate that ChatGPT rather poorly supports the genera-\ntion of workflow scripts for using these alternative tools. In\nonly one scenario, i.e., substituting FASTP with Trimmomatic,\nthe produced script could be run without syntactical errors, and\nin one other scenario, i.e., replacing STAR with Bowtie, the\nscript could be fixed within 20 minutes to be syntactically and\nsemantically valid. In the used version of ChatGPT and the\nselected setup, an increase in e fficiency cannot be recorded or\nanticipated, highlighting the need for further research e fforts.\nHowever, when interpreting the results, it is essential to remem-\nber that ChatGPT is a general-purpose LLM rather focusing on\nhuman language. A potential option for improvement could be\ntesting generative models more strongly adapted to program-\nming code, such as GitHub Copilot or Code Llama [19].\n7.3. Extending Scientific Workflows\nFinally, we investigate RQ3 by conducting Study III. To\nthis end, we requested ChatGPT to extend an existing (partially\ngiven) workflow to achieve specific goals. The study results\nconfirm the findings from the two previous studies and empha-\nsize ChatGPT’s di fficulties in solving more complex and ex-\nploratory problems. In this case, explaining the necessary steps\n11\nTable 4: Overview of the used prompts to investigate ChatGPT’s capabilities in extending a given partial workflow (Study III). We distinguish two types of prompts:\nworkflow exploration and workflow extension. For the latter, we developed two variants specially designed for tool- (P3 2a) and script-based workflows (P3 2b).\nID Category Prompt\nP3 1 Workflow\nExploration\nThe following text contains a [domain] workflow written in Nextflow:\n[workflow-description]\nThe workflow should be used to [overall-goal]. Which steps are missing in order to perform [overall-\ngoal]? Please specify only the absolutely necessary steps. For each step name up to three [domain]\ntools that can be used to perform the task.\nP3 2a Workflow\nExtension\nThe following text contains a [domain] workflow written in Nextflow:\n[workflow-description]\nPlease extend to the given workflow to include one further step which [step-description] using [tool].\nPlease specify the new process description in a file at [file-name]. Please use version 2 of the Nextflow\nworkflow language. The new process should take the output of [predecessor-step] as input.\nP3 2b Workflow\nExtension\nThe following text contains a [domain] workflow written in Nextflow:\n[workflow-description]\nPlease extend to the given workflow to include one further task which performs [step] using an\n[programming-language] script. For this, please generate an [programming-language] script, stored\nin [script-file-name], which performs the following computations:\n[verbal-task-description]\nNext to the [programming-language] script generate the Nextflow process description in a file named\n[process-file-name] and the updated workflow. Please use version 2 of the Nextflow workflow language.\nThe new process should take the output of [predecessor-step] as input.\nto answer the given research questions and the generation of the\nworkflow script for the next step offered (partly) severe issues.\nSimilar to the results of Study I, the picture is mixed regarding\nthe different research domains, earth observation and bioinfor-\nmatics. For the latter, the generated scripts form a relatively\ngood basis for the implementation, having only (minor) syn-\ntactical issues that the expert could quickly fix. In contrast, in\nthe case of earth observation, the script quality was consider-\nably worse, hindering a fast correction by the expert. These\nresults imply that efficient user support is possible for pipelines\nmainly leveraging external tools. However, further research is\nnecessary to investigate user-support strategies for workflows\napplying specially implemented analysis scripts.\n7.4. Scientific Workflows in LLM Training Data\nLLMs are trained on large amounts of textual data from the\nweb, including programming code and workflow scripts [42].\nTherefore, it is crucial to consider whether and to what extent\nan LLM was already able to access the workflow scripts of our\nstudy during its pretraining. According to public information10,\nChatGPT was trained on data gathered until September 2021,\nmeaning that initial versions of two of the five tested workflows\n(i.e., WF3-FORCE2NXF-Rangeland and WF5-Force) could have\nbeen part of the LLM’s training routine. However, the spe-\ncific training dataset used for ChatGPT is not accessible to the\npublic, preventing a conclusive assessment. To attain a more\nprecise estimate of the potential number of workflow scripts\n10https://platform.openai.com/docs/models/gpt-3-5 - last ac-\ncess 20-10-2023\nwithin the training data in general, we initiated searches for\nscientific workflow repositories on GitHub. We leverage the\nrepository search engine of the website11 and use the names of\nfour widely-used workflow management systems, i.e., Apache\nAirflow, Nextflow, Snakemake, and Taverna as a query term.\nWe filter all repositories with creation data less than 2021-09-\n01 from the query results. Of course, the results must be inter-\npreted carefully since not every repository containing the name\nhas to deal with scientific workflows, even if the names are very\npeculiar. Detailed statistics from our search results are in Ap-\npendix F. As of 09 /2021, there were between 352 and 1,900\nrepositories containing one of the workflow system names in\ntheir description. Moreover, the results highlight the increas-\ning popularity of workflows since, for all systems except for\nTaverna, the number of repositories has almost doubled over\nthe last two years. We also checked the number of Nextflow\npipelines available in nf-core. As of September 2021, 35 pipe-\nlines were published, and 19 were under development12. Today,\nnf-core hosts 55 published pipelines and 33 in development.\nIn summary, we can hypothesize from these results that Chat-\nGPT can likely rely only on a relatively small base of workflow\nscripts during its training compared to classical programming\ncode (e.g., GitHub currently hosts over 3.9 million Java and\nover 2.2 million Python repositories13) making user support for\nworkflow design and implementation particularly challenging.\n11https://github.com/search - last access 20-10-2023\n12https://nf-co.re/stats - last access 20-10-2023\n13Determined by using GitHub repository search and the search queries “lan-\nguage:Java” and “language:Python”. – last access 20-10-2023\n12\nTable 5: Overview of the results of the workflow modification use case in which the tools performing a specific task are replaced by alternative ones. For each\ntask, we provide the original tool (in parenthesis) and the investigated alternative ones suggested by ChatGPT. For each combination, we highlight ( ✓=yes,×=no)\nwhether the generated workflow script could be executed (Exec.), whether it is semantically valid (Val.), and whether it could be fixed within 20 minutes (Fix). For\nthe latter, (✓) indicates cases where the script could be fixed to be executable but not entirely semantically correct. Moreover, we provide excerpts from the domain\nexpert’s comments.\nTask Alt. Tool Exec. Val. Fix Problems / Comments\nRead\nquality\ncontrol\n(FASTP [77])\nTrimmomatic\n[82]\n✓ × (✓) •Missing implementation to perform special quality control feature (i.e.\nPolyG trimming)\n•Invalid description of workflow differences:\n- ChatGPT stated that polyG trimming isn’t available in Trimmomatic,\nbut it actually is, however extra implementation for data adaptation is\nneeded\n- Differences of the generated output files not described accurately\nCutadapt\n[83]\n× × (✓) • Wrong program call: syntactically incorrect specification of two pa-\nrameters\n•Missing provision of valid adapter sequences\n•Invalid description of differences of the workflow script:\n- ChatGPT stated that polyG trimming isn’t available in Cutadapt, but it\nactually is\n- Differences of the generated output files not described accurately\nGenome\nindexing\n&\nalignment\n(STAR [78])\nHISAT2\n[81]\n× × × •Invalid definition and linkage of in- and output between genome index\ngeneration and alignment\n•Syntactically incorrect call of the alignment process\n•Does not take parameter strandedness of the input data into account\n•Output files aren’t generated correctly\nBowtie\n[79]\n× × ✓ •Invalid definition and linkage of in- and output between genome index\ngeneration and alignment\n•Wrong output definition of genome indexing task\n7.5. Prompt Design Challenges\nWhile creating prompts for the studies, we identified several\nchallenges and issues that arose while interacting with Chat-\nGPT.\n7.5.1. Representation of Workflows\nFor the representation of the workflow scripts, there is no\nstraightforward option on how to include them in a prompt. The\nworkflow descriptions are often spread over several files con-\ntaining sub-workflows and task descriptions. In our approach,\nwe first specify the main workflow and then all sub-workflows\nand task descriptions in order of occurrence. However, there\nmight be other, more e fficient prompt solutions (with respect\nto the generative language model). Furthermore, the work-\nflow scripts might exceed the maximum allowed input length\nof the language model, e.g., ChatGPT variants allow only for\n4K to 16K words / tokens 14 in the input sequence. In particu-\nlar, workflows heavily relying on specially implemented scripts\nhaving hundreds of code lines will face this issue.\n14https://platform.openai.com/docs/models/gpt-3-5 - last ac-\ncess 20-10-2023\n7.5.2. Loss of Focus\nSome of the prompts are very long due to the specification\nof the entire workflow script, which challenges ChatGPT to\nmaintain focus. Adding additional instructions to the prompt\nhelped to avoid or reduce this phenomenon, e.g. for the ex-\nplanation use case (Study I) we added to the prompt “Don’t\nexplain nextflow concepts” (see P1 1 and P1 2 in Table 2) and\n“Don’t explain the workflow itself” (P1 3) to prevent ChatGPT\nto generate outputs describing features of the workflow man-\nagement system or the complete workflow when requesting in-\nput data specification.\n7.5.3. Technological Details\nIn some cases, adaptation to technological details of the\nspecified workflows were necessary. For example, the Nextflow\nsystem offers two language versions for describing processing\npipelines. The Nextflow workflows in our study all used the\nnew version of the language. However, when extending work-\nflows in Study III, we had to specify the desired version (see\nP3 2 in Table 4) to get the correct output. This observation is\nsurprising since the partially given workflow is already in the\nrespective version. Interestingly, this was only necessary for\nthe workflow extension but not for their modification (P2 4 in\nTable 3) in which the phenomena did not occur.\n13\nTable 6: Overview of the results of the workflow extension use case in which we provide ChatGPT a partial workflow and request the LLM to extend it by one\nfurther computational step. For each investigated use case, we highlight (✓=yes, ×=no) whether the generated workflow script could be executed (Exec.), whether it\nis semantically valid (Val.), and whether it could be fixed within 20 minutes (Fix). For the latter, (✓) indicates cases where the script could be fixed to be executable\nbut not entirely semantically correct. Moreover, we provide excerpts from the domain expert’s comments.\nWorkflow Task /Tool Exec. Val. Fix Problems / Comments\nWF2\nRS-Star\nTranscript\nquantification/\nCufflinks\n[85]\n× × ✓ •Syntax errors: process definition for CUFFLINKS declares one input\nchannel but two were specified\n• Input tuple does not match input set cardinality declared by process\ndefinition\n•Wrong variable name: sorted bam (wrong) instead of sample bam\nFormat\nconversion/\nSAMtools\n[86]\n× × ✓ •Syntactical errors: Usage of wrong variable name (sample sam)\n• Incorrect syntax for connecting the new task to the pre-\nvious one SAMTOOLS(STAR ALIGN.sample sam) (wrong) vs.\nSAMTOOLS(STAR ALIGN.out.sample sam) (correct)\nWF4\nGrasslands\nAR\nanalysis/\nR script\n× × × •Input to the R script is a path to a directory, not a TIFF file\n• Incorrect use of remotePARTS library: There is no function called\nautoTrend in this package\n•Calculation needs to be parallelized (as specified in the request)\n•Computation should be implemented for four types of inputs, namely:\nGV , NPV , SOIL, and SHADE.\n•Desired outputs from the AR model needs to be retrieved and written\nout (missing)\n•Script declares a Conda environment (Python), not R environment.\nIn summary, the efficient and effective formation of prompts\noffers a wide range of possible solutions. In our study, we iden-\ntified initial clues and difficulties, but further research is needed\nto detect further potential for improving the interaction between\ndomain experts and ChatGPT and generative LLMs in general.\n7.6. Limitations and Future Work\nIn the following, we highlight the limitations of this work\nthat merit further research.\n7.6.1. Study Design\nIn each of our three studies, we created and provided the\nprompts for testing ChatGPT’s capabilities concerning the dif-\nferent use cases and the domain experts only evaluated the out-\nputs of ChatGPT, leading to a rather indirect interaction be-\ntween the domain scientist and the LLM. An alternative de-\nsign for the study would be to have the experts interact directly\nwith ChatGPT by developing and refining the prompts indepen-\ndently. In addition to assessing the capabilities of ChatGPT, this\nwould have the advantage of gaining initial insights into inter-\naction forms and patterns of the different experts with ChatGPT.\nMoreover, this would allow for improved customization of the\nprompts to the particular research domain and the idiosyncratic\nproperties and characteristics of each workflow. Extended op-\ntimization of the prompting strategy by the domain scientist\ncould lead to better results but reduce potential time savings\nin solving the actual task. Our study design was motivated by\nthe fact that the experts had strongly limited time budgets for\nthe study. For example, even for evaluating ChatGPT’s outputs\nin Study I, the experts already needed up to three hours to accu-\nrately check the generated explanations. A study design that en-\nvisages direct interaction involves high efforts in terms of intro-\nduction and explanation to ChatGPT and prompting strategies\nfor the domain scientists, thus limiting the scope of research\nquestions that can be investigated. In addition, the selected\nstudy design has the advantage of using the same prompts for\nthe different domains, which contributes to better comparabil-\nity of the results and eliminates the influence of di fferences for\nindividual prompt differences.\nIn our study, we focused solely on ChatGPT as genera-\ntive language model. However, there are many other general-\npurpose models available (e.g., PaLM-2 [44], BARD15 or Llama-\n2 [19]) as well as models more specially designed for program-\nming tasks (e.g., GitHub Copilot, Code Llama [87] or OpenAI\nCodex16) publicly available and worth investigating. Our stud-\nies only highlight the results of ChatGPT in the version used\n(GPT-3.5) but do not claim generalizability for other LLMs.\nFinally, recent research showed that placebo effects can under-\nmine the validity of study results when user expectations are\naltered through the presence of an AI [88, 89]. In future work\nand in the case of using LLMs, placebo conditions must be in-\ncluded to avoid findings that are not a result of increased user\nexpectations towards the capabilities of ChatGPT.\n15https://bard.google.com/ - last access 20-10-2023\n16https://openai.com/blog/openai-codex - last access 20-10-2023\n14\n7.6.2. Prompting Strategy\nNext to other models, the prompts used in our studies also\nconstitute a limiting factor. We cannot exclude the possibility\nthat other prompts, using a different structure or wordings, may\nachieve better results for the investigated use cases. In addition,\nit should be emphasized that the generated texts are subject to\nstochastic processes, which can lead to deviations even when\nreusing the same prompts.\n7.6.3. Limited Number of Domain Experts\nIn the context of our studies, only four domain experts eval-\nuated the outputs of ChatGPT. In some cases, generated expla-\nnations were assessed by one person only (e.g., Study II). This\nlow number of experts limits the validity and generalizability\nof the results and o ffers the risk of subjective bias. However,\nrecruitment for such studies is di fficult because the number of\npotential participants is small and they often have strongly lim-\nited time budgets, making study design challenging. Please\nnote that for experts in the field, even “just” familiarizing them-\nselves with an unfamiliar workflow is a challenging and time-\nconsuming endeavor.\n7.6.4. Investigated domains and selected workflows\nOur study explores real-world workflows from the two do-\nmains, bioinformatics and earth observation. Of course, these\nonly represent part of the full range of workflows in the natu-\nral sciences. It constitutes an exciting follow-up research ques-\ntion: how suitable ChatGPT and other generative LLMs are in\nother research contexts, such as climate research [32] and as-\ntronomy [31], and whether it is possible to identify categories\nor groups of domains which are particularly well (or poorly)\nsupported. Furthermore, we only examined two workflow sys-\ntems, Nextflow and Apache Airflow, leaving other alternatives,\nsuch as Snakemake, Taverna, and Pegasus, for future work.\n7.6.5. Explored Use Cases\nThis work focused on comprehending, modifying, and ex-\ntending workflows with ChatGPT. These use cases represent\nonly a partial scope of user support opportunities and are worth\nconsidering and evaluating other use cases. For instance, mi-\ngrating workflows implemented in legacy workflow manage-\nment systems to more recent ones, e.g., transforming Taverna\n[74] scripts to Snakemake or Nextflow, or adapting them to dif-\nferent infrastructure stacks poses an interesting research ques-\ntion. Moreover, user support in workflow debugging, error iden-\ntification, or optimization, as done in classical programming [61],\nwould be a valuable contribution to research scientists.\n8. Conclusion\nThe significance of large-scale data analysis workflows in\nadvancing research in the natural sciences is growing steadily.\nDevelopers of such workflows, primarily researchers from di-\nverse scientific fields, are challenged with the increasing com-\nplexity and scale of their analyses, which involve (next to their\ndomain knowledge) working with di fferent frameworks, tools,\nprogramming languages, and infrastructure stacks. Although\na few tools for creating and maintaining workflows are avail-\nable, improving user efficiency remains an open research area.\nIn this work, we contribute to this situation by evaluating the\nsuitability of ChatGPT for comprehending, modifying, and ex-\ntending scientific workflows. In three user studies with four\nresearchers from different scientific domains, we evaluated the\ncorrectness of ChatGPT regarding explainability, exchange of\nsoftware components, and extension when providing real-world\nscientific workflow descriptions. Our results show a high ac-\ncuracy for comprehending and explaining scientific workflows\nwhile achieving a reduced performance for modifying and ex-\ntending workflow descriptions. These findings clearly illustrate\nthe need for further research in this area.\nAcknowledgments\nThis work is supported by German Research Foundation\n(DFG), CRC 1404: ”FONDA: Foundations of Workflows for\nLarge-Scale Scientific Data Analysis” (Project-ID 414984028).\nDeclaration of competing interest\nThe authors declare that they have no known competing fi-\nnancial interests or personal relationships that could have ap-\npeared to influence the work reported in this paper.\nAuthor contributions\nMario S ¨anger: Conceptualization, Methodology, Investi-\ngation, Visualization, Writing - Original Draft Ninon De Mec-\nquenem: Resources, Data Curation, Writing - Review & Edit-\ning Katarzyna Ewa Lewi´nska: Resources, Data Curation, Writ-\ning - Review & Editing Vasilis Bountris: Resources, Data Cu-\nration, Writing - Review & Editing Fabian Lehmann : Re-\nsources, Data Curation, Writing - Review & EditingUlf Leser:\nConceptualization, Writing - Review & Editing, Project admin-\nistration, Funding acquisition Thomas Kosch: Conceptualiza-\ntion, Methodology, Writing - Original Draft , Writing - Review\n& Editing, Validation, Project administration\nDeclaration of generative AI and AI-assisted technologies in\nthe writing process\nDuring the preparation of this work the author(s) used Gram-\nmarly and ChatGPT for linguistic revision. After using this\ntool/service, the author(s) reviewed and edited the content as\nneeded and take(s) full responsibility for the content of the pub-\nlication.\nReferences\n[1] S. B. Davidson, J. Freire, Provenance and scientific workflows: Chal-\nlenges and opportunities, in: Proceedings of the 2008 ACM SIGMOD\nInternational Conference on Management of Data, SIGMOD ’08, As-\nsociation for Computing Machinery, New York, NY , USA, 2008, p.\n1345–1350.\n15\n[2] S. Cohen-Boulakia, K. Belhajjame, O. Collin, J. Chopard, C. Froidevaux,\nA. Gaignard, K. Hinsen, P. Larmande, Y . L. Bras, F. Lemoine, F. Mareuil,\nH. M ´enager, C. Pradal, C. Blanchet, Scientific workflows for computa-\ntional reproducibility in the life sciences: Status, challenges and opportu-\nnities, Future Generation Computer Systems 75 (2017) 284–298.\n[3] L. Wratten, A. Wilm, J. G ¨oke, Reproducible, scalable, and shareable anal-\nysis pipelines with bioinformatics workflow managers, Nature methods\n18 (10) (2021).\n[4] J. Ison, H. Ienasescu, P. Chmura, E. Rydza, H. M ´enager, M. Kala ˇs,\nV . Schw ¨ammle, B. Gr ¨uning, N. Beard, R. Lopez, S. Duvaud,\nH. Stockinger, B. Persson, R. Va ˇrekov´a, T. Ra ˇcek, J. V ondr´aˇsek, H. Pe-\nterson, A. Salumets, I. Jonassen, R. Hooft, T. Nyr ¨onen, A. Valencia,\nS. Capella, J. Gelp ´ı, F. Zambelli, B. Savakis, B. Lesko ˇsek, K. Rapacki,\nC. Blanchet, R. Jimenez, A. Oliveira, G. Vriend, O. Collin, J. van Helden,\nP. Løngreen, S. Brunak, The bio.tools registry of software tools and data\nresources for the life sciences, Genome Biology (Online Edition) 20 (1)\n(2019).\n[5] R. Ferreira da Silva, H. Casanova, K. Chard, I. Altintas, R. M. Badia,\nB. Balis, T. Coleman, F. Coppens, F. Di Natale, B. Enders, T. Fahringer,\nR. Filgueira, G. Fursin, D. Garijo, C. Goble, D. Howell, S. Jha, D. S. Katz,\nD. Laney, U. Leser, M. Malawski, K. Mehta, L. Pottier, J. Ozik, J. L. Pe-\nterson, L. Ramakrishnan, S. Soiland-Reyes, D. Thain, M. Wolf, A com-\nmunity roadmap for scientific workflows research and development, in:\nWorkshop on Workflows in Support of Large-Scale Science (WORKS),\n2021.\n[6] S. Cohen-Boulakia, U. Leser, Search, adapt, and reuse: the future of sci-\nentific workflows, ACM SIGMOD Record 40 (2) (2011) 6–16.\n[7] E. Deelman, T. Peterka, I. Altintas, C. D. Carothers, K. K. van Dam,\nK. Moreland, M. Parashar, L. Ramakrishnan, M. Taufer, J. Vetter, The\nfuture of scientific workflows, The International Journal of High Perfor-\nmance Computing Applications 32 (1) (2018) 159–175.\n[8] B. A. Myers, M. B. Rosson, Survey on user interface programming, in:\nProceedings of the SIGCHI conference on Human factors in computing\nsystems, 1992, pp. 195–202.\n[9] B. R. Barricelli, F. Cassano, D. Fogli, A. Piccinno, End-user development,\nend-user programming and end-user software engineering: A systematic\nmapping study, Journal of Systems and Software 149 (2019) 101–137.\n[10] S. Lau, S. S. Srinivasa Ragavan, K. Milne, T. Barik, A. Sarkar, Tweakit:\nSupporting end-user programmers who transmogrify code, in: Proceed-\nings of the 2021 CHI Conference on Human Factors in Computing Sys-\ntems, 2021, pp. 1–12.\n[11] N. Peitek, A. Bergum, M. Rekrut, J. Mucke, M. Nadig, C. Parnin, J. Sieg-\nmund, S. Apel, Correlates of programmer efficacy and their link to expe-\nrience: A combined eeg and eye-tracking study, in: Proceedings of the\n30th ACM Joint European Software Engineering Conference and Sym-\nposium on the Foundations of Software Engineering, ESEC /FSE 2022,\nAssociation for Computing Machinery, New York, NY , USA, 2022, p.\n120–131.\n[12] T. Kosch, J. Karolus, J. Zagermann, H. Reiterer, A. Schmidt, P. W.\nWo´zniak, A Survey on Measuring Cognitive Workload in Human-\nComputer Interaction, ACM Computing Surveys (jan 2023).\n[13] T. J.-J. Li, A. Azaria, B. A. Myers, Sugilite: creating multimodal smart-\nphone automation by demonstration, in: Proceedings of the 2017 CHI\nconference on human factors in computing systems, 2017, pp. 6038–\n6049.\n[14] A. R. Sereshkeh, G. Leung, K. Perumal, C. Phillips, M. Zhang, A. Fazly,\nI. Mohomed, Vasta: a vision and language-assisted smartphone task au-\ntomation system, in: Proceedings of the 25th international conference on\nintelligent user interfaces, 2020, pp. 22–32.\n[15] S. G. Tamilselvam, N. Panwar, S. Khare, R. Aralikatte, A. Sankaran,\nS. Mani, A visual programming paradigm for abstract deep learning\nmodel development, in: Proceedings of the 10th Indian Conference on\nHuman-Computer Interaction, 2019, pp. 1–11.\n[16] E. Coronado, D. Deu ff, P. Carreno-Medrano, L. Tian, D. Kuli´c, S. Sumar-\ntojo, F. Mastrogiovanni, G. Venture, Towards a modular and distributed\nend-user development framework for human-robot interaction, IEEE Ac-\ncess 9 (2021) 12675–12692.\n[17] T. J.-J. Li, End user programing of intelligent agents using demonstra-\ntions and natural language instructions, in: Proceedings of the 24th In-\nternational Conference on Intelligent User Interfaces: Companion, 2019,\npp. 143–144.\n[18] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\nC. Zhang, S. Agarwal, K. Slama, A. Ray, et al., Training language mod-\nels to follow instructions with human feedback, Advances in Neural In-\nformation Processing Systems 35 (2022) 27730–27744.\n[19] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar, et al.,\nLlama: Open and e fficient foundation language models, arXiv preprint\narXiv:2302.13971 (2023).\n[20] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili ´c, D. Hesslow, R. Castagn´e,\nA. S. Luccioni, F. Yvon, M. Gall´e, et al., Bloom: A 176b-parameter open-\naccess multilingual language model, arXiv preprint arXiv:2211.05100\n(2022).\n[21] G. Bimbatti, D. Fogli, L. Gargioni, Can chatgpt support end-user devel-\nopment of robot programs? (2023).\n[22] D. Sobania, M. Briesch, C. Hanna, J. Petke, An analysis of the automatic\nbug fixing performance of chatgpt, arXiv preprint arXiv:2301.08653\n(2023).\n[23] M. X. Liu, A. Sarkar, C. Negreanu, B. Zorn, J. Williams, N. Toronto,\nA. D. Gordon, “what it wants me to say”: Bridging the abstraction gap\nbetween end-user programmers and code-generating large language mod-\nels, in: Proceedings of the 2023 CHI Conference on Human Factors in\nComputing Systems, 2023, pp. 1–31.\n[24] J. White, S. Hays, Q. Fu, J. Spencer-Smith, D. C. Schmidt, Chatgpt\nprompt patterns for improving code quality, refactoring, requirements\nelicitation, and software design, arXiv preprint arXiv:2303.07839 (2023).\n[25] N. M. S. Surameery, M. Y . Shakor, Use chat gpt to solve programming\nbugs, International Journal of Information Technology & Computer En-\ngineering (IJITC) ISSN: 2455-5290 3 (01) (2023) 17–22.\n[26] P. Ewels, A. Peltzer, S. Fillinger, H. Patel, J. Alneberg, A. Wilm, M. Gar-\ncia, P. Di Tommaso, S. Nahnsen, The nf-core framework for community-\ncurated bioinformatics pipelines., Nature biotechnology 38 (3) (2020).\n[27] B. Almarie, P. E. Teixeira, K. Pacheco-Barrios, C. Rossetti, F. Fregni, The\nuse of large language models in science: Opportunities and challenges,\nPrinciples and Practice of Clinical Research 9 (1) (2023) 1–4.\n[28] H. Hassani, E. S. Silva, The role of chatgpt in data science: how ai-\nassisted conversational interfaces are revolutionizing the field, Big data\nand cognitive computing 7 (2) (2023) 62.\n[29] C. Liang, J. Karolus, T. Kosch, A. Schmidt, On the suitability of real-\ntime assessment of programming proficiency using gaze properties, in:\nProceedings of the 7th ACM International Symposium on Pervasive Dis-\nplays, PerDis ’18, Association for Computing Machinery, New York, NY ,\nUSA, 2018.\n[30] L. Wratten, A. Wilm, J. G ¨oke, Reproducible, scalable, and shareable anal-\nysis pipelines with bioinformatics workflow managers, Nature methods\n18 (10) (2021) 1161–1168.\n[31] F. Ahmad, W. Ahmad, An e fficient astronomical image processing tech-\nnique using advance dynamic workflow scheduler in cloud environment,\nInternational Journal of Information Technology 14 (6) (2022) 2779–\n2791.\n[32] J. M. Kunkel, L. R. Pedro, Potential of i/o aware workflows in climate and\nweather, Supercomputing Frontiers and Innovations 7 (2) (2020) 35–53.\n[33] F. Lehmann, D. Frantz, S. Becker, U. Leser, P. Hostert, Force on nextflow:\nScalable analysis of earth observation data on commodity clusters., in:\nCIKM Workshops, 2021.\n[34] J. Yu, R. Buyya, A taxonomy of scientific workflow systems for grid com-\nputing, ACM Sigmod Record 34 (3) (2005) 44–49.\n[35] C. S. Liew, M. P. Atkinson, M. Galea, T. F. Ang, P. Martin, J. I. V . Hemert,\nScientific workflows: moving across paradigms, ACM Computing Sur-\nveys (CSUR) 49 (4) (2016) 1–39.\n[36] Y . Gil, E. Deelman, M. Ellisman, T. Fahringer, G. Fox, D. Gannon,\nC. Goble, M. Livny, L. Moreau, J. Myers, Examining the challenges of\nscientific workflows, Computer 40 (12) (2007) 24–32.\n[37] B. P. Harenslak, J. de Ruiter, Data Pipelines with Apache Airflow, Simon\nand Schuster, 2021.\n[38] J. Goecks, A. Nekrutenko, J. Taylor, G. T. team@ galaxyproject. org,\nGalaxy: a comprehensive approach for supporting accessible, repro-\nducible, and transparent computational research in the life sciences,\nGenome biology 11 (2010) 1–13.\n[39] P. Di Tommaso, M. Chatzou, E. W. Floden, P. P. Barja, E. Palumbo,\nC. Notredame, Nextflow enables reproducible computational workflows,\nNature biotechnology 35 (4) (2017) 316–319.\n16\n[40] E. Deelman, K. Vahi, G. Juve, M. Rynge, S. Callaghan, P. J. Maechling,\nR. Mayani, W. Chen, R. F. Da Silva, M. Livny, et al., Pegasus, a workflow\nmanagement system for science automation, Future Generation Computer\nSystems 46 (2015) 17–35.\n[41] J. K ¨oster, S. Rahmann, Snakemake—a scalable bioinformatics workflow\nengine, Bioinformatics 28 (19) (2012) 2520–2522.\n[42] J. D. M.-W. C. Kenton, L. K. Toutanova, Bert: Pre-training of deep bidi-\nrectional transformers for language understanding, in: Proceedings of\nNAACL-HLT, 2019, pp. 4171–4186.\n[43] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., Language models\nare few-shot learners, Advances in neural information processing systems\n33 (2020) 1877–1901.\n[44] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shak-\neri, E. Taropa, P. Bailey, Z. Chen, et al., Palm 2 technical report, arXiv\npreprint arXiv:2305.10403 (2023).\n[45] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T.\nCheng, A. Jin, T. Bos, L. Baker, Y . Du, et al., Lamda: Language models\nfor dialog applications, arXiv preprint arXiv:2201.08239 (2022).\n[46] A. Yuan, A. Coenen, E. Reif, D. Ippolito, Wordcraft: story writing with\nlarge language models, in: 27th International Conference on Intelligent\nUser Interfaces, 2022, pp. 841–852.\n[47] S. Petridis, N. Diakopoulos, K. Crowston, M. Hansen, K. Henderson,\nS. Jastrzebski, J. V . Nickerson, L. B. Chilton, Anglekindling: Supporting\njournalistic angle ideation with large language models, in: Proceedings\nof the 2023 CHI Conference on Human Factors in Computing Systems,\n2023, pp. 1–16.\n[48] E. Jiang, K. Olson, E. Toh, A. Molina, A. Donsbach, M. Terry, C. J. Cai,\nPromptmaker: Prompt-based prototyping with large language models, in:\nCHI Conference on Human Factors in Computing Systems Extended Ab-\nstracts, 2022, pp. 1–8.\n[49] B. Wang, G. Li, Y . Li, Enabling conversational interaction with mobile ui\nusing large language models, in: Proceedings of the 2023 CHI Confer-\nence on Human Factors in Computing Systems, 2023, pp. 1–17.\n[50] S. Wang, S. Petridis, T. Kwon, X. Ma, L. B. Chilton, Popblends: Strate-\ngies for conceptual blending with large language models, in: Proceedings\nof the 2023 CHI Conference on Human Factors in Computing Systems,\n2023, pp. 1–19.\n[51] H. Osone, J.-L. Lu, Y . Ochiai, Buncho: ai supported story co-creation via\nunsupervised multitask learning to increase writers’ creativity in japanese,\nin: Extended Abstracts of the 2021 CHI Conference on Human Factors in\nComputing Systems, 2021, pp. 1–10.\n[52] C. Wang, X. Liu, D. Song, Language models are open knowledge graphs,\narXiv preprint arXiv:2010.11967 (2020).\n[53] P. Manakul, A. Liusie, M. J. Gales, Selfcheckgpt: Zero-resource black-\nbox hallucination detection for generative large language models, arXiv\npreprint arXiv:2303.08896 (2023).\n[54] B. Peng, M. Galley, P. He, H. Cheng, Y . Xie, Y . Hu, Q. Huang, L. Liden,\nZ. Yu, W. Chen, et al., Check your facts and try again: Improving large\nlanguage models with external knowledge and automated feedback, arXiv\npreprint arXiv:2302.12813 (2023).\n[55] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Ed-\nwards, Y . Burda, N. Joseph, G. Brockman, et al., Evaluating large lan-\nguage models trained on code, arXiv preprint arXiv:2107.03374 (2021).\n[56] C. Clement, D. Drain, J. Timcheck, A. Svyatkovskiy, N. Sundaresan,\nPymt5: multi-mode translation of natural language and python code with\ntransformers, in: Proceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP), 2020, pp. 9052–9065.\n[57] Y . Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Ec-\ncles, J. Keeling, F. Gimeno, A. Dal Lago, et al., Competition-level code\ngeneration with alphacode, Science 378 (6624) (2022) 1092–1097.\n[58] H. Le, Y . Wang, A. D. Gotmare, S. Savarese, S. C. H. Hoi, Coderl: Mas-\ntering code generation through pretrained models and deep reinforcement\nlearning, Advances in Neural Information Processing Systems 35 (2022)\n21314–21328.\n[59] N. Jain, S. Vaidyanath, A. Iyer, N. Natarajan, S. Parthasarathy, S. Ra-\njamani, R. Sharma, Jigsaw: Large language models meet program syn-\nthesis, in: Proceedings of the 44th International Conference on Software\nEngineering, 2022, pp. 1219–1231.\n[60] E. Jiang, E. Toh, A. Molina, K. Olson, C. Kayacik, A. Donsbach, C. J.\nCai, M. Terry, Discovering the syntax and strategies of natural language\nprogramming with generative language models, in: Proceedings of the\n2022 CHI Conference on Human Factors in Computing Systems, 2022,\npp. 1–19.\n[61] P. Vaithilingam, T. Zhang, E. L. Glassman, Expectation vs. experience:\nEvaluating the usability of code generation tools powered by large lan-\nguage models, in: Chi conference on human factors in computing systems\nextended abstracts, 2022, pp. 1–7.\n[62] A. M. Dakhel, V . Majdinasab, A. Nikanjam, F. Khomh, M. C. Desmarais,\nZ. M. J. Jiang, Github copilot ai pair programmer: Asset or liability?,\nJournal of Systems and Software 203 (2023) 111734.\n[63] M. Kazemitabaar, J. Chow, C. K. T. Ma, B. J. Ericson, D. Weintrop,\nT. Grossman, Studying the e ffect of ai code generators on supporting\nnovice learners in introductory programming, in: Proceedings of the 2023\nCHI Conference on Human Factors in Computing Systems, 2023, pp. 1–\n23.\n[64] P. Maddigan, T. Susnjak, Chat2vis: Generating data visualisations via\nnatural language using chatgpt, codex and gpt-3 large language models,\nIEEE Access (2023).\n[65] M. M. Hassan, A. Knipper, S. K. K. Santu, Chatgpt as your personal data\nscientist, arXiv preprint arXiv:2305.13657 (2023).\n[66] Z. Zahra, Z. Li, R. Filgueira, Laminar: A new serverless stream-\nbased framework with semantic code search and code completion, arXiv\npreprint arXiv:2309.00584 (2023).\n[67] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion,\nO. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, et al., Scikit-\nlearn: Machine learning in python, the Journal of machine Learning re-\nsearch 12 (2011) 2825–2830.\n[68] P. Sedgwick, N. Greenwood, Understanding the hawthorne e ffect, Bmj\n351 (2015).\n[69] M. Sanvicente-Garc ´ıa, A. Garc ´ıa-Valiente, S. Jouide, J. Jaraba-Wallace,\nE. Bautista, M. Escobosa, A. S´anchez-Mej´ıas, M. G¨uell, Crispr-analytics\n(crispr-a): A platform for precise analytics and simulations for gene edit-\ning, PLOS Computational Biology 19 (5) (2023) e1011137.\n[70] M. Sudmanns, D. Tiede, S. Lang, H. Bergstedt, G. Trost, H. Augustin,\nA. Baraldi, T. Blaschke, Big earth data: disruptive changes in earth ob-\nservation data management and analysis?, International Journal of Digital\nEarth 13 (7) (2020) 832–850.\n[71] K. E. Lewi ´nska, J. Buchner, B. Bleyhl, P. Hostert, H. Yin, T. Kuemmerle,\nV . C. Radeloff, Changes in the grasslands of the caucasus based on cu-\nmulative endmember fractions from the full 1987–2019 landsat record,\nScience of Remote Sensing 4 (2021) 100035.\n[72] K. E. Lewi ´nska, P. Hostert, J. Buchner, B. Bleyhl, V . C. Radeloff, Short-\nterm vegetation loss versus decadal degradation of grasslands in the cau-\ncasus based on cumulative endmember fractions, Remote Sensing of En-\nvironment 248 (2020) 111969.\n[73] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea, H. Gilbert, A. El-\nnashar, J. Spencer-Smith, D. C. Schmidt, A prompt pattern catalog to en-\nhance prompt engineering with chatgpt, arXiv preprint arXiv:2302.11382\n(2023).\n[74] T. Oinn, M. Addis, J. Ferris, D. Marvin, M. Senger, M. Greenwood,\nT. Carver, K. Glover, M. R. Pocock, A. Wipat, et al., Taverna: a tool\nfor the composition and enactment of bioinformatics workflows, Bioin-\nformatics 20 (17) (2004) 3045–3054.\n[75] E. L. Van Dijk, H. Auger, Y . Jaszczyszyn, C. Thermes, Ten years of next-\ngeneration sequencing technology, Trends in genetics 30 (9) (2014) 418–\n426.\n[76] T. Hu, N. Chitnis, D. Monos, A. Dinh, Next-generation sequencing tech-\nnologies: An overview, Human Immunology 82 (11) (2021) 801–811.\n[77] S. Chen, Y . Zhou, Y . Chen, J. Gu, fastp: an ultra-fast all-in-one fastq\npreprocessor, Bioinformatics 34 (17) (2018) i884–i890.\n[78] A. Dobin, C. A. Davis, F. Schlesinger, J. Drenkow, C. Zaleski, S. Jha,\nP. Batut, M. Chaisson, T. R. Gingeras, Star: ultrafast universal rna-seq\naligner, Bioinformatics 29 (1) (2013) 15–21.\n[79] B. Langmead, C. Trapnell, M. Pop, S. L. Salzberg, Ultrafast and memory-\nefficient alignment of short dna sequences to the human genome, Genome\nbiology 10 (3) (2009) 1–10.\n[80] B. Langmead, S. L. Salzberg, Fast gapped-read alignment with bowtie 2,\nNature methods 9 (4) (2012) 357–359.\n[81] D. Kim, J. M. Paggi, C. Park, C. Bennett, S. L. Salzberg, Graph-based\ngenome alignment and genotyping with hisat2 and hisat-genotype, Nature\nbiotechnology 37 (8) (2019) 907–915.\n17\n[82] A. M. Bolger, M. Lohse, B. Usadel, Trimmomatic: a flexible trimmer for\nillumina sequence data, Bioinformatics 30 (15) (2014) 2114–2120.\n[83] M. Martin, Cutadapt removes adapter sequences from high-throughput\nsequencing reads, EMBnet. journal 17 (1) (2011) 10–12.\n[84] C. Zeyen, L. Malburg, R. Bergmann, Adaptation of scientific workflows\nby means of process-oriented case-based reasoning, in: Case-Based Rea-\nsoning Research and Development: 27th International Conference, IC-\nCBR 2019, Otzenhausen, Germany, September 8–12, 2019, Proceedings\n27, Springer, 2019, pp. 388–403.\n[85] C. Trapnell, B. A. Williams, G. Pertea, A. Mortazavi, G. Kwan, M. J.\nVan Baren, S. L. Salzberg, B. J. Wold, L. Pachter, Transcript assembly\nand quantification by rna-seq reveals unannotated transcripts and isoform\nswitching during cell differentiation, Nature biotechnology 28 (5) (2010)\n511–515.\n[86] P. Danecek, J. K. Bonfield, J. Liddle, J. Marshall, V . Ohan, M. O.\nPollard, A. Whitwham, T. Keane, S. A. McCarthy, R. M. Davies, H. Li,\nTwelve years of SAMtools and BCFtools, GigaScience 10 (2), giab008\n(02 2021). arXiv:https://academic.oup.com/gigascience/\narticle-pdf/10/2/giab008/36332246/giab008.pdf,\ndoi:10.1093/gigascience/giab008.\nURL https://doi.org/10.1093/gigascience/giab008\n[87] B. Rozi `ere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y . Adi,\nJ. Liu, T. Remez, J. Rapin, et al., Code llama: Open foundation models\nfor code, arXiv preprint arXiv:2308.12950 (2023).\n[88] A. M. Kloft, R. Welsch, T. Kosch, S. Villa, ”AI enhances our perfor-\nmance, I have no doubt this one will do the same”: The Placebo Effect Is\nRobust to Negative Descriptions of AI, 2023. arXiv:2309.16606.\n[89] T. Kosch, R. Welsch, L. Chuang, A. Schmidt, The Placebo E ffect of Ar-\ntificial Intelligence in Human-Computer Interaction, ACM Transaction\nComputer-Human Interaction 29 (6) (jan 2023).\n18\nAppendix A. Questionnaire Study I\nTable A.1: Feedback form for the first user study which investigates the capabilities of ChatGPT to capture the content of a workflow description. For each item,\nwe added a comment field to report issues and errors in the generated explanations if the domain expert does not fully apply the content.\nPrompt ID Question Answer options\nP1 1 Q1 1 The generated explanation matches the research area\nof the workflow\n5-point Likert scale\nQ1 2 The generated explanation matches the overall aim of\nthe workflow\n5-point Likert scale\nP1 2 Q1 3 How many of the tasks of the workflow (see task\noverview below) are contained in the description?\nNumerical\nQ1 4 The explanation describes the tasks of the workflow\ncorrectly\n(1) Don’t know\n(2) None of the tasks is correct\n(3) Few tasks are correct\n(4) Most tasks are correct\n(5) All tasks are correct\nQ1 5 How many of the software programs / tools used in\nthe workflow are mentioned in the explanation?\nNumerical\nQ1 6 The explanation of the software programs / tools used\nis correct\n(1) Don’t know\n(2) None of the software programs / tools is correct\n(3) Few software programs / tools are correct\n(4) Most software programs / tools are correct\n(5) All software programs / tools are correct\nP1 3 Q1 7 The explanation matches the input data specification\nof the workflow\n5-point Likert scale\nP1 4 Q1 8 The generated explanation matches the overall result\nof the workflow\n5-point Likert scale\nP1 5 Q1 9 How many of the generated research questions are\nvalid?\nNumerical\n19\nAppendix B. Questionnaire Study II\nTable B.1: Feedback form for the second user study which investigates the capabilities of ChatGPT in exchanging the used tools in a scientific workflow. For each\nitem we added a comment field to report issues and errors in the generated explanations if the domain expert doesn’t fully apply with the content.\nPrompt ID Question Answer options\nP2 1 Q2 1 How many of the 10 alternative tools are valid? Numerical\nP2 2 Q2 2 The selected tools are reasonable alternatives for the task? 5-point Likert scale\nQ2 3 The generated explanation for the first tool highlights the suitability of the tool for\nthe task well\n5-point Likert scale\nQ2 4 The generated explanation for second tool highlights the suitability of the tool for\nthe task well\n5-point Likert scale\nP2 3 Q2 5 The generated explanation helps to understand the methodical di fferences 5-point Likert scale\nQ2 6 The generated explanation highlights strengthens and weaknesses of [original-\ntool] correctly\n5-point Likert scale\nQ2 7 The generated explanation highlights strengthens and weaknesses of [alternative-\ntool] correctly\n5-point Likert scale\nP2 4 Q2 8 The adaption of the workflow is correct 5-point Likert scale\nQ2 9 The adaption of the task [task-name] is correct 5-point Likert scale\nQ2 10 The explanation of the not-supported features by [alternative-tool] is correct 5-point Likert scale\nQ2 11 Can the workflow be executed without errors? Yes /No\nQ2 12 How long did it take to correct the workflow? Numerical\nQ2 13 What had to be adapted to make the workflow executable? Free-text\n20\nAppendix C. Questionnaire Study III\nTable C.1: Feedback form for the third user study which investigates the capabilities of ChatGPT to extend a given (partial) workflow script. For each item we\nadded a comment field to report issues and errors in the generated explanations if the domain expert doesn’t fully apply with the content.\nPrompt ID Question Answer options\nP3 1 Q3 1 Is the list of necessary steps complete? Yes /No\nQ3 2 How many of the proposed steps are correct? Please give your answer\nin the form of ”x out of y steps”.\nFree text\nQ3 3 How many of the proposed tools are correct? Please give your answer\nin the form of ”x out of y tools”.\nFree text\nP3 2a/b Q3 4 The extension of the workflow is correct 5-point Likert scale\nQ3 5 Can the workflow be executed without errors? Yes /No\nQ3 6 How long did it take to correct the workflow? Enter ”not finished” when\nyou set yourself a time limit that has run up.\nFree text\nQ3 7 What had to be adapted to make the workflow executable? Free text\n21\nAppendix D. Schema WF4-Grasslands\nNormalized L2\nTime Series\nEndmember\nDefinitions\nSMA & LSP & RBF\nforce\nGrowing Season\nExtraction\npython\nTS Composition\n(fold and fill)\npython\nCumulative Endmember\nFractions (CEF)\npython\nStatistical\nResults\nAutoregressive Trend\nAnalyses\nR\nGeneralized Least\nSquare Regression\nR\nTime Series\nSOS & EOS\nRBF Time Series\n(gv, npv, soil, shade)\nTime Series\n(gv, npv, soil, shade, RMSE)\nCEF Time Series\n(gv, npv, soil, shade)\nGrassland\nMask\nAbbreviations:\nEOS: End of Season\ngv: Green Vegetation\nnpv: Non-phtotosynthetic Vegetation\nLSP: Land Surface Phenology\nRBF: Radial Basis Function\nRMSE: Root Mean Square Error\nSMA: Spectral Mixture Analysis\nSOS: Start of Season \nTS: Time Series\nTrend Component,\nCoefficients, Residuals\n(gv, npv, soil, shade)\nMonthly Time Series\n(gv, npv, soil, shade)\nSOS\nRaster\nEOS\nRaster\nInput Data\nComputational Task / Step\nIntermediate Result Data\nOutput Data\nFigure D.1: Overview of the earth observation workflowWF4-Grasslands developed by one of the domain experts. The workflow aims at understanding differences\nin long-term changes (1984-2022) in ground cover fractions specific to European grasslands depending on the definition of endmembers (i.e., unique spectral\nsignatures of a specific material or ground cover) approximating these fractions. The figure highlights the conceptual schema and data flow of the workflow.\n22\nAppendix E. Verbal Task Description\nIn Study III, we use the following task description, provided by the earth observation expert, for extending WF4-Grasslands\nwith an autoregressive trend analysis:\n- step 1: it reads the data from a multi-band TIFF file into a raster brick as defined with the raster package. The input\nfile needs to be selected based on a location specified by a string parameter passed into the script\n- step 2: it converts all 0s in the data brick to ’NA’\n- step 3: it fits per-pixel time trend with Auto Regressive trend function from the remotePARTS R package. Each\nraster in a data brick represents one time step in a time series over which the trend is fitted. The result of the trend\nfitting should be a multi-band raster in a TIFF format that comprises bands with information on trend’s slope, intercept,\np-value, temporal auto-correlation, and residuals for each time step.\n- The steps 1-3 should be applied repetitively and independently to four raster datasets available for each location,\nnamely: GV , NPV , SOIL, and SHADE.\n- The script should be implemented using parallel cluster processing functionality in the raster package and provide\ncontrol over the number of nodes, and RAM available to the process.\nAppendix F. GitHub Search Results\nTable F.1: Statistics of the search results for four different scientific workflow systems using the GitHub search engine. For each system we use the system name as\nsearch term and restrict the result repositories to be created before the date give by the column (group).\nSearch <2021-09-01 <2023-09-01\nTerm # Repositories # Pull requests # Repositories # Commits\napache airflow 1,200 27,000 2,600 85,000\nnextflow 1,700 5,000 3,200 14,000\nsnakemake 1,900 5,000 3,400 18,000\ntaverna 352 244 472 645\n23",
  "topic": "Workflow",
  "concepts": [
    {
      "name": "Workflow",
      "score": 0.9380757808685303
    },
    {
      "name": "Computer science",
      "score": 0.7625135183334351
    },
    {
      "name": "Dependability",
      "score": 0.6598851680755615
    },
    {
      "name": "Scalability",
      "score": 0.6035113334655762
    },
    {
      "name": "Data science",
      "score": 0.5146623253822327
    },
    {
      "name": "Software engineering",
      "score": 0.4103173315525055
    },
    {
      "name": "Database",
      "score": 0.20696574449539185
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I39343248",
      "name": "Humboldt-Universität zu Berlin",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I135310074",
      "name": "University of Wisconsin–Madison",
      "country": "US"
    }
  ]
}