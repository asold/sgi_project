{
  "title": "Understanding Large-Language Model (LLM)-powered Human-Robot Interaction",
  "url": "https://openalex.org/W4390722831",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5008142086",
      "name": "Callie Y. Kim",
      "affiliations": [
        "University of Wisconsin‚ÄìMadison"
      ]
    },
    {
      "id": "https://openalex.org/A3097963240",
      "name": "Christine P Lee",
      "affiliations": [
        "University of Wisconsin‚ÄìMadison"
      ]
    },
    {
      "id": "https://openalex.org/A2149956271",
      "name": "Bilge Mutlu",
      "affiliations": [
        "University of Wisconsin‚ÄìMadison"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2594816543",
    "https://openalex.org/W2160379528",
    "https://openalex.org/W2127241481",
    "https://openalex.org/W1985945240",
    "https://openalex.org/W4323536841",
    "https://openalex.org/W2610957708",
    "https://openalex.org/W2161460193",
    "https://openalex.org/W2913542295",
    "https://openalex.org/W2012372995",
    "https://openalex.org/W2057009224",
    "https://openalex.org/W2899785433",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4323537006",
    "https://openalex.org/W4386211903",
    "https://openalex.org/W3113432594",
    "https://openalex.org/W4226391566",
    "https://openalex.org/W2111040806",
    "https://openalex.org/W2245843004",
    "https://openalex.org/W4280534475",
    "https://openalex.org/W2899858254",
    "https://openalex.org/W2553375856",
    "https://openalex.org/W2611961900",
    "https://openalex.org/W2951846435",
    "https://openalex.org/W3017863658",
    "https://openalex.org/W1987768879",
    "https://openalex.org/W3111368623",
    "https://openalex.org/W2063688338",
    "https://openalex.org/W1975401588",
    "https://openalex.org/W2143427281",
    "https://openalex.org/W2082173922",
    "https://openalex.org/W4312965281",
    "https://openalex.org/W2929924974",
    "https://openalex.org/W614131420",
    "https://openalex.org/W2132891289",
    "https://openalex.org/W1979472948",
    "https://openalex.org/W15672360",
    "https://openalex.org/W2048596731",
    "https://openalex.org/W4387993547",
    "https://openalex.org/W4205131770",
    "https://openalex.org/W4383108457",
    "https://openalex.org/W2111633154",
    "https://openalex.org/W4383046944",
    "https://openalex.org/W2072919217",
    "https://openalex.org/W2106291409",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W4386054934",
    "https://openalex.org/W4379054524",
    "https://openalex.org/W4372348475",
    "https://openalex.org/W1975273460",
    "https://openalex.org/W4322718421",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4242073862",
    "https://openalex.org/W4224912544",
    "https://openalex.org/W3143073753",
    "https://openalex.org/W4229506019",
    "https://openalex.org/W1523992493",
    "https://openalex.org/W2127561630",
    "https://openalex.org/W2032568497",
    "https://openalex.org/W4389010438",
    "https://openalex.org/W4323572061",
    "https://openalex.org/W4386213035",
    "https://openalex.org/W4388968137",
    "https://openalex.org/W4386395818",
    "https://openalex.org/W3128912454",
    "https://openalex.org/W135517074",
    "https://openalex.org/W4387642400",
    "https://openalex.org/W2296270309",
    "https://openalex.org/W52943720",
    "https://openalex.org/W4394828156",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2786204509",
    "https://openalex.org/W3212993480",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4364384540",
    "https://openalex.org/W3150248956",
    "https://openalex.org/W3103741966"
  ],
  "abstract": "Large-language models (LLMs) hold significant promise in improving\\nhuman-robot interaction, offering advanced conversational skills and\\nversatility in managing diverse, open-ended user requests in various tasks and\\ndomains. Despite the potential to transform human-robot interaction, very\\nlittle is known about the distinctive design requirements for utilizing LLMs in\\nrobots, which may differ from text and voice interaction and vary by task and\\ncontext. To better understand these requirements, we conducted a user study (n\\n= 32) comparing an LLM-powered social robot against text- and voice-based\\nagents, analyzing task-based requirements in conversational tasks, including\\nchoose, generate, execute, and negotiate. Our findings show that LLM-powered\\nrobots elevate expectations for sophisticated non-verbal cues and excel in\\nconnection-building and deliberation, but fall short in logical communication\\nand may induce anxiety. We provide design implications both for robots\\nintegrating LLMs and for fine-tuning LLMs for use with robots.\\n",
  "full_text": "Understanding Large-Language Model (LLM)-powered\nHuman-Robot Interaction\nCallie Y. Kim‚àó\nDepartment of Computer Sciences\nUniversity of Wisconsin‚ÄìMadison\nMadison, Wisconsin, USA\ncykim6@cs.wisc.edu\nChristine P Lee‚àó\nDepartment of Computer Sciences\nUniversity of Wisconsin‚ÄìMadison\nMadison, Wisconsin, USA\ncplee5@cs.wisc.edu\nBilge Mutlu\nDepartment of Computer Sciences\nUniversity of Wisconsin‚ÄìMadison\nMadison, Wisconsin, USA\nbilge@cs.wisc.edu\nABSTRACT\nLarge-language models (LLMs) hold significant promise in improv-\ning human-robot interaction, offering advanced conversational\nskills and versatility in managing diverse, open-ended user requests\nin various tasks and domains. Despite the potential to transform\nhuman-robot interaction, very little is known about the distinctive\ndesign requirements for utilizing LLMs in robots, which may differ\nfrom text and voice interaction and vary by task and context. To\nbetter understand these requirements, we conducted a user study\n(ùëõ= 32) comparing an LLM-powered social robot against text- and\nvoice-based agents, analyzing task-based requirements in conver-\nsational tasks, including choose, generate, execute, and negotiate.\nOur findings show that LLM-powered robots elevate expectations\nfor sophisticated non-verbal cues and excel in connection-building\nand deliberation, but fall short in logical communication and may\ninduce anxiety. We provide design implications both for robots\nintegrating LLMs and for fine-tuning LLMs for use with robots.\nCCS CONCEPTS\n‚Ä¢ Human-centered computing ‚ÜíHCI design and evaluation\nmethods; ‚Ä¢ Computing methodologies ‚ÜíNatural language\nprocessing; ‚Ä¢ Computer systems organization ‚ÜíRobotics.\nKEYWORDS\nSocial robots; large language models; human-robot interaction\nACM Reference Format:\nCallie Y. Kim, Christine P Lee, and Bilge Mutlu. 2024. Understanding Large-\nLanguage Model (LLM)-powered Human-Robot Interaction. In Proceedings\nof the 2024 ACM/IEEE International Conference on Human-Robot Interaction\n(HRI ‚Äô24), March 11‚Äì14, 2024, Boulder, CO, USA. ACM, New York, NY, USA,\n10 pages. https://doi.org/10.1145/3610977.3634966\n1 INTRODUCTION\nAcross a wide range of day-to-day activities, robots are envisioned\nto possess social and communication skills that allow them to en-\ngage seamlessly and naturally with users [ 9, 34]. Past research\n‚àóBoth authors contributed equally to this research.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nHRI ‚Äô24, March 11‚Äì14, 2024, Boulder, CO, USA\n¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0322-5/24/03. . . $15.00\nhttps://doi.org/10.1145/3610977.3634966\nFigure 1: We investigate people‚Äôs perceptions of and pref-\nerences toward LLM-powered robots. We conducted a user\nstudy that compared an LLM-powered social robot against\ntext-based and voice-based agents. Left: Users participated in\none of four tasks: choose, generate, execute, and negotiate.\nRight: The user engages with (1) the text-based agent by en-\ntering and receiving text-based prompts, (2) the voice-based\nagent through spoken prompts (achieved by the robot‚Äôs voice\nwith the robot concealed behind a black screen, out of the\nuser‚Äôs view), and (3) the LLM-powered social robot via spoken\nprompts, in a counterbalanced order.\non robots has focused on developing these skills, including con-\nversational speech [26, 31], gestures [12, 24, 60], gaze [39, 47, 49],\nand appearance [20, 32, 40] to facilitate effective, continuous, and\ndependable interactions with users. The recent emergence of large-\nlanguage models (LLMs) provides a novel opportunity for robots\nto augment their social and communicative abilities [70]. As these\nmodels enable lifelike conversations, contextual adaptation, and\nconsistent interaction [10, 68], robots can leverage these capabilities\nto improve their communicative proficiency to effectively address\ndiverse user requests across a range of tasks and application do-\nmains. Despite the immense potential of LLM-equipped robots to\ntransform human-robot interaction, a gap exists in the knowledge\nregarding the unique design requirements for robots that harness\narXiv:2401.03217v1  [cs.RO]  6 Jan 2024\nHRI ‚Äô24, March 11‚Äì14, 2024, Boulder, CO, USA Callie Y. Kim, Christine P. Lee, and Bilge Mutlu\nLLMs for conversational and communicative skills, as well as which\ntasks most benefit from utilizing these capabilities.\nRobots are known to have a unique effect on user experience\nand perceptions compared to other forms of embodiment, including\ntext, voice, or virtual agents [16, 35, 41, 46, 53, 54]. Specifically, the\npresence of a robot triggers different cognitive activities, behaviors,\nor actions of a user and elicits different responses such as increased\nenjoyment, perceived social competence, and trust towards the\nrobot [3, 16, 28, 42, 55, 71]. Therefore, it is conceivable that when\nusers engage with robots powered by LLMs, the embodiment of the\nrobot can shape distinct user expectations and perceptions of the\nsophisticated conversational system, which might have implications\nfor how LLMs need to be specifically designed for human-robot\ninteraction or integrated into a robot system.\nThe growing interest in integrating LLMs with robots neces-\nsitates a need to understand the unique design requirements of\nLLMs that are expected to work with robots, including design\nneeds tailored to the tasks and contexts in which LLM-powered\nrobots operate. Previous design requirements for robots have been\ngained through exploring user perceptions regarding various task\nattributes and robot roles [16, 48, 72]. This exploration can similarly\nuncover design opportunities and optimal tasks for LLM-powered\nrobots, shaping future guidelines for robot design and LLM devel-\nopment. To understand the design requirements for utilizing LLMs\nfor robots and identify tasks suitable for integrating LLM-powered\nrobot agents, we formulate the following three research questions\nto guide this investigation: (1) how do people perceive robots using\nLLMs; (2) how do people‚Äôs perceptions of robots using LLMs vary\nacross different task settings; and (3) what task contexts benefit\nfrom the embodiment of a robot when people interact with LLMs?\nTo address our research questions, we conducted a user study\nwith 32 participants that compared different agent types‚Äî text,\nvoice, and robot‚Äîto better understand people‚Äôs perceptions of LLM-\npowered robots compared to other forms of embodiment through\nwhich people interact with LLMs. Additionally, we designed four\nconversational tasks‚Äìexecute, generate, negotiate, and choose, based\non the ‚Äútask circumplex‚Äù by McGrath [44]‚Äîto assess which tasks\ncan benefit from LLM-powered robots. Our findings show that LLM-\npowered robots elicit new expectations for sophisticated non-verbal\ncues, and are preferred in tasks involving connection-building and\ndeliberation between the user and the robot. Conversely, LLM-\npowered robots are less preferred when the LLM‚Äôs rich social capa-\nbilities result in verbose responses, logical and communication er-\nrors, or induce anxiety during task interactions. Finally, we present\ndesign recommendations for LLM-powered robots to enhance fu-\nture HRI. We make the following contributions:\n(1) Compare LLM-powered agents (i.e., text-based, voice-based,\nand social robot) to uncover unique design requirements for\nLLM-powered robots;\n(2) Evaluate LLM effectiveness across tasks (i.e., generate, choose,\nnegotiate, execute) to identify optimal interaction contexts\nwith robot embodiment;\n(3) Present empirical evidence on user perceptions and prefer-\nences for LLM-powered robots in diverse task settings;\n(4) Provide design implications for developing LLM-powered\nrobots and LLMs to improve future human-robot interaction.\n2 RELATED WORK\nEmbodiment. Embodiment plays a pivotal role in shaping how hu-\nmans perceive and engage with robots. We adopt the definition of\nembodiment ‚Äústructural coupling‚Äù from Ziemke [76] such that a\nsystem is embodied if mutual perturbative channels exist. We focus\non physically embodied robots that can leverage rich channels of\ncommunication such as gesture, posture, gaze, facial expressions,\nproxemics, and social touch [16]. Prior research shows that interac-\ntions with physically embodied robots lead to higher user engage-\nment, enjoyment, trust, and empathy compared to text, voice-based,\nor virtual agents [4, 6, 63, 67]. Additionally, embodiment influences\nuser behavior, affecting interaction duration and distance [45, 59].\nSeveral studies have explored how physical embodiment affects\ntask performance and impression by comparing physically embod-\nied robots to virtual agents [21, 46, 62, 72]. These studies indicate\nthat user preferences for embodied agents are influenced not only\nby embodiment but also by the specific task context.\nLLM in Robotics.Robots function as the vital bridge connecting\nthe tangible real world and LLMs. This connection enables LLM\nto infer knowledge from the physical environment through data\ncollected by sensors. Simultaneously, LLMs empower the robot\nwith the capability to comprehend semantic meanings and engage\nin flexible dialogue interactions. Thus, LLMs with robots find their\nprimary applications in task planning [1, 17, 66] or human-robot\ncollaboration [30, 75]. For instance, Ye et al. [75] investigated the\nimplications of LLM-powered robots when users controlled the\nrobot through text for assembly tasks in virtual reality.\nResearchers have also explored the effectiveness of LLMs for con-\nversational robots in specific tasks. Cherakara et al. [11] designed a\nsystem in which the robot displays appropriate facial expressions\nwhen conveying information about the National Robotarium. Irfan\net al. [25] utilized LLMs to create a personalized companion robot\nand examined the challenges associated with open-domain dialogue\nwhen interacting with older adults. Khoo et al. [29] applied LLMs\nto a social robot to enhance the well-being of older adults by gen-\nerating empathetic responses. Yamazaki et al. [74] constructed a\nscenario-based dialogue system for a robot and demonstrated the\neffectiveness of LLMs while establishing trust with users. While\nprior research has primarily concentrated on evaluating the efficacy\nof LLM-powered robots in specific tasks, we aim to explore a wider\narray of tasks and contexts where LLM-powered robots can offer\nadvantages and comprehend the unique design requirements to ef-\nfectively incorporate LLMs with robots across diverse task settings.\n3 METHOD\n3.1 Embodiment Design\nTo understand people‚Äôs perceptions of robots when powered by\nLLMs, we compare a social robot agent against two other agents‚Äìa\ntext-based agent and a voice-based agent. All three agents were\nequipped with GPT-3.5, OpenAI‚Äôs text-davinci-003 model [10] with-\nout fine-tuning. The model parameters were set to temperature =\n0.7 with max tokens = 2048. Pre-prompts were used to outline the\nfour tasks, with parameters identical to those used by Billing et al.\n[7] in Pepperchat.\nUnderstanding Large-Language Model (LLM)-powered Human-Robot Interaction HRI ‚Äô24, March 11‚Äì14, 2024, Boulder, CO, USA\nFigure 2: Interaction Examples per Each Task ‚Äî Participants\nwere assigned to one task among the four ( i.e., execute, nego-\ntiate, choose, and generate) and engaged with all three types\nof agents ( i.e., text, voice, and robot.) Top left to clockwise:\nshows interaction examples of the four tasks.\n3.1.1 Text Agent. Resembling a chatbot, the users interacted with\nthe text agent through text input and output. The user sent and\nreceived prompts via the GPT model with OpenAI API.\n3.1.2 Voice Agent. Simulating a voice assistant, the voice agent\ncommunicated exclusively through voice commands. For the voice\nagent, the participant and the robot were separated by a screen such\nthat the participant only interacted with the agent through voice.\nIt utilized the robot‚Äôs module, ‚ÄúALAudioDevice [57]‚Äù to capture the\nuser‚Äôs speech. The audio recording is then sent to Google Cloud\nservice [19] for speech-to-text analysis, then forwarded to the GPT\nmodel via OpenAI API. The GPT model generates a response, which\nis converted into a speech using the robot Pepper‚Äôs [ 58] module,\n‚ÄúALAnimatedSpeech [56]. ‚Äù The same robot was used for both the\nvoice and robot agent instead of a smart speaker to avoid favoring\none specific technology over another within the broad space of\nvoice-based agents (i.e., smart speakers, smart displays, and virtual\nassistants) and to ensure consistent voice interactions across both\nvoice and robot agent conditions.\n3.1.3 Robot Agent. The social robot, Pepper, was used to engage\nwith users through animated gestures, text-to-speech, and face\nrecognition. For successful communication between the participant\nand the LLM-powered robot, we employed Pepperchat [7], which\nutilizes Google Cloud speech-to-text functionality for speech-based\ndialogue, contributing to a seamless and responsive communica-\ntion experience. We chose a minimalist design for the robotic agent,\nemphasizing its basic embodiment to highlight high-level differ-\nences among text, voice, and robot embodiments, rather than fully\nutilizing non-verbal cues. Thus, we chose to accept an out-of-the-\nbox implementation of each agent, rather than each agent having\nspecific design features (e.g., visual cues for the voice agent.)\n3.2 Task Design\nTo understand the design requirements for LLM-powered robots\nacross various task settings, we designed different tasks based on\nthe Group Task Circumplex Model proposed by McGrath [44]. The\ncircumplex model is structured around two dimensions, ranging\nfrom conflict-based to cooperative, and conceptual to behavioral.\nThe circumplex model classifies group tasks into four categories: (1)\ngenerate: tasks that involve generating ideas or plans; (2) choose:\ntasks that involve choosing a solution or plan from a set of alterna-\ntives where the correct or agreed-upon answer exists; (3) negotiate:\ntasks that involve resolving conflict of viewpoints, interests, and\nmotives; and (4) execute: tasks that involve executing a plan or\nperformance. This framework offers a structured approach to com-\nprehend the nature of the tasks that groups undertake. Figure 2\nshows examples of task interactions. Below we discuss the specific\ntasks designed for our study.\n3.2.1 Generation Task. In the generation task, the agent and the\nparticipant collaboratively create an imaginary story. Participants\nwere asked to follow a general guideline to introduce characters,\nfeatures of the characters, and the setting for story development.\nTo create the foundation and actual story, the participant and agent\ntook turns each adding a sentence. To construct a comprehensive\nstory, the participants were told to ideally incorporate obstacles,\nsolutions to address the obstacles, a climax in the story, and a plot.\n3.2.2 Choosing Task. In the choosing task, the agent assisted the\nparticipants in selecting a subset of items from a collection of items.\nThere was a different theme for the collection of items for each\ntask, including a ski, beach, and camping trip. Participants were\ntold to select items that focused on practicality over leisure. The\nitem criteria were based on those commonly featured as essential\non various travel websites. Participants engaged in discussion with\nthe agent to finalize their item list.\n3.2.3 Execution Task. In the execution task, the agent acted as an\ninstructor and the participant acted as a student. The agent‚Äôs role\nwas to teach the participant how to prepare a beverage in a cafe\nsetting. Only the agent knew which drink to make and participants\nwere asked to follow the instructions. Participants were told to ask\nthe agent if they had any confusion or questions.\n3.2.4 Negotiate Task. In the negotiation task, the agent acted as\na seller of second-hand items and the participant acted as the po-\ntential buyer. The agent‚Äôs goal was to sell the item as expensive as\npossible and the participant‚Äôs goal was to buy the item as cheap as\npossible. The agent was not aware of how much money the partici-\npant held. To control the task settings and provide consistency, an\nabsolute minimum price line was set for the item.\n4 USER STUDY\n4.1 Study Design\nThe study followed a mixed-factorial design with scenario tasks\nas the between-subjects factor and the agent embodiment as the\nwithin-subjects factor. Participants were randomly assigned to one\nof four tasks ( i.e., generate, choose, execute, and negotiate) and\nthen engaged with the three different agents (i.e., text agent, voice\nagent, and robot) in counterbalanced order. At the beginning of the\nstudy, participants were shown interaction examples with the LLM-\npowered agents that involved disagreeing with suggestions, asking\nfollow-up questions, and tracking task progress. Additionally, the\ntask given per agent differed slightly in topic to avoid the learning\neffect (e.g., a camping, beach, and ski trip). Prompts for the tasks\nHRI ‚Äô24, March 11‚Äì14, 2024, Boulder, CO, USA Callie Y. Kim, Christine P. Lee, and Bilge Mutlu\ncan be found in the supplementary materials 1. After interaction\nwith each agent, participants completed questionnaires and a semi-\nstructured interview about their experience. All sessions were held\nin person, audio and video recorded through Zoom [77] on a laptop.\n4.2 Measures\n4.2.1 Subjective Measures. To measure participants‚Äô perception of\nthe agents, we used a modified version of the Godspeed question-\nnaire [5], which includes a series of semantic scales for measuring\nthe robot‚Äôs animacy (Cronbach‚Äôs ùõº = 0.91), anthropomorphism\n(Cronbach‚Äôs ùõº = 0.89), likeability (Cronbach‚Äôs ùõº = 0.93), perceived\nintelligence (Cronbach‚Äôs ùõº = 0.91), and perceived safety (Cron-\nbach‚Äôs ùõº = 0.72) on a five-point rating scale. We modified the\nquestions such that the items asked about their perceptions of\n‚Äúagents‚Äù instead of ‚Äúrobots. ‚Äù Our analysis of the item reliability of\nthe perceived safety subscale found a Cronbach‚Äôs ùõº of ‚àí0.27, due\nto a miscoded item in the subscale.\nGodspeed items are written in a consistent way, such that in\neach group high values of a variable indicate a similar direction.\nSpecifically, the miscoded variable, still (anchored at 1) to surprised\n(anchored at 5) appeared to differ in direction from other items:\nanxious (anchored at 1) to relaxed (anchored at 5) and agitated\n(anchored at 1) to calm (anchored at 5). After re-coding the still-\nsurprised item by flipping the scale so the semantic meaning of the\nitem would be consistent with others, we calculated Cronbach‚Äôs ùõº\nof 0.72. This miscoding of the still-surprised item and correction\nby reverse-coding have been reported by prior work that used the\nGodspeed questionnaire [e.g., 2, 8, 61]. Additionally, upon closer\ninspection of the items of this subscale, which included anxious-\nrelaxed, agitated-calm, and surprised-still (after reversion), we de-\ntermined these items to be a poor fit to the overall construct of\n‚Äúperceived safety‚Äù and decided to exclude it from our analysis.\nIn addition to the Godspeed questionnaire, we measured par-\nticipants‚Äô satisfaction (Cronbach‚Äôs ùõº = 0.96) with the interaction\non a seven-point rating scale (1 = strongly disagree; 7 = strongly\nagree) using the satisfaction subscales from the Usefulness, Satis-\nfaction, and Ease of Use (USE) Questionnaire proposed by Lund\n[38]. The overall Cronbach‚Äôs ùõº value for the Godspeed attributes\nand satisfaction was 0.97.\n4.2.2 Behavior Measures. To observe and understand participant\nbehaviors, we collected measures of the total number of input\ntokens derived from the participants‚Äô prompts. This approach in-\nvolves counting discrete units that the OpenAI API divides from the\nuser‚Äôs input to process the prompt. This metric enables assessment\nof the length of dialogue input provided by the user within the\nconversation during the task.\n4.2.3 Performance Measures. To understand the quality of the in-\nteraction, we measured the number of failures that occurred during\nthe interaction. We considered two categories of failures: (1) tech-\nnical errors , such as interruptions by the agent, and inaccurate\ntranscriptions from Automatic Speech Recognition (ASR); and (2)\nhallucinations, where the response from the LLM is nonsensical or\nunfaithful to the provided source input [27].\n1The supplementary materials can be found at https://osf.io/exjrd/?view_only=\n88c0b1ff4b2b4f969928a614c9fa8fff\n4.3 Participants\nWe recruited 32 participants (10 male, 20 female, 1 gender-queer,\n1 non-binary) through a university mailing list between the ages\nof 18 and 59 ( ùëÄ = 27.47,ùëÜùëáùê∑ = 10.30) where 69% were White,\n28% were Asian, and 3% preferred not to answer. Participants were\nrequired to be in the United States, fluent in English, and at least\n18 years old. All participants agreed to participate in our study via\nour institution‚Äôs IRB-approved consent form. The study lasted for\napproximately 60 minutes and participants received $15 per hour\nfor compensation upon study completion.\n4.4 Analysis\nFactorial repeated-measures analysis of variance (ANOVA) was\nused to determine whether the task and agent embodiment had a\nsignificant effect on all measures. If the ANOVA test showed sig-\nnificant effects, we tested our data for pairwise differences using\nTukey honest significance test (HSD), which controls for Type I\nerror considering all possible comparisons. The qualitative data was\nanalyzed using Thematic Analysis (TA), following the guidelines\ndeveloped by Clarke and Braun [13] and McDonald et al. [43]. The\nfirst authors became acquainted with the data by conducting the\nstudies and initially creating a codebook [ 15]. Through ongoing\nteam discussions, codes were grouped into categories and refined\nuntil a consensus was reached. These categories were then fur-\nther organized and reiterated to extract themes that emerged from\nour study data. Once all potential themes were reviewed, the final\nthemes are presented as our findings.\n5 RESULTS\nWe present the findings derived from our quantitative and qual-\nitative data analysis. In section 5.1, we show the results of our\nquantitative data analysis highlighting the overall patterns from\nthe interactions between the LLM-powered agents and participants.\nAs the quantitative data showed high variance, we present the find-\nings of our qualitative analysis in Section 5.2 to Section 5.2.4, to\ngain further insights into the detailed factors that affected user\npreference and perceptions towards LLM-powered robots.\n5.1 Data from Quantitative Measures\nWe examined the influence of embodiment on interactions with\nLLM-powered agents through an analysis of data from our quan-\ntitative measures. Figure 3 summarizes significant findings. Over-\nall, embodiment had a significant effect on input prompt length,\nùêπ(2,56)= 14.30,ùëù < .001. When comparing the input length across\nembodiment conditions, participants provided significantly longer\ninputs to the text agent than the voice agent or the robot. Embod-\niment also had a significant effect on input length within tasks,\nùêπ(6,56)= 4.25,ùëù = .001. The generation task, in particular, had a\nsignificantly longer length of input in the text condition than other\nembodiment conditions. Finally, embodiment had a significant ef-\nfect on failures, ùêπ(2,56)= 55.16,ùëù < .001. In comparing failures\nacross embodiment conditions, participants encountered the most\nfailures with the voice agent, followed by the robot and text agents.\nWe observed a higher occurrence of failures in the generation task,\nunderscoring the difficulties faced by agents that used voice-based\ninput when confronted with extended input, especially within this\nUnderstanding Large-Language Model (LLM)-powered Human-Robot Interaction HRI ‚Äô24, March 11‚Äì14, 2024, Boulder, CO, USA\nFigure 3: Boxplots with data points overlaid on user satisfaction, length of input prompts, and interaction failures. Embodiment:\n(T)ext, (V)oice, (R)obot. Tasks: (N)egotiate, (G)enerate, (C)hoose, (E)xecute. Horizontal lines indicate significant pairwise\ncomparisons with Tukey HSD ( ùëù < .05‚àó, ùëù < .01‚àó‚àó, ùëù < .001‚àó‚àó‚àó).\nspecific task context, ùêπ(6,56)= 5.94,ùëù < .001. The diverse range of\nuser experiences related to the quality of interactions led to a sig-\nnificant variance in participants‚Äô satisfaction when interacting with\ndifferent agents, ùêπ(2,56)= 3.81,ùëù = .028. Participants rated their\nlevel of satisfaction with the text agent to be higher than the voice\nagent and marginally higher than the robotic agent. We attribute\nthese differences in the satisfaction scores to the results of the fail-\nures participants experienced with voice-based interaction with\nthe voice and robotic agents. There were no statistically significant\ndifferences across embodiments or tasks in other subjective metrics,\nincluding anthropomorphism, animacy, likeability, and perceived\nintelligence, which can be found in the supplementary materials.\n5.2 Data from the Qualitative Measures\nIn this section, we present the findings of our qualitative analysis\nin the order of tasks in which LLM-powered robots were more\npreferred by participants, namely: (1) execute; (2) negotiate; (3)\nchoose; and (4) generate. In each task category, we present design\nthemes explaining the positive and negative effects of LLM-powered\nrobot agents, supported by quantitative results.\n5.2.1 Execute. Below are themes that emerged in the Execute task.\nConversational Interactions for Effective Learning. Across all the\nagents, the LLM‚Äôs capability to facilitate natural conversations while\ndelivering instructions and responses with contextual understand-\ning significantly benefited the participants‚Äô engagement in the inter-\naction. For the execution task, the agent received task instructions\nbefore engaging with the participant and then responded freely\nto the participant‚Äôs requests. All participants frequently sought\nguidance on how to proceed in the task, thereby leading to concise\nand clear prompts that were easy for the agent to comprehend and\nrespond to. As shown in Figure 3, the input length of the prompts\ntended to be shorter in the execution task. Moreover, seven partici-\npants expressed satisfaction with the agent‚Äôs response, the LLM‚Äôs\ncontextual understanding ability enabled the agent to provide suffi-\ncient responses to follow-up questions. P26: ‚ÄúThe robot was able to\nanswer all the spontaneous questions that I had for it, which really\nsurprised me and we were able to have an actual conversation. He‚Äôs\nsmart enough to teach me!‚Äù Given the seamless communication,\nthere were minimal instances of agents failing to understand and\nrespond logically to requests, as shown in Figure 3.\nRobot‚Äôs Social Aspects Enhancing User Engagement. Six partici-\npants expressed a preference for the robot agent over the voice and\ntext agents due to its efficiency in interacting and enriching engage-\nment with the social aspects of the robot. As participants physically\nprepared drinks while simultaneously seeking instructions from\nthe agent, they expressed that interacting with the robot or voice\nagent through spoken communication was easier and facilitated\nmultitasking, unlike the text agent, which required them to pause\ntheir actions and type queries.P25: ‚ÄúI could start asking the follow-up\nquestion as I was doing a task versus text, I had to finish the whole\ntask and then type the question. I thought it went by a little smoother. ‚Äù\nFive participants encountered additional difficulties with the voice\nagent, struggling to time their prompts with the voice agent, leading\nto discomfort and reduced interest in engaging with the agent. P30:\n‚ÄúThat one [voice agent] for me feels the most choppy and disconnected,\nso it was hard for me to tell when I could ask something compared to\nthe others [agents]. ‚Äù\nMoreover, four participants noted that the robot‚Äôs social cues\nand physical presence enhanced their receptiveness to instructions\nand task engagement, as it resembled real-life communication. P26:\n‚ÄùWhen you‚Äôre able to see Pepper, you can kind of look at the tilted head\nto understand whether it‚Äôs like thinking or not. But when you can‚Äôt\nsee Pepper, it‚Äôs like, what‚Äôs going on? Those little things help us com-\nmunicate. ‚ÄùFour participants also expressed appreciation towards\nthe robot‚Äôs social cues, such as maintaining eye contact, waiting\nfor task completion, and offering encouragement, as these inter-\nactions made participants feel a genuine sense of companionship\nand support. P27: ‚ÄúSo especially when you‚Äôre learning, part of the\nHRI ‚Äô24, March 11‚Äì14, 2024, Boulder, CO, USA Callie Y. Kim, Christine P. Lee, and Bilge Mutlu\nFigure 4: Summary of Qualitative Findings ‚Äî Our findings indicate user preference for LLM-powered robots in the execution and\nnegotiation tasks. These tasks necessitated the establishment of social relationships and rapport, and the robot‚Äôs social aspects\nbenefited from effective synergy with LLM capabilities. LLM-powered robots were less favored in the choice and generation\ntasks. In these cases, the robot‚Äôs interaction medium and its social presence hindered optimal user performance. Additionally, a\nhigher occurrence of technical communication errors contributed to participants‚Äô lower preference for robot agents.\nlearning is from interacting. And that relates to the emotional con-\nnections and things that are underneath. So you need the actual robot\nto do it together, physically engaging. ‚Äù The social cues presented\nwith the robot‚Äôs social presence increased the participants‚Äô focus\nand immersion in the task, driven by a desire to P25: ‚Äúimpress the\nrobot, because he is watching me. ‚Äù Finally for future interactions,\nfive participants envisioned the robot utilizing its arms and body\nparts for instruction demonstration. Participants explained that\nthey expected the robot to have sophisticated non-verbal cues to\nmatch the advanced capabilities of its conversational skills. P26:\n‚ÄúThe robot‚Äôs movements reminded me it was still in development. They\nwere random and didn‚Äôt have any relation to what it was saying,\nwhen the way it talked was such high quality. Made it kind of creepy. ‚Äù\n5.2.2 Negotiate. We found the themes below in the Negotiate task.\nInformation Exchange with Contextual Understanding. During\nthe negotiation task, participants engaged with the agent to reach\na mutual agreement on the price of an item. This negotiation pro-\ncess involved participants posing questions about item specifics,\nusage history, potential bundle deals, and more. The LLM‚Äôs ability\nto understand the context within a conversation, considering the\ndialogue history to generate coherent responses, was effective in\nmaintaining a seamless, life-like conversation. P22: ‚ÄúOkay, hold on\n[robot], first let‚Äôs sit down and talk about this more. Tell me a little bit\nmore about this bike. Once all my questions are answered sufficiently,\nthen we can start to negotiate. ‚Äù\nParticipants‚Äô queries for negotiation were generally longer when\ninteracting with the text agent compared to the voice and robot\nagent, as shown in Figure 3. Five participants explained that it was\nmore convenient to input their questions via text to the agent as\nopposed to verbally articulating their inquiries. The primary ques-\ntions posed across the agents were largely similar, with participants\nposing additional queries based on the agent‚Äôs response. The text\nand robot agents had clear distinction when their response was\nfinished, as the text agent displayed a complete response on the\nscreen, while the robot agent indicated completion through non-\nverbal cues such as putting down its arms, tilting its head, and\nadjusting its gaze. As illustrated in Figure 3, participants encoun-\ntered difficulties less frequently when engaging with these agents,\nwhereas the voice agent encountered a higher incidence of failures,\nas participants had challenges in determining when to speak and\nwhether the agent accurately understood their prompts.\nRobot Establishing Rapport for Negotiation. Although the robot\nagent was not the most convenient to use, four participants ex-\npressed that the robot was most effective in establishing the con-\nnection and rapport that was required for successful negotiations.\nTwo participants described that building rapport and personal con-\nnections with the agent was crucial to resolving the conflicts for\nnegotiation. P22: ‚ÄúNegotiation starts with building trust, obviously. ‚Äù\nThe social characteristics, such as the natural language produced\nby the LLM and the behavioral aspects of gaze, facial expressions,\nand body movements, contributed to a sense of engaging in a gen-\nuine conversation with a social entity. Five participants described\nthat the ability to see and physically interact with the robot agent\ncreated a personal interaction atmosphere, enhancing the agent‚Äôs\nreliability compared to the voice or text agent that lacked physical\nsocial cues. P17: ‚ÄúI do feel like for me that‚Äôs important. Just to be able\nto engage with some visual cues, eyes, you know, a face, that seems\nto be more appealing, inviting a further engagement. Making a real\nconversation. ‚ÄùAlthough the text agent was efficient in providing\nimmediate and informative responses, it was perceived more as\na search engine and less as an agent genuinely interested in me-\ndiating deals to participants‚Äô preferences. This perception made\nthree participants less inclined to negotiate for more expensive\nitems with the text agent. P5: ‚ÄúYeah coffee machine no big deal, but\nthe car I wouldn‚Äôt just negotiate that with the text [agent]. It seems\ntoo machinery and sketchy. ‚Äù Similar to other tasks, the voice agent\nwas least positively perceived among agents for negotiations, as\nit was challenging to discern intentions and gauge conversational\nprogress during the negotiation.\n5.2.3 Choose. We identified the themes below in the Choose task.\nRecurrence of Errors in Communication and Logic. During the\nchoosing task, participants chose a final set of items from a list\nof items based on practicality and preference through discussions\nwith the agents. The discussions required led participants to ar-\nticulate the reasons for and validate their selections to the agent.\nParticipants also disagreed with the agent‚Äôs suggestions, prompting\nthem to elaborate on their rationale. Similar to the negotiation task,\nfive participants described that the text agent was the most effec-\ntive at facilitating accurate and expeditious information exchange\nUnderstanding Large-Language Model (LLM)-powered Human-Robot Interaction HRI ‚Äô24, March 11‚Äì14, 2024, Boulder, CO, USA\ncompared to the voice and robot agents. This efficiency resulted\nin longer prompts in the text agent, as illustrated in Figure 3. Ad-\nditionally, as participants iterated the item selection criteria and\nrequested validation on the final item selections, the LLM occasion-\nally exhibited errors in logic or inconsistencies with the dialogue\nhistory. For instance, one participant described P24: ‚ÄúHe [robot] also\nhad some unusual answers. I asked what I should bring on my ski\ntrip and Pepper said that I could bring a sand baking tray and ski on\nthe tray. I was like, not sure how that would work out. So I felt less\nengaged in obviously, I felt less of a connection with Pepper in this in-\nstance. ‚ÄùIn another example, the agent altered its recommendations\nmultiple times or presented conflicting arguments for a single item.\nUnforeseen failures, such as misinterpreting participants‚Äô requests,\nprematurely responding before participants, and introducing flawed\nlogic in the agent‚Äôs responses, led to increased failures of the voice\nand robot agent as illustrated in Figure 3. P10: ‚ÄúThere‚Äôs often a dis-\nconnect between, it [agent] knowing the facts but not knowing that it\ndoesn‚Äôt make sense. ‚Äù Such failures were further described by four\nparticipants to decrease the satisfaction and motivation to engage\nwith the voice or robot agent.\nInefficient and Time-consuming Interaction with Robot. During\nthe task, participants described that they initially held a general\nidea of the items they intended to select, and they intended to have\nspecific discussions with the agents to efficiently narrow down\ntheir choices. Participants sought details regarding the advantages\nand disadvantages of these items and validation to determine their\ninclusion. During this engagement, four participants noted that\nthe conversational interactions necessitated repetitive and overly\nverbose exchanges with the agent to acquire information equiv-\nalent to a ‚Äúquick search, ‚Äù resulting in an undue amount of time.\nIn contrast, the text-based agent promptly provided the requested\ninformation and additionally preserved logs for users to reference\nwhen finalizing their decision. P8: ‚ÄúI appreciated Pepper being all\nnice, but sometimes it wasn‚Äôt the exact information I was looking for\nand there was a lot of fluff. And then I would have to wait for him to\nfinish to ask again. And in the end, I don‚Äôt even remember what he\nsaid! So in those terms, the text was much more efficient. ‚Äù\n5.2.4 Generate. The Generate task included the themes below.\nCommunication Barriers for Creative Collaboration. The creative\nnature of the generation task guided participants to devise prompts\nthat were more personal and situation-specific. These prompts in-\ncluded the introduction of character names and attributes, intricate\nplot developments, and distinctive settings. As a result, participants‚Äô\nprompts tended to be more extensive during the generation task, as\nshown in Figure 3. A difference in input length appeared between\nthe text agent versus the voice and robot agents, as participants\nexpressed difficulties in verbally expressing their creative thoughts.\nThese communication difficulties were due to the timely manner\nof the task. Participants frequently encountered situations where\nthey had a lot to convey but struggled to do so spontaneously in\nreal-time, without excessive pauses or verbosity in response to the\nrobot. Moreover, the collaborative nature of this creative process\nmeant that participants needed additional time to carefully consider\nhow they wanted to incorporate the agent‚Äôs ideas into their story\nand shape the subsequent storyline. As a result, seven participants\nfound verbal communication to be less preferable and more chal-\nlenging compared to using text inputs, where text inputs allowed\nthem to express their ideas in a more organized manner. The com-\nmunication difficulties led to malfunctions as the agents frequently\nmisinterpreted the user‚Äôs prompts, overlooked important elements\nof the participants‚Äô instructions, or interrupted the participants. P7:\n‚ÄúBut for both cases, the voice and the robot agent. There are a couple\nof times that they just ignore, or interrupt what you say that makes\nyou more frustrated. It would just start rattling off when I wasn‚Äôt\ndone talking. ‚Äù Among all the tasks, the generation task showed the\nhighest number of communication failures, as shown in Figure 3.\nThus, six participants perceived the text agent to be more practical\nin tracking the storyline and avoiding communication errors.\nDiscomfort with Robot in Content Creation. During the task, four\nparticipants expressed discomfort with the robot‚Äôs social presence\nwhen trying to contemplate creative ideas. This discomfort was due\nto the participants‚Äô expectations of the agent being ‚Äúsmart, ‚Äù due to\nits sophisticated verbal capabilities from the LLM. P19: ‚ÄúIt seems\nlike I am talking to a person, because it [robot] is so smart, and I‚Äôm\nlike, oh, they might remember this. ‚Äù These expectations made the\nrobot‚Äôs social presence cause pressure on the participants, and even\nanxiety when they were trying to come up with the next storyline.\nAs the robot would continue to gaze at the user or make subtle\nmovements and facial expressions as it awaited the next prompt, the\nparticipants described that this action created a sense of urgency,\ncompelling them to generate their ideas quickly without allowing\nfor thorough reflection. P6: ‚ÄúI thought more when I was using the\ntext, because it wasn‚Äôt just off the top of my head. But with the robot,\nI did just kind of say more random stuff because I felt like I needed\nto respond right away. ‚Äù Another participant supported this finding\nby describing P19: ‚ÄúI felt the most comfortable with the text event in\ngenerating ideas because I had no accountability. Or the robot looking\nat me in the face. I feel a little bit more embarrassed. ‚Äù\n6 DISCUSSION\nIn this work, we explored the distinctive design requirements for\nintegrating LLMs with robots. To understand how LLMs should\nbe tailored for robot applications, we conducted a user study in-\nvolving 32 participants that compared a text, voice, and robot agent\nacross four tasks: execute, negotiate, choose, and generate. Our\nfindings show that the LLM-powered robot elicited user expecta-\ntions for sophisticated non-verbal cues and was more favored in the\nExecute and Negotiate tasks, where building connections and en-\ngaging in social discussions were crucial. However, LLM-powered\nrobots were less preferred in the Choose and Generate tasks, due\nto communication difficulties and the potential anxiety during col-\nlaboration. Below, we present design implications that address the\ndistinctive design needs for robots utilizing LLMs, as well as the\nunique design requirements for LLMs intended for use with robots.\n6.1 Combining LLM-powered Robots with\nNon-verbal Interaction Cues\nOur findings reveal that interactions with LLM-powered robots\nestablished unique expectations for users regarding non-verbal\ncues. In contrast, users interacting with text and voice-based agents\ndid not actively seek non-verbal cues. These expectations were\nHRI ‚Äô24, March 11‚Äì14, 2024, Boulder, CO, USA Callie Y. Kim, Christine P. Lee, and Bilge Mutlu\nnot solely shaped by the robot‚Äôs physical form but were rather a\nresult of the robot‚Äôs advanced language capabilities powered by the\nLLM. This sophistication in language abilities led users to anticipate\nequally sophisticated non-verbal cues from the robot. Therefore, it\nis recommended that LLM-powered robots explore and incorporate\na diverse range of rich non-verbal cues, such as gaze [25], gestures\n[33, 64], behaviors, and facial expressions [11], during interactions\nwith users. These non-verbal elements should be tailored to match\nthe heightened expectations set by the robot‚Äôs advanced spoken\nlanguage capabilities. For instance, combinations of non-verbal\ncues can be developed to demonstrate appropriate behaviors and\nexplanations in various contexts, such as reactions to different user\ninputs, respecting user boundaries, and failures in responding to\nuser requests. The alignment of verbal and non-verbal cues can\nenhance the experience between the user and LLM-powered robots,\nmaking the engagement more sophisticated and natural for users.\n6.2 Considering Task Characteristics when\nUtilizing LLMs with Robots\nOur study findings indicate that LLM-powered robots exhibited a\npreference for certain tasks over others due to the unique character-\nistics of each task. Therefore to effectively utilize LLMs for robots,\ncustomization [50, 74] and fine-tuning [37] are crucial for different\ntasks. While existing state-of-the-art LLMs can be suitable for tasks\nsimilar to Execute and Negotiate, for tasks resembling Choose and\nGenerate, LLM adaptation will be required. One method can be\nfine-tuning LLMs to fit the goal and context of the task, such as\nsimplifying rich social descriptions to enhance efficiency, intuitive-\nness, and directness. The fine-tuning process can include selecting\na pre-trained model, defining task objectives [18], preparing task-\nspecific data [29], configuring fine-tuning parameters [22, 23, 36],\ntraining the model, validating and evaluating performance [ 52],\nand deploying the fine-tuned model onto the robot.\n6.3 Utilizing LLMs for Robot Design\nDuring our study, we observed several design opportunities to\nleverage LLMs with robots. During the interactions with users,\nLLMs demonstrated the potential to empower robots to adapt to a\nbroader array of user requests and effectively capture user needs\nand preferences. These instances emphasize the capacity of LLMs\nto either substitute or complement traditionally challenging tasks\nin the realm of robot design and implementation. For instance, dur-\ning robot application development, significant time is often spent\nimplementing the dialogue system, defining the robot‚Äôs intent and\nentity, and training it to handle user requests and communication\nvariability effectively. LLMs can address these challenges by flexibly\naccommodating task models variations and processing a wide range\nof inputs. This adaptability can guide robots to offer personalized\nuser experiences through iterative and engaging interactions.\nHowever, it is crucial to acknowledge that integrating LLMs\nmay also introduce risks and errors, such as causing robots to\ndeviate from context or produce hallucination errors. LLM-powered\nrobots in real-world settings may display unexpected behaviors or\nmake statements inconsistent with their intended character, leading\nto a mismatch between the situational context and the robot‚Äôs\nintended personality. Furthermore, as illustrated by instances in\nour study, LLMs may introduce hallucination errors, leading the\nrobot to provide information that is inaccurate or nonsensical. As\na result, LLMs on robots must be viewed as both a feature and\na potential challenge, requiring the establishment of appropriate\nboundaries regarding what LLMs can and cannot achieve. Technical\nmethods such as curated datasets for pre-training [69, 73], program\nverification [14, 65], human-in-the-loop review [51], fine-tuning,\nand other measures can be used for LLM action boundaries.\n6.4 Limitations and Future Work\nOur study has several limitations. First, we chose to compare LLM-\npowered robots to two other forms in which people interact with\nLLMs: text agents and voice agents. While this comparison was\ninformative on how people‚Äôs perceptions of LLM-powered robots\ndiffered from other LLM-powered agents, we would have ideally\ncompared the LLM-powered robot to a non-LLM-powered robot.\nHowever, it was difficult to specify what a ‚Äúnon-LLM‚Äù condition\nwould look like and how such a condition would be implemented.\nNonetheless, the lack of comparison against a non-LLM-powered\nrobot limits our ability to study the unique effects of the integration\nof LLMs in robots. We plan to explore this question in our future\nwork, for example, using a Wizard-of-Oz approach with human op-\nerators generating responses or a rule-based approach with scripted\ndialogues to achieve sophisticated but fixed conversational capa-\nbilities for the robot. Second, the quantitative data from subjective\nmeasures exhibited high variance, leading to non-significant results\nin multiple items. This high variability can be attributed to our sam-\nple size, which limits the generalizability of our findings. Future\nwork may include larger-scale studies. Third, our minimalist ro-\nbot design lacked diverse non-verbal behavior, potentially causing\nusers to perceive the three agents as more similar than in real-world\nscenarios. Future research can explore how LLM-powered robots\nmight use the full range of their embodied capabilities, which could\nalso improve their communication performance with users.\n7 CONCLUSION\nThis research investigates the design requirements for robots con-\nnected to LLMs and in tasks where they excel. We compare three\nLLM-powered agents‚Äîtext, voice, and robot‚Äîacross four tasks‚Äî\ngenerate, negotiate, choose, and execute. Findings reveal that LLM-\nequipped robots enhance user expectations for non-verbal cues,\nexcel in connection building and deliberation, but face challenges in\ncommunication difficulties and creating social pressure. We provide\ndesign insights for robots adopting LLMs and LLMs used for robots.\nACKNOWLEDGMENTS\nThis work was supported by the Sheldon B. and Marianne S. Lubar\nProfessorship, an H.I. Romnes Faculty Fellowship, and the National\nScience Foundation award (#1925043).\nREFERENCES\n[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes,\nByron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Haus-\nman, et al . 2022. Do as i can, not as i say: Grounding language in robotic\naffordances. arXiv preprint arXiv:2204.01691 (2022).\n[2] Alexander Mois Aroyo, Francesco Rea, and Alessandra Sciutti. 2017. Will You\nRely on a Robot to Find a Treasure?. In Proceedings of the Companion of the 2017\nACM/IEEE International Conference on Human-Robot Interaction (Vienna, Austria)\nUnderstanding Large-Language Model (LLM)-powered Human-Robot Interaction HRI ‚Äô24, March 11‚Äì14, 2024, Boulder, CO, USA\n(HRI ‚Äô17) . Association for Computing Machinery, New York, NY, USA, 71‚Äì72.\nhttps://doi.org/10.1145/3029798.3038394\n[3] Wilma A Bainbridge, Justin Hart, Elizabeth S Kim, and Brian Scassellati. 2008.\nThe effect of presence on human-robot interaction. In RO-MAN 2008-The 17th\nIEEE International Symposium on Robot and Human Interactive Communication .\nIEEE, 701‚Äì706.\n[4] Wilma A Bainbridge, Justin W Hart, Elizabeth S Kim, and Brian Scassellati. 2011.\nThe benefits of interactions with physically present robots over video-displayed\nagents. International Journal of Social Robotics 3 (2011), 41‚Äì52.\n[5] Christoph Bartneck, Dana Kuliƒá, Elizabeth Croft, and Susana Zoghbi. [n. d.].\nMeasurement Instruments for the Anthropomorphism, Animacy, Likeability,\nPerceived Intelligence, and Perceived Safety of Robots. 1, 1 ([n. d.]), 71‚Äì81.\nhttps://doi.org/10.1007/s12369-008-0001-3\n[6] Timothy W Bickmore and Rosalind W Picard. 2005. Establishing and maintain-\ning long-term human-computer relationships. ACM Transactions on Computer-\nHuman Interaction (TOCHI) 12, 2 (2005), 293‚Äì327.\n[7] Erik Billing, Julia Ros√©n, and Maurice Lamb. 2023. Language models for human-\nrobot interaction. In ACM/IEEE International Conference on Human-Robot Interac-\ntion, March 13‚Äì16, 2023, Stockholm, Sweden . ACM Digital Library, 905‚Äì906.\n[8] Sa≈°a Bodiro≈æa. 2017. Gestures in human-robot interaction . Ph. D. Dissertation.\nHumboldt-Universit√§t zu Berlin, Mathematisch-Naturwissenschaftliche Fakult√§t.\nhttps://doi.org/10.18452/17705\n[9] Cynthia Breazeal, Kerstin Dautenhahn, and Takayuki Kanda. 2016. Social robotics.\nSpringer handbook of robotics (2016), 1935‚Äì1972.\n[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877‚Äì1901.\n[11] Neeraj Cherakara, Finny Varghese, Sheena Shabana, Nivan Nelson, Abhiram\nKarukayil, Rohith Kulothungan, Mohammed Afil Farhan, Birthe Nesset, Meriam\nMoujahid, Tanvi Dinkar, et al . 2023. FurChat: An Embodied Conversational\nAgent using LLMs, Combining Open and Closed-Domain Dialogue with Facial\nExpressions. arXiv preprint arXiv:2308.15214 (2023).\n[12] Vijay Chidambaram, Yueh-Hsuan Chiang, and Bilge Mutlu. 2012. Designing\npersuasive robots: how robots might persuade people using vocal and nonverbal\ncues. In Proceedings of the seventh annual ACM/IEEE international conference on\nHuman-Robot Interaction . 293‚Äì300.\n[13] Victoria Clarke and Virginia Braun. 2014. Thematic analysis. In Encyclopedia of\ncritical psychology . Springer, 1947‚Äì1952.\n[14] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,\nLukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,\nChristopher Hesse, and John Schulman. 2021. Training Verifiers to Solve Math\nWord Problems. arXiv:2110.14168 [cs.LG]\n[15] Jessica T DeCuir-Gunby, Patricia L Marshall, and Allison W McCulloch. 2011.\nDeveloping and using a codebook for the analysis of interview data: An example\nfrom a professional development research project. Field methods 23, 2 (2011),\n136‚Äì155.\n[16] Eric Deng, Bilge Mutlu, Maja J Mataric, et al . 2019. Embodiment in socially\ninteractive robots. Foundations and Trends ¬Æ in Robotics 7, 4 (2019), 251‚Äì356.\n[17] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdh-\nery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu,\nWenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey\nLevine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy\nZeng, Igor Mordatch, and Pete Florence. 2023. PaLM-E: An Embodied Multimodal\nLanguage Model. arXiv:2303.03378 [cs.LG]\n[18] Yingqiang Ge, Wenyue Hua, Kai Mei, Jianchao Ji, Juntao Tan, Shuyuan Xu, Zelong\nLi, and Yongfeng Zhang. 2023. OpenAGI: When LLM Meets Domain Experts.\narXiv:2304.04370 [cs.AI]\n[19] Google. 2023. Google Cloud Services‚ÄîSpeech to text. \"Accessed = 09-29-2023\".\n[20] Guy Hoffman, Oren Zuckerman, Gilad Hirschberger, Michal Luria, and Tal\nShani Sherman. 2015. Design and evaluation of a peripheral robotic conver-\nsation companion. In Proceedings of the tenth annual ACM/IEEE international\nconference on human-robot interaction . 3‚Äì10.\n[21] Laura Hoffmann and Nicole C. Kr√§mer. 2013. Investigating the effects of phys-\nical and virtual embodiment in task-oriented and conversational contexts. In-\nternational Journal of Human-Computer Studies 71, 7 (2013), 763‚Äì774. https:\n//doi.org/10.1016/j.ijhcs.2013.04.007\n[22] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin\nDe Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-Efficient Transfer Learning for NLP. In Proceedings of the 36th Interna-\ntional Conference on Machine Learning (Proceedings of Machine Learning Research,\nVol. 97), Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, 2790‚Äì2799.\nhttps://proceedings.mlr.press/v97/houlsby19a.html\n[23] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language\nModels. CoRR abs/2106.09685 (2021). arXiv:2106.09685 https://arxiv.org/abs/\n2106.09685\n[24] Chien-Ming Huang and Bilge Mutlu. 2013. Modeling and Evaluating Narrative\nGestures for Humanlike Robots.. InRobotics: Science and Systems , Vol. 2. Citeseer.\n[25] Bahar Irfan, Sanna-Mari Kuoppam√§ki, and Gabriel Skantze. 2023. Between Reality\nand Delusion: Challenges of Applying Large Language Models to Companion\nRobots for Open-Domain Dialogues with Older Adults. https://doi.org/10.21203/\nrs.3.rs-2884789/v1\n[26] Jesin James, Catherine Inez Watson, and Bruce MacDonald. 2018. Artificial\nempathy in social robots: An analysis of emotions in speech. In 2018 27th IEEE\nInternational symposium on robot and human interactive communication (RO-\nMAN). IEEE, 632‚Äì637.\n[27] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,\nYe Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of Hallucination in\nNatural Language Generation. ACM Comput. Surv. 55, 12, Article 248 (mar 2023),\n38 pages. https://doi.org/10.1145/3571730\n[28] Younbo Jung and Kwan Min Lee. 2004. Effects of physical embodiment on social\npresence of social robots. Proceedings of PRESENCE 2004 (2004), 80‚Äì87.\n[29] Weslie Khoo, Long-Jing Hsu, Kyrie Jig Amon, Pranav Vijay Chakilam, Wei-Chu\nChen, Zachary Kaufman, Agness Lungu, Hiroki Sato, Erin Seliger, Manasi Swami-\nnathan, Katherine M. Tsui, David J. Crandall, and Selma Sabanoviƒá. 2023. Spill\nthe Tea: When Robot Conversation Agents Support Well-Being for Older Adults.\nIn Companion of the 2023 ACM/IEEE International Conference on Human-Robot\nInteraction (Stockholm, Sweden) (HRI ‚Äô23). Association for Computing Machinery,\nNew York, NY, USA, 178‚Äì182. https://doi.org/10.1145/3568294.3580067\n[30] Krishna Kodur, Manizheh Zand, Matthew Tognotti, Cinthya Jauregui, and Maria\nKyrarini. 2023. Structured and Unstructured Speech2Action Frameworks for\nHuman-Robot Collaboration: A User Study. (2023).\n[31] Guy Laban, Jean-No√´l George, Val Morrison, and Emily S Cross. 2020. Tell me\nmore! Assessing interactions with social robots from speech. Paladyn, Journal of\nBehavioral Robotics 12, 1 (2020), 136‚Äì159.\n[32] Christine P Lee, Bengisu Cagiltay, and Bilge Mutlu. 2022. The unboxing experi-\nence: Exploration and design of initial interactions between children and social\nrobots. In Proceedings of the 2022 CHI conference on human factors in computing\nsystems. 1‚Äì14.\n[33] Yoon Kyung Lee, Yoonwon Jung, Gyuyi Kang, and Sowon Hahn. 2023. Developing\nSocial Robots with Empathetic Non-Verbal Cues Using Large Language Models.\narXiv:2308.16529 [cs.RO]\n[34] Iolanda Leite, Carlos Martinho, and Ana Paiva. 2013. Social robots for long-term\ninteraction: a survey. International Journal of Social Robotics 5 (2013), 291‚Äì308.\n[35] Jamy Li. 2015. The benefit of being physically present: A survey of experimen-\ntal works comparing copresent robots, telepresent robots and virtual agents.\nInternational Journal of Human-Computer Studies 77 (2015), 23‚Äì37.\n[36] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang,\nMohit Bansal, and Colin A Raffel. 2022. Few-shot parameter-efficient fine-tuning\nis better and cheaper than in-context learning. Advances in Neural Information\nProcessing Systems 35 (2022), 1950‚Äì1965.\n[37] Zhengliang Liu, Aoxiao Zhong, Yiwei Li, Longtao Yang, Chao Ju, Zihao Wu,\nChong Ma, Peng Shu, Cheng Chen, Sekeun Kim, Haixing Dai, Lin Zhao, Dajiang\nZhu, Jun Liu, Wei Liu, Dinggang Shen, Quanzheng Li, Tianming Liu, and Xiang\nLi. 2024. Tailoring Large Language Models to Radiology: A Preliminary Approach\nto LLM Adaptation for a Highly Specialized Domain. In Machine Learning in\nMedical Imaging , Xiaohuan Cao, Xuanang Xu, Islem Rekik, Zhiming Cui, and\nXi Ouyang (Eds.). Springer Nature Switzerland, Cham, 464‚Äì473.\n[38] Arnold M Lund. 2001. Measuring usability with the use questionnaire12.Usability\ninterface 8, 2 (2001), 3‚Äì6.\n[39] Michal Luria, Jodi Forlizzi, and Jessica Hodgins. 2018. The effects of eye design\non the perception of social robots. In 2018 27th IEEE International Symposium on\nRobot and Human Interactive Communication (RO-MAN) . IEEE, 1032‚Äì1037.\n[40] Michal Luria, Guy Hoffman, Benny Megidish, Oren Zuckerman, and Sung Park.\n2016. Designing Vyo, a robotic Smart Home assistant: Bridging the gap between\ndevice and social agent. In 2016 25th IEEE International Symposium on Robot and\nHuman Interactive Communication (RO-MAN) . IEEE, 1019‚Äì1025.\n[41] Michal Luria, Guy Hoffman, and Oren Zuckerman. 2017. Comparing social robot,\nscreen and voice interfaces for smart-home control. In Proceedings of the 2017\nCHI conference on human factors in computing systems . 580‚Äì628.\n[42] Michal Luria, Samantha Reig, Xiang Zhi Tan, Aaron Steinfeld, Jodi Forlizzi, and\nJohn Zimmerman. 2019. Re-Embodiment and Co-Embodiment: Exploration of\nsocial presence for robots and conversational agents. In Proceedings of the 2019\non Designing Interactive Systems Conference . 633‚Äì644.\n[43] Nora McDonald, Sarita Schoenebeck, and Andrea Forte. 2019. Reliability and\nInter-rater Reliability in Qualitative Research: Norms and Guidelines for CSCW\nand HCI Practice. Proceedings of the ACM on Human-Computer Interaction 3 (11\n2019), 1‚Äì23. https://doi.org/10.1145/3359174\n[44] Joseph Edward McGrath. 1984. Groups: Interaction and performance . Vol. 14.\nPrentice-Hall Englewood Cliffs, NJ.\n[45] Jonathan Mumm and Bilge Mutlu. 2011. Human-Robot Proxemics: Physical and\nPsychological Distancing in Human-Robot Interaction. In Proceedings of the 6th\nInternational Conference on Human-Robot Interaction (Lausanne, Switzerland)\n(HRI ‚Äô11) . Association for Computing Machinery, New York, NY, USA, 331‚Äì338.\nhttps://doi.org/10.1145/1957656.1957786\nHRI ‚Äô24, March 11‚Äì14, 2024, Boulder, CO, USA Callie Y. Kim, Christine P. Lee, and Bilge Mutlu\n[46] Bilge Mutlu. 2021. The virtual and the physical: two frames of mind. iScience 24,\n2 (2021), 101965. https://doi.org/10.1016/j.isci.2020.101965\n[47] Bilge Mutlu, Takayuki Kanda, Jodi Forlizzi, Jessica Hodgins, and Hiroshi Ishiguro.\n2012. Conversational gaze mechanisms for humanlike robots. ACM Transactions\non Interactive Intelligent Systems (TiiS) 1, 2 (2012), 1‚Äì33.\n[48] Bilge Mutlu, Steven Osman, Jodi Forlizzi, Jessica Hodgins, and Sara Kiesler. 2006.\nTask structure and user attributes as elements of human-robot interaction design.\nIn ROMAN 2006-The 15th IEEE International Symposium on Robot and Human\nInteractive Communication . IEEE, 74‚Äì79.\n[49] Bilge Mutlu, Toshiyuki Shiwa, Takayuki Kanda, Hiroshi Ishiguro, and Norihiro\nHagita. 2009. Footing in human-robot conversations: how robots might shape\nparticipant roles using gaze cues. InProceedings of the 4th ACM/IEEE international\nconference on Human robot interaction . 61‚Äì68.\n[50] Teresa Onorati, √Ålvaro Castro-Gonz√°lez, Javier Cruz del Valle, Paloma D√≠az, and\nJos√© Carlos Castillo. 2023. Creating Personalized Verbal Human-Robot Interac-\ntions Using LLM with the Robot Mini. In Proceedings of the 15th International\nConference on Ubiquitous Computing & Ambient Intelligence (UCAmI 2023) , Jos√©\nBravo and Gabriel Urz√°iz (Eds.). Springer Nature Switzerland, Cham, 148‚Äì159.\n[51] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schul-\nman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Pe-\nter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language\nmodels to follow instructions with human feedback. arXiv:2203.02155 [cs.CL]\n[52] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan\nHuang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check Your\nFacts and Try Again: Improving Large Language Models with External Knowledge\nand Automated Feedback. arXiv:2302.12813 [cs.CL]\n[53] Aaron Powers, Sara Kiesler, Susan Fussell, and Cristen Torrey. 2007. Compar-\ning a computer agent with a humanoid robot. In Proceedings of the ACM/IEEE\ninternational conference on Human-robot interaction . 145‚Äì152.\n[54] Samantha Reig, Elizabeth J Carter, Terrence Fong, Aaron Steinfeld, and Jodi\nForlizzi. 2022. Perceptions of explicitly vs. implicitly relayed commands between\na robot and smart speaker. In 2022 17th ACM/IEEE International Conference on\nHuman-Robot Interaction (HRI) . IEEE, 1012‚Äì1016.\n[55] Samantha Reig, Jodi Forlizzi, and Aaron Steinfeld. 2019. Leveraging robot em-\nbodiment to facilitate trust and smoothness. In 2019 14th ACM/IEEE International\nConference on Human-Robot Interaction (HRI) . IEEE, 742‚Äì744.\n[56] Aldebaran Robotics. 2023. Animated Speech. \"Accessed = 09-29-2023\".\n[57] Aldebaran Robotics. 2023. Audio Device API. \"Accessed = 09-29-2023\".\n[58] Soft Bank Robotics. 2023. Pepper Robot. \"Accessed = 09-29-2023\".\n[59] Eduardo Rodriguez-Lizundia, Samuel Marcos, Eduardo Zalama, Jaime G√≥mez-\nGarc√≠a-Bermejo, and Alfonso Gordaliza. 2015. A bellboy robot: Study of the\neffects of robot behaviour on user engagement and comfort.International Journal\nof Human-Computer Studies 82 (2015), 83‚Äì95. https://doi.org/10.1016/j.ijhcs.2015.\n06.001\n[60] Maha Salem, Stefan Kopp, Ipke Wachsmuth, Katharina Rohlfing, and Frank\nJoublin. 2012. Generation and evaluation of communicative robot gesture. Inter-\nnational Journal of Social Robotics 4 (2012), 201‚Äì217.\n[61] Guido Schillaci, Sa≈°a Bodiro≈æa, and Verena Vanessa Hafner. 2013. Evaluating\nthe effect of saliency detection and attention manipulation in human-robot\ninteraction. International Journal of Social Robotics 5 (2013), 139‚Äì152.\n[62] Elena M√°rquez Segura, Michael Kriegel, Ruth Aylett, Amol Deshmukh, and\nHenriette Cramer. 2012. How do you like me in this: User embodiment preferences\nfor companion agents. In Intelligent Virtual Agents: 12th International Conference,\nIVA 2012, Santa Cruz, CA, USA, September, 12-14, 2012. Proceedings 12 . Springer,\n112‚Äì125.\n[63] Stela H. Seo, Denise Geiskkovitch, Masayuki Nakane, Corey King, and James E.\nYoung. 2015. Poor Thing! Would You Feel Sorry for a Simulated Robot? A Com-\nparison of Empathy toward a Physical and a Simulated Robot. In Proceedings of\nthe Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction\n(Portland, Oregon, USA) (HRI ‚Äô15) . Association for Computing Machinery, New\nYork, NY, USA, 125‚Äì132. https://doi.org/10.1145/2696454.2696471\n[64] Gabriel J Serfaty, Virgil O Barnard, and Joseph P Salisbury. 2023. Generative\nFacial Expressions and Eye Gaze Behavior from Prompts for Multi-Human-\nRobot Interaction. In Adjunct Proceedings of the 36th Annual ACM Symposium on\nUser Interface Software and Technology (<conf-loc>, <city>San Francisco</city>,\n<state>CA</state>, <country>USA</country>, </conf-loc>) (UIST ‚Äô23 Adjunct) .\nAssociation for Computing Machinery, New York, NY, USA, Article 13, 3 pages.\nhttps://doi.org/10.1145/3586182.3616623\n[65] Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun\nLiu. 2021. Generate & Rank: A Multi-task Framework for Math Word Problems.\narXiv:2109.03034 [cs.CL]\n[66] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan\nTremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. 2023. Progprompt:\nGenerating situated robot task plans using large language models. In 2023 IEEE\nInternational Conference on Robotics and Automation (ICRA) . IEEE, 11523‚Äì11530.\n[67] Leila Takayama, Victoria Groom, and Clifford Nass. 2009. I‚Äôm Sorry, Dave: I‚Äôm\nAfraid i Won‚Äôt Do That: Social Aspects of Human-Agent Conflict. In Proceedings\nof the SIGCHI Conference on Human Factors in Computing Systems (Boston, MA,\nUSA) (CHI ‚Äô09) . Association for Computing Machinery, New York, NY, USA,\n2099‚Äì2108. https://doi.org/10.1145/1518701.1519021\n[68] Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021. Understanding\nthe capabilities, limitations, and societal impact of large language models. arXiv\npreprint arXiv:2102.02503 (2021).\n[69] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos\nGuestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Stanford alpaca: An\ninstruction-following llama model.\n[70] Sai Vemprala, Rogerio Bonatti, Arthur Bucker, and Ashish Kapoor. 2023. Chatgpt\nfor robotics: Design principles and model abilities. Microsoft Auton. Syst. Robot.\nRes 2 (2023), 20.\n[71] Joshua Wainer, David J Feil-Seifer, Dylan A Shell, and Maja J Mataric. 2006. The\nrole of physical embodiment in human-robot interaction. In ROMAN 2006-The\n15th IEEE International Symposium on Robot and Human Interactive Communica-\ntion. IEEE, 117‚Äì122.\n[72] Joshua Wainer, David J Feil-Seifer, Dylan A Shell, and Maja J Mataric. 2007.\nEmbodiment and human-robot interaction: A task-based perspective. In RO-\nMAN 2007-The 16th IEEE International Symposium on Robot and Human Interactive\nCommunication. IEEE, 872‚Äì877.\n[73] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel\nKhashabi, and Hannaneh Hajishirzi. 2023. Self-Instruct: Aligning Language\nModels with Self-Generated Instructions. arXiv:2212.10560 [cs.CL]\n[74] Takato Yamazaki, Katsumasa Yoshikawa, Toshiki Kawamoto, Tomoya Mizumoto,\nMasaya Ohagi, and Toshinori Sato. 2023. Building a hospitable and reliable dia-\nlogue system for android robots: a scenario-based approach with large language\nmodels. Advanced Robotics 37, 21 (2023), 1364‚Äì1381.\n[75] Yang Ye, Hengxu You, and Jing Du. 2023. Improved Trust in Human-Robot\nCollaboration With ChatGPT. IEEE Access 11 (2023), 55748‚Äì55754. https://doi.\norg/10.1109/ACCESS.2023.3282111\n[76] Tom Ziemke. 2013. What‚Äôs that thing called embodiment? In Proceedings of the\n25th Annual Cognitive Science Society . Psychology Press, 1305‚Äì1310.\n[77] Zoom. 2023. Video Conferencing Platform. \"Accessed = 09-29-2023\".",
  "topic": "Robot",
  "concepts": [
    {
      "name": "Robot",
      "score": 0.702650785446167
    },
    {
      "name": "Computer science",
      "score": 0.6506626009941101
    },
    {
      "name": "Human‚Äìcomputer interaction",
      "score": 0.5789894461631775
    },
    {
      "name": "Negotiation",
      "score": 0.5704348683357239
    },
    {
      "name": "Task (project management)",
      "score": 0.5431522130966187
    },
    {
      "name": "Context (archaeology)",
      "score": 0.534116804599762
    },
    {
      "name": "Human‚Äìrobot interaction",
      "score": 0.4945387542247772
    },
    {
      "name": "Deliberation",
      "score": 0.48225241899490356
    },
    {
      "name": "Social robot",
      "score": 0.427898108959198
    },
    {
      "name": "Artificial intelligence",
      "score": 0.29333630204200745
    },
    {
      "name": "Mobile robot",
      "score": 0.19193780422210693
    },
    {
      "name": "Engineering",
      "score": 0.17437481880187988
    },
    {
      "name": "Robot control",
      "score": 0.15487203001976013
    },
    {
      "name": "Political science",
      "score": 0.08225122094154358
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I135310074",
      "name": "University of Wisconsin‚ÄìMadison",
      "country": "US"
    }
  ],
  "cited_by": 80
}