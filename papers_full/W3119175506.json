{
  "title": "XLM-T: Scaling up Multilingual Machine Translation with Pretrained Cross-lingual Transformer Encoders",
  "url": "https://openalex.org/W3119175506",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2628404079",
      "name": "Ma Shuming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2008127672",
      "name": "Yang Jian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2306251904",
      "name": "Huang, Haoyang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3163607453",
      "name": "Chi, Zewen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097573093",
      "name": "Dong Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1818543254",
      "name": "Zhang, Dongdong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281860461",
      "name": "Awadalla, Hany Hassan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281860459",
      "name": "Muzio, Alexandre",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4283838737",
      "name": "Eriguchi, Akiko",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224447770",
      "name": "Singhal, Saksham",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108846051",
      "name": "Song Xia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281452648",
      "name": "Menezes, Arul",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2389670735",
      "name": "Wei, Furu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963993537",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W3032816972",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2951451051",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3092327118",
    "https://openalex.org/W2550821151",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3105813095",
    "https://openalex.org/W2963247703",
    "https://openalex.org/W2555745756",
    "https://openalex.org/W2971033911",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W2963088995",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W3105038888",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2964007535",
    "https://openalex.org/W3035812575",
    "https://openalex.org/W2888456631"
  ],
  "abstract": "Multilingual machine translation enables a single model to translate between different languages. Most existing multilingual machine translation systems adopt a randomly initialized Transformer backbone. In this work, inspired by the recent success of language model pre-training, we present XLM-T, which initializes the model with an off-the-shelf pretrained cross-lingual Transformer encoder and fine-tunes it with multilingual parallel data. This simple method achieves significant improvements on a WMT dataset with 10 language pairs and the OPUS-100 corpus with 94 pairs. Surprisingly, the method is also effective even upon the strong baseline with back-translation. Moreover, extensive analysis of XLM-T on unsupervised syntactic parsing, word alignment, and multilingual classification explains its effectiveness for machine translation. The code will be at https://aka.ms/xlm-t.",
  "full_text": "XLM-T: Scaling up Multilingual Machine Translation with\nPretrained Cross-lingual Transformer Encoders\nShuming Ma, Jian Yang, Haoyang Huang, Zewen Chi, Li Dong,\nDongdong Zhang, Hany Hassan Awadalla, Alexandre Muzio, Akiko Eriguchi,\nSaksham Singhal, Xia Song, Arul Menezes, Furu Wei\nMicrosoft Corporation\n{shumma,t-jianya,haohua,v-zechi,lidong1,dozhang}@microsoft.com\n{hanyh,alferre,akikoe,saksingh,xiaso,arulm,fuwei}@microsoft.com\nAbstract\nMultilingual machine translation enables a sin-\ngle model to translate between different lan-\nguages. Most existing multilingual machine\ntranslation systems adopt a randomly initial-\nized Transformer backbone. In this work, in-\nspired by the recent success of language model\npre-training, we present XLM-T, which initial-\nizes the model with an off-the-shelf pretrained\ncross-lingual Transformer encoder and ﬁne-\ntunes it with multilingual parallel data. This\nsimple method achieves signiﬁcant improve-\nments on a WMT dataset with 10 language\npairs and the OPUS-100 corpus with 94 pairs.\nSurprisingly, the method is also effective even\nupon the strong baseline with back-translation.\nMoreover, extensive analysis of XLM-T on un-\nsupervised syntactic parsing, word alignment,\nand multilingual classiﬁcation explains its ef-\nfectiveness for machine translation.1\n1 Introduction\nMultilingual neural machine translation (NMT) en-\nables a single model to translate between multiple\nlanguage pairs, which has drawn increasing atten-\ntion in the community (Firat et al., 2016a; Ha et al.,\n2016; Johnson et al., 2017; Aharoni et al., 2019;\nFan et al., 2020). Recent work shows that multilin-\ngual machine translation achieves promising results\nespecially for low-resource and zero-resource ma-\nchine translation (Firat et al., 2016b; Zoph et al.,\n2016; Sen et al., 2019; Zhang et al., 2020).\nPre-training-then-ﬁne-tuning framework (Devlin\net al., 2019; Liu et al., 2019; Dong et al., 2019;\nSong et al., 2019; Raffel et al., 2020) has shown\nsubstantial improvements on many natural lan-\nguage processing (NLP) tasks by pre-training a\nmodel on a large corpus and ﬁne-tuning it on the\ndownstream tasks. Pre-training multilingual lan-\nguage models (Conneau and Lample, 2019; Con-\n1The code will be at https://aka.ms/xlm-t.\nEncoder\nDecoder \nOff-the-shelf\nPretrained\nCross-lingual\nEncoder\nMultilingual NMT\nInitialize\nFine-tune Multilingual \nParallel Data\nFigure 1: Framework of XLM-T. We use off-the-shelf\npretrained cross-lingual encoders (such as XLM-R) to\ninitialize both the encoder and decoder of the multilin-\ngual NMT model. Then we ﬁne-tune the model on mul-\ntilingual parallel data.\nneau et al., 2020; Chi et al., 2020a,b; Xue et al.,\n2020) obtains signiﬁcant performance gains on a\nwide range of cross-lingual tasks, which is natu-\nrally applicable to multilingual machine transla-\ntion where the representations are shared among\ndifferent languages. Moreover, pre-training has\ngreat potential in efﬁciently scaling up multilin-\ngual NMT, while existing methods, such as back-\ntranslation (Sennrich et al., 2016), are expensive in\nthe multilingual setting.\nMost existing work (Conneau and Lample, 2019;\nSong et al., 2019; Lewis et al., 2020) on leveraging\npretrained models for machine translation mainly\nlies in the bilingual setting. How to effectively\nand efﬁciently use these existing pretrained mod-\nels for multilingual machine translation is not fully\nexplored. Liu et al. (2020) introduce a sequence-\nto-sequence denoising auto-encoder (mBART) pre-\ntrained on large-scale monolingual corpora in many\nlanguages. Lin et al. (2020) propose to pretrain\nthe multilingual machine translation models with\na code-switching objective function. However,\nthis model requires a large-scale parallel data for\npre-training, which hinders its application to low-\nresource and zero-resource languages.\nIn this work, we present a simple and effective\nmethod XLM-T that initializes multilingual ma-\narXiv:2012.15547v1  [cs.CL]  31 Dec 2020\nchine translation with a pretrained cross-lingual\nTransformer encoder and ﬁne-tunes it using multi-\nlingual parallel data. The cross-lingual pretrained\nencoders are off-the-shelf for general cross-lingual\nNLP tasks so we do not need to speciﬁcally pre-\ntrain for machine translation. We adopt XLM-\nR (Conneau et al., 2020) as the pretrained encoder\nand conduct extensive experiments on multilingual\nmachine translation with 10 language pairs from\nWMT datasets2 and 94 language pairs from OPUS\ndatasets3. This simple method achieves signiﬁcant\nand consistent gains on both large-scale datasets.\nThe improvement is still signiﬁcant over the strong\nbaseline with back-translation.\nTo analyze how the pretrained encoders beneﬁt\nmultilingual machine translation, we perform some\nprobing tasks for both XLM-T and a randomly\ninitialized multilingual NMT baseline. Empirical\nstudies show that XLM-T improves the abilities\nof syntactic parsing, word alignment, and multi-\nlingual classiﬁcation. We believe that this work\ncan shed light on further improvements of applying\npretrained models to machine translation.\n2 XLM-T\nIn this section, we introduce our proposed model:\nCross-lingual Language Modeling Pre-training for\nTranslation, which is denoted as XLM-T.\n2.1 Multilingual Machine Translation\nSuppose we have L languages to translate in a\nmodel. Among these languages, we have N bilin-\ngual corpora, each of which contains parallel sen-\ntences {(x1\nLi,x1\nLj ),··· ,(xk\nLi,xk\nLj )}between Li\nand Lj, where kis the number of training instances.\nGiven the corpora, we are able to train a multilin-\ngual model Pθ that enables the translation among\ndifferent languages. With the parallel data of N\nlanguage direction, the model is learnt with a com-\nbination of different objective:\nL= −\n∑\ni,j,k\nlog Pθ(xk\nLi,xk\nLj ) (1)\nTypically, the multilingual NMT model uses a\nuniﬁed model that shares the encoders and decoders\nfor all translation directions. In this work, we adopt\nthe state-of-the-art Transformer as the backbone\nmodel Pθ. Following the methods of Ha et al.\n(2016) and Johnson et al. (2017), we prepend a\n2http://www.statmt.org\n3http://opus.nlpl.eu/opus-100.php\ntarget language token to each source sentence to\nindicate which language should be translated on\nthe target side.\n2.2 Cross-lingual Pretrained Encoders\nIn this work, we argue that multilingual NMT mod-\nels can be scaled up by pre-training the encoder\nwith large-scale monolingual data. Multilingual\nNMT encourages a shared representation among\ndifferent languages so that the data in one language\nhelps to model the other language. Meanwhile,\ncross-lingual pretrained encoders prove to be effec-\ntive in transferring cross-lingual representations.\nIn this work, we adopt XLM-R BASE (Conneau\net al., 2020) as the pretrained encoder. It was\ntrained in 100 languages, using more than two ter-\nabytes of ﬁltered CommonCrawl data. XLM-R\nis based on the Transformer architecture, trained\nusing the multilingual masked language model\n(MLM) objective (Conneau and Lample, 2019).\nIt has a shared vocabulary of 250,000 tokens based\non SentencePiece model (Kudo and Richardson,\n2018).\n2.3 Initialization Strategy\nGiven the above pretrained encoder, we can use\nit to initialize the encoder and decoder of the\nTransformer-based multilingual NMT model.\nInitializing cross-lingual encoder There are dif-\nferent Transformer variants in terms of the NMT\nencoder. To initialize our NMT encoder with pre-\ntrained XLM-R, we make their architectures con-\nsistent. We add a layer normalization layer after\nthe embedding layer and do not scale up the word\nembedding. We use post layer normalization for\nboth the attention layers and feed-forward layers.\nThe activation insides the feed-forward layers is\nGELU (Hendrycks and Gimpel, 2016). The posi-\ntional embedding is learned during training.\nInitializing cross-lingual decoder The pre-\ntrained encoder can be also used to initialize the\ndecoder. The architecture of the decoder is the\nsame as that of the encoder, except that there is a\ncross-attention layer after the self-attention layer.\nDue to this difference, we explore several meth-\nods to initialize the decoder, including sharing the\nweights of cross-attention layers and self-attention\nlayers and randomly initializing the cross-attention.\n2.4 Multilingual Fine-tuning\nWe can now ﬁne-tune our XLM-T model with the\nobjective function (Eq. 1). A simple concatenation\nof all parallel data will lead to poor performance\non low-resource translation because of the imbal-\nanced data. Following the previous work (Aha-\nroni et al., 2019; Wang et al., 2020), we adopt a\ntemperature-based batch balance method by sam-\npling the sentence pairs in different languages ac-\ncording to a multinomial distribution with proba-\nbilities {q1,q2,··· ,qN}:\nqi = p\n1\nT\ni\n∑L\nj=1 p\n1\nT\nj\n(2)\npi = |Li|∑\nL|Lj| (3)\nwhere N is the number of translation directions,\n|Li|is the number of parallel data for i-th direction,\nand T is a temperature.\nTo reduce over-sampling of low-resource lan-\nguages in the early stage of training, we employ\na dynamic temperate sampling mechanism (Wang\net al., 2020). The temperature is low at the begin-\nning of training and is gradually increased for the\nﬁrst several epochs. Formally, the temperature can\nbe written as:\nTi = min(T,T0 + i\nN(T −T0)) (4)\nwhere T0 is the initial temperature, T is the peak\ntemperature, and N is the number of warming-up\nepochs. For a fair comparison, we set T0 = 1.0,\nT = 5.0, and N = 5for all the experiments in our\nwork.\n3 Experimental Setup\n3.1 Data\nWMT-10 Following (Wang et al., 2020), we use\na collection of parallel data in different languages\nfrom the WMT datasets to evaluate the models.\nThe parallel data is between English and other 10\nlanguages, including French (Fr), Czech (Cs), Ger-\nman (De), Finnish (Fi), Latvian (Lv), Estonian (Et),\nRomanian (Ro), Hindi (Hi), Turkish (Tr) and Gu-\njarati (Gu). We choose the data from the latest\navailable year of each language and exclude Wiki-\nTiles. We also remove the duplicated samples and\nlimit the number of parallel data in each language\npair up to 10 million by randomly sampling from\nthe whole corpus. We use the same test sets and\nvalidation set as in (Wang et al., 2020). The details\ncan be found in Appendix.\nIn the back-translation setting, we collect large-\nscale monolingual data for each language from\nNewsCrawl4. We remove the data with low quality,\nand randomly sample 5 million sentences in each\nlanguage. For the languages without enough data\n(Fi, Lv, Et, Gu), we also sample additional data\nfrom CCNet (Wenzek et al., 2020) to combine with\nthat from NewsCrawl. We use a target-to-source\nmultilingual NMT model to back-translate these\nmonolingual data as the augmented parallel data.\nOPUS-100 To evaluate our model in the mas-\nsively multilingual machine translation setting, we\nuse the OPUS-100 corpus provided by Zhang et al.\n(2020). OPUS-100 is an English-centric multilin-\ngual corpus covering 100 languages, which is ran-\ndomly sampled from the OPUS collection.\nThe dataset is split into training, development,\nand test sets. The training set has up to 1 million\nsentence pairs per language pair, while the devel-\nopment and test sets contain up to 2000 parallel\nsentences. The whole dataset contains approxi-\nmately 55 million sentence pairs. We remove 5\nlanguages without any development set or test sets,\nwhich results in 95 languages including English.\n3.2 Pretrained Models and Baselines\nWe use the state-of-the-art Transformer model for\nall our experiments with the fairseq5 implementa-\ntion (Ott et al., 2019). For the baseline model of\nthe WMT-10 dataset, we adopt a Transformer-big\narchitecture with a 6-layer encoder and decoder.\nThe hidden size, embedding size and the number\nof attention head is 1024, 1024, and 16 respec-\ntively, while the dimension of feedforward layer\nis 4096. We tokenize the data with SentencePiece\nmodel (Kudo and Richardson, 2018) with a vo-\ncabulary size of 64,000 tokens extracted from the\ntraining set.\nFor XLM-T, we initialize with XLM-R base\nmodel, which has 12-layer encoder, 6-layer de-\ncoder, 768 hidden size, 12 attention head, and\n3,072 dimensions of feedforward layers. We do\nnot use a deeper decoder because our preliminary\nexperiments show no improvement by increasing\nthe number of decoder layers, which is consis-\ntent with the observations in (Kasai et al., 2020).\n4http://data.statmt.org/news-crawl\n5https://github.com/pytorch/fairseq\nX →En Fr Cs De Fi Lv Et Ro Hi Tr Gu Avg\nTrain on Original Parallel Data (Bitext)\nBilingual NMT 36.2 28.5 40.2 19.2 17.5 19.7 29.8 14.1 15.1 9.3 23.0\nMany-to-One 34.8 29.0 40.1 21.2 20.4 26.2 34.8 22.8 23.8 19.2 27.2\nXLM-T 35.9 30.5 41.6 22.5 21.4 28.4 36.6 24.6 25.6 20.4 28.8\nMany-to-Many 35.9 29.2 40.0 21.1 20.4 26.3 35.5 23.6 24.3 20.6 27.7\nXLM-T 35.5 30.0 40.8 22.1 21.5 27.8 36.5 25.3 25.0 20.6 28.5\nTrain on Original Parallel Data andBack-Translation Data (Bitext+BT)\n(Wang et al., 2020) 35.3 31.9 45.4 23.8 22.4 30.5 39.1 28.7 27.6 23.5 30.8\nMany-to-One 35.9 32.6 44.1 24.9 23.1 31.5 39.7 28.2 27.8 23.1 31.1\nXLM-T 36.0 33.1 44.8 25.4 23.9 32.7 39.8 30.1 28.8 23.6 31.8\n(Wang et al., 2020) 35.3 31.2 43.7 23.1 21.5 29.5 38.1 27.5 26.2 23.4 30.0\nMany-to-Many 35.7 31.9 43.7 24.2 23.2 30.4 39.1 28.3 27.4 23.8 30.8\nXLM-T 36.1 32.6 44.3 25.4 23.8 32.0 40.3 29.5 28.7 24.2 31.7\nTable 1: X →En test BLEU for bilingual, many-to-one, and many-to-many models on WMT-10. On the top are\nthe models trained with original parallel data, while the bottom are combined with back-translation. The languages\nare ordered from high-resource (left) to low-resource (right).\nDifferent from WMT-10, massively multilingual\nNMT suffers from weak capacity (Zhang et al.,\n2020). Therefore, for the baseline of the OPUS-\n100 dataset, we adopt the same architecture and\nvocabulary as XLM-T but randomly initializing the\nparameters so that the numbers of parameters are\nthe same. We tie the weights of encoder embed-\ndings, decoder embeddings, and output layers in\nall experiments.\n3.3 Training and Evaluation\nWe train all models with Adam Optimizer (Kingma\nand Ba, 2015) with β1 = 0.9 and β2 = 0.98.\nThe learning rate is among {3e-4, 5e-4 }with a\nwarming-up step of 4,000. The models are trained\nwith the label smoothing cross-entropy, and the\nsmoothing ratio is 0.1. We set the dropout of atten-\ntion layers as 0.0, while the rest of the dropout rate\nis 0.1. We limit the source length and the target\nlength to be 256. For the WMT-10 dataset, the\nbatch size is 4,096 and we accumulate the gradi-\nents by 16 batches. For the OPUS-100 dataset, we\nset the batch size as 2,048 and the gradients are\nupdated every 32 batches. All experiments on the\nWMT-10 dataset are conducted on 8 V100 GPUs,\nwhile the experiments on OPUS-100 are on a DGX-\n2 machine with 16 V100 GPUs.\nDuring testing, we use the beam search algo-\nrithm with a beam size of 5. We set the length\npenalty as 1.0. The last 5 checkpoints are averaged\nfor evaluation. We report the case-sensitive detok-\nenized BLEU using sacreBLEU6 (Post, 2018).\n4 Results\n4.1 WMT-10\nWe study the performance of XLM-T in three mul-\ntilingual translation scenarios, including many-to-\nEnglish (X →En), English-to-many (En→X), and\nmany-to-many (X →Y). For many-to-many, we\nuse a combination of English-to-many and many-to-\nEnglish as the training data. We compare XLM-T\nwith both the bilingual NMT and the multilingual\nNMT models to verify the effectiveness.\nTable 1 reports the results on the X →En test\nsets. Compared with the bilingual baseline, the mul-\ntilingual models achieve much better performance\non the low-resource languages and are worse on\nthe high-resource languages. In general, the mul-\ntilingual baseline outperforms the bilingual base-\nlines by an average of +4.2 points. In the many-\nto-English scenario, XLM-T achieves signiﬁcant\nimprovements over the multilingual baseline across\nall 10 languages. The average gain is +1.6 points.\nIn the many-to-many scenario, the gain becomes\nnarrow, but still reaches +0.8 points over the multi-\nlingual baseline. We further combine the parallel\n6BLEU+case.mixed+lang.{src}-\n{tgt}+numrefs.1+smooth.exp+tok.13a+version.1.4.14\nEn →X Fr Cs De Fi Lv Et Ro Hi Tr Gu Avg\nTrain on Original Parallel Data (Bitext)\nBilingual NMT 36.3 22.3 40.2 15.2 16.5 15.0 23.0 12.2 13.3 7.9 20.2\nOne-to-Many 34.2 20.9 40.0 15.0 18.1 20.9 26.0 14.5 17.3 13.2 22.0\nXLM-T 34.8 21.4 39.9 15.4 18.7 20.9 26.6 15.8 17.4 15.0 22.6\nMany-to-Many 34.2 21.0 39.4 15.2 18.6 20.4 26.1 15.1 17.2 13.1 22.0\nXLM-T 34.2 21.4 39.7 15.3 18.9 20.6 26.5 15.6 17.5 14.5 22.4\nTrain on Original Parallel Data andBack-Translation Data (Bitext+BT)\n(Wang et al., 2020) 36.1 23.6 42.0 17.7 22.4 24.0 29.8 19.8 19.4 17.8 25.3\nOne-to-Many 36.8 23.6 42.9 18.3 23.3 24.2 29.5 20.2 19.4 13.2 25.1\nXLM-T 37.3 24.2 43.6 18.1 23.7 24.2 29.7 20.1 20.2 13.7 25.5\n(Wang et al., 2020) 35.8 22.4 41.2 16.9 21.7 23.2 29.7 19.2 18.7 16.0 24.5\nMany-to-Many 35.9 22.9 42.2 17.5 22.5 23.4 28.9 19.8 19.1 14.5 24.7\nXLM-T 36.6 23.9 42.4 18.4 22.9 24.2 29.3 20.1 19.8 12.8 25.0\nTable 2: En →X test BLEU for bilingual, many-to-one, and many-to-many models on WMT-10. On the top are\nthe models trained with original parallel data, while the bottom are combined with back-translation. The languages\nare ordered from high-resource (left) to low-resource (right).\nModels X → En En → X\nHigh Med Low Avg WR High Med Low Avg WR\nBest System from (Zhang et al., 2020) 30.3 32.6 31.9 31.4 - 23.7 25.6 22.2 24.0 -\nMany-to-Many 31.5 35.1 36.0 33.6 ref 25.6 30.5 30.5 28.2 ref\nXLM-T 32.4 35.9 36.9 34.5 89.4 26.1 30.9 31.0 28.6 75.5\nTable 3: X →En and En →X test BLEU for high/medium/low resource language pairs in many-to-many setting\non OPUS-100 test sets. The BLEU scores are average across all language pairs in the respective groups. “WR”:\nwin ratio (%) compared to ref.\ndata with back-translation. Back-translation results\nin a large gain of +3.9 BLEU score over the base-\nline. Therefore, back-translation is a strong base-\nline for multilingual NMT. In the back-translation\nsetting, XLM-T can further improve this strong\nbaseline by a signiﬁcant gain of +0.7 points, show-\ning the effectiveness of XLM-T. As for the many-\nto-many setting, the improvement is even larger,\nreaching a difference of +0.9 points. We compare\nXLM-T with Wang et al. (2020)’s method. Be-\nsides back-translation, they use the monolingual\ndata (i.e. the target side of back-translation data)\nwith two tasks of Mask Language Model (MLM)\nand Denoising AutoEncoder (DAE). It shows that\nXLM-T can outperform this method in both the\nmany-to-one and many-to-many settings.\nTable 2 summarizes the results on the En →X\ntest sets. Similar to the results of X →En, the\nmultilingual NMT improves the average BLEU\nscore of the bilingual baseline, while XLM-T beats\nthe multilingual baseline by +0.6 points. As for\nthe many-to-many and back-translation scenarios,\nXLM-T yields the increments of +0.4 points, +0.4\npoints, and +0.3 points, respectively. Compared\nwith Wang et al. (2020)’s method, XLM-T has sim-\nilar performance in the one-to-many setting, and a\nslightly improvement of +0.5 BLEU in the many-\nto-many scenario. The performance of XLM-T in\nGu is worse than that of Wang et al. (2020). We\nconjecture that this is related to the implementation\ndetails of data sampling. Generally, the improve-\nments are smaller than X →En. We believe it is\nbecause the multilingual part of En →X is at the\ndecoder side, which XLM-R is not an expert in.\nHow to improve En →X with pretrained models\nis a promising direction to explore in the future.\n4.2 OPUS-100\nTo further verify the effectiveness of XLM-T on\nmassively multilingual machine translation, we\nModels #Layer #Hidden BLEU\nMultilingual NMT 6/6 1024 27.2\nMultilingual NMT 12/6 768 26.9\nXLM-T 12/6 768 28.8\nTable 4: Ablation study of Transformer architectures\non WMT-10 test sets. The BLEU scores are average\nacross 10 languages on WMT-10 X →En test sets.\n#Layer denotes the number of encoder/decoder layers,\nwhile #Hidden means the hidden size.\nconduct experiments on OPUS-100, which consists\nof 100 languages including English. After remov-\ning 5 languages without test sets, we have 94 lan-\nguage pairs from and to English. Following Zhang\net al. (2020), we group the languages into three cate-\ngories, including high-resource languages (≥0.9M,\n45 languages), low-resource languages (<0.1M, 21\nlanguages), and medium-resource languages (the\nrest, 28 languages). According to the previous\nwork (Zhang et al., 2020), the performance of mas-\nsively multilingual machine translation is sensitive\nto the model size (i.e. the number of parameters),\nbecause the model capacity is usually the bottle-\nneck when the numbers of languages and data are\nmassive. Therefore, we make the architectures of\nbaseline and XLM-T consistent to ensure the pa-\nrameters are exactly equal.\nBoth the multilingual baseline and XLM-T are\ntrained in the many-to-many setting. Table 3 re-\nports their results on OPUS-100 as well as the\nperformance of the best system from Zhang et al.\n(2020). For the X →En test sets, XLM-T has con-\nsistent and signiﬁcant gains over the multilingual\nbaseline for all the high (+0.9 BLEU), medium\n(+0.8 BLEU), and low (+0.9 BLEU) resource lan-\nguages. The overall improvement is +0.9 points\nby averaging all 94 En →X language pairs. For\nthe En →X test sets, XLM-T also beneﬁts the\nhigh/medium/low resource languages. Generally,\nthe performance improves by +0.4 points in terms\nof the average BLEU scores on En →X test sets.\nWe also compute the win ratio (WR), which counts\nthe proportion of languages where XLM-T outper-\nforms the baselines. It shows that XLM-T is better\nin 89.4% of the language pairs on the X →En test\nsets and 75.5% on the En →X test sets.\n4.3 Ablation Studies\nEffect of architectures For the WMT-10 experi-\nments, the architecture of XLM-T is different from\nthe multilingual baseline, including the number of\nModels X → En En → X\nMultilingual NMT 27.7 22.0\nXLM-T (enc.) 28.4 22.0\nXLM-T (enc.+dec.) 28.5 22.4\nTable 5: Ablation study on different initialization strate-\ngies in the many-to-many setting on WMT-10 test sets.\nThe BLEU scores are average on each test set.\nencoder layers, the hidden size, the layer normaliza-\ntion layer, and the activation function. To identify\nwhether the architecture or the weights of XLM-T\nimproves the performance, we perform an ablation\nstudy by initializing XLM-T with random weights.\nTable 4 shows that the architecture of XLM-T does\nnot improve the performance of the multilingual\nbaseline, leading to a slight drop of -0.3 points.\nWith our initialization strategies, XLM-T improves\nby a signiﬁcant gain of +1.9 points. This proves\nthat the initialization of XLM-T is the main con-\ntribution of the improvement. For the OPUS-100\nexperiments, the architecture of XLM-T is the same\nas the multilingual baseline, so we do not need any\nadditional ablation on the architecture.\nEffect of initialization strategies To analyze the\neffect of the proposed initialization strategies, we\nconduct an ablation study by removing the encoder\ninitialization and decoder initialization. Table 5\nsummarizes the results. It shows that the encoder\ninitialization mainly contributes to the improve-\nments of X →En. It is because that the source\nsides of this scenario are multilingual, while that\nof E →X is English-only. Similarly, the decoder\ninitialization mainly beneﬁts E →X, whose tar-\nget side is multilingual. Moreover, it concludes\nthat the encoder initialization contributes to more\ngains than the decoder initialization for multilin-\ngual NMT. The reason may be XLM-R is more\nconsistent with the encoder, while lacks the model-\ning of cross-attention layer for the decoder.\n5 Analysis\nTo analyze how XLM-T improves multilingual ma-\nchine translation, we perform three probing tasks,\nincluding unsupervised dependency parsing, multi-\nlingual classiﬁcation, and word alignment retrieval.\n5.1 Word Alignment\nWord alignment is an important metric to evalu-\nate the ability to transfer between different lan-\nguages. We assume that XLM-T improves the inter-\nModels Cs De Fr Ro Avg\nXLM-R 30.78 26.46 26.24 31.74 28.81\nMultilingual NMT 24.16 21.37 31.18 28.90 26.40\nXLM-T 20.97 21.47 30.89 24.91 24.56\nTable 6: Analysis of word alignment error on Sabet\net al. (2020)’s alignment datasets. We report alignment\nerror rate scores (the lower the better).\nnal translation transfer by improving the similarity\nof encoder representations between two translated\nwords. Therefore, the ability to translate one lan-\nguage can easily beneﬁt that of translating the other\nlanguage. To evaluate the performance of word\nalignment, we use the same labeled alignment data\nas in (Sabet et al., 2020), which is original from\nEuroparl and WPT datasets. The alignment data is\nbetween English and six other languages, including\nCzech, German, French, Hindi, Romanian, and Per-\nsian. We discard Persian and Hindi, which is either\nnot in WMT-10 or only contains 90 test samples.\nSetup We compare the alignment error rate be-\ntween XLM-T and multilingual NMT baseline.\nBoth models are trained with the WMT-10 dataset\nin the many-to-many scenario. Given a sentence\npair, we prepend a language token to each sentence\nand compute the representations of each word by\naveraging the representations of its subwords. A\nsimilarity matrix can be obtained by calculating\nthe cosine distance between words from two sen-\ntences. With the similarity matrices, we use the\nIterMax (Sabet et al., 2020) algorithm to extract\nthe alignments. IterMax is iterative Argmax, which\nmodiﬁes the similarity matrix conditioned on the\nalignment edges found in a previous iteration. We\ncompare the extracted alignments with the ground\ntruth to measure the alignment error rate.\nResults Table 6 summarizes the performance of\nmultilingual NMT and XLM-T. The scores are\nlower-the-better. We also report the score of XLM-\nR for the reference. Both multilingual NMT and\nXLM-T outperform XLM-R because MT data ben-\neﬁts the word alignment. Compared XLM-T with\nthe baseline, it shows that there are signiﬁcant\ngains in En-Cs, En-Fr, and En-Ro, indicating much\nhigher similarities of XLM-T between two trans-\nlated words in these languages. In general, the aver-\nage alignment error rate across different languages\nfor XLM-T achieves 24.56%, outperforming the\nmultilingual baseline by 1.84%. This supports our\nassumption that XLM-T improves the similarities\nof the encoder representations between two lan-\nguages.\n5.2 Unsupervised Dependency Parsing\nPrior work (Raganato and Tiedemann, 2018) prove\nthat the encoder of Transformer-based NMT learns\nsome syntactic information. We investigate that\nwhether XLM-T can induce better syntactic tree\nstructures. The self-attention insides Transformer\ncomputes the weights between pairs of tokens,\nwhich can be formulated as a weighted graph.\nTherefore, we extract a tree structure from the\ngraph. We compare the extracted tree with its an-\nnotated dependency tree to see whether XLM-T\nimproves the ability of unsupervised dependency\nparsing.\nSetup We compare the accuracy of dependency\nparsing between multilingual NMT baseline and\nXLM-T. Both models are trained with the WMT-\n10 dataset in the many-to-many setting. We use\nUniversal Dependencies7 as the test set to probe\nthe performance and evaluate in 10 languages (i.e.,\nEnglish, French, Czech, German, Finnish, Latvian,\nEstonian, Romanian, Hindi, and Turkish) that ap-\npear in both WMT-10 and Universal Dependencies.\nTo extract dependency trees, we average the at-\ntention scores overall heads in each layer as the\nweights and compute the maximum spanning trees\nwith Chu-Liu/Edmonds’ Algorithm. Since the sen-\ntence is tokenized with SentencePiece, we average\nthe weights of all tokens for each word. The gold\nroot of each sentence is used as the starting node\nfor the maximum spanning tree algorithm. We com-\npute the Unlabeled Attachment Score (UAS) with\nCoNLL 2017’s evaluation script8.\nResults As shown in Table 7, we compare the\nUAS F1 score of multilingual NMT and XLM-T.\nWe evaluate the performance of each layer and\nsummarize the results of the layer with the high-\nest average score over all languages. According\nto Table 7, of all 10 languages, the multilingual\nbaseline outperforms XLM-T in 3 languages (De,\nEt, Ro), while XLM-T beats the baseline in the rest\n7 languages. For Cs, Fi, and Hi, XLM-T has a sig-\nniﬁcant gain of more than 3 points compared with\nthe baseline. Generally, XLM-T gets 32.81% UAS,\nimproving the baseline by 1.43%. This proves that\nXLM-T induces a better syntactic tree structure\n7https://universaldependencies.org\n8http://universaldependencies.org/conll17/evaluation.html\nModels En Cs De Fi Lv Et Ro Hi Tr Fr Avg\nMultilingual NMT 31.64 27.61 40.72 31.88 31.61 25.92 24.25 32.82 31.72 35.58 31.38\nXLM-T 32.71 33.34 39.51 35.52 33.27 25.09 21.79 37.82 32.86 36.21 32.81\nTable 7: Analysis of unsupervised dependency parsing performance on Universal Dependencies. The evaluation\nmetric is UAS F1 score (%).\nModels En De Hi Tr Fr Avg\nXLM-R 85.8 79.3 72.8 76.2 79.4 78.7\nMultilingual NMT 77.1 73.4 66.6 69.7 72.6 71.9\nXLM-T 80.4 75.2 66.7 74.0 75.3 74.3\nTable 8: Analysis of multilingual classiﬁcation on\nXNLI. The evaluation metric is accuracy (%).\nacross different languages, which potentially im-\nproves multilingual NMT.\n5.3 Multilingual Classiﬁcation\nSince multilingual NMT uses a shared represen-\ntation for different languages, we assume XLM-T\nbeneﬁts multilingual NMT by improving the mul-\ntilingual representations. To verify this, we use\nthe XNLI dataset, which is a widely used testbed\nfor multilingual representation. We evaluate the\nperformance of each language separately.\nSetup We compare the accuracy of XNLI be-\ntween multilingual NMT baseline and XLM-T.\nBoth models are trained with the WMT-10 dataset.\nWe retain the encoders and put a projection layer\non the top of the ﬁrst token. The premise and hy-\npothesis are concatenated as the input and fed into\nthe model to produce a label indicating whether\nthere is an entailment, contradiction, or neutral re-\nlationship. We ﬁne-tune with the training data of\neach language. We evaluate the performance in 5\nlanguages (i.e., English, German, Hindi, Turkish,\nFrench) that are shared by WMT-10 and XNLI.\nResults Table 8 reports the results on the XNLI\ndataset. XLM-R is the best, showing ﬁne-tuning\nwith MT data degrades the performance on XNLI.\nThis is because the training objective biases to-\nwards translation. It shows that XLM-T beats the\nmultilingual baseline in 4 languages with signif-\nicant gains (En +3.3%, De +1.8%, Tr +4.3%, Fr\n+2.7%) as well as slightly better accuracy in Hi\n(+0.1%). The average accuracy across 5 languages\nis 74.2%, improving the baseline by 2.3%. The\nresults indicate that XLM-T improves the represen-\ntations among different languages, which is impor-\ntant for multilingual NMT, especially when trans-\nlating low-resource languages.\n6 Related Work\nMultilingual Machine Translation Firat et al.\n(2016a) proposed a many-to-many model to sup-\nport translating between multiple languages by us-\ning speciﬁc encoders and decoders for each lan-\nguage while sharing the attention mechanism. Ha\net al. (2016) and Johnson et al. (2017) introduced a\nuniﬁed model that shared the encoders, decoders,\nand the attention mechanism for all languages.\nThey used a language token to indicate which tar-\nget language to be translated. Firat et al. (2016b)\nproved that this multilingual NMT model can\ngeneralize to untrained language pairs, which en-\nabled zero-resource translation. Zoph et al. (2016)\nshowed that training on high-resource languages\nhelps transfer to low-resource machine translation.\nMore recent work focused on model architecture\nwith different strategies of sharing parameters or\nrepresentations. Blackwood et al. (2018) proposed\nto share all parameters but that of the attention\nlayers. Platanios et al. (2018) introduced a model\nthat learns to generate speciﬁc parameters for a lan-\nguage pair while sharing the rest parameters. Gu\net al. (2018) utilized a transfer-learning approach\nto share lexical and sentence level representations\nacross multiple source languages into one target\nlanguage. In contrast, we do not modify the archi-\ntecture of multilingual machine translation.\nRecently, there are some work focusing on scal-\ning up multilingual machine translation. Aharoni\net al. (2019) performed extensive experiments in\ntraining massively multilingual NMT models, en-\nabling the translation of up to 102 languages within\na single model. Zhang et al. (2020) set up a bench-\nmark collected from OPUS for massively multilin-\ngual machine translation research and experiments.\nGpipe (Huang et al., 2019) scaled up multilin-\ngual NMT with a very large and deep Transformer\nmodel. Gshard (Lepikhin et al., 2020) enabled to\nscale up multilingual NMT model with Sparsely-\nGated Mixture-of-Experts beyond 600 billion pa-\nrameters using automatic sharding. M2M-100 (Fan\net al., 2020) built a multilingual parallel dataset\nthrough large-scale mining. They also investigated\nthe methods to increase model capacity through a\ncombination of dense scaling and language-speciﬁc\nsparse parameters. Different from these work, we\ndo not scale the training data or increase the model\nsize. Instead, we propose to leverage a pretrained\nmodel that has been learned on large-scale mono-\nlingual data.\nLanguage Model Pre-training Devlin et al.\n(2019) and Liu et al. (2019) use masked language\nmodeling to pretrain the model on large-scale\nmonolingual corpora and transferred to various\ndownstream datasets. Yang et al. (2019) proposed\na generalized auto-aggressive pre-training method\nthat enables learning bidirectional contexts by max-\nimizing the expected likelihood over all permu-\ntations of the factorization order. UniLM (Dong\net al., 2019; Bao et al., 2020) are uniﬁed pretrained\nlanguage models that can be ﬁne-tuned for both nat-\nural language understanding and generation tasks.\nHao et al. (2019) show that language model pre-\ntraining provides a good initial point for NLP tasks,\nwhich improves performance and generalization\ncapability . In addition, XLM (Conneau and Lam-\nple, 2019), XLM-R (Conneau et al., 2020) and\nInfoXLM (Chi et al., 2020b) are the multilingual\npretrained language models that achieve signiﬁ-\ncant gains for a wide range of cross-lingual tasks.\nThere are some models (Song et al., 2019; Raf-\nfel et al., 2020; Xue et al., 2020; Lewis et al.,\n2020; Liu et al., 2020; Tang et al., 2020) based on\nthe encoder-decoder framework that enables ﬁne-\ntuning the whole models for language generation\ntasks. Lin et al. (2020) pretrain the multilingual ma-\nchine translation models with a code-switching ob-\njective function. Compared with previous work, we\nfocus on how to ﬁne-tune pretrained cross-lingual\nencoders towards multilingual machine translation.\n7 Conclusion\nIn this work, we propose XLM-T to scale up multi-\nlingual machine translation using pretrained cross-\nlingual encoders. This is achieved by initializing\nthe multilingual NMT model with the off-the-shelf\nXLM-R model. XLM-T can achieve signiﬁcant im-\nprovements on two large-scale multilingual trans-\nlation benchmarks, even over the strong baseline\nwith back-translation. We perform three probing\ntasks for XLM-T, including word alignment, un-\nsupervised dependency parsing, and multilingual\nclassiﬁcation. The probing results explain its ef-\nfectiveness for machine translation. This simple\nmethod can be used as a new strong baseline for\nfuture multilingual NMT systems.\nReferences\nRoee Aharoni, Melvin Johnson, and Orhan Firat. 2019.\nMassively multilingual neural machine translation.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2019, Volume 1, pages 3874–3884. As-\nsociation for Computational Linguistics.\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan\nYang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Song-\nhao Piao, Ming Zhou, and Hsiao-Wuen Hon. 2020.\nUniLMv2: Pseudo-masked language models for\nuniﬁed language model pre-training. In Proceed-\nings of the 37th International Conference on Ma-\nchine Learning, ICML 2020, 13-18 July 2020, Vir-\ntual Event, volume 119 of Proceedings of Machine\nLearning Research, pages 642–652. PMLR.\nGraeme Blackwood, Miguel Ballesteros, and Todd\nWard. 2018. Multilingual neural machine transla-\ntion with task-speciﬁc attention. In Proceedings of\nthe 27th International Conference on Computational\nLinguistics, pages 3112–3122, Santa Fe, New Mex-\nico, USA. Association for Computational Linguis-\ntics.\nZewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-\nLing Mao, and Heyan Huang. 2020a. Cross-lingual\nnatural language generation via pre-training. In The\nThirty-Fourth AAAI Conference on Artiﬁcial Intelli-\ngence, AAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 7570–7577. AAAI Press.\nZewen Chi, Li Dong, Furu Wei, Nan Yang, Sak-\nsham Singhal, Wenhui Wang, Xia Song, Xian-Ling\nMao, Heyan Huang, and Ming Zhou. 2020b. In-\nfoXLM: An information-theoretic framework for\ncross-lingual language model pre-training. CoRR,\nabs/2007.07834.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2020,\nOnline, July 5-10, 2020, pages 8440–8451. Associa-\ntion for Computational Linguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances\nin Neural Information Processing Systems 32: An-\nnual Conference on Neural Information Processing\nSystems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada, pages 7057–7067.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171–4186. Association for Computa-\ntional Linguistics.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Uniﬁed language\nmodel pre-training for natural language understand-\ning and generation. In Advances in Neural Infor-\nmation Processing Systems 32: Annual Conference\non Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver,\nBC, Canada, pages 13042–13054.\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\nMa, Ahmed El-Kishky, Siddharth Goyal, Man-\ndeep Baines, Onur Celebi, Guillaume Wenzek,\nVishrav Chaudhary, Naman Goyal, Tom Birch, Vi-\ntaliy Liptchinsky, Sergey Edunov, Edouard Grave,\nMichael Auli, and Armand Joulin. 2020. Be-\nyond english-centric multilingual machine transla-\ntion. CoRR, abs/2010.11125.\nOrhan Firat, Kyunghyun Cho, and Yoshua Bengio.\n2016a. Multi-way, multilingual neural machine\ntranslation with a shared attention mechanism. In\nNAACL HLT 2016, The 2016 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, June 12-17, 2016, pages 866–875. The Associ-\nation for Computational Linguistics.\nOrhan Firat, Baskaran Sankaran, Yaser Al-Onaizan,\nFatos T. Yarman-Vural, and Kyunghyun Cho. 2016b.\nZero-resource translation with multi-lingual neural\nmachine translation. In Proceedings of the 2016\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2016, Austin, Texas,\nUSA, November 1-4, 2016, pages 268–277. The As-\nsociation for Computational Linguistics.\nJiatao Gu, Hany Hassan, Jacob Devlin, and Victor O. K.\nLi. 2018. Universal neural machine translation for\nextremely low resource languages. In Proceedings\nof the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2018,\nNew Orleans, Louisiana, USA, June 1-6, 2018, Vol-\nume 1 (Long Papers) , pages 344–354. Association\nfor Computational Linguistics.\nThanh-Le Ha, Jan Niehues, and Alexander H. Waibel.\n2016. Toward multilingual neural machine trans-\nlation with universal encoder and decoder. CoRR,\nabs/1611.04798.\nYaru Hao, Li Dong, Furu Wei, and Ke Xu. 2019. Visu-\nalizing and understanding the effectiveness of BERT.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 4143–\n4152, Hong Kong, China. Association for Computa-\ntional Linguistics.\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\nYanping Huang, Youlong Cheng, Ankur Bapna, Orhan\nFirat, Dehao Chen, Mia Xu Chen, HyoukJoong\nLee, Jiquan Ngiam, Quoc V . Le, Yonghui Wu, and\nZhifeng Chen. 2019. GPipe: Efﬁcient training of gi-\nant neural networks using pipeline parallelism. In\nAdvances in Neural Information Processing Systems\n32: Annual Conference on Neural Information Pro-\ncessing Systems 2019, NeurIPS 2019, December 8-\n14, 2019, Vancouver, BC, Canada, pages 103–112.\nMelvin Johnson, Mike Schuster, Quoc V . Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Tho-\nrat, Fernanda B. Vi ´egas, Martin Wattenberg, Greg\nCorrado, Macduff Hughes, and Jeffrey Dean. 2017.\nGoogle’s multilingual neural machine translation\nsystem: Enabling zero-shot translation. Trans. As-\nsoc. Comput. Linguistics, 5:339–351.\nJungo Kasai, Nikolaos Pappas, Hao Peng, James Cross,\nand Noah A. Smith. 2020. Deep encoder, shallow\ndecoder: Reevaluating the speed-quality tradeoff in\nmachine translation. CoRR, abs/2006.10369.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2018: System Demonstrations, Brussels, Belgium,\nOctober 31 - November 4, 2018 , pages 66–71. As-\nsociation for Computational Linguistics.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,\nDehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. 2020.\nGShard: Scaling giant models with conditional\ncomputation and automatic sharding. CoRR,\nabs/2006.16668.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2020, Online, July 5-10, 2020 ,\npages 7871–7880. Association for Computational\nLinguistics.\nZehui Lin, Xiao Pan, Mingxuan Wang, Xipeng Qiu,\nJiangtao Feng, Hao Zhou, and Lei Li. 2020. Pre-\ntraining multilingual neural machine translation by\nleveraging alignment information. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2020, On-\nline, November 16-20, 2020 , pages 2649–2663. As-\nsociation for Computational Linguistics.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising\npre-training for neural machine translation. Trans.\nAssoc. Comput. Linguistics, 8:726–742.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2019,\nMinneapolis, MN, USA, June 2-7, 2019, Demonstra-\ntions, pages 48–53. Association for Computational\nLinguistics.\nEmmanouil Antonios Platanios, Mrinmaya Sachan,\nGraham Neubig, and Tom M. Mitchell. 2018. Con-\ntextual parameter generation for universal neural ma-\nchine translation. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\nProcessing, Brussels, Belgium, October 31 - Novem-\nber 4, 2018 , pages 425–435. Association for Com-\nputational Linguistics.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, WMT 2018,\nOctober 31 - November 1, 2018, pages 186–191. As-\nsociation for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nAlessandro Raganato and J ¨org Tiedemann. 2018. An\nanalysis of encoder representations in transformer-\nbased machine translation. In Proceedings of the\nWorkshop: Analyzing and Interpreting Neural Net-\nworks for NLP , BlackboxNLP@EMNLP 2018, Brus-\nsels, Belgium, November 1, 2018 , pages 287–297.\nAssociation for Computational Linguistics.\nMasoud Jalili Sabet, Philipp Dufter, Franc ¸ois Yvon,\nand Hinrich Sch¨utze. 2020. SimAlign: High quality\nword alignments without parallel training data using\nstatic and contextualized embeddings. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: Findings, EMNLP\n2020, 16-20 November 2020, pages 1627–1643. As-\nsociation for Computational Linguistics.\nSukanta Sen, Kamal Kumar Gupta, Asif Ekbal, and\nPushpak Bhattacharyya. 2019. Multilingual unsu-\npervised NMT using shared encoder and language-\nspeciﬁc decoders. In Proceedings of the 57th Con-\nference of the Association for Computational Lin-\nguistics, ACL 2019, Florence, Italy, July 28- August\n2, 2019, Volume 1: Long Papers, pages 3083–3089.\nAssociation for Computational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving neural machine translation mod-\nels with monolingual data. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics, ACL 2016, August 7-12, 2016,\nBerlin, Germany, Volume 1: Long Papers. The Asso-\nciation for Computer Linguistics.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. MASS: masked sequence to se-\nquence pre-training for language generation. In Pro-\nceedings of the 36th International Conference on\nMachine Learning, ICML 2019, 9-15 June 2019, vol-\nume 97 of Proceedings of Machine Learning Re-\nsearch, pages 5926–5936. PMLR.\nYuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Na-\nman Goyal, Vishrav Chaudhary, Jiatao Gu, and An-\ngela Fan. 2020. Multilingual translation with exten-\nsible multilingual pretraining and ﬁnetuning. CoRR,\nabs/2008.00401.\nYiren Wang, ChengXiang Zhai, and Hany Hassan.\n2020. Multi-task learning for multilingual neural\nmachine translation. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2020, Online, Novem-\nber 16-20, 2020, pages 1022–1034. Association for\nComputational Linguistics.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzm ´an, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet: Ex-\ntracting high quality monolingual datasets from web\ncrawl data. In Proceedings of The 12th Language\nResources and Evaluation Conference, LREC 2020,\nMarseille, France, May 11-16, 2020 , pages 4003–\n4012. European Language Resources Association.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2020. mT5: A mas-\nsively multilingual pre-trained text-to-text trans-\nformer. CoRR, abs/2010.11934.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural\nInformation Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n2019, NeurIPS 2019, December 8-14, 2019, Vancou-\nver, BC, Canada, pages 5754–5764.\nBiao Zhang, Philip Williams, Ivan Titov, and Rico Sen-\nnrich. 2020. Improving massively multilingual neu-\nral machine translation and zero-shot translation. In\nProceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2020,\nOnline, July 5-10, 2020, pages 1628–1639. Associa-\ntion for Computational Linguistics.\nBarret Zoph, Deniz Yuret, Jonathan May, and Kevin\nKnight. 2016. Transfer learning for low-resource\nneural machine translation. In Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2016, Austin, Texas,\nUSA, November 1-4, 2016 , pages 1568–1575. The\nAssociation for Computational Linguistics.\nA Dataset Statistics\nTable 9 lists the statistics of 10 language pairs from\nWMT-10. The monolingual data is back-translated\nas the augmented training data. WMT provides var-\nious resources of training data for each language\npair. We use all data except Wikititles follow-\ning (Wang et al., 2020).\nTable 10 summarizes the number of training,\nvalidation, and test samples for each language from\nOPUS-100. We remove 5 languages without any\nvalidation or test example.\nB Results on OPUS-100\nWe provide the test BLEU of the multilingual base-\nline and XLM-T for all 94 language pairs on OPUS-\n100 test sets. Table 11 reports the scores on X →\nEn test sets. Table 12 is on En →X test sets.\nCode Language #Bitext #Mono Training Valid Test\nFr French 10M 5.0M WMT15 Newstest13 Newstest15\nCs Czech 10M 5.0M WMT19 Newstest16 Newstest18\nDe German 4.6M 5.0M WMT19 Newstest16 Newstest18\nFi Finnish 4.8M 5.0M WMT19 Newstest16 Newstest18\nLv Latvian 1.4M 5.0M WMT17 Newsdev17 Newstest17\nEt Estonian 0.7M 5.0M WMT18 Newsdev18 Newstest18\nRo Romanian 0.5M 5.0M WMT16 Newsdev16 Newstest16\nHi Hindi 0.26M 5.0M WMT14 Newsdev14 Newstest14\nTr Turkish 0.18M 5.0M WMT18 Newstest16 Newstest18\nGu Gujarati 0.08M 5.0M WMT19 Newsdev19 Newstest19\nTable 9: Statistics and sources of the training, validation, and test sets from WMT. The languages are ranked with\nthe size of parallel corpus.\nCode Language Train Valid Test Code Language Train Valid Test\naf Afrikaans 275512 2000 2000 lv Latvian 1000000 2000 2000\nam Amharic 89027 2000 2000 mg Malagasy 590771 2000 2000\nar Arabic 1000000 2000 2000 mk Macedonian 1000000 2000 2000\nas Assamese 138479 2000 2000 ml Malayalam 822746 2000 2000\naz Azerbaijani 262089 2000 2000 mr Marathi 27007 2000 2000\nbe Belarusian 67312 2000 2000 ms Malay 1000000 2000 2000\nbg Bulgarian 1000000 2000 2000 mt Maltese 1000000 2000 2000\nbn Bengali 1000000 2000 2000 my Burmese 24594 2000 2000\nbr Breton 153447 2000 2000 nb Norwegian Bokm ˚al 142906 2000 2000\nbs Bosnian 1000000 2000 2000 ne Nepali 406381 2000 2000\nca Catalan 1000000 2000 2000 nl Dutch 1000000 2000 2000\ncs Czech 1000000 2000 2000 nn Norwegian Nynorsk 486055 2000 2000\ncy Welsh 289521 2000 2000 no Norwegian 1000000 2000 2000\nda Danish 1000000 2000 2000 oc Occitan 35791 2000 2000\nde German 1000000 2000 2000 or Oriya 14273 1317 1318\nel Greek 1000000 2000 2000 pa Panjabi 107296 2000 2000\neo Esperanto 337106 2000 2000 pl Polish 1000000 2000 2000\nes Spanish 1000000 2000 2000 ps Pashto 79127 2000 2000\net Estonian 1000000 2000 2000 pt Portuguese 1000000 2000 2000\neu Basque 1000000 2000 2000 ro Romanian 1000000 2000 2000\nfa Persian 1000000 2000 2000 ru Russian 1000000 2000 2000\nﬁ Finnish 1000000 2000 2000 rw Kinyarwanda 173823 2000 2000\nfr French 1000000 2000 2000 se Northern Sami 35907 2000 2000\nfy Western Frisian 54342 2000 2000 sh Serbo-Croatian 267211 2000 2000\nga Irish 289524 2000 2000 si Sinhala 979109 2000 2000\ngd Gaelic 16316 1605 1606 sk Slovak 1000000 2000 2000\ngl Galician 515344 2000 2000 sl Slovenian 1000000 2000 2000\ngu Gujarati 318306 2000 2000 sq Albanian 1000000 2000 2000\nha Hausa 97983 2000 2000 sr Serbian 1000000 2000 2000\nhe Hebrew 1000000 2000 2000 sv Swedish 1000000 2000 2000\nhi Hindi 534319 2000 2000 ta Tamil 227014 2000 2000\nhr Croatian 1000000 2000 2000 te Telugu 64352 2000 2000\nhu Hungarian 1000000 2000 2000 tg Tajik 193882 2000 2000\nid Indonesian 1000000 2000 2000 th Thai 1000000 2000 2000\nig Igbo 18415 1843 1843 tk Turkmen 13110 1852 1852\nis Icelandic 1000000 2000 2000 tr Turkish 1000000 2000 2000\nit Italian 1000000 2000 2000 tt Tatar 100843 2000 2000\nja Japanese 1000000 2000 2000 ug Uighur 72170 2000 2000\nka Georgian 377306 2000 2000 uk Ukrainian 1000000 2000 2000\nkk Kazakh 79927 2000 2000 ur Urdu 753913 2000 2000\nkm Central Khmer 111483 2000 2000 uz Uzbek 173157 2000 2000\nkn Kannada 14537 917 918 vi Vietnamese 1000000 2000 2000\nko Korean 1000000 2000 2000 wa Walloon 104496 2000 2000\nku Kurdish 144844 2000 2000 xh Xhosa 439671 2000 2000\nky Kyrgyz 27215 2000 2000 yi Yiddish 15010 2000 2000\nli Limburgan 25535 2000 2000 zh Chinese 1000000 2000 2000\nlt Lithuanian 1000000 2000 2000 zu Zulu 38616 2000 2000\nTable 10: Statistics of the training, validation, and test sets from OPUS-100. The languages are ranked in alphabet\norder.\nCode af am ar as az be bg bn br bs\nMultilingual NMT 51.8 23.0 36.0 55.7 27.2 28.4 32.1 21.9 23.3 30.8\nXLM-T 53.2 22.5 37.8 58.3 26.6 28.7 32.5 23.2 23.5 31.0\nCode ca cs cy da de el eo es et eu\nMultilingual NMT 38.0 34.1 48.8 36.3 33.8 32.3 37.9 39.5 35.8 20.1\nXLM-T 38.7 34.8 49.8 37.1 34.9 33.5 38.3 40.9 36.2 20.6\nCode fa ﬁ fr fy ga gd gl gu ha he\nMultilingual NMT 22.9 24.5 33.9 42.5 61.5 75.4 30.6 59.8 24.1 34.4\nXLM-T 23.6 25.3 34.6 40.6 63.3 77.7 31.0 61.6 24.1 36.0\nCode hi hr hu id ig is it ja ka kk\nMultilingual NMT 27.5 31.0 26.7 33.8 53.9 23.4 35.7 14.0 22.4 28.7\nXLM-T 28.4 31.9 28.6 34.6 55.1 24.2 36.1 14.8 22.9 29.1\nCode km kn ko ku ky li lt lv mg mk\nMultilingual NMT 37.5 41.2 15.0 24.8 39.0 36.4 41.9 45.5 28.1 34.0\nXLM-T 37.2 43.6 15.6 26.0 41.6 37.9 43.7 46.3 29.0 35.0\nCode ml mr ms mt my nb ne nl nn no\nMultilingual NMT 18.9 50.7 29.7 62.3 19.5 43.3 46.9 31.3 37.0 25.0\nXLM-T 19.2 52.3 30.0 63.0 20.7 44.6 47.4 32.1 37.8 25.6\nCode oc or pa pl ps pt ro ru rw se\nMultilingual NMT 16.4 33.5 45.5 26.4 38.5 36.8 37.5 33.8 28.3 16.0\nXLM-T 15.3 35.1 46.2 27.7 41.6 37.3 39.0 35.1 28.4 14.8\nCode sh si sk sl sq sr sv ta te tg\nMultilingual NMT 55.2 22.2 38.2 27.7 41.7 30.3 31.9 29.1 43.2 24.6\nXLM-T 56.4 23.5 39.1 28.4 43.1 31.5 32.9 30.1 43.8 24.3\nCode th tk tr tt ug uk ur uz vi wa\nMultilingual NMT 21.1 48.5 24.6 19.9 20.8 27.0 21.5 20.2 25.2 31.2\nXLM-T 21.9 49.1 25.1 20.3 20.7 28.0 21.6 18.7 26.2 33.3\nCode xh yi zh zu\nMultilingual NMT 24.6 27.3 37.8 50.4\nXLM-T 26.5 29.9 39.0 50.5\nTable 11: X →En test BLEU for 94 language pairs in many-to-many setting on the OPUS-100 test sets. The\nlanguages are ranked in alphabet order.\nCode af am ar as az be bg bn br bs\nMultilingual NMT 45.1 18.7 20.0 41.5 28.5 26.2 28.8 11.6 25.0 21.5\nXLM-T 44.6 21.2 20.4 41.9 27.9 26.5 29.8 11.4 25.2 21.9\nCode ca cs cy da de el eo es et eu\nMultilingual NMT 35.2 26.7 42.2 34.8 30.1 26.7 33.6 37.0 30.2 14.1\nXLM-T 35.8 26.6 44.1 35.4 30.7 27.3 34.3 37.4 29.8 14.3\nCode fa ﬁ fr fy ga gd gl gu ha he\nMultilingual NMT 9.6 20.9 32.4 33.1 50.4 27.6 27.3 52.4 47.5 28.2\nXLM-T 9.4 21.2 32.9 34.5 51.1 31.6 27.8 52.5 48.6 28.8\nCode hi hr hu id ig is it ja ka kk\nMultilingual NMT 19.8 24.2 20.3 30.0 45.8 21.1 29.4 12.0 16.6 25.1\nXLM-T 20.9 24.7 20.3 30.3 45.7 20.8 30.5 12.3 17.4 25.0\nCode km kn ko ku ky li lt lv mg mk\nMultilingual NMT 19.6 28.6 6.0 8.0 33.4 32.3 35.4 39.5 22.4 33.3\nXLM-T 20.2 29.4 6.7 7.9 35.1 31.5 36.2 40.1 22.6 33.9\nCode ml mr ms mt my nb ne nl nn no\nMultilingual NMT 5.1 31.8 24.2 47.4 13.0 37.7 42.4 27.2 30.7 28.5\nXLM-T 5.7 33.5 24.5 48.0 11.4 38.6 42.3 27.9 30.6 28.9\nCode oc or pa pl ps pt ro ru rw se\nMultilingual NMT 24.2 34.1 43.6 20.8 41.6 31.6 31.0 28.4 69.4 25.7\nXLM-T 23.3 31.5 42.9 21.3 41.9 32.3 31.3 28.7 68.8 26.2\nCode sh si sk sl sq sr sv ta te tg\nMultilingual NMT 50.9 10.6 29.7 24.2 37.0 20.8 31.0 18.8 32.2 28.8\nXLM-T 51.4 10.5 30.3 25.2 37.4 21.5 31.7 19.4 32.3 28.9\nCode th tk tr tt ug uk ur uz vi wa\nMultilingual NMT 8.7 45.4 16.7 19.6 12.0 15.6 19.6 15.2 21.9 27.5\nXLM-T 9.1 45.1 17.1 20.2 12.4 16.7 19.7 16.3 22.1 29.4\nCode xh yi zh zu\nMultilingual NMT 14.0 27.9 41.1 35.5\nXLM-T 13.6 27.2 41.5 36.3\nTable 12: En →X test BLEU for 94 language pairs in many-to-many setting on the OPUS-100 test sets. The\nlanguages are ranked in alphabet order.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8565818071365356
    },
    {
      "name": "Machine translation",
      "score": 0.792324423789978
    },
    {
      "name": "Transformer",
      "score": 0.7496558427810669
    },
    {
      "name": "Encoder",
      "score": 0.7133496999740601
    },
    {
      "name": "Natural language processing",
      "score": 0.6460518836975098
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5729211568832397
    },
    {
      "name": "AKA",
      "score": 0.5534468293190002
    },
    {
      "name": "Parsing",
      "score": 0.5321385860443115
    },
    {
      "name": "Language model",
      "score": 0.5118646025657654
    },
    {
      "name": "Translation (biology)",
      "score": 0.45149490237236023
    },
    {
      "name": "Code (set theory)",
      "score": 0.4121430814266205
    },
    {
      "name": "Programming language",
      "score": 0.21079960465431213
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Library science",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 23
}