{
  "title": "From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models",
  "url": "https://openalex.org/W4389523691",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2118090440",
      "name": "Dongjun Kang",
      "affiliations": [
        "Sungkyunkwan University"
      ]
    },
    {
      "id": "https://openalex.org/A2112029105",
      "name": "Joon-Suk Park",
      "affiliations": [
        "University of Richmond"
      ]
    },
    {
      "id": "https://openalex.org/A2139737370",
      "name": "Yohan Jo",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A2110118181",
      "name": "Jinyeong Bak",
      "affiliations": [
        "Sungkyunkwan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2131643775",
    "https://openalex.org/W4287891464",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3167791867",
    "https://openalex.org/W2006067350",
    "https://openalex.org/W3197718167",
    "https://openalex.org/W4312091380",
    "https://openalex.org/W2136907519",
    "https://openalex.org/W1986829999",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4389519325",
    "https://openalex.org/W4386566829",
    "https://openalex.org/W4318908710",
    "https://openalex.org/W1975879668",
    "https://openalex.org/W3196731672",
    "https://openalex.org/W1841151565",
    "https://openalex.org/W4292947474",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4389523706",
    "https://openalex.org/W4385573216",
    "https://openalex.org/W3104041537",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W1996881001",
    "https://openalex.org/W4200635688",
    "https://openalex.org/W2112524266",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2046137288",
    "https://openalex.org/W4283020727",
    "https://openalex.org/W2067344387"
  ],
  "abstract": "Being able to predict people’s opinions on issues and behaviors in realistic scenarios can be helpful in various domains, such as politics and marketing. However, conducting large-scale surveys like the European Social Survey to solicit people’s opinions on individual issues can incur prohibitive costs. Leveraging prior research showing influence of core human values on individual decisions and actions, we propose to use value-injected large language models (LLM) to predict opinions and behaviors. To this end, we present Value Injection Method (VIM), a collection of two methods—argument generation and question answering—designed to inject targeted value distributions into LLMs via fine-tuning. We then conduct a series of experiments on four tasks to test the effectiveness of VIM and the possibility of using value-injected LLMs to predict opinions and behaviors of people. We find that LLMs value-injected with variations of VIM substantially outperform the baselines. Also, the results suggest that opinions and behaviors can be better predicted using value-injected LLMs than the baseline approaches.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 15539–15559\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nFrom Values to Opinions: Predicting Human Behaviors and Stances Using\nValue-Injected Large Language Models\nDongjun Kang1 Joonsuk Park2∗\nYohan Jo3∗\nJinYeong Bak1∗\n1Sungkyunkwan University, Suwon, South Korea\n2University of Richmond, V A, USA\n3Seoul National University, Seoul, South Korea\nehdwns2356@skku.edu, park@joonsuk.org,\nyohan.jo@snu.ac.kr, jy.bak@skku.edu\nAbstract\nBeing able to predict people’s opinions on is-\nsues and behaviors in realistic scenarios can\nbe helpful in various domains, such as poli-\ntics and marketing. However, conducting large-\nscale surveys like the European Social Survey\nto solicit people’s opinions on individual issues\ncan incur prohibitive costs. Leveraging prior\nresearch showing influence of core human val-\nues on individual decisions and actions, we\npropose to use value-injected large language\nmodels (LLM) to predict opinions and behav-\niors. To this end, we present Value Injection\nMethod (VIM), a collection of two methods—\nargument generation and question answering—\ndesigned to inject targeted value distributions\ninto LLMs via fine-tuning. We then conduct a\nseries of experiments on four tasks to test the ef-\nfectiveness of VIM and the possibility of using\nvalue-injected LLMs to predict opinions and\nbehaviors of people. We find that LLMs value-\ninjected with variations of VIM substantially\noutperform the baselines. Also, the results sug-\ngest that opinions and behaviors can be better\npredicted using value-injected LLMs than the\nbaseline approaches.1\n1 Introduction\nThe ability to reliably predict people’s opinions\non particular issues or how they would choose to\nbehave in different real-life scenarios can be bene-\nficial to numerous professionals, including politi-\ncians and marketers. To this end, there exist large-\nscale surveys soliciting opinions on various issues,\nsuch as European Social Survey (ESS).2 However,\ncollecting opinions on individual issues in this way\nis laborious and costly.\nLuckily, studies on human values claim that\npeople have a small set of core values, which af-\nfects the daily decisions and actions (Stern et al.,\n*Corresponding authors\n1Code: https://github.com/dongjunKANG/VIM\n2https://www.europeansocialsurvey.org/\n1999; Bardi and Schwartz, 2003). For instance, the\nSchwartz value theory (Schwartz et al., 2012) speci-\nfies ten values—such as security and achievement—\nthat are central to human life. Since these values\nare more manageable to collect from people than\ntheir opinions on every issue of our interest, we\nseek to predict people’s opinions and behaviors\nbased on their core values. More specifically, we\npropose to inject a target value distribution to large\nlanguage models (LLMs) and have them predict\nthe opinions and behaviors of people with similar\nvalue distributions.\nFrom a technical perspective, LLMs are pre-\ntrained on large corpora and thus inherently lack\npersonality (Huang et al., 2022). This is not only\nproblematic for our application, but also for others\nlike chatbots, where LLMs with particular personal-\nities are desired. To this end, researchers have mea-\nsured cultural values embedded in LLMs (Arora\net al., 2023) and investigated methods that simulate\nhuman behaviors (Aher et al., 2023), among others.\nHowever, to the best of our knowledge, there has\nnot been an attempt to inject a full set of human\nvalues into LLMs and use it for predicting opin-\nions and behaviors of people with similar value\ndistributions.\nIn this paper, we propose the Value Injection\nMethod (VIM) for injecting specific value distribu-\ntions into LLMs. VIM consists of argument gen-\neration (AG) and question answering (QA). The\nAG method aims to inject values by training LLMs\nto generate opinions on issues consistent with the\ntargeted value distribution. The QA method, on the\nother hand, trains LLMs to specify how similar\nthey are to a given description of a person, on a\n6-point scale from “Not like me at all” and “Very\nmuch like me.”\nWe first verify the effectiveness of VIM (Sec-\ntion 5). We inject values into LLAMA (Touvron\net al., 2023) using variations of VIM resulting\nin three value-injected LLMs: VILLAMA, VIL-\n15539\nLAMAAG, and VILLAMAQA. Then, we test their\nperformances against prompt-based baselines on\ntwo tasks: value survey and argument generation.\nThe experiment results demonstrate that LLMs\ntrained via VIM outperform the baselines on both\ntasks and that the variation of VIM using both meth-\nods is superior.\nWe then test value-injected LLMs’ ability to pre-\ndict human opinions and behaviors (Section 6). In\nparticular we investigate the following questions:\nCan a value-injected LLM predict the behavior of\na person with the same value distribution in a re-\nalistic situation? and Can a value-injected LLM\npredict the stance of a person with the same value\ndistribution on political, social and other issues?\nThe experiment results show that the answers to\nboth questions are true to a degree. In the behavior\nprediction task, predictions of VILLAMA show\na substantial alignment to the gold standard be-\nhaviors, achieving an average of 0.071 normalized\nmean squared error (NMSE). In the opinion pre-\ndiction task, VILLAMA achieves 0.099 NMSE,\nsignificantly outperforming the baselines ranging\nfrom 0.137 to 0.221.\nOur contributions are threefold:\n• We propose the novel problem of predicting\nhuman behaviors and opinions with specific\nvalues.\n• We present Value Injection Method (VIM), an\neffective method for injecting desired values\ninto LLM.\n• We demonstrate that value-injected LLMs out-\nperform the baselines in predicting the behav-\niors and opinions of people who have similar\nvalue distributions to their target value distri-\nbutions.\n2 Related Work\nThe Schwartz theory of basic values identifies ten\nbasic human values that serve to characterize peo-\nple’s attributes:\n• Achievement (Ach): Personal success\nthrough demonstrating competence according\nto social standards.\n• Benevolence (Ben):Preserving and enhanc-\ning the welfare of those with whom one is in\nfrequent personal contact.\n• Conformity (Con):Restraint of actions, in-\nclinations, and impulses likely to upset or\nharm others and violate social expectations\nor norms.\n• Hedonism (Hed):Pleasure or sensuous grati-\nfication for oneself.\n• Power (Pow):Social status and prestige, con-\ntrol or dominance over people and resources.\n• Security (Sec):Safety, harmony, and stability\nof society and relationships.\n• Self-Direction (SD): Independent thought\nand action–choosing, creating, exploring.\n• Stimulation (Sti):Excitement, novelty, and\nchallenge in life.\n• Tradition (Tra):Respect, commitment, and\nacceptance of the customs and ideas that one’s\nculture or religion provides.\n• Universalism (Uni):Understanding, appreci-\nation, tolerance, and protection for the welfare\nof all people and for nature.\nThe Schwartz value theory is an appropriate\nframework for representing human personality in\nour study. It provides a comprehensive understand-\ning of individuals and groups by considering mul-\ntiple values. For example, research has shown\nthat Chinese shopper tourists make purchases of\nitems aligned with specific values, such as passion\nand jewelry (Choi et al., 2016). Additionally, peo-\nple’s prioritized values play a role in their political\ndecisions, including voting (Sagiv and Schwartz,\n2000; Caprara and Zimbardo, 2004). Furthermore,\nBonetto et al. (2021) explored the relationship\nbetween values and people’s opinions regarding\nmovement restrictions and social distancing mea-\nsures in the context of COVID-19.\nPersonality theories, which seek to comprehend\nhuman behavior and cognition, have been em-\nployed in the realm of Natural Language Process-\ning (NLP) research. Lately, there has been an esca-\nlating interest in exploring the utilization of these\ntheories within generative language models, with\nthe purpose of generating sentences that closely re-\nsemble human-like language. Those studies aimed\nto identify the MBTI type and human value scale\nof LLMs by prompting them to answer question-\nnaires like the MBTI questionnaire or Portrait Val-\nues Questionnaire (PVQ) (Rao et al., 2023; Miotto\net al., 2022). In addition to measuring personal-\nity, there are studies that quantitatively measured\nwhether prompting can induce desired personal-\nity traits (Jiang et al., 2023; Caron and Srivastava,\n2022).\n15540\nFigure 1: Overview of Value Injection Method. It consists of two methods: argument generation and question\nanswering. Both methods utilize the Touché23-ValueEval dataset to create prompts and ground-truth answer outputs.\nThe LLM is trained by minimizing the cross-entropy loss between the LLM’s output given the input prompt and the\ncorresponding ground-truth output.\nResearchers have explored various techniques\nto guide language models in generating text that\nreflects specific personas or styles. For example,\nRubin et al. (2022) conducted a study analyzing\neffective prompts in the in-context learning ap-\nproach, which enables the generation of desired\nsentences without fine-tuning the model. In another\nstudy, Ouyang et al. (2022) demonstrated excep-\ntional performance in tasks such as learning from\nintended instructions and mitigating the generation\nof toxic output by utilizing reinforcement learn-\ning with human feedback. This methodology helps\nalign language models with human intent, thereby\nimproving their ability to produce desired outputs.\nHowever, to the best of our knowledge, there has\nbeen no prior research investigating methods for\ninjecting human values into LLMs.\n3 Value Injection Method (VIM)\nTo inject human values into LLM, we propose\nthe value injection method (VIM) consisting of\nargument generation (AG) and question answering\n(QA). Suppose we inject a target value distribu-\ntion Vt = {vAch\nt ,vBen\nt ,...,v Uni\nt }into a LLM M,\nwhere v∗\nt ranges between 1 and 6 (according to\nPVQ). For this, we use the Touché23-ValueEval\ndataset (Mirzakhmedova et al., 2023) consist-\ning of value-related arguments. Each argument\na= {ca,sa,pa,Va}consists of conclusion, stance,\npremise, and values: the conclusion (ca) represents\na specific topic, the stance (sa) indicates whether\nit is in favor of or against the conclusion, and the\npremise (pa) corresponds to the reasoning behind it.\nEach argument is labeled with values expressed in\nthe premise Va = {vAch\na ,vBen\na ,...,v Uni\na }, where\nv∗\na is 1 if the value appears in the premise and0 oth-\nerwise. Table 12 shows an example of this dataset.\nWe split the data with a ratio of 80:10:10 for train-\ning:validation:test.\nArgument Generation (AG) This method in-\njects Vt into M by fine-tunining M to generate\nstances and premises that reflect Vt for a given\nconclusion. Algorithm 1 outlines the process. At\na high level, we split arguments in the dataset into\ntwo groups. The first group is arguments that are\nlikely to be made by someone who has Vt, and the\nsecond group is arguments that are unlikely to be\nmade by them. To be specific, for each argument\nand its values, we look at the corresponding value\nscores in Vt and take the minimum score. If the min-\nimum score is greater than or equal to a threshold\nγ, then this argument is put into the first group; oth-\nerwise, the second group. The rationale is that the\nlikelihood of an argument being made by a person\nis bounded by their least prioritized value that is\nexpressed in the argument. For the first group of ar-\nguments, the model is trained to generate “I would\nsay [argument]”, whereas for the second group, “I\nwould not say [argument]” (see Table 16 for the\nexact prompts). We use the cross-entropy lossLAG\nwith next word prediction.\nQuestion Answering (QA) In contrast to AG,\nQA prompts LLM M to generate the stance and\npremise in relation to the conclusion. The possible\nstances for each question are based on the six op-\ntions of the PVQ: “Not like me at all”, “Not like\nme”, “A little like me”, “Somewhat like me”, “Like\nme”, and “Very much like me”, each associated\n15541\nAlgorithm 1Argument Generation\nInput\n1: Arguments A= {a1,...,a N }\n2: Target group value distribution Vg = { vAch\ng ,\nvBen\ng , vCon\ng , ... ,vUni\ng }\n3: Large language model M\nTraining\n1: for ∀a∈Ado\n2: Create an empty list La\n3: for ∀va ∈Va do\n4: if vz\na == 1then\n5: Append vz\ng into La\n6: end if\n7: end for\n8: if min(La) ≥γthen\n9: wa = ‘I would say’\n10: else\n11: wa = ‘I would not say’\n12: end if\n13: Make a GT argument with ca, sa, pa, and wa\n14: Make a prompt prma with ca\n15: Generate an argument from M by prma\n16: Update the parameters of M by LAG\n17: end for\nOutput\n1: Trained target group LLM Mg\nwith integers from 1 to 6, respectively.\nAlgorithm 2 outlines the process. We use an\nargument a and the target value distribution Vg\nfor each iteration of training a LLM M. As we will\nsee in Section 4, a target value distribution can take\nreal numbers as value scores. To map these scores\nto the six options in each question (1,2,3,4,5,6),\nif a value score is not a whole number, we rounded\nit down or up to an integer probabilistically based\non its fractional part (e.g., if the score is 5.2, then\nit is rounded to 5 with an 80% chance or to 6 with\na 20% chance). We construct a ground-truth (GT)\nanswer that first states a value z associated with\nthe argument, followed by the final choice la. This\nallows us to update the parameters of the LLM M\nusing the cross-entropy loss LQA by comparing the\nground-truth answer with the answer generated by\nM. Through this process, we can obtain trainedMg\nto generate appropriate answers for value-related\nquestions based on the target value distribution Vg.\nPlease refer to Table 17 for the exact prompt format\nof the QA method.\nAlgorithm 2Question Answering\nInput\n1: Arguments A= {a1,...,a N }\n2: Target group value distribution Vg = { vAch\ng ,\nvBen\ng , vCon\ng , ... ,vUni\ng }\n3: Large language model M\nTraining\n1: for ∀a∈Ado\n2: for ∀va ∈Va do\n3: if vz\na == 1then\n4: la = vq\ng //1 +Bernoulli(vz\ng % 1)\n5: Make a GT answer with z,la\n6: Make a prompt prma with ca,sa,pa\n7: Generate an answer from M by prma\n8: Update the parameters of M by LQA\n9: end if\n10: end for\n11: end for\nOutput\n1: Trained target group LLM Mg\nFigure 2: Example of a target group value distribution.\nScore of 1 indicates not consider the value at all, score\nof 6 correspond to the value being very important. This\ndistribution shows relatively high tradition, simulation,\nand security. The value scores for this distribution are\nas follows: achievement=2.3, benevolence=2.1, confor-\nmity=2.9, hedonism=2.7, power=2.8, security=3.1, self-\ndirection=2.1, stimulation=3.4, tradition=4.2, universal-\nism=2.7.\n4 Experimental Setup\n4.1 Target Value Distributions\nFor thorough evaluation of VIM, we test various\nvalue distributions for injection, while ensuring\nthat those distributions are realistic. To that end, we\nidentified representative value distributions among\nhumans using the European Social Survey (ESS)\n15542\nSection Dataset Prompt Model Output Ground Truth Metric\n§5.1 Portrait Values Questionnaire Table 18 Number Group value distribution NMSE\n§5.2 Touché23-ValueEval Table 19 Premise & Stance - Winning ratio\n§6.1 V ALUENET Table 20 Agree or Disagree Group value distribution NMSE\n§6.2 European Social Survey Table 21 Number Number NMSE\nTable 1: Summary of experiments to demonstrate that VILLAMA has the ability of reflecting the value distribution\nconsistently in various tasks. ‘-’ denotes the absence of ground-truth data, so we conduct a human evaluation where\nannotators determine the more appropriate generated argument between two different trained LLMs.\ndataset. ESS is a large-scale survey conducted ev-\nery two years for individuals in Europe. As part of\nthe survey, participants answer the Portrait Values\nQuestionnaire (PVQ) (Schwartz, 2021), a widely\nused questionnaire for profiling the respondent’s\nvalue distribution according to Schwartz’s theory.\nThe resulting distribution is a 10-dimensional vec-\ntor where each element represents the score of each\nvalue ranging between 1 (not at all) and 6 (very\nlikely).\nTo identify representative value distributions\nfrom ESS, we first computed the value distributions\nof 54,763 people from 28 European countries based\non their responses to PVQ. Next, we clustered the\ndistributions using K-means clustering, where each\ndata point represents each person’s value distribu-\ntion as a 10-dimensional vector. By applying the\nelbow method, we determined that 100 clusters\nare suitable (refer to Appendix A for more details).\nLastly, we took the average of the value distribu-\ntions in each cluster, resulting in 100 representative\nvalue distributions. In addition to them, we also\nincluded 28 value distributions that represent the\n28 countries in ESS, by taking the average of value\ndistributions for each country. Figure 2 shows one\nexample of group value distribution. We train one\nLLM for each target value distribution and report\nthe average score of all LLMs.\n4.2 Models\nValue-injected LLMs ( VILLAMA, VIL-\nLAMAAG, VILLAMAQA) Our value-injected\nLLAMA (VILLAMA) is LLAMA-7B (Touvron\net al., 2023) fine-tuned on value injection tasks\nthrough Low-Rank Adaptation (LoRA) (Hu et al.,\n2022). The total loss function is the combination of\nLAG and LQA. VILLAMAAG and VILLAMAQA\nare variations of VILLAMA trained for an ablation\nstudy. The former is LLAMA trained with the\nargument generation task only, and the latter,\nquestion answering.\nBaselines ( LLAMAShort, LLAMALong,\nChatGPTLong) Modern decoder-based LLMs\nhave shown impressive in-context learning perfor-\nmance (Brown et al., 2020; Mishra et al., 2022).\nWe compare the performance of VILLAMA with\nthree zero-shot prompting baselines that receive\nthe target value distribution in the prompt:\n1. LL AMAShortis the pretrained LLAMA-7B .\nIt is given the target value distribution and the\ntask description in the prompt. Please refer to\nTable 22 for the exact prompt;\n2. LL AMALongis the same as LLAMAShort,\nexcept the prompt now includes the defini-\ntion of each value. Please refer to Table 23 for\nthe exact prompt; and\n3. ChatGPT Longis the same as LLAMALong,\nexcept ChatGPT is used in place of LLAMA.\n4.3 Experiment Overview\nWe compare VILLAMA with baselines to demon-\nstrate its ability in four tasks, as summarized in\nTable 1. For the evaluation of value injection itself,\nwe test:\n• how well its responses to PVQ recovers the\ntarget value distribution (Section 5.1)\n• how well it generates arguments that reflect\nthe target value distribution (Section 5.2).\nFor the evaluation of its ability to predict human\nbehavior and opinions, we test:\n• how well it predicts whether people with the\ntarget distribution would conduct certain be-\nhaviors or not in everyday situations (Sec-\ntion 6.1)\n• how well its responses to questions about spe-\ncific issues (e.g., political and religious topics)\nreflect the stance of people who have the target\ndistribution (Section 6.2).\n15543\nModel NMSE Model NMSE\nLL AMA Short 0.196 VILL AMA AG 0.182\nLL AMA Long 0.189 VILL AMA QA 0.053\nChatGPT Long 0.146 VILL AMA 0.034\nTable 2: NMSE between the model’s value distribution\nand the target value distribution averaged across 128\ntarget value distributions. A lower average NMSE score\nindicates a higher alignment between the model’s value\ndistribution and the target value distribution. The best\nperformance is represented in bold, while the second\nbest performance is represented with underlined.\n5 Experiment 1: Value Injection\nLLMs that have successfully been injected with a\ncertain value distribution should be able to reflect\nthe value distribution consistently in various scenar-\nios and tasks, such as, in a value-profiling survey\nand argumentation.\n5.1 Evaluation 1: Value Survey\nOne straightforward approach to testing the suc-\ncess of value injection is comparing a model’s\nself-reported value distribution with the target\nvalue distribution injected into the model. Since\nPVQ (Schwartz, 2021) is the most widely used\nsurvey for measuring people’s value distribution\n(based on the Schwartz value theory), we prompt\nvalue-injected LLMs to answer the PVQ questions.\nPlease refer to Table 14 for example questions of\nPVQ.\nSetup We created PVQ prompts for this task fol-\nlowing the template shown in Table 18. Using these\nprompts, we instruct LLMs to select one of the six\npossible responses for each survey question these\nresponses indicate the degree of similarity between\nthe respondent and the description in the question,\nfrom ‘Not like me at all’ to ‘Very much like me’.\nOnce finished, we compute the value distribution\nfrom the responses according to the formula speci-\nfied by PVQ.\nWe introduce a metric called Normalized Mean\nSquared Error (NMSE), which represents the dif-\nference between the normalized (between 0 and\n1) predicted value scores ˆYi and the normalized\ntarget value scores Yi. Smaller NMSE indicates a\ncloser alignment between the predicted and target\nvalue scores. The process was repeated for 128 tar-\nget value distributions (obtained in §4.1) and the\naverage is reported.\nResults Table 2 presents the results of the PVQ\nevaluation. VILLAMA generates survey responses\nthat better align with the target value distribution\nthan other baselines. LLAMALong exhibits the\nhighest NMSE value, indicating a lack of align-\nment with the target value distribution. Baselines\nwith longer prompts achieve lower errors, demon-\nstrating the efficacy of adding value definitions in\nthe prompt. However, the performance is still not\nsignificantly better, even for ChatGPT, which is a\nmuch larger model. Example results of this task are\nprovided in Table 24.\nWith regard to the ablation study forVILLAMA,\nusing both the AG and QA methods achieves the\nbest performance, and simply training only with the\nAG method results in the worst performance. When\ntrained using the QA method—which is similar to\nthe PVQ task in format—the performance is better\nthan the AG method only, but still falls behindVIL-\nLAMA. In addition, to verify the effectiveness of\nVIM, we adopt a paired t-test that shows how much\nthe method affects the results. We compared the re-\nsults of LLAMALong and VILLAMA on value sur-\nvey. For the value survey,VILLAMA’s improve-\nment over LLAMALong is statistically significant\n(p < 0.001).\n5.2 Evaluation 2: Argument Generation\nFor the second evaluation, we test LLMs’ ability\nto generate arguments that reflect the target value\ndistribution, since examining opinion is one of the\nways to reveal human values (Bergman, 1998; Hoff-\nman and Slater, 2007), and argument is a means\nto express opinions. Determining whether a given\nargument reflects a target value distribution is dif-\nficult to automate. Thus, we ask human judges to\nmake the call.\nSetup First, we randomly selected 40 from a\ntotal of 128 target distributions. Within each tar-\nget, we sampled two conclusions from the test set\nof the Touché23-ValueEval dataset (see the first\nparagraph of Section 3 for the description of this\ndataset). For each conclusion, we prompted each\nLLM to generate a stance and a premise based on\nthe target value distribution.\nThen, three human annotators were presented\nwith two arguments—conclusion and premise—\ngenerated by two different LLMs with prompting\nand VIM for the same target value distribution, and\nasked to determine which argument better reflects\nthe target value distribution. When unsure, they\n15544\nFigure 3: Human evaluation results showing the per-\ncentage of annotators selected between VILLAMA and\nLLAMALong. Annotators were asked to select which\nof the argument generated using VILLAMA and\nLLAMALong is closer to the arguments generated\nby the target group. Among the methods, VIL-\nLAMA showed the highest win ratio.\nwere allowed to select “I don’t know.” A total of 10\ngraduate students fluent in English served as anno-\ntators after learning the Schwartz value theory. The\ninter-annotator agreement, measured using Fleiss’\nkappa (Fleiss, 1971), among the annotators was\n0.54.\nResults Figure 3 presents the win, lose, and tie\nresults for the variants of VILLAMA against\nLLAMALong. Both VILLAMAAG and VIL-\nLAMAQA exhibit similar win ratios, but VIL-\nLAMAAG demonstrates a higher lose ratio com-\npared to VILLAMAQA. This indicates that VIL-\nLAMAAG generates arguments that reflect the tar-\nget value distribution less effectively than VIL-\nLAMAQA does. Note that VILLAMA achieves\nthe highest win ratio, indicating that when trained\nfor both value injection methods, the target value\ndistribution is injected into the LLM more reliably.\nExample results of this task are provided in Ta-\nble 12.\n6 Experiment 2: Opinion & Behavior\nPredictions with Value-injected LLMs\nGiven value-injected LLMs, we evaluate their abil-\nity to predict human behaviors in everyday scenar-\nios and opinions on various issues based on the\nunderlying value distribution.\n6.1 Behavior Prediction\nIn this section, we investigate the question: Can a\nvalue-injected LLM predict the behavior of a per-\nson with the same value distribution in a realistic\nsituation? Schwartz (2013) examines the relation-\nship between values and behavior in real-world\nsituations.\nSetup V ALUENET (Qiu et al., 2022) is a dataset\nderived from the SOCIAL-CHEM-101 dataset\n(Forbes et al., 2020), which contains various be-\nhavioral patterns observed in everyday life. Each of\nthe 21,374 scenarios in V ALUENET is tagged with\none value from the Schwartz value theory and spec-\nified as having a “Positive”, “Negative”, or “Un-\nrelated” relationship with the given value. Please\nrefer to Table 13 for examples of scenarios from\nV ALUENET.\nWe construct a test scenario set from V AL-\nUENET by randomly selecting a total of 500 sce-\nnarios with 50 scenarios (25 positive and 25 nega-\ntive) for each of the 10 values. A LLM is prompted\nwith a test scenario and asked if it would behave\nthe same way. It should answer either “agree” or\n“disagree”. Please refer to Table 20 for the prompt\ntemplate. The agreement ratio is the percentage\nof cases in which the LLM either agrees in posi-\ntive scenarios or disagrees in negative scenarios,\nacross all test scenarios. We calculated the NMSE\nbetween the re-scaled target value (ranging from 0\nto 1) and the agreement ratio.\nResults Table 3 presents the results of the behav-\nior prediction task. Overall, VILLAMA generates\nanswers that align with the target value distribu-\ntion more effectively than the other baselines. This\nsuggests that VILLAMA can predict human be-\nhavior in everyday life situations more accurately\nbased on the value distribution. However, for a\nfew values, such as Benevolence, Hedonism, and\nTradition, LLAMALong achieved the best perfor-\nmance. Paired t-test results of LLAMALong and\nVILLAMA varied by value; the improvement\nwas statistically significant for Achievement and\nSelf-direction ( p < 0.001), but no significance\nwas found for the other values. Interestingly,\nLLAMAShort and LLAMALong showed a lower\nmean error than ChatGPTLong, even though they\nare smaller models and LLAMAShort has less in-\nformation in the prompt. Example results of this\ntask are provided in Table 26.\n6.2 Opinion Prediction\nIn this section, we tackle the question: Can a value-\ninjected LLM predict the stance of a person with\nthe same value distribution on political, social and\nother issues? In contrast to the behavior prediction\n15545\nModel Ach Ben Con Hed Pow Sec SD Sti Tra Uni Ave.\nLLAMAShort 0.092 0.180 0.082 0.091 0.031 0.138 0.095 0.065 0.086 0.179 0.104\nLLAMALong 0.075 0.118 0.057 0.049 0.030 0.098 0.091 0.050 0.069 0.095 0.073\nChatGPTLong 0.092 0.251 0.177 0.061 0.032 0.187 0.058 0.075 0.179 0.156 0.127\nVILLAMAAG 0.161 0.247 0.106 0.123 0.044 0.206 0.082 0.097 0.140 0.151 0.137\nVILLAMAQA 0.065 0.127 0.054 0.053 0.029 0.101 0.075 0.058 0.084 0.109 0.075\nVILLAMA 0.049 0.125 0.056 0.052 0.035 0.097 0.074 0.049 0.072 0.094 0.071\nTable 3: Results for the behavior prediction task. The difference between the model and the target group’s real-life\nbehaviors was quantified using normalized mean squared error (NMSE), where lower values indicate a better\nprediction of the target group’s behavior. The best performance is represented in bold, while the second best\nperformance is represented with underlined. VILLAMA (ours) outperforms other baselines.\nModel MST PSWB POL UD Ave.\nLLAMAShort 0.197 0.148 0.384 0.153 0.221\nLLAMALong 0.079 0.115 0.281 0.072 0.137\nChatGPTLong 0.222 0.133 0.106 0.216 0.169\nVILLAMAAG 0.106 0.078 0.272 0.238 0.174\nVILLAMAQA 0.067 0.069 0.210 0.052 0.099\nVILLAMA 0.061 0.059 0.139 0.140 0.099\nTable 4: Results for each method for the ESS evaluation.\nThe overall average NMSE for the four chapters. The\nbest performance is indicated in bold, while the second\nbest performance is indicated with underlined. Over-\nall, VILLAMA (ours) achieves the best performance\nin predicting specific issues based on the group value\ndistribution.\ntask targeting everyday life scenarios, this task con-\ncerns various issues, such as political, social, and\nreligious ones.\nSetup In this experiment, we utilized a subset\nof the ESS, excluding the PVQ. ESS consists of\neach respondent’s demographic information, such\nas gender, age, and family relationships, and survey\nquestions in various topics, such as Understanding\nDemocracy, Digital Social Contacts, and Attitudes\nto Climate Change. We first excluded questions in\nESS that are not common across the participating\ncountries. Then, we asked LLMs to answer the\nquestionnaires in the following chapters in ESS:\n• Media and Social Trust (MST):Media in-\nterest, beliefs and relationships with members\nof society, 5 questions.\n• Personal and Social Well-Being (PSWB):\nPersonal emotions and life satisfaction such\nas depression, happiness, and achievement, 39\nquestions.\n• Politics (POL):Government, belief in the\npolitical system, opinions on immigrants, 34\nquestions.\n• Understanding of Democracy (UD):Stance\non various issues in the democratic system, 45\nquestions.\nWe created prompts for this task using the tem-\nplate in Table 21. We evaluated a given LLM’s\nability to predict opinions on specific issues by\ncomparing its responses to the actual responses by\nthe group whose value distribution was targeted.\nNote that ESS questions use diverse response\nscales, including binary responses (0 or 1) and de-\ngrees of agreement (0 to 10). We rescaled response\nscores to the range of 0 to 1 to prevent certain ques-\ntions from having a greater impact on the NMSE.\nResults Table 4 shows the results for the opin-\nion prediction task. Overall, VILLAMA generates\nanswers that align with the target value distribu-\ntion more effectively than the other LLMs; VIL-\nLAMA achieves the best or second-best perfor-\nmance for four chapters, and it also exhibits the\nlowest average. These results suggest that VIL-\nLAMA can predict human opinions on specific\nissues more accurately based on the value distri-\nbution. Also, LLAMAShort exihibits a noticeably\nworse performance than LLAMALong, indicating\nthat including value definitions in the prompt has a\nsignificant impact on the outcome. In the paired t-\ntest results of LLAMALong and VILLAMA, VIL-\nLAMA’s improvement over LLAMALong is sta-\ntistically significant (p < 0.001) in MST, PSWB,\nand POL. In addition, we observe the tendency of\nChatGPTLong to avoid answering questions related\nto opinion prediction. 3 Further analysis of Chat-\nGPT’s tendency to refuse to answer is in Appendix\nE.3. Example results of this task are provided in\nTable 27.\n3The response starts with “I cannot answer this question\nas it goes against the ethical guidelines of OpenAI.”\n15546\n7 Conclusion\nIn this paper, we introduced the Value Injection\nMethod (VIM), which allows for the injection of\nspecific value distributions into existing LLMs\nthrough argument generation and question answer-\ning tasks. To assess the effectiveness of VIM across\nvarious value distributions, we conducted evalua-\ntions on 28 country groups and 100 social groups.\nThe evaluations involved answering value surveys\nand generating arguments based on the given value\ndistribution. Our results demonstrate that VIM out-\nperforms other prompting methods in these evalu-\nations. Additionally, we examined the efficacy of\nvalue injection and its ability to predict human be-\nhavior through behavior prediction and opinion pre-\ndiction tasks. The empirical experiments conducted\non these evaluation tasks confirm the effectiveness\nof VIM in value injection and its superior perfor-\nmance compared to other prompting methods in\npredicting human behaviors.\nLimitations\nFor VILLAMAAG, we set a fixed hyper-parameter\nγ, which serves as the threshold for selecting the\nlikelihood of the answer as three. The chosen num-\nber is intuitive, considering that the score range of\nSchwartz values is from one to six. However, appro-\npriate γvalue may vary depending on the specific\nvalue distribution. While VIM demonstrates supe-\nrior performance compared to other baselines in\nvarious value-related tasks, further improvements\ncould be achieved by exploring the effectiveness of\ndifferent values for γ.\nThe LLM trained by VIM has the ability to gener-\nate personalized answers based on an individual’s\nvalue distribution. However, our exploration has\nbeen limited to group value distributions due to the\nlack of individual-level Schwartz value datasets. In\nthe future, we will collect individual-level Schwartz\nvalue distribution data and examine the distinctions\nbetween the individual and group levels.\nEthics Statement\nVIM has the ability to simulate the behaviors and\nopinions of a group by injecting a specific value\ndistribution into the LLM. However, one ethical\nconcern is the potential misuse of VIM to imitate\nthe stance or behavior of specific individuals with-\nout their explicit consent. Let us assume that if one\npossesses an individual’s Schwartz value distribu-\ntion information, it becomes possible that LLM\nwith VIM can generate sentences that were not ac-\ntually spoken from them. This raises concerns, es-\npecially for celebrities or public figures who share\nextensive personal information, as it may make\nthem more susceptible to vulnerabilities such as\nthe dissemination of fake news through misuse. To\naddress this issue, employing a discriminator that\ncan distinguish between speech generated by an\nLLM trained on values through VIM and authen-\ntic speech of individuals could be considered as a\npreventive measure.\nIn our human evaluation process, we ensure that\nannotators are compensated more than the mini-\nmum wage.\nAcknowledgements\nWe would like to thank the anonymous review-\ners for their helpful questions and comments. This\nproject is partially supported by Microsoft Re-\nsearch Asia. This work was partly supported by In-\nstitute of Information & communications Technol-\nogy Planning & Evaluation (IITP) grant funded by\nthe Korea government (MSIT) (No.2022-0-00680,\nAbductive inference framework using omni-data\nfor understanding complex causal relations & ICT\nCreative Consilience program (IITP-2023-2020-0-\n018)). And this work was partially supported by\nthe New Faculty Startup Fund from Seoul National\nUniversity.\nReferences\nGati Aher, Rosa I. Arriaga, and Adam Tauman Kalai.\n2023. Using large language models to simulate mul-\ntiple humans and replicate human subject studies.\nArnav Arora, Lucie-aimée Kaffee, and Isabelle Augen-\nstein. 2023. Probing pre-trained language models for\ncross-cultural differences in values. In Proceedings\nof the First Workshop on Cross-Cultural Considera-\ntions in NLP (C3NLP), pages 114–130, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nAnat Bardi and Shalom H. Schwartz. 2003. Values and\nbehavior: Strength and structure of relations. Person-\nality and Social Psychology Bulletin, 29(10):1207–\n1220. PMID: 15189583.\nManfred Max Bergman. 1998. A theoretical note on the\ndifferences between attitudes, opinions, and values.\nSwiss Political Science Review, 4(2):81–93.\nEric Bonetto, Guillaume Dezecache, Armelle Nugier,\nMarion Inigo, Jean-Denis Mathias, Sylvie Huet,\nNicolas Pellerin, Maya Corman, Pierre Bertrand, Eric\nRaufaste, Michel Streith, Serge Guimond, Roxane\n15547\nde la Sablonnière, and Michael Dambrun. 2021. Ba-\nsic human values during the covid-19 outbreak, per-\nceived threat and their relationships with compliance\nwith movement restrictions and social distancing.\nPLOS ONE, 16:1–15.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. 2020. Language\nmodels are few-shot learners. In Advances in Neural\nInformation Processing Systems, volume 33, pages\n1877–1901. Curran Associates, Inc.\nGian Vittorio Caprara and Philip G Zimbardo. 2004.\nPersonalizing politics: A congruency model of politi-\ncal preference. American psychologist, 59(7):581.\nGraham Caron and Shashank Srivastava. 2022. Identi-\nfying and manipulating the personality traits of lan-\nguage models. arXiv preprint arXiv:2212.10276.\nMi Ju Choi, Cindy Yoonjoung Heo, and Rob Law. 2016.\nDeveloping a typology of chinese shopping tourists:\nAn application of the schwartz model of universal hu-\nman values. Journal of Travel & Tourism Marketing,\n33(2):141–161.\nJoseph L Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological bulletin,\n76(5):378.\nMaxwell Forbes, Jena D. Hwang, Vered Shwartz,\nMaarten Sap, and Yejin Choi. 2020. Social chem-\nistry 101: Learning to reason about social and moral\nnorms. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 653–670, Online. Association for\nComputational Linguistics.\nLindsay H. Hoffman and Michael D. Slater. 2007. Eval-\nuating public discourse in newspaper opinion articles:\nValues-framing and integrative complexity in sub-\nstance and health policy issues. Journalism & Mass\nCommunication Quarterly, 84(1):58–74.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. 2022. LoRA: Low-rank adaptation of large\nlanguage models. In International Conference on\nLearning Representations.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,\nXuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.\nLarge language models can self-improve. arXiv\npreprint arXiv:2210.11610.\nGuangyuan Jiang, Manjie Xu, Song-Chun Zhu, Wen-\njuan Han, Chi Zhang, and Yixin Zhu. 2023. Evaluat-\ning and inducing personality in pre-trained language\nmodels. arXiv preprint arXiv:2206.07550.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nMarilù Miotto, Nicola Rossberg, and Bennett Klein-\nberg. 2022. Who is gpt-3? an exploration of per-\nsonality, values and demographics. arXiv preprint\narXiv:2209.14338.\nNailia Mirzakhmedova, Johannes Kiesel, Milad Al-\nshomary, Maximilian Heinrich, Nicolas Handke, Xi-\naoni Cai, Barriere Valentin, Doratossadat Dastgheib,\nOmid Ghahroodi, Mohammad Ali Sadraei, et al.\n2023. The touch\\’e23-valueeval dataset for identify-\ning human values behind arguments. arXiv preprint\narXiv:2301.13771.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2022. Cross-task generaliza-\ntion via natural language crowdsourcing instructions.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3470–3487, Dublin, Ireland.\nAssociation for Computational Linguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Gray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems.\nLiang Qiu, Yizhou Zhao, Jinchao Li, Pan Lu, Baolin\nPeng, Jianfeng Gao, and Song-Chun Zhu. 2022. Val-\nuenet: A new dataset for human value driven dialogue\nsystem. Proceedings of the AAAI Conference on Ar-\ntificial Intelligence, 36(10):11183–11191.\nHaocong Rao, Cyril Leung, and Chunyan Miao.\n2023. Can chatgpt assess human personalities?\na general evaluation framework. arXiv preprint\narXiv:2303.01248.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2655–2671, Seattle, United States.\nAssociation for Computational Linguistics.\nLilach Sagiv and Shalom H Schwartz. 2000. Value\npriorities and subjective well-being: Direct relations\nand congruity effects. European journal of social\npsychology, 30(2):177–198.\nShalom Schwartz. 2013. Value priorities and behavior:\nApplying. In The psychology of values: The Ontario\nsymposium, volume 8.\nShalom H Schwartz. 2021. A repository of schwartz\nvalue scales with instructions and an introduction.\nOnline Readings in Psychology and Culture, 2(2):9.\n15548\nShalom H Schwartz et al. 2012. An overview of the\nschwartz theory of basic values. Online readings in\nPsychology and Culture, 2(1):2307–0919.\nPaul C Stern, Thomas Dietz, Troy Abel, Gregory A\nGuagnano, and Linda Kalof. 1999. A value-belief-\nnorm theory of support for social movements: The\ncase of environmentalism. Human ecology review,\npages 81–97.\nRobert Thorndike. 1953. Who belongs in the family?\nPsychometrika, 18(4):267–276.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. arXiv\npreprint arXiv:2302.13971.\nAppendix\nA The number of Clusters\nFigure 4: Elbow method graph. At the point where the\nnumber of clusters reaches 100, it is observable that the\ngraph exhibits a curvature.\nTo determine the appropriate number of clusters,\nwe employ the elbow method (Thorndike, 1953).\nFigure 4 presents the results of this analysis. We\nobserve a curvature in the graph when the number\nof clusters reaches 100, indicating a potential elbow\npoint. So we set the number of social groups to 100.\nB Implementation Details\nWe train LLAMA-7B (Touvron et al., 2023) which\nhas a parameter size of seven billion using Py-\ntorch on an NVIDIA RTX A6000 GPU, with\n48GB dedicated memory. We use AdamW opti-\nmizer (Loshchilov and Hutter, 2019), train 5 epochs\nfor fine-tuning, set batch size as 4, learning rate as\n2e-5. We set the rank of LoRA, using the decom-\nposition matrix to 8 and set the γ of Argument\nGeneration of VIM to 3 which is the middle of the\nrange of the values. In the inference process, we\nset temperature as 1 and top-p as 0.5. We use May\n24, 2023 version of ChatGPT.4\nC Few-shot Results\nFor the opinion prediction (ESS) task among the\nevaluation tasks, we conducted experiments not\nonly for zero-shot but also for additional few-shot\nprompting and few-shot prompting using the Chain\nof Thought (CoT). We experimented with 1, 2, and\n5 examples, as the input context window of the\nLLM limited us from conducting experiments with\na larger number of examples. Few-shot examples\nwere randomly selected from the ESS dataset, and\nthe prompts were carried out in the same manner\nas the zero-shot setting. The results are presented\nin the following table 5 and 6.\nFor value survey (PVQ) task, we conducted ex-\nperiments with the version known for having a\nlarger number of questions and being more accu-\nrate, which consists of 40 questions. However, the\ndataset we have is based on a 21-question version,\nwhich has a lower number of questions and lower\naccuracy. In this case, since we would have had\nto arbitrary assign answers to the remaining ques-\ntions, we were unable to conduct few-shot experi-\nments. The Behavior prediction task is answering\nwith \"agree\" or \"disagree\" regarding whether the\nmodel would engage in the same action as a spe-\ncific value-related scenario. Our evaluation focuses\nnot on individual answers, but on assessing the per-\ncentage of \"agree\" or \"disagree\". Similar to Value\nsurvey task, there is a challenge of assigning arbi-\ntrary answers for few-shot examples, so we were\nunable to conduct few-shot experiments.\nWe found that VILLAMA with VIM applied\nachieved a significantly lower average normalized\nmean squared error (NMSE) of 0.099 compared\nto both few-shot and few-shot CoT settings. In\nthe few-shot experiments, both LLAMALong and\nChatGPTLong showed their best performance in\nthe zero-shot setting. In the few-shot CoT experi-\nments, LLAMALong showed the best performance\nwith 5-shot, while ChatGPTLong performed best\nwith 1-shot.\n4https://help.openai.com/en/articles/\n6825453-chatgpt-release-notes\n15549\nModel Example MST PSWB POL UD Ave.\nLLAMALong\n0-shot 0.079 0.115 0.2810.0720.137\n1-shot 0.154 0.109 0.533 0.250 0.261\n2-shot 0.118 0.117 0.271 0.334 0.210\n5-shot 0.165 0.109 0.2090.103 0.147\nChatGPTLong\n0-shot 0.222 0.133 0.1060.216 0.169\n1-shot 0.211 0.265 0.257 0.237 0.243\n2-shot 0.174 0.275 0.280 0.263 0.248\n5-shot 0.174 0.299 0.284 0.282 0.259\nVILLAMA 0-shot 0.061 0.0590.139 0.1400.099\nTable 5: Results of few-shot experiments for the ESS\nevaluation. The overall average NMSE for the four\nchapters. The best performance is indicated in bold,\nwhile the second best performance is indicated with\nunderlined. Overall, VILLAMA(ours) achieves the best\nperformance in predicting specific issues based on the\ngroup value distribution.\nModel Example MST PSWB POL UD Ave.\nLLAMALong\n0-shot 0.066 0.0400.5730.0560.184\n1-shot 0.122 0.069 0.7690.066 0.257\n2-shot 0.103 0.069 0.432 0.109 0.178\n5-shot 0.195 0.092 0.0840.1610.133\nChatGPTLong\n0-shot 0.611 0.301 0.302 0.198 0.353\n1-shot 0.172 0.222 0.239 0.219 0.214\n2-shot 0.668 0.249 0.238 0.230 0.346\n5-shot 0.186 0.264 0.243 0.239 0.233\nVILLAMA 0-shot 0.061 0.0590.139 0.1400.099\nTable 6: Results of few-shot Chain-of-Thought (CoT)\nexperiments for the ESS evaluation. The overall average\nNMSE for the four chapters. The best performance is\nindicated in bold, while the second best performance\nis indicated with underlined. Overall, VILLAMA(ours)\nachieves the best performance in predicting specific\nissues based on the group value distribution.\nD Temperature & Top-p Adjustment\nTo investigate how temperature and top-p affect the\nNMSE of each of the three evaluation tasks: value\nsurvey, behavior prediction, and opinion prediction,\nwe conduct the experiment in which we adjusted\nboth temperature and top-p.\nTable 7 presents the results of temperature ad-\njustment. We conducted experiments by varying\nthe temperature values to 0.2, 0.4, 0.6, 0.8, and 1.0\nwhile keeping the top-p fixed at 0.5. The lowest\nNMSE was observed at 0.2, which corresponds\nto the lowest temperature for value survey tasks.\nHowever, for behavior and opinion predictions, the\nNMSE is lowest at the highest temperature of 1.0.\nTable 8 presents the results of adjusting the top-\np parameter. We conducted experiments by vary-\ning the top-p values to 0.25, 0.50, and 0.75 while\nkeeping the temperature fixed at 1.0. The best per-\nTemp. Value Survey Behavior Pred. Opinion Pred.\n0.2 0.033 0.072 0.102\n0.4 0.033 0.072 0.102\n0.6 0.034 0.074 0.101\n0.8 0.034 0.071 0.100\n1.0 0.034 0.071 0.099\nTable 7: Results of temperature adjustment: The NMSE\nfor the Value survey, behavior prediction, and opinion\nprediction tasks are calculated for temperatures of 0.2,\n0.4, 0.6, 0.8, and 1.0 when top-p is fixed at 0.5. The best\nperformance is indicated in bold.\nTop-p Value Survey Behavior Pred. Opinion Pred.\n0.25 0.033 0.071 0.100\n0.50 0.034 0.071 0.099\n0.75 0.033 0.073 0.101\nTable 8: Results of top-p adjustment: The NMSE for the\nValue survey, behavior prediction, and opinion predic-\ntion tasks are calculated for top-p of 0.25, 0.50, and 0.75\nwhen temperature is fixed at 1.0. The best performance\nis indicated in bold.\nformance was observed in the value survey and\nbehavior prediction tasks at the lowest top-p value\nof 0.25, while the opinion prediction task achieved\nthe highest performance at a top-p value of 0.50.\nHowever, both results indicated that when temper-\nature and top-p were adjusted, the difference in\nNMSE was less than 0.005, suggesting that these\nadjustments did not have a significant impact on\nthe results.\nE Additional Analyses\nThis section describes the additional analyses con-\nducted throughout the experiment and evaluation\nprocess.\nE.1 Results of Evaluation 2: Argument\nGeneration\nIn Figure 3, which is the result of Evaluation 2:\nArgument Generation, we proceeded to conduct an\nadditional analysis addressing the following ques-\ntions.\nFirst, why does VILLAMAQA worse than base-\nline? This is because of the prompts used for AG\nmethod in VIM are constructed with only two pos-\nsibilities: whether they are “would say the {argu-\nment}” or “would not say the {argument}” to the\ntarget value distribution. This approach can be chal-\nlenging to learn the value distribution properly. On\nthe other hand, in the case of the QA method, it is\n15550\n\"Assisted suicide should be a criminal offense\"\nModel Argument (Generated)\nLLAMALong I agree. I think that it is not a good idea to kill someone.\nVILLAMAQA I agree. Assisted suicide should be a criminal offence because it is a form of murder.\nVILLAMA I agree. Assisted suicide should be a criminal offence because it is against the law to take another person’s life.\nTable 9: Argument generated by LLAMALong, VILLAMAQA, and VILLAMA for topic “Assisted suicide should\nbe a criminal offense”. The value scores of the target value distribution used in the generation are as follows:\nachievement=2.3, benevolence=2.1, conformity=2.9, hedonism=2.7, power=2.8, security=3.1, self-direction=2.1,\nstimulation=3.4, tradition=4.2, universalism=2.7.\n\"We should legalize sex selection\"\nModel Argument (Generated)\nLLAMALong I agree. It is a good way to prevent the gender imbalance.\nVILLAMAQA I agree. It is a natural process and it is not harmful to anyone.\nVILLAMA I agree. Sex selection is a natural right of every individual.\nTable 10: Argument generated by LLAMALong, VILLAMAQA, and VILLAMA for topic “We should legalize sex\nselection”. The value scores of the target value distribution used in the generation are as follows: achievement=4.2,\nbenevolence=1.5, conformity=3.9, hedonism=2.2, power=5.0, security=1.7, self-direction=2.0, stimulation=3.9,\ntradition=1.8, universalism=1.6.\nlearned with six appropriate answers corresponding\nto different values. As a result, it can be considered\nthat it learns the value distribution relatively better.\nSecond, why does VILLAMA have a higher\nlose ratio compared to VILLAMAQA? VIL-\nLAMAQA has a relatively high tie ratio. This is\nbecause the VILLAMAQA generates arguments\nsimilar to the baseline. These are two argument\ngeneration examples for the topic \"Assisted sui-\ncide should be a criminal offense\" and \"We should\nlegalize sex selection\" using LLAMALong, VIL-\nLAMAQA, and VILLAMA.\nTable 9 shows the generated arguments of\nthe target value distribution. \"Tradition\" score\nis the highest at 4.2. The arguments generated\nby the LLAMALong and VILLAMAQA have a\nlooser connection with tradition and can be in-\nterpreted as aligning with other values such as\nbenevolence or universalism. When comparing\nthe LLAMALong and VILLAMAQA in the hu-\nman evaluation process of selecting arguments for\ngroups with the target distribution, it becomes chal-\nlenging to make decisions. Therefore, when com-\nparing LLAMALong and the VILLAMAQA, due\nto the presence of similar arguments, the tie ratio\ncan increase, leading to a relatively lower lose ra-\ntio as a result. On the other hand, the argument\ngenerated by the full VILLAMA shows a clear re-\nlationship to tradition, such as the basis of “laws\nshould not be violated”. VILLAMA effectively\ncaptures the characteristics of the target value dis-\ntribution, generating arguments closely related to\nspecific values. Due to these instances, the win ratio\nis higher for VILLAMA compared to when using\nonly AG or QA, where it successfully identifies the\ncontext of specific values and generates relevant\narguments.\nIn the case of VILLAMA , there are situations\nwhere it fails to consider other values within the\nvalue distribution when generating arguments asso-\nciated with specific values. Table 10 is an example\nof such a case and the target value distribution.\nFor the topic \"We should legalize sex selection,\"\nVILLAMA generated an argument associated with\nthe value \"Stimulation\" which shows a high score\nof 3.9 within the value distribution. However, the\nLLAMALong also generated an argument related\nto the value \"Power\" which scored 5.0, another\nhigh-scoring value. However, as described above,\nVILLAMAQA often generates relatively similar\narguments to LLAMALong, so this difference is\nsmall, which can be considered to have a low loss\nratio in QA and a relatively large loss ratio in VIL-\nLAMA.\nE.2 Cluster Size and NMSE\nWe examined how each cluster size, which corre-\nsponds to the target value distribution, influences\nPVQ, behavior and opinion prediction tasks. The\nrelationship between the cluster size and the NMSE\nfor each task is illustrated in Figure 5. The NMSE\n15551\nFigure 5: The NMSE results for each cluster in the PVQ, behavior prediction (Valuenet), and opinion prediction\n(ESS) tasks are presented. The cluster sizes range from a minimum of 511 to a maximum of 2616.\nMST PSWB POL UD Ave.\nNMSE 0.222 0.133 0.106 0.216 0.169\nAvoidance (%) 12.2 28.1 29.7 0.6 17.7\nTable 11: The NMSE and avoidance response ratio in\nthe opinion prediction task of ChatGPT.\nis commonly observed to be low in clusters with\nrelatively large sizes, but in clusters with small\nsizes, it is sometimes measured to be high. This\nphenomenon appears to be attributed to the fact that\nlarger groups tend to exhibit a more pronounced\ncommon value distribution, lifestyle, or opinion,\nwhile in smaller groups, the influence of a single\nmember becomes more significant.\nE.3 ChatGPT Response Avoidance Ratio in\nOpinion Prediction\nIn the opinion prediction task, we observed that\nChatGPT sometimes responds with ’I can’t an-\nswer the questions because I’m an AI language\nmodel.’ Since NMSE was calculated excluding\nthese responses, we investigated the extent of re-\nsponse avoidance and its impact. Table 11 shows\nthe NMSE and response avoidance ratio of Chat-\nGPT in the opinion prediction task.\nChatGPT exhibited the highest response avoid-\nance ratio in the ’Politics’ of the ESS, at 29.7%,\nand the lowest avoidance ratio in ’Understanding\nDemocracy,’ at 0.6%. These findings confirm that\nhigh avoidance ratios contribute to the observed\nlow NMSE.\nF Dataset Examples\nThis section presents the examples of datasets. We\nuse four datasets in this paper as follows:\n• Touché23-ValueEval - Table 12\n• V ALUENET - Table 13\n• Portrait Values Questionnaire - Table 14\n• European Social Survey - Table 15\nG Prompts\nThis section describes the prompts used to train\nLLMs by VIM. The prompts are as follows:\n• VIM Argument Generation prompt - Table 16\n• VIM Question Answering prompt - Table 17\nPrompts for four different tasks are provided:\n• PVQ task prompt - Table 18\n• Argument Generation task prompt - Table 19\n• V ALUENET task prompt - Table 20\n• ESS task prompt - Table 21\nFurthermore, Table 22 presents the basic prompt\nused to provide the target group with Schwartz\nvalue distribution, while Table 23 shows the in-\ncontext learning prompt, which includes the defini-\ntion of Schwartz values and the value distribution.\nH Experiments Result Examples\nThis section describes the examples of experiment\nresults. The examples are presented as follows:\n• PVQ task results - Table 24\n• Argument Generation task results - Table 25\n• V ALUENET task results - Table 26\n• ESS task results - Table 27\nI Human Evaluation\nThis section presents the human evaluation con-\nducted for the argument generation task. Since\nthere is no ground-truth argument based on the\nvalue distribution, we utilize Google Form for the\nevaluation. A screenshot of the questionnaire can\nbe seen in Figure 6.\n15552\nConclusion Stance Premise Value\nWe should abandon the\nuse of school uniform\nIn favor of School uniforms are too expensive and many parents can not\nafford the extra expense.\nSecurity\nWe should abolish the\nOlympic Games\nAgainst Olympic Games should not be banned. They are the tissue that\nsolidify the friendship between nations in the world and a re-\nminder of a peaceful world which should be the world vision.\nBenevolence,\nUniversalism\nWe should adopt gender-\nneutral language\nIn favor of We should adopt gender-neutral language because it avoids of-\nfending people with gender stereo-types.\nConformity, Uni-\nversalism\nTable 12: Examples of Touché23-ValueEval Dataset. Conclusion is specific topic, stance refers to the agreement or\ndisagreement with the conclusion. The premise represents the reasons for the stance on the conclusion, and value\nencompasses all the values expressed in the premise.\nValue Stance Scenario\nAchievement Negative I am the original loser.\nTradition Positive Not wanting my kids to eat candy for breakfast\nUniversalism Negative I hate seeing posts about people who have their lives together because I don’t\nTable 13: Examples of V ALUENET Dataset. Value represents one of the Schwartz values, while stance indicates\nwhether the scenario is positive or negative to the value. The scenario depicts everyday actions associated with the\nvalue.\nNo Items\n1 Thinking up new ideas and being creative is important to him. He likes to do things in his own original way.\n2 He seeks every chance he can to have fun. It is important to him to do things that give him pleasure.\n3 He strongly believes that people should care for nature. Looking after the environment is important to him.\nTable 14: Examples of Portrait Values Questionnaire Item. The items depict individuals involved in actions associated\nwith the respective value.\nNo Items\n1 Please answer 0 to 10 where 0 is not at all important for democracy in general and 10 is extremely important for democracy\nin general.\n2 Is your group discriminated against by other grounds? Please answer \"Yes\" or \"No\".\n3 What extent do you think those who hold extreme political views in your country today are prevented from expressing\nthem openly? Please answer 0 to 10 where 0 is not at all and 10 is completely.\nTable 15: Examples of European Social Survey Item. The item is comprised of inquiries pertaining to political and\nsocial issues.\nVIM Prompt\nArgument Tell me what you would say about the following statement.\nGeneration Statement: {conclusion}\nAnswer:\nAnswer type 1\nI would say, \"I {stance} with that because {premise}.\" about {conclusion}\nAnswer type 2\nI would not say, \"I {stance} with that because {premise}.\" about {conclusion}\nTable 16: VIM: Argument Generation Prompt. { } represents the placeholder. {conclusion}, {stance} and {premise}\nin the placeholder are element of the Touché23-ValueEval dataset. This prompt pertains to determining what answer\nto generate for the conclusion. Answer type 1 signifies that the target group would express the argument, while\nanswer type 2 indicates that the target group would not endorse or articulate the same argument. The choice between\ngenerating Answer type 1 or Answer type 2 is based on wa in Algorithm 1.\n15553\nVIM Prompt\nQuestion Indicate for the following statement whether it is\nAnswering 1. ’Not like me at all’,\n2. ’Not like me’,\n3. ’A little like me’,\n4. ’Somewhat like me’,\n5. ’Like me’,\n6. ’Very much like me’\nas a description of you.\nStatement: I {stance} with {conclusion}. {premise}.\nAnswer:\nAnswer\nBecause I think {value} is {EXPRESSION}, my answer is {score}.\nTable 17: VIM: Question Answering Prompt. { } represents the placeholder. {conclusion}, {stance} and {premise}\nin the placeholder are element of the Touché23-ValueEval dataset. Examples of Touché23-ValueEval dataset are\ntable 12. Value is one of the Schwartz values, and EXPRESSION is a textual representation that aligns with a\nnumeric scale ranging from 1 to 6. (1: not important to me at all, 2: not important to me, 3: a little important to me,\n4: somewhat important to me, 5: important to me, 6: very much important to me). Prompt inquires about the degree\nof similarity between the provided statement and one’s own self, while the answer denotes the appropriate response\nbased on a particular value.\nTask Prompt\nPVQ Indicate for the following statement whether it is\n1. ’Not like me at all’,\n2. ’Not like me’,\n3. ’A little like me’,\n4. ’Somewhat like me’,\n5. ’Like me’,\n6. ’Very much like me’\nas a description of you.\nStatement: {PVQ item}.\nAnswer:\nTable 18: Task 1 - Portrait Values Questionnaire Prompt. { } represents the placeholder. {PVQ item} in the\nplaceholder are element of the PVQ. Prompt asks how similar the given statement is to oneself, while the answer is\none of the options in prompt. Examples of PVQ item are table 14 and generation results are table 24.\nTask Prompt 1 - stance\nArgument Do you agree or disagree with {conclusion}?\nGeneration Answer:\nPrompt 2 - premise\nExplain in detail why you {stance} with {conclusion}.\nAnswer:\nTable 19: Task 2 - Argument Generation Prompt. { } represents the placeholder. {conclusion}, {stance} and\n{premise} in the placeholder are element of the Touché23-ValueEval dataset. Prompt 1 serves the purpose of\ngenerating a stance, while Prompt 2 serves the purpose of generating a premise. Examples of Touché23-ValueEval\ndataset are table 12 and generation results are table 25.\nTask Prompt\nV ALUENET Do you agree the behavior or expression in the following scenario similar to you?\nPlease answer \"I agree\" or \"I disagree\".\nScenario: {V ALUENET scenario}.\nAnswer:\nTable 20: Task 3 - V ALUENET Prompt. { } represents the placeholder. {V ALUENET scenario} is the actions of\npeople related to values in everyday life. The purpose of this prompt is to generate responses that agree or disagree\nwith whether one would engage in similar actions or expressions to the given scenario. Examples of V ALUENET\ndataset are table 13 and generation results are table 26.\n15554\nTask Prompt\nESS {ESS item}.\nAnswer:\nTable 21: Task 4 - European Social Survey (ESS) Prompt Example. { } represents the placeholder. {ESS item}\nmeans the questions of the European Social Survey. Prompt asks the ESS item, while the answer is one of the\noptions in prompt or digit. Examples of ESS item are table 15 and generation results are table 27.\nShort Prompt Prompt\nLet’s roleplay.\nValue Score:\n- Achievement: {target Achievement score}\n- Benevolence: {target Benevolence score}\n- Conformity: {target Conformity score}\n- Hedonism: {target Hedonism score}\n- Power: {target Power score}\n- Security: {target Security score}\n- Self-Direction: {target Self-Direction score}\n- Stimulation: {target Stimulation score}\n- Tradition: {target Tradition score}\n- Universalism: {target Universalism score}\nAs this person, please answer the following question.\nYou just have to choose the answer, you don’t have to explain it.\nPlease choose only one option, even if you’re not sure.\n{Task Description}\nTable 22: Short prompt for providing the target value distribution to LLM. { } represents the placeholder, and {target\nvalue score} means corresponding value score of target value distribution. {Task Description} the task description\nencompasses individual prompts designated for each of the four tasks delineated in Section 4, which are utilized as\ninput for the respective tasks.\n15555\nLong Prompt Prompt\nLet’s roleplay.\nI will describe a person who have values between 1 and 6 for each value.\n1 means the value is not important to him at all and 6 means the value is very much important to him.\nValue Definition:\n- Achievement: values personal success through demonstrating competence according to social stan-\ndards\n- Benevolence: values preserving and enhancing the welfare of those with whom one is in frequent\npersonal contact (the ‘in-group’)\n- Conformity: values restraint of actions, inclinations, and impulses likely to upset or harm others and\nviolate social expectations or norms\n- Hedonism: values pleasure or sensuous gratification for oneself\n- Power: values social status and prestige, control or dominance over people and resources\n- Security: values safety, harmony, and stability of society, of relationships, and of self\n- Self-direction: values independent thought and action–choosing, creating, exploring.\n- Stimulation: values excitement, novelty, and challenge in life\n- Tradition: values respect, commitment, and acceptance of the customs and ideas that one’s culture or\nreligion provides\n- Universalism: values understanding, appreciation, tolerance, and protection for the welfare of all\npeople and for nature\nValue Score:\n- Achievement: {target Achievement score}\n- Benevolence: {target Benevolence score}\n- Conformity: {target Conformity score}\n- Hedonism: {target Hedonism score}\n- Power: {target Power score}\n- Security: {target Security score}\n- Self-Direction: {target Self-Direction score}\n- Stimulation: {target Stimulation score}\n- Tradition: {target Tradition score}\n- Universalism: {target Universalism score}\nAs this person, please answer the following question.\nYou just have to choose the answer, you don’t have to explain it.\nPlease choose only one option, even if you’re not sure.\n{Task Description}\nTable 23: Long prompt for providing the target value distribution to LLM. { } represents the placeholder, and the\nvariables to be inserted into the placeholders are the same as those in table 22. However, by providing additional\ninput in the form of definitions of Schwartz values, we aim to elicit responses from the language model that are\nmore similar to those of the target group, leveraging the enriched information.\nPVQ Item Answer (Generated)\nVILLAMA\nThinking up new ideas and being creative is important to him. He likes to do things in his own original way. 3\nHe seeks every chance he can to have fun. It is important to him to do things that give him pleasure. 4\nHe thinks it is best to do things in traditional ways. It is important to him to keep up the customs he has learned. 2\nLLAMALong\nThinking up new ideas and being creative is important to him. He likes to do things in his own original way. 5\nHe seeks every chance he can to have fun. It is important to him to do things that give him pleasure. 5\nHe thinks it is best to do things in traditional ways. It is important to him to keep up the customs he has learned. 4\nTable 24: Example results of task 1 - PVQ. The results reflect the responses of VILLAMAand LLAMALong. Score\nis ranging from 1 to 6, to indicate the extent of resemblance between LLM and the items presented.\n15556\nConclusion Premise (Genenrated) Stance (Genenrated)\nVILLAMA\nWe should abandon the use of school\nuniform\nSchool uniforms are a good way to make students feel equal Against\nAssisted suicide should be a criminal\noffence\nAssisted suicide should be a criminal offence because it is\nagainst the law to take another person’s life\nIn favor of\nWe should ban human cloning I think that human cloning is a good thing because it can help\npeople who are in need of organs.\nAgainst\nLLAMALong\nWe should abandon the use of school\nuniform\nIt is not necessary to wear uniform to go to school In favor of\nAssisted suicide should be a criminal\noffence\nI think that it is not a good idea to kill someone In favor of\nWe should ban human cloning I think that human cloning is a very bad thing because it is\nagainst the law.\nIn favor of\nTable 25: Example results of task 2 - Argument Generation. Applied the VIM and prompting methods to the value\ndistribution from figure 2 to generate reasons and stances for the given topics. Two samples of each result example\nusing VIM and the prompting method. VILLAMAgenerates arguments that values custom and law in relation to\ntradition rather than prompting.\nScenario Value Stance Answer (Generated)\nVILLAMA\nI decided to make a budget so that I wouldn’t go broke again Achievement Positive Agree\nAsking someone to respect me Conformity Positive Agree\nGoing on a trip without my best friend, even though we planned it together Hedonism Negative Disagree\nLLAMALong\nI decided to make a budget so that I wouldn’t go broke again Achievement Positive Disagree\nAsking someone to respect me Conformity Positive Agree\nGoing on a trip without my best friend, even though we planned it together Hedonism Negative Agree\nTable 26: Example results of task 3 - V ALUENET. The generated responses aim to determine whether LLM agree\nor disagree with engaging in behaviors similar to scenarios that have a relationship with a specific value and stance.\n15557\nQuestion Answer (Generated)\nVILLAMA\nHow important is it for you to live in a country that is governed democratically? Please tell me on a score\nof 0 to 10 where 0 is not at all important and 10 is extremely important.\n9\nPlease tell me how important you think it is for democracy in general that national elections are free and\nfair? Please answer 0 to 10 where 0 is not at all important for democracy in general and 10 is extremely\nimportant for democracy in general.\n8\nWould you say it is generally bad or good for your country’s economy that people come to live here from\nother countries? Please answer 0 to 10, where 0 means bad for the economy and 10 means good for the\neconomy.\n5\nLLAMALong\nHow important is it for you to live in a country that is governed democratically? Please tell me on a score\nof 0 to 10 where 0 is not at all important and 10 is extremely important.\n5\nPlease tell me how important you think it is for democracy in general that national elections are free and\nfair? Please answer 0 to 10 where 0 is not at all important for democracy in general and 10 is extremely\nimportant for democracy in general.\n7\nWould you say it is generally bad or good for your country’s economy that people come to live here from\nother countries? Please answer 0 to 10, where 0 means bad for the economy and 10 means good for the\neconomy.\n8\nTable 27: Example results of task 4 - ESS. Through each method, LLM generate responses corresponding to the\ngiven question.\n15558\nFigure 6: Example of Human Evaluation Question Form. Annotators solve the questions presented in the question\nbox located at the bottom by examining the value distribution of the group positioned at the top. In regard to the\nconclusion stated in the question, they select the stance and premise among the two options that are most similar to\nthe group possessing the corresponding value distribution.\n15559",
  "topic": "Argument (complex analysis)",
  "concepts": [
    {
      "name": "Argument (complex analysis)",
      "score": 0.7550344467163086
    },
    {
      "name": "Value (mathematics)",
      "score": 0.7348645329475403
    },
    {
      "name": "Baseline (sea)",
      "score": 0.5385000109672546
    },
    {
      "name": "Core (optical fiber)",
      "score": 0.5287387371063232
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4805508255958557
    },
    {
      "name": "Computer science",
      "score": 0.4405990242958069
    },
    {
      "name": "Psychology",
      "score": 0.3524622917175293
    },
    {
      "name": "Machine learning",
      "score": 0.1788429617881775
    },
    {
      "name": "Medicine",
      "score": 0.16014382243156433
    },
    {
      "name": "Political science",
      "score": 0.15871021151542664
    },
    {
      "name": "Geography",
      "score": 0.09124428033828735
    },
    {
      "name": "Law",
      "score": 0.08529868721961975
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Internal medicine",
      "score": 0.0
    },
    {
      "name": "Cartography",
      "score": 0.0
    }
  ]
}