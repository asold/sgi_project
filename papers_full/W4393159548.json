{
  "title": "Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model",
  "url": "https://openalex.org/W4393159548",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2104682643",
      "name": "Mingxin Li",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2167356477",
      "name": "Richong Zhang",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2222388541",
      "name": "Zhijie Nie",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2108210332",
      "name": "Yongyi Mao",
      "affiliations": [
        "University of Ottawa"
      ]
    },
    {
      "id": "https://openalex.org/A2104682643",
      "name": "Mingxin Li",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2167356477",
      "name": "Richong Zhang",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2222388541",
      "name": "Zhijie Nie",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2108210332",
      "name": "Yongyi Mao",
      "affiliations": [
        "University of Ottawa"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2133458109",
    "https://openalex.org/W2126400076",
    "https://openalex.org/W2462305634",
    "https://openalex.org/W6691695795",
    "https://openalex.org/W6682338420",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W4372270660",
    "https://openalex.org/W2790235966",
    "https://openalex.org/W2612953412",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W131533222",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W6683895063",
    "https://openalex.org/W4221159403",
    "https://openalex.org/W6785645804",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2250790822",
    "https://openalex.org/W2949380545",
    "https://openalex.org/W2952186591",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3154863804",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3154229486",
    "https://openalex.org/W6657534600",
    "https://openalex.org/W3027758526",
    "https://openalex.org/W2014902591",
    "https://openalex.org/W4306294927",
    "https://openalex.org/W3164054899",
    "https://openalex.org/W4313908941",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W4206636317",
    "https://openalex.org/W3105816068",
    "https://openalex.org/W4385574084",
    "https://openalex.org/W2028175314",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W2160660844",
    "https://openalex.org/W2152180407",
    "https://openalex.org/W4385573170",
    "https://openalex.org/W2794557536",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385572035",
    "https://openalex.org/W3118062200",
    "https://openalex.org/W2251861449",
    "https://openalex.org/W3175362188"
  ],
  "abstract": "Sentence Representation Learning (SRL) is a fundamental task in Natural Language Processing (NLP), with the Contrastive Learning of Sentence Embeddings (CSE) being the mainstream technique due to its superior performance. An intriguing phenomenon in CSE is the significant performance gap between supervised and unsupervised methods, with their only difference lying in the training data. Previous works attribute this performance gap to differences in two representation properties (alignment and uniformity). However, since alignment and uniformity only measure the results, they fail to answer \"What aspects of the training data contribute to the performance gap?\" and \"How can the performance gap be narrowed?\", In this paper, we conduct empirical experiments to answer these \"What\" and \"How\" questions. We first answer the \"What\" question by thoroughly comparing the behavior of supervised and unsupervised CSE during their respective training processes. From the comparison, we identify the similarity pattern as a key factor to the performance gap, and introduce a metric, called Relative Fitting Difficulty (RFD), to measure the complexity of the similarity pattern. Then, based on the insights gained from the \"What\" question, we tackle the \"How\" question by increasing the pattern complexity of the training data. We achieve this by leveraging the In-Context Learning (ICL) capability of the Large Language Model (LLM) to generate data that simulates complex patterns. By utilizing the hierarchical patterns in the LLM-generated data, we effectively narrow the gap between supervised and unsupervised CSE. We release our codes and appendix at https://github.com/BDBC-KG-NLP/NGCSE.",
  "full_text": "Narrowing the Gap between Supervised and Unsupervised\nSentence Representation Learning with Large Language Model\nMingxin Li1, Richong Zhang1,2*, Zhijie Nie1,3, Yongyi Mao4\n1SKLSDE, School of Computer Science and Engineering, Beihang University, Beijing, China\n2Zhongguancun Laboratory, Beijing, China\n3Shen Yuan Honors College, Beihang University, Beijing, China\n4School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, Canada\nflimx,zhangrc,niezjg@act.buaa.edu.cn, ymao@uottawa.ca\nAbstract\nSentence Representation Learning (SRL) is a fundamental\ntask in Natural Language Processing (NLP), with the Con-\ntrastive Learning of Sentence Embeddings (CSE) being the\nmainstream technique due to its superior performance. An\nintriguing phenomenon in CSE is the signiﬁcant performance\ngap between supervised and unsupervised methods, with their\nonly difference lying in the training data. Previous works at-\ntribute this performance gap to differences in two represen-\ntation properties (alignment and uniformity). However, since\nalignment and uniformity only measure the results, they fail\nto answer “What aspects of the training data contribute to the\nperformance gap?” and “How can the performance gap be\nnarrowed?”. In this paper, we conduct empirical experiments\nto answer these “What ” and “How” questions. We ﬁrst an-\nswer the “What ” question by thoroughly comparing the be-\nhavior of supervised and unsupervised CSE during their re-\nspective training processes. From the comparison, we iden-\ntify the similarity pattern as a key factor to the performance\ngap, and introduce a metric, called Relative Fitting Difﬁculty\n(RFD), to measure the complexity of the similarity pattern.\nThen, based on the insights gained from the “What ” ques-\ntion, we tackle the “How” question by increasing the pattern\ncomplexity of the training data. We achieve this by leveraging\nthe In-Context Learning (ICL) capability of the Large Lan-\nguage Model (LLM) to generate data that simulates complex\npatterns. By utilizing the hierarchical patterns in the LLM-\ngenerated data, we effectively narrow the gap between su-\npervised and unsupervised CSE. We release our codes and\nappendix at https://github.com/BDBC-KG-NLP/NGCSE\n.\nIntroduction\nSentence Representation Learning (SRL) is a crucial task in\nNatural Language Processing (NLP), which learns represen-\ntations (or embeddings) for sentences in the feature space.\nIt is a fundamental task that underpins many NLP applica-\ntions, including Semantic Textual Similarity (STS) (Wang\nand Isola 2020), Information Retrieval (IR) (Cer et al. 2018),\nand text classiﬁcation (Pang and Lee 2004). Contrastive\nlearning of Sentence Embeddings (CSE) has been recently\nintroduced into SRL (Yan et al. 2021; Gao, Yao, and Chen\n*Corresponding author\nCopyright © 2024, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n−0.5 −0.4 −0.3 −0.2 −0.1 0.0 0.1 0.2\nRFDu\n−0.25\n−0.20\n−0.15\n−0.10\n−0.05\n0.00\n0.05\n0.10\nRFDa\n78\n80\n82\n84\nNLI.STS_HT\nNLI.SUP\n(85.5)\n(84.6)Wiki.STS_HT\nNLI.NLI\nNLI.STS\n(84.1)\n(83.0)\n(83.3)\nWiki.NLI\n(81.5)\nWiki.STS\n(81.4)\nWiki.dropout\nWiki.token_cutoff\n(81.2)\n(81.1) NLI.token_cutoff\nWiki.token_shuffle\n(79.7)\n(79.6)\nNLI.token_shuffle\n(80.0)\nNLI.dropout\n(78.1)\nFigure 1: RFD u-RFDa plot of models based on BERT base.\nThe colors of the points and the numbers in brackets rep-\nresent Spearman’s correlation in the evaluation data, i.e.,\nthe validation split of STS Benchmark dataset. RFD is a\nmetric we propose to measure the complexity of the simi-\nlarity pattern in the training data. This metric helps us an-\nswer the “What” question and further leads us to address the\n“How” question. “\u0004” is trained with unsupervised data, “ ”\nis trained with supervised data, and “F” is trained with data\ngenerated from the LLM.\n2021) and has drawn much attention as it signiﬁcantly im-\nproves the performance of sentence embeddings. CSE can\nbe trained in both supervised and unsupervised settings,\nwhere the primary difference is the training data. However,\nwith only this difference, supervised CSE can outperform\nunsupervised CSE on STS tasks by a large margin. Gao,\nYao, and Chen (2021) explain this performance gap by re-\nferring to two properties (alignment and uniformity) from\n(Wang and Isola 2020). Speciﬁcally, compared to the rep-\nresentations trained by the unsupervised method, they ﬁnd\nthat the representations trained by supervised data exhibit\nbetter alignment, uniformity or both (as shown in Figure 2),\nthereby resulting in better performance on STS tasks. This\nexplanation still rests on the ﬁnal results, and cannot explain\nthe mechanism that led to these results. In this paper, we\nfocus on the training data and its relationship with the per-\nformance gap. Speciﬁcally, we pose two questions: “What\naspects of the training data contribute to the performance\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13590\ngap?” and “How can the performance gap be narrowed?”,\nand answer them with empirical experiments in this study.\nTo answer the “What” question, we record the variety of\nalignment and uniformity in the training processes of both\nsupervised and unsupervised CSE, where we identify the\nsimilarity pattern, i.e., how a dataset deﬁnes similar and dis-\nsimilar sentence pairs, as a key factor to the performance\ngap. The more complex the similarity pattern of a training\ndataset, the higher the performance that training with such\na dataset can yield. We also ﬁnd that the complexity of the\nsimilarity pattern (pattern complexity for short) can be mea-\nsured by the relative magnitude of alignment and unifor-\nmity between the training data and the evaluation data. More\nspeciﬁcally, the similarity pattern of the supervised training\ndata is more difﬁcult to ﬁt than that of the evaluation data,\nresulting in higher alignment and uniformity values in the\ntraining data than those in the evaluation data. In contrast,\nthe similarity pattern of the unsupervised training data is\nsimpler to ﬁt, resulting in lower alignment and uniformity\nvalues. Therefore, we deﬁne a metric called Relative Fitting\nDifﬁculty (RFD) to measure the pattern complexity and pro-\nvide the answer to the “What ” question: the increase of the\npattern complexity leads to the performance gap between\nsupervised and unsupervised CSE.\nBased on the insight gained from answering the “ What”\nquestion, we answer the “How” question by introducing\ncomplex similarity patterns into the unsupervised train-\ning data. This is achieved by leveraging the In-Context\nLearning (ICL) capability of the Large Language Model\n(LLM) (Brown et al. 2020) to simulate the similarity pat-\nterns in STS (Agirre et al. 2012) and NLI (Gao, Yao, and\nChen 2021) datasets. Furthermore, we notice the hierarchi-\ncal nature of the STS dataset, where the semantic similar-\nity between two sentences is measured with a score ranging\nfrom 0 to 5, rather than simply classiﬁed as similar or dis-\nsimilar. This ﬁnding motivates us to simulate the hierarchi-\ncal pattern of the STS dataset. And to utilize the hierarchical\npattern, we propose a loss called Hierarchical Triplet (HT)\nloss to ensure that such a pattern can be learned during train-\ning, which helps us further narrow the performance gap.\nBrieﬂy, our main contributions are as follows:\n• We propose a new metric, i.e., Relative Fitting Difﬁculty\n(RFD), to measure the complexity of the similarity pat-\ntern and demonstrate that the higher RFDs on both align-\nment and uniformity correlate with better performance\non STS tasks;\n• We narrow the performance gap on STS tasks between\nsupervised and unsupervised CSE by introducing the\ntraining data with complex similarity patterns, which is\nobtained by the ICL capability of LLMs, and introduce a\nnovel loss function called Hierarchical Triplet (HT) loss\nto utilize the hierarchical patterns effectively;\n• We conduct extensive further experiments to validate our\nﬁndings on RFDs and to verify the effectiveness of our\nproposed methods in narrowing the performance gap.\n−2.4 −2.2 −2.0 −1.8 −1.6 −1.4 −1.2\nuniform\n0.24\n0.29\n0.34\n0.39\n0.44\nalign\n50\n60\n70\n80\nNLI.token_cutoff\nNLI.dropout\nNLI.token_shuffle\nWiki.token_shuffle\nWiki.dropout Wiki.token_cutoff\nNLI.SUP\nFigure 2: Uniformity-alignment plot of models based on\nBERTbase. The colors of the points and the numbers in\nbrackets represent Spearman’s correlation in the validation\nsplit of STS Benchmark dataset. “N” is the pre-trained\nmodel, “\u0004” is trained with unsupervised data, and “ ” is\ntrained with supervised data.\nBackground\nIn this section, we ﬁrst detail the differences in training and\nperformance between supervised and unsupervised CSE to\nhelp readers understand the background better, and then give\nsome notations for the convenience of the later narrative.\nPerformance Gap on the STS Tasks\nContrastive learning of Sentence Embeddings (CSE) (Yan\net al. 2021; Gao, Yao, and Chen 2021) is a prevalent tech-\nnique in SRL for its superior performance. CSE reﬁnes the\nsentence embeddings in two ways: 1) pulling the anchor sen-\ntence si and its semantically similar sentence (or positive\nsentence sp\ni) closer; 2) pushing the anchor sentence si and\nits semantically dissimilar sentence (or negative sentence\nsn\ni) apart. The commonly used contrastive learning loss, In-\nfoNCE (Oord, Li, and Vinyals 2018), can be expressed as\nLC = \u0000log ef(si)>f(sp\ni )=\u001c\nef(si)>f(sp\ni )=\u001c + PN\nj=1 ef(si)>f(sn\ni;j)=\u001c; (1)\nwhere \u001c is a hyper-parameter and fsn\ni;jgN\nj=1 is a set of nega-\ntive sentences corresponding to si.\nThere are two settings of CSE, namely Supervised CSE\n(S-CSE) and Unsupervised CSE (U-CSE). Many improve-\nments have been proposed based on both S-CSE and U-\nCSE (Jiang et al. 2022; Wu et al. 2022), but we focus on the\ntypical paradigm in this study. For U-CSE, the augmented\nview of si is treated as its positive sentence sp\ni, and a ran-\ndomly sampled sentence is treated as its negative sentence\nsn\ni. The common data augmentation methods used to gener-\nate this augmented view include dropout, token shufﬂe, and\ntoken cutoff (Yan et al. 2021). U-CSE is typically trained\nwith a Wikipedia dataset (Gao, Yao, and Chen 2021), con-\nsisting of one million sentences extracted from Wikipedia.\nFor S-CSE, it relies on human-annotated sp\ni and sn\ni, with\nthe supervision signal from the Natural Language Infer-\nence (NLI) task (Conneau et al. 2017). Speciﬁcally, given\na premise, the entailment hypothesis is treated as sp\ni and\nthe contradiction hypothesis is treated as sn\ni. Gao, Yao, and\nChen (2021) collect a widely used NLI dataset (Gao, Yao,\nand Chen 2021) for CSE.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13591\nThe performance of CSE is usually evaluated with the\nSentEval toolkit (Conneau and Kiela 2018), which includes\nthe STS tasks and the transfer tasks. The STS task quan-\ntiﬁes the semantic similarity between two sentences with\na score ranging from 0 to 5 and takes Spearman’s corre-\nlation as the metric for performance. There are seven STS\ndatasets are included for evaluation: STS 2012-2016 (Agirre\net al. 2012, 2013, 2014, 2015, 2016), STS Benchmark (Cer\net al. 2017), SICK Relatedness (Marelli et al. 2014). The\ntransfer tasks evaluate the transfer capability of sentence\nembeddings by performing logistic regression. There are\nalso seven datasets included for the evaluation of transfer\ntask: MR (Pang and Lee 2005), CR (Hu and Liu 2004),\nSUBJ (Pang and Lee 2004), MPQA (Wiebe, Wilson, and\nCardie 2005), SST-2 (Socher et al. 2013), TREC (V oorhees\nand Tice 2000), and MRPC (Dolan and Brockett 2005).\nBoth S-CSE and U-CSE exhibit strong performance in the\ntransfer tasks, but there exists a signiﬁcant performance gap\nin the STS task, even when the sole difference between them\nlies in the training data. Gao, Yao, and Chen (2021) borrow\ntwo properties, alignment and uniformity, from the empirical\nwork of Wang and Isola (2020) to better understand it, and\nthe two properties can be expressed as\nLalign , E\n(s;sp)\u0018ppos\nkf(s) \u0000f(sp)k\u000b\n2 (2)\nLuniform ,log E\n(si;sj)\ni.i.d.\n\u0018pdata\ne\u0000tkf(si)\u0000f(sj)k2\n2 (3)\nwhere \u000band tare two hyper-parameters,ppos is the distribu-\ntion of positive sentence pairs, andpdata is the distribution of\nsentences. These properties measure the quality of the sen-\ntence embeddings. Gao, Yao, and Chen (2021) has shown\nthat the performance gap results from the better alignment\nand uniformity of S-CSE in comparison to U-CSE (as shown\nin Figure 2). This explains the result of the performance gap,\nbut does not shed light on the question of “What aspects of\nthe training data contribute to the performance gap?”. In this\nstudy, we seek to answer this “What” question. Furthermore,\nbased on the insights from the “What” question, we will ex-\nplore “How can the performance gap be narrowed?”.\nNotation\nWe conduct experiments under various training data settings\nto study how the training data affects the performance of\nCSE. To maintain consistency, we organize all settings us-\ning the same naming format: “[Data-Domain].[Similarity-\nPattern]”, where [Data-Domain] represents how the anchor\nsentences are collected, and [Similarity-Pattern] represents\nhow the positive and negative sentences are deﬁned. Two\ndata domains are included: (1) Wiki, consisting of sentences\nfrom Wikipedia (Gao, Yao, and Chen 2021); (2) NLI, con-\nsisting of the premises from the NLI dataset (Gao, Yao, and\nChen 2021). And similarity patterns are divided into three\ntypes, including supervision signals (denoted as SUP), data\naugmentations, and our proposed pattern simulation tech-\nniques, which will be explained in later sections.\nWhat Aspects of the Training Data Contribute\nto the Performance Gap?\n0.2\n0.3\n0.4\n0.5\n0.6\nalign\nNLI.SUP\ntrain\neval\nNLI.dropout Wiki.dropout\n0 1000 2000 3000\nstep\n−3.0\n−2.5\n−2.0\n−1.5uniform\n0 1000 2000 3000\nstep\n0 1000 2000 3000\nstep\nFigure 3: Alignment and uniformity in both the held-out\ntraining data and the evaluation data during the training pro-\ncess. We only plot the results of “NLI.SUP”, “NLI.dropout”,\nand “Wiki.dropout” here for comparison. The results of\n“token\nshufﬂe” and “token cutoff” are similar to those of\n“dropout”, so we plot them in the appendix.\nObservation in the Training Process\nIn this section, we study how the training data affects the\nperformance of CSE. To ensure the results are only cor-\nrelated with training data, we select pre-trained BERT base\nmodel (Devlin et al. 2019) as the backbone and ensure iden-\ntical settings across all training, with details illustrated in\nthe appendix. Then, we record changes in alignment and\nuniformity in both the training and evaluation data during\nthe training process and observe how these changes vary be-\ntween S-CSE and U-CSE. Note that the widely-used training\ndata for S-CSE (“NLI.SUP”) and U-CSE (“Wiki.dropout”,\n“Wiki.token\ncutoff”, “Wiki.token shufﬂe”) differ in both\ntheir data domains and similarity patterns. To study the\nimpact of these two factors separately, we introduce three\nadditional training data settings to U-CSE: “NLI.dropout”,\n“NLI.token\ncutoff”, and “NLI.token shufﬂe”. Part of the re-\nsults are shown in Figure 3.\nWe ﬁrst ﬁx the similarity pattern to investigate the im-\npact of the data domain, e.g., comparing “NLI.dropout” with\n“Wiki.dropout”. The results in Figures 2 and 3 show that the\nperformance of the STS task and the trends in alignment and\nuniformity are similar across different data domains, indicat-\ning that the data domain does not inﬂuence the performance\nsigniﬁcantly. Next, we ﬁx the data domain to investigate the\nimpact of the similarity pattern, e.g., comparing “NLI.SUP”\nwith “NIP.dropout”. From the comparison, we can observe\nthe performance gap in Figure 2, indicating that the similar-\nity pattern is a key factor in such phenomenon. Furthermore,\nFigure 3 shows that the changes in alignment and unifor-\nmity during the training process in S-CSE are quite different\nfrom those in U-CSE. Speciﬁcally, S-CSE exhibits higher\nalignment and uniformity values in the held-out training data\nthan those in the evaluation data. In contrast, U-CSE exhibits\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13592\nlower alignment and uniformity values in the held-out train-\ning data than those in the evaluation data. We argue that this\ndifference in the training process, in fact, reﬂects the dif-\nference in the complexity of the similarity pattern (pattern\ncomplexity for short).\nRelative Fitting Difﬁculty\nThe similarity pattern in S-CSE, which is deﬁned by su-\npervision signals, is far more complex than the similarity\npattern in U-CSE, which is deﬁned by data augmentations.\nMoreover, the similarity pattern in S-CSE training data is\nmore difﬁcult to ﬁt than that of the evaluation data, while\nthe similarity pattern in U-CSE training data is simpler to ﬁt\nthan that of the evaluation data. This difference in the pat-\ntern complexity results in the difference in the training pro-\ncess between S-CSE and U-CSE. Therefore, we introduce a\nmetric called Relative Fitting Difﬁculty (RFD) to act as an\nindicator of the pattern complexity. RFD is deﬁned as the\ndifference in ﬁtting difﬁculty between the held-out training\ndata and the evaluation data, i.e., the relative magnitude of\nalignment and uniformity between the held-out training data\nand the evaluation data during the training process.\nLet the alignment and uniformity of a sentence encoder\nf at time step t in the held-out training data be denoted\nby ah(f;t) and uh(f;t), while those in the evaluation data\nbe denoted by ae(f;t) and ue(f;t). We can then deﬁne the\nRFD for alignment over a set of time steps T = ftigM\ni=1 as\nRFDa(f;T ) = 1\nM\nMX\ni=1\nah(f;ti) \u0000ae(f;ti); (4)\nand the RFD for uniformity as\nRFDu(f;T ) = 1\nM\nMX\ni=1\nuh(f;ti) \u0000ue(f;ti): (5)\nWe calculate the RFD for the six U-CSE settings and one\nS-CSE setting mentioned in the last subsection, and present\nthe results in Figure 1 using “\u0004” and “ ”. By comparing the\nresults within each data domain, we can observe two facts:\n(1) when one setting has both lower RFDa and RFDu val-\nues than another setting, it will have lower performance in\nthe STS task accordingly; (2) When either RFDa or RFDu\nvalue increases, the performance in the STS task tends to im-\nprove. These observations show a tendency that higher RFD\nvalues correspond to better performance. In other words,\ncompared to U-CSE, S-CSE has higher ﬁtting difﬁculty in\nalignment and uniformity, i.e., higher pattern complexity,\nwhich leads to better performance in the STS task. In fact,\nwe conjecture it may be the answer to the what question.\nHowever, this answer is drawn from only seven points in\ntwo data domains. If we need to get this answer more con-\nclusive, we need to experiment under more settings and to\nget more RFD coordinates and their corresponding STS task\nperformance. Therefore, in the next section, we will intro-\nduce some artiﬁcial settings and explore the correlation be-\ntween their RFDs and STS performance to further corrobo-\nrate the conclusions of this section.\nHow Can the Performance Gap Be Narrowed?\nIn this section, we answer the “How” based on the insights\ngained from the “What”. At the same time, we will provide\nadditional validation for our answer to the “What” question.\nPattern Simulation With LLM\nOur answer to the “What ” question reveals the correlation\nbetween the pattern complexity in training data and the STS\ntask performance. Therefore, to narrow the performance\ngap, we propose to increase the RFD of U-CSE by intro-\nducing complex similarity patterns (patterns for short) into\nU-CSE. To realize this, we leverage the In-Context Learn-\ning (ICL) (Brown et al. 2020) capability of LLM to sim-\nulate the patterns in NLI and STS datasets. We adopt the\ngpt-3.5-turbo-0613 as the LLM through the ofﬁcial\nAPI 1 from OpenAI with default parameters.\nFigure 4 illustrates the overall procedure of pattern sim-\nulation and dataset generation. The datasets are generated\nfrom two types of sources: a corpus source and a pattern\nsource. For the corpus source, we consider the two data do-\nmains used in the previous experiments: (1) Wiki, which\nconsists of sentences from Wikipedia, and (2) NLI, which\nconsists of the premises from the NLI dataset. For the pat-\ntern source, we also consider two classes of patterns: (1)\nSTS patterns, which adopt the training split of the STS12\ndataset (Agirre et al. 2012) as the source of patterns, and (2)\nNLI patterns, which adopt the same NLI dataset as previ-\nously as the source patterns.\nWhen generating datasets, we randomly sample 20,000\nsentences from the corpus source and use the LLM to simu-\nlate STS and NLI patterns separately. We refer to these sen-\ntences as source sentences. For every source sentencesi, we\nsubsequently generate a positive sentence sp\ni and a negative\nsentence sn\ni using the following process:\n1. Sampling Pattern Example: To simulate STS patterns,\nWe randomly sample three sentence pairs with STS\nscores above 4 as examples for the pattern of positive\nsentence pairs (referred to as positive examples), and\nthree sentence pairs with STS scores below 1 as examples\nfor the pattern of negative sentence pairs (referred to as\nnegative examples). Similarly, to simulate NLI patterns,\nwe randomly sample three premises and their entailment\nhypotheses as positive examples, and three premises and\ntheir contradiction hypotheses as negative examples;\n2. Generating Positive Sentence sp\ni: To generate sp\ni that\nsimulates STS patterns, we prompt the LLM with posi-\ntive examples to generate a sentence that is semantically\nsimilar to si. And to generate sp\ni that simulate NLI pat-\nterns, we prompt the LLM with positive examples to gen-\nerate a sentence that is an entailment hypothesis to si;\n3. Generating Negative Sentence sn\ni: To generate sn\ni that\nsimulates STS patterns, we prompt the LLM with neg-\native examples to generate a sentence with a distinct\nmeaning compared to sp\ni. And to generate sn\ni that sim-\nulate NLI patterns, we prompt the LLM with negative\nexamples to generate a sentence that contradicts sp\ni.\n1https://openai.com/api\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13593\nThe village is not ad-\nministrated by Gram...\nThe village is not ad-\nministrated by Gram...\nHe was not someone else.He was not someone else.\nThe village has a Gram \nPanchayat in charge...\nThe village has a Gram \nPanchayat in charge...\nHe was someone else.He was someone else.\nThe city \ncouncil \ngoverns... \nThe city \ncouncil \ngoverns... \nSarah was \nthe person.\nSarah was \nthe person.\nThe Gram Pan-\nchayat admin-\nisters the...\nThe Gram Pan-\nchayat admin-\nisters the...\nJon was not \nthe person.\nJon was not \nthe person.\nGram Panchayat \nadministers the \nvillage.\nGram Panchayat \nadministers the \nvillage.\nHowever, Jon was \nnot the per-\nson in question.\nHowever, Jon was \nnot the per-\nson in question.\nThe show originally aired on BBC \nRadio 4, with repeats also \nairing the following week on BBC \nRadio 4 Extra.\nBut he was not Jon.\nThe show originally aired on BBC \nRadio 4, with repeats also \nairing the following week on BBC \nRadio 4 Extra.\nBut he was not Jon.\nLarge Language Model\nThe village has a Gram \nPanchayat in charge...\nHe was someone else. He was not someone else.\nNLI Corpus\nGram Panchayat \nadministers the \nvillage.\nThe Gram Pan-\nchayat admin-\nisters the...\nThe city \ncouncil \ngoverns... \nSarah was \nthe person.\nHT loss\nFeature Space\nsource sentence\npositive sentence\nintermediate sentence\nnegative sentence\nSimulation of NLI Patterns\nInstruc-\ntion Examples with\nSTS Patterns\nPrompt\nExamples with\nNLI Patterns \nrandon negative sentence\nC loss\nSimulation of STS Patterns\nWikipedia Corpus\nThe village is \nadministrated by Gram \nPanchayat.\nHowever, Jon was \nnot the person \nin question.\nJon was not \nthe person.But he was not Jon.\nThe village is not ad-\nministrated by Gram...\nFigure 4: The procedure of pattern simulation and pattern utilization. We simulate the STS and NLI patterns separately with the\nICL capability of the LLM. Then, we adopt a combination of contrastive loss and HT loss proposed by us to utilize the pattern.\nThe prompt consists of three examples randomly sampled from the pattern source and its detail is shown in the appendix.\nThe data generated in these processes is called “LLM-\ngenerated data” for simplicity. We combine the LLM-\ngenerated data with the remaining sentences in the source\ncorpus to form hybrid datasets, resulting in a total of four\nhybrid datasets. We name the training data settings of these\nfour datasets by: “Wiki.STS”, “Wiki.NLI”, “NLI.STS”, and\n“NLI.NLI”. Note that (1) For “Wiki.NLI” and “NLI.NLI”,\nthe supervision signals in the NLI dataset are used; (2) For\n“NLI.STS”, we only utilize the premises, which do not con-\ntain any supervision signals in the NLI dataset.\nThen we employ the four training data settings to perform\nCSE under the same setting as in the Observation section.\nThis allows us to examine whether the introduced complex\npatterns can help narrow the performance gap between S-\nCSE and U-CSE. Additionally, we calculate bothRFDaand\nRFDu for all four settings to further validate our answer to\nthe “What” question. The results are plotted in Figure 1 us-\ning “F”. From the results, it is evident that all the results\ntrained by the hybrid datasets outperform the U-CSE, indi-\ncating success in narrowing the performance gap. Moreover,\nall of these new settings exhibit larger RFD values (RFD a\nor RFDu or both) compared to U-CSE, which indicates that\nwe indeed introduce complex patterns to the training data\nthat lead to an increase in performance. Also, these obser-\nvations can be viewed as evidence to support our answer to\nthe“What” question.\nPattern Utilization With Hierarchical Triplet Loss\nIn the previous subsection, we have managed to narrow\nthe performance gap to some extent. However, there is still\nsomething not being fully utilized, which is the hierarchical\nnature of the STS patterns. Instead of deﬁning the positive\nsentence pair and negative sentence pair, the STS task adopts\na score ranging from 0 to 5 to reﬂect the semantic similarity\nbetween two sentences, which makes the similarity pattern\nin STS dataset hierarchical. To maintain such hierarchical\nnature of STS patterns, we revise our process of pattern sim-\nulation (as shown in Figure 4). Speciﬁcally, we prompt the\nLLM to generate an intermediate sentence sm\ni which con-\ntains the less details compared to the positive sentence sp\ni,\nand we randomly sample three sentences from the source of\nSTS patterns with STS scores between 1 and 4 as examples\nin the prompt. Then, we propose a method to utilize all three\nsentences sp\ni, sm\ni and sn\ni by adopting a sequence of triplet\nlosses. This approach ensures that the hierarchical pattern\ncan be learned by the sentence encoder. We refer to this loss\nas the Hierarchical Triplet (HT) loss, and we provide its for-\nmal deﬁnition below.\nFor a source sentencesiand the three sentences generated\nbased on it sp\ni, sm\ni and sn\ni, the HT loss is deﬁned as\nLHT = 1\n2( max(f(si)>f(sm\ni ) \u0000f(si)>f(sp\ni) +m1;0)+\nmax(f(si)>f(sn\ni) \u0000f(si)>f(sm\ni ) +m2;0));\n(6)\nwhere m1;m2 are two hyper-parameters that control the\nmargin of the triplet loss, and f is the sentence encoder that\nmaps sentences into a hypersphere. The HT loss is combined\nwith the contrastive loss 1 to form the ﬁnal loss:\nL= LC + \fLHT; (7)\nwhere \f is a hyper-parameter controls the weight of LHT.\nNote that LHT is calculated only on the LLM-generated\ndata, which covers 20,000 instances in the hybrid dataset.\nWe perform CSE with this ﬁnal loss on “Wiki.STS”\nand “NLI.STS” under the same setting as in the Ob-\nservation section, These training settings are denoted as\n“Wiki.STS\nHT” and “NLI.STS HT” respectively. For all\nsettings in this section, we set m1 = 5e\u00003, m2 = 1e\u00002\nand \f = 1. Similarly, we calculate the RFD of these set-\ntings and plot the results in Figure 1 using Fpoints. It can\nbe observed that training with LHL increases both RFDa\nand RFDu, and then improves the performance. The rise\nof RFD values can be explained as follows: the common\npattern only determines which sentence pair is similar and\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13594\nMethod STS12 STS13 STS14 STS15 STS16 STS-B SICK-R Avg.\nSupervised methods\nInferSenty 52.86 66.75 62.15 72.77 66.87 68.03 65.65 65.01\nSBERTy 70.97 76.53 73.19 79.09 74.30 77.03 72.91 74.89\nConSERT* 74.07 83.93 77.05 83.66 78.76 81.36 76.77 79.37\nSimCSE* 75.30 84.67 80.19 85.40 80.82 84.25 80.39 81.57\nPromptBERTy 75.48 85.59 80.57 85.99 81.08 84.56 80.52 81.97\nUnsupervised methods\nBERT-whiteningz 57.83 66.90 60.90 75.08 71.31 68.24 63.73 66.28\nConSERT* 64.64 78.49 69.07 79.72 75.95 73.97 67.31 72.74\nSimCSE* 68.40 82.41 74.38 80.91 78.56 76.85 72.23 76.25\nPromptBERTy 71.56 84.58 76.98 84.47 80.60 81.60 69.87 78.54\nInfoCSEz 70.53 84.59 76.40 85.10 81.95 82.00 71.37 78.85\nLM-based methods\nDino§ 72.61 81.92 75.09 80.42 76.26 77.10 70.43 76.26\nCLAIF§ 70.62 81.51 76.29 85.05 81.36 84.34 78.22 79.63\nOur methods\nWiki.STS HT 72.46 \u00060:15 84.88\u00060:40 77.80\u00060:63 83.85\u00060:66 81.11\u00060:44 81.90\u00060:18 76.56\u00060:26 79.79\u00060:23\nNLI.STS HT 72.94 \u00060:19 84.32\u00060:27 77.71\u00060:29 84.20\u00060:40 80.85\u00060:27 82.21\u00060:19 78.04\u00060:35 80.02\u00060:22\nTable 1: Spearman’s correlation on STS Tasks. All models adopt BERTbase as the backbone. y: results from (Jiang et al. 2022),\nz: results from (Wu et al. 2022), §: results from (Cheng et al. 2023), *: results from their original paper. We bold the highest\nresults among all models and underline the highest results among the models that are not supervised.\ndissimilar, while the hierarchical pattern determines which\nsentence pair is more similar and dissimilar than another. In\nother words, the hierarchical pattern extends the ideas of the\ncommon pattern, thereby increasing the pattern complexity\nand raising RFD values.\nThrough the above subsections, we signiﬁcantly narrow\nthe performance gap between S-CSE and U-CSE. We now\nprovide our answer to the “How” question: By utilizing the\nICL capability of LLM, we can simulate the patterns in the\nNLI and STS datasets, thereby introducing complex patterns\nto the unsupervised training dataset. This process narrows\nthe performance gap to some extent. Subsequently, we thor-\noughly exploit the hierarchical patterns in the STS dataset\nwith the HT loss, further narrowing the performance gap.\nFinal Performance\nIn this section, we compare our methods with various well-\nknown and state-of-the-art baselines:\nUnsupervised baselines include a post-processing method,\nBERT-whitening (Su et al. 2021), as well as contrastive\nlearning methods like ConSERT (Yan et al. 2021), Sim-\nCSE (Gao, Yao, and Chen 2021),PromptBERT (Jiang et al.\n2022), and InfoCSE (Wu et al. 2022).\nSupervised baselines include some traditional super-\nvised methods such as InferSent (Conneau et al. 2017),\nSBERT (Reimers and Gurevych 2019), and some of the con-\ntrastive learning methods mentioned above, which can also\nbe utilized in a supervised setting.\nLM-based baselines includes Dino (Schick and Sch ¨utze\n2021), which generates training data with the Pre-trained\nLanguage Model (PLM), and CLAIF (Cheng et al. 2023),\nwhich generates training data with the LLM.\nIn the previous sections, all experiments were conducted\nunder the same settings for a fair comparison. While in\nthis section, we run the “Wiki.STS\nHT” and “NLI.STS HT”\ntraining settings under a group of hyper-parameters and de-\ncide the best combination of hyper-parameters with the eval-\nuation data, i.e., the validation split STS Benchmark dataset.\nThe details are provided in the appendix. We get sentence\nembeddings following Jiang et al. (2022) to achieve bet-\nter and more stable performance. We evaluate our method\nfollowing the standard evaluation protocol mentioned in\nthe Background section and compare our method with\nthe baselines on the STS tasks in Table 1 (with BERT base\nas backbone) and 2 (with RoBERTa base (Liu et al. 2019)\nas backbone). By comparing our method with both su-\npervised and unsupervised baselines, we observe that al-\nthough our method is still inferior to state-of-the-art super-\nvised methods, it outperforms all unsupervised baselines by\na large margin. This indicates that we successfully narrow\nthe performance gap between supervised and unsupervised\nCSE. When compared to data-generation-based methods,\nour method outperforms them by generating only 20,000 in-\nstances, which is signiﬁcantly fewer than them.\nFurther Study\nIn this section, we investigate how each component of our\nmethod impacts the performance, and how the embeddings\ntransfer to the downstream tasks. We conduct the experi-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13595\nMethod STS12 STS13 STS14 STS15 STS16 STS-B SICK-R Avg.\nSRoBERTay 71.54 72.49 70.80 78.74 73.69 77.77 74.46 74.21\nSimCSE* 76.53 85.21 80.95 86.03 82.57 85.83 80.50 82.52\nPromptRoBERTay 76.75 85.93 82.28 86.69 82.80 86.14 80.04 82.95\nRoBERTa-whiteningy 57.83 63.24 57.23 71.36 68.99 61.36 62.91 61.73\nSimCSE* 70.16 81.77 73.24 81.36 80.65 80.22 68.56 76.57\nPromptRoBERTay 73.94 84.74 77.28 84.99 81.74 81.88 69.50 79.15\nDino§ 71.24 81.55 75.67 81.42 78.77 80.10 71.31 77.15\nCLAIF§ 68.33 82.26 77.00 85.18 83.43 85.05 78.02 79.90\nWiki.STS HT 75.68 \u00060:41 84.97\u00060:63 78.08\u00060:69 84.82\u00060:23 83.41\u00060:37 83.79\u00060:25 77.66\u00060:20 81.20\u00060:21\nNLI.STS HT 74.54 \u00060:51 85.10\u00060:42 79.10\u00060:15 85.48\u00060:19 82.93\u00060:28 83.87\u00060:17 78.31\u00060:27 81.33\u00060:09\nTable 2: Spearman’s correlation on STS Tasks. All models adopt RoBERTabase as the backbone. y: results from (Jiang et al.\n2022), §: results from (Cheng et al. 2023), *: results from their original paper. We bold the highest results among all models\nand underline the highest results among the models that are not supervised.\nments on “Wiki.STS HT” with BERTbase as the backbone.\n0 2500 5000 7500 10000 12500 15000 17500 20000\nThe number of LLM-generated data\n78.0\n78.5\n79.0\n79.5\n80.0Avg.\n(a) The average Spearman’s correlation on STS tasks w.r.t the num-\nber of LLM-generated data in the hybrid training dataset.\n0.0 0.1 0.2 0.3\nm1\n78.0\n78.5\n79.0\n79.5Avg.\n0.00 0.01 0.02 0.03 0.04 0.05\nm2\n(b) The average Spearman’s correlation on STS tasks w.r.t the mar-\ngins (m1 and m2) in HT loss.\nFigure 5: The study of hyper-parameters.\nThe Number of LLM-Generated Data\nIn this section, we want to investigate the impact of the num-\nber, which is 20,000 previously, of LLM-generated data on\nperformance. To this end, We run our method under different\nnumbers of LLM-generated data and plot the results in Fig-\nure 5a. From this ﬁgure, we can observe a scaling effect that\nas the number of LLM-generated data increases, the perfor-\nmance of our method tends to improve.\nThe Margins of HT Loss\nThere are two margins in the HT loss, m1 and m2. In pat-\ntern simulation, both sp\ni and sm\ni can be regarded as positive\nsamples to the source sentence. As such, we set a small m1\nto ensure that the distance between f(sp\ni) and f(sm\ni ) is not\nlarge when learning their hierarchical pattern. Conversely,\nsn\ni can be treated as a negative sample to the source sen-\ntence, so we set a large m2 to ensure that the distance be-\ntween f(sm\ni ) and f(sn\ni) is sufﬁciently large when learning\ntheir hierarchical pattern. In this section, we invert these set-\ntings to investigate the impact of a large m1 and small m2\non performance. The results are plotted in Figure 5b, and\nthey conform to our expectation that large m1 and small m2\nwould adversely affect the performance.\nAvg.\nWiki.STS 79.79\u00060:23\nCL.single psitive 78.72\u00060:24\nCL.multiple positive 79.14\u00060:27\nTable 3: The average Spearman’s correlation on STS tasks\nwhen HT loss is replaced with Contrastive Loss (CL).\nHT Loss vs. Contrastive Loss\nThe HT loss is proposed to ensure that the sentence encoder\ncan learn the hierarchical STS pattern. However, training\nwith the HT loss includes more positive samples (i.e., sm\ni )\nthan training without the HT loss. To investigate whether\nthe improvement in performance brought about by the HT\nloss is solely due to the presence of more positive samples,\nwe run our method with only the contrastive loss in two set-\ntings: single positive, where only sp\ni is treated as positive\nsamples, and multiple positive, while both sp\ni and sm\ni are\ntreated as positive samples. The results are shown in Table 3.\n“CL.multiple positive” outperforms “CL.single positive”, in-\ndicating that more positive samples indeed improve perfor-\nmance. However, it still underperforms when compared to\ntraining with the HT loss, suggesting that the HT loss indeed\nbrings more effective signals to the training process.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13596\nModel MR CR SUBJ MPQA SST TREC MRPC Avg.\nInferSenty 81.57 86.54 92.50 90.38 84.18 88.20 75.77 85.59\nSBERTy 83.64 89.43 94.39 89.86 88.96 89.60 76.00 87.41\nSimCSE* 82.69 89.25 94.81 89.59 87.31 88.40 73.51 86.51\nPromptBERTy 83.14 89.38 94.49 89.93 87.37 87.40 76.58 86.90\nAvg. BERT embedy 78.66 86.25 94.37 88.66 84.40 92.80 69.54 84.94\nBERT-[CLS] embedy 78.68 84.85 94.21 88.23 84.13 91.40 71.13 84.66\nSimCSE* 81.18 86.46 94.45 88.88 85.50 89.80 74.43 85.81\nPromptBERTy 80.74 85.49 93.65 89.32 84.95 88.20 76.06 85.49\nInfoCSE* 81.76 86.57 94.90 88.86 87.15 90.60 76.58 86.63\nDino§ 79.96 85.27 93.67 88.87 84.29 88.60 69.62 84.33\nCLAIF§ 81.64 87.98 94.24 89.34 86.16 89.80 77.16 86.62\nWiki.STS HT 82.12 \u00060:63 87.96\u00060:27 94.82\u00060:18 90.10\u00060:13 86.84\u00061:38 89.20\u00061:74 75.88\u00061:81 86.70\u00060:76\nNLI.STS HT 82.36 \u00060:61 88.19\u00060:11 94.62\u00060:15 90.15\u00060:19 87.75\u00060:45 90.60\u00060:60 76.93\u00060:53 87.23\u00060:24\nTable 4: Results on the transfer tasks. All models adopt BERT base as the backbone. We use the names of hybrid datasets to\ndenote our models. y: results from (Jiang et al. 2022), §: results from (Cheng et al. 2023), *: results from their original paper.\nWe bold the highest results among all models and underline the highest results among the models that are not supervised.\nTransfer Tasks\nOur study focuses on improving sentence embeddings in the\nSTS task performance, so it remains a mystery how well the\nlearned sentence embeddings can be applied to the down-\nstream tasks. In this subsection, we evaluate the performance\nof transfer tasks for our methods following the standard eval-\nuation protocol mentioned in the Background section, and\ncompare our methods with the baselines in Table 4. The\nstatistics in the table show that, by improving the STS task\nperformance, our methods can learn sentence embeddings\nthat are suitable for the downstream tasks.\nRelated Work\nSentence Representation Learning (SRL) (Conneau et al.\n2017; Reimers and Gurevych 2019; Li et al. 2020) is a fun-\ndamental task in NLP, aiming to learn representations for\nsentences that maintain semantic information. The super-\nvised (Conneau et al. 2017; Reimers and Gurevych 2019)\nand unsupervised (Li et al. 2020; Su et al. 2021) settings of\nSRL used to diverge a lot, where supervised SRL (Conneau\net al. 2017; Reimers and Gurevych 2019) focused on how to\nutilize NLI datasets and unsupervised SRL (Li et al. 2020;\nSu et al. 2021) focused on how to mitigate the anisotropy\nproblem (Li et al. 2020). With the introduction of contrastive\nlearning into SRL (Yan et al. 2021; Gao, Yao, and Chen\n2021), many recent works (Gao, Yao, and Chen 2021; Jiang\net al. 2022) can be applied to both supervised and unsuper-\nvised SRL, building a bridge between these two settings. Al-\nthough these works boost the performance of SRL under the\ncontrastive learning paradigm, they do not explore the un-\nderlying processes leading to the performance gap between\nsupervised CSE (S-CSE) and unsupervised CSE (U-CSE),\nwhich motivates our study. Our study also provides a method\nto improve U-CSE, which can be related to the studies (Wu\net al. 2022) that speciﬁcally focus on U-CSE.\nOur study utilizes the LLM in SRL, and a recent study\nby (Cheng et al. 2023) employs this approach as well. How-\never, our study differs from theirs in both the method of data\ngeneration and the intention of using the LLM. They gen-\nerate data by predicting masks, while we do so by pattern\nsimulation. They use the LLM to enhance the performance\nof CSE, while we use the LLM to narrow the performance\ngap between supervised and unsupervised CSE, and concur-\nrently validate our ﬁndings about the ﬁtting difﬁculty. There\nis also an early work (Schick and Sch ¨utze 2021) that gener-\nates datasets with Pre-trained Language Models (PLM) in a\nway similar to ours. Though our methods seem similar, their\nintention is to mitigate the need for human-generated data,\nwhich is different from our intention.\nTo the best of our knowledge, we are the ﬁrst to study\nwhat aspects of the training data contribute to the perfor-\nmance gap between supervised and unsupervised CSE.\nConclusion\nIn this study, we investigate the training process of S-CSE\nand U-CSE, where we ﬁnd that the similarity pattern of\ntraining data is a key factor to the STS task performance.\nThen, we deﬁne a new metric called Relative Fitting Difﬁ-\nculty (RFD) to quantify the complexity of the similarity pat-\ntern in the training data, and prove that higher RFD values\ncorrelate with improved performance. Building on this in-\nsight, we successfully narrow the performance gap between\nS-CSE and U-CSE by introducing STS and NLI patterns to\nthe unsupervised data. Moreover, we introduce a Hierarchi-\ncal Triplet (HT) loss to utilize the hierarchical STS patterns,\nfurther narrowing the gap. The fact that we train better sen-\ntence embeddings with hierarchical STS patterns than with\nNLI patterns indicates that a more advanced model may\nbe trained by replacing the long-used NLI dataset with a\ncarefully-crafted hierarchical STS dataset. Such a dataset,\npreviously difﬁcult to create due to the lack of sentences\nwith hierarchical semantic similarities, is now attainable\nthanks to the powerful LLM.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13597\nAcknowledgements\nThis work was supported by the STI 2030-Major Projects\nunder Grant 2022ZD0120200, in part by the National Nat-\nural Science Foundation of China (No. U23B2056), in part\nby the Fundamental Research Funds for the Central Univer-\nsities, and in part by the State Key Laboratory of Software\nDevelopment Environment.\nReferences\nAgirre, E.; Banea, C.; Cardie, C.; Cer, D. M.; Diab, M. T.;\nGonzalez-Agirre, A.; Guo, W.; Lopez-Gazpio, I.; Maritx-\nalar, M.; Mihalcea, R.; Rigau, G.; Uria, L.; and Wiebe, J.\n2015. SemEval-2015 Task 2: Semantic Textual Similarity,\nEnglish, Spanish and Pilot on Interpretability. In Cer, D. M.;\nJurgens, D.; Nakov, P.; and Zesch, T., eds., Proceedings\nof the 9th International Workshop on Semantic Evaluation,\nSemEval@NAACL-HLT 2015, Denver, Colorado, USA, June\n4-5, 2015, 252–263. The Association for Computer Linguis-\ntics.\nAgirre, E.; Banea, C.; Cardie, C.; Cer, D. M.; Diab, M. T.;\nGonzalez-Agirre, A.; Guo, W.; Mihalcea, R.; Rigau, G.; and\nWiebe, J. 2014. SemEval-2014 Task 10: Multilingual Se-\nmantic Textual Similarity. In Nakov, P.; and Zesch, T., eds.,\nProceedings of the 8th International Workshop on Seman-\ntic Evaluation, SemEval@COLING 2014, Dublin, Ireland,\nAugust 23-24, 2014, 81–91. The Association for Computer\nLinguistics.\nAgirre, E.; Banea, C.; Cer, D. M.; Diab, M. T.; Gonzalez-\nAgirre, A.; Mihalcea, R.; Rigau, G.; and Wiebe, J. 2016.\nSemEval-2016 Task 1: Semantic Textual Similarity, Mono-\nlingual and Cross-Lingual Evaluation. In Bethard, S.; Cer,\nD. M.; Carpuat, M.; Jurgens, D.; Nakov, P.; and Zesch,\nT., eds., Proceedings of the 10th International Workshop\non Semantic Evaluation, SemEval@NAACL-HLT 2016, San\nDiego, CA, USA, June 16-17, 2016, 497–511. The Associa-\ntion for Computer Linguistics.\nAgirre, E.; Cer, D. M.; Diab, M. T.; and Gonzalez-Agirre,\nA. 2012. SemEval-2012 Task 6: A Pilot on Semantic Tex-\ntual Similarity. In Agirre, E.; Bos, J.; and Diab, M. T., eds.,\nProceedings of the 6th International Workshop on Seman-\ntic Evaluation, SemEval@NAACL-HLT 2012, Montr ´eal,\nCanada, June 7-8, 2012, 385–393. The Association for\nComputer Linguistics.\nAgirre, E.; Cer, D. M.; Diab, M. T.; Gonzalez-Agirre, A.;\nand Guo, W. 2013. *SEM 2013 shared task: Semantic Tex-\ntual Similarity. In Diab, M. T.; Baldwin, T.; and Baroni, M.,\neds., Proceedings of the Second Joint Conference on Lexi-\ncal and Computational Semantics, *SEM 2013, June 13-14,\n2013, Atlanta, Georgia, USA, 32–43. Association for Com-\nputational Linguistics.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Mod-\nels are Few-Shot Learners. In Larochelle, H.; Ranzato, M.;\nHadsell, R.; Balcan, M.-F.; and Lin, H.-T., eds., Advances\nin Neural Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual.\nCer, D.; Yang, Y .; Kong, S.-y.; Hua, N.; Limtiaco, N.; John,\nR. S.; Constant, N.; Guajardo-Cespedes, M.; Yuan, S.; Tar,\nC.; Sung, Y .-H.; Strope, B.; and Kurzweil, R. 2018. Uni-\nversal Sentence Encoder. CoRR, abs/1803.11175. ArXiv:\n1803.11175.\nCer, D. M.; Diab, M. T.; Agirre, E.; Lopez-Gazpio, I.; and\nSpecia, L. 2017. SemEval-2017 Task 1: Semantic Textual\nSimilarity Multilingual and Crosslingual Focused Evalua-\ntion. In Bethard, S.; Carpuat, M.; Apidianaki, M.; Moham-\nmad, S. M.; Cer, D. M.; and Jurgens, D., eds., Proceedings\nof the 11th International Workshop on Semantic Evalua-\ntion, SemEval@ACL 2017, Vancouver, Canada, August 3-4,\n2017, 1–14. Association for Computational Linguistics.\nCheng, Q.; Yang, X.; Sun, T.; Li, L.; and Qiu, X. 2023.\nImproving Contrastive Learning of Sentence Embeddings\nfrom AI Feedback. In Rogers, A.; Boyd-Graber, J. L.; and\nOkazaki, N., eds., Findings of the Association for Computa-\ntional Linguistics: ACL 2023, Toronto, Canada, July 9-14,\n2023, 11122–11138. Association for Computational Lin-\nguistics.\nConneau, A.; and Kiela, D. 2018. SentEval: An Evalua-\ntion Toolkit for Universal Sentence Representations. In Cal-\nzolari, N.; Choukri, K.; Cieri, C.; Declerck, T.; Goggi, S.;\nHasida, K.; Isahara, H.; Maegaard, B.; Mariani, J.; Mazo,\nH.; Moreno, A.; Odijk, J.; Piperidis, S.; and Tokunaga,\nT., eds., Proceedings of the Eleventh International Confer-\nence on Language Resources and Evaluation, LREC 2018,\nMiyazaki, Japan, May 7-12, 2018. European Language Re-\nsources Association (ELRA).\nConneau, A.; Kiela, D.; Schwenk, H.; Barrault, L.; and Bor-\ndes, A. 2017. Supervised Learning of Universal Sentence\nRepresentations from Natural Language Inference Data. In\nPalmer, M.; Hwa, R.; and Riedel, S., eds., Proceedings of\nthe 2017 Conference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2017, Copenhagen, Denmark,\nSeptember 9-11, 2017, 670–680. Association for Computa-\ntional Linguistics.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Burstein, J.; Doran, C.; and\nSolorio, T., eds., Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, NAACL-\nHLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Vol-\nume 1 (Long and Short Papers), 4171–4186. Association for\nComputational Linguistics.\nDolan, W. B.; and Brockett, C. 2005. Automatically Con-\nstructing a Corpus of Sentential Paraphrases. In Proceed-\nings of the Third International Workshop on Paraphras-\ning, IWP@IJCNLP 2005, Jeju Island, Korea, October 2005,\n2005. Asian Federation of Natural Language Processing.\nGao, T.; Yao, X.; and Chen, D. 2021. SimCSE: Simple\nContrastive Learning of Sentence Embeddings. In Moens,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13598\nM.-F.; Huang, X.; Specia, L.; and Yih, S. W.-t., eds., Pro-\nceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2021, Virtual Event\n/ Punta Cana, Dominican Republic, 7-11 November, 2021 ,\n6894–6910. Association for Computational Linguistics.\nHu, M.; and Liu, B. 2004. Mining and summarizing\ncustomer reviews. In Kim, W.; Kohavi, R.; Gehrke, J.;\nand DuMouchel, W., eds., Proceedings of the Tenth ACM\nSIGKDD International Conference on Knowledge Discov-\nery and Data Mining, Seattle, Washington, USA, August 22-\n25, 2004, 168–177. ACM.\nJiang, T.; Jiao, J.; Huang, S.; Zhang, Z.; Wang, D.; Zhuang,\nF.; Wei, F.; Huang, H.; Deng, D.; and Zhang, Q. 2022.\nPromptBERT: Improving BERT Sentence Embeddings with\nPrompts. In Goldberg, Y .; Kozareva, Z.; and Zhang, Y ., eds.,\nProceedings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2022, Abu Dhabi,\nUnited Arab Emirates, December 7-11, 2022, 8826–8837.\nAssociation for Computational Linguistics.\nLi, B.; Zhou, H.; He, J.; Wang, M.; Yang, Y .; and Li, L.\n2020. On the Sentence Embeddings from Pre-trained Lan-\nguage Models. In Webber, B.; Cohn, T.; He, Y .; and Liu,\nY ., eds.,Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP 2020,\nOnline, November 16-20, 2020, 9119–9130. Association for\nComputational Linguistics.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. RoBERTa: A Robustly Optimized BERT Pretraining\nApproach. CoRR, abs/1907.11692. ArXiv: 1907.11692.\nMarelli, M.; Menini, S.; Baroni, M.; Bentivogli, L.;\nBernardi, R.; and Zamparelli, R. 2014. A SICK cure for the\nevaluation of compositional distributional semantic mod-\nels. In Calzolari, N.; Choukri, K.; Declerck, T.; Loftsson,\nH.; Maegaard, B.; Mariani, J.; Moreno, A.; Odijk, J.; and\nPiperidis, S., eds., Proceedings of the Ninth International\nConference on Language Resources and Evaluation, LREC\n2014, Reykjavik, Iceland, May 26-31, 2014, 216–223. Euro-\npean Language Resources Association (ELRA).\nOord, A. v. d.; Li, Y .; and Vinyals, O. 2018. Representa-\ntion Learning with Contrastive Predictive Coding. CoRR,\nabs/1807.03748. ArXiv: 1807.03748.\nPang, B.; and Lee, L. 2004. A Sentimental Education: Senti-\nment Analysis Using Subjectivity Summarization Based on\nMinimum Cuts. In Scott, D.; Daelemans, W.; and Walker,\nM. A., eds., Proceedings of the 42nd Annual Meeting of\nthe Association for Computational Linguistics, 21-26 July,\n2004, Barcelona, Spain, 271–278. ACL.\nPang, B.; and Lee, L. 2005. Seeing Stars: Exploiting Class\nRelationships for Sentiment Categorization with Respect to\nRating Scales. In Knight, K.; Ng, H. T.; and Oﬂazer, K.,\neds., ACL 2005, 43rd Annual Meeting of the Association for\nComputational Linguistics, Proceedings of the Conference,\n25-30 June 2005, University of Michigan, USA, 115–124.\nThe Association for Computer Linguistics.\nReimers, N.; and Gurevych, I. 2019. Sentence-BERT: Sen-\ntence Embeddings using Siamese BERT-Networks. In Inui,\nK.; Jiang, J.; Ng, V .; and Wan, X., eds., Proceedings of the\n2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Confer-\nence on Natural Language Processing, EMNLP-IJCNLP\n2019, Hong Kong, China, November 3-7, 2019, 3980–3990.\nAssociation for Computational Linguistics.\nSchick, T.; and Sch¨utze, H. 2021. Generating Datasets with\nPretrained Language Models. In Moens, M.-F.; Huang, X.;\nSpecia, L.; and Yih, S. W.-t., eds., Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, EMNLP 2021, Virtual Event / Punta Cana, Do-\nminican Republic, 7-11 November, 2021, 6943–6951. Asso-\nciation for Computational Linguistics.\nSocher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning,\nC. D.; Ng, A. Y .; and Potts, C. 2013. Recursive Deep Mod-\nels for Semantic Compositionality Over a Sentiment Tree-\nbank. In Proceedings of the 2013 Conference on Empirical\nMethods in Natural Language Processing, EMNLP 2013,\n18-21 October 2013, Grand Hyatt Seattle, Seattle, Washing-\nton, USA, A meeting of SIGDAT, a Special Interest Group of\nthe ACL, 1631–1642. ACL.\nSu, J.; Cao, J.; Liu, W.; and Ou, Y . 2021. Whitening Sen-\ntence Representations for Better Semantics and Faster Re-\ntrieval. CoRR, abs/2103.15316. ArXiv: 2103.15316.\nV oorhees, E. M.; and Tice, D. M. 2000. Building a ques-\ntion answering test collection. In Yannakoudakis, E. J.;\nBelkin, N. J.; Ingwersen, P.; and Leong, M.-K., eds., SIGIR\n2000: Proceedings of the 23rd Annual International ACM\nSIGIR Conference on Research and Development in Infor-\nmation Retrieval, July 24-28, 2000, Athens, Greece, 200–\n207. ACM.\nWang, T.; and Isola, P. 2020. Understanding Contrastive\nRepresentation Learning through Alignment and Unifor-\nmity on the Hypersphere. In Proceedings of the 37th In-\nternational Conference on Machine Learning, 9929–9939.\nPMLR. ISSN: 2640-3498.\nWiebe, J.; Wilson, T.; and Cardie, C. 2005. Annotating Ex-\npressions of Opinions and Emotions in Language. Lang.\nResour. Evaluation, 39(2-3): 165–210.\nWu, X.; Gao, C.; Lin, Z.; Han, J.; Wang, Z.; and Hu, S.\n2022. InfoCSE: Information-aggregated Contrastive Learn-\ning of Sentence Embeddings. In Goldberg, Y .; Kozareva, Z.;\nand Zhang, Y ., eds., Findings of the Association for Com-\nputational Linguistics: EMNLP 2022, Abu Dhabi, United\nArab Emirates, December 7-11, 2022, 3060–3070. Associa-\ntion for Computational Linguistics.\nYan, Y .; Li, R.; Wang, S.; Zhang, F.; Wu, W.; and Xu,\nW. 2021. ConSERT: A Contrastive Framework for Self-\nSupervised Sentence Representation Transfer. In Zong, C.;\nXia, F.; Li, W.; and Navigli, R., eds.,Proceedings of the 59th\nAnnual Meeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference on Nat-\nural Language Processing, ACL/IJCNLP 2021, (Volume 1:\nLong Papers), Virtual Event, August 1-6, 2021, 5065–5075.\nAssociation for Computational Linguistics.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n13599",
  "topic": "Sentence",
  "concepts": [
    {
      "name": "Sentence",
      "score": 0.6564107537269592
    },
    {
      "name": "Natural language processing",
      "score": 0.6292879581451416
    },
    {
      "name": "Artificial intelligence",
      "score": 0.620387613773346
    },
    {
      "name": "Computer science",
      "score": 0.586461067199707
    },
    {
      "name": "Representation (politics)",
      "score": 0.5612612962722778
    },
    {
      "name": "Unsupervised learning",
      "score": 0.489907443523407
    },
    {
      "name": "Language model",
      "score": 0.4108509421348572
    },
    {
      "name": "Linguistics",
      "score": 0.361836314201355
    },
    {
      "name": "Machine learning",
      "score": 0.3200666308403015
    },
    {
      "name": "Philosophy",
      "score": 0.058552175760269165
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ]
}