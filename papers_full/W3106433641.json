{
    "title": "An Empirical Study of Pre-trained Transformers for Arabic Information Extraction",
    "url": "https://openalex.org/W3106433641",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2668218462",
            "name": "Wuwei Lan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2097128869",
            "name": "Yang Chen",
            "affiliations": [
                "Georgia Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A1987386386",
            "name": "Wei Xu",
            "affiliations": [
                "Georgia Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2118575463",
            "name": "Alan Ritter",
            "affiliations": [
                "Georgia Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2971016465",
        "https://openalex.org/W2933138175",
        "https://openalex.org/W2970854433",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W2995015695",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W2948902769",
        "https://openalex.org/W2963118869",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2995230342",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2971051558",
        "https://openalex.org/W2123442489",
        "https://openalex.org/W2250710764",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3035547806",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3088592174",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3035008906",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3008110149",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W2252046065",
        "https://openalex.org/W4299579390"
    ],
    "abstract": "Multilingual pre-trained Transformers, such as mBERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020a), have been shown to enable effective cross-lingual zero-shot transfer. However, their performance on Arabic information extraction (IE) tasks is not very well studied. In this paper, we pre-train a customized bilingual BERT, dubbed GigaBERT, that is designed specifically for Arabic NLP and English-to-Arabic zero-shot transfer learning. We study GigaBERT's effectiveness on zero-short transfer across four IE tasks: named entity recognition, part-of-speech tagging, argument role labeling, and relation extraction. Our best model significantly outperforms mBERT, XLM-RoBERTa, and AraBERT (Antoun et al., 2020) in both the supervised and zero-shot transfer settings. We have made our pre-trained models publicly available at: https://github.com/lanwuwei/GigaBERT.",
    "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4727–4734,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n4727\nAn Empirical Study of Pre-trained Transformers for\nArabic Information Extraction\nWuwei Lan1, Yang Chen2, Wei Xu2, Alan Ritter2\n1 Department of Computer Science and Engineering, Ohio State University\n2 School of Interactive Computing, Georgia Institute of Technology\nlan.105@osu.edu {yang.chen, wei.xu, alan.ritter}@cc.gatech.edu\nAbstract\nMultilingual pre-trained Transformers, such\nas mBERT (Devlin et al., 2019) and XLM-\nRoBERTa (Conneau et al., 2020a), have been\nshown to enable the effective cross-lingual\nzero-shot transfer. However, their perfor-\nmance on Arabic information extraction (IE)\ntasks is not very well studied. In this pa-\nper, we pre-train a customized bilingual BERT,\ndubbed GigaBERT, that is designed speciﬁ-\ncally for Arabic NLP and English-to-Arabic\nzero-shot transfer learning. We study Giga-\nBERT’s effectiveness on zero-short transfer\nacross four IE tasks: named entity recognition,\npart-of-speech tagging, argument role labeling,\nand relation extraction. Our best model signiﬁ-\ncantly outperforms mBERT, XLM-RoBERTa,\nand AraBERT (Antoun et al., 2020) in both\nthe supervised and zero-shot transfer set-\ntings. We have made our pre-trained models\npublicly available at https://github.com/\nlanwuwei/GigaBERT.\n1 Introduction\nFine-tuning pre-trained Transformer models (De-\nvlin et al., 2019; Liu et al., 2019; Yang et al., 2019)\nhas recently achieved state-of-the-art results on a\nwide range of NLP tasks where supervised train-\ning data is available. When trained on multilingual\ncorpora, BERT-based models have demonstrated\nthe ability to learn multilingual representations that\nsupport zero-shot cross-lingual transfer learning\nsurprisingly effectively (Wu and Dredze, 2019;\nPires et al., 2019; Lample and Conneau, 2019).\nWithout access to any parallel text or target lan-\nguage annotations, multilingual BERT (mBERT;\nDevlin et al., 2019) even supports cross-lingual\ntransfer for language pairs that are written in differ-\nent scripts, for example, English-to-Arabic. How-\never, the transfer learning performance still lags far\nbehind where supervised data is available in the\ntarget language. In this paper, we explore to what\nextent it is possible to improve performance in the\nzero-shot scenario by building a customized bilin-\ngual BERT for English and Arabic, a particularly\nchallenging language pair for cross-lingual transfer\nlearning.\nWe present GigaBERT, a customized BERT\nfor English-to-Arabic cross-lingual transfer that\nis trained on newswire text in the Gigaword corpus\n(Graff et al., 2003; Parker et al., 2009) in addi-\ntion to Wikipedia and web crawl data. We sys-\ntematically compare our pre-trained models of dif-\nferent conﬁgurations against the mBERT (Devlin\net al., 2019) and XLM-RoBERTa (XLM-R; Con-\nneau et al., 2020a). By using a customized vocabu-\nlary and code-switched data speciﬁcally created for\nEnglish-to-Arabic transfer learning, our GiagBERT\noutperforms mBERT andXLM-Rbase (both support\nmore than 100 languages) on a range of IE tasks,\nincluding named entity recognition, part-of-speech\ntagging, argument role labeling, and relation extrac-\ntion. Further performance gains are demonstrated\nby augmenting the pre-training corpus with synthet-\nically generated code-switched data. This demon-\nstrates the usefulness of anchor points for zero-shot\ncross-lingual transfer learning. GigaBERT also per-\nforms well when annotated Arabic data is available,\noutperforming AraBERT (Antoun et al., 2020), the\nstate-of-the-art Arabic-speciﬁc BERT model, on\nvarious Arabic IE tasks.\n2 Related Work\nThe existing Arabic pre-trained models are either\nmonolingual, such as hULMonA (ElJundi et al.,\n2019) and AraBERT (Antoun et al., 2020); or\nmultilingual with several or over a hundred lan-\nguages, such as mBERT (Devlin et al., 2019), XLM\n(Lample and Conneau, 2019), and XLM-RoBERTa\n(Conneau et al., 2020a). There is no bilingual pre-\n4728\nModels\nTraining Data Vocabulary Conﬁguration\nsource #tokens (all/en/ar) tokenization size (all/en/ar) cased size #parameters\nAraBERT newswire 2.5B/ – /2.5B SentencePiece 64k/ – / 58k no base 136M\nmBERT Wiki 21.9B/2.5B/153M WordPiece 110k/53k/5k yes base 172M\nXLM-R base CommonCrawl 295B/55.6B/2.9B SentencePiece 250k/80k/14k yes base 270M\nXLM-R large CommonCrawl 295B/55.6B/2.9B SentencePiece 250k/80k/14k yes large 550M\nGiagBERT-v0 Gigaword 4.7B/3.6B/1.1B SentencePiece 50k/28k/19k yes base 125M\nGigaBERT-v1 Gigaword, Wiki 7.4B/6.1B/1.3B WordPiece 50k/25k/23k yes base 125M\nGigaBERT-v2/3 Gigaword, Wiki, Oscar 10.4B/6.1B/4.3B WordPiece 50k/21k/26k no base 125M\nGigaBERT-v4 Gigaword, Wiki, Oscar (+ code-switch) 10.4B/6.1B/4.3B WordPiece 50k/21k/26k no base 125M\nTable 1: Conﬁguration comparisons for AraBERT (Antoun et al., 2020), mBERT (Devlin et al., 2019), XLM-\nRoBERTa (Conneau et al., 2020a), and GigaBERT (this work).\ntrained language model designed speciﬁcally for\nEnglish-Arabic. K et al. (2020) pre-trained small-\nscale (e.g., 1GB data and 2M training steps) bilin-\ngual BERT for English-Hindi, English-Spanish,\nand English-Russian to study the impact of lin-\nguistic properties of the languages, the architecture\nof the model, and the learning objectives on cross-\nlingual transfer. Kim et al. (2019) presented a bilin-\ngual BERT using multi-task learning for translation\nquality estimation with regards to English-Russian\nand English-German. Conneau et al. (2020b) fo-\ncused on the bilingual XLM for English-French,\nEnglish-Russian, and English-Chinese to analyze\nthe cross-lingual transfer ability with domain sim-\nilarity, anchor points, parameter sharing, and lan-\nguage similarity.\n3 GigaBERT\nWe present ﬁve versions of GigaBERT pre-trained\nusing the Transformer encoder (Vaswani et al.,\n2017) with BERTbase conﬁgurations: 12 attention\nlayers, each has 12 attention heads and 768 hidden\ndimensions, which attributes 110M parameters. Ta-\nble 1 shows a detailed summary of the training data\nand model parameters.\n3.1 Training Data\nWe pre-train our GigaBERT models using the ﬁfth\nedition of English and Arabic Gigaword corpora.1\nThe Gigaword data consists of 13 million news ar-\nticles2 and matches the domain of many NLP tasks.\nWe split English and Arabic sentences without to-\nkenization by a modiﬁed version of the Stanford\n1https://catalog.ldc.upenn.edu/\nLDC2011T07 and https://catalog.ldc.upenn.\nedu/LDC2011T11\n2We ﬂattened the Gigaword data with https://\ngithub.com/nelson-liu/flatten_gigaword.\nCoreNLP tool (Manning et al., 2014). 3 We also\nadd Wikipedia data processed by WikiExtractor4\nfor better coverage. As the English Wikipedia (to-\ntal 2.5B tokens) is much larger than the Arabic\nWikipedia (total 0.15B tokens), we balance the pre-\ntraining data by (1) up-sampling the Arabic data by\nrepeating the Wikipedia portion ﬁve times and the\nGigaword portion three times; (2) adding the Ara-\nbic section of the Oscar corpus (Ortiz Su´arez et al.,\n2019), a large-scale multilingual dataset ﬁltered\nfrom the Common Crawl.\nCode-Switched Data Augmentation. To fur-\nther improve cross-lingual transfer capability, we\nleverage English-Arabic dictionaries to create syn-\nthetic code-switched training data (Conneau et al.,\n2020a). We experimented with three dictionaries:\nPanLex (Kamholz et al., 2014), MUSE (Conneau\net al., 2018), and Wikipedia parallel titles. We ex-\ntract parallel article titles in Wikipedia based on\nthe inter-language links and the entities based on\nthe Wikidata (Jiang et al., 2020).5 The dictionaries\nof PanLex, MUSE, Wikipedia contain 24K, 44K,\n2M entries, respectively, and on overage 4.6, 1.4\nand 1 translations per entry (English or Arabic).\nFor training GigaBERT-v4, we code-switch up to\n50% random sentences for both English and Arabic\nand up to 30% of tokens for each sentence. During\nthe replacement process, we prioritize substitutions\nbased on the Wikipedia titles, then PanLex and\nMUSE if the proportion of tokens being replaced\nhas not reached 30% for a given sentence.\n3In the early versions of GigaBERT (v0/1/2/3), we split\nArabic sentences at period, exclamation, and question mark.\n4https://github.com/attardi/\nwikiextractor\n5https://github.com/clab/\nwikipedia-parallel-titles and https:\n//dumps.wikimedia.org/wikidatawiki/\nentities/\n4729\n3.2 Vocabulary\nThe vocabulary size is critical to the performance\nof pre-training models, as it directly impacts the\nsubword granularity and the number of parameters.\nThe original English BERT (Devlin et al., 2019)\nuses a 30k vocabulary size for ∼3B tokens of train-\ning data, while the multilingual BERT and XLM-R\nhave ∼5k and ∼14k Arabic subwords in their vo-\ncabularies respectively (Table 1).6 We choose a\nvocabulary size of 50k for our GigaBERT models\nbased on preliminary experiments. For GigaBERT-\nv0, we use the unigram language model in the Sen-\ntencePiece (Kudo and Richardson, 2018) to create\n30k cased English subwords and 20k Arabic sub-\nwords separately.7 For GigaBERT-v1/2/3/4, we\ndid not distinguish Arabic and English subword\nunits, instead, we train a uniﬁed 50k vocabulary\nusing WordPiece (Wu et al., 2016). 8 The vocab-\nulary is cased for GigaBERT-v1 and uncased for\nGigaBERT-v2/3/4, which use the same vocabulary.\n3.3 Optimization\nWe use the ofﬁcial implementation of BERT (De-\nvlin et al., 2019) in TensorFlow for pre-training.\nWe use Adam optimizer (Kingma and Ba, 2015)\nwith a learning rate of 1e-4, β1 = 0.9, β2 = 0.999,\nL2 weight decay of 0.01. The learning rate is\nwarmed up over the ﬁrst 100,000 steps to a peak\nvalue of 1e-4, then linearly decayed. The dropout\nis set to 0.1 for all layers. We use the whole word\nmask for GigaBERT-v0 and the regular subword\nmask for v1/2/3/4. The batch size is set to 512.\nGigaBERT-v0/1/2 are trained for 1.2 million steps\non Google Cloud TPUs with a max sequence length\nof 128. GigaBERT-v3 is additionally trained for\n140k steps with a max sequence length of 512. The\nmaximum number of masked LM predictions per\nsequence is set to 20 when max sequence length is\n128 and set to 80 when max sequence length is 512.\nGigaBERT-v4 is trained from the GigaBERT-v3\ncheckpoint for another 140K steps on the code-\nswitched data. We also experiment with different\nthresholds for the code-switched data augmenta-\ntion, as well as training models from scratch on the\ncode-switched data (Appendix A).\n6We check the Unicode range of characters to classify\nword pieces as English or Arabic.\n7There are 633 word pieces shared by both languages. We\nadd 633 unused symbols (e.g., unused-1, unused-2, etc.) to\nmake up the 50k combined vocabulary.\n8We use Hugging Face’s implementation: https://\ngithub.com/huggingface/tokenizers\nTask #Train (en/ar) #Dev (en/ar) #Test (en/ar) Metric\nNER 7634/2683 1005/322 1095/238 F 1\nPOS 12543/6174 2002/786 2077/704 Acc\nARL 21875/11587 3345/1221 2603/1568 F 1\nRE 63177/32984 10218/4482 6861/4638 F 1\nTable 2: Statistics of the datasets for IE tasks.\n4 Experiments\n4.1 Downstream IE Tasks\nWe demonstrate the effectiveness of GigaBERT on\nnamed entity recognition (NER), part-of-speech\ntagging (POS), argument role labeling (ARL), and\nrelation extraction (RE) tasks. We use the ACE\n2005 corpus (Walker et al., 2006) in the NER,\nARL, and RE evaluations, and use the Universal\nDependencies Treebank v1.4 (Nivre et al., 2016)\nin the POS experiments. All of these datasets\nare from the news domain, as summarized in\nTable 2. For NER, we use the same English\ndocument splits as Lu and Roth (2015) and ran-\ndomly shufﬂe Arabic documents into train/dev/test\n(80%/10%/10%). For ARL and RE, we randomly\nshufﬂe both English and Arabic documents into\ntrain/dev/test (80%/10%/10%). For POS, we follow\nthe train/dev/test split by Wu and Dredze (2019).\nIn the ARL ﬁne-tuning experiment, we pair each\ntrigger with its argument mentions as positive in-\nstances and with other entities in the sentence as\nnegative instances. As for RE, we use gold relation\nmentions as positive examples and create negative\nexamples by randomly pairing two entities in a sen-\ntence. We perform these tasks following the same\nﬁne-tuning pipeline as BERT (Devlin et al., 2019).\nWe feed input sentences into a pre-trained model,\nthen extract the necessary hidden representations,\ni.e., all token representations for NER/POS and ar-\ngument/entity spans for ARL/RE, before applying\none linear layer for classiﬁcation. We evaluate for\neach language in the standard supervised learning\nsetting, as well as the zero-shot transfer learning\nsetting from English to Arabic, where the model is\ntrained on the annotated English training data and\nevaluated on the Arabic test set.\n4.2 Implementations\nWe implement the ﬁne-tuning experiments with\nthe PyTorch framework (Paszke et al., 2019) and\nchoose hyperparameters by grid search.9 We set the\n9The search range includes learning rate (1e-5. 2e-5. 5e-5.\n1e-4), batch size (4, 8, 16, 32) and epoch number (3, 7, 10).\n4730\nModels NER (F1) POS (Accuracy) ARL (F 1) RE (F 1)\nen ar en →ar en ar en →ar en ar en →ar en ar en →ar\nAraBERT - 78.6 - - 97.6 - - 73.3 - - 83.1 -\nmBERT 80.3 72.9 30.8/31.1 97.0 97.3 50.8/50.8 70.4 64.5 44.4/45.9 77.9 75.3 30.1/30.1\nXLM-Rbase 81.0 81.5 43.5/43.5 97.8 97.6 59.6/61.1 69.4 56.4 54.4/53.7 78.2 79.2 40.4/36.0\nGigaXLM-Rbase 82.0 80.8 45.4/45.0 97.3 97.7 60.7/61.4 70.1 71.4 52.6/52.6 79.6 79.5 43.3 /44.0\nGigaBERT-v0 79.1 76.6 43.9/45.9 96.8 97.5 49.7/54.1 69.1 66.1 42.3/42.2 76.6 72.5 21.5/20.9\nGigaBERT-v1 82.8 72.9 49.1/49.1 97.2 96.6 51.9/52.2 72.8 67.7 44.6/45.5 80.4 73.2 36.0/31.1\nGigaBERT-v2 82.5 75.2 48.3/48.2 97.2 97.8 53.1/53.4 72.0 66.7 42.5/44.1 79.4 74.2 31.9/36.8\nGigaBERT-v3 83.4 83.1 48.9/48.3 97.1 97.8 53.3/54.7 72.3 76.5 51.0/51.0 79.9 84.3 48.2 /46.8\nGigaBERT-v4 83.8 84.1 51.5/51.5 97.1 97.7 54.6/55.5 71.9 73.9 52.7/56.1 79.1 83.6 43.3/48.2\nXLM-Rlarge 85.8 84.8 49.3/50.4 98.0 97.8 61.7/61.2 72.3 73.4 58.0/57.4 83.2 82.1 52.5/57.5\nGigaXLM-Rlarge 85.8 84.5 51.0/51.0 97.9 97.8 62.0/63.6 73.1 71.1 56.5/51.9 82.5 82.3 54.0/58.2\nTable 3: Evaluation on four Arabic IE tasks that compares AraBERT (Antoun et al., 2020), multilingual BERT\n(Devlin et al., 2019), XLM-RoBERTa (Conneau et al., 2020a), GigaBERT/GigaXLM-R (this work). All models\nuse BERTbase architecture except XLM-Rlarge. GigaBERT-v4 is continued pre-training of GigaBERT-v3 on code-\nswitched data. GigaXLM-R is domain adapted pre-training of XLM-R on Gigaword data.\nlearning rate to 2e-5, batch size to 8, max sequence\nlength to 128, and the number of ﬁne-tuning epochs\nto 7. Some exceptions include a learning rate of\n1e-4 in NER experiments, max sequence length of\n512, and batch size of 4 in RE experiments. For RE,\nwe also use gradient accumulation to simulate the\nlarger batch size of 32 when using models based\non BERTlarge architecture.\n4.3 Results and Analysis\nTable 3 shows experimental results for the pre-\ntrained models on both English and Arabic IE tasks.\nFor the zero-shot transfer (en →ar), we report two\nscores on the Arabic test set, where the best check-\npoint is selected based on the English dev set and\nthe Arabic dev set, respectively. In summary, we\nﬁnd the key factors of improved pre-training per-\nformance are a large amount of training data in\nthe target language, customized vocabulary, longer\nmax length of sentence, and more anchor points\nfrom code-switched data. We also add experiments\nwith XLM-Rlarge models as a reference, but the\ncomparison focuses on the pre-trained models with\nBERTbase conﬁguration for fairness.\nSingle-language Performance. All versions of\nGigaBERT perform very competitively, especially\nthe GigaBERT-v3/4. After adding Wikipedia and\nOscar data, GigaBERT-v2 starts to outperform\nmBERT and XLM-Rbase on most tasks. We ﬁnd\nit crucial to continue training GigaBERT-v2 with\na longer max sentence length of 512 word pieces,\nas the resulting GigaBERT-v3 model shows im-\nprovements in all four IE tasks. GigaBERT-v3 also\noutperforms AraBERT (Antoun et al., 2020), the\nstate-of-the-art Arabic-speciﬁc BERT model by a\nlarge margin, showing that our bilingual GigaBERT\ndoes not sacriﬁce per-language performance. It is\nworth noting that GigaBERT-v4 also has competi-\ntive single-language performance after training on\nthe synthetically created code-switched data.\nCross-lingual Zero-shot Transfer Learning. All\npre-trained models show varied performance when\nwe select checkpoints based on the English dev\nset and Arabic dev set, indicating that the best\nsingle-language performance does not necessarily\nimply the best cross-lingual performance. Com-\npared to GigaBERT-v0, additional data used to train\nGigaBERT-v1/2 helps improve zero-shot transfer\ncapability, even though the added data is not from\nthe news domain. Different from previous works\n(Wu and Dredze, 2019; Pires et al., 2019) that at-\ntribute cross-lingual ability to shared subwords,\nGigaBERT-v3 has nearly no shared word pieces or\nscripts between English and Arabic, but still shows\nstrong cross-lingual performance. We hypothesize\nthe Transformer encoder projects similar contextual\nrepresentations and enables cross-lingual transfer\n(Conneau et al., 2020b).\nCode-Switched Pre-training. We show that we\ncan further improve GigaBERT’s cross-lingual\ntransfer capability with a carefully designed code-\nswitching procedure. Our GigaBERT-v4 pre-\ntrained with code-switched data shows signiﬁ-\ncant improvement over GigaBERT-v3, achieving\n4731\nFigure 1: Cosine similarity between sentence repre-\nsentations of parallel sentences (bitext) and randomly\npaired sentences (random).\nnew state-of-the-art for zero-shot transfer from En-\nglish to Arabic on NER, ARL, and RE. Our code-\nswitched pre-training differs from Conneau et al.\n(2020b) in two aspects: 1) we explored multiple\nbilingual dictionaries, including PanLex (Kamholz\net al., 2014), MUSE (Conneau et al., 2018) and\nWikipedia titles, while MUSE appears to be the\nmost effective; 2) we keep at least half of the sen-\ntences unchanged to balance between real data and\nartiﬁcial data. In practice, the generated data for\nGigaBERT-v4 has 47.4% of the sentences code-\nswitched. We present more comparison experi-\nments using varied code-switching mixes and dif-\nferent bilingual lexicons in Appendix A.\nDomain-adapted Pre-training. We also explore\nwhether XLM-RoBERTa can be improved by ad-\nditional pre-training on Gigaword data, as Guru-\nrangan et al. (2020) have shown that the continued\npre-training with in-domain data is helpful. We cre-\nate GigaXLM-R models by continuing pre-training\nfrom XLM-Rbase and XLM-Rlarge checkpoints in\nthe Fairseq toolkit (Ott et al., 2019) for 500k steps\non shufﬂed Arabic and English Gigaword corpus\n(max sequence length 512 and batch size 4). Al-\nthough only ∼1% of the Gigaword corpus is used\nin this continued training step due to computing re-\nsource limit, GigaXLM-R still improves zero-shot\ntransfer performance for NER, POS, and RE over\nthe original XLM-R models as shown in Table 3.\nWe could expect more performance improvement\nwith a larger batch size and longer training time.\nEmbedding Space Analysis. We further analyze\nthe semantic similarity of parallel English-Arabic\nsentence representations and ﬁnd that GigaBERT\nis able to distinguish parallel sentences from ran-\ndomly paired sentences more effectively compared\nto its counterparts. Our hypothesis is that cross-\nlingual representations for parallel English-Arabic\nsentences should be similar, but randomly paired\nsentences should be dissimilar. To evaluate cross-\nlingual similarity, we extract sentence represen-\ntation of 5340 English-Arabic parallel sentences\nfrom the GALE corpus10 and the same number of\nrandomly paired sentences with pre-trained models\nacross all 12 layers. We use the average of hid-\nden representations, excluding [CLS] and [SEP],\nas a sentence representation. Cosine similarity is\ncalculated for each sentence pairs and averaged\nacross the whole corpus. In Figure 1, GigaBERT\nshows high similarity between parallel sentences\nand low similarity between randomly paired sen-\ntences. A clear separation for two types of paired\nsentences is shown across all the layers. In contrast,\nXLM-R is not able to distinguish between them but\nshows high similarity scores. mBERT shows low\nsimilarity in both cases. This suggests that our Gi-\ngaBERT preserves language independent semantic\ninformation in the sentence representations, which\nmight contribute to the competitive performance in\ndownstream IE tasks.\n5 Conclusions\nIn this paper, we show that the performance of\nzero-shot cross-lingual transfer can be improved by\ntraining customized bilingual BERT for a given lan-\nguage pair and text domain. We pre-trained several\nmasked language models (GigaBERTs) for Arabic-\nEnglish and conducted a focused study on informa-\ntion extraction tasks in the newswire domain. The\nexperiments show that our GigaBERT model out-\nperforms multilingual BERT, XLM-RoBERTa, and\nthe monolingual AraBERT on NER, POS, ARL and\nRE tasks. We also achieve the new state-of-the-art\nperformance fro zero-shot transfer learning from\nEnglish to Arabic. We additionally studied code-\nswitched pre-training for GigaBERT and domain-\nadapted pre-training for XLM-RoBERTa.\nAcknowledgement\nWe thank Nizar Habash and anonymous reviewers\nfor their valuable suggestions. We also thank the\nGoogle TFRC program for providing free TPU ac-\ncess. This material is based in part on research\nsponsored by the NSF (IIS-1845670), ODNI,\n10https://catalog.ldc.upenn.edu/\nLDC2014T10\n4732\nand IARPA via the BETTER program (2019-\n19051600004), DARPA via the ARO (W911NF-17-\nC-0095) in addition to an Amazon Research Award.\nThe views and conclusions contained herein are\nthose of the authors and should not be interpreted\nas necessarily representing the ofﬁcial policies, ei-\nther expressed or implied, of ODNI, IARPA, ARO,\nDARPA, or the U.S. Government. The U.S. Gov-\nernment is authorized to reproduce and distribute\nreprints for governmental purposes notwithstand-\ning any copyright annotation therein.\nReferences\nWissam Antoun, Fady Baly, and Hazem M. Hajj. 2020.\nAraBERT: Transformer-based model for arabic lan-\nguage understanding. In Proceedings of the Interna-\ntional Conference on Language Resources and Eval-\nuation.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020a. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the Association for Computational\nLinguistics.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Herv ´e J´egou. 2018.\nWord translation without parallel data. In Proceed-\nings of International Conference on Learning Repre-\nsentation.\nAlexis Conneau, Shijie Wu, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020b. Emerging\ncross-lingual structure in pretrained language mod-\nels. In Proceedings of the Association for Computa-\ntional Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies.\nObeida ElJundi, Wissam Antoun, Nour El Droubi,\nHazem Hajj, Wassim El-Hajj, and Khaled Shaban.\n2019. hULMonA: The universal language model in\nArabic. In Proceedings of the Fourth Arabic Natural\nLanguage Processing Workshop.\nDavid Graff, Junbo Kong, Ke Chen, and Kazuaki\nMaeda. 2003. English Gigaword. Linguistic Data\nConsortium.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the Association for Computational\nLinguistics.\nChao Jiang, Mounica Maddela, Wuwei Lan, Yang\nZhong, and Wei Xu. 2020. Neural crf model for sen-\ntence alignment in text simpliﬁcation. In Proceed-\nings of the Association for Computational Linguis-\ntics.\nKarthikeyan K, Zihan Wang, Stephen Mayhew, and\nDan Roth. 2020. Cross-lingual ability of multilin-\ngual BERT: An empirical study. In Proceedings of\nthe International Conference on Learning Represen-\ntations.\nDavid Kamholz, Jonathan Pool, and Susan Colowick.\n2014. PanLex: Building a resource for panlingual\nlexical translation. In Proceedings of the Interna-\ntional Conference on Language Resources and Eval-\nuation.\nHyun Kim, Joon-Ho Lim, Hyun-Ki Kim, and Seung-\nHoon Na. 2019. QE BERT: Bilingual BERT using\nmulti-task learning for neural quality estimation. In\nProceedings of the Fourth Conference on Machine\nTranslation.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof International Conference on Learning Represen-\ntation.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of Emperical Methods in Natural Lan-\nguage Processing.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. In Proceedings\nof Advances in Neural Information Processing Sys-\ntems.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nWei Lu and Dan Roth. 2015. Joint mention extrac-\ntion and classiﬁcation with mention hypergraphs. In\nProceedings of Emperical Methods in Natural Lan-\nguage Processing.\nChristopher D Manning, Mihai Surdeanu, John Bauer,\nJenny Rose Finkel, Steven Bethard, and David Mc-\nClosky. 2014. The stanford CoreNLP natural lan-\nguage processing toolkit. In Proceedings of the As-\nsociation for Computational Linguistics.\nJoakim Nivre, ˇZeljko Agi´c, Lars Ahrenberg, Maria Je-\nsus Aranzabe, Masayuki Asahara, Aitziber Atutxa,\nMiguel Ballesteros, John Bauer, Kepa Bengoetxea,\nYevgeni Berzak, Riyaz Ahmad Bhat, Eckhard Bick,\nCarl B ¨orstell, Cristina Bosco, Gosse Bouma, Sam\nBowman, G¨uls ¸en Cebiro˘glu Eryi˘git, Giuseppe G. A.\n4733\nCelano, Fabricio Chalub, C ¸ a˘grı C ¸¨oltekin, Miriam\nConnor, Elizabeth Davidson, Marie-Catherine\nde Marneffe, Arantza Diaz de Ilarraza, Kaja Do-\nbrovoljc, Timothy Dozat, Kira Droganova, Puneet\nDwivedi, Marhaba Eli, Toma ˇz Erjavec, Rich ´ard\nFarkas, Jennifer Foster, Claudia Freitas, Katar ´ına\nGajdoˇsov´a, Daniel Galbraith, Marcos Garcia, Moa\nG¨ardenfors, Sebastian Garza, Filip Ginter, Iakes\nGoenaga, Koldo Gojenola, Memduh G ¨okırmak,\nYoav Goldberg, Xavier G ´omez Guinovart, Berta\nGonz´ales Saavedra, Matias Grioni, Normunds\nGr¯uz¯ıtis, Bruno Guillaume, Jan Hajiˇc, Linh H `a M˜y,\nDag Haug, Barbora Hladk ´a, Radu Ion, Elena\nIrimia, Anders Johannsen, Fredrik Jørgensen, H¨uner\nKas ¸ıkara, Hiroshi Kanayama, Jenna Kanerva, Boris\nKatz, Jessica Kenney, Natalia Kotsyba, Simon\nKrek, Veronika Laippala, Lucia Lam, Phuong\nLˆe H`ˆong, Alessandro Lenci, Nikola Ljubeˇsi´c, Olga\nLyashevskaya, Teresa Lynn, Aibek Makazhanov,\nChristopher Manning, C ˘at˘alina M ˘ar˘anduc, David\nMareˇcek, H ´ector Mart´ınez Alonso, Andr´e Martins,\nJan Ma ˇsek, Yuji Matsumoto, Ryan McDonald,\nAnna Missil ¨a, Verginica Mititelu, Yusuke Miyao,\nSimonetta Montemagni, Keiko Sophie Mori,\nShunsuke Mori, Bohdan Moskalevskyi, Kadri Muis-\nchnek, Nina Mustaﬁna, Kaili M ¨u¨urisep, Lu’o’ng\nNguy˜ˆen Thi., Huy` ˆen Nguy˜ˆen Thi. Minh, Vitaly\nNikolaev, Hanna Nurmi, Petya Osenova, Robert\n¨Ostling, Lilja Øvrelid, Valeria Paiva, Elena Pascual,\nMarco Passarotti, Cenel-Augusto Perez, Slav Petrov,\nJussi Piitulainen, Barbara Plank, Martin Popel,\nLauma Pretkalnin ¸a, Prokopis Prokopidis, Tiina Puo-\nlakainen, Sampo Pyysalo, Alexandre Rademaker,\nLoganathan Ramasamy, Livy Real, Laura Rituma,\nRudolf Rosa, Shadi Saleh, Baiba Saul ¯ıte, Sebastian\nSchuster, Wolfgang Seeker, Mojgan Seraji, Lena\nShakurova, Mo Shen, Natalia Silveira, Maria Simi,\nRadu Simionescu, Katalin Simk ´o, M ´aria ˇSimkov´a,\nKiril Simov, Aaron Smith, Carolyn Spadine, Alane\nSuhr, Umut Sulubacak, Zsolt Sz ´ant´o, Takaaki\nTanaka, Reut Tsarfaty, Francis Tyers, Sumire\nUematsu, Larraitz Uria, Gertjan van Noord, Viktor\nVarga, Veronika Vincze, Lars Wallin, Jing Xian\nWang, Jonathan North Washington, Mats Wir ´en,\nZdenˇek ˇZabokrtsk´y, Amir Zeldes, Daniel Zeman,\nand Hanzhi Zhu. 2016. Universal Dependencies\n1.4.\nPedro Javier Ortiz Su ´arez, Benoˆıt Sagot, and Laurent\nRomary. 2019. Asynchronous pipeline for process-\ning huge corpora on medium to low resource infras-\ntructures. In Proceedings of the 7th Workshop on the\nChallenges in the Management of Large Corpora.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies.\nRobert Parker, David Graff, Ke Chen, Junbo Kong, and\nKazuaki Maeda. 2009. Arabic Gigaword.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. PyTorch:\nAn imperative style, high-performance deep learn-\ning library. In Proceedings of Advances in Neural\nInformation Processing Systems.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Pro-\nceedings of the Association for Computational Lin-\nguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of Advances in Neural In-\nformation Processing Systems.\nChristopher Walker, Stephanie Strassel, Julie Medero,\nand Kazuaki Maeda. 2006. ACE 2005 multilin-\ngual training corpus. Linguistic Data Consortium,\nPhiladelphia, 57.\nShijie Wu and Mark Dredze. 2019. Beto, Bentz, Be-\ncas: The surprising cross-lingual effectiveness of\nBERT. In Proceedings of Empirical Methods in\nNatural Language Processing and the International\nJoint Conference on Natural Language Processing.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural machine\ntranslation system: Bridging the gap between hu-\nman and machine translation. arXiv preprint\narXiv:1609.08144.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. In Proceedings of Ad-\nvances in Neural Information Processing Systems.\n4734\nA Comparison Experiments for\nCode-Switched Pre-training\nGiven the English and Arabic monolingual corpus\nand the bilingual lexicons, we have different thresh-\nolds to control the code-switched data generation:\n1) the percentage of sentences being code-switched\nwithin the whole corpus, we set sentence replace-\nment threshold to limit the changed sentences; 2)\nthe percentage of tokens being replaced within the\nsentence, we set token replacement threshold to\nlimit the changed tokens; 3) the choice of bilingual\nlexicons, where we explore different combinations\nof PanLex, MUSE and Wiki titles. With the gen-\nerated code-switched data, we can pre-train Giga-\nBERT from scratch or load the existing checkpoint\n(GigaBERT-v3) for continued pre-training, which\nare s1 and s2 in Table 4, respectively.\nAs shown in Table 4, it’s better to keep some sen-\ntences unchanged for code-switched pre-training.\nThe continued pre-training (s2) shows slightly bet-\nter performance than that training from scratch (s1).\nDuring the data augmentation, we need to keep\na relatively low ratio for token replacement. The\nresults also reveal that the MUSE dictionary is very\npromising, which outperforms the combinations of\nall dictionaries in some cases.\nModels NER (F1) POS (Accuracy) ARL (F 1) RE (F 1)\nen ar en → ar en ar en → ar en ar en → ar en ar en → ar\ns1-0.5-0.3-all 82.1 83.3 49.7 97.0 97.7 58.3 72.4 74.4 48.6 80.0 84.1 47.0\ns1-1.0-0.5-all 83.1 82.9 48.3 97.0 97.7 55.0 71.1 74.6 46.9 79.0 82.7 40.2\ns1-0.5-0.3-pm 83.5 83.9 51.3 97.2 97.8 56.9 71.4 73.4 38.3 74.9 82.8 47.6\ns1-0.5-0.3-m 82.4 84.7 52.9 97.1 97.8 58.6 70.7 72.4 52.1 77.3 83.7 46.0\ns1-0.5-0.1-mw 83.1 83.9 52.2 97.2 97.6 55.0 71.7 72.7 49.0 78.2 84.1 54.0\ns1-0.5-0.3-mw 83.3 83.3 53.5 97.1 97.7 56.0 71.9 72.8 46.8 79.2 84.2 44.3\ns1-1.0-0.3-mw 82.7 84.4 48.2 97.1 97.7 56.1 70.6 72.7 51.4 77.9 84.6 47.3\ns1-1.0-0.001-mw 83.4 83.8 54.1 97.2 97.7 55.1 72.3 73.3 48.0 78.7 83.5 41.2\ns1-0.5-0.3-w 82.8 83.8 49.9 97.1 97.8 53.8 71.4 73.8 50.8 77.1 82.7 54.3\ns2-0.5-0.3-all 83.8 84.1 51.5 97.1 97.7 55.5 71.9 73.9 56.1 79.1 83.6 48.2\ns2-1.0-0.5-all 82.2 83.7 51.7 97.0 97.8 56.1 71.3 74.5 51.1 79.2 82.0 45.8\ns2-0.5-0.3-pm 83.2 83.8 50.9 97.1 97.7 55.7 72.0 73.7 48.4 79.3 82.9 45.3\ns2-0.5-0.3-m 83.4 83.4 52.9 97.2 97.7 52.9 71.0 73.9 55.0 78.8 83.5 52.5\ns2-0.5-0.1-mw 83.0 85.1 52.7 97.2 97.8 53.6 71.9 75.0 50.0 79.0 83.7 52.2\ns2-0.5-0.3-mw 83.4 85.0 51.0 97.1 97.7 52.4 72.2 74.9 49.3 81.0 83.7 49.8\ns2-1.0-0.3-mw 83.2 83.7 50.2 97.0 97.7 53.5 71.0 71.8 54.7 67.2 81.3 42.9\ns2-1.0-0.001-mw 83.6 84.2 49.6 97.4 97.7 52.3 71.8 73.2 51.2 79.0 84.0 42.6\ns2-0.5-0.3-w 83.7 83.9 50.4 97.2 97.7 53.1 72.6 74.4 48.2 76.2 83.6 47.4\nTable 4: Comparison experiments of different code-switching conﬁgurations. The model name is composed of four\nparts: s1 (pre-train from scratch)/ s2 (continue pre-training), sentence replacement threshold, token replacement\nthreshold and bilingual lexicons, where all uses PanLex, MUSE and Wiki titles, pm uses PanLex and MUSE, mw\nuses MUSE and Wiki, m uses MUSE only and w uses Wiki only. The model s2-0.5-0.3-all is GigaBERT-v4 in the\npaper. The best checkpoint for en→ar is selected with Arabic dev set."
}