{
  "title": "TransVOD: End-to-End Video Object Detection With Spatial-Temporal Transformers",
  "url": "https://openalex.org/W4221166276",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2354339597",
      "name": "Zhou, Qianyu",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2875632884",
      "name": "Li, Xiangtai",
      "affiliations": [
        "Peking University",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2103752681",
      "name": "He Lu",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2227264241",
      "name": "Yang Yibo",
      "affiliations": [
        "Jingdong (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2358840862",
      "name": "Cheng, Guangliang",
      "affiliations": [
        "Group Sense (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2188180711",
      "name": "Tong, Yunhai",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A1974913825",
      "name": "Ma Lizhuang",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A1960309444",
      "name": "Tao, Dacheng",
      "affiliations": [
        "Jingdong (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3084874594",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3092900809",
    "https://openalex.org/W2982723417",
    "https://openalex.org/W2963585656",
    "https://openalex.org/W2964080601",
    "https://openalex.org/W2996794639",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W4312598093",
    "https://openalex.org/W6838809002",
    "https://openalex.org/W6785762920",
    "https://openalex.org/W4312844845",
    "https://openalex.org/W3097550038",
    "https://openalex.org/W4312511177",
    "https://openalex.org/W3108457314",
    "https://openalex.org/W3034467781",
    "https://openalex.org/W2552900565",
    "https://openalex.org/W2983827899",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W6787985011",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6799191659",
    "https://openalex.org/W6760166208",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6703733040",
    "https://openalex.org/W2994810768",
    "https://openalex.org/W764651262",
    "https://openalex.org/W2981858655",
    "https://openalex.org/W3133275180",
    "https://openalex.org/W2921015377",
    "https://openalex.org/W3100094580",
    "https://openalex.org/W2982770724",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2336589871",
    "https://openalex.org/W6695799263",
    "https://openalex.org/W3106643287",
    "https://openalex.org/W6760424586",
    "https://openalex.org/W2963516811",
    "https://openalex.org/W3204023867",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W3188394685",
    "https://openalex.org/W6784094891",
    "https://openalex.org/W3174156594",
    "https://openalex.org/W6788023325",
    "https://openalex.org/W3104340075",
    "https://openalex.org/W6746472748",
    "https://openalex.org/W6749530778",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W3186193138",
    "https://openalex.org/W6786708909",
    "https://openalex.org/W6765764431",
    "https://openalex.org/W6714138976",
    "https://openalex.org/W639708223",
    "https://openalex.org/W3206836360",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2962855257",
    "https://openalex.org/W3016536541",
    "https://openalex.org/W2969727121",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W3200157107",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2904617485",
    "https://openalex.org/W6748516020",
    "https://openalex.org/W6631943919",
    "https://openalex.org/W2964286567",
    "https://openalex.org/W2962766617",
    "https://openalex.org/W2222512263",
    "https://openalex.org/W2963653352",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W6755600014",
    "https://openalex.org/W3028265921",
    "https://openalex.org/W3010594275",
    "https://openalex.org/W2990578161",
    "https://openalex.org/W6761178061",
    "https://openalex.org/W3172752666",
    "https://openalex.org/W607748843",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W4394657967",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2898044248",
    "https://openalex.org/W4292692470",
    "https://openalex.org/W2950800384",
    "https://openalex.org/W3103358580",
    "https://openalex.org/W4283311214",
    "https://openalex.org/W3115390238",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2793130599",
    "https://openalex.org/W3171516518",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W3104010045",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2957958490",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W3119686997",
    "https://openalex.org/W2964086649",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W4293437100",
    "https://openalex.org/W4302275081",
    "https://openalex.org/W4288408844",
    "https://openalex.org/W4376626065",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2963212638",
    "https://openalex.org/W2919935710",
    "https://openalex.org/W4299603580"
  ],
  "abstract": "Detection Transformer (DETR) and Deformable DETR have been proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance as previous complex hand-crafted detectors. However, their performance on Video Object Detection (VOD) has not been well explored. In this paper, we present TransVOD, the first end-to-end video object detection system based on spatial-temporal Transformer architectures. The first goal of this paper is to streamline the pipeline of VOD, effectively removing the need for many hand-crafted components for feature aggregation, e.g., optical flow model, relation networks. Besides, benefited from the object query design in DETR, our method does not need complicated post-processing methods such as Seq-NMS. In particular, we present a temporal Transformer to aggregate both the spatial object queries and the feature memories of each frame. Our temporal transformer consists of two components: Temporal Query Encoder (TQE) to fuse object queries, and Temporal Deformable Transformer Decoder (TDTD) to obtain current frame detection results. These designs boost the strong baseline deformable DETR by a significant margin (3%-4% mAP) on the ImageNet VID dataset. Then, we present two improved versions of TransVOD including TransVOD++ and TransVOD Lite. The former fuses object-level information into object query via dynamic convolution while the latter models the entire video clips as the output to speed up the inference time. We give detailed analysis of all three models in the experiment part. In particular, our proposed TransVOD++ sets a new state-of-the-art record in terms of accuracy on ImageNet VID with 90.0% mAP. Our proposed TransVOD Lite also achieves the best speed and accuracy trade-off with 83.7% mAP while running at around 30 FPS on a single V100 GPU device.",
  "full_text": "IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X 1\nTransVOD: End-to-End Video Object Detection\nwith Spatial-Temporal Transformers\nQianyu Zhou†, Xiangtai Li†, Lu He†, Yibo Y ang, Guangliang Cheng,\nYunhai Tong, Lizhuang Ma, Dacheng Tao, Fellow, IEEE\nAbstract—Detection Transformer (DETR) and Deformable DETR have been proposed to eliminate the need for many hand-designed\ncomponents in object detection while demonstrating good performance as previous complex hand-crafted detectors. However, their\nperformance on Video Object Detection (VOD) has not been well explored. In this paper, we present TransVOD, the ﬁrst end-to-end\nvideo object detection system based on simple yet effective spatial-temporal Transformer architectures. The ﬁrst goal of this paper is to\nstreamline the pipeline of current VOD, effectively removing the need for many hand-crafted components for feature aggregation,e.g.,\noptical ﬂow model, relation networks. Besides, beneﬁted from the object query design in DETR, our method does not need\npost-processing methods such as Seq-NMS. In particular, we present a temporal Transformer to aggregate both the spatial object\nqueries and the feature memories of each frame. Our temporal transformer consists of two components: Temporal Query Encoder\n(TQE) to fuse object queries, and Temporal Deformable Transformer Decoder (TDTD) to obtain current frame detection results. These\ndesigns boost the strong baseline deformable DETR by a signiﬁcant margin (3 %-4 % mAP) on the ImageNet VID dataset. TransVOD\nyields comparable performances on the benchmark of ImageNet VID. Then, we present two improved versions of TransVOD including\nTransVOD++ and TransVOD Lite. The former fuses object-level information into object query via dynamic convolution while the latter\nmodels the entire video clips as the output to speed up the inference time. We give detailed analysis of all three models in the\nexperiment part. In particular, our proposed TransVOD++ sets a new state-of-the-art record in terms of accuracy on ImageNet VID with\n90.0 % mAP . Our proposed TransVOD Lite also achieves the best speed and accuracy trade-off with 83.7 % mAP while running at\naround 30 FPS on a single V100 GPU device. Code and models are available at https://github.com/SJTU-LuHe/TransVOD.\nIndex Terms—Video Object Detection, Vision Transformers, Scene Understanding, Video Understanding.\n!\n1 I NTRODUCTION\nV\nIDEO Object Detection (VOD) extends image object de-\ntection to video scenarios, which aims to detect every\nobject given video clips. It enables various applications in\nthe real world, e.g., autonomous driving. However, still-\nimage detectors [1], [2], [3], [4] cannot be directly applied\nto much challenging video data, due to the appearance\n• Qianyu Zhou, Lu He and Lizhuang Ma are with the Department of Com-\nputer Science and Engineering, Shanghai Jiao Tong University, Shang-\nhai 200240, China (E-mail: {zhouqianyu, 147258369}@sjtu.edu.cn, ma-\nlz@cs.sjtu.edu.cn).\n• Xiangtai Li and Yunhai Tong are with the School of Artiﬁcial In-\ntelligence, Peking University, Beijing 100871, China (E-mail: {lxtpku,\nyhtong}@pku.edu.cn).\n• Yibo Yang and Dacheng Tao are with JD Explore Academy, Beijing\n100176, China (E-mail: ibo@pku.edu.cn, dacheng.tao@sydney.edu.au)\n• Guangliang Cheng is with the SenseTime Research, Beijing 100080, China\n(E-mail: guangliangcheng2014@gmail.com).\n• † indicates the ﬁrst three authors have equal contributions.\nManuscript received 10 March 2022; revised 14 September 2022; accepted\n9 November 2021. Date of publication XX 2023; date of current version\n18 November 2022.\nThis work is supported by National Key Research and Development\nProgram of China (2019YFC1521104), National Natural Science Foun-\ndation of China (72192821, 61972157), Shanghai Municipal Science and\nTechnology Major Project (2021SHZDZX0102), Shanghai Science and\nTechnology Commission (21511101200, 22YF1420300), and Art major\nproject of National Social Science Fund (I8ZD22).\n(Corresponding authors: Lizhuang Ma and Xiangtai Li.)\nRecommended for acceptance by XXX.\nDigital Object Identiﬁer no. 10.1109/TP AMI.2022.3223955.\nFig. 1: Speed and Accuracy trade-off of video object detection\n(VOD) results in ImageNet VID dataset. The blue points plot\nthe state-of-the-art (SOTA) VOD methods, and the red ones are\nour proposed method TransVOD Lite, achieving thebest trade-\noff between the speed and accuracy with different backbones.\nSwinB, SwinS and SwinT mean Swin Base, Small and Tiny.\ndeterioration and changes of video frames, e.g., motion blur,\npart occlusion, camera refocous and rare poses.\nPrevious VOD methods mainly leverage the temporal\ninformation in two different manners. The ﬁrst one relies\non post-processing of temporal information [5], [6], [7], [8]\narXiv:2201.05047v4  [cs.CV]  22 Nov 2022\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X 2\nto make the detection results more coherent and stable.\nThese methods usually apply a still-image detector to obtain\ndetection results, then associate the results. Another line\nof approaches [9], [10], [11], [12], [13], [14], [15], [16], [17]\nexploits the feature aggregation of temporal information.\nSpeciﬁcally, they improve features of the current frame by\naggregating that of adjacent frames or entire clips to boost\nthe detection performance via speciﬁc operator design. In\nthis way, the problems such as motion blur, part occlusion,\nand fast appearance change can be well solved. In particular,\nmost methods [15], [16], [17], [18] use two-stage detector\nFaster-RCNN [1] or R-FCN [2] as the still-image baseline.\nDespite the gratifying success of these approaches, most\nof the two-stage pipelines for video object detection are over\nsophisticated, requiring many hand-crafted components,\ne.g., optical ﬂow model [19], [20], [21], [22], [23], recurrent\nneural network [16], [18], [24], deformable convolution fu-\nsion [14], [25], [26], relation networks [16], [27], [28]. In\naddition, most of them need complicated post-processing\nmethods by linking the same object across the video to form\ntubelets and aggregating classiﬁcation scores in the tubelets\nto achieve the state-of-the-art performance [5], [6], [7], [8].\nMeanwhile, there are also several studies [9], [10], [26], [29],\n[30], [31] focusing on real-time video object detection. How-\never, these works still need sophisticated designs. Thus, it is\nin desperate need to builda simple yet effectiveVOD framework\nin a fully end-to-end manner.\nTransformers [32], [33], [34], [35], [36] have shown\npromising potential in computer vision. Especially,\nDETR [33], [34] simpliﬁes the detection pipeline by mod-\neling the object queries and achieving comparative perfor-\nmance with highly optimized CNN-based detectors. How-\never, such static detectors cannot handle motion blur, part\nocclusion, video defocus, or rare poses well due to the lack\nof temporal information, which will be shown in the experi-\nment part. Thus, how to model the temporal information in\na long-range video clip is a very critical problem.\nIn this paper, our goal is to extend the DETR-like object\ndetector into the video object detection domain. Our insights\nare four aspects. Firstly, we observe that the video clip\ncontains rich inherent temporal information, e.g., rich visual\ncues of motion patterns. Thus, it is natural to view video\nobject detection as a sequence-to-sequence task with the\nadvantages of Transformers [37]. The whole video clip is\nlike a sentence, and each frame contributes similarly to\neach word in natural language processing. Transformers\ncan not only be used in inner each frame to model the\ninteraction of each object, but also be used to link objects\nalong the temporal dimension. Secondly, object query is\none key component design in DETR [33] which encodes\ninstance-aware information. The learning process of DETR\ncan be seen as the grouping process: grouping each object\ninto an object query. Thus, these query embeddings can\nrepresent the instances of each frame, and it is natural to\nlink these sparse query embeddings via another temporal\ntransformer. Thirdly, the output memory from the DETR\ntransformer encoder contains rich spatial information which\ncan also be modeled jointly with query embeddings along\nthe temporal dimension. Fourthly, adopting clip-level inputs\nof Transformers can speed up the object detection process in\na video, which is needed in many real-world applications.\nMotivated by the above facts, we propose TransVOD,\na novel end-to-end video object detection framework\nbased on a spatial-temporal Transformer architecture. Our\nTransVOD views video object detection as an end-to-end\nsequence decoding/prediction problem. For the current\nframe, as shown in Fig. (2)(a), it takes multiple frames\nas inputs and directly outputs the current frame detection\nresults via a Transformer-like architecture. In particular, we\ndesign a novel temporal Transformer to link each object\nquery and outputs of memory encodings simultaneously.\nOur proposed temporal Transformer mainly contains three\ncomponents: Temporal Deformable Transformer Encoder\n(TDTE) to encode the multiple frame spatial details, Tem-\nporal Query Encoder (TQE) to fuse object queries in one\nvideo clip, and Temporal Deformable Transformer Decoder\n(TDTD) to obtain the ﬁnal detection results of the current\nframe. TDTE efﬁciently aggregates the spatial information\nvia temporal deformable attention and avoids the back-\nground noises. TQE ﬁrst adopts a coarse-to-ﬁne strategy\nto select relevant object queries in one clip and fuse such\nselected queries via several self-attention layers [37]. TDTD\nis another decoder that takes the outputs of TDTE and\nTQE as inputs, and directly outputs the ﬁnal detection\nresults. These modules are shared for each frame and can be\ntrained in an fully end-to-end manner. We carry out exten-\nsive experiments on ImageNet VID dataset [38]. Compared\nwith the single-frame baseline [34], our TransVOD achieves\nsigniﬁcant improvements (2%∼4% mAP).\nBased on the TransVOD framework, which is pub-\nlished in ACM MM 2021 [39], we present two improved\nversions including TransVOD++ and TransVOD Lite. For\nTransVOD++, regarding that there exists large redundancy\nin both the number of object queries and the targets, we\npresent a hard query mining (HQM) strategy to sample\nthe hardest queries during the training inspired from the\nhard pixels mining in image object detection and segmen-\ntation [3], [40], [41], as shown in Fig. 2(b). Moreover, we\npresent a novel query and RoI fusion (QRF) module via dy-\nnamic convolutions. In this way, the object-level appearance\ninformation is injected into each query and TDTE can be\navoided since the spatial fusion can be replaced with QRF.\nCompared with previous TransVOD, we ﬁnd both improve-\nments lead to better results with faster speed. Moreover,\nwhen deploying the vision Transformer backbone [42], we\npresent a simply-aligned fusion to fuse multi-scale features\nfor TDTD. After adopting Swin base as the backbone, our\nTransVOD++ achieves 90% mAP on the ImageNet VID\ndataset and suppress previous works by a signiﬁcant margin\n(5%∼6 %) with a simpler pipeline. Our method is the ﬁrst\nto achieve 90% mAP on ImageNet VID dataset .\nInherited from TranVOD, we present TransVOD Lite,\naiming at real-time VOD and modeling the VOD task\nas a sequence-to-sequence prediction problem which is\nadopted in machine translation [37]. The pipeline is shown\nin Fig. 2(c). In particular, given a window size T (T can be\nchosen in 8, 16), we take multiple frames as inputs and ob-\ntain multiple frame results simultaneously. Then, one video\nclip results can be obtained in a temporal window manner.\nIn this way, we can fully use the memory of GPU to speed\nup inference time. Our TransVOD Lite can boost the single\nimage baseline by 2%∼3% mAP but with a faster speed (4x-\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X 3\nCurrent Frame Spatial Transformer Feature Memory Share WeightReference Frame\n(a). TransVOD (b). TransVOD ++ (c). TransVOD Lite\nFt-4 Ft-2 Ft Ft+2 Ft+4\nF F ST Object Query\nResult\nST ST ST ST ST\nFt-4 Ft-2 Ft Ft+2 Ft+4\nST ST ST ST ST\nQuery and RoI Fusion\nResult\nHard Query MiningTemporal Transformer\nST\nVideo Clip\nResult\nSequential Hard Query Mining\nTemporal Transformer\nResultResult ResultResult\nTemporal Transformer\nFt Ft+1 Ft+2 Ft+3 Ft+4\nFig. 2: Illustration of our proposed TransVOD series. (a) Original TransVOD: our network is based on spatial Transformers which\noutputs spatial object query and feature memory of each frame. We propose a temporal Transformer to link both the spatial object\nqueries and feature memories in a temporal dimension to obtain the results of the current frame. The ﬁnal detection results are\nobtained via a shared feed-forward network (FFN). (b) Based on TransVOD, our TransVOD++ add two improvements including\nHard Query Mining (HQM) and Query and RoI Fusion module (QRF). (c) Inherited from TransVOD, our TransVOD Lite models\nthe VOD task as a sequence-to-sequence prediction problem, and directly outputs all the detection results of the entire sequence\nin the window via Sequential Hard Query Mining (SeqHQM).\n6x). After adopting the Swin Transformer, as shown in Fig. 1,\nour methods achieve the best speed and accuracy trade-\noff. Our methods lead to a signiﬁcant margin (3% ∼4%mAP ,\n5∼15 FPS) compared with previous VOD methods in both\nspeed and accuracy. Our best model can achieve 83.7% mAP\nwhile running at around 30 FPS. In summary, following\nthe TransVOD framework, we present TransVOD++ and\nTransVOD Lite. Both models set new state-of-the-art results\non the challenging ImageNet VID dataset in two different\nsettings: accuracy for non-real-time models and best speed-\naccuracy trade-off on real-time models. These results indi-\ncate our method can be new solid baseline for VOD.\n2 R ELATED WORK\nVideo Object Detection. VOD task requires detecting ob-\njects in each frame and linking the same objects across\nframes. State-of-the-art methods typically develop sophis-\nticated pipelines to tackle it. In general, VOD task can be\ndivided into two directions: improving detection accuracy via\ntemporal fusing and performing real-time video object detection\nwhile keeping the accuracy.\nFor the ﬁrst aspect, most previous works [9], [10], [11],\n[12], [13], [14], [15], [16], [17], [18], [20], [21], [22] to amend\nthis problem is feature aggregation that enhances per-frame\nfeatures by aggregating the features of nearby frames. Ear-\nlier works adopt ﬂow-based warping to achieve feature\naggregation. Speciﬁcally, FGFA [20] and THP [22] both\nutilize the optic ﬂow from FlowNet [43] to model the motion\nrelation via different temporal feature aggregation strate-\ngies. To calibrate the pixel-level features with inaccurate\nﬂow estimation, MANet [21] dynamically combines pixel-\nlevel and instance-level calibration according to the motion.\nNevertheless, these ﬂow-warping-based methods have sev-\neral disadvantages: 1) Training a model for ﬂow extraction\nrequires large amounts of ﬂow data, which may be difﬁcult\nand costly to obtain. 2) integrating a ﬂow network and a\ndetection network into a single model may be challeng-\ning due to multitask learning. Another line of attention-\nbased approaches [14], [16], [24], [25], [26], [44] utilize self-\nattention [45] and non-local [46] to capture long-range de-\npendencies of temporal contexts. SELSA [44] treats video as\na bag of unordered frames and proposes to aggregate fea-\ntures in the full-sequence level. STSN [25] and TCENet [14]\npropose to utilize deformable convolution to aggregate the\ntemporal contexts within a complicated framework with so\nmany heuristic designs. RDN [27] introduces a new design\nto capture the interactions across the objects in spatial-\ntemporal context. LWDN [26] adopts a memory mechanism\nto propagate and update the memory feature from key\nframes to key frames. OGEMN [24] present to use object-\nguided external memory to store the pixel and instance-\nlevel features for further global aggregation. MEGA [16]\nconsiders aggregating both the global information and lo-\ncal information from the video and presents a long-range\nmemory. Despite the great success of these approaches, most\nof the pipelines for VOD are too sophisticated, requiring\nmany hand-crafted components, e.g., extra optic ﬂow model,\nmemory mechanism, or recurrent neural network. In addi-\ntion, most of them need complicated post-processing meth-\nods such as Seq-NMS [5], Tubelet rescoring [6], Seq-Bbox\nMatching [7] or REPP [8] by linking the same object across\nthe video to form tubelets and aggregating classiﬁcation\nscores in the tubelets to achieve the state-of-the-art. Instead,\nour previous work TransVOD builds a simple and end-to-end\ntrainable VOD framework without these designs. Beyond\nthat, our improved version TransVOD++ incorporates more\nappearance information into the object query design and\nsimpliﬁes the whole pipeline by removing the temporal\nencoder (TDTE) of original TransVOD. It achieves better re-\nsults than TransVOD and the state-of-the-art performances\non the ImageNet VID dataset.\nFor the second aspect, starting from DFF [19], several\nworks [9], [10], [26], [29], [30], [31], [47], [48] focus on\nreal-time video object detection while keeping accuracy\nunchanged or even improved. In general, most of these\nworks also perform speciﬁc architecture designs with many\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X 4\nhand-crafted components and human prior such as object-\nlevel tracker in [9], patchwork cell with attention in [49] and\nConvolutional LSTMs in [30]. Our proposed TransVOD Lite\nmodels the entire VOD pipeline as a sequence to sequence\nproblem, as Transformer did in machine translation [37]. It\nachieves signiﬁcant improvements over the strong image\nbaseline along with a faster speed.\nVision Transformers. Recently, vision Transformers [33],\n[34], [35], [36], [42], [50] make a great progress. It can be\nmainly divided into two directions: replacing CNN back-\nbone with Transformer-Like architecture [35], [42], [51], [52]\nand using object query to represent instance for scene under-\nstanding [33], [34], [53], [54], [55]. Our work is related to the\nsecond part. DETR [33] builds a fully end-to-end object de-\ntection system based on Transformers, which largely simpli-\nﬁes the traditional detection pipeline. It also achieves on par\nperformances compared with highly-optimized CNN-based\ndetectors [1]. However, it suffers from slow convergence and\nlimited feature spatial resolution, Deformable DETR [34] im-\nproves DETR by designing a deformable attention module,\nwhich attends to a small set of sampling locations as a pre-\nﬁlter for prominent key elements out of all the feature map\npixels. Our work is inspired by DETR [33] and Deformable\nDETR [34]. The above works show the effectiveness of\nTransformers in image object detection tasks. There are sev-\neral con-current works that applied Transformer into video\nunderstanding, e.g., video instance segmentation (VIS) [56],\nmulti-object tracking (MOT). TransTrack [36] introduces a\nquery-key mechanism into the multi-object tracking model,\nwhile Trackformer [50] directly adds track query for MOT.\nHowever, both only leverage limited temporal information,\ni.e., just the previous frame. We suppose that this way\ncan not fully use enough temporal contexts from a video\nclip. VisTR [32] views the VIS task as a direct end-to-end\nparallel sequence prediction problem. The targets of a clip\nare disrupted in such an instance sequence, and directly per-\nforming target assignment is not optimal. Instead, we aim to\nlink the outputs of the spatial Transformer, i.e., object query,\nthrough a temporal Transformer, which acts in a completely\ndifferent way from VisTR [32]. To our knowledge, there\nare no prior applications of Transformers to video object\ndetection (VOD) tasks so far. It is intuitive to see that the\nTransformers’ advantage of modeling long-range dependen-\ncies in learning temporal contexts across multiple frames for\nVOD task. Our previous work, TransVOD [39], leverages\nboth the spatial Transformer and the temporal Transformer,\nand then provide an afﬁrmative answer to that. In this\npaper, based on the TransVOD framework, we provide two\nextra solutions including TransVOD++ and TransVOD Lite.\nThe former aims to improve the performance of TransVOD\nwhile keeping inference efﬁciency, while the latter carry out\nreal-time VOD detection with much faster inference speed.\n3 M ETHOD\nOverview. We ﬁrst review the previous works, including\nboth DETR [33] and Deformable DETR [34] in Sec. 3.1. Then,\nwe give detailed descriptions of our proposed TransVOD\nframework in Sec. 3.2. It contains three key components:\nTemporal Deformable Transformer Encoder (TDTE), Tempo-\nral Query Encoder (TQE), and Temporal Deformable Trans-\nformer Decoder (TDTD). Then, we present two advanced\nversions of our TransVOD including TransVOD++ (Sec. 3.3\n) and TransVOD Lite (Sec. 3.4). Finally, we describe the loss\nfunctions and details of inference in Sec. 3.5.\n3.1 Revisiting DETR and Deformable DETR\nDETR [33] treats object detection as a set prediction prob-\nlem. A CNN backbone [57] extracts visual feature maps\nf ∈RC×H×W from an image and H,W are the height and\nwidth of the visual feature map, respectively. The visual\nfeatures augmented with position embedding fpe would\nbe fed into the encoder of the Transformer. Self-attention\nwould be applied tofpe to generate the key, query, and value\nfeatures K,Q,V to exchange information between features\nat all spatial positions. Let Ωq and Ωk indicate the set of\nquery and key elements, respectively. Then, q ∈Ωq denotes\nthe query element and k ∈ Ωk denotes the key element,\nrespectively, which indexes the query feature zq ∈RC, and\nkey feature xk ∈RC, where Cdenotes the dimension of the\nfeature. Then, the multi-head attention feature is as follows:\nMultiHeadAttn(zq,x) =\nM∑\nm=1\nWm\n[ ∑\nk∈Ωk\nAmqk ·W′\nmxk\n]\n,\n(1)\nwhere m indexes the attention head, W′\nm ∈ RCv×C and\nWm ∈ RC×Cv are learnable weights ( Cv = C/M by\ndefault). The attention weights Amqk are normalized as:\nAmqk ∝exp{zT\nq UT\nm Vmxk\n√Cv\n},\n∑\nk∈Ωk\nAmqk = 1, (2)\nwhere Um,Vm ∈RCv×C are learnable weights. The features\nzq and xk are the concatenation/summation of element\ncontents and positional embeddings in practice. The de-\ncoder’s output features of each object query are then further\ntransformed by a Feed-Forward Network (FFN) to output\nclass score and box location for each object. Given box\nand class prediction, the Hungarian algorithm is applied\nbetween predictions and ground-truth box annotations to\nidentify the learning targets of each object query for one-\nto-one matching. Deformable DETR [34] replaces the multi-\nhead self-attention layer with a deformable attention layer\nto efﬁciently sample local pixels rather than all pixels. More-\nover, to handle missing small objects, they also propose a\ncross-attention module that incorporates multi-scale feature\nrepresentations. Due to the fast convergence and computa-\ntion efﬁciency, we adopt Deformable DETR [34] as our still\nimage Transformer detector.\n3.2 TransVOD Framework\nThe overall TransVOD architecture is shown in Fig. 3. It\ntakes multiple frames of a video clip as inputs and outputs\nthe detection results for the current frame. It contains four\nmain components: Spatial Transformers for single frame\nobject detection, extracting both object queries and compact\nfeatures representation (memory for each frame), Temporal\nDeformable Transformer Encoder (TDTE) to fuse memory\noutputs from Spatial Transformers, Temporal Query En-\ncoder (TQE) to link objects in each frame along the temporal\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X 5\nTop k1\nTop k2\nTop kn\n    \n    \nSTE\nInitial object query\nSpatial object query\nFt-i+n\nFt-i+1\nFt-i\nFt\n···\nReference Frame \nCurrent Frame \nSTD\nFeature memory\nSTE\nSTD\nSTE\nSTD\nSTE\nSTD\nCNNCNNCNNCNN\nSorting by Score\nSelection\nPrediction\nTQE\nTDTD\nTQE\nTQE\nTemporal object query Shared weight\nTDTE\n… …\nConcatenate\n    \n    \nTQE\nSelf-Attention\nDeformable Attention\nFeed-Forward Network\nSelf-Attention\nCross-Attention\nFeed-Forward Network\nTDTD\n    \nTDTE\nSelf-Attention\nTempDeformAttention\nFeed-Forward Network\ncurrent queryreference query\nreference memory current memory\nfused memorytemporal query\nFFN FFN\nSTE Spatial Transformer Encoder\nSTD Spatial Transformer Decoder\nCls        Bbox\nQuery flow\nMemory flow\n··· ··· ···\nFig. 3: The whole pipeline of TransVOD. A shared CNN backbone extracts features of multiple frames. Next, a series of\nshared Spatial Transformer Encoders (STE) produce the feature memories and these memories are linked and fed into Temporal\nDeformable Transformer Encoder (TDTE). Meanwhile, the Spatial Transformer Decoder (STD) decodes the spatial object queries.\nNaturally, we use a Temporal Query Encoder (TQE) to model the relations of different queries and aggregate these queries, thus\nwe can enhance the object query of the current frame. Both the temporal object query and the temporal feature memories are fed\ninto the Temporal Deformable Transformer Decoder (TDTD) to learn the temporal contexts across different frames. Our TransVOD\nframework can be trained in a fully end-to-end manner.\ndimension and Temporal Deformable Transformer Decoder\n(TDTD) to obtain ﬁnal outputs for the current frame.\nSpatial Transformer. We use Deformable DETR [34] as\nour still image detector. In particular, to simplify complex\ndesigns in [34], we do not use multi-scale features in both\nTransformer encoders and decoders. We only use the last\nstage of the backbone as the input of the deformable Trans-\nformer. The modiﬁed detector includes Spatial Transformer\nEncoder (STE) and Spatial Transformer Decoder (STD),\nwhich encodes each frame F (including Reference Frame\nand Current Frame) into two compact representations: spa-\ntial object query Qand memory encoding E.\nTemporal Deformable Transformer Encoder. The goal of\nTDTE is to encode the spatial-temporal feature representa-\ntions and provide the location cues for the ﬁnal decoder out-\nput. Since most adjacent features contain similar appearance\ninformation, directly using naive Transformer encoders [33],\n[37] may bring much extra computation (much useless com-\nputation on object background). Deformable attention [34]\nsamples only partial information efﬁciently according to\nthe learned offset ﬁeld. Thus, we can link these memory\nencodings Et through this operation in a temporal dimen-\nsion. The core idea of the temporal deformable attention\nmodules is that we only attend to a small set of key sampling\npoints around a reference efﬁciently. Thus, TDTE receives\nthe feature memories of the reference frame and the current\nframe as inputs, and outputs the enhanced current memory.\nThe multi-head temporal deformable attention (TempDefor-\nmAttn) is as follows:\nTempDeformAttn(zq,ˆpq,{xl}L\nl=1) =\nM∑\nm=1\nWm\n[ L∑\nl=1\nK∑\nk=1\nAmlqk\nxl(φl( ˆpq) + ∆pmlqk)\n]\n,\n(3)\nwhere m indexes the attention head, l indexes the frame\nsampled from the same video clip, and k indexes the\nsampling points, and ∆pmlqk and Amlqk indicate the sam-\npling offset and attention weights of the kth sampling point\nin the lth frame and the mth attention head, respectively.\nAmlqk denotes the scalar attention weight in the range of\n[0,1], normalized by ∑L\nl=1\n∑K\nk=1 Amlqk = 1. ∆plmqk ∈R2\nare of 2-d real numbers with unconstrained range. Since\npq + ∆pmlqk is fractional, we apply bilinear interpolation\nin [58] for computing x(pq + ∆pmlqk). For each frame l,\nboth ∆pmlqk and Amlqk are calculated by feeding the query\nfeature zq to a linear projection of 3MK channels, where the\nﬁrst 2MK channels encode the sampling offsets ∆pmlqk,\nand the remaining MK channels are fed to a Softmax\nfunction to obtain the attention weights Amlqk. Here, we\nuse normalized coordinates ˆpq ∈ [0,1]2 for the clarity of\nscale formulation, in which (0,0) and (1,1) indicate the top-\nleft and the bottom-right image corners, respectively. φl(ˆpq)\nre-scales the normalized coordinates ˆpq to the input feature\nmap of l-th frame. The multi-frame temporal deformable\nattention samples LK points from L feature maps instead\nof Kpoints from single-frame feature maps. There exist total\nM attention heads in each TDTE layer.\nTemporal Query Encoder. As mentioned in the previous\npart, learnable object queries can be regarded as the non-\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X 6\ngeometric anchors, which automatically learns the statistical\nfeatures of the whole still image datasets during the training\nprocess. It means that the spatial object queries are not\nrelated to temporal contexts across different frames. Thus,\nwe propose a simple yet effective encoder to measure the\ninteractions between the objects in the current frame and\nthe objects in reference frames.\nOur key idea is to link these spatial object queries in each\nframe via a temporal Transformer, and thus learn the tem-\nporal contexts across different frames. We name our module\nTemporal Query Encoder (TQE). TQE takes all the spatial\nqueries from reference frames to enhance the spatial output\nquery of the current frame, and it outputs the temporal\nquery for the current frame. Moreover, inspired from [27],\nwe design a coarse-to-ﬁne spatial object query aggregation\nstrategy to progressively schedule the interactions between\nthe current object query and the reference object queries.\nThe beneﬁt of such a coarse-to-ﬁne design is that we can\nreduce the computation cost to some extent.\nSpeciﬁcally, we combine the spatial object query from\nall reference frames, denoted as Qref . Then, we perform\nthe scoring and selection in a coarse-to-ﬁne manner. In\nparticular, we use an extra Feed Forward Networks (FFN)\nto predict the class logits, which are jointly trained with\nthe spatial Transformers and the parameters are ﬁxed when\nﬁne-tuning the temporal Transformers. After that, we get the\nsigmoid value of that: p= Sigmoid[FFN (Qref )]. Then, we\nsort all the reference points by p value and select the top-\nconﬁdent k values from these reference points. The higher\np score means most likely objects and trained jointly with\nclassiﬁcation loss. The prediction head is only trained for\nimage object detection and is ﬁxed for the training of video\nobject detection. As most current DETR-like detectors [33],\n[34] use the cascaded heads to reﬁne detection results, we\nadopt a similar coarse-to-ﬁne design to select less but precise\nobject queries in the latter stages since most queries are not\nused and duplicated in the latter stages.\nAs shown in the blue part of Fig. 3, TQE includes a\nself-attention layer, cross-attention, and FFN. The temporal\nobject queries are progressively reﬁned and interacted with\nthe spatial object queries extracted from different frames,\ncalculating the co-attention between the reference queries\nand the query feature of the current frame. Note that the\ncross-attention plays the role of a cascade feature reﬁner\nwhich updates the output queries of each spatial Trans-\nformer iteratively. As such, TQE receives the object queries\nof the reference frames and the current frame as inputs and\noutputs the reﬁned temporal object query of the current\nframe.\nTemporal Deformable Transformer Decoder. This decoder\naims to obtain the current frame output according to both\noutputs from TDTE (fused memory encodings) and TQE\n(temporal object queries). Given the aggregated feature\nmemories ˆE and the temporal queries ˆOq, our Tempo-\nral Deformable Transformer Decoder (TDTD) performs co-\nattention between online queries and the temporal aggre-\ngated features. The deformable co-attention [34] of the tem-\nporal decoder layer is shown as follows:\nDeformAttn(zq,pq,x) =\nM∑\nm=1\nWm\n[ K∑\nk=1\nAmqk\n·W′\nmx(pq + ∆pmqk)\n]\n, (4)\nwhere m indexes the attention head, k indexes the sam-\npled keys, and K is the total number of the sampled\nkeys ( K ≪ HW). pmqk and Amqk indicate the sampling\noffset and attention weight of the kth sampling point in\nthe mth attention head, respectively. The attention weight\nAmqk ∈[0,1], normalized by ∑K\nk=1 Amqk = 1. ∆pmqk ∈R2\nare of 2-d real numbers with unconstrained range. Due to\nthe fact that pq + ∆pmqk is fractional, we also adopt bilinear\ninterpolation in computing x(pq + ∆pmqk) following [58].\nBoth ∆pmqk and Amqk are obtained via linear projection\nover the query feature zq. In our implementation, the query\nfeature zq is fed to a linear projection operator. The output\nof TDTD is sent to one feed-forward network (FFN) for the\nﬁnal classiﬁcation and box regression as the detection results\nof the current frame.\n3.3 TransVOD++\nCompared with previous work, despite TransVOD simpli-\nfying the pipeline of VOD, it has several limitations. Firstly,\nit contains heavy computation costs in TDTE. Secondly,\nthe performance of TransVOD is still limited. To solve\nthese problems, we present TransVOD++ which contains the\nfollowing improvements including Query and RoI Fusion\n(QRF), Hard Query Mining (HQM), and a strong backbone.\nThe pipeline is shown in Fig 4.\nQuery and RoI Fusion. Previous works [16], [59] show that\nregion features are useful and contain precise appearance\ninformation for temporal fusion. Our motivation is to re-\nplace TDTE with features in region of interest (RoI) via the\nproxy strategy where each RoI feature is injected into each\nquery, thus utilizing the object-level appearance information\nto enhance the object query.\nSpeciﬁcally, given the detection boxes from spatial Trans-\nformers, we get the region of interest (RoI) of each frame in\na video clip. Then, according to those RoIs and the feature\nfrom the STE, we calculate the RoI feature ERoI\ncur and ERoI\nref\nof the current frame and the reference frames, respectively.\nNext, the cropped RoI features are used to weigh each query\nvia the transformation of MLP , as shown in the green part of\nFig. 4. The current RoI feature ERoI\ncur is aggregated onto the\nobject query of the current frame to generate the enhanced\ncurrent query feature ˆQcur, where feature aggregation is\nconducted through dynamic convolutions.\nˆQj+1\ncur =\n{ QRF(Qj\ncur,ERoI\ncur ), if j = 1\nQRF( ˆQj\ncur,ERoI\ncur ), otherwise (5)\nwhere Qj\ncur is the spatial object query of the current frame\nbefore the jth temporal query encoder (TQE), and ˆQj\ncur de-\nnotes the temporal object query before the jth TQE module.\nSimilarly, for each reference frame, the reference RoI features\nERoI\ncur of the ith frame are fused with the reference query of\nthe ith frame via QRF.\nThe details of the QRF module are described as follows:\ngiven the object query and RoI feature memory, we ﬁrst\nfeed the object query to a multi-head self-attention layer to\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X 7\n    \nTop k1\nTop k2\nTop kn\n    \n    \nSTE\nInitial object query\nSpatial object query\nFt-i+2\nFt-i+1\nFt-i\nFt\n···\nPrediction\nFused query\nReference Frame \nCurrent Frame \nSTD\nQuery and ROI Fusion (QRF)\nFeature memory\nSTE\nSTD\nSTE\nSTD\nSTE\nSTD\nCNNCNNCNNCNN\nSort by Score\nSelection\nFFN\nFinal\nPrediction\nFFN\nTQE\nTDTD\nTQE\nTDTD\nTQE\nTDTD\n···\nHard Query Mining  (HQM)\nTemporal object queryShared weight\nLaux\nQFH\nQFH\nPredictionFFN\n    Self-Attention\nDeformable Attention\nFeed-Forward Network\nTDTD\ncurrent memorytemporal query\n    \nQFH\nSorting\nSelection\nSigmoid\nclass logits query\nTop k\nfiltered query \n    \nQRF\nDynamic Conv\n1 2\n31 2 3 ROI\n1 2 3\n1 2 3\nCropSelf-Attention\nspatial query feature memory \nFig. 4: The whole pipeline of TransVOD++. Compared with the original TransVOD, TransVOD++ adds the Query and RoI\nFusion (QRF) and Hard Query Mining (HQM) module. To avoid redundant spatial information in TDTE, we present QRF by\nfully injecting the object-level appearance information into each object query. Then, to dynamically reduce the query number and\ntarget number, we present HQM for mining the hardest query with multiple TDTD modules and multiple auxiliary TDTD losses.\nreason about the relations between objects. Then, each RoI\nfeature will interact with the corresponding object query to\nﬁlter out ineffective bins and outputs the ﬁnal object query.\nInspired from [60], we carry out two consecutive 1 ×1\nconvolutions with ReLU activation function for light design.\nThe kth object query generates dynamic parameters of these\ntwo convolutions for the corresponding kth RoI feature via\na linear projection. Finally, the aggregated reference queries\nˆQj\nref are used to enhance the aggregated current query ˆQj\ncur\nvia a TQE, thus learning the temporal contexts across differ-\nent frames, which is described as: ˆQj\ncur = TQE( ˆQj\ncur, ˆQj\nref )\nHard Query Mining. Considering that both the spatial\nobject queries and temporal object queries contain much\nredundant information across the dataset, for example, 300\nqueries reﬂect the temporal appearance distributions of 30\ncategories, and those queries need to match more than 300\nground truths during the training procedure, and there is no\nneed to maintain so many object queries/targets in both the\nspatial and temporal dimension. As such, we are motivated\nto dynamically reduce the redundancy of query number and\ntarget number in the training of temporal Transformers, and\nmeanwhile, we mine the hardest query in both the current\nframe and the reference frames.\nConcretely, given the spatial object query Qref of the\nreference frames, we fed it into a Query Filter Head (QFH),\nwhich ﬁlters the redundant object query and select the\nmost conﬁdent ones to reduce the computation redundancy.\nSpeciﬁcally, Qref are fed forward to the class embedding\nlayer of the spatial Transformer, i.e., a linear classiﬁcation\nlayer with sigmoid activation, to generate class logits. Then,\nthose reference queries are concatenated in the dimension\nof the query number. Next, according to the probability\nof the reference logits, we sort and then select the top k\nconﬁdent query, which is illustrated in the salmon part of\nFig. 4. Inherited from TransVOD, we adopt the coarse-to-\nﬁne query aggregation strategy to progressively model the\nrelationships between the current query and the reference\nqueries via TQE module.\nThe differences between TransVOD++ and TransVOD lie in\nseveral aspects. Firstly, in contrast to TransVOD that only\nselects the reference query, our TransVOD++ selects not\nonly the reference query but also the current query. Both of\nthem are treated differently in a coarse-to-ﬁne manner, thus\nreducing the computation cost in the temporal Transformer.\nSecondly, compared to TransVOD, we add a Temporal\nDefomrable Transformer Decoder (TDTD) after each TQE\nmodule and supervise the object query with different query\nnumbers via an auxiliary TDTD loss , denoted as Laux.\nWe ﬁnd it helpful to use auxiliary TDTD losses Laux in\ntemporal Transformer during training, especially to help the\nmodel output the correct number of objects of each class. We\nadd prediction FFNs and Hungarian loss after each TDTD\nmodule. All prediction FFNs share their parameters.\nStrong Backbone. We further adopt Swin Transformer\nas the strong backbone network. However, Swin Trans-\nformer [42] generates multi-scale features adopted with\nFPN-like framework [61] which is not suitable for our\nTransVOD framework. We propose a simple yet effective\nsolution via fusing multi-scale features into one scale where\nwe directly add multi-scale features into one scale.\n3.4 TransVOD Lite\nDespite TransVOD and TransVOD++ make the VOD\npipeline much simpler, the inference time is still limited due\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X 8\n                    \nSTE\nFt+2\nFt+1\nFt\nFt+i\n···\nPrediction\nVideo Clip\nSTD\nFFN\nFinal\nPrediction\nTQE\nTDTD\nTQE\nTDTD\nTQE\nTDTD\n···\nSequential Hard Query Mining\n(SeqHQM)\nCNN\nInitial object query\nSpatial object query\nTemporal object query\nFt+2\nFt+1\nFt\nFt+i\n···\n   Prediction\nClip\nMemory\n Clip\nQuery Clip\nQFH\nQFH\nPredictionFFN\nFFN\n    \nQuery Filter Head \n(QFH)\nSorting\nLaux\nSelection\nSigmoid\nclass logits\nMax\n object query\nTop k\nfiltered query \nFeature Memory\nFig. 5: The whole pipeline of TransVOD Lite. Compared with TransVOD and TransVOD++, TransVOD Lite aims at real-time\nvideo object detection. It takes multiple frames as inputs and outputs all the results simultaneously. We propose Sequential Hard\nQuery Mining (SeqHQM) to mine the hardest query in a video clip for selectively reducing the redundancy of sequential object\nqueries and targets in the training of temporal Transformers via Query Filter Head (QFH) and auxiliary TDTD losses Laux.\nto multiple frame query fusing. As mentioned in Section 2,\nthe inference time is critical for real-world applications.\nTo embrace the advantage of modeling sequence data in\nTransformer [32], [45], we present TransVOD Lite where it\ntakes multi frames as inputs and output detection results of\nall frames directly, as shown in Fig. 5.\nDirect Multiple Frame Predictions. In TransVOD Lite, we\nabandon the feature aggregation paradigm, which requires\nmuch more computation costs in terms of time and memory\nspace. Instead, a sequence of video clips is fed as input\nand output a sequence of results. As shown in Fig. 5,\nTransVOD Lite inherits the Hard Query Mining from the\nTransVOD++ and spatial-temporal transformer design in\nTransVOD including TQE, and TDTD. The main difference\nis that TransVOD Lite directly outputs the multiple frame\nprediction with a hyper-parameterTw which is the temporal\nwindow size of the input clip or the number of the input\nframes. When Tw is larger, the inference speed is faster while\nthe memory is increased. In this way, we can fully use the\nmemory of GPU to speed up the inference time. We provide\ndetailed experiments on the effect of choosing Tw in the\nexperiment part.\nSequential Hard Query Mining. Different from TransVOD\nand TransVOD++, we do not need to discriminate whether\nan object query is the reference query or the current query\nfor ﬁltering, all object queries in the whole sequence are\nequally selected in a coarse-to-ﬁne manner, thus increasing\nthe speed, e.g., FPS, to Tw times in temporal Transformer\nthan original TransVOD, where Tw denotes the temporal\nwindow size in a given clip. We name our method “sequen-\ntial hard query mining” (SeqHQM). For example, Tw = 12\nmeans the input frames are 12 in the video clip, and then\nwe need to generate the results of those 12 frames, if each\nframe has 300 object queries, there are 3600 object queries\nin total. There is no doubt that there exists large redundant\ninformation of those large number of object queries, and it\nis necessary to dynamically reduce the computation costs to\nboost the inference speed, as well as achieve good results in\nmodeling the temporal motion.\nWe then describe SeqHQM in detail. Speciﬁcally, a se-\nquence of spatial object query Qseq, is fed forwarded into a\nQuery Filter Head (QFH) to select the most credible object\nqueries. The number of object queries and targets is dynami-\ncally decreasing to reduce the computation redundancy. For\nTransVOD Lite, we implement the QFH differently before\nthe kth TQE module. If k = 1, we use the class embedding\nlayer of the spatial Transformer to generate class logits and\ngo through a sigmoid activation function, which is similar\nas QFH in TransVOD ++. If k> 1, the class logits are gener-\nated through the learnable temporal class embedding layer\nthen with a sigmoid activation function. Next, we compute\nthe maximum probability and select the top k conﬁdent\nquery by sorting and selection in a coarse-to-ﬁne manner,\nwhich is illustrated in the green part of Fig. 5. Similar to\nTransVOD++, we add a TDTD after each TQE module and\nsupervise the object query with different query numbers via\nan auxiliary TDTD loss , denoted as Laux. Laux is essential\nto help the model output the correct number of objects of\neach class. We add prediction FFNs and Hungarian loss\nafter each TDTD module. All prediction FFNs share their\nparameters.\n3.5 Loss Functions and Inference\nLoss functions. Original DETR [33] avoids post-processing\nand adopts a one-to-one label assignment rule. Follow-\ning [33], [34], [62], we match predictions from STD/TDTD\nwith ground truth by Hungarian algorithm [63] and thus the\nentire training process of spatial Transformer is the same\nas original DETR. The temporal Transformer uses similar\nloss functions given the box and class prediction output by\nthe FFN. The matching cost is deﬁned as the loss function.\nFollowing [33], [34], [60], the loss function is:\nLaux =\nJ∑\nj=1\n[\nλcls ·Lcls + λL1 ·LL1 + λgiou ·Lgiou\n]\n, (6)\nwhere J denotes the total number of TDTD modules in the\ntemporal Transformers, where J = 1 for TransVOD and J =\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X 9\n3 for TransVOD++ and TransVOD Lite in all experiments.\nLcls represents focal loss [3] for classiﬁcation. LL1 and\nLgiou represent L1 loss and generalized IoU loss [64] in for\nlocalization. λcls, λL1 and λgiou are coefﬁcients of them. We\nbalance these loss functions following the same setting in\n[34]. For TransVOD Lite, we apply such a loss function for\nall input frames.\nInference for TransVOD Lite. In TransVOD Lite, the win-\ndow size of a given video is deﬁned as Tw and the interval\nbetween the two adjacent frames within one clip is denoted\nas Iw, respectively. Given a videoV = {F1,F2,··· ,FN }, we\nﬁrst expand the video size to the integer multiples of Tw as:\nˆN = ⌈N\nTw\n⌉Tw. Then, for each expanded video, we divide the\nvideo into two parts and adopt different sampling strategies\nfor these two parts.\nAs for the ﬁrst part, the clip is normal where the interval\nof different frames is Iw. The index of the ﬁrst frame in each\nvideo clip is S = TwIwi+ j, where i ∈{0,1,··· ,K −1},\nj ∈{1,··· ,Iw −1}, K = ⌊\nˆN\nTwIw\n⌋. We feed the normal clip\nsequentially with window size Tw and interval size Iw into\nthe model. For the second part, the frames are not divisible\nby TwIw. The index of the ﬁrst frame is the clip is Twk+ 1.\nThere are ˆN −TWk frames in this clip. Those frames are\nrandomly divided into\nˆN\nTw\n−KIw video clips, with the size\nof each clip as Tw.\nBesides, we introduce another sampling strategy us-\ning random shufﬂing. We ﬁnd that if we ﬁrst randomly\nshufﬂe ˆv and split it to\nˆN\nTw\nclips, our model could model\nthe temporal motions better due to the large view of the\nvideo. The empirical evidence perceived by the human\nvisual system illustrates that when people are not certain\nabout the identity of an object, they would seek to ﬁnd a\ndistinct object from other frames that share high semantic\nsimilarity with the current object and assign them together.\nRegarding that Transformers are effective in modeling long-\nrange dependencies, if we randomly shufﬂe the video, we\ncould increase the data diversity and fully utilize the global\ninformation of the video. The effectiveness of both strategies\nis demonstrated in Sec. 4.3.3.\n4 E XPERIMENT\n4.1 Experimental Setup\nDatasets: We empirically conduct experiments on the Ima-\ngeNet VID dataset [38] which is a large-scale benchmark for\nvideo object detection. It contains 3862 training videos and\n555 validation videos with annotated bounding boxes of 30\nclasses. Since the ground truth of the ofﬁcial testing set is not\npublicly available, following common VOD protocols [20],\n[21], [27], [44], we train our models using a combination of\nImageNet VID and DET datasets [38] and measure the per-\nformance on the validation set using mean average precision\n(mAP) metric.\nNetwork architectures: In this work, we use Deformable\nDETR [34] as the image detector, and following [9], [29],\n[65], the detector is pre-trained on the COCO dataset [66].\nFollowing the widely used implementation protocols in\nprevious works [20], [21], [27], [44], We use ResNet-50 [57]\nand ResNet-101 [57] as the network backbone. Besides,\nwe also adopt Swin Transformer [42] as the backbone for\nbetter performances, which uses the same hyper-parameters\nas ResNet backbone. Note that we do not use the multi-\nscale features of the FPN-like network and fuse the multi-\nscale features by adding into the largest scale. All these\nbackbones are pre-trained on ImageNet [67] dataset. More\nimplementation details could be referred in our code link.\nTraining details: Following Deformable DETR [34], we\nuse the AdamW [68] optimizer, the initial learning rate is\n2 ×10−4 for Transformers, and 2 ×10−5 for the backbone,\nand weight decay is 10−4. All Transformer weights are\ninitialized with Xavier [69]. The number of initial object\nqueries is set as 300 for ResNet [57] and 100 for Swin\nTransformer [42]. During the training, the batch size is 1,\nand the number of reference frames is 14 for TransVOD\nand TransVOD++ in all experiments. Following the sam-\npling strategy in MMTracking [70], we adopt the bilateral\nuniform sampling for reference frames, which means ref-\nerence images are randomly sampled from the two sides\nof the nearby frames of the current frame. For TransVOD\nlite, the total frames of the video clip are sequentially fed\ninto the model. In all experiments, we use the same data\naugmentation as MEGA [16], including random horizontal\nﬂip, randomly resizing. We train the model for 14 epochs\nin an end-to-end manner. For better convergence, we ﬁrst\ntrain the spatial Transformers for 7 epochs and then ﬁne-\ntune the temporal Transformers for another 7 epochs. In\nthe ﬁne-tuning process, we freeze the parameters of spatial\nTransformers and only optimize the temporal Transformers.\nInference details: The inference runtime (FPS) of Table 2 is\ncalculated on a single V100 GPU card. During the inference\nphase, the batch size is 1 and we sample the reference frame\nwith a ﬁxed frame stride for TransVOD and TransVOD++.\nAs mentioned in Sec 3.5, we sample the frames via random\nshufﬂing for TransVOD Lite. During the inference phase, we\nuse the same data augmentation as MEGA [16] for image\nresizing such that the shortest side of the image is at least\n600 while the longest is at most 1000. Note that we do\nnot need any sophisticated post-processing method, which\nlargely simpliﬁes the pipeline of VOD.\n4.2 Main Results\nWe ﬁrst compare our proposed TransVOD and TransVOD++\nusing ResNet-50 backbone in Table 1. Then we present\nthe detailed results with the previous state-of-the-art VOD\nmethods in Table 3. Finally, we compare the real-time VOD\nmodels in Table 2.\nResults using ResNet-50 backbone. Table 1 shows the\ncomparison results with the state-of-the-art VOD methods\nwith ResNet-50 backbone. For a fair comparison, we also\nreport the performance of existing VOD methods that use\nthe COCO-pretraining model. Despite the use of COCO-\npretraining weights boosts the mAP of existing VOD meth-\nods, our proposed TransVOD still achieves superior perfor-\nmance against the state-of-the-art methods by a large mar-\ngin. In particular, TransVOD achieves 79.9 % with ResNet-\n50, which makes 1.3% ∼2.6% absolute improvements over\nthe best competitor MEGA [16]. Moreover, our proposed\nTransVOD++ further improves the original TransVOD by\n0.6%, achieving 80.5% on the ImageNet VID validation set.\nResults with stronger backbone. We further report stronger\nbackbone results to compare with the state-of-the-art meth-\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X 10\nTABLE 1: Comparison with state-of-the-art methods on\nImageNet VID with Res50 backbone. † means COCO pre-\ntraining.\nMethods Base Detector mAP (%)\nSingle Frame Baseline [1] Faster-RCNN 71.8\nDFF [19] Faster-RCNN 70.4\nFGFA [20] Faster-RCNN 74.0\nRDN [27] Faster-RCNN 76.7\nMEGA [16] Faster-RCNN 77.3\nSingle Frame Baseline† [1] Faster-RCNN† 72.7\nDFF† [19] Faster-RCNN† 71.6\nFGFA† [20] Faster-RCNN† 75.1\nRDN† [27] Faster-RCNN† 77.6\nMEGA† [16] Faster-RCNN† 78.3\nSingle Frame Baseline [34] Deformable DETR 76.0\nTransVOD Deformable DETR 79.9\nTransVOD++ Deformable DETR 80.5\nTABLE 2: Performance comparison with state-of-the-artreal-\ntime VOD methods on ImageNet VID validation set. In terms\nof both accuracy and speed, Our method outperforms most\nof them and has fewer parameters than existing models.\nModel mAP (%) Runtime\n(FPS)\n#Params\n(M) Backbone\nDFF [19] 73.1 20.25 97.8 Res101\nD &T [71] 75.8 7.8 - Res101\nLWDN [26] 76.3 20 77.5 Res101\nOGEMNet [24] 76.8 14.9 - Res101\nTHP [22] 78.6 13.0 - Res101+DCN\nRDN [27] 81.8 10.6 - Res101\nSELSA [44] 80.3 7.2 - Res101\nLRTR [28] 80.6 10 - Res101\nPSLA [18] 77.1 18.7 63.7 Res101\nPSLA [18] 80.0 13.3 72.2 Res101+DCN\nLSTS [10] 77.2 23.0 64.5 Res101\nLSTS [10] 80.1 21.2 65.5 Res101+DCN\nTransVOD Lite 80.5 32.3 74.2 Res101\nTransVOD Lite 83.7 29.6 46.9 SwinT\nTransVOD Lite 85.8 22.2 68.3 SwinS\nTransVOD Lite 90.1 14.9 106.3 SwinB\nods in Table 3. When equipped with a stronger backbone\nResNet-101, the mAP of our TransVOD++ is further boosted\nup to 82.0%, which outperforms most state-of-the-art VOD\nmethods [19], [20], [21], [22], [71]. Speciﬁcally, our model\nis remarkably better than FGFA [20] (76.3 % mAP) and\nMANet [21] (78.1 % mAP), which both aggregate features\nbased on optical ﬂow estimation, and the mAP improve-\nments are +5.6 % mAP and +3.8 % mAP respectively. When\ncompared with some relation-based methods (LRTRN [28]\n(81.0% mAP), RDN [27] (81.8 % mAP), SELSA [44] (80.3\n% mAP)), our method also shows its superiority in case\nof detection precision. Moreover, our proposed method\nboosts the strong baseline i.e., deformable DETR [34] by\na signiﬁcant margin ( 3%∼4% mAP ). After adopting Swin\nBase (SwinB) as the backbone, our TransVOD++ achieve\n90.0% mAP and it outperforms previous works by a large\nmargin (about 4 % ∼5% mAP), which further demonstrate\nits effectiveness.\nResults using TransVOD Lite In Table 2, we report the\nresults of our TransVOD Lite and compare it with previous\nreal-time VOD models. As shown in that table, using the\nResNet-101 backbone, our method achieves the best speed\nTABLE 3: Comparison with the state-of-the-art VOD meth-\nods on ImageNet VID. Most VOD methods use ResNet 101\nas the backbone. ⋆ denotes using Swin-Base as backbone.\nMethods Base Detector mAP(%)\nSingle Frame Baseline [72] R-FCN 73.6\nDFF [19] R-FCN 73.0\nAdaScale [71] R-FCN 75.5\nD&T [47] R-FCN 75.8\nFGFA [20] R-FCN 76.3\nLWDN [26] R-FCN 76.3\nIFF-Net [23] R-FCN 77.1\nSCNet [73] R-FCN 77.9\nAFA [65] R-FCN 77.9\nTHP [22] R-FCN 78.6\nSTSN [25] R-FCN 78.9\nPSLA [18] R-FCN 80.0\nOGEMN [24] R-FCN 80.0\nSTMN [74] R-FCN 80.5\nTCENet [14] R-FCN 80.3\nMAMBA [17] R-FCN 80.8\nSingle Frame Baseline [1] Faster RCNN 76.7\nST-Lattice [15] Faster RCNN 79.0\nBFAN [75] Faster RCNN 79.1\nSTCA [76] Faster RCNN 80.3\nSELSA [44] Faster RCNN 80.3\nMINet [77] Faster RCNN 80.6\nLRTR [28] Faster RCNN 81.0\nRDN [27] Faster RCNN 81.8\nTROI [78] Faster RCNN 82.0\nMEGA [16] Faster RCNN 82.9\nHVRNet [11] Faster RCNN 83.2\nTF-Blender [79] Faster RCNN 83.8\nDSFNet [13] Faster RCNN 84.1\nMAMBA [17] Faster RCNN 84.6\nEBFA [12] Faster RCNN 84.8\nCFA-Net [80] Faster RCNN 85.0\nSingle Frame Baseline [81] CenterNet 73.6\nCHP [82] CenterNet 76.7\nSingle Frame Baseline [34] Deformable DETR 78.3\nTransVOD Lite Deformable DETR 80.5\nTransVOD++ Deformable DETR 82.0\nTransVOD++⋆ Deformable DETR 90.0\nand accuracy trade-off. After adopting Swin-Tiny as the\nbackbone, our TranVOD Lite achieves 83.7 % mAP while\nrunning at nearly 30 FPS. Our best TransVOD Lite model\nwith a Swin base backbone can achieve 90.1 % mAP while\nrunning at around 15 FPS. Furthermore, the parameter\ncount (46.9M) is fewer than other video object detectors (e.g.,\naround 100M in [19]), which also indicates that our method\nis more friendly for mobile devices.\n4.3 Ablation Study and Analysis\nOverview. In this section, we demonstrate the effect of key\ncomponents in our proposed methods including TransVOD,\nTransVOD++ and TransVOD Lite. For TransVOD, we\nadopt ResNet-50 as the backbone. For TransVOD++ and\nTransVOD Lite, we adopt Swin Transformer as the back-\nbone.\n4.3.1 Ablation for TransVOD\nEffectiveness of each component in TransVOD. Table 4(a)\nsummarizes the effects of different design components on\nthe ImageNet VID dataset. The single-frame baseline of De-\nformable DETR [34] is 76.0 % and 88.3% with ResNet50 and\nSwin-Base Transformer, respectively. By merely using TDTE\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X 11\nTABLE 4: Ablation studies of TransVOD on ImageNet VID using ResNet 50 as the backbone.\n(a) Effect of each component in TransVOD. TDTE: Temporal Deformable\nTransformer Encoder. TQE: Temporal Query Encoder. TDTD: Temporal De-\nformable Transformer Decoder. †means the results using the Swin-base\nbackbone.\nSingle Frame Baseline TDTE TQE TDTD mAP (%) mAP † (%)\n✓ 76.0 88.3\n✓ ✓ ✓ 77.1 88.8\n✓ ✓ 78.9 89.3\n✓ ✓ ✓ 79.3 89.6\n✓ ✓ ✓ ✓ 79.9 89.6\n(b) Ablation studies on the number of of top k spatial object query\nin three temporal deformable transformer encoder (TDTE) layers.\nOur coarse-to-ﬁne (C2F) temporal query aggregation strategy has\nbetter results.\nkj I II III IV V VI VII\nk1 30 30 30 50 50 80 80\nk2 20 20 30 30 50 50 80\nk3 10 20 30 20 50 20 80\nmAP(%) 79.7 79.6 79.3 79.6 79.5 79.9 79.7\nTABLE 5: Ablation studies on TransVOD: number of en-\ncoder layers NTDTE in TDTE, number of encoder layers\nNTQE in TQE, number of decoder layers NTDTD in TDTD\nand top kspatial query in TQE with one decoder layer.\n(a) Number of encoder layers NTDTE in TDTE.\nNTDTE 0 1 2 3 4\nmAP(%) 77.0 77.7 77.6 77.8 77.7\n(b) number of encoder layers NTQE in TQE\nNTQE 1 2 3 4 5 6\nmAP(%) 78.8 79.4 79.6 79.6 79.7 79.7\n(c) number of decoder layers NTDTD in TDTD.\nNTDTD 1 2 3 4 5 6\nmAP (%) 78.2 77.7 77.1 76.2 74.8 72.3\n(d) top k spatial query in TQE with one decoder layer.\nk 25 50 100 200 300\nmAP(%) 78.0 78.1 78.3 77.9 77.7\n(d) Number of reference frames Nref .\nNref 2 4 8 10 12 14\nmAP(%) 77.7 78.3 79.0 79.1 79.0 79.3\nand TDTD, we boost the baseline with an additional +1.1 %\nand +0.5% on the two backbones, respectively. Besides, by\nonly adding TQE, we boost the baseline with an addi-\ntional +2.9 %, +1.0 % on the two backbones, respectively.\nThe combination of TQE and TDTD increase the mAP to\n79.3%, 89.6%, respectively. Finally, the proposed TransVOD\nincluding all components achieves 79.9 % and 89.6 % with\nResNet50 and Swin-Base Transformer, respectively. These\nimprovements show the effects of individual components\nof our TransVOD. Interestingly, we ﬁnd the effect of TDTE\nfades away if we use a stronger backbone, e.g., Swin Trans-\nformer.\nNumber of encoder layers in TDTE. Table 5(a) illustrates\nthe ablation study on the number of encoder layers in TDTE.\nWe observe that when the number of TDTE encoder layers\nare larger than 1, it brings no signiﬁcant beneﬁts to the ﬁnal\nperformance. This experiment also proves the claim that\naggregating the feature memories in a temporal dimension\nvia deformable attention is useful for learning the temporal\ncontexts across different frames.\nNumber of encoder layers in TQE. Table 5(b) shows the\nablation study on the number of encoder layers in TQE. It\nshows that the best result occurs when the number query\nlayer is set to 5. When the number of layers is up to 3, the\nperformance is basically unchanged. Thus, we use 3 encoder\nlayers in our ﬁnal method.\nNumber of decoder layers in TDTD. Table 5(c) illustrates\nthe ablation study on the number of decoder layers in\nFig. 6: Ablations of TransVOD++: (a). Effect on the number of\nreference frames Nref using Swin Base as the backbone. (b)\nImprovements over the different single frame baseline.\nTDTD. The basic setting is 4 reference frames, 1 encoder\nlayer in TQE, and 1 encoder layer in TDTE. The results\nindicate that only one decoder layer in TDTD is needed,\nand we set this number by default.\nNumber of top kobject queries in TQE. To verify the effec-\ntiveness of our coarse-to-ﬁne Temporal Query Aggregation\nstrategy, we conduct ablation experiments in Table 5(d) and\nTable 4(b) to study how they contribute to the ﬁnal per-\nformance. All the experiments in each table are conducted\nunder the same setting. The ﬁrst experiment is that when\nwe use 1 encoder layer in TQE with 4 reference frames, the\nbest performance is achieved when we choose the top 100\nspatial object queries for each reference frame. The second\nexperiment is conducted in a multiple TQE encoder layers\ncase, i.e., 3 encoder layers in TQE. We denote the ﬁne-to-ﬁne\n(F2F) selection by using a small number of spatial object\nqueries in each TQE encoder layer. Coarse-to-coarse (C2C)\nmeans selecting a large number of spatial object queries\nwhen performing the aggregation in each layer. Our pro-\nposed coarse-to-ﬁne (C2F) aggregation strategy uses larger\nnumber of spatial object queries in the shallow layers and a\nsmaller number of spatial object queries in the deep layers.\nThe results in Table 4(b) show that our C2F aggregation\nstrategy is superior to both the C2C and F2F selection.\nNumber of reference frames in TransVOD. Table 5(d)\nillustrates the ablations on number of reference. The basic\nsetting is 3 encoder layers in TQE, 1 encoder layer in TDTE,\nand 1 decoder layer in TDTD. As shown in Table 5(d),\nthe mAP improves when the number of reference frames\nincreases, and it tends to stabilize when the number is up to\n8. Thus, in all experiments, we set the reference frames to 14\nfor TransVOD with different backbones.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X 12\nTABLE 6: Ablation studies of TransVOD++ on ImageNet VID using Swin Transformer Base (SwinB) as the backbone.\n(a) Effect of each component of TransVOD++\nComponent (a) (b) (c)\nSingle Frame Baseline ✓ ✓ ✓\nRoI and Query Fusion ✓ ✓\nHard Query Mining ✓\nmAP50 (%) 88.3 89.7 90.0\nmAP50:95 (%) 67.3 67.2 67.8\nmAP50:95 (%) (small) 14.2 16.0 17.6\nmAP50:95 (%) (medium) 39.0 41.5 42.1\nmAP50:95 (%) (large) 73.9 73.7 74.4\n(b) Effect of multi-level feature fusion.\nComponent (a) (b)\nSingle Frame Baseline ✓ ✓\nMulti-level feature fusion ✓\nmAP50 (%) 87.7 88.3\nmAP50:95 (%) 65.0 67.3\nmAP50:95 (%) (small) 12.2 14.2\nmAP50:95 (%) (medium) 35.3 39.0\nmAP50:95 (%) (large) 72.2 73.9\n(c) Effect of COCO pre-training.\nComponent (a) (b)\nSingle Frame Baseline ✓ ✓\nCOCO pre-training ✓\nmAP50 (%) 44.8 88.3\nmAP50:95 (%) 28.5 67.3\nmAP50:95 (%) (small) 4.5 14.2\nmAP50:95 (%) (medium) 12.8 39.0\nmAP50:95 (%) (large) 33.3 73.9\nTABLE 7: Ablation studies of TransVOD Lite on ImageNet VID.\n(a) Effect of window size Tw with Swin Tiny as backbone.\nTw mAP50 mAP50:95 mAPS mAPM mAPL FPS\n1 76.6 55.1 12.6 31.5 63.7 16.5\n2 79.1 56.7 12.4 34.1 65.0 21.7\n4 80.9 57.9 12.1 35.1 66.0 23.5\n6 81.5 58.3 14.3 35.8 66.4 22.9\n8 82.1 58.6 13.7 36.3 66.6 22.5\n10 82.3 58.7 13.7 35.9 66.7 29.2\n12 82.7 59.0 13.7 36.6 67.0 30.1\n14 82.5 58.8 14.4 36.6 66.8 32.2\n15 83.7 66.2 14.7 35.1 67.3 29.6\n(b) Effect of window size Tw with Swin Base as backbone.\nTw mAP50 mAP50:95 mAPS mAPM mAPL FPS\n1 85.4 64.1 13.8 39.1 72.0 10.6\n2 87.6 66.2 14.2 41.2 74.0 12.7\n4 88.6 66.5 14.9 42.4 74.1 14.2\n6 89.2 67.0 14.2 42.3 74.5 15.2\n8 88.9 66.7 14.3 42.6 74.2 15.4\n10 88.8 66.4 14.4 42.6 74.0 15.6\n12 90.1 67.7 13.7 43.1 75.3 16.2\n14 88.9 66.7 14.4 42.3 74.2 15.4\n15 90.0 67.3 14.9 41.6 74.9 15.0\n(c) Effect of interval size Iw using Swin Base as backbone.\n(a) Interval size Iw when window size Tw = 4.\nIw(Tw = 4) 1 4 8 12 Randomly Shufﬂe\nmAP(%) 86.3 86.9 87.2 87.5 88.6\n(a) Interval size Iw when window size Tw = 8.\nIw(Tw = 8) 1 4 8 12 Randomly Shufﬂe\nmAP(%) 86.6 87.3 88.0 88.2 88.9\n(b) Interval size Iw when window size Tw = 12.\nIw(Tw = 12) 1 4 8 12 Randomly Shufﬂe\nmAP(%) 86.9 88.0 88.7 89.3 90.1\n(d) Ablation of top k query numbers in SeqHQM using ResNet-\n101 as backbone where the window size is set to 14.\nk1 30 30 50 80 100 100\nk2 20 20 30 50 80 80\nk3 10 20 25 30 50 30\nmAP50(%) 78.4 78.4 78.8 80.4 80.3 79.9\nmAP50:95(%) 56.2 56.4 56.5 58.3 58.2 58.0\nmAPS(%) 11.1 11.3 11.4 10.1 10.0 10.0\nmAPM (%) 30.8 31.2 31.4 29.1 29.3 28.6\nmAPL(%) 65.0 65.1 65.2 65.4 65.2 65.1\n4.3.2 Ablation for TransVOD++\nEffect of each component in TransVOD++ on strong\nbaseline. In Table 6(a), we verify the effectiveness of each\ncomponent in TransVOD++ on a strong baseline. Adding\nRoI and Query Fusion results in 1.4 % mAP improvements,\nwhile applying Hard Query Mining leads to extra 0.3 %\nmAP improvements and 1.6% mAP improvements on small\nobjects. This proves that our proposed Hard Query Mining\nis suitable for detecting small objects.\nEffect of reference frames in TransVOD++.In Fig. 6 (a), we\nshow the effect of reference frames in TransVOD++ where\nwe ﬁnd the best reference frames is 14. This is different from\nthe original TransVOD. We argue that utilizing more RoI\ninformation rather than full-frame fusion in the temporal\ndimension leads to better results. This ﬁnding is consistent\nwith previous works [16], [27], [80] focusing on RoI-wised\nfusion in Faster-RCNN framework. We set the number of\nreference frames to 14 by default.\nImprovements over different baselines. In Fig. 6 (b), we\nshow the improvements over different single-frame base-\nlines including Swin Transformer [42] and ResNet [57]. Swin\nBase, Swin Small, and Swin Tiny are abbreviated as SwinB,\nSwinS, SwinT, respectively. Our proposed TransVOD++ can\nboost the gain over 1.7% ∼4.2% mAP on various baselines.\nWe observe that our TransVOD++ outperforms TransVOD\nLite under different backbones. Especially, with ResNet-\n50 and ResNet-101, the improvements of TransVOD++\n(2.3%∼4.2% mAP ) are larger than the ones (0.8% ∼1.6%)\nmAP of TransVOD Lite. Interestingly, we found that\nwith the backbones of Swin Transformers, TransVOD Lite\nachieves almost the same performance as TransVOD++.\nThis is mainly because the single frame baselines of Swin\nTransformer are too strong (88.3%) mAP) and these im-\nprovements over the strong baseline are not as obvious as\nthe ones of ResNet.\nEffect of multi-level feature fusion. Table 6(b) shows the\nimprovements on multi-level feature fusion. In total, there\nis a 0.6 % mAP50 gain. Moreover, there is a more signiﬁ-\ncant gain (2.3%) on mAP 50:95 which indicates multi-scale\ninformation leads to more accurate detection results. Thus,\nwe adopt the simple multi-level feature fusion by default\nwhen adopting Swin Transformer as the backbone for both\nTransVOD++ and TransVOD Lite.\nEffect of COCO pre-training using Swin base. Following\nprevious VOD methods [9], [29], [65], we pre-train our\nimage detector on the COCO dataset [66]. As shown in\nTable 6(c), removing COCO pre-training leads to a huge\nperformance drop. The main reason lies in the fact that\nvision Transformers [35] need more training samples for\nbetter convergence and vision Transformers are typically\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X 13\nFig. 7: The visualization of the deformable cross-attention in\nthe last spatial Transformer decoder layer and temporal Trans-\nformer decoder layer. We visualize the sampling locations of the\ntemporal object query and corresponding spatial object query\nin one ﬁgure. Each sampling point of the temporal object query\nis marked as a red-ﬁlled circle, while the blue circle represents\nthe sampling point of the spatial query.\nFig. 8: Effect of the temporal window size Tw of a video clip\non the mean Average Precision (mAP) (a) and on the Frame Per\nSecond (FPS) (b) in TransVOD Lite using Swin Base and Swin\nTiny as the backbone, respectively.\npre-trained on large-scale datasets. Thus, we pre-train the\nTransVOD series on the COCO dataset.\n4.3.3 Ablation for TransVOD Lite\nEffect of window size in TransVOD Lite. In Fig. 8 (a)\nand Fig. 8 (b), we show the effect of window size on both\naccuracy and inference time where the interval mode is\nrandomly shufﬂed within the window for all experiments.\nAs shown in these ﬁgures, increasing window size leads\nto both accuracy improvements and FPS increase for both\nSwin Tiny and Swin base as backbones. In Table 7(a) and\nTable 7(b), we detail the results of the above ﬁgures. We\nchoose the best window size Tw as 15 for all models.\nEffect of interval size and mode in TransVOD Lite. In\nTable 7(c), we show the effect of interval size between frames\nin each ﬁxed window. For different window sizes, increasing\nthe interval size leads to better results. This indicates that\nfusing more global temporal information leads to better\nresults. However, adopting our proposed randomly shufﬂed\nstrategy results in the best performance on different window\nsizes. This is mainly because random shufﬂes increase the\ndiversity of each frame. For example, the global and local\ntemporal information can exist in one window. Moreover,\nFig. 9: Failure case analysis. First and second row: miss de-\ntection. Third and fourth row: false detection. The results are\nobtained via our TransVOD Lite with Swin Base backbone.\nduring training, the frames are randomly selected from\neach clip. Thus, randomly shufﬂed inputs share the same\ndistribution with training examples. We report the ﬁnal\nperformance using such settings. Moreover, as shown in\nTable 7(c), even with the sequential inputs, our methods\ncan still achieve the best performance compared with the\nmethods in Table 2.\nAblation on query numbers in Sequential Hard Query\nMining. In Table 7(d), we perform ablation studies on Se-\nquential Hard Query Mining (SeqHQM) in TransVOD Lite.\nFrom the table, we ﬁnd the best hyper-parameter with 80,\n50, 30 queries for each stage. We use that setting for all the\nTransVOD Lite models.\n4.4 Visualization and Analysis\nVisual detection results. As shown in Fig. 10, we show\nthe visual detection results of still image detector, i.e., De-\nformable DETR [34] and our proposed TransVOD in odd\nand even rows, respectively. The still image detector is easy\nto cause false detection ( e.g., turtle detected as a lizard) and\nmissed detection ( e.g., zebra not detected), in the case of\nmotion blur, part occlusion. Compared with Deformable\nDETR [34], our method effectively models the long-range\ndependencies across different video frames to enhance the\nfeatures of the detected image. Thus, our TransVOD not\nonly increases the conﬁdence of correct prediction, but also\neffectively reduces the number of cases that are missed\nor falsely detected. Besides, as shown in Fig. 10 (b), our\nTransVOD Lite shows more conﬁdent scores than the single\nframe baseline [34].\nVisual sampling locations of object query in TransVOD.\nTo further explore the advantages of TQE, we visualize the\nsampling locations of both spatial object query and temporal\nobject query in Fig. 7. The sample locations indicate the most\nrelevant context for each detection. As shown in the ﬁgure,\nfor each frame in each clip, our temporal object query has\nmore concentrated and precise results on foreground objects\nwhile the original spatial object query has more diffuse\nresults. This proves that our temporal object query is more\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X 14\nsuitable for detecting objects in video. This explains the\neffectiveness of our temporal query fusion.\nFailure case analysis. In Fig. 9, we present several failure\ncases using our best TransVOD Lite model. The ﬁrst two\nrows show the missing detection problems. The ﬁrst row is\nmainly due to the larger motion blur, and the second row\nis caused by the various background change. The last two\nrows show the false detection where a car is detected as a\nbus. This is caused by the large occlusion. Both cases show\nthat tackling occlusion and more stable temporal modeling\nare needed for the further works.\n5 C ONCLUSION\nIn this paper, we proposed a novel video object detec-\ntion framework, namely TransVOD, which provides a new\nperspective of feature aggregation by leveraging spatial-\ntemporal Transformers. TransVOD effectively removes the\nneed for many hand-crafted components and complicated\npost-processing methods. Our core idea is to aggregate both\nthe spatial object queries and the memory encodings in each\nframe via temporal Transformers. Our TransVOD boosts the\nstrong baseline deformable DETR by a signiﬁcant margin\n(3%-4% mAP) on the ImageNet VID dataset on various\nbaselines. To our knowledge, our work is the ﬁrst one that\napplies the Transformers to VOD. Based on the TransVOD\nframework, we present two advanced versions, namely\nTransVOD++ and TransVOD Lite. The former improves the\nperformance of TransVOD via better Query and RoI fusion\n(QRF), and Hard Query Mining (HQM) to fully utilize\nthe object-level information, and dynamically reduce the\nnumber of object queries and targets. The latter focuses\non real-time video object detection by modeling VOD as\na sequence-to-sequence prediction problem via Sequential\nHard Query Mining (SeqHQM). Both models set new state-\nof-the-art results on the ImageNet VID dataset on two dif-\nferent settings: accuracy for non-real-time models and best\nspeed-accuracy trade-off on real-time models. Our method\nis the ﬁrst work that achieves 90 % mAP on ImageNet VID\ndataset. Moreover, TransVOD is the ﬁrst work that achieves\n83.7% mAP while running in real time. We believe our\nmodels can be new baselines for this area.\nREFERENCES\n[1] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: towards\nreal-time object detection with region proposal networks,” IEEE\ntransactions on pattern analysis and machine intelligence, vol. 39, no. 6,\npp. 1137–1149, 2016.\n[2] J. Dai, Y. Li, K. He, and J. Sun, “R-fcn: Object detection via\nregion-based fully convolutional networks,” in Advances in neural\ninformation processing systems, 2016, pp. 379–387.\n[3] T.-Y. Lin, P . Goyal, R. Girshick, K. He, and P . Doll ´ar, “Focal loss\nfor dense object detection,” in Proceedings of the IEEE international\nconference on computer vision, 2017, pp. 2980–2988.\n[4] Z. Tian, C. Shen, H. Chen, and T. He, “Fcos: Fully convolutional\none-stage object detection,” in Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, 2019, pp. 9627–9636.\n[5] W. Han, P . Khorrami, T. L. Paine, P . Ramachandran,\nM. Babaeizadeh, H. Shi, J. Li, S. Yan, and T. S. Huang, “Seq-nms\nfor video object detection,” arXiv preprint arXiv:1602.08465, 2016.\n[6] K. Kang, H. Li, J. Yan, X. Zeng, B. Yang, T. Xiao, C. Zhang, Z. Wang,\nR. Wang, X. Wanget al., “T-cnn: Tubelets with convolutional neural\nnetworks for object detection from videos,” IEEE Transactions on\nCircuits and Systems for Video Technology , vol. 28, no. 10, pp. 2896–\n2907, 2017.\n[7] H. Belhassen, H. Zhang, V . Fresse, and E.-B. Bourennane, “Improv-\ning video object detection by seq-bbox matching.” in VISIGRAPP\n(5: VISAPP), 2019, pp. 226–233.\n[8] A. Sabater, L. Montesano, and A. C. Murillo, “Robust and efﬁ-\ncient post-processing for video object detection,” arXiv preprint\narXiv:2009.11050, 2020.\n[9] C.-H. Yao, C. Fang, X. Shen, Y. Wan, and M.-H. Yang, “Video ob-\nject detection via object-level temporal aggregation,” in European\nconference on computer vision. Springer, 2020, pp. 160–177.\n[10] Z. Jiang, Y. Liu, C. Yang, J. Liu, P . Gao, Q. Zhang, S. Xiang,\nand C. Pan, “Learning where to focus for efﬁcient video object\ndetection,” in European Conference on Computer Vision . Springer,\n2020, pp. 18–34.\n[11] M. Han, Y. Wang, X. Chang, and Y. Qiao, “Mining inter-video pro-\nposal relations for video object detection,” in European Conference\non Computer Vision. Springer, 2020, pp. 431–446.\n[12] L. Han, P . Wang, Z. Yin, F. Wang, and H. Li, “Exploiting better\nfeature aggregation for video object detection,” inProceedings of the\n28th ACM International Conference on Multimedia , 2020, pp. 1469–\n1477.\n[13] L. Lin, H. Chen, H. Zhang, J. Liang, Y. Li, Y. Shan, and H. Wang,\n“Dual semantic fusion network for video object detection,” in\nProceedings of the 28th ACM International Conference on Multimedia ,\n2020, pp. 1855–1863.\n[14] F. He, N. Gao, Q. Li, S. Du, X. Zhao, and K. Huang, “Temporal\ncontext enhanced feature aggregation for video object detection,”\nin Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 34,\nno. 07, 2020, pp. 10 941–10 948.\n[15] K. Chen, J. Wang, S. Yang, X. Zhang, Y. Xiong, C. C. Loy, and\nD. Lin, “Optimizing video object detection via a scale-time lattice,”\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2018, pp. 7814–7823.\n[16] Y. Chen, Y. Cao, H. Hu, and L. Wang, “Memory enhanced global-\nlocal aggregation for video object detection,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2020, pp. 10 337–10 346.\n[17] G. Sun, Y. Hua, G. Hu, and N. Robertson, “Mamba: Multi-level\naggregation via memory bank for video object detection,” in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence , vol. 35,\nno. 3, 2021, pp. 2620–2627.\n[18] C. Guo, B. Fan, J. Gu, Q. Zhang, S. Xiang, V . Prinet, and C. Pan,\n“Progressive sparse local attention for video object detection,” in\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2019, pp. 3909–3918.\n[19] X. Zhu, Y. Xiong, J. Dai, L. Yuan, and Y. Wei, “Deep feature ﬂow for\nvideo recognition,” in Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2017, pp. 2349–2358.\n[20] X. Zhu, Y. Wang, J. Dai, L. Yuan, and Y. Wei, “Flow-guided feature\naggregation for video object detection,” in Proceedings of the IEEE\nInternational Conference on Computer Vision, 2017, pp. 408–417.\n[21] S. Wang, Y. Zhou, J. Yan, and Z. Deng, “Fully motion-aware\nnetwork for video object detection,” in Proceedings of the European\nconference on computer vision (ECCV), 2018, pp. 542–557.\n[22] X. Zhu, J. Dai, L. Yuan, and Y. Wei, “Towards high performance\nvideo object detection,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2018, pp. 7210–7218.\n[23] R. Jin, G. Lin, C. Wen, J. Wang, and F. Liu, “Feature ﬂow: In-\nnetwork feature ﬂow estimation for video object detection,” Pat-\ntern Recognition, vol. 122, p. 108323, 2022.\n[24] H. Deng, Y. Hua, T. Song, Z. Zhang, Z. Xue, R. Ma, N. Robert-\nson, and H. Guan, “Object guided external memory network for\nvideo object detection,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision, 2019, pp. 6678–6687.\n[25] G. Bertasius, L. Torresani, and J. Shi, “Object detection in video\nwith spatiotemporal sampling networks,” in Proceedings of the\nEuropean Conference on Computer Vision (ECCV), 2018, pp. 331–346.\n[26] Z. Jiang, P . Gao, C. Guo, Q. Zhang, S. Xiang, and C. Pan, “Video\nobject detection with locally-weighted deformable neighbors,” in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence , vol. 33,\nno. 01, 2019, pp. 8529–8536.\n[27] J. Deng, Y. Pan, T. Yao, W. Zhou, H. Li, and T. Mei, “Relation\ndistillation networks for video object detection,” in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision, 2019, pp.\n7023–7032.\n[28] M. Shvets, W. Liu, and A. C. Berg, “Leveraging long-range tem-\nporal relationships between proposals for video object detection,”\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X 15\nin Proceedings of the IEEE/CVF International Conference on Computer\nVision, 2019, pp. 9756–9764.\n[29] M. Liu, M. Zhu, M. White, Y. Li, and D. Kalenichenko, “Looking\nfast and slow: Memory-guided mobile video object detection,”\narXiv preprint arXiv:1903.10172, 2019.\n[30] M. Liu and M. Zhu, “Mobile video object detection with\ntemporally-aware feature maps,” in Proceedings of the IEEE confer-\nence on computer vision and pattern recognition , 2018, pp. 5686–5695.\n[31] K. Chen, J. Wang, S. Yang, X. Zhang, Y. Xiong, C. C. Loy, and\nD. Lin, “Optimizing video object detection via a scale-time lattice,”\n2018 IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion, pp. 7814–7823, 2018.\n[32] Y. Wang, Z. Xu, X. Wang, C. Shen, B. Cheng, H. Shen, and\nH. Xia, “End-to-end video instance segmentation with transform-\ners,” arXiv preprint arXiv:2011.14503, 2020.\n[33] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in\nEuropean Conference on Computer Vision . Springer, 2020, pp. 213–\n229.\n[34] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable detr:\nDeformable transformers for end-to-end object detection,” arXiv\npreprint arXiv:2010.04159, 2020.\n[35] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly\net al. , “An image is worth 16x16 words: Transformers for image\nrecognition at scale,” The International Conference on Learning Rep-\nresentations (ICLR), 2021.\n[36] P . Sun, Y. Jiang, R. Zhang, E. Xie, J. Cao, X. Hu, T. Kong, Z. Yuan,\nC. Wang, and P . Luo, “Transtrack: Multiple-object tracking with\ntransformer,” arXiv preprint arXiv:2012.15460, 2020.\n[37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin Advances in neural information processing systems, 2017, pp. 5998–\n6008.\n[38] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet\nlarge scale visual recognition challenge,” International journal of\ncomputer vision, vol. 115, no. 3, pp. 211–252, 2015.\n[39] L. He, Q. Zhou, X. Li, L. Niu, G. Cheng, X. Li, W. Liu,\nY. Tong, L. Ma, and L. Zhang, “End-to-end video object detection\nwith spatial-temporal transformers,” in Proceedings of the 29th\nACM International Conference on Multimedia (ACM MM) , 2021, p.\n1507–1516.\n[40] A. Shrivastava, A. Gupta, and R. Girshick, “Training region-based\nobject detectors with online hard example mining,” in Proceedings\nof the IEEE conference on computer vision and pattern recognition, 2016,\npp. 761–769.\n[41] Z. Wu, C. Shen, and A. v. d. Hengel, “High-performance semantic\nsegmentation using very deep fully convolutional networks,”\narXiv preprint, 2016.\n[42] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,\n“Swin transformer: Hierarchical vision transformer using shifted\nwindows,” ICCV, 2021.\n[43] A. Dosovitskiy, P . Fischer, E. Ilg, P . Hausser, C. Hazirbas, V . Golkov,\nP . Van Der Smagt, D. Cremers, and T. Brox, “Flownet: Learning\noptical ﬂow with convolutional networks,” in Proceedings of the\nIEEE international conference on computer vision , 2015, pp. 2758–\n2766.\n[44] H. Wu, Y. Chen, N. Wang, and Z. Zhang, “Sequence level seman-\ntics aggregation for video object detection,” in Proceedings of the\nIEEE/CVF International Conference on Computer Vision , 2019, pp.\n9217–9225.\n[45] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\narXiv preprint arXiv:1706.03762, 2017.\n[46] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural\nnetworks,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2018, pp. 7794–7803.\n[47] T.-W. Chin, R. Ding, and D. Marculescu, “Adascale: Towards real-\ntime video object detection using adaptive scaling,” arXiv preprint\narXiv:1902.02910, 2019.\n[48] X. Li, H. He, H. Ding, K. Yang, G. Cheng, J. Shi, and Y. Tong,\n“Improving video instance segmentation via temporal pyramid\nrouting,” arXiv preprint arXiv:2107.13155, 2021.\n[49] Y. Chai, “Patchwork: A patch-wise attention network for efﬁ-\ncient object detection and segmentation in video streams,” 2019\nIEEE/CVF International Conference on Computer Vision (ICCV) , pp.\n3414–3423, 2019.\n[50] T. Meinhardt, A. Kirillov, L. Leal-Taixe, and C. Feichtenhofer,\n“Trackformer: Multi-object tracking with transformers,” arXiv\npreprint arXiv:2101.02702, 2021.\n[51] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\nH. J ´egou, “Training data-efﬁcient image transformers & distil-\nlation through attention,” in International Conference on Machine\nLearning. PMLR, 2021, pp. 10 347–10 357.\n[52] J. Zhang, X. Li, Y. Wang, C. Wang, Y. Yang, Y. Liu, and D. Tao,\n“Eatformer: Improving vision transformer inspired by evolution-\nary algorithm,” arXiv preprint arXiv:2206.09325, 2022.\n[53] X. Li, W. Zhang, J. Pang, K. Chen, G. Cheng, Y. Tong, and C. C.\nLoy, “Video k-net: A simple, strong, and uniﬁed baseline for video\nsegmentation,” in CVPR, 2022.\n[54] X. Li, S. Xu, Y. Yang, G. Cheng, Y. Tong, and D. Tao, “Panoptic-\npartformer: Learning a uniﬁed model for panoptic part segmenta-\ntion,” in Eur. Conf. Comput. Vis., 2022.\n[55] S. Xu, X. Li, J. Wang, G. Cheng, Y. Tong, and D. Tao, “Fashion-\nformer: A simple, effective and uniﬁed baseline for human fashion\nsegmentation and recognition,” in Eur. Conf. Comput. Vis., 2022.\n[56] L. Yang, Y. Fan, and N. Xu, “Video instance segmentation,” in\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2019, pp. 5188–5197.\n[57] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” inProceedings of the IEEE conference on computer\nvision and pattern recognition, 2016, pp. 770–778.\n[58] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei,\n“Deformable convolutional networks,” in Proceedings of the IEEE\ninternational conference on computer vision, 2017, pp. 764–773.\n[59] H. Hu, J. Gu, Z. Zhang, J. Dai, and Y. Wei, “Relation networks for\nobject detection,” in Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2018, pp. 3588–3597.\n[60] P . Sun, R. Zhang, Y. Jiang, T. Kong, C. Xu, W. Zhan, M. Tomizuka,\nL. Li, Z. Yuan, C. Wang et al. , “Sparse r-cnn: End-to-end object\ndetection with learnable proposals,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2021, pp.\n14 454–14 463.\n[61] K. He, G. Gkioxari, P . Doll ´ar, and R. Girshick, “Mask r-cnn,” in\nProceedings of the IEEE international conference on computer vision ,\n2017, pp. 2961–2969.\n[62] R. Stewart, M. Andriluka, and A. Y. Ng, “End-to-end people\ndetection in crowded scenes,” in Proceedings of the IEEE conference\non computer vision and pattern recognition , 2016, pp. 2325–2333.\n[63] H. W. Kuhn, “The hungarian method for the assignment prob-\nlem,” Naval research logistics quarterly , vol. 2, no. 1-2, pp. 83–97,\n1955.\n[64] H. Rezatoﬁghi, N. Tsoi, J. Gwak, A. Sadeghian, I. Reid, and\nS. Savarese, “Generalized intersection over union: A metric and\na loss for bounding box regression,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2019, pp.\n658–666.\n[65] Y. Qian, L. Yu, W. Liu, G. Kang, and A. G. Hauptmann, “Adaptive\nfeature aggregation for video object detection,” in Proceedings of\nthe IEEE/CVF Winter Conference on Applications of Computer Vision\nWorkshops, 2020, pp. 143–147.\n[66] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P . Perona, D. Ramanan,\nP . Doll´ar, and C. L. Zitnick, “Microsoft coco: Common objects in\ncontext,” in European Conference on Computer Vision, 2014.\n[67] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Im-\nagenet: A large-scale hierarchical image database,” in 2009 IEEE\nconference on computer vision and pattern recognition . IEEE, 2009,\npp. 248–255.\n[68] I. Loshchilov and F. Hutter, “Decoupled weight decay regulariza-\ntion,” arXiv preprint arXiv:1711.05101, 2017.\n[69] X. Glorot and Y. Bengio, “Understanding the difﬁculty of training\ndeep feedforward neural networks,” in Proceedings of the thirteenth\ninternational conference on artiﬁcial intelligence and statistics, 2010, pp.\n249–256.\n[70] M. Contributors, “MMTracking: OpenMMLab video percep-\ntion toolbox and benchmark,” https://github.com/open-mmlab/\nmmtracking, 2020.\n[71] C. Feichtenhofer, A. Pinz, and A. Zisserman, “Detect to track and\ntrack to detect,” in Proceedings of the IEEE international conference on\ncomputer vision, 2017, pp. 3057–3065.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X 16\n[72] J. Dai, Y. Li, K. He, and J. Sun, “R-fcn: Object detection\nvia region-based fully convolutional networks,” arXiv preprint\narXiv:1605.06409, 2016.\n[73] F. Wang, Z. Xu, Y. Gan, C.-M. Vong, and Q. Liu, “Scnet: Scale-\naware coupling-structure network for efﬁcient video object detec-\ntion,” Neurocomputing, vol. 404, pp. 283–293, 2020.\n[74] F. Xiao and Y. J. Lee, “Video object detection with an aligned\nspatial-temporal memory,” inProceedings of the European Conference\non Computer Vision (ECCV), 2018, pp. 485–501.\n[75] Y. Wu, H. Zhang, Y. Li, Y. Yang, and D. Yuan, “Video object\ndetection guided by object blur evaluation,” IEEE Access , vol. 8,\npp. 208 554–208 565, 2020.\n[76] H. Luo, L. Huang, H. Shen, Y. Li, C. Huang, and X. Wang, “Object\ndetection in video with spatial-temporal context aggregation,”\narXiv preprint arXiv:1907.04988, 2019.\n[77] J. Deng, Y. Pan, T. Yao, W. Zhou, H. Li, and T. Mei, “Minet:\nMeta-learning instance identiﬁers for video object detection,”IEEE\nTransactions on Image Processing, vol. 30, pp. 6879–6891, 2021.\n[78] T. Gong, K. Chen, X. Wang, Q. Chu, F. Zhu, D. Lin, N. Yu, and\nH. Feng, “Temporal roi align for video object recognition,” in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence , vol. 35,\nno. 2, 2021, pp. 1442–1450.\n[79] Y. Cui, L. Yan, Z. Cao, and D. Liu, “Tf-blender: Temporal feature\nblender for video object detection,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV) , October 2021,\npp. 8138–8147.\n[80] L. Han, P . Wang, Z. Yin, F. Wang, and H. Li, “Class-aware feature\naggregation network for video object detection,” IEEE Transactions\non Circuits and Systems for Video Technology, pp. 1–1, 2021.\n[81] X. Zhou, D. Wang, and P . Kr ¨ahenb ¨uhl, “Objects as points,” arXiv\npreprint arXiv:1904.07850, 2019.\n[82] Z. Xu, E. Hrustic, and D. Vivet, “Centernet heatmap propagation\nfor real-time video object detection,” in European Conference on\nComputer Vision. Springer, 2020, pp. 220–234.\nIEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X 17\nBaseline Ours Baseline Ours \n Ours Baseline \n Baseline Baseline \nBaseline \n Ours Ours Ours \n(a) TransVOD\n(b) TransVOD Lite\nFig. 10: The visualization results of single frame baseline method [34] and TransVOD (a), TransVOD Lite (b) in different scenarios.\nCompared with single frame baseline, our proposed TransVOD and TransVOD lite show better and consistent detection results\nin the cases of part occlusion (top two rows of (a) and (b)), motion blur (middle two rows of (a) and (b)) and rare pose (last two\nrows of (a) and (b)), respectively. Best view it on the screen and zoom in.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7899175882339478
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6201319694519043
    },
    {
      "name": "Object detection",
      "score": 0.6157814264297485
    },
    {
      "name": "Transformer",
      "score": 0.5559223890304565
    },
    {
      "name": "Encoder",
      "score": 0.5204420685768127
    },
    {
      "name": "Computer vision",
      "score": 0.4984095096588135
    },
    {
      "name": "Feature extraction",
      "score": 0.42335790395736694
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3372338116168976
    },
    {
      "name": "Engineering",
      "score": 0.08044302463531494
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}