{
  "title": "Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking",
  "url": "https://openalex.org/W4389520309",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5101462518",
      "name": "Yuxiang Wu",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A5008320336",
      "name": "Guanting Dong",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A5001956210",
      "name": "Weiran Xu",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4206496297",
    "https://openalex.org/W4389009440",
    "https://openalex.org/W2964006684",
    "https://openalex.org/W3200895474",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W4225553189",
    "https://openalex.org/W2945475330",
    "https://openalex.org/W4287795696",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4224275713",
    "https://openalex.org/W3021096583",
    "https://openalex.org/W3206345746",
    "https://openalex.org/W4385573341",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4362655923",
    "https://openalex.org/W3168491067",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4288288848",
    "https://openalex.org/W4385572953"
  ],
  "abstract": "Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring and annotating task-oriented dialogues, which can be time-consuming and costly. However, DST extends beyond simple slot-filling and requires effective updating strategies for tracking dialogue state as conversations progress. In this paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to introduce additional intricate updating strategies in zero-shot DST. Our approach reformulates the DST task by leveraging powerful Large Language Models (LLMs) and translating the original dialogue text to JSON through semantic parsing as an intermediate state. We also design a novel framework that includes more modules to ensure the effectiveness of updating strategies in the text-to-JSON process. Experimental results demonstrate that our approach outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to existing ICL methods.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 11093–11099\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nSemantic Parsing by Large Language Models for Intricate Updating\nStrategies of Zero-Shot Dialogue State Tracking\nYuxiang Wu1∗, Guanting Dong1∗, Weiran Xu1 ∗\n1Beijing University of Posts and Telecommunications, Beijing, China\n{yuxiangw,dongguanting,xuweiran}@bupt.edu.cn\nAbstract\nZero-shot Dialogue State Tracking (DST) ad-\ndresses the challenge of acquiring and annotat-\ning task-oriented dialogues, which can be time-\nconsuming and costly. However, DST extends\nbeyond simple slot-filling and requires effective\nupdating strategies for tracking dialogue state\nas conversations progress. In this paper, we pro-\npose ParsingDST, a new In-Context Learning\n(ICL) method, to introduce additional intricate\nupdating strategies in zero-shot DST. Our ap-\nproach reformulates the DST task by leveraging\npowerful Large Language Models (LLMs) and\ntranslating the original dialogue text to JSON\nthrough semantic parsing as an intermediate\nstate. We also design a novel framework that\nincludes more modules to ensure the effective-\nness of updating strategies in the text-to-JSON\nprocess. Experimental results demonstrate that\nour approach outperforms existing zero-shot\nDST methods on MultiWOZ, exhibiting sig-\nnificant improvements in Joint Goal Accuracy\n(JGA) and slot accuracy compared to existing\nICL methods.\n1 Introduction\nDialogue State Tracking (DST) is crucial in Task-\nOriented Dialogue (TOD) systems to understand\nand manage user intentions(Wu et al., 2019;\nHosseini-Asl et al., 2020; Heck et al., 2020; Lee\net al., 2021; Zhao et al., 2022). Collecting and\nannotating dialogue states at the turn level is chal-\nlenging and expensive (Budzianowski et al., 2018),\nand commercial applications often need to expand\nthe schema and include new domains. Therefore, it\nis vital to develop DST learning strategies that can\nadapt and scale effectively with minimal data.\nMost of the existing fine-tuning methods for\nzero-shot DST have primarily focused on domain-\ntransfer approaches (Hosseini-Asl et al., 2020; Lin\net al., 2021b,a), which have not yielded satisfactory\n∗The first two authors contribute equally. Weiran Xu is\nthe corresponding author.\nDecode State directly\nSystem: \nThe address of it is XXX \nstreet, do you want the XXX \nhotel with a low price?\nUser: \nNo, I want an attraction XXX\ninstead.\nHotel-name: XXX hotel\nPrice-range: cheap\nAttraction-name: XXX\nAttraction-name: XXX\n{Sys: {…\nInfo: {Hotel:{\naddress: [XXX street], \nname: [XXX hotel],  \nprice-range: [cheap]}}, \n…}\n{User: {\nReject: {Hotel: {\nname: [XXX hotel]}}, \nRequest: {Attraction: {\nname: [XXX]}}}\nState Updating Strategies\nPrompt  (Instruction + Schema description + Example)\nContext state Hotel-name: XXX hotel\nOnly achieve slot filling task \nbut hard to understand \nintricate state updating \nstrategies in DST …\nGolden label! \nTranslate to JSON\nMistake\nNot an entity/ \nnot in Context\nOut of schema\n[Delete]\nCurrent turn dialogue\nBlack box\nControllable,\ninterpretable\nFigure 1: An example of the comparison between the\nprevious ICL method and ours.\nperformance. Recent studies have showcased the\nimpressive and adaptable expertise possessed by\nlarge language models (LLMs)(Raffel et al., 2020;\nOuyang et al., 2022; Liu et al., 2023). However,\nwith the discovery of the emergent ability of large\nlanguage models in downstream tasks (Cobbe et al.,\n2021; Wei et al., 2022a,b), the In-Context Learning\n(ICL) method (Brown et al., 2020) has garnered\nmore attention in DST research. ICL offers a more\nflexible and cost-effective approach as it eliminates\nthe need for retraining when new slots or domains\nare introduced. Some recent studies have explored\nthe effectiveness of ICL in DST (Hu et al., 2022;\nMadotto et al., 2021; Xie et al., 2022) such as the\nIC-DST method, which has demonstrated remark-\nable performance surpassing previous fine-tuning\nmethods.\nNevertheless, all the previous methods directly\ngenerate the dialogue state or its change, which\nonly achieves a simple slot-filling task that extracts\nthe slot’s value from dialogue but does not explain\n11093\nSystem: \nSorry, I can not book it \nfor 19:00, how about \n20:00 for the restaurant \nin the city center?\nUser: \nOk, it sounds good, \ncould you tell me the \nprice?\nContext state\nRestaurant-name: XXX\nRestaurant-book_time: 19:00\nResult state\nRestaurant-name: XXX\nRestaurant-book_time: 20:00\n{Sys: {\nNot_available: {Restaurant: {\nbook_time: [19:00]}},\nInfo: {Restaurant: {\nbook_time : [20:00], \narea: [centre]}},\nAsk_for: {}}}\n{User: {\nReject: {}, \nRequest: {Restaurant: {\nprice_range: []}}}}\nStrategy-based \nupdating\nTranslate to JSON\nTranslate to JSON\n{Sys: {\nNot_available: {}, \nInfo: {Restaurant : {\nbook_time : [20:00]}},\nAsk_for: {}}}\nTemporary state\nRestaurant-name: XXX\nRestaurant-book_time: 20:00 Strategy-based \nupdating\nFilter JSON\nBased on Context state \nBased on Temporary state \n{User: {\nReject: {}, \nRequest: {Restaurant: {\nname: [XXX], \nbook_time: [19:00]}}}}\nContext\nMessage\nNew! \nContext\nMessage\n{User: {\nReject: {}, \nRequest: {Restaurant: {\nbook_time: [20:00]}}}}\n{Sys: {\nNot_available: {}, \nInfo: {Restaurant : {\nname: [XXX]}},\nAsk_for: {}}\nModule\nFigure 2: Our framework of ParsingDST that includes filter module.\nhow the state updating strategies work during DST\nwhich makes the DST process a black box. And\nwe find the main obstacle in the current zero-shot\nDST method is that the large language models\n(LLMs)’ understanding of updating strategies have\nnot aligned with the DST task.\nIn this work, we proposed a new ICL method\nnamed ParsingDST for zero-shot DST. Specifically,\nwe reformulate the DST task by semantic parsing\nthat translates the origin dialogue text into the\ndesigned JSON format (Text-to-JSON)as the in-\ntermediate state through the LLMs The translated\nJSON data will include the information about the\nspeaker, interaction action, domain, slot, and cor-\nresponding values in a formatted and structured\nmanner, which enables the introduction of manual\nstate updating strategies later.\nWe illustrated the significance of updating strate-\ngies by presenting an intricate scenario in Figure 1.\nPrevious ICL approaches typically decode the dia-\nlogue to directly extract the values of all slots, such\nas \"hotel-name\", \"price-range\", and \"attraction-\nname\" in the example, without considering the up-\ndating strategies, which leads to errors in the result.\nIn contrast, our method applies additional updating\nstrategies after the text-to-JSON conversion. We\ndelete non-entity slots like \"price_range\" when it\nappears in the system but not in the context, and we\nset the value of the \"hotel name\" to \"[Delete]\" when\nit is mentioned in the user’s \"reject\" action, which\nmakes our result aligns with the golden label.\nBased on our observations, we noticed that when\nmerging context, system, and user utterances as di-\nalogue input like the previous methods, the LLMs\nwill often exhibit confusion between these differ-\nent types of information, which will invalidate the\nupdating strategies. To address this, we also de-\nsigned a companion framework with more process\nmodules to ensure the effectiveness of updating\nstrategies and conducted experiments to validate\nthem.1 We choose to test and compare ICL meth-\n1Codes in github.com/ToLightUpTheSky/ParsingDST\nods with the powerful LLMs by OpenAI2: gpt-3.5-\nturbo-0301 3 and text-davinci-003 (Brown et al.,\n2020) on MultiWoz 2.1&2.4 dataset. :\nIn summary, our work makes the following con-\ntributions: (1) To our best knowledge, we are the\nfirst to apply text-to-JSON by semantic parsing as\nthe transition between dialogue to state, making it\npossible to tool additional state updating strategies\nand other modules, making the process of DST\nmore controllable and interpretable. (2) We pro-\nposed a novel framework with modules in the pro-\ncess of text-to-JSON to maintain the effectiveness\nof updating strategies. (3) We experiment with\ndifferent ICL methods with various LLMs on Mul-\ntiWOZ 2.1&2.4, proving our ICL method outper-\nforms the previous in the same model and achieves\na new state-of-art in zero-shot DST task.\n2 DST System\n2.1 Data format design\nAs we mentioned before, we translate the origin\ndialogue text to the JSON format which includes\ninteractive actions. The domain, slot, and value in\nJSON are inferred from the utterance. And the\nformats of User JSON and System JSON are\ndifferent. More details about JSON format and our\nprompt are in Appendix A.3.\n2.2 Dialogue context representation\nIn order to maintain semantic consistency, we still\nuse JSON format as context representation. When\ntranslating system’s utterance, we put all domain-\nslot-value pairs from the context state into the \"re-\nquest\" action of the data that is User JSON’s format\nas context representation. Then When translating\nuser’s utterance, we merge the previous context\nrepresentation and the System JSON as the new\ncontext representation.\n2More details about models in https://openai.com/\n3Snapshot of gpt-3.5-turbo from March 1st, 2023, this\nmodel will not receive updates.\n11094\nMultiWoZ 2.1 Attraction Hotel Restaurant Taxi Train A VG\nSupervised fine-tuning method\nSimpleTOD++ (Hosseini-Asl et al., 2020) 28.01 17.69 15.57 59.22 27.75 29.65\nT5DST + description (Lin et al., 2021b) 33.09 21.21 21.65 64.62 35.42 35.20\nTransferQA (Lin et al., 2021a) 31.25 22.72 26.28 61.87 36.72 35.77\nPrevious sota ICL method with different LLMs\nIC-DST Codex (deprecated) 59.97 46.69 57.28 71.35 49.37 56.92\nIC-DST Text-davinci-003 50.69 38.55 43.98 71.25 45.99 50.09\nIC-DST Gpt-3.5-turbo-0301 59.32 40.20 46.50 68.32 52.87 53.13\nOur method with different LLMs\nParsingDST Text-davinci-003 64.60+13.91 39.92+1.37 62.55+18.57 80.45+9.20 51.89+5.90 59.88+9.79\nParsingDST Gpt-3.5-turbo-0301 64.95+5.63 46.76+6.56 67.04+20.54 80.26+11.94 62.78+9.91 63.36+10.23\n- filter module 63.15+3.20 45.35+5.15 66.60+20.10 80.25+11.93 58.41+5.54 62.75+9.44\n- framework 64.92+5.60 46.63+6.43 66.95+20.45 75.42+7.10 52.63−0.24 61.71+8.40\nMultiWoZ 2.4 Attraction Hotel Restaurant Taxi Train A VG\nIC-DST Gpt-3.5-turbo-0301 59.81 40.20 46.80 68.32 52.87 53.60\nParsingDST Gpt-3.5-turbo-0301 65.63+5.81 46.76+6.56 67.67+20.87 80.58+12.26 62.59+9.72 64.65+11.05\nTable 1: Zero-shot per-domain JGA on MultiWOZ 2.1&2.4, we calculate the average of all per-domain JGA as the\nmeasure of overall performance, and we use subscripts to indicate the JGA changes of our methods compared to the\nprevious sota ICL method under the same model.\n2.3 Updating Strategies\nThrough the observation of the dataset, we have\nformulated some status update strategies, which\nare based on rulers and will influenced by the\nspeaker, interactive action, and entity or not. As\nfor slots in generated User JSON: (1) The slot in\n\"reject\" action will be deleted later; (2) The slot in\n\"request\" action will be updated. As for System\nJSON: (1) The slot in \"ask_for\" or \"not_avaliable\"\nactions wil be ignored, these actions are for\nreducing the mistakes of interaction classification\nin semantic parsing; (2) The slot in \"info\" action\nwill be updated when it is an entity or it also in\ncontext state, but in other cases, we will ignore it .\n2.4 Modules\nThe introduction of the new framework makes it\npossible to use additional modules to correct the\ngenerated previous content. In this paper, we tried\na simple module that filters System JSON to make\nit only include the updated slot or entity slot in our\nframework.\n2.5 Framework design\nThe motivation for designing this framework is to\navoid harmful information from the history state or\nprevious speaker being mixed into the JSON gen-\nerated by the subsequent speaker and affecting the\neffectiveness of the update strategy. Our framework\nmodifies the shared input-output method which pro-\ncesses all together. We make it asynchronous and\nadd more modules in the pipeline: process the con-\ntext, the system utterance, and the user utterance\ndividedly. As shown in figure 2. The details of the\nsteps in our framework pipeline are as follows:\nStep1. In the t turn of dialogue, convert the context\nstate St−1 to JSON format Ct as context represen-\ntation (mentioned in section 2.2) in equation (1),\nthen merge it with the system utterance At\nutt as\nthe dialogue input for LLM, generate the System\nJSON At\nJSON in equation (2):\nCt = State2JSON (St−1) (1)\nAt\nJSON = Trans 2JSON (Ct, At\nutt) (2)\nStep2. Then the context state is updated by updat-\ning strategies and the System JSON and saved as\nthe temporary state St\ntemp as in equation (3):\nSt\ntemp = Update(St−1, At\nJSON ) (3)\nStep3. Equation (4) will filter the generated System\nJSON with the module mentioned in section 2.4:\nAt\nJSON = Filter (At\nJSON ) (4)\nStep4. Equation (5) combines context JSON and\nfiltered System JSON as the new context represen-\ntation, then equation (6) inputs the new context\nwith user’s utterance Ut\nutt together and generate\nUser JSON Ut\nJSON with LLM:\nCt = Ct + At\nJSON (5)\nUt\nJSON = Trans 2JSON (Ct, Ut\nutt) (6)\nStep5. Finally, the temporary state is updated by\nthe updating strategies and the User JSON to get\nour dialogue state of this turn St in quation (7):\nSt = Update(St\ntemp, Ut\nJSON ) (7)\nIf it is the first turn, the process will start from\nstep4 and At\nJSON will be empty.\n11095\n98.52\n96.67\n98.3\n98.96\n98.2397.94\n94.73\n95.95\n97.78\n97.16\nAttraction Hotel Restaurant Taxi Train\nSlot Accuracy  (%)\nParsingDST IC-DST\nFigure 3: The average slot accuracy in 5 different do-\nmains of MultiWOZ 2.1 with GPT -3.5 model.\n3 Experiment\n3.1 Dataset, Metric, and Evaluation\nMultiWOZ (Budzianowski et al., 2018) is a multi-\ndomain human-human written dialogue dataset, the\nlabels and utterances have been refined in subse-\nquent versions, e.g., MultiWOZ 2.1 (Eric et al.,\n2019) and MultiWOZ2.4 (Ye et al., 2021) is a\ncleaner version. We use Joint Goal Accuracy (JGA)\nwhich is the average accuracy of predicting all slot\nassignments for a given service in a turn correctly\nto evaluate the main results of models. Regarding\ntext preprocessing and labeling, we followed most\nof the previous work of IC-DST (Hu et al., 2022) on\npreprocessing and labeling data. The only change\nis that we renamed some names of the slots and do-\nmains. More details about baselines and zero-shot\nsettings are in Appendix A.1 and Appendix A.2.\n3.2 Results and Analysis\nMain Result. Table 1 shows the zero-shot per-\ndomain JGA result on MultiWOZ 2.1, and A VG\nJGA as the measure of overall performance. Be-\ncause IC-DST’s best performance model Codex-\nDavinci (Chen et al., 2021) has been deprecated\nby openAI, we supplement two experiment results\nwith GPT-3.5 and Text-davinci models. We can\nfind from the result that the supervised fine-tuning\nmethod is far inferior to the ICL method. And no\nmatter in which domain, our method always out-\nperforms the previous sota ICL method IC-DST by\na large-scale margin in the same model. For exam-\nple, using the same GPT-3.5 model, we achieved a\n20.54% increase in JGA in the restaurant domain.\nAlthough the supplemented experiments of IC-DST\nshow worse performance compared to the Codex\nmodel, our ParsingDST method still outperforms\nthe IC-DST Codex in all five domains with the\nsame model of supplemented experiments, which\nshows the strong ability of our method and the great\nID: MUL1350.json    Turn: 1\nContext state:\n{\"hotel-area\": \"east\", \"hotel-pricerange\": \"cheap\"}\nDialogue: \nSystem: i have found 3 , all of them are guest houses and all of them have free internet and parking . \nwould you like me to give your their information ?\nUser: sure , that sounds great .\nID: MUL1489.json    Turn: 2\nContext state:\n{\"restaurant-food\": \"french\", \"restaurant-area\": \"centre\"} \nDialogue: \nSystem: i have the cote in the centre . it is in the expensive range . would you like to make a booking ? \nUser: yes , please . i need a table for 8 on friday at 17:30 , please . \nIC-DST:\n{\"restaurant-food\": \"french\", \"restaurant-area\": \"centre\", \"restaurant-name\": \"cote\", \n\"restaurant-pricerange\": \"expensive\", \"restaurant-book time\": \"17:30\", \"restaurant-book day\": \n\"friday\", \"restaurant-book people\": \"8\"} \nParsingDST:\n{\"restaurant-food\": \"french\", \"restaurant-area\": \"centre\", \"restaurant-name\": \"cote’, \"restaurant-book \npeople\": \"8\", \"restaurant-book time\": \"17:30\", \"restaurant-book day\": \"friday\"} \nPasrsingDST: \nCompletion: \n1. {\"system\": {\"not_available\": {}, \"info\": {\"lodging\": {\"lodging_type\": [\"guest house\"], \"internet\": \n[\"yes\"], \"parking\": [\"yes\"]}}, \"ask_for\": {\"lodging\": []}}} \n2. {\"user\": {\"reject\": {}, \"request\": {\"lodging\": {\"price_range\": [\"cheap\"], \"direction\": [\"east\"]}}}}\nResult: \n{\"hotel-pricerange\": \"cheap\", \"hotel-area\": \"east\"}\nParsingDST (w/o framework): \nCompletion: \n1. {\"system\": {\"not_available\": {}, \"info\": {\"lodging\": {\"lodging_type\": [\"guest house\"], \"internet\": \n[\"yes\"], \"parking\": [\"yes\"]}}, \"ask_for\": {}}, \"user\": {\"reject\": {}, \"request\": {\"lodging\": \n{\"price_range\": [\"cheap\"], \"direction\": [\"east\"], \"internet\": [\"yes\"], \"parking\": [\"yes\"]}}}}\nResult:\n{\"hotel-pricerange\": \"cheap\", \"hotel-area\": \"east\", \"hotel-internet\": \"yes\", \"hotel-parking\":  \"yes\"}\nFigure 4: Two representative samples in the test set of\nMultiWOZ 2.1.\ninfluence of updating strategies in zero-shot DST.\nAblation Studies.We conduct an ablation study to\nbetter prove the effectiveness of our modules and\nnew framework. In the situation without the frame-\nwork, we follow the traditional setting to merge\ncontext, user, and system utterance as dialogue in-\nput, then output JSON all at once. As shown in Ta-\nble 1, the overall performance degrades whether the\nfilter module or our framework is discarded. How-\never, we observed that the JGA in certain domains\nremained similar to the previous performance dur-\ning our ablation studies, we believe that is because\nour modules and updating strategies still need to\nbe optimized.\nSlot Accuracy Analysis.Figure 3 shows the slot\naccuracy of models using PasingDST and IC-DST\nwith the Gpt-3.5 model. It can be seen that our\nmethod achieves better results on all 5 domains\ncompared to the IC-DST (e.g., 2.88% improvement\nin the restaurant domain), which further proves the\neffectiveness of our method. We speculate that it is\ncrucial for DST to introduce an appropriate dialog\nstate update strategy into the model.\nCase Studies. To further illustrate the effective-\nness of our framework, Figure 4 shows two rep-\nresentative samples of different methods’ predic-\ntions. In the first case, the IC-DST method makes\na wrong prediction that includes the non-entity slot\n\"pricerange\" from system utterance that does not\nappear in context, however, our method can handle\nthe situation well because of the introduction of\nupdating strategies. In the second case, parsing-\n11096\nDST makes a wrong prediction when removing\nits framework, because its User JSON included\nslots \"internet\" and \"parking\" from system utter-\nance, which makes the updating strategies invalid\nbecause these slots will be updated rather than be\nignored. In our method, we can process System\nJSON and context JSON with modules that are\nincluded in the framework before the harmful in-\nformation influences the latter User JSON.\n4 Conclusion\nIn this paper, we reformulate DST to semantic pars-\ning with LLMs that translate the dialogue text into\nJSON as the intermediate state, which enables us\nto introduce updating strategies and makes DST\nprocess more controllable and interpretable. Fur-\nthermore, we present a novel framework that in-\ncludes more modules within the text-to-JSON con-\nversion process to ensure the effectiveness of up-\ndating strategies. Through experiments on the Mul-\ntiWOZ 2.1&2.4 dataset, our system achieves sota\nperformance in zero-shot settings. Our analyses\nshowed that our method surpasses the previous\nICL method and the innovative modules and frame-\nwork benefit the overall performance. In future\nwork, we plan to explore and evaluate more variety\nof updating strategies and modules to further en-\nhance our framework and we will test our method\nin more open-source LLMs like Llama (Touvron\net al., 2023).\nLimitations\nThis work has two main limitations: (1) The per-\nformance of our framework is highly dependent on\nthe inference language model, which may limit the\nframework’s usage. For example, it depends on the\nability of LLMs that are pre-trained in JSON and\nnatural language data and can understand both. (2)\nLack the detailed comparison of various updating\nstrategies and modules in the framework. We will\ndesign and test more kinds of updating strategies\nand modules for our framework in future work.\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nPaweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, Inigo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Gaši ´c. 2018. Multiwoz–a\nlarge-scale multi-domain wizard-of-oz dataset for\ntask-oriented dialogue modelling. arXiv preprint\narXiv:1810.00278.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nMihail Eric, Rahul Goel, Shachi Paul, Adarsh Kumar,\nAbhishek Sethi, Peter Ku, Anuj Kumar Goyal, San-\nchit Agarwal, Shuyang Gao, and Dilek Hakkani-Tur.\n2019. Multiwoz 2.1: A consolidated multi-domain\ndialogue dataset with state corrections and state track-\ning baselines. arXiv preprint arXiv:1907.01669.\nMichael Heck, Carel van Niekerk, Nurul Lubis, Chris-\ntian Geishauser, Hsien-Chin Lin, Marco Moresi, and\nMilica Gaši´c. 2020. Trippy: A triple copy strategy\nfor value independent neural dialog state tracking.\narXiv preprint arXiv:2005.02877.\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu,\nSemih Yavuz, and Richard Socher. 2020. A simple\nlanguage model for task-oriented dialogue. Advances\nin Neural Information Processing Systems, 33:20179–\n20191.\nYushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu,\nNoah A Smith, and Mari Ostendorf. 2022. In-context\nlearning for few-shot dialogue state tracking. arXiv\npreprint arXiv:2203.08568.\nChia-Hsuan Lee, Hao Cheng, and Mari Ostendorf.\n2021. Dialogue state tracking with a language model\nusing schema-driven prompting. arXiv preprint\narXiv:2109.07506.\nZhaojiang Lin, Bing Liu, Andrea Madotto, Seungwhan\nMoon, Paul Crook, Zhenpeng Zhou, Zhiguang Wang,\nZhou Yu, Eunjoon Cho, Rajen Subba, et al. 2021a.\nZero-shot dialogue state tracking via cross-task trans-\nfer. arXiv preprint arXiv:2109.04655.\nZhaojiang Lin, Bing Liu, Seungwhan Moon, Paul\nCrook, Zhenpeng Zhou, Zhiguang Wang, Zhou Yu,\nAndrea Madotto, Eunjoon Cho, and Rajen Subba.\n2021b. Leveraging slot descriptions for zero-shot\ncross-domain dialogue state tracking. arXiv preprint\narXiv:2105.04222.\nYiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang,\nYuanyuan Yang, Jiaming Tian, Hao He, Antong Li,\nMengshen He, Zhengliang Liu, et al. 2023. Summary\nof chatgpt/gpt-4 research and perspective towards\n11097\nthe future of large language models. arXiv preprint\narXiv:2304.01852.\nAndrea Madotto, Zhaojiang Lin, Genta Indra Winata,\nand Pascale Fung. 2021. Few-shot bot: Prompt-\nbased learning for dialogue systems. arXiv preprint\narXiv:2110.08118.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nYizhong Wang, Swaroop Mishra, Pegah Alipoor-\nmolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan\nDhanasekaran, Atharva Naik, David Stap, et al. 2022.\nBenchmarking generalization via in-context instruc-\ntions on 1,600+ language tasks. arXiv preprint\narXiv:2204.07705.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022a. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022b. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nChien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-\nAsl, Caiming Xiong, Richard Socher, and Pascale\nFung. 2019. Transferable multi-domain state genera-\ntor for task-oriented dialogue systems. arXiv preprint\narXiv:1905.08743.\nTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong,\nTorsten Scholak, Michihiro Yasunaga, Chien-Sheng\nWu, Ming Zhong, Pengcheng Yin, Sida I Wang,\net al. 2022. Unifiedskg: Unifying and multi-tasking\nstructured knowledge grounding with text-to-text lan-\nguage models. arXiv preprint arXiv:2201.05966.\nFanghua Ye, Jarana Manotumruksa, and Emine Yilmaz.\n2021. Multiwoz 2.4: A multi-domain task-oriented\ndialogue dataset with essential annotation corrections\nto improve state tracking evaluation. arXiv preprint\narXiv:2104.00773.\nJeffrey Zhao, Raghav Gupta, Yuan Cao, Dian Yu,\nMingqiu Wang, Harrison Lee, Abhinav Rastogi,\nIzhak Shafran, and Yonghui Wu. 2022. Description-\ndriven task-oriented dialog modeling. arXiv preprint\narXiv:2201.08904.\nA Appendix\nA.1 Baseline\nSimpleTOD (Hosseini-Asl et al., 2020)Success-\nfully leverage the pretrained language model GPT-\n2 for the end-to-end TOD modeling in the unified\nway.\nT5DST (Lin et al., 2021b)A slot description en-\nhanced approach for zero-shot & few-shot cross-\ndomain DST based on T5.\nTransferQA (Lin et al., 2021a)Reformulated DST\nas QA problem. The model is pre-trained with a\nlarge amount of QA data. At inference time, the\nmodel predicts slot values by taking synthesized\nextractive questions as input\nIC-DST (Hu et al., 2022)An in-context learn-\ning method for zero-shot & few-shot DST by text-\nto-SQL with LLMs. It’s the previous state-of-art\nmethod that outperforms the supervised fine-tuning\nmethods in zero-shot DST.\nA.2 Zero-shot setting of ICL frameworks\nThere are no labeled examples to retrieve, but for-\nmatting examples are included, following previous\nzero-shot learning work. (Wang et al., 2022), and\nwe set the temperature parameter of LLMs to 0 for\na stable result.\nA.3 Prompt design\nBelow are the template of the user and system’s\nprompt in zero-shot settings. The last example\nis the test instance that needs to be completed.\nWe find the example is more useful than the\ntask-description prompt in the in-context learning,\nso we use a simple example that includes all slots\nof the domain to map the relationship between\nslot and value in the dialogue rather than directly\ndescribe the slot and its value. Furthermore, for\nLLMs to understand the meaning of action in\nJSON format, we also formulate some examples to\ndemonstrate some interaction typical scenarios.\nSystem prompt template\ntranslate system message to JSON:\ndata format of JSON:\ninput message:\nsystem: \"...\"\noutput JSON:\n{\"system\": {\"not_available\": {domain: {slot: [value]}}, \"info\": {domain: {slot:\n[value]}}, \"ask_for\": {domain: [slot]}}}\n[END]\nexample:\n11098\ninput message:\nsystem: \"the booking for restaurant at 10:00 on sunday was successful\"\noutput JSON:\n{\"system\": {\"not_available\": {}, \"info\": {\"restaurant\": {\"clock_book\":\n[\"10:00\"], \"week_day\": [\"sunday\"]}}, \"ask_for\": {}}}\n[END]\nexample:\ninput message:\nsystem: \"it is a chinese restaurant in the centre\"\noutput JSON:\n{\"system\": {\"not_available\": {}, \"info\": {\"cuisine\": [\"chinese\"], \"direction\":\n[\"centre\"]}, \"ask_for\": {}}}\n[END]\nexample:\ninput message:\nsystem: \"how about abc restaurant in the city centre\"\noutput JSON:\n{\"system\": {\"not_available\": {}, \"info\": {\"restaurant\": {\"full_name\": [\"abc\nrestaurant\"], \"direction\": [\"centre\"]}}, \"ask_for\": {}}}\n[END]\nexample:\ninput message:\nsystem: \"how about the part of the area and food type for the restaurant\"\noutput JSON:\n{\"system\": {\"not_available\": {}, \"info\": {}, \"ask_for\": {\"restaurant\":\n[\"direction\", \"cuisine\"]}}}\n[END]\nexample:\ninput message:\nsystem: \"do you need certain price range or part of area for restaurant\"\noutput JSON:\n{\"system\": {\"not_available\": {}, \"info\": {}, \"ask_for\": {\"restaurant\":\n[\"price_range\", \"direction\"]}}}\n[END]\nexample:\ninput message:\nsystem: \"sorry i can not book restaurant nusa for you . i can only find nandos\"\noutput JSON:\n{\"system\": {\"not_available\": {\"restaurant\": {\"full_name\": [\"nusha\"]}}, \"info\":\n{\"restaurant\": {\"full_name\": [\"nandos\"]}}, \"ask_for\": {}}}\n[END]\n[DM]\n[ST]\n[KW]\nexample:\ncontext:\n[PREDIC]\ninput message:\n[DIALOG]\noutput JSON:\nUser prompt template\ntranslate user message to JSON:\ndata format of JSON:\ninput message:\nuser: \"...\"\noutput JSON:\n{\"user\": {\"reject\": {domain: [slot]}, \"request\": {domain: {slot: [value]}}}}\n[END]\nexample: input message:\nuser: \"i want a place to eat . in the city centre . with cheap price\"\noutput JSON:\n{\"user\": {\"reject\": {}, \"request\": {\"restaurant\": {\"direction\": [\"centre\"],\n\"price_range\": [\"cheap\"]}}}}\n[END]\nexample:\ninput message:\nuser: \"no particular food type\"\noutput JSON:\n{\"user\": {\"reject\": {}, \"request\": {\"restaurant\": {\"cuisine\": [\"any\"]}}}}\n[END]\nexample:\n{\"system\": {\"not_avaliable\": {}, \"info\": {}, \"ask_for\": {\"restaurant\":\n[\"price_range\", \"cuisine\"]}}}\ninput message: user: \"no , i am not picky as long as it book for 4 on sunday\"\noutput JSON:\n{\"user\": {\"reject\": {}, \"request\": {\"restaurant\": {\"price_range\": [\"any\"],\n\"cuisine\": [\"any\"], \"num_people\": [\"4\"], \"week_day\": [\"sunday\"]}}}}\n[END]\nexample:\ninput message:\nuser: \"i want to be in the east of town . can i get their phone number and\naddress please\"\noutput JSON:\n{\"user\": {\"reject\": {}, \"request\": {\"restaurant\": {\"direction\": [\"east\"],\n\"phone_number\": [], \"address\": []}}}}\n[END]\nexample:\ninput message:\nuser: \"nusha is not a restaurant but an attraction\"\noutput JSON:\n{\"user\": {\"reject\": {\"restaurant\": [\"full_name\"]}, \"request\": {\"attraction\":\n{\"full_name\": [\"nusha\"]}}}}\n[END]\n[DM]\n[EXM]\n[ST]\n[KW]\nexample:\ncontext:\n[PREDIC]\ninput message:\n[DIALOG]\noutput JSON:\nSpecial tokens in prompt\nThe prompt has some special tokens which will\nbe replace by something else in the data-bulding\nprocess:\n\"[DM]\" is the domain list.\n\"[EXM]\" is the domain example which is a user\nutterance that includes all slots of the domain and\nits translated JSON.\n\"[ST]\" is the slot list of the domain.\n\"[KW]\" is the possible choice of some slot.\n\"[PREDIC]\" is the context JSON.\n\"[DIALOG]\" is the utterance that should be trans-\nlated to JSON.\n\"[END]\" is the stop token.\n11099",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8385095000267029
    },
    {
      "name": "JSON",
      "score": 0.790308952331543
    },
    {
      "name": "Parsing",
      "score": 0.7653211355209351
    },
    {
      "name": "Task (project management)",
      "score": 0.6165472269058228
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5808308720588684
    },
    {
      "name": "Natural language processing",
      "score": 0.5723229646682739
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5437269806861877
    },
    {
      "name": "Shot (pellet)",
      "score": 0.470093697309494
    },
    {
      "name": "State (computer science)",
      "score": 0.44275227189064026
    },
    {
      "name": "Process (computing)",
      "score": 0.4293655753135681
    },
    {
      "name": "Tracking (education)",
      "score": 0.425401508808136
    },
    {
      "name": "Programming language",
      "score": 0.3183872699737549
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Pedagogy",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139759216",
      "name": "Beijing University of Posts and Telecommunications",
      "country": "CN"
    }
  ]
}