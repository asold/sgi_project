{
  "title": "Vision-and-Language Pretrained Models: A Survey",
  "url": "https://openalex.org/W3213351348",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2995014304",
      "name": "Siqu Long",
      "affiliations": [
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A4382325315",
      "name": "Feiqi Cao",
      "affiliations": [
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2119577058",
      "name": "Soyeon Caren Han",
      "affiliations": [
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2167365918",
      "name": "Haiqin Yang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3033518368",
    "https://openalex.org/W3039265083",
    "https://openalex.org/W3035485997",
    "https://openalex.org/W3172675210",
    "https://openalex.org/W3025569729",
    "https://openalex.org/W3034500398",
    "https://openalex.org/W3106008498",
    "https://openalex.org/W2912041567",
    "https://openalex.org/W3093124244",
    "https://openalex.org/W3035688398",
    "https://openalex.org/W3165195325",
    "https://openalex.org/W3201264086",
    "https://openalex.org/W3164733978",
    "https://openalex.org/W3128785555",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W3139253280",
    "https://openalex.org/W3027142126",
    "https://openalex.org/W3014611590",
    "https://openalex.org/W3035011799",
    "https://openalex.org/W3164784120",
    "https://openalex.org/W3168851777",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W3023074479",
    "https://openalex.org/W3108458441",
    "https://openalex.org/W3173220247",
    "https://openalex.org/W3201703290",
    "https://openalex.org/W3109097593",
    "https://openalex.org/W3177224328",
    "https://openalex.org/W3119655479",
    "https://openalex.org/W2069898171",
    "https://openalex.org/W3127930610",
    "https://openalex.org/W2294516783",
    "https://openalex.org/W3116651605",
    "https://openalex.org/W2611386757",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W3157889929",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2918342466",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W4287555689",
    "https://openalex.org/W3102269903",
    "https://openalex.org/W2138427043",
    "https://openalex.org/W3165938948",
    "https://openalex.org/W3164087725",
    "https://openalex.org/W1664911395",
    "https://openalex.org/W2943714799",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W3193402170",
    "https://openalex.org/W2982291409",
    "https://openalex.org/W3199618438",
    "https://openalex.org/W3089843526",
    "https://openalex.org/W3037601681",
    "https://openalex.org/W3034727271",
    "https://openalex.org/W3088680691",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W3174476431",
    "https://openalex.org/W3154623081",
    "https://openalex.org/W2775061087",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W3136745296",
    "https://openalex.org/W2945591540",
    "https://openalex.org/W3184784418",
    "https://openalex.org/W4312910992",
    "https://openalex.org/W3166304536",
    "https://openalex.org/W2963716836",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W2997021962",
    "https://openalex.org/W2768308213",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W3173909648",
    "https://openalex.org/W3095775920",
    "https://openalex.org/W3175095612",
    "https://openalex.org/W3198196812",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W3208314443",
    "https://openalex.org/W3157960714",
    "https://openalex.org/W2299417213",
    "https://openalex.org/W3130005562",
    "https://openalex.org/W2802200505",
    "https://openalex.org/W3209274285",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3178697201"
  ],
  "abstract": "Pretrained models have produced great success in both Computer Vision (CV) and Natural Language Processing (NLP). This progress leads to learning joint representations of vision and language pretraining by feeding visual and linguistic contents into a multi-layer transformer, Visual-Language Pretrained Models (VLPMs). In this paper, we present an overview of the major advances achieved in VLPMs for producing joint representations of vision and language. As the preliminaries, we briefly describe the general task definition and genetic architecture of VLPMs. We first discuss the language and vision data encoding methods and then present the mainstream VLPM structure as the core content. We further summarise several essential pretraining and fine-tuning strategies. Finally, we highlight three future directions for both CV and NLP researchers to provide insightful guidance.",
  "full_text": "Vision-and-Language Pretrained Models: A Survey\nSiqu Long1 , Feiqi Cao1 , Soyeon Caren Han1 and Haiqin Yang2∗\n1School of Computer Science, The University of Sydney, Australia\n2International Digital Economy Academy (IDEA), China\n{slon6753, fcao0492}@uni.sydney.edu.au, caren.han@sydney.edu.au, hqyang@ieee.org\nAbstract\nPretrained models have produced great success in\nboth Computer Vision (CV) and Natural Language\nProcessing (NLP). This progress leads to learning\njoint representations of vision and language pre-\ntraining by feeding visual and linguistic contents\ninto a multi-layer transformer, Visual-Language\nPretrained Models (VLPMs). In this paper, we\npresent an overview of the major advances achieved\nin VLPMs for producing joint representations of vi-\nsion and language. As the preliminaries, we briefly\ndescribe the general task definition and genetic ar-\nchitecture of VLPMs. We first discuss the lan-\nguage and vision data encoding methods and then\npresent the mainstream VLPM structure as the core\ncontent. We further summarise several essential\npretraining and fine-tuning strategies. Finally, we\nhighlight three future directions for both CV and\nNLP researchers to provide insightful guidance.\n1 Introduction\nIn both Computer Vision (CV) and Natural Language Pro-\ncessing (NLP) communities, pretrained models have made\nsignificant progress. While CV researchers use VGG and\nResNet using ImageNet to predict the categorical label of\na given image, BERT [Devlin et al., 2019 ] has been used\nand revolutionised many NLP tasks, such as natural lan-\nguage inference, and reading comprehension. Motivated by\nthis, many cross-modal Vision-Language Pretrained Models\n(VLPMs) have been designed[Lu et al., 2019; Suet al., 2019;\nChen et al., 2020; Li et al. , 2020b ]. This pretrain-then-\ntransfer learning approach to vision-language tasks naturally\nfollows its widespread use in both CV and NLP. It has become\nthe de facto standard due to the ease of use and solid repre-\nsentational power of large, publicly available models trained\non large-scaled data sources.\nIn this paper, we present an overview of the rise and major\nadvances achieved in the topic of VLPMs. Figure 1 illustrates\na generic architecture of VLPMs. It involves the design of\nfour main components: 1) V/L (Vision and Language) Raw\n∗Corresponding author: hqyang@ieee.org. Work done when\nSiqu and Feiqi were interned at IDEA.\nFigure 1: General architecture of VLPMs\nInput Data defines the representative raw data streams from\nlanguage and visual contents respectively, such as a single or\nmultiple sentence(s) and one or a set of image(s). 2) V/L\nRepresentation processes the raw data input into the desired\nformat of modality representations that can be used for 3)\nV-L (Vision-Language) Interaction Model, which then en-\nforces the cross-modal modeling between the two modalities.\nFor instance, a common design is that the textual sentence\nis first tokenized and converted into the Bert-formatted input\nembedding while the image is processed into a set of spatial-\naware RoI (Region of Interest) features. Those two modality\nrepresentations are then concatenated and fed into the trans-\nformer encoder layers in which the cross-modal interaction\nis modeled via the multi-head self-attention mechanism. 4)\nV-L Representation defines the possible cross-modal repre-\nsentations, which can be a V-L representation for the single\nmodality (i.e., Language or Vision) and/or theV-L representa-\ntion of joint modalities (i.e., Language and Vision). With the\nwell-designed task supervision and learning guidelines from\nthe pretraining, the V-L representation finally learns to rep-\nresent the generic cross-modal semantics, which would be\ntransferred to help with the downstream V-L tasks via fine-\ntuning. This generic architecture applies to most of the ex-\nisting VLPMs. The designs are various for each component,\ntheir pretraining strategies and transfer applications.\nExisting surveys in this area have only partially reviewed\nsome related tasks [Mogadala et al., 2021] or focused mainly\non systematical analysis [Bugliarello et al., 2021 ]. To the\nbest of our knowledge, this is the first work that presents a\ncomprehensive review of VLPMs. Our paper aims to provide\nboth CV and NLP researchers insightful guidance for visual\nand language cross-modal learning via pretraining.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSurvey Track\n5530\n2 Input Data Encoding\n2.1 Language Encoding\nMost VLPMs represent the language input by taking a sin-\ngle textual sentence, which directly aligns with the image (vi-\nsual modality) since their pretraining process mainly relies on\npairwise image-text corpus [Li et al., 2019; Tan and Bansal,\n2019; Lu et al., 2019; Li et al., 2020a; Chen et al., 2020;\nYu et al., 2021; Li et al., 2020b]. Some VLPMs use multiple\nsentences for representing the language input, especially with\nvisual dialogue and multi-lingual settings [Wang et al., 2020;\nNi et al., 2021; Fei et al., 2021 ]. Some special VLPMs ap-\nply visual input in the form of text and encode it as a part of\nthe language input [Li et al., 2020b; Yang et al., 2021]. For\ninstance, OSCAR appends a set of object class tags detected\nfrom an image to the textual sentence as a language input\nto learn object-aligned V-L representation [Li et al., 2020b].\nThis early-fusion strategy of vision to language (V2L) at the\nraw input level serves as an anchor point to ease the cross-\nmodal alignment learning.\nTo process the language input into a suitable language\nrepresentation for cross-modal modeling, almost all VLPMs\ndirectly adapt the Bert-formatted input representation [De-\nvlin et al., 2019 ], which sums up three types of learnable\nembeddings for each token in the textual sequence: 1) to-\nken embedding, 2) position embedding, and 3) segment em-\nbedding. The token embedding mostly follows the original\nBert and encodes the textual sequence in the general form of\n“[CLS]w1w2...wi[SEP]” or with some minor modifications,\nwhere wi represents the ith tokenized (sub-)word based on\nthe WordPiece vocabulary and[CLS]/[SEP] are special tokens\nindicating the starting and ending of this sequence. Some\nmodels introduce additional special tokens to better align\nwith specific data or pretraining tasks [Wang et al., 2020;\nZhou et al., 2020; Xia et al., 2021], such as the [EOT] tokens\nfor separating each dialogue turns [Wang et al., 2020 ]. The\nposition embedding is used exactly the same as in the original\nBert, but the segment embedding is adjusted to be modality\nembedding in order to differentiate the two modalities or to\nfurther distinguish the multiple data streams within the lan-\nguage modality when multiple sentences are used, e.g., the\nsegment tokens A/B/C used in VL-Bert [Su et al., 2019 ].\nAs a special case, the visual feature (See Sec.2.2) can also be\nincluded into the language representation as the fourth type of\nembedding, which can be regarded as an early-fusion strategy\nof vision to language (V2L) at the representation level [Su et\nal., 2019; Chiou et al., 2021]. With the Bert input representa-\ntion format, it allows the VLPMs to enable initialising input\nembedding from Bert and directly adopt transformer encoder\nor its variants for the further cross-modal interaction model-\ning with a multi-head self-attention mechanism.\nIntra-modality Processing. In particular, some VLPMs\napply additional intra-modality processing with self-\nattention-based transformer blocks to the aforementioned\nBert-formatted language embeddings for further encoding\nthe intra-modal contextual information, in order to better\nbalance with the already high-level visual feature produced\nby the deep CNN-based extractor from the vision modality\n(see Sec. 2.2) and enable a robust single-modal representa-\ntion of language [Lu et al., 2019; Tan and Bansal, 2019;\nLi et al., 2021b; Yang et al., 2021; Majumdar et al., 2020 ].\nInstead, the resultant transformer output representation would\nbe used as input for the V-L interaction model.\n2.2 Vision Encoding\nWith the processing of language input, the visual input for\nVLPMs is normally a single image that directly aligns with\nthe paired text input [Li et al., 2019; Tan and Bansal, 2019;\nLu et al., 2019; Li et al., 2020a; Chen et al., 2020; Yu et\nal., 2021; Li et al., 2020b ], or a set of image(s) that are se-\nmantically correlated with each other[Majumdar et al., 2020;\nHao et al., 2020 ]. For instance, the pretraining data for\nVision-Language Navigation(VLN) is formed by a textual in-\nstruction with a group of panorama images along the naviga-\ntion trajectory path [Majumdar et al., 2020].\nSimilar to the language representation, most studies en-\ncode visual input into the Bert-style sequential representa-\ntion, which consists of the aforementioned three major em-\nbeddings. The segment embedding is created the same as\nthat in language representation, but the token embedding and\nposition embedding are modified to visual feature and spa-\ntial position embedding for capturing the visual semantics.\nSpecifically, the visual feature is extracted using the CNN-\nbased feature extractors, the most common way in the CV\ndomain. For this, the granularity of representation, i.e., the\ngrouping of image pixels into the sequential visual tokens,\ndecides the alignment level of cross-modal modeling in the\nimage content: 1) RoI -based VLPMs typically apply a pre-\ntrained Faster R-CNN object detector with a CNN backbone\nand extract the visual features of the detected object regions\nfor the visual tokens [Lu et al., 2019; Tan and Bansal, 2019;\nWang et al., 2020; Chen et al., 2020; Li et al., 2020a;\nZhou et al., 2020 ] This is under the assumption that most\nimage-text pairwise data is supposed to have its text describe\nthe salient object (regions) in the corresponding image. Com-\nparatively, some VLPMs simply split the whole image into\ncontinuous 2) patches [Gao et al., 2020; Wang et al., 2021b;\nWen et al., 2021; Huang et al., 2021 ] or even more fine-\ngrained 3) pixels [Huang et al., 2020 ] as visual tokens for\nCNN-based feature extraction and refine the visual represen-\ntation end-to-end, leading to significant speed improvement.\nMore recent studies even propose the shallow, convolution-\nfree embedding that utilizes simple linear projection to en-\ncode visual tokens at thepatches/pixels level as in ViT[Doso-\nvitskiy et al., 2020] for a further efficiency gain [Wang et al.,\n2021a; Singh et al., 2021; Kim et al., 2021 ]. Besides, there\nare also VLPMs encoding each integral 4) image as a visual\ntoken by taking the pooling layer result from a CNN model\n(e.g., EfficientNet) [Hao et al., 2020].\nUnlike the textual tokens with sequential positional rela-\ntion, visual tokens entail spatial positional relationinstead,\nvarying based on different granularity, which can be encoded\nby spatial position embedding. RoI-based VLPMs commonly\nadopt the coordinate-based position embedding [Lu et al.,\n2019; Li et al., 2020a; Zhou et al., 2020; Chen et al., 2020],\nsuch as the 5-dimensional vector representing the normalized\ncoordinates of the RoI bounding boxes and the fraction of\nimage area. Comparatively, pixel/patch-based VLPMs rep-\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSurvey Track\n5531\nresent the pixel location using the 2D-aware vector for the\nrow/column number [Huang et al., 2021 ]. PREV ALENT\n[Hao et al., 2020] for Vision-Language Navigation (VLN) is\nthe only image-based VLPM that specifies the spatial posi-\ntion embedding, which uses the elevation and heading an-\ngle relative to the agent to represent the positional relations\namong panorama images. The spatial position embedding\nis normally combined with the visual feature with or with-\nout segment embedding via one or two FC layers followed by\nLayer Normalization to form the visual representation.\nIntra-modality Processing. Since the transformer block\nused for additional intra-modality processing in language\n(See Sec.2.1) is a global operator whereas the CNN-based vi-\nsual feature extractor is a local operator, it may lead to a dif-\nferent feature distribution between the two modalities [Wen\net al., 2021 ]. Thus, some VLPMs also apply self-attention-\nbased transformer blocks to the initial Bert-style sequen-\ntial visual representation to align with the language intra-\nmodality processing [Tan and Bansal, 2019; Haoet al., 2020;\nSun et al., 2021; Wen et al., 2021 ]. Using transformers\nfor intra-modality processing enables a modality-customized\nencoding with the freedom of selecting a different num-\nber of blocks for each modality. In practice, the language\nmodality normally uses more transformer blocks than the vi-\nsion modality for a better balance [Tan and Bansal, 2019;\nLu et al., 2019; Hao et al., 2020 ]. Besides, several models\nadopt non-Transformer processing, e.g., AoANet [Xia et al.,\n2021] and Visual Dictionary mapping [Huang et al., 2021].\n3 V-L Interaction Model (V-LIM)\nThere are two types of mainstream VLPM model struc-\ntures: (1) Single-stream (1-stream) [Li et al., 2019; Chen\net al., 2020; Li et al., 2020b ], which directly fuse the ini-\ntial language/visual representation by using the joint cross-\nmodal encoder at the initial stage, and (2) Double-stream\n(2-stream) [Tan and Bansal, 2019; Lu et al., 2019; Murahari\net al., 2020], which separately apply the intra-modality pro-\ncessing to two modalities along with a shared cross-modal\nencoder. However, this way of classification is mainly based\non the perspective of intra-modality data-handling. In this\nsection, we briefly review the model structure and the major\noutput of VLPMs by focusing on their V-L interaction mod-\nels (V-LIM) instead, which can be: 1) Self-attention-based,\n2) Co-attention-based or 3) VSE-based V-LIM.\n1) Self-attention-based V-LIM. Most single-stream [Li et\nal., 2019; Li et al., 2020a; Su et al., 2019; Huang et al., 2020;\nGao et al., 2020; Chenet al., 2020; Liet al., 2020b] and some\nof the double-stream VLPMs[Huang et al., 2021] are consid-\nered as a self-attention-based V-LIMs since they directly ap-\nply single-stream self-attention module to the modality repre-\nsentations for cross-modal modeling. They simply concate-\nnate the language and visual representation (as introduced in\nSec.2.1 and Sec.2.2) to produce an integrated sequence rep-\nresentation so that a self-attention stream in the following\ntransformer blocks can enforce the dense interaction model-\ning for both intra-modal (V-V&L-L) and cross-modal (V-L).\nFollowing the transformer, the output representation of the\nglobal-level special token, such as [CLS], in the multi-modal\nsequence would be taken as the holistic joint V-L represen-\ntation for both pretraining and transfer learning. The output\nrepresentation in the language or vision sequence could rep-\nresent the contextualized V-L semantics and thus can be used\nas the V/L representation for Language or Vision.\n2) Co-attention-based V-LIM. Comparatively, VLPMs\nwith co-attention-based V-LIM decouple the intra- and cross-\nmodal modeling processes. They keep two separate streams\nof transformer blocks for each modality that are entangled\nonly at the specific cross-attention sub-layer(s). These sub-\nlayers enforce exchange of the key and value in self-attention\nmodule between the two modality streams. In this way, they\nlimit the cross-modal modeling only to those co-attention\nsub-layers while leaving the rest of the sub-layers indepen-\ndent to focus on intra-modality processing and modeling.\nThis single-modality focus makes them align well with the\ndefinition of double-stream VLPMs [Tan and Bansal, 2019;\nLu et al., 2019; Murahari et al., 2020; Majumdar et al., 2020;\nHao et al., 2020; Yu et al., 2021]. As a result, the V/L repre-\nsentation for Language or Visioncan be taken from the output\nrepresentation of the two separate modality streams whereas\nthe V-L representation for joint modality is derived by taking\nthe global token representation from either modality sequence\n(or their aggregation).\nEncoder-decoder VLPMs. There are some VLPMs that\napply a transformer-based encoder-decoder structure to em-\npower the V-L generation ability. They can be catego-\nrized into either single-stream [Zhou et al., 2020; Wang et\nal., 2020; Wang et al., 2021b; Li et al., 2021a ] or double-\nstream [Xia et al., 2021; Li et al., 2021b ]. Regarding the\nV-LIM, few of them adopt unified encoder-decoder struc-\nture with self-attention-based V-LIM [Zhou et al., 2020;\nWang et al., 2020; Li et al., 2021a]. Others try to emphasize\nthe different peculiarities of the understanding/generation dis-\nciplines and instead use the conventional decoupled encoder-\ndecoder structure [Wang et al., 2021b; Xia et al., 2021;\nLi et al., 2021b ]. Their V-LIM can be either self- or co-\nattention-based or even their combination, depending on how\nthe attention modules in encoder/decoder are utilized for han-\ndling the multi-modalities. In general, these encoder-decoder\nVLPMs derive the V-L representation via the cross-modal en-\ncoder in a similar way to those self-attention-based VLPMs.\n3) VSE-based V-LIM. More recently, there is an-\nother emerging mainstream of VLPMs that simply uti-\nlizes the dual-encoder structured model with Visual-\nSemantic-Embedding(VSE)-based cross-modal contrastive\nlearning [Radford et al., 2021; Jia et al., 2021; Sun et al.,\n2021; Wen et al., 2021 ]. They tend to have a special fo-\ncus on large-scale cross-modal retrieval with high demand\nfor efficiency. They utilize the intra-modality processing\nto derive the vision and language representation (as de-\nscribed in Sec.2.1 and Sec.2.2) between which the similarity-\nbased cross-modal alignment would be modeled in the shared\nVSE space at the global level. This dual-encoder structure\neradicates the fusion-style attention-based V-LIMs that are\ncomputation-costly and time-consuming. At the same time,\nit enables independent V/L representation encoding of both\nmodalities, making the pre-computation possible for more\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSurvey Track\n5532\nefficient retrieval. The resultant V/L representation for lan-\nguage and vision can be independently derived by the learned\nmodality encoders whereas their fused representation can rep-\nresent the V-L representation for joint modality.\n4 Pretraining\nIn this section, we review the pretraining regarding the\ndatasets, tasks and objective designs.\n4.1 Datasets\nConceptual Captions (CC, roughly 3M) and SBU Captions\n(SBU, around 1M) of enormous size and diversified na-\nture are the most commonly used webly collected datasets\nfor VLPM pretraining [Lu et al., 2019; Zhou et al., 2020;\nLi et al., 2021b; Li et al., 2020a; Fei et al., 2021 ]. It is\nfound that a larger sized corpus leads to better performance in\ndownstream transfer tasks [Lu et al., 2019; Wen et al., 2021;\nJia et al., 2021 ]. Some simple Dual-encoder VLPMs [Rad-\nford et al., 2021; Jia et al., 2021 ] collect even larger scaled\ncorpus from the web, such as WIT (400M) [Radford et al.,\n2021] and ALIGN(1.8B) [Jia et al., 2021].\nAnother combination that leads to better domain adaptation\nis the out-of-domain+in-domain datasets, where they define\nthe web-based CC/SBU as out-of-domain and the MS-COCO\n(COCO)/Visual Genome (VG) as the in-domain datasets be-\ncause most downstream tasks are built on them [Chen et al.,\n2020; Yu et al., 2021; Sun et al., 2021; Kim et al., 2021;\nWang et al., 2021a; Li et al., 2021a ]. The in-domain\ndatasets can also be the task-specific datasets that specifi-\ncally come from the commonly evaluated downstream tasks\n(e.g., GQA/VQA2.0) [Lu et al., 2020; Xia et al., 2021;\nZhang et al., 2021; Li et al., 2020b; Wen et al., 2021].\nSome VLPMs target specialized tasks/domains such as Vi-\nsual Dialogue (VD) and pretrain on only the domain/task-\nspecific dataset due to their reduced suitability to the afore-\nmentioned text-image datasets regarding domain nature or\ndataset format [Gao et al., 2020; Zhuge et al., 2021; Wang\net al., 2020; Hao et al., 2020 ]. There are also efforts\nof jointly pretraining with single-modal tasks on auxiliary\nsingle-modality data source, i.e., non-paired image or text\ncollection, for reinforcing single-modal representations [Su\net al., 2019; Li et al., 2021a; Fei et al., 2021; Ni et al., 2021;\nSingh et al., 2021].\n4.2 Tasks and Objectives\nThere are three most commonly used cross-modal pretrain-\ning tasks 1: 1) Cross-modal Masked Language Modeling\n(CMLM) extends the Masked Language Modeling (MLM)\nfrom Bert pretraining [Devlin et al., 2019 ] to multi-modal\nsetting for learning the contextualized V-L representation,\nutilizing the bi-directional attention-based V-LIM. This is\nproved to be helpful for transferring the pretrained language\nmodel Bert into multi-modal setting [Wang et al., 2020 ]\nand thus makes it one of the most essential VLPM pre-\ntraining tasks [Su et al., 2019; Huang et al., 2020; Chen\n1Task/objective names may vary slightly in different papers.\net al., 2020 ]. The task goal is to predict the masked to-\nkens in text sequence based on the observation of their sur-\nrounding context, including both the unmasked textual to-\nkens and all the visual tokens, by minimising the nega-\ntive log-likelihood (NLL) loss. While most VLPMs mask\nout WordPiece sub-tokens as in Bert, some achieve better\ntransfer performance via masking over semantically inte-\ngral token groups such as complete word [Kim et al., 2021;\nGao et al., 2020] or segment [Yuet al., 2021; Li et al., 2021a;\nLi et al., 2020b].\n2) Cross-modal Masked Region Modeling (CMRM)\nis a vision-supervised counterpart of CMLM, initially pro-\nposed by RoI-based VLPMs. It randomly masks out to-\nkens in the visual sequence instead [Chen et al., 2020;\nTan and Bansal, 2019; Lu et al., 2019; Li et al., 2020a;\nSu et al., 2019 ]. Thus far, there exist three variant objec-\ntives: a) Region Label Classification (CMRM C) predicts\nthe object class of each masked region via minimizing the\ncross-entropy (CE) loss calculated based on the one-hot en-\ncoded object class from the object detector as ground-truth\nand the normalized distribution of the VLPM prediction over\nthe possible object classes. To mitigate the potential classi-\nfication error of the object detector, b) Label Distribution\nApproximation (CMRM D) uses the probability distribu-\ntion of the object class as a soft supervision by minimis-\ning the KL divergence loss between the object class distri-\nbution from the object detector and the normalized VLPM\nprediction distribution. Comparatively, c) Region Feature\nRegression (CMRMR) learns to regress the VLPM output\nof each masked region into its input feature derived from the\nobject detector using the L2 loss. It is commonly applied to-\ngether with CMRMC [Tan and Bansal, 2019; Ni et al., 2021;\nLi et al., 2021a ] or CMRM D [Sun et al., 2021; Chen et\nal., 2020] for more robust visual content modeling and joint\ncross-modal learning. Especially, when the visual token is\nrepresented at patch level instead of RoI, CMRM can also\nbe extended to masked patch modeling [Huang et al., 2021;\nZhuge et al., 2021; Gao et al., 2020].\nVLPMs with attention-based V-LIM commonly treat 3)\nCross-modal Alignment (CA) as a binary classification\nproblem, aiming to predict whether the input image-text pair\nis semantically matched or not based on the global V-L\nrepresentation, by applying binary cross-entropy loss. The\nnegative pairs can be sampled by randomly replacing ei-\nther the image/text in the positive pair with another im-\nage/text from other data pairs in the corpus. On the other\nhand, dual-encoder based VLPMs with VSE-based V-LIM\nall regard CA as a ranking problem for finding the best\nmatch and apply the contrastive learning optimization objec-\ntive [Radford et al., 2021; Sun et al., 2021; Jia et al., 2021;\nWen et al., 2021 ]. They learn the decoupled V/L represen-\ntation in the semantically shared VSE space. Most of them\nadopt in-batch negatives for convenience, which simply treats\nthe text/image from the remaining pairs within the batch as\nnegatives. In addition to the two global-level alignment CA\nobjectives above, there are also several fine-grained align-\nment variants [Chen et al., 2020; Kim et al., 2021 ] that try\nto enforce the matching between the fine-grained components\nsuch as visual regions/patches and textual words of the image-\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSurvey Track\n5533\ntext pairs. One essential difference between the global-level\nand the fine-grained objectives is that the former introduces\nnegative samples, which is found harmful for the fusion-\nbased VLPMs with attention-based V-LIMs[Su et al., 2019].\nThus, some VLPMs [Li et al., 2021b; Singh et al., 2021;\nWang et al., 2021a] introduce the single-modal encoders for\nits global-level CA while simultaneously keeping the fusion-\nbased encoders for other tasks, to combine the advantages of\nthe dual-encoder-based and fusion-based structures.\nHowever, these three tasks cannot fulfil the goal of V-L\ngeneration tasks, such as Image Generation (IC) [Zhou et al.,\n2020; Xia et al., 2021; Li et al., 2021b; Wang et al., 2021b].\nThus, encoder-decoder-based VLPMs especially design the\npretraining tasks to involve a decoding process. Those uni-\nfied encoder-decoder VLPMs simply extend the bidirectional\nCMLM to seq2seq CMLM as an auxiliary pretraining task\nfor text generation [Zhou et al., 2020; Li et al., 2021a;\nWang et al., 2020 ]. Other VLPMs with decoupled encoder-\ndecoder emphasize the inherently different peculiarities of the\nunderstanding and generation disciplines, where the former\nentails the unrestricted information passing across modalities\nwhereas the latter only involves visual-to-textual information\npassing. They keep the bi-directional cross-modal model-\ning only to the encoder while generating the text via a sep-\narate decoder in an auto-regressive manner [Xia et al., 2021;\nLi et al., 2021b; Wang et al., 2021b].\nFrom another perspective, applying downstream task such\nas the IC above for pretraining can be regarded as a kind of\ndownstream-driven pretraining task [Tan and Bansal, 2019;\nGao et al., 2020 ]. Some special pretraining tasks especially\ndesigned for the specific downstream task/domain can also be\ndownstream-driven [Wang et al., 2020; Murahari et al., 2020;\nHao et al., 2020; Yang et al., 2021], e.g., the Answer Predic-\ntion (AnP) for the Visual Dialogue(VD) task [Wang et al.,\n2020]. Besides, there are VLPMs that jointly pretrain on ad-\nditional text and/or image collection with the single-modal\ntasks [Su et al., 2019; Wang et al., 2021a; Wen et al., 2021;\nLi et al., 2021a], including the single-modal multi-lingual set-\ntings [Ni et al., 2021; Fei et al., 2021].\n5 Transfer Learning and Evaluation\nIn this section, we will introduce the downstream Visual-\nLinguistic understanding (V-L Understanding), Visual-\nLinguistic generation (V-L Generation) and single modal\ntasks that VLPMs mainly focus on.\nV-L Understanding tasks refer to the tasks that require a\nmodel to capture both visual and linguistic semantics from\nthe input and learn the correspondence between them. Most\nVLPMs apply a variety of V-L Understanding tasks for eval-\nuation, while some just focus on a specific domain. Table 1\nsummarises the downstream tasks in V-L Understanding.\nVisual Question Answering (VQA). Most works [Tan and\nBansal, 2019; Lu et al., 2019; Zhou et al., 2020; Su et al.,\n2019; Gan et al., 2020; Wang et al., 2021a] formalize it as a\nclassification problem to select an answer from a pool of com-\nmon answers. However, SimVLM [Wang et al., 2021b] tries\nto decode open-ended answers in an auto-regressive manner\nto remove the limit posed by the fixed answer vocabulary.\nAmong these models, Self-attention-based VLPMs work bet-\nter than Co-attention-based VLPMs, and SimVLM outper-\nforms all other VLPMs on the VQA v2.0 dataset.\nCross Modal Retrieval (CMR). Some VLPMs [Li et al.,\n2020b; Gao et al., 2020] treat it as a binary classification task\nto predict whether each image-caption pair is matching, while\nothers [Chen et al., 2020; Li et al., 2021b; Wen et al., 2021;\nLi et al., 2020a] solve it as a ranking problem to learn a simi-\nlarity score to be maximized between positive pairs and min-\nimized between negative pairs with a contrastive or CE loss.\nText Classification. Some VLPMs [Tan and Bansal, 2019;\nGan et al., 2020; Lu et al., 2020; Huang et al., 2020;\nChen et al., 2020] work on the Natural Language for Visual\nReasoning (NLVR)task to perform binary classification based\non special token representations, and some works[Gan et al.,\n2020; Lu et al., 2020; Chen et al., 2020; Li et al., 2021a;\nHuang et al., 2021; Kim et al., 2021] focus on the Visual En-\ntailment (VE) problem to evaluate their methods.\nVisual Commonsense Reasoning (VCR). The VCR task\nis usually divided into two sub-tasks to predict an answer\nand a rationale separately, and each sub-task is similar to\nanswering a multiple-choice question out of four options.\nTherefore, most VLPMs [Li et al., 2019; Lu et al., 2019;\nSu et al., 2019 ] formulate each sub-task as a binary classi-\nfication problem for each option, or a 4-way classification\nproblem as done by Unicoder-VL [Li et al., 2020a].\nReferring Expression Comprehension (REC). VLPMs\nfor REC usually perform a binary classification on each im-\nage region for whether it is the target of the phrase and select\nthe region with the highest score at inference stage [Lu et\nal., 2019; Su et al., 2019; Gan et al., 2020; Lu et al., 2020;\nChen et al., 2020; Yu et al., 2021 ]. The VLPMs pretrained\nwith in-domain datasets[Chen et al., 2020; Gan et al., 2020]\ngenerally work better on this task than those pretrained with\nonly out-of-domain datasets.\nVisual Relationship Detection (VRD). Only RVL-BERT\nfocuses on this domain with two formulations [Chiou et al.,\n2021]. It can either be formulated as a ranking problem\nwith the goal of ranking all possible subject-predicate-object\ntriplets between any pair of object regions or as a binary\nclassification problem to predict if a given subject-predicate-\nobject triplet holds.\nVisual Dialogue (VD). This problem can be formulated in\na discriminative manner to select an answer from 100 can-\ndidates as done by VisDialBERT [Murahari et al., 2020 ]\nand VDBERT [Wang et al., 2020 ], or in a generative man-\nner to auto-regressively decode the answer as done by VD-\nBERT [Wang et al., 2020].\nVisual Linguistic Navigation (VLN). Some VLPMs [Ma-\njumdar et al., 2020 ] try to solve this task in a discrimina-\ntive setting by selecting the correct path among one positive\npath and several negative sample paths. In contrast, other\nVLPMs [Hong et al., 2021] try to solve it in a generative set-\nting to make an action in between each state of the path by\napplying a reinforcement learning objective together with an\nimitation learning objective.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSurvey Track\n5534\nTasks Papers\nVQA [Li et al., 2019 ], [Tan and Bansal, 2019 ], [Lu et al., 2019 ], [Zhou et al., 2020 ], [Su et al., 2019 ], [Gan et al., 2020 ], [Lu\net al., 2020 ], [Huang et al., 2020 ], [Chen et al., 2020 ], [Li et al., 2020b ], [Yu et al., 2021 ], [Li et al., 2021b ], [Zhang et al.,\n2021], [Huang et al., 2021 ], [Zhuge et al., 2021 ], [Wang et al., 2021b ], [Li et al., 2021a ], [Wang et al., 2021a ], [Kim et al.,\n2021], [Singh et al., 2021]\nCMR [Lu et al., 2019], [Li et al., 2020a], [Gan et al., 2020], [Lu et al., 2020], [Huang et al., 2020], [Gao et al., 2020], [Chen et\nal., 2020], [Li et al., 2020b], [Yu et al., 2021], [Li et al., 2021b], [Zhang et al., 2021], [Ni et al., 2021], [Sun et al., 2021],\n[Huang et al., 2021], [Li et al., 2021a], [Wen et al., 2021], [Fei et al., 2021], [Jia et al., 2021], [Wang et al., 2021a], [Kim et\nal., 2021], [Singh et al., 2021]\nNLVR [Li et al., 2019], [Tan and Bansal, 2019], [Gan et al., 2020], [Lu et al., 2020], [Huang et al., 2020], [Chen et al., 2020], [Li et\nal., 2020b], [Zhang et al., 2021], [Huang et al., 2021], [Wang et al., 2021b], [Wang et al., 2021a], [Kim et al., 2021]\nVE [Gan et al., 2020], [Lu et al., 2020], [Chen et al., 2020], [Li et al., 2021a], [Huang et al., 2021], [Wang et al., 2021b], [Kim\net al., 2021], [Singh et al., 2021]\nVCR [Li et al., 2019], [Lu et al., 2019], [Li et al., 2020a], [Su et al., 2019], [Gan et al., 2020], [Chen et al., 2020], [Yu et al.,\n2021], [Li et al., 2021b]\nREC [Lu et al., 2019], [Su et al., 2019], [Gan et al., 2020], [Lu et al., 2020], [Chen et al., 2020], [Yu et al., 2021]\nVRD [Chiou et al., 2021]\nVD [Wang et al., 2020], [Murahari et al., 2020]\nVLN [Majumdar et al., 2020], [Hao et al., 2020], [Hong et al., 2021]\nTable 1: Categories of Downstream Tasks in Visual Language Understanding\nV-L Generation tasks generate texts/images when the\nother modality is included in the input. Most generative\nVLPMs [Zhou et al., 2020; Xia et al., 2021; Li et al., 2020b;\nZhang et al., 2021; Li et al., 2021a; Li et al., 2021b;\nWanget al., 2021b; Huet al., 2021] focus on Image Caption-\ning (IC) task on COCO Captioning dataset by minimizing the\nNLL loss. Some of them [Li et al., 2020b; Zhanget al., 2021;\nWang et al., 2021b; Hu et al., 2021 ] try the Nocaps task,\nwhich generates captions for images containing open domain\nnovel objects. In addition, KaleidoBERT[Zhuge et al., 2021]\nextends this task to the fashion domain, and TAP [Yang et\nal., 2021] focuses on the TextCaps task (and the Text VQA\ntask), which requires models to understand scene texts by ap-\nplying CE loss at multiple decoding steps. VLPMs pretrained\nwith extra in-domain datasets generally perform better except\non the Nocaps. Similarly, multi-lingual VLPMs [Wang et\nal., 2021b] focus on the Multi-modal Machine Translation\n(MMT) task by minimizing the NLL loss.\nDespite focusing on multi-modal pretraining, some\nVLPMs also investigate how their models can generalize to\nsingle modal tasks. SimVLM [Wang et al., 2021b ] and\nALIGN [Jia et al., 2021] perform the image classification task\nof predicting object categories, and KaleidoBERT [Zhuge et\nal., 2021] does it similarly to predict the category and subcat-\negory of commercial products. Some VLPMs [Wang et al.,\n2021b; Li et al., 2021a ] try to evaluate the natural language\nunderstanding capability of their models based on the GLUE\nbenchmark tasks (or its subset). An improvement in linguis-\ntic tasks has been observed when pretrained with in-domain\ndatasets, even with smaller pretraining data sizes.\n6 Conclusion and Future Research Directions\nThis paper presents an overview of the recent advances in\nVLPMs for producing joint representations of visions and\nlanguages, images and text pairs. We mainly summarise vi-\nsion and language input encoding methods and mainstream\nVLPMs. We also discuss several useful pretraining and fine-\ntuning tasks and strategies. We do hope that the paper will\nprovide insightful guidance for both CV and NLP researchers\nwho work on joint and cross-modal learning. To advance this,\nthere are several promising future directions for VLPMs.\nV-L Interaction Modeling. Although various VL interaction\nmodel extensions have been proposed in Section 3, there is\nstill a significant challenge in aligning vision and language\ncontent. Most pretrained models focus on masking at the task\nlevel or input level, which does not directly align the features\nbetween image and text. Incorporating masking strategy at\nthe embedding level has been shown to be effective [Zhuge\net al., 2021]. It is promising to investigate how to explicitly\nalign the embedding features between image and text so that\nit can learn fine-grain representations.\nVLPM Pretraining Strategy. There still lacks systematic\nexperiments and analysis on V-L-based multi-tasking syn-\nergy for VLPM pretraining. Few VLPMs explore multi-stage\ntraining [Li et al., 2019; Wanget al., 2020; Hong et al., 2021;\nXia et al., 2021; Chen et al., 2020 ], and 12-in-1 [Lu et al.,\n2020] is the only one that tries to group the tasks of sim-\nilar natures and test the performance boost from intra- and\ninter-group pretraining perspectives. It faces significant chal-\nlenges to finding the answer in one step because it entails mul-\ntiple factors such as selecting datasets (both multi-modal and\nsingle-modal), task design, task grouping, and order (multi-\nstage). Moreover, the effectiveness of the pretraining process\nmay vary for different downstream targeted tasks. However,\nit is still worth exploring, step by step, how the V-L based\nmulti-tasking can be implemented for VLPM pretraining that\ncan generate the best transfer performance on the specific tar-\ngeted domain/tasks, which will provide promising and valu-\nable guidelines for the future development of VLPMs.\nTraining Evaluation. The trained VLPMs can be only eval-\nuated during the downstream tasks. This may waste a lot of\ncomputation cost when the models are defective. It is worth\nexploring some metrics, such as perplexity, during the train-\ning procedure. Hence, we can guarantee the performance of\nthe trained VLPMs in advance.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSurvey Track\n5535\nReferences\n[Bugliarello et al., 2021] Emanuele Bugliarello, Ryan Cot-\nterell, Naoaki Okazaki, and Desmond Elliott. Multi-\nmodal pretraining unmasked: A meta-analysis and a uni-\nfied framework of vision-and-language berts.Transactions\nof the Association for Computational Linguistics, 9:978–\n994, 2021.\n[Chen et al., 2020] Yen-Chun Chen, Linjie Li, Licheng Yu,\nAhmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng,\nand Jingjing Liu. Uniter: Universal image-text representa-\ntion learning. In European conference on computer vision,\npages 104–120. Springer, 2020.\n[Chiou et al., 2021] Meng-Jiun Chiou, Roger Zimmermann,\nand Jiashi Feng. Visual relationship detection with visual-\nlinguistic knowledge from multimodal representations.\nIEEE Access, 9:50441–50451, 2021.\n[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 4171–4186, 2019.\n[Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition\nat scale. In International Conference on Learning Repre-\nsentations, 2020.\n[Fei et al., 2021] Hongliang Fei, Tan Yu, and Ping Li. Cross-\nlingual cross-modal pretraining for multimodal retrieval.\nIn Proceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational Lin-\nguistics: Human Language Technologies, 2021.\n[Gan et al., 2020] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen\nZhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial\ntraining for vision-and-language representation learning.\nIn NeurIPS, 2020.\n[Gao et al., 2020] Dehong Gao, Linbo Jin, Ben Chen,\nMinghui Qiu, Peng Li, Yi Wei, Yi Hu, and Hao Wang.\nFashionbert: Text and image matching with adaptive loss\nfor cross-modal retrieval. InProceedings of the 43rd Inter-\nnational ACM SIGIR Conference on Research and Devel-\nopment in Information Retrieval, pages 2251–2260, 2020.\n[Hao et al., 2020] Weituo Hao, Chunyuan Li, Xiujun Li,\nLawrence Carin, and Jianfeng Gao. Towards learning a\ngeneric agent for vision-and-language navigation via pre-\ntraining. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 13137–\n13146, 2020.\n[Hong et al., 2021] Yicong Hong, Qi Wu, Yuankai Qi, Cris-\ntian Rodriguez-Opazo, and Stephen Gould. Vln bert: A\nrecurrent vision-and-language bert for navigation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 1643–1653, 2021.\n[Hu et al., 2021] Xiaowei Hu, Xi Yin, Kevin Lin, Lei Zhang,\nJianfeng Gao, Lijuan Wang, and Zicheng Liu. Vivo: Vi-\nsual vocabulary pre-training for novel object captioning.\nProceedings of the AAAI Conference on Artificial Intelli-\ngence, 35(2):1575–1583, May 2021.\n[Huang et al., 2020] Zhicheng Huang, Zhaoyang Zeng, Bei\nLiu, Dongmei Fu, and Jianlong Fu. Pixel-bert: Aligning\nimage pixels with text by deep multi-modal transformers.\narXiv preprint arXiv:2004.00849, 2020.\n[Huang et al., 2021] Zhicheng Huang, Zhaoyang Zeng, Yu-\npan Huang, Bei Liu, Dongmei Fu, and Jianlong Fu. See-\ning out of the box: End-to-end pre-training for vision-\nlanguage representation learning. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 12976–12985, 2021.\n[Jia et al., 2021] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting\nChen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan\nSung, Zhen Li, and Tom Duerig. Scaling up visual and\nvision-language representation learning with noisy text su-\npervision. arXiv preprint arXiv:2102.05918, 2021.\n[Kim et al., 2021] Wonjae Kim, Bokyung Son, and Ildoo\nKim. Vilt: Vision-and-language transformer with-\nout convolution or region supervision. arXiv preprint\narXiv:2102.03334, 2021.\n[Li et al., 2019] Liunian Harold Li, Mark Yatskar, Da Yin,\nCho-Jui Hsieh, and Kai-Wei Chang. Visualbert: Asimple\nand performant baseline for vision and language. arXiv\npreprint arXiv:1908.03557, 2019.\n[Li et al., 2020a] Gen Li, Nan Duan, Yuejian Fang, Ming\nGong, and Daxin Jiang. Unicoder-vl: A universal encoder\nfor vision and language by cross-modal pre-training. In\nProceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 34, pages 11336–11344, 2020.\n[Li et al., 2020b] Xiujun Li, Xi Yin, Chunyuan Li,\nPengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan\nWang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar:\nObject-semantics aligned pre-training for vision-language\ntasks. In European Conference on Computer Vision, pages\n121–137. Springer, 2020.\n[Li et al., 2021a] Wei Li, Can Gao, Guocheng Niu, Xinyan\nXiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang.\nUnimo: Towards unified-modal understanding and gener-\nation via cross-modal contrastive learning. In Proceedings\nof the 59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint Con-\nference on Natural Language Processing (Volume 1: Long\nPapers), pages 2592–2607, 2021.\n[Li et al., 2021b] Yehao Li, Yingwei Pan, Ting Yao, Jing-\nwen Chen, and Tao Mei. Scheduled sampling in vision-\nlanguage pretraining with decoupled encoder-decoder net-\nwork. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 35, pages 8518–8526, 2021.\n[Lu et al., 2019] Jiasen Lu, Dhruv Batra, Devi Parikh, and\nStefan Lee. Vilbert: Pretraining task-agnostic visiolin-\nguistic representations for vision-and-language tasks. Ad-\nvances in NeurIPS, 32:13–23, 2019.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSurvey Track\n5536\n[Lu et al., 2020] Jiasen Lu, Vedanuj Goswami, Marcus\nRohrbach, Devi Parikh, and Stefan Lee. 12-in-1: Multi-\ntask vision and language representation learning. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 10437–10446, 2020.\n[Majumdar et al., 2020] Arjun Majumdar, Ayush Shrivas-\ntava, Stefan Lee, Peter Anderson, Devi Parikh, and Dhruv\nBatra. Improving vision-and-language navigation with\nimage-text pairs from the web. In European Conference\non Computer Vision, pages 259–274. Springer, 2020.\n[Mogadala et al., 2021] Aditya Mogadala, Marimuthu\nKalimuthu, and Dietrich Klakow. Trends in integration of\nvision and language research: A survey of tasks, datasets,\nand methods. Journal of Artificial Intelligence Research,\n71:1183–1317, 2021.\n[Murahari et al., 2020] Vishvak Murahari, Dhruv Batra,\nDevi Parikh, and Abhishek Das. Large-scale pretraining\nfor visual dialog: A simple state-of-the-art baseline. InEu-\nropean Conference on Computer Vision, pages 336–352.\nSpringer, 2020.\n[Ni et al., 2021] Minheng Ni, Haoyang Huang, Lin Su, Ed-\nward Cui, Taroon Bharti, Lijuan Wang, Dongdong Zhang,\nand Nan Duan. M3p: Learning universal representations\nvia multitask multilingual multimodal pre-training. InPro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 3977–3986, 2021.\n[Radford et al., 2021] Alec Radford, Jong Wook Kim, Chris\nHallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin,\nJack Clark, et al. Learning transferable visual mod-\nels from natural language supervision. arXiv preprint\narXiv:2103.00020, 2021.\n[Singh et al., 2021] Amanpreet Singh, Ronghang Hu,\nVedanuj Goswami, Guillaume Couairon, Wojciech\nGaluba, Marcus Rohrbach, and Douwe Kiela. Flava: A\nfoundational language and vision alignment model. arXiv\npreprint arXiv:2112.04482, 2021.\n[Su et al., 2019] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li,\nLewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training\nof generic visual-linguistic representations. In Interna-\ntional Conference on Learning Representations, 2019.\n[Sun et al., 2021] Siqi Sun, Yen-Chun Chen, Linjie Li,\nShuohang Wang, Yuwei Fang, and Jingjing Liu. Light-\nningdot: Pre-training visual-semantic embeddings for\nreal-time image-text retrieval. In Proceedings of the 2021\nConference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Language\nTechnologies, pages 982–997, 2021.\n[Tan and Bansal, 2019] Hao Tan and Mohit Bansal. Lxmert:\nLearning cross-modality encoder representations from\ntransformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 5100–5111,\n2019.\n[Wang et al., 2020] Yue Wang, Shafiq Joty, Michael Lyu, Ir-\nwin King, Caiming Xiong, and Steven CH Hoi. Vd-bert:\nA unified vision and dialog transformer with bert. In Pro-\nceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages 3325–\n3338, 2020.\n[Wang et al., 2021a] Wenhui Wang, Hangbo Bao, Li Dong,\nand Furu Wei. Vlmo: Unified vision-language pre-\ntraining with mixture-of-modality-experts. arXiv preprint\narXiv:2111.02358, 2021.\n[Wang et al., 2021b] Zirui Wang, Jiahui Yu, Adams Wei Yu,\nZihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Sim-\nple visual language model pretraining with weak supervi-\nsion. arXiv preprint arXiv:2108.10904, 2021.\n[Wen et al., 2021] Keyu Wen, Jin Xia, Yuanyuan Huang,\nLinyang Li, Jiayan Xu, and Jie Shao. Cookie: Con-\ntrastive cross-modal knowledge sharing pre-training for\nvision-language representation. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 2208–2217, 2021.\n[Xia et al., 2021] Qiaolin Xia, Haoyang Huang, Nan Duan,\nDongdong Zhang, Lei Ji, Zhifang Sui, Edward Cui, Taroon\nBharti, and Ming Zhou. Xgpt: Cross-modal generative\npre-training for image captioning. In CCF International\nConference on Natural Language Processing and Chinese\nComputing, pages 786–797. Springer, 2021.\n[Yang et al., 2021] Zhengyuan Yang, Yijuan Lu, Jianfeng\nWang, Xi Yin, Dinei Florencio, Lijuan Wang, Cha Zhang,\nLei Zhang, and Jiebo Luo. Tap: Text-aware pre-training\nfor text-vqa and text-caption. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8751–8761, 2021.\n[Yu et al., 2021] Fei Yu, Jiji Tang, Weichong Yin, Yu Sun,\nHao Tian, Hua Wu, and Haifeng Wang. Ernie-vil: Knowl-\nedge enhanced vision-language representations through\nscene graphs. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 35, pages 3208–3216, 2021.\n[Zhang et al., 2021] Pengchuan Zhang, Xiujun Li, Xiaowei\nHu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi,\nand Jianfeng Gao. Vinvl: Revisiting visual representa-\ntions in vision-language models. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5579–5588, 2021.\n[Zhou et al., 2020] Luowei Zhou, Hamid Palangi, Lei\nZhang, Houdong Hu, Jason Corso, and Jianfeng Gao. Uni-\nfied vision-language pre-training for image captioning and\nvqa. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 34, pages 13041–13049, 2020.\n[Zhuge et al., 2021] Mingchen Zhuge, Dehong Gao, Deng-\nPing Fan, Linbo Jin, Ben Chen, Haoming Zhou, Minghui\nQiu, and Ling Shao. Kaleido-bert: Vision-language\npre-training on fashion domain. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 12647–12657, 2021.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\nSurvey Track\n5537",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8059587478637695
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6133720278739929
    },
    {
      "name": "Transformer",
      "score": 0.6127100586891174
    },
    {
      "name": "Natural language processing",
      "score": 0.5878169536590576
    },
    {
      "name": "Architecture",
      "score": 0.5018312931060791
    },
    {
      "name": "Question answering",
      "score": 0.47702404856681824
    },
    {
      "name": "Natural language",
      "score": 0.4630378186702728
    },
    {
      "name": "Language understanding",
      "score": 0.45274385809898376
    },
    {
      "name": "Encoding (memory)",
      "score": 0.4513210654258728
    },
    {
      "name": "Mainstream",
      "score": 0.43365025520324707
    },
    {
      "name": "Task (project management)",
      "score": 0.4310646653175354
    },
    {
      "name": "Language acquisition",
      "score": 0.4191567897796631
    },
    {
      "name": "Joint (building)",
      "score": 0.4116944670677185
    },
    {
      "name": "Linguistics",
      "score": 0.1823689341545105
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Theology",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I129604602",
      "name": "The University of Sydney",
      "country": "AU"
    }
  ]
}