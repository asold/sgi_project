{
  "title": "GenQREnsemble: Zero-Shot LLM Ensemble Prompting for Generative Query Reformulation",
  "url": "https://openalex.org/W4393073563",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5115902564",
      "name": "Kaustubh D. Dhole",
      "affiliations": [
        "Emory University"
      ]
    },
    {
      "id": "https://openalex.org/A2283615530",
      "name": "Eugene Agichtein",
      "affiliations": [
        "Emory University"
      ]
    },
    {
      "id": "https://openalex.org/A5115902564",
      "name": "Kaustubh D. Dhole",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2283615530",
      "name": "Eugene Agichtein",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1993692165",
    "https://openalex.org/W4315481736",
    "https://openalex.org/W4309698332",
    "https://openalex.org/W3210968241",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3209981429",
    "https://openalex.org/W6607643177",
    "https://openalex.org/W4200635941",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3133594109",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4296713955",
    "https://openalex.org/W2558203065",
    "https://openalex.org/W4385569686",
    "https://openalex.org/W4402670429",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4385569968",
    "https://openalex.org/W4303648559",
    "https://openalex.org/W2740492458",
    "https://openalex.org/W2032039936",
    "https://openalex.org/W3085914694",
    "https://openalex.org/W3128581554",
    "https://openalex.org/W2105157020",
    "https://openalex.org/W2117473841",
    "https://openalex.org/W1990244040",
    "https://openalex.org/W2069870183",
    "https://openalex.org/W4385565351",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W3172286436",
    "https://openalex.org/W2042024605",
    "https://openalex.org/W3100107515",
    "https://openalex.org/W4384652592",
    "https://openalex.org/W3156166379"
  ],
  "abstract": null,
  "full_text": "GenQREnsemble: Zero-Shot LLM Ensemble Prompting for\nGenerative Query Reformulation\nKaustubh D. Dhole, Eugene Agichtein\nDepartment of Computer Science\nEmory University\nAtlanta, USA\n{kaustubh.dhole, eugene.agichtein}@emory.edu\nAbstract. Query Reformulation(QR) is a set of techniques used to transform a user’s orig-\ninal search query to a text that better aligns with the user’s intent and improves their search\nexperience. Recently, zero-shot QR has been shown to be a promising approach due to\nits ability to exploit knowledge inherent in large language models. By taking inspiration\nfrom the success of ensemble prompting strategies which have benefited many tasks, we\ninvestigate if they can help improve query reformulation. In this context, we propose an\nensemble based prompting technique, GenQREnsemble which leverages paraphrases of a\nzero-shot instruction to generate multiple sets of keywords ultimately improving retrieval\nperformance. We further introduce its post-retrieval variant, GenQREnsembleRF to incor-\nporate pseudo relevant feedback. On evaluations over four IR benchmarks, we find that\nGenQREnsemble generates better reformulations with relative nDCG@10 improvements\nup to 18% and MAP improvements upto 24% over the previous zero-shot state-of-art. On\nthe MSMarco Passage Ranking task, GenQREnsembleRF shows relative gains of 5% MRR\nusing pseudo-relevance feedback, and 9% nDCG@10 using relevant feedback documents.\nKeywords: Query Reformulation · Zero-Shot · Prompting · Relevance Feedback\n1 Introduction\nUsers searching for relevant documents might not always be able to accurately express their\ninformation needs in their initial queries. This could result in queries being vague or ambiguous\nor lacking the necessary domain vocabulary. Query Reformulation (QR) is a set of techniques\nused to transform a user’s original search query to a text that better aligns with the user’s intent\nand improves their search experience. Such reformulation alleviates the vocabulary mismatch\nproblem by expanding the query with related terms or paraphrasing it into a suitable form by\nincorporating additional context.\nRecently, with the success of large language models (LLMs) [5,23], a plethora of QR ap-\nproaches have been developed. The generative capabilities of LLMs have been exploited to\nproduce novel queries [22], as well as useful keywords to be appended to the users’ original\nqueries [29]. By gaining exposure to enormous amounts of text during pre-training, prompting\nhas become a promising avenue for utilizing knowledge inherent in an LLM for the benefit of\nsubsequent downstream tasks [27] especially QR [14,32].\nUnlike training or few-shot learning, zero-shot prompting does not rely on any labeled ex-\namples. The advantage of a zero-shot approach is the ease with which a standalone generative\narXiv:2404.03746v1  [cs.IR]  4 Apr 2024\n2 Kaustubh D. Dhole, Eugene Agichtein\nmodel can be used to reformulate queries by prompting a templated piece of instruction along\nwith the original query. Particularly, zero-shot QR can be used to generate keywords by prompt-\ning the user’s original query along with an instruction that defines the task of query reformulation\nin natural language like Generate useful search terms for the given query:‘List\nall the breweries in Austin’.\nInstruction Expansions GeneratedIncrease the search efficacy by offering age goldfish grow outsmart outlivebeneficial expansion keywords for the query ageing species...Enhance search outcomes by recommending beneficial Goldfish breed sizes What kind ofexpansion terms to supplement the query goldfish grows the fastest...\nFig. 1. Keywords generated for the query (“do\ngoldfish grow”) differ drastically when gener-\nated from two paraphrastic instructions prompted\nto flan-t5-xxl [7].\nHowever, such a zero-shot prompting ap-\nproach is still contingent on the exact instruc-\ntion appearing in the prompt providing plenty\nof avenues of improvement. While LLMs\nhave been known to vary significantly in per-\nformance across different prompts [36,8] and\ngeneration settings [33], many natural lan-\nguage tasks have benefited by exploiting such\nvariation via ensembling multiple prompts or\ngenerating diverse reasoning paths [16,3,31].\nWhether such improvements also transfer to\ntasks like QR is yet to be determined. In Fig-\nure 1, a vast difference is noticed in the keywords generated when the input instruction is altered\nto a semantically similar variant. We hypothesize that QR might naturally benefit from such vari-\nation – An ensemble of zero-shot reformulators with paraphrastic instructions can be tasked to\nlook at the input query in diverse ways so as to elicit different expansions. This work proposes\nthe following contributions:\n– We propose a novel method, GenQREnsemble – a zero-shot Ensemble based Generative\nQuery Reformulator which exploits multiple zero-shot instructions for QR to generate a\nmore effective query reformulation than possible with an individual instruction. (Section 3)\n– We further introduce an extensionGenQREnsembleRF to incorporate Relevance Feedback\ninto the process. (Section 3)\n– We evaluate the proposed methods over four standard IR benchmarks, demonstrating signif-\nicant relative improvements vs. recent state of the art, of up to 18% on nDCG@10 in pre-\nretrieval settings, and of up to 9% nDCG@10 on post-retrieval (feedback) settings, demon-\nstrating increased generalizability of our approach.\nNext, we summarize the prior work to place our contributions in context.\n2 Related Work\nQuery reformulation has been shown to be effective in many settings [6]. It can be done pre-\nretrieval, or post-retrieval, via incorporating evidence from feedback, obtained either from a user\nor from top-ranked results in the sparse retrieval setting [15], and in the dense retrieval set-\nting [30,35].\nRecently, zero-shot approaches to query reformulation have received considerable attention.\nWang et al. [29] design a query reformulator by fine-tuning a sequence-to-sequence transformer,\nT5 [25] on pairs of raw and transformed queries. Their zero-shot prompting approach uses an\ninstruction-tuned model, FlanT5 [7] to generate keywords for query expansion and incorporating\nPRF. Jagerman et al. [14] demonstrate LLMs can be more powerful than traditional methods\nGenQREnsemble and GenQREnsembleRF 3\nfor query expansion. Mo et al. [19] propose a framework to reformulate conversational search\nqueries using LLMs. Gao et al. [11]’s framework performs retrieval through fake documents\ngenerated by prompting LLMs with user queries. Alaofi et al. [2] prompt LLMs with information\ndescriptions to generate query variants.\nHowever, using a single query reformulation can sometimes degrade performance compared\nto the original query. To address this drawback, prior efforts have incorporated ensemble strate-\ngies via keywords from numerous sources or fusing documents from different queries. Gao\net al. [10], combine features derived from various translation models to generate better query\nrewrites. Si et al. [26] perform QR by utilizing multiple external biomedical resources. Hsu and\nTaksa [13] present a data fusion framework suggesting that diverse query formulations repre-\nsent distinct evidence sources for inferring document relevance. Later, Mohankumar et al. [20]\ngenerated diverse queries by introducing a diversity-driven RL algorithm. For other tasks, recent\nworks demonstrated the benefits of ensemble strategies for prompting LLMs, including self-\nconsistency [31] for arithmetic and common sense tasks, Chain of Verification [9] for improving\nfactuality, and Diverse [16] for question answering. However, zero-shot based ensemble methods\nfor LLM have not been explored for the Query Reformulation task, as we propose in this paper.\n3 Proposed Approach: GenQREnsemble\nIn this section, we describe two variations of our proposed approach, for the pre- and post-\nretrieval settings. In the pre-retrieval setting, a Query Reformulation R transforms a user’s ex-\npressed query q0 into a novel reformulated version qr to improve retrieval effectiveness for a\ngiven search task (e.g., passage or document retrieval). We also consider the post-retrieval set-\nting, wherein the reformulator can incorporate additional contextual information like document\nor passage-level feedback.\nFig. 2.The complete flow and algorithm shown on the top right.\nPre-retrieval: We propose GenQREnsemble – an ensemble prompting based query refor-\nmulator which uses N diverse paraphrases of a QR instruction to enhance retrieval. Specifically,\n4 Kaustubh D. Dhole, Eugene Agichtein\nwe first use an LLM to paraphrase the instruction I1 to create N instructions with different sur-\nface forms viz. I1 to IN. This is required to be done once. Each instruction is then prompted along\nwith the user’s query q0 to generate instruction-specific keywords. All the keywords are then\nappended to the original query, resulting in a reformulated query, which is then executed against\na document index D to retrieve relevant documents Dr. The complete process and algorithm are\nshown in Figure 2.\nPost-retrieval: To assess how well our method can incorporate additional context like doc-\nument feedback, we introduce GenQREnsembleRF. Here, we prepend the N instructions de-\nscribed earlier with a fixed context capturing string “Based on the given context information\n{C},” used1 in [29] to create their PRF counterparts – where C is a space (‘ ’) delimited concate-\nnation of feedback documents C = d1 + . . .+ dm, obtained either as pseudo-relevance feedback\nfrom initial retrieval or manually chosen by the user.\n4 Experiments\n# Instruction1Improve the search effectiveness by suggesting expansion terms for the query2Recommend expansion terms for the query to improve search results3Improve the search effectiveness by suggesting useful expansion terms for the query4Maximize search utility by suggesting relevant expansion phrases for the query5Enhance search efficiency by proposing valuable terms to expand the query6Elevate search performance by recommending relevant expansion phrases for the query7Boost the search accuracy by providing helpful expansion terms to enrich the query8Increase the search efficacy by offering beneficial expansion keywords for the query9Optimize search results by suggesting meaningful expansion terms to enhance the query10Enhance search outcomes by recommending beneficial expansion terms to supplement the query\nFig. 3.Reformulation instructions generated (N=10).\nWe now describe the experiments and analy-\nsis performed for different retrieval settings.\nTo instruct the LLM to generate query\nreformulations, we start with the instruc-\ntion empirically chosen by Wang et al. [29]\n– as our base QR instruction I1. We use\nthis instruction to generate N paraphrases\nof the instruction ( N = 10). To this aim,\nwe invoke GPT-3.5 API with the paraphrase\ngenerating prompt, namely, Ip=Generate\n10 paraphrases for the following instruction:–\nand the base QR instruction I1 to obtain I2 to I10. These paraphrases serve as our instruction set\nfor subsequent experiments.\nFor generating the actual query reformulations, we employflan-t5-xxl [7], an instruction-\ntuned model. The FlanT5 set of models is created by fine-tuning the text-to-text transformer\nmodel, T5 [25] on instruction data of a variety of NL tasks. We use the checkpoint 2 provided\nthrough HuggingFace’s Transformers library [34]. Nucleus sampling is performed with a cutoff\nprobability of 0.92 keeping the top 200 tokens (top_k) and a repetition penalty of 1.2.\nFor evaluation, we use four popular benchmarks through IRDataset [17]’s interface: 1)TP19:\nTREC 19 Passage Ranking which uses the MSMarco dataset [21,14] consisting of search engine\nqueries. 2)TR04: TREC Robust 2004 Track, a task intended for testing poorly performing topics.\nIn our experiments, we use the Title as our choice of query. And two tasks from the BEIR [28]\nbenchmark 3)WT: Webis Touche [4] for argument retrieval 4)DE: DBPedia Entity Retrieval [12].\n1 We found prepending the string in the prompt performs better than appending it at the end\n2 https://huggingface.co/google/flan-t5-xxl\nGenQREnsemble and GenQREnsembleRF 5\n4.1 Baselines:\nWe compare our work against the following using the Pyterrier [18] framework. For all the post-\nretrieval experiments, we use 5 documents as feedback.\nWith BM25 Retriever:\n– BM25: Here, we retrieve using raw queries without any reformulation\n– FlanQR [29]: We implement Wang et al’s single-instruction zero-shot QR [29] which is also\na specific case of GenQREnsemble when N=1\n– BM25+RM3 [1]: BM25 retrieval with RM3 expanded queries (#feedback terms=10)\n– BM25+FlanPRF [29]: BM25 retrieval with FlanPRF expanded queries\nWith Neural Reranking: Here, we re-evaluate the above settings in conjunction with a MonoT5\nneural reranker [24] with all other parameters constant.\n– BM25+MonoT5: BM25 retrieval using raw queries, re-ranked with MonoT5 model [24]\n– FlanQR+MonoT5: BM25 retrieval with FlanQR reformulations, re-ranked with MonoT5\nmodel\n– BM25+RM3+MonoT5: BM25 retrieval with RM3 expanded queries, re-ranked with MonoT5\nmodel\n– BM25+FlanPRF+MonoT5: BM25 retrieval with FlanPRF expanded queries, re-ranked with\nMonoT5 model\n5 Results and Analysis\nWe now report the results of query reformulation for pre- and post-retrieval settings.\n5.1 Pre-Retrieval Performance\n-0.75\n-0.50\n-0.25\n0.00\n0.25\n0.50\n0.75\n                                           \nGenQR GenEnsembleQR\nFig. 4.nDCG@10 Scores of GenQREnsemble and\nFlanQR relative to BM25\nWe first compare the retrieval performances\nof raw queries and reformulations from\nFlanQR, and GenQREnsemble in Table 1.Gen-\nQREnsemble outperforms the raw queries as\nwell as generates better reformulations than\nFlanQR’s reformulated queries across all the\nfour benchmarks over a BM25 retriever, in-\ndicating the usefulness of paraphrasing initial\ninstructions. On TP19, nDCG@10 and MAP\nimprove significantly with relative improve-\nments of 18% and 24% respectively. This is\nfurther validated through the querywise analysis shown in Figure 4 – Relative to BM25,\nnDCG@10 scores of GenQREnsemble (shown in green) are overall better than FlanQR (shown\nin blue).GenQREnsemble seems more robust too as it avoids drastic degradation in at least 6\nqueries on which FlanQR fails.\nWe further look at GenQREnsemble under the neural reranker setting shown at the bottom\nhalf of Table 1. In three of the four settings, viz., TP19, WT, and DE, GenQREnsemble is prefer-\nable to its zero-shot variant, FlanQR. Evidently, the gains of both the zero-shot approaches in the\n6 Kaustubh D. Dhole, Eugene Agichtein\nTable 1.Performance of GenQREnsemble on the four benchmarks. α denotes significant improvements\n(paired t-test with Holm-Bonferroni correction, p < 0.05) over FlanQR. +% indicates % improvements\nrelative to FlanQR (as whole numbers).\nEvaluation Set TREC Passage 19 TREC Robust 04 Webis Touche DBpedia Entity\nSetting nDCG@10 MAP MRRP@10 nDCG@10 MRRnDCG@10 MAP MRRnDCG@10 MAP MRRBM25 .480 .286 .642 .426 .434 .154 .260 .206 .454 .321 .168 .297FlanQR .477 .302 .593 .473 .483 .151 .315 .241 .511 .342 .196 .345FlanQRβ=.05 .511 .323 .621 .469 .477 .150 .276 .221 .476 .353 .188 .339\nGenQREnsemble .564α+18%.375α+24%.706+19%.500α+6%.513α+6%.159+6%.317+1%.257+6%.555+9%.374α+9%.212α+8%.376α+9%GenQREnsembleβ=.05 .575α .377α .714 .502α .512α .159 .292 .242 .489 .377α .212α .380α\nBM25+MonoT5 .718 .477 .881 .492 .513 .173 .299 .216 .525 .414 .249 .444FlanQR+MonoT5 .707 .486 .847 .490 .510 .170 .292 .215 .530 .415 .255 .446GenQREnsemble+MonoT5.722+2% .503+3% .862+2%.484-1% .506-1% .170.298+3%.219+2%.548+3%.420+1%.258+1%.450+1%\ntraditional setting are stronger vis-à-vis the neural setting. We hypothesize this could be due to\nGenQREnsemble and FlanQR both expanding the query via incorporating semantically similar\nbut lexically different keywords. Comparatively, neural models are adept at capturing notions of\nsemantic similarity and might benefit less with query expansion. This also is in line with Weller\net al.’s [32] recent analysis on the non-ensemble variant.\n5.2 Post-Retrieval Performance\nTable 2.Comparison of PRF performance on the TREC 19 Passage Ranking Task\nWith BM25 Retriever With Neural Reranking\nSetting nDCG@10 nDCG@20 MAP MRR nDCG@10 nDCG@20 MAP MRR\nBM25 .480 .473 .286 .642 .718 .696 .477 .881\nRM3 .504 .496 .311 .595 .716 .699 .480 .858\nFlanPRF .576 .553 .363 .715 .722 .703 .486 .874\nGenQREnsembleRF .585+2% .560+1% .373+3%.753+5%.729+1% .706+1% .501+3%.894+2%\nFlanPRF (Oracle) .753 .728 .501 .936 .742 .734 .545 .881\nGenQREnsembleRF (Oracle).820α+9% .773+6% .545+9%.977+4%.756+2% .751+2% .545 .897 +2%\nWe now investigate if GenQREnsembleRF can effectively incorporate PRF in Table 2. We\nfind that GenQREnsembleRF improves retrieval performance as compared to other PRF ap-\nproaches and is able to incorporate feedback from a BM25 retriever better than RM3 as well as its\nzero-shot counterpart. To assess if GenQREnsembleRF and FlanPRF can at all benefit from in-\ncorporating relevant documents, we perform oracle testing by providing the highest relevant gold\ndocuments as context. We find that GenQREnsembleRF is able to improve over GenQREnsem-\nble (without feedback) showing that it is able to capture context effectively as well as benefit\nfrom it. Further, it can incorporate relevant feedback better than its single-instruction counterpart\nFlanPRF. We notice improvements even under the neural reranker setting as GenQREnsembleRF\noutperforms RM3 and FlanPRF. Besides, the oracle improvements are higher with only a BM25\nretriever as compared to when a neural reranker is introduced.\nWe further evaluate the effect of varying the number of feedback documents from 0 to 5. We\nnotice that resorting to an ensemble approach is highly beneficial. In the BM25 setting, the en-\nsemble approach seems always preferable. Under the neural reranker setting too,GenQREnsembleRF\nalmost always outperforms FlanPRF.\nGenQREnsemble and GenQREnsembleRF 7\n# Feedback Documents\nnDCG@10\n0.450\n0.500\n0.550\n0.600\n0.650\n0 1 2 3 4 5\nBM25 RM3 FlanPRF GenEnsemblePRF\n# Feedback Documents\nnDCG@10\n0.705\n0.710\n0.715\n0.720\n0.725\n0.730\n0.735\n0 1 2 3 4 5\nBM25 RM3 FlanPRF GenEnsemblePRF\nFig. 5.Effect of feedback documents under sparse (BM25) and neural (MonoT5) rankers\n6 Conclusions\nZero-shot QR is advantageous since it does not rely on any labeled relevance judgements and\nallows eliciting pre-trained knowledge in the form of keywords by prompting the model with\nthe original query and appropriate instruction. By introducing GenQREnsemble, we show that\nzero-shot performance can be further enhanced by using multiple views of the initial instruction.\nWe also show that the extension GenQREnsembleRF is able to effectively incorporate relevance\nfeedback, either automated or from users. While generative QR greatly benefits from our ensem-\nble approach, the proposed methods come at a cost of potentially increased latency, but this is\nbecoming less problematic with the increased availability of batch inference for LLMs. The pro-\nposed ensemble approach could also be applied to other settings, for example, to address different\naspects of queries or metrics to optimize, or to better control the generated reformulations.\nReferences\n1. Abdul-Jaleel, N., Allan, J., Croft, W.B., Diaz, F., Larkey, L., Li, X., Smucker, M.D., Wade, C.: Umass\nat trec 2004: Novelty and hard. Computer Science Department Faculty Publication Series p. 189 (2004)\n2. Alaofi, M., Gallagher, L., Sanderson, M., Scholer, F., Thomas, P.: Can generative llms cre-\nate query variants for test collections? an exploratory study. In: Proceedings of the 46th Inter-\nnational ACM SIGIR Conference on Research and Development in Information Retrieval (SI-\nGIR ’23). pp. 1869–1873. Association for Computing Machinery, New York, NY , USA (2023).\nhttps://doi.org/10.1145/3539618.3591960\n3. Arora, S., Narayan, A., Chen, M.F., Orr, L., Guha, N., Bhatia, K., Chami, I., Re, C.: Ask me anything: A\nsimple strategy for prompting language models. In: The Eleventh International Conference on Learning\nRepresentations (2022)\n4. Bondarenko, A., Fröbe, M., Beloucif, M., Gienapp, L., Ajjour, Y ., Panchenko, A., Biemann, C., Stein,\nB., Wachsmuth, H., Potthast, M., et al.: Overview of touché 2020: argument retrieval. In: Experimental\nIR Meets Multilinguality, Multimodality, and Interaction: 11th International Conference of the CLEF\nAssociation, CLEF 2020, Thessaloniki, Greece, September 22–25, 2020, Proceedings 11. pp. 384–395.\nSpringer (2020)\n5. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P.,\nSastry, G., Askell, A., et al.: Language models are few-shot learners. Advances in neural information\nprocessing systems 33, 1877–1901 (2020)\n8 Kaustubh D. Dhole, Eugene Agichtein\n6. Carpineto, C., Romano, G.: A survey of automatic query expansion in information retrieval. Acm\nComputing Surveys (CSUR) 44(1), 1–50 (2012)\n7. Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y ., Fedus, W., Li, Y ., Wang, X., Dehghani, M.,\nBrahma, S., et al.: Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416\n(2022)\n8. Dhole, K., Gangal, V ., Gehrmann, S., Gupta, A., Li, Z., Mahamood, S., Mahadiran, A., Mille, S.,\nShrivastava, A., Tan, S., et al.: Nl-augmenter: A framework for task-sensitive natural language aug-\nmentation. Northern European Journal of Language Technology 9(1) (2023)\n9. Dhuliawala, S.Z., Komeili, M., Xu, J., Raileanu, R., Li, X., Celikyilmaz, A., Weston, J.E.: Chain-of-\nverification reduces hallucination in large language models (2023)\n10. Gao, J., Xie, S., He, X., Ali, A.: Learning lexicon models from search logs for query expansion. In:\nTsujii, J., Henderson, J., Pa¸ sca, M. (eds.) Proceedings of the 2012 Joint Conference on Empirical\nMethods in Natural Language Processing and Computational Natural Language Learning. pp. 666–\n676. Association for Computational Linguistics, Jeju Island, Korea (Jul 2012), https://aclanthology.\norg/D12-1061\n11. Gao, L., Ma, X., Lin, J., Callan, J.: Precise zero-shot dense retrieval without relevance labels. In: Pro-\nceedings of the 61st Annual Meeting of the Association for Computational Linguistics (V olume 1:\nLong Papers). pp. 1762–1777 (2023)\n12. Hasibi, F., Nikolaev, F., Xiong, C., Balog, K., Bratsberg, S.E., Kotov, A., Callan, J.: Dbpedia-entity v2:\na test collection for entity search. In: Proceedings of the 40th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval. pp. 1265–1268 (2017)\n13. Hsu, F.D., Taksa, I.: Comparing rank and score combination methods for data fusion in information\nretrieval. Information Retrieval 8(3), 449–480 (2005)\n14. Jagerman, R., Zhuang, H., Qin, Z., Wang, X., Bendersky, M.: Query expansion by prompting large\nlanguage models. arXiv preprint arXiv:2305.03653 (2023)\n15. Li, H., Mourad, A., Zhuang, S., Koopman, B., Zuccon, G.: Pseudo relevance feedback with deep lan-\nguage models and dense retrievers: Successes and pitfalls. ACM Transactions on Information Systems\n41(3), 1–40 (2023)\n16. Li, Y ., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.G., Chen, W.: Making language models better\nreasoners with step-aware verifier. In: Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (V olume 1: Long Papers). pp. 5315–5333 (2023)\n17. MacAvaney, S., Yates, A., Feldman, S., Downey, D., Cohan, A., Goharian, N.: Simplified data wran-\ngling with ir_datasets. In: Proceedings of the 44th International ACM SIGIR Conference on Research\nand Development in Information Retrieval. pp. 2429–2436 (2021)\n18. Macdonald, C., Tonellotto, N., MacAvaney, S., Ounis, I.: Pyterrier: Declarative experimentation in\npython from bm25 to dense retrieval. In: Proceedings of the 30th acm international conference on\ninformation & knowledge management. pp. 4526–4533 (2021)\n19. Mo, F., Mao, K., Zhu, Y ., Wu, Y ., Huang, K., Nie, J.Y .: Convgqr: Generative query reformulation for\nconversational search. In: Proceedings of the 61st Annual Meeting of the Association for Computa-\ntional Linguistics (V olume 1: Long Papers). pp. 4998–5012 (2023)\n20. Mohankumar, A.K., Begwani, N., Singh, A.: Diversity driven query rewriting in search advertising.\nIn: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. pp.\n3423–3431 (2021)\n21. Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., Deng, L.: Ms marco: A\nhuman-generated machine reading comprehension dataset (2016)\n22. Nogueira, R., Lin, J., Epistemic, A.: From doc2query to doctttttquery. Online preprint 6(2) (2019)\n23. Peng, B., Li, C., He, P., Galley, M., Gao, J.: Instruction tuning with gpt-4. arXiv preprint\narXiv:2304.03277 (2023)\n24. Pradeep, R., Nogueira, R., Lin, J.: The expando-mono-duo design pattern for text ranking with pre-\ntrained sequence-to-sequence models. arXiv preprint arXiv:2101.05667 (2021)\nGenQREnsemble and GenQREnsembleRF 9\n25. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., Liu, P.J.:\nExploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine\nlearning research 21(140), 1–67 (2020)\n26. Si, L., Lu, J., Callan, J.: Combining multiple resources, evidences and criteria for genomic information\nretrieval. In: TREC (November 2006)\n27. Srivastava, A., Rastogi, A., Rao, A., Shoeb, A.A.M., Abid, A., Fisch, A., Brown, A.R., Santoro, A.,\nGupta, A., Garriga-Alonso, A., et al.: Beyond the imitation game: Quantifying and extrapolating the\ncapabilities of language models. Transactions on Machine Learning Research (2023)\n28. Thakur, N., Reimers, N., Rücklé, A., Srivastava, A., Gurevych, I.: Beir: A heterogeneous benchmark for\nzero-shot evaluation of information retrieval models. In: Thirty-fifth Conference on Neural Information\nProcessing Systems Datasets and Benchmarks Track (Round 2) (2021)\n29. Wang, X., MacAvaney, S., Macdonald, C., Ounis, I.: Generative query reformulation for effective adhoc\nsearch. arXiv preprint arXiv:2308.00415 (2023)\n30. Wang, X., Macdonald, C., Tonellotto, N., Ounis, I.: Colbert-prf: Semantic pseudo-relevance feedback\nfor dense passage and document retrieval. ACM Transactions on the Web17(1), 1–39 (2023)\n31. Wang, X., Wei, J., Schuurmans, D., Le, Q.V ., Chi, E.H., Narang, S., Chowdhery, A., Zhou, D.: Self-\nconsistency improves chain of thought reasoning in language models. In: The Eleventh International\nConference on Learning Representations (2022)\n32. Weller, O., Lo, K., Wadden, D., Lawrie, D., Van Durme, B., Cohan, A., Soldaini, L.: When do gen-\nerative query and document expansions fail? a comprehensive study across methods, retrievers, and\ndatasets. In: Graham, Y ., Purver, M. (eds.) Findings of the Association for Computational Linguis-\ntics: EACL 2024. pp. 1987–2003. Association for Computational Linguistics, St. Julian’s, Malta (Mar\n2024), https://aclanthology.org/2024.findings-eacl.134\n33. Wiher, G., Meister, C., Cotterell, R.: On decoding strategies for neural text generators. Transactions of\nthe Association for Computational Linguistics 10, 997–1012 (2022)\n34. Wolf, T., Debut, L., Sanh, V ., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R.,\nFuntowicz, M., et al.: Transformers: State-of-the-art natural language processing. In: Proceedings of\nthe 2020 conference on empirical methods in natural language processing: system demonstrations. pp.\n38–45 (2020)\n35. Yu, H., Xiong, C., Callan, J.: Improving query representations for dense retrieval with pseudo relevance\nfeedback. In: Proceedings of the 30th ACM International Conference on Information & Knowledge\nManagement. pp. 3592–3596 (2021)\n36. Zhao, Z., Wallace, E., Feng, S., Klein, D., Singh, S.: Calibrate before use: Improving few-shot perfor-\nmance of language models. In: International conference on machine learning. pp. 12697–12706. PMLR\n(2021)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8413118124008179
    },
    {
      "name": "Shot (pellet)",
      "score": 0.7101271748542786
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.6476693749427795
    },
    {
      "name": "Generative grammar",
      "score": 0.5907137393951416
    },
    {
      "name": "Artificial intelligence",
      "score": 0.415446937084198
    },
    {
      "name": "Generative model",
      "score": 0.4110676944255829
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I150468666",
      "name": "Emory University",
      "country": "US"
    }
  ],
  "cited_by": 9
}