{
  "title": "Weed Recognition Method based on Hybrid CNN-Transformer Model",
  "url": "https://openalex.org/W4385482504",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2096219712",
      "name": "Jun Zhang",
      "affiliations": [
        "South China Agricultural University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4205126619",
    "https://openalex.org/W4223586001",
    "https://openalex.org/W3165807380",
    "https://openalex.org/W3046356862",
    "https://openalex.org/W4221075466",
    "https://openalex.org/W3140101410",
    "https://openalex.org/W3010345596",
    "https://openalex.org/W3040940104",
    "https://openalex.org/W4322710570",
    "https://openalex.org/W3163910408",
    "https://openalex.org/W4285483923",
    "https://openalex.org/W4320036918",
    "https://openalex.org/W2962953743"
  ],
  "abstract": "As an important task in precision agriculture, weed recognition plays a crucial role in crop management and yield increase. However, achieving high accuracy and efficiency at the same time remains a challenge. To address the balance between accuracy and timeliness in weed recognition, this paper proposes a hybrid CNN-Transformer model for weed recognition. The model uses a combination of convolutional neural network (CNN) and Transformer structures for feature extraction and classification, taking into account both global and local information. In addition, the proposed Transformer Block incorporates the SDTA (Segmentation Depth Transpose Attention) mechanism to improve timeliness. Furthermore, this paper improves the original ViT model to enhance its accuracy. Experimental results on the Deep Weeds dataset by Olsen et al. show that the proposed hybrid model outperforms the original Vision Transformer model in weed recognition accuracy (89.43% vs. 96.08%). This research provides an effective solution for weed recognition using a hybrid model, with high practical value and application prospects.",
  "full_text": "Frontiers in Computing and Intelligent Systems \nISSN: 2832-6024 | V ol. 4, No. 2, 2023 \n \n72 \nWeed\tRecognition\tMethod\tbased\ton\tHybrid\tCNN‐\nTransformer\tModel\t\nJun Zhang * \nCollege of Software Engineering, South China Agriculture University, Guangzhou, Guangdong, 510642, China \n* Corresponding author Email: opip47@163.com \n \nAbstract: As an important task in precision agriculture, weed recognition  plays a crucial role in crop management and yield \nincrease. However, achieving high accuracy and efficiency at the same time remains a challenge. To address the balance between \naccuracy and timeliness in weed recognition, this paper proposes a hybrid CNN-Transformer model for weed recognition. The \nmodel uses a combination of convolutional neural network (CNN) and Transformer structures for feature extraction and \nclassification, taking into account both global and local infor mation. In addition, the proposed Transformer Block incorporates \nthe SDTA (Segmentation Depth Transpose Attention) mechanism to improve timeliness. Furthermore, this paper improves the \noriginal ViT model to enhance its accuracy. Experimental result s on the Deep Weeds dataset by Olsen et al. show that the \nproposed hybrid model outperforms the original Vision Transformer model in weed recognition accuracy (89.43% vs. 96.08%). \nThis research provides an effective solution for weed recognition using a hybrid model, with high practical value and application \nprospects. \nKeywords: Weed Recognition; Vision Transformer; Hybrid CNN-Transformer Model; SDTA. \n \n1. Introduction \nW i t h  t h e  r a p i d  d e v e l o p m e n t  o f  g l o b a l  a g r i c u l t u r e ,  w e e d  \nproblem has become one of the urgent issues to be solved in \nagricultural production[1]. Weeds grow profusely and \ncompete with crops for nutrients, water and sunlight, which \nnot only severely affects the growth of crops, but also \nsignificantly reduces crop yields[2]. In order to achieve the \ngoal of weeding, people began to spray large amounts of \nherbicides for weeding, but this method is prone to \nenvironmental pollution and hazards. Therefore, in order to \nimprove agricultural production efficiency, reduce \nenvironmental pollution and hazards, an accurate and \nefficient method to identify and classify weeds is needed[3]. \nWeed identification and management has become a frontier \nand hotspot issue in the current research field of weed control, \nand the development of image processing technology and \nartificial intelligence technology provides new ideas and \npossibilities for solving this problem[4]. The current weed \nrecognition technology can be primarily divided into two \ncategories in academic research: machine learning methods \nbased on traditional manual feature extraction, and deep \nlearning models based on automatic feature extraction. There \nalgorithms not only have the ability to automatically extract \nfeatures from weed images, but also achieve higher \nrecognition accuracy and real-time performance. \nIn current weed identification technology, the application \nof machine learning and deep learning models based on \ntraditional manual feature extr action is becoming more and \nmore popular. These algorithms  c a n  n o t  o n l y  a u t o m a t i c a l l y  \nextract features from weed images, but also have higher \nrecognition accuracy and real-time performance[5]. \nMethod based on manual feature extractor and classifier: \nThis method usually requires manual design and extraction of \nfeatures related to weeds, and uses classifiers for \nclassification, such as Sup port Vector Machine (SVM), \nRandom Forest, etc. [4].Cristóbal[6] et al. used Hough \ntransform and SVM classifier to separate weeds in corn fields, \nachieving good results. Lottes[7] et al. proposed a weed \nspecies detection method based on dense stereo vision, \ncomparing the depth value with manually selected weed \nheight lines to identify the pr esence and species of weeds. \nZhang[7] et al. preprocessed weed images using \nmorphological filtering and threshold segmentation, extracted \na series of features, and trained a random forest classifier to  \nclassify different types of weeds. However, this method has \nsome drawbacks. Firstly, this method requires manual design \nand extraction of features related to weeds, which may require \nexpert experience and sample data, and important information \nmay be lost in the feature extraction process. Secondly, the \nperformance and accuracy of the classifier mainly depend on \nthe selected features. If the features are not representative, the \nrecognition effect and accuracy may be affected. Finally, \ncompared with using deep learning methods for weed \nidentification, this method requires more manual operations \nand time-consuming[3]. \nMethod based on deep learning: This is a method that uses \ndeep learning technology for  object extraction, feature \nextraction, and classification[8]. Due to the characteristics of \nweed data, the dataset needs to be preprocessed to remove the \nbackground from the image and only retain the weed body to \nimprove the performance of the subsequent model[9]. \nCommon preprocessing methods include the supergreen \nalgorithm[10] and Otsu threshold segmentation algorithm[10]. \nIn terms of feature extraction, Convolutional Neural Network \n(CNN) is one of the most widely used models. It can \nautomatically learn the most r epresentative features from \nimages and generate a set of h igh-level abstract feature \nrepresentations[10]. Based on these features, various \nclassifiers can be used for cl assification, such as Support \nVector Machine (SVM), Random Forest, etc. In recent years, \nmany new deep learning models and methods have emerged. \nShah[11] et al. proposed an im age classification algorithm \nmainly targeting pests in soybeans, which was trained and \nclassified using ResNet-50. Zhan[12] et al. used high-\nresolution remote sensing images as input and achieved \n \n73 \nautomatic segmentation of different crop types by training an \nFCN model with dilated convolution kernels. Zhu[13] et al. \nintroduced a self-attention mechanism based on the improved \nU-Net model and achieved automatic segmentation of \ndifferent crop types through training. These models have \noutstanding performance in image segmentation and \ndetection tasks and also have potential in weed identification \nin the agricultural field. In summary, deep learning models \nhave the advantages of strong adaptive ability and high \nrecognition rate, but they require a large amount of training \ndata and high computing resources. \nHowever, weed identification technology still has some \nlimitations and deficiencies. F irstly, weeds of different \nspecies or growth stages have different shapes and features, \nand weeds are very similar to crops, making the recognition \ntask more complex, which puts higher demands on the \nrecognition algorithm[3]. Secondly, given that most of the \nexisting popular methods pursue higher accuracy, they cannot \nmaintain faster inference speed, which leads to poor \nperformance in deployment on resource-constrained edge \ndevices[14]. Finally, currently, most of the existing weed \nidentification algorithms are based on CNN for feature \nextraction. The dense computation and parameter sharing of \nCNN make its ability to extract local semantic information \nquite strong, but in real-world scenarios, not only local \ninformation needs to be focused on, but also global semantic \ninformation needs to be paid attention to[15]. \nTherefore, in this paper, a weed identification method \ncombining CNN and Transformer hybrid model will be \nproposed to solve the above problems. This method first uses \ncontinuous convolutional layers for local feature extraction, \nthen uses Transformer Block for global feature extraction, and \nfinally combines the features of both to conduct image \nclassification. Among them, the Transformer Block is based \non the EdgeNeXt architecture, introducing SDTA \n(Segmentation Depth Transpose Attention), and using the \n\"Patchify\" strategy in the input section, followed by \ncontinuous stacking of three convolutional encoders to extract \nlocal features. At the same time, this paper will optimize it \naccording to the characteristic s of practical application \nscenarios and datasets, and use real data for testing to verify  \nthe accuracy and precision  of this method in weed \nidentification, providing scientific support for achieving more \naccurate and efficient weed identification. \n2. Experimental Materials \n2.1. Dataset \n \nFig 1. Images of different weed categories. \nThe weed dataset used in this project comes from the \nDeepWeeds weed dataset publicly available by Olsen[16] et \nal. The dataset consists of 9 categories, including 8 weed \ncategories and 1 negative class, covering some common weed \nspecies such as Chineseapple, Siam weed, Parkinsonia, etc. \nThe dataset contains images captured in natural environments \nwith various lighting and angles. The dataset contains a total \nof 17,509 images, which have been divided into 14,036 \ntraining set images and 3,473 testing set images in this study. \nSample images from the dataset are shown in Fig.1. \n2.2. Data Preprocessing \nViT model was trained on the ImageNet dataset, so its input \nimage size is 224×224. In order to adapt the DeepWeeds \ndataset to the ViT network, improve computational efficiency, \nand make the model easier to train and optimize, all images \nwere resized to 224×224 and the data was normalized the data \nto [0,1] in this study to avoid numerical overflow while \nmaintaining numerical stability. \n3. Experimental Procedure \nThe weed recognition network proposed in this paper is \nmainly composed of a CNN-Transformer hybrid model, \nwhich includes StemConv module, ConvBlock(CNN) \nmodule, Transformer encoder module, and pooling fully \nconnected classification module. \n3.1. Overall Model Architecture \n \nFig 2. Overall architecture diagram of the hybrid model \n \nFig 3. The specific structure of block i (1~4) \n \nThe overall structure of the CNN-Transformer hybrid \nmodel proposed in this study is shown in Fig.2. Firstly, the \ninput image is passed into the StemConv layer (which \n\n \n74 \nincludes four convolutional layers) to perform preliminary \nfeature extraction and downsampling. This mainly aims to \nreduce network computation and improve time efficiency, as \nhigh-resolution images require high-performance devices for \ntraining a network using Trans former to extract features. \nAdditionally, this step transforms the data to a form suitable \nfor subsequent neural network processing. Secondly, the \npreprocessed data is fed into a four-stage network structure, \nwhere each block receives the output of the previous block as \nits input and performs further feature extraction and \ntransformation. Specifically, this includes n ConvBlocks at \nthe beginning and 1 TransBlock at the end, as shown in Fig.3, \nwith the number of ConvBlocks adjusted based on pretraining \n(in this study, the number of ConvBlocks for the four blocks \nwere 3, 4, 10, and 3 respectively). Finally, the model performs \nfinal classification by reducing the number of features \nthrough a pooling layer, taking the average value at each \nfeature position in the feature map and mapping it to the \nnumber of output categories through a fully connected layer \nto display the predicted results of the input image. \n3.2. ConvBlock \nThe ConvBlock is designed to extract high-level features \nfrom input data in a deep learning model, improving its \ngeneralization performance and enabling it to better complete \nvarious tasks. Through operations such as convolutional \nlayers, pooling layers, batch normalization layers, and \nnonlinear activation functions, the input data is gradually \nabstracted and compressed, enhancing the model's \nperformance and generalization abilities. In this paper, we \ncontinuously trained the model and designed the \nconvolutional block shown in Fig.4. When the input is passed \ninto the ConvBlock, the data is first grouped locally through \nPatch Embedding for subsequent convolutional operations. \nThen, 3x3 and 1x1 sliding windows are used to perform \nconvolutional operations on the input, which are activated \nthrough BN batch standardization and GELU functions. \nSubsequently, pooling operations are used to reduce the size \nof the feature map, followed by batch normalization and \nfinally a fully connected layer that abstracts the high-level \nfeatures of the data for downstream tasks. \n \n \nFig 4. The structure of ConvBlock \n3.3. TransBlock \nIn the TransBlock module of the neural network structure \nproposed in this paper, the EdgeNeXt [15] architecture is used \nto segment and extract features from input data. The \nEdgeNeXt model is an efficient and deployable deep learning \narchitecture that can be applie d to mobile visual tasks. The \nmodel consists of two main components: the Convolution \nEncoder for extracting image features, and the STDA Encoder \nfor combining time and space information to achieve more \naccurate object tracking. \nThe specific structure of EdgeNeXt can be seen in Fig.5, \nwhich follows the standard \"four-stage\" pyramid-style design \nspecification and includes two core modules: the \nconvolutional encoder and the STDA encoder. At the \nbeginning of the overall structure, a \"Patchify\" strategy \nsimilar to ViT and SwinTransformer is used, employing non-\noverlapping convolutions of size 4x4 to achieve better \npooling effects, resulting in an output size of H/4xW/4xC1. \nNext, three stacked convolutional encoders of kernel size 3x3 \nare used to extract local features without changing the feature \nmap size. In Stage 2, down-sampling is achieved through \nconvolution with a stride of (2,2), after which two consecutive \nencoder layers with kernel size 5x5 are stacked and position \nencoding is added via element-wise addition before entering \nthe STDA module. In Stages 3 and 4, summing convolutional \nencoding layers are used, and different kernel sizes are \napplied to implement an adaptive kernel size mechanism. \nThis design was adopted to increase the CNN's local \nreceptive field and improve model performance while \navoiding the expensive computational cost associated with \ndirectly using larger kernels. Therefore, the pyramid-style \ndesign is a reasonable approach. Furthermore, adding position \nencoding only once in the four stages reduces the impact on \ndetection, segmentation, and other tasks while improving \nmodel inference speed. \n \nFig 5. The schematic diagram of the EdgeNeXt architecture \n3.4. SDTA (Segmentation Depthwise Transpose \nAttention) \nWhen trying to combine the excellent characteristics of \nVision Transformer (ViT) and CNN, any combination method \ninevitably brings a new problem: due to the characteristics of \nself-attention calculation, t he inference speed of the \narchitecture is severely limited. Therefore, in order to make \nthe model balance performance and inference speed, the \nauthors of EdgeNeXt introduce a Segmentation Depthwise \nTranspose Attention (SDTA[15]) encoder as shown in Fig.6, \nwhich achieves efficient integration without increasing \nadditional parameter volume and multiplication-addition \noperation amount (MAdds). The SDTA encoder consists of \ntwo main components: the feature encoding module and the \nself-attention calculation module. \nIn the feature encoding module, the input features are first \ndivided into s subsets, each with an equal size, and each subset \nis encoded by fusing the output features of the previous subset \nand then passing through a 3x3 depth-wise convolution. The \nSDTA Encoder uses an adaptive number of subsets to allow \nflexibility in feature encoding, and the output features of s \nsubsets are concatenated to obtain output features with \nmultiple scales. This module tries to learn an adaptive multi-\nscale feature representation, encoding different spatial levels  \non the input image and intuitively encoding the global image \nrepresentation. \nThe self-attention calculation module is the second \ncomponent of the SDTA Encoder, which calculates attention \non the input features to obtain important features. In this \nmodule, the input features are first transformed into Q, K, and \nV tensors through three linear layers, and L2 normalization is \nperformed on the Q and K tensors before calculating the \ncross-covariance attention to stabilize the training. This \nmodule uses dot product operation to calculate the dot product \nbetween Q T Q and K K T in the channel dimension, resulting \nin an attention matrix of C x C. The attention matrix is further \nprocessed by softmax and dot product with the V tensor to \ncalculate the final attention map. Finally, two 1x1 point-wise \n\n \n75 \noperations are used, with LN and GELU activation layers to \nproduce non-linear features. To avoid complexity problems, \nthis module borrows from the transpose QK attention feature \nmap and uses MSA's dot product operation in the channel \ndimension to achieve linear complexity. \n \nFig 6. The STDA Encoder structure diagram \n4. Results and Analysis \n4.1. Experimental Configuration \nThe experiments were conducted on a Windows 10 \noperating system, with an AMD Ryzen 7 4800H CPU and an \nNVIDIA GeForce GTX 1650 GPU. The programming \nenvironment used for training was Python 3.7 and the deep \nlearning framework was PyTorch 1.8.0. \n4.2. Training Settings \nThe SGD optimizer was selected for model training, with a \nlearning rate initialized to 0.001, momentum of 0.9, weight \ndecay of 5e-5, batch_size of 8, and 100 iterations. \nAdditionally, the cosine annealing function was applied to \ndynamically adjust the learning rate, with the maximum \nlearning rate set to 0.01. \nSpecifically, the learning rate corresponding to the current \nepoch was calculated based on the shape of the cosine \nfunction during each iteration, and then updated using the \nscheduler. At the end of each e poch, the relevant indicators \nfor that epoch, including trai ning loss, training accuracy, \nvalidation loss, validation accuracy, and current learning rate, \nwere recorded in TensorBoard for easy observation and \ncomparison of results between different experiments. At the \nsame time, the model parameters for the current epoch were \nsaved to a designated path for future testing or further fine-\ntuning. \n4.3. Performance Evaluation Metrics \nIn this paper, we used five specific metrics to demonstrate \nthe performance of the model: accuracy, precision, recall, F1 \nscore, and confusion matrix. In this paper, assuming TP and \nTN are the number of corr ectly identified positive and \nnegative samples, and FP and FN are the number of \nincorrectly identified positive and negative samples, \nrespectively, then: \nAccuracy refers to the proportion of samples correctly \nclassified by the classifier to the total number of samples: \nൌ\n்௉ା்ே\n்௉ା்ேାி௉ାிே                  ( 1 )  \nPrecision refers to the proportion of actual positive samples \namong the samples predicted as positive by the classifier: \nൌ\n்௉\n்௉ାி௉                    ( 2 )  \nRecall refers to the proportion of samples correctly \npredicted as positive by the classifier among all true positive  \nsamples: \nൌ\n்௉\n்௉ାிே                       ( 3 )  \nF1 score is a metric that considers both precision and recall, \nwhich is the harmonic mean of precision and recall: \n1 ൌ\nଶൈ௣௥௘௖௜௦௜௢௡ൈ௥௘௖௔௟௟\n௣௥௘௖௜௦௜௢௡ା௥௘௖௔௟௟ \t                ( 4 )  \nThe confusion matrix can help gain insight into the model's \nperformance on different categories and provide some \nreference for subsequent model adjustments. \n4.4. Experimental Results and Analysis \nTo evaluate the accuracy of the model, the CNN-\nTransformer hybrid model was compared with some classical \nmodels such as Vision Transformer, ResNet50, AlexNet, \nVGG16, etc. in terms of accuracy, as shown in Table 1. \n \nTable 1. Comparison of Model Accuracy \nModel Accurac y/%\nVGG16 86.21\nGoogLeNet 79.23\nAlexNet 80.09\nViT 89.43\nCNN-Transformer 96.08\n \nBased on Table 1, it can be seen that the use of a \nTransformer network can bring significant performance \nimprovements compared to som e classical models obtained \nby pre-training CNN models. Due to the stronger sequence \nmodeling ability and the ability to capture long-range \ndependencies, the hybrid model and the original ViT model \nhave higher accuracy than other models. Compared with the \noriginal ViT model, the CNN-Transformer hybrid model can \nbetter balance local and global features, resulting in further \nperformance improvement. \n \nFig7. Learning rate curve \n \nFig 8. accuracy curve on the training set (left) and validation set \n(right) \n \nTo demonstrate the model's fitting ability during the \ntraining process and the model's performance on unknown \ndata, the learning rate curve (Fig.7), the accuracy curve on the \ntraining set and validation set (Fig.8), and the loss curve \n(Fig.9) were plotted. Overall, the model showed a good \nperformance. On the accuracy and loss curves of the test set, \nthere were slight fluctuations and instability, which may be \n\n \n76 \ndue to the presence of noisy or irregular data in the test set \nthat are different from the data in the training set, including  \nincorrectly labeled data or mi ssing data, but the fluctuation \nrange is still acceptable. \n \n \nFig 9. loss curve on the training set (left) and validation set (right) \n4.5. Model Performance Evaluation \n \nFig 10. Confusion matrix of the original ViT (left) and the hybrid \nmodel (right) \n \nFig 11. Class 7 is recognized as Class 0 \n \nThe confusion matrix is a tool used to evaluate the \nperformance of classification models. It can help understand \nthe model's performance on different categories, and quantify \nand visualize its classification performance. Here, a \ncomparison was made between the confusion matrices of the \noriginal ViT model and the CNN-Transformer hybrid model \nas shown in Fig.10. The horizontal axis represents the \npredicted categories on the test set, while the vertical axis \nrepresents the true categories of the test set images. The \nnumbers on the main diagonal represent the number of correct \npredictions by the model, while the others are the numbers of \nincorrect predictions. In the original dataset, there are 8 \ncategories of weeds, labeled from 0 to 7, and the negative \nclass is labeled as 8. The sample distribution is uneven, so \nthere are more cases where the eighth category is \nmisclassified as other categor ies or other categories are \nmisclassified as the eighth cat egory. However, considering \nthat the number of images in the eighth category is almost 8 \ntimes that of other categories, this error rate can be accepted. \nThe hybrid model repeatedly misidentified images of \ncategory 7 as images of category 0. Based on the analysis of \nthe images as shown in Fig.11, it is speculated in this study \nthat this is due to poor lighting and a similarity in the \nmorphology of the two types of weeds, leading to difficult \nrecognition. Overall, according to the analysis of the \nconfusion matrix, the hybrid model has good performance on \nthis dataset, and has made significant improvements in \nperformance compared to the original ViT model. \n5. Conclusion \nTo simultaneously capture local and global information in \nthe feature extraction and cl assification process of weed \nrecognition, this paper combines the advantages of traditional \nCNN models and Vision Transformer models to design a \nmore suitable CNN-Transformer hybrid model for weed \nrecognition. Based on a four-stage CNN structure, each stage \nuses stacked ConvBlocks to perform local feature extraction, \nand a TransBlock is added at the end of each stage for global \nfeature extraction. In the Tr ansBlock, an STDA attention \nmechanism is introduced, which efficiently combines the \nCNN and ViT models to minimize the impact of the slower \ninference speed caused by the self-attention calculation \nmethod. Experimental results show that the hybrid model has \nhigh accuracy, precision, recall, and F1 score. This model \novercomes the limitations of CNN in modeling global \ninformation and does not require the massive computation of \nthe ViT model, making it more suitable for deployment on \nedge devices for weed recognition. \nHowever, there are still challenges in practical applications, \nsuch as the diversity and quantity of weed species increasing \nthe difficulty of model training, requiring more computing \nresources and memory space to ensure the efficient operation \nof the model, and different shooting angles and lighting \nconditions in different environments may also affect the \nstability and accuracy of the model. Future research can \nfurther optimize the structure and parameter settings of the \nhybrid model, explore more efficient computing methods and \nalgorithms, and combine the model with other weed \nclassifiers and agricultural technologies to improve the \nefficiency and sustainability of agricultural production. \nReferences \n[1] Haq, M. A. (2022). CNN Based Automated Weed Detection \nSystem Using UAV Imagery. Computer Systems Science and \nEngineering, 42(2), 837-849.  \n[2] Razfar, N., True, J., Bassiouny,  R., Venkatesh, V., & Kashef, \nR. (2021). Weed detection in soybean crops using custom \nlightweight deep learning models. Computers and Electronics \nin Agriculture, 185, 106016.  \n[3] Wu, Z., Chen, Y., Zhao, B., Kang, X., & Ding, Y. (2021). \nReview of Weed Detection Me thods Based on Computer \nVision. Frontiers in Plant Science, 12, 634505.  \n[4] Su, W.-H. (2021). Advanced  Machine Learning in Point \nSpectroscopy, RGB- and Hyperspectral-Imaging for \nAutomatic Discriminations of Crops and Weeds: A Review. \nSensors, 21(14), 4707.  \n[5] Tao, T., & Wei, X. (2020). A hybrid CNN-SVM classifier for \nweed recognition in winter rape field. Precision Agriculture, \n21(1), 26-37. \n[6] Cristóbal, J., Moreda, G. P., & Muñoz-Rodríguez, M. (2006). \nSupport vector machine classifi cation to localize weeds in \nimages of a maize field. Spa nish Journal of Agricultural \nResearch, 4(4), 433-444.  \n\n \n77 \n[7] Lottes, P., & Simard, P. (2010). Weed species detection using \ndense stereo vision. Proceedings of the 17th International \nConference on Image Processing (ICIP), 1337-1340. Zhang, \nM., Liu, H., Dong, T., & Slaughter, D. C. (2015). Automated \nweed identification using an ensemble of optimized \nsegmentation and classifica tion methods. Computers and \nElectronics in Agriculture, 116, 225-232. \n[8] Hu, K., Wang, Z., Coleman, G., Bender, A., Yao, T., Zeng, S., \nSong, D., Schumann, A., & Walsh, M. (2020). Deep Learning \nTechniques for In-Crop Weed Identification: A Review. \nComputers and Electronics in Agriculture, 178, 105715.  \n[9] Espejo-Garcia, B., Mylonas, N., Athanasakos, L., Fountas, S., \n& Vasilakoglou, I. (2019). Towards weeds identification \nassistance through transfer learning. Computers and \nElectronics in Agriculture, 162, 183-193.  \n[10] Wang, Y., Liu, H., Wang, D., & Liu, D. (2020). Image \nprocessing in fault identification for power equipment based on \nimproved super green algorit hm. Measurement, 154, 107503. \nXiao, L., Ouyang, H., & Fan, C. (2019). An improved Otsu \nmethod for threshold segmentation based on set mapping and \ntrapezoid region intercept histogram. Optik, 180, 718-726. \nJiang, H., Zhang, C., Qiao, Y., Zhang, Z., Zhang, W., & Song, \nC. (2020). CNN feature based graph convolutional network for \nweed and crop recognition in smart farming. Computers and \nElectronics in Agriculture, 178, 105797. \n[11] Shah, D., Gupta, R., Patel, K., Jariwala, D., & Kanani, J. (2021). \nDeep Learning based Pest Classification in Soybean crop using \nResidual Network-50. 2021 International Conference on \nInventive Systems and Control (ICISC), 95-99. \n[12] Zhan, Z., Li, Y., Liu, F., Zhang, H., & Xu, L. (2021). Semantic \nsegmentation of agricultural cr ops using a fully convolutional \nneural network with dilated c onvolutions. Remote Sensing, \n13(2), 285. \n[13] Zhu, B., Zhen, F., Li, S., Hu , J., & Liu, Y. (2021). Crop \nsegmentation with an improved U-Net model based on self-\nattention mechanism. Remot e Sensing, 13(9), 1857. doi: \n10.3390/rs13091857. \n[14] Li, J., Xia, X., Li, W., Li, H., Wang, X., Xiao, X., Wang, R., \nZheng, M., & Pan, X. (2022). Next-ViT: Next Generation \nVision Transformer for Efficient Deployment in Realistic \nIndustrial Scenarios. IEEE/CVF International Conference on \nComputer Vision and Pattern Recognition (CVPR) Workshops.  \n[15] Maaz, M., Shaker, A., Cholakkal, H., Khan, S., Zamir, S. W., \nAnwer, R. M., & Khan, F. S. (2023). EdgeNeXt: Efficiently \nAmalgamated CNN-Transformer Architecture for Mobile \nVision Applications. In L. Ka r l i n s k y ,  T .  M i c h a e l i ,  &  K .  \nNishino (Eds.), Computer Vision – ECCV 2022 Workshops. \nECCV 2022. Lecture Notes in Computer Science, vol 13807. \nSpringer, Cham.  \n[16] Olsen, A., Konvalina, D. A., Philippa, B., et al. (2019). \nDeepWeeds: A Multiclass Weed Species Image Dataset for \nDeep Learning. Scientific Reports, 9(1), 1-12. \n[17] Dosovitskiy, A., Beyer, L., K olesnikov, A., We issenborn, D., \nZhai, X., Unterthiner, T., ... &  Houlsby, N. (2021). An Image \nIs Worth 16x16 Words: Transformers for Image Recognition \nAt Scale. arXiv preprint arXiv:2010.11929. \n[18] Wang, X., Chan, K., Gao, Y., & Yang, M. H. (2020). Training \nattention networks for high-resolution image processing: An \nempirical study. Proceedings of the IEEE/CVF Conference on \nComputer Vision and Pattern Recognition (CVPR), 10432-\n10441. \n[19] Kolesnikov, A., Lampert, C.  H., & Abdelsalam, M. (2021). \nImproved Convolutions via Hebbian Block-Sparse Kernel and \nConvex Optimization. Proce edings of the IEEE/CVF \nConference on Computer Vision and Pattern Recognition \n(CVPR), 8144-8153. \n \n ",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7163470983505249
    },
    {
      "name": "Computer science",
      "score": 0.7094641923904419
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6195369362831116
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5886577367782593
    },
    {
      "name": "Weed",
      "score": 0.5736231207847595
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4625647962093353
    },
    {
      "name": "Deep learning",
      "score": 0.4625287652015686
    },
    {
      "name": "Segmentation",
      "score": 0.43793079257011414
    },
    {
      "name": "Machine learning",
      "score": 0.39227157831192017
    },
    {
      "name": "Voltage",
      "score": 0.14944198727607727
    },
    {
      "name": "Engineering",
      "score": 0.10473409295082092
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Agronomy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I101479585",
      "name": "South China Agricultural University",
      "country": "CN"
    }
  ]
}