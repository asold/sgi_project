{
  "title": "K-XLNet: A General Method for Combining Explicit Knowledge with Language Model Pretraining",
  "url": "https://openalex.org/W3156167137",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5024318194",
      "name": "Ruiqing Yan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5001374877",
      "name": "Lanchang Sun",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5022502922",
      "name": "Fang Wang",
      "affiliations": [
        "Beijing Institute of Petrochemical Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5100462591",
      "name": "Xiaoming Zhang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963159690",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2963077256",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W1552847225",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2144550395",
    "https://openalex.org/W3040558716",
    "https://openalex.org/W2620787630",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W3012572520",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2891328459",
    "https://openalex.org/W1591840823",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3027879771"
  ],
  "abstract": "Though pre-trained language models such as Bert and XLNet, have rapidly advanced the state-of-the-art on many NLP tasks, they implicit semantics only relying on surface information between words in corpus. Intuitively, background knowledge influences the efficacy of understanding. Inspired by this common sense, we focus on improving model pretraining by leveraging explicit knowledge. Different from recent research that optimize pretraining model by knowledge masking strategies, we propose a simple but general method to combine explicit knowledge with pretraining. To be specific, we first match knowledge facts from knowledge graph (KG) and then add a knowledge injunction layer to transformer directly without changing its architecture. The present study seeks to find the direct impact of explicit knowledge on transformer per-training. We conduct experiments on various datasets for different downstream tasks. The experimental results show that solely by adding external knowledge to transformer can improve the learning performance on many NLP tasks.",
  "full_text": "None\nDigital Object Identiﬁer None\nK-XLNet: A General Method for\nCombining Explicit Knowledge with\nLanguage Model Pretraining\nRUIQING YAN1, LANCHANG SUN2, FANG WANG1 AND XIAOMING ZHANG1\n1College of Information Engineering, Beijing Institute of Petrochemical Technology, Beijing, 102617, CHN\n2School of computer science, Beijing University of Posts and Telecommunications, Beijing, 100876, CHN\nCorresponding author: Fang Wang (e-mail: fangwang@bipt.edu.cn)\nABSTRACT Though pre-trained language models such as Bert and XLNet, have rapidly advanced the\nstate-of-the-art on many NLP tasks, they implicit semantics only relying on surface information between\nwords in corpus. Intuitively, background knowledge inﬂuences the efﬁcacy of understanding. Inspired\nby this common sense, we focus on improving model pretraining by leveraging explicit knowledge.\nDifferent from recent research that optimize pretraining model by knowledge masking strategies, we\npropose a simple but general method to combine explicit knowledge with pretraining. To be speciﬁc, we\nﬁrst match knowledge facts from knowledge graph (KG) and then add a knowledge injunction layer to\ntransformer directly without changing its architecture. The present study seeks to ﬁnd the direct impact of\nexplicit knowledge on transformer per-training. We conduct experiments on various datasets for different\ndownstream tasks. The experimental results show that solely by adding external knowledge to transformer\ncan improve the learning performance on many NLP tasks.\nINDEX TERMS Knowledge Graph, Pretraining, XLNet, Language Model\nI. INTRODUCTION\nR\nECENTLY, substantial work has shown that pre-trained\nmodels [1]–[4] can learn language representations over\nlarge-scale text corpora, which are beneﬁcial for downstream\nNLP tasks. For example, XLNet [5] obtains the state-of-the-\nart results on twelve NLP tasks including reading compre-\nhension, question answering and text classiﬁcation. However,\nmost existing works model the representations by predicting\nthe missing word only through the contexts, without consid-\nering the background knowledge in the text. And trained on\ngeneral-purpose large-scale text corpora, the models usually\nlack domain adaptability.\nBackground knowledge inﬂuences the efﬁcacy of under-\nstanding. It has been observed that one major step in improv-\ning reading is to improve prior knowledge of the topics being\nread [6]. The pre-trained transformers can model implicit\nsemantic information between surface form words [7]. But it\nis solely at the token level. Considering Background knowl-\nedge can lead to better language understanding. For instance,\ngiven the sentence “Xiaomi was ofﬁcially listed on the main\nboard of HKEx”, the background knowledge includesXiaomi\nis a science and technology company , HKEx refers to Hong\nKong Exchanges and Clearing Limited and main board is\na economic term . The explicit knowledge can help better\nunderstand the word sense and sentence topic.\nMost recently, there are some improved models based on\nBert [8]–[10] or GPT [11], which prove that injecting extra\nknowledge information can signiﬁcantly enhance original\nmodels. Differently, this paper seeks to ﬁnd the direct impact\nof explicit knowledge on transformer pretraining. Based on\nXLNet, we propose a simple but general method for com-\nbining knowledge without changing the model architecture.\nGiven a sentence, We ﬁrst use a simple dictionary look-\nup method to map its background knowledge. A knowl-\nedge injection layer is designed to combine the background\nknowledge with the original sentence, in a way that is close\nto natural language and accepted by XLNet without losing\nthe structure information. Finally, we take the output of the\nknowledge injection layer directly as the input for XLNet.\nA three-stage model training method is proposed to save\ntraining time and hardware cost.To seek the impact of explicit\nknowledge, we leverage open domain and domain-speciﬁc\nknowledge to combine with XLNet and test their perfor-\nmances on various NLP tasks.\nOur contributions in this paper are three folds: 1) proposal\nof a simple but general knowledge injection method for\nVOLUME 4, 2016 1\narXiv:2104.10649v2  [cs.CL]  29 May 2021\nRuiqing Y anet al.: K-XLNet: A General Method for Combining Explicit Knowledge with Language Model Pretraining\nKnowledge \nInjection \nKnowledge \nInjection K-XLNet \nSentence \nembedding \nKnowledge \nembedding \nFeed forward \nAdd & Norm \nOutput \nOutput embedding \nOutput \npositional \nencoding \nOutputs (shifted right)\nSentence \npositional \nencoding \nKnowledge \npositional \nencoding \n+\n +\nArray \nsplicing \nMulti-Head \nAttention \nAdd & Norm \nFeed forward\nAdd & Norm\nMulti-Head \nAttention\nAdd & Norm\nNx \n+\nKnowledge \nMatching \nRaw data Knowledge \nbase \nXLNet Connection \nTransformer-XL \nFine-tuning \nಹ \nಹ \nDownstream tasks \nNER Reading \ncomprehension Classifictiaon \nಹ\nDownstream tasks\nNER\n Reading \ncomprehension\nClassifictiaon \nEmbedding layer \nTransformer \nMult i Head \nMasked \nMulti-Head \nAttention \nAdd & Norm \nMulti-Head \nAttention Add & Norm \nFeed forward Add & Norm \nOutput e \nmbedding \nOutput \npositional \nencoding\nOutputs (\nshifted right)\n+\nMasked \nMulti-Head \nAttention\nMasked \nAdd & Norm\nMulti-Head\nAttention\n Add & Norm \nFeed forward\n Add & Norm\nNx \nFIGURE 1: The overall framework of K-XLNet.\nlanguage model pretraining; 2) proposal of K-XLNet for an\nimplementation of the proposed knowledge injection method\non XLNet; 3) empirical veriﬁcation of the effectiveness of\nK-XLNet on various downstream tasks.\nThe rest of the paper is organized as follows: Section II\nsummarizes related work. In Section III, we elaborate on\nour knowledge injection method taking XLNet as a running\nexample. Section IV reports experimental results, and ﬁnally\nSection V concludes the whole paper.\nII. RELATEDWORK\nIn recent years, with the rapid development of deep learning,\nthe pre-training technology [1]–[3], [12], [13] for the ﬁeld\nof natural language processing has made great progress.\nMany efforts [4], [5] are devoted to pre-training language\nrepresentation models for learning language representations\nover large-scale corpus, and then utilizing the representations\nfor downstream NLP tasks such as question answering [14],\ntext classiﬁcation [15]. For instance, XLNet [5] has obtained\nthe state-of-the-art results on twelve NLP tasks.\nThough the language representation models have achieved\ngreat success, they are still far from ready to serve as an\n“unsupervised multitask learner.” [11] There are still gaps\nbetween model pretraining and task-speciﬁc ﬁne-tuning [16].\nPretraining models usually learn universal language repre-\nsentation from general-purpose large-scale text corpora, but\nlacking of domain or task speciﬁc knowledge. This leads to\nthe need of huge effort for task-speciﬁc ﬁne-tuning or over-\nparameterization [17].\nRecent work shows that combining with knowledge is\na promising way to improve language model. Based on\nBERT and improved by reﬁning the transformer architec-\nture with knowledge, ERNIE [8], ERNIE1(THU) [9] and\nK-Bert [10] have revealed promising result in knowledge-\ndriven applications such as named entity recognition, entity\ntyping and relation extraction. Based on GTP2.0 [18], KALM\n[11] signiﬁcantly improves downstream tasks like zero-shot\nquestion-answering, by adding entity signals to the input of\nthe transformer and an entity prediction task to the output.\nThis paper puts forward an attempt to combine knowledge\nwith XLNet. Different with the above-mentioned methods,\nwe focus on ﬁnd the direct impact of explicit knowledge on\ntransformer pretraining. Instead of changing the architecture\nof XLNet, we take sentences and their matched background\nknowledge facts as input into a knowledge injector com-\nposed of self-attention structure. The knowledge injector\nwill combine the knowledge with original text and generate\nknowledge enriched output as the input of Transformer-XL.\nFurther, we study the impact of various knowledge types on\ndifferent downstream NLP tasks.\nIII. METHODOLOGY\nFor combing knowledge with XLNet, we propose a simple\nbut general knowledge injection method called K-XLNet.\nFigure 1 shows its overall framework. We can see that K-\nXLNet does not change the original architecture of XLNet.\nA knowledge injunction layer is designed and connected to\nTransformer-XL, so as to study the inﬂuence of background\nknowledge on model pretraining. We elaborate on K-XLNet\nin the following three subsections.\n2 VOLUME 4, 2016\nRuiqing Y anet al.: K-XLNet: A General Method for Combining Explicit Knowledge with Language Model Pretraining\nToken \nembedding \nPosition \nencoding \nXiaomi listed in \nSentence token \n Token of matched knowledge facts \nHong Kong \n[1,1] [2,2] [3,3] [4,4]\nXiaomi is_a science and technology company \n[5,1] [6,1] [7,1] [7,1] [7,1] [7,1]\nHong Kong is_a city \n[8,4] [9,4] [10,4]\n+ + + + + + + + + + + + +\nArray Splicing \n Array Splicing \nSPO1 SPO2\n[i,j]\n[iPositional \ninformation \ni, absolute position index \nj, relative position index \nFIGURE 2: An example of embedding representation\nA. KNOWLEDGE MATCHING\nRegarding to the given text, how to effectively extract\nits background knowledge is a primary issue for knowl-\nedge combing with language models. We take the Subject-\nPredicate-Object (SPO) triples in knowledge graph (e.g. DB-\npedia) as the source of knowledge facts, and use a subject\ntokenizer to match the related knowledge facts for a given\nsentence. To be speciﬁc, the subject tokenizer segments the\nsentence into subjects using a surface form dictionary. It\nmaps word n-grams to subjects in KG and then gets the\nrelated SPO triples as knowledge facts for the given sentence.\nFor example, given the sentence “ Xiaomi listed in Hong\nKong”, we map two knowledge facts from KG as follows:\n(Xiaomi, is_a, science and technology company), (Hong\nKong, is_a, city). Instead of using a more precise entity linker\n[19], we use a frequency-based dictionary look up to map the\nsubject, because the dictionary look up is more efﬁcient for\nlarge-scale text corpus and using a highly tuned entity linker\nmay propagate its own biases into the transformer.\nB. KNOWLEDGE INJECTION\nIt is a challenge to fuse lexical, syntactic, and knowledge\ninformation in the pre-training procedure for language repre-\nsentation. Instead of designing a special pretraining objective\nfor the fusion, we aim to integrate knowledge naturally into\nthe original text in a way that conforms to the grammatical\nrules of natural language. For example, given the sentence\n“Xiaomi listed in Hong Kong ” and a knowledge fact (Xi-\naomi, is_a, science and technology company), generate a\nknowledge-enriched sentence like “ Xiaomi, a science and\ntechnology company, listed in Hong Kong ”. By this way,\nwe let the pretraining model to use clues in the knowledge-\nenriched text to learn word representation that better reﬂect\nhow language conveys knowledge.\nTherefore, we treat the knowledge injection problem like a\nmachine translation problem and design a knowledge injector\nwith the similar structure to the transformer used in the ﬁeld\nof natural language translation, as the knowledge injunction\nlayer shown in Fig.1. It mainly consists of two modules,\ni.e., embedding layer and transformer. For the input sentence\nand knowledge facts, embedding layer ﬁrst converts them\ntogether with positional information into embedding repre-\nsentation and then feds into the transformer for knowledge\ncombining.\n1) Embedding layer\nThe function of the embedding layer is to convert the given\nsentence and its related knowledge facts into an embedding\nrepresentation that can be fed into the following transformer.\nIn order to express the positional information of the origi-\nnal sentence and the SPO triplet, we splice the original sen-\ntence with the matched knowledge triplets, When splicing the\noriginal sentence and SPO triples, we arrange the matching\nSPO triples according to the order of their corresponding\nwords in the original sentence, such as\nW1,W2,W3,W4,W5,W6...SPOW2 ,SPO W3 ,SPO W5 ...\n(1)\nNext use a two-dimensional array composed of two ele-\nments to encode the position of each word. The ﬁrst element\nis the sequence number of the word position, called absolute\nposition index. The second element is the sequence number\nof the word that matches the subject in the original sentence,\ncalled relative position index.\nFig.2 shows an embedding example. The encoding of\nwords in the original sentence is composed of two elements\nwith the same value, because each word in the original\nsentence matches itself. For instance, the encode of the ﬁrst\nword “Xiaomi” is [1,1]. For the matched knowledge triples,\neach SPO triple is horizontally spliced into a whole sequence,\nand S, P, O are sequentially indexed. For instance, the encode\nof the ﬁrst matched knowledge subject “Xiaomi” is[5,1]. By\nthis way, the information of the original sentence, matched\nSPO triples and their positions is all preserved.\nThe calculation method of position code is to add absolute\nposition index and relative position index and normalize\nthem, that is, POS is equal to absolute position index and\nrelative position index. From the calculation method of gen-\nerating absolute position index and relative position index, it\nis easy to know that the value of absolute position index plus\nrelative position index in different information positions is\nVOLUME 4, 2016 3\nRuiqing Y anet al.: K-XLNet: A General Method for Combining Explicit Knowledge with Language Model Pretraining\nunique. After getting the position code, we use the following\nformula to normalize the position code.\nα= absolutepositionindex\nβ = relativepositionindex\npos= α+ β\nPE(pos,2i) = sin\n(\npos/100002i/dmodel\n)\nPE(pos,2i+1) = cos\n(\npos/100002i/dmodel\n)\n(2)\n2) Transformer\nWe use a transduction model to inject the matched knowl-\nedge triples to the original sentence. Regarding the matched\nknowledge triples as a different language with the original\nsentence, we turn the knowledge injection problem into a ma-\nchine translation problem, and translate them into a language\nthat conﬁrms to the natural language grammar. By this way,\nthe knowledge-enriched output can be naturally used as the\ninput of Transformer-XL. As shown in Fig.1, the transformer\nconsists of an encoder and a decoder, which has the same\nstructure with the typical Transformer [20] used in machine\ntranslation area.\nC. XLNET CONNECTION\nXLNet is one of state-of-the-art natural language models. We\ntake XLNet as a running example for combining knowledge\nwith language model pre-training. The XLNet connected\nafter the knowledge injection layer does not have a Tokeniza-\ntion module, namely the Transformer-XL. Tokenization and\nencoding have been performed in the embedding module.\nXLNet \nPre-training \nPre-trained \nXLNet \nLarge-scale \ncopus \nXLnet Fine-tuning \nTask-specific \ndataset \nPre-trained \nXLNet \nK-XLnet training \nKnowledge \nbase \nFixed parameters\nPre-\ntrained\nXLNet\nDense \nDense \nKnowledge \nInjection \nPre-\ntrained \nXLNet\ng\nFIGURE 3: A simple method for training K-XLNet\nNormally, retraining the pre-training model is necessary\nfor model reﬁnement by leverage knowledge information.\nHowever, the cost of retraining is very high, both in terms\nof time and hardware cost. We propose a simple and general\nway to resolve this problem, inspired by the mainstream idea\nof pre-training and ﬁne-tuning. Figure3 shows the process of\ntraining K-XLNet. It mainly has three stages: XLNet pre-\ntraining, task-speciﬁc ﬁne-tuning and K-XLNet training for\nthe speciﬁc task. The ﬁrst two stages are consistent with the\nusual two-stage pre-training model. Instead of pre-training\nK-XLNet on large-scale general corpus, we train it on spe-\nciﬁc tasks by leveraging external knowledge. In addition to\ncost savings, this approach makes it easy to ﬂexibly test the\neffects of different knowledge bases on different downstream\ntasks. The following experiments show that this method is\neffective.\nIV. EXPERIMENT\nIn this section, we evaluate the performance of K-XLNet\nthrough seven downstream tasks, among which one is an\nEnglish task for a speciﬁc domain, six are Chinese tasks for\nopen-domain.\nA. EXPERIMENT SETUP\n1) Preprocessing\nWe use the pre-trained word2vec 1 for word embedding,\nwhich was trained on Google News. To cover the new words\nfrom knowledge and datasets of downstream tasks, we re-\ntrain it on knowledge SPO triples and task-speciﬁc corpus.\n2) Knowledge Graph\nWe leverage HowNet2 and CN-DBpedia3 for Chinses tasks,\nDBpedia4 and MovieKG 5 for English tasks. We donot do\nany preprocessing for these KGs, since only the matched\nknowledge SPO triples will be used for training K-XLNet.\nDetail information of the KGs are as follows.\n• HowNet is a large-scale language knowledge base\nfor Chinese vocabulary and concepts [21]. It contains\n52576 sets of SPO triples.\n• CN-DBpedia [22] is a largescale open-domain encyclo-\npedic KG developed by the Knowledge Work Labora-\ntory of Fudan University, It contains 5168865 sets of\nSPO triples.\n• DBpedia is a large-scale, multilingual knowledge base\nextracted from Wikipedia [23].\n• MovieKG is a bilingual movie knowledge graph con-\nstructed by the knowledge engineering laboratory of the\ndepartment of computer science, Tsinghua university.\nUnfortunately, the database is ofﬂine at present. It in-\ncludes 23 concepts, 91 attributes, 0.7+ million entities\nand 10+ million triples. Its data sources include Linked-\nIMDB, baidu baike, douban, etc.\n• MedicalKG is a map of Chinese medical knowledge,\nwhich contains 13864 sets of SPO triples.\n3) Baselines\nThe proposed knowledge injection method is model-\nindependent. Therefore, we compare K-XLNet to two re-\nleased XLNet models 6: XLNet-Base and XLNet-Large.\nFollowing XLNet, we design two K-XLNet models as fol-\nlows:\n1https://github˙com/xgli/word2vec-api\n2http://www˙keenage˙com/\n3http://kw.fudan.edu.cn/cndbpedia/intro/\n4https://wiki.dbpedia.org/\n5http://www.openkg.cn/dataset/keg-bilingual-moviekg\n6https://github.com/zihangdai/xlnet\n4 VOLUME 4, 2016\nRuiqing Y anet al.: K-XLNet: A General Method for Combining Explicit Knowledge with Language Model Pretraining\n• K-XLNet-Base has 3-layer, 128-hidden and 2-heads in\nknowledge injection layer, with the same Transformer-\nXL parameters as XLNet-Base.\n• K-XLNet-Large has 4-layer, 128-hidden, and 3-\nheads in knowledge injection layer, with the same\nTransformer-XL parameters with XLNet-Large.\nTABLE 1: Results on emotion classiﬁcation task (Acc: %)\nMethod Dev Test\nXLNet-Base 95.32 94.37\nK-XLNet-Base (DBpedia) 95.51 94.88\nK-XLNet-Base (MovieKG) 95.82 95.03\nXLNet-Large 96.21 95.13\nK-XLNet-Large (DBpedia) 96.74 95.62\nK-XLNet-Large (MovieKG) 96.87 95.99\nB. DOMAIN-SPECIFIC TASK\nWe ﬁrst compare the performance of K-XLNet with the\noriginal XLNet on an English domain-speciﬁc task, namely\nemotion classiﬁcation for movie reviews.\nTo be speciﬁc, we use the IMDB [24] dataset for this\ntest. It includes 25,000 positive reviews and 25,000 negative\nreviews. We divided it into three parts:train, dev, and test. We\nused the train part to ﬁne-tune the model and then evaluated\nits performance on the dev and test parts. For knowledge\ninjection in K-XLNet, we used MovieKG and DBpedia re-\nspectively. Table 1 shows the experimental results.\nIt can be seen that K-XLNet is superior to the original\nXLNet in both parameter settings (base and large). This\nshows that our approach of knowledge injection to XLNet\nis effective. In addition, MovieKG performs better than\nDBpedia, indicating that domain knowledge is preferred for\nspeciﬁc domain tasks.\nWe further investigate the effect of different SPO triple\n(knowledge) amounts in K-XLNet. In this test, we use\nMovieKG for knowledge injection and set the amount of\nknowledge triples to be 1,000, 5,000, 6,000, and 7,000 re-\nspectively. The results are shown in Figure 4.\n94.59 \n95.03 95.01 95.02 \n95.34 \n95.99 \n95.6 \n95.97 \n1k 5k 6k 7k \nAcc: % \nSPO triple amount \nK-XLNet-Base K-XLNet-Large \nFIGURE 4: Performance of K-XLNet models injected differ-\nent triple (knowledge) amounts.\nWe can see that from 1K to 5K, the performances of K-\nXLNet models are improving with the increase of knowledge\ninjection. After Performance that, the performance tends to\nbe stable or even slightly decreased. This gives us a hint that\nwhen using knowledge for model improvement, more is not\nbetter.\nIn the following experiments, we set the triple (knowledge)\namount to 5k for all K-XLNet models, and compare our K-\nXLNet to the original XLNet using the Large setting, since\nthe Base setting has similar performance trend.\nC. OPEN DOMAIN TASKS\nWe conduct six experiments to evaluate the performance of\nK-XLNet on open-domain tasks. Speciﬁcally, Book_review\n[10], Shopping [10] and Weibo [10] are single-sentence\nclassiﬁcation tasks. XNLI [25] and LCQMC [26] are two-\nsentence classiﬁcation tasks. MSRA-NER [27] is a Named\nEntity Recognition (NER) task.\n• Book_review7 contains 20,000 positive reviews and\n20,000 negative reviews collected from Douban.\n• Shopping8 is a online shopping review dataset that con-\ntains 40,000 reviews, including 21,111 positive reviews\nand 18,889 negative reviews.\n• Weibo9 is a dataset with emotional annotations from\nSina Weibo, including 60,000 positive samples and\n60,000 negative samples.\n• XNLI10 is a cross-language language understanding\ndataset in which each entry contains two sentences and\nthe task is to determine their relation (“Entailment”,\n“Contradict” or “Neutral”).\n• LCQMC11 is a large-scale Chinese question matching\ncorpus. The goal of this task is to determine if the two\nquestions have a similar intent.\n• MSRA-NER12 is a NER dataset published by Mi-\ncrosoft. It is to recognize the named entities in the\ntext, including person names, place names, organization\nnames, etc..\nSimilarly, the open-domain datasets are split into three\nparts: train, dev, and test, used for ﬁne-tuning, model se-\nlection and model test, respectively. Table 2 shows the test\nresults of various models in terms of accuracy. We can see\nthat K-XLNet performs better than XLNet consistently on\nthe six open-domain tasks. To be speciﬁc, the improvements\nare signiﬁcant on NER task, but not on sentence classiﬁcation\ntasks. Moreover, the model leveraging Hownet performs\nbetter than that using Cn-DBpedia on sentence classiﬁcation\ntasks, but It is the opposite on the NER task on the NER\ntask. The above observations show that knowledge injection\nto XLNet is also effective on open-domain downstream tasks,\nbut it is important to choose appropriate knowledge base\naccording to the speciﬁc task.\n7https://embedding˙githubio/evaluation\n8https://share ˙weiyun˙com/5xxYiig\n9https://share.weiyun.com/5lEsv0w\n10https://github.com/NLPchina/XNLI\n11https://github.com/Lizhengo/lcqmc_data\n12https://github.com/littleWhiteTre/msra_ner/\nVOLUME 4, 2016 5\nRuiqing Y anet al.: K-XLNet: A General Method for Combining Explicit Knowledge with Language Model Pretraining\nTABLE 2: Results of various models on various open-domain tasks (Accuracy%)\nModels\\Datasets Book review Shopping Weibo XNLI LCQMC MSRA-NER\nDev Test Dev Test Dev Test Dev Test Dev Test Dev Test\nXLNet 88.71 87.69 96.82 96.73 98.04 97.98 76.87 76.33 88.79 87.25 95.08 94.97\nK-XLNet (Hownet) 88.83 88.67 97.04 97.14 98.17 98.05 77.18 77.13 89.02 87.37 96.29 96.26\nK-XLNet (CN-DBpedia) 88.76 87.71 96.89 96.77 98.12 98.41 76.97 76.39 88.87 87.31 96.31 96.24\nV. CONCLUSION\nIn this paper, we propose a simple but general knowledge\ninjection method for pretraining language model. Taking the\nXLNet as a running example, we construct K-XLNet to\nshow the effectiveness of the proposed method. Extensive\nexperiments show that K-XLNet performs better than XLNet\nin both open domain and domain-speciﬁc tasks, and the\nimprovement on domain-speciﬁc task is more signiﬁcant than\nthat on open domain task. However, there is still much room\nfor improvement such as improving the interpretability of\nthe sentence generator, reducing the impact of knowledge\nnoise, and optimizing the role of structured information of\nknowledge.\nREFERENCES\n[1] M. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and\nL. Zettlemoyer, “Deep contextualized word representations,” in Proceed-\nings of the 2018 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language Technologies,\nV olume 1 (Long Papers), Jun. 2018, pp. 2227–2237.\n[2] A. Dai and Q. Le, “Semi-supervised sequence learning,” in In Proceedings\nof NIPS, 11 2015, p. 3079–3087.\n[3] S. Singh, erek Hoiem, and D. A. Forsyth, “Swapout: Learning an ensemble\nof deep architectures,” in Proceedings of 2016 Conference on Advances in\nNeural Information Processing Systems, 2016, pp. 28–36.\n[4] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[5] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V . Le,\n“Xlnet: Generalized autoregressive pretraining for language understand-\ning,” in Advances in neural information processing systems, 2019, pp.\n5754–5764.\n[6] K. C. Stevens, “The effect of background knowledge on the reading\ncomprehension of ninth graders,” Journal of Reading Behavior, vol. 12,\nno. 2, pp. 151–154, 1980.\n[7] F. Petroni, T. Rocktäschel, S. Riedel, P. Lewis, A. Bakhtin, Y . Wu, and\nA. Miller, “Language models as knowledge bases?” in Proceedings of the\n2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Process-\ning (EMNLP-IJCNLP), 2019, pp. 2463–2473.\n[8] Y . Sun, S. Wang, Y . Li, S. Feng, X. Chen, H. Zhang, X. Tian,\nD. Zhu, H. Tian, and H. Wu, “ERNIE: enhanced representation through\nknowledge integration,” CoRR, vol. abs/1904.09223, 2019. [Online].\nAvailable: http://arxiv.org/abs/1904.09223\n[9] Z. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun, and Q. Liu, “ERNIE: En-\nhanced language representation with informative entities,” in Proceedings\nof ACL 2019, 2019.\n[10] W. Liu, P. Zhou, Z. Zhao, Z. Wang, Q. Ju, H. Deng, and P. Wang, “K-bert:\nEnabling language representation with knowledge graph,” arXiv preprint\narXiv:1909.07606, 2019.\n[11] C. Rosset, C. Xiong, M. Phan, X. Song, P. Bennett, and S. Tiwary,\n“Knowledge-aware language model pretraining,” 2020.\n[12] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, “Distributed\nrepresentations of words and phrases and their compositionality,” in\nProceedings of the 26th International Conference on Neural Information\nProcessing Systems - V olume 2, 2013, p. 3111–3119.\n[13] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors\nfor word representation,” in Empirical Methods in Natural Language\nProcessing (EMNLP), 2014, pp. 1532–1543. [Online]. Available:\nhttp://www.aclweb.org/anthology/D14-1162\n[14] R. Zellers, Y . Bisk, R. Schwartz, and Y . Choi, “SW AG: A large-scale\nadversarial dataset for grounded commonsense inference,” in Proceedings\nof the 2018 Conference on Empirical Methods in Natural Language\nProcessing, 2018, pp. 93–104.\n[15] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman, “GLUE:\nA multi-task benchmark and analysis platform for natural language under-\nstanding,” in Proceedings of the 2018 EMNLP Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP, Nov. 2018, pp. 353–\n355.\n[16] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,\nH. Küttler, M. Lewis, W. tau Yih, T. Rocktäschel, S. Riedel, and\nD. Kiela, “Retrieval-augmented generation for knowledge-intensive nlp\ntasks,” 2020.\n[17] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,\nS. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws for neural\nlanguage models,” 2020.\n[18] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n“Language models are unsupervised multitask learners,” 2019.\n[19] Z. Fang, Y . Cao, R. Li, Z. Zhang, Y . Liu, and S. Wang, “High quality\ncandidate generation and sequential graph attention network for entity\nlinking,” in Proceedings of The Web Conference 2020, ser. WWW ’20,\n2020, p. 640–650.\n[20] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. u. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances in\nNeural Information Processing Systems 30, 2017, pp. 5998–6008.\n[21] Z. Dong, Q. Dong, and C. Hao, “Hownet and its computation of mean-\ning,” in COLING 2010, 23rd International Conference on Computational\nLinguistics, Demonstrations V olume, 23-27 August 2010, Beijing, China,\n2010, pp. 53–56.\n[22] B. Xu, Y . Xu, J. Liang, C. Xie, B. Liang, W. Cui, and Y . Xiao, “Cn-\ndbpedia: A never-ending chinese knowledge extraction system,” in 30th\nInternational Conference on Industrial Engineering and Other Applica-\ntions of Applied Intelligent Systems, IEA/AIE 2017, ser. Lecture Notes\nin Computer Science, S. Benferhat, K. Tabia, and M. Ali, Eds., vol. 10351,\n2017, pp. 428–438.\n[23] J. Lehmann, R. Isele, M. Jakob, A. Jentzsch, D. Kontokostas, P. N.\nMendes, S. Hellmann, M. Morsey, P. van Kleef, S. Auer, and C. Bizer,\n“Dbpedia – a large-scale, multilingual knowledge base extracted from\nwikipedia,” Semantic Web, vol. 6, no. 2, pp. 167–195, Jan. 2015.\n[24] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y . Ng, and C. Potts,\n“Learning word vectors for sentiment analysis,” in Proceedings of the\n49th Annual Meeting of the Association for Computational Linguistics:\nHuman Language Technologies. Portland, Oregon, USA: Association\nfor Computational Linguistics, Jun. 2011, pp. 142–150.\n[25] A. Conneau, R. Rinott, G. Lample, A. Williams, S. Bowman, H. Schwenk,\nand V . Stoyanov, “Xnli: Evaluating cross-lingual sentence representa-\ntions,” in Proceedings of the 2018 Conference on Empirical Methods in\nNatural Language Processing, 2018, pp. 2475–2485.\n[26] Y . Cao, L. Hou, J. Li, Z. Liu, C. Li, X. Chen, and T. Dong, “Joint\nrepresentation learning of cross-lingual words and entities via attentive\ndistant supervision,” in Proceedings of 2018 Conference on Empirical\nMethods in Natural Language Processing, 2018.\n[27] G.-A. Levow, “The third international Chinese language processing bake-\noff: Word segmentation and named entity recognition,” in Proceedings of\nthe Fifth SIGHAN Workshop on Chinese Language Processing. Associ-\nation for Computational Linguistics, 2006, pp. 108–117.\n6 VOLUME 4, 2016\nRuiqing Y anet al.: K-XLNet: A General Method for Combining Explicit Knowledge with Language Model Pretraining\nRUIQING YAN is currently a junior at Beijing\ninstitute of petrochemical technology in Beijing,\nChina. Majoring in computer science and tech-\nnolegy, He is very interested in deep learning\ntechnology and its applications in naturl language\nprocessing.\nLANCHANG SUN received his B.S. degree in\ncomputer science from Beijing institute of petro-\nchemical technology in 2018. He is currently pur-\nsuing a master’s degree at Beijing University of\nPosts and telecommunications in Beijing, China.\nHis research interests include machine learning\nand pattern recognition, especially the application\nof deep learning techniques in natural language\nprocessing and image recognition.\nFANG WANG graduated with B.S. degree and\nM.S. degree in computer science from Beijing\nTechnology and Business Universtiy in 2008 and\n2011, respectively. She received her Ph.D. degree\nin Computer Science from Beihang University in\n2017. After that, she works with Beijing Institute\nof Petrochemical Technology. Her research inter-\nests include data mining, knowledge engineering\nand natural language processing.\nXIAOMING ZHANGis working as a professor of\nBeijing Institute of Petrochemical Technology. He\ngraduated with B.S. degree from Zhejiang Univer-\nsity in 1989, received his M.S. and Ph.D. degree\nin Automation Engineering from Dalian Univer-\nsity of Technology in 1992 and 1996 respectively.\nHis current research interests include data mining,\nmachine learning and big data technology.\nVOLUME 4, 2016 7",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.4978148937225342
    },
    {
      "name": "Natural language processing",
      "score": 0.4325251579284668
    },
    {
      "name": "Language model",
      "score": 0.4127911329269409
    },
    {
      "name": "Psychology",
      "score": 0.40705233812332153
    },
    {
      "name": "Cognitive psychology",
      "score": 0.37959060072898865
    },
    {
      "name": "Linguistics",
      "score": 0.37793880701065063
    },
    {
      "name": "Philosophy",
      "score": 0.12285313010215759
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130541836",
      "name": "Beijing Institute of Petrochemical Technology",
      "country": "CN"
    }
  ],
  "cited_by": 3
}