{
  "title": "Pretrained Language Models for Document-Level Neural Machine Translation",
  "url": "https://openalex.org/W2987998469",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221540874",
      "name": "Li, Liangyou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2030371270",
      "name": "Jiang Xin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1952241286",
      "name": "Liu Qun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2799051177",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2962802109",
    "https://openalex.org/W2891534142",
    "https://openalex.org/W2184135559",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963842551",
    "https://openalex.org/W2964093087",
    "https://openalex.org/W2808508619",
    "https://openalex.org/W2555428947",
    "https://openalex.org/W2962712961"
  ],
  "abstract": "Previous work on document-level NMT usually focuses on limited contexts because of degraded performance on larger contexts. In this paper, we investigate on using large contexts with three main contributions: (1) Different from previous work which pertrained models on large-scale sentence-level parallel corpora, we use pretrained language models, specifically BERT, which are trained on monolingual documents; (2) We propose context manipulation methods to control the influence of large contexts, which lead to comparable results on systems using small and large contexts; (3) We introduce a multi-task training for regularization to avoid models overfitting our training corpora, which further improves our systems together with a deeper encoder. Experiments are conducted on the widely used IWSLT data sets with three language pairs, i.e., Chinese--English, French--English and Spanish--English. Results show that our systems are significantly better than three previously reported document-level systems.",
  "full_text": "Pretrained Language Models for Document-Level Neural Machine\nTranslation\nLiangyou Li and Xin Jiang and Qun Liu\nHuawei Noah’s Ark Lab\nliliangyou@huawei.com\nAbstract\nPrevious work on document-level NMT usu-\nally focuses on limited contexts because of\ndegraded performance on larger contexts. In\nthis paper, we investigate on using large con-\ntexts with three main contributions: (1) Dif-\nferent from previous work which pertrained\nmodels on large-scale sentence-level parallel\ncorpora, we use pretrained language models,\nspeciﬁcally BERT (Devlin et al., 2018), which\nare trained on monolingual documents; (2) We\npropose context manipulation methods to con-\ntrol the inﬂuence of large contexts, which lead\nto comparable results on systems using small\nand large contexts; (3) We introduce a multi-\ntask training for regularization to avoid models\noverﬁtting our training corpora, which further\nimproves our systems together with a deeper\nencoder. Experiments are conducted on the\nwidely used IWSLT data sets with three lan-\nguage pairs, i.e., Chinese–English, French–\nEnglish and Spanish–English. Results show\nthat our systems are signiﬁcantly better than\nthree previously reported document-level sys-\ntems.\n1 Introduction\nRecently, document-level Neural Machine Trans-\nlation (NMT) is drawing more attention from re-\nsearchers studying on incorporating contexts into\ntranslation models (Wang et al., 2017; Jean et al.,\n2017; Tiedemann and Scherrer, 2017; V oita et al.,\n2018; Zhang et al., 2018; Miculicich et al., 2018;\nKuang et al., 2018; Tu et al., 2018). It has\nbeen shown that nmt can be improved by taking\ndocument-level context information into consider-\nation. However, one of the common practices in\nprevious work is to only consider very limited con-\ntexts (e.g., two or three sentences) and therefore\nlong dependencies in documents are usually absent\nduring modeling the translation process. Although\nprevious work has shown that when increasing the\nlength of contexts, system performance would be\ndegraded, to the best of our knowledge, none of\nthem addresses the problem this work considers.\nGiven the importance of long-range dependen-\ncies (Dai et al., 2019), in this paper we investigate\napproaches to take large contexts (up to 512 words\nin our experiments) into consideration. Our model\nis based on the Transformer architecture (Vaswani\net al., 2017) and we propose methods to narrow the\nperformance gap between systems using different\nlengths of contexts. In summary, we make three\nmain contributions:\n•We use pretrained language models (PLMs)\nto initialize parameters of encoders. Differ-\nent from pretrained models on large-scale\nsentence-level parallel corpora (Tu et al.,\n2018; Zhang et al., 2018), PLMs are trained\non monolingual documents which are easier\nto obtain than bilingual corpora.\n•We propose methods to manipulate the inte-\ngration of context information to control the\ninﬂuence of large contexts. In our experi-\nments, these methods lead to comparable re-\nsults on systems using small and large con-\ntexts.\n•We introduce a multi-task training which adds\nan extra task on the encoder side regularizing\nour model and further improving our systems\ntogether with a deeper encoder.\nExperimental results on the widely used IWSLT\ndata sets (Cettolo et al., 2012) show that our\nﬁnal systems signiﬁcantly outperform systems\nin previous work on three language pairs, i.e.,\nChinese–English (Zh-En), French–English (Fr-En)\nand Spanish–English (Es-En). Our results also\ndemonstrate the necessity of PLMs and usefulness\nof the multi-task training and the context manipula-\ntion methods.\narXiv:1911.03110v1  [cs.CL]  8 Nov 2019\nFigure 1: The proposed encoder structure. An input\nsentence “ It likes ﬁsh ” is concatenated with its con-\ntexts “His cat is cute ”. A separation mark “ [SEP]”\nis inserted between them. Compared with the origi-\nnal Transformer architecture, we make the following\nchanges: (1) segment embeddings are added to distin-\nguish contexts from the input; (2)reversed position em-\nbeddings are introduced as an alternative of the original\nsequential position embeddings; (3) context masks are\nused during decoding to avoid attention weights on the\ncontexts. In this ﬁgure, each “ E∗” denotes an embed-\nding vector, while each “H∗” denotes an output vector\nof the encoder.\n2 Document-Level NMT\nIn this paper, we consider source contexts, i.e., sen-\ntences before the current input to be translated. Fol-\nlowing Tiedemann and Scherrer (2017), the current\ninput and its contexts are concatenated, as shown\nin Figure 1. Instead of deﬁning an additional hyper-\nparameter on the length of contexts (e.g., k sen-\ntences), we set the maximum total length of an\ninput and its contexts to 512 words, which is de-\nﬁned by the model capacity (i.e., the maximum\ninput length).\nHowever, incorporating such large contexts\ncould result in unstable training and introduces\nmuch irrelevant information. To alleviate these\nproblems, we (1) use PLMs trained on large-scale\nmonolingual documents to initialize parameters of\nencoders; (2) propose a few changes of the encoder\narchitecture to control impacts of contexts; and (3)\nintroduce a multi-task training mechanism to regu-\nlarize our model. Figure 1 shows the input format\nand encoder architecture we use in this paper.\n2.1 Pretrained Language Models\nBecause large-scale parallel corpora with document\nboundaries are usually unavailable, researchers\nhave tried to make use of sentence-level parallel\ncorpora to help training a document-level NMT\nmodel (Zhang et al., 2018; Tu et al., 2018). Dif-\nferent from them, we use large-scale monolingual\ndata which is much easier to obtain, e.g., from\nWikipedia or other public websites. Instead of train-\ning a language model from scratch on monolingual\ndocuments, we directly use pretrained BERT mod-\nels to initialize our encoder and then ﬁne-tune our\nNMT model on document-level parallel corpora.\nBERT is chosen based on the following reasons: (1)\nBERT is based on the Transformer architecture con-\nsidering bidirectional contexts which makes it com-\npatible with our encoder; (2) BERT is trained over\nlong sequences and learns relationships between\nthem, and thus is suitable to model document-level\ncontexts; (3) BERT codes and multilingual mod-\nels are publicly available, which makes it easier to\nreplicate our results.\n2.2 Context Manipulation\nUnlike previous work which uses additional compo-\nnents, such as context encoders or attention layers,\nto encode and integrate contexts, in this paper we\nuse a single encoder on the concatenation of an\ninput and its contexts. Although a model can be\ndirectly trained without any modiﬁcation (Tiede-\nmann and Scherrer, 2017), we found it does not\nwork well when large contexts are used especially\nwithout PLMs. We presume this is because large\ncontexts introduce much more irrelevant informa-\ntion which would overwhelm the source sentence\nto be translated. To alleviate this problem, we intro-\nduce three techniques into the encoder to explicitly\nmake a distinction between contexts and the input.\n2.2.1 Segment Embeddings\nThe concept of segment embeddings is introduced\nin BERT. The basic idea is that each sequence has\na unique embedding so as to distinguish from other\nsequences. In this paper, we directly adapt the\nidea into the encoder and add different segment\nembeddings to contexts and the source sentence,\nrespectively.\n2.2.2 Reversed Position Embeddings\nBy default, position embeddings in the Transformer\nare assigned words by words. However, when con-\ntexts are concatenated to an input sentence, posi-\ntion embeddings of the source input will depend\non length of the contexts which precede the source.\nTo alleviate this, we propose to ﬁrst assign posi-\ntion embeddings to the source input and then to its\ncontexts, called reversed position embeddings, as\nillustrated in Figure 1, which keeps the positional\nrepresentations of source sentences stable.\nTo alleviate this problem, we propose to ﬁrst as-\nsign position embeddings to the source input and\nthen to its contexts, called reversed position em-\nbeddings, as illustrated in Figure 1, which keeps\nthe positional representations of source sentences\nstable.\n2.2.3 Context Masks\nTiedemann and Scherrer (2017) has shown that\ndirectly augmenting an input with its contexts im-\nproves translation quality in RNN-based models.\nHowever, they only use the immediately previous\nsentence as contexts in experiments. When larger\ncontexts are used, this kind of method does not\nwork well because large contexts could result in\nunstable training and it would be more challeng-\ning for the model to learn appropriate attention\nweights to distinguish contexts and inputs. There-\nfore, in this paper we add context masks to avoid\nthe decoder attending to the contexts as we pre-\nsume representations of the source part are already\ncontext-aware through the underlying self-attention\nin the encoder.\n2.3 Multi-task Training\nInspired by Ramachandran et al. (2017), we intro-\nduce an extra task on the encoder side to avoid the\nmodel overﬁtting our training corpus. The task we\nuse is called masked language model (MLM) pre-\ndiction which is also used to train BERT. When\nMLM is considered, the training objective be-\ncomes:\nˆθ= argmax\nθ\n∑\n(X,C,Y)∈D\nlog P(Y|S)\n+\n∑\nk∈M\nlog P(sk|S),\nwhere S is the extended input by combining the\ninput X and context C but with some words ran-\ndomly masked (about 16% with a maximum num-\nber of 20 words), M is the set of masked positions,\nsk is the real word form at position k.\n3 Experiments\n3.1 Data sets and Settings\nWe conduct experiments on the widely used IWSLT\ndata sets with three language pairs: Zh-En, Fr-En\nand Es-En, each of which contains around 0.2M\nsentence pairs. We use dev2010 for development.\ntst2010-2013 (Zh-En), tst2010 (Fr-En) and tst2010-\n2012 (Es-En) are used for testing.\nThe size of our baseline NMT model follows\nthat of BERT-base models. We train models up to\n300K steps with each batch around 3072 source\nor target tokens. Adam (Kingma and Ba, 2015) is\nused to optimize parameters with the same learning\nrate as the original Transformer. We directly use\npretrained Chinese and multilingual BERT models1\nto initialize encoders for Zh, Fr and Es, respectively.\nBeam search is used with a beam width of 4 and a\nlength penalty (Wu et al., 2016) of 1.\n3.2 Overall Results\nIn this section, we compare our document-level\nNMT systems with three existing approaches (Tu\net al., 2018; Miculicich et al., 2018; Zhang et al.,\n2018). To ensure a fair comparison, all systems\nare based on the Transformer architecture, and our\ntranslations are processed and evaluated following\nthese papers as well.\nTable 1 shows the overall evaluation results on\ntest sets in all three language pairs. Our sys-\ntems with BERT and context manipulation methods\nachieve signiﬁcantly better BLEU scores than pre-\nvious work. Speciﬁcally, gains on Zh-En, Fr-En\nand Es-En are 2.80 BLEU, 2.06 BLEU and 2.84\nBLEU, respectively. We also found using a deeper\n(12 layers) encoder improves systems compared\nto a shallower (6 layers) encoder (by up to 0.49\nBLEU on Zh-En, 0.64 BLEU on Fr-En, and 0.55\nBLEU on Es-En, respectively). When we introduce\nthe MLM task into the deep model, the systems is\nfurther improved.\n3.3 Ablation Study\nDespite the overall improvements shown in Table 1,\nit would be interesting to know the contribution of\neach method we applied. Therefore, in this section,\nwe conduct ablation study with results shown in\nTable 2. We found that simply using the BERT\nto initialize parameters of the encoder improves\nthe baseline system by 1.04 BLEU. When contexts\nare concatenated with source sentences which are\nthen directly taken as inputs of the model without\nany changes in the network structure, the system\n(i.e., the one with “ +Large Context”) is further\nimproved by 1.31 BLEU. Finally, when the three\ncontext manipulation methods are integrated, the\nsystem achieves the best BLEU score.\n1https://github.com/google-research/\nbert\nTable 1: BLEU scores and increment over the best previous approach on three language pairs. “ +Large Context\nManipulation” denotes the three context manipulation methods on large contexts. “ +Encoder-12” means to in-\ncrease the number of encoder layers to 12. “ +MLM” means adding the MLM prediction task into the training\nobjective. The best BLEU scores are in bold.\nSystems Zh–En Fr–En Es–En\nprevious\n(Tu et al., 2018) 17.32 - 36.46\n(Miculicich et al., 2018) 17.79 - 37.24\n(Zhang et al., 2018) - 36.04 -\nour work\nBaseline 17.31 35.33 37.01\n+BERT +Large Context Manipulation 20.10 (+2.31) 37.46 (+1.42) 39.53 (+2.29)\n+Encoder-12L 20.59 (+2.80) 38.10 (+2.06) 40.08 (+2.84)\n+MLM 20.72 (+2.93) 38.76 (+2.72) 40.31 (+3.07)\nTable 2: BLEU scores and increment over the base-\nline on dev2010 in ablation study. “ +Large Context”\nmeans large contexts are simply concatenated with in-\nputs following Tiedemann and Scherrer (2017). Ctx-\nMask, SegEmb and RevPos denote Context Masks, Seg-\nment Embeddings and Reverse Position Embeddings,\nrespectively.\nSystems Zh–En\nBaseline 12.19\n+BERT 13.23 (+1.04)\n+Large Context 14.54 (+2.35)\n+CtxMask 14.87 (+2.68)\n+SegEmb 15.00 (+2.81)\n+RevPos 15.30 (+3.11)\n3.4 Context Length\nTable 3 shows results of varying context length. We\nfound that NMT models with (especially large) con-\ntexts considered do not work well without pretrain-\ning. When parameters are initialized with BERT,\nboth small and large contexts bring signiﬁcant im-\nprovements even without using the three manipu-\nlation methods. This suggests the importance of\npretraining when document-level parallel corpora\nare in small-scale and is consistent with ﬁndings in\nprevious work (Tu et al., 2018; Zhang et al., 2018).\nHowever, a difference is that they pretrained mod-\nels on sentence-level parallel corpora, which we\nthink could help to further improve our systems.\nAnother ﬁnding is that using smaller contexts\nachieves a signiﬁcantly better BLEU score (+1.0)\nthan larger contexts, similar to Zhang et al. (2018);\nMiculicich et al. (2018). However, when our ma-\nnipulation methods are applied, the system with\nlarge contexts is further improved resulting a nar-\nrowed gap (0.2 BLEU difference) between it and\nTable 3: BLEU scores and increment over the baseline\non dev2010 when context length is varied. Small con-\ntext denotes the immediately previous sentence. +Ma-\nnipulation means the three context manipulation tech-\nniques.\nSystems Zh–En\nBaseline 12.19\n+Small Context 12.29 (+0.1)\n+Large Context Diverge\n+BERT 13.23 (+1.04)\n+Small Context 15.54 (+3.35)\n+Large Context 14.54 (+2.35)\n+BERT +Manipulation -\n+Small Context 15.50 (+3.31)\n+Large Context 15.30 (+3.11)\nthe system using small contexts. We also found\nthat our manipulation methods does not improve\nthe system with small contexts. This is expected\nsince they are designed for controlling the inﬂu-\nence of large contexts. Our results suggest that so-\nphisticated manipulation on the integration of large\ncontexts is necessary and promising to achieve a\nbetter performance.\n4 Conclusion\nIn this paper, we investigate document-level NMT\nusing large contexts. We (1) use pretrained lan-\nguage models, i.e. BERT, to initialize the encoder;\n(2) propose manipulation methods to control the\ninﬂuence of large contexts; and (3) introduce a\nmulti-task training mechanism for model regular-\nization. Experiments on IWSLT data sets showed\nthat our systems achieved the best BLEU scores\ncompared with previous work on Chinese–English,\nFrench–English and Spanish–English.\nReferences\nMauro Cettolo, Christian Girardi, and Marcello Fed-\nerico. 2012. Wit3: Web inventory of transcribed and\ntranslated talks. In Proceedings of the 16th Confer-\nence of the European Association for Machine Trans-\nlation (EAMT), pages 261––268, Trento, Italy.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. CoRR, abs/1810.04805.\nSebastien Jean, Stanislas Lauly, Orhan Firat, and\nKyunghyun Cho. 2017. Does neural machine\ntranslation beneﬁt from larger context? CoRR,\nabs/1704.05135.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nShaohui Kuang, Deyi Xiong, Weihua Luo, and\nGuodong Zhou. 2018. Modeling coherence for\nneural machine translation with dynamic and topic\ncaches. In Proceedings of the 27th International\nConference on Computational Linguistics , pages\n596–606, Santa Fe, New Mexico, USA. Association\nfor Computational Linguistics.\nLesly Miculicich, Dhananjay Ram, Nikolaos Pappas,\nand James Henderson. 2018. Document-level neu-\nral machine translation with hierarchical attention\nnetworks. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning, pages 2947–2954, Brussels, Belgium. Associa-\ntion for Computational Linguistics.\nPrajit Ramachandran, Peter Liu, and Quoc Le. 2017.\nUnsupervised pretraining for sequence to sequence\nlearning. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 383–391, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nJ¨org Tiedemann and Yves Scherrer. 2017. Neural ma-\nchine translation with extended context. In Proceed-\nings of the Third Workshop on Discourse in Machine\nTranslation, pages 82–92, Copenhagen, Denmark.\nAssociation for Computational Linguistics.\nZhaopeng Tu, Yang Liu, Shuming Shi, and Tong Zhang.\n2018. Learning to remember translation history with\na continuous cache. Transactions of the Association\nfor Computational Linguistics, 6:407–420.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nElena V oita, Pavel Serdyukov, Rico Sennrich, and Ivan\nTitov. 2018. Context-aware neural machine trans-\nlation learns anaphora resolution. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1264–1274, Melbourne, Australia. Associa-\ntion for Computational Linguistics.\nLongyue Wang, Zhaopeng Tu, Andy Way, and Qun Liu.\n2017. Exploiting cross-sentence context for neural\nmachine translation. In Proceedings of the 2017\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2826–2831, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation. CoRR, abs/1609.08144.\nJiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei\nZhai, Jingfang Xu, Min Zhang, and Yang Liu. 2018.\nImproving the transformer translation model with\ndocument-level context. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing , pages 533–542, Brussels, Bel-\ngium. Association for Computational Linguistics.",
  "topic": "Machine translation",
  "concepts": [
    {
      "name": "Machine translation",
      "score": 0.809839129447937
    },
    {
      "name": "Computer science",
      "score": 0.7328077554702759
    },
    {
      "name": "Natural language processing",
      "score": 0.681546688079834
    },
    {
      "name": "Translation (biology)",
      "score": 0.6731480360031128
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5625938177108765
    },
    {
      "name": "Language model",
      "score": 0.5268532037734985
    },
    {
      "name": "Chemistry",
      "score": 0.0570182204246521
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ]
}