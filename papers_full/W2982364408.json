{
  "title": "Multilingual Language Models for Named Entity Recognition in German and English",
  "url": "https://openalex.org/W2982364408",
  "year": 2019,
  "authors": [
    {
      "id": null,
      "name": "Trinity College Dublin, Dublin 2, Ireland",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2982399051",
      "name": "Antonia Baumann",
      "affiliations": [
        "Trinity College Dublin"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2144578941",
    "https://openalex.org/W2883818012",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2795247881",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2964054038",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2523246573",
    "https://openalex.org/W2952729433",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2880875857",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W2890225082",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2799108347",
    "https://openalex.org/W3216234416",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2170973209"
  ],
  "abstract": "We assess the language specificity of recent language models by exploring the potential of a multilingual language model.In particular, we evaluate Google's multilingual BERT (mBERT) model on Named Entity Recognition (NER) in German and English.We expand the work on language model fine-tuning by Howard and Ruder (2018), applying it to the BERT architecture.We successfully reproduce the NER results published by Devlin et al. (2019).Our results show that the multilingual language model generalises well for NER in the chosen languages, matching the native model in English and comparing well with recent approaches for German.However, it does not benefit from the added finetuning methods.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8704603910446167
    },
    {
      "name": "Named-entity recognition",
      "score": 0.8500627279281616
    },
    {
      "name": "German",
      "score": 0.7902578115463257
    },
    {
      "name": "Natural language processing",
      "score": 0.7396948337554932
    },
    {
      "name": "Language model",
      "score": 0.7056067585945129
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6536994576454163
    },
    {
      "name": "Entity linking",
      "score": 0.5804142951965332
    },
    {
      "name": "Linguistics",
      "score": 0.4670608937740326
    },
    {
      "name": "Architecture",
      "score": 0.43891435861587524
    },
    {
      "name": "History",
      "score": 0.08752262592315674
    },
    {
      "name": "Task (project management)",
      "score": 0.08538705110549927
    },
    {
      "name": "Knowledge base",
      "score": 0.06787422299385071
    },
    {
      "name": "Engineering",
      "score": 0.05488866567611694
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ]
}