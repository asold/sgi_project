{
  "title": "Prompting Language Models for Linguistic Structure",
  "url": "https://openalex.org/W4385572404",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2516232129",
      "name": "Terra Blevins",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2671828835",
      "name": "Hila Gonen",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A334758317",
      "name": "Luke Zettlemoyer",
      "affiliations": [
        "University of Washington"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3037116584",
    "https://openalex.org/W3213241618",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3037636427",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2740840489",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2773956126",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4224247062",
    "https://openalex.org/W4304697835",
    "https://openalex.org/W2287225235",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W4385573907",
    "https://openalex.org/W4293060592",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4221151371",
    "https://openalex.org/W3176489198",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W2299976354",
    "https://openalex.org/W4285225959",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W4303648904",
    "https://openalex.org/W4285247752",
    "https://openalex.org/W2963651521",
    "https://openalex.org/W3018647120",
    "https://openalex.org/W4389520103",
    "https://openalex.org/W4226155321",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4297234531",
    "https://openalex.org/W4293043115",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W2951299559",
    "https://openalex.org/W2250263931",
    "https://openalex.org/W4285107714",
    "https://openalex.org/W2970476646"
  ],
  "abstract": "Although pretrained language models (PLMs) can be prompted to perform a wide range of language tasks, it remains an open question how much this ability comes from generalizable linguistic understanding versus surface-level lexical patterns. To test this, we present a structured prompting approach for linguistic structured prediction tasks, allowing us to perform zero- and few-shot sequence tagging with autoregressive PLMs. We evaluate this approach on part-of-speech tagging, named entity recognition, and sentence chunking, demonstrating strong few-shot performance in all cases. We also find that while PLMs contain significant prior knowledge of task labels due to task leakage into the pretraining corpus, structured prompting can also retrieve linguistic structure with arbitrary labels. These findings indicate that the in-context learning ability and linguistic knowledge of PLMs generalizes beyond memorization of their training data.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 6649–6663\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nPrompting Language Models for Linguistic Structure\nTerra Blevins Hila Gonen Luke Zettlemoyer\nPaul G. Allen School of Computer Science & Engineering,\nUniversity of Washington\n{blvns, lsz}@cs.washington.edu\nhilagnn@gmail.com\nAbstract\nAlthough pretrained language models (PLMs)\ncan be prompted to perform a wide range of lan-\nguage tasks, it remains an open question how\nmuch this ability comes from generalizable lin-\nguistic understanding versus surface-level lex-\nical patterns. To test this, we present a struc-\ntured prompting approach for linguistic struc-\ntured prediction tasks, allowing us to perform\nzero- and few-shot sequence tagging with au-\ntoregressive PLMs. We evaluate this approach\non part-of-speech tagging, named entity recog-\nnition, and sentence chunking, demonstrating\nstrong few-shot performance in all cases. We\nalso find that while PLMs contain significant\nprior knowledge of task labels due to task\nleakage into the pretraining corpus, structured\nprompting can also retrieve linguistic structure\nwith arbitrary labels. These findings indicate\nthat the in-context learning ability and linguis-\ntic knowledge of PLMs generalizes beyond\nmemorization of their training data.\n1 Introduction\nThe rapid increase in the scale of pretrained lan-\nguage models (PLMs) has led to a new paradigm of\nNLP modeling: in-context learning, or prompting\n(e.g., Brown et al., 2020; Raffel et al., 2020). In\nthis setting, the model is used to perform a task\ndirectly via the predictions of the LM head with-\nout additional finetuning on the target task, often\nwith a few demonstrations of the desired behavior\nprovided within the input. This setup has led to\nimpressive few-shot performance on various tasks\nranging from classification to summarization and\ngeneration (Liu et al., 2021a).\nDue to their broad success on tasks requiring\nlanguage understanding, we hypothesize that these\nmodels also contain significant linguistic knowl-\nedge. However, we are not aware of existing\nprompting methods that can directly test this hy-\npothesis on autoregressive PLMs. Behavioral anal-\nysis of PLMs (Belinkov et al., 2020) uses meth-\nC: Alright what is this ? \nT: Alright_INTJ what_PRON is_AUX this_PRON ?_PUNCT\nC: I love this color .\nT: I_PRON love_VERB this_DET color_NOUN ._PUNCT\ndemonstration\nC: It took a while .\nT: It_PRON took_VERB a_\nPLM\nexampleNOUN\nDET\nVERB…\nFigure 1: Sequence tagging via structured prompting.\nEach predicted label is appended to the context along\nwith the next word to iteratively tag the full sentence.\nods similar to prompting to measure knowledge\nstored in language models (Gulordava et al., 2018;\nPetroni et al., 2019), but this technique is difficult\nto generalize to tasks that predict more complex\nstructures. Additionally, current approaches for\napplying PLMs to linguistic structured prediction\ntasks finetune on the downstream task (e.g., Ma\net al., 2022), which confounds measuring underly-\ning model knowledge.\nWe propose a new approach, structured prompt-\ning, that iteratively prompts autoregressive PLMs\nto probe for word- and span-level linguistics framed\nas sequence tagging tasks (Section 2). At timestep\nt, a label for the t-th word in the sequence is de-\ncoded from the LM; the model prediction is then\nfed back into the model along with the next word\nto progress to timestep t+ 1. We evaluate our\napproach on three sequence tagging tasks: POS\ntagging, sentence chunking, and NER. Our experi-\nments show that PLMs can perform effective few-\nshot sequence tagging in the structured prompt-\ning setup, and that performance increases with the\ndemonstration set size and model size, consistent\nwith other prompting methods (Section 4).\nWe further analyze structured prompting by ex-\n6649\namining how the model generalizes to various rep-\nresentations for labels (Section 5) as well as by\nanalyzing the presence of task data in the pretrain-\ning corpus and how this affects model performance\n(Section 6). These experiments show that struc-\ntured prompting can recover linguistic information\nfrom the model without using standard task labels,\nindicating that PLMs contain this knowledge in a\ngeneral manner beyond memorization of the task\nfrom pretraining data. Interestingly, while PLMs\nperform best with meaningful labels (such as origi-\nnal task labels or full class names in English), the\nmodel can also in-context learn from arbitrary la-\nbels. Additionally, the model exhibits strong prior\nknowledge of the task labels’ mapping onto the\nunderlying classes, likely due to the prevalence of\ntask data in the pretraining corpus.\nThe contributions of this work are therefore\nthreefold: (1) we introduce a new paradigm, struc-\ntured prompting, that probes PLMs for sequence\nknowledge without further training, (2) we find that\nthis approach recovers linguistic structure from\nPLMs in a few-shot manner, and (3) we present\nan analysis to quantify the effect of label form\nand pretraining data on in-context learning perfor-\nmance. Overall, our findings provide insight into\nboth the linguistic generalizations learned by PLMs\nand how in-context learning works in general.\n2 Structured Prompting of Pretrained\nLanguage Models\nWe propose a sequential method for performing se-\nquence tagging with PLMs via in-context learning,\nwhich we refer to as structured prompting (Figure\n1). The model is given k(context, tagged sequence)\npairs as the task demonstration and the example\nsentence to be labeled. The model then iteratively\ntags the words in the example with constrained\ndecoding over a fixed set of labels.\nMore specifically, given a set of labels Land an\ninput sequence ccontaining kdemonstration pairs\nas well as the full text of the example sentence\nS = s0,...,s n, at each time step t the language\nmodel M encodes [c; st] and labels st with ˆℓt =\nargmax\nℓ∈L\nPM (ℓ|c,st). We then update the input se-\nquence by appending the current word st and the\npredicted label ˆℓt to the end ofc. Multi-token labels\nare scored with the average log-likelihood over all\ntokens PM (ℓ|c) = 1\n|ℓ|\n∑|ℓ|\ni=0 PM (yi|c,y0,...,y i−1),\nwhere yj is the jth subword token in ℓ.\nThis approach to in-context learning tags an en-\ntire sequence with a single pass over the context.\nIt also allows the model to condition on past pre-\ndictions while labeling the current word. As we\ndemonstrate in Section 4, these features allow us\nto apply large autoregressive language models to a\nbroad class of core NLP tasks in a few-shot manner.\n3 Experimental Setup\n3.1 Prompt Formatting\nWe use a lightweight prompt format with limited\nnatural language guidance about the task provided\nto the model as shown in Figure 1; the letters “C”\nand “T” in the figure represent the inputs “Con-\ntext” and “Tagged” respectively. For each task, we\nrepresent each tag with the token or sequence of\ntokens corresponding to the surface form of the\nlabel provided by the dataset.\nIn general, our preliminary experiments with\nvaried prompt formats had little effect on perfor-\nmance. Specifically, performance was stable across\nthe choice of delimiter and other minor formatting\ndifferences. However, we note that including the\nword in the “Tagged” sequence is important; on\nGPT-J, performance degrades by 84% on POS and\n79% on NER when decoding the label sequence\nwithout repeating the word (i.e., “Tagged: DET\nNOUN...”).\n3.2 Sequence Tagging Tasks\nWe consider the following English tasks framed\nas sequence tagging problems in evaluating the\nproposed structured prompting method. For tasks\ninvolving tagging spans of text, we label each token\nin the span using theBIO label format: given a span\nof m tokens labeled ℓ, the first token is labeled as\nthe beginning of the span with “B-ℓ”, the remaining\nm-1 tokens are labeled as inside the span with “I-ℓ”,\nand tokens not included in the span are labeled as\noutside the span or “O”).\nPart-of-Speech (POS) Tagging We evaluate\nPOS tagging performance on English Universal\nDependencies (UD) with the UPOS tagset (Nivre\net al., 2020). Specifically, we use the treebank an-\nnotated on the GUM corpus (Zeldes, 2017).\nSentence Chunking Chunking, or shallow pars-\ning, partitions the words in a sentence into non-\noverlapping spans of syntactic meaning. We eval-\nuate PLMs on chunking with the CONLL2000\ndataset from Sang and Buchholz (2000), which\nframes chunking as a BIO tagging task.\n6650\nPOS NER Chunking\nT ask\n0\n20\n40\n60\n80Acc./F1\nMajority\nPer Word Maj.\nGPT-NeoX\n(a)\n0.0 0.5 1.0 1.5 2.0\nModel Size 1e10\n20\n40\n60\n80Acc./F1\nPOS\nNER\nChunking (b)\n0 2 4 6 8 10\nK\n0\n20\n40\n60\n80Acc./F1\nPOS\nNER\nChunking (c)\nFigure 2: Results of the structured prompting evaluation. POS is evaluated on accuracy; the other tasks are evaluated\nwith F1. (a) Results of GPT-NeoX (20B parameters) compared to task baselines. (b) Performance across different\nmodel sizes. (c) Performance of GPT-NeoX across different quantities of kdemonstrations.\nNamed Entity Recognition (NER) We evaluate\nthe ability of structured prompting to extract named\nentities from PLMs with NER. This is measured\nas a BIO tagging task on the CONLL2003 dataset\n(Sang and De Meulder, 2003).\n3.3 Models\nWe report performance on seven language models,\nranging from 125 million to 175 billion parameters.\nGPT-Neo This set of PLMs contains models\ntrained on the Pile (Gao et al., 2020) that from\n125 million to 2.7 billion parameters (Gao et al.,\n2020), 6.7 billion parameters (Wang and Komat-\nsuzaki, 2021), and 20 billion parameters (Black\net al., 2022). We use the GPT-Neo models avail-\nable through Huggingface (Wolf et al., 2019).\nGPT-3 We also perform structured prompting\nwith the GPT-3 models (Brown et al., 2020) via\nthe OpenAI API. We use the base GPT-Curie\n(∼6B parameters) and GPT-Davinci ( ∼175B\nparameters) models that have undergone no\nadditional instruction finetuning on POS tagging.\nDue to the cost of running these models through\nthe API, we generate the GPT-Davinci output\nwith unconstrained top-1 sampling rather than the\nconstrained decoding setup described in Section 2.\nIn preliminary experiments, we also tested struc-\ntured prompting on several OPT models (Zhang\net al., 2022). We found their performance was\nsignificantly worse and did not scale with model\nsize (up to 66B parameters) on POS tagging and\nNER. We leave a more thorough examination of\nthis behavior discrepancy for future work.\n3.4 Additional Experimental Details\nWe report the mean and standard error across m\nruns for each experiment. For each of these runs,\nk demonstrations are sampled from the training\ndataset at random, with the condition that the k\ndemonstrations cover the label space of the task\nif possible. We use k = 10 sentences as demon-\nstrations and perform m= 5 runs per experiment\nunless otherwise stated.\nEach model is evaluated on 1000 examples ran-\ndomly sampled from the task test set (see Appendix\nA.1 for a discussion on how this choice affects per-\nformance estimates). The evaluation subset is held\nfixed across all five runs, and the evaluation data\nand selection of demonstrations for each run are\nfixed across models for each task.\nTo obtain the tag sequence for each example, we\ngreedily take the top-1 label (with the highest log\nlikelihood) for each word. We also enforce hard\nconstraints for the span-labeling tasks involving\nBIO tagging (chunking, NER) to ensure a valid\nBIO tag sequence (e.g., I-X tags can only follow a\nprevious B-X or I-X tag). Empirically, we find that\nenforcing BIO constraints makes little difference\nin the method’s overall performance; however, we\nuse them as they ensure valid output sequences.\nAppendix A.2 compares model performance with\nand without BIO constraints.\n4 Structured Prompting Results\nWe measure the performance of structured prompt-\ning on three sequence tagging tasks. This evalua-\ntion aims to (1) validate that structured prompting\nfollows prior prompting setups in terms of model\nand k-shot scaling trends and (2) investigate the\nextent to which the approach extracts these struc-\n6651\n0.0 0.5 1.0 1.5 2.0\nModel Size 1e10\n60\n65\n70\n75\n80\n85Acc.\nPOS T agging\n(a)\nTrue\nPredicted\nPOS (20B) (b)\nTrue\nPredicted\nNER (20B) (c)\nFigure 3: Error analysis of structured prompting for GPT-Neo series. (a) POS performance on different sets of\n10-shot demonstrations. (b) Confusion matrix for GPT-NeoX on NER and (c) POS tagging, aggregated across runs.\ntures from the model. We then quantify the types\nof errors made with structured prompting.\n4.1 Overall Results\nFigure 2 presents the results of our primary struc-\ntured prompting evaluation. We consider the perfor-\nmance of GPT-NeoX (Black et al., 2022) compared\nto task baselines: overall majority, in which each\nword is labeled with the most frequent tag in the\ntraining set, and per-word majority, where each\nword is labeled with the tag it most commonly ap-\npeared within the training data (left panel). 1 All\nbaselines are calculated on the full training set and\nso use more labeled data than the PLM; the per-\nword majority is a particularly strong baseline as\nwords frequently occur with the same tag.\nStructured prompting performs effective few-\nshot sequence tagging We find that GPT-NeoX\nsignificantly outperforms each baseline on POS\ntagging and NER, and the model slightly underper-\nforms the per-word majority baseline on sentence\nchunking by 4.2 points. Overall, the approach per-\nforms worse for the BIO span-labeling tasks than\nfor word-level POS tagging. We hypothesize that\nthe former tasks are more complex, as they require\nthe model to determine spans and more detailed\nlinguistic knowledge.\nStructured prompting scales with model and\ndemonstration size We observe that the perfor-\nmance of structured prompting improves with scale\nacross GPT-Neo models (center panel). Model per-\nformance also improves with additional demonstra-\ntions (right panel); both of these trends are con-\n1For BIO tasks, the majority labels correspond to “O”\n(NER) and “I-NP” (chunking). The CONLL evaluation script\nonly scores labeled spans, giving an overall majority F1 of 0.\nSize Model k Acc. SE\n∼6B GPT-J∗ 5 79.01 2.95\nGPT-Curie 5 66.27 0.46\n∼175B GPT-Davinci† 5 59.65 2.84\nGPT-Davinci† 10 65.90 1.34\nTable 1: Structured Prompting results on POS tagging\nfor GPT-Curie and GPT-Davinci. SE is standard error.\n∗: model from GPT-Neo series of a similar size to Curie;\n†: evaluated with greedy unconstrained decoding.\nsistent with prior prompting results (e.g., Black\net al., 2022). However, the extent to which addi-\ntional demonstrations help varies: NER improves\nmore with larger sizes of kthan POS and chunk-\ning, likely because labeled spans are more sparse\nin NER. Notably, in the zero-shot case the model\nachieves around 17% accuracy on POS tagging\nwhen randomly predicting labels would yield 5.8%.\nStructured prompting with GPT-3 Table 1\ncompares two GPT-3 models to the GPT-Neo series\non POS tagging.2 We first compare the 6B param-\neter GPT-Curie (Gao, 2021) to the similarly sized\nGPT-J model in a 5-shot setting. We find that GPT-\nCurie underperforms GPT-J by 12.7 points; both\nmodels also underperform the per-word majority\nbaseline in this setting.\nWe then evaluate the largest GPT-3 model,\nGPT-Davinci, on POS tagging with greedy uncon-\nstrained decoding of the entire output sequence.\nDavinci performs reasonably well and scores simi-\nlarly to Curie despite the more difficult decoding\nsetting; many errors arise from format errors in the\ngenerated output for longer sentences. If we only\n2Each experiment reported in this section is repeated across\nthree runs rather than five.\n6652\nevaluate examples that occur prior to these format\nerrors, performance on that subset of the evaluation\ndata is 72.85 ± 1.3 at k=5 and 78.04 ± 0.8 at k=10.\n4.2 Error Analysis\nFigure 3 presents an error analysis of structured\nprompting; complete analyses for other tasks are\nprovided in Appendix A.3. We first break out per-\nformance across runs and evaluate how the choice\nof in-context examples affects performance (left\npanel). For POS tagging, the choice of demonstra-\ntions makes a difference, with some sets perform-\ning better than others across models and a perfor-\nmance gap of 4.8 accuracy points between the best\nand worst run on the 20B parameter model. NER\nexhibits similar results to POS; however, chunk-\ning performance of different demonstration sets is\nmuch more varied and inconsistent across models.\nNext, we examine common error types in struc-\ntured prompting with confusion matrices (center\nand right panel). We zero out the diagonal (rep-\nresenting correct predictions) and normalize the\nmatrices for clarity. Many of the mistakes made\nby the 20B parameter model on POS tagging are\nfor syntactically similar roles, such as confusing\nproper nouns for nouns and labeling auxiliary verbs\nas verbs. However, for BIO tagging the models are\nnot always well-calibrated: on NER, the model\nmost often mislabels “O” tokens, indicating that\nthe model overpredicts named entities.\nGiven that the choice of demonstrations affects\nPLM performance, another consideration is: how\nconsistent are the error types across runs? To\ninvestigate this, we calculate the pairwise Spear-\nman correlations between the confusion matrices\nof each run. These correlations are very high for\nthe 20B parameter model, indicating the model\nmakes similar types of error across runs: on aver-\nage ρ= 0.77 for POS tagging, 0.83 for NER, and\n0.88 for chunking; all pairwise correlations have\np-values <<0.001. Additionally, the models seem\nto become more robust across demonstration sets\nat scale; confusion matrix correlations for the 2.7B\nmodel are lower ( ρ = 0 .71,0.64,0.66 for POS,\nNER, and chunking, respectively).\n5 When Does Structured Prompting\nWork?\nWe now investigate how structured prompting sur-\nfaces linguistic structure from PLMs, using the\nbehavior of GPT-NeoX on POS tagging and NER\nFigure 4: Performance of GPT-NeoX when the labels\nare seen vs. not seen in the demonstration. Partial labels\nare those only seen as B-<label> tags in BIO tagging.\nas a case study. We find that (1) in some cases, the\nmodel generalizes to labels not seen in the demon-\nstration, and (2) the label form has a large effect on\nperformance. Specifically, the model can learn in\ncontext when arbitrary labels represent classes but\nwill ignore label mappings in the demonstration\nthat contradict its prior task knowledge.\n5.1 Effect of Seen Labels\nIn Section 4.1, we see that the model obtains above\nrandom chance accuracy on zero-shot POS tagging,\nsuggesting that the model does not need to observe\nthe label to associate it with the correct class. To\nanalyze this, we compare the model’s performance\nwhen the label is and is not seen in the demonstra-\ntion, averaged across k-shot runs.\nModel performance on unseen tags, and the\ngain in performance after observing the tag, varies\ngreatly by label class (Figure 4). For some classes\nin POS tagging, such as ADJ and PUNCT, the\nmodel obtains around 50% accuracy without see-\ning the label. However, unseen performance on\nAUX in POS tagging and MISC in NER is close to\n0%. Furthermore, while observing tags like LOC\nin NER greatly improves performance, other tags\nlike ADJ and MISC improve much less when seen.\n5.2 Effect of Label Form\nWe hypothesize that the behavior observed in Sec-\ntion 5.1 depends on how informative the label form\nis for the class. Therefore, we compare the model\n6653\nFigure 5: Results of ablating the surface form of the\nlabels for structured prompting.\nperformance on (1) the original task labels; (2)\nshuffled task labels, where we shuffle the label sur-\nface forms but maintain underlying class correspon-\ndences to words; and (3) proxy labels, where we\nrepresent the classes with arbitrary tokens – here,\nconsecutive integers ranging from 11 to 27 (POS)\nand from 11 to 14 (NER). (Figure 5).\nLabel shuffling confuses GPT-NeoX Shuffling\nthe labels greatly hurts overall model performance,\nwith POS scores decreasing overall by 50.5%, and\nNER by 65.9%. Some classes are more robust to\nthe shuffled labels than others: the AUX and DET\nparts-of-speech score within the standard error of\nthe original class performance, whereas ADJ accu-\nracy drops by 96.2% to near zero.\nInterestingly, most mistakes made in the shuffled\nsetting (61.4%) result from the model predicting\nthe true class label rather than the shuffled one from\nthe demonstration. This occurs more frequently for\nclasses whose performance severely degrades when\nshuffled: 93.9% of errors on the NOUN class are\ndue to this phenomenon, and across classes, there\nis a strong correlation between performance degra-\ndation and the percent of errors predicting the true\nlabel (ρ = 0.69,p <0.05). This result suggests\nthat PLMs ignore in-context label mappings when\nthe model already associates the label with a spe-\ncific class, similar to findings in Min et al. (2022).\nGPT-NeoX in-context learns with arbitrary\nproxy labels Model behavior with the proxy la-\nbels is closer to the original labels, with perfor-\nmance decreasing by 25.8% on POS and 30.5%\non NER. Indeed, on many labels that significantly\ndegrade with label shuffling, the model performs\nsignificantly better on the proxy labels (NOUN and\nCCONJ in POS tagging, PER in NER). These re-\nsults demonstrate that the model is able to perform\nin-context learning to extract linguistic structure,\neven when the tags are uninformative.\n6 Sources of Linguistic Knowledge in\nPretraining Corpus\nThe results in Section 5 demonstrate that the choice\nof label form can greatly affect structured prompt-\ning performance and implies that the model con-\ntains prior task knowledge. We analyze contexts in\nwhich the labels for POS tagging and NER appear\nin the Pile (Gao et al., 2020) to better understand\nwhat, if any, task information GPT-NeoX learns\nfrom pretraining.\nOur analysis shows that task information occurs\nin the pretraining data, both as labeled examples\n(Section 6.1) and in other related contexts (Sec-\ntion 6.2). However, we find no evidence of test\ndata leakage. Given these findings, we evaluate\nthe model in a new setting that substitutes an En-\nglish description of each class (e.g., “adjective”,\n“person”) for the label in order to control for la-\nbel leakage while still providing meaningful labels\n(Section 6.3).\n6.1 Task Data Contamination\nA likely location for task labels to occur is leaked\ntask examples from pretraining data sources. To\ntest this, we search the Pile for instances of labeled\nPOS and NER data (Table 2, the full results are\ngiven in Appendix A.4).\nPOS Tagging Since the POS data is obtained\nfrom UD treebanks, we search the Pile for each\nlabel as it would appear in the treebank (with tab\nwhitespace on either side of it, see CCONJ example\ncontext). We find a significant amount of UD data\nformatted in this manner: up to 33,000 occurrences\nfor an individual label (NOUN). This is unsurpris-\ning given that Github – where UD treebanks are\nhosted – is a data source for the Pile. However, we\nfind no evidence of test data leakage across any of\nthe POS label occurrences when compared to the\nGUM treebank (Zeldes, 2017).3\n3We also compare the test set against the Pile via other\nmethods (exact document match and searching for individual\nlines); none of these match any test data against the Pile.\n6654\nLabel Freq. Task Stats Example Contexts\nPOS Tagging UD Format\nThe 10 most frequent relations where parent and child node agree in ‘Polarity‘:NOUN 360k 9.29% <tt>NOUN —-> ADJ</tt> (2; 100%) (GitHub)\nCCONJ 22k 23.48% 13 \\t und \\t und \\t CCONJ \\t KON \\t _ \\t 14 \\t cc (GitHub)\nDET 1.53M 0.72% DET: determiner, e.g. a, an, the \\n INTJ: interjection, e.g. psst... (StackExchange)\nNER Relevant?\nBacterial pellets were lysed in 10 ml B-PER Bacterial Protein ExtractionB-PER 5,655 26/100 Reagent... (PubMed)\nI-LOC 2,197 43/100 y = np.asarray(\"B-PER O O B-LOC I-LOC O B-ORG\".split()) (StackExchange)\n*I-PER* label usually follows *B-PER* and *I-PER*, but it cannot followB-ORG 2603 80/100 *B-ORG* or *I-ORG*. (Arxiv)\nI-MISC 907 76/100 My(O) favorite(O) book(O) is(O) harry(B-MISC) potter(I-MISC)... (StackExchange)\nTable 2: Analysis of the Pile for labels from UD POS tagset and CONLL03 NER tagset. Task Stats document the\npercentage of occurrences that are in the UD format for POS tagging and the proportion of sampled documents\nrelevant to NER. Some examples are slightly edited for readability.\nWe also perform a closer analysis of the CCONJ\nlabel: we compare each occurrence against all nine\nEnglish treebanks in UD and manually examine\nit. We find that many CCONJ occurrences can be\nfound in the English Web Treebank (EWT; Silveira\net al., 2014) (1052/118/155 from the train/dev/test\nsplits); others match with Parallel Universal Depen-\ndencies (PUD; Zeman et al., 2017) (10 occurrences\nfrom test set) and ParaTUT (Sanguinetti and Bosco,\n2014) (1 occurrence from development set).\nOur manual analysis finds that most of the\nCCONJ occurrences are in non-English documents\n(77%); other languages whose treebanks we see\ninclude Finnish, German, and Arabic, among many\nothers.4 We also observe that every tab-separated\ninstance of CCONJ occurs in the UD treebank for-\nmat, indicating that this automatic filter is a reason-\nable estimate of UD data leakage across labels.\nNER Task data leakage for NER is much more\nlimited than POS: the most frequent label occurs\n5,655 times in the Pile (other than “O” which oc-\ncurs very frequently in many contexts). Since the\nCONLL format separates the tags with spaces in-\nstead of tabs, it is more difficult to filter for data\nleakage. Instead, we manually evaluate 100 exam-\nples for the BIO labels and give the proportion of\nthe sample that is relevant for NER.\nOnly a subset of relevant occurrences includes\nlabeled data – our analysis found that labeled data\nis not common, and most cases are single exam-\nple sentences annotated in various ways that do\nnot necessarily follow the CONLL format (see I-\n4This is unsurprising: though the Pile is characterized as\nan \"English text corpus\" (Gao et al., 2020), prior work has\nfound similar corpora derived from the web contain significant\namounts of non-English text (Blevins and Zettlemoyer, 2022).\nMISC example context). Similar to POS tagging,\nwe also find labeled examples in non-English lan-\nguages; notably, some of the examples observed\nare incorrectly labeled.5 This highlights that while\nthe model sees task data during pretraining, the\nquality and accuracy of that data are unverified.\n6.2 Labels in Other Contexts\nDuring the data analysis, we also observe tags from\nour tasks in settings other than labeled data. Other\nrelevant contexts are task documentation or descrip-\ntions (see NOUN, DET, and B-ORG example con-\ntexts) and code related to the task (I-LOC example\ncontext). These contexts are particularly interest-\ning, as they provide information that may help the\nmodel learn by explaining the task in natural lan-\nguage or code, rather than via input/output pairs.\nWe also observe instances of labels that are unre-\nlated to the task. This is more common for the POS\ntags; whereas, for NER labels, up to 80% of the\nsampled contexts are related to the task. The topic\nof these unrelated contexts varies widely across\nlabels, from biomedical and legal texts (see B-\nPER example context) to unrelated source code\nand news articles.\n6.3 Relationship Between Labels and Classes\nDue to the quantity of task data uncovered in the\nPile, we would like to control for the effect of pre-\ntraining on labeled data. To this end, we evaluate\nGPT-NeoX on semantically meaningful labels not\npreviously seen in labeled contexts; specifically, we\nreplace the task labels with the English name for\n5For example, the phrase “l’entreprise/O SpaceX/O...” oc-\ncurs in a WebText2 document; however, SpaceX is a named\nentity that should be labeled as B-ORG.\n6655\nLabel Sets\nOrigin. Shuffle Proxy Words\nPOS Tagging\n∆\nAcc.\nOrigin. 83.55\nShuffle -42.11 41.44\nProxy -21.57 20.54 61.98\nWords -5.43 36.67 16.13 78.11\nρ\nOrigin. 1\nShuffle 0.676 1\nProxy 0.934* 0.718 1\nWords 0.924* 0.667 0.909* 1\nNER\n∆\nF1\nOrigin. 58.05\nShuffle -38.28 19.77\nProxy -17.65 20.63 40.40\nWords -1.17† 37.11 -16.48 56.88\nTable 3: Performance deltas ( ∆, column - row) and\nspearman correlations (ρ) of classes between label sets.\n∆ diagonals report performance with that set. †: delta\nis within standard error; *: p << 0.001.\neach class (e.g., adjective, B-location), which we\nrefer to as the words label set. The model achieves\nan accuracy of 78.11 ± 1.46 on POS tagging and\nan F1 score of 56.88 ± 0.86 for NER in this setting.\nIn Table 3, we compare the performance between\nthese label sets and evaluate how correlated indi-\nvidual class performances are across these sets. We\nobserve an identical ranking across label sets in\nPOS tagging and NER. On NER, the difference\nin model performance between the true labels and\nwords as labels is within standard error. However,\non POS there is a small but significant decrease\nof 5.4 points between the two; this drop in perfor-\nmance likely quantifies the benefit of observing the\nPOS task data in the Pile.\nThe correlation study shows that performance\nacross classes on the original, proxy, and words\nlabel sets for POS tagging are all strongly corre-\nlated (ρ> 0.9). However, their correlations with the\nshuffled labels are less significant; this difference is\nlikely due to the prior task knowledge GPT-NeoX\nhas for UD labels leading to predicting the actual\nlabel of the class rather than the shuffled one, as\nseen in Section 5.2.\n7 Related Work\nPrompting PLMs for Sequence Information\nRecent work has applied various prompting ap-\nproaches to sequence tagging tasks, primarily fo-\ncusing on NER (Cui et al., 2021; Ma et al., 2022).\nHowever, these approaches also require further\ntraining, most often by learning new prompt embed-\ndings for the task (Li et al., 2022; Liu et al., 2022b;\nChen et al., 2022). Other work has finetuned lan-\nguage models to apply them to sequence tagging\ntasks (Liu et al., 2022a). In contrast, our approach\nrequires no additional parameters to be learned.\nMore similar to our work is the sequence tagging\nmethod in Shliazhko et al. (2022), though their ap-\nproach prompts the model separately for each word\nin the sentence. Additionally, similar approaches\nto prompting have been proposed for other tasks;\nthese methods decompose a target task and repeat-\nedly prompt the model on subtasks, building on\nthe model’s outputs to generate the final prediction\n(Zhou et al., 2022; Press et al., 2022). However,\nthese approaches solve a different subset of NLP\ntasks and use the outputs from the intermediate\nprompting steps differently (i.e., by conditioning\non them in future prompting steps, whereas in struc-\ntured prompting each output is a predicted label).\nProbing Pretrained Models There is extensive\nwork on probing models for their underlying knowl-\nedge (Belinkov et al., 2017; Blevins et al., 2018;\nGulordava et al., 2018, inter alia.). The approach\nhas become particularly popular for analyzing\nmasked PLMs (e.g., Liu et al., 2019, 2021b), with\nbehavioral probes (e.g. Petroni et al., 2019; Bala-\nsubramanian et al., 2020) in particular using the\nLM setup to elicit knowledge from the model.\nHowever, prompting autoregressive PLMs\n(Brown et al., 2020; Schick and Schütze, 2021; Gao\net al., 2021), though technically similar to behav-\nioral probing, is usually not framed as probing the\nunderlying model for knowledge. Some exceptions\nare Alivanistos et al. (2022), which uses prompting\ntechniques to probe the LM for knowledge base\nrelations, and Li et al. (2022), which replaces diag-\nnostic probes with trained prompt embeddings for\nmodel analysis. We extend this framing by apply-\ning structured prompting as a behavioral probe for\nlinguistic structure.\nAnalysis of Prompting Methods The results of\nthe structured prompting setup ablations are consis-\ntent with prior work. Specifically, our observation\nof the model’s prior label knowledge is similar to\nMin et al. (2022). We expand on their findings by\nshowing that the model can still perform in-context\nlearning with proxy labels where the model has no\nprior mapping for the task.\nOther work has also documented the presence of\ntask data in common pretraining corpora (Dodge\net al., 2021), shown the effect of pretraining term\n6656\nfrequencies on in-context performance (Razeghi\net al., 2022), and demonstrated the ability of LMs\nto learn from task data during pretraining (Magar\nand Schwartz, 2022). Similarly, we document the\npresence of task data and labels in the Pile and find\nthat this signal can help task performance due to\nthe model prior over the labels.\n8 Conclusion\nWe propose structured prompting , a general\nparadigm for sequence tagging with autoregressive\nPLMs. Our experiments show structured prompt-\ning performs well on three few-shot sequence tag-\nging tasks. Further analysis shows that (1) the\napproach can elicit linguistic structure in many set-\ntings, including when the labels are unrelated to\nthe task, and (2) while labeled task data is present\nin the pretraining corpora, using informative labels\nnot found in task data gives similar performance to\nusing the task labels. These findings indicate that\nthe model’s knowledge of linguistic structure is\nmore general than the memorization of the task\ndata. More generally, our approach provides a\nmethod to probe PLMs for sequence knowledge\nwithout training new or existing parameters.\nLimitatons\nData Leakage As discussed in Section 6.1, we\nfind evidence of labeled task data for POS tagging\nand (to a more limited extent) NER in the Pile. We\nattempt to control for this leakage by evaluating\nwith class names as labels rather than the origi-\nnal tag set; however, due to the cost of training\nrecent PLMs and their large pretraining corpora,\nit is impossible to control for data leakage when\nprompting existing models completely.\nBoth Brown et al. (2020) and Chowdhery et al.\n(2022) discuss the presence of task data in their pre-\ntraining corpora when training PLMs and the diffi-\nculty of controlling for it in their evaluations. For\ndownstream users, this issue is further compounded\nin cases where the pretraining data is unavailable,\nas it is impossible to even check for contamination\nin those cases (such as our GPT-3 experiments).\nExperimental Limitations with GPT-3 We only\nperform a subset of our evaluations of structured\nprompting on GPT-3, due to the cost of running the\nmodels in the API; this also means we do not run\ncomprehensive prompt ablations to better tailor the\nsetup for these models. Additionally, the results\n(i.e., lower performance than comparable GPT-Neo\nmodels) are difficult to interpret due to the black\nbox nature of the GPT-3 models – it may be due\nto pretraining data differences (as mentioned in the\nprevious limitation), the lack of prompt engineering\nfor the models, or some other discrepancy.\nEnglish-only Experiments The experiments in\nthis paper focus on English sequence tagging tasks,\nand it is unclear how well the proposed method\ngeneralizes to other languages. We find evidence\nof task-relevant data in pretraining corpora in non-\nEnglish languages, which suggests there is signal\nfor the approach to work in other languages. How-\never, prior work shows that PLMs behave much\nworse when prompted outside of English (Lin et al.,\n2022; Shi et al., 2022) but does not address the ef-\nfect of pretraining data on this phenomenon.\nAcknowledgements\nWe would like to thank Sewon Min and Ari Holtz-\nman for their helpful conversations about the work.\nReferences\nDimitrios Alivanistos, Selene Báez Santamaría, Michael\nCochez, Jan-Christoph Kalo, Emile van Krieken,\nand Thiviyan Thanapalasingam. 2022. Prompting\nas probing: Using language models for knowledge\nbase construction. In LM-KBC 22: Knowledge Base\nConstruction from Pre-trained Language Models.\nSriram Balasubramanian, Naman Jain, Gaurav Jin-\ndal, Abhijeet Awasthi, and Sunita Sarawagi. 2020.\nWhat’s in a name? are BERT named entity represen-\ntations just as good for any other name? In Proceed-\nings of the 5th Workshop on Representation Learning\nfor NLP , pages 205–214, Online. Association for\nComputational Linguistics.\nYonatan Belinkov, Sebastian Gehrmann, and Ellie\nPavlick. 2020. Interpretability and analysis in neural\nNLP. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics: Tu-\ntorial Abstracts, pages 1–5, Online. Association for\nComputational Linguistics.\nYonatan Belinkov, Lluís Màrquez, Hassan Sajjad, Nadir\nDurrani, Fahim Dalvi, and James Glass. 2017. Evalu-\nating layers of representation in neural machine trans-\nlation on part-of-speech and semantic tagging tasks.\nIn Proceedings of the Eighth International Joint Con-\nference on Natural Language Processing (Volume 1:\nLong Papers), pages 1–10.\nSid Black, Stella Biderman, Eric Hallahan, Quentin An-\nthony, Leo Gao, Laurence Golding, Horace He, Con-\nnor Leahy, Kyle McDonell, Jason Phang, Michael\nPieler, USVSN Sai Prashanth, Shivanshu Purohit,\n6657\nLaria Reynolds, Jonathan Tow, Ben Wang, and\nSamuel Weinbach. 2022. GPT-NeoX-20B: An open-\nsource autoregressive language model. In Proceed-\nings of the ACL Workshop on Challenges & Perspec-\ntives in Creating Large Language Models.\nTerra Blevins, Omer Levy, and Luke Zettlemoyer. 2018.\nDeep RNNs encode soft hierarchical syntax. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers), pages 14–19.\nTerra Blevins and Luke Zettlemoyer. 2022. Language\ncontamination helps explain the cross-lingual capa-\nbilities of English pretrained models. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing (EMNLP).\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nXiang Chen, Lei Li, Shumin Deng, Chuanqi Tan,\nChangliang Xu, Fei Huang, Luo Si, Huajun Chen,\nand Ningyu Zhang. 2022. LightNER: A lightweight\ntuning paradigm for low-resource NER via plug-\ngable prompting. In Proceedings of the 29th Inter-\nnational Conference on Computational Linguistics,\npages 2374–2387, Gyeongju, Republic of Korea. In-\nternational Committee on Computational Linguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nLeyang Cui, Yu Wu, Jian Liu, Sen Yang, and Yue Zhang.\n2021. Template-based named entity recognition us-\ning BART. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n1835–1845, Online. Association for Computational\nLinguistics.\nJesse Dodge, Maarten Sap, Ana Marasovi ´c, William\nAgnew, Gabriel Ilharco, Dirk Groeneveld, Margaret\nMitchell, and Matt Gardner. 2021. Documenting\nlarge webtext corpora: A case study on the colossal\nclean crawled corpus. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1286–1305.\nLeo Gao. 2021. On the sizes of openai api\nmodels. https://blog.eleuther.ai/\ngpt3-model-sizes/. Accessed: 2022-10-27.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830.\nKristina Gulordava, Piotr Bojanowski, Édouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless green\nrecurrent networks dream hierarchically. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1195–1205.\nJiaoda Li, Ryan Cotterell, and Mrinmaya Sachan. 2022.\nProbing via prompting. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 1144–1157.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, et al. 2022.\nFew-shot learning with multilingual language models.\nIn Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021a. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nTianyu Liu, Yuchen Jiang, Nicholas Monath, Ryan Cot-\nterell, and Mrinmaya Sachan. 2022a. Autoregres-\nsive structured prediction with language models. In\nProceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengx-\niao Du, Zhilin Yang, and Jie Tang. 2022b. P-tuning:\nPrompt tuning can be comparable to fine-tuning\nacross scales and tasks. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 61–68.\nZeyu Liu, Yizhong Wang, Jungo Kasai, Hannaneh Ha-\njishirzi, and Noah A Smith. 2021b. Probing across\ntime: What does roberta know and when? In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2021, pages 820–842.\nRuotian Ma, Xin Zhou, Tao Gui, Yiding Tan, Qi Zhang,\nand Xuanjing Huang. 2022. Template-free prompt\ntuning for few-shot ner. In Proceedings of the 2022\n6658\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies.\nInbal Magar and Roy Schwartz. 2022. Data contamina-\ntion: From memorization to exploitation. In Proceed-\nings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers),\npages 157–165.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work? In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing (EMNLP).\nJoakim Nivre, Marie-Catherine de Marneffe, Filip\nGinter, Jan Hajic, Christopher D Manning, Sampo\nPyysalo, Sebastian Schuster, Francis Tyers, and\nDaniel Zeman. 2020. Universal dependencies v2:\nAn evergrowing multilingual treebank collection. In\nProceedings of the 12th Language Resources and\nEvaluation Conference, pages 4034–4043.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. arXiv preprint arXiv:2210.03350.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nYasaman Razeghi, Robert L Logan IV , Matt Gardner,\nand Sameer Singh. 2022. Impact of pretraining term\nfrequencies on few-shot reasoning. arXiv preprint\narXiv:2202.07206.\nErik Tjong Kim Sang and Sabine Buchholz. 2000. In-\ntroduction to the conll-2000 shared task chunking. In\nFourth Conference on Computational Natural Lan-\nguage Learning and the Second Learning Language\nin Logic Workshop.\nErik Tjong Kim Sang and Fien De Meulder. 2003. In-\ntroduction to the conll-2003 shared task: Language-\nindependent named entity recognition. In Proceed-\nings of the Seventh Conference on Natural Language\nLearning at HLT-NAACL 2003, pages 142–147.\nManuela Sanguinetti and Cristina Bosco. 2014. Con-\nverting the parallel treebank partut in universal stan-\nford dependencies. Converting the parallel treebank\nParTUT in Universal Stanford Dependencies, pages\n316–321.\nTimo Schick and Hinrich Schütze. 2021. It’s not just\nsize that matters: Small language models are also few-\nshot learners. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2339–2352.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,\nSuraj Srivats, Soroush V osoughi, Hyung Won Chung,\nYi Tay, Sebastian Ruder, Denny Zhou, et al. 2022.\nLanguage models are multilingual chain-of-thought\nreasoners. arXiv preprint arXiv:2210.03057.\nOleh Shliazhko, Alena Fenogenova, Maria Tikhonova,\nVladislav Mikhailov, Anastasia Kozlova, and Tatiana\nShavrina. 2022. mgpt: Few-shot learners go multilin-\ngual. arXiv preprint arXiv:2204.07580.\nNatalia Silveira, Timothy Dozat, Marie-Catherine\nde Marneffe, Samuel Bowman, Miriam Connor, John\nBauer, and Christopher D. Manning. 2014. A gold\nstandard dependency corpus for English. In Pro-\nceedings of the Ninth International Conference on\nLanguage Resources and Evaluation (LREC-2014).\nBen Wang and Aran Komatsuzaki. 2021. GPT-\nJ-6B: A 6 Billion Parameter Autoregressive\nLanguage Model. https://github.com/\nkingoflolz/mesh-transformer-jax.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nAmir Zeldes. 2017. The GUM corpus: Creating mul-\ntilayer resources in the classroom. Language Re-\nsources and Evaluation, 51(3):581–612.\nDaniel Zeman, Martin Popel, Milan Straka, Jan Hajic,\nJoakim Nivre, Filip Ginter, Juhani Luotolahti, Sampo\nPyysalo, Slav Petrov, Martin Potthast, et al. 2017.\nConll 2017 shared task: Multilingual parsing from\nraw text to universal dependencies. In CoNLL 2017\nShared Task: Multilingual Parsing from Raw Text to\nUniversal Dependencies, pages 1–19. Association for\nComputational Linguistics.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOPT: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\nLeast-to-most prompting enables complex reason-\ning in large language models. arXiv preprint\narXiv:2205.10625.\nA Further Ablations and Analysis\nIn this section, we test additional factors that may\naffect the performance of our proposed method.\n6659\n0.0 0.5 1.0 1.5 2.0\nModel Size 1e10\n10\n20\n30\n40\n50\n60F1\nNER\n(a)\n0.0 0.5 1.0 1.5 2.0\nModel Size 1e10\n30\n40\n50\n60F1\nChunking (b)\nTrue\nPredicted\nChunk (20B) (c)\nFigure 6: Additional error analysis results for Section 4.2: performance across model sizes for different demonstra-\ntions sets on (a) NER and (b) chunking, and (c) confusion matrix for GPT-NeoX on chunking.\nTask Model Eval Setting\nFixed Varied\nPOS\n(Acc.)\nGPT-Neo-125M 64.35 ± 1.6 64.38 ± 1.6\nGPT-Neo-2.7B 70.36 ± 3.0 70.32 ± 3.0\nGPT-J-6B 83.13 ± 1.1 83.10 ± 1.1\nNER\n(F1)\nGPT-Neo-125M 16.03 ± 1.7 16.63 ± 2.1\nGPT-Neo-2.7M 38.90 ± 2.7 38.72 ± 2.6\nGPT-J-6B 51.43 ± 0.7 52.10 ± 0.9\nTable 4: Results of ablating the choice of evaluation\ndata for structured prompting on POS tagging and NER.\nTask Model With BIO Constraints?\nYes No\nNER\n(F1)\nGPT-Neo-125M 15.52 ± 1.7 16.03 ± 1.8\nGPT-J-6B 53.03 ± 1.0 51.43 ± 0.7\nGPT-NeoX-20B 58.05 ± 2.1 57.00 ± 1.9\nChunk\n(F1)\nGPT-Neo-125M 36.85 ± 1.3 38.32 ± 1.5\nGPT-J-6B 39.63 ± 3.4 40.12 ± 3.5\nGPT-NeoX-20B 57.60 ± 2.4 59.25 ± 2.7\nTable 5: Results of ablating the BIO constraints for\nstructured prompting on NER and chunking.\nA.1 Choice of evaluation set\nFor computational reasons, the models are evalu-\nated on a fixed subset of 1000 randomly sampled\ntest examples for each task. As using a smaller\nevaluation set can introduce noise into our perfor-\nmance estimates, we run a similar experiment on\na number of the smaller models but resample the\nevaluation examples across five runs in addition to\nvarying the demonstrations (Table 4). We find that\nvarying the evaluation examples has a minimal ef-\nfect on both the average performance and standard\nerror on both POS tagging and NER.\nA.2 Ablating BIO Constraints\nDuring this work, we found that limiting the poten-\ntial output tag space from the model with global\nBIO constraints made little difference in model per-\nformance for both NER and chunking (Table 5).\nSpecifically, in every case, the difference between\nthe two settings was within the standard error of the\nmeans across runs, with NER performing slightly\nbetter with the constraints and chunking perform-\ning slightly worse.\nA.3 Full Results of Error Analysis\nWe provide additional error analysis results from\nSection 4.2 in Figure 6.\nA.4 Full Results of Pretraining Data Analysis\nThe complete data analysis for labels not shown in\nSection 6 is detailed in Table 7.\nB Complete Results of Structured\nPrompting Experiments\nWe provide the full numerical results for the exper-\niments in Section 4.1 in Table 6.\nC Responsible NLP Miscellanea\nThis section details information from the Respon-\nsible NLP Checklist not covered elsewhere in the\npaper.\nCompute Costs The computational cost of each\nprompting experiment on the GPT-Neo series of\nmodels varies depending on the task and size of the\nunderlying PLM: run times for a single experiment\nrange from around 43 minutes for POS tagging on\nthe 125M parameter model to approximately 50\nhours for chunking with GPT-NeoX (20B parame-\nters). The smaller GPT-neo models (fewer than 6B\nparameters) are run on a single Nvidia RTX-6000,\nand larger models are run on one or more Nvidia\nA40 GPUs.\n6660\nModel Size k = Task\nPOS (Acc.) NER (F1) Chunk (F1)\n125M\n10\n64.35 ± 1.6 15.52 ± 1.7 36.85 ± 1.3\n1.3B 68.45 ± 1.7 39.07 ± 1.2 37.56 ± 4.5\n2.7B 70.36 ± 3.0 40.16 ± 2.6 53.18 ± 2.1\n6B 83.13 ± 1.1 53.03 ± 1.0 39.63 ± 3.4\n20B 83.56 ± 0.8 58.05 ± 2.1 57.60 ± 3.4\n20B\n0 17.20 3.79 1.08\n1 70.84 ± 1.9 10.26 ± 1.1 32.02 ± 3.9\n3 79.08 ± 1.1 33.63 ± 2.8 48.33 ± 3.6\n5 81.72 ± 1.2 40.60 ± 1.6 50.98 ± 3.0\n7 82.67 ± 0.8 52.12 ± 3.7 54.00 ± 2.7\n9 83.56 ± 0.8 58.08 ± 1.8 54.84 ± 2.9\nBaselines\nMajority Label 17.75 0.00 0.00\nPer-Span Majority 80.76 47.52 61.84\nTable 6: Full Results of GPT-Neo series experiments from Section 4.1.\nFor the GPT-3 POS tagging experiments, we\nrun the models through the OpenAI API. When\nperforming constrained decoding through the API,\neach example requires multiple calls per word in\nthe sentence to decode the label forms, since model\nstate caching for custom decoding is not available.\nFor GPT-Curie (k=5), with constrained decoding,\non average 230M tokens are submitted to the API\nper run; with Davinci (k=10, where we only per-\nformed unconstrained decoding), an average of\n1.2M tokens are submitted per run.\nIntended Usage of Artifacts To the best of our\nknowledge, our experiments all fall within the in-\ntended use cases of the GPT-Neo models and the\nPile dataset, as well as the usage policy of the Ope-\nnAI API.\nLabel Freq. Task Stats\nPOS Tagging UD Format\nADJ 449,789 2.49%\nADP 1,847,009 0.80%\nADV 2,315,004 0.42%\nAUX 572,373 1.71%\nCCONJ 22,050 23.48%\nDET 1,528,722 0.72%\nINTJ 28,882 2.11%\nNOUN 360,034 9.29%\nNUM 3,642,199 0.10%\nPART 4,573,194 0.09%\nPRON 130,754 11.00%\nPROPN 50,247 18.81%\nPUNCT 131,344 18.27%\nSCONJ 18,307 17.68%\nSYM 1,189,552 0.08%\nVERB 451,447 4.66%\nX – –\nNER\nB-PER 5,655 –\nI-PER 4,678 –\nB-ORG 2,603 –\nI-ORG 3,793 –\nB-LOC 4,467 –\nI-LOC 2,197 –\nB-MISC 1,133 –\nI-MISC 907 –\nO – –\nTable 7: Automatic analysis of the Pile for labels from\nUD POS tagset and CONLL03 NER tagset. Task Stats\ndocument the percentage of occurrences that are in the\nUD format for POS tagging. We do not search labels\nthat are individual characters due to how frequently they\nappear in the corpus.\n6661\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nIn the required Limitations Section (after Conclusions)\n□\u0017 A2. Did you discuss any potential risks of your work?\nThis work presents and analyzes a general prompting technique for core NLP tasks (sequence\ntagging); there are very limited risks with regards to this work.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nThe Abstract and Section 1 (Introduction)\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSections 3 through 6 (used existing artifacts)\n□\u0013 B1. Did you cite the creators of artifacts you used?\nIn Sections 3 through 6 when discussed\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. We did not create or release any new artifacts\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nAppendix C\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. We did not collect any new data\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nLeft blank.\nC □\u0013 Did you run computational experiments?\nSections 3 through 5\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 3 and Appendix C\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n6662\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 3 (not model hyperparameters, but prompting format decisions)\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nNo response.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 3\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n6663",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7742995023727417
    },
    {
      "name": "Natural language processing",
      "score": 0.716964066028595
    },
    {
      "name": "Chunking (psychology)",
      "score": 0.6735931038856506
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6073802709579468
    },
    {
      "name": "Task (project management)",
      "score": 0.5347166657447815
    },
    {
      "name": "Sentence",
      "score": 0.5111172795295715
    },
    {
      "name": "Language model",
      "score": 0.48264771699905396
    },
    {
      "name": "Linguistics",
      "score": 0.434165358543396
    },
    {
      "name": "Memorization",
      "score": 0.4326728880405426
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    }
  ],
  "cited_by": 13
}