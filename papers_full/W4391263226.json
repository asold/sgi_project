{
  "title": "SRBerta—A Transformer Language Model for Serbian Cyrillic Legal Texts",
  "url": "https://openalex.org/W4391263226",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4366036115",
      "name": "Miloš Bogdanović",
      "affiliations": [
        "University of Nis"
      ]
    },
    {
      "id": "https://openalex.org/A1923176642",
      "name": "Jelena Kocic",
      "affiliations": [
        "University of Nis"
      ]
    },
    {
      "id": "https://openalex.org/A720661482",
      "name": "Leonid Stoimenov",
      "affiliations": [
        "University of Nis"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6739901393",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W4306981185",
    "https://openalex.org/W4287887886",
    "https://openalex.org/W4287889363",
    "https://openalex.org/W2962739339"
  ],
  "abstract": "Language is a unique ability of human beings. Although relatively simple for humans, the ability to understand human language is a highly complex task for machines. For a machine to learn a particular language, it must understand not only the words and rules used in a particular language, but also the context of sentences and the meaning that words take on in a particular context. In the experimental development we present in this paper, the goal was the development of the language model SRBerta—a language model designed to understand the formal language of Serbian legal documents. SRBerta is the first of its kind since it has been trained using Cyrillic legal texts contained within a dataset created specifically for this purpose. The main goal of SRBerta network development was to understand the formal language of Serbian legislation. The training process was carried out using minimal resources (single NVIDIA Quadro RTX 5000 GPU) and performed in two phases—base model training and fine-tuning. We will present the structure of the model, the structure of the training datasets, the training process, and the evaluation results. Further, we will explain the accuracy metric used in our case and demonstrate that SRBerta achieves a high level of accuracy for the task of masked language modeling in Serbian Cyrillic legal texts. Finally, SRBerta model and training datasets are publicly available for scientific and commercial purposes.",
  "full_text": null,
  "topic": "Serbian",
  "concepts": [
    {
      "name": "Serbian",
      "score": 0.9530238509178162
    },
    {
      "name": "Transformer",
      "score": 0.5546798706054688
    },
    {
      "name": "Linguistics",
      "score": 0.5163849592208862
    },
    {
      "name": "Computer science",
      "score": 0.5032276511192322
    },
    {
      "name": "Natural language processing",
      "score": 0.41083914041519165
    },
    {
      "name": "Philosophy",
      "score": 0.1856485903263092
    },
    {
      "name": "Engineering",
      "score": 0.13457149267196655
    },
    {
      "name": "Electrical engineering",
      "score": 0.10871577262878418
    },
    {
      "name": "Voltage",
      "score": 0.05463898181915283
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I152518017",
      "name": "University of Nis",
      "country": "RS"
    }
  ],
  "cited_by": 5
}