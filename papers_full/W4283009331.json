{
  "title": "XMorpher: Full Transformer for Deformable Medical Image Registration via Cross Attention",
  "url": "https://openalex.org/W4283009331",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2692090773",
      "name": "Jiacheng Shi",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A2100728983",
      "name": "Yuting He",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A2623122019",
      "name": "Youyong Kong",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A2476591294",
      "name": "Jean-Louis Coatrieux",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2157537541",
      "name": "Huazhong Shu",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A2114803924",
      "name": "Guanyu Yang",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A2118732852",
      "name": "Shuo Li",
      "affiliations": [
        "Case Western Reserve University",
        "Western University"
      ]
    },
    {
      "id": "https://openalex.org/A2692090773",
      "name": "Jiacheng Shi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100728983",
      "name": "Yuting He",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2623122019",
      "name": "Youyong Kong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2476591294",
      "name": "Jean-Louis Coatrieux",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2157537541",
      "name": "Huazhong Shu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114803924",
      "name": "Guanyu Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2118732852",
      "name": "Shuo Li",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2787740020",
    "https://openalex.org/W2891631795",
    "https://openalex.org/W4226497331",
    "https://openalex.org/W2891590469",
    "https://openalex.org/W6931320883",
    "https://openalex.org/W6799166919",
    "https://openalex.org/W3180275357",
    "https://openalex.org/W3083087624",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W2133287637",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2113576511",
    "https://openalex.org/W2115167851",
    "https://openalex.org/W2346062110",
    "https://openalex.org/W2066470779",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3104370314",
    "https://openalex.org/W3199376581",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2291593693",
    "https://openalex.org/W3098269293"
  ],
  "abstract": null,
  "full_text": "XMorpher: Full Transformer for Deformable\nMedical Image Registration via Cross Attention\nJiacheng Shi1, Yuting He1, Youyong Kong1,2,3, Jean-Louis Coatrieux2,3, and\nHuazhong Shu1,2,3 Guanyu Yang1,2,3(B) Shuo Li4\n1 LIST, Key Laboratory of Computer Network and Information Integration\n(Southeast University), Ministry of Education, Nanjing, China\n2 Jiangsu Provincial Joint International Research Laboratory of Medical Information\nProcessing\n3 Centre de Recherche en Information Biomedicale Sino-Franc, ais (CRIBs)\n4 Dept. of Medical Biophysics, University of Western Ontario, London, ON, Canada\nyang.list@seu.edu.cn\nAbstract. An eﬀective backbone network is important to deep learning-\nbased Deformable Medical Image Registration (DMIR), because it ex-\ntracts and matches the features between two images to discover the\nmutual correspondence for ﬁne registration. However, the existing deep\nnetworks focus on single image situation and are limited in registra-\ntion task which is performed on paired images. Therefore, we advance\na novel backbone network, XMorpher, for the eﬀective corresponding\nfeature representation in DMIR.1) It proposes a novel full transformer\narchitecture including dual parallel feature extraction networks which ex-\nchange information through cross attention, thus discovering multi-level\nsemantic correspondence while extracting respective features gradually\nfor ﬁnal eﬀective registration.2) It advances the Cross Attention Trans-\nformer (CAT) blocks to establish the attention mechanism between im-\nages which is able to ﬁnd the correspondence automatically and prompts\nthe features to fuse eﬃciently in the network.3) It constrains the atten-\ntion computation between base windows and searching windows with dif-\nferent sizes, and thus focuses on the local transformation of deformable\nregistration and enhances the computing eﬃciency at the same time.\nWithout any bells and whistles, our XMorpher gives Voxelmorph2.8%\nimprovement on DSC , demonstrating its eﬀective representation of the\nfeatures from the paired images in DMIR. We believe that our XMor-\npher has great application potential in more paired medical images. Our\nXMorpher is open onhttps://github.com/Solemoon/XMorpher\n1 Introduction\nA powerful backbone network is important to deep learning (DL)-based De-\nformable Medical Image Registration (DMIR)[1,5,15]. The backbone network is\nable to extract the features of moving and ﬁxed images in DMIR and then match\nthe features to obtain correspondences from moving images to ﬁxed images, thus\narXiv:2206.07349v1  [cs.CV]  15 Jun 2022\n2 J. Shi et al.\nwrapped image\n(a) Fusion-first with feature distortion \nin mixed regions \n(b) Fusion-last with biased feature \nextraction  \n(c) Our cross-attention-based fusion with \ninner-network gradual correspondence\nSpatial transform for DMIR Feature Extraction F Fusion\nmoving image fixed image moving image fixed image\nFF\nϕϕ\n...... ......\n...... ......\nCross-Attention\nCross-Attention\n... ...\nϕ\nwrapped image\nCross-Attention\nϕϕ\n......\n............\nwrapped image\nmoving image fixed image\nϕϕ\n......\nFF Biased Biased Mixed\nGradually extract and \nmatch features\nWeak alignment \non mixed regions\nWeak alignment \non ignored regions\nFine alignment\nExtract features Match features\nExtract and match \nfeaturesMix image\nDiscover \ncorrespondence \nvia attention\nFig. 1.The limitations of existing methods and our superiority. (a) Fusion-ﬁrst with\nfeature distortion in mixed regions making inaccurate representation for correspond-\ning features. (b) Fusion-last with biased feature extraction losing correspondence of\nignored features. (c) Our cross-attention-based fusion with inner-network gradual cor-\nrespondence for ﬁne alignment of diﬀerent level features.\ncontributing to a ﬁne registration with these eﬀective correspondences. The mov-\ning images are transformed into the same coordinate system as the ﬁxed images\nafter registration, which makes it convenient and eﬃcient to compare diﬀerent\nimages for doctors, thus greatly promoting the eﬃciency of diagnosis and reduc-\ning the cost of disease.\nAlthough existing deep networks[13,16] have strong performance in single\nimage feature representation, these Single Image Networks (SINs) are still lim-\nited in feature extraction and match of a pair of images in DMIR:1) Fusion-\nﬁrst with feature distortion in mixed regions.As shown in Fig.1(a), some DMIR\nmethods[1,2,19]fusethemovingandﬁxedimagestosimulatethesingle-imagein-\nput condition, and the fusion is sent to a SIN for moving-ﬁxed features. But these\nmethods mix the feature extraction and feature matching processes together,\nleading to the feature distortion and weak alignment in the mixed regions, thus\nmaking the networks unable to identify one-to-one correspondences between the\nimage pairs. The ineﬃcient capacity of feature representation ﬁnally contributes\nto the absence of critical structures and poor registering details.2) Fusion-last\nwith biased feature extraction.As shown in Fig.1(b), these networks[11] send\nmoving and ﬁxed images to dual SINs respectively and fuse features from dif-\nferent networks at the end. But these networks separate the feature extraction\nand feature matching processes absolutely, leading to ﬁnal match of two biased\nfeatures from diﬀerent SINs, thus making the networks ignore the diﬀerent levels\n(such as multiscale) of the features in some regions. The unicity of feature repre-\nsentation limits the correspondence of diﬀerent information between images and\nleads to poor registration ﬁnally.\nThe attention mechanism of transformer[7] provides the potential application\nin registration because of its outstanding capacity of catching relevance in im-\nages, but existing researches of transformer[3,10,12,21] only focus on single image\nFull Transformer for Deformable Image Registration 3\nscenarios and lack related design for moving-ﬁxed correspondence between two\nimages in DMIR. These transformers utilize the self-attention function to obtain\nan output with the weight information which points out the areas needing to be\nfocused in one image. This mechanism digs out the relationships of internal basic\nelements and extracts the most task-related features, and thus transformer has\na good performance in single-image tasks and is potential in DMIR.However,\ncurrent transformers[4,20] for DMIR still take the same attention mechanism\nas single-image tasks which focus on the relevance in one image but ignore the\ncorrespondences between image pairs. The competence of capturing correspon-\ndences between moving and ﬁxed images restrains transformers to ﬁnd eﬀective\nregistering features for ﬁne registration.\nIn this paper, we proposed a novel transformer, X-shape Morpher (XMor-\npher) for dual images input in DMIR, it advances Cross Attention to the trans-\nformer architecture for eﬃcient and multi-level semantic feature fusion, thus\neﬀectively improving the performance of registration. In short, the contributions\nof our work are summarized as follows:1) We proposed a novel full transformer\nbackbone network for DMIR. As shown in Fig.1(c), it includes dual parallel fea-\nture extraction sub-networks whose respective features are fused and matched in\nthe form of cross attention. Through the progressive and commutative network,\nthe features from diﬀerent images are fused and matched gradually through\nthe cross-attention-based fusion modules, thus achieving eﬀective feature repre-\nsentation of moving-ﬁxed correspondences and gaining ﬁne-grained multi-level\nsemantic information for ﬁne registration.2) We present a new attention mech-\nanism, Cross Attention Transformer (CAT) block, for suﬃcient communication\nbetween a pair of features from moving and ﬁxed images. CAT block utilizes\nthe attention mechanism to compute the mutual relevance, thus learning the\ncorrespondences between two images and promoting the features to match au-\ntomatically in the network. 3) We constrain the feature matching process in\nwindows based on the local transformation of DMIR, which narrows the search-\ning range between moving and ﬁxed images, thus increase the computation eﬃ-\nciency and reduce the interference from similar structure if matching in a large\nspace. This window-based feature communication greatly improve the accuracy\nand eﬃciency of registration.\n2 Methodology\nAsisshowninFig.2,XMorpherisbasedonDL-basedregistrationnetworks[2,19],\nand it is used to extract and match moving and ﬁxed features in registration for\neﬀective representation of input image pairs. The ﬁnal representation generates\nthe DVFφ and obtain the ﬁne wrapped imagew through spatial transform[9].\nOur XMorpher includes:1)A X-shape transformer architecture with dual paral-\nlel U-shape feature extraction sub-networks which keeps exchanging information\nthrough cross-attention-based feature fusion modules.2)CAT blocks for feature\nfusion between two features tokens from diﬀerent sub-networks.3) Multi-size\nwindow partition in CAT for precise local-wise correspondences.\n4 J. Shi et al.\nWindow\nPartition\nWindow\nArea Partition\nLN+Linear LN+Linear\nW-MCA\nLN\nMLP\nQ,VK\n(e) W-MCA  for window-\nbased moving -fixed attention   \nbase window\nsearching window\nSliding\nSliding\nWindow Partition\nWindow Area Partition\nSliding\nWindow Partition\nWindow Area Partition\n(d) Multi-Size Window Partition  for local-wise correspondence\nSkip Connection\nUp Sample\nDown Sample\nFeature Tokens\nConcat+Conv \n(c) CAT computing cross attention\nSplit into \npatches\nSplit into \npatches\n          \n             \n \n \n(a) XMorpher  for moving-fixed correspondences(a) XMorpher  for moving-fixed correspondences\n\nDVF\nDVF\nWrapped \nImage\nWrapped \nImage\nSpatial Transform\nSpatial Transform\nFixed image\nFixed image\nMoving image\n \nMoving image\n \nLinear Projection Linear ProjectionLinear Projection Linear Projection\nCross Attention \nTransformer\nCross Attention \nTransformer\nCross Attention \nTransformer\nCross Attention \nTransformer\n(b) Cross-attention-based \nFeature Fusion  \nbase windows\nsearching windows\nK\nQ, V\nsb\nFig. 2.Overall architecture of our XMorpher. (a) It includes dual U-shape networks\nwhich exchange features through our cross-attention-based feature fusion. (b) The fea-\nture fusion module is composed of two CAT blocks sharing parameters for mutual\ncorrespondences. (c) The detailed construction of our CAT block which fuses two in-\nput features into one with attention information. (d) Our two diﬀerent methods of\nwindow partition to generate base and searching windows. (e) We compute the cross\nattention between base and searching windows through W-MCA.\n2.1 XMorpher for eﬃcient and multi-level semantic feature represen-\ntation in registration\nI. X-shape architecture with parallel communicating feature extrac-\ntion networks.As shown in Fig.2(a), we utilize dual U-shape networks to ex-\ntract the features of moving and ﬁxed images respectively and the two networks\ncommunicate through feature fusion modules, thus forming a X-shape network,\nso we name it as XMorpher. The two parallel networks follow the structure of\nUnet[13] with encoding and decoding parts, but we replace the convolutions with\nour CAT blocks which play important role in the attention-wise feature fusion\nmodules (Fig.2(b)) between the two networks. Through the parallel communi-\ncating networks, our XMorpher exchange cross-image information vertically and\nkeeps reﬁning features horizontally. So the ﬁnal output features have strong abil-\nity of representing the correspondences between moving and ﬁxed images.\nII. Cross-attention-based feature fusion module.As shown in Fig.2(b),\nthe corresponding features Tm and Tf coming from the parallel sub-networks\nobtain their mutual attention through two CAT blocks by exchanging the order\nof inputs. Then the two outputs with the other’s attention return to the original\nFull Transformer for Deformable Image Registration 5\npipelines and prepare for next deeper communication. There arektimes of com-\nmunication in total in a feature fusion module for suﬃcient mutual information.\nThrough the attention-wise feature fusion modules between two networks, fea-\ntures from diﬀerent networks with diﬀerent semantic information communicate\nfrequently, thus our XMorpher keeping learning multi-level semantic features for\nﬁnal ﬁne registration.\n2.2 Cross Attention Transformer block for corresponding attention\nCAT block aims to compute new feature tokens with corresponding relevance\nfrom input featureb to feature s through the attention mechanism. As shown\nin Fig.2(c), b and s are respectively partitioned in diﬀerent ways (described in\nsection 2.3) into two sets of windows, base window setSba and searching win-\ndow setSse, for next window-based attention calculation.Sba and Sse have the\nsame size n and diﬀerent window sizes. Each base window inSba is projected\nto the query setquery and each searching window is projected to knowledge set\nkey and value through linear layer. Then our Window-based Muti-head Cross\nAttention (W-MCA) compute the cross attention between two windows and the\nattention is added to the base window so that each base window gets the corre-\nsponding weighed information from searching window. Finally, the new output\nset is sent to a 2-layer MLP[18] with GELU non-linearity to enhance learning\nability. A LayerNorm (LN) layer is applied before each W-MCA and each MLP\nmodule to guarantee validity of each layer.\n2.3 Multi-size window partitions for local-wise correspondence\nI. Window Partition (WP) and Window Area Partition (WAP).Since\nthe deformable image registration focuses on local displacement of voxel and\nthere is no large span correspondence between moving and ﬁxed images, we\nproposed the window-based cross attention mechanism which utilizes multi-size\nwindow partitions to limit attention computation in windows. Multi-size win-\ndow partitions include two diﬀerent methods, WP and WAP, to divide the input\nfeature tokensband sinto windows of diﬀerent sizes. As shown in Fig.2(d), WP\npartition feature tokens directly into base window setSba with size ofn×h×w×d\nand WAP enlarges the window size with the magniﬁcationsα, β and γ. So the\nbase and searing window size are calculated as:\nhba,wba,dba = h,w,d\nhse,wse,dse = α·h,β ·w,γ ·d (1)\nwhere hba,wba,dba are the size of base windows andhse,wse,dse are the size of\nsearching windows. To obtain the same amount of two window sets, WAP takes\nadvantage of a sliding window and the stride is set as the base window size, and\nthus Sse has size ofn×α·h×β·w×γ·d. Through the corresponding windows\nwith diﬀerent sizes, CAT blocks compute the cross attention between two feature\ntokens eﬃciently and avoid large-span searches for precise correspondence.\nII. Window-based Muti-head Cross Attention (W-MCA).We proposed\n6 J. Shi et al.\nW-MCA to compute the cross attention between acquired base windows and\nsearching windows to ﬁnd the mutual correspondences. W-MCA is a function\nmapping a query and a set of key-value pairs to an output, where the query\ncomes from the base windows, keys and values come from the searching windows.\nThe output is computed as a weighted sum of the values, where the weight\nassigned to each value is computed by a compatibility function of the query\nwith the corresponding key. W-MCA adopts multi-head attention[18] for ample\nrepresentation subspaces. W-MCA computes the dot products of query and keys\nand applies a softmax function to obtain the weights on the values. So our cross\nattention is computed as:\nW −MCA (Qba,Kse,Vse) =softmax\n(QbaKT\nse√\nd\n)\nVse (2)\nwhere Qba,Kse,Vse are thequery,key and value matrices. Qba ∈Rn×s×c is the\nlinear projection ofSba and Kse,Vse ∈Rn×µ·s×c are linear projections ofSse,\ns= h×w×d and µ= α·β·γ, andc is the dimension of each feature token.\n3 Experiment\n3.1 Experiment protocol\nWe perform eﬀective experiments to evaluate our XMorpher’s superiority both in\nunsupervised and semi-supervised strategies.1) Dataset. We validate the per-\nformance of our XMorpher on the whole heart registration tasks on the CT\ndataset from MM-WHS 2017 Challenge[22] which has 20 labeled images and\n40 unlabeled images and ASOCA[6] which has 60 unlabeled images. We use\nall the unlabeled images (100 images) and 5 labeled images to compose 500\nlabeled-unlabeled image pairs and 9900 unlabeled-unlabeled image pairs as the\ntraining set for unsupervised and semi-supervised experiments. The remaining\n15 of the labeled images compose 210 image pairs as testing set. All images were\npreprocessed to the same spatial coordinates through aﬃne transformation[17].\n2) Implementation. We apply our XMorpher as backbone in two registration\nframeworks: unsupervised Voxelmorph[1] (VM-XMorpher) and semi-supervised\nPC-Reg[8] (PC-XMorpher). Furthermore, the hyperparameters of XMorpher is\nset as h = w = d = 2 and α = β = γ = 3. The proposed framework is imple-\nmented by PyTorch on NVIDIA GeForce RTX 3090 GPUs with 24 GB memory.\n3) Comparison settings.We set up a set of comparative experiments to prove\nthe advancement of our XMorpher. Controlled experiments include BSpline[14],\nVoxelmorph[1], PC-Reg[8], Transmorph[4].4) Evaluation metrics. We use the\nmean dice similarity coeﬃcients (DSC) of all labels to evaluate the performance\nof registration and the Jacobian matrix (|J(ψ)|⩽ 0 (%)) to evaluate the ratio-\nnality of registration ﬁelds.\n3.2 Results and analysis\nQuantitativecomparison. OurVM-XMorpherachievesthehighestDSCscore\nof 83.0% and has the top-ranked performance on Jacobian matrix at the same\nFull Transformer for Deformable Image Registration 7\nTable 1.The proposed XMorphers achieve the state-of-the-art performance on DSC\nboth under unsupervised and semi-supervised strategies, as well as have top-ranked\nperformance on Jacobian matrix (|Jφ|≤ 0 (%)).\nMethod Un-/Semi- Backbone DSC |Jφ|≤ 0 (%)\nAﬃne initialization - - 69.2 ±7.2 -\nBSpline [14]\nUnsup\n- 80.9 ±7.6 5 .25 ±3.27\nVoxelmorph [1] CNN 80.2 ±5.5 4 .02 ±0.82\nTransmorph [4] CNN+Transformer 81.1 ±5.2 3 .46 ±0.75\nOur no-cross XMorpher Full Transformer 81.5 ±5.4 0.94±0.26\nOur VM-XMorpher Full Transformer 83.0±4.7 3.15 ±0.79\nPC-Reg[8] Semi CNN 86.0 ±2.5 0 .36 ±0.20\nOur PC-XMorpher Full Transformer 86.9±2.4 0.32±0.18\nFixedMoving\nVoxelmorph\nTransmorph\nVM-XMorpher\nPC-Reg\nPC-XMorpher\nFig. 3.The visual results of Voxelmorph, Transmorph, PC-Reg and ours. XMorphers\nhave obvious visual advantages both in unsupervised and semi-supervised strategies.\ntime (tab.1), which shows that it not only has a strong registration eﬀect, but\nalso has a strong maintenance of the image structure. VM-XMorpher is2.8%\nhigher on DSC and 0.87% lower on Jacobian matrix than Voxelmorph[1], il-\nlustrating that XMorpher represents the correspondence eﬀectively and thus\nresults in reﬁned details and strong topology preservation compared with fusion-\nﬁrst networks. VM-XMorpher still has an obvious improvement in contrast to\nTransmorph[4] which combines the CNN and transformer, evaluating that cross\nattention mechanism has an superiority over the existing transformers and full\ntransformer is good at focus on the cross-image correspondence. Furthermore,\nPC-Reg has achieved considerable performance on registration while our PC-\nXMorpher is still0.9% higher on DSC and has better performance on Jacobian\nmatrix. These results prove the superiority of our XMorpher under diﬀerent\ntraining strategies.\nVisual superiority.Fig.3 shows one case of the visual results of our XMor-\npher and other compared experiments in diﬀerent training strategies. Our PC-\nXMorpher has better details compared with PC-reg such as the more smooth\nboundaries. Furthermore, our XMorpher has obvious visual superiority under\n8 J. Shi et al.\n(a) DSC and Jacobian matrix varying with window size\nwindow size\nDSC\nJacobian matrix\n(2,2,2) (7,7,7)\n81.0\n83.0\n81.4\n82.2\n81.8\n82.6\n1.00\n4.00\n1.60\n2.80\n2.20\n3.40\nDSC\nJacobian matrix\nDSC\nJacobian matrix\n(4,4,4)\nlarger window size better \nJacobian matrixsmaller window size \nbetter DSC\n(b) slices of overall attention\nWrapped images with uniformly distributed \nattentions focusing on information-rich regions\nFig. 4.(a) The DSC and Jacobian matrix of XMorpher have inverse variation with the\nwindow size. (b)The uniformly distributed window attentions indicate that XMorpher\neﬀectively ﬁnd the keys in every windows and the keys correspond to the heart region.\nunsupervised experiments.VM-XMorpher still has much better ability of bound-\nary recognition than other methods and stronger resolution of the neighbouring\nregions while other models (Voxelmorph, Transmorph) have the obvious mix-\nture of two borders. Furthermore, the deformation grids of VM-XMorpher is\nalso smooth compared with other methods which have many cracks in the grids.\nAblation study.Through the ablation experiments, we demonstrated the ef-\nfect of CAT blocks and the inﬂuence of the window size.a) Ablation for CAT\nblocks. We designed no-cross XMorpher whose input is the concatenation of\nmoving and ﬁxed images. Its transformer blocks computes the internal atten-\ntion of the concatenation and the other components are same as XMorpher. Our\nVM-XMorpher has 1.5 percent increase on DSC demonstrating that the eﬀective\ncorrespondence representation enhances the interaction between the two images\nin DMIR but brings the risk of less smooth deformation at the same time. But\nour VM-XMorpher still has better performance on Jacobian matrix than other\nmodels. b) Ablation for window size. Fig.4(a) demonstrates that the larger win-\ndows has better performance on the Jacobian matrix because the more structure\ninformation of the images will be extracted in a wider horizon, but the smaller\nwindows result in a better DSC owing to ﬁner searching space.\nAnalysis of visual cross attention.The uniformly distributed visual atten-\ntion (Fig.4(b)) illustrates that the window-based attention mechanism disperses\nthe correspondence into windows and takes advantage of these uniform map-\npings of small regions for a ﬁne registration. Furthermore, the high-attention\nregions with ample information correspond to the heart region, demonstrating\nthe powerful image comprehensive ability of our XMorpher.\nFull Transformer for Deformable Image Registration 9\n4 Conclusion\nWe propose a full transformer network XMorpher which eﬀectively represents\nmulti-level correspondence between moving and ﬁxed images. Furthermore, we\nuse window-based CAT blocks to extract and match the features in the network\neﬃciently to obtain ﬁnal cross-image correspondences for ﬁne registration. The\ncompared experiments have prove that XMorpher has outstanding performance\nin DMIR with diﬀerent training strategies and we believe that it has great ap-\nplication prospect in diagnosis and treatment.\nAcknowledgment This work was supported in part by the National Key Re-\nsearch and Development Program of China (No.2021ZD0113202), in part by the\nNational Natural Science Foundation under grants (61828101), CAAI-Huawei\nMindSpore Open Fund, CANN(Compute Architecture for Neural Networks),\nAscend AI Processor, and Big Data Computing Center of Southeast University.\nReferences\n1. Balakrishnan, G., Zhao, A., Sabuncu, M.R., Guttag, J., Dalca, A.V.: An unsuper-\nvised learning model for deformable medical image registration. In: Proceedings of\nthe IEEE conference on computer vision and pattern recognition. pp. 9252–9260\n(2018)\n2. Balakrishnan, G., Zhao, A., Sabuncu, M.R., Guttag, J., Dalca, A.V.: Voxelmorph:\na learning framework for deformable medical image registration. IEEE transactions\non medical imaging38(8), 1788–1800 (2019)\n3. Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A.L., Zhou,\nY.:Transunet:Transformersmakestrongencodersformedicalimagesegmentation.\narXiv preprint arXiv:2102.04306 (2021)\n4. Chen, J., Du, Y., He, Y., Segars, W.P., Li, Y., Frey, E.C.: Transmorph: Transformer\nfor unsupervised medical image registration. arXiv preprint arXiv:2111.10480\n(2021)\n5. De Vos, B.D., Berendsen, F.F., Viergever, M.A., Sokooti, H., Staring, M., Išgum,\nI.: A deep learning framework for unsupervised aﬃne and deformable image regis-\ntration. Medical image analysis52, 128–143 (2019)\n6. Gharleghi,R.,Samarasinghe,D.G.,Sowmya,P.A.,Beier,D.S.:Automatedsegmen-\ntation of coronary arteries (Mar 2020). https://doi.org/10.5281/zenodo.3819799,\nhttps://doi.org/10.5281/zenodo.3819799\n7. Han, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., Tang, Y., Xiao, A., Xu,\nC., Xu, Y., et al.: A survey on vision transformer. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence (2022)\n8. He, Y., Li, T., Ge, R., Yang, J., Kong, Y., Zhu, J., Shu, H., Yang, G., Li,\nS.: Few-shot learning for deformable medical image registration with perception-\ncorrespondence decoupling and reverse teaching. IEEE Journal of Biomedical and\nHealth Informatics (2021)\n9. He, Y., Li, T., Yang, G., Kong, Y., Chen, Y., Shu, H., Coatrieux, J.L., Dillenseger,\nJ.L., Li, S.: Deep complementary joint model for complex scene registration and\nfew-shot segmentation on medical images. In: European Conference on Computer\nVision. pp. 770–786. Springer (2020)\n10 J. Shi et al.\n10. Khan, S., Naseer, M., Hayat, M., Zamir, S.W., Khan, F.S., Shah, M.: Transformers\nin vision: A survey. ACM Computing Surveys (CSUR) (2021)\n11. Klein, S., Staring, M., Murphy, K., Viergever, M.A., Pluim, J.P.: Elastix: a tool-\nbox for intensity-based medical image registration. IEEE transactions on medical\nimaging 29(1), 196–205 (2009)\n12. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin\ntransformer:Hierarchicalvisiontransformerusingshiftedwindows.In:Proceedings\nof the IEEE/CVF International Conference on Computer Vision. pp. 10012–10022\n(2021)\n13. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi-\ncal image segmentation. In: International Conference on Medical image computing\nand computer-assisted intervention. pp. 234–241. Springer (2015)\n14. Rueckert, D., Sonoda, L.I., Hayes, C., Hill, D.L., Leach, M.O., Hawkes, D.J.: Non-\nrigid registration using free-form deformations: application to breast mr images.\nIEEE transactions on medical imaging18(8), 712–721 (1999)\n15. Sotiras, A., Davatzikos, C., Paragios, N.: Deformable medical image registration:\nA survey. IEEE transactions on medical imaging32(7), 1153–1190 (2013)\n16. Tajbakhsh, N., Shin, J.Y., Gurudu, S.R., Hurst, R.T., Kendall, C.B., Gotway,\nM.B., Liang, J.: Convolutional neural networks for medical image analysis: Full\ntraining or ﬁne tuning? IEEE transactions on medical imaging35(5), 1299–1312\n(2016)\n17. Tustison, N.J., Cook, P.A., Klein, A., Song, G., Das, S.R., Duda, J.T., Kandel,\nB.M., van Strien, N., Stone, J.R., Gee, J.C., et al.: Large-scale evaluation of ants\nand freesurfer cortical thickness measurements. Neuroimage99, 166–179 (2014)\n18. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,\nŁ., Polosukhin, I.: Attention is all you need. Advances in neural information pro-\ncessing systems30 (2017)\n19. Vos, B.D.d., Berendsen, F.F., Viergever, M.A., Staring, M., Išgum, I.: End-to-end\nunsupervised deformable image registration with a convolutional neural network.\nIn: Deep learning in medical image analysis and multimodal learning for clinical\ndecision support, pp. 204–212. Springer (2017)\n20. Zhang, Y., Pei, Y., Zha, H.: Learning dual transformer network for diﬀeomorphic\nregistration. In: de Bruijne, M., Cattin, P.C., Cotin, S., Padoy, N., Speidel, S.,\nZheng, Y., Essert, C. (eds.) Medical Image Computing and Computer Assisted In-\ntervention – MICCAI 2021. pp. 129–138. Springer International Publishing, Cham\n(2021)\n21. Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T.,\nTorr, P.H., et al.: Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. In: Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition. pp. 6881–6890 (2021)\n22. Zhuang, X., Shen, J.: Multi-scale patch and multi-modality atlases for whole heart\nsegmentation of mri. Medical image analysis31, 77–87 (2016)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8614469766616821
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7202237844467163
    },
    {
      "name": "Image registration",
      "score": 0.6439277529716492
    },
    {
      "name": "Transformer",
      "score": 0.5872253179550171
    },
    {
      "name": "Computer vision",
      "score": 0.5180402398109436
    },
    {
      "name": "Feature extraction",
      "score": 0.4319087266921997
    },
    {
      "name": "Fuse (electrical)",
      "score": 0.4125981032848358
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3732413649559021
    },
    {
      "name": "Image (mathematics)",
      "score": 0.2949658930301666
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}