{
    "title": "Continuous-Time Sequential Recommendation with Temporal Graph Collaborative Transformer",
    "url": "https://openalex.org/W3193835619",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5017213607",
            "name": "Ziwei Fan",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5100321247",
            "name": "Zhiwei Liu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5100462813",
            "name": "Jiawei Zhang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5001877137",
            "name": "Yun Xiong",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5106587693",
            "name": "Lei Zheng",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5036357902",
            "name": "Philip S. Yu",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3014930781",
        "https://openalex.org/W2605350416",
        "https://openalex.org/W3098231197",
        "https://openalex.org/W2991079997",
        "https://openalex.org/W3005071803",
        "https://openalex.org/W2583674722",
        "https://openalex.org/W2972577885",
        "https://openalex.org/W2767597557",
        "https://openalex.org/W2964015378",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2955162038",
        "https://openalex.org/W3119313768",
        "https://openalex.org/W2726499916",
        "https://openalex.org/W2984100107",
        "https://openalex.org/W2783944588",
        "https://openalex.org/W3173075577",
        "https://openalex.org/W2624407581",
        "https://openalex.org/W2945623882",
        "https://openalex.org/W3080374445",
        "https://openalex.org/W3081170586",
        "https://openalex.org/W3158237717",
        "https://openalex.org/W3031029974",
        "https://openalex.org/W2474909202",
        "https://openalex.org/W2996931760",
        "https://openalex.org/W3135311786",
        "https://openalex.org/W3008933340",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2970733709",
        "https://openalex.org/W3007404067",
        "https://openalex.org/W2262817822",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3101707147",
        "https://openalex.org/W3003799861",
        "https://openalex.org/W3139159537",
        "https://openalex.org/W2964044287",
        "https://openalex.org/W2140310134",
        "https://openalex.org/W3210802119",
        "https://openalex.org/W2146456494",
        "https://openalex.org/W2015500481",
        "https://openalex.org/W3156927220",
        "https://openalex.org/W3025066464",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W3101588560",
        "https://openalex.org/W1886704267",
        "https://openalex.org/W3193441787",
        "https://openalex.org/W3035178789",
        "https://openalex.org/W1500188831",
        "https://openalex.org/W2219888463",
        "https://openalex.org/W3129178271",
        "https://openalex.org/W2080320419",
        "https://openalex.org/W2963367478",
        "https://openalex.org/W2798918712",
        "https://openalex.org/W2171279286",
        "https://openalex.org/W2027731328",
        "https://openalex.org/W3100278010",
        "https://openalex.org/W3123895104",
        "https://openalex.org/W3169632866",
        "https://openalex.org/W3045200674",
        "https://openalex.org/W3012752604",
        "https://openalex.org/W2783272285"
    ],
    "abstract": "In order to model the evolution of user preference, we should learn user/item embeddings based on time-ordered item purchasing sequences, which is defined as Sequential Recommendation (SR) problem. Existing methods leverage sequential patterns to model item transitions. However, most of them ignore crucial temporal collaborative signals, which are latent in evolving user-item interactions and coexist with sequential patterns. Therefore, we propose to unify sequential patterns and temporal collaborative signals to improve the quality of recommendation, which is rather challenging. Firstly, it is hard to simultaneously encode sequential patterns and collaborative signals. Secondly, it is non-trivial to express the temporal effects of collaborative signals. Hence, we design a new framework Temporal Graph Sequential Recommender (TGSRec) upon our defined continuous-time bi-partite graph. We propose a novel Temporal Collaborative Trans-former (TCT) layer in TGSRec, which advances the self-attention mechanism by adopting a novel collaborative attention. TCT layer can simultaneously capture collaborative signals from both users and items, as well as considering temporal dynamics inside sequential patterns. We propagate the information learned fromTCTlayerover the temporal graph to unify sequential patterns and temporal collaborative signals. Empirical results on five datasets show that TGSRec significantly outperforms other baselines, in average up to 22.5% and 22.1%absolute improvements in Recall@10and MRR, respectively.",
    "full_text": "Continuous-Time Sequential Recommendation with Temporal\nGraph Collaborative Transformer\nZiwei Fan*, Zhiwei Liu*\nDepartment of Computer Science,\nUniversity of Illinois at Chicago\nUSA\n{zfan20,zliu213}@uic.edu\nJiawei Zhang\nIFM Lab, Department of Computer\nScience, University of California,\nDavis\nUSA\njiawei@ifmlab.org\nYun Xiong\nShanghai Key Laboratory of Data\nScience, School of Computer Science,\nFudan University\nChina\nyunx@fudan.edu.cn\nLei Zheng\nPinterest Inc.\nUSA\nlzheng@pinterest.com\nPhilip S. Yu\nDepartment of Computer Science,\nUniversity of Illinois at Chicago\nUSA\npsyu@uic.edu\nABSTRACT\nIn order to model the evolution of user preference, we should learn\nuser/item embeddings based on time-ordered item purchasing se-\nquences, which is defined as Sequential Recommendation (SR) prob-\nlem. Existing methods leverage sequential patterns to model item\ntransitions. However, most of them ignore crucial temporal collab-\norative signals, which are latent in evolving user-item interactions\nand coexist with sequential patterns. Therefore, we propose to unify\nsequential patterns and temporal collaborative signals to improve\nthe quality of recommendation, which is rather challenging. Firstly,\nit is hard to simultaneously encode sequential patterns and collab-\norative signals. Secondly, it is non-trivial to express the temporal\neffects of collaborative signals.\nHence, we design a new framework Temporal Graph Sequential\nRecommender (TGSRec) upon our defined continuous-time bi-\npartite graph. We propose a novel Temporal Collaborative Trans-\nformer (TCT) layer in TGSRec, which advances the self-attention\nmechanism by adopting a novel collaborative attention. TCT layer\ncan simultaneously capture collaborative signals from both users\nand items, as well as considering temporal dynamics inside sequen-\ntial patterns. We propagate the information learned fromTCT layer\nover the temporal graph to unify sequential patterns and temporal\ncollaborative signals. Empirical results on five datasets show that\nTGSRec significantly outperforms other baselines, in average up\nto 22.5% and 22.1% absolute improvements in Recall@10 and MRR,\nrespectively. Our code is available online in https://github.com/\nDyGRec/TGSRec.\n*Both authors contribute equally.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nCIKM â€™21, November 1â€“5, 2021, Virtual Event, QLD, Australia\nÂ© 2021 Association for Computing Machinery.\nACM ISBN 978-1-4503-8446-9/21/11. . . $15.00\nhttps://doi.org/10.1145/3459637.3482242\nCCS CONCEPTS\nâ€¢ Information systems â†’Collaborative filtering; Recommender\nsystems; Personalization.\nKEYWORDS\nSequential Recommendation, Temporal Effects, Graph Neural Net-\nwork, Transformer\nACM Reference Format:\nZiwei Fan*, Zhiwei Liu*, Jiawei Zhang, Yun Xiong, Lei Zheng, and Philip\nS. Yu. 2021. Continuous-Time Sequential Recommendation with Temporal\nGraph Collaborative Transformer. In Proceedings of the 30th ACM Interna-\ntional Conference on Information and Knowledge Management (CIKM â€™21),\nNovember 1â€“5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY,\nUSA, 10 pages. https://doi.org/10.1145/3459637.3482242\n1 INTRODUCTION\nRecommender system has become essential in providing personal-\nized information filtering services in a variety of applications [21,\n26, 31, 40, 41]. It learns the user and item embeddings from his-\ntorical records on the user-item interactions [ 8, 34]. In order to\nmodel the dynamics of the user-item interaction, current research\nworks [5, 9, 35, 37, 42] leverage historical time-ordered item pur-\nchasing sequences to predict future items for users, referred to as\nthe sequential recommendation (SR) problem [ 9, 12]. One of the\nfundamental assumptions of SR is that the usersâ€™ interests change\nsmoothly [9, 12, 37, 42]. Thus, we can train a model to infer the\nitems more likely to appear in the future sequence. For example,\nwith the recent developments of Transformer [38], current endeav-\nors design a series of self-attention SR models to predict future\nitem sequences [12, 36, 44]. A self-attention model infers sequence\nembeddings at position ğ‘¡ by assigning an attention weight to each\nhistorical item and aggregating these items. The attention weights\nreveal impacts of previous items to the current state at time pointğ‘¡.\nDespite their effectiveness, existing works only leverage the\nsequential patterns to model the item transitions within sequences,\nwhich is still insufficient to yield satisfactory results. The reason\nis that they ignore the crucial temporal collaborative signals ,\nwhich are latent in evolving user-item interactions and coexist with\narXiv:2108.06625v2  [cs.IR]  22 Aug 2021\nFigure 1: A toy example of temporal collaborative signals.\nGiven the items that users ğ‘¢1,ğ‘¢2,ğ‘¢3 and ğ‘¢4 like in the past\ntimestamps ğ‘¡1,ğ‘¡2,ğ‘¡3 and ğ‘¡4, the target is to recommend an\nitem to ğ‘¢4 at ğ‘¡5 as the next item after ğ‘–2.\nsequential patterns. To be specific, we present the effects of temporal\ncollaborative signals in Figure 1. The target is to recommend an\nitem to ğ‘¢4 at ğ‘¡5 as the next item after ğ‘–2. By only considering the\nsequential patterns,ğ‘–3 is recommended since it appears2 times after\nğ‘–2 as inğ‘¢1 and ğ‘¢3, compared withğ‘–4 of only 1 time inğ‘¢2. However, if\nalso taking account of collaborative signals, we would recommend\nğ‘–4, because both ğ‘¢2 and ğ‘¢4 have interactions with ğ‘–1 at ğ‘¡2 and ğ‘–2 at\nğ‘¡3 and ğ‘¡4, respectively, which indicates their high similarity. Hence,\nğ‘¢2â€™s sequential patterns are of more impacts to ğ‘¢4. This motivates\nus to unify sequential patterns and temporal collaborative signals .\nHowever, incorporating temporal collaborative signals in SR is\nrather challenging. The first challenge is that it is hard to simultane-\nously encode collaborative signals and sequential patterns. Current\nmodels capture the sequential pattern based on the transition of\nitems within sequences [9, 12, 28], thus lacking the mechanism to\nmodel the collaborative signals across sequences. Jodie [ 17] and\nDGCF [20] employs LSTM to model the dynamics and interactions\nof user and item embeddings but they cannot learn the impacts of all\nhistorical items, thus unable to encode sequences. SASRec [12] pro-\nposed to use a self-attention mechanism to encode item sequence,\nwhile the effects of users, i.e., collaborative signals, are omitted. SSE-\nPT [44] implicitly models collaborative signals by directly adding\nthe same user embedding into the sequence encoding. However, it\nfails to model the interactions between user and item, thus unable\nto explicitly express collaborative signals.\nThe second challenge is that it is hard to express the temporal\neffects of collaborative signals. In other words, it remains unclear\nhow to measure the impacts of those signals from a temporal per-\nspective. For example, in Figure 1, ğ‘–1 is interacted with ğ‘¢2 and ğ‘¢4 at\nğ‘¡1, while ğ‘–2 is interacted with them respectively at ğ‘¡3 and ğ‘¡4. Since\nthere is a lag, it is problematic to ignore the time gap and assume\nthey are of equal contributions. We should use temporal informa-\ntion to infer the importance of those collaborative signals to the\nrecommendation on ğ‘¡5. Existing works [9, 12, 20, 36] assume that\nitems appear discretely with equal time intervals. Thus, they only\nfocus on the orders/positions of items in the sequence, which limits\ntheir capacity in expressing the temporal information. Some recent\nworks [19, 50] also notice the importance of time span. But their\nmodels either fail to capture time differences between historical\ninteractions or are unable to generalize to any unseen future times-\ntamps or time difference, thus are still far from revealing the actual\ntemporal effects of collaborative signals.\n(a) Example of CTBG\n (b) Temporal Inference\nFigure 2: The associated CTBG of Figure 1 and the inference\nof temporal embeddings of ğ‘¢4 and ğ‘–4 at ğ‘¡5.\nCurrent transformer-based models [12, 36] adopt self-attention\nmechanism, which has query, key, and value inputs from item em-\nbeddings and employs dot-product to learn their correlation scores.\nThe limitation is that self-attention is only able to capture item-item\nrelationships in sequences. Additionally, they have no module to\ncapture temporal correlations of items. To this end, we propose a\nnew model Temporal Graph Sequential Recommender (TGSRec).\nIt consists of two novel components: (1) the Temporal Collaborative\nTransformer (TCT) layer and (2) graph information propagation.\nThe first component advances current transformer-based models\nas it can explicitly model collaborative signals in sequences and\nexpress temporal correlations of items in sequences. To be more\nspecific, TCT layer adopts collaborative attention among user-item\ninteractions, where the query input to the collaborative attention\nis from the target node (user/item), while the key and value in-\nputs are from connected neighbors. As such, TCT layer learns the\nimportance of those interactions, thus well characterizing the collab-\norative signals. Moreover,TCT layer fuses the temporal information\ninto the collaborative attention mechanism, which explicitly ex-\npresses the temporal effects of those interactions. Altogether, the\nTCT layer captures temporal collaborative signals.\nThe second module is devised upon our proposed Continuous-\nTime Bipartite Graph (CTBG). The CTBG consists of user/item\nnodes, and interaction edges with timestamps, as shown in Figure 2a.\nGiven timestamps, neighbor items of users preserve sequential\npatterns. We propagate temporal collaborative information learned\naround each node to surrounding neighbors over CTBG. Therefore,\nit unifies sequential patterns with temporal collaborative signals.\nIn this work, we propose to use temporal embeddings of nodes\nfor recommendation, which are dynamic and inferred at specified\ntimestamps. For example, at time ğ‘¡, we infer the temporal user\nembedding by aggregating the context. We illustrate the temporal\ninference of ğ‘¢4 and ğ‘–4 at time ğ‘¡5 in Figure 2b. The temporal em-\nbeddings are inferred by our proposed TCT layer. It uses temporal\ninformation to discriminate impacts of those historical interactions\nand makes inferences of temporal node embeddings. The contribu-\ntions of this paper are as follows:\nGraph Sequential Recommendation: We connect the SR prob-\nlem with graph embedding methods, which focuses on unifying\nthe sequential patterns and temporal collaborative signals.\nTemporal Collaborative Transformer: We propose a novel tem-\nporal collaborative attention mechanism to infer temporal node\nembeddings, which jointly models collaborative signals and tem-\nporal effects. This overcomes the inadequacy of the traditional\nself-attention mechanism on capturing the temporal effects and\nuser-item collaborative signals.\nExtensive Experiments: We conduct a comparison experiment on\nfive real-world datasets. Comprehensive experiments demonstrate\nthe state-of-the-art performance of TGSRec and its effectiveness of\nmodeling temporal collaborative signals.\n2 RELATED WORK\nIn this section, we first review some related work, which includes\nsequential recommendation (SR), temporal information, and some\ngraph-based recommender systems.\n2.1 Sequential Recommendation\nSR predicts the future items in the user shopping sequence by\nmining the sequential patterns. An initial solution to the SR prob-\nlem is to build a Recurrent Neural Network (RNN) [9, 42, 51, 55].\nGRU4Rec [9] is proposed to predict the next item in a session by\nemploying the GRU modules. Later, a Hierarchical RNN [33] is pro-\nposed to enhance the RNN model regarding the personalizing infor-\nmation. Additionally, LSTM [10, 42] can be applied to explore both\nthe long-term and short-term sequential patterns. Moreover, in or-\nder to capture the intent of users at local sub-sequence, NARM [18]\nis proposed by combining the RNN model with attention weights.\nThe major drawback of the RNN model is that it can only generate a\nsingle hidden vector, which limits its power to encode sequences [2].\nRecently, owing to the success of self-attention model [ 3, 22,\n38, 52] in NLP tasks, a series of attention-based SR models are pro-\nposed [11, 12, 23, 28, 32, 36, 43]. SASRec [12] applies the transformer\nlayer to assign weights to items in the sequence. Later, inspired by\nthe BERT [3] model, BERT4Rec [36] is proposed with a bidirectional\ntransformer layer. [28] introduce the sequence to sequence training\nprocedure in SR. SSE-PT [44] designs a personalized transformer\nto improve the SR performance. ASReP [23] proposes augmenting\nshort sequences to alleviate the cold-start issue in Transformer. Ti-\nSASRec [19] enhances SASRec with the time-interval information\nfound in the training data. However, these models only focus on\nthe item transitions within sequences, while unable to unify the\nimportant temporal collaborative signals with sequential patterns\nand are not generalized to unseen timestamps.\n2.2 Temporal Information\nPreviously mentioned SR works are specifically designed to cap-\nture sequential patterns, while ignoring the important temporal\ninformation [15, 17, 19, 46, 47]. In practice, the context of users\nand items changes over time, which is crucial for modeling the\ntemporal dynamics in SR. TimeSVD++ [15] is a representative work\nwhich models the temporal information into collaborative filter-\ning (CF) method. It simply treats the bias as a function over time.\nBPTF [47] extends matrix factorization to tensor factorization and\nuses time as the third dimension. MS-IPF [46] defines a temporal\ngraph, where it operates PageRank algorithm for recommendation.\nRecently, CDTNE [30] is proposed by applying temporal random\nwalk over its defined continuous-time dynamic network. TGAT [49]\nalso introduces temporal attention for learning dynamic graph em-\nbeddings. JODIE [17] develops user and item RNNs to update user\nand item embeddings. Regarding the SR problem, a few recent\nworks [19, 50] also notice the importance of temporal information.\nCTA [43], MTAM [11], and TiSASRec [19] all consider to use time\nintervals between successive items in sequences. TASER [50] en-\ncodes both the absolute time and relative time as vectors, which are\nprocessed to attention models to complete the SR task. However,\nthese models are not able to unify temporal collaborative signals\nwith sequential patterns.\n2.3 Graph-based Recommendation\nBecause we solve the SR problem based on the graph structure [53,\n54], we also review some graph-based recommender system mod-\nels [7, 24â€“26, 30, 40], especially those based on Graph Neural Net-\nwork (GNN) methods [1, 7, 14, 40]. Compared with directly learning\nfrom sequences, graph-based models can also capture the struc-\ntural information [1, 30]. Both NGCF [40] and LightGCN [7] argue\nthat graph-based models are able to effectively model collaborative\nsignals, which is crucial in learning user/item embeddings. The\nsuccesses of GNN in recommender systems [1, 7, 39, 40] provide\nsimple yet effective methods in learning user/item embeddings from\ngraphs. GNN models learn the embeddings by aggregating neigh-\nbors [7, 40]. Therefore, it is easy to stack multiple layers to learn\nboth the first-order and high-order collaborative signals [ 7, 40].\nCTDNE [30] defines a temporal graph to learn dynamic embed-\ndings of nodes. TGAT [49] learns the dynamic graph embeddings\nbased on the graph attention model. Basconv [25] characterizes het-\nerogeneous graphs to learn user/item embeddings. Those models\nargue that graph is powerful in modeling both the structural and\ntemporal information. However, few works investigate the possi-\nbility of solving SR problems based on graphs. SR-GNN [45] learns\nembeddings of session graphs by using a GNN to aggregate item\nembeddings but fails to model temporal collaborative signals.\n3 DEFINITIONS AND PRELIMINARIES\nIn this section, we introduce some definitions and preliminaries.\nDifferent from using usersâ€™ interactions sequences as inputs in\nSR, we introduce the Continuous-Time Bipartite Graph (CTBG) to\nrepresent all temporal interactions. Each edge in this graph has the\ntimestamp as the attribute. The directly connected neighbors of\nevery user/item node in this graph preserve the sequential order\nvia the timestamps at edges. The formal definition of CTBG are\ngiven in the following:\nDefinition 3.1 (Continuous-Time Bipartite Graph). A con-\ntinuous time bipartite graph with ğ‘ nodes and ğ¸ edges for recom-\nmendations is defined as B= {U,I,ET}, where Uand Iare two\ndisjoint node sets of users and items, respectively. Every edge ğ‘’ âˆˆET\nis denoted as a tuple ğ‘’ = (ğ‘¢,ğ‘–,ğ‘¡ ), where ğ‘¢ âˆˆU, ğ‘– âˆˆI, and ğ‘¡ âˆˆR+\nas the edge attribute. Each triplet (ğ‘¢,ğ‘–,ğ‘¡ )denotes the interaction of a\nuser ğ‘¢with item ğ‘– at timestamp ğ‘¡.\nThis paper focuses on the SR problem with continuous times-\ntamps. An example of the CTBG is presented in Figure 2a. LetIğ‘¢ (ğ‘¡)\ndenote the set of items interacted with the user ğ‘¢ before times-\ntamp ğ‘¡, and I\\Iğ‘¢ (ğ‘¡)denote the remaining items. We defined the\ncontinuous-time sequential recommendation problem which we\nstudy in this paper as following:\nDefinition 3.2 (Continuous-Time Recommendation). At a\nspecific timestamp ğ‘¡, given user set U, item set I, and the associated\nCTBG, the continuous-time recommendation of ğ‘¢ is to generate a\nranking list of items from I\\Iğ‘¢ (ğ‘¡), where the items that ğ‘¢is interested\nwill be ranked top in the list.\nThen, the SR problem is equivalent to make continuous-time\nrecommendations on a set of future timestamps Tğ‘¢ for each user ğ‘¢:\nDefinition 3.3 (Continuous-Time Seqential Recommenda-\ntion). For a specific user ğ‘¢, given a set of future timestamps Tğ‘¢ > ğ‘‡,\nthe continuous-time sequential recommendation for this user is to\nmake a continuous-time recommendation for every timestamp ğ‘¡ âˆˆTğ‘¢.\nThis is a generalized definition compared with other works [12,\n28]. We explicitly consider timestamps, while others only care about\nthe orders/positions. Therefore, differing from existing works using\nnext-item prediction to evaluate sequential recommendation, future\ntimestamps should be present to make a prediction. If timestamps\nare position numbers in sequences, the studied problem is reduced\nto the same definition as using only orders/positions information.\nNote that timestamp can be any real value, thus being continuous.\n4 PROPOSED MODEL\nIn this section, we present theTGSRec model, which unifies sequen-\ntial patterns and temporal collaborative signals. The framework of\nthe TGSRec model is presented in Figure 3. There are three major\ncomponents: 1) Embedding layer, which encodes nodes and times-\ntamps in a consistent way to connect the SR problem with graph\nembedding method; 2) Temporal Collaborative Transformer (TCT)\nlayer, which employs a novel temporal collaborative attention mech-\nanism to discriminate temporal impacts of neighbors, and aggre-\ngates both node and time embeddings to infer the temporal node\nembedding; 3) Prediction layer, which utilizes output embeddings\nfrom the final TCT layer to calculate the score.\n4.1 Embedding Layer\nWe encode two types of embeddings in this paper, one being the\nlong-term embeddings of nodes, and the other being the continuous-\ntime embeddings of timestamps on edges.\n4.1.1 Long-Term User/Item Embeddings. Long-term embeddings\nfor users and items are necessary [4] for long-term collaborative\nsignals representation. In CTBG, it functions as node features and\nis optimized to model the holistic structural information. A user\n(item) node is parameterized by a vectorğ’†ğ‘¢ (ğ’†ğ‘– )âˆˆ Rğ‘‘ . Since we learn\nembeddings for nodes in the CTBG, we retrieve the embedding of\na node by indexing an embedding table ğ‘¬ = [ğ‘¬U; ğ‘¬I]âˆˆ Rğ‘‘Ã—|V|,\nwhere V= UâˆªI . Note that the embedding table ğ‘¬ serves as a\nstarting state for the inference of temporal user/item embeddings.\nDuring the training process, ğ‘¬ will be optimized.\n4.1.2 Continuous-Time Embedding. The continuous time encod-\ning [48, 50] behaves as a function that maps those scalar timestamps\ninto vectors, i.e.,Î¦ : ğ‘‡ â†¦â†’Rğ‘‘ğ‘‡ , whereğ‘‡ âˆˆR+. Based on previous SR\nmodels [19, 43, 50], time span plays a vital component in expressing\nthe temporal effects and uncovering sequential patterns. The time\nencoding function embeds timestamps into vectors so as to repre-\nsent the time span as the dot product of corresponding encoded\ntime embeddings. Therefore, we define the temporal effects as a\nfunction of time span in continuous time space: given a pair of\ninteractions (ğ‘¢,ğ‘–,ğ‘¡ 1)and (ğ‘¢,ğ‘—,ğ‘¡ 2)of the same user, the temporal\neffect is defined as a function ğœ“(ğ‘¡1 âˆ’ğ‘¡2)â†¦â†’ R, which is expressed\nas a kernel value of the time embeddings of ğ‘¡1 and ğ‘¡2:\nğœ“(ğ‘¡1 âˆ’ğ‘¡2)= K(ğ‘¡1,ğ‘¡2)= Î¦(ğ‘¡1)Â· Î¦(ğ‘¡2), (1)\nwhere Kis the temporal kernel and Â·denotes the dot product\noperation. The temporal effect ğœ“(ğ‘¡1 âˆ’ğ‘¡2)measures the temporal\ncorrelation between two timestamps. Moreover, the time encoding\nfunction should be generalized to any unseen timestamp such that\nany time span not found in training data can still be inferred by\nthe encoded time embeddings. Unlike modeling the absolute time\ndifference like [ 19], representing temporal effects as a kernel is\ngeneralized to any timestamp as it models the time representations\ndirectly. Therefore, the temporal effect of any pair of timestamps can\nbe inductively inferred by the dot product of time representations.\nEq. (1) can be achieved by a continuous and translation-invariant\nkernel K(ğ‘¡1,ğ‘¡2)based on Bochnerâ€™s Theorem [ 27]. By explicitly\nrepresenting the temporal features, the temporal embedding is:\nÎ¦(ğ‘¡)â†¦â†’\nâˆšï¸‚ 1\nğ‘‘ğ‘‡\n\u0002\ncos(ğœ”1ğ‘¡),sin(ğœ”1ğ‘¡),..., cos(ğœ”ğ‘‘ğ‘‡ ğ‘¡),sin(ğœ”ğ‘‘ğ‘‡ ğ‘¡)\n\u0003âŠ¤,\n(2)\nwhere ğ =\n\u0002\nğœ”1,...,ğœ” ğ‘‘ğ‘‡\n\u0003âŠ¤are learnable and ğ‘‘ğ‘‡ is the dimension.\n4.2 Temporal Collaborative Transformer\nNext, we present the novel TCT layer of TGSRec. We intend to\naddress two strengths of a TCT layer: (1) constructing informa-\ntion from both user/item embeddings and temporal embedding,\nwhich explicitly characterizes temporal effects of the correlations;\n(2) a collaborative attention module, which advances existing self-\nattention mechanism by modeling the importance of user-item\ninteractions, which is thus able to explicitly recognize collaborative\nsignals. To achieve this, we first present the information construc-\ntion and aggregation from a user perspective. Then, we introduce\na novel collaborative attention mechanism to infer importance of\ninteractions. Finally, we demonstrate how to generalize to items.\n4.2.1 Information Construction. We construct input information of\neach TCT layer as the combination of long term node embeddings\nand time embeddings. As such, we can unify temporal information\nand collaborative signals. In particular, thequery input information\nat the ğ‘™-th layer for user ğ‘¢at time ğ‘¡ is:\nğ’‰(ğ‘™âˆ’1)\nğ‘¢ (ğ‘¡)= ğ’†(ğ‘™âˆ’1)\nğ‘¢ (ğ‘¡)âˆ¥Î¦(ğ‘¡), (3)\nwhere ğ‘™ = 1,2,...,ğ¿ . ğ’‰(ğ‘™âˆ’1)\nğ‘¢ (ğ‘¡)âˆˆ Rğ‘‘+ğ‘‘ğ‘‡ is the information for ğ‘¢at\nğ‘¡, ğ’†(ğ‘™âˆ’1)\nğ‘¢ (ğ‘¡)âˆˆ Rğ‘‘ is the temporal embedding of ğ‘¢, and Î¦(ğ‘¡)âˆˆ Rğ‘‘ğ‘‡\ndenotes the time vector of ğ‘¡. âˆ¥denotes the concatenation operation.\nOther operations including summation are possible. However, we\nuse concatenation for simplicity. It also provides intuitive interpre-\ntation in the attention, as shown in Eq. (7). Note that when ğ‘™ = 1, it\nis the firstTCT layer. The temporal embeddingğ’†(0)\nğ‘¢ (ğ‘¡)= ğ‘¬ğ‘¢, i.e., the\nlong-term user embedding. When ğ‘™ > 1, the temporal embedding\nis generated from the previous TCT layer.\nIn addition to the query node itself, to we also propagate temporal\ncollaborative information from its neighbors. We randomly sample\nğ‘†different interactions ofğ‘¢before timeğ‘¡as Nğ‘¢ (ğ‘¡)= {(ğ‘–,ğ‘¡ğ‘  )|(ğ‘¢,ğ‘–,ğ‘¡ ğ‘  )âˆˆ\nEğ‘¡ and ğ‘¡ğ‘  < ğ‘¡}. The input information at the ğ‘™-th layer for each\nğ‘¢1\nğ‘¢2\nğ‘¢3\nğ‘¢4\nğ‘–1\nğ‘–2\nğ‘–4\nğ‘–3\nğ‘¡5\nğ‘¡4\nğ‘¡3\nNeighbor \nSampling TCT â€“layer 2Temporal Collaborative Transformer (TCT) â€“layer 1\nTemporal Collaborative \nAttention \nğ›Ÿ(ğ‘¡5)ğ’†ğ‘¢4\n(ğŸ)\nğ›Ÿ(ğ‘¡3)\nğ›Ÿ(ğ‘¡4)\nğ‘¢4\nğ‘–1\nğ‘¡3\nğ‘–2\nğ‘¡4\nğ‘¡5\nkey\nvalue\nquery\nğ’†ğ‘¢4\nğŸ (ğ’•ğŸ“)\nğ’†ğ‘–2\nğŸ\nğ’†ğ‘–1\nğŸ\nğ›Ÿ(ğ‘¡5)\nğ›Ÿ(ğ‘¡3)\nğ›Ÿ(ğ‘¡4)ğ’†ğ‘–2\nğŸ (ğ’•ğŸ’)\nğ’†ğ‘–1\nğŸ (ğ’•ğŸ‘)\nConcat\nFFN\nLinearLinear Linear\nTemporal Collaborative Attention \nConcat\nFFN\nğ’†ğ‘¢4\nğ‘³ (ğ’•ğŸ“)\nğ’†ğ‘–4\nğ‘³ (ğ’•ğŸ“)â€¦\nBPR  Loss\nPrediction\nğ‘¯ğ’©ğ‘¢4\nğŸ (ğ’•ğŸ“)\nğ‘¢1\nğ‘¢2\nğ‘¢3\nğ‘¢4\npropagation\nLinearLinear Linear\nTCTâ€“Layer Lâ€¦\nFigure 3: The framework of TGSRec. The query node is ğ‘¢4, whose final temporal embedding at time ğ‘¡5 is ğ’‰(2)\nğ‘¢4 (ğ‘¡5). The TCT layer\nsamples its neighbor nodes and edges. Timestamps on edges are encoded as vectors by using mapping function Î¦. Node em-\nbeddings for the first TCT layer are long-term embeddings. Node embeddings for other TCT layers (e.g. layer 2) are propagated\nfrom the previous TCT layer, thus being temporal node embeddings.\n(ğ‘–,ğ‘¡ğ‘  )pair is:\nğ’‰(ğ‘™âˆ’1)\nğ‘– (ğ‘¡ğ‘  )= ğ’†(ğ‘™âˆ’1)\nğ‘– (ğ‘¡ğ‘  )âˆ¥Î¦(ğ‘¡ğ‘  ), (4)\nwhere ğ’‰ğ‘– (ğ‘¡ğ‘  )is the information for item ğ‘– at ğ‘¡ğ‘  , ğ’†ğ‘– (ğ‘¡ğ‘  )denotes\nthe temporal embedding of ğ‘– at ğ‘¡ğ‘  . Again, note that when ğ‘™ = 1,\nğ’†(0)\nğ‘– (ğ‘¡ğ‘  )= ğ‘¬ğ‘– , i.e., the long-term item embedding. When ğ‘™ > 1, the\ntemporal embedding is output from the previous TCT layer.\n4.2.2 Information Propagation. After constructing the information,\nwe propagate the information of sampled neighbors Nğ‘¢ (ğ‘¡)to infer\nthe temporal embeddings. Since the neighbors are involving with\ntime ğ‘¡, in this way, we can unify the sequential patterns with tem-\nporal collaborative signals. We compute the linear combination of\nthe information from all sampled interactions as:\nğ’†(ğ‘™)\nNğ‘¢\n(ğ‘¡)=\nâˆ‘ï¸\n(ğ‘–,ğ‘¡ğ‘  )âˆˆğ‘ğ‘¢ (ğ‘¡)\nğœ‹ğ‘¢\nğ‘¡ (ğ‘–,ğ‘¡ğ‘  )ğ‘¾ (ğ‘™)\nğ‘£ ğ’‰(ğ‘™âˆ’1)\nğ‘– (ğ‘¡ğ‘  ), (5)\nwhere ğœ‹ğ‘¢\nğ‘¡ (ğ‘–,ğ‘¡ğ‘  )1 denotes the importance of an interaction (ğ‘¢,ğ‘–,ğ‘¡ ğ‘  )\nand ğ‘¾ğ‘£ âˆˆRğ‘‘Ã—(ğ‘‘+ğ‘‘ğ‘‡ )is the linear transformation matrix. ğœ‹ğ‘¢\nğ‘¡ (ğ‘–,ğ‘¡ğ‘  )\nrepresents the impact of a historical interaction (ğ‘¢,ğ‘–,ğ‘¡ ğ‘  )to the\ntemporal inference ofğ‘¢at timeğ‘¡, which is calculated by the temporal\ncollaborative attention.\n4.2.3 Temporal Collaborative Attention. We adopt the novel tem-\nporal collaborative attention mechanism to measure the weights\nğœ‹ğ‘¢\nğ‘¡ (ğ‘–,ğ‘¡ğ‘  ), which considers both neighboring interactions and the\ntemporal information on edges. Both factors contribute to the im-\nportance of historical interactions. Thus, it is a better mechanism\nto capture temporal collaborative signals than self-attention mecha-\nnism that only models item-item correlations. The attention weight\nğœ‹ğ‘¢\nğ‘¡ (ğ‘–,ğ‘¡ğ‘  )is formulated as follows:\nğœ‹ğ‘¢\nğ‘¡ (ğ‘–,ğ‘¡ğ‘  )= 1âˆšï¸\nğ‘‘+ğ‘‘ğ‘‡\n\u0010\nğ‘¾ (ğ‘™)\nğ‘˜ ğ’‰(ğ‘™âˆ’1)\nğ‘– (ğ‘¡ğ‘  )\n\u0011âŠ¤\nğ‘¾ (ğ‘™)\nğ‘ ğ’‰(ğ‘™âˆ’1)\nğ‘¢ (ğ‘¡), (6)\nwhere ğ‘¾ (ğ‘™)\nğ‘˜ and ğ‘¾ (ğ‘™)\nğ‘ are both linear transformation matrices, and\nthe factor 1âˆšğ‘‘+ğ‘‘ğ‘‡\nprotects the dot-product from growing large with\nhigh dimensions. We adopt dot-product attention because if we\nignore transformation matrices and the scalar factor, based on Eq. (3)\n1ğœ‹ğ‘¢\nğ‘¡ (ğ‘–, ğ‘¡ğ‘  )also has a superscript of the layer numberğ‘™, which is ignored for simplicity.\nand Eq. (4), the right-hand side of Eq. (6) can be rewritten as:\nğ’†(ğ‘™âˆ’1)\nğ‘¢ (ğ‘¡)Â· ğ’†(ğ‘™âˆ’1)\nğ‘– (ğ‘¡ğ‘  )+Î¦(ğ‘¡)Â· Î¦(ğ‘¡ğ‘  ), (7)\nwhere the first term denotes the user-item collaborative signal, and\nthe second term models the temporal effect according to Eq. (1).\nWith more stacked layers, collaborative signals and temporal ef-\nfects are entangled and tightly connected. Hence, the dot-product\nattention can characterize impacts of temporal collaborative signals.\nHereafter, we normalize the attention weights across all sampled\ninteractions by employing a softmax function:\nğœ‹ğ‘¢\nğ‘¡ (ğ‘–,ğ‘¡ğ‘  )= exp \u0000ğœ‹ğ‘¢\nğ‘¡ (ğ‘–,ğ‘¡ğ‘  )\u0001\nÃ\n(ğ‘–â€²,ğ‘¡â€²ğ‘  )âˆˆğ‘ğ‘¢ (ğ‘¡)exp \u0000ğœ‹ğ‘¢\nğ‘¡ (ğ‘–â€²,ğ‘¡â€²ğ‘  )\u0001 . (8)\nMoreover, the computation is implemented by packing the in-\nformation of all sampled interactions. To be more specific, we\nstack the information (Eq. (4)) of all sampled interactions as a\nmatrix ğ‘¯ (ğ‘™âˆ’1)\nNğ‘¢\n(ğ‘¡)âˆˆ R(ğ‘‘+ğ‘‘ğ‘‡ )Ã—ğ‘† , as illustrated2 in Figure 3. We de-\nnote ğ‘² (ğ‘™âˆ’1)\nğ‘¢ (ğ‘¡)= ğ‘¾ (ğ‘™)\nğ‘˜ ğ‘¯ (ğ‘™âˆ’1)\nNğ‘¢\n(ğ‘¡), ğ‘½ (ğ‘™âˆ’1)\nğ‘¢ (ğ‘¡)= ğ‘¾ (ğ‘™)\nğ‘£ ğ‘¯ (ğ‘™âˆ’1)\nNğ‘¢\n(ğ‘¡)and\nğ’’(ğ‘™âˆ’1)\nğ‘¢ (ğ‘¡)= ğ‘¾ (ğ‘™)\nğ‘ ğ’‰(ğ‘™âˆ’1)\nğ‘¢ (ğ‘¡), which are respectively the key, value\nand query input for the temporal collaborative attention module.\nWe illustrate this in Figure 3 as green blocks. For simplicity and\nwithout ambiguity, we ignore the superscripts and time ğ‘¡ and com-\nbine Eq. (6) and Eq. (8). Then, we can rewrite the Eq. (5) as:\nğ’†Nğ‘¢ = ğ‘½ğ‘¢ Â·Softmax\n \nğ‘²âŠ¤ğ‘¢ ğ’’ğ‘¢âˆšï¸\nğ‘‘+ğ‘‘ğ‘‡\n!\n, (9)\nwhich is in the form of dot-product attention in Transformer [38].\nTherefore, we can safely apply the multi-head attention operation\nand concatenate the output from each head as the information for\naggregation, which is presented in Figure 3. Note that our attention\nis not a self-attention but a temporal collaborative attention, which\njointly models user-item interactions and temporal information.\n4.2.4 Information Aggregation. To output the temporal node em-\nbedding, the final step of a TCT layer is to aggregate the query\ninformation in Eq. (3) and the neighbor information in Eq. (5). We\nconcatenate and send them to a feed-forward neural network (FFN):\nğ’†(ğ‘™)\nğ‘¢ (ğ‘¡)= FFN\n\u0010\nğ’†(ğ‘™)\nNğ‘¢\n(ğ‘¡)âˆ¥ğ’‰(ğ‘™âˆ’1)\nğ‘¢ (ğ‘¡)\n\u0011\n, (10)\n2In figure 3, an embedding is a row vector, while in notations, it is a column vector.\nwhere ğ’†(ğ‘™)\nğ‘¢ (ğ‘¡)is the temporal embedding of ğ‘¢at ğ‘¡ on ğ‘™-th layer, and\nFFN consists of two linear transformation layers with a ReLU acti-\nvation function in between [38]. The output temporal embedding\nğ’†(ğ‘™)\nğ‘¢ (ğ‘¡)can either be sent to the next layer or output as the final\ntemporal node embedding for prediction.\n4.2.5 Generalization to items. Though we only present the TCT\nlayer from the user query perspective, it is analogous if the query\nis an item at a specific time. We only need to alternate the user\nquery information to the item query information, and change the\nneighbor information in Eq. (4) and Eq. (5) accordingly as user-time\npairs. Then, we can make an inference of the temporal embedding\nof item ğ‘– at time ğ‘¡ as ğ’†(ğ‘™)\nğ‘– (ğ‘¡), which is sent to the next layer.\n4.3 Model Prediction\nThe TGSRec model consists of ğ¿TCT layers. For each test triplet\n(ğ‘¢,ğ‘–,ğ‘¡ ), it yields temporal embeddings for both ğ‘¢and ğ‘– at ğ‘¡ on the\nlast TCT layer, denoting as ğ’†(ğ¿)\nğ‘¢ (ğ‘¡)and ğ’†(ğ¿)\nğ‘– (ğ‘¡), respectively. Then,\nthe prediction score is:\nğ‘Ÿ(ğ‘¢,ğ‘–,ğ‘¡ )= ğ’†(ğ¿)\nğ‘¢ (ğ‘¡)Â· ğ’†(ğ¿)\nğ‘– (ğ‘¡), (11)\nwhere ğ‘Ÿ(ğ‘¢,ğ‘–,ğ‘¡ )denotes the score to recommend ğ‘– for ğ‘¢ at time ğ‘¡.\nWith the generalized continuous-time embeddings and the pro-\nposed TCT layers, we can generalize and infer user/item embed-\ndings at any timestamp, thus making multiple steps recommenda-\ntion feasible while existing work only predicts next item. Recall\nthat based on the Definition 3.3, we recommend each user a ranking\nlist of items at the given timestamp. Therefore, we use Eq. (11) to\ncalculate scores of all candidate items and sort them by scores.\n4.4 Model Optimization\nTo learn the model parameters, we use the pairwise BPR loss [34],\nwhich is widely used for top-N recommendation. The pairwise BPR\nloss assumes that the observed implicit feedback items have greater\nprediction scores than those unobserved and is also designed for\nranking based top-N recommendation. The objective function is:\nLğ‘ğ‘ğ‘Ÿ =\nâˆ‘ï¸\n(ğ‘¢,ğ‘–,ğ‘—,ğ‘¡ )âˆˆOğ‘‡\nâˆ’logğœ(ğ‘Ÿ(ğ‘¢,ğ‘–,ğ‘¡ )âˆ’ğ‘Ÿ(ğ‘¢,ğ‘—,ğ‘¡ ))+ğœ†||Î˜||2\n2, (12)\nwhere Oğ‘‡ denotes the training samples, Î˜ includes all learnable\nparameter, and ğœ(Â·)is a sigmoid function. The training samples\nOğ‘‡ = {(ğ‘¢,ğ‘–,ğ‘—,ğ‘¡ )|(ğ‘¢,ğ‘–,ğ‘¡ )âˆˆE ğ‘‡ ,ğ‘— âˆˆI\\I ğ‘¢ (ğ‘¡)}, where the positive\ninteraction (ğ‘¢,ğ‘–,ğ‘¡ )comes from the edge set Eğ‘‡ of CTBG, the neg-\native item ğ‘— is sampled from unobserved items I\\Iğ‘¢ (ğ‘¡)of user\nğ‘¢ at timestamp ğ‘¡; Î˜ includes long-term embedding ğ¸, time em-\nbedding parameter ğœ”, and all linear transformation matrices. The\nloss is optimized via mini-batch Adam [13] with adaptive learning\nrate. Alternatively, we can optimize the model with a Binary Cross\nEntropy (BCE) loss as:\nLğ‘ğ‘ğ‘’ =\nâˆ‘ï¸\n(ğ‘¢,ğ‘–,ğ‘—,ğ‘¡ )âˆˆOğ‘‡\nlog ğœ(ğ‘Ÿ(ğ‘¢,ğ‘–,ğ‘¡ ))+log ğœ(1 âˆ’ğ‘Ÿ(ğ‘¢,ğ‘—,ğ‘¡ ))+ğœ†||Î˜||2\n2,\n(13)\nwhich is compared with BPR loss in experiments.\n5 EXPERIMENTS\nIn this section, we present the experimental setups and results to\ndemonstrate the effectiveness of TGSRec. The experiments answer\nthe following Research Questions (RQs):\nâ€¢RQ1: Does TGSRec yield better recommendation?\nâ€¢RQ2: How do different hyper-parameters (e.g., number of neigh-\nbors ğ‘†, etc.) affect the performance of TGSRec?\nâ€¢RQ3: How do different modules ( e.g., temporal collaborative\nattention, etc.) affect the performance of TGSRec?\nâ€¢RQ4: Can TGSRec effectively unify sequential patterns and tem-\nporal collaborative signals? (Reveal temporal correlations)\n5.1 Datasets\nWe conduct our experiments on four Amazon review datasets [29]\nand MovieLens ML-100K dataset [6]. The Amazon datasets are col-\nlected from different domains3, from the Amazon website during\nMay 1996 to July 2014. The Movie Lens dataset is collected from\nSeptember 19th, 1997 through April 22nd, 1998. We use Unix times-\ntamps on all datasets. For each dataset, we chronologically split for\ntrain/validation/test in 80%/10%/10% ratio based on the interaction\ntimestamps. More details, such as data descriptions and statistics,\nare presented in the Table 1. We can find amazon datasets are much\nsparser and their time spans are much longer compared with ML-\n100K dataset. For Amazon related datasets, the time intervals of\nsuccessive interactions are typically in days, while ML-100k has\nshorter time intervals, ranging from seconds to days.\nTable 1: Statistics of datasets.\nDataset Toys Baby Tools Music ML100K\n#Users 17,946 17,739 15,920 4,652 943\n#Items 11,639 6,876 10,043 3,051 1,682\n#Edges 154,793 146,775 127,784 54,932 48,569\n#Train 134,632 128,833 107,684 51,765 80,003\n#Valid 11,283 10,191 10,847 2,183 1,516\n#Test 8,878 7,751 9,253 984 1,344\nDensity 0.07% 0.12% 0.08% 0.38% 6.30%\nAvg. Int. 85 days 61 days 123 days 104 days 4.8 hours\nâ€œAv. Int. â€ denotes average time interval.\n5.2 Experimental Settings\n5.2.1 Baselines. We compared TGSRec with the state-of-the-art\nmethods in three different groups. Static models: Static models ig-\nnore the temporal information and generate static user/item embed-\ndings for a recommendation. We compare with the most standard\nbaseline BPRMF [34], and also compare with a recent GNN-based\nmodel LightGCN [7]. Temporal models: We compare some rele-\nvant temporal methods, such as CTDNE [30] and one recent model\nTiSASRec [19], which utilize time information. We also try to com-\npare with JODIE [17]. However, we do not report it because has\nout-of-memory errors on most datasets. Transformer-based SR\nmodels: Since our model is built upon the transformer, we mainly\nfocus on comparing with the recent transformer-based SR methods,\n3https://jmcauley.ucsd.edu/data/amazon/\nTable 2: Overall Performance w.r.t. Recall@{10,20} and MRR.\nDatasets Metric BPR LightGCN SR-GNN GRU4Rec Caser SSE-PT BERT4Rec SASRec TiSASRec CDTNE TGSRec Improv.\nRecall@10 0.0021 0.0016 0.0020 0.0274 0.0138 0.1213 0.1273 0.1452 0.1361 0.0016 0.3650 0.2198\nRecall@20 0.0036 0.0026 0.0033 0.0288 0.0238 0.1719 0.1865 0.2044 0.1931 0.0045 0.3714 0.1670Toys\nMRR 0.0024 0.0018 0.0018 0.0277 0.0082 0.0595 0.0643 0.0732 0.0671 0.0025 0.3661 0.2929\nRecall@10 0.0028 0.0036 0.0030 0.0036 0.0077 0.0911 0.0884 0.0975 0.1040 0.0218 0.2235 0.1195\nRecall@20 0.0039 0.0045 0.0062 0.0048 0.0193 0.1418 0.1634 0.1610 0.1662 0.0292 0.2295 0.0663Baby\nMRR 0.0019 0.0024 0.0024 0.0028 0.0071 0.0434 0.0511 0.0455 0.0521 0.0157 0.2147 0.1626\nRecall@10 0.0023 0.0021 0.0051 0.0048 0.0077 0.0775 0.1296 0.0913 0.0946 0.0186 0.2457 0.1161\nRecall@20 0.0036 0.0035 0.0092 0.0059 0.0161 0.1155 0.1784 0.1337 0.1356 0.0271 0.2559 0.0775Tools\nMRR 0.0026 0.0023 0.0028 0.0051 0.0068 0.0419 0.0628 0.0460 0.0480 0.0203 0.2468 0.1840\nRecall@10 0.0122 0.0142 0.0051 0.0549 0.0183 0.0915 0.1352 0.1372 0.1372 0.0071 0.5935 0.4563\nRecall@20 0.0152 0.0183 0.0092 0.0589 0.0346 0.1494 0.2093 0.2094 0.1951 0.0163 0.5986 0.3892Music\nMRR 0.0057 0.0064 0.0028 0.0540 0.0106 0.0423 0.0824 0.0768 0.0681 0.0037 0.3820 0.2996\nRecall@10 0.0461 0.0565 0.0045 0.0996 0.0246 0.1079 0.1116 0.09450 0.1332 0.0350 0.3118 0.1786\nRecall@20 0.0766 0.0960 0.0060 0.1168 0.0417 0.1801 0.1786 0.1808 0.2232 0.0536 0.3252 0.1020ML100k\nMRR 0.0213 0.0252 0.0012 0.0938 0.0147 0.0519 0.0600 0.0448 0.0605 0.0162 0.2416 0.1478\n0.0\n0.1\n0.2\n0.3\n.0743 .0666 .0816 .0754\n.3649Toys\nBERT4Rec\nSSE-PT\nSASRec\nTiSASRec\nTGSRec\n(a) NDCG@10 in Toys\n0.00\n0.05\n0.10\n0.15\n0.20\n.0484 .0464 .0482 .0546\n.2157Baby\nBERT4Rec\nSSE-PT\nSASRec\nTiSASRec\nTGSRec (b) NDCG@10 in Baby\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n.0627\n.0440 .0496 .0521\n.2450Tools\nBERT4Rec\nSSE-PT\nSASRec\nTiSASRec\nTGSRec (c) NDCG@10 in Tools\n0.0\n0.1\n0.2\n0.3\n0.4\n.0823\n.0445\n.0813 .0758\n.4296Music\nBERT4Rec\nSSE-PT\nSASRec\nTiSASRec\nTGSRec (d) NDCG@10 in Music\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n.0608 .0542 .0451\n.0645\n.2565ML100K\nBERT4Rec\nSSE-PT\nSASRec\nTiSASRec\nTGSRec (e) NDCG@10 in ML100K\nFigure 4: NDCG@10 Performance in all Datasets. We ignore other methods because of their low values.\nwhich are SASRec [12], BERT4Rec [36], SSE-PT [44], and TiSAS-\nRec [19]. Other SR models : In addition, we also compare with\nother type of SR models, i.e., FPMC [35], GRU4Rec [9], Caser [37],\nand SR-GNN [45], for comprehensive study.\nFor each testing interaction (ğ‘¢,ğ‘–,ğ‘¡ ğ‘¡ğ‘’ğ‘ ğ‘¡ ), our continuous-time se-\nquential recommendation setting allows models to use any history\ninteractions {(ğ‘¢,ğ‘–,ğ‘¡ )|ğ‘¡ < ğ‘¡ğ‘¡ğ‘’ğ‘ ğ‘¡ }during the prediction stage, regard-\nless of whether the historical interactions are in training portion,\nvalidation part or even in testing set. However, all parameters of\nmodels are only learned from the training data.\nWe implementTGSRec with Pytorch in a Nvidia 1080Ti GPU. We\ngrid search all parameters and report the test performance based\non the best validation results. For all models, we search for the\ndimensions of embeddings ğ‘‘ in range of [8,16,32,64]and we tune\nthe learning rate in [10âˆ’2,10âˆ’3,10âˆ’4], search the L2 regularization\nweight from [5 Ã—10âˆ’1,10âˆ’1,10âˆ’2,10âˆ’3]. For sequential methods,\nwe search the maximum length of sequence in [50,100], number of\nlayers from [1,2,3], and number of heads in [1,2,4].\n5.2.2 Evaluation Protocol. All models will generate a ranking list\nof items for each testing interaction. Each evaluation metric is av-\neraged over the total number of interactions as the final reported\nresult. In order to accelerate the evaluation, we sample 1,000 nega-\ntive items for evaluation instead of full set of negative items. For\neach interaction (ğ‘¢,ğ‘–,ğ‘¡ )in validation/test sets, we treat items thatğ‘¢\nhas no interactions with before ğ‘¡ as negative items. Regarding the\nsampling bias for evaluation [16], we apply the unbiased estimator\nin [16] to correct the sampled ranks. We evaluate the top-N rec-\nommendation performance by standard ranking-based evaluation\nmetrics Recall@N, NDCG@N, and Mean Reciprocal Rank (MRR) . We\nset N to be either 10 or 20 for a comprehensive comparison.\n5.3 Performance Comparison (RQ1)\nWe compare the performance of all models and demonstrate the\nsuperiority of TGSRec. We report the Recall and MRR of all models\nin Table 2. Additionally, we visualize the comparisons of NDCG in\nFigure 4. We have the following observations:\nâ€¢TGSRec consistently and significantly outperforms all baselines\nin all datasets. In particular, for absolute performance improve-\nment gains relative to the 2nd best, TGSRec achieves 22.51%,\n16.90% and 22.15% absolute gains at recall@10, recall@20, and\nMRR, respectively. TGSRec also significantly outperforms others\nin NDCG, as shown in Figure 4. Several factors together deter-\nmine the superiority of TGSRec: (1) TGSRec captures temporal\ncollaborative signals; (2) TGSRec explicitly expresses temporal\neffects; and (3) TGSRec stacks multiple TCT layers to propagate\nthe information.\nâ€¢Those static methods achieve the worst performance among all\nmodels. A simple GRU4Rec even performs 10 times better than\nthem. This indicates that static embeddings fail to utilize the\ntemporal information, limiting its recommendation ability in SR.\nThus, it is important to model dynamics.\nâ€¢The CDTNE performs better than Caser and GRU4Rec in Tools\nand Baby datasets. This suggests the benefit of modeling temporal\nTable 3: Ablation analysis (Recall@10) on five datasets. Bold\nscore indicates performance better than the default version,\nwhile â†“indicates a performance drop more than 50%.\nArchitecture Toys Baby Tools Music ML100K\n(0) Default 0.3649 0.2235 0.3623 0.5935 0.3118\n(1) Mean 0.0027 â†“ 0.0210â†“ 0.0055â†“ 0.0051â†“ 0.0647â†“\n(2) LSTM 0.0991 â†“ 0.1237 0.1266 â†“ 0.3740 0.3088\n(3) Fixed ğœ” 0.0854â†“ 0.0944â†“ 0.0910â†“ 0.3679 0.2789\n(4) Position 0.0380 â†“ 0.0243â†“ 0.0209â†“ 0.0742â†“ 0.0878â†“\n(5) Empty 0.0139 â†“ 0.0240â†“ 0.0018â†“ 0.0346â†“ 0.0603â†“\n(6) BCELoss 0.2200 0.1916 0.1763 â†“ 0.4624 0.3542\ninformation with a graph. But it is still much worse than those\ntransformer-based methods, which again proves the strength\nof transformer in encoding sequences. We also notice the poor\nperformance of SR-GNN. We analyze the data and find time\nintervals between successive interactions vary a lot. Since SR-\nGNN is originally designed for session-based sequences, it is not\nsuitable for SR with a long time span.\nâ€¢The transformer-based SR methods consistently outperform all\nother types of baselines, which demonstrates the effectiveness of\nusing transformer structure to encode sequence. Among them,\nTiSASRec is better than SASRec on two datasets, which proves\nthe effectiveness of using time information. But it is still far worse\nthan TGSRec. The reason is twofold. One is that only the interval\ninformation is not enough to unify the temporal information\nwith sequential patterns. The other is that the proposed temporal\ncollaborative attention in TCT layer captures more precise and\ngeneralized temporal effects. We find that BERT4Rec is better\nthan the other baselines on the Tools dataset but not better on\nother datasets. Since the main difference between BERT4Rec and\nSASRec is the bi-directional sequence encoding, it may break\ncausal relations among items within a sequence. The TGSRec\nperforms much better than SR models, showing the necessity of\nunifying sequential patterns and temporal collaborative signals.\n5.4 Parameter Sensitivity (RQ2)\nIn this section, we conduct sensitivity analyses of the hyperparame-\nters of TGSRec, including the number of layersğ¿, embedding sizeğ‘‘,\nand the number of neighborsğ‘†. The results are reported in Figure 5.\nThe number of layers. The number of TCT layers ğ¿is searched\nfrom {0,1,2}. The results are shown in the top row of Figure 5.\nWhen ğ¿ = 0, TGSRec has no TCT layer, thus unable to infer tem-\nporal embeddings. We can observe it performs the worst on all\ndataset, which justify the benefit of temporal inference. When\nğ¿= 1, it makes temporal inference, but without propagation to the\nnext layer. Therefore, it still performs worse than ğ¿ = 2 on most\ndatasets. When ğ¿ = 2, it can not only make temporal inference,\nbut also propagate the information to capture high-order signals,\nwhich alleviates the data sparsity problem.\nEmbedding size. The embedding size ğ‘‘ of TCT layers is searched\nfrom {8,16,32,64}, which is presented at the mid-row in Figure 5.\nWe can find that the performance increases as the embedding\nTop - # of layers: ğ¿\nMid - embedding size: ğ‘‘\nBottom - # of neighbors: ğ‘†\nRecall@10Recall@10 Recall@10\nToys Baby Tools Music ML100K\nFigure 5: Recall@10 w.r.t. ğ¿,ğ‘‘ and ğ‘† on 5 datasets.\nsize enlarges. However, when the embedding size is too large, e.g.,\nğ‘‘ = 64, the performance drops, which results from the over-fitting\nproblem because of too many parameters.\nNumber of neighbors. The number of neighbors ğ‘† is searched in\n{5,10,20}, which is illustrated in the bottom row of Figure 5. We\ncan observe that TGSRec has performance gains on most datasets\nas the number of neighbors grows. It is because more neighbors\ncan provide more information for encoding both sequences and\ntemporal collaborative signals.\n5.5 Ablation Study (RQ3 & RQ4)\nIn this section, we conduct experiments to analyze different compo-\nnents in TGSRec. We develop several variants to better understand\ntheir effectiveness. Table 3 shows the performance w.r.t. Recall@10\nof the default TGSRec and other variants. We label each row with\nan index number for quick reference. The default isTGSRec with all\ncomponents and labeled as 0. We develop the variants by substitut-\ning some components, which are temporal collaborative attention\n(1-2), continuous-time embeddings (3-5), and loss function (6):\nTemporal collaborative attention. We replace the proposed tem-\nporal collaborative attention of sampled neighbors with a mean\npooling or LSTM module, both of which are widely used to encode\nsequences. Results are labeled as (1)and (2)in Table 3. We can\nobserve that substituting collaborative attention with a mean pool-\ning layer severely spoils the performance. Compared with that, the\nadoption of LSTM is much better, indicating the necessity of encod-\ning sequential patterns by considering item transitions. However,\nboth of them are worse than the default one, which implies the ad-\nvantage of temporal collaborative attention in encoding sequences.\nContinuous-time embedding. We use three variants to verify\nthe efficacy of the time mapping function Î¦. The first variant is\nthat we sample ğœ”in Eq. (2) directly from a normal distribution. The\nsecond and third variants replace the Î¦ with a learnable positional\nembedding as in [ 12] and emptying all zeros, respectively. The\nresults are labeled as (3)âˆ’( 5)in Table 3. Because of the better per-\nformance of position embedding compared with empty embedding,\nwe can conclude that TGSRec has the ability to encode sequen-\ntial patterns. In addition, we also find that even a fixed ğœ” to learn\nTable 4: Variants of Temporal Information Construction\nVariant Toys Baby Tools Music ML100K\nTGSRec 0.3649 0.2235 0.3623 0.5935 0.3118\nUw/o T 0.0103 0.0138 0.0106 0.0112 0.1555\nIw/o T 0.1013 0.0961 0.0836 0.2785 0.2336\nUltimate Prince\nThe Loon\nLady Mr Johnson\nTodd Smith\nStadium Arcadium\nCarnavas\nCrook By Da Book\nPimpalation\nLetoyaOkonokos\nHistorical Interacted Music (left to right: oldest to latest)\n+1h\n+1d\n+5d\n+10d\n+30d\nnext\n+60d\n+100dTime Increment\n0.0\n0.2\n0.4\n0.6\nFigure 6: Temporal Attention Weights Visualization\nthe time embedding can significantly outperform the position em-\nbedding, indicating the necessity of using the temporal kernel to\ncapture temporal effects in sequences. Moreover, the default ver-\nsion, using a trainable ğœ”, achieves the best performance, which\nindicates its capacity to learn temporal effects from data.\nLoss function. We also compare BPR loss and BCE Loss, which\nis labeled as (6)in Table 3. The results indicate that the BCE loss\nperforms inferior to BPR loss, except for the ML100K dataset. This\nis because BPR loss is optimized for ranking while BCE loss is de-\nsigned for binary classification.\n5.6 Temporal Correlations (RQ4)\nThough we have already indicated the answer of RQ4 in Sec. 5.5, this\nsection also conducts detailed analyses of the temporal correlation\nwithin sequences to directly answer RQ4.\n5.6.1 Temporal Information Construction. We develop two variants\nby dismissing the time vector in either Eq. (3) or Eq. (4), i.e., users\nwithout time vectors or items without time vectors. The results\nare presented in Table 4. The observations are two-fold. Firstly, the\nperformance of items without time is better than users without time.\nIt implies that the temporal inference of user embeddings are rather\nimportant, which matches the intuition that the preference of users\nare dynamic while items are relatively more static. Secondly, the\nperformance deteriorates significantly in both variants, indicating\nagain TGSRec is able to model temporal effects of collaborative\nsignals while also encoding sequences.\n5.6.2 Temporal Attention Weights Visualization. We visualize the\nattention weights of TGSRec on the Music dataset for a user, which\nis shown in Figure 6. Each row is associated with an increment\n(â€˜hâ€™ for hour and â€˜dâ€™ for day) from the last interactive timestamp\nğ‘‡ = 1159142400, where â€˜nextâ€™ denotes the timestamp (ğ‘‡+34d) for\nTable 5: Recommendation w.r.t. time increments after the\nlast interaction at timestamp ğ‘‡ = 1159142400. â€˜nextâ€™ is the\ntimestamp of the test interaction. The ground truth item is\nin red color. Items also predicted by SASRec and TiSASRec\nare in blue color.\nTime Rank-1 Rank-2 Rank-3 Rank-4\nT+5d Letoya H. of Blue L. Ult. Prince Veneer\nT+30d J. of A Gemini Living Lgds. Killing Joke Crane Wife\nnext Buf. S.F. Killing Joke Empire Stadium Arc.\nT+60d D. of Future P. Even Now L. Mks. Wd. Przts. Author\nSAS. Crane Wife Empire H. Fna. Are You in Rev.\nTiSAS. Crane Wife Empire WTE. P. S. Stadium Arc.\nthe test interaction. Each column is associated with an item. We can\nobserve that the attention weights for items are dynamic at different\ntimestamps, which indicates the temporal inference characteristics\nof TGSRec. Moreover, the time increments can be arbitrary values,\nwhich verifies its continuity.\n5.6.3 Recommendation Results. Besides the attention visualization,\nwe also present a part of the recommendation results of the same\nuser in Table 5. Additionally, we also show the results of SASRec\nand TiSASRec, which only leverage sequential patterns. We find\nthat only TGSRec can predict the ground truth item (Killing Joke )\nin top-4 predictions at the time of interests. When time (e.g.,ğ‘‡+30d)\nbecomes close to the predicting timestampe â€˜nextâ€™ (i.e., ğ‘‡+34d), the\nground truth item appears in the top-4 predictions. We can observe\nthat the top predicted items from SASRec are also recommended by\nTGSRec, though in lower ranks. It again proves that TGSRec can\nunify sequential patterns and temporal collaborative signals.\n6 CONCLUSION\nIn this paper, we design a new SR model, TGSRec, to unify sequen-\ntial patterns and temporal collaborative signals. TGSRec is defined\nupon the proposed CTBG. We apply a temporal kernel to map\ncontinuous timestamps on edges to vectors. Then, we introduce\nthe TCT layer, which can infer temporal embeddings of nodes. It\nsamples neighbors and learns attention weights to aggregate both\nnode embeddings and time vectors. In this way, a TCT layer is able\nto encode both sequential patterns and collaborative signals, as\nwell as reveal temporal effects. Extensive experiments on five real-\nworld datasets demonstrate the effectiveness of TGSRec. TGSRec\nsignificantly outperforms existing transformer-based sequential\nrecommendation models. Moreover, the ablation study and detailed\nanalyses verify the efficacy of those components in TGSRec. In\nconclusion, TGSRec is a better framework to solve the SR problem\nwith temporal information.\n7 ACKNOWLEDGMENTS\nThis work is supported in part by NSF under grants III-1763325, III-\n1909323, III-2106758, and SaTC-1930941. This work is also partially\nsupported by NSF through grant IIS-1763365 and by UC Davis.\nThis work is also funded in part by the National Natural Science\nFoundation of China Projects No. U1936213\nREFERENCES\n[1] Rianne van den Berg, Thomas N Kipf, and Max Welling. 2017. Graph convolu-\ntional matrix completion. arXiv preprint arXiv:1706.02263 (2017).\n[2] Xu Chen, Hongteng Xu, Yongfeng Zhang, Jiaxi Tang, Yixin Cao, Zheng Qin, and\nHongyuan Zha. 2018. Sequential recommendation with user memory networks.\nIn WSDM. 108â€“116.\n[3] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding.arXiv\npreprint (2018).\n[4] Robin Devooght and Hugues Bersini. 2017. Long and short-term recommenda-\ntions with recurrent neural networks. In Proceedings of the 25th Conference on\nUser Modeling, Adaptation and Personalization . 13â€“21.\n[5] Ziwei Fan, Zhiwei Liu, Lei Zheng Zheng, Shen Wang, and S. Philip Yu. 2021.\nModeling Sequences as Distributions with Uncertainty for Sequential Recommen-\ndation. In Proceedings of the 30th ACM International Conference on Information\nand Knowledge Management . ACM.\n[6] F Maxwell Harper and Joseph A Konstan. 2015. The movielens datasets: History\nand context. TIIS 5, 4 (2015), 1â€“19.\n[7] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, YongDong Zhang, and Meng\nWang. 2020. LightGCN: Simplifying and Powering Graph Convolution Network\nfor Recommendation. In SIGIR (SIGIR â€™20) . Association for Computing Machinery,\nNew York, NY, USA, 639â€“648.\n[8] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng\nChua. 2017. Neural collaborative filtering. In WWW. 173â€“182.\n[9] BalÃ¡zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\n2015. Session-based recommendations with recurrent neural networks. arXiv\npreprint arXiv:1511.06939 (2015).\n[10] Sepp Hochreiter and JÃ¼rgen Schmidhuber. 1997. Long short-term memory.Neural\ncomputation 9, 8 (1997), 1735â€“1780.\n[11] Wendi Ji, Keqiang Wang, Xiaoling Wang, TingWei Chen, and Alexandra Cristea.\n2020. Hybrid Sequential Recommender via Time-aware Attentive Memory Net-\nwork. CIKM (2020).\n[12] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recom-\nmendation. In ICDM. IEEE, 197â€“206.\n[13] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti-\nmization. In ICLR, Yoshua Bengio and Yann LeCun (Eds.).\n[14] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with\nGraph Convolutional Networks. In ICLR.\n[15] Yehuda Koren. 2009. Collaborative filtering with temporal dynamics. In SIGKDD.\n447â€“456.\n[16] Walid Krichene and Steffen Rendle. 2020. On Sampled Metrics for Item Recom-\nmendation. In SIGKDD. 1748â€“1757.\n[17] Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. Predicting Dynamic Em-\nbedding Trajectory in Temporal Interaction Networks. In KDD. ACM.\n[18] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. 2017.\nNeural attentive session-based recommendation. In CIKM. 1419â€“1428.\n[19] Jiacheng Li, Yujie Wang, and Julian McAuley. 2020. Time Interval Aware Self-\nAttention for Sequential Recommendation. In WSDM. 322â€“330.\n[20] Xiaohan Li, Mengqi Zhang, Shu Wu, Zheng Liu, Liang Wang, and Philip S Yu.\n2020. Dynamic Graph Collaborative Filtering. ICDM.\n[21] Dawen Liang, Laurent Charlin, James McInerney, and David M Blei. 2016. Mod-\neling user exposure in recommendation. In WWW. 951â€“961.\n[22] Ye Liu, Yao Wan, Jianguo Zhang, Wenting Zhao, and S Yu Philip. 2021. Enriching\nNon-Autoregressive Transformer with Syntactic and Semantic Structures for\nNeural Machine Translation. InProceedings of the 16th Conference of the European\nChapter of the Association for Computational Linguistics: Main Volume . 1235â€“1244.\n[23] Zhiwei Liu, Ziwei Fan, Yu Wang, and Philip S. Yu. 2021. Augmenting Sequential\nRecommendation with Pseudo-Prior Items via Reversely Pre-training Trans-\nformer. In Proceedings of the 44th International ACM SIGIR Conference on Research\nand Development in Information Retrieval .\n[24] Zhiwei Liu, Xiaohan Li, Ziwei Fan, Stephen Guo, Kannan Achan, and Philip S.\nYu. 2020. Basket Recommendation with Multi-Intent Translation Graph Neural\nNetwork. In 2020 IEEE International Conference on Big Data (Big Data) . 728â€“737.\nhttps://doi.org/10.1109/BigData50022.2020.9377917\n[25] Zhiwei Liu, Mengting Wan, Stephen Guo, Kannan Achan, and Philip S Yu. 2020.\nBasConv: Aggregating Heterogeneous Interactions for Basket Recommendation\nwith Graph Convolutional Neural Network. In Proceedings of the 2020 SIAM\nInternational Conference on Data Mining . SIAM, 64â€“72.\n[26] Zhiwei Liu, Lei Zheng, Jiawei Zhang, Jiayu Han, and Philip S. Yu. 2019. JSCN: Joint\nSpectral Convolutional Network for Cross Domain Recommendation. Bigdata\nabs/1910.08219 (2019).\n[27] Lynn H Loomis. 2013. Introduction to abstract harmonic analysis . Courier Corpo-\nration.\n[28] Jianxin Ma, Chang Zhou, Hongxia Yang, Peng Cui, Xin Wang, and Wenwu Zhu.\n2020. Disentangled Self-Supervision in Sequential Recommenders. In SIGKDD.\n483â€“491.\n[29] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel.\n2015. Image-based recommendations on styles and substitutes. In SIGIR. 43â€“52.\n[30] Giang Hoang Nguyen, John Boaz Lee, Ryan A Rossi, Nesreen K Ahmed, Eunyee\nKoh, and Sungchul Kim. 2018. Continuous-time dynamic network embeddings.\nIn The Web Conference . 969â€“976.\n[31] Bo Peng, Zhiyun Ren, Srinivasan Parthasarathy, and Xia Ning. 2020. M2: Mixed\nModels with Preferences, Popularities and Transitions for Next-Basket Recom-\nmendation. arXiv preprint arXiv:2004.01646 (2020).\n[32] Bo Peng, Zhiyun Ren, Srinivasan Parthasarathy, and Xia Ning. 2021. HAM:\nhybrid associations models for sequential recommendation. IEEE Transactions on\nKnowledge and Data Engineering .\n[33] Massimo Quadrana, Alexandros Karatzoglou, BalÃ¡zs Hidasi, and Paolo Cremonesi.\n2017. Personalizing session-based recommendations with hierarchical recurrent\nneural networks. In RecSys. 130â€“137.\n[34] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n2009. BPR: Bayesian personalized ranking from implicit feedback. In UAI. 452â€“\n461.\n[35] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factor-\nizing personalized markov chains for next-basket recommendation. In WWW.\n811â€“820.\n[36] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.\n2019. BERT4Rec: Sequential recommendation with bidirectional encoder repre-\nsentations from transformer. In CIKM. 1441â€“1450.\n[37] Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommendation\nvia convolutional sequence embedding. In WSDM. 565â€“573.\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS. 5998â€“6008.\n[39] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. Kgat:\nKnowledge graph attention network for recommendation. In SIGKDD. 950â€“958.\n[40] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.\nNeural Graph Collaborative Filtering. In SIGIR. 165â€“174.\n[41] Yu Wang, Zhiwei Liu, Ziwei Fan, Philip S Yu, and Lichao Sun. 2021. DSKReG: Dif-\nferentiable Sampling on Knowledge Graph forRecommendation with Relational\nGNN. In Proceedings of the 30th ACM International Conference on Information and\nKnowledge Management . ACM.\n[42] Chao-Yuan Wu, Amr Ahmed, Alex Beutel, Alexander J Smola, and How Jing.\n2017. Recurrent recommender networks. In WSDM. 495â€“503.\n[43] Jibang Wu, Renqin Cai, and Hongning Wang. 2020. DÃ©jÃ  vu: A Contextualized\nTemporal Attention Mechanism for Sequential Recommendation. In The Web\nConference. 2199â€“2209.\n[44] Liwei Wu, Shuqing Li, Cho-Jui Hsieh, and James Sharpnack. 2020. SSE-PT:\nSequential Recommendation Via Personalized Transformer. In RecSys. ACM,\n328â€“337.\n[45] Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. 2019.\nSession-based recommendation with graph neural networks. In AAAI, Vol. 33.\n346â€“353.\n[46] Liang Xiang, Quan Yuan, Shiwan Zhao, Li Chen, Xiatian Zhang, Qing Yang, and\nJimeng Sun. 2010. Temporal recommendation on graphs via long-and short-term\npreference fusion. In KDD. 723â€“732.\n[47] Liang Xiong, Xi Chen, Tzu-Kuo Huang, Jeff Schneider, and Jaime G Carbonell.\n2010. Temporal collaborative filtering with bayesian probabilistic tensor factor-\nization. In SDM. SIAM, 211â€“222.\n[48] Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan.\n2019. Self-attention with functional time representation learning. In NeurIPS.\n15915â€“15925.\n[49] Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan.\n2020. Inductive Representation Learning on Temporal Graphs. arXiv preprint\narXiv:2002.07962 (2020).\n[50] Wenwen Ye, Shuaiqiang Wang, Xu Chen, Xuepeng Wang, Zheng Qin, and Dawei\nYin. 2020. Time Matters: Sequential Recommendation with Complex Temporal\nInformation. In SIGIR. 1459â€“1468.\n[51] Feng Yu, Qiang Liu, Shu Wu, Liang Wang, and Tieniu Tan. 2016. A dynamic\nrecurrent model for next basket recommendation. In SIGIR. 729â€“732.\n[52] Jian-Guo Zhang, Kazuma Hashimoto, Yao Wan, Ye Liu, Caiming Xiong, and\nPhilip S Yu. 2021. Are Pretrained Transformers Robust in Intent Classification? A\nMissing Ingredient in Evaluation of Out-of-Scope Intent Detection.arXiv preprint\narXiv:2106.04564 (2021).\n[53] Yao Zhang, Yun Xiong, Xiangnan Kong, Zhuang Niu, and Yangyong Zhu. 2019.\nIGE+: A Framework for Learning Node Embeddings in Interaction Graphs. IEEE\nTransactions on Knowledge and Data Engineering (2019).\n[54] Yao Zhang, Yun Xiong, Xiangnan Kong, and Yangyong Zhu. 2017. Learning node\nembeddings in interaction graphs. In Proceedings of the 2017 ACM on Conference\non Information and Knowledge Management . 397â€“406.\n[55] Lei Zheng, Ziwei Fan, Chun-Ta Lu, Jiawei Zhang, and Philip S. Yu. 2019. Gated\nSpectral Units: Modeling Co-Evolving Patterns for Sequential Recommenda-\ntion. In ACM SIGIR. Association for Computing Machinery, New York, NY, USA,\n1077â€“1080."
}