{
  "title": "Global Adaptive Transformer for Cross-Subject Enhanced EEG Classification",
  "url": "https://openalex.org/W4380355219",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2480043917",
      "name": "Yonghao Song",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A1985865479",
      "name": "Qingqing Zheng",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Shenzhen Institutes of Advanced Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2106972883",
      "name": "Qiong Wang",
      "affiliations": [
        "Shenzhen Institutes of Advanced Technology",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2004916865",
      "name": "Xiaorong Gao",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2133318398",
      "name": "Pheng Ann Heng",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Shenzhen Institutes of Advanced Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2982126608",
    "https://openalex.org/W1547702425",
    "https://openalex.org/W3047208690",
    "https://openalex.org/W6735913928",
    "https://openalex.org/W2741907166",
    "https://openalex.org/W2760834907",
    "https://openalex.org/W4280581470",
    "https://openalex.org/W1996167318",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4376121133",
    "https://openalex.org/W4316660747",
    "https://openalex.org/W4229068725",
    "https://openalex.org/W3170753307",
    "https://openalex.org/W3167195439",
    "https://openalex.org/W2792724009",
    "https://openalex.org/W4214809163",
    "https://openalex.org/W2559463885",
    "https://openalex.org/W2888355470",
    "https://openalex.org/W2132360759",
    "https://openalex.org/W3046051368",
    "https://openalex.org/W2746829572",
    "https://openalex.org/W4296027702",
    "https://openalex.org/W3115305254",
    "https://openalex.org/W3080222908",
    "https://openalex.org/W3100059780",
    "https://openalex.org/W3017642177",
    "https://openalex.org/W2963727766",
    "https://openalex.org/W3041698047",
    "https://openalex.org/W3210269077",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3131225866",
    "https://openalex.org/W4316655758",
    "https://openalex.org/W4288061928",
    "https://openalex.org/W3195947465",
    "https://openalex.org/W3082367983",
    "https://openalex.org/W4304481492",
    "https://openalex.org/W4309164566",
    "https://openalex.org/W4282829678",
    "https://openalex.org/W4321021843",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W3102455230",
    "https://openalex.org/W4295521014",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Due to the individual difference, EEG signals from other subjects (source) can hardly be used to decode the mental intentions of the target subject. Although transfer learning methods have shown promising results, they still suffer from poor feature representation or neglect long-range dependencies. In light of these limitations, we propose Global Adaptive Transformer (GAT), an domain adaptation method to utilize source data for cross-subject enhancement. Our method uses parallel convolution to capture temporal and spatial features first. Then, we employ a novel attention-based adaptor that implicitly transfers source features to the target domain, emphasizing the global correlation of EEG features. We also use a discriminator to explicitly drive the reduction of marginal distribution discrepancy by learning against the feature extractor and the adaptor. Besides, an adaptive center loss is designed to align the conditional distribution. With the aligned source and target features, a classifier can be optimized to decode EEG signals. Experiments on two widely used EEG datasets demonstrate that our method outperforms state-of-the-art methods, primarily due to the effectiveness of the adaptor. These results indicate that GAT has good potential to enhance the practicality of BCI.",
  "full_text": "IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023 2767\nGlobal Adaptive Transformer for Cross-Subject\nEnhanced EEG Classification\nY onghao Song\n , Student Member, IEEE, Qingqing Zheng\n , Member, IEEE,\nQiong Wang\n , Xiaorong Gao\n , Member, IEEE, and Pheng-Ann Heng\n , Senior Member, IEEE\nAbstract— Due to the individual difference, EEG sig-\nnals from other subjects (source) can hardly be used\nto decode the mental intentions of the target subject.\nAlthough transfer learning methods have shown promising\nresults, they still suffer from poor feature representation\nor neglect long-range dependencies. In light of these lim-\nitations, we propose Global Adaptive Transformer (GAT),\nan domain adaptation method to utilize source data for\ncross-subject enhancement. Our method uses parallel con-\nvolution to capture temporal and spatial features first. Then,\nwe employ a novel attention-based adaptor that implicitly\ntransfers source features to the target domain, emphasiz-\ning the global correlation of EEG features. We also use a\ndiscriminator to explicitly drive the reduction of marginal\ndistribution discrepancy by learning against the feature\nextractor and the adaptor. Besides, an adaptive center\nloss is designed to align the conditional distribution. With\nthe aligned source and target features, a classifier can\nbe optimized to decode EEG signals. Experiments on two\nwidely used EEG datasets demonstrate that our method\noutperforms state-of-the-art methods, primarily due to the\neffectiveness of the adaptor. These results indicate that\nGAT has good potential to enhance the practicality of BCI.\nIndex Terms— EEG classification, domain adaptation,\ntransformer, brain-computer interface (BCI), motor imagery.\nManuscript received 18 April 2023; revised 30 May 2023; accepted\n3 June 2023. Date of publication 12 June 2023; date of current ver-\nsion 27 June 2023. This work was supported in part by the National\nNatural Science Foundation of China under Project U2241208, Project\n62206270, and Project 62171473; in part by the Guangdong Provincial\nBasic and Applied Basic Research Fund-Regional Joint Fund under\nProject 2020B1515130004 and Project 2021A1515110598; in part by\nthe Research Grants Council of the Hong Kong Special Administrative\nRegion, China, under Project T45-401/22-N; in part by the Key Research\nand Development Program of Ningxia under Grant 2022CMG02026;\nin part by the Shenzhen Science and Technology Program under\nProject JCYJ20200109114244249; and in part by the SIAT Innovation\nProgram for Excellent Y oung Researchers. (Corresponding authors:\nQingqing Zheng; Pheng-Ann Heng.)\nY onghao Song and Xiaorong Gao are with the Department of Biomed-\nical Engineering, School of Medicine, Tsinghua University, Beijing\n100084, China.\nQingqing Zheng and Qiong Wang are with the Guangdong Provincial\nKey Laboratory of Computer Vision and Virtual Reality Technology,\nShenzhen Institute of Advanced Technology, Chinese Academy of Sci-\nences, Shenzhen 518055, China (e-mail: qq.zheng@siat.ac.cn).\nPheng-Ann Heng is with the Department of Computer Science and\nEngineering and Institute of Medical Intelligence and XR, The Chi-\nnese University of Hong Kong, Hong Kong, China, and also with\nthe Guangdong Provincial Key Laboratory of Computer Vision and\nVirtual Reality Technology, Shenzhen Institute of Advanced Technol-\nogy, Chinese Academy of Sciences, Shenzhen 518055, China (e-mail:\npheng@cse.cuhk.edu.hk).\nDigital Object Identifier 10.1109/TNSRE.2023.3285309\nI. I NTRODUCTION\nB\nRAIN-COMPUTER interface (BCI) is a cutting-edge\ntechnology that bridges humans and external devices\nby allowing users to directly control machines with their\nintentions, which are decoded from brain signals of potential\nactivities [1]. Among many techniques for detecting brain\nsignals, electroencephalograph (EEG), as a kind of non-\ninvasive physiological signal, has been widely used to record\nthe voltage of multiple electrodes on the scalp by wearing\nan electrode cap. Due to its reliability and convenience,\nEEG-based BCI has broad prospects in many application\nfields in daily life, ranging from specific scenarios such as\nfunctional rehabilitation for patients with movement disor-\nders [2], sleep stage classification [3], and emotion regula-\ntion to general intelligent applications like brain-controlled\nsystems [4].\nNumerous machine learning methods have been intensively\ninvestigated for EEG classification, which plays a crucial\nrole in the performance of EEG-based BCI. However, the\nsignificant individual differences of EEG make it still very\nchallenging to learn a general model applicable across sub-\njects [5]. Conventional methods have to collect a large number\nof labeled samples over a few training sessions and then\nuse these data to train a new model for each subject since\nthere is no significant gain or even deterioration in clas-\nsification performance by directly using data from existing\nsubjects. Unfortunately, this demand is time-consuming and\nmay lead to poor user experience. Therefore, transfer learning\nis developed to leverage information from existing subjects and\nthus make up for the limitation of training data [6], [7], [8].\nAs a popular branch of transfer learning, domain adaptation\napproaches refer to the EEG samples from the new subject\nas the target domain and those from existing subjects as the\nsource domain [9]. They aim to align the feature distribution\nof the source domain with the target domain, thereby expand-\ning useful information in the target domain. For example,\nsome methods use statistical metrics to narrow the difference\nbetween the two domains, such as maximum mean discrepancy\n(MMD) and correlation alignment [10], [11]. Inspired by the\ngenerative adversarial network (GAN), approaches based on\nadversarial learning employ a discriminator to automatically\nreduce distribution discrepancy and learn domain-invariant\nfeatures [12], [13]. These domain-adaptive methods align\ndistributions by constraining the similarity of features or\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2768 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023\nstatistical metrics but ignoring the global dependencies within\nthe EEG context of different domains, leading to inefficient\nalignment.\nDuring the process of distribution alignment, feature repre-\nsentation is of particular importance. Recently, neural networks\nwith deep architectures and convolutional neural networks\n(CNN) have demonstrated strong capabilities in EEG feature\nlearning [14]. For example, the deep ConvNet in [15] employs\nthe combination of temporal and spatial convolution filters\nas the first convolution-pooling block and introduces three\nmore convolution-pooling blocks to further reduce the feature\ndimension before feeding the output to the classification layer.\nSimilarly, EEGNet [16] starts with a temporal convolution\nto learn frequency filters, then uses a depthwise convolution\nto extract frequency-specific spatial features, followed by a\nseparable convolution to learn temporal representation further.\nC2CM [17] also breaks the two-dimensional convolution into\ntwo separate one-dimensional layers and applies spatial filters\nto temporal-filtered features. However, these methods usually\nprioritize learning temporal filtering and then spatial filtering,\nwhich may fail to preserve both temporal and spatial structure\nsimultaneously.\nTo tackle the above issues, we propose a novel domain\nadaptation method called Global Adaptive Transformer (GAT)\nfor cross-subject enhanced EEG classification. The feature\nrepresentation of existing subjects is well aligned with GAT to\nhelp the classification performance of the target subject. GAT\ncomprises four components: feature extractor, global adaptor,\ndiscriminator, and classifier. We first design a parallel temporal\nand spatial convolution module to simultaneously preserve\nthe spatio-temporal structure information. The global adaptor,\na Transformer-based module, measures the non-local correla-\ntion of the target domain and implicitly maps to the source\ndomain. The discriminator distinguishes aligned features from\nthe source or target domain to drive marginal distribution\nalignment during training. We also design a loss function to\nreduce conditional distribution discrepancy. Finally, we adopt\na simple classifier optimized with target and aligned source\nfeatures for classification.\nTo sum up, our contributions are fourfold :\n• We propose a domain adaptation framework that uses\nattention and adversarial learning to address the issue of\nglobal dependencies and marginal distribution alignment.\n• We introduce an adaptive center loss to leverage the label\ninformation and further reduce the conditional distribu-\ntion discrepancy.\n• We investigate a parallel convolution module to learn\nrobust representation, which preserves the spatial-\ntemporal structure information.\n• We conduct extensive experiments on two real EEG\ndatasets to demonstrate the capability of our method to\nutilize the source domain for enhancement.\nThe remainder of this paper is organized as follows. Related\nworks are presented in Section II. The proposed method\nis illustrated in Section III. The details of implementation,\nexperiments, and results are shown in Section IV. A careful\ndiscussion is in Section V. Finally, we come to a conclusion\nin Section VI.\nII. R ELATED WORKS\nA. Machine Learning for EEG Classification\nA wide variety of machine learning methods are devoted\nto improving the performance of EEG decoding via fea-\nture extraction and classification. Traditional methods usu-\nally rely on prior knowledge to extract hand-crafted feature\nrepresentation, such as fast Fourier transform (FFT), con-\ntinuous wavelet transform (CWT) [18], and common spatial\npattern (CSP) [19]. In addition, Ang et al. [19] presented\nfiler bank CSP (FBCSP) by employing spatial information\nof multiple frequency bands to enhance category discrim-\nination. The extracted features are subsequently fed into\nlinear or nonlinear classifiers, such as linear discriminant\nanalysis (LDA), support vector machine (SVM), and multi-\nperceptron (MLP) [20], [21]. More recently, deep learning\nmethods have become increasingly popular in EEG decod-\ning. Sakhavi et al. [17] proposed CNN to learn the temporal\nfeatures of EEG for classification. Schirrmeister et al. [15]\ndesigned deep and shallow ConvNet to decode task-related\ninformation from EEG without hand-crafted features, achiev-\ning competitive performance as FBCSP. However, these meth-\nods usually have poor performance in cross-subject cases due\nto the large individual difference in EEG signals.\nB. Transfer Learning and Domain Adaptation\nTransfer learning is an important technique to deal with\ninter-subject variability in EEG [22]. It allows transferring of\nknowledge learned from existing subjects to reduce calibra-\ntion time and improve the target subject’s performance [23].\nTo solve the individual difference, some works learned features\nor data structures invariant across subjects. For example,\nLiu et al. [7] performed transfer learning by aligning the\nspatial pattern and covariance. Zanini et al. [24] presented\na Riemannian geometry framework to center the covariance\nmatrices of every domain with respect to a reference covari-\nance matrix. Zhang et al. [25] pre-trained a deep network\nwith the data of all available subjects excluding the target\nsubject and fine-tuned the fully-connected layer with the\ndata of the target subject. Zhang et al. [26] compared the\neffects of different transfer schemes in detail. Besides, domain\nadaptation gives a more interpretable way to align the distri-\nbution of the source and the target domain. Zhao et al. [12]\nmatched the deep representation obtained by CNN with the\nhelp of adversarial learning. Hong et al. [27] proposed a\ndynamic joint domain adaptation framework and reduced the\nmarginal and conditional distribution discrepancies by multiple\ndiscriminators.\nC. Attention Mechanism and Transformer\nRecently, Transformer based on the attention mechanism\nhas aroused great interest in the fields of computer vision and\nnatural language processing [28]. The attention mechanism\nenables models to capture long-term dependencies with no\nlimitation of sequence length due to its non-local character-\nistic [29]. Liu et al. [30] proposed a hierarchical Transformer\nwith sifted windows to serve as a general-purpose backbone\nfor computer vision, which achieved satisfactory performance\nSONG et al.: GLOBAL ADAPTIVE TRANSFORMER FOR CROSS-SUBJECT ENHANCED EEG CLASSIFICATION 2769\nFig. 1. The goal is to align the marginal and conditional discrepancies\nbetween the source and target domain caused by individual differences.\nThus we can use source data to enhance the classification performance\nof the target data. We consider global correlation within EEG for better\ndistribution alignments.\non the tasks of image classification, object detection, and\nsemantic segmentation. Wang et al. [31] presented a pyramid\nvision Transformer for dense prediction without convolutions.\nBagchi et al. [32] combined both CNN and Transformer and\ncombined them to learn both local temporal features and\ninter-region interactions. The long-range dependencies that\nTransformer can capture are rarely explored in EEG sequences,\nnot to mention the individual difference cases. Therefore,\nwe propose a global adaptive Transformer framework for\ncross-subject EEG classification, which encodes the non-local\ncorrelation of EEG between the source and target domains\nduring the domain adaptation.\nIII. M ETHODS\nA. Overview\nIn the context of EEG-based BCI, we assume the annotated\nEEG signals from the target subject and those from the\nsource subjects belong to two different but related domains,\nnamely, the target domain Dt and the source domain Ds.\nBoth domains share the same feature space and label space.\nLet\n{\n(xt\ni , yt\ni )\n}Nt\ni=1 ∈ Dt denote Nt trials of EEG signals\nin the target domain, where xt\ni ∈ RC×T is the ith EEG\ntrial with C electrode channels and T time samples, yi =\n{1, · · ·, M} ∈ZM is its corresponding label of M categories.\nSimilarly,\n{\n(xs\nj , ys\nj )\n}Ns\nj=1\n∈ Ds denote Ns EEG trials in the\nsource domain. In the real world, the sample size Nt is\nusually too small to train a reliable model for the target\nsubject, while the annotated data in the source domain is much\nmore than that in the target domain. However, due to the\nlarge individual difference and the special characteristics of\nEEG signals, both domains draw different joint distributions,\nnamely, P(xt , yt ) ̸= P(xs, ys), which can be further split into\nthe marginal distribution P(xt ) ̸= P(xs) and the conditional\ndistribution P(yt |xt ) ̸= P(ys|xs) as in Fig. 1. In this regard,\nhow to better reduce the distribution discrepancies and lever-\nage useful information from the source domain to improve\nthe decoding performance for the target domain remains a\nchallenging problem.\nTo tackle this problem, we propose a novel domain adapta-\ntion framework, GAT, for cross-subject enhanced EEG classi-\nfication. The overall framework of GAT is depicted in Fig. 2.\nThe whole structure can be divided into four components:\na feature extractor, an essential global adaptor, a domain\ndiscriminator, and a classifier. In the training phase, samples\nfrom different domains are fed into the feature extractor\nto learn feature maps for the source and target domains,\nrespectively. The feature extractor consists of two parallel\ntemporal and spatial convolutional branches, which allows\nfor simultaneously preserving both the temporal and spatial\nstructures for feature representation. With the features learned\nfrom different domains, a global adaptor subsequently encodes\nthe global dependencies in the target domain, and utilizes them\nto guide the transfer of features from the source domain to the\ntarget domain. After then, a domain discriminator is adopted\nto determine whether the features come from the source or the\ntarget domain. Based on the adversarial learning, the discrimi-\nnator is alternately updated and learns against the combination\nof the feature extractor and the global adaptor until a Nash\nequilibrium reaches [33]. In this way, the features learned\nfrom different domains are similar enough to confuse the\ndiscriminator, and thus, the marginal distribution discrepancy\nis gradually eliminated. Furthermore, an adaptive center loss\nis proposed to make use of the labeled information, which\npulls features from the same categories closer while pushing\naway features from different classes. Therefore, the conditional\ndistribution is also aligned, and the joint distributions can be\nconsidered similar between the source and the target domains.\nFinally, a classifier can be safely trained with the aligned\nfeatures from both domains for EEG decoding. In the test\nor online phase, we can directly use the well-learned feature\nextractor and the classifier to decode newly arrived EEG trials\nfor the target subject.\nB. Preprocessing\nTo avoid structural information loss, we use as few prepro-\ncessing operations as possible before feeding the raw EEG\ntrials into the model. Here, we independently apply bandpass\nfiltering and standardization to raw EEG trials for each subject.\nTo remove various high- or low-frequency artifacts and remain\nvaluable rhythms, we employ a 6-order Chebyshev filter to\nconstrain the frequency of EEG data to [W1, W2] Hz, based\non the paradigm. Then, a z-score standardization is performed\non the filtered data to reduce the non-stationarity with\nxo = xi − µ√\nσ2\n, (1)\nwhere xi and xo denote the band-pass filtered data and\nstandardized output, respectively. µ and σ2 represent the mean\nvalue and variance, which are calculated from the filtered\ntraining trials and used directly on the test data.\nC. Network Architecture\n1) Feature Extractor: After preprocessing, we construct a\nfeature extractor with convolutional layers to take as input\n2770 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023\nFig. 2. The overall framework consists of four components, a feature extractor with two parallel branches to capture temporal and spatial features,\na global adaptor to adapt global dependencies with attention mechanism, a discriminator to drive adversarial learning and a classifier. The adaptive\ncenter loss is given to reduce the conditional distribution discrepancy. Only the feature extractor and the classifier, connected by thick black lines,\nare used in the test phase.\nand learn robust feature representation from the source and the\ntarget EEG trials of size C × T . Since the dimensional units\nare different and C is much smaller than T , we decompose\nthe 2D convolution into two 1D convolutions along the spatial\nand temporal dimensions. As shown in Fig. 3(a), conventional\nfeature extractors typically apply the spatial convolution to\nthe output feature maps of the temporal convolution, and as a\nresult, the original spatial information is inevitably destroyed.\nTherefore, we propose a more efficient combination of parallel\ntemporal and spatial branches in Fig. 3(b). For the temporal\nbranch, the convolutional kernel is of size (k × 1 × n), where\nk denotes the number of feature maps, n denotes the kernel\nlength along the temporal dimension. Similarly, the kernel\nsize of the spatial convolution is (k × m × 1), where m is\ngenerally equal to EEG channels C. The resulting outputs\nof the temporal and spatial convolutions are connected to an\nadditional convolutional layer to reshape the feature maps for\nfurther additive merging. Each convolution layer is followed\nby batch normalization and an ELU activation function to\nenhance the nonlinear representation capability. In this way,\nthese two parallel branches can simultaneously preserve the\ntemporal-spatial structural information, which is helpful for\nsubsequent feature transfer between different domains.\n2) Global Adaptor: Because of the coherent brain-driven\nbehavior, we assume the corresponding long EEG sequences\nare also context-dependent. However, due to the limited per-\nceptual field of convolutional operation, the global depen-\ndencies between the EEG signals are rarely explored and\nconsidered during the domain adaptation procedure. Therefore,\nwe propose a global adaptor based on the attention mechanism\nto enhance feature alignment via encoding the non-local\ncorrelation in the EEG features. Specifically, we first flatten\nthe channel of feature maps and split the source and target\nFig. 3. Feature extractor of different connection strategies. (a) traditional\nseries connection. (b) our parallel connection.\nFig. 4. The details of the global adaptor. The attention scores are\ncalculated with the query (Q) and the key (K) of the target features and\nutilized as the weights for the value (V) of the source features.\nfeatures into multiple slices with a length of d in the temporal\ndimension. In this way, we obtain (T − n + 1)/d slices of\nSONG et al.: GLOBAL ADAPTIVE TRANSFORMER FOR CROSS-SUBJECT ENHANCED EEG CLASSIFICATION 2771\nsize 1 × dk , each slice representing a small feature segment.\nAs depicted in Fig. 4, the slices of the source and the target\ndomain are linearly transformed into vectors of the same size,\nnamely the target query (Q), the target key (K ), and the source\nvalue ( V ). Then, the global correlation in the target domain\ncan be evaluated by the pairwise similarity between the target\nquery and key, which can be calculated by the dot product\nbetween Q and K with a scaling of 1 /\n√\nd. The Sof tmax\nfunction converts the global correlation into attention scores\nto guide a transformation for the source value. Two additional\nfully-connected layers are used to improve the fitting ability.\nThis attention process can be formulated as\nAttention(Q, K, V ) = Sof tmax ( QK T\n√\nd\n)V. (2)\nIn the implementation, we use the multi-head attention\n(MHA) layer for improving the performance of the global\nadaptor. The attention input is separated equally into smaller\nparts, called the head. Afterward, the attention process is\nperformed individually on each head, and the resulting outputs\nare concatenated as follows :\nMHA(Q, K, V ) = [head0; · · · ;headh−1],\nheadl = Attention(xt W Q\nl , xt W K\nl , xs W V\nl ), (3)\nwhere MHA denotes multi-head attention, W Q\nl , W K\nl , W V\nl ∈\nRdk ×dk /h is the learnable linear transformation to obtain the\nquery, key, and value, respectively, and l is the index of\nheads. Then, the multi-head attention is followed by two fully\nconnected layers to improve the fitting ability. In this way, the\nglobal attention module encodes the non-local correlation of\nthe target domain and uses it to guide the feature mapping for\nthe source domain.\n3) Domain Discriminator: Inspired by generative adversar-\nial networks (GAN) [33], we use a discriminator to train\nagainst the previous feature processing part. Taking the fea-\ntures learned as input, the discriminator aims to distinguish\nwhich domain the feature comes from. This is formulated as\na binary classification problem and achieved by two fully-\nconnected layers connected with a neuron as output. In con-\ntrast, the features of different domains generated from the\nfeature extractors and the global adaptor are similar enough to\nfool the discriminator. In this way, the marginal distribution\nbetween the source and target features is gradually aligned\nuntil the discriminator is confused by the feature extractor\nand the global adaptor. Following WGAN [34], we employ the\nWasserstein loss for the discriminator to ensure the stability\nof the adversarial learning procedure with\nLD = Exs ∼Ds [D(A(F(xs)))]\n− Ext ∼Dt [D(F(xt ))] +λgp G P(ˆx), (4)\nG P(ˆx) = Eˆx∼Dˆx [(\n∇ˆx D(ˆx)\n\n2 − 1)2], (5)\nwhere ˆx = αxt + (1 − α)xs. D, A, and F are short for the\ndiscriminator, the global adaptor, and the feature extractor.G P\ndenotes the gradient penalty, and α represents a random value\nbetween (0, 1).\nThe feature extractor and global adaptor are viewed as a\ngenerator with the constraint of\nLG = −Exs ∼Ds [D(A(F(xs)))]. (6)\n4) Classifier: After feature alignment, features extracted\nfrom both domains can be used to train the classifier. Here,\nwe adopt a simple network with two fully-connected layers\nas the classifier. The output goes through Sof tmax to get a\nM-dimensional vector; each dimension represents the proba-\nbilities of different categories. With the predicted output ˆy and\nground truth label y, we use the cross-entropy loss with\nLcls = − 1\nNs + Nt\nNs +Nt∑\ni=1\nM∑\nc=1\nyic log( ˆyic ). (7)\n5) Adaptive Center Loss: Although the marginal distribution\nbetween the source and target domains is aligned by the\ndomain discriminator, the conditional distribution remains\ndifferent. Therefore, we further present the adaptive center loss\nto align the conditional distribution as\nLact = Ex∼Ds\nA(F(xs\nc )) − ct\nyc\n\n2\n+ Ex∼Dt\nF(xt\nc) − ct\nyc\n\n2\n,\n(8)\nwhere ct\nyc denotes the deep feature center of the c class in the\ntarget domain. Under this constraint, the intra-class variation\nis reduced, and the inter-class distance is amplified. At the\nsame time, the source domain features converge towards the\ncorresponding class center of the target domain.\nFor the whole network, we set up two optimizers and\nalternately train them, one optimizing the discriminator with\nLD, and the other jointly optimizing the feature extractor,\nglobal adaptor, and classifier with\nLjoint = Lcls + ωG LG + ωact Lact , (9)\nwhere ωG and ωact are the hyperparameters. The overall\nprocessing can be summarized in the Algorithm 1.\nIV. E XPERIMENTS AND RESULTS\nA. Datasets\nWe evaluate our method on two real public EEG datasets,\nnamely, Dataset 2a and Dataset 2b of the BCI competition IV .\nThese two datasets use different experimental designs, acqui-\nsition devices, and data sizes, which can help us verify the\ngeneralization of our method. The motor imagery paradigm\nused in these datasets is one of the most influenced by\nindividual differences.\n1) Dataset 2a of BCI Competition IV: The EEG datasets 1\nprovided by Graz University of Technology was acquired\nfrom nine subjects with twenty-two Ag/AgCl electrodes at a\nsampling rate of 250 Hz. Four different motor imagery tasks\nwere collected in two sessions on different days, including\nimagination of moving the left hand, right hand, both feet, and\ntongue. Each session contained 288 trials (72 trials per task),\nwith [2, 6] second of each trial being used in our experiments.\nThe data were band-pass filtered to [4, 40] Hz as [19], [35].\nThe first session was used as the training set, and the second\nsession was used as the test set in our experiments.\n1https://www.bbci.de/competition/iv/desc_2a.pdf\n2772 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023\nAlgorithm 1 Global Adaptive Transformer\n1: Input: Source EEG data xs ∈ RC×T , source label ys,\ntarget EEG data xt ∈ RC×T , target label yt .\n2: Output: y pred , prediction results of target data.\n3: Initialize: the networks of feature extractor F, adaptor A,\ndiscriminator D, and classifier C with normal weights.\n4: while not converaged do\n5: Update D:\n6: obtain D(A(F(xs))) and D(F(xt ));\n7: compute the loss LD by Eq. 4;\n8: update D weights with LD;\n9: Jointly Update F, A, and C:\n10: obtain D(A(F(xs)));\n11: compute the loss LG by Eq. 6;\n12: obtain C(A(F(xs))) and C(F(xt )));\n13: compute the loss Lcls by Eq. 7;\n14: obtain A(F(xs)) and F(xt );\n15: compute the loss Lact by Eq. 8;\n16: update F, A, and C weights with Ljoint by Eq. 9;\n17: end while\n18: Return: F and C trained with aligned xs and limited xt .\n19: Testing or Online Phase: decode newly arrived target\nEEG trials xt ′ with trained F and C.\n2) Dataset 2b of BCI Competition IV: The EEG datasets 2\nwere collected from nine subjects with three bipolar electrodes\nat a sampling rate of 250 Hz. Two motor imagery tasks were\ncollected in five sessions, including imagination of moving\nleft hand and right hand. 120 trials of each session with [3, 7]\nsecond of each trial were used in our experiments. The data\nwere band-pass filtered to [4, 40] Hz. We train with the first\nthree sessions and test with the last two sessions. We train with\nthe first three sessions and test with the last two sessions.\nB. Experiment Details\nOur method is implemented with the PyTorch library in\nPython 3.6 with a Geforce 2080Ti GPU. All EEG channels\nin the datasets were used, discarding the electrooculogram\nchannels. We train the model using Adam optimizer with the\nlearning rate, β1, and β2 chosen to be 0.0002, 0.5, and 0.999,\nrespectively. The batch size is set to 64. In the feature extractor,\nthe number of filters k1, k2, and k are set to 2, 5, and 10.\nThe kernel size for temporal convolution n is set to 51 to\nencapsulate local temporal information. In the global adaptor,\nboth the slice length d and the number of heads h are chosen\nto be 5. In the discriminator, λgp is typically set to 10. The\nloss weight ωG and ωact are set to 1 and 0.2.\nWe employ classification accuracy and kappa for evalua-\ntion metrics. kappa is a normalized measurement taking into\naccount the chance level, and is calculated with\nkappa = po − pe\n1 − pe\n(10)\nwhere po denotes the average accuracy of all the trials and\npe denotes the accuracy of a random guess. Besides, we use\n2https://www.bbci.de/competition/iv/desc_2b.pdf\nFig. 5. EEG classification performance of degraded method trained\nwith just target data, mixed source and target data, and our method.\nthe Wilcoxon Signed-Rank Test to analyze the statistical\nsignificance of performance comparisons.\nC. Baseline Comparison\nTo evaluate our framework broadly, we compare the pro-\nposed method on both Dataset 2a and 2b with the state-of-the-\nart approaches, including the skillfully hand-crafted feature\nextractors FBCSP [19] and ssCSP [36], the conventional\nEEG classifiers SSMM [37], the end-to-end convolutional\nneural networks ConvNet [15], MI-CNN [38], C2CM [17]\nand SHNN [39], and the adversarial learning based domain\nadaptation model DRDA [12]. The classification performance\nfor each individual and the average results are presented in\nTable I and Table II. It can be seen that the proposed global\nadaptation framework has achieved superior performance on\nboth datasets, with the highest average accuracy of 76.58% and\nkappa value of 0 .6877 for Dataset 2a. Our method performs\nsignificantly better than the methods with subtly hand-crafted\nfeature extraction, like FBCSP and ssCSP, as well as those\nend-to-end CNN-based models such as ConvNet and MI-CNN.\nThough C2CM and SHNN inherit both the advantages of\nconventional feature extractors and CNNs, even fine-tunes the\nmodel hyperparameters for each subject, it is still inferior to\nours. Compared with the method of DRDA, our method pays\nattention to the global correlation of the target subject during\nthe feature adaption and thus gains a 1 .84% improvement\non average accuracy. Similarly, our method also outperforms\nother methods on Dataset 2b, and achieves the highest average\naccuracy of 84.44% and kappa of 0 .6889, which validates the\neffective generalization of our method.\nD. Effect of GAT -Based Domain Adaptation\nThe key of the proposed method is the attention-based\ndomain adaptation framework to improve the decoding per-\nformance of the target subject via leveraging useful infor-\nmation from other subjects. Therefore, we first confirm the\nsignificance of our method by evaluating the efficiency of the\nSONG et al.: GLOBAL ADAPTIVE TRANSFORMER FOR CROSS-SUBJECT ENHANCED EEG CLASSIFICATION 2773\nTABLE I\nCOMPARISONS WITH STATE-OF-THE -ART METHODS ON BCI C OMPETITION DATASETS IV-2A\nTABLE II\nCOMPARISONS WITH STATE-OF-THE -ART METHODS ON BCI C OMPETITION DATASETS IV-2B\ndomain adaptation module. Specifically, we obtain a degraded\nmethod without domain adaptation by removing the pivotal\nglobal adaptor, domain discriminator, and adaptive center loss.\nThen, we train two degraded models using only the target\ndata and the mixing data from both domains, respectively.\nFig. 5 shows the comparison of the test performance of these\ntwo degraded models and our domain adaptation method on\nDataset 2a and Dataset 2b. It can be observed the degraded\nmodel with mixing domains increases the accuracy by 1.58%\n(p = 0.2419), taking the degraded model trained on the target\ndata as a baseline. Accuracy even decreases on Subject 3,\n7, 8, and 9. There is no obvious boost to directly fuse the\nsource data into the target data as a training set, which\ndemonstrates the detrimental effect of individual differences\non the cross-subject EEG classification. On the contrary, our\nmethod with domain adaptation has significant improvement\nin the average test accuracy, showing 13 .62% ( p < 0.01) and\n12.04% ( p < 0.01) higher than the baseline and the degraded\none with mixing data, respectively. Kappa values show the\nsame trend as the accuracy, where the baseline is slightly\nlower than that with mixing data by 0 .0211 and much lower\nthan our domain adaptation by 0 .1815. Besides, our model\nalso provides significant help on Dataset 2b, compared to\nthe baseline. The accuracy of each subject is improved and\nan average accuracy improvement of 4 .49% ( p < 0.01) has\nbeen achieved. The kappa value obtained with our method also\nincreases by 0 .0898 than just using target data as the training\nset. Therefore, the proposed domain adaptation framework is\ncapable of handling individual differences and making good\nuse of the source domain, thus enhancing the cross-subject\nclassification performance.\nE. Ablation Study\nAfter validating the whole framework, we conduct ablation\nexperiments to evaluate the effectiveness of several key feature\ntransfer components by removing each component from the\nentire model each time. The experimental results on Dataset\n2a are presented in Table III and Table IV.\n1) Global Adaptor: The global adaptor is the most criti-\ncal part of the overall framework. It encodes the non-local\ndependencies of the target subject into the feature mapping\nfor the source data. Thus the source features learned have a\nsimilar global correlation with the target features. As shown in\nTable III, we compare the test performance with and without\nthe global adaptor on Dataset 2a. There is a noticeable drop\nof 9 .45% ( p < 0.01) in the average classification accuracy\nwhen we remove the global adaptor. Among these subjects,\nthose with poorer performance are more affected, i.e., the\ntest accuracy of S04 is reduced by 15 .96%; even the least\naffected subject S03 degrades by 3 .82%. Similar phenomena\nalso occur on Dataset 2b as shown in Table IV. The average\naccuracy drops by 3.71% ( p < 0.01) without the global\nadaptor. This illustrates the efficiency of feature adaption of the\nsource domain by considering the global correlation within the\ntarget subject, so our adaptor module considerably improves\nthe results.\n2) Discriminator: The discriminator aims to align the\nmarginal distribution between the source and target domain\nby learning against the feature extractor and global adaptor\ncombination based on adversarial learning. The second row of\nTable III and Table IV presents the classification performance\nof the proposed method without the discriminator module.\nIt shows that the average accuracy on Dataset 2a is 3 .32%\n(p < 0.01) lower than the whole framework. In addition,\nalmost all subjects have degraded performance, except for\nS06. The absence of the discriminator also introduces a 3 .23%\n(p < 0.01) degradation on Dataset 2b. The results show that\nthe discriminator used for adversarial learning plays a staple\nrole in driving domain adaptation.\n2774 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023\nTABLE III\nABLATION EXPERIMENTS ON DATASET 2A WITH THE ADAPTOR , THE DISCRIMINATOR , AND THE ADAPTIVE CENTER LOSS REMOVED\nTABLE IV\nABLATION EXPERIMENTS ON DATASET 2B WITH THE ADAPTOR , THE DISCRIMINATOR , AND THE ADAPTIVE CENTER LOSS REMOVED\nTABLE V\nCOMPARISON OF THE DESIGN OF ADAPTIVE CENTER LOSS (APPLIED ONLY ON TARGET OR SOURCE DATA )\n3) Adaptive Center Loss: We propose the adaptive center\nloss to constrain the inter- and inner-class distance further\nsince the conditional distribution varies across subjects. From\nTable III, we can see that removing the adaptive center loss\nnegatively impacts the overall performance for Dataset 2a. The\naverage classification accuracy degenerates 3.16% ( p < 0.01),\nwith the worst case S04 dropping by 5 .89% and the least\ncase S07 dropping by 1 .73%. From Table IV, we can see\nsimilar results where the average accuracy degenerates 1 .96%\n(p < 0.01) on Dataset 2b without the adaptive center loss.\nThe results show that the adaptive center loss is substantially\nhelpful in reducing conditional distribution discrepancies and\nenhancing domain adaptation performance. We also compare\nthe design of adaptive center loss to illustrate the effect\nof aligning the source and target domains. The results that\nconstrain features close to the class center within target and\nsource data are given in Table V. It can be seen that the\naverage accuracy significantly decreased by 2.66% ( p < 0.05)\njust applying center constraint on target data for Dataset 2a.\nThe same trends are obtained on Dataset 2b, where just\nconstraining target data brings a 1.99% ( p < 0.01) decrease\nin average accuracy. Constraining only the source domain\neven brings slightly negative effects. The average accuracy\nwas reduced by 3.63% ( p < 0.01) on Dataset 2a and 2.35%\n(p < 0.01) on Dataset 2b compared to using the complete\nadaptive center loss.\nF . Evaluation the Design of Feature Extractor\nConvolutional neural networks are the current benchmark\nfor EEG feature extraction. Convolutional layers are connected\nlayer by layer to extract features from different dimensions.\nHowever, the typical series connection inevitably leads to\nFig. 6. Comparison of different connection strategies for feature\nextractor.\ninformation loss in the adjacent layers. Therefore, we propose\na parallel strategy to simultaneously capture the temporal\nand spatial structural information in two branches. In this\nexperiment, we compare the results of the proposed parallel\nstrategy and two different serial connections. One of the\nserial connections is shown in Fig. 3(a), where the temporal\nconvolutional filter is followed by the spatial filter ( i.e. temp-\nspat); and the other is the spatial filter followed by the\ntemporal one ( i.e., spat-temp). The results are illustrated in\nFig. 6. We can see that the parallel temporal-spatial branches,\nas in Fig. 3(b), have a significant advantage over the series\nconnection on all nine subjects. The average accuracy of\nparallel connection is 6.13% ( p < 0.01) higher than temp-spat\nSONG et al.: GLOBAL ADAPTIVE TRANSFORMER FOR CROSS-SUBJECT ENHANCED EEG CLASSIFICATION 2775\nFig. 7. Visualization of the marginal and the conditional distribution alignment by t-SNE. Blue indicates data of the target domain, and other colors\nindicate the data from different subjects in the source domain. (a) Original feature distribution of nine subjects. (b) Training with the feature extractor\nand the classifier. (c) Adding the discriminator based on (b). (d) Adding the global adaptor based on (b). (e) Adding both the discriminator and the\nglobal adaptor based on (b). (f) Further adding the adaptive center loss based on (e) for the overall model.\nTABLE VI\nCOMPUTATIONAL COSTS OF TWO KINDS OF CONNECTION\nconnection and 4.55% ( p < 0.01) higher than spat-temp\nconnection. Unexpectedly, the spat-temp connection performs\nbetter at 1.58% ( p = 0.0273) than the temp-spat connection.\nIt is worth noting that better performance may cause more\ncomputational costs. The parameters and FLOPs comparison\nof three convolution connections is given in Table VI.\nG. Visualization\nIn our method, the marginal distributions between the source\nand target domains are aligned through the synergy of the\nglobal adaptor and the discriminator, while the adaptive center\nloss further constrains the discrepancies between the condi-\ntional distributions. Here we visualize the features to show\nthe contribution of each component to the feature alignment\nvia t-SNE [40].\nFor illustrative purposes, we treat the data of S01 from\nDataset 2a as the target domain and the remaining subjects’\ndata as the source domain. Fig. 7 visualizes the feature distri-\nbution of the source and target domains under the influence of\nthe discriminator, the global adaptor, and the adaptive center\nloss progressively. Blue dots denote the target domain, and the\nother colors denote the source domain.\nFrom Fig. 7(a), it is evident that the original feature dis-\ntributions between individuals exhibit significantly different.\nTherefore, training the target model directly with mixed data\nfrom the source and target domains, as shown in Fig. 7(b),\nwould lead to poor performance for the target subject. Intro-\nducing the discriminator for adversarial learning helps align\nthe feature distribution between the source and target domains,\nas observed in Fig. 7(c). However, the feature distribution\nremains disordered. On the other hand, when incorporating\nthe global adaptor without the discriminator, as depicted in\nFig. 7(d), there is a noticeable improvement in the align-\nment of the marginal distribution. By combining both the\ndiscriminator and the global adaptor, as shown in Fig. 7(e), the\nmarginal distribution between the source and target domains\nbecomes uniformly similar. This demonstrates the effective\nrole played by the discriminator and the global adaptor in fea-\nture alignment, with mutual enhancement when used together.\nNext, let’s focus on the adjustment of the adaptive center\nloss on the conditional distribution. Fig. 7(e) demonstrates\nthat the features are scattered without any constraint on the\nconditional distribution, and the boundaries between different\ncategories become blurred. However, with the adaptive center\nloss, as illustrated in Fig. 7(f), the features belonging to the\nsame category from different domains are aggregated, resulting\nin clearer boundaries between different classes. This gradual\nadaptation of the conditional distribution of the source domain\nto the target domain is achieved. Overall, the results highlight\nthe effective role of both the discriminator and the global\nadaptor in feature alignment between the source and target\ndomains. Additionally, integrating the adaptive center loss\n2776 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023\nhelps adapt the conditional distribution, leading to improved\nperformance in aligning the domains.\nV. D ISCUSSION\nDue to large individual differences, people have to collect\nsufficient target subject data to calibrate EEG-based BCI sys-\ntems. Domain adaptation provides a feasible way to enhance\nEEG classification of the target subject by leveraging the data\nfrom other subjects. The existing methods commonly use fixed\nconstraints or directly employ adversarial learning to align\nfeatures from source and target domains. However, EEG series\nthat reflect the intention of our brain is context-dependent in\nmost tasks. In this case, such methods are limited by poor\nfeature representation due to ignoring long-term dependencies.\nTherefore, we propose Global Adaptive Transformer for more\neffective feature representation by emphasizing the global\ncorrelation of the target domain. Parallel convolutions are\nfirst adopted to obtain the local spatial-temporal features of\nEEG initially. Then an attention-based adaptor is designed to\ntransfer the global dependencies of the target domain to the\nsource domain.\nIn experiments, we can observe that GAT achieves state-\nof-the-art performance on public datasets and a significant\nimprovement compared to only using target data or directly\nmixing target and source data for training. The results prove\nthat our method performs effective domain adaptation, so that\nthe data from other subjects can be used to calibrate the\ntarget model. The ablation study shows that attention-based\nglobal adaptor plays a major role in aligning feature dis-\ntribution. The discriminator and adaptive center loss further\nimprove the overall performance by a large margin. It is\nworth noting that the parallel connection works better than\nthe series connection of temporal and spatial convolutional\nlayers widely used in EEG analysis. The visualization shows\nthat the GAT framework successfully aligns the marginal dis-\ntribution and conditional distribution of the source and target\ndomains.\nThere are still several limitations to our method. Firstly,\nwe only evaluate GAT on motor imagery EEG data, which\nis heavily affected by individual differences. The performance\non other paradigms still needs further validation. In addition,\nwe have confirmed that global interactions calculated by the\nattention mechanism are significant for EEG data during\ndomain adaptation, but neglect to further explore whether the\nglobal representation is helpful for EEG decoding. Besides,\nwe treat the data from all subjects as the source domain\nfor practical implementation, but the contribution of different\nsubjects may vary in the domain adaptation process. In the\nfuture, we will promote our method across multiple paradigms\nand further explore its online practice on multi-distributed\nsource data. Another important issue is that this paper explores\ndomain adaptation for EEG classification from a deep learning\nperspective, while ignoring the comparison with some tradi-\ntional methods [41], [42]. These methods have good inter-\npretability, focusing on specific signal characteristics. We will\nalso incorporate these methods into deep learning for better\nperformance.\nVI. C ONCLUSION\nIn this paper, we have introduced a domain adaptation\nframework designed for cross-subject enhanced EEG clas-\nsification. Our framework incorporates parallel convolution\nlayers to capture the temporal-spatial structure information,\nan attention-based adaptor to align non-local correlations, and\nan adaptive center loss to address conditional discrepancies.\nThrough extensive experiments conducted on two real EEG\ndatasets, we have demonstrated the effectiveness of our pro-\nposed method in leveraging the source data. Our approach has\nachieved remarkable improvements compared to state-of-the-\nart methods in EEG classification. We believe that our method\nholds significant potential in facilitating EEG classification\ntasks and enhancing the practicality of BCI systems.\nREFERENCES\n[1] X. Gao, Y . Wang, X. Chen, and S. Gao, “Interface, interaction, and\nintelligence in generalized brain–computer interfaces,” Trends Cognit.\nSci., vol. 25, no. 8, pp. 671–684, Aug. 2021.\n[2] S. Samejima et al., “Brain-computer-spinal interface restores upper limb\nfunction after spinal cord injury,” IEEE Trans. Neural Syst. Rehabil.\nEng., vol. 29, pp. 1233–1242, 2021.\n[3] R. Yu, Z. Zhou, S. Wu, X. Gao, and G. Bin, “MRASleepNet: A\nmulti-resolution attention network for sleep stage classification using\nsingle-channel EEG,” J. Neural Eng. , vol. 19, no. 6, Dec. 2022,\nArt. no. 066025.\n[4] X. Shen, X. Zhang, Y . Huang, S. Chen, Z. Yu, and Y . Wang, “Inter-\nmediate sensory feedback assisted multi-step neural decoding for rein-\nforcement learning based brain-machine interfaces,” IEEE Trans. Neural\nSyst. Rehabil. Eng., vol. 30, pp. 2834–2844, 2022.\n[5] N. Shi, X. Li, B. Liu, C. Yang, Y . Wang, and X. Gao, “Representative-\nbased cold start for adaptive SSVEP-BCI,” IEEE Trans. Neural Syst.\nRehabil. Eng., vol. 31, pp. 1521–1531, 2023.\n[6] D. Wu, X. Jiang, and R. Peng, “Transfer learning for motor imagery\nbased brain–computer interfaces: A tutorial,” Neural Netw., vol. 153,\npp. 235–253, Sep. 2022.\n[7] B. Liu, X. Chen, X. Li, Y . Wang, X. Gao, and S. Gao, “Align and\npool for EEG headset domain adaptation (ALPHA) to facilitate dry\nelectrode based SSVEP-BCI,” IEEE Trans. Biomed. Eng., vol. 69, no. 2,\npp. 795–806, Feb. 2022.\n[8] W. Zhang and D. Wu, “Lightweight source-free transfer for privacy-\npreserving motor imagery classification,” IEEE Trans. Cognit. Develop.\nSyst., vol. 15, no. 2, pp. 938–949, Jun. 2023.\n[9] Y . Wang, S. Qiu, X. Ma, and H. He, “A prototype-based SPD matrix\nnetwork for domain adaptation EEG emotion recognition,” Pattern\nRecognit., vol. 110, Feb. 2021, Art. no. 107626.\n[10] Y . Ma, W. Zhao, M. Meng, Q. Zhang, Q. She, and J. Zhang, “Cross-\nsubject emotion recognition based on domain similarity of EEG signal\ntransfer learning,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 31,\npp. 936–943, 2023.\n[11] C. Zhao, R. Peng, and D. Wu, “Source-free domain adaptation (SFDA)\nfor privacy-preserving seizure subtype classification,” IEEE Trans. Neu-\nral Syst. Rehabil. Eng., vol. 31, pp. 2315–2325, 2023.\n[12] H. Zhao, Q. Zheng, K. Ma, H. Li, and Y . Zheng, “Deep representation-\nbased domain adaptation for nonstationary EEG classification,” IEEE\nTrans. Neural Netw. Learn. Syst., vol. 32, no. 2, pp. 535–545, Feb. 2021.\n[13] J. Li, S. Qiu, C. Du, Y . Wang, and H. He, “Domain adaptation for EEG\nemotion recognition based on latent representation similarity,” IEEE\nTrans. Cognit. Develop. Syst., vol. 12, no. 2, pp. 344–353, Jun. 2020.\n[14] D. Priyasad, T. Fernando, S. Denman, S. Sridharan, and C. Fookes,\n“Affect recognition from scalp-EEG using channel-wise encoder net-\nworks coupled with geometric deep learning and multi-channel feature\nfusion,” Knowl.-Based Syst., vol. 250, Aug. 2022, Art. no. 109038.\n[15] R. T. Schirrmeister et al., “Deep learning with convolutional neural\nnetworks for EEG decoding and visualization,” Hum. Brain Mapping,\nvol. 38, no. 11, pp. 5391–5420, Nov. 2017.\n[16] V . J. Lawhern, A. J. Solon, N. R. Waytowich, S. M. Gordon, C. P. Hung,\nand B. J. Lance, “EEGNet: A compact convolutional neural network for\nEEG-based brain–computer interfaces,” J. Neural Eng., vol. 15, no. 5,\nOct. 2018, Art. no. 056013.\nSONG et al.: GLOBAL ADAPTIVE TRANSFORMER FOR CROSS-SUBJECT ENHANCED EEG CLASSIFICATION 2777\n[17] S. Sakhavi, C. Guan, and S. Yan, “Learning temporal information for\nbrain-computer interface using convolutional neural networks,” IEEE\nTrans. Neural Netw. Learn. Syst. , vol. 29, no. 11, pp. 5619–5629,\nNov. 2018.\n[18] P. Kant, S. H. Laskar, J. Hazarika, and R. Mahamune, “CWT based\ntransfer learning for motor imagery classification for brain computer\ninterfaces,” J. Neurosci. Methods, vol. 345, Nov. 2020, Art. no. 108886.\n[19] K. Keng Ang, Z. Yang Chin, H. Zhang, and C. Guan, “Filter bank\ncommon spatial pattern (FBCSP) in brain-computer interface,” in Proc.\nIEEE Int. Joint Conf. Neural Netw. (IEEE World Congr. Comput. Intell.),\nJun. 2008, pp. 2390–2397.\n[20] B. Li, Y . Lin, X. Gao, and Z. Liu, “Enhancing the EEG classification\nin RSVP task by combining interval model of ERPs with spatial and\ntemporal regions of interest,” J. Neural Eng., vol. 18, no. 1, Feb. 2021,\nArt. no. 016008.\n[21] X. Jia, Y . Song, L. Yang, and L. Xie, “Joint spatial and tem-\nporal features extraction for multi-classification of motor imagery\nEEG,” Biomed. Signal Process. Control , vol. 71, Jan. 2022,\nArt. no. 103247.\n[22] D. Wu, Y . Xu, and B. Lu, “Transfer learning for EEG-based\nbrain–computer interfaces: A review of progress made since 2016,”\nIEEE Trans. Cognit. Develop. Syst. , vol. 14, no. 1, pp. 4–19,\nMar. 2022.\n[23] W. Zhang, Z. Wang, and D. Wu, “Multi-source decentralized transfer\nfor privacy-preserving BCIs,” IEEE Trans. Neural Syst. Rehabil. Eng.,\nvol. 30, pp. 2710–2720, 2022.\n[24] P. Zanini, M. Congedo, C. Jutten, S. Said, and Y . Berthoumieu, “Trans-\nfer learning: A Riemannian geometry framework with applications to\nbrain–computer interfaces,” IEEE Trans. Biomed. Eng., vol. 65, no. 5,\npp. 1107–1116, May 2018.\n[25] R. Zhang, Q. Zong, L. Dou, X. Zhao, Y . Tang, and Z. Li, “Hybrid\ndeep neural network using transfer learning for EEG motor imagery\ndecoding,” Biomed. Signal Process. Control , vol. 63, Jan. 2021,\nArt. no. 102144.\n[26] K. Zhang, N. Robinson, S.-W. Lee, and C. Guan, “Adaptive transfer\nlearning for EEG motor imagery classification with deep convolutional\nneural network,” Neural Netw., vol. 136, pp. 1–10, Apr. 2021.\n[27] X. Hong et al., “Dynamic joint domain adaptation network for motor\nimagery classification,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 29,\npp. 556–565, 2021.\n[28] A. Vaswani et al., “Attention is all you need,” in Proc. Adv. Neural Inf.\nProcess. Syst., vol. 30, 2017.\n[29] G. Peng, K. Zhao, H. Zhang, D. Xu, and X. Kong, “Temporal\nrelative transformer encoding cooperating with channel attention for\nEEG emotion analysis,” Comput. Biol. Med., vol. 154, Mar. 2023,\nArt. no. 106537.\n[30] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using\nshifted windows,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV),\nOct. 2021, pp. 9992–10002.\n[31] W. Wang et al., “Pyramid vision transformer: A versatile backbone for\ndense prediction without convolutions,” in Proc. IEEE/CVF Int. Conf.\nComput. Vis. (ICCV), Oct. 2021, pp. 548–558.\n[32] S. Bagchi and D. R. Bathula, “EEG-ConvTransformer for single-trial\nEEG-based visual stimulus classification,” Pattern Recognit., vol. 129,\nSep. 2022, Art. no. 108757.\n[33] I. Goodfellow et al., “Generative adversarial nets,” in Proc. Adv. Neural\nInf. Process. Syst., vol. 27, Z. Ghahramani, M. Welling, C. Cortes,\nN. Lawrence, and K. Q. Weinberger, Eds. Red Hook, NY , USA: Curran\nAssociates, 2014.\n[34] I. Gulrajani, F. Ahmed, M. Arjovsky, V . Dumoulin, and A. C. Courville,\n“Improved training of Wasserstein GANs,” in Proc. Adv. Neural Inf.\nProcess. Syst. Long Beach, CA, USA, Dec. 2017, pp. 5769–5779.\n[35] D. J. McFarland, L. A. Miner, T. M. Vaughan, and J. R. Wolpaw,\n“Mu and beta rhythm topographies during motor imagery and actual\nmovements,” Brain Topogr., vol. 12, no. 3, pp. 177–186, Feb. 2000.\n[36] W. Samek, F. C. Meinecke, and K. Müller, “Transferring subspaces\nbetween subjects in brain-computer interfacing,” IEEE Trans. Biomed.\nEng., vol. 60, no. 8, pp. 2289–2298, Aug. 2013.\n[37] Q. Zheng, F. Zhu, J. Qin, B. Chen, and P.-A. Heng, “Sparse support\nmatrix machine,” Pattern Recognit., vol. 76, pp. 715–726, Apr. 2018.\n[38] H. Dose, J. S. Møller, H. K. Iversen, and S. Puthusserypady, “An end-to-\nend deep learning approach to MI-EEG signal classification for BCIs,”\nExpert Syst. Appl., vol. 114, pp. 532–542, Dec. 2018.\n[39] C. Liu et al., “SincNet-based hybrid neural network for motor imagery\nEEG decoding,” IEEE Trans. Neural Syst. Rehabil. Eng. , vol. 30,\npp. 540–549, 2022.\n[40] L. van der Maaten and G. Hinton, “Visualizing data using t-SNE,”\nJ. Mach. Learn. Res., vol. 9, pp. 2579–2605, Nov. 2008.\n[41] H. He and D. Wu, “Transfer learning for brain–computer interfaces: A\nEuclidean space data alignment approach,” IEEE Trans. Biomed. Eng.,\nvol. 67, no. 2, pp. 399–410, Feb. 2020.\n[42] L. Xu, M. Xu, Y . Ke, X. An, S. Liu, and D. Ming, “Cross-dataset\nvariability problem in EEG decoding with deep learning,” Frontiers\nHum. Neurosci., vol. 14, p. 103, Apr. 2020.",
  "topic": "Electroencephalography",
  "concepts": [
    {
      "name": "Electroencephalography",
      "score": 0.595094621181488
    },
    {
      "name": "Transformer",
      "score": 0.5549083948135376
    },
    {
      "name": "Computer science",
      "score": 0.5192204117774963
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3952312767505646
    },
    {
      "name": "Speech recognition",
      "score": 0.384113609790802
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.34531915187835693
    },
    {
      "name": "Psychology",
      "score": 0.22694191336631775
    },
    {
      "name": "Engineering",
      "score": 0.18351203203201294
    },
    {
      "name": "Neuroscience",
      "score": 0.15528926253318787
    },
    {
      "name": "Electrical engineering",
      "score": 0.13327908515930176
    },
    {
      "name": "Voltage",
      "score": 0.09813997149467468
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210145761",
      "name": "Shenzhen Institutes of Advanced Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "HK"
    }
  ],
  "cited_by": 36
}