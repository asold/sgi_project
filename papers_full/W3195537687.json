{
  "title": "TPH-YOLOv5: Improved YOLOv5 Based on Transformer Prediction Head for Object Detection on Drone-captured Scenarios",
  "url": "https://openalex.org/W3195537687",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Zhu, Xingkui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225546304",
      "name": "Lyu Shuchang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103723868",
      "name": "Wang Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2069353866",
      "name": "Zhao Qi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2986357608",
    "https://openalex.org/W3002524819",
    "https://openalex.org/W3120052154",
    "https://openalex.org/W2899607431",
    "https://openalex.org/W2993756598",
    "https://openalex.org/W2109255472",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3042011474",
    "https://openalex.org/W2963995737",
    "https://openalex.org/W2991089415",
    "https://openalex.org/W3127743092",
    "https://openalex.org/W3208645658",
    "https://openalex.org/W3042689508",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W2570343428",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W3034971973",
    "https://openalex.org/W3138006590",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2798799804",
    "https://openalex.org/W2797527871",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2796347433",
    "https://openalex.org/W2935837427",
    "https://openalex.org/W2608264083",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2797090510",
    "https://openalex.org/W2963604034",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2810030371",
    "https://openalex.org/W2982770724",
    "https://openalex.org/W2963857746",
    "https://openalex.org/W3018757597",
    "https://openalex.org/W2031489346",
    "https://openalex.org/W2769291631",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W3012573144",
    "https://openalex.org/W2772489440",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W2964444661",
    "https://openalex.org/W3184439416",
    "https://openalex.org/W2963786238",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3106250896",
    "https://openalex.org/W3180134609",
    "https://openalex.org/W2612445135",
    "https://openalex.org/W2102605133"
  ],
  "abstract": "Object detection on drone-captured scenarios is a recent popular task. As drones always navigate in different altitudes, the object scale varies violently, which burdens the optimization of networks. Moreover, high-speed and low-altitude flight bring in the motion blur on the densely packed objects, which leads to great challenge of object distinction. To solve the two issues mentioned above, we propose TPH-YOLOv5. Based on YOLOv5, we add one more prediction head to detect different-scale objects. Then we replace the original prediction heads with Transformer Prediction Heads (TPH) to explore the prediction potential with self-attention mechanism. We also integrate convolutional block attention model (CBAM) to find attention region on scenarios with dense objects. To achieve more improvement of our proposed TPH-YOLOv5, we provide bags of useful strategies such as data augmentation, multiscale testing, multi-model integration and utilizing extra classifier. Extensive experiments on dataset VisDrone2021 show that TPH-YOLOv5 have good performance with impressive interpretability on drone-captured scenarios. On DET-test-challenge dataset, the AP result of TPH-YOLOv5 are 39.18%, which is better than previous SOTA method (DPNetV3) by 1.81%. On VisDrone Challenge 2021, TPHYOLOv5 wins 5th place and achieves well-matched results with 1st place model (AP 39.43%). Compared to baseline model (YOLOv5), TPH-YOLOv5 improves about 7%, which is encouraging and competitive.",
  "full_text": "TPH-YOLOv5: Improved YOLOv5 Based on Transformer Prediction Head for\nObject Detection on Drone-captured Scenarios\nXingkui Zhu1 * Shuchang Lyu1 * Xu Wang 1 Qi Zhao1 †\n1 Beihang University, Beijing, China\n{adlith, lyushuchang, sy2002406, zhaoqi}@buaa.edu.cn\nAbstract\nObject detection on drone-captured scenarios is a re-\ncent popular task. As drones always navigate in different\naltitudes, the object scale varies violently, which burdens\nthe optimization of networks. Moreover, high-speed and\nlow-altitude flight bring in the motion blur on the densely\npacked objects, which leads to great challenge of object\ndistinction. To solve the two issues mentioned above, we\npropose TPH-YOLOv5. Based on YOLOv5, we add one\nmore prediction head to detect different-scale objects. Then\nwe replace the original prediction heads with Transformer\nPrediction Heads (TPH) to explore the prediction poten-\ntial with self-attention mechanism. We also integrate con-\nvolutional block attention model (CBAM) to find attention\nregion on scenarios with dense objects. To achieve more\nimprovement of our proposed TPH-YOLOv5, we provide\nbags of useful strategies such as data augmentation, multi-\nscale testing, multi-model integration and utilizing extra\nclassifier. Extensive experiments on dataset VisDrone2021\nshow that TPH-YOLOv5 have good performance with im-\npressive interpretability on drone-captured scenarios. On\nDET-test-challenge dataset, the AP result of TPH-YOLOv5\nare 39.18%, which is better than previous SOTA method\n(DPNetV3) by 1.81%. On VisDrone Challenge 2021, TPH-\nYOLOv5 wins 5th place and achieves well-matched results\nwith 1st place model (AP 39.43%). Compared to baseline\nmodel (YOLOv5), TPH-YOLOv5 improves about 7%, which\nis encouraging and competitive.\n1. Introduction\nObject detection technology on drone-captured scenarios\nhas been widely used in many practical applications, such as\nplant protection [18, 41], wildlife protection [23, 22] and ur-\nban surveillance [1, 15]. In this paper, we focus on improv-\n*Contribute Equally.\n†Corresponding author.\nFigure 1. Intuitive cases to explain the three main problems in ob-\nject detection on drone-captured images. The cases in first row,\nsecond row and third row respectively shows the size variation,\nhigh-density and large coverage of objects on drone-captured im-\nages.\ning the performance of object detection on drone-captured\nimages and providing insight for the above-mentioned nu-\nmerous applications.\nRecent years have witnessed significant progresses in ob-\nject detection tasks using deep convolutional neural net-\nworks [40, 37, 34, 27, 58]. Some notable benchmark\ndatasets like MS COCO [30] and PASCALVOC [9] greatly\narXiv:2108.11539v1  [cs.CV]  26 Aug 2021\nFigure 2. The overview of working pipeline using TPH-YOLOv5. Compared to original version, we mainly improve the head by applying\nTransformer Prediction Head (TPH). We also add one more head to better detect different scale objects. In addition, we employ bag of\ntricks like data augmentation, multi-scale testing, model ensemble and self-trained classifier to make TPH-YOLOv5 stronger.\npromote the development of object detection application.\nHowever, most previous deep convolutional neural net-\nworks are designed for natural scene images. Directly ap-\nplying previous models to tackle object detection task on\ndrone-captured scenarios mainly has three problems, which\nare intuitively illustrated by some cases in Fig.1. First,\nthe object scale varies violently because the flight altitude\nof drones change greatly. Second, drone-captured images\ncontain objects with high density, which brings in occlu-\nsion between objects. Third, drone-captured images always\ncontain confusing geographic elements because of covering\nlarge area. The above-mentioned three problems make the\nobject detection of drone-captured images very challenging.\nIn object detection task, YOLO series [37, 38, 39, 2]\nplay an important role in one-stage detectors. In this pa-\nper, we propose an improved model, TPH-YOLOv5 based\non YOLOv5 [21] to solve the above-mentioned three prob-\nlems. The overview of the detection pipeline using TPH-\nYOLOv5 is shown in Fig.2. We respectively use CSPDark-\nnet53 [52, 2] and path aggregation network (PANet [33])\nas backbone and neck of TPH-YOLOv5, which follows the\noriginal version. In the head part, we first introduce one\nmore head for tiny object detection. Totally, TPH-YOLOv5\ncontains four detection heads separately used for the detec-\ntion of tiny, small, medium, large objects. Then, we replace\nthe original prediction heads with Transformer Prediction\nHeads (TPH) [7, 49] to explore the prediction potential. To\nfind the attention region in images with large coverage, we\nadopt Convolutional Block Attention Module (CBAM [54])\nto sequentially generate the attention map along channel-\nwise and spatial-wise dimensions. Compared to YOLOv5,\nour improved TPH-YOLOv5 can better deal with drone-\ncaptured images.\nTo further improve the performance of TPH-YOLOv5,\nwe employ bag of tricks (Fig.2). Specifically, we adopt data\naugmentation during training, which promote the adapta-\ntion for dramatic size changes of objects in images. We\nalso add multi-scale testing (ms-testing) and multi-model\nensemble strategies during inference to obtain more con-\nvincing detection results. Moreover, through visualizing\nthe failure cases, we find that our proposed architecture has\nexcellent localization ability but poor classification ability,\nespecially on some similar categories like “tricycle” and\n“awning-tricycle”. To solve this problem, we provide a self-\ntrained classifier (ResNet18 [17]) using the image patches\ncropping from training data as classification training set.\nWith self-trained classifier, our method has 0.8%∼1.0% im-\nprovement on AP value.\nOur contributions are listed as follows:\n• We add one more prediction head to deal with large\nscale variance of objects.\n• We integrate the Transformer Prediction Heads (TPH)\ninto YOLOv5, which can accurately localize objects in\nhigh-density scenes.\n• We integrate CBAM into YOLOv5, which can help the\nnetwork to find region of interest in images that have\nlarge region coverage.\n• We provide useful bag of tricks and filtering some use-\nless tricks for object detection task on drone-captured\nscenarios.\n• We use self-trained classifier to improve the classifica-\ntion ability on some confusing categories.\n• On VisDrone2021 test-challenge dataset, our proposed\nTPH-YOLOv5 achieve 39.18% (AP), outperforming\nDPNetV3 (previous SOTA method) by 1.81%. In Vis-\nDrone2021 DET challenge, TPH-YOLOv5 wins 5th\nplace and has minor gap comparing with 1st place\nmodels.\n2. Related Work\n2.1. Data Augmentation\nThe effectiveness of data augmentation is to expand the\ndataset, so that the model has higher robustness to the im-\nages obtained from different environments. Photometric\ndistortions and geometric distortions are wildly used by re-\nsearchers. As for photometric distortion, we adjusted the\nhue, saturation and value of the images. In dealing with ge-\nometric distortion, we add random scaling, cropping, trans-\nlation, shearing, and rotating. In addition to the above-\nmentioned global pixel augmentation methods, there are\nsome more unique data augmentation methods. Some re-\nsearchers have proposed methods using multiple images to-\ngether for data augmentation i.e. MixUp [57], CutMix [56]\nand Mosaic [2]. MixUp randomly select two samples from\nthe training images to perform random weighted summa-\ntion, and the labels of the samples also correspond to the\nweighted summation. Unlike occlusion works that gener-\nally use zero-pixel ”black cloth” to occlude a image, Cut-\nMix uses an area of another image to cover the occluded\narea. Mosaic is an improved version of the CutMix. Mosaic\nstitches four images, which greatly enriches the background\nof the detected object. In addition, batch normalization cal-\nculates the activation statistics of 4 different images on each\nlayer.\nIn TPH-YOLOv5, we use a combination of MixUp, Mo-\nsaic and traditional methods in data augmentation.\n2.2. Multi-Model Ensemble Method in Object De-\ntection\nDeep learning neural networks are non-linear methods.\nThey provide greater flexibility and can scale in proportion\nto the amount of training data. One disadvantage of this\nflexibility is that they learn through random training algo-\nrithms, which means that they are sensitive to the details\nof the training data, and may find a different set of weights\neach time they train, resulting in different predictions. This\ngives the neural network a high variance. A successful way\nto reduce the variance of neural network models is to train\nmultiple models instead of a single model, and combine the\npredictions of these models.\nThere are three different methods to ensemble boxes\nfrom different object detection models: Non-maximum sup-\npression (NMS) [36], Soft-NMS [53], weighted boxes fu-\nsion (WBF) [43]. In the NMS method, if the overlap, in-\ntersection over union (IoU) of the boxes is higher than a\ncertain threshold, they are considered to belong to the same\nobject. For each object, NMS only leaves one bounding\nbox with the highest confidence, and other bounding boxes\nare deleted. Therefore, the box filtering process depends\non the choice of this single IoU threshold, which have a\nbig impact on model performance. Soft-NMS has made\na slightly change to NMS, which made Soft-NMS shows\na significant improvement over traditional NMS on stan-\ndard benchmark datasets (such as PASCAL VOC [10] and\nMS COCO [30]). It sets an attenuation function for the\nconfidence of adjacent bounding boxes based on the IoU\nvalue instead of completely setting their confidence scores\nto zero and delete them. WBF works differently from NMS.\nBoth NMS and Soft-NMS exclude some boxes, while WBF\nmerges all boxes to form the final result. Therefore, it can\nsolve all the inaccurate predictions of the model. We use\nWBF to ensemble final models, which performs much bet-\nter than NMS.\n2.3. Object Detection\nCNN-based object detectors can be divided into\nmany types: 1) one-stage detectors: YOLOX [11],\nFCOS [48], DETR [65], Scaled-YOLOv4 [51], Effi-\ncientDet [45]. 2) two-stage detectors: VFNet [59],\nCenterNet2 [62]. 3) anchor-based detectors: Scaled-\nYOLOv4 [51], YOLOv5 [21]. 4) anchor-free detectors:\nCenterNet [63], YOLOX [11], RepPoints [55]. Some detec-\ntors are specially designed for Drone-captured images like\nRRNet [4], PENet [46], CenterNet [63] etc. But from the\nperspective of components, they generally consist of two\nparts, an CNN-based backbone, used for image feature ex-\ntraction, and the other part is detection head used to predict\nthe class and bounding box for object. In addition, the ob-\nject detectors developed in recent years often insert some\nlayers between the backbone and the head, people usually\ncall this part the neck of the detector. Next, we will sepa-\nrately introduce these three structures in detail.\nBackbone. The backbone that are often used in-\nclude VGG [42], ResNet [17], DenseNet [20], Mo-\nbileNet [19], EfficientNet [44], CSPDarknet53 [52], Swin\nTransformer [35] etc., rather than networks designed by\nourselves. Because these networks have proven that they\nhave strong feature extraction capabilities on classification\nand other issues. But researchers will also fine-tune the\nbackbone to make it more suitable for specific tasks.\nNeck. The neck is designed to make better use of the fea-\ntures extracted by the backbone. It reprocesses and ratio-\nnally uses the feature maps extracted by Backbone at dif-\nferent stages. Usually, a neck consists of several bottom-up\npaths and several top-down paths. Neck is a key link in the\ntarget detection framework. The earliest neck is the use of\nup and down sampling block. The feature of this method is\nthat there is no feature layer aggregation operation, such as\nSSD [34], directly follow the head after the multi-level fea-\nture map. Commonly used path-aggregation blocks in neck\nare: FPN [28], PANet [33], NAS-FPN [12], BiFPN [45],\nASFF [32], SFAM [61].The commonality of these methods\nis to repeatedly use various up-and-down sampling, splic-\ning, dot sum or dot product to design aggregation strate-\nFigure 3. The architecture of the TPH-YOLOv5. a) CSPDarknet53 backbone with three transformer encoder blocks at the end. b) The\nNeck use the structure like PANet. c) Four TPHs (transformer prediction heads) use the feature maps from transformer encoder blocks in\nNeck. In addition, the number of each block is marked with orange numbers on the left side of the block.\ngies. There are also some additional blocks used in neck,\nlike SPP [16], ASPP [5], RFB [31], CBAM [54].\nHead. As a classification network, the backbone cannot\ncomplete the positioning task, and the head is designed to\nbe responsible for detecting the location and category of the\nobject by the features maps extracted from the backbone.\nHeads are generally divided into two kinds: one-stage ob-\nject detector and two-stage object detector. Two-stage de-\ntectors have long been the dominant method in the field of\nobject detection, and the most representative one is the R-\nCNN series [14, 13, 40]. Compared with the two-stage de-\ntector, the one-stage detector predicts the bounding box and\nthe class of objects at the same time. The speed advan-\ntage of the one-stage detector is obvious, but the accuracy\nis lower. For one-stage detectors, the most representative\nmodels are YOLO series [37, 38, 39, 2], SSD [34] and Reti-\nnaNet [29].\n3. TPH-YOLOv5\n3.1. Overview of YOLOv5\nYOLOv5 has four different models including YOLOv5s,\nYOLOv5m, YOLOv5l and YOLOv5x. Generally, YOLOv5\nrespectively uses the architecture of CSPDarknet53 with an\nSPP layer as backbone, PANet as Neck and YOLO detec-\ntion head [37]. To further optimize the whole architecture,\nbag of freebies and specials [2] are provided. Since it is the\nmost notable and convenient one-stage detector, we select it\nas our baseline.\nWhen we train the model using VisDrone2021\ndataset [64] with data augmentation strategy (Mosaic and\nMixUp), we find that the results of YOLOv5x are much bet-\nter than YOLOv5s, YOLOv5m and YOLOv5l, and the gap\nof AP value is more than 1.5%. Even though the training\ncomputation cost of the YOLOv5x model is more than that\nof other three models, we still choose to use YOLOv5x to\npursue the best detection performance. In addition, accord-\ning to the features of drone-captured images, we adjust the\nparameters of commonly used photometric distortions and\ngeometric distortions.\n3.2. TPH-YOLOv5\nThe framework of TPH-YOLOv5 is illustrated in Fig. 3.\nWe modify the original YOLOv5 to make it specialize in\nthe VisDrone2021 dataset.\nPrediction head for tiny objects. We investigate the Vis-\nDrone2021 dataset and find that it contains many extremely\nsmall instances, so we add one more prediction head for tiny\nobjects detection. Combined with the other three prediction\nheads, our four-head structure can ease the negative influ-\nence caused by violent object scale variance. As shown in\nFig. 3, the prediction head (head No.1) we add is generated\nfrom low-level, high-resolution feature map, which is more\nsensitive to tiny objects. After adding an additional detec-\ntion head, although the computation and memory cost in-\ncrease, the performance of tiny objects detection gets large\nimprovement.\nFigure 4. The architecture of transformer encoder, which contains\ntwo main blocks, a multi-head attention block and a feed-forward\nneural network (MLP). LayerNorm and Dropout layers help the\nnetwork converge better and prevent the network from over fitting.\nMulti-head attention can help the current node not only pay at-\ntention to the current pixels, but also obtain the semantics of the\ncontext.\nTransformer encoder block. Inspired by the vision trans-\nformer [6], we replace some convolutional blocks and CSP\nbottleneck blocks in original version of YOLOv5 with\ntransformer encoder blocks. The structure is shown in\nFig. 4. Compared to original bottleneck block in CSPDark-\nnet53, we believe that transformer encoder block can cap-\nture global information and abundant contextual informa-\ntion. Each transformer encoder contains two sub-layers.\nThe first sub-layer is a multi-head attention layer and the\nsecond one (MLP) is a fully-connected layer. Residual con-\nnections are used between each sub-layer. Transformer en-\ncoder blocks increase the ability to capture different local\ninformation. It can also explore the feature representation\npotential with self-attention mechanism [50]. On the Vis-\nDrone2021 dataset, transformer encoder blocks have better\nperformance on occluded objects with high-density.\nBased on YOLOv5, we only apply transformer encoder\nblocks in the head part to form Transformer Prediction Head\n(TPH) and the end of backbone. Because the feature maps\nat the end of the network have low resolution. Applying\nTPH on low-resolution feature maps can decrease the ex-\npensive computation and memory cost. Moreover, when\nwe enlarge the resolution of input images, we optional re-\nmove some TPH blocks at early layers to make the training\nFigure 5. The overview of CBAM module. Two sequential sub-\nmodules are used to refine feature map that go through CBAM,\nresidual paths are also used.\nprocess available.\nConvolutional block attention module (CBAM).\nCBAM [54] is a simple but effective attention module.\nIt is a lightweight module that can be integrated into\nmost notable CNN architectures, and it can be trained\nin an end-to-end manner. Given a feature map, CBAM\nsequentially infers the attention map along two separate\ndimensions of channel and spatial, and then multiplies\nthe attention map with the input feature map to perform\nadaptive feature refinement. The structure of the CBAM\nmodule is shown in the Fig. 5. According to the experiment\nin the paper [54], after integrating CBAM into different\nmodels on different classification and detection datasets,\nthe performance of the model get large improved, which\nproves the effectiveness of this module.\nOn drone-captured images, large covering region always\ncontains confusing geographical elements. Using CBAM\ncan extract the attention area to help TPH-YOLOv5 resist\nthe confusing information and focus on useful target ob-\njects.\nMs-testing and model ensemble. We train five different\nmodels in terms of different perspectives for model ensem-\nble. During inference phase, we first perform ms-testing\nstrategy on single model. The implementation details of\nms-testing are the following three steps. 1) Scaling the test-\ning image to 1.3 times. 2) Respectively reducing the image\nto 1 time, 0.83 times, and 0.67 times. 3) Flipping the im-\nages horizontally. Finally, we feed the six different-scaling\nimages to TPH-YOLOv5 and use NMS to fuse the testing\npredictions.\nOn different models, we perform the same ms-testing op-\neration and fuse the final five predictions by WBF to get the\nfinal result.\nSelf-trained classifier. After training the VisDrone2021\ndataset with TPH-YOLOv5, we test the test-dev dataset and\nthen analyze the results by visualizing the failure cases and\ndraw a conclusion that TPH-YOLOv5 has excellent local-\nization ability but poor classification ability. We further ex-\nplore the confusion matrix which is shown in Fig.6, and\nobserve that the precision of the some hard categories such\nas tricycle and awning-tricycle are very low. Therefore, we\npropose an extra self-trained classifier. First, we construct\na training set by cropping the ground-truth bounding boxes\nand resizing each image patches to 64×64. Then we select\nResNet18 [17] as classifier network. As shown in experi-\nmental results, our method get around 0.8%˜1.0% improve-\nment on AP value with the help of this self-trained classifier.\nFigure 6. Confusion matrix was made at IoU threshold of 0.45,\nconfidence threshold of 0.25.\n4. Experiments\nWe use the testset-challenge and testset-dev of the Vis-\nDrone2021 dataset to evaluate our model, and we report\nmAP (average of all 10 IoU thresholds, ranging from\n[0.5: 0.95]) and AP50. VisDrone2021-DET dataset is the\nsame as VisDrone2019-DET dataset and VisDrone2018-\nDET dataset.\n4.1. Implementation Details\nWe implement TPH-YOLOv5 on Pytorch 1.8.1. All of\nour models use an NVIDIA RTX3090 GPU for training and\ntesting. In the training phase, we use part of pre-trained\nmodel from yolov5x, because TPH-YOLOv5 and YOLOv5\nshare most part of backbone (block 0˜8) and some part of\nhead (block 10˜13 and block 15˜18), there are many weights\ncan be transferred from YOLOv5x to TPH-YOLOv5, by us-\ning these weights we can save a lot of training time.\nBecause the VisDrone2021 training set is a bit small,\nwe only train the model on VisDrone2021 trainset for 65\nepochs, and the first 2 epochs are used for warm-up. We\nuse adam optimizer for training, and use 3e-4 as the initial\nlearning rate with the cosine lr schedule. The learning rate\nof the last epoch decays to 0.12 of the initial learning rate.\nThe size of the input image of our model is very large, the\nlong side of the image is 1536 pixels, which leads to the\nbatch size is only 2.\nData analysis. According to our previous engineering ex-\nperience, it is very important to walk through dataset before\nFigure 7. Some images were taken too high, resulting in many\nsmall objects, which cannot be recognized.\ntraining the model, which can often be of great help to the\nimprovement of mAP. We have analyzed bounding boxes\nin the VisDrone2021 dataset. When the input image size is\nset to 1536, there are 622 of 342391 labels are less than 3\npixels in size. As shown in Fig. 7, these small objects are\nhard to recognize. When we use gray squares to cover these\nsmall objects and train our model on the processed dataset,\nthe mAP improves by 0.2, better than not.\nMs-testing. When training neural network models for com-\nputer vision problems, data augmentation is a technique of-\nten used to improve performance and reduce generalization\nerrors. When using a model to make predictions, image data\naugmentation of test dataset can also be applied to allow the\nmodel to make predictions on multiple different versions of\nimages. The prediction of the augmented images can be\naveraged to get better prediction performance.\nWe scale the test images to three different sizes in ms-\ntesting, and then flip them horizontally, so that a total of\n6 different images are obtained. After testing six different\nimages and fusing the results, we get the final test result.\n4.2. Comparisons with the State-of-the-art\nOn VisDrone2021-DET testset-challenge.\nDue to the limited number of submissions in the Vis-\nDrone2021 competition server, we only obtained the re-\nsults of 4 models on testset-challenge and the final results\nof the ensemble of 5 models. We finally got a good score\nof 39.18 on testset-challenge, which is much higher than\nVisDrone2020’s best score of 37.37. Ranked fifth in the\nVisDrone 2021 leader board, our score is 0.25 lower than\nthe 39.43 of the first place. If the number of submissions\nis not used up, we will definitely get better results. Table\n1 lists the score of our model, compared with the scores in\nthe previous year’s VisDrone competition and the scores of\nalgorithms submitted by the committee.\nMethods mAP (%) AP50 (%)\nRetinaNet[29] 11.81 21.37\nRefineDet[60] 14.90 28.76\nDetNet59[26] 15.26 29.23\nCascade-RCNN[3] 16.09 31.91\nFPN[28] 16.51 32.20\nLight-RCNN[25] 16.53 32.78\nCornetNet[24] 17.41 34.12\nRRNet (20192nd)[4] 29.13 55.82\nDPNet-ensemble (2019 SOTA) [8]29.62 54.00\nSMPNet (20202nd)[47] 35.98 59.53\nDPNetV3 (2020 SOTA)[47] 37.37 62.05\nTPH-YOLOv5 ensemble 39.18 \\\nTable 1. The comparison of the performance in VisDrone2021\ntestset-challenge\n4.3. Ablation Studies\nOn VisDrone2021-DET testset-dev. we analyze impor-\ntance of each proposed component on local testset-dev as\nwe cannot test these on VisDrone2021 competition server,\nthe number of submissions to the competition server is very\nvaluable. The impact of each component is listed in the ta-\nble 2.\nMethods mAP (%) AP50 (%)\nYOLOv5 28.88 49.33\nYOLOv5+P2 31.03 (↑2.15) 51.61 (↑2.28)\nYOLOv5+P2+transformer 32.84 (↑1.81) 53.87 (↑2.26)\nTPH-YOLOv5 (previous+CBAM)33.63 (↑0.79) 54.77 (↑0.90)\nTPH-YOLOv5+ms-testing 34.90 (↑1.27) 56.40 (↑1.63)\nTPH-YOLOv5+ms-testing+Classifier35.74 (↑0.84) 57.31 (↑0.91)\nTable 2. Ablation Study on VisDrone2021 testset-dev.\nEffect of extra prediction head. Adding a detection head\nfor tiny objects makes the number of layers of the origi-\nnal YOLOv5x change from 607 to 719, and GFLOPs from\n219.0 to 259.0. This of course increases the amount of cal-\nculation, but the mAP improvement is also very high. From\nFig. 9 we can see that TPH-YOLOv5 performs well when\ndetecting small objects, so the increasing in calculation is\nworthwhile.\nEffect of transformer encoder blocks. After using the\ntransformer encoder block, the total layers of the model de-\ncrease from 719 to 705, and GFLOPs from 259.0 to 237.3.\nUse transformer encoder blocks can not only increase mAP,\nbut also reduce the size of the network. At the same time, it\nalso plays a role in the detection of dense objects and large\nobjects.\nEffect of model ensemble. We list the mAP of the final re-\nsults of our five different models in each category and com-\npared them with the fusion model in table 3. In training\nphrase, we use different input image sizes and change the\nweight of each category to make each model unique. So\nthat the final ensemble model can get a relatively balanced\nresult. 1) TPH-YOLOv5-1 use the input image size of 1920\nand all categories have equal weights. 2) TPH-YOLOv5-\n2 use the input image size of 1536 and all categories have\nequal weights. 3) TPH-YOLOv5-3 use the input image size\nof 1920 and the weight of each category is related to the\nnumber of labels, which is shown in Fig. 8. The more la-\nbels of a certain category, the lower the weight it is given. 4)\nTPH-YOLOv5-4 use the input image size of 1536 and the\nweight of each category is related to the number of labels.\n5) TPH-YOLOv5-5 use the backbone of YOLOv5l and use\nthe input image size of 1536.\nFigure 8. The number of labels of each category.\nSome detection result on VisDrone2021 testset-\nchallenge. We have selected some representative images\nas the display of the test results. Fig. 9 shows the result\nof large objects, tiny objects, dense objects and the image\ncovering a large area.\n5. Conclusion\nIn this paper, we add some cutting-edge techniques\ni.e. transformer encoder block, CBAM and some experi-\nenced tricks to YOLOv5 and form a state-of-the-art detec-\ntor called TPH-YOLOv5, which is especially good at ob-\nject detection in drone-captured scenarios. We refresh the\nrecord of VisDrone2021 dataset, our experiments showed\nthat TPH-YOLOv5 achieved state-of-the-art performance in\nVisDrone2021 dataset. We have tried a large number of fea-\ntures, and used some of them to improve the accuracy of\nobject detector. We hope this report can help developers\nand researchers get a better experience in the analysis and\nprocessing of drone-captured scenarios.\nMethods all pedestrianpeople bicycle car van trunk tricycle awning-tricyclebus motor\nTPH-YOLOv5-1 34.90 27.52 15.32 15.21 65.99 44.23 47.56 23.96 22.11 58.85 28.44\nTPH-YOLOv5-2 34.29 27.97 14.88 14.17 67.63 45.01 44.76 25.12 20.48 55.72 27.74\nTPH-YOLOv5-3 34.68 22.88 16.01 19.26 48.88 42.98 47.82 32.86 35.65 54.16 28.25\nTPH-YOLOv5-4 34.17 23.48 15.79 17.62 49.99 42.76 47.13 31.66 32.21 54.19 27.37\nTPH-YOLOv5-5 33.04 25.98 14.90 13.10 63.05 43.45 42.56 25.20 21.06 53.65 27.10\nTPH-YOLOv5 ensemble37.32 29.00 16.75 15.69 68.94 49.79 45.16 27.33 24.72 61.80 30.90\nTable 3. Comparison of TPH-YOLOv5 models‘ performances on VisDrone2021 testset-dev for each category.\nFigure 9. Some visualization results from our TPH-YOLOv5 on testset-challenge, different category use bounding boxes with different\ncolor. The performance is good at localization tiny objects, dense objects and objects blurred by motion.\n6. Acknowledgments\nThis work was supported by National Natural Science\nFoundation of China (62072021).\nReferences\n[1] Nicolas Audebert, Bertrand Le Saux, and S ´ebastien Lef`evre.\nBeyond rgb: Very high resolution urban remote sensing with\nmultimodal deep networks. ISPRS Journal of Photogramme-\ntry and Remote Sensing, 140:20–32, 2018.\n[2] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-\nYuan Mark Liao. Yolov4: Optimal speed and accuracy of\nobject detection. arXiv preprint arXiv:2004.10934, 2020.\n[3] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delv-\ning into high quality object detection. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 6154–6162, 2018.\n[4] Changrui Chen, Yu Zhang, Qingxuan Lv, Shuo Wei, Xi-\naorui Wang, Xin Sun, and Junyu Dong. Rrnet: A hybrid\ndetector for object detection in drone-captured images. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision Workshops, pages 0–0, 2019.\n[5] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille. Deeplab: Semantic image\nsegmentation with deep convolutional nets, atrous convolu-\ntion, and fully connected crfs. IEEE transactions on pattern\nanalysis and machine intelligence, 40(4):834–848, 2017.\n[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image\nis worth 16x16 words: Transformers for image recognition\nat scale. In 9th International Conference on Learning Rep-\nresentations, ICLR 2021, Virtual Event, Austria, May 3-7,\n2021, 2021.\n[8] Dawei Du, Pengfei Zhu, Longyin Wen, Xiao Bian, Haibin\nLin, Qinghua Hu, Tao Peng, Jiayu Zheng, Xinyao Wang, Yue\nZhang, et al. Visdrone-det2019: The vision meets drone ob-\nject detection in image challenge results. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision\nWorkshops, pages 0–0, 2019.\n[9] Mark Everingham, Luc Van Gool, Christopher K. I.\nWilliams, John M. Winn, and Andrew Zisserman. The pas-\ncal visual object classes (VOC) challenge. Int. J. Comput.\nVis., 88(2):303–338, 2010.\n[10] Mark Everingham, Luc Van Gool, Christopher KI Williams,\nJohn Winn, and Andrew Zisserman. The pascal visual object\nclasses (voc) challenge. International journal of computer\nvision, 88(2):303–338, 2010.\n[11] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian\nSun. Yolox: Exceeding yolo series in 2021. arXiv preprint\narXiv:2107.08430, 2021.\n[12] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Nas-fpn:\nLearning scalable feature pyramid architecture for object\ndetection. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 7036–\n7045, 2019.\n[13] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-\nnational conference on computer vision , pages 1440–1448,\n2015.\n[14] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra\nMalik. Rich feature hierarchies for accurate object detection\nand semantic segmentation. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition , pages\n580–587, 2014.\n[15] Jingjing Gu, Tao Su, Qiuhong Wang, Xiaojiang Du, and\nMohsen Guizani. Multiple moving targets surveillance based\non a cooperative network for multi-uav. IEEE Commun.\nMag., 56(4):82–89, 2018.\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nSpatial pyramid pooling in deep convolutional networks for\nvisual recognition. IEEE transactions on pattern analysis\nand machine intelligence, 37(9):1904–1916, 2015.\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016.\n[18] Jennifer N. Hird, Alessandro Montaghi, Gregory J. McDer-\nmid, Jahan Kariyeva, Brian J. Moorman, Scott E. Nielsen,\nand Anne C. S. McIntosh. Use of unmanned aerial vehicles\nfor monitoring recovery of forest vegetation on petroleum\nwell sites. Remote. Sens., 9(5):413, 2017.\n[19] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. Mobilenets: Efficient convolu-\ntional neural networks for mobile vision applications. arXiv\npreprint arXiv:1704.04861, 2017.\n[20] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\nian Q Weinberger. Densely connected convolutional net-\nworks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 4700–4708, 2017.\n[21] Glenn Jocher, Alex Stoken, Jirka Borovec, NanoCode012,\nAyush Chaurasia, TaoXie, Liu Changyu, Abhiram V , Laugh-\ning, tkianai, yxNONG, Adam Hogan, lorenzomammana,\nAlexWang1900, Jan Hajek, Laurentiu Diaconu, Marc,\nYonghye Kwon, oleg, wanghaoyang0106, Yann Defretin,\nAditya Lohia, ml5ah, Ben Milanko, Benjamin Fineran,\nDaniel Khromov, Ding Yiwei, Doug, Durgesh, and Francisco\nIngham. ultralytics/yolov5: v5.0 - YOLOv5-P6 1280 mod-\nels, AWS, Supervise.ly and YouTube integrations, Apr. 2021.\n[22] Benjamin Kellenberger, Diego Marcos, and Devis Tuia. De-\ntecting mammals in uav images: Best practices to address a\nsubstantially imbalanced dataset with deep learning. Remote\nSensing of Environment, 216:139–153, 2018.\n[23] Benjamin Kellenberger, Michele V olpi, and Devis Tuia. Fast\nanimal detection in UA V images using convolutional neu-\nral networks. In 2017 IEEE International Geoscience and\nRemote Sensing Symposium, IGARSS 2017, Fort Worth, TX,\nUSA, July 23-28, 2017, pages 866–869. IEEE, 2017.\n[24] Hei Law and Jia Deng. Cornernet: Detecting objects as\npaired keypoints. In Proceedings of the European confer-\nence on computer vision (ECCV), pages 734–750, 2018.\n[25] Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yang-\ndong Deng, and Jian Sun. Light-head r-cnn: In defense of\ntwo-stage object detector. arXiv preprint arXiv:1711.07264,\n2017.\n[26] Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yangdong\nDeng, and Jian Sun. Detnet: A backbone network for object\ndetection. arXiv preprint arXiv:1804.06215, 2018.\n[27] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He,\nand Piotr Doll ´ar. Focal loss for dense object detection. In\nIEEE International Conference on Computer Vision, ICCV\n2017, Venice, Italy, October 22-29, 2017, pages 2999–3007.\nIEEE Computer Society, 2017.\n[28] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie. Feature pyra-\nmid networks for object detection. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 2117–2125, 2017.\n[29] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Doll´ar. Focal loss for dense object detection. In Pro-\nceedings of the IEEE international conference on computer\nvision, pages 2980–2988, 2017.\n[30] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision , pages 740–755.\nSpringer, 2014.\n[31] Songtao Liu, Di Huang, et al. Receptive field block net for\naccurate and fast object detection. In Proceedings of the Eu-\nropean Conference on Computer Vision (ECCV), pages 385–\n400, 2018.\n[32] Songtao Liu, Di Huang, and Yunhong Wang. Learning spa-\ntial fusion for single-shot object detection. arXiv preprint\narXiv:1911.09516, 2019.\n[33] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.\nPath aggregation network for instance segmentation. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition, pages 8759–8768, 2018.\n[34] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian\nSzegedy, Scott Reed, Cheng-Yang Fu, and Alexander C\nBerg. Ssd: Single shot multibox detector. In European con-\nference on computer vision, pages 21–37. Springer, 2016.\n[35] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows. arXiv preprint arXiv:2103.14030, 2021.\n[36] Alexander Neubeck and Luc Van Gool. Efficient non-\nmaximum suppression. In 18th International Conference on\nPattern Recognition (ICPR’06), volume 3, pages 850–855.\nIEEE, 2006.\n[37] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\nFarhadi. You only look once: Unified, real-time object de-\ntection. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 779–788, 2016.\n[38] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster,\nstronger. InProceedings of the IEEE conference on computer\nvision and pattern recognition, pages 7263–7271, 2017.\n[39] Joseph Redmon and Ali Farhadi. Yolov3: An incremental\nimprovement. arXiv preprint arXiv:1804.02767, 2018.\n[40] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. Advances in neural information process-\ning systems, 28:91–99, 2015.\n[41] Zhenfeng Shao, Congmin Li, Deren Li, Orhan Altan, Lei\nZhang, and Lin Ding. An accurate matching method for pro-\njecting vector data into surveillance video to monitor and\nprotect cultivated land. ISPRS Int. J. Geo Inf. , 9(7):448,\n2020.\n[42] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014.\n[43] Roman Solovyev, Weimin Wang, and Tatiana Gabruseva.\nWeighted boxes fusion: Ensembling boxes from different\nobject detection models. Image and Vision Computing ,\n107:104117, 2021.\n[44] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model\nscaling for convolutional neural networks. In International\nConference on Machine Learning, pages 6105–6114. PMLR,\n2019.\n[45] Mingxing Tan, Ruoming Pang, and Quoc V Le. Efficientdet:\nScalable and efficient object detection. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10781–10790, 2020.\n[46] Ziyang Tang, Xiang Liu, Guangyu Shen, and Baijian Yang.\nPenet: object detection using points estimation in aerial im-\nages. arXiv preprint arXiv:2001.08247, 2020.\n[47] Visdrone Team. Visdrone 2020 leaderboard.\nWebsite, 2020. http://aiskyeye.com/\nvisdrone-2020-leaderboard/.\n[48] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos:\nFully convolutional one-stage object detection. In Proceed-\nings of the IEEE/CVF international conference on computer\nvision, pages 9627–9636, 2019.\n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neu-\nral Information Processing Systems 30: Annual Conference\non Neural Information Processing Systems 2017, December\n4-9, 2017, Long Beach, CA, USA, pages 5998–6008, 2017.\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998–6008, 2017.\n[51] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-\nYuan Mark Liao. Scaled-yolov4: Scaling cross stage\npartial network. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition , pages\n13029–13038, 2021.\n[52] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu,\nPing-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. Cspnet: A\nnew backbone that can enhance learning capability of cnn.\nIn Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition workshops , pages 390–391,\n2020.\n[53] Derui Wang, Chaoran Li, Sheng Wen, Qing-Long Han,\nSurya Nepal, Xiangyu Zhang, and Yang Xiang. Daedalus:\nBreaking nonmaximum suppression in object detection via\nadversarial examples. IEEE Transactions on Cybernetics ,\n2021.\n[54] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So\nKweon. Cbam: Convolutional block attention module. In\nProceedings of the European conference on computer vision\n(ECCV), pages 3–19, 2018.\n[55] Ze Yang, Shaohui Liu, Han Hu, Liwei Wang, and Stephen\nLin. Reppoints: Point set representation for object detection.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 9657–9666, 2019.\n[56] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regular-\nization strategy to train strong classifiers with localizable fea-\ntures. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 6023–6032, 2019.\n[57] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. arXiv preprint arXiv:1710.09412, 2017.\n[58] Haoyang Zhang, Ying Wang, Feras Dayoub, and Niko Sun-\nderhauf. Varifocalnet: An iou-aware dense object detector.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 8514–8523, 2021.\n[59] Haoyang Zhang, Ying Wang, Feras Dayoub, and Niko Sun-\nderhauf. Varifocalnet: An iou-aware dense object detector.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 8514–8523, 2021.\n[60] Shifeng Zhang, Longyin Wen, Xiao Bian, Zhen Lei, and\nStan Z Li. Single-shot refinement neural network for ob-\nject detection. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 4203–4212,\n2018.\n[61] Qijie Zhao, Tao Sheng, Yongtao Wang, Zhi Tang, Ying Chen,\nLing Cai, and Haibin Ling. M2det: A single-shot object de-\ntector based on multi-level feature pyramid network. In Pro-\nceedings of the AAAI conference on artificial intelligence ,\nvolume 33, pages 9259–9266, 2019.\n[62] Xingyi Zhou, Vladlen Koltun, and Philipp Kr ¨ahenb¨uhl.\nProbabilistic two-stage detection. arXiv preprint\narXiv:2103.07461, 2021.\n[63] Xingyi Zhou, Dequan Wang, and Philipp Kr ¨ahenb¨uhl. Ob-\njects as points. arXiv preprint arXiv:1904.07850, 2019.\n[64] Pengfei Zhu, Longyin Wen, Xiao Bian, Haibin Ling, and\nQinghua Hu. Vision meets drones: A challenge. arXiv\npreprint arXiv:1804.07437, 2018.\n[65] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection. arXiv preprint\narXiv:2010.04159, 2020.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7533668875694275
    },
    {
      "name": "Interpretability",
      "score": 0.7227380275726318
    },
    {
      "name": "Drone",
      "score": 0.6746814846992493
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6296716332435608
    },
    {
      "name": "Transformer",
      "score": 0.55843186378479
    },
    {
      "name": "Classifier (UML)",
      "score": 0.527137041091919
    },
    {
      "name": "Block (permutation group theory)",
      "score": 0.43969541788101196
    },
    {
      "name": "Machine learning",
      "score": 0.4162417948246002
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.37579280138015747
    },
    {
      "name": "Data mining",
      "score": 0.3474871516227722
    },
    {
      "name": "Engineering",
      "score": 0.09202471375465393
    },
    {
      "name": "Mathematics",
      "score": 0.08337327837944031
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": []
}