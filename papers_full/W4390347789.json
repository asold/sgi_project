{
    "title": "Empathy and Equity: Key Considerations for Large Language Model Adoption in Health Care",
    "url": "https://openalex.org/W4390347789",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5093185499",
            "name": "Erica Koranteng",
            "affiliations": [
                "Harvard University"
            ]
        },
        {
            "id": "https://openalex.org/A4209078159",
            "name": "Arya Rao",
            "affiliations": [
                "Harvard University"
            ]
        },
        {
            "id": "https://openalex.org/A2584028300",
            "name": "Efren Flores",
            "affiliations": [
                "Harvard University"
            ]
        },
        {
            "id": "https://openalex.org/A2397532141",
            "name": "Michael Lev",
            "affiliations": [
                "Harvard University"
            ]
        },
        {
            "id": "https://openalex.org/A2239059690",
            "name": "Adam Landman",
            "affiliations": [
                "Harvard University"
            ]
        },
        {
            "id": "https://openalex.org/A4260817445",
            "name": "Keith Dreyer",
            "affiliations": [
                "Harvard University"
            ]
        },
        {
            "id": "https://openalex.org/A4319498974",
            "name": "Marc Succi",
            "affiliations": [
                "Massachusetts General Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A5093185499",
            "name": "Erica Koranteng",
            "affiliations": [
                "Harvard University"
            ]
        },
        {
            "id": "https://openalex.org/A4209078159",
            "name": "Arya Rao",
            "affiliations": [
                "Harvard University"
            ]
        },
        {
            "id": "https://openalex.org/A2584028300",
            "name": "Efren Flores",
            "affiliations": [
                "Harvard University"
            ]
        },
        {
            "id": "https://openalex.org/A2397532141",
            "name": "Michael Lev",
            "affiliations": [
                "Harvard University"
            ]
        },
        {
            "id": "https://openalex.org/A2239059690",
            "name": "Adam Landman",
            "affiliations": [
                "Harvard University"
            ]
        },
        {
            "id": "https://openalex.org/A4260817445",
            "name": "Keith Dreyer",
            "affiliations": [
                "Harvard University"
            ]
        },
        {
            "id": "https://openalex.org/A4319498974",
            "name": "Marc Succi",
            "affiliations": [
                "Massachusetts General Hospital"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4368367885",
        "https://openalex.org/W3016719704",
        "https://openalex.org/W3134678353",
        "https://openalex.org/W4206590911",
        "https://openalex.org/W4367186868",
        "https://openalex.org/W4383346782",
        "https://openalex.org/W4381480701",
        "https://openalex.org/W4319341091",
        "https://openalex.org/W4322208207",
        "https://openalex.org/W4382796617",
        "https://openalex.org/W3126967920",
        "https://openalex.org/W4317757464",
        "https://openalex.org/W4367310920",
        "https://openalex.org/W3173950402",
        "https://openalex.org/W3033570122",
        "https://openalex.org/W4376125980",
        "https://openalex.org/W3176477796",
        "https://openalex.org/W4317749076",
        "https://openalex.org/W4380373864",
        "https://openalex.org/W4379769651",
        "https://openalex.org/W4323350039",
        "https://openalex.org/W4367834585",
        "https://openalex.org/W4386045865"
    ],
    "abstract": "The growing presence of large language models (LLMs) in health care applications holds significant promise for innovative advancements in patient care. However, concerns about ethical implications and potential biases have been raised by various stakeholders. Here, we evaluate the ethics of LLMs in medicine along 2 key axes: empathy and equity. We outline the importance of these factors in novel models of care and develop frameworks for addressing these alongside LLM deployment.",
    "full_text": "Viewpoint\nEmpathy and Equity: Key Considerations for Large Language\nModel Adoption in Health Care\nErica Koranteng1*, MBChB, MBE; Arya Rao1*, BA; Efren Flores1, MD; Michael Lev1, MD; Adam Landman1, MD,\nMIS, MHS, MS; Keith Dreyer1, PhD, DO; Marc Succi2, MD\n1Harvard Medical School, Boston, MA, United States\n2Massachusetts General Hospital, Boston, United States\n*these authors contributed equally\nCorresponding Author:\nMarc Succi, MD\nMassachusetts General Hospital\n55 Fruit St\nBoston, 02114\nUnited States\nPhone: 1 617 935 9144\nEmail: msucci@mgh.harvard.edu\nAbstract\nThe growing presence of large language models (LLMs) in health care applications holds significant promise for innovative\nadvancements in patient care. However, concerns about ethical implications and potential biases have been raised by various\nstakeholders. Here, we evaluate the ethics of LLMs in medicine along 2 key axes: empathy and equity. We outline the importance\nof these factors in novel models of care and develop frameworks for addressing these alongside LLM deployment.\n(JMIR Med Educ 2023;9:e51199) doi: 10.2196/51199\nKEYWORDS\nChatGPT; AI; artificial intelligence; large language models; LLMs; ethics; empathy; equity; bias; language model; health care\napplication; patient care; care; development; framework; model; ethical implication\nIntroduction\nThe rapid proliferation of applications that leverage the ability\nof large language models (LLMs) to use large amounts of\ncomplex information to find relevant patterns and apply them\nto novel use cases promises great innovation in health care and\nmany other sectors. Many health care applications, such as\nclinical decision support, patient education, electronic health\nrecords (EHRs), and workflow optimization, have been proposed\n[1]. Despite the immense potential advantages of this\ntechnology, various key stakeholders have raised concerns\nregarding its ethical implications and potential perpetuation of\nexisting biases and structural barriers [2-6]. Furthermore, its\ngrowing usage in the health care setting also raises the concern\nof transparency or disclosure about its use and role in patient\nmanagement. Ethically incorporating LLMs into health care\ndelivery requires honest dialogue about the principles we aim\nto uphold in patient care and a comprehensive analysis of the\nvarious ways in which LLMs could bolster or impair these.\nStudies have demonstrated the utility of LLMs as a clinical\ndecision support tool in various settings, including in triage,\ndiagnostics, and treatment [7-11]. While LLMs show great\npromise in improving the efficiency of clinical workflows, they\nlack one key facet of physician-patient encounters: empathy.\nThough LLMs can be trained to use empathetic language [12]\nand have been able to use empathetic language in patient\ninteractions [13], this concept of artificial empathy is easily\ndistinguishable from real empathy from a patient’s perspective,\nand real empathy matters to patients [14]. The concept of\nartificial empathy, which aims to imbue artificial intelligence\n(AI) with human-like empathy, ought not to be considered\ninterchangeable with human empathy. Efforts made to design\nartificial empathy, while commendable, should aim to be\ncomplementary to human empathy in order to avoid further\nisolating patients in their time of need by destroying the\ntherapeutic alliance between patients and physicians [15].\nLoneliness is one of the key public health crises of our time,\nand conflating technology with human-to-human interaction\nwill only exacerbate this [16]. Empathic care for patients should\nbe one of the core mandates of the health care sector, and true\nJMIR Med Educ 2023 | vol. 9 | e51199 | p. 1https://mededu.jmir.org/2023/1/e51199\n(page number not for citation purposes)\nKoranteng et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\nempathy requires human connection. Therefore, while LLMs\nshow great promise in clinical workflows, they should augment,\nrather than replace, physician-led care (Table 1).\nIn addition to empathy, equity is crucial in novel models of care.\nThe current most popular LLMs, including ChatGPT, Bard,\nMed-PaLM, and others, are trained on vast sources of data,\nincluding wide swaths of the internet. These sources are rife\nwith inherent biases and lack transparency regarding the contents\nof the training data sets. They also lack specific evaluation of\nmodel biases, which may be harbingers of ethical dilemmas via\nthe rapid incorporation of LLMs into clinical spaces. While\nthere is little consensus regarding the degree of bias in current\nLLMs, in most embedding models, which have similar\nunderlying architecture, there is evidence of racial, gender, and\nage bias [17]. LLMs have been demonstrated to associate\nnegative terms with given names that are popular among the\nAfrican American as well as with the masculine poles of most\ngender axes [17]. Until systematic evaluation of LLMs is\nperformed in clinical use cases to understand and mitigate biases\nagainst vulnerable demographics, careful risk-benefit\ncalculations and a regulatory framework should be implemented\nby relevant governing bodies before LLMs are permitted in\nclinical care. This framework must ensure that these models are\nimproving health care delivery and outcomes for all.\nImportantly, the US Food and Drug Administration lacks a\nrobust authorization pathway for software as a medical device;\nthis in itself is challenging, and given the rapid development of\nLLMs, would benefit from expeditious guidelines [18] (see\nTable 2 for proactive measures to ensure the equitable\nincorporation of LLMs into health care). Following a previously\npublished ethical framework for integrating innovative domains\ninto medicine, we suggest an LLM framework guided by Blythe\net al [19] grounded in principled primary motivations as detailed\nin Tables 1 and 2.\nDespite these ethical risks, the potential benefits of incorporating\nLLMs into health care are numerous. LLMs are adept at quickly\nsynthesizing large amounts of complex data, which can form\nthe basis for numerous applications in the health care sector,\nincluding the management and interpretation of EHRs and\nclinical notes, adjuncts for patient visits (eg, encounter\ntranscription and patient translation), billing for medical\nservices, patient education, and more [20,21]. Thus, the key\nethical question at hand is as follows: do the benefits outweigh\nthe risks?\nFrom a utilitarian perspective, we must consider this question\nto not only enhance decision-making but also take advantage\nof opportunities to mitigate potential harms. Proposals for the\nincorporation of a systematized, frequently reevaluated method\nof bias evaluation into clinical applications of LLMs [3], the\naddition of human verification steps at both the input and output\nstages for LLM-guided generation of clinical texts [22], and the\nimplementation of self-questioning—a novel prompting strategy\nthat encourages prompt iteration to improve accuracy in a\nmedical context—are all steps in the correct direction.\nComprehensive frameworks that include the use of diverse\ntraining data sources and continuous evaluation of bias, such\nas those proposed by the World Economic Forum and the\nCoalition for Health AI, can provide useful guardrails as new\nproposals for ethical validation and have been tested [23,24].\nFurthermore, ensuring that physicians are actively involved in\nthe development and evaluation of LLMs for health care is\nessential in keeping with a physician-led approach. Strategies\nsuch as these are key in navigating the ethics of empathy and\nequity in the development of novel clinical technologies.\nIt is essential to approach the ethical conundrums of LLM\nadoption in clinical care with a balanced perspective. LLMs\nthat were built on data with inherent systemic biases must be\nimplemented strategically into health care through a\njustice-oriented innovation lens to advance health equity. To\nkeep pace with the accelerated adoption of LLMs in the clinic,\nethical evaluations should be conducted together with an\nevaluation of use case efficacy to ensure both efficient and\nethical health care. A complete assessment of the risks and\nbenefits associated with this technology—an admittedly\nchallenging task—may remain elusive if not tested in real-world\nsettings. Clinical use cases of LLMs are already being tested;\ndelaying collaboration among all stakeholders, including health\ncare professionals, ethicists, AI researchers, and (crucially)\npatients, will only delay the discovery of potential harms.\nReal-world pilots, therefore, should be deployed alongside\nregular monitoring, oversight, and feedback from all parties.\nAs we collectively seek to make full use of this exciting new\ntechnology, we must keep empathy and equity at the forefront\nof our minds.\nTable 1. Approaches to the incorporation of large language models (LLMs) in clinical care.\nImpact on empathy and health equityPrimary motivationApproach\nAdvancement-driven: incorporation of new and\nsophisticated technologies mainly aimed at im-\nproving efficiency\nLLM-led clinical care or patient-facing LLMs • Perpetuates and exacerbates inequities and\nbiases on which it was built, making it.\ndetrimental to achieving health equity\n• Replaces human empathy with artificial\nempathy, which threatens patient dignity\nHolistic, equitable, and empathetic health care\ndelivery\nPhysician-led LLM incorporation in clinical care • Early recognition of ways in which models\nperpetuate inequity and appropriate mea-\nsures to prevent this\n• Opportunity to actively leverage LLMs to\nmitigate existing inequities\n• Use of LLMs as tools in a physician’s\ntoolkit allows more time to engage in em-\npathetic dialogue with patients\nJMIR Med Educ 2023 | vol. 9 | e51199 | p. 2https://mededu.jmir.org/2023/1/e51199\n(page number not for citation purposes)\nKoranteng et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\nTable 2. Potential proactive measures for promoting equitable incorporation of large language models (LLMs) into clinical care.\nExamples of proactive measuresStakeholder\nRegulatory bodies • Development of robust regulations for software as a medical device that ensure appropriate strategies\nfor (1) continuous evaluation of evolving technology and (2) assessment of use cases that have\nsignificant impact in health care given the broad capabilities of LLMs\nProfessional societies • Development and continuous updates of guidelines for equitable use of LLMs in health care\n• Allocation of grant funding toward projects that aim to use LLMs to ameliorate inequities\nJournals • Prioritizing publications that focus on (1) novel methods of leveraging LLMs for equitable care\ndelivery and (2) comparisons of use cases of LLMs for equitable care delivery\nSoftware developers and industry • Collaboration with health care workers on model improvement strategies that improve health equity\nAcknowledgments\nThis project was supported in part by an award from the National Institute of General Medical Sciences (T32GM144273). The\ncontent is solely the responsibility of the authors and does not necessarily represent the official views of the National Institute of\nGeneral Medical Sciences or the National Institutes of Health.\nConflicts of Interest\nEF is co-chair of the Radiological Society of North America (RSNA) Health Equity Committee; associate editor and editorial\nboard member of the Journal of the American College of Radiology (JACR); has received speaker honoraria for academic Grand\nRounds, from WebMD and from GO2 for Lung Cancer foundation; GO2 Foundation Travel support; grant funding from NCI\nK08 1K08CA270430-01A1. ML is a consultant for GE Healthcare and for Takeda, Roche, and SeaGen Pharma. AL is a consultant\nfor the Abbott Medical Device Cybersecurity Council.\nReferences\n1. Dave T, Athaluri SA, Singh S. ChatGPT in medicine: an overview of its applications, advantages, limitations, future\nprospects, and ethical considerations. Front Artif Intell. 2023 May 4;6:1169595 [FREE Full text] [doi:\n10.3389/frai.2023.1169595] [Medline: 37215063]\n2. Rozado D. Wide range screening of algorithmic bias in word embedding models using large sentiment lexicons reveals\nunderreported bias types. PLoS One. 2020 Apr 21;15(4):e0231189 [FREE Full text] [doi: 10.1371/journal.pone.0231189]\n[Medline: 32315320]\n3. Garrido-Muñoz  I, Montejo-Ráez  A, Martínez-Santiago  F, Ureña-López  LA. A survey on bias in deep NLP. Appl Sci.\n2021 Apr 02;11(7):3184 [doi: 10.3390/app11073184]\n4. Liu R, Jia C, Wei J, Xu G, Vosoughi S. Quantifying and alleviating political bias in language models. Artificial Intelligence.\n2022 Mar;304:103654 [doi: 10.1016/j.artint.2021.103654]\n5. Li H, Moon JT, Purkayastha S, Celi LA, Trivedi H, Gichoya JW. Ethics of large language models in medicine and medical\nresearch. Lancet Digital Health. 2023 Jun;5(6):e333-e335 [doi: 10.1016/s2589-7500(23)00083-3]\n6. Meskó B, Topol EJ. The imperative for regulatory oversight of large language models (or generative AI) in healthcare.\nNPJ Digit Med. 2023 Jul 06;6(1):120 [FREE Full text] [doi: 10.1038/s41746-023-00873-0] [Medline: 37414860]\n7. Rao A, Kim J, Kamineni M, Pang M, Lie W, Dreyer KJ, et al. Evaluating GPT as an adjunct for radiologic decision making:\nGPT-4 versus GPT-3.5 in a breast imaging pilot. J Am Coll Radiol. 2023 Oct;20(10):990-997 [doi: 10.1016/j.jacr.2023.05.003]\n[Medline: 37356806]\n8. Rao A, Kim J, Kamineni M, Pang M, Lie W, Succi M. Evaluating ChatGPT as an adjunct for radiologic decision-making.\nmedRxiv. Preprint posted online February 7, 2023 [FREE Full text] [doi: 10.1101/2023.02.02.23285399] [Medline:\n36798292]\n9. Rao A, Pang M, Kim J, Kamineni M, Lie W, Prasad A, et al. Assessing the utility of ChatGPT throughout the entire clinical\nworkflow. medRxiv. Preprint posted online February 26, 2023 [FREE Full text] [doi: 10.1101/2023.02.21.23285886]\n[Medline: 36865204]\n10. Varney ET, Lee CI. The potential for using ChatGPT to improve imaging appropriateness. J Am Coll Radiol. 2023\nOct;20(10):988-989 [doi: 10.1016/j.jacr.2023.06.005] [Medline: 37400048]\n11. Chonde DB, Pourvaziri A, Williams J, McGowan J, Moskos M, Alvarez C, et al. RadTranslate: an artificial\nintelligence-powered intervention for urgent imaging to enhance care equity for patients with limited English proficiency\nduring the COVID-19 pandemic. J Am Coll Radiol. 2021 Jul;18(7):1000-1008 [FREE Full text] [doi:\n10.1016/j.jacr.2021.01.013] [Medline: 33609456]\nJMIR Med Educ 2023 | vol. 9 | e51199 | p. 3https://mededu.jmir.org/2023/1/e51199\n(page number not for citation purposes)\nKoranteng et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\n12. Sharma A, Lin IW, Miner AS, Atkins DC, Althoff T. Human–AI collaboration enables more empathic conversations in\ntext-based peer-to-peer mental health support. Nat Mach Intell. 2023 Jan 23;5(1):46-57 [doi: 10.1038/s42256-022-00593-2]\n13. Ayers JW, Poliak A, Dredze M, Leas EC, Zhu Z, Kelley JB, et al. Comparing physician and artificial intelligence chatbot\nresponses to patient questions posted to a public social media forum. JAMA Intern Med. 2023 Jun 01;183(6):589-596 [doi:\n10.1001/jamainternmed.2023.1838] [Medline: 37115527]\n14. Guidi C, Traversa C. Empathy in patient care: from 'Clinical Empathy' to 'Empathic Concern'. Med Health Care Philos.\n2021 Dec 01;24(4):573-585 [FREE Full text] [doi: 10.1007/s11019-021-10033-4] [Medline: 34196934]\n15. Smoktunowicz E, Barak A, Andersson G, Banos RM, Berger T, Botella C, et al. Consensus statement on the problem of\nterminology in psychological interventions using the internet or digital components. Internet Interv. 2020 Sep;21:100331\n[FREE Full text] [doi: 10.1016/j.invent.2020.100331] [Medline: 32577404]\n16. Jaffe S. US Surgeon General: loneliness is a public health crisis. The Lancet. 2023 May;401(10388):1560 [doi:\n10.1016/s0140-6736(23)00957-1]\n17. Nadeem M, Bethke A, Reddy S. StereoSet: Measuring stereotypical bias in pretrained language models. 2021 Presented\nat: 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing; 2021; Online p. 5356-5371 [doi: 10.18653/v1/2021.acl-long.416]\n18. Dortche K, McCarthy G, Banbury S, Yannatos I. Promoting health equity through improved regulation of artificial intelligence\nmedical devices. JSPG. 2023 Jan 23;21(03) [doi: 10.38126/JSPG210302]\n19. Blythe JA, Flores EJ, Succi MD. Justice and innovation in radiology. J Am Coll Radiol. 2023 Jul;20(7):667-670 [doi:\n10.1016/j.jacr.2023.05.005] [Medline: 37315912]\n20. Jiang LY, Liu XC, Nejatian NP, Nasir-Moin M, Wang D, Abidin A, et al. Health system-scale language models are\nall-purpose prediction engines. Nature. 2023 Jul 07;619(7969):357-362 [FREE Full text] [doi: 10.1038/s41586-023-06160-y]\n[Medline: 37286606]\n21. Ali SR, Dobbs TD, Hutchings HA, Whitaker IS. Using ChatGPT to write patient clinic letters. Lancet Digital Health. 2023\nApr;5(4):e179-e181 [doi: 10.1016/s2589-7500(23)00048-1]\n22. Singh S, Djalilian A, Ali MJ. ChatGPT and ophthalmology: exploring its potential with discharge summaries and operative\nnotes. Semin Ophthalmol. 2023 Jul 03;38(5):503-507 [doi: 10.1080/08820538.2023.2209166] [Medline: 37133418]\n23. A Blueprint for Equity and Inclusion in Artificial Intelligence 2022. World Economic Forum. URL: https://www.weforum.org/\nwhitepapers/a-blueprint-for-equity-and-inclusion-in-artificial-intelligence/ [accessed 2023-11-14]\n24. Blueprint for Trustworthy AI Implementation Guidance and Assurance for Healthcare 2023. Coalition for Health AI. URL:\nhttps://www.coalitionforhealthai.org/papers/blueprint-for-trustworthy-ai_V1.0.pdf [accessed 2023-11-14]\nAbbreviations\nAI: artificial intelligence\nEHR: electronic health record\nLLM: large language model\nEdited by K Venkatesh; submitted 24.07.23; peer-reviewed by SY Tan, B Bizzo, YD Cheng, L Zhu; comments to author 28.09.23;\nrevised version received 01.10.23; accepted 14.10.23; published 28.12.23\nPlease cite as:\nKoranteng E, Rao A, Flores E, Lev M, Landman A, Dreyer K, Succi M\nEmpathy and Equity: Key Considerations for Large Language Model Adoption in Health Care\nJMIR Med Educ 2023;9:e51199\nURL: https://mededu.jmir.org/2023/1/e51199\ndoi: 10.2196/51199\nPMID: 38153778\n©Erica Koranteng, Arya Rao, Efren Flores, Michael Lev, Adam Landman, Keith Dreyer, Marc Succi. Originally published in\nJMIR Medical Education (https://mededu.jmir.org), 28.12.2023. This is an open-access article distributed under the terms of the\nCreative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution,\nand reproduction in any medium, provided the original work, first published in JMIR Medical Education, is properly cited. The\ncomplete bibliographic information, a link to the original publication on https://mededu.jmir.org/, as well as this copyright and\nlicense information must be included.\nJMIR Med Educ 2023 | vol. 9 | e51199 | p. 4https://mededu.jmir.org/2023/1/e51199\n(page number not for citation purposes)\nKoranteng et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX"
}