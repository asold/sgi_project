{
    "title": "Graph-BERT and language model-based framework for protein–protein interaction identification",
    "url": "https://openalex.org/W4362663312",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2107292334",
            "name": "Kanchan Jha",
            "affiliations": [
                "Indian Institute of Technology Patna"
            ]
        },
        {
            "id": "https://openalex.org/A2588066669",
            "name": "Sourav Karmakar",
            "affiliations": [
                "National Institute of Technology Durgapur"
            ]
        },
        {
            "id": "https://openalex.org/A2130936750",
            "name": "Sriparna Saha",
            "affiliations": [
                "Indian Institute of Technology Patna"
            ]
        },
        {
            "id": "https://openalex.org/A2107292334",
            "name": "Kanchan Jha",
            "affiliations": [
                "Indian Institute of Technology Patna"
            ]
        },
        {
            "id": "https://openalex.org/A2588066669",
            "name": "Sourav Karmakar",
            "affiliations": [
                "National Institute of Technology Durgapur"
            ]
        },
        {
            "id": "https://openalex.org/A2130936750",
            "name": "Sriparna Saha",
            "affiliations": [
                "Indian Institute of Technology Patna"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2087574732",
        "https://openalex.org/W2076710572",
        "https://openalex.org/W2120973253",
        "https://openalex.org/W2169228168",
        "https://openalex.org/W2145753039",
        "https://openalex.org/W2315196278",
        "https://openalex.org/W3041304706",
        "https://openalex.org/W2967109100",
        "https://openalex.org/W2913099012",
        "https://openalex.org/W4280617006",
        "https://openalex.org/W3129073614",
        "https://openalex.org/W2786016794",
        "https://openalex.org/W3045012301",
        "https://openalex.org/W6600120041",
        "https://openalex.org/W2964051675",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6702248584",
        "https://openalex.org/W2995514860",
        "https://openalex.org/W2966916628",
        "https://openalex.org/W2099862356",
        "https://openalex.org/W2152705149",
        "https://openalex.org/W2056335234",
        "https://openalex.org/W2337686908",
        "https://openalex.org/W2213532196",
        "https://openalex.org/W2744103888",
        "https://openalex.org/W2624968853",
        "https://openalex.org/W2618265628",
        "https://openalex.org/W2622345656",
        "https://openalex.org/W2804331675",
        "https://openalex.org/W3120712761",
        "https://openalex.org/W2885583144",
        "https://openalex.org/W2911614559",
        "https://openalex.org/W2957436444",
        "https://openalex.org/W3095543875",
        "https://openalex.org/W4220800055",
        "https://openalex.org/W2044523575",
        "https://openalex.org/W2097651291",
        "https://openalex.org/W2155006702",
        "https://openalex.org/W2156125289",
        "https://openalex.org/W2951559648",
        "https://openalex.org/W3040739508",
        "https://openalex.org/W2085809045",
        "https://openalex.org/W2032408510",
        "https://openalex.org/W3156754474",
        "https://openalex.org/W4225897718",
        "https://openalex.org/W2062989416",
        "https://openalex.org/W2808890768"
    ],
    "abstract": "Abstract Identification of protein–protein interactions (PPI) is among the critical problems in the domain of bioinformatics. Previous studies have utilized different AI-based models for PPI classification with advances in artificial intelligence (AI) techniques. The input to these models is the features extracted from different sources of protein information, mainly sequence-derived features. In this work, we present an AI-based PPI identification model utilizing a PPI network and protein sequences. The PPI network is represented as a graph where each node is a protein pair, and an edge is defined between two nodes if there exists a common protein between these nodes. Each node in a graph has a feature vector. In this work, we have used the language model to extract feature vectors directly from protein sequences. The feature vectors for protein in pairs are concatenated and used as a node feature vector of a PPI network graph. Finally, we have used the Graph-BERT model to encode the PPI network graph with sequence-based features and learn the hidden representation of the feature vector for each node. The next step involves feeding the learned representations of nodes to the fully connected layer, the output of which is fed into the softmax layer to classify the protein interactions. To assess the efficacy of the proposed PPI model, we have performed experiments on several PPI datasets. The experimental results demonstrate that the proposed approach surpasses the existing PPI works and designed baselines in classifying PPI.",
    "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2023) 13:5663  | https://doi.org/10.1038/s41598-023-31612-w\nwww.nature.com/scientificreports\nGraph‑BERT and language \nmodel‑based framework \nfor protein–protein interaction \nidentification\nKanchan Jha 1*, Sourav Karmakar 2 & Sriparna Saha 1\nIdentification of protein–protein interactions (PPI) is among the critical problems in the domain of \nbioinformatics. Previous studies have utilized different AI‑based models for PPI classification with \nadvances in artificial intelligence (AI) techniques. The input to these models is the features extracted \nfrom different sources of protein information, mainly sequence‑derived features. In this work, we \npresent an AI‑based PPI identification model utilizing a PPI network and protein sequences. The PPI \nnetwork is represented as a graph where each node is a protein pair, and an edge is defined between \ntwo nodes if there exists a common protein between these nodes. Each node in a graph has a feature \nvector. In this work, we have used the language model to extract feature vectors directly from protein \nsequences. The feature vectors for protein in pairs are concatenated and used as a node feature vector \nof a PPI network graph. Finally, we have used the Graph‑BERT model to encode the PPI network \ngraph with sequence‑based features and learn the hidden representation of the feature vector for \neach node. The next step involves feeding the learned representations of nodes to the fully connected \nlayer, the output of which is fed into the softmax layer to classify the protein interactions. To assess \nthe efficacy of the proposed PPI model, we have performed experiments on several PPI datasets. The \nexperimental results demonstrate that the proposed approach surpasses the existing PPI works and \ndesigned baselines in classifying PPI.\nProteins are essential to all living species as they are involved in every cellular process and function. All pro-\nteins are made up of twenty standard amino acids (AAs), considered the building blocks of proteins, which are \narranged differently for different proteins. How these amino acids are arranged for each protein decides each \nprotein’s function. Mostly, proteins need a partner to work with, such as proteins, DNA, or RNA. The function-\nality of a protein in a cell is constrained if it exists alone, but when all the necessary proteins are present, they \ncooperate to perform their functions. protein–protein interactions (PPI) happen when two or more proteins \nhave physical contact due to some biochemical events or communicate through a signaling  process1. Through \nthese interactions, proteins control and assist various cellular functions, and biological processes, including cell \nsignaling, cellular transport, muscle contraction, catalytic activity, DNA transcription, and  replication2,3.\nThe thorough study of PPI has aided in modeling functional pathways to demonstrate the molecular mecha-\nnisms of cellular processes and in discovering drug  targets4. As PPI involves more heterogeneous processes, it is \nnecessary to identify and analyze the consequences of those interactions to gain a deeper understanding of their \nsignificance in the cell. Collecting a good amount of PPIs data for various species has become more accessible \ndue to large-scale experimental PPIs detection technologies like yeast two-hybrid (Y2H) screens, tandem affinity \npurification (TAP), mass spectrometric protein complex identification (MS-PCI), and other high-throughput \nbiological techniques. Despite significant efforts to uncover PPIs, there is still a considerable gap between PPIs \nidentified through experiments and PPIs found in nature. Experimentally identified PPIs cover only a small \npercentage of PPI networks. The rationale is that the entire PPI network cannot be explored using these experi-\nmental methods for PPI detection due to their costs and time requirements. Additionally, because the output is \naffected by the experimental setting and device resolution, there is a high rate of false positives and negatives in \nthe PPI data obtained using these  methods5. Therefore, high-throughput computational approaches are essential \nOPEN\n1Department of Computer Science and Engineering, Indian Institute of Technology Patna, Patna, Bihar 801103, \nIndia. 2Department of Computer Science and Engineering, National Institute of Technology Durgapur, Durgapur, \nWest Bengal 713209, India. *email: jha.kanchan15@gmail.com\n2\nVol:.(1234567890)Scientific Reports |         (2023) 13:5663  | https://doi.org/10.1038/s41598-023-31612-w\nwww.nature.com/scientificreports/\nfor identifying protein interactions. These computational-based techniques, when used in conjunction with \nexperimental methods, also enhance the quality and accuracy of PPI  prediction6.\nIntelligent computational approaches based on ML/DL are urgently needed to automate the identification \nand analysis of the interactions between proteins. Many computational techniques have been introduced to \ninvestigate PPI networks in organisms, using different protein information types, such as protein sequence, \nstructure, gene co-expression, gene ontology, etc. Sequence-based features are widely utilized to identify the \ninteractions between proteins among these sources of protein information due to their ease of  availability7. The \nsequence-based features can be categorized into two categories: manually crafted features and auto-engineered \nfeatures. The former method requires background biological knowledge to convert the symbolic representation \nof protein sequences into real-number feature vectors, whereas the latter method does not need any background \nknowledge. Most of the previous studies for PPI prediction have used manually crafted features as the initial fea-\nture vector, which are fed into the deep learning models to capture relevant features from the raw features. Some \nresearchers have recently devised techniques that provide fixed-length embeddings (per-protein and per-residue) \nfor variable-length protein  sequences8. The idea is to view each amino acid protein sequence as a sentence and \neach amino acid as a character or word. The language models from the NLP domain are borrowed and gener -\nate the embeddings for sequences with some modifications to handle the large length of amino acid sequences.\nA recent trend has seen graph neural networks picking up pace and becoming an essential tool in graph-based \napplications. For example, Huang et al.9 have used graph convolution to identify the associations between miRNA \nand drug resistance. The problem is constructed as a link prediction problem to predict their associations. Graph \nneural network-based approaches have been proposed to address other problems, such as chemical stability \n prediction10, protein interface  prediction11, protein interaction  prediction12, protein solubility  prediction13, and \nto model the adverse effects of  polypharmacy14. Y ang et al.15 have proposed a model for PPI prediction utilizing \nPPI network topology as a graph and the conjoint-triad (CT) method to get the node’s features. In a PPI network \ngraph, each node represents a protein, and an edge defines the relationship between protein pairs (interacting \nor non-interacting). Authors have used a signed variational graph auto-encoder (S-VGAE), which employs \ngraph convolution layers, to learn the hidden or compact representations of nodes. The learned representation \nof proteins in pairs is concatenated and fed into the neural network classifier to predict PPI.\nAll the above-mentioned studies solving different biomedical problems have used graph convolutional net-\nworks (GCN)16. The existing graph neural network variants, including graph convolutional networks, may suffer \nfrom the problems of suspended  animation17 and over-smoothing 18 because of the overreliance on the graph \nlinks. To address these issues, the Graph-BERT  model19 based on attention  mechanisms20,21 has been proposed, \nin which training is done by sampling nodes with their context, known as linkless subgraphs, from the input \ngraph. In this work, we present a framework that employs the Graph-BERT and SeqVec  language  model22 to \npredict PPI more effectively.\nThe contributions of our proposed work are listed below:\n• We devise the PPI prediction problem as a node classification problem by building a graph where each node \nrepresents a protein pair and an edge defines the relationship between two pairs if there is a common protein \nbetween them.\n• We develop a framework utilizing the Graph-BERT model to learn the hidden representations of graph nodes \nby focusing on linkless subgraphs instead of a complete input graph and SeqVec language model to generate \nembedding for each node in a graph.\n• We demonstrate that the proposed graph-based approach to predicting PPI is better than the existing graph-\nbased  approach15.\nRelated works\nSo far, several computational approaches have been put forth to categorize the interactions between proteins. \nSarkar and  Saha23 have reviewed the computational methods employing machine learning algorithms such as \nSVM, naive bayes, decision tree, and random forest, along with the different sources of protein information that \nare input to these algorithms. SVM is the most widely used machine learning algorithm to predict PPI among \nthese  algorithms24–27. Later, some studies have shown that the random/rotation forest-based approaches perform \nbetter than the popular SVM-based PPI  methods28,29.\nWith technological advances, researchers have started using deep learning algorithms to predict PPI and \nachieved better results than conventional machine learning-based approaches. For example, Wang et al. 30, and \nSun et al.31 have employed a stacked auto-encoder to learn the hidden or compact representation of input features, \nwhich are derived from protein sequences. Patel et al.32, and Zhang et al.33 have devised the protein interaction \nprediction tool named as DeepInteract and EnsDNN, respectively, both employing the deep neural network \n(DNN). In order to extract information from the protein sequence, Wang et al. 34 have proposed a sequence-\nstatistics-content (SSC) protein sequence encoding format. A 2D convolutional neural network is then used to \npredict the PPI utilizing SSC. All the studies discussed above have used hand-engineered or manually crafted \nfeatures as inputs to the classifiers. The rapidly developing deep learning technique that enables automatic feature \nengineering is having immense success in numerous fields. Some studies on PPI have also used auto-engineered \nfeatures. For instance, Li et al.35 have presented a framework utilizing a deep neural network, DNN-PPI, which \nconsiders features learned from protein sequences automatically to predict PPI. In this framework, the sequences \nof two interacting proteins are input to the two separate sequential layers of encoding, embedding, convolutional \nneural networks (CNN), and long-short-term memory (LSTM). The two outputs from the LSTM layer are \nthen merged and fed into a dense layer to predict labels. Gonzalez-Lopez et al.36 have suggested using an embed-\nding technique with a recurrent neural network-based architecture to predict interactions directly from protein \n3\nVol.:(0123456789)Scientific Reports |         (2023) 13:5663  | https://doi.org/10.1038/s41598-023-31612-w\nwww.nature.com/scientificreports/\nsequences. Chen et al. 37 have proposed a deep learning-based framework, PIPR, composed of convolutional \nlayers with pooling followed by bi-directional residual gated recurrent units. These two components are stacked \nalternatively to extract local and global features. The input to the PIPR model is the pre-trained embeddings of \nprotein sequences.\nThe deep learning algorithms can handle high-dimensional data and capture hidden associations in data with \nmulti-modal distributions. Recently some studies on  PPI38,39 have utilized multiple sources of protein informa-\ntion such as protein sequence, 3D structure, and gene ontology. They have developed deep multi-modal PPI \nmodels to predict the protein interactions utilizing different combinations of the available protein information. \nThey have used the latest deep learning algorithms to extract relevant features from these modalities. A living \norganism’s PPI data (interacting and non-interacting) can also be defined graphically as a PPI network, where \nnodes represent proteins and interactions between them are represented by edges. A graph-based deep learning \nmodel has been developed by Y ang et al.15 to predict protein interactions from PPI network topology. The authors \nhave used a signed variational graph auto-encoder to learn low-dimensional features from graph structure. The \nconjoint-triad (CT) method, which belongs to the manually crafted sequence-based method, has been utilized \nto get a node’s feature vector.\nCurrent work is also a step toward utilizing graph-based neural networks to predict the interaction between \nthe proteins. We formulate the PPI prediction as a node classification problem, where each node represents a \nprotein pair (interacting or non-interacting). We have used the Graph-BERT  model19 to learn the hidden repre-\nsentation of the node’s feature vector obtained by the language model SeqVec22 directly from protein sequences.\nMaterials and methodology\nThis section deals with the datasets that are used to substantiate the proposed approach, followed by the formula-\ntion of PPI prediction as a node classification problem. The proposed approach to predict protein interactions \ncomprises three modules: protein sequence embedding; learning hidden representations of graph nodes using \na graph transformer model; and PPI classification using learned representations. This section discusses each \nmodule in detail.\nDatasets. The Pan’s human PPI dataset (http:// www. csbio. sjtu. edu. cn/ bioinf/ LR_ PPI/ Data. htm)40containing \nboth positive and negative samples, has been used as a benchmark dataset to validate our proposed approach. \nThe HPRD (https:// www. hprd. org/) dataset serves as the source for the interacting pairs. The non-interacting \ndataset is comprised of non-interacting pairs from the negatome  database41 as well as pairings of proteins from \ndifferent subcellular localizations. The Swiss-Prot database has information related to the proteins’ subcellular \nlocalization. In this dataset, protein sequences with less than 50 amino acids and those with unknown amino \nacids were excluded. The self-interacting protein pairs were also discarded from this PPI dataset.\nWe have also used the datasets of other species, such as E. coli, Drosophila, and C. elegan, provided by Guo \net al.42 to evaluate the efficacy of the suggested approach. To create non-redundant PPI subsets of these species, \nthe CD-HIT  program43 has been used. A protein is discarded if its sequence identity is high ( > 40% ) or if it has \nfewer than 50 amino acids. The characteristics of these datasets are tabulated in Table 1.\nProblem definition. Given the PPI database and protein sequences, we build the graph G ppi= (V , E) , \nwhere V ={ P12 ,P13 ,P23 ...} is the set of protein pairs (interacting and non-interacting) or nodes, and E is the set \nof edges defined between two nodes if there is a common protein between them. The set of initial feature vectors \ngenerated from protein sequences for all nodes v ∈ V  in a graph G ppi built from the PPI network is represented \nby X. The objective is to predict the labels for all nodes (protein pairs) in a graph using the learned low-dimen-\nsional representations of nodes, which are passed through the fully connected (FC) layer followed by an output \nlayer (sofmax), expressed by Eq. (1):\nProtein sequence embedding. A protein’s amino acid sequence representation can be thought of as a \nlanguage. The concept is to view each protein sequence as a sentence, with each amino acid acting as a word or \ncharacter in the referred sentence. In order to model protein sequences, one can leverage the language model \ndeveloped for NLP tasks. In this work, protein sequences are encoded into useful feature vectors using the \nSeqVec22 embedding technique. The SeqVec is context-dependent and is an adaptation of the bidirectional lan-\nguage model ELMO (Embeddings from language models)44. The configuration of the model is similar to that of \n(1)Y = softmax(FC (Graph − BERT (X , E)))\nTable 1.  Statistics of PPIs datasets.\nDataset # Positive samples # Negative samples\nHuman 36,545 36,323\nE. coli 5576 4031\nDrosophila 19,712 14,900\nC. elegan 2877 1670\n4\nVol:.(1234567890)Scientific Reports |         (2023) 13:5663  | https://doi.org/10.1038/s41598-023-31612-w\nwww.nature.com/scientificreports/\nthe ELMO architecture with a few adjustments. Some of the changes made to the model include having a lesser \nnumber of tokens and making sure there are more unrolling steps to handle a longer protein sequence length. \nThe number of tokens was reduced to 28. These are the following: 20 tokens for standard amino acids, 3 for \nambiguous or unknown amino acids, 2 for uncommon amino acids, 2 for marking the beginning and ending of \nthe sequence, and 1 for masking.\nThe SeqVec embedder consists of one character convolution (charCNN)  layer45 followed by two bi-directional \nLSTM layers. Without taking into account the information from nearby words, the charCNN layer, which is \nthe first layer, maps each amino acid of a protein sequence to a fixed-length (1024) latent space. The output of \nthe charCNN layer serves as the input to the following layer, which is the first bi-LSTM layer. By concatenating \n512 from the forward pass and 512 from the backward pass, each bi-LSTM layer has a dimension of 1024. A \nprotein sequence of any length can be input to the ELMO-based SeqVec model, which is trained on UniRef50, \nand two types of embeddings are produced: per-residue (word-level) and per-protein (sentence-level). A protein \nsequence is first padded with <START> and <END> tokens to denote the beginning and end of the input before \nbeing passed to the first layer (charCNN) of the embedder. The pre-trained embedder creates an embedding \nof size (3, L, 1024) for an L length protein sequence by concatenating the outputs of three layers: one charCNN \nand two bi-LSTMs. Each amino acid in the protein sequence is associated with a 1024-dimensional embedding \nin each layer. The sum of the embeddings of the three layers yields the per-residue  embedding. We obtain the \nper-protein embedding of size 1024 by averaging the per-residue embeddings across the L length of the sequence. \nAs they generate uniform-length feature vectors for various-length protein sequences, we have used per-protein \nembeddings as feature vectors for nodes in the PPI network graph.\nPPI graph construction and classification. We have formulated the prediction of protein–protein \ninteractions using the PPI network and sequence-based features as a node classification problem. First, we built \nthe PPI network graph, where each node or vertex is a protein pair, and an edge is defined between two nodes if \nthere is a common protein between them, as depicted in Figure 1. In this work, we have used the SeqVec language \nmodel to get a feature vector of size 2048 for each node, i.e., protein pairs. To learn low-dimensional features \nfrom a graph and then classify the PPI, we have used the Graph-BERT model. The prime advantage of this model \nis that it addresses both the suspended animation problem and the over-smoothing problem of other graph \nneural networks, such as GCN. The reason is that Graph-BERT does not rely on graph links for representation \nlearning; instead, the model focuses on sampled linkless subgraphs. A suspended animation  problem17 refers to \na situation where the GCN model stops responding to training data and becomes unlearnable when the depth \nof the GCN model reaches a certain limit, known as the suspended animation limit. The GCN model is a variant \nof Laplacian smoothing that combines the features of a vertex with those of its close neighbors. The main reason \nGCNs are so effective is the smoothing process, which makes the features of vertices in the same cluster similar \nand significantly simplifies the classification task. The output features, however, may be over-smoothed 18 if a \nGCN is deep with multiple convolutional layers, making it difficult to distinguish between vertices from various \nclusters. The working steps of the Graph-BERT-based approach are as follows:\nFigure 1.  Illustration of the proposed approach.\n5\nVol.:(0123456789)Scientific Reports |         (2023) 13:5663  | https://doi.org/10.1038/s41598-023-31612-w\nwww.nature.com/scientificreports/\n• The input to the Graph-BERT model is the PPI network graph with node features.\n• Graph-BERT is trained using linkless subgraph batches sampled from the input graph rather than the entire \ngraph, G ppi . So, the next step is to perform the batching of linkless subgraphs. To control sample-related \nrandomness, the Graph-BERT model uses the top-k intimacy sampling strategy for sampling the subgraphs \nfrom the input graph. This sampling method is based on the graph intimacy matrix S ∈ R|V |×|V | , where entry \nS(i, j) calculates the degree of intimacy between nodes, vi and v j . The intimacy score S is based on the PageR-\nank algorithm defined as: S = α · (I− (1 − α)AD −1)−1 . Here, α ∈[ 0, 1] , AD−1 is the column-normalized \nadjacency matrix with A as the adjacency matrix and D  as the diagonal matrix of A. The intimacy matrix S \ndefines the learning context for any target node v ∈ V  by covering both local neighbors of v  and the nodes \nfar away.\n• The next step is to prepare the input node vector embeddings, which cover four parts: 1. Raw feature vector \nembedding is expressed as ex = Embed(x) ∈ Rdh×1 , converts the raw feature vector (x ) of each node v in \nsubgraph gi into a shared feature space with dh as dimension. Here, Embed  represents the fully connected \nlayer for numerical input attributes. 2. Weisfeiler-Lehman (WL) absolute role embedding is expressed as: \ner = Position_Embed (WL (v)) . Here, WL (v) ∈ N is the WL code for each node v based on the node’s struc-\ntural roles in the graph data and is invariant for different sampled subgraphs as it is pre-computed using the \nentire graph. The term Position_Embed  is borrowed from Vaswani et al.20 to preserve positional information. \n3. Intimacy-based relative positional embedding is expressed as: ep = Position_Embed (P(v)) ∈ R dh×1 . The \nterm P(v) is a positional index of node v  and is different for different sampled subgraphs for the identical \nnode, v. By default, the P(v) is 0, and the nodes closer to v have a small positional index. 4. Hop-based relative \ndistance embedding is expressed as: ed = Position_Embed (H (v,v′)) ∈ Rdh×1 . The term H (v, v′ ) represents \nthe relative distance of node v in subgraph gi in hops to v′ in the original input graph. The embedding ed is \nconsidered as the balance between the WL absolute role embedding and intimacy-based relative positional \nembedding.\n• The initial feature vector for node v in subgraph gi is calculated as: \n Here, the aggregate function is defined as the vector summation. This feature vector for all nodes in a sub -\ngraph gi is organized into a matrix, (H0) , fed to the graph transformer-based encoder. This encoder updates \nthe nodes’ feature vector recursively with several layers (D layers). The output of the lth layer is defined as : \n where Q = Hl−1W l\nQ; K = Hl−1W l\nK; V = Hl−1W l\nV . Here, W l\nQ , W l\nK , and W l\nV are the weight matrices for \nquery, key, and value, respectively. The term G _R(H l−1 ,Xi) denotes the graph residual term introduced by \nZhang et al.17. It enables each layer of the model to be fed with the nodes’ initial features (X i) or intermediate \nrepresentations (H l−1) to maintain the effective representations for the inputs. This term is added to overcome \nthe suspended animation problem. X i represents the raw features for all nodes in the subgraph, gi.\n• The next step is the representation fusion, defined as zi = Fusion(HD ) . The function Fusion is the average of \nrepresentations of all nodes in the subgraph, gi . zi is the final representation for the target node, vi.\n• The final step is to predict the label for each node in the graph. For that purpose, a functional component is \nattached to the Graph-BERT transformer module. The input to that component is the learned representation, \n(zi) . The functional component is defined as ˆyi = softmax(FC (zi)).\nExperimental setup. In this work, we have used the Graph-BERT (https:// github. com/ jwzha nggy/ Graph- \nBert) model, which is trained on our PPI datasets using GeForce GTX 1080 as the computing infrastructure. \nThe deep learning libraries we need to run the code are pytorch (https:// anaco nda. org/ pytor ch/ pytor ch), sklearn \n(https:// anaco nda. org/ anaco nda/ scikit- learn), transformers (https:// anaco nda. org/ conda- forge/ trans forme rs), \nand networkx (https:// anaco nda. org/ anaco nda/ netwo rkx). Based on the  literature19, the values of the param-\neters are chosen as attention head number: 2, hidden layer number: D=2,  hidden dropout rate: 0.5, attention \ndropout rate: 0.3, subgraph size: k=7, learning rate: 0.001. During model training, the binary cross-entropy loss is \nminimized with the help of Adam  optimizer46. The maximum number of epochs is chosen as 200 with an early \nstopping method to reduce overfitting. The proposed architecture is trained using a standard 80:20 train-test \nsplit training approach. The predictive capability of the proposed model is measured in terms of the parameters \nincluding accuracy, sensitivity, specificity, precision, F-score, and MCC. We have also reported the average 5-fold \ncross-validation results with standard deviation (Std. dev) values for all PPI datasets.\nResults and analysis\nHere, we analyze the results obtained by our method, which is followed by a comparison with previous studies \nand some designed baselines.\nResults on benchmark PPI dataset. The proposed graph-based approach is different from the existing \ngraph-based  method15 in three aspects: (1) Graph construction; (2) Node features; and (3) A graph-based neu-\nral network for learning low-dimensional features. The approach suggested by Y ang et al.15 constructs a graph \nwhere each node represents a protein, whereas in our case, each node represents a protein pair. We have used a \nlanguage model (auto-engineered) to generate the initial feature vector for each node, whereas previous work \n(2)h0 = aggregate(ex ,er,ep ,ed )\n(3)H l = softmax\n( QK T\n√dh\n)\nV + G _R(H l−1 ,Xi)\n6\nVol:.(1234567890)Scientific Reports |         (2023) 13:5663  | https://doi.org/10.1038/s41598-023-31612-w\nwww.nature.com/scientificreports/\nhas used the conjoint-triad method (manually crafted). Last, we have used a transformer-based graph model \ndesigned to address the problems of existing graph neural networks, whereas the previous work has used graph \nconvolutional neural networks, which may suffer from the problems of over-smoothing and suspended anima-\ntion.\nThe test set results of the proposed approach are tabulated in Table 2 for the benchmark human PPI dataset \nin terms of several evaluation metrics. We have designed some baselines to validate the correctness and effective-\nness of our approach. In baseline-1, also known as Modified S-VGAE, we have used the same model proposed \nby Y ang et al.15 [S-VGAE (https:// github. com/ fangy angbit/ S- VGAE)] but changed the feature vector of nodes \nwith our features, i.e., obtained by the SeqVec  method. Y ang et al. have used the conjoint-triad (CT) method \nto get each node’s feature vector. Results are reported in Table  2 (Modified S-VGAE). The designed baseline’s \naccuracy, sensitivity, specificity, and precision are 97.90%, 98.90%, 93.82%, and 98.09%, respectively. To make \nfair comparisons, we downloaded the code provided by Y ang et al. and carried out experiments on the human \nPPI dataset with different node feature vectors (Conjoint-triad-based and SeqVec language model-based). With \nthe CT-based feature vector and S-VGAE model, we achieve an accuracy of 97.07% (less than that reported by \nY ang et al. in their paper), whereas an accuracy of 97.90% is achieved with language model-based features and \nthe S-VGAE model. As it is evident, the performance of the S-VGAE using language model-based node features \nperforms better than the CT-based features. The accuracy (99.10%) that we get by our method shows that the \nSeqVec language model-based features and Graph-BERT together yield better results than the earlier graph-based \n work15 to predict PPI. We have designed another baseline-2, which is similar to our proposed approach. The only \ndifference is that we have generated embeddings for protein sequences in baseline-2 using the ProtBert47 language \nmodel. This language model is trained on the BFD-100 dataset and has employed the BERT  model21 to generate \nthe embeddings for protein sequences. The designed baseline’s accuracy, sensitivity, specificity, F-score, and MCC \nare 96.18%, 97.12%, 95.24%, 96.19%, and 92.37%, respectively, reported in Table  2 (Proposed Approach (Prot-\nBert)). As evident, the Graph-BERT-based PPI model utilizing the SeqVec language model embeddings performs \nbetter than those using the ProtBert language model embeddings to categorize the protein–protein interactions.\nResults on other PPI datasets. We have also validated the proposed approach’s effectiveness on other \nPPI datasets, such as C. elegan, Drosophila, and E. coli. The test results on these datasets are presented in Table 3. \nThe accuracy and F-score of C. elegan, Drosophila, and E. coli PPI datasets are {99.44%, 99.98%, 99.74%}, and \n{99.56%, 99.98%, 99.68%}, respectively. We have also presented the average of 5-fold cross-validation results \nof the suggested method in Table  4 to check if the proposed approach is able to generalize a pattern in results \nor not. The standard deviation values are presented inside parentheses. On training the proposed approach on \nthe combined dataset (C. elegan, Drosophila, and E. coli), we achieved a test accuracy of 94.79%, a specificity of \n94.77%, and a sensitivity of 94.81%. All the results mentioned here are statistically significant. The Welch’s t-test48 \nat 5% significance level has been conducted, which gives the p value. We get p values < 0.05 , which implies that \nthe improvement in results is statistically significant.\nTable 2.  Test set results on a benchmark human PPI dataset of baselines and the proposed approach.\nModel Accuracy Sensitivity Specificity Precision F-score MCC\nModified S-VGAE 97.90 98.90 93.82 98.09 98.50 93.56\nProposed approach (ProtBert) 96.18 97.12 95.24 95.28 96.19 92.37\nProposed approach (SeqVec) 99.10 97.92 100 100 98.94 98.19\nTable 3.  Test set results on other PPI datasets of the proposed approach.\nDatasets Accuracy Sensitivity Specificity Precision F-score MCC\nC. elegan 99.44 99.83 98.78 99.30 99.56 98.80\nDrosophila 99.98 99.96 100 100 99.98 99.96\nE. coli 99.74 99.62 99.82 99.75 99.68 99.46\nTable 4.  Average 5-fold cross-validation results with standard deviation values inside brackets on all PPI \ndatasets of the proposed approach.\nDatasets Accuracy Sensitivity Specificity Precision F-score MCC\nHuman 99.02 (0.13) 99.15 (0.95) 98.57 (1.19) 98.94 (0.88) 99.04 (0.10) 98.00 (0.28)\nC. elegan 99.51 (0.05) 99.20 (0.59) 99.51 (0.6) 99.72 (0.33) 99.46 (0.13) 98.96 (0.11)\nDrosophila 99.99 (0.007) 99.99 (0.01) 100 (0.00) 100 (0.00) 99.99 (0.007) 99.99 (0.01)\nE. coli 99.78 (0.09) 99.63 (0.12) 99.89 (0.10) 99.85 (0.14) 99.74 (0.11) 99.55 (0.19)\n7\nVol.:(0123456789)Scientific Reports |         (2023) 13:5663  | https://doi.org/10.1038/s41598-023-31612-w\nwww.nature.com/scientificreports/\nComparison with existing techniques. To date, a lot of studies have been conducted to predict PPI \nemploying artificial intelligence-based approaches. Following the state-of-the-art methods, we have divided the \nwhole dataset into a training set (80%) to train the model and a test set (20%) to evaluate the model’s perfor -\nmance. Initial  studies40,42,49,50 utilizing sequence-based feature vectors and traditional machine learning-based \nalgorithms to learn relevant features have reported training accuracy ranging from 90 to 97.90% for the human \nPPI dataset. Our approach’s training accuracy is more than 99.5% with an average test accuracy of 99.02%, which \nindicates the superiority of our method over these traditional methods. We have also compared our work with \nrecent studies employing the latest deep learning algorithms such as stacked auto-encoder31, CNN and  LSTM35, \ngraph-based  approaches12,15, and multi-modal PPI  models38,39. The results are summarized in Table 5. In Table 5, \nwe have presented the test set results to make the comparisons between the suggested and previous studies, and \nthe report illustrates the superior performance that our model delivers over the older one and validates the cor-\nrectness of our proposed approach. We re-implemented the graph-based approach (S-VGAE) 15 as the source \ncodes and PPI datasets are publicly available, and we have used the same PPI datasets to demonstrate our PPI \nmodel’s performance. For the human PPIs dataset, the accuracy and MCC of the proposed approach are 2.09% \nand 6.86% greater than those of the earlier graph-based  approach15. Table 6 reports the performance compari-\nsons for other species in terms of accuracy. The tabulated results (Tables 5 and 6) prove the efficacy of the sug-\ngested method over the existing methods.\nDiscussion. Current work to predict the interaction between proteins has highlighted two important aspects: \n(1) Language model-based feature extraction needs to be explored more as it captures important features directly \nfrom protein sequences. We have also shown that the SeqVec language-model-based features, with an accuracy \nof 97.90%, perform better than the conjoint-triad-based features, which have an accuracy of 97.07%. (2) The \ngraph-based transformer model (Graph-BERT) outperforms its graph convolutional counterpart in learning \nlow-dimensional node features.\nIn our previous  work12, utilizing the molecular graph of proteins and residue-level features obtained by lan-\nguage models, we have shown that the graph attention-based PPI model using SeqVec-based residue embeddings \nperforms better than those of ProtBERT -based embeddings. Current work has also shown the same pattern of \nresults. The proposed approach based on SeqVec -based protein embeddings (accuracy: 99.10%) outperforms \nthe PPI model based on ProtBERT-based protein embeddings (accuracy: 96.18%). In the future, we will explore \nother language models to generate embeddings for protein sequences and analyze the results.\nTo further analyze the predictive capability of the proposed approach, we trained our model on imbalanced \ndatasets. We created the imbalanced datasets by randomly selecting interacting and non-interacting pairs in dif-\nferent ratios. The test results on the human PPI dataset, along with the chosen ratio (interacting:non-interacting), \nare presented in Table 7. From these results, we can infer that the proposed approach performs well on imbal-\nanced datasets as well. We also created imbalanced datasets with interacting and non-interacting samples in a 1:10 \nratio to assess the proposed approach’s performance on skewed datasets of other species. The test set results are \npresented in Table 8. Based on the sensitivity and specificity values reported in Tables 7 and 8, we can conclude \nthat our approach performs well on skewed datasets.\nTable 5.  Performance comparisons of our approach with earlier approaches for Human dataset. Best values \nare highlighted in bold.\nModel Accuracy Sensitivity Specificity Precision F-score MCC\nSun et al.31 96.82 – – – – –\nLi et al.35 98.36 97.68 – 98.89 98.23 96.72\nJha and  Saha38 97.20 98.07 95.04 97.99 98.03 93.16\nY ang et al.15 97.07 98.19 93.46 97.98 98.09 91.88\nJha and  Saha39 97.52 98.20 95.92 98.26 98.23 94.07\nJha et al.12 98.13 98.84 96.18 98.62 98.73 95.20\nProposed approach 99.10 97.92 100 100 98.94 98.19\nTable 6.  Performance comparisons of our approach with earlier approaches for other PPI datasets in term of \naccuracy. Best values are highlighted in bold.\nSpecies Proposed Y ang et al.15 Li et al.35 Sun et al.31 Guo et al.25\nE. coli 99.74 95.61 95.94 93.23 95.28\nDrosophila 99.98 96.71 98.38 93.48 96.23\nC. elegan 99.44 94.69 98.66 97.86 97.32\n8\nVol:.(1234567890)Scientific Reports |         (2023) 13:5663  | https://doi.org/10.1038/s41598-023-31612-w\nwww.nature.com/scientificreports/\nConclusion\nIn this work, we devise the PPI prediction problem as a node classification problem. Firstly, we build the protein \nnetwork graph using the PPI database and assign each node (protein pair) a feature vector. The feature vectors \nare generated using the SeqVec language model from protein sequences. The Graph-BERT is used to learn the \nlow-dimensional embeddings for graph nodes, which are input to the fully connected layer (FC). The output of \nFC is passed through the softmax layer to predict the PPI labels. The obtained results showcase the superiority \nof the suggested approach over previous work, including the graph-based approach by Y ang et al. (S-VGAE)15. \nThe final improvements can be summarized under the following pointers: (1) On replacing the conjoint-triad-\nbased feature vector of the existing work (S-VGAE) with SeqVec language model-based embeddings (modified \nS-VGAE), we could find the accuracy improving from 97.07 to 97.90%. ( 2) By reformulating our problem as a \nnode-classification problem and then using Graph-BERT with the language model-based embeddings, we could \nfind the accuracy improving from 97.90 to 99.10%. Based on these experimental results, we can infer that the use \nof the pre-trained language model and Graph-BERT together boosts the predictive capability of the PPI model. \nFuture work aims to explore other pre-trained language models to generate embeddings for protein sequences. \nMoreover, we will explore the use of other sources of protein information, such as gene co-expression, which \ncan be utilized as a node feature vector in a PPI network graph.\nData availability\nThe source code for training and data to train the model are available at https:// github. com/ JhaKa nchan 15/ PPI_ \nGBERT. The dataset used in this study is available at https:// github. com/ fangy angbit/ S- VGAE/ tree/ master/ data.\nReceived: 28 November 2022; Accepted: 14 March 2023\nReferences\n 1. Keskin, O., Gursoy, A., Ma, B. & Nussinov, R. Principles of protein- protein interactions: What are the preferred ways for proteins \nto interact?. Chem. Rev. 108, 1225–1244 (2008).\n 2. Alberts, B. The cell as a collection of protein machines: Preparing the next generation of molecular biologists. Cell  92, 291–294 \n(1998).\n 3. Skrabanek, L., Saini, H. K., Bader, G. D. & Enright, A. J. Computational prediction of protein-protein interactions. Mol. Biotechnol. \n38, 1–17 (2008).\n 4. Pedamallu, C. S. & Posfai, J. Open source tool for prediction of genome wide protein-protein interaction network based on ortholog \ninformation. Source Code Biol. Med. 5, 1–6 (2010).\n 5. Mrowka, R., Patzak, A. & Herzel, H. Is there a bias in proteome research?. Genome Res. 11, 1971–1973 (2001).\n 6. Y ou, Z.-H., Zhou, M., Luo, X. & Li, S. Highly efficient framework for predicting interactions between proteins. IEEE Trans. Cybern. \n47, 731–743 (2016).\n 7. Ding, Z. & Kihara, D. Computational methods for predicting protein-protein interactions using various protein features. Curr. \nProtoc. Protein Sci. 93, e62 (2018).\n 8. Elnaggar, A. et al. Prottrans: Towards cracking the language of lifes code through self-supervised deep learning and high perfor -\nmance computing. IEEE Trans. Pattern Anal. Mach. Intell. (2021).\n 9. Huang, Y .-A., Hu, P ., Chan, K. C. & Y ou, Z.-H. Graph convolution for predicting associations between mirna and drug resistance. \nBioinformatics 36, 851–858 (2020).\n 10. Li, X. et al. Deepchemstable: Chemical stability prediction with an attention-based graph convolution network. J. Chem. Inf. Model. \n59, 1044–1049 (2019).\n 11. Fout, A. M. Protein Interface Prediction Using Graph Convolutional Networks. Ph.D. thesis, Colorado State University (2017).\n 12. Jha, K., Saha, S. & Singh, H. Prediction of protein-protein interaction using graph neural networks. Sci. Rep. 12, 1–12 (2022).\n 13. Chen, J., Zheng, S., Zhao, H. & Y ang, Y . Structure-aware protein solubility prediction from sequence through graph convolutional \nnetwork and predicted contact map. J. Cheminform. 13, 1–10 (2021).\n 14. Zitnik, M., Agrawal, M. & Leskovec, J. Modeling polypharmacy side effects with graph convolutional networks. Bioinformatics 34, \ni457–i466 (2018).\n 15. Y ang, F ., Fan, K., Song, D. & Lin, H. Graph-based prediction of protein-protein interactions with attributed signed graph embed-\nding. BMC Bioinform. 21, 1–16 (2020).\nTable 7.  Proposed approach’s test results for different ratios on human PPI dataset.\nRatio Accuracy Sensitivity Specificity Precision F-score MCC\n1:2.5 98.53 99.89 97.92 95.58 97.69 96.67\n1:5 98.63 97.42 98.89 94.78 96.08 95.27\n1:10 99.44 99.06 99.47 94.62 96.79 96.51\nTable 8.  Proposed approach’s test results for other PPI datasets with interacting:non-interacting ratio as 1:10.\nSpecies Accuracy Sensitivity Specificity Precision F-score MCC\nC. elegan 99.18 99.99 99.11 90.62 95.08 94.77\nDrosophila 99.96 99.66 99.99 99.99 99.83 99.82\nE. coli 99.66 98.75 99.75 97.53 98.13 97.95\n9\nVol.:(0123456789)Scientific Reports |         (2023) 13:5663  | https://doi.org/10.1038/s41598-023-31612-w\nwww.nature.com/scientificreports/\n 16. Kipf, T. N. & Welling, M. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv: 1609. 02907  \n(2016).\n 17. Zhang, J. & Meng, L. Gresnet: Graph residual network for reviving deep gnns from suspended animation. arXiv preprint arXiv: \n1909. 05729 (2019).\n 18. Li, Q., Han, Z. & Wu, X.-M. Deeper insights into graph convolutional networks for semi-supervised learning. In Thirty-Second \nAAAI Conference on Artificial Intelligence (2018).\n 19. Zhang, J., Zhang, H., Xia, C. & Sun, L. Graph-bert: Only attention is needed for learning graph representations. arXiv preprint \narXiv: 2001. 05140 (2020).\n 20. Vaswani, A. et al. Attention is all you need. Advances in Neural Information Processing Systems30 (2017).\n 21. Devlin, J., Chang, M.-W ., Lee, K. & Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. \narXiv preprint arXiv: 1810. 04805 (2018).\n 22. Heinzinger, M. et al. Modeling aspects of the language of life through transfer-learning protein sequences. BMC Bioinform.  20, \n1–17 (2019).\n 23. Sarkar, D. & Saha, S. Machine-learning techniques for the prediction of protein-protein interactions. J. Biosci. 44, 1–12 (2019).\n 24. Ben-Hur, A. & Noble, W . S. Kernel methods for predicting protein-protein interactions. Bioinformatics 21, i38–i46 (2005).\n 25. Guo, Y ., Yu, L., Wen, Z. & Li, M. Using support vector machine combined with auto covariance to predict protein-protein interac-\ntions from protein sequences. Nucleic Acids Res. 36, 3025–3030 (2008).\n 26. Y ou, Z.-H. et al. Detecting protein-protein interactions with a novel matrix-based protein sequence representation and support \nvector machines. BioMed. Res. Int.https:// doi. org/ 10. 1155/ 2015/ 867516 (2015).\n 27. Bandyopadhyay, S. & Mallick, K. A new feature vector based on gene ontology terms for protein-protein interaction prediction. \nIEEE/ACM Trans. Comput. Biol. Bioinf. 14, 762–770 (2016).\n 28. Wong, L. et al. Detection of interactions between proteins through rotation forest and local phase quantization descriptors. Int. J. \nMol. Sci. 17, 21 (2016).\n 29. Zhou, C., Yu, H., Ding, Y ., Guo, F . & Gong, X.-J. Multi-scale encoding of amino acid sequences for predicting protein interactions \nusing gradient boosting decision tree. PLoS ONE 12, e0181426 (2017).\n 30. Wang, Y .-B. et al. Predicting protein-protein interactions from protein sequences by a stacked sparse autoencoder deep neural \nnetwork. Mol. BioSyst. 13, 1336–1344 (2017).\n 31. Sun, T., Zhou, B., Lai, L. & Pei, J. Sequence-based prediction of protein protein interaction using a deep-learning algorithm. BMC \nBioinform. 18, 1–8 (2017).\n 32. Patel, S., Tripathi, R., Kumari, V . & Varadwaj, P . Deepinteract: Deep neural network based protein-protein interaction prediction \ntool. Curr. Bioinform. 12, 551–557 (2017).\n 33. Zhang, L., Yu, G., Xia, D. & Wang, J. Protein-protein interactions prediction based on ensemble deep neural networks. Neurocom-\nputing 324, 10–19 (2019).\n 34. Wang, Y . et al. Performance improvement for a 2d convolutional neural network by using ssc encoding on protein-protein interac-\ntion tasks. BMC Bioinform. 22, 1–16 (2021).\n 35. Li, H., Gong, X.-J., Yu, H. & Zhou, C. Deep neural network based predictions of protein interactions using primary sequences. \nMolecules 23, 1923 (2018).\n 36. Gonzalez-Lopez, F ., Morales-Cordovilla, J. A., Villegas-Morcillo, A., Gomez, A. M. & Sanchez, V . End-to-end prediction of protein-\nprotein interaction based on embedding and recurrent neural networks. In 2018 IEEE International Conference on Bioinformatics \nand Biomedicine (BIBM), 2344–2350 (IEEE, 2018).\n 37. Chen, M. et al. Multifaceted protein-protein interaction prediction based on siamese residual rcnn. Bioinformatics 35, i305–i314 \n(2019).\n 38. Jha, K. & Saha, S. Amalgamation of 3d structure and sequence information for protein-protein interaction prediction. Sci. Rep. \n10, 1–14 (2020).\n 39. Jha, K. & Saha, S. Analyzing effect of multi-modality in predicting protein-protein interactions. IEEE/ACM Trans. Comput. Biol. \nBioinform.https:// doi. org/ 10. 1109/ TCBB. 2022. 31575 31 (2022).\n 40. Pan, X.-Y ., Zhang, Y .-N. & Shen, H.-B. Large-scale prediction of human protein- protein interactions from amino acid sequence \nbased on latent topic features. J. Proteome Res. 9, 4992–5001 (2010).\n 41. Smialowski, P . et al. The negatome database: A reference set of non-interacting protein pairs. Nucleic Acids Res.  38, D540–D544 \n(2010).\n 42. Guo, Y . et al. Pred_ppi: A server for predicting protein-protein interactions based on sequence data with probability assignment. \nBMC. Res. Notes 3, 1–7 (2010).\n 43. Li, W . & Godzik, A. Cd-hit: A fast program for clustering and comparing large sets of protein or nucleotide sequences. Bioinfor-\nmatics 22, 1658–1659 (2006).\n 44. Peters, M. E. et al. Deep contextualized word representations. arXiv preprint arXiv: 1802. 05365 (2018).\n 45. Kim, Y ., Jernite, Y ., Sontag, D. & Rush, A. M. Character-aware neural language models. In Thirtieth AAAI Conference on Artificial \nIntelligence (2016).\n 46. Kingma, D. P . & Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv: 1412. 6980 (2014).\n 47. Elnaggar, A. et al. Prottrans: Towards cracking the language of life’s code through self-supervised deep learning and high perfor-\nmance computing. arXiv preprint arXiv: 2007. 06225 (2020).\n 48. Welch, B. L. The generalization of ‘student’s’problem when several different population varlances are involved. Biometrika  34, \n28–35 (1947).\n 49. Zhang, Y .-N., Pan, X.-Y ., Huang, Y . & Shen, H.-B. Adaptive compressive learning for prediction of protein-protein interactions \nfrom primary sequence. J. Theor. Biol. 283, 44–52 (2011).\n 50. Y ou, Z.-H., Yu, J.-Z., Zhu, L., Li, S. & Wen, Z.-K. A mapreduce based parallel svm for large-scale predicting protein-protein interac-\ntions. Neurocomputing 145, 37–43 (2014).\nAuthor contributions\nK.J. and S.S. conceived the idea, K.J. and S.K. conducted the experiments, K.J. and S.S. analyzed the results, K.J. \nwrote the manuscript with valuable input from S.S. All authors reviewed the manuscript.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to K.J.\nReprints and permissions information is available at www.nature.com/reprints.\n10\nVol:.(1234567890)Scientific Reports |         (2023) 13:5663  | https://doi.org/10.1038/s41598-023-31612-w\nwww.nature.com/scientificreports/\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023"
}