{
  "title": "T-GSA: Transformer with Gaussian-weighted self-attention for speech enhancement",
  "url": "https://openalex.org/W2980878766",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1785740175",
      "name": "Kim Jae-Young",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287283992",
      "name": "El-Khamy, Mostafa",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2020237244",
      "name": "Lee, Jungwon",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2069681747",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1983108229",
    "https://openalex.org/W2126942983",
    "https://openalex.org/W2963341071",
    "https://openalex.org/W2144404214",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2912191105",
    "https://openalex.org/W2964089206",
    "https://openalex.org/W1635512741",
    "https://openalex.org/W2094721231",
    "https://openalex.org/W2357464558",
    "https://openalex.org/W2603567530",
    "https://openalex.org/W1506438021",
    "https://openalex.org/W2963103134",
    "https://openalex.org/W2121973264",
    "https://openalex.org/W2120847449",
    "https://openalex.org/W2141411743"
  ],
  "abstract": "Transformer neural networks (TNN) demonstrated state-of-art performance on many natural language processing (NLP) tasks, replacing recurrent neural networks (RNNs), such as LSTMs or GRUs. However, TNNs did not perform well in speech enhancement, whose contextual nature is different than NLP tasks, like machine translation. Self-attention is a core building block of the Transformer, which not only enables parallelization of sequence computation, but also provides the constant path length between symbols that is essential to learning long-range dependencies. In this paper, we propose a Transformer with Gaussian-weighted self-attention (T-GSA), whose attention weights are attenuated according to the distance between target and context symbols. The experimental results show that the proposed T-GSA has significantly improved speech-enhancement performance, compared to the Transformer and RNNs.",
  "full_text": "T-GSA: TRANSFORMER WITH GAUSSIAN-WEIGHTED SELF-ATTENTION\nFOR SPEECH ENHANCEMENT\nJaeyoung Kim\njykim1@alumni.stanford.edu\nMostafa El-Khamy, Jungwon Lee\nSOC R&D, Samsung Semiconductor, Inc. USA\nEmails:{mostafa.e, jungwon2.lee}@samsung.com\nABSTRACT\nTransformer neural networks (TNN) demonstrated state-of-\nart performance on many natural language processing (NLP)\ntasks, replacing recurrent neural networks (RNNs), such as\nLSTMs or GRUs. However, TNNs did not perform well in\nspeech enhancement, whose contextual nature is different\nthan NLP tasks, like machine translation. Self-attention is\na core building block of the Transformer, which not only\nenables parallelization of sequence computation, but also\nprovides the constant path length between symbols that is es-\nsential to learning long-range dependencies. In this paper, we\npropose a Transformer with Gaussian-weighted self-attention\n(T-GSA), whose attention weights are attenuated according to\nthe distance between target and context symbols. The experi-\nmental results show that the proposed T-GSA has signiï¬cantly\nimproved speech-enhancement performance, compared to the\nTransformer and RNNs.\nIndex Termsâ€” Self-attention, Transformer, LSTM\n1. INTRODUCTION\nDeep neural networks have shown great success in speech en-\nhancement [1, 2, 3, 4, 5, 6] and performed better than the\npopular model-based statistical approaches, such as MMSE\nSTSA [7] or OM-LSA [8, 9].\nRecurrent neural networks (RNNs), such as LSTM [10]\nor GRU [11] were the most popular neural network architec-\ntures in speech enhancement, due to their powerful sequence\nlearning. Recently, the Transformer [12] was presented as a\nnew sequence-learning architecture with signiï¬cant improve-\nments over RNNs in machine translation and many other nat-\nural language processing tasks. The Transformer uses a self-\nattention mechanism to compute symbol-by-symbol correla-\ntions in parallel, over the entire input sequence, which are\nused to predict the similarity between the target and neighbor-\ning context symbols. The predicted similarity vector is nor-\nmalized by the softmax function and used as attention weights\nto combine context symbols.\nUnlike RNNs, the Transformer can process an input se-\nquence in parallel, which can signiï¬cantly reduce training\nand inference times. Moreover, the Transformer provides a\nï¬xed path length, that is the number of time steps to traverse,\nbefore computing attention weights or symbol correlations.\nTypically, RNNs have the path length proportional to the dis-\ntance between target and context symbols due to sequential\nprocessing, which makes it difï¬cult to learn long-range de-\npendencies between symbols. The Transformer resolved this\nissue with the self-attention mechanism.\nCurrent Transformer networks did not show improve-\nments in acoustic signal processing, such as speech enhance-\nment or speech denoising. The ï¬xed path length property that\nbeneï¬ted many NLP tasks is not compatible with the physical\ncharacteristics of acoustic signals, which tend to be more\ncorrelated with the closer components. Therefore, positional\nencoding is required to penalize attention weights according\nto the acoustic signal characteristics, such that less attention\nis provided to more distant symbols. In this paper, we pro-\npose a Transformer with Gaussian-weighted self-attention\n(T-GSA), whose attention weights are attenuated according\nto the distance between correlated symbols. The attenuation\nis determined by the Gaussian variance which can be learned\nduring training. Our evaluation results show that the proposed\nT-GSA signiï¬cantly improves over existing Transformer ar-\nchitectures, as well as over the former best recurrent model\nbased on the LSTM architecture.\n2. PROPOSED ARCHITECTURES\nFigure 1 shows the proposed denoising network based on\nthe Transformer encoder architecture. The original Trans-\nformer consists of encoder and decoder networks. In speech\ndenoising, the input and output sequences have the same\nlength. Hence, we only used the encoder network and align-\nment between input and output sequences is not necessary.\nThe network input, |Yu\nm,k|, is the short-time Fourier transform\n(STFT) spectrum magnitude of the noisy time-domain speech\nyu(n). uis the utterance index, mis the frame index, and k\nis the frequency index. The input noisy signal is given by\nyu(n) =xu(n) +nu(n), (1)\nwhere xu(n) and nu(n) are the clean and noisy speech sig-\nnals, respectively. Each encoder layer consists of multi-head\narXiv:1910.06762v3  [eess.AS]  11 Feb 2020\nLayer Norm\nFully-Connected\nMulti-Head SA\nLayer Norm\nğ»ğ‘™+1\nğ‘¢  \nEncoder layer\nEncoder layer\nEncoder layer\nEncoder layer\nEncoder layer\nEncoder layer\nEncoder layer\nEncoder layer\nEncoder layer\nEncoder layer\nEncoder layer\nEncoder layer\n ğ‘Œğ‘š,ğ‘˜\nğ‘¢   \nğ‘€ğ‘š,ğ‘˜\nğ‘¢  \nğ»ğ‘™\nğ‘¢  \nFig. 1. Block diagram of the Transformer encoder for speech\nenhancement\nself-attention, layer normalization and fully-connected layers,\nwhich is the same as the original Transformer encoder. The\nnetwork output is a time-frequency mask that predicts clean\nspeech by scaling the noisy input:\n|Ë†Xu\nm,k|= Mu\nm,k|Yu\nm,k|. (2)\nThe estimated clean spectrum magnitude|Ë†Xu\nm,k|is multiplied\nwith the phase of the input spectrum, from which the time-\ndomain signal, Ë†xu(n), is obtained by the inverse short-time\nFourier transform (ISTFT).\n2.1. GSA: Gaussian-weighted Self-Attention\nG. W.\nreshape\nreshape reshape reshape\nSoftmax\n ğ¾ğ‘™\nğ‘¢ ğ‘‡: (ğµ âˆ— ğ¸) Ã— (ğ·/ğ¸) Ã— ğ‘‡ \nğ‘†ğ‘™\nğ‘¢: (ğµ âˆ— ğ¸) Ã— ğ‘‡ Ã— ğ‘‡ \nğ»ğ‘™âˆ’1\nğ‘¢ : ğµ Ã— ğ‘‡ Ã— ğ· \nğ‘Šğ‘„ âˆ¶ ğ· Ã— ğ· \nğ‘Šğ¾ âˆ¶ ğ· Ã— ğ· \nğ‘Šğ‘‰ âˆ¶ ğ· Ã— ğ· \nğ‘‰ğ‘™\nğ‘¢ âˆ¶  ğµ âˆ— ğ¸ Ã— ğ‘‡ Ã— (ğ·/ğ¸) \nğ‘„ğ‘™\nğ‘¢: (ğµ âˆ— ğ¸) Ã— ğ‘‡ Ã— (ğ·/ğ¸) \nğ‘‚ğ‘™\nğ‘¢: (ğµ âˆ— ğ¸) Ã— ğ‘‡ Ã— (ğ·/ğ¸) \nğ´ğ‘™\nğ‘¢: ğµ Ã— ğ‘‡ Ã— ğ· \nğ‘Šğ‘‚ âˆ¶ ğ· Ã— ğ· \nFig. 2. Block diagram of the proposed multi-head self-\nattention: The G.W. block performs element-wise multiplica-\ntion of the Gaussian-weight matrix with the generated score\nmatrix. The matrix dimensions are noted besides each signal.\nFigure 2 describes the proposed Gaussian-weighted\nmulti-head self-attention. B, T and D are the batch size,\nsequence length, and input dimension. E is the number of\nself-attention units. Query, key and value matrices are deï¬ned\nas follows:\nQu\nl = WQHu\nlâˆ’1 (3)\nKu\nl = WKHu\nlâˆ’1 (4)\nVu\nl = WV Hu\nlâˆ’1 (5)\nwhere Hu\nl is lth hidden layer output. WQ, WK, and WV are\nnetwork parameters.\nThe multi-head attention module in our proposed T-GSA\nis modiï¬ed by deploying a Gaussian weighting matrix to scale\nthe score matrix, which is computed from the key and query\nmatrix multiplication as follows:\nSu\nl = Gl â—¦\n(Qu\nl (Ku\nl )T\nâˆš\nd\n)\n= Gl â—¦Cu\nl (6)\nGl is the Gaussian weight matrix which is element-wise mul-\ntiplied with the score matrix, Cu\nl . The proposed Gaussian\nweighting matrix is calculated as follows:\nGl =\nï£®\nï£¯ï£¯ï£¯ï£°\ngl\n1,1 gl\n1,2 Â·Â·Â· gl\n1,T\ngl\n2,1 gl\n2,2 Â·Â·Â· gl\n2,T\n...\ngl\nT,1 gl\nT,2 Â·Â·Â· gl\nT,T\nï£¹\nï£ºï£ºï£ºï£» (7)\nwhere gl\ni,j is e\nâˆ’|iâˆ’j|2\nÏƒ2\nl , iis a target frame index, jis a context\nframe index and Ïƒl is a trainable parameter that determines\nthe weight variance. For example, gl\ni,j corresponds to the\nscaling factor for the context frame j when the target frame\nindex is i. The diagonal terms in Gl correspond to the scaling\nfactors for the target frames, which is always set to be 1.gl\ni,j is\ninversely proportional to the distance between the target and\ncontext frames to provide larger attenuation of the attention\ngiven to the more distant context frames and smaller atten-\nuation for the closer ones. Since we let Ïƒl to be a trainable\nparameter, context localization can be learned by the acous-\ntic training data consisting of clean and noisy speech signals.\nAfter the softmax function, the self-attention matrix is multi-\nplied by the value matrix Vu\nl :\nOu\nl = SoftMax (|Su\nl |) Vu\nl (8)\nOne thing to note is that the absolute value of the matrix Su\nl\nis applied to the softmax function. The reason for this is that\nunlike NLP tasks, the negative correlation information in the\nsignal estimation is as important as the positive correlation.\nBy taking the absolute value of the Gaussian weighted score\nmatrix, the resultant self-attention weights will only depend\non the score magnitude, which enables to equally utilize both\npositive and negative correlations.\nRemark 1: The attention biasing [13] used for an acous-\ntic model design is a different positional encoding scheme,\nwhere additive bias is applied to the score matrix in the self-\nattention block. Different from our proposed GSA, the ad-\nditive bias can totally alter the attention signs which depend\non whether there is positive or negative correlation with the\nsymbol which is attended to. However, our proposed GSA\npreserves the correlation sign, and just alters its scale accord-\ning to the distance from the attended symbol.\n2.2. Extension to Complex Transformer Architecture\nEncoder layer\nEncoder layer\nEncoder layer\nEncoder layer\nEncoder layer\nEncoder layer\ndecoder layer\ndecoder layer\ndecoder layer\ndecoder layer\ndecoder layer\ndecoder layer\n ğ‘Œğ‘Ÿ\nğ‘¢  \nğ‘€ ğ‘Ÿ\nğ‘¢ \nLayer Norm\nComplex Fully-Connected\nLayer Norm\n ğ‘Œğ‘–\nğ‘¢  \nğ‘€ ğ‘–\nğ‘¢ \nMulti-Head SA\nLayer Norm\nMulti-Head SA\nLayer Norm\nMulti-Head SA\n \nLayer Norm\nMulti-Head SA\nLayer Norm\nğ»ğ‘™,ğ‘Ÿ\nğ‘¢  \nğ»ğ‘™,ğ‘–\nğ‘¢  \nğ»ğ‘™+1,ğ‘Ÿ\nğ‘¢  \nğ»ğ‘™+1,ğ‘–\nğ‘¢  \nFig. 3. Block Diagram of Complex Transformer architecture\nWe proposed a complex Transformer architecture for\nspeech enhancement, as shown in Figure 3. Compared with\nthe real Transformer architecture in Figure 1, the complex\nTransformer has two inputs and two outputs, corresponding\nto the real and imaginary parts of the input and output STFTs,\nrespectively. The network inputs, Yu\nr and Yu\ni , are the real\nand imaginary parts of the input noisy spectrum. By esti-\nmating both the real and complex parts of the output clean\nspeech spectrum, the complex Transformer denoiser showed\nsigniï¬cantly better SDR and PESQ performance. The net-\nwork output is a complex mask that generates the complex\ndenoised output Ë†Xu\nm,k as follows:\nË†Xu\nr,m,k = |Yu\nr,m,k|Mu\nr,m,k âˆ’|Yu\ni,m,k|Mu\ni,m,k (9)\nË†Xu\ni,m,k = |Yu\nr,m,k|Mu\ni,m,k + |Yu\ni,m,k|Mu\nr,m,k (10)\nwhere a subscript r means a real part and a subscript i cor-\nresponds to an imaginary part. The right grey block in Fig-\nure 3 describes the decoder layer of the complex Transformer\nnetwork. Hu\nl,r and Hu\nl,i are the real and imaginary outputs\nof the lth layer, respectively. The ï¬rst multi-head self atten-\ntion blocks are applied to each real and imaginary input sepa-\nrately. After layer normalization, the second multi-head atten-\ntion gets mixed inputs from the real and imaginary paths. For\nexample, the left second multi-head attention gets the right-\nside layer normalization output as key and value input in Fig-\nure 2. The query input comes from the left-side layer nor-\nmalization. The main idea is to exploit the cross-correlation\nbetween the real and imaginary parts by mixing them in the\nattention block. After another layer normalization, a com-\nplex fully-connected layer is applied. The complex fully-\nconnected layer has real and imaginary weights and the stan-\ndard complex operation is performed on the complex input\nfrom the second layer normalization.\n2.3. End-to-End Metric Optimization\nA multi-task denoising scheme has been recently proposed\nto train speech enhancement networks by jointly optimizing\nboth the Signal to Distortion Ratio (SDR) and the Perceptual\nEvaluation of Speech Quality (PESQ) metrics [14]. The pro-\nposed denoising framework outperformed the existing spec-\ntral mask estimation schemes [2, 1, 3] and generative mod-\nels [4, 5, 6] to provide a new state of the art performance.\nWe adopt the overall training framework shown in Figure 2\nin [14] to train our networks. First, the denoised complex\nspectrum is transformed into the time-domain acoustic signal\nvia Grifï¬n-Lim ISTFT [15]. Second, the proposed SDR and\nPESQ loss functions in [14] are computed based on the acous-\ntic signal. The two loss functions are jointly optimized by this\ncombined loss function:\nLSDR-PESQ = LSDR + Î±LPESQ, (11)\nwhere LSDR and LPESQ are SDR and PESQ loss functions de-\nï¬ned in Eq. 20 and 32 in [14], respectively. Î± is a hyper-\nparameter to adjust relative importance between the SDR and\nPESQ loss functions, and is set to be 3.2 after grid-search on\nthe validation set.\n3. EXPERIMENTAL RESULTS\n3.1. Experimental Settings\nTwo datasets were used for training and evaluation of the pro-\nposed Transformer architectures:\nQUT-NOISE-TIMIT [16]: QUT-NOISE-TIMIT is syn-\nthesized by mixing 5 different background noise sources with\nthe TIMIT [17]. For the training set, -5 and 5 dB SNR data\nwere used but the evaluation set contains all SNR ranges. The\ntotal length of train and test data corresponds to 25 hours\nand 12 hours, respectively. The detailed data selection is de-\nscribed at Table 1 in [14].\nVoiceBank-DEMAND [18]: 30 speakers selected from\nV oice Bank corpus [19] were mixed with 10 noise types: 8\nfrom Demand dataset [20] and 2 artiï¬cially generated one.\nTest set is generated with 5 noise types from Demand that\ndoes not coincide with those for training data.\nTable 1. SDR and PESQ results on QUT-NOISE-TIMIT: Test set consists of 6 SNR ranges:-10, -5, 0, 5, 10, 15 dB. The highest\nSDR or PESQ scores for each SNR test data were highlighted with bold fonts.\nSDR PESQ\nLoss Type -10 dB -5 dB 0 dB 5 dB 10 db 15 dB -10 dB -5 dB 0 dB 5 dB 10 db 15 dB\nNoisy Input -11.82 -7.33 -3.27 0.21 2.55 5.03 1.07 1.08 1.13 1.26 1.44 1.72\nCNN-LSTM -2.31 1.80 4.36 6.51 7.79 9.65 1.43 1.65 1.89 2.16 2.35 2.54\nO-T -3.25 0.92 3.39 5.35 6.39 8.10 1.29 1.45 1.63 1.87 2.07 2.29\nT-AB -2.80 1.18 3.67 5.67 6.78 8.18 1.49 1.67 1.85 2.01 2.28 2.50\nT-GSA (ours) -1.66 2.35 4.95 7.10 8.40 10.36 1.54 1.76 2.00 2.28 2.51 2.74\nC-T-GSA (ours) -1.57 2.51 5.03 7.36 8.58 10.40 1.43 1.64 1.88 2.17 2.40 2.67\n3.2. Main Result\nTable 1 shows SDR and PESQ performance of Transformer\nmodels on the QUT-NOISE-TIMIT corpus. CNN-LSTM is\nthe prior best performing recurrent model which is comprised\nof convolutional and LSTM layers. Its network architecture\nis described at Section 3 in [14]. O-T represents the original\nTransformer encoder, T-AB is the Transformer model with\nattention biasing explained in Remark 1, T-GSA is the real\nTransformer with Gaussian-weighted self-attention, and C-T-\nGSA is the complex Transformer model. The real transform-\ners consisted of 10 encoder layers and the complex Trans-\nformer has 6 decoder layers. The encoder and decoder layers\nwere described in Figure 1 and 3 and they have 1024 input and\noutput dimensions. All the neural network models evaluated\nin this section were trained to minimize LSDR-PESQ.\nFirst, O-T showed large performance degradation com-\npared with CNN-LSTM over all SNR ranges. Second, the\nT-AB substantially improved SDR and PESQ performance\nover O-T, which suggested that the positional encoding is an\nimportant factor to improve Transformer performance on this\ndenoising problem. However, the T-AB still suffered from the\nlarge loss compared with the recurrent model, CNN-LSTM.\nFinally, with the proposed Gaussian-weighting, the T-GSA\nmodel signiï¬cantly outperformed all the previous networks\nincluding CNN-LSTM. Especially, the large performance gap\nbetween attention biasing and Gaussian weighting suggested\nthat using negative correlations is as important as using posi-\ntive ones.\nThe complex Transformer showed 0.1 to 0.2 dB SDR\nimprovement over all the SNR ranges compared with the\nreal Transformer. However, the PESQ performance degraded\ncompared with the real Transformer. The reason for degra-\ndation could be overï¬tting due to the larger parameter size\nor due to the difï¬culty in predicting the phase spectrum. We\nare considering future research to make the complex network\nprovide consistent performance gains on both the SDR and\nPESQ metrics.\nTable 2. Evaluation on V oiceBank-DEMAND corpus\nModels CSIG CBAK COVL PESQ SSNR SDR\nNoisy Input 3.37 2.49 2.66 1.99 2.17 8.68\nSEGAN 3.48 2.94 2.80 2.16 7.73 -\nW A VENET 3.62 3.23 2.98 - - -\nTF-GAN 3.80 3.12 3.14 2.53 - -\nCNN-LSTM 4.09 3.54 3.55 3.01 10.44 19.14\nT-GSA (ours) 4.18 3.59 3.62 3.06 10.78 19.57\n3.3. Comparison with Generative Models\nTable 2 shows comparisons with other generative models.\nAll the results except CNN-LSTM and T-GSA (SEGAN [4],\nW A VENET [5] and TF-GAN [6]) are from the original pa-\npers. CSIG, CBAK and COVL are objective measures where\nhigh value means better quality of speech [21]. CSIG is mean\nopinion score (MOS) of signal distortion, CBAK is MOS of\nbackground noise intrusiveness and COVL is MOS of the\noverall effect. SSNR is Segmental SNR deï¬ned in [22].\nThe proposed Transformer model outperformed all the\ngenerative models for all the perceptual speech metrics listed\nin Table 2 with large margin. The main improvement came\nfrom the joint SDR and PESQ optimization schemes in [14]\nthat beneï¬ted both CNN-LSTM and T-GSA. Furthermore,\nT-GSA showed consistently better performance over CNN-\nLSTM for all the metrics, which agrees with the result at Ta-\nble 1.\n4. CONCLUSION\nWe proposed a Transformer architecture with Gaussian-\nweighted self-attention for speech enhancement. The at-\ntention weights are attenuated proportionally to the distance\nbetween the target frame and the symbols attended to, while\npreserving the correlation signs. The performance evalua-\ntion result showed that the proposed self-attention scheme\nsigniï¬cantly improved both the SDR and PESQ scores over\nprevious state-of-art recurrent and transformer networks.\n5. REFERENCES\n[1] Arun Narayanan and DeLiang Wang, â€œIdeal ratio mask\nestimation using deep neural networks for robust speech\nrecognition,â€ in Acoustics, Speech and Signal Process-\ning (ICASSP), 2013 IEEE International Conference on.\nIEEE, 2013, pp. 7092â€“7096.\n[2] Hakan Erdogan, John R Hershey, Shinji Watanabe, and\nJonathan Le Roux, â€œPhase-sensitive and recognition-\nboosted speech separation using deep recurrent neural\nnetworks,â€ in Acoustics, Speech and Signal Process-\ning (ICASSP), 2015 IEEE International Conference on.\nIEEE, 2015, pp. 708â€“712.\n[3] Yuxuan Wang, Arun Narayanan, and DeLiang Wang,\nâ€œOn training targets for supervised speech separation,â€\nIEEE/ACM Transactions on Audio, Speech and Lan-\nguage Processing (TASLP), vol. 22, no. 12, pp. 1849â€“\n1858, 2014.\n[4] Santiago Pascual, Antonio Bonafonte, and Joan Serra,\nâ€œSegan: Speech enhancement generative adversarial\nnetwork,â€ arXiv preprint arXiv:1703.09452, 2017.\n[5] Dario Rethage, Jordi Pons, and Xavier Serra, â€œA\nwavenet for speech denoising,â€ in 2018 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP). IEEE, 2018, pp. 5069â€“5073.\n[6] Meet H Soni, Neil Shah, and Hemant A Patil, â€œTime-\nfrequency masking-based speech enhancement using\ngenerative adversarial network,â€ 2018.\n[7] Yariv Ephraim and David Malah, â€œSpeech enhancement\nusing a minimum-mean square error short-time spectral\namplitude estimator,â€ IEEE Transactions on acoustics,\nspeech, and signal processing, vol. 32, no. 6, pp. 1109â€“\n1121, 1984.\n[8] Yariv Ephraim and David Malah, â€œSpeech enhancement\nusing a minimum mean-square error log-spectral ampli-\ntude estimator,â€IEEE transactions on acoustics, speech,\nand signal processing, vol. 33, no. 2, pp. 443â€“445, 1985.\n[9] Israel Cohen and Baruch Berdugo, â€œSpeech enhance-\nment for non-stationary noise environments,â€ Signal\nprocessing, vol. 81, no. 11, pp. 2403â€“2418, 2001.\n[10] Sepp Hochreiter and J Â¨urgen Schmidhuber, â€œLong short-\nterm memory,â€ Neural computation, vol. 9, no. 8, pp.\n1735â€“1780, 1997.\n[11] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,\nand Yoshua Bengio, â€œEmpirical evaluation of gated re-\ncurrent neural networks on sequence modeling,â€ arXiv\npreprint arXiv:1412.3555, 2014.\n[12] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser,\nand Illia Polosukhin, â€œAttention is all you need,â€ in Ad-\nvances in neural information processing systems, 2017,\npp. 5998â€“6008.\n[13] Matthias Sperber, Jan Niehues, Graham Neubig, Sebas-\ntian StÂ¨uker, and Alex Waibel, â€œSelf-attentional acoustic\nmodels,â€ arXiv preprint arXiv:1803.09519, 2018.\n[14] Jaeyoung Kim, Mostafa El-Kharmy, and Jungwon Lee,\nâ€œEnd-to-end multi-task denoising for joint sdr and pesq\noptimization,â€ arXiv preprint arXiv:1901.09146, 2019.\n[15] Daniel Grifï¬n and Jae Lim, â€œSignal estimation from\nmodiï¬ed short-time fourier transform,â€ IEEE Transac-\ntions on Acoustics, Speech, and Signal Processing, vol.\n32, no. 2, pp. 236â€“243, 1984.\n[16] David B Dean, Sridha Sridharan, Robert J V ogt, and\nMichael W Mason, â€œThe qut-noise-timit corpus for the\nevaluation of voice activity detection algorithms,â€ Pro-\nceedings of Interspeech 2010, 2010.\n[17] John S Garofolo, Lori F Lamel, William M Fisher,\nJonathan G Fiscus, and David S Pallett, â€œDarpa timit\nacoustic-phonetic continous speech corpus cd-rom. nist\nspeech disc 1-1.1,â€ NASA STI/Recon technical report n,\nvol. 93, 1993.\n[18] Cassia Valentini, Xin Wang, Shinji Takaki, and Junichi\nYamagishi, â€œInvestigating rnn-based speech enhance-\nment methods for noise-robust text-to-speech,â€ in 9th\nISCA Speech Synthesis Workshop, 2016, pp. 146â€“152.\n[19] Christophe Veaux, Junichi Yamagishi, and Simon King,\nâ€œThe voice bank corpus: Design, collection and data\nanalysis of a large regional accent speech database,â€\nin Oriental COCOSDA held jointly with 2013 Confer-\nence on Asian Spoken Language Research and Evalua-\ntion (O-COCOSDA/CASLRE), 2013 International Con-\nference. IEEE, 2013, pp. 1â€“4.\n[20] Joachim Thiemann, Nobutaka Ito, and Emmanuel Vin-\ncent, â€œThe diverse environments multi-channel acous-\ntic noise database: A database of multichannel environ-\nmental noise recordings,â€ The Journal of the Acousti-\ncal Society of America, vol. 133, no. 5, pp. 3591â€“3591,\n2013.\n[21] Yi Hu and Philipos C Loizou, â€œEvaluation of objective\nquality measures for speech enhancement,â€IEEE Trans-\nactions on audio, speech, and language processing, vol.\n16, no. 1, pp. 229â€“238, 2008.\n[22] Schuyler R Quackenbush, â€œObjective measures of\nspeech quality (subjective).,â€ 1986.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8281961679458618
    },
    {
      "name": "Computer science",
      "score": 0.7058709859848022
    },
    {
      "name": "Recurrent neural network",
      "score": 0.5985986590385437
    },
    {
      "name": "Machine translation",
      "score": 0.5739414691925049
    },
    {
      "name": "Computation",
      "score": 0.5543790459632874
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5084454417228699
    },
    {
      "name": "Artificial neural network",
      "score": 0.4965515732765198
    },
    {
      "name": "Speech recognition",
      "score": 0.4598364233970642
    },
    {
      "name": "Gaussian",
      "score": 0.44657766819000244
    },
    {
      "name": "Natural language processing",
      "score": 0.3693697452545166
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3223963975906372
    },
    {
      "name": "Algorithm",
      "score": 0.313212126493454
    },
    {
      "name": "Engineering",
      "score": 0.09688487648963928
    },
    {
      "name": "Electrical engineering",
      "score": 0.08484980463981628
    },
    {
      "name": "Physics",
      "score": 0.07098415493965149
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2250650973",
      "name": "Samsung (South Korea)",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I4210101778",
      "name": "Samsung (United States)",
      "country": "US"
    }
  ]
}