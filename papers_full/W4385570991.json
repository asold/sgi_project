{
    "title": "UMUTeam and SINAI at SemEval-2023 Task 9: Multilingual Tweet Intimacy Analysis using Multilingual Large Language Models and Data Augmentation",
    "url": "https://openalex.org/W4385570991",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2244707873",
            "name": "José Antonio García-Díaz",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2792595805",
            "name": "Ronghao Pan",
            "affiliations": [
                "Universidad de Murcia"
            ]
        },
        {
            "id": "https://openalex.org/A3202598104",
            "name": "Salud Maria Jimenez-Zafra",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5092596225",
            "name": "María-Teresa Martn-Valdivia",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2270656199",
            "name": "L Alfonso Ureña-López",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2303035916",
            "name": "RAFAEL VALENCIA-GARCÍA",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4385572469",
        "https://openalex.org/W3037109418",
        "https://openalex.org/W4205931544",
        "https://openalex.org/W3122890974",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4214730577",
        "https://openalex.org/W2563351168",
        "https://openalex.org/W3101637242",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W4200252192"
    ],
    "abstract": "José Antonio García-Díaz, Ronghao Pan, Salud María Jiménez Zafra, María-Teresa Martn-Valdivia, L. Alfonso Ureña-López, Rafael Valencia-García. Proceedings of the The 17th International Workshop on Semantic Evaluation (SemEval-2023). 2023.",
    "full_text": "Proceedings of the The 17th International Workshop on Semantic Evaluation (SemEval-2023), pages 293–299\nJuly 13-14, 2023 ©2023 Association for Computational Linguistics\nUMUTeam and SINAI at SemEval-2023 Task 9: Multilingual Tweet\nIntimacy Analysis using Multilingual Large Language Models and Data\nAugmentation\nJosé Antonio García-Díaz1, Ronghao Pan1, Salud María Jiménez Zafra2,\nMaría-Teresa Martín-Valdivia2, L. Alfonso Ureña-López2, Rafael Valencia-García1\n1 Facultad de Informática, Universidad de Murcia, Campus de Espinardo, 30100 Murcia, Spain\n{joseantonio.garcia8, ronghao.pan, valencia}@um.es\n2Computer Science Department, SINAI, CEATIC, Universidad de Jaén, 23071, Spain\n{sjzafra, maite, laurena}@ujaen.es\nAbstract\nThis work presents the participation of the\nUMUTeam and the SINAI research groups in\nthe SemEval-2023 Task 9: Multilingual Tweet\nIntimacy Analysis. The goal of this task is to\npredict the intimacy of a set of tweets in 10 lan-\nguages: English, Spanish, Italian, Portuguese,\nFrench, Chinese, Hindi, Arabic, Dutch and Ko-\nrean, of which, the last 4 are not in the training\ndata. Our approach to address this task is based\non data augmentation and the use of three mul-\ntilingual Large Language Models (multilingual\nBERT, XLM and mDeBERTA) by ensemble\nlearning. Our team ranked 30th out of 45 partic-\nipants. Our best results were achieved with two\nunseen languages: Korean (16th) and Hindi\n(19th).\n1 Introduction\nIn Natural Language Processing (NLP), intimacy\ncan be described as how people communicate their\nperception and willingness to share personal data\nand emotions to their audience (Pei and Jurgens,\n2020). The SemEval 2023 Task 9, entitled Multilin-\ngual Tweet Intimacy Analysis (MTIA) (Pei et al.,\n2023), consists of a regression task in which the\nparticipants should rate in a score from 1 to 5 the in-\ntimacy of short documents written in 10 languages:\nEnglish, Spanish, Italian, Portuguese, French, Chi-\nnese, Hindi, Arabic, Dutch and Korean. This task\nwas co-organized by University of Michigan and\nSnap Inc. There are two main challenges concern-\ning this task. On the one hand, the training dataset\nprovided to the participants does not cover all the\nevaluated languages, but only six of them: English,\nSpanish, Italian, Portuguese, French, and Chinese.\nHowever, the evaluation is conducted in those six\nlanguages plus Hindi, Arabic, Dutch and Korean.\nOn the other hand, participants were only allowed\nto submit a unique run, which hinders the shared\ntask.\nOur strategy to solve the MTIA challenge con-\nsists of an ensemble learning composed of three\nmultilingual Large Language Models (LLM): mul-\ntilingual BERT (Devlin et al., 2018), XLM (Lample\nand Conneau, 2019), and mDeBERTA (He et al.,\n2021). Besides, we use data augmentation incor-\nporating to the training the dataset suggested by\nthe organizers and provided in the work of Pei and\nJurgens (2020), with more than two thousand En-\nglish questions from Reddit and other sources and\nannotated with intimacy scores in the range [-1, 1].\nOur participation achieved modest results in the\ntask, reaching the 30th position in the leader-board,\nwith a Pearson’s R of 0.53. The best result is\nachieved by Lazybob, with a Pearson’s R of 0.62.\nAs commented above, as the participants were only\nallowed to submit a unique run, the analysis of\nour proposal is mainly based on a custom valida-\ntion split. Additional resources concerning our par-\nticipation can be found at https://github.com/\nNLP-UMUTeam/semeval-2023-mtia .\n2 Background\nThe organisers of the task provided the participants\nwith the novel MINT dataset (Pei et al., 2023),\nwhose original training split consists of 9491 tweets\nrated with an intimacy score. The tweets were com-\npiled between 2018 and 2022. To obtain tweets in\ndifferent languages, the authors combined language\nfilters in Twitter with language detectors models\nsuch as fastText (Joulin et al., 2016). Next, the\nauthors created clusters of tweets of each language\nand several annotators rated the tweets in a scale\nfrom 1 (not intimate at all) to 5 (very intimate).\nAs it can be observed, in the histogram plotted in\nFigure 1, most of the samples are rated with low\nscores. Regarding the six languages involved dur-\ning the training, these are almost balanced, with\n1596 documents written in Portuguese and Chinese,\n293\n1592 in Spanish, 1588 in French, 1587 in English\nand 1532 in Italian. An example of the dataset is\nthe Spanish text “Necesito paz mental”1, rated with\nan intimacy score of 2.8. In Figure 2 the rounding\nlabel distribution is shown. The majority of labels\nare between 2 and 3 and with fewer instances of\nlabels near to 0 or 5.\nCH\nEN\nES\nFR\nIT\nPT\nlang\n0\n200\n400\n600\n800\n1000\n1200\n(0.996, 1.8]\n(1.8, 2.6]\n(2.6, 3.4]\n(3.4, 4.2]\n(4.2, 5.0]\nFigure 1: Histogram of the Intimacy score over the\ndataset, grouped per language\nTexts\nScore\n0\n1\n2\n3\n4\n5\n0 2500 5000 7500 10000 12500\nFigure 2: Rounding label distribution\nThe participants of the task were encouraged to\nuse the dataset provided in Pei and Jurgens (2020);\nwhich contains English sentences with an intimacy\nscore between -1 and 1.\n3 System Overview\nOur pipeline for solving the MTIA 2023 shared\ntask is depicted in Figure 3. In a nutshell, it can\nbe described as follows. First, we clean and pre-\nprocess the MTIA dataset and keep a small portion\nof the training split to create a custom validation\n1In English: I need peace of mind\nsplit. Second, we perform a data augmentation\nstage applying Google Translate to the dataset of\nPei and Jurgens (2020). Third, we evaluate three\nmulti-lingual LLMs and one model based on lin-\nguistic features. Forth, we build an ensemble learn-\ning model that averages the predictions of the three\nLLMs to send our final predictions to the organizers\nof the task.\nConcerning the data cleaning stage, we strip hy-\nperlinks, hashtags, mentions and white space char-\nacters. Regarding the dataset splitter step, we re-\nserve a 20% of the tweets from the training split\nfor custom validation purposes. Next, we enlarge\nthe training dataset proposed by incorporating the\ndataset provided in Pei and Jurgens (2020). This\ndataset contains sentences written in English. We\nuse Google Translate to translate these sentences to\nSpanish, Italian, Portuguese, French, Hindi, Arabic,\nDutch and Korean. This way, we could incorpo-\nrate 21573 new sentences to the training. As this\ndataset is rated in rank from -1 to 1, we translate\nthe ratings to a scale from 1 to 5, maintaining the\nratio. Besides, it is worth noting that none of these\nnew instances are used for custom validation.\n4 Experimental Setup\nDuring the evaluation phase, apart from the mul-\ntilingual LLMs, we evaluate the usage of lin-\nguistic features from UMUTextStats (García-Díaz\net al., 2022c). The linguistic features from UMU-\nTextStats have been evaluated in several NLP tasks,\nsuch as author profiling (García-Díaz et al., 2022a),\nsatire identification (García-Díaz and Valencia-\nGarcía, 2022), and hate-speech detection (García-\nDíaz et al., 2022b).\nUMUTextStats is designed for the Spanish lan-\nguage, but it has a subset of language-independent\nfeatures. These features are stylometric features\nand features related to Named Entity Recognition\n(NER) and Part-of-Speech (PoS). To extract these\nfeatures, UMUTextStats relies on Stanza (Qi et al.,\n2020) to extract some features related to PoS and\nNER. However, not all the languages involved in\nthe MTIA shared task have models per Stanza, so\nthe linguistic features were not useful for some of\nthe languages involved on this shared task. Ac-\ncordingly, we decided not to include the Linguistic\nFeatures (LF) in the final submission.\nHowever, we use the linguistic features to make\nan analysis of the Spanish split of the dataset and\nwe observe a correlation with misspelled words\n294\nData \nAugmentation \nDataset \nSplitter \nModel \nT rainer \nEnsemble \nLearning \nClassification \nReport \nMTIA \nDataset \nData \nCleaning \nFigure 3: System architecture\nwith intimacy followed by morphological features\nrelated to proper and common nouns, personal pro-\nnouns in first, second person, and third person. We\nalso identify a correlation with stylometric clues\nconcerning the length of the tweets and with the us-\nage of hyperboles, proper from figurative language.\nThese results are depicted in Figure 4.\nCoefficient\nplot_index\n(ERR) orthographics\n(ERR) orthographics-misspelled-words\n(MOR) morphology-nouns\n(MOR) morphology-pronouns-personal-person-second\n(MOR) morphology-pronouns-personal\n(MOR) morphology-nouns-proper\n(MOR) morphology-pronouns-personal-number\n(MOR) morphology-pronouns-personal-person-first\n(MOR) morphology-pronouns-personal-person-third\n(MOR) morphology-nouns-common\n(MOR) morphology-pronouns-personal-number-plural\n(MOR) morphology-pronouns\n(STY) corpus-readability\n(STY) corpus-words-with-1ltr\n(MOR) morphology-pronouns-personal-person\n(PRA) figurative-language-hyperboles\n(STY) corpus-words-per-sentence\n(MOR) morphology-verbs-tense-present\n(SOC) urls\n0.00 0.03 0.05 0.08 0.10 0.13\nFigure 4: Information gain of the best 20 linguistic\nfeatures\nNext, the regression neural network architecture\nis described. For each LLM we conduct an hyper-\nparameter optimization stage consisting of training\n10 models for each LLM evaluating different pa-\nrameters, including the learning rate, the number\nof epochs for traning, the warm up steps and the\nweight decay. The results of the best model for\neach LLM are depicted in Table 1. It can be ob-\nserved that the best models require for small train-\ning epochs and all require of warmup steps and\nweight decay. Next, we extract the sentence em-\nbeddings for each LLM. This results in a vector of\nsize 768 for each document.\nFinally, we conduct another hyperparameter op-\ntimization stage using Keras. We follow this step\nto be consistent with the LFs and the LLMs. The\nresults of this experiment are reported in Table 2.\nFor each feature set, we evaluate 55 models chang-\ning the neural network architecture (its number of\nneurons and hidden layers), the dropout, the batch\nsize and the activation function. We can observe\nthat the best models for the LF and for mBERT are\ncomplex neural networks with 5 and 8 hidden lay-\ners respectively. The LF neural network has brick\nsize (all layers have the same number of neural\nnetworks) but mBERT has a diamond shape (the\ninner layers have much more neurons). All models\nbenefit for a strong dropout mechanism and most\nof them also benefit from large batch sizes.\n5 Results\n5.1 Validation results\nThe goal of the MTIA shared task is to predict the\nintimacy of tweets with a range from 1 to 5. The\nperformance of each submission is ranked based\non Pearson’s R over the test split. However, for our\nanalysis with our custom validate split, we evaluate\nthe following metrics: (1) Explained Variance (EV),\nthat measures how many information we lose by ap-\nproximating the dataset. A small EV indicates the\ntraining process has strong oscillations; (2) Root\nMean Squared Logarithmic Error (RMSLE), is the\nroot mean squared error of the log-transformed pre-\ndicted and log-transformed real values. RMSLE is\nan effective metric when the label has exponential\ngrowth and when we want to measure the percent-\nage of errors, instead of the absolute value of errors;\n(3) Pearson’s R, measures the strength of the lin-\near association between the predicted and the real\nvalues. Pearson’s R is the official metric for the\nMTIA shared task; (4) R-squared (R2), which is\nthe proportion of variation in the outcome that is\nexplained by the predictor variables. The higher the\nR-squared, the better the model; (5) Mean Absolute\nError (MAE), that is the average absolute differ-\nence between the predicted and real values. MAE\nis less sensitive to outliers; (6) Mean Squared Error\n295\nTable 1: Hyperparameter optimization stage of the LLMs\nLLM Learning Rate Training Epochs Warmup steps Weight decay\nMBERT 2e-05 1 500 0.034\nXLM 3.3e-05 2 500 0.28\nMDEBERTA 2.7e-05 1 250 0.081\nTable 2: Hyperparameter optimization stage of the feature sets\nShape Layers Neurons Dropout Batch size Activation\nLF brick 5 8 0.2 64 elu\nMBERT diamond 8 37 0.2 64 tanh\nXLM brick 2 37 0.3 64 relu\nMDEBERTA brick 2 128 0.3 32 relu\n(MSE), which is the average squared difference be-\ntween the observed real values and the predictions\nof the model; and the (7) Root Mean Squared Error\n(RMSE), which is the square root of MSE. The\nlower the RMSE and MSE, the better the model.\nTable 3 contains the results with the custom vali-\ndation split. The ensemble model of MBERT, XLM\nand MDEBERTA is the best model concerning all\nthe evaluated metrics, reaching 0.46 of EV , 0.042\nof RMSLE, 0.46 of R2, 0.516 of MAE, 0.426 of\nMSE and 0.652 of RMSE. As expected, the model\nbased on linguistic features (LF) is the most limited\nmodel, as not all the evaluated languages contain\nmodels in Stanza to get the set of language inde-\npendent variables. Accordingly, we decided not to\ninclude the LF in the final ensemble, as it would\ndecrease the overall performance of the ensemble.\nOut of the LLMs evaluated, a relevant different is\nfound between multilingual BERT and the other\ntwo multilingual LLMs. However, as the ensemble\nlearning method improves the results of the three\nLLMs, we consider that these LLMs complement\neach other.\n5.2 Official leader board\nThe test split of MTIA 2023 consists of 13697\nsentences. Similar to the train split, the testing split\nis also almost balanced, from Hindi, with 1260\ntweets to Korean, with 1410 tweets.\nA total of 45 teams participated in the MTIA\n2023 shared task. Table 4 contains the official\nleader-board. For the sake of simplicity, we only\ninclude here the top 5 teams, our result and the\nresult of the last position. The average Pearson’s R\nof all participants is 0.5105797444 with a standard\ndeviation of 0.145161199. As it can be observed,\nour proposal based on ensemble learning scored\n0.532, which is slightly superior of the average but\nfar from the top-five scores.\nOur result in the official leader board (0.532 of\nPearson’s R) is more limited than our result with\nour custom validation split (0.682 of Pearson’s R).\nIt is possible that this difference is due to the unseen\nlanguages that are incorporated for the test split.\nThe results of our proposal by language are de-\npicted in Table 5. These results are organized into\ntwo groups. The first group are the six languages\nused during the training and the second group are\nthe 4 unseen languages during the training. It can\nbe observed that our best result is achieved with\nan unseen language, Korean, reaching position 16.\nOur second best score is also with another unseen\nlanguage, Hindi. However, we got limited results\nfor Portuguese, Dutch, and Arabic.\n5.3 Error Analysis\nIn order to understand visually the classifications,\nwe rounded the predictions to create a confusion\nmatrix (see Figure 5). As we can observe, the\nmodel has good performance with the ratings that\nare in the rank between 1 and 3, being the ratings\nequal or higher than 4 the ones with major limita-\ntions. It seems that our ensemble model assigns\nhigher values than the actual ones. For example,\n347 instances with scores near to 1 were assigned\nto the bin of labels near to 2, and 141 labels with\nscores near to 2 were assigned to the bin of la-\nbels near to 3. However, out of the 144 tweets with\nscores near to 4, the ensemble model assigns scores\nnear of 3 to 101 tweets, and scores near to 2 to 33\n296\nTable 3: Results with the custom evaluation split. Reporting the Explained Variance (EV), Root Mean Squared\nLogarithmic Error (RMSLE), Pearson’s R, R-squared (R2), Mean Absolute Error (MAE), Mean Squared Error\n(MSE), and Root Mean Squared Error (RMSE)\nEV RMSLE PEARSON’S R R2 MAE MSE RMSE\nLF 0.197 0.062 0.444 0.197 0.640 0.633 0.796\nMBERT 0.374 0.048 0.612 0.374 0.555 0.494 0.703\nXLM 0.432 0.044 0.658 0.432 0.524 0.448 0.669\nMDEBERTA 0.449 0.043 0.670 0.449 0.516 0.434 0.659\nENSEMBLE 0.460 0.042 0.682 0.460 0.516 0.426 0.652\nTable 4: Top 5 results of the leaderboard compared with\nour result and the result of the last position\nTeam Score Ranking\nlazybob 0.616 1\nUZH_CLyp 0.614 2\nopi 0.613 3\ntmn 0.599 4\nOPD 0.599 5\nUMUTeam-SINAI 0.532 30\nuaic_mt_2023 0.004 45\nTable 5: Detailed results per language\nLanguage Score Ranking\nEnglish 0.642 33\nSpanish 0.705 26\nPortuguese 0.582 35\nItalian 0.659 31\nFrench 0.611 35\nChinese 0.704 23\nTotal 0.664 31\nHindi 0.220 19\nDutch 0.539 35\nKorean 0.362 16\nArabic 0.503 32\nTotal 0.360 32\nTotal 0.532 30\ntweets, leaving only 10 tweets correctly classified.\nThese results suggest that our model is not suitable\nfor tweets with larger intimacy scores.\n1\n2\n3\n4\n5\nPredicted\n1\n2\n3\n4\n5\nActual\n188 347 21 0 0\n53 596 141 0 0\n2 186 199 3 0\n0 33 101 10 0\n0 3 15 1 0\nFigure 5: Confusion matrix with the validation split\nwith the ensemble model\n5.4 Ablation Analysis\nTo understand the contribution of the data augmen-\ntation stage in our pipeline, a experiment with the\ncustom validation split but without the augmented\ndata is performed. The results are reported in Table\n6 in which the difference between both experiments\nare shown. As some of the metrics are the lower,\nthe betterwe have included the symbols ↑, ↓, and\n- to indicate when the effect of data augmentation\nimproves, downgrades or it does not have effect in\nthe performance. As it can be observed, the data\naugmentation step of our pipeline is beneficial for\nthe performance, but not for all the experiments.\nHowever, the contribution is not very high. For ex-\nample, there is only a difference of 0.0009 with the\nRMSE for the ensemble, between the experiments\nwith and without data augmentation.\n297\nTable 6: Ablation analysis of the data augmentation with the custom evaluation split. We report the difference\nbetween the results achieved with and without data augmentation. The ↑ symbol indicates that data augmentation\nimproves the performance whereas ↓ indicates the performance decreases. The \"-\" symbol denotes no effect at\nall. The metrics are the Explained Variance (EV), Root Mean Squared Logarithmic Error (RMSLE), Pearson’s R,\nR-squared (R2), Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE).\n.\nEV RMSLE PEARSON’S R R2 MAE MSE RMSE\nLF 0.003 ↑ 0 - 0.002 ↑ 0.004 ↑ 0.008 ↓ -0.003 ↑ -0.002 ↑\nMBERT -0.004 ↓ 0.001 ↓ -0.003 ↓ -0.004 ↓ 0.012 ↓ 0.003 ↓ 0.003 ↓\nXLM 0.001 ↑ 0 ↓ 0.001 ↑ 0.001 ↑ -0.002 ↑ 0 - -0.001 ↑\nMDEBERTA - 0.006 ↓ 0.001 - -0.005 ↓ -0.006 ↓ 0.004 ↓ 0.004 ↓ 0.003 ↓\nENSEMBLE 0.013 ↑ 0 - -0.004 ↓ 0.014 ↑ -0.005 ↑ -0.01 ↑ -0.009 ↑\n6 Conclusion\nDespite the fact that our results are limited, we are\nvery pleased with our participation. First, because\nthis is the first time we participated in a shared\ntask concerning intimacy. Second, because the\nMTIA shared task was challenging as we could\nonly send one result and because there are four\nunseen languages during testing. Our proposal\nbased on ensemble learning on three multilingual\nLLM reached position 30th in the official leader-\nboard from a total of 45 participants. Our best\nresults are achieved with two unseen languages:\nKorean (16th) and Hindi(19th).\nAfter the evaluation of our results, we consider\nthat there are several ways in which we could have\nimproved our results. First, we should have con-\nducted an in-deep analysis of the dataset. However,\nthis was not easy for us because we are not flu-\nent speakers of many of these languages, so we\ncan miss important aspects related to the context.\nSecond, it is possible that the data augmentation\nprocess was not beneficial for the performance of\nour model, as the translations could be less accurate\nin some languages or it is possible that cultural and\nbackground differences are not well represented in\nthe dataset. However, we consider that we could\nhave translated all sentences into a common lan-\nguage (Spanish or English, for instance) and could\ninclude features related to topics to our model. We\nwill explore this path in future multilingual shared\ntasks. Three, our models could be biased to our\ncustom validation split. In this sense, we will in-\ncorporate to our pipeline a nested-cross validation\nevaluation. Fourth, our ablation analysis is limited,\nas we only consider the data augmentation step.\nHowever, we need to conduct more experiments in\norder to gain understanding of other modules such\nas the preprocessing module.\nAcknowledgments\nThis work is part of the research projects\nAIInFunds (PDC2021-121112-I00) and LT-\nSWM (TED2021-131167B-I00) funded by\nMCIN/AEI/10.13039/501100011033 and by the\nEuropean Union NextGenerationEU/PRTR.\nThis work is also part of the research\nproject LaTe4PSP (PID2019-107652RB-\nI00/AEI/ 10.13039/501100011033) funded\nby MCIN/AEI/10.13039/501100011033. This\nwork has been partially supported by Project\nCONSENSO (PID2021-122263OB-C21), Project\nMODERATES (TED2021-130145B-I00) and\nProject SocialTox (PDC2022-133146-C21) funded\nby MCIN/AEI/10.13039/501100011033 and by\nthe European Union NextGenerationEU/PRTR,\nand Big Hug project (P20_00956, PAIDI 2020)\nand WeLee project (1380939, FEDER Andalucía\n2014-2020) funded by the Andalusian Regional\nGovernment. In addition, José Antonio García-\nDíaz is supported by Banco Santander and the\nUniversity of Murcia through the Doctorado Indus-\ntrial programme, and Salud María Jiménez-Zafra\nis partially supported by a grant from Fondo\nSocial Europeo and Administración de la Junta de\nAndalucía (DOC_01073).\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nJosé Antonio García-Díaz, Ricardo Colomo-Palacios,\nand Rafael Valencia-García. 2022a. Psychographic\n298\ntraits identification based on political ideology: An\nauthor analysis study on spanish politicians’ tweets\nposted in 2020. Future Generation Computer Sys-\ntems, 130:59–74.\nJosé Antonio García-Díaz, Salud María Jiménez-\nZafra, Miguel Angel García-Cumbreras, and Rafael\nValencia-García. 2022b. Evaluating feature combi-\nnation strategies for hate-speech detection in spanish\nusing linguistic features and transformers. Complex\n& Intelligent Systems, pages 1–22.\nJosé Antonio García-Díaz and Rafael Valencia-García.\n2022. Compilation and evaluation of the spanish sati-\ncorpus 2021 for satire identification using linguistic\nfeatures and transformers. Complex & Intelligent\nSystems, 8(2):1723–1736.\nJosé Antonio García-Díaz, Pedro José Vivancos-Vicente,\nÁngela Almela, and Rafael Valencia-García. 2022c.\nUMUTextStats: A linguistic feature extraction tool\nfor Spanish. In Proceedings of the Thirteenth Lan-\nguage Resources and Evaluation Conference, pages\n6035–6044, Marseille, France. European Language\nResources Association.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. Deberta: Decoding-enhanced\nbert with disentangled attention. In International\nConference on Learning Representations.\nArmand Joulin, Edouard Grave, Piotr Bojanowski,\nMatthijs Douze, Hérve Jégou, and Tomas Mikolov.\n2016. Fasttext.zip: Compressing text classification\nmodels. arXiv preprint arXiv:1612.03651.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. CoRR,\nabs/1901.07291.\nJiaxin Pei and David Jurgens. 2020. Quantifying inti-\nmacy in language. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 5307–5326. Association\nfor Computational Linguistics.\nJiaxin Pei, Vítor Silva, Maarten Bos, Yozon Liu,\nLeonardo Neves, David Jurgens, and Francesco Bar-\nbieri. 2023. Semeval 2023 task 9: Multilingual tweet\nintimacy analysis. In Proceedings of the 17th Inter-\nnational Workshop on Semantic Evaluation, Toronto,\nCanada. Association for Computational Linguistics.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton,\nChristopher D Manning, Yuhao Zhang, Yuhui Zhang,\nPeng Qi, Christopher D Manning, and Curtis P Lan-\nglotz. 2020. Stanza: A {Python} natural language\nprocessing toolkit for many human languages. In\nProceedings of the 58th Annual Meeting of the Associ-\nation for Computational Linguistics: System Demon-\nstrations.\n299"
}