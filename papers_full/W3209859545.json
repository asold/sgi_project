{
  "title": "Transformers in computational visual media: A survey",
  "url": "https://openalex.org/W3209859545",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2096640705",
      "name": "Yi-Fan Xu",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Institute of Automation"
      ]
    },
    {
      "id": "https://openalex.org/A2946831969",
      "name": "Huapeng Wei",
      "affiliations": [
        "Jilin University"
      ]
    },
    {
      "id": "https://openalex.org/A3006862348",
      "name": "Minxuan Lin",
      "affiliations": [
        "Institute of Automation",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2123066467",
      "name": "Yingying Deng",
      "affiliations": [
        "Institute of Automation",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2555578089",
      "name": "Kekai Sheng",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2319165320",
      "name": "Mengdan Zhang",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2098232510",
      "name": "Fan Tang",
      "affiliations": [
        "Jilin University"
      ]
    },
    {
      "id": "https://openalex.org/A2099000317",
      "name": "Weiming Dong",
      "affiliations": [
        "Institute of Automation"
      ]
    },
    {
      "id": "https://openalex.org/A2696333288",
      "name": "Feiyue Huang",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2154166576",
      "name": "Changsheng Xu",
      "affiliations": [
        "Institute of Automation"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2194775991",
    "https://openalex.org/W1583837637",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W3107634219",
    "https://openalex.org/W2964080601",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W6601250092",
    "https://openalex.org/W1847618513",
    "https://openalex.org/W6610712822",
    "https://openalex.org/W6629340653",
    "https://openalex.org/W6600838504",
    "https://openalex.org/W6729404389",
    "https://openalex.org/W6600662924",
    "https://openalex.org/W6635487051",
    "https://openalex.org/W6607999579",
    "https://openalex.org/W6600351811",
    "https://openalex.org/W6600175266",
    "https://openalex.org/W3176196997",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W6600042225",
    "https://openalex.org/W6605614091",
    "https://openalex.org/W3171516518",
    "https://openalex.org/W2602856279",
    "https://openalex.org/W6631455383",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W6601473709",
    "https://openalex.org/W2912083425",
    "https://openalex.org/W6600553734",
    "https://openalex.org/W6600047755",
    "https://openalex.org/W3111535274",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W6600013530",
    "https://openalex.org/W2326925005",
    "https://openalex.org/W2612063021",
    "https://openalex.org/W3034838697",
    "https://openalex.org/W2286300105",
    "https://openalex.org/W1885185971",
    "https://openalex.org/W2964101377",
    "https://openalex.org/W2963729050",
    "https://openalex.org/W6600018615",
    "https://openalex.org/W2962770929",
    "https://openalex.org/W1986760892",
    "https://openalex.org/W6828894009",
    "https://openalex.org/W6783944145",
    "https://openalex.org/W6600109629",
    "https://openalex.org/W3035422918",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W6600783858",
    "https://openalex.org/W6602646189",
    "https://openalex.org/W6603713569",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W3034502973",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W2336525064",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W6600213211",
    "https://openalex.org/W2980088508",
    "https://openalex.org/W6600238479",
    "https://openalex.org/W2962730651",
    "https://openalex.org/W2963488642",
    "https://openalex.org/W2559085405",
    "https://openalex.org/W2963781481",
    "https://openalex.org/W3034750257",
    "https://openalex.org/W2916798096",
    "https://openalex.org/W6603944243",
    "https://openalex.org/W2989676862",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W6600129504",
    "https://openalex.org/W2982770724",
    "https://openalex.org/W607748843",
    "https://openalex.org/W2612624696",
    "https://openalex.org/W3025966429",
    "https://openalex.org/W3176774696",
    "https://openalex.org/W3034993937",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W2463955103",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W6828408542",
    "https://openalex.org/W6603963165",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W2185175083",
    "https://openalex.org/W6600263792",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W3177224328",
    "https://openalex.org/W6600617704",
    "https://openalex.org/W2970869018",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W3175095612",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W2911293880",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2963518342",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W4288083516",
    "https://openalex.org/W6629997333",
    "https://openalex.org/W2963530300",
    "https://openalex.org/W6676167930",
    "https://openalex.org/W2963115613",
    "https://openalex.org/W2251512949",
    "https://openalex.org/W2897926040",
    "https://openalex.org/W2998655204",
    "https://openalex.org/W3108242852",
    "https://openalex.org/W2964245886",
    "https://openalex.org/W3159891475",
    "https://openalex.org/W6600595061",
    "https://openalex.org/W2964081807",
    "https://openalex.org/W2965658867",
    "https://openalex.org/W3035251378",
    "https://openalex.org/W3034528892",
    "https://openalex.org/W3101068231",
    "https://openalex.org/W3137963805",
    "https://openalex.org/W2946948417",
    "https://openalex.org/W2798979442",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W2545625743",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3102995547",
    "https://openalex.org/W3038476992",
    "https://openalex.org/W3029699545",
    "https://openalex.org/W3166368936",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3170227631",
    "https://openalex.org/W2995460200",
    "https://openalex.org/W3134189020",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W3134051502",
    "https://openalex.org/W3166513219",
    "https://openalex.org/W2950761309",
    "https://openalex.org/W3127839344",
    "https://openalex.org/W3107036272",
    "https://openalex.org/W2949989304",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3134873017",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W3111551570",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3000514857",
    "https://openalex.org/W3174738881",
    "https://openalex.org/W3125364632",
    "https://openalex.org/W3125056032",
    "https://openalex.org/W3170544306",
    "https://openalex.org/W3139445856",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W2963981733",
    "https://openalex.org/W3121480429",
    "https://openalex.org/W3122411985",
    "https://openalex.org/W2997076984",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W3146091044",
    "https://openalex.org/W3035652667",
    "https://openalex.org/W2952355681",
    "https://openalex.org/W2963799213",
    "https://openalex.org/W2109586012",
    "https://openalex.org/W3127751679"
  ],
  "abstract": "Abstract Transformers, the dominant architecture for natural language processing, have also recently attracted much attention from computational visual media researchers due to their capacity for long-range representation and high performance. Transformers are sequence-to-sequence models, which use a self-attention mechanism rather than the RNN sequential structure. Thus, such models can be trained in parallel and can represent global information. This study comprehensively surveys recent visual transformer works. We categorize them according to task scenario: backbone design, high-level vision, low-level vision and generation, and multimodal learning. Their key ideas are also analyzed. Differing from previous surveys, we mainly focus on visual transformer methods in low-level vision and generation. The latest works on backbone design are also reviewed in detail. For ease of understanding, we precisely describe the main contributions of the latest works in the form of tables. As well as giving quantitative comparisons, we also present image results for low-level vision and generation tasks. Computational costs and source code links for various important works are also given in this survey to assist further development.",
  "full_text": "Computational Visual Media\nhttps://doi.org/10.1007/s41095-021-0247-3 Vol. 8, No. 1, March 2022, 33–62\nReview Article\nTransformers in computational visual media: A survey\nYifan Xu1,2, Huapeng Wei3, Minxuan Lin1,2, Yingying Deng1,2, Kekai Sheng4, Mengdan Zhang4,\nFan Tang3, Weiming Dong1,2,5 (\u0000 ), Feiyue Huang4, and Changsheng Xu1,2,5\nc⃝ The Author(s) 2021.\nAbstract Transformers, the dominant architecture\nfor natural language processing, have also recently\nattracted much attention from computational visual\nmedia researchers due to their capacity for long-range\nrepresentation and high performance. Transformers\nare sequence-to-sequence models, which use a self-\nattention mechanism rather than the RNN sequential\nstructure. Thus, such models can be trained in parallel\nand can represent global information. This study\ncomprehensively surveys recent visual transformer\nworks. We categorize them according to task scenario:\nbackbone design, high-level vision, low-level vision and\ngeneration, and multimodal learning. Their key ideas\nare also analyzed. Diﬀering from previous surveys,\nwe mainly focus on visual transformer methods in\nlow-level vision and generation. The latest works\non backbone design are also reviewed in detail. For\nease of understanding, we precisely describe the main\ncontributions of the latest works in the form of tables.\nAs well as giving quantitative comparisons, we also\npresent image results for low-level vision and generation\ntasks. Computational costs and source code links for\nvarious important works are also given in this survey to\nassist further development.\n1 NLPR, Institute of Automation, Chinese Academy of Sciences,\nBeijing 100190, China. E-mail: Y. Xu, xuyifan2019@ia.ac.cn;\nM. Lin, linminxuan2018@ia.ac.cn; Y. Deng, dengyingying2017@\nia.ac.cn; W. Dong, weiming.dong@ia.ac.cn ( \u0000 ); C. Xu,\nchangsheng.xu@ia.ac.cn.\n2 School of Artiﬁcial Intelligence, University of Chinese\nAcademy of Sciences, Beijing 100040, China.\n3 School of Artificial Intelligence, Jilin University, Changchun\n130012, China. E-mail: H. Wei, weihp20@jlu.edu.cn;\nF. Tang, tangfan@jlu.edu.cn.\n4 Youtu Lab, Tencent Inc., Shanghai 200233, China. E-mail:\nK. Sheng, saulsheng@tencent.com; M. Zhang, davinazhang@\ntencent.com; F. Huang, garyhuang@tencent.com.\n5 CASIA-LLVISION Joint Lab, Beijing 100190, China.\nManuscript received: 2021-06-17; accepted: 2021-07-16\nKeywords visual transformer; computational visual\nmedia (CVM); high-level vision; low-level\nvision; image generation; multi-modal\nlearning\n1 Introduction\nConvolutional neural networks (CNNs) [ 1–3] have\nbecome the fundamental architecture in computa-\ntional visual media (CVM). Researchers began to\nincorporate a self-attention mechanism into CNNs to\nmodel long-range relationships, due to the problem\nof locality of convolutional kernels [ 4–8]. Recently,\nDosovitskiy et al. [9] found that using a self-attention-\nonly structure, without convolution, works well\nin computer vision. Since then, the transformer\narchitecture [\n10], a non-convolutional architecture\ndominating the research ﬁeld of natural language\nprocessing (NLP), has has been used in computer\nvision. Introducing transformers into computer vision\nprovides four advantages that CNNs lack:\n•\nTransformers learn with more inductive bias and\nperforms better when trained on large datasets\n(e.g., ImageNet-21K or JFT-300M) [9, 11].\n• Transformers provide a more general architecture\nsuitable for most ﬁelds, including NLP, CV, and\nmultimodal learning.\n• Transformers powerfully model long-range\ninteractions in a computationally-eﬃcient manner\n[12, 13].\n• The learned representation of relationships is\nmore general and robust than the local patterns\nfrom convolution modules [14].\nAs Table 1 shows, an increasing number of works on\nvisual transformers have come out in various subﬁelds\nof computational visual media. An instructive survey\nis important because of the diﬃculties in arranging\n33\n\n34 Y. Xu, H. Wei, M. Lin, et al.\nT able 1 Recent visual transformers introduced in this survey\nArea Secondary area Method Contributions\nBackbone\nnetwork\nClassiﬁcation\nT2T ViT [15] An eﬀective and eﬃcient tokens-to-token module\nTNT [16] The ﬁrst to exploit the beneﬁt of pixel-level relations\nCPVT [17] An instance-level position embedding module\nConViT [18] Adaptive reception ﬁeld in visual transformers\nDeepViT [19] A Re-Attention module for deep-layer ViTs\nSwin Transformer [20] A shifted-window based MSA & a deep-narrow module\nPiT [21] The ﬁrst to investigate the beneﬁt of pooling in ViTs\nLocalViT [22] A depth-wise convolution based module to exploit locality\nVisualization Transformer-Explainability [23] A better tool to visualize feature maps from ViT models\nHigh-level\nvision\nDetection\nDETR [24] First transformer-based detection SOTA model\nDeformable DETR [25] An eﬃcient attention module reducing time consumption\nUP-DETR [26] An unsupervised pre-training method for DETR\nPVT [27] A general transformer architecture for dense prediction\nSegmentation VisTR [28] First transformer-based segmentation model\nSegFormer [29] A lightweight eﬃcient segmentation transformer model\nLow-level\nvision\nColorization ColTran [30] First transformer-based image colorization model\nText-to-image\nTIME [31] Text-to-image generation\nDALL·E [32] Zero-shot text-to-image generation framework\nSuper resolution IPT [11] Image processing model\nTTSR [33] Flexible application of transformer\nImage generation\nTransGAN [34] First pure transformer-based GAN for generation\nGANsformer [35] A bipartite transformer\nVQGAN [36] A transformer-based high-resolution image generator\nImage restoration Uformer [37] A transformer-based hierarchical encoder–decoder network\nStyle transfer StyTr 2 [38] First transformer-based style transfer model\nPoint cloud learning PCT [39] Among the ﬁrst transformer-based point cloud models\nMulti-modality\nlearning\nTwo-stream model ViLBERT [40] The ﬁrst proposed two-stream model for V+L tasks\nSingle-stream model UNITER [41] A universal model for joint multi-modal embedding\nMixed model SemVLP [42] First mixed single- and two-stream model\nsuch fast and abundant developments. Due to the\nfast development of visual transformer backbones,\nthis survey speciﬁcally focuses on the latest works in\nthat area, as well as low-level vision tasks.\nSpeciﬁcally, this study is mainly arranged into four\nspeciﬁc ﬁelds: backbone design, high-level vision (e.g.,\nobject detection and semantic segmentation), low-\nlevel vision and generation, and multimodal learning.\nWe highlight backbone design and low-level vision\nas our main focus in Fig. 1. The developments to\nbe introduced are summarised in Table 1. For\nbackbone design, several latest works are introduced,\nconsidering two aspects: (i) injecting convolutional\nprior knowledge into ViT, and (ii) boosting the\nrichness of visual features. We also summarize the\nbreakthrough ideas of each work in Fig. 1. For high-\nlevel vision, we introduce the mainstream of DETR-\nbased transformer detection models [24]. For low-level\nvision and generation, we arrange papers according\nto diﬀerent subareas including colorization [ 30, 43–\n45], text-to-image [31, 32, 46], super-resolution [47–\n49], and image generation [ 50–54]. For multimodal\nlearning, we review some recent representative\nworks on vision-plus-language (V+L) models and\nsummarize pretraining objectives in this ﬁeld.\nWe comprehensively compare results in diﬀerent\nﬁelds and give training details, including computa-\ntional cost and source code links to facilitate and\nencourage further research. Some images resulting\nfrom low-level vision models are also illustrated. The\nrest of the paper is organized as follows. Section 2\nintroduces visual transformers. Section 3 lists latest\n\nTransformers in computational visual media: A survey 35\nFig. 1 Organisation of recent works on visual transformers.\ndevelopments in backbone networks for visual\ntransformers in image classiﬁcation. Section 4\ndescribes several recent advanced designs using visual\ntransformers in object detection. Section 5 introduces\ntransformer-based methods for various low-level\nvision tasks. Section 6 reviews recent representative\nworks on multimodal learning. Finally, we draw\nconclusions from diﬀerent research ﬁelds in Section 7.\n2 Visual transformers\nBefore introducing the latest developments, we give\nthe basic formulation of visual transformers by using\nViT [9] as an example. As shown in Fig. 2, a typical\nViT mainly contains ﬁve basic procedures: splitting\ninput images into smaller local patches, preparing\nthe input token (patch tokens, class token, and\nposition embedding), a series of stacked transformer\nblocks [55] (i.e., layer normalization (LN) [56] + multi-\nhead self-attention (MSA) [ 57] + skip-connection\nlayer [ 1] + multilayer perception (MLP) or feed-\nforward network (FFN)), and post-process module.\nFormally, given an input imageX ∈ RH×W ×C and\nits labels Y , X is ﬁrst reshaped into a sequence of\nﬂattened 2D image patches Xp ∈ RN ×(P 2·C).T h e n ,\nfollowing BERT [ 10], a class token and several\nposition tokens are used to record extra meaningful\ninformation for inference. Together, the input is\nformulated as follows:\nz0 =[ xcls; x1\np · E; ··· ; xN\np\n· E]\n+[ Ecls\npos; E1\npos; ··· ; EN\npos]\nwhere xcls ∈ RD is the class token, E ∈ R(P 2·C)×D is\na linear projection of each patch Xp,a n dEi\npos ∈ RD\nis the learnable position embedding for the i-th token.\nThen, the input is sent into several sequential\nFig. 2 Framework of ViT (left) and typical pipeline of a transformer encoder (right). Reproduced with permission from Ref. [ 9], c⃝ The\nAuthor(s) 2021.\n\n36 Y. Xu, H. Wei, M. Lin, et al.\ntransformer blocks:\nz\n′\nl+1 = zl + MSA(LN(zl))\nzl+1 = z\n′\nl+1 + MLP(LN(z\n′\nl+1))\nwhere l ∈{ 0, ··· ,L − 1} denotes the layer, L is\nthe number of transformer blocks, the MLP includes\ntwo fully-connected layers using GELU [ 58]a st h e\nactivation function, LN(·) is a layer-normalization\nmodule [56], and the MSA module is formulated as\nMSA(z)=[ S A1(z); ··· ;S AH (z)] × Umsa\nSAi(Q, K, V )= σ\n(\nQ · KT\n√dk\n)\n· V\nwhere z is the input, [ Q, K, V ]= z × U i\nqkv , U i\nqkv ∈\nRD×(3·Dh ) projects the D-dimensional input z to Dh-\ndimensional Q, K,a n dV in the head i, σ(·)i st h e\nsoftmax function, and Umsa ∈ R(H·Dh )×D re-casts the\noutput from H heads of the MSA module into one\nD-dimensional output. Several variants of MSA, like\nReformer [59], Performer [ 60], and LinFormer [ 61],\nare available.\n3 Backbone design\nIn this section, we describe several recent designs\nfor the backbone of ViT models. Without loss\nof generality, we focus on the image classiﬁcation\ntask. We divide recent progress into two mainstream\napproaches: (i) injecting convolutional prior know-\nledge into ViT, works including T2T-ViT [\n15],\nConViT [18], PiT [ 21], and Swin Transformer [ 20],\nand (ii) boosting the richness of visual features,\nincluding TNT [ 16], CPVT [ 17], DeepViT [ 19],\nand LocalViT [ 22]. We also briefly describe recent\ndevelopments in visualizing feature maps of ViT\nmodels [23, 62, 63], which help to better understand the\nworking mechanism of ViT models. We list core details\nof their performance on ImageNet [64] in Table 2.\n3.1 Latest developments\n3.1.1 T2T-ViT\nYuan et al. [15] note that the method to convert input\nimages into tokens in a typical ViT [ 9] ineﬀectively\nmodels the spatial structure of image data and\nmay lead to poor training eﬃciency and suboptimal\nperformance. They propose two eﬀective approaches\nto address the aforementioned problem. First, they\npropose a token-to-token (T2T) module to inject\nspatial information into the tokenization of image\npatches and reduce the length of tokens progressively\nfor the sake of computational and parameter eﬃciency.\nInspired by CNN architectures [ 1–3], they also\ndevise a deep-narrow ViT framework to reduce the\nnumber of parameters and enhance training eﬃciency.\nOverall, they train ViT models from scratch on\nImageNet without additional datasets.\n3.1.2 TNT\nHan et al. [ 16] propose a novel Transformer-iN-\nTransformer (TNT) framework to further exploit\nthe intrinsic spatial structural information in image\ndata. As Fig. 3 shows, TNT considers patch and\npixel level relations in learning useful visual features.\nThey propose a TNT block to utilize the pixel-\nlevel representations eﬀectively and eﬃciently. They\nintroduce an additional transformer called an Inner\nT-Block to model pixel-level relationships in each\npatch and then reinforce the patch-level features\nwith the calculated pixel-level ones. Consequently,\nTNT achieves 81 .3% top-1 classiﬁcation accuracy\non ImageNet [ 64] at the cost of only moderate\nadditional computation. The experimental results\nverify the positive eﬀects of pixel-level relation\nmodeling.\n3.1.3 ConViT\nD’Ascoli et al. [ 18] propose a novel ViT model\nwith soft convolutional inductive biases (ConViT) to\nendow transformers with an adaptive receptive ﬁeld.\nFigure 4 schematically shows the core block, called\na gated positional self-attention (GPSA) module. A\nGPSA block has two branches: Wqry or Wkey is used\nto model the global or long-range relationship, and\nvpos is utilized to model the relationship within local\nregions. To adaptively trade-oﬀ between the two\nbranches, they adopt a learnable parameter λ,w h i c h\nFig. 3 Framework of TNT. Reproduced with permission from\nRef. [16], c⃝ The Author(s) 2021.\n\nTransformers in computational visual media: A survey 37\nT able 2 Classiﬁcation accuracy on ImageNet [64] for various visual transformers\nMethod Image size FLOGs (G) #Param (M) Acc (%) Source (GitHub)\nConvolution-based neural network\nResNet [1] 2242 4.1 25.6 76.2 —\nRegNetY-4G [3] 2242 4.0 21 80.0 facebookresearch/pycls\nRegNetY-16G [3] 2242 16.0 84 82.9\nEﬃcientNet-B0 [2] 2242 0.4 5.3 77.1\nrwightman/gen-efficientnet-pytorch\nEﬃcientNet-B1 [2] 2242 0.7 7.8 79.1\nEﬃcientNet-B3 [2] 3002 1.8 12 81.6\nEﬃcientNet-B5 [2] 4562 9.9 30 83.6\nEﬃcientNet-B7 [2] 6002 37.0 66 84.3\nVisual transformer\nViT [9] 3842 55.4 86 77.9 google-research/vision_transformer\n3842 190.7 307 76.5\nDeiT [65] 2242 4.6 22 79.8 facebookresearch/deit\n3842 55.4 86 83.1\nT2T ViT [15] 2242 5.2 21.5 80.7 yitu-opensource/T2T-ViT\nTNT [16] 2242 5.2 23.8 81.3 huawei-noah/noah-research/tree/master/TNT\n2242 14.1 65.6 82.8\nCPVT [17] 2242 — 23 81.5 Meituan-AutoML/CPVT\n2242 — 88 82.3\nConViT [18] 2242 5.4 27 81.3 —\n2242 17 86 82.4\nDeepViT [19] 2242 — 27 82.3 zhoudaquan/dvit_repo\n2242 — 55 83.1\nSwin Transformer [20] 2242 4.5 29 81.3 microsoft/Swin-Transformer\n3842 47.0 88 84.2\nPiT [21] 2242 4.6 22.1 81.9 naver-ai/pit\n2242 12.5 73.8 84.0\nLocalViT [22] 2242 4.6 22.4 80.8 ofsoundof/LocalViT\nFig. 4 Framework of ConViT and the gated positional self-attention\nmechanism. Reproduced with permission from Ref. [ 18], c⃝ The\nAuthor(s) 2021.\nis initialized as 1 for all layers and all heads in MSA.\nWith the proposed GPSA module, they manage to\nadaptively expand the self-attention receptive ﬁeld\nduring training.\n3.1.4 CPVT\nChu et al. [ 17] resort to a novel design of position\nembedding module to further reinforce the richness\nof learned visual features from ViT. Instead of a\npredeﬁned position embedding that is independent\nof the input data, they propose a conditional\nposition embedding scheme to generate diﬀerent\npositional encodings for various input tokens, akin\nto dynamic neural network design [\n66]. In their\nimplementation, they also rearrange the input\ntokens in a spatial manner and apply convolution\noperations to extract the position embedding in a\nlearnable way. In this way, they also maintain the\nlocal neighborhood information during tokenization,\nbeneﬁting classiﬁcation performance.\n\n38 Y. Xu, H. Wei, M. Lin, et al.\nTwo further ViT models, LeViT [12] x and CoaT [67] y,\ninvestigate the importance of position embedding\nand propose diﬀerent implementations. We do not\ndescribe them further due to lack of space.\n3.1.5 Swin Transformer\nOn the basis of the observations that image data\ncontain much redundant spatial information and\ngiven the success of deep-narrow CNN architectures,\nLiu et al. [ 20] propose a novel hierarchical visual\ntransformer design. Figure 5(a) illustrates the\ncore idea of the window MSA (W-MSA) and the\nshifted W-MSA (SW-MSA) within Swin Transformer,\nwhich separate local patches into several windows\nand run the MSA module window by window.\nWith the W-MSA mechanism, they reduce the\ncomputation complexity fromO(4HWC 2+2(HW )2C)\nto O(4HWC 2 +2 M2HWC ), where H and W\nrepresent the size of input patches, M × M is\nthe number of windows, and C is the feature\ndimension. A shifted window design is also proposed\nto encourage cross-window communication for rich\nvisual features. They also propose a deep-narrow\narchitecture (see Fig. 5(b)). Extensive experiments on\nImageNet, COCO, and ADE-20K demonstrate that\nSwin Transformer enhances eﬃcient use of parameters\nand achieves state-of-the-art object detection and\nsemantic segmentation.\nDong et al. [68] also propose another vision transfor-\nmer model, CSWin Transformer, which utilizes a\ncross-shaped window self-attention mechanism (akin\nto criss-cross attention [ 69] or strip pooling [ 70])\nand a locally enhanced position encoding. CSWin\nTransformer obtains even better performance than\nSWin Transformer.\n3.1.6 DeepViT\nLayer scaling (e.g., 152-layer ResNet [ 1]) is an\nimportant aspect of CNN architectures. With regard\nto ViT models, Zhou et al. [ 19] empirically ﬁnd that\nthe performance of deep layer ViT models saturates\nwhen we stack more than 20 transformer blocks even\nwith the help of skip-connection layers. They unveil\nthat the reason is attention collapse: the feature\nmaps extracted from each head in one MSA module\nshare increasingly similar patterns, leading to huge\ninformation redundancy and low training eﬃciency.\nIf the communication between the MSA heads is\npromoted, the information redundancy between each\nhead and rich learned visual feature can be reduced.\nFig. 5 (a) Window MSA (W-MSA) greatly reduces computational cost and facilitates communication between each isolated W-MSA.\n(b) Overview of Swin Transformer. Reproduced with permission from Ref. [20], c⃝ The Author(s) 2021.\nx https://github.com/facebookresearch/LeViT\ny https://github.com/mlpc-ucsd/CoaT\n\nTransformers in computational visual media: A survey 39\nOn the basis of the aforementioned motivation, they\npropose a simple and eﬀective Re-Attention module:\nNorm\n(\nΘTsoftmax\n(\nQ · KT\n√\nd\n))\n· V\nwhere Θ ∈ RH×H is a learnable parameter\nto facilitate the communication between the H\nheads within one MSA module. Experiments on\nImageNet [64] verify that a 32-layer ViT model can\nbe trained without performance saturation with the\nhelp of a Re-Attention module.\nNotably, concurrent work, which is termed\nCaiT [71], also investigates the topic of layer scaling\nand proposes a diﬀerent perspective. Further details\ncan be obtained from their paper.\n3.1.7 PiT\nConsidering the importance of the pooling layer to\nmodel capability and generalization performance of\nCNN architectures, Heo et al. [ 21] investigate the\npossibility of taking advantage of pooling modules\nin ViT. The pooling layer in a conventional CCN\narchitecture conducts spatial information aggregation\nfor spatially invariant features. On the basis of\nthis observation, they propose to implement spatial\ninformation condensation via depth-wise convolution.\nAs shown in Fig. 6, they ﬁrst split the obtained input\ntokens into class tokens and spatial ones, and then\nthey recover the spatial shape of the latter. Next,\nthey leverage a depth-wise convolution operation\non the spatial branch for the purpose of a pooling\nlayer. Meanwhile, they apply a fully connected layer\nto project the class token into the same dimension.\nWith a simple and eﬀective pooling module, they\npropose a pooling-based ViT (PiT) and achieve an\noptimal trade-oﬀ between computation eﬃciency and\nclassiﬁcation performance.\nFig. 6 Pooling layer in the PiT architecture. Reproduced with\npermission from Ref. [21], c⃝ The Author(s) 2021.\n3.1.8 LocalViT\nLi et al. [ 22] study the diﬀerences between ViT\nmodels and CNN architectures. They ﬁnd that visual\ntransformers are good at modeling global relations\nwhile lacking a local scheme to learn interactions\nwithin a local region, which is the characteristic of\nconvolution. A local mechanism is important and\nuseful for modeling spatial structures for image data.\nThus, they believe that visual transformers must\nreinforce the model’s capability for local relation\nmodeling to promote the learned visual features\nfrom ViT models. Speciﬁcally, they investigate\nseveral possible blocks and then propose local ViT\n(LocalViT), as shown in Fig. 7(right). Experiments\non ImageNet [64] indicate that the LocalViT module\nis a practical local mechanism which boosts the\nperformance of various ViT models [ 15, 16, 27, 65].\n3.2 Comparison on ImageNet\nWe compare the classiﬁcation accuracies of the latest\nViT models on the ImageNet benchmark [ 64]i n\nTable 2, together with their implementation details,\nnamely, #FLOGs, #Param, and source code, to\nfacilitate further research. The experimental values\nindicate that ViT models have potential to achieve\ncomparable performance or even outperform state-\nof-the-art CNN architectures like RegNet [\n3]a n d\nEﬃcientNet [2], which are based on expert-designed\nbasic modules and the power of neural architecture\nsearch (NAS) techniques. We also observe very\nrecent exciting progress, in that the latest proposed\nViT models possess higher model capability and\nbetter parameter eﬃciency for vision than the original\nversion of ViT.\nFig. 7 Comparison of the convolutional version of the FFN module in\nViT models (left), inverted residual blocks (center), and the proposed\nmodule in LocalViT to exploit the beneﬁt of locality in ViT (right).\nReproduced with permission from Ref. [22], c⃝ The Author(s) 2021.\n\n40 Y. Xu, H. Wei, M. Lin, et al.\n3.3 Visualization of ViT\nVisualizing the feature maps in ViT is also an\ninteresting and worthy research topic. As ViT\nmodels leverage diﬀerent basic components from\nCNN models, we should adopt diﬀerent visualization\nmethods correspondingly. As shown in Fig. 8, the\nlatest tools specialized for MSA modules and ViT\nmodels, namely, partial LRP [\n63] and Transformer-\nExplainability [23]x, can generate better results for\nfeature map visualization than the visualization\nmethods for CNN. The visualizations indicate that\nViT models can learn additional meaningful spatial\ninformation with image-level annotations alone.\nTherefore, ViT models have potential values in weakly\nsupervision scenarios, such as weakly supervised\nobject detection.\n4 High-level vision\nIn this section, we focus on representative recent high-\nlevel vision tasks based on transformer framework.\nHigh-level vision refers to stages of visual processing\nthat transition from analyzing local image structure\nto exploring the structure of the external world that\nproduced those images. The main tasks include\nobject detection [24–26], segmentation [28, 29, 74–79],\nand key-point detection [ 80–85]. As the focus of\nthis survey is low-level vision tasks, we only\nbrieﬂy introduce some interesting works in object\ndetection. Modern detection methods address the\nset prediction task by deﬁning a large set of propos-\nals [86, 87], anchors [88], or window centers [ 89, 90].\nUnlike previous attempts [91–96], transformer-based\ndetection raises the possibility of total anchor-free\nand end-to-end models. We begin with the stream\nof DETR [\n24], followed by Deformable DETR [ 25]\nand UP-DETR [ 26]. A more complete approach,\nPVT [27], which is the earliest transformer backbone\nfor dense prediction tasks like detection, is also\nintroduced. Additional recent high-level backbones like\nSwin Transformer [20]a n dT w i n s[97] are introduced\nin Section 3. A comparison is provided in Table 3.\n4.1 DETR\nCarion et al. [24] were the ﬁrst to provide a completely\nend-to-end detection model based on the transformer\nencoder–decoder architecture. It gives researchers a\nnew insight that the transformer architecture can\nachieve state-of-the-art performance in detection.\nUnlike previous detection models, DETR does not\nrely on artiﬁcially designed anchors. The overall\nstructure is illustrated in Fig. 9. The transformer\nencoders are arranged after a convolution feature\nFig. 8 Class-speciﬁc visualization results from ViT. Left to right: input image, rollout [ 62], raw-attention, GradCAM [ 72], LRP [ 73], partial\nLRP [63], and Transformer-Explainability [23]. Reproduced with permission from Ref. [23], c⃝ The Author(s) 2021.\nx https://github.com/hila-chefer/Transformer-Explainability\n\nTransformers in computational visual media: A survey 41\nT able 3 Comparison of transformer-based detection models on the COCO 2017 val set\nMethod\nT raining\nAP AP 50 AP75 APS APM APL\n#Param FPS FLOPS Source\nepochs (M) (G) (GitHub)\nConvolution-based models\nFCOS [90] 36 41.0 59.8 44.1 26.2 44.6 52.2 — 23 177 tianzhi0549/FCO\nFaster R-CNN+FPN [86] 109 42.0 62.1 45.5 26.6 45.4 53.4 42 26 180 rbgirshick/py-faster-rcnn\nTransformer-based models\nDETR [24] 500 42.0 62.4 44.2 20.5 45.8 61.1 41 28 86 facebookresearch/detr\nDETR-DC5 [24] 500 43.3 63.1 45.9 22.5 47.3 61.1 41 12 187\nDeformable DETR [25] 50 43.8 62.6 47.7 26.4 47.1 58.0 40 19 173 fundamentalvision/Deformable-DETR\nUP-DETR [26] 150 40.5 60.8 42.6 19.0 44.1 60.0 41 — — dddzg/up-detr\nUP-DETR [26] 300 42.8 63.0 45.3 20.8 47.1 61.7 41 — —\nPVT-T [27] 300 36.7 56.9 38.9 22.6 38.8 50.0 23.0 — — whai362/PVT\nPVT-M [27] 300 41.9 63.1 44.3 25.0 44.9 57.6 53.9 — —\nViT-B/16-FRCNN [98] 21 37.8 57.4 40.1 17.8 41.4 57.3 21 — — —\nFig. 9 Overall structure of DETR. Reproduced with permission\nfrom Ref. [24], c⃝ Springer Nature Switzerland AG 2020.\nextractor. We introduce the encoder and decoder\nmodules in order.\nIn the DETR encoder, ﬁrst, the output feature map\nof CNN is decomposed into patches as for ViT [ 9]\nintroduced in Section 3. Then, the patches are\nmapped to one-dimensional vectors to go through\nseveral traditional transformer encoders. DETR\nand traditional BERT encoders diﬀer only in the\npositional embedding. The positional embedding is\ninjected into all encoder blocks rather than only the\ninput layer to preserve the positional information;\nthey claim that high-level vision detection needs\nmore positional information than classiﬁcation. Only\nqueries and keys are also injected into the positional\nembedding.\nThe decoder has two inputs. The ﬁrst is the object\nquery, which only serves as queries in the second\nMSA layers. The second is the output of the encoder\nmodule, which serves as values and keys for the second\nMSA layers. To easily understand the mechanism,\nreaders can treat the object queries as information of\ndiﬀerent target objects and suppose the decoder aims\nto ﬁnd whether the similar patterns to the object\nqueries exist in the image features. The output of\nthe decoder is then passed through two branches,\nnamely, the box and class branches. The box branch\npredicts the positions of the target objects, while the\nclass branch serves to predict the category of each\npredicted box.\n4.2 Deformable DETR\nAlthough considerable progress has been achieved\nby DETR in transformer-based detection, its main\ndeﬁciency is its huge computational cost. Training\na DETR on one V100 GPU is reported to take 48\ndays, which is unaﬀordable for common institutions.\nThus, Zhu et al. [ 25] propose the deformable self-\nattention module to reduce the training time to 340\nGPU hours at the same time as improving the original\nperformance. The core idea of deformable attention\nis to ﬁnd the nearest K values of an input query to\n\n42 Y. Xu, H. Wei, M. Lin, et al.\ncalculate attention. Nearest here refers to semantic\ndistance rather than spatial distance. Deformable\nattention is illustrated in Fig. 10, which is drawn\nfrom deformable convolution [99]. A linear model is\nestablished to learn the oﬀsets of the nearestK values,\nand then another linear model is established to learn\nthe attention score of each value. In summary, the\nmain contributions of deformable attention module\nare (i) only K corresponding values rather than all\nvalues are required to calculate the attention of one\nquery and (ii) the attention scores are learned by\na network rather than by simple multiplication of\nqueries and keys.\n4.3 UP-DETR\nDai et al. [ 26] propose a novel unsupervised pre-\ntraining method called random query patch detection\nfor DETR [24, 25], which leads to better performance.\nFigure 11(a) illustrates their pretraining method. A\nrandom query patch is randomly cropped from an\ninput image. Then, the query patch is added to the\nobject queries of the DETR decoder. The ﬁnal goal\nis to predict two things: (i) Lcls, that is, existence\nof objects in the query patch, and (ii) Lbox,t h a t\nis, the location of the query patch in the image. A\nreconstruction loss\nLrec is also designed to ensure\nthat the CNN has extracted full information from\nthe query patch. This pretraining method leads to\nmore ﬂexible training. As shown in Fig. 11(b), a more\nrobust representation can be learned after augmenting\nthe random query patches.\n4.4 PVT\nDiverging from DETR, Wang et al. [ 27] also\npropose a pure transformer-based backbone, called\na pyramid vision transformer (PVT), for detection\nand segmentation. Its framework is shown in Fig. 12.\nAfter each stage, the output is rearranged to recover\nspatial structure and is then down-sampled to half\nresolution. Notably, the spatial reduction is only\nconducted on the key\nK and value V while the\nspatial size of the query Q is maintained. In practice,\nthe full architecture of PVT-based detection models\nincludes a PVT backbone and a general detection\nhead, such as RetinaNet [ 88] and Mask R-CNN [ 100].\nRecently, several dense prediction backbones have\ncome out after PVT [ 27], like Swin Transformer [20],\nCPVT [17], and Twins [97], which we introduced in\nSection 3.\n5 Low-level vision and generation\nIn this section, we focus on some representative\nrecent transformer-based works on low-level vision\nFig. 10 Deformable attention module. Reproduced with permission from Ref. [25], c⃝ The Author(s) 2020.\n\nTransformers in computational visual media: A survey 43\nFig. 11 (a) Random single query patch detection of UP-DETR. (b) More robust representation is derived by augmenting the query patch.\nReproduced with permission from Ref. [26], c⃝ The Author(s) 2020.\nFig. 12 Framework of PVT. Reproduced with permission from Ref. [27], c⃝ The Author(s) 2021.\ntasks, as listed in Table 4. Low-level vision tasks\ninclude super-resolution [ 101], denoising [101], image\nT able 4 Source code links for ViT-based models for low-level vision\ntasks\nMethod Source (GitHub)\nTIME [31] —\nIPT [101] —\nColTran [30] google-research/google-research\n/tree/master/coltran\nTTSR [73] researchmm/TTSR\nGANsformer [35] —\nTransGAN [34] VITA-Group/TransGAN\nDALL·E [32] openai/DALL-E\nVQGAN [102] CompVis/taming-transformers\nStyTr\n2 [38] —\nPCT [39] Strawberry-Eat-Mango/PCT_Pytorch\ncolorisation [30], text-to-image generation [31], and\nimage generation [ 34, 35]. We separately introduce\nhow these tasks use transformers to achieve good\nresults (see examples in Fig. 13).\n5.1 TIME\nAs a pre-trained NLP model is always required for the\ntext-to-image (T2I) task, it may introduce inﬂexibility\nfor the whole model. Liu et al. [ 31] propose an\neﬃcient model for T2I tasks: Text and Image\nMutual Translation Adversarial Networks (TIME).\nTIME can jointly handle T2I and image captioning\nusing a single network without a pretrained NLP\nmodel. As Fig. 14 shows, TIME introduces a multi-\nhead and multi-layer transformer to the generator\nand text decoder, which can be used to eﬀectively\n\n44 Y. Xu, H. Wei, M. Lin, et al.\nFig. 13 Representative results for low-level tasks, such as text-to-image generation, basic image processing tasks, colorization, image super\nresolution, and image generation. Images are taken from the corresponding papers.\ncombine image features and the sequence of word\nembeddings into the output. The Text-Conditioned\nImage Transformer takes image feature fi and the\nsequence of word embeddings ft, and outputs the\nrevised image fit according to the word embeddings.\nThe Image-Captioning Transformer is similar to the\n\nTransformers in computational visual media: A survey 45\nFig. 14 TIME model overview. Reproduced with permission from Ref. [ 31], c⃝ Association for the Advancement of Artiﬁcial Intelligence 2021.\nText-Conditioned Image Transformer but for image\ncaptioning [103–105]. T2I and the image captioning\ntask are jointly trained in the generative adversarial\nnetwork (GAN) manner. TIME achieves state-of-the-\nart T2I performance without pretraining.\nDALL·E. Text-to-image generation is a classical\ngeneration problem, which needs to construct a\nmapping between two streams. Ramesh et al. [ 32]\npropose a transformer-based framework to better\nalign text and image semantic information. A two-\nstage model is applied to model the text and image\ntokens. They ﬁrst train a discrete variational\nautoencoder [\n106] to build 1024 image tokens and\nadopt 256 BPE-encoded text tokens to represent\nthe text information. Thereafter, an auto-regressive\ntransformer is used to capture the joint distribution\nof the text and image tokens. They also use a mixed-\nprecision training strategy and PowerSGD [\n57]t os a v e\nGPU memory. The model consumes approximately\n24 GB memory in 16-bit precision.\n5.2 IPT\nClassiﬁcation models can be pretrained on large-\nscale datasets to enlarge model representation ability.\nRelated low-level vision tasks such as image super-\nresolution, inpainting, and deraining are combined\nin a model to help one another. The generalized\npretraining procedure solves the problem of task-\nspeciﬁc data limitation. Therefore, Chen et al. [ 101]\ndevelop a pretrained model for image processing using\nthe transformer architecture, the Image Processing\nTransformer (IPT). The model architecture is shown\nin Fig. 15. To adapt to diﬀerent vision tasks,\nChen et al. [ 101] design a multi-head and multi-\ntail architecture, which involves three convolutional\nlayers. The transformer body consists of an encoder\nand a decoder described in Ref. [ 57]. Like the\ndiscriminator in Ref. [34], they split the given features\ninto patches and each patch is regarded as a “word”\nbefore features are input into the transformer body.\nUnlike the original transformer, they utilize a task-\nspeciﬁc embedding as an additional input to the\ndecoder. The model is pretrained on ImageNet, which\nis a key factor for success.\n5.3 Uformer\nWang et al. [ 37] propose an eﬀective and eﬃcient\ntransformer-based architecture for image restoration.\nIt uses a transformer module to construct a\nhierarchical encoder–decoder network. Two core\ndesigns of Uformer make it suitable for image\nrestoration. The ﬁrst is a local-enhanced window\ntransformer block. Speciﬁcally, a nonoverlapping\nwindow-based self-attention is used to reduce the\n\n46 Y. Xu, H. Wei, M. Lin, et al.\nFig. 15 Overview of IPT. Reproduced with permission from Ref. [101], c⃝ The Author(s) 2021.\ncomputational cost, and depth-wise convolution\nis used in the FFN to further improve its\nability to capture local context. The second is the\nskip-connection mechanism, which is explored to\neﬀectively deliver the encoder information to the\ndecoder. Uformer can capture useful dependencies\nfor image restoration because of the two designs\nabove. The network structure of Uformer is shown\nin Fig. 16. Its performance has been veriﬁed through\nseveral image restoration tasks, including denoising,\nderaining, and deblurring.\n5.4 TransGAN\nDriven by curiosity, Jiang et al. [ 34] ﬁrst design\na GAN using pure transformer-based structures\nto determine whether transformers perform well\nwhen applied to generative adversarial networks\n(GANs) [\n107]. This network consists of a memory-\nfriendly transformer-based generator and a patch-\nlevel discriminator. Jiang et al. [\n34] also imitate\nthe philosophy in CNN-based GANs and design a\nnovel structure for image generation to avoid the\nhigh cost when applying transformers from NLP\nFig. 16 (a) Overview of Uformer. (b) Structure of the LeWin transformer block. Reproduced with permission from Ref. [37], c⃝ The Author(s) 2021.\n\nTransformers in computational visual media: A survey 47\nFig. 17 Model overview of TransGAN. Reproduced with permission from Ref. [34], c⃝ The Author(s) 2021.\nto visual tasks. As shown in Fig. 17(left), the\nmemory-friendly transformer-based generator has\nmultiple stages, thus increasing the feature resolution\nwhile decreasing the embedding dimension. The\ndiscriminator splits the generated images into small\npatches and regards them as “words” . The tokens\nare taken by the classiﬁcation head to output the\nreal/fake prediction. The whole net is trained with\nthree ingenious strategies: data augmentation, self-\nsupervised auxiliary task (super task) cooperative\ntraining, and locality-aware initialization. The results\nin CIFAR-10 and STL-10 are comparable to those of\nsome state-of-the-art works using CNN-based GANs.\n5.5 TTSR\nTexture is often damaged during downsampling and\nalso cannot be easily recovered. Traditional single\nimage super-resolution always leads to blurring eﬀects\nin the output. Therefore, Yang et al. [ 33] propose\na reference-based image super resolution method,\nnamely, the Texture Transformer Network for Image\nSuper Resolution (TTSR). As shown in Fig. 18, the\nLearnable Texture Extractor is ﬁrst used to extract\nproper texture information, which is crucial for super\nresolution. Then, the input to the texture transformer\ncan be expressed as follows:\nQ = LTE(LR ↑)\nK = LTE(Ref ↓↑)\nV = LTE(Ref)\nwhere LR↑, Ref, and Ref ↓↑ denote the image to be\nreconstructed, the reference image, and the reference\nimage that is down-sampled and then up-sampled\nrespectively. The texture transformer contains a\nFig. 18 Model overview of TTSR. Reproduced with permission from\nRef. [33], c⃝ IEEE 2020.\nHard-Attention and a Soft-Attention, and it is\napplied to the high-resolution feature guided by the\nreference image. Finally, they propose a cross-scale\nfeature integration module to exchange information\nbetween the features at diﬀerent scales for better\nrepresentation at diﬀerent scales.\n5.6 ColTran\nImage colorization is a challenging task that needs\nto determine the image semantics. Most colorization\nmodels estimate log-likelihood based on neural\ngenerative approaches. Kumar et al. [ 30] propose\nthe Colorization Transformer (ColTran) using a\n\n48 Y. Xu, H. Wei, M. Lin, et al.\nself-attention mechanism to promote the eﬀects\nof a probabilistic colorization model. ColTran\nreplaces self-attention blocks with axial self-attention,\nwhich decreases the computational complexity from\nO(D2)t o O(D\n√\nD). Kumar et al. [ 30] adopt a\nconditional variant of the Axial Transformer [ 108]\nfor low-resolution coarse colorization. As shown in\nFig. 19, the ColTran core consists of Conditional\nSelf-Attention, MLP, and Layer Norm modules, and\nit applies conditioning to the auto-regressive core.\nThey also design a Color Upsampler and Spatial\nUpsampler to produce high-ﬁdelity colorized images\nfrom low resolution results. The Color Upsampler\nconverts the coarse image of 512 colors back into a\n3-bit RGB image with 8 symbols per channel. The\nSpatial Upsampler generates colorized images with\nhigh resolution. ColTran can handle grayscale images\nof 256 × 256 pixels.\n5.7 GANsformer\nThe cognitive science literature talks about two mech-\nanisms by which human perception interacts, namely,\nbottom–up and top–down processing. Previous vision\ntasks using CNNs do not reﬂect this bidirectional\nnature because the local receptive ﬁeld reduces their\nability to model long-range dependencies. Therefore,\nHudson and Zitnick [35] aim to design a transformer\nnetwork with a highly adaptive architecture centered\naround relational attention and dynamic interaction.\nThey propose a Bipartite Transformer to eliminate\nthe limitation of huge computational complexity\nof self-attention of transformers. Unlike the self-\nattention operator which considers all pairwise\nrelations between input elements, the Bipartite\nTransformer generalizes this formulation by featuring\na bipartite graph between two groups of variables\n(latent and image features) instead. As shown in\nFig. 20, simplex attention distributes information in a\nsingle direction over the Bipartite Transformer, while\nDuplex attention supports bidirectional interaction\nbetween the elements. The bipartite structure makes\na good balance between expressiveness and eﬃciency,\nand it constructs the interaction between latent and\nvisual features to generate good results.\n5.8 StyTr 2\nConsidering the limited receptive ﬁelds of CNNs,\nobtaining global information about input images is\ndiﬃcult but is critical for the image style transfer\ntask. The content leak problem also occurs when\nCNN-based models are adopted for style transfer.\nTherefore, Deng et al. [38] propose the first transformer-\nbased style transfer model using the ability for long-\nrange extraction (Fig. 21). The unbiased Style\nTransfer Transformer framework StyTr 2 contains\ntwo transformer encoders to obtain domain-speciﬁc\nFig. 19 Overview of colorization transformer. Reproduced with permission from Ref. [30], c⃝ The Author(s) 2021.\n\nTransformers in computational visual media: A survey 49\nFig. 20 Overview of the GANsformer framework. Reproduced with\npermission from Ref. [35], c⃝ The Author(s) 2021.\ninformation. Following encoding, a multilayer trans-\nformer decoder generates the output sequences.\nMoreover, Deng et al. [ 38] propose a content-aware\nmechanism to learn the positional encoding based on\nimage semantic features and dynamically expand the\nposition to suit diﬀerent image sizes.\n5.9 VQGAN\nHigh-resolution image synthesis is a diﬃcult genera-\ntion problem which aims to generate high-ﬁdelity\nimages within a reasonable time. Convolutional\napproaches exploit the local structure of the image,\nwhile transformer methods are good at establishing\nlong-range interactions. Esser et al. [ 102] utilize the\nadvantages of CNNs and transformers to build a high-\nresolution image generation framework. They propose\na variant of VQVAE [ 36] and adopt adversarial\nlearning to achieve vivid results. The content hidden\nspace consists of a discrete codebook, and diﬀerent\ncodes in the codebook are combined according\nto a certain probability to represent the content\ninformation. The key to sampling in a discrete\nspace is to predict the distribution of discrete codes,\nand the transformer can deal with the issue. Given\nthe ﬁrst i codes, the transformer module is used to\npredict the probability of occurrence of the i-th code.\nThe number of codes in the codebook is 512–4096\naccording to the dataset. The model can synthesize\nthe results containing 1280 × 460 pixels.\n5.10 PCT\nUnlike CNNs, transformers are inherently permuta-\ntion invariant when processing a series of points and\nare thus suitable for point cloud learning. Guo et\nal. [39] propose a state-of-the-art transformer-based\npoint cloud model based on oﬀset-attention with an\nimplicit Laplace operator. They enhance the input\nembedding based on farthest point sampling and\nnearest neighbor search to better capture the local\ncontext in the point cloud.\n6 Multimodal learning\nThe above sections cover developments in conven-\ntional computer vision. Apart from pure vision\ntasks, transformer-based models have also achieved\npromising progress in language and vision multimodal\ntasks, such as visual question answering (VQA) [ 109,\n110], image captioning [111], and image retrieval [112],\ndue to the high performance achieved by the NLP\ntransformers. Transformer-based vision-language\n(V+L) approaches are often pretrained on multiple\ntasks and ﬁne-tuned on diverse downstream sub-tasks.\nInputs of diﬀerent modalities share the analogous\nsingle- or two-stream architecture.\nIn this section, we start from recently representative\ntransformer-based works on V+L tasks with diﬀerent\nframeworks (Section 6.1), and then summarise\npretraining objectives (Section 6.2) and compare\ndetails (Section 6.3).\nFig. 21 Overview of StyTr2. Reproduced with permission from Ref. [38], c⃝ The Author(s) 2021.\n\n50 Y. Xu, H. Wei, M. Lin, et al.\n6.1 Transformer-based V+L works\nMost transformer-based V+L works are based on two\nkinds of structures: the two-stream (each stream\nfor a single modality) framework or the single-\nstream (common stream for jointly learning cross-\nmodal representation) framework. ViLBERT [40]a n d\nUNITER [41] are representative works for two- and\nsingle-stream frameworks, respectively. Meanwhile,\nSemVLP [42] uniﬁes the two mainstream architectures\nfor aligning the cross-modal semantics.\n6.1.1 ViLBERT\nViLBERT [ 40] is a representative two-stream\ntransformer-based model for V+L. Two separate\nstreams are used for vision and language processing.\nFigure 23 shows the architecture of ViLBERT. Two\nparallel BERT-style models operate on image regions\nand text tokens. Each stream connects a series\nof transformer blocks (TRM) and co-attentional\ntransformer layers (Co-TRM). As shown in Fig. 22,\nthe Co-TRM layers enable information exchange\nbetween modalities, and the modiﬁed attention\nmechanism is the key technical innovation. By\nexchanging key–value pairs in multi-headed attention,\nthe Co-TRM structure allows for variable network\nFig. 22 (a) Architecture of a standard encoder transformer block.\n(b) Co-attention transformer layer in ViLBERT. Reproduced with\npermission from Ref. [40], c⃝ The Author(s) 2019.\ndepth for each modality and enables cross-modal\nconnections at diﬀerent depths.\n6.1.2 UNITER\nChen et al. [ 41] propose UNITER: UNiversal Image-\nTExt Representation. It can power heterogeneous\ndownstream V+L tasks with joint multimodal\nembeddings. As shown in Fig. 24, UNITER ﬁrst\nencodes image regions (visual features and bounding\nbox features) and textual words (tokens and positions)\ninto a shared embedding space with image and text\nembedders. Then, UNITER applies a transformer\nmodule to learn the joint embedding of the two\nmodalities through designed pretraining tasks that\ninclude classic image–text matching (ITM), masked\nlanguage modeling (MLM), and masked region\nmodeling (MRM). UNITER uses conditional masking\non MLM and MRM, which means masking only one\nmodality while keeping the other untainted. A novel\nword–region alignment pretraining task via optimal\ntransport is also proposed to encourage ﬁne-grained\nalignment between words and image regions. The\nauthors consider the matching of word tokens and\nRoI regions as minimizing the distance of two discrete\ndistributions, where the distance is computed based\non optimal transport. UNITER, as a single-stream\nmodel, achieved state-of-the-art performance when\nproposed. ViLLA [\n113], which combines UNITER\nand adversarial training, achieves higher performance.\n6.1.3 SemVLP\nLi et al. [42] present a novel V+L framework, SemVLP.\nIt uniﬁes both mainstream architectures. By fusing\nsingle- and two-stream architectures, SemVLP utilizes\ncross-modal semantics. Its framework is detailed\nin Fig. 25. On the basis of a shared bidirectional\ntransformer encoder with cross-modal attention\nmodule, SemVLP can encode the input text and\nimage into diﬀerent semantics. It adopts common\npretraining methods with a special training strategy:\nsingle- and two-stream frameworks are updated in\nFig. 23 Overview of ViLBERT. Reproduced with permission from Ref. [40], c⃝ The Author(s) 2019.\n\nTransformers in computational visual media: A survey 51\nFig. 24 Overview of UNITER. Reproduced with permission from Ref. [41], c⃝ Springer Nature Switzerland AG 2020.\nFig. 25 Overview of SemVLP. Reproduced with permission from Ref. [42], c⃝ The Author(s) 2021.\neach half of the training time for each mini-batch of\nimage–text pairs.\n6.2 Multimodal pretraining\nDesigning reasonable pretraining objectives for\ntransformer-based models, such as masked language\nmodeling (MLM) and next sentence classiﬁcation\nfrom BERT, has brought excellent results on NLP\ntasks. These methods also work in the cross-modal\nﬁeld with V+L. The key challenge is the way to\nreplicate or extend large-scale pretraining to cross-\nmodal methods and to design novel pretraining\nobjectives for multimodal learning. In this section,\nwe brieﬂy introduce pretraining tasks extended from\nBERT. These extended approaches include MLM,\nmasked region modeling (MRM), and image–text\nmatching (ITM). We also list other specially designed\npretraining tasks for multimodal learning.\n6.2.1 Masked language modeling\nMost recent V+L works follow BERT in using\nMLM for cross-modal tasks. UNITER modiﬁes\nMLM by introducing visual information. Speciﬁcally,\nUNITER attempts to predict masked words based on\n\n52 Y. Xu, H. Wei, M. Lin, et al.\nobservation of the surrounding words and all image\nregions. InterBERT [114] changes MLM to masked\nsegment modeling. In the case of using a random\nword to replace the selected word, masked segment\nmodeling masks a continuous segment of text instead\nof random words.\n6.2.2 Image–text matching\nFor another pretraining task of BERT, next sentence\nclassiﬁcation has been converted to an ITM problem,\nwhich determines whether a pair of sentence and\nimage regions match. This task is widely used in\nadvanced V+L works. InterBERT [ 114] performs\nITM with hard negatives by regarding the image–\ntext pairs in the dataset as positive samples, pairing\nthe images with uncorrelated texts, and regarding\nthe pairs as negative samples. VL-BERT [\n115]a n d\nUniﬁedVLP [116] also do not use ITM, tending to\nuse other eﬃcient choices like MRM introduced next.\n6.2.3 Masked region modeling\nThe existing masking method, MRM, is the dual\ntask of MLM. MLM can be easily applied to visual\ninput. Some researchers have proposed several novel\npretraining methods by masking input visual tokens\nto extend masked modeling to vision. Masked\nregion feature regression (MRFR) is one of these\napproaches applied by ViLBERT [\n40]. ViLBERT\ntrains the model to regress the masked input RoI\npooled feature, which is extracted by Faster R-\nCNN [\n86]. Most models perform optimization with\nL2 loss. VL-BERT [ 115] also follows MRFR instead\nof using ITM. It uses masked RoI classiﬁcation with\nlinguistic clues, predicting the category label of the\nmasked RoI obtained by Fast R-CNN [ 117] from\nthe other clues. On the contrary, some models\nchoose masked region classiﬁcation, which lets the\nmodel predict the object semantic class for each\nmasked region. Models are often optimized by\ncross-entropy loss or KL-divergence to learn the\nclass distribution. These MRM tasks are performed\nin UNITER and UNIMO [ 118]. InterBERT [ 114]\nalso changes MRM strategy in the visual modality\nby masking objects which have a high proportion\nof mutual intersection with zero vectors to avoid\ninformation leakage due to overlap between objects.\nNotably, earlier transformer-based works, such as\nVisualBERT [119] and B2T2 [ 120], do not extend\nMLM to the visual domain.\n6.2.4 Other designs for V+L\nSome models are also trained with unique, newly\ndesigned pretraining strategies. In Oscar [ 121], each\nimage–text pair is deﬁned as a triple and thus consists\nof a word sequence, a set of object tags, and a set of\nimage region features. Therefore, in addition to MLM\non words and object tags, Oscar uses a contrastive\nloss to encourage the model to distinguish the original\nand modiﬁed triple. By diﬀerently using contrastive\nlearning, UNIMO creates image–text pairs by a novel\ntext rewriting method. ERNIE-ViL [ 122] introduces\na scene graph to design advanced pretrained tasks,\nincluding object prediction, attribute prediction, and\nrelationship prediction. Li et al. [ 123] add masked\nsentence generation to optimize their model: a cross-\nmodal decoder is taught to autoregressively decode\nthe input sentence word-by-word conditioned on\nthe input image. Training directly on downstream\ntasks like QA is also used in LXMERT [\n124]a n d\nSemVLP [42].\n6.3 Comparisons and implementation de-\ntails\nTable 5 details implementations and open source. The\nMSCOCO dataset [111], maintained by Microsoft, is\nwidely used in multiple tasks like object detection. The\nConceptual Captions Dataset (CC) [ 125] is provided\nby Google AI and consists of nearly 3.3 million\nimages annotated with captions harvested from the\nweb. The SBU Captions Dataset [ 126] includes\nimage captions collected from 1 million images from\nFlickrx. MSCOCO, CC, and SBU all can be used\nfor image caption tasks. The Visual Genome [ 127]\nis a dataset including images and image content\nsemantic information. Visual Genome, VQA [ 109],\nVQAv2 [110], and GQA [128] datasets can all be used\nfor VQA pretraining. Notably, partial datasets are\nused as benchmarks simultaneously. Table 6 shows\nthe performance of models reported above on diﬀerent\nV+L benchmark datasets. The results are obtained\nby models ﬁne-tuned on the corresponding datasets.\n7 Conclusions and discussion\n7.1 Backbone design\nSection 3 describes several recent developments in\nthe backbone design of visual transformers, including\nx https://www.flickr.com\n\nTransformers in computational visual media: A survey 53\nT able 5 Model setting in various papers. COCO refers to MS COCO [ 129], CC to Conceptual Captions [ 125], VG to Visual Genome [ 127],\nSBU to SBU captions [126], and OI to OpenImages [130]\nModel Dataset(s) for pre-training Params Batch size Hard-aware Source (GitHub)\nViLBERT [40] CC 221M 512 8 TitanX jiasenlu/vilbert_beta\nVL-BERT [115] CC 110M 256 16 V100 jackroos/VL-BERT\nUNITER [41] CC/COCO/VG/SBU 110M Dynamic 16 V100 ChenRocks/UNITER\nOscar [121] CC/COCO/VG/SBU/Flicker30K/VQA/GQA 110M 512 — microsoft/Oscar\nVILLA [113] CC/COCO/VG/SBU — Task-speciﬁc — zhegan27/VILLA\nERNIE-ViL [122] CC/SBU 210M 512 8 V100 PaddlePaddle/ERNIE\nUNIMO [118] CC/COCO/VG/SBU — — — weili-baidu/UNIMO\nVinVL [131] CC/COCO/VG/SBU/Flicker30K/VQA/GQA/OI — 1024 — pzzhang/VinVL\nTDEN [123] CC — 1024 16 P40 YehLi/TDEN\nUniT [132] COCO/VG/VQAv2 — 64 64 V100 —\nSemVLP [42] CC/COCO/VG/SBU/VQAv2/GQA 140M 256 4 V100 —\nT able 6 Comparison of transformer-based V+L models on VQA [ 109], GQA [ 128], Flickr30K [ 112], CoCo Caption [ 111], NLVR2 [ 133],\nSNLI-VE [134], VCR [135], and RefCOCO+ [136] benchmarks\nVQA GQA IR-Flickr30K TR-Flickr30K CoCo Caption NLVR2 SNLI-VE VCR RefCOCO+\ntest-dev test-std test-dev test-std R@1 R@5 R@10 R@1 R@5 R@10 BLUE4 CIDEr dev test-P val test Q/A QA/R Q/AR val testA testB\nViLBERT [40] 70.55 70.92 — — 58.20 84.90 91.52 — — — — — — — — — 73.3 74.6 54.8 72.34 78.52 58.20\nVL-BERT [115] 71.79 72.22 — — — — — — — — — — — — — — 75.8 78.4 59.7 80.31 83.62 75.45\nUNITER [41] 73.82 74.02 — — 75.56 94.08 96.76 87.3 98.0 99.2 — — 79.12 79.98 79.39 79.38 77.3 80.8 62.8 84.25 86.34 79.75\nOscar [121] 73.61 73.82 61.58 61.62 — — — — — — 41.7 140.0 79.12 80.37 — — — — — 84.40 86.22 80.00\nVILLA [113] 74.69 74.87 76.26 94.24 96.84 87.9 97.5 98.8 — — 79.76 81.47 80.18 80.02 78.9 79.1 60.6 84.40 86.22 80.00\nERNIE-ViL [122] 74.95 75.10 — — 76.66 94.16 96.76 89.2 98.5 99.2 — — — — — — 79.2 83.5 66.3 75.89 82.37 66.91\nUNIMO [118] 73.79 74.02 — — — — — — — — 38.6 124.1 — — 80.00 79.10 — — — — — —\nVinVL [131] 76.52 76.60 65.05 64.65 75.40 92.90 93.30 58.8 83.5 90.3 41.0 140.9 82.67 83.98 — — — — — — — —\nTDEN [123] 72.50 72.80 — — — — — — — — 40.2 133.4 — — — — 75.7 76.4 58.0 — — —\nSemVLP [42] 74.52 74.68 62.87 63.62 74.80 93.43 96.12 87.7 98.2 99.3 — — 79.00 79.55 — — — — — — — —\nfeature map visualization approaches. Recent pro-\ngress can be technically divided into two main\nstreams: (i) enhancing the capability of visual\ntransformers in modeling spatial structure and\nlocality mechanism, such as a better image-to-token\nmodule, a pixel-level transformer block, a depth-\nwise convolution-based pooling layer, and an SW-\nMSA module, and (ii) boosting the richness of\nlearned visual features and promoting eﬃcient use\nof parameters, such as conditional position encoding,\na message communication scheme between the MSA\nheads, and deep-narrow ViT architectures.\nAs the ﬁrst visual transformer was proposed very\nrecently (October 2020), we believe that the potential\nof the ViT model has not been fully exploited and\nseveral research topics are worthy of consideration\nand eﬀort:\n•\nAdvanced designs of basic ViT operation or\nmodules and the corresponding learning scheme\nfor CV tasks, like injecting prior knowledge of\nimage data or the computer vision task into the\nmodule design or the learning scheme of visual\ntransformer models, and making the transformer\nmore computationally eﬃcient, are of interest.\nThe versatility of ViT models in additional\nreal-world scenarios, such as aesthetic visual\nanalysis [\n137–139], face anti-spooﬁng [ 140, 141],\nand point cloud learning [ 142], is also worthy of\nexploration.\n• The transformer block can be placed in the\nperspective of NAS. One of the goals of the\nNAS framework [143–145] is to search for optimal\nnetwork architectures for a given task without\nhuman intervention. Interesting architectures can\nbe considered and practical insights for further\ndevelopments can be gained by building on a well-\ndesigned search space that contains a transformer\nblock. Several recent works have investigated this\ntopic. Wang et al. [\n146] and So et al. [147] leverage\nNAS techniques to seek for eﬀective and eﬃcient\ntransformer-based architectures automatically. Li\net al. [148] propose a novel scheme, BossNAS, to\nachieve optimal solutions which trade-oﬀ CNN\narchitecture and transformer blocks.\n\n54 Y. Xu, H. Wei, M. Lin, et al.\n• Understanding of the working mechanism and\ntheoretical rationale of visual transformers can\nbe enhanced. Several researchers have achieved\npromising progress in unveiling the power of\ntransformer models, from such perspectives as\ninformation bottlenecks [\n149, 150] and better\nvisualization tools [23, 63].\n7.2 High-level vision\nIn Section 4, we introduce several representative\nworks on object detection. The basic logic follows\nthe line of DETR [ 24]. PVT [ 27], which is a general\nbackbone for dense prediction, is also introduced.\nSeveral problems still need to be addressed despite\nimprovements brought by these works. Unlike\nCNN-based methods, such as Faster-RCNN [\n86],\ncurrent transformers for dense prediction tasks suﬀer\nfrom high computation time. Thus, eﬃciency of\ntransformers for high-level vision remains a pressing\nresearch direction.\n7.3 Low-level vision and generation\nIn Section 5, we introduce some low-level vision\nand image generation tasks using transformer-based\nmodels. They can achieve outstanding results but\nhave diﬃculty generating large images. Therefore,\nextending a pure transformer with CNN layers is\nwidely adopted by many works. A pure transformer\nstructure still faces the challenge of high computation\ntime.\n7.4 Multimodal learning\nIn Section 6, we introduce several representative\ntransformer-based models proposed in the past 2\nyears for vision and language tasks. We also review\nmainstream pretraining tasks in the V+L ﬁeld.\nMeanwhile, transformer-based models have succeeded\nfor the tasks listed in Table 6, but performance can\nstill be improved:\n•\nPure transformers may be an alternative choice\nfor the image mode.\n• Design of eﬃcient pretraining tasks can lead to\nbetter results and performance.\nAcknowledgements\nWe thank the anonymous reviewers for their\nvaluable comments. This work was supported\nby National Key R&D Program of China under\nGrant No. 2020AAA0106200, and by National\nNatural Science Foundation of China under Grant\nNos. 61832016 and U20B2070.\nReferences\n[1] He, K. M.; Zhang, X. Y.; Ren, S. Q.; Sun, J.\nDeep residual learning for image recognition. In:\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 770–778, 2016.\n[2] Tan, M.; Le, Q. EﬃcientNet: Rethinking model\nscaling for convolutional neural networks. In:\nProceedings of the 36th International Conference on\nMachine Learning, 2019.\n[3] Radosavovic, I.; Kosaraju, R. P.; Girshick, R.; He,\nK. M.; Doll´ ar, P. Designing network design spaces.\nIn: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 10425–\n10433, 2020.\n[4] Yin, M. H.; Yao, Z. L.; Cao, Y.; Li, X.; Zhang, Z.; Lin,\nS.; Hu, H. Disentangled non-local neural networks.\nIn: Computer Vision–ECCV 2020. Lecture Notes in\nComputer Science, Vol. 12360. Vedaldi, A.; Bischof,\nH.; Brox, T.; Frahm, J. M. Eds. Springer Cham, 191–\n207, 2020.\n[5] Hu, H.; Gu, J. Y.; Zhang, Z.; Dai, J. F.; Wei,\nY. C. Relation networks for object detection. In:\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 3588–3597, 2018.\n[6] Wang, X. L.; Girshick, R.; Gupta, A.; He, K. M.\nNon-local neural networks. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision and\nPattern Recognition, 7794–7803, 2018.\n[7] Hu, H.; Zhang, Z.; Xie, Z. D.; Lin, S. Local relation\nnetworks for image recognition. In: Proceedings of the\nIEEE/CVF International Conference on Computer\nVision, 3463–3472, 2019.\n[8] Yuan, Y.; Huang, L.; Guo, J.; Zhang, C.; Chen, X.;\nWang, J. OCNet: Object context network for scene\nparsing. arXiv preprint arXiv:1809.00916, 2018.\n[9] Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weis-\nsenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani,\nM.; Minderer, M.; Heigold, G.; Gelly, S.; Uszkoreit,\nJ.; Houlsby, N. An image is worth 16x16 words:\nTransformers for image recognition at scale. In:\nProceedings of the International Conference on\nLearning Representations, 2021.\n[10] Devlin, J.; Chang, M.-W.; Lee, K.; Toutanova, K.\nBERT: Pre-training of deep bidirectional transformers\nfor language understanding. In: Proceedings of the\nConference of the North American Chapter of the\n\nTransformers in computational visual media: A survey 55\nAssociation for Computational Linguistics: Human\nLanguage Technologies, 4171–4186, 2019.\n[11] Chen, M.; Radford, A.; Child, R.; Wu, J.; Jun,\nH.; Luan, D.; Sutskever, I. Generative pretraining\nfrom pixels. In: Proceedings of the 37th International\nConference on Machine Learning, 1691–1703, 2020.\n[12] Graham, B.; El-Nouby, A.; Touvron, H.; Stock, P.;\nJoulin, A.; J´ egou, H.; Douze, M. LeViT: A vision\ntransformer in ConvNet’s clothing for faster inference.\narXiv preprint arXiv:2104.01136, 2021.\n[13] Tay, Y.; Dehghani, M.; Bahri, D.; Metzler, D.\nEﬃcient transformers: A survey. arXiv preprint\narXiv:2009.06732, 2020.\n[14] Liang, J.; Hu, D.; He, R.; Feng, J. Distill and ﬁne-\ntune: Eﬀective adaptation from a black-box source\nmodel. arXiv preprint arXiv:2104.01539, 2021.\n[15] Yuan, L.; Chen, Y.; Wang, T.; Yu, W.; Shi, Y.; Tay, F.\nE.; Feng, J.; Yan, S. Tokens-to-Token ViT: Training\nvision transformers from scratch on ImageNet. arXiv\npreprint arXiv:2101.11986, 2021.\n[16] Han, K.; Xiao, A.; Wu, E.; Guo, J.; Xu, C.;\nWang, Y. Transformer in transformer. arXiv preprint\narXiv:2103.00112, 2021.\n[17] Chu, X. X.; Tian, Z.; Zhang, B.; Wang, X. L.; Wei,\nX. L.; Xia, H. X.; Shen, C. Conditional positional\nencodings for vision transformers. arXiv preprint\narXiv:2102.10882, 2021.\n[18] D’Ascoli, S.; Touvron, H.; Leavitt, M. L.; Morcos, A.\nS.; Biroli, G.; Sagun, L. ConViT: Improving vision\ntransformers with soft convolutional inductive biases.\nIn: Proceedings of the 38th International Conference\non Machine Learning, 2286–2296, 2021.\n[19] Zhou, D.; Kang, B.; Jin, X.; Yang, L.; Lian, X.;\nHou, Q.; Feng, J. DeepViT: Towards deeper vision\ntransformer. arXiv preprint arXiv:2103.11886, 2021.\n[20] Liu, Z.; Lin, Y. T.; Cao, Y.; Hu, H.; Guo, B. N. Swin\ntransformer: Hierarchical vision transformer using\nshifted windows. arXiv preprint arXiv:2103.14030,\n2021.\n[21] Heo, B.; Yun, S.; Han, D.; Chun, S.; Oh, S. J.\nRethinking spatial dimensions of vision transformers.\narXiv preprint arXiv:2103.16302, 2021.\n[22] Li, Y. W.; Zhang, K.; Cao, J. Z.; Timofte, R.; Gool, L.\nV. LocalViT: Bringing locality to vision transformers.\narXiv preprint arXiv:2104.05707, 2021.\n[23] Chefer, H.; Gur, S.; Wolf, L. Transformer\ninterpretability beyond attention visualization. In:\nProceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 782–791,\n2021.\n[24] Carion, N.; Massa, F.; Synnaeve, G.; Usunier,\nN.; Kirillov, A.; Zagoruyko, S. End-to-end object\ndetection with transformers. In: Computer Vision–\nECCV 2020. Lecture Notes in Computer Science, Vol.\n12346. Vedaldi, A.; Bischof, H.; Brox, T.; Frahm, J.\nM. Eds. Springer Cham, 213–229, 2020.\n[25] Zhu, X. Z.; Su, W. J.; Lu, L. W.; Li, B.; Dai,\nJ. F. Deformable DETR: Deformable transformers\nfor end-to-end object detection. arXiv preprint\narXiv:2010.04159, 2020.\n[26] Dai, Z. G.; Cai, B. L.; Lin, Y. G.; Chen, J. Y. UP-\nDETR: Unsupervised pre-training for object detection\nwith transformers. arXiv preprint arXiv:2011.09094,\n2020.\n[27] Wang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.;\nLiang, D.; Lu, T.; Luo, P.; Shao. L. Pyramid\nvision transformer: A versatile backbone for dense\nprediction without convolutions. arXiv preprint\narXiv:2102.12122, 2021.\n[28] Wang, Y.; Xu, Z.; Wang, X.; Shen, C.; Cheng,\nB.; Shen, H.; Xia, H. End-to-end video instance\nsegmentation with transformers. In: Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 8741–8750, 2021.\n[29] Xie, E.; Wang, W.; Yu, Z.; Anandkumar, A.; Alvarez,\nJ. M.; Luo, P. SegFormer: Simple and eﬃcient design\nfor semantic segmentation with transformers. arXiv\npreprint arXiv:2105.15203, 2021.\n[30] Kumar, M.; Weissenborn, D.; Kalchbrenner, N.\nColorization transformer. In: Proceedings of the 9th\nInternational Conference on Learning Representations,\n2021.\n[31] Liu, B. C.; Song, K. P.; Zhu, Y. Z.; de Melo,\nG.; Elgammal, A. TIME: Text and image mutual-\ntranslation adversarial networks. In: Proceedings of\nthe 35th AAAI Conference on Artiﬁcial Intelligence,\n2082–2090, 2021.\n[32] Ramesh, A.; Pavlov, M.; Goh, G.; Gray, S.; Voss, C.;\nRadford, A.; Chen, M.; Sutskever, I. Zero-shot text-\nto-image generation. arXiv preprint arXiv:2102.12092,\n2021.\n[33] Yang, F. Z.; Yang, H.; Fu, J. L.; Lu, H. T.;\nGuo, B. N. Learning texture transformer network\nfor image super-resolution. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision and\nPattern Recognition, 5790–5799, 2020.\n[34] Jiang, Y. F.; Chang, S. Y.; Wang, Z. Y. TransGAN:\nTwo transformers can make one strong GAN. arXiv\npreprint arXiv:2102.07074, 2021.\n\n56 Y. Xu, H. Wei, M. Lin, et al.\n[35] Hudson, D. A.; Zitnick, C. L. Generative adversarial\ntransformers. arXiv preprint arXiv:2103.01209, 2021.\n[36] Van den Oord, A.; Vinyals, O.; Kavukcuoglu,\nK. Neural discrete representation learning. In:\nProceedings of the 31st International Conference on\nNeural Information Processing Systems, 6309–6318,\n2017.\n[37] Wang, Z.; Cun, X.; Bao, J.; Liu, J. Uformer: A\ngeneral U-shaped transformer for image restoration.\narXiv preprint arXiv:2106.03106, 2021.\n[38] Deng, Y. Y.; Tang, F.; Pan, X. J.; Dong, W. M.;\nXu, C. S. StyTr2: Unbiased image style transfer with\ntransformers. arXiv preprint arXiv:2105.14576, 2021.\n[39] Guo, M.-H.; Cai, J.-X.; Liu, Z.-N.; Mu, T.-J.; Martin,\nR. R.; Hu, S.-M. PCT: Point cloud transformer.\nComputational Visual Media Vol. 7, No. 2, 187–199,\n2021.\n[40] Lu, J.; Batra, D.; Parikh, D.; Lee, S. ViLBERT: Pre-\ntraining task-agnostic visiolinguistic representations\nfor vision-and-language tasks. In: Proceedings of the\n33rd Conference on Neural Information Processing\nSystems, 13–23, 2019.\n[41] Chen, Y.-C.; Li, L. J.; Yu, L. C.; El Kholy, A.;\nAhmed, F.; Gan, Z.; Cheng, Y.; Liu, J. UNITER:\nUNiversal image-TExt representation learning. In:\nComputer Vision–ECCV 2020. Lecture Notes in\nComputer Science, Vol. 12375. Vedaldi, A.; Bischof,\nH.; Brox, T.; Frahm, J. M. Eds. Springer Cham, 104–\n120, 2020.\n[42] Li, C. L.; Yan, M.; Xu, H. Y.; Luo, F. L.;\nHuang, S. F. SemVLP: Vision-language pre-training\nby aligning semantics at multiple levels.arXiv preprint\narXiv:2103.07829, 2021.\n[43] Zhang, R.; Isola, P.; Efros, A. A. Colorful image\ncolorization. In: Computer Vision–ECCV 2016.\nLecture Notes in Computer Science, Vol. 9907. Leibe,\nB.; Matas, J.; Sebe, N.; Welling, M. Eds. Springer\nCham, 649–666, 2016.\n[44] Zhang, R.; Zhu, J.-Y.; Isola, P.; Geng, X. Y.; Lin,\nA. S.; Yu, T. H.; Efros, A. A. Real-time user-guided\nimage colorization with learned deep priors. arXiv\npreprint arXiv:1705.02999, 2017.\n[45] Su, J.-W.; Chu, H.-K.; Huang, J.-B. Instance-\naware image colorization. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision and\nPattern Recognition, 7965–7974, 2020.\n[46] Pang, L.; Lan, Y.; Guo, J.; Xu, J.; Wan, S.; Cheng, X.\nText matching as image recognition. In: Proceedings\nof the 30th AAAI Conference on Artiﬁcial Intelligence,\n2793–2799, 2016.\n[47] Dong, C.; Loy, C. C.; He, K. M.; Tang, X. O. Image\nsuper-resolution using deep convolutional networks.\nIEEE Transactions on Pattern Analysis and Machine\nIntelligence Vol. 38, No. 2, 295–307, 2016.\n[48] Zhang, Y. L.; Tian, Y. P.; Kong, Y.; Zhong, B. N.; Fu,\nY. Residual dense network for image super-resolution.\nIn: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2472–2481,\n2018.\n[49] Haris, M.; Shakhnarovich, G.; Ukita, N. Deep\nback-projection networks for super-resolution. In:\nProceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 1664–1673,\n2018.\n[50] Chen, X.; Duan, Y.; Houthooft, R.; Schulman, J.;\nSutskever, I.; Abbeel, P. InfoGAN: Interpretable\nrepresentation learning by information maximiz-\ning generative adversarial nets. arXiv preprint\narXiv:1606.03657, 2016.\n[51] Salimans, T.; Goodfellow, I.; Zaremba, W.; Cheung,\nV.; Radford, A.; Chen, X. Improved techniques\nfor training GANs. arXiv preprint arXiv:1606.03498,\n2016.\n[52] Heusel, M.; Ramsauer, H.; Unterthiner, T.; Nessler,\nB.; Hochreiter, S. GANs trained by a two time-scale\nupdate rule converge to a local Nash equilibrium.\narXiv preprint arXiv:1706.08500, 2017.\n[53] Karras, T.; Laine, S.; Aila, T. M. A style-based gener-\nator architecture for generative adversarial networks.\nIn: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 4396–4405,\n2019.\n[54] Gulrajani, I.; Ahmed, F.; Arjovsky, M.; Dumoulin, V.;\nCourville, A. Improved training of wasserstein GANs.\narXiv preprint arXiv:1704.00028, 2017.\n[55] Bebis, G.; Georgiopoulos, M. Feed-forward neural\nnetworks. IEEE Potentials Vol. 13, No. 4, 27–31, 1994.\n[56] Ba, J. L.; Kiros, J. R.; Hinton, G. E. Layer\nnormalization. arXiv preprint arXiv:1607.06450, 2016.\n[57] Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.;\nJones, L.; Gomez, A. N.; Kaiser, L.; Polosukhin, I.\nAttention is all you need. In: Proceedings of the\n31st International Conference on Neural Information\nProcessing Systems, 6000–6010, 2017.\n[58] Hendrycks, D.; Gimpel, K. Gaussian error linear units\n(GELUs). arXiv preprint arXiv:1606.08415, 2016.\n[59] Kitaev, N.; Kaiser, L.; Levskaya, A. Reformer:\nThe eﬃcient transformer. In: Proceedings of the\nInternational Conference on Learning Representations,\n2020.\n\nTransformers in computational visual media: A survey 57\n[60] Choromanski, K. M.; Likhosherstov, V.; Dohan, D.;\nSong, X.; Gane, A.; Sarlos, T.; Hawkins, P.; Davis,\nJ. Q.; Mohiuddin, A.; Kaiser, L. et al. Rethinking\nattention with performers. In: Proceedings of the\nInternational Conference on Learning Representations,\n2021.\n[61] Wang, S.; Li, B.; Khabsa, M.; Fang, H.; Ma, H.\nLinformer: Self-attention with linear complexity.\narXiv preprint arXiv:2006.04768, 2020.\n[62] Abnar, S.; Zuidema, W. Quantifying attention ﬂow in\ntransformers. arXiv preprint arXiv:2005.00928, 2020.\n[63] Voita, E.; Talbot, D.; Moiseev, F.; Sennrich, R.; Titov,\nI. Analyzing multi-head self-attention: Specialized\nheads do the heavy lifting, the rest can be pruned.\nIn: Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, 5797–5808,\n2019.\n[64] Russakovsky, O.; Deng, J.; Su, H.; Krause, J.;\nSatheesh, S.; Ma, S. A.; Huang, Z.; Karpathy, A.;\nKhosla, A.; Bernstein, M. et al. ImageNet large scale\nvisual recognition challenge. International Journal of\nComputer Vision Vol. 115, No. 3, 211–252, 2015.\n[65] Touvron, H.; Cord, M.; Douze, M.; Massa, F.;\nSablayrolles, A.; Jegou, H. Training data-eﬃcient\nimage transformers & distillation through attention.\nIn: Proceedings of the 38th International Conference\non Machine Learning, 10347–10357, 2021.\n[66] Han, Y. Z.; Huang, G.; Song, S. J.; Yang, L.; Wang, Y.\nL. Dynamic neural networks: A survey. arXiv preprint\narXiv:2102.04906, 2021.\n[67] Xu, W.; Xu, Y.; Chang, T.; Tu, Z. Co-scale\nconv-attentional image transformers. arXiv preprint\narXiv:2104.06399, 2021.\n[68] Dong, X. Y.; Bao, J. M.; Chen, D. D.; Zhang,\nW. M.; Yu, N. H.; Yuan, L.; Chen, D.; Guo, B.\nCSWin transformer: A general vision transformer\nbackbone with cross-shaped windows. arXiv preprint\narXiv:2107.00652, 2021.\n[69] Huang, Z. L.; Wang, X. G.; Huang, L. C.; Huang,\nC.; Wei, Y. C.; Liu, W. CCNet: Criss-cross attention\nfor semantic segmentation. In: Proceedings of the\nIEEE/CVF International Conference on Computer\nVision, 603–612, 2019.\n[70] Hou, Q. B.; Zhang, L.; Cheng, M. M.; Feng, J. S. Strip\npooling: Rethinking spatial pooling for scene parsing.\nIn: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 4002–4011,\n2020.\n[71] Touvron, H.; Cord, M.; Sablayrolles, A.; Synnaeve,\nG.; Jegou, H. Going deeper with image transformers.\narXiv preprint arXiv:2103.17239, 2021.\n[72] Selvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.;\nParikh, D.; Batra, D. Grad-CAM: Visual explanations\nfrom deep networks via gradient-based localization.\nIn: Proceedings of the IEEE International Conference\non Computer Vision, 618–626, 2017.\n[73] Binder, A.; Montavon, G.; Lapuschkin, S.; M¨ uller,\nK.-R.; Samek, W. Layer-wise relevance propagation\nfor neural networks with local renormalization layers.\nIn: Artiﬁcial Neural Networks and Machine Learning–\nICANN 2016. Lecture Notes in Computer Science,\nVol. 9887. Villa, A.; Masulli, P.; Pons Rivero, A. Eds.\nSpringer Cham, 63–71, 2016.\n[74] Zheng, S.; Lu, J.; Zhao, H.; Zhu, X.; Luo, Z.;\nWang, Y.; Fu, Y.; Feng, J.; Xiang, T.; Torr, P.\nH. et al. Rethinking semantic segmentation from a\nsequence-tosequence perspective with transformers.\nIn: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 6881–6890,\n2021.\n[75] Duke, B.; Ahmed, A.; Wolf, C.; Aarabi, P.;\nTaylor, G. W. SSTVOS: Sparse spatiotemporal\ntransformers for video object segmentation. arXiv\npreprint arXiv:2101.08833, 2021.\n[76] Chen, J. N.; Lu, Y. Y.; Yu, Q. H.; Luo, X. D.;\nZhou, Y. Y. TransUNet: Transformers make strong\nencoders for medical image segmentation. arXiv\npreprint arXiv:2102.04306, 2021.\n[77] Ye, L. W.; Rochan, M.; Liu, Z.; Wang, Y. Cross-\nmodal self-attention network for referring image\nsegmentation. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern\nRecognition, 10494–10503, 2019.\n[78] Wang, H.; Zhu, Y.; Adam, H.; Yuille, A.; Chen, L.-C.\nMax-deeplab: End-to-end panoptic segmentation with\nmask transformers. arXiv preprint arXiv:2012.00759,\n2020.\n[79] Durner, M.; Boerdijk, W.; Sundermeyer, M.; Friedl,\nW.; Marton, Z.-C.; Triebel, R. Unknown object\nsegmentation from stereo images. arXiv preprint\narXiv:2103.06796, 2021.\n[80] Cao, Z.; Hidalgo, G.; Simon, T.; Wei, S.-E.; Sheikh, Y.\nOpenPose: Realtime multi-person 2D pose estimation\nusing part aﬃnity ﬁelds. IEEE Transactions on\nPattern Analysis and Machine Intelligence Vol. 43,\nNo. 1, 172–186, 2021.\n\n58 Y. Xu, H. Wei, M. Lin, et al.\n[81] Simon, T.; Joo, H.; Matthews, I.; Sheikh, Y.\nHand keypoint detection in single images using\nmultiview bootstrapping. In: Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, 4645–4653, 2017.\n[82] Cao, Z.; Simon, T.; Wei, S.-E.; Sheikh, Y. Realtime\nmulti-person 2D pose estimation using part aﬃnity\nﬁelds. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 1302–1310,\n2017.\n[83] Fang, H.-S.; Xie, S. Q.; Tai, Y.-W.; Lu, C. W.\nRMPE: Regional multi-person pose estimation. In:\nProceedings of the IEEE International Conference on\nComputer Vision, 2353–2362, 2017.\n[84] Zhang, F.; Zhu, X. T.; Dai, H. B.; Ye, M.; Zhu,\nC. Distribution-aware coordinate representation for\nhuman pose estimation. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision and\nPattern Recognition, 7091–7100, 2020.\n[85] Sun, K.; Xiao, B.; Liu, D.; Wang, J. D. Deep\nhigh-resolution representation learning for human\npose estimation. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern\nRecognition, 5686–5696, 2019.\n[86] Ren, S.; He, K.; Girshick, R.; Sun, J. Faster R-\nCNN: Towards real-time object detection with region\nproposal networks. arXiv preprint arXiv:1506.01497,\n2015.\n[87] Cai, Z. W.; Vasconcelos, N. Cascade R-CNN: High\nquality object detection and instance segmentation.\nIEEE Transactions on Pattern Analysis and Machine\nIntelligence Vol. 43, No. 5, 1483–1498, 2021.\n[88] Lin, T. Y.; Goyal, P.; Girshick, R.; He, K. M.;\nDoll´ ar, P. Focal loss for dense object detection. In:\nProceedings of the IEEE International Conference on\nComputer Vision, 2999–3007, 2017.\n[89] Zhou, X.; Wang, D.; Kr¨ ahenb¨ uhl, P. Objects as points.\narXiv preprint arXiv:1904.07850, 2019.\n[90] Tian, Z.; Shen, C. H.; Chen, H.; He, T. FCOS:\nFully convolutional one-stage object detection.\nIn: Proceedings of the IEEE/CVF International\nConference on Computer Vision, 9626–9635, 2019.\n[91] Stewart, R.; Andriluka, M.; Ng, A. Y. End-to-end\npeople detection in crowded scenes. In: Proceedings\nof the IEEE Conference on Computer Vision and\nPattern Recognition, 2325–2333, 2016.\n[92] Hosang, J.; Benenson, R.; Schiele, B. Learning\nnon-maximum suppression. In: Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, 6469–6477, 2017.\n[93] Rezatoﬁghi, S. H.; Kaskman, R.; Motlagh, F. T.;\nShi, Q. F.; Cremers, D.; Leal-Taix´ e, L.; Reid, I.\nDeep perm-set net: Learn to predict sets with\nunknown permutation and cardinality using deep\nneural networks. arXiv preprint arXiv:1805.00613,\n2018.\n[94] Pan, X. J.; Tang, F.; Dong, W. M.; Gu, Y.; Song,\nZ. C.; Meng, Y. P.; Xu, P.; Deussen, O.; Xu,\nC. Self-supervised feature augmentation for large\nimage object detection. IEEE Transactions on Image\nProcessing Vol. 29, 6745–6758, 2020.\n[95] Pan, X.; Gao, Y.; Lin, Z.; Tang, F.; Dong, W.;\nYuan, H.; Huang, F.; Xu, C. Unveiling the potential\nof structure preserving for weakly supervised object\nlocalization. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern\nRecognition, 11642–11651, 2021.\n[96] Pan, X. J.; Ren, Y. Q.; Sheng, K. K.; Dong, W.\nM.; Yuan, H. L.; Guo, X. W.; Ma, C.; Xu, C.\nDynamic reﬁnement network for oriented and densely\npacked object detection. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision and\nPattern Recognition, 11204–11213, 2020.\n[97] Chu, X. X.; Tian, Z.; Wang, Y. Q.; Zhang, B.; Shen,\nC. H. Twins: Revisiting spatial attention design in\nvision transformers. arXiv preprint arXiv:2104.13840,\n2021.\n[98] Beal, J.; Kim, E.; Tzeng, E.; Park, D. H.; Kislyuk,\nD. Toward transformer-based object detection. arXiv\npreprint arXiv:2012.09958, 2020.\n[99] Dai, J. F.; Qi, H. Z.; Xiong, Y. W.; Li, Y.; Zhang,\nG. D.; Hu, H.; Wei, Y. Deformable convolutional\nnetworks. In: Proceedings of the IEEE International\nConference on Computer Vision, 764–773, 2017.\n[100] He, K. M.; Gkioxari, G.; Doll´ ar, P.; Girshick, R. Mask\nR-CNN. In: Proceedings of the IEEE International\nConference on Computer Vision, 2980–2988, 2017.\n[101] Chen, H.; Wang, Y.; Guo, T.; Xu, C.; Deng, Y.; Liu,\nZ.; Ma, S.; Xu, C.; Xu, C.; Gao, W. Pre-trained\nimage processing transformer. In: Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 12299–12310, 2021.\n[102] Esser, P.; Rombach, R.; Ommer, B. Taming\ntransformers for high-resolution image synthesis.\narXiv preprint arXiv:2012.09841, 2020.\n[103] Kaiser, L.; Bengio, S. Can active memory replace\nattention? In: Proceedings of the 30th International\nConference on Neural Information Processing Systems,\n3781–3789, 2016.\n\nTransformers in computational visual media: A survey 59\n[104] Vinyals, O.; Toshev, A.; Bengio, S.; Erhan, D. Show\nand tell: Lessons learned from the 2015 MSCOCO\nimage captioning challenge. IEEE Transactions on\nPattern Analysis and Machine Intelligence Vol. 39,\nNo. 4, 652–663, 2016.\n[105] Vinyals, O.; Toshev, A.; Bengio, S.; Erhan, D. Show\nand tell: A neural image caption generator. In:\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 3156–3164, 2015.\n[106] Rolfe, J. T. Discrete variational autoencoders. arXiv\npreprint arXiv:1609.02200, 2016.\n[107] Goodfellow, I. J.; Pouget-Abadie, J.; Mirza, M.; Xu,\nB.; Warde-Farley, D.; Ozair, S.; Courville, A.; Bengio,\nY. Generative adversarial networks. arXiv preprint\narXiv:1406.2661, 2014.\n[108] Ho, J.; Kalchbrenner, N.; Weissenborn, D.; Salimans,\nT. Axial attention in multidimensional transformers.\narXiv preprint arXiv:1912.12180, 2019.\n[109] Antol, S.; Agrawal, A.; Lu, J. S.; Mitchell, M.; Batra,\nD.; Zitnick, C. L.; Parikh, D. VQA: Visual question\nanswering. In: Proceedings of the IEEE International\nConference on Computer Vision, 2425–2433, 2015.\n[110] Goyal, Y.; Khot, T.; Summers-Stay, D.; Batra, D.;\nParikh, D. Making the V in VQA matter: Elevating\nthe role of image understanding in visual question\nanswering. In: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 6325–\n6334, 2017.\n[111] Chen, X. L.; Fang, H.; Lin, T.-Y.; Vedantam, R.;\nGupta, S.; Dollar, P.; Zitnick, C. L. Microsoft COCO\ncaptions: Data collection and evaluation server. arXiv\npreprint arXiv:1504.00325, 2015.\n[112] Young, P.; Lai, A.; Hodosh, M.; Hockenmaier, J.\nFrom image descriptions to visual denotations: New\nsimilarity metrics for semantic inference over event\ndescriptions. Transactions of the Association for\nComputational Linguistics Vol. 2, 67–78, 2014.\n[113] Gan, Z.; Chen, Y.-C.; Li, L.; Zhu, C.; Cheng, Y.;\nLiu, J. Large-scale adversarial training for vision-\nand-language representation learning. In: Advances\nin Neural Information Processing Systems, Vol. 33.\nLarochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.\nF.; Lin, H. Eds. Curran Associates, Inc., 6616–6628,\n2020.\n[114] Lin, J. Y.; Yang, A.; Zhang, Y. C.; Liu, J.; Yang, H. X.\nInterBERT: Vision-and-language interaction for multi-\nmodal pretraining. arXiv preprint arXiv:2003.13198,\n2020.\n[115] Su, W.; Zhu, X.; Cao, Y.; Li, B.; Lu, L.; Wei, F.; Dai,\nJ. VL-BERT: Pre-training of generic visual-linguistic\nrepresentations. In: Proceedings of the International\nConference on Learning Representations, 2020.\n[116] Zhou, L. W.; Palangi, H.; Zhang, L.; Hu, H. D.; Corso,\nJ.; Gao, J. F. Uniﬁed vision-language pre-training for\nimage captioning and VQA. In: Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, Vol. 34,\nNo. 7, 13041–13049, 2020.\n[117] Girshick, R. Fast R-CNN. In: Proceedings of the\nIEEE International Conference on Computer Vision,\n1440–1448, 2015.\n[118] Li, W.; Gao, C.; Niu, G. C.; Xiao, X. Y.; Wang, H. F.\nUNIMO: Towards uniﬁed-modal understanding and\ngeneration via cross-modal contrastive learning. arXiv\npreprint arXiv:2012.15409, 2020.\n[119] Li, L. H.; Yatskar, M.; Yin, D.; Hsieh, C. J.; Chang, K.\nW. VisualBERT: A simple and performant baseline for\nvision and language. arXiv preprint arXiv:1908.03557,\n2019.\n[120] Alberti, C.; Ling, J.; Collins, M.; Reitter, D. Fusion of\ndetected objects in text for visual question answering.\nIn: Proceedings of the Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing, 2131–2140, 2019.\n[121] Li, X. J.; Yin, X.; Li, C. Y.; Zhang, P. C.; Hu, X. W.;\nZhang, L.; Wang, L.; Hu, H.; Dong, L.; Wei, F. et\nal. OSCAR: Object-semantics aligned pre-training for\nvision-language tasks. In: Computer Vision–ECCV\n2020. Lecture Notes in Computer Science, Vol. 12375.\nVedaldi, A.; Bischof, H.; Brox, T.; Frahm, J. M. Eds.\nSpringer Cham, 121–137, 2020.\n[122] Yu, F.; Tang, J.; Yin, W.; Sun, Y.; Tian, H.; Wu, H.;\nWang, H. ERNIE-ViL: Knowledge enhanced vision-\nlanguage representations through scene graph. In:\nProceedings of the AAAI Conference on Artiﬁcial\nIntelligence, 2021.\n[123] Li, Y.; Pan, Y.; Yao, T.; Chen, J.; Mei, T. Scheduled\nsampling in vision-language pretraining with decou-\npled encoder–decoder network. In: Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, 8518–8526,\n2021.\n[124] Tan, H.; Bansal, M. LXMERT: Learning cross-\nmodality encoder representations from transformers.\nIn: Proceedings of the Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing, 5100–5111, 2019.\n\n60 Y. Xu, H. Wei, M. Lin, et al.\n[125] Sharma, P.; Ding, N.; Goodman, S.; Soricut, R.\nConceptual captions: A cleaned, hypernymed, image\nalt-text dataset for automatic image captioning. In:\nProceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics, 2556–2565,\n2018.\n[126] Ordonez, V.; Kulkarni, G.; Berg, T. L. Im2Text: De-\nscribing images using 1 million captioned photographs.\nIn: Proceedings of the 24th International Conference\non Neural Information Processing Systems, 1143–1151,\n2011.\n[127] Krishna, R.; Zhu, Y. K.; Groth, O.; Johnson, J.;\nHata, K. J.; Kravitz, J.; Chen, S.; Kalantidis, Y.;\nLi, L.-J.; Shamma, D. A. et al. Visual genome:\nConnecting language and vision using crowdsourced\ndense image annotations. International Journal of\nComputer Vision Vol. 123, No. 1, 32–73, 2017.\n[128] Hudson, D. A.; Manning, C. D. GQA: A new dataset\nfor real-world visual reasoning and compositional\nquestion answering. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern\nRecognition, 6693–6702, 2019.\n[129] Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.;\nPerona, P.; Ramanan, D., Dollar, P.; Zitnick, C.\nL. Microsoft COCO: Common objects in context.\nIn: Computer Vision–ECCV 2014. Lecture Notes in\nComputer Science, Vol. 8693. Fleet, D.; Pajdla, T.;\nSchiele, B.; Tuytelaars, T. Eds. Springer Cham, 740–\n755, 2014.\n[130] Kuznetsova, A.; Rom, H.; Alldrin, N.; Uijlings, J.;\nKrasin, I.; Pont-Tuset, J.; Kamali, S.; Popov, S.;\nMalloci, M.; Kolesnikov, A. et al. The open images\ndataset V4. International Journal of Computer Vision\nVol. 128, No. 7, 1956–1981, 2020.\n[131] Zhang, P.; Li, X.; Hu, X.; Yang, J.; Zhang, L.;\nWang, L.; Choi, Y.; Gao, J. VinVL: Revisiting\nvisual representations in vision-language models.\nIn: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 5579–5588,\n2021.\n[132] Hu, R.; Singh, A. UniT: Multimodal multitask\nlearning with a uniﬁed transformer. arXiv preprint\narXiv:2102.10772, 2021.\n[133] Suhr, A.; Zhou, S.; Zhang, A.; Zhang, I.; Bai, H.\nJ.; Artzi, Y. A corpus for reasoning about natural\nlanguage grounded in photographs. In: Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, 6418–6428, 2019.\n[134] Xie, N.; Lai, F.; Doran, D.; Kadav, A. Visual\nentailment: A novel task for ﬁne-grained image\nunderstanding. arXiv preprint arXiv:1901.06706,\n2019.\n[135] Zellers, R.; Bisk, Y.; Farhadi, A.; Choi, Y. From\nrecognition to cognition: Visual commonsense reason-\ning. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 6713–6724,\n2019.\n[136] Kazemzadeh, S.; Ordonez, V.; Matten, M.; Berg, T.\nReferItGame: Referring to objects in photographs of\nnatural scenes. In: Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing,\n787–798, 2014.\n[137] Sheng, K. K.; Dong, W. M.; Ma, C. Y.; Mei, X.;\nHuang, F. Y.; Hu, B.-G. Attention-based multi-\npatch aggregation for image aesthetic assessment.\nIn: Proceedings of the 26th ACM International\nConference on Multimedia, 879–886, 2018.\n[138] Sheng, K. K.; Dong, W. M.; Chai, M. L.; Wang,\nG. H.; Zhou, P.; Huang, F. Y.; Hu, B.-G.; Ji, R.;\nMa, C. Revisiting image aesthetic assessment via self-\nsupervised feature learning. In: Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, Vol. 34,\nNo. 4, 5709–5716, 2020.\n[139] Sheng, K. K.; Dong, W. M.; Huang, H. B.; Chai,\nM. L.; Zhang, Y.; Ma, C. Y.; Hu, B.-G. Learning to\nassess visual aesthetics of food images. Computational\nVisual Media Vol. 7, No. 1, 139–152, 2021.\n[140] Zhang, S. F.; Wang, X. B.; Liu, A.; Zhao, C. X.;\nWan, J.; Escalera, S.; Shi, H.; Wang, Z.; Li, S.\nZ. A dataset and benchmark for large-scale multi-\nmodal face anti-spooﬁng. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision and\nPattern Recognition, 919–928, 2019.\n[141] Chen, Z.; Yao, T.; Sheng, K.; Ding, S.; Tai, Y.; Li,\nJ.; Huang, F.; Jin, X. Generalizable representation\nlearning for mixture domain face anti-spooﬁng. In:\nProceedings of the AAAI Conference on Artiﬁcial\nIntelligence, 1132–1139, 2021.\n[142] Zhao, H.; Jiang, L.; Jia, J.; Torr, P.; Koltun, V. Point\ntransformer. arXiv preprint arXiv:2012.09164, 2020.\n[143] Zoph, B.; Le, Q. V. Neural architecture search\nwith reinforcement learning. In: Proceedings of the\nInternational Conference on Learning Representations,\n2017.\n[144] Zoph, B.; Vasudevan, V.; Shlens, J.; Le, Q. V.\nLearning transferable architectures for scalable image\nrecognition. In: Proceedings of the IEEE/CVF\n\nTransformers in computational visual media: A survey 61\nConference on Computer Vision and Pattern\nRecognition, 8697–8710, 2018.\n[145] Real, E.; Aggarwal, A.; Huang, Y. P.; Le, Q. V.\nRegularized evolution for image classiﬁer architecture\nsearch. In: Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, Vol. 33, 4780–4789, 2019.\n[146] Wang, H. R.; Wu, Z. H.; Liu, Z. J.; Cai, H.; Zhu,\nL. G.; Gan, C.; Han, S. HAT: Hardware-aware\ntransformers for eﬃcient natural language processing.\nIn: Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, 7675–7688,\n2020.\n[147] So, D.; Le, Q.; Liang, C. The evolved transformer. In:\nProceedings of the 36th International Conference on\nMachine Learning, 5877–5886, 2019.\n[148] Li, C. L.; Tang, T.; Wang, G. R.; Peng, J. F.; Chang,\nX. J. BossNAS: Exploring hybrid CNN-transformers\nwith Block-wisely Self-supervised neural architecture\nsearch. arXiv preprint arXiv:2103.12424, 2021.\n[149] Schulz, K.; Sixt, L.; Tombari, F.; Landgraf, T.\nRestricting the ﬂow: Information bottlenecks for\nattribution. In: Proceedings of the International\nConference on Learning Representations, 2019.\n[150] Jiang, Z.; Tang, R.; Xin, J.; Lin, J. Inserting\ninformation bottleneck for attribution in transformers.\nIn: Proceedings of the Conference on Empirical\nMethods in Natural Language Processing: Findings,\n3850–3857, 2020.\nYifan Xu is currently a postgraduate\nof the National Laboratory of Pattern\nRecognition (NLPR) at the Institute\nof Automation, Chinese Academy of\nSciences. He received a his B.Eng. degree\nfrom Beijing Institute of Technology in\n2015. His research interests include\ntransfer learning, machine learning, and\ncomputational visual media.\nHuapeng Wei is a postgraduate of the\nSchool of Artiﬁcial Intelligence, Jilin\nUniversity. He received his B.Sc. degree\nfrom Jilin University in 2020. His\nresearch interests include computational\nvisual media and image processing.\nMinxuan Linreceived his B.Sc. degree\nin computer science and technology\nfrom the Ocean University of China in\n2018. He is currently a postgraduate\nof NLPR. His research interests include\ncomputational visual media and machine\nlearning.\nYingying Deng received her B.Sc.\ndegree in automation from the University\nof Science and Technology, Beijing in\n2017. She is currently working towards\nher Ph.D. degree in NLPR. Her research\ninterests include computational visual\nmedia and machine learning.\nKekai Sheng received his Ph.D. degree\nfrom NLPR in 2019. He received\nhis B.Eng. degree in telecommunication\nengineering from the University of\nScience and Technology, Beijing in 2014.\nHe is currently a research engineer at\nYoutu Lab, Tencent Inc. His research\ninterests include domain adaptation,\nneural architecture search, and AutoML.\nMengdan Zhang received her Ph.D.\ndegree from NLPR in 2018. She received\nher B.Eng. degree in automation from\nXi’an Jiao Tong University in 2013. She\nis currently a research engineer at Youtu\nLab, Tencent Inc. Her research interests\ninclude computer vision and machine\nlearning.\nFan Tang is an assistant professor in\nthe School of Artiﬁcial Intelligence, Jilin\nUniversity. He received his B.Sc. degree\nin computer science from North China\nElectric Power University in 2013 and\nhis Ph.D. degree from NLPR in 2019.\nHis research interests include computer\ngraphics, computer vision, and machine\nlearning.\n\n62 Y. Xu, H. Wei, M. Lin, et al.\nWeiming Dongis a professor in NLPR.\nHe received his B.Eng. and M.S. degrees\nin computer science in 2001 and 2004\nfrom Tsinghua University. He received\nhis Ph.D. degree in information tech-\nnology from the University of Lorraine,\nFrance, in 2007. His research interests\ninclude visual media synthesis and\nevaluation. Weiming Dong is a member of the ACM and\nIEEE.\nFeiyue Huang is the director of the\nYoutu Lab, Tencent Inc. He received his\nB.Sc. and Ph.D. degrees in computer\nscience in 2001 and 2008 respectively,\nboth from Tsinghua University, China.\nHis research interests include image\nunderstanding and face recognition.\nChangsheng Xu is a professor in\nNLPR. His research interests include\nmultimedia content analysis, indexing\nand retrieval, pattern recognition, and\ncomputer vision. Prof. Xu has served\nas associate editor, guest editor, general\nchair, program chair, area/track chair,\nspecial session organizer, session chair and TPC member\nfor over 20 prestigious IEEE and ACM multimedia journals,\nconferences, and workshops. Currently he is the editor-in-\nchief of Multimedia Systems. Changsheng Xu is an IEEE\nFellow, IAPR Fellow, and ACM Distinguished Scientist.\nOpen Access This article is licensed under a Creative\nCommons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduc-\ntion in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link\nto the Creative Commons licence, and indicate if changes\nwere made.\nThe images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and\nyour intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission\ndirectly from the copyright holder.\nTo view a copy of this licence, visit http://\ncreativecommons.org/licenses/by/4.0/.\nOther papers from this open access journal are available\nfree of charge from http://www.springer.com/journal/41095.\nTo submit a manuscript, please go to https://www.\neditorialmanager.com/cvmj.\n",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.761467456817627
    },
    {
      "name": "Transformer",
      "score": 0.6647558212280273
    },
    {
      "name": "Categorization",
      "score": 0.5812304019927979
    },
    {
      "name": "Architecture",
      "score": 0.537872850894928
    },
    {
      "name": "Graphics",
      "score": 0.5007829666137695
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49076589941978455
    },
    {
      "name": "Computer graphics",
      "score": 0.45618146657943726
    },
    {
      "name": "Visualization",
      "score": 0.43875136971473694
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3986501395702362
    },
    {
      "name": "Machine learning",
      "score": 0.3473541736602783
    },
    {
      "name": "Engineering",
      "score": 0.15541675686836243
    },
    {
      "name": "Computer graphics (images)",
      "score": 0.14909178018569946
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210112150",
      "name": "Institute of Automation",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I194450716",
      "name": "Jilin University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    }
  ]
}