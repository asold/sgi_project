{
    "title": "Hybrid Attention Fusion Embedded in Transformer for Remote Sensing Image Semantic Segmentation",
    "url": "https://openalex.org/W4391305653",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2091638196",
            "name": "Yan Chen",
            "affiliations": [
                "Hefei University"
            ]
        },
        {
            "id": "https://openalex.org/A2098030815",
            "name": "QUAN Dong",
            "affiliations": [
                "Hefei University"
            ]
        },
        {
            "id": "https://openalex.org/A2096212953",
            "name": "Xiaofeng Wang",
            "affiliations": [
                "Hefei University"
            ]
        },
        {
            "id": "https://openalex.org/A2097443723",
            "name": "Qianchuan Zhang",
            "affiliations": [
                "Hefei University"
            ]
        },
        {
            "id": "https://openalex.org/A4290950770",
            "name": "Menglei Kang",
            "affiliations": [
                "Hefei University"
            ]
        },
        {
            "id": "https://openalex.org/A2313432427",
            "name": "Wenxiang Jiang",
            "affiliations": [
                "Hefei University"
            ]
        },
        {
            "id": "https://openalex.org/A2147812621",
            "name": "Mengyuan Wang",
            "affiliations": [
                "Hefei University"
            ]
        },
        {
            "id": "https://openalex.org/A2106669146",
            "name": "Lixiang Xu",
            "affiliations": [
                "Hefei University"
            ]
        },
        {
            "id": "https://openalex.org/A2096971521",
            "name": "Chen Zhang",
            "affiliations": [
                "Hefei University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2940726923",
        "https://openalex.org/W2774558171",
        "https://openalex.org/W2782934949",
        "https://openalex.org/W2648242067",
        "https://openalex.org/W2755013453",
        "https://openalex.org/W3110908156",
        "https://openalex.org/W1938976761",
        "https://openalex.org/W1905829557",
        "https://openalex.org/W4384406330",
        "https://openalex.org/W1903029394",
        "https://openalex.org/W2963881378",
        "https://openalex.org/W2787091153",
        "https://openalex.org/W2560023338",
        "https://openalex.org/W2886934227",
        "https://openalex.org/W3196904463",
        "https://openalex.org/W4296079303",
        "https://openalex.org/W3043645330",
        "https://openalex.org/W4313524854",
        "https://openalex.org/W6857839881",
        "https://openalex.org/W4379984088",
        "https://openalex.org/W4226060438",
        "https://openalex.org/W3212386989",
        "https://openalex.org/W4289752563",
        "https://openalex.org/W3129042754",
        "https://openalex.org/W3109998321",
        "https://openalex.org/W4285171099",
        "https://openalex.org/W4388157208",
        "https://openalex.org/W4382119132",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W2901442382",
        "https://openalex.org/W2778539913",
        "https://openalex.org/W2970773259",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2955058313",
        "https://openalex.org/W3109301572",
        "https://openalex.org/W4388739256",
        "https://openalex.org/W3043181422",
        "https://openalex.org/W4285195596",
        "https://openalex.org/W3103092912",
        "https://openalex.org/W3098881417",
        "https://openalex.org/W2945127520",
        "https://openalex.org/W3177272171",
        "https://openalex.org/W4361274959",
        "https://openalex.org/W3190334976",
        "https://openalex.org/W6797399245",
        "https://openalex.org/W4321232185",
        "https://openalex.org/W4214893857",
        "https://openalex.org/W4386885062",
        "https://openalex.org/W3161825146",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W4200547174",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W6790275670",
        "https://openalex.org/W2752782242",
        "https://openalex.org/W3034552520",
        "https://openalex.org/W4386076493",
        "https://openalex.org/W2618530766",
        "https://openalex.org/W3137572916",
        "https://openalex.org/W3200075728",
        "https://openalex.org/W4283450732",
        "https://openalex.org/W4380537181",
        "https://openalex.org/W4386634500",
        "https://openalex.org/W4226289601",
        "https://openalex.org/W4383113434",
        "https://openalex.org/W4387010050",
        "https://openalex.org/W2963794428",
        "https://openalex.org/W6848963243",
        "https://openalex.org/W3211490618",
        "https://openalex.org/W3127751679",
        "https://openalex.org/W4388685310",
        "https://openalex.org/W3183174367"
    ],
    "abstract": "In the context of fast progress in deep learning, convolutional neural networks have been extensively applied to the semantic segmentation of remote sensing images and have achieved significant progress. However, certain limitations exist in capturing global contextual information due to the characteristics of convolutional local properties. Recently, Transformer has become a focus of research in computer vision and has shown great potential in extracting global contextual information, further promoting the development of semantic segmentation tasks. In this article, we use ResNet50 as an encoder, embed the hybrid attention mechanism into Transformer, and propose a Transformer-based decoder. The Channel-Spatial Transformer Block further aggregates features by integrating the local feature maps extracted by the encoder with their associated global dependencies. At the same time, an adaptive approach is employed to reweight the interdependent channel maps to enhance the feature fusion. The global cross-fusion module combines the extracted complementary features to obtain more comprehensive semantic information. Extensive comparative experiments were conducted on the ISPRS Potsdam and Vaihingen datasets, where mIoU reached 78.06&#x0025; and 76.37&#x0025;, respectively. The outcomes of multiple ablation experiments also validate the effectiveness of the proposed method.",
    "full_text": " \n \nHybrid Attention Fusion Embedded in Transformer \nfor Remote Sensing Image Semantic Segmentation \n \n \nYan Chen, Quan Dong*, Xiaofeng Wang, Qianchuan Zhang, Menglei Kang, Wenxiang Jiang, Mengyuan Wang, \nLixiang Xu, Chen Zhang  \n \n  \nAbstractâ€”In the context of fast progress  in deep learning, \nconvolutional neural networks (CNNs) have been extensively \napplied to the semantic segmentation of remote sensing images \nand have achieved significant progress. However, certain \nlimitations exist in capturing global contextual information due \nto the characteristics of convolutional local properties.  Recently, \nTransformer has become a focus of research in computer  vision \nand has shown great potential in extracting global contextual \ninformation, further promoting  the development of semantic \nsegmentation tasks . In this paper, we use ResNet50 as an \nencoder, embed the hybrid attention mechanism into \nTransformer, and propose a Transformer -based decoder.  The \nChannel-Spatial Transformer Block (CSTB)  further aggregates \nfeatures by integrating the local feature maps extracted by the \nencoder with their associated global dependencies. At the same \ntime, an adaptive approach is employed to reweight the \ninterdependent channel maps to enhance the feature fusion. The \nGlobal Cross -Fusion Module (GCFM) combines the extracted \ncomplementary features to obtain more comprehensive semantic \ninformation. Extensive comparative experiments were conducted \non the ISPRS Potsdam and Vaihingen datasets, where mIoU \nreached 78.06% and 76.37% , respectively. The outcomes of \nmultiple ablation experiments also validate the effectiveness of \nthe proposed method. \n \nIndex Terms â€”Hybrid attentio n, global c ross-fusion, remote \nsensing image, semantic segmentation, transformer. \n \nI. INTRODUCTION \nith the continuous development of satellite and \nremote sensing technologies, n umerous high-\nresolution images can be easily acquired  [1]. \nSemantic  segmentation  plays a crucial role in various remote  \n \n \nThis work was funded by the National Natural Science Foundation of \nChina (Grants No. 62176085), the Key Scientific Research Foundation of the \nEducation Department of Province Anhui (Grant No. KJ2020A0658), the \nUniversity Natural Sciences Research Project of Province Anhui(Grant No. \nKJ2021ZD0118), the Hefei University Talent Research Funding (Grant No. \n20RC13), the Hefei University Scientific Research Development Funding \n(Grant No. 20ZR03ZDA), the Program for Scientific Research Innovation \nTeam in Colleges and Universities of Anhui Province(Grant No. \n2022AH010095), and the Hefei Specially Recruited Foreign Expert support.  \n(Corresponding author: Quan Dong).   \nYan Chen, Quan Dong, Xiaofeng Wang, Qianchuan Zhang, Menglei Kang, \nWenxiang Jiang, Mengyuan Wang, Lixiang Xu and Chen Zhang are with the \nSchool of Artificial Intelligence and Big Data, Hefei University, Hefei 230601, \nChina(e-mail: chenyan@hfuu.edu.cn; dq2112774778@163.com; \nxfwang@hfuu.edu.cn; qianczhang@163.com; kangml@stu.hfuu.edu.cn;  \njiangwx@stu.hfuu.edu.cn;wangmy@stu.hfuu.edu.cn;  xulixiang@hfuu.edu.cn;  \nzhangchen@hfuu.edu.cn ) \n \n \n \nsensing applications , including urban construction and  \nplanning, land surveying, environmental monitoring, and \ndisaster assessment, to name a few.  In the classical paradigm \nof geographic object analysis based on image analysis, it is \nexpected first to  use unsupervised segmentation methods to \nsegment the image and then classify the segmented regions  \n[2]. However, semantic s egmentation employs a pixel -level \nsupervised style and assigns each pixel with a pre -designed \nlabel. Although remote sensing images contain a wealth of \ndetailed ground object information, the dist ribution of ground \nobject categories is often imbalanced. Additionally, due to \nvariations in shape, color, texture, and other features, the \nground objects exhibit significant intra -class variance and \nslight inter-class variance in the imaging process. \nConsequently, the semantic segmentation task for remote \nsensing images poses substantial challenges [3].  \nMethods based on hand-crafted feature extraction were the \nprimary approach in the early stages of addressing semantic \nsegmentation on remote ly sensed images [4]. However, as \nremote sensing image resolution continues to improve , \nconventional methods face significant challenges in extracting \nground object features and achieving accurate semantic \nsegmentation [5]. These conventional methods often rely \nheavily on domain knowledge and expertise, requiring manual \nfeature design and the selection of suitable machine learning \nalgorithms.  However, high -resolution remote sensing images \ncontain abundant details and intricate scenes , making it \ndifficult for these methods to perform refined feature \nextraction and segmentation.  In addition, traditional methods \nhave poor adaptability, as each task may require parameter \nadjustments or method redesign, which increases manual \nintervention and development costs and limits the scalability \nand generality of the algorithms [6]. \nDeep learning techniques, particularly convolutional neural \nnetworks (CNNs), have emerged as the dominant approach for \nvarious semantic segmentation tasks in recent years  [7, 8]. \nUnlike conventional methods, deep learning models  do not \nrequire manual feature design. They automatically learn \nfeature representations from data, thereby increasing \nautomation and possessing strong nonlinear modeling \ncapabilities [9]. By employing deep learning architectures \nsuch as CNNs, it becomes possible to capture more fine -\ngrained local contextual information, effectively extracting the \ncomplex advanced features of objects on the ground and \nachieving more accurate semantic segmentation.  Inspired b y \nthe end -to-end fully convolutional neural network (FCN) \nW \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358851\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \nframework [10], many semantic segmentation networks have \nbeen developed.  For example, SegNet [11] employs an \nencoder-decoder architecture for feature extraction and \nupsampling, thereby generating efficient pixel -level \nsegmentation results. DeepLab v3+ [12] introduced the atrous \nspatial pyramid pooling (ASPP) module to obtain spatial \ncontextual feature information. Regular convolutions are \nreplaced by atrous convolutions to expand the receptive field \nand achieve multi -scale feature extraction. PSPNet [13] \nachieves multi -scale receptive fields and fine -grained feature \nfusion through a pyramid pooling structure to enhance the \nunderstanding of semantic information in images. Both \nBiSeNet V1 [14] and V2 [15] adopt a concise and efficient \ndual-path encoder structure, which respectively extracts spatial \ndetail information and high -level semantic information, \nachieving high accuracy and fast speed while enhancing \ndiscriminative ability.  MPSegNet [ 16] incorporates a scale \nguidance module, enabling the subnetwork to focus on \nspecific scale objects with large -scale variations in the image.  \nCGFDN [17] designs a feature decoupling module to encode \nthe co-occurrence relationship into the convolutional features, \nthereby decoupling the most discriminative features. SLA -\nNET [18] proposes a spatial -logic aggregation network based \non morphological transformations, where morphological \noperators effectively embed trainable structural elements to \nform unique morphological representations.  However, \ncompared to natural images, remote sensing images contain \nmore abundant information. The convolutional operations \nused for feature extra ction also introduce significant noise \ninterference. Spectral data provides valuable information for \nscene understanding. SpectralGPT  [19], ExViT  [20], and \nGSANet [21], among others, all integrate complementary \ninformation from different modalities to achieve more \ncomprehensive and accurate results.  Other works specifically \ndesign attention mechanisms targeting specific problems and \nchallenges to effectively alleviate the impact of noise on \nsegmentation [22].  \nSubsequently, attention mechanism s aroused a wave of \nenthusiasm in the field of deep learning and gained \nwidespread application in semantic segmentation tasks. For \nexample, Convolutional Block Attention Module  (CBAM) [23] \ncombines mechanisms of channel attention and spatial \nattention, enabling the network to adaptively adjust the \nperformance of feature maps in the channel dimension and \nspatial dimension , thereby enhancing the model's ability to \nfocus on essential features and suppress noise. MACU-Net [24] \nadopts asymmetric convolutional blocks to enhance the feature \nrepresentation capability, replacing the standard convolutional \nlayers. It also designs multi -scale skip connections combined \nwith channel attention to combine and refine the semantically \ngenerated features at multiple levels. MANet [25] proposes a \nmulti-scale strategy based on kernel and channel attention to \naggregate relevant contextual features at different levels. \nMsanlfNet [26] uses multi -scale attention and fast Fourier \ntransform to obtain fine multi -scale spatial features and global \ncontextual information, effectively balancing performance and \ncomputational complexity . The introduction of these efficient \nattention mechanisms has effectively alleviated the weakness \nof CNN s in handling global dependency relationships, \nresulting in significant improvements in semantic \nsegmentation accuracy. However, there is still room for \nimprovement in modeling global context. For example, as \ndepicted in Fig. 1. (a), the lack of global context information \nin MANet leads to an inaccurate understanding of the \ncomplete shape and boundaries of the target object, resulting \nin erroneous segmentation results. Additionally, as shown in \nFig. 1. (b), trees and low -vegetation areas may be influenced \nby clouds, shadows, occlusions, and other interferences, \ncausing them to exhibit similar texture features. This can lead \nto category confusion when MsanlfNet encounters these \nregions. \n \n Fig. 1.  Examples of predictions on the ISPR S Vaihingen \ndataset are as follows: (a) MANet incorrectly segmented the \ncomplete boundaries and shape of the target object; (b) \nMsanlfNet encountered category confusion and misclassified \nlow-vegetation as trees. \nMany studies have also started to address the challenges \nposed by remote sensing images. For example,  HighDAN [27] \nproposes a high -resolution domain adaptation network,  by \nembedding the adversarial learning -based DAâ€™s idea into HR -\nNet and utilizing the Dice Loss to mitigate the effects of class \nimbalance. MMT [ 28] proposes a mixed -mask attention \nmechanism that assists the network in learning more explicit \nintraclass and interclass correlations by capturing long -range \ninterdependent representations.  The stud y in [2 9] highlights \nthe significant progress and breakthroughs achieved by \nintroducing Transformer methods into semantic segmentation \ntasks. Drawing from the recent vital advances of Transformers \nin computer vision, we propose a novel Transformer \narchitecture to address the aforementioned issues . HAFNet \nprimarily combines a CNN -based encoder with a specially \ncrafted Transformer decoder, forming a hybrid architecture. \nThis design fully leverages the strengths of both CNN and \nTransformer, enabling the network to efficiently extract and \nprocess complex contextual information. The main \ncontributions of this paper are outlined as follows: \n1) A novel hybrid attention-based Transformer architecture, \nCSTB, is intended to aggregate local detail , channel and \nglobal information at different levels to obtain more \ncomprehensive semantic information. \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358851\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \n2) GCFM employs an interactive fusion strategy that \npromotes synergy between different branching features \nto establish effective correlations between features so \nthat the network can capture critical information from \nthe image as a whole. \n3) A novel decoder architec ture constructed by CSTB and \nGCFM, based on the encoder of ResNet50 has been \nproposed for improving representation capacity . \nComparisons between the different methods are carried \nout on the ISPRS Potsdam and Vaihingen datasets, and \nthe experimental results indicate that our proposed \napproach exhibits higher efficiency compared to other \nadvanced approaches. \nThe subsequent sections of this paper are organized in the \nfollowing manner.  Section 2 reviews some related work. \nSection 3 provides a thorough description of the proposed \napproach. Section 4 introduces the datasets and evaluation \nmetrics used in the experiments and presents a detailed \nanalysis of the experimental results. In Section 5, we \nsummarize our work. \n \nII. RELATED WORK \nThis section concisely reviewed the semantic segmentation \nmethods related to our study and analyzed their constraints. \n \nA. Encoderâ€“Decoder-Based Architectures \nThe FCN is a classic end -to-end method for semantic \nsegmentation. It uses convolution and deconvolution \noperations to restore feature maps to the original image  \nresolution, achieving pixel -level classification. However, the \noverly simplified encoder -decoder structure in FCN leads to \ncoarse segmentation results, thereby reducing segmentation \naccuracy. U-Net [30] effectively solves this issue by \nemploying a symmetric structure that connects the encoder \nand decoder, known as the contraction -expansion path. This \narchitecture preserves abundant contextual information and \nhigh-resolution features, leading to more accurate image \nsegmentation. Specifically, the encoder progressively \ndecreases the feature map size and captures semantic features, \nwhile the decoder restores details and spatial detail \ninformation through a combination of upsampling and skip \nconnections, achieving accurate segmentation while \npreserving details.  The encoder -decoder architecture has \nsubsequently achieved excellent performance and wide \napplication in remote sensing image semantic segmentation \ntasks [31]. In related studies [32, 33], various improvements \nhave been made at the decoder stage to extract rich semantic \ninformation. \nWhile the CNN -based encoder -decoder architecture has \nachieved remarkable performance, it faces certain limitations \nregarding the receptive field. If the emphasis is solely placed \non extracting local semantic features, the network may have \ndifficulty effectively capturing the comprehensive image \ninformation, especially in high -resolution remote sensing \nurban scene images with rich features. This limitation can \npose considerable challenges in accurately identifying \ncomplex target objects, leading to erroneous segmentation \nresults. \n \nB. Attention Mechanism \nCombining deep learning and attention mechanisms has seen \nwidespread use in many fields. To address the problem of \nCNN focusing too much on local patterns, numerous attempts \nhave been made to model global information, with the widely \nfavored approach being to introduce attention mechanisms \ninto the network.  For example, Non -local Neural Networks \n[34] introduce non -local modules to capture global \ndependencies by computing similarities between pixels, \novercoming the limitations of conventional convolutional \nneural networks in handling long -range dependencies. DANet \n[35] proposes combining positional attention with channel \nattention to better capture dependencies and contextual \ninformation between diverse pixel positions. OCRNet  [36] \nconstructs soft object regions for each category in advance and \nenhances the feature representation ability of pixels by \nlearning the relationship between pixels and object regions, \nthus fusing local feature information with global contextual \ninformation.  \nWith the development and broad application of numerous \nattention mechanisms further validating their great potential, \nattention mechanisms have also contributed significantly to \nthe progress in the semantic segmentation on remotely sensed \nimages. LSCNet [37] introduces a large kernel sparse \nConvNet weighted by multi -frequency attention (MFA), \nutilizing two parallel rectangular convolutional kernels and \nemploying an adaptive sparse optimization strategy to   \ndynamically optimize the fixed neuron connections between \ndifferent convolutional layers . In [ 38], a spectral attention \nsubnetwork and a spatial attention subnetwork are constructed \nto focus on more discriminative information in the spectral \ndomain and spatial domain, respectively. In [39], a multimodal \nattention-aware convolutional network is proposed, where \ndesigned cascaded blocks facilitate multistage information \nexchange. LANet [40] proposes a local attention embedding \nmethod, which allows the network to focus on  leveraging the \nconnection between local detail features and global features to \ncapture comprehensive contextual information . AFNet [41] \ndesigns an attention fusion network that allows the network to \nretain more detail ed information while adaptively performing \nmulti-scale feature fusion to capture more integrated semantic \ninformation. CANet [42] leverages the techniques of multi -\nscale residual concatenation and spatial pyramid pooling to \naggregate rich contextual information at different levels. \nDespite these advantages, the convolution operation based on \nlimited receptive fields primarily focuses on extracting local \nfeature information. At the same time, these attention modules \nheavily rely on convolutional operations, making it \nchallenging to efficiently extract global context dependencies \nand acquire long-range dependencies. As a result, it can easily \nlead to ambiguity in classifying certain pixels in remote \nsensing images [43, 44]. Finally, if only a single attention \nmodule is used at the decoder, the network lacks the ability to  \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358851\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \nFig. 2. Schematic of the HAFNet \nglobally model multi-level semantic features. \n \nC. Transformer-based Semantic Segmentation Methods \nWith its excellent sequence -to-sequence modeling  \ncapability, the Transformer model  has shown outstanding \nperformance in extracting global contextual information \ncompared to the models mentioned above  that only use the \nregular attention mechanism. Subsequently, many researchers \nbegan to explore applying the Transformer model to the \nsemantic segmentation task of remote ly sensed images [45]. \nThe main advantage of the Transformer model in dealing with \nfine high-resolution remote sensing imager of urban scenes is \nits capacity to efficiently establish long -range dependency \nrelationships, thereby improving the accuracy and robustness \nof semantic segmentation.  \nCurrently, most Transformer models used for semantic \nsegmentation still adopt an encoder -decoder structure. Based \non different combinations of encoders and decoders, \nTransformer models for semantic segmentation can be \nclassified into two types. The first type is models that fully  \nutilize the Transformer structure. For example, SegFormer  \n[46], SwinUnet  [47], and Segmenter [48] leverage the \nadvantages of Transformers to better consider global semantic \ninformation, leading to significant improvements in  \nsegmentation performance and efficiency.  The second type \ninvolves using the Transformer in the encoder while \nemploying CNN in the decoder. For example, GMHF [ 49] \nproposes a bi -branch global + multiscale hybrid network that \nextracts both multi -scale global and local features.  These \nfeatures are further integrated through correlation modules to \nprovide the network with more comprehensive semantic \ninformation. In DC -Swin [50], the Swin Transformer [51] is \nused in the encoder to en hance the network's ability to handle \nlong-range dependencies.  Conversely, the decoder  \nincorporates a densely  connected feature aggregation module \nbased on convolution operations. In the research by [52], \nSwinTF-U-Net, SwinTF -PSP, and SwinTF -FPN also utilize \nthe Swin Transformer as the encoder, while the decoder is \nbased on multi -scale CNN architectures such as U -Net, PSP, \nand FPN. Although the second type of Transformer combined \nwith CNN has achieved impressive results in remote sensing \nimage semantic segmentation tasks, its computational cost is \nmuch higher than that of CNN -based encoder [53]. Therefore, \nthe proposed network architecture in this paper adopts CNN as \nthe encoder and utilizes the Transformer in the decoder part. \n \nIII. METHODOLOGY \nThis section presents the structure of HAFNet and provides an \noverview of the model's framework, as depicted in Fig . 2. The \nconstruction of the model involves the utilization of a CNN-based \nencoder and a Transformer -based decoder. A comprehensive \ndescription of every component follows. \n \nA. CNN-based Encoder \nThe feasibility of ResNet50 in remote sensing image \nsemantic segmentation tasks has been verified in previous \nstudies [25, 26]. We adopt a pre -trained ResNet50 as the \nencoder, which cons ists of four residual block layers. Each \nresidual block layer performs a down -sampling of the feature \nmaps by a factor of 2. The deep structure enhances the \nnetwork's generalization ability and representation power. \nMulti-scale skip connections ensure that our model efficiently \nhandles both fine -grained and coarse -grained semantic \ninformation in the images. To extract multi -level semantic \nfeatures while maintaining computational efficiency, we apply \nchannel compression to the end of the backbone network, \nreducing the final output channels to 256 and ensuring \nconsistency throughout the decoder with 256 channels. This \ndesign aims to balance performance and computational cost. \n \nB. Transformer-based Decoder \nThe mainstream methods for capturing global contextual \ninformation can be divided into two types. The first approach \nis to add attention modules at the end of the encoder [ 34]. \nHowever, this design may make it difficult for the network to \ncapture multi -scale global semantic features. The second \napproach is to directly use a Transformer model in the encoder \n[54]. However, this not only increases the computational \nburden and number of parameters but may also result in the \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358851\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \n \nFig. 3. Schematic of the Channel Adaptive Module \n \nloss of spatial detail feature information. In contrast, HAFNet \nintroduces the CSTB and GCFM in our decoder. These  \ncomponents work together to extract global context correlation \nwithout sacrificing spatial details.  Different from the \ntraditional multi -head self -attention in the regular \nTransformer, the proposed CSTB mainly includes a channel \nattention module and a spatial attention module to optimize \nthe network's utilization of channel information and capture \nglobal contextual information. Additionally, the residual \nbranch in the CSTB effectively aggregates local details, \nchannels, and global contextual information by leveraging the \nlocal feature information extracted by ResNet50, which helps \nthe network capture more comprehensive semantic \ninformation. \n \nC. Channel Adaptive Module (CAM) \nChannel attention is to learn channel weights to weigh \ndifferent channels of the input feature map, which enables the \nmodel to  dynamically select and adjust the importance of \nchannels. CAM adopts a two -branch structure compared to \nprevious classical channel attention modules [55, 56]. \nSpecifically, as shown in Fi g. 3., for an input feature map of \nsize ğ‘‹ âˆˆ RCxHxW , firstly, maximum pooling and average \npooling operations are performed on it to aggregate the global \nsemantic information of the feature map, resulting in two  \nchannel maps with the same dimensions, i.e., the maximum \npooling feature ğ‘‹ğ‘€ğ‘ âˆˆ RCx1x1 and the average pooling feature \nğ‘‹ğ´ğ‘ âˆˆ RCx1x1 . In the subsequent Bottleneck structure, the \nchannel dimension is reduced by a 4x factor through \nconvolution. Next, LayerNorm is introduced to enhance the \nmodel's robustness. Subsequently, the channel dimension is \nrestored to its original value through convolution. Finally, we \nuse the Sigmoid activation function to acquire weight \ncoefficients and multiply the input feature map with these two \nchannel weights to enhance the effective features. The two \ncorresponding results are then fused together using a \nsummation operation. The relevant formulas are as follows: \nğ‘€ğ¶(ğ‘‹) = ğœ(ğ‘Š1ğ‘¥1(ğ¿ğ‘(ğ‘Š1ğ‘¥1(ğ´ğ‘£ğ‘”ğ‘ƒğ‘œğ‘œğ‘™(ğ‘¥))))) âˆ™ ğ‘‹\n+  ğœ(ğ‘Š1ğ‘¥1(ğ¿ğ‘(ğ‘Š1ğ‘¥1(ğ‘€ğ‘ğ‘¥ğ‘ƒğ‘œğ‘œğ‘™(ğ‘¥))))) âˆ™ ğ‘‹    \n= ğœ(ğ‘Š1ğ‘¥1(ğ¿ğ‘(ğ‘Š1ğ‘¥1(ğ‘‹ğ‘€ğ‘)))))       \n+  ğœ(ğ‘Š1ğ‘¥1(ğ¿ğ‘(ğ‘Š1ğ‘¥1(ğ‘‹ğ´ğ‘)))                        (1) \nwhere ğ´ğ‘£ğ‘”ğ‘ƒğ‘œğ‘œğ‘™  represents the average pooling operation, \nğ‘€ğ‘ğ‘¥ğ‘ƒğ‘œğ‘œğ‘™ denotes the maximum pooling operation, ğ¿ğ‘  denotes \nthe Layernorm operation, ğœ represents the Sigmoid activation \nfunction, and ğ‘Š1ğ‘¥1 represents the 1x1 convolution operation.  \n \nD. Spatial Context Module (SCM) \nSCM can focus the attention on the global scope of the \nwhole image or feature map, enhancing the model's ability to \nperceive global features, achieving a more comprehensive and \nintegrated grasp of the overall information of the image, and \nbetter interpretingthe relationship between different regions.  \nAs shown in Fig. 4., the primary emphasis lies in merging two \ndistinct branches . Firstly, in the upper part, the input feature \nmap ğ‘‹ âˆˆ RCxHxW  is subjected to average pooling and max \npooling operations along the channel dimension separately. \nThe two results are then concatenated, and a convolutional \noperation is performed to generate the spatial feature map (ğœ–). \nFinally, by applying the Sigmoid activation function, the \nresulting map is multiplied with the input feature map to \nacquire the spatial attention map  ğ¸0(ğ‘¥) , capturing the \ndependency relationships between different positions. The \nformula is as follows: \n \n        ğ¸0(ğ‘‹) = ğœ(ğ‘Š1ğ‘¥1[ğ‘€ğ´ğ‘ƒ; ğ´ğ‘‰ğ‘ƒ])  âˆ™  ğ‘‹ \n= ğœ(ğœ–)  âˆ™  ğ‘‹                                         (2) \nwhere ğ‘€ğ´ğ‘ƒ  represents max pooling across the channel \ndimension, while  ğ´ğ‘‰ğ‘ƒ represents average pooling along the \nchannel dimension.  In the following part , the spatial feature \nmap (ğœ–) is dimensionally transformed, and the Softmax \nactivation function is applied to the dimensionally adjusted \nresult (i.e., H*W dimension) . T he dimension is added to the \noutput to obtain ğ‘‹ğ‘† âˆˆ R1xHWx1. Furthermore, the input feature \nmap is reshaped to obtain ğ‘‹ğ‘… âˆˆ RCxHW . Subsequently, the \nresult of the unsqueeze transformation, denoted as ğ‘‹ğ‘ˆ âˆˆ\nR1xCxHW, undergoes matrix multiplication with ğ‘‹ğ‘† âˆˆ R1xHWx1. \nFinally, the channel map  ğ¸1(ğ‘¥) with dimensions of Cx1x1 is \nobtained through the reshape operation.  This part is the \ndistillation of spatial information for global perceptual \nconvergence. The specific steps are formulated as follows. \nğ¸1(ğ‘‹) =  ğ‘…(ğ‘ˆğ‘›(ğ‘†ğ‘‘2(ğ‘…(ğœ–))) âˆ™  ğ‘ˆğ‘›(ğ‘…(ğ‘‹))) \n=  ğ‘…(ğ‘‹ğ‘† âˆ™  ğ‘ˆğ‘›(ğ‘‹ğ‘…))           \n=  ğ‘…(ğ‘‹ğ‘† âˆ™ ğ‘‹ğ‘ˆ)                                           (3) \nwhere ğ‘…  denotes the Reshape operation, ğ‘†ğ‘‘2  denotes the \nSoftmax activation function acting on the second dimension \nand ğ‘ˆğ‘› signifies the unsqueeze operation that expands on the \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358851\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \n \nFig. 4. Schematic of the Spatial Context Module. \n \ndata dimensions. The subsequent Bottleneck structure is used  \nto enhance the inter -channel dependencies. Firstly, the  \nchannels are compressed to C/r by a convolutional layer, \nwhere r is the reduction factor. Additionally, LayerNorm is \ninserted in the middle, allowing the normalization operation to \nbe performed before the non -linear activation function GELU. \nThis facilitates the preservation of the data's dynamic range \nand improve the model's generalization ability.  After that, the \nresult ğ¸ğ¶ âˆˆ RCx1x1 , which aggregates the global feature \ninformation, is superimposed on the spatial feature ğ¸ğ‘œ(ğ‘¥) . \nFinally, the fused result is summed with the input feature map \nto obtain ğ¸(ğ‘‹), as shown below: \n      ğ¸(ğ‘‹) = ğ‘Š3ğ‘¥3(ğ›¿(ğ¿ğ‘(ğ‘Š3ğ‘¥3(ğ¸1(ğ‘‹))))) + ğ¸0(ğ‘‹) + ğ‘‹ \n= ğ¸ğ¶ + ğ¸0(ğ‘‹) + ğ‘‹                                             (4) \nwhere ğ‘Š3ğ‘¥3  represents the 3x3 convolution operation, and ğ›¿ \ndenotes the GELU activation function. \n \nE. Redesigned MLP Layer in Transformer \n Although the FFN layer in the previous Transformer Block \nhas strong non-linear modeling ability, its local perceptiveness \nis inadequate  as it only focuses on the feature information of  \nthe current position. Therefore, further improvements have \nbeen made to the FFN in the Transformer Block. As shown in \nFig. 5., the improved FFN layer first utilizes a 7x7 depth -wise \nconvolution to enhance the model's local perceptiveness. \nNext, in the subsequent fully connected layer, we introduce \nthe simple and efficient Global Response Normalization (GRN) \nfrom ConvNext v2  [57]. It aggregates global features, \nnormalizes features, and calibrates features , thereby helping \nthe network suppress feature collapse and enhance channel \ncontrast and selectivity. The three specific steps of GRN are as \nfollows. First, the spatial feature s ğ‘‹ğ‘–  are aggregated into a \nvector ğ‘”ğ‘¥ using the global function ğ’¢(âˆ™): \nğ’¢(ğ‘‹): = ğ‘‹ âˆˆ ğ‘…ğ»ğ‘¥ğ‘Šğ‘¥ğ¶ â†’ ğ‘”ğ‘¥ âˆˆ ğ‘…ğ¶                 (5) \nThis can be seen as a n elementary pooling layer.  Next, we \napply the response normalization function ğ’©(âˆ™)  to the \naggregated value. Specifically, we use the following standard \ndivision normalization.  \nğ’©(âˆ¥ ğ‘‹ğ‘– âˆ¥): =âˆ¥ ğ‘‹ğ‘– âˆ¥âˆˆ ğ‘… â†’\nâˆ¥ğ‘‹ğ‘–âˆ¥\nâˆ‘ğ‘—=1,....ğ¶âˆ¥ğ‘‹ğ‘—âˆ¥ âˆˆ ğ‘…        (6) \nwhere âˆ¥ ğ‘‹ğ‘– âˆ¥  represents the L2 -norm of the i -th channel. \nIntuitively, Equation 6 quantifies the importance of the i -th \nchannel relative to all other channels in the entire feature map.  \nSimilar to other normalization methods [58], this step aims to \ninduce competition and mutual inhibition among different \nchannels, thereby enhancing the expressive power of features. \nUltimately, the original input response is adjusted using the \ncomputed feature normalization scores, as formulated below:  \nğ‘‹ğ‘– = ğ‘‹ğ‘– âˆ— ğ’©(ğ’¢(ğ‘‹)ğ‘–)  âˆˆ ğ‘…ğ»ğ‘¥ğ‘Š                  (7) \n \n \nFig. 5. Schematic of the Channel-Spatial Transformer Block. \n \nF. Global Cross-Fusion Module  \nIn the decoder, conventional feature fusion methods typically \ninvolve using fixed interpolation or convolution operations to \nupsamplethe image and then directly fusing it with the features \nobtained from skip connections. However, this often leads to \nloss of detailed information and blurriness. Thus, we propose a \nGlobal Cross -Fusion Module (GCFM) to obtain more \ncomprehensive and global semantic information, enabling more \naccurate restoration of image details.  As shown in Fig . 6., the \ninformation interaction fusion of different branches \ncompensates for the information loss caused by the reduction in \nthe number of channels and  enhances the spatial and semantic \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358851\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \n \nFig. 6. Schematic of the Global Cross-Fusion Module. \n \ninformation. Secondly, a global average pooling layer is utilized \nto generate the attention map ğ‘‹ğ¶ âˆˆ RCx1x1. Then, the channel \ndimension C is reduced by four times, expanded to the \noriginalsize, and multiplied with the low-level features to obtain \nthe attention feature map ğ‘Œğ¶ âˆˆ RCxHxW . This suppresses \nunnecessary noise and selectively focuses on essential parts of \nthe image. Finally, the upsampling operation is performed on \nthe high-level features, and the obtained result is fused with the \nattention feature map using summation. The formula is as \nfollows: \nğº(ğ‘‹) = ğ¶ğµğ‘…(ğ¶ğ‘Ÿğµğ‘…(ğ´ğ‘£ğ‘”ğ‘ƒğ‘œğ‘œğ‘™(ğ‘‹â¨€ğ‘Œ))) âˆ™ ğ¶ğµğ‘…(ğ‘Œâ¨€ğ‘‹) \n+ğ‘ˆğ‘(ğ‘‹) \n    =  ğ¶ğµğ‘…(ğ¶ğ‘Ÿğµğ‘…(ğ‘‹ğ¶ )) âˆ™ ğ¶ğµğ‘…(ğ‘Œâ¨€ğ‘‹) + ğ‘ˆğ‘(ğ‘‹) \n=  ğ‘Œğ¶ + ğ‘ˆğ‘(ğ‘‹)                                                        (8) \nwhere X represents the deep semantic features, Y represents the \nshallow semantic features, Xâ¨€Y denotes the feature fusion by \nsummation of Y after upsampling and dimension adjustment \nwith X, and Yâ¨€X  follows the same logic. CBR represents \nConvolution, Batch Normalization, and ReLU, while Up \nrepresents bilinear interpolation upsampling. \n \nIV. EXPERIMENTAL RESULTS  AND ANALYSIS \nIn this section, we will outline the datasets used, the \nevaluation metrics employed, and provide details of the \nconducted experiments. We will analyze and discuss the \nexperimental results on both datasets. \n \nA. Dataset Description \n1) Potsdam :  \nThis dataset, provided by ISPRS, and was collected from \naerial imagery of the German city of Potsdam, with a total of 38 \nhigh-resolution images. Each image has an average size of \n6000 Ã— 6000 pixels and a ground sampling distance of 5 cm. \nThis dataset includes six categories: Impervious surfaces, \nbuildings, low vegetation, trees, cars, and clutter /background. \nSpecifically, we use images with IDs 2_11, 2_12, 3_10, 3_11, \n3_12, 4_10, 4_11, 4_12, 5_10, 5_11, 5_12, 6_7, 6_8, 6_9, 6_10, \n6_11, 6_12, 7_7, 7_8, 7_9, 7_10, 7_11, and 7_12 for training. \nWe use the images with IDs 2_10 for validation and the rest for \ntesting. In order to reduce the amount of computation, a digital \nsurface model (DSM)  and normalized digital surface model \n(NDSM) are not used in our experiments.  Additionally, red, \ngreen, and blue bands are used in our experiments. We also crop \nthe original images into patches of 512 Ã— 512 size and perform \ndata enhancement by randomly rotating, scaling the size, \nvertically flipping, horizontally flipping, and adding random \nGaussian noise. \n2) Vaihingen :  \nThis dataset, provided by ISPRS, was collected from aerial \nimagery of the German city of Vaihingen and consists of 33 \nhigh-resolution images. Each image has an average size of \n2994 Ã—  2064 pixels and a ground sampling distance of 5 cm.  \nThe ground reality comprises six categories identical to those in \nthe ISPRS Potsdam benchmark.  The training set includes 16 \nimages, while the test set consists of 17 images. For training, we \nuse images with IDs 1, 3, 5, 7, 11, 13, 15, 17, 21, 23, 26, 28, 32, \n34, and 37. Image with ID 30 was reserved for validation, while \nthe rest 17 images were used for testing. The dataset is \nprocessed in the same manner as Potsdam. \n \nB. Evaluation Metrics \nThe experimental results are evaluated based on three \ncommonly used metrics: overall accuracy (OA), mean F1 score \n(mF1), and mean intersection over Union (mIoU). Their \ncalculations are as follows:  \n \nğ‘‚ğ´ =  \nâˆ‘ğ‘–=0\nğ¶ ğ¾ğ‘–ğ‘–\nâˆ‘ğ‘–=0\nğ¶ âˆ‘ğ‘—=0\nğ¶ ğ¾ğ‘–ğ‘—\n                               (9) \nğ¹1 =\n1\nğ¶+1 âˆ‘ğ‘–=0\nğ¶ ğ¾ğ‘–ğ‘–\nğ¾ğ‘–ğ‘–+1\n2 âˆ‘ (ğ¾ğ‘–ğ‘—+ğ¾ğ‘—ğ‘–)ğ¶\nğ‘—=0\n                (10) \nğ‘šğ¼ğ‘œğ‘¢ =\n1\nğ¶+1 âˆ‘ ğ¾ğ‘–ğ‘–\nâˆ‘ ğ¾ğ‘–ğ‘—ğ¶\nğ‘—=0 +âˆ‘ (ğ¾ğ‘—ğ‘–âˆ’ğ¾ğ‘–ğ‘–)ğ¶\nğ‘—=0\nğ¶\nğ‘–=0             (11) \nwhere ğ¾ğ‘–ğ‘– expresses the number of correctly classified pixels for \nclass i. ğ¾ğ‘–ğ‘—  expresses the number of pixels incorrectly classified \nas class j. ğ¾ğ‘—ğ‘–  denotes the number of pixels from class j that are \nincorrectly classified as class i. C characterizes the total number \nof categories. \n \nC. Implementation Details \nAll experiments were conducted using PyTorch on a single \nserver equipped with an NVIDIA GeForce RTX 3090 GPU, \nwhich has 24 GB of memory. In all experiments, we utilized the \nAdamW optimizer to accelerate convergence. The baseline \nlearning rate was set to 6e -4, and a cosine strategy was \nemployed to update the learning rate. The batch size was set to \n8, and the maximum number of epochs for training was set to \n105. Moreover, during the training process, we combined the \nsoft cross entropy loss and dice loss using a weighted sum to \nform the final joint loss function. Lastly, all our experimental \nresults are individual results, and the experimental environment \nis PyTorch 2.0.1 and CUDA 11.8. \n \nD. Ablation Experiments \nIn order to comprehensively evaluate the performance of  \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358851\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \n \nFig. 7. Visualization of ablation experiments on two datasets.  \n \nTABLE I \nABLATION STUDY OF EACH COMPONENT OF THE \nHAFNET.THE BEST VALUES IN THE COLUMN ARE \nEMPHASIZED IN BOLD. \n \nDataset Method OA  mF1 mIoU \nPotsdam \n \nBaseline \nBaseline + GCFM \nBaseline + CAM \nBaseline + SAM \nBaseline + CS \nBaseline + CSTB \nBaseline + CSTB + \nGCFM \n \n \n89.62 \n89.73 \n90.01 \n89.70 \n89.81 \n90.44 \n90.45 \n \n84.86 \n85.59 \n85.44 \n85.47 \n85.89 \n86.02 \n86.47 \n \n76.19 \n76.88 \n76.99 \n76.75 \n77.16 \n77.66 \n78.06 \nVaihingen \n \nBaseline \nBaseline + GCFM \nBaseline + CAM \nBaseline + SAM \nBaseline + CS \nBaseline + CSTB \nBaseline + CSTB + \nGCFM \n \n \n89.89 \n90.04 \n90.22 \n89.88 \n90.08 \n90.14 \n90.29 \n \n84.13 \n84.70 \n84.50 \n84.85 \n85.01 \n85.23 \n85.93 \n \n74.25 \n74.84 \n74.77 \n75.00 \n75.18 \n75.45 \n76.37 \n \ndifferent modules, Table I lists the conducted ablation  \nexperiments on two datasets under different configurations . \nThe baseline model uses ResNet50 as the backbone network \nand models only local contextual information at the decoder. \nBaseline + GCFM represents the use of only the global cross \nfusion module, while Baseline + CSTB represents the use of \nonly the Channel-Spatial Transformer Block. To demonstrate \nthe contribution of the improved MLP in CSTB, we also \nconstruct a simple variant Baseline + CS by removing MLP \nand using the attention mechanism only. \n1) Baseline +  GCFM: At the decoder, the interactive \nfusion of features from different branches can be \naccomplished by using GCFM to establish the correlation \nbetween the features and enable the network to better extract \nthe global contextual information. mIoU is improved by \n0.69% and 0.59% on the Potsdam and Vaihingen datasets, \nrespectively, and this improvement proves the effectiveness  \nof the GCFM module. \n2) Baseline + CAM:  CAM can learn channel weights to \nweight different channels of the input feature map, enabling \nthe model to dynamically select and adjust the importance of \nchannels. This leads to significant improvements over the \nBaseline on two datasets. \n3) Baseline + SAM:  SAM models the relationships \nbetween different positions to enhance the model's perception \nability and better understand the semantic information in the \nimage. It improves mIoU by at least 0.56% on two datasets. \n4) Baseline + C S: By effectively aggregating \ncomplementary features using the channel -spatial attention \nmodule, the network gains more discriminative features. This \nleads to significant improvements of 1.03% in mFl and \n0.97% in mIoU on the Potsdam dataset, showcasing the \neffectiveness of CS. \n5) Baseline + CSTB: CSTB is applied at different \npositions in the encoder. It fuses local details, channel s, and \nglobal information, enabling the network to capture more \ncomprehensive semantic information. On both datasets, the \nperformance of mFl and mIoU has improved by at least 1.1% \nand 1.47%, respectively. Furthermore, the mIoU of \"Baseline \n+ CSTB\" is higher than that of \"Baseline + CS\" by \n0.5%/0.27%, highlighting the necessity of CSTB. \n6) Baseline + CSTB + GCFM: This efficient fusion \napproach utilizes three CSTBs and three GCFMs, enabling \nthe network to understand and analyze images from different \nperspectives. It significantly suppresses the impact of \nredundant feature information and achieves the highest \naccuracy on both datasets. This also further validates the \nfeasibility of our designed decoder. \nIn summary, the experimental results not only show the \neffectiveness of GCFM but also demonstrate the the essential \nrole of CGTB. Finally, our method not only achieves the best \nvalues on the three metrics but also brings an improvement of \nat least 1.87% compared to the baseline mIoU on the two \npublic datasets. In addition, we visualize the results of the \nablation experiments, as shown in Fig. 7. The segmentation\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358851\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \nTABLE II \nTHE RESULTS FROM EXPERIMENTS CONDUCTED ON THE POTSDAM TEST SET.  \n \nMethod \nPer-class IoU (%) Evaluate Metrics (%) \nImp.surf Building Low.veg. Tree Car Clutter mF1 OA mIoU \nPSPNet 82.77 90.01 74.45 76.97 90.48 31.69 83.46 88.65 74.39 \nBiSeNeV1 82.95 89.63 73.92 74.36 88.22 35.45 83.60 88.25 74.09 \nBiSeNeV2 80.91 87.18 71.95 73.16 81.94 34.66 82.05 86.95 71.63 \nMAResU-Net 84.34 91.76 75.40 76.73 90.70 40.90 85.53 89.54 76.64 \nMsanlfNet 83.98 90.24 74.02 74.98 88.70 37.57 84.26 88.69 74.92 \nABCNet 84.78 92.02 76.52 78.49 91.70 36.39 85.21 89.92 76.65 \nMANet 84.49 91.48 76.36 78.12 91.03 41.17 85.85 89.81 77.11 \nBANet 83.35 89.14 73.55 74.56 87.99 33.88 83.26 88.17 73.74 \nPVT 83.35 91.36 73.33 75.03 81.41 37.24 85.79 88.91 77.32 \nUnetFormer 84.81 91.40 74.34 77.54 91.77 36.51 84.85 89.38 76.06 \nCMTFNet 85.63 92.65 76.18 78.36 91.60 40.52 86.01 90.17 77.49 \nBuildFormer 83.04 89.02 72.84 74.8 88.87 37.56 83.84 88.00 74.23 \nMCCANet 85.84 92.51 76.60 79.05 92.16 39.01 85.93 90.31 77.53 \nGCDNet 85.67 92.40 75.65 78.22 91.26 38.32 85.51 89.99 76.92 \nOurs 85.94 92.54 76.89 79.32 91.58 42.12 86.47 90.45 78.06 \n \nperformance on both datasets gradually improves. \nE. Comparing with Existing Works \nWe have also conducted extensive experiments on ISPRS \nPotsdam and Vaihingen datasets. Also, to ensure a fair \ncomparison, all experiments were performed under the same \ntraining and testing setup. We compare our method with \nPSPNet [13], BiSeNet V1 [14], BiSeNet V2 [15], MAResU-Net \n[59], MsanlfNet [26], MANet [25], ABCNet [60], BANet [45], \nUnetFormer [61], PVT [62], CMTFNet [63], BuildFormer [64], \nMCCANet [65] and GCDNet [ 66]. As shown in Table II, the \nproposed HAFNet achieves the best F1, OA and mIoU metrics \non the Potsdam dataset, significantly outperforming other CNN \nand Transformer based networks.  MCCANet captures channel \nattention at various scales through multiscale channelwise cross \nattention, allowing dynamic and adaptive feature fusion in a \ncontext-scale aware manner, thus focusing on both large and \nsmall objects distributed throughout the input.  It achieves the \nbest results on tasks involving small objects such as cars. \nCMTFNet combines the advantages of CNN and Transformer \nby using attention to learn multi -scale feature representations \nand efficiently aggregate deep and shallow features, achieving \nthe best results on the category of buildings. It is worth noting \nthat our method's IoU on car an d building categories is slightly \nlower than the best value by around 0.35%, while the remaining \nfour categories achieve the best results.  \nTo further illustrate the performance of our proposed \napproach, we conducted the same comparative experiments on \nthe Vaihingen dataset. In Table III, our HAFNet achieved a \nmean F1 score of 85. 93% and an mIoU of 76.37% on the \nVaihingen dataset. It is worth noting that because of the unequal \nclass distribution in the Vaihingen dataset, the Clutter class \noften poses significant challenges during prediction. However, \nour method performs exceptionally well in the Clutter class, \nwith an IoU surpassing other networks by more than 4.63%. \nAdditionally, especially in the categories of Low Vegetation \nand Trees, which have complex features, our approach yields  \nthe best results on both datasets. This further proves that \nHAFNet can alleviate interference and error propagation among \ncategories, thereby improving the network's response capability \nto differences between samples.  Furthermore, we also \nconducted a simple performance evaluation of HAFNet, as \ndepicted in Fig.  8. The circles in the figure d enote FPS, and \nthe larger the circle, the higher the FPS value . We also provide \nthe modelâ€™s parameter quantity and computational complexity \nin Table IV. Based on the comprehensive analysis of the \npresented figures and table, although our model has achieved a \nfavorable balance between performance and efficiency, \nBiSeNetV2 is a highly efficient network for lightweight real -\ntime semantic segmentation. Although its precision is not as  \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358851\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \nTABLE III \nTHE RESULTS FROM EXPERIMENTS CONDUCTED ON THE VAIHINGEN TEST SET. \n  \nMethod \nPer-class IoU (%) Evaluate Metrics (%) \nImp.surf Building Low.veg. Tree Car Clutter mF1 OA mIoU \nPSPNet 84.76 89.25 70.56 80.13 72.30 44.95 83.95 87.88 73.66 \nBiSeNetV1 83.94 87.75 68.73 78.96 69.60 42.09 82.63 88.77 71.84 \nBiSeNetV2 81.14 85.56 66.65 78.10 60.27 39.90 80.29 87.52 68.60 \nMAResU-Net 84.92 90.26 70.31 79.84 79.58 39.67 83.92 89.75 74.10 \nMsanlfNet 84.20 88.85 68.12 78.89 71.31 38.02 82.19 88.93 71.57 \nABCNet 84.86 89.72 69.14 79.85 76.80 45.81 84.44 89.49 74.37 \nMANet 85.37 90.50 70.18 80.12 80.23 41.49 84.37 89.90 74.65 \nBANet 82.82 86.61 67.42 78.19 65.67 39.36 81.25 88.09 70.01 \nPVT 80.77 85.31 67.71 75.77 57.12 36.60 82.05 88.87 71.79 \nUnetFormer 84.71 89.22 69.59 80.22 75.44 42.90 83.86 89.49 73.68 \nCMTFNet 85.89 90.91 71.44 80.29 79.66 42.84 84.78 90.23 75.17 \nBuildFormer 83.44 87.83 68.50 79.15 72.99 41.54 82.87 88.75 72.24 \nMCCANet 85.98 90.95 71.07 80.79 80.05 44.96 85.19 90.31 75.63 \nGCDNet 85.39 90.40 70.24 80.11 79.24 44.48 84.76 89.87 74.98 \nOurs 85.35 90.96 71.57 80.88 79.01 50.44 85.93 90.29 76.37 \n \n \n \nFig. 8. Visualization of network performance evaluation. \n \nhigh as our proposed method, it exhibits significant \nadvantages in terms of Params, FPS, and Flops. This is \nprecisely where the limitation of our method lies. \nIn order to verify the effectiveness of the proposed channel -\nspatial attention, we conducted ablation experiments by \nreplacing it with  several other cutting -edge attention \nmechanisms while keeping other modules unchanged. These  \ninclude Concurrent Spatial and Channel Squeeze  & Excitation \n(SCSE) [67], Convolutional Block Attention Module (CBAM) \n[23], Dual Attention (DA) [35], Kernel Attention Mechanism & \nChannel Attention Mechanism (KAM&CAM) [ 25], Efficient \nglobal-local attention (EGLA) [61], multi-scale multihead self-\nattention (M2SA) [ 63], and squeeze -enhanced axial attention \n(SEAA) [68]. Due to the efficient coupling of the channel -\nspatial attention module in HAFNet, which enhances the \nnetwork's understanding of images, our method consistently \nachieves the best mean F1 score and mIoU on the Vaihingen \ndataset, as shown in Table V. Although our channel -spatial \nattention did not reach the highest IoU v alues in the four \ncategories, it was only lower by about 0.38%  from the best \nvalue. Significantly, our method still outperforms other attention \nmechanisms by over 1.67% in the  Clutter category, which \nfurther demonstrates the strong learning ability of our approach, \nas well as its superiority and robustness in handling tasks with \nfew samples. This also provides valuable references and \ninspiration for applying our approach to other fields. \n \nF. Qualitative Analysis of the Segmentation Results \nAs shown in Fig. 9. and Fig. 10., We have provided \nvisualization results on two public remote sensing datasets,   \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358851\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \nTABLE IV \nCOMPARISON OF COMPLEXITIES BETWEEN OUR METHOD AND OTHER METHODS. \n \nMethod mF1(%) mIoU(%) Params(M) Flops(G) \nPSPNet 83.95 73.66 53.32 201.59 \nBiSeNetV1 82.63 71.84 23.11 40.90 \nBiSeNetV2 80.29 68.60 5.10 11.19 \nMAResU-Net 83.92 74.10 26.77 35.11 \nMsanlfNet 82.19 71.57 32.24 27.81 \nABCNet 84.44 74.37 11.68 15.63 \nMANet 84.37 74.65 35.86 77.76 \nBANet 81.25 70.01 28.58 58.10 \nPVT 82.05 71.79 25.52 - \nUnetFormer 83.86 73.68 11.68 11.74 \nCMTFNet 84.78 75.17 30.68 33.62 \nBuildFormer 82.87 72.24 48.11 126.76 \nMCCANet 85.19 75.63 42.38 102.53 \nGCDNet 84.76 74.98 60.56 281.13 \nOurs 85.93 76.37 38.51 114.64 \n \n \nTABLE V \nABLATION STUDIES OF DIVERSE ATTENTION MECHANISMS ON THE VAIHINGEN TEST SET. \n \nAttention \nMechanism \nPer-class IoU (%) Evaluate Metrics (%) \nImp.surf Building Low.veg. Tree Car Clutter mF1 OA mIoU \n+SCSE 85.18 90.26 71.02 80.43 76.51 45.55 84.73 89.93 74.83 \n+CBAM 85.30 90.49 70.68 81.11 77.95 46.57 85.10 90.10 75.35 \n+DA 85.82 90.82 71.71 81.27 78.99 43.98 85.02 90.39 75.43 \n+KAM&CAM 85.72 90.94 71.31 81.08 78.91 45.48 85.18 90.31 75.57 \n+EGLA 85.65 90.59 71.38 80.77 79.54 45.30 85.16 90.22 75.54 \n+M2SA 85.33 90.84 71.13 81.05 77.93 48.77 85.52 90.23 75.84 \n+SEAA 85.59 90.67 71.27 81.15 78.69 44.95 85.04 90.24 75.39 \nOurs 85.35 90.96 71.57 80.88 79.01 50.44 85.93 90.29 76.37 \n \ndenoted as a -n in the following order: PSPNet, BiSeNetV1,  \nBiSeNetV1, MAResU -Net, MsanlfNet, ABCNet,  MANet , \nBANet, UnetFormer, CMTFNet, BuildFormer, MCCANet, \nGCDNet, HAFNet.  Firstly, the segmentation results on the \nPotsdam test set are shown in Fig. 9., where CSTB leverages the \ncontextual information and interdependencies within feature \nmaps from different layers to support the network obtain more \ncomprehensive semantic information, which enables the \nsegmentation results to better preserve the geometric details and \ncomplex contours. For regular round and regular shaped objects, \nthe segmentation results from our method are clearer and have \nsmoother edges than other methods. From the segmentation \nresults of the first three figures, our method not only suppresses \nthe interference of background features well but also predicts \noutcomes very close to Ground Truth.  Next, the segmentation \noutcomes on the Vaihingen test set are shown in Fig. 10., where \nGCFM fuses semantic information at different scales, \neffectively mitigates semantic gaps between features, and \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358851\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \n \n \nFig. 9. Visualization comparisons on the Potsdam test set.  \n \nprecisely controls features with fine and coarse grained, and \ndemonstrates a high degree of granularity in the segmentation \nresults of the first two graphs, allowing HAFNet to outperform \nother methods in dealing with the problems of mis -\nsegmentation and category confusion. Especially in the first \nfigure, when dealing with the \"clutter category\", our method is \nbetter at extracting the objects' full contours than other methods. \nIn addition, for objects with complex texture features such as \ntrees and low vegetation, the final two segmentation result  \nimages in Fig. 9. and Fig. 10. vividly showcase the outstanding \nperformance of our approach. \n \nV. CONCLUSION \nThis paper propose s a inimitable decoder based on the \nTransformer architecture with ResNet50 as the encoder. Our \ndesigned CSTB can effectively integrate local details, \nchannels, and global information, reducing the interference of  \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358851\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \n \n \n Fig. 10. Visualization comparisons on the Vaihingen test set.\n \nredundant feature information and enhancing the significant \nfeature representation of objects. The GCFM performs an \ninteractive fusion of features from different branches , \nestablishing correlations between various elements and  \nimproving the model â€™s understanding of contextual \ninformation to acquire a more comprehensive understanding \nof semantic information. The experimental results on the \nISPRS Potsdam and Vaihingen datasets demonstrate that \nHAFNet outperforms the compared baselines. HAFNet also \nperforms exceptionally well in handling complex objects with \nsimilar texture features, such as low vegetation and trees,  \neffectively addressing the issue of class confusion. \nAdditionally, it also effectively alleviates the problem of class \nimbalance in remote sensing images. Finally, comprehensive \nablation studies have provided evidence of the effectiveness of \nevery component in the proposed approach . Additionally, our \nresearch only focuses on how to improving segmentation \naccuracy, while there are still deficiencies in the number of \nmodel parameters and computational complexity. Therefore, \nin future research, we will continue to explore how to fully \nutilise the respective advantages of CNN and Transformer to \nfurther improve the model efficiency and performance by \naddressing this aspect of the problem. \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358851\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \nREFERENCES \n \n[1] L. Ma, Y. Liu, X. Zhang, Y. Ye, G. Yin, and B. A. Johnson, \"Deep \nlearning in remote sensing applications: A meta -analysis and review,\" \nISPRS J. Photogramm. Remote Sens., vol. 152, pp. 166 -177, Jun. 2019, \ndoi: 10.1016/j.isprsjprs.2019.04.015. \n[2] P. Wang, C. Huang, J. C. Tilton, B. Tan, and E. C. B. d. Colstoun, \n\"HOTEX: An approach for global mapping of human built -up and \nsettlement extent,\" in 2017 IEEE International Geoscience and Remote \nSensing Symposium (IGARSS) , Jul. 2017, pp. 1562 -1565, doi: \n10.1109/IGARSS.2017.8127268.  \n[3] G. Chen, Q. Weng, G. J. Hay, and Y. He, \"Geographic object -based \nimage analysis (GEOBIA): emerging trends and future opportunities,\" \nGISci. Remote Sens., vol. 55, no. 2, pp. 159 -182, Mar. 2018, doi: \n10.1080/15481603.2018.1426092. \n[4] L. Ma, M. Li, X. Ma, L. Cheng, P. Du, and Y. Liu, \"A review of \nsupervised object -based land -cover image classification,\" ISPRS J. \nPhotogramm. Remote Sens., vol. 130, pp. 277 -293, Aug. 2017, doi: \n10.1016/j.isprsjprs.2017.06.001. \n[5] D. Phiri and J. Morgenroth, \"Developments in Landsat Land Cover \nClassification Methods: A Review,\" vol. 9, no. 9, p. 967, 2017. [Online]. \nAvailable: https://www.mdpi.com/2072-4292/9/9/967. \n[6]   X. Yuan, J. Shi, and L. Gu, \"A review of deep learning methods for \nsemantic segmentation of remote sensing imagery,\" Expert Syst. Appl., \nvol. 169, May 2021, Art no. 114417, doi: 10.1016/j.eswa.2020.114417.  \n[7] M. Mostajabi, P. Yadollahpour, and G. Shakhnarovich, \"Feedforward \nsemantic segmentation with zoom -out features,\" in Pro. IEEE Conf. \nComput. Vis. Pattern Recognit., 2015, pp. 3376-3385.  \n[8] D. Eigen and R. Fergus, \"Predicting depth, surface normals and \nsemantic labels with a common multi -scale convolutional architecture,\" \nin Proc. IEEE Int. Conf. Comput. Vis., 2015, pp. 2650-2658. \n[9] J. Lv, Q. Shen, M. Lv, Y. Li, L. Shi, and P. Zhang, \"Deep learning -\nbased semantic segmentation of remote sensing images: a review,\" \nFrontiers in Ecology and Evolution, vol. 11, Jul 2023, Art no. 1201125, \ndoi: 10.3389/fevo.2023.1201125.  \n[10] J. Long, E. Shelhamer, and T. Darrell, \"Fully convolutional networks for \nsemantic segmentation,\" in Pro. IEEE Conf. Comput. Vis. Pattern \nRecognit., 2015, pp. 3431-3440.  \n[11] V. Badrinarayanan, A. Kendall, and R. Cipolla, \"SegNet: A Deep \nConvolutional Encoder-Decoder Architecture for Image Segmentation,\" \nIEEE Trans. Pattern Anal. Mach. Intell., vol. 39, no. 12, pp. 2481-2495, \n2017, doi: 10.1109/TPAMI.2016.2644615. \n[12] L. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, \"Encoder -\ndecoder with atrous separable convolution for semantic image \nsegmentation,\" in Proc. Eur. Conf. Comput. Vis., 2018, pp. 801-818.  \n[13] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \"Pyramid scene parsing \nnetwork,\" in Pro. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. \n2881-2890.  \n[14] C. Yu, J. Wang, C. Peng, C. Gao, G. Yu, and N. Sang, \"Bisenet: \nBilateral segmentation network for real-time semantic segmentation,\" in \nProc. Eur. Conf. Comput. Vis., 2018, pp. 325-341.  \n[15] C. Yu, C. Gao, J. Wang, G. Yu, C. Shen, and N. Sang, \"BiSeNet V2: \nBilateral Network with Guided Aggregation for Real -Time Semantic \nSegmentation,\" Int. J. Comput. Vision, vol. 129, no. 11, pp. 3051 -3068, \nNov. 2021, doi: 10.1007/s11263-021-01515-2. \n[16] R. Hang, P. Yang, F. Zhou, and Q. Liu, \"Multiscale Progressive \nSegmentation Network for High -Resolution Remote Sensing Imagery,\" \nIEEE Trans. Geosci. Remote Sens., vol. 60, 2022, Art no. 5412012, doi: \n10.1109/tgrs.2022.3207551. \n[17] F. Zhou, R. Hang, and Q. Liu, \"Class -Guided Feature Decoupling \nNetwork for Airborne Image Segmentation,\" IEEE Trans. Geosci. \nRemote Sens., vol. 59, no. 3, pp. 2245 -2255, Mar 2021, doi: \n10.1109/tgrs.2020.3006872. \n[18] M. Zhang, W. Li, X. Zhao, H. Liu, R. Tao, and Q. Du, \"Morphological \nTransformation and Spatial -Logical Aggregation for Tree Species \nClassification Using Hyperspectral Imagery,\" IEEE Trans. Geosci. \nRemote Sens., vol. 61, 2023, Art no. 5501212, doi: \n10.1109/tgrs.2022.3233847. \n[19] D. Hong et al., \"SpectralGPT: Spectral Foundation Model,\" 2023. \n[Online].Available: https://doi.org/10.48550/arXiv.2311.07113 \n[20] J. Yao, B. Zhang, C. Li, D. Hong, and J. Chanussot, \"Extended Vision \nTransformer (ExViT) for Land Use and Land Cover Classification: A \nMultimodal Deep Learning Framework,\" IEEE Trans. Geosci. Remote \nSens., vol. 61, 2023, Art no. 5514415, doi: 10.1109/tgrs.2023.3284671. \n[21] W. Li, J. Wang, Y. Gao, M. Zhang, R. Tao, and B. Zhang, \"Graph -\nFeature-Enhanced Selective Assignment Network for Hyperspectral and \nMultispectral Data Classification,\" IEEE Trans. Geosci. Remote Sens., \nvol. 60, 2022, Art no. 5526914, doi: 10.1109/tgrs.2022.3166252. \n[22] M. Guo  et al., \"Attention mechanisms in computer vision: A survey,\" \nComputational Visual Media, vol. 8, no. 3, pp. 331-368, Sep. 2022, doi: \n10.1007/s41095-022-0271-y. \n[23] S. Woo, J. Park, J. Y. Lee, and I. S. Kweon, â€œCBAM: Convolutional  \nblock attention module,â€ in Proc. Eur. Conf. Comput. Vis., vol. 11211. \nCham, Switzerland: Springer, Oct. 2018, pp. 3â€“19. \n[24] R. Li, C. Duan, S. Zheng, C. Zhang, and P. M. Atkinson, \"MACU -Net \nfor Semantic Segmentation of Fine -Resolution Remotely Sensed \nImages,\" IEEE Geosci. Remote Sens. Lett., vol. 19, 2022, Art no. \n8007205, doi: 10.1109/lgrs.2021.3052886. \n[25] R. Li  et al. , \"Multiattention Network for Semantic Segmentation of \nFine-Resolution Remote Sensing Images,\" IEEE Trans. Geosci. Remote \nSens., vol. 60, pp. 1-13, 2022, doi: 10.1109/TGRS.2021.3093977. \n[26] L. Bai, X. Lin, Z. Ye, D. Xue, C. Yao, and M. Hui, \"MsanlfNet: \nSemantic Segmentation Network With Multi -scale Attention and \nNonlocal Filters for High -Resolution Remote Sensing Images,\" IEEE \nGeosci. Remote Sens. Lett., vol. 19, 2022, Art no. 6512405, doi: \n10.1109/lgrs.2022.3185641. \n[27] D. Hong et al., \"Cross -city matters: A multimodal remote sensing \nbenchmark dataset for cross -city semantic segmentation using high -\nresolution domain adaptation networks,\" Remote Sens. Environ., vol. \n299, Dec 2023, Art no. 113856, doi: 10.1016/j.rse.2023.113856. \n[28] Z. Xu, J. Geng, and W. Jiang, \"MMT: Mixed -Mask Transformer for \nRemote Sensing Image Semantic Segmentation,\" IEEE Trans. Geosci. \nRemote Sens., vol. 61, 2023, Art no. 5613415, doi: \n10.1109/tgrs.2023.3289408. \n[29] S. Zheng et al., \"Rethinking semantic segmentation from a sequence-to-\nsequence perspective with transformers,\" in Pro. IEEE/CVF Conf. \nComput. Vis. Pattern Recognit., 2021, pp. 6881-6890.  \n[30] O. Ronneberger, P. Fischer, and T. Brox, â€œU -Net: Convolutional \nnetworks for biomedical image segmentation,â€ in Medical Image \nComputing and Computer -Assisted Intervention â€”MICCAI2015, vol. \n9351, N. Navab, J. Hornegger, W. M. Wells, and A. F. Frangi, Eds. \nCham, Switzerland: Springer, 2015, pp. 234 â€“241, doi: 10.1007/978 -3-\n319-24574-4_28.  \n[31] Y. Sun, Y. Tian, and Y. Xu, \"Problems of encoder -decoder frameworks \nfor high -resolution remote sensing image segmentation: Structural \nstereotype and insufficient learning,\" Neurocomputing, vol. 330, pp. \n297-304, Feb 2019, doi: 10.1016/j.neucom.2018.11.051. \n[32] Y. Liu, B. Fan, L. Wang, J. Bai, S. Xiang, and C. Pan, \"Semantic \nlabeling in very high resolution images via a self -cascaded \nconvolutional neural network,\" ISPRS J. Photogramm. Remote Sens., \nvol. 145, pp. 78-95, Nov. 2018, doi: 10.1016/j.isprsjprs.2017.12.007. \n[33] Y. Shen, J. Chen, L. Xiao, and D. Pan, \"Optimizing multi -scale \nsegmentation with local spectral heterogeneity measure for high \nresolution remote sensing images,\" ISPRS J. Photogramm. Remote Sens., \nvol. 157, pp. 13-25, Nov 2019, doi: 10.1016/j.isprsjprs.2019.08.014. \n[34] X. Wang, R. Girshick, A. Gupta, and K. He, \"Non -local neural \nnetworks,\" in Pro. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. \n7794-7803.  \n[35] J. Fu  et al. , \"Dual attention network for scene segmentation,\" in Pro. \nIEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 3146-3154.  \n[36] Y. Yuan, X. Chen, and J. Wang, \"Object-Contextual Representations for \nSemantic Segmentation,\" in Proc. Eur. Conf. Comput. Vis. , Cham,  \nSpringer International Publishing, 2020, pp. 173 -190, doi: 10.1007/978-\n3-030-58539-6_11. \n[37] J. Wang, W. Li, M. Zhang, and J. Chanussot, \"Large Kernel Sparse \nConvNet Weighted by Multi -Frequency Attention for Remote Sensing \nScene Understanding,\" IEEE Trans. Geosci. Remote Sens., vol. 61, pp. \n1-12, 2023, doi: 10.1109/TGRS.2023.3333401. \n[38] R. Hang, Z. Li, Q. Liu, P. Ghamisi, and S. S. Bhattacharyya, \n\"Hyperspectral Image Classification With Attention -Aided CNNs,\" \nIEEE Trans. Geosci. Remote Sens., vol. 59, no. 3, pp. 2281 -2293, Mar \n2021, doi: 10.1109/tgrs.2020.3007921. \n[39] H. Zhang, J. Yao, L. Ni, L. Gao, and M. Huang, \"Multimodal Attention-\nAware Convolutional Neural Networks for Classification of \nHyperspectral and LiDAR Data,\" IEEE J. Sel. Top. Appl. Earth Obs. \nRemote Sens., vol. 16, pp. 3635 -3644, 2023, doi: \n10.1109/jstars.2022.3187730. \n[40] L. Ding, H. Tang, and L. Bruzzone, \"LANet: Local Attention \nEmbedding to Improve the Semantic Segmentation of Remote Sensing \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358851\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \nImages,\" IEEE Trans. Geosci. Remote Sens., vol. 59, no. 1, pp. 426-435, \nJan. 2021, doi: 10.1109/tgrs.2020.2994150. \n[41] R. Liu, L. Mi, and Z. Chen, \"AFNet: Adaptive Fusion Network for \nRemote Sensing Image Semantic Segmentation,\" IEEE Trans. Geosci. \nRemote Sens., vol. 59, no. 9, pp. 7871 -7886, Sep. 2021, doi: \n10.1109/tgrs.2020.3034123. \n[42] W. Cheng, W. Yang, M. Wang, G. Wang, and J. Chen, \"Context \nAggregation Network for Semantic Labeling in Aerial Images,\" Remote \nSens., vol. 11, no. 10, May. 2019, Art no. 1158, doi: \n10.3390/rs11101158. \n[43] M. Yang, S. Kumaar, Y. Lyu, and F. Nex, \"Real -time Semantic \nSegmentation with Context Aggregation Network,\" ISPRS J. \nPhotogramm. Remote Sens., vol. 178, pp. 124 -134, Aug. 2021, doi: \n10.1016/j.isprsjprs.2021.06.006. \n[44] Z. Chen, J. Zhao, and H. Deng, \"Global Multi -Attention UResNeXt for \nSemantic Segmentation of High -Resolution Remote Sensing Images,\" \nRemote Sens., vol. 15, no. 7, Apr. 2023, Art no. 1836, doi: \n10.3390/rs15071836. \n[45] L. Wang, R. Li, D. Wang, C. Duan, T. Wang, and X. Meng, \n\"Transformer Meets Convolution: A Bilateral Awareness Network for \nSemantic Segmentation of Very Fine Resolution Urban Scene Images,\" \nRemote Sens., vol. 13, no. 16, Aug. 2021, Art no. 3065, doi: \n10.3390/rs13163065. \n[46] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. J. A. i. \nN. I. P. S. Luo, \"SegFormer: Simple and efficient design for semantic \nsegmentation with transformers,\" vol. 34, pp. 12077-12090, 2021. \n[47] H. Cao  et al. , \"Swin -Unet: Unet -Like Pure Transformer for  Medical \nImage Segmentation,\" in Proc. Eur. Conf. Comput. Vis. Workshops , \nCham, L. Karlinsky, T. Michaeli, and K. Nishino, Eds.,  Switzerland: \nSpringer, 2022, pp. 205-218, doi: 10.1007/978-3-031-25066-8_9.  \n[48] R. Strudel, R. Garcia, I. Laptev, and C. Schmid, \"Segmenter: \nTransformer for semantic segmentation,\" in Proc. IEEE/CVF Int. Conf. \nComput. Vis., 2021, pp. 7262-7272. \n[49] A. Zhao, C. Wang, and X. Li, \"A global + multiscale hybrid network for \nhyperspectral image classification,\" Remote Sens. Lett., vol. 14, no. 9, \npp. 1002-1010, Sep 2023, doi: 10.1080/2150704x.2023.2258467. \n[50] L. Wang, R. Li, C. Duan, C. Zhang, X. Meng, and S. Fang, \"A Novel \nTransformer Based Semantic Segmentation Scheme for Fine-Resolution \nRemote Sensing Images,\" IEEE Geosci. Remote Sens. Lett., vol. 19, \n2022, Art no. 6506105, doi: 10.1109/lgrs.2022.3143368. \n[51] Z. Liu et al., \"Swin transformer: Hierarchical vision transformer using \nshifted windows,\" in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2021, pp. \n10012-10022.  \n[52] T. Panboonyuen, K. Jitkajornwanich, S. Lawawirojwong, P. \nSrestasathiern, and P. Vateekul, \"Transformer -Based Decoder Designs \nfor Semantic Segmentation on Remotely Sensed Images,\" Remote Sens., \nvol. 13, no. 24, Dec. 2021, Art no. 5100, doi: 10.3390/rs13245100. \n[53] A. Vaswani et al., â€œAttention is all you need,â€ in Proc. Adv. Neural Inf.  \nProcess. Syst. (NeurIPS), 2017, pp. 6000â€“6010. \n[54] J. Chen  et al. , \"Transunet: Transformers make strong encoders for \nmedical image segmentation,\" 2021. [Online].Available:  \nhttps://doi.org/10.48550/arXiv.2102.04306 \n[55] J. Hu, L. Shen, and G. Sun, \"Squeeze -and-excitation networks,\" in Pro. \nIEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 7132-7141.  \n[56] Q. Wang, B. Wu, P. Zhu, P. Li, W. Zuo, and Q. Hu, \"ECA -Net: \nEfficient channel attention for deep convolutional neural networks,\" in \nPro. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2020, pp. 11534-\n11542.  \n[57] S. Woo et al., \"Convnext v2: Co -designing and scaling convnets with \nmasked autoencoders,\" in Pro. IEEE/CVF Conf. Comput. Vis. Pattern \nRecognit., 2023, pp. 16133-16142.  \n[58] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"ImageNet classification \nwith deep convolutional neural networks,\" Commun. ACM, vol. 60, no. \n6, pp. 84â€“90, 2017, doi: 10.1145/3065386. \n[59] R. Li, S. Zheng, C. Duan, J. Su, and C. Zhang, \"Multistage Attention \nResU-Net for Semantic Segmentation of Fine -Resolution Remote \nSensing Images,\" IEEE Geosci. Remote Sens. Lett., vol. 19, 2022, Art \nno. 8009205, doi: 10.1109/lgrs.2021.3063381. \n[60] R. Li, S. Zheng, C. Zhang, C. Duan, L. Wang, and P. M. Atkinson, \n\"ABCNet: Attentive bilateral contextual network for efficient semantic \nsegmentation of Fine -Resolution remotely sensed imagery,\" ISPRS J. \nPhotogramm. Remote Sens., vol. 181, pp. 84 -98, Nov. 2021, doi: \n10.1016/j.isprsjprs.2021.09.005. \n[61] L. Wang  et al. , \"UNetFormer: A UNet -like transformer for efficient \nsemantic segmentation of remote sensing urban scene imagery,\" ISPRS \nJ. Photogramm. Remote Sens., vol. 190, pp. 196 -214, Aug. 2022, doi: \n10.1016/j.isprsjprs.2022.06.008. \n[62] S. Du and M. Liu, \"Class -Guidance Network Based on the Pyramid \nVision Transformer for Efficient Semantic Segmentation of High -\nResolution Remote Sensing Images,\" IEEE J. Sel. Top. Appl. Earth Obs. \nRemote Sens., vol. 16, pp. 5578 -5589, 2023, doi: \n10.1109/jstars.2023.3285632. \n[63] H. Wu, P. Huang, M. Zhang, W. Tang, and X. Yu, \"CMTFNet: CNN \nand Multi -scale Transformer Fusion Network for Remote -Sensing \nImage Semantic Segmentation,\" IEEE Trans. Geosci. Remote Sens., vol. \n61, 2023, Art no. 2004612, doi: 10.1109/tgrs.2023.3314641. \n[64] L. Wang, S. Fang, X. Meng, and R. Li, \"Building Extraction With \nVision Transformer,\" IEEE Trans. Geosci. Remote Sens., vol. 60, 2022, \nArt no. 5625711, doi: 10.1109/tgrs.2022.3186634. \n[65] J. Zheng, A. Shao, Y. Yan, J. Wu, and M. Zhang, \"Remote Sensing \nSemantic Segmentation via Boundary Supervision -Aided Multiscale \nChannelwise Cross Attention Network,\" IEEE Trans. Geosci. Remote \nSens., vol. 61, 2023, Art no. 4405814, doi: 10.1109/tgrs.2023.3292112. \n[66] J. Cui, J. Liu, J. Wang, and Y. Ni, \"Global Context Dependencies Aware \nNetwork for Efficient Semantic Segmentation of Fine -Resolution \nRemoted Sensing Images,\" IEEE Geosci. Remote Sens. Lett., vol. 20, \n2023, Art no. 2505205, doi: 10.1109/lgrs.2023.3318348. \n[67] A. G. Roy, N. Navab, and C. Wachinger, \"Concurrent Spatial and \nChannel â€˜Squeeze & Excitationâ€™ in Fully Convolutional Networks,\" in \nMedical Image Computing and Computer Assisted Intervention â€“ \nMICCAI 2018, vol. 11070 , A. F. Frangi, J. A. Schnabel, C. Davatzikos, \nC. Alberola-LÃ³ pez, and G. Fichtinger, Eds.,  Cham, Springer, 2018, pp. \n421-429, doi: 10.1007/978-3-030-00928-1_48.  \n[68] Q. Wan, Z. Huang, J. Lu, G. Yu, and L. Zhang, \"Seaformer: Squeeze -\nenhanced axial transformer for mobile semantic segmentation,\" 2023. \n[Online].Available: https://doi.org/10.48550/arXiv.2301.13156 \n \n \n \n \n \n \nYan Chen received his M.Sc. degree from \nthe China University of Mining and \nTechnology (Xuzhou, Jiangsu, China) in \n2014, and his doctorate from the TU \nDortmund (Dortmund, Germany) in 2019. \nThere, he studied spatial information \nmanagement and modeling as a member \nof the Spatial Information Management \nand Modelling Department of the School \nof Spatial Planning. He joined the Collaborative Innovation \nCentre for Computer Vision and Pattern Recognition \n(CICCVPR) within the School of Artificial Intelligence and \nBig Data at Hefei University in 2022. \nHis research interests include remote sensing image analysis, \ncomputer vision and pattern recognition, as well as deep \nlearning and optimization algorithms. \n \n \n \n \nQuan Dong received his bachelor's \ndegree in Computer Science and \nTechnology from  Nanjing Normal \nUniversity Zhongbei College  in 2022. He \nis pursuing his master's degree in the \nSchool of Artificial Intelligence and Big \nData at Hefei University. \nHis research interests are semantic \nsegmentation of remote sensing images. \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358851\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \n \nXiaofeng Wang  received his bachelor \nfrom the Anhui University (Hefei, Anhui, \nChina) in 1999, his M.Sc. degree from the \nUniversity of Chinese Academy of \nSciences (Beijing, China) in 2005, and his \ndoctorate from the University of Science \nand Technology of China (Hefei, Anhui, \nChina) in 2009. He is a Professor at the \nSchool of Artificial Intelligence and Big \nData at Hefei University, and is a provincial academic and \ntechnical leader reserve candidate.  \nHis research interests include computer vision and pattern \nrecognition, image processing. \n \nQianchuan Zhang received his bachelor's \ndegree from Sichuan Minzu \nCollege,College of Science and \nTechnology in 2020. He is pursuing a \nmaster's degree in the School of Artificial  \nIntelligence and Big Data, Hefei \nUniversity. \nHis research interests include intelligent \nparsing of remote sensing images and \ndeep learning. \n \nMenglei Kang  received his bachelor in \nComputer and Information Engineering \nfrom Chuzhou University in 2021. He is \npursuing his master's degree in the School \nof Artificial Intelligence and Big Data at \nHefei University.  \nHis research interests are semantic \nsegmentation of remote sensing images. \n \nWenxiang Jiang  received the bachelor's \ndegree from the College of Computer \nScience and Technology, Anhui University \nof Technology, Ma'anshan, China, in 2021. \nHe is pursuing a master's degree in \nCollaborative Innovation Centre for \nComputer Vision and Pattern Recognition \n(CICCVPRï¼‰ of the School of Artificial \nIntelligence and Big Data at Hefei University, Hefei, China. \nHis research interests include computer vision in deep \nlearning and remote sensing image analysis. \n \nMengyuan Wang  received the bachelor's \ndegree from the School of Internet Of \nThing Engineering, Applied Technology \nCollege of Soochow University, Jiangsu, \nChina, in 2020. She is pursuing the \nmaster's degree with the School of \nArtificial Intelligence and Big Data, Hefei \nUniversity, Hefei, China.  \nHer research interests include semantic \nsegmentation of high -resolution remote sensing images and \ndeep learning. \n \n \nLixiang Xu received the B.Sc. degree and \nM.Sc. degree in applied mathematics in \n2005 and 2008, respectively. He worked at \nHuawei Technologies Co., Ltd. in 2008 \nbefore joining Hefei University in the \nfollowing year. He received his Ph.D. \ndegree from the School of Computer \nScience and Technology, Anhui \nUniversity, Hefei, China, in 2017. He has \nbeen awarded a scholarship to pursue his \nstudy in Germany as a joint Ph.D. student from 2015 to 2017. \nPresently, he is doing postdoctoral research at the University \nof Science and Technology of China.  \nHis current research interests include structural pattern \nrecognition, machine learning, graph spectral analysis, image \nand graph matching, especially in kernel methods and \ncomplexity analysis on graphs and networks. \n \nChen Zhang was born in Anhui, China. \nShe received the M.S. degrees in \ncomputational mathematics from the \nAnhui University in 2011 and the Ph.D. \ndegree in information management and \nsystem from Hefei University of \nTechnology, China, in 2016. She is now \nan Associate Professor in the School of \nArtificial Intelligence and Big Data, Hefei \nUniversity, China.  \nHer research interests include machine learning and artificial \nintelligence. \n \n \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358851\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
}