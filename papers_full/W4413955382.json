{
  "title": "LLM-D12: A Dual-Dimensional Scale of Instrumental and Relational Dependencies on Large Language Models",
  "url": "https://openalex.org/W4413955382",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2008075979",
      "name": "Ala Yankouskaya",
      "affiliations": [
        "Bournemouth University"
      ]
    },
    {
      "id": "https://openalex.org/A2432200580",
      "name": "Areej Babiker",
      "affiliations": [
        "Hamad bin Khalifa University"
      ]
    },
    {
      "id": "https://openalex.org/A2680655826",
      "name": "Syeda Rizvi",
      "affiliations": [
        "Hamad bin Khalifa University"
      ]
    },
    {
      "id": "https://openalex.org/A4282179555",
      "name": "Sameha Alshakhsi",
      "affiliations": [
        "Hamad bin Khalifa University"
      ]
    },
    {
      "id": "https://openalex.org/A1925403175",
      "name": "Magnus Liebherr",
      "affiliations": [
        "University of Duisburg-Essen"
      ]
    },
    {
      "id": "https://openalex.org/A2101701767",
      "name": "Raian Ali",
      "affiliations": [
        "Hamad bin Khalifa University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2110171129",
    "https://openalex.org/W4243906228",
    "https://openalex.org/W4399790131",
    "https://openalex.org/W4393177920",
    "https://openalex.org/W4403583423",
    "https://openalex.org/W4409956450",
    "https://openalex.org/W4304172392",
    "https://openalex.org/W4407637219",
    "https://openalex.org/W4315491255",
    "https://openalex.org/W2314751257",
    "https://openalex.org/W4407777119",
    "https://openalex.org/W2122517769",
    "https://openalex.org/W2143545365",
    "https://openalex.org/W1656810637",
    "https://openalex.org/W3125770638",
    "https://openalex.org/W4384934594",
    "https://openalex.org/W4327518740",
    "https://openalex.org/W4392783116",
    "https://openalex.org/W4387306661",
    "https://openalex.org/W4211002924",
    "https://openalex.org/W3042990871",
    "https://openalex.org/W4240476866",
    "https://openalex.org/W3214797170",
    "https://openalex.org/W2076457657",
    "https://openalex.org/W2595367318",
    "https://openalex.org/W4375951402",
    "https://openalex.org/W2055097229",
    "https://openalex.org/W3006245018",
    "https://openalex.org/W4253638624",
    "https://openalex.org/W4312204748",
    "https://openalex.org/W4312179235",
    "https://openalex.org/W1837865419",
    "https://openalex.org/W4310643312",
    "https://openalex.org/W1995474267",
    "https://openalex.org/W2129569911",
    "https://openalex.org/W3144406692",
    "https://openalex.org/W2162114525",
    "https://openalex.org/W3124923678",
    "https://openalex.org/W2082050557",
    "https://openalex.org/W3126055879",
    "https://openalex.org/W2580536144",
    "https://openalex.org/W2886960137",
    "https://openalex.org/W2147913675",
    "https://openalex.org/W2069913666",
    "https://openalex.org/W2807270404",
    "https://openalex.org/W2132555912",
    "https://openalex.org/W2033590892",
    "https://openalex.org/W2097137621",
    "https://openalex.org/W2106096361",
    "https://openalex.org/W2729551988",
    "https://openalex.org/W2209976290",
    "https://openalex.org/W3154793835",
    "https://openalex.org/W4321226920",
    "https://openalex.org/W4395049484",
    "https://openalex.org/W1997475706",
    "https://openalex.org/W2149595999",
    "https://openalex.org/W2132164645",
    "https://openalex.org/W4406703231",
    "https://openalex.org/W2955103858",
    "https://openalex.org/W4282963908",
    "https://openalex.org/W2109898199",
    "https://openalex.org/W2751485140",
    "https://openalex.org/W1991015565",
    "https://openalex.org/W4401898127",
    "https://openalex.org/W4221014470",
    "https://openalex.org/W2143074722",
    "https://openalex.org/W2135907989",
    "https://openalex.org/W4391971084",
    "https://openalex.org/W2196790309",
    "https://openalex.org/W4401981824",
    "https://openalex.org/W2988200200",
    "https://openalex.org/W2397551464",
    "https://openalex.org/W4408167892",
    "https://openalex.org/W2517793972",
    "https://openalex.org/W2239306219",
    "https://openalex.org/W2089358714",
    "https://openalex.org/W2921068806",
    "https://openalex.org/W2103676101",
    "https://openalex.org/W4244229132",
    "https://openalex.org/W4390833061",
    "https://openalex.org/W3044784125",
    "https://openalex.org/W4401413185",
    "https://openalex.org/W271007954",
    "https://openalex.org/W4365136391",
    "https://openalex.org/W4231042348",
    "https://openalex.org/W4409885395",
    "https://openalex.org/W3188669645",
    "https://openalex.org/W4387934614",
    "https://openalex.org/W4313294616",
    "https://openalex.org/W572985010"
  ],
  "abstract": "There is growing interest in understanding how people interact with large language models (LLMs) and whether such models elicit dependency or even addictive behaviour. Validated tools to assess the extent to which individuals may become dependent on LLMs are scarce and primarily build on classic behavioral addiction symptoms, adapted to the context of LLM use. We view this as a conceptual limitation, as the LLM-human relationship is more nuanced and warrants a fresh and distinct perspective. To address this gap, we developed and validated a new 12-item questionnaire to measure LLM dependency, referred to as LLM-D12. The scale was based on the authors' prior theoretical work, with items developed accordingly and responses collected from 526 participants in the UK. Exploratory and confirmatory factor analyses, performed on separate halves of the total sample using a split-sample approach, supported a two-factor structure: Instrumental Dependency (six items) and Relationship Dependency (six items). Instrumental Dependency reflects the extent to which individuals rely on LLMs to support or collaborate in decision-making and cognitive tasks. Relationship Dependency captures the tendency to perceive LLMs as socially meaningful, sentient, or companion-like entities. The two-factor structure demonstrated excellent internal consistency and clear discriminant validity. External validation confirmed both the conceptual foundation and the distinction between the two subscales. The psychometric properties and structure of our LLM-D12 scale were interpreted in light of the emerging view that dependency on LLMs does not necessarily indicate dysfunction but may still reflect reliance levels that could become problematic in certain contexts.",
  "full_text": "This is a preprint; the paper is currently in press.  https://doi.org/10.48550/arXiv.2506.06874 \n \n \n \n \n \n \n \n \n \n \n \nLLM-D12: A Dual-Dimensional Scale of Instrumental and Relational Dependencies \non Large Language Models  \n \nAla Yankouskaya* \nDepartment of Psychology, Bournemouth University, Poole, UK, ayankouskaya@bournemouth.ac.uk \nAreej B. Babiker \nCollege of Science and Engineering, Hamad Bin Khalifa University, Doha, Qatar, arbabiker@hbku.edu.qa \nSyeda W. F. Rizvi \nCollege of Science and Engineering, Hamad Bin Khalifa University, Doha, Qatar, srizvi@hbku.edu.qa \nSameha Alshakhsi \nCollege of Science and Engineering, Hamad Bin Khalifa University, Doha, Qatar, salshakhsi@hbku.edu.qa \nMagnus Liebherr \nDepartment of Mechatronics, University Duisburg-Essen, Duisburg, Germany, magnus.liebherr@uni-due.de \nRaian Ali \nCollege of Science and Engineering, Hamad Bin Khalifa University, Doha, Qatar, raali2@hbku.edu.qa \nThere is growing interest in understanding how people interact with large language models (LLMs) and whether such models elicit \ndependency or even addictive behaviour. Validated tools to assess the extent to which individuals may become dependent on LLM s are \nscarce and primarily build on classic behavioral addiction symptoms, adapted to the context of LLM use. We view this as a concep tual \nlimitation, as the LLM -human relationship is more nuanced and warrants a fresh and distinct perspective. To address this gap, we \ndeveloped and validated a new 12-item questionnaire to measure LLM dependency, referred to as LLM-D12. The scale was based on the \nauthors' prior theoretical work, with items developed accordingly and responses collected from 526 participants in the UK. Ex ploratory \nand confirmatory factor analyses, performed on separate halves of the total sample using a split-sample approach, supported a two-factor \nstructure: Instrumental Dependency (six items) and Relationship Dependency (six items). Instrumental Dependency reflects the extent to \nwhich individuals rely on LLMs to support or collaborate in decision-making and cognitive tasks. Relationship Dependency captures the \ntendency to perceive LLMs as socially meaningful, sentient, or companion-like entities. The two-factor structure demonstrated excellent \ninternal consistency and clear discriminant validity. External validation confirmed both the conceptual foundation and the di stinction \nbetween the two subscales. The psychometric properties and structure of our LLM-D12 scale were interpreted in light of the emerging \nview that dependency on LLMs does not necessarily indicate dysfunction but may still reflect reliance levels that could becom e \nproblematic in certain contexts.    \nCCS CONCEPTS   Human-Centered Computing → Human computer interaction (HCI)  → HCI design and evaluation \nmethods \nAdditional Keywords and Phrases: Large Language Models, Dependency, Relational Dependency, Instrumental \nDependency Decision-Making. \n \n2 \n1 INTRODUCTION \nThe use of large language models (LLMs) has grown at an unprecedented rate, becoming increasingly embedded \nin both professional and personal life. In 2024, the global LLM market was valued at USD 6.5 billion, with forecasts \nprojecting expansion to over USD 140 billion by 2033, reflecting a compound annual growth rate of 40.7% [1]. This growth \nis not limited to the commercial sector. By 2025, it is estimated that over 750 million software products will incorporate \nLLMs, automating approximately 50% of digital workflows  [2]. The widespread adoption of these systems reflects not \nonly their functionality but their growing influence on how people search for information, make decisions, and complete \ncognitively demanding tasks [3], [4]. At first glance, this integration appears wholly beneficial: LLMs streamline problem-\nsolving, improve productivity, and support a wide range of users across domains. However, the very features that make \nthem valuable (e.g., efficiency, fluency, and personalised responsiveness) also can create the conditions for psychological \ndependency. \nReliance on technology is typically defined as functional trust in a system to carry out tasks based on perceived \ncompetence and reliability [5]. Dependency, by contrast, refers to a habitual or automatic form of engagement, often \ninvolving diminished autonomy or reduced critical effort [6]. This shift can be understood through the concept of cognitive \noffloading: when individuals routinely delegate reasoning, memory, or judgement to external systems, this behaviour can \nbecome embedded in everyday cognition [7]. When applied to LLMs, offloading is not only encouraged but often rewarded \nthrough rapid, seemingly authoritative outputs that reinforce repeated use. Heersmink [8] argues that this process may \ngradually erode users’ own cognitive engagement, particularly in writing, problem -solving, and decision -making. \nEmpirical evidence supports these concerns: frequent AI use has been associated with lower levels of critical thin king, \nespecially among younger users who show higher levels of reliance on LLM-generated answers  [9], [10]. \nWhat distinguishes LLMs from other technologies is not only their cognitive utility but their capacity to simulate \nemotional and interpersonal presence [11] [12] [13]. Their conversational design, responsiveness, and apparent sensitivity \nto user intent may lead individuals to attribute human -like traits to these systems. Anthropomorphisation , already well -\ndocumented in human-computer interaction, increases the likelihood of emotional bonding and perceived companionship \n[14]. Recent studies show that users interacting with anthropomorphic AI agents report significantly higher levels of media \ndependency, particularly, when those interactions are emotionally satisfying [15]. It has to be noted that OpenAI’s system \ncard for GPT -4o acknowledges these risks explicitly, cautioning that users may develop emotional attachments to the \nmodel’s expressive voice features, potentially reducing their engagement with real-world relationships [16]. Longitudinal \nfindings further suggest that regular, emotionally charged interactions with conversational agents can increase perceptions \nof empathy and deepen feelings of attachment over time [17]. Research also shows that when LLMs respond to emotionally \nvalenced prompts, their outputs shift in line with users’ expectations, reinforcing anthropomorphic illusions and creating \nthe impression of sentient responsiveness [18]. \nRecent theoretical work has highlighted that LLMs may have the potential to be addictive in ways that are \nmeaningfully different from other digital technologies [19]. For instance, in contrast to platforms such as social media or \nonline gaming, which often promote compulsive use through rewards, rapid stimulation, or social comparison, LLMs \nappear to encourage repeated engagement through mechanisms such as self-specific feedback, emotional reassurance, and \ninstrumental success. These features can create a time - and cost-effective, high-reward interaction that is perceived as \nhelpful and at times even empathetic. Theoretical analysis has further identified experiences of flow, perceived autonomy, \nand pseudo -social bonding as central to this emerging form of dependency [19]. These psychological processes not \ntypically addressed by existing behavioural addiction frameworks, but increasingly relevant in understanding user \ninteraction with intelligent systems. \n3 \nTaken together, the emerging body of evidence suggests that LLM dependency is not simply a more intensive form \nof digital media use. Rather, it reflects a qualitatively different psychological process involving emotional validation, \npseudo-social bonding, and behavioural reinforcement. This contrasts traditional behavioural dependencies such as \ngaming, social media, or smartphone use that are typically driven by novelty, social feedback, or fear of exclusion [20] \n[21].  LLMs operate through a different set of incentives: instrumental success [22], perceived empathy [13], and parasocial \nbounding [23]. As a result, dependency on LLMs may not be perceived as problematic by users themselves, despite \npotential impacts on autonomy, critical thinking, and social connection. \nAlthough growing awareness surrounds the potential risks associated with dependence on  LLMs, tools designed \nto measure this emerging phenomenon remain relatively limited. One of the earliest contributions is the Problematic \nChatGPT Use Scale (PCUS) [24]. Drawing on the diagnostic criteria for Internet Gaming Disorder (IGD) outlined in the \nDSM-5, the PCUS assesses maladaptive patterns of interaction with ChatGPT. It includes 11 items rated on a 4 -point \nLikert scale, addressing domains such as preoccupatio n, withdrawal, tolerance, loss of control, conflict, and mood \nmodification. More recently, the AI Chatbot Dependence Scale has been introduced as an 8-item unidimensional measure, \nfocusing on information-seeking behaviour, task execution, and overall intensity of use [25]. This tool captures key aspects \nof functional engagement, including time spent and cognitive offloading, offering a practical lens through which to examine \nreliance on conversational AI. However, its scope remains focused primarily on observable usage pat terns, without \nextending to the psychological or affective dimensions of dependency. An alternative perspective conceptualises LLM \ndependency as a dual-factor construct, distinguishing between two dimensions of dependence - functional and existential \n[26]. According to this study,  functional dependence includes aspects such as efficiency, task performance, and decision \nsupport, and is underpinned by the Human-Computer Trust Model, which considers perceived competence, benevolence, \nreciprocity, and risk [5]. In contrast, existential refers to a more profound psychological attachment, encompassing \nelements of identity, emotional regulation, and perceived companionship. This component draws upon contemporary \ntheories of addiction to capture deeper forms of reliance [26].  \nWhile this multidimensional framework offers a broader understanding of LLM dependency, particularly by \nacknowledging both instrumental and emotional aspects, it is framed within the language of addiction. As such, it invites \nreflection on whether constructs like existential engagement can or should be interpreted through a clinical lens, or whether \nthey may represent qualitatively distinct psychological experiences. It has to be noted that clinically defined symptoms of \naddiction, such as those found in su bstance use or behavioural disorders, may not fully capture the nuances of LLM \ndependency. For instance, individuals may turn to LLMs for efficiency, emotional support, or cognitive scaffolding, which \ndo not necessarily reflect pathological use. Applying clinical addiction criteria may therefore risk pathologising behaviours \nthat are adaptive, context-dependent, or rooted in emerging forms of human-technology interaction. \nThe current study aims to contribute to the growing body of research on assessing dependency on large language \nmodels (LLMs). Our conceptualisation of LLM dependency is informed by established psychological frameworks that \nexplain how users engage with tec hnology across cognitive, emotional, and motivational dimensions [19]. One \nfoundational concept is self-specificity, which refers to the capacity of personalised, emotionally resonant interactions to \nengage an individual’s sense of self, thereby increasing subjective relevance and intrinsic motivation [27]. This is supported \nby Self -Determination Theory, which postulates that the fulfilment of basic psychological needs such as competence, \nautonomy, and relatedness enhances engagement and psychological well-being [28]. Additionally, our model incorporates \nthe notion of flow, a state of optimal experience characterised by deep absorption, challenge -skill balance, and distorted \ntime perception - conditions often facilitated by adaptive, responsive technologies like ChatG PT [29]. Crucially, we also \ndraw on parasocial interaction theory to account for the formation of emotionally meaningful, one-sided bonds with LLMs \n4 \n[30]. These relationships, although not reciprocal, can facilitate emotional attachment, perceived companionship, and \nsocial substitution [31] [32]. By integrating these perspectives, the current study seeks to advance measurement approaches \nthat capture not only the instrumental use of LLMs but also the deeper psychological connections that may underpin \npatterns of habitual or dependent use. \n2 METHOD \nIn this section, we describe the theoretical underpinnings of the developed LLM -D12 scale for measuring \ndependency on LLMs. We then outline the procedure and method in detail, including the participants, scale items, external \nvalidation design, and data analysis. \n2.1 Theoretical underpinnings  \nCognitive and informational dependency on LLMs . Despite LLMs offering substantial benefits in enhancing \nproductivity and problem-solving, there is an increasing concern about the user’s dependence on these AI systems  [19]. \nRegular usage of these systems can reduce an individual's cognitive abilities and information retention capacity and \nincrease the reliance or dependency on these systems for information [33], [34]. This dependency could contribute to \nindividuals accepting the information provided by them without verifying the output [35], [36]. Individuals may tend to \nover-trust the information provided by LLMs, even when the answers provided by the system can be erroneous [37]. This \nis known as automation bias, which may result in a reduction in users’ ability to critically evaluate the validity of the \ninformation provided [38].  \nThis overreliance on LLMs is amplified by cognitive biases where an individual seeks to find mental shortcuts or \nheuristics, which may lead to uncritical acceptance of the information provided by LLM systems [39]. This dependency \non the information provided by the AI system without validation could create misclassification and misinterpretation [40]. \nAs the information produced by these systems may not always be correct, use of this information for research can pose a \nrisk for plagiarism, fabrication, and falsification [40]. Over time, this dependency can significantly impact human \nautonomy, including the personal agency and self -confidence of a person [41]. Research suggests lower self-confidence \ncould reduce  an individual ’s critical thinking and cognitive effort, which may  further increase dependency on these \nsystems, possibly resulting in uncritical acceptance of the information provided to them [42]. This could be because these \nsystems have redefined the role of AI from simple task automation to collaborative reasoning and problem solving, referred \nto as “co-intelligence” [31]. The ability for these systems to act as a cognitive partner and help the users in decision-making \ndeepens the potential of individuals to be dependent on these systems.  \nProductivity, instant gratification and task-based dependency. Research indicates that productivity is the primary \nreason for AI adoption at work, with 57% of respondents citing this in a survey assessing the use of LLM to improve \nproductivity and work quality [43]. LLMs can boost productivity by streamlining tasks by providing real-time support and \noffering creative solutions [19]. Its use can save time and help users in making faster and more informed decisions. The \ngrowing emphasis on increased productivity in workplaces as a measure for performance could have motivated individuals \nto adopt LLMs for help in their tasks [44]. Productivity is enhanced as LLMs can answer questions quickly, generate \ncontent, and retrieve information, saving time and helping individuals to make quicker and more informed decisions [45]. \nHowever, the quick responses from these AI systems can reinforce compulsive use and increase dependency for task \ncompletion. This is known as the Instant Gratification Effect, where individuals tend to expect things to happen quickly, \nand with the emergence of LLMs, the users experience quick and personalized responses, making their experience seamless \nand satisfying [46], [47]. As a result, users’ tolerance for delays may reduce, resulting in them preferring LLMs over other \nways to gather information or solve problems [48]. Additionally, depending on LLMs for task completion might increase \n5 \nan individual’s sense of accomplishment and dopamine -driven satisfaction, which may improve their work -reward cycle \n[49]. This work -reward cycle may contribute to individuals taking larger workloads, which could lead to burnout, \nemotional exhaustion, and work -life imbalance, which are predictors of behavioral addiction patterns [50]. As LLM \nsystems can help with task automation and increase productivity, individuals may depend on them to maintain high \nperformance [19].  \nEmotional and social dimensions of LLM dependency . LLMs can simulate human -like conversations, offering \ncompanionship and support. As a result, individuals may prefer to engage with LLMs instead of real-life human interaction, \nwhich could lead to social isolation and reduced interpersonal skills [51], [52]. This could arise because people may feel \nmore comfortable when they engage with LLMs,  due the non -judgmental, accepting language used by LLMs, which \nfacilitates trust and encourages self -disclosure, strengthening the user -chatbot relationship [51],[53]. Such one -sided \nconnections, known as parasocial bonds, involve emotional attachment to an AI system that cannot reciprocate [54], [55]. \nResearch shows that individuals form a bond with AI chatbots and perceive them as social entities that are capable of \nunderstanding them [53]. Constant engagement with LLM models can reinforce feelings of familiarity and trust, which \nmay contribute to dependency. Moreover, from perspective of Social exchange theory, people may prefer interacting with \nLLMs due to the perceived rewards, which may include the immediate support, validation, and companionship provided \nby these systems, which may outweigh the emotional cost of engaging with these systems [54]. In addition, research has \ndiscovered that AI authenticity and anthropomorphism are key drivers of the relationship between humans and chatbots \n[55]. Taken together, although the human-like qualities of LLMs may offer psychological comfort, they also carry the risk \nof increasing emotional dependency and reducing real-life social engagement.  \nPersonalisation, self-specificity and immersive engagement. LLM systems can engage in human-like conversations \nwith users and provide tailored responses, displaying behaviours that mimic human emotions and social cues [56]. \nExtending beyond basic information mediators, LLMs involve personalization and the ability to adapt to the users’ needs, \nmaking communication personally relevant and self-specific [19]. Self-specificity refers to providing the systems with the \nindividual’s unique sense of self, which makes the experience with these AI systems personally relevant [57]. This concept \naligns with the Social Verification Theory, which suggests that users may engage more deeply with systems that align with \nan individual’s cognitive functioning and emotional experiences through social validation, personalized feedback, or \nmeaningful interaction [58]. Research suggests that individuals may respond more positively to AI systems that mirror \ntheir emotional cues with responses that are personalized and relevant to the user’s sense of self [59]. Personal relevance \nand self-specificity can be fostered in several ways. Firstly, LLMs help nurture an individual’s sense of self-esteem through \nthe use of respectful and non -offensive language when answering user queries [60]. Secondly, LLMs have the ability to \nremember previous conversations, which makes the user experience a familiarity effect, fostering trust and validated by \nthe AI system [61]. Thirdly, LLMs are competent in various tasks, which could make the engagement feel more rewarding \nand encourage the users to engage for a longer time [19]. This dynamic supports the concept of flow in psychology, where \nusers become deeply engaged with the interaction with these AI systems [62]. While flow is typically linked with positive \nengagement, using LLM for longer durations can contribute to compulsive usage behavior [63]. Over time, this immersive \nengagement can lead to overreliance on LLMs, highlighting the role of self-specific engagement evolving into dependency \non these systems, especially when users seek to replicate the rewarding states associated with the flow states [64].  \n2.2 Participants \nSample size estimation. To determine the appropriate sample size, we employed a priory sample size calculation for \nStructural Equation Models [65]. This calculation was based on several parameters, including the number of observed and \n6 \nlatent variables in the model, the expected effect size (λ = 0.10), the desired level of statistical significance (α = 0.05), and \nthe target statistical power (1 – β = 0.80). The minimum required sample size for model structure was 200 participants. To \naccommodate both Exploratory Factor Analysis (EFA) and Confirmatory Factor Analysis (CFA), this number was \ndoubled, allowing the dataset to be split randomly between the two procedures. In total, data were collected from 646 \nindividuals. \n      Several inclusion criteria were set for participants’ eligibility for the study. First, they must be 18 years or older, \ncurrently residing in the UK, and identifying as British in terms of culture and norms . Second, they have to be familiar \nwith LLM. Several questions were asked to determine their familiarity. Participants who reported that they do not use it, \nwere excluded from the next step of the survey.  \nDesign \nItem development. Our LLM-D12 scale was developed to measure user dependency on LLMs. An initial set of 26 \nitems was generated through an iterative process and a detailed conceptualization of symptoms related to user-reliance on \nLLMs.  \nFirst, based on  our previous work [19]. we identified target areas of LLM dependence: personal relevance , \nparasocial bonding, over-reliance for decision-making and productivity boost and task automation . Six items were \ngenerated for each of personal relevance and work-related productivity dimensions while seven items were generated for \nparasocial bonding and over-reliance for decision -making. Each item was formulated to reflect observed or theorized \nsymptoms of depe ndency on LLM . The generated items were  refined through multiple reviews  and evaluated for face \nvalidity by two of the authors , both experts in the field and authors of the original work in  [19]. Redundant items were \nremoved. All items were rated on a 6 -point Likert scale ranging from 1 (“Strongly disagree”) to 6 (“Strongly agree”) , \neliminating a neutral midpoint to encourage participants to express either agreement or disagreement with the statements. \nThis approach increases response variability, which can enhance discriminant validity and improve the robustness of \nstatistical analyses.  \nIn addition , the initial version of the questionnaire, the items referred specifically to \"ChatGPT,\" which was \nreplaced with “LLM” to generalize beyond a single platform, account for variations in participants' preferences, and ensure \nthey responded based on the LLM they personally use. More specifically, the questions were about their declared Primary \nLLM, i.e. the one they use the most. This aligns with recommendations in scale development literature to ensure stability \nover time, across situations, and a cross cases, ensuring responses reflect attitudes toward LLMs broadly rather than a \nspecific product [66].  \nSecond, for each of the four theoretical dimensions, we developed several items (26 items in total) ( full list of \ninitial items can be found in Table 1):  \n1) Personal relevance (6 items).  This dimension f ocuses on how users experience the LLMs as enhancing their \nself-perceived value, confidence and sense of validation as well as understanding their needs. For example, the item: “It \nunderstands me well and provides empathetic answers”  captures the  positive feedback of validation d escribed by \nYankouskaya et al., [19] wherein users internalize the feeling that LLM’s knows them and understands them. The authors \nargue that LLMs have three distinct features that fuel users’ sense of personal relevance: (1) they are designed to avoid \noffensive language and respond seriously to all user queries, (2) they can remember previous conversations, giving users \nthe impression that the system understands them , reinforcing a positive feedback loop of validation, and (3) they act as \ncompetent assistants that help with various tasks, supporting users’ need for competence. These abilities to adapt to users’ \npreferences stimulate prolonged engagement and make it more difficult for users to disconnect  \n7 \n2) Parasocial bounding (7 items). This dimension measures emotional attachment, parasocial interaction and social \ndisplacement. It reflects the emergence of one -sided emotional connections between users and LLMs. Users may treat \nthe LLM as a companion,  sharing personal information and developing a sense of connection that mimics aspects of \nhuman interaction. This behavior may stem from perceiving LLMs as low -cost, low-effort, and controllable sources of \ninteraction which may lead to reliance on LLMs for emotional and social needs. For example, the item: “I share details \nabout my private life with it ”, reflects the emotional attachment to LLMs, which is encouraged by their personalized, \nhuman-like responses that simulate empathy and social presence, often aligning with the user's viewpoint. \n3) Productivity boost and task automation (6 items). Productivity enhancement and task automation through LLM, \nas suggested by Yankouskaya et al. [19], may foster dependency by positioning LLMs as essential tools for maintaining \nefficiency gains. Items in this subscale reflect the perceived benefits of using LLMs to automate routine tasks, streamline \nworkflows and offer creative solutions . The immediate responses provided by LLMs may be perceived  as rewarding, \ninitiating a work-reward cycle that motivates users to increase their workload to maintain a sense of accomplishment and \nsatisfaction. For example, item: “It significantly improve my work conditions and enhance my job satisfaction” assesses \nthe role of LLMs i n achieving  job satisfaction . Over time , u sers may begin delegating  increasingly complex \nresponsibilities to LLMs in order to sustain productivity standards, potentially leading to withdrawal-like symptoms such \nas frustration and anxiety when the tool is unavailable. Items in this subscale also assess aspects such as work-life balance \nand changes in workload associated with the use of LLMs. \n4) Over-reliance for decision-making (7 items). This subscale measures individuals’ tendency to rely on LLMs for \nmaking decisions, often at the expense of their own judgment. While the use of LLMs as heuristic tools may reduce \ncognitive load, it also risks diminishing users' self -confidence and personal agency. The subscale items, shaped by \nconcepts such as automation bias and withdrawal symptoms when LLMs are unavailable, reflect trust in the objectivity \nand speed of LLMs in offering data-driven suggestions. For example, the item: “It is my go-to for assistance in decision-\nmaking”, captures the salience of LLMs in daily choices. By providing personalized, rapid responses that save time and \neffort, and are free from emotional or cognitive bias, LLMs may reinforce an addictive behavioural cycle and amplify \ndecision-related anxiety and uncertainty in their absence, effects that may be more pronounced than with task-specific \nAI tools. \nPilot testing. Pilot testing (N = 27) was performed to assess item understanding. Some items showed a right-skewed \ndistribution. Accordingly, the wording was further  refined to reduce social desirability bias , make the items less \nconfrontational and enhance clarity [67]. For example, the item “I spend more time on LLM than originally planned” was \nlater revised to a more neutral phrasing: “I stick to the amount of time I originally planned to spend on it, without going \nover”. Therefore, the wording was refined further to re duce social desirability bias and enhance clarity. The final version \nwas rephrased as: “I spend exactly the time and have the conversations I need, without exceeding.” This version aimed to \nbalance clarity with neutral framing, minimize potential response bias, and more accurately capture the intended construct. \nMeasurements for external validation . External validation measurements  were used to examine the pattern of \nassociations between the dependency subscales ( Instrumental and Relationships) and theoretically relevant external \nvariables to assess the convergent validity of the dependency subscales.  Internet Addiction (IA), Attitude Toward AI \n(ATAI) (acceptance and fear), Need for Cognition (NFC) and Trustworthiness of primary LLM  were the theoretically \nrelevant validated measures used for the external validation analysis.  \nInternet Addiction was measured using the Internet Addiction Test (IAT -7) including 7 items [60]. TParticipants \nresponded using a 5-point Likert scale where 1 corresponds to never and 5 corresponds to always. The scores for the 7 \n8 \nitems were summed to obtain the total score of internet addiction. Therefore, the theoretical range for the total score of \ninternet addiction is 7 to 35. Cronbach’s alpha for IA in the present study was 0.826.  \nAttitudes towards AI were measured using the ATAI scale which consists of two subscales: ATAI Acceptance and \nATAI Fear [68]. ATAI Acceptance is assessed using 2 items which are measured on an 11-point Likert scale with 0 as \nstrongly disagree and 10 as strongly agree. The total ATAI Acceptance score is calculated by obtaining the sum for the \nscores of the two items and the theoretical range of the responses was 0 to 20. ATAI fear is assessed using 3 items which \nare measured on an 11-point Likert scale with 0 as strongly disagree and 10 as strongly agree. The total ATAI Fear score \nis calculated by obtaining the sum for the scores of the three items  and the theoretical range of the responses was 0 to 30. \nCronbach’s alpha for ATAI in the present study was 0.687 for Acceptance subscale and 0.715 for Fear subscale.  \nParticipants Need for Cognition (NFC) was measured using the NFC scale [59]. The responses to the 6 items were \nmeasured using a 5 -point Likert scale with 1 indicating extremely uncharacteristic of me and 5 indicating extremely \ncharacteristic of me. The total score for the need of cognition measured was obtained by summing the score for the 6 items. \nThe theoretical range for NFC score is 6 to 30. Cronbach’s alpha was 0.861.  \nTrustworthiness of primary LLM . We also measured perceived Trustworthiness of p articipants’ primary LLM , \nwhich was assessed using an 8-item measure, based on a previously established framework [70].  This framework identified \neight core dimensions to evaluate the trustworthiness of LLMs (truthfulness, safety, fairness, robustness, privacy, machine \nethics, transparency, and accountability). Each dimension was measured using one item. Participants were asked to indicate \nthe extent to which they agree on each dimension and their responses were measured on an 11 -point Likert scale ranging \nfrom 0 (“not at all’) to 10 (“Completely”). For instance, truthfulness of the information provided by the primary LLM was \nmeasured using the question “I trust my primary LLM will provide information that is factually accurate and reliable”. The \nscores for the 8 dimensions were summed to obtain the total score for Trustworthiness of primary LLM. The theoretical \nrange for the total score of trust in LLM is 0 to 80. The Cronbach Alpha for this measure was 0.908 which showed excellent \nreliability for trust in LLM scale. \nCharacteristics of LLM usage. We assessed frequency of LLM use; the method of interaction; the device primarily \nused for accessing LLM and the typical location of LLM use .  The frequency of LLM use was measured on an 11-point \nLikert scale ranging from 0 (“very unfrequently’) to 10 (“very frequently”) . The method of interaction with their primary \nLLM included the following options: text, voice, text and voice, and other. The question about the device they primarily \nuse for accessing their LLM included the following options: PC, mobile, tablet, smart speaker (e.g., Amazon Echo, Gthat \nparticioogle Nest), dedicated AI device (e.g., Humane AI Pin, Rabbit R1), and other. Participants were further asked to \nreport the typical location of LLM use, choosing from: stationary (e.g., at home, office, school), on the move (e.g., while \ncommuting, traveling), both - stationary and on the move, and other.  \n2.3 Procedure \nThis study was approved by the Ethical Committee at Bournemouth University, UK (N62239, 03.03.2025). The \nparticipants were recruited using Prolific online platform (www.prolific.com). The survey was developed and distributed \nusing SurveyMonkey (www.surveymonkey.com). The pilot test was conducted at the end of March 2025. All participants \nwere provided with information about the study and were required to provide informed consent prior to starting the \nquestionnaire. They were also made aware of their right to withdraw from study at any time. Participants who successfully \ncompleted the survey received monetary compensation. The main study was conducted between the end of March 2025 \nand mid-April 2025. The data associated with this work are available at the Open Science Framework (OSF).  \n9 \n2.4 Data Analysis \n2.4.1 Data Preprocessing. \nAfter initial inspection and removing participants who did not complete or failed attention checks, 532 participants \n(Mean age = 28.17, SD = 5.91, 50.29% males, 48.57% females, 1.14% non-binary and missing the response). Participants \nwho identified themselves as non-binary (2) and missing responses (4) were excluded from the analysis due to small groups \nsize.  \n2.4.2 Descriptive statistics. \nThe mean, standard deviation, and distribution of participants’ responses were calculated for each item to examine \ncentral tendency, variability, and overall response patterns. \n2.4.3 Item quality check procedures. \nPrior to entering data into analysis, item quality was assessed by testing the monotonicity assumption  [71]. \nAccording to this assumption, individuals with higher levels of the underlying trait (e.g., dependency on LLM) should be \nmore likely to select higher response options. Testing this assumption in the present study is essential for two reasons. \nFirst, it supports our scale reliability and dimensionality checks because violations of this assumption might indicate that \nthe item doesn’t behave consistently across the continuum of all measured items. Second, since items that violate \nmonotonicity may reflect multidimensionality or measurement error, identifying such items at the early stage of analysis \nhelps flag poorly constructed items that may confuse respondents or function differently across groups. The analysis was \ncarried out using the `mokken` package in R [71]. For each item, the analysis generated a scalability coefficient (H), the \nproportion of violated monotonicity comparisons, the maximum standardised violation (zmax), and a critical flag \nindicating whether violations exceeded predefined thresholds. Graphical plots of each item were also examined to confirm \nthe consistency of the findings (Supplementary Material 1). \n2.4.4 Exploratory Factor Analysis. \nThe aim of the  Exploratory Factor Analysis ( EFA) was to examine the underlying latent structure of the \nquestionnaire items and to identify coherent factors that reflect the conceptual dimensions of our constructs. The analysis \nwas conducted using R (Version 2024.12.1+563) with the psych and GPArotation packages [72]. \nSampling adequacy was assessed using the Kaiser -Meyer-Olkin (KMO) measure. To determine the optimal \nnumber of factors, a scree plot and parallel analysis were conducted. Both indicated support for a three-factor solution. As \nsuch, a three-factor model was extracted using Principal axis factoring (PAF) with Promax rotation. Principal axis factoring \n(PAF) was selected as the extraction method due to its suitability for identifying underlying latent constructs when the data \nmay not meet the assumption of multi variate normality. In addition, given the theoretical expectation that psychological \nconstructs are often interrelated rather than orthogonal, Promax rotation provides a more realistic representation of the data \nstructure by allowing for correlations among factors.  \nInitially, EFA was conducted on the full dataset to gain an understanding of the underlying structure and item \nbehaviour. Items with factor loadings of 0.50 or higher were retained for further analysis, while those falling below this \nthreshold, as well as items that failed to meet the criteria set out in the data quality check procedure, were excluded. \nFollowing this initial refinement, a second EFA was performed to re -examine the factor structure based solely on the \nretained items, with the aim of confirming the stability and interpretability of the emergent factor solution (Supplementary \nMaterial 2). \nTo cross-validate the structure identified in the EFA, we employed network psychometrics, which estimate direct \nrelationships between items rather than latent constructs  [73]. Using the R packages qgraph, bootnet, and igraph, we \nconstructed a partial correlation network from the cleaned questionnaire dataset, excluding reverse -coded and poorly \nperforming items. The network was estimated using the EBICglasso algorithm with default hyperparameters (γ = 0.5) for \n10 \nsparsity and interpretability  [74]. Community detection was performed using the walktrap algorithm on an adjacency \nmatrix derived from the weighted network  [75]. Centrality metrics (strength, closeness, and betweenness) were extracted \nto identify influential nodes, and bridge centrality was computed to locate items acting as connectors between communities \nusing the networktools package (Supplementary Material 3). \n2.4.5 Confirmatory factor analysis. \nFollowing the EFA, a Confirmatory Factor Analysis (CFA) was conducted to test the fit of the hypothesised factor \nstructure derived from the EFA using an independent sample. The CFA was performed using the lavaan package in R \n(version 0.6-19), employing the maximum likelihood estimation method. This approach was selected due to its capacity to \nproduce reliable parameter estimates under conditions of moderate sample size and approximate normality. The factor \nstructure tested in the CFA was based on the final solution obtained in the second EFA, with only those items retained that \nmet the criteria of loading ≥ .50 and passed all data quality checks. Each latent factor was modelled to load onto its \ncorresponding set of observed variables, and no cross-loadings were permitted, in line with a strict confirmatory approach. \nCovariances between latent variables were freely estimated to account for potential inter -factor correlations identified \nduring the EFA. Model fit was evaluated using multiple goodness-of-fit indices (absolute, incremental, and parsimonious \nfit) (Supplementary Material 4). \nThe chi-square (χ²) test was used as a measure of exact fit, where a non -significant result indicates a good fit \nbetween the model and the observed data. However, given the chi -square test’s known sensitivity to sample size, it was \ninterpreted alongside other fit indices. The Comparative Fit Index (CFI) and Tucker-Lewis Index (TLI) were examined as \nindicators of incremental fit, with values of 0.90 or above considered acceptable, and values of 0.95 or higher indicative \nof a good model fit. The Root Mean Squ are Error of Approximation (RMSEA) was also assessed, where values less than \nor equal to 0.08 suggest an adequate fit and values below 0.06 are indicative of a close fit; the 90% confidence interval \naround the RMSEA value was also reported. Finally, the St andardised Root Mean Square Residual (SRMR) was \nconsidered, with values at or below 0.08 viewed as evidence of acceptable model fit. \nThe adequacy of the measurement model was further assessed through the evaluation of individual parameter \nestimates. Standardised factor loadings were examined, with values of 0.50 or higher considered acceptable, and values \nabove 0.70 regarded as indicative of a strong association between observed indicators and their respective latent constructs \n[76]. The proportion of explained variance (R²) for each item was also reviewed to ensure that observed variables \ncontributed meaningfully to their designated factors. Additionally, the correlations between latent variables were inspected \nto assess discriminant validity and to ensure that multicollinearity was not present, with correlations below 0.85 considered \nacceptable [76]. \nPrior to conducting the confirmatory factor analysis, the data were screened for assumptions relevant to structural \nmodelling. Multivariate outliers were identified using Mahalanobis distance, and cases with p -values below .001 were \nexamined for potential exclusion. The pattern and extent of missing data were examined (1.78%), and data were assumed \nto be missing at random. Full information maximum likelihood (FIML) estimation was employed to handle missing values, \nas this approach allows for the inclusion of all available data and yields unbiased estimates under the MAR assumption. \n2.4.6 Reliability Testing. \nTo assess the internal consistency of the newly developed scale we conducted a reliability analysis for each subscale \n(factor) and the total scale score. First, Cronbach’s alpha was calculated to estimate the internal consistency of each factor \nand the full scale. Values above 0.70 were interpreted as acceptable, with higher thresholds (0.80 and 0.90) indicating good \nand excellent reliability, respectively [77]. Second, we computed item-total correlations for each item to assess how well \nan individual item correlated with the sum of the remaining items in its subscale. Higher values indicated that the item \ncontributed meaningfully to the construct. Third, we cal culated “alpha if item deleted” statistics to determine whether \n11 \nremoving any item would improve the overall internal consistency of its respective subscale. This metric helped to identify \nitems that may reduce the reliability of the scale. \n2.4.7 Validation. \nTo evaluate the convergent and discriminant validity of the LLM dependency scale, we conducted a set of \npsychometric analyses using item -level data for each of the t wo subscales: Instrumental Dependency  and Relationship \nDependency. Convergent validity was assessed through the calculation of Average Variance Extracted (AVE) and \nComposite Reliability (CR), while discriminant validity was examined using the Heterotrait –Monotrait (HTMT) ratio  \n(Supplementary Material 5). \nExternal validity of the LLM Dependency subscales was examined using both standardised multiple regression \nand relative importance analysis (Johnson’s weights). Regression models identified variables that uniquely predicted each \nsubscale score, controlling  for all other predictors. Relative importance analysis decomposed the total variance (R²) \nexplained by the model, quantifying each predictor’s contribution based on both unique and shared effects (i.e., capturing \ntheir relative impact). \nA multiple linear regression analysis was carried out for each of the subscales to test the external validity of the \nLLM Dependency Scale. The purpose of these analyses was to examine how well a set of established psychological and \nbehavioural concepts predicted scores on the LLM dependency dimensions. Each regression model included the following \nvariables as predictors: Internet Addiction Score (IAS), AI attitudes (ATAI Acceptance and Fear), need for cognition  \n(NFC) and perceived Trustworthiness of primary LLM. For each model, both unstandardised and standardised coefficients \nwere calculated to allow comparison of relative effect sizes. \nTo further evaluate the contribution of each external variable in explaining variance in the t wo dimensions of LLM \ndependency, we conducted a relative importance analysis using Johnson’s weights. Unlike standardised beta coefficients, \nwhich may underestimate the contribution of correlated variables, Johnson’s relative weights provide a more accurate \nestimation of the relative impact of predictors by orthogonalising them and calculating their contribution to the outcome \nvariable. Separate analyses were conducted for each of the two subscales of the LLM Dependency Scale. \n3 RESULTS \n3.1 Descriptive statistics \nAn overview of how participants responded to each item in the questionnaire is reported in Table 1. Skewness \nand kurtosis values were also examined to assess the normality of distribution. Most items displayed acceptable levels of \nskewness (between -1 and 1) and kurtosis (within ±2), suggesting no severe departures from normality. Item -to-item \ncorrelation matrix is reported in Supplementary Material 6. \nTable 1: Central tendency and normality of the data (N=526) for the initial 26 items \nItem code Mean SD Skewness Kurtosis \nresponses_personalized 4.772 0.831 -0.749 1.406 \nr_confidence_unchanged 2.817 1.189 0.451 -0.331 \nunderstands_me 4.061 1.230 -0.448 -0.300 \nless_confident_without 3.310 1.421 0.093 -0.891 \nimmersed_in_it 3.363 1.544 0.048 -1.112 \nr_spend_exactly_time 2.738 1.263 0.593 -0.261 \nshare_private_life 2.523 1.645 0.731 -0.806 \nr_no_expressing_emotions 3.335 1.567 0.099 -1.145 \ngenuine_companion 2.937 1.544 0.257 -1.139 \nfeel_less_alone 2.371 1.618 0.927 -0.459 \nadds_social_life 2.013 1.343 1.133 0.009 \nr_solely_tool 2.401 1.562 0.900 -0.420 \n12 \nItem code Mean SD Skewness Kurtosis \nno_need_talk_others 2.059 1.395 1.147 0.162 \nimproves_work 4.580 1.020 -0.837 1.073 \nmore_rewarded 3.992 1.319 -0.350 -0.597 \nempowers_me 3.829 1.431 -0.319 -0.878 \nless_time_life 2.861 1.428 0.512 -0.766 \nr_same_performance_without 3.146 1.243 0.185 -0.483 \nmore_work_without 3.943 1.400 -0.474 -0.605 \nadditional_brain 4.283 1.365 -0.875 0.102 \nr_less_credible_than_me 4.002 1.127 -0.419 -0.307 \nturn_to_when_able 3.835 1.371 -0.615 -0.497 \nr_same_ease 3.181 1.208 0.196 -0.333 \nr_equally_certain 3.027 1.201 0.246 -0.494 \nmy_go_to 3.897 1.431 -0.456 -0.707 \ndecisions_uneasy_without 2.766 1.426 0.495 -0.639 \n \n3.2 Item quality check \nOut of 26 items, the majority demonstrated satisfactory scalability and very few violations (Table 2). Twelve items \nshowed strong performance and can be retained confidently in the final scale. Seven items showed slightly more violations \nbut remained within acceptable psychometric limits. These items were included in the next step of analysis with a detailed \nanalysis on how they perform in factoring. The remaining seven items showed violation of monotonicity assumption. \nVisual inspection of the monotonicity plots confirmed the test leading to our decision to remove these items. \nTable 2: Monotonicity testing results for original items \nItem code H Percent Violated zmax Interpretation \nadds_social_life 0.38 0 0 Strong \nfeel_less_alone 0.39 0.01 7 Strong \nmy_go_to 0.37 0.01 8 Strong \nno_need_talk_others 0.39 0.02 6 Strong \nunderstands_me 0.3 0.02 16 Strong \ngenuine_companion 0.36 0.02 26 Strong \nless_confident_without 0.33 0.02 15 Strong \nturn_to_when_able 0.33 0.03 18 Strong \nshare_private_life 0.36 0.03 25 Strong \ndecisions_uneasy_without 0.38 0.03 18 Strong \nadditional_brain 0.35 0.03 17 Strong \nempowers_me 0.3 0.03 24 Strong \nmore_rewarded 0.31 0.04 54 Moderate \nmore_work_without 0.27 0.05 26 Moderate \nr_solely_tool 0.29 0.07 41 Moderate \nimproves_work 0.27 0.08 40 Moderate \nresponses_personalized 0.23 0.08 39 Moderate \nimmersed_in_it 0.37 0.01 22 Moderate \nless_time_life 0.24 0.03 37 Moderate \nr_confidence_unchanged 0.14 0.18 120 Problematic \nr_spend_exactly_time 0.15 0.22 143 Problematic \nr_same_performance_without 0.15 0.14 112 Problematic \nr_no_expressing_emotions 0.18 0.14 66 Problematic \nr_same_ease 0.18 0.25 149 Problematic \nr_equally_certain 0.17 0.22 101 Problematic \nr_less_credible_than_me 0.05 0.29 147 Problematic \nNote. H - scalability coefficient, Percent Violated the proportion of violated monotonicity comparisons,  \nthe maximum standardised violation (zmax) \n13 \n3.3 Exploratory Factor Analysis \nWe ran an EFA on a dataset comprising a randomly selected half of the participants (N = 263), using the set of 19 \nitems from the monotonicity check. The parallel analysis suggested 3 factors; however, the third factor was close to  \neigenvalue=1. Moreover, the 3-factor structure yielded in poor fit (RMSEA =0 .112, 90% CI [0.082, 0.132], TLI = 0. 791, \nSRMR=0.090, χ²(207) = 712.449, p < .001). The model fit for the two-factor EFA also yielded poor fit (RMSEA = 0.102, \n90% CI [0.092 to 0.112], TLI = 0.832, χ²(229) = 498.64, p < .001), suggesting that although the fit slightly improved, the \nmodel does not reproduce the observed covariance matrix well.  \nA detailed inspection revealed that several items either failed to load ≥ .50 on any factor or showed ambiguous \npatterning (Table 3). These items were removed from the item pool.  In addition, several items were removed based on \nevidence from the residual correlation matrix, which indicates the extent to which the factor model failed to account for \nobserved relationships among items. The following items exhibited notable residual co rrelations, suggesting local \ndependence or redundancy: (i) empowers_me had a high residual correlation of .26 with  more_work_without, the largest \namong all residuals observed in this cluster; (ii) more_work_without demonstrated residual correlations of .26 with \nempowers_me and –.11 with additional_brain, implying that these items share variance not adequately explained by the \ncurrent factor solution; (iii) additional_brain showed residual correlations of –.11 with more_work_without and .15 with \nmy_go_to. Although its primary loading was acceptable, the pattern of residuals suggests it may reflect variancassociated \nwith an unmodeled third factor, beyond the two -factor solution currently adopted; (iv) improves_work showed residual \ncorrelations with more_rewarded (.14) also suggests conceptual overlap within a cluster of performance-oriented items. \nAfter removing the items, we reran EFA with 2 and 3 factors. The 3-factor solution still did not show a good fit \n(RMSEA =0 .100, 90% CI [0.083, 0.118], TLI = 0. 890, SRMR=0.078, χ²(33) = 92.1, p < .001 ) while 2-factor structure \nshowed acceptable fit (RMSEA =0 . 078, 90% CI [0.0 71, 0.084], TLI = 0. 925, SRMR=0.05, χ²(53) = 73.1, p < .001). \nTherefore, we decided to retain the 2-factor solution. \nTable 3: Loading evaluation in EFA \nItem code Loading Communalities (h2) Concern \nresponses_personalized 0.4 0.22 Low loading \nunderstands_me 0.39 0.34 Low loading and unclear factor alignment  \nless_confident_without 0.58 0.39 Moderate communality \nimmersed_in_it 0.59 0.51 Acceptable  \nshare_private_life 0.8 0.64 Strong loading \ngenuine_companion 0.72 0.59 Strong loading \nfeel_less_alone 0.94 0.87 Very strong loading \nadds_social_life 0.71 0.61 Strong loading \nr_solely_tool 0.67 0.42 Acceptable loading \nno_need_talk_others 0.95 0.86 Very strong loading \nimproves_work 0.7 0.42 Acceptable loading \nmore_rewarded 0.68 0.48 Acceptable loading \nempowers_me 0.74 0.51 Strong loading \nless_time_life 0.37 0.2 Low loading and low communality \nmore_work_without 0.74 0.46 Strong loading \nadditional_brain 0.67 0.47 Acceptable loading \nturn_to_when_able 0.56 0.42 Moderate loading and communality \nmy_go_to 0.66 0.55 Acceptable loading \ndecisions_uneasy_without 0.58 0.49 Acceptable loading \nNote. Problematic items are highlighted in italic \n3.3.1 Cross-validation of the EFA using the Network Analysis. \nAfter removing all problematic items, we performed the Network analysis. We estimated a regularised partial \ncorrelation network using the EBICglasso algorithm. This method offers a complementary approach by examining the \n14 \ndirect statistical dependencies among observed items rather than positing latent variables. The resulting network was \nstructured, with 18 non-zero edges among 12 nodes, resulting in a moderate density of 0.273. \nCommunity detection using the walktrap algorithm identified two clearly delineated item clusters. The first \ncommunity consisted of items which aligned closely with the second factor from the EFA and reflect instrumental \ndependency (Figure 1). The second co mmunity grouped items, corresponding to the first EFA factor and representing \nrelationship dependency on LLMs. The overlap between these communities and the EFA factors provides strong structural \nvalidation for the two-dimensional latent model. \nIt has to be noted that centrality analysis revealed that the item decisions_uneasy_without had one of the highest \nstrength centrality scores, indicating its importance within the instrumental dependency cluster. Similarly, feel_less_alone \nemerged as a key node within the relationship cluster. Bridge centrality analysis indicated that while the two communities \nwere largely modular, several items served integrative functions across clusters. For example, adds_social_life and \nmore_rewarded showed elevated bridge strength, suggesting their dual relevance across both domains.  \n \nFigure 1: Network Structure of LLM Dependency scale after EFA. The plot displays partial correlation structure among questionnaire \nitems, identifying two distinct item communities: one representing instrumental/task dependency (blue nodes) and the other capturing \nrelationship dependency (red nodes). Solid edges denote stronger within-community associations, while dashed edges indicate inter-\ncommunity connections \n3.4 Confirmatory Factor Analysis \nThe aim of the confirmatory factor analysis (CFA) was to evaluate the factorial structure of our questionnaire using \nan independent sample (N=263).  \nThe model demonstrated a good overall fit. The chi -square test was significant (χ² = 133.98, df = 53, p < .001), \nwhich is common in large samples or with complex models, and thus not a sole indicator of misfit. The CFI was 0.958 and \nthe TLI was 0.947, both in line with  the conventional cutoff of 0.95 for good fit. The RMSEA was 0.076, which is \nmarginally above the commonly accepted threshold of 0.07, but the confidence interval 90% CI [ 0.060, 0.092] and \nassociated tests suggest it is not a poor fit. The SRMR was 0.046, indicating an excellent fit (values below 0.08 are generally \ndesired). Taken together, these indices suggest that the hypothesised two -factor model provides a reasonably good fit to \nthe data. \n\n15 \nAll items loaded significantly on their respective latent factors. All items in the first latent factor, labelled here as \n“Instrumental Dependence”, reflect a sense of dependency or reliance on an LLM to support work -related decisions and \nconfidence. Standardised factor loadings ranged from 0.569 to 0.778, all within an acceptable to strong range, and R -\nsquared values showed that each item shared a meaningful proportion of variance with the latent construct (ranging from \n32% to 61%) (Table 4). The content o f the second latent factor, Relationship Dependency, reflects a more affective, \nparasocial, and relational dimension in which an LLM as a companion or social presence. Standardised loadings for this \nfactor were also strong, ranging from 0.646 to 0.941, wit h two items (‘feel_less_alone’ and ‘no_need_talk_others’) \nshowing particularly high loadings above 0.9. \nTable 4: Factor loadings in the final LLM Dependency Scale  \nItem code Factor Loading R² \nless_confident_without Instrumental Dependency 0.706 0.499 \nimmersed_in_it Instrumental Dependency 0.715 0.511 \nmore_rewarded Instrumental Dependency 0.569 0.324 \nturn_to_when_able Instrumental Dependency 0.718 0.515  \nmy_go_to Instrumental Dependency 0.778 0.605 \ndecisions_uneasy_without Instrumental Dependency 0.754 0.569 \nshare_private_life Relationship Dependency 0.791 0.625 \ngenuine_companion Relationship Dependency 0.756 0.572 \nfeel_less_alone Relationship Dependency 0.941 0.885 \nadds_social_life Relationship Dependency 0.772 0.596 \nr_solely_tool Relationship Dependency 0.646 0.417 \nno_need_talk_others Relationship Dependency 0.927 0.860 \nTable 5 presents the final LLM-D12 Dependency Scale with its two components: Instrumental Dependency (6 items) \nand Relational Dependency (6 items). The final version of the questionnaire is provided in Appendix 1 and is suitable for \nuse in various contexts, including research. Appendix 2 provides a mapping of individual LLM-D12 items into underlying \ntheoretical constructs and measurements guiding the use of the scale at the item level. \nTable 5: LLM-D12: The Large Language Models Dependency 12-item Scale \nItem code Factor Question  \nless_confident_without Instrumental Dependency Without it, I feel less conﬁdent when making decisions. \nimmersed_in_it Instrumental Dependency I use it sometimes without realizing how much time I spend immersed in it. \nmore_rewarded Instrumental Dependency I feel much more rewarded and pleased when completing tasks using it. \nturn_to_when_able Instrumental Dependency I turn to it for support in decisions, even when I can make them myself with \nsome eﬀort. \n \nmy_go_to Instrumental Dependency It is my go-to for assistance in decision-making. \ndecisions_uneasy_without Instrumental Dependency Making decisions without it feels somewhat uneasy. \nshare_private_life Relationship Dependency I share details about my private life with it. \ngenuine_companion Relationship Dependency I interact with it as if it were a genuine companion. \nfeel_less_alone Relationship Dependency It helps me feel less alone when I need to talk to someone. \nadds_social_life Relationship Dependency It adds to my social life, making socializing more engaging and interesting. \nr_solely_tool [R] Relationship Dependency I use it solely as a tool, not to express my feelings or expect it to understand \nme. \nno_need_talk_others Relationship Dependency It helps me feel less alone, reducing the need to talk to others. \nNote. [R] indicates that the item shall be reversed when scoring. Items are rated on a 6-point scale: 1 = Strongly Disagree, 2 = Disagree, \n3 = Somewhat Disagree, 4 = Somewhat Agree, 5 = Agree, 6 = Strongly Agree. \n16 \n3.5 Validation \n3.5.1Internal consistency. \nInternal consistency for both factors was high. Cronbach’s alpha for Relationship Dependency was 0.91 and for \nInstrumental Dependency was 0.84, indicating excellent reliability for both scales. All items showed substantial item-\ntotal correlations, indicating that each contributes meaningfully to the overall construct (Table 6). \nTable 6: Item-level internal consistency of LLM-D12 \nRelationship Dependency Subscale Instrumental Dependency Subscale \nItem code Item-Total \ncorrelation \nCronbach’s \nAlpha if item \ndeleted \nItem code Item-Total \ncorrelation \nCronbach’s \nAlpha if item \ndeleted \nshare_private_life 0.765 0.893 less_confident_without 0.658 0.811 \ngenuine_companion 0.749 0.894 immersed_in_it 0.580 0.828 \nfeel_less_alone 0.865 0.877 more_rewarded 0.515 0.838 \nadds_social_life 0.712 0.900 turn_to_when_able 0.621 0.819 \nr_solely_tool [R] 0.607 0.915 my_go_to 0.714 0.800 \nno_need_talk_others 0.827 0.885 decisions_uneasy_without 0.656 0.811 \n3.5.2 Discriminant validity. \nTo evaluate discriminant validity between the two latent constructs, we used three tests. First, the Fornell-Larcker \ncriterion was applied by comparing the average variance extracted (AVE) for each construct to the squared inter-construct \ncorrelation. The AVE for the first factor, labelled Relation ship Dependency was 0.659, and for the second factor, \nInstrumental Dependency was 0.504. The squared correlation between the two latent variables was 0.503. As both AVE \nvalues exceeded the squared correlation, thi s test supports adequate discriminant validity between the two constructs. \nSecond, the Heterotrait -Monotrait Ratio of correlations (HTMT ) was also computed with the resulting HTMT value of \n0.589. This is well below the commonly used thresholds of 0.85 or 0.90, further supporting discriminant validity between \nour latent constructs. Finally, a chi -square difference test was performed to assess  whether modelling all items under a \nsingle latent factor would provide a comparable fit to the hypothesised two -factor model. The comparison revealed that \nthe one-factor model fit significantly worse than the two-factor solution. Specifically, the chi-square value increased from \n133.98 in the two -factor model to 507.74 in the one -factor model, with a difference of 373.76  (df = 1, p < .001). This \ndeterioration in fit provides further evidence that the two constructs are not interchangeable and should be treated as distinct \ndimensions. \n3.5.3 External validation. \nThe descriptive statistics for the external validation variables are presented in Table 7. These variables include \ncategorical variables (location of use, device used, and interaction method), and continuous variables (frequency of LLM \nuse, Trustworthiness of LLM, Internet Addiction, ATAI Acceptance and Fear, and Need for Cognition).  \n \nTable 7: Descriptive statistics for the external validation variables \n Total Sample (n = 526) \nCategorical Variables  N % \nPrimary LLM Interaction Method      \nText 456 87.19% \nText and Voice 67 12.81% \nPrimary LLM Device of Use      \nMobile  115 22.12% \nPC 405 77.89% \n17 \nPrimary LLM Location of Use      \nStationary (e.g., at home, office, school) 350 66.67% \nBoth – Stationary and On the Move 175 33.33% \nContinuous Measures Mean SD \nFrequency of LLM Use  8.49 1.38 \nTrustworthiness of LLM (Total Score) 57.40 13.63 \nInternet Addiction (Total Score) 19.27 5.85 \nATAI Acceptance (Total Score) 12.29 3.63 \nATAI Fear (Total Score) 13.21 5.99 \nNeed for Cognition (Total Score) 20.62 5.03 \nNote: The sums for the two categories in the categorical variables do not add up to the total (526) because responses to other categories \nwere nulled.  \n \nTo examine the pattern of associations between the dependency subscales and external variables, Pearson \ncorrelations were computed. Significant positive correlations were expected with theoretically related constructs such as \nTrustworthiness of LLM and Internet Addiction. Instrumental Dependency demonstrated strong positive correlations with \nTrustworthiness of LLM, Internet Addiction, and ATAI Acceptance, but weak negative correlation with ATAI Fear and \nNeed for Cognition (Figure 2). \nRelationship dependency showed a similar pattern, being positively associated with Trustworthiness of LLMs, \nInternet Addiction, and ATAI Acceptance, but not with Need for Cognition. The negative correlation with ATAI Fear was \nsmall and at borderline of significance (p=.048) (Figure 2). \n \nFigure 2: Associations between Instrumental and Relationship Dependency and Trustworthiness of LLM, Internet Addiction, ATAI \nAcceptance, ATAI Fear and Need for Cognition  \nWe also computed a series of tests to investigate whether the strength of associations between external validation \nvariables differed across the two dependency subscales (Instrumental Dependency and Relationship Dependency). Because \nboth correlations (e.g., Instrumental - Trustworthiness of LLMs and Relationship - Trustworthiness of LLMs) were derived \nfrom the same sample of participants, we applied the Steiger’s Z -test for dependent (overlapping) correlations  [78] \n\n18 \nimplemented in the cocor package in R). The results showed that Instrumental Dependency was more strongly associated \nwith Trustworthiness of LLMs than Relationship Dependency (Steiger’s Z = 3.76, p = .0002). Similarly, Instrumental \nDependency showed a stronger relationship with internet addiction compared to Relationship Dependency (Steiger’s Z = \n2.75, p = .006). No difference between Instrumental Dependency and Relationship Dependency were found for ATAI \nAcceptance Score (Steiger’s Z = 1.14, p = .25), ATAI Fear (Steiger’s Z = -0.32, p = .75) and Need for Cognition (Steiger’s \nZ = -0.87, p = .39).  \nWe further assessed the validity of our subscales by examining their relationships with theoretically relevant and \npreviously validated constructs ( Trustworthiness of LLMs , attitudes toward AI (ATAI: Fear and Acceptance), Need for \nCognition, Internet Addiction) as well as key characteristics of LLM usage. Figure 3 provides an overview of the analytical \nworkflow and main findings, followed by a detailed report. \n \nFigure 3: Workflow and main results of external validation \n \nMultivariate regression models were assessed to investigate the unique contributions of each external variable in \npredicting dependency subscale scores. For the Instrumental Dependency, the regression model was significant, F(5, 520) \n= 59.12, p < .001, with an adjusted R² of .36. Trustworthiness of LLMs (β = 0.18, p < .001), Internet Addiction (β = 0.31, \np < .001), and ATAI Acceptance (β = 0.37, p < .001) were significant positive predictors. Need for Cognition showed a \ntrend toward a negative association ( β = -0.09, p = .054), while ATAI Fear was not a significant predictor (β = 0.03, p \n= .485). The regression model for Relationship Dependency explained 20% of variation among the predictors F(5, 520) = \n26.52, p < .001). Trustworthiness of  LLMs (β = 0.11, p < .001), Internet Addiction (β = 0.21, p < .001), and ATAI \nAcceptance (β = 0.57, p < .001) predicted relationship dependency. Neither ATAI Fear (β = 0.04, p = .470) nor Need for \nCognition (β = -0.09, p = .141) were significant predictors. \nTo address the potential threat of common method bias arising from the use of self -report measures, a Harman’s \nsingle factor test was conducted using confirmatory factor analysis. We tested a model where all indicators loaded onto a \n\n19 \nsingle latent factor. The model demonstrated poor fit (χ²(14) = 194.16, p < .001, CFI = .76, TLI = .65, RMSEA = .156, and \nSRMR = .089) suggesting that no evidence of severe common method bias contaminating the observed relationships. It \nhas to be noted that while not exhaustive, Harman’s single factor test  widely used as a standard diagnostic tool  in self-\nreport questionnaire validation research (e.g., [79] [80]), and our approach aligns with previous studies in applied research \n(e.g., [81] [82]). Other procedural and statistical remedies, such as the use of a marker variable, the unmeasured latent \nmethod factor (ULMF) approach, or temporal and methodological separation, were not feasible in the context of this study. \nSpecifically, temporal separat ion was not applicable due to the single -session survey format, and the use of a marker \nvariable or latent method factor would have introduced unnecessary model complexity given the early -stage nature of \nconstruct development and the absence of established unrelated variables.  \nIn addition, we tested whether LLM usage characteristics predicted scores on the Instrumental and Relationship \nDependency subscales, using a multivariate multiple regression. The predictors included: frequency of LLM use, location \nof use, device used, and interaction method. Prior to the analysis,  categories with a small number of responses were \nexcluded. Regarding the location of use, most of the participants responded with ‘stationary’, followed by ‘both – stationary \nand on the move’. Only one participant  responded with ‘on the move’ and due to the small number of responses for this \ncategory, the response was nulled and removed from the subsequent analysis. For device used, many of the participants \nresponded with ‘PC’, followed by ‘mobile’ as their device of use for primary LLM. No participants had selected dedicated \nAI device or smart speaker, and o nly six participants had responded with ‘ tablet’ as their device of use for the primary \nLLM. Due to the small number of participants who selected this category, their response was nulled and removed from the \nsubsequent analysis. Lastly, most of the participants reported using ‘text’ to interact with their primary LLM, whereas a \nfew participants indicated they use ‘text and voice’ to interact with their primary LLM. Only two participants had selected \n‘voice’, and one participant had selected ‘oth er’ interaction method. These responses were nulled and removed from the \nsubsequent analysis. With this removal of categories with small number of responses, we were left with binary options for \neach of the categorical variables used in the external validation analysis.  \nA MANOVA using Wilks’ Lambda indicated a statistically significant multivariate effect across the two \ndependency outcomes (Instrumental and Relationship Dependency) for each predi ctor: frequency of use: Λ = .884, F(2, \n510) = 33.57, p < .001; location of use: Λ = .896, F(2, 510) = 29.59, p < .001; device: Λ = .975, F(2, 510) = 6.44, p = .002; \ninteraction method: Λ = .944, F(2, 510) = 15.19, p < .001. These results suggest that the set of predictors reliably explained \nvariation in the combined dependency outc omes. Follow-up univariate regressions were conducted for each dependency \nsubscale separately.  \nThe regression model predicting Instrumental Dependency was significant, F(4, 511) = 24.20, p < .001, accounting \nfor 15.9% of the variance (R² = .159, adjusted R² = .153). Frequency of LLM use was a significant positive predictor (β = \n1.36, SE = 0.19, t = 7.18, p < .001), indicating that more frequent use was associated with higher Instrumental Dependency \nscores. Individuals who used LLMs stationary reported significantly lower scores of Instrumental Dependency than those \nwho used them in both stationary and mobile settings (β = -1.96, SE = 0.59, t = -3.33, p < .00 1). In addition, users who \nengaged with LLMs using both text and voice input methods reported higher levels of Instrumental Dependency compared \nto those who used text only (β = 2.48, SE = 0.82, t = 3.03, p = .003). Device type was not a significant predict or in this \nmodel (p = .79) \nThe regression model for Relationship Dependency was also significant, F(4, 511) = 29.97, p < .001, and explained \n19.0% of the variance (R² = .190, adjusted R² = .184). Frequency of use was a significant positive predictor (β = 0.68, SE \n= 0.22, t = 3.06,  p = .002), with individuals who used LLMs more frequently reporting higher Relationship Dependency \n20 \nscores. Those who use LLMs in both stationary and on the move reported significantly higher levels of Relationship \nDependency compared to those using it in stationary settings (β = 3.30, SE = 0.69, t = 4.79, p < .001). Similarly, individuals \nusing LLMs primarily on mobile devices reported higher Relationship Dependency scores than those using it primarily on \nPCs (β = 2.21, SE = 0.76, t = 2.93, p = .004). The interaction method was also significant, with users who engaged via both \ntext and voice input report ing substantially higher Relationship Dependency than those using text only (β = 5.23, SE = \n0.95, t = 5.49, p < .001). \nTo test whether these predictors had different effects across the two types of dependency, Z -tests were conducted \ncomparing the standardised regression coefficients. Frequency of use was more strongly associated with Instrumental \nDependency than with Relat ionship Dependency (z = 2.35, p = .019). Device type had a significantly greater effect on \nRelationship Dependency (z = 2.05, p = .040), and interaction method also showed a stronger association with Relationship \nDependency (z = 2.19, p = .028). These resu lts support the theoretical distinction between the subscales and suggest that \ndifferent aspects of LLM interaction are differentially linked to instrumental and relational forms of dependency. \n4 DISCUSSION \nThe present study makes an important theoretical and methodological contribution  to the field of human –AI \ninteraction by developing and validating  a novel scale, LLM-D12, that captures distinct forms of psychological reliance \non LLMs. Guided by recent theoretical perspectives on human-AI interaction, the questionnaire was constructed to reflect \nhow LLM use may exhibit symptoms of habitual or preferred reliance [83] [19].Our analysis provides compelling evidence \nfor the psychometric soundness and conceptual clarity of the newly developed LLM Dependency Scale (LLM-D12). This \nscale captures two distinct but related dimensions of dependency: Instrumental Dependency and Relationship Dependency. \nThe reliability of the LLM-D12 was high across both subscales, demonstrating strong internal consistency, reliability, and \ndiscriminant validity. \n4.1 Instrumental Dependency subscale \nThe Instrumental Dependency subscale comprises six items that together capture users’ reliance on LLMs for \ncognitive and decision -making support. Each item addresses a distinct but complementary facet of how individuals \nintegrate LLMs into their task management strategies. One important dimension, which may be labeled as dependency on \ncognitive support, reflects the tendency to feel less confident without the LLM and to experience uneasiness when making \ndecisions in its absence. These behavioural dependenci es are well explained by Cognitive Offloading Theory, which \nproposes that individuals increasingly shift cognitive effort onto external systems to conserve internal resources and reduce \nuncertainty [7]. A second facet, task absorption through assistance, is measured by the item assessing immersion in the \nLLM. This form of cognitive engagement where users become absorbed in activities that match their skills and provide \nimmediate feedback, consistent with the principles of Flow Theory [84]. Another dimension, habitual resource seeking, is \nembodied in turning to the LLM when possible and using it as a habitual ‘go-to’ resource. This behaviour aligns with Habit \nFormation Theory, which suggests that repeated successful interactions lead to automatic behavioural patterns over time  \n[85]. All these items showed strong loading (all ≥ 0.70) to the Instrumental Dependency subscale and explained variances \nwere between about 0.50 and 0.61, suggesting that each item shares a substantial proportion of its variance with the factor. \nIt has to be noted that the item feeling more rewarded through LLM use has a comparatively lower factor loading \nthan the other Instrumental Dependency items. However, its inclusion is theoretically and psychometrically justified. From \na conceptual perspective, experiencing reward has been linked to the motivational architecture that sustains cognitive \n21 \noffloading [86], habitual behaviours [87], and decision -making [88]. Because this item taps into a broader motivational \nprocess, it may show a slightly lower loading but still plays an important role in explaining why dependency persists over \ntime. For example, it was suggested that such items are often characterised by moderately lower factor loadings because \nthey represent underlying drivers rather than discrete actions  [89]. Psychometrically, the item demonstrates acceptable \nproperties. Despite the relatively lower factor loading, it remains well above conventional thresholds for retention, and the \nsquared multiple correlation (R² = 0.324) indicates a substantial proportion o f explained variance. Furthermore, the item \nshowed a strong item-total correlation, and its removal did not result in any improvement in Cronbach’s alpha, suggesting \nthat it contributes positively to the internal consistency of the scale. \nBased on the content of the Instrumental Dependency subscale, we propose the following definition of this form \nof LLM dependency as a working construct. Instrumental Dependency is a multidimensional form of overreliance on large \nlanguage models for cognitive and decision-making support in the context of task management. \n4.2 Relationship Dependency subscale \nThe Relationship Dependency subscale captures a novel and increasingly relevant phenomenon: the emergence of \nparasocial bonds between users and LLMs. The subscale reflects users’ cognitive and behavioural tendencies to perceive \nand engage with LLMs as soci ally meaningful agents [19]. Each item contributes to this conceptualisation. Specifically, \nthe willingness to share private life with the LLM measures the extent to which users ascribe a degree of confidentiality \nand responsiveness to the model, a behaviour consistent with parasocial  processes identified in media psychology [90]. \nAnother key facet is the attribution of companionship: perceiving an LLM as a genuine companion reflects users’ tendency \nto anthropomorphise and socially personify technology, in line with research showing that individuals form one -sided \nsocial bonds with media and AI agents [91]. The subscale also measures the use of LLMs as a means of mitigating social \nisolation. This reflects users’ perceptions that interactions with the LLM help them feel less alone, a phenomenon linked \nto the social surrogacy hypothesis which proposed that technological interactions can partially substitute for human social \ncontacts [92]. The idea that LLMs add to one’s social life further extends this concept, assessing the integration of LLM \ninto an individual’s sense of social connectedness  [93]. Finally, the reduced perceived need to engage with other people \nwhen using LLMs reflects a deeper substitution effect, where LLM interaction is seen as fulfilling some of the roles \ntraditionally occupied by human social partners, a trend also observed in studies of robotic companionship [94]. \nPsychometric results confirm  the coherence of these constructs. All items loaded substantially onto a single \nrelational dependency factor, with especially high loadings observed for perceived reduction in loneliness and social \nsubstitution, suggesting that these are central features of parasocial bonding with LLMs. The communalities were \nconsistently high, indicating that the items share a large proportion of their variance with the underlying relational \nconstruct. Together, the results support the validity of the Relationship Dependency subscale as a reliable measure of users’ \nemerging parasocial engagement with LLMs. \nBased on the scope of the Relationship Dependency subscale, we articulate the following definition of this type of \nLLM dependency. Relationship Dependency refers to users’ cognitive and behavioural tendency to engage with large \nlanguage models as socially meaningful agents, forming parasocial bonds that reflect perceived companionship, emotional \ncloseness, and social fulfilment. \n22 \n4.3 Instrumental and Relationship Dependency as distinct but related constructs \nOur findings indicate that Instrumental and Relationship Dependency represent distinct, though related, dimensions \nof users’ engagement with LLMs. Several lines of evidence support this conclusion. First, the Average Variance Extracted \n(AVE) for each subsc ale exceeded the squared correlation between them, suggesting that each construct captures more \nunique variance than it shares. Second, the HTMT was well below the conventional thresholds for discriminant validity \n[95], further confirming that the subscales do not simply reflect variations of a single underlying factor. Third, a chi-square \ndifference test demonstrated a significant deterioration in model fit when collapsing the two dimensions into a single latent \nconstruct, providing direct empirical support for the superiority of a two-factor solution.  \nThe distinction between Instrumental Dependency and Relationship Dependency reflects well-established ideas in \nthe psychology of human -technology interaction. A growing body of research shows that digital technologies, and \nincreasingly AI systems, are used to meet both practical and social needs, though they do so through different psychological \nmechanisms and patterns of use [96]. For instance, instrumental engagement typically arises when technologies provide a \nclear functional benefit (e.g., helping users to make decisions, solve problems, complete tasks more efficiently , generate \nideas, or solve complex problems, while also feeling more confident, efficient, or empowered when using it ). Such usage \naligns with theories of cognitive offloading  [97] and distributed cognition  [98], suggesting that incorporating these \ntechnologies into mental routines can also come with a growing reliance on them [99]. \nIn contrast, Relationship Dependency emerges when users begin to relate to LLM in socially meaningful ways. \nThis form of dependency is not about emotional attachment in the traditional sense, but rather about perceiving the LLM \nas something more than a too l (e.g., a kind of interactive partner , using LLMs for emotional support, sharing personal \nexperiences with the model, seeking validation or empathy, or consistently attributing human-like qualities). For example, \nusers may turn to the LLM for conversation when feeling isolated, or find comfort in its availability, even when they know \nit is not truly sentient. Research has shown that people often respond to conversational agents in ways that mirror human \ninteraction, applying social norms such as politeness or reciprocity, and even forming one -sided relationships over time \n[83] [100].  \nThese parasocial responses are thought to emerge particularly in systems with natural language abilities, where \nconversational cues encourage users to treat technology as socially present [15]. Relationship Dependency, therefore, \nreflects a deeper integration of the LLM into the user’s social world. However, reliance on LLMs for social connection \nmay, over time, strengthen the dependency on these systems as preferred sources of interaction. This dynamic has been \nwell-documented in the social media literature, where users who turn to digital platforms for social engagement often begin \nto favour online over offline interactions. For example, one psychological mechanism underpinning this shift is the  \nCompensatory Internet Use model, which suggests that individuals may turn to online technologies to cope with unmet \nsocial or emotional needs, and that this coping strategy can gradually displace real-life social engagement [101]. This shift \nmay make them particularly appealing to individuals who experience anxiety, rejection sensitivity, or loneliness in \ntraditional social contexts [102]. In such cases, social reliance on LLMs may move from occasional support to a form of \nrelational dependency, especially if it consistently offers users psychological relief and control. \nTaken together, Instrumental Dependency appears to be driven primarily by the perceived cognitive utility of \nLLMs. While the moderate correlation between the two subscales indicates conceptual relatedness, it also reinforces their \ndistinction, supporting the view  that human-LLM engagement encompasses both instrumental  and relational forms of \ninteraction. This is consistent with broader perspectives in human-computer interaction, which argue that instrumental and \n23 \nrelational uses of technology often coexist and interact  [103]. The perception of LLMs as reliable and rewarding \ninstrumental tools may also shift to a gradual attribution of social agency. This is likely because LLM has been designed \nto mimic human conversational patterns  [104]. Similarly, relational engagement may, in turn, reinforce instrumental use \nby increasing trust and habitual reliance. Thus, the two forms of LLM dependency reflect different but interrelated aspects \nof users’ integration of LLMs into their cognitive and social environments. \n4.4 External validation of instrumental and relationship dependency \nThe observed associations between the LLM Dependency subscales and external psychological variables offer \nsupport for the theoretical and construct validity of the subscales. The Instrumental Dependency subscale was most strongly \nassociated with trust in LLMs, internet addiction, and positive attitudes towards AI, which aligns with its conceptualisation \nas a form of cognitive reliance [105]. It has to be noted that trust is central to any instrumental use of algorithmic systems, \nas users must believe in the system’s reliability, accuracy, and utility in order to delegate cognitive tasks  [106]. It was \nidentified as a key determinant of technology adoption and integration, particularly when systems are perceived to enhance \ndecision-making or productivity  [107]. The association between Instrumental Dependency and trust in AI may explain \nusers’ confidence in the tool’s competence, reliability, and functional utility  [105] [108]. Similarly, the association with \ninternet addiction suggests that habitual or excessive engagement with LLMs may reflect broader digital dependency \npatterns, particularly when the technology becomes embedded in daily cognitive tasks. However, it is notewo rthy that \nInstrumental Dependency was negatively related to Need for Cognition, albeit weakly. This suggests that individuals with \nlower intrinsic motivation for effortful thinking may be more likely to offload cognitive tasks to LLMs  [109] [7]. \nInterestingly, our analysis revealed positive but modest association between the Instrumental Dependency and internet \naddiction. The moderate size of the correlation is in line with our prediction that these constructs share some behavioural \noverlap but are driven by distinct psychological mechanisms. For instance, Instrumental Dependency appears to reflect \nusers’ deliberate integration of LLMs into problem-solving and decision-making routines rather than indication compulsive \nusage. \nThe Relationship Dependency displayed a different relationship. While it also correlated with trust and internet \naddiction, the associations were significantly weaker and unrelated to cognitive effort. This pattern is consistent with \nprevious research suggesting that users can form one -sided social bonds with AI agents in response to unmet social or \nrelational needs  [90] [110]. The absence of a significant link with Need for Cognition suggests that Relationship \nDependency is not driven by cognitive motivation, but rather by social and emotional context particularly among users \nwho experience LLMs as accessible, low-risk companions.  \nImportantly, these relationships remained consistent despite tests indicating no evidence of severe common method \nbias, which enhances confidence that the associations observed are not the result of shared measurement artefacts. \n4.5 New type of dependency or extension of Internet or social media addiction? \nTheoretical and empirical research into behavioural dependency on digital technologies is still in its infancy, that \nplaces constraints on how dependency can be defined, measured, and interpreted  [111]. Our findings contribute to a \ngrowing body of evidence suggesting that LLM dependency may represent a qualitatively distinct form of behavioural \nreliance, one that is not easily captured by existing models developed for internet addiction or gaming disorder. \nWhile clinical frameworks such as ICD -11 or DSM -5 provide useful starting points and this was recently \ndemonstrated [110], they may overlook critical features specific to LLM use. For example, unlike gaming or general \n24 \nscreen-based activity, LLM interactions are typically language -driven, goal -oriented, and cognitively framed, often \nembedded in contexts such as work, education, or parasocial  bounding [19]. The development of our scale suggests that \nthis may be the case. Several items initially included in the LLM Dependency Scale were removed due to poor \npsychometric performance, including low factor loadings, violations of monotonicity, or high residual cor relations. \nNotably, these excluded items align closely with established symptoms of Internet or Social Media Addiction (e.g., time \npreoccupation, loss of control, and compensatory use for emotional regulation [97] [112]. For instance, items assessing the \nperception of spending ‘exactly the right amount of time’ or expressing uncertainty about performance without the LLM \nreflect more generalised concerns about digital overuse than LLM -specific behaviours. Similarly, items  about being \n‘empowered’ or ‘understood’ by the LLM can related to a broader gratification or self-related feedback often seen in social \nmedia use. Their exclusion from the final factor structure strengthens the argument that LLM Dependency is not simply a \nsubset of existing digital addiction models, but rather reflects a novel behavioural pattern driven by distinct psychological \nmechanisms such as cognitive offloading, trust in algorithmic assistance, and parasocial engagement with LLM. \nIt has to be noted that recent studies on AI-based companionship and digital assistants (e.g., Alexa, Replika) show \nusers may form stable, intentional relationships with AI, without necessarily losing control or experiencing harm [113], as \nrequired by clinical addiction models. However, what remains unclear is identifying the point at which such companionship \nshifts from being a supportive interaction to a form of psychological dependency, particularly when the LLM begins to \nreplace rat her than complement human relationships. This distinction warrants further conceptual development, \nparticularly in designing diagnostic tools that can differentiate adaptive reliance on LLM from emerging forms of \nfunctional or relational overdependence. \nAnother important angle to consider is whether LLM dependency can be a manifestation of underlying personality \ntraits. Converging models of technology -related behavioural disorders indicate that “addictive -like” engagement often \nreflects predisposing person factors , most prominently impulsivity and neuroticism , rather than mere exposure to a \nplatform. For example, t he I-PACE framework specifies that trait vulnerabilities (e.g., impulsivity, negative affectivity) \ninteract with affective/cognitive responses and executive control to drive compulsive, cue-reactive use [114]. By extension, \nLLM dependency may arise when these traits meet the immediate, on -demand reinforcement LLMs provide ( e.g., rapid \nfeedback, novelty, social-surrogate interaction). However, testing this explicitly is a key next step for construct validation \nof LLM-dependency scales. \n4.6 Limitations and future directions \nFuture research should work towards refining criteria that capture the unique psychological mechanisms and usage \npatterns associated with LLMs. Our findings indicate that LLM dependency does not follow the typical profile of digital \naddiction but instead i nvolves distinct instrumental and relational components that reflect users’ integration of these \nsystems into everyday thinking and interaction.  However, given the cross -sectional nature of the present study, we are \nunable to determine how these forms of dependency may evolve over time. Longitudinal research will be particularly \nimportant in assessing whether reliance on LLMs remains a stable and f unctional part of users’ cognitive and social \nroutines, or whether it develops into a more problematic form of behavioural dependency.  \nAnother limitation of our study is the use of a convenience sample recruited through online platforms. While this \napproach is widely used in research on digital technologies, especially in early -stage work where participant pools are \noften self-selecting and interest-driven [115], it restricts the generalisability of the findings [116]. Participants who actively \nengage with LLMs and voluntarily respond to surveys may not reflect broader user populations, particularly those with \n25 \nlimited exposure to or different attitudes towards AI. Moreover, cultural and demographic homogeneity within such \nsamples may obscure important variation in how dependency develops. Although true random sampling in LLM research \nis difficult to achieve due to the distributed and self -regulated nature of usage, future studies should aim for more diverse \nand representative samples wherever possible. In particular, testing the structure and expression of Instrumental and \nRelationship Dependency across different cultural contexts would be an important next step.  \nMoreover, in the present study we did not collect qualitative data on participants’ perception of instrumental and \nrelationship dependency which may have provided additional depth and nuance to the interpretation of LLM dependency. \nWhile the focus of this study was on the psychometric development and validation of the proposed subscales, future phases \nof this work will incorporate qualitative methods to support item refinement and further conceptual clarification. \nIt has to be noted that the majority of our participants were aged 18 –40 years, with only a small proportion being \nolder. This age range was expected to have greater familiarity and engagement with LLMs, as younger and middle -aged \nadults are more likely to  adopt and integrate emerging digital technologies into daily life. However, patterns of use and \ndependency in adolescents and elderly individuals could differ due to developmental, cognitive, and generational \ndifferences in technology adoption. Future res earch should adapt and validate the present scale for these age groups to \nensure its applicability across the lifespan. \nA key direction for future research is the cross -cultural validation of the LLM Dependency Scale (LLM -D12). \nWhile the current study, conducted with a UK-based sample, provides strong initial evidence for its psychometric validity, \nthis sampling frame may l imit the applicability of the findings in a broader global context. Cultural norms strongly \ninfluence how people relate to technology, including expectations around autonomy, trust in automation, and attitudes \ntoward socially responsive systems. Such cultu ral factors can shape both instrumental and relational reliance on LLMs, \npotentially leading to differences in the expression or structure of LLM dependency across societies. Validating the scale \ncross-culturally would enable researchers to determine wheth er LLM dependency reflects a generalisable psychological \nconstruct or one that is shaped by culture -specific values, practices, and technology adoption patterns. Moreover, such \nwork could help identify cultural moderators of potentially problematic forms o f dependency, contributing to a more \nnuanced understanding of how human -LLM relationships develop globally. Because our scale is grounded in the core \npsychological dynamics of human-LLM interaction, it offers a suitable foundation for such comparative work. \nFinally, while frequency of use was included as a predictor in our external validation analyses, we acknowledge \nthe broader potential of exploring LLM use patterns , such as time spent, content shared, and types of tasks performed , as \na means of generating new insights into user -LLM interaction. We now consider such investigations as a direction for \nfuture research focused on understanding behavioural dynamics which can facilitate our understanding of overreliance on \nLLM. \n5 CONCLUSION \nThe LLM Dependency Scale (LLM-D12) is a newly developed and validated tool for assessing users’ instrumental \nand relationship dependency on large language models. By distinguishing between Instrumental Dependency and \nRelationship Dependency, the scale moves beyond clinically oriented models  of internet or social media addiction to \ncapture forms of engagement specific to LLM use. This distinction reflects growing interest in understanding not just \nwhether people overuse AI technologies, but how they come to perceive these systems as cognitively useful or socially \nmeaningful. The scale does not assume that dependency is inherently negative but offers a structure for identifying when \npatterns of reliance may become problematic. As LLMs become increasingly embed ded in personal, academic, and \n26 \nprofessional settings, LLM-D12 provides a theoretically grounded foundation for future research into the psychological \nimplications of this emerging form of technology engagement.  It may also support the development of responsible AI \ndesign, user feedback mechanisms, and digital literacy programs aimed at mitigating emerging forms of overdependence. \nACKNOWLEDGMENTS \nOpen Access Fund has been provided by Qatar National Library. This publication was supported by NPRP - 14-Cluster \nGrant # NPRP 14 C - 0916–210015 from the Qatar National Research Fund (a member of the Qatar Foundation). The \nfindings herein reflect the work and are solely the authors’ responsibility. \nAUTHORS’ CONTRIBUTION  \nAY, ML, and RA wrote the original paper in [19], which served as the foundation for this work. AB, AY, and RA developed \nthe first version of the scale. RA, AY, and AB revised the scale following the pilot test. ML, SA, and SR reviewed and \nprovided feedback during both phases. SA, AB, RA, and AY curate d the data. AY designed and conducted the statistical \nanalysis and reported it along with the discussion. RA contributed to the data analysis. AY, AB, SR and ML  wrote the \ninitial draft. SA and RA critically reviewed and revised the manuscript. RA secured the funding. \nSUPPLEMENTARY MATERIAL \nThe study design, dataset, and analysis files are available on the Open Science Framework (OSF) at the following link: \nhttps://osf.io/5fxq7, DOI 10.17605/OSF.IO/5FXQ7 \nREFERENCES \n[1] Dimension Market Research, “Explosive Growth Predicted: Large Language Model Market Set.” Accessed: May 05, 2025. [Online]. A vailable: \nhttps://www.globenewswire.com/news-release/2024/04/16/2864042/0/en/Explosive-Growth-Predicted-Large-Language-Model-Market-Set-to-\nReach-USD-6-5-Billion-by-2024-To-USD-140-8-Billion-by-2033-Dimension-Market-Research.html \n[2] H. Jones, “The Automation Takeover: Are Software Engineers Becoming Obsolete?” Accessed: May 05, 2025. [Online]. Available: \nhttps://www.forbes.com/sites/hessiejones/2024/09/21/the -automation-takeover-are-software-engineers-becoming-obsolete/ \n[3] V. C. Storey, W. T. Yue, J. L. Zhao, and R. Lukyanenko, “Generative Artificial Intelligence: Evolving Technology, Growing Soc ietal Impact, and \nOpportunities for Information Systems Research,” Information Systems Frontiers , pp. 1 –22, Feb. 2025, doi: 10.1007/S10796 -025-10581-\n7/TABLES/4. \n[4] R. Raman, R. Kowalski, K. Achuthan, A. Iyer, and P. Nedungadi, “Navigating artificial general intelligence development: socie tal, technological, \nethical, and brain -inspired pathways,” Sci Rep , vol. 15, no. 1, pp. 1 –22, Dec. 2025, doi: 10.1038/S41598 -025-92190-\n7;SUBJMETA=117,1759,2787,639,704,705,844;KWRD=COMPUTER+SCIENCE,PSYCHOLOGY+AND+BEHAVIOUR,SOCIOECONOMIC\n+SCENARIOS. \n[5] J. D. Lee and K. A. See, “Trust in automation: Designing for appropriate reliance,” Hum Factors , vol. 46, no. 1, pp. 50 –80, 2004, doi: \n10.1518/HFES.46.1.50_30392. \n[6] M. Brenner et al., “A systematic concept analysis of ‘technology dependent’: challenging the terminology,” Eur J Pediatr , vol. 180, no. 1, Jan. \n2021, doi: 10.1007/S00431-020-03737-X,. \n[7] E. F. Risko and S. J. Gilbert, “Cognitive Offloading,” Trends Cogn Sci, vol. 20, no. 9, pp. 676–688, 2016, doi: 10.1016/j.tics.2016.07.002. \n[8] R. Heersmink, “Use of large language models might affect our cognitive skills,” Nat Hum Behav , vol. 8, no. 5, pp. 805 –806, May 2024, doi: \n10.1038/S41562-024-01859-Y;SUBJMETA=179,4007,438;KWRD=ETHICS,PHILOSOPHY.  \n[9] L. J. Moore and M. Gerlich, “AI Tools in Society: Impacts on Cognitive Offloading and the Future of Critical Thinking,” Societies 2025, Vol. 15, \nPage 6, vol. 15, no. 1, p. 6, Jan. 2025, doi: 10.3390/SOC15010006.  \n[10] C. Zhai, S. Wibowo, and L. D. Li, “The effects of over-reliance on AI dialogue systems on students’ cognitive abilities: a systematic review,” Smart \nLearning Environments 2024 11:1, vol. 11, no. 1, pp. 1–37, Jun. 2024, doi: 10.1186/S40561-024-00316-7. \n[11] Z. Elyoseph, D. Hadar -Shoval, K. Asraf, and M. Lvovsky, “ChatGPT outperforms humans in emotional awareness evaluations,” Front Psychol, \nvol. 14, p. 1199058, May 2023, doi: 10.3389/FPSYG.2023.1199058/BIBTEX.  \n[12] H. R. Lawrence, R. A. Schneider, S. B. Rubin, M. J. Matarić, D. J. McDuff, and M. Jones Bell, “The Opportunities and Risks of  Large Language \nModels in Mental Health.,” JMIR Ment Health, vol. 11, no. 1, p. e59479, Jul. 2024, doi: 10.2196/59479.  \n[13] V. Sorin et al. , “Large Language Models and Empathy: Systematic Review.,” J Med Internet Res , vol. 26, no. 1, p. e52597, Dec. 2024, doi: \n10.2196/52597. \n27 \n[14] C. Liang, P. Zhou, and Y. Xie, “Is Anthropomorphism Better for Older People? Exploring the Effects of Anthropomorphic Compani on Robots on \nSatisfaction,” Int J Hum Comput Interact, pp. 1–15, Apr. 2025, doi: 10.1080/10447318.2025.2496429. \n[15] J. Kim and I. Im, “Anthropomorphic response: Understanding interactions between humans and artificial intelligence agents,” Comput Human \nBehav, vol. 139, p. 107512, Feb. 2023, doi: 10.1016/J.CHB.2022.107512.  \n[16] OpenAI, “GPT-4o System Card | OpenAI.” [Online]. Available: https://openai.com/index/gpt -4o-system-card/ \n[17] M. Chandra et al., “Longitudinal Study on Social and Emotional Use of AI Conversational Agent,” arXiv preprint arXiv:2504.14112 , Apr. 2025, \nAccessed: May 05, 2025. [Online]. Available: https://arxiv.org/pdf/2504.14112  \n[18] J. Coda -Forno et al. , “Inducing anxiety in large language models can induce bias,” Apr. 2023, Accessed: May 05, 2025. [Online]. Available: \nhttps://arxiv.org/pdf/2304.11111 \n[19] A. Yankouskaya, M. Liebherr, and · Raian Ali, “Can ChatGPT Be Addictive? A Call to Examine the Shift from Support to Dependen ce in AI \nConversational Large Language Models,” Human-Centric Intelligent Systems 2025 5:1 , vol. 5, no. 1, pp. 77 –89, Feb. 2025, doi: 10.1007/S44230 -\n025-00090-W. \n[20] A. Kaya, N. Türk, H. Batmaz, and M. D. Griffiths, “Online Gaming Addiction and Basic Psychological Needs Among Adolescents: The Mediating \nRoles of Meaning in Life and Responsibility,” International Journal of Mental Health and Addiction 2023 22:4, vol. 22, no. 4, pp. 2413–2437, Jan. \n2023, doi: 10.1007/S11469-022-00994-9. \n[21] C. S. Andreassen et al., “The Relationship between Addictive Use of Social Media and Video Games and Symptoms of Psychiatric Disorders: A \nLarge-scale Cross-sectional Study,” Psychol Addict Behav, vol. 30, no. 2, pp. 252–262, 2016, doi: 10.1037/adb0000160. \n[22] M. Kosinski, “Evaluating large language models in theory of mind tasks,” Proc Natl Acad Sci U S A, vol. 121, no. 45, p. e2405460121, Nov. 2024, \ndoi: 10.1073/PNAS.2405460121/SUPPL_FILE/PNAS.2405460121.SAPP.PDF.  \n[23] J. M. Mittelstädt, J. Maier, P. Goerke, F. Zinn, and M. Hermes, “Large language models can outperform humans in social situat ional judgments,” \nSci Rep , vol. 14, no. 1, p. 27449, Dec. 2024, doi: 10.1038/S41598 -024-79048-\n0;SUBJMETA=117,2811,477,631,639,705;KWRD=COMPUTER+SCIENCE,HUMAN+BEHAVIOUR,PSYCHOLOGY.  \n[24] S. C. Yu, H. R. Chen, and Y. W. Yang, “Development and validation the Problematic ChatGPT Use Scale: a preliminary report,” Current \nPsychology, vol. 43, no. 31, pp. 26080–26092, Aug. 2024, doi: 10.1007/S12144-024-06259-Z/TABLES/4. \n[25] X. Zhang, M. Yin, M. Zhang, Z. Li, and H. Li, “The Development and Validation of an Artificial Intelligence Chatbot Dependenc e Scale,” \nCyberpsychol Behav Soc Netw, vol. 28, no. 2, Feb. 2025, doi: 10.1089/cyber.2024.0240.  \n[26] Z. Li, Z. Zhang, M. Wang, and Q. Wu, “From assistance to reliance: Development and validation of the large language model dep endence scale,” \nInt J Inf Manage, vol. 83, p. 102888, Aug. 2025, doi: 10.1016/J.IJINFOMGT.2025.102888.  \n[27] G. Northoff, “Is the self a higher -order or fundamental function of the brain? The ‘basis model of self -specificity’ and its encoding by the brain’s \nspontaneous activity,” Cogn Neurosci, vol. 7, no. 1–4, pp. 203–222, Oct. 2016, doi: 10.1080/17588928.2015.1111868,. \n[28] E. L. Deci and R. M. Ryan, “The ‘what’ and ‘why’ of goal pursuits: Human needs and the self-determination of behavior,” Psychol Inq, vol. 11, no. \n4, pp. 227–268, 2000, doi: 10.1207/S15327965PLI1104_01. \n[29] J. Nakamura and M. Csikszentmihalyi, “The concept of flow,” Flow and the Foundations of Positive Psychology: The Collected Works of Mihaly \nCsikszentmihalyi, pp. 239–263, Apr. 2014, doi: 10.1007/978-94-017-9088-8_16/FIGURES/2. \n[30] J. L. Dibble, T. Hartmann, and S. F. Rosaen, “Parasocial Interaction and Parasocial Relationship: Conceptual Clarification and a Critical Assessment \nof Measures,” Hum Commun Res, vol. 42, no. 1, pp. 21–44, Jan. 2016, doi: 10.1111/HCRE.12063. \n[31] D. Horton and R. R. Wohl, “Mass Communication and Para -Social Interaction,” Psychiatry, vol. 19, no. 3, pp. 215 –229, Aug. 1956, doi: \n10.1080/00332747.1956.11023049. \n[32] M. Skjuve, A. Følstad, K. I. Fostervold, and P. B. Brandtzaeg, “My Chatbot Companion - a Study of Human -Chatbot Relationships,” Int J Hum \nComput Stud, vol. 149, p. 102601, May 2021, doi: 10.1016/J.IJHCS.2021.102601.  \n[33]   Marzuki, W. Utami, R. Diyenti,   D., and I. and Indrawati, “The impact of AI writing tools on the content and organization of students’ writing: \nEFL teachers’ perspective,” Cogent Education, vol. 10, no. 2, p. 2236469, Dec. 2023, doi: 10.1080/2331186X.2023.2236469.  \n[34] K. and Z. P. and B. S. H. Dergaa Ismail   and Chamari, “From human writing to artificial intelligence generated text: examining the prospects and \npotential threats of ChatGPT in academic writing,” Biol Sport, vol. 40, no. 2, pp. 615–622, 2023, doi: 10.5114/biolsport.2023.125623. \n[35] C. A. Gao et al. , “Comparing scientific abstracts generated by ChatGPT to original abstracts using an artificial intelligence output detector , \nplagiarism detector, and blinded human reviewers,” BioRxiv, pp. 2012–2022, 2022. \n[36] M. Abbas, F. A. Jam, and T. I. Khan, “Is it harmful or helpful? Examining the causes and consequences of generative AI usage among university \nstudents,” International Journal of Educational Technology in Higher Education , vol. 21, no. 1, p. 10, 2024, doi: 10.1186/s41239 -024-00444-7. \n[37] L. Vicente and H. Matute, “Humans inherit artificial intelligence biases,” Sci Rep, vol. 13, no. 1, p. 15737, 2023, doi: 10.1038/s41598 -023-42384-\n8. \n28 \n[38] S. Alon -Barkat and M. Busuioc, “Human –AI Interactions in Public Sector Decision Making: ‘Automation Bias’ and ‘Selective Adherence’ to \nAlgorithmic Advice,” Journal of Public Administration Research and Theory, vol. 33, no. 1, pp. 153–169, Jan. 2023, doi: 10.1093/jopart/muac007. \n[39] A. Tversky and D. Kahneman, “Judgment under Uncertainty: Heuristics and Biases,” Science (1979), vol. 185, no. 4157, pp. 1124–1131, Sep. 1974, \ndoi: 10.1126/science.185.4157.1124. \n[40] Y. Xie, K. Wang, and Y. Kong, “Prevalence of Research Misconduct and Questionable Research Practices: A Systematic Review and  Meta-\nAnalysis,” Sci Eng Ethics, vol. 27, no. 4, p. 41, 2021, doi: 10.1007/s11948-021-00314-9. \n[41] R. Wiczorek and J. Meyer, “Effects of Trust, Self-Confidence, and Feedback on the Use of Decision Automation,” Front Psychol, vol. Volume 10-\n2019, 2019, doi: 10.3389/fpsyg.2019.00519. \n[42] H.-P. H. Lee et al., “The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From \na Survey of Knowledge Workers,” 2025. \n[43] P. Kallas, “LLM survey 2024: generative AI adoption statistics at work,” Amperly.  \n[44] B. Waber and N. J. Fast, “Is GenAI’s Impact on Productivity Overblown?,” Harvard Business Review .  \n[45] S. Noy and W. Zhang, “Experimental evidence on the productivity effects of generative artificial intelligence,” Science (1979), vol. 381, no. 6654, \npp. 187–192, Jul. 2023, doi: 10.1126/science.adh2586. \n[46] D. Snyder-Young, “Responding to Contemporary Events in an Era of Instant Gratification,” in Theatre of Good Intentions: Challenges and Hopes \nfor Theatre and Social Change, Springer, 2013, pp. 110–131. \n[47] M. Kreth, “Why We’re Addicted to ChatGPT: The Science Behind Our Obsession with AI,” Medium.  \n[48] Z. Gao et al., “The neural basis of delayed gratification,” Sci Adv, vol. 7, no. 49, p. eabg6611, Apr. 2025, doi: 10.1126/sciadv.abg6611.  \n[49] E. S. Bromberg-Martin, M. Matsumoto, and O. Hikosaka, “Dopamine in Motivational Control: Rewarding, Aversive, and Alerting,” Neuron, vol. \n68, no. 5, pp. 815–834, Dec. 2010, doi: 10.1016/j.neuron.2010.11.022. \n[50] G. Aronsson et al., “A systematic review including meta -analysis of work environment and burnout symptoms,” BMC Public Health, vol. 17, no. \n1, p. 264, 2017, doi: 10.1186/s12889-017-4153-7. \n[51] T. Xie, I. Pentina , and T. Hancock, “Friend, mentor, lover: does chatbot  engagement lead to psychological dependence?,” Journal of Service \nManagement, vol. 34, no. 4, pp. 806–828, Jan. 2023, doi: 10.1108/JOSM-02-2022-0072. \n[52] K. S. YOUNG, “Internet Addiction: The Emergence of a New Clinical Disorder,” CyberPsychology & Behavior , vol. 1, no. 3, pp. 237 –244, Jan. \n1998, doi: 10.1089/cpb.1998.1.237. \n[53] A. Gambino, J. Fox, and R. A. Ratan, “Building a stronger CASA: Extending the computers are social actors paradigm,” Human-Machine \nCommunication, vol. 1, pp. 71–85, 2020. \n[54] K. S. Cook, C. Cheshire, E. R. W. Rice, and S. Nakagawa, “Social Exchange Theory,” in Handbook of Social Psychology , J. DeLamater  and A. \nWard, Eds., Dordrecht: Springer Netherlands, 2013, pp. 61 –88. doi: 10.1007/978-94-007-6772-0_3. \n[55] I. Pentina, T. Hancock, and T. Xie, “Exploring relationship development with social chatbots: A mixed -method study of replika,” Comput Human \nBehav, vol. 140, p. 107600, 2023, doi: https://doi.org/10.1016/j.chb.2022.107600.  \n[56] L. Laestadius, A. Bishop, M. Gonzalez, D. Illenčík, and C. Campos -Castillo, “Too human and not human enough: A grounded theory analysis of \nmental health harms from emotional dependence on the social chatbot Replika,” New Media Soc , vol. 26, no. 10, pp. 5923 –5941, 2024, doi: \n10.1177/14614448221142007. \n[57] G. Northoff, “Is the self a higher -order or fundamental function of the brain? The ‘basis model of self -specificity’ and its encoding by the brain’s \nspontaneous activity,” Cogn Neurosci, vol. 7, no. 1–4, pp. 203–222, Oct. 2016, doi: 10.1080/17588928.2015.1111868.  \n[58]  James G Hillman,  Devin I Fowlie, and  Tara K MacDonald, “Social Verification Theory: A New Way to Conceptualize Validation, Dissonance, \nand Belonging,” Personality and Social Psychology Review, vol. 27, no. 3, pp. 309–331, Dec. 2022, doi: 10.1177/10888683221138384. \n[59] G. Riva et al., “Affective Interactions Using Virtual Reality: The Link between Presence and Emotions,” CyberPsychology & Behavior , vol. 10, \nno. 1, pp. 45–56, 2007, doi: 10.1089/cpb.2006.9993. \n[60] J. E. Stets and P. J. Burke, “Self -Esteem and Identities,” Sociological Perspectives , vol. 57, no. 4, pp. 409 –433, 2014, doi: \n10.1177/0731121414536141. \n[61] S. Y. X. Komiak and I. Benbasat, “The Effects of Personalization and Familiarity on Trust and Adoption of Recommendation Agen ts,” MIS \nQuarterly, vol. 30, no. 4, pp. 941–960, 2006, doi: 10.2307/25148760. \n[62] J. Nakamura and M. Csikszentmihalyi, “The Concept of Flow,” in Flow and the Foundations of Positive Psychology: The Collected Works of \nMihaly Csikszentmihalyi, M. Csikszentmihalyi, Ed., Dordrecht: Springer Netherlands, 2014, pp. 239 –263. doi: 10.1007/978-94-017-9088-8_16. \n[63] C. Peifer and G. Wolters, “Flow in the Context of Work,” in Advances in Flow Research , C. Peifer and S. Engeser, Eds., Cham: Springer \nInternational Publishing, 2021, pp. 287–321. doi: 10.1007/978-3-030-53468-4_11. \n29 \n[64] C. M. Finneran and P. Zhang, “A person –artefact–task (PAT) model of flow antecedents in computer -mediated environments,” Int J Hum Comput \nStud, vol. 59, no. 4, pp. 475–496, 2003, doi: https://doi.org/10.1016/S1071-5819(03)00112-5. \n[65] Soper, “Free A -priori Sample Size Calculator for Structural Equation Models - Free Statistics Calculators.” Accessed: Apr. 30, 2025. [Online]. \nAvailable: https://www.danielsoper.com/statcalc/calculator.aspx?id=89  \n[66] S. B. MacKenzie, P. M. Podsakoff, and N. P. Podsakoff, “Construct measurement and validation procedures in MIS and behavioral  research: \nIntegrating new and existing techniques,” MIS Q, vol. 35, no. 2, pp. 293–334, 2011, doi: 10.2307/23044045. \n[67] F. F. R. Morgado, J. F. F. Meireles, C. M. Neves, A. C. S. Amaral, and M. E. C. Ferreira, “Scale development: ten main limita tions and \nrecommendations to improve future research practices,” Psicologia: Reflexão e Crítica 2017 30:1 , vol. 30, no. 1, pp. 1 –20, Jan. 2017, doi: \n10.1186/S41155-016-0057-1. \n[68] C. Sindermann et al., “Assessing the Attitude Towards Artificial Intelligence: Introduction of a Short Measure in German, Chinese, and English \nLanguage,” KI - Kunstliche Intelligenz, vol. 35, no. 1, pp. 109–118, Mar. 2021, doi: 10.1007/S13218-020-00689-0/TABLES/5. \n[69] G. Lins de Holanda Coelho, P. H. P. Hanel, and L. J. Wolf, “The Very Efficient Assessment of Need for Cognition: Developing a Six-Item Version,” \nAssessment, vol. 27, no. 8, pp. 1870–1885, Dec. 2020, doi: 10.1177/1073191118793208.  \n[70] Y. Huang et al., “TrustLLM: Trustworthiness in Large Language Models,” arXiv Preprint, p. 40, Jan. 2024, Accessed: May 05, 2025. [Online]. \nAvailable: https://arxiv.org/pdf/2401.05561 \n[71] L. A. van der Ark, “Mokken Scale Analysis in R,” J Stat Softw, vol. 20, no. 11, pp. 1–19, Feb. 2007, doi: 10.18637/JSS.V020.I11. \n[72] C. A. Bernaards and R. I. Jennrich, “Gradient projection algorithms and software for arbitrary rotation criteria in factor an alysis,” Educ Psychol \nMeas, vol. 65, no. 5, pp. 676–696, 2005, doi: 10.1177/0013164404272507. \n[73] S. Epskamp, G. Maris, L. J. Waldorp, and D. Borsboom, “Network psychometrics,” The Wiley Handbook of Psychometric Testing: A \nMultidisciplinary Reference on Survey, Scale and Test Development , vol. 2–2, pp. 953–986, Jun. 2017, doi: 10.1002/9781118489772.ch30.  \n[74] J. Friedman, T. Hastie, and R. Tibshirani, “Sparse inverse covariance estimation with the graphical lasso,” Biostatistics, vol. 9, no. 3, pp. 432–441, \nJul. 2008, doi: 10.1093/BIOSTATISTICS/KXM045. \n[75] P. Pons and M. Latapy, “Computing communities in large networks using random walks,” J Graph Algorithms Appl, vol. 10, no. 2, pp. 191 –218, \n2006. \n[76] G. W. Cheung, H. D. Cooper -Thomas, R. S. Lau, and L. C. Wang, “Reporting reliability, convergent and discriminant validity with structural \nequation modeling: A review and best -practice recommendations,” Asia Pacific Journal of Management , vol. 41, no. 2, pp. 745 –783, Jun. 2024, \ndoi: 10.1007/S10490-023-09871-Y/TABLES/7. \n[77] J. C. Nunnally and I. H. Bernstein, “Psychometric theory,” Published in 1994 in New York by McGraw -Hill, pp. 9 –11, 1994, Accessed: Apr. 30, \n2025. [Online]. Available: https://lib.ugent.be/catalog/rug01:000331515  \n[78] J. H. Steiger, “Tests for comparing elements of a correlation matrix,” Psychol Bull, vol. 87, no. 2, pp. 245 –251, Mar. 1980, doi: 10.1037/0033 -\n2909.87.2.245. \n[79] P. M. Podsakoff, S. B. MacKenzie, J. -Y. Lee, and N. P. Podsakoff, “Common Method Biases in Behavioral Research: A Critical Review of the \nLiterature and Recommended Remedies,” Journal of Applied Psychology, vol. 88, no. 5, pp. 879–903, 2003, doi: 10.1037/0021-9010.88.5.879. \n[80] S. Tehseen, T. Ramayah, and S. Sajilan, “Testing and Controlling for Common Method Variance: A Review of Available Methods,” Journal of \nManagement Sciences, vol. 4, no. 2, pp. 142–168, Mar. 2017, doi: 10.20547/JMS.2014.1704202.  \n[81] C. M. Fuller, M. J. Simmering, G. Atinc, Y. Atinc, and B. J. Babin, “Common methods variance detection in business research,” J Bus Res, vol. 69, \nno. 8, pp. 3192–3198, Aug. 2016, doi: 10.1016/J.JBUSRES.2015.12.008.  \n[82] F. Kock, A. Berbekova, and A. G. Assaf, “Understanding and managing the threat of common method bias: Detection, prevention a nd control,” \nTour Manag, vol. 86, p. 104330, Oct. 2021, doi: 10.1016/J.TOURMAN.2021.104330.  \n[83] M. Liebherr, E. Enkel, E. Law, M. Mousavi, M. Sammartino, and P. Sieberg, “Dynamic Calibration of   Trust and Trustworthiness in   AI-Enabled \nSystems,” International Journal on Software Tools for Technology Transfer , Mar. 2025, doi: 10.1007/s10009-023-00699-x. \n[84] M. Csikszentmihalyi and M. Csikzentmihaly, Flow: The psychology of optimal experience, vol. 1990. Harper & Row New York, 1990. \n[85] W. Wood and D. T. Neal, “A New Look at Habits and the Habit -Goal Interface,” Psychol Rev , vol. 114, no. 4, pp. 843 –863, Oct. 2007, doi: \n10.1037/0033-295X.114.4.843,. \n[86] B. Wahn and L. Schmitz, “A bonus task boosts people’s  willingness to offload cognition to an algorithm,” Cogn Res Princ Implic, vol. 9, no. 1, p. \n24, Dec. 2024, doi: 10.1186/S41235-024-00550-0. \n[87] G. Judah, B. Gardner, M. G. Kenward, B. DeStavola, and R. Aunger, “Exploratory study of the impact of perceived reward on hab it formation,” \nBMC Psychol, vol. 6, no. 1, Dec. 2018, doi: 10.1186/S40359-018-0270-Z,. \n[88] N. Sánchez -Fuenzalida, S. van Gaal, S. M. Fleming, J. M. Haaf, and J. J. Fahrenfort, “Predictions and rewards affect decision -making but not \nsubjective experience,” Proc Natl Acad Sci U S A, vol. 120, no. 44, 2023, doi: 10.1073/PNAS.2220749120,.  \n30 \n[89] H. W. Marsh, K. -T. Hau, and D. Grayson, “Goodness of Fit in Structural Equation Models,” in Contemporary psychometrics: A festschrift for \nRoderick P. McDonald, A. ; M. J. J. Maydeu-Olivares, Ed., Mahwah, NJ: Lawrence Erlbaum Associates Publishers, 2005, pp. 275 –340. \n[90] D. C. Giles, “Parasocial Interaction: A Review of the Literature and a Model for Future Research,” Media Psychol, vol. 4, no. 3, pp. 279–305, 2002, \ndoi: 10.1207/S1532785XMEP0403_04. \n[91] T. Hartmann, “Parasocial interaction, parasocial relationships, and well -being,” in The Routledge handbook of media use and well -being, L. \nReinecke and M. B. Oliver, Eds., Routledge, 2016, pp. 131 –144. \n[92] H. Gardner and K. Davis, The app generation: How today’s youth navigate identity, intimacy, and imagination in a digital world . Yale University \nPress, 2013. \n[93] K. L. Nowak and F. Biocca, “The Effect of the Agency and Anthropomorphism on Users’ Sense of Telepresence, Copresence, and So cial Presence \nin Virtual Environments,” Presence: Teleoperators and Virtual Environments , vol. 12, no. 5, pp. 481 –494, Oct. 2003, doi: \n10.1162/105474603322761289. \n[94] M. R. Banks, L. M. Willoughby, and W. A. Banks, “Animal -Assisted Therapy and Loneliness in Nursing Homes: Use of Robotic versus Living \nDogs,” J Am Med Dir Assoc, vol. 9, no. 3, pp. 173–177, 2008, doi: 10.1016/j.jamda.2007.11.007. \n[95] J. Henseler, C. M. Ringle, and M. Sarstedt, “A new criterion for assessing discriminant validity in variance -based structural equation modeling,” J \nAcad Mark Sci, vol. 43, no. 1, pp. 115–135, Jan. 2015, doi: 10.1007/S11747-014-0403-8/FIGURES/8. \n[96] E. Gößwein and M. Liebherr, “Embracing Change in the Modern Working Environment: Exploring the Role of Trust, Experimentation , and \nAdaptability in the Acceptance of New Technologies,” Sage Open, vol. 15, no. 1, Jan. 2025, doi: 10.1177/21582440241311126.  \n[97] X. Hu, L. Luo, and S. M. Fleming, “A role for metamemory in cognitive offloading,” Cognition, vol. 193, p. 104012, Dec. 2019, doi: \n10.1016/J.COGNITION.2019.104012. \n[98] J. G. Boyle, M. R. Walters, S. Jamieson, and S. J. Durning, “Distributed cognition: Theoretical insights and practical applic ations to health \nprofessions education: AMEE Guide No. 159,” Med Teach, vol. 45, no. 12, pp. 1323–1333, 2023, doi: 10.1080/0142159X.2023.2190479,. \n[99] R. Heersmink, “The Internet, Cognitive Enhancement, and the Values of Cognition,” Minds Mach (Dordr), vol. 26, no. 4, pp. 389–407, Dec. 2016, \ndoi: 10.1007/S11023-016-9404-3/METRICS. \n[100] A. Alabed, A. Javornik, and D. Gregory -Smith, “AI anthropomorphism and its effect on users’ self -congruence and self –AI integration: A \ntheoretical framework and research agenda,” Technol Forecast Soc Change , vol. 182, p. 121786, Sep. 2022, doi: \n10.1016/J.TECHFORE.2022.121786. \n[101] D. Kardefelt-Winther, “A conceptual and methodological critique of internet addiction research: Towards a model of compensatory internet \nuse,” Comput Human Behav, vol. 31, pp. 351–354, 2014. \n[102] S. Casale and G. Fioravanti, “Why narcissists are at risk for developing Facebook addiction: The need to be admired and the n eed to belong,” \nAddictive Behaviors, vol. 76, pp. 312–318, Jan. 2018, doi: 10.1016/j.addbeh.2017.08.038.  \n[103] C. Nass and Y. Moon, “Machines and mindlessness: Social responses to computers,” Journal of Social Issues , vol. 56, no. 1, pp. 81 –103, Jan. \n2000, doi: 10.1111/0022-4537.00153. \n[104] I. A. Zahid et al., “Unmasking large language models by means of OpenAI GPT-4 and Google AI: A deep instruction-based analysis,” Intelligent \nSystems with Applications, vol. 23, p. 200431, Sep. 2024, doi: 10.1016/J.ISWA.2024.200431.  \n[105] H. Choung, P. David, and A. Ross, “Trust in AI and Its Role in the Acceptance of AI Technologies,” Int J Hum Comput Interact, vol. 39, no. 9, \npp. 1727–1739, 2023, doi: 10.1080/10447318.2022.2050543.  \n[106] K. A. Hoff and M. Bashir, “Trust in automation: Integrating empirical evidence on factors that influence trust,” Hum Factors, vol. 57, no. 3, pp. \n407–434, May 2015, doi: 10.1177/0018720814547570.  \n[107] P. A. Hancock, D. R. Billings, K. E. Schaefer, J. Y. C. Chen, E. J. De Visser, and R. Parasuraman, “A meta -analysis of factors affecting trust in \nhuman-robot interaction,” Hum Factors, vol. 53, no. 5, pp. 517–527, Oct. 2011, doi: 10.1177/0018720811417254.  \n[108] L. Wang et al., “Prompt engineering in consistency and reliability with the evidence -based guideline for LLMs,” NPJ Digit Med, vol. 7, no. 1, \npp. 1–9, Dec. 2024, doi: 10.1038/s41746-024-01029-4. \n[109] N. Barr, G. Pennycook, J. A. Stolz, and J. A. Fugelsang, “The brain in your pocket: Evidence that Smartphones are used to sup plant thinking,” \nComput Human Behav, vol. 48, pp. 473–480, Jul. 2015, doi: 10.1016/J.CHB.2015.02.029. \n[110] H. Li and R. Zhang, “Finding love in algorithms: deciphering the emotional contexts of close encounters with AI chatbots,” Journal of Computer-\nMediated Communication, vol. 29, no. 5, p. 15, Aug. 2024, doi: 10.1093/JCMC/ZMAE015.  \n[111] C. Montag, E. Wegmann, R. Sariyska, Z. Demetrovics, and M. Brand, “How to overcome taxonomical problems in the study of Inter net use \ndisorders and what to do with ‘smartphone addiction’?,” J Behav Addict, vol. 9, no. 4, pp. 908–914, Jan. 2021, doi: 10.1556/2006.8.2019.59. \n[112] R. J. J. M. Van Den Eijnden, J. S. Lemmens, and P. M. Valkenburg, “The Social Media Disorder Scale,” Comput Human Behav , vol. 61, pp. \n478–487, Aug. 2016, doi: 10.1016/J.CHB.2016.03.038.  \n31 \n[113] D. Hu, Y. Lan, H. Yan, and C. W. Chen, “What makes you attached to social companion AI? A two-stage exploratory mixed-method study,” Int \nJ Inf Manage, vol. 83, p. 102890, Aug. 2025, doi: 10.1016/J.IJINFOMGT.2025.102890.  \n[114]   Brand M, Young KS, Laier C, Wölfling K, Potenza MN. Integrating psychological and neurobiological considerations regarding the development \nand maintenance of specific Internet-use disorders: An Interaction of Person-Affect-Cognition-Execution (I-PACE) model. Neurosci Biobehav Rev. \n2016 Dec;71:252-266. doi: 10.1016/j.neubiorev.2016.08.033. Epub 2016 Aug 30. PMID: 27590829.  \n[115] I. Etikan, S. Abubakar Musa, and R. Sunusi Alkassim, “Comparison of Convenience Sampling and Purposive Sampling,” American Journal of \nTheoretical and Applied Statistics, vol. 5, no. 1, pp. 1–4, 2016, doi: 10.11648/j.ajtas.20160501.11. \n[116] J. Henrich, S. J. Heine, and A. Norenzayan, “Most People are not WEIRD,” Nature, vol. 466, no. 7302, p. 29, 2010, doi: 10.1038/466029a. \n  \n  \n32 \nAppendix 1 \nLarge Language Model Dependency Questionnaire (LLM-D12) \nThinking specifically about the large language model (LLM) you use most often (such as ChatGPT, \nDeepSeek, Gemini, Copilot, etc.), please indicate how much you agree or disagree with each of the \nfollowing statements \nItem nr  Strongly \nDisagree \nDisagree Somewhat \nDisagree \nSomewhat \nAgree \nAgree Strongly \nAgree \nID1 Without it, I feel less conﬁdent \nwhen making decisions \n      \nID2 I use it sometimes without realizing \nhow much time I spend immersed \nin it \n      \nID3 I feel much more rewarded and \npleased when completing tasks \nusing it \n      \nID4 I turn to it for support in decisions, \neven when I can make them myself \nwith some eﬀort \n      \nID5 It is my go-to for assistance in \ndecision-making \n      \nID6 Making decisions without it feels \nsomewhat uneasy \n      \nRD1 I share details about my private life \nwith it \n      \nRD2 I interact with it as if it were a \ngenuine companion \n      \nRD3 It helps me feel less alone when I \nneed to talk to someone \n      \nRD4 It adds to my social life, making \nsocializing more engaging and \ninteresting \n      \nRD-5 [R] I use it solely as a tool, not to \nexpress my feelings or expect it to \nunderstand me \n      \nRD6 It helps me feel less alone, reducing \nthe need to talk to others \n      \n \nNote:  \nThe LLM examples in the introductory context can be updated to reflect either the popular models of the time or those \nmost relevant to the context in which the scale is administered. \nScoring:  \nItems are rated on a 6-point scale: 1 = Strongly Disagree, 2 = Disagree, 3 = Somewhat Disagree, 4 = Somewhat Agree, 5 \n= Agree, 6 = Strongly Agree.  \n[R] indicates that the item shall be reversed when scoring \nID – Instrumental Dependency, RD – Relationship Dependency \nAdd up the item scores for ID to get the Instrumental Dependency score. \nAdd up the item scores for RD to get the Relationship Dependency score. \nThe overall LLM Dependency score is the sum of the two subscale scores. \n \n33 \nAppendix 2 \nMapping individual items of LLM-D12 into underlying theoretical constructs and measurements \n    \nItem \nID \nItem Underlying \ntheoretical \nconstruct \nMeasuring the extent of \nID1 Without it, I feel less conﬁdent \nwhen making decisions \nDecision-making \nconfidence \ncognitive dependency on the LLM for decision-making \nconfidence \nID2 I use it sometimes without \nrealizing how much time I spend \nimmersed in it \nImmersive \nengagement \nbecoming deeply absorbed in the interaction to the point of \nreduced awareness of surroundings or schedules \nID3 I feel much more rewarded and \npleased when completing tasks \nusing it \nPositive \nreinforcement \nfrom use \nexperiencing greater gratification or accomplishment when \nthe LLM is part of the task process \nID4 I turn to it for support in decisions, \neven when I can make them \nmyself with some eﬀort \nCognitive \noffloading \nleaning on the LLM to reduce cognitive effort \n \nID5 It is my go-to for assistance in \ndecision-making \nDecision-making \nreliance \na person turns to the LLM as their primary source of help \nwhen making decisions \nID6 Making decisions without it feels \nsomewhat uneasy \nDecision-making \nunease \na person experiences discomfort or uncertainty in decision-\nmaking when LLM is unavailable (sense of altered decision \nexperience) \nRD1 I share details about my private \nlife with it \nSelf-disclosure a person shares personal or private information with LLM \n(based on the belief that the interaction is private and non-\njudgmental) \nRD2 I interact with it as if it were a \ngenuine companion \nCompanionship assigning human-like qualities, intentions, or \nresponsiveness to the LLM in the context of ongoing \ninteraction \nRD3 It helps me feel less alone when I \nneed to talk to someone \nPerceived social \nconnectedness \nperceived social connectedness through LLM interaction \nRD4 It adds to my social life, making \nsocializing more engaging and \ninteresting \nPositive social \nenhancement \nbelief that one’s social life is more engaging, dynamic, or \nstimulating because of the LLM \nRD-5 \n[R] \nI use it solely as a tool, not to \nexpress my feelings or expect it to \nunderstand me \nWithholding \npersonal \nexpression \none avoids sharing inner states such as opinions, \nexperiences, values, or emotions \nRD6 It helps me feel less alone, \nreducing the need to talk to others \nSocial \nsubstitution \nLLM interaction replaces interaction with other people \nbecause the LLM fulfils that role \nNote on feasibility of using LLM-D12 at its item level \n    Examining responses to individual items in the LLM-D12, rather than relying solely on total subscale scores, can \nprovide a more detailed understanding of the specific behaviours within instrumental and relationship LLM dependency. \nThis approach may be particularly beneficial when: \n• Identifying distinct behaviours that could inform targeted interventions. \n• Comparing specific aspects of LLM use between different groups or across measurements. \n• Determining which item of dependency are most closely associated with other psychological or behavioural \nvariables. \n• Conducting exploratory or formative studies where fine-grained behavioural insights are needed. \n• Estimating prevalence of individual indicators of dependent behaviour. \n• Identifying the hierarchy of indicators of Instrumental or Relationship LLM dependency. \n34 \n \n    The psychometric properties of the LLM-D12 indicate that the use of individual items as micro-measures is feasible. \nBoth subscales showed good internal consistency and a coherent factor structure, suggesting that each item is \nmeaningfully related to the construct measured by its subscale. However, the following limitations should be considered: \n• Each item contributes differently to the corresponding subscale, so item scores cannot be treated as \nequivalent. \n• The relationships between items within each subscale differ in strength. It is recommended to review item \nloadings and inter-item relationships, such as through factor analysis or network modelling before \ninterpreting individual item scores. \n• Interpreting each item as if it measures a separate construct may be misleading, as items within a subscale \nshare variance due to their focus on related aspects of the same underlying concept. \n \n  \n \n \n \n \n \n   \n \n ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.728352963924408
    },
    {
      "name": "Dual (grammatical number)",
      "score": 0.652819812297821
    },
    {
      "name": "Scale (ratio)",
      "score": 0.6205405592918396
    },
    {
      "name": "Theoretical computer science",
      "score": 0.4380936920642853
    },
    {
      "name": "Programming language",
      "score": 0.41124480962753296
    },
    {
      "name": "Linguistics",
      "score": 0.10974285006523132
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I9300472",
      "name": "Bournemouth University",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210144839",
      "name": "Hamad bin Khalifa University",
      "country": "QA"
    },
    {
      "id": "https://openalex.org/I62318514",
      "name": "University of Duisburg-Essen",
      "country": "DE"
    }
  ],
  "cited_by": 1
}