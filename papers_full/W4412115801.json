{
  "title": "Large language models for terminology work: A question of the right prompt?",
  "url": "https://openalex.org/W4412115801",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2276576768",
      "name": "Barbara Heinisch",
      "affiliations": [
        "Center for Applied Linguistics"
      ]
    },
    {
      "id": "https://openalex.org/A2276576768",
      "name": "Barbara Heinisch",
      "affiliations": [
        "Eurac Research"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4393871063",
    "https://openalex.org/W4387929411",
    "https://openalex.org/W4253348150",
    "https://openalex.org/W4367369394",
    "https://openalex.org/W4330336443",
    "https://openalex.org/W4288076474",
    "https://openalex.org/W4361200339",
    "https://openalex.org/W6822266525",
    "https://openalex.org/W4386428294",
    "https://openalex.org/W6813353047",
    "https://openalex.org/W6872474432",
    "https://openalex.org/W4402670310",
    "https://openalex.org/W1982821603"
  ],
  "abstract": "Text-generative large language models (LLMs) offer promising possibilities for terminology work, including term extraction, definition creation and assessment of concept relations. This study examines the performance of ChatGPT, Perplexity and Microsoft CoPilot for conducting terminology work in the field of the Austrian and British higher education systems using strategic prompting frameworks. Despite efforts to refine prompts by specifying language variety and system context, the LLM outputs failed to reliably differentiate between the Austrian and German systems and fabricated terms. Factors such as the distribution of German-language training data,potential pivot translation via English and the lack of transparency in LLM training further complicated evaluation. Additionally, output variability across identical prompts highlights the unpredictability of LLM-generated terminology. The study underscores the importance of human expertise in evaluating LLM outputs, as inconsistencies may undermine the reliability of terminology derived from such models. Without domain-specific knowledge (encompassing both subject-matter expertise and familiarity with terminology principles) as well as LLM literacy, users are unable to critically assess the quality of LLM outputs in terminological contexts. Rather than indiscriminately applying LLMs to all aspects of terminology work, it is crucial to assess their suitability for specific tasks.",
  "full_text": "Barbara Heinisch\nLarge language models for terminology work: A question of the\nright prompt?\nAbstract\nText-generative large language models (LLMs) offer promising possibilities for\nterminology work, including term extraction, definition creation and assessment of\nconcept relations. This study examines the performance of ChatGPT, Perplexity and\nMicrosoft CoPilot for conducting terminology work in the field of the Austrian and\nBritish higher education systems using strategic prompting frameworks. Despite efforts\nto refine prompts by specifying language variety and system context, the LLM outputs\nfailed to reliably differentiate between the Austrian and German systems and\nfabricated terms. Factors such as the distribution of German-language training data,\npotential pivot translation via English and the lack of transparency in LLM training\nfurther complicated evaluation. Additionally, output variability across identical\nprompts highlights the unpredictability of LLM-generated terminology. The study\nunderscores the importance of human expertise in evaluating LLM outputs, as\ninconsistencies may undermine the reliability of terminology derived from such models.\nWithout domain-specific knowledge (encompassing both subject-matter expertise and\nfamiliarity with terminology principles) as well as LLM literacy, users are unable to\ncritically assess the quality of LLM outputs in terminological contexts. Rather than\nindiscriminately applying LLMs to all aspects of terminology work, it is crucial to\nassess their suitability for specific tasks.\n1 Introduction\nLarge Language Models (LLMs), capable of processing and generating human-like text,\nare transforming numerous professions (Eloundou, Manning, Mishkin, & Rock, 2023),\nincluding specialized translation and terminology management. Since LLMs utilize\ndistinct approaches to generate and comprehend language, they fundamentally change\nthe function of terminology (Massion, 2024) and the way how terminologists and\nspecialized translators approach terminology work. Nevertheless, technology has long\nsupported translation, from Computer-Assisted Translation (CAT) tools to terminology\nextraction software, corpus analysis and alignment tools (Rothwell, Moorkens,\nFernández-Parra, Drugan, & Austermühl, 2023). While machine translation tools have\nlong been a staple for translators, LLMs bring a new level of versatility, as they cannot\nonly be used for translation per se but also for translation-related tasks, such as\nclarifying meaning, editing style, detecting errors or assuring quality (Siu, 2023).\nJLCL 2025 – Band 38(2) – 13–30\nHeinisch\nAlso terminologists use a wide range of specialized software for terminology work,\nincluding term extraction (Steurs, de Wachter, & de Malsche, 2015) or the management\nof terminology in terminological databases (Drewer & Schmitz, 2017). LLMs can\nenhance terminology work by efficiently extracting relevant terms (Hamm, 2025),\ngenerating definitions (Reineke, 2023) in context and by assessing language-variety-\nspecific terminology (Heinisch, 2020). They can be used for finding equivalents across\nlanguages and terminology validation. Additionally, LLMs help terminologists establish\nrelationships between concepts and verify proper terminology use, streamlining the\noverall process (Massion, 2024).\nTerminology, defined as “set of designations [...] and concepts [...] belonging to\none domain [...] or subject [...]” (ISO 1087:2019 Terminology work and terminology\nscience — Vocabulary, 2019) is crucial in specialized communication because it ensures\nprecision, consistency and clarity in communication. In specialized translation,\neffective terminology management improves translation efficiency and ensures the\nquality of the target text. Therefore, terminology management is pivotal, including the\nidentification of terms and concepts, the extraction of candidate terms as well as the\norganization and validation of terms before storing and maintaining them in\nterminological databases (Steurs et al., 2015). One major challenge in multilingual\nterminology work is determining the equivalence between concepts (Hohnhold, 1990).\nTo accurately interpret and use terminology, terminologists and translators must\nconsider its domain, the system it belongs to and the specific context in which it\nappears. Thus, terminology is domain-specific, system-bound and context-dependent.\nTherefore, the question arises how LLMs perform in (selected) tasks aimed at\n(multilingual) terminology work.\n2 Method\nThe theoretical framework of this paper is grounded in Wüster’s General Theory\nof Terminology (Wüster, 1974), whose foundational principles continue to inform\ncontemporary terminological practice as codified in the ISO standard (ISO 704:2022\nTerminology work — Principles and methods, 2022) on terminology work — Principles\nand methods. This study forms part of the larger projectUniTermGPT: University\nTerminology in German in the Age of ChatGPT, which explores how ChatGPT handles\nuniversity terminology across selected German language varieties. Therefore, this pilot\nstudy also addresses prompt engineering in the context of terminology injection into\nLLMs with a focus on system-bound terminology. The objective was to evaluate the\npotential of multilingual LLM terminology work in the field of university terminology\nin both German and English.\nSince the larger project focusses on ChatGPT only, this study aimed to assess the\ngeneral suitability of LLMs for translation-oriented terminology tasks by using three\ndifferent LLMs: ChatGPT (GPT-4o-mini), Perplexity (‘default model’) and Microsoft\nCoPilot. The selection of ChatGPT, Perplexity and Microsoft Copilot for this study\nwas partly informed by their prominence in contemporary academic and professional\n14 JLCL\nLLMs for terminology work\ncontexts. ChatGPT was chosen due to its widespread adoption, Perplexity, as an early\nmodel integrating LLM outputs with real-time web search, is particularly suited to\nterminology work involving emergent terms, a phenomenon often underrepresented\nin static models. Microsoft Copilot, integrated into Microsoft 365, is widely used in\norganizations through its presence in tools like Word and Excel.\nGiven the practical orientation of this study, the LLMs were prompted (in German\nand partly in English) to address four terminology-related tasks: (1) the identification of\nkey terms within a domain (distinct from term extraction, which typically presupposes\nthe existence of a corpus), (2) term extraction from web-based sources (without having\nto compile a corpus beforehand), (3) the generation and extraction of definitions, and\n(4) the establishment of concept relations. These tasks simulate realistic scenarios faced\nby terminologists or specialized translators who require a foundational terminological\ndatabase under time constraints.\nThe performance of the LLM also depends on the prompt being used. Selecting\nthe right (user) prompt involves understanding user intent, model understanding and\nthe specificity of the domain. Clear, specific prompts tailored to the task and any\nnecessary constraints help guide the model to produce better results (Ekin, 2023).\nPrompt engineering principles (Bozkurt, 2024; Chen, Zhang, Langrené, & Zhu, 2013;\nSaleem, 2024) are a dime a dozen, ranging from general guidelines to prompt engineering\nframeworks. In this study, the CARE and RACE frameworks were used. CARE (context,\naction, result, example) consists of context, i.e. background information, action as\nthe definition of the tasks to be completed, result to state the expected outcome and\nexample to provide the LLM with concrete examples of what the output should be.\nRACE (role, action, context, expectation), on the other hand, focusses on the role, also\nsometimes referred to as persona the LLM should assume, action and context (which are\nsimilar to CARE) and expectation to specify the expected result. Additionally, these\nprompts were improved by LLMs for optimizing prompts. Moreover, prompt chaining,\nwhereby a large task is broken into a sequence of smaller subtasks, each handled by\nits own prompt, was used and the domain specificity, system boundness and context\ndependence of terminology was considered in the prompt (Table 1).\nJLCL 2025 – Band 38(2) 15\nHeinisch\nTable 1:Consideration of domain specificity, system boundness and context dependence of\nterminology in the prompts\nTerminology\ncharacteristics\nAspect Considered in the prompt\nDomain\nspecificity\nUniversity terminology;\nstudies (subdomain);\nadmission (focus)\n\"in the university context\", \"in the\nfield of university admission\"\nSystem\nboundness\n(system)\n(Austrian) university\nsystem\n\"Austrian higher education\nsystem\", \"universities in Austria\"\nSystem\nboundness\n(language\nvariety)\nAustrian \"Austrian German\", \"the Austrian\nvariety of the German language\"\nContext\ndependence\nUniversity vs. university\nof applied sciences;\nterminological variation;\ncertain universities\nBy including source hierarchy,\ndomains (ac.at) from which terms\nand definitions should be\nextracted and varying the\nterminology used in the prompt,\ne.g. \"university\" or \"higher\neducation system\"; \"Benennungen\"\nor \"Termini\" (in German)\nTo determine whether the issue was simply finding the ’right prompt’ or if it was\ninfluenced by factors beyond a single model, three different LLMs were tested. The\ngoal was not to compare these models (or to compare them with traditional corpus\nanalysis tools) but to gain a broader understanding of how useful LLMs are for\nterminology-related tasks. By using multiple models, the analysis was not limited to\njust one LLM, allowing for more comprehensive conclusions. This study adopts a\nqualitative approach to analyzing both the design of prompts and the outputs\ngenerated by the language model, with a focus on understanding LLM capabilities in\nspecific terminological contexts, including domain-specific and system-bound\n(language-variety-specific) terminology. Therefore, the analysis focusses on the\nfollowing aspects: 1) If the term actually exists (or is hallucinated); 2) If the term is\nbound to the correct system (e.g. Austrian university terminology or corporate\nlanguage, if prompted) and 3) if the term is specific to the domain (and not from any\nother domain), i.e. the university or higher education domain (Table 2).\n16 JLCL\nLLMs for terminology work\nTable 2:Criteria and aspects considered in analyzing the LLM output\nCriterion Subcriterion Guiding questions\nTerm existence Real term (vs\nhallucination,\npseudo-terminology)\nIs the term real and used\nin recognized sources (e.g.\ntermbases, glossaries)?\nHas the LLM generated a\nnon-existent or fabricated\nterm?\nDomain specificity University terminology;\nstudies (subdomain);\nadmission (focus)\nDoes the term belong to\nthe relevant specialized\n(sub-)field?\nSystem boundness Correct system Is the term from the\n(Austrian or British)\nuniversity system? Does\nthe term cover the\nrelevant language variety?\nContext dependence Corporate language (if\nrequested)\nIs the term used by the\nrelevant university? Is it\nthe preferred term (at the\nuniversity)?\n3 Results\nLLMs face challenges in several key areas of bilingual terminology work. The three\nLLMs analyzed in this study struggle with completing multiple steps or sequences,\neven when given step-by-step instructions within a single prompt. Additionally, they\noften fail to provide accurate terminological definitions, especially when requested in\nstructured formats like tables. Moreover, they often do not provide original terms and\ndefinitions but translations of (German) terms and definitions (in English). LLMs\nalso tend to mix Austrian university terminology with terms from the German higher\neducation system, making it difficult to focus exclusively on the desired system, language\nvariety and context. Lastly, when extracting definitions from websites, the models can\nproduce inconsistent results unless the task is confined to a single, focused source.\nDespite prompts that took the domain specificity, the system boundness and the context\ndependence of terminology into account, it was not possible to achieve the desired result.\nIn some cases, the LLM outputs did not differentiate between the Austrian and German\nhigher education systems. As a result, the outputs, which should have been related to the\nAustrian university system, contained terms from both contexts. For example,Numerus\nClausus (NC)is not a restriction for university admission in Austria, whereas in Germany\nit is. The analyzed LLMs generally do not distinguish between Austrian and German\nuniversity terminology despite prompts specifying the ‘Austrian university system’ or\nJLCL 2025 – Band 38(2) 17\nHeinisch\n‘Austrian German’. However, they occasionally generate university-specific terms, even\nwithout being prompted, including proprietary system names likeu:space, a platform of\nthe University of Vienna. Also, when prompted for relations between concepts, terms\nfrom the German university system were included. While the concept relations generated\nby the LLMs are generally usable, they should be approached with caution. The\nterminologist must possess the ability to differentiate between ’German’ and ’Austrian’\nuniversity terminology in order to develop an accurate concept system based on the LLM\noutput. For example, ChatGPT defined the following subordinate concepts for admission\nto studies (Zulassung zum Studium): „allgemeine Universitätsreife, fachgebundene\nUniversitätsreife, Studienberechtigungsprüfung, Berufsreifeprüfung, Quotenregelung”.\nWhile the results may seem plausible to non-experts, they contain pseudo-terminology\n(or hallucinations) asfachgebundene Universitätsreifeis not a term used within the\nAustrian higher education system. The correct, albeit not commonly used, term would\nbe fachgebundene Hochschulreife.\nIn the case of bilingual terminology work, in which a RACE prompt (illustrated in the\nAppendix) was used, the LLMs did not provide original definitions for the English or\nGerman terms provided as, in some cases, the LLMs just translated the definitions into\nthe other language, thereby even inventing terms. For example,conditional offerand\nunconditional offerrefer to university admissions with or without certain conditions\nin the English higher education system. The German termsunbedingte Zulassungand\nbedingte Zulassungas provided by ChatGPT are, however, not used in Austria at all.\nThis means that the LLM outputs were often not useful to create or assess relationships\nbetween concepts and to prepare concept systems for the Austrian and (British) English\nhigher education systems. Even by varying the specification of the respective language\nvariety (e.g. ’Austrian German’ or ’Austrian variety of German’), as well as the further\nspecification of the system (e.g. ’Austrian higher education system’, sometimes also ’at\nthe University of Vienna’) and the context, the LLM outputs could not be significantly\nimproved.\nAlthough the quality of the prompt has a significant impact on the quality of the LLM\noutput, the characteristics of each LLM also play a role. The user’s knowledge of these\ncharacteristics is termed ‘model understanding’ (Ekin, 2023). These characteristics\ninclude, for example, how up-to-date the training data are and how the training data\nfor German are distributed across the German, Austrian, Swiss (and other) varieties\nof German. This is aggravated by the fact that the providers of the LLMs are often\nnon-transparent with regard to such information. It is equally opaque whether the\nstudied LLMs use English as a pivot language (i.e. translate the prompt into English),\nwhen the user enters German prompts, before returning the output to German. Some\noutputs allude to that, for example, the termNotendurchschnitt (GPA)was output by\nMicrosoft CoPilot. However, the English abbreviationGPA (grade point average)is not\na common abbreviation forNotendurchschnitt in German. This may lead to biases in\nmultilingual terminology work and specialized translation in general.\n18 JLCL\nLLMs for terminology work\n4 Discussion\nThe variability of LLM outputs, even when using the same prompts, presents\nsignificant challenges in terminology-related tasks. Problems such as the creation of\npseudo-terminology or inconsistencies highlight the critical importance of\ndomain-specific expertise when using LLMs for terminology work. Without sufficient\nknowledge in the relevant field, such as Austrian or British university terminology,\nusers may struggle to evaluate the quality and relevance of the generated output. This\nunderscores the need for LLM literacy and a critical approach to LLM use in such\nspecialized tasks. Human expertise remains crucial to meet the specific demands of\nterminology work, even when LLMs are involved. One key factor influencing the\nquality of LLM output is prompt engineering. The iterative process of refining prompts\nto better suit the task at hand is essential (Ekin, 2023), particularly in terminology\nwork where domain specificity is vital. LLMs, unlike traditional tools such as corpus\nanalysis software, require well-crafted prompts to produce accurate and relevant\nterminology outputs. In contrast, traditional tools can provide frequent terms within a\ndomain without requiring extensive domain-specific input.\nDespite their usefulness, LLMs do not necessarily enhance the productivity of\nterminologists. This study suggests that applying prompt chaining (breaking down\ntasks into smaller, sequential prompts) yields better results than attempting to address\neverything in a single prompt. This method is particularly important in multilingual\nterminology work, where equivalences between languages need to be established.\nHowever, LLMs as tools in terminology work can also be time-consuming, requiring\nterminologists to carefully scrutinize the output, verify sources and ensure the accuracy\nof definitions and web sources provided by the model. LLMs pose several challenges for\nterminology work, including biases and multilingual limitations. Their reliance on\npredominantly English training data (Wang et al., 2024) affects multilingual\nterminology since terms may be inherently altered by being filtered through English\n(Heinisch, in print), which makes it difficult to find equivalents in languages, other than\nEnglish. Biases in, or a lack of training data can skew terminology work, particularly\nin emerging or niche domains (Heinisch, in print), and LLMs’ tendency to hallucinate\n(terms) further complicates their reliability. Moreover, ecological concerns related to\nthe energy consumption and carbon footprint of LLMs (Rojas, 2024) should be\nconsidered when choosing appropriate tools for terminology tasks. The limitations of\nthis study lie in the selection of the LLMs and the prompting frameworks used: the\nanalyzed models are not representative of all commercially available LLMs. Since\nterminology work is often multilingual (as demonstrated in this study), future research\ncould include models with a stronger multilingual focus, such as EuroLLM (Martins et\nal., 2025). Furthermore, future research could employ more advanced prompting\nstrategies and frameworks, as those used in this study were intentionally kept simple\nand concise. Moreover, the larger UniTermGPT project intends to include more\nlanguage varieties as well as additional annotators. Given the study’s focus on the\npractical application of LLMs by terminologists and specialized translators, no German\nJLCL 2025 – Band 38(2) 19\nHeinisch\nfine-tuned models were used. The aim was to reflect realistic workflows using a single\ngeneral-purpose model, particularly for high-resource languages like German and\nEnglish, which are typically well-supported by such models. While\nRetrieval-Augmented Generation (RAG) enables LLMs to access supplementary\ninformation, Terminology-Augmented Generation (TAG) (Fleischmann & Lang, 2025)\nrepresents a complementary approach tailored to domain-specific language use. TAG\nintegrates several components: deterministic retrieval from structured terminological\ndatabases (as opposed to probabilistic retrieval from vectorized data), the generation of\nprecise and processable outputs in standardized or prose-like terminology formats and\nreal-time access to terminology resources via APIs (Fleischmann & Lang, 2025). This\nmethodology is particularly beneficial for content generation tasks that require\nadherence to domain-specific or corporate language norms, such as specialized\ntranslation or technical communication, where consistent use of (validated) terminology\nis essential.\n5 Conclusion\nWhile large language models offer promising possibilities for terminology work, it is\nevident that not all domains, languages and language varieties are equally supported\nby these systems. The presence of biases and hallucinations poses significant challenges\nin multilingual and domain-specific terminology tasks, underscoring the importance of\nhuman expertise in mitigating these issues. To ensure reliable outputs, LLM literacy\nis essential for users engaging with these tools in terminology work. Furthermore,\nprompt engineering plays a crucial role in shaping the quality of the LLM’s responses,\nthough the inherent characteristics of the model, such as its training data and its\ncapabilities, also influence the results. In some cases, smaller, fine-tuned models focused\non terminology tasks or even traditional tools like corpus analysis tools may be more\nsuitable. The balance between LLM usage and traditional terminology tools, as well as\nthe expertise required to navigate LLM outputs, is crucial for effective and high-quality\nterminology work.\nAcknowledgement\nThis paper was funded by the EC-MCSA Seal of Excellence Programme of the\nAutonomous Province of Bozen/Bolzano – Department for Innovation, Research and\nUniversity, project University terminology in German in the age of ChatGPT\n(UniTermGPT).\n20 JLCL\nLLMs for terminology work\nReferences\nBozkurt, A. (2024). Tell Me Your Prompts and I Will Make Them True: The\nAlchemy of Prompt Engineering and Generative AI. Open Praxis, 16(2),\n111–118. Retrieved from https://search.informit.org/doi/pdf/10.3316/\ninformit.T2024041000014390073541090\nChen, B., Zhang, Z., Langrené, N., & Zhu, S. (2013).Unleashing the potential of prompt\nengineering in Large Language Models: a comprehensive review.Retrieved from\nhttp://arxiv.org/pdf/2310.14735\nDrewer, P., & Schmitz, K.-D. (2017).Terminologiemanagement: Grundlagen - Methoden\n- Werkzeuge. Berlin, Heidelberg: Springer Berlin Heidelberg. doi: 10.1007/\n978-3-662-53315-4\nEkin, S. (2023). Prompt Engineering for ChatGPT: A Quick Guide to Techniques,\nTips, and Best Practices. Institute of Electrical and Electronics Engineers (IEEE).\ndoi: 10.36227/techrxiv.22683919.v1\nEloundou, T., Manning, S., Mishkin, P., & Rock, D. (2023).GPTs are GPTs: An Early\nLook at the Labor Market Impact Potential of Large Language Models. Retrieved\nfrom https://doi.org/10.48550/arXiv.2303.10130\nFleischmann, K., & Lang, C. (2025). Terminologie in der KI. Wie mit Terminologie der\nOutput von LLMs und GenAI optimiert werden kann. In P. Drewer, F. Mayer,\n& D. Pulitano (Eds.),Terminologie in der KI – KI in der Terminologie. Akten\ndes Symposions Worms, 27.–29. März 2025(pp. 83–95). München / Karlsruhe /\nBern: Deutscher Terminologie-Tag e.V.\nHamm, J. (2025). Terminologische Konsistenz und generative KI – ein Perfect Match?\nProduktiver Einsatz von Sprachmodellen im Terminologiemanagement und beim\nPost-Editing. In P. Drewer, F. Mayer, & D. Pulitano (Eds.),Terminologie in der\nKI – KI in der Terminologie. Akten des Symposions Worms, 27.–29. März 2025\n(pp. 151–163). München / Karlsruhe / Bern: Deutscher Terminologie-Tag e.V.\nHeinisch, B. (2020). Sprachvarietätenabhängige Terminologie in der neuronalen\nmaschinellen Übersetzung: Eine Analyse in der Sprachrichtung Englisch-Deutsch\nmit Schwerpunkt auf der österreichischen Varietät der deutschen Sprache. In\nC. Schöch (Ed.),DHd 2020 Spielräume: Digital Humanities zwischen Modellierung\nund Interpretation. Konferenzabstracts (pp. 211–214). doi: 10.5281/zenodo\n.4621962\nHeinisch, B. (in print). Next-Gen Terminology: Transforming Terminology Work with\nLarge Language Models.Across Languages and Cultures.\nHohnhold, I. (1990). Übersetzungsorientierte Terminologiearbeit: Eine Grundlegung\nfür Praktiker. Stuttgart: InTra, 1. Fachübersetzergenossenschaft. Retrieved from\nhttp://media.obvsg.at/AC00246114-1001\nISO 1087:2019 Terminology work and terminology science — Vocabulary (2nd\ned.). (2019). https://www.iso.org/standard/73906.html. (Published by\nInternational Organization for Standardization (ISO))\nISO 704:2022 Terminology work — Principles and methods(4th ed.). (2022).https://\nJLCL 2025 – Band 38(2) 21\nHeinisch\nwww.iso.org/standard/81503.html. (Published by International Organization\nfor Standardization (ISO))\nMartins, P. H., Fernandes, P., Alves, J., Guerreiro, N. M., Rei, R., Alves, D. M.,\n... Martins, A. F. (2025). EuroLLM: Multilingual Language Models for\nEurope. Procedia Computer Science, 255, 53-62. Retrieved fromhttps://www\n.sciencedirect.com/science/article/pii/S1877050925006210 (Proceedings\nof the Second EuroHPC user day) doi: https://doi.org/10.1016/j.procs.2025.02\n.260\nMassion, F. (2024). Terminology in the Age of AI: The Transformation of Terminology\nTheory and Practice. Journal of Translation Studies, 4(1). Retrieved from\nhttps://www.peterlang.com/document/1495905 doi: 10.3726/JTS012024.04\nReineke, D. (2023). Terminologiearbeit mit ChatGPT & Co. Fachzeitschrift für\nTerminologie, 19(1), 25–28. Retrieved fromhttp://dttev.org/images/edition/\nausgaben/edition-2023-1-e-version.pdf\nRojas, S. (2024). Evaluating the Environmental Impact of Large Language Models:\nSustainable Approaches and Practices.Innovative Computer Sciences Journal,\n10(1), 1–6. Retrieved fromhttps://innovatesci-publishers.com/index.php/\nICSJ/article/view/153\nRothwell, A., Moorkens, J., Fernández-Parra, M., Drugan, J., & Austermühl, F. (2023).\nTranslation tools and technologies(1st edition ed.). London New York: Routledge,\nTaylor & Francis Group. doi: 10.4324/9781003160793\nSaleem, M. (2024).11 ChatGPT Prompt Frameworks Every Marketer Should Know.\nRetrieved fromhttps://buttercms.com/blog/chatgpt-prompt-frameworks/\nSiu, S. C. (2023). ChatGPT and GPT-4 for Professional Translators: Exploring the\nPotential of Large Language Models in Translation.SSRN Electronic Journal.\ndoi: 10.2139/ssrn.4448091\nSteurs, F., de Wachter, K., & de Malsche, E. (2015). Terminology tools. In H. J. Kockaert\n& F. Steurs (Eds.),Handbook of terminology(pp. 222–249). Amsterdam: John\nBenjamins Publishing Company.\nWang, W., Tu, Z., Chen, C., Yuan, Y., Huang, J.-t., Jiao, W., & Lyu, M. (2024).\nAll Languages Matter: On the Multilingual Safety of LLMs. In Findings\nof the Association for Computational Linguistics ACL 2024(pp. 5865–5877).\nStroudsburg, PA, USA: Association for Computational Linguistics. doi: 10.18653/\nv1/2024.findings-acl.349\nWüster, E. (1974). Die Allgemeine Terminologielehre - Ein Grenzgebiet zwischen\nSprachwissenschaft, Logik, Ontologie, Informatik und den Sachwissenschaften.\nLinguistics, 12(119), 61–106. Retrieved 2025-05-21, from https://doi.org/\n10.1515/ling.1974.12.119.61 doi: doi:10.1515/ling.1974.12.119.61\n22 JLCL\nLLMs for terminology work\nAppendix\nThe appendix contains examples of prompts (according to the RACE and CARE\nframeworks) used in this study. The LLMs were prompted in German or English\ndepending on the task at hand: For example, if the task was related to Austrian\nuniversity terminology, the prompt was written in German, and if the task was solely\nrelated to the British university terminology, the prompt was in English. As all the\nGerman prompts will be made available as part of the larger UniTermGPT project, the\nfollowing section only gives examples of the prompts in English (including translations of\nthe German prompts). First, the different aspects of the selected prompting frameworks\nare illustrated. Second, examples of full prompts are provided. Third, an example of a\nprompt improved byPrompt Makerby Ruben Hassid is shown.\nSelected prompting frameworks applied to terminology tasks\nThe two selected prompting frameworks (RACE and CARE) applied to terminology\ntasks:\nTable 3:Prompting frameworks applied to terminology tasks\nTask CARE prompt RACE prompt\n(1) Term\nidentification\nContext: Identify university\nadmission terminology in\nAustria (and the UK).\nAction: Create list of key\nterms.\nResult: A list of admission-\nrelated terms (with brief\ndescriptions).\nExample: transcript of\nrecords = official document\nsummarizing a university\nstudent’sacademicperformance\nand progress to date.\nRole: Terminologist for higher\neducation.\nAction: Generate list of\nadmission terms.\nContext: Austrian and British\nuniversity system.\nExpectation: 10–15 terms per\nsystem (with brief definitions).\nJLCL 2025 – Band 38(2) 23\nHeinisch\nTask CARE prompt RACE prompt\n(2) Term\nextraction\nContext: Analyze university\nadmission terminology in\nAustria (and the UK).\nAction: Extract relevant terms\nfrom websites or authoritative\nsources.\nResult: A glossary of\nadmission-related terms (with\nsource).\nExample: Sammelzeugnis =\nZeugnis über alle absolvierten\nPrüfungen eines Studierenden\nan einer Universität.\nRole: Terminologist for higher\neducation.\nAction: Generate list of\nadmission terms from university\nwebsites.\nContext: Austrian and British\nuniversity system.\nExpectation: 10–15 terms per\nsystem (with source).\n(3) Definition\ngeneration or\nextraction\nContext: List of terms\ncollected from domain of\nuniversity admission.\nAction: Define each term\nusing ISO 704/1087 principles.\nResult: Structured definitions\nin German/English.\nExample:\n“Zulassungsbescheid” –\nVerwaltungsakt, mit\ndem eine Universität\nStudienwerber*innen formal\nmitteilt, dass sie für ein\nStudium in einer bestimmten\nStudienrichtung unter den\nangegebenen Bedingungen\naufgenommen sind.\nRole: Expert in ISO-compliant\nterminology work.\nAction: Write precise\ndefinitions.\nContext: Multilingual\nterminology work and\nmanagement.\nExpectation: 10-15\ndefinitions of concepts in\nGerman (and English).\n(4) Concept\nrelations\nContext: In terminology,\nconcepts are related to each\nother.\nAction: Identify hierarchical\n(generic and partitive) (and\nassociative) relations.\nResult: Concept system or\nstructure with relation types.\nExample: “University” →\n“Faculty” (partitive).\nRole: Terminologist comparing\nuniversity systems.\nAction: Mapconceptrelations.\nContext: Prepare concept\nsystem in German / English.\nExpectation: Hierarchical or\nassociative concept relations.\n24 JLCL\nLLMs for terminology work\nExamples of full prompts according to the selected prompting frameworks\nExamples of full prompts (mainly translated from German) according to the RACE\nand CARE prompting frameworks for the four selected terminology tasks are listed in\nthe following:\nTable 4:Prompting frameworks for terminology tasks using prose-style prompts\nTask CARE prompt RACE prompt\n(1) Term\nidentification\nIn the context of university\nadmission in Austria and the\nUK, generate a list of the most\nfrequently used terms in this\ndomain. Your task is to identify\nand list core terminology used in\nuniversity admission processes.\nThe result should be a bilingual list\n(Austrian German–British English)\nof 10 terms including definitions\naccording to terminological\nprinciples. For example:\nStudienwerber*in: Person, die an\neiner Universität die Zulassung\nzu einem bestimmten Studium\nbeantragt. – Applicant: person\nwho has submitted an application\nfor admission to a university.\nAs a terminologist specialising\nin higher education, identify\nkey terms related to university\nadmissions in both Austria and\nthe UK. Consider how admission\nis structured in each country.\nYour output should be a list of\n10 admission-related terms per\nsystem, each with a short definition\nin context.\nJLCL 2025 – Band 38(2) 25\nHeinisch\nTask CARE prompt RACE prompt\n(2) Term\nextraction\nFor terminology management in\nthe Austrian university system,\nusing official Austrian websites\nor normative documents, extract\nkey terms related to the study\nadmission process in Austria.\nThis includes identifying domain-\nspecific terminology from legal\ntexts or university materials.\nProvide the term, its source (and\na short contextual definition)\nin a three-column table. For\nexample: 1) Studieneingangs- und\nOrientierungsphase, 2) Angebot\nvon Lehrveranstaltungen aus\nden das jeweilige Diplom- oder\nBachelorstudium besonders\nkennzeichnenden Fächern, das der\nInformation und der Orientierung\nder Studienanfängerinnen und\nStudienanfänger dient, 3)URL.\nAct as a terminology expert\nconducting web-based term\nextraction in the field of Austrian\nuniversity terminology. Focus on\nextracting university admission\nterminology from Austrian\ninstitutionalwebsites(e.g. ministry\nor university portals). Present\n10–15 relevant terms from the\nAustrian higher education system\nand include their source (URL or\ndocument title).\n26 JLCL\nLLMs for terminology work\nTask CARE prompt RACE prompt\n(3)\nDefinition\ngeneration or\nextraction\nFor comparative terminology work\nin the university sector in German\nand English, compile a list of the\nmost common terms in the field\nof university admissions. Add\ndefinitions for these terms from\nAustrian and British websites\nand preferably from normative\nor official sources, such as laws\nor documents from authorities or\norganisations. Compare terms\nfrom the Austrian and British\nhigher education systems. Here\nis an example: Sammelzeugnis\n= ‘Zeugnis über alle absolvierten\nPrüfungen eines Studierenden an\neiner Universität’, the English\nequivalent is: transcript of records\n= ‘official document summarising\na university student’s academic\nperformance and progress to date’.\nAdd the URL of the website where\nyou found the definitions.\nYou are a terminologist and\nneed definitions of terms in the\nuniversity context in Austrian\nGerman and British English in\nthe field of university admissions.\nGiven a preselected list of terms\nrelated to university admission,\ndefine each term using principles\nfrom ISO 704 and ISO 1087. Where\npossible, extract official definitions\nfrom trusted sources (e.g. laws\nor university guidelines). Create\na table with 6 columns: 1st\ncolumn: German term. 2nd\ncolumn: German definition for this\nterm. 3rd column: Source of\nthe definition from the Internet\nby specifying a real website from\nAustria (domain: ac.at). 4th\ncolumn: English term. 5th\ncolumn: English definition. 6th\ncolumn: Source of the definition\nfrom the Internet by specifying a\nreal website from Great Britain\n(domain: ac.uk).\n(4) Concept\nrelations\nIn the context of terminological\nconcept systems, analyze the\nrelationships between the concepts\n(and terms) identified in the\nAustrian university context. Your\ntask is to construct a concept\nstructure for the field of university\nadmission in Austria showing\nhierarchical (generic and partitive)\nor associative relationships. The\nresult should support the creation\nof (comparative) concept systems\nin multilingual terminology work,\ne.g. University → Faculty\n(partitive).\nAs an ISO-compliant terminologist\ncomparing the Austrian and British\nhigher education systems, identify\nconcept relationships among\nuniversity admission terminology.\nDevelop a concept system that\nincludes hierarchy (e.g. admission\ndocument → Zulassungsbescheid)\nand differences between the\nAustrian and British university\nadmission systems. Document\nthe structure clearly for bilingual\ncomparison.\nJLCL 2025 – Band 38(2) 27\nHeinisch\nExample of prompt improved by Prompt Maker LLM\nExample of prompt (in German) improved by Prompt Maker by Ruben Hassid\n(although the terminology of terminology is not correct in the improved prompt, e.g.\nFachbegriff instead ofBenennung, the LLM output with the improved prompt was not\nchanged, except for the headings in the sample table):\nPromptbeschreibung: Terminologiearbeit im Hochschulbereich (AT/UK)\nRolle & Kontext: Agiere als Experte für vergleichende Terminologiearbeit im\nHochschulbereich mit Spezialisierung auf Studienzulassungsverfahren in Österreich und\nGroßbritannien. Deine Aufgabe ist es, eine präzise, systematische Liste der häufigsten\nFachbegriffe in diesem Bereich zu erstellen und die entsprechenden Begriffe auf Deutsch\nund Englisch zu vergleichen.\nZiel & Anforderungen:\n1. Erstelle eine umfassende Liste relevanter Begriffe aus dem Bereich der\nStudienzulassung.\n2. Für jeden Begriff liefere folgende Informationen:\n• Deutsche Bezeichnung\n• Englische Entsprechung (britisches Hochschulsystem)\n• Definition in beiden Sprachen, basierend auf offiziellen Quellen (z.B.\ngesetzliche Regelungen, Dokumente von Hochschulen oder Behörden)\n• Quelle der Definition(falls verfügbar)\n3. Beachte spezifische Unterschiede zwischen dem österreichischen und britischen\nHochschulsystem. Falls es keine exakte Entsprechung gibt, erläutere die\nUnterschiede.\n28 JLCL\nLLMs for terminology work\nBeispieltabelle\nTable 5:Vergleich von Fachbegriffen der Studienzulassung (AT/UK)\nDeutsche\nBenennung\nEnglische\nBenennung\nDeutsche\nDefinition\nQuelle\nDef. DE\nEnglische\nDefinition\nQuelle\nDef. EN\nSammel-\nzeugnis\ntranscript\nof records\nZeugnis\nüber alle\nabsolvierten\nPrüfungen\neines\nStudierenden\nan einer\nUniversität.\n[Name der\nQuelle]\nOfficial\ndocument\nsummarizing\na\nuniversity\nstudent’s\nacademic\nperformance\nand\nprogress to\ndate.\n[Name der\nQuelle]\nDetaillierter Arbeitsablauf\n1. Recherche: Identifiziere die wichtigsten Begriffe im Bereich der Studienzulassung\nin Österreich und Großbritannien.\n2. Kategorisierung: Ordne die Begriffe in sinnvolle Gruppen ein (z.B.\nBewerbungsunterlagen, Zulassungsbedingungen, Prüfungsanforderungen).\n3. Definitionserstellung: Nutze vertrauenswürdige Quellen, um exakte\nDefinitionen bereitzustellen. Falls es keine direkte Entsprechung gibt, erkläre die\nUnterschiede.\n4. Formatierung: Präsentiere die Ergebnisse in einer strukturierten Tabelle für\nbessere Übersichtlichkeit.\nWichtige Hinweise:\n• Bevorzuge offizielle Quellen wie Hochschulgesetze, Webseiten von Universitäten\noder Ministerien.\n• Falls Begriffe Mehrdeutigkeiten aufweisen, gib eine kurze Kontext-Erklärung dazu.\n• Falls es im britischen System kein direktes Äquivalent gibt, erläutere den\nUnterschied.\nNimm dir Zeit und arbeite die AufgabeSchritt für Schrittdurch.\nJLCL 2025 – Band 38(2) 29\nHeinisch\nCorrespondence\nBarbara Heinisch\nEurac Research\nInstitute for Applied Linguistics\nBolzano, Italy\nbarbara.heinisch@eurac.edu\n30 JLCL",
  "topic": "Terminology",
  "concepts": [
    {
      "name": "Terminology",
      "score": 0.7958253622055054
    },
    {
      "name": "Work (physics)",
      "score": 0.5667300820350647
    },
    {
      "name": "Computer science",
      "score": 0.5085150003433228
    },
    {
      "name": "Linguistics",
      "score": 0.5076302289962769
    },
    {
      "name": "Engineering",
      "score": 0.16106903553009033
    },
    {
      "name": "Philosophy",
      "score": 0.11131495237350464
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ]
}