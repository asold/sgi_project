{
  "title": "The Impact of Pretrained Language Models on Negation and Speculation Detection in Cross-Lingual Medical Text: Comparative Study",
  "url": "https://openalex.org/W3096461368",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5071169048",
      "name": "Renzo Rivera Zavala",
      "affiliations": [
        "Universidad Carlos III de Madrid",
        "Catholic University of Santa María"
      ]
    },
    {
      "id": "https://openalex.org/A2099769895",
      "name": "Paloma Martínez",
      "affiliations": [
        "Universidad Carlos III de Madrid"
      ]
    },
    {
      "id": "https://openalex.org/A5071169048",
      "name": "Renzo Rivera Zavala",
      "affiliations": [
        "Catholic University of Santa María",
        "Universidad Carlos III de Madrid"
      ]
    },
    {
      "id": "https://openalex.org/A2099769895",
      "name": "Paloma Martínez",
      "affiliations": [
        "Universidad Carlos III de Madrid"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4212881197",
    "https://openalex.org/W2883897757",
    "https://openalex.org/W1982464493",
    "https://openalex.org/W180510140",
    "https://openalex.org/W1987268614",
    "https://openalex.org/W2139865360",
    "https://openalex.org/W2111698097",
    "https://openalex.org/W2901413829",
    "https://openalex.org/W2550523331",
    "https://openalex.org/W2105182048",
    "https://openalex.org/W2886493354",
    "https://openalex.org/W2891950275",
    "https://openalex.org/W2136242459",
    "https://openalex.org/W2522034247",
    "https://openalex.org/W2142786899",
    "https://openalex.org/W40385435",
    "https://openalex.org/W1963753246",
    "https://openalex.org/W2038889169",
    "https://openalex.org/W1504212872",
    "https://openalex.org/W1984187075",
    "https://openalex.org/W2171660026",
    "https://openalex.org/W2123564229",
    "https://openalex.org/W2115915625",
    "https://openalex.org/W2401132586",
    "https://openalex.org/W2618063915",
    "https://openalex.org/W2064362294",
    "https://openalex.org/W2949176808",
    "https://openalex.org/W2963522845",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2135932125",
    "https://openalex.org/W2086817924",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W2129500526",
    "https://openalex.org/W2123207900",
    "https://openalex.org/W2575386989",
    "https://openalex.org/W1910766207",
    "https://openalex.org/W2175675225"
  ],
  "abstract": "Background Negation and speculation are critical elements in natural language processing (NLP)-related tasks, such as information extraction, as these phenomena change the truth value of a proposition. In the clinical narrative that is informal, these linguistic facts are used extensively with the objective of indicating hypotheses, impressions, or negative findings. Previous state-of-the-art approaches addressed negation and speculation detection tasks using rule-based methods, but in the last few years, models based on machine learning and deep learning exploiting morphological, syntactic, and semantic features represented as spare and dense vectors have emerged. However, although such methods of named entity recognition (NER) employ a broad set of features, they are limited to existing pretrained models for a specific domain or language. Objective As a fundamental subsystem of any information extraction pipeline, a system for cross-lingual and domain-independent negation and speculation detection was introduced with special focus on the biomedical scientific literature and clinical narrative. In this work, detection of negation and speculation was considered as a sequence-labeling task where cues and the scopes of both phenomena are recognized as a sequence of nested labels recognized in a single step. Methods We proposed the following two approaches for negation and speculation detection: (1) bidirectional long short-term memory (Bi-LSTM) and conditional random field using character, word, and sense embeddings to deal with the extraction of semantic, syntactic, and contextual patterns and (2) bidirectional encoder representations for transformers (BERT) with fine tuning for NER. Results The approach was evaluated for English and Spanish languages on biomedical and review text, particularly with the BioScope corpus, IULA corpus, and SFU Spanish Review corpus, with F-measures of 86.6%, 85.0%, and 88.1%, respectively, for NeuroNER and 86.4%, 80.8%, and 91.7%, respectively, for BERT. Conclusions These results show that these architectures perform considerably better than the previous rule-based and conventional machine learning–based systems. Moreover, our analysis results show that pretrained word embedding and particularly contextualized embedding for biomedical corpora help to understand complexities inherent to biomedical text.",
  "full_text": "Original Paper\nThe Impact of Pretrained Language Models on Negation and\nSpeculation Detection in Cross-Lingual Medical Text: Comparative\nStudy\nRenzo Rivera Zavala1,2*, MSc; Paloma Martinez1*, PhD\n1Department of Computer Science and Engineering, Carlos III University of Madrid, Madrid, Spain\n2Department of Computer Science and Engineering, Universidad Católica de Santa Maria, Arequipa, Peru\n*all authors contributed equally\nCorresponding Author:\nRenzo Rivera Zavala, MSc\nDepartment of Computer Science and Engineering\nCarlos III University of Madrid\nAvda. Universidad, 30\nLeganes\nMadrid, 28911\nSpain\nPhone: 34 916249433\nEmail: renzomauricio.rivera@alumnos.uc3m.es\nAbstract\nBackground: Negation and speculation are critical elements in natural language processing (NLP)-related tasks, such as\ninformation extraction, as these phenomena change the truth value of a proposition. In the clinical narrative that is informal, these\nlinguistic facts are used extensively with the objective of indicating hypotheses, impressions, or negative findings. Previous\nstate-of-the-art approaches addressed negation and speculation detection tasks using rule-based methods, but in the last few years,\nmodels based on machine learning and deep learning exploiting morphological, syntactic, and semantic features represented as\nspare and dense vectors have emerged. However, although such methods of named entity recognition (NER) employ a broad set\nof features, they are limited to existing pretrained models for a specific domain or language.\nObjective: As a fundamental subsystem of any information extraction pipeline, a system for cross-lingual and domain-independent\nnegation and speculation detection was introduced with special focus on the biomedical scientific literature and clinical narrative.\nIn this work, detection of negation and speculation was considered as a sequence-labeling task where cues and the scopes of both\nphenomena are recognized as a sequence of nested labels recognized in a single step.\nMethods: We proposed the following two approaches for negation and speculation detection: (1) bidirectional long short-term\nmemory (Bi-LSTM) and conditional random field using character, word, and sense embeddings to deal with the extraction of\nsemantic, syntactic, and contextual patterns and (2) bidirectional encoder representations for transformers (BERT) with fine\ntuning for NER.\nResults: The approach was evaluated for English and Spanish languages on biomedical and review text, particularly with the\nBioScope corpus, IULA corpus, and SFU Spanish Review corpus, with F-measures of 86.6%, 85.0%, and 88.1%, respectively,\nfor NeuroNER and 86.4%, 80.8%, and 91.7%, respectively, for BERT.\nConclusions: These results show that these architectures perform considerably better than the previous rule-based and conventional\nmachine learning–based systems. Moreover, our analysis results show that pretrained word embedding and particularly\ncontextualized embedding for biomedical corpora help to understand complexities inherent to biomedical text.\n(JMIR Med Inform 2020;8(12):e18953) doi: 10.2196/18953\nKEYWORDS\nnatural language processing; clinical text; deep learning; long short-term memory; contextual information\nJMIR Med Inform 2020 | vol. 8 | iss. 12 | e18953 | p. 1https://medinform.jmir.org/2020/12/e18953\n(page number not for citation purposes)\nRivera Zavala & MartinezJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nIntroduction\nA part of clinical data is often described in unstructured free\ntext, such as that recorded in electronic health records (EHRs),\nmedical records, and clinical narrative, which is not analyzed.\nBesides, scientific literature databases collect valuable\npublications necessary to extract biomedical data, such as drug\nor protein interactions, adverse drug effects, disabilities,\ndiseases, treatments, detection of cancer symptoms, and suicide\nprevention. Biomedical experts and clinicians need to access\ninformation and knowledge in their different research areas,\nconvert research results into clinical practice, accelerate\nbiomedical research, provide clinical decision support, and\ngenerate data and information in a structured way for\ndownstream processing and applications, such as those specified\npreviously [1]. However, identifying all the data in unstructured\ndocuments and translating these data to structured data can be\na complex and time-consuming task. It is impossible for experts\nto process all the documents without tools that filter, classify,\nand extract information. That is why new techniques are\nnecessary for the extraction of useful knowledge in a precise\nand efficient way.\nOne of the main tools currently used for text mining is natural\nlanguage processing (NLP) and specifically an information\nextraction system. Information extraction is devoted to\nprocessing text and detecting relevant information about specific\nsubjects (for instance, a disease of a patient in a clinical note or\na carcinoma in a radiologic report). In information extraction,\nwe can identify low-level tasks and high-level tasks (Figure 1).\nLow-level tasks are more feasible and affordable processing\ntasks, such as sentence segmentation, tokenization, and word\ndecomposition. High-level tasks are more complex tasks because\nthey require semantic and contextual knowledge that is provided\nby domain-specific resources, such as ontologies, and they\ninvolve disambiguating terms (such as abbreviations that are\nhighly ambiguous terms) and making inferences with the\nextracted knowledge. These high-level tasks are named entity\nrecognition (NER), relation extraction, and negation and\nspeculation detection, among others (Tables 1 and 2). For\nexample, extracting a patient’s current diagnostic information\ninvolves NER, disambiguation, negation and speculation\ndetection, relation extraction, and temporal inference. Figure 2\nprovides an example of an annotation generated by a medical\ninformation extraction system [2].\nFigure 1. Typical information extraction pipeline. NLP: natural language processing; PoS: part of speech.\nJMIR Med Inform 2020 | vol. 8 | iss. 12 | e18953 | p. 2https://medinform.jmir.org/2020/12/e18953\n(page number not for citation purposes)\nRivera Zavala & MartinezJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nTable 1. Natural language processing low-level tasks.\nChallengeObjectiveTask\nHigh use of abbreviations and titles such as “mg” and “Dr” makes\nthis task difficult.\nDetection limit of a sentence.Sentence segmentation\nTerms combining different types of alphanumeric characters and\nother signs, such as hyphens, slash, and separators (“10 mg/day”\nand “N-acetylcysteine”).\nDetection of words and punctuation marks.Tokenization\nUse of homographs and gerunds.Assigns a PoS tag to a term.Part-of-speech (PoS) tagging\nMany medical terms, such as “nasogastric,” need decomposition\nto understand the meaning of the term.\nWord stemming by removing suffixes. Very\nimportant for concept normalization.\nDecomposition/lemmatization\nInherent complexities from the language (for instance, prepositional\nattachment).\nIdentification of the phrases of a sentence.Shallow parsing\nIn a clinical report, identify sections, such as patient’s history, di-\nagnosis, treatment, etc.\nDivision of the text into relevant parts, such\nas paragraphs, sections, and others.\nText segmentation\nTable 2. Natural language processing high-level tasks.\nChallengeObjectiveTask\nMultitoken concepts (“acute rhinovirus bronchitis”) and short\nconcepts (“mg”).\nIdentification and classification of concepts\nof interest, such as diseases, drugs, and\ngenes.\nNamed entity recognition\nA considerable number of abbreviations with several senses, such\nas Pt (patient/physiotherapy) and LFT (liver function test/lung\nfunction test).\nIdentification of the correct sense of a term\ngiven a specific context.\nDisambiguation\nThey are commonly marked in the clinical narrative by words such\nas “not” and “without.”\nInferring whether a named entity is present\nor absent.\nNegation and speculation detection\nRelation between a particular disease and a specific symptom or\ndrug-drug interaction. For example, pharmacodynamic interaction\nbetween aspirin and ibuprofen (antagonistic interaction).\nIdentification of relationships between\nconcepts.\nRelation extraction\nThe most complex task in information extraction. For example,\n“asbestos exposure and smoking until a particular genetic mutation\noccurs causes lung cancer in 1-3 years with a probability of 0.2.”\nGiven temporal expressions or temporal\nrelationships, inferences are made about\nprobable events in another temporal space.\nTemporal inferences\nFigure 2. Information extraction pipeline annotation result [2].\nConsequently, information extraction tools must address many\ninherent natural language challenges, such as ambiguity, spelling\nvariations, abbreviations, speculation, and negation. In this\nwork, we address the negation and speculation problems.\nNegation and speculation expressions are extensively used both\nin spoken and written communications. Negation converts a\nproposition represented by a linguistic unit (sentence, phrase,\nor word) into its opposite, for instance, the existence or absence\nof medical conditions in a clinical narrative. It is marked by\nwords (such as “not” and “without”), suffixes (such as “less”),\nor prefixes (such as “a”). Around 10% of the sentences in\nMEDLINE abstracts include negation phenomena [3]. The\nBioScope corpus contains more than 20,000 sentences, among\nwhich almost 2000 (11.4%) are negated or uncertain sentences\n[4]. In the general domain, the SFU ReviewSP-NEG corpus is\ncomposed of approximately 9455 sentences, among which\nnearly a third are negated or uncertain sentences [5]. Different\nworks have shown the importance of dealing with negations,\nfor instance, during the analysis of EHRs [1] or in information\nretrieval tasks on rare disease patient records related to Crohn\ndisease, lupus, and NPHP1 from a clinical data warehouse [6].\nIn relation to speculation (or modality), both are referred to as\nexpressing facts that are not known with certainty (such as\nhypotheses and conjectures). There are different types of\nJMIR Med Inform 2020 | vol. 8 | iss. 12 | e18953 | p. 3https://medinform.jmir.org/2020/12/e18953\n(page number not for citation purposes)\nRivera Zavala & MartinezJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nexpressions that have speculation meanings as follows: modal\nauxiliaries (must/should/might/may/could be), judgment verbs\n(suggest), evidential verbs (appear), deductive verbs (conclude),\nadjectives (likely), adverbs (perhaps), nouns (there is a\npossibility), conditional words, etc.\nThese phenomena have a scope, that is, affect a part of the text\ndenoted by the presence of negation or speculation cues. Cues\nusually occur in the context of some assumption, which works\nto deny or counteract that assumption. These cues can be single\nwords, simple phrases, or complex verb phrases, which may\nprecede or succeed the words that are within their scope [7].\nAccording to grammar, the scope of the negation or speculation\ncorresponds to the totality of words affected by it. In NLP,\nnegation or speculation cues act as operators that can change\nthe meaning of the words in their scope. Thus, they establish\nwhat is a fact and what is not, owing to the ability to affect the\ntruth value of a phrase or sentence [8]. However, negation\ndetection is a complex task owing to the multiple forms in which\nit can appear as follows: (1) syntactic (ie, negation in sentences,\nclauses, and phrases that include words expressing negation,\nsuch as no/not, never/ever, and nothing), (2) lexical negation\n(eg, “lack of”), and (3) morphological negation (eg, illegal and\nimpossible) [5].\nNegation processing can be divided into two phases. First,\nkeywords/cues indicating negation or speculation are detected,\nand second, definition of the linguistic scope of these cues is\nmade at the sentence level. In English, negation and speculation\ndetection is a well-studied phenomenon. However, in other\nlanguages, such as Spanish, it is an underaddressed and even\nmore complicated task owing to the limited number of annotated\ncorpora and the inherent complexities of the language, such as\ndouble negation (eg, the hospital will not allow no more\nvisitors). NegEx [9], one of the most popular rule-based\nalgorithms for negation detection in English, is a simple regular\nexpression-based algorithm that uses negation cue words without\nconsidering the semantics of a sentence. Some recent works\nalso exploit this algorithm for negation detection in other\nlanguages, such as French, German, and Swedish [10], Swedish\n[11], and Spanish [12]. Machine learning methods have been\napplied to cope with the negation detection task, using mainly\na conditional random field (CRF) algorithm with dense vector\nfeatures, such as character or word embedding [13,14]. More\nrecently, deep learning approaches using recurrent neural\nnetworks (RNNs), convolutional neuronal networks (CNNs),\nand encoder-decoder models have also been exploited to solve\nthis task [15-17].\nIn this work, we addressed the negation and speculation\ndetection tasks as named entity recognition (NER) tasks that\nsolve the identification of cues and scope of this phenomena in\na single step. We present two deep learning approaches. First,\nwe implemented two bidirectional long short-term memory\n(Bi-LSTM) layers with a CRF layer based on the NeuroNER\nmodel proposed previously [18]. Specifically, we extended\nNeuroNER by adding context information to the character and\nword-level information, such as part-of-speech (PoS) tags and\ninformation about overlapping or nested entities. Moreover, in\nthis work, we used several pretrained word-embedding models\nas follows: (1) word2vec model (Spanish Billion Word\nEmbeddings [19]), which was trained on the 2014 dump of\nWikipedia, (2) pretrained word2vec model of word embedding\ntrained with PubMed and PubMed Central articles [20], and (3)\nsense-disambiguation embedding model [21], where different\nword senses are represented with different sense vectors. To\nthe best of our knowledge, no previous work has exploited a\nsense embedding model for the negation detection task. Finally,\nwe implemented the bidirectional encoder representations for\ntransformers (BERT) model with fine tuning using a BERT\nmultilingual pretrained model.\nSince the health care system has started adopting cutting-edge\ntechnologies, there is a vast amount of data collected mainly in\nunstructured formats, such as clinical narratives, electronic\nreports, and EHRs. Therefore, there is a high amount of\nunstructured data. All of these data involve relevant challenges\nfor information extraction and utilization in the health care\ndomain through various applications of NLP in health care,\nsuch as clinical trial matching [22], automated registry reporting,\nclinical decision support [23], and predicting health care\nutilization [24]. However, all these applications must deal with\ninherent NLP challenges, with negation and speculation\ndetection being highly crucial owing to the abuse of negation\nand speculation particles in the clinical narrative and clinical\nrecords.\nWork in negation detection has focused on the following two\nsubtasks: (1) cue detection to identify negation terms and (2)\nscope resolution to determine the coverage of a cue in a phrase\nor sentence. However, in previous research, negation detection\nhas focused on the straight detection of negated entities [17].\nEarly negation detection work has relied on rule-based\napproaches. Rule-based approaches have been shown to be\neffective in NLP challenges. They use hand-crafted rules based\non grammatical patterns and keyword matching. Some\ntoken-based systems are NegEx [25], NegFinder [26],\nNegHunter [27], and NegExpander [28]. DepNeg [29] uses\nsyntactic parsing. Among rule-based approaches, the most used\nnegation detection tool in English is NegEx [13], which employs\nan exact match to a list of medical entities and negation triggers\n(eg, “NO history of exposure” and “DENIES any nausea”).\nNegEx was adapted to address negation detection for other\nlanguages, such as Swedish [11], French [30], German [12],\nand Spanish [31]. Light et al [3] used a hand-crafted list of\nnegation cues to identify speculation sentences in MEDLINE\nabstracts. Likewise, several biomedical NLP studies have used\nrules to identify the speculation of extracted information [32-35].\nAn analysis of a set of Spanish clinical notes from a hospital\n[36] reported some statistics of several groups of patterns\nconsidering the groups defined in the NegEx algorithm [25] as\nfollows: morphologically negates, adverbs, prenegative phrases,\npostnegative phrases, and pseudonegative phrases. These\npatterns were applied to the data set, and only the more frequent\npatterns were inspected (about 100 contexts per pattern). Figure\n3 shows the frequencies of the set of negation patterns in the\nstudied corpus, where negation patterns using adverbs (“no,”\n“ni,” and “sin”) are the more productive patterns, followed by\nadverbs together with evidential and perception verbs (eg, “no\nse evidencia” + symptom). There are other negation words, such\nJMIR Med Inform 2020 | vol. 8 | iss. 12 | e18953 | p. 4https://medinform.jmir.org/2020/12/e18953\n(page number not for citation purposes)\nRivera Zavala & MartinezJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nas “nadie” (nobody) and “negative” (negative), which do not appear in the data set.\nFigure 3. Statistics of the set of negation patterns [30].\nApproaches to speculation and negation detection that exploit\nsemisupervised or supervised machine learning models require\nmanually labeled corpora. Medlock [37] used spare word\nrepresentation features as inputs to classify sentences from\nbiological articles (included in the molecular biology database\nFlyBase) as certain or uncertain based on semiautomatically\ncollected training examples. Vincze et al [4] extended this\napproach [37] incorporating n-gram features and a\nsemisupervised selection of keyword features. Morante and\nDaelemans [38] created a negation cue and scope detection\nsystem in biomedical text. This system identifies negation cues\nusing the compressed decision tree (IGTREE) algorithm. It uses\na meta-learner based on memory-based learning, a support\nvector machine, and conditional random fields (CRFs) for\ndetermining the scope of the negation. The system was evaluated\non the BioScope data set [4], with an F-measure of 98.74% for\ncue detection and 89.15% for scope determination. Cruz et al\n[39] focused on negation cue detection in the BioScope corpus\nusing the C4.5 and naive bayes algorithms, with the top\nF-measure of 86.8% for biomedical articles. Other studies have\nincorporated POS tag information [40] or different classifiers\n[41] that followed the two-step approach. Zou et al [42]\nproposed a tree kernel–based method for scope identification,\nbased on structured syntactic parse features. The system was\nevaluated on the BioScope corpus, achieving a valuable\nimprovement compared with the state-of-the-art approach, with\nan F-measure of 92.8% for negation detection.\nIn previous years, negation and speculation detection was being\naddressed as a sequence-labeling task. One of the most used\nalgorithms for negation detection is CRF. White et al [43]\nproposed a CRF-based model with a set of lexical, structural,\nand syntactic features for scope detection. Kang et al [14]\nincorporated character-level and word-level dense\nrepresentations (embeddings) in a CRF algorithm. The best\nF-measure was 99% for cue detection and 94% for scope\ndetection in Chinese text, and it was concluded that embedding\nfeatures can help to achieve better performance. Santiso et al\n[13] proposed a similar system using spare and dense word\nfeature representations and a CRF algorithm to detect only\nnegated entities in Spanish clinical text. The system obtained\nF-measures of 45.8% and 81.2% for the IxaMed-GS corpus [44]\nand the IULA corpus [45], respectively.\nHowever, more recently, deep learning approaches are getting\nmore attention, specifically RNNs and CNNs. Lazib et al [46]\nproposed a hybrid RNN and CNN system with a feature set of\nword embedding and a syntactic path (the shortest syntactic\npath from the candidate token to the cue in both constituency\nand dependency parse trees) to treat this task, and it proved to\nbe very powerful in capturing the potential relationship between\nthe token and the cue. Later, Lazib et al [47] proposed various\nRNN models to automatically find the part of the sentence\naffected by a negation cue. They used an automatically extracted\nword embedding representation of the terms as the only feature.\nTheir Bi-LSTM model achieved an F-measure of 89.38% for\nthe SFU review corpus [48], outperforming all previous\nhand-encoded feature-based approaches.\nSimilarly, Fancellu et al [49] used a Bi-LSTM model to solve\nthe task of negation scope detection, and it outperformed the\nbest result of Sem shared task 2012 [50]. Some approaches were\nproposed to rely on syntactic parse information to automatically\nextract the most relevant features [51]. Qian et al [15] designed\na CNN-based model with probabilistic weighted average pooling\nto address speculation and negation scope detection. Evaluation\nof the BioScope corpus showed that their approach achieved\nsubstantial improvement. Finally, Bathia et al [17] proposed an\nend-to-end neural model to jointly extract entities and negations\nbased on the hierarchical encoder-decoder NER model. The\nJMIR Med Inform 2020 | vol. 8 | iss. 12 | e18953 | p. 5https://medinform.jmir.org/2020/12/e18953\n(page number not for citation purposes)\nRivera Zavala & MartinezJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nsystem was evaluated on the 2010 i2b2/VA challenge data set,\nobtaining an F-score of 90.5% for negation detection.\nMotivated by the recent success of machine learning and deep\nlearning approaches in solving various NLP issues, in this paper,\nwe proposed the following two methods: (1) a machine and\ndeep learning model combining two Bi-LSTM networks and a\nlast CRF network, and (2) a BERT model with fine tuning to\nsolve negation and speculation detection issues in multidomain\ntext in both English and Spanish. Negation processing in the\nSpanish clinical narrative has been little addressed in previous\nyears. Moreover, to the best of our knowledge, sense or context\nembedding has not been exploited for the negation detection\ntask.\nMethods\nOverview\nWe addressed the task of negation and speculation detection as\na sequence-labeling task, where we classified each token in a\nsentence as being part of the negation or speculation cue or\nnegation scope. We have presented the data sets used for\ntraining, validating, and evaluating our systems. We have\npresented a deep network with a preprocessing step, a learning\ntransfer phase, two recurrent neural network layers, and the last\nlayer with a CRF classifier. Moreover, to compare our system\nperformance, we used a baseline model based on a multilayer\nbidirectional transformer encoder.\nNER Architecture\nWe have address the NER task as a sequence-labeling task. In\norder to train our model, first, text must be preprocessed to\ncreate the input for the deep network. Sentences were split and\ntokenized using Spacy [52], an open-source library for advanced\nNLP with support for 26 languages. The output from the\nprevious process was formatted to BRAT format [53]. BRAT\nis a standoff format where each line represents an annotation\n(such as entity, relation, and event). We used the information\nfrom the BRAT format (example in Figure 4) to annotate each\ntoken in a sentence using BMEWO-V extended tag encoding\n(entity tags used in Table 3), which allowed us to capture\ninformation about the sequence of tokens in the sentence.\nFigure 4. Examples of annotations in BRAT format over a sentence extracted from the IULA Spanish Clinical Record corpus (translation to English:\nsoft, depressible abdomen, no masses or megalias, not painful).\nTable 3. Entity tags for BMEWO-V tag encoding in the IULA Spanish Clinical Record corpus.\nTagsEntity\nB/M/E/W/V-NegMarkerNegMarkera\nB/M/E/W/V-NegPolItemNegPolItemb\nB/M/E/W/V-NegPredMarkerNegPredMarkerc\nB/M/E/W/V-PROCPROCd\nB/M/E/W/V-DISODISOe\nB/M/E/W/V-PHRASEPHRASEf\nB/M/E/W/V-BODYBODYg\nB/M/E/W/V-SUBSSUBSh\nOOthers\naNegMarker: no, tampoco, sin [4].\nbNegPolItem: ni, ninguno, ... [4].\ncNegPredMarker: negative verbs, nouns, and adjectives [4].\ndPROC: procedure.\neDISO: clinical finding.\nfPHRASE: nonmedical text spans.\ngBODY: body structure.\nhSUBS: substance pharmacological/biological product.\nJMIR Med Inform 2020 | vol. 8 | iss. 12 | e18953 | p. 6https://medinform.jmir.org/2020/12/e18953\n(page number not for citation purposes)\nRivera Zavala & MartinezJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nIn BMEWO-V encoding, the B tag indicates the start of an\nentity, the M tag represents the continuity of an entity, the E\ntag indicates the end of an entity, the W tag indicates a single\nentity, and the O tag represents other tokens that do not belong\nto any entity. The V tag allows representation of overlapping\nentities. BMEWO-V is similar to other previous encodings [54];\nhowever, it also allows the representation of discontinuous\nentities and overlapping or nested entities. As a result, we\nobtained the sentences annotated in CoNLL-2003 format (Table\n4).\nTable 4. Tokens annotated in the ConLL-2003 format.\nTagTagEnd offsetStart offsetFileToken\nOOa70negation_iac_3_corrAbdomen\nOO148negation_iac_3_corrblando\nOO1514negation_iac_3_corr,\nOO2616negation_iac_3_corrdepresible\nOO2726negation_iac_3_corr,\nW-NegMarkerW-NegMarkerb3028negation_iac_3_corrno\nW-DISOdV-Phrasec3631negation_iac_3_corrmasas\nW-NegPolIteneV-Phrase3937negation_iac_3_corrni\nW-DISOV-Phrase4840negation_iac_3_corrmegalias\nOO4948negation_iac_3_corr,\nW-NegMarkerW-NegMarker5250negation_iac_3_corrno\nW-DISOW-DISO6153negation_iac_3_corrdoloroso\nOO6261negation_iac_3_corr.\naO: other (no entity annotation).\nbNegMarker: no, tampoco, sin [4].\ncPhrase: nonmedical text spans.\ndDISO: clinical finding.\neNegPolItem: ni, ninguno, ... [4].\nUnlike other detection approaches that detect negation or\nspeculation cues in the first stage and recognize the scope of\nboth of them in the second stage (two-stage system), we\nproposed a one-stage approach (threaten cue entities within\nscope entities as nested entities, recognizing both entities [cues\nand scopes] in a single stage).\nBi-LSTM CRF Model: NeuroNER Extended\nOur proposal involves the adaption of a state-of-the-art NER\nmodel named NeuroNER [18] based on deep learning to identify\nentities as negation and speculation. The architecture of our\nmodel consists of an initial Bi-LSTM layer for character\nembedding. In the second layer, we concatenate the output of\nthe first layer with word embedding and sense-disambiguate\nembedding for the second Bi-LSTM layer. Finally, the last layer\nuses a CRF to obtain the most suitable labels for each token.\nAn overview of the system architecture can be seen in Figure\n5.\nJMIR Med Inform 2020 | vol. 8 | iss. 12 | e18953 | p. 7https://medinform.jmir.org/2020/12/e18953\n(page number not for citation purposes)\nRivera Zavala & MartinezJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nFigure 5. The architecture of the hybrid Bi-LSTM CRF model for negation and speculation recognition. Bi-LSTM: bidirectional long short-term\nmemory; CRF: conditional random field.\nTo facilitate training of our model, we first performed a learning\ntransfer step. Learning transfer aims to perform a task on a data\nset using knowledge learned from a previous data set [55]. As\nis shown in many studies, speech recognition [56], sentence\nclassification [57], and NER [58] learning transfer improves\ngeneralization of the model, reduces training time on the target\ndata set, and reduces the amount of labeled data needed to obtain\nhigh performance. We propose learning transfer as input for\nour model using the following two different pretrained\nembedding models: (1) word embedding and (2)\nsense-disambiguation embedding. Word embedding is an\napproach to represent words as vectors of real numbers, which\nhas gained much popularity among the NLP community because\nit is able to capture syntactic and semantic information among\nwords.\nAlthough word embedding models are able to capture syntactic\nand semantic information, other linguistic information, such as\nmorphological information, orthographic transcription, and POS\ntags, are not exploited in these models. According to a previous\nreport [59], the use of character embedding improves learning\nfor specific domains and is useful for morphologically rich\nlanguages (as is the case of the Spanish language). For this\nreason, we decided to consider the character embedding\nrepresentation in our system to obtain morphological and\northographic information from words. We used a 25-feature\nvector to represent each character. In this way, tokens in\nsentences are represented by their corresponding character\nembeddings, which are the inputs for our Bi-LSTM network.\nWe used the Spanish Billion Words model [19], which is a\npretrained model of word embedding trained on different text\ncorpora written in Spanish (such as Ancora Corpus [60] and\nWikipedia). Furthermore, we used a pretrained word embedding\nmodel induced from PubMed and PubMed Central texts and\ntheir combination using the word2vec tool [20]. PubMed text\nconsiders abstracts of scientific articles as of the end of\nSeptember 2013, with a total of 22 million records. PubMed\nCentral text considers full-text articles as of the end of\nSeptember 2013 and constitutes a total of 600,000 articles. These\nresources were derived from the combination of abstracts from\nPubMed and full-text documents from the PubMed Central\nJMIR Med Inform 2020 | vol. 8 | iss. 12 | e18953 | p. 8https://medinform.jmir.org/2020/12/e18953\n(page number not for citation purposes)\nRivera Zavala & MartinezJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nOpen Access subset written in English. We also experimented\nwith Google word2vec embedding [61] trained on 100 billion\nwords from Google News [62].\nWe also integrated the sense2vec [21] model, which provides\nmultiple embeddings for each word based on the sense of the\nword. This model is able to analyze the context of a word and\nthen assign a more adequate vector for the meaning of the word.\nIn particular, we used the Reddit Vector, a pretrained model of\nsense-disambiguation representation vectors introduced\npreviously [21]. This model was trained on a collection of\ncomments published on Reddit (corresponding to the year 2015).\nThe details of pretrained embedding models are shown in Table\n5.\nTable 5. Details of the pretrained embedding models.\nRedditPubMed and PubMed CentralGoogle NewsSpanish Billion WordsDetail\nMultilingualEnglishEnglishSpanishLanguage\n2 billion6 trillion100 billion1.5 billionCorpus size\n1 million2 million3 million1 millionVocab size\n128200300300Array size\nSense2VecSkip-gram BOWSkip-gram BOWSkip-gram BOWAlgorithm\nThe output of the first layer was concatenated with word\nembedding and sense-disambiguation embedding obtained from\npretrained models for each token in a given input sentence. This\nconcatenation of features was the input for the second Bi-LSTM\nlayer. The goal of the second layer was to obtain a sequence of\nprobabilities corresponding to each label of the BMEWO-V\nencoding format. In this way, for each input token, this layer\nreturned six probabilities (one for each tag in BMEWO-V). The\nfinal tag should be with the highest probability for each token.\nTo improve the accuracy of predictions, we also used a CRF\n[63] model, which takes as input the label probability for each\nindependent token from the previous layer and obtains the most\nprobable sequence of predicted labels based on the correlations\nbetween labels and their context. Handling independent labels\nfor each word shows sequence limitations. For example,\nconsidering the drug sequence-labeling problem, an\n“I-NEGATION” tag cannot be found before a “B-NEGATION”\ntag or an “I- NEGATION” tag cannot be found after a\n“B-NEGATION” tag. Finally, once tokens have been annotated\nwith their corresponding labels in the BMEWO-V encoding\nformat, the entity mentions must be transformed into the BRAT\nformat. V tags, which identify nested or overlapping entities,\nare generated as new annotations within the scope of other\nmentions.\nMultilayer Bidirectional Transformer Encoder: BERT\nThe use of word representations from pretrained unsupervised\nmethods is a crucial step in NER pipelines. Previous models,\nsuch as word2vec [62], Glove [64], and FastText [65], focused\non context-independent word representations or word\nembedding. However, in the last few years, models have focused\non learning context-dependent word representations, such as\nELMo [66], CoVe [67], and the state-of-the-art BERT model\n[68], and then fine tuning these pretrained models on\ndownstream tasks.\nBERT is a context-dependent word representation model that\nis based on a masked language model and is pretrained using\nthe transformer architecture [69]. BERT replaces the sequential\nnature of language modeling. Previous models, such as RNN\n(LSTM & GRU), combine two unidirectional layers (ie,\nBi-LSTM), and as a replacement for the sequential approach,\nthe BERT model employs a much faster attention-based\napproach. BERT is pretrained in the following two unsupervised\ntasks: (1) masked language modeling that predicts randomly\nmasked words in a sequence and hence can be used for learning\nbidirectional representations by jointly conditioning both left\nand right contexts in all layers and (2) next sentence prediction\nto train a model that understands sentence relationships. A\nprevious report [70] provides a detailed description of BERT.\nOwing to the benefits of the BERT model, we adopted a\npretrained BERT model with 12 transformer layers (12 layers,\n768 hidden, 12 heads, 110 million parameters) and an output\nlayer with SoftMax to perform the NER task. The transformer\nlayer has the following two sublayers: a multihead self-attention\nmechanism, and a position-wise, fully connected, feed-forward\nnetwork, followed by a normalization layer. An overview of\nthe BERT architecture is presented in Figure 6.\nJMIR Med Inform 2020 | vol. 8 | iss. 12 | e18953 | p. 9https://medinform.jmir.org/2020/12/e18953\n(page number not for citation purposes)\nRivera Zavala & MartinezJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nFigure 6. BERT pretraining and fine-tuning architecture overview [62]. BERT: bidirectional encoder representations from transformers.\nData Sets\nThe proposed systems are evaluated for the following three data\nsets: (1) the BioScope corpus introduced in the CoNLL-2010\nShared Task [7] for the detection of speculation cues and their\nlinguistic scope [4], (2) the SFU ReviewSP-NEG corpus used\nin Task 2 in the 2018 edition of the Workshop on Negation in\nSpanish (NEGES 2018) [71], and (3) the IULA Spanish Clinical\nRecord corpus [72]. Therefore, we evaluated the proposed\nsystem in two different languages (English and Spanish) and\ndifferent text types (clinical narrative, biomedical literature, and\nuser reviews). Spanish, contrary to other languages such as\nEnglish, does not have enough corpora, data sets, pretrained\nmodels, and resources. Furthermore, research on Spanish\nnegation and speculation detection is insufficient, and this is\neven more in the biomedical domain. Being aware of this\nsetback, in this particular study, we used the scarce Spanish\nresources available.\nThe BioScope corpus is a widely used and freely available\nresource consisting of medical and biological texts written in\nEnglish annotated with speculative and negative cues and their\nscopes. BioScope includes the following three different\nsubcorpora: (1) clinical free texts (clinical radiology records),\n(2) full biological papers from Flybase and the BMC\nBioinformatics website, and (3) biological abstracts from the\nGENIA corpus [73]. The corpus statistics are shown in Table\n6.\nTable 6. BioScope corpus details.\nClinical narrativesFull papersAbstractsVariable\nTotal\n127391954Number of documents\n11,87226246383Number of sentences\nSpeculation\n8555192101Number of sentences\n11126722659Number of scopes\nNegation\n8653391597Number of sentences\n8703761719Number of scopes\nConcerning negation and speculation, the CoNNLL-2010 Shared\nTasks divide the BioScope data set into three subtasks. The first\ntwo subtasks are as follows: (1) Task 1B sentence speculation\ndetection for biological abstracts and full articles and (2) Task\n1W sentence speculation detection for paragraphs from\nWikipedia, possibly containing weasel information. Both tasks\nconsist of a binary classification problem for detecting\nspeculation cues and speculation at the sentence level and the\nfinal task (Task 2), which aims the in-sentence hedge scope to\ndistinguish uncertain information from facts in general and\nbiomedical domains. The BioScope corpus includes a different\ndata set for each subtask. Detailed information about these data\nsets can be seen in Table 7.\nJMIR Med Inform 2020 | vol. 8 | iss. 12 | e18953 | p. 10https://medinform.jmir.org/2020/12/e18953\n(page number not for citation purposes)\nRivera Zavala & MartinezJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nTable 7. BioScope subtask data sets.\nNumber of scopesNumber of cuesNumber of sentencesNumber of documentsTask and subset\nTask 1B\nN/Aa254010,806966Training\nN/A8363735316Validation\nN/AN/A500315Testing\nTask 1W\nN/A236383431646Training\nN/A7702768540Validation\nN/AN/A96342346Testing\nTask 2\n2519255611,009966Training\n8088203533316Validation\nN/AN/A500315Testing\naN/A: not applicable.\nThe IULA Spanish Clinical Record corpus consists of 300\nmanually annotated and anonymized clinical records from\nseveral services of one of the main hospitals in Barcelona. These\nclinical records are written in Spanish. The corpus contains\nannotations on syntactic and lexical negation markers and their\nrespective scopes. Morphological negation was excluded. There\nare 3194 sentences, and of these, 1093 (34.22%) were annotated\nwith negation cues. IULA Spanish Clinical Record corpus details\nand its entity distribution can be found in Tables 8 and 9,\nrespectively.\nTable 8. IULA Spanish Clinical Record corpus details.\nClinical narrative, nItem\n300Documents\n3194Sentences\n1093Annotated sentences\n1456Negated entities\nJMIR Med Inform 2020 | vol. 8 | iss. 12 | e18953 | p. 11https://medinform.jmir.org/2020/12/e18953\n(page number not for citation purposes)\nRivera Zavala & MartinezJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nTable 9. IULA Spanish Clinical Record corpus entity distribution.\nTotal, nEntity\n1007NegMarkera\n86NegPredMarkerb\n114NegPolItemc\n7BODYd\n14SUBSe\n1064DISOf\n93PROCg\n278Phraseh\naNegMarker: no, tampoco, sin [4].\nbNegPredMarker: negative verbs, nouns, and adjectives [4].\ncNegPolItem: ni, ninguno, ... [4].\ndBODY: body structure.\neSUBS: substance pharmacological/biological product.\nfDISO: clinical finding.\ngPROC: procedure.\nhPHRASE: nonmedical text spans.\nTo the best of our knowledge, the IULA Spanish Clinical Record\ncorpus has not been used in any task or challenge. Therefore,\nwe randomly split the data set into training, validation, and\ntesting data sets. Details about the data sets can be seen in Table\n10.\nTable 10. IULA Spanish Clinical Record data sets.\nNumber of entitiesNumber of sentencesSubset\n28391774Training\n924701Validation\n920719Testing\nThe SFU ReviewSP-NEG corpus is the first Spanish corpus\nthat includes event negation as part of the annotation scheme,\nas well as the annotation of discontinuous negation markers.\nMoreover, it is the first corpus where the negation scope is\nannotated. The corpus also includes syntactic negation, scope,\nand focus. However, neither lexical nor morphological negation\nis included. Annotations on the event and on how negation\naffects the polarity of the words within its scope are also\nincluded. The Spanish SFU Review corpus consists of 400\nreviews from the Ciao website [74] from the following eight\ndifferent domains: cars, hotels, washing machines, books,\nphones, music, computers, and movies. It is composed of 9455\nsentences, and of these, 3022 (31.97%) contain at least one\nnegation cue. SFU ReviewSP-NEG corpus text distribution can\nbe found in Table 11. The SFU ReviewSP-NEG corpus was\nused in Task 2 of NEGES 2018 for identifying negation cues\nin Spanish. The data set was randomly divided into training,\nvalidation, and testing data sets. Details about the data sets can\nbe seen in Table 12.\nTable 11. SFU ReviewSP-NEG corpus details.\nReviews, nItem\n400Comments\n9455Sentences\n3022Annotated sentences\n3941Negated entities\nJMIR Med Inform 2020 | vol. 8 | iss. 12 | e18953 | p. 12https://medinform.jmir.org/2020/12/e18953\n(page number not for citation purposes)\nRivera Zavala & MartinezJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nTable 12. SFU ReviewSP-NEG data sets.\nNegated entities, nSentences, nReviews, nSubset\n6061774264Training\n20970156Validation\n28571980Testing\nNegation cues and scope are annotated in each corpus (the IULA\ncorpus does not include the subject within the scope). Regarding\nthe negation in coordinated structures, the corpora also show\ndifferences. In the SFU ReviewSP-NEG corpus, a distinction\nis made between the coordinated negative structures. Each\nnegation cue is independent and has its own scope. Moreover,\nthe scopes of those negative structures with discontinuous\nnegation cues consider the whole coordination. The IULA\nSpanish Clinical Record always includes coordination within\nthe scope. Furthermore, we found that double negation (eg, “No\nsíntoma de disnea NI dolor torácico” [No symptoms of dyspnea\nor chest pain]) and negation locutions, which are multiword\nexpressions that express negation (eg, “con AUSENCIA DE\nvasoespasmo” [with absence of vasospasm]) were only\naddressed in the SFU ReviewSP-NEG corpus. Additionally,\nspeculative expressions and uncertain annotations (eg, “Earths\nand clays MAY have provided prehistoric peoples”) were only\naddressed in the BioScope corpus.\nResults\nWe evaluated the negation detection system using the training,\nvalidation, and testing data sets provided by the task organizers\nfor the CoNLL-2010 Shared Task (BioScope) and for Task 2\nof NEGES 2018 (SFU ReviewSP-NEG). The IULA Spanish\nClinical Record corpus has not been previously applied to any\ntask or competition. Therefore, we split the corpus randomly\ninto training and testing data sets to evaluate the proposal in the\nclinical domain.\nThe Bi-LSTM CRF model was trained using available pretrained\nword and sense embedding models on general and biomedical\ndomains for Spanish, English, and multilingual texts. We\nevaluated the use of multidomain and multilanguage pretrained\nembedding models (general domain word and sense embeddings\nand multilanguage NLP tools) on the BioScope Task 1W data\nsets (biomedical domain and English text), with a precision,\nrecall, and F-score of 86.2%, 87%, and 86.6%, respectively.\nBased on our experiments, we found that the use of specific\ndomain (biomedical) and specific language (English)\nembeddings highly improved the negation and speculation\ndetection task (Table 13). Moreover, to evaluate the performance\nimpact, we evaluated each of our proposed features and made\ncomparisons with base NeuroNER implementation with PubMed\nand PubMed Central word embeddings on the BioScope Task\n1W test data set. As shown in Table 14, sense feature\nrepresentation and the BIOES-V tag encoding format improved\neach token representation, which implies that features play\ndifferent roles in capturing token-level features for NER tasks,\nthus making improvements in their combination.\nTable 13. Pretrained word embedding model evaluation on the BioScope Task 1W test data set.\nF-score (%)Recall (%)Precision (%)Name–embedding\n79.380.478.3NeuroNER–Google News\n81.482.180.8NeuroNER–PubMed and PubMed Central\n81.783.280.2NeuroNER Extended–Google News\n86.687.086.2NeuroNER Extended–PubMed and PubMed Central\nTable 14. Feature evaluation on the BioScope Task 1W test data set.\nF-score (%)Recall (%)Precision (%)Name–feature\n81.480.478.3NeuroNER–Base\n85.486.284.7NeuroNER–Sense\n82.683.581.7NeuroNER–BIOES-V\n86.687.086.2NeuroNER–Sense and BIOES-V\nMoreover, we used the pretrained BERT multilingual general\ndomain model with 12 transformer layers (12 layers, 768 hidden,\n12 heads, 110 million parameters) trained on the general domain\nWikipedia and Bookcorpus corpora, and fine-tuned for NER\nusing a single output layer based on the representations from\nits last layer to compute only token-level BIOES-V probabilities.\nBERT directly learns WordPiece embeddings during the\npretraining and fine-tuning steps.\nPrecision, recall, and the F-score were used to evaluate the\nperformance of our system. The parameters of the sets and the\nhyperparameters for our Bi-LSTM CRF model are summarized\nin Table 15. The hyperparameters were optimized on each\nvalidation data set.\nJMIR Med Inform 2020 | vol. 8 | iss. 12 | e18953 | p. 13https://medinform.jmir.org/2020/12/e18953\n(page number not for citation purposes)\nRivera Zavala & MartinezJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nTable 15. NeuroNER system hyperparameters for each task.\nSFU ReviewSP-NEGIULABioScopeParameter\nSpanishSpanishEnglishLanguage\nSpanish Billion Words + RedditSpanish Billion Words + RedditPubMed and PubMed Central +\nReddit\nPretrained word embedding\n128128128Sense-disambiguation embedding\ndimension\n300300200Word embedding dimension\n505050Character embedding dimension\n100100100Hidden layers dimension (for each\nLSTM)\nStochastic gradient descentStochastic gradient descentStochastic gradient descentLearning method\n0.50.50.5Dropout rate\n0.0050.0050.005Learning rate\n100100100Epochs\nThe CoNLL-2010 Shared Task [75] considers two different\nevaluation criteria. Task 1 is made at the sentence level, and\ncue annotations in the sentence are not considered. However,\nit is optionally evaluated. The F-measure of the speculation\nclass is employed as the chief evaluation metric. Task 2 involves\nthe annotation of “cue” + “xcope” tags in sentences. The\nscope-level F-measure is used as the chief metric where true\npositives are scopes that match the gold standard clue words\nand gold standard scope boundaries assigned to the clue words.\nTables 16 to 20 compare the results obtained by the participating\nsystems in the CoNLL-2010 Shared Task and our deep learning\napproach using pretrained embedding models and the\nBMEWO-V encoding format. Our extended version of\nNeuroNER achieved similar results to the best work presented\nin this task. In particular, our system achieved the highest\nprecision (83.2%), with lower recall.\nFor subtask 1 (identification speculation at the sentence level\nand cue annotations), our system obtained the top F-score for\nspeculation and cue detection (see Tables 16 to 18).\nTable 16. Task 1B Wikipedia sentence-level speculation detection (BioScope).\nF-score (%)Recall (%)Precision (%)Name\n60.251.772.0Georgescul [76]\n58.755.362.7Ji et al [77]\n57.449.768.0Chen et al [78]\n61.448.583.7BERT\n54.941.083.2NeuroNER Extended\nTable 17. Task 1B Wikipedia cue-level detection (BioScope).\nF-score (%)Recall (%)Precision (%)Name\n36.525.763.0Tang et al [79]\n33.721.676.1Li et al [80]\n19.514.728.9Özgür et al [81]\n43.633.263.7BERT\n36.525.763.0NeuroNER Extended\nJMIR Med Inform 2020 | vol. 8 | iss. 12 | e18953 | p. 14https://medinform.jmir.org/2020/12/e18953\n(page number not for citation purposes)\nRivera Zavala & MartinezJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nTable 18. Task 1W biological sentence-level speculation detection (BioScope).\nF-score (%)Recall (%)Precision (%)Name\n86.487.785.0Tang et al [79]\n85.885.186.5Zhou et al [82]\n85.481.090.4Li et al [80]\n86.487.385.5BERT\n86.687.086.2NeuroNER Extended\nTable 19. Task 1W biological cue-level detection (BioScope).\nF-score (%)Recall (%)Precision (%)Name\n81.381.081.7Tang et al [79]\n80.978.883.1Zhou et al [82]\n79.873.487.4Li et al [80]\n80.179.580.7BERT\n80.379.281.4NeuroNER Extended\nTable 20. Task 2 cue-level detection and scope determination (BioScope).\nF-score (%)Recall (%)Precision (%)Name\n57.355.259.6Morante et al [83]\n55.654.656.7Rei et al [6]\n55.354.056.7Velldal et al [84]\n50.455.646.1BERT\n44.840.350.4NeuroNER Extended\nTable 21 shows the results for the IULA corpus. Furthermore,\nwe compared our results with the work presented previously\n[85]. We used the evaluation criteria presented in this work;\nhowever, the subsets were different. As can be seen, our system\noutperformed the results obtained previously [85], with a\ndifference of nearly 4 points for the F-measure.\nTable 21. Results of cue level and scope detection for the IULA Clinical Record data set.\nF-score (%)Recall (%)Precision (%)Name\n81.283.579.1Santiso et al [85]\n80.884.377.8BERT\n85.085.984.2NeuroNER Extended\nThe NEGES 2018 Task 2 negation cue detection uses the\nevaluation script proposed in the SEM 2012 Shared\nTask–Resolving the Scope and Focus of Negation [50]. Table\n22 shows the results for the different domains included in the\ndata set. It can be observed that the F-score was always over\n80%. We compared our results with the participating systems\npresented in this task. A detailed description of the evaluation\nhas been provided previously [71]. As can be seen in Table 23,\nour system outperformed the rest of the participating systems.\nFurthermore, we compared NeuroNER Extended and BERT\nimplementations in terms of resources and time consumption\non the IULA Clinical Record training and validation subsets.\nAs shown in Table 24, the training time was slightly higher in\nNeuroNER Extended. However, training implies the generation\nof character and token level embeddings, unlike the BERT\nimplementation that obtains word vector representations directly\nfrom the pretrained model. In terms of hardware resource\nconsumption, we found that BERT implementation had a high\nuse of resources, especially RAM and GPU.\nJMIR Med Inform 2020 | vol. 8 | iss. 12 | e18953 | p. 15https://medinform.jmir.org/2020/12/e18953\n(page number not for citation purposes)\nRivera Zavala & MartinezJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nTable 22. NeuroNER Extended results of negation detection for the SFU ReviewSP-NEG data set.\nF-score (%)Recall (%)Precision (%)Domain\n80.4674.4787.5Cars\n85.4677.0595.92Hotels\n83.9575.5694.44Washing machines\n91.387.595.45Books\n93.8490.8397.06Phones\n92.3192.3192.31Music\n87.580.7795.45Computers\n89.8684.5595.88Movies\nTable 23. Results of negation cues and scope detection for the SFU ReviewSP-NEG data set.\nF-score (%)Recall (%)Precision (%)Name\n68.059.679.5Fabregat et al [86]\n81.283.579.1Loharja et al [87]\n91.790.892.6BERT\n88.182.994.3NeuroNER Extended\nTable 24. Training parameters for the deep learning models.\nBERTNeuroNER ExtendedSpecificationsTraining parameter\n30%50%Intel Core i7 7700 at 3.60 GHzCPU\n80%40%16 GB DDR4RAM\n80%40%GeForce RTX 2060 SUPER 16 RAMGPU\n13 min15 minMinutesTraining time\nDiscussion\nPrincipal Findings\nWe used different pretrained models and investigated their\neffects on performance. For NeuroNER Extended, we used\ngeneral and domain-specific pretrained word embedding models,\nand likewise, we used pretrained multilanguage and\nlanguage-specific models. We found that the use of specific\ndomain (biomedical) and specific language pretrained models\nhighly improved the negation and speculation detection.\nMoreover, to the best of our knowledge, there is no pretrained\nbiomedical Spanish model for context-dependent word\nrepresentations (pretrained BERT). The low performance of the\nBERT model is mainly attributed to the use of a general domain\nand multilingual pretrained model. However, the BERT model\noutperformed the NeuroNER Extended model and other\nstate-of-the-art approaches in general domain data sets, such as\nSFU ReviewSP-NEG, and the specific domain BioScope (Task\n1B data set corpus obtained from Wikipedia text).\nMoreover, we presented the analysis of the most frequent false\nnegatives and false positives for negation and speculation cues\nand scope detection. Negation and speculation cues, such as\n“would,” “apenas” (“barely”), “ni” (“neither” or “nor”),\n“except,” “could,” “idea,” “notion,” and “may,” are half of the\ntime labeled as negation and speculation cues. This ambiguity\nled our system to classify some tokens as false positive or\ninversely as false negative, causing a drop in performance.\nFurthermore, some multitoken negation and speculation cues,\nsuch as “ni siquiera” (“not even”), “ni tan siquiera” (“not even”),\n“ni si quiera” (“not even”), and “en ningún momento” (“not at\nany moment”), are sometimes labeled as a single token word\n(ie, “ni_siquiera,” “ni_tan_siquiera,” “ni_si_quiera,” and\n“en_ningún_momento”), and some others are labeled as\nmultitoken cues. Long multitoken negation and speculation\ncues, such as “remains to be determined” and “raising the\nintriguing possibility,” are not detected or partially matched.\nThis proves that shorter sentences, with shorter scopes and\nshorter negation and speculation cues, are easier to process. A\nlonger sentence has a more complex syntactic structure and is\ntougher to be processed by the system. It should be noted that\nclinical text is undoubtedly distinct from biomedical text. It is\ncharacterized by short sentences (usually phrases) and\nmisspellings, with abuse of negation particles and abbreviations,\namong other important features.\nFurthermore, in the context of real medical applications,\nnegation and speculation detection is a fundamental task in any\ninformation extraction system. For instance, in cohort selections\nfor a clinical trial, patients with a specific condition are required,\nand it is essential to know if a term representing a disease or\nany other feature is negated or not in a clinical note in order to\nget the right answer to the query (Is the variable V valid for\nJMIR Med Inform 2020 | vol. 8 | iss. 12 | e18953 | p. 16https://medinform.jmir.org/2020/12/e18953\n(page number not for citation purposes)\nRivera Zavala & MartinezJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\npatient P?). An additional example would be the detection of\nadverse drug reactions, that is, the extraction of causal relations\nbetween drugs and diseases. It is a crucial step to discard the\nabsence of adverse drug reactions early and thus prevent medical\napplications from analyzing them or providing wrong\ninformation.\nConclusions\nIn this work, we proposed a system for the detection of negated\nentities, negation cues, negation scope, and speculation in\nmultidomain text in English and Spanish. We addressed the\nspeculation and negation detection task as a sequence-labeling\ntask. Although previous studies have already applied deep\nlearning to this task, our approach is the first to exploit sense\nembedding as the input of the deep network. In a sense\nembedding model, each meaning word is represented with a\ndifferent vector. Therefore, sense embedding models can help\nto solve ambiguity, which is one of the most critical challenges\nin NLP.\nOur experiments show that the use of dense representation of\nwords (word-level embedding, character-level embedding, and\nsense embedding) provides good results in detecting negated\nentities, negation cues, and negation scope determination.\nCompared with previous work, our system achieved an F-score\nperformance of over 85%, outperforming most current\nstate-of-the-art methods for negation and speculation detection.\nMoreover, our work is one of the few that addressed the task\nfor Spanish text and different domains using\ncontext-independent and context-dependent pretrained models.\nIn future work, we plan to test whether other supervised\nclassifiers, such as Markov random fields and optimum path\nforest, would obtain more benefits from dense vector\nrepresentation. That is to say, we would use the same continuous\nrepresentations with the Markov random fields and optimum\npath forest classifiers. Moreover, we plan to train word\ncontext-dependent and independent embeddings obtained from\nmultiple Spanish biomedical corpora to enhance word\nrepresentations using different models, such as FastText and\npretrained BERT. Furthermore, we plan to explore different\nmodels for embeddings that combine in a single representation\nnot only words but also semantic information contained in\ndomain-specific resources, such as UMLS [88] and\nSNOMED-CT [89].\nAcknowledgments\nThis work was supported by the Research Program of the Ministry of Economy and Competitiveness, Government of Spain\n(DeepEMR Project TIN2017-87548-C2-1-R).\nConflicts of Interest\nNone declared.\nReferences\n1. Dalianis H. Clinical Text Mining. Cham, Switzerland: Springer; 2018.\n2. Thompson P, Daikou S, Ueno K, Batista-Navarro R, Tsujii J, Ananiadou S. Annotation and detection of drug effects in\ntext for pharmacovigilance. J Cheminform 2018 Aug 13;10(1):37 [FREE Full text] [doi: 10.1186/s13321-018-0290-y]\n[Medline: 30105604]\n3. Light M, Qiu XY, Srinivasan P. The Language of Bioscience: Facts, Speculations, and Statements In Between. ACL\nAnthology. 2004. URL: https://www.aclweb.org/anthology/W04-3103/ [accessed 2020-11-22]\n4. Vincze V, Szarvas G, Farkas R, Móra G, Csirik J. The BioScope corpus: biomedical texts annotated for uncertainty, negation\nand their scopes. BMC Bioinformatics 2008 Nov 19;9 Suppl 11:S9 [FREE Full text] [doi: 10.1186/1471-2105-9-S11-S9]\n[Medline: 19025695]\n5. Jiménez-Zafra SM, Morante R, Martin M, Ureña-López LA. A review of Spanish corpora annotated with negation. ACL\nAnthology. 2018. URL: https://www.aclweb.org/anthology/C18-1078/ [accessed 2020-11-22]\n6. Rei M, Briscoe T. Combining Manual Rules and Supervised Learning for Hedge Cue and Scope Detection. ACL Anthology.\nURL: https://www.aclweb.org/anthology/W10-3008 [accessed 2020-11-22]\n7. Farkas R, Vincze V, Móra G, Csirik J, Szarvas G. The CoNLL-2010 Shared Task: Learning to Detect Hedges and their\nScope in Natural Language Text. ACL Anthology. 2010. URL: https://www.aclweb.org/anthology/W10-3001/ [accessed\n2020-11-22]\n8. Kato Y. A natural history of negation. By LAURENCE R. HORN. Chicago: The University of Chicago Press, 1989. Pp.\nxxii, 637. EL 1991 Jul 01;8:190-208. [doi: 10.9793/elsj1984.8.190]\n9. Chapman WW, Bridewell W, Hanbury P, Cooper GF, Buchanan BG. A simple algorithm for identifying negated findings\nand diseases in discharge summaries. J Biomed Inform 2001 Oct;34(5):301-310 [FREE Full text] [doi:\n10.1006/jbin.2001.1029] [Medline: 12123149]\n10. Chapman WW, Hillert D, Velupillai S, Kvist M, Skeppstedt M, Chapman BE, et al. Extending the NegEx lexicon for\nmultiple languages. Stud Health Technol Inform 2013;192:677-681 [FREE Full text] [Medline: 23920642]\n11. Skeppstedt M. Negation detection in Swedish clinical text: An adaption of NegEx to Swedish. J Biomed Semantics 2011;2\nSuppl 3:S3 [FREE Full text] [doi: 10.1186/2041-1480-2-S3-S3] [Medline: 21992616]\nJMIR Med Inform 2020 | vol. 8 | iss. 12 | e18953 | p. 17https://medinform.jmir.org/2020/12/e18953\n(page number not for citation purposes)\nRivera Zavala & MartinezJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\n12. Cotik V, Stricker V, Vivaldi J, Rodriguez H. Syntactic methods for negation detection in radiology reports in Spanish. ACL\nAnthology. 2016. URL: https://www.aclweb.org/anthology/W16-2921/ [accessed 2020-11-22]\n13. Santiso S, Casillas A, Pérez A, Oronoz M. Word embeddings for negation detection in health records written in Spanish.\nSoft Comput 2018 Nov 23;23(21):10969-10975. [doi: 10.1007/s00500-018-3650-7]\n14. Kang T, Zhang S, Xu N, Wen D, Zhang X, Lei J. Detecting negation and scope in Chinese clinical notes using character\nand word embedding. Comput Methods Programs Biomed 2017 Mar;140:53-59. [doi: 10.1016/j.cmpb.2016.11.009]\n[Medline: 28254090]\n15. Qian Z, Li P, Zhu Q, Zhou G, Luo Z, Luo W. Speculation and Negation Scope Detection via Convolutional Neural Networks.\nACL Anthology. 2016. URL: https://www.aclweb.org/anthology/D16-1078/ [accessed 2020-11-22]\n16. Lazib L, Qin B, Zhao Y, Zhang W, Liu T. A syntactic path-based hybrid neural network for negation scope detection. Front.\nComput. Sci 2018 Aug 2;14(1):84-94. [doi: 10.1007/s11704-018-7368-6]\n17. Bhatia P, Busra Celikkaya E, Khalilia M. End-to-End Joint Entity Extraction and Negation Detection for Clinical Text. In:\nShaban-Nejad A, Michalowski M, editors. Precision Health and Medicine. W3PHAI 2019. Studies in Computational\nIntelligence, vol 843. Cham: Springer; 2019:139-148.\n18. Dernoncourt F, Lee JY, Szolovits P. NeuroNER: an easy-to-use program for named-entity recognition based on neural\nnetworks. ACL Anthology. 2017. URL: https://www.aclweb.org/anthology/D17-2017/ [accessed 2020-11-22]\n19. Cardellino C. Spanish Billion Words Corpus and Embeddings. Cristian Cardellino. 2016 Mar. URL: https://crscardellino.\ngithub.io/SBWCE/ [accessed 2020-11-22]\n20. Pyysalo S, Ginter F, Moen H, Salakoski T, Ananiadou S. Distributional Semantics Resources for Biomedical Text Processing.\nIn: Proceedings of LBM 2013. 2013 Presented at: 5th International Symposium on Languages in Biology and Medicine;\nDecember 12-13, 2013; Tokyo, Japan p. 39-44.\n21. Trask A, Michalak P, Liu J. sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word\nEmbeddings. arXiv. 2015 Nov 19. URL: https://arxiv.org/abs/1511.06388 [accessed 2020-11-22]\n22. Helgeson J, Rammage M, Urman A, Roebuck MC, Coverdill S, Pomerleau K, et al. Clinical performance pilot using\ncognitive computing for clinical trial matching at Mayo Clinic. JCO 2018 May 20;36(15_suppl):e18598-e18598. [doi:\n10.1200/jco.2018.36.15_suppl.e18598]\n23. Imler TD, Morea J, Imperiale TF. Clinical decision support with natural language processing facilitates determination of\ncolonoscopy surveillance intervals. Clin Gastroenterol Hepatol 2014 Jul;12(7):1130-1136. [doi: 10.1016/j.cgh.2013.11.025]\n[Medline: 24316106]\n24. Agarwal V, Zhang L, Zhu J, Fang S, Cheng T, Hong C, et al. Impact of Predicting Health Care Utilization Via Web Search\nBehavior: A Data-Driven Analysis. J Med Internet Res 2016 Sep 21;18(9):e251 [FREE Full text] [doi: 10.2196/jmir.6240]\n[Medline: 27655225]\n25. Chapman WW, Bridewell W, Hanbury P, Cooper GF, Buchanan BG. A simple algorithm for identifying negated findings\nand diseases in discharge summaries. J Biomed Inform 2001 Oct;34(5):301-310 [FREE Full text] [doi:\n10.1006/jbin.2001.1029] [Medline: 12123149]\n26. Mutalik PG, Deshpande A, Nadkarni PM. Use of general-purpose negation detection to augment concept indexing of\nmedical documents: a quantitative study using the UMLS. J Am Med Inform Assoc 2001 Nov 01;8(6):598-609 [FREE Full\ntext] [doi: 10.1136/jamia.2001.0080598] [Medline: 11687566]\n27. Gindl S, Kaiser K, Miksch S. Syntactical negation detection in clinical practice guidelines. Stud Health Technol Inform\n2008;136:187-192 [FREE Full text] [Medline: 18487729]\n28. Aronow DB, Fangfang F, Croft WB. Ad hoc classification of radiology reports. J Am Med Inform Assoc 1999 Sep\n01;6(5):393-411 [FREE Full text] [doi: 10.1136/jamia.1999.0060393] [Medline: 10495099]\n29. Lapponi E, Read J, Øvrelid L. Representing and Resolving Negation for Sentiment Analysis. In: 2012 IEEE 12th International\nConference on Data Mining Workshops. 2012 Presented at: 12th International Conference on Data Mining Workshops;\nDecember 10, 2012; Brussels, Belgium p. 687-692. [doi: 10.1109/ICDMW.2012.23]\n30. Deléger L, Grouin C. Detecting negation of medical problems in French clinical notes. In: IHI '12: Proceedings of the 2nd\nACM SIGHIT International Health Informatics Symposium. 2012 Presented at: 2nd ACM SIGHIT International Health\nInformatics Symposium; January 2012; Miami, Florida p. 697-702. [doi: 10.1145/2110363.2110443]\n31. Costumero R, Lopez F, Gonzalo-Martín C, Millan M, Menasalvas E. An Approach to Detect Negation on Medical Documents\nin Spanish. In: Śl zak D, Tan AH, Peters JF, Schwabe L, editors. Brain Informatics and Health. BIH 2014. Lecture Notes\nin Computer Science, vol 8609. Cham: Springer; 2014:366-375.\n32. Friedman C, Alderson PO, Austin JHM, Cimino JJ, Johnson SB. A general natural-language text processor for clinical\nradiology. J Am Med Inform Assoc 1994 Mar 01;1(2):161-174 [FREE Full text] [doi: 10.1136/jamia.1994.95236146]\n[Medline: 7719797]\n33. Chapman W, Dowling J, Chu D. ConText: An Algorithm for Identifying Contextual Features from Clinical Text. ACL\nAnthology. 2007. URL: https://www.aclweb.org/anthology/W07-1011/ [accessed 2020-11-22]\n34. Aramaki E, Miura Y, Tonoike M, Ohkuma T, Mashuichi H, Ohe K. TEXT2TABLE: Medical Text Summarization System\nBased on Named Entity Recognition and Modality Identification. ACL Anthology. 2009. URL: https://www.aclweb.org/\nanthology/W09-1324/ [accessed 2020-11-22]\nJMIR Med Inform 2020 | vol. 8 | iss. 12 | e18953 | p. 18https://medinform.jmir.org/2020/12/e18953\n(page number not for citation purposes)\nRivera Zavala & MartinezJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\n35. Conway M, Doan S, Collier N. Using Hedges to Enhance a Disease Outbreak Report Text Mining System. In: BioNLP\n'09: Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing. 2009 Presented at:\nWorkshop on Current Trends in Biomedical Natural Language Processing; June 2009; Boulder, Colorado p. 142-143. [doi:\n10.3115/1572364.1572384]\n36. Campillos Llanos L, Martinez P, Segura-Bedmar I. A preliminary analysis of negation in a Spanish clinical records dataset.\nIn: Actas del Taller de NEGación en Español. NEGES-2017. 2017 Presented at: Taller de NEGación en Español; 2017;\nSpain p. 33-37.\n37. Medlock B, Briscoe T. Weakly Supervised Learning for Hedge Classification in Scientific Literature. ACL Anthology.\n2007. URL: https://www.aclweb.org/anthology/P07-1125/ [accessed 2020-11-22]\n38. Morante R, Daelemans W. Learning the Scope of Hedge Cues in Biomedical Texts. In: BioNLP '09: Proceedings of the\nWorkshop on Current Trends in Biomedical Natural Language Processing. 2009 Presented at: Workshop on Current Trends\nin Biomedical Natural Language Processing; June 2009; Boulder, Colorado p. 28-36. [doi: 10.3115/1572364.1572369]\n39. Cruz Díaz NP, Maña López MJ, Vázquez J, Álvarez V. A machine‐learning approach to negation and speculation detection\nin clinical texts. J Am Soc Inf Sci Tec 2012 May 31;63(7):1398-1410. [doi: 10.1002/asi.22679]\n40. Agarwal S, Yu H. Biomedical negation scope detection with conditional random fields. J Am Med Inform Assoc 2010 Nov\n01;17(6):696-701 [FREE Full text] [doi: 10.1136/jamia.2010.003228] [Medline: 20962133]\n41. Konstantinova N, de Sousa SCM, Cruz NP, Maña MJ, Taboada M, Mitkov R. A review corpus annotated for negation,\nspeculation and their scope. ACL Anthology. 2012. URL: https://www.aclweb.org/anthology/L12-1298/ [accessed\n2020-11-22]\n42. Zou B, Zhou G, Zhu Q. Tree Kernel-based Negation and Speculation Scope Detection with Structured Syntactic Parse\nFeatures. ACL Anthology. 2013. URL: https://www.aclweb.org/anthology/D13-1099/ [accessed 2020-11-22]\n43. White JP. UWashington: Negation Resolution using Machine Learning Methods. ACL Anthology. 2012. URL: https:/\n/www.aclweb.org/anthology/S12-1044/ [accessed 2020-11-22]\n44. Casillas A, Pérez A, Oronoz M, Gojenola K, Santiso S. Learning to extract adverse drug reaction events from electronic\nhealth records in Spanish. Expert Systems with Applications 2016 Nov;61:235-245. [doi: 10.1016/j.eswa.2016.05.034]\n45. Donatelli L. Cues, Scope, and Focus: Annotating Negation in Spanish Corpora. In: Proceedings of NEGES 2018: Workshop\non Negation in Spanish. 2018 Presented at: Workshop on Negation in Spanish; September 18, 2018; Seville, Spain p. 29-34\nURL: http://ceur-ws.org/Vol-2174/paper3.pdf\n46. Lazib L, Zhao Y, Qin B, Liu T. Negation Scope Detection with Recurrent Neural Networks Models in Review Texts. In:\nSocial Computing. ICYCSEE 2016. Communications in Computer and Information Science, vol 623. Singapore: Springer;\n2016:494-508.\n47. Lazib L, Qin B, Zhao Y, Zhang W, Liu T. A syntactic path-based hybrid neural network for negation scope detection. Front.\nComput. Sci 2018 Aug 2;14(1):84-94. [doi: 10.1007/s11704-018-7368-6]\n48. Jiménez-Zafra SM, Taulé M, Martín-Valdivia MT, Ureña-López LA, Martí MA. SFU ReviewSP-NEG: a Spanish corpus\nannotated with negation for sentiment analysis. A typology of negation patterns. Lang Resources & Evaluation 2017 May\n22;52(2):533-569. [doi: 10.1007/s10579-017-9391-x]\n49. Fancellu F, Lopez A, Webber B, He H. Detecting negation scope is easy, except when it isn’t. ACL Anthology. 2017. URL:\nhttps://www.aclweb.org/anthology/E17-2010/ [accessed 2020-11-22]\n50. Morante R, Blanco E. *SEM 2012 Shared Task: Resolving the Scope and Focus of Negation. ACL Anthology. 2012. URL:\nhttps://www.aclweb.org/anthology/S12-1035/ [accessed 2020-11-22]\n51. Mehrabi S, Krishnan A, Sohn S, Roch AM, Schmidt H, Kesterson J, et al. DEEPEN: A negation detection system for\nclinical text incorporating dependency relation into NegEx. J Biomed Inform 2015 Apr;54:213-219 [FREE Full text] [doi:\n10.1016/j.jbi.2015.02.010] [Medline: 25791500]\n52. spaCy. URL: https://spacy.io/ [accessed 2020-11-22]\n53. Stenetorp P, Pyysalo S, Topić G, Ohta T, Ananiadou S, Tsujii J. brat: a Web-based Tool for NLP-Assisted Text Annotation.\nACL Anthology. 2012. URL: https://www.aclweb.org/anthology/E12-2021/ [accessed 2020-11-22]\n54. Borthwick A, Sterling J, Agichtein E, Grishman R. Exploiting Diverse Knowledge Sources via Maximum Entropy in Named\nEntity Recognition. ACL Anthology. 1998. URL: https://www.aclweb.org/anthology/W98-1118/ [accessed 2020-11-22]\n55. Giorgi JM, Bader GD. Transfer learning for biomedical named entity recognition with neural networks. Bioinformatics\n2018 Dec 01;34(23):4087-4094 [FREE Full text] [doi: 10.1093/bioinformatics/bty449] [Medline: 29868832]\n56. Wang D, Zheng TF. Transfer learning for speech and language processing. In: 2015 Asia-Pacific Signal and Information\nProcessing Association Annual Summit and Conference (APSIPA). 2015 Presented at: 2015 Asia-Pacific Signal and\nInformation Processing Association Annual Summit and Conference (APSIPA); December 16-19, 2015; Hong Kong, China.\n[doi: 10.1109/APSIPA.2015.7415532]\n57. Mou L, Meng Z, Yan R, Li G, Xu Y, Zhang L, et al. How Transferable are Neural Networks in NLP Applications? ACL\nAnthology. 2016 Nov. URL: https://www.aclweb.org/anthology/D16-1046/ [accessed 2020-11-22]\n58. Lee JY, Dernoncourt F, Szolovits P. Transfer Learning for Named-Entity Recognition with Neural Networks. ACL Anthology.\n2018. URL: https://www.aclweb.org/anthology/L18-1708/ [accessed 2020-11-22]\nJMIR Med Inform 2020 | vol. 8 | iss. 12 | e18953 | p. 19https://medinform.jmir.org/2020/12/e18953\n(page number not for citation purposes)\nRivera Zavala & MartinezJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\n59. Ling W, Dyer C, Black AW, Trancoso I. Two/Too Simple Adaptations of Word2Vec for Syntax Problems. ACL Anthology.\n2015. URL: https://www.aclweb.org/anthology/N15-1142/ [accessed 2020-11-22]\n60. Taulé M, Martí MA, Recasens M. AnCora: Multilevel Annotated Corpora for Catalan and Spanish. ACL Anthology. 2008.\nURL: https://www.aclweb.org/anthology/L08-1222/ [accessed 2020-11-22]\n61. word2vec. URL: http://word2vec.googlecode.com/svn/trunk/ [accessed 2020-08-25]\n62. Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J. Distributed representations of words and phrases and their\ncompositionality. In: NIPS'13: Proceedings of the 26th International Conference on Neural Information Processing Systems.\n2013 Presented at: 26th International Conference on Neural Information Processing Systems; December 2013; Red Hook,\nNew York p. 3111-3119. [doi: 10.5555/2999792.2999959]\n63. Lafferty JD, McCallum AK, Pereira FCN. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling\nSequence Data. In: ICML '01: Proceedings of the Eighteenth International Conference on Machine Learning. 2001 Presented\nat: Eighteenth International Conference on Machine Learning; June 2001; San Francisco, California p. 282-289. [doi:\n10.5555/645530.655813]\n64. Pennington J, Socher R, Manning C. GloVe: Global Vectors for Word Representation. ACL Anthology. 2014. URL: https:/\n/www.aclweb.org/anthology/D14-1162/ [accessed 2020-11-22]\n65. Bojanowski P, Grave E, Joulin A, Mikolov T. Enriching Word Vectors with Subword Information. TACL 2017\nDec;5:135-146. [doi: 10.1162/tacl_a_00051]\n66. Peters M, Neumann M, Iyyer M, Gardner M, Clark C, Lee K, et al. Deep Contextualized Word Representations. ACL\nAnthology. 2018. URL: https://www.aclweb.org/anthology/N18-1202/ [accessed 2020-11-22]\n67. McCann B, Bradbury J, Xiong C, Socher R. Learned in Translation: Contextualized Word Vectors. arXiv. 2017. URL:\nhttps://arxiv.org/abs/1708.00107 [accessed 2020-11-22]\n68. Devlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding. ACL Anthology. 2019. URL: https://www.aclweb.org/anthology/N19-1423/ [accessed 2020-11-22]\n69. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need. arXiv. 2017. URL:\nhttps://arxiv.org/abs/1706.03762 [accessed 2020-11-22]\n70. Devlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding. arXiv. 2018. URL: http://arxiv.org/abs/1810.04805 [accessed 2020-11-22]\n71. Jiménez-Zafra SM, Cruz Díaz NP, Morante R, Martín-Valdivia MT. NEGES 2018 Task 2: Negation Cues Detection. In:\nProceedings of NEGES 2018: Workshop on Negation in Spanish. 2018 Presented at: Workshop on Negation in Spanish,\nNEGES 2018; September 18, 2018; Seville, Spain p. 35-41.\n72. Montserrat M, Vivaldi J, Bel N. Annotation of negation in the IULA Spanish Clinical Record Corpus. ACL Anthology.\n2017. URL: https://www.aclweb.org/anthology/W17-1807/ [accessed 2020-11-22]\n73. Collier N, Park HS, Ogata N, Tateishi Y, Nobata C, Ohta T, et al. The GENIA project: corpus-based knowledge acquisition\nand information extraction from genome research papers. In: EACL '99: Proceedings of the ninth conference on European\nchapter of the Association for Computational Linguistics. 1999 Presented at: Ninth conference on European chapter of the\nAssociation for Computational Linguistics; June 1999; Bergen, Norway p. 271-272. [doi: 10.3115/977035.977081]\n74. Ciao. URL: https://www.ciao.es/ [accessed 2020-11-22]\n75. CoNLL-2010 Shared Task. MTA-SZTE Research Group on Artificial Intelligence. URL: https://rgai.inf.u-szeged.hu/node/\n118 [accessed 2020-08-25]\n76. Georgescul M. A Hedgehop over a Max-Margin Framework Using Hedge Cues. ACL Anthology. 2010. URL: https://www.\naclweb.org/anthology/W10-3004/ [accessed 2020-11-22]\n77. Ji F, Qiu X, Huang X. Detecting Hedge Cues and their Scopes with Average Perceptron. ACL Anthology. 2010. URL:\nhttps://www.aclweb.org/anthology/W10-3005/ [accessed 2020-11-22]\n78. Chen L, Di Eugenio B. A Lucene and Maximum Entropy Model Based Hedge Detection System. ACL Anthology. 2010.\nURL: https://www.aclweb.org/anthology/W10-3016 [accessed 2020-11-22]\n79. Tang B, Wang X, Wang X, Yuan B, Fan S. A Cascade Method for Detecting Hedges and their Scope in Natural Language\nText. ACL Anthology. 2010. URL: https://www.aclweb.org/anthology/W10-3002 [accessed 2020-11-22]\n80. Li X, Shen J, Gao X, Wang X. Exploiting Rich Features for Detecting Hedges and their Scope. ACL Anthology. 2010.\nURL: https://www.aclweb.org/anthology/W10-3011 [accessed 2020-11-22]\n81. Özgür A, Radev DR. Detecting Speculations and their Scopes in Scientific Text. In: EMNLP '09: Proceedings of the 2009\nConference on Empirical Methods in Natural Language Processing. 2009 Presented at: 2009 Conference on Empirical\nMethods in Natural Language Processing; August 2009; Singapore p. 1398-1407. [doi: 10.3115/1699648.1699686]\n82. Zhou H, Li X, Huang D, Li Z, Yang Y. Exploiting Multi-Features to Detect Hedges and their Scope in Biomedical Texts.\nACL Anthology. 2010. URL: https://www.aclweb.org/anthology/W10-3015 [accessed 2020-11-22]\n83. Morante R, Van Asch V, Daelemans W. Memory-Based Resolution of In-Sentence Scopes of Hedge Cues. ACL Anthology.\n2010. URL: https://www.aclweb.org/anthology/W10-3006 [accessed 2020-11-22]\n84. Velldal E, Øvrelid L, Oepen S. Resolving Speculation: MaxEnt Cue Classification and Dependency-Based Scope Rules.\nACL Anthology. 2010. URL: https://www.aclweb.org/anthology/W10-3007 [accessed 2020-11-22]\nJMIR Med Inform 2020 | vol. 8 | iss. 12 | e18953 | p. 20https://medinform.jmir.org/2020/12/e18953\n(page number not for citation purposes)\nRivera Zavala & MartinezJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\n85. Santiso S, Casillas A, Pérez A, Oronoz M. Word embeddings for negation detection in health records written in Spanish.\nSoft Comput 2018 Nov 23;23(21):10969-10975. [doi: 10.1007/s00500-018-3650-7]\n86. Fabregat H, Martinez-Romo J, Araujo L. Deep Learning Approach for Negation Cues Detection in Spanish. In: Proceedings\nof NEGES 2018: Workshop on Negation in Spanish. 2018 Presented at: Workshop on Negation in Spanish; September 18,\n2019; Seville, Spain p. 43-48 URL: http://ceur-ws.org/Vol-2174/paper5.pdf\n87. Loharja H, Padró L, Turmo J. Negation Cues Detection Using CRF on Spanish Product Review Texts. In: Proceedings of\nNEGES 2018: Workshop on Negation in Spanish. 2018 Presented at: Workshop on Negation in Spanish; September 18,\n2018; Seville, Spain p. 49-54 URL: http://ceur-ws.org/Vol-2174/paper6.pdf\n88. Bodenreider O. The Unified Medical Language System (UMLS): integrating biomedical terminology. Nucleic Acids Res\n2004 Jan 01;32(Database issue):D267-D270 [FREE Full text] [doi: 10.1093/nar/gkh061] [Medline: 14681409]\n89. De Silva TS, MacDonald D, Paterson G, Sikdar KC, Cochrane B. Systematized nomenclature of medicine clinical terms\n(SNOMED CT) to represent computed tomography procedures. Comput Methods Programs Biomed 2011\nMar;101(3):324-329. [doi: 10.1016/j.cmpb.2011.01.002] [Medline: 21316117]\nAbbreviations\nBERT: bidirectional encoder representations from transformers\nBi-LSTM: bidirectional long short-term memory\nCNN: convolutional neural network\nCRF: conditional random field\nNER: named entity recognition\nNLP: natural language processing\nPoS: part of speech\nRNN: recurrent neural network\nEdited by G Eysenbach; submitted 29.03.20; peer-reviewed by L Zhang, J Kim, G Lim; comments to author 29.06.20; revised version\nreceived 25.08.20; accepted 28.10.20; published 03.12.20\nPlease cite as:\nRivera Zavala R, Martinez P\nThe Impact of Pretrained Language Models on Negation and Speculation Detection in Cross-Lingual Medical Text: Comparative\nStudy\nJMIR Med Inform 2020;8(12):e18953\nURL: https://medinform.jmir.org/2020/12/e18953\ndoi: 10.2196/18953\nPMID: 33270027\n©Renzo Rivera Zavala, Paloma Martinez. Originally published in JMIR Medical Informatics (http://medinform.jmir.org),\n03.12.2020. This is an open-access article distributed under the terms of the Creative Commons Attribution License\n(https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium,\nprovided the original work, first published in JMIR Medical Informatics, is properly cited. The complete bibliographic information,\na link to the original publication on http://medinform.jmir.org/, as well as this copyright and license information must be included.\nJMIR Med Inform 2020 | vol. 8 | iss. 12 | e18953 | p. 21https://medinform.jmir.org/2020/12/e18953\n(page number not for citation purposes)\nRivera Zavala & MartinezJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7938350439071655
    },
    {
      "name": "Negation",
      "score": 0.7879412174224854
    },
    {
      "name": "Natural language processing",
      "score": 0.7346991300582886
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6759659647941589
    },
    {
      "name": "Speculation",
      "score": 0.6158967614173889
    },
    {
      "name": "Conditional random field",
      "score": 0.4216860234737396
    },
    {
      "name": "Programming language",
      "score": 0.13469499349594116
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Macroeconomics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I50357001",
      "name": "Universidad Carlos III de Madrid",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I4210116357",
      "name": "Catholic University of Santa María",
      "country": "PE"
    }
  ],
  "cited_by": 29
}