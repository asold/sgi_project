{
  "title": "Medical malpractice liability in large language model artificial intelligence: legal review and policy recommendations",
  "url": "https://openalex.org/W4391362878",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2143126041",
      "name": "David O. Shumway",
      "affiliations": [
        "Keesler Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A5093823403",
      "name": "Hayes J. Hartman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2143126041",
      "name": "David O. Shumway",
      "affiliations": [
        "Keesler Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A5093823403",
      "name": "Hayes J. Hartman",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2001771035",
    "https://openalex.org/W6601893370",
    "https://openalex.org/W4383302950",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W2979279941",
    "https://openalex.org/W4383301640",
    "https://openalex.org/W4246962821",
    "https://openalex.org/W2034629895",
    "https://openalex.org/W2012890880",
    "https://openalex.org/W2045069056",
    "https://openalex.org/W4383302171",
    "https://openalex.org/W4383186800",
    "https://openalex.org/W2990768863"
  ],
  "abstract": "Abstract The emergence of generative large language model (LLM) artificial intelligence (AI) represents one of the most profound developments in healthcare in decades, with the potential to create revolutionary and seismic changes in the practice of medicine as we know it. However, significant concerns have arisen over questions of liability for bad outcomes associated with LLM AI-influenced medical decision making. Although the authors were not able to identify a case in the United States that has been adjudicated on medical malpractice in the context of LLM AI at this time, sufficient precedent exists to interpret how analogous situations might be applied to these cases when they inevitably come to trial in the future. This commentary will discuss areas of potential legal vulnerability for clinicians utilizing LLM AI through review of past case law pertaining to third-party medical guidance and review the patchwork of current regulations relating to medical malpractice liability in AI. Finally, we will propose proactive policy recommendations including creating an enforcement duty at the US Food and Drug Administration (FDA) to require algorithmic transparency, recommend reliance on peer-reviewed data and rigorous validation testing when LLMs are utilized in clinical settings, and encourage tort reform to share liability between physicians and LLM developers.",
  "full_text": "Innovations Commentary\nDavid O. Shumway*, DO and Hayes J. Hartman, JD\nMedical malpractice liability in large language\nmodel artiﬁcial intelligence: legal review and\npolicy recommendations\nhttps://doi.org/10.1515/jom-2023-0229\nReceived October 11, 2023; accepted January 3, 2024;\npublished online January 31, 2024\nAbstract: The emergence of generative large language\nmodel (LLM) artiﬁcial intelligence (AI) represents one of the\nmost profound developments in healthcare in decades, with\nthe potential to create revolutionary and seismic changes in\nthe practice of medicine as we know it. However, signiﬁcant\nconcerns have arisen over questions of liability for bad\noutcomes associated with LLM AI-inﬂuenced medical deci-\nsion making. Although the authors were not able to identify a\ncase in the United States that has been adjudicated on\nmedical malpractice in the context of LLM AI at this time,\nsuﬃcient precedent exists to interpret how analogous situ-\nations might be applied to these cases when they inevitably\ncome to trial in the future. This commentary will discuss\nareas of potential legal vulnerability for clinicians utilizing\nLLM AI through review of past case law pertaining to\nthird-party medical guidance and review the patchwork\nof current regulations relating to medical malpractice\nliability in AI. Finally, we will propose proactive policy rec-\nommendations including creating an enforcement duty at\nthe US Food and Drug Administration (FDA) to require\nalgorithmic transparency, recommend reliance on peer-\nreviewed data and rigorous validation testing when LLMs\nare utilized in clinical settings, and encourage tort reform to\nshare liability between physicians and LLM developers.\nKeywords: artiﬁcial intelligence; ChatGPT; large language\nmodels; liability; medical malpractice\nEver since Alan Turing proposed his“Imitation Game” in\n1950 [1], the technological world has raced to produce arti-\nﬁcial intelligence (AI) that equals or exceeds humans. The\nnewest wave of generative large language model (LLM) AI,\nsuch as OpenAI’s ChatGPT, represents a signiﬁcant leap\ntoward the realization of this dream. The integration of\ngenerative LLM AI into clinical settings marks a trans-\nformative moment in the history of healthcare, with the\npotential to revolutionize diagnostics and treatment plan-\nning [2, 3]. However, as LLM AI systems begin to play a more\nprominent role in patient care, uncertainty has arisen\nregarding liability when AI is involved in medical decision\nmaking [4].“Hallucinations,” a phenomenon in which LLMs\ncreate false information to answer a user ’s prompt,\nunknown reliability of source information utilized to train\nLLMs, and a potential inability for physicians to indepen-\ndently evaluate the accuracy of an LLM AI’s output, are all\nfactors that increase the risk of liability for physicians uti-\nlizing these algorithms to make diagnostic and treatment\ndecisions [5].\nAlthough the authors were not able to identify a case in\nthe United States that has been adjudicated on medical\nmalpractice in the context of LLM AI [4, 5], there exists\npossible legal precedent that can guide our understanding.\nIn this commentary, we will discuss how analogous situa-\ntions in which physicians have either heeded or disregarded\nthird-party guidance in making medical decisions provide a\nlens through which we can anticipate future legal\ninterpretations when LLM AI is involved, review current\nregulations regarding LLM AI, and give recommendations\nfor new health policy to address this issue proactively.\nSources of liability in medical\ndecision making inﬂuenced by AI\nbased on historical review\nThe roots of the legal deﬁnition of malpractice are encap-\nsulated in English jurist William Blackstone’s Commentaries\non the Laws of England [6], where he characterized it as a\n“misdemeanor and oﬀense at common law, whether it be for\ncuriosity and experiment, or by neglect; because it breaks\nthe trust which party had placed in his physician, and tends\n*Corresponding author: David O. Shumway, DO, Keesler Medical Center,\nKeesler Air Force Base, 301 Fisher Street, Biloxi, MS 39534, USA,\nE-mail: doshumway@live.com\nHayes J. Hartman,JD, Attorney, Mountain Home, ID, USA\nJ Osteopath Med 2024; 124(7): 287–290\nOpen Access. © 2024 the author(s), published by De Gruyter. This work is licensed under the Creative Commons Attribution 4.0 International License.\nto the patient’s destruction.” Patients retain, and frequently\nexercise, a cause of action in civil courts for the redress of\ninjuries resulting from medical malpractice. As a general\nrule, a medical provider engages in malpractice if their\nconduct deviates from the ordinary standard of care in their\njurisdiction, and this can diﬀer widely between jurisdictions\n[7]. Therefore, a physician’s use of LLM AI in treating a\npatient will likely be analyzed through the lens of the\nprevailing standard of care [5]. Although the novelty of LLM\nAI in clinical practice lends uncertainty as to what sort of\nevidence the courts will treat as being probative of its proper\nuse, it seems most likely that LLM AI-generated advice will\nbe treated as third-party medical guidance. Fortunately, a\npractitioner’s reliance on (or disregard for) third-party\nmedical guidance is not a novel topic for the courts.\nIt is important to note that publicly available LLMs such\nas Chat-GPT are not exclusively utilizing expert-reviewed\ndata to produce medical guidance. In order to make a more\ndirect comparison to the examples that follow, let us suppose\nthat a future LLM marketed directly to physicians will.\nOur ﬁrst analogous situation involves the degree to\nwhich package inserts drafted by pharmaceutical manu-\nfacturers can be utilized to establish the standard of care\nconcerning the administration of a drug. Drug package\ninserts are derived from source information and clinical\nstudies reviewed by physicians; however, they may not be\nauthored by them. In Julian vs. Barker, the Supreme Court of\nIdaho held that a trial court erred in barring the admission of\nan information sheet that provided directions on the proper\nadministration of sodium pentothal, as the manufacturer\nwas presumed to be qualiﬁed to give directions concerning\nthe use of its product [8]. In coming to this conclusion, the\nJulian Court noted that a drug manufacturer ’s written\ndirections represented prima facie (“facially suﬃcient”– a\nlegal term meaning that an argument or position is sound\nenough atﬁrst glance to be presumed valid) evidence of the\ndrug’s proper administration. In Mueller vs. Mueller, the\nSupreme Court of South Dakota upheld a jury instruction\nthat directed the jury to consider evidence that a physician\ndeviated from a drug manufacturer’si n s t r u c t i o n so nt h e\nproper administration of its product as evidence of\nnegligence [9]. The Mueller Court supported its decision\nby noting that drug manufacturers were increasingly be-\ni n gf o u n dl i a b l ef o rd e f e c t i v ep r o d u c t s ,r e n d e r i n gt h e i r\ninstructions more reliable. Also, per the Mueller Court, a\nbusy modern physician had no choice but to rely upon a\nmanufacturer ’s instructions, as they could not be expected\nto independently verify the propriety of a drug’s partic-\nular application.\nTouching upon physicians’ usage of third-party medical\nguidance even more closely analogous to LLM AI, other cases\nhave probed the extent to which adherence to a point-of-care\ndecision resource can be considered standard of care. In\nSpensieri vs. Lasky, the Court of Appeals of the State of New\nYork rejected an argument that drug information compiled\nin the Physician’s Desk Reference was prima facie evidence\nof the standard of care, noting that a patient’s individual\ncircumstances were vital to that analysis [10]. Ultimately, the\nLasky Court held that Physician’s Desk Reference could\nproperly be incorporated into an expert’s testimony but was\nnot standalone proof of the standard of care.\nAs evidenced by the split in authority discussed above,\nsome jurisdictions may deem an AI utilizing expert-reviewed\ndata to give medical guidance as representing the standard\nof care, whereas others may generally reject its applicability.\nA hybrid approach is also possible, in which courts permit\nt h ea d m i s s i o no fag e n e r a t i v eA I’sr e s p o n s et oa ni n q u i r yb ya\nphysician but require supplemental testimony from a qualiﬁed\nmedical expert. Uncertainty and doubt that the courts will\nconsider guidance from a validated LLM AI to be the standard\nof care across the board may produce a future dilemma for the\n“busy modern physician” when deciding to heed or reject this\nguidance.\nThe courts’collective approach to the interplay between\ngenerative AI and the standard of care will likely continue to\nevolve, as courts frequently modify or clarify established\nprecedent based upon the unique facts of a particular case\n[9]. In Lhotka vs. Larson, for instance, the Court noted that a\njury instruction that deviation from a manufacturer ’s\ndirections constituted evidence of negligence would be\nappropriate only if said directive was clear and unambigu-\nous. However, because the directive was ambiguous, such an\ninstruction was not warranted [11]. As medical guidance\ngiven by generative AI approximates the tone of a human\nconsultant, in practice, it is rarely unambiguous.\nBrief review of existing laws and\nregulations applicable to AI\nLegislative and regulatory eﬀorts to address LLM AI have\nbeen limited and piecemeal so far, with the dynamic nature\nof these systems posing unique challenges for traditional\nregulatory approaches [12, 13]. Currently, the closest thing\nthat exists to a comprehensive federal regulation to address\nliability for LLM AI-inﬂuenced medical decisions is a 2022\nrevision to the Aﬀordable Care Act’s Section 1557, which\nstates that physicians and covered entities are“liable for\nmedical decisions made in reliance with clinical algorithms\n[14].” This regulation is problematic, because while the\nintent of this rule is to address and prevent discrimination\n288 Shumway and Hartman: Medical malpractice liability in LLM AI\nagainst historically marginalized communities that can\nresult from AI algorithms [15], reasonable interpretation\ncould apply this statement broadly to any medical decision\nmade utilizing AI, including guidance given by LLMs.\nThe US Food and Drug Administration (FDA) is begin-\nning to evaluate some AI systems as medical devices [16] and\nhas published nonbinding recommendations for classifying\nclinical decision support (CDS) software as medical devices\nor not [17]. These recommendations interpret the scope of\nsection 520(o)(1)I(i) of the Federal Food, Drug, and Cosmetic\nAct (FD&C Act), and when applied to a LLM, appear to\nexempt these systems from being classi ﬁed as ‘devices’\n(i.e., a nondevice CDS). This determination rests on the\nobservation that LLMs, when utilized clinically, are designed\nfor decision support and not (at least currently) involved in\nacquisition or processing of diagnostic images [17]. Impor-\ntantly, this section also parallels the Lasky court in stating\nthat recommendations given by a CDS tool should be inde-\npendently considered by the physician in view of the indi-\nvidual patient and not utilized as the sole determination of\ndiagnostic or treatment decisions [17].\nIn Congress, no signiﬁcant legislation that speciﬁcally\nregulates LLM AI in healthcare has been proposed. The most\npotentially consequential bill is the Algorithmic Account-\nability Act of 2019 [18], which was recently re-introduced\nin 2023. This legislation aims to create protections for peo-\nple aﬀected in a negative way by utilization of AI in decisions\non housing, credit, education, and other high-impact appli-\ncations. If passed, it would create an enforcement duty at the\nFederal Trade Commission to require that automated sys-\ntems be assessed for biases and hold bad actors accountable\n[19]. However, this bill does not mention medical malprac-\ntice and proposes no plans to assess clinically utilized LLM AI\nalgorithms for reliability.\nDiscussion, limitations, and policy\nrecommendations\nIt should be restated that the most signiﬁcant limitation of\nthis commentary is that to our knowledge, there are no\ncurrent or previous cases litigated in the United States that\nspeciﬁcally address malpractice liability for physicians\nutilizing LLM AI. As a result, the preceding legal review\nremains speculative by nature, representing our best\nguess at what evidence future courts may consider in\nthese cases. Due to the paucity of legislation and regula-\ntory e ﬀorts identi ﬁed by our review in the previous\nsection, we suggest that there is a critical need and\nopportunity for proactive a ction to address this issue\nthrough policy rather than waiting for resolution through\nthe legal system.\nTo ensure the reliability of AI systems, protect patients,\nand promote the fair application of medical malpractice\nliability, federal policy should mandate rigorous validation\nand testing of AI tools before their deployment in clinical\nsettings. The US FDA is the preferred agency to regulate\nclinical AI reliability, given its expertise in medical devices\nand software, and should extend this responsibility to LLM\nalgorithms. This process could require AI developers to\nmake their algorithms available for independent validation\nwhen utilized in clinical practice, ensuring that AI systems\nprovide clear explanations for their recommendations with\nveriﬁed, peer-reviewed data. Finally, if utilization of high-\nquality LLM AI increasingly becomes considered the stan-\ndard of care in most jurisdictions, liability reform will be\nneeded to shift some responsibility for AI-generated medical\nguidance to algorithm developers, as shown relating to drug\npackage inserts discussed previously. Achieving this may\nrequire state-level, rather than federal-level, tort reform,\nand could be an issue to be resolved later by the courts\nthemselves if policy cannot be enactedﬁrst.\nConclusions\nUntil there is clarity or action in determining the scope\nof malpractice liability as a result of medical decisions\ninﬂuenced by AI, a signi ﬁcant barrier to the adoption\nand application to the full potential of this technology in\nmedicine will remain. We recommend consideration and\nadoption of the policy recommendations given in this com-\nmentary as a proactive solution to protect patients and\nreduce the risk of malpractice liability for physicians that\nchoose to take advantage of the potential of generative LLM\nAI for clinical applications.\nResearch ethics:Not applicable.\nInformed consent:Not applicable.\nAuthor contributions: Both authors provided substantial\ncontributions to conception and design, acquisition of data,\nor analysis and interpretation of data; both authors drafted\nthe article or revised it critically for important intellectual\ncontent; both authors gaveﬁnal approval of the version of\nthe article to be published; and both authors agree to be\naccountable for all aspects of the work in ensuring that\nquestions related to the accuracy or integrity of any part of\nthe work are appropriately investigated and resolved.\nCompeting interests:None declared.\nShumway and Hartman: Medical malpractice liability in LLM AI 289\nResearch funding:None declared.\nData availability:Not applicable.\nDisclaimer: The views expressed in this material are those of\nthe authors, and do not reﬂect the oﬃcial policy or position\nof the U.S. Government, the Department of Defense, or the\nDepartment of the Air Force.\nReferences\n1. Turing AM. Computing machinery and intelligence. Mind 1950;59:\n433–60.\n2. Cutler DM. What artiﬁcial intelligence means for health care. JAMA\nHealth Forum 2023;4:e232652.\n3. Ayers JW, Poliak A, Dredze M, Leas EC, Zhu Z, Kelley JB, et al. Comparing\nphysician and artiﬁcial intelligence Chatbot responses to patient\nquestions posted to a public social media forum. JAMA Intern Med\n2023;183:589–96.\n4. Price WN, Gerke S, Cohen IG. Potential liability for physicians using\nartiﬁcial intelligence. JAMA 2019;322:1765–6.\n5. Duﬀourc M, Gerke S. Generative AI in health care and liability risks for\nphysicians and safety concerns for patients. JAMA 2023;330:313–14.\n6. Blackstone W. Commentaries on the laws of England. Boston: Beacon\nPress; 1962.\n7. American Medical Association. State medical liability reform.\nhttps://www.ama-assn.org/practice-management/sustainability/\nstate-medical-liability-reform [Accessed 21 Sep 2023].\n8. Julian v. Barker, 75 Idaho 413, 423; 1954.\n9. Mueller v. Mueller, 221 N.W.2d 39, 43 (S.D.); 1974.\n10. Spensieri v. Lasky, 94 N.Y.2d 231, 239; 1999.\n11. Lhotka v. Larson, 307 Minn 121, 126; 1976.\n12. Minssen T, Vayena E, Cohen IG. The challenges for regulating medical\nuse of ChatGPT and other large language models. JAMA 2023;330:\n315–16.\n13. Clark P, Kim J, Aphinyanaphongs Y. Marketing and US Food and Drug\nAdministration clearance of artiﬁcial intelligence and machine learning\nenabled software in and as medical devices: a systematic review. JAMA\nNetw Open 2023;6:e2321792.\n14. Centers for Medicare and Medicaid Services. Aﬀordable Care Act\nSection 1557 nondiscrimination in health programs and activities: use\nof clinical algorithms in decision making (§ 92.210); 2022. https://www.\ngovinfo.gov/content/pkg/FR-2022-08-04/pdf/2022-16217.pdf\n[Accessed 21 Sep 2023].\n15. Parikh RB, Teeple S, Navathe AS. Addressing bias in artiﬁcial\nintelligence in health care. JAMA 2019;322:2377–8.\n16. Clark P, Kim J, Aphinyanaphongs Y. Marketing and US Food and Drug\nAdministration clearance of artiﬁcial intelligence and machine learning\nenabled software in and as medical devices: a systematic review. JAMA\nNetw Open 2023;6:e2321792.\n17. U.S. Food and Drug Administration. “Clinical Decision Support\nSoftware ” guidance Document, FDA-2017-D-6569; 2022.\nhttps://www.fda.gov/regulator y-information/search-fda-\nguidance-documents/clinical -decision-support-software\n[Accessed 21 Sep 2023].\n18. S. 1108 – algorithmic accountability Act of 2019. https://www.\ncongress.gov/bill/116th-congress/senate-bill/1108 [Accessed 22 Sep\n2023].\n19. Wyden, Booker and Clarke introduce bill to regulate use of artiﬁcial\nintelligence to make critical decisions like housing, employment and\neducation. U.S. Senator Ron Wyden of Oregon. www.wyden.senate.\ngov, https://www.wyden.senate.gov/news/press-releases/wyden-\nbooker-and-clarke-introduce-bill-to-regulate-use-of-artiﬁcial-\nintelligence-to-make-critical-decisions-like-housing-employment-and-\neducation [Accessed 26 Sep 2023].\n290 Shumway and Hartman: Medical malpractice liability in LLM AI",
  "topic": "Liability",
  "concepts": [
    {
      "name": "Liability",
      "score": 0.6133041381835938
    },
    {
      "name": "Tort",
      "score": 0.5295040607452393
    },
    {
      "name": "Medical malpractice",
      "score": 0.520209014415741
    },
    {
      "name": "Malpractice",
      "score": 0.5062376856803894
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4686291515827179
    },
    {
      "name": "Medicine",
      "score": 0.4014696478843689
    },
    {
      "name": "Political science",
      "score": 0.3878169059753418
    },
    {
      "name": "Business",
      "score": 0.36250120401382446
    },
    {
      "name": "Law",
      "score": 0.319396436214447
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210089251",
      "name": "Keesler Medical Center",
      "country": "US"
    }
  ],
  "cited_by": 21
}