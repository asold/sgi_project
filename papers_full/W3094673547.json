{
  "title": "Deep Learning-Based Short-Term Load Forecasting for Transformers in Distribution Grid",
  "url": "https://openalex.org/W3094673547",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2138812912",
      "name": "Renshu Wang",
      "affiliations": [
        "Fujian Electric Power Survey & Design Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2098902528",
      "name": "Jing Zhao",
      "affiliations": [
        "Education Department of Fujian Province"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6603514168",
    "https://openalex.org/W6600077625",
    "https://openalex.org/W6600339963",
    "https://openalex.org/W2792961021",
    "https://openalex.org/W2601171548",
    "https://openalex.org/W2558748708",
    "https://openalex.org/W2985107936",
    "https://openalex.org/W3045628664",
    "https://openalex.org/W2170866695",
    "https://openalex.org/W1580230477",
    "https://openalex.org/W2516939159",
    "https://openalex.org/W2902025189",
    "https://openalex.org/W2013438082",
    "https://openalex.org/W2558460151",
    "https://openalex.org/W2805797750",
    "https://openalex.org/W2891898214",
    "https://openalex.org/W2550224173",
    "https://openalex.org/W2809317444",
    "https://openalex.org/W2962949934",
    "https://openalex.org/W2897220206",
    "https://openalex.org/W2954534233",
    "https://openalex.org/W2963165299",
    "https://openalex.org/W4248035209",
    "https://openalex.org/W2597866042",
    "https://openalex.org/W2606004785",
    "https://openalex.org/W2269227734",
    "https://openalex.org/W2513927208",
    "https://openalex.org/W2963690547",
    "https://openalex.org/W2786147899",
    "https://openalex.org/W2531409750"
  ],
  "abstract": null,
  "full_text": "I\nnternational Journal of Computational Intelligence Systems\nVol. 14(1), 2021,pp. 1‚Äì10\nDOI: https://doi.org/10.2991/ijcis.d.201027.001; ISSN: 1875-6891; eISSN: 1875-6883\nhttps://www.atlantis-press.com/journals/ijcis/\nResear\nch Article\nDeep Learning-Based Short-Term Load Forecasting for\nTransformers in Distribution Grid\nRenshu Wang1,*,\n ,\nJing Zhao2,\n1\nE\nlectric Power Research Institute of State Grid Fujian Electric Power Co., Ltd, No.48, Fuyuan Branch Road, Cangshan District, Fuzhou 350007, Fujian, China\n2\nManagement Training Department of State Grid Fujian Management Training Center, No.19, Gongyuan West Road, Cangshan District, Fuzhou 350007, Fujian, China\nA R T I C L E I N F O\nArticle History\nReceived 19 Dec 2019\nAccepted 16 Oct 2020\nKeywords\nLoad forecasting\nconvolutional neural network\n(CNN)\nLong short-term memory (LSTM)\nInception structure\nResidual connection\nA\nB S T R A C T\nLoad of transformer in distribution grid fluctuates according to many factors, resulting in overload frequently which affects the\nsafety of power grid. And short-term load forecasting is considered. To improve forecasting accuracy, the input information and\nthe model structure are both considered. First, the multi-dimensional information containing numerical data and textual data is\ntaken as the inputs of constructed deep learning model, and textual data is encoded by one-hot method. Then, for the purpose\nof mining the features of data better, based on the framework composed of convolutional neural network (CNN) and long short-\nterm memory (LSTM), the modified inception structure is introduced to extract more detailed features and adaptive residual\nconnection is added to settle the problem of gradient diffusion when the layers of model grow more. At last, the comparison is\ncarried out and the improvements are presented after the textual data is added and the structure of model is modified. And the\nforecasting error is reduced, especially when the load is heavy, which is beneficial for the prevention of overload of transformer\nin distribution gird.\n¬© 2021 Th\ne Authors. Published by Atlantis Press B.V .\nThis is an open access article distributed under the CC BY-NC 4.0 license (http://creativecommons.org/licenses/by-nc/4.0/).\n1. INTRODUCTION\nPower load forecasting is important for the effective management\nof modern power grids. The accurate load forecasting can improve\nthe real-time scheduling and operation planning of power system,\nreducing the excessive consumption of energy, increasing the oper-\nation security level and contributing for the promotion of economic\nand social development [1‚Äì3]. There have been many load forecast-\ning methods, which can be classified into long-term, medium-term,\nshort-term, ultra-short-term according to the forecasting periods.\nAmong these methods, the short-term forecasting method predicts\nthe load of the power system in hours or weeks, which is the basis for\ndispatching center to formulate power generation plans and power\nplant quotations. And it is also important for the operation, control\nand planning of power system [4,5].\nThe transformer in distribution grid plays an important role in\npower supply. And the overload of transformer should be avoided\nas possible, which will not only cause the increasement of power\nloss, even compromise the security operation of distribution net-\nwork. However, the load of transformer fluctuates according to\nmany factors, such as weather condition, utilizer, festival, and so\non. For example, in Fujian area, there are more than 200 transform-\ners overloading during the National Day in 2019. For overload will\nlead to unstable power supply or even burning down of transformer,\nthe overload rate has been a key assessment index of the opera-\ntion of distribution grid. So, it‚Äôs necessary to forecast the load for\n*C\norresponding author. Email:932521880@qq.com\ntransformers in distribution grid, especially the short-term load\nforecasting, taking early measures to reduce the probability of over-\nload. However, the accuracy of short-term load is impacted by lots\nof factors, especially the transformer in distribution grid is more\nvulnerable to the factors for its limited capacity [6,7]. So, the short-\nterm load forecasting of transformer in distribution network is dif-\nficult but significant.\nLots of scholars have conducted extensive explorations, and pro-\nposed various prediction methods to improve the accuracy of short-\nterm load forecasting, including time series method [8], wavelet\nanalysis [9], regression analysis [10], Kalman filter [11], neural net-\nwork [12], and so on. In these methods, the forecasting models are\nconstructed reasonably based on complicated theories, but some\nparts of parameters or weights in the proposed model are chosen\naccording to experience, resulting in the incomplete feature extrac-\ntion, or the model is suitable for some specific conditions.\nTo further settle the forecasting error caused by subjective factor in\nthe feature extraction, the deep learning methods are considered for\nits outstanding performance of feature extraction. There have been\nmany types of deep learning models according to the composition\nstructures and calculation methods, such as convolutional neural\nnetwork (CNN), recursive neural network (RNN), long-term and\nshort-term memory neural network (LSTM), and so on. And the\napplication of deep learning used for forecasting has been a trend.\nIn paper [13], a short-term power load prediction method based on\nLSTM is proposed to reduce the dimensionality of the data volume\nrequired by the load prediction model, having higher prediction\n2 R\n. Wang and J. Zhao / International Journal of Computational Intelligence Systems 14(1)1‚Äì10\naccuracy than traditional BP neural network algorithm. In paper\n[14], the model with the main framework of stacked LSTM are\napplied for the load forecasting. In paper [15], aiming at the charac-\nteristics of time series and nonlinearity of load data, a mixed model\nshort-term load forecasting method based on CNN and LSTM is\nproposed with the input of historical load data, weather data, date\ninformation and electricity price data. In paper [16], a deep resid-\nual network is modified and a two-stage ensemble strategy is con-\nstructed to enhance the generalization capability of the proposed\nmodel and improve the forecast results. In the existing research, the\ndeep learning methods have shown better performance than other\nmethods.\nIn this paper, based on the existing research results, CNN and LSTM\ncan be adopted as the foundation for forecasting model. Besides\nthat, the multi-dimensional information of numerical data in hour\n(containing historical load, meteorological data, electricity price)\nand textual data (containing date type, states of distribution lines)\nare both taken as input to increase the prediction precision. Mean-\nwhile, the framework optimization for forecasting model is also\nconsidered. At last, the comparison of forecasting results are pre-\nsented.\n2. RELATED WORK AND BASIC PRINCIPLE\nIn traditional methods, the load forecasting is mainly decided on\nthe numerical data, such as historical load, electricity price, pop-\nulation, meteorological data, and so on [8‚Äì12]. In these methods,\nthe correlation analysis methods mainly based on the probabil-\nity theory, such as canonical correlation analysis. The consensus\nis that the accuracy of the short-term load forecasting depends\non the performance of mining associations among related data to\na large extent. In these methods, for there are some parameters\ndecided subjectively or empirically and the association of different\nvariables is uncertain, the accuracy of forecasting model may also\nbe unstable.\nTake the relationship between load and temperature, for example,\nwhich is also the most often considered relation in load forecast-\ning. In Figure1, the load and temperature of every day from Jan-\nuary to December are presented. The maximum load occurs when\nthe temperature also stays high in the summer season. However, in\nDecember, the temperature decreases but the load grows higher.\nIn Figure 2, trend 1 is collected on the day in winter, trend 2 in\nspring, trend 3 in summer and trend 4 in autumn. It can be found\nthat transformer loads are inconsistent with temperatures and the\nrelationship between load and temperature varies with time. It\nmeans that temperature is some part related to load trend and other\ninfluence factors need to be taken into consideration in short-term\nload forecasting. Such as the influence of festival, the load trend 1\ncollected in Spring Festival is higher than trend 2 and trend 4.\nThrough the review of the exiting methods, it can be found that the\nmore effective information is considered in forecasting model, the\nbetter forecasting accuracy can be derived. To realize a better per-\nformance of load forecasting, the factors including historical load\ndata, weather data, states of distribution lines and the day type (fes-\ntival, workday or weekend) are considered. Meanwhile, with the\nconvolution computation and word embedding of deep learning,\nthe relationship between different kinds of factors can be modeled\nqualitatively. And based on the traditional methods, it is difficult to\nF\nigure 1 Th\ne trends of load and temperature in a year: (a) load trend in a year and (b) temperature trend in a year.\nF\nigure 2 L\noad and temperature trends in different days from four seasons: (a) load trends in different days and (b)\ntemperature trends in different days.\nR\n. Wang and J. Zhao / International Journal of Computational Intelligence Systems 14(1)1‚Äì10 3\nestablish the quantitative and accurate model with the input infor-\nmation both of numerical data and textual data. So, a novel fore-\ncasting model based on deep learning is needed.\n2.1. One-Dimensional (1D) CNN\nCNN contains convolutional calculations and has a deep struc-\nture. It‚Äôs a kind of feed forward neural networks [17,18]. CNN has\nthe ability of representation learning and can realize shift-invariant\nclassification of input information according to the hierarchical\nstructure, extracting high-order features.\nThere are several types of CNN according to the calculation dimen-\nsion, containing 1D, 2D, 3D and 4D [19]. While obtaining inter-\nested features of in shorter segments with fixed length in the overall\ndata set, 1D CNN is effective. The process of 1D CNN is shown in\nFigure 3.\nFor single filter, the mathematical expression can be described as\nfollows:\nMm√ón ‚äó Fp√ón =\n‚é°‚é¢‚é¢\n‚é¢\n‚é£\nM1\np√ón ‚ãÖ Fp√ón\n‚ãÆ\nMm‚àíp+1\np√ón ‚ãÖ Fp√ón\n‚é§‚é•‚é•‚é•\n‚é¶.m‚àíp+1/√ón\n(1)\nwhere Mm√ón is the input matrix andMi\np√ón is theith submatrix of\nMm√ón, Fp√ón (x) is the filter,p is the window length for 1D convolu-\ntion. And in the practical calculation, there are a certain number of\nfilters to realize the 1D convolutional calculation.\n2.2. ¬†Recursive Neural Network\nFor 1D CNN is not sensitive enough to the time sequence, the\nperformance of feature extraction will decrease while settling with\nlonger data sequence, so RNN is considered. In RNN, the modu-\nlar architecture of the cyclic neural network is adopted which can\nbetter reflect the correlation between current results and historical\ndata [20].\nThe core part of RNN is the directed graph and the chained ele-\nments in a directed graph expansion is called RNN cell. In gen-\neral, the chain connection formed by the RNN cells can be analo-\ngous to the hidden layers in the feed forward neural network, but in\ndifferent discussions, the ‚Äúlayer‚Äù of the RNN may refer to a single\ntime step loop unit or all loop units [21]. The framework of RNN is\nshown as Figure4.\nF\nigure 3 W\nork process of one-dimensional convolutional neural\nnetwork (1D CNN).\nThe calculation and update process of the output and state of the\ncyclic neural network can be described as follows:\n{st = g .wssst‚àí1 + wsxxt‚àí1 + bs/ , s0 = 0\nyt = g\n(\nwysst + by\n) (2)\nwhere, x* is the input,y* is the output, s* is the state,w** stands\nfor the weights of corresponding variable, b* corresponding offset\nvalue, the subscriptt is the processing step number.\nAlthough RNN utilizes the history data by circulation, there is a gra-\ndient disappearance problem. When data sequence is long, the error\nof the backward time step cannot be propagated to the previous time\nstep for optimization calculation, thereby causing the information\nbefore and after the certain time distance to not be correlated.\n2.3. Long Short-Term Memory\nTo settle the problem in the processing of long data sequence, the\ngated algorithm is considered. Through the gating unit, the RNN\ncontrols the accumulation of internal information. When learning,\nit can grasp long-distance dependence and selectively forget infor-\nmation to prevent overload. There are also different types of LSTM.\nA typical framework of LSTM is show in Figure5.\nIn Figure 5, the symbol ‚ÄúF‚Äù means that the input data is fused\ninto an array. In LSTM, it adds a method of carrying information\nacross multiple time steps, allowing previous information to be re-\nentered in subsequent calculations, thus solving the problem of gra-\ndient vanishing [22,23]. Therefore, use LSTM to enhance the cor-\nrelation analysis of historical data and improve prediction accu-\nracy. In LSTM, there are novel designs: forget gate determines that\nsome part of the data in the previous moment needs to be forgotten,\ninput gate to determine that some part of the current input needs to\nremain in the state, and output gate to determine the system input\nfrom the current moment, the input from the previous moment.\nF\nigure 4 F\nramework of recursive neural network¬†(RNN).\nF\nigure 5 F\nramework of long short-term memory (LSTM).\n4 R\n. Wang and J. Zhao / International Journal of Computational Intelligence Systems 14(1)1‚Äì10\nSome parts of the information combined with the status informa-\ntion can be used as the final output. The expressions are as follows:\nit = ùúé.Wxixt + Whiht‚àí1 + Wcict‚àí1 + bi/ (3)\nft = ùúé\n(\nWxfxt + Whfht‚àí1 + Wcfct‚àí1 + bf\n)\n(4)\nct = ftct‚àí1 + it tanh .Wxcxt + Whcht‚àí1 + bc/ (5)\not = ùúé.Wxoxt + Whoht‚àí1 + Wcoct‚àí1 + bo/ (6)\nht = ot tanh .ct/ (7)\nwhere, it is the input gate,ft is the forget gate,ct is the cell state,ot\nis the output gate,w* is the weight,b* is the bias,h* is the output of\nthe cell,x* is the input,ùúé is the sigmod function.\n3. PROPOSED METHOD\nCompared with the existing load forecasting methods, the main\ninnovations of proposed method in this research contain two\naspects. First, to reduce the forecasting error, several highly associ-\nated factors are considered as inputs of the forecasting model that\nthe numerical and textual information are both considered. Second,\nwhile constructing the deep learning model for load forecasting,\nthe inception structure and residual connection are applied in the\nmodel modification to tackle with the numerical and textual infor-\nmation, improving the ability of feature extraction and the effect of\nbackpropagation.\n3.1. Textual Information Processing Method\nfor Load Forecasting\nBesides the numerical data, some text information also indicates\nthe load trend, such as the description that which festival will come.\nIn traditional method, the text information is quite hard quantified\nand used for the prediction work. With the advantage of powerful\ncomputing capability of deep learning network, the text informa-\ntion can be vectorized based on word embedding, which is different\nfrom the traditional model.\nThere are many vectorization methods. Among these methods,\nboth consider the distinguish effect and computing workload, the\none-hot coding method is adopted for there is a small amount of\ndescription words [24]. Using one-hot coding, the value of the dis-\ncrete feature is extended to the European space, and a certain value\nof the discrete feature corresponds to a certain point in the Euro-\npean space. Using one-hot encoding for discrete features will make\nthe distance calculation between features more reasonable.\nAccording to the requirement of grid operation, the demand of the\nelectricity utilization has been ensured during the mayor festival,\nsuch as National Day, Spring Festival. And we can have the setDtype,\ncontaining the types of day.\nDtype = {‚Ä≤workday‚Äµ,‚Ä≤ weekend‚Äµ,‚Ä≤ National ‚àí Day‚Äµ, ‚Ä¶} (8)\nAnd the one-hot coding ofDtype can be described as\nV\n(\nDtype\n) one-hot\n‚Üí\n‚é°‚é¢\n‚é¢‚é¢\n‚é£\n1 0 ‚ãØ 0\n0 1 ‚ãØ 0\n‚ãÆ ‚ãÆ ‚ã± ‚ãÆ\n0 0 ‚ãØ 1\n‚é§‚é•\n‚é•‚é•\n‚é¶len\n(\nDtype\n)\n√ólen\n(\nDtype\n)\n(9)\nwhere, the elements inDtype are coded as row vectors.\nThe types of day are represented by the derived row vectors cor-\nrespondingly. And similarly, the states of distribution lines can be\nvectorized. The states of distribution lines contain the insulation sit-\nuation and the historical situation of power supply area. For exam-\nple, the description is ‚ÄúThe insulation situation is poor, the histori-\ncal load varies greatly. ‚Äù And all these messages are encoded for fore-\ncasting.\nAfter the textual data is vectorized, the dimension needs to be fur-\nther extended to the same dimension of the numerical data which\nis collected in minutes. With the branch of the text input processing\nadded, the framework of load forecasting is presented as Figure6.\n3.2. Inception Structure for Feature\nExtraction of Numerical Data\nThe aforementioned CNN, RNN and LSTM provide basic deep\nlearning framework. However, the simple CNN or LSTM has a\nlimited ability of data extraction. Meanwhile, when the CNN and\nLSTM are both introduced, the network grows larger, resulting in\nthe gradient vanishing or computing efficiency decreasing. So, the\nperformance of the model needs to be optimized for a better result.\nThere have been several operations for improvement of deep learn-\ning network, such as residual connection, stacking integration,\ninception structure, and so on. Among these improvement meth-\nods, the inception structure expands the width of model and\nreduces the scale of filters to get better performance, changing\nthe thought that the network is constructed deeper by stacking\nconvolution layers. So, for the purpose of mining the features of\ndata better, the inception structure is also introduced [25]. Such\nas GoogLeNet [26], it proposed inception structure, improving the\nlocal topology, performing multiple convolution and pooling oper-\nations on the input image in parallel, and splicing all the output\nresults into a very deep feature map. There have been several pop-\nular inception structures, known as Inception V1~V4 [27] and\nInception-ResNet [28].\nFor the existing Inception series algorithms is suitable for the image\nrecognition, it costs computing resource and the sizes of filters\nare not suitable for data prediction, so the new designed incep-\ntion structure for the load forecasting is proposed. It is shown as\nFigure 7.\nThe proposed inception structure makes some optimization,\nincluding factorization of kernels with small size and parallel\nF\nigure 6 F\nramework of textual data proposed model.\nR\n. Wang and J. Zhao / International Journal of Computational Intelligence Systems 14(1)1‚Äì10 5\nbranch computing. With the inception module introduced, the net-\nwork can mine more subtle features through the 3 branches, while\nit also owns higher computational efficiency.\nMeanwhile, in this forecasting model, the stacking integration strat-\negy can combine deep learning networks together with the advan-\ntage of all networks. To further improve the performance of the\nload forecasting model, the CNN and LSTM are both considered\nto be applied in the new solution, meanwhile, inception structure\nis adopted to prevent the gradient vanishing when the network of\nmodel grows larger.\n3.3. Residual Connection of Numerical and\nTextual Data\nBased on the framework of inception and stacked LSTM, with the\nnumber of layers in the network increasing, there may be the prob-\nlem of gradient diffusion [29,30]. In Figure8, it shows that the orig-\ninal data are decomposed into the numerical and textual data, and\nthen both of data are concentrated after the feature extraction in\neach branch for further calculation. In the deep learning process,\nattenuation of the backpropagation of textual data and numerical\ndata will occur.\nThe topology of the deep learning framework can be described as\nFigure 9. CNN1 is the CNN with inception structure. CNN2 and\nF\nigure 7 I\nnception structure proposed for load\nprediction.\nF\nigure 8 F\nramework of the data processing.\nCNN3 are the convolution layers with different number of filters\nand different sizes of kernel.\nThe input of x is decomposed into x1 and x2. The deep learn-\ning model can be recognized as that it has two input. To settle\nthe gradient diffusion, two bypass residual connection structure in\nFigure 10.\nSuppose that L(.) is the loss function, there arem layers in the\nmodel, Zm is the corresponding output andY is the input of thenth\nlayers. With the residual connection added,Y is\nY = zn‚àí1 + x1 + x2 (10)\nAnd the derivation of loss function to inputx1 can be derived as\nùúïL\nùúïx1\n= ùúïL\nùúïzm\nùúïzm\nùúïzm‚àí1\n‚ãØ ùúïY\nùúïx1\n= ùúïL\nùúïzm\nm‚àín\n‚àè\ni=0\nùúïzm‚àíi\nùúïzm‚àíi‚àí1\nùúï .x1 + x2 + zn‚àí1/\nùúïx1\n= ùúïL\nùúïzm\nm‚àín\n‚àè\ni=0\nùúïzm‚àíi\nùúïzm‚àíi‚àí1\n(\n1\n+ ùúïzn‚àí1\nùúïx1\n)\n(11)\nS\nimilarly, the derivation to inputx2 is\nùúïL\nùúïx2\n= ùúïL\nùúïzm\nm‚àín\n‚àè\ni=0\nùúïzm‚àíi\nùúïzm‚àíi‚àí1\n(\n1\n+ ùúïzn‚àí1\nùúïx2\n)\n(12)\nF\nor the exiting of activation function attached to convolutional\nlayer, such as Relu function, the derivation to inputx1 and x2 will\nnot be negative number, which improves the effect of backpropa-\ngation.\nSummarizing the above method, first the numerical data and tex-\ntual information on the time axis are adopted to predict the future\nload, then the modified inception structure extracts essential fea-\ntures from high-dimensional features for prediction and improves\nfeature expression ability. Meanwhile, the residual connection is\nintroduced to tackle with the problem of gradient diffusion, with\nthe inputs of numerical and textual data considered.\nF\nigure 9 M\nain procedure of the forecasting without residual connection.\nF\nigure 10 M\nain procedure of the forecasting with residual\nconnection.\n6 R\n. Wang and J. Zhao / International Journal of Computational Intelligence Systems 14(1)1‚Äì10\nF\nigure 11 M\nain implementation of load forecasting.\n4. MAIN IMPLEMENTATION PROCESS\nIn this part some key implementation program is introduced to\nrealize the proposed framework based on the deep learning frame-\nwork, such as Tensorflow. And the main process is shown in\nFigure 11.\nFirst, the data has to be collected. load data, date and the states of\ndistribution lines are extracted from the related monitoring system\nof distribution network. And different types of data are processed\nwith the same dimension and every record has temporal correspon-\ndence.\nThen, the textual data is vectorized by one-hot coding and all data\nshould be reorganized. In this procedure, a data generator is con-\nstructed to realize the reorganization (seen in Appendix A). The\nbenefits of constructed generator include (1) The generator can\ngenerate the historical data containing textual and numerical data,\nbased on which the short-term forecasting is carried out, and target\nload values dynamically. (2) With the generator, the training set and\ntesting set can be generated dynamically and quickly, so the storage\nspace occupied by the data set can be reduced. (3) In the training\nprocess, the data sets can be flexibly adjusted according to the train-\ning results.\nThirdly, based on the framework of deep learning, the proposed\nmodel is constructed. The main program of the constructed model\nis presented in Appendix B with the inception and residual connec-\ntion. Through the above procedures, the forecasting load for trans-\nformer in distribution network can be realized based on the modi-\nfied model.\n5. TEST RESULTS\n5.1. Training Results\nTo test the performance of forecasting model, the operation data of\nthe transformer located in Fuzhou with rated capacity of 315kW is\nextracted with the maximum load factor 0.93, which overloaded in\nthe period of National Day in 2019.\nAnd the experiment is carried out based on the dataset of the col-\nlected weather, load and date type. All of the data is processed\nwith the interval of one hour. There are 64478 records, divided into\n50000 training samples and validation samples with the left records.\nTensorflow and Keras are used as the program framework. The fore-\ncasting rule is using the past 168h records, predicting the load in\nnext 6h. And the parameters are set as value of batch size is 64,\nnumber of iterations in each epoch is 100 and number of epochs is\n30. The results are shown in Figure12.\nThrough comparison, it can be found the training can be carried\nout but the training loss is high based on CNN and LSTM. And then\nthe textual information added, the training loss decreases because\nthe useful text provides other relationship for prediction, however\nthe validation loss is still greater than training loss, even there is\nover-fitting phenomenon to some extent. And it is obvious that\nthe proposed method can derive less loss performance than CNN\n+ LSTM method. And meanwhile the validation loss and training\nloss are more consistent.\n5.2. Forecasting Results\nFurther, to testify the performance of the models, the comparison\nof short-term load prediction is carried out. Based on the corre-\nsponding recorded data in past 168h, the forecasting load in next\n72h is shown. In Figure13, the forecasting results of the models\nbased on LSTM [13], stacked LSTM [14], CNN-LSTM [15] with\nnumerical data as input are presented respectively. Although in the\nprevious published papers, it has verified the performance of the\nLSTM-based forecasting model is better than traditional methods,\nsuch as BP neural network, random forest model and so on, there\nis still room for improvement.\nIt can be found that the errors of forecasting results are large. To\nfurther show the improved performance when the textual informa-\ntion and residual connection are introduced, the results predicted\nby the CNN-LSTM with text input and the proposed model are also\npresented in Figure14.\nThrough Figure14, the forecasting results are better than the results\nin Figure13. Meanwhile, in this research the key point is that the\nsituation of overload is prevented as possible. So, the forecasting\nresults should be analyzed and there are 11 overload-points that the\nload of transformer is greater than 292kW , which is the approxi-\nmate value derived by rated capacity timing load factor (315kW √ó\n0.93). To quantify the errors of the above forecasting results, the\nrelative error comparison is calculated based on the formula of\n|loadforecast ‚àí loadtrue| /loadtrue √ó 100%.\nFor convenience, the error of the forecasting result of LSTM is\ndenoted as Error 1. The error of stacked LSTM is denoted as Error\n2. The error of CNN-LSTM with numerical data is denoted as Error\n3. The error of CNN and LSTM with textual and numerical data is\ndenoted as Error 4. The error of the proposed model in this paper\nis denoted as Error 5. And the comparison of different errors when\noverload occurred in transformer is shown in Figure15.\nFurther, the numerical comparison result is summarized in Table1.\nIt shows that the Max-error of Error 1 is the smallest and the Ave-\nerror of Error 5 is the smallest. Combined with Figures13 and 14,\nR\n. Wang and J. Zhao / International Journal of Computational Intelligence Systems 14(1)1‚Äì10 7\nF\nigure 12 (a)\nLoss trends of training and validation loss in the situation that model with the input of numerical data is\nconstructed based on convolutional neural network¬†(CNN) and long short-term memory (LSTM); (b) is the training and\nvalidation loss trends when textual data is taken as input of CNN and LSTM and (c) the training and validation loss trends\nusing the above proposed methods.\nF\nigure 13 F\norecasting results of the long short-term memory (LSTM)-based models.\nF\nigure 14 F\norecasting results of the improved models.\nit can be found that the Max-error occurs when the load is light.\nAnd it shows the overall performance of Error 5 is better than others\nwith the smallest Ave-error. Further, for overload prevention is the\nmain purpose, the forecasting errors are also evaluated when load\nis greater than 292kW .\nWhile overloading, the Max-error of Error 5 is 37.9kW , which is\nnearly 20.5% smaller than the other models. And the Ave-error of\nError is 21.9kW , which is 24.7% smaller than the others. So based on\nthe proposed model, the occurrence of the overload in transformers\nin distribution grid can be reduced.\n8 R\n. Wang and J. Zhao / International Journal of Computational Intelligence Systems 14(1)1‚Äì10\nF\nigure 15 C\nomparison of errors of 5 models when overload occurred.\n5.3. Limitations\nIn former section, the performance of the proposed model is pre-\nsented. There are some still limitations of the proposed method,\nmainly including:\n1. Complicated structure. For the combination of different types\nof structures in the model, the number of the parameters in\nproposed model is more than other models.\n2. More training time. For the number of the parameters in the\nmodel is large and the no optimization of training strategy is\ncarried out in model training yet. The training of model needs\nmore time.\n3. The prediction error is large when the load fluctuates greatly\nand there is still room for the improvement of proposed model.\nFor the load of transformer is affected by too many random\nfactors and fluctuates greatly in hour in practical situation,\nalthough many factors are considered in this paper, there are\nstill some important factors, such as the usage habits of power\nutilizers, dispatching plan of power grid, and so on. To improve\nthe accuracy of load forecasting, the related factors should be\nalso considered and transformed into the input of forecasting\nmodel.\n6. CONCLUSIONS\nWith the development of deep learning, to improve the accuracy\nin prediction of load, the main framework of the model consists of\nCNN and LSTM with the numerical data and textual information\nas inputs. Meanwhile, to settle the problem while the layers growing\nmore, residual connection and inception structure are combined to\nimprove the forecasting effect and computational efficiency. At last,\nthe comparison is carried out and the results are presented to show\nthe improvement. For there are too many factors influencing the\nvariation of load and the load of transformer varies ceaselessly, the\nforecasting of load in hour is a hard work and there is still much\nfurther work to improve the prediction accuracy.\nTable 1\nN\numerical comparison of prediction errors (in kW).\nN\nO. Max-error Ave-error Max-error\n(load >\n292kW)\nAve-error\n(load >\n292kW)\nEr\nror 1 124.7 34.2 54.1 36.7\nError 2 191.9 39.9 47.7 33.3\nError 3 143.9 35.7 52.8 34.9\nError 4 140.8 36.0 50.4 33.6\nError 5 140.1 31.7 37.9 21.9\nCONFLICTS\nOF INTEREST\nThe authors declare no conflicts of interest.\nAUTHORS‚Äô CONTRIBUTIONS\nMethodology, Renshu Wang; software, Renshu Wang; validation,\nRenshu Wang, Jing Zhao; formal analysis, Renshu Wang; investiga-\ntion, Renshu Wang; resources, Renshu Wang; data curation, Ren-\nshu Wang; writing‚Äîoriginal draft preparation, Jing Zhao; writing‚Äî\nreview and editing, Jing Zhao; visualization, Renshu Wang; supervi-\nsion, Renshu Wang; project administration, Renshu Wang; funding\nacquisition, Renshu Wang.\nFunding Statement\nThis research was funded by the research project of State Grid Cor-\nporation of China (Electric artificial intelligence experiment and\npublic service platform technology), grant number 52130419000J\nand research project of State Grid Fujian Electric Power Co., Ltd.,\ngrant number 52130420000D.\nACKNOWLEDGMENTS\nThis research is supported by the research program from Postdoc-\ntoral Workstation of State Grid Fujian Electric Power Co., Ltd. and\nthe research projects of 52130419000J and 52130420000D. Thank\nreviewers and editors for their helpful comments on this paper. And\nR\n. Wang and J. Zhao / International Journal of Computational Intelligence Systems 14(1)1‚Äì10 9\nwe also appreciate Bin Chen, Chaohui Xie, Lifang Wu, Bingqian Liu,\nWuxiao Chen for their efforts on research guidance and data col-\nlection.\nREFERENCES\n[1] V . Vittal, J.A. Momoh, F.F. Wu, Applied Mathematics for Restruc-\ntured Electric Power Systems[J], IEEE Transactions on Automatic\nControl, 50 (2005), 1470‚Äì1471.\n[2] Z.E.N.G. Pan, S.H.E.N.G. Chang, J.I.N. Min, A learning frame-\nwork based on weighted knowledge transfer for holiday load fore-\ncasting[J], J. Mod. Power Syst. Clean Energy. 7 (2019), 329‚Äì339.\n[3] H. Li, Short term load forecasting by adaptive neural network[J],\nIOP Conf. Ser. Mater. Sci. Eng. 449 (2018), 012‚Äì028.\n[4] D. Zhenhai, Y. Rengang, J. Jiao, Method of short-term load fore-\ncasting based on mean generating function-optimal subset regres-\nsion[J], Transactions of the Chinese Society of Agricultural Engi-\nneering, Beijing, China, 29 (2013), 178‚Äì184.\n[5] H. Wang, Y. Zhao, S. Tan. Short-term load forecasting of power\nsystem based on time convolutional network[C], 2019 8th Inter-\nnational Symposium on Next Generation Electronics (ISNE),\n2019.\n[6] P . Zhang, X. Wu, X. Wang, et al., Short-term load forecasting based\non big data technologies[J], CSEE J. Power Energy Syst. 1 (2015),\n59‚Äì67.\n[7] S.R. Khuntia, J.L. Rueda, M.A.M.M. van der Meijden, Forecast-\ning the load of electrical power systems in mid-and long-term\nhorizons: a review[J], IET Gener. Transm. Distrib. 10 (2016),\n3971‚Äì3977.\n[8] M. Ghofrani, D. Carson, M. Ghayekhloo, Hybrid clustering-\ntime series-bayesian neural network short-term load forecasting\nmethod[C], in 2016 North American Power Symposium (NAPS).\nIEEE, Denver, CO, USA, 2016.\n[9] P . Audet, J.C. Mareschal, Wavelet analysis of the coherence\nbetween Bouguer gravity and topography: application to the elas-\ntic thickness anisotropy in the Canadian Shield[J], Geophys. J. Int.\n168 (2018), 287‚Äì298.\n[10] L. Ziluan, L. Xin, Short-term traffic forecasting based on princi-\npal component analysis and a generalized regression neural net-\nwork for satellite networks[J], J China Univ. Posts Telecommun.\n25 (2018), 15‚Äì28+36.\n[11] P .E.N.G. Xiuyan, Z.H.A.N.G. Biao, C.U.I. Yanqing, The short-\nterm load forecasting of electric power system based on combina-\ntion forecast model[C], in The 27th China Conference on Control\nand Decision, Qingdao, China, 2015.\n[12] P . Singh, P . Dwivedi, Integration of new evolutionary approach\nwith artificial neural network for solving short term load forecast\nproblem[J], Appl. Energy. 217 (2018), 537‚Äì549.\n[13] H. Chen, C. Wang, B. Xu, et al., Short-term load forecasting based\non LSTM network[J], Electr. Eng. 29 (2019), 27‚Äì29.\n[14] W . Kong, Z.Y. Dong, D.J. Hill, et al., Short-Term Residential\nLoad Forecasting Based on Resident Behaviour learning[J], IEEE\nTransactions on Power Systems, 2017, 1087‚Äì1088.\n[15] J. Lu, Q. Zhang, Z. Yang, et al., Short-term load forecasting method\nbased on CNN-LSTM hybrid neural network model[J], Autom.\nElectr. Power Syst. 43 (2019), 191‚Äì197.\n[16] K. Chen, K. Chen, Q. Wang, et al., Short-term load forecast-\ning with deep residual networks[J], IEEE Trans. Smart Grid. 10\n(2019), 3943‚Äì3952.\n[17] N. Ketkar, Convolutional Neural Networks[M], Apress, Berkeley,\nCA, 2017.\n[18] J. Gu, Z. Wang, J. Kuen, et al., Recent Advances in Convolutional\nNeural Networks[J]. Pattern Recognition, 77 (2018), 354‚Äì377.\n[19] F. Monti, D. Boscaini, J. Masci et al., Geometric deep learning\non graphs and manifolds using mixture model CNNs[C], in 2017\nIEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), IEEE, Honolulu, HI, USA, 2017, pp. 5425‚Äì5434.\n[20] H. Shi, M. Xu, R. Li, Deep learning for household load forecast-\ning‚ÄîA novel pooling deep RNN[J], IEEE Transactions on Smart\nGrid. 9 (2018), 5271‚Äì5280.\n[21] A. Jain, A.R. Zamir, S. Savarese, et al., Structural-RNN: deep\nlearning on spatio-temporal graphs[C], in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition,\nSan Francisco, CA, USA, USA, 2016, pp. 5308‚Äì5317.\n[22] Y. Cai, Z. Cai, M. Zeng, X. Liu, J. Wu, et al., A Novel Deep Learning\nApproach: Stacked Evolutionary Auto-encoder[C], 2018 Inter-\nnational Joint Conference on Neural Networks (IJCNN), Rio de\nJaneiro, 2018, 1‚Äì8.\n[23] S. Bouktif, A. Fiaz, A. Ouni et al., Optimal deep learning\nlstm model for electric load forecasting using feature selec-\ntion and genetic algorithm: comparison with machine learning\napproaches[J], Energies. 11 (2018), 1636.\n[24] F. Jafarzadehpour, A.S. Molahosseini, A.A.E. Zarandi, et al., Effi-\ncient modular adder designs based on thermometer and one-hot\ncoding[J], IEEE Trans. Very Large Scale Integr. (VLSI) Syst. 27\n(2019), 2142‚Äì2155.\n[25] X. Qian, Y. Fu, Y.G. Jiang, et al., Multi-scale deep learning\narchitectures for person re-identification[C], in Proceedings of\nthe IEEE International Conference on Computer Vision, Venice,\nItaly, 2017, pp. 5399‚Äì5408.\n[26] Z. Zhong, L. Jin, Z. Xie, High performance offline handwritten\nchinese character recognition using googlenet and directional fea-\nture maps[C], in 2015 13th International Conference on Docu-\nment Analysis and Recognition (ICDAR), IEEE, Tunis, Tunisia,\n2015, pp. 846‚Äì850.\n[27] F. Chollet, Xception: deep learning with depthwise separable con-\nvolutions[C], in Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, IEEE, Honolulu, HI, USA,\n2017, pp. 1251‚Äì1258.\n[28] S.S. Han, M.S. Kim, W . Lim, et al., Classification of the clin-\nical images for benign and malignant cutaneous tumors using\na deep learning algorithm[J], J. Invest. Dermatol. 138 (2018),\n1529‚Äì1538.\n[29] M.M. Bronstein, J. Bruna, Y. LeCun, et al., Geometric deep learn-\ning: going beyond euclidean data[J], IEEE Signal Process. Mag. 34\n(2017), 18‚Äì42.\n[30] Y. Wang, M. Liu, Z. Bao, Deep learning neural network for power\nsystem fault diagnosis[C], in 2016 35th Chinese Control Confer-\nence (CCC), IEEE, Chengdu, China, 2016, pp. 6678‚Äì6683.\n10 R\n. Wang and J. Zhao / International Journal of Computational Intelligence Systems 14(1)1‚Äì10\nAppendix A\nwhere num_data is the original numerical data, text_data is the vec-\ntorized data, onehot_dim is the dimension of the vector after one-\nhot coding, lookback is the range of the history data for prediction,\nfuture is the moment at which the load will be predicted, minind is\nthe minimal index of the data setÔºåmaxind is the maximum index\nof the data setÔºåbhsize is the size of the batch of samples.\nP\nrogram 1: Data generator\nP\nrocess: define generator(num_data, text_data, future, onehot_dim,\nlookback, minind, maxindÔºåbhsize)\nwhile 1:\nif i + bhsize >= maxind:\ni = minind + lookback\nrows=np.arange(i, min(i+bhsize, maxind))\ni += len(rows)\nsamples=np.zeros((len(rows),lookback// step, num_data.shape[-1]))\ntargets = np.zeros((len(rows),))\ntxtsamples=np.zeros((len(rows),lookback//step,onehot_dim))\nfor j, row in enumerate(rows):\nindices=range(rows[j]-lookback, rows[j], step)\nsamples[j] = num_data[indices]\ntxtsamples[j]=text_data[indices]\ntargets[j] = data[rows[j] + future][1]\nyield [samples,txtsamples], targets\nAppendix\nB\nwhere the num and txt represent the inputs of numerical data and\ntextual data respectively, ki is the number of kernels in correspond-\ning layers, t1 is the output result.\nP\nrogram 2: model construction\nP\nrocess:num=Input(shape=(None, num_data.shape[-1]))\ntxt=Input(shape=(None, onehot_dim))\nt1_a=layers.Conv1D(k1,1,activation=‚Äùrelu‚Äô , padding=‚Äùsame‚Äô)(num)\nt1_b=layers.Conv1D(k2, 1, activation=‚Äùrelu‚Äô , padding=‚Äùsame‚Äô)(num)\nt1_b=layers.Conv1D(k3, 3, activation=‚Äùrelu‚Äô , padding=‚Äùsame‚Äô)(t1_b)\nt1_c=layers.Conv1D(k4,3,activation=‚Äùrelu‚Äô , padding=‚Äùsame‚Äô)(num)\nt1_c=layers.Conv1D(k5, 3, activation=‚Äùrelu‚Äô , padding=‚Äùsame‚Äô)(t1_c)\nt1=layers.concatenate([t1_a,t1_b,t1_c], axis=-1)\ntxt= layers.Conv1D(k6,3,activation=‚Äùrelu‚Äô , padding=‚Äùsame‚Äô)(txt).\n‚Ä¶\nt1=layers.LSTM(k7, dropout=0.5)(t1)\nt1=layers.Dense(1)(t1)\nmodel=Model([num, txt], t1)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7011640071868896
    },
    {
      "name": "Term (time)",
      "score": 0.6455767154693604
    },
    {
      "name": "Grid",
      "score": 0.5445653200149536
    },
    {
      "name": "Transformer",
      "score": 0.5391170978546143
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5323997139930725
    },
    {
      "name": "Machine learning",
      "score": 0.36356061697006226
    },
    {
      "name": "Electrical engineering",
      "score": 0.17344114184379578
    },
    {
      "name": "Mathematics",
      "score": 0.13113537430763245
    },
    {
      "name": "Engineering",
      "score": 0.0999290943145752
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210111970",
      "name": "Fujian Electric Power Survey & Design Institute",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210111400",
      "name": "Education Department of Fujian Province",
      "country": "CN"
    }
  ],
  "cited_by": 12
}