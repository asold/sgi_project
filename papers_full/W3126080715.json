{
  "title": "Segmenting Transparent Object in the Wild with Transformer",
  "url": "https://openalex.org/W3126080715",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2355899877",
      "name": "Xie, Enze",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1966306494",
      "name": "Wang, Wenjia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2226718446",
      "name": "Wang Wenhai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221641988",
      "name": "Sun, Peize",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1975116581",
      "name": "Xu Hang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2072270871",
      "name": "Liang Ding",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117169576",
      "name": "Luo, Ping",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2563705555",
    "https://openalex.org/W2803097213",
    "https://openalex.org/W2143700590",
    "https://openalex.org/W2890782586",
    "https://openalex.org/W3119686997",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W3115390238",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W3035618398",
    "https://openalex.org/W2590916945",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2962826264",
    "https://openalex.org/W2898910301",
    "https://openalex.org/W2886934227",
    "https://openalex.org/W2964242696",
    "https://openalex.org/W2124592697",
    "https://openalex.org/W2962772649",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2134156429",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2963890956",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2973465872",
    "https://openalex.org/W1923697677",
    "https://openalex.org/W2971198903",
    "https://openalex.org/W2963563573",
    "https://openalex.org/W2799213142",
    "https://openalex.org/W3015119179",
    "https://openalex.org/W2965380104",
    "https://openalex.org/W3108995912",
    "https://openalex.org/W2921526792",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2896469545",
    "https://openalex.org/W2987175876",
    "https://openalex.org/W2175023734",
    "https://openalex.org/W2172859194"
  ],
  "abstract": "This work presents a new fine-grained transparent object segmentation dataset, termed Trans10K-v2, extending Trans10K-v1, the first large-scale transparent object segmentation dataset. Unlike Trans10K-v1 that only has two limited categories, our new dataset has several appealing benefits. (1) It has 11 fine-grained categories of transparent objects, commonly occurring in the human domestic environment, making it more practical for real-world application. (2) Trans10K-v2 brings more challenges for the current advanced segmentation methods than its former version. Furthermore, a novel transformer-based segmentation pipeline termed Trans2Seg is proposed. Firstly, the transformer encoder of Trans2Seg provides the global receptive field in contrast to CNN's local receptive field, which shows excellent advantages over pure CNN architectures. Secondly, by formulating semantic segmentation as a problem of dictionary look-up, we design a set of learnable prototypes as the query of Trans2Seg's transformer decoder, where each prototype learns the statistics of one category in the whole dataset. We benchmark more than 20 recent semantic segmentation methods, demonstrating that Trans2Seg significantly outperforms all the CNN-based methods, showing the proposed algorithm's potential ability to solve transparent object segmentation.",
  "full_text": "Segmenting Transparent Object in the Wild with Transformer\nEnze Xie1 , Wenjia Wang2, Wenhai Wang3, Peize Sun1,\nHang Xu1, Ding Liang2, Ping Luo1\n1The University of Hong Kong 2Sensetime Research 3Nanjing University\nAbstract\nThis work presents a new Ô¨Åne-grained transpar-\nent object segmentation dataset, termed Trans10K-\nv2, extending Trans10K-v1, the Ô¨Årst large-scale\ntransparent object segmentation dataset. Unlike\nTrans10K-v1 that only has two limited categories,\nour new dataset has several appealing beneÔ¨Åts. (1)\nIt has 11 Ô¨Åne-grained categories of transparent ob-\njects, commonly occurring in the human domes-\ntic environment, making it more practical for real-\nworld application. (2) Trans10K-v2 brings more\nchallenges for the current advanced segmentation\nmethods than its former version. Furthermore,\na novel transformer-based segmentation pipeline\ntermed Trans2Seg is proposed. Firstly, the trans-\nformer encoder of Trans2Seg provides the global\nreceptive Ô¨Åeld in contrast to CNN‚Äôs local recep-\ntive Ô¨Åeld, which shows excellent advantages over\npure CNN architectures. Secondly, by formulat-\ning semantic segmentation as a problem of dic-\ntionary look-up, we design a set of learnable pro-\ntotypes as the query of Trans2Seg‚Äôs transformer\ndecoder, where each prototype learns the statis-\ntics of one category in the whole dataset. We\nbenchmark more than 20 recent semantic segmen-\ntation methods, demonstrating that Trans2Seg sig-\nniÔ¨Åcantly outperforms all the CNN-based methods,\nshowing the proposed algorithm‚Äôs potential ability\nto solve transparent object segmentation. Code is\navailable in github.com/xieenze/Trans2Seg.\n1 Introduction\nModern robots, mainly mobile robots and mechanical manip-\nulators, would beneÔ¨Åt a lot from the efÔ¨Åcient perception of\nthe transparent objects in residential environments since the\nenvironments vary drastically. The increasing utilization of\nglass wall and transparent door in the building interior and\nthe glass cups and bottles in residential rooms has resulted in\nthe wrong detection in various range sensors. In robotic re-\nsearch, most systems perceive the environment by multi-data\nsensor fusion via sonars or lidars. The sensors are relatively\nconsistent in detecting opaque objects but are still affected by\nthe scan mismatching due to transparent objects. The unique\n(a) Selected images and corresponding high-quality masks.\n40\n45\n50\n55\n60\n65\n70\n75\nTrans2SegTransLabDe epLabv3+\nDANe tPSPNetOCNet\nDe nseASPP\nFCN DUNe tBiSeNetRe fineNetHardNetHRNet\nFast SCNNContextNet\nLEDNetDFANet\nmIoU of Trans10K-v2\n200\n0\nGFLOPs\n(b) Performance comparison on Trans10K-v2.\nFigure 1 ‚Äì (a) shows the high diversity of our dataset and high-\nquality annotations. (b) is Comparisons between Trans2Seg and\nother CNN-based semantic segmentation methods. All meth-\nods are trained on Trans10K-v2 with same epochs. mIoU is\nchosen as the metric. Deeper color bar indicates methods with\nlarger FLOPS. Our Trans2Seg signiÔ¨Åcantly surpasses other meth-\nods with lower Ô¨Çops.\nfeature of reÔ¨Çection, refraction, and light projection from the\ntransparent objects may confuse the sensors. Thus a reliable\nvision-based method, which is much cheaper and more robust\nthan high-precision sensors, would be efÔ¨Åcient.\nAlthough some transparent objects dataset[Xu et al., 2015;\nChen et al., 2018a; Mei et al., 2020] were proposed, there are\nsome obvious problems. (1) Limited dataset scale. These\ndatasets often have less than 1K images captured from the\nreal-world and less than 10 unique objects. (2) Poor diver-\nsity. The scene of these datasets is monotonous. (3) Fewer\nclasses. All these datasets have only two classes, background\nand transparent objects. They lack Ô¨Åne-grained categories,\nwhich limited their practicality. Recently, [Xie et al., 2020]\nproposed a large-scale and high-diversity dataset termed\nTrans10K, which divide transparent objects as ‚ÄòThings‚Äô and\n‚ÄòStuff‚Äô. The dataset is high diversity, but it also lacks Ô¨Åne-\narXiv:2101.08461v3  [cs.CV]  23 Feb 2021\ngrained transparent categories.\nIn this paper, we proposes a Ô¨Åne-grained transparent ob-\nject segmentation dataset termed Trans10K-v2 with more\nelaborately deÔ¨Åned categories. The images are inherit from\nTrans10K-v1 [Xie et al., 2020]. We annotate the 10428 im-\nages with 11 Ô¨Åne-grained categories: shelf, jar, freezer, win-\ndow, glass door, eyeglass, cup, glass wall, glass bowl, water\nbottle, storage box. In Trans10K-v1, transparent things are\ndeÔ¨Åned to be grabbed by the manipulators and stuff are for\nrobot navigation. Though two basic categories can partially\nhelp robots to interact with transparent objects, the provided\nÔ¨Åne-grained classes in Trans10K-v2 can provide more. We\nanalyze these objects‚Äô functions and how robots interact with\nthem in appendix.\nBased on this challenging dataset, we design Trans2Seg,\nintroducing Transformer into segmentation pipeline for its\nencoder-decoder architecture. First, the transformer encoder\nprovides a global receptive Ô¨Åeld via self-attention. Larger re-\nceptive Ô¨Åeld is essential for segmenting transparent objects\nbecause transparent objects often share similar textures and\ncontext with its surroundings. Second, the decoder stacks\nsuccessive layers to interact query embedding with trans-\nformer encoder output. To facilitate the robustness of trans-\nparent objects, we carefully design a set of learnable class\nprototype embeddings as the query for transformer decoder\nand the key is the feature map from the transformer encoder.\nCompared with convolutional paradigm, where the class pro-\ntotypes is the Ô¨Åxed parameters of convolution kernel weight,\nour design provides a dynamic and context-aware implemen-\ntation. As shown in Figure. 1b, we train and evaluate 20\nexisting representative segmentation methods on Trans10K-\nv2, and found that simply applying previous methods to this\ntask is far from sufÔ¨Åcient. By successfully introducing Trans-\nformer into this task, our Trans2Seg signiÔ¨Åcantly surpasses\nthe best TransLab [Xie et al., 2020] by a large margin (72.1\nvs. 69.0 on mIoU).\nIn summary, our main contributions are three-fold:\n‚Ä¢ We propose the largest glass segmentation dataset\n(Trans10K-v2) with 11 Ô¨Åne-grained glass image cate-\ngories with a diverse scenario and high resolution. All\nthe images are elaborately annotated with Ô¨Åne-shaped\nmasks and function-oriented categories.\n‚Ä¢ We introduce a new transformer-based network for\ntransparent object segmentation with transformer\nencoder-decoder architecture. Our method provides\na global receptive Ô¨Åeld and is more dynamic in mask\nprediction, which shows excellent advantages.\n‚Ä¢ We evaluate more than 20 semantic segmentation meth-\nods on Trans10K-v2, and our Trans2Seg signiÔ¨Åcantly\noutperforms these methods. Moreover, we show this\ntask is largely unsolved. Thus more research is needed.\n2 Related Work\nSemantic Segmentation. In deep learning era, convolutional\nneural network (CNN) puts forwards the development of se-\nmantic segmentation in various datasets, such as ADE20K,\nCityScapes and PASCAL VOC. One of the pioneer works\napproaches, FCN [Long et al., 2015], transfers semantic seg-\nmentation into an end-to-end fully convolutional classiÔ¨Åca-\ntion network. For improving the performance, especially\naround object boundaries, [Chen et al., 2017; Lin et al., 2016;\nZheng et al., 2015] propose to use structured prediction mod-\nule, conditional random Ô¨Åelds (CRFs) [Chen et al. , 2014 ],\nto reÔ¨Åne network output. Dramatic improvements in perfor-\nmance and inference speed have been driven by aggregating\nfeatures at multiples scales, for example, PSPNet[Zhao et al.,\n2017] and DeepLab [Chen et al., 2017; Chen et al., 2018b],\nand propagating structured information across intermediate\nCNN representations [Gadde et al., 2016; Liu et al., 2017;\nWang et al., 2018].\nTransparent Object Datasets. [Xu et al. , 2015 ] intro-\nduces TransCut dataset which only contain 49 images of 7\nunique objects. To generate the segmentation result, [Xu et\nal., 2015] optimized an energy function based on LF-linearity\nwhich also need to utilize the light-Ô¨Åeld cameras. [Chen et\nal., 2018a] proposed TOM-Net. It contains 876 real images\nand 178K synthetic images which are generated by POV-\nRay. However, only 4 unique objects are used in synthesiz-\ning the training data. Recnetly, [Xie et al., 2020] introduce\na Ô¨Årst large-scale real-world transparent object segmentation\ndataset, termed Trans10K. It has 10K+ images. However,\nthere are two categories in this dataset, which limits its practi-\ncal use. In this work, our Trans10K-v2 inherited the data and\nannotates 11 Ô¨Åne-grained categories.\nTransformer in Vision Tasks. Transformer [Vaswani\net al. , 2017 ] has been successfully applied in both high-\nlevel vision and low-level vision [Han et al. , 2020 ]. In\nViT [Dosovitskiy et al. , 2020 ], Transformer is directly ap-\nplied to sequences of image patches to complete image clas-\nsiÔ¨Åcation. In object detection areas [Carion et al. , 2020;\nZhu et al., 2020 ], DETR reasons about the relations of the\nobject queries and the global image context via Transformer\nand outputs the Ô¨Ånal set of predictions in parallel without non-\nmaximum suppression(NMS) procedures and anchor gener-\nation. SETR [Zheng et al., 2020 ] views semantic segmen-\ntation from a sequence-to-sequence perspective with Trans-\nformer. IPT [Chen et al., 2020] applies Transformer model\nto low-level computer vision task, such as denoising, super-\nresolution and deraining. In video processing, Transformer\nhas received signiÔ¨Åcantly growing attention. VisTR [Wang\net al. , 2020 ] accomplishes instance sequence segmentation\nby Transformer. Multiple-object tracking [Sun et al., 2020;\nMeinhardt et al., 2021] employs Transformers to decode ob-\nject queries and feature queries of the previous frame into\nbounding boxes of the current frame, and merged by Hungar-\nian Algorithm or NMS.\n3 Trans10K-v2 Dataset\nDataset Introduction. Our Trans10K-v2 dataset is based on\nTrans10K dataset [Xie et al., 2020]. Following Trans10K, we\nuse 5000, 1000 and 4428 images in training, validation and\ntesting respectively. The distribution of the images is abun-\ndant in occlusion, spatial scales, perspective distortion. We\nfurther annotate the images with more Ô¨Åne-grained categories\ndue to the functional usages of different objects. Trans10K-\nbackground\nshelf\nJar/Kettle\nfreezer\nwindow\ndoor\neyeglass\ncup wall\nbowl\nbottle\nbox\nFigure 2 ‚Äì Images in Trans10K-v2 dataset are carefully annotated with high quality. The Ô¨Årst row shows sample images and the\nsecond shows the segmentation masks. The color scheme which encodes the object categories are listed on the right of the Ô¨Ågure. Zoom in\nfor best view.\nTrans10Kv2 shelf door wall box freezer window cup bottle jar bowl eyeglass\nimage num 280 1572 3059 603 90 501 3315 1472 997 340 410\nCMCC 3.36 5.19 5.61 2.57 3.36 4.27 1.97 1.82 1.99 1.31 2.56\npixel ratio(%) 2.49 9.23 38.42 3.67 1.02 4.28 22.61 6.23 6.75 3.67 0.78\nTable 1 ‚Äì Statistic information of Translabv2. ‚ÄòCMCC‚Äô denotes Mean Connected Components of each category. ‚Äòimage num‚Äô denotes\nthe image number. ‚Äòpixel ratio‚Äô is the pixel number of a certain category accounts in all the pixels of transparent objects in Trans10K-v2.\nv2 dataset contains 10,428 images, with two main categories\nand 11 Ô¨Åne-grained categories: (1) Transparent Things con-\ntaining cup, bottle, jar, bowl and eyeglass. (2) Transparent\nStuff containing windows, shelf, box, freezer, glass walls\nand glass doors . In respect to Ô¨Åne-grained categories and\nhigh diversity, Trans10K-v2 is very challenging, and have\npromising potential in both computer vision and robotic re-\nsearches.\nAnnotation Principle. The transparent objects are man-\nually labeled by expert annotators with professional labeling\ntool. The annotators were asked to provide more than 100\npoints when they trace the boundaries of each transparent\nobject, which ensures the high-quality outline of the mask\nshapes. The way of annotation is mostly the same with se-\nmantic segmentation datasets such as ADE20K. We set the\nbackground with 0, and the 11 categories from 1 to 11. We\nalso provide the scene environment of each image locates\nat. The annotators are asked to strictly following principles\nwhen they label the images: (I) Only highly transparent pix-\nels are annotated as masks, other semi-transparent and non-\ntransparent pixels are ignored. Highly transparent objects no\nmatter made of glass, plastics or crystals should also be anno-\ntated. (II) When occluded by opaque objects, the pixels will\nbe cropped from the masks. (III) The setting of all 11 Ô¨Åne-\ngrained categories are elaborately observed and induced from\nthe point of function. We analyze Ô¨Årstly how the robots need\nto deal with the transparent objects as avoiding or grasping or\nmanipulating, then categorize the objects similar in shape and\nfunction into a Ô¨Åne-grained category. The detailed principle\nof how we categorize the objects is listed in appendix.\nDataset Statistics. The statistic information of CMCC,\nimaga number, pixel proportion are listed in Table 1 in de-\ntail. From Table1, the sum of all the image numbers is larger\nthan 10428 since some image has multiple category of ob-\njects. CMCC denotes Mean Connected Components of each\ncategory. It is caculated by dividing the connected compo-\nnents number of a certain category by the image number. The\nnumber of connected components are counted by the bound-\nary of the masks. It represents the complexity of the transpar-\nent objects.\nEvaluation Metrics. Results are reported in three metrics\nthat are widely used in semantic segmentation to benchmark\nthe performance of Ô¨Åne-grained transparent object segmenta-\ntion. (1) Pixel Accuracy indicates the proportion of correctly\nclassiÔ¨Åed pixels. (2) Mean IoU indicates mean intersection\nover union. (3) Category IoU indicates the intersection over\nunion of each category.\n4 Method\n4.1 Overall Pipeline\nThe overall Trans2Seg architecture contains a CNN back-\nbone, an encoder-decoder transformer, and a small convo-\nlutional head, as shown in Figure 3. For an input image of\n(H, W,3),\n‚Ä¢ The CNN backbone generates image feature map of\n( H\n16 , W\n16 , C).\n‚Ä¢ The encoder takes in the summation of Ô¨Çattened feature\nof ( H\n16\nW\n16 , C) and positional embedding of ( H\n16\nW\n16 , C),\nand outputs encoded feature of ( H\n16\nW\n16 , C).\n‚Ä¢ The decoder interacts the learned class prototypes of\n(N, C) with encoded feature, and generates attention\nmap of (N, M,H\n16\nW\n16 ), where N is number of categories,\nM is number of heads in multi-head attention.\n‚Ä¢ The small convolutional head up-samples the attention\nmap to (N, M,H\n4 , W\n4 ), fuses it with high-resolution fea-\nture map Res2 and outputs attention map of(N, H\n4 , W\n4 ).\nThe Ô¨Ånal segmentation is obtained by pixel-wise argmax op-\neration on the output attention map.\n3 x H x W\nC x (ùêª\n16\nùëä\n16)\nposition\nembedding\nCNN\n‚Ä¶\n‚Ä¶\nN x C\nLearned Class Prototypes\n‚Ä¶\n‚Ä¶\nC x (ùêª\n16\nùëä\n16) N x M x ùêª\n16\tx ùëä\n16\nattention mapùë≠ ùë≠ùíÜ\nùë¨ùíÑùíçùíî\ninput image predicted mask\nN: number of categories\nC: feature channels\nH: image height\nW: image width\nM: number heads in Transformer\nsmall\nconv\nheadTransformer\nEncoder\nTransformer\nDecoder\nFigure 3 ‚Äì The whole pipeline of our hybrid CNN-Transformer architecture. First, the input image is fed to CNN to extract fea-\ntures F. Second, for transformer encoder, the features and position embedding are Ô¨Çatten and fed to transformer for self-attention, and\noutput feature(Fe) from transformer encoder. Third, for transformer decoder, we speciÔ¨Åcally deÔ¨Åne a set of learnable class prototype\nembeddings(Ecls) as query, Fe as key, and calculate the attention map with Ecls and Fe. Each class prototype embedding corresponds\nto a category of Ô¨Ånal prediction. We also add a small conv head to fuse attention map and Res2 feature from CNN backbone. Details of\ntransformer decoder and small conv head refer to Figure 4. Finally, we can get the predict results by doing pixel-wise argmax on the atten-\ntion map. For example, in this Ô¨Ågure, the segmentation mask of two categories ( Bottle and Eyeglass) corresponds to two class prototypes\nwith same colors.\nDecoderLayer\n‚Ä¶‚Ä¶\nDecoderLayer\nDecoderLayer\nDecoderLayer\noldquery\nNxC\nC x !\n\"# x $\n\"#\nMulti-headattention\n‚Ä¶\nN x M x !\n\"#\tx $\n\"#\nAttention Map\nkey & value\nnew query\nCategory\nPrototypes\nEncoded\nFeatures\nTransformer \nDecoder\n4xup&Concat\nConv&BN&Relu\nConv1x1\nPoint-wiseArgmax\nPseudo Code of Small Conv Head\n# Attn: attentionmap,[N,M,H/16,W/16]\n# Res2:backbonefeature,[1,C,H/4,W/4]\nRes2= Res2.repeat(N,1,1,1)#[N,C,H/4,W/4]\nAttn= Up(Attn,(H/4,W/4))#[N,M,H/4,W/4]\nX= Concat([Res2,Attn]) #[N,M+C,H/4,W/4]\nX= Conv_BN_ReLU(X)#[N,M+C,H/4,W/4]\nX= Conv(X)#[N,1,H/4,W/4]\nRes= X.reshape(N,H/4,W/4)#[N,H/4,W/4]\nEncoded Features\nAttn. Map\nRes2\nFeat.\nSmallConvHead\nCNN backbone\nNxHxW\nResult\nFigure 4 ‚Äì Detail of Transformer Decoder and small conv\nhead. Input: The learnable category prototypes as query, features\nfrom transformer encoder as key and value. The inputs are fed\nto transformer decoder, which consists of several decoder layers.\nThe attention map from last decoder layer and the Res2 feature\nfrom CNN backbone are combined and fed to a small conv head\nto get Ô¨Ånal prediction result. We also provide the Pseudo Code of\nsmall conv head for better understanding.\n4.2 Encoder\nThe Transformer encoder takes a sequence as input, so the\nspatial dimensions of the feature map ( H\n16 , W\n16 , C) is Ô¨Çattened\ninto one dimension ( H\n16\nW\n16 , C). To compensate missing spa-\ntial dimensions, positional embedding [Gehring et al., 2017]\nis supplemented to one dimension feature to provide infor-\nmation about the relative or absolute position of the feature\nin the sequence. The positional embedding has the same di-\nmension ( H\n16\nW\n16 , C) with the Ô¨Çattened feature. The encoder\nis composed of stacked encoder layers, each of which con-\nsists of a multi-head self-attention module and a feed forward\nnetwork [Vaswani et al., 2017].\n4.3 Decoder\nThe Transformer decoder takes input a set of learnable class\nprototype embeddings as query, denoted byEcls, the encoded\nfeature as key and value, denoted byFe, and output the atten-\ntion map followed by Small Conv Head to obtain Ô¨Ånal seg-\nmentation result, as shown in Figure 4.\nThe class prototype embeddings are learned category pro-\ntotypes, updated iteratively by a series of decoder layers\nthrough multi-head attention mechanisms. We denoted it-\nerative update rule by ‚®Ä, then the class prototype in each\ndecoder layer is:\nEs\ncls =\n‚®Ä\ni=0,..,s‚àí1\nsoftmax(Ei\nclsFe)Fe (1)\nIn the Ô¨Ånal decoder layer, the attention map is extracted out\nto into small conv head:\nattention map = Es\nclsFe (2)\nThe pseudo code of small conv head is shown in shown in\nFigure 4. The attention map from Transformer decode is the\nshape of (N, M,H\n16\nW\n16 ), where N is number of categories, M\nis number of heads in multi-head attention. It is up-sampled\nto (N, M,H\n4 , W\n4 ), then fused with high-resolution feature\nmap Res2 in the second dimension to(N, M+C, H\n4 , W\n4 ), and\nÔ¨Ånally transformed into output attention map of (N, H\n4 , W\n4 ).\nThe Ô¨Ånal segmentation is obtained by pixel-wise argmax op-\neration on the output attention map.\n4.4 Discussion\nThe most related work with Trans2Seg is SETR and\nDETR [Zheng et al., 2020; Carion et al., 2020]. In this sec-\ntion we discuss the relations and differences in details.\nSETR. Trans2Seg and SETR are both segmentation\npipelines. Their key difference is reÔ¨Çected in the design of\nthe decoder. In SETR, the decoder is simple several con-\nvolutional layers, which is similar with most previous meth-\nods. However, the decoder of Trans2Seg is also transformer,\nwhich fully utilize the advantages of attention mechanism in\nsemantic segmentation.\nDETR. Trans2Seg and DETR share similar components\nin the pipeline, including CNN backbone, Transformer en-\ncoder and decoder. The biggest difference is the deÔ¨Ånition of\nquery. In DETR, the decoder‚Äôs queries represents N learn-\nable objects because DETR is designed for object detection.\nHowever, in Trans2Seg, the queries represents N learnable\nclass prototypes, where each query represents one category.\nWe could see that the minor change on query design could\ngeneralize Transformer architecture to apply to diverse vision\ntasks, such as object detection and semantic segmentation.\n5 Experiments\n5.1 Implementation Details.\nWe implement Trans2Seg with Pytorch. The ResNet-50 [He\net al., 2016] with dilation convolution at last stage. is adoped\nas the CNN extractor. For loss optimization, we use Adam\noptimizer with epsilon 1e-8 and weight decay 1e-4. Batch\nsize is 8 per GPU. We set learning rate 1e-4 and decayed by\nthe poly strategy [Yu et al., 2018] for 50 epochs. We use 8\nV100 GPUs for all experiments. For all CNN based methods,\nwe random scale and crop the image to 480 √ó 480 in train-\ning, and resize image to 513 √ó 513 in inference, following\ncommon setting on PASCAL VOC [Everingham and Winn,\n2011]. For our Trans2Seg, we adopt transformer architecture\nand need to keep the shape of learned position embedding\nsame in training/inference, so we directly resize the image to\n512 √ó 512. Code has been released for community to follow.\n5.2 Ablation Studies.\nWe use the FCN[Long et al., 2015] as our baseline. FCN is a\nfully convolutional network with very simple design, and it is\nalso a very classic semantic segmentation method. First, we\ndemonstrate that transformer encoder can build long range at-\ntention between pixels, which has much larger receptive Ô¨Åeld\nthan CNN Ô¨Ålters. Second, we remove the CNN decoder in\nFCN and replace by our Transformer decoder, we design a\nset of learnable class prototypes as queries and show that this\ndesign further helps improve the accuracy. Third, we verify\nour method with transformer at different scales.\nSelf-Attention of Transformer Encoder. As shown in\nFigure 2, the FCN baseline without transformer encoder\nachieves 62.7% mIoU, when adding transformer encoder,\nthe mIoU directly improves 6.1%, achieving 66.8% mIoU.\nIt demonstrates that the self-attention module in transformer\nencoder provides global receptive Ô¨Åled, which is better than\nCNN‚Äôs local receptive Ô¨Åeld in transparent object segmenta-\ntion.\nCategory Prototypes of Transformer Decoder. In Fig-\nure 2, we verify the effectiveness of learnable category pro-\ntotypes in transformer decoder. In column 2, with traditional\nid Trans. Enc. Trans. Dec. CNN Dec. mIoU\n0 √ó √ó ‚úì 62.7\n1 ‚úì √ó ‚úì 68.8\n2 ‚úì ‚úì √ó 72.1\nTable 2 ‚Äì Effectiveness of Transformer encoder and decoder.\n‚ÄòTrans.‚Äô indicates Transformer. ‚ÄòEnc.‚Äô and ‚ÄòDec.‚Äô means encoder\nand decoder.\nScale hyper-param. GFlops MParams mIoU\nsmall e128-n1-m2 40.9 30.5 69.2\nmedium e256-n4-m3 49.0 56.2 72.1\nlarge e768-n12-m4 221.8 327.5 70.3\nTable 3 ‚Äì Performance of Transformer at different scales.\n‚Äòe{a}-n{b}-m{c}‚Äô means the transformer with number of ‚Äòa‚Äô em-\nbedding dims, ‚Äòb‚Äô layers and ‚Äòc‚Äô mlp ratio.\nCNN decoder, the mIoU is 68.8%. However, with our trans-\nformer decoder, the mIoU boosts up to 72.1% with 3.3% im-\nprovement. The strong performance beneÔ¨Åts from the Ô¨Çexible\nrepresentation that learnable category prototypes as queries to\nÔ¨Ånd corresponding pixels in feature map.\nScale of Transformer. The scale of transformer is mainly\ninÔ¨Çuenced by three hyper-parameters: (1) embedding dim of\nfeature. (2) number of attention layers. (3) mlp ratio in feed\nforward layer. We are interested in whether enlarge the model\nsize can continuously improve performance. So we set three\ncombinations, as shown in Figure 3. We can Ô¨Ånd that with\nthe size of transformer increase, the mIoU Ô¨Årst increase then\ndecrease. We argue that if without massive data to pretrain,\ne.g. BERT [Devlin et al., 2019] used large-scale nlp data, the\ntransformer size is not the larger the better for our task.\n5.3 Comparison to the state-of-the-art.\nWe select more than 20 semantic segmentation methods [Xie\net al., 2020; Chen et al., 2018c; Li et al., 2019a; Zhao et al.,\n2017; Yuan and Wang, 2018; Yang et al., 2018; Long et al.,\n2015; Ronneberger et al., 2015; Yu et al., 2018; Lin et al.,\n2017; Chao et al., 2019; Wang et al., 2019a; Poudel et al.,\n2019; Poudel et al., 2018; Wanget al., 2019b; Jin et al., 2019;\nZhao et al., 2018; Li et al., 2019a; Liu and Yin, 2019; Li et\nal., 2019b; Fu et al., 2019; Mehta et al., 2019 ] to evaluate\non our Trans10K-v2 dataset, the methods selection largely\nfollows the benchmark of TransLab [Xie et al., 2020]. For\nfair comparsion, we train all the methods with 50 epochs.\nTable 4 reports the overall quantitative comparison results\non test set. Our Trans2Seg achieves state-of-the-art 72.15%\nmIoU and 94.14% pixel ACC, signiÔ¨Åcant outperforms other\npure CNN-based methods. For example, our method is 2.1%\nhigher than TransLab, which is the previous SOTA method.\nWe also Ô¨Ånd that our method tend to performs much better\non small objects, such as ‚Äòbottle‚Äô and ‚Äôeyeglass‚Äô (10.0% and\n5.0% higher than previous SOTA). We consider that the trans-\nformer‚Äôs long range attention beneÔ¨Åts the small transparent\nobject segmentation.\nIn Figure 5, we visualize the mask prediction of Trans2Seg\nand other CNN-based methods. We can Ô¨Ånd that beneÔ¨Åt from\ntransformer‚Äôs large receptive Ô¨Åeld and attention mechanism,\nMethod FLOPs ACC ‚Üë mIoU‚Üë Category IoU‚Üë\nbg shelf Jar freezer window door eyeglass cup wall bowl bottle box\nFPENet 0.76 70.31 10.14 74.97 0.01 0.00 0.02 2.11 2.83 0.00 16.84 24.81 0.00 0.04 0.00\nESPNetv2 0.83 73.03 12.27 78.98 0.00 0.00 0.00 0.00 6.17 0.00 30.65 37.03 0.00 0.00 0.00\nContextNet 0.87 86.75 46.69 89.86 23.22 34.88 32.34 44.24 42.25 50.36 65.23 60.00 43.88 53.81 20.17\nFastSCNN 1.01 88.05 51.93 90.64 32.76 41.12 47.28 47.47 44.64 48.99 67.88 63.80 55.08 58.86 24.65\nDFANet 1.02 85.15 42.54 88.49 26.65 27.84 28.94 46.27 39.47 33.06 58.87 59.45 43.22 44.87 13.37\nENet 2.09 71.67 8.50 79.74 0.00 0.00 0.00 0.00 0.00 0.00 0.00 22.25 0.00 0.00 0.00\nHRNetw18 4.20 89.58 54.25 92.47 27.66 45.08 40.53 45.66 45.00 68.05 73.24 64.86 52.85 62.52 33.02\nHardNet 4.42 90.19 56.19 92.87 34.62 47.50 42.40 49.78 49.19 62.33 72.93 68.32 58.14 65.33 30.90\nDABNet 5.18 77.43 15.27 81.19 0.00 0.09 0.00 4.10 10.49 0.00 36.18 42.83 0.00 8.30 0.00\nLEDNet 6.23 86.07 46.40 88.59 28.13 36.72 32.45 43.77 38.55 41.51 64.19 60.05 42.40 53.12 27.29\nICNet 10.64 78.23 23.39 83.29 2.96 4.91 9.33 19.24 15.35 24.11 44.54 41.49 7.58 27.47 3.80\nBiSeNet 19.91 89.13 58.40 90.12 39.54 53.71 50.90 46.95 44.68 64.32 72.86 63.57 61.38 67.88 44.85\nDenseASPP 36.20 90.86 63.01 91.39 42.41 60.93 64.75 48.97 51.40 65.72 75.64 67.93 67.03 70.26 49.64\nDeepLabv3+ 37.98 92.75 68.87 93.82 51.29 64.65 65.71 55.26 57.19 77.06 81.89 72.64 70.81 77.44 58.63\nFCN 42.23 91.65 62.75 93.62 38.84 56.05 58.76 46.91 50.74 82.56 78.71 68.78 57.87 73.66 46.54\nOCNet 43.31 92.03 66.31 93.12 41.47 63.54 60.05 54.10 51.01 79.57 81.95 69.40 68.44 78.41 54.65\nReÔ¨ÅneNet 44.56 87.99 58.18 90.63 30.62 53.17 55.95 42.72 46.59 70.85 76.01 62.91 57.05 70.34 41.32\nTranslab 61.31 92.67 69.00 93.90 54.36 64.48 65.14 54.58 57.72 79.85 81.61 72.82 69.63 77.50 56.43\nDUNet 123.69 90.67 59.01 93.07 34.20 50.95 54.96 43.19 45.05 79.80 76.07 65.29 54.33 68.57 42.64\nUNet 124.55 81.90 29.23 86.34 8.76 15.18 19.02 27.13 24.73 17.26 53.40 47.36 11.97 37.79 1.77\nDANet 198.00 92.70 68.81 93.69 47.69 66.05 70.18 53.01 56.15 77.73 82.89 72.24 72.18 77.87 56.06\nPSPNet 187.03 92.47 68.23 93.62 50.33 64.24 70.19 51.51 55.27 79.27 81.93 71.95 68.91 77.13 54.43\nTrans2Seg 49.03 94.14 72.15 95.35 53.43 67.82 64.20 59.64 60.56 88.52 86.67 75.99 73.98 82.43 57.17\nTable 4‚Äì Evaluated state-of-the-art semantic segmentation methods. Sorted by FLOPs. Our proposes Trans2Seg surpasses all the other\nmethods in pixel accuracy and mean IoU, as well as most of the category IoUs (8 in 11).\nTrans2Seg DeepLabv3+ FCN ICNet PSPNetImage GroundTruth\nFigure 5 ‚Äì Visual comparison of Trans2Seg to other CNN-based\nsemantic segmentation methods. Our Trans2Seg clearly outper-\nforms others thanks to the transformer‚Äôs global receptive Ô¨Åeld and\nattention mechanism, especially in dash region. Zoom in for best\nview. Refer to supplementary materials for more visualized re-\nsults.\nMethod #Param (M) GFLOPs mIoU (%)\nR50-SemanticFPN 28.5 45.6 36.7\nR50-d8+DeeplabV3+ 26.8 120.5 41.5\nR50-d16+DeeplabV3+ 26.8 45.5 40.6\nR50-d16+Trans2Seg 56.1 79.3 39.7\nTable 5 ‚Äì Performance of Trans2Seg on ADE20K dataset.\nTrans2Seg also works well on general semantic segmentation\ntasks. ‚Äúd8‚Äù and ‚Äúd16‚Äù means dilation 8 and 16, respectively.\n‚ÄúR50‚Äù means ResNet-50 backbone.\nour method can distinguish background and different cate-\ngories transparent objects much better than other methods,\nespecially when multiple objects with different categories oc-\ncurs in one image. Moreover, our method can obtain high\nquality detail information, e.g. boundary of object, and tiny\ntransparent objects, while other CNN-based methods fail to\ndo so. More results are shown in supplementary material.\n5.4 General Semantic Segmentation\nWe try to transfer Trans2Seg on general semantic segmenta-\ntion and it also achieves satisÔ¨Åed performance.\nExperiment Settings. We choose ADE20K [Zhou et al.,\n2017], a challenging scene parsing benchmark for semantic\nsegmentation. ADE20K contains 150 Ô¨Åne-grained semantic\ncategories, where there are 20210, 2000, and 3352 images for\ntraining, validation and, testing, respectively. We set learning\nrate to 2.5e-5 for ADE20K experiments. We train all models\nwith 40k iterations with 8 images/GPU and 8 GPUs, and use\nsingle-scale test in inference. The data augmentation is same\nas DeeplabV3+ [Chen et al., 2018c].\nResults. As shown in Table 5, compared with Semantic\nFPN and DeeplabV3+, our Trans2Seg achieves 39.7 mIoU,\nwhich is a satisÔ¨Åed performance. Our Trans2Seg veriÔ¨Åes\nrobust transfer ability on challenging general segmentation\ndataset. Please note that we do not carefully tuned the hyper-\nparameters of Trans2Seg on ADE20K dataset. We are highly\ninterested to design a better transformer-based general se-\nmantic segmentation pipeline in the future.\n6 Conclusion\nIn this paper, we present a new Ô¨Åne-grained transparent\nobject segmentation dataset with 11 common categories,\ntermed Trans10K-v2, where the data is based on the previous\nTrans10K. We also discuss the challenging and practical of\nthe proposed dataset. Moreover, we propose a transformer-\nbased pipeline, termed Trans2Seg, to solve this challenging\ntask. In Trans2Seg, the transformer encoder provides global\nreceptive Ô¨Åeld, which is essential for transparent objects seg-\nmentation. In the transformer decoder, we model the segmen-\ntation as dictionary look up with a set of learnable queries,\nwhere each query represents one category. Finally, we evalu-\nate more than 20 mainstream semantic segmentation methods\nand shows our Trans2Seg clearly surpass these CNN-based\nsegmentation methods.\nIn the future, we are interested in exploring our Trans-\nformer encoder-decoder design on general segmentation\ntasks, such as Cityscapes and PASCAL VOC. We will also\nput more effort to solve transparent object segmentation task.\n7 Appendix\n7.1 Detailed Dataset Information\nMore Visualized Demonstration of Trans10K-v2.\nIn this section we show more visualized demonstrations to\nshow the diversity and quality of Trans10K-v2. In Figure 6\nand Figure 7, we show more cropped objects to illustrate the\nhigh-diversity of the objects. We also show more images and\nground-truth masks in Figure 8. All images and transparent\nobjects in Trans10K-v2 are selected from complex real-world\nscenarios that have large variations such as scale, viewpoint,\ncontrast, occlusion, categories and transparency. From Fig-\nure 8, we can also Ô¨Ånd that it is challenging for current se-\nmantic segmentation methods.\ncup\nJar/kettle\nbottle\nbowl\neyeglass\nFigure 6 ‚Äì Cropped objects of 5 kinds of transparent things: cup,\njar, bottle, bowl, eyeglass. Zoom in for the best view.\nwall\nfreezer\nbox\ndoor\nshelf\nwindow\nFigure 7 ‚Äì Cropped objects of 6 kinds of transparent stuff: wall,\nfreezer, box, door, shelf, window. Zoom in for the best view.\nScene information\nWe also provide each image with a scene label that represents\nwhere the objects located in. As shown in the upper part of\nTable 6, we list the statistics of the distribution in different\nscenarios of each category in detail. The distribution highly\nfollows the distribution of our residential environments. For\nexample, the cups, bowls, and bottles are mostly placed on\nthe desk, while glass walls are often located in mega-malls\nor ofÔ¨Åce buildings.\nThe visualized demonstration of our diverse scene distri-\nbution is shown in Figure 9. Trans10k-v2 contains abundant\nscenarios and we induce them into 13 categories: on the desk,\nmega-mall, store, bedroom, sitting room, kitchen, bathroom,\nwindowsill, ofÔ¨Åce, ofÔ¨Åce building, outdoor, in the vehicle,\nstudy-room. This information is mainly used to demonstrate\nour abundant image distribution which could cover most of\nthe common real-life scenarios. Each image is provided with\na scene label.\nHow Robots Deal with Transparent Objects\nTransparent objects are widespread in human residential envi-\nronments, so the human-aiding robots Ô¨Ånd ways to deal with\ntransparent objects. Some former robotic research illustrates\nthe substantial value of solving this problem, mainly from\ngrasping and navigation. This research primarily focuses on\nmodifying the algorithm to deal with optical signals reÔ¨Çected\nfrom the transparent objects.\nFor the manipulator grasping, previous work mainly fo-\ncuses on grabbing water cups.[Klank et al., 2011] propose an\napproach to reconstruct an approximate surface of the trans-\nparent cups and bottles by the internal sensory contradiction\nfrom two ToF (time of Ô¨Çight) images captured from an SR4k\ncamera. The robot arm could grasp and manipulate the ob-\njects. [Spataro et al., 2015] set up a BCI-robot platform to\nhelp patients suffering from limb muscle paralysis by grasp-\ning a glass cup for the patients. Starting from the point that\nthe usual glass material absorbs light in speciÔ¨Åc wavelengths,\n[Zhou et al. , 2018 ] propose the Depth Likelihood V olume\n(DLV), which uses a Monte Carlo object localization algo-\nrithm to help the Michigan Progress Fetch robot localize and\nmanipulate translucent objects.\nFor the mobile robot navigation, some work also Ô¨Ånds\nways to exclude the side-effect of transparent stuff in residen-\ntial scenarios. [Foster et al., 2013] modify the standard occu-\npancy grid algorithm during the procedure of autonomous-\nmapping robot localize transparent objects from certain an-\ngles. [Kim and Chung, 2016 ] design a novel scan matching\nalgorithm by comparing all candidate distances scanned by\nthe laser range Ô¨Ånder penetrate and reÔ¨Çected from the glass\nwalls. [Singh et al., 2018] use information fusion by combin-\ning a laser scanner and a sonar on an autonomous-mapping\nmobile robot to reduce the uncertainty caused by glass.\nWe analyze how robots deal with transparent objects from\nprevious work and grade them into 4 patterns: navigation,\ngrasping, manipulation, human-aiding. Navigation and\ngrasping are the two fundamental interactions between robots\nand objects. Manipulation happens on complex objects like\nwindows, doors, or bottles with lids. Human-aiding is the\nhighest level of robot mission, and this kind of interaction al-\nways involve human, especially disabled patients. From these\n4 patterns, we can then analyze and categorize the transparent\nobjects in respect to functions.\nbackground\nshelf\nJar/Kettle\nfreezer\nwindow\ndoor\neyeglass\ncup\nwall\nbowl\nbottle\nbox\nFigure 8 ‚Äì More images and corresponding high-quality masks in Trans10K-v2.Our dataset is high diversity in scale, categories, pose,\ncontrast, occlusion, and transparency. Zoom in for the best view.\nCategorization Principle\nThe 11 Ô¨Åne-grained categories are based on how the robots\nneed to deal with transparent objects like avoiding or grasp-\ning or manipulating. For example, the goblet and cup are both\nopen-mouthed and mainly used to drink water. These objects\nneed to be grasped carefully since they do not have lids. They\nhave different interactive actions with the robots. So they are\nboth categorized as cup. We show the detailed demonstration\nof each category: (1) Shelf. Containing bookshelf, showcase,\ncabinet, etc. They mostly have sliding glass doors and are\nused to store goods. (2) Freezer. Containing vending ma-\nchine, horizontal freezer, etc. They are electrical equipment\nand are used to storing drinks and food. (3) Door. Con-\ntaining automatic glass door, standard glass door, etc. The\ndoors are located in mega-mall, bathroom or ofÔ¨Åce building.\nThey are highly transparent and extensive. They could be\nused in navigation and helping disabled people pass through.\n(4) Wall. Glass walls look like doors. However, walls can\nnot be opened. This clue should be perceived during mo-\nbile robots‚Äô mapping procedure. Glass walls are common\nin mega-mall and ofÔ¨Åce buildings. (5) Window. Windows\ncould be opened like glass doors but should not be traveled\nthrough. (6) Box. Large boxes may not need to be grasped,\nbut the manipulator robot needs to open the box and search\nfor speciÔ¨Åc items. (7) Cup. We category all open-mouthed\ncups like goblets and regular cups into this category. Cups\nare used for drinking water. The manipulators need to grasp\na cup carefully and be able to assist disabled people to drink\nwater. (8) Bottle. Bottles are also used to drink water. But\nbottles have lids, so they need careful manipulation. (9) Eye-\nglass. Eyeglasses need careful grasping and manipulation to\nhelp disable people wear the eyeglasses. (10) Jar. This cat-\negory contains jars, kettles and other transparent containers\nused to hold water, Ô¨Çavoring and food. (11) Bowl. Bowls are\nusually used to contain water or food. Different from jars,\nthey do not have lids and need careful grasping. The sample\nobjects of these categories could be Ô¨Ånd in Figure 8. We show\nthe most common type of different categories by cropping the\nobjects through masks.\nAs shown in the lower part of Table 6, we analyze and list\nthe interactive patterns of all the 11 Ô¨Åne-grained categories\nof objects. Navigation is the basic interactive pattern of stuff\nand grasping is the basic interactive pattern of things. All the\nobjects with some complex interactions need to be manipu-\nlated like the robots helping people open the shelf or window.\nHuman-aiding is the highest level of interaction and it always\ninvolves patients. The patients need robots to help with open-\ning the door, or feeding water by a cup or bottle.\n7.2 More Visual Results Comparison.\nIn this section, we visualize more test examples produced by\nour Trans2Seg and other CNN-based methods on Trans10K-\nv2 dataset in Figure 11. From these results, we can easily\nobserve that our Trans2Seg outputs very high-quality trans-\nparent object segmentation masks than other methods. Such\nstrong results mainly beneÔ¨Åt from the successfully introduc-\ning Transformer into transparent object segmentation, which\nis the lack in other CNN-based methods.\n7.3 Failure Case Analysis\nAs shown in Figure 10, our method also has some limitations.\nFor instance, in Figure 10 (a), when transparent objects are\noccluded by different categories, our method would confuse\nand fail to segment part of the items. In Figure 10 (b), when\nthe objects are of extreme transparency, our method would\nalso confuse and output wrong segmentation results. In such\na case, even humans would also fail to distinguish these trans-\nparent objects.\n1\n10\n100\n1000\n10000\nOn the Desk Mega-Mall\nStore\nBedroom\nSitting Room\nKitchen Bathroom Windowsill\nOffice\nOffice Building\nOutdoor\nIn the Vehicle Study-Room\nScene Distribution\nFigure 9 ‚Äì The image number distribution and selected images of different scenes in Trans10K-v2. For better demonstration, the image\nnumber in vertical axis is listed as logarithmic.\nImage GT Trans2Seg\n(a) Occlusion and Crowd.\nImage GT Trans2Seg (b) Extreme Transparency.\nFigure 10 ‚Äì Failure cases analysis. Our Trans2Seg fails to segment transparent objects in some complex scenarios.\nScene/Category\nInteraction\nStuff Things\nshelf freezer door wall window box cup bottle eyeglass jar bowl\non the desk 3 0 0 2 4 227 1946 834 239 302 117\nmega-mall 219 35 450 1762 76 128 169 36 75 94 14\nstore 13 36 5 19 3 75 444 111 1 175 57\nbedroom 6 0 4 9 23 2 23 33 6 6 1\nliving room 10 0 7 14 19 52 310 167 25 139 67\nkitchen 0 8 6 4 4 19 79 23 0 46 66\nbathroom 0 0 33 31 8 4 5 3 4 0 2\nwindowsill 0 0 0 31 209 4 17 8 8 17 2\nofÔ¨Åce room 15 7 25 43 12 84 298 235 51 158 2\nofÔ¨Åce building 8 3 1021 1107 131 5 1 5 0 2 0\noutdoor 0 0 13 20 2 0 0 2 0 0 0\nin the vehicle 0 0 2 0 1 0 4 0 0 0 0\nstudy-room 4 0 3 2 4 1 4 1 0 2 0\nnavigation ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì\ngrasping ‚úì ‚úì ‚úì ‚úì ‚úì\nmanipulation ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì\nhuman-aiding ‚úì ‚úì ‚úì ‚úì ‚úì\nTable 6 ‚Äì The upper part of this table: the number of the scene. The lower part of this table: the interaction pattern of each category.\nBiseNetTrans2Seg DeepLab FCN ICNet PSPNet UNet HRNetImage GroundTruth\nFigure 11 ‚Äì Visualized results of comparison with state-of-the-art methods. Our Trans2Seg has the best mask prediction among all\nmethods. Zoom in for the best view.\nReferences\n[Carion et al., 2020] Nicolas Carion, Francisco Massa,\nGabriel Synnaeve, Nicolas Usunier, Alexander Kirillov,\nand Sergey Zagoruyko. End-to-End object detection with\ntransformers. In ECCV, 2020.\n[Chao et al., 2019] Ping Chao, Chao-Yang Kao, Yu-Shan\nRuan, Chien-Hsiang Huang, and Youn-Long Lin. Hard-\nnet: A low memory trafÔ¨Åc network. In ICCV, 2019.\n[Chen et al., 2014] Liang-Chieh Chen, George Papandreou,\nIasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Se-\nmantic image segmentation with deep convolutional nets\nand fully connected crfs. arXiv, 2014.\n[Chen et al., 2017] Liang-Chieh Chen, George Papandreou,\nIasonas Kokkinos, Kevin Murphy, and Alan L Yuille.\nDeeplab: Semantic image segmentation with deep convo-\nlutional nets, atrous convolution, and fully connected crfs.\nTPAMI, 2017.\n[Chen et al., 2018a] Guanying Chen, Kai Han, and Kwan-\nYee K. Wong. Tom-net: Learning transparent object mat-\nting from a single image. In CVPR, 2018.\n[Chen et al., 2018b] Liang-Chieh Chen, Yukun Zhu, George\nPapandreou, Florian Schroff, and Hartwig Adam.\nEncoder-decoder with atrous separable convolution for se-\nmantic image segmentation. In ECCV, 2018.\n[Chen et al., 2018c] Liang-Chieh Chen, Yukun Zhu, George\nPapandreou, Florian Schroff, and Hartwig Adam.\nEncoder-decoder with atrous separable convolution for se-\nmantic image segmentation. In ECCV, 2018.\n[Chen et al., 2020] Hanting Chen, Yunhe Wang, Tianyu\nGuo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma,\nChunjing Xu, Chao Xu, and Wen Gao. Pre-trained image\nprocessing transformer. arXiv preprint arXiv:2012.00364,\n2020.\n[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. BERT: pre-training of\ndeep bidirectional transformers for language understand-\ning. In Jill Burstein, Christy Doran, and Thamar Solorio,\neditors, Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, NAACL-HLT\n2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1\n(Long and Short Papers) , pages 4171‚Äì4186. Association\nfor Computational Linguistics, 2019.\n[Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition\nat scale. arXiv preprint arXiv:2010.11929, 2020.\n[Everingham and Winn, 2011] Mark Everingham and John\nWinn. The pascal visual object classes challenge 2012\n(voc2012) development kit. Pattern Analysis, Statistical\nModelling and Computational Learning, Tech. Rep, 2011.\n[Foster et al., 2013] Paul Foster, Zhenghong Sun, Jong Jin\nPark, and Benjamin Kuipers. Visagge: Visible angle grid\nfor glass environments. In 2013 IEEE International Con-\nference on Robotics and Automation , pages 2213‚Äì2220.\nIEEE, 2013.\n[Fu et al., 2019] Jun Fu, Jing Liu, Haijie Tian, Yong Li,\nYongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual at-\ntention network for scene segmentation. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 3146‚Äì3154, 2019.\n[Gadde et al., 2016] Raghudeep Gadde, Varun Jampani,\nMartin Kiefel, Daniel Kappler, and Peter V Gehler. Super-\npixel convolutional networks using bilateral inceptions. In\nECCV, 2016.\n[Gehring et al., 2017] Jonas Gehring, Michael Auli, David\nGrangier, Denis Yarats, and Yann N Dauphin. Convo-\nlutional sequence to sequence learning. arXiv preprint\narXiv:1705.03122, 2017.\n[Han et al., 2020] Kai Han, Yunhe Wang, Hanting Chen,\nXinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang,\nAn Xiao, Chunjing Xu, Yixing Xu, et al. A survey on vi-\nsual transformer. arXiv preprint arXiv:2012.12556, 2020.\n[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing\nRen, and Jian Sun. Deep residual learning for image recog-\nnition. In CVPR, 2016.\n[Jin et al., 2019] Qiangguo Jin, Zhaopeng Meng, Tuan D\nPham, Qi Chen, Leyi Wei, and Ran Su. Dunet:\nA deformable network for retinal vessel segmentation.\nKnowledge-Based Systems, 2019.\n[Kim and Chung, 2016] Jiwoong Kim and Woojin Chung.\nLocalization of a mobile robot using a laser range Ô¨Ånder\nin a glass-walled environment. IEEE Transactions on In-\ndustrial Electronics, 63(6):3616‚Äì3627, 2016.\n[Klank et al., 2011] Ulrich Klank, Daniel Carton, and\nMichael Beetz. Transparent object detection and recon-\nstruction on a mobile platform. In IEEE International\nConference on Robotics & Automation, 2011.\n[Li et al., 2019a] Gen Li, Inyoung Yun, Jonghyun Kim, and\nJoongkyu Kim. Dabnet: Depth-wise asymmetric bottle-\nneck for real-time semantic segmentation. arXiv, 2019.\n[Li et al., 2019b] Hanchao Li, Pengfei Xiong, Haoqiang\nFan, and Jian Sun. Dfanet: Deep feature aggregation for\nreal-time semantic segmentation. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 9522‚Äì9531, 2019.\n[Lin et al., 2016] Guosheng Lin, Chunhua Shen, Anton Van\nDen Hengel, and Ian Reid. EfÔ¨Åcient piecewise training\nof deep structured models for semantic segmentation. In\nCVPR, 2016.\n[Lin et al., 2017] Guosheng Lin, Anton Milan, Chunhua\nShen, and Ian Reid. ReÔ¨Ånenet: Multi-path reÔ¨Ånement\nnetworks for high-resolution semantic segmentation. In\nCVPR, 2017.\n[Liu and Yin, 2019] Mengyu Liu and Hujun Yin. Feature\npyramid encoding network for real-time semantic segmen-\ntation. arXiv, 2019.\n[Liu et al., 2017] Sifei Liu, Shalini De Mello, Jinwei Gu,\nGuangyu Zhong, Ming-Hsuan Yang, and Jan Kautz.\nLearning afÔ¨Ånity via spatial propagation networks. In\nNIPS, 2017.\n[Long et al., 2015] Jonathan Long, Evan Shelhamer, and\nTrevor Darrell. Fully convolutional networks for seman-\ntic segmentation. In CVPR, 2015.\n[Mehta et al., 2019] Sachin Mehta, Mohammad Rastegari,\nLinda Shapiro, and Hannaneh Hajishirzi. Espnetv2: A\nlight-weight, power efÔ¨Åcient, and general purpose convo-\nlutional neural network. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n9190‚Äì9200, 2019.\n[Mei et al., 2020] Haiyang Mei, Xin Yang, Yang Wang,\nYuanyuan Liu, Shengfeng He, Qiang Zhang, Xiaopeng\nWei, and Rynson W.H. Lau. Don‚Äôt hit me! glass detec-\ntion in real-world scenes. InProceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), June 2020.\n[Meinhardt et al., 2021] Tim Meinhardt, Alexander Kirillov,\nLaura Leal-Taixe, and Christoph Feichtenhofer. Track-\nformer: Multi-object tracking with transformers. arXiv\npreprint arXiv:2101.02702, 2021.\n[Poudel et al., 2018] Rudra PK Poudel, Ujwal Bonde,\nStephan Liwicki, and Christopher Zach. Contextnet: Ex-\nploring context and detail for semantic segmentation in\nreal-time. arXiv, 2018.\n[Poudel et al., 2019] Rudra PK Poudel, Stephan Liwicki,\nand Roberto Cipolla. Fast-scnn: fast semantic segmen-\ntation network. arXiv, 2019.\n[Ronneberger et al., 2015] Olaf Ronneberger, Philipp Fis-\ncher, and Thomas Brox. U-net: Convolutional networks\nfor biomedical image segmentation. In MICCAI, 2015.\n[Singh et al., 2018] Ravinder Singh, Kuldeep Singh Nagla,\nJohn Page, and John Page. Multi-data sensor fusion frame-\nwork to detect transparent object for the efÔ¨Åcient mobile\nrobot mapping. International Journal of Intelligent Un-\nmanned Systems, pages 00‚Äì00, 2018.\n[Spataro et al., 2015] R. Spataro, R. Sorbello, S. Tramonte,\nG. Tumminello, M. Giardina, A. Chella, and V . La Bella.\nReaching and grasping a glass of water by locked-in als\npatients through a bci-controlled humanoid robot. Fron-\ntiers in Human Neuroscience, 357:e48‚Äìe49, 2015.\n[Sun et al., 2020] Peize Sun, Yi Jiang, Rufeng Zhang,\nEnze Xie, Jinkun Cao, Xinting Hu, Tao Kong, Ze-\nhuan Yuan, Changhu Wang, and Ping Luo. Transtrack:\nMultiple-object tracking with transformer. arXiv preprint\narXiv:2012.15460, 2020.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In Advances in neural information processing sys-\ntems, pages 5998‚Äì6008, 2017.\n[Wang et al., 2018] Xiaolong Wang, Ross Girshick, Abhinav\nGupta, and Kaiming He. Non-local neural networks. In\nCVPR, 2018.\n[Wang et al., 2019a] Jingdong Wang, Ke Sun, Tianheng\nCheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu,\nYadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep\nhigh-resolution representation learning for visual recogni-\ntion. arXiv, 2019.\n[Wang et al., 2019b] Yu Wang, Quan Zhou, Jia Liu, Jian\nXiong, Guangwei Gao, Xiaofu Wu, and Longin Jan Late-\ncki. Lednet: A lightweight encoder-decoder network for\nreal-time semantic segmentation. In ICIP, 2019.\n[Wang et al., 2020] Yuqing Wang, Zhaoliang Xu, Xinlong\nWang, Chunhua Shen, Baoshan Cheng, Hao Shen, and\nHuaxia Xia. End-to-end video instance segmentation with\ntransformers. arXiv preprint arXiv:2011.14503, 2020.\n[Xie et al., 2020] Enze Xie, Wenjia Wang, Wenhai Wang,\nMingyu Ding, Chunhua Shen, and Ping Luo. Seg-\nmenting transparent objects in the wild. arXiv preprint\narXiv:2003.13948, 2020.\n[Xu et al., 2015] Yichao Xu, Hajime Nagahara, Atsushi Shi-\nmada, and Rin-ichiro Taniguchi. Transcut: Transparent\nobject segmentation from a light-Ô¨Åeld image. In ICCV,\n2015.\n[Yang et al., 2018] Maoke Yang, Kun Yu, Chi Zhang, Zhi-\nwei Li, and Kuiyuan Yang. Denseaspp for semantic seg-\nmentation in street scenes. In CVPR, 2018.\n[Yu et al., 2018] Changqian Yu, Jingbo Wang, Chao Peng,\nChangxin Gao, Gang Yu, and Nong Sang. Bisenet: Bi-\nlateral segmentation network for real-time semantic seg-\nmentation. In ECCV, 2018.\n[Yuan and Wang, 2018] Yuhui Yuan and Jingdong Wang.\nOcnet: Object context network for scene parsing. arXiv,\n2018.\n[Zhao et al., 2017] Hengshuang Zhao, Jianping Shi, Xiao-\njuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene\nparsing network. In CVPR, 2017.\n[Zhao et al., 2018] Hengshuang Zhao, Xiaojuan Qi, Xiaoy-\nong Shen, Jianping Shi, and Jiaya Jia. Icnet for real-\ntime semantic segmentation on high-resolution images. In\nECCV, 2018.\n[Zheng et al., 2015] Shuai Zheng, Sadeep Jayasumana,\nBernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su,\nDalong Du, Chang Huang, and Philip HS Torr. Con-\nditional random Ô¨Åelds as recurrent neural networks. In\nICCV, 2015.\n[Zheng et al., 2020] Sixiao Zheng, Jiachen Lu, Hengshuang\nZhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\nFu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al.\nRethinking semantic segmentation from a sequence-to-\nsequence perspective with transformers. arXiv preprint\narXiv:2012.15840, 2020.\n[Zhou et al., 2017] Bolei Zhou, Hang Zhao, Xavier Puig,\nSanja Fidler, Adela Barriuso, and Antonio Torralba. Scene\nparsing through ade20k dataset. 2017.\n[Zhou et al., 2018] Zheming Zhou, Zhiqiang Sui, and\nOdest Chadwicke Jenkins. Plenoptic monte carlo object\nlocalization for robot grasping under layered translucency.\nIn 2018 IEEE/RSJ International Conference on Intelligent\nRobots and Systems (IROS), pages 1‚Äì8. IEEE, 2018.\n[Zhu et al., 2020] Xizhou Zhu, Weijie Su, Lewei Lu, Bin\nLi, Xiaogang Wang, and Jifeng Dai. Deformable detr:\nDeformable transformers for end-to-end object detection.\narXiv preprint arXiv:2010.04159, 2020.",
  "topic": "Segmentation",
  "concepts": [
    {
      "name": "Segmentation",
      "score": 0.8313357830047607
    },
    {
      "name": "Computer science",
      "score": 0.8124169111251831
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6847416162490845
    },
    {
      "name": "Encoder",
      "score": 0.6594308614730835
    },
    {
      "name": "Market segmentation",
      "score": 0.6223324537277222
    },
    {
      "name": "Transformer",
      "score": 0.543938934803009
    },
    {
      "name": "Scale-space segmentation",
      "score": 0.507824718952179
    },
    {
      "name": "Segmentation-based object categorization",
      "score": 0.4907339811325073
    },
    {
      "name": "Computer vision",
      "score": 0.45659148693084717
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4224499464035034
    },
    {
      "name": "Image segmentation",
      "score": 0.40963611006736755
    },
    {
      "name": "Engineering",
      "score": 0.06287837028503418
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}