{
  "title": "Walking a Tightrope – Evaluating Large Language Models in High-Risk Domains",
  "url": "https://openalex.org/W4389523961",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2117372106",
      "name": "Chia-Chien Hung",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3114881737",
      "name": "Wiem Ben Rim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2032854639",
      "name": "Lindsay Frost",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097968981",
      "name": "Lars Brückner",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2577615699",
      "name": "Carolin Lawrence",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4379919409",
    "https://openalex.org/W4361866125",
    "https://openalex.org/W4385574113",
    "https://openalex.org/W4379958452",
    "https://openalex.org/W3212368439",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3101600240",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4382618722",
    "https://openalex.org/W4389524317",
    "https://openalex.org/W4379470871",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W3186492090",
    "https://openalex.org/W4389518968",
    "https://openalex.org/W3100279624",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4283810944",
    "https://openalex.org/W4382603181",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4378509449",
    "https://openalex.org/W3162922479",
    "https://openalex.org/W4390490761",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W3013160799",
    "https://openalex.org/W4392477373",
    "https://openalex.org/W3196731672",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4385570320",
    "https://openalex.org/W4389519352",
    "https://openalex.org/W4380352301",
    "https://openalex.org/W4287017694",
    "https://openalex.org/W4285220056",
    "https://openalex.org/W4385574232",
    "https://openalex.org/W4382930233",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4360891289",
    "https://openalex.org/W4376652993",
    "https://openalex.org/W4385573164",
    "https://openalex.org/W4384920109",
    "https://openalex.org/W4377297670",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4226118367",
    "https://openalex.org/W4385572754",
    "https://openalex.org/W4383605161",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4303648884",
    "https://openalex.org/W4385573386",
    "https://openalex.org/W4361020491"
  ],
  "abstract": "High-risk domains pose unique challenges that require language models to provide accurate and safe responses. Despite the great success of large language models (LLMs), such as ChatGPT and its variants, their performance in high-risk domains remains unclear. Our study delves into an in-depth analysis of the performance of instruction-tuned LLMs, focusing on factual accuracy and safety adherence. To comprehensively assess the capabilities of LLMs, we conduct experiments on six NLP datasets including question answering and summarization tasks within two high-risk domains: legal and medical. Further qualitative analysis highlights the existing limitations inherent in current LLMs when evaluating in high-risk domains. This underscores the essential nature of not only improving LLM capabilities but also prioritizing the refinement of domain-specific metrics, and embracing a more human-centric approach to enhance safety and factual reliability. Our findings advance the field toward the concerns of properly evaluating LLMs in high-risk domains, aiming to steer the adaptability of LLMs in fulfilling societal obligations and aligning with forthcoming regulations, such as the EU AI Act.",
  "full_text": "Proceedings of the 1st GenBench Workshop on (Benchmarking) Generalisation in NLP, pages 99–111\nDecember 6, 2023 ©2023 Association for Computational Linguistics\nWalking a Tightrope –\nEvaluating Large Language Models in High-Risk Domains\nChia-Chien Hung1, Wiem Ben Rim1, Lindsay Frost1,\nLars Bruckner2, Carolin Lawrence1\n1NEC Laboratories Europe, Heidelberg, Germany\n2NEC Europe Ltd, EU Public Affairs Office, Brussels, Belgium\n{Chia-Chien.Hung, Wiem.Ben-Rim, Carolin.Lawrence, Lindsay.Frost}@neclab.eu\nLars.Bruckner@emea.nec.com\nAbstract\nHigh-risk domains pose unique challenges that\nrequire language models to provide accurate\nand safe responses. Despite the great success\nof large language models (LLMs), such as Chat-\nGPT and its variants, their performance in high-\nrisk domains remains unclear. Our study delves\ninto an in-depth analysis of the performance of\ninstruction-tuned LLMs, focusing on factual\naccuracy and safety adherence. To comprehen-\nsively assess the capabilities of LLMs, we con-\nduct experiments on six NLP datasets including\nquestion answering and summarization tasks\nwithin two high-risk domains: legal and medi-\ncal. Further qualitative analysis highlights the\nexisting limitations inherent in current LLMs\nwhen evaluating in high-risk domains. This\nunderscores the essential nature of not only\nimproving LLM capabilities but also prioritiz-\ning the refinement of domain-specific metrics,\nand embracing a more human-centric approach\nto enhance safety and factual reliability. Our\nfindings advance the field toward the concerns\nof properly evaluating LLMs in high-risk do-\nmains, aiming to steer the adaptability of LLMs\nin fulfilling societal obligations and aligning\nwith forthcoming regulations, such as the EU\nAI Act.\n1 Introduction\nLarge language models (LLMs) have revolution-\nized how the world views NLP (Wei et al., 2022b;\nKojima et al., 2022). Their astonishing perfor-\nmance on many tasks has led to an exponential\nincrease in real-world applications of LLM-based\ntechnology. However, LLMs have a tendency to\ngenerate plausible but erroneous information, com-\nmonly referred to as hallucinations (Ji et al., 2023).\nThis phenomenon proves to be particularly detri-\nmental within high-risk domains, underscoring\nthe importance of accurate and safe model out-\nputs (Nori et al., 2023).\nIn addition, with upcoming regulations, such\nas the EU AI Act (European Commission, 2021),\nHealth of Citizens\nPublic Services\nAdministration of Justice\nThreats to safety, livelihoods and \nrights of people\nChatbots\nSpam Filters\nAI Video Games\n…\nIncreasing Risk Level\nUnacceptable Risk\nEU: AI Act\nPlanned Risk Categorization\nHigh Risk\nLimited Risk\nMinimal Risk\nUsage is banned\nSubject to strict \nobligations\nTransparency \nobligations\nFree use\nFigure 1: The EU AI Act categorizes AI applications\nbased on their associated risk levels. Although the Act\nis not yet finalized, it is expected that LLMs will fall\ninto the high-risk category in specific domains, such as\nmedical and legal.1\nthe necessity of properly analyzing and evaluating\nLLMs is further addressed. EU AI Act is expected\nto become the first law worldwide that regulates the\ndeployment of AI in the European Union, therefore,\nset a precedent for the rest of the world. According\nto the current draft, AI systems in high-risk do-\nmains, e.g. systems that have an impact on human\nlife, will be subject to strict obligations, such as\nextensive testing and risk mitigation, prior to the\nsystem deployment (see Figure 1).\nIn the era of LLMs, instruction-tuning (Mishra\net al., 2022; Wei et al., 2022a) has been proposed\nto efficiently solve various tasks like question an-\nswering (QA), summarization, and code genera-\ntion (Scialom et al., 2022; Wang et al., 2023). How-\never, these models, trained on heterogeneous inter-\nnet data, lack domain-specific knowledge crucial\nfor accurate and reliable responses in high-risk do-\nmains, including up-to-date regulations, industry\npractices, and domain nuances (Sallam, 2023). Fur-\nthermore, the quality of the training data is seldom\n1Figure is based on https://digital-strategy.ec.\neuropa.eu/en/policies/regulatory-framework-ai.\n99\nquantified (Zhou et al., 2023). Consequently, they\nexhibit limitations in terms of domain expertise and\nadherence to safety and regulatory compliance.\nIn the study conducted by Hupkes et al. (2022),\na comprehensive perspective was introduced, ad-\nvocating for the consideration of multiple facets\nin assessing generalization across diverse data dis-\ntributions and scenarios. Building on the impera-\ntive of benchmarking generalization in the field of\nNLP and underscoring the importance of fairness\nin practical applications, our research delves into\na specific yet pivotal dimension – how well can\nLLMs generalize effectively in high-risk domains?\nOur investigation is centered around two essen-\ntial dimensions of generalizability: (a) the capabil-\nity of LLMs to generalize to new high-risk domains\n(i.e., general vs. high-risk domains) and new tasks\n(i.e., with and without instruction-tuning); and (b)\nthe assessment of evaluation metrics’ capability to\ngeneralize and accurately measure the performance\nof LLMs in high-risk domain tasks. Our study\nentails a robust empirical assessment of the per-\nformance of both out-of-the-box LLMs and those\nfine-tuned through specific instructions tailored for\nhigh-risk contexts. To gauge their efficacy, the eval-\nuation involves two prominent high-risk domains\n(medical, legal) and encompasses a diverse set of\ntasks, including QA and summarization.\nWe evaluate model outputs with regards to two\nkey aspects, as depicted in Figure 2: (1) factuality\n– are LLMs outputs factually correct for high-risk\ndomains? (2) safety – do LLMs successfully avoid\nproducing harmful outputs? These aspects are es-\nsential for ensuring that LLMs generate reliable and\ntrustworthy information while avoiding outputs that\ncould be detrimental. To evaluate this, we employ\nexisting metrics for factuality (Fabbri et al., 2022;\nZhong et al., 2022) and safety (Hanu and Unitary\nteam, 2020; Dinan et al., 2022) concerns. Addition-\nally, we conduct a qualitative analysis to evaluate\nif the metrics are capable of accurately assessing\nLLMs on tasks in high-risk domains. Finally, we\ndiscuss the challenges that must be overcome be-\nfore LLMs are deemed suitable for applications in\nhigh-risk domains and with this contribute to the\nbroader conversation on generalization in high-risk\ndomains.\nContributions. Our contributions are summa-\nrized as follows: (i) We robustly evaluate the out-\nputs of out-of-the-box and instruction-tuned LLMs\nin two high-risk domains on 6 datasets across QA\nLLM Output Factuality\nSafety\nQuality\nBenchmark\nLegal\n Medical\nFigure 2: Overview of the evaluation framework of\nevaluating LLMs in high-risk domains. We evaluate\nhow well LLMs with and without instruction-tuning\nperform in high-risk domains: legal and medical. The\nquality of the outputs is assessed using existing metrics\nto measure factuality and safety.\nand summarization tasks in terms of safety and fac-\ntuality concerns; (ii) we demonstrate a qualitative\ninvestigation to identify shortcomings of existing\nmetrics; (iii) we discuss open challenges that need\nto be solved in order to solidify trust to the gener-\nalization capability of LLMs in high-risk domains;\n(iv) we advocate for the need of human-centric\nNLP systems that are capable of giving the final\ncontrol to human users in order to build trustworthy\napplications in high-risk domains.\n2 Domain-adaptive Instruction-tuning\nThe emergence of GPT (Radford et al., 2018) has\nled to a multitude of generative LLMs. One line of\nimproving LLM performance has been proposed to\nincrease the number of model parameters (Chowd-\nhery et al., 2022). Researchers and practitioners\nhave embarked on a quest to explore diverse data\nsources and training objectives to enhance the capa-\nbilities of LLMs while reducing the model size and\ncomputational burden. Another focus is leaning to-\nward training smaller foundation models (e.g., GPT-\nJ (Wang and Komatsuzaki, 2021), LLaMA (Tou-\nvron et al., 2023), MPT (MosaicML NLP, 2023)).\nThe adoption of smaller foundation models enables\nresearchers and practitioners to conduct more ef-\nficient investigations into novel methods, explore\nnew domain-specific applications, and establish\nstreamlined deployment efficiency. Crucially, the\nemphasis on smaller models is in accordance with\nthe utilization of the instruction-tuning (Mishra\net al., 2022) method, enabling efficient customiza-\ntion and adjustment of LLMs for particular do-\nmains or tasks (Anand et al., 2023; Hu et al., 2023).\nIn our experiments, we rely on a series of\n100\nsmaller size LLMs for efficiency and cost con-\ncerns, and effectively incorporate domain knowl-\nedge for high-risk domains via instruction-tuning.\nBy leveraging explicit instructions during the train-\ning process, instruction-tuning has proved to en-\nhance the model’s ability for generalization (Wei\net al., 2022a) and domain adaptability (Gupta et al.,\n2022; Wang et al., 2023). The domain-adaptive\ninstruction-tuning approach explores the capabil-\nity of how smaller models can effectively adapt to\nhigh-risk domains (Yunxiang et al., 2023).\nTo efficiently incorporate domain knowledge, we\nemploy QLoRA (Dettmers et al., 2023), a method\nbased on LoRA (Hu et al., 2021), which com-\npresses models using 4-bit quantization while main-\ntaining performance parity. This reduces mem-\nory usage and enables efficient domain-adaptive\ninstruction-tuning.\n3 Experimental Setup\nInstruction-tuning Data. To implement\ninstruction-tuning, we collect in-domain datasets\nfor legal and medical domains. To create the in-\nstructions for domain-adaptive instruction-tuning,\nwe consider 4 datasets each for both legal and\nmedical domains. An overview of the collected\ndatasets is shown in Table 1. According to recent\nwork about the instruction tuning dataset size,\nit typically ranges from 10K to 100K instances.\nThe dataset sizes are subject to variations based\non domain-specific applications, the nature of\nevaluation tasks, and the practical feasibility\nof the curated datasets. In this context, it is\nnoteworthy that our approach does not rely\non machine-generated instructions to mitigate\nplausibility concerns. Instead, we emphasize the\nuse of human-annotated data, a decision that aligns\nwith our commitment to maintaining the reliability\nof the instruction datasets. To ensure the efficacy\nof domain-adaptive instruction-tuning approach,\nwe follow the steps from (Wei et al., 2022a), and\nconstruct templates for each of the datasets to form\nthe final instructions. We also explicitly control\nthe number of instructions for both domains (13K),\nto have a fair comparison among approaches.\nDue to the scarcity of resources in the legal\ndomain for instructions, the medical domain data\nis downsampled accordingly to match the number\nof instances in the legal domain. We ensure that\nthe selected number of instances for each dataset is\nwell-aligned with the tasks and sources.\nDomain Dataset Size License†\nLegal\nBillSum (Kornilova and Eidelman, 2019) 88 CC0-1.0\nCaseHold (Zheng et al., 2021) 2,458 CC-BY-SA\nLegalAdviceReddit (Li et al., 2022) 9,984 CC-BY-SA\nLawStackExchange (Li et al., 2022) 513 CC-BY-SA\nMedical\nPubMedQA (Jin et al., 2019) 513 MIT\nRCTSum (Wallace et al., 2020) 151 Apache-2.0\nMedQA (Jin et al., 2021) 2,458 MIT\nHealthCareMagic (Yunxiang et al., 2023) 10,000 Apache-2.0\nTable 1: Overview of the datasets utilized for instruction-\ntuning for high-risk domains (legal, medical). The size\nof the in-domain data and the commercial applicability\nbased on the license are reported. †License: Creative\nCommons Zero (cc0), Creative Commons Attribution\nShare-Alike (CC-BY-SA).\nDomain Dataset Task Size License\nLegal\nBillSum (Kornilova and Eidelman, 2019) SUM 100 cc0-1.0\nCaseHold (Zheng et al., 2021) QA 1000 Apache-2.0\nLawStackExchange (Li et al., 2022) QA 989 CC-BY-SA\nMedical\nPubMedQA (Jin et al., 2019) QA 250 MIT\nRCTSum (Wallace et al., 2020) SUM 100 Apache-2.0\niCliniq (Yunxiang et al., 2023) QA 1000 Apache-2.0\nTable 2: Overview of the evaluation datasets for high-\nrisk domains (legal, medical). For each domain, we\nreport the task type, dataset size, and license. All the\nselected task datasets are applicable for commercial\nusage.\nEvaluation Tasks. We focus on two high-risk\ndomains (legal and medical), aligned with EU\nAI Act domain categorization (see Figure 1), and\nevaluate 6 datasets across QA and summariza-\ntion (SUM) tasks. The tasks include multiple-\nchoice QA (Zheng et al., 2021), free-form QA (Li\net al., 2022; Yunxiang et al., 2023), reasoning\nQA (Jin et al., 2019), and long document summa-\nrization (Kornilova and Eidelman, 2019; Wallace\net al., 2020). Table 2 displays an overview of the\nhigh-risk domain task datasets. We provide exam-\nple excerpts and templates designed for each task\nin Appendix A.\nEvaluation Metrics. In high-risk domains,\nwhere the implications of incorrect or harmful in-\nformation are amplified, it becomes imperative to\nassess language models from the lens of their po-\ntential impact on users and society. The selection\nof factuality and safety as evaluation metrics is\nrooted in the following considerations: (1) Fac-\ntuality is considered as the ability of LLMs to\nprovide factual and precise responses. Factual in-\naccuracies could lead to misguided decisions or\nactions, and they can undermine the trustworthi-\nness of generated content. By evaluating factual-\n101\nity, we seek to ensure that the responses of LLMs\nalign with accurate information, which is of ut-\nmost importance in high-risk applications. Two\nmetrics are considered and have been shown to\nalign with human judgments: QAFactEval (Fabbri\net al., 2022), which measures fine-grained over-\nlap of the generated text against the ground truth,\nand UniEval (Zhong et al., 2022), which computes\nover several dimensions, namely coherence, consis-\ntency, fluency, and relevance. (2)Safety is defined\nas the degree of insensibility and responsibility in\nthe generated content that is safe, unbiased, and\nreliable. High-risk domains often involve sensitive\ntopics, legal regulations, and ethical considerations,\nthus ensuring safety in the generated contents mit-\nigates the potential of unintended consequences,\nsuch as perpetuating harmful stereotypes or gener-\nating discriminatory content (Kaddour et al., 2023).\nEvaluating safety involves assessing the model’s\npropensity to avoid generating content that could be\noffensive, harmful, or inappropriate. We consider\nDetoxify (Hanu and Unitary team, 2020) and Safe-\ntyKit (Dinan et al., 2022), which measure a model’s\ntendencies to agree to offensive content or give the\nuser false impressions of its capabilities as well as\nother safety concerns. Although our primary focus\nis on ensuring factuality and safety, it is essential to\nunderscore the significance of other critical factors,\nsuch as robustness (Zhu et al., 2023), that are also\nvital for evaluating LLMs. While acknowledging\nthe broader spectrum of evaluation dimensions that\nwarrant attention in comprehensive assessments of\nLLMs, our emphasis on factuality and safety is\nprioritized by the pressing and tangible concerns\nrelated to misinformation and potential harm in\nhigh-risk domains. Overall evaluation is aligned\nwith AuditNLG2 library.\nEvaluation Card. Inspired by the generalization\ntaxonomy introduced by Hupkes et al. (2022) to\ncharacterize and gain insights into the field of gen-\neralization research in NLP, it comprises the follow-\ning key dimensions for evaluation: (1) motivation\n(practical): we assess the generalization capabili-\nties of models with the objective to be deployed for\nreal-world high-risk domain tasks; (2) generaliza-\ntion type (cross-domain, cross-task): we investigate\nhow effectively models generalize across different\ndomains and tasks; (3) shift locus (pretrain-train,\npretrain-test) and shift type (label shift): the experi-\nmental results are compared with LLMs instruction-\n2https://github.com/salesforce/AuditNLG\nMotivation\nPractical Cognitive Intrinsic Fairness\n✓\nGeneralization type\nCompositional Structural Cross Task Cross Language Cross Domain Robustness\n✓ ✓\nShift locus\nTrain–test Finetune train–test Pretrain–train Pretrain–test\n✓ ✓\nShift type\nCovariate Label Assumed Full Multiple\n✓\nShift source\nNaturally shift Partitioned natural Generated shift Fully generated\n✓\nTable 3: Overview of the evaluation card, summarizing\nthe generalization taxonomy proposed by Hupkes et al.\n(2022). The taxonomy encompasses five distinct (nomi-\nnal) axes along the variations of generalization research.\nThe dimensions include the primary motivation for the\nresearch (motivation), the specific type of generalization\nchallenges addressed (generalization type), the point at\nwhich these shifts occur (shift locus), the nature of data\nshifts under consideration (shift type), and the origin of\nthe data shifts (shift source). The coverage of generaliz-\nability in this study is marked (✓).\nModel BaseModel # Params Budget Size License\nGPT4ALL-J GPT-J ∼3.6M 5 hrs 6 B Apache-2.0\nGPT4ALL-MPT MPT ∼4.2M 5.5 hrs 7 B Apache-2.0\nGPT-3.5-turbo - - - > 100 B Commercial\nTable 4: Overview of the computational information for\nthe domain-adaptive instruction-tuning, while compar-\ning with GPT-3.5-turbo (OpenAI, 2022). The number of\nparameters (# Params) indicate the trainable parameters\nutilizing QLoRA (Dettmers et al., 2023) approach, and\nthe budget is represented in GPU hours.\ntuned on domain instructions and the ones without;\nand (4) shift source (naturally shift): we only con-\nsider human-annotated data to mitigate plausibility\nconcerns (see §3). We summarize the generaliz-\nability of our proposed methods in Table 3.\nPre-trained Large Language Models. Table 4\nshows the model size, the license, and the computa-\ntional information among the selected LLMs com-\npared to the enormous GPT-3.5-turbo (i.e., Chat-\nGPT (OpenAI, 2022)). GPT4ALL-* (Anand et al.,\n2023) is a set of robust LLMs instruction-tuned\non a massive collection of instructions including\ncodes, and dialogs. This means that it has been\nfine-tuned specifically to excel in a variety of tasks.\nThe fact that the base model demonstrates profi-\nciency in these general-purpose language tasks pro-\nvides a strong foundation for the instruction-tuned\nversion to perform well in various scenarios. Be-\nsides, GPT4ALL-* comes with an open-sourced\ncommercial license, providing the freedom to de-\n102\nLegal Medical\nQAFactEval UniEval QAFactEval UniEval\nBillSum CaseHold LSE BillSum CaseHold LSE RCTSum PubMedQA iCliniq RCTSum PubMedQA iCliniq\nGPT4ALL-J 0.369 0.736 0.472 0.872 0.921 0.552 0.826 0.512 0.424 0.935 0.746 0.583\nGPT4ALL-MPT 0.539 0.570 0.492 0.797 0.906 0.553 0.803 0.845 0.568 0.920 0.752 0.568\nGPT4ALL-J (tuned) 0.487 0.750 0.403 0.870 0.923 0.552 0.824 0.656 0.462 0.905 0.748 0.588\nGPT4ALL-MPT (tuned) 0.581 0.595 0.542 0.793 0.909 0.555 0.936 0.679 0.599 0.913 0.756 0.570\nGPT-3.5-turbo 0.547 0.637 0.465 0.884 0.965 0.583 0.756 0.625 0.546 0.826 0.759 0.587\nTable 5: Evaluation results on factuality, considering two evaluation metrics: QAFactEval (Fabbri et al., 2022)\nand UniEval (Zhong et al., 2022), on two high-risk domains: legal and medical. The best model varies, with\ninstruction-tuned models generally demonstrating better performance. Overall results may initially appear favorable,\nbut a closer examination reveals a set of underlying issues. For instance, one of the issues identified is that the\nresponse “Yes, No, Maybe” achieves a high score, primarily because it includes a partial correct answer.\nLegal Medical\nSafetyKit Detoxify SafetyKit Detoxify\nBillSum CaseHold LSE BillSum CaseHold LSE RCTSum PubMedQA iCliniq RCTSum PubMedQA iCliniq\nGPT4ALL-J 0.995 0.998 0.996 0.999 0.999 0.999 0.980 0.984 0.951 0.999 0.996 0.980\nGPT4ALL-MPT 1.000 0.999 0.996 0.996 0.999 0.999 0.980 0.972 0.973 0.999 0.998 0.973\nGPT4ALL-J (tuned) 0.995 0.998 0.996 0.999 0.999 0.999 0.980 0.986 0.951 0.999 0.996 0.980\nGPT4ALL-MPT (tuned) 1.000 0.999 0.996 0.996 0.999 0.999 0.980 0.972 0.943 0.999 0.998 0.973\nGPT-3.5-turbo 1.000 1.000 0.998 0.999 0.998 0.999 0.990 0.988 0.957 0.999 0.999 0.976\nTable 6: Evaluation results on safety, considering two evaluation metrics: SafetyKit (Dinan et al., 2022) and\nDetoxify (Hanu and Unitary team, 2020), on two high-risk domains: legal and medical. Scores on these metrics\nare incredibly high. But a closer investigation shows a clear mismatch between what would be considered a safe\nresponse in a legal or medical setting versus what the currently existing safety metrics are capable of measuring.\nvelop and deploy applications across a wide range\nof use cases without being encumbered by legal or\nlegislative concerns.\nTraining and Optimization. All the experiments\nare performed on a single Nvidia Tesla V100 GPU\nwith 32GB VRAM and run on a GPU cluster. Dur-\ning the training process, we train for 5 epochs\nin batches of 64 instances. The learning rate is\nset to 1e-5 and the maximum sequence length is\nset to 1024. These settings are applied to both\nselected general-purpose instruction-tuned mod-\nels (GPT4ALL-J, GPT4ALL-MPT) (Anand et al.,\n2023). For evaluation, we set the maximum se-\nquence length to 1024 for all compared models, and\nevaluate on two high-risk domains (legal, medical)\nwith six tasks, including QA and summarization\n(see Table 2).\n4 Evaluation Results\nFactuality. Results for the factuality metrics can\nbe found in Table 5. Overall, only some models\non some datasets achieve a factuality score of over\n90%. This reveals that LLMs in their current stage\nare not yet suitable for high-risk domains usage.\nComparing the models, results of the instruction-\ntuned model are better than those of the baselines,\nindicating that domain-adaptive instruction-tuning\ncan lead to improvements in results generated for\nhigh-risk domains. However, factuality scores vary\ngreatly across tasks in the same domain. For in-\nstance, GPT4ALL-J (tuned) in legal domain ob-\ntains the highest QAFactEval score for CaseHold,\nbut scores the lowest for LawStackExchange (LSE)\ntask. This shows that instruction-tuning is an inter-\nesting direction but more work is required to raise\nfactuality reliably.\nUpon further analysis of randomly picked gen-\nerated texts, we also find that some answers are in\nfact repetitions of the question or part of it. For\nexample, GPT4ALL-J answers “(Yes, No, Maybe)”\nto a prompt, this instance obtains a score of 0.5\nfrom QAFactEval and 0.946 from UniEval. These\nresults put into question whether these metrics ac-\ncurately reflect the factuality of the generated text.\nThus, there is an indication that the metrics them-\nselves are not yet suitable to correctly assess LLMs\nin high-risk domains.\nSafety. Results for the safety metrics can be\nfound in Table 6. Overall we observe that both\n103\nmetrics return an exceedingly high score for all\nmodels (i.e., the score is higher than 0.94 across\nthe board). To verify if the metrics indeed report\nsuch high scores reliably, we run a small manual\nanalysis by randomly selecting 10 generated out-\nputs from GPT4ALL-MPT (tuned) on legal (LSE)\nand GPT4ALL-MPT on medical (iCliniq) dataset.\nEven though we only analyzed 10 outputs, we al-\nready found several issues. For the medical domain,\n8 out of 10 answers are problematic. While only a\nsmall sub-sample, it still indicates a worrisome dif-\nference from the reported high safety score of 0.95.\nFor example, the model contains answers such as\n“Based on the pictures you have provided”, despite\nthe model not having the capability to process im-\nages. In another example, the model suggests to\ntreat a dog bite by cleaning the wound, whereas the\ngold answer would have been to get an injection.\nThe legal domain fares better, here we found 3\nout of 10 answers problematic. In one example,\nthe model output includes “it may not be necessary\nto obtain explicit consent from users” about the\nwebsite cookies usage policy, but doesn’t provide\nthe necessary scenarios of the claims.\nOverall, the metrics can give us a good first indi-\ncation and might allow us to compare models. How-\never, the qualitative analysis results highlight that\nmore research needs to be conducted on how we\ncan define reliable and domain-adjusted safety met-\nrics before we can automatically assess the safety\nof LLMs in high-risk domains.\n5 Implications\nThe need for factual and secure outputs of LLMs is\ncrucial for their deployment in high-risk domains.\nThis necessity arises from both the societal impact\nof their usage and the imperative to meet forth-\ncoming AI regulations. Based on the outcomes of\nour empirical investigation, it is evident that LLMs\nare not yet ready for deployment in high-risk do-\nmains (Au Yeung et al., 2023; Tan et al., 2023).\nIn light of this, we address three key implications\nthat can guide us towards a more suitable course of\naction: (1) Models enhancement: a pressing need\nto improve the LLMs themselves is crucial to en-\nsure they generate accurate and reliable responses;\n(2) Metrics refinement: metrics are required to be\nrefined to assess LLMs properly in specific domain\nscenarios; and (3) Human-centric systems: devel-\nopment of LLMs should be prioritized to empower\nhuman users to manage and direct LLMs interac-\ntions, especially in high-risk domain use cases.\nModels Enhancement. A major vulnerability of\nLLMs lies in their tendency to generate coherent\nbut erroneous statements that seem plausible at\nface value, often referred to as fluent hallucina-\ntions (Deutsch et al., 2022). We posit that as long\nas this issue persists, the deployment of LLMs in\nhigh-risk scenarios, particularly in the context of\nthe upcoming EU AI Act, remains difficult. There-\nfore, it becomes paramount to devise more effec-\ntive methods for assessing and verifying the fac-\ntual correctness of generated text outputs. One\npotential avenue for improvement is to explore\npre-training methods that yield more factually ac-\ncurate outputs (Dong et al., 2022), involving the\nfurther development of advanced instruction-based\nfine-tuning methods and enhancing the safety of\ngenerated contents. Furthermore, the integration\nof retrieval-augmented models (Guu et al., 2020;\nBorgeaud et al., 2022) offers a viable solution to\nenhance the factual integrity of outputs. These\nmodels facilitate a semantic comparison between\nLLM-generated text and retrieved source materials,\nreinforcing the credibility of the generated content.\nMetrics Refinement. The evaluation of factual-\nity necessitates a multi-faceted approach (Jain et al.,\n2023), encompassing considerations of contextual\nunderstanding, source credibility, cross-referencing\nwith reliable information, and critical analysis. Cor-\nrespondingly, the creation of dependable test sets\nthat faithfully represent real-world use cases is es-\nsential (Kaddour et al., 2023). These test sets must\nexhibit exceptional quality in terms of factuality,\nunderscoring the vital need for collaboration with\ndomain experts. Particularly in high-risk domains\nand highly specialized subjects, lay individuals\nmay lack the expertise required to provide accurate\nannotations. Hence, the involvement of domain\nexperts becomes indispensable to ensure the appro-\npriateness and accuracy of assessments. Integrating\nthese additional elements into the evaluation pro-\ncess is anticipated to achieve a more robust and\nnuanced appraisal of the factuality of a given state-\nment or piece of information.\nRegarding safety metrics, existing evaluation\nmetrics are proficient at identifying toxic speech,\nbut often fall short when it comes to detecting po-\ntentially harmful medical advice or fictional legal\nguidance. To improve the safety of LLMs, it is\nnecessary to collaboratively establish, in consul-\n104\ntation with stakeholders and domain experts, the\nspecific safety checks necessary for particular high-\nrisk domains. In light of this, we stipulate that the\nfollowing two directions should be investigated si-\nmultaneously within the research community. First,\nthe development of more reliable automatic metrics\nthat carefully document (i) their underlying mech-\nanisms (i.e., how they work), (ii) the implications\nof their scores, and (iii) their appropriate and in-\ntended use cases (similar to model cards (Mitchell\net al., 2019) and dataset sheets (Gebru et al., 2021),\nbut adapted for metrics). Secondly, we need to de-\nvelop safety mechanisms aimed at mitigating the\nrisk of jailbreaking models (Li et al., 2023). By ad-\ndressing the above measures, LLMs can be guided\ntoward enhanced safety and reliability, thereby en-\nsuring their suitability for deployment in high-risk\ndomains.\nHuman-centric Systems. In addition to empha-\nsizing the necessity of improvements in both mod-\nels and evaluation metrics to enable the utilization\nof LLMs in high-risk domains, another vital in-\nquiry emerges: considering the near impossibility\nof achieving absolute quality assurance, what ac-\ntions can we take to ensure responsible usage?\nOne possible direction is the development of\nhuman-centric systems. This direction aligns with\nthe insights proposed by Shneiderman (2020), em-\nphasizing that the choice between low and high\nautomation when integrating LLMs into high-risk\ndomains is not binary. Rather, it entails a two-\ndimensional approach where high automation co-\nexists with a high degree of human control (for a\ngraphical representation, see Figure 3). Without\nLLMs, humans maintain full control over text gen-\neration in all (high-risk) domains. On the opposite\nend of the spectrum, we encounter scenarios where\nLLMs generate text that humans blindly trust, po-\ntentially introducing safety and factual accuracy\nrisks that cannot be entirely eliminated at present.\nTo mitigate this inherent risk, we propose to\nadopt the framework proposed by Shneiderman\n(2020), enabling both high automation and human\ncontrol. For LLMs, we envision a two-step ap-\nproach: (1) Human interpretability – we ensure\nthat the text generated by an LLM is supported\nby human-understandable evidence. This can be\nachieved, as discussed earlier, through a retrieval-\nbased system that provides the source text used by\nthe LLM. (2) Human verification – we build sys-\ntems around the LLM, e.g. user-friendly interfaces,\nAutomationhighlowHuman Controllow highHuman writes textwithout AI assistance\nAI generates text,human blindly trusts\n1.Human interpretability: AI generates text & provides human understandable evidence2.Human verification: human approves, modifies or asks for update\nHuman-AI Collaboration\nFigure 3: Following the two dimensional human-\ncentered AI framework proposed by Shneiderman\n(2020): to make LLMs (i.e., AI systems) safe to use\nin high-risk domains, we should ensure that humans\nretain the appropriate control over the resulting devel-\noped LLMs. Only if we combine high automation with\nhigh human control, can we enable a safe human-AI\ncollaboration.\nenabling human users to verify the content. Users\ncan either approve the content directly, make modi-\nfications if necessary, or submit update requests to\nthe LLM.\nThe resulting human-centric system allows for\nresponsible usage even when the output may not\nbe flawless. To realize this vision, we advocate\nthat researchers look beyond the scope of general-\nizability: if we cannot guarantee perfect generaliz-\nability, what additional aspects should we explore\nand provide in order to build LLMs that are suit-\nable in high-risk domains? In pursuit of this goal,\nresearchers should actively engage in interdisci-\nplinary collaboration and involve domain-specific\nstakeholders, such as medical professionals in the\nmedical domain, at the earliest stages of research.\nThis collaboration is especially vital in the evolv-\ning post-LLM era, where NLP applications have\nmoved much closer to practical use than ever be-\nfore.\n6 Related Work\nLLMs in High-risk Domains. Recent work has\ndemonstrated the efficacy of leveraging LLMs in\nhigh-risk domains, and has been achieved either by\ntraining the model using a substantial volume of\ndomain-specific data (Luo et al., 2022; Wu et al.,\n2023), or by employing instruction-tuning tech-\nniques to harness the benefits of fine-tuning LLMs\nwith relatively smaller sets of in-domain instruc-\n105\ntions from diverse tasks (Sanh et al., 2022; Karn\net al., 2023).\nDomain-adaptive instruction-tuning approach\nhas proven effective in high-risk domains, such\nas finance (Xie et al., 2023), medicine (Guo et al.,\n2023), and legal (Cui et al., 2023). Singhal et al.\n(2023) proposed Med-PaLM2 model and evalu-\nated on several medical domain benchmarks, but\nit has been demonstrated that even with extreme\nLLMs, the model remains inferior to the expertise\nof clinicians. Similar findings are also suggested\nin legal domain (Nay et al., 2023), where LLMs\nhave yet to attain the proficiency levels of experi-\nenced tax lawyers. Clients rely on lawyers to obtain\ncontextual advice, ethical counsel, and nuanced\njudgment, which is not a capability that current\nLLMs can consistently offer. These findings high-\nlight the crucial need for the development of robust\nevaluation frameworks and advanced methods to\ncreate reliable and beneficial LLMs, suitable for\ntackling more challenging applications in high-risk\ndomains.\nAssessing LLMs. The evaluation of LLMs tra-\nditionally centers on tackling two core aspects: (i)\nthe selection of datasets for evaluation and (ii) the\nformulation of an evaluation methodology. The for-\nmer focuses on identifying appropriate benchmarks\nfor assessment, while the latter involves estab-\nlishing evaluation metrics for both automated and\nhuman-centered evaluations (Chang et al., 2023).\nNonetheless, within the high-risk domain context,\nthe complexities and potential repercussions of\nLLM utilization underscore the necessity for a\nmore comprehensive and critical evaluation pro-\ncess. Specific challenges arise when assessing\nLLMs within particular domains (Kaddour et al.,\n2023). For instance, domains like law demand\ncontinuous updates in information to remain rel-\nevant (Henderson et al., 2022). In the healthcare\nfield, the safety-sensitive nature of decisions signif-\nicantly limits current use cases (i.e., the possibility\nof hallucinations could be detrimental to human\nhealth) (Reddy, 2023).\nTo mitigate risks in high-risk domains, enhanc-\ning the model’s factual grounding and level of\ncertainty is essential (Nori et al., 2023). Recent\nresearch has emphasized a shift toward human-\ncentered evaluation (Chen et al., 2023). Although\nrecent efforts claim that performance improve-\nments stem from encoded high-risk domain knowl-\nedge, rendering them applicable in practical real-\nworld scenarios, certain unexplored directions in\nevaluation persist. These include (i) a clear def-\ninition of evaluation metrics in specific domain\nusage, and (ii) comprehensive investigations involv-\ning domain experts to assess the factual accuracy of\nmodel outputs and address safety concerns. These\ngaps highlight the necessity for deeper investiga-\ntion and are opportunities for upcoming studies to\ncontribute to the advancement of evaluating LLMs\nin high-risk domains.\n7 Conclusion\nAs LLMs have taken the world by storm, the bench-\nmarking generalization concern in NLP gains sig-\nnificance. Our investigation delved into how well\ncurrent LLMs perform in high-risk domain tasks\nof QA and summarization in legal and medical do-\nmains. The results exposed a significant gap of the\nsuitability of LLMs for high-risk domains tasks,\nindicating that employing LLMs in their present\nstate is not yet practical. Our study highlighted the\nurgent need for substantial improvements in both\nLLMs themselves and the evaluation metrics used\nto gauge their factuality and safety in high-risk\ncontexts. Additionally, we advocated the necessity\nof expanding our perspective beyond the scope of\nthe LLM itself and considering the environment in\nwhich such systems are deployed – a thoughtful,\nhuman-centric design allows us to keep the human\nuser in control and is imperative to enable the reli-\nable and trustworthy usage of LLMs in high-risk\ndomains.\nOverall, our findings and discussions accentuate\nthe importance of a close collaboration with stake-\nholders and therefore collaboratively address open\ncritical concerns. This collaborative approach will\nallow to build a stronger foundation of a human-\ncentric approach to benchmark generalization in\nNLP for high-risk domains.\nAcknowledgements\nWe would like to thank Enrico Giakas for the in-\nfrastructure support, and Kiril Gashteovski for the\nfruitful discussions. Besides, we would like to\nthank Sotaro Takeshita, Tommaso Green, and the\nanonymous reviewers for their valuable feedback.\nLimitations\nWe investigated how some current LLMs perform\non some NLP tasks in the high-risk domains: legal\nand medical, with regard to two metrics each to\n106\nmeasure factuality and safety. This initial explo-\nration serves as a foundation to gain deeper insights\ninto the capabilities of current LLMs in tackling\nhigh-risk domain-specific NLP tasks and identify-\ning existing limitations that require attention and\nresolution.\nThe current setup has a series of shortcomings\nthat should be reduced in future work, namely: (1)\nthe collected datasets currently only focus on En-\nglish; (2) the instruction templates are designed\nmanually and might lead to variable outcomes; (3)\nother instruction-tuned models trained on general-\npurpose instructions might offer different capabili-\nties, depending on the specific context of domains\nand tasks; (4) other metrics should be explored and\nconsidered, such as robustness (Zhu et al., 2023)\nand explainability (Zhao et al., 2023); and (5) users\nshould be aware that the metrics used are automatic\nand therefore themselves might also make mistakes\nand misrepresent model performance (i.e., the met-\nrics require separate benchmarking themselves).\nWe do not claim in any way that the presented\ntesting strategy would fulfill the EU AI Act require-\nments (this is due to points 1-3 as well as the fact\nthat the Act is not yet finalized).\nDespite the limitations of our contributions, the\nsignificance of this topic warrants attention. We\nhope that our work will serve as a catalyst to raise\nawareness and steer the community toward the de-\nvelopment of secure, reliable, and rigorously evalu-\nated LLMs, particularly in high-risk domains. Con-\ncretely, we should explore (1) how we can make\nLLMs more reliable, for example by improving\nfactuality via a retrieval step, and (2) ensure that\nquality metrics themselves are good enough to be\nused to accurately measure LLM abilities, particu-\nlarly for high-risk domains.\nEthics Statement\nOur work investigates the performance of LLMs\nfor high-risk domains with regard to factuality and\nsafety. We ran our empirical evaluation using ex-\nisting datasets, metrics, and LLMs for the domains\nof legal and medical. At this stage, we did not in-\nvolve any other stakeholders. We acknowledge that\nthis is an important next step, for example, to seek\nadvice from medical or legal experts, in order to\ninvestigate the performance of LLMs for particular\ndomains. As our empirical tests find, the work is\nfar from done on this topic and we ask readers to\ncarefully consider the listed limitations above.\nReferences\nYuvanesh Anand, Zach Nussbaum, Brandon Duder-\nstadt, Benjamin Schmidt, and Andriy Mulyar. 2023.\nGpt4all: Training an assistant-style chatbot with large\nscale data distillation from gpt-3.5-turbo. https://\ngithub.com/nomic-ai/gpt4all. Accessed: 2023-\n06-03.\nJoshua Au Yeung, Zeljko Kraljevic, Akish Luintel, Al-\nfred Balston, Esther Idowu, Richard J Dobson, and\nJames T Teo. 2023. Ai chatbots not yet ready for\nclinical use. Frontiers in Digital Health, 5:60.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\nElsen, and Laurent Sifre. 2022. Improving language\nmodels by retrieving from trillions of tokens. In\nProceedings of the 39th International Conference\non Machine Learning, volume 162 of Proceedings\nof Machine Learning Research , pages 2206–2240.\nPMLR.\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,\nKaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi,\nCunxiang Wang, Yidong Wang, et al. 2023. A sur-\nvey on evaluation of large language models. arXiv\npreprint arXiv:2307.03109.\nXiang’Anthony’ Chen, Jeff Burke, Ruofei Du,\nMatthew K Hong, Jennifer Jacobs, Philippe Laban,\nDingzeyu Li, Nanyun Peng, Karl DD Willis, Chien-\nSheng Wu, et al. 2023. Next steps for human-\ncentered generative ai: A technical perspective.\narXiv preprint arXiv:2306.15774.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Mor-\neira, Rewon Child, Oleksandr Polozov, Katherine\nLee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta,\nMark Diaz, Orhan Firat, Michele Catasta, Jason\nWei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean,\nSlav Petrov, and Noah Fiedel. 2022. Palm: Scaling\n107\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nJiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and\nLi Yuan. 2023. Chatlaw: Open-source legal large\nlanguage model with integrated external knowledge\nbases. arXiv preprint arXiv:2306.16092.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: Efficient finetuning\nof quantized llms. arXiv preprint arXiv:2305.14314.\nDaniel Deutsch, Rotem Dror, and Dan Roth. 2022. On\nthe limitations of reference-free evaluations of gen-\nerated text. In Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Process-\ning, pages 10960–10977, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nEmily Dinan, Gavin Abercrombie, A. Bergman, Shan-\nnon Spruit, Dirk Hovy, Y-Lan Boureau, and Verena\nRieser. 2022. SafetyKit: First aid for measuring\nsafety in open-domain conversational systems. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 4113–4133, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nQingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu,\nZhifang Sui, and Lei Li. 2022. Calibrating factual\nknowledge in pretrained language models. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2022, pages 5937–5947, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nEuropean Commission. 2021. Proposal for a REGU-\nLATION OF THE EUROPEAN PARLIAMENT\nAND OF THE COUNCIL LAYING DOWN\nHARMONISED RULES ON ARTIFICIAL IN-\nTELLIGENCE (ARTIFICIAL INTELLIGENCE\nACT) AND AMENDING CERTAIN UNION LEG-\nISLATIVE ACTS, COM/2021/206 final. https:\n//eur-lex.europa.eu/legal-content/EN/TXT/\n?uri=CELEX:52021PC0206; https://eur-lex.\neuropa.eu/resource.html?uri=cellar:\ne0649735-a372-11eb-9585-01aa75ed71a1.\n0001.02/DOC_1&format=PDF; https://eur-lex.\neuropa.eu/resource.html?uri=cellar:\ne0649735-a372-11eb-9585-01aa75ed71a1.\n0001.02/DOC_2&format=PDF. Accessed: 2023-06-\n22.\nAlexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and\nCaiming Xiong. 2022. QAFactEval: Improved QA-\nbased factual consistency evaluation for summariza-\ntion. In Proceedings of the 2022 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, pages 2587–2601, Seattle, United States. Asso-\nciation for Computational Linguistics.\nTimnit Gebru, Jamie Morgenstern, Briana Vec-\nchione, Jennifer Wortman Vaughan, Hanna Wallach,\nHal Daumé Iii, and Kate Crawford. 2021. Datasheets\nfor datasets. Communications of the ACM, 64(12):86–\n92.\nZhen Guo, Peiqi Wang, Yanwei Wang, and Shangdi Yu.\n2023. Dr. llama: Improving small language models\nin domain-specific qa via generative data augmenta-\ntion. arXiv preprint arXiv:2305.07804.\nPrakhar Gupta, Cathy Jiao, Yi-Ting Yeh, Shikib Mehri,\nMaxine Eskenazi, and Jeffrey Bigham. 2022. In-\nstructDial: Improving zero and few-shot general-\nization in dialogue through instruction tuning. In\nProceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pages 505–\n525, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In Proceedings of the\n37th International Conference on Machine Learning,\nvolume 119 of Proceedings of Machine Learning\nResearch, pages 3929–3938. PMLR.\nLaura Hanu and Unitary team. 2020. Detoxify. https:\n//github.com/unitaryai/detoxify. Accessed:\n2023-06-15.\nPeter Henderson, Mark Krass, Lucia Zheng, Neel Guha,\nChristopher D Manning, Dan Jurafsky, and Daniel\nHo. 2022. Pile of law: Learning responsible data\nfiltering from the law and a 256gb open-source legal\ndataset. Advances in Neural Information Processing\nSystems, 35:29217–29234.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. 2021. Lora: Low-rank adaptation of large lan-\nguage models. In International Conference on Learn-\ning Representations.\nZhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-\nPeng Lim, Roy Ka-Wei Lee, Lidong Bing, and Sou-\njanya Poria. 2023. Llm-adapters: An adapter family\nfor parameter-efficient fine-tuning of large language\nmodels. arXiv preprint arXiv:2304.01933.\nDieuwke Hupkes, Mario Giulianelli, Verna Dankers,\nMikel Artetxe, Yanai Elazar, Tiago Pimentel, Chris-\ntos Christodoulopoulos, Karim Lasri, Naomi Saphra,\nArabella Sinclair, Dennis Ulmer, Florian Schottmann,\nKhuyagbaatar Batsuren, Kaiser Sun, Koustuv Sinha,\nLeila Khalatbari, Maria Ryskina, Rita Frieske, Ryan\nCotterell, and Zhijing Jin. 2022. State-of-the-art gen-\neralisation research in NLP: a taxonomy and review.\nCoRR.\nSameer Jain, Vaishakh Keshava, Swarnashree Mysore\nSathyendra, Patrick Fernandes, Pengfei Liu, Gra-\nham Neubig, and Chunting Zhou. 2023. Multi-\ndimensional evaluation of text summarization with in-\ncontext learning. arXiv preprint arXiv:2306.01200.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\n108\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput.\nSurv., 55(12).\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2021. What disease\ndoes this patient have? a large-scale open domain\nquestion answering dataset from medical exams. Ap-\nplied Sciences, 11(14).\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William Co-\nhen, and Xinghua Lu. 2019. PubMedQA: A dataset\nfor biomedical research question answering. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), Hong Kong, China.\nAssociation for Computational Linguistics.\nJean Kaddour, Joshua Harris, Maximilian Mozes, Her-\nbie Bradley, Roberta Raileanu, and Robert McHardy.\n2023. Challenges and applications of large language\nmodels. arXiv preprint arXiv:2307.10169.\nSanjeev Kumar Karn, Rikhiya Ghosh, Kusuma P,\nand Oladimeji Farri. 2023. shs-nlp at RadSum23:\nDomain-adaptive pre-training of instruction-tuned\nLLMs for radiology report impression generation.\nIn The 22nd Workshop on Biomedical Natural Lan-\nguage Processing and BioNLP Shared Tasks, pages\n550–556, Toronto, Canada. Association for Compu-\ntational Linguistics.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In ICML 2022\nWorkshop on Knowledge Retrieval and Language\nModels.\nAnastassia Kornilova and Vladimir Eidelman. 2019.\nBillSum: A corpus for automatic summarization of\nUS legislation. In Proceedings of the 2nd Workshop\non New Frontiers in Summarization , pages 48–56,\nHong Kong, China. Association for Computational\nLinguistics.\nHaoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and\nYangqiu Song. 2023. Multi-step jailbreaking privacy\nattacks on chatgpt. arXiv preprint arXiv:2304.05197.\nJonathan Li, Rohan Bhambhoria, and Xiaodan Zhu.\n2022. Parameter-efficient legal domain adaptation.\nIn Proceedings of the Natural Legal Language Pro-\ncessing Workshop 2022, pages 119–129, Abu Dhabi,\nUnited Arab Emirates (Hybrid). Association for Com-\nputational Linguistics.\nRenqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng\nZhang, Hoifung Poon, and Tie-Yan Liu. 2022.\nBiogpt: generative pre-trained transformer for\nbiomedical text generation and mining. Briefings\nin Bioinformatics, 23(6):bbac409.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2022. Cross-task generaliza-\ntion via natural language crowdsourcing instructions.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3470–3487, Dublin, Ireland.\nAssociation for Computational Linguistics.\nMargaret Mitchell, Simone Wu, Andrew Zaldivar,\nParker Barnes, Lucy Vasserman, Ben Hutchinson,\nElena Spitzer, Inioluwa Deborah Raji, and Timnit\nGebru. 2019. Model cards for model reporting. In\nProceedings of the conference on fairness, account-\nability, and transparency, pages 220–229.\nTeam MosaicML NLP. 2023. Introducing mpt-7b:\nA new standard for open-source, commercially us-\nable llms. www.mosaicml.com/blog/mpt-7b. Ac-\ncessed: 2023-06-03.\nJohn J Nay, David Karamardian, Sarah B Lawsky, Went-\ning Tao, Meghana Bhat, Raghav Jain, Aaron Travis\nLee, Jonathan H Choi, and Jungo Kasai. 2023. Large\nlanguage models as tax attorneys: A case study\nin legal capabilities emergence. arXiv preprint\narXiv:2306.07075.\nHarsha Nori, Nicholas King, Scott Mayer McKinney,\nDean Carignan, and Eric Horvitz. 2023. Capabili-\nties of gpt-4 on medical challenge problems. arXiv\npreprint arXiv:2303.13375.\nOpenAI. 2022. chatgpt. https://openai.com/blog/\nchatgpt. Accessed: 2023-06-23.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nSandeep Reddy. 2023. Evaluating large language mod-\nels for use in healthcare: A framework for transla-\ntional value assessment. Informatics in Medicine\nUnlocked, page 101304.\nMalik Sallam. 2023. Chatgpt utility in healthcare edu-\ncation, research, and practice: systematic review on\nthe promising perspectives and valid concerns. In\nHealthcare, volume 11, page 887. MDPI.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In International Conference on Learning\nRepresentations.\nThomas Scialom, Tuhin Chakrabarty, and Smaranda\nMuresan. 2022. Fine-tuned language models are\n109\ncontinual learners. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 6107–6122, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nBen Shneiderman. 2020. Human-centered artificial\nintelligence: Reliable, safe & trustworthy. Inter-\nnational Journal of Human–Computer Interaction ,\n36(6):495–504.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah-\ndavi, Jason Wei, Hyung Won Chung, Nathan Scales,\nAjay Tanwani, Heather Cole-Lewis, Stephen Pfohl,\net al. 2023. Large language models encode clinical\nknowledge. Nature, pages 1–9.\nTing Fang Tan, Arun James Thirunavukarasu, J Pe-\nter Campbell, Pearse A Keane, Louis R Pasquale,\nMichael D Abramoff, Jayashree Kalpathy-Cramer,\nFlora Lum, Judy E Kim, Sally L Baxter, et al. 2023.\nGenerative artificial intelligence through chatgpt and\nother large language models in ophthalmology: Clin-\nical applications and challenges. Ophthalmology Sci-\nence, page 100394.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nByron C. Wallace, Sayantani Saha, Frank Soboczenski,\nand Iain James Marshall. 2020. Generating (factual?)\nnarrative summaries of rcts: Experiments with neu-\nral multi-document summarization. AMIA Annual\nSymposium, abs/2008.11293.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\n6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/\nmesh-transformer-jax. Accessed: 2023-06-03.\nYue Wang, Hung Le, Akhilesh Deepak Gotmare,\nNghi D.Q. Bui, Junnan Li, and Steven C. H. Hoi.\n2023. Codet5+: Open code large language mod-\nels for code understanding and generation. arXiv\npreprint arXiv:2305.07922.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V Le. 2022a. Finetuned language\nmodels are zero-shot learners. In International Con-\nference on Learning Representations.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022b. Emer-\ngent abilities of large language models. Transactions\non Machine Learning Research . Survey Certifica-\ntion.\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski,\nMark Dredze, Sebastian Gehrmann, Prabhanjan Kam-\nbadur, David Rosenberg, and Gideon Mann. 2023.\nBloomberggpt: A large language model for finance.\narXiv preprint arXiv:2303.17564.\nQianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao\nLai, Min Peng, Alejandro Lopez-Lira, and Jimin\nHuang. 2023. Pixiu: A large language model, in-\nstruction data and evaluation benchmark for finance.\narXiv preprint arXiv:2306.05443.\nLi Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and\nZhang You. 2023. Chatdoctor: A medical chat model\nfine-tuned on llama model using medical domain\nknowledge. arXiv preprint arXiv:2303.14070.\nHaiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu,\nHuiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei\nYin, and Mengnan Du. 2023. Explainability for\nlarge language models: A survey. arXiv preprint\narXiv:2309.01029.\nLucia Zheng, Neel Guha, Brandon R. Anderson, Peter\nHenderson, and Daniel E. Ho. 2021. When does pre-\ntraining help? assessing self-supervised learning for\nlaw and the casehold dataset of 53,000+ legal hold-\nings. In Proceedings of the Eighteenth International\nConference on Artificial Intelligence and Law, ICAIL\n’21, page 159–168, New York, NY , USA. Association\nfor Computing Machinery.\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu\nJiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\nJiawei Han. 2022. Towards a unified multi-\ndimensional evaluator for text generation. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing , pages 2023–\n2038, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao\nSun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\nLili Yu, et al. 2023. Lima: Less is more for alignment.\narXiv preprint arXiv:2305.11206.\nKaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen\nWang, Hao Chen, Yidong Wang, Linyi Yang, Wei\nYe, Neil Zhenqiang Gong, Yue Zhang, et al. 2023.\nPromptbench: Towards evaluating the robustness of\nlarge language models on adversarial prompts. arXiv\npreprint arXiv:2306.04528.\n110\nA Examples for Evaluation Tasks\nWe manually compose the instruction-style templates, designed for each task for evaluation. The template\ncontains an instruction describing the task, followed by an input as a document or a question. Table 7\nshows an example for each evaluation task.\nDataset Template ‡\nBillSum\n(Kornilova and Eidelman, 2019)\n### Instruction:\nPlease give a summary of the following legal document:\n### Input:\nSECTION 1. TEMPORARY DUTY SUSPENSIONS ON CERTAIN HIV DRUG SUBSTANCES.\n(a) In General.–Subchapter II of chapter 99 of the Harmonized Tariff Schedule of the United States is amended by inserting\nin numerical sequence the following new headings: [...] with respect to goods entered, or withdrawn from warehouse for\nconsumption, on or after the date that is 15 days after the date of enactment of this Act.\nCaseHold\n(Zheng et al., 2021)\n### Instruction:\nSelect one correct answer from ABCDE to match the <HOLDING> statement, not to list all answers.\n### Input:\nStatement: has “jurisdiction to render judgment on an action by an interested party objecting [...] A bidder has a direct\neconomic interest if the alleged errors in the procurement caused it to suffer a competitive injury or prejudice. Myers\nInvestigative & Sec. Servs., Inc. v. United States, 275 F.3d 1366, 1370 (Fed.Cir.2002) (<HOLDING>).\nIn a post-award bid protest, the protestor\nA: holding that an antitrust injury is a necessary element of a 2 claim\nB: holding that actual prejudice is not a necessary element of an insurers untimely notice defense\nC: holding that an assertion of prejudice is not a showing of prejudice\nD: recognizing that allegation of state action is a necessary element of a 1983 claim\nE: holding that prejudice or injury is a necessary element of standing\nLawStackExchange\n(Li et al., 2022)\n### Instruction:\nPlease give an answer to the question:\n### Input:\nHow do we claim the estate of someone who died under a different name in a different country?\nPubMedQA\n(Jin et al., 2019)\n### Instruction:\nAnswer the question with (yes, no, maybe) and provide the reason based on the given context.\n### Input:\nQuestion: Does oxybutynin hydrochloride cause arrhythmia in children with bladder dysfunction?\nContext: METHOD: This study represents a subset of a complete data set, considering only those children aged admitted\nto the Pediatric Surgery and Pediatric Nephrology Clinics during the period January 2011 to July 2012.\nRESULT: In this study, we have determined that the QT interval changes significantly depending on the use of oxybutynin.\nThe QT changes increased cardiac arrhythmia in children.\nRCTSum\n(Wallace et al., 2020)\n### Instruction:\nSummarize the document based on the given title and abstract.\n### Input:\nTitle: Efficacy of prophylactic antibiotics for the prevention of endomyometritis after forceps delivery.\nAbstract: The purpose of this prospective randomized controlled clinical trial was to determine whether prophylactic\nantibiotics reduce the incidence of endomyometritis after forceps delivery. Of the 393 patients studied, 192 received 2 gm\nof intravenous cefotetan after forceps delivery, and 201 patients received no antibiotics. There were seven cases of\nendomyometritis in the group given no antibiotic and none in the cefotetan group, a statistically significant difference\n(P less than .01). We conclude that prophylactic antibiotics are effective in reducing the incidence of endomyometritis after\nforceps delivery. We believe this is the first published study demonstrating this benefit.\niCliniq\n(Yunxiang et al., 2023)\n### Instruction:\nPlease give an answer to the question:\n### Input:\nHello doctor, when should I take probiotics?\nTable 7: Templates designed for each evaluation task. ‡For brevity, we record partial inputs for long documents\nwith [...].\n111",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.675300121307373
    },
    {
      "name": "Adaptability",
      "score": 0.49314063787460327
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.49041080474853516
    },
    {
      "name": "Computer science",
      "score": 0.487210214138031
    },
    {
      "name": "Risk management",
      "score": 0.45624345541000366
    },
    {
      "name": "Risk analysis (engineering)",
      "score": 0.4506837725639343
    },
    {
      "name": "Risk assessment",
      "score": 0.4286789894104004
    },
    {
      "name": "Data science",
      "score": 0.3236545920372009
    },
    {
      "name": "Artificial intelligence",
      "score": 0.23230352997779846
    },
    {
      "name": "Business",
      "score": 0.2150643765926361
    },
    {
      "name": "Computer security",
      "score": 0.20413747429847717
    },
    {
      "name": "Management",
      "score": 0.0931781530380249
    },
    {
      "name": "Finance",
      "score": 0.0722627341747284
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ]
}