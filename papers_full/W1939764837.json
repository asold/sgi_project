{
  "title": "Neural Probabilistic Language Model for System Combination",
  "url": "https://openalex.org/W1939764837",
  "year": 2012,
  "authors": [
    {
      "id": "https://openalex.org/A5026921207",
      "name": "Tsuyoshi Okita",
      "affiliations": [
        "Dublin City University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W589247908",
    "https://openalex.org/W98255950",
    "https://openalex.org/W100623710",
    "https://openalex.org/W2164684057",
    "https://openalex.org/W10287945",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W2154099718",
    "https://openalex.org/W1511986666",
    "https://openalex.org/W1489483588",
    "https://openalex.org/W201231365",
    "https://openalex.org/W146624170",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W764766372",
    "https://openalex.org/W48813473",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2083545877",
    "https://openalex.org/W2915991136",
    "https://openalex.org/W2044905690",
    "https://openalex.org/W2138414624",
    "https://openalex.org/W2117130368",
    "https://openalex.org/W1722351164",
    "https://openalex.org/W2917029047",
    "https://openalex.org/W2034666332",
    "https://openalex.org/W9218159",
    "https://openalex.org/W2158899491",
    "https://openalex.org/W1483849869",
    "https://openalex.org/W1970689298",
    "https://openalex.org/W134434835",
    "https://openalex.org/W2155617236",
    "https://openalex.org/W1503398984",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W2250379827",
    "https://openalex.org/W2912225506",
    "https://openalex.org/W2120861206",
    "https://openalex.org/W2100238596",
    "https://openalex.org/W2397936140",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W13235082",
    "https://openalex.org/W38072894",
    "https://openalex.org/W43079696",
    "https://openalex.org/W2120826678",
    "https://openalex.org/W56456111",
    "https://openalex.org/W1976519809"
  ],
  "abstract": "This paper gives the system description of the neural probabilistic language modeling (NPLM) team of Dublin City University for our participation in the system combination task in the Second Workshop on Applying Machine Learning Techniques to Optimise the Division of Labour in Hybrid MT (ML4HMT-12). We used the information obtained by NPLM as meta information to the system combination module. For the Spanish-English data, our paraphrasing approach achieved 25.81 BLEU points, which lost 0.19 BLEU points absolute compared to the standard confusion network-based system combination. We note that our current usage of NPLM is very limited due to the difficulty in combining NPLM and system combination.",
  "full_text": "Neural Probabilistic Language Model for System\nCombination\nTsu y oshi Oki t a 1\n(1) Dublin City University , Glasnevin, Dublin 9\ntokita@computing.dcu.ie\nAB S T R A C T\nThis paper gives the system description of the neural probabilisti c language modeling (NPLM)\nteam of Dublin City University for our participation in the system combination task in the Sec-\nond W orkshop on Applying Machine Learning T echniques to Optimise the Division of Labour\nin Hybrid MT (ML4HMT -12). W e used the information obtained by NPLM as meta information\nto the system combination module. For the Spanish-English data , our paraphrasing approach\nachieved 25.81 BLEU points, which lost 0.19 BLEU points absolut e compared to the standard\nconfusion network-based system combination. W e note that our curren t usage of NPLM is very\nlimited due to the difﬁculty in combining NPLM and system combinati on.\nKE Y W O R D S: statistical machine translation, neural probabilistic languag e model, system com-\nbination.\n1 Introduction\nThis paper describes a new extension to our system combination mo dule developed in Dublin\nCity University (Du and W ay, 2010a,b; Okita and van Genabith, 20 12). W e have added a neu-\nral probabilistic language model (NPLM) (Bengio et al., 2000, 2 005) to our system combina-\ntion module and tested it in the system combination task at the M L4HMT -2012 workshop.\nA neural probabilistic language model (NPLM) (Bengio et al., 20 00, 2005) and the distributed\nrepresentations (Hinton et al., 1986) provide an idea to achieve th e better perplexity than n-\ngram language model (Stolcke, 2002) and their smoothed langua ge models (Kneser and Ney,\n1995; Chen and Goodman, 1998; T eh, 2006). Recently , the latte r one, i.e. smoothed language\nmodel, has had a lot of developments in the line of nonparametric Bayesian methods such as\nhierarchical Pitman- Y or language model (HPYLM) (T eh, 2006) and Se quence Memoizer (SM)\n(W ood et al., 2009; Gasthaus et al., 2010), including an appli cation to SMT (Okita and W ay,\n2010a,b, 2011). A NPLM considers the representation of data in orde r to make the probability\ndistribution of word sequences more compact where we focus on the simi lar semantical and\nsyntactical roles of words. For example, when we have two sentences “The cat is walking in\nthe bedroom” and “ A dog was running in a room” , these sentences can be more compactly\nstored than the n-gram language model if we focus on the similarity between (the, a), (bed-\nroom, room), (is, was), and (running, walking). Thus, a NPLM provid es the semantical and\nsyntactical roles of words as a language model. A NPLM of Bengio et al . (2000) implemented\nthis using the multi-layer neural network and yielded 20% to 35% be tter perplexity than the\nlanguage model with the modiﬁed Kneser-Ney methods (Chen and Go odman, 1998).\nThere are several successful applications of NPLM (Schwenk, 2007; Col lobert and W eston,\n2008; Schwenk, 2010; Collobert, 2011; Collobert et al., 2011; D eschacht et al., 2012;\nSchwenk et al., 2012). First, one category of applications include POS tagging, NER tag-\nging, and parsing (Collobert et al., 2011; Bordes et al., 2011). This category uses the fea-\ntures provided by a NPLM in the limited window size. 1 It is often the case that there is no\nsuch long range effects that the decision cannot be made beyond the l imited windows which\nrequires to look carefully the elements in a long distance. Second, the other category of ap-\nplications include Semantic Role Labeling (SRL) task (Collob ert et al., 2011; Deschacht et al.,\n2012). This category uses the features within a sentence. A typical element is the predi-\ncate in a SRL task which requires the information which sometimes in a long distance but\nwithin a sentence. Both of these approaches do not require to obtai n the best tag sequence,\nbut these tags are independent. Third, the ﬁnal category includes M ERT process (Schwenk,\n2010) and possibly many others where most of them remain undevelo ped. The objective\nof this learning in this category is not to search the best tag for a w ord but the best se-\nquence for a sentence. Hence, we need to apply the sequential lea rning approach. 2 Al-\nthough most of the applications described in (Collobert and W est on, 2008; Collobert, 2011;\nCollobert et al., 2011; Deschacht et al., 2012) are monolingual t asks, the application of this\napproach to a bilingual task introduces really astonishing aspect s, which we can call “cre-\native words” (V eale, 2012), automatically into the traditional resource constrained SMT com-\nponents. For example, the training corpus of word aligner is ofte n strictly restricted to the\n1 It is possible to implement a parser in the way of the second category . Howe ver , we adopt the categorization\nwhich was implemented by (Collobert et al., 2011).\n2 The ﬁrst and second approaches do not often appear in the context of SMT , w hile the third category includes most\nof the decoding algorithm appeared in SMT including MAP decoding, MBR dec oding, and (monotonic) consensus\ndecoding. The latter two decoding appears in system combination.\ngiven parallel corpus. However , a NPLM allows this training with hug e monolingual corpus.\nAlthough most of this line has not been even tested mostly due t o the problem of compu-\ntational complexity of training NPLM, Schwenk et al. (2012) appl ied this to MERT process\nwhich reranks the n-best lists using NPLM. This paper aims at diffe rent task, a task of sys-\ntem combination (Bangalore et al., 2001; Matusov et al., 2006; T romble et al., 2008; Du et al.,\n2009; DeNero et al., 2009; Okita and van Genabith, 2012). This ca tegory of tasks employs\nthe sequential method such as Maximum A Posteriori (MAP) inference (Viterbi decoding)\n(Koller and Friedman, 2009; Sontag, 2010; Murphy, 2012) on Cond itional Random Fields\n(CRFs) / Markov Random Fields (MRFs). 3\nThe remainder of this paper is organized as follows. Section 2 de scribes our algorithms. In\nSection 3, our experimental results are presented. W e conclude in Se ction 4.\n2 Our Algorithms\nThe aim of NPLM is to capture the semantically and syntactically simi lar words in a way that\na latent word depends on the context. There would be many ways to us e this language model.\nHowever , one difﬁculty resides in how such information can be incorpora ted to the module of\nsystem combination. Due to this difﬁculty , we present here a method which is rather restricted\ndespite the power of NPLM.\nThis paper presents two methods based on the intuitive observati on that we will get the variety\nof words if we condition on the ﬁxed context, which would form paraphra ses in theory . Then,\nwe present the second method that we examine the dependency struct ure of candidates sen-\ntence replaced with alternative expressions (or paraphrases). Our a lgorithms consist of three\nsteps shown in Algorithm 1. The details of Step 1 and 2 will be ex plained in Section 2.1 and\nStep 3 will be explained in Section 2.2.\nAlgorithm 1 Our Algorithm\nGiven: For given testset g , prepare N translation outputs {s1 , . . . , sN }from several systems.\nStep 1 : T rain NPLM with monolingual corpus. Note that this monolingual corpus would be\nbetter in the same domain as the testset g .\nStep 2 : Modify the translation outputs {s1 , . . . , sN }replaced with alternative expressions (or\nparaphrases).\nStep 3 : Augment the sentences of translation outputs prepared in Step 2 .\nStep 4 : Run the system combination module.\n2.1 Paraphrasing using NPLM\n2.1.1 Plain Paraphrasing\nW e introduce our algorithm via a word sense disambiguation (WSD) t ask which selects the\nright disambiguated sense for the word in question. This task i s necessary due to the fact that\na text is natively ambiguous accommodating with several different meanings. The task of WSD\n(Deschacht et al., 2012) can be written as in (1):\nP (synseti |featuresi , θ) = 1\nZ (features)\n∏\nm\ng (synseti , k)f (feature\nk\ni ) (1)\n3 Note that the (monotonic) consensus decoding in system combination is the sub set of this.\nwhere k ranges over all possible features, f (featurek\ni ) is an indicator function whose value is\n1 if the feature exists, and 0 otherwise, g (synseti , k) is a parameter for a given synset and\nfeature, θis a collection of all these parameters in g (synseti , k), and Z is a normalization\nconstant. Note that we use the term “synset” as an analogy of the W ordNet (Miller, 1995):\nthis is equivalent to “sense” or “meaning”. Note also that NPLM will be included as one of the\nfeatures in this equation. If features include sufﬁcient statist ics, a task of WSD will succeed.\nOtherwise, it will fail.\nNow we assume that the above WSD component was trained. W e would l ike to consider\nthe paraphrasing in connection with this. W e consider a sentence wi th some words re-\nplaced by the alternative surface form. In this context, we are interes ted in the words which\nshare the same synset (or meaning) but the realized surface form is d ifferent. Let us de-\nnote P (surfacei |synset j , features k , θ) by the probability of such words. Then, we suppose that\nwe compare P(surface i =xi |synset j ,featuresk ,θ) and P(surface i =x ′\ni |synset j ,featuresk ,θ) under\nthe condition that synset j , features k , and θare the same, and that the relationships below hold\nas in (2):\nP (surfacei = xi |synset j , features k , θ)> P (surfacei = x ′\ni |synset j , features k , θ) (2)\nThen, the alternative surfaces form xi in higher probability will be chosen instead of the other\none x ′\ni among paraphrases {xi , x ′\ni }of this word.\nOn the one hand, the paraphrases obtained in this way have attracti ve aspects that can be\ncalled “a creative word” (V eale, 2012). This is since the traditiona l resource that can be used\nwhen building a translation model by SMT are constrained on parale ll corpus. However , NPLM\ncan be trained on huge monolingual corpus. On the other hand, unfo rtunately in practice, the\nnotorious training time of NPLM only allows us to use fairly small monolingual corpus although\nmany papers made an effort to reduce it (Mnih and T eh, 2012). Due to this, we cannot ignore\nthe fact that NPLM trained not on a huge corpus may be affected by nois e. Conversely , we\nhave no guarantee that such noise will be reduced if we train NPLM on a h uge corpus. It is\nquite likely that NPLM has a lot of noise for small corpora. Hence, t his paper also needs to\nprovide the way to overcome difﬁculties of noisy data. In order to avoid this difﬁculty , we limit\nthe paraphrase only when it includes itself in high probability .\n2.1.2 Paraphrasing with Modiﬁed Dependency Score\nAlthough we modiﬁed a suggested paraphrase without any intervent ion in the above algorithm,\nit is also possible to examine whether such suggestion should be adopted or not. If we add\nparaphrases and the resulted sentence has a higher score in terms of t he modiﬁed dependency\nscore (Owczarzak et al., 2007) (See Figure 1), this means that the a ddition of paraphrases is a\ngood choice. If the resulted score decreases, we do not need to add the m. One difﬁculty in this\napproach is that we do not have a reference which allows us to score it in t he usual manner .\nFor this reason, we adopt the naive way to deploy the above and we deploy this with pseudo\nreferences. First, if we add paraphrases and the resulted sentence does not ha ve a very bad\nscore, we add these paraphrases since these paraphrase are not very bad ( naive way). Second,\nwe do scoring between the sentence in question with all the other candidates (pseudo references )\nand calculate an average of them. Thus, our second algorithm is to se lect a paraphrase which\nmay not achieve a very bad score in terms of the modiﬁed dependency score us ing NPLM.\nS\nNP NP VP\nYesterday John\nresigned\nV\nSUBJ          PRED    john\n                    NUM     sg\n                    PERS     3\nPRED         resign\nTENSE       past\nADJ            ([PRED yesterday])\nS\nNP VP\nV NPJohn\nresigned yesterday\nSUBJ PRED   john\nNUM    sg\nPERS    3\nPRED   resign\nTENSE   past\nADJ          ([PRED yesterday])\nDifferent structure Same representation\nc−structure f−structure\nin c−structure in f−structure\nFigure 1: By the modiﬁed dependency score (Owczarzak et al., 2007), t he score of these two\nsentences, “ John resigned yesterday” and “Y esterday John resigned ”, are the same. Figure\nshows c-structure and f-structure of two sentences using Lexical Functio nal Grammar (LFG)\n(Bresnan, 2001).\nModiﬁed Dependency Score W e mention here the reason why we use the modiﬁed de-\npendency score. First, unlike BLEU or NIST, the modiﬁed dependency score can capture\nthe syntactical construction and grammaticality of sentences. Second , our current dataset\nseems to be interesting since it includes Lucy L T RBMT outputs. (T he last year’s ML4HMT -11\n(Okita and van Genabith, 2011) also included the translation ou tput of Lucy L T RBMT out-\nputs.) If we choose the entire Lucy’s output as a backbone and run a sys tem combination,\nthe resulted score is the highest among various system combination s trategies we tried (See\n“s2 backbone” in T able 2). These two facts suggest us the follow ing strategy: if we have prior\nknowledge that the Lucy backbone will obtain a high score, it woul d be interesting to start from\nthe Lucy backbone and pursue whether we can improve the overall score furt her by adding\nparaphrases. As is evident from the fact that the Lucy backbone is goo d, our interest will not\nbe BLEU or NIST, but M O D I F I E D D E P E N D E N C Y S C O R E . This may lead to a higher BLEU score than\nthe system combination results with Lucy backbone. Note that in ord er to make it a universal\nalgorithm, we need to remove Lucy backbone from this algorithm. Hence , only the modiﬁed\ndependency score remains, which forms the algorithm already mention ed.\nsystem translation output precision recall F -score\ns1 these do usually in a week . 0.080 0.154 0.105\ns2 these are normally made in a week . 0.200 0.263 0.227\ns3 they are normally in one week . 0.080 0.154 0.105\ns4 they are normally on a week . 0.120 0.231 0.158\nref the funding is usually offered over a one-week period .\nT able 1: An example of modiﬁed dependency score for a set of transla tion outputs.\n2.2 System Combination\nAs is mentioned at the beginning of this section, the interface b etween the NPLM and sys-\ntem combination has some difﬁculties. This contrasts with the tas k of n-best reranking\n(Schwenk et al., 2012). In the case of n-best reranking, the probabi lity provided by NPLM\ncan be used immediately in the re-evaluation of the n-best lists.\nDifﬁculties of Interface In our case, due to the reason below despite the advantage of word\nvarieties it is difﬁcult to incorporate this into the translation out puts in a straight forward way .\nThe two decoding strategies used by a confusion network-based syst em combination, i.e. MBR\ndecoding and (monotonic) consensus decoding, have difﬁculties in e ach step.\nFirst, in MBR decoding in the ﬁrst step, the inputs, i.e. each transl ation outputs, are quantiﬁed\nby the loss function with its score in the sentence level. This mecha nism does not allow us\nto add fragments freely in the word level. Therefore, it requires us to increase the number of\nsentences with only a slight replacement in the sentence level. Th is paper takes this strategy\nfor the ﬁrst step to circumvent this difﬁculty .\nSecond, in (monotonic) consensus decoding in the second step, the word posterior probabil-\nities in the confusion network do not reﬂect the probability quant iﬁed globally , but is rather\nlocally in accordance with other probability of words in the same pos ition. Similarly , one way\nwould be to augment in the sentence level.\nInputs to System Combination Module W e check the possibilities whether the word can\nhave alternative expression and whether the probability of such ex pression is bigger than that\nof the original word or not. If this holds, we replace such words with alternative expressions.\nThis will make a new sentence.\n3 Experimental Results\nML4HMT -2012 provides four translation outputs ( s1 to s4) which are MT outputs by two RBMT\nsystems, A P E RT I U M and L U C Y, PB-SMT (M O S E S) and HPB-SMT (M O S E S), respectively . The tun-\ning data consists of 20,000 sentence pairs, while the test data co nsists of 3,003 sentence pairs.\nOur experimental setting is as follows. W e use our system combin ation module (Du and W ay,\n2010a,b; Okita and van Genabith, 2012), which has its own langu age modeling tool, MERT\nprocess, and MBR decoding. W e use the BLEU metric as loss function i n MBR decoding. W e\nuse TER P4 as alignment metrics in monolingual word alignment. W e trained N PLM using\n500,000 sentence pairs from English side of EN-ES corpus of E U R O PA R L5.\n(a) the Government wants to limit the torture of the \" witches \" , as it pu blished in a\nbrochure\n(b) the Government wants to limit the torture of the \" witches \" , as i t published in the\nproceedings\n(a) the women that he \" return \" witches are sent to an area isolated , so t hat they do\nnot hamper the rest of the people .\n(b) the women that he \" return \" witches are sent to an area eligible , so that they do\nnot affect the rest of the country .\nT able 2: T able includes two examples of plain paraphrase.\nThe results show that the ﬁrst algorithm of NPLM-based paraphrased au gmentation, that is\n4 http://www.cs.umd.edu/~snover/terp\n5 http://www.statmt.org/europarl\nNPLM plain, achieved 25.61 BLEU points, which lost 0.39 BLEU poin ts absolute over the stan-\ndard system combination. The second algorithm, NPLM dep, achieved sl ightly better results\nof 25.81 BLEU points, which lost 0.19 BLEU points absolute over the standard system combi-\nnation. Note that the baseline achieved 26.00 BLEU points, the best single system in terms\nof BLEU was s4 which achieved 25.31 BLEU points, and the best sing le system in terms of\nMETEOR was s2 which achieved 0.5853.\nNIST BLEU METEOR WER PER\ns1 6.4996 0.2248 0.5458641 64.2452 49.9806\ns2 6.9281 0.2500 0.5853446\n62.9194 48.0065\ns3 7.4022 0.2446 0.5544660 58.0752 44.0221\ns4 7.2100 0.2531\n0.5596933 59.3930 44.5230\nNPLM plain 7.6041 0.2561 0.5593901 56.4620 41.8076\nNPLM dep 7.6213 0.2581 0.5601121 56.1334 41.7820\nBLEU-MBR 7.6846 0.2600 0.5643944 56.2368 41.5399\nmin ave TER-MBR 7.6231 0.2638 0.5652795 56.3967 41.6092\nDA 7.7146 0.2633 0.5647685 55.8612 41.7264\nQE 7.6846 0.2620 0.5642806 56.0051 41.5226\ns2 backbone 7.6371 0.2648 0.5606801 56.0077 42.0075\nmodDep precision 7.6670 0.2636 0.5659757 56.4393 41.4986\nmodDep recall 7.6695 0.2642 0.5664320 56.5059 41.5013\nmodDep Fscore 7.6695 0.2642 0.5664320 56.5059 41.5013\nmodDep precision modDep recall modDep Fscore\naverage s1 0.244 (586) 0.208 0.225\naverage s2 0.250 (710) 0.188 0.217\naverage s3 0.189 (704) 0.145 0.165\naverage s4 0.195 (674) 0.167 0.180\nT able 3: This table shows single best performance, the performance of two algorithms in\nthis paper (NPLM plain and dep), MBR-decoding with BLEU loss fun ction and TER loss\nfunction, the performance of domain adaptation (Okita et al., 20 12b) and quality estimation\n(Okita et al., 2012a), the performance of Lucy backbone, and the pe rformance of the selection\nof sentences by modiﬁed dependency score (precision, recall, and F -score each). The four lines\nat the bottom marked with average s1 to s4 indicates the average perfo rmance of s1 in terms\nof precision, recall, and F -score (from the 2nd to 4th columns) when we m ake the backbone\nby choosing the maximum score in terms of the modiﬁed dependency score . For example, the\nﬁrst line of “modDep precision” shows when we chose a backbone by the maximum modiﬁed\ndependency score in terms of precision. 586 sentences were selected fro m s1, 710 sentences\nwere from s2, and so forth. The average BLEU score of these 586 sentence s was 24.4.\nConclusion and Perspectives\nThis paper deployed meta information obtained by NPLM into a syst em combination module.\nNPLM captures the semantically and syntactically similar words in a w ay that a latent word\ndepends on the context. First, we interpret the information obtai ned by NPLM as paraphrases\nwith regard to the translation outputs. Then, we incorporate the au gmented sentences as\ninputs to the system combination module. Unfortunately , this s trategy lost 0.39 BLEU points\nabsolute compared to the standard confusion network-based system co mbination. A revised\nstrategy to assess the quality of paraphrases achieved 25.81 BLEU points, which lost 0.19 BLEU\npoints absolute.\nThere are many further avenues. First, as already mentioned, this p aper only scratched the\nsurface of NPLM. One problem was the interface between NPLM and system combination. Our\nmotivation behind using NPLM was the possibility that NPLM woul d supply the semantically\nand syntactically rich synonyms and similar words to the rather restricte d translation outputs,\nas well as the traditional functions as LM, which are to be supplie d to the system combination\nmodule. For this reason, we believe that paraphrases generated us ing NPLM will not be a bad\ndirection. However , there would be other approach as well. Collobe rt and W eston (2008) and\nBordes et al. (2011) integrate NPLM in their software. When we int egrate our approach, one\nway would be to implement it without employing the knowledge o f paraphrases. It would\nbe interesting to compare this and our approaches in this paper . Al ternatively , prior knowl-\nedge about the speriority of Lucy output can be embedded into syste m combination by prior\n(Okita et al., 2010b,a; Okita, 2012).\nSecond, we show some positive results about the modiﬁed dependen cy score (Owczarzak et al.,\n2007). W e used this as sentence-based criteria to select a backbone i n three ways: maximum\nprecision, recall, and F -score. Results are shown in T able 3. Indeed, these criteria worked quite\nwell. Unfortunately , these scores were still lower than that of L ucy’s backbone. The lower\nparts of this table show statistics when we select a backbone by th e modiﬁed dependency\nscore. Interestingly , the modiﬁed dependency score of s2 (Lucy) was th e best in precision\nscore, but was not the best in recall or in F -score. This shows that the selection of backbone\nby the modiﬁed dependency score did not work as much as that of the (ﬁxe d) Lucy backbone.\nW e need to search another explanation why the Lucy backbone obtai ned the highest score.\nThird, this paper did not mention much about noise. Understandi ng the mechanism of noise\non NPLM may be related to the learning mechanism of NPLM, if we draw an analogy from the\ncase when we examined the noise of word alignment (Okita, 2009; Okita et al., 2010b). This\nmay also be related to the smoothing mechanism (Okita and W ay, 20 10a,b, 2011).\nAcknowledgments\nW e thank Maite Melero for proof-reading. This research is partly supp orted by the 7th Frame-\nwork Programme and the ICT Policy Support Programme of the European Comm ission through\nthe T4ME (Grant agreement No. 249119) project as well as by Science F oundation Ireland\n(Grant No. 07 /CE/I1142) as part of the Centre for Next Generation Localisation (ww w .cngl.ie)\nat Dublin City University .\nReferences\nBangalore, S., Bordel, G., and Riccardi, G. (2001). Computing cons ensus translation from\nmultiple machine translation systems. In Proceedings of the IEEE Automatic Speech Recognition\nand Understanding Workshop (ASRU) , pages 350–354.\nBengio, Y ., Ducharme, R., and Vincent, P . (2000). A neural probabi listic language model. In\nProceedings of Neural Information Systems .\nBengio, Y ., Schwenk, H., Senécal, J.-S., Morin, F ., and Gauvain, J .-L. (2005). Neural proba-\nbilistic language models. Innovations in Machine Learning: Theory and Applications Edited by\nD. Holmes and L. C. Jain .\nBordes, A., Glorot, X., W eston, J., and Bengio, Y . (2011). T owa rds open-text semantic parsing\nvia multi-task learning of structured embeddings. CoRR, abs /1107.3663.\nBresnan, J. (2001). Lexical functional syntax. Blackwell.\nChen, S. and Goodman, J. (1998). An empirical study of smoothin g techniques for language\nmodeling. T echnical report TR-10-98 Harvard University .\nCollobert, R. (2011). Deep learning for efﬁcient discriminative pa rsing. In Proceedings of the\n14th International Conference on Artiﬁcial Intelligence and Statistic s (AISTATS) .\nCollobert, R. and W eston, J. (2008). A uniﬁed architecture for natu ral language process-\ning: Deep neural networks with multitask learning. In International Conference on Machine\nLearning (ICML 2008) .\nCollobert, R., W eston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P . (2011).\nNatural language processing (almost) from scratch. Journal of Machine Learning Research ,\n12:2493–2537.\nDeNero, J., Chiang, D., and Knight, K. (2009). F ast consensus d ecoding over translation\nforests. In proceedings of the Joint Conference of the 47th Annual Meeting of t he ACL and the\n4th International Joint Conference on Natural Language Processing of the AFNLP , pages 567–\n575.\nDeschacht, K., Belder , J. D., and Moens, M.-F . (2012). The laten t words language model.\nComputer Speech and Language , 26:384–409.\nDu, J., He, Y ., Penkale, S., and W ay , A. (2009). MaT rEx: the DCU MT System for WMT 2009.\nIn Proceedings of the Third EACL Workshop on Statistical Machine T ransla tion, pages 95–99.\nDu, J. and W ay , A. (2010a). An incremental three-pass system combin ation framework by\ncombining multiple hypothesis alignment methods. International Journal of Asian Language\nProcessing, 20(1):1–15.\nDu, J. and W ay , A. (2010b). Using terp to augment the system combi nation for smt. In\nProceedings of the Ninth Conference of the Association for Machine T ran slation (AMTA2010) .\nGasthaus, J., W ood, F ., and T eh, Y . W . (2010). Lossless compres sion based on the sequence\nmemoizer . DCC 2010 .\nHinton, G. E., McClelland, J. L., and Rumelhart, D. (1986). Dis tributed representations. Paral-\nlel Distributed Processing: Explorations in the Microstructure of Cognitio n(Edited by D.E. Rumel-\nhart and J.L. McClelland) MIT Press , 1.\nKneser , R. and Ney , H. (1995). Improved backing-off for n-gram lang uage modeling. In\nProceedings of the IEEE International Conference on Acoustics, Spee ch and Signal Processing ,\npages 181–184.\nKoller , D. and Friedman, N. (2009). Probabilistic graphical mode ls: Principles and techniques.\nMIT Press .\nMatusov , E., Uefﬁng, N., and Ney , H. (2006). Computing consens us translation from multiple\nmachine translation systems using enhanced hypotheses alignmen t. In Proceedings of the 11st\nConference of the European Chapter of the Association for Computa tional Linguistics (EACL) ,\npages 33–40.\nMiller , G. A. (1995). W ordnet: A lexical database for english. Communications of the ACM ,\n38(11):39–41.\nMnih, A. and T eh, Y . W . (2012). A fast and simple algorithm for t raining neural probabilistic\nlanguage models. In Proceedings of the International Conference on Machine Learning .\nMurphy , K. P . (2012). Machine learning: A probabilistic perspecti ve. The MIT Press .\nOkita, T . (2009). Data cleaning for word alignment. In Proceedings of Joint Conference of the\n47th Annual Meeting of the Association for Computational Linguistics and the 4th International\nJoint Conference on Natural Language Processing of the Asian Federation of Natural Language\nProcessing (ACL-IJCNLP 2009) Student Research Workshop , pages 72–80.\nOkita, T . (2012). Annotated corpora for word alignment between j apanese and english and\nits evaluation w ith map-based word aligner . In Calzolari, N., Ch oukri, K., Declerck, T ., Do ˘gan,\nM. U., Maegaard, B., Mariani, J., Odijk, J., and Piperidis, S., ed itors, Proceedings of the Eighth\nInternational Conference on Language Resources and Evalu ation ( LREC-2012), pages 3241–\n3248, Istanbul, T urkey . European Language Resources Associatio n (ELRA). ACL Anthology\nIdentiﬁer: L12-1655.\nOkita, T ., Graham, Y ., and W ay , A. (2010a). Gap between theory a nd practice: Noise sensitive\nword alignment in machine translation. In Proceedings of the Workshop on Applications of\nPattern Analysis (WAPA2010). Cumberland Lodge, England.\nOkita, T ., Guerra, A. M., Graham, Y ., and W ay , A. (2010b). Multi- W ord Expression sensi-\ntive word alignment. In Proceedings of the Fourth International Workshop On Cross Lingual\nInformation Access (CLIA2010, collocated with COLING2010), Beij ing, China. , pages 1–8.\nOkita, T ., Rubino, R., and van Genabith, J. (2012a). Sentence-l evel quality estimation for mt\nsystem combination. In Proceedings of ML4HMT Workshop (collocated with COLING 2012) .\nOkita, T ., T oral, A., and van Genabith, J. (2012b). T opic model ing-based domain adaptation\nfor system combination. In Proceedings of ML4HMT Workshop (collocated with COLING 2012) .\nOkita, T . and van Genabith, J. (2011). DCU Confusion Network-ba sed System Combination\nfor ML4HMT. Shared T ask on Applying Machine Learning techniques to optimising the d ivision\nof labour in Hybrid MT (ML4HMT -2011, collocated with LIHMT -2011) .\nOkita, T . and van Genabith, J. (2012). Minimum bayes risk decodin g with enlarged hypothesis\nspace in system combination. In Proceedings of the 13th International Conference on Intelligent\nT ext Processing and Computational Linguistics (CICLING 2012). LNC S 7182 Part II. A. Gelbukh\n(Ed.), pages 40–51.\nOkita, T . and W ay , A. (2010a). Hierarchical pitman-yor language mo del in machine transla-\ntion. In Proceedings of the International Conference on Asian Language Proce ssing (IALP 2010) .\nOkita, T . and W ay , A. (2010b). Pitman-Yor process-based language model for Machine T rans-\nlation. International Journal on Asian Language Processing , 21(2):57–70.\nOkita, T . and W ay , A. (2011). Given bilingual terminology in st atistical machine translation:\nMwe-sensitve word alignment and hierarchical pitman-yor process-based translation model\nsmoothing. In Proceedings of the 24th International Florida Artiﬁcial Intelligence R esearch Soci-\nety Conference (FLAIRS-24) , pages 269–274.\nOwczarzak, K., van Genabith, J., and W ay , A. (2007). Evaluating m achine translation with\nLFG dependencies. Machine T ranslation, 21(2):95–119.\nSchwenk, H. (2007). Continuous space language models. Computer Speech and Language ,\n21:492–518.\nSchwenk, H. (2010). Continuous space language models for stat istical machine translation.\nThe Prague Bulletin of Mathematical Linguistics , 83:137–146.\nSchwenk, H., Rousseau, A., and Attik, M. (2012). Large, pruned o r continuous space language\nmodels on a gpu for statistical machine translation. In Proceeding of the NAACL workshop on\nthe Future of Language Modeling .\nSontag, D. (2010). Approximate inference in graphical models usi ng LP relaxations. Mas-\nsachusetts Institute of T echnology (Ph.D. thesis) .\nStolcke, A. (2002). SRILM – An extensible language modeling t oolkit. In Proceedings of the\nInternational Conference on Spoken Language Processing , pages 901–904.\nT eh, Y . W . (2006). A hierarchical bayesian language model based on pitman-yor processes. In\nProceedings of the 44th Annual Meeting of the Association for Computa tional Linguistics (ACL-\n06), Prague, Czech Republic , pages 985–992.\nT romble, R., Kumar , S., Och, F ., and Macherey , W . (2008). Lattice m inimum bayes-risk de-\ncoding for statistical machine translation. Proceedings of the 2008 Conference on Empirical\nMethods in Natural Language Processing , pages 620–629.\nV eale, T . (2012). Exploding the creativity myth: The computation al foundations of linguistic\ncreativity . London: Bloomsbury Academic .\nW ood, F ., Archambeau, C., Gasthaus, J., James, L., and T eh, Y . W . (2009). A stochastic\nmemoizer for sequence data. In Proceedings of the 26th International Conference on Machine\nLearning, pages 1129–1136.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8091837167739868
    },
    {
      "name": "Task (project management)",
      "score": 0.7136633396148682
    },
    {
      "name": "Artificial neural network",
      "score": 0.6841129660606384
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6811678409576416
    },
    {
      "name": "Confusion",
      "score": 0.6570168137550354
    },
    {
      "name": "Probabilistic logic",
      "score": 0.6449295282363892
    },
    {
      "name": "Language model",
      "score": 0.583276629447937
    },
    {
      "name": "Natural language processing",
      "score": 0.5169667601585388
    },
    {
      "name": "Scheme (mathematics)",
      "score": 0.4878294765949249
    },
    {
      "name": "Machine learning",
      "score": 0.48583218455314636
    },
    {
      "name": "Engineering",
      "score": 0.074878990650177
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Psychoanalysis",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I42934936",
      "name": "Dublin City University",
      "country": "IE"
    }
  ],
  "cited_by": 3
}