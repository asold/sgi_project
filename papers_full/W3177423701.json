{
  "title": "Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models",
  "url": "https://openalex.org/W3177423701",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2129999472",
      "name": "Junyi Li",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2133263866",
      "name": "Tianyi Tang",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2307999729",
      "name": "Wayne Xin Zhao",
      "affiliations": [
        "Renmin University of China",
        "Beijing Institute of Big Data Research",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2096838253",
      "name": "Zhicheng Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096490164",
      "name": "Nicholas Jing Yuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3212238123",
      "name": "Ji-Rong Wen",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4288375838",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3035565536",
    "https://openalex.org/W2250937897",
    "https://openalex.org/W2250342921",
    "https://openalex.org/W3088227725",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W4307653761",
    "https://openalex.org/W3003265726",
    "https://openalex.org/W3034080136",
    "https://openalex.org/W3173273620",
    "https://openalex.org/W3115091907",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2146008005",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1560729591",
    "https://openalex.org/W1533230146",
    "https://openalex.org/W4299547686",
    "https://openalex.org/W2468355276",
    "https://openalex.org/W3034822304",
    "https://openalex.org/W4229543565",
    "https://openalex.org/W2481265265",
    "https://openalex.org/W2951048068",
    "https://openalex.org/W2963091658",
    "https://openalex.org/W4288375073",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2905103081",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2322584079",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3187134297",
    "https://openalex.org/W2604314403",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2786660442",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4288089799"
  ],
  "abstract": "This paper studies how to automatically generate a natural language text that describes the facts in knowledge graph (KG).Considering the few-shot setting, we leverage the excellent capacities of pretrained language models (PLMs) in language understanding and generation.We make three major technical contributions, namely representation alignment for bridging the semantic gap between KG encodings and PLMs, relation-biased KG linearization for deriving better input representations, and multi-task learning for learning the correspondence between KG and text.Extensive experiments on three benchmark datasets have demonstrated the effectiveness of our model on KG-to-text generation task.In particular, our model outperforms all comparison methods on both fully-supervised and fewshot settings.Our code and datasets are available at https:",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1558–1568\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1558\nFew-shot Knowledge Graph-to-Text Generation\nwith Pretrained Language Models\nJunyi Li1,3, Tianyi Tang1, Wayne Xin Zhao1,3,5∗\n, Zhicheng Wei4,\nNicholas Jing Yuan4 and Ji-Rong Wen1,2,3\n1Gaoling School of Artiﬁcial Intelligence, Renmin University of China\n2School of Information, Renmin University of China\n3Beijing Key Laboratory of Big Data Management and Analysis Methods\n4Huawei Cloud\n5Beijing Academy of Artiﬁcial Intelligence, Beijing, 100084, China\n{lijunyi,jrwen,steven_tang}@ruc.edu.cn\n{batmanfly,nicholas.jing.yuan}@gmail.com weizhicheng1@huawei.com\nAbstract\nThis paper studies how to automatically gen-\nerate a natural language text that describes the\nfacts in knowledge graph (KG). Considering\nthe few-shot setting, we leverage the excel-\nlent capacities of pretrained language models\n(PLMs) in language understanding and gener-\nation. We make three major technical contri-\nbutions, namely representation alignment for\nbridging the semantic gap between KG encod-\nings and PLMs, relation-biased KG lineariza-\ntion for deriving better input representations,\nand multi-task learning for learning the cor-\nrespondence between KG and text. Exten-\nsive experiments on three benchmark datasets\nhave demonstrated the effectiveness of our\nmodel on KG-to-text generation task. In par-\nticular, our model outperforms all compari-\nson methods on both fully-supervised and few-\nshot settings. Our code and datasets are avail-\nable at https://github.com/RUCAIBox/\nFew-Shot-KG2Text.\n1 Introduction\nKnowledge graphs (KGs), such as Wikidata and\nDBpedia, are essential for many natural language\nprocessing (NLP) applications (Ji et al., 2020). To\nunderstand the structured information in KG, the\ntask of KG-to-text generation has been proposed to\nautomatically generate a descriptive text for a given\nknowledge graph (Koncel-Kedziorski et al., 2019;\nRibeiro et al., 2020a). Figure 1 illustrates a KG\nwith the corresponding descriptive text, in which\nthe nodes (e.g., Stan Lee and Iron Man) represent\nentities and the edges (e.g., creator and alias) de-\nscribe the relations between connected entities.\nIn recent years, with the help of crowdsourcing\nplatforms and information extraction (IE) systems,\nlarge-scale labelled pairs of KG and its descrip-\ntive text have been created, such as WikiBio (Le-\nbret et al., 2016) and WebNLG Challenge (Gardent\n∗Corresponding author\nJack Kirby\ncreator \nStan Lee\ncreator \nStan Lee\nfounder \nThe Avengers Thor\nfounder \nIron Man\nTony Stark\nalias \nIron Man is a fictional superhero\nwho wears a suit of armor. He was\ncreated by writer Stan Lee, and\ndesigned by artists Jack Kirby   .\nIron Man's alter ego is Tony Stark.\nHe has found the superhero team the\nAvengers alongside Thor. \nEntity mentionKG Descriptive Text\nFigure 1: A knowledge graph (subgraph) with its de-\nscriptive text. The underlined words represent the con-\ntext keywords about entities.\net al., 2017). Based on these datasets, data-driven\nmodels have shown impressive capabilities to pro-\nduce informative and ﬂuent text for a given KG (Lo-\ngan et al., 2019; Moryossef et al., 2019). However,\ndue to the great expense in annotation process, it\nis not always feasible to generate large-scale la-\nbelled datasets for a variety of domains in practice.\nMotivated by this, we propose to study the task of\nfew-shot KG-to-text generation that aims to pro-\nduce satisfactory output text given only a handful\nof (several hundred) labelled instances.\nTo fulﬁl this task, we need to fully understand\nthe complicated semantic relations between enti-\nties from various domains, which is challenging\nwith limited labelled data. Our solution is inspired\nby the excellent few-shot capabilities of pretrained\nlanguage models (PLMs) on language understand-\ning and generation tasks (Brown et al., 2020; Chen\net al., 2020; Li et al., 2021a). Pretrained on the\nlarge-scale corpora, PLMs encode vast amounts of\nworld knowledge into their parameters (Li et al.,\n2021b), which is potentially beneﬁcial to under-\nstand and describe the KG facts in our task.\nHowever, applying PLMs to few-shot KG-to-\ntext generation still faces two challenges. First,\nPLMs are usually pretrained on natural language\ntext, while the KG inputs for our task are structured\ngraphs. This semantic gap makes it difﬁcult to\neffectively inject KG representations into PLMs\n1559\nespecially with limited labelled instances. Second,\nunlike many other text generation tasks, KG-to-text\ngeneration requires faithful generation based on the\nunderstanding of KG facts. It needs to learn an\naccurate semantic correspondence between input\nKG and output text, which will be more difﬁcult in\nfew-shot settings.\nTo address the above issues, in this paper, we pro-\npose a few-shot KG-to-text generation model based\non PLMs. There are three major technical contri-\nbutions in our model. First, in order to bridge the\nsemantic gap, we enforce the representation align-\nment by learning the correspondence between KG\nrepresentations (encoded by graph neural networks)\nand PLM-based entity representations. Second, to\nfeed KG into PLMs, we propose a relation-biased\nbreadth-ﬁrst search (RBFS) strategy to linearize\nKG into a well-planned entity sequence. Finally,\nwe jointly train the primary text generation task\nand an auxiliary KG reconstruction task under the\nframework of multi-task learning. This step further\nenhances the semantic correspondence between in-\nput KG and output text, based on which our model\ncan generate faithful text about KG.\nTo the best of our knowledge, we are the ﬁrst\nstudy to investigate PLMs for few-shot KG-to-text\ngeneration. Extensive experiments on three bench-\nmark datasets demonstrate the effectiveness of our\nfew-shot KG-to-text generation model.\n2 Related Work\nIn this work, we mainly focus on generating text\nfrom knowledge graphs using PLMs.\nKG-to-Text Generation. Early works mainly cen-\ntered around statistical methods, applying grammar\nrules to generate text (Konstas and Lapata, 2013;\nFlanigan et al., 2016). Recently, neural based ap-\nproaches have been proposed to generate text from\nlinearized KG triples (Gardent et al., 2017), how-\never, unable to model structural information about\nKG. Many works explored how to encode the graph\nstructure using Graph Neural Networks (GNNs) or\nTransformers explicitly. Koncel-Kedziorski et al.\n(2019) leveraged a graph Transformer encoder to\ncompute node representations by attending over\nlocal neighborhoods via self-attention. In contrast,\nRibeiro et al. (2020a) focused on combining global\nand local message passing mechanisms based on\nGNNs, capturing complementary graph contexts.\nGuo et al. (2020) presented an unsupervised train-\ning method that can iteratively back translate be-\ntween the text and graph data. Different from them,\nwe explore how to utilize large PLMs for few-shot\nKG-to-text generation.\nPretrained Language Model. Recent years have\nwitnessed prominent achievement of PLMs in NLP\ntasks (Devlin et al., 2019; Radford et al., 2019).\nPretrained on massive corpora, pretrained models\nshowcase unprecedented generalization ability to\nsolve related downstream tasks (Li et al., 2021b).\nHowever, most of existing PLMs were conditioned\non text data (Radford et al., 2019; Lewis et al.,\n2020), lacking consideration of structured data\ninput. Ribeiro et al. (2020b) proposed to utilize\nPLMs for KG-to-text generation by randomly lin-\nearizing graph into a sequence of triples. While,\nthese methods do not explicitly model the structural\nrelations of KG, which is critical for generating\nfaithful text. Our work aims to consider the KG\nstructure and bridge the semantic gap between KG\nencodings and PLMs.\n3 Problem Formulation\nKG-to-text generation (Ribeiro et al., 2020a) aims\nto automatically generate a natural language text\nthat describes the facts in KG.\nFormally, the input KG consists of a set of triples,\ndenoted as G = {⟨e,r,e′⟩|e,e′ ∈ E,r ∈ R},\nwhere Eand Rdenote the entity set and relation\nset, respectively. A triple ⟨e,r,e′⟩denotes the fact\nthat relation rexists between head entity eand tail\nentity e′. Note that the input KG is a small and com-\npact subgraph extracted from large-scale knowl-\nedge graphs ( e.g., DBpedia). Following Koncel-\nKedziorski et al. (2019), a text describing the input\nKG is usually available in this task. Let Vdenote\nthe vocabulary. The target is to generate a natural\nlanguage text Y = ⟨w1,...,w j,...,w T ⟩(wj ∈V )\nthat represents the correct and concise semantics of\nentities and their relations in the given knowledge\ngraph. The text contains a set of entity mentions\nM= {me|me = ⟨e,se,oe⟩,e ∈E}, where eis\nthe target entity, se and oe are the start and end\nindices of this mention in text Y, respectively. In\nother words, ⟨wse,...,w oe⟩specially corresponds\nto entity e. For entities with multiple mentions in\ntext, we only keep the ﬁrst mention of each entity\nin M. By replacing each word of mentions with\nthe token “[MASK]”, we can obtain a masked text,\ndenoted as Y[mask], which is also taken as input for\nrepresentation alignment in Section 4.1.\nIn practice, it is difﬁcult to collect massive pairs\n1560\nKG Reconstruction\nDecoder\n(Higher Layers)\nText Encoder\n(Lower Layers)\nKG Encoder\n(GNN)\nRBFS Strategy\nText\nGeneration\nPre-trained\nLanguage\nModel\n<MASK> <MASK> is a fictional\nsuperhero wearing a suit of armor\nand his alter ego is <MASK>\n<MASK>. He was created\nby <MASK> <MASK>.\nRA\nIron Man is a fictional superhero\nwearing a suit of armor and his\nalter ego is Tony Stark. He was\ncreated by Stan Lee. Stan LeeTony Stark\nIron Mancreator\nalias\nKG\nStan LeeTony Stark\nIron Mancreator\nalias\nKG\nNo BP\nIron Man, Tony Stark, Stan Lee\nFigure 2: Overview of our proposed model. “RA” and\n“BP” denote representation alignment and back propa-\ngation, respectively. We organize the PLM into lower\nlayers and higher layers. The former provides PLM-\nbased entity representations for alignment with KG en-\ncodings, and the latter acts as a decoder for generating\ntext and reconstructing KG facts. After representation\nalignment, KG embeddings can be directly fed into the\nhigher layers of PLMs for generating text.\nof KG and its descriptive text for training. In this\npaper, we study the task of few-shot KG-to-text\ngeneration with a handful of training instances (e.g.,\n200 instances) based on a given PLM (e.g., GPT-2).\n4 Approach\nFor our task, two major challenges are how to\nlearn effective input representations and capture\nthe semantic correspondence between KG and text.\nTo address the two challenges, we propose three\nmajor technical contributions, namely representa-\ntion alignment between KG encodings and PLMs,\nrelation-biased BFS strategy for KG linearization,\nand multi-task learning with KG reconstruction.\nFigure 2 presents an illustrative overview of our\nmodel. Next we will describe each part in detail.\n4.1 Representation Alignment\nUnlike previous works (Ribeiro et al., 2020b; Yang\net al., 2020) that directly transform KG into text se-\nquence, we employ graph neural network (GNN) as\nknowledge graph encoder to explicitly encode en-\ntity relations in KG. Based on the input KG, GNN\nwould produce a set of entity embeddings, which\ncan be regarded as the input word embeddings of\nPLM for generating text. However, the GNN-based\nentity embeddings and the PLM-based word (en-\ntity) embeddings come from two distinct semantic\nspaces. To bridge such a semantic gap, we pro-\npose a representation alignment method to align\nthe GNN-based and PLM-based entity embeddings\nin different semantic spaces.\nKG Encoder. The GNN-based KG encoder aims\nto generate entity embeddings for KG. Let ve ∈\nRdE denote the entity embedding for a general en-\ntity ein KG, where dE is the embedding size. In\nour work, the entity embeddings are shared across\ndifferent KGs and initialized with pretrained KG\nembeddings (Yang et al., 2015). We apply R-\nGCN (Schlichtkrull et al., 2018) to generate entity\nembeddings by leveraging multi-relational infor-\nmation in KG. Then, the embedding of entity eat\nthe l+ 1-th layer of R-GCN can be computed as:\nv(l+1)\ne = σ(\n∑\nr∈R\n∑\ne′∈Nre\nW (l)\nr v(l)\ne′ +W (l)\n0 v(l)\ne ), (1)\nwhere W (l)\n0 and W (l)\nr are trainable matrices, and\nNr\ne = {e′|⟨e,r,e′⟩,⟨e′,r,e⟩∈G} denotes the set\nof neighbors of entity eunder relation r. Finally,\nafter stacking Ltimes, the output entity embedding\nv(L)\ne from the last R-GCN layer is used as the ﬁnal\nentity embedding ˜ve.\nNote that, we represent an entity as a set of nodes.\nFor instance, the entity Iron Man in Figure 1 will\nbe represented by two nodes: one for the token\nIron and the other for the token Man. This would\nenhance the generalization ability of KG encoder\non unseen entities, since it learns entity embeddings\nat the token level.\nText Encoder. To obtain the PLM-based entity\nembeddings, we feed the masked text Y[mask] into\nthe text encoder, i.e., the lower layers of PLM. As\nshown in Figure 1, compared with short entity men-\ntions, the masked text contains rich context infor-\nmation about entities. Therefore, similar to masked\nlanguage model (Devlin et al., 2019), the embed-\ndings of masked text can be computed as:\n⟨ˆvw1 ,..., ˆvwT ⟩= Lower-Layers(Y[mask]), (2)\nwhere the entity mention me corresponds to the\nembedding sequence ⟨ˆvwse ,..., ˆvwoe ⟩and the PLM-\nbased entity embedding ˆve can be computed by an\naverage pooling over this embedding sequence.\n1561\nTo bridge the semantic gap, we model the repre-\nsentation alignment by minimizing the Euclidean\ndistance in semantic space between the GNN-based\nand PLM-based entity embeddings as:\nLRA =\n∑\ne∈E\n∥˜ve −ˆve∥2, (3)\nwhere ˜ve and ˆve are GNN-based and PLM-based\nentity embeddings, respectively.\nWith representation alignment, the GNN-based\nentity embeddings can be aligned with the PLM-\nbased entity embeddings in semantic space, which\nenables us to effectively inject KG representations\ninto PLM for improving generation quality.\n4.2 Knowledge Graph Linearization\nTo feed the KG into decoder (i.e., the higher lay-\ners of PLM), we need to linearize KG into an en-\ntity sequence. Previous work (Yang et al., 2020;\nRibeiro et al., 2020b) usually relies on random or\npre-deﬁned rules, which is not ﬂexible to model\nKG structures. Here, we propose to utilize breadth-\nﬁrst search (BFS) strategy to traverse KG. BFS, a\ngraph traversal algorithm, starts at the root node\nand explores all the nodes at the present layer be-\nfore moving on to the nodes at the next depth layer1.\nHere, we assume that nodes at the same layer po-\ntentially express relevant semantics and should be\nplaced in close positions of the entity sequence.\nFurthermore, we observe that some relations are\noften lexicalized before others, e.g., the nationality\nof a person often precedes the birthplace in descrip-\ntive text. Considering such relation priority, in this\npaper, we propose a relation-biased breadth ﬁrst\nsearch (RBFS) strategy to traverse and linearize\nKG into entity sequence. Speciﬁcally, we ﬁrst com-\npute RBFS weights αe′ for each entity e′based on\ntheir relations as:\nαe′ = σ(˜v⊤\ne W (L)\nr ˜ve′), ⟨e,r,e′⟩∈G , (4)\nwhere W (L)\nr is a relation matrix from Eq. 1. Then,\nfor two sibling entities e′and e′′at the same layer,\nwe traverse e′before e′′if αe′ is greater than αe′′,\nand vice versa. Finally, through RBFS, we can\nobtain a linearized entity sequence taken as input\nof the decoder for text generation.\n4.3 KG-enhanced Multi-task Learning\nAfter obtaining the linearized entity sequence, we\nnext take it as input and perform text generation.\n1https://en.wikipedia.org/wiki/Breadth-ﬁrst_search\nDifferent from other text generation tasks, KG-to-\ntext generation aims to generate text reﬂecting the\nconcise facts in KG. Inspired by Liu et al. (2019),\nwe incorporate an auxiliary KG reconstruction task\nto reconstruct the facts in KG for learning the se-\nmantic correspondence between text and KG.\nText Generation. The text generation task is per-\nformed upon the higher layers of PLM. The objec-\ntive is to maximize the likelihood of the reference\ntext, which is equivalent to minimize the negative\nlog-likelihood as:\nLLM = −\nT∑\nj=1\nlog pgen(wj|w1,...,w j−1; G), (5)\nwhere pgen is the generative probability from PLM.\nBesides, in KG-to-text generation, some tokens in\ndescriptive text correspond to KG entities shown\nin Figure 1. The ability to copy entities from KG\nwould enrich the generated text content, which can\nbe achieved by the pointer generator (See et al.,\n2017). By feeding the hidden states of PLM and\nthe token embedding, the copy probability pj\ncopy of\nthe j-th token wj can be computed as:\npj\ncopy = σ(W1sj + W2vwj + bcopy), (6)\nwhere W1, W2, and bcopy are trainable parameters,\nvwj is the embedding of tokenwj, and sj is the j-th\nhidden state from the top layer of PLM. Then, we\nexplicitly “teach” our model how to switch between\ngeneration and copy via the copy loss as:\nLPG =\n∑\nwj\npj\ncopy +\n∑\nwk\n(1 −pk\ncopy). (7)\nOur intuition is aimed at minimizing the copy prob-\nability pj\ncopy of token wj (generated from vocabu-\nlary) and maximizing the copy probability pk\ncopy of\ntoken wk (copied from KG entities).\nKG Reconstruction. Following Song et al. (2020),\nwe formalize the KG reconstruction task as pre-\ndicting the relations between any two entities. In\ndetail, given a head entity eand a tail entity e′in\ngenerated text, we can obtain the hidden states of\ntheir mentions from the top layer of decoder, i.e.,\n⟨sse,..., soe⟩and ⟨sse′,..., soe′⟩. Then, the entity\nhidden states he and te′ can be computed by an\naverage pooling over their mention hidden states.\nThe probability for a relation ris calculated as:\np(r|e,e′) =softmax(W3[he; te′; he ⊙te′] +b2),\n(8)\n1562\nwhere W3 and b2 are trainable parameters. The\nloss for reconstructing KG is also deﬁned as the\nnegative log-likelihood of all target triples in KG:\nLGR = −\n∑\n⟨e,r,e′⟩∈G\nlog p(r|e,e′). (9)\nBy incorporating the KG reconstruction task, our\nmodel is able to capture the semantic correspon-\ndence between input KG and output text, which\nfurther improves generating faithful text.\nFinally, the total training loss consists of text gen-\neration loss LLM (Eq. 5), copy loss LPG (Eq. 7),\nrepresentation alignment loss LRA (Eq. 3) and KG\nreconstruction loss LGR (Eq. 9) as:\nLtotal = LLM +λ1LPG +λ2LRA+λ3LGR, (10)\nwhere λ1, λ2 and λ3 are combination coefﬁcients.\n4.4 Discussion and Learning\nIn this part, we present the model discussion and\nthe model optimization.\nFew-shot Learning. In few-shot KG-to-text gen-\neration, the key lies in how to bridge the semantic\ngap between KG and PLMs with limited dataset.\nTo achieve this goal, we ﬁrst utilize representation\nalignment in Section 4.1 to align the semantic space\nbetween KG encodings and PLMs, and then intro-\nduce a KG reconstruction task in Section 4.3 to\nfurther learn the semantic correspondence between\ninput KG and output text. Besides, we observe that\nKG entities are often multi-word expressions. To\ndeal with unseen entities in few-shot learning, we\nemploy the Byte Pair Encoding (BPE) (Sennrich\net al., 2016) and sub-word vocabulary (Radford\net al., 2019) to split entity words into smaller se-\nmantic units. Our work is also empowered by the\nexcellent few-shot capacities of PLMs with vast\namounts of world knowledge learned from large-\nscale corpora.\nOptimization. For PLM, we employ BART-Large\nmodel (Lewis et al., 2020). Specially, we adopt the\nﬁrst 6 layers of BART encoder as the lower layers,\nand the remaining 6 layers of BART encoder and\nBART decoder as the higher layers. Note that, the\ntarget text and text encoder will not be used at\ntest time. In particular, the target text is just used\nat training time and encoded as PLM-based entity\nembeddings for representation alignment, while the\nalignment is not needed at test time. We optimize\nall parameters according to the total loss in Eq. 10\nDataset #Train #Valid #Test #Relations\nAGENDA 29,720 1,000 10,000 42\nWebNLG 7,362 1,389 5,427 107\nGenWiki 48,020 1,000 10,000 250\nTable 1: Statistics of three datasets.\nwith the OpenAI AdamW optimizer (Loshchilov\nand Hutter, 2019). The learning rate, batch size, R-\nGCN layers and embedding size are set to 1e-5, 20,\n2 and 1024, respectively. The weights λ1, λ2 and\nλ3 in Eq. 10 are set to 0.7, 0.5 and 0.5, respectively,\naccording to performance on validation set. During\ninference, we apply the beam search method with\na beam size of 8.\n5 Experiments\nIn this section, we ﬁrst set up the experiments, and\nthen report the results and analysis.\n5.1 Experimental Setup\nDatasets. To evaluate our model on few-shot\nKG-to-text generation, we conduct experiments on\nthree benchmarks, including AGENDA (Koncel-\nKedziorski et al., 2019), WebNLG (Gardent et al.,\n2017) and GenWiki Fine (Jin et al., 2020). We\nadopt three large domains ( i.e., Airport , Build-\ning and Food) for WebNLG and two large do-\nmains (i.e., Sports and Games) for GenWiki. Ta-\nble 1 shows the statistics for each dataset. Each\ninstance of these datasets contains a knowledge\ngraph in the form of triples and a target text de-\nscribing the graph. The three datasets have orig-\ninally provided the alignment records from en-\ntity mentions to KG entities. Take an example\nfrom WebNLG dataset “AGENT-1 is located in\nPATIENT-1”: the entity mention is tagged as\n“AGENT-1” and the tag “AGENT-1” maps to the\nentity “11th_Mississippi_Infantry_Monument” in\nKG. If such alignments are not available, we can\nutilize entity linking tools (e.g., NER packages) for\npreprocessing.\nBaselines. We make a comparison against ﬁve KG-\nto-text generation models:\n•GraphWriter (Koncel-Kedziorski et al., 2019)\nintroduces a graph transformer encoder and a se-\nquence decoder for generating text based on KG.\n• CGE-LW (Ribeiro et al., 2020a) proposes a\ngraph-to-text model by combining both global and\nlocal node aggregation strategies.\n1563\nDatasets AGENDA W EBNLG G ENWIKI FINE\n#Metrics B-4 R-L CIDEr Chrf B-4 R-L CIDEr Chrf B-4 R-L CIDEr Chrf\nGraphWriter 15.30 22.03 0.24 38.33 45.84 60.62 3.14 55.53 29.73 55.46 2.68 46.87\nCGE-LW 18.01 25.62 0.33 46.69 48.60 62.52 3.85 58.66 30.67 56.37 3.20 47.79\nCycleGT 20.16 25.77 0.69 48.26 50.20 68.30 3.81 68.91 38.57 59.37 3.50 62.46\nBART-base 22.01 26.44 0.90 48.02 49.81 63.10 3.45 67.65 48.20 59.21 4.02 65.80\nBART-large 23.65 28.76 1.15 50.44 52.49 65.61 3.50 72.00 50.70 61.90 4.51 68.15\nT5-base 20.59 29.41 0.81 48.15 48.86 65.57 3.99 66.08 45.72 58.28 3.74 65.68\nT5-large 22.15 30.68 0.87 48.88 58.78 68.22 4.10 74.40 47.11 60.64 3.74 68.47\nOurs 25.15 35.12 3.23 55.89 61.88 75.74 6.03 79.10 48.46 65.65 5.19 64.00\nTable 2: Performance comparisons of different methods for fully-supervised KG-to-text generation under three\ndomains. B- nand R-nare short for BLEU- nand ROUGE-n. Bold and underline fonts denote the best and the\nsecond best methods (the same as below).\nDatasets AGENDA W EBNLG G ENWIKI FINE\n#Instances 50 100 200 500 50 100 200 500 50 100 200 500\nBART-large 5.71 6.15 7.59 10.71 9.05 15.70 19.38 27.91 9.14 13.38 15.39 24.14\nT5-large 2.69 2.73 4.65 7.52 7.18 14.52 16.88 21.68 6.30 6.36 10.37 17.72\nOurs 6.22 9.40 10.21 17.93 10.60 17.46 20.00 31.79 10.75 14.44 16.84 28.89\nTable 3: BLEU-4 results of different methods for few-shot KG-to-text generation under three domains. To mitigate\nthe randomized effects of samples, we report the average results over ﬁve training runs (the same as below).\nDatasets AGENDA W EBNLG G ENWIKI FINE\n#Instances 50 100 200 500 50 100 200 500 50 100 200 500\nBART-large 14.33 15.28 16.94 20.70 22.57 26.21 30.68 49.34 26.59 29.60 34.56 47.50\nT5-large 14.11 14.17 15.88 21.72 20.80 22.71 24.18 38.36 21.02 21.36 20.07 35.72\nOurs 15.10 16.65 18.88 25.72 24.80 28.38 33.12 55.13 28.02 31.36 38.07 50.72\nTable 4: ROUGE-L results of different methods for few-shot KG-to-text generation under three domains.\n•CycleGT (Guo et al., 2020) jointly learns two\ndual tasks (graph-to-text generation and text-to-\ngraph relation classiﬁcation) via cycle training.\n• BART-Base/Large (Ribeiro et al., 2020b) lin-\nearizes the KG into sequence and applies BART-\nBase/Large (Lewis et al., 2020) to generate text.\n• T5-Base/Large (Ribeiro et al., 2020b) lin-\nearizes KG into a triple sequence and employs\nT5-Base/Large (Raffel et al., 2020) to generate text.\nAmong these baselines, GraphWriter and CGE-\nLW are GNN-based generation models; CycleGT is\nan unsupervised model using cycle training; GPT2-\nBase/Large and BART-Base/Largeare the most rele-\nvant comparisons, which also employ PLMs in KG-\nto-text generation. These baselines were trained on\nthe whole training dataset, i.e., all KG-text pairs.\nFollowing previous few-shot work (Chen et al.,\n2020), we train our model on different few-shot\nsettings with training dataset size ranging from 50,\n100, 200 to 500. All the comparison methods are\noptimized based on validation performance. In\nour model, the entity embeddings of GNN are ini-\ntialized with pretrained KG embeddings and the\nGNN weights are transferred from CGE-LW. We\nalso pretrain GNN weights based on the large-scale\nKG, i.e., Wikipedia. Based on the pretrained entity\nembeddings and weights, we continue to train our\nmodel.\nEvaluation Metrics. For performance compari-\nson, we adopt ﬁve automatic evaluation metrics\nwidely used by previous graph-to-text work (Guo\net al., 2020), i.e., BLEU (Papineni et al., 2002),\nROUGE (Lin, 2004), CIDEr (Vedantam et al.,\n2015) and CHRF++ (Popovic, 2015). Speciﬁ-\ncally, BLEU-nand ROUGE-ncompute the ratios\nof overlapping n-grams between generated and\nreal text, CIDEr computes the TF-IDF weights for\neach n-gram in generated/real text, and CHRF++\ncomputes F-score averaged on both character- and\nword-level n-grams.\n1564\nModels B-4 R-L CIDEr Chrf\nOurs 31.79 55.13 3.94 57.38\nw/o RA 23.14 41.34 1.90 43.34\nw/o GR 27.56 46.69 2.82 48.90\nw/o PG 29.30 48.66 3.58 53.44\nTable 5: Ablation analysis on WEBNLG dataset.\n5.2 Main Results\nTable 2, 3, and 4 present the fully-supervised and\nfew-shot results of our model and other baselines,\nrespectively.\nFirst, by combining global and local entity con-\ntext, CGE-LW performs better than GraphWriter.\nFurthermore, with two elaborate designed dual\ntasks, CycleGT becomes the best non-PLM base-\nline, outperforming GraphWriter and CGE-LW.\nSecond, as the most direct comparison with our\nmodel, BART-Base/Large and T5-Base/Large per-\nform better than baselines by leveraging encoded\nsemantics in PLMs, which reveals the feasibility of\nutilizing PLMs for KG-to-text generation.\nFinally, we observe that our model achieves the\nbest performance on both fully-supervised and few-\nshot settings. Large-scale PLMs can encode world\nknowledge by reading a large amount of text, mak-\ning it easier to recover KG facts. Given only a\nhandful of examples, the performances of base-\nlines drop drastically, while the performance of\nour model only descents slightly. Furthermore,\nwith only 500 labelled instances, our model im-\nproves over CGE-LW and CycleGT, and achieves\nthe best performance in most cases. Compared to\nthese PLM-based KG-to-text baselines, we adopt\nGNN to explicitly encode KG structure and rep-\nresentation alignment to bridge the semantic gap\nbetween PLM and GNN. This helps produce effec-\ntive semantic representations for few-shot learning.\nFurthermore, we incorporate an auxiliary KG re-\nconstruction task to learn semantic correspondence\nbetween input KGs and output text. These results\nindicate that our model can achieve more superior\nperformance on KG-to-text generation task in a\nfew-shot setting.\n5.3 Detailed Analysis\nNext, we conduct detailed analysis experiments\non our model. We only report the test results on\nWEBNLG dataset with 500 training instances due\nto similar ﬁndings in other datasets.\nAblation Analysis. In our ablation study, we eval-\nRBFS-Train RDFS-Train FFS-Train RS-Train\n20\n25\n30\n35BLEU\nRBFS-Test\nRDFS-Test\nFFS-Test\nRS-Test\nFigure 3: Linearization analysis on W EBNLG dataset.\nModels #Supp. ↑ #Cont.↓ Naturalness↑\nGold 4.40 0.36 4.26\nOurs 3.77 1.01 3.96\nBART-Large 3.20 1.90 3.55\nCEG-LW 2.87 2.13 2.56\nTable 6: Human evaluation on W EBNLG dataset. Co-\nhen’s kappa coefﬁcients for labelling three factors are\nas follows: 0.78, 0.71, and 0.75.\nuate the effect of each loss LPG , LRA and LGR on\nthe overall model performance. Here, we consider\nthree variants:\n• w/o PG : the variant removes the copy loss\nLPG .\n•w/o RA: the variant removes the representation\nalignment loss LRA.\n• w/o GR: the variant removes the KG recon-\nstruction loss LGR.\nAs can be seen from Table 5, by removing any\nof the three losses, the BLEU/ROUGE/CIDEr per-\nformance drops compared to the complete model,\nespecially removing LRA and LGR. The proposed\nrepresentation alignment bridges the semantic gap\nbetween PLM and GNN, which is helpful for adapt-\ning KG representations to PLM. The KG recon-\nstruction task learns the correspondence between\nKG and text ensuring faithful generation about KG.\nWe also observe a small performance drop by re-\nmoving LPG . It is likely because PLM has learned\nsome common phrase expressions about these KG\nfacts from large-scale pretraining corpus.\nKG Linearization Analysis. In Section 4.2, we\npropose a novel relation-biased BFS (RBFS) strat-\negy to linearize the input KG into entity sequence.\nTo verify the effectiveness of this strategy, we con-\nduct linearization analysis by comparing RBFS\nwith three traversal strategies, including relation-\nbiased depth-ﬁrst search (RDFS), forest ﬁre search\n(FFS) and random search (RS). Speciﬁcally, RDFS\ncombines both DFS and the relation factor similar\n1565\nReal\nKnowledge\nGraph\nasam pedas\nmalaysia\nsumatra and malay peninsula\nputrajava\nmalaysian malay\nmalaysian chinese\nregion\ncountry\nethnicgroup\nethnicgroup\ncapital\nathens\ngreece\nathens international airport\ngreek language\nalexis tsipras\nnikos voutsis\ncityserved\ncountry\nleadername\nleadername\nlanguage\nReference\nasam pedasis a food found in the region of\nsumatra and malay peninsulain malaysia ,\nthe capital of which is putrajaya , and whose\nethnic groups include malaysian malayand\nmalaysian chinese.\nathens international airportserves the\ncity athens in greece , greek languageis\nspoken in greece and the leaders names in\ngreece are alexis tsiprasand nikos voutsis.\nBART\nLinearized\nKG 1⃝3⃝→1⃝2⃝→1⃝6⃝→2⃝5⃝→2⃝4⃝ 1⃝2⃝→2⃝4⃝→2⃝5⃝→1⃝3⃝→2⃝6⃝\nGenerated\nText\nasam pedasis a dish from malaysia and\nsumatra where the capital is putrajava .\nmalaysian malayand chinese are ethnic\ngroups in sumatra .\nathens in greece is led by alexis tsipras\nand is served by athens international\nairport greece speaks greek language.\nOurs\nLinearized\nKG 1⃝→3⃝→2⃝→6⃝→5⃝→4⃝ 1⃝→3⃝→2⃝→6⃝→5⃝→4⃝\nGenerated\nText\nasam pedascomes from the region of sumatra\nand malay peninsulain malaysia , where the\ncapital is putrajava , malaysian malayand\nmalaysian chineseare ethnic groups .\nathens is served by athens international\nairport in greece , which speaks greek\ntextbflanguage . greece is led by alexis\ntsipras and nikos voutsis.\nTable 7: Sample text generated by BART-Large baseline and our model from theFood and Airport domains of the\nWEBNLG benchmark. Since BART linearizes KG as triple sequence and an entity may involve in several triples,\nthere are repeated entities used by BART (we omit the relations between entities). Bold and underlined words\ncorrespond to entity words and keywords.\nto RBFS, where DFS starts at the root node and\nexplores as far as possible along each branch be-\nfore backtracking2; FFS is a randomized version\nof RBFS randomly exploring all the nodes at the\nsame layer (Leskovec and Faloutsos, 2006); and\nRS randomly traverses all the nodes in the input\nKG. By re-training our model with the above three\nstrategies, we report the comparison of BLEU re-\nsults in Figure 3. It can be observed that, RBFS and\nFFS strategies achieve better results compared to\nthe rest strategies. Nodes at the same layer tend to\nexpress more relevant semantics, thus searching by\nlayer could produce more reasonable and coherent\nentity sequence especially considering the relations\nof entities as our RBFS strategy.\nHuman Evaluation. Following previous work in\ndata-to-text (Chen et al., 2020), we conduct human\nevaluation on the generated text. We randomly sam-\nple 200 KG subgraphs along with corresponding\ngenerated text from CGE-LW, BART-Large and our\nmodel. In order to reduce the variance caused by\nhuman, three workers were asked to score the text\nwith respect to two aspects: Factual correctness\nand Language naturalness. The ﬁrst criterion eval-\nuates how well the generated text correctly conveys\n2https://en.wikipedia.org/wiki/Depth-ﬁrst_search\ninformation in the KG, by counting the number\nof facts in text supported by the KG (denoted as\n#Supp.) and contradicting with or missing from the\nKG (denoted as #Cont.). The second criterion eval-\nuates whether the generated text is grammatically\ncorrect and ﬂuent. The scoring mechanism adopts\na 5-point Likert scale (Likert, 1932), ranging from\n1-point (“very terrible”) to 5-point (“very satisfy-\ning”). We further average the three scores from\nthe three human judges over the 200 inputs. The\nresults in Table 6 show that our model produces\nmore ﬁdelity and ﬂuent texts than previous models.\nIn our approach, the KG reconstruction task and\npointer generator enhance the awareness of KG\nfacts and alleviate producing incorrect facts. Also,\nwith some learned common phrase expressions in\nPLMs, our model can generate natural text while\nkeeping ﬁdelity.\nQualitative Analysis. In this part, we present\nintuitive explanations why our model performs\nwell. Table 7 presents two descriptions and the\ncorresponding generated entity sequences and texts\nby BART-Large baseline and our model. As we\ncan see, based on KG linearization, the generated\ntexts by our model show reasonable and similar\ncontent sketch with real texts (e.g., peninsula (re-\ngion)→malaysia (country)→putrajava (capital)).\n1566\nBesides, the baseline model incorrectly merges and\ngenerates unfaithful facts (e.g., malaysia and suma-\ntra) or misses facts (e.g., nikos voutsis), while our\nmodel describes all the KG facts correctly. This\nimprovement could be attributed to the KG recon-\nstruction task, which enables our model to learn\nthe correspondence between the input KG facts and\noutput text. Finally, the entity words in our gener-\nated text are enriched and connected by meaningful\nkeywords (e.g., entity greek language and keyword\nspeaks). The reason might be that, with the help of\nrepresentation alignment, the GNN entity embed-\ndings are aligned with the PLM word embeddings.\n6 Conclusion\nThis paper presented a few-shot KG-to-text gen-\neration model based on PLMs. We make three\nimportant technical contributions, namely repre-\nsentation alignment for bridging the semantic gap\nbetween KG encodings and PLMs, relation-biased\nKG linearization for deriving better input KG repre-\nsentations, and multi-task learning for learning the\ncorrespondence between KG and text. Extensive\nexperiments on three benchmark datasets demon-\nstrate the effectiveness of our few-shot KG-to-text\ngeneration model. As future work, we will con-\nsider adopting KG-enhanced PLMs (Zhang et al.,\n2019; Peters et al., 2019) for improving the task\nperformance, which explicitly inject knowledge\ninformation into PLMs.\nAcknowledgement\nThis work was partially supported by the Na-\ntional Natural Science Foundation of China un-\nder Grant No. 61872369 and 61832017, Bei-\njing Academy of Artiﬁcial Intelligence (BAAI)\nunder Grant No. BAAI2020ZJ0301, Beijing Out-\nstanding Young Scientist Program under Grant No.\nBJJWZYJH012019100020098, the Fundamental\nResearch Funds for the Central Universities, and\nthe Research Funds of Renmin University of China\nunder Grant No.18XNLG22 and 19XNQ047. Xin\nZhao is the corresponding author.\nReferences\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Informa-\ntion Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual.\nZhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu,\nand William Yang Wang. 2020. Few-shot NLG with\npre-trained language model. In Proceedings of the\n58th Annual Meeting of the Association for Com-\nputational Linguistics, ACL 2020, Online, July 5-\n10, 2020 , pages 183–190. Association for Compu-\ntational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171–4186. Association for Computa-\ntional Linguistics.\nJeffrey Flanigan, Chris Dyer, Noah A. Smith, and\nJaime G. Carbonell. 2016. Generation from abstract\nmeaning representation using tree transducers. In\nNAACL HLT 2016, San Diego California, USA, June\n12-17, 2016 , pages 731–739. The Association for\nComputational Linguistics.\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\nand Laura Perez-Beltrachini. 2017. The webnlg\nchallenge: Generating text from RDF data. In Pro-\nceedings of the 10th International Conference on\nNatural Language Generation, INLG 2017, Santi-\nago de Compostela, Spain, September 4-7, 2017 ,\npages 124–133. Association for Computational Lin-\nguistics.\nQipeng Guo, Zhijing Jin, Xipeng Qiu, Weinan Zhang,\nDavid Wipf, and Zheng Zhang. 2020. Cy-\nclegt: Unsupervised graph-to-text and text-to-graph\ngeneration via cycle training. arXiv preprint\narXiv:2006.04702.\nShaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Martti-\nnen, and Philip S. Yu. 2020. A survey on knowledge\ngraphs: Representation, acquisition and applications.\nCoRR, abs/2002.00388.\nZhijing Jin, Qipeng Guo, Xipeng Qiu, and Zheng\nZhang. 2020. Genwiki: A dataset of 1.3 million\ncontent-sharing text and graphs for unsupervised\ngraph-to-text generation. In Proceedings of the 28th\nInternational Conference on Computational Linguis-\ntics, COLING 2020, Barcelona, Spain (Online), De-\ncember 8-13, 2020, pages 2398–2409. International\nCommittee on Computational Linguistics.\nRik Koncel-Kedziorski, Dhanush Bekal, Yi Luan,\nMirella Lapata, and Hannaneh Hajishirzi. 2019.\nText generation from knowledge graphs with graph\n1567\ntransformers. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2019, Minneapo-\nlis, MN, USA, June 2-7, 2019, Volume 1 (Long and\nShort Papers) , pages 2284–2293. Association for\nComputational Linguistics.\nIoannis Konstas and Mirella Lapata. 2013. Inducing\ndocument plans for concept-to-text generation. In\nProceedings of the 2013 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2013, 18-21 October 2013, Grand Hyatt Seattle,\nSeattle, Washington, USA, A meeting of SIGDAT, a\nSpecial Interest Group of the ACL, pages 1503–1514.\nACL.\nRémi Lebret, David Grangier, and Michael Auli. 2016.\nNeural text generation from structured data with ap-\nplication to the biography domain. In EMNLP 2016,\nAustin, Texas, USA, November 1-4, 2016 , pages\n1203–1213. The Association for Computational Lin-\nguistics.\nJure Leskovec and Christos Faloutsos. 2006. Sampling\nfrom large graphs. In Proceedings of the Twelfth\nACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining, Philadelphia, PA,\nUSA, August 20-23, 2006, pages 631–636. ACM.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2020, Online, July 5-10, 2020 ,\npages 7871–7880. Association for Computational\nLinguistics.\nJunyi Li, Tianyi Tang, Gaole He, Jinhao Jiang, Xi-\naoxuan Hu, Puzhao Xie, Zhipeng Chen, Zhuohao\nYu, Wayne Xin Zhao, and Ji-Rong Wen. 2021a.\nTextBox: A uniﬁed, modularized, and extensible\nframework for text generation. In ACL.\nJunyi Li, Tianyi Tang, Wayne Xin Zhao, and Ji-Rong\nWen. 2021b. Pretrained language models for text\ngeneration: A survey. In Proceedings of the 30th\nInternational Joint Conference on Artiﬁcial Intelli-\ngence, IJCAI 2021.\nRensis Likert. 1932. A technique for the measurement\nof attitudes. Archives of psychology.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nTianyu Liu, Fuli Luo, Qiaolin Xia, Shuming Ma,\nBaobao Chang, and Zhifang Sui. 2019. Hierarchical\nencoder with auxiliary supervision for neural table-\nto-text generation: Learning better representation\nfor tables. In AAAI 2019, IAAI 2019, AAAI Sym-\nposium on Educational Advances in Artiﬁcial Intelli-\ngence, EAAI 2019, Honolulu, Hawaii, USA, January\n27 - February 1, 2019 , pages 6786–6793. AAAI\nPress.\nRobert Logan, Nelson F Liu, Matthew E Peters, Matt\nGardner, and Sameer Singh. 2019. Barack’s wife\nhillary: Using knowledge graphs for fact-aware lan-\nguage modeling. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5962–5971.\nIlya Loshchilov and Frank Hutter. 2019. Decou-\npled weight decay regularization. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .\nOpenReview.net.\nAmit Moryossef, Yoav Goldberg, and Ido Dagan. 2019.\nStep-by-step: Separating planning from realization\nin neural data-to-text generation. In NAACL-HLT\n2019, Minneapolis, MN, USA, June 2-7, 2019, Vol-\nume 1 (Long and Short Papers) , pages 2267–2277.\nAssociation for Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Compu-\ntational Linguistics, pages 311–318.\nMatthew E. Peters, Mark Neumann, Robert L. Logan\nIV , Roy Schwartz, Vidur Joshi, Sameer Singh, and\nNoah A. Smith. 2019. Knowledge enhanced con-\ntextual word representations. In EMNLP-IJCNLP ,\nHong Kong, China, November 3-7, 2019, pages 43–\n54. Association for Computational Linguistics.\nMaja Popovic. 2015. chrf: character n-gram f-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\nWMT@EMNLP 2015, 17-18 September 2015, Lis-\nbon, Portugal, pages 392–395. The Association for\nComputer Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer.\nLeonardo F. R. Ribeiro, Yue Zhang, Claire Gardent,\nand Iryna Gurevych. 2020a. Modeling global and\nlocal node contexts for text generation from knowl-\nedge graphs. Trans. Assoc. Comput. Linguistics ,\n8:589–604.\nLeonardo FR Ribeiro, Martin Schmitt, Hinrich Schutze,\nand Iryna Gurevych. 2020b. Investigating pre-\ntrained language models for graph-to-text genera-\ntion. arXiv preprint arXiv:2007.08426.\n1568\nMichael Sejr Schlichtkrull, Thomas N. Kipf, Peter\nBloem, Rianne van den Berg, Ivan Titov, and Max\nWelling. 2018. Modeling relational data with graph\nconvolutional networks. In The Semantic Web - 15th\nInternational Conference, ESWC 2018, Heraklion,\nCrete, Greece, June 3-7, 2018, Proceedings, volume\n10843 of Lecture Notes in Computer Science, pages\n593–607. Springer.\nAbigail See, Peter J. Liu, and Christopher D. Man-\nning. 2017. Get to the point: Summarization with\npointer-generator networks. In ACL 2017, Vancou-\nver, Canada, July 30 - August 4, Volume 1: Long\nPapers, pages 1073–1083. Association for Compu-\ntational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2016, August 7-12, 2016, Berlin, Ger-\nmany, Volume 1: Long Papers. The Association for\nComputer Linguistics.\nLinfeng Song, Ante Wang, Jinsong Su, Yue Zhang,\nKun Xu, Yubin Ge, and Dong Yu. 2020. Structural\ninformation preserving for graph-to-text generation.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2020,\nOnline, July 5-10, 2020, pages 7987–7998. Associa-\ntion for Computational Linguistics.\nRamakrishna Vedantam, C. Lawrence Zitnick, and\nDevi Parikh. 2015. Cider: Consensus-based image\ndescription evaluation. In IEEE Conference on Com-\nputer Vision and Pattern Recognition, CVPR 2015,\nBoston, MA, USA, June 7-12, 2015 , pages 4566–\n4575. IEEE Computer Society.\nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng\nGao, and Li Deng. 2015. Embedding entities and\nrelations for learning and inference in knowledge\nbases. In 3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA,\nMay 7-9, 2015, Conference Track Proceedings.\nZixiaofan Yang, Arash Einolghozati, Hakan Inan,\nKeith Diedrick, Angela Fan, Pinar Donmez, and\nSonal Gupta. 2020. Improving text-to-text pre-\ntrained models for the graph-to-text task. In Pro-\nceedings of the 3rd WebNLG Workshop on Natu-\nral Language Generation from the Semantic Web\n(WebNLG+ 2020), Dublin, Ireland (Virtual). Asso-\nciation for Computational Linguistics.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: en-\nhanced language representation with informative en-\ntities. In Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 1441–1451. Association\nfor Computational Linguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8161462545394897
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.6908230781555176
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6607722640037537
    },
    {
      "name": "Natural language processing",
      "score": 0.6579527854919434
    },
    {
      "name": "Language model",
      "score": 0.5544707179069519
    },
    {
      "name": "Graph",
      "score": 0.4716436266899109
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4711393713951111
    },
    {
      "name": "Bridging (networking)",
      "score": 0.4536839723587036
    },
    {
      "name": "Task (project management)",
      "score": 0.4418449401855469
    },
    {
      "name": "Natural language understanding",
      "score": 0.4403766691684723
    },
    {
      "name": "Knowledge graph",
      "score": 0.42864394187927246
    },
    {
      "name": "Machine learning",
      "score": 0.4015677273273468
    },
    {
      "name": "Natural language",
      "score": 0.38962703943252563
    },
    {
      "name": "Theoretical computer science",
      "score": 0.11090439558029175
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}