{
    "title": "A dataset and benchmark for hospital course summarization with adapted large language models",
    "url": "https://openalex.org/W4392736665",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5046938499",
            "name": "Asad Aali",
            "affiliations": [
                "The University of Texas at Austin",
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2807774868",
            "name": "Dave Van Veen",
            "affiliations": [
                "Stanford University",
                "Artificial Intelligence in Medicine (Canada)"
            ]
        },
        {
            "id": null,
            "name": "Yamin Ishraq Arefeen",
            "affiliations": [
                "The University of Texas at Austin"
            ]
        },
        {
            "id": "https://openalex.org/A2081653727",
            "name": "Jason Hom",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A4291043079",
            "name": "Christian Bluethgen",
            "affiliations": [
                "Stanford University",
                "University Hospital of Zurich"
            ]
        },
        {
            "id": "https://openalex.org/A2905526917",
            "name": "Eduardo Pontes Reis",
            "affiliations": [
                "Artificial Intelligence in Medicine (Canada)",
                "Hospital Israelita Albert Einstein",
                "Hospital São Paulo"
            ]
        },
        {
            "id": "https://openalex.org/A2238476999",
            "name": "Sergios Gatidis",
            "affiliations": [
                "Stanford University",
                "Artificial Intelligence in Medicine (Canada)"
            ]
        },
        {
            "id": "https://openalex.org/A5108315552",
            "name": "Namuun Clifford",
            "affiliations": [
                "The University of Texas at Austin"
            ]
        },
        {
            "id": "https://openalex.org/A2945644742",
            "name": "Joseph Daws",
            "affiliations": [
                "University of California San Francisco Medical Center"
            ]
        },
        {
            "id": null,
            "name": "Arash S Tehrani",
            "affiliations": [
                "University of California San Francisco Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A2109996741",
            "name": "Jangwon Kim",
            "affiliations": [
                "Amazon (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2688709558",
            "name": "Akshay S. Chaudhari",
            "affiliations": [
                "Stanford University",
                "Artificial Intelligence in Medicine (Canada)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3120224639",
        "https://openalex.org/W2938712967",
        "https://openalex.org/W2337116044",
        "https://openalex.org/W4386692532",
        "https://openalex.org/W1994256263",
        "https://openalex.org/W4319301505",
        "https://openalex.org/W4384071683",
        "https://openalex.org/W4392193048",
        "https://openalex.org/W6873149012",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W6810081322",
        "https://openalex.org/W6769627184",
        "https://openalex.org/W6802712164",
        "https://openalex.org/W6794243946",
        "https://openalex.org/W4362603438",
        "https://openalex.org/W4386272371",
        "https://openalex.org/W6959356716",
        "https://openalex.org/W6884907361",
        "https://openalex.org/W6903577382",
        "https://openalex.org/W6809646742",
        "https://openalex.org/W6853251322",
        "https://openalex.org/W4322766882",
        "https://openalex.org/W6898505805",
        "https://openalex.org/W6682631176",
        "https://openalex.org/W6679410207",
        "https://openalex.org/W4377010595",
        "https://openalex.org/W4367834585",
        "https://openalex.org/W4391098193",
        "https://openalex.org/W4393157213",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2135303634",
        "https://openalex.org/W4378509449",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4385456320",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W4403420201",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3170450419"
    ],
    "abstract": "Abstract Objective Brief hospital course (BHC) summaries are clinical documents that summarize a patient’s hospital stay. While large language models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for healthcare applications such as synthesizing BHCs from clinical notes have not been shown. We introduce a novel preprocessed dataset, the MIMIC-IV-BHC, encapsulating clinical note and BHC pairs to adapt LLMs for BHC synthesis. Furthermore, we introduce a benchmark of the summarization performance of 2 general-purpose LLMs and 3 healthcare-adapted LLMs. Materials and Methods Using clinical notes as input, we apply prompting-based (using in-context learning) and fine-tuning-based adaptation strategies to 3 open-source LLMs (Clinical-T5-Large, Llama2-13B, and FLAN-UL2) and 2 proprietary LLMs (Generative Pre-trained Transformer [GPT]-3.5 and GPT-4). We evaluate these LLMs across multiple context-length inputs using natural language similarity metrics. We further conduct a clinical study with 5 clinicians, comparing clinician-written and LLM-generated BHCs across 30 samples, focusing on their potential to enhance clinical decision-making through improved summary quality. We compare reader preferences for the original and LLM-generated summary using Wilcoxon signed-rank tests. We further request optional qualitative feedback from clinicians to gain deeper insights into their preferences, and we present the frequency of common themes arising from these comments. Results The Llama2-13B fine-tuned LLM outperforms other domain-adapted models given quantitative evaluation metrics of Bilingual Evaluation Understudy (BLEU) and Bidirectional Encoder Representations from Transformers (BERT)-Score. GPT-4 with in-context learning shows more robustness to increasing context lengths of clinical note inputs than fine-tuned Llama2-13B. Despite comparable quantitative metrics, the reader study depicts a significant preference for summaries generated by GPT-4 with in-context learning compared to both Llama2-13B fine-tuned summaries and the original summaries (P&amp;lt;.001), highlighting the need for qualitative clinical evaluation. Discussion and Conclusion We release a foundational clinically relevant dataset, the MIMIC-IV-BHC, and present an open-source benchmark of LLM performance in BHC synthesis from clinical notes. We observe high-quality summarization performance for both in-context proprietary and fine-tuned open-source LLMs using both quantitative metrics and a qualitative clinical reader study. Our research effectively integrates elements from the data assimilation pipeline: our methods use (1) clinical data sources to integrate, (2) data translation, and (3) knowledge creation, while our evaluation strategy paves the way for (4) deployment.",
    "full_text": "A dataset and benchmark for hospital course\nsummarization with adapted large language models\nAsad Aali, MS1, 2,*, Dave Van Veen, PhD3, 4, Yamin Ishraq Arefeen, PhD2, Jason Hom, MD5,\nChristian Bluethgen, MS, MD5, 6, Eduardo Pontes Reis, MD3, 7, Sergios Gatidis, MD1, 3,\nNamuun Clifford, MSN, FNP8, Joseph Daws, PhD9, Arash S. Tehrani, PhD9, Jangwon Kim,\nPhD10, and Akshay S. Chaudhari, PhD1, 3, 11\n1Department of Radiology, Stanford University, Stanford, CA, USA\n2Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX, USA\n3Center for Artificial Intelligence in Medicine and Imaging, Stanford University, Palo Alto, CA, USA\n4Department of Electrical Engineering, Stanford University, Stanford, CA, USA\n5Department of Medicine, Stanford, CA, USA\n6University Hospital Zurich, Zurich, Switzerland\n7Albert Einstein Israelite Hospital, S ˜ao Paulo, Brazil\n8School of Nursing, The University of Texas at Austin, Austin, TX, USA\n9One Medical, San Francisco, CA, USA\n10Amazon, Seattle, WA, USA\n11Department of Biomedical Data Science, Stanford University, Stanford, CA, USA\n*Corresponding Author:\nName: Asad Aali\nInstitute: Stanford University\nAddress: 1701 Page Mill Rd, Palo Alto, CA, 94304, USA\nEmail: asadaali@stanford.edu\narXiv:2403.05720v5  [cs.CL]  23 Apr 2025\nABSTRACT\nObjective: Brief hospital course (BHC) summaries are clinical documents that summarize a patient’s hospital stay. While large\nlanguage models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for healthcare applica-\ntions such as synthesizing BHCs from clinical notes have not been shown. We introduce a novel pre-processed dataset, the\nMIMIC-IV-BHC, encapsulating clinical note and brief hospital course (BHC) pairs to adapt LLMs for BHC synthesis. Furthermore,\nwe introduce a benchmark of the summarization performance of two general-purpose LLMs and three healthcare-adapted LLMs.\nMaterials and Methods: Using clinical notes as input, we apply prompting-based (using in-context learning) and\nfine-tuning-based adaptation strategies to three open-source LLMs (Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two\nproprietary LLMs (GPT -3.5, GPT -4). We evaluate these LLMs across multiple context-length inputs using natural language\nsimilarity metrics. We further conduct a clinical study with five clinicians, comparing clinician-written and LLM-generated BHCs\nacross 30 samples, focusing on their potential to enhance clinical decision-making through improved summary quality. We\ncompare reader preferences for the original and LLM-generated summary using Wilcoxon Signed-Rank tests. We further\nrequest optional qualitative feedback from clinicians to gain deeper insights into their preferences, and we present the\nfrequency of common themes arising from these comments.\nResults: The Llama2-13B fine-tuned LLM outperforms other domain-adapted models given quantitative evaluation metrics of\nBLEU and BERT -Score. GPT -4 with in-context learning shows more robustness to increasing context lengths of clinical note\ninputs than fine-tuned Llama2-13B. Despite comparable quantitative metrics, the reader study depicts a significant preference\nfor summaries generated by GPT -4 with in-context learning compared to both Llama2-13B fine-tuned summaries and the\noriginal summaries (p < 0.001), highlighting the need for qualitative clinical evaluation.\nDiscussion and Conclusion:We release a foundational clinically relevant dataset, the MIMIC-IV-BHC, and present an\nopen-source benchmark of LLM performance in BHC synthesis from clinical notes. We observe high-quality summarization\nperformance for both in-context proprietary and fine-tuned open-source LLMs using both quantitative metrics and a qualitative\nclinical reader study. Our research effectively integrates elements from the data assimilation pipeline: our methods use (1)\nclinical data sourcesto integrate (2) data translationand (3) knowledge creation, while our evaluation strategy paves the way\nfor (4) deployment.\nKeywords: Natural Language Processing, Machine Learning, Electronic Health Records, Information Storage and Retrieval\nWord Count:3,905\n2\nOBJECTIVE\nClinicians spend significant time on clinical documentation [1–3], a crucial component of data assimilation [4] beginning\nwith data sources such as EHRs and progressing through data translation and knowledge creation where these records\ncan be processed for clinically-relevant machine learning tasks. The discharge summary (clinical note) is a key document\nclinicians refer to in important scenarios, including when a discharged patient presents to the clinic for post-hospitalization\nfollow-up. The brief hospital course (BHC) fulfills an important role within the discharge summary, particularly for long and\ncomplex hospitalizations. Clinicians who are pressed for time may choose to focus on the BHC summary instead of the entire\ndischarge summary. Synthesizing a BHC given a clinical note requires substantial clinician time and expertise, with errors\npossibly causing patient harm [5]. Automating BHC generation is important since discharge summaries can lack essential\ninformation, contain incorrect information, and may be incomplete before follow-up appointments [6]. The success of large\nlanguage models (LLMs) has provided a promising avenue for this clinically important task of BHC synthesis. Having LLMs\ngenerate the first draft of a BHC may lead to fewer errors by reducing cognitive load and fatigue-related mistakes, produce\nhigher-quality summaries by consistently incorporating all relevant clinical details, and substantially reduce clinician time spent\non documentation by automating the initial drafting process [7–9]. However, while LLMs can produce high-quality drafts,\nclinical use should consider automation bias, where these drafts could be accepted as final without a thorough review, requiring\ninterfaces designed to promote critical assessment and evaluation [10].\nBACKGROUND AND SIGNIFICANCE\nRecent advancements in natural language processing (NLP) have been characterized by transformer-based language models\n[11], such as Bidirectional Encoder Representations from Transformers (BERT) [12] and Unidirectional Generative Pre-trained\nTransformer 2 (GPT-2) [13]. The evolution of transformer models involved pretraining on large corpora of text and subsequent\nfine-tuning on domain-specific data, as demonstrated by GPT-3 [14], PaLM [15], and T5 [16]. After pre-training, these LLMs\nare often exposed to supervised fine-tuning given input-output pairs to improve their ability to follow instructions (instruction\nturning) [17]. Following instruction tuning, reinforcement learning with human feedback (RLHF) is employed to align the\nLLM with human preferences [17, 18]. A recent work presented a multi-document brief hospital course dataset to promote the\nadvancement of hospital visit summarization [19].\nAutomating documentation tasks such as BHC synthesis can potentially alleviate clinician workload and improve documen-\ntation quality. While previous works have explored deep learning-based automation for hospital course summarization [20–22],\nno open-source benchmark exists to evaluate state-of-the-art (SOTA) LLM performance on BHC synthesis. Developing a\nbenchmark and standardized framework for evaluating model effectiveness can help advance NLP applications in reducing clin-\nical documentation burdens. Our study addresses this gap by introducing a foundational task-specific dataset, MIMIC-IV-BHC\n[23], which is tailored specifically for BHC summarization. By benchmarking SOTA LLMs on this dataset, we aim to facilitate\nthe development of more efficient and accurate models for brief hospital course summarization. The significance of this work\n3/18\nTable 1.a) A sample from MIMIC-IV-BHC, our novel pre-processed dataset extracted from raw MIMIC-IV notes.\nInput Example\nSEX F\nSERVICE SURGERY\nALLERGIES No Known Allergies\nCHIEF COMPLAINT Splenic laceration\nMAJOR PROCEDURE NONE\nHISTORY OF PRESENT ILLNESS s/p routine colonoscopy this morning with polypectomy (report not available) ...\nPAST MEDICAL HISTORY Mild asthma, hypothyroid\nFAMILY HISTORY Non-contributory\nPHYSICAL EXAM Gen: Awake and alert CV: RRR Lungs: CTAB Abd: Soft, nontender, nondistended\nPERTINENT RESULTS 03:45 PM BLOOD WBC-5.5 RBC-3.95 Hgb-14.1 ...\nMEDICATIONS ON ADMISSION 1. Levothyroxine Sodium 100 mcg PO DAILY 2. Flovent HFA (fluticasone) ...\nDISCHARGE DISPOSITION Home\nDISCHARGE DIAGNOSIS Splenic laceration\nDISCHARGE CONDITION Mental Status: Clear and coherent. Level of Consciousness: Alert and interactive ...\nDISCHARGE INSTRUCTIONS You were admitted to ... in the intensive care unit for monitoring after a ...\nOutput Example\nBRIEF HOSPITAL COURSE Ms. ... was admitted to ... on .... After getting a colonoscopy and polypectomy, she ...\nb) Statistics for the MIMIC-IV-BHC subsets, binned across multiple context length ranges for adaptation tasks.\nContext Range Train/Test Split Input Tokens BHC Tokens\n0 - 1,024 2 ,000/100 711 ±199 104 ±43\n1,024 - 2,048 2 ,000/100 1 ,471 ±275 148 ±36\n2,048 - 4,096 2 ,000/100 2 ,496 ±388 225 ±55\nlies in its potential to reduce documentation errors, improve the quality of clinical summaries, and free up clinician time, which\ncould directly contribute to enhancing patient care and safety.\nMATERIALS AND METHODS\nWe utilize the MIMIC-IV-Note [24] dataset to curate a foundational task-specific dataset, the MIMIC-IV-BHC [23], tailored for\nthe task of BHC summarization. Subsequently, we present an open-source benchmark of LLM performance on this dataset,\nacross a variety of adaptation strategies.\nDataset\nWe utilize a raw clinical data source [4] called MIMIC-IV-Note [24], a compilation of 331,794 de-identified discharge\nsummaries from 145,915 patients admitted to the Beth Israel Deaconess Medical Center. MIMIC-IV-Note [24] is a publicly\navailable dataset reflective of real-world clinical notes. We pre-process MIMIC-IV [24] notes to create a labeled task-specific\ndataset, the MIMIC-IV-BHC [23] (Table 1), emphasizing the relationship between clinical notes and BHCs. Our open-source\nnovel dataset is crucial to allow researchers to pursue further benchmarking studies. This study was institutional review board\nexempt because the MIMIC dataset does not meet the criteria for human subjects research described by our institution.\n4/18\nFigure 1.A full-length clinical note with its respective clinician-written and LLM-generated BHC (with feedback). This note\nwas sampled from the MIMIC-IV-BHC 0 to 1,024 context range subset. \"Summary 1\" is the actual BHC written by a clinician,\nand \"Summary 2\" is the BHC generated by GPT-4 adapted through in-context learning (ICL). The summaries were presented\nfor feedback to the reader randomly, without specifying “clinician” or “GPT-4”.\nMIMIC-IV-BHC Data Processing\nWe perform data translation[4], a process of transforming raw, unstructured data into a standardized and structured format\nfor downstream machine-learning tasks. This includes steps such as whitespace removal, section identification, tokenization,\nand other transformations that create a uniform structure for the MIMIC-IV-BHC dataset [23]. Using regular expressions and\nnatural language toolkit (NLTK) in Python, we apply: whitespace removal, uniform spacing, section identification, header\nstandardization, extraneous symbol removal, and tokenization. We extract the BHC from each clinical note to create the labeled\nMIMIC-IV-BHC [23] (Table 1) dataset containing 270,033 clinical notes–BHC pairs with an average input token length of\n2,267 ± 914 and an average output token length of 564 ± 410. We apply a uniform set of processing steps across all samples\nand manually review 100 randomly drawn samples from our dataset to validate the effectiveness of the pre-processing pipeline.\n5/18\nThe choice of 100 samples was based on a balance between confirming data quality and the resources available for manual\nreview. Furthermore, the preprocessed dataset underwent an extensive review process before its publication on PhysioNet.\nFinally, our clinicians manually reviewed 30 clinical note-summary pairs from the dataset, without reporting any quality issues\n(Figure 1).\nThe MIMIC-IV-BHC [23] dataset is a function of the context length of the clinical notes. Recognizing that summarizing\nlonger-length clinical notes is a challenging task in the context of text summarization, we further bin the dataset into three\nindependent context length ranges for our adaptation tasks through random sampling as described in Table 1b. For each subset\nin Table 1b, we apply a minimum and maximum context length filter to the clinical notes and a minimum context length filter\nfor the BHCs, helping remove note-BHC pairs with short texts, and ensuring the task’s focus on informative summarization.\nFor fine-tuning tasks, we employ the 2,000 training samples within each context range from Table 1b.\nLarge Language Models\nWe categorize the LLMs used in this study as open-source LLMs and proprietary LLMs as shown in Figure 2.\nOpen-Source LLMs\nThe models in this category were trained either with a sequence-to-sequence (seq2seq) or autoregressive objective. The seq2seq\narchitecture maps input sequences directly to output sequences. In contrast, autoregressive architectures predict the next token\ngiven the preceding context. The following models were considered:\n• Clinical-T5-Large is a \"text-to-text transfer transformer\" [25] pre-trained on medical text, that employs transfer learning\nwith the Sequence-to-Sequence (seq2seq) architecture. Clinical-T5-Large supports a context window of only 512 tokens.\n• FLAN-UL2 hails from the T5 [16] family, utilizing the seq2seq architecture. It employs a modified pre-training procedure\nto incorporate a context length of 2,048 tokens instead of only 512 tokens context length of the original FLAN-T5.\nFLAN-UL2 is further instruction tuned [26] to enhance the model’s capability to understand complex narratives.\n• Llama2-13B stems from the Llama family of LLMs [27] and is an open-source autoregressive model tailored for expanded\npretraining on 2 trillion tokens. Llama2-13B allows up to 4,096 tokens as input.\nProprietary LLMs\nThis category includes large-scale proprietary autoregressive models that utilize reinforcement learning with human feedback\n(RLHF) to further improve performance over instruction tuning using feedback from expert human evaluators.\n• GPT-3.5 (Azure API version GPT-3.5-Turbo-0613) [28] contains 175 billion parameters and has been extensively fine-\ntuned for general tasks using human feedback, enhancing its ability to capture intricate details within any summarization\ntask. This GPT-3.5 version can support input context lengths of up to 16,384 tokens.\n6/18\nFigure 2.Overall schematic of our study. We evaluate a variety ofmodels, including open-source models containing up to 20\nbillion parameters, and larger-scale proprietary models. Each model is adapted to the summarization task using the adaptation\nstrategies displayed (except QLoRA is not applied to GPT-3.5 and GPT-4). We evaluate each model’s performance by\ncomparing its outputs with expert clinician summaries. Each model paired with the adaptation strategy is evaluated using\nquantitative similarity metrics. Finally, we perform a clinical studywhere five clinicians rate three summaries (randomized\norder) for every summarization task: best-performing open-source model, best-performing proprietary model, and\nclinician-written. The ∗ indicates GPT-4’s maximum context length at the time of experimentation (later increased to 128,000).\n• GPT-4 (Azure API version GPT-4-0613) [29] achieves SOTA NLP performance and supports a context window of 32,768\n(later expanded to 128,000) tokens, the highest compared to models listed in Figure 2, making it a suitable choice in\nmulti-document summarization scenarios.\nClinical-T5-Large [25], and FLAN-UL2 [30] have a maximum input context length limit of 512 and 2,048 tokens,\nrespectively. Hence, for the context length analysis in our results section, we only select models that allow at least 4,096 context\nlength inputs, allowing exploration of model behavior for varying extents of the input length.\nAdaptation Strategies\nWe integrate knowledge creation[4] by applying a series of lightweight domain adaptation methods to pre-trained models as\nshown in Figure 2. The adaptation methods mentioned below gradually increase the level of adaptation to a downstream task.\n• Null Prompting: This is a discrete zero-shot adaptation strategy [31] where the clinical note is supplied with a basic\nprompt, such as \"brief hospital course\". This is a baseline technique to evaluate how models respond to minimal guidance.\n7/18\n• Prefix Prompting: Building upon the null prompting approach, we now provide a detailed instructional prompt [26, 31],\nlike \"summarize the clinical note,\" which serves to provide the model with context for the BHC synthesis task.\n• In-Context Prompting: We employ in-context learning (ICL) [26] using a discrete few-shot prompt. ICL involves\nproviding the LLM with examples within the prompt itself, demonstrating a desired behavior [26]. We enhance our null\nprompting strategy by prepending one example clinical note and BHC pair, selected randomly from the training dataset.\n• Quantized Low-Rank Adaptation (QLoRA) [32, 33]: This technique utilizes 4-bit quantization to enable parameter\nefficient fine-tuning [34] by injecting rank-decomposition matrices into each layer of the model. We use QLoRA to\nfine-tune only open-source LLMs on the supervised task of BHC synthesis on a mix of NVIDIA A10G and V100 GPU\nmachines. While model weights for proprietary LLMs are not publicly accessible, fine-tuning is possible through their\nAPI. However, technical details regarding the fine-tuning process remain private, making comparison with QLoRA\nfine-tuning difficult.\nExperimental Setup\nQuantitative Evaluation\nTo identify the best-performing open-source and proprietary models, we conduct a performance analysis by randomly selecting\n70 independent samples from the 100 test samples in the 0-1,024 context range. We reserve the remaining 30 independent\nsamples for qualitative evaluation by clinicians. The 70:30 split between quantitative and qualitative analysis was determined\nto ensure a significant portion of the dataset is used for quantitative evaluation while keeping the workload for clinicians\nmanageable. This separation also mitigates data leakage, providing a more objective assessment of model generalization, as\nthe samples used for quantitative evaluation were not included in the reader study. To accurately assess the performance of\nmodels, we employ quantitative metrics commonly used in summarization tasks to evaluate the syntactic similarity and semantic\nrelevance of the summaries. Bilingual Evaluation Understudy (BLEU) [35] evaluates the overlap between the generated and\nreference summaries using a weighted average of 1 to 4-gram precision. ROUGE-L [36] assesses the precision and recall of\nthe longest common subsequence. We focus only on ROUGE-L instead of other ROUGE variants to avoid redundancy, as it\neffectively captures essential n-gram overlap in our context and correlates well with other ROUGE variants [37]. BLEU and\nROUGE-L depict lexical overlap between generated and reference summaries. In contrast, we also use semantic similarity\nmetrics like BERT-Score [38] which leverages contextual BERT embeddings to evaluate the similarity between the generated\nand reference summaries. Finally, we perform an independent context-length analysis using fine-tuned Llama2-13B and GPT-4\nwith ICL, the best-performing LLMs allowing context lengths greater than 4,000 tokens as input using 100 test samples from\neach of the three context-length test sets from Table 1b. Output lengths were set to the models’ default configurations and were\nnot specifically controlled in this analysis. To assess potential health equity implications, we perform a stratified subgroup\nanalysis across patient-reported sex when assessing the quantitative performance of BHC summarization techniques.\n8/18\nQualitative Evaluation\nTo evaluate the viability ofclinical deployment[4], we conduct a study with five board-certified clinicians to compare BHCs\nwritten by clinicians during clinical practice to BHCs generated by our best-performing open-source and proprietary models\nusing 30 clinical note-BHC pairs. We pair each clinical note with three BHCs: (1) original BHC written by an expert clinician,\n(2) BHC generated by the best-performing open-source LLM, and (3) BHC generated by the best-performing proprietary\nLLM. We present these pairs to five diverse clinicians from different institutions and backgrounds to reflect diversity in clinical\npractice. The clinicians each represent a clinical focus in internal medicine, nursing, thoracic radiology, pediatric radiology, and\nneuroradiology (with 11, 7, 5, 6, and 7 years of experience, respectively). The five clinicians rank the 30 note-summary pairs in\nrandomized and blinded order on a Likert scale of 1 (poor) to 5 (excellent) based on four evaluation criteria:\n• Comprehensiveness: How well does the summary capture important information? This assesses the recall of clinically\nsignificant details from the input text.\n• Conciseness: How well does the summary exclude non-important information? This compares how condensed, consider-\ning the value of a summary decreases with superfluous information.\n• Factual Correctness: How well does the summary agree with the facts outlined in the clinical note? This evaluates the\nprecision of the information provided.\n• Fluency: How well does the summary exhibit fluency? This assesses the readability and natural flow of the content.\nFor the reader study, we gather diverse and well-informed feedback by ensuring that all participating clinicians possess at\nleast five years of experience and a solid understanding of discharge summaries—a standard yet crucial task for clinicians.\nTo reduce potential bias, we randomize the order of the three summaries, labeling them as ’Summary 1,’ ’Summary 2,’ or\n’Summary 3.’ All clinicians see questions in the same order, helping maintain consistency and reduce potential biases. To\nstandardize score values, we provide a detailed rating scale across each aspect. For example, in rating comprehensiveness,\na score of 1 indicates minimal capture of critical information, whereas a score of 5 denotes full coverage of key details.\nFurthermore, we guide clinicians to comparatively assess summaries within each note, ensuring that the scores consistently\nreflect differences in quality. To quantitatively evaluate the consistency of ratings, we calculate Krippendorff’s alpha based on\nall scores provided by clinicians.\nIn total, we received 150 reviews across all five clinicians (30 clinical note-summary pairs for each of the five clinicians).\nAs part of the study, we request optional qualitative feedback from clinicians on each note-BHC pair to dive deeper into the\nreasons behind clinician preferences. Of the 150 reviews, 37 received optional free-text comments from clinicians, while the\nothers were left blank. We analyze all 37 comments to determine insights from observed common themes: 1) Factual Mistakes:\nDoes the summary inaccurately represent critical information from the original note?, 2) Missing Critical Information: Does\nthe summary omit critical information part of the original note?, 3) Hallucinations: Does the summary contain information that\ncannot be inferred from the original note?\n9/18\nFigure 3.Quantitative metric results for each choice of model, across increasing domain-adaptation strategies. In summary,\nQLoRA as an adaptation strategy outperforms other adaptation methods. Specifically, QLoRA Llama2-13B outperforms other\nmodels in BLEU score, while achieving comparable performance to Clinical-T5-Large in BERT-Score and ROUGE-L.\nStatistical Analysis\nWe compare the best-performing proprietary and open-source LLMs with clinician summaries by exploring the following two\nhypotheses across each evaluation criterion: 1) Does our pool of five diverse clinicians exhibit a preference for LLM-generated\nsummaries over clinician-written ones?, and 2) Does our pool of five diverse clinicians exhibit a preference for summaries\ngenerated by adapted proprietary LLMs over summaries generated by adapted open-source LLMs?. We use the non-parametric\nWilcoxon signed-rank tests with a significance level α = 0.05, and with Bonferroni corrections using XLSTAT.\nRESULTS\nModel Performance Analysis\nDespite being the smallest model, Clinical-T5-Large (750M parameters), exhibits competitive performance after QLoRA\nfine-tuning and achieves the largest performance improvement with increasing adaptation (Figure 3). FLAN-UL2 displays\nstrong performance across all adaptation strategies, except ICL. Its improvement in performance with fine-tuning is limited.\nLlama2-13B’s performance improves substantially following QLoRA fine-tuning, achieving the highest BLEU and BERT\nscores across all LLMs and strategies. GPT-3.5 shows good performance but exhibits limited benefits with increasing adaptation.\nGPT-4 with ICL outperforms GPT-3.5, becoming the top-performing proprietary LLM. When evaluating the deviation of each\nmodel’s performance across the subgroups of patient-reported sex, we notice large variations with the Clinical-T5-Large model\n10/18\nFigure 4.a) Quantitative evaluation metrics across increasing input context lengths. GPT-4 shows consistency in performance\nwhereas Llama2-13B shows a drop in summarization with increasing context length inputs. b) Context size analysis for\nQLoRA Llama2-13B (in/out-of-distribution), where each item on the y-axis displays an independent model fine-tuned on\nsamples from a specific context length range. The summarization performance of the combined model trained on 0 - 4,096\ncontext length inputs outperforms other models with longer input clinical notes at inference (more than 1,024 tokens).\n(Figure 3). With increasing adaptation, GPT models show higher variance from their original ROUGE-L and BERT scores.\nLLMs after QLoRA adaptation generally show lower variations among subgroups than other adaptation strategies.\nContext Length Analysis\nWe compare the performance of LLMs when the adaptation (training) and inference (testing) context lengths are identical\n(in-distribution) (Figure 4a). QLoRA Llama2-13B exhibits a drop in in-distribution performance. In contrast, in-context GPT-4\ndisplays consistent performance in-distribution as the context length of the train and test set increases. Overall, in-context\nGPT-4 exhibits more robustness than QLoRA Llama2-13b with increasing context lengths. We explore LLM performance with\ndiffering training and testing context lengths (Figure 4b). We observe that models at testing perform best in-distribution. The\nperformance of models trained on smaller context-length samples deteriorates when testing on longer-context-length samples.\nClinical Reader Study\nWe find that our five clinical readers strongly prefer in-context GPT-4 summaries for attributes of comprehensiveness, con-\nciseness, factual correctness, and fluency, compared to the original clinician-written summaries ( p < 0.001) and QLoRA\nLlama2-13B summaries ( p < 0.001) (Figure 5a). However, we find no significant differences between reader preferences\ncomparing Llama2 summaries and the original clinician-written summaries (p = 0.05). Overall, GPT-4 scores have a narrower\nrange, showcasing lower variation. We report the proportion of scores from readers between the range of 3 to 5 for GPT-4,\nLlama2-13B, and clinician summaries: 100%, 47%, and 40%, respectively. To further explore reasons behind clinician\npreferences, we analyze 37 optional \"free-text\" comments provided by clinicians and report common themes with respective\nfrequencies in Figure 5b. In-context GPT-4 summaries consistently report the least amount of cases across all themes: factual\nmistakes, omission of critical information, and hallucinations. QLoRA Llama2-13B and clinician-written summaries exhibit\n11/18\nFigure 5.a) Violin plot showing results from the reader study with five clinicians. Clinicians exhibit a strong preference for\nin-context GPT-4 (adapted large-scale proprietary LLM) summaries over QLoRA Llama2-13B (adapted open-source LLM) and\nclinician-written summaries with statistical significance by the Wilcoxon signed-rank test (*p < 0.001) across each attribute.\n(NS: Not Significant). b) Plot showing common themes derived from detailed reader comments. This sub-analysis reiterates the\npreference for in-context GPT-4 while exhibiting comparable performance of open-source models and clinicians.\nsimilar frequencies across all themes. As part of the reader study, we observe moderate consistency among raters given the\ninherently subjective nature of the task (Krippendorff’s α = 0.47). Overall, this study underscores the strength of LLMs in\ngenerating summaries that are both statistically superior and preferred by our panel of clinicians in the majority of cases.\nDISCUSSION\nRecent adaptations of LLMs in the clinical domain emphasize their potential in understanding medical language. Noteworthy\nrecent works [7, 9, 39–42] use open-source datasets and display the performance of LLMs in multiple summarization tasks in\nmedicine: radiology reports, patient health questions, progress notes, doctor-patient conversations, and discharge summaries.\nOther works [8, 43] further explore domain adaptation of LLMs in the clinical domain through fine-tuning for improved\nperformance in medical tasks. Our benchmark shows that open-source LLMs can produce high-quality BHCs and that Llama2-\n12/18\n13B comes out as a superior choice for further analysis of open-source LLMs based on its performance and an expanded context\nlength window (Figure 3a). The subgroup analysis (Figure 3b) further emphasizes the importance of a multi-faceted analysis\nwhen evaluating models. Furthermore, our context-length analysis (Figure 4) showcases the benefit of fine-tuning LLMs on\ndatasets with a wide range of context lengths. A deeper qualitative analysis through our reader study (Figure 5) suggests that\nadapted proprietary LLMs like GPT-4 can outperform other adapted LLMs as well as clinicians, while open-source LLMs\nlike Llama2-13B can produce summaries similar in quality to clinician summaries. However, open-source models provide the\nbenefit of ease in implementation, where sending data via an API may not be possible due to privacy constraints. Our benchmark\nalso reports strong summarization performance from Clinical-T5-Large, while it is the smallest parameter model, possibly due\nto extensive pre-training on medical text. However, Clinical-T5-Large also presents a large variation in performance across\nsubgroups, making it less suitable.\nFor our quantitative benchmarking study, we employ a targeted evaluation strategy by including a slew of metrics whose\nindividual strengths cover the weaknesses of other metrics. We reported ROUGE-L, BERT-Score, and BLEU for shorter\ncontext length pairs (0-1,024 tokens) as these metrics remain widely used for evaluating summaries in this length regime. For\nlonger context length pairs (>1,024 tokens), we report only BERT-Score, recognizing the potential shortcomings of ROUGE-L\nfor longer summaries. ROUGE-L primarily captures syntactic matches rather than semantic similarity, which can result in\nlimited content coverage and a preference for summaries that closely follow the original wording [9, 36, 38, 44]. BERT-Score\noffers a more semantic similarity assessment, though it too may underrepresent fine-grained information overlap, particularly\nin long-form texts [38]. We also acknowledge that there is currently no perfect metric for evaluating complex, long-form\nsummarizations in clinical settings [9].\nOur study bridges computational and clinical disciplines by combining quantitative NLP metrics with qualitative clinical\nassessments, ensuring the evaluation reflects both technical performance and clinical relevance. It is crucial to recognize\nthat quantitative measures, such as NLP metrics, and qualitative assessments by clinical readers gauge different aspects of\nsummarization quality. NLP metrics quantify the level of (textual, semantic, contextual) similarity between two summaries,\nwhereas clinical readers inform perceptual (subjective) quality differences. We observe that fine-tuned Llama2-13B consistently\noutperforms in-context GPT-4 in quantitative similarity metrics. However, the reader study results present a nuanced narrative\nwhere clinicians express a preference for GPT-4-generated summaries. This finding reflects a critical point: high quantitative\nsimilarity scores do not necessarily translate to a \"better summary\" from a clinician’s perspective, as well as across subgroups.\nEnhancing the alignment between quantitative metrics and qualitative assessments will enable a more holistic evaluation for\nclinical text summarization [9].\nWhile categorizing models as strictly open-source or proprietary may not comprehensively capture the many differences\nbetween the models, we opt for this categorization because fine-tuning proprietary models is possible but only with unreported\nmechanisms, whereas, fine-tuning open-source LLMs can lead to repeatable and verifiable models. For the broader quantitative\nstudy, we justify the model choice by aiming for diverse models, considering factors like model accessibility and architecture.\n13/18\nIn contrast, for the qualitative study, we select the best model from each of the open-source and proprietary categories based\non their performance in the quantitative study. From the proprietary LLM category, we select GPT-4 (SOTA at the time of\nexperimentation), serving as a robust reference for proprietary models, and GPT-3.5, a smaller, more cost-effective comparison\nwithin the same category. Due to resource constraints and the proprietary nature of fine-tuning mechanisms for GPT-3.5 and\nGPT-4, we limit fine-tuning to open-source models with up to 20 billion parameters (such as Llama2-13B, Clinical-T5-Large,\nand FLAN-UL2), as these can be fine-tuned on consumer-grade GPUs.\nWhile noting the strengths of our study, we also discuss its limitations. The first limitation we observe is that because\nClinical-T5-Large can only accept inputs of less than 512 tokens, the quality of the Clinical-T5-Large generated summaries in\nour experiments might be impacted. However, recent works [39] have demonstrated the strong performance of domain-adapted\nseq2seq models from the T5 family in radiology report summarization, motivating our inclusion of this model. Moreover, based\non prior work [45], given strong decoder models, there is potential to use multi-stage refinement and summarization techniques\nto condense the size of the input prompt itself. Another limitation of our work is the limited availability of publicly available\ndatasets for clinical note summarization, making it difficult to gauge the performance of our adapted LLMs in diverse scenarios\nacross hospitals and clinical practices. The MIMIC-IV dataset is derived from a specific U.S.-based healthcare system, and\nits generalizability to other healthcare systems worldwide may be limited. Similarly, the clinician population in our study\nmay not be fully representative of broader clinical populations. Additional public clinical note datasets may help alleviate\nthese challenges. We also acknowledge that our clinician reader mix, primarily composed of radiology subspecialists with one\nclinical physician, may affect the generalizability of findings, as discharge summaries are typically reviewed by clinical doctors.\nA final limitation of our work is that newer adaptation strategies and LLMs, such as Llama-3 (released after our experimentation\nphase), could not be incorporated into this study. However, our framework, and dataset are designed to be model agnostic,\nenabling future studies to evaluate newer LLM families and adaptation strategies as they emerge. We encourage further research\nto iterate on these advancements using the MIMIC-IV-BHC dataset [23].\nCONCLUSION\nIn this investigation, we develop the MIMIC-IV-BHC [23] dataset and perform a comprehensive quantitative and qualitative\nevaluation of using LLMs for synthesizing BHCs using adaptation strategies for both open-sourced and closed-sourced\nmodels. Via a clinical reader study, we depict that adapted open-source models can match the quality of clinician-written\nsummaries, while adapted proprietary models can outperform the quality of clinician-written summaries across dimensions\nof comprehensiveness, conciseness, factual correctness, and fluency. This suggests LLMs could streamline documentation,\nreduce errors, and enhance clinical workflows, improving patient safety. Overall, this study provides a framework for evaluating\nLLMs, showcasing their potential for reducing clinicians’ documentation burden and improving patient outcomes through\nbetter documentation practices.\n14/18\nCONTRIBUTORSHIP STATEMENT\nAA designed and executed the entire study, led the acquisition and development of the novel dataset, and drafted the manuscript.\nDVV provided essential methodological expertise, contributed to experimental design, and assisted in data analysis and\nmanuscript writing. YIA contributed to the clinical study analysis and assisted in manuscript preparation. JH, CB, EPR, SG, and\nNC provided clinical expertise by reviewing clinical notes with their respective summaries and contributing to the manuscript\nreview. JD, AST, and JK contributed to dataset development, LLM pipeline establishment, and provided critical feedback on\nthe manuscript. ASC oversaw the project and contributed to study design, data interpretation, and manuscript writing. All\nauthors contributed to a critical review of the manuscript and approved the final version.\nFUNDING STATEMENT\nA.C. receives research support from NIH grants R01 HL167974, R01 HL169345, R01 AR077604, R01 EB002524, R01\nAR079431, and P41 EB027060; from NIH contracts 75N92020C00008 and 75N92020C00021; from Stanford Center for\nArtificial Intelligence and Medicine, Stanford Institute for Human Centered AI, from Stanford Center for Digital Health, from\nStanford Cardiovascular Institute, from Stanford Center for Precision Health and Integrated Diagnostics; from GE Healthcare,\nPhilips and Amazon. C.B. received research support independent of this project from the ProMedica Foundation, Chur,\nSwitzerland. Computing resources were partially provided by One Medical and Stanford University. Microsoft provided Azure\nOpenAI credits for this project via the Accelerate Foundation Models Academic Research (AFMAR) program.\nDATA AVAILABILITY STATEMENT\nTo allow researchers to replicate our adaptation and evaluation approach for BHC summarization, we publicly release:\n1. The MIMIC-IV-BHC dataset, published on PhysioNet [23].\n2. The underlying code for data pre-processing, LLM adaptation, and evaluation: github.com/StanfordMIMI/clin-bhc-summ.\nFor this study, we use Microsoft Azure API versions GPT-3.5-Turbo-0613 and GPT-4-0613, as permitted under the\nguidelines set by PhysioNet, following all dataset usage protocols. Furthermore, this study was institutional review board\nexempt because it did not meet the criteria for human subjects research. Due to the use of MIMIC data in fine-tuning, we are\nunable to publicly share the fine-tuned model weights. However, these weights are available to credentialed users of MIMIC\ndatasets upon request.\nCOMPETING INTERESTS STATEMENT\nAll authors declare no financial or non-financial competing interests.\n15/18\nReferences\n[1] Moy AJ, Schwartz JM, Chen R, et al. Measurement of clinical documentation burden among physicians and nurses using\nelectronic health records: a scoping review. J Am Med Inform Assoc2021;28:998–1008.\n[2] Chaiyachati KH, Shea JA, Asch DA, et al. Assessment of inpatient time allocation among first-year internal medicine\nresidents using time-motion observations. JAMA Intern Med2019;179:760–767.\n[3] Mamykina L, Vawdrey DK, and Hripcsak G. How do residents spend their shift time? A time and motion study with a\nparticular focus on the use of computers. Acad Med2016;91:827–832.\n[4] Albers D, Behn CD, and Hripcsak G. Data Assimilation in Medicine. SIAM News. (accessed 12 Aug 2024). 2020.\nhttps://dsweb.siam.org/The-Magazine/All-Issues/data-assimilation-in-medicine .\n[5] Clough RAJ, Sparkes W A, Clough OT, et al. Transforming healthcare documentation: harnessing the potential of AI to\ngenerate discharge summaries. BJGP open2024;8.\n[6] Kripalani S, LeFevre F, Phillips CO, et al. Deficits in communication and information transfer between hospital-based\nand primary care physicians: implications for patient safety and continuity of care. Jama 2007;297:831–841.\n[7] Patel SB and Lam K. ChatGPT: the future of discharge summaries? Lancet Digit Health2023;5:e107–e108.\n[8] Singhal K, Azizi S, Tu T, et al. Large language models encode clinical knowledge. Nature 2023;620:172–180.\n[9] Van Veen D, Van Uden C, Blankemeier L, et al. Adapted large language models can outperform medical experts in\nclinical text summarization. Nat Med2024;30:1134–1142.\n[10] Warraich HJ, Tazbaz T, and Califf RM. FDA Perspective on the Regulation of Artificial Intelligence in Health Care and\nBiomedicine. JAMA 2024.\n[11] Vaswani A, Shazeer N, Parmar N, et al. Attention is All you Need. Adv Neural Inf Process Syst2017;30.\n[12] Devlin J, Chang M.-W, Lee K, et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\nProc Conf North Am Chapter Assoc Comput Linguist2019:4171–4186.\n[13] Radford A, Wu J, Child R, et al. Language models are unsupervised multitask learners. OpenAI blog2019;1:9.\n[14] Brown T, Mann B, Ryder N, et al. Language models are few-shot learners. Adv Neural Inf Process Syst2020;33:1877–\n1901.\n[15] Chowdhery A, Narang S, Devlin J, et al. Palm: Scaling language modeling with pathways. J Mach Learn Res2023;24:1–\n113.\n[16] Raffel C, Shazeer N, Roberts A, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J\nMach Learn Res2020;21:1–67.\n16/18\n[17] Zhang S, Dong L, Li X, et al. Instruction tuning for large language models: A survey. arXiv preprint arXiv:2308.10792\n2023.\n[18] Wang B, Xie Q, Pei J, et al. Pre-trained language models in biomedical domain: A systematic survey. ACM Comput Surv\n2023;56:1–52.\n[19] Adams G, Alsentzer E, Ketenci M, et al. What’s in a summary? laying the groundwork for advances in hospital-course\nsummarization. Proc Conf North Am Chapter Assoc Comput Linguist2021;2021:4794.\n[20] Searle T, Ibrahim Z, Teo J, et al. Discharge summary hospital course summarisation of in patient Electronic Health\nRecord text with clinical concept guided deep pre-trained Transformer models. Journal of Biomedical Informatics\n2023;141:104358.\n[21] Hartman VC, Bapat SS, Weiner MG, et al. A method to automate the discharge summary hospital course for neurology\npatients. Journal of the American Medical Informatics Association2023;30:1995–2003.\n[22] Jung H, Kim Y, Choi H, et al. Enhancing Clinical Efficiency through LLM: Discharge Note Generation for Cardiac\nPatients. arXiv preprint arXiv:2404.051442024.\n[23] Aali A, Van Veen D, Arefeen YI, et al. MIMIC-IV-Ext-BHC: Labeled Clinical Notes Dataset for Hospital Course\nSummarization. PhysioNet. 2024. https://doi.org/10.13026/fh2q-4148.\n[24] Johnson AEW, Pollard TJ, Horng S, et al. MIMIC-IV-Note: Deidentified free-text clinical notes. PhysioNet. 2023.\nhttps://www.physionet.org/content/mimic-iv-note/2.2/.\n[25] Lehman E and Johnson A. Clinical-t5: Large language models built using mimic clinical text. PhysioNet. 2023. https:\n//physionet.org/content/clinical-t5/1.0.0/.\n[26] Lampinen AK, Dasgupta I, Chan SC, et al. Can language models learn from explanations in context? arXiv preprint\narXiv:2204.02329 2022.\n[27] Touvron H, Lavril T, Izacard G, et al. Llama: Open and efficient foundation language models.arXiv preprint arXiv:2302.13971\n2023.\n[28] OpenAI. ChatGPT. Open AI Blog. (accessed 10 Sep 2023). 2022. https://openai.com/blog/chatgpt.\n[29] OpenAI AJ, Adler S, Agarwal S, et al. GPT-4 Technical Report. arXiv preprint arXiv:2303.087742024.\n[30] Tay Y, Dehghani M, Tran VQ, et al. Ul2: Unifying language learning paradigms. arXiv preprint arXiv:2205.051312022.\n[31] Wei J, Wang X, Schuurmans D, et al. Chain-of-thought prompting elicits reasoning in large language models.Adv Neural\nInf Process Syst2022;35:24824–24837.\n[32] Hu EJ, Shen Y, Wallis P, et al. Lora: Low-rank adaptation of large language models.arXiv preprint arXiv:2106.09685\n2021.\n17/18\n[33] Dettmers T, Pagnoni A, Holtzman A, et al. Qlora: Efficient finetuning of quantized llms. Adv Neural Inf Process Syst\n2024;36.\n[34] Ding N, Qin Y, Yang G, et al. Parameter-efficient fine-tuning of large-scale pre-trained language models. Nat Mach\nIntell 2023;5:220–235.\n[35] Papineni K, Roukos S, Ward T, et al. Bleu: a method for automatic evaluation of machine translation.Assoc Comput\nLinguist 2002:311–318.\n[36] Lin C.-Y. Rouge: A package for automatic evaluation of summaries. Assoc Comput Linguist2004:74–81.\n[37] Liu F and Liu Y. Exploring correlation between ROUGE and human evaluation on meeting summaries.IEEE Transactions\non Audio, Speech, and Language Processing2009;18:187–196.\n[38] Zhang T, Kishore V, Wu F, et al. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.096752019.\n[39] Van Veen D, Van Uden C, Attias M, et al. RadAdapt: Radiology report summarization via lightweight domain adaptation\nof large language models. arXiv preprint arXiv:2305.011462023.\n[40] Chen Z, Varma M, Wan X, et al. Toward expanding the scope of radiology report summarization to multiple anatomies\nand modalities. arXiv preprint arXiv:2211.085842022.\n[41] Lyu Q, Tan J, Zapadka ME, et al. Translating radiology reports into plain language using ChatGPT and GPT-4 with\nprompt learning: results, limitations, and potential. Vis Comput Ind Biomed Art2023;6:9.\n[42] Singh S, Djalilian A, and Ali MJ. ChatGPT and ophthalmology: exploring its potential with discharge summaries and\noperative notes. Semin Ophthalmol2023;38:503–507.\n[43] Wang H, Gao C, Dantona C, et al. DRG-LLaMA: tuning LLaMA model to predict diagnosis-related group for hospitalized\npatients. npj Digit Med2024;7:16.\n[44] Koh HY, Ju J, Zhang H, et al. How Far are We from Robust Long Abstractive Summarization? arXiv preprint\narXiv:2210.16732 2022.\n[45] Fleming SL, Lozano A, Haberkorn WJ, et al. Medalign: A clinician-generated dataset for instruction following with\nelectronic medical records. Proc AAAI Conf Artif Intell2024;38:22021–22030.\n18/18"
}