{
  "title": "Auto Completion of User Interface Layout Design Using Transformer-Based Tree Decoders",
  "url": "https://openalex.org/W2998811520",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1993907199",
      "name": "Li Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2749099557",
      "name": "Amelot, Julien",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102210614",
      "name": "Zhou Xin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2746056815",
      "name": "Bengio, Samy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2013341205",
      "name": "Si Si",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963355447",
    "https://openalex.org/W2136384865",
    "https://openalex.org/W2905664673",
    "https://openalex.org/W2963124123",
    "https://openalex.org/W1976373002",
    "https://openalex.org/W2606712314",
    "https://openalex.org/W2741261073",
    "https://openalex.org/W2963073614",
    "https://openalex.org/W2890194927",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W1571082009",
    "https://openalex.org/W2898607334",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2955310349",
    "https://openalex.org/W2962750131",
    "https://openalex.org/W2617273419",
    "https://openalex.org/W2906002453",
    "https://openalex.org/W2909483446",
    "https://openalex.org/W2081428746",
    "https://openalex.org/W2963069010",
    "https://openalex.org/W2963794306",
    "https://openalex.org/W2897966957",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W2960053204"
  ],
  "abstract": "It has been of increasing interest in the field to develop automatic machineries to facilitate the design process. In this paper, we focus on assisting graphical user interface (UI) layout design, a crucial task in app development. Given a partial layout, which a designer has entered, our model learns to complete the layout by predicting the remaining UI elements with a correct position and dimension as well as the hierarchical structures. Such automation will significantly ease the effort of UI designers and developers. While we focus on interface layout prediction, our model can be generally applicable for other layout prediction problems that involve tree structures and 2-dimensional placements. Particularly, we design two versions of Transformer-based tree decoders: Pointer and Recursive Transformer, and experiment with these models on a public dataset. We also propose several metrics for measuring the accuracy of tree prediction and ground these metrics in the domain of user experience. These contribute a new task and methods to deep learning research.",
  "full_text": "Auto Completion of User Interface Layout Design\nUsing Transformer-Based Tree Decoders\nYang Li, Julien Amelot, Xin Zhou, Samy Bengio, Si Si\nGoogle Research\nMountain View, CA 94043\n{liyang, jamelot, zhouxin, bengio, sisidaisy}@google.com\nAbstract\nIt has been of increasing interest in the ﬁeld to develop automatic machineries to\nfacilitate the design process. In this paper, we focus on assisting graphical user\ninterface (UI) layout design, a crucial task in app development. Given a partial\nlayout, which a designer has entered, our model learns to complete the layout by\npredicting the remaining UI elements with a correct position and dimension as well\nas the hierarchical structures. Such automation will signiﬁcantly ease the effort\nof UI designers and developers. While we focus on interface layout prediction,\nour model can be generally applicable for other layout prediction problems that\ninvolve tree structures and 2-dimensional placements. Particularly, we design two\nversions of Transformer-based tree decoders: Pointer and Recursive Transformer,\nand experiment with these models on a public dataset. We also propose several\nmetrics for measuring the accuracy of tree prediction and ground these metrics in\nthe domain of user experience. These contribute a new task and methods to deep\nlearning research.\n1 Introduction\nLayout design is a universal task in many domains, ranging from mechanical design to graphical\nlayouts. There has been a long tradition in developing computer-aided design (CAD) tools in both\nacademia and industry 1(Hurst et al., 2009). Recently, there has been increasing interest in developing\ndeep learning models for design generation (Zheng et al., 2019; Ha and Eck, 2017; Li et al., 2019;\nIsola et al., 2016). In this paper, we focus on models that can automate graphical user interface (GUI)\nlayout design, a central task in modern software development such as smartphone apps.\nAs a designer or app developer creates an interface design, a model recommends UI elements based\non the partial design that has been entered. The automation can signiﬁcantly ease the effort of\ndesigners and developers for creating interfaces, because it not only reduces the input required but\nalso brings design knowledge to the process. This is analogous to auto completion that is widely\navailable in text editing tools, although layout completion is a much more complicated problem. The\ntask is appealing to the deep learning ﬁeld for a number of reasons. First, it is non-linear and involves\n2D placement of elements, instead of the sequential next word prediction for text completion. Second,\nthe problem is highly structural that takes structural input—a partial tree—and generates structural\noutput—a completed tree.\nFor auto-completion of words, a typical choice of solution is using a language model that is trained to\npredict next word wt+1 given previous words w∗\nt+1 = arg maxP(wt+1|w0, ..., wt). As a generative\nmodel, a language model can essentially complete a sentence by predicting the rest words in an\nauto-regressive fashion. Intuitively, we need a layout model—a layout decoder—that predicts the rest\nelements and structures needed to complete a given partial tree.\n1https://balsamiq.com, https://www.sketch.com\narXiv:2001.05308v1  [cs.HC]  14 Jan 2020\nPreviously, models for tree structure generation have been proposed for a number of problems such\nas language syntax trees (Vinyals et al., 2015b) and program generation (Chen et al., 2018; Dai et al.,\n2018). In these problems, the model ﬁrst encodes the source input via an encoder then generates\ntree structures in a target domain via decoding. For layout auto completion, we focus on solely\ninvolving the decoding process, although we can potentially include an encoder to bring in additional\ninformation. Additionally, our problem involves unique aspects of 2D placements.\nWe design our layout decoder model based on Transformer, an model that has gained popularity on\na number of tasks (Vaswani et al., 2017). The attention-based nature of Transformer allows us to\neasily model structures, similar to Pointer Networks (Vinyals et al., 2015a), and represent 2D spatial\nrelationships. In particular, we examine two versions of Transformer-based tree decoder: Pointer\nTransformer and Recursive Transformer for this task.\nLayout completion, as a structural prediction problem, lacks evaluation metrics. We design three sets\nof metrics to measure the quality of layout prediction based on the literature and the domain speciﬁcs\nof user interface interaction. We experimented with these models on a public dataset of over 50K\nAndroid user interface layouts. The experiments indicate that these models can bring values to a user\ninterface layout design process.\n2 Related Work\nRecently, there have been increasing efforts in using deep learning to enhance design practice with\nGenerative Adversarial Networks (GANs) (Goodfellow et al., 2014), Variational Auto Encoders\n(Kingma and Welling, 2014), and Autoregressive models (Reed et al., 2017) as three major underlying\napproaches. There is a rich body of work based on Generative Adversarial Networks (GANs)\n(Goodfellow et al., 2014; Isola et al., 2016; Zhu et al., 2017) where a discriminator is often introduced\nduring training for distinguishing synthesized and real designs, which in the end the model learns\nto generate realistic designs. In particular, LayoutGAN (Li et al., 2019) is the most related to our\nwork in the literature, which takes a collection of randomly parameterized (e.g., positioned and sized)\nelements via an encoder and generates the well-arranged elements via a generator. Although we also\ngenerate 2D layouts, our problem is substantially different because it takes in a tree-structured 2D\npartial layout and outputs a tree-structured 2D full layout. Our model does not see all the elements as\nLayoutGAN does and our input and output involves a tree structure.\nOur approach is related to SketchRNN (Ha and Eck, 2017), a model that can generate sketch\ndrawings. SketchRNN takes a latent representation that is learned via Variational Inference (Kingma\nand Welling, 2014) and generate or complete a drawing using an autoregressive decoder (Reed et al.,\n2017). Similar to SketchRNN, our decoder is also an autoregressive model. The unique challenge in\nour case is the structure representation and generation. We can easily extend our models to take a\nvaritional input as SketchRNN, although we focus on tree 2D decoding in this paper and leave the\nvaritional input to future work. pix2code is another work that is related to our effort, which generates\nUI speciﬁcations from a screenshot pixels (Beltramelli, 2017). It uses a CNN to encode a UI screen\nand generate UI speciﬁcation with an LSTM autoregressive decoder. In this work, a tree structure is\nhandled as a ﬂattened sequence such that the decoding can be handled in the same way as language\nsentences, which a similar treatment is conducted for syntax parse tree prediction (Vinyals et al.,\n2015b). Zhu et al. took one step further to propose an attention-based hierarchical decoder (Zhu\net al., 2018) where two LSTMs are used: one for deciding the blocks that the program needs and the\nother generating tokens within a block. Our recursive Transformer is applied hierarchically as well\nalthough we use the same model repetitively.\nWhile our target domain is user interface layouts, our problem is fundamentally related to structure\nprediction. Previously, much efforts have been devoted to program generation (Chen et al., 2018; Dai\net al., 2018; Si et al., 2019). Jenatton et al. (Jenatton et al., 2017) proposed an approach to predict\ntree structures by using Bayesian optimization to combine independent Gaussian Processes with a\nlinear model that encodes a tree-based structure. An important strategy that was explored previously\nis to extend inherently sequential recurrent models such as LSTM to the hierarchical situation (Tai\net al., 2015), which can handle a range of tree structures. Particularly, Chen et al.’s decoder generates\na binary tree by applying LSTM recursively (Chen et al., 2018), with one LSTM for generating the\nleft child and the other for generating the right child. Dong and Lapata applied LSTM in a recursive\nfashion to generate logic forms from language input (Dong and Lapata, 2016). Based on the previous\n2\nwork, we design our recursive tree decoder based on Transformer (Vaswani et al., 2017) to handle\narbitrary trees. Rather than carrying the hidden state and cell values as recurrent nets, our model can\naccess all the nodes on the ancestry path via attention. Our positional encoding of 2D coordinates is\nsimilar to Image Transformer (Parmar et al., 2018). It is possible to employ tree positional encoding\nas proposed in (Shiv and Quirk, 2019) when we keep the hidden states of all the previously generated\nnodes. Our current design for recursive Transformer decoder is to make the ancestry hidden states\naccessible for decoding child nodes. Another version of the tree decoder we experiment with in the\npaper is designed based on Pointer Networks (Vinyals et al., 2015a) where each node points to its\nparent based on the dot product similarity of their hidden states, which is an simple extension based\non Transformer.\nFinally, the problem we focus on is originated from the domain of human computer interaction (HCI)\nand graphical layout design. A rich body of works have been produced previously (Hurst et al., 2009).\nThe idea of UI auto completion has been previously envisioned (Li and Chang, 2011). DesignScape\nis a full-ﬂedged tool that provides both reﬁnement suggestions and brainstorming suggestions during\ngraphical layout design. Recently, Liu et al. mined a rich set of Android apps that produced over\n66K UI screens with semantically labeled elements (Liu et al., 2018), including their types, bounding\nboxes and the hierarchical tree structures. This dataset lays the foundation for investigating the\nproblem of structured graphical layout prediction, which constitutes the dataset for our work.\nOur work is also nourished by the rich literature in the HCI domain on human performance modeling.\nOne important aspect that is signiﬁcantly underexplored for tree prediction is the accuracy metrics.\nWe design a tree-edit distance metric based on keystroke level GOMS models (Card et al., 1980)\nthat estimates how much effort it needs for a designer to transform the predicted layout tree into\nthe ground-truth tree, which indicates the usefulness of a predicted layout tree. Along a few other\nmetrics, these deﬁne a new problem that deep learning models can further tackle.\n3 UI Layout Completion Problem\nHere, we consider the problem of completing a partial tree, which represents the graphical layout that\nhas been entered by the designer so far (see Figure 1). A design process starts with an empty design\ncanvas, which corresponds to a partial tree with only the root node. Each node has several properties,\nincluding its type, e.g., a button or a list, whether the node is terminal, and the rectangular bounds of\nthe node. As the designer adds more elements to the canvas, the tree grows. Node A is a parent of\nnode B if A is a non-terminal node and A contains B. The problem is extremely challenging due to\nthe large space of possibilities.\nFigure 1: A schematic illustration of UI layout completion problems. The elements with solid\nbounding boxes are already added by the designer, and those with dashed bounding boxes are\npredicted to complete the layout given the solid elements—the partial layout. The tree on the right\nshows the UI structure with the line colors and styles corresponding the UI elements on the left. The\nﬁgure is best viewed in color.\n3.1 Problem Deﬁnition\nA graphical layout tree, ˆG, contains a collection of |ˆG|nodes where its i-th node carries both spatial\nand semantic properties: (ci, di, ti, pi, bi), 1 ≤i ≤| ˆG|and |ˆG|is the number of nodes in the tree.\nci is a categorical value that indicates the node type, ci ∈C where C is the set of possible element\ntypes. di is an integer that represents the depth of the node in the tree. ti is a binary value that\n3\nrepresents whether the node is a terminal node. pi is the index position of the node i’s parent node,\n1 ≤pi ≤| ˆG|. bi is a tuple that represents the bounding box of the node, including its top-left and\nright-bottom corners: bi = (xi, yi, ˆxi, ˆyi).\nA partial tree, ˆP, contains a collection of k nodes (k <|ˆG|). It is required that the root node must\nbe in ˆP. In addition, the parent of each node in ˆP, except the root node, should also be in ˆP, i.e.,\npj ∈ ˆP if j ∈ ˆP, such that the parent index, pj, is always valid and addresses an existing node in\nthe partial tree. This requirement matches how a design tool is typically implemented where each\nUI element (a node) is added to either the design canvas (the root node) or a container node that is\nalready attached to the layout tree.\nGiven the partial tree ˆP, the task here is to predict the full treeˆG\n′\nbased on the partial tree: ˆG\n′\n= F( ˆP).\nWe elaborate on how the function F can be realized with a variety of Transformer decoders in the\nnext section.\n4 Transformer-Based Tree Decoders\nPrevious work for tree prediction is primarily based on LSTM, a recurrent neural network that is\namenable to handle data with an arbitrary length. In this work, we design all our tree decoder models\nbased on Transformer (Vaswani et al., 2017), an architecture that has shown advantages on a number\nof tasks. Particularly, the positional embedding that Transformer’s attentional mechanism is based on\ncan be easily extended to represent 2D spatial relationships. We here discuss three versions of the 2D\nlayout tree decoder model: Vanilla, Pointer and Recursive Transformer.\n4.1 Representing Layout Nodes & Decoding Steps\nBefore diving into each decoder model, we ﬁrst discuss the representation of each node in the layout\ntree, which is shared by all three decoder models. We use the general concept of embedding to\nrepresent each node (Bengio et al., 2003). More speciﬁcally, for node i, we ﬁrst embed each property\nof that node as the following. For the type property, ci, and the terminal property, ti, we embed these\ncategorical values as Equation 1 and 2:\nec\ni = 1 (ci)Ec (1)\net\ni = 1 (li)Et (2)\nwhere ec\ni and et\ni are embedding for ci and ti respectively; 1 (·) is a one-hot vector and Ec ∈R|Vc|×|E|\nand El ∈R|Vt|×|E|are the embedding matrix, where |Vc|and |Vt|are the vocabulary size of each\nproperty, and |E|is the embedding dimension. For each coordinate value in bi, we treat them\nas discrete values and represent them through a similar embedding process. See Equation 3 for\nembedding a coordinate x. A similar treatment is done previously for pixel coordinates in an image\n(Parmar et al., 2018).\nex\ni = 1 (xi)Ex (3)\nwhere Ex ∈R|Vx|×|E|/4 and |Vx|is the number of possible x positions. Similarly, we embed the\nfour coordinates in bi, xi, yi, ˆxi, ˆyi as exi\ni , eyi\ni , e ˆxi\ni , e ˆyi\ni . We concatenate these coordinate embeddings\nto form the bounding box embedding (see Equation 4), which form the positional embedding for\nusing Transformer:\neb\ni = [exi\ni ; eyi\ni ; eˆxi\ni ; eˆyi\ni ]. (4)\nWe then combine these embeddings of node properties to form the ﬁnal embedding for i-th node\nusing Equation 5:\nei = eb\ni + ec\ni + et\ni. (5)\n4\nWith each node represented as an embedding vector, we now discuss how the Transformer decoder\nmodel (Vaswani et al., 2017) is used to represent each decoding step. The Transformer decoder model\nis a multi-layer, multihead attention-based architecture. It computes, hl\ni, the hidden state of step i at\nlayer l by attending to the hidden states of all the steps so far in the previous layer (See Equation 9).\nFor an L-layer Transformer, 0 ≤l ≤L, and h0\ni = ei. hl\ni ∈R|H|where |H|is the dimension of the\nhidden state.\nhl\ni = Attention(Q(hl−1\ni ), K(hl−1\n1:i ), V(hl−1\n1:i )) (6)\nWe use Attention to represent the multihead attention operation in Transformer. Q(·), K(·), and V (·)\nare feedforward nets to compute queries, keys and values for attention computation (Vaswani et al.,\n2017).\n4.2 Vanilla Transformer Decoder\nFor the vanilla version of Transformer Tree Decoder, similar to previous work for predicting language\nparsing trees (Vinyals et al., 2015b), we linearize a tree, based on the preorder depth-ﬁrst traversal, as\na sequence of tokens. The process adds the opening \"(\" and closing \")\" tokens for the children of\neach parent except the case when the parent has a single child, as illustrated in Figure 2 in (Vinyals\net al., 2015b). Note that this representation requires the partial tree to be a preﬁx of the depth-ﬁrst\ntraversal. This data representation transforms tree prediction to a sequence prediction problem.\nWe can then deﬁne the distribution over a sequence ofm decoded tree nodes, nj, k+ 1≤j ≤k + m,\ngiven a sequence of k nodes from a given partial tree ˆP: nj, 1 ≤j ≤k, as the following (see\nEquation 7).\nP(nk+1, nk+2, ..., nk+m|n1, n2, ..., nk) =\nk+m∏\ni=k+1\nP(ni|n1, n2, ..., nk, nk+1, ..., ni−1)\n=\nk+m∏\ni=k+1\nP(ci, ti, xi, yi, ˆxi, ˆyi|n1, n2, ..., nk, nk+1, ..., ni−1)\n=\nk+m∏\ni=k+1\n∏\nz∈{ci,ti,xi,yi, ˆxi, ˆyi}\nsoftmax(WzhL\ni )\n(7)\nwhere ni = (ci, ti, xi, yi, ˆxi, ˆyi). Wz ∈R|Vz|×|H|is the output embedding weights for each property\nof a node where |Vz|is the vocabulary size for the property and |H|is the embedding dimension. The\nmodel does not directly predict pi. Instead, pi is acquired by reconstructing the tree from brackets.\nNote that the opening and closing bracket are considered two special nodes that need to be embedded\nand predicted as well. For these two nodes, only ci matters while other node properties are irrelevant.\n4.3 Pointer Transformer Decoder\nInstead of introducing the beginning and closing tokens to represent tree hierarchies and being limited\nto only the depth-ﬁrst traversal order, Pointer Networks (Vinyals et al., 2015a) provide a natural\nway to represent child-parent relationship. We deﬁne n\n′\ni = (ci, ti, xi, yi, ˆxi, ˆyi, pi) that includes the\nindex position of the node’s parent,pi. We then extend Equation 7 to predict pi as the following (see\nEquation 8).\nP(n\n′\nk+1, ..., n\n′\nk+m|n1, ..., nk) =\nk+m∏\ni=k+1\nP(ci, ti, xi, yi, ˆxi, ˆyi, pi|n1, ..., nk, nk+1, ..., ni−1)\n=\nk+m∏\ni=k+1\n[softmax(HL\n<ihL\ni )\n∏\nz∈{ci,ti,xi,yi, ˆxi, ˆyi}\nsoftmax(WzhL\ni )]\n(8)\n5\nwhere HL\n<i ∈ R(i−1)×|H| is the embedding weights with jth row be hL\nj , 1 ≤ j ≤ i −1.\nsoftmax(HL\n<ihL\ni ) computes a softmax over the dot product alignments between the hidden state\nof the ith node and that of each previous node.\n1\n2\n5\n3 4\n6\n7 8\n1 2 3 4\n4 EOS\nh3h1\nh1 h2 h3\n3 5 6\n6 EOS\nh3’ h5\nLayer 1\nLayer 2\nh3h1\n6 7 8\n8 EOS\nh6’ h7Layer 3\nh4\nh6\nh6\nh8\n7\nancestor states\nancestor states\nFigure 2: A schematic illustration of Recursive Transformer for decoding the tree on the left. The\nsame Transformer model is applied to decoding the children for each parent, top-down from the root.\nThe hidden states of ancestry nodes are used for computing attention in downstream layers. The gray\nboxes represent Transformer layers and the dashed lines denote attention dependencies. The dashed\nnodes are predicted.\n4.4 Recursive Transformer Decoder\nThe third version of decoder models that we investigate here applies a Transformer decoder model in\na recursive manner by using the same model to decode the children of each parent node (see Figure\n2). To do so, we ﬁrst compute the decoder self attention in a similar way to Equation 9, except that\nwe only use the parent and sibling nodes for computing attention keys and values. ˜hl\ns represents the\nhidden state after taking the sth sibling node.\n˜hl\ns = Attention(Q(hl−1\ns ), K(hl−1\n1:s ), V(hl−1\n1:s )) (9)\nTo leverage the information beyond the parent node and siblings, we involve the ancestry nodes in\nthe attention computation by attending to their hidden states.\nhl\ns = Attention(Q(˜hl\ns), K(HL\nA), V(HL\nA)) (10)\nwhere Adonates the set of ancestry nodes, and HL\nAare the set of hidden states of the ancestry nodes,\nwhich have been previously computed as the decoding is performed in a top-down manner starting\nfrom the root node. The ﬁrst node fed into the decoder is the parent node. The hidden states hL\ns\ncapture the information beyond the ancestry nodes as these nodes were able to access other nodes in\nthe tree during attention computation. We can then deﬁne the distribution over a sequence of sibling\nnodes given the partial tree ˆP in a similar fashion to Equation 7.\nThe process not only decodes siblings but also produces hidden states that will be accessed in the\ndownstream layers of the tree as ancestry states. The decoding process starts from decoding the the\nchildren of the root node, because the root node that corresponds the blank design canvas is always\ngiven. For each non-terminal nodes, ti = 0, we apply the same transformer model to decode its\n6\nchildren. The size of ancestry state grows linearly with respect to the depth of the tree (see the schema\nillustration in Figure 2). Note that in this model, we do not ﬂatten the tree representation and we do\nnot compute parent pointer either. This design allows us to easily batch the training and decoding for\nmultiple trees at the same time, where multiple trees can be treated as a forest.\n5 Evaluation\nIn this section, we evaluate the tree decoders we propose for UI layout completion. We start by\ndescribing the dataset we use and elaborate on data processing we conducted. We then deﬁne the\nmetrics for this new problem. Lastly, we report on the results of our experiments.\n5.1 Datasets\nWe use a public dataset that includes over 60K UI layouts where each element in a layout is labeled\nwith a set of properties including its type and bounding boxes (Liu et al., 2018). These screens and\nhierarchies capture 25 UI component categories, such as text, buttons lists and icons. For each layout,\na corresponding UI hierarchy represents the tree structure of the UI.\nWe preprocess the data to ﬁlter out ill-formed layouts. When a node is out of screen or outside its\nparent, we remove the entire layout out of the dataset. We also ﬁlter out long-tail examples when a\nlayout has more than 100 nodes in the tree or a parent has more than 30 children. These long-tail\nexamples are a small portion of the entire dataset. The resulted dataset has more than 55K examples.\nThere are 25 type categories, the average number of nodes per layout is 16 (max=49 and min=2),\nand the average depth of each layout tree is 3 (max=5, min=2). We also scale down the coordinate\nvalues to the range of 72x128 for the horizontal and vertical dimensions. At this scale, a layout is still\nreadable for human eyes.\n5.2 Metrics\nAs far as we know, there are no established metrics for the problem of layout design completion. To\nevaluate the quality of each decoder model, we propose three metrics, which address different user\nscenarios.\nLayout Tree Edit Distance The purpose of this metric is to estimate how much effort is required\nfor a designer to transform a predicted layout into the target one. We implement this metric based on\noptimal tree edit distance calculation (Zhang and Shasha, 1989), and ground each tree manipulation\ncost into time effort based on Keystroke-level GOMS model (Card et al., 1980; Kieras, 2019).\nThere are three edit operators in the edit distance calculation: Insertion, Deletion and Change. For\nKeystroke-level GOMS analysis, we assume a typical layout editing environment with all the UI\nelements presented in a palette. For Insertion, the effort involves the designer selecting a target\nelement from the palette, dragging and dropping it onto the design canvas and then resizing it to a\ntarget dimension. For Change, if the type is incorrect, the designer needs to right click on the element\nand then select a target type. If the location or size is different, the designer needs to correct them by\ndragging and dropping as well. Lastly, for Deletion, since the designer can ignore the predictions, the\ncost is set to minimal. Note that these cost analyses should not be treated as an absolute measure of\neffort. Rather, this metric should be considered as a relative measure to compare models.\nParent-Child Pair Retrieval Accuracy The other measure we propose is to treat the task as an\ninformation retrieval problem (IR) in which we measure how well the model can predict parent-child\npairs against the set of pairs in the ground-truth tree. This metric is less sensitive to the overall\ntree structure and is only concerned with local layout structures. A parent-child node pair, (p, s), is\ndeﬁned as a concatenation of their type and spatial properties, (cp, xp, yp, ˆxp, ˆyp, cs, xs, ys, ˆxs, ˆys).\nA pair is successfully retrieved only if all the values in the predicted tuple match a ground-truth tuple.\nWith this formation, we can compute scores such as precision, recall and their combination F1 scores.\nNext-Element Prediction Accuracy Because it is challenging to predict the entire tree, we look\ninto the metrics that can capture how well a model predicts next element the designer needs. This\nis analogous to next word prediction in language models. For this metric, given a preﬁx of the\npreorder or breadth-ﬁrst traversal sequence of the ground-truth layout tree, we test if the next element,\nimmediately following the preﬁx, is predicted by the model.\n7\n5.3 Experiments\nModel Conﬁguration & Training We implemented these models in TensorFlow based on the\nTransformer implementation in Tensor2Tensor2. We split the dataset for training (80%), valida-\ntion (10%) and test (10%). Based on the training and the validation datasets, we determined an\noptimal model architecture and hyperparameters, including ﬁnding the hidden size in the range of\n[128, 256, 512] and the number of layers [2, 4, 6] as well as tuning on the learning rates and dropout\nratios. We trained each of the models on a single machine with 8 Tesla P100 or V100 GPU cores,\nusing a batch size of 128—the number layout trees. The training strategy is similar to the one\nintroduced by Transformer (Vaswani et al., 2017). We trained these models until they converge.\nPartial Tree Setups Our models do not require a design process to be carried out in a certain order.\nHowever, to examine how these models would aid a design, we need to assume a variety of design\nﬂows that a designer might follow in a realistic design process, which would lead to different layout\ntrees.\nOne factor is the order that the designer wants to progress a design structurally: the depth-ﬁrst versus\nthe breadth-ﬁrst strategies. For the depth-ﬁrst strategy, the designer starts by adding the container\nelements (parent nodes) and then ﬁnishes the child elements in the container before moving onto the\nnext group of elements. For the breadth-ﬁrst strategy, the designer will layout top-level elements\nbefore adding more detailed designs to each container. In reality, a designer might alternate between\nthe two, which we leave for future evaluation. All these strategies are assumed for examining the\nmodels although the models can be used for completing a design of an arbitrary order.\nFor each of these ﬂows, we vary the size of the partial layout given to the model, including 10%, 50%\nand 80% of the full tree. These correspond to the portion of a layout design that has been already\nentered by the designer. Given a partial tree, there can be more than one way to complete the layout.\nGiven a 10%, 50% and 80% BFS partial layout, the mean number of completions of the layout is\n2.97, 1.23 and 1.17 respectively. Given a 10%, 50% and 80% DFS partial layout, the mean number\nof completions is 3.63, 1.24, and 1.17 respectively.\nResults As we can see, Recursive Transformer Decoder outperforms the Pointer Decoder for most\ncases when the design ﬂow follows the breadth-ﬁrst traversal (see Table 3), even though Pointer\nDecoder has the direct access to all the partial tree and previously decoded nodes and Recusive\nDecoder does not. Vanilla Decoder is not tested for the BFS case because it is only structured for\nDFS as discussed earlier. For the depth-ﬁrst traversal case (see Table 2), both Pointer and Recursive\noutputform the Vanilla version with a large margin. However, the trend between Pointer and Recursive\ndecoders is less obvious than the BFS case. This is understandable because the Recursive Transformer\ndecodes in a top-down manner, which cannot leverage the given nodes in the partial tree in deeper\nlayers. The depth-ﬁrst traversal tree put Recursive decoder in a disadvantageous position. In contrast,\nPointer decoder has direct access to all the nodes in the given partial tree. Nevertheless, Recursive\nTransformer still consistently outperforms Pointer Transformer on Next Item prediction with a large\nmargin.\nTable 1: The prediction accuracy for the breadth-ﬁrst layout design ﬂow. The F1 score (%) for\nretrieving parent-child pairs, next item accuracy (%), and Edit distances are calculated given 10%,\n50% and 80% of the ground-truth tree as the partial tree. For F1 and Next Item Accuracy, the larger\nis the better, while for Edit Distance, the smaller the better.\nModels BFS 10% BFS 50% BFS 80%\nF1 Next Edit F1 Next Edit F1 Next Edit\nPointer 11.19 12.25 87.40 61.31 16.76 35.78 87.24 27.56 13.55\nRecursive 15.23 18.4 71.26 64.07 26.84 46.47 86.41 36.58 27.22\n2https://github.com/tensorﬂow/tensor2tensor\n8\nTable 2: The prediction accuracy for the depth-ﬁrst layout design ﬂow. The F1 score (%), next item\naccuracy (%), and Edit distances are calculated given 10%, 50% and 80% of the ground-truth tree.\nModels DFS 10% DFS 50% DFS 80%\nF1 Next Edit F1 Next Edit F1 Next Edit\nVanilla 5.35 0.74 234.17 40.4 5.35 138.88 68.82 17.03 84.32\nPointer 13.12 20.21 76.44 65.49 12.03 21.41 88.96 22.38 9.13\nRecursive 15.05 29.02 62.49 60.79 22.93 52.26 84.65 33.16 34.24\nTable 3: The prediction accuracy for the breadth-ﬁrst layout design ﬂow with a random spatial\nordering. The F1 score (%) for retrieving parent-child pairs, next item accuracy (%), and Edit\ndistances are calculated given 10%, 50% and 80% of the ground-truth tree as the partial tree. For F1\nand Next Item Accuracy, the larger is the better, while for Edit Distance, the smaller the better.\nModels BFS 10% BFS 50% BFS 80%\nF1 Next Edit F1 Next Edit F1 Next Edit\nPointer 11.19 12.25 87.40 61.31 16.76 35.78 87.24 27.56 13.55\nRecursive 15.23 18.4 71.26 70.0 17.0 22.46 91.8 30.47 11.79\n6 Discussions\nIn this paper, we introduce a new problem of auto completion for UI layout design. We formulate the\nproblem as partial tree completion, and investigate a range of variations of layout decoders based\non Transformer. The two models we proposed, Pointer and Recursive Transformer, gave reasonable\npredictions (see Figure 3). We also deﬁne task setups and evaluation metrics for examining the quality\nof prediction and report the results based on a public dataset.\nWhile both Pointer and Recursive Transformer clearly outperformed the baseline model, we found\nthat layout completion remains a challenging task. Our model is often able to predict layout structures\nand semantic properties well, but less accurate on bounding boxes. There are many UIs with the\nsame layout structure but different spatial details, while our current eval is using hard metrics. When\nrelaxing the metrics by only considering structures and semantic properties, all the models got much\nbetter accuracy, e.g., Next item for 80% BFS partial (Recursive: 95.3%, Pointer: 92.9%) and for 80%\nDFS partial (Recursive: 93.4%, Pointer: 88.4%, Vanilla: 76.6%). Recursive still shows signiﬁcant\nadvantages over other models. We experimented with continuous coordinate output using squared\nerrors for loss. The results showed that the model with continuous coordinates performs substantially\nworse than treating coordinates as one-hot categorical values, which deserves further investigation.\nReferences\nT. Beltramelli. pix2code: Generating code from a graphical user interface screenshot. CoRR,\nabs/1705.07962, 2017. URL http://arxiv.org/abs/1705.07962.\nY . Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. J.\nMach. Learn. Res., 3:1137–1155, Mar. 2003. ISSN 1532-4435. URL http://dl.acm.org/\ncitation.cfm?id=944919.944966.\nS. K. Card, T. P. Moran, and A. Newell. The keystroke-level model for user performance time\nwith interactive systems. Commun. ACM, 23(7):396–410, July 1980. ISSN 0001-0782. doi:\n10.1145/358886.358895. URL http://doi.acm.org/10.1145/358886.358895.\nX. Chen, C. Liu, and D. Song. Tree-to-tree neural networks for program transla-\ntion. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems 31 , pages\n2547–2557. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/\n7521-tree-to-tree-neural-networks-for-program-translation.pdf .\n9\npartial prediction ground-truth\nFigure 3: Examples of predicted layouts given a partial layout, with respect to the corresponding\ngroundtruth complete layout.\nH. Dai, Y . Tian, B. Dai, S. Skiena, and L. Song. Syntax-directed variational autoencoder for structured\ndata. CoRR, abs/1802.08786, 2018. URL http://arxiv.org/abs/1802.08786.\nL. Dong and M. Lapata. Language to logical form with neural attention. CoRR, abs/1601.01280,\n2016. URL http://arxiv.org/abs/1601.01280.\nI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and\nY . Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. D.\nLawrence, and K. Q. Weinberger, editors,Advances in Neural Information Processing Systems 27,\npages 2672–2680. Curran Associates, Inc., 2014. URL http://papers.nips.cc/paper/\n5423-generative-adversarial-nets.pdf .\nD. Ha and D. Eck. A neural representation of sketch drawings. CoRR, abs/1704.03477, 2017. URL\nhttp://arxiv.org/abs/1704.03477.\nN. Hurst, W. Li, and K. Marriott. Review of automatic document formatting. In Proceedings of\nthe 9th ACM Symposium on Document Engineering, DocEng ’09, pages 99–108, New York,\nNY , USA, 2009. ACM. ISBN 978-1-60558-575-8. doi: 10.1145/1600193.1600217. URL\nhttp://doi.acm.org/10.1145/1600193.1600217.\nP. Isola, J. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial\nnetworks. CoRR, abs/1611.07004, 2016. URL http://arxiv.org/abs/1611.07004.\nR. Jenatton, C. Archambeau, J. González, and M. Seeger. Bayesian optimization with tree-structured\ndependencies. In D. Precup and Y . W. Teh, editors,Proceedings of the 34th International Con-\nference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages\n1655–1664, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR. URL\nhttp://proceedings.mlr.press/v70/jenatton17a.html.\nD. Kieras. Goms models for task analysis. 05 2019.\n10\nD. P. Kingma and M. Welling. Auto-encoding variational bayes. In 2nd International Conference on\nLearning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track\nProceedings, 2014. URL http://arxiv.org/abs/1312.6114.\nJ. Li, J. Yang, A. Hertzmann, J. Zhang, and T. Xu. Layoutgan: Generating graphic layouts with\nwireframe discriminators. CoRR, abs/1901.06767, 2019. URL http://arxiv.org/abs/\n1901.06767.\nY . Li and T.-H. Chang. Auto-completion for user interface design. US Patent, 2011. URLhttps:\n//patents.google.com/patent/US20150169140.\nT. F. Liu, M. Craft, J. Situ, E. Yumer, R. Mech, and R. Kumar. Learning design semantics for\nmobile apps. In Proceedings of the 31st Annual ACM Symposium on User Interface Software\nand Technology, UIST ’18, pages 569–579, New York, NY , USA, 2018. ACM. ISBN 978-1-\n4503-5948-1. doi: 10.1145/3242587.3242650. URL http://doi.acm.org/10.1145/\n3242587.3242650.\nN. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, and D. Tran. Image\ntransformer. In J. Dy and A. Krause, editors, Proceedings of the 35th International Con-\nference on Machine Learning, volume 80 of Proceedings of Machine Learning Research,\npages 4055–4064, Stockholmsmässan, Stockholm Sweden, 10–15 Jul 2018. PMLR. URL\nhttp://proceedings.mlr.press/v80/parmar18a.html.\nS. E. Reed, A. van den Oord, N. Kalchbrenner, S. G. Colmenarejo, Z. Wang, D. Belov, and N. de Fre-\nitas. Parallel multiscale autoregressive density estimation. CoRR, abs/1703.03664, 2017. URL\nhttp://arxiv.org/abs/1703.03664.\nV . L. Shiv and C. Quirk. Novel positional encodings to enable tree-structured transformers, 2019.\nURL https://openreview.net/forum?id=SJerEhR5Km.\nX. Si, Y . Yang, H. Dai, M. Naik, and L. Song. Learning a meta-solver for syntax-guided program\nsynthesis. In International Conference on Learning Representations, 2019. URL https://\nopenreview.net/forum?id=Syl8Sn0cK7.\nK. S. Tai, R. Socher, and C. D. Manning. Improved semantic representations from tree-structured\nlong short-term memory networks. CoRR, abs/1503.00075, 2015. URL http://arxiv.org/\nabs/1503.00075.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin.\nAttention is all you need. CoRR, abs/1706.03762, 2017. URL http://arxiv.org/abs/\n1706.03762.\nO. Vinyals, M. Fortunato, and N. Jaitly. Pointer networks. In C. Cortes, N. D. Lawrence, D. D. Lee,\nM. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28,\npages 2692–2700. Curran Associates, Inc., 2015a. URL http://papers.nips.cc/paper/\n5866-pointer-networks.pdf.\nO. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever, and G. Hinton. Grammar as a foreign\nlanguage. In Proceedings of the 28th International Conference on Neural Information Processing\nSystems - Volume 2, NIPS’15, pages 2773–2781, Cambridge, MA, USA, 2015b. MIT Press. URL\nhttp://dl.acm.org/citation.cfm?id=2969442.2969550.\nK. Zhang and D. Shasha. Simple fast algorithms for the editing distance between trees and related\nproblems. SIAM J. COMPUT, 18(6), 1989.\nX. Zheng, X. Qiao, Y . Cao, and R. W. H. Lau. Content-aware generative modeling of graphic\ndesign layouts. ACM Trans. Graph., 38(4):133:1–133:15, July 2019. ISSN 0730-0301. doi:\n10.1145/3306346.3322971. URL http://doi.acm.org/10.1145/3306346.3322971.\nJ. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent\nadversarial networks. CoRR, abs/1703.10593, 2017. URL http://arxiv.org/abs/1703.\n10593.\nZ. Zhu, Z. Xue, and Z. Yuan. Automatic graphics program generation using attention-based hierar-\nchical decoder. CoRR, abs/1810.11536, 2018.\n11",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7597861289978027
    },
    {
      "name": "Pointer (user interface)",
      "score": 0.5970399379730225
    },
    {
      "name": "Transformer",
      "score": 0.5151296854019165
    },
    {
      "name": "Page layout",
      "score": 0.47903791069984436
    },
    {
      "name": "Graphical user interface",
      "score": 0.436346173286438
    },
    {
      "name": "User interface",
      "score": 0.42492809891700745
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3889577388763428
    },
    {
      "name": "Data mining",
      "score": 0.35611051321029663
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2687522768974304
    },
    {
      "name": "Programming language",
      "score": 0.12761586904525757
    },
    {
      "name": "Engineering",
      "score": 0.12157958745956421
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Advertising",
      "score": 0.0
    }
  ],
  "institutions": []
}