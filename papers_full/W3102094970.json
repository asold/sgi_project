{
  "title": "What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding",
  "url": "https://openalex.org/W3102094970",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3129628085",
      "name": "Yu An Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4208538295",
      "name": "Yun-Nung Chen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2594990650",
    "https://openalex.org/W2995971510",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2150355110",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W46679369",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W1538131130",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2106597925",
    "https://openalex.org/W2963925437"
  ],
  "abstract": "In recent years, pre-trained Transformers have dominated the majority of NLP benchmark tasks. Many variants of pre-trained Transformers have kept breaking out, and most focus on designing different pre-training objectives or variants of self-attention. Embedding the position information in the self-attention mechanism is also an indispensable factor in Transformers however is often discussed at will. Hence, we carry out an empirical study on position embedding of mainstream pre-trained Transformers mainly focusing on two questions: 1) Do position embeddings really learn the meaning of positions? 2) How do these different learned position embeddings affect Transformers for NLP tasks? This paper focuses on providing a new insight of pre-trained position embeddings by feature-level analysis and empirical experiments on most of iconic NLP tasks. It is believed that our experimental results can guide the future works to choose the suitable positional encoding function for specific tasks given the application property.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6840–6849,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n6840\nWhat Do Position Embeddings Learn?\nAn Empirical Study of Pre-Trained Language Model Positional Encoding\nYu-An Wang Yun-Nung Chen\nDepartment of Computer Science and Information Engineering\nNational Taiwan University, Taipei, Taiwan\nr08922019@csie.ntu.edu.tw y.v.chen@ieee.org\nAbstract\nIn recent years, pre-trained Transformers have\ndominated the majority of NLP benchmark\ntasks. Many variants of pre-trained Trans-\nformers have kept breaking out, and most fo-\ncus on designing different pre-training objec-\ntives or variants of self-attention. Embedding\nthe position information in the self-attention\nmechanism is also an indispensable factor in\nTransformers however is often discussed at\nwill. Therefore, this paper carries out an\nempirical study on position embeddings of\nmainstream pre-trained Transformers, which\nmainly focuses on two questions: 1) Do po-\nsition embeddings really learn the meaning of\npositions? 2) How do these different learned\nposition embeddings affect Transformers for\nNLP tasks? This paper focuses on providing\na new insight of pre-trained position embed-\ndings through feature-level analysis and empir-\nical experiments on most of iconic NLP tasks.\nIt is believed that our experimental results can\nguide the future work to choose the suitable\npositional encoding function for speciﬁc tasks\ngiven the application property.1\n1 Introduction\nWord ordering often determines the meaning of\na sentence; therefore how to utilize the position\ninformation of a word sequence has been an impor-\ntant topic in NLP and widely investigated recently.\nA common approach for modeling word ordering\nis to use recurrent neural networks (RNN), such\nas long short-term memory (LSTM) (Hochreiter\nand Schmidhuber, 1997) or gated recurrent unit\n(GRU) (Chung et al., 2014), which use a hidden\nstate to represent the information of an ordered se-\nquence and update model weights by backpropaga-\ntion through time (BPTT) (Werbos, 1990); thus the\n1The source code is available at: https://github.\ncom/MiuLab/PE-Study\nordering information can be modeled by this struc-\nture. However, RNN and BPTT are very inefﬁcient\nin modern GPU computation due to the difﬁculty of\nparallelization with the time dependency. To solve\nthis problem, recent work, such as convolutional\nseq2seq (Gehring et al., 2017) and Transformers\n(Vaswani et al., 2017) which apply convolutional\nneural network (CNN) (LeCun et al., 1995) and\nself-attention respectively, succeed to eliminate the\ntime dependency to take the computational advan-\ntage of GPU. Instead of storing the information of\nordered sequences, these models utilize the posi-\ntion information by using a feature-level positional\nencoding. For example, convolutional seq2seq pro-\nposed learnable position embeddings to represent\nthe positions in a sequence.\nRecently, various pre-trained Transformer lan-\nguage models keep breaking state-of-the-art results\nin numerous NLP tasks. There are many different\nways to pre-train a Transformer language model.\nFor example, using an encoder, decoder, or the\nwhole part of the Transformer, adapting the self-\nattention masks, or training with different objec-\ntives (Devlin et al., 2018; Liu et al., 2019; Radford\net al., 2018, 2019; Lewis et al., 2019; Raffel et al.,\n2019; Yang et al., 2019). However, in terms of po-\nsitional encoding, most work only used a learned\nposition embedding which is originally proposed\nin convolutional seq2seq (Gehring et al., 2017)\nwithout any analysis, even different objectives may\nlearn completely different position information.\nMotivated by the above observations, our goal\nis to investigate what position information the\npre-trained Transformers could learn under differ-\nent settings. We conduct a deep analysis of the\nlearned position embeddings among three iconic\npre-trained Transformer language models: BERT\n(Devlin et al., 2018), RoBERTa (Liu et al., 2019)\nand GPT-2 (Radford et al., 2019). To examine the\nperformance of different NLP types, we conduct\n6841\nthe experiments on text classiﬁcation, language\nmodeling, and machine translation, and empirically\nanalyze and explain the meaning and inﬂuence of\nposition embeddings from different aspects.\nThe contributions of this paper are 3-fold:\n•This paper is among the ﬁrst study that pro-\nvides a complete analysis about what learned\nposition embeddings capture in different pre-\ntrained models.\n•This paper empirically examines the perfor-\nmance of different position embeddings for\nmany NLP tasks.\n•This paper connects the empirical perfor-\nmance with the task property based on the\nanalysis, providing the guidance of the future\nwork for choosing the suitable positional en-\ncoding method in the target task.\n2 Related Work\nThe concept of using position embedding on\nposition-insensitive models was ﬁrst proposed by\nconvolutional seq2seq (Gehring et al., 2017), which\nbuilt an encoder-decoder architecture on convo-\nlutional neural networks. Vaswani et al. (2017)\nproposed Transformers that used the self-attention\nmechanism in the basic blocks. Because the atten-\ntion mechanism is position-insensitive, it proposed\na pre-deﬁned sinusoidal function as positional en-\ncoding. Pre-trained language models became a\ntrend among many NLP tasks after (Peters et al.,\n2018) introduced ELMo. Affected by ELMo, Ope-\nnAI GPT (Radford et al., 2018) is the ﬁrst pre-\ntrained language model using a Transformer archi-\ntecture, then many different variant of pre-trained\nTransformer including BERT (Devlin et al., 2018),\nRoBERTa (Roberts, 2005) and GPT-2 (Radford\net al., 2019) started evolving the researches of NLP\ntremendously. In Transformers, the attention val-\nues are the same in each input position. Thus, Shaw\net al. (2018) proposed a relative position represen-\ntation in the attention level to address this issue.\nDai et al. (2019) used a segment-level recurrence\nmechanism on Transformers and also utilized an\nadaptive version of relative position embeddings\ninspired by Shaw et al. (2018). Furthermore, Wang\net al. (2019) extended the embedding space from\nreal numbers to complex values , and also pro-\nposed a new learnable positional encoding function\ninstead of a simple position embedding mapping.\n3 Transformer\nTransformer is an encoder-decoder sequence-to-\nsequence model proposed by Vaswani et al. (2017).\nIn the architecture, Transformer is composed of\nself-attention blocks that are position-insensitive\nmodules. Therefore, a positional embedding should\nbe considered together with the NLP tasks. To elab-\norate on the experiments we conduct, this section\nbrieﬂy introduces Transformers.\nInput Representation Due to the property of\nposition-insensitive in the attention module, the\ninput representations should also contain the posi-\ntion information. In Transformers (Vaswani et al.,\n2017), a word embedding is directly added with\nthe positional encoding as the ﬁnal representation:\nzi = WE(xi) + PE(i),\nwhere xi is the token at the i-th position, WE is\nthe word embedding, and PE is the positional en-\ncoding, which can be either a learnable embedding\nor a pre-deﬁned function.\nMulti-Head Self-Attention The attention mech-\nanism is often used in an encoder-decoder architec-\nture, and there are many variants of attention im-\nplementations (Bahdanau et al., 2014; Britz et al.,\n2017). In Transformers, the scaled dot-product\nattention is applied:\nattention(Q,K,V ) = softmax(QWKT W√dk\n)VW,\nwhere W is a linear projection and Q, K, V repre-\nsent query, key and value matrices respectively.\nTransformer blocks are composed of multi-head\nself-attention. Literally, the inputs Q, K, V are the\nsame and the attention is performed multiple times,\nand then the output heads are concatenated as the\nﬁnal output hidden state h. This process can be\nformulated as\nheadi = attention(Q,K,V )\nh= concat([head1,..., headn])W.\nTransformer Encoder A Transformer encoder\nlayer is composed of multi-head self-attention\nfollowing a position-wise feed-forward network\n(FFN) with the residual connection (He et al., 2016)\nand layer normalization (Ba et al., 2016):\noutput = layernorm(h+ FFN(h)),\nand then stacked the layers sequentially to form a\nTransformer encoder.\n6842\nTransformer Decoder The Transformer de-\ncoder is also stacked by self-attention blocks, and\nit only has two major differences from the encoder:\n1. Each Transformer decoder layer has an addi-\ntional sub-layer to perform attention on the\nencoder output.\n2. To ensure the decoder can only decode tokens\ndepending on the tokens in the past, it uses an\nattention mask to mask the attention values of\nthe subsequent tokens.\nTherefore, the Transformer decoder can decode\ntokens autoregressively like other conventional lan-\nguage models such as RNN.\n4 Position Embedding Analysis\nIn this section, we conduct feature-level analyses of\nthe pre-trained position embeddings of two Trans-\nformer encoders: BERT (Devlin et al., 2018) and\nRoBERTa (Liu et al., 2019), one Transformer de-\ncoder: GPT-2 (Radford et al., 2019), and also the si-\nnusoidal function proposed by Vaswani et al. (2017)\nis deﬁned as\nPE(i,2j) = sin(i/100002j/dmodel ),\nPE(i,2j+1) = cos(i/100002j/dmodel ),\nwhere iis the position index and jis the dimension\nindex.\n4.1 Do Embeddings Learn the Meaning of\nPositions?\nGiven the position space Pand the embedding\nspace X, the goal of the position embedding func-\ntion is to learn a mapping f : P →X. In the\nfollowing experiments, we focus on answering two\nquestions for better understanding what the embed-\ndings capture:\n1. Can the learned embedding space Xrepresent\nthe absolute positions of the words?\n2. Are Pand Xisomorphic?\n4.1.1 Absolute Position Regression\nIf a position embedding can actually capture its\nabsolute position, it should be easy to reconstruct\na reversed mapping function g : X →P. Thus,\nwe use linear regression to learn a function gthat\ntransfers the embeddings to the original positions.\nThe feature dimension is 768, and the maximum\nType PE MAE\nLearned\nBERT 34.14\nRoBERTa 6.06\nGPT-2 1.03\nPre-Deﬁned sinusoid 0.0\nTable 1: Mean absolute error of the reversed mapping\nfunction learned by linear regression.\nType PE Error Rate\nLearned\nBERT 19.72%\nRoBERTa 7.23%\nGPT-2 1.56%\nPre-Deﬁned sinusoid 5.08%\nTable 2: Error rate of the relative position regression.\nposition in GPT-2 is trimmed from 1024 to 512\nfor comparison which BERT and RoBERTa. Be-\ncause we only have512 data points for each learned\nembedding, a 5-fold cross-validation is applied to\navoid overﬁtting. The reversed mapping functions\nare evaluated by Mean Absolute Error (MAE) ,\nand the result is shown in Table 1.\nFrom the results, the reversed mapping function\nof sinusoid can perfectly represent the absolute\npositions, and GPT-2 only has a small error. In\ncontrast, the embeddings learned by Transformer\nencoders do not learn the information about the\nabsolute positions, especially BERT which has an\nextremely high mean absolute error.\nAdditionally, we have also tried some more com-\nplicated non-linear models such as SVM or MLP\nto map the embeddings back. However, they easily\noverﬁt and the testing results are even worse than\nlinear models. This implies that the position infor-\nmation in Transformer can actually be modeled by\na linear model.\n4.1.2 Relative Position Regression\nIn addition to absolute positions, the relation be-\ntween positions is also informative (relative posi-\ntions). If Pand Xare isomorphic, there should\nexist a bijection of distance operation between two\nspaces. Thus we deﬁne a mapping function of dis-\ntances from Xto P: h(xi,xj) = ∥i−j∥, where\ni, j are two position indices, ∥i−j∥is the dis-\ntance between i and j in the space P, and xk is\nthe position embedding at the k-th position. In\nthis scenario, we can also build a mapping func-\n6843\nFigure 1: Visualization of position-wise cosine similarity of different position embeddings. Lighter in the ﬁgures\ndenotes the higher similarity.\ntion to check whether the embeddings can capture\nthe relation between positions. However, in our\npreliminary experiments, we ﬁnd that using linear\nregression to predict the distance of positions is too\nhard, since the relation between positions in the\nspace Xmay not be completely linear. Hence, we\nsimplify this problem to whether the embeddings\ncapture the order of every two positions:\nh(xi,xj) =\n{ 1 if i≥j\n0 if i<j\n}\n,\nand then use logistic regression to learn this binary\nclassiﬁcation problem.\nThe results in Table 2 show that the position em-\nbeddings of Transformer encoders still learn less in-\nformation about their position relations, especially\nfor BERT. Moreover, the sinusoid function, which\ncan represent absolute positions perfectly, has a\nhigher error rate than GPT-2 in relative positions\nbut better than Transformer encoders, indicating\nthe surprising capability of capturing such relations\nin GPT-2.\n4.2 What do Transformer Encoders Capture\nabout Positions?\nAccording to the previous analyses, Transformer\nencoders (BERT and RoBERTa) may not well cap-\nture the meaning of positions (absolute and rela-\ntive positions). Therefore, the interested question\nbecomes “what do Transformer encoders capture\nabout positions?”.\n4.2.1 Position-Wise Cosine Similarity\nFigure 1 shows the visualization of position-wise\ncosine similarity of each position embedding. The\npoint at (i,j) indicates the similarity between the i-\nth position and the j-th position. First, we observe\nthat the embedding space of sinusoid and GPT-2\nFigure 2: Accumulated top eigenvalues of position em-\nbeddings.\nhave obvious periodic patterns along with position\norders, which aligns the ﬁndings in the section 4.1,\nwhere these two embeddings can actually capture\nthe meanings of positions. With regard to BERT,\nwe can only observe that embedding vectors are\nsimilar to the positions nearby but have no explain-\nable patterns in long-term relations. Also, another\nobservation is that BERT position embeddings have\nan obvious gap at the position 128, because the pre-\ntrained procedure of BERT trains on the sentences\nwith the length of 128 in the ﬁrst stage, and then\nextends to the length of 512 in the second stage.\nThe ﬁgure illustrates that the learned position in-\nformation in the ﬁrst stage can not be completely\ngeneralized to the second stage. Last but not least,\nthe visualization of RoBERTa is similar to BERT,\nbut have some limited non-periodic visible patterns\nat the positions nearby.\n4.2.2 Informativeness of Position\nEmbeddings\nIn order to examine the informativeness of the\nlearned position embeddings, we apply singular\n6844\nvalue decomposition (SVD) on position embed-\ndings and analyze their eigenvectors. Figure 2\nshows the curves of accumulated top neigenvalues\nversus the proportion of total eigenvalues. Mathe-\nmatically, the summation of the top neigenvalues\nindicates how informative can a matrix be if the\nmatrix is transformed into a n-dim space. For a\nposition space P, which is a 1-dim space, we may\nnot need a high-dimension embedding space Xto\nrepresent the positions. Thus, the summation of the\ntop neigenvalues in a position embedding should\naccount for the most proportion of total eigenval-\nues with only a very small n. However, in Figure\n2, we ﬁnd that the position embeddings of BERT\ntake a very large nto achieve a high proportion of\ntotal eigenvalues, and RoBERTa also takes a larger\nnthan GPT-2 and sinusoid. This implies that the\nposition embeddings of Transformer encoders may\nlearn more complex information rather than only\nabout positions, and this rich information may only\nbe useful in Transformer encoders. This assump-\ntion will be further investigated in the experiments.\n4.3 What Make the Differences?\nThus far, it can be found that the learned position\nembeddings between Transformer encoders and\ndecoders are completely different. In this section,\nwe will illustrate what makes these embeddings\ndifferent.\n4.3.1 Pre-Training Objectives\nOne of the main reason for the difference is the\npre-training objectives. Pre-trained Transformer\nencoders minimize the masked language modeling\nloss, which can be formulated as\nL(U) =\n∑\ni\nlog P(ui|u1,...ui−1,ui+1,...ul; θ),\nwhere U = {u1,...ul}is the pre-training corpus\nwith the length l, and θ is the model parameters.\nFor Transformer decoders, the objective is the tra-\nditional autoregressive language modeling loss:\nL(U) =\n∑\ni\nlog P(ui|u1,...ui−1; θ).\nTransformer encoders can predict tokens depend-\ning on the tokens in both directions, while decoder\ncan only predict depending on the token in the past.\nWith enough context information, it is believed that\nTransformer encoders can succeed to predict to-\nkens by only performing attention on the tokens\nnearby. That is why position embeddings learned\nFigure 3: Visualized position-wise cosine similarity of\nthe simpliﬁed RoBERTa position embeddings.\nby Transformer encoders do not need to involve\nthe precise position information, aligning with the\nprevious experiments in section 4.1.\nWe infer that encoder position embeddings may\ncapture the local position information, which can\nforce the output capturing the positions nearby, es-\npecially BERT almost involving nothing about ab-\nsolute positions. The inference makes the previous\nobservations in sections 4.2.2 and 4.2.1 sensible\nand explainable, and we will verify this inference\nthrough empirical experiments in section 5.\n4.3.2 Differences Between BERT and\nRoBERTa\nBoth BERT and RoBERTa use the same Trans-\nformer encoder architecture, but there are still\nsome obvious differences in the previous discus-\nsion. Hence, we want to know what makes the\nposition embeddings of BERT and RoBERTa dif-\nferent. The main improvements from BERT to\nRoBERTa are (Liu et al., 2019):\n1. Sequentially increasing the batch size from\n256 to 8192 during training.\n2. Dynamically changing the token masks.\n3. Eliminating the additional next sentence pre-\ndiction (NSP) loss.\nDue to the limitation of computing resources for\nexperiments with a large batch size, we instead\ntrain a simpliﬁed version of RoBERTa that remains\nthe 256 batch size and the shorter block length\nfor faster convergence. The visualized position-\nwise cosine similarity of the simpliﬁed RoBERTa\nposition embeddings showing in Figure 3 is very\nsimilar to BERT but without a gap at the position\n6845\n128. As a result, we infer that a large batch pre-\ntraining can make the position embedding more\nrobust and involve more clear position information\nin Transformer encoders.\n5 Performance Effect\nIn addition to the behavior analysis, we are inter-\nested in the performance difference of positional\nembeddings for different NLP tasks, where we con-\nduct text classiﬁcation (encoding), language model-\ning (decoding) and machine translation (encoding\nand decoding). Note that each chosen task has its\nown important property where position information\nmay cause different effects in Transformers.\n5.1 Text Classiﬁcation\nGenerally, for a text segment s = {x1,x2,...xn}\ncontaining ntokens, a Transformer for classiﬁca-\ntion can be formulated as\nh0 = [z1,...,z n] ,\nhi = transformer block(hi−1),\nP(y|s) = softmax(hl\nn),\nwhere zi is the representation for the token xi, y\nis the output class and hi is the i-th layer output\nhidden state in Transformers.\nConventionally, a special token, usually[eos]\nor [CLS] would be appended to the end of input\ntokens, so that the output hidden state can perform\nattention on all other input tokens. In other words,\nno matter an encoder or a decoder is applied, the\nattention mask of the output hidden state and the\nobjective can be identical. Therefore, we conduct a\nfair comparison with pre-trained position embed-\ndings of both encoders and decoders in order to\ncheck whether all settings achieve similar perfor-\nmance.\nExperimental Setup We experiment on six com-\nmon text classiﬁcation datasets: SST2, TREC,\nSUBJ, CR, MR, and MPQA. Since the last four\ndatasets have no train/dev/test splits, we evaluate\nthem with 5-fold cross-validation. We use the same\nmodel architecture as Wang et al. (2019), building a\n1 layer Transformer encoder with 256 and 512 hid-\nden size for self-attention and feed-forward respec-\ntively and 8 attention heads. Then ﬁve settings of\nthe initialized position embeddings are performed:\nrandom, BERT, RoBERTa, GPT-2, and sinusoid,\nand other weights are initialized randomly.\nFigure 4: Length versus accuracy in text classiﬁcation.\nDiscussions Table 3 shows the results of text clas-\nsiﬁcation accuracy. BERT and RoBERTa position\nembeddings perform much worse than GPT-2 and\nsinusoid in most cases. Because the output hidden\nstate can utilize the information of all input tokens,\nthe importance of absolute positions is certainly\ngreater than local position information. However,\nin TREC and MPQA, the difference between 5\nsettings is insigniﬁcant, and we notice that the aver-\nage lengths of these two sets are much shorter than\nothers shown in the bottom of Table 3. Therefore,\nthe position information is not very important in\nthese tasks (TREC and MPQA), considering that\nthe local positions or even random initialization\ncan result in the performance as well as one with\nabsolute positions. The experiments imply that\neven though text classiﬁcation allows the model to\nutilize all tokens when making the prediction, the\nabsolute positions, which GPT-2 can capture, may\nbe still salient for longer inputs.\nLength Sensitivity To further analyze how the\nposition embeddings affect text classiﬁcation with\ndifferent sentence lengths, we plot different ranges\nof lengths versus accuracy in Figure 4. Here we\nonly calculate the average accuracy of SUBJ, SST,\nand CR since the average lengths of TREC and\nMPQA are too short. MR dataset is also excluded,\nbecause we ﬁnd the distribution of length and accu-\nracy in MR is too different from other three datasets\nand it may cause a huge bias in the ﬁgure. Note\nthat the results of MR roughly agrees with others.\nIn Figure 4, sinusoid and GPT-2 still have higher\naccuracy with the length shorter than one standard\ndeviation of the whole dataset, but the difference\nis very subtle. In contrast, there is a signiﬁcant\ngap between Transformer encoders and GPT-2 in\nlonger sentences. In terms of extremely long sen-\n6846\nPE Average ∆ SUBJ SST2 CR MR TREC MPQA\nRandom 0.7797 - 0.8638 0 .7166 0 .7313 0 .7039 0 .8520 0 .8104\nBERT 0.7868 (+0 .0071) 0.8753 0 .7221 0 .7388 0 .7142 0 .8540 0 .8125\nRoBERTa 0.7886 (+0 .0089) 0.8820 0 .7353 0 .7491 0 .7266 0 .8380 0 .8004\nGPT-2 0.7969 (+0 .0172) 0.8845 0.7446 0.7581 0.7314 0 .8540 0 .8087\nsinusoid 0.7983 (+0.0186) 0.8801 0.7474 0.7549 0.7369 0.8580 0 .8125\nAverage Length 23 19 19 20 10 † 3†\nTable 3: Testing accuracy of text classiﬁcation. †indicates the much shorter average length in TREC and MPQA,\nso position embedding can not signiﬁcantly affect the result.\nLM PE Wikitext-2 Wikitext-103\nPerplexity ∆ Perplexity ∆\nMLM\nBERT 147.93 - 12.45 -\n+skip position 198.61 (+50 .68) 323 .12 (+310 .67)\nRoBERTa 157.98 - 12.61 -\n+skip position 199.13 (+41 .14) 14 .44 (+1 .83)\nAutoregressive GPT-2 172.97 - 25.83 -\n+skip position 171.20 ( −1.77) 25 .74 ( −0.09)\nTable 4: Testing perplexity in Wikitext-2 and Wikitext-103.\ntences (longer than one standard deviation), we can\nonly observe that BERT and random initialization\nperform much worse than others. We consider that\nthe data distributions in this range have a too large\nbias so the results may not be robust. Therefore, the\nanalysis provides a hint that GPT-2 may be better\nto tackle the longer inputs for classiﬁcation.\n5.2 Language Modeling\nIn section 4.3.1, we have introduced the objectives\nof the masked language model and autoregressive\nlanguage model for Transformer encoders and de-\ncoders respectively. Also, in the previous discus-\nsions, it is believed that the masked language model\nonly learns the local position information to make\nthe output tokens capture the positions nearby. To\nfurther verify this inference, we propose the skip\nposition attack on position embeddings.\nSkip Position Attack We propose askip position\nattack that skips the position index of input tokens.\nOriginally, the input embedding can be represented\nas\nzi = WE(xi) + PE(i)\n. However, in this attack, we multiply the input\nposition index by a constant k, then the input em-\nbedding of token xi becomes\nzi = WE(xi) + PE(i∗k).\nIf the embedding only learns the local position\ninformation, skip position attack will skip the posi-\ntion indices nearby and lose local information. On\nthe other hand, the absolute positions will not be in-\nﬂuenced so much, because the order of the skipped\npositions is still the same. Based on the design, we\nconduct experiments to validate our inference.\nExperimental Setup We conduct the experi-\nments on the Wikitext-2 and Wikitext-103 datasets,\nwhich have 2 million and 103 million tokens\nrespectively. For model architecture, we take\nBERT-Base for a masked language model and\nGPT-2-Base as an autoregressive language\nmodel, both models have 12 Transformer layers\nand 768 hidden size. Similar to text classiﬁcation,\nall weights are randomly initialized except position\nembeddings. The constant kin the skip position at-\ntack is set to 4, and we slice the corpus into blocks\nof 128 length to ﬁt the maximum length of pre-\ntrained position embeddings, which is 512.\nDiscussions Table 4 shows the results. On av-\nerage, the masked language models (BERT and\nRoBERTa) have slightly lower perplexity than the\nautoregressive language model (GPT-2) due to their\nbidirectional token dependency. However, the skip\nposition attack signiﬁcantly harms the performance\nof the masked language models, while it affects\n6847\nnothing on the autoregressive language model.\nAnother observation is that, in Wikitext-2, the\ndistribution of position information is not robust\nenough so the difference between position embed-\ndings of BERT and RoBERTa is not signiﬁcant.\nHowever, in the larger dataset: Wikitext-103, skip\nposition attack leads BERT position embeddings\nto extremely awful performance. The observation\nhere is consistent with the inferences mentioned\nin section 4, and we can conclude that position\nembeddings of Transformer encoders focus on cap-\nturing the information nearby, especially BERT,\nwhich involves even less position information than\nRoBERTa.\n5.3 Machine Translation\nNeural machine translation is often trained by\na sequence-to-sequence model (Sutskever et al.,\n2014), which includes both encoder and decoder\nin the model. Thus, there are two position embed-\ndings in a Transformer for machine translation, and\nthe position embeddings in the encoder and in the\ndecoder may cause different effects in this task.\nExperimental Setup We experiment on the\nMulti30k English-German dataset from WMT2016\nshared tasks. The properties of the dataset are\nshown in Table 5. We use the scripts implemented\nby Fairseq (Ott et al., 2019) for a faster training pro-\ncess. The encoder and decoder have both 6 layers\nwhere each layer has 4 heads, 512, and 1024 hid-\nden size for attention head and feed-forward respec-\ntively. Also, byte-pair encoding (BPE) (Sennrich\net al., 2015; Gage, 1994) is applied to the corpus\nand the vocabulary size is reduced to 10,000.\nTrain Valid Test\nSentence Pairs 29,000 1 ,015 1 ,000\nAverage Length 12 12 12\nTable 5: Statistics of Multi30k dataset.\nTo respectively investigate the effectiveness on\nthe encoder and the decoder, there are total four dif-\nferent initialization settings of pre-trained position\nembeddings:\n1. Position embeddings only for the encoder\n2. Position embeddings only for the decoder\n3. Different types of position embeddings for the\nencoder and decoder\n4. Same position embeddings for both encoder\nand decoder\nPE BLEU\nEncoder Decoder Full Set Length\n>2σ\nRandom Random 32.19 18 .04\nBERT - 35.54 22 .98\nGPT-2 - 34.36 22 .05\n- BERT 32.08 17 .77\n- GPT-2 32.81 18 .05\nBERT GPT-2 34.11 21 .29\nGPT-2 BERT 32.80 21 .96\nBERT BERT 35.94 23.60\nRoBERTa RoBERTa 35.47 24 .50\nGPT-2 GPT-2 35.80 25.12\nTable 6: BLEU scores on full set and long sentences\n(>2σ) of Multi30k translation data. The hyphen (-) in\nthe table means the same as the baseline (random).\nFor the ﬁrst three settings, only BERT and GPT-2\nare performed for conciseness.\nThe results are shown in Table 6, where we eval-\nuate the BLEU scores on the sentences longer than\n2 standard deviation (for both source and target) to\nanalyze the effectiveness of longer sentences with\nconsideration that the average length of Multi30k\nis relatively short.\nEncoder Both BERT and GPT-2 position embed-\nding can be effective in the encoder, especially\nBERT. The reason is that the decoded tokens can\nperform attention on all encoder outputs, thus the\nobjective would be similar to the masked language\nmodeling.\nDecoder The effectiveness of position embed-\ndings in the decoder is not as signiﬁcant as one in\nthe encoder, because the decoder cannot capture\nthe order of the source language. We also observe\nthat applying BERT position embeddings on the\ndecoder even slightly harms the performance, since\nit may make the decoder tend to focus on the tokens\nnearby only.\nDifferent for Encoder/Decoder According to\nthe previous results, we hypothesize that using\nBERT in the encoder and GPT-2 in the decoder\ncould perform best. However, in our experiments,\nusing different pre-trained position embeddings is\neven worse, probably because the divergence of\nposition embeddings trained by different models\nis quite huge and mixing them in the same model\n6848\nmay not suitable. Also, we swap the position em-\nbeddings in the encoder and decoder to see the\nimpact. The BLEU score of the full set drops a lot,\nbut in terms of long sentences, using GPT-2 in the\nencoder may not lose too much performance.\nSame for Encoder/Decoder The results show\nthat the performance between three pre-trained po-\nsition embeddings are very close in the full set.\nHowever, in terms of longer sentences, GPT-2 is\nmuch better than BERT and RoBERTa. This obser-\nvation aligns well with the previous analysis that\nthe absolute position information is more important\nfor longer sentences.\nTo sum up, there main observations are found:\n1) The effectiveness of position embeddings in the\nencoder is more signiﬁcant than one in the decoder.\n2) Mixing different position embeddings in a model\nis not suitable. 3) GPT-2 position embeddings out-\nperform others when modeling longer sentences.\n6 Conclusion\nThis paper investigates the implicit meaning of pre-\ntrained Transformer position embeddings. Trans-\nformer encoders learn the local position informa-\ntion that can only be effective in masked language\nmodeling. On the other hand, the Transformer\ndecoders for autoregressive language modeling ac-\ntually learn about absolute positions. The empiri-\ncal experiments on the pre-trained position embed-\ndings validate our hypothesis. We also show that\ndifferent NLP tasks with different model architec-\ntures and different training objectives may utilize\nthe position information in different ways. As a re-\nsult, it is believed that this study will beneﬁt future\nwork about choosing suitable positional encoding\nfunctions or designing other modeling methods for\nposition information in the target NLP tasks based\non their properties.\nAcknowledgements\nWe thank reviewers for their insightful comments.\nThis work was ﬁnancially supported from the\nYoung Scholar Fellowship Program by Ministry\nof Science and Technology (MOST) in Taiwan,\nunder Grant 109-2636-E-002-026.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nDenny Britz, Anna Goldie, Minh-Thang Luong, and\nQuoc Le. 2017. Massive exploration of neural\nmachine translation architectures. arXiv preprint\narXiv:1703.03906.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho,\nand Yoshua Bengio. 2014. Empirical evaluation of\ngated recurrent neural networks on sequence model-\ning. arXiv preprint arXiv:1412.3555.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language mod-\nels beyond a ﬁxed-length context. arXiv preprint\narXiv:1901.02860.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nPhilip Gage. 1994. A new algorithm for data compres-\nsion. C Users Journal, 12(2):23–38.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N Dauphin. 2017. Convolutional\nsequence to sequence learning. In Proceedings\nof the 34th International Conference on Machine\nLearning-Volume 70, pages 1243–1252. JMLR. org.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation,\n9(8):1735–1780.\nYann LeCun, Yoshua Bengio, et al. 1995. Convolu-\ntional networks for images, speech, and time series.\nThe handbook of brain theory and neural networks,\n3361(10):1995.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. arXiv preprint arXiv:1910.13461.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\n6849\nMichael Auli. 2019. fairseq: A fast, extensi-\nble toolkit for sequence modeling. arXiv preprint\narXiv:1904.01038.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. arXiv preprint arXiv:1802.05365.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nAngus Roberts. 2005. Learning meronyms from\nbiomedical text. In Proceedings of the ACL Stu-\ndent Research Workshop, pages 49–54, Ann Arbor,\nMichigan. Association for Computational Linguis-\ntics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. arXiv preprint arXiv:1803.02155.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in neural information processing sys-\ntems, pages 3104–3112.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nBenyou Wang, Donghao Zhao, Christina Lioma, Qi-\nuchi Li, Peng Zhang, and Jakob Grue Simonsen.\n2019. Encoding word order in complex embeddings.\narXiv preprint arXiv:1912.12333.\nPaul J Werbos. 1990. Backpropagation through time:\nwhat it does and how to do it. Proceedings of the\nIEEE, 78(10):1550–1560.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5754–5764.\nA Reproducibility\nA.1 Datasets\nThe datasets we used can be downloaded from the\nfollowing linked pages, and the details of datasets\nare also described in the pages.\n•Text Classiﬁcation: link\n•Language Modeling: link\n•Machine Translation: link\nA.2 Training Details\nText Classiﬁcation\ntokenizer spacy\noptimizer Adam\nlr 1−4\nbatch size 32\nmax epoch 40\nLanguage Modeling\nWikitext02 Wikitext-103\nlr - 2.5−4\nbatch size 32 32\nmax epoch 20 3\nwarup steps - 4000\nMachine Translation\noptimizer Adam\nweight decay 0.0001\nlr 1−4\nmax tokens 2048\nmax epoch 40\nlr scheduler inverse sqrt\nwarup steps 4000\nlabel smooth 0.1\nSince the goal of this paper is to compare po-\nsition embedding, we do not try too many hyper-\nparameters on Transformers, and most setting are\ndefault as the implementation of hugginface and\nFairseq.\nA.3 Running Time\nAll our experiments are trained on 1 GTX 2080\nTI GPU. Except language modeling on Wikitext-\n103 takes about 10 hours, all other trainings can be\ndone within 2 hours.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7627918124198914
    },
    {
      "name": "Computer science",
      "score": 0.7373517751693726
    },
    {
      "name": "Embedding",
      "score": 0.698733925819397
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5604020953178406
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5163360834121704
    },
    {
      "name": "Empirical research",
      "score": 0.4882943630218506
    },
    {
      "name": "Natural language processing",
      "score": 0.44856685400009155
    },
    {
      "name": "Machine learning",
      "score": 0.3753649592399597
    },
    {
      "name": "Engineering",
      "score": 0.1474124789237976
    },
    {
      "name": "Mathematics",
      "score": 0.08790561556816101
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I16733864",
      "name": "National Taiwan University",
      "country": "TW"
    }
  ]
}