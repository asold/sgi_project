{
  "title": "A Large Language Model Outperforms Other Computational Approaches to the High-Throughput Phenotyping of Physician Notes",
  "url": "https://openalex.org/W4399986826",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Munzir, Syed I.",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Hier, Daniel B.",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Oommen, Chelsea",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Carrithers, Michael D.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3030645549"
  ],
  "abstract": "High-throughput phenotyping, the automated mapping of patient signs and symptoms to standardized ontology concepts, is essential to gaining value from electronic health records (EHR) in the support of precision medicine. Despite technological advances, high-throughput phenotyping remains a challenge. This study compares three computational approaches to high-throughput phenotyping: a Large Language Model (LLM) incorporating generative AI, a Natural Language Processing (NLP) approach utilizing deep learning for span categorization, and a hybrid approach combining word vectors with machine learning. The approach that implemented GPT-4 (a Large Language Model) demonstrated superior performance, suggesting that Large Language Models are poised to be the preferred method for high-throughput phenotyping of physician notes.",
  "full_text": "A Large Language Model Outperforms Other Computational Approaches to\nthe High-Throughput Phenotyping of Physician Notes\nSyed I. Munzir1, Daniel B. Hier1,2, Chelsea Oommen1, Michael D. Carrithers1\n1Department of Neurology and Rehabilitation, University of Illinois at Chicago, Chicago, USA\n2Kummer Institute, Missouri University of Science and Technology, Rolla MO, USA\nAbstract\nHigh-throughput phenotyping, the automated mapping of patient signs and symptoms to standardized ontology con-\ncepts, is essential to gaining value from electronic health records (EHR) in the support of precision medicine. Despite\ntechnological advances, high-throughput phenotyping remains a challenge. This study compares three computational\napproaches to high-throughput phenotyping: a Large Language Model (LLM) incorporating generative AI, a Nat-\nural Language Processing (NLP) approach utilizing deep learning for span categorization, and a hybrid approach\ncombining word vectors with machine learning. The approach that implemented GPT-4 (a Large Language Model)\ndemonstrated superior performance, suggesting that Large Language Models are poised to be the preferred method\nfor high-throughput phenotyping of physician notes.\nIntroduction\nThe advent of precision medicine has intensified the need for high-throughput phenotyping of electronic health records\n(EHR). This task remains challenging due to the complexity and volume of physician notes. High-throughput pheno-\ntyping—the automated mapping of patient symptoms to standardized ontology concepts—is crucial for this endeavor\n[1–5]. Although traditional and some advanced Natural Language Processing (NLP) methods have progressed toward\nthis goal, their limitations underscore the need for more efficient methods. The emergence of Large Language Models\n(LLMs) like GPT-4 introduces a promising new approach to address this unsolved problem. This study compares the\nperformance of an LLM, NLP, and hybrid approaches to the high-throughput phenotyping of physician notes from an\nEHR.\nThe precision medicine initiative, which aims to match treatments and outcomes with the individual characteristics\nof each patient, requires computable descriptions of patient signs and symptoms. These descriptions must be both de-\ntailed and automated. Despite this critical need for high-throughput methods, their implementation in human medicine\nhas lagged behind other fields, such as agriculture [6, 7]. There is a pressing need for more efficient high-throughput\nmethods [8–10].\nHistorically, Natural Language Processing (NLP) methods to extract medical concepts from medical text evolved\nfrom rule-based and dictionary-based systems [11–13]. Second-generation systems used machine learning and statis-\ntical models to find medical concepts in text [14, 15]. The third generation of approaches to concept extraction saw\nthe application of deep learning methods to this problem, notably RNN (recurrent neural networks) and CNN (con-\nvolutional neural networks) [16–20]. The fourth generation systems introduced transformer architecture and BERT\n(bidirectional encoder representations from transformers), achieving gains in performance due to improvements in\nattention and language understanding [21–26]. The emergence of fifth-generation large language models (LLMs)–\nsuch as GPT-4 (Generative Pre-Trained Transformer)–offers new flexibility, scalability, and generalizability that have\nallowed an attack on previously unsolvable NLP problems, including the high-throughput phenotyping of physician\nnotes [27–29].\nBuilding on a prior pilot study [30], which demonstrated the potential of GPT-4 for high-throughput phenotyping,\nthis study compares three computational approaches on a larger corpus of physician notes:\n1. LLM Approach: GPT-4, that combines a Large Language Model with generative AI capabilities;\n2. Hybrid Approach: NimbleMiner, that blends machine learning classification with word vector expansion; and\n3. NLP Approach: spaCy spancat, which creates an NLP pipeline that depends on deep learning and word tok-\nenization for span categorization.\narXiv:2406.14757v1  [cs.AI]  20 Jun 2024\nThis comparison evaluates the performance of these approaches for high-throughput phenotyping, providing in-\nsights into their suitability for precision medicine. To establish a ground-truth data set, we manually annotated the\nsigns and symptoms (phenotype) of the patients described in 170 physician notes. We evaluated the accuracy, pre-\ncision, and recall of the three approaches. The results show the potential for advanced computational approaches\nto make practical high-throughput phenotyping of EHRs and foreshadow a potential shift towards the dominance of\napproaches based on large language models.\nBox 1: Instructions to GPT-4 and the human annotator for finding neurological phenotypes in physician notes\nHere are instructions for GPT-4 on how to do high-throughput neurological phenotyping. This exercise aims to find neurological signs and\nsymptoms in physician notes and assign each one to one of the 20 phenotype categories below.\nbehavior [including anxiety, depression, delusion, psychosis, hallucination, etc.]\ncognitive [including memory loss, forgetfulness, confusion, cognitive impairment, inattention, dementia, etc.]\nEOM [including double vision, abnormal eye movements, diplopia, sixth nerve palsy, third nerve palsy, skew deviation, etc.]\nfatigue [including tiredness, lack of energy, poor energy, etc.]\ngait [including abnormal gait, spastic gait, ataxic gait, poor balance, imbalance, falling down, falling, using a cane, using a walker, etc.]\nhyperreflexia [including increased reflexes, increased biceps reflex, biceps +++, triceps +++, biceps ++++, triceps +++, etc.]\nhypertonia [including increased tone, spasticity, hypertonia, muscle spasms, etc.]\nhyporeflexia [including decreased reflexes, areflexia, hyporeflexia, bicep 1+ 1+, absent ankle reflex, ankle reflex 1+ 1+, etc.]\nsphincter [including urinary frequency, urinary incontinence, constipation, bowel incontinence, urinary retention, etc.]\nincoordination [including ataxia, dysmetria, poor coordination, etc.]\nON [including optic neuritis, apd, afferent pupillary defect, pale disk, disk atrophy, on?\npain [including shooting pain, burning pain, allodynia, arm pain, headache, head pain, leg pain, etc.]\nparesthesias [including numbness, tingling, loss of sensation, sensory loss, hypesthesia, etc.].\nseizure [including seizures, convulsions, fits, attacks, etc.]\nsleep [including hypersomnia, insomnia, restless legs, abnormal sleep, trouble sleeping, etc.]\nspeech [including lack of speech, slurred speech, dysarthria, aphasia, etc.]\ntremor [including tremor, tremulousness, action tremor, resting tremor, rubral tremor, etc.]\nvision [including impaired vision, decreased visual acuity, visual loss, etc.]\nweakness [including weakness, loss of strength, difficulty lifting arms, difficulty lifting legs, biceps 4 4, triceps 4 4, hip flexors 4 4, etc.]\ncranial nerve [including any brainstem dysfunction or cranial nerve dysfunction, such as deafness, hearing loss, tinnitus, vertigo]\nThe output should be a list of tuples, where the first element of the tuple is a phenotype category, and the second element is a 1 if the\nphenotype was found and a 0 if it was not.\nMethods\nData Acquisition: We analyzed physician notes from electronic health records (EHR) of neurology patients diagnosed\nwith multiple sclerosis (MS, ICD-10 code: G35), visiting the University of Illinois at Chicago Neurology Clinic\nbetween 2019 and 2022. The physician notes were extracted from the REDCap (Research Electronic Data Capture)\nsystem as a CSV file. To avoid analyzing notes with little substantive content, we selected the first physician progress\nnote for each patient that contained at least 600 words. We excluded discharge summaries, admission notes, and\nconsultation notes. Non-ASCII characters and quotation marks were removed to facilitate conversion to the JSONL\nformat. Of 547 unique patient records, 188 were used to train the spaCy spancat model (training dataset), and 170\nnotes were assigned for model evaluation (test dataset).\nSelection of Phenotype Categories: The 20 phenotype categories (Box 1) were carefully selected based on their\nfrequency in and clinical relevance to multiple sclerosis (MS) [31]. Although many granular terms are available to\ndescribe neurology phenotypes in the Human Phenotype Ontology, we chose to “roll up” these terms into 20 high-level\ncategories to increase the interpretability and comprehensibility of the findings.\nPhenotyping by Human Annotator: ground-truth labels for the notes were generated using the Prodigy annotation\ntool (Explosion AI, Berlin). The task involved identifying text spans corresponding to one of 20 neurological signs\nand symptoms categories. The initial phase of annotations used thespans.manual recipe in Prodigy. After the first\nten notes were annotated, a preliminary spaCy spancat model was trained, allowing a switch to thespans.correct\nrecipe, which suggests potential spans (Figure 1). The human annotator received the same instructions as the GPT-4\nAPI (Box 1). Our methods for phenotype annotation have been previously described, including high levels of inter-\nrater agreement (κ = 0.85) [32].\nPhenotyping by Hybrid Approach: NimbleMiner [33] is a tool for the recognition of medical concepts in clinical\ntexts that are implemented in R (compatible with version 4.0.2 [34]). It is a hybrid model that combines machine\nlearning classifiers with word embeddings (word2vec) to identify medical concepts. By transforming seed terms into\nan internal lexicon called simclins, NimbleMiner uses machine learning classifiers to find matching phrases in clinical\nnarratives. NimbleMiner adheres to a label classification strategy that uses only positive labels and excludes negated\nconcepts. An initial list of signs and symptoms of multiple sclerosis was augmented by text spans from neurology\nnotes (Figure 3a). NimbleMiner uses prenegations, terms that precede and negate a span (e.g., no sign of weakness),\nand postnegations, terms that follow and negate a span (e.g., weakness negative) to exclude negated phenotypes.\nWe selected an SVM classifier within NimbleMiner due to the known proficiency of SVM classifiers on the text\ncategorization tasks [35]. The SVM classifier determined the binary presence of the 20 neurological phenotypes in the\nphysician notes (Figure 3b).\n(a) Screen for text span leg with weakness annotated as weak-\nness. Note that the physician note contains grammatical and\nspelling errors.\n(b) Screen for Hip Flexors 3 4 annotated as weakness. Note that\nthe physician has coded weakness as a numerical score on a 0 to\n5 scale.\nFigure 1: Annotations screens for Prodigy for text spans indicating weakness. The annotator has a choice of 20 labels\nfor selected text spans.\nPhenotyping by NLP Approach: We used the SpaCy NLP spancat pipeline (Explosion AI, Berlin) to recognize the\n20 target phenotype labels. We implemented the default tok2vec component for token-to-vector encoding and the\ndefault spancat component for span categorization. Initially, we set the parameters in the config.cfg file to\ntheir default values, including the system, components, training, batch, and initialization sections. Our initial training\ndataset for the spaCy spancat pipeline was 188 physician notes with 11,688 annotated lines. We used thedata-to-spacy\nrecipe from Prodigy (Explosion AI) to create training and validation sets. The initial F score was unsatisfactory at 0.34,\nand a class imbalance with poor recall in minority classes was recognized (Figure 2). Manually created synthetic data\nwith more examples from the underrepresented speech, seizure, and tremor classes were added. Furthermore, due to\nthe problematic dual representation of hyporeflexia, hyperreflexia, and weakness as text and numeric values (see Figure\n1, additional examples of numerically encoded phenotypes were added, resulting in a dataset of 15,052 annotated\nlines. With data augmentation, the F in the validation dataset increased to 0.62. We then added transfer learning\nfrom a previously trained spancat model to achieve an F score of 0.77. Although we attempted to improve spancat\nperformance further by adding external word vectors and a transformer architecture, software version incompatibility\nprevented us from implementing those improvements. The outputted model-best was applied to the unseen test\ndataset to find neurological phenotypes in each note.\nPhenotyping by LLM Approach. The instructions for phenotyping the physician notes were passed to GPT-4 API\n[36] as a prompt (Box 1). Initial modifications to the prompt were made interactively with GPT-4 in the chat mode. We\nused the chat mode to resolve ambiguities in the prompt, such as whether to categorize “facial weakness” as a finding\nof weakness or a finding of cranial nerve. Further prompt modifications were needed to obtain a GPT-4 output that\ncould be parsed into a pandas DataFrame. We wrote a Python script that iterated through the physician notes using the\nGPT-4 API. No speed or complexity limitations were experienced with the high-throughput phenotyping of the 170\nphysician notes (each approximately 1,000 words). GTP-4 generated a list of phenotypes (parsable by a Python script)\n(Figure 4a) and a brief explanation of its choices (Figure 4b). The GPT-4 output was saved in a pandas DataFrame for\nfurther analysis.\nCalculation of Performance Metrics. The ground-truth labels for each physician note were stored in the Prodigy\nSQLite database. We used Python to convert the ground-truth annotations into a 170 x 21 pandas DataFrame where\nFigure 2: Due to class imbalance in the training dataset for the NLP spancat model, synthetic data was added, increas-\ning the number of lines annotated from 11,688 to 15,052 and thus increasing the minority classes ON (optic neuritis),\nseizure, sleep, and tremor. In addition, additional training examples were added to the hyperreflexia, hyporeflexia, and\nweakness classes due to low recall in these classes (see discussion)\nthe first column was the Record ID, and the next 20 columns held the binarized value for each of the 20 phenotypes\n(present or absent). We created similar 170 x 21 dataframes from the binarized predictions of the NLP, Hybrid, and\nLLM approaches. To evaluate the performance of the computational approaches for high-throughput phenotyping,\nwe selected precision, recall, and accuracy as our primary metrics. These metrics were chosen for their direct inter-\npretability and specific relevance to the binary classification tasks. Python was used to calculate accuracy, precision,\nand recall by standard methods [37, 38]. A micro average was calculated for each of the twenty phenotype categories,\nand an unweighted overall macro average across all categories.\nHuman Studies: The research was approved by the Institutional Review Board of the University of Illinois at\nChicago. All physician notes were unidentified, and all protected health information was deleted. GPT-4 did not retain\nor use patient health information for LLM training.\nResults\nWe compared the performance of three high-throughput phenotyping approaches. After iterative improvements to the\nHybrid (NimbleMiner) and the NLP (spaCy spancat) approaches, all showed good accuracy (Figure 5). The LLM\n(GPT-4) Approach performed the best (0.88), followed by the Hybrid Approach (NimbleMiner) (0.81) and the NLP\nApproach (spaCy spancat) (0.78). Precision (with higher scores reflecting lower false positive rates) was high for\nall three approaches, although the LLM Approach performed best. Recall (with higher scores reflecting lower false\nnegative rates) was higher for the LLM Approach (0.77), lower for the Hybrid Approach (0.65), and lowest for the\nNLP Approach (0.42).\nBoth the training dataset (Figure 2) and the test dataset (not shown) demonstrated class imbalances. In particular,\nthe classes of tremor, speech, and seizures were underrepresented. Three phenotype classes ( hyporeflexia, hyper-\nreflexia, weakness) were dual encoded by physicians in notes who used either descriptive text or numeric scores, such\nas weakness documented as the text (”leg with weakness”) or as the numerical score (”Hip Flexors 3 4”) (see Figure\n1).\nRegarding recall—with a few exceptions—the LLM Approach outperformed the Hybrid and the NLP approaches\nin almost all phenotype categories, with pain and seizure being the exceptions. Although the superiority of the LLM\nApproach in precision for individual phenotypes was less pronounced, it surpassed the Hybrid and NLP approaches\n(a) Sample seed terms passed to NimbleMiner used\nto generate the simclins utilized by the SVM classi-\nfier.\n(b) Example output screen from the Hybrid method (Nimble Miner).\nNimbleMiner has identified text spans related to the phenotype “inco-\nordination”. “Dysmetria” is a simclin for incoordination. NimbleMiner\nhas highlighted two negated examples of dysmetria (red font) and one\npositive example (green font.)\nFigure 3: Examples of seed terms used to generate simclins (a) and examples of positive and negated text spans for\nphenotype identified by NimbleMiner (b).\nin several categories, including speech, sleep, ON, incoordination, fatigue, and CN. The LLM Approach consistently\noutperformed the other approaches across the macro overall performance metrics, including accuracy, precision, and\nrecall (Figure 5).\nDiscussion\nHigh-throughput phenotyping of patient data, crucial to advancing precision medicine, involves converting signs and\nsymptoms from clinical notes into computable codes. Given the number of electronic health records and various\nlinguistic challenges that include synonymy, polysemy, irregular abbreviations, colloquialisms, misspellings, and non-\nstandard terminologies, automated methods are essential but face significant obstacles. To be useful, the phenotyping\nof text held in EHRs must be fast, accurate, and detailed [3].\nWe performed high-throughput phenotyping on 170 physician notes using three computational approaches: Hy-\nbrid, NLP, and LLM. All notes were written by neurologists and carried a diagnosis of multiple sclerosis. Phenotyping\ninvolved finding the number of occurrences of 20 categories of neurological symptoms. Since writing styles and habits\ndiffer between physicians (some repeat signs and symptoms multiple times in their notes, others do not), the results\nwere binarized so that the occurrence of a neurological sign or symptom (phenotype) was recorded as “present” or\n“absent” in each note.\nHybrid, NLP, and LLM approaches performed at high levels of accuracy (0.81, 0.77, and 0.88, respectively (Figure\n5). These accuracies are impressive given that the level of agreement between human annotators reaches a ceiling at\nκ ≈ 0.85 [32]. The superior performance of the LLM approach is notable given the complexity of this multiclass\nclassification task with high number of classes and class imbalances [39, 40]. In particular, the NLP approach (spaCY\nspancat) faced challenges due to low counts in minority classes (seizures, sleep, and EOM), as shown in Figure 5. This\ndifficulty was partially addressed by class rebalancing using synthetic data. Dual encoding certain phenotypes as a\nnumerical score and textual description (see Figure 1 proved challenging for all approaches but less so for the LLM\napproach.\nThe superior performance of the LLM method on minority phenotype classes (speech, tremor, seizure) and dually\nencoded phenotype classes (hyporeflexia, hyperreflexia, weakness) is notable. With its superior performance in under-\nrepresented classes and its better performance on dually represented phenotypes, GPT-4 demonstrated the ability to\nhandle class imbalances and decode mixed-format data, likely related to its extensive pre. These results highlight the\npotential role of the LLM approach in high-throughput phenotyping of physician notes. The extensive pretraining of\nGPT -4 gave it advantages over the other approaches when analyzing misspelled, irregular, or ambiguous text. Fur-\nthermore, the LLM (GPT-4) method offered explanations for its selections without prompting (Figure 4), suggesting\n(a) Example list of phenotype list re-\nturned by GPT-4.\n(b) Explanations provided by GPT-4 for phenotyping.\nFigure 4: For physician note, GPT-4 outputted a list of phenotypes (a) and explanations for its choices (b).\nthat advances in explanatory AI (XAI) [41] had been incorporated into the model architecture.\nSeveral differences in the ease of implementation between the three approaches should be mentioned. The im-\nplementation of the LLM method (GPT-4) was straightforward. We used the GPT-4 chat mode to refine the prompt\nfor high-throughput phenotyping (Box 1). When we implemented the GPT-4 API, additional changes were needed\nin the prompt to resolve ambiguities and obtain the appropriate output for conversion to a pandas DataFrame (Figure\n4a). The configuration of the Hybrid approach (NimbleMiner) required a meticulous selection of seed terms for each\nof the 20 phenotype categories as well as a rigorous curation of the generated simclins. We went through several\niterations of seed generation and simclin curation until acceptable levels of accuracy were obtained. Implementing the\nNLP approach (spaCy spancat) was the most time-consuming. We created a training dataset by annotating physician\nnotes for the initial spancat pipeline. Due to poor model accuracy in minority classes (especially low recall) and class\nimbalances, additional model training was performed with synthetic data. Further improvements in the performance\nof the spaCy spancat pipeline depended on the implementation of transfer learning from a previously trained model.\nImplementation of high-throughput neurological phenotyping was easiest with the LLM Approach. Furthermore,\nthe LLM Approach outperformed the NLP and Hybrid approaches in accuracy and recall (Figure 5). Although our\nresults with the LLM Approach (GPT-4) are encouraging, confirmation of these results with a larger and more diverse\ncorpus of physician notes is needed. Several limitations of this study should be mentioned.\n1. High through phenotyping was done with a limited number of neurological notes, all with a diagnosis of multiple\nsclerosis. High-throughput phenotyping on more notes with different diagnoses should be studied.\n2. The phenotyping was done at a coarse level of detail. We used 20 broad categories for the phenotyping. How-\never, phenotyping can be performed at a more granular level. For example, the Human Phenotype Ontology has\napproximately 7,500 terms to document human phenotypes [42]. The ability of the LLM method to phenotype\nat higher levels of granularity should be studied.\nFigure 5: Heat map showing precision, recall, and accuracy for three high-throughput phenotyping approaches: Hy-\nbrid, NLP, and LLM. Individual phenotype category metrics are micro averages; the overall metrics are macro aver-\nages. Abbreviations include CN (cranial nerve and brainstem), EOM (extraocular eye movements), and ON (optic\nneuritis). The category paresthesias includes sensory loss, numbness, and tingling)\n.\n3. Additional fine-tuning of the Hybrid Approach (NimbleMiner) would likely have improved accuracy. Additional\nseed terms and simclin curation could have improved performance in some low-performing categories such as\n“cognitive”, “sphincter”, and “EOM”.\n4. Modifications to the NLP Approach (spaCy spancat) would probably have improved performance. Changes\nthat would likely have improved performance include adding a transformer architecture to the pipeline, adding\nspecialized pre-trained word vectors to the pipeline, additional training examples, and better balancing of the\nphenotype classes in the training dataset.\nThis study is an indication of the power, simplicity, and generalizability of large language model approaches when\napplied to the high throughput phenotyping of EHRs. Large language models (LLMs) are poised to become the\ndominant approach to high-throughput phenotyping. GPT-4 outperformed more traditional approaches and proved\neasier to implement. A broader integration of large language models into electronic health records for phenotyping\nwill depend on additional research that validates these findings with different note types, different EHR data types, and\nin different medical fields. If large language models are to be used in patient care, a determination of their regulatory\nstatus will be needed as well as an evaluation of safety, privacy, and security concerns. An assessment of the accuracy\nof large language models for high throughput phenotyping using recognized ground-truth datasets is needed. Large\nlanguage models are a significant advance in high-throughput EHR phenotyping. Greater accuracy can be expected\nwith additional training and fine-tuning of the underlying models.\nReferences\n[1] Sahu M, Gupta R, Ambasta RK, Kumar P. Artificial intelligence and machine learning in precision medicine: A\nparadigm shift in big data analysis. Progress in Molecular Biology and Translational Science. 2022;190(1):57-\n100.\n[2] Afzal M, Islam SR, Hussain M, Lee S. Precision medicine informatics: principles, prospects, and challenges.\nIEEE Access. 2020;8:13593-612.\n[3] Robinson PN. Deep phenotyping for precision medicine. Human mutation. 2012;33(5):777-80.\n[4] Hier D, Yelugam R, Azizi S, Wunsch III D. A focused review of deep phenotyping with examples from neurology.\nEur Sci J. 2022;18:4-19.\n[5] Hier D, Yelugam R, Azizi S, Carrithers M, Wunsch I. DC. High throughput neurological phenotyping with\nMetaMap. Eur Sci J. 2022;18:37-49.\n[6] Mir RR, Reynolds M, Pinto F, Khan MA, Bhat MA. High-throughput phenotyping for crop improvement in the\ngenomics era. Plant Science. 2019;282:60-72.\n[7] Gehan MA, Kellogg EA. High-throughput phenotyping. American journal of botany. 2017;104(4):505-8.\n[8] Alzoubi H, Alzubi R, Ramzan N, West D, Al-Hadhrami T, Alazab M. A review of automatic phenotyping\napproaches using electronic health records. Electronics. 2019;8(11):1235.\n[9] Pathak J, Kho AN, Denny JC. Electronic health records-driven phenotyping: challenges, recent advances, and\nperspectives. Journal of the American Medical Informatics Association. 2013;20(e2):e206-11.\n[10] Shivade C, Raghavan P, Fosler-Lussier E, Embi PJ, Elhadad N, Johnson SB, et al. A review of approaches to iden-\ntifying patient phenotype cohorts using electronic health records. Journal of the American Medical Informatics\nAssociation. 2014;21(2):221-30.\n[11] Krauthammer M, Nenadic G. Term identification in the biomedical literature. Journal of biomedical informatics.\n2004;37(6):512-26.\n[12] Eltyeb S, Salim N. Chemical named entities recognition: a review on approaches and applications. Journal of\ncheminformatics. 2014;6(1):1-12.\n[13] Quimbaya AP, M ´unera AS, Rivera RAG, Rodr ´ıguez JCD, Velandia OMM, Pe ˜na AAG, et al. Named entity\nrecognition over electronic health records through a combined dictionary-based approach. Procedia Computer\nScience. 2016;100:55-61.\n[14] Hirschman L, Morgan AA, Yeh AS. Rutabaga by any other name: extracting biological names. Journal of\nBiomedical Informatics. 2002;35(4):247-59.\n[15] Uzuner ¨O, South BR, Shen S, DuVall SL. 2010 i2b2/V A challenge on concepts, assertions, and relations in\nclinical text. Journal of the American Medical Informatics Association. 2011;18(5):552-6.\n[16] Lample G, Ballesteros M, Subramanian S, Kawakami K, Dyer C. Neural architectures for named entity recogni-\ntion. arXiv preprint arXiv:160301360. 2016.\n[17] Chiu JP, Nichols E. Named entity recognition with bidirectional LSTM-CNNs. Transactions of the Association\nfor Computational Linguistics. 2016;4:357-70.\n[18] Habibi M, Weber L, Neves M, Wiegandt DL, Leser U. Deep learning with word embeddings improves biomedical\nnamed entity recognition. Bioinformatics. 2017;33(14):i37-48.\n[19] Gehrmann S, Dernoncourt F, Li Y , Carlson ET, Wu JT, Welt J, et al. Comparing deep learning and concept\nextraction based methods for patient phenotyping from clinical narratives. PloS one. 2018;13(2):e0192360.\n[20] Arbabi A, Adams DR, Fidler S, Brudno M, et al. Identifying clinical terms in medical text using ontology-guided\nmachine learning. JMIR medical informatics. 2019;7(2):e12596.\n[21] Devlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-training of deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:181004805. 2018.\n[22] Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need. In: Advances\nin neural information processing systems; 2017. p. 5998-6008.\n[23] Zhu R, Tu X, Huang JX. Utilizing BERT for biomedical and clinical text mining. In: Data Analytics in Biomed-\nical Engineering and Healthcare. Elsevier; 2021. p. 73-103.\n[24] Yu X, Hu W, Lu S, Sun X, Yuan Z; IEEE. BioBERT based named entity recognition in electronic medical record.\n2019 10th international conference on information technology in medicine and education (ITME). 2019:49-52.\n[25] Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, et al. BioBERT: a pre-trained biomedical language representation\nmodel for biomedical text mining. Bioinformatics. 2020;36(4):1234-40.\n[26] Ji Z, Wei Q, Xu H. Bert-based ranking for biomedical entity normalization. AMIA Summits on Translational\nScience Proceedings. 2020;2020:269.\n[27] Yan C, Ong H, Grabowska M, Krantz M, Su WC, Dickson A, et al. Large Language Models Facilitate the\nGeneration of Electronic Health Record Phenotyping Algorithms. medRxiv. 2023:2023-12.\n[28] Yang J, Liu C, Deng W, Wu D, Weng C, Zhou Y , et al. Enhancing phenotype recognition in clinical notes using\nlarge language models: PhenoBCBERT and PhenoGPT. Patterns. 2023.\n[29] Wang A, Liu C, Yang J, Weng C. Fine-tuning Large Language Models for Rare Disease Concept Normalization.\nbioRxiv. 2023:2023-12.\n[30] Munzir SI, Hier DB, Carrithers MD. High Throughput Phenotyping of Physician Notes with Large Language\nand Hybrid NLP Models. ArXiv. 2024. Accessed March 12, 2024. Available from: https://arxiv.org/\nabs/2403.05920.\n[31] Howlett-Prieto Q, Oommen C, Carrithers MD, Wunsch DC, Hier DB. Subtypes of relapsing-remitting multiple\nsclerosis identified by network analysis. Frontiers in Digital Health. 2023;4:1063264.\n[32] Oommen C, Howlett-Prieto Q, Carrithers MD, Hier DB. Inter-rater agreement for the annotation of neurologic\nsigns and symptoms in electronic health records. Frontiers in Digital Health. 2023;5:1075771.\n[33] Topaz M, Murga L, Bar-Bachar O, McDonald M, Bowles K. NimbleMiner: an open-source nursing-sensitive\nnatural language processing system based on word embedding. CIN: Computers, Informatics, Nursing.\n2019;37(11):583-90.\n[34] R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Comput-\ning. 2020. Available from: https://www.R-project.org/.\n[35] Joachims T. Text categorization with support vector machines: Learning with many relevant features. In:\nEuropean conference on machine learning. Springer; 1998. p. 137-42.\n[36] OpenAI. ChatGPT (4); 2024. Large language model. Available from: https://chat.openai.com.\n[37] Velupillai S, Dalianis H, Hassel M, Nilsson GH. Developing a standard for de-identifying electronic patient\nrecords written in Swedish: precision, recall and F-measure in a manual and computerized annotation trial.\nInternational journal of medical informatics. 2009;78(12):e19-26.\n[38] Precision and Recall. Wikimedia Foundation; 2024. Accessed: [January 29, 2024]. In Wikipedia. Available\nfrom: https://en.wikipedia.org/wiki/Precision_and_recall.\n[39] Grandini M, Bagli E, Visani G. Metrics for multi-class classification: an overview. arXiv preprint\narXiv:200805756. 2020.\n[40] Aly M. Survey on multiclass classification methods. Neural Netw. 2005;19(1-9):2.\n[41] Minh D, Wang HX, Li YF, Nguyen TN. Explainable artificial intelligence: a comprehensive review. Artificial\nIntelligence Review. 2022:1-66.\n[42] K ¨ohler S, Gargano M, Matentzoglu N, Carmody LC, Lewis-Smith D, Vasilevsky NA, et al. The human phenotype\nontology in 2021. Nucleic acids research. 2021;49(D1):D1207-17.",
  "topic": "Throughput",
  "concepts": [
    {
      "name": "Throughput",
      "score": 0.6942787766456604
    },
    {
      "name": "Computer science",
      "score": 0.6066713929176331
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3840680718421936
    },
    {
      "name": "Natural language processing",
      "score": 0.367853581905365
    },
    {
      "name": "Operating system",
      "score": 0.058219075202941895
    },
    {
      "name": "Wireless",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I39422238",
      "name": "University of Illinois Chicago",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I20382870",
      "name": "Missouri University of Science and Technology",
      "country": "US"
    }
  ],
  "cited_by": 1
}