{
  "title": "Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking",
  "url": "https://openalex.org/W4389520486",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3016094439",
      "name": "Shengyao Zhuang",
      "affiliations": [
        "Commonwealth Scientific and Industrial Research Organisation"
      ]
    },
    {
      "id": "https://openalex.org/A1973602695",
      "name": "Bing Liu",
      "affiliations": [
        "Commonwealth Scientific and Industrial Research Organisation"
      ]
    },
    {
      "id": "https://openalex.org/A2154133250",
      "name": "Bevan Koopman",
      "affiliations": [
        "University of Queensland",
        "Commonwealth Scientific and Industrial Research Organisation"
      ]
    },
    {
      "id": "https://openalex.org/A1551779932",
      "name": "Guido Zuccon",
      "affiliations": [
        "University of Queensland"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4306317389",
    "https://openalex.org/W3118668786",
    "https://openalex.org/W3155114168",
    "https://openalex.org/W4313680149",
    "https://openalex.org/W4240913316",
    "https://openalex.org/W3100107515",
    "https://openalex.org/W4320813768",
    "https://openalex.org/W2798658104",
    "https://openalex.org/W4385571915",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1972594981",
    "https://openalex.org/W4284669679",
    "https://openalex.org/W3198691721",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3198073108",
    "https://openalex.org/W3193367516",
    "https://openalex.org/W4385573057",
    "https://openalex.org/W2032039936",
    "https://openalex.org/W4312091890",
    "https://openalex.org/W3174203100",
    "https://openalex.org/W4379468930",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4300427681",
    "https://openalex.org/W3104657626",
    "https://openalex.org/W3148323213",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4389520055",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W4297162632",
    "https://openalex.org/W3128581554",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3166441238",
    "https://openalex.org/W3198431451",
    "https://openalex.org/W4284682639",
    "https://openalex.org/W4206765718",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W4224980447"
  ],
  "abstract": "In the field of information retrieval, Query Likelihood Models (QLMs) rank documents based on the probability of generating the query given the content of a document. Recently, advanced large language models (LLMs) have emerged as effective QLMs, showcasing promising ranking capabilities. This paper focuses on investigating the genuine zero-shot ranking effectiveness of recent LLMs, which are solely pre-trained on unstructured text data without supervised instruction fine-tuning. Our findings reveal the robust zero-shot ranking ability of such LLMs, highlighting that additional instruction fine-tuning may hinder effectiveness unless a question generation task is present in the fine-tuning dataset. Furthermore, we introduce a novel state-of-the-art ranking system that integrates LLM-based QLMs with a hybrid zero-shot retriever, demonstrating exceptional effectiveness in both zero-shot and few-shot scenarios. We make our codebase publicly available at https://github.com/ielab/llm-qlm.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 8807–8817\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nOpen-source Large Language Models are Strong Zero-shot\nQuery Likelihood Models for Document Ranking\nShengyao Zhuang1, Bing Liu1, Bevan Koopman1,2, Guido Zuccon2\n1CSIRO, 2The University of Queensland\n1{shengyao.zhuang,bing.liu,bevan.koopman}@csiro.au\n2g.zuccon@uq.edu.au\nAbstract\nIn the field of information retrieval, Query Like-\nlihood Models (QLMs) rank documents based\non the probability of generating the query given\nthe content of a document. Recently, advanced\nlarge language models (LLMs) have emerged\nas effective QLMs, showcasing promising rank-\ning capabilities. This paper focuses on inves-\ntigating the genuine zero-shot ranking effec-\ntiveness of recent LLMs, which are solely pre-\ntrained on unstructured text data without super-\nvised instruction fine-tuning. Our findings re-\nveal the robust zero-shot ranking ability of such\nLLMs, highlighting that additional instruction\nfine-tuning may hinder effectiveness unless a\nquestion generation task is present in the fine-\ntuning dataset. Furthermore, we introduce a\nnovel state-of-the-art ranking system that inte-\ngrates LLM-based QLMs with a hybrid zero-\nshot retriever, demonstrating exceptional effec-\ntiveness in both zero-shot and few-shot scenar-\nios. We make our codebase publicly available\nat https://github.com/ielab/llm-qlm.\n1 Introduction\nRanking models (or rankers) are a fundamen-\ntal component in many information retrieval\n(IR) pipelines. Pre-trained language models\n(PLMs) have recently been leveraged across bi-\nencoder (Karpukhin et al., 2020; Xiong et al., 2021;\nZhuang and Zuccon, 2021c; Wang et al., 2022a;\nGao and Callan, 2022), cross-encoder (Nogueira\nand Cho, 2019; Nogueira et al., 2020; Zhuang et al.,\n2021), and sparse (Lin and Ma, 2021; Formal et al.,\n2021; Zhuang and Zuccon, 2021b) ranker architec-\ntures, showing impressive ranking effectiveness.\nDespite this success, the strong effectiveness\nof PLM-based rankers does not always generalise\nwithout sufficient in-domain training data (Thakur\net al., 2021; Zhuang and Zuccon, 2021a, 2022).\nTransferring knowledge from other domains has\nbeen used to overcome this issue (Lin et al., 2023)\nby training these rankers on large-scale supervised\nQA datasets such as MS MARCO (Nguyen et al.,\n2017). Alternatively, generative large language\nmodels (LLMs) like GPT3 (Brown et al., 2020)\nhave been used to synthesize domain-specific train-\ning queries, which are then used to train these\nrankers (Bonifacio et al., 2022; Dai et al., 2023).\nDespite their effectiveness, all of these methods\nconsume significant expenses in training a PLM-\nbased ranker.\nIn this paper, we consider a third avenue to ad-\ndress this challenge: leveraging LLMs to function\nas Query Likelihood Models (QLMs) (Ponte and\nCroft, 1998; Hiemstra, 2000; Zhai and Lafferty,\n2001). Essentially, QLMs are expected to under-\nstand the semantics of documents and queries, and\nestimate the possibility that each document can an-\nswer a certain query. Notably, recent advances in\nthis direction have greatly enhanced the ranking\neffectiveness of QLM-based rankers by leveraging\nPLMs like BERT (Devlin et al., 2019) and T5 (Raf-\nfel et al., 2020). These PLM-based QLMs are fine-\ntuned on query generation tasks and subsequently\nemployed to rank documents as per their likeli-\nhood (Nogueira dos Santos et al., 2020; Zhuang\net al., 2021; Lesota et al., 2021; Zhuang and Zuc-\ncon, 2021c).\nWe focus on a specific PLM-based QLM, the re-\ncently proposed Unsupervised Passage Re-ranker\n(UPR) (Sachan et al., 2022). UPR leverages ad-\nvanced LLMs to obtained the query likelihood esti-\nmations. Empirical results show that using the T0\nLLM (Sanh et al., 2022) as a QLM, large gains in\nranking effectiveness can be obtained. A key aspect\nof this work is that this effectiveness is obtained\nwithout requiring additional fine-tuning data, mak-\ning Sachan et al. highlight the zero-shot ranking ca-\npabilities of their LLM-based QLM. However, we\nargue that the experimental setting used by Sachan\net al. does not fully align with a genuine zero-shot\nscenario for the QLM ranking task. This is because\nT0 has already undergone fine-tuning on numer-\n8807\nous question generation (QG) tasks and datasets,\nsubsequent to its unsupervised pre-training.1 Con-\nsequently, there exists a discernible task leakage to\nthe downstream QLM ranking task, thereby render-\ning their approach more akin to a transfer learning\nsetting, rather than a true zero-shot approach.\nTo gain a comprehensive understanding of the\nzero-shot ranking capabilities of LLM-based QLM\nrankers, in this paper we take a fresh examination\nof this topic. Our approach involves harnessing\nthe power of state-of-the-art transformer decoder-\nonly LLMs, such as LLaMA (Touvron et al., 2023),\nwhich have undergone pre-training solely on un-\nstructured text through unsupervised next token\nprediction. Importantly, the models we consider\nhave not undergone any additional supervised in-\nstruction fine-tuning, ensuring a truly complete\nzero-shot setting for our investigation.\nWe further extend our analysis by comparing\nthe effectiveness of these LLMs with various popu-\nlar instruction-tuned LLMs in the context of zero-\nshot ranking tasks. Interestingly, our findings re-\nveal that further instruction fine-tuning adversely\naffects the effectiveness of QLM ranking, particu-\nlarly when the fine-tuning datasets lack specific QG\ntasks. This insight highlights the strong zero-shot\nQLM ranking ability of LLMs that solely rely on\npre-training, thereby suggesting that further instruc-\ntion fine-tuning is unnecessary for achieving strong\nzero-shot effectiveness. Building upon these in-\nsights, we push the boundaries of zero-shot ranking\neven further by integrating a hybrid zero-shot first-\nstage retrieval system, followed by re-ranking us-\ning the zero-shot LLM-based QLM re-rankers and\na relevance score interpolation technique (Wang\net al., 2021). Our approach achieves state-of-the-\nart effectiveness in zero-shot ranking on a subset\nof the BIER dataset (Thakur et al., 2021).\n2 Methodology\nZero-shot QLM re-ranker: We follow the setting\nintroduced in previous works (Zhuang et al., 2021;\nSachan et al., 2022) to evaluate the zero-shot QLM\nranking capability of LLMs. Specifically, given a\nsequence of query tokens q and a set D contain-\ning candidate documents retrieved by a first-stage\nzero-shot retriever such as BM25, the objective is\nto rank all candidate documents d∈D based on\n1There are at least 16 QG datasets according to the open-\nsourced T0 training: https://huggingface.co/datasets/\nbigscience/P3\nthe average log likelihood of generating all query\ntokens, as estimated by a LLM. The relevance scor-\ning function is defined as:\nSQLM(q,d) = 1\n|q|\n∑\nt\nlog LLM(qt|p,d, q<t) (1)\nhere, qt denotes the t-th token of the query, p\nis a model and task specific prompt used for\nprompting the LLM to behave like a question gen-\nerator (see Appendix A for more details), and\nLLM(qt|p,d, q<t) refers to the probability of gen-\nerating the token qt given the prompt p, the candi-\ndate document d, and the preceding query tokens\nq<t. It is important to note that, in a truly zero-shot\nranking pipeline, the first-stage retriever should be\na zero-shot method and the QLM re-ranker should\nexclusively be pre-trained on unsupervised unstruc-\ntured text data and no fine-tuning is performed\nusing any QG data.\nInterpolating with first-stage retriever: Fol-\nlowing Wang et al. (2021), instead of solely relay-\ning on the query likelihood scores estimated by the\nLLMs, we also linearly interpolate the QLM score\nwith the BM25 scores from the first-stage retriever\nby using the weighted score sum:\nS(q,d) =α·SBM25(q,d) + (1−α) ·SQLM(q,d), (2)\nHere, α∈[0,1] represents the weight assigned to\nbalance the contribution of the BM25 score and\nthe QLM score. In our experiments, we heuristi-\ncally apply min-max normalization to the scores\nand assign more weight to the QLM scores, given\nits pivotal role as the second-stage re-ranker. This\nis achieved by setting α = 0.2 without conduct-\ning any grid search. We use the python library\nranx2 (Bassani and Romelli, 2022) to implement\nthe interpolation algorithm.\nFew-shot QLM re-ranker: Since LLMs are\nstrong few-shot learners (Brown et al., 2020), we\nalso conducted experiments to explore how LLM-\nbased QLM re-rankers could be further enhanced\nby providing a minimal number of human-judged\nexamples. To achieve this, we employed a prompt\ntemplate known as “Guided by Bad Questions”\n(GBQ) (Bonifacio et al., 2022). The GBQ template\nconsists of only three document, good question,\nand bad question triples. We use it to guide the\nLLM-based QLM to produce more accurate query\nlikelihood estimations. We refer readers to the orig-\ninal paper for details about the GBQ template.\n2https://github.com/AmenRa/ranx\n8808\n3 Experimental Settings\nLLMs: Our focus is on the response of LLMs in\nthe QLM ranking task, specifically in a genuine\nzero-shot setting. To accomplish this, we used\nLLaMA (Touvron et al., 2023) and Falcon (Al-\nmazrouei et al., 2023), both of which are trans-\nformer decoder-only models that are pre-trained\nsolely on large, publicly available unstructured\ndatasets (Penedo et al., 2023). We specifically con-\nsider open-source LLMs because we can control\nthe data used to train them, thus guaranteeing no\nQG dataset was used.\nTo evaluate the influence of instruction fine-\ntuning data on QLM estimation, we compared these\nmodels with other well-known LLMs that were\nfine-tuned with instructions, including T5 (Raffel\net al., 2020), Alpaca (Taori et al., 2023), StableLM,\nStableVicuna, and Falcon-instruct (Almazrouei\net al., 2023). It is important to note that the fine-\ntuning instruction data for these models are un-\nlikely to include QG tasks.3 Additionally, we fol-\nlow Sachan et al. (2022) to include T0 (Sanh et al.,\n2022) and FlanT5 (Chung et al., 2022), which un-\nderwent fine-tuning specifically for QG instruc-\ntions. All LLMs used in this paper are openly\navailable, see Appendix B for more details.\nBaselines and datasets: In our evaluation, we\ncompared LLM-based QLMs with several exist-\ning methods, including BM25 (Robertson and\nZaragoza, 2009), QLM-Dirichlet (Zhai and Laf-\nferty, 2004), Contriever (Izacard et al., 2022), and\nHyDE (Gao et al., 2022), which are zero-shot\nfirst-stage retrievers. We also compared them\nwith fine-tuned retrievers and re-rankers trained on\nMS MARCO passage ranking data, representing\na transfer learning setting. Specifically, the evalu-\nated retrievers are Contriever-msmarco, SPLADE-\ndistill (Formal et al., 2022), and DRAGON+ (Lin\net al., 2023), while the re-rankers are T5-QLM-\nlarge (Zhuang et al., 2021), monoT5-3B (Nogueira\net al., 2020), and monoT5-3B-Inpars-v2 (Jeronymo\net al., 2023). Additionally, we compared our\nbest QLM ranking pipeline with PROMPTAGA-\nTOR (Dai et al., 2023), a state-of-the-art zero-shot\nand few-shot method. See Appendix C for detailed\ninformation about the baselines.\nTo ensure feasibility, we conducted experiments\non a popular subset of the BEIR benchmark\n3These models however employ the self-instruction ap-\nproach (Wang et al., 2022b), which may involve a small num-\nber of randomly generated QG instructions.\nTable 1: Main results. Re-rankers re-rank Top100 doc-\numents retrieved by BM25. Transferred retrievers and\nre-rankers are fine-tuned on MS MARCO.\nMethods TRECC DBpedia FiQA Robust04 Avg\nZero-shot Retrievers\nBM25 59.5 31.8 23.6 40.7 38.9\nQLM-Dirichlet 50.8 29.5 20.5 40.7 35.4\nContriever 23.3 29.2 24.5 31.6 27.2\nHyDE 58.2 37.2 26.6 41.8 41.0\nInstruction tuned QLM Re-rankers\nWithout QG task\nT5-3B 48.7 21.9 16.2 38.0 31.2\nT5-11B 67.9 33.7 31.0 27.4 40.3\nAlpaca-7B 67.1 35.0 33.7 44.6 45.1\nStableLM-7B 74.0 37.2 34.1 48.3 48.4\nStableVicuna-13B 71.8 39.4 39.1 51.3 50.4\nFalcon-7B-instruct 66.8 38.2 33.4 50.7 47.3\nFalcon-40B-instruct 70.2 40.5 40.9 51.3 50.7\nWith QG task\nT0-3B 71.6 38.8 41.4 50.1 50.5\nT0-11B 73.9 38.7 43.8 49.7 51.5\nFlanT5-3B 71.1 39.7 41.2 50.0 50.5\nFlanT5-11B 74.9 41.7 43.3 52.4 53.1\nZero-shot QLM Re-rankers\nLLaMA-7B 69.4 39.9 41.5 53.6 51.1\nLLaMA-13B 69.8 37.6 41.8 54.250.9\nFalcon-7B 73.3 41.7 41.3 52.5 52.2\nFalcon-40B 75.2 41.0 43.1 53.1 53.1\nTransferred Retrievers\nContriever-msmarco 59.6 41.3 32.9 47.3 45.3\nSPLADE-distill 71.1 44.2 35.1 45.8 49.1\nDRAGON+ 75.9 41.7 35.6 47.9 50.3\nTransferred Re-rankers\nT5-QLM-large 71.4 38.0 39.0 47.7 49.0\nmonoT5-3B 79.8 44.8 46.0 56.2 56.7\nmonoT5-3B-InPars-v2 83.8 46.6 46.1 58.5 58.8\ndatasets4: TRECC (V oorhees et al., 2021), DBPe-\ndia (Hasibi et al., 2017), FiQA (Maia et al., 2018),\nand Robust04 (V oorhees, 2005). The evaluation\nmetric used is nDCG@10, the official metric of the\nBEIR benchmark.\nStatistical significance analysis was performed\nusing Student’s two-tailed paired t-test with cor-\nrections, as per common practice in information\nretrieval. The results of this analysis is reported in\nAppendix D due to space constraints.\n4 Results\n4.1 Main results\nWe present our main results in Table 1, highlighting\nkey findings. For fair comparison, all the re-rankers\nconsider the top 100 documents retrieved by BM25.\nFirstly, it is evident that retrievers and re-rankers\nfine-tuned on MS MARCO training data consis-\ntently outperform zero-shot retrievers and QLM\n4Due to limited computational resources and numerous\nLLMs with various settings to run, and in order to ensure\nfeasibility, we considered a subset of BEIR that includes the\nmost widely used datasets in the literature. Despite being a\nsubset, it comprises a total of 1,347 queries with deep ranking\njudgments across 4 distinct domains.\n8809\nre-rankers across all datasets, except for T5-QLM-\nlarge, which is based on a smaller T5 model. This\noutcome is expected since these methods benefit\nfrom utilizing extensive human-judged QA training\ndata and the knowledge can be effectively trans-\nferred to the datasets we tested.\nOn the other hand, zero-shot QLMs and QG\nfine-tuned QLMs exhibit competitive, similar ef-\nfectiveness. This finding is somewhat surprising,\nconsidering that QG fine-tuned QLMs are explic-\nitly trained on QG tasks, making them a form of\ntransfer learning. This finding suggests that pre-\ntrained-only models such as LLaMA and Falcon\npossess strong zero-shot QLM ranking capabilities.\nAnother interesting finding is that instruction\ntuning can hinder LLMs’ QLM ranking ability if\nthe QG task is not included in the instruction fine-\ntuning data. This is evident in the results of Alpaca-\n7B, StableVicuna-13B, Falcon-7B-instruct and\nFalcon-40B-instruct, which are instruction-tuned\nversions of LLaMA-7B, LLaMA-13B, Falcon-7B\nand Falcon-40B, respectively. Our hypothesis to\nthis unexpected finding is that instruction-tuned\nmodels tend to pay more attention to the task in-\nstructions and less attention to the input content\nitself. Although they are good at following instruc-\ntions in the generation task, the most important\ninformation for evaluating query likelihood is in\nthe document content, thus instruction-tuning hurts\nquery likelihood estimation for LLMs. On the other\nhand, QG instruction-tuned LLMs show large im-\nprovements in QLM ranking. For example, the T0\nand FlanT5 models are QG-tuned versions of T5\nmodels, and they perform better. These results con-\nfirm that T0 and FlanT5 leverage their fine-tuning\ndata, thus should be considered within the transfer\nlearning setting.\nIn terms of model size, larger LLMs generally\ntend to be more effective, although there are ex-\nceptions. For instance, LLaMA-7B outperforms\nLLaMA-13B on DBpedia.\n4.2 Interpolation with BM25\nTable 2 demonstrates the impact of interpolating\nwith BM25 scores. Notably, we observe a large\ndecrease in the effectiveness of monoT5 re-rankers,\nwhich are trained on large-scale QA domain data,\nwhen interpolating with BM25. This finding aligns\nwith a study conducted by Yates et al. (2021). In\ncontrast, QLM re-rankers consistently exhibited\nhigher effectiveness across most datasets when us-\nTable 2: Interpolation results. Increased/decreased\nscores are noted with ↑/ ↓.\nMethods TRECC DBpedia FiQA Robust04 Avg\nwithout interpolationmonoT5-3B 79.8 44.8 46.0 56.2 56.7monoT5-3B-InPars-v2 83.8 46.6 46.1 58.5 58.8FlanT5-3B 72.0 37.0 41.7 47.0 49.4FlanT5-11B 75.1 39.9 44.9 50.8 52.7LLaMA-7B 68.0 37.5 41.8 51.6 49.7Falcon-7B 73.1 39.5 41.7 49.2 50.9\nwith interpolationmonoT5-3B 66.3 ↓ 44.6↓ 41.5↓ 55.1↓ 51.9↓\nmonoT5-3B-InPars-v2 82.1↓ 45.5↓ 43.5↓ 54.0↓ 56.3↓\nFlanT5-3B 71.1 ↓ 39.7↑ 41.2↓ 50.0↑ 50.5↑\nFlanT5-11B 74.9 ↓ 41.7↑ 43.3↓ 52.4↑ 53.1↑\nLLaMA-7B 69.4 ↑ 39.9↑ 41.5↓ 53.6↑ 51.1↑\nFalcon-7B 73.3 ↑ 41.7↑ 41.3↓ 52.5↑ 52.2↑\ning interpolation with BM25. It is worth noting\nthat this improvement is (almost) cost-free, as it\ndoes not require any additional relevance score es-\ntimation; it simply involves linearly interpolating\nwith scores from the first stage.\nWe note that the results in Table 2 are obtained\nby setting α = 0.2 without tuning this parameter\nbecause we are testing our method in zero-shot\nsetting where this parameter needs to be set with-\nout validation data. Nonetheless, we conduct a\npost-hoc analysis on TRECC to understand the sen-\nsitivity of this parameter. The results are presented\nin Figure 1. From the results, we can draw the\nfollowing conclusions:\n1. The interpolation strategy consistently has a\nnegative impact on monoT5-3B, while it con-\nsistently benefits instruction-tuned and zero-\nshot rerankers.\n2. Instruction-tuned rerankers consistently un-\nderperform their corresponding zero-shot\nrerankers, regardless of the set alpha value.\n3. Optimal values of αfor both instruction-tuned\nand zero-shot rerankers fall within the range\nof 0.1 to 0.4.\n4.3 Effective ranking pipeline\nIn Table 3 we push the boundary of our two-stage\nQLM ranking pipeline in both zero-shot and few-\nshot setting to obtain high ranking effectiveness.\nFor this purpose, we use the same linear interpola-\ntion as Equation 2 with α= 0.5 to combine BM25\nand HyDE as the zero-shot first-stage retriever. 5\nThe top 100 documents retrieved by this hybrid\nretriever are then re-ranked using QLMs.\n5This value was chosen to provide equal weight to the two\ncomponents, and no parameter exploration was undertaken.\n8810\nα\nNDCG@10\n55\n60\n65\n70\n75\n80\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nmonoT5-3B Falcon-7B-instruction Falcon-7B\nAlpaca-7B LLaMA-7B\nFigure 1: Impact of αon TRECC dataset.\nFirstly, our results suggest that the effectiveness\nof zero-shot first-stage retrieval can be improved\nby simply interpolating sparse and dense retrievers.\nMoreover, after QLM re-ranking, the nDCG@10\nvalues surpass those in Table 1. This indicates that\nzero-shot QLM re-rankers benefit from a stronger\nfirst-stage retriever, leading to improved overall\nranking effectiveness. For the few-shot results, we\nobserve that providing only three GBQ examples to\nthe model further enhances ranking effectiveness,\nalthough this effect is less pronounced for FlanT5.\nRemarkably, our QLM ranking pipeline achieves\nnDCG@10 on par with or higher than the state-of-\nthe-art PROMPTAGATOR method on comparable\ndatasets in both zero-shot and few-shot settings.\nIt is important to note that PROMPTAGATOR re-\nquires training on a large amount of synthetically\ngenerated data for both the retriever and re-ranker,\nwhereas our approach does not require any train-\ning. It’s worth highlighting that instruction-tuned\nLLMs continue to exhibit lower effectiveness com-\npared to their pre-trained-only LLMs, even when a\nbetter first-stage retriever is employed and under a\nfew-shot setting.\n5 Conclusion\nIn this paper, we adapt recent advanced LLMs into\nQLMs for ranking documents and comprehensively\nstudy their zero-shot ranking ability. Our results\nhighlight that these LLMs possess remarkable zero-\nshot ranking effectiveness. Moreover, we observe\nthat additional instruction fine-tuned LLMs unper-\nformed in this task. This important insight is over-\nlooked in previous studies. Furthermore, our study\nshows that by integrating LLM-based QLMs with\na hybrid zero-shot retriever, a novel state-of-the-art\nranking pipeline can be obtained that excels in both\nTable 3: Zero-shot/few-shot ranking systems.\n∗PROMPTAGATOR++ re-rankers use their own\nzero/few-shot PROMPTAGATOR first-stage retrievers,\nscores are copied from the original paper as the model\nis not publicly available. Other re-rankers consider the\nTop100 documents retrieved by BM25 + HyDE.\nMethods TRECC DBpedia FiQA Robust04 Avg\nZero-shot RetrieversBM25 + HyDE 69.8 41.7 30.9 49.7 48.0∗PROMPTAGATOR 72.7 36.4 40.4 - -Few-shot Retrievers∗PROMPTAGATOR 75.6 38.0 46.2 - -\nZero-shot Re-rankers∗PROMPTAGATOR++ 76.0 41.3 45.9 - -FlanT5-11B 75.8 46.2 49.5 56.6 57.0StableLM-7B 74.2 41.8 38.0 53.2 51.8Alpaca-7B 71.6 40.1 39.5 50.9 50.5LLaMA-7B 72.4 45.4 46.8 57.4 55.5Falcon-7B-instruct 68.8 43.1 37.4 54.6 51.0Falcon-7B 76.6 46.1 45.8 55.1 55.9Few-shot Re-rankers∗PROMPTAGATOR++ 76.2 43.4 49.4 - -FlanT5-11B 77.2 45.1 49.7 58.2 57.6StableLM-7B 72.2 42.8 38.0 51.7 51.2Alpaca-7B 72.3 42.5 41.8 53.0 52.4LLaMA-7B 77.8 47.7 50.4 59.5 58.8Falcon-7B-instruct 74.9 45.2 42.8 56.1 54.8Falcon-7B 78.6 48.0 48.6 59.0 58.5\nzero-shot and few-shot scenarios, showcasing the\neffectiveness and versatility of LLM-based QLMs.\nLimitations\nWhile theoretically our QLM method can be ap-\nplied to any LLM, for practical implementation,\naccess to the model output logits is required. There-\nfore, in this paper, our focus has been solely on\nopen-source LLMs where we can have access to the\nmodel weights. In contrast, approaches like Inpars\nand PROMPTAGATOR, which extract knowledge\nfrom the text produced by LLMs, do not require\naccess to the model weights. Common commer-\ncial API services that expose popular close-source\nmodels such as GPT-4, however, do not provide\naccess to model logits. offered by popular close-\nsource models such as GPT-4 . These can easily\nbe used within Inpars and PROMPTAGATOR by\ndirectly leveraging the generated text. However,\nour method cannot use these models because they\ndo not provide access to the logits. It might be\npossible that in future commercial LLM provides\nwould add functionalities in their APIs to access\nmodel logits.\nOur focus on open-source LLMs also offers us\nthe opportunity to scrutinise the data used to train\nthe LLMs to ascertain that no QG data was used.\nThis reassures a genuine zero-shot setting is consid-\nered, as opposed to previous work on LLM-based\n8811\nQLMs (Sachan et al., 2022). Although LLaMA\nand Falcon are primarily pre-trained using unsuper-\nvised learning on unstructured text data, it remains\npossible that the pre-training data contains text snip-\npets that serve as instructions and labels for the QG\ntask. In order to ascertain the authenticity of the\nzero-shot setting, it may be necessary to thoroughly\nanalyze and identify such text snippets within the\npre-training data.\nIn the paper, we could not report a complete sta-\ntistical significance analysis of the results due to\nspace limitation. Appendix D reports a detailed\nanalysis. However, our analysis was limited by the\nunavailability of run files for some of the models\npublished in previous works, as they were not re-\nleased by authors. In these cases, we could not\nperform statistical comparisons with respect to the\nruns we produced. We note this is a common prob-\nlem when authors do not release their models’ runs.\nWe make all run files available, along with code, at\nhttps://github.com/ielab/llm-qlm.\nReferences\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-\nshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nMerouane Debbah, Etienne Goffinet, Daniel Hes-\nlow, Julien Launay, Quentin Malartic, Badreddine\nNoune, Baptiste Pannier, and Guilherme Penedo.\n2023. Falcon-40B: an open large language model\nwith state-of-the-art performance.\nElias Bassani and Luca Romelli. 2022. ranx.fuse: A\npython library for metasearch. In CIKM, pages 4808–\n4812. ACM.\nLuiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and\nRodrigo Nogueira. 2022. Inpars: Unsupervised\ndataset generation for information retrieval. In Pro-\nceedings of the 45th International ACM SIGIR Con-\nference on Research and Development in Information\nRetrieval, SIGIR ’22, page 2387–2392, New York,\nNY , USA. Association for Computing Machinery.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nProceedings of the 34th International Conference on\nNeural Information Processing Systems , NIPS’20,\nRed Hook, NY , USA. Curran Associates Inc.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nZhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo\nNi, Jing Lu, Anton Bakalov, Kelvin Guu, Keith Hall,\nand Ming-Wei Chang. 2023. Promptagator: Few-\nshot dense retrieval from 8 examples. InThe Eleventh\nInternational Conference on Learning Representa-\ntions.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nThibault Formal, Carlos Lassance, Benjamin Pi-\nwowarski, and Stéphane Clinchant. 2022. From dis-\ntillation to hard negative sampling: Making sparse\nneural ir models more effective. In Proceedings of\nthe 45th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval,\nSIGIR ’22, page 2353–2359, New York, NY , USA.\nAssociation for Computing Machinery.\nThibault Formal, Benjamin Piwowarski, and Stéphane\nClinchant. 2021. Splade: Sparse lexical and expan-\nsion model for first stage ranking. In Proceedings\nof the 44th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval,\nSIGIR ’21, page 2288–2292, New York, NY , USA.\nAssociation for Computing Machinery.\nLuyu Gao and Jamie Callan. 2022. Unsupervised cor-\npus aware language model pre-training for dense pas-\nsage retrieval. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2843–2853,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nLuyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.\n2022. Precise zero-shot dense retrieval without rele-\nvance labels. arXiv preprint arXiv:2212.10496.\nFaegheh Hasibi, Fedor Nikolaev, Chenyan Xiong, Krisz-\ntian Balog, Svein Erik Bratsberg, Alexander Kotov,\nand Jamie Callan. 2017. Dbpedia-entity v2: A test\ncollection for entity search. In Proceedings of the\n40th International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval ,\nSIGIR ’17, page 1265–1268, New York, NY , USA.\nAssociation for Computing Machinery.\nDjoerd Hiemstra. 2000. Using language models for\ninformation retrieval. Ph.D. thesis, University of\nTwente.\n8812\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-\ntian Riedel, Piotr Bojanowski, Armand Joulin, and\nEdouard Grave. 2022. Unsupervised dense informa-\ntion retrieval with contrastive learning. Transactions\non Machine Learning Research.\nVitor Jeronymo, Luiz Bonifacio, Hugo Abonizio,\nMarzieh Fadaee, Roberto Lotufo, Jakub Zavrel, and\nRodrigo Nogueira. 2023. Inpars-v2: Large language\nmodels as efficient dataset generators for information\nretrieval. arXiv preprint arXiv:2301.01820.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nOleg Lesota, Navid Rekabsaz, Daniel Cohen, Klaus An-\ntonius Grasserbauer, Carsten Eickhoff, and Markus\nSchedl. 2021. A modern perspective on query likeli-\nhood with deep generative retrieval models. In Pro-\nceedings of the 2021 ACM SIGIR International Con-\nference on Theory of Information Retrieval, ICTIR\n’21, page 185–195, New York, NY , USA. Association\nfor Computing Machinery.\nJimmy Lin and Xueguang Ma. 2021. A few brief notes\non deepimpact, coil, and a conceptual framework\nfor information retrieval techniques. arXiv preprint\narXiv:2106.14807.\nSheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz,\nJimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun\nChen. 2023. How to train your dragon: Diverse\naugmentation towards generalizable dense retrieval.\narXiv preprint arXiv:2302.07452.\nXueguang Ma, Ronak Pradeep, Rodrigo Nogueira, and\nJimmy Lin. 2022. Document expansion baselines\nand learned sparse lexical representations for ms\nmarco v1 and v2. In Proceedings of the 45th In-\nternational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval , SIGIR ’22,\npage 3187–3197, New York, NY , USA. Association\nfor Computing Machinery.\nMacedo Maia, Siegfried Handschuh, André Freitas,\nBrian Davis, Ross McDermott, Manel Zarrouk, and\nAlexandra Balahur. 2018. Www’18 open challenge:\nFinancial opinion mining and question answering. In\nCompanion Proceedings of the The Web Conference\n2018, WWW ’18, page 1941–1942, Republic and\nCanton of Geneva, CHE. International World Wide\nWeb Conferences Steering Committee.\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\n2017. MS MARCO: A human-generated MAchine\nreading COmprehension dataset.\nRodrigo Nogueira and Kyunghyun Cho. 2019. Pas-\nsage re-ranking with bert. arXiv preprint\narXiv:1901.04085.\nRodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and\nJimmy Lin. 2020. Document ranking with a pre-\ntrained sequence-to-sequence model. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2020, pages 708–718, Online. Association\nfor Computational Linguistics.\nRodrigo Nogueira and Jimmy Lin. 2019. From\ndoc2query to docTTTTTquery.\nCicero Nogueira dos Santos, Xiaofei Ma, Ramesh Nalla-\npati, Zhiheng Huang, and Bing Xiang. 2020. Beyond\n[CLS] through ranking by generation. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n1722–1727, Online. Association for Computational\nLinguistics.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow,\nRuxandra Cojocaru, Alessandro Cappelli, Hamza\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The RefinedWeb dataset\nfor Falcon LLM: outperforming curated corpora\nwith web data, and web data only. arXiv preprint\narXiv:2306.01116.\nJay M. Ponte and W. Bruce Croft. 1998. A language\nmodeling approach to information retrieval. In Pro-\nceedings of the 21st Annual International ACM SI-\nGIR Conference on Research and Development in\nInformation Retrieval , SIGIR ’98, page 275–281,\nNew York, NY , USA. Association for Computing\nMachinery.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(1).\nStephen Robertson and Hugo Zaragoza. 2009. The\nprobabilistic relevance framework: Bm25 and be-\nyond. Found. Trends Inf. Retr., 3(4):333–389.\nDevendra Sachan, Mike Lewis, Mandar Joshi, Armen\nAghajanyan, Wen-tau Yih, Joelle Pineau, and Luke\nZettlemoyer. 2022. Improving passage retrieval with\nzero-shot question generation. In Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3781–3797, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\n8813\nThomas Wolf, and Alexander M Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In International Conference on Learning\nRepresentations.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. BEIR:\nA heterogeneous benchmark for zero-shot evaluation\nof information retrieval models. In Thirty-fifth Con-\nference on Neural Information Processing Systems\nDatasets and Benchmarks Track (Round 2).\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nEllen V oorhees, Tasmeer Alam, Steven Bedrick, Dina\nDemner-Fushman, William R. Hersh, Kyle Lo, Kirk\nRoberts, Ian Soboroff, and Lucy Lu Wang. 2021.\nTrec-covid: Constructing a pandemic information\nretrieval test collection. SIGIR Forum, 54(1).\nEllen M. V oorhees. 2005. The trec robust retrieval track.\nSIGIR Forum, 39(1):11–20.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao,\nLinjun Yang, Daxin Jiang, Rangan Majumder, and\nFuru Wei. 2022a. Simlm: Pre-training with represen-\ntation bottleneck for dense passage retrieval. arXiv\npreprint arXiv:2207.02578.\nShuai Wang, Shengyao Zhuang, and Guido Zuccon.\n2021. Bert-based dense retrievers require interpo-\nlation with bm25 for effective passage retrieval. In\nProceedings of the 2021 ACM SIGIR International\nConference on Theory of Information Retrieval, IC-\nTIR ’21, page 317–324, New York, NY , USA. Asso-\nciation for Computing Machinery.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022b. Self-instruct: Aligning lan-\nguage model with self generated instructions. arXiv\npreprint arXiv:2212.10560.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\nArnold Overwijk. 2021. Approximate nearest neigh-\nbor negative contrastive learning for dense text re-\ntrieval. In International Conference on Learning\nRepresentations.\nAndrew Yates, Rodrigo Nogueira, and Jimmy Lin. 2021.\nPretrained transformers for text ranking: BERT and\nbeyond. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies: Tutorials, pages 1–4, Online. Association\nfor Computational Linguistics.\nChengxiang Zhai and John Lafferty. 2001. A study of\nsmoothing methods for language models applied to\nad hoc information retrieval. In Proceedings of the\n24th Annual International ACM SIGIR Conference\non Research and Development in Information Re-\ntrieval, SIGIR ’01, page 334–342, New York, NY ,\nUSA. Association for Computing Machinery.\nChengxiang Zhai and John Lafferty. 2004. A study\nof smoothing methods for language models applied\nto information retrieval. ACM Trans. Inf. Syst. ,\n22(2):179–214.\nShengyao Zhuang, Hang Li, and Guido Zuccon. 2021.\nDeep query likelihood model for information re-\ntrieval. In Advances in Information Retrieval: 43rd\nEuropean Conference on IR Research, ECIR 2021,\nVirtual Event, March 28 – April 1, 2021, Proceedings,\nPart II, page 463–470, Berlin, Heidelberg. Springer-\nVerlag.\nShengyao Zhuang and Guido Zuccon. 2021a. Deal-\ning with typos for BERT-based passage retrieval and\nranking. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2836–2842, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nShengyao Zhuang and Guido Zuccon. 2021b. Fast pas-\nsage re-ranking with contextualized exact term match-\ning and efficient passage expansion. arXiv preprint\narXiv:2108.08513.\nShengyao Zhuang and Guido Zuccon. 2021c. Tilde:\nTerm independent likelihood model for passage re-\nranking. In Proceedings of the 44th International\nACM SIGIR Conference on Research and Devel-\nopment in Information Retrieval , SIGIR ’21, page\n1483–1492, New York, NY , USA. Association for\nComputing Machinery.\nShengyao Zhuang and Guido Zuccon. 2022. Character-\nbert and self-teaching for improving the robustness\nof dense retrievers on queries with typos. In Proceed-\nings of the 45th International ACM SIGIR Confer-\nence on Research and Development in Information\nRetrieval, SIGIR ’22, page 1444–1454, New York,\nNY , USA. Association for Computing Machinery.\n8814\nTable 4: Prompts used for each LLM-dataset pair. For Alpaca-7B and StableLM-7B we also prepend a system\nprompt according to the fine-tuning recipe of the each model. For Alpaca-7B is “Below is an instruction that\ndescribes a task, paired with an input that provides further context. Write a response that appropriately completes\nthe request.\\n\\n”. For SableLM-7B is “<|SYSTEM|># StableLM Tuned (Alpha version)\\n- StableLM is a helpful\nand harmless open-source AI language model developed by StabilityAI.\\n- StableLM is excited to be able to help\nthe user, but will refuse to do anything that could be considered harmful to the user.\\n- StableLM is more than just\nan information source, StableLM is also able to write poetry, short stories, and make jokes.\\n- StableLM will refuse\nto participate in anything that could harm a human.\\n”\nLLMs TRECC DBpedia FiQA Robust04T5-3B/T5-11B/FlanT5-3B/FlanT5-11BGenerate a question that is the most rel-evant to the given article’s title and ab-stract.\\n{doc}\nGenerate a query that includes an en-tity and is also highly relevant to thegiven Wikipedia page title and ab-stract.\\n{doc}\nGenerate a question that is the most rel-evant to the given document.\\n{doc}Generate a question that is the most rel-evant to the given document.\\n{doc}\nT0-3B/T0-11BPlease write a question based on thispassage.\\n{doc} Please write a question based on thispassage.\\n{doc} Please write a question based on thispassage.\\n{doc} Please write a question based on thispassage.\\n{doc}LLaMA-7B/LLaMA13B/Falcon-7B/Falcon-13B/Falcon-7B-instruct/Falcon-13B-instruct\nGenerate a question that is the most rel-evant to the given article’s title and ab-stract.\\n{doc}\\n\\nHere is a generatedrelevant question:\nGenerate a query that includes an en-tity and is also highly relevant to thegiven Wikipedia page title and ab-stract.\\n{doc}\\n\\nHere is a generatedrelevant question:\nGenerate a question that is the most rel-evant to the given document.\\nThe doc-ument: {doc}\\n\\nHere is a generatedrelevant question:\nGenerate a question that is the most rel-evant to the given document.\\nThe doc-ument: {doc}\\n\\nHere is a generatedrelevant question:\nAlpaca-7B ### Instruction:\\nGenerate a questionthat is the most relevant to the givenarticle’s title and abstract.\\n\\n### In-put:\\n{doc}\\n\\n### Response:\n### Instruction:\\nGenerate a querythat includes an entity and is alsohighly relevant to the given Wikipediapage title and abstract.\\n\\n### In-put:\\n{doc}\\n\\n###Response:\n### Instruction:\\nGenerate a questionthat is the most relevant to the given doc-ument.\\n\\n### Input:\\n{doc}\\n\\n###Response:\n### Instruction:\\nGenerate a questionthat is the most relevant to the given doc-ument.\\n\\n### Input:\\n{doc}\\n\\n###Response:\nStableLM-7B <|USER|>Generate a questionthat is the most relevant tothe given article’s title and ab-stract.\\n{doc}<|ASSISTANT|>Here isa generated relevant question:\n<|USER|>Generate a query that includesan entity and is also highly relevant tothe given Wikipedia page title and ab-stract.\\n{doc}<|ASSISTANT|>Here isa generated relevant question:\n<|USER|>Generate a questionthat is the most relevant to thegiven document.\\nThe document:{doc}<|ASSISTANT|>Here is agenerated relevant question:\n<|USER|>Generate a questionthat is the most relevant to thegiven document.\\nThe document:{doc}<|ASSISTANT|>Here is agenerated relevant questionStableVicuna-13B### Human: Generate a question that isthe most relevant to the given article’s ti-tle and abstract.\\n{doc}\\n### Assistant:Here is a generated relevant question:\n### Human: Generate a query that in-cludes an entity and is also highly rel-evant to the given Wikipedia page ti-tle and abstract.\\n{doc}\\n### Assistant:Here is a generated relevant question:\n### Human: Generate a question thatis the most relevant to the given docu-ment.\\nThe document: {doc}\\n### As-sistant: Here is a generated relevantquestion:\n### Human: Generate a question thatis the most relevant to the given docu-ment.\\nThe document: {doc}\\n### As-sistant: Here is a generated relevantquestion:\nA Models and datasets prompts\nGiven that various instruction-tuned LLMs might\nbe fine-tuned using diverse system and instruction\nprompts, coupled with the fact that datasets vary\nin document formats across different domains, it\nbecomes necessary to employ specific prompts tai-\nlored to each LLM-dataset pair to achieve optimal\nzero-shot ranking performance. Thus, we design\na prompt for each LLM-dataset pair based on the\nLLM usage instruction provided by the original\nauthors and dataset features. To facilitate clarity,\nwe have compiled a comprehensive list of all the\nprompts utilized for each LLM-dataset pair, which\ncan be found in Table 4.\nB List of Huggingface model names\nTable 5 provides links to the Huggingface model\nhub (Wolf et al., 2020) for the LLMs used in this\npaper. All the models can be conveniently down-\nloaded directly from the Huggingface model hub,\nwith the exception of Alpaca-7B. For Alpaca-7B,\nwe followed an open-sourced github repository to\nperform the fine-tuning of LLaMA-7B ourselves.\nC Descriptions of Baselines\n• BM25 (Robertson and Zaragoza, 2009): A\nwidely used statistical bag-of-words approach\nTable 5: Huggingface model hub links for LLMs used\nin this paper.\nLLMs Link\nT5-3B t5-3b\nT5-11B t5-11b\nStableLM-7B stabilityai/stablelm-tuned-alpha-7b\nStableVicuna-13BTheBloke/stable-vicuna-13B-HF\nFalcon-7B tiiuae/falcon-7b\nFalcon-7B-instructtiiuae/falcon-7b-instruct\nFalcon-40B tiiuae/falcon-40b\nFalcon-40B-instructtiiuae/falcon-40b-instruct\nT0-3B bigscience/T0_3B\nT0-11B bigscience/T0\nFlanT5-3B google/flan-t5-xl\nFlanT5-11B google/flan-t5-xxl\nLLaMA-7B huggyllama/llama-7b\nLLaMA-13B huggyllama/llama-13b\nAlpaca-7B https://github.com/tatsu-\nlab/stanford_alpaca\n8815\nTable 6: Overall effectiveness of the models and statistical significance analysis. The best results are highlighted in\nboldface. Superscripts denote significant differences (t-test, p≤0.05). x-> ydenotes the xretriever re-ranked by y\nre-ranker.\n# Model TRECC DBpedia FiQA Robust04\na BM25 59.5b 31.8b 23.6b 40.7e\nb QLM-Dirichlet 50.8 29.5 20.5 40.7e\nc HyDE 58.2 37.1abe 26.6ab 41.8e\nd BM25+HyDE 69.9abc 41.6abcefghiklmop 30.9abc 49.7abcef\ne BM25 -> T5-11B 67.9abc 33.7ab 32.0abc 27.4f BM25 -> Alpaca-7B 67.0abc 35.0ab 33.7abcd 44.6abe\ng BM25 -> StableLM-7B 74.0abcefi 37.2abef 34.1abcde 48.3abcef\nh BM25 -> StableVicuna-13B71.8abcfi 39.4abefgp 39.1abcdefgi 51.3abcefg\ni BM25 -> Falcon-7B-instruct66.8abc 38.2abef 33.4abcd 50.7abcefg\nj BM25 -> Falcon-40B-instruct70.2abc 40.5abcefgiklp 40.8abcdefghi 51.3abcefg\nk BM25 -> T0-3B 71.6abc 38.8abefg 41.4abcdefghi 50.1abcefg\nl BM25 -> T0-11B 73.9abcefijop 38.7abefg 43.8abcdefghijkmopq 49.7abcef\nm BM25 -> FlanT5-3B 71.1abc 39.7abcefgip 41.2abcdefghi 50.0abcefg\nn BM25 -> FlanT5-11B 74.9abcdefijkmop 41.7abcefghiklmop 43.3abcdefghijkmopq 52.4abcdefgiklm\no BM25 -> LLaMA-7B 69.4abc 39.9abcefgip 41.5abcdefghi 53.6abcdefghijklm\np BM25 -> LLaMA-13B 69.8abc 37.6abef 41.8abcdefghi 54.2abcdefghijklmn\nq BM25 -> Falcon-7B 73.3abcefiop 41.4abcefghiklmop 41.2abcdefghi 52.5abcdefgiklm\nr BM25 -> Falcon-40B 75.2abcdefijkmop 41.0abcefghiklop 43.1abcdefghijkmopq 53.1abcdefghijklm\ns BM25 -> monoT5-3B 79.8abcdefghijklmopqv 44.8abcdefghijklmnopqr46.0abcdefghijklmnopqr56.2abcdefghijklmnopqr\nt BM25 -> monoT5-3B-InPars-v283.7abcdefghijklmnopqrsuvwxyz46.5abcdefghijklmnopqrs46.1abcdefghijklmnopqr58.5abcdefghijklmnopqrsw\nu BM25+HyDE -> FlanT5-11B75.8abcdefijkmop 46.2abcdefghijklmnopqrx49.5abcdefghijklmnopqrstvw56.6abcdefghijklmnoqr\nv BM25+HyDE -> LLaMA-7B72.4abc 45.4abcdefghijklmnopqr46.8abcdefghijklmnopqr57.4abcdefghijklmnopqrw\nw BM25+HyDE -> Falcon-7B76.6abcdefijkmopv 46.1abcdefghijklmnopqr45.8abcdefghijklmnopqr55.1abcdefghijklmq\nx BM25+HyDE -> FlanT5-11B-fewshot77.2abcdefhijkmopuv 45.1abcdefghijklmnopqr49.7abcdefghijklmnopqrstvw58.3abcdefghijklmnopqruw\ny BM25+HyDE -> LLaMA-7B-fewshot77.8abcdefhijkmopv 47.7abcdefghijklmnopqrsuvwx50.4abcdefghijklmnopqrstvwz59.5abcdefghijklmnopqrsuvw\nz BM25+HyDE -> Falcon-7B-fewshot78.6abcdefghijkmopqv 48.0abcdefghijklmnopqrsuvwx48.6abcdefghijklmnopqrstvw59.0abcdefghijklmnopqrsuvw\nthat is commonly used as the zero-shot first-\nstage retrieval method. We use the Pyserini\n“two-click reproductions” (Ma et al., 2022) to\nproduce the BM25 results on BEIR datasets.\n• QLM-Dirichlet (Zhai and Lafferty, 2001):\nThe traditional QLM method that exploits\nterm statistics and Dirichlet smoothing tech-\nnique to estimate query likelihood, we also\nuse Pyserini implementation for this baseline.\n• Contriever (Izacard et al., 2022): A zero-shot\ndense retriever that pre-trained on text para-\ngraphs with unsupervised contrastive learning.\n• HyDE (Gao et al., 2022): A two-step zero-\nshot first-stage retriever that leverages gen-\nerative LLMs and Contriever. In the first\nstep, a prompt is provided to a LLM to gener-\nate multiple documents relevant to the given\nquery. Subsequently, in the second step,\nthe generated documents are encoded into\nvectors using the Contriever query encoder\nand then aggregated to form a new query\nvector for the search process. We utilized\nthe open-sourced implementation provided\nby the original authors for our experiments:\nhttps://github.com/texttron/hyde.\n• Contriver-msmarco. A Contriever check-\npoint further pre-trained on MS MARCO\ntraining data. We use the Pyserini provided\npre-build dense vector index and model check-\npoint for this baseline.\n• SPLADE-distill (Formal et al., 2022): A\nfirst-stage sparse retrieval model that exploits\nBERT PLM to learn query/document sparse\nterm expansion and weights. We use the Py-\nserini provided pre-build index and SPLADE\ncheckpoint to produce the results.\n• DRAGON+ (Lin et al., 2023): A dense re-\ntriever model that fine-tuned on augmented\nMS MARCO corpus and uses multiple retriev-\ners to conduct automatical relevance labeling.\nIt stands as the current state-of-the-art dense\nretriever in the transfer learning setting. We\nuse the scores reported on the BEIR learder-\nboard 6 for this baseline.\n• T5-QLM-large (Zhuang et al., 2021): A\nT5-based QLM method that fine-tuned on\nMS MARCO QG training data. We use the\nimplement this method with open-sourced\ndocTquery-T5 (Nogueira and Lin, 2019)\ncheckpoint 7.\n• monoT5-3B (Nogueira et al., 2020). A T5-\nbased cross-encoder re-ranker that fine-tuned\non MS MARCO training data. We use the\n6https://eval.ai/web/challenges/\nchallenge-page/1897/leaderboard/4475\n7castorini/doc2query-t5-large-msmarco\n8816\nopen-sourced implementation provided by In-\npars authors 8.\n• monoT5-3B-Inpars-v2 (Jeronymo et al.,\n2023): A T5-based cross-encoder re-ranker\nthat fine-tuned on MS MARCO training data\nand in-domain synthetic queries that gener-\nated by LLMs. It is the current state-of-the-art\nre-ranker in transfer learning setting. We use\nthe open-sourced implementation provided by\nthe original authors 9.\n• PROMPTAGATOR(Dai et al., 2023): These\nmethods consist of a Transformer encoder-\nbased retriever and re-ranker that are trained\nusing synthetic queries generated by LLMs.\nThey offer both zero-shot and few-shot set-\ntings. As public model checkpoints are not\ncurrently available, we refer to the scores re-\nported in the original paper as our point of\nreference for comparing against our own meth-\nods and baselines.\nD Statistical significance analysis\nIn Table 6 we report a statistical significance analy-\nsis for all the methods for which we can obtain a\nrun file, along with our methods. The analysis was\nperformed using the Student’s two-tailed paired t-\ntest with corrections, as per common practice in\ninformation retrieval. We used the Python toolkit\nranx (Bassani and Romelli, 2022) for generating\nthe report.\n8https://github.com/zetaalphavector/InPars\n9https://github.com/zetaalphavector/InPars\n8817",
  "topic": "Ranking (information retrieval)",
  "concepts": [
    {
      "name": "Ranking (information retrieval)",
      "score": 0.8338927030563354
    },
    {
      "name": "Computer science",
      "score": 0.7757576704025269
    },
    {
      "name": "Shot (pellet)",
      "score": 0.7069698572158813
    },
    {
      "name": "Information retrieval",
      "score": 0.5600592494010925
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.5530675649642944
    },
    {
      "name": "Task (project management)",
      "score": 0.5505030751228333
    },
    {
      "name": "Language model",
      "score": 0.519492506980896
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.4519590139389038
    },
    {
      "name": "Field (mathematics)",
      "score": 0.45185840129852295
    },
    {
      "name": "Codebase",
      "score": 0.43559950590133667
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3886420726776123
    },
    {
      "name": "Source code",
      "score": 0.20074257254600525
    },
    {
      "name": "Mathematics",
      "score": 0.07497945427894592
    },
    {
      "name": "Engineering",
      "score": 0.06197276711463928
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ]
}