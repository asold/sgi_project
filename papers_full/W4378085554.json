{
  "title": "Pre-trained Language Models",
  "url": "https://openalex.org/W4378085554",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4266796369",
      "name": "Gerhard Paaß",
      "affiliations": [
        "Fraunhofer Institute for Intelligent Analysis and Information Systems"
      ]
    },
    {
      "id": "https://openalex.org/A2800251637",
      "name": "Sven Giesselbach",
      "affiliations": [
        "Fraunhofer Institute for Intelligent Analysis and Information Systems"
      ]
    },
    {
      "id": "https://openalex.org/A4266796369",
      "name": "Gerhard Paaß",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2800251637",
      "name": "Sven Giesselbach",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2760268776",
    "https://openalex.org/W3176390156",
    "https://openalex.org/W4413288418",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W1787224781",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W71499226",
    "https://openalex.org/W3164280131",
    "https://openalex.org/W3174752098",
    "https://openalex.org/W2798820905",
    "https://openalex.org/W2163922914",
    "https://openalex.org/W6680532216",
    "https://openalex.org/W2110798204",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W4212883601",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3101981467",
    "https://openalex.org/W3125516434",
    "https://openalex.org/W3208624098",
    "https://openalex.org/W6629637077",
    "https://openalex.org/W2963773505",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2954460650",
    "https://openalex.org/W6683738474",
    "https://openalex.org/W3171087525",
    "https://openalex.org/W3168165246",
    "https://openalex.org/W3090395639",
    "https://openalex.org/W3147852756",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2970602317",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W3026554633",
    "https://openalex.org/W601603264",
    "https://openalex.org/W6743806954",
    "https://openalex.org/W3031696893",
    "https://openalex.org/W3183048323",
    "https://openalex.org/W2806659157",
    "https://openalex.org/W3117576675",
    "https://openalex.org/W4243640523",
    "https://openalex.org/W6730267373",
    "https://openalex.org/W2144499799",
    "https://openalex.org/W3035711539",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W2966284335",
    "https://openalex.org/W6761551260",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W3201139004",
    "https://openalex.org/W2969262604",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W2970463839",
    "https://openalex.org/W3174036644",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W2988841832",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3085380432",
    "https://openalex.org/W2153803020",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2560321925",
    "https://openalex.org/W2964231383",
    "https://openalex.org/W2133512280",
    "https://openalex.org/W3036332271",
    "https://openalex.org/W2172734211",
    "https://openalex.org/W2964159778",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W6796707148",
    "https://openalex.org/W2962863107",
    "https://openalex.org/W3035898727",
    "https://openalex.org/W2618851150",
    "https://openalex.org/W2947433239",
    "https://openalex.org/W3101597781",
    "https://openalex.org/W2740711318",
    "https://openalex.org/W2889326414",
    "https://openalex.org/W6784827901",
    "https://openalex.org/W3036276779",
    "https://openalex.org/W3185146124",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W2993587506",
    "https://openalex.org/W3167623166",
    "https://openalex.org/W4309088836",
    "https://openalex.org/W3133264589",
    "https://openalex.org/W3100688298",
    "https://openalex.org/W3146366485",
    "https://openalex.org/W6763143685",
    "https://openalex.org/W2160920198",
    "https://openalex.org/W1549465597",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2294798173",
    "https://openalex.org/W2908802752",
    "https://openalex.org/W3129831491",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2420245003",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2979736636",
    "https://openalex.org/W2916877561",
    "https://openalex.org/W6782529094",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W2950768109",
    "https://openalex.org/W6758657797",
    "https://openalex.org/W6734194636",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W3021293129",
    "https://openalex.org/W2229162816",
    "https://openalex.org/W2958089299",
    "https://openalex.org/W2950784811",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3168622113",
    "https://openalex.org/W3035251378",
    "https://openalex.org/W2167433878",
    "https://openalex.org/W3004547119",
    "https://openalex.org/W2974273066",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2975495759",
    "https://openalex.org/W3163465001",
    "https://openalex.org/W2767321762",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2945785363",
    "https://openalex.org/W2995273672",
    "https://openalex.org/W3035475042",
    "https://openalex.org/W2970903692",
    "https://openalex.org/W3120903390",
    "https://openalex.org/W6761205521",
    "https://openalex.org/W3089119299",
    "https://openalex.org/W3123920941",
    "https://openalex.org/W3124373176"
  ],
  "abstract": "Abstract This chapter presents the main architecture types of attention-based language models, which describe the distribution of tokens in texts: Autoencoders similar to BERT receive an input text and produce a contextual embedding for each token. Autoregressive language models similar to GPT receive a subsequence of tokens as input. They produce a contextual embedding for each token and predict the next token. In this way, all tokens of a text can successively be generated. Transformer Encoder-Decoders have the task to translate an input sequence to another sequence, e.g. for language translation. First they generate a contextual embedding for each input token by an autoencoder. Then these embeddings are used as input to an autoregressive language model, which sequentially generates the output sequence tokens. These models are usually pre-trained on a large general training set and often fine-tuned for a specific task. Therefore, they are collectively called Pre-trained Language Models (PLM). When the number of parameters of these models gets large, they often can be instructed by prompts and are called Foundation Models. In further sections we described details on optimization and regularization methods used for training. Finally, we analyze the uncertainty of model predictions and how predictions may be explained.",
  "full_text": "Chapter 2 \nPre-trained Language Models \nAbstract This chapter presents the main architecture types of attention-based \nlanguage models, which describe the distribution of tokens in texts: Autoencoders \nsimilar to BERT receive an input text and produce a contextual embedding for each \ntoken. Autoregressive language models similar to GPT receive a subsequence of \ntokens as input. They produce a contextual embedding for each token and predict \nthe next token. In this way, all tokens of a text can successively be generated. \nTransformer Encoder-Decoders have the task to translate an input sequence to \nanother sequence, e.g. for language translation. First they generate a contextual \nembedding for each input token by an autoencoder. Then these embeddings are \nused as input to an autoregressive language model, which sequentially generates \nthe output sequence tokens. These models are usually pre-trained on a large general \ntraining set and often ﬁne-tuned for a speciﬁc task. Therefore, they are collectively \ncalled Pre-trained Language Models (PLM). When the number of parameters of \nthese models gets large, they often can be instructed by prompts and are called \nFoundation Models. In further sections we described details on optimization and \nregularization methods used for training. Finally, we analyze the uncertainty of \nmodel predictions and how predictions may be explained. \nKeywords BERT · Language model · GPT-2 · Transformer · Pre-training · \nFine-tuning · Sequence-to-sequence model \nA model that either computes the joint probability or the conditional probability \nof natural language texts is called a language model as it potentially covers all \ninformation about the language. In this chapter, we present the main architecture \ntypes of attention-based language models ( LMs), which process texts consisting of \nsequences of tokens , i.e. words, numbers, punctuation, etc.: \n• Autoencoders ( AE) receive an input text and produce a contextual embedding \nfor each token. These models are also called BERT models and are described in \nSect. 2.1.\n© The Author(s) 2023 \nG. Paaß, S. Giesselbach, Foundation Models for Natural Language Processing , \nArtiﬁcial Intelligence: Foundations, Theory, and Algorithms, \nhttps://doi.org/10.1007/978-3-031-23190-2_2\n19\n20 2 Pre-trained Language Models\n• Autoregressive language models (AR) receive a subsequence .v1,...,v t−1 of \ntokens of the input text. They generate contextual embeddings for each token \nand use them to predict the next token . vt. In this way, they can successively \npredict all tokens of the sequence. These models are also called GPT models and \nare outlined in Sect. 2.2. \n• Transformer Encoder-Decoders have the task to translate an input sequence in to \nanother sequence, e.g. for language translation. First they generate a contextual \nembedding for each input token by an autoencoder. Then these embeddings are \nused as input to an autoregressive language model, which sequentially generates \nthe output sequence tokens. These models are also called Transformers and are \ndeﬁned in Sect.\n2.3.\nIn this chapter, we focus on NLP , where we consider sequences of text tokens. \nHistorically, the transformer encoder-decoder was developed in 2017 by V aswani \net al. [ 141] to perform translation of text into another language. The autoencoder \n[39] and the autoregressive language model [ 118] are the encoder-part and the \ndecoder-part of this transformer encoder-decoder and were proposed later. As they \nare conceptually simpler, they are introduced in preceding sections. A ﬁnal section \n(Sect.\n2.4) describes methods for optimizing models during training, determining a \nmodel architecture, and estimating the uncertainty of model predictions. \nIt turned out that the models can ﬁrst be trained on a large training set of general \ntext documents and are able to acquire the distribution of tokens in correct and ﬂuent \nlanguage. Subsequently, they can be adapted to a speciﬁc task, e.g. by ﬁne-tuning \nwith a small supervised classiﬁcation task. Therefore, the models are called Pre-\ntrained Language models. \nAs we will see later, all models can be applied to arbitrary sequences, e.g. musical \nnotes, sound, speech, images, or even videos. When the number of parameters of \nthese models gets large, they often can be instructed by prompts and are called \nFoundation Models. \n2.1 BERT: Self-Attention and Contextual Embeddings \nCommon words often have a large number of different meanings. For the word \n“bank”, for instance, the lexical database WordNet [ 94] lists 18 different senses \nfrom “sloping land” to “ﬁnancial institution” . In a simple embedding of the word \n“bank” introduced in Sect. 1.5 all these meanings are conﬂated. As a consequence, \nthe interpretation of text based on these embeddings is ﬂawed. \nAs an alternative, contextual embeddings or contextualized embeddings were \ndeveloped, where the details of a word embedding depend on the word itself as \nwell as on the neighboring words occurring in the speciﬁc document. Consequently, \neach occurrence of the same word in the text has a different embedding depending \non the context. Starting with the Transformer [\n141], a number of approaches have \nbeen designed to generate these contextual embeddings, which are generally trained \nin an unsupervised manner using a large corpus of documents.\n2.1 BERT: Self-Attention and Contextual Embeddings 21\nBERT (Bidirectional Encoder Representations from Transformers) was pro-\nposed by Devlin et al. [ 39] and is the most important approach for generating \ncontextual embeddings. BERT is based on the concept of attention [ 8] and on \nprior work by V aswani et al. [ 141]. The notion of attention is inspired by a brain \nmechanism that tends to focus on distinctive parts of memory when processing \nlarge amounts of information. The details of the computations are explained by \nRush [\n126]. \n2.1.1 BERT Input Embeddings and Self-Attention \nAs input BERT takes some text which is converted to tokens, e.g. by the Wordpiece \ntokenizer (Sect. 1.2) with a vocabulary of a selected size, e.g. 30,000. This means \nthat frequent words like “dog” are represented by a token of their own, but more \nrare words like “playing” are split into several tokens, e.g. “play” and “##ing”, \nwhere “##” indicates that the token is part of a word. As all characters are retained \nas tokens, arbitrary words may be represented by a few tokens. In addition, there \nare special tokens like [CLS] at the ﬁrst position of the input text and two “[SEP]” \ntokens marking the end of text segments. Finally, during training, there are [MASK] \ntokens as explained later. Each token is represented by a token embedding, a vector \nof ﬁxed length .demb,e . g ..demb =768 . Input sequences of variable length are padded \nto the maximal length with a special padding token. \nSince all token embeddings are processed simultaneously, the tokens need an \nindication of their position in the input text. Therefore, each position is marked \nwith position embeddings of the same length as the token embeddings, which \nencode the position index. The BERT paper encodes the position number by \ntrainable embeddings, which are added to the input token embeddings [ 39]. Finally, \nBERT compares the ﬁrst and second input segment. Therefore, the algorithm needs \nthe information, which token belongs to the ﬁrst and second segment. This is \nalso encoded by a trainable segment embedding added to the token and position \nembedding. The sum of all embeddings is used as input embedding for BERT. An \nexample is shown in Fig.\n2.1. \nSelf-Attention to Generate Contextual Embeddings \nBERT starts with input embeddings . xt of length .demb for each token . vt of the input \nsequence .v1,...,v T . These embeddings are transformed by linear mappings to so-\ncalled query-vectors . qt, key-vectors . kt and value-vectors . vt. These are computed by \nmultiplying . xt with the matrices .W(q), .W(k), and .W(v) with dimensions .demb ×d q, \n.demb ×d q and .demb ×d v respectively \n.q⊺\nt =x ⊺\nt\nW(q) k⊺\nt\n=x ⊺\nt\nW(k) v⊺\nt\n=x ⊺\nt\nW(v). (2.1)\n22 2 Pre-trained Language Models\n[ ] [ ] [ ] ## [ ] \n1 2 3 4 5 7 8 9 10 116 \n++++++++++ \n++++++++++ \n[ ] my dog is [ ] [ ] he likes play ##ing [ ]input tokens \ntoken \nembeddings \nsegment \nembeddings \nposi/g415on \nembeddings \n+\n+\nFig. 2.1 The input of the BERT model consist of a sequence of embeddings corresponding to the \ninput tokens. Each token is represented by a sum consisting of the embedding of the token text, the \nembedding of its segment indicator and an embedding of its position [\n39] \nNote that the query- and key-vectors have the same length. Then scalar products \n.q⊺\nr kt between the query-vector . qr of a target token . vr and the key-vectors . kt of all \ntokens of the sequence are computed: \n.(αr,1,...,α r,T )=softmax\n( q⊺\nr\nk1√dk\n,..., q⊺\nr\nkT√dk\n)\n. (2.2) \nEach scalar product yields a real-valued association score .(q⊺\nr\nkt)/√dk between \nthe tokens, which depends on the matrices .W(q) and .W(k). This association score is \ncalled scaled dot-product attention. It is normalized to a probability score . αr,t by the \nsoftmax function. The factor .1/√dk avoids large values, where the softmax function \nhas only tiny gradients. With these weights a weighted average of the value vectors \n. vt of all sequence elements is formed yielding the new embedding . ˘xr of length . dv\nfor the target token . vr: \n.˘xr =α r,1 ∗v 1 +···+α r,T ∗v T . (2.3) \nThis algorithm is called self-attention and was ﬁrst proposed by V aswani et al. [ 141]. \nFigure 2.2 shows the computations for the r-th token “mouse”. Note that the \nresulting embedding is a contextual embedding as it includes information about all \nwords in the input text. A component of . vt gets a high weight whenever the scalar \nproduct .q⊺\nr kt is large. It measures a speciﬁc form of a correlation between . xr and \n. xt and is maximal if the vector .x⊺\nr\nW(q) points in the same direction as .x⊺\nt W(k). \nThe self-attention mechanism in general is non-symmetric, as the matrices . W(q)\nand .W(k) are different. If token . vi has a high attention to token . vj (i.e. . q⊺\ni kj\nis large), this does not necessarily mean that . vj will highly attend to token . vi\n(i.e. .q⊺\nj\nki also is large). The inﬂuence of . vi on the contextual embedding of . vj\ntherefore is different from the inﬂuence of . vj on the contextual embedding of . vi. \nConsider the following example text “Fred gave roses to Mary” . Here the word \n“gave” has different relations to the remaining words. “Fred” is the person who \nis performing the giving, “roses” are the objects been given, and “Mary” is the \nrecipient of the given objects. Obviously these semantic role relations are non-\nsymmetric. Therefore, they can be captured with the different matrices .W(q) and \n.W(k) and can be encoded in the embeddings.\n2.1 BERT: Self-Attention and Contextual Embeddings 23\nFig. 2.2 Computation of a contextual embedding for a single token “mouse” by self-attention. By \nincluding the embedding of “cheese”, the embedding of mouse can be shifted to the meaning of \n“rodent” and away from “computer pointing device”. Such an embedding is computed for every \nword of the input sequence \nSelf-attention allows for shorter computation paths and provides direct avenues \nto compare distant elements in the input sequence, such as a pronoun and its \nantecedent in a sentence. The multiplicative interaction involved in attention \nprovides a more ﬂexible alternative to the inﬂexible ﬁxed-weight computation of \nMLPs and CNNs by dynamically adjusting the computation to the input at hand. \nThis is especially useful for language modeling, where, for instance, the sentence \n“She ate the ice-cream with the X” is processed. While a feed-forward network \nwould always process it in the same way, an attention-based model could adapt its \ncomputation to the input and update the contextual embedding of the word \n“ate” if \nX is “spoon”, or update the embedding of “ice-cream” if X refers to “strawberries” \n[17]. \nIn practice all query, key, and value vectors are computed in parallel by . Q=\nXW(q), .K=XW (k), .V=XW (v), where . X is the .T×d emb matrix of input \nembeddings [ 141]. The query-vectors . qt, key-vectors . kt and value vectors . vt are \nthe rows of . Q, . K, . V respectively. Then the self-attention output matrix A TTL(X) is \ncalculated by one large matrix expression \n. ˘X=A TTL (X)=A TTL (Q,K,V)=softmax\n( QK⊺\n√dk\n)\nV, (2.4)\n24 2 Pre-trained Language Models\nresulting in a .T×d v-matrix . ˘X.I t s r-th row contains the new embedding . ˘xr of the \nr-th token . vr. \nA number of alternative compatibility measures instead of the scaled dot-product \nattention (2.2) have been proposed. They are, however, rarely used in PLMs, as \ndescribed in the surveys [ 27, 46]. \nIt turns out that a single self-attention module is not sufﬁcient to characterize \nthe tokens. Therefore, in a layer .dhead parallel self-attentions are computed with \ndifferent matrices .W(q)\nm , .W(k)\nm , and .W(v)\nm\n, .m=1,...,d head, yielding partial new \nembeddings \n. ˘Xm =A TTL (XW(q)\nm ,XW (k)\nm ,XW (v)\nm\n). (2.5) \nThe emerging partial embeddings .˘xm,t for a token . vt are able to concentrate on \ncomplementary semantic aspects, which develop during training. \nThe BERT .BASE model has .dhead =12 of these parallel attention heads.T h e \nlengths of these head embeddings are only a fraction .demb/dhead of the original \nlength .demb. The resulting embeddings are concatenated and multiplied with a \n.(dhead ∗d v)×d emb-matrix .W(o) yielding the matrix of intermediate embeddings \n. ˘X=\n[\n˘X1,..., ˘Xdhead\n]\nW0, (2.6) \nwhere .W0 is a parameter matrix. If the length of the input embeddings is .demb, \nthe length of the query, key, and value vector is chosen as .dk =d v =d emb/dhead. \nTherefore, the concatenation again creates a .T×d emb matrix . ˘X. This setup is called \nmulti-head self-attention. Because of the reduced dimension of the individual heads, \nthe total computational cost is similar to that of a single-head attention with full \ndimensionality. \nSubsequently, each row of . ˘X, the intermediate embedding vectors . ˘x⊺\nt ,i s \nconverted by a fully connected layer F CL with a ReLU activation followed by \nanother linear transformation [ 141] \n.˜x⊺\nt =F CL(˘xt)=ReLU( ˘x⊺\nt\n∗W 1 +b ⊺\n1 )∗W 2 +b ⊺\n2 . (2.7) \nThe matrices .W0,W 1,W 2 and the vectors .b1,b 2 are parameters. These transfor-\nmations are the same for each token . vt of the sequence yielding the embedding . ˜xt. \nTo improve training speed, residual connections are added as a “bypass”, which \nsimply copy the input. They were shown to be extremely helpful for the optimization \nof multi-layer image classiﬁers [ 54]. In addition, layer normalization [ 6]i su s e d \nfor regularization (Sect. 2.4.2), as shown in Fig. 2.3. Together the multi-head self-\nattention ( 2.5), the concatenation ( 2.6), and the fully connected layer ( 2.7)f o r ma n \nencoder block. \nThis procedure is repeated for a number of k layers with different encoder blocks, \nusing the output embeddings of one block as input embeddings of the next block. \nThis setup is shown in Fig.\n2.4. The embeddings .˜xk,t of the last encoder block\n2.1 BERT: Self-Attention and Contextual Embeddings 25\nFig. 2.3 Multi-head self-attention computes self-attentions for each layer l and head m with \ndifferent matrices .W(q)\nl,m, .W(k)\nl,m,a n d .W(v)\nl,m\n. In this way, different aspects of the association \nbetween token pairs, e.g. “mouse” and “cheese”, can be computed. The resulting embeddings are \nconcatenated and transformed by a feedforward network. In addition, residual connections and \nlayer normalization improve training convergence [\n39] \nFig. 2.4 Parallel computation of contextual embeddings in each encoder block by BERT. The \noutput embeddings of an encoder block are used as input embeddings of the next encoder block. \nFinally, masked tokens are predicted by a logistic classiﬁer L using the corresponding contextual \nembedding of the last encoder block as input\n26 2 Pre-trained Language Models\nprovides the desired contextual embeddings. The structure of an encoder block \novercomes the limitations of RNNs (namely the sequential nature of RNNs) by \nallowing each token in the input sequence to directly determine associations with \nevery other token in the sequence. BERT\n.BASE has .k=12 encoder blocks. It was \ndeveloped at Google by Devlin et al. [ 39]. More details on the implementation of \nself-attention can be found in these papers [ 38, 41, 126]. \n2.1.2 Training BERT by Predicting Masked T okens \nThe BERT model has a large number of unknown parameters. These parameters are \ntrained in a two-step procedure. \n• Pre-training enables the model to acquire general knowledge about language in \nan unsupervised way. The model has the task to ﬁll in missing words in a text. \nAs no manual annotation is required, pre-training can use large text corpora. \n• Fine-tuning adjusts the pre-trained model to a speciﬁc task, e.g. sentiment \nanalysis. Here, the model parameters are adapted to solve this task using a smaller \nlabeled training dataset. \nThe performance on the ﬁne-tuning task is much better than without pre-training \nbecause the model can use the knowledge acquired during pre-training through \ntransfer learning. \nTo pre-train the model parameters, a training task is designed: the masked \nlanguage model (MLM). Roughly 15% of the input tokens in the training documents \nare selected for prediction, which is performed by a logistic classiﬁer (Sect.\n1.3) \n.p(Vt|v1,...,v t−1 ,vt+1 ...,v T )=softmax(A ˜xk,t +b), (2.8) \nreceiving the embedding .˜xk,t of the last layer at position t as input to predict the \nrandom variable . Vt of possible tokens at position t. This approach avoids cycles \nwhere words can indirectly “see themselves”. \nThe tokens to be predicted have to be changed, as otherwise the prediction would \nbe trivial. Therefore, a token selected for prediction is replaced by: \n• a special [MASK] token for 80% of the time (e.g., “the mouse likes cheese” \nbecomes “the mouse [MASK] cheese” ); \n• a random token for 10% of the time (e.g., “the mouse likes cheese” becomes “the \nmouse absent cheese” ); \n• the unchanged label token for 10% of the time (e.g., “the mouse likes cheese” \nbecomes “the mouse likes cheese” ). \nThe second and third variants were introduced, as there is a discrepancy between \npre-training and the subsequent ﬁne-tuning, were there is no \n[MASK] token. The \nauthors mitigate this issue by occasionally replacing [MASK] with the original \ntoken, or by sampling from the vocabulary. Note that in 1.5% of the cases a\n2.1 BERT: Self-Attention and Contextual Embeddings 27\nrandom token is inserted. This occasional noise encourages BERT to be less biased \ntowards the masked token (especially when the label token remains unchanged) \nin its bidirectional context encoding. To predict the masked token, BERT has to \nconcentrate all knowledge about this token in the corresponding output embedding \nof the last layer, which is the input to the logistic classiﬁer. Therefore, it is often \ncalled an autoencoder, which generates extremely rich output embeddings. \nIn addition to predicting the masked tokens, BERT also has to predict, whether \nthe next sentence is a randomly chosen sentence or the actual following sentence \n(next sentence prediction). This requires BERT to consider the relation between two \nconsecutive pieces of text. Again a logistic classiﬁer receiving the embedding of the \nﬁrst \n[CLS] token is used for this classiﬁcation. However, this task did not have a \nmajor impact on BERT’s performance, as BERT simply learned if the topics of both \nsentences are similar [ 158]. \nIn Fig. 2.4 the task is to predict a high probability of the token “likes” for the \ninput text “The mouse [MASK] cheese” . At the beginning of the training this \nprobability will be very small ( .≈1/no. of tokens ). By backpropagation for each \nunknown parameter the derivative can be determined, indicating how the parameters \nshould be changed to increase the probability of \n“likes”. The unknown parameters \nof BERT comprise the input embeddings for each token of the vocabulary, the \nposition embeddings for each position, matrices .W(q)\nl,m, .W(k)\nl,m, .W(v)\nl,m\nfor each layer \nl and attention head m ( 2.4), the parameters of the fully connected layers ( 2.7)a s \nwell as .A,b of the logistic classiﬁer ( 2.8). BERT uses the Adam algorithm [ 69]f o r \nstochastic gradient descent. \nThe BERT.BASE model has a hidden size of .demb =768, .k=12 encoder blocks \neach with .dhead =12 attention heads, and a total of 110 million parameters. The \nBERT.LARGE model has a hidden size of .demb =1024, and .k=24 encoder blocks \neach with .dhead =16 attention heads and a total of 340 million parameters [ 39]. The \nEnglish Wikipedia and a book corpus with 3.3 billion words were encoded by the \nWordPiece tokenizer [ 154] with a vocabulary of 30,000 tokens and used to pre-train \nBERT. No annotations of the texts by humans were required, so the training is self-\nsupervised. The pre-training took 4 days on 64 TPU chips, which are very fast GPU \nchips allowing parallel processing. Fine-tuning can be done on a single Graphical \nProcessing Unit (GPU). \nTo predict the masked tokens, the model has to learn many types of language \nunderstanding features: syntax (\n[MASK] is a good position for a verb), seman-\ntics (e.g. the mouse prefers cheese), pragmatics, coreference, etc. Note that the \ncomputations can be processed in parallel for each token of the input sequence, \neliminating the sequential dependency in Recurrent Neural Networks. This par-\nallelism enables BERT and related models to leverage the full power of modern \nSIMD (single instruction multiple data) hardware accelerators like GPUs/TPUs, \nthereby facilitating training of NLP models on datasets of unprecedented size. \nReconstructing missing tokens in a sentence has long been used in psychology. \nTherefore, predicting masked tokens is also called a cloze task from ‘closure’ in \nGestalt theory (a school of psychology).\n28 2 Pre-trained Language Models\nIt turns out that BERT achieves excellent results for the prediction of the masked \ntokens, and that additional encoder blocks markedly increase the accuracy. For \nexample, BERT is able to predict the original words (or parts of words) with an \naccuracy of 45.9%, although in many cases several values are valid at the target \nposition [\n125]. In contrast to conventional language models, the MLM takes into \naccount the tokens before and after the masked target token. Hence, it is called a \nbidirectional encoder. In addition, self-attention directly provides the relation to \ndistant tokens without recurrent model application. Finally, self-attention is fast, as \nit can be computed in parallel for all input tokens of an encoder block. \n2.1.3 Fine-Tuning BERT to Downstream T asks \nNeural networks have already been pre-trained many years ago [ 16], but the success \nof pre-training has become more evident in recent years. During pre-training BERT \nlearns general syntactic and semantic properties of the language. This can be \nexploited for a special training task during subsequent ﬁne-tuning with a modiﬁed \ntraining task. This approach is also called transfer learning as the knowledge \nacquired during pre-training is transferred to a related application. In contrast to \nother models, BERT requires minimal architecture changes for a wide range of \nnatural language processing tasks. At the time of its publication, BERT improved \nthe S\nOTA on various natural language processing tasks. \nUsually, a ﬁne-tuning task requires a classiﬁcation, solved by applying a logistic \nclassiﬁer L to the output embedding .˜xk,1 of the [CLS] token at position 1 of \nBERT’s last encoder block. There are different types of ﬁne-tuning tasks, as shown \nin Fig.\n2.5. \n• Text classiﬁcation assigns a sentence to one of two or more classes. Examples are \nthe classiﬁcation of restaurant reviews as positive/negative or the categorization \nof sentences as good/bad English. Here the output embedding of the start token \n[CLS] is used as input to L to generate the ﬁnal classiﬁcation. \n• Text pair classiﬁcation compares two sentences separated by “[SEP]”. Examples \ninclude classifying whether the second sentence implies, contradicts, or is neutral \nwith respect to the ﬁrst sentence, or whether the two sentences are semantically \nequivalent. Again the output embedding of the start token \n[CLS] is used as \ninput to L. Sometimes more than one sentence is compared to the root sentence. \nThen outputs are computed for every sentence pair and jointly normalized to a \nprobability. \n• Word annotation marks each word or token of the input text with a speciﬁc \nproperty. An example is Named Entity Recognition (NER) annotating the tokens \nwith ﬁve name classes (e.g. “person”, “location”, \n...., “other”). Here the same \nlogistic model L is applied to every token output embedding .˜xk,t at position t \nand yields a probability vector of the different entity classes.\n2.1 BERT: Self-Attention and Contextual Embeddings 29\nFig. 2.5 For ﬁne-tuning, BERT is enhanced with an additional layer containing one or more \nlogistic classiﬁers L using the embeddings of the last layer as inputs. This setup may be employed \nfor text classiﬁcation and comparison of texts with the embedding of [CLS] as input of the logistic \nclassiﬁer. For sequence tagging, L predicts a class for each sequence token. For span prediction, \ntwo logistic classiﬁers . L1 and . L2 predict the start and end of the answer phrase [ 39] \n• Span prediction tags a short sequence of tokens within a text. An example is \nquestion answering. The input to BERT consists of a question followed by \n“[SEP]” and a context text, which is assumed to contain the answer. Here two \ndifferent logistic classiﬁers L and . ˜L are applied to every token output embedding \n.˜xk,t of the context and generate the probability that the answer to the question \nstarts/ends at the speciﬁc position. The valid span (i.e. the end is not before \nthe start) with the highest sum of start/end scores is selected as the answer. An \nexample is the input “[CLS] When did Caesar die ? [SEP] . . . On the Ides of \nMarch, 44 BC, Caesar was assassinated by a group of rebellious senators ...” , \nwhere the answer to the question is the span “Ides. start of March, 44 BC . end”. Span \nprediction may be applied to a number of similar tasks. \nTherefore, BERT just needs an extra layer with one or more logistic classiﬁers for \nﬁne-tuning. During ﬁne-tuning with a downstream application, parameters of the \nlogistic models are learned from scratch and usually all parameters in the pre-trained \nBERT model are adapted. The parameters for the logistic classiﬁers of the masked \nlanguage model and the next sentence prediction are not used during ﬁne-tuning.\n30 2 Pre-trained Language Models\n2.1.4 Visualizing Attentions and Embeddings \nAccording to Bengio et al. [ 14], a good representation of language should capture \nthe implicit linguistic rules and common sense knowledge contained in text data, \nsuch as lexical meanings, syntactic relations, semantic roles, and the pragmatics of \nlanguage use. The contextual word embeddings of BERT can be seen as a big step \nin this direction. They may be used to disambiguate different meanings of the same \nword. \nThe self-attention mechanism of BERT computes a large number of “associa-\ntions” between tokens and merges embeddings according to the strengths of these \nassociations. If .x1,...,x T are the embeddings of the input tokens .v1,...,v T ,t h e \nassociations .q⊺\nr kt are determined between the query .q⊺\nr\n=x ⊺\nr\nW(q) and the key \n.k⊺\nt =x ⊺\nt\nW(k) vectors ( 2.1). Then a sum of value vectors .v⊺\nt\n=x ⊺\nt\nW(v) weighted \nwith the normalized associations is formed yielding the new embeddings ( 2.3). \nThis is repeated with different matrices .W(q)\nl,m,W (k)\nl,m,W (v)\nl,m\nin m self-attention \nheads and l layers. Each layer and head the new embeddings thus captures different \naspects of the relations between the embeddings of each layer. For BERT .BASE we \nhave .l=12 layers and .m=12 bidirectional self-attention heads in each layer \nyielding 144 different “associations” or self-attentions. For the input sentence “The \ngirl and the boy went home. She entered the door.” Figure 2.6 shows on the left side \nthe strength of associations for one of the 144 self-attention heads. Between every \npair of tokens of the sentence an attention value is calculated and its strength is \nsymbolized by lines of different widths. We see that the pronoun “she” is strongly \nassociated with “the girl” . In the subsequent calculations (c.f. Fig. 2.2)t h e w o r d \n“she” is disambiguated by merging its embedding with the embeddings of “the” and \n“girl” generating a new contextual embedding of “she”, which includes its relation \nto “girl”. On the right side of the ﬁgure the input “The girl and the boy went home. \nHe entered the door.” is processed. Then the model creates an association of “boy” \nwith “he”. \nFig. 2.6 Visualization of a speciﬁc self-attention in the ﬁfth layer of a BERT model with BERTviz \n[142]. If the next sentence contains the pronoun “she” this is associated with “the girl” .I ft h i s \npronoun is changed to “he” it is related to “the boy” . Image created with BERTviz [ 142], with \nkind permission of the author\n2.1 BERT: Self-Attention and Contextual Embeddings 31\nFig. 2.7 Visualization of some of the 144 self-attention patterns computed for the sentence “[CLS] \nthe cat sat on the mat [SEP] the cat lay on the rug[SEP]” with BERTviz. Image reprinted with kind \npermission of the author [ 142] \nFigure 2.7 shows a subset of the self-attention patterns for the sentence “[CLS] \nthe cat sat on the mat [SEP] the cat lay on the rug [SEP]” . The self-attention \npatterns are automatically optimized in such a way that they jointly lead to an \noptimal prediction of the masked tokens. It can be seen that the special tokens \n[CLS] and [SEP] often are prominent targets of attentions. They usually function as \nrepresentatives of the whole sentence [ 124]. Note, however, that in a multilayer PLM \nthe embeddings generated by different heads are concatenated and transformed by \na nonlinear transformation. Therefore, the attention patterns of a single head do \nnot contain the complete information [\n124]. Whenever the matrices are randomly \ninitialized, the self-attention patterns will be completely different, if the training \nis restarted with new random parameter values. However, the overall pattern of \nattentions between tokens will be similar. \nFigure 2.10 shows on the left side a plot of six different senses of the token \nembeddings of “bank” in the Senseval-3 dataset projected to two dimensions by \nT-SNE [ 140]. The different senses are identiﬁed by different colors and form well-\nseparated clusters of their own. Senses which are difﬁcult to distinguish, like “bank \nbuilding” and “ﬁnancial institution” show a strong overlap [ 153]. The graphic\n32 2 Pre-trained Language Models\ndemonstrates that BERT embeddings have the ability to distinguish different senses \nof words which are observed frequently enough. \nThere is an ongoing discussion on the inner workings of self attention.Tay \net al [ 134] empirically evaluated the importance of the dot product .q⊺\nr ks on \nnatural language processing tasks and concluded that query-key interaction is \n“useful but not that important”. Consequently they derived alternative formulae, \nwhich in some cases worked well and failed in others. A survey of attention \napproaches is provided by de Santana Correia et al. [\n37]. There are a number \nof different attention mechanisms computing the association between embedding \nvectors [\n50, 61, 104, 151]. However, most current large-scale models still use the \noriginal scaled dot-product attention with minor variations, such as other activation \nfunctions and regularizers (c.f. Sect.\n3.1.4). \nThe fully connected layers .FCL(˘xt) in ( 2.7) contain 2/3 of the parameters of \nBERT, but their role in the network has hardly been discussed. Geva et al. [ 49] \nshow that fully connected layers operate as key-value memories, where each key \nis correlated with text patterns in the training samples, and each value induces a \ndistribution over the output vocabulary. For a key the authors retrieve the training \ninputs, which yield the highest activation of the key. Experts were able to assign \none or more interpretations to each key. Usually lower fully connected layers were \nassociated with shallow patterns often sharing the last word. The upper layers are \ncharacterized by more semantic patterns that describe similar contexts. The authors \ndemonstrate that the output of a feed-forward layer is a composition of its memories. \n2.1.5 Natural Language Understanding by BERT \nAn outstanding goal of PLMs is Natural Language Understanding (NLU). This \ncannot be evaluated against a single task, but requires a set of benchmarks covering \ndifferent areas to assess the ability of machines to understand natural language text \nand acquire linguistic, common sense, and world knowledge. Therefore, PLMs are \nﬁne-tuned to corresponding real-world downstream tasks. \nGLUE [ 146] is a prominent benchmark for NLU. It is a collection of nine NLU \ntasks with public training data, and an evaluation server using private test data. \nIts benchmarks cover a number of different aspects, which can be formulated as \nclassiﬁcation problems: \n• Determine the sentiment (positive/negative) of a sentences (SST-2). \n• Classify a sentence as grammatically acceptable or unacceptable (CoLA). \n• Check if two sentences are similar or are paraphrases (MPRC, STS-B, QQP). \n• Determine if the ﬁrst sentence entails the second one (MNLI, RTE). \n• Check if sentence B contains the answer to question A (QNLI). \n• Specify the target of a pronoun from a set of alternatives (WNLI).\n2.1 BERT: Self-Attention and Contextual Embeddings 33\nEach task can be posed as text classiﬁcation or text pair classiﬁcation problem. \nThe performance of a model is summarized in a single average value, which has \nthe value 87.1 for human annotators [ 145]. Usually, there is an online leaderboard \nwhere the performance of the different models are recorded. A very large repository \nof leaderboards is on the PapersWithCode website [ 109]. Table 2.1 describes the \ntasks by examples and reports the performance of BERT .LARGE. BERT was able to \nlift the S OTA of average accuracy from 75.2 to 82.1%. This is a remarkable increase, \nalthough the value is still far below the human performance of 87.1 with much room \nfor improvement. Recent benchmark results for NLU are described in Sect. 4.1 for \nthe more demanding SuperGLUE and other benchmarks. \nBERT’s Performance on Other Fine-Tuning Tasks \nThe pre-training data is sufﬁcient to adapt the large number of BERT parameters \nand learn very detailed peculiarities about language. The amount of training data \nfor pre-training usually is much higher than for ﬁne-tuning. Fine-tuning usually \nonly requires two or three passes through the ﬁne-tuning training data. Therefore, \nthe stochastic gradient optimizer changes most parameters only slightly and sticks \nrelatively close to the optimal pre-training parameters. Consequently, the model is \nusually capable to preserve its information about general language and to combine \nit with the information about the ﬁne-tuning task. \nBecause BERT can reuse its general knowledge about language acquired during \npre-training, it produces excellent results even with small ﬁne-tuning training data \n[\n39]. \n• CoNLL 2003 [ 128] is a benchmark dataset for Named entity recognition (NER), \nwhere each token has to be marked with a named entity tag, e.g. PER (for \np e r s o n ) , L O C ( f o r l o c a t i o n ) , ..., O ( f o r n o n a m e ) ( Sect.5.3). The task involves \ntext annotation, where a label is predicted for every input token. BERT increased \nSOTA from 92.6% to 92.8% F1-value on the test data. \n• SQuAD 1.0 [ 120] is a collection of 100k triples of questions, contexts, and \nanswers. The task is to mark the span of the answer tokens in the context. \nAn example is the question \n“When did Augustus die?” , where the answer “14 \nAD” has to be marked in the context “ ...t h e death of Augustus in AD 14 ...” \n(Sect. 6.2). Using span prediction BERT increased the S OTA of SQuAD from \n91.7% to 93.2%, while the human performance was measured as 91.2%. \nFrom these experiments a large body of evidence has been collected demonstrating \nthe strengths and weaknesses of BERT [\n124]. This is discussed in Sect. 4.2. \nIn summary, the advent of the BERT model marks a new era of NLP . It combines \ntwo pre-training tasks, i.e., predicting masked tokens and determining whether the \nsecond sentence matches the ﬁrst sentence. Transfer learning with unsupervised pre-\ntraining and supervised ﬁne-tuning becomes the new standard.\n34 2 Pre-trained Language Models\nTable 2.1 GLUE language understanding tasks. BERT .LARGE was trained for three epochs on the \nﬁne-tuning datasets [ 38]. The performance of the resulting models is printed in the last column \nyielding an average value of 82.1 \nTask Description Example Metric BERT \nCoLA Is the sentence \ngrammatical or \nungrammatical? \n“This building is than that \none.” . → Ungrammatical \nMatthews \ncorrelation \n60.5 \nSST-2 Is the movie positive, \nnegative, or neutral? \n“The movie is funny, smart, \nvisually inventive, and most \nof all, alive.” . → Positive \nAccuracy 94.9 \nMRPC Is the sentence B a \nparaphrase of \nsentence A? \nA: “Today, Taiwan reported \n35 new infections.” B: \n“Taiwan announced another \n35 probable cases at noon.” \n. → Paraphrase \nAccuracy 89.3 \nSTS-B How similar are \nsentences A and B? \nA: “Elephants are walking \ndown a trail.” B: “A herd of \nelephants is walking down a \ntrail.” . → Similar \nPearson/ \nSpearman \ncorrelation \n86.5 \nQQP Are the two questions \nsimilar? \nA: “How can I increase the \nspeed of my Internet \nconnection while using a \nVPN?” B: “How can Internet \nspeed be increased by \nhacking through DNS?” \n. → Not Similar \nAccuracy 72.1 \nMNLI-mm Does sentence A \nentail or contradict \nsentence B? \nA: “Tourist information \nofﬁces can be very helpful.” \nB: “Tourist information \nofﬁces are never of any help.” \n. → Contradiction \nAccuracy 85.9 \nQNLI Does sentence B \ncontain the answer to \nthe question in \nsentence A? \nA: “Which collection of \nminor poems are sometimes \nattributed to Virgil.” B: “A \nnumber of minor poems, \ncollected in the Appendix \nV ergiliana, are often \nattributed to him.” \n. → contains answer \nAccuracy 92.7 \nRTE Does sentence A \nentail sentence B? \nA: “Y unus launched the \nmicrocredit revolution, \nfunding 50,000 beggars, \nwhom Grameen Bank \nrespectfully calls ‘Struggling \nMembers.”’ B: “Y unus \nsupported more than 50,000 \nStruggling Members.” \n. → Entailed \nAccuracy 70.1 \nWNLI Sentence B replaces \nsentence A’s pronoun \nwith a noun - is this \nthe correct noun? \nA: “Lily spoke to Donna, \nbreaking her concentration.” \nB: “Lily spoke to Donna, \nbreaking Lily’s \nconcentration.” . → Incorrect \nAccuracy 60.5\n2.1 BERT: Self-Attention and Contextual Embeddings 35\n2.1.6 Computational Complexity \nIt is instructive to illustrate the computational effort required to train PLMs. Its \ngrowth determines the time needed to train larger models that can massively \nimprove the quality of language representation. Assume D is the size of the hidden \nembeddings and the input sequence has length T, then the intermediate dimension of \nthe fully connected layer F CL is set to 4D and the dimension of the keys and values \nare set to .D/H as in V aswani et al. [ 141]. Then according to Lin et al. [ 81] we get \nthe following computational complexities and parameters counts of self-attention \nand the position-wise F CL (2.7): \nModule Complexity # Parameters \nSelf-attention .O(T 2 ∗D) . 4D2\nPosition-wise F CL .O(T∗D 2) . 8D2\nAs long as the input sequence length T is small, the hidden dimension D \nmainly determines the complexity of self-attention and position-wise F CL.T h e \nmain limiting factor is the F CL. But when the input sequences become longer, \nthe sequence length T gradually dominates the complexity of these modules, so \nthat self-attention becomes the bottleneck of the PLM. Moreover, the computation \nof self-attention requires that an attention score matrix of size \n.T×T is stored, \nwhich prevents the computation for long input sequences. Therefore, modiﬁcations \nreducing the computational effort for long input sequences are required. \nTo connect all input embeddings with each other, we could employ different \nmodules. Fully connected layers require .T∗T networks between the different \nembeddings. Convolutional layers with a kernel width K do not connect all pairs \nand therefore need \n.O(logK(T )) layers in the case of dilated convolutions. RNNs \nhave to apply a network T times. This leads to the following complexities per layer \n[\n81, 141] \nSequential Maximum \nLayer type Complexity per layer operations path length \nSelf-attention .O(T 2 ∗D) .O(1) . O(1)\nRecurrent .O(T∗D 2) .O(T) . O(T)\nFully connected .O(T 2 ∗D 2) .O(1) . O(1)\nConvolutional .O(K∗T∗D 2) .O(1) . O(logK(T ))\nRestricted self-attention .O(R∗T∗D) .O(1) . O(T/R)\nThe last line describes a restricted self-attention, where self-attention only \nconsiders a neighborhood of size R to reduce computational effort. Obviously the \ncomputational complexity per layer is a limiting factor. In addition, computation for \nrecurrent layers need to be sequential and cannot be parallelized, as shown in the\n36 2 Pre-trained Language Models\ncolumn for sequential operations. The last column shows the path length, i.e. the \nnumber of computations to communicate information between far-away positions. \nThe shorter these paths between any combination of positions in the input and output \nsequences, the easier it is to learn long-range dependencies. Here self-attention \nhas a deﬁnite advantage compared to all other layer types. Section \n3.2 discusses \nadvanced approaches to process input sequences of larger length. In conclusion, \nBERT requires less computational effort than alternative layer types. \n2.1.7 Summary \nBERT is an autoencoder model whose main task is to derive context-sensitive \nembeddings for tokens. In a preliminary step, tokens are generated from the words \nand letters of the training data in such a way that most frequent words are tokens \nand arbitrary words can be composed of tokens. Each token is encoded by an input \nembedding. To mark the position of each input token, a position embedding is added \nto the input embedding. \nIn each layer of BERT, the lower layer embeddings are transformed by self-\nattention to a new embedding. Self-attention involves the computation of scalar \nproducts between linear transformations of embeddings. In this way, the embed-\ndings in the next layer can adapt to tokens from the context, and the embed-\ndings become context-sensitive. The operation is performed in parallel for several \nattention heads involving different linear projections. The heads can compute \nassociations in parallel with respect to different semantic features. The resulting \npartial embeddings are concatenated to a new embedding. In addition to self-\nattention heads, each encoder block contains a fully connected layer as well as \nnormalization operations. \nThe original BERT model consists of six encoder blocks and generates a ﬁnal \nembedding for each input token. BERT is pre-trained on a very large document \ncollection. The main pre-training task is to predict words from the input sequence, \nwhich have been replaced by a [MASK] token. This is done by using the last \nlayer embedding of the token as input to a logistic classiﬁer, which predicts the \nprobabilities of tokens for this position. During pre-training the model parameters \nare optimized by stochastic gradient descent. This forces the model to collect all \navailable information about that token in the output embedding. The ﬁrst input token \nis the [CLS] token. During pre-training, it is used for next sentence prediction, where \na logistic classiﬁer with the [CLS]-embedding as input has to decide, if the ﬁrst and \nsecond sentence of the input sequence belong together or not. \nTypically, the pre-trained model is ﬁne-tuned for a speciﬁc task using a small \nannotated training dataset. An example is the supervised classiﬁcation task of \nwhether the input text expresses a positive, negative or neutral sentiment. Again \na logistic classiﬁer with the [CLS]-embedding as input has to determine the \nprobability of the three sentiments. During pre-training all parameters of the model \nare adjusted slightly. It turns out that this transfer learning approach has a much\n2.2 GPT: Autoregressive Language Models 37\nhigher accuracy than supervised training only on the small training dataset, since \nthe model can use knowledge about language acquired during pre-training. \nExperiments show that BERT is able to raise the S OTA considerably in many \nlanguage understanding tasks, e.g. the GLUE benchmark. Other applications are \nnamed entity recognition, where names of persons, locations, etc. have to be \nidentiﬁed in a text, or question answering, where the answer to a question has to \nbe extracted from a paragraph. An analysis of computational complexity shows that \nBERT requires less computational effort than alternative layer types. Overall, BERT \nis the workhorse of natural language processing and is used in different variants to \nsolve language understanding problems. Its encoder blocks are reused in many other \nmodels. \nChapter 3 describes ways to improve the performance of BERT models, espe-\ncially by designing new pre-training tasks (Sect. 3.1.1). In Chap. 4 the knowledge \nacquired by BERT models is discussed. In the Chaps. 5–7, we describe a number \nof applications of BERT models such as relation extraction (Sect. 5.4) or document \nretrieval (Sect. 6.1). \n2.2 GPT: Autoregressive Language Models \n2.2.1 The T ask of Autoregressive Language Models \nTo capture the information in natural language texts the conditional probability \nof tokens can be described by a language model. These autoregressive language \nmodels aim to predict the probability of the next token in a text given the previous \ntokens. If \n.Vt+1 is a random variable whose values are the possible tokens . vt+1\nat position .t+1 , we have to calculate the conditional probability distribution \n.p(Vt+1 |v1,...,v t). According to the deﬁnition of conditional probability the \nprobability of the complete text .v1,...,v T can be computed as \n.p(V1 =v1,...,V T =vT )=p(V T =vT |v1,...,v T−1 )∗···∗p(V 1 =v1). (2.9) \nTherefore, the conditional probability can represent all information about valid \nsentences, including adequate and bad usage of language. Qudar et al. [\n115] provide \na recent survey of language models. \nIn Sect. 1.6, we used RNNs to build language models. However, these had \nproblems determining long-range interactions between tokens. As an alternative, \nwe can employ self-attention to infer contextual embeddings of the past tokens \n.v1,...,v t and predict the next token .vt+1 based on these embeddings. \nConsequently, we need to restrict self-attention to the tokens .v1,...,v t.T h i si s \nthe approach taken by the Generative Pre-trained Transformer (GPT)[ 116, 118]. \nBefore training, the text is transformed to tokens, e.g. by byte-pair encoding \n(Sect.\n1.2). On input, these tokens are represented by token embeddings and \nposition embeddings (Sect. 2.1.1). During training the GPT-model performs the self-\nattention computations described in Sect. 2.1.1 in the same way as for BERT. For\n38 2 Pre-trained Language Models\npredicting the probabilities of different tokens at position .t+1 , the self-attentions \nare restricted to previous tokens .v1,...,v t and their embeddings. The probability \nof the possible next tokens at position .t+1 is computed by a logistic classiﬁer \n.p(Vt+1 |v1,...,v t)=softmax(A ˜xk,t +b), (2.10) \nwhich takes as input the embedding .˜xk,t of the last layer k at position t to predict the \nrandom variable .Vt+1 of possible tokens at position .t+1 (Fig. 2.8). This approach is \ncalled masked self-attention or causal self-attention because the prediction depends \nonly on past tokens. Since GPT generates the tokens by sequentially applying the \nsame model, it is called an autoregressive language model. \n2.2.2 Training GPT by Predicting the Next T oken \nThe training objective is adapted to the language modeling task of GPT. Figure 2.8 \nshows the range of computations for two consecutive tokens. By teacher forcing the \nmodel uses the observed tokens .v1,...,v t up to positiont to compute self-attentions \nand predict the token probabilities for the next token .vt+1 . This is justiﬁed by the \nfactorization ( 2.9) of the full distribution. Note that the contextual embedding of \na token . vs, .s<t , changes each time when a new token .vt+1 ,vt+2 ,... is taken \ninto account in the masked self-attention. As GPT considers only the tokens before \nthe target token .vt+1 , it is called an unidirectional encoder. An intuitive high-level \noverview over GPT is given by Alammar [ 3]. \nDuring training the model parameters have to be changed by optimization such \nthat the probabilities of observed documents ( 2.9) get maximal. By this Maximum \nLikelihood estimation (MLE) the parameters can be optimized for a large corpus \nof documents. To avoid numerical problems this is solved by maximizing the log-\nlikelihood, sum of logarithms of (\n2.9) \n. logp(v 1,...,v T )=logp(v T |v1,...,v T−1 )+···+logp(v 2|v1)+logp(v 1).\n(2.11) \nAlternatively we can minimize the negative log-likelihood .−logp(v 1,...,v T ). \nGPT-2 can process an input sequence of 1024 tokens with an embedding size \nof 1024. In its medium version it has 345M parameters and contains 24 layers, \neach with 12 attention heads. For the training with gradient descent a batch size of \n512 was utilized. The model was trained on 40 GB of text crawled from Reddit, a \nsocial media platform. Only texts that were well rated by other users were included, \nresulting in a higher quality data set. The larger model was trained on 256 cloud \nTPU v3 cores. The training duration was not disclosed, nor the exact details of \ntraining. \nThe quality of a language model may be measured by the probability \n.p(v1,...,v T ) of a given text collection .v1,...,v T . If we normalize its inverse\n2.2 GPT: Autoregressive Language Models 39\nFig. 2.8 The input of the GPT model are the embeddings of tokens .v1,...,v t up to position t. \nGPT computes contextual self-embeddings of these tokens in different layers and uses the output \nembedding of the last token .vt =“to” in the highest layer to predict the probabilities of possible \ntokens at position .t+1 with a logistic classiﬁer L. This probability should be high for the actually \nobserved token “new” (left). Then the observed token .vt+1 =“new” is appended to the input \nsequence and included in the self-attention computation for predicting the probabilities of possible \ntokens at position\n.t+2 , which should be high for “york” (right) \nby the number T of tokens we get the perplexity [ 28] \n.ppl(v1,...,v T ):=p(v 1,...,v T )− 1\nT . (2.12) \nA low perplexity indicates a high probability of the text. If we assume that \nthe conditional probabilities .p(vt|v1,...,v t−1 ) are identical for all t, we get \n.ppl(v1,...,v T )=1/p(v t|v1,...,v t−1 ), i.e. the inverse probability of the next \ntoken. GPT-2 was able to substantially reduce the perplexity on a number of \nbenchmark data sets, e.g. from 46.5 to 35.8 for the Penn Treebank corpus [\n117] \nmeaning that the actual words in the texts were predicted with higher probability. \nVisualizing GPT Embeddings \nKehlbeck et al. [ 66] investigated the relative location of embeddings in multivariate \nspace for both BERT and GPT-2, each with 12 layers. They calculated 3-D \nprojections using both principal component analysis (PCA) [\n111] and UMAP [ 89]. \nThe latter can preserve the local structure of neighbors, but—differently to PCA—is \nunable to correctly maintain the global structure of the data. These 3d-scatterplots \ncan be interactively manipulated on the website [\n66]. It turns out that GPT-2 forms \ntwo separate clusters: There is a small cluster containing just all tokens at position \n0, while the embeddings at other positions form ribbon-like structures in the second \ncluster. \nCareful investigations have indicated that most embedding vectors are located \nin a narrow cone, leading to high cosine similarities between them [ 25]. The \nauthors identify isolated clusters and low dimensional manifolds in the contextual \nembedding space. Kehlbeck et al. [ 66] show that tokens with the same part-of-\nspeech tag form ribbon-like structures in the projections (Fig. 2.9 left). Function \nwords are all located on a tight circular structure, whereas content words like nouns\n40 2 Pre-trained Language Models\nFig. 2.9 Visualization of embeddings with PCA together with the corresponding part-of speech \ntags. On the left side are GPT-2 embeddings of layer 0 of tokens of positions .>0 which form \nribbon-like structures for the different POS tags, with function words close to the top. On the right \nside the embeddings of BERT for layer 0 are shown. Image reprinted with kind permission of the \nauthor [\n66] \nand verbs are located in other elongated structures and have overlap with other POS-\ntags. The embeddings generated by BERT form one or more clusters (Fig. 2.9 right). \nThey are quite separated for function words, but show some overlap for content \nwords like nouns, verbs, or adjectives. \nThe GPT-2 embeddings of content words like “banks” and “material” at \npositions .>0 form elongated band-structures, as shown in the right part of Fig. 2.10. \nFor higher layers the PCA projections get more diffuse. The user can read the token \ncontext by pointing to each dot. \nToken-based self-similarity is the mean cosine similarity of the same token found \nin different sentences. In BERT as well as GPT-2, the self-similarity is higher \nfor content than function words [\n66]. This may indicate that function words have \nmore diverse semantic roles in different contexts. It is interesting to evaluate the 10 \nnearest neighbors of a token with respect to cosine similarity. In the lower layers, \nfor both models the nearest tokens were in most cases the same tokens, except \nfor a few content words. In the higher layers this changed and different tokens \nwere the nearest tokens. This shows that more and more context is included in the \nembeddings of higher layers. \nThe authors also investigated the embeddings generated by a number of other \nPLM types. They ﬁnd that their structure is very different as they form different \nclusters and manifolds. They argue that this structure has to be taken into account \nfor new applications of the models.\n2.2 GPT: Autoregressive Language Models 41\nFig. 2.10 Plot of BERT-embeddings of different senses of “bank” projected to two dimensions \nby T-SNE (left). The legend contains a short description of the respective WordNet sense and the \nfrequency of occurrence in the training data. Image[ 153]. The right side shows PCA projections \nof the embeddings of “banks” (lower strip) and “material” ( m i d d l es t r i p )a sw e l la so t h e r w o r d s \ncomputed for different contexts. Image interactively generated, printed with kind permission of the \nauthors [\n66] \n2.2.3 Generating a Sequence of Words \nAfter training the GPT model can predict the probabilities of the tokens at the next \nposition .t+1 given the previous tokens .v1,...,v t. To generate a text we have to \nselect a sequence of tokens according to these probabilities. \n• Random sampling selects the next token according to the predicted probabilities. \nThis approach sometimes can select very improbable tokens such that the prob-\nability of the whole sentence gets too low. Although the individual probabilities \nare tiny, the probability of selecting an element of the group of improbable tokens \nis quite high. In addition, the estimates of small probability are often affected by \nerrors. \n• Top-k sampling takes into account only the k tokens with the highest probability \nto generate the next token. The probability mass is redistributed among them [ 42] \nand used for randomly selecting a token. \n• Top-p sampling considers the smallest set of top candidates with the cumulative \nprobability above a threshold (e.g. .p=0.95 ) and then selects the next \ntoken according to the redistributed probabilities [ 58]. This approach limits the \nprobability mass of rare tokens which are ignored. \nThere are also strategies which explicitly avoid previously generated tokens by \nreducing the corresponding scores in the update formula [\n67]. Both top-k and top-p \nsampling usually generate plausible token sequences and are actually employed to \ngenerate texts.\n42 2 Pre-trained Language Models\nThere are a number of approaches to improve token selection. Meister et al. [ 90] \nfound that human-produced text tends to have evenly distribution of “surprise”. This \nmeans that the next token should on average not be too rare and not be too frequent. \nThey propose a number of sampling criteria, e.g. a variance regularizer. \nMartins et al. [\n86] argue that softmax-generated output distributions are unre-\nalistic, as they assign a positive probability to every output token. They propose \nthe Entmax transformation which generates a sparse probability distribution from \nthe computed scores, where part of the probabilities are exactly zero. The Entmax \ntransformation can be controlled by a parameter \n.α≥1 .F o r .α=1 we get softmax \nand .α=∞ recovers .arg max. For intermediate values .∞>α>1.0 some \ntokens get exactly zero probability. Entmax losses are convex and differentiable \nand therefore may be trained by backpropagation. As in top-p sampling and in \nopposition to top-k sampling, Entmax sampling considers a varying number of \ntokens depending on the context. Experiments show that Entmax leads to better \nperplexities and less repetitions than other approaches. Compared with top-p \nsampling it has a higher variation in the number of tokens considered. \nKhandelwal et al. [\n68] try to improve the estimated probabilities of the language \nmodel by statistics of token n-grams. They perform a nearest neighbor search on the \nlast tokens already processed. As distance measure they use the distances of the pre-\ntrained embedding space. From the retrieved nearest neighbors they get additional \nevidence on the probable next token, which is merged with the token probabilities of \nthe language model. In this way, they are able to improve the perplexity of language \nmodels. The approach is particularly helpful in predicting rare patterns, e.g. factual \nknowledge. \nYang et al. [\n157] analyze the properties of the softmax function. They ﬁnd that \nthe standard softmax does not have enough capacity to model natural language, \nas it restricts the rank of the mapping to probabilities. They propose to predict \nprobabilities by a Mixture of Softmaxes, a convex combination of different logistic \nclassiﬁers, which is more expressive than a single softmax. The authors show that \nthis modiﬁcation yields better perplexities in language modeling and also improves \nthe performance of other transformer architectures [\n101]. \n2.2.4 The Advanced Language Model GPT -2 \nGPT-2 [ 118] is the ﬁrst language model, which is able to generate documents of \ngrammatically correct and semantically plausible text. Its largest version has 48 \nencoder blocks with 1.5B parameters and covers sequences of 1600 tokens. Given \nan initial text the model adapts to the style and content of this text and generates an \nanswer, which often cannot be distinguished from human-generated continuations. \nLonger generated texts, however, sometimes tend to be repetitive and less coherent. \nFor GPT-2 top-k truncated sampling was used to generate the example text [ 117] \nshown in Fig. 2.11. As can be seen there are no syntax errors and the generated \ncontent is plausible. The authors remark that one in two trials were of high quality.\n2.2 GPT: Autoregressive Language Models 43\nFig. 2.11 Given the input text, GPT-2 generates a continuation by top-k sampling [ 117]. Quoted \nwith kind permission of the authors \nThe model adapts to the style and content of the input text. This allows the user to \ngenerate realistic and coherent continuations about a topic they like. Obviously the \ntopic has to be mentioned in the Reddit training data, which covers a broad spectrum \nof themes such as news, music, games, sports, science, cooking, and pets. \nThe model was able to solve many tasks better than previous models without \nbeing trained on the speciﬁc task. This type of learning is called zero-shot learning. \nFor example, GPT-2 had a perplexity of 35.8 on the test set of the Penn Treebank \ncompared to the inferior prior S\nOTA of 46.5 [ 117]. This was achieved without \ntraining GPT-2 on the Penn Treebank corpus [ 135]. \n2.2.5 Fine-Tuning GPT \nBy ﬁne-tuning, GPT-2 may be adapted to new types of text, for example new genres \nof text. To create song lyrics, for example, St-Amant [\n4] uses a dataset of 12,500 \nEnglish rock song lyrics and ﬁne-tunes GPT-2 for 5 epochs. Then the model is \nable to continue the lyrics of pop songs, which had not been seen by the model \nduring training. The model had a high B\nLEU score of 68 when applied to song \nlyrics. Another experiment describes the generation of poetry [ 19]. \nSimilar to BERT, a pre-trained GPT-2 can also be modiﬁed to perform a \nclassiﬁcation task. An example is ﬁne-tuning to the classiﬁcation of the sentiment \nof a document as positive or negative. Radford et al. [ 116] encode the classiﬁcation \ntask as a text with speciﬁc tokens and a ﬁnal end token [END]. Then the model has \nto predict the sequence. The embedding of [END] in the highest layer is used as\n44 2 Pre-trained Language Models\ninput to a logistic classiﬁer, which is trained to predict the probability of classes. \nThe authors found that including language modeling ( 2.11) of the ﬁne-tuning data \nas an auxiliary objective to ﬁne-tuning improved generalization and accelerated \nconvergence. They were able to improve the score on GLUE (Sect. 2.1.5) from \n68.9 to 72.8 and achieved S OTA in 7 out of 8 GLUE tasks for natural language \nunderstanding. The results show that language models capture relevant information \nabout syntax and semantics. \nHowever, GPT operates from left to right when predicting the next token. In the \nsentences “I went to the bank to deposit cash” and “I went to the bank to sit down” , \nit will create the same context-sensitive embedding for “bank” when predicting “sit” \nor “deposit”, although the meaning of the token “bank” is different in both contexts. \nIn contrast, BERT is bidirectional and takes into account all tokens of the text when \npredicting masked tokens. This fact explains why BERT for some tasks shows a \nbetter performance. \n2.2.6 Summary \nGPT has an architecture similar to a BERT model that generates the tokens of \na sentence one by one. It starts with an input sequence of tokens, which can be \nempty. Tokens are encoded as a sum of token embeddings and position embeddings. \nGPT uses the same encoder blocks as BERT, but the computations are masked, \ni.e. restricted to the already generated tokens. For these tokens the model produces \ncontextual embeddings in several layers. The embedding of the last token in the top \nlayer is entered into a logistic classiﬁer and this calculates the probability of the \ntokens for the next position. Subsequently, the observed token is appended to the \ninput at the next position and the computations are repeated for the next but one \nposition. Therefore, GPT is called an autoregressive language model. \nDuring training the parameters are changed by stochastic gradient descent in such \na way that the model predicts high probabilities of the observed tokens in the training \ndata. The maximum likelihood criterion is used, which optimizes the probability of \nthe input data. When the model has been trained on a large text dataset it can be \napplied. Conditional to a start text it can sequentially compute the probability of the \nnext token. Then a new token can be selected according to the probabilities. \nIf all alternative tokens are taken into account, rare tokens are often selected. \nUsually, the number of eligible tokens is restricted to k high-probability tokens \n(top-k sampling) or only high-probability tokens are included up to a prescribed \nprobability sum p (top-p sampling). In this way, much better texts are generated. \nAdvanced language models like GPT-2 have billions of parameters and are able to \ngenerate plausible stories without syntactic errors. \nGPT models can also be ﬁne-tuned. A ﬁrst type of ﬁne-tuning adapts the model \nto a speciﬁc text genre, e.g. poetry. Alternatively, GPT can be used as a classiﬁer, \nwhere the output embedding of the most recently generated token for an input text is \ninput to a logistic classiﬁer. With this approach, GPT-2 was able to improve S\nOTA for\n2.3 Transformer: Sequence-to-Sequence Translation 45\nmost natural language understanding task in the GLUE benchmark. This shows that \nGPT-2 has acquired a comprehensive knowledge about language. However, since \nself-attention is only aware of past tokens, models like BERT are potentially better \nas they can take into account all input tokens during computations. \nChapter 3 discusses how to improve the performance of GPT models, in \nparticular by using more parameters (Sect. 3.1.2). These large models with billions \nof parameters can be instructed to perform a number of tasks without ﬁne-tuning \n(Sect. 3.6.3). In the Chaps. 5–7, we describe a number of applications of GPT-\nmodels such as question-answering (Sect. 6.2.3), story generation (Sect. 6.5), or \nimage generation from text (Sect. 7.2.6). \n2.3 Transformer: Sequence-to-Sequence Translation \n2.3.1 The Transformer Architecture \nTranslation models based on Recurrent Neural Networks (Sect. 1.6)h a v eam a j o r \nlimitation caused by the sequential nature of RNNs. The number of operations \nrequired to determine the relation between tokens . vs and . vt grows with the distance \n.t−s between positions. The model has to store the relations between all tokens \nsimultaneously in a vector, making it difﬁcult to learn complex dependencies \nbetween distant positions. \nThe Transformer [ 141]—similar to RNN-translation models—is based on an \nencoder and a decoder module (Fig. 2.13). The encoder is very similar to BERT, \nwhile the decoder resembles GPT. It is a sequence-to-sequence model (Seq2seq), \nwhich translates a source text of the input language to a target text in the target \nlanguage. Instead of relating distant tokens by a large number of computation steps, \nit directly computes the self-attention between these token in parallel in one step. \nThe encoder generates contextual embeddings .˜x1,..., ˜xTsrc of the source text \ntokens .v1,...,v Tsrc with exactly the same architecture as the BERT model (Fig. 2.4). \nThe original transformer [ 141] uses 6 encoder blocks. The generated embeddings of \nthe last layer are denoted as .˘x1,..., ˘xTsrc . \nThe transformer decoder step by step computes the probability distributions \n.p(St|s1,...,s t−1 ,v1,...,v Tsrc ) of target tokens . st similar to the Recurrent Neural \nNetwork. Note that the source tokens . vi as well as observed target tokens . sj are \ntaken as conditions. By the deﬁnition of conditional probability this yields the total \nprobability of the output distribution \n. p(S1 =s1,...,S T =sT |v1,...,v Tsrc )(2.13)\n=p(S T =sT |s1,...,s T−1 ,v1,...,v Tsrc )···p(S 1 =s1|v1,...,v Tsrc ),\nwhere . St is a random variable with the possible target tokens . st at position t as its \nvalues. This probability is maximized during training.\n46 2 Pre-trained Language Models\nFig. 2.12 The transformer [ 141] uses k encoder blocks with the same architecture as in BERT \n(Fig. 2.4) to generate contextual embeddings of all tokens of the input text. The decoder block \nis an autoregressive language model (Fig. 2.8) and sequentially predicts the next token in the \ntarget language. Each encoder block contains a multi-head self-attention for the current sequence \nof output tokens. By cross-attention the information from the input sequence is included. The \ncalculations are repeated for all current input tokens and are very similar to the self-attention \ncomputations. The resulting vector is transformed by a fully connected layer yielding the \nembeddings of that layer \nWe denote the already translated tokens by .s0,s1,...,s t−1 were . s0 is the token \n“[BOS]” indicating the beginning of the output text. The decoder ﬁrst computes \na self-attention for these tokens using the formula ( 2.4) as for BERT. As only \npart of the target tokens are covered and the rest is ‘masked’, this layer is called \nmasked multi-head self-attention yielding intermediate contextual embeddings \n.˜s0, ˜s1,..., ˜st−1 for the target tokens .s0,s1,...,s t−1 . \nCross-Attention \nThen the decoder performs a cross-attention .CAT L( ˜V, ˘X) with the input text \nembeddings of the highest encoder block (Fig. 2.12). Here the query-vectors are \ncomputed for the embeddings of the target tokens .˜St =( ˜s0, ˜s1,..., ˜st−1 ) provided \nby the respective decoder block. The key and value vectors are computed for the \nembeddings \n. ˘X= ˘x1,..., ˘xTsrc of the last encoder block. Note that cross attention \nemploys the same Eq. ( 2.4) with matrices .W(q),W (k),W (v) as the BERT self-\nattentions. This is done in parallel and called multi-head cross-attention.I nt h i s\n2.3 Transformer: Sequence-to-Sequence Translation 47\nFig. 2.13 The transformer [ 141] uses an encoder with the same architecture as BERT to generate \nembeddings of all tokens of the input sentence. Each encoder block performs multi-head self-\nattention of the input sequence followed by a fully connected layer (FCL) . The decoder is similar \nto a GPT model and sequentially predicts the next token in the target language. Each encoder block \ncontains a multi-head cross-attention including the ﬁnal embeddings of the encoder. Using the last \noutput embedding of the ﬁnal decoder block, a logistic classiﬁer L predicts probabilities of the \nnext token of the output sentence \nway, information from the source text is taken into account. Subsequently, the \nembeddings computed by different heads are concatenated ( 2.6) and the result is \ntransformed by a fully connected layer with ReLU activation ( 2.7). In addition, \nresidual “bypass” connections are used as well as layer normalization [ 6]f o r \nregularization. The output of the fully connected layer yields a new ‘output’ \nembedding\n.˜s0,..., ˜st−1 for the target tokens .s1,...,s t−1 . Together these layers are \ncalled a decoder block (Fig. 2.13). \nThe next decoder block gets the computed token output embeddings of the \nprevious block as input and computes a new embedding of the target tokens \n.s1,...,s t−1 . The decoder consists of several decoder blocks (6 in the original \nmodel). Using the output embedding .˘st−1 of the righmost token .st−1 in the last \ndecoder block, the token probabilities .p(St =s t|s1,...,s t−1 ,v1,...,v Tsrc ) of the \nnext token . st of the target text at position t are predicted by a logistic classiﬁer, e.g. \nfor the token “Maus” in Fig. 2.13. \nNote that for the prediction of a further token at position .t+1 the observed \ntoken . st is added to the computation ( 2.13) of the self-attentions in the decoder. \nHence, the decoder embeddings change and all decoder computations have to be \nrepeated. In this respect the model still works in a recursive way. Nevertheless, all\n48 2 Pre-trained Language Models\nself-attentions and cross-attentions in each layer are computed in parallel. However, \nthe computations for the encoder are only performed once. \nSequences of variable length are padded with a special token up to the maximal \nlength. This is done for the input and the output sequence. If a sequence is very \nshort, a lot of space is wasted. Therefore, the sequence length may be varied in \ndifferent mini-batches called buckets in the training data. \nThe transformer has a large set of parameters. First it requires embeddings of the \ninput and target token vocabularies. Then there are the .W(q),W (k),W (v) matrices \nfor the multi-head self-attention, the masked multi-head self-attention and the multi-\nhead cross-attention of the different heads and layers. In addition, the parameters of \nthe fully connected networks and the ﬁnal logistic classiﬁer have to be speciﬁed. \nWhile the base model had an input sequence length of 512 and 65M parameters, the \nbig model had an input sequence length of 1024 and 213M parameters [\n141]. The \nvalues of all these parameters are optimized during training. \nThe training data consists of pairs of an input sentence and the corresponding \ntarget sentence. Training aims to generate the target tokens with maximal probability \nfor the given input tokens to maximize the joint conditional probability ( 2.13)o ft h e \noutput sequence by stochastic gradient descent. In our example in Fig. 2.13 for the \ngiven input text “The mouse likes cheese” the product of conditional probabilities of \nthe output tokens “Die Maus mag Käse” has to be maximized. The original model \n[141], for instance, used 36M sentences of the WMT English-French benchmark \ndata encoded as 32,000 wordpiece tokens. Both the encoder and decoder are trained \nsimultaneously by stochastic gradient descent end-to-end, requiring 3.5 days with \n8G P U s . \nCross-attention is the central part of the transformer, where the information from \nthe input sentence is related to the translated output sentence. In Fig. 2.14 aG e r m a n \ninput sentence is displayed together with its English translation. Both sentences are \ntokenized by byte-pair encoding, where the beginning of a word is indicated by “_”. \nBelow the strength of cross-attentions between the input tokens and output tokens \nis depicted for two different heads. Obviously the ﬁrst input token \n“_The” has a \nspecial role. \n2.3.2 Decoding a Translation to Generate the Words \nAfter training, the Transformer is able to predict the probabilities of output tokens \nfor an input sentence. For a practical translation, however, it is necessary to generate \nan explicit sequence of output tokens. Computing the output sequence with maximal \nprobability is computationally hard, as then all output possible sequences have to be \nconsidered. Therefore, an approximate solution is obtained using greedy decoding \nor beam search. \nGreedy decoding simply picks the most likely token with the highest probability \nat each decoding step until the end-of-sentence token is generated. The problem with \nthis approach is that once the output is chosen at any time step t, it is impossible to\n2.3 Transformer: Sequence-to-Sequence Translation 49\n_The _log _file _can _be _sent _secret ly _with _email _or _FTP _to \n_a _specified _receiver \n_Die _Protokoll datei _kann _heimlich _per _E-Mail _oder _FTP _an \n_einen _bestimmte n _Empfänger _gesendet _werden. \nFig. 2.14 An English input sentence tokenized by Byte-Pair encoding and the translated tokenized \nGerman output sentence. Below are two cross-attention graphs from different heads of the 4-th \ndecoder layer [\n126]. Dark values indicate a low cross-attention score. Image source: [ 126] \ngo back and change the selection. In practice there are often problems with greedy \ndecoding, as the available probable continuation tokens may not ﬁt to a previously \nassigned token. As the decision cannot be revised, this may lead to suboptimal \ngenerated translations. \nBeam search [ 52] keeps a ﬁxed number k of possible translations .s1,...,s t of \ngrowing length (Fig. 2.15). At each step each translation of length t is enlarged by k \ndifferent tokens at position .t+1 with the highest conditional probabilities . p(St+1 =\nst+1 |s1,...,s t,v1,...,v Tsrc ). From these .k∗k token sequences only the k sequences \nwith largest total probabilities .p(s1,...,s t+1 |v1,...,v Tsrc ) are retained. A complete \ntranslation (containing the end-of-sentence token) is added to the ﬁnal candidate list. \nThe algorithm then picks the translation with the highest probability (normalized by \nthe number of target words) from this list. For .k=1 beam search reduces to greedy \ndecoding. In practice, the translation quality obtained via beam search (size of 4) is \nsigniﬁcantly better than that obtained via greedy decoding. Larger beam sizes often \nlead to suboptimal solutions [ 31]. However, beam search is computationally very \nexpensive (25%–50% slower depending on the base architecture and the beam size) \nin comparison to greedy decoding [ 29].\n50 2 Pre-trained Language Models\nFig. 2.15 Beam search is a technique for decoding a language model and producing text. At every \nstep, the algorithm keeps track of the k most probable partial translations (bold margin). The score \nof each translation is equal to its log probability. The beam search continues until it reaches the \nend token for every branch [\n78] \n2.3.3 Evaluation of a Translation \nTraditionally, evaluation is done by comparing one or more reference translations \nto the generated translation, as described in the survey [ 127]. There are a number of \nautomatic evaluation metrics: \nBLEU compares counts of 1-grams to 4-grams of tokens. The B LEU metric \nranges from 0 to 1, where 1 means an identical output with the reference. Although \nBLEU correlates well with human judgment [ 110], it relies on precision alone and \ndoes not take into account recall—the proportion of the matched n-grams out of the \ntotal number of n-grams in the reference translation. \nROUGE [80] unlike B LEU is a recall-based measure and determines which \nfraction of the words or n-grams in the reference text appear in the generated text. It \ndetermines, among other things, the overlap of unigrams or bigrams as well as the \nlongest common subsequence between a pair of texts. Different versions are used: \nR\nOUGE -1 measures the overlap of unigram (single words) between the pair of texts. \nROUGE -2 determines the overlap of bigrams (two-words sequences) between the \npair of texts. R OUGE -L: measures the length of the longest sequence of words (not \nnecessarily consecutive, but still in order) that is shared between both texts. This \nlength is divided by the number of words in the reference text. \nMETEOR [75] was proposed to address the deﬁcits of B LEU . It performs a word-\nto-word alignment between the translation output and a given reference translation. \nThe alignments are produced via a sequence of word-mapping modules. These\n2.3 Transformer: Sequence-to-Sequence Translation 51\ncheck, if the words are exactly the same, same after they are stemmed using the \nPorter stemmer, and if they are synonyms of each other. After obtaining the ﬁnal \nalignment, M ETEOR computes an F-value, which is a parameterized harmonic mean \nof unigram precision and recall. M ETEOR has also demonstrated to have a high level \nof correlation with human judgment, often even better than B LEU . \nBERTscore [ 164] takes into account synonyms and measures the similarity \nof embeddings between the translation and the reference. It computes the cosine \nsimilarity between all token embeddings of both texts. Then a greedy matching \napproach is used to determine assignments of tokens. The maximum assignment \nsimilarity is used as BERTscore. \nFor high-quality translations, however, there is a noticeable difference between \nhuman judgment and automatic evaluation. Therefore, most high-end comparisons \ntoday use human experts to assess the quality of translation and other text generation \nmethods. Since the transformer was proposed by V aswani et al. [\n141] in 2017, its \nvariants were able to raise the S OTA in language translation performance, e.g. for \ntranslation on WMT2014 English-French from 37.5 to 46.4 B LEU score. \nThe transformer architecture was analyzed theoretically. Y un et al. [ 160, 161] \nshowed that transformers are expressive enough to capture all continuous sequence \nto sequence functions with a compact domain. Pérez et al. [\n112] derived that the full \ntransformer is Turing complete, i.e. can simulate a full Turing machine. \n2.3.4 Pre-trained Language Models and Foundation Models \nA model language model either computes the joint probability or the conditional \nprobability of natural language texts and potentially includes all information about \nthe language. BERT is an autoencoder language models containing encoder blocks \nto generate contextual embeddings of tokens. GPT is an autoregressive language \nmodels which predicts the next token of a sequence and restricts self-attention \nto tokens which already have been generated. Transformers (or Transformer \nencoder-decoders) use a transformer encoder to convert the input text to contextual \nembeddings and generate the translated text with an autoregressive transformer \ndecoder utilizing the encoder embeddings as inputs (Fig.\n2.16). These models are the \nbackbone of modern NLP and are collectively called Pre-trained Language Models \n(PLM). \nAll these models, especially BERT and GPT, are initialized via pre-training \non a large corpus of text documents. During pre-training, parts of the input are \nhidden from the model, and the model is trained to reconstruct these parts. This \nhas proven to be extremely effective in building strong representations of language \nand in ﬁnding parameter initializations for highly expressive NLP models that can \nbe adapted to speciﬁc tasks. Finally, these models provide probability distributions \nover language that we can sample from. \nMost network types have some built-in assumptions called inductive bias. Con-\nvolutional networks have local kernel functions that are shifted over the input matrix\n52 2 Pre-trained Language Models\nthe [MASK] eats cheese \nTransformer \nEncoder Blocks \nmouse \nthe mouse \nTransformer \nDecoder Blocks \nthe mouse eats cheese \nTransformer \nEncoder Blocks \nTransformer \nDecoder Blocks \ndie maus frisst \nTransformer Encoder-Decoder \nBOS die mausBOS \nthe mouse eats \nGPT Language Model BERT Autoencoder \nL\nL\nL \nFig. 2.16 Autoencoders like BERT (left) and autoregressive LMs like GPT-2 (middle) use \ntransformer blocks to generate contextual embeddings of tokens. The transformer (right) combines \na transformer encoder and an autoregressive transformer decoder to produce a translation. All \nmodels predict the probability of tokens with a logistic classiﬁer L . Collectively these models are \ncalled Pre-trained Language Models (PLMs) \nCollobert et al \nl \nl \nl \nl l l l \nl \nll \nl \nl l \nl \nll l \nll \nl \nFig. 2.17 Timeline for the development of embeddings, pre-training and ﬁne-tuning \nand therefore have an inductive bias of translation invariance and locality. Recurrent \nnetworks apply the same network to each input position and have a temporal \ninvariance and locality. The BERT architecture makes only few assumptions about \nthe structural dependency in data. The GPT model is similar to the RNN as it \nassumes a Markovian structure of dependencies to the next token. As a consequence, \nPLMs often require more training data to learn the interactions between different \ndata points, but can later represent these interactions more accurately than other \nmodel types. \nHistorically, learned embedding vectors were used as representations of words \nfor downstream tasks (Fig. 2.17). As early as 2003 Bengio et al. [ 15] proposed a \ndistributed vector representation of words to predict the next word by a recurrent \nmodel. In 2011 Collobert et al. [ 32] successfully employed word embeddings \nfor part-of-speech tagging, chunking, named entity recognition, and semantic role \nlabeling. In 2013 Mikolov et al. [ 93] derived their word embeddings using a logistic \nclassiﬁer. In 2015 Dai et al. [ 33] trained embeddings with an RNN language model \nin a self-supervised way and later applied it to text classiﬁcation. In 2017 McCann \net al. [ 87] pre-trained multilayer LSTMs for translation computing contextualized \nword vectors, which are later used for various classiﬁcation tasks.\n2.3 Transformer: Sequence-to-Sequence Translation 53\nIn the same year V aswani et al. [ 141] developed the attention-only transformer \nfor language translation. In 2018 Howard et al. [ 59] pre-trained a language model \n(ULMFiT), and demonstrated the effectiveness of ﬁne-tuning to different target \ntasks by updating the full (pre-trained) model for each task. In the same year Howard \net al. [ 116] used a pre-trained autoregressive part of the transformer [ 141]t os o l v e \na large number of text understanding problems by ﬁne-tuned models. At the same \ntime Devlin et al. [ 39] pre-trained the autoencoder using the masked language model \nobjective and adapted this BERT model to many downstream tasks by ﬁne-tuning. \nIn 2019 Radford et al. [\n118] presented the GPT-2 language model, which was able \nto generate semantically convincing texts. Brown et al. [ 21] proposed the GPT-3 \nmodel, which could be instructed to solve NLP-tasks by a task description and \nsome examples. In 2021 Ramesh et al. [\n121] applied language modeling to text \nand pictures and were able to create impressive pictures from textual descriptions. \nBorgeaud et al. [ 18] presented the Retro model that answers questions by retrieving \ninformation from a text collection of 2 trillion tokens and composes an answer in \nnatural language. \nAlmost all state-of-the-art NLP models are now adapted from one of a few Pre-\ntrained Language Models, such as BERT, GPT-2, T5, etc. PLMs are becoming larger \nand more powerful, leading to new breakthroughs and attracting more and more \nresearch attention. Due to the huge increase in performance, some research groups \nhave suggested that large-scale PLMs should be called Foundation Models,a st h e y \nconstitute a ‘foundational’ breakthrough technology that can potentially impact \nmany types of applications [\n17, p. 3]. In this book, we reserve the term ‘Foundation \nModels’ for large Pre-trained Language Models with more than a billion parameters, \nsince these models are able of generating ﬂuent text, can potentially handle different \nmedia, and can usually be instructed by prompts to perform speciﬁc tasks. \nIf one of these models is improved, this high degree of homogeneity can lead to \nimmediate beneﬁts for many NLP applications. On the other hand all systems could \nshare the same problematic biases present in a few basic models. As we will see \nin later chapters PLM-based sequence modeling approaches are now applied to text \n(Sect.\n2.2), speech (Sect. 7.1), images (Sect. 7.2), videos (Sect. 7.3), computer code \n(Sect. 6.5.6), and control (Sect. 7.4). These overarching capabilities of Foundation \nModels are depicted in Fig. 2.18. \nThe next Sect. 2.4 discusses some common techniques for optimizing and \nregularizing pre-trained language models. In addition, some approaches to modify \nthe architecture of these networks are presented. In Chap.\n3 we present a number \nof approaches to improve the capabilities of PLMs, especially by modifying the \ntraining tasks (Sect. 3.1.3). In the Chaps. 5–7 we discuss a number of applications \nof PLMs. Chapter 5 covers traditional NLP tasks like named entity recognition and \nrelation extraction, where PLMs currently perform best. Most important applica-\ntions of Foundation Models are on the one hand text generation and related tasks \nlike question-answering and dialog systems, which are introduced in Chap.\n6.O n \nthe other hand Foundation Models can simultaneously process different media and \nperform tasks like image captioning, object detection in images, image generation \nfollowing a text description, video interpretation, or computer game control, which\n54 2 Pre-trained Language Models\nData \nText \nImages \nSpeech Training Foundation \nModel \nQuestion \nAnswering \nSentiment \nInformation \nExtraction \nImage \nCaptioning \nObject \nRecognition \nInstruction \nFollowing \nTasks \nadaption \nVideo \nControl \nFig. 2.18 A Foundation Model can integrate the information in the data from different modalities. \nSubsequently it can be adapted, e.g. by ﬁne-tuning, to a wide range of downstream tasks [ 17,p .6 ] . \nCredits for image parts in Table A.1 \nare discussed in Chap. 7. Because of the potential social and societal consequences \nof such Foundation Models, it is particularly important that researchers in this ﬁeld \nkeep society’s values and human rights in mind when developing and applying these \nmodels. These aspects are summarized in Sect. 8.2. \nAvailable Implementations \n• The source code for many pre-trained language models (BERT, GPT, Transform-\ners) as well as pre-trained models for different languages and text corpora can \nbe downloaded from Hugging Face \nhttps://huggingface.co/transformers/, Fairseq \nhttps://github.com/pytorch/fairseq, TensorFlow https://www.tensorﬂow.org/ and \nPyTorch https://pytorch.org/. These toolkits also allow the ﬂexible formulation \nof Deep Neural Networks and provide the automatic computation of gradients as \nwell as optimization methods. All are able to execute computations in parallel \nand distribute them to different CPUs and Graphical Processing Units (GPUs). \n• PLMs are getting larger than the memory of a single GPU and require to \ndistribute training code among several GPUs. This is supported by libraries \nlike FastSeq https://github.com/microsoft/fastseq, LightSeq https://github.com/ \nbytedance/lightseq, and FastT5 https://github.com/Ki6an/fastT5. \n• DeepSpeed [ 122] was used to train the MT-NLG autoregressive LM with 530B \nparameters (Sect. 3.1.2) https://github.com/microsoft/DeepSpeed. \n• Ecco [2] https://github.com/jalammar/ecco and BertViz [ 144] https://github.com/ \njessevig/bertviz are tools to visualize the attentions and embeddings of PLMs. \n• Transformers-interpret https://github.com/cdpierse/transformers-interpret is a \nmodel explainability tool designed for the Hugging Face package. \n• Captum [ 70] is a library https://captum.ai/ to generate interpretations and expla-\nnations for the predictions of PyTorch models.\n2.3 Transformer: Sequence-to-Sequence Translation 55\n2.3.5 Summary \nA transformer is a sequence-to-sequence model, which translates a source text of \nthe input language into a target text in the target language. It consists of an encoder \nwith the same architecture as an autoencoder BERT model that computes contextual \nembeddings of tokens of the source text. The decoder resembles an autoregressive \nGPT model and sequentially generates the tokens of the target text. Internally, \ncontextual embeddings of the target tokens are computed in the different layers. \nEach decoder block has an additional cross-attention module in which the query \nvectors are taken from the embeddings of the target tokens and the key and value \nvectors are computed for the embeddings of the source tokens of the last layer. \nIn this way, the information from the source text is communicated to the decoder. \nThe embedding of the last token in the top layer is entered into a logistic classiﬁer \nand this calculates the probability of the tokens for the next position. Subsequently, \nthe observed token at the next position is appended to the target input and the \ncomputations are repeated for the next but one position. \nDuring training the parameters of the transformer are adapted by stochastic \ngradient descent in such a way that the model assigns high probabilities to the \nobserved target tokens of the translation in the training data. When the model has \nbeen trained on a large text dataset it can be applied for translation. Conditional on \nan input text, it can sequentially compute the probability of the next token of the \ntranslation. \nDuring application of a trained model either the token with the maximal \nprobability is selected or several alternatives are generated by beam search and the \nﬁnal output sequence with maximal probability is chosen. The evaluation of the \ntranslations quality is difﬁcult as different translations may be correct. A number of \nmetrics, e.g. B\nLEU , have been developed, which compare the machine translation \nto one or more reference translations by comparing the number of common word \nn-grams with .n=1,...,4 . Often the results are assessed by human raters. \nThe transformer was able to generate better translation than prior models. In the \nmeantime the translation quality for a number of language pairs is on par with \nhuman translators. \nIn the previous sections, we discussed autoencoder BERT models, autoregressive \nGPT models and the encoder-decoder Transformers. Collectively these models are \ncalled pre-trained language models, as transfer learning with a pre-training step \nusing a large training set and a subsequent ﬁne-tuning step is a core approach for all \nthree variants. The self-attention and cross-attention modules are central building \nblocks used by all three models. Despite the development of many variations in \nrecent years, the original architecture developed by V aswani et al. [\n141] is still \ncommonly employed. \nIt turns out that these models can be applied not only to text, but to various \ntypes of sequences, such as images, speech, and videos. In addition, they may be \ninstructed to perform various tasks by simple prompts. Therefore, large PLMs are \nalso called Foundation Models, as they are expected to play a crucial role in the \nfuture development of text and multimedia systems.\n56 2 Pre-trained Language Models\n2.4 Training and Assessment of Pre-trained Language \nModels \nThis section describes some techniques required to train and apply PLMs. \n• We need optimization techniques which can process millions and billions of \nparameters and training examples. \n• Speciﬁc regularization methods are required to train the models and to avoid \noverﬁtting. \n• The uncertainty of model predictions has to be estimated to asses the perfor-\nmance of models. \n• The explanation of model predictions can be very helpful for the acceptance of \nmodels. \nApproaches to solving these problems are discussed in this section. PLMs are \nusually speciﬁed in one of the current Deep Learning frameworks. Most popular \nare TensorFlow provided from Google [\n137] and PyTorch from Meta [ 114]. Both \nare based on the Python programming language and include language elements to \nspecify a network, train it in parallel on dedicated hardware, and to deploy it to \ndifferent environments. A newcomer is the JAX framework [\n22], which is especially \nﬂexible for rapid experimentation. It has a compiler for linear algebra to accelerate \ncomputations for machine learning research. \n2.4.1 Optimization of PLMs \nBasics of PLM Optimization \nFor the i.i.d. training sample .Tr={(x [1],y [1]) ,...,(x [N],y [N] )} parameter \noptimization for Deep Neural Networks aims to ﬁnd a model that minimizes the \nloss function\n. L(x[i],y [i];w)\n. minw L(w)=L(x [1],y [1];w)+···+L(x [N],y [N] ;w). (2.14) \nFirst-order optimization methods, also known as gradient-based optimization, are \nbased on ﬁrst-order derivatives. A requirement is that the loss function \n.L(w) is \nsmooth, i.e. is continuous and in addition differentiable at almost all parameter \nvalues .w=(w 1,...,w k). Then the partial derivatives .∂L(w)\n∂wj of .L(w) with respect \nto any component . wj of . w can be computed at almost all points. The gradient of \n.L(w) in a speciﬁc point . w is the vector \n.\n∂L(w)\n∂w =\n( ∂L(w)\n∂w1\n,..., ∂L(w)\n∂wk\n) ⊺\n. (2.15)\n2.4 Training and Assessment of Pre-trained Language Models 57\nFig. 2.19 On all points of a grid the negative gradients are computed for this two-dimensional \nfunction .L(w) (left). The gradient descent algorithm follows the negative gradients and approaches \nthe local minima (right). The blue lines are the paths taken during minimization. Image credits in \nTable \nA.1 \nThe gradient points into the direction, where .L(w) in point . w has its steepest \nascent. Consequently, the direction of the steepest descent is in the opposite \ndirection .−∂L(w)\n∂w . The batch gradient descent algorithm therefore changes the \ncurrent parameter .w(t) in the direction of the negative gradient to get closer to the \nminimum \n.w(t+1) =w (t) −λ ∂L(w)\n∂w . (2.16) \nThe learning rate . λ determines the step-size or how much to move in each iteration \nuntil an optimal value is reached. As the gradient is usually different for each \nparameter .w(t) it has to be recomputed for every new parameter vector (Fig. 2.19). \nThe iteration process is repeated until the derivative becomes close to zero. A \nzero gradient indicates a local minimum or a saddle point [ 51, p. 79]. In practical \napplications it is sufﬁcient to repeat the optimization beginning with different .w-\nvalues and stop, if the derivative is close to zero. \nDeep Neural Networks often require many millions of training examples. The \nrepeated computation of the gradient for all these examples is extremely costly. The \nStochastic Gradient Descent (SGD) algorithm does not use the entire dataset but \nrather computes the gradient only for a small mini-batch of m training examples at \na time. In general, a mini-batch has sizes m ranging from 32 up to 1024, with even \nhigher values for recent extremely large models. Subsequently, the parameters of \nthe model are changed according to (\n2.16). \nFor each iteration a new mini-batch is selected randomly from the training data. \nAccording to the law of large numbers the gradients computed from these mini-\n58 2 Pre-trained Language Models\nbatches ﬂuctuate around the true gradient for the whole training set. Therefore, the \nmini-batch gradient on average indicates an adequate direction for changing the \nparameters. Mertikopoulos et al. [ 91] show that by iteratively reducing the learning \nrate to 0, the SGD exhibits almost sure convergence, avoids spurious critical points \nsuch as saddle points (with probability 1), and stabilizes quickly at local minima. \nThere are a number of variations of the SGD algorithm, which are described below \n[\n65]. \nAn important step of optimization is the initialization of parameters. Their initial \nvalues can determine whether the algorithm converges at all and how fast the \noptimization approaches the optimum. To break symmetry, the initial parameters \nmust be random. Furthermore, the mean and variance of the parameters in each layer \nare set such that the resulting outputs of the layer have a well-behaved distribution, \ne.g. expectation 0.0 and variance 1.0. In addition, all gradients also should have \nsuch a benign distribution to avoid exploding or vanishing gradients. All Deep \nLearning software frameworks contain suitable initialization routines. A thorough \nintroduction is given by Goodfellow et al. [\n51, p. 292]. \nVariants of Stochastic Gradient Descent \nMomentum is a method that helps SGD to increase the rate of convergence in \nthe relevant direction and reduce oscillations. Basically a moving average \n.u(t) of \nrecent gradients with a parameter .γ≈0.9 is computed and the parameter update is \nperformed with this average by \n.u(t) =γu (t−1) −λ ∂L(w)\n∂w wherew (t) =w (t−1) −u (t). (2.17) \nNote that in addition to the parameter vector .w(t) the moving average .u(t) of \nthe same length has to be stored requiring the same memory as the parameter \nvector \n. w. This can consume a large additional memory size if the number of \nparameters approaches the billions. In recent years a number of further optimizers \nwere developed [\n65]: \n• AdaGrad adapts the learning rate dynamically based on the previous gradients. \nIt uses smaller learning rates for features occurring often, and higher learning \nrates for features occurring rarely. \n• AdaDelta modiﬁes AdaGrad. Instead of accumulating all past gradients, it \nrestricts the accumulation window of the past gradients to some ﬁxed size k. \n• RMSProp is also a method in which the learning rate is adapted for each of \nthe parameters. The idea is to divide the learning rate for a weight by a running \naverage of the magnitudes of recent gradients for that weight. \n• Adam combines the advantages of both AdaGrad and RMSProp. Adam is based \non adaptive estimates of lower-order moments. It uses running averages of both \nthe gradients and the second moments of the gradients.\n2.4 Training and Assessment of Pre-trained Language Models 59\nDue to the extremely large number of parameters of PLMs second order optimiza-\ntion methods like Conjugate Gradient or Quasi-Newton are rarely employed. As the \nnumber of second order derivatives grows quadratically, only crude approximations \nmay be used. An example is Adam, as described before. \nAn important architectural addition to PLMs to improve training are residual \nconnections, which were proposed by V aswani et al. [ 141] for the Transformer. \nResidual connections have been shown to be very successful for image classiﬁcation \nnetworks such as ResNet [ 54] and allowed training networks with several hundred \nlayers. The identity shortcuts skip blocks of layers to preserve features. Zhang \net al. [ 163] analyze the representational power of networks containing residual \nconnections. \nParallel Training for Large Models \nRecently, there have been suggestions to reduce the optimization effort by employ-\ning larger mini-batches. Y ou et al. [ 159] propose the LAMB optimizer with \nlayerwise adaptive learning rates to accelerate training of PLMs using large mini-\nbatches. They prove the convergence of their approach to a stationary point in \na general nonconvex setting. Their empirical results demonstrate the superior \nperformance of LAMB. It is possible to reduce the BERT training time from 3 days \nto just 76 min with very little hyperparameter tuning and batch sizes of 32,868 \nwithout any degradation of performance. The LAMB program code is available \nonline [\n97]. In addition, the memory requirements of the optimization may be \nreduced [ 119] to enable parallelization of models resulting in a higher training \nspeed. \nLarge models such as GPT-3 have many billion parameters that no longer ﬁt \ninto the memory of a single computational device, e.g. a GPU. Therefore, the \ncomputations have to be distributed among several GPUs. There are different \nparallelization techniques [ 156]: \n• Data parallelism assigns the same model code and parameters to each GPU but \ndifferent training examples [\n72]. Gradients are computed in parallel and ﬁnally \nsummarized. \n• Pipeline parallelism partitions the model into different parts (e.g. layers) that are \nexecuted on different GPUs. If a part is computed it sends its results to the next \nGPU. This sequence is reversed in the backward pass of training. \n• Within-layer model parallelism distributes the weights of a single layer across \nmultiple GPUs. \nThe implementation of a parallelization strategy for a model is a tedious process. \nSupport is given by the DeepSpeed library [ 122] that makes distributed training \neasy, efﬁcient, and effective. Recently the GSPMD system [ 156] was developed \nwhich automates this process and is able to combine different parallelism paradigms \nin a uniﬁed way. GSPMD infers the distribution of computations to a network of \nGPUs based on limited user annotations to the model deﬁnition. It was, for instance, \napplied to distribute models with 1 trillion parameters on 2048 GPUs.\n60 2 Pre-trained Language Models\n2.4.2 Regularization of Pre-trained Language Models \nIf a model contains too many parameters it can nearly perfectly adapt to the \ntraining data by optimization, reﬂecting nearly all details of the training data. \nDuring this overﬁtting the model learns the random variations expressed in the \ntraining data and deviates from the mean underlying distribution. Consequently, \nit has usually a lower performance on test data and a larger generalization error. \nTo avoid this phenomenon, the representational capacity of the model has to be \nreduced by regularization methods, which often have the same effect as reducing \nthe number of parameters. Well known approaches for Deep Learning models are \nthe \n. L2 regularization and . L1 regularization penalizing large parameter values, or \nDropout temporarily setting randomly selected hidden variables to 0. A survey of \nregularization strategies for Deep Neural Networks is given by Moradi et al. [\n96]. \nThe training of PLMs is often non-trivial. One problem is the occurrence \nof vanishing or exploding gradients, which is connected to the problem of the \nvanishing or exploding variance of input values of different layers [\n55]. Batch \nnormalization normalizes the values of the components of hidden units to mean 0.0 \nand variance 1.0 and thus reduces the variation of input values. For a mini-batch of \ntraining cases the component values are aggregated to compute a mean and variance, \nwhich are then used to normalize the input of that component on each training \ncase [\n62]. It can be shown that batch normalization makes hidden representations \nincreasingly orthogonal across layers of a Deep Neural Network [ 35]. \nIn their paper on the Transformer, V aswani et al. [ 141] use a variant called layer \nnormalization [ 6] for regularization. The authors compute the mean and variance of \nthe different components of hidden units for each training example and use this to \nnormalize the input to mean 0.0 and variance 1.0. In addition, they apply dropout to \nthe output of self-attention. Finally, they use label smoothing [ 133] where the loss \nfunction is reformulated such that the observed tokens are not certain but alternative \ntokens may be possible with a small probability. This is a form of regularization \nwhich makes optimization easier. The RMSNorm [ 162] is a variant of the layer \nnormalization, which only normalizes the input by division with the root-mean-\nsquare error without shifting the mean. In experiments, it compares favorably with \nthe layer normalization [ 101]. \n2.4.3 Neural Architecture Search \nThe structure of the self-attention block was manually designed, and it is not \nclear, whether it is optimal in all cases. Therefore, there are some approaches to \ngenerate the architecture of PLMs in an automatic way called Neural Architecture \nSearch (NAS). A survey is provided by He et al. [\n56], who argue that currently the \ncontributions of architecture search to NLP tasks are minor. Zöller [ 166] evaluate \narchitecture search for machine learning models.\n2.4 Training and Assessment of Pre-trained Language Models 61\nWang et al. [ 149] propose an architecture search space with ﬂexible encoder-\ndecoder attentions and heterogeneous layers. The architecture search produces \nseveral transformer versions and ﬁnally concentrates on hardware restrictions to \nadapt the computations to processors at hand. The authors report a speedup of 3 and \na size reduction factor of 3.7 with no performance loss. For relation classiﬁcation \nZhu et al. [\n165] design a comprehensive search space. They explore the search \nspace by reinforcement learning strategy and yield models which have a better \nperformance. \nArchitecture search may also be formulated as a ranking task. RankNAS [ 60] \nsolves this by a series of binary classiﬁcation problems. The authors investigate \ntranslation and language models. For translation the usual encoder-decoder is \nincluded in a super-net, where each of the\n.1023 subnetworks is a unique architecture. \nThe importance of an architectural feature (e.g., the number of layers) is measured \nby the increase in the model error after permuting the feature. The authors use \nan evolutionary optimization strategy and evaluate their approach on translation \n(WMT2014 En-De). They get increases in B\nLEU -values at a fraction of cost of other \napproaches. \nRecently differentiable architecture search has been developed, which embeds \narchitecture search in a continuous search space and ﬁnds the optimal architecture \nby gradient descent. This leads to an efﬁcient search process that is orders of \nmagnitude faster than the discrete counterparts. This idea is applied by Fan et \nal. [\n43], who propose a gradient-based NAS algorithm for machine translation. \nThey explore attention modules and recurrent units, automatically discovering \narchitectures with better performances. The topology of the connection among \ndifferent units is learned in an end-to-end manner. On a number of benchmarks \nthey were able to improve the performance of the Transformer, e.g. from 28.8 \nto 30.1 B\nLEU scores for the WMT2014 English-to-German translation. There are \nother successful architecture search approaches for neural translation [ 130], named \nentity recognition [ 64], and image classiﬁcation models [ 34, 147, 148], which may \npossibly be applied to other NLP tasks. \n2.4.4 The Uncertainty of Model Predictions \nV ariations in the outcome of a PLM can have two main sources: \n• Epistemic uncertainty reﬂects our limited knowledge about the real world. The \nreal world situation corresponding to the training set can change causing a \ndistribution shift. Moreover, the collected documents can have biases or errors \nand cover unwanted types of content. It is clear that the structure of the real \nworld and the PLM differ. Therefore, a PLM can only approximate the correct \nconditional probabilities of language. This type of uncertainty is often called \nstructural uncertainty and is difﬁcult to estimate.\n62 2 Pre-trained Language Models\n• Aleatoric uncertainty is caused by random variations which can be assessed \nmore easily. The training data is usually a sample of the underlying data in \nthe population and therefore affected by the sampling variation. If a model \nis randomly re-initialized, it generates a completely different set of parameter \nvalues which leads to different predictions. Finally, language models predict \nprobabilities of tokens and the generation of new tokens is also affected by \nuncertainty. The Bayesian framework offers a well-founded tool to assess this \ntype of uncertainty in Deep Learning [\n44]. \nA recent survey of methods for estimating the model uncertainty is provided by \nGawlikowski et al.[ 47]. We will describe three approaches to capture model uncer-\ntainty: Bayesian statistics, a Dirichlet distributions, and ensemble distributions. \nBayesian Neural Networks \nBayesian Neural Networks directly represent the uncertainty of the estimated \nparameters .w=(w 1,...,w dw ) by the posterior distribution \n.p(w|X,Y)∝p(y|X,w)p(w). (2.18) \nHere . X and . Y are the observed inputs and outputs in the training set and . p(Y|X,w)\nis the likelihood, i.e. the probability of the outputs given . X and a parameter vector \n. w.T h e prior distribution .p(w) describes the distribution of parameters before data \nis available. The distribution of predictions for a new input . ˜x is given by \n.p(˜y| ˜x,X,Y)=\n∫\np(˜y| ˜x,w)p(w|X,Y)dw. (2.19) \nThe integral usually cannot be solved analytically and has to be approximated. Often \na Monte Carlo approximation is used, which infers the integral by a sum over \ndifferent parameter values \n.w[i] distributed according to the posterior distribution \n.p(w|X,Y) .I f .˜y[i] =f( ˜x,w [i]) is a deterministic network predicting the output for \na parameter .w[i] and input . ˜x, the resulting sample .˜y[1],..., ˜y[k] can be considered \nas a sample of the output distribution .p(˜y| ˜x,X,Y) [108]. \nBayesian predictive distributions can be approximated in different ways: \n• Sampling approaches use a Markov Chain Monte Carlo algorithm to generate \nparameter values distributed according to the posterior distributions, from which \nrealizations can be sampled [\n102]. Markov Chain Monte Carlo deﬁnes a sampling \nstrategy, where ﬁrst a new parameter value . w is randomly generated and then the \nalgorithm computes the probability to accept . w, or to keep the previous parameter \nvalue. Welling et al. [ 150] combined this approach with stochastic gradient \ndescent and demonstrated that Bayesian inference on Deep Neural Networks can \nbe done by a noisy SGD. A review of the favorable convergence properties has\n2.4 Training and Assessment of Pre-trained Language Models 63\nbeen given by Nemeth et al. [ 103]. Practical evaluations of this technique are \nperformed by Wenzel et al. [ 152]. \n• V ariational inference approximates the posterior distribution by a product . q(w)\nof simpler distributions, which are easier to evaluate [ 9]. Using multiple GPUs \nand practical tricks, such as data augmentation, momentum initialization and \nlearning rate scheduling, and learning rate scheduling, Osawa et al. [ 105] \ndemonstrated that variational inference can be scaled up to ImageNet size data-\nsets and architectures. \nIt can be shown [ 45] that dropout regularization (Sect. 2.4.2) can be considered \nas approximate variational inference. Hence, the predictive uncertainty can be \nestimated by employing dropout not only during training, but also at test time. A \nvariant called Drop connect randomly removes incoming activations of a node, \ninstead of dropping an activation for all following nodes. This approach yields a \nmore reliable uncertainty estimate and can even be combined with the original \ndropout technique [\n88]. \n• Laplace approximation considers the logarithm of the posterior distribution \naround a local mode \n. ˆw and approximate it by a normal distribution . N( ˆw,[H+\nβI] −1) over the network weights [ 9]. H is the Hessian, the matrix of second \nderivatives, of .logp(w|X,Y) . This approximation may be computed for already \ntrained networks and can be applied to Deep Neural Networks [ 76]. A problem is \nthe large number of coefﬁcients of H, which limits the computations to elements \non the diagonal. Extensions have been proposed by George et al. [ 48]. \nEstimating Uncertainty by a Single Deterministic Model \nMost PLMs predict tokens by a discrete probability distribution. If the softmax \nfunction is used to compute these probabilities, the optimization over the training \nset usually leads to very extreme probabilities close to 0 or 1. The network is often \noverconﬁdent and generates inaccurate uncertainty estimates. To assess uncertainty, \nthe difference between the estimated distribution and the actual distribution has \nto be described. If \n.v1,...,v dv is the vocabulary of tokens and . π a discrete \ndistribution over these tokens, then we can use the Dirichlet distribution . p(π|α(x))\nto characterize a distribution over these discrete distributions. The vector . α depends \non the input . x and has a component . αi for each . vi.T h e s u m.\n∑\ni αi characterizes the \nvariance. If it gets larger, the estimate for the probability of . vi has a lower variance. \nMalinin et al. [ 85] use the expected divergence between the empirical distribution \nand the predicted distribution to estimate the .p(π|α(x)) for a given input . x.I nt h e \nregion of the training data the network is trained to minimize the expected Kullback-\nLeibler (KL) divergence between the predictions of in-distribution data and a low-\nvariance Dirichlet distribution. In the region of out-of-distribution data a Dirichlet \ndistribution with a higher variance is estimated. The distribution over the outputs \ncan be interpreted as a quantiﬁcation of the model uncertainty, trying to emulate the \nbehavior of a Bayesian modeling of the network parameters [\n44].\n64 2 Pre-trained Language Models\nLiu et al. [ 83] argue that the distance between training data elements is relevant \nfor prediction uncertainty. To avoid that the layers of a network cause a high \ndistortion of the distances of the input space, the authors propose a spectral nor-\nmalization. This SNGP approach limits the distance\n.∥h(x [1])−h(x [2])∥ compared \nto .∥x [1] −x [2]∥, where .x[1] and .x[2] are two inputs and .h(x) is a deep feature \nextractor. Then they pass .h(x) into a distance-aware Gaussian Process output layer. \nThe Gaussian Process posterior is approximated by a Laplace approximation, which \ncan be predicted by a deterministic Deep Neural Network. \nThe authors evaluate SNGP on BERT .BASE to decide, if a natural utterance input \nis covered by the training data (so that it can be handled by the model) or outside. \nThe model is only trained on in-domain data, and their predictive accuracy is \nevaluated on in-domain and out-of-domain data. While ensemble techniques have \na slightly higher prediction accuracy, SNGP has a better calibration of probabilities \nand out-of-distribution detection. An implementation of the approach is available \n[\n138]. \nA number of alternative approaches are described in [ 47, p. 10f], which also \ndiscuss mixtures of Dirichlet distributions to characterize predictive uncertainty. In \ngeneral single deterministic methods are computational less demanding in training \nand evaluation compared to other approaches. However, they rely on a single \nnetwork conﬁguration and may be very sensitive to the underlying network structure \nand the training data. \nRepresenting the Predictive Distribution by Ensembles \nIt is possible to emulate the sampling variability of a training set by resampling \nmethods. A well-founded approach is bagging, where \n. nb s a m p l e so fs i z e n are \ndrawn with replacement from a training set of n elements [ 20, 107]. For the i-th \nsample a model may be trained yielding a parameter . ˆw[i]. Then the distribution \nof predictions .f(x, ˆw[i]) represent the uncertainty in the model prediction for an \ninput . x, and it can be shown that their mean value . 1\nnb\n∑\ni f(x, ˆw[i]) has a lower \nvariance than the original model prediction [ 73]. In contrast to many approximate \nmethods, ensemble approaches may take into account different local maxima of the \nlikelihood function and may cover different network architectures. There are other \nmethods to introduce data variation, e.g. random parameter initialization or random \ndata augmentation. A survey on ensemble methods is provided by Dong et al. [\n40]. \nBesides the improvement in the accuracy, ensembles are widely used for \nrepresenting prediction uncertainty of Deep Neural Networks [ 73]. In empirical \ninvestigations, the approach was at least as reliable as Bayesian approaches (Monte \nCarlo Dropout, Probabilistic Backpropagation) [\n73]. Reordering the training data \nand a random parameter initialization induces enough variability in the models \nfor the prediction of uncertainty, while bagging may reduce the reliability of \nuncertainty estimation [\n77]. Compared to Monte Carlo Dropout, ensembles yield \nmore reliable and better calibrated prediction uncertainties and are applicable to \nreal-world training data [\n13, 53]. Already for a relatively small ensemble size of\n2.4 Training and Assessment of Pre-trained Language Models 65\nﬁve, deep ensembles seem to perform best and are more robust to data set shifts \nthan the compared methods [ 106]. \nAlthough PLMs have been adapted as a standard solution for most NLP tasks, \nthe majority of existing models is unable to estimate the uncertainty associated \nwith their predictions. This seems to be mainly caused by the high computational \neffort of uncertainty estimation approaches. In addition, the concept of uncertainty \nof a predicted probability distribution is difﬁcult to communicate. However, it is \nextremely important to get a diagnosis, when a PLM is given an input outside the \nsupport of its training data, as then the predictions get unreliable. \nAmong the discussed approaches the ensemble methods seem to be most reliable. \nHowever, they require a very high computational effort. New algorithms like SNGP \nare very promising. More research is needed to reduce this effort or develop \nalternative approaches. Recently benchmark repositories and datasets have been \ndeveloped to provide high-quality implementations of standard and S\nOTA methods \nand describe best practices for uncertainty and robustness benchmarking [ 99]. \nImplementations \nUncertainty Baselines [\n10, 98] provide a collection high-quality implementations of \nstandard and state-of-the-art methods for uncertainty assessment. \n2.4.5 Explaining Model Predictions \nPLMs such as BERT are considered as black box models, as it is hard to understand, \nwhat they really learn and what determines their outputs. Hence, a lot of research \ngoes into investigating the behavior of these models. There are three main reasons \nto explain the model predictions. Trust in the model predictions is needed, i.e. that \nthe model generates reliable answers for the problem at hand and can be deployed \nin real-world applications. Causality asserts that the change of input attributes leads \nto sensible changes in the model predictions. Understanding of the model enables \ndomain experts to compare the model prediction to the existing domain knowledge. \nThis is a prerequisite for the ability to adjust the prediction model by incorporating \ndomain knowledge. \nExplanations can also be used to debug a model. A striking example was an \nimage classiﬁcation, where a horse was not detected by its shape, but by a label in \nthe image [\n74]. Explanations are most important for critical decisions that involve \nhumans or can cause high damage. Examples are health care, the judicial system, \nbanking, or self-driving cars. \nExplanation methods roughly can be grouped into local explanations or global \nexplanations. A local explanation provides information or justiﬁcation for the \nmodel’s prediction for a speciﬁc input \n. x, whereas global explanations cover the \nmodel in general. A large majority of models aims at local explanations, as these \nmay be used to justify speciﬁc predictions. Surveys on methods for the explanation \nof PLMs are provided by Danilevsky et al. [ 36], Burkart and Huber [ 23], Xu et al.\n66 2 Pre-trained Language Models\n[155], Bauckhage et al. [ 11], Tjoa and Guan [ 139], and Belle and Papantonis [ 12]. \nMolnar [ 95] devotes a whole book to this topic and Bommasani et al. [ 17, p. 125] \nprovide a recent overview. For language models different types of explanation can \nbe used: \n• Feature importance measures the inﬂuence of single input features, e.g. tokens, \non the prediction. It often corresponds to the ﬁrst derivative of a feature with \nrespect to the output [\n79]. As the meaning of input tokens is easily understood, \nthis type of explanation is readily interpretable by humans. \n• Counterfactual explanations investigate, how an input . x has to be modiﬁed, to \ngenerate a different target output. \n• Surrogate models explain model predictions by a second, simpler model. One \nwell-known example is LIME [\n123], which trains a local linear model around a \nsingle input . x of interest. \n• Example-driven explanations illustrate the prediction of an input . x by selecting \nother labeled instances that are semantically similar to . x. This is close to the \nnearest neighbor approach to prediction and has, for instance, been used for text \nclassiﬁcation [\n1]. \n• Source citation is a general practice of scientiﬁc work in which a claim is \nsupported by citing respectable scientiﬁc sources. The same can be done for a \ntext generated by language models with a retrieval component [ 57]. \nOther approaches like a sequence of reasoning steps or rule invocations are unusable \nfor PLMs with many millions of parameters. \nThe self-attention mechanism is the central function unit of PLMs.BertViz [ 144] \nis a visualization tool that allows users to explore the strength of attention between \ndifferent tokens for the heads and layers in a PLM and allows users to get a quick \noverview of relevant attention heads. However, Jain et al. [\n63] demonstrate that \nattention does not correlate with feature importance methods and counterfactual \nchanges of attention do not lead to corresponding changes in prediction. This may, \nfor instance, be caused by the concatenation of head outputs and their subsequent \nprocessing by a fully connected nonlinear layer. Attentions are noisy predictors of \nthe overall importance of components, but are not good at identifying the importance \nof features [\n129]. \nLinear Local Approximations \nAn important concept is the contribution of input . xi towards an output . yj ,e . g .a \nclass probability. Gradient-based explanations estimate the contribution of input . xi\ntowards an output . yj , e.g. a class probability, by computing the partial derivative \n.∂yj /∂xi. This derivative is often called saliency and can be interpreted as linear \napproximation to the prediction function at input . x. LIME [ 123] deﬁnes a local \nlinear regression model around a single input . x. Because of correlation of features, \nthe coefﬁcients of the input features depend on the presence or absence of the other \ninput features. The SHAP approach therefore determines the inﬂuence of a feature\n2.4 Training and Assessment of Pre-trained Language Models 67\nFig. 2.20 Contributions for the question classiﬁcation task (left). Red marks positive inﬂuence, \nblue negative, and black tokens are neutral. Contributions for the task of translating “good morning \nladies and gentlemen” to the German “Guten Morgen Damen und Herren” are shown on the right \nside [ 132]. Words are tokenized to word pieces \nby the average inﬂuence of the feature for all combinations of other features [ 84]. \nThe authors show the favorable theoretical properties of this approach and derive \nseveral efﬁcient computation strategies. \nNonlinear Local Approximations \nSundararajan et al. [ 132] formulate two basic requirements for this type of expla-\nnation. Sensitivity: if the inputs .x[1] and .x[2] differ in just one feature and lead \nto different predictions, then the differing feature should be given a non-zero \ncontribution. Implementation invariance: i.e., the attributions are always identical \nfor two functionally equivalent networks. As the prediction functions are usually \nnonlinear, gradient-based methods violate both requirements and may focus on \nirrelevant attributes. \nIntegrated Gradients [ 132] generates an approximation to the prediction \nfunction .F:R n →[0,1] , which captures nonlinear dependencies. To assess the \ndifference from baseline input .x[1] to another input . x[2], the authors compute the \nmean value of gradients .∂F(x)/∂x of the output with respect to inputs along the line \nfrom .x[1] to .x[2] by an integral. It can be shown that this approach meets the above \nrequirements. The authors apply the approach to question classiﬁcation according \nto the type of the answer (Fig.\n2.20). The baseline input is the all zero embedding \nvector. Another application considers neural machine translation. Here the output \nprobability of every output token is attributed to the input tokens. As baseline all \ntokens were zeroed except the start and end markers. A similar analysis is based on \na Taylor expansion of the prediction function [\n7]. \nLiu et al. [ 82] propose a generative explanation framework which simultaneously \nlearns to make classiﬁcation decisions and generate ﬁne-grained explanations for \nthem. In order to reach a good connection between classiﬁcation and explanation \nthey introduce a classiﬁer that is trained on their explanation. For product reviews \nthey, for instance, generate the following positive explanations \n“excellent picture,\n68 2 Pre-trained Language Models\nattractive glass-backed screen, hdr10 and dolby vision” and negative reasons “very \nexpensive”. The authors introduce an explanation factor, which represents the \ndistance between the probabilities of the classiﬁer trained on the explanations vs. \nthe classiﬁer trained on the original input and the gold labels. They optimize their \nmodels with minimum risk training. \nExplanation by Retrieval \nRecently, Deep Learning models have been playing an increasingly important role in \nscience and technology. The algorithms developed by Facebook are able to predict \nuser preferences better than any psychologist [\n24, 71]. AlphaFold, developed by \nDeepMind, makes the most accurate predictions of protein structures based on their \namino acids [\n131]. And the PaLM and Retro models are capable of generating \nstories in ﬂuent English, the latter with the knowledge of the Internet as background. \nHowever, none of the programs were actually able to justify their decisions and \ncannot indicate why a particular sequence was generated or on what information a \ndecision was based on. \nIn 2008, Anderson [ 5] predicted the end of theory-based science. In his view, \ntheories are an oversimpliﬁcation of reality, and the vast amount of accumulated \ndata contains knowledge in a much more detailed form, so theories are no longer \nnecessary. This is also the problem of Explainable AI, which aims to explain the \ndecisions of Deep Learning models. It is always faced with a trade-off where \npredictive accuracy must be sacriﬁced in order to interpret the model output. \nAs large autoregressive language models are combined with retrieval com-\nponents, document retrieval can be used not only to incorporate more accurate \nknowledge into the language generation process, but also to support the generated \nanswers by authoritative citations. Metzler et al. [\n92] argues that future PLMs should \njustify created text by referring to supporting documents in the training data or \nbackground document collection. To implement this approach Nakano et al. [\n100] \ncombine GPT-3 with the search engine BING to enhance language generation for \nquestion-answering by retrieved documents. Their WebGPT [ 100] ﬁrst creates a \ntext in natural language (Sect. 6.2.3). After that, it enhances the generated sentences \nby different references to the found documents, similar to the way a scientist \nexpands his texts by references. By this procedure WebGPT is able to justify and \nexplain the created answer. This could be a way to make the generated text more \ntrustworthy. Note that the advanced dialog model LaMDA can include links to \nexternal documents supporting an answer (Sect.\n6.6.3). \nExplanation by Generating a Chain of Thought \nLarge autoregressive PLMs like GPT-3 are able to produce a very convincing \ncontinuation of a start text, and, for instance, generate the answer for a question. \nIt turned out that their ability to generate the correct answer could drastically be\n2.4 Training and Assessment of Pre-trained Language Models 69\nFig. 2.21 Explaining by a chain of thoughts. The ﬁrst box contains two examples of thought \nchains, which are used for every query. This chain-of-thought prompt was input to the PaLM \nmodel together with the input query, and the model output was generated by PaLM [\n30, p. 38] \nimproved by giving a few examples with a chain of thought (Sect. 3.6.4) for deriving \nthe correct answer. This has been demonstrated for the PaLM language model [ 30]. \nA generated thought chain can be used for other purposes. First, it can be checked \nwhether the model produces the correct answer for the “right reasons”, rather than \njust exploiting superﬁcial statistical correlations. In addition, the explanation can \npotentially be shown to an end-user of the system to increase or decrease their \nconﬁdence in a given prediction. Finally, for some queries (e.g., explaining a joke), \nthe explanation itself is the desired output [\n30]. \nFigure 2.21 contains a few-shot query and the resulting answer. For application \nonly a few example chains of thought are necessary, which can be reused. To \ngenerate the best answer for the question greedy decoding has to be used, yielding \nthe optimal prediction. As PaLM shows, the enumeration of argument steps works \nempirically. However, a sound theory of how models actually use such arguments \ninternally is still lacking. Further, it is not known under which circumstances the \nderivation of such a chain of thoughts succeeds. It should be investigated to what \nextent the reasoning of a model corresponds to the reasoning steps performed by \nhumans.\n70 2 Pre-trained Language Models\nImplementations \nEcco [ 2] and BertViz [ 143] are tools to visualize the attentions and embeddings \nof PLMs. An implementation and a tutorial on integrated gradients is available for \nTensorFlow [136]. Captum [ 26, 70] is an open-source library to generate interpre-\ntations and explanations for the predictions of PyTorch models containing most of \nthe approaches discussed above. Transformers-interpret [\n113] is an alternative open-\nsource model explainability tool for the Hugging Face package. \n2.4.6 Summary \nSimilar to other large neural networks, PLMs are optimized with simple stochastic \ngradient descent optimizers that are able to approach the region of minimal cost \neven for huge models with billions of parameters and terabytes of training data. \nThis requires parallel training on computing networks which can be controlled by \nsuitable software libraries. There are many recipes in the literature for setting hyper-\nparameters such as batch size and learning rate schedules. Important ingredients \nare residual connections to be able to optimize networks with many layers and \nregularization modules to keep parameters in a manageable range. \nNeural architecture search is a way to improve performance and reduce memory \nrequirements of networks. A number of approaches have been proposed that signiﬁ-\ncantly speed up training. Some methods provide models with better performance \nand lower memory footprint. There are new differential methods that have the \npotential to derive better architectures with little effort. \nPLMs aim to capture relations between language concepts and can only do \nso approximately. Therefore, it is important to evaluate their inherent uncertainty. \nThree different approaches to analyze the uncertainty are described. Among these, \nensemble methods appear to be the most reliable, but involve a high computational \ncost. New algorithms such as SNGP , which are based on a single model, are very \npromising. \nTo enable a user to decide whether a model result makes sense, it is necessary \nto explain how the result was obtained. Explanations can be provided by showing \nthe importance of features for a result, by exploring the PLM by related examples \nor by approximating the PLM with a simple model. Some libraries are available \nthat allow routine use of these methods. A new way of explaining texts generated \nby PLMs is to enhance the texts with appropriate citations of relevant supporting \ndocuments. Finally, a PLM can be instructed by chain-of-thought prompts to provide \nan explanation for the model response. This type of explanation is particularly easy \nto understand and can reﬂect the essential parts of a chain of arguments. \nThe next chapter discusses approaches to improve the three basic PLM types by \nnew pre-training tasks or architectural changes. The fourth chapter examines the \nknowledge, which can be acquired by PLMs and that can be used to interpret text \nand to generate new texts.\nReferences 71\nReferences \n1. A. Abujabal, R. S. Roy, M. Yahya, and G. Weikum. “Quint: Interpretable Question Answering \nover Knowledge Bases”. In: Proc. 2017 Conf. Empir . Methods Nat. Lang. Process. Syst. \nDemonstr. 2017, pp. 61–66. \n2. J. Alammar. “Ecco: An Open Source Library for the Explainability of Transformer Language \nModels”. In: Proc. 59th Annu. Meet. Assoc. Comput. Linguist. 11th Int. Jt. Conf. Nat. Lang. \nProcess. Syst. Demonstr. 2021, pp. 249–257. \nURL : https://github.com/jalammar/ecco. \n3. J. Alammar. The Illustrated GPT-2 (Visualizing Transformer Language Models). Oct. 12, \n2019. \nURL : http://jalammar.github.io/illustrated-gpt2/ (visited on 01/24/2021). \n4. F. St-Amant. How to Fine-Tune GPT-2 for Text Generation. Medium. May 8, 2021. URL : \nhttps://towardsdatascience.com/how-to-ﬁne- tune-gpt-2-for-text-generation-ae2ea53bc272 \n(visited on 07/29/2021). \n5. C. Anderson. “The End of Theory: The Data Deluge Makes the Scientiﬁc Method Obsolete”. \nIn: Wired (June 23, 2008). \nISSN : 1059–1028. URL : https://www.wired.com/2008/06/pb-\ntheory/ (visited on 01/11/2022). \n6. J. L. Ba, J. R. Kiros, and G. E. Hinton. “Layer Normalization”. 2016. arXiv: 1607.06450. \n7. S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. Müller, and W. Samek. “On Pixel-Wise \nExplanations for Non-Linear Classiﬁer Decisions by Layer-Wise Relevance Propagation”. In: \nPloS one 10.7 (2015), e0130140. \n8. D. Bahdanau, K. Cho, and Y . Bengio. “Neural Machine Translation by Jointly Learning to \nAlign and Translate”. 2014. arXiv: 1409.0473. \n9. D. Barber and C. M. Bishop. “Ensemble Learning in Bayesian Neural Networks”. In: Nato \nASI Ser . F Comput. Syst. Sci. 168 (1998), pp. 215–238. \n10. baselines. Uncertainty Baselines. Google, Dec. 5, 2021. URL : https://github.com/google/ \nuncertainty-baselines (visited on 12/06/2021). \n11. C. Bauckhage, J. Fürnkranz, and G. Paass. “V ertrauenswürdiges, Transparentes Und Robustes \nMaschinelles Lernen”. In: Handbuch Der Künstlichen Intelligenz. de Gruyter, 2021. \nISBN : \n978-3-11-065984-9. \n12. V . Belle and I. Papantonis. “Principles and Practice of Explainable Machine Learning”. \nIn: Front. Big Data 4 (2021), p. 39. \nISSN : 2624-909X. https://doi.org/10.3389/fdata.2021. \n688969. \n13. W. H. Beluch, T. Genewein, A. Nürnberger, and J. M. Köhler. “The Power of Ensembles \nfor Active Learning in Image Classiﬁcation”. In: Proc. IEEE Conf. Comput. Vis. Pattern \nRecognit. 2018, pp. 9368–9377. \n14. Y . Bengio, A. Courville, and P . Vincent. “Representation Learning: A Review and New \nPerspectives”. In: IEEE Trans. Pattern Anal. Mach. Intell. 35.8 (2013), pp. 1798–1828. \n15. Y . Bengio, R. Ducharme, P . Vincent, and C. Jauvin. “A Neural Probabilistic Language \nModel”. In: J. Mach. Learn. Res. 3 (Feb 2003), pp. 1137–1155. \n16. Y . Bengio, P . Lamblin, D. Popovici, and H. Larochelle. “Greedy Layer-Wise Training of Deep \nNetworks”. In: Adv. Neural Inf. Process. Syst. 19 (2006). \n17. R. Bommasani et al. “On the Opportunities and Risks of Foundation Models”. 2021. arXiv: \n2108.07258. \n18. S. Borgeaud et al. “Improving Language Models by Retrieving from Trillions of Tokens”. \nDec. 8, 2021. arXiv: 2112.04426 [cs]. \n19. G. Branwen. “GPT-2 Neural Network Poetry”. In: (Mar. 3, 2019). URL : https://www.gwern. \nnet/GPT-2 (visited on 01/27/2021). \n20. L. Breiman. “Bagging Predictors”. In: Mach. Learn. 24.2 (1996), pp. 123–140. \n21. T. B. Brown et al. “Language Models Are Few-Shot Learners”. 2020. arXiv: 2005.14165. \n22. D. Budden and M. Hessel. Using JAX to Accelerate Our Research. Dec. 4, 2020. URL : https:// \nwww.deepmind.com/blog/using-jax-to-accelerate-our-research (visited on 06/21/2022). \n23. N. Burkart and M. F. Huber. “A Survey on the Explainability of Supervised Machine \nLearning”. In: J. Artif. Intell. Res. 70 (2021), pp. 245–317.\n72 2 Pre-trained Language Models\n24. C. Cadwalladr and E. Graham-Harrison. “How Cambridge Analytica Turned Facebook \n‘Likes’ into a Lucrative Political Tool”. In: Guard. 17032018 (2018). \n25. X. Cai, J. Huang, Y . Bian, and K. Church. “Isotropy in the Contextual Embedding Space: \nClusters and Manifolds”. In: Int. Conf. Learn. Represent. 2020. \n26. Captum. Captum· Model Interpretability for PyTorch. 2021. URL : https://captum.ai/ (visited \non 12/06/2021). \n27. S. Chaudhari, V . Mithal, G. Polatkan, and R. Ramanath. “An Attentive Survey of Attention \nModels”. In: ACM Trans. Intell. Syst. Technol. TIST 12.5 (2021), pp. 1–32. \n28. S. F. Chen, D. Beeferman, and R. Rosenfeld. “Evaluation Metrics for Language Models”. \nIn: (1998). \nURL : https://kilthub.cmu.edu/articles/EvaluationMetricsForLanguageModels/ \n6605324/ﬁles/12095765.pdf. \n29. Y . Chen, V . O. Li, K. Cho, and S. R. Bowman. “A Stable and Effective Learning Strategy for \nTrainable Greedy Decoding”. 2018. arXiv: 1804.07915. \n30. A. Chowdhery et al. “PaLM: Scaling Language Modeling with Pathways”. Apr. 5, 2022. \narXiv: 2204.02311 [cs]. \n31. E. Cohen and C. Beck. “Empirical Analysis of Beam Search Performance Degradation in \nNeural Sequence Models”. In: Int. Conf. Mach. Learn. PMLR, 2019, pp. 1290–1299. \n32. R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P . Kuksa. “Natural \nLanguage Processing (Almost) from Scratch”. In: J. Mach. Learn. Res. 12 (2011), pp. 2493– \n2537. \n33. A. M. Dai and Q. V . Le. “Semi-Supervised Sequence Learning”. In: Adv. Neural Inf. Process. \nSyst. 2015, pp. 3079–3087. \n34. Z. Dai, H. Liu, Q. V . Le, and M. Tan. “CoAtNet: Marrying Convolution and Attention for All \nData Sizes”. Sept. 15, 2021. arXiv: 2106.04803 [cs]. \n35. H. Daneshmand, A. Joudaki, and F. Bach. “Batch Normalization Orthogonalizes Representa-\ntions in Deep Random Networks”. June 7, 2021. arXiv: 2106.03970 [cs, stat]. \n36. M. Danilevsky, K. Qian, R. Aharonov, Y . Katsis, B. Kawas, and P . Sen. “A Survey of the State \nof Explainable AI for Natural Language Processing”. 2020. arXiv: 2010.00711. \n37. A. de Santana Correia and E. L. Colombini. “Attention, Please! A Survey of Neural Attention \nModels in Deep Learning”. In: Artif. Intell. Rev. (2022), pp. 1–88. \n38. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. “Annotated BERT: Pre-training of Deep \nBidirectional Transformers for Language Understanding”. In: Proc. 2019 Conf. North Am. \nChapter Assoc. Comput. Linguist. Hum. Lang. Technol. V ol. 1 Long Short Pap. NAACL-HLT \n2019. Minneapolis, Minnesota: Association for Computational Linguistics, June 2019, pp. \n4171–4186. \nhttps://doi.org/10.18653/v1/N19-1423. \n39. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. “Bert: Pre-training of Deep Bidirectional \nTransformers for Language Understanding”. 2018. arXiv: 1810.04805. \n40. X. Dong, Z. Y u, W. Cao, Y . Shi, and Q. Ma. “A Survey on Ensemble Learning”. In: Front. \nComput. Sci. 14.2 (2020), pp. 241–258. \n41. K. Doshi. Transformers Explained Visually (Part 3): Multi-head Attention, Deep \nDive. Medium. June 3, 2021. \nURL : https://towardsdatascience.com/transformers-explained-\nvisuallypart-3-multi-head-attention-deep-dive-1c1ff1024853 (visited on 11/19/2021). \n42. A. Fan, M. Lewis, and Y . Dauphin. “Hierarchical Neural Story Generation”. 2018. arXiv: \n1805.04833. \n43. Y . Fan, F. Tian, Y . Xia, T. Qin, X.-Y . Li, and T.-Y . Liu. “Searching Better Architectures for \nNeural Machine Translation”. In: IEEEACM Trans. Audio Speech Lang. Process. 28 (2020), \npp. 1574–1585. \n44. Y . Gal and Z. Ghahramani. “Bayesian Convolutional Neural Networks with Bernoulli \nApproximate V ariational Inference”. 2015. arXiv: 1506.02158. \n45. Y . Gal, J. Hron, and A. Kendall. “Concrete Dropout”. 2017. arXiv: 1705.07832. \n46. A. Galassi, M. Lippi, and P . Torroni. “Attention in Natural Language Processing”. In: IEEE \nTransactions on Neural Networks and Learning Systems 32 (Oct. 1, 2021), pp. 4291–4308. \nhttps://doi.org/10.1109/TNNLS.2020.3019893.\nReferences 73\n47. J. Gawlikowski et al. “A Survey of Uncertainty in Deep Neural Networks”. 2021. arXiv: \n2107.03342. \n48. T. George, C. Laurent, X. Bouthillier, N. Ballas, and P . Vincent. “Fast Approximate Natural \nGradient Descent in a Kronecker-Factored Eigenbasis”. 2018. arXiv: 1806.03884. \n49. M. Geva, R. Schuster, J. Berant, and O. Levy. “Transformer Feed-Forward Layers Are Key-\nV alue Memories”. In: (Dec. 29, 2020). \nURL : https://arxiv.org/abs/2012.14913v2 (visited on \n11/08/2021). \n50. B. Ghojogh and A. Ghodsi. “Attention Mechanism, Transformers, BERT, and GPT: Tutorial \nand Survey”. In: (2020). \nURL : https://osf.io/m6gcn/download. \n51. I. Goodfellow, Y . Bengio, and A. Courville. Deep Learning. V ol. 1. MIT press Cambridge, \n2016. URL : https://www.deeplearningbook.org/. \n52. A. Graves. “Sequence Transduction with Recurrent Neural Networks”. 2012. arXiv: \n1211.3711. \n53. F. K. Gustafsson, M. Danelljan, and T. B. Schon. “Evaluating Scalable Bayesian Deep \nLearning Methods for Robust Computer Vision”. In: Proc. IEEECVF Conf. Comput. Vis. \nPattern Recognit. Workshop. 2020, pp. 318–319. \n54. K. He, X. Zhang, S. Ren, and J. Sun. “Deep Residual Learning for Image Recognition”. In: \nProc. IEEE Conf. Comput. Vis. Pattern Recognit. 2016, pp. 770–778. \n55. K. He, X. Zhang, S. Ren, and J. Sun. “Delving Deep into Rectiﬁers: Surpassing Human-Level \nPerformance on Imagenet Classiﬁcation”. In: Proc. IEEE Int. Conf. Comput. Vis. 2015, pp. \n1026–1034. \n56. X. He, K. Zhao, and X. Chu. “AutoML: A Survey of the State-of-the-Art”. In: Knowl.-Based \nSyst. 212 (2021), p. 106622. \n57. J. Hilton. WebGPT: Improving the Factual Accuracy of Language Models through \nWeb Browsing. OpenAI. Dec. 16, 2021. URL : https://openai.com/blog/improving-factual-\naccuracy/ (visited on 01/12/2022). \n58. A. Holtzman, J. Buys, L. Du, M. Forbes, and Y . Choi. “The Curious Case of Neural Text \nDegeneration”. Feb. 14, 2020. arXiv: 1904.09751 [cs]. \n59. J. Howard and S. Ruder. “Universal Language Model Fine-tuning for Text Classiﬁcation”. \nIn: Proc. 56th Annu. Meet. Assoc. Comput. Linguist. V ol. 1 Long Pap. ACL 2018. Melbourne, \nAustralia: Association for Computational Linguistics, July 2018, pp. 328–339. \nhttps://doi.org/ \n10.18653/v1/P18-1031. \n60. C. Hu et al. “RankNAS: Efﬁcient Neural Architecture Search by Pairwise Ranking”. 2021. \narXiv: 2109.07383. \n61. D. Hu. “An Introductory Survey on Attention Mechanisms in NLP Problems”. In: Proc. SAI \nIntell. Syst. Conf. Springer, 2019, pp. 432–448. \n62. S. Ioffe and C. Szegedy. “Batch Normalization: Accelerating Deep Network Training by \nReducing Internal Covariate Shift”. In: Int. Conf. Mach. Learn. PMLR, 2015, pp. 448–456. \n63. S. Jain and B. C. Wallace. “Attention Is Not Explanation”. 2019. arXiv: 1902.10186. \n64. Y . Jiang, C. Hu, T. Xiao, C. Zhang, and J. Zhu. “Improved Differentiable Architecture \nSearch for Language Modeling and Named Entity Recognition”. In: Proc. 2019 Conf. Empir . \nMethods Nat. Lang. Process. 9th Int. Jt. Conf. Nat. Lang. Process. EMNLP-IJCNLP. 2019, \npp. 3576–3581. \n65. M. Kastrati and M. Biba. “A State-of-the-Art Survey of Advanced Optimization Methods in \nMachine Learning”. In: RTA-CSIT (May 1, 2021), pp. 1–10. \n66. R. Kehlbeck, R. Sevastjanova, T. Spinner, T. Stähle, and M. El-Assady. Demystifying the \nEmbedding Space of Language Models. July 31, 2021. \nURL : https://bert-vs-gpt2.dbvis.de/. \n67. N. S. Keskar, B. McCann, L. R. V arshney, C. Xiong, and R. Socher. “CTRL: A Condi-\ntional Transformer Language Model for Controllable Generation”. Sept. 20, 2019. arXiv: \n1909.05858. \n68. U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and M. Lewis. “Generalization through \nMemorization: Nearest Neighbor Language Models”. Feb. 14, 2020. arXiv: 1911.00172. \n69. D. P . Kingma and J. Ba. “Adam: A Method for Stochastic Optimization”. 2014. arXiv: \n1412.6980.\n74 2 Pre-trained Language Models\n70. N. Kokhlikyan et al. “Captum: A Uniﬁed and Generic Model Interpretability Library for \nPyTorch”. Sept. 16, 2020. arXiv: 2009.07896. \n71. M. Kosinski, D. Stillwell, and T. Graepel. “Private Traits and Attributes Are Predictable from \nDigital Records of Human Behavior”. In: Proc. Natl. Acad. Sci. 110.15 (2013), pp. 5802– \n5805. \n72. A. Krizhevsky, I. Sutskever, and G. E. Hinton. “Imagenet Classiﬁcation with Deep Convolu-\ntional Neural Networks”. In: Adv. Neural Inf. Process. Syst. 2012, pp. 1097–1105. \n73. B. Lakshminarayanan, A. Pritzel, and C. Blundell. “Simple and Scalable Predictive Uncer-\ntainty Estimation Using Deep Ensembles”. In: Adv. Neural Inf. Process. Syst. 30 (2017). \n74. S. Lapuschkin, A. Binder, G. Montavon, K.-R. Muller, and W. Samek. “Analyzing Classiﬁers: \nFisher V ectors and Deep Neural Networks”. In: Proc. IEEE Conf. Comput. Vis. Pattern \nRecognit. 2016, pp. 2912–2920. \n75. A. Lavie and A. Agarwal. “METEOR: An Automatic Metric for MT Evaluation with High \nLevels of Correlation with Human Judgments”. In: Proc. Second Workshop Stat. Mach. \nTransl. 2007, pp. 228–231. \n76. J. Lee, M. Humt, J. Feng, and R. Triebel. “Estimating Model Uncertainty of Neural Networks \nin Sparse Information Form”. In: Int. Conf. Mach. Learn. PMLR, 2020, pp. 5702–5713. \n77. S. Lee, S. Purushwalkam, M. Cogswell, D. Crandall, and D. Batra. “Why M Heads Are Better \nthan One: Training a Diverse Ensemble of Deep Networks”. 2015. arXiv: 1511.06314. \n78. M. Lewis. Decoding Language Models · Deep Learning. Apr. 20, 2020. URL : https://atcold. \ngithub.io/pytorch-Deep-Learning/en/week12/12-2/ (visited on 07/30/2021). \n79. J. Li, X. Chen, E. Hovy, and D. Jurafsky. “Visualizing and Understanding Neural Models in \nNlp”. 2015. arXiv: 1506.01066. \n80. C.-Y . Lin. “Rouge: A Package for Automatic Evaluation of Summaries”. In: Text Summ. \nBranches Out. 2004, pp. 74–81. \n81. T. Lin, Y . Wang, X. Liu, and X. Qiu. “A Survey of Transformers”. 2021. arXiv: \n2106.04554. \n82. H. Liu, Q. Yin, and W. Y . Wang. “Towards Explainable NLP: A Generative Explanation \nFramework for Text Classiﬁcation”. June 11, 2019. arXiv: 1811.00196. \n83. J. Z. Liu, Z. Lin, S. Padhy, D. Tran, T. Bedrax-Weiss, and B. Lakshminarayanan. “Simple \nand Principled Uncertainty Estimation with Deterministic Deep Learning via Distance \nAwareness”. Oct. 25, 2020. arXiv: 2006.10108. \n84. S. M. Lundberg and S.-I. Lee. “A Uniﬁed Approach to Interpreting Model Predictions”. In: \nProc. 31st Int. Conf. Neural Inf. Process. Syst. 2017, pp. 4768–4777. \n85. A. Malinin and M. Gales. “Reverse Kl-Divergence Training of Prior Networks: Improved \nUncertainty and Adversarial Robustness”. 2019. arXiv: 1905.13472. \n86. P . H. Martins, Z. Marinho, and A. F. Martins. “Sparse Text Generation”. 2020. arXiv: \n2004.02644. \n87. B. McCann, J. Bradbury, C. Xiong, and R. Socher. “Learned in Translation: Contextualized \nWord V ectors”. In: Adv. Neural Inf. Process. Syst. 2017, pp. 6294–6305. \n88. P . McClure and N. Kriegeskorte. “Robustly Representing Uncertainty through Sampling in \nDeep Neural Networks”. 2016. arXiv: 1611.01639. \n89. L. McInnes, J. Healy, and J. Melville. “Umap: Uniform Manifold Approximation and \nProjection for Dimension Reduction”. 2018. arXiv: 1802.03426. \n90. C. Meister, T. Vieira, and R. Cotterell. “If Beam Search Is the Answer, What Was the \nQuestion?” Jan. 17, 2021. arXiv: 2010.02650 [cs]. \n91. P . Mertikopoulos, N. Hallak, A. Kavis, and V . Cevher. “On the Almost Sure Conver-\ngence of Stochastic Gradient Descent in Non-Convex Problems”. June 19, 2020. arXiv: \n2006.11144. \n92. D. Metzler, Y . Tay, D. Bahri, and M. Najork. “Rethinking Search: Making Experts out of \nDilettantes”. May 5, 2021. arXiv: 2105.02274 [cs]. \n93. T. Mikolov, K. Chen, G. Corrado, and J. Dean. “Efﬁcient Estimation of Word Representations \nin V ector Space”. 2013. arXiv: 1301.3781.\nReferences 75\n94. G. A. Miller. “WordNet: A Lexical Database for English”. In: Commun. ACM 38.11 (1995), \npp. 39–41. \n95. C. Molnar. Interpretable Machine Learning. Jan. 21, 2022. URL : https://christophm.github.io/ \ninterpretable-ml-book/ (visited on 01/26/2022). \n96. R. Moradi, R. Berangi, and B. Minaei. “A Survey of Regularization Strategies for Deep \nModels”. In: Artif. Intell. Rev. 53.6 (2020), pp. 3947–3986. \n97. S. Morgan. Tensorﬂow/Addons. tensorﬂow, Dec. 1, 2020. URL : https://github.com/tensorﬂow/ \naddons/blob/0c0fd8dfb4427df6b824c88f700ba5c7efd43bec/tensorﬂowaddons/optimizers/ \nlamb.py (visited on 11/08/2021). \n98. Z. Nado. Baselines for Uncertainty and Robustness in Deep Learning. Google AI Blog. Oct. \n14, 2021. URL : http://ai.googleblog.com/2021/10/baselines-for-uncertainty-and.html (visited \non 10/25/2021). \n99. Z. Nado et al. “Uncertainty Baselines: Benchmarks for Uncertainty & Robustness in Deep \nLearning”. June 7, 2021. arXiv: 2106.04015. \n100. R. Nakano et al. “WebGPT: Browser-assisted Question-Answering with Human Feedback”. \n2021. arXiv: 2112.09332. \n101. S. Narang et al. “Do Transformer Modiﬁcations Transfer Across Implementations and \nApplications?” Sept. 10, 2021. arXiv: 2102.11972 [cs]. \n102. R. M. Neal. Bayesian Training of Backpropagation Networks by the Hybrid Monte Carlo \nMethod. Technical Report CRG-TR-92-1, Dept. of Computer Science, University of Toronto. \nCiteseer, 1992. \n103. C. Nemeth and P . Fearnhead. “Stochastic Gradient Markov Chain Monte Carlo”. In: J. Am. \nStat. Assoc. 116.533 (2021), pp. 433–450. \n104. Z. Niu, G. Zhong, and H. Y u. “A Review on the Attention Mechanism of Deep Learning”. In: \nNeurocomputing 452 (2021), pp. 48–62. \n105. K. Osawa, S. Swaroop, A. Jain, R. Eschenhagen, R. E. Turner, R. Y okota, and M. E. Khan. \n“Practical Deep Learning with Bayesian Principles”. 2019. arXiv: 1906.02506. \n106. Y . Ovadia et al. “Can Y ou Trust Y our Model’s Uncertainty? Evaluating Predictive Uncertainty \nunder Dataset Shift”. 2019. arXiv: 1906.02530. \n107. G. Paass. “Assessing and Improving Neural Network Predictions by the Bootstrap Algo-\nrithm”. In: Adv. Neural Inf. Process. Syst. Citeseer, 1993, pp. 196–203. \n108. G. Paass and J. Kindermann. “Bayesian Classiﬁcation Trees with Overlapping Leaves \nApplied to Credit-Scoring”. In: Res. Dev. Knowl. Discov. Data Min. Ed. by X. Wu, R. \nKotagiri, and K. B. Korb. Lecture Notes in Computer Science. Berlin, Heidelberg: Springer, \n1998, pp. 234–245. \nISBN : 978-3-540-69768-8. https://doi.org/10.1007/3-540-64383-4_20. \n109. Paperswithcode. Browse State-of-the-Art in AI. 2019. URL : https://paperswithcode.com/sota. \n110. K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. “Bleu: A Method for Automatic Evaluation \nof Machine Translation”. In: Proc. 40th Annu. Meet. Assoc. Comput. Linguist. 2002, pp. 311– \n318. \n111. K. Pearson. “On Lines and Planes of Closest Fit to Systems of Points in Space”. In: Lond. \nEdinb. Dublin Philos. Mag. J. Sci. 2.11 (1901), pp. 559–572. \n112. J. Pérez, J. Marinkoviæ, and P . Barceló. “On the Turing Completeness of Modern Neural \nNetwork Architectures”. 2019. arXiv: 1901.03429. \n113. C. Pierse. Transformers Interpret. V ersion 0.5.2. Feb. 2021. URL : https://github.com/cdpierse/ \ntransformers-interpret (visited on 11/23/2021). \n114. Pytorch. PyTorch. 2019. URL : https://pytorch.org/. \n115. M. Qudar and V . Mago. A Survey on Language Models. Sept. 7, 2020. URL : https://www. \nresearchgate.net/publication/344158120ASurveyonLanguage_Models/. \n116. A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. “Improving Language Understand-\ning by Generative Pre-Training”. In: (2018). \n117. A. Radford, J. Wu, D. Amodei, D. Amodei, J. Clark, M. Brundage, and I. Sutskever. “Better \nLanguage Models and Their Implications”. In: OpenAI Blog (2019). \nURL : https://openai. \n%20com/blog/better-language-models.\n76 2 Pre-trained Language Models\n118. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. “Language Models Are \nUnsupervised Multitask Learners”. In: OpenAI blog 1.8 (2019), p. 9. \n119. S. Rajbhandari, J. Rasley, O. Ruwase, and Y . He. “ZeRO: Memory Optimizations Toward \nTraining Trillion Parameter Models”. May 13, 2020. arXiv: 1910.02054v3. \n120. P . Rajpurkar, J. Zhang, K. Lopyrev, and P . Liang. “Squad: 100,000+ Questions for Machine \nComprehension of Text”. 2016. arXiv: 1606.05250. \n121. A. Ramesh, M. Pavlov, G. Goh, and S. Gray. {DALL·E}: Creating Images from Text.J a n .5 , \n2021. \nURL : https://openai.com/blog/dall-e/. \n122. J. Rasley. DeepSpeed. Microsoft, Dec. 20, 2021. URL : https://github.com/microsoft/ \nDeepSpeed (visited on 12/20/2021). \n123. M. T. Ribeiro, S. Singh, and C. Guestrin. “Model-Agnostic Interpretability of Machine \nLearning”. 2016. arXiv: 1606.05386. \n124. A. Rogers, O. Kovaleva, and A. Rumshisky. “A Primer in {Bertology}: What We Know about \nHow {BERT} Works”. In: Trans. Assoc. Comput. Linguist. 8 (2021), pp. 842–866. \n125. S. Rönnqvist, J. Kanerva, T. Salakoski, and F. Ginter. “Is Multilingual BERT Fluent in \nLanguage Generation?” 2019. arXiv: 1910.03806. \n126. A. Rush. “The Annotated Transformer”. In: Proc. Workshop NLP Open Source Softw. NLP-\nOSS Melbourne, Australia: Association for Computational Linguistics, July 2018, pp. 52–60. \nhttps://doi.org/10.18653/v1/W18-2509. \n127. A. B. Sai, A. K. Mohankumar, and M. M. Khapra. “A Survey of Evaluation Metrics Used for \nNLG Systems”. 2020. arXiv: 2008.12009. \n128. E. F. Sang and F. De Meulder. “Introduction to the CoNLL-2003 Shared Task: Languagein-\ndependent Named Entity Recognition”. 2003. arXiv: cs/0306050. \n129. S. Serrano and N. A. Smith. “Is Attention Interpretable?” 2019. arXiv: 1906.03731. \n130. D. So, Q. Le, and C. Liang. “The Evolved Transformer”. In: Int. Conf. Mach. Learn. PMLR, \n2019, pp. 5877–5886. \n131. L. Spinney. “Are We Witnessing the Dawn of Post-Theory Science?” In: The Guardian. \nTechnology (Jan. 9, 2022). ISSN : 0261-3077. URL : https://www.theguardian.com/technology/ \n2022/jan/09/are-we-witnessing-the-dawn-of-post-theory-science (visited on 01/11/2022). \n132. M. Sundararajan, A. Taly, and Q. Yan. “Axiomatic Attribution for Deep Networks”. In: Int. \nConf. Mach. Learn. PMLR, 2017, pp. 3319–3328. \n133. C. Szegedy, V . V anhoucke, S. Ioffe, J. Shlens, and Z. Wojna. “Rethinking the Inception \nArchitecture for Computer Vision”. In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit. \n2016, pp. 2818–2826. \n134. Y . Tay, D. Bahri, D. Metzler, D.-C. Juan, Z. Zhao, and C. Zheng. “Synthesizer: Rethinking \nSelf-Attention in Transformer Models”. May 24, 2021. arXiv: 2005.00743 [cs]. \n135. A. Taylor, M. Marcus, and B. Santorini. “The Penn Treebank: An Overview”. In: Treebanks \n(2003), pp. 5–22. \n136. Tensorﬂow. Integrated Gradients | TensorFlow Core. TensorFlow. Nov. 25, 2021. \nURL : https://www.tensorﬂow.org/tutorials/interpretability/integratedgradients (visited on \n12/06/2021). \n137. Tensorﬂow. Tensorﬂow Webseite. 2019. URL : https://www.tensorﬂow.org/. \n138. tensorﬂow. Uncertainty-Aware Deep Learning with SNGP | TensorFlow Core. Tensor-Flow. \n2021. URL : https://www.tensorﬂow.org/tutorials/understanding/sngp (visited on 07/25/2021). \n139. E. Tjoa and C. Guan. “A Survey on Explainable Artiﬁcial Intelligence (Xai): Toward Medical \nXai”. In: IEEE Trans. Neural Netw. Learn. Syst. (2020). \n140. L. van der Maaten and G. Hinton. “Visualizing Data Using T-SNE”. In: J. Mach. Learn. Res. \n9 (Nov 2008), pp. 2579–2605. \n141. A. V aswani et al. “Attention Is All Y ou Need”. In: Adv. Neural Inf. Process. Syst. 2017, pp. \n5998–6008. \n142. J. Vig. “A Multiscale Visualization of Attention in the Transformer Model”. 2019. arXiv: \n1906.05714. \n143. J. Vig. BertViz. Nov. 23, 2021. URL : https://github.com/jessevig/bertviz (visited on \n11/23/2021).\nReferences 77\n144. J. Vig. BERTVIZ: A Tool for Visualizing Multihead Self-Attention in the BERT Model. 2019. \nURL : https://debug-ml-iclr2019.github.io/cameraready/DebugML-19paper2.pdf. \n145. Wang. SuperGLUE Benchmark. SuperGLUE Benchmark. 2021. URL : https://super. \ngluebenchmark.com/ (visited on 02/23/2021). \n146. A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. “Glue: A Multi-Task \nBenchmark and Analysis Platform for Natural Language Understanding”. Feb. 22, 2019. \narXiv: 1804.07461. \n147. D. Wang, C. Gong, M. Li, Q. Liu, and V . Chandra. “AlphaNet: Improved Training of Supernet \nwith Alpha-Divergence”. 2021. arXiv: 2102.07954. \n148. D. Wang, M. Li, C. Gong, and V . Chandra. “Attentivenas: Improving Neural Architecture \nSearch via Attentive Sampling”. In: Proc. IEEECVF Conf. Comput. Vis. Pattern Recognit. \n2021, pp. 6418–6427. \n149. H . W a n g , Z . W u ,Z .L i u ,H .C a i ,L .Z h u , C . G a n , a n dS .H a n .“ H a t : H a r d w a r e - a w a r e \nTransformers for Efﬁcient Natural Language Processing”. 2020. arXiv: 2005.14187. \n150. M. Welling and Y . W. Teh. “Bayesian Learning via Stochastic Gradient Langevin Dynamics”. \nIn: Proc. 28th Int. Conf. Mach. Learn. ICML-11. 2011, pp. 681–688. \n151. L. Weng. Attention? Attention! Lil’Log. June 24, 2018. URL : https://lilianweng.github.io/ \n2018/06/24/attention-attention.html (visited on 11/19/2021). \n152. F. Wenzel et al. “How Good Is the Bayes Posterior in Deep Neural Networks Really?” 2020. \narXiv: 2002.02405. \n153. G. Wiedemann, S. Remus, A. Chawla, and C. Biemann. “Does BERT Make Any Sense? \nInterpretable Word Sense Disambiguation with Contextualized Embeddings”. 2019. arXiv: \n1909.10430. \n154. Y . Wu et al. “Google’s Neural Machine Translation System: Bridging the Gap between \nHuman and Machine Translation”. 2016. arXiv: 1609.08144. \n155. F. Xu, H. Uszkoreit, Y . Du, W. Fan, D. Zhao, and J. Zhu. “Explainable AI: A Brief Survey on \nHistory, Research Areas, Approaches and Challenges”. In: CCF Int. Conf. Nat. Lang. Process. \nChin. Comput. Springer, 2019, pp. 563–574. \n156. Y . Xu et al. “GSPMD: General and Scalable Parallelization for ML Computation Graphs”. \nDec. 23, 2021. arXiv: 2105.04663 [cs]. \n157. Z. Yang, Z. Dai, R. Salakhutdinov, and W. W. Cohen. “Breaking the Softmax Bottleneck: A \nHigh-Rank RNN Language Model”. 2017. arXiv: 1711.03953. \n158. Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V . Le. “Xlnet: Generalized \nAutoregressive Pretraining for Language Understanding”. In: Adv. Neural Inf. Process. Syst. \n2019, pp. 5753–5763. \n159. Y . Y ou et al. “Large Batch Optimization for Deep Learning: Training Bert in 76 Minutes”. \n2019. arXiv: 1904.00962. \n160. C. Y un, S. Bhojanapalli, A. S. Rawat, S. J. Reddi, and S. Kumar. “Are Transformers Universal \nApproximators of Sequence-to-Sequence Functions?” 2019. arXiv: 1912.10077. \n161. C. Y un, Y .-W. Chang, S. Bhojanapalli, A. S. Rawat, S. J. Reddi, and S. Kumar. “O(n) \nConnections Are Expressive Enough: Universal Approximability of Sparse Transformers”. \n2020. arXiv: 2006.04862. \n162. B. Zhang and R. Sennrich. “Root Mean Square Layer Normalization”. 2019. arXiv: \n1910.07467. \n163. C. Zhang et al. “Resnet or Densenet? Introducing Dense Shortcuts to Resnet”. In: Proc. \nIEEECVF Winter Conf. Appl. Comput. Vis. 2021, pp. 3550–3559. \n164. T. Zhang, V . Kishore, F. Wu, K. Q. Weinberger, and Y . Artzi. “BERTScore: Evaluating Text \nGeneration with BERT”. Feb. 24, 2020. arXiv: 1904.09675. \n165. W. Zhu, X. Wang, X. Qiu, Y . Ni, and G. Xie. “AutoRC: Improving BERT Based Relation \nClassiﬁcation Models via Architecture Search”. 2020. arXiv: 2009.10680. \n166. M.-A. Zöller and M. F. Huber. “Benchmark and Survey of Automated Machine Learning \nFrameworks”. In: J. Artif. Intell. Res. 70 (2021), pp. 409–472.\n78 2 Pre-trained Language Models\nOpen Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 \nInternational License ( http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate \ncredit to the original author(s) and the source, provide a link to the Creative Commons license and \nindicate if changes were made. \nThe images or other third party material in this chapter are included in the chapter’s Creative \nCommons license, unless indicated otherwise in a credit line to the material. If material is not \nincluded in the chapter’s Creative Commons license and your intended use is not permitted by \nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder.",
  "topic": "Security token",
  "concepts": [
    {
      "name": "Security token",
      "score": 0.824573278427124
    },
    {
      "name": "Computer science",
      "score": 0.789812445640564
    },
    {
      "name": "Language model",
      "score": 0.7006212472915649
    },
    {
      "name": "Embedding",
      "score": 0.6366748213768005
    },
    {
      "name": "Transformer",
      "score": 0.6120531558990479
    },
    {
      "name": "Autoregressive model",
      "score": 0.5434598326683044
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5427153706550598
    },
    {
      "name": "Sequence (biology)",
      "score": 0.49732688069343567
    },
    {
      "name": "Natural language processing",
      "score": 0.4962223172187805
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.47586625814437866
    },
    {
      "name": "Encoder",
      "score": 0.4497423768043518
    },
    {
      "name": "Speech recognition",
      "score": 0.4484529197216034
    },
    {
      "name": "Chunking (psychology)",
      "score": 0.44733014702796936
    },
    {
      "name": "Task (project management)",
      "score": 0.43008261919021606
    },
    {
      "name": "Mathematics",
      "score": 0.08365672826766968
    },
    {
      "name": "Engineering",
      "score": 0.08265760540962219
    },
    {
      "name": "Econometrics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ]
}