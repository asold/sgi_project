{
    "title": "LLM aided semi-supervision for efficient Extractive Dialog Summarization",
    "url": "https://openalex.org/W4389520242",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2186927888",
            "name": "Nishant Mishra",
            "affiliations": [
                "Amsterdam University Medical Centers",
                "GGD Amsterdam",
                "University of Amsterdam"
            ]
        },
        {
            "id": "https://openalex.org/A2185362157",
            "name": "Gaurav Sahu",
            "affiliations": [
                "University of Waterloo",
                "ServiceNow (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2250213072",
            "name": "Iacer Calixto",
            "affiliations": [
                "University of Amsterdam",
                "Amsterdam University Medical Centers",
                "GGD Amsterdam"
            ]
        },
        {
            "id": "https://openalex.org/A2168072400",
            "name": "Ameen Abu-Hanna",
            "affiliations": [
                "GGD Amsterdam",
                "University of Amsterdam",
                "Amsterdam University Medical Centers"
            ]
        },
        {
            "id": "https://openalex.org/A3173936353",
            "name": "Issam Laradji",
            "affiliations": [
                "University of British Columbia",
                "ServiceNow (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4284971361",
        "https://openalex.org/W3204846429",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W4205737716",
        "https://openalex.org/W2971034336",
        "https://openalex.org/W3085400433",
        "https://openalex.org/W2924690340",
        "https://openalex.org/W3035160371",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W3094342783",
        "https://openalex.org/W2888482885",
        "https://openalex.org/W3110659220",
        "https://openalex.org/W3172762059",
        "https://openalex.org/W2976223659",
        "https://openalex.org/W3214437934",
        "https://openalex.org/W3212362289",
        "https://openalex.org/W2963929190",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4225893029",
        "https://openalex.org/W4221150595",
        "https://openalex.org/W2962704246",
        "https://openalex.org/W4385571411",
        "https://openalex.org/W2963736842",
        "https://openalex.org/W2904790185",
        "https://openalex.org/W2889984458",
        "https://openalex.org/W2970419734"
    ],
    "abstract": "Generating high-quality summaries for chat dialogs often requires large labeled datasets. We propose a method to efficiently use unlabeled data for extractive summarization of customer-agent dialogs. In our method, we frame summarization as a question-answering problem and use state-of-the-art large language models (LLMs) to generate pseudo-labels for a dialog. We then use these pseudo-labels to fine-tune a chat summarization model, effectively transferring knowledge from the large LLM into a smaller specialized model. We demonstrate our method on the TWEETSUMM dataset, and show that using 10% of the original labelled data set we can achieve 65.9/57.0/61.0 ROUGE-1/-2/-L, whereas the current state-of-the-art trained on the entire training data set obtains 65.16/55.81/64.37 ROUGE-1/-2/-L. In other words, in the worst case (i.e., ROUGE-L) we still effectively retain 94.7% of the performance while using only 10% of the data.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10002–10009\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nLLM aided semi-supervision for Extractive Dialog Summarization\nNishant Mishra1,2∗ Gaurav Sahu3,4 Iacer Calixto1,2\nAmeen Abu-Hanna1,2 Issam H. Laradji4,5\n1Amsterdam UMC, Department of Medical Informatics, University of Amsterdam\n2Amsterdam Public Health, Methodology, Amsterdam, The Netherlands\n3University of Waterloo 4ServiceNow Research\n5University of British Columbia\nAbstract\nGenerating high-quality summaries for chat di-\nalogs often requires large labeled datasets. We\npropose a method to efficiently use unlabeled\ndata for extractive summarization of customer-\nagent dialogs. In our method, we frame sum-\nmarization as a question-answering problem\nand use state-of-the-art large language models\n(LLMs) to generate pseudo-labels for a dia-\nlog. We then use these pseudo-labels to fine-\ntune a chat summarization model, effectively\ntransferring knowledge from the large LLM\ninto a smaller specialized model. We demon-\nstrate our method on the TWEET SUMM dataset,\nand show that using 10% of the original la-\nbelled data set we can achieve 65.9/57.0/61.0\nROUGE-1/-2/-L, whereas the current state-of-\nthe-art trained on the entire training data set\nobtains 65.16/55.81/64.37 ROUGE-1/-2/-L. In\nother words, in the worst case (i.e., ROUGE-L)\nwe still effectively retain 94.7% of the perfor-\nmance while using only 10% of the data.\n1 Introduction\nCustomer support chats are gaining popularity as\na primary medium for servicing clients in indus-\ntry. Summarization models that robustly capture\ncritical information are therefore extremely benefi-\ncial for companies to support various downstream\nactivities such as record-keeping, training. Unfortu-\nnately, standard supervised training of these models\nrely on large labeled datasets, which can be pro-\nhibitively expensive to build. While unsupervised\nsummarization techniques exist (Zou et al., 2021;\nZhang et al., 2021; Shang et al., 2018), enforcing\nthe summary quality and style is still an ongoing\nchallenge.\nIn general terms, chat summarization can be\nabstractive, where the generated summary is un-\nconstrained(Goo and Chen, 2018; Chen and Yang,\n2021; Gupta and Gupta, 2019), orextractive, where\n∗ Corresponding author n.mishra@amsterdamumc.nl.\nWork (partially) done during a research visit at ServiceNow.\nthe summary consists of parts of the original in-\nput (Feigenblat et al., 2021; Liu, 2019). In this\nwork, we propose a novel extractive dialog summa-\nrization method based on a semi-supervised learn-\ning paradigm, and demonstrate that we either im-\nprove or perform comparably to the current state-\nof-the-art on the TWEET SUMM dataset (Feigenblat\net al., 2021), while using only 10% of the training\ndata.\nPseudolabeling(Lee, 2013; Pham et al., 2021) is\na popular and efficient method for semi-supervised\nlearning. Pre-trained LLMs encode a vast breadth\nof knowledge in various tasks and can generate hu-\nman like response. They are good annotators(Wang\net al., 2021; Ding et al., 2023) and also ideal for\ntransferring knowledge through distillation(Kim\net al., 2022; Kim and Rush, 2016; Liu et al., 2021).\nHence, we decided to use LLMs 1) as weak la-\nbellers to generate automatic summaries for a large\nset of unlabelled examples (i.e., pseudo-labels);\nand 2) as evaluators to choose only a small num-\nber of high-quality (pseudo-labelled) examples to\ninclude in the next training cycle. We report strong\nresults on the TWEET SUMM dataset in zero- and\nfew-shot semi-supervised settings. Our main con-\ntributions are as follows.\n• We introduce a semi-supervised method for\nextractive summarization to distill knowledge\nfrom general-purpose LLMs into smaller spe-\ncialized summarization models.\n• We show that our methods have better or\ncomparable performance to the current state-\nof-the-art trained on the entire training data\n(according to ROUGE), while requiring only\n10% of the labelled data.\n• We show that framing extractive summa-\nrization as a question-answering problem\nallows us to efficiently leverage large lan-\nguage models for weak supervision (e.g., GPT-\n3.5; Ouyang et al. 2022).\n10002\nLabelled\ndata \nUnlabelled\ndata \nPseudo-\nlabelled\ndata \nFiltered pseudo-\nlabelled data \nStep1:\nWeakly label with\nGPT-QA prompt\nStep 2:\nEvaluate\n(token\nprobability)\nNew\nlabelled set \nNew unlabelled set\nStep 3:\nTrain DistilBART\niterate  times\nFigure 1: Overview of our approach.\nRelated work Chen and Yang (2021) introduced\nConversational Data Augmentation (CODA), a sim-\nple conversational data augmentation framework\nfor abstractive dialog summarization with an op-\ntional extension for a semi-supervised learning set-\nting (He et al., 2020; Xie et al., 2020). Sznajder\net al. (2022) propose a heuristic-based weakly-\nsupervised abstractive summarization model for\nthe TWEET SUMM dataset. They first train separate\nmodels for customer and agent on a weakly-labeled\nset obtained by using the LONG and LEAD heuris-\ntics and then finetune such models on labelled con-\nversation data. Liu and Lapata (2019) use pre-\ntrained encoders like BERT and propose PreSumm,\na model for abstractive and extractive summariza-\ntion based on sentence classification.\n2 Methodology\n2.1 Notation\nWe assume a training dataset D= DL ∪DU with\nlabelled (DL) and unlabelled examples (DU ). La-\nbelled examples DL = {ui,si}DL\ni=1 consist of a\ndialog instance ui = {ui,1,ui,2,··· ,ui,N }com-\nposed of N input tokens and dialog ui’s summary\nsi = {si,1,si,2,··· ,si,M }with M tokens. Unla-\nbelled examples DU = {uj}DU\nj=1 consist of a dialog\ninstance uj = {uj,1,uj,2,··· ,uj,K}with Kinput\ntokens for which there is no summary. In most\npractical cases, DU ≫DL.\n2.2 Iterative procedure for knowledge\ndistillation\nIn Figure 1, we show an overview of our approach.\nWe use an idea similar to teacher-student trans-\nfer learning(Sanh et al., 2020; Shleifer and Rush,\n2020a; Tang and Huang, 2022) and propose an\niterative training procedure with C cycles. Each\ncycle of training consists of three steps: 1) Use\na weak-labeller to generate pseudo-labels for di-\nalogs uj ∈DU , 2) evaluate and select high-quality\npseudo-labelled examples (DP ) for the next step\nwithout replacement, and 3) retrain summarization\nmodel using labelled and pseudo-labelled examples\nDnew\nL .\nIn practice, we pseudo-label all unlabelled sam-\nples DU only once using GPT-3.5 and re-use the\npredicted labels at each cycle c ∈C. We can do\nthis efficiently for two reasons: the number of un-\nlabelled samples in TWEET SUMM is small (i.e.,\n850 data points); and we do not fine-tune GPT-3.5,\nwhich is an efficient and performant alternative (as\nwe will show in our experimental results).\n1) Weak labelling Before the first cycle (c= 0),\nwe use GPT-3.5 (OpenAI, 2023) in a few-shot set-\nting to generate pseudo-labels for all unlabelled ex-\namples DU . We refer to this set of pseudo-labelled\nexamples as DP = {uj,ˆsj}DU\nj=1. We framed\nthe extractive summarization task as a question-\nanswering problem for GPT-3.5. The dialogues\nwere converted into a sequence of numbered sen-\ntences. The prompt consisted of an instruction that\nasked the model to return sentence numbers that\nadequately summarised the dialogue from both per-\nspectives. We also provided several examples as\ncontext. We then extracted the sentences in the dia-\nlog corresponding to the numbers that the model re-\nturned to construct the pseudo-labeled summaries.\nThis helped ensure that the pseudo-labels thus ob-\ntained were always extractive. Please refer to Ap-\npendix A for more details about the overall process\nof pseudo-labelling and samples of the prompts\nused.\n2) Evaluating and selecting pseudo-labelled ex-\namples We first evaluate the quality of each gen-\nerated pseudo-labelled example in DP . We use the\nlog-probability assigned by GPT-3.5 to tokens cor-\nresponding to thenumbers of all sentences included\nas part of the summary in step 1, and sum these log-\nprobabilities to compute a score for each pseudo-\nlabelled example in DP . We then select a subset\nD′\nP ⊂DP of the D′\nP highest-scoring examples. Fi-\nnally, we merge the original labelled data DL with\nthe selected pseudo-labelled data D′\nP , generating a\nnew set of labelled examples Dnew\nL = DL ∪D′\nP to\nuse to train the summarization model.\n3) Train summarization model usingDnew\nL We\ntrain a sequence-to-sequence model on the updated\nset of labelled examples Dnew\nL , which includes the\noriginal labelled examples DL and the pseudo-\nlabelled examples D′\nP selected in step 2. We train\n10003\nthis model to generate an extractive summary si\nfor a dialog ui by minimising the negative log-\nlikelihood L(S) =L(DL) +L(D′\nP), given that\nL(DL) =\nDL∑\ni=1\n−log p(si,t|si,<t,ui; θ),\nL(D′\nP) =\nD′\nP∑\nj=1\n−log p(ˆsj,t|ˆsj,<t,uj; θ),\nwhere si,<t (ˆsj,<t) are the first t−1 tokens of the\ngold-standard summary si (the generated summary\nˆsj).\n4) Convert generative summaries to Extractive\nThe summarization model is given a dataset where\nthe ground truth summaries always contain sen-\ntences from within the dialog, thus we hypothesize\nthat the model should learn representations in a way\nto output exclusively extractive summaries. Even\nwith qualitative and quantitative results validating\nthis, there is no guarantee of an exclusively ex-\ntractive output from a generative seq-to-seq model.\nThus we add another step to ensure extractive sum-\nmarization.\nThe generated summaries si are converted to ex-\ntractive summaries by a sentence matching proce-\ndure. Each sentence in the generated summary and\nthe dialogue was first embedded using a pre-trained\nBERT-based sentence transformer model1(Reimers\nand Gurevych, 2019). For each sentence embed-\nding in the generated summary, we calculated its co-\nsine distance with each sentence embedding from\nthe original dialogue to obtain semantic textual sim-\nilarity. We then replaced the summary sentences\nwith the corresponding sentence in the dialog that\nhad the highest similarity i.e. lowest cosine dis-\ntance from them, constructing the final extractive\nsummary.\n3 Experimental Setup\nData In our experiments, we use the TWEET -\nSUMM dataset (Feigenblat et al., 2021), which con-\ntains 1,100 labelled examples DL of dialog be-\ntween customers and customer support agent. We\nuse the original splits and have 880, 110, and 110\nexamples for training, model selection, and testing,\nrespectively. In semi-supervised experiments, we\nsubsample a fraction of data as labelled and rest is\nused as unlabelled. We repeat these experiments\n1We used the Sentence-Transformers library\nwith three random seeds and average results to ac-\ncount for variance. All the evaluations of extractive\nsummarization for the models described in the ex-\nperiments below are performed in a limited length\nof 80 tokens setting.\nOur Method We used the method described\nin section 2.2. Concretely, we use Distil-\nBART (Shleifer and Rush, 2020b)—a distilled ver-\nsion of BART (Lewis et al., 2019) with 6 encoder\nand 6 decoder layers—as the sequence-to-sequence\nmodel. We set the number of examples D′\nP se-\nlected by the evaluator at each cycle as16, the total\nnumber of cycles C = 10, and in each cycle we\ntrain the extractive summarization model for 10\nepochs.\nVanilla method We also explore a ‘vanilla’ ver-\nsion of our method. Crucially, with this method\nwe do not transfer knowledge from a massive LLM\n(e.g., GPT-3.5) into the summarization model but\nuse the summarization model in a two-step noisy\nself-training paradigm(Chen et al., 2021).\n3.1 Iterative training and GPT-3.5\nWe devise a number of baselines for this experi-\nment. We use LEAD-1 and LEAD-2 (Kryscinski\net al., 2019), two competitive extractive summa-\nrization baselines which take the first one (two)\nleading sentences each from the customer and from\nthe agent as the final summary. We also useLONG-\n1 where the agent’s and the customer’s longest\nsentences are taken as the final summary. These\nbaselines are not iterative. We directly measure\nthese heuristically generated summaries on the test\nset. We propose two iterative semi-supervised\nbaselines where we use the agent’s and the cus-\ntomer’s first sentence (LEAD-1-i) and longest sen-\ntence (LONG-1-i), respectively, as the initial sum-\nmary (§ 2.2, step 1), but instead of selecting the X\nhighest-scoring examples (§ 2.2, step 2), we ran-\ndomly select X examples to use as D′\nP (§ 2.2, step\n3).\n3.2 Few-shot learning\nFor this set of experiments, we retrain state-of-the-\nart task-tuned language models for extractive dia-\nlog summarization. We use DistilBART fine-tuned\non the XSum (Narayan et al., 2018, DistilBART-\nxsum) and CNN-dailymail (Nallapati et al., 2016,\nDistilBART-cnn) datasets, and further fine-tune\nthese two models once using various subsamples\nof the TWEET SUMM dataset.\n10004\nModels Iter. R-1 R-2 R-L\nLEAD-1 40.89 34.22 38.74\nLEAD-2 53.51 43.27 47.76\nLONG-1 54.47 46.56 50.58\nLEAD-1-i 62.64 53.13 57.46\nLONG-1-i 59.80 50.12 54.36\nOurs (GPT-3.5) 65.93 56.94 60.96\nTable 1: Results on TWEET SUMM ’s test set. Compari-\nson between five simple baselines and our method with\nGPT 3.5 as weak labeller and evaluator and DistilBART-\ncnn as summarizer. Baselines can be iterative ( ) or\nnot ( ). R-1, R-2, and R-L correspond to ROUGE-1,\nROUGE-2, and ROUGE-L F-measure, respectively.\n3.3 Full data scenario\nHere, we use state-of-the-art models for ex-\ntractive dialog summarization—including models\nused in the original TWEET SUMM dataset pa-\nper (Feigenblat et al., 2021)—as well as instruction-\ntuned LLMs to directly summarize dialogs. Pre-\nSumm (Liu and Lapata, 2019) frames extractive\nsummarization as a sentence classification problem\nby predicting whether sentences are noteworthy,\nand is the current SotA on TWEET SUMM for ex-\ntractive summarization. In GPT-3.5 we directly\nprompt the model to auto-regressively generate the\nentire summary for a dialog.\nIn GPT-3.5-QA we treat extractive summariza-\ntion as a question-answering problem as we do in\nour method (§ 2.2),\nWe provide more details on how we prompt\nGPT-3.5 autoregressively and in a QA setting in\nAppendix A. For both GPT-3.5 and GPT-3.5-QA,\nwe experiment in different few-shot in-context\n(Brown et al., 2020) settings– {0,1,2,4,8}–within\nthe 4096 token limit. We measure the performance\nusing Prompted Direct Inference(PDI), directly\nmeasuring the performance on test data.\n4 Results\nIn § 4.1, we first discuss how our method compares\nto simple baselines (iterative and non-iterative),\nclearly showing it surpasses both. In § 4.2, we ex-\namine how our approach fares in a few-shot learn-\ning setting where there is no-to-little training data\navailable. In § 4.3, we compare our best few-shot\nmodels with the current SotA using all training\ndata available, and show that our method performs\ncomparably to the current SotA while using only\nModels Iter. # labelled / unlabelled data points\n0 / 850 1 / 849 8 / 842 80 / 770\nD-BART-xsum 3.93 33.51 44.56 47.16\nD-BART-cnn 30.95 42.87 46.65 50.75\nOurs (vanilla)‡ 50.72 50.16 51.42 51.83\nOurs (GPT-3.5)‡ 53.75 54.15 55.82 56.94\nTable 2: ROUGE-2 F-measures on TWEET SUMM ’s test\nset. D-BART is DistilBART. ‡: Our methods either\nonly use DistilBART-cnn as weak labeller and sum-\nmarizer (vanilla) or use GPT-3.5 as weak labeller and\nDistilBART-cnn as summarizer (for details, see § 2.2).\n10% of the training data. We use the ROUGE-1,\nROUGE-2 and ROUGE-L metrics (Lin, 2004)\nsince these are widely used metrics to automati-\ncally evaluate text summarization models, and also\nfor comparing our results with existing work.\n4.1 Iterative training\nWe report results in Table 1. We observe that Our\napproach significantly outperforms baselines di-\nrectly using first and longest sentences from the\ncustomer and agent as the summary in both itera-\ntive and non-iterative scenario across all metrics.\nMoreover, cyclic baselines clearly outperform their\nnon-cyclic versions, highlighting the importance of\ndistilling knowledge into the summarization model\nin small steps.\n4.2 Few-shot learning\nIn Table 2, we first observe consistent gains from\nusing semi-supervised learning and iteratively train-\ning the summarization model (DistilBART) us-\ning more and more pseudo-labelled dialogs, over\nthe same models fine-tuned on only labelled data.\nWe also consistently improve DistilBART’s per-\nformance by using GPT-3.5 as the weak labeller\nand evaluator, i.e., between3.3%–5.1% increase in\nROUGE-2 compared to our vanilla method.\n4.3 Full data scenario\nIn Table 3, we show that our best model using GPT-\n3.5—trained on only 10% of the data—outperforms\nPreSumm—which is trained on the entire training\ndataset—according to ROUGE-1 and ROUGE-2,\nand is only 3.5% behind according to ROUGE-L.\nWe also outperform DistillBART models finetuned\non the entire dataset by almost 2 points across\nall metrics using only 10% of the labelled data.\nAmong the in-context prompt based models, GPT-\n3.5-QA outperforms GPT-3.5 which is notable as\nit shows why we used it for pseudo-labelling. The\n10005\nModels Iter. Semi. R-1 R-2 R-L\nGPT 3.5 in-ctx 58.11 49.48 54.86\nGPT 3.5-QA in-ctx 62.05 52.98 57.63\nFull data\nD-BART-xsum 61.24 54.05 58.10\nD-BART-cnn 63.61 54.84 58.91\nPreSumm† 65.16 55.81 64.37\n10% of training data\nOurs (vanilla)‡ 61.30 51.83 56.89\nOurs (GPT-3.5)‡ 65.93 56.94 60.96\nTable 3: Results onTWEET SUMM ’s test set. D-BARTis\nDistilBART.†: PreSumm is the model proposed in Liu\nand Lapata (2019). ‡: These are our best models in\neach case, which is always the corresponding model\ntrained on 10% of the training data. R-1, R-2, and R-L\ncorrespond to ROUGE-1, ROUGE-2, and ROUGE-L\nF-measure, respectively.\nscores for GPT-3.5 and GPT-3.5-QA are from two-\nshot in-context learning which outperformed zero,\none, four and eight shot performances.\n5 Conclusions and Future Work\nIn this work, we introduce a semi-supervised\napproach for extractive summarization to dis-\ntill knowledge from general-purpose LLMs into\nsmaller specialized summarization models. We\ndemonstrate our method on the TWEET SUMM\ndataset, and show that while training on only 10%\nof the data our method is competitive to PreSumm,\nthe current state-of-the-art, which uses 100% of\nthe training data. We outperform PreSumm by\n0.3% according to ROUGE-1 and ROUGE-2, but\nare still 4% behind according to ROUGE-L. As\nfuture work, we will extend our method to other\ntext summarization datasets. We will also explore\nusing various open-source, tunable LLMs for both\nsummarization and pseudolabelling.\nLimitations\nWe show promising results achieved using\ninstruction-tuned LLMs to help with semi-\nsupervised dialog summarization, But there are\na few limitations to the work that are laid out in\nthis section. One of the major issues is in terms of\nbreadth of the experiments. The current work deals\nwith only one dataset i.e TweetSumm and one kind\nof summarization, Extractive dialogue summariza-\ntion. Further experiments involving other popular\ndatasets comprising different domains, sizes and\ntasks will help consolidate the performance of our\nmethod.\nAnother significant limitation is the closed\nsource nature of the model we are using for pseudo-\nlabelling i.e GPT-3.5(text-davinci-003). This\nmakes it prohibitively expensive to fine-tune or\nreproduce in cases where the number of samples\nto be pseudo-labelled is substantial. Therefore we\nended up using a fixed teacher model and generat-\ning the pseudo-labels only once. Having a more\ntunable, open-source instruction-tuned LLM as the\nweak labeller would have enabled prompt-learning\nto further improve pseudo-labels. This might have\nhelped us unlock other aspects of teacher-student\nlearning by allowing feedback from the smaller\nstudent model to improve/align the larger model\nupdating the pseudo-labels every cycle i.e meta\npseudo-labeling (Pham et al., 2021). It would also\nhave provided more insight into the method while\nbeing less expensive for our task. We tried to over-\ncome this limitation by exploring open-source in-\nstruction tuned models like alpaca-7B, vicuna-13B\nas our weak labeller. But the compute requirements\nto host and infer with these models, inferior and\nslow inference using the low parameter and quan-\ntized versions proved that they are not yet tractable\nand robust.\nEthics Statement\nThe dataset we used, TWEET SUMM is constructed\nfrom an open-source customer support dataset\navailable here .The data, although from real pub-\nlic interaction, has been anonymized. It’s annota-\ntion was crowdsourced. Further we use GPT-3.5\nfor pseudo-labelling.Since this is from real-world\ncustomer-agent interactions we can, in rare cases\nand based on context get spurious, undesirable or\nbiased outputs. Although chances of that are mini-\nmized by framing it into a QA problem and limiting\nthe generated tokens to a very small window.\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\n10006\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nJiaao Chen and Diyi Yang. 2021. Simple conversational\ndata augmentation for semi-supervised abstractive\ndialogue summarization. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 6605–6616.\nYiming Chen, Yan Zhang, Chen Zhang, Grandee Lee,\nRan Cheng, and Haizhou Li. 2021. Revisiting self-\ntraining for few-shot learning of language model.\nBosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken\nChia, Shafiq Joty, Boyang Li, and Lidong Bing. 2023.\nIs gpt-3 a good data annotator?\nGuy Feigenblat, Chulaka Gunasekara, Benjamin Szna-\njder, Sachindra Joshi, David Konopnicki, and Ranit\nAharonov. 2021. Tweetsumm–a dialog summariza-\ntion dataset for customer service. arXiv preprint\narXiv:2111.11894.\nChih-Wen Goo and Yun-Nung (Vivian) Chen. 2018.\nAbstractive dialogue summarization with sentence-\ngated modeling optimized by dialogue acts. 2018\nIEEE Spoken Language Technology Workshop (SLT),\npages 735–742.\nSom Gupta and Sanjai Kumar Gupta. 2019. Abstractive\nsummarization: An overview of the state of the art.\nExpert Systems with Applications, 121:49–65.\nJunxian He, Jiatao Gu, Jiajun Shen, and Marc’Aurelio\nRanzato. 2020. Revisiting self-training for neural\nsequence generation. In Proceedings of ICLR.\nSu Young Kim, Hyeonjin Park, Kyuyong Shin, and\nKyung-Min Kim. 2022. Ask me what you need:\nProduct retrieval using knowledge from gpt-3. arXiv\npreprint arXiv:2207.02516.\nYoon Kim and Alexander M. Rush. 2016. Sequence-\nlevel knowledge distillation. In Proceedings of the\n2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1317–1327, Austin,\nTexas. Association for Computational Linguistics.\nWojciech Kryscinski, Nitish Shirish Keskar, Bryan Mc-\nCann, Caiming Xiong, and Richard Socher. 2019.\nNeural text summarization: A critical evaluation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 540–551, Hong\nKong, China. Association for Computational Linguis-\ntics.\nDong-Hyun Lee. 2013. Pseudo-label : The simple and\nefficient semi-supervised learning method for deep\nneural networks.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-\nnoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension.\narXiv preprint arXiv:1910.13461.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nYang Liu. 2019. Fine-tune bert for extractive summa-\nrization. arXiv preprint arXiv:1903.10318.\nYang Liu and Mirella Lapata. 2019. Text summariza-\ntion with pretrained encoders. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3730–3740, Hong Kong,\nChina. Association for Computational Linguistics.\nYang Liu, Sheng Shen, and Mirella Lapata. 2021. Noisy\nself-knowledge distillation for text summarization.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 692–703, Online. Association for Computa-\ntional Linguistics.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nÇa˘glar Gulçehre, and Bing Xiang. 2016. Abstrac-\ntive text summarization using sequence-to-sequence\nRNNs and beyond. In Proceedings of the 20th\nSIGNLL Conference on Computational Natural Lan-\nguage Learning, pages 280–290, Berlin, Germany.\nAssociation for Computational Linguistics.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1797–1807, Brussels, Bel-\ngium. Association for Computational Linguistics.\nOpenAI. 2023. GPT-3.5. \"https://platform.\nopenai.com/docs/models/gpt-3-5\".\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nHieu Pham, Zihang Dai, Qizhe Xie, and Quoc V . Le.\n2021. Meta pseudo labels. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 11557–11568.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\n10007\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2020. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter.\nGuokan Shang, Wensi Ding, Zekun Zhang, Antoine Tix-\nier, Polykarpos Meladianos, Michalis Vazirgiannis,\nand Jean-Pierre Lorré. 2018. Unsupervised abstrac-\ntive meeting summarization with multi-sentence com-\npression and budgeted submodular maximization. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 664–674, Melbourne, Australia.\nAssociation for Computational Linguistics.\nSam Shleifer and Alexander M. Rush. 2020a. Pre-\ntrained summarization distillation.\nSam Shleifer and Alexander M Rush. 2020b. Pre-\ntrained summarization distillation. arXiv preprint\narXiv:2010.13002.\nBenjamin Sznajder, Chulaka Gunasekara, Guy Lev,\nSachin Joshi, Eyal Shnarch, and Noam Slonim.\n2022. Heuristic-based inter-training to improve few-\nshot multi-perspective dialog summarization. arXiv\npreprint arXiv:2203.15590.\nYing-Jhe Tang and Hen-Hsen Huang. 2022. A teacher-\nstudent approach to cross-domain transfer learning\nwith multi-level attention. In IEEE/WIC/ACM Inter-\nnational Conference on Web Intelligence and Intelli-\ngent Agent Technology, WI-IAT ’21, page 494–499,\nNew York, NY , USA. Association for Computing\nMachinery.\nShuohang Wang, Yang Liu, Yichong Xu, Chenguang\nZhu, and Michael Zeng. 2021. Want to reduce la-\nbeling cost? GPT-3 can help. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021, pages 4195–4205, Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nQizhe Xie, Eduard H. Hovy, Minh-Thang Luong, and\nQuoc V . Le. 2020. Self-training with noisy student\nimproves imagenet classification. 2020 IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition (CVPR), pages 10684–10695.\nXinyuan Zhang, Ruiyi Zhang, Manzil Zaheer, and Amr\nAhmed. 2021. Unsupervised abstractive dialogue\nsummarization for tete-a-tetes. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 35, pages 14489–14497.\nYicheng Zou, Jun Lin, Lujun Zhao, Yangyang Kang,\nZhuoren Jiang, Changlong Sun, Qi Zhang, Xuanjing\nHuang, and Xiaozhong Liu. 2021. Unsupervised\nsummarization for chat logs with topic-oriented rank-\ning and context-aware auto-encoders. In Proceedings\nof the AAAI Conference on Artificial Intelligence, vol-\nume 35, pages 14674–14682.\nA Appendix\nA.1 Pseudo-labelling using GPT\nWe used instruction tuned GPT-3.5(text-davinci-\n003) for Prompt Guided Unlabeled data annotation\nfor extractive summarization. We explore two dif-\nferent approaches to the problem that are described\nbelow. We test and report results for the two meth-\nods through Prompted Direct Inference(PDI), di-\nrectly measuring the performance by annotating\ntest data.\nA.1.1 GPT-3.5 QA\nHere we frame the task of weak labelling as a\nquestion-answering problem: we first split a given\ndialog uj into sentences using a sentence splitter;2\nwe then number these sentences from 1 until N,\nwhere N is the total number of sentences found in\nuj (e.g., if the original sentence reads ‘This is a\nsentence. ’and it is the 2nd sentence in the dialog,\nit becomes ‘2) This is a sentence. ’); In order to con-\nvert the summary to the suitable format, we first\ncompare the sentences in summary to the sentences\nin dialogue to find their positions in the dialog and\nthen map the summary into a fixed structured sen-\ntence that highlights the numbers of the sentence\ncontaining the summary from both perspectives.\nWe build a prompt containing a few labelled\nexamples (i.e., gold-standard dialog and summary)\nas context followed by the dialog uj for which we\nwish to generate a summary; we prompt GPT-3.5\nasking it for the numbers of all the sentences that\nmust be included as part of the summary. To be\nmore precise, we ask it to answer with sentence\nnumbers that describe the issue being faced by the\ncustomer in the dialog and sentence numbers that\nbest highlight the answers provided by the agent.\nOnce we obtain the numbers of the sentences\nGPT-3.5 deems suitable to be part of the summary,\nwe build the summaryˆsj by simply extracting these\nsentences from the original input uj.\nFigure 2 shows an example of a prompt we built\nfor pseudolabelling along with the response from\nGPT-3.5.\nA.1.2 GPT-3.5\nIn GPT-3.5 we directly prompt the model to auto-\nregressively generate the entire summary for a dia-\nlog. We provide labeled pairs of dialogue and sum-\nmary as context and ask it to generate summary\nfor the next dialogue. Since this is an extractive\n2We used the spaCy sentencizer for this\n10008\nInstruction:\nYou are given two dialogues between a customer and a customer support agent numbered by sentences along with answers\nthat highlight the sentence numbers in the dialogue which best summarize the issue raised by the customer and the solution\nproposed by the agent, as context. Your task is to provide a similar answer highlighting the sentence numbers that describe\nthe customer issue and the resolution for the next numbered dialogue. You should choose at least two sentences and at most\nfour sentences in total so as to provide a short summary. The summary should be short and should capture the issues faced\nby the customer and the resolution\nExamples: \nDialogue: \n1. Customer:    @AmazonHelp @115821 Wow, expected 4 packages yesterday, but only 2 showed up.\n2. Customer:    50% failure rate-not impressed.\n3. Customer:    Glad I paid for fast shipping.\n4. Customer:    @AmazonHelp @115821 Last month driver delivered box, sat in van for 10 min on phone, and drove off.\n5. Customer:    Then got notice that package was \"lost in transit\".\n6. Agent:           @258930 I'm sorry you only received two of the orders.\n7. Agent:           Is this happening with the same carrier each time?\n8. Agent:           We can see what options are available for the lost items, reach us by phone or chat\n here: https://t.co/hApLpMlfHN ^MG\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  \n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  \n20. Customer:   @AmazonHelp Got shipping refunded via chat, and packages will be here when they get here.\n22. Agent:         @258930 Keep us posted, Dave!\nAnswer: The issue faced by the customer is summarized by sentences 1, 2, 4 and the resolution given by the agent is\nsummarized by sentences 8,16\nDialogue: \n1. Customer:    So neither my iPhone nor my Apple Watch are recording my steps/activity, and Health doesn’t recognise either\n source anymore for some reason.\n2. Customer:    Any ideas?\n3. Customer:    https://t.co/m9DPQbkftD\n4. Customer:    @AppleSupport please read the above.\n5. Agent:          @135060 Let’s investigate this together.\n6. Agent:          To start, can you tell us the software versions your iPhone and Apple Watch are running currently?\n7. Customer:    @AppleSupport My iPhone is on 11.1.2, and my watch is on 4.1.\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  \n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  \n16. Agent:       When reaching out in DM, let us know when this first started happening please.\n17. Agent:       For example, did it start after an update or after installing a certain app?\nAnswer: \nThe issue faced by the customer is summarized by sentences 1, 7 and the resolution given by the agent is \nsummarized by sentences 6,15\nPromptOutput\nFigure 2: The prompt used for GPT-3.5 QA in-\ncontext extractive dialog summarization and pseudo-\nlabelling. The first part contains the prompt with instruc-\ntions and examples, and the second part is the response\ngenerated. Blue represents the dialogs, red represents\nthe sample summaries/answers part of the prompt and\ngreen represents the final summary/answer generated.\nWe show only one shot in-context here for brevity\nsummarization problem, we make sure to clarify\nin the prompt instruction to use sentences that are\npart of the dialog and not to synthesize new sen-\ntences or paraphrase. Figure 3 shows the prompt\nthat we used for getting summaries with GPT-3.5\nby completion.\nB Test learning curves\nFigure 4 shows the test learning curves for our\nsummarization model(distillBART-cnn) across 10\ncycles of semi-supervised training. Each curve\nrepresents different settings in terms of numbers\nof labelled samples at the start. Every cycle, we\nadd new pseudo-labelled samples generated with\nGPT-3.5 QA.\nInstruction:\nYou are given two examples of dialogues between a customer and a customer support agent along with an extractive summary of the\ndialogues highlighting the issue and suggested solution as context, your task is to generate an extractive summary of the next dialogue. The\nsummary should be short and should capture the issues faced by the customer and suggested solution. Further, the summary should only\ncontain sentences from the dialogue.\nExamples: \nDialogue:     \nCustomer:    So neither my iPhone nor my Apple Watch are recording my steps/activity, and Health doesn’t recognise either source anymore\n     for some reason. Any ideas? https://t.co/m9DPQbkftD\nCustomer:    @AppleSupport please read the above.\nAgent:          @135060 Let’s investigate this together. To start, can you tell us the software versions your iPhone and Apple Watch are running\ncurrently?\nCustomer:    @AppleSupport My iPhone is on 11.1.2, and my watch is on 4.1.\nAgent:          @135060 Thank you. Have you tried restarting both devices since this started happening?\nCustomer:    @AppleSupport I’ve restarted both, also un-paired then re-paired the watch.\nAgent:          @135060 Got it. When did you first notice that the two devices were not talking to each other. Do the two devices communicate\n     through other apps such as Messages?\nCustomer:    @AppleSupport Yes, everything seems fine, it’s just Health and activity.\nAgent:          @135060 Let’s move to DM and look into this a bit more. When reaching out in DM, let us know when this first \n     started happening please. For example, did it start after an update or after installing a certain app? \nSummary: \nCustomer:    So neither my iPhone nor my Apple Watch are recording my steps/activity, and Health doesn’t recognise either source \n     anymore for some reason.\nAgent:           To start, can you tell us the software versions your iPhone and Apple Watch are running currently?\nCustomer:    @AppleSupport My iPhone is on 11.1.2, and my watch is on 4.1.\nDialogue:\nCustomer:    @hulu_support My watchlist is not updating with new episodes (past couple days).\nCustomer:    Any idea why?\nAgent:          @658975 Apologies for the trouble, Norlene!\nAgent:          We're looking into this.\nAgent:          In the meantime, try navigating to the season / episode manually.\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  \n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  \nCustomer:    Some shows updated overnight, but others did not...\nAgent:          @658975 We definitely understand, Norlene.\nAgent:          For now, we recommend checking the show page for these shows as the new eps will be there\nCustomer:    @hulu_support As of this morning, the problem seems to be resolved.\nCustomer:    Watchlist updated overnight with all new episodes.\nCustomer:    Thank you for your attention to this matter!\nCustomer:    I love Hulu\nAgent:          That's what we love to hear.\nAgent:          If you happen to need anything else, we'll be here to support!\nSummary:\nCustomer:    @hulu_support My watchlist is not updating with new episodes (past couple days).\nAgent:          In the meantime, try navigating to the season / episode manually.\nCustomer:    @hulu_support Tried logging out/back in, that didn’t help\nAgent:          We assure you that our team is working hard to investigate, and we hope to have a fix ready soon!\nAgent:          For now, we recommend checking the show page for these shows as the new eps will be there    \nCustomer:    As of this morning, the problem seems to be resolved\",\nPromptOutput\nFigure 3: The prompt used for GPT-3.5 in-context\nextractive dialog summarization.The first part con-\ntains the prompt with instructions and examples, and the\nsecond part is the response generated.Blue represents\nthe dialogs, red represents the sample summaries part\nof the prompt and green represents the final summary\ngenerated. We show only one shot in-context here for\nbrevity\n0 2 4 6 8\nCycles\n42\n44\n46\n48\n50\n52\n54\n56\n58T est ROUGE-2\n#Labelled Samples\n1\n8\n85\nFigure 4: Test curves for our method (using GPT-3.5 as\nweak labeller and evaluator) when trained on 1, 8, and\n85 samples (i.e., approximately 0.1%, 1%, and 10% of\nthe TWEET SUMM training data).\n10009"
}