{
  "title": "Comparative analysis of large language models in the Royal College of Ophthalmologists fellowship exams",
  "url": "https://openalex.org/W4376114558",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2931916413",
      "name": "Raffaele Raimondi",
      "affiliations": [
        "Sunderland Eye Infirmary"
      ]
    },
    {
      "id": "https://openalex.org/A2989152130",
      "name": "Nikolaos Tzoumas",
      "affiliations": [
        "Sunderland Eye Infirmary",
        "Newcastle University"
      ]
    },
    {
      "id": "https://openalex.org/A2892391799",
      "name": "Thomas Salisbury",
      "affiliations": [
        "James Cook University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2479173406",
      "name": "Sandro Di Simplicio",
      "affiliations": [
        "Royal Victoria Infirmary"
      ]
    },
    {
      "id": "https://openalex.org/A2499620760",
      "name": "Mario R. Romano",
      "affiliations": [
        "Humanitas University"
      ]
    },
    {
      "id": null,
      "name": "North East Trainee Research in Ophthalmology Network (NETRiON)",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2748936357",
      "name": "Tejaswi Bommireddy",
      "affiliations": [
        "Royal Victoria Infirmary"
      ]
    },
    {
      "id": "https://openalex.org/A2427522847",
      "name": "Harshika Chawla",
      "affiliations": [
        "Royal Victoria Infirmary"
      ]
    },
    {
      "id": "https://openalex.org/A2102673737",
      "name": "Yanmei Chen",
      "affiliations": [
        "Sunderland Eye Infirmary"
      ]
    },
    {
      "id": "https://openalex.org/A2525084650",
      "name": "Sinéad Connolly",
      "affiliations": [
        "Royal Victoria Infirmary"
      ]
    },
    {
      "id": "https://openalex.org/A4376708845",
      "name": "Samy El Omda",
      "affiliations": [
        "Newcastle University"
      ]
    },
    {
      "id": "https://openalex.org/A2601997334",
      "name": "Melissa Gough",
      "affiliations": [
        "Newcastle University"
      ]
    },
    {
      "id": "https://openalex.org/A42348906",
      "name": "Lyudmila Kishikova",
      "affiliations": [
        "Royal Victoria Infirmary"
      ]
    },
    {
      "id": "https://openalex.org/A1998917732",
      "name": "Thomas McNally",
      "affiliations": [
        "Royal Victoria Infirmary"
      ]
    },
    {
      "id": "https://openalex.org/A4376708849",
      "name": "Salman N. Sadiq",
      "affiliations": [
        "Sunderland Eye Infirmary"
      ]
    },
    {
      "id": "https://openalex.org/A2330345051",
      "name": "Samuel A Simpson",
      "affiliations": [
        "Cumberland Infirmary"
      ]
    },
    {
      "id": "https://openalex.org/A2564544718",
      "name": "Boon Lin Teh",
      "affiliations": [
        "Royal Victoria Infirmary"
      ]
    },
    {
      "id": "https://openalex.org/A3091680709",
      "name": "Steven Toh",
      "affiliations": [
        "Newcastle University"
      ]
    },
    {
      "id": "https://openalex.org/A2111369431",
      "name": "Vishal Vohra",
      "affiliations": [
        "Sunderland Eye Infirmary"
      ]
    },
    {
      "id": "https://openalex.org/A4202677157",
      "name": "Mohaimen Al-Zubaidy",
      "affiliations": [
        "Royal Victoria Infirmary"
      ]
    },
    {
      "id": "https://openalex.org/A2931916413",
      "name": "Raffaele Raimondi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2989152130",
      "name": "Nikolaos Tzoumas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2892391799",
      "name": "Thomas Salisbury",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2479173406",
      "name": "Sandro Di Simplicio",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2499620760",
      "name": "Mario R. Romano",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2748936357",
      "name": "Tejaswi Bommireddy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2427522847",
      "name": "Harshika Chawla",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102673737",
      "name": "Yanmei Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2525084650",
      "name": "Sinéad Connolly",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4376708845",
      "name": "Samy El Omda",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2601997334",
      "name": "Melissa Gough",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A42348906",
      "name": "Lyudmila Kishikova",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1998917732",
      "name": "Thomas McNally",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4376708849",
      "name": "Salman N. Sadiq",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2330345051",
      "name": "Samuel A Simpson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2564544718",
      "name": "Boon Lin Teh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3091680709",
      "name": "Steven Toh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111369431",
      "name": "Vishal Vohra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202677157",
      "name": "Mohaimen Al-Zubaidy",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4323794409",
    "https://openalex.org/W4365143687",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4313451803",
    "https://openalex.org/W4319662928"
  ],
  "abstract": null,
  "full_text": "COMMENT OPEN\nComparative analysis of large language models in the Royal\nCollege of Ophthalmologists fellowship exams\nRaffaele Raimondi1, Nikolaos Tzoumas 1,2 ✉, Thomas Salisbury3, Sandro Di Simplicio 4, Mario R. Romano 5 and North East Trainee\nResearch in Ophthalmology Network (NETRiON)*\n© The Author(s) 2023\nEye (2023) 37:3530–3533; https://doi.org/10.1038/s41433-023-02563-3\nINTRODUCTION\nRecent years have witnessed an increasing interest in the\napplication of Arti ﬁcial Intelligence (AI) and deep learning in\nOphthalmology [1]. Large language models (LLMs) have become a\npopular area of research in thisﬁeld, and have been integrated\ninto publicly available chatbots such as ChatGPT 3.5 and 4.0\n(OpenAI, CA, US), Google Bard (Alphabet Inc., CA, US), and Bing\nChat (Microsoft Corporation, WA, US) [2– 5]. LLMs have been trained\non vast amounts of data, enabling them to generate human-like\ntext and answer complex questions. This capability has the\npotential to revolutionise clinical practice and assessment [2, 6, 7].\nWe evaluated the performance of LLM-driven AI chatbots on\nthe Fellowship of Royal College of Ophthalmologists (FRCOphth)\nexams required for autonomous Ophthalmology practice in the\nUK. We focused on testing the capability of these models in the\nPart 1 and Part 2 FRCOphth Written exams. These advanced\npostgraduate exams consist of multiple-choice questions and\ncover the learning outcomes of the Ophthalmology Specialty\nTraining curriculum in theﬁrst two years of training and towards\nthe end of training, respectively.\nMETHODS\nWe obtained sample multiple-choice questions from the Royal\nCollege of Ophthalmologists website, covering both the Part 1 and\nPart 2 examinations [8, 9]. We excluded image-based questions,\nresulting in 48 Part 1 and 43 Part 2 questions, categorised\naccording to their topics. Specialty trainees who had recently\npassed the exams rated the difﬁculty of each question on a scale\nof 1– 5, with 1 being“not at all difﬁcult” and 5 being“extremely\ndifﬁcult” (Supplementary Materials). The mean difﬁculty score was\nconsistent across all respondents.\nWe tested each LLM-chatbot three times on the sample questions\nat different timepoints. Additionally, for Part 2 questions, we\nevaluated ChatGPT-4.0 using various prompting strategies, such as\nasking the chatbot to answer the question from the perspective of a\npharmacist or statistician. When the LLM-chatbot could not answer\nthe question, it was recorded as incorrect. We did not provide\nadditional instruction or training data.\nWe analysed the association between accuracy and\nLLM-chatbot using Chi-squared testing and multilevel (mixed\neffect) logistic regressions. Difﬁculty and topic were included as\nﬁxed effects, and question ID as a random effect. We selected\nthe models with the lowest Akaike information criterion. Part 1\nand Part 2 data were analysed separately. All statistical analyses\nwere conducted in R.\nRESULTS\nThe LLM-chatbots achieved overall accuracies of 65.5% and 67.6%\nfor Part 1 and Part 2 questions, respectively (Fig.1). ChatGPT-3.5,\nGoogle Bard, and Bing Chat had respective accuracies of 55.1%\nand 49.6%, 62.6% and 51.9%, and 78.9% and 82.9% on the sample\nquestions. ChatGPT-4.0 achieved an accuracy of 79.1% on Part 2\nquestions, which increased to 88.4% with prompting. Signiﬁcant\ndifferences in accuracy were observed between the LLM-chatbots\non both question sets (Chi-squared testsP < 0.001). Despite a 4%\nmean difference in accuracy with each iteration, no statistically\nsigniﬁcant differences in performance were observed for any\nindividual LLM-chatbot.\nOn multilevel testing, Bing Chat outperformed ChatGPT-3.5 (OR\n6.37, 95% CI 3.16– 12.83, P < 0.001) and Google Bard (OR 3.73, 95%\nCI 1.88 – 7.37, P < 0.001) in Part 1 questions. No signi ﬁcant\nassociations were found between accuracy and question difﬁculty\nor topic. In Part 2 questions, ChatGPT-3.5 ’s performance was\nsurpassed by both ChatGPT-4.0 and Bing Chat, regardless of\nwhether prompting was used or not (Table1). LLM accuracy was\nsigniﬁcantly higher for questions on the“Cornea & External Eye”\ntopic (Table 1). However, we found no other signiﬁcant associa-\ntions between LLM-chatbot accuracy and other covariates.\nDISCUSSION\nThis study is theﬁrst to demonstrate that publicly available LLM-\ndriven chatbots can consistently provide accurate responses to\npostgraduate Ophthalmology specialty examinations, achieving\nan impressive accuracy of up to 82.9% without prompting or\ninstruction tuning. This performance was independent of question\ntopic and difﬁculty. Notably, most LLMs performed well enough to\npass the high standards of these exams, which typically require a\nscore of between 58% and 66% [10, 11]. Previous reports have\nshown that LLMs can achieve accuracies of up to 67.6% in\nReceived: 14 April 2023 Revised: 19 April 2023 Accepted: 24 April 2023\nPublished online: 9 May 2023\n1Sunderland Eye Inﬁrmary, Sunderland, UK.2Biosciences Institute, Newcastle University, Newcastle upon Tyne, UK.3Department of Ophthalmology, The James Cook University\nHospital, Middlesbrough, UK.4Newcastle Eye Centre, Royal Victoria Inﬁrmary, Newcastle upon Tyne, UK.5Department of Biomedical Science, Humanitas University, Milan, Italy.\n*A list of authors and their afﬁliations appears at the end of the paper.✉email: nik.tzoumas@ncl.ac.uk\nwww.nature.com/eye\n1234567890();,:\ngeneralist medical examinations with the use of different training\ndata and instruction prompt tuning [7, 12].\nWe observed variation in the accuracy of responses between\nLLM-chatbots (Fig. 1), but each consistently provided similar\naccuracy with each iteration. Curated prompting strategies\nenhanced performance. LLMs demonstrated equal proﬁciency in\nanswering basic science and clinical questions and performed\nsimilarly across difﬁculties and topics, except for Part 2 Cornea/\nExternal Eye questions, answered correctly 96% of the time\n(Table 1). This may reﬂect the use of different training data by\nLLMs, as our analyses accounted for question dif ﬁculty and\ncharacteristics. Limited of ﬁcially-available questions precluded\ndeﬁnitive topic-based comparisons (Supplementary Materials).\nOur study has broad implications for theﬁeld of Ophthalmol-\nogy, where large-scale medical AI models are being developed to\naid clinical decision-making through free-text explanations,\nspoken recommendations, or image annotations [2]. LLMs out-\nperformed our specialist examinations, raising questions about the\nadequacy of traditional assessments in measuring clinical\ncompetence. Alternative assessment methods, such as simulations\nor objective structured clinical examinations, may be needed to\nbetter capture the multifaceted skills and knowledge required for\nclinical practice.\nMedical AI technology has great potential, but it also poses\nlimitations and challenges. Clinicians may hold the AI system to a\nhigh standard of accuracy, creating barriers to effective human-\nmachine collaboration. Responsibility for the answers generated\nby these technologies in a clinical setting is unclear; our testing\nrevealed that LLMs could provide incorrect explanations and\nanswers without the ability to recognise their own limitations [6].\nAdditionally, the use of LLMs for clinical purposes is restricted by\ninherent biases in data and algorithms used, raising major\nFig. 1 Performance of LLM-chatbots on FRCOphth examinations.\nThe chart displays the average scores obtained by the LLM-chatbots\non Part 1 (left) and Part 2 (right) FRCOphth written examinations.\nThe x-axis denotes the name of the LLM-chatbots, while they-axis\nrepresents the average scores.\nTable 1. Comparing the accuracy of responses to FRCOphth Part 2 written questions with different LLM-chatbots.\nCovariate Levels Inaccurate\nresponse\nAccurate\nresponse\nUnivariable OR Multilevel OR\nLLM-chatbot ChatGPT-3.5 65 (50.4) 64 (49.6) - -\nGoogle Bard 62 (48.1) 67 (51.9) 1.10 (0.67 –1.79, p =0.71) 1.16 (0.63 –2.14, p = 0.64)\nBing Chat 22 (17.1) 107 (82.9) 4.94 (2.82 –8.92,\np < 0.001)***\n11.90 (5.54–25.53,\np < 0.001)***\nChatGPT-4.0 27 (20.9) 102 (79.1) 3.84 (2.24 –6.71,\np < 0.001)***\n8.10 (3.95–16.62,\np < 0.001)***\nChatGPT-4.0\nprompted\n5 (11.6) 38 (88.4) 7.72 (3.10 –23.52,\np < 0.001)***\n23.36 (6.51–83.80,\np < 0.001)***\nDifﬁculty Mean (SD) 2.8 (0.8) 2.4 (0.8) 0.60 (0.48 –0.75,\np < 0.001)***\n0.52 (0.21–1.25, p = 0.14)\nTopica Investigations 26 (40.0) 39 (60.0) - -\nTrauma 16 (61.5) 10 (38.5) 0.42 (0.16 –1.05, p =0.07) 0.11 (0.01 –1.92, p = 0.13)\nOculoplastic & Orbit 13 (50.0) 13 (50.0) 0.67 (0.26 –1.67, p =0.39) 0.39 (0.03 –5.40, p = 0.48)\nGlaucoma 24 (61.5) 15 (38.5) 0.42 (0.18 –0.93,\np = 0.035)*\n0.13 (0.01–1.41, p = 0.09)\nStrabismus 10 (38.5) 16 (61.5) 1.07 (0.42 –2.77, p =0.89) 0.68 (0.05 –9.31, p = 0.77)\nPaediatrics 7 (26.9) 19 (73.1) 1.81 (0.69 –5.19, p =0.24) 3.75 (0.23 –60.75, p = 0.35)\nRetina 23 (29.5) 55 (70.5) 1.59 (0.80 –3.21, p =0.19) 1.37 (0.19 –10.03, p = 0.76)\nCataract 5 (12.8) 34 (87.2) 4.53 (1.68 –14.57,\np = 0.005)**\n2.66 (0.16–45.68, p = 0.50)\nCornea & External Eye 2 (3.8) 50 (96.2) 16.67 (4.60 –107.44,\np < 0.001)***\n23.55 (1.42–390.93,\np = 0.028)*\nUveitis & Oncology 10 (25.6) 29 (74.4) 1.93 (0.82 –4.79, p =0.14) 2.22 (0.23 –21.91, p = 0.49)\nNeurology 13 (25.0) 39 (75.0) 2.00 (0.91 –4.55, p =0.09) 1.59 (0.16 –15.53, p = 0.69)\nGenetics 6 (46.2) 7 (53.8) 0.78 (0.23 –2.66, p =0.68) 1.70 (0.05 –61.67, p = 0.77)\nPharmacology 12 (46.2) 14 (53.8) 0.78 (0.31 –1.96, p =0.59) 0.89 (0.06 –12.26, p = 0.93)\nMiscellaneous 14 (26.9) 38 (73.1) 1.81 (0.83 –4.05, p =0.14) 1.42 (0.16 –12.53, p = 0.75)\naEffect estimates of question topic on accurate responses, adjusted for LLM-chatbot and difﬁculty, compared to the reference topic of“Investigations”.\nSigniﬁcant differences are indicated by * forP < 0.05, ** forP < 0.01, and *** forP < 0.001. For the performance of individual LLM-chatbots on different topics\nplease see Supplemental Table 2.\nLLM Large language model,OR Odds ratio, SD Standard deviation.\nR. Raimondi et al.\n3531\nEye (2023) 37:3530 – 3533\nconcerns [2, 6]. Ensuring the explainability of AI systems is a\npotential solution to this problem, and an interesting research\ntopic. Issues related to validation, computational expenses, data\nprocurement, and accessibility must also be addressed [2].\nAI systems will become increasingly integrated into online\nlearning and clinical practice, highlighting the need for ophthal-\nmologists to develop AI literacy. Future research should focus on\nbuilding open-access LLMs trained speci ﬁcally with truthful\nOphthalmology data to improve accuracy and reliability. Overall,\nLLMs offer signi ﬁcant opportunities to advance ophthalmic\neducation and care.\nSUMMARY\nWhat was known before\n● Large-scale medical AI models such as Large Language Models\n(LLMs) are being developed to aid clinical decision-making\nthrough free-text explanations, spoken recommendations, or\nimage annotations.\n● Previous studies have shown that LLMs can achieve accuracies\nof up to 67.6% in generalist medical examinations using\ndifferent training data and instruction prompt tuning.\nWhat this study adds\n● This study is the ﬁrst to demonstrate that LLMs can\nconsistently provide accurate responses to postgraduate\nOphthalmology specialty examinations, achieving an impress-\nive accuracy rate of up to 82.9% without prompting or\ninstruction tuning.\n● LLMs outperformed the standards of these specialist examina-\ntions, indicating that traditional assessments may not\nadequately measure clinical competence.\n● Issues related to validation, computational expenses, data\nprocurement, and accessibility must be addressed to ensure\nthe safe and effective integration of AI systems into online\nlearning and clinical practice.\nDATA AVAILABILITY\nSummative data generated or analysed during this study are included in this\npublished article and its supplementary materials. FRCOphth Part 1 and Part 2\nWritten examination sample questions are freely available online. Dif ﬁculty\nmeasurements for individual FRCOphth examination questions are available on\nrequest.\nREFERENCES\n1. Jiang X, Xie M, Ma L, Dong L, Li D. International publication trends in the\napplication of arti ﬁcial intelligence in ophthalmology research: An updated\nbibliometric analysis. Ann Transl Med. 2023;11:219.\n2. Moor M, Banerjee O, Abad ZSH, Krumholz HM, Leskovec J, Topol EJ, et al.\nFoundation models for generalist medical arti ﬁcial intelligence. Nature.\n2023;616:259– 65.\n3. OpenAI ChatGPT (Mar 13 version) [Large language model] Available at:https://\nopenai.com/blog/chatgpt. [Accessed April 13, 2023].\n4. Bard, an experiment by Google (Mar 21 version). Available at: https://\nbard.google.com/. [Accessed April 13, 2023].\n5. Microsoft Bing Chat (Feb 7 version). Available at: https://www.bing.com/new.\n[Accessed April 13, 2023].\n6. Sallam M. ChatGPT utility in healthcare education, research, and practice: Sys-\ntematic review on the promising perspectives and valid concerns. Healthc (Basel,\nSwitz). 2023;11:887.\n7. Kung TH, Cheatham M, Medenilla A, Sillos C, De Leon L, Elepaño C, et al. Per-\nformance of ChatGPT on USMLE: Potential for AI-assisted medical education\nusing large language models. PLOS Digit Heal. 2023;2:e0000198.\n8. The Royal College of Ophthalmologists (2016). Part 1 FRCOphth Sample MCQs.\nAvailable at: https://www.rcophth.ac.uk/wp-content/uploads/2022/01/Part-1-\nFRCOphth-Sample-MCQs.pdf. [Accessed April 13, 2023].\n9. The Royal College of Ophthalmologists (2016). Part 2 FRCOphth Written Sample\nMCQs. Available at: https://www.rcophth.ac.uk/wp-content/uploads/2022/01/\nPart-2-FRCOphth-Written-Sample-MCQs-20160524.pdf. [Accessed April 13, 2023].\n10. Turner M, Budzynski D, Smith B. Examination Report Part 1 Fellowship of the\nRoyal College of Ophthalmologists (FRCOphth) Examination. (2020). Available at:\nhttps://www.rcophth.ac.uk/wp-content/uploads/2021/12/Part-2-Written-Report-\nJuly-2021.pdf. [Accessed April 13, 2023].\n11. Budzynski D, Turner M, Smith B. Examination Report and Part 2 Fellowship of the\nRoyal College of Ophthalmologists (FRCOphth) written examination. (2021).\nAvailable at: https://www.rcophth.ac.uk/wp-content/uploads/2020/10/Part-1-\nFRCOphth-Exam-Report_Jan2020.docx. [Accessed April 13, 2023].\n12. Singhal K, Azizi S, Tu T, Mahdavi SS, Wei J, Chung HW, et al. Large Language\nModels Encode Clinical Knowledge. (2022). Preprint at https://arxiv.org/abs/\n2212.13138.\nAUTHOR CONTRIBUTIONS\nRR and NT designed and directed the project; TS developed the theoretical framework;\nRR, NT, and TS planned and carried out the simulations; NETRiON members provided\nthe question difﬁculty measurements required for the suggested experiment; NT\nprocessed the experimental data, derived the models, and analysed the data; SDS and\nMR aided in interpreting the results and proposed further computations; RR and NT\nwrote the manuscript in consultation with TS, SDS, MR, and NETRiON members. All\nauthors discussed the results and contributed to theﬁnal manuscript.\nFUNDING\nNT is supported by a National Institute for Health and Care Research (NIHR) Academic\nClinical Fellowship (ACF-2021-01-008).\nCOMPETING INTERESTS\nThe authors declare no competing interests.\nADDITIONAL INFORMATION\nSupplementary information The online version contains supplementary material\navailable at https://doi.org/10.1038/s41433-023-02563-3.\nCorrespondence and requests for materials should be addressed to Nikolaos\nTzoumas.\nReprints and permission information is available at http://www.nature.com/\nreprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims\nin published maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons license, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly\nfrom the copyright holder. To view a copy of this license, visit http://\ncreativecommons.org/licenses/by/4.0/.\n© The Author(s) 2023\nR. Raimondi et al.\n3532\nEye (2023) 37:3530 – 3533\nNORTH EAST TRAINEE RESEARCH IN OPHTHALMOLOGY NETWORK (NETRION)\nTejaswi Bommireddy4, Harshika Chawla4, Yanmei Chen1, Sinéad Connolly4, Samy El Omda2, Melissa Gough2, Lyudmila Kishikova4,\nThomas McNally4, Salman N. Sadiq1, Thomas Salisbury3, Samuel Simpson6, Boon Lin Teh4, Steven Toh2, Nikolaos Tzoumas 1,2 ✉,\nVishal Vohra1 and Mohaimen Al-Zubaidy4\n6Cumberland Inﬁrmary, Carlisle, UK.\nR. Raimondi et al.\n3533\nEye (2023) 37:3530 – 3533",
  "topic": "Medicine",
  "concepts": [
    {
      "name": "Medicine",
      "score": 0.6970255970954895
    },
    {
      "name": "Optometry",
      "score": 0.5896097421646118
    },
    {
      "name": "Ophthalmology",
      "score": 0.5669761896133423
    },
    {
      "name": "MEDLINE",
      "score": 0.4103374183177948
    },
    {
      "name": "Family medicine",
      "score": 0.3520040512084961
    },
    {
      "name": "Political science",
      "score": 0.10239115357398987
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210089629",
      "name": "Sunderland Eye Infirmary",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I2801184099",
      "name": "James Cook University Hospital",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I2800516725",
      "name": "Royal Victoria Infirmary",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I1339471597",
      "name": "Humanitas University",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I84884186",
      "name": "Newcastle University",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I2802416069",
      "name": "Cumberland Infirmary",
      "country": "GB"
    }
  ],
  "cited_by": 78
}