{
  "title": "Deepening Hidden Representations from Pre-trained Language Models",
  "url": "https://openalex.org/W3023676446",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5089183792",
      "name": "Junjie Yang",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A5036050911",
      "name": "Hai Zhao",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2806120502",
    "https://openalex.org/W2294370754",
    "https://openalex.org/W2144578941",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2109553965",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W2913340405",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2396767181",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2525127255",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W3006647218",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2998230451",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2888329843",
    "https://openalex.org/W2904631866",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2970597249"
  ],
  "abstract": "Transformer-based pre-trained language models have proven to be effective for learning contextualized language representation. However, current approaches only take advantage of the output of the encoder's final layer when fine-tuning the downstream tasks. We argue that only taking single layer's output restricts the power of pre-trained representation. Thus we deepen the representation learned by the model by fusing the hidden representation in terms of an explicit HIdden Representation Extractor (HIRE), which automatically absorbs the complementary representation with respect to the output from the final layer. Utilizing RoBERTa as the backbone encoder, our proposed improvement over the pre-trained models is shown effective on multiple natural language understanding tasks and help our model rival with the state-of-the-art models on the GLUE benchmark.",
  "full_text": "Deepening Hidden Representations from Pre-trained Language Models\nJunjie Yang1,2,3, Hai Zhao2,3,4∗\n,\n1SJTU-ParisTech Elite Institute of Technology, Shanghai Jiao Tong University, Shanghai, China\n2Department of Computer Science and Engineering, Shanghai Jiao Tong University\n3Key Laboratory of Shanghai Education Commission for Intelligent Interaction\nand Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China\n4MoE Key Lab of Artiﬁcial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China\njj-yang@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn\nAbstract\nTransformer-based pre-trained language mod-\nels have proven to be effective for learning\ncontextualized language representation. How-\never, current approaches only take advantage\nof the output of the encoder’s ﬁnal layer when\nﬁne-tuning the downstream tasks. We argue\nthat only taking single layer’s output restricts\nthe power of pre-trained representation. Thus\nwe deepen the representation learned by the\nmodel by fusing the hidden representation in\nterms of an explicit HIdden Representation Ex-\ntractor (HIRE), which automatically absorbs\nthe complementary representation with respect\nto the output from the ﬁnal layer. Utilizing\nRoBERTa as the backbone encoder, our pro-\nposed improvement over the pre-trained mod-\nels is shown effective on multiple natural lan-\nguage understanding tasks and help our model\nrival with the state-of-the-art models on the\nGLUE benchmark.\n1 Introduction\nLanguage representation is essential to the under-\nstanding of text. Recently, pre-trained language\nmodels based on Transformer (Vaswani et al., 2017)\nsuch as GPT (Radford et al., 2018), BERT (De-\nvlin et al., 2019), XLNet (Yang et al., 2019), and\nRoBERTa (Liu et al., 2019c) have been shown to\nbe effective for learning contextualized language\nrepresentation. These models have since continued\nto achieve new state-of-the-art results on a variety\nof natural language processing tasks. They include\nquestion answering (Rajpurkar et al., 2018; Lai\net al., 2017), natural language inference (Williams\net al., 2018; Bowman et al., 2015), named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003), sentiment analysis (Socher et al., 2013) and\n∗Corresponding author. This paper was partially supported\nby National Key Research and Development Program of China\n(No. 2017YFB0304100) and Key Projects of National Natural\nScience Foundation of China (U1836222 and 61733011).\nsemantic textual similarity (Cer et al., 2017; Dolan\nand Brockett, 2005).\nNormally, Transformer-based models are pre-\ntrained on large-scale unlabeled corpus in an unsu-\npervised manner, and then ﬁne-tuned on the down-\nstream tasks through introducing task-speciﬁc out-\nput layer. When ﬁne-tuning on the supervised\ndownstream tasks, the models pass directly the out-\nput of Transformer encoder’s ﬁnal layer, which is\nconsidered as the contextualized representation of\ninput text, to the task-speciﬁc layer.\nHowever, due to the numerous layers (i.e., Trans-\nformer blocks) and considerable depth of these pre-\ntrained models, we argue that the output of the last\nlayer may not always be the best representation of\nthe input text during the ﬁne-tuning for downstream\ntasks. Devlin et al. (2019) shows diverse combina-\ntions of different layers’ outputs of the pre-trained\nBERT result in distinct performance on CoNNL-\n2003 Named Entity Recognition (NER) task (Tjong\nKim Sang and De Meulder, 2003). Peters et al.\n(2018b) points out for pre-trained language mod-\nels, including Transformer, the most transferable\ncontextualized representations of input text tend\nto occur in the middle layers, while the top layers\nspecialize for language modeling. Therefore, the\nonefold use of the last layer’s output may restrict\nthe power of the pre-trained representation.\nIn this paper, we propose an extra network com-\nponent design for Transformer-based model, which\nis capable of adaptively leveraging the hidden in-\nformation in the Transformer’s hidden layers to\nreﬁne the language representation. Our introduced\nadditional components include two main additional\ncomponents:\n1. HIdden Representation Extractor (HIRE) dy-\nnamically learns a complementary representa-\ntion which contains the information that the\nﬁnal layer’s output fails to capture.\narXiv:1911.01940v2  [cs.CL]  29 Apr 2020\n2. Fusion network integrates the hidden informa-\ntion extracted by the HIRE with Transformer\nﬁnal layer’s output through two steps of func-\ntionalities, leading to a reﬁned contextualized\nlanguage representation.\nTaking advantage of the robustness of RoBERTa\nby using it as our backbone Transformer-based en-\ncoder (Liu et al., 2019c), we conduct experiments\non GLUE benchmark (Wang et al., 2018), which\nconsists of nine Natural Language Understanding\n(NLU) tasks. With the help of HIRE, our model out-\nperforms the baseline on 5/9 of them and advances\nthe state-of-the-art on SST-2 dataset. Keeping the\nbackbone Transformer model unchanged on its ar-\nchitecture, pre-training procedure and training ob-\njectives, we get comparable performance with other\nstate-of-the-art models on the GLUE leaderboard,\nwhich veriﬁes the effectiveness of the proposed\nHIRE enhancement over Transformer model.\n2 Related Work\nTransformer model is empowered by self-attention\nmechanism and has been applied as an effective\nmodel architecture design in quite a lot of pre-\ntrained language models (Vaswani et al., 2017).\nOpenAI GPT (Radford et al., 2018) is the ﬁrst\nmodel that introduced Transformer architecture\ninto unsupervised pre-training. The model is pre-\ntrained on 12-layer left-to-right Transformer on\nBooksCorpus (Zhu et al., 2015) dataset. But in-\nstead of unidirectional training like GPT, BERT\n(Devlin et al., 2019) adopts Masked LM objec-\ntive when pre-training, which enables the represen-\ntation to incorporate context from both direction.\nThe next sentence prediction (NSP) objective is\nalso used by BERT to better model the relation-\nship between sentences. The training of BERT is\nconducted on a combination of BooksCorpus plus\nEnglish Wikipedia. XLNet (Yang et al., 2019),\nas a generalized autoregressive language model,\nuses a permutation language modeling objective\nduring pre-training on the other hand. In addition\nto BooksCorpus and English Wikipedia, its train-\ning corpus also includes Giga5, ClueWeb 2012-\nB and Common Crawl. Trained with dynamic\nmasking, large mini-batches and a larger byte-\nlevel BPE, full-sentences without NSP, RoBERTa\n(Liu et al., 2019c) improves BERT’s performance\non the downstream tasks from a better BERT\nre-implementation training on BooksCorpus, CC-\nNews, Openwebtext and Stories.\nIn terms of ﬁne-tuning on downstream tasks, all\nthese powerful Transformer-based models help var-\nious NLP tasks continuously achieve new state-of-\nthe-art results. Diverse new methods have been\nproposed recently for ﬁne-tuning the downstream\ntasks, including multi-task learning (Liu et al.,\n2019b), adversarial training ( ?) or incorporating\nsemantic information into language representation\n(Zhang et al., 2020).\nTraditionally, downstream tasks or back-end part\nof models take representations from the last layer\nof the pre-trained language models as the default\ninput. However, recent studies trigger researchers’\ninterest on the intermediate layers’ representation\nlearned by pre-trained models. Jawahar et al.\n(2019) show a rich hierarchy of linguistic infor-\nmation is encoded by BERT, with surface features,\nsyntactic features and semantic features lying from\nbottom to top in the layers. Tenney et al. (2019)\nﬁnd that using ELMo-style (Peters et al., 2018a)\nscalar mixing of layer activations, both deep Trans-\nformer models (BERT and GPT) gain a signiﬁcant\nperformance improvement on a novel edge prob-\ning task. They attribute this to the most relevant\ninformation being contained in intermediate lay-\ners. When studying the linguistic knowledge and\ntransferability of contextualized word representa-\ntions with a series of seventeen diverse probing\ntasks, Liu et al. (2019a) observe that Transformers\ntend to encode transferable features in their inter-\nmediate layers, aligned with the results by Peters\net al. (2018b). Zhu et al. (2018) adopt a linear\ncombination of embeddings from different layers\nin BERT to encode tokens for conversational ques-\ntion answering but in a feature-based way. So far,\nto our best knowledge, exploiting the representa-\ntions learned by hidden layers is still stuck with\nlimited empirical observations, which motivates us\nto propose a general solution for fully exploiting all\nlevels of representations learned by the pre-trained\nmodels.\n3 Model\n3.1 Transformer-based encoder\nTransformer-based encoder is responsible for en-\ncoding input text into contextualized representation.\nLet {w1,...,w n}represent a sequence of nwords\nof input text, Transformer-based encoder encodes\nthe input sequence into its universal contextualized\nrepresentation R∈Rn×d:\nR= Encoder({w1,...,w n}) (1)\n2-layer Bi-GRU\n• • •\n• • •\nSum Hadamard Product\n• • •\nw1 w2 w3 w4 • • • wn\n \nTask-speciﬁc output layer\nConcatenation\nLayer #n\nLayer #2\nLayer #1\n \n  \nTransformer-based\nEncoder\n HIRE\n can </s> ##ardGod\n   Fusion Network\n���\nFigure 1: Architecture of our model. HIRE denotes HIdden Representation Extractor, in which the Bi-GRUs share\nthe same parameters for each layer’s output.\nwhere dis the hidden size of the encoder and R\nis the output of Transformer-based encoder’s last\nlayer which has the same length as the input text.\nWe call it preliminary representation in this pa-\nper to distinguish it with the one that we introduce\nin section 3.2. Here, we omit a rather extensive\nformulations of Transformer and refer readers to\nVaswani et al. (2017), Radford et al. (2018) and\nDevlin et al. (2019) for more details.\n3.2 Hidden Representation Extractor\nTransformer-based encoder normally has many\nstructure-identical layers stacked together, for ex-\nample, BERTLARGE and XLNetLARGE all contain\n24 layers of the identical structure, either outputs\nfrom these hidden layers or the last layer, but not\nonly limited to the latter, may be extremely helpful\nfor speciﬁc downstream task. To make a full use of\nthe representations from these hidden layers, we in-\ntroduce an extra component attached to the original\nencoder, HIdden Representation Extractor (HIRE)\nto capture the complementary information that the\noutput of the last layer fails to capture. Since each\nlayer does not take the same importance to repre-\nsent a certain input sequence for different down-\nstream tasks, we design an adaptive mechanism\nthat can compute the importance dynamically. We\nmeasure the importance by an importance score.\nThe input to the HIRE is {H0,...,H j,...,H l}\nwhere lrepresents the number of layers in the en-\ncoder. Here H0 is the initial embedding of input\ntext, which is the input of the encoder’s ﬁrst layer\nbut is updated during training and Hj ∈Rn×d is\nthe hidden-state of the encoder at the output of\nlayer j. For the sake of simplicity, we call them all\nhidden-state afterwards.\nWe use the same 2-layer Bidirectional Gated\nRecurrent Unit (GRU) (Cho et al., 2014) to sum-\nmarize each hidden-state of the encoder. Instead\nof taking the whole output of GRU as the represen-\ntation of the hidden state, we concatenate GRU’s\neach layer and each direction’s ﬁnal state together.\nIn this way, we manage to summarize the hidden-\nstate into a ﬁxed-sized vector. Hence, we obtain\nU ∈R(l+1)×4d with Ui the summarized vector of\nHi:\nUi = Bi-GRU(Hi) ∈R4d (2)\nwhere 0 ≤i≤l. Then the importance value αi for\nhidden-state Hi is calculated by:\nαi = ReLU(WT Ui + b) ∈R (3)\nwhere WT ∈R4d and b∈R are trainable parame-\nters. Let αi={αi}be normalized into a probability\ndistribution Sthrough a softmax layer:\nS = softmax(α) ∈R(l+1) (4)\nwhere Si is the normalized weight of hidden-state i\nwhen computing the representation. Subsequently,\nwe obtain the input sequence’s new representation\nAby:\nA=\nl+1∑\ni=0\nSiHi ∈Rn×d (5)\nWith the same shape as the output of Transformer-\nbased encoder’s ﬁnal layer, HIRE’s output A is\nexpected to contain the additional useful informa-\ntion from the encoder’s hidden-states and we call it\ncomplementary representation.\n3.3 Fusion network\nThis module fuses the information contained in the\noutput of Tansformer-based encoder and the one\nextracted from encoders’ hidden states by HIRE.\nGiven the preliminary representation R, instead\nof letting it ﬂow directly into task-specﬁc output\nlayer, we combine it together with the complemen-\ntary representation Ato yeild M, deﬁned by:\nM = [R; A; R+ A; R◦A] ∈Rn×4d (6)\nwhere ◦is elementwise multiplication (Hadamard\nProduct) and [; ]is concatenation across the last\ndimension.\nLater, two-layer bidirectional GRU, with the out-\nput size of dfor each direction, is used to fully fuse\nthe information contained in the preliminary rep-\nresentation and the complementary representation.\nWe concatenate the outputs of the GRUs in two\ndimensions together for the ﬁnal contextualized\nrepresentation:\nF = Bi-GRU(M) ∈Rn×2d (7)\n3.4 Output layer\nThe output layer is task-speciﬁc. The following are\nthe concerned implementation details on two tasks,\nclassiﬁcation and regression.\nFor classiﬁcation task, given the input text’s con-\ntextualized representation F, following (Devlin\net al., 2019), we take the ﬁrst row C ∈R2d of\nF corresponding to the ﬁrst input token (<s> ) as\nthe aggregated representation. Let mbe the num-\nber of labels in the datasets, we pass C through a\nfeed-forward network (FFN):\nQ= WT\n2 ·tanh(WT\n1 C+ b1) +b2 ∈Rm (8)\nwith W1 ∈R2d×d, W2 ∈Rd×m, b1 ∈Rd and\nb2 ∈Rm the only parameters that we introduce in\noutput layer. Finally, the probability distribution of\npredicted label is computed as:\np= softmax(Q) ∈Rm (9)\nFor regression task, we obtain Qin the same man-\nner with m= 1, and take Qas the predicted value.\n3.5 Training\nFor classiﬁcation task, the training loss to be mini-\nmized is deﬁned by the Cross-Entropy:\nL(θ) =−1\nN\nN∑\ni\nm∑\nc\nyi,c log(pi,c) (10)\nwhere θis the set of all parameters in the model,\nN is the number of examples in the dataset, pi,c is\nthe predicted probability of class cfor example i\nand yis the binary indicator deﬁned as below:\nyi,c =\n\n\n\n1 if label cis the correct classiﬁcation\nfor example i\n0 otherwise\nFor regression task, we deﬁne the training loss\nby mean squared error (MSE):\nL(θ) =−1\nN\nN∑\ni\n(Qi −yi)2 (11)\nwhere Qi is the predicted value for example iand\nyi is the ground truth value for example i.\n4 Experiments\n4.1 Dataset\nWe conducted the experiments on the General\nLanguage Understanding Evaluation (GLUE)\nbenchmark (Wang et al., 2018) to evaluate our\nproposed method. GLUE is a collection of 9\ndiverse datasets for training, evaluating, and\nanalyzing natural language understanding models.\nThree different tasks are presented in GLUE\nbenchmark according to the original paper:\nSingle Sentence Similarity and Paraphrase Natural Language Inference\nModel CoLA SST-2 MRPC QQP STS-B MNLI-m/mm QNLI RTE\n(Mc) (Acc) (Acc) (Acc) (Pearson) (Acc) (Acc) (Acc)\nMT-DNN 63.5 94.3 87.5 91.9 90.7 87.1/86.7 92.9 83.4\nXLNET-large 69.0 97.0 90.8 92.3 92.5 90.8/90.8 94.9 85.9\nALBERT-xxlarge (1.5M) 71.4 96.9 90.9 92.2 93.0 90.8/- 95.3 89.2\nRoBERTa-large 68.0 96.4 90.9 92.2 92.4 90.2/90.2 94.7 86.6\nRoBERTa+HIRE 69.7 96.8 90.9 92.0 92.4 90.7/90.4 95.0 86.6\n(+1.7) (+ 0.4) - (- 0.2) - (+ 0.5/+0.2) (+ 0.3) -\nTable 1: GLUE Dev results. Our results are based on single model trained with single task and a median over ﬁve\nruns with different random seed but the same hyperparameter is reported for each task. The results of MT-DNN,\nXLNETLARGE, ALBERT and RoBERTa are from Liu et al. (2019b), Yang et al. (2019), Lan et al. (2020) and Liu\net al. (2019c). See the lower-most row for the performance of our approach. Mc, acc and pearson denote Matthews\ncorrelation, accuracy and Person correlation coefﬁcient respectively.\nSingle-sentence tasks: The Corpus of Linguistic\nAcceptability ( CoLA) (Warstadt et al., 2018)\nrequires the model to determine whether a\nsentence is grammatically acceptable; the Stanford\nSentiment Treebank (SST-2) (Socher et al., 2013)\nis to predict the sentiment of movie reviews with\nlabel of positive or negative.\nSimilarity and paraphrase tasks: Similarity\nand paraphrase tasks are to predict whether each\npair of sentences captures a paraphrase/semantic\nequivalence relationship. The Microsoft Research\nParaphrase Corpus (MRPC) (Dolan and Brockett,\n2005), the Quora Question Pairs (QQP) 1 and the\nSemantic Textual Similarity Benchmark (STS-B)\n(Cer et al., 2017) are presented in this category.\nNatural Language Inference (NLI) tasks:\nNatural language inference is the task of deter-\nmining whether a hypothesis is true (entailment),\nfalse (contradiction), or undetermined (neutral)\ngiven a premise. GLUE benchmark contains\nthe following tasks: the Multi-Genre Natural\nLanguage Inference Corpus ( MNLI) (Williams\net al., 2018), the converted version of the Stanford\nQuestion Answering Dataset (QNLI) (Rajpurkar\net al., 2016), the Recognizing Textual Entailment\n(RTE) (Dagan et al., 2006; Roy et al., 2006;\nGiampiccolo et al., 2007; Bentivogli et al., 2009)\nand the Winograd Schema Challenge ( WNLI)\n(Levesque et al., 2012).\nFour ofﬁcial metrics are adopted to evaluate\n1https://www.quora.com/q/quoradata/First-Quora-\nDataset-Release-Question-Pairs\nthe model performance: Matthews correlation\n(Matthews, 1975), accuracy, F1 score, Pearson and\nSpearman correlation coefﬁcients. More details\nwill be presented in section 4.3.\n4.2 Implementation\nOur implementation of HIRE and its related fusion\nnetwork is based on the PyTorch implementation\nof Transformer 2.\nPreprocessing: Following Liu et al. (2019c), we\nadopt GPT-2 (Radford et al., 2019) tokenizer\nwith a Byte-Pair Encoding (BPE) vocabulary of\nsubword units size 50K. The maximum length of\ninput sequence is 128 tokens.\nModel conﬁgurations: We use RoBERTaLARGE\nas the Transformer-based encoder and load the\npre-training weights of RoBERTa (Liu et al.,\n2019c). Like BERTLARGE, RoBERTaLARGE model\ncontains 24 Transformer-blocks, with the hidden\nsize being 1024 and the number of self-attention\nheads being 16 (Liu et al., 2019c; Devlin et al.,\n2019).\nOptimization: We use Adam optimizer (Kingma\nand Ba, 2015) with β1 = 0.9, β2 = 0.98 and\nϵ= 10−6 and the learning rate is selected amongst\n{1e-5, 2e-5}with a warmup rate of 0.06 depending\non the nature of the task. The number of training\nepochs ranges from 3 to 27 with the early stop and\nthe batch size is selected amongst {16, 32, 48}. In\naddition to that, we clip the gradient norm within 1\nto prevent exploding gradients problem occurring\n2https://github.com/huggingface/transformers\nSingle Sentence Similarity and Paraphrase Natural Language Inference\nModel CoLA SST-2 MRPC QQP STS-B MNLI-m/mm QNLI RTE WNLI Score\n8.5k 67k 3.7k 364k 7k 393k 108k 2.5k 634\nBERTa 60.5 94.9 89.3/85.4 72.1/89.3 87.6/86.5 86.7/85.9 92.7 70.1 65.1 80.5\nMT-DNNb 68.4 96.5 92.7/90.3 73.7/89.9 91.1/90.7 87.9/87.4 96.0 86.3 89.0 87.6\nFreeLB-RoBERTac 68.0 96.8 93.1/90.8 74.8/90.3 92.3/92.1 91.1/90.7 95.6 88.7 89.0 88.4\nXLNetd 70.2 97.1 92.9/90.5 74.7/90.4 93.0/92.6 90.9/90.9 - 88.5 92.5 -\nALBERTe 69.1 97.1 93.4/91.2 74.2/90.5 92.5/92.0 91.3/91.0 - 89.2 91.8 -\nRoBERTaf 67.8 96.7 92.3/89.8 74.3/90.2 92.2/91.9 90.8/90.2 95.4 88.2 89.0 88.1\nRoBERTa+HIRE68.6 97.1 93.0/90.7 74.3/90.2 92.4/92.0 90.7/90.4 95.5 87.9 89.0 88.3\n(+0.8) (+ 0.4) (+ 0.7/+0.9) - (+ 0.2/+0.1) (- 0.1/+0.2) (+ 0.1) (- 0.3) - (+ 0.2)\nTable 2: GLUE Test results, scored by the ofﬁcial evaluation server. All the results are obtained from GLUE leader-\nboard (https://gluebenchmark.com/leaderboard). The number below each task’s name indicates the\nsize of training dataset. Recently the GLUE benchmark has forbidden all the submissions to treat the QNLI as a\nranking task, which results in the missing of some models’ accuracies on the QNLI. aDevlin et al. (2019); bLiu\net al. (2019b); cZhu et al. (2020); dYang et al. (2019); eLan et al. (2020); f Liu et al. (2019c).\nin the recurrent neural networks in our model.\nRegularization: We employ two types of regular-\nization methods during training. We apply dropout\n(Srivastava et al., 2014) of rate 0.1 to all layers in\nthe Transformer-based encoder and GRUs in the\nHIRE and fusion network. We additionally adopt\nL2 weight decay of 0.1 during training.\n4.3 Main results\nTable 1 compares our method with a list of\nTransformer-based models on the development\nset. Model parameter comparison is shown in\nTable 3. To obtain a direct and fair comparison\nwith our baseline model RoBERTa, following the\noriginal paper (Liu et al., 2019c), we ﬁne-tune\nRoBERTa+HIRE separately for each of the GLUE\ntasks, using only task-speciﬁc training data. The\nsingle-model results for each task are reported. We\nrun our model with ﬁve different random seeds\nbut the same hyperparameters and take the median\nvalue. Due to the problematic nature of WNLI\ndataset, we exclude its results in this table. The re-\nsults shows that RoBERTa+HIRE consistently out-\nperforms RoBERTa on 4 of the GLUE task develop-\nModel Params Shared\n(M) (M)\nALBERT-xxlarge 235 -\nMT-DNN 350 340\nRoBERTa 355 -\nRoBERTa+HIRE 437 -\nTable 3: Parameter Comparison.\nment sets, with an improvement of 1.7 points, 0.4\npoints, 0.5/0.2 points, 0.3 points on CoLA, SST-2,\nMNLI and QNLI respectively. And on the MRPC,\nSTS-B and RTE task, our model get the same result\nas RoBERTa. It should be noted that the improve-\nment is entirely attributed to the introduction of\nHIdden Representation Extractor and fusion net-\nwork in our model.\nTable 2 presents the results of HIRE enhance-\nment and other models on the test set that have\nbeen submitted to the GLUE leaderboard. Fol-\nlowing Liu et al. (2019c), we ﬁne-tune STS-B\nand MRPC starting from the MNLI single-task\nmodel. Given the simplicity between RTE, WNLI\nand MNLI, and the large-scale nature of MNLI\ndataset (393k), we also initialize RoBERTa+HIRE\nwith the weights of MNLI single-task model be-\nfore ﬁne-tuning on RTE and WNLI. We sub-\nmitted the ensemble-model results to the leader-\nboard. The results show that RoBERTa+HIRE\nstill boosts the strong RoBERTa baseline model\non the test set. To be speciﬁc, RoBERTa+HIRE\noutperforms RoBERTa over CoLA, SST-2, MRPC,\nSST-B, MNLI-mm and QNLI with an improvement\nof 0.8 points, 0.4 points, 0.7/0.9 points, 0.2/0.1\npoints, 0.2 points and 0.1 points respectively. In\nthe meantime, RoBERTa+HIRE gets the same re-\nsults as RoBERTa on QQP and WNLI. By cat-\negory, RoBERTa+HIRE has better performance\nthan RoBERTa on the single sentence tasks, simi-\nlarity and paraphrase tasks. It is worth noting that\nour model obtains state-of-the-art results on SST-2\ndataset, with a score of 97.1. The results are quite\npromising since HIRE does not modify the encoder\ninternal architecture (Yang et al., 2019) or rede-\nﬁne the pre-training procedure (Liu et al., 2019c) ,\ngetting the comparable results with them.\n5 Ablation Study\nIn this section, we perform a set of ablation exper-\niments to understand the effects of our proposed\ntechniques during ﬁne-tuning. All the results re-\nported in this section are a median of ﬁve random\nruns.\n5.1 Model design consideration\nTo individually evaluate the importance of HIRE\nand fusion network, we vary our model in the fol-\nlowing ways and conduct the experiments on the\ndevelopment set of CoLA and STS-B. The results\nare in Table 4:\n•We remove the HIRE from our model and\npass the preliminary representation Rdirectly\ninto the fusion network. In order to keep the\nfusion network, we fuse the preliminary repre-\nsentation with itself, which means we deﬁne\ninstead M by:\nM = [R; R; R+ R; R◦R] ∈Rn×4d (12)\nThe results are presented in row 2.\n•We remove the fusion network, and take the\noutputs of HIRE as the ﬁnal representation of\nthe input and ﬂow it directly into the output\nlayer and present the results in row 3.\nAs can be seen from the table, to use of HIRE to ex-\ntract the hidden information is crucial to the model:\nMatthews correlation of CoLA and Pearson corre-\nlation coefﬁcient of STS-B drop dramatically by\n1.2 and 0.9 points if it’s removed. We also observe\nDev set\nModel CoLA STS-B\n(Mc) (Pearson)\nOur model 69.7 92.4\nw/o HIRE 68.5 91.5\nw/o Fusion network 68.2 92.2\nTable 4: Ablation study over model design considera-\ntion on the development set of CoLA and STS-B. The\nresult for each model is a median of ﬁve random runs.\nBoth HIRE and fusion network signiﬁcantly improve\nthe model performance on all two datasets.\nDev set\nMethod CoLA\n(Mc)\nHIRE (dynamic) 69.7\nmean, all 25 layers 68.9 (- 0.8)\nmean, last 6 layers 68.0 (- 1.7)\nmean, ﬁrst 6 layers 68.8 (- 0.9)\nrandom, all 25 layers 69.3 (- 0.4)\nrandom, last 6 layers 68.1 (- 1.6)\nTable 5: Effect of dynamic mechanism when comput-\ning the importance scores. A median Matthews corre-\nlation of ﬁve random runs is reported for CoLA on the\ndevelopment set.\nthat fusion network is an important component that\ncontributes 1.5/0.2 gains on the CoLA and STS-B\ntasks. The results are aligned with our assumption\nthat HIRE extracts the complementary information\nfrom the hidden layers while the fusion network\nfuses it with the preliminary representation.\n5.2 Effect of dynamic mechanism\nWe investigate how the adaptive assignment mech-\nanism of the importance score affects the model’s\nperformance. CoLA is chosen as the downstream\ntask. Table 5 compares our proposed mechanism\nwith diverse ﬁxed combinations of importance\nscores over different range of layers: ﬁrst 6, last 6\nor all 25 layers. Two strategies are studied: mean\nand random. In the mean situation, we suppose all\nthe layers contribute exactly the same. On the con-\ntrary, under the random condition, we generate a\nscore for each layer randomly and do the SoftMax\noperation across all the layers.\nFrom the table, we observe that ﬁxing each\nlayer’s importance score for all examples hurts the\nmodel performance. Across all rows, we ﬁnd that\nnone of single strategy can yield the best perfor-\nmance. The results enable us to conclude that the\nhidden-state weighting mechanism introduced by\nHIRE may indeed adapt to diverse downstream\ntasks for better performance.\n5.3 Effect of GRU layer number in fusion\nnetwork\nTo investigate the effect of GRU layer number in\nthe fusion network, we set the GRU layer number\nfrom 0 to 3 and conduct the ablation experiments on\nthe development dataset of CoLA. Table 6 shows\nNumber 0 1 2 3\nMc 68.5 69.3 69.7 68.5\nTable 6: Ablation study over GRU layer number in fu-\nsion network on the development set of CoLA. The re-\nsults are a median Matthews correlation of ﬁve random\nruns. The best result is in bold.\nthe results. We observe that the modest number 2\nwould be a better choice.\n6 Analysis\nFigure 2: Distribution of importance scores over differ-\nent layers when computing the complementary repre-\nsentation for various NLU tasks. The numbers on the\nabscissa axis indicate the corresponding layer with 0\nbeing the ﬁrst layer and 24 being the last layer.\nWe compare the importance score’s distribution\nof different NLU tasks. For each task, we run our\nbest single model over the development set and\nthe results are calculated by averaging the values\nacross all the examples within each dataset. The\nresults are presented in Figure 2. From the top to\nthe bottom of the heatmap, the results are placed\nin the following order: single-sentence tasks, sim-\nilarity and paraphrase tasks, and natural language\ninference tasks. From ﬁgure 2, we ﬁnd that the dis-\ntribution differs among the different tasks, which\ndemonstrates HIRE’s dynamic ability to adapt for\ndistinct tasks when computing the complimentary\nrepresentation. The most important contribution\noccurs below the ﬁnal layer for all the tasks except\nSTS-B and RTE. All layers have a close contribu-\ntion for STS-B and RTE task.\nFigure 3 presents the distribution of importance\nscores over different layers for each example of\nSST-2 dataset. The number on the ordinate axis de-\nnotes the index of the example. It shows that HIRE\ncan adapt not only distinct tasks but also the differ-\nent examples. In the meantime, we observe also\nthat even though there are subtle differences among\nthese examples, they follow certain same patterns\nwhen calculating the complementary representa-\ntion, for example, layers 21 and 22 contribute the\nmost for almost all the examples and also the lay-\ners around them. But the ﬁgure shows also that for\nsome examples, all layers contribute nearly equally.\nFigure 3: Distribution of importance scores over dif-\nferent layers for each example of SST-2 dataset. The\nnumber on the ordinate axis denotes the index of the\nexample.\n7 Conclusion\nThis paper presents HIdden Representation Extrac-\ntor (HIRE), a novel enhancement component that\nreﬁnes language representation by adaptively lever-\naging the Transformer-based model’s hidden layers.\nIn our proposed model design, HIRE dynamically\ngenerates complementary representation from all\nhidden layers other than that from the default last\nlayer. A lite fusion network then incorporates the\noutputs of HIRE into those of the original model.\nThe experimental results demonstrate the effective-\nness of reﬁned language representation for natural\nlanguage understanding. The analysis highlights\nthe distinct contribution of each layer’s output for\ndiverse tasks and different examples.\nReferences\nLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo\nGiampiccolo, and Bernardo Magnini. 2009. The\nﬁfth PASCAL recognizing textual entailment chal-\nlenge. In Proceedings of Text Analysis Conference\n(TAC 2009).\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 632–642, Lisbon, Portugal.\nDaniel Cer, Mona Diab, Eneko Agirre, I ˜nigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada.\nKyunghyun Cho, Bart van Merri ¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using RNN encoder–decoder\nfor statistical machine translation. In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 1724–\n1734, Doha, Qatar.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The PASCAL recognising textual entailment\nchallenge. In Machine Learning Challenges. Eval-\nuating Predictive Uncertainty, Visual Object Classi-\nﬁcation, and Recognising Tectual Entailment, pages\n177–190, Berlin, Heidelberg.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies (NAACL), Volume 1 (Long and Short Pa-\npers), pages 4171–4186, Minneapolis, Minnesota.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The third PASCAL recogniz-\ning textual entailment challenge. In Proceedings of\nthe ACL-PASCAL Workshop on Textual Entailment\nand Paraphrasing, pages 1–9, Prague.\nGanesh Jawahar, Beno ˆıt Sagot, and Djam ´e Seddah.\n2019. What does BERT learn about the structure of\nlanguage? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics\n(ACL), pages 3651–3657, Florence, Italy.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations\n(ICLR), San Diego, CA, USA.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. RACE: Large-scale ReAd-\ning comprehension dataset from examinations. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 785–794, Copenhagen, Denmark.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. In Inter-\nnational Conference on Learning Representations\n(ICLR).\nHector J. Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In\nProceedings of the Thirteenth International Confer-\nence on Principles of Knowledge Representation\nand Reasoning, KR’12, pages 552–561.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies (NAACL), Volume 1 (Long and\nShort Papers), pages 1073–1094, Minneapolis, Min-\nnesota.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019b. Multi-task deep neural networks\nfor natural language understanding. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics (ACL), pages 4487–4496,\nFlorence, Italy.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019c.\nRoBERTa: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nB.W. Matthews. 1975. Comparison of the predicted\nand observed secondary structure of T4 phage\nlysozyme. Biochimica et Biophysica Acta (BBA) -\nProtein Structure, 405(2):442 – 451.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018a. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies (NAACL), Volume 1 (Long Pa-\npers), pages 2227–2237, New Orleans, Louisiana.\nMatthew Peters, Mark Neumann, Luke Zettlemoyer,\nand Wen-tau Yih. 2018b. Dissecting contextual\nword embeddings: Architecture and representation.\nIn Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 1499–1509, Brussels, Belgium.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. Technical re-\nport, OpenAI.\nAlec Radford, Jeffrey Wu, Dario Amodei, Daniela\nAmodei, Jack Clark, Miles Brundage, and Ilya\nSutskever. 2019. Better language models and their\nimplications. Technical report, OpenAI.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for SQuAD. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (ACL) (Volume 2: Short Papers) , pages\n784–789, Melbourne, Australia.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 2383–\n2392, Austin, Texas.\nBarHaim Roy, Dagan Ido, Dolan Bill, Ferro Lisa, Gi-\nampiccolo Danilo, Magnini Bernardo, and Szpektor\nIdan. 2006. The second PASCAL recognising tex-\ntual entailment challenge. In In Proceedings of the\nSecond PASCAL Challenges Workshop on Recognis-\ning Textual Entailment, Venice, Italy.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1631–1642, Seattle, Washington,\nUSA.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overﬁtting. Journal of Machine Learning Re-\nsearch (JMLR), 15:1929–1958.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R. Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R. Bowman, Dipan-\njan Das, and Ellie Pavlick. 2019. What do you\nlearn from context? Probing for sentence structure\nin contextualized word representations. In 7th Inter-\nnational Conference on Learning Representations\n(ICLR), New Orleans, LA, USA.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natu-\nral Language Learning at HLT-NAACL 2003, pages\n142–147.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30 (NIPS), pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353–355, Brussels, Belgium.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2018. Neural network acceptability judgments.\narXiv preprint arXiv:1805.12471.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies (NAACL),\nVolume 1 (Long Papers), pages 1112–1122, New Or-\nleans, Louisiana.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural\nInformation Processing Systems 32 (NIPS) , pages\n5754–5764.\nZhuosheng Zhang, Yuwei Wu, Hai Zhao, Zuchao Li,\nShuailiang Zhang, Xi Zhou, and Xiang Zhou. 2020.\nSemantics-aware bert for language understanding.\nIn Thirty-Fourth AAAI Conference on Artiﬁcial In-\ntelligence (AAAI-2020).\nChen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Gold-\nstein, and Jingjing Liu. 2020. FreeLB: Enhanced\nAdversarial Training for Natural Language Under-\nstanding. In International Conference on Learning\nRepresentations (ICLR).\nChenguang Zhu, Michael Zeng, and Xuedong Huang.\n2018. SDNet: Contextualized attention-based\ndeep network for conversational question answering.\narXiv preprint arXiv:1812.03593.\nY . Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Ur-\ntasun, A. Torralba, and S. Fidler. 2015. Aligning\nbooks and movies: Towards story-like visual expla-\nnations by watching movies and reading books. In\n2015 IEEE International Conference on Computer\nVision (ICCV), pages 19–27.",
  "topic": "Representation (politics)",
  "concepts": [
    {
      "name": "Representation (politics)",
      "score": 0.7541308403015137
    },
    {
      "name": "Computer science",
      "score": 0.7498804330825806
    },
    {
      "name": "Transformer",
      "score": 0.7001949548721313
    },
    {
      "name": "Language model",
      "score": 0.657825231552124
    },
    {
      "name": "Extractor",
      "score": 0.6184125542640686
    },
    {
      "name": "Encoder",
      "score": 0.6183305978775024
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5811872482299805
    },
    {
      "name": "Layer (electronics)",
      "score": 0.5132608413696289
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4902823269367218
    },
    {
      "name": "Natural language understanding",
      "score": 0.48410260677337646
    },
    {
      "name": "Feature learning",
      "score": 0.4709385633468628
    },
    {
      "name": "Natural language processing",
      "score": 0.4271274209022522
    },
    {
      "name": "Natural language",
      "score": 0.37213391065597534
    },
    {
      "name": "Engineering",
      "score": 0.09361696243286133
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Process engineering",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    }
  ]
}