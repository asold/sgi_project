{
    "title": "Compressing Large-Scale Transformer-Based Models: A Case Study on BERT",
    "url": "https://openalex.org/W3007007518",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A4286933106",
            "name": "Ganesh, Prakhar",
            "affiliations": [
                "Advanced Digital Sciences Center"
            ]
        },
        {
            "id": "https://openalex.org/A2054682089",
            "name": "Chen Yao",
            "affiliations": [
                "Advanced Digital Sciences Center"
            ]
        },
        {
            "id": "https://openalex.org/A2350767375",
            "name": "Lou Xin",
            "affiliations": [
                "Advanced Digital Sciences Center"
            ]
        },
        {
            "id": "https://openalex.org/A2181476503",
            "name": "Khan, Mohammad Ali",
            "affiliations": [
                "Advanced Digital Sciences Center"
            ]
        },
        {
            "id": "https://openalex.org/A2095884620",
            "name": "Yang Yin",
            "affiliations": [
                "Hamad bin Khalifa University"
            ]
        },
        {
            "id": "https://openalex.org/A2115101224",
            "name": "Sajjad Hassan",
            "affiliations": [
                "Hamad bin Khalifa University"
            ]
        },
        {
            "id": "https://openalex.org/A4223231446",
            "name": "Nakov, Preslav",
            "affiliations": [
                "Hamad bin Khalifa University"
            ]
        },
        {
            "id": "https://openalex.org/A2075373126",
            "name": "Chen Deming",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A4284349445",
            "name": "Winslett, Marianne",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6775854423",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W3035408713",
        "https://openalex.org/W3034560159",
        "https://openalex.org/W6780482815",
        "https://openalex.org/W2783538964",
        "https://openalex.org/W6757750581",
        "https://openalex.org/W6787645895",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W2198098822",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W3085341943",
        "https://openalex.org/W6768080748",
        "https://openalex.org/W6775717222",
        "https://openalex.org/W3095319910",
        "https://openalex.org/W6751979845",
        "https://openalex.org/W3038012435",
        "https://openalex.org/W6779436764",
        "https://openalex.org/W6768429542",
        "https://openalex.org/W6775706467",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W6727208969",
        "https://openalex.org/W3105966348",
        "https://openalex.org/W3035281298",
        "https://openalex.org/W6771626834",
        "https://openalex.org/W6767535143",
        "https://openalex.org/W6768021236",
        "https://openalex.org/W3100980998",
        "https://openalex.org/W6784562325",
        "https://openalex.org/W6779124799",
        "https://openalex.org/W3106070274",
        "https://openalex.org/W6777001201",
        "https://openalex.org/W3035030897",
        "https://openalex.org/W6766673545",
        "https://openalex.org/W2251610689",
        "https://openalex.org/W3115511229",
        "https://openalex.org/W6762945437",
        "https://openalex.org/W3035317797",
        "https://openalex.org/W2888482885",
        "https://openalex.org/W6787874997",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W3099438082",
        "https://openalex.org/W3103754749",
        "https://openalex.org/W3011574394",
        "https://openalex.org/W6769627184",
        "https://openalex.org/W3106504817",
        "https://openalex.org/W2963323070",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W3118485687",
        "https://openalex.org/W6776129198",
        "https://openalex.org/W2998183051",
        "https://openalex.org/W6777017071",
        "https://openalex.org/W6767627205",
        "https://openalex.org/W6767997687",
        "https://openalex.org/W6772784952",
        "https://openalex.org/W2970454332",
        "https://openalex.org/W3101066076",
        "https://openalex.org/W3034457371",
        "https://openalex.org/W6793874019",
        "https://openalex.org/W2988022164",
        "https://openalex.org/W3021293129",
        "https://openalex.org/W6771251021",
        "https://openalex.org/W6782310929",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W2988533489",
        "https://openalex.org/W6773815586",
        "https://openalex.org/W3103412034",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W3035038672",
        "https://openalex.org/W3005444338",
        "https://openalex.org/W6763701032",
        "https://openalex.org/W3100985894",
        "https://openalex.org/W3177265267",
        "https://openalex.org/W6772230799",
        "https://openalex.org/W6779405553",
        "https://openalex.org/W6779313456",
        "https://openalex.org/W1566289585"
    ],
    "abstract": "Abstract Pre-trained Transformer-based models have achieved state-of-the-art performance for various Natural Language Processing (NLP) tasks. However, these models often have billions of parameters, and thus are too resource- hungry and computation-intensive to suit low- capability devices or applications with strict latency requirements. One potential remedy for this is model compression, which has attracted considerable research attention. Here, we summarize the research in compressing Transformers, focusing on the especially popular BERT model. In particular, we survey the state of the art in compression for BERT, we clarify the current best practices for compressing large-scale Transformer models, and we provide insights into the workings of various methods. Our categorization and analysis also shed light on promising future research directions for achieving lightweight, accurate, and generic NLP models.",
    "full_text": null
}