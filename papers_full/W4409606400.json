{
  "title": "Length Instruction Fine-Tuning with Chain-of-Thought (LIFT-COT): Enhancing Length Control and Reasoning in Edge-Deployed Large Language Models",
  "url": "https://openalex.org/W4409606400",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5077251979",
      "name": "Pinzhe Chen",
      "affiliations": [
        "Jiangsu University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2086285432",
      "name": "Zhen Li",
      "affiliations": [
        "Jiangsu University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5077251979",
      "name": "Pinzhe Chen",
      "affiliations": [
        "Jiangsu University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2086285432",
      "name": "Zhen Li",
      "affiliations": [
        "Jiangsu University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4390660356",
    "https://openalex.org/W4408696053",
    "https://openalex.org/W6852473727",
    "https://openalex.org/W4404179055",
    "https://openalex.org/W6852874933",
    "https://openalex.org/W4402670301",
    "https://openalex.org/W4366341216",
    "https://openalex.org/W4407036413",
    "https://openalex.org/W4393213364",
    "https://openalex.org/W3006948887",
    "https://openalex.org/W4402495272",
    "https://openalex.org/W4409210461",
    "https://openalex.org/W4389519928",
    "https://openalex.org/W4409118116",
    "https://openalex.org/W4386748059",
    "https://openalex.org/W6851275496",
    "https://openalex.org/W4401387006",
    "https://openalex.org/W4402393776",
    "https://openalex.org/W4362508231",
    "https://openalex.org/W4378468563"
  ],
  "abstract": "This paper investigates the effectiveness of Chain-of-Thought (COT) reasoning in addressing length bias when deploying Large Language Models (LLMs) on the device side within wireless network environments. By conducting systematic experiments on two representative datasets, we perform a comparative evaluation of LLM performance under standard prompting versus COT-augmented prompting, using two benchmark tests. The study emphasizes how the COT strategy regulates response length and improves compliance with predefined maximum length (max_len) constraints. Experimental results demonstrate that COT reasoning significantly mitigates length bias, enhances output length control, and improves overall performance. These findings contribute to the theoretical foundation of LLM deployment in edge computing scenarios and provide actionable insights for future research on resource-constrained language model applications.",
  "full_text": null,
  "topic": "Lift (data mining)",
  "concepts": [
    {
      "name": "Lift (data mining)",
      "score": 0.7063477039337158
    },
    {
      "name": "Enhanced Data Rates for GSM Evolution",
      "score": 0.6368832588195801
    },
    {
      "name": "Computer science",
      "score": 0.5166252255439758
    },
    {
      "name": "Chain (unit)",
      "score": 0.4861011505126953
    },
    {
      "name": "Control (management)",
      "score": 0.46031898260116577
    },
    {
      "name": "Artificial intelligence",
      "score": 0.27016496658325195
    },
    {
      "name": "Physics",
      "score": 0.14590510725975037
    },
    {
      "name": "Machine learning",
      "score": 0.0763498842716217
    },
    {
      "name": "Astronomy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210096899",
      "name": "Jiangsu University of Science and Technology",
      "country": "CN"
    }
  ]
}