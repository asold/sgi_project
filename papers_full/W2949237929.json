{
  "title": "Expoiting Syntactic Structure for Language Modeling",
  "url": "https://openalex.org/W2949237929",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4293488235",
      "name": "Chelba, Ciprian",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": null,
      "name": "Jelinek, Frederick",
      "affiliations": [
        "Johns Hopkins University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1606548921",
    "https://openalex.org/W1607229519",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2049633694",
    "https://openalex.org/W2110882317"
  ],
  "abstract": "The paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history, thus enabling the use of long distance dependencies. The model assigns probability to every joint sequence of words--binary-parse-structure with headword annotation and operates in a left-to-right manner --- therefore usable for automatic speech recognition. The model, its probabilistic parameterization, and a set of experiments meant to evaluate its predictive power are presented; an improvement over standard trigram modeling is achieved.",
  "full_text": "arXiv:cs/9811022v2  [cs.CL]  25 Jan 2000\nExploiting Syntactic Structure for Language Modeling\nCiprian Chelba and Frederick Jelinek\nCenter for Language and Speech Processing\nThe Johns Hopkins University, Barton Hall 320\n3400 N. Charles St., Baltimore, MD-21218, USA\n{chelba,jelinek}@jhu.edu\nAbstract\nThe paper presents a language model that devel-\nops syntactic structure and uses it to extract mean-\ningful information from the word history, thus en-\nabling the use of long distance dependencies. The\nmodel assigns probability to every joint sequence\nof words–binary-parse-structure with headword an-\nnotation and operates in a left-to-right manner —\ntherefore usable for automatic speech recognition.\nThe model, its probabilistic parameterization, and a\nset of experiments meant to evaluate its predictive\npower are presented; an improvement over standard\ntrigram modeling is achieved.\n1 Introduction\nThe main goal of the present work is to develop a lan-\nguage model that uses syntactic structure to model\nlong-distance dependencies. During the summer96\nDoD Workshop a similar attempt was made by the\ndependency modeling group. The model we present\nis closely related to the one investigated in (Chelba\net al., 1997), however diﬀerent in a few important\naspects:\n• our model operates in a left-to-right manner, al-\nlowing the decoding of word lattices, as opposed to\nthe one referred to previously, where only whole sen-\ntences could be processed, thus reducing its applica-\nbility to n-best list re-scoring; the syntactic structure\nis developed as a model component;\n• our model is a factored version of the one\nin (Chelba et al., 1997), thus enabling the calculation\nof the joint probability of words and parse structure;\nthis was not possible in the previous case due to the\nhuge computational complexity of the model.\nOur model develops syntactic structure incremen-\ntally while traversing the sentence from left to right.\nThis is the main diﬀerence between our approach\nand other approaches to statistical natural language\nparsing. Our parsing strategy is similar to the in-\ncremental syntax ones proposed relatively recently\nin the linguistic community (Philips, 1996). The\nprobabilistic model, its parameterization and a few\nexperiments that are meant to evaluate its potential\nfor speech recognition are presented.\nthe_DT   contract_NN  ended_VBD cents_NNS after\ncents_NP\nof_PP\nloss_NP\nloss_NP\nended_VP'\nwith_PP\ncontract_NP\nwith_IN a_DT   loss_NN   of_IN   7_CD\nFigure 1: Partial parse\n2 The Basic Idea and Terminology\nConsider predicting the word after in the sentence:\nthe contract ended with a loss of 7 cents\nafter trading as low as 89 cents.\nA 3-gram approach would predict after from\n(7, cents) whereas it is intuitively clear that the\nstrongest predictor would be ended which is outside\nthe reach of even 7-grams. Our assumption is that\nwhat enables humans to make a good prediction of\nafter is the syntactic structure in the past. The\nlinguistically correct partial parse of the word his-\ntory when predicting after is shown in Figure 1.\nThe word ended is called the headword of the con-\nstituent (ended (with (...))) and ended is an ex-\nposed headword when predicting after — topmost\nheadword in the largest constituent that contains it.\nThe syntactic structure in the past ﬁlters out irrel-\nevant words and points to the important ones, thus\nenabling the use of long distance information when\npredicting the next word.\nOur model will attempt to build the syntactic\nstructure incrementally while traversing the sen-\ntence left-to-right. The model will assign a probabil-\nity P (W, T ) to every sentence W with every possible\nPOStag assignment, binary branching parse, non-\nterminal label and headword annotation for every\nconstituent of T .\nLet W be a sentence of length n words to which\nwe have prepended <s> and appended </s> so\nthat w0 =<s> and wn+1 =</s>. Let Wk be the\nword k-preﬁx w0 . . . w k of the sentence and WkTk\n(<s>, SB)   .......   (w_p, t_p) (w_{p+1}, t_{p+1}) ........ (w_k, t_k) w_{k+1}.... </s>\nh_0 = (h_0.word, h_0.tag)h_{-1}h_{-m} = (<s>, SB)\nFigure 2: A word-parse k-preﬁx\n(<s>, SB)  (w_1, t_1)  ..................... (w_n, t_n) (</s>, SE)\n(</s>, TOP')\n(</s>, TOP)\nFigure 3: Complete parse\nthe word-parse k-preﬁx . To stress this point, a\nword-parse k-preﬁx contains — for a given parse\n— only those binary subtrees whose span is com-\npletely included in the word k-preﬁx, excluding\nw0 =<s>. Single words along with their POStag\ncan be regarded as root-only trees. Figure 2 shows\na word-parse k-preﬁx; h_0 .. h_{-m} are the ex-\nposed heads, each head being a pair(headword, non-\nterminal label), or (word, POStag) in the case of a\nroot-only tree. A complete parse — Figure 3 — is\nany binary parse of the\n(w1, t1) . . . (wn, tn) (</s>, SE) sequence with the\nrestriction that (</s>, TOP’) is the only allowed\nhead. Note that (( w1, t1) . . . (wn, tn)) needn’t be a\nconstituent, but for the parses where it is, there is\nno restriction on which of its words is the headword\nor what is the non-terminal label that accompanies\nthe headword.\nThe model will operate by means of three mod-\nules:\n• WORD-PREDICTOR predicts the next word\nwk+1 given the word-parse k-preﬁx and then passes\ncontrol to the TAGGER;\n• TAGGER predicts the POStag of the next word\ntk+1 given the word-parse k-preﬁx and the newly\npredicted word and then passes control to the\nPARSER;\n• PARSER grows the already existing binary\nbranching structure by repeatedly generating the\ntransitions:\n(unary, NTlabel), (adjoin-left, NTlabel) or\n(adjoin-right, NTlabel) until it passes control\nto the PREDICTOR by taking a null transition.\nNTlabel is the non-terminal label assigned to the\nnewly built constituent and {left,right} speciﬁes\nwhere the new headword is inherited from.\nThe operations performed by the PARSER are\nillustrated in Figures 4-6 and they ensure that all\npossible binary branching parses with all possible\n<s> <s>\nT_{-m}\nh_{-1} h_0h_{-2}\n......... \nh_{-1} h_0h_{-2}\nT_{-2} T_{-1} T_0......... \nFigure 4: Before an adjoin operation\n...............\nT'_0\nT_{-1} T_0<s> T'_{-1}<-T_{-2}\nh_{-1} h_0\nh'_{-1} = h_{-2}\nT'_{-m+1}<-<s>\nh'_0 = (h_{-1}.word, NTlabel)\nFigure 5: Result of adjoin-left under NTlabel\nheadword and non-terminal label assignments for\nthe w1 . . . w k word sequence can be generated. The\nfollowing algorithm formalizes the above description\nof the sequential generation of a sentence with a\ncomplete parse.\nTransition t; // a PARSER transition\npredict (<s>, SB);\ndo{\n//WORD-PREDICTOR and TAGGER\npredict (next_word, POStag);\n//PARSER\ndo{\nif(h_{-1}.word != <s>){\nif(h_0.word == </s>)\nt = (adjoin-right, TOP’);\nelse{\nif(h_0.tag == NTlabel)\nt = [(adjoin-{left,right}, NTlabel),\nnull];\nelse\nt = [(unary, NTlabel),\n(adjoin-{left,right}, NTlabel),\nnull];\n}\n}\nelse{\nif(h_0.tag == NTlabel)\nt = null;\nelse\nt = [(unary, NTlabel), null];\n}\n}while(t != null) //done PARSER\n}while(!(h_0.word==</s> && h_{-1}.word==<s>))\nt = (adjoin-right, TOP); //adjoin <s>_SB; DONE;\nThe unary transition is allowed only when the\nmost recent exposed head is a leaf of the tree —\na regular word along with its POStag — hence it\ncan be taken at most once at a given position in the\n............... T'_{-1}<-T_{-2} T_0\nh_0h_{-1}\n<s>\nT'_{-m+1}<-<s>\nh'_{-1}=h_{-2}\nT_{-1}\nh'_0 = (h_0.word, NTlabel)\nFigure 6: Result of adjoin-right under NTlabel\ninput word string. The second subtree in Figure 2\nprovides an example of a unary transition followed\nby a null transition.\nIt is easy to see that any given word sequence\nwith a possible parse and headword annotation is\ngenerated by a unique sequence of model actions.\nThis will prove very useful in initializing our model\nparameters from a treebank — see section 3.5.\n3 Probabilistic Model\nThe probability P (W, T ) of a word sequence W and\na complete parse T can be broken into:\nP (W, T ) =\n∏ n+1\nk=1 [ P (wk/Wk−1Tk−1) · P (tk/Wk−1Tk−1, wk) ·\nNk∏\ni=1\nP (pk\ni /Wk−1Tk−1, wk, tk, pk\n1 . . . p k\ni−1)](1)\nwhere:\n• Wk−1Tk−1 is the word-parse ( k − 1)-preﬁx\n• wk is the word predicted by WORD-PREDICTOR\n• tk is the tag assigned to wk by the TAGGER\n• Nk − 1 is the number of operations the PARSER\nexecutes before passing control to the WORD-\nPREDICTOR (the Nk-th operation at position k is\nthe null transition); Nk is a function of T\n• pk\ni denotes the i-th PARSER operation carried out\nat position k in the word string;\npk\n1 ∈ { (unary, NTlabel),\n(adjoin-left, NTlabel),\n(adjoin-right, NTlabel), null},\npk\ni ∈ { (adjoin-left, NTlabel),\n(adjoin-right, NTlabel)}, 1 < i < N k ,\npk\ni =null, i = Nk\nOur model is based on three probabilities:\nP (wk/Wk−1Tk−1) (2)\nP (tk/wk, Wk−1Tk−1) (3)\nP (pk\ni /wk, tk, Wk−1Tk−1, pk\n1 . . . p k\ni−1) (4)\nAs can be seen, ( wk, tk, Wk−1Tk−1, pk\n1 . . . p k\ni−1) is one\nof the Nk word-parse k-preﬁxes WkTk at position k\nin the sentence, i =\n1, Nk.\nTo ensure a proper probabilistic model (1) we\nhave to make sure that (2), (3) and (4) are well de-\nﬁned conditional probabilities and that the model\nhalts with probability one. Consequently, certain\nPARSER and WORD-PREDICTOR probabilities\nmust be given speciﬁc values:\n• P (null/WkTk) = 1, if h_{-1}.word = <s> and\nh_{0} ̸= (</s>, TOP’) — that is, before predicting\n</s> — ensures that (<s>, SB) is adjoined in the\nlast step of the parsing process;\n• P ((adjoin-right, TOP)/WkTk) = 1, if\nh_0 = (</s>, TOP’) and h_{-1}.word = <s>\nand\nP ((adjoin-right, TOP’)/WkTk) = 1, if\nh_0 = (</s>, TOP’) and h_{-1}.word ̸= <s>\nensure that the parse generated by our model is con-\nsistent with the deﬁnition of a complete parse;\n• P ((unary, NTlabel)/WkTk) = 0, if h_0.tag ̸=\nPOStag ensures correct treatment of unary produc-\ntions;\n• ∃ ǫ > 0, ∀Wk−1Tk−1, P (wk=</s>/Wk−1Tk−1) ≥ ǫ\nensures that the model halts with probability one.\nThe word-predictor model (2) predicts the next\nword based on the preceding 2 exposed heads, thus\nmaking the following equivalence classiﬁcation:\nP (wk/Wk−1Tk−1) = P (wk/h0, h−1)\nAfter experimenting with several equivalence clas-\nsiﬁcations of the word-parse preﬁx for the tagger\nmodel, the conditioning part of model (3) was re-\nduced to using the word to be tagged and the tags\nof the two most recent exposed heads:\nP (tk/wk, Wk−1Tk−1) = P (tk/wk, h0.tag, h−1.tag)\nModel (4) assigns probability to diﬀerent parses of\nthe word k-preﬁx by chaining the elementary oper-\nations described above. The workings of the parser\nmodule are similar to those of Spatter (Jelinek et al.,\n1994). The equivalence classiﬁcation of the WkTk\nword-parse we used for the parser model (4) was the\nsame as the one used in (Collins, 1996):\nP (pk\ni /WkTk) = P (pk\ni /h0, h−1)\nIt is worth noting that if the binary branching\nstructure developed by the parser were always right-\nbranching and we mapped the POStag and non-\nterminal label vocabularies to a single type then our\nmodel would be equivalent to a trigram language\nmodel.\n3.1 Modeling Tools\nAll model components — WORD-PREDICTOR,\nTAGGER, PARSER — are conditional probabilis-\ntic models of the type P (y/x1, x2, . . . , x n) where\ny, x1, x2, . . . , x n belong to a mixed bag of words,\nPOStags, non-terminal labels and parser operations\n(y only). For simplicity, the modeling method we\nchose was deleted interpolation among relative fre-\nquency estimates of diﬀerent orders fn(·) using a\nrecursive mixing scheme:\nP (y/x1, . . . , x n) =\nλ(x1, . . . , x n) · P (y/x1, . . . , x n−1) +\n(1 − λ(x1, . . . , x n)) · fn(y/x1, . . . , x n), (5)\nf−1(y) = uniform (vocabulary (y)) (6)\nAs can be seen, the context mixing scheme dis-\ncards items in the context in right-to-left order. The\nλ coeﬃcients are tied based on the range of the\ncount C(x1, . . . , x n). The approach is a standard\none which doesn’t require an extensive description\ngiven the literature available on it (Jelinek and Mer-\ncer, 1980).\n3.2 Search Strategy\nSince the number of parses for a given word preﬁx\nWk grows exponentially with k, |{Tk}| ∼ O(2k), the\nstate space of our model is huge even for relatively\nshort sentences so we had to use a search strategy\nthat prunes it. Our choice was a synchronous multi-\nstack search algorithm which is very similar to a\nbeam search.\nEach stack contains hypotheses — partial parses\n— that have been constructed by the same number of\npredictor and the same number of parser operations .\nThe hypotheses in each stack are ranked according\nto the ln( P (W, T )) score, highest on top. The width\nof the search is controlled by two parameters:\n• the maximum stack depth — the maximum num-\nber of hypotheses the stack can contain at any given\nstate;\n• log-probability threshold — the diﬀerence between\nthe log-probability score of the top-most hypothesis\nand the bottom-most hypothesis at any given state\nof the stack cannot be larger than a given threshold.\nFigure 7 shows schematically the operations asso-\nciated with the scanning of a new word wk+1. The\nabove pruning strategy proved to be insuﬃcient so\nwe chose to also discard all hypotheses whose score\nis more than the log-probability threshold below the\nscore of the topmost hypothesis. This additional\npruning step is performed after all hypotheses in\nstage k′ have been extended with the null parser\ntransition and thus prepared for scanning a new\nword.\n3.3 Word Level Perplexity\nThe conditional perplexity calculated by assigning\nto a whole sentence the probability:\nP (W/T ∗) =\nn∏\nk=0\nP (wk+1/WkT ∗\nk ), (7)\nwhere T ∗ = argmaxT P (W, T ), is not valid because\nit is not causal: when predicting wk+1 we use T ∗\nwhich was determined by looking at the entire sen-\ntence. To be able to compare the perplexity of our\n(k) (k') (k+1)\n0 parser 0 parser 0 parser\np parser op\n op\np parser op p parser op\np+1 parser p+1 parser p+1 parser \nP_k parser P_k parser P_k parser\nk+1 predict. k+1 predict.\nk+1 predict.\nk+1 predict.\nk+1 predict.\nk+1 predict.\nk+1 predict.\nk+1 predict.\nk+1 predict.\nk+1 predict.\nP_k+1parserP_k+1parser\nword predictor\nand tagger\nparser adjoin/unary  transitions\nnull parser transitions\n op\nk predict.\nk predict.\nk predict.\nk predict.\n op\nFigure 7: One search extension cycle\nmodel with that resulting from the standard tri-\ngram approach, we need to factor in the entropy of\nguessing the correct parse T ∗\nk before predicting wk+1,\nbased solely on the word preﬁx Wk.\nThe probability assignment for the word at posi-\ntion k + 1 in the input sentence is made using:\nP (wk+1/Wk) =∑\nTk∈Sk\nP (wk+1/WkTk) · ρ(Wk, Tk), (8)\nρ(Wk, Tk) = P (WkTk)/\n∑\nTk∈Sk\nP (WkTk) (9)\nwhich ensures a proper probability over strings W ∗,\nwhere Sk is the set of all parses present in our stacks\nat the current stage k.\nAnother possibility for evaluating the word level\nperplexity of our model is to approximate the prob-\nability of a whole sentence:\nP (W ) =\nN∑\nk=1\nP (W, T (k)) (10)\nwhere T (k) is one of the “N-best” — in the sense\ndeﬁned by our search — parses for W . This is a\ndeﬁcient probability assignment, however useful for\njustifying the model parameter re-estimation.\nThe two estimates (8) and (10) are both consistent\nin the sense that if the sums are carried over all\npossible parses we get the correct value for the word\nlevel perplexity of our model.\n3.4 Parameter Re-estimation\nThe major problem we face when trying to reesti-\nmate the model parameters is the huge state space of\nthe model and the fact that dynamic programming\ntechniques similar to those used in HMM parame-\nter re-estimation cannot be used with our model.\nOur solution is inspired by an HMM re-estimation\ntechnique that works on pruned — N-best — trel-\nlises(Byrne et al., 1998).\nLet ( W, T (k)), k = 1 . . . N be the set of hypothe-\nses that survived our pruning strategy until the end\nof the parsing process for sentence W . Each of\nthem was produced by a sequence of model actions,\nchained together as described in section 2; let us call\nthe sequence of model actions that produced a given\n(W, T ) the derivation(W, T ).\nLet an elementary event in the derivation(W, T )\nbe ( y(ml)\nl , x\n(ml)\nl ) where:\n• l is the index of the current model action;\n• ml is the model component — WORD-\nPREDICTOR, TAGGER, PARSER — that takes\naction number l in the derivation(W, T );\n• y(ml)\nl is the action taken at position l in the deriva-\ntion:\nif ml = WORD-PREDICTOR, then y(ml)\nl is a word;\nif ml = TAGGER, then y(ml)\nl is a POStag;\nif ml = PARSER, then y(ml)\nl is a parser-action;\n• x\n(ml)\nl is the context in which the above action was\ntaken:\nif ml = WORD-PREDICTOR or PARSER, then\nx\n(ml)\nl = ( h0.tag, h0.word, h−1.tag, h−1.word);\nif ml = TAGGER, then\nx(ml)\nl = (word-to-tag , h0.tag, h−1.tag).\nThe probability associated with each model ac-\ntion is determined as described in section 3.1, based\non counts C(m)(y(m), x\n(m)), one set for each model\ncomponent.\nAssuming that the deleted interpolation coeﬃ-\ncients and the count ranges used for tying them stay\nﬁxed, these counts are the only parameters to be\nre-estimated in an eventual re-estimation procedure;\nindeed, once a set of counts C(m)(y(m), x\n(m)) is spec-\niﬁed for a given model m, we can easily calculate:\n• the relative frequency estimates\nf(m)\nn (y(m)/x(m)\nn ) for all context orders\nn = 0 . . .maximum-order(model(m));\n• the count C(m)(x(m)\nn ) used for determining the\nλ(x(m)\nn ) value to be used with the order- n context\nx(m)\nn .\nThis is all we need for calculating the probability of\nan elementary event and then the probability of an\nentire derivation.\nOne training iteration of the re-estimation proce-\ndure we propose is described by the following algo-\nrithm:\nN-best parse development data; // counts.Ei\n// prepare counts.E(i+1)\nfor each model component c{\ngather_counts development model_c;\n}\nIn the parsing stage we retain for each “N-best” hy-\npothesis ( W, T (k)), k = 1 . . . N, only the quantity\nφ(W, T (k)) = P (W, T (k))/ ∑ N\nk=1 P (W, T (k))\nand its derivation(W, T (k)). We then scan all\nthe derivations in the “development set” and, for\neach occurrence of the elementary event ( y(m), x\n(m))\nin derivation(W, T (k)) we accumulate the value\nφ(W, T (k)) in the C(m)(y(m), x(m)) counter to be\nused in the next iteration.\nThe intuition behind this procedure is that\nφ(W, T (k)) is an approximation to the P (T (k)/W )\nprobability which places all its mass on the parses\nthat survived the parsing process; the above proce-\ndure simply accumulates the expected values of the\ncounts C(m)(y(m), x\n(m)) under the φ(W, T (k)) con-\nditional distribution. As explained previously, the\nC(m)(y(m), x\n(m)) counts are the parameters deﬁning\nour model, making our procedure similar to a rigor-\nous EM approach (Dempster et al., 1977).\nA particular — and very interesting — case is that\nof events which had count zero but get a non-zero\ncount in the next iteration, caused by the “N-best”\nnature of the re-estimation process. Consider a given\nsentence in our “development” set. The “N-best”\nderivations for this sentence are trajectories through\nthe state space of our model. They will change\nfrom one iteration to the other due to the smooth-\ning involved in the probability estimation and the\nchange of the parameters — event counts — deﬁn-\ning our model, thus allowing new events to appear\nand discarding others through purging low probabil-\nity events from the stacks. The higher the number\nof trajectories per sentence, the more dynamic this\nchange is expected to be.\nThe results we obtained are presented in the ex-\nperiments section. All the perplexity evaluations\nwere done using the left-to-right formula (8) (L2R-\nPPL) for which the perplexity on the “development\nset” is not guaranteed to decrease from one itera-\ntion to another. However, we believe that our re-\nestimation method should not increase the approxi-\nmation to perplexity based on (10) (SUM-PPL) —\nagain, on the “development set”; we rely on the con-\nsistency property outlined at the end of section 3.3\nto correlate the desired decrease in L2R-PPL with\nthat in SUM-PPL. No claim can be made about\nthe change in either L2R-PPL or SUM-PPL on test\ndata.\nZ\nZ'\nZ'\nZ'\nB\nZ\nZ'\nZ'\nZ'\nA\nY_1             Y_k                 Y_nY_1               Y_k                 Y_n\nFigure 8: Binarization schemes\n3.5 Initial Parameters\nEach model component — WORD-PREDICTOR,\nTAGGER, PARSER — is trained initially from a\nset of parsed sentences, after each parse tree ( W, T )\nundergoes:\n• headword percolation and binarization — see sec-\ntion 4;\n• decomposition into its derivation(W, T ).\nThen, separately for each m model component, we:\n• gather joint counts C(m)(y(m), x\n(m)) from the\nderivations that make up the “development data”\nusing φ(W, T ) = 1;\n• estimate the deleted interpolation coeﬃcients on\njoint counts gathered from “check data” using the\nEM algorithm.\nThese are the initial parameters used with the re-\nestimation procedure described in the previous sec-\ntion.\n4 Headword Percolation and\nBinarization\nIn order to get initial statistics for our model com-\nponents we needed to binarize the UPenn Tree-\nbank (Marcus et al., 1995) parse trees and perco-\nlate headwords. The procedure we used was to ﬁrst\npercolate headwords using a context-free (CF) rule-\nbased approach and then binarize the parses by us-\ning a rule-based approach again.\nThe headword of a phrase is the word that best\nrepresents the phrase, all the other words in the\nphrase being modiﬁers of the headword. Statisti-\ncally speaking, we were satisﬁed with the output\nof an enhanced version of the procedure described\nin (Collins, 1996) — also known under the name\n“Magerman & Black Headword Percolation Rules”.\nOnce the position of the headword within a con-\nstituent — equivalent with a CF production of the\ntype Z → Y1 . . . Yn , where Z, Y1, . . . Y n are non-\nterminal labels or POStags (only for Yi) — is iden-\ntiﬁed to be k, we binarize the constituent as follows:\ndepending on the Z identity, a ﬁxed rule is used\nto decide which of the two binarization schemes in\nFigure 8 to apply. The intermediate nodes created\nby the above binarization schemes receive the non-\nterminal label Z′.\n5 Experiments\nDue to the low speed of the parser — 200 wds/min\nfor stack depth 10 and log-probability threshold\n6.91 nats (1/1000) — we could carry out the re-\nestimation technique described in section 3.4 on only\n1 Mwds of training data. For convenience we chose\nto work on the UPenn Treebank corpus. The vocab-\nulary sizes were:\n• word vocabulary: 10k, open — all words outside\nthe vocabulary are mapped to the <unk> token;\n• POS tag vocabulary: 40, closed;\n• non-terminal tag vocabulary: 52, closed;\n• parser operation vocabulary: 107, closed;\nThe training data was split into “development” set\n— 929,564wds (sections 00-20) — and “check set”\n— 73,760wds (sections 21-22); the test set size was\n82,430wds (sections 23-24). The “check” set has\nbeen used for estimating the interpolation weights\nand tuning the search parameters; the “develop-\nment” set has been used for gathering/estimating\ncounts; the test set has been used strictly for evalu-\nating model performance.\nTable 1 shows the results of the re-estimation tech-\nnique presented in section 3.4. We achieved a reduc-\ntion in test-data perplexity bringing an improvement\nover a deleted interpolation trigram model whose\nperplexity was 167.14 on the same training-test data;\nthe reduction is statistically signiﬁcant according to\na sign test.\niteration DEV set TEST set\nnumber L2R-PPL L2R-PPL\nE0 24.70 167.47\nE1 22.34 160.76\nE2 21.69 158.97\nE3 21.26 158.28\n3-gram 21.20 167.14\nTable 1: Parameter re-estimation results\nSimple linear interpolation between our model and\nthe trigram model:\nQ(wk+1/Wk) =\nλ · P (wk+1/wk−1, wk) + (1 − λ) · P (wk+1/Wk)\nyielded a further improvement in PPL, as shown in\nTable 2. The interpolation weight was estimated on\ncheck data to be λ = 0 .36.\nAn overall relative reduction of 11% over the trigram\nmodel has been achieved.\n6 Conclusions and Future Directions\nThe large diﬀerence between the perplexity of our\nmodel calculated on the “development” set — used\niteration TEST set TEST set\nnumber L2R-PPL 3-gram interpolated PPL\nE0 167.47 152.25\nE3 158.28 148.90\n3-gram 167.14 167.14\nTable 2: Interpolation with trigram results\nfor model parameter estimation — and “test” set —\nunseen data — shows that the initial point we choose\nfor the parameter values has already captured a lot\nof information from the training data. The same\nproblem is encountered in standard n-gram language\nmodeling; however, our approach has more ﬂexibility\nin dealing with it due to the possibility of reestimat-\ning the model parameters.\nWe believe that the above experiments show the\npotential of our approach for improved language\nmodels. Our future plans include:\n• experiment with other parameterizations than the\ntwo most recent exposed heads in the word predictor\nmodel and parser;\n• estimate a separate word predictor for left-to-\nright language modeling. Note that the correspond-\ning model predictor was obtained via re-estimation\naimed at increasing the probability of the ”N-best”\nparses of the entire sentence;\n• reduce vocabulary of parser operations; extreme\ncase: no non-terminal labels/POS tags, word only\nmodel; this will increase the speed of the parser\nthus rendering it usable on larger amounts of train-\ning data and allowing the use of deeper stacks —\nresulting in more “N-best” derivations per sentence\nduring re-estimation;\n• relax — ﬂatten — the initial statistics in the re-\nestimation of model parameters; this would allow the\nmodel parameters to converge to a diﬀerent point\nthat might yield a lower word-level perplexity;\n• evaluate model performance on n-best sentences\noutput by an automatic speech recognizer.\n7 Acknowledgments\nThis research has been funded by the NSF\nIRI-19618874 grant (STIMULATE).\nThe authors would like to thank to Sanjeev Khu-\ndanpur for his insightful suggestions. Also to Harry\nPrintz, Eric Ristad, Andreas Stolcke, Dekai Wu and\nall the other members of the dependency model-\ning group at the summer96 DoD Workshop for use-\nful comments on the model, programming support\nand an extremely creative environment. Also thanks\nto Eric Brill, Sanjeev Khudanpur, David Yarowsky,\nRadu Florian, Lidia Mangu and Jun Wu for useful\ninput during the meetings of the people working on\nour STIMULATE grant.\nReferences\nW. Byrne, A. Gunawardana, and S. Khudanpur.\n1998. Information geometry and EM variants.\nTechnical Report CLSP Research Note 17, De-\npartment of Electical and Computer Engineering,\nThe Johns Hopkins University, Baltimore, MD.\nC. Chelba, D. Engle, F. Jelinek, V. Jimenez, S. Khu-\ndanpur, L. Mangu, H. Printz, E. S. Ristad,\nR. Rosenfeld, A. Stolcke, and D. Wu. 1997. Struc-\nture and performance of a dependency language\nmodel. In Proceedings of Eurospeech , volume 5,\npages 2775–2778. Rhodes, Greece.\nMichael John Collins. 1996. A new statistical parser\nbased on bigram lexical dependencies. In Proceed-\nings of the 34th Annual Meeting of the Associ-\nation for Computational Linguistics , pages 184–\n191. Santa Cruz, CA.\nA. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.\nMaximum likelihood from incomplete data via the\nEM algorithm. In Journal of the Royal Statistical\nSociety, volume 39 of B, pages 1–38.\nFrederick Jelinek and Robert Mercer. 1980. Inter-\npolated estimation of markov source parameters\nfrom sparse data. In E. Gelsema and L. Kanal, ed-\nitors, Pattern Recognition in Practice , pages 381–\n397.\nF. Jelinek, J. Laﬀerty, D. M. Magerman, R. Mercer,\nA. Ratnaparkhi, and S. Roukos. 1994. Decision\ntree parsing using a hidden derivational model.\nIn ARPA, editor, Proceedings of the Human Lan-\nguage Technology Workshop , pages 272–277.\nM. Marcus, B. Santorini, and M. Marcinkiewicz.\n1995. Building a large annotated corpus of En-\nglish: the Penn Treebank. Computational Lin-\nguistics, 19(2):313–330.\nColin Philips. 1996. Order and Structure . Ph.D.\nthesis, MIT. Distributed by MITWPL.",
  "topic": "Trigram",
  "concepts": [
    {
      "name": "Trigram",
      "score": 0.8433529734611511
    },
    {
      "name": "Computer science",
      "score": 0.8431648015975952
    },
    {
      "name": "Natural language processing",
      "score": 0.7421445846557617
    },
    {
      "name": "Parsing",
      "score": 0.6790496110916138
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6780736446380615
    },
    {
      "name": "Language model",
      "score": 0.6660334467887878
    },
    {
      "name": "Probabilistic logic",
      "score": 0.5946811437606812
    },
    {
      "name": "Word (group theory)",
      "score": 0.5773722529411316
    },
    {
      "name": "Syntactic structure",
      "score": 0.5235902070999146
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5034880042076111
    },
    {
      "name": "USable",
      "score": 0.4973633587360382
    },
    {
      "name": "Sequence (biology)",
      "score": 0.49610933661460876
    },
    {
      "name": "Speech recognition",
      "score": 0.3311206102371216
    },
    {
      "name": "Syntax",
      "score": 0.2508559823036194
    },
    {
      "name": "Linguistics",
      "score": 0.1825622320175171
    },
    {
      "name": "Programming language",
      "score": 0.12470757961273193
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "World Wide Web",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "cited_by": 123
}