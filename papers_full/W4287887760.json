{
  "title": "Text Style Transfer for Bias Mitigation using Masked Language Modeling",
  "url": "https://openalex.org/W4287887760",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287895229",
      "name": "Ewoenam Kwaku Tokpo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2064105222",
      "name": "Toon Calders",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4298393544",
    "https://openalex.org/W4205329051",
    "https://openalex.org/W2964529779",
    "https://openalex.org/W2962937198",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2475287302",
    "https://openalex.org/W2963631950",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2134800885",
    "https://openalex.org/W2963078909",
    "https://openalex.org/W2963667126",
    "https://openalex.org/W2970562804",
    "https://openalex.org/W2331128040",
    "https://openalex.org/W3130478189",
    "https://openalex.org/W3204561666",
    "https://openalex.org/W2603777577",
    "https://openalex.org/W2617566453",
    "https://openalex.org/W2914442349",
    "https://openalex.org/W2964321064",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W4246322658",
    "https://openalex.org/W2105971942",
    "https://openalex.org/W2539350388",
    "https://openalex.org/W2962912551",
    "https://openalex.org/W2516809705",
    "https://openalex.org/W3093908736",
    "https://openalex.org/W2888173624",
    "https://openalex.org/W2567639685",
    "https://openalex.org/W2768440179",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W3198409578",
    "https://openalex.org/W4220820301",
    "https://openalex.org/W2898799786",
    "https://openalex.org/W2955041501"
  ],
  "abstract": "Ewoenam Kwaku Tokpo, Toon Calders. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop. 2022.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies: Student Research Workshop, pages 163 - 171\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nText Style Transfer for Bias Mitigation\nusing Masked Language Modeling\nEwoenam Kwaku Tokpo\nDepartment of Computer Science\nUniversity of Antwerp\nToon Calders\nDepartment of Computer Science\nUniversity of Antwerp\nAbstract\nIt is well known that textual data on the inter-\nnet and other digital platforms contain signifi-\ncant levels of bias and stereotypes. Various re-\nsearch findings have concluded that biased texts\nhave significant effects on target demographic\ngroups. For instance, masculine-worded job\nadvertisements tend to be less appealing to fe-\nmale applicants. In this paper, we present a\ntext-style transfer model that can be trained on\nnon-parallel data and be used to automatically\nmitigate bias in textual data. Our style transfer\nmodel improves on the limitations of many ex-\nisting text style transfer techniques such as the\nloss of content information. Our model solves\nsuch issues by combining latent content encod-\ning with explicit keyword replacement. We will\nshow that this technique produces better con-\ntent preservation whilst maintaining good style\ntransfer accuracy.\n1 Introduction\nAuthors such as Bolukbasi et al. (2016) and May\net al. (2019) have drawn attention to some fair-\nness problems in the NLP domain. In a post on\nBuzz-Feed (Subbaraman, 2017) with the title, “Sci-\nentists Taught A Robot Language. It Immediately\nTurned Racist\", the author reports how various au-\ntomated language systems are disturbingly learn-\ning discriminatory patterns from data. Another\nprominent example of bias in NLP is Amazon’s\nAI recruitment tool which turned out to be biased\nagainst female applicants (Dastin, 2018). Mitigat-\ning bias in textual data before training can be an\nimportant preprocessing step in training fair lan-\nguage systems like chatbots, language translation\nsystems, and search engines, but a more direct need\nfor mitigating bias in textual data has been pointed\nout by various researchers (Gaucher et al., 2011;\nTang et al., 2017; Hodel et al., 2017) who have\nuncovered the worrying issue of bias in job adver-\ntisements. This can have significant implications\non the job recruitment process. As a matter of fact,\nGaucher et al. (Gaucher et al., 2011) explored the\neffect of biased job advertisements on participants\nof a survey. They found that changing the wording\nof a job advertisement to favor a particular gender\ngroup considerably reduced the appeal of the job to\napplicants not belonging to that gender, regardless\nof the gender stereotype traditionally associated\nwith the job. Consequent to such findings, a few\ntools and models have been developed to detect\nand mitigate biases in job advertisements. Some of\nthese tools include text editors like Textio which\nhas been successfully used by companies such as\nAtlassian to increase diversity in their workforce\n(Daugherty et al., 2019).\nAnother area of impact, regarding biased texts,\nis in news publications; Kiesel et al. (2019) ex-\nplore the issue of hyperpartisan news from an ex-\ntreme left or right-wing perspective. Again, with\nthe prevalence of hate speech and microaggres-\nsion perpetuated on various social media platforms,\nthere have been growing concerns about fairness in\nsuch areas.\nA machine learning technique that can be em-\nployed to mitigate bias in text documents is style\ntransfer. Style transfer is a technique that involves\nconverting text or image instances from one do-\nmain to another, such that the content and mean-\ning of the instance largely remain the same but\nthe style changes. However, a problem that has\nchallenged research in text style transfer is the rela-\ntive unavailability of parallel data that would ide-\nally be required to train such models (Rao and\nTetreault, 2018; Fu et al., 2018; Shen et al., 2017).\nTraining with parallel data makes it possible to di-\nrectly map training instances from one domain to\nthe other, hence, facilitating the learning process.\nDue to this, most style transfer systems mainly\nemploy training techniques that fall under two cat-\negories: keyword replacement and auto-encoder\nsequence-to-sequence techniques. In the case of\nkeyword replacement, biased words are deleted\n163\nand replaced with alternative words. In the case of\nthe auto-encoder sequence-to-sequence generative\napproach, the input text is directly encoded by an\nencoder to get a latent representation of the text,\nwhich is subsequently decoded by a decoder.\nThe main contributions of this work include:\n1. The development of an end-to-end text bias\nmitigation model that can convert a piece of bi-\nased text to a neutral version1 whilst maintain-\ning significant content information. For exam-\nple, given the female-biased text, “The event\nwas kid-friendly for all the mothers working\nin the company\", our task is to transform this\ntext into a gender-neutral version like “The\nevent was kid-friendly for all theparents work-\ning in the company\" . Our model is trained\nexclusively on nonparallel data. Since parallel\ncorpora are relatively hard to obtain, training\nwith only non-parallel data is of great impor-\ntance.\n2. A novel way of improving content preserva-\ntion and fluency in text style transfer by com-\nbining keyword replacement and latent con-\ntent information. Some other key novelties in\nour work include our approach to generating\nlatent content representation and our approach\nto identifying attribute tokens.\nWe make the code and data used in this work\navailable 2.\n2 Style transfer\nStyle transfer has been widely explored in com-\nputer vision to convert images from one style to\nanother (Gatys et al., 2016; Huang and Belongie,\n2017; Johnson et al., 2016). However, directly\napplying image style transfer techniques for text\nis problematic because of the unique characteris-\ntics of both domains. For instance, in text, style\nand content are more tightly coupled and harder\nto separate (Hu et al., 2020). In addition to that,\nthe non-differentiability of discrete words causes\noptimization problems (Yang et al., 2018; Lample\net al., 2018).\nIn NLP, style transfer has mostly been explored\nin areas such as sentiment analysis (Li et al., 2018;\nFu et al., 2018; Zhang et al., 2018) and machine\ntranslation (Lample et al., 2017). A few style trans-\nfer learning techniques use parallel data for training.\n1See Section 7 for discussion on how we define bias.\n2https://github.com/EwoeT/MLM-style-transfer\nHu et al. (2020) give an elaborate survey on such\nmodels. In this paper, we will only focus on models\nthat are trained on non-parallel data, some of which\nwe will review in the following subsection.\n2.1 Auto-encoder sequence-to-sequence\nmodels\nAuto-encoder sequence-to-sequence models basi-\ncally consist of an encoder that encodes the given\ntext into a latent representation which is then de-\ncoded by a decoder. Many of these models adopt an\nadversarial approach to learn to remove any style\nattribute from the latent representation. The result-\ning disentangled latent representation is decoded\nby the decoder in a sequential generative manner.\nShen et al. (2017) propose two models for text\nstyle transfer based on the auto-encoder sequence-\nto-sequence technique: an aligned auto-encoder\nmodel and a variant of that, called the cross-aligned\nauto-encoder model. Prabhumoye et al. (2018) pro-\npose a style transfer model using back-translation.\nThis is based on prior research that suggests that\nlanguage translation retains the meaning of a text\nbut not the stylistic features (Rabinovich et al.,\n2017).\nAn issue with Auto-encoder sequence-to-\nsequence models, in general, is the loss of infor-\nmation due to compression when encoding. Fur-\nthermore, Wu et al. (2019) note that sequence-to-\nsequence models for style transfer often have lim-\nited abilities to produce high-quality hidden repre-\nsentations and are unable to generate long meaning-\nful sentences. Nonetheless, sequence-to-sequence\ngenerative models can prove more effective in ap-\nplications where the text needs to be considerably\nrephrased (eg. from informal style to a formal\nstyle).\n2.2 Explicit Style Keyword Replacement\nThese methods follow the general approach of\nidentifying attribute markers, deleting these mark-\ners, and predicting appropriate replacements for\nthese markers which conform to the target style.\nLi et al. (2018) propose the DeleteOnly and the\nDelete&Retrieve, which use a three-step Delete,\nRetrieve, and Generate approach. Sudhakar et al.\n(2019) introduce Blind Generative Style Trans-\nformer (B-GST) and Guided Generative Style\nTransformer (G-GST) as improvements on Dele-\nteOnly and the Delete&Retrieve from (Li et al.,\n2018).\n164\nSince Explicit Style Keyword Replacement\nmethods only delete a small portion of the input\ntext, they preserve much more information. These\nsystems on the other hand are unable to properly\ncapture information of the deleted tokens (Sud-\nhakar et al., 2019), leading to examples such as\n“The event was kid-friendly for all themothers work-\ning in the company\"→“The event was kid-friendly\nfor all the children working in the company\".\n3 Methodology\nThe goal of our model is to transform any piece of\nbiased text into a neutral version. If we take the two\nstyle attributes sa and sb to represent neutral style\nand biased style respectively, given a text sample\nxb that belongs to sb, our goal is to convertxb to xa,\nsuch that xa belongs to style sa but has the same\nsemantic content as xb except for style information.\nOur model is composed of four main compo-\nnents, as illustrated in Fig 1. We also illustrate the\nprocess with an example in Fig. 2.\n3.1 Attribute Masker\nThe Attribute Masker identifies the attribute words\n(words responsible for bias in a text) and masks\nthese words with a special [MASK] symbol. The\nresultant text is fed as input to the Token Embedder.\nWe use LIME (Ribeiro et al., 2016), a model ag-\nnostic explainer that can be used on textual data, to\nidentify attribute tokens. Although very effective,\nusing LIME can increase computational time, espe-\ncially for long text sequences. Some Explicit Style\nKeyword Replacement models use relatively sim-\nple techniques to identify attribute words. Li et al.\n(2018) use the relative frequency of words in the\nsource style. Others like Sudhakar et al. (2019) em-\nploy more advanced methods like using attention\nweights. However, using techniques like attention\nweights to identify attribute tokens has been proven\nto not be very effective (Jain and Wallace, 2019).\nTo use LIME to detect attribute words, we first\nneed to train a text classifierfthat predicts whether\na given text is biased. We fine-tune BERT (Devlin\net al., 2019), a pretrained language model, as a text\nclassifier by training it on a labeled corpus contain-\ning both biased and neutral texts. Lime linearly\napproximates the local decision boundary of f and\nassigns weights to tokens based on their influence\non the classification outcome. With these weights\n(scores), we set a threshold value µto select words\nto be masked. These words are replaced by a spe-\ncial [MASK] token.\n3.2 Token Embedder\nThe Token Embedder is responsible for generating\ntoken embeddings for the masked tokens. To do\nthis, we train a BERT model for masked language\nmodeling on a corpus of unbiased texts. The Token\nEmbedder outputs a set of all token embeddings\nW = {w1,...,w n}∈ Rn×d. Following the con-\nvention used by Devlin et al. (2019), we take the\nsize of every embedding to be d= 768throughout\nthis paper.\n3.3 Latent-content Encoder\nThe Latent-content Encoder takes the original (un-\nmasked) text as input and encodes it into a latent\ncontent representation. An important part of this\nstage is our approach to disentangle the resulting\nlatent content representation from the biased style.\nThe Latent-content Encoder is responsible for\ngenerating a latent content representation of the\ninput sentence. For this, we train a BERT embed-\nding model that takes as input the original text\n(unmasked) xb and generates a target latent repre-\nsentation ˆz.\nWhen xb is given as an input, the Latent-content\nEncoder first generates token embeddings vi ∈Rd\nfor each token ti ∈xb. The set of token embed-\ndings V = {v1,...,v n}∈ Rn×d is mean-pooled\nto generate ˆz ∈Rd. Since we want ˆzto have the\nsame content as xb but not the bias that exists in xb,\nwe use a dual objective training to debias ˆz.\nBoth the Latent-content Encoder and the Source\nContent Encoder take xb as input. The Latent-\ncontent Encoder generates output ˆz whereas the\nSource Content Encoder generates z. Firstly, the\ngoal is to make ˆz and z have the same content,\nhence, we want them to be as similar as possi-\nble. We use the cosine-similarity to quantify this\nsimilarity. The similarity loss is minimized using\nmean-squared error; defined as:\nLsim = 1\nN\nN∑\nj=1\n(cosine_similarity(ˆzj,zj) −1)2\nSecondly, a bias detector takes ˆz as input and re-\nturns the class probabilities of ˆz. Because we want\nˆzto belong to the neutral class, the Latent-content\nEncoder has to learn to generate ˆz that is always\nclassified as neutral. This is achieved by minimiz-\ning the cross-entropy loss:\n165\nFigure 1: The architecture of our proposed model. The model consists of four main components. The arrows show\nthe flow of information within the model, and how the various components interact with each other.\nFigure 2: An example to illustrate the end-to-end bias mitigation process. This demonstrates the operation of each\ncomponent of the model. In the case of multiple attribute words, these attribute words are all masked and replaced\nsimultaneously. The Latent-content Encoder aims to remove traces of gender information from sentence-level\nsemantic content before being added to the token embeddings.\nLaccˆzj\n= −\nN∑\nj=1\nlogP(sa|ˆzj)\nP(sa|ˆzj) is the classifier’s prediction of the prob-\nability of ˆzbeing neutral.\nCombining both losses we get the dual objective:\nLCE_loss= (1−λ)Lsim + λLaccˆzj\n3.4 Token Decoder\nThe Token Decoder computes the average of each\ntoken embedding and the latent content representa-\ntion to generate new token embeddings. The Token\nDecoder uses these embeddings to predict the cor-\nrect tokens.\nThe Token Decoder first adds latent content in-\nformation to word embeddings. To do this, the To-\nken Decoder takes as inputs bothW from the Token\nEmbedder and ˆzfrom the Latent-content Encoder.\nFor each wi ∈W, a new token embedding ˆwi ∈\nRdis generated by computing the weighted average\nof wi ∈Rd and ˆz ∈Rd. After generating ˆwi, the\nToken Decoder uses it to predict the right token by\ncomputing the probability distribution over all the\ntokens in the vocabulary. We compute the decoding\nloss as: Ldec = −\nn∑\ni=1;tπi∈TΠ\nlogP(tπi|ˆwiΠ)\nTo augment this process, we use a pretrained\nclassifier to ensure that the output sentence xa is\nalways neutral. A dual objective is again used in\nthis process: TD_loss= (1−γ)Ldec + γLaccxa.\nWhere Laccxa is the loss from the classifier. Be-\ncause xa is made up of discrete tokens (one-hot en-\ncodings) which are non-differentiable during back-\npropagation, we use a soft sampling approach as\nwas done in (Wu et al., 2019; Prabhumoye et al.,\n2018): tπi ∼softmax(ot/τ)\n4 Experiments\nFor our experiments, we focus on gender bias (we\nlimit our work to a binary definition of gender) 3.\nThe use of gender is motivated by the relative avail-\nability of resources such as datasets. Nonetheless,\nwe believe that our work is adaptable to other forms\nof biases such as racial bias since the technique is\nnot dependent on the domain (only neutral and bias\nexamples are needed). To show our technique’s\napplicability in different domains, we experiment\non gender obfuscation, where instead of mitigating\nthe bias, we try to convert female-authored texts\nto \"look like\" male-authored texts. We arbitrar-\nily chose to convert from female to male just for\nthe sake of experiment; the same technique can be\napplied for male to female as well.\nAll experiments are conducted using English lan-\nguage corpus. In the future, we hope to extend our\nwork to cover other languages as well. We dis-\ncuss the details of our experiments in the following\nsubsections.\n4.1 Dataset\nWe run our experiments4 on two datasets discussed\nbelow. Some statistics of the datasets are given in\nA Table 3\n3See Section 7\n4All experiments are run on a Tesla V100-SXM3 GPU\nwith 32Gb memory.\n166\n4.1.1 Jigsaw dataset:\nThe Jigsaw datasets5 consists of comments that are\nlabeled by humans with regard to bias towards or\nagainst particular demographics. Using the value\n0.5 as a threshold, we extract all texts with gender\n(male or female) label ≥0.5 as the gender-biased\nclass of texts and extract a complementary set with\ngender labels <0.5 as the neutral class.\n4.1.2 Yelp dataset:\nWe extract this dataset from the preprocessed Yelp\ndataset used by (Prabhumoye et al., 2018; Reddy\nand Knight, 2016a). This dataset contains short\nsingle sentences which we use for author gender\nobfuscation.\n4.2 Evaluation models and metrics\nTo evaluate the performance of our model, we com-\npare it to six other models; Delete-only, Delete-\nand-retrieve (Li et al., 2018), B-GST, G-GST (Sud-\nhakar et al., 2019), CAE (Shen et al., 2017) and\nBST (Prabhumoye et al., 2018).\nThe evaluation is based on three automated eval-\nuation metrics for style transfer discussed by Hu\net al. (2020); style transfer accuracy (Transfer\nstrength), content preservation, and fluency.\nStyle transfer accuracy:This gives the percent-\nage of texts that were successfully flipped from\nthe source style (bias style) to the target style (neu-\ntral style) by our model. To predict whether a text\nwas successfully flipped, we use a trained BERT\nclassifier different from the one used to train the\nrespective models.\nContent preservation: We measure content\npreservation by computing the similarity between\nthe generated text and the original text. Similar\nto Fu et al. (2018), we use the cosine similar-\nity between the original text embedding and the\ntransferred text embedding to measure the content\npreservation. To make this more effective, we gen-\nerate text embeddings with SBERT (Reimers and\nGurevych, 2019), a modified version of pre-trained\nBERT that generates semantically meaningful sen-\ntence embeddings for sentences so that similar sen-\ntences have similar sentence embeddings, that can\nbe compared using cosine-similarity.\nFluency: Similar to (Subramanian et al., 2018),\nwe measure the fluency of the generated text using\nthe perplexity produced by a Kneser–Ney smooth-\n5https://www.kaggle.com/c/Jigsaw-unintended-bias-in-\ntoxicity-classification/data\nTable 1: Jigsaw dataset-Transfer strength and Content\npreservation scores for the models on all three datasets.\nC.P.: Content preservation, PPL: Fluency (Perplexity),\nAccuracy: Style transfer accuracy, Original*: refers to\nthe original input text. For A.C., C.P.and Agg, higher\nvalues are better. For PPL, lower values are better\nC.P. PPL AC%\nOriginal* 100.00 12.51 0.08\nDel 97.47 363.64 92.30\nDel&ret 97.50 242.33 71.70\nB-GST 96.73 1166.4 10.10\nG-GST 99.11 621.50 38.80\nCAE 95.60 795.58 83.70\nOur model 99.71 76.75 88.10\nTable 2: Yelp dataset-Transfer strength and Content\npreservation scores for the models for the . C.P.: Con-\ntent preservation, PPL: Fluency (Perplexity), Accuracy:\nStyle transfer accuracy,Original*: refers to the original\ninput text. . For A.C., C.P.and Agg, higher values are\nbetter. For PPL, lower values are better\nC.P. PPL AC%\nOriginal* 100.00 11.39 17.80\nDel 98.70 41.03 33.79\nDel&ret 98.25 57.73 30.90\nB-GST 95.94 141.81 23.90\nG-GST 97.28 70.24 21.00\nCAE 98.48 43.78 32.09\nBST 95.49 63.33 68.80\nOur model 99.05 45.17 43.20\ning 5-gram language model, KenLM (Heafield,\n2011) trained on the respective datasets.\n4.3 Results and discussion\nFrom Table 1, as we expected from the compared\nmodels, the models that perform considerably well\nin one metric suffer significantly in other metrics.\nFor instance, Delete-Only (Del) produces the best\ntransfer accuracy but lags behind other models\nin content preservation and fluency. For content\npreservation and fluency, our model produces im-\nproved results over all the other models. This result\nis consistent with our expectation of improving con-\ntent preservation with our techniques. Again, the\naccuracy score (second highest) produced by our\nmodel confirms the claim that our model preserves\ncontent information without a significant drop in\ntransfer accuracy.\nFrom Table 2, the same observation is made\nfor gender obfuscation; models that perform very\n167\nwell in one metric fall short in other metrics. BST\nproduces the best style transfer accuracy but at the\nsame time has the worst content preservation score.\nFrom the results from both datasets, one key ob-\nservation is that models that perform very well in\none metric tend to fall short in other metrics. This\ngoes to show the difficulty for style transfer models\nto preserve content information whilst maintain-\ning a strong transfer accuracy. This observation\nis confirmed by previous works (Li et al., 2018;\nWu et al., 2019; Hu et al., 2020) which mention\nthe general trade-off between style transfer accu-\nracy and content preservation. Our model shows\ngood results in maintaining a good balance across\nall metrics. Some text samples from our experi-\nments are shown in Appendix A Table 4. Also,\nin Appendix A, Table 5 and Table 6 show the re-\nsults from an ablation analysis on the Yelp dataset,\nwhere we strip off components of our model to\nanalyze the effect. Text samples from the ablation\nstudy are also provided in Appendix A, Table 7 and\nTable 8.\n5 Related work\nHe et al. (2021) propose DePen, a Detect and Per-\nturb approach to neutralize biased texts, using grad-\nuate school admissions as a case study. Sun et al.\n(2021) propose a method that aims to rewrite En-\nglish texts with gender-neutral English (in partic-\nular, the use of singular they for gender pronouns)\nusing a combination of regular expressions, a de-\npendency parser, and GPT-2 (Radford et al., 2019)\nmodel. Nogueira dos Santos et al. (2018) propose\nan RNN-based auto-encoder model to neutralize\noffensive language on social media, using a combi-\nnation of classification loss and reconstruction loss\nto ensure style transfer and to improve text gen-\neration. In a different but related context, Reddy\nand Knight (2016b) propose a gender obfuscation\ntechnique to disguise or change the gender of an\nauthor of a text as a means of privacy protection\nor for the prevention of inadvertent discrimination\nagainst the author. Their method is a word substitu-\ntion technique based on word2vec (Mikolov et al.,\n2013).\n6 Conclusion\nIn this work, we introduce a style transfer model\nthat can be used to mitigate bias in textual data.\nWe show that explicit keyword replacement can be\neffectively combined with latent content represen-\ntation to improve the content preservation of text\nstyle transfer models.\nAs part of our future work, we intend to expand\nthis work to other languages, we plan to explore\npossible improvements to the model such as adver-\nsarial learning, and also to include human evalua-\ntors for qualitative evaluation. Again, we intend to\ninvestigate other forms of attributes beyond tokens,\nsuch as sentence length, and how that affects bias\nin textual data. We also plan to apply our model\nas a preprocessing technique to train fair language\nmodels. We believe this could significantly reduce\nbiases found in automated language systems.\n7 Ethical considerations\nWorks like Dev et al. (2021) have drawn attention\nto gender exclusivity and issues relating to non-\nbinary representation in NLP, particularly in the\nEnglish language. For practical constraints such as\nthe limited availability of non-binary gender data\nand/or the significant under-representation of non-\nbinary gender identities in available datasets, we\nlimit this study to a binary definition of gender. For\nthe same reasons stated above, our definition of\ngender is analogous to female and male definitions\nof sex (Walker and Cook, 1998). Although this is\nan obvious limitation to our work, we believe this\nwork opens the door to extensively explore similar\nissues in non-binary gender settings, which need a\nmore expansive discussion.\nSince the definition of a biased text is highly do-\nmain, context, and task dependent, especially when\nit relates to the use of language (English in this\ncase), our approach identifies “biased” and “neu-\ntral” texts as per how they are defined or annotated\nin the training data for a specific task. Hence, the\nlabels (fair or biased) assigned to certain text ex-\namples may not be perceived accordingly in other\nsettings and tasks. We also note that, although the\nuse of explicit gender terms in certain domains may\nbe deemed to introduce biases (in some recruitment\nscenarios for instance), this practice may be accept-\nable or even encouraged in other domains such as\nin text discussions about diversity and sexism.\nAcknowledgements\nThis research was supported by the Flemish Gov-\nernment under the “Onderzoeksprogramma Artifi-\nciële Intelligentie (AI) Vlaanderen” programme.\n168\nReferences\nTolga Bolukbasi, Kai-Wei Chang, James Zou,\nVenkatesh Saligrama, and Adam Kalai. 2016. Man\nis to computer programmer as woman is to home-\nmaker? debiasing word embeddings. NIPS’16, page\n4356–4364, Red Hook, NY , USA. Curran Associates\nInc.\nJeffrey Dastin. 2018. Amazon scraps secret ai\nrecruiting tool that showed bias against women.\nhttps://www.reuters.com/article/\nus-amazon-com-jobs-automation-\ninsight/amazon-scraps-secret-ai-\nrecruiting-tool-that-showed-bias-\nagainst-women-idUSKCN1MK08G. Ac-\ncessed: 2021-08-21.\nPaul R Daugherty, H James Wilson, and Rumman\nChowdhury. 2019. Using artificial intelligence to\npromote diversity. MIT Sloan Management Review,\n60(2):1.\nSunipa Dev, Masoud Monajatipoor, Anaelia Ovalle, Ar-\njun Subramonian, Jeff Phillips, and Kai-Wei Chang.\n2021. Harms of gender exclusivity and challenges in\nnon-binary representation in language technologies.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1968–1994, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nZhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao,\nand Rui Yan. 2018. Style transfer in text: Explo-\nration and evaluation. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 32.\nLeon A Gatys, Alexander S Ecker, and Matthias Bethge.\n2016. Image style transfer using convolutional neural\nnetworks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 2414–\n2423.\nDanielle Gaucher, Justin Friesen, and Aaron C Kay.\n2011. Evidence that gendered wording in job adver-\ntisements exists and sustains gender inequality. Jour-\nnal of personality and social psychology, 101(1):109.\nZexue He, Bodhisattwa Prasad Majumder, and Julian\nMcAuley. 2021. Detect and perturb: Neutral rewrit-\ning of biased and sensitive text via gradient-based\ndecoding. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2021 , pages 4173–\n4181, Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nKenneth Heafield. 2011. KenLM: Faster and smaller\nlanguage model queries. In Proceedings of the Sixth\nWorkshop on Statistical Machine Translation, pages\n187–197, Edinburgh, Scotland. Association for Com-\nputational Linguistics.\nLea Hodel, Magdalena Formanowicz, Sabine Sczesny,\nJana Valdrová, and Lisa von Stockhausen. 2017.\nGender-fair language in job advertisements: A cross-\nlinguistic and cross-cultural analysis. Journal of\nCross-Cultural Psychology, 48(3):384–401.\nZhiqiang Hu, Roy Ka-Wei Lee, and Charu C Aggarwal.\n2020. Text style transfer: A review and experiment\nevaluation.\nXun Huang and Serge Belongie. 2017. Arbitrary style\ntransfer in real-time with adaptive instance normal-\nization. In Proceedings of the IEEE International\nConference on Computer Vision, pages 1501–1510.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot Explanation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3543–3556, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nJustin Johnson, Alexandre Alahi, and Li Fei-Fei. 2016.\nPerceptual losses for real-time style transfer and\nsuper-resolution. In European conference on com-\nputer vision, pages 694–711. Springer.\nJohannes Kiesel, Maria Mestre, Rishabh Shukla, Em-\nmanuel Vincent, Payam Adineh, David Corney,\nBenno Stein, and Martin Potthast. 2019. Semeval-\n2019 task 4: Hyperpartisan news detection. In Pro-\nceedings of the 13th International Workshop on Se-\nmantic Evaluation, pages 829–839.\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer,\nand Marc’Aurelio Ranzato. 2017. Unsupervised ma-\nchine translation using monolingual corpora only.\nGuillaume Lample, Sandeep Subramanian, Eric Smith,\nLudovic Denoyer, Marc’Aurelio Ranzato, and Y-Lan\nBoureau. 2018. Multiple-attribute text rewriting. In\nInternational Conference on Learning Representa-\ntions.\nJuncen Li, Robin Jia, He He, and Percy Liang. 2018.\nDelete, retrieve, generate: a simple approach to senti-\nment and style transfer. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers) ,\npages 1865–1874, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\nBowman, and Rachel Rudinger. 2019. On measuring\nsocial biases in sentence encoders. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\n169\nShort Papers), pages 622–628, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efficient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nCicero Nogueira dos Santos, Igor Melnyk, and Inkit\nPadhi. 2018. Fighting offensive language on social\nmedia with unsupervised text style transfer. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers), pages 189–194, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nShrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhut-\ndinov, and Alan W Black. 2018. Style transfer\nthrough back-translation. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 866–876,\nMelbourne, Australia. Association for Computational\nLinguistics.\nElla Rabinovich, Raj Nath Patel, Shachar Mirkin, Lucia\nSpecia, and Shuly Wintner. 2017. Personalized ma-\nchine translation: Preserving original author traits. In\nProceedings of the 15th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Volume 1, Long Papers, pages 1074–1084,\nValencia, Spain. Association for Computational Lin-\nguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nSudha Rao and Joel Tetreault. 2018. Dear sir or madam,\nmay I introduce the GY AFC dataset: Corpus, bench-\nmarks and metrics for formality style transfer. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 129–140, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nSravana Reddy and Kevin Knight. 2016a. Obfuscating\ngender in social media writing. In Proceedings of the\nFirst Workshop on NLP and Computational Social\nScience, pages 17–26.\nSravana Reddy and Kevin Knight. 2016b. Obfuscating\ngender in social media writing. In Proceedings of the\nFirst Workshop on NLP and Computational Social\nScience, pages 17–26, Austin, Texas. Association for\nComputational Linguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2016. \" why should i trust you?\" explaining\nthe predictions of any classifier. In Proceedings of\nthe 22nd ACM SIGKDD international conference on\nknowledge discovery and data mining, pages 1135–\n1144.\nTianxiao Shen, Tao Lei, Regina Barzilay, and Tommi\nJaakkola. 2017. Style transfer from non-parallel text\nby cross-alignment. In Proceedings of the 31st Inter-\nnational Conference on Neural Information Process-\ning Systems, NIPS’17, page 6833–6844, Red Hook,\nNY , USA. Curran Associates Inc.\nNidhi Subbaraman. 2017. Scientists taught a\nrobot language. it immediately turned racist.\nhttps://www.buzzfeednews.com/\narticle/nidhisubbaraman/robot-\nracism-through-language. Accessed:\n2021-08-22.\nSandeep Subramanian, Guillaume Lample,\nEric Michael Smith, Ludovic Denoyer, Marc’Aurelio\nRanzato, and Y-Lan Boureau. 2018. Multiple-\nattribute text style transfer.\nAkhilesh Sudhakar, Bhargav Upadhyay, and Arjun Ma-\nheswaran. 2019. \"transforming\" delete, retrieve,\ngenerate approach for controlled text style transfer.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 3267–3277.\nAssociation for Computational Linguistics.\nTony Sun, Kellie Webster, Apu Shah, William Yang\nWang, and Melvin Johnson. 2021. They, them,\ntheirs: Rewriting with gender-neutral english. arXiv\npreprint arXiv:2102.06788.\nShiliang Tang, Xinyi Zhang, Jenna Cryan, Miriam J\nMetzger, Haitao Zheng, and Ben Y Zhao. 2017. Gen-\nder bias in the job market: A longitudinal analysis.\nvolume 1, pages 1–19. ACM New York, NY , USA.\nPhillip L Walker and Della Collins Cook. 1998. Brief\ncommunication: Gender and sex: Vive la difference.\nAmerican Journal of Physical Anthropology: The\nOfficial Publication of the American Association of\nPhysical Anthropologists, 106(2):255–259.\nXing Wu, Tao Zhang, Liangjun Zang, Jizhong Han, and\nSonglin Hu. 2019. Mask and infill: Applying masked\nlanguage model for sentiment transfer. In Proceed-\nings of the Twenty-Eighth International Joint Con-\nference on Artificial Intelligence, IJCAI-19 , pages\n5271–5277. International Joint Conferences on Arti-\nficial Intelligence Organization.\nZichao Yang, Zhiting Hu, Chris Dyer, Eric P. Xing, and\nTaylor Berg-Kirkpatrick. 2018. Unsupervised text\nstyle transfer using language models as discrimina-\ntors. NIPS’18, page 7298–7309, Red Hook, NY ,\nUSA. Curran Associates Inc.\nZhirui Zhang, Shuo Ren, Shujie Liu, Jianyong Wang,\nPeng Chen, Mu Li, Ming Zhou, and Enhong Chen.\n2018. Style transfer as unsupervised machine trans-\nlation.\n170\nA Appendix\nTable 3: Dataset statistics\nDataset Attributes Classifier Train Dev Test\nJigsaw Sexist 24K 32K 1K 1K\nNeutral 24K 92K 3K 3K\nYelp Male 100K 100K 1K 1K\nFemale 100K 100K 1K 1K\nTable 4: Sample text outputs from experiments\nGender bias mitigation (biased→neutral): Jigsaw\ninput text i hope the man learned his\nlesson to slow down and\nbuckle up .\nour model i hope the driver learned\nhis lesson to slow down and\nbuckle up .\ninput text i married a wonderful ma-\nture , loyal and dedicated\nforeign women while work-\ning abroad ...\nour model i married a wonderful ma-\nture , loyal and dedicated\nforeign person while work-\ning abroad ...\nGender obfuscation (female→male): Yelp\ninput text overall , worth the extra\nmoney to stay here .\nour model overall , worth the damn\nmoney to eat here .\ninput text i had prosecco and my\nboyfriend ordered a beer .\nour model i had prosecco and my wife\nordered a beer .\nTable 5: Ablation study of our model on theJigsaw gen-\nder dataset. Without-LR: model with soft sampling\n(class constraint) but no latent content representation,\nWithout-LR&SS: model with no class constraint and\nno latent content representation\nC.P PPL ACC%\nOur model 99.71 76.75 88.10\nWithout-LR 99.69 98.87 93.44\nWithout-LR&SS 99.70 98.68 93.44\nTable 6: Ablation study of our model on the Yelp\ndataset. Without-LR: model with soft sampling\n(class constraint) but no latent content representation,\nWithout-LR&SS: model with no class constraint and\nno latent content representation. Although Without-LR\nhas a very high accuracy score, as can be seen from the\nexample in 8, many of the Without-LR texts are unable\nto preserve content information\nC.P PPL ACC%\nOur model 99.05 45.17 43.20\nWithout-LR 96.62 45.72 84.20\nWithout-LR&SS 96.89 41.84 41.00\nTable 7: Sample text outputs from ablation study from\nJigsaw dataset\nGender bias mitigation (biased→neutral): Jigsaw\ninput text if there was an article dis-\nparaging women as idiots\nthere would be a protest and\na parade .\nour model if there was an article dis-\nparaging them as idiots\nthere would be a protest and\na parade .\nWithout-LR if there was an article dis-\nparaging muslims as idiots\nthere would be a protest and\na parade .\nWithout-LR&SS if there was an article dis-\nparaging muslims as idiots\nthere would be a protest and\na parade .\nTable 8: Sample text outputs from ablation study Yelp\ndataset\nGender obfuscation (female→male): Yelp\ninput text i did not buy extra insur-\nance !\nour model i did not buy auto insurance\n!\nWithout-LR i did not buy life insurance\n!\nWithout-LR&SS i did not buy the pistol !\n171",
  "topic": "Style (visual arts)",
  "concepts": [
    {
      "name": "Style (visual arts)",
      "score": 0.7339949607849121
    },
    {
      "name": "Computer science",
      "score": 0.662661075592041
    },
    {
      "name": "Computational linguistics",
      "score": 0.5972126722335815
    },
    {
      "name": "Natural language processing",
      "score": 0.5335636138916016
    },
    {
      "name": "Linguistics",
      "score": 0.5329065918922424
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45988190174102783
    },
    {
      "name": "Art",
      "score": 0.13402801752090454
    },
    {
      "name": "Philosophy",
      "score": 0.12865638732910156
    },
    {
      "name": "Literature",
      "score": 0.0971401035785675
    }
  ]
}