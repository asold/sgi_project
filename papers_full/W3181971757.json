{
  "title": "COVID-VIT: Classification of COVID-19 from CT chest images based on vision transformer models",
  "url": "https://openalex.org/W3181971757",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2051075754",
      "name": "Gao Xiaohong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1981771317",
      "name": "Qian Yu",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Gao, Alice",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3016448052",
    "https://openalex.org/W3007764760",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3002108456",
    "https://openalex.org/W3001118548",
    "https://openalex.org/W3001897055",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3006082171",
    "https://openalex.org/W3171916733",
    "https://openalex.org/W3002715510",
    "https://openalex.org/W2769215776",
    "https://openalex.org/W3014337038",
    "https://openalex.org/W3086650124"
  ],
  "abstract": "This paper is responding to the MIA-COV19 challenge to classify COVID from non-COVID based on CT lung images. The COVID-19 virus has devastated the world in the last eighteen months by infecting more than 182 million people and causing over 3.9 million deaths. The overarching aim is to predict the diagnosis of the COVID-19 virus from chest radiographs, through the development of explainable vision transformer deep learning techniques, leading to population screening in a more rapid, accurate and transparent way. In this competition, there are 5381 three-dimensional (3D) datasets in total, including 1552 for training, 374 for evaluation and 3455 for testing. While most of the data volumes are in axial view, there are a number of subjects' data are in coronal or sagittal views with 1 or 2 slices are in axial view. Hence, while 3D data based classification is investigated, in this competition, 2D images remains the main focus. Two deep learning methods are studied, which are vision transformer (ViT) based on attention models and DenseNet that is built upon conventional convolutional neural network (CNN). Initial evaluation results based on validation datasets whereby the ground truth is known indicate that ViT performs better than DenseNet with F1 scores being 0.76 and 0.72 respectively. Codes are available at GitHub at .",
  "full_text": "COVID-VIT: Classification of Covid-19 from CT chest images based on vision transformer models \nXiaohong Gao, Yu Qian, Alice Gao* \nDepartment of Computer Science, Middlesex University, London, UK.  \nx.gao@mdx.ac.uk. \n* A&E Department, Newham University Hospital, Barts Health NHS Trust, London, UK.  \nalicegao@doctors.org.uk. \n \nAbstract \nThis paper is responding to the MIA -COV19 challenge to classify COVID from non - COVID based on CT lung \nimages. The COVID-19 virus has devastated the world in the last eighteen months by infecting more than 182 \nmillion people and causing over 3.9 million deaths. The overarching aim is to predict the diagnosis of the COVID-\n19 virus from chest radiographs, through the development of explainable vision transformer deep learning tech-\nniques, leading to population screening in a more rapid, accurate and transparent way. In this competition, there \nare 5381 three-dimentaional (3D) datasets in total, including 1552 for training, 374 for evaluation and 3455 for \ntesting. While most of the data volume are in axial view, there are a number of subjects’ data are in coronal or \nsagittal views with 1 or 2 slices are in axial view. Hence, while 3D data based classification is investigated, in this \ncompetition, 2D images remains the main focus. Two deep learning methods are studied, which are vision trans-\nformer (ViT) based on attention models and DenseNet that is built upon conventional convolutional ne ural net-\nwork (CNN). Initial evaluation results based on validation datasets whereby the ground truth is known indicate \nthat ViT performs better than DenseNet with F1 scores being 0.76 and 0.72 respectively.  (Codes are available at \nGitHub at https://github.com/xiaohong1/COVID-ViT).  \n \n1. Introduction \nCOVID-19, officially known as SARS -CoV-2 is a strain of coronavirus. The first cases were seen in \nWuhan, China, in late December 2019 before spreading globally [1-3] which as and was classified as a \npandemic in March 2020 [4]. At present there are more than 182 million people infected with the virus \nand 3.9 million of deaths [5] with new variants keep appearing. \nThe clinical picture can range from a mild common cold-like illness, to a severe viral pneumonia leading \nto acute respiratory distress syndrome (ARDS) that is potentially fatal. The presence of COVID-19 in \nrespiratory specimens was detected by next generation sequencing or real -time reverse transcription \npolymerase chain reaction (RT-PCR) methods, a laboratory technique combining reverse transcription \nof Ribonucleic acid (RNA) into Deoxyribonucleic acid (DNA) and amplification of specific DNA tar-\ngets. While PCR tests offer many advantages, results are not usually available for at least several hours. \nOn the other hand, high resolution Computerised Tomography (CT) are non-invasive, easy to operate \nand prevalent and hence can assist diagnosis for COVID-19 rapidly. \n \nWith regard to imaging features, it appears that bilateral infiltrates with peripheral opacities and patchy \nconsolidation are the most common findings on chest radiographs (CXR) [ 6,7] and bilateral ground \nglass opacities is often a key finding on CT [8,9]. \nAs confirmed cases continues to increase considerably all over the world, timely detection of the disease \nnot only can provide supportive care required by patients but also can prevent further spread of the virus.  \nConsequently, effective screening of infected patients appears to be a critical step in this fight agains t \nCOVID-19 as well as to circumvent the temporary shortage of RT -PCR kits to confirm COVID -19 \ninfection. \nThe challenge here facing detecting COVID-19 based on chest CT images is that when the disease is at \nits early onset, the characteristic patterns present less obvious to the human eyes [10]. Hence, machine \nlearning based approaches are applied to investigate COVID -specific biomarkers. In this study vision \ntransformer architectures are investigated. \n2. Related work \nVision transformer (ViT) have recently demonstrated its potentials in image processing by achieving \ncomparable results while requiring fewer computational resources.  Based on self-attention architec-\ntures, transformer becomes the leading model in natural language processing (NLP) [11]. For NLP, by \nemploying attention models, i.e. transformers, training speed can be significantly improved hence en-\nhancing the performance of neural machine translation applications. For image processing, vision trans-\nformers are emerging and starting to show protentials by applying to computer vision tasks, such as \nimage recognition [12]. Specifically, ViT appears to demonstrate excellent performance when trained \non sufficient data, outperforming a comparable state -of-the-art CNN with four times fewer computa-\ntional resources. \n \nOne of the advantages that that Transformers present is computational efficiency and scalability. It has \nbecome possible to train models of unprecedented size, with over 100 billion parameters [13].  \n \nFigure 1 illustrates the architecture of ViT employed in this study. In this study, the ViT is implemented \nin pytorch and heavily based on the code  at [14].  \n \nThe training process takes place at a GPU sever that equipped with one Quadro RTX 8000 GPU and \n64GB memory under Debian Linux operating system. While in the training, the 2D images are resized \nto 224×224×3 and 224×224×32 for 3D.  For 3D training, each subject’s 2D slices (in JPG format) are \nfirstly converted into Analyze (7.5) format, with both header (.hdr) and image (.img) files. The patch \nsize for the application of ViT model is 7×7 for 2D images and  8×8×8 for 3D volumes. It takes about \n24 hours for training 80 epochs for 2D images and ~30 hours for 3D volumes. \n \n \n \nFigure 1. The ViT architecture implemented in this work. \n \n \n\n \n3. The MIA-COV19 competition datasets \nThe CT thorax  lung images are collected from MIA-COV19 competition, composing of training, vali-\ndation and testing datasets. Table 1 lists the detailed information of these data.  The resolution of the \nthese images is either 512×512 or 768×768 pixels whereas the depth of each volu me ranges from 4 \nslices to 1026 as detailed in MIA-COV19 competition papers [15-18]. \nTable 1. The datasets from MIA -COV19 competition applied in this paper. Note 2D slice numbers are the slices \nthat have undertaken pre-processing stage and removed those with little lung contents. \nLabel Train Validation Testing Total \n 3D subject 2D slice 3D subject 2D slice 3D subject 2D slice  \nCOVID 687 63,808 165 13,839    \nNon-\nCOVID \n865 66,880 209 15,793    \ntotal 1,552 130,688 374 29,632 3,455 444,524 5381 \n \n3.1 Image pre-processing \nBecause the diseased regions of a COVID-19 dataset occupy less than 10% of the whole volume and \npresent in a number of slices, image pre-processing take place first to maximise the large visibility of \ndiseased slices while removing scanner artefact. Figure 2  demonstrates a montage view of all the slices \nfor one subject. It shows that the first 3 slices hardly depict any lung content wh ereas the boundary \ninformation as well as the background in each slice accommodates more than half of the slice in concern \nin each 2D image. In addition, the heart (arrow) and liver (arrow head) also make a large appearance in \nseveral slices. \n \n\nFigure 2. An axial view of a data volume in the form of montage. Arrow: heart. Arrow head: liver. This subject \nhas confirmed diagnosis of COVID-19. \nHence, before the training, all images undertake pre-processing stage to remove the boundary, which is \nillustrated in Figure 3. Not only are the boundary of the lung removed but also the presence of the heart \nand liver, which leads to more focused content of interest. After this pre-processing stage by creating \nCT masks and segmentation, the final images have a resolution of 440 × 360. This task was completed \nusing Matlab. \n \nFigure 3. The images after pre-processing from the data volume in Figure 1, which are then applied to training \nor evaluation. \n \n3.2 The challenges detecting COVID from Non-COVID CT images \nSince there are still many unknowns regarding to COVID -19 virus, many biomarkers attributed to \nCOVID-19 are not specific. The common visible patterns of COVID-19 include bilateral involvement \nand peripheral distribution, with superimposed interlobular septal thickening and visible intralobular \nlines. However, other patterns , such as with unilobar, perihilar patchy ground glass distribution do exist \nwith COVID-19 patients [19]. \n \nFigure 4 demonstrates a group of images for a nonCovid patient, which present abnormal cloudy fea-\ntures. \n \nFigure 3. An example of nonCOVID dataset with cloudy patches. \n \n4. Experimental results \nThe classification results are subject based, which is calculated from the predicted scores of all the 2D \nimages for that subject. Considering the artefact that might be introduced during the pre -processing \nstage and a COVID volume contains nonCOVID 2D slices, the subject is classified as having COVID \nif more than a threshold (e.g. 25% ) number of slices are predicted as COVID. Similarly, if the remain-\ning number, e.g. 75% or more , slices are predicted as nonCOVID , this subject will be classified as \nnonCOVID patient. Table 1 presents the confusion matrixes for the two deep learning systems, one is \n\nCOVID-CT system based on DenseNet [20] and one is Vision Transformer architecture shown in Figure \n1. The evaluation results are based on validation dataset whlist the training employs the training datasets. \nSince the test data have no labels, they are not included in Table 2. \nTable 2. Evaluation results for the two deep leaning system, CNN-DenseNet and VIT. \n CNN – DenseNet Vit—Vision transformer \n COVID Non COVID Acc(%) F1 COVID \n \nNon COVID Acc(%) F1 \nCOVID \n (predict) \n119 29 80.4 0.71 117 31 79.1 0.74 \nNon COVID \n (predict) \n64 130 67.0 0.73 50 144 74.2 0.78 \nAverage   73.7 0.72   76.6 0.76 \n \nFour predictions are submitted to this MIA-COV2019 competition with 2 thresholds for ViT (25% and \n20%) and 2 for DenseNet (6% and 5%). \n \n5. Conclusion \nWhile participating MIA-COV19 competition, another aim of this work is to build an explainable sys-\ntem for medical application.  Vision transformer architectures are built upon attention models and ae \nscalable when compared with CNN based models. Specifically, in the medical domain, the number of \ndata sets can never be as large as current benchmark databases, e.g. ImageNet, with over millions of \nimages. Hence a system that can still achieve good performance while employing limited number of \ndatasets will make significant impact in the medical applications. \nIn comparison with CNN based model DenseNet for COVID-CT, ViT model appears to perform better \nwith 76.6% accuracy whereas DenseNet realised accuracy of 73.7%. \nWhile chest CT images are in 3D volume, it is a natural approach t o process these data in 3D form. \nHowever, due to the large variation s of slice numbers (depth), ranging from 4 to 1000 +, with varying \nresolutions, generating 3D volumes present a challenge. Hence a depth of 32 slices, i.e. a volume of \n224×224×32, is created for those subjects with sufficient depth images, by selecting slices evenly cross \nthe whole volume. For example, if a 3D dataset has 64 slices in depth, then two sub-volumes are created \nfor this subject with sub-volume 1 containing slices 1,3, .. 63 and sub-volume 2 having slices of 2,4, …, \n64.  \nAs address previously, the lesioned regions are proportionally small comparing to the whole volume, \nwhich might constitute the main reason that 3D based system perform far worse than 2D based models. \nFuture work will further investigate this challenging issue.  Another challenge remains to be the data \nvolume size when preforming pre-pressing. Overall the training, validation and testing sizes are around \n100 GB. Therefore pre -processing to segment lung content takes about 12 hours for all the subjects ’ \ndataset. \n \nReferences: \n[1]  Zhu N, Zhang D, Wang W, Li, X, et al, A Novel Coronavirus from Patients with Pneumonia in China, \n2019 , New England Journal of Medicine, 382(8):727-733, 2020.  \n[2]  Perlman, S., Another Decade, Another Coronavirus. (2020) New England Journal of Medicine, \n382(8): 760:762, 2020.  \n[3]  Hui DS, I Azhar E, Madani TA, Ntoumi F, Kock R, et al, The continuing 2019-nCoV epidemic threat \nof novel coronaviruses to global health - The latest 2019 novel coronavirus outbreak in Wuhan, China, \nInternational journal of infectious diseases : IJID : official publication of the International Society for \nInfectious Diseases. 91: 264-266, 2020. \n[4]  WHO, Director -General's opening remarks at the media  briefing on COVID -19 - 11 March 2020, \nWho.int. 2020. https://www.who.int/dg/speeches/detail/who-director-general-s-opening-remarks-at-\nthe-media-briefing-on-covid-19---11-march-2020. Retrieved in April 2020. \n[5]  Resource Centre, Johns Hopkins University and Medicine,  https://coronavirus.jhu.edu/. \n[6]  Chen N, Zhou M, Dong X, et al., Epidemiological and clinical characteristics of 99 cases of 2019 \nnovel coronavirus pneumonia in Wuhan, China: a descriptive study , The Lancet , 395 \n(10332):507:513, 2020. \n[7]  Huang, C., Wang, Y., Li, X., et al, Clinical features of patients infected with 2019 novel coronavirus \nin Wuhan, China, Lancet, 395 (10223):497-506, 2020 \n[8]  Raptis C, Hammer M, Sho rt R, et al. Chest CT and Coronavirus Disease (COVID -19): A Critical \nReview of the Literature to Date,  American Journal of Roentgenology, 1-4. 10.2214/AJR.20.23202, \n2020. \n[9]  Kanne J, Little B, Chung J, Elicker B, Ketai L, Essentials for Radiologists on COVID -19: An Up-\ndate—Radiology Scientific Expert Panel, Radiology, in press, 2020. \n[10]  Ng M, Lee E, Yang J, et al, Imaging Profile of the COVID -19 Infection: Radiologic Findings and \nLiterature Review, Radiology: Cardiothoracic Imaging, 2:1, 2020. \n[11]  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz \nKaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017. \n[12]  Dosovitskiy A., et al., An image is worth 16x16 words: transformers for image recognition at scale, \nICLR 2021. \n[13]  Brown TB, Mann B, Ryder N, Subbiah M, et al., Language models are few-shot learners. arXiv, 2020. \n[14]  ViT implementation, https://github.com/lucidrains/vit-pytorch.  \n[15]  Kollias D, Arsenos A, Soukissian L, Kollias S, MIA-COV19D: COVID-19 Detection through 3 -D \nChest CT Image Analysis, 2021, arXiv preprint arXiv:2106.07524, 2021. \n[16]  D. Kollias, et. al., Deep transparent prediction through latent representation analysis , 2020,  arXiv \npreprint arXiv:2009.07044, 2020. \n[17]   D. Kollias, et. al., Transparent Adaptation in Deep Medical Image Diagnosis, TAILOR, pp251-267, \n2020. \n[18]   D. Kollias, et. al.: \" Deep neural architectures for prediction in healthcare \", 2018, Complex & Intel-\nligent Systems, 4(2):119-131, 2018. \n[19]  Tran TA, Cezar R, Frandon J, et al., CT scan does not make a diagnosis od Covid -19: a cautionary \ncase report, International Journal of indfectious diseases, 100: 182-183, 2020. \n[20]  Zhao J,  Zhang Y , He,X , Xie, P, COVID -CT-Dataset: a CT scan dataset about COVID -19, arXiv \npreprint arXiv:2003.13865, 2020. \n \n \n \n \n \n ",
  "topic": "Coronavirus disease 2019 (COVID-19)",
  "concepts": [
    {
      "name": "Coronavirus disease 2019 (COVID-19)",
      "score": 0.7916420698165894
    },
    {
      "name": "Convolutional neural network",
      "score": 0.7427538633346558
    },
    {
      "name": "Deep learning",
      "score": 0.6428061127662659
    },
    {
      "name": "Transformer",
      "score": 0.6180434226989746
    },
    {
      "name": "Sagittal plane",
      "score": 0.6009727716445923
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5972175002098083
    },
    {
      "name": "Coronal plane",
      "score": 0.5720764398574829
    },
    {
      "name": "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)",
      "score": 0.5538203716278076
    },
    {
      "name": "2019-20 coronavirus outbreak",
      "score": 0.5336815714836121
    },
    {
      "name": "Computer science",
      "score": 0.5110886693000793
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3474683463573456
    },
    {
      "name": "Medicine",
      "score": 0.262066125869751
    },
    {
      "name": "Pathology",
      "score": 0.22021383047103882
    },
    {
      "name": "Radiology",
      "score": 0.1914876401424408
    },
    {
      "name": "Engineering",
      "score": 0.16277462244033813
    },
    {
      "name": "Infectious disease (medical specialty)",
      "score": 0.08372437953948975
    },
    {
      "name": "Disease",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Outbreak",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": []
}