{
    "title": "Are Neural Language Models Good Plagiarists? A Benchmark for Neural Paraphrase Detection",
    "url": "https://openalex.org/W4234918518",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A4224451961",
            "name": "Wahle, Jan Philip",
            "affiliations": [
                "University of Wuppertal"
            ]
        },
        {
            "id": "https://openalex.org/A4223315846",
            "name": "Ruas, Terry",
            "affiliations": [
                "University of Wuppertal"
            ]
        },
        {
            "id": "https://openalex.org/A3160266820",
            "name": "Meuschke, Norman",
            "affiliations": [
                "University of Wuppertal"
            ]
        },
        {
            "id": "https://openalex.org/A3162852365",
            "name": "Gipp, Bela",
            "affiliations": [
                "University of Wuppertal"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4206706211",
        "https://openalex.org/W3125358881",
        "https://openalex.org/W2980420460",
        "https://openalex.org/W2996573939",
        "https://openalex.org/W6776048684",
        "https://openalex.org/W6605323724",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W6632555176",
        "https://openalex.org/W4213122582",
        "https://openalex.org/W4255807467",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3138008297",
        "https://openalex.org/W131533222",
        "https://openalex.org/W4299567010",
        "https://openalex.org/W1544505227",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4292779060"
    ],
    "abstract": "The rise of language models such as BERT allows for high-quality text\\nparaphrasing. This is a problem to academic integrity, as it is difficult to\\ndifferentiate between original and machine-generated content. We propose a\\nbenchmark consisting of paraphrased articles using recent language models\\nrelying on the Transformer architecture. Our contribution fosters future\\nresearch of paraphrase detection systems as it offers a large collection of\\naligned original and paraphrased documents, a study regarding its structure,\\nclassification experiments with state-of-the-art systems, and we make our\\nfindings publicly available.\\n",
    "full_text": "Related papers at https://jpwahle.com/pub/ and https://gipp.com/pub/\nJ. P. Wahle, T. Ruas, N. Meuschke and B. Gipp, Are Neural Language Models Good Plagiarists? A Benchmark for\nNeural Paraphrase Detection, 2021 ACM/IEEE Joint Conference on Digital Libraries (JCDL), 2021, pp. 226-229, doi:\n10.1109/JCDL52503.2021.00065.\nClick to download: BibTeX — RIS\nAre Neural Language Models Good Plagiarists?\nA Benchmark for Neural Paraphrase Detection\nJan Philip Wahle\nUniversity of Wuppertal\nWuppertal, Germany\nwahle@uni-wuppertal.de\nTerry Ruas\nUniversity of Wuppertal\nWuppertal, Germany\nruas@uni-wuppertal.de\nNorman Meuschke\nUniversity of Wuppertal\nWuppertal, Germany\nmeuschke@uni-wuppertal.de\nBela Gipp\nUniversity of Wuppertal\nWuppertal, Germany\ngipp@uni-wuppertal.de\nAbstract—Neural language models such as BERT allow for\nhuman-like text paraphrasing. This ability threatens academic\nintegrity, as it aggravates identifying machine-obfuscated pla-\ngiarism. We make two contributions to foster the research on\ndetecting these novel machine-paraphrases. First, we provide\nthe ﬁrst large-scale dataset of documents paraphrased using the\nTransformer-based models BERT, RoBERTa, and Longformer.\nThe dataset includes paragraphs from scientiﬁc papers on arXiv,\ntheses, and Wikipedia articles and their paraphrased counter-\nparts (1.5M paragraphs in total). We show the paraphrased\ntext maintains the semantics of the original source. Second, we\nbenchmark how well neural classiﬁcation models can distinguish\nthe original and paraphrased text. The dataset and source code\nof our study are publicly available.\nIndex Terms—Paraphrase detection, BERT, transformers\nI. I NTRODUCTION\nTransformer-based language models [1] have reshaped nat-\nural language processing (NLP) and become the standard\nparadigm for most NLP downstream tasks [2], [3]. Now,\nthese models are rapidly advancing to other domains such as\ncomputer vision [4]. We anticipate Transformer-based models\nwill similarly inﬂuence plagiarism detection research in the\nnear future [5]. Plagiarism is the use of ideas, concepts, words,\nor structures without proper source acknowledgment. Often\nplagiarists employ paraphrasing to conceal such practices [6].\nParaphrasing tools, such as SpinBot1 and SpinnerChief 2, fa-\ncilitate the obfuscation of plagiarised content and threaten the\neffectiveness of plagiarism detection systems (PDS).\nWe expect that paraphrasing tools will abandon deter-\nministic machine-paraphrasing approaches in favor of neural\nlanguage models, which can incorporate intrinsic features\nfrom human language effectively [3]. The ability of models\nsuch as GPT-3 [3] to produce human-like texts raises major\nconcerns in the plagiarism detection community as statistical\nand traditional machine learning solutions cannot distinguish\nsemantically similar texts reliably [7]. Using Transformer-\nbased models for the classiﬁcation seems to be intuitive to\ncounteract this new form of plagiarism. However, Transformer-\nbased solutions typically require sufﬁciently large sets of la-\nbeled training data to achieve high classiﬁcation effectiveness.\nAs the use of neural language models for paraphrasing is a\nrecent trend, data for the training of PDS is lacking.\n1https://spinbot.com\n2https://spinnerchief.com/\nThis paper contributes to the development of future detec-\ntion methods for paraphrased text by providing, to our knowl-\nedge, the ﬁrst large-scale dataset of text paraphrased using\nTransformer-based language models. We study how word-\nembeddings and three Transformer-based models used for\nparaphrasing (BERT [2], RoBERTa [8], and Longformer [9])\nperform in classifying paraphrased text to underline the dif-\nﬁculty of the task and the dataset’s ability to reﬂect it. The\ndataset and source code of our study are publicly available 3.\nWe grant access to the source code after accepting the terms\nand conditions designed to prevent misuse. Please see the\nrepository for details.\nII. R ELATED WORK\nParaphrase identiﬁcation is a well-researched NLP prob-\nlem with numerous applications, e.g., in information retrieval\nand digital library research [6]. To identify paraphrases,\nmany approaches combine lexical, syntactical, and semantic\ntext analysis [7]. The Microsoft Research Paraphrase Corpus\n(MRPC) [10], a collection of human-annotated sentence pairs\nextracted from news articles, is among the most widely-used\ndatasets for training and evaluating paraphrase identiﬁcation\nmethods. Another popular resource for paraphrase identiﬁ-\ncation is the Quora Question Pairs (QQP) dataset included\nin GLUE [11]. The dataset consists of questions posted on\nQuora4, a platform on which users can ask for answers to\narbitrary questions. The task is to identify questions in the\ndataset that share the same intent. The datasets published as\npart of the PAN workshop series on plagiarism detection,\nauthorship analysis, and other forensic text analysis tasks 5\nare the most comprehensive and widely-used resource for\nevaluating plagiarism detection systems.\nNeither the PAN nor the MRPC and QQP datasets include\nparaphrases created using state-of-the-art neural language\nmodels. The MRPC and QQP datasets consist of human-made\ncontent, which is unsuitable for training classiﬁers to recognize\nmachine-paraphrased text. The PAN datasets contain cases that\nwere obfuscated using basic automated heuristics that do not\nmaintain the meaning of the text. Examples of such heuristics\ninclude randomly removing, inserting, or replacing words or\n3https://doi.org/10.5281/zenodo.4621403\n4https://quora.com/about\n5https://pan.webis.de/\n1\narXiv:2103.12450v5  [cs.CL]  10 Nov 2022\nTABLE I\nAN ILLUSTRATIVE SAMPLE FOR EACH PARAPHRASING MODEL AND DATA SOURCE . RED BACK GROUND HIGHLIGHTS CHANGED TOKENS COMPARED TO\nTHE ORIGINAL VERSION . THE ELLIPSIS “...” INDICATES THE REMAINDER OF THE PARAGRAPH .\nOriginal Parapgraphs:\n– A mathematically rigorous approach to quantum ﬁeld theory based on operator algebras is called an algebraic quantum ﬁeld theory...\n– ”Nuts” contains 5 instrumental compositions written and produced by Streisand, with the exception of ”The Bar”, including additional\nwriting from Richard Baskin. All of the songs were recorded throughout 1987...\n– Agriculture is the foundation for economic growth, development and poverty annihilation in developing countries. Ghana is endowed\nwith a variety of mineral and agricultural product (Breisinger, 2008) Ghana is a country...\nBERT Paraphrased Source MLM Prob.\nThe mathematically rigorous approach to quantum ﬁeld theory based upon operator equations is called an algebraic\nQuantum ﬁeld theory...\narXiv 0.15\nRoBERTa Paraphrased\n”Nuts” contains ﬁve instrumental compositions written or produced by Streisand, with the exception of ”Yourbars”, which\nincludes credited writing from Richard Baskin. All of these songs were recorded in 1987...\nWikipedia 0.15\nLongformer Paraphrased\nAgriculture is the foundation builder for economic growth, development and poverty annihilation in developing countries.\nGhana is endowed with a variety of biodiversity and agricultural product (Breisinger, 2008) Ghana became a country...\nthesis 0.15\nphrases and substituting words with their synonyms, antonyms,\nhyponyms, or hypernyms selected at random [12]. These\ncases are not representative of the sophisticated paraphrases\nproduced by state-of-the-art Transformer-based models.\nCurrently, the HuggingFace API offers few neural language\nmodels capable of paraphrasing text excerpts. Most models\nare based on the same technique and trained to process short\nsentences. Plagiarists reuse paragraphs most frequently [6].\nHence, the ability to identify paragraph-sized paraphrases is\nmost relevant for a PDS in practice. Prior to our study, no\ndataset of paragraphs paraphrased using Transformer-based\nmodels existed and could be used for training PDS.\nPrior studies mitigated the lack of suitable datasets by\nparaphrasing documents using the paid services SpinBot and\nSpinnerChief [7], [13]. As the evaluations in these studies\nshowed, text obfuscated by these tools already poses a signif-\nicant challenge to current plagiarism detection systems. Nev-\nertheless, the sophistication of the paraphrased text obtained\nfrom such tools to date is lower than that of paraphrases\ngenerated by Transformer-based models. Therefore, we extend\nthe earlier studies [7] and [13] by using Transformer-based ar-\nchitectures [1] to generate paraphrases that reﬂect the strongest\nlevel of disguise technically feasible to date.\nIII. D ATASET CREATION\nOur neural machine-paraphrased dataset is derived from\nprevious studies [7], [13]. The dataset of Foltynek et al. [7]\nconsists of featured Wikipedia articles6 in English. The dataset\nof Wahle et al. [13] comprises scientiﬁc papers randomly\nsampled from the no problems category of the arXMLiv 7\nproject, and randomly selected graduation theses by English as\na Second Language(ESL) students at the Mendel University\nin Brno, Czech Republic.\n6https://en.wikipedia.org/wiki/Wikipedia:Content assessment\n7https://kwarc.info/projects/arXMLiv/\nTABLE II\nOVERVIEW OF THE ORIGINAL PARAGRAPHS IN OUR DATASET .\nFeatures arXiv Theses Wiki Wiki-Train\nParagraphs 20 966 5 226 39 241 98 282\n# Words 3 194 695 747 545 5 993 461 17 390 048\nAvg. Words 152.38 143.04 152.73 176.94\nThe earlier studies employed the paid online paraphrasing\nservices SpinBot and SpinnerChief for text obfuscation. Since\nwe investigate neural language models for paraphrasing, we\nonly use the 163 715 original paragraphs from the earlier\ndataset. Table II shows the composition of these original\nparagraphs used for our dataset.\nFor paraphrasing, we used BERT [2], RoBERTa [8], and\nLongformer [9]. We chose BERT as a strong baseline for\ntransformer-based language models; RoBERTa and Long-\nformer improve BERT’s architecture through more training\nvolume and an efﬁcient attention mechanism, respectively.\nMore speciﬁcally, we used the masked language model (MLM)\nobjective of all three Transformer-based models to create the\nparaphrases. The MLM hides a conﬁgurable portion of the\nwords in the input, for which the model then has to infer the\nmost probable word-choices. We excluded named entities and\npunctuation, e.g., brackets, digits, currency symbols, quotation\nmarks from paraphrasing to avoid producing false information,\nor inconsistent punctuation compared to the original source.\nThen, we masked words and forwarded them to each model\nto obtain word candidates and their conﬁdence scores. Lastly,\nwe replaced each masked word in the original with the\ncorresponding candidate word having the highest conﬁdence\nscore. Examples of original and paraphrased text using dif-\nferent models and data sources are illustrated in Table I. We\nalso experimented with sampling uniformly from the top-k\nword predictions but neglected this method because of poor\nparaphrasing quality.\n0.15 0.2 0.3 0.4 0.5\nMLM Probability\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00F1-score\nModel\nBERT\nLongformer\nRoBERTa\nDataset\narXiv\nWikipedia\nThesis\nFig. 1. Classiﬁcation accuracy of fastText + SVM for neural-paraphrased test\nsets depending on masked language model probabilities.\nWe ran an ablation study to understand how the masking\nprobability of the MLM affects the difﬁculty of classifying\ndocuments as either paraphrased or original. For this pur-\npose, we employed each neural language model with varying\nmasking probabilities to paraphrase the arXiv, theses, and\nWikipedia subsets. We encoded all original and paraphrased\ntexts as features using the sentence embedding of fastText\n(subword)8, which was trained on a 2017 dump of the full\nEnglish Wikipedia, the UMBC WebBase corpus, and StatMT\nnews dataset with 300 dimensions. Lastly, we applied the\nsame SVM classiﬁer to all fastText feature representations to\ndistinguish between original and paraphrased content.\nFig. 1 shows the results of the ablation study. Higher\nmasking probabilities consistently led to higher classiﬁcation\naccuracy. In other terms, replacing more words reduced the dif-\nﬁculty of the classiﬁcation task. This correlation has also been\nobserved for non-neural paraphrasing tools [13]. Paragraphs\nfrom theses were most challenging for the classiﬁer regardless\nof the paraphrasing model. We hypothesize that sub-optimal\nword choice and grammatical errors in the texts written by\nESL students increase the difﬁculty of classifying these texts.\nThe F1-scores for paragraphs from arXiv and Wikipedia were\nconsistently higher than for theses. We attribute the high score\non the Wikipedia test set to the documents’ similarity with the\ntraining set which consists only of Wikipedia articles.\nMasking 15% of the words posed the hardest challenge for\nthe classiﬁer. This ratio corresponds to the masking probability\nused for pre-training BERT [2], and falls into the percentage\nrange of words that paid online paraphrasing tools replace on\naverage (12.58% to 19.37%) [13]. Thus, we used a masking\nprobability of 15% for creating all paraphrased data.\nAs a proxy for paraphrasing quality, we evaluated the\nsemantic equivalence of original and paraphrased text. Specif-\nically, we analyzed the BERT embeddings of 30 randomly\nselected original paragraphs from arXiv, theses, and Wikipedia\nand their paraphrased counterparts created using BERT,\nRoBERTa, and Longformer. Fig. 2 visualizes the results using\na t-distributed Stochastic Neighbor Embedding (t-SNE) for\n8https://fasttext.cc/docs/en/english-vectors.html\n15\n 10\n 5\n 0 5 10\nt-SNE-1\n10\n5\n0\n5\n10\nt-SNE-2\nLabel\noriginal\nparaphrased\nDataset\narXiv\nWikipedia\nThesis\nFig. 2. Two-dimensional representation of BERT embeddings for 30 original\nand paraphrased paragraphs from each source. The overlap of the embeddings\nsuggests semantic equivalence of the original and paraphrased content.\ndimensionality reduction. The embeddings of original and\nparaphrased text overlap considerably despite changing ap-\nprox. 15% of the words. This indicates the Transformer-based\nlanguage models maintain the original text’s semantics.\nIV. C LASSIFICATION BENCHMARK\nTo check whether our dataset poses a realistic challenge\nfor state-of-the-art classiﬁers and to establish a performance\nbenchmark, we employed four models to label paragraphs\nas either original or paraphrased. A prior study showed that\ncurrent plagiarism detection systems, which are essentially\ntext-matching software, fail to identify machine-paraphrased\ntext reliably while word embeddings, machine-learning clas-\nsiﬁers, and particularly Transformer-based models performed\nconsiderably better [13]. Therefore, we evaluated the classi-\nﬁcation effectiveness of the three BERT-related models used\nfor paraphrasing and the fastText + SVM classiﬁer we applied\nand described for our ablation study (cf. Section III). We\nlimited the number of input tokens for each model to 512\nfor a fair comparison of the models without losing relevant\ncontext information9. Unless speciﬁed differently, we used all\nhyperparameters in their default conﬁguration.\nWe derived training data exclusively from Wikipedia as it is\nthe largest of the three collections. We used arXiv papers and\ntheses to obtain test sets that allow verifying a model’s ability\nto generalize to data from sources unseen during training. We\nused BERT to generate the paraphrased training set (Wiki-\nTrain) and BERT, RoBERTa, and Longfomer to create three\nparaphrased test sets. The classiﬁcation models were exposed\nto mutually exclusive paragraphs to avoid memorizing the dif-\nferences between aligned paragraphs. Evaluating each model\nusing text paraphrased by the same model allows us to verify\nan assumption from related work, i.e., the best classiﬁer is the\nlanguage model used to generate the paraphrased text [14].\nTable III shows the F1-Macro scores of each classiﬁcation\nmodel for the paraphrased test sets consisting of arXiv, theses,\nand Wikipedia paragraphs. The baseline model (fastText +\n999.35% of the datasets’ text can be represented with less than 512 tokens.\nTABLE III\nCLASSIFICATION RESULTS (F1-M ACRO SCORES ). BOLDFACE SHOWS THE\nBEST RESULT PER CLASSIFICATION MODEL .\nClassiﬁcation Model Dataset Paraphrase Model\nBERT RoBERTa Longformer\nfastText + SVM\n(baseline)\narXiv 70.40% 70.68% 71.17%\nTheses 68.94% 65.70% 66.85%\nWikipedia 71.50% 68.70% 70.05%\nAverage 70.28% 68.36% 69.36%\nBERT\narXiv 80.83% 68.90% 68.49%\nTheses 74.74% 67.39% 66.04%\nWikipedia 83.21% 68.85% 69.46%\nAverage 79.59% 68.38% 68.00%\nRoBERTa\narXiv 70.41% 85.40% 82.95%\nTheses 68.99% 79.13% 77.76%\nWikipedia 72.18% 84.20% 82.15%\nAverage 70.53% 82.91% 80.95%\nLongformer\narXiv 65.18% 85.46% 89.93%\nTheses 65.72% 77.96% 81.31%\nWikipedia 69.98% 81.76% 86.03%\nAverage 66.96% 81.73% 85.76%\nSVM) performed similarly for all paraphrasing models with\nscores ranging from F1=68.36% (RoBERTa) to F1=70.28%\n(BERT). With scores ranging from F1=79.59% (BERT) to\nF1=85.76% (Longformer), neural language models consis-\ntently identiﬁed text paraphrased using the same model best.\nThis observation supports the ﬁndings of Zellers et al. [14].\nNeural language models applied to paraphrases created\nby other models (e.g., BERT classiﬁes text paraphrased by\nLongformer), typically achieved comparable scores to fast-\nText+SVM. The average F1-scores for text paraphrased by\nunseen models range from F1=68.00% (BERT for Longformer\nparaphrases) to F1=81.73% (Longformer for RoBERTa para-\nphrases) with an average of 72.75%. These results are lower\nthan the average scores for classifying paraphrases created\nfor the same subsets using paid paraphrasing services (i.e.,\nF1=99.65% to F1=99.87% for SpinBot) [13]. This ﬁnding\nshows Transformer-based neural language models produce\nhard-to-identify paraphrases, which make our new dataset a\nchallenging benchmark task for state-of-the-art classiﬁers.\nRoBERTa and Longformer achieved comparable results for\nall datasets, which we attribute to their overlapping pre-\ntraining datasets. BERT uses a subset of RoBERTa’s and\nLongformer’s training data and identiﬁes the text paraphrased\nby the other two models with comparable F1-scores.Averaged\nover all paraphrasing techniques, RoBERTa achieved the best\nresult (F1=78.15%), making it the most general model we\ntested for detecting neural machine-paraphrases.\nAll classiﬁcation models performed best for Wikipedia\narticles, which is expected given their overlapping training\ncorpus. The three neural language models identiﬁed arXiv\narticles similarly well which is in line with our ablation study\n(cf. Fig. 1). As in our ablation study, theses by ESL students\nwere most challenging for our classiﬁcation models, again\ncorroborating our assumption that a higher ratio of gram-\nmatical and linguistic errors causes the drop in classiﬁcation\neffectiveness.\nV. C ONCLUSION AND FUTURE WORK\nWe presented a large-scale aligned dataset 3 of original\nand machine-paraphrased paragraphs to foster the research\non plagiarism detection methods. The paragraphs originate\nfrom arXiv papers, theses, and Wikipedia articles and have\nbeen paraphrased using BERT, RoBERTa, and Longformer.\nWe showed that the machine-paraphrased texts have a high\nsemantic similarity to their original sources which reinforces\nour manual observation that neural language models produce\nhard-to distinguish, human-like paraphrases.\nFurthermore, we showed Transformers are comparable in\nclassifying original and paraphrased content to static word\nembeddings (i.e., fastText) and most effective for identifying\ntext that was paraphrased using the same model. RoBERTa\nachieved the best overall result for detecting paraphrases.\nIn our future work, we will investigate other autoencoding\nmodels, and add autoregressive models to our study such as\nGPT-3 [3] for paraphrase generation and detection.\nREFERENCES\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin neural information processing systems 30. Curran Associates, Inc.,\n2017, pp. 5998–6008.\n[2] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\ntraining of Deep Bidirectional Transformers for Language Understand-\ning,” arXiv:1810.04805 [cs], May 2019, arXiv: 1810.04805.\n[3] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, and D. ... Amodei,\n“Language Models are Few-Shot Learners,”arXiv:2005.14165 [cs], Jun.\n2020, tex.ids: BrownMRS20a arXiv: 2005.14165.\n[4] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah,\n“Transformers in Vision: A Survey,” arXiv:2101.01169 [cs], Feb. 2021,\narXiv: 2101.01169.\n[5] N. Dehouche, “Plagiarism in the age of massive generative pre-trained\ntransformers (gpt-3),” Ethics in Science and Environmental Politics,\nvol. 21, pp. 17–23, 2021.\n[6] T. Folt ´ynek, N. Meuschke, and B. Gipp, “Academic Plagiarism Detec-\ntion: A Systematic Literature Review,”ACM Computing Surveys, vol. 52,\nno. 6, pp. 112:1–112:42, 2019.\n[7] T. Folt ´ynek, T. Ruas, P. Scharpf, N. Meuschke, M. Schubotz, W. Grosky,\nand B. Gipp, “Detecting Machine-obfuscated Plagiarism,” in Proceed-\nings of the iConference 2020, ser. LNCS. Springer, 2020.\n[8] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, “RoBERTa: A Robustly Optimized\nBERT Pretraining Approach,” arXiv:1907.11692 [cs], Jul. 2019, arXiv:\n1907.11692.\n[9] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The Long-\nDocument Transformer,” arXiv:2004.05150 [cs], Apr. 2020, arXiv:\n2004.05150.\n[10] B. Dolan and C. Brockett, “Automatically constructing a corpus of sen-\ntential paraphrases,” in Third International Workshop on Paraphrasing\n(IWP2005). Asia Fed. of Natural Language Processing, January 2005.\n[11] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,\n“GLUE: A Multi-Task Benchmark and Analysis Platform for Natural\nLanguage Understanding,” arXiv:1804.07461 [cs], Feb. 2019.\n[12] M. Potthast, B. Stein, A. Barr ´on-Cede˜no, and P. Rosso, “An Evaluation\nFramework for Plagiarism Detection,” in Proceedings Int. Conf. on\nComputational Linguistics, vol. 2, pp. 997–1005.\n[13] J. P. Wahle, T. Ruas, T. Folt ´ynek, N. Meuschke, and B. Gipp, “Iden-\ntifying Machine-Paraphrased Plagiarism,” arXiv:2103.11909 [cs], Jan.\n2021.\n[14] R. Zellers, A. Holtzman, H. Rashkin, Y . Bisk, A. Farhadi, F. Roesner,\nand Y . Choi, “Defending Against Neural Fake News,”arXiv:1905.12616\n[cs], Oct. 2019, arXiv: 1905.12616."
}