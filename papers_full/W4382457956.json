{
  "title": "Spatial-Spectral Transformer for Hyperspectral Image Denoising",
  "url": "https://openalex.org/W4382457956",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2567568540",
      "name": "Miaoyu Li",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2109509426",
      "name": "Ying Fu",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2297316014",
      "name": "Yulun Zhang",
      "affiliations": [
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2567568540",
      "name": "Miaoyu Li",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2297316014",
      "name": "Yulun Zhang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2520430674",
    "https://openalex.org/W3211469849",
    "https://openalex.org/W3212235541",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W2914736033",
    "https://openalex.org/W2752896014",
    "https://openalex.org/W2747865121",
    "https://openalex.org/W2724686744",
    "https://openalex.org/W2005353255",
    "https://openalex.org/W2056370875",
    "https://openalex.org/W3103465009",
    "https://openalex.org/W2984522085",
    "https://openalex.org/W3135593154",
    "https://openalex.org/W2763820948",
    "https://openalex.org/W2153663612",
    "https://openalex.org/W6767912030",
    "https://openalex.org/W2221872968",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W3134490757",
    "https://openalex.org/W2903684270",
    "https://openalex.org/W1944540851",
    "https://openalex.org/W2190198903",
    "https://openalex.org/W1985242206",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W1928626817",
    "https://openalex.org/W1974438823",
    "https://openalex.org/W4296104906",
    "https://openalex.org/W2171520281",
    "https://openalex.org/W3118496544",
    "https://openalex.org/W2998841120",
    "https://openalex.org/W2160675285",
    "https://openalex.org/W6746034047",
    "https://openalex.org/W6786708909",
    "https://openalex.org/W3012470401",
    "https://openalex.org/W2039596145",
    "https://openalex.org/W2806155925",
    "https://openalex.org/W3212228063",
    "https://openalex.org/W6780685329",
    "https://openalex.org/W3134334503",
    "https://openalex.org/W1994040806",
    "https://openalex.org/W3103919952",
    "https://openalex.org/W3205965083",
    "https://openalex.org/W2790888198",
    "https://openalex.org/W4226277663",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W4286856903",
    "https://openalex.org/W2975017131",
    "https://openalex.org/W4225672218",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3172099915",
    "https://openalex.org/W3151034650",
    "https://openalex.org/W1987128286",
    "https://openalex.org/W3013064625",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3017506038",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3171516518",
    "https://openalex.org/W2980079746",
    "https://openalex.org/W3106728613",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3165924482"
  ],
  "abstract": "Hyperspectral image (HSI) denoising is a crucial preprocessing procedure for the subsequent HSI applications. Unfortunately, though witnessing the development of deep learning in HSI denoising area, existing convolution-based methods face the trade-off between computational efficiency and capability to model non-local characteristics of HSI. In this paper, we propose a Spatial-Spectral Transformer (SST) to alleviate this problem. To fully explore intrinsic similarity characteristics in both spatial dimension and spectral dimension, we conduct non-local spatial self-attention and global spectral self-attention with Transformer architecture. The window-based spatial self-attention focuses on the spatial similarity beyond the neighboring region. While, the spectral self-attention exploits the long-range dependencies between highly correlative bands. Experimental results show that our proposed method outperforms the state-of-the-art HSI denoising methods in quantitative quality and visual results. The code is released at https://github.com/MyuLi/SST.",
  "full_text": "Spatial-Spectral Transformer for Hyperspectral Image Denoising\nMiaoyu Li1, Ying Fu1*, Yulun Zhang2\n1Beijing Institute of Technology\n2ETH Z¬®urich\n{miaoyli, fuying}@bit.edu.cn, yulun100@gmail.com\nAbstract\nHyperspectral image (HSI) denoising is a crucial preprocess-\ning procedure for the subsequent HSI applications. Unfortu-\nnately, though witnessing the development of deep learning in\nHSI denoising area, existing convolution-based methods face\nthe trade-off between computational efficiency and capabil-\nity to model non-local characteristics of HSI. In this paper,\nwe propose a Spatial-Spectral Transformer (SST) to alleviate\nthis problem. To fully explore intrinsic similarity characteris-\ntics in both spatial dimension and spectral dimension, we con-\nduct non-local spatial self-attention and global spectral self-\nattention with Transformer architecture. The window-based\nspatial self-attention focuses on the spatial similarity beyond\nthe neighboring region. While, the spectral self-attention ex-\nploits the long-range dependencies between highly correl-\native bands. Experimental results show that our proposed\nmethod outperforms the state-of-the-art HSI denoising meth-\nods in quantitative quality and visual results. The code is re-\nleased at https://github.com/MyuLi/SST.\nIntroduction\nHyperspectral images (HSIs) provide abundant information\nin spectral dimension and have been widely applied to the\nfields of remote sensing (Cloutis 1996), material recogni-\ntion (Thai and Healey 2002), agriculture (Kersting et al.\n2012), medical diagnosis (Fei 2020) and so on. However,\nin the sensing process, due to limited light, photon effects,\nand atmospheric interference, HSIs often suffer from corrup-\ntion and noise, which negatively influences the subsequent\nHSI applications. Therefore, HSI denoising is a critical pre-\nprocessing procedure to enhance image quality for the afore-\nmentioned high-level computer vision tasks.\nCompared to color images, HSIs offer pixel-level spec-\ntral features by imaging narrow spectral bands over a con-\ntinuous spectral range. It means there are statistical similar-\nities between bands. Early denoising works, such as dictio-\nnary learning method (Elad and Aharon 2006), and BM3D\n(Dabov et al. 2007), focused on the non-local similarity in\nspatial dimension but did not take the spectral features into\naccount. Thus, more HSI denoising works exploit both the\nspatial similarity and spectral correlation. Multilinear tools\n*Corresponding author\nCopyright ¬© 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nwere used in (Renard, Bourennane, and Blanc-Talon 2008)\nto extract spectral components and spatial information for\ndenoising. The parallel factor analysis model was employed\nin (Liu, Bourennane, and Fossati 2012) to exploit the decom-\nposition uniqueness and single rank character of HSIs. Low-\nrank prior (Chen et al. 2017; Chang et al. 2020), sparse rep-\nresentation (Zhuang and Bioucas-Dias 2018), and total vari-\nation regularization (Du et al. 2018) have also been widely\nadopted to HSI denoising. Despite the various hand-crafted\npriors, traditional model-based HSI denoising methods are\nalways hard to optimize and time-consuming.\nWith the development of deep learning, convolutional\nneural networks (CNNs) based HSI denoising meth-\nods (Yuan et al. 2018; Chang et al. 2018; Dong et al. 2019;\nZhang et al. 2019; Shi et al. 2021) have shown distinguished\nadvantages over traditional HSI denoising methods. CNN-\nbased methods rely on convolution filters to model the data\ndependencies in spatial dimension and spectral dimension,\nfacing the trade-off between computational efficiency and\nthe ability to model the non-local similarity of HSIs. Be-\nsides, the learned convolution filters are with static weights,\nwhich means the filters used for feature extraction are fixed\nin the testing phase. The denoising process is based on the\nknowledge learned from training dataset, but, the inner char-\nacteristics of the target HSI are not fully exploited.\nRecently, Transformer models have been applied to vi-\nsion tasks (Zhang et al. 2020; Carion et al. 2020; Dai\net al. 2021). Transformers apply the self-attention mecha-\nnism (Wang et al. 2018) across image regions and can well\ncapture the internal similarity of target image. According\nto previous analysis (Cai et al. 2022; Zamir et al. 2022),\nTransformer could be a powerful alternative to CNNs in HSI\ndenoising task. However, existing Transformer-based works\nmainly aim at natural images with ignorance in spectral sim-\nilarity. Since HSIs have strong spectral correlations, such ne-\nglect would negatively affect denoising results. A practical\nway of employing Transformer to HSI denoising is applying\nspectral-wise attention to well utilize spectral correlation.\nIn this work, we propose a Spatial-Spectral Transformer\nto sufficiently explore the non-local spatial similarity and\nglobal spectral correlation of HSIs. First, the spatial infor-\nmation of HSI is restored by the shifted window-based self-\nattention, which conducts a non-local spatial-wise coarse de-\nnoising beyond neighboring pixels. Secondly, the high cor-\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n1368\nrelation between bands is exploited by spectral-wise self-\nattention, which conducts globally weighted denoising on\neach pixel with fine details. Finally, the output from self-\nattention module is passed through the multi-layer percep-\ntron (MLP) and skip-connection for smooth convergence. In\nsummary, the main contributions of our work are as follows:\n‚Ä¢ We propose a Spatial-Spectral Transformer to fully ex-\nploit both the non-local spatial similarity and global spec-\ntral correlation of target noisy HSI.\n‚Ä¢ We design an efficient denoising module by integrating\nwindow-based spatial self-attention with spectral self-\nattention. The coarse spatial features are finely weighted\nby spectral attention to obtain detailed features.\n‚Ä¢ Extensive experimental results on various noise degrada-\ntions show that our proposed method outperforms state-\nof-the-art methods in terms of both objective metrics and\nsubjective visual quality.\nRelated Work\nIn this section, we briefly review two major research di-\nrections which are related to our work, including the latest\nprogress in HSI denoising and vision Transformers.\nHSI Denoising\nExisting HSI denoising methods could be roughly classified\ninto two categories, including traditional model-based meth-\nods and deep learning-based methods.\nModel-based HSI denoising methods usually utilize hand-\ncrafted prior knowledge of HSIs. The non-local similarity\n(Zhang et al. 2019), total variation (He et al. 2015; Yuan,\nZhang, and Shen 2012), low-rank (Zhang et al. 2013; Chang,\nYan, and Zhong 2017; He et al. 2021; Chang et al. 2020), and\nsparse representation (Lu et al. 2015; Zhuang and Bioucas-\nDias 2018) are frequently used to exploit the intrinsic spatial\nand spectral characteristics of HSIs. Fuet al. (Fu et al. 2015)\npresented an adaptive spatial-spectral dictionary learning\nmethod and high correlations in both domains are exploited.\nIn (He et al. 2019), the spatial non-local similarity and global\nspectral low-rank property were integrated for HSI denois-\ning. Zhang et al. (Zhang et al. 2021) proposed a double low-\nrank matrix decomposition method. These methods gener-\nally formulated HSI denoising as a complex iterative prob-\nlem and required a long time to optimize.\nExisting learning-based HSI denoising methods (Chang\net al. 2018; Yuan et al. 2018; Zhang et al. 2019; Sidorov\nand Yngve Hardeberg 2019) rely on deep CNNs to automat-\nically learn the prior from large-scale datasets. In (Chang\net al. 2018), a spatial-spectral deep residual CNN was em-\nployed for HSI restoration. To exploit the global dependency\nand correlation information in both dimensions, Shi et al.\n(Shi et al. 2021) proposed dual-attention denoising network\nthat could obtain more essential feature extraction of HSI.\nWei et al. (Wei, Fu, and Huang 2020) designed a 3D quasi-\nrecurrent neural network (QRNN3D) to make full use of\nstructural spatial-spectral correlation as well as global corre-\nlation along the spectrum. In (Bodrito et al. 2021), a hybrid\ntrainable spectral-spatial sparse coding model was proposed.\nAlthough these CNN-based HSI denoising methods have\nachieved excellent performance, data similarities in spectral\ndimension is always underestimated. Moreover, with trained\nparameters fixed, convolution filters follow a stereotyped\nparadigm to extract features and lack the adaptability to ex-\nploit the intrinsic similarity characteristic of noisy HSI.\nVision Transformers\nTransformer was first proposed in (Vaswani et al. 2017)\nfor NLP tasks and then was successfully applied to numer-\nous vision tasks, such as image classification (Zhang et al.\n2020), image segmentation (Wang et al. 2021b), object de-\ntection (Dai et al. 2021), and face recognition (Zhong and\nDeng 2021). Generally, Transformer contains three essen-\ntial components, including LayerNorm (LN) module, self-\nattention module, and multi-layer perceptron (MLP) mod-\nule. When Transformer is first applied to visual tasks, the\nwhole image was treated as a sequence of non-overlapping\nmedium-sized image patches in ViT (Dosovitskiy et al.\n2021). To make Transformer more efficient, pyramid vision\nTransformer was proposed in (Wang et al. 2021a). Swin\nTransformer (Liu et al. 2021) utilized a shift operation and\nhad linear computational complexity to image size. In (Za-\nmir et al. 2022), an efficient Transformer for high-resolution\nimage restoration was proposed.\nThough Transformer has achieved excellent performance,\nexisting Transformers mainly focus on exploiting spatial\nsimilarity and lack exploration of spectral correlation. Thus,\ndirectly applying previous Transformers to the HSI task\n(Zhong et al. 2021; Pang, Gu, and Cao 2022) may be less\neffective in exploiting two-dimensional features of HSIs.\nMethod\nIn this section, we first illustrate the problem formulation of\nHSI denoising and the motivation of our work. Then, we de-\nscribe the proposed spatial-spectral multi-head self-attention\nmodule in detail (see Figure 1). Finally, the overall architec-\nture of our proposed SST is provided (see Figure 2).\nMotivation and Formulation\nMathematically, the additive noise degradation model for\none HSI could be formulated as follows\ny = x + n, (1)\nwhere x ‚àà RH√óW√óB stands for the clean HSI, and y ‚àà\nRH√óW√óB is the noisy HSI corrupted by additive noise n.\nH and W stand for spatial height and width, respectively.B\ndenotes the spectral resolution of HSI.\nHSIs in the real-world are usually degraded by multifar-\nious noise, including Gaussian noise, stripe noise, impulse\nnoise, deadline noise, or a mixture of them (Zhang et al.\n2013; Chang et al. 2020). The goal of HSI denoising is to\nrestore the desired clean HSI x from noisy observation y.\nThe effectiveness of non-local spatial similarity prior has\nbeen verified by previous traditional model-based HSI de-\nnoising works (Zhang et al. 2019; Chang et al. 2020). Since\nthese methods use hand-crafted priors, they lack the explo-\nration of statistical information from external datasets. For-\ntunately, the spatial self-attention mechanism could provide\n1369\nLinear\nSoftMax\nC√óH√óW\nTranspose\nC√óC\nH√óW√óC\nq\nv\nk\nH√óW√óC\nLinear\nLinear\nq\nk\nv\nSoftMax Position \nEmbedding\nM√óM√óC\nTranspose\nMM√óMM\nM√óM√óC\nSpatial\nAttention\nWindow\nPartition\nWindow\nReverse\nSpectral\nAttention\nH√óW√óC ùêªùëä\nùëÄ2 M√óM√óC\nùëØùëæ\nùë¥ùüê M√óM√óC H√óW√óC H√óW√óC\nS\nS Shift Operation\nMatrix Multiplication\nElement-wise Addition\nLinear\nFinal Version\nFigure 1: Illustration of Spatial-Spectral Multi-head self-Attention (SSMA). The SSMA module mainly contains non-local\nspatial multi-head self-attention and global spectral multi-head self-attention.\na comparative ability to obtain the non-local similarity of\nimages. What is more, with learnable parameters to obtain\ndiverse feature expression, a network integrated with self-\nattention operation can automatically learn the deep prior\nknowledge from large-scale datasets, and obtains better HSI\ndenoising results. Thus, it is natural to apply the spatial self-\nattention operation to learning-based HSI denoising.\nMoreover, HSI is usually regarded as a data cube and is\nespecially rich in spectral information. The similarities exist\nnot only in the spatial dimension but also in the spectral di-\nmension. Since most existing vision Transformers only con-\nduct spatial self-attention to get representative spatial fea-\ntures, the comprehensive consideration of spatial dimension\nand spectral dimension is lacking. Therefore, in this work,\nwe combine spectral self-attention with spatial self-attention\nto exploit both the non-local spatial similarity and spec-\ntral high correlation of HSIs. With shifted local spatial win-\ndow attention, the non-local neighborhood information be-\nyond the pixel is well exploited with computation efficiency.\nWith global spectral attention, the spectral correlation is well\nmodeled and utilized to benefit the denoising process.\nSpatial-Spectral Transformer\nIn this section, we first introduce our proposed spatial-\nspectral Transformer layer (SSTL) with our designed self-\nattention module. To effectively exploit the non-local spatial\nsimilarity and spectral correlation of target HSI, we propose\na spatial-spectral multi-head self-attention (SSMA) module\nfor HSI denoising. Moreover, according to (Dong, Cordon-\nnier, and Loukas 2021), skip connections and MLP are ben-\neficial to prevent the network from falling into rank collapse.\nThus, we follow the structure proposed in (Dosovitskiy et al.\n2021) with self-attention module followed by MLP layer,\nresidual connections, and LayerNorm.\nConcretely, let Zl‚àí1 ‚àà RH√óW√óC stands for the input\nfeature embeddings of the l-th SSTL, the overall processing\nstructure of SSTL could be expressed as:\nZ‚Ä≤\nl = SSMA(LN(Zl‚àí1)) +Zl‚àí1,\nZl = MLP(LN(Z‚Ä≤\nl)) +Z‚Ä≤\nl, (2)\nwhere Z‚Ä≤\nl and Zl denote the outputs of SSMA and SSTL.\nThe details of our SSMA are illustrated in Figure 1. It\nmainly includes a non-local spatial self-attention (NLSA)\nlayer and a global spectral self-attention (GSA) layer.\nGiven normalized input features, Zin ‚àà RH√óW√óC, a\nwindow partition operation is first conducted in spatial di-\nmension with a window size of M. Thus, the whole input\nfeatures are divided into HW\nM2 non-overlapping patches as\b\nZin\n1 , ...,Zin\ni , ...,Zin\nS\n\t\n, where Zin\ni ‚àà RM2√óC. After parti-\ntion, each patch Zin\ni individually passes through the NLSA\nlayer to exploit the non-local similarity in the spatial dimen-\nsion. This process can be expressed as follows:\n\b\nZin\ni\n\t\n= WinPartition\n\u0000\nZin\u0001\n, i= 1, ..., S (3)\nZns\ni = NLSA\n\u0000\nZin\ni\n\u0001\n, i= 1, ..., S (4)\nZns = WinReverse ({Zns\ni }) , i= 1, ..., S. (5)\nAfter gathering output patches from NLSA layer together\nthrough the window reverse operation, the non-local spatial\nfeatures Zns are fed directly to GSA layer. The global spec-\ntral correlation is utilized and helps the removal of noise as:\nZgs = GSA (Zns) . (6)\nNon-Local Spatial self-Attention (NLSA).For real-world\nHSI denoising applications, there is a fundamental trade-\noff between model capability and model flexibility. Though\nglobal spatial self-attention could bring spatial long-range\ndependencies in a large perspective field, it is frustratingly\nnot suitable for the HSI denoising task since its quadratic\ncomplexity grows with spatial size. Thus, we employ self-\nattention with shifted window to obtain the internal non-\nlocal spatial similarity information beyond neighbors. Con-\nsequently, NLSA layer provides considerable ability to ex-\npress spatial features in linear complexity.\nFor NLSA layer, each input patch Zin\ni ‚àà RM2√óC is lin-\nearly projected into query Qns\ni , key Kns\ni , and value V ns\ni ‚àà\nRM2√óC to increase the representation ability as:\nQns\ni = Zin\ni Wns\nq , Kns = Zin\ni Wns\nk , V ns = Zin\ni Wns\nv ,\n(7)\nwhere Wns\nq , Wns\nk , and Wns\nv are weights of size C √ó C.\nSubsequently, to jointly assemble information from dif-\nferent representation subspaces, multi-head mechanism is\n1370\nconv\nSSTL\n...\nconv\nconv\nconv\nMLP\nLN\nSSMA\nLN\n(a) SST\nRSSB\nRSSB\n(b) RSSB (c) SSTL\nNoisy Input Output\n...\nSSTL\nFigure 2: Overall architecture of SST. (a) The basic pipeline\nof SST. (b) Residual spatial-spectral block (RSSB). (c)\nSpatial-spectral Transformer layer (SSTL).\nemployed. Qns\ni , Kns\ni , and V ns\ni are split into N heads as\nQns\nij , Kns\nij , and V ns\nij , respectively. Thus, the non-local spa-\ntial self-attention matrix for each headns\nij is computed as:\nAns\nij = Softmax(Qns\nij Kns\nij\nT /\n‚àö\nd + B), j= 1, ..., N\nheadns\nij = Ans\nij V ns\nij , j = 1, ..., N,\n(8)\nwhere d is the dimension of Qns\nij , specifically as C/N. And\nB is the relative bias defined in (Liu et al. 2021). After self-\nattention operation, the outputs headns\nij are embedded to-\ngether and linearly projected to get Zns\ni in Eq. (3). More-\nover, we also conduct a spatial shift operation between each\nSSTL to obtain more comprehensive spatial information. It\ncan bring interactions between local windows. Specifically,\nthe shift operation is conducted by shifting the input features\nby ‚åä|M\n2 |, |M\n2 |‚åã pixels before partitioning.\nGlobal Spectral self-Attention (GSA).After the non-local\nspatial self-attention operation, the features are already spa-\ntially representative for HSI. But it still lacks spectral repre-\nsentations. Since HSI has underlying spectral correlations,\nit provides sufficient similarity information for the self-\nattention operation to obtain long-range dependencies. Thus,\nwe employ global spectral self-attention after non-local spa-\ntial self-attention. By doing so, the non-local spatial similar-\nity and spectral correlation are both finely considered.\nFor a single GSA layer, given an input from NSLA layer,\nZns‚ààRH√óW√óC, the input is first transposed and reshaped\ninto ZT ‚ààRC√óHW . Then, it is also linearly projected toQ ‚àà\nRC√óHW , K ‚àà RC√óHW , and V gs ‚àà RC√óHW as:\nQgs = Wgs\nq ZT , Kgs = Wgs\nk ZT , V gs = Wgs\nv ZT , (9)\nwhere Wqs\nq , Wqs\nk , Wqs\nv are weights of size C √ó C.\nSimilar to non-local spatial attention, we split Qgs, Kgs,\nand V gs into N heads. Each headgs\nj is defined by:\nAgs\nj = Softmax(Kgs\nj\nT Qj\ngs/\n‚àö\nd), j= 1, ..., N\nheadgs\nj = V gs\nj Ags\nj , j = 1, ..., N.\n(10)\nThe outputs headgs\nj are concatenated in spectral dimen-\nsion and projected to Zgs in Eq. (6). It is worth emphasizing\nthat Zgs contains more spectral details without losing criti-\ncal spatial information compared to Zns.\nComputational Complexity.We analyze the computational\ncomplexity of our proposed SSMA module as:\nO (NLSA ) =\n\u0000\nM2HWC\n\u0001\n, O (GSA) =\n\u0000\nC2HW\n\u0001\n,\nO (SSMA ) =\n\u0000\nM2HWC + C2HW\n\u0001\n,\n(11)\nwhere M and C are predefined constants. Thus, our pro-\nposed SSMA module achieves linear computation cost.\nOverall Network Architecture.As shown in Figure 2, our\nwhole Transformer first employs one 3√ó3 convolution layer\nto extract low-level features embeddings F0 ‚àà RH√óW√óC\nfrom noisy observation y ‚àà RH√óW√óB. Then, shallow\nfeatures pass through T Residual Spatial-Spectral Block\n(RSSB) layers with a fixed feature size of H√óW√óC. The\nprocess of deep feature extraction could be denoted as:\nFt = Ht(Ft‚àí1), t= 1, ..., T, (12)\nwhere Ht is the t-th RSSB layer and Ft stands for the t-th\nspatial-spectral feature map obtained by RSSB layer.\nThe designed RSSB contains 6 Spatial-Spectral Trans-\nformer layers. Each of RSSB ends up with a 3√ó3 convo-\nlution layer. We have adopted a skip connection that adds\nresidual features from the previous block.\nTo recover from deep feature FT , two 3√ó 3 convolution\nlayers are concatenated with shallow features via skip con-\nnections. With a global skip connection that adds the noisy\ninput to the output, the middle network actually learns a\nnoise pattern that matches the noise distribution of input.\nExperiments\nIn this section, we first introduce the datasets and settings\nused in our experiments. The quantitative metrics and com-\npeting methods are also covered. Then, we provide denois-\ning results quantitatively and qualitatively on simulated data\nand real data. Finally, ablation studies are carried out to an-\nalyze the effectiveness of the components in our model.\nSimulated Experiments\nDatasets. We evaluate our method mainly on ICVL (Arad\nand Ben-Shahar 2016) dataset. It consists of 201 images\nwith 1392√ó1300 spatial resolution and 31 bands from 400\nnm to 700 nm. We follow settings in (Bodrito et al. 2021),\nwhich uses 100 HSIs for training and 50 HSIs for testing\nto ensure pictures captured from the same scene are only\nused once. Specifically, we center crop training images to\nsize 1024√ó1024 and normalize them to [0, 1]. Then, we ex-\ntract patches of size 64√ó 64 at different scales, with strides\n64, 32, and 32. As for testing samples, each HSI is cropped\nto size 512√ó512√ó31 for better visual effects. Normalization\nis also conducted on testing HSIs.\nBenchmark Models. We compare our proposed method\nwith four traditional methods, including BM4D (Maggioni\net al. 2012), LLRT (Chang, Yan, and Zhong 2017), TSLRLN\n(He et al. 2021), and NG-Meet (He et al. 2019). Three deep\nlearning methods are also used for comparison, including\nQRNN3D (Wei, Fu, and Huang 2020), HSID-CNN (Yuan\net al. 2018), and T3SC (Bodrito et al. 2021). It is worth\nmentioning that HSID-CNN method traversed the noisy HSI\n1371\nMethod\n10 30 50 70 10-70\nPSNR SSIM SAM PSNR SSIM SAM PSNR SSIM SAM PSNR SSIM SAM PSNR SSIM SAM\nNoisy 28.13 0.879 18.72 18.59 0.552 37.9 14.15 0.348 49.01 11.23 0.230 56.45 17.24 0.478 41.94\nBM4D (Maggioni\net al. 2012) 40.78 0.993 2.99 37.69 0.987 5.02 34.96 0.985 6.81 33.15 0.955 8.40 36.62 0.977 5.51\nLLRT\n(Chang, Yan, and Zhong 2017) 46.72 0.998 1.60 41.052 0.992 2.52 38.24 0.983 3.47 36.23 0.973 4.46 40.06 0.986 3.24\nTSLRLN(He et\nal. 2021) 46.07 0.998 1.82 41.26 0.994 3.03 38.37 0.989 4.36 36.44 0.983 5.69 40.22 0.991 3.58\nNGMeet (He\net al. 2019) 47.90 0.999 1.39 42.44 0.982 2.06 39.69 0.966 2.49 38.05 0.953 2.83 41.67 0.994 2.19\nHSID-CNN (Y\nuan et al. 2018) 43.14 0.992 2.12 40.30 0.985 3.14 37.72 0.975 4.27 34.95 0.952 5.84 39.04 0.978 3.71\nQRNN3D (W\nei, Fu, and Huang 2020) 45.61 0.998 1.80 42.18 0.996 2.21 39.70 0.9924 3.00 38.09 0.988 3.42 41.34 0.994 2.42\nT3SC (Bodrito\net al. 2021) 45.81 0.998 2.02 42.44 0.996 2.44 40.39 0.993 2.85 38.80 0.990 3.26 41.64 0.994 2.61\nSST (Ours) 48.28 0.999 1.30 43.32 0.997 1.87 41.09 0.995 2.19 39.55 0.992 2.46 42.57 0.996 1.99\nTable 1: Denoising comparisons under Gaussian noise with known variance on ICVL dataset. The best results are in bold.\nGavyam 0823-0950-1 from ICVL\nNoisy BM4D LLRT NGMeet\n(14.15/0.4417) (33.42/0.9786) (36.27/0.9832) (37.95/0.9938)\nQRNN3D T3SC SST (Ours) GroundTruth\n(38.08/0.9940) (38.39/0.9942) (39.29/0.9955) (PSNR/SSIM)\nFigure 3: Visual quality comparison under Gaussian noise level œÉ=50 on ICVL dataset using pseudo color image.\nthrough a one-by-one manner, specifically, inputs of the net-\nwork are the current noisy band and its adjacent bands.\nMetrics. To quantitatively evaluate our method, we employ\nthree performance metrics, including peak signal-to-noise\nratio (PSNR), structure similarity (SSIM), and spectral an-\ngle mapper (SAM). Larger values of PSNR and SSIM imply\nbetter performance, while smaller values of SAM indicate\nthe high fidelity of denoising results.\nNoise Patterns. Following same settings in previous HSI\ndenoising works (Wei, Fu, and Huang 2020; Bodrito et al.\n2021), we evaluate our method on different noise patterns:\n‚Ä¢ i.i.d Gaussian noise with known variance œÉ from 10 to\n70. The noise level is the same on all bands.\n‚Ä¢ Unknown non-i.i.d Gaussian noise with variance œÉ from\n10 to 70. Different bands contain different level of noise.\n‚Ä¢ Non-i.i.d Gaussian noise with deadline noise, impulse\nnoise, stripe noise, or mixture of them. The detailed set-\ntings could be found in (Wei, Fu, and Huang 2020).\nImplementation Details. We use Adam (Kingma and Ba\n2014) to optimize the network with parameters initialized by\nXavier initialization (Glorot and Bengio 2010). The batch\nsize is set to 8 with 100 epochs of training. The learning\nrate is set to 1√ó10 ‚àí4 and is divided by 10 after 60 epoch.\nCompeting deep learning methods (HSID-CNN, QRNN3D,\nand T3SC) and our proposed Transformer are implemented\nwith PyTorch and run with a GeForce RTX 3090. Traditional\nmethods, including BM4D, LLRT, TSLRLN, and NG-Meet,\nare implemented with Matlab and run with an Intel Core i9-\n10850K CPU. All parameters involved in these competing\nalgorithms were optimally assigned or automatically chosen\nas described in the reference papers.\nGaussian Noise with Known Variance.In this case, zero\nmean additive white Gaussian noises with different variance\nœÉ are added to the HSI to generate the noisy observations.\nThe quantitative results of our method and compared\nmethods on ICVL dataset are shown in Table 1. Our method\nsignificantly outperforms all compared methods. With a\nnoise level of œÉ=70, our method increases the PSNR by\nmore than 0.7 dB. Furthermore, it can be seen that compared\ndeep learning methods can achieve comparable results to tra-\nditional methods under high noise levels, but they are less ef-\nfective to handle HSIs with low noise. Our proposed Trans-\nformer still achieve the best results under low level noise,\nshowing its robustness and generalization ability.\nWe show the visual results of one denoised HSI from dif-\nferent methods under œÉ=50 in Figure 3. To further illustrate\nthe result of the spectral fidelity, we use pseudo color im-\n1372\nMethod\nNon-i.i.d Deadline Impulse Stripe Mixture\nPSNR SSIM SAM PSNR SSIM SAM PSNR SSIM SAM PSNR SSIM SAM PSNR SSIM SAM\nNoisy 18.29 0.512 46.20 17.50 0.477 47.55 14.93 0.376 46.98 17.51 0.487 46.98 13.91 0.340 51.53\nBM4D (Maggioni\net al. 2012) 36.18 0.977 5.78 33.77 0.962 6.85 29.79 0.861 21.59 35.63 0.973 6.26 28.01 0.842 23.59\nLLRT\n(Chang, Yan, and Zhong 2017) 34.18 0.962 4.88 32.98 0.956 5.29 28.85 0.882 18.17 34.27 0.963 4.93 28.06 0.870 19.37\nTSLRLN (He\net al. 2021) 41.95 0.994 2.89 36.56 0.977 5.52 33.72 0.893 22.89 38.41 0.985 4.99 27.25 0.852 25.23\nNGMeet (He\net al. 2019) 34.90 0.975 5.37 33.41 0.967 6.55 27.02 0.788 31.20 34.88 0.967 5.42 26.13 0.778 31.89\nHSID-CNN (Y\nuan et al. 2018) 39.28 0.982 3.80 38.33 0.978 3.99 36.21 0.966 5.48 38.09 0.977 4.59 35.30 0.959 6.29\nQRNN3D (W\nei, Fu, and Huang 2020) 42.18 0.995 2.84 41.69 0.994 2.61 40.32 0.991 4.31 41.68 0.994 2.97 39.08 0.989 4.80\nT3SC (Bodrito\net al. 2021) 41.95 0.992 4.18 39.59 0.992 4.86 37.85 0.984 6.53 41.32 0.994 3.27 35.53 0.977 8.12\nSST (Ours) 43.57 0.997 2.05 42.74 0.996 2.21 41.66 0.995 2.60 42.97 0.996 2.15 38.78 0.991 2.99\nTable 2: Denoising comparisons under five complex noise cases on ICVL dataset. The best results are in bold.\nNachal 0823-1152 from ICVL\nNoisy BM4D LLRT NGMeet\n(18.26/0.5703) (33.06/0.9706) (31.91/0.9581) (32.09/0.9681)\nQRNN3D T3SC SST (Ours) GroundTruth\n(38.08/0.9940) (38.39/0.9942) (39.29/0.9955) (PSNR/SSIM)\nFigure 4: Visual quality comparison under deadline noise on ICVL dataset using pseudo color image.\nages that is composed of bands 9, 15 and 28 for the red,\ngreen, and blue channels. BM4D results in spatial discon-\ntinuity and loses many high frequency patterns. NG-Meet\nand LLRT obtain relatively good results, but they still lose\nfine textures as shown in the detailed figures. The results of\nQRNN3D and T3SC have obvious artifacts near the edge of\nthe building. Our method obtains the most satisfying image\nalong the spatial dimension and spectral dimension.\nComplex Noise with Unknown Non-i.i.d Gaussian Noise.\nAverage results of all the competing methods on ICVL with\ncomplex noise are shown in Table 2. The visual compari-\nson under non-i.i.d Gaussian noise + deadline noise case are\nshown in Figure 4. Our method outperforms other methods\nunder various types of noise degradation. An interesting ob-\nservation is that traditional methods almost all fail to restore\nclean HSIs under impulse noise and mixed noise, while deep\nlearning methods achieve considerable results. We specu-\nlate that the introduction of complex noise to HSI makes\nit lose certain informative characteristics across the spatial\nand spectral dimensions. Without the guidance of clean HSI,\nhand-crafted priors are not strong enough to work well on\nnoisy HSI. Our method not only focuses on the similarity\ninformation of HSI, but also has better adaptability to ex-\ntract features through training, thus obtaining better results.\n(a) Outputs of NLSA at layer 1, 3, 5, and 6\n(b) Outputs of GSA at layer 1, 3, 5, and 6\nFigure 5: Feature maps before and after GSA module.\nFeature Representation.In Figure 5, we provide grayscale\nfeatures maps after NLSA layer and GSA layer at differ-\nent stages of our proposed Transformer, respectively. The\noutput of the GSA layer obtains more detailed informa-\ntion and structural texture than the outputs of NLSA layer.\nSince there is a close relationship between bands, different\nbands could complement each other by global spectral self-\nattention, resulting in a better-refined feature expression.\n1373\nNoisy TSLRLN NGMeet HSID-CNN QRNN3D T3SC SST (Ours)\nFigure 6: Visual comparison on real dataset Urban of band 105.\nMethod Param\n(M) GFLOPs PSNR (dB)\nQRNN3D 0.86 19.6 39.70\nQRNN3D-L 1.34 30.6 39.82\nHSID-CNN 0.40 50.8 37.72\nT3SC 0.83 N/A 40.39\nRestormer 26.15 9.5 40.53\nRestormer-L 45.14 21.9 40.68\nSST (Ours) 4.14 20.7 41.09\nTable 3: Complexity comparisons with size 64 √ó 64 √ó 31.\nTime Complexity. Parameters and GFLOPs are provided\nin Table 3 under œÉ=50. QRNN3D-L stands for QRNN3D\nwith deeper layers and more channels compared to the one\nin (Wei, Fu, and Huang 2020). Moreover, we also include\nRestormer (Zamir et al. 2022) for comparison. Similarly,\nRestormer-L stands for a larger one. Besides, since HSID-\nCNN conducts denoising process in band-by-band manner,\nthe GFLOPs is calculated by multiplying the band number\nand time that is required for one band. Our method achieves\nbetter result under comparable computation cost.\nReal Data Experiments\nSetup. We evaluate our method on a real-world noisy HSI\ndataset named Urban. Urban dataset consists of 307√ó307\npixels with 210 bands. Following (Bodrito et al. 2021), our\nTransformer and compared deep models are pre-trained on\nthe APEX (Itten et al. 2008) dataset, which has similar spec-\ntral coverage and band number to the Urban dataset.\nVisual Comparison. As there is no clean image for real\ndata, we only present grayscale images before and after de-\nnoising to visually evaluate competing methods. The results\nof one noisy band are shown in Figure 6. We can observe\nthat the original HSI suffers from complex noise, which se-\nriously affects the image quality. NGMeet fails to preserve\nstructural content and detail texture due to over-smoothing.\nThough QRNN3D and T3SC can recover a relatively com-\nplete image from the noisy band, vertical stripes still exist\nin the restored images. Our method achieves satisfactory de-\nnoising results and restores the image texture well.\nAblation Study\nTo verify the effectiveness of our method, we perform abla-\ntion studies with noise level œÉ=50 on ICVL dataset.\nComponent Analysis.In Table 4, we investigate the effect\nof subcomponents in the SSMA module, which includes a\nMethod Params\n(M) GFLOPs PSNR (dB)\nw/o NLSA 3.00 14.3 34.67\nw/o GSA 2.98 13.1 40.44\nNLSA-NLSA 4.23 20.1 40.56\nGSA-GSA 4.08 21.4 39.82\nGSA-NLSA 4.14 20.7 40.69\nNLSA-GSA (Ours) 4.14 20.7 41.09\nTable 4: Ablation to study the effect of SSMA module.\nWindo\nw Size 2 4 8 16\nGFLOPs 16.87 17.50 20.70 30.24\nPSNR (dB) 42.13 42.38 42.57 42.59\nSSIM 0.9951 0.9953 0.9955 0.9955\nTable 5: Analysis of the effect of window size.\nNLSA layer and a GSA layer. Without NLSA or GSA, the\nPSNR is 0.5 dB lower. To mitigate the impact of computa-\ntional complexity, we replace GSA layer with NLSA layer,\nresulting in a NLSA-NLSA module. Similarly, we perform\nthe experiment on the GSA-GSA module. The results re-\nveal that our attention module outperforms other methods\nin terms of efficiency. GSA-GSA obtains the worst result\namong the last four methods. With NLSA layer before GSA\nlayer, the feature extraction of GSA is more reliable.\nHyperparamter Analysis. To investigate the influence of\nwindow size M in the NLSA layer, we conducted experi-\nments with different sizes of M in Table 5. As M increases,\nthe network gets better performance with higher computa-\ntion cost. To make a better trade-off between performance\nand computation cost, we choose M=8 in our experiments.\nConclusion\nIn this paper, we propose a Spatial-Spectral Transformer for\nhyperspectral image denoising. The proposed Transformer\nconsiders both spatial similarity and spectral correlation in\nHSIs through the spatial non-local self-attention and spectral\nglobal self-attention. The spatial non-local self-attention ex-\nploits the coarse features beyond neighboring pixels. Then,\nthe spectral self-attention enriches the representation with\nmore details. Extensive experiments verify the superiority\nof our method over state-of-the-art methods under various\nnoise degradations quantitatively and visually. In the future,\nit is worth investigating how to use noise estimation as guid-\nance to boost the denoising performance of Transformer.\n1374\nAcknowledgments\nThis work was supported by the National Natural Sci-\nence Foundation of China under Grants No. 62171038, No.\n61827901, and No. 62088101.\nReferences\nArad, B.; and Ben-Shahar, O. 2016. Sparse recovery of hy-\nperspectral signal from natural RGB images. In ECCV, 19‚Äì\n34.\nBodrito, T.; Zouaoui, A.; Chanussot, J.; and Mairal, J. 2021.\nA Trainable Spectral-Spatial Sparse Coding Model for Hy-\nperspectral Image Restoration. NeurIPS.\nCai, Y .; Lin, J.; Hu, X.; Wang, H.; Yuan, X.; Zhang, Y .; Tim-\nofte, R.; and Van Gool, L. 2022. Mask-guided spectral-wise\ntransformer for efficient hyperspectral image reconstruction.\nIn CVPR, 17502‚Äì17511.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In ECCV, 213‚Äì229.\nChang, Y .; Yan, L.; Fang, H.; Zhong, S.; and Liao, W. 2018.\nHSI-DeNet: Hyperspectral image restoration via convolu-\ntional neural network. IEEE Transactions on Geoscience\nand Remote Sensing, 57(2): 667‚Äì682.\nChang, Y .; Yan, L.; Zhao, X.-L.; Fang, H.; Zhang, Z.; and\nZhong, S. 2020. Weighted low-rank tensor recovery for hy-\nperspectral image restoration. IEEE Transactions on Cyber-\nnetics, 50(11): 4558‚Äì4572.\nChang, Y .; Yan, L.; and Zhong, S. 2017. Hyper-laplacian\nregularized unidirectional low-rank tensor recovery for mul-\ntispectral image denoising. In CVPR, 4260‚Äì4268.\nChen, Y .; Guo, Y .; Wang, Y .; Wang, D.; Peng, C.; and He,\nG. 2017. Denoising of hyperspectral images using noncon-\nvex low rank matrix approximation. IEEE Transactions on\nGeoscience and Remote Sensing, 55(9): 5366‚Äì5380.\nCloutis, E. A. 1996. Review article hyperspectral geological\nremote sensing: evaluation of analytical techniques. Inter-\nnational Journal of Remote Sensing, 17(12): 2215‚Äì2242.\nDabov, K.; Foi, A.; Katkovnik, V .; and Egiazarian, K. 2007.\nImage denoising by sparse 3-D transform-domain collabo-\nrative filtering. IEEE TIP, 16(8): 2080‚Äì2095.\nDai, Z.; Cai, B.; Lin, Y .; and Chen, J. 2021. Up-detr: Unsu-\npervised pre-training for object detection with transformers.\nIn CVPR, 1601‚Äì1610.\nDong, W.; Wang, H.; Wu, F.; Shi, G.; and Li, X. 2019.\nDeep spatial‚Äìspectral representation learning for hyperspec-\ntral image denoising. IEEE Transactions on Computational\nImaging, 5(4): 635‚Äì648.\nDong, Y .; Cordonnier, J.-B.; and Loukas, A. 2021. Attention\nis not all you need: Pure attention loses rank doubly expo-\nnentially with depth. In ICML, 2793‚Äì2803.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2021. An image is worth\n16x16 words: Transformers for image recognition at scale.\nIn ICLR.\nDu, B.; Huang, Z.; Wang, N.; Zhang, Y .; and Jia, X. 2018.\nJoint weighted nuclear norm and total variation regulariza-\ntion for hyperspectral image denoising. International Jour-\nnal of Remote Sensing, 39(2): 334‚Äì355.\nElad, M.; and Aharon, M. 2006. Image denoising via\nsparse and redundant representations over learned dictionar-\nies. IEEE TIP, 15(12): 3736‚Äì3745.\nFei, B. 2020. Hyperspectral imaging in medical applica-\ntions. In Data Handling in Science and Technology, vol-\nume 32, 523‚Äì565.\nFu, Y .; Lam, A.; Sato, I.; and Sato, Y . 2015. Adaptive spatial-\nspectral dictionary learning for hyperspectral image denois-\ning. In ICCV, 343‚Äì351.\nGlorot, X.; and Bengio, Y . 2010. Understanding the diffi-\nculty of training deep feedforward neural networks. In Pro-\nceedings of the thirteenth international conference on artifi-\ncial intelligence and statistics, 249‚Äì256.\nHe, C.; Sun, L.; Huang, W.; Zhang, J.; Zheng, Y .; and Jeon,\nB. 2021. TSLRLN: Tensor subspace low-rank learning with\nnon-local prior for hyperspectral image mixed denoising.\nSignal Processing, 184: 108060.\nHe, W.; Yao, Q.; Li, C.; Yokoya, N.; and Zhao, Q. 2019.\nNon-local meets global: An integrated paradigm for hyper-\nspectral denoising. In CVPR, 6868‚Äì6877.\nHe, W.; Zhang, H.; Zhang, L.; and Shen, H. 2015. Total-\nvariation-regularized low-rank matrix factorization for hy-\nperspectral image restoration. IEEE Transactions on Geo-\nscience and Remote Sensing, 54(1): 178‚Äì188.\nItten, K. I.; Dell‚ÄôEndice, F.; Hueni, A.; Kneub ¬®uhler, M.;\nSchl¬®apfer, D.; Odermatt, D.; Seidel, F.; Huber, S.; Schopfer,\nJ.; Kellenberger, T.; et al. 2008. APEX-the hyperspectral\nESA airborne prism experiment. Sensors, 8(10): 6235‚Äì\n6259.\nKersting, K.; Xu, Z.; Wahabzada, M.; Bauckhage, C.; Thu-\nrau, C.; Roemer, C.; Ballvora, A.; Rascher, U.; Leon, J.;\nand Pluemer, L. 2012. Pre-symptomatic prediction of plant\ndrought stress using dirichlet-aggregation regression on hy-\nperspectral images. In Twenty-Sixth AAAI Conference on\nArtificial Intelligence.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980.\nLiu, X.; Bourennane, S.; and Fossati, C. 2012. Denoising of\nhyperspectral images using the PARAFAC model and sta-\ntistical performance analysis. IEEE Transactions on Geo-\nscience and Remote Sensing, 50(10): 3717‚Äì3724.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vision\ntransformer using shifted windows. In ICCV, 10012‚Äì10022.\nLu, T.; Li, S.; Fang, L.; Ma, Y .; and Benediktsson, J. A.\n2015. Spectral‚Äìspatial adaptive sparse representation for\nhyperspectral image denoising. IEEE Transactions on Geo-\nscience and Remote Sensing.\nMaggioni, M.; Katkovnik, V .; Egiazarian, K.; and Foi, A.\n2012. Nonlocal transform-domain filter for volumetric data\ndenoising and reconstruction. IEEE TIP, 22(1): 119‚Äì133.\n1375\nPang, L.; Gu, W.; and Cao, X. 2022. TRQ3DNet: A 3D\nQuasi-Recurrent and Transformer Based Network for Hy-\nperspectral Image Denoising. Remote Sensing, 14(18):\n4598.\nRenard, N.; Bourennane, S.; and Blanc-Talon, J. 2008. De-\nnoising and dimensionality reduction using multilinear tools\nfor hyperspectral images. IEEE Geoscience and Remote\nSensing Letters, 5(2): 138‚Äì142.\nShi, Q.; Tang, X.; Yang, T.; Liu, R.; and Zhang, L. 2021.\nHyperspectral image denoising using a 3-D attention denois-\ning network. IEEE Transactions on Geoscience and Remote\nSensing, 59(12): 10348‚Äì10363.\nSidorov, O.; and Yngve Hardeberg, J. 2019. Deep hyper-\nspectral prior: Single-image denoising, inpainting, super-\nresolution. In ICCV Workshops.\nThai, B.; and Healey, G. 2002. Invariant subpixel material\ndetection in hyperspectral imagery. IEEE Transactions on\nGeoscience and Remote Sensing, 40(3): 599‚Äì608.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017.\nAttention is All you Need. In NeurIPS, volume 30.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021a. Pyramid vision trans-\nformer: A versatile backbone for dense prediction without\nconvolutions. In ICCV, 568‚Äì578.\nWang, X.; Girshick, R.; Gupta, A.; and He, K. 2018. Non-\nlocal neural networks. In CVPR, 7794‚Äì7803.\nWang, Y .; Xu, Z.; Wang, X.; Shen, C.; Cheng, B.; Shen, H.;\nand Xia, H. 2021b. End-to-end video instance segmentation\nwith transformers. In CVPR, 8741‚Äì8750.\nWei, K.; Fu, Y .; and Huang, H. 2020. 3-D quasi-recurrent\nneural network for hyperspectral image denoising. TNNLS,\n32(1): 363‚Äì375.\nYuan, Q.; Zhang, L.; and Shen, H. 2012. Hyperspectral\nimage denoising employing a spectral‚Äìspatial adaptive to-\ntal variation model. IEEE Transactions on Geoscience and\nRemote Sensing, 50(10): 3660‚Äì3677.\nYuan, Q.; Zhang, Q.; Li, J.; Shen, H.; and Zhang, L. 2018.\nHyperspectral image denoising employing a spatial‚Äìspectral\ndeep residual convolutional neural network. IEEE Transac-\ntions on Geoscience and Remote Sensing, 57(2): 1205‚Äì1218.\nZamir, S. W.; Arora, A.; Khan, S.; Hayat, M.; Khan, F. S.;\nand Yang, M.-H. 2022. Restormer: Efficient transformer for\nhigh-resolution image restoration. In CVPR, 5728‚Äì5739.\nZhang, D.; Zhang, H.; Tang, J.; Wang, M.; Hua, X.; and Sun,\nQ. 2020. Feature pyramid transformer. In ECCV, 323‚Äì339.\nZhang, H.; Cai, J.; He, W.; Shen, H.; and Zhang, L. 2021.\nDouble low-rank matrix decomposition for hyperspectral\nimage denoising and destriping. IEEE Transactions on Geo-\nscience and Remote Sensing, 60: 1‚Äì19.\nZhang, H.; He, W.; Zhang, L.; Shen, H.; and Yuan, Q. 2013.\nHyperspectral image restoration using low-rank matrix re-\ncovery. IEEE Transactions on Geoscience and Remote Sens-\ning, 52(8): 4729‚Äì4743.\nZhang, Q.; Yuan, Q.; Li, J.; Liu, X.; Shen, H.; and Zhang, L.\n2019. Hybrid noise removal in hyperspectral imagery with\na spatial‚Äìspectral gradient network. IEEE Transactions on\nGeoscience and Remote Sensing, 57(10): 7317‚Äì7329.\nZhong, Y .; and Deng, W. 2021. Face transformer for recog-\nnition. arXiv preprint arXiv:2103.14803.\nZhong, Z.; Li, Y .; Ma, L.; Li, J.; and Zheng, W.-S. 2021.\nSpectral‚Äìspatial transformer network for hyperspectral im-\nage classification: A factorized architecture search frame-\nwork. IEEE Transactions on Geoscience and Remote Sens-\ning, 60: 1‚Äì15.\nZhuang, L.; and Bioucas-Dias, J. M. 2018. Fast hyperspec-\ntral image denoising and inpainting based on low-rank and\nsparse representations. IEEE Journal of Selected Topics\nin Applied Earth Observations and Remote Sensing, 11(3):\n730‚Äì742.\n1376",
  "topic": "Hyperspectral imaging",
  "concepts": [
    {
      "name": "Hyperspectral imaging",
      "score": 0.8001276850700378
    },
    {
      "name": "Artificial intelligence",
      "score": 0.690041184425354
    },
    {
      "name": "Computer science",
      "score": 0.6817964911460876
    },
    {
      "name": "Preprocessor",
      "score": 0.6283524036407471
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.6242979764938354
    },
    {
      "name": "Noise reduction",
      "score": 0.5851207971572876
    },
    {
      "name": "Self-similarity",
      "score": 0.4238385260105133
    },
    {
      "name": "Computer vision",
      "score": 0.41538023948669434
    },
    {
      "name": "Mathematics",
      "score": 0.18468374013900757
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I125839683",
      "name": "Beijing Institute of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I35440088",
      "name": "ETH Zurich",
      "country": "CH"
    }
  ],
  "cited_by": 76
}