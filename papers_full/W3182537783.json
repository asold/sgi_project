{
  "title": "Re-Transformer: A Self-Attention Based Model for Machine Translation",
  "url": "https://openalex.org/W3182537783",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4223004542",
      "name": "Huey-Ing Liu",
      "affiliations": [
        "Fu Jen Catholic University"
      ]
    },
    {
      "id": "https://openalex.org/A2394633132",
      "name": "Wei‐Lin Chen",
      "affiliations": [
        "Fu Jen Catholic University"
      ]
    },
    {
      "id": "https://openalex.org/A4223004542",
      "name": "Huey-Ing Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2394633132",
      "name": "Wei‐Lin Chen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6737778391",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W2983669550",
    "https://openalex.org/W3035691519",
    "https://openalex.org/W2986562961",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2888520903",
    "https://openalex.org/W2964093309",
    "https://openalex.org/W2963736842",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2257408573",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W1724438581",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2792643794"
  ],
  "abstract": "Machine translation is one of the most popular and hardest tasks in Natural Language Processing. This paper proposes a self-attention based model for machine translation, named Re-Transformer, by transforming the Transformer [1]. Different from prevailing approach through module or GPU stacking to improve the system performance, Re-Transformer modifies the basic architecture; there are four refinements as follows. First, Re-Transformer adopts sub-word Tokenization in corpus preprocessing to overcome rare words. In the encoder layer, dual Self-Attention stacks and less Point Wise Feed Forward layer are adopted to obtain better comprehension of an input sentence. In decoder, reduced the stack of \"Decoder\" are used to speedup training and inferring speed meanwhile keeps the same level of BLEU. The experiment results show that Re-Transformer with BLEU metric score 31.36, 38.45 (four-layer decoder) and 32.14, 55.62 (two-layer decoder) improves around 4 and 17 points of BLEU metric against the Transformer over the WMT 2014 English-German and English-French Translation Corpus.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8971089720726013
    },
    {
      "name": "Machine translation",
      "score": 0.7685203552246094
    },
    {
      "name": "Transformer",
      "score": 0.7654223442077637
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5386391282081604
    },
    {
      "name": "Preprocessor",
      "score": 0.4953714609146118
    },
    {
      "name": "BLEU",
      "score": 0.47483164072036743
    },
    {
      "name": "Natural language processing",
      "score": 0.470580518245697
    },
    {
      "name": "Encoder",
      "score": 0.4653025269508362
    },
    {
      "name": "Sentence",
      "score": 0.4541867971420288
    },
    {
      "name": "Speech recognition",
      "score": 0.42022615671157837
    },
    {
      "name": "Voltage",
      "score": 0.13808125257492065
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I114150738",
      "name": "Fu Jen Catholic University",
      "country": "TW"
    }
  ],
  "cited_by": 36
}