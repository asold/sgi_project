{
  "title": "A hybrid approach for text summarization using semantic latent Dirichlet allocation and sentence concept mapping with transformer",
  "url": "https://openalex.org/W4386999869",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5114084902",
      "name": "Bharathi Mohan Gurusamy",
      "affiliations": [
        "Amrita Vishwa Vidyapeetham"
      ]
    },
    {
      "id": "https://openalex.org/A5113007514",
      "name": "Prasanna Kumar Rengarajan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1945996508",
      "name": "Parthasarathy Srinivasan",
      "affiliations": [
        "Oracle (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3042185737",
    "https://openalex.org/W2479531437",
    "https://openalex.org/W3175875420",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3105952014",
    "https://openalex.org/W6761268247",
    "https://openalex.org/W2962972512",
    "https://openalex.org/W2952138241",
    "https://openalex.org/W3006440582",
    "https://openalex.org/W2970263339",
    "https://openalex.org/W2996057367",
    "https://openalex.org/W2735674392",
    "https://openalex.org/W2559013665",
    "https://openalex.org/W2940745042",
    "https://openalex.org/W2799149803",
    "https://openalex.org/W2985619053",
    "https://openalex.org/W2945884522",
    "https://openalex.org/W2972704643",
    "https://openalex.org/W2612920290",
    "https://openalex.org/W3035050380",
    "https://openalex.org/W3175518973",
    "https://openalex.org/W4285156758",
    "https://openalex.org/W3184381422",
    "https://openalex.org/W2773170071",
    "https://openalex.org/W6753920505",
    "https://openalex.org/W3175155790",
    "https://openalex.org/W4236900068",
    "https://openalex.org/W3170490008",
    "https://openalex.org/W3159259047",
    "https://openalex.org/W2800631850",
    "https://openalex.org/W3123362069",
    "https://openalex.org/W2896159974",
    "https://openalex.org/W4306318603",
    "https://openalex.org/W4304166298",
    "https://openalex.org/W2896739098",
    "https://openalex.org/W2924690340",
    "https://openalex.org/W2888886397"
  ],
  "abstract": "&lt;span lang=\"EN-US\"&gt;Automatic text summarization generates a summary that contains sentences reflecting the essential and relevant information of the original documents. Extractive summarization requires semantic understanding, while abstractive summarization requires a better intermediate text representation. This paper proposes a hybrid approach for generating text summaries that combine extractive and abstractive methods. To improve the semantic understanding of the model, we propose two novel extractive methods: semantic latent Dirichlet allocation (semantic LDA) and sentence concept mapping. We then generate an intermediate summary by applying our proposed sentence ranking algorithm over the sentence concept mapping. This intermediate summary is input to a transformer-based abstractive model fine-tuned with a multi-head attention mechanism. Our experimental results demonstrate that the proposed hybrid model generates coherent summaries using the intermediate extractive summary covering semantics. As we increase the concepts and number of words in the summary the rouge scores are improved for precision and F1 scores in our proposed model.&lt;/span&gt;",
  "full_text": "International Journal of Electrical and Computer Engineering (IJECE)  \nVol. 13, No. 6, December 2023, pp. 6663~6672 \nISSN: 2088-8708, DOI: 10.11591/ijece.v13i6.pp6663-6672 ÔÅ≤     6663  \n \nJournal homepage: http://ijece.iaescore.com \nA hybrid approach for text summarization using semantic \nlatent Dirichlet allocation and sentence concept mapping with \ntransformer \n \n \nBharathi Mohan Gurusamy1, Prasanna Kumar Rengarajan1, Parthasarathy Srinivasan2 \n1Department of Computer Science Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Chennai, India  \n2Oracle SGSISTS, Lehi, United States \n \n \nArticle Info  ABSTRACT \nArticle history: \nReceived Oct 11, 2022 \nRevised Mar 13, 2023 \nAccepted Apr 3, 2023 \n \n Automatic text summarization generates a summary that contains sentences \nreflecting the essential and relevant information of the original documents. \nExtractive summarization requires semantic understanding, while abstractive \nsummarization requires a bette r intermediate text representation. This paper \nproposes a hybrid approach for generating text summaries that combine \nextractive and abstractive methods. To improve the semantic understanding \nof the model, we propose two novel extractive methods: semantic l atent \nDirichlet allocation (semantic LDA) and sentence concept mapping. We then \ngenerate an intermediate summary by applying our proposed sentence \nranking algorithm over the sentence concept mapping . This intermediate \nsummary is input to a transformer -based abstractive model fine-tuned with a \nmulti-head attention mechanism. Our experimental results demonstrate that \nthe proposed hybrid model generates coherent summaries using the \nintermediate extractive summary covering semantics.  As we increase the \nconcepts and number of words in the summary the rouge scores are \nimproved for precision and F1 scores in our proposed model. \nKeywords: \nHybrid model \nSemantic latent Dirichlet \nallocation \nSentence concept mapping \nText summarization \nTransformer \nThis is an open access article under the CC BY-SA license. \n \nCorresponding Author: \nBharathi Mohan Gurusamy \nDepartment of Computer Science Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham  \nChennai, India  \nEmail: g_bharathimohan@ch.amrita.edu \n \n \n1. INTRODUCTION \nToday‚Äôs world is full of information, mostly from web articles [1]. Users read the articles on the \nweb based on their requirements and need to process the data further. Users need to read one or more articles \nmany times to understand and comprehend the required information. The main goal of a text summarizer is to \napply some methods and natural language processing (NLP) to reduce the original data in text documents. \nWhen generating a summary, we reduce the content of the original documents without compromising their \nmain concepts [2]. The summary we generate from a large document helps the user to skim the documents, \nsaving them time.  Text summar ization is a challenging task that has been studied extensively, and the \napproaches used for this task can be broadly classified into three categories: extractive, abstractive, and \nhybrid summarizers [1]. Extractive summarization techniques extract information from the original \ndocument‚Äôs content and arrange the sentences to provide a summary. Ranking sentences in a document \ninvolves statistical and semantic approaches, which assign a weight to each sentence based on its position in \nthe ordered list. \nIn contrast, abstractive summariz ation approaches aim to create a semantic and meaningful \nsummary by generating new sentences that convey essential information from the original document(s) using \n\n      ÔÅ≤          ISSN: 2088-8708 \nInt J Elec & Comp Eng, Vol. 13, No. 6, December 2023: 6663-6672 \n6664 \nnatural language generation techniques [2]. These techniques are more widely used than extractive \nsummarization approaches to create concise, informative, and readable summaries.  As the name suggests, \nhybrid summarizers combine extractive and abstractive approaches to leverage their respective strengths [3]. \nFor instance, a hybrid approach may generate an extractive summary using semantic and statistical methods \nand refine it using an abstractive summarization model. The hybrid method can produce a more coherent and \ninformative summary than either method alone. BART, T5, Marian, and mBART are examples of \ntransformer models commonly used for tasks such as summarization, question answering, and translation  [4]. \nTransformers can accomplish NLP tasks such as sentence and word classification, text generation, text \nanswer extraction, and sentence development. \nAn abstractive summarizer creates a summary by learning the essential concepts in the original text. \nIt is also called natural language generation -the mostly encoder -decoder neural networks used along with \nsome attention models in abstractive summarization. Abstractive summarization generates a document \nsummary based on its content, using natural language techniques rather than representing the original \ndocument in an intermediate format. It is worth noting that hugging face transformer has also been used in \nabstractive text summarization in recent times [5]. A hybrid summarizer [6] takes advantage of extractive and \nabstractive approaches, the extractive model initially fed the original text to obtain a summary based on a \nstatistical measure. The summary generated by this approach relies solely on the word count or percentage of \nthe original text. The abstractive model then further refines the initial summary generated by the extractive \nmodel. To generate the final summary, the summary obtained from the extractive model serves as input to the \nabstractive model. \nThe proposed method takes advantage of two powerful approaches: extractive and abstractive. The \ninput text is first fed into the extractive model to obtain extracted content. The paper used the semantic latent \nDirichlet allocation (semantic LDA)  approach to find the hidden topics and several concepts in Wikipedia \narticles. This paper applied a sentence concept mapping strategy to map articles ‚Äô sentences to different \nWikipedia concepts. Since one or more concepts may map onto many sentences, a sentence ranking \nalgorithm retrieves highly ranked sentences from other concepts. The intermediate summary generated from \nthe extractive approach is more semantically related and c overs different article concepts. The content is \nfurther generalized using the abstractive model. Our experimental results on real -world data show that the \nproposed hybrid semantic model achieves better competitive results over extractive and abstractive models. \n \n \n2. LITERATURE REVIEW \nExtractive text summary generation using neural networks proposed in recent times; one such \napproach is BERTSUM [7]. The author proposed a variant of BERT for extractive text summarization to \nlearn complex features. A novel training algorithm, which optimizes the ROUGE matrices through \nreinforcement learning objectives, improves the performance of summarizers [8]. The algorithm trains the \nneural network model on convolutional neural network ( CNN) and Daily Mail datasets. SummaRuNNer [9] \nis a simple recurrent network-based sequence classifier that treats extractive summarization as a classification \nproblem. The algorithm processes every sentence in the original document in document order and decides \nwhether it can be part of the final summary. An extractive model using Restricted Boltz Machine [10] was \nused to enhance the selected features of sentences. Enhanced features help to provide better sentence scores \nto choose sentences that are part of the summary. The sentences are represented as continuous vectors [11] to \nmake them a semantically aware repr esentation for finding similarities between sentences. The constant \nvector representation is handy for multi -document summarization. It employs the feed forward neural \nnetwork by using a set window of words as input and predicting the next term. \nSuleiman and Awajan [12] has comprehensively reviewed deep learning -based text summarization \napproaches, including datasets and evaluation metrics. It helps understand the importance of deep learning in \nextractive summarization. The methods include  a restricted Boltzmann machine (RBM), recurrent neural \nnetwork (RNN), convolutional neural network (CNN) , and variation auto -encoder (VAE). The datasets used \nfor evaluating and training were DUC  2006, DUC  2007, DUC  2002, Daily Mail, SKE, BC3 datasets, and  \nEssex Arabic Summaries Corpus (EASC) . ROUGE metrics are more frequently used as the evaluation \nmeasure that evaluates most approaches. Neural network model sentence relation -based summarization \nSRSum [13] learns sentence relation features from data. The model uses five sub models: PriorSum uses a \nconvolutional neural network to understand the sentence ‚Äôs meaning. SFSum models surface information \nusing sentence length and position, CSRSum considers the relation of a sentence with its local context, \nTSRSum models the semantic relationship between sente nces and titles, and QSRSum assigns weights to \nrelevant queries to capture the relation between query and sentence. \nContextual relation-based summarization [13] is another neural network model that learns sentence \nand context representation. U sing a two -level attention mechanism, it retains the similarity scores between \nInt J Elec & Comp Eng  ISSN: 2088-8708 ÔÅ≤ \n \n A hybrid approach for text summarization using semantic latent Dirichlet ‚Ä¶ (Bharathi Mohan Gurusamy) \n6665 \nsentences and context. Multiview CNN [14] enhanced version of CNN is used to find the crucial features of \nsentences. Word embedding is used to train the model and features of sentences lea rned to rank the \nsentences. Sentence position embedding is also used to increase the learning capacity of the neural model. \nTest summarization can be considered classification [15] by a multi -modal RNN model using the sentence -\nimage classification method. It creates a summary of documents with images. The prop osed method encodes \nsentences and words in the RNN model and the image set is encoded with the CNN model. It uses a logistic \nclassifier for selecting sentences based on their probability and sentence -image alignment probability. \nA new deep neural network (DNN) model for extractive summarization sentences and words from \nalternating pointer networks (SWAP-NET) [16] used encoder -decoder architecture for selecting essential \nsentences. The a rchitecture uses keywords in the selection of sentences. The attention -based mechanism is \nused to learn important words and sentences. CNN/DM [17] used the approach fo r dividing the training set \nbased on its domain. CNN/DM achieves significant improvement in training the neural network BERT. The \nauthor explored constituent and style factors to analyze their effect on the generalization of neural \nsummarization models. Th ey examined how different model architectures; pre -training strategies react to \ndatasets. Some combined supervised learning with unsupervised learning to measure the importance of a \nsentence in the document [18]. The author used thr ee methods: the first used a graph and a supervised model \nseparately and then combined them to assign a score to the sentence. The second method evaluated the \nimportance of sentences by using the graph model as an independent feature of the supervised mode l. The \nthird model used a priori value to the graph model to score the sentences using a supervised approach.  \nMulti-document summarization using deep learning architecture as a hybrid model [19] generates \ncomprehensive summaries from news articles on specific topics. The architecture performed better than the \ntraditional extractive model when evaluated using DUC  2004 data. Extracting the gist  of documents is \npossible by using information such as titles, image captions, and side headings [20]. The author has proposed \na single -document summarizer framework with a hierarchical document encoder with attention to side \ninformation. The extractive summarization framework with side information generates a better summary with \nfluency. Another framework matches extracted summary with the original document in semantic space  [21] \nand models sentence relationships. It  also provides a deep analysis of the gap between summary -level and \nsentence-level extractors based on the features of a dataset. \nOne of the main driving forces in recent development in abstractive text summarization is the \navailability of new neural archi tectures and new strategies in training. However, there is a need to address \nissues such as a proper model and data analysis tool and understanding the failure model of summarization. \nSummVis [22], an open -source tool, allows us to visualize, generate a summary, and analyze the \nsummarization models and the evaluation metrics used. Topic modeling has been recently used in text \nsummarization to identify hidden topics in the document [23]. Latent Dirichlet allocation (LDA) performs \nbetter than latent semantic analysis (LSA) if the number of features increases in the sentences. A hybrid \napproach for text summarization [24] proposed a novel sentence scoring method for extractive \nsummarization. The sentence scoring parameter significantly impro ves the performance of the model. The \nresearchers presented a single -document text summarization technique based on sentence similarity and \ndocument context [25]. Their approach utilized undirected graph -based scoring to evaluate sentences and \ndetermine which ones should be included in the summary. Extractive text summarization [26] considers \nsentence length, position, cue phrases, and cohesion when selecting sentences for summarization. In recent \nyears, the use of neural networks for text summarization has become widespread, as these models can  learn \nsentence patterns. \nMostly used deep learning method is the recursive neural network (RNN) [27]. Long short-term \nmemory (LSTM), gated recurrent units (GRU), and transformers were other approaches for solving gradient \ndisappearance. The extractive m ethod has given results since it can easily combine many techniques and \nimprove performance. Using content attention, two -phase multi -document summarization [28] extracts \nsubtopics from documents. The summary was formulated using different sub -topics as an optimization \nproblem of minimizing the sentence distance. Huang et al. [29] employed Hepos, a novel encoder -decoder \nattention, to extract features from original documents. They also introduced a new dataset called GovReport, \nwhich includes lengthy documents and their corresponding summaries. The evaluation model for text \nsummarization h as its shortcoming in using neural networks [30]. The author has tried to overcome the \nshortcomings of evaluation metrics of text summarization in five dimensions. He re -evaluated the metrics \nusing neural networks and benchmarked metrics using recent text summarization models. Sentence \nregression [31] identifies essential features that represent the sentences. The sentence relation such as \ncontextual sentence relations, query sentence relations, and title sentence relations are used to extract basic \nsentences from the documents. \nTraining a neural network for text summarization has some difficulty processing large documents. \nWe can replace phrases with general terms in semantic content generalization. Some used the pointer \ngenerator network [32], copying the original content and combining the  semantic content generalization. \n      ÔÅ≤          ISSN: 2088-8708 \nInt J Elec & Comp Eng, Vol. 13, No. 6, December 2023: 6663-6672 \n6666 \nThere are many attention -based mechanisms to generate an article summary; one such method is attentive \nencoder-based summarization (AES) combined with unidirectional recurrent neural network (Uni-AES) [33]. \nMohan et al. [34], [35] also compared the performance of bidirectional recurrent neural network (Bi-AES) \nwith Uni-AES and experiment results showed that Bi-AES shows better results than Uni-AES. \n \n \n3. METHOD \n3.1.  Dataset \nThe WikiHow  dataset is a publicly available dataset [36] comprising articles and their \ncorresponding summaries collected from the WikiHow website. The dataset includes over 200,000 articles \nwith their respective summari es, making it one of the most extensive datasets available for text \nsummarization research. Each article in the WikiHow dataset consists of a title, a summary, a list of steps or \ninstructions, and an optional image. The summaries in the dataset are written  by the authors of the articles \nand are intended to provide a brief overview of the article‚Äôs content. \n \n3.2.  Hybrid model \nThe proposed system has built a hybrid text summarization model combining the best features of an \nextractive and an abstractive summa rizer. Figure 1  shows the overall architecture diagram for out proposed \nmethodology. First, the system has extracted data from Wikipedia prepossessed and given it as input to the \nextractive summarizer. Then it needs to identify related concepts of the Wiki pedia page, and then it has to \napply LDA to know the topics present in the articles by suing semantic LDA. The model will perform \nsentence concept to find the map each sentence with concepts identified in the semantic LDA process. The K \nconcepts are chosen  and sentence are ranking using our sentence ranking algorithm and top N sentences are \nchosen as part of the intermediate extractive summary. The intermediate extractive summary given as input \nto our T5 transformer which is fined tuned to produce the abstractive summary.  \n \n \n \n \nFigure 1. Hybrid approach for text summarization \n \n \nThe semantic LDA is the probabilistic model tries to learn the distributions of the topic using two \ndistribution parameters. Those two parameters are the word distribution parameter w and the document \ndistribution parameter d utilizing the expression. The algorithm 1 for embedding semantics LDA is as: \n \nAlgorithm 1. Semantic LDA \n1. Let the number of topics K \n2. for every document ùê∑(ùëë1,ùëë2,‚Ä¶ùëëùëõ}. \n3. for each word w in Document D \nAssign randomly one of the topics. \nRepresents topic of all n documents and distribution of word of all the K topics \nCalculate the probability of words reflecting a topic ùëù(ùêæ|ùê∑) \nCalculate the probability of words w assigned to K topics in D as ùëù(ùë§|ùêæ) \nAdjust the association of the topic to w with ùëù(ùêæ|ùê∑)‚àóùëù(ùë§|ùêæ). \n4. Until convergence \n\nInt J Elec & Comp Eng  ISSN: 2088-8708 ÔÅ≤ \n \n A hybrid approach for text summarization using semantic latent Dirichlet ‚Ä¶ (Bharathi Mohan Gurusamy) \n6667 \n3.2.1. Extractive text summarization \nIn the extractive approach, the system used the genism python library, which extracts semantic \ntopics from input documents. Gensim is an open -source vector space and topic modeling toolkit that \nimplements the TextRank algorithm. Text summarization using ge nism uses a summarizer based on the text \nrank algorithm. TextRank algorithm ranks sentences by constructing a graph model. It builds a graph \nrepresentation of text using keyword extraction and sentence extraction. Since the TexkRank algorithm is \nbetter suited for sentence extraction, it will rank sentences considering each sentence as vertex and edge as \nthe relationship between sentences. The summarizer can produce a summary based on word count and the \nratio of summary based on the original document. \nThe input document is represented as D; the text document is parsed into sentences and mapped to \nWikipedia concepts. The sentences are prepossessed and described as queries; based on the query, Wikipedia \narticles are extracted and represented as concepts. After obtaining D‚Äôs sentence-concept mapping, the system \nhas to find some sentence overlap across one or more wiki concepts. The model represents sentence concept \nmapping as two sets of nodes, one as a set of document sentences and another as wiki concepts. It c an be \nviewed as a bipartite graph, where the edge between the document sentence node and wiki concept node \nrepresents a sentence -concept mapping. The sentences are ranked based on sentence -concept mapping, and \nsentences mapped to more concepts are selected  first. The sentence is ranked based on the decreasing order \nof their mapping degree. The sentence with a ranking of more than some k is chosen to be part of the \nsummary. The value of K is changed from 2 to 5 and captures the summary generated from our mod el. \nAlgorithm 2 shows the algorithm of sentence ranking. \n \nAlgorithm 2. Sentence ranking \nInput: Sentence-Concept (S-C) mapping \nOutput: Sentence/concept Score and Sentence Ranks \n1. Initialize rank of Sentence (si )=a; concept (ci)=b \n2. loop until convergence (k=1...10) \n3.  Compute b as the sum of sentences belonging to the set  \n4.  Compute an as the sum of all concepts belonging to the set \n5. Normalize a \n6. end loop \n7. Rank sentences in descending order of scores r=desc (a) \n \n3.2.2. Abstractive summarization \nThe output of an extractive summarizer using genism extracts sentences and forms the top k ranked \nsentence as a summary Figure 2. The final summary depends upon the  word count and the ratio parameter is \npassed in the summarizer. The output of the extractive summarizer is given as input to the more abstract \nsummary model. Here the paper used an abstractive summarizer based on hugging face transformers . That \nproduces a  summary using entirely different text. The model creates new sentences in a new form using \ntransformers. Hugging face transformers provide many pre -trained models for major NLP tasks, including \ntext summarization, classification, machine translation, text  generation, and chatbot. The most \nstraightforward implementation involves using transformers as a pipeline application programming interface \n(API) in the summary model. Summarization is given a task to the pipeline to download model architecture, \nweights, and token level configurations. \n \n \n \n \nFigure 2. Extractive text summarization \n\n      ÔÅ≤          ISSN: 2088-8708 \nInt J Elec & Comp Eng, Vol. 13, No. 6, December 2023: 6663-6672 \n6668 \nThe proposed system configured the model to generate a summary using the T5 transformer model and \nits tokenizer, shown in Figure 3. First, the model needs to encode the text to tensors of integers using tokenizers. \nTransformers are also used for machine translation apart from text summarization. So, it can be used to convert \ntext from English to German by specifying the parameters in the model. Many parameters are passed to \ngenerate the model: max_length for determining the number of tokens, min_length specifies a minimum number \nof tokens to generate, and length_penalty is used to increase the size of the output. Another parameter system \nthat can change is num_beams, used to set the model for beam search. The last parameter is early_stoppping, set \nto True to make generation finish when the beam search reaches the end of the string token. \n \n \n \n \nFigure 3. Abstractive summarizer using T5 transformer \n \n \n3.2.3. Transformer model  \nAny transformer model can be loaded and trained through a single API, providing ease of  work, \nflexibility, and simplicity. The pipeline used in transformers is grouped into three categories: preprocessing, \ninput to model, and post-processing. Since transformers cannot process raw text, the model must first convert \ntext into numbers. The mode l needs a tokenizer for identifying tokens; each token is mapped to an integer. \nTransformers have some pre -trained models, such as base modules, where the output will be a hidden state \ncalled features for a given input. The production of the base module wi ll be significant with three \ndimensions; where the first one is batch size represents the number of sentences processed. The second \ndimension represents the length of a sequence, sequence length. The last dimension is hidden size: the length \nof hidden feat ures or vectors. Model process the high -dimension features to different and lower dimensions \nfor text summarization. The output from the transformer model is processed by the model head using \narchitecture for text summarization. The model head can be confi gured for summary length regarding the \nnumber of words or percentage concerning the original summary. \n \n \n4. EXPERIMENTAL RESULTS AND DISCUSSION \nThe proposed system built a hybrid text summarizer model where the output of the extractive \nsummarizer will be input  for the abstractive summarizer. For the input, the model considered the Wikipedia \ncontent for text summarization. The model initially used the Genism model for summary generation. Gensim \nis available as a python package and uses a text rank algorithm. NLP  preprocessing is applied over the \nextracted Wikipedia content using a package as a pipeline trained on web text such as blogs, news, and \ncomments. The proposed algorithm semantic LDA is applied as topic modeling to identify the hidden topics \nin the articl es. The sentence is ranked based on the concept mapping of sentences. The summarizer method \nuses sentence concept mapping in the summary model. The generated extractive summary using our model \nhas been compared with state -of-the-art text summarization meth ods, such as Seq -to-Seq with an attention \nmechanism and the Text Rank-based summary model. The performance of our extractive summarization \nmodel shows better results when using the longest common subsequence, as shown in Table 1.  \nThe summary can also be gen erated by specifying the desired number of concepts, K, to be present \nin the generated summary. The resulting summary will be an extractive summary ranked based on statistical \napproaches, such as the number of concepts covered and semantic approaches, such  as the number of topics \ncovered in the document. The output summary will consist of the top -ranked sentences. Table 2 shows the \naverage Rouge-1 and Rouge-2 scores for different sentence concept mapping K numbers. The sentence which \n\nInt J Elec & Comp Eng  ISSN: 2088-8708 ÔÅ≤ \n \n A hybrid approach for text summarization using semantic latent Dirichlet ‚Ä¶ (Bharathi Mohan Gurusamy) \n6669 \ncaptures most of the co ncepts gives better results across all the summarizers. Our proposed approach for \nextracted summarization using topic modeling and sentence -concepts mapping shows better results in \noverall rouge scores. \n \n \nTable 1. Comparison of performance of summarization models on WikiHow dataset  \n Models \nMetrics Seq-to-seq with attention TextRank Semantic LDA-based extractive summarizes \nRouge-1 22.04 27.53 27.10 \nRouge-2 6.27 7.4 6.98 \nRouge-L 20.87 20.00 25.34 \n \n \nTable 2. Average Rouge 1 and Rouge-2 scores for concepts K \nSummarizer K=2 K=3 K=5 \nRouge 1 Rouge-2 Rouge 1 Rouge-2 Rouge 1 Rouge-2 \nFirst few sentences 0.45 0.22 0.46 0.22 0.48 0.24 \nRandom sentences 0.41 0.15 0.44 0.17 0.45 0.19 \nBest sentences 0.51 0.29 0.49 0.28 0.5 0.26 \nProposed (Wiki concept) 0.46 0.23 0.49 0.23 0.51 0.28 \nFrequency-based 0.47 0.23 0.46 0.21 0.45 0.2 \n \n \nTable 3 shows the performance of our model compared with the existing model as a BERT \nextractive summary, graph -based extractive model. The performance of our model outperforms the related \nmodel as it effectively identifies the semantically related topics from the document. The output of the \nextractive summarizer is given as input to the next -level abstractive summarizer. The proposed hybrid model \nbuilds an abstractive summarizer using hugging face transformers. Transformers are pipelined to process of \nextraction of features from the input text. The summarizer model is made using a T5 transformer and there, \nwe can set the length of the summary. The summary generated using the T5 hugging face model is compared \nwith the summary generated from the extractive summary approach. The Rouge metrics were used to \nevaluate the model in terms of F -measure, Precision, and Recall. The ex periment results show that the \nperformance of the hybrid approach is better than the extractive approach.  \nThe performance of the hybrid model is evaluated using ROUGE metrics. Figure 4 shows that as we \nincrease the number of words in the end summary, the R OUGE metrics precision improves better, although  \nF-measure and recall remain more or less the same. The sample articles from WikiHow dataset with human \nannotated summary was shown in Table 4. We tested our hybrid model with a few random samples and \nevaluated the performance of our model in each approach using rouge scores. The summary generated by our \nmodel in extractive and abstractive is given in the Table 5. \n \n \nTable 3. Comparison of performance of summarization models on the DUC2002 dataset \n Models \n \nMetrics BERT based extractor and \nLSTM pointer network \nTopic modeling based \nweighted graph representation \nProposed model topic modeled \nusing semantic LDA \nRouge-1 43.39% 48.10%  48.35% \nRouge-2 19.38% 23.30% 29.53%  \nRouge-L 40.14% NA 41.72% \n \n \n \n \nFigure 4. ROUGE-1 vs. words in summary \n\n      ÔÅ≤          ISSN: 2088-8708 \nInt J Elec & Comp Eng, Vol. 13, No. 6, December 2023: 6663-6672 \n6670 \nTable 4. Sample of a WikiHow article along with its corresponding human-annotated summary \nSample Article \nTitle: How to Change Your Name \nIntroduction: Changing your name is a big decision, but sometimes it  is right. You may choose to change your name for personal \nreasons, because you have recently married, divorced, or changed your gender, or simply because you do not like the name you were \ngiven. Whatever your reason, you can change your name legally and fairly easily, depending on where you live. \nHuman-annotated summary: Changing your name can be a big decision, but it is legally and fairly easy to do. You need to understand \nthe reasons why you want to change your name and check your state‚Äôs specific requirements. Once you have filled out the necessary \npaperwork, you must submit it to the court and wait for the court to approve your request. Once your name has been legally changed, \nyou will need to update your personal information on legal documents such as your driver‚Äôs license, Social Security card, passport, \nand any other legal documents. \n \n \nTable 5. Output summary for sample article shown on Table 4 \nExtractive summary Abstractive summary \nExtractive summary: changing your name can be done legally \nand fairly easily, but it is important to understand your state‚Äôs \nspecific requirements. You will need to fill out the necessary \npaperwork, submit it to the court, and wait for approval. Once \nyour name has been legally changed, you must update your \npersonal information on all legal documents. \nChanging your name can be a daunting decision, but it can also be \na necessary one for personal or legal reasons. The process varies by \nstate, but typically involves filling out paperwork and waiting for \ncourt approval. Once your name is legally changed, you will need \nto update all relevant documents. It is important to understand the \nprocess and requirements before beginning. \n \n \n5. CONCLUSION AND FUTURE WORK \nThe need for text summarization is indeed an automatic choice for many web -based readers. Text \nsummarization is classified as extractive and abstractive summarization. The extractive text summarizers \nused statistical features in the documents to generate s ummaries. Semantic summaries make interesting to \nreaders. The semantics summaries are generated by applying a deep learning approach. This paper introduces \na hybrid model that combines the best features of both extractive and abstractive text summarization . Using \nsemantic LDA and sentence concep t mapping algorithms, our hybrid model is first trained to generate an \nextractive summary over the WikiHow dataset. semantic LDA is used to identify the hidden topics in the \ndocument, and sentence concept mapping is used to map different concepts in the articles. Then, our \nproposed system configures the pipeline of hugging face transformers to generate an abstractive summary \nfrom the extracted summary. Our experimental results show that our model ‚Äôs performance is better compared \nto extractive summarization alone and the precision improves as we increase the number of concepts and \nwords in the article. In the future, one has to focus on improving the performance of the summary model by \nenhancing semantic features.  \n \n \nREFERENCES \n[1] ‚ÄúTotal number of websites ,‚Äù Internet Live Stats . https://www.internetlivestats.com/total-number-of-websites/ (accessed Sep. 05, \n2022). \n[2] W. S. El -Kassas, C. R. Salama, A. A. Rafea, and H. K. Moham ed, ‚ÄúAutomatic text summarization: a comprehensive survey,‚Äù \nExpert Systems with Applications, vol. 165, Mar. 2021, doi: 10.1016/j.eswa.2020.113679. \n[3] N. Moratanch and S. Chitrakala, ‚ÄúA survey on abstractive text summarization,‚Äù in 2016 International Conf erence on Circuit, \nPower and Computing Technologies (ICCPCT), Mar. 2016, pp. 1‚Äì7, doi: 10.1109/ICCPCT.2016.7530193. \n[4] T. Ma, Q. Pan, H. Rong, Y. Qian, Y. Tian, and N. Al -Nabhan, ‚ÄúT -BERTSum: topic -aware text summarization based  \non BERT,‚Äù IEEE Transactions on Computational Social Systems , vol. 9, no. 3, pp. 879 ‚Äì890, Jun. 2022, doi: \n10.1109/TCSS.2021.3088506.  \n[5] T. Wolf et al., ‚ÄúTransformers: state-of-the-art natural language processing,‚Äù in Proceedings of the 2020 conference on empirical \nmethods in natural language processing: system demonstrations, 2020, pp. 38‚Äì45. \n[6] C. Ma, W. E. Zhang, M. Guo, H. Wang, and Q. Z. Sheng, ‚ÄúMulti -document summarization via deep learning techniques: a \nsurvey,‚Äù ACM Computing Surveys, vol. 55, no. 5, pp. 1‚Äì37, May 2023, doi: 10.1145/3529754. \n[7] Y. Liu, ‚ÄúFine-tune BERT for extractive summarization,‚Äù arXiv preprint arXiv:1903.10318, Mar. 2019. \n[8] S. Narayan, S. B. Cohen, and M. Lapata, ‚ÄúRanking sentences for extractive summarization with reinforcement learning,‚Äù in \nProceedings of the 2018 Conference of the North American Chapter of the Associ ation for Computational Linguistics: Human \nLanguage Technologies, Volume 1 (Long Papers), 2018, pp. 1747‚Äì1759, doi: 10.18653/v1/N18-1158. \n[9] R. Nallapati, F. Zhai, and B. Zhou, ‚ÄúSummaRuNNer: A recurrent neural network based sequence model for extractive \nsummarization of documents,‚Äù in Proceedings of the AAAI Conference on Artificial Intelligence , Feb. 2017, vol. 31, no. 1, doi: \n10.1609/aaai.v31i1.10958. \n[10] S. Verma and V. Nidhi, ‚ÄúExtractive summarization using deep learning,‚Äù Research in Computing Scienc e, vol. 147, no. 10,  \npp. 107‚Äì117, Dec. 2018, doi: 10.13053/rcs-147-10-9. \n[11] W. Xiao and G. Carenini, ‚ÄúExtractive summarization of long documents by combining global and local context,‚Äù in Proceedings \nof the 2019 Conference on Empirical Methods in Natura l Language Processing and the 9th International Joint Conference on \nNatural Language Processing (EMNLP-IJCNLP), 2019, pp. 3009‚Äì3019, doi: 10.18653/v1/D19-1298. \n[12] D. Suleiman and A. A. Awajan, ‚ÄúDeep learning based extractive text summarization: approache s, datasets and evaluation \nmeasures,‚Äù in 2019 Sixth International Conference on Social Networks Analysis, Management and Security (SNAMS) , Oct. 2019, \npp. 204‚Äì210, doi: 10.1109/SNAMS.2019.8931813. \nInt J Elec & Comp Eng  ISSN: 2088-8708 ÔÅ≤ \n \n A hybrid approach for text summarization using semantic latent Dirichlet ‚Ä¶ (Bharathi Mohan Gurusamy) \n6671 \n[13] P. Ren, Z. Chen, Z. Ren, F. Wei, J. Ma, and M. de Rijke , ‚ÄúLeveraging contextual sentence relations for extractive summarization \nusing a neural attention model,‚Äù in Proceedings of the 40th International ACM SIGIR Conference on Research and Development \nin Information Retrieval, Aug. 2017, pp. 95‚Äì104, doi: 10.1145/3077136.3080792. \n[14] Y. Zhang, M. J. Er, R. Zhao, and M. Pratama, ‚ÄúMultiview convolutional neural networks for multidocument extractive \nsummarization,‚Äù IEEE Transactions on Cybernetics, vol. 47, no. 10, pp. 3230‚Äì3242, 2017, doi: 10.1109/TCYB.2016.2628402. \n[15] J. Chen and H. Zhuge, ‚ÄúExtractive summarization of documents with images based on multi -modal RNN,‚Äù Future Generation \nComputer Systems, vol. 99, pp. 186‚Äì196, Oct. 2019, doi: 10.1016/j.future.2019.04.045. \n[16] A. Jadhav and V. Rajan, ‚ÄúExtractive sum marization with SWAP-NET: sentences and words from alternating pointer networks,‚Äù \nin Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , 2018,  \npp. 142‚Äì151, doi: 10.18653/v1/P18-1014. \n[17] M. Zhong, D. Wang, P. Liu, X. Qiu, and X. Huang, ‚ÄúA closer look at data bias in neural extractive summarization models,‚Äù in \nProceedings of the 2nd Workshop on New Frontiers in Summarization, 2019, pp. 80‚Äì89, doi: 10.18653/v1/D19-5410. \n[18] X. Mao, H. Yang, S. Hu ang, Y. Liu, and R. Li, ‚ÄúExtractive summarization using supervised and unsupervised learning,‚Äù Expert \nSystems with Applications, vol. 133, pp. 173‚Äì181, Nov. 2019, doi: 10.1016/j.eswa.2019.05.011. \n[19] A. K. Singh and M. Shashi, ‚ÄúDeep learning architecture for multi -document summarization as a cascade of abstractive and \nextractive summarization approaches,‚Äù International Journal of Computer Sciences and Engineering , vol. 7, no. 3, pp. 950 ‚Äì954, \nMar. 2019, doi: 10.26438/ijcse/v7i3.950954. \n[20] S. Narayan, N. P apasarantopoulos, S. B. Cohen, and M. Lapata, ‚ÄúNeural extractive summarization with side information,‚Äù arXiv \npreprint arXiv:1704.04530, Apr. 2017. \n[21] M. Zhong, P. Liu, Y. Chen, D. Wang, X. Qiu, and X. Huang, ‚ÄúExtractive summarization as text matching,‚Äù i n Proc. of the 58th \nAnnual Meeting of the Association for Computational Linguistics, 2020, pp. 6197‚Äì6208, doi: 10.18653/v1/2020.acl-main.552. \n[22] J. Vig, W. Kryscinski, K. Goel, and N. Rajani, ‚ÄúSummVis: interactive visual analysis of models, data, and eva luation for text \nsummarization,‚Äù in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th \nInternational Joint Conference on Natural Language Processing: System Demonstrations , 2021, pp. 150 ‚Äì158, doi: \n10.18653/v1/2021.acl-demo.18. \n[23] G. B. Mohan and R. P. Kumar, ‚ÄúA comprehensive survey on topic modeling in text summarization,‚Äù in Micro-Electronics and \nTelecommunication Engineering, Springer Nature Singapore, 2022, pp. 231‚Äì240. \n[24] C. Jyothi and M. Supriya, ‚ÄúA bstractive text summarization on templatized data,‚Äù in Lecture Notes of the Institute for Computer \nSciences, Social Informatics and Telecommunications Engineering, Springer International Publishing, 2021, pp. 225‚Äì239. \n[25] S. S. Rani, K. Sreejith, and A. S anker, ‚ÄúA hybrid approach for automatic document summarization,‚Äù in 2017 International \nConference on Advances in Computing, Communications and Informatics (ICACCI) , Sep. 2017, pp. 663 ‚Äì669, doi: \n10.1109/ICACCI.2017.8125917. \n[26] M. Kirmani, N. M. Hakak, M. Mohd, and M. Mohd, ‚ÄúHybrid text summarization: a survey,‚Äù in Advances in Intelligent Systems \nand Computing, Springer Singapore, 2019, pp. 63‚Äì73. \n[27] S. Turky, A. A. AL -Jumaili, and R. Hasoun, ‚ÄúDeep learning based on different methods for text summary: a s urvey,‚Äù Journal of \nAl-Qadisiyah for Computer Science and Mathematics, vol. 13, no. 1, pp. 26‚Äì35, 2021. \n[28] L. Dong, M. N. Satpute, W. Wu, and D. -Z. Du, ‚ÄúTwo -phase multidocument summarization through content -attention-based \nsubtopic detection,‚Äù IEEE Transactions on Computational Social Systems , vol. 8, no. 6, pp. 1379 ‚Äì1392, Dec. 2021, doi: \n10.1109/TCSS.2021.3079206. \n[29] L. Huang, S. Cao, N. Parulian, H. Ji, and L. Wang, ‚ÄúEfficient attentions for long document summarization,‚Äù in Proceedings of the \n2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language \nTechnologies, 2021, pp. 1419‚Äì1436, doi: 10.18653/v1/2021.naacl-main.112. \n[30] A. R. Fabbri, W. Kry≈õci≈Ñski, B. McCann, C. Xiong, R. Socher, and D. Radev, ‚ÄúSummEval: Re -evaluating summarization \nevaluation,‚Äù Transactions of the Association for Computational Linguis tics, vol. 9, pp. 391 ‚Äì409, Apr. 2021, doi: \n10.1162/tacl_a_00373. \n[31] P. Ren et al., ‚ÄúSentence relations for extractive summarization with deep neural networks,‚Äù ACM Transactions on Information \nSystems, vol. 36, no. 4, pp. 1‚Äì32, Oct. 2018, doi: 10.1145/3200864. \n[32] Y. Wu and K. Wakabayashi, ‚ÄúEffect of semantic content generalization on pointer generator network in text summarization,‚Äù in \nProceedings of the 22nd International Conference on Information Integration and Web -based Applications and Services , Nov. \n2020, pp. 72‚Äì76, doi: 10.1145/3428757.3429118. \n[33] C. Feng, F. Cai, H. Chen, and M. de Rijke, ‚ÄúAttentive encoder -based extractive text summarization,‚Äù in Proc. of the 27th ACM \nInternational Conference on Information and Knowledge Management, 2018, pp. 1499‚Äì1502, doi: 10.1145/3269206.3269251. \n[34] G. B. Mohan and R. P. Kumar, ‚ÄúLattice abstraction -based content summarization using baseline abstractive lexical chaining \nprogress,‚Äù International Journal of Information Technology, vol. 15, no. 1, pp. 369‚Äì378, 2023, doi: 10.1007/s41870-022-01080-y. \n[35] G. B. Mohan and R. P. Kumar, ‚ÄúSurvey of text document summarization based on ensemble topic vector clustering model,‚Äù in \nIoT Based Control Networks and Intelligent Systems, Springer Nature Singapore, 2023, pp. 831‚Äì847. \n[36] M. Koupaee and W. Y. Wang, ‚ÄúWikihow: a large scale text summarization dataset,‚Äù arXiv preprint arXiv:1810.09305, 2018. \n \n \nBIOGRAPHIES OF AUTHORS \n \n \nBharathi Mohan Gurusamy     pursuing Ph.D. degree in Amrita School of \nEngineering Amrita Vishwa Vidyapeetham Chennai, Tamil Nadu, India . He has 15 years of \nteaching experience and 3 experience of research experience. He is currently working as an \nAssistant Professor with the Amrita School of  Computing, Amrita Vishwa Vidyapeetham \nUniversity, and Chennai, India. Since 2021 he has been publishing scientific papers in text \nsummarization. His research interests include natural language processing , computational \nintelligence, machine learning, and deep learning. He can be contacted at emai l: \ng_bharathimohan@ch.amrita.edu. \n\n      ÔÅ≤          ISSN: 2088-8708 \nInt J Elec & Comp Eng, Vol. 13, No. 6, December 2023: 6663-6672 \n6672 \n \nPrasanna Kumar Rengarajan     received his Ph.D. degree from Anna University, \nChennai, Tamil NƒÅdu, and India. He is currently working as Chairperson and Associate \nProfessor in Amrita School of Computing, Amrita University, India . He has 20 years of \nexperience in teaching. Since 2010 he has published scientific papers in data mining, data \nanalysis, time series analysis and computer vision . His areas of interest include data analytics, \nmachine learning, theory of computation, compiler design, and python programming . He can \nbe contacted at email: r_prasannakumar@ch.amrita.edu. \n  \n \nParthasarathy Srinivasan     currently working as the S enior Technical Analyst  \nat oracle, Lehi USA. He has more than 20 years‚Äô experience  in database management  \nsystem and contributed to niche areas of software Artifacts by researching and leveraging  \nthe knowledge to procedure optimal solutions . He can be contacted at emai l: \nparthasarathy.s.srinivasan@oracle.com. \n \n",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.923530101776123
    },
    {
      "name": "Computer science",
      "score": 0.8154087066650391
    },
    {
      "name": "Latent Dirichlet allocation",
      "score": 0.7972612977027893
    },
    {
      "name": "Transformer",
      "score": 0.7308787107467651
    },
    {
      "name": "Sentence",
      "score": 0.6846681237220764
    },
    {
      "name": "Natural language processing",
      "score": 0.6710599660873413
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6077247858047485
    },
    {
      "name": "Latent semantic analysis",
      "score": 0.5109254717826843
    },
    {
      "name": "Information retrieval",
      "score": 0.40618690848350525
    },
    {
      "name": "Topic model",
      "score": 0.37312981486320496
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}