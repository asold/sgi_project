{
  "title": "Toward Domain-Free Transformer for Generalized EEG Pre-Training",
  "url": "https://openalex.org/W4390969286",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5012067997",
      "name": "Sung-Jin Kim",
      "affiliations": [
        "Korea University"
      ]
    },
    {
      "id": "https://openalex.org/A5068559438",
      "name": "Dae-Hyeok Lee",
      "affiliations": [
        "Korea University"
      ]
    },
    {
      "id": "https://openalex.org/A5070086344",
      "name": "Heon-Gyu Kwak",
      "affiliations": [
        "Korea University"
      ]
    },
    {
      "id": "https://openalex.org/A5011014617",
      "name": "Seong‚ÄêWhan Lee",
      "affiliations": [
        "Korea University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3035550421",
    "https://openalex.org/W2804035092",
    "https://openalex.org/W2742638082",
    "https://openalex.org/W2128909182",
    "https://openalex.org/W4381733655",
    "https://openalex.org/W3040262915",
    "https://openalex.org/W2132360759",
    "https://openalex.org/W2950323440",
    "https://openalex.org/W1966527337",
    "https://openalex.org/W3194020089",
    "https://openalex.org/W4386225004",
    "https://openalex.org/W3188349400",
    "https://openalex.org/W4294690816",
    "https://openalex.org/W4364302091",
    "https://openalex.org/W4312892255",
    "https://openalex.org/W4385656459",
    "https://openalex.org/W3129064953",
    "https://openalex.org/W4296027702",
    "https://openalex.org/W3090425814",
    "https://openalex.org/W2887280559",
    "https://openalex.org/W3198659451",
    "https://openalex.org/W4292263626",
    "https://openalex.org/W3200300741",
    "https://openalex.org/W3197261948",
    "https://openalex.org/W3041698047",
    "https://openalex.org/W3177342940",
    "https://openalex.org/W3134896108",
    "https://openalex.org/W3092342532",
    "https://openalex.org/W4286542909",
    "https://openalex.org/W6963588625",
    "https://openalex.org/W4294811271",
    "https://openalex.org/W6685913841",
    "https://openalex.org/W2144691514",
    "https://openalex.org/W1967260085",
    "https://openalex.org/W160785014",
    "https://openalex.org/W6780218876",
    "https://openalex.org/W4312597583",
    "https://openalex.org/W2560384232",
    "https://openalex.org/W2055903898",
    "https://openalex.org/W3035725276",
    "https://openalex.org/W3158818505",
    "https://openalex.org/W3157397381",
    "https://openalex.org/W3184196062",
    "https://openalex.org/W2741907166",
    "https://openalex.org/W2559463885",
    "https://openalex.org/W2604096629",
    "https://openalex.org/W3156463266",
    "https://openalex.org/W2963727766",
    "https://openalex.org/W2346353886",
    "https://openalex.org/W4364368397",
    "https://openalex.org/W2022424636",
    "https://openalex.org/W3102455230",
    "https://openalex.org/W3194758190",
    "https://openalex.org/W1569059855",
    "https://openalex.org/W2181785117"
  ],
  "abstract": "Electroencephalography (EEG) signals are the brain signals acquired using the non-invasive approach. Owing to the high portability and practicality, EEG signals have found extensive application in monitoring human physiological states across various domains. In recent years, deep learning methodologies have been explored to decode the intricate information embedded in EEG signals. However, since EEG signals are acquired from humans, it has issues with acquiring enormous amounts of data for training the deep learning models. Therefore, previous research has attempted to develop pre-trained models that could show significant performance improvement through fine-tuning when data are scarce. Nonetheless, existing pre-trained models often struggle with constraints, such as the necessity to operate within datasets of identical configurations or the need to distort the original data to apply the pre-trained model. In this paper, we proposed the domain-free transformer, called DFformer, for generalizing the EEG pre-trained model. In addition, we presented the pre-trained model based on DFformer, which is capable of seamless integration across diverse datasets without necessitating architectural modification or data distortion. The proposed model achieved competitive performance across motor imagery and sleep stage classification datasets. Notably, even when fine-tuned on datasets distinct from the pre-training phase, DFformer demonstrated marked performance enhancements. Hence, we demonstrate the potential of DFformer to overcome the conventional limitations in pre-trained model development, offering robust applicability across a spectrum of domains.",
  "full_text": "GENERIC COLORIZED JOURNAL, VOL. XX, NO. XX, XXXX 2023 1\nTowards Domain-free Transformer for\nGeneralized EEG Pre-training\nSung-Jin Kim, Dae-Hyeok Lee, Heon-Gyu Kwak and Seong-Whan Lee‚àó, Fellow, IEEE\nAbstract‚Äî Electroencephalography (EEG) signals are\nthe brain signals acquired using the non-invasive ap-\nproach. Owing to the high portability and practicality, EEG\nsignals have found extensive application in monitoring hu-\nman physiological states across various domains. In recent\nyears, deep learning methodologies have been explored to\ndecode the intricate information embedded in EEG signals.\nHowever, since EEG signals are acquired from humans,\nit has issues with acquiring enormous amounts of data\nfor training the deep learning models. Therefore, previous\nresearch has attempted to develop pre-trained models that\ncould show significant performance improvement through\nfine-tuning when data are scarce. Nonetheless, existing\npre-trained models often struggle with constraints, such as\nthe necessity to operate within datasets of identical config-\nurations or the need to distort the original data to apply the\npre-trained model. In this paper, we proposed the domain-\nfree transformer, called DFformer, for generalizing the EEG\npre-trained model. In addition, we presented the pre-trained\nmodel based on DFformer, which is capable of seamless\nintegration across diverse datasets without necessitating\narchitectural modification or data distortion. The proposed\nmodel achieved competitive performance across motor im-\nagery and sleep stage classification datasets. Notably, even\nwhen fine-tuned on datasets distinct from the pre-training\nphase, DFformer demonstrated marked performance en-\nhancements. Hence, we demonstrate the potential of DF-\nformer to overcome the conventional limitations in pre-\ntrained model development, offering robust applicability\nacross a spectrum of domains.\nIndex Terms‚Äî Electroencephalogram, transformer,\nautoencoder, motor imagery, sleep stage classification\nThis research was supported by the Challengeable Future Defense\nTechnology Research and Development Program through the Agency\nFor Defense Development(ADD) funded by the Defense Acquisition\nProgram Administration(DAPA) in 2024(No.912911601).\nThis work was supported by the Institute of Information & Commu-\nnications Technology Planning & Evaluation (IITP) grant, funded by\nthe Korea government (MSIT) (No. 2019-0-00079, Artificial Intelligence\nGraduate School Program (Korea University)\nThe National Research Foundation of Korea (NRF) grant funded\nby the MSIT (No.2022-2-00975, MetaSkin: Developing Next-generation\nNeurohaptic Interface Technology that enables Communication and\nControl in Metaverse by Skin Touch)\nS.-J. Kim, H.-G. Kwak, and S.-W. Lee are with the Department\nof Artificial Intelligence, Korea University, Seongbuk-ku, Seoul 02841,\nRepublic of Korea. E-mail: s j kim@korea.ac.kr, hg kwak@korea.ac.kr,\nsw.lee@korea.ac.kr.\nD.-H. Lee is with the Department of Brain and Cognitive Engineering,\nKorea University, Seongbuk-ku, Seoul 02841, Republic of Korea. E-mail:\nlee dh@korea.ac.kr\n*S.-W. Lee is the corresponding author.\nI. INTRODUCTION\nB\nrain-computer interfaces (BCI) have garnered significant\nattention due to their potential to bridge human cognition\nwith computational systems. Electroencephalography (EEG)\nsignals, one of non-invasive methods that captures the electri-\ncal activity of the brain, are central to BCI technology. The\nhigh portability and practicality of devices for acquiring EEG\nsignals have paved the way for advancements in neuroscience,\nmedical diagnostics, and other interdisciplinary domains [1]‚Äì\n[6]. However, the inherent complexity and non-linearity of\nEEG signals pose challenges for extracting meaningful in-\nformation. Traditional signal processing techniques, such as\nfiltering, Fourier transform, or cross-correlation, have been\nwidely used for analyzing EEG signals [7]‚Äì[10]. These tech-\nniques still require domain-specific knowledge and extensive\nmanual feature engineering, which could be time-consuming\nand unsuitable for applications based on EEG signals char-\nacterized by high non-linearity and variability. Moreover,\ntheir performance might be susceptible when confronted with\naugmented or previously unseen data [11]. In recent years,\ndeep learning has emerged as a promising approach for ana-\nlyzing EEG signals. Deep learning models have demonstrated\nsignificant success in various EEG-based applications, such as\nsleep staging [12], [13], seizure detection [14], [15], motor\nimagery (MI) [16], [17], or emotion classification [18], [19].\nHowever, the limited availability of diverse and high-quality\ndata for training the models presents a major challenge when\napplying deep learning to EEG signal analysis [20]. EEG\ndatasets are typically small and highly imbalanced, which\nposes a challenge in training deep learning models that could\neffectively generalize to new data.\nLeveraging the pre-trained model as a foundation for train-\ning the model is an effective strategy when the amount of\ndata is insufficient [21]. By guiding the information from\npreviously acquired knowledge, these models could potentially\nconverge to a more refined optimal solution [22]. Conse-\nquently, several studies have developed pre-trained models for\nvarious tasks of EEG signals. He et al. [23] proposed the\nMLP-Mixer-based neural network and self-supervised learning\nalgorithm to develop the pre-trained model by training the\nproposed model to predict the following EEG signals. The\nauthors achieved superior performance using the pre-trained\nmodel compared to the conventional methods when conducting\nthe MI-based downstream task. Jiang et al. [24] developed\na strategy for constructing the pre-trained model based on\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2024.3355434\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2 GENERIC COLORIZED JOURNAL, VOL. XX, NO. XX, XXXX 2023\ncontrastive learning. They reported the optimal combination\nof data augmentation methods, which was pivotal for effective\ncontrastive learning. In addition, they achieved significant\nperformance in the sleep stage classification task compared\nwith that of other methods. Zhang et al.[25] presented the ad-\nversarial learning-based self-supervised learning algorithm to\ndevelop the pre-trained model. Their strategy revolved around\nthe reconstruction of arbitrarily masked segments within EEG\nsignals. By integrating their proposed algorithm, they en-\nhanced the classification accuracy within emotion recognition\ndatasets.\nAlthough using the pre-trained model proved effective in\nthe small number of datasets, there were significant challenges\nin generally applying the pre-trained model to analyze EEG\nsignals owing to the different characteristics of the dataset.\nSpecifically, the differences in configurations, such as the\nnumber and order of channels or sampling rate, of EEG\nelectrodes across various datasets pose significant challenges\nwhen implementing the pre-trained model [20], [26]. The con-\nventional methods have limitations when utilizing previously\ntrained models because the architectures of the pre-trained and\ntarget models differ if the configuration in the target dataset\nvaries. Spherical spline interpolation and channel selection\nhave been employed to circumvent these limitations. Spherical\nspline interpolation is the fundamental method for addressing\nthis limitation. By applying spherical spline interpolation, EEG\nsignals of the empty electrodes are reconstructed using the\ninformation gleaned from the signals of the existing electrodes.\nChannel selection refers to choosing only the electrodes that\nare common to both datasets. Kostas et al. [27] strategically\nchose the 19 electrodes that were most commonly used across\nall datasets. Furthermore, missing data for specific electrodes\nwere assigned as zero. Wei et al. [28] observed that meth-\nods achieving superior performance typically employed the\nchannel selection approach to mitigate issues arising from\nvariations in electrode types. They standardized the electrode\ntypes by selecting those commonly found across all datasets\nused. However, spherical spline interpolation might introduce\nsignificant errors when the electrode density is low [29]. Fur-\nthermore, the channel selection method has its own limitations\nas it could not utilize all available data, given that it discards\ninformation from uncommon electrodes.\nIn this paper, we proposed the domain-free transformer,\ncalled DFformer, capable of consistently decoding EEG sig-\nnals irrespective of the diverse configurations across different\ndatasets. We defined one domain as one configuration of the\ndataset. In addition, we appended the class tokens along each\naxis to establish a pre-trained model that effectively com-\npressed EEG signals into representative vectors in each axis.\nBy utilizing the class tokens, we could perform autoencoder-\nbased signal reconstruction tasks, the fundamental pre-training\ntechnique, to create pre-trained models adaptable to various\ndatasets. We verified the effectiveness of the guidance from\nthe pre-trained model based on our proposed model when de-\ncoding EEG signals. Furthermore, upon analyzing the trained\nclass tokens in the proposed model, we found that the model\npredominantly focused on the fundamental information for\nanalyzing EEG signals. By implementing our proposed model,\nwe could effectively address the constraints of the conventional\nmethods that struggled with building and applying pre-trained\nmodels due to domain differences. By alleviating these lim-\nitations, we could develop a unified pre-trained model that\ncould be applied to different datasets without distortion or\nloss of information in the data. In addition, the knowledge\nacquired from pre-training also could guide the model toward\na more optimized space when applied to other datasets in the\ndownstream step. This resulted in an enhanced performance\ncompared with that of models entirely trained from scratch. In\nsummary, the main contributions of this study are as follows;\n‚Ä¢ We proposed DFformer a novel architecture adept at de-\ncoding EEG signals from various datasets with disparate\nconfigurations without modifying the architecture. The\nsource code of our implementation is readily accessible\non GitHub.1\n‚Ä¢ The pre-trained model based on DFformer was devel-\noped by leveraging an autoencoder paradigm. By using\nDFformer as an encoder, we could construct the pre-\ntrained model without utilizing the previous approaches\nfor alleviating the differences in the domain.\n‚Ä¢ We achieved the performance improvement when we\ntransferred knowledge from the pre-trained model com-\npared with training from scratch. This finding validated\nthat the knowledge acquired in the pre-trained model\ncould guide in decoding an EEG dataset not utilized\nduring the pre-training phase.\nII. METHODS\nA. Datasets and Preprocessing\nFor our experiments, we used two prominent MI datasets\nand two sleep-stage classification datasets. These publicly\navailable datasets are frequently used to evaluate various\nmethods in their respective domains [30], [31].\n1) BCI Competition IV-2a (BCIC2a): BCIC2a [32] includes\nEEG data from 22 electrodes and nine participants. With a\n250 Hz sampling rate, it covers four motor imagination tasks:\nleft hand (Left), right hand (Right), foot (Foot), and tongue\n(Tongue). Two sessions contain 288 trials each, focusing on\nthe 2-6 seconds motor imagination duration. EEG signals were\nfiltered with a 38 Hz low-pass Butterworth filter [33].\n2) BCI Competition IV-2b (BCIC2b): BCIC2b [34] contains\nEEG data from three electrodes and nine participants at 250\nHz. The participants performed two MI tasks: Left and Right,\nacross five sessions. The initial sessions consists of 120 trials\nwithout visual feedback, whereas the next session consisted\nof 160 trials with feedback. The imagination interval was\nbetween 3-7 seconds, and the signals were filtered with a 38\nHz low-pass Butterworth filter [33].\n3) Sleep-EDF: Sleep-EDF [35], [36] features the\npolysomnographic recordings of 20 individuals. EEG\nsignals are acquired at a sampling rate of 100 Hz. The sleep\nstages were categorized by professionals. In this study, the\nN3 and N4 stages were combined into the N3 stage [37],\nand only data from the Fpz-Cz EEG electrode was used. In\n1DFformer codebase:\nhttps://github.com/comojin1994/dfformer\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2024.3355434\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAUTHOR Kim et al.: TOWARDS DOMAIN-FREE TRANSFORMER FOR GENERALIZED EEG PRE-TRAINING 3\nCLS\nCLS\nOutput of TokenizerAddition of Intra-channel CLS Tokens\nFeed-ForwadNetworkIntra-ChannelMHAKQV\nKQV\nInter-ChannelMHA\nFeed-ForwadNetwork\nAddition of Inter-channel CLS Tokens Output of Embedded Feature‚Ñù!√ó#√ó$ ‚Ñù!√ó#%&√ó$ ‚Ñù\t(!%&)√ó#%&√ó$ ‚Ñù\t(!%&)√ó#%&√ó$\nKQV\nEEG signals\nTemporal-wiseCNNEncoder\nDFformer Block\nN √ó\nIntra-ChannelMHA\nFeed-ForwadNetwork\nInter-ChannelMHA\nFeed-ForwadNetwork\nClassificationHead\nBiaxial Information Embedding Block\nIntra-ChannelMHA\nFeed-ForwadNetwork\nInter-ChannelMHA\nFeed-ForwadNetworkKQVCLSCLS\nùëá\nùê∂ùê∑\nKQV\nKQV\nTokenizer\nFig. 1. The overall architecture of DFformer is presented in the upper row, which contains the tokenizer, biaxial information embedding layer,\nDFformer block, and classification head. The detail process of the biaxial information embedding block is visualized in the lower row. The green and\nblue tokens indicate intra- and inter-channel class tokens (CLS), respectively.\naddition, we only included 30-minute segments from these\nperiods immediately before and after sleep segments, given\nthat our primary interest was in sleep periods. We divided\nthe classification into five stages (W AKE, N1, N2, N3, and\nREM) for the evaluation. Classification was performed using\nsets of 30 trials.\n4) Sleep Heart Health Study (SHHS): SHHS [38] is the\ndataset obtained from studies on the connection between sleep-\nbreathing disorders and cardiovascular issues. It consists of\nEEG signals from 5,793 participants over the night, initially\nat 150 Hz, but is downsampled to 100 Hz for compatibility\nwith Sleep-EDF [12]. The performances of the models were\nassessed using five categories identical to those in Sleep-EDF\ndataset. The evaluations used the C4-A1 electrode and grouped\n20 trials as a set.\nB. DFformer\nIn the previous method, significant challenge in developing\nthe pre-trained model for various datasets was the dependency\non fixed parameters based on the configuration of the dataset.\nThis constraint implied that, once trained, the model could\nnot be seamlessly deployed without necessitating architectural\nmodifications [20]. As a result, our primary goal while design-\ning the model was to design a model that would be applicable\ndespite the different shapes of EEG signals encountered. Since\nthe transformer could apply to variable shapes of data without\nmodifying the architecture, it was utilized as the base structure.\nAs shown in Fig. 1, the model consists of four main\ncomponents: i) Tokenizer compresses information from the\nhigh-frequency raw EEG signals into the patch. ii) Biaxial\ninformation embedding block enriches the compressed EEG\ndata with auxiliary information, including positional encoding\nand both intra- and inter-channel class tokens. iii) DFformer\nblocks extract significant information from the processed fea-\ntures in the preceding stages. iv) Classification head interprets\nthe features extracted by DFformer, facilitating the prediction\nof proper labels for each task. By applying these components\nthat could accommodate changes in the shape of the data, we\ncould develop the more robust and adaptable architecture that\ncould be applied across diverse domains.\n1) Tokenizer: Since the raw EEG signals ( X ‚àà\nR B√óC√óTraw , B indicates the batch size, C and Traw are\nthe number of channels and the number of time points in\nthe raw EEG signals, respectively) were acquired from a\nhigh-sampling rate, we converted the signals into patches\nby applying tokenization through a module like wav2vec\n[39]. Previous tokenization approaches commonly involve a\nspatial convolution layer with a spatial kernel whose shape\ndepends on the number of channels in EEG signals [27],\n[40]. Such a dependency necessitates the architectural mod-\nification each time when training on different datasets, thus\ncausing complexity and constraints. Therefore, to train the\nmodel independently with channel configurations, the spatial\nconvolution used in previous approaches was omitted. Instead,\nthe tokenization module was conducted using only a 1-D\nconvolutional neural network (CNN) in the temporal direc-\ntion after reshaping EEG signals by aggregating the batch\nand channel axes. The tokenization module comprised three\nembedding layers with kernel sizes of [125, 8, 4] and strides of\n[1, 4, 2]. Layer normalization and GELU activation functions\nwere applied to each layer. After performing the tokenization\nmodule ( f : R BC√óTraw ‚Üí R BC√óT√óD, D indicates the\nnumber of embedded dimensions and T is the length of the\ndownsampled EEG signals), we split the data on both the\nbatch and channel axes, which were combined. Therefore,\nwe could generate the downsampled patches ( H = f(X) ‚àà\nR B√óC√óT√óD) from the raw EEG signals.\n2) Biaxial information embedding block: EEG signals con-\ntain intra-channel (temporal) features, such as variations in\nfrequency over time, and inter-channel features, including dif-\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2024.3355434\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n4 GENERIC COLORIZED JOURNAL, VOL. XX, NO. XX, XXXX 2023\nTABLE I\nTHE ARCHITECTURE , PARAMETERS , AND OUTPUT SHAPE OF THE\nCLASSIFICATION HEAD FOR EACH TASK\nTask Layer Parameter Output\nMI\nConv C √ó 1, D B √ó D √ó 1 √ó T\nELU - B √ó D √ó 1 √ó T\nBatch normalization - B √ó D √ó 1 √ó T\nDropout p = 0.5 B √ó D √ó 1 √ó T\nFlatten - B √ó DT\nLinear - B √ó # of classes\nSleep stage\nclassification\nReshape - BC √ó D √ó T\nELU - BC √ó D √ó T\nBatch normalization - BC √ó D √ó T\nDropout p = 0.5 BC √ó D √ó T\nFlatten - BC √ó DT\nLinear - BC √ó # of classes\nferences in activity between channels. However, these features\nare not uniformly represented across all tasks of the EEG\ndata. Instead, dominant features might differ according to\nthe specific task. For example, in sleep stage classification\ndata, the target frequency band depends on the sleep stage\n[41]. Conversely, in motor imagery classification data, the\ndifferences in activity between channels at a certain time, such\nas event-related desynchronization/synchronization, are repre-\nsentative features [42]. Since EEG signals contain significant\nfeatures that differ intra- and inter-channel-wise, we designed\nthe biaxial information embedding block to encode significant\ninformation for each feature by adding positional encoding and\nclass tokens in the intra- and inter-channel axes, respectively.\nEEG signals are time-series data; therefore, we assumed that\ninformation on the temporal order is significant for compre-\nhending the relationship between channels. After tokenizing\nEEG signals into patches, the channel and batch axes were\nmerged. We added intra-channel sinusoidal positional encod-\ning, POS intra ‚àà R BC√ó(T+1)√óD, along the temporal axis and\nconcatenated the BC number of class tokens (intra-channel\nclass tokens), CLS intra ‚àà R BC√óD, at the beginning of the\nintra-channel axis. The temporally preprocessed data have\nthe dimensionality BC √ó (T + 1)√ó D. After intra-channel\npositional encoding and class tokens were added, multi-head\nattention (MHA) and a feed-forward network were applied\nto the channel-wise-divided data to encode significant intra-\nchannel information. Through this subsequent process, the\nintra-channel class tokens containing the temporal features\nwithin each channel were generated. After the intra-channel\ninformation was encoded for each channel, we transformed\nthe shape of the data to extract the inter-channel information\nfor each patch. We applied the inter-channel-wise sinusoidal\npositional encoding, POS inter ‚àà R B(T+1)√ó(C+1)√óD, along\nthe channel axis and attached B(T + 1) numbers of class\ntokens (inter-channel class tokens), CLS inter ‚àà R B(T+1)√óD,\nat the beginning of the temporally segmented data. The\nshape of the inter-channel-wise-preprocessed feature maps is\nB(T + 1) √ó (C + 1) √ó D. By applying inter-channel-wise\nmulti-head attention and a feed-forward network to the tem-\nDFformer\nDecoder\nIntra-channelCLSInter-channelCLS\nBasis Matrix Generation\nEEG signals\nReconstructed EEG signals\nBasis Matrix‚Ñù!√ó#\nIntra-Channel CLS Tokens‚Ñù!√ó$ Inter-Channel CLS Tokens‚Ñù$√ó#\nFig. 2. The overall flow of the signal reconstruction task and visualiza-\ntion of the process for generating the basis matrix based on intra- and\ninter-channel class tokens.\nporally divided data, we extracted significant information be-\ntween the inter-channels within a specific time. In this manner,\nevery inter-channel class token captured information about the\nrelationships between the channels within each patch. The\nfinal shape of preprocessed data after conducting the biaxial\ninformation embedding layer is B √ó (C + 1)√ó (T + 1)√ó D.\nTherefore, we could effectively encode significant features\nfrom the downsampled EEG patches by applying the biaxial\ninformation embedding block using two different types of class\ntokens after the tokenization.\n3) DFformer block: DFformer block was applied to enhance\nthe generalization ability by extracting the high-level features\nfrom preprocessed data from the biaxial information embed-\nding block. The structure of DFformer is similar to that of\nthe biaxial information embedding block except for the added\npositional encoding and class tokens in the embedding block.\nThis block employs MHA and feed-forward networks for both\nintra- and inter-channel axes. By applying DFformer blocks,\nwe could design a flexible architecture that allowed us to\nchange the depth of the model dynamically. Therefore, we\ncould effectively fuse intra- and inter-channel information to\nextract significant features from the EEG data as the depth\nof the model increased by applying this block structure.\nThe preprocessed features without class tokens derived from\nDFformer blocks served as input for the classification head.\nFurthermore, the tokenizer, biaxial information embedding\nblock, and DFformer blocks were utilized during the pre-\ntraining phase.\n4) Classification head: Although the shape of the input\ndata is B √ó C √ó Traw for all data, the format required to\npredict varies based on the domains. For instance, in MI signal\nclassification, the input data are multi-channel EEG signals,\nhowever, they contain the information from only one trial of\na single class. Consequently, we aim to derive the predicted\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2024.3355434\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAUTHOR Kim et al.: TOWARDS DOMAIN-FREE TRANSFORMER FOR GENERALIZED EEG PRE-TRAINING 5\nprobability for a single class to predict the label. Conversely,\nthe input data consisted of several sequential single-channel\nEEG signals for sleep stage classification. Therefore, the\nnumber of predicted values should match the number of the\ncombined channels. In these tasks, customized classification\nheads are required for each domain after extracting the features\nfrom the backbone model. Hence, we applied a simple task-\nspecific classification head for each assignment to obtain the\nproper results, as shown in Table I.\nC. Strategy for pre-training\nSince the main purpose of DFformer is to design a pre-\ntrained model that is not affected by various domains, to check\nthe effectiveness of the pre-trained model, we developed a pre-\ntrained model by performing a reconstruction task based on an\nautoencoder, one of the simple self-supervised learning meth-\nods [43]. We assumed that, by utilizing EEG signals directly\nas input and subsequently encoding and reconstructing them,\nthe fundamental characteristics of the EEG could be trained\nwithout being biased by the distinctive attributes specific to\nany individual dataset.\nWe utilized the intra- and inter-channel class tokens to\nreconstruct the original EEG signals. Since each class token\ncontained information about intra- and inter-channels, we\ngenerated the basis matrix B by multiplying these class tokens,\nas shown in Fig. 2. The decoder with a simple architecture,\ncontaining three convolution modules, was used to reconstruct\nthe signals from the basis matrix. These modules are the\nreverse process of the tokenization. Each module contains the\ntranspose convolution layer to extend the shape of the feature,\nand the convolution layer to fuse the information derived from\nthe transpose convolution layer. GELU activation function was\nused for each layer. However, since the input EEG signals were\nnormalized between 0 and 1, a sigmoid activation function was\nused for the final layer. Within each module, both the transpose\nconvolution and convolution layers utilized kernel sizes of [4,\n8, 125]. Only the transpose convolution, which was employed\nto scale the data, had a stride size of [2, 4, 1]. Additionally,\npadding was applied to the convolution layer to ensure that the\ndata size remained consistent before and after processing. To\nensure stability during the training process, we used the mean\nabsolute error as a loss function. The channel permutation was\nconducted to reduce the bias in channel order information.\nD. Evaluation settings\nWe conducted a quantitative evaluation of the decoding\nperformance of DFformer and evaluated the efficacy of the\npre-trained DFformer. The proposed model was trained from\nscratch to assess its performance. Furthermore, to evaluate the\nefficacy of the pre-trained model, we compared the results\nobtained by fine-tuning the target dataset; evaluation methods\nwere selected based on this dataset. Leave-one-subject-out\ncross-validation was employed for MI and Sleep-EDF datasets,\nwhich have a limited number of participants. In contrast,\nin SHHS, which has sufficient number of participants, we\nconducted 20-fold cross-validation [31], [44]. Performance in-\ndicators were carefully selected to ensure meaningful compar-\nisons across the experiments. For MI datasets, the evaluation\nTABLE II\nCOMPARISON OF PERFORMANCES FOR DECODING EEG SIGNALS IN\nBCIC2 a AND BCIC2 b DATASETS AMONG THE CONVENTIONAL MI\nDECODING METHODS AND THE PROPOSED METHOD .\nDataset Method Overall metrics\nAcc. Kappa F1-score\nBCIC2a\nDeepConvNet [47] 0.5679 0.4239 0.5677\nEEGNet [48] 0.5584 0.4113 0.5519\nShallowConvNet [33] 0.5783 0.4378 0.5778\nProposed 0.5841 0.4455 0.5837\nBCIC2b\nDeepConvNet [47] 0.7657 0.5235 0.7656\nEEGNet [48] 0.7457 0.5098 0.7457\nShallowConvNet [33] 0.7558 0.5167 0.7475\nProposed 0.7618 0.5208 0.7552\nmetrics included the average classification accuracy (Acc.),\nkappa value (Kappa), and F1-score. The average classifica-\ntion accuracy is the most common indicator for evaluating\nthe architecture of the classification task. Since the kappa\nvalue could handle off-diagonal components in the confusion\nmatrix, it is utilized to supplement the accuracy value [45].\nAlthough the F1-score is usually used when the number\nof class labels is imbalanced, we applied this indicator to\nevaluate the performance of the architecture from various\nperspectives. For the sleep stage classification datasets, the\naverage classification accuracy, kappa value, and F1-score\nwere also utilized. Moreover, to address class imbalances in\nthe sleep stage classification datasets, the per-class F1-score\nwas employed to compare performance across different sleep\nstages [44], [46]. All results were the average values obtained\nfrom ten repetitions with different seeds.\nFor a consistent analysis, when pre-training DFformer, we\nfixed the number of blocks in DFformer as three, including the\nembedding block, the number of heads in MHA as four, and\nthe dimension as 64. The pre-training was executed over 100\nepochs for each dataset with 10 epochs warm up, adopting\na learning rate of 3e‚Äì4 for MI dataset and 3e‚Äì5 for the\nsleep stage classification dataset. During training from scratch\nand fine-tuning, the learning rates for MI and sleep stage\nclassification datasets were set to 3e‚Äì3 and 3e‚Äì4, respectively.\nTo compare the effects of the pre-trained model, the training\nepochs were 100 and 30 for MI and sleep stage classification\ndatasets, respectively. Throughout the optimization phase, the\nAdamW optimizer was selected using a cosine-learning rate\nscheduler.\nVarious preprocessing and data augmentation methods could\nhelp to train the robust model. For example, He et al. [51]\nproposed the Euclidean-space alignment (EA) which could\nalleviate the issues from covariance shift between data. By\nconducting EA, they effectively extracted significant features\nin EEG signals. However, since we aimed to verify the effect\nderived from our proposed model, only min-max normaliza-\ntion was employed during the training and evaluation phase,\nwithout other preprocessing and data augmentation techniques.\nBy minimizing the utilization of these techniques, we could\nevaluate the effect of DFformer without interference with other\nvariables.\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2024.3355434\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6 GENERIC COLORIZED JOURNAL, VOL. XX, NO. XX, XXXX 2023\nTABLE III\nCOMPARISON OF PERFORMANCES FOR DECODING EEG SIGNALS IN SLEEP -EDF AND SHHS DATASETS AMONG THE CONVENTIONAL SLEEP\nSTAGE CLASSIFICATION METHODS AND THE PROPOSED METHOD .\nDataset Method Overall metrics Per-class F1-score\nAcc. Kappa F1-score W AKE N1 N2 N3 REM\nSleep-EDF\nDeepSleepNet [49] 0.8190 0.7600 0.7660 0.8670 0.4550 0.8510 0.8330 0.8260\nRobustSleepNet [46] 0.8270 0.7700 0.7530 0.8670 0.3660 0.8600 0.8640 0.8100\nU-sleep [50] 0.7980 0.7200 0.7480 0.8240 0.4470 0.8300 0.8480 0.7900\nProposed 0.8370 0.7778 0.7809 0.9087 0.4412 0.8657 0.8472 0.8416\nSHHS\nDeepSleepNet [49] 0.8100 0.7300 0.7390 0.8540 0.4050 0.8250 0.7930 0.8190\nRobustSleepNet [46] 0.8140 0.7400 0.7360 0.7990 0.3960 0.8310 0.8200 0.8360\nU-sleep [50] 0.8410 0.7800 0.7690 0.8550 0.4360 0.8580 0.8330 0.8600\nProposed 0.8389 0.7739 0.7620 0.9104 0.4420 0.8417 0.7615 0.8541\n1.031.010.990.970.95\nAcc.\nTongue\nFeet\nRightLeft\nF1-score\nKappa\n 1.031.010.990.970.95\nAcc.\nKappaRight\nLeft F1-score\n1.031.010.990.970.95\n1.05F1-scoreREM\nN3\nN2 N1 Wake\nKappa\nAcc.\n F1-scoreREM\nN3\nN2 N1 Wake\nKappa\nAcc.0.900.850.800.750.70\n0.951.00\nBaselineBCIC2bSleep-EDFSHHS\nBaselineBCIC2aSleep-EDFSHHS\nBaselineSHHSBCIC2aBCIC2b\nBaselineSleep-EDFBCIC2aBCIC2b\n1.031.010.990.970.95\nAcc.\nTongue\nFeet\nRight Left\nF1-score\nKappa\n 1.031.010.990.970.95\nAcc.\nKappaRight\nLeft F1-score\nF1-score\nREM\nN3\nN2\nN1\nWake\nKappa\nAcc.0.900.850.800.750.70\n0.951.00\n(a) (b) (c) (d)\nBaselineBCIC2bSleep-EDFSHHS\nBaselineBCIC2aSleep-EDFSHHS\nBaselineSleep-EDFBCIC2aBCIC2b\n1.031.010.990.970.95\n1.05F1-score\nREM\nN3\nN2\nN1\nWake\nKappa\nAcc.\nBaselineSHHSBCIC2aBCIC2b\n1.03\n1.01\n0.99\n0.970.95\nAcc.\nTongue\nFeet\nRight Left\nF1-score\nKappa\n 1.03\n1.01\n0.99\n0.970.95\nAcc.\nKappaRight\nLeft F1-score\nF1-score\nREM\nN3\nN2\nN1\nWake\nKappa\nAcc.0.900.850.800.750.70\n0.951.00\n(a) (b) (c) (d)\nBaselineBCIC2bSleep-EDFSHHS\nBaselineBCIC2aSleep-EDFSHHS\nBaselineSleep-EDFBCIC2aBCIC2b\n1.031.010.990.970.95\n1.05F1-score\nREM\nN3\nN2\nN1\nWake\nKappa\nAcc.\nBaselineSHHSBCIC2aBCIC2b\nFig. 3. Radar chart for comparing the performance of models fine-tuned from the pre-trained models across various datasets with those trained\nfrom scratch. The black line represents the baseline performance of the model trained from scratch. Other colors signify the performance of models\nfine-tuned from each dataset to the baseline dataset. The baseline datasets are (a) BCIC2a, (b) BCIC2b, (c) Sleep-EDF , and (d) SHHS, respectively.\nIII. RESULTS\nA. Comparison among the conventional models\nAlthough the main goal of designing DFformer was to\nensure that it works sufficiently across domains, it is also\ncritical that DFformer performs similarly to the conventional\narchitectures. In Tables II and III, we evaluated the perfor-\nmances of DFformer compared with the conventional models\ntrained from scratch on MI and sleep stage classification\ndatasets.\nIn Table II, we compared DFformer with three commonly\nused models for decoding MI-based EEG signals: DeepCon-\nvNet [47], EEGNet [48], and ShallowConvNet [33]. DFformer\nachieved the highest classification accuracy, kappa value, and\nF1-score of 0.5841, 0.4455, and 0.5837 at BCIC2a, respec-\ntively. Although DeepConvNet achieved the highest perfor-\nmance in all indicators at BCIC2b, DFformer achieved the\nsecond-highest performance on all indicators behind Deep-\nConvNet. The classification accuracy, kappa value, and F1-\nscore of DFformer at BCIC2b were 0.7618, 0.5208, and\n0.7552, respectively.\nTable III presents the results of the comparison between\nDFformer and three architectures, DeepSleepNet [49], Robust-\nSleepNet [46], and U-Sleep [50], used in sleep stage classifi-\ncation. In Sleep-EDF, DFformer achieved the highest overall\nclassification accuracy, kappa value, and F1-score of 0.8370,\n0.7778, and 0.7809, respectively. DFformer accomplished\nthe most significant performance in the W AKE, N2, and\nREM stages, with the per-class F1-score of 0.9087, 0.8657,\nand 0.8416, respectively. In the N1 and N3 stages, U-sleep\nachieved the best per-class F1-scores of 0.4470 and 0.8480,\nrespectively. In SHHS, DFformer showed the second-highest\nclassification accuracy, kappa value, and F1-score of 0.8389,\n0.7739, and 0.7620, respectively. In addition, we achieved the\nhighest per-class F1-scores of 0.9104 and 0.4420 in the W AKE\nand N1 stages, respectively. In this dataset, U-sleep achieved\nthe best performance on overall metrics and the per-class F1-\nscores of the N2, N3, and REM stages. In the overall metrics,\nthe classification accuracy, kappa value, and F1-score were\n0.8410, 0.7800, and 0.7690, respectively. The per-class F1-\nscores at the N2, N3, and REM stages were 0.8580, 0.8330,\nand 0.8600, respectively. Although DFformer did not achieve\nthe highest performance on every metric for all datasets, it\nwas verified that DFformer performed competitively compared\nwith the conventional architectures, regardless of the domains.\nB. Evaluation of the pre-trained model‚Äôs efficacy\nTo assess the efficacy of using a pre-trained model de-\nrived from the autoencoder-based signal reconstruction task,\nwe evaluated the performance of the fine-tuned models in\neach of the previously utilized four datasets. As shown in\nFig. 3, the data in black represent the baseline performance\nevaluated using DFformer trained from scratch on the selected\nbaseline dataset. Furthermore, the performance differences are\npresented as ratios on the radar chart, to allow an intuitive\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2024.3355434\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAUTHOR Kim et al.: TOWARDS DOMAIN-FREE TRANSFORMER FOR GENERALIZED EEG PRE-TRAINING 7\ncomparison across various indicators. For MI datasets, in\naddition to the three indicators used in the previous evaluation\nstep, we compared the per-class F1-scores, similar to the sleep\nstage classification evaluation indicators. For the sleep stage\nclassification datasets, we evaluated the effectiveness of the\npre-trained model using the identical indicators which were\nutilized in the previous assessment.\nAs shown in Fig. 3(a), when BCIC2a was used as the base-\nline dataset, the performance was mostly improved when fine-\ntuning DFformer using models pre-trained on other datasets. In\nparticular, using the model pre-trained on BCIC2b, one of the\nMI datasets, there was significant performance improvement,\nwith an increase of 2.50 % in the kappa value and 1.46 %\nin the F1-score. Moreover, when the model pre-trained on\nSHHS was applied, the accuracy improved by 2.76 %. As\nshown in Fig. 3(b), when BCIC2b was chosen as the baseline\ndataset, the scores of all indicators were enhanced, regardless\nof the dataset used for pre-training. Notably, with the model\npre-trained on BCIC2a, the performance was significantly\nimproved, with increases of 2.77 % in accuracy, 4.67 %\nin the kappa value, and 1.46 % in the F1-score. Fig. 3(c)\nindicates the results obtained when Sleep-EDF was chosen as\nthe baseline dataset. All overall metrics showed improvements\nwhen utilizing models pre-trained on the other datasets. In\nparticular, the per-class F1-score of the N1 stage showed\nan average gain of 4.81 % when the pre-trained models\nwere applied. However, as shown in Fig. 3(d), for SHHS,\nwe observed that only when utilizing the model pre-trained\non Sleep-EDF, another sleep stage classification dataset, did\nthe performances aligned closely with DFformer trained from\nscratch. In other cases, the performances decreased compared\nwith the baseline.\nBased on these results, we confirmed that fine-tuning with\nmodels pre-trained on datasets of similar tasks generally\nenhanced the performance. Impressively, we found that the\nutilization of the pre-trained model was effective even though\nthe channel types, numbers, and sampling rates were different.\nThis suggests that the information embedded in the pre-trained\nmodels could guide DFformer toward better decoding out-\ncomes, capturing foundational characteristics of EEG signals\nwithout being heavily biased by dataset-specific features. We\nobserved considerable performance enhancement when MI\ndatasets with limited data were fine-tuned using pre-trained\nmodels. In contrast, in the case of sleep stage classification\ndatasets with enormous amounts of data, we found that\nutilizing pre-trained models from limited data might have a\nnegative effect, due to the discrepancy in distribution between\nthe datasets.\nIV. DISCUSSION\nWe confirmed the enhanced decoding performance when\nDFformer was fine-tuned with models pre-trained on various\ndomains. To better understand DFformer, we analyzed the fea-\ntures learned during the pre-training phase, which contributed\nto performance improvement during fine-tuning. Additionally,\nsince intra-channel class tokens were intended to capture\ndistinctive features within a single channel and inter-channel\nFzFC3FC1FCzFC2FC4C5C3C1CzC2C4C6CP3CP1CPzCP2CP4P1PzP2POz\n1.00.90.80.70.60.5\n(a) (b)\nFzFC3FC1FCzFC2FC4C5C3C1CzC2C4C6CP3CP1CPzCP2CP4P1PzP2POz\nFig. 4. The grand-average channel connectivity in BCIC2a based\non cosine similarity within DFformer‚Äôs intra-channel class tokens, pre-\ntrained with (a) Sleep-EDF and (b) SHHS.\nclass tokens were aimed to extract the correlations between\nchannels at specific times, we investigated whether both types\nof channel class tokens were trained as intended.\nA. Generalization performance of the pre-trained model\nDuring the pre-training phase, we intended to train the\nfundamental features of EEG signals without being biased\nby the unique characteristics of the datasets. We compared\nthe similarity using the cosine similarity between the intra-\nchannel class tokens of BCIC2a encoded by two DFformers,\npre-trained using Sleep-EDF and SHHS. Connectivity maps\nwere created by displaying the 100 highest similarity scores for\neach representation. In addition, we plotted the grand-average\ndata from all BCIC2a participants, to observe their general\ntendencies.\nAs shown in Fig. 4, although BCIC2a was inferred by\nDFformer only pre-trained on the sleep stage classification\ndatasets, we found that DFformer could cluster channels lo-\ncated in the identical brain regions associated with MI features.\nAs shown in Fig. 4(a), the model pre-trained on Sleep-\nEDF showed high correlations within channels grouped into\nthree distinct regions: the central, fronto-central, and centro-\nparietal regions. Meanwhile, based on the model pre-trained\non SHHS, the channels were mainly clustered in two regions:\nthe central and fronto-central regions, as shown in Fig. 4(b).\nNotably, when using the model pre-trained on SHHS, which\nachieved the highest accuracy among the models fine-tuned to\nBCIC2a, we observed that the channels in the central region,\nrecognized as an important region for classifying MI dataset\n[52], showed the highest correlation. Based on these results,\neven though the model was pre-trained using the sleep stage\nclassification dataset unrelated to MI, the pre-trained DFformer\ncould capture significant MI-related features. Therefore, since\nDFformer could learn the fundamental features of EEG signals\nduring the pre-training phase without a bias in dataset-specific\ncharacteristics, we determined that the pre-trained DFformer\nachieved significant generalization performance, regardless of\nthe domain.\nB. Effectiveness of applying the pre-trained model\nWe conducted a qualitative analysis to understand the effec-\ntiveness of DFformer fine-tuned with a pre-trained model. We\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2024.3355434\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n8 GENERIC COLORIZED JOURNAL, VOL. XX, NO. XX, XXXX 2023\n333333331221222222212212222224\n333333331221222222212212222224333333331221222222212212222224\n333333331221222222212212222224333333331221222222212212222224\n333333331221222222212212222224\n1.0\n0.9\n0.8\n0.7\n0.6\n0.5\n1.0\n0.9\n0.8\n0.7\n0.6\n0.5333333331221222222212212222224\n333333331221222222212212222224333333331221222222212212222224\n333333331221222222212212222224333333331221222222212212222224\n333333331221222222212212222224\n1.0\n0.9\n0.8\n0.7\n0.6\n0.5333333331221222222212212222224\n333333331221222222212212222224333333331221222222212212222224\n333333331221222222212212222224333333331221222222212212222224\n333333331221222222212212222224\n(a)Embedding Block1st Block2nd Block\nModel trained from scratchFine-tuned modelPre-trained model\n1.0\n0.9\n0.8\n0.7\n0.6\n0.5333333333333322223324444444444\n333333333333322223324444444444 333333333333322223324444444444\n333333333333322223324444444444 333333333333322223324444444444\n333333333333322223324444444444\n333333333333322223324444444444\n333333333333322223324444444444 333333333333322223324444444444\n333333333333322223324444444444 333333333333322223324444444444\n333333333333322223324444444444\n1.0\n0.9\n0.8\n0.7\n0.6\n0.5\n333333333333322223324444444444\n333333333333322223324444444444 333333333333322223324444444444\n333333333333322223324444444444 333333333333322223324444444444\n333333333333322223324444444444\n1.0\n0.9\n0.8\n0.7\n0.6\n0.5\n(b)Embedding Block1st Block2nd Block\nModel trained from scratchFine-tuned modelPre-trained model\nFig. 5. Visualization of the similarity matrices between each intra-channel class token in each block. (a) and (b) represent data of random samples\nfrom Sleep-EDF . The first row indicates the similarity matrices derived from DFformer pre-trained with BCIC2a. The second row presents the\nsimilarity matrices from DFformer fine-tuned with Sleep-EDF , beginning with the pre-trained model from BCIC2a. The third row shows the similarity\nmatrices of DFformer trained entirely from scratch with Sleep-EDF . The x- and y-axes represent the labels for each channel in the data. 1, 2, 3, and\n4 indicate N1, N2, N3, and REM, respectively.\nutilized DFformer fine-tuned to Sleep-EDF based on the pre-\ntrained model from BCIC2a which showed the most significant\nimprovement in the per-class F1-score for the N1, which\nis the most challenging sleep stage to classify. We assessed\nthe similarity between intra-channel class tokens within each\nblock of DFformer by generating a similarity matrix using\ncosine similarity. For this comparison, we used three models:\nDFformer pre-trained on BCIC2a, DFformer fine-tuned on\nSleep-EDF using the pre-trained model, and DFformer trained\nfrom scratch on Sleep-EDF.\nFig. 5 shows the similarity matrices generated by three\nmodels based on the randomly selected two data in Sleep-\nEDF. We found that, as the depth of the model increased,\nthe difference in similarity between intra-channel class to-\nkens from different sleep stages became more apparent. In\naddition, a high similarity was observed between the identical\nsleep stages, despite inferring data from Sleep-EDF using the\nmodel pre-trained on MI dataset, as shown in the first row\nof Fig. 5(a) and 5(b). We also found significant difference\nbetween the similarity matrices of the fine-tuned model and\nthose of the model trained from scratch. When the patterns\nof the sleep stages shifted, DFformer trained from scratch\nstruggled to specify the transition points between the different\nsleep stages. By contrast, the fine-tuned DFformer showed\nan enhanced performance in recognizing these moments of\ntransition. Moreover, as shown in the second row of Fig. 5(a),\nthe fine-tuned DFformer was more effective in identifying tem-\nporary changes in the sleep stages, compared with DFformer\ntrained from scratch. Based on these results, when DFformer\nis fine-tuned using the pre-trained model, the training begins\nfrom a more refined starting point than when training from\nscratch. Thus, we confirmed that utilizing the pre-trained\nmodel is significantly important to enhance the performance\nof DFformer.\nC. Analysis of intra- and inter-class tokens\nThe intra- and inter-channel class tokens have specific roles\nin decoding EEG signals. We evaluated whether the tokens\nwere trained as intended. DCformer fine-tuned on BCIC2a\nwith the model pre-trained on SHHS was used. Based on\nthis model, we evaluated the accurately classified data from\nBCIC2a and investigated the similarity matrix between the\nintra- and inter-channel class tokens. Using this similarity\nmatrix, we identified the pivotal channels at specific moments.\nFig. 6(a) shows the difference in the similarity matrix\nbetween the entire data and class-specific data. To identify the\nfeature differences between classes relative to the entire data,\nwe computed the difference between the average similarity\nmatrices for all data and each class. Fig. 6(a)-i), ii), iii), and\niv) represent the results from the Left, Right, Foot, and Tongue\nclasses in BCIC2a, respectively. The greatest differences were\nfound at the C4 for Left, the C3 for Right, the Cz for Foot,\nand the POz for Tongue. These observations are in agreement\nwith established neurophysiological knowledge. Specifically,\nwhen imagining the movements of the right and left hands,\nthe left and right areas of the motor cortex are activated,\nrespectively, due to the crossing over of motor nerve fibers,\nknown as pyramidal decussation [53]. Additionally, the central\nregion of the motor cortex is stimulated when imagining foot\nmovements [54]. For imagining tongue movement, the parietal\nlobe exhibited apparent differences compared to the overall\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2024.3355434\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAUTHOR Kim et al.: TOWARDS DOMAIN-FREE TRANSFORMER FOR GENERALIZED EEG PRE-TRAINING 9\n(a) (b)\nFzFC3FC1FCzFC2FC4C5C3C1CzC2C4C6CP3CP1CPzCP2CP4P1PzP2POz\n0 20 40 60 80 100\n0.050.040.030.020.010.00\nFzFC3FC1FCzFC2FC4C5C3C1CzC2C4C6CP3CP1CPzCP2CP4P1PzP2POz 0.050.040.030.020.010.000 20 40 60 80 100\nFzFC3FC1FCzFC2FC4C5C3C1CzC2C4C6CP3CP1CPzCP2CP4P1PzP2POz 0.050.040.030.020.010.000 20 40 60 80 100\nFzFC3FC1FCzFC2FC4C5C3C1CzC2C4C6CP3CP1CPzCP2CP4P1PzP2POz 0.050.040.030.020.010.000 20 40 60 80 100\n1) 2) 3) 4) 1.51.00.50.0-0.5-1.0-1.5\n1) 2) 3) 4) 1.51.00.50.0-0.5-1.0-1.5\nFzFC3FC1FCzFC2FC4C5C3C1CzC2C4C6CP3CP1CPzCP2CP4P1PzP2POz\n0 20 40 60 80 100\n0.30.20.10.0-0.1-0.2-0.3\n1) 2)3) 4)\nFzFC3FC1FCzFC2FC4C5C3C1CzC2C4C6CP3CP1CPzCP2CP4P1PzP2POz\n 0.30.20.10.0-0.1-0.2-0.30 20 40 60 80 100\n1) 2) 3) 4)\ni)\nii)\niii)\niv)\ni)\nii)\nFig. 6. (a) The grand-average difference between the similarity matrices for the entire BCIC2a dataset and its individual classes. These matrices\nare derived from the cosine similarity between intra- and inter-channel class tokens in DFformer pre-trained with SHHS. The class of each matrix\nis i) Left, ii) Right, iii) Foot, and iv) Tongue, respectively. (b) The first and third rows indicate the difference between the similarity matrix of randomly\nsampled data from BCIC2a and the average matrix for all data. The second and fourth rows show the topographies associated with the two highest\nand two lowest sums of differences along the temporal axis of the similarity matrix. The class of each matrix isi) Left and ii) Right, respectively. The\nx- and y-axes indicate the list of channels and temporally tokenized patches, respectively.\ndata, given the close connection between tongue movement\nand speech processes [55]. Furthermore, significant differences\nwere observed in the channels located at both ends of the\nmotor cortex, C5 and C6, which are associated with tongue\nmovement [56].\nTo compare the difference between the entire data and class-\nspecific single trial data, we used the data from the Left and\nRight classes, where the target activation region is clearly\ndifferent. Fig. 6(b)-i) and 6(b)-ii) present the results from Left\nand Right, respectively. In Fig. 6(b), the first row of each\nfigure represents the disparity between the average similarity\nmatrices for the entire data and those of the individual trial\ndata. The second row of each figure shows the topographies\nof EEG signals at the time points correlated with the black\nboxes in the first row. We investigated the actual brainwave\npatterns corresponding to these differences in similarity based\non these topographies. The black boxes indicate the moments\nat which the two greatest and two smallest value differences\nexisted in the average similarity score on the temporal axis.\nBefore visualizing the topographies, we utilized averaged re-\nreferencing. We verified that the topographies for the moments\nwith significant differences from the overall data contained\nthe neurophysiological characteristics corresponding to each\nclass, such as the activation of the right motor cortex during\nleft hand imagery and the left motor cortex during right-hand\nimagery [53]. In contrast, for moments with low similarity\ndifferences compared to the entire data, the topographies\ncontained features that were less correlated with each class.\nFrom the analysis presented in Fig. 6(a), it is evident that\nthe intra-channel class tokens accurately capture the dominant\nintra-channel attributes of the data. Meanwhile, the analysis in\nFig. 6(b) reveals that the inter-channel class tokens incorporate\nthe inter-channel relationships at specific time intervals. Thus,\nwe confirmed that DFformer was trained in alignment with its\nintended design.\nV. CONCLUSIONS\nIn this paper, we proposed the domain-free transformer,\nDFformer, that could be utilized in datasets with various\nconfigurations, without modifying the architecture. Although\nresearch on developing the pre-trained model has been con-\nducted in the EEG domain, there remains an issue with\ndeveloping and applying the pre-trained model in previous\nresearch. Since the configurations of each dataset are different,\napplying the pre-trained model to other datasets without mod-\nifying the architecture is difficult. Unifying the configuration\nof datasets through channel selection and interpolation raises\nanother problem that distorts the original data. To alleviate this\nissue, we designed DFformer that could be applied to various\ndatasets without modifying the architecture, by separating\nthe encoding part of intra- and inter-channel information. In\naddition, based on the intra- and inter-class tokens, we could\ndevelop the pre-trained model by conducting the autoencoder-\nbased reconstruction task without distorting the original EEG\nsignals. DFformer achieved classification accuracies of 0.5841\nand 0.7618 for BCIC2a and BCIC2b, respectively. In the\nsleep stage classification dataset, DFformer achieved overall\nF1-scores of 0.7809 and 0.7602 for Sleep-EDF and SHHS,\nrespectively. Therefore, we confirmed that DFformer achieved\ncompetitive decoding performances compared with the con-\nventional methods, regardless of the domain. In addition, we\nverified the performance improvement when DFformer was\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2024.3355434\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n10 GENERIC COLORIZED JOURNAL, VOL. XX, NO. XX, XXXX 2023\nfine-tuned based on the pre-trained model. We also demon-\nstrated the effect of using the pre-trained model for fine-tuning\ncompared to the model trained from scratch, by analyzing\nthe class tokens. As shown in the above results, the designed\nDFformer showed significant performance regardless of the\ndatasets used, without modifying the architecture. Further-\nmore, we could successfully transfer the information between\ndatasets with different domains by developing the pre-trained\nmodel based on DFformer. Hence, we could alleviate the issue\nwith the conventional approach of developing the pre-trained\nmodel. However, there still remain limitations in our approach.\nAlthough the class token, marked by the dotted line in Fig.\n1, could be utilized as the representative token of the single-\ntrial EEG data, it was difficult to optimize. In addition, when\nfine-tuning to SHHS using the pre-trained model developed\nfrom the autoencoder-based reconstruction task, we achieved\nthe relatively low-performance improvement compared with\nother datasets. In the future, we will develop the improved\nself-supervised method based on DFformer to create a pre-\ntrained model. By applying the refined self-supervised method,\nwe will develop more optimized pre-training models that could\nimprove the performance on a wider variety of datasets. Hence,\nwe will construct the generalized model that could be applied\nas the foundation model for decoding EEG data.\nACKNOWLEDGMENT\nThe authors would like to thank the reviewers for providing\ncomments which significantly improve the quality of this\npaper.\nREFERENCES\n[1] N. Richer, R. J. Downey, W. D. Hairston, D. P. Ferris, and A. D. Nordin,\n‚ÄúMotion and muscle artifact removal validation using an electrical head\nphantom, robotic motion platform, and dual layer mobile EEG,‚Äù IEEE\nTrans. Neural Syst. Rehabil. Eng., vol. 28, no. 8, pp. 1825‚Äì1835, 2020.\n[2] M.-H. Lee, J. Williamson, D.-O. Won, S. Fazli, and S.-W. Lee, ‚ÄúA high\nperformance spelling system based on EEG-EOG signals with visual\nfeedback,‚Äù IEEE Trans. Neural Syst. Rehabil. Eng., vol. 26, no. 7, pp.\n1443‚Äì1459, 2018.\n[3] D.-O. Won, H.-J. Hwang, D.-M. Kim, K.-R. M ¬®uller, and S.-W. Lee,\n‚ÄúMotion-based rapid serial visual presentation for gaze-independent\nbrain-computer interfaces,‚Äù IEEE Trans. Neural Syst. Rehabil. Eng.,\nvol. 26, no. 2, pp. 334‚Äì343, 2017.\n[4] L. F. Nicolas-Alonso and J. Gomez-Gil, ‚ÄúBrain computer interfaces, a\nreview,‚Äù Sensors, vol. 12, no. 2, pp. 1211‚Äì1279, 2012.\n[5] D.-H. Lee, J.-H. Jeong, B.-W. Yu, T.-E. Kam, and S.-W. Lee, ‚ÄúAu-\ntonomous system for EEG-based multiple abnormal mental states clas-\nsification using hybrid deep neural networks under flight environment,‚Äù\nIEEE Trans. Syst. Man Cybern. Syst., vol. 53, no. 10, pp. 6426‚Äì6437,\n2023.\n[6] D.-H. Lee, J.-H. Jeong, K. Kim, B.-W. Yu, and S.-W. Lee, ‚ÄúContinuous\nEEG decoding of pilots‚Äô mental states using multiple feature block-\nbased convolutional neural network,‚Äù IEEE Access, vol. 8, pp. 121 929‚Äì\n121 941, 2020.\n[7] K. K. Ang, Z. Y . Chin, H. Zhang, and C. Guan, ‚ÄúFilter bank common\nspatial pattern (FBCSP) in brain-computer interface,‚Äù in Int. Jt. Conf.\nNeural Netw. (IJCNN), 2008, pp. 2390‚Äì2397.\n[8] S.-B. Lee et al., ‚ÄúComparative analysis of features extracted from EEG\nspatial, spectral and temporal domains for binary and multiclass motor\nimagery classification,‚Äù Inf. Sci., vol. 502, pp. 190‚Äì200, 2019.\n[9] H.-I. Suk, S. Fazli, J. Mehnert, K.-R. M ¬®uller, and S.-W. Lee, ‚ÄúPredicting\nBCI subject performance using probabilistic spatio-temporal filters,‚Äù\nPloS one, vol. 9, no. 2, p. e87056, 2014.\n[10] S.-H. Lee, M. Lee, and S.-W. Lee, ‚ÄúNeural decoding of imagined speech\nand visual imagery as intuitive paradigms for BCI communication,‚Äù\nIEEE Trans. Neural Syst. Rehabil. Eng., vol. 28, no. 12, pp. 2647‚Äì2659,\n2020.\n[11] H. Altaheri et al., ‚ÄúDeep learning techniques for classification of\nelectroencephalogram (EEG) motor imagery (MI) signals: A review,‚Äù\nNeural. Comput. Appl., pp. 1‚Äì42, 2021.\n[12] M. Lee, H.-G. Kwak, H.-J. Kim, D.-O. Won, and S.-W. Lee, ‚ÄúSe-\nriesSleepNet: An EEG time series model with partial data augmentation\nfor automatic sleep stage scoring,‚Äù Front. Physiol., vol. 14, 2023.\n[13] Z. Jia et al., ‚ÄúSalientSleepNet: Multimodal salient wave detection\nnetwork for sleep staging,‚Äù in Int. Jt. Conf. Artif. Intell. (IJCAI), 2021,\npp. 1‚Äì7.\n[14] R. Peng et al., ‚ÄúTIE-EEGNet: Temporal information enhanced EEGNet\nfor seizure subtype classification,‚Äù IEEE Trans. Neural Syst. Rehabil.\nEng., vol. 30, pp. 2567‚Äì2576, 2022.\n[15] I. Ahmad et al., ‚ÄúA hybrid deep learning approach for epileptic seizure\ndetection in EEG signals,‚Äù IEEE J. Biomed. Health Inform., 2023.\n[16] H.-J. Ahn, D.-H. Lee, J.-H. Jeong, and S.-W. Lee, ‚ÄúMultiscale convolu-\ntional transformer for EEG classification of mental imagery in different\nmodalities,‚Äù IEEE Trans. Neural Syst. Rehabil. Eng., vol. 31, pp. 646‚Äì\n656, 2022.\n[17] S. Li, Z. Wang, H. Luo, L. Ding, and D. Wu, ‚ÄúT-TIME: Test-time\ninformation maximization ensemble for plug-and-play BCIs,‚Äù IEEE\nTrans. Biomed. Eng., 2023.\n[18] Z. Liang et al., ‚ÄúEEGFuseNet: Hybrid unsupervised deep feature char-\nacterization and fusion for high-dimensional EEG with an application to\nemotion recognition,‚Äù IEEE Trans. Neural Syst. Rehabil. Eng., vol. 29,\npp. 1913‚Äì1925, 2021.\n[19] W. Zhang, Z. Wang, and D. Wu, ‚ÄúMulti-source decentralized transfer\nfor privacy-preserving BCIs,‚Äù IEEE Trans. Neural Syst. Rehabil. Eng.,\nvol. 30, pp. 2710‚Äì2720, 2022.\n[20] Z. Wan, R. Yang, M. Huang, N. Zeng, and X. Liu, ‚ÄúA review on transfer\nlearning in EEG signal analysis,‚Äù Neurocomputing, vol. 421, pp. 1‚Äì14,\n2021.\n[21] C. Tan et al., ‚ÄúA survey on deep transfer learning,‚Äù in Int. Conf. Artif.\nNeural Netw. (ICANN), 2018, pp. 270‚Äì279.\n[22] X. Han et al., ‚ÄúPre-trained models: Past, present and future,‚Äù AI Open,\nvol. 2, pp. 225‚Äì250, 2021.\n[23] Y . He, Z. Lu, J. Wang, S. Ying, and J. Shi, ‚ÄúA self-supervised\nlearning based channel attention MLP-Mixer network for motor imagery\ndecoding,‚Äù IEEE Trans. Neural Syst. Rehabil. Eng., vol. 30, pp. 2406‚Äì\n2417, 2022.\n[24] X. Jiang, J. Zhao, B. Du, and Z. Yuan, ‚ÄúSelf-supervised contrastive\nlearning for EEG-based sleep staging,‚Äù in Int. Jt. Conf. Neural Netw.\n(IJCNN), 2021, pp. 1‚Äì8.\n[25] Z. Zhang, S.-H. Zhong, and Y . Liu, ‚ÄúGANSER: A self-supervised data\naugmentation framework for EEG-based emotion recognition,‚Äù IEEE\nTrans. Affect. Comput., 2022.\n[26] D. Wu, Y . Xu, and B.-L. Lu, ‚ÄúTransfer learning for EEG-based brain-\ncomputer interfaces: A review of progress made since 2016,‚Äù IEEE\nTrans. Cogn. Develop. Syst., vol. 14, no. 1, pp. 4‚Äì19, 2020.\n[27] D. Kostas, S. Aroca-Ouellette, and F. Rudzicz, ‚ÄúBENDR: Using trans-\nformers and a contrastive self-supervised learning task to learn from\nmassive amounts of EEG data,‚Äù Front. Hum. Neurosci., vol. 15, p.\n653659, 2021.\n[28] X. Wei et al., ‚Äú2021 BEETL Competition: Advancing transfer learning\nfor subject independence & heterogenous EEG data sets,‚Äù in Adv. Neural\nInf. Process. Syst. (NeurIPS), 2022, pp. 205‚Äì219.\n[29] M. Svantesson, H. Olausson, A. Eklund, and M. Thordstein, ‚ÄúVirtual\nEEG-electrodes: Convolutional neural networks as a method for upsam-\npling or restoring channels,‚Äù J. Neurosci. Methods, vol. 355, p. 109126,\n2021.\n[30] A. Al-Saegh, S. A. Dawwd, and J. M. Abdul-Jabbar, ‚ÄúDeep learning\nfor motor imagery EEG-based classification: A review,‚Äù Biomed. Signal\nProcess. Control, vol. 63, p. 102172, 2021.\n[31] N. Goshtasbi, R. Boostani, and S. Sanei, ‚ÄúSleepFCN: A fully convo-\nlutional deep learning framework for sleep stage classification using\nsingle-channel electroencephalograms,‚Äù IEEE Trans. Neural Syst. Reha-\nbil. Eng., vol. 30, pp. 2088‚Äì2096, 2022.\n[32] C. Brunner, R. Leeb, G. M ¬®uller-Putz, A. Schl ¬®ogl, and G. Pfurtscheller,\n‚ÄúBCI competition 2008-Graz data set A,‚Äù Inst. Knowl. Discovery, Lab.\nBrain-Comput. Interfaces, Graz Univ. Technol., vol. 16, pp. 1‚Äì6, 2008.\n[33] S.-J. Kim, D.-H. Lee, and S.-W. Lee, ‚ÄúRethinking CNN architecture for\nenhancing decoding performance of motor imagery-based EEG signals,‚Äù\nIEEE Access, vol. 10, pp. 96 984‚Äì96 996, 2022.\n[34] R. Leeb, C. Brunner, G. M ¬®uller-Putz, A. Schl ¬®ogl, and G. Pfurtscheller,\n‚ÄúBCI competition 2008-Graz data set B,‚Äù Graz Univ. Technol., Austria,\npp. 1‚Äì6, 2008.\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2024.3355434\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAUTHOR Kim et al.: TOWARDS DOMAIN-FREE TRANSFORMER FOR GENERALIZED EEG PRE-TRAINING 11\n[35] A. L. Goldberger et al., ‚ÄúPhysioBank, PhysioToolkit, and PhysioNet:\nComponents of a new research resource for complex physiologic sig-\nnals,‚Äù Circulation, vol. 101, no. 23, pp. e215‚Äìe220, 2000.\n[36] B. Kemp, A. H. Zwinderman, B. Tuk, H. A. Kamphuisen, and J. J.\nOberye, ‚ÄúAnalysis of a sleep-dependent neuronal feedback loop: The\nslow-wave microcontinuity of the EEG,‚Äù IEEE Trans. Biomed. Eng.,\nvol. 47, no. 9, pp. 1185‚Äì1194, 2000.\n[37] C. M. Sinton and R. W. McCarley, ‚ÄúNeurophysiological mechanisms\nof sleep and wakefulness: A question of balance,‚Äù in Seminars in\nNeurology, vol. 24, no. 03, 2004, pp. 211‚Äì223.\n[38] S. F. Quan et al., ‚ÄúThe sleep heart health study: Design, rationale, and\nmethods,‚Äù Sleep, vol. 20, no. 12, pp. 1077‚Äì1085, 1997.\n[39] A. Baevski, Y . Zhou, A. Mohamed, and M. Auli, ‚Äúwav2vec 2.0: A\nframework for self-supervised learning of speech representations,‚Äù in\nAdv. Neural Inf. Process. Syst. (NeurIPS), 2020, pp. 12 449‚Äì12 460.\n[40] Y . Song, Q. Zheng, B. Liu, and X. Gao, ‚ÄúEEG conformer: Convolutional\ntransformer for EEG decoding and visualization,‚Äù IEEE Trans. Neural\nSyst. Rehabil. Eng., vol. 31, pp. 710‚Äì719, 2022.\n[41] R. Boostani, F. Karimzadeh, and M. Nami, ‚ÄúA comparative review on\nsleep stage classification methods in patients and healthy individuals,‚Äù\nComput. Methods Programs Biomed., vol. 140, pp. 77‚Äì91, 2017.\n[42] C. S. Nam, Y . Jeon, Y .-J. Kim, I. Lee, and K. Park, ‚ÄúMovement imagery-\nrelated lateralization of event-related (de) synchronization (ERD/ERS):\nMotor-imagery duration effects,‚Äù Clin. Neurophysiol., vol. 122, no. 3,\npp. 567‚Äì577, 2011.\n[43] X. Liu et al., ‚ÄúSelf-supervised learning: Generative or contrastive,‚Äù IEEE\nTrans. Knowl. Data Eng., vol. 35, no. 1, pp. 857‚Äì876, 2021.\n[44] E. Eldele et al., ‚ÄúAn attention-based deep learning approach for sleep\nstage classification with single-channel EEG,‚Äù IEEE Trans. Neural Syst.\nRehabil. Eng., vol. 29, pp. 809‚Äì818, 2021.\n[45] G. Dornhege, J. R. Mill ¬¥an, T. Hinterberger, D. J. McFarland, and K.-R.\nM¬®uller, Evaluation Criteria for BCI Research. MIT Press, 2007, ch.\nToward Brain-Computer Interfacing, pp. 327‚Äì342.\n[46] A. Guillot and V . Thorey, ‚ÄúRobustSleepNet: Transfer learning for\nautomated sleep staging at scale,‚Äù IEEE Trans. Neural Syst. Rehabil.\nEng., vol. 29, pp. 1441‚Äì1451, 2021.\n[47] R. T. Schirrmeister et al., ‚ÄúDeep learning with convolutional neural\nnetworks for EEG decoding and visualization,‚Äù Hum. Brain Mapp.,\nvol. 38, no. 11, pp. 5391‚Äì5420, 2017.\n[48] V . J. Lawhern et al., ‚ÄúEEGNet: A compact convolutional neural network\nfor EEG-based brain-computer interfaces,‚Äù J. Neural Eng., vol. 15, no. 5,\np. 056013, 2018.\n[49] A. Supratak, H. Dong, C. Wu, and Y . Guo, ‚ÄúDeepSleepNet: A model\nfor automatic sleep stage scoring based on raw single-channel EEG,‚Äù\nIEEE Trans. Neural Syst. Rehabil. Eng., vol. 25, no. 11, pp. 1998‚Äì2008,\n2017.\n[50] M. Perslev et al., ‚ÄúU-Sleep: Resilient high-frequency sleep staging,‚Äù NPJ\nDigit. Med., vol. 4, no. 1, p. 72, 2021.\n[51] H. He and D. Wu, ‚ÄúTransfer learning for brain‚Äìcomputer interfaces: A\nEuclidean space data alignment approach,‚Äù IEEE Trans. Biomed. Eng.,\nvol. 67, no. 2, pp. 399‚Äì410, 2019.\n[52] E. Marieb and K. Hoehn, Human Anatomy & Physiology. Pearson\nBenjamin Cummings, 2007.\n[53] K. S. Saladin, C. A. Gan, and H. N. Cushman, Anatomy & Physiology:\nThe Unity of Form and Function, 2021.\n[54] M. Hamedi, S.-H. Salleh, and A. M. Noor, ‚ÄúElectroencephalographic\nmotor imagery brain connectivity analysis for BCI: A review,‚Äù Neural\nComput., vol. 28, no. 6, pp. 999‚Äì1041, 2016.\n[55] J. U. Henschke and J. M. Pakan, ‚ÄúEngaging distributed cortical and\ncerebellar networks through motor execution, observation, and imagery,‚Äù\nFront. Syst. Neurosci., vol. 17, p. 1165307, 2023.\n[56] W. Penfield and E. Boldrey, ‚ÄúSomatic motor and sensory representation\nin the cerebral cortex of man as studied by electrical stimulation,‚Äù Brain,\nvol. 60, no. 4, pp. 389‚Äì443, 1937.\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2024.3355434\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7880443334579468
    },
    {
      "name": "Electroencephalography",
      "score": 0.731426477432251
    },
    {
      "name": "Software portability",
      "score": 0.7068910002708435
    },
    {
      "name": "Transformer",
      "score": 0.6326591968536377
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5833116173744202
    },
    {
      "name": "Machine learning",
      "score": 0.4600491523742676
    },
    {
      "name": "Deep learning",
      "score": 0.45727092027664185
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.369645893573761
    },
    {
      "name": "Speech recognition",
      "score": 0.3616122007369995
    },
    {
      "name": "Engineering",
      "score": 0.0999077558517456
    },
    {
      "name": "Neuroscience",
      "score": 0.07249987125396729
    },
    {
      "name": "Voltage",
      "score": 0.06827020645141602
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I197347611",
      "name": "Korea University",
      "country": "KR"
    }
  ]
}