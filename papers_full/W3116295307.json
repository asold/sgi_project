{
  "title": "IndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained Language Model for Indonesian NLP",
  "url": "https://openalex.org/W3116295307",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A1976851905",
      "name": "Fajri Koto",
      "affiliations": [
        "University of Melbourne"
      ]
    },
    {
      "id": "https://openalex.org/A2115447961",
      "name": "Afshin Rahimi",
      "affiliations": [
        "University of Queensland"
      ]
    },
    {
      "id": "https://openalex.org/A2095936123",
      "name": "Jey Han Lau",
      "affiliations": [
        "University of Melbourne"
      ]
    },
    {
      "id": "https://openalex.org/A2096893938",
      "name": "Timothy Baldwin",
      "affiliations": [
        "University of Melbourne"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3110604908",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2250338670",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2307381258",
    "https://openalex.org/W2783617855",
    "https://openalex.org/W2174661732",
    "https://openalex.org/W2963324947",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W3102483398",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W1547763504",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2044599851",
    "https://openalex.org/W1502539328",
    "https://openalex.org/W2801524894",
    "https://openalex.org/W3035497479",
    "https://openalex.org/W4250641076",
    "https://openalex.org/W2890043737",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W2096765155",
    "https://openalex.org/W3034715004",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W2740840489",
    "https://openalex.org/W2954278700",
    "https://openalex.org/W2140676672",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2923978210",
    "https://openalex.org/W2607331001",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W2988304195",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2283747648",
    "https://openalex.org/W3216164635",
    "https://openalex.org/W2970049541",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W4230725344",
    "https://openalex.org/W2189472871",
    "https://openalex.org/W3038440544",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W331019419",
    "https://openalex.org/W3098637735",
    "https://openalex.org/W2462305634",
    "https://openalex.org/W3014635508",
    "https://openalex.org/W2792017515",
    "https://openalex.org/W2897803242",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2108439614",
    "https://openalex.org/W2970850076",
    "https://openalex.org/W2801109130",
    "https://openalex.org/W2574032915",
    "https://openalex.org/W2963643701",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3086966320",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2559264616",
    "https://openalex.org/W2952750383",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W1568793342",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2552110825",
    "https://openalex.org/W3014667327",
    "https://openalex.org/W2088115296",
    "https://openalex.org/W2785461518",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2970529259",
    "https://openalex.org/W2963571341",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W2889088930",
    "https://openalex.org/W1997064390"
  ],
  "abstract": "Although the Indonesian language is spoken by almost 200 million people and the 10th most spoken language in the world, it is under-represented in NLP research. Previous work on Indonesian has been hampered by a lack of annotated datasets, a sparsity of language resources, and a lack of resource standardization. In this work, we release the IndoLEM dataset comprising seven tasks for the Indonesian language, spanning morpho-syntax, semantics, and discourse. We additionally release IndoBERT, a new pre-trained language model for Indonesian, and evaluate it over IndoLEM, in addition to benchmarking it against existing resources. Our experiments show that IndoBERT achieves state-of-the-art performance over most of the tasks in IndoLEM.",
  "full_text": "Proceedings of the 28th International Conference on Computational Linguistics, pages 757–770\nBarcelona, Spain (Online), December 8-13, 2020\n757\nIndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained\nLanguage Model for Indonesian NLP\nFajri Koto1 Afshin Rahimi2 Jey Han Lau1 Timothy Baldwin1\n1The University of Melbourne\n2The University of Queensland\nffajri@student.unimelb.edu.au, afshinrahimi@gmail.com\njeyhan.lau@gmail.com, tb@ldwin.net\nAbstract\nAlthough the Indonesian language is spoken by almost 200 million people and the 10th most-\nspoken language in the world, 1 it is under-represented in NLP research. Previous work on In-\ndonesian has been hampered by a lack of annotated datasets, a sparsity of language resources,\nand a lack of resource standardization. In this work, we release the I NDO LEM dataset com-\nprising seven tasks for the Indonesian language, spanning morpho-syntax, semantics, and dis-\ncourse. We additionally release I NDO BERT, a new pre-trained language model for Indonesian,\nand evaluate it over I NDO LEM, in addition to benchmarking it against existing resources. Our\nexperiments show that INDO BERT achieves state-of-the-art performance over most of the tasks\nin INDO LEM.\n1 Introduction\nDespite there being over 200M ﬁrst-language speakers of the Indonesian language, the language is under-\nrepresented in NLP. We argue that there are three root causes: a lack of annotated datasets, a sparsity\nof language resources, and a lack of resource standardization. In English, on the other hand, there are\never-increasing numbers of datasets for different tasks (Hermann et al., 2015; Luong and Manning, 2016;\nRajpurkar et al., 2018; Agirre et al., 2016), (pre-)trained models for language modelling and language\nunderstanding tasks (Devlin et al., 2019; Yang et al., 2019; Radford et al., 2019), and standardized tasks\nto benchmark research progress (Wang et al., 2019b; Wang et al., 2019a; Williams et al., 2018), all of\nwhich have contributed to rapid progress in the ﬁeld in recent years.\nWe attempt to redress this situation for Indonesian, as follows. First, we introduce I NDO LEM\n(“Indonesian Language Evaluation Montage”2), a comprehensive dataset encompassing seven NLP tasks\nand eight sub-datasets, ﬁve of which are based on previous work and three are novel to this work. As\npart of this, we standardize data splits and evaluation metrics, to enhance reproducibility and robust\nbenchmarking. These tasks are intended to span a broad range of morpho-syntactic, semantic, and dis-\ncourse analysis competencies for Indonesian, to be able to benchmark progress in Indonesian NLP. First,\nfor morpho-syntax, we examine part-of-speech (POS) tagging (Dinakaramani et al., 2014), dependency\nparsing with two Universal Dependency (UD) datasets, and two named entity recognition (NER) tasks\nusing public data. For semantics, we examine sentiment analysis and single-document summarization.\nFor discourse, we create two Twitter-based document coherence tasks: Twitter response prediction (as a\nmultiple-choice task), and Twitter document thread ordering.\nSecond, we develop and release I NDO BERT, a monolingual pre-trained BERT language model for\nIndonesian (Devlin et al., 2019). This is one of the ﬁrst monolingual BERT models for the Indonesian\nlanguage, trained following the best practice in the ﬁeld.3\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://\ncreativecommons.org/licenses/by/4.0/.\n1https://www.visualcapitalist.com/100-most-spoken-languages/\n2Yes, guilty as charged, a slightly-forced backronym from lem, which is Indonesian for “glue”, following the English\nbenchmark naming trend (e.g. GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a)).\n3Turns out we weren’t the ﬁrst to think to train a monolingual BERT model for Indonesian, or to name it IndoBERT, with (at\nleast) two contemporaneous BERT models for Indonesian that are named “IndoBERT”: Azhari and Lintang (2020) and Wilie\net al. (2020).\n758\nOur contributions in this paper are: (1) we release INDO LEM, which is by far the most comprehensive\nNLP dataset for Indonesian, and intended to provide a benchmark to catalyze further NLP research on\nthe language; (2) as part of I NDO LEM, we develop two novel discourse tasks and datasets; and (3)\nwe follow best practice in developing and releasing for general use I NDO BERT, a BERT model for\nIndonesian, which we show to be superior to existing pre-trained models based on I NDO LEM. The\nINDO LEM dataset, I NDO BERT model, and all code associated with this paper can be accessed at:\nhttps://indolem.github.io.\n2 Related Work\nTo comprehensively evaluate natural language understanding (NLU) methods for English, collections of\ntools and corpora such as GLUE (Wang et al., 2019b) and SuperGLUE (Wang et al., 2019a) have been\nproposed. Generally, such collections aim to benchmark models across various NLP tasks covering a\nvariety of corpus sizes, domains, and task formulations. GLUE comprises nine language understanding\ntasks built on existing public datasets, while SuperGLUE is a set of eight tasks that is not only diverse in\ntask format but also includes low-resource settings. SuperGLUE is a more challenging framework, and\nBERT models trail human performance by 20 points at the time of writing.\nIn the cross-lingual setting, XGLUE (Liang et al., 2020) was introduced as a benchmark dataset that\ncovers nearly 20 languages. Unlike GLUE, XGLUE includes language generation tasks such as question\nand headline generation. One of the largest cross-lingual corpora is dependency parsing provided by\nUniversal Dependencies.4 It has consistent annotation of 150 treebanks across 90 languages, constructed\nthrough an open collaboration involving many contributors. Recently, other cross-lingual benchmarks\nhave been introduced, such as Hu et al. (2020) and Lewis et al. (2020). While these three cross-lingual\nbenchmarks contain some resources/datasets for Indonesian, the coverage is low and data is limited.\nBeyond the English and cross-lingual settings, ChineseGLUE5 is a comprehensive NLU collection for\nMandarin Chinese, covering eight different tasks. For the Vietnamese language, Nguyen and Nguyen\n(2020) gathered a dataset covering four tasks (NER, POS tagging, dependency parsing, and language\ninference), and empirically evaluated them against a monolingual BERT. Elsewhere, there are individual\nefforts to maintain a systematic catalogue of tasks and datasets, and state-of-the-art methods for each\nacross multiple languages,6 including one speciﬁcally for Indonesian.7 However, there is no comprehen-\nsive dataset for evaluating NLU systems in the Indonesian language, a void which we seek to ﬁll with\nINDO LEM.\n3 I NDO BERT\nTransformers (Vaswani et al., 2017) have driven substantial progress in NLP research based on pre-\ntrained models in the last few years. Although attention-based models are data- and GPU-hungry, the\nfull attention mechanisms and parallelism offered by the transformer are highly compatible with the\nhigh levels of parallelism that GPU computation offers, and have been shown to be highly effective\nat capturing the syntax (Jawahar et al., 2019) and sentence semantics of text (Sun et al., 2019). In\nparticular, transformer-based language models (Devlin et al., 2019; Radford et al., 2018; Conneau and\nLample, 2019; Raffel et al., 2019) pre-trained on large volumes of text based on simple tasks such as\nmasked word prediction and sentence ordering prediction, have quickly become ubiquitous in NLP and\ndriven substantial empirical gains across tasks including NER (Devlin et al., 2019), POS tagging (Devlin\net al., 2019), single document summarization (Liu and Lapata, 2019), syntactic parsing (Kitaev et al.,\n2019), and discourse analysis (Nie et al., 2019). However, this effect has been largely observed for\nhigh-resource languages such as English.\nINDO BERT is a transformer-based model in the style of BERT (Devlin et al., 2019), but trained purely\nas a masked language model trained using the Huggingface8 framework, following the default conﬁgura-\n4https://universaldependencies.org/\n5https://github.com/ChineseGLUE/ChineseGLUE\n6https://github.com/sebastianruder/NLP-progress\n7https://github.com/kmkurn/id-nlp-resource\n8https://huggingface.co/\n759\nData #train #dev #test 5-Fold Evaluation\nMorpho-syntax/Sequence Labelling Tasks\nPOS Tagging* 7,222 802 2,006 Yes Accuracy\nNER UI 1,530 170 425 No micro-averaged F1\nNER UGM 1,687 187 469 No micro-averaged F1\nUD-Indonesian GSD* 4,477 559 557 No UAS, LAS\nUD-Indonesian PUD (Corrected Version) 700 100 200 Yes UAS, LAS\nSemantic Tasks\nSentiment Analysis 3,638 399 1,011 Yes F1\nIndoSum* 14,262 750 3,762 Yes ROUGE\nCoherency Tasks\nNext Tweet Prediction (NTP) 5,681 811 1,890 No Accuracy\nTweet Ordering 5,327 760 1,521 Yes Rank Corr\nTable 1: Summary of datasets incorporated in INDO BERT. Datasets marked with ‘*’ were already avail-\nable with canonical splits.\ntion for BERT-Base (uncased). It has 12 hidden layers each of 768d, 12 attention heads, and feed-forward\nhidden layers of 3,072d. We modify the Huggingface framework to read a separate text stream for dif-\nferent document blocks, 9 and set the training to use 512 tokens per batch. We train I NDO BERT with\n31,923-size Indonesian WordPiece vocabulary.\nIn total, we train I NDO BERT over 220M words, aggregated from three main sources: (1) Indonesian\nWikipedia (74M words); (2) news articles from Kompas,10 Tempo11 (Tala et al., 2003), and Liputan612\n(55M words in total); and (3) an Indonesian Web Corpus (Medved and Suchomel, 2017) (90M words).\nAfter preprocessing the corpus into 512-token document blocks, we obtain 1,067,581 train instances and\n13,985 development instances (without reduplication). In training, we use 4 Nvidia V100 GPUs (16GB\neach) with a batch size of 128, learning rate of 1e-4, the Adam optimizer, and a linear scheduler. We\ntrained the model for 2.4M steps (180 epochs) for a total of 2 calendar months,13 with the ﬁnal perplexity\nover the development set being 3.97 (similar to English BERT-base).\n4 I NDO LEM: Tasks\nIn this section, we present an overview of I NDO LEM, in terms of the NLP tasks and sub-datasets it\nincludes. We group the tasks into three categories: morpho-syntax/sequence labelling, semantics, and\ndiscourse coherence. We summarize the sub-datasets include in I NDO LEM in Table 1, in addition to\ndetailing related work on the respective tasks.\n4.1 Morpho-syntax and Sequence Labelling Tasks\nPart-of-speech (POS) tagging . The ﬁrst Indonesian POS tagging work was done over a 15K-token\ndataset. Pisceldo et al. (2009) deﬁnes 37 tags covering ﬁve main POS tags: kata kerja (verb), kata sifat\n(adjective), kata keterangan (adverb), kata benda (noun), and kata tugas (function words). They utilized\nnews domain and partial data from the PanLocalisation project (“PANL10N” 14). In total, “PANL10N”\ncomprises 900K tokens, and was generated by machine-translating an English POS-tagged dataset and\nnoisily projecting the POS tags from English to the Indonesian translations.\n9The existing implementation merges all documents into one text stream\n10https://kompas.com\n11https://koran.tempo.co\n12https://liputan6.com\n13We checkpointed the model at 1M and 2M steps, and found that 2M steps yielded a lower perplexity over the dev set.\n14http://www.panl10n.net/\n760\nTo create a larger and more reliable corpus, Dinakaramani et al. (2014) published a manually-annotated\ncorpus of 260K tokens (10K sentences). The text was sourced from the IDENTIC parallel corpus\n(Larasati, 2012), which was translated from data in the Penn Treebank corpus. The text is manually\nannotated with 23 tags based on Indonesian tag deﬁnition of Adriani et al. (2009). For I NDO LEM,\nwe use the Indonesian POS tagging dataset of Dinakaramani et al. (2014), and 5-fold partitioning of\nKurniawan and Aji (2018).15\nNamed entity recognition (NER). Budi et al. (2005) was the ﬁrst study on named entity recognition\nfor Indonesian, where roughly 2,000 sentences from a news portal were annotated with three NE classes:\nperson, location, and organization. In other work, Luthﬁ et al. (2014) utilized Wikipedia\nand DBPedia to automatically generate an NER corpus, and trained a model with Stanford CRF-NER\n(Finkel et al., 2005). Rachman et al. (2017) studied LSTM performance over 480 tweets with the same\nthree named entity classes. None of these authors released the datasets used in the research.\nThere are two publicly-available Indonesian NER datasets. The ﬁrst, NER UI, comprises 2,125 sen-\ntences obtained via an annotation assignment in an NLP course at the University of Indonesia in 2016\n(Gultom and Wibowo, 2017). The corpus has the same three named entity classes as its predecessors\n(Budi et al., 2005). The second, NER UGM, comprises 2,343 sentences from news articles, and was con-\nstructed at the University of Gajah Mada (Fachri, 2014) based on ﬁve named entity classes: person,\norganization, location, time, and quantity.\nDependency parsing. Kamayani and Purwarianti (2011) and Green et al. (2012) pioneered depen-\ndency parsing for the Indonesian language. Kamayani and Purwarianti (2011) developed language-\nspeciﬁc dependency labels based on 20 sentences, adapted from Stanford Dependencies (de Marneffe\nand Manning, 2016). Green et al. (2012) annotated 100 sentences of IDENTIC without dependency\nlabels, and used an ensemble SVM model to build a parser. Later, Rahman et al. (2017) conducted a\ncomparative evaluation over models trained using off-the-shelf tools such as MaltParser (Nivre et al.,\n2005) on 2,098 annotated sentences from the news domain. However, this corpus is not publicly avail-\nable.\nThe Universal Dependencies (UD) project16 has released two different Indonesian corpora of relatively\nsmall size: (1) 5,593 sentences of UD-Indo-GSD (McDonald et al., 2013); 17 and (2) 1,000 sentences of\nUD-Indo-PUD (Zeman et al., 2018). 18 Alﬁna et al. (2019) found that these corpora contain annotation\nerrors and did not deal adequately with Indonesian morphology. They released a corrected version of\nUD-Indo-PUD by ﬁxing annotations for reduplicated-words, clitics, compound words, and noun phrases.\nWe include two UD-based dependency parsing datasets in INDO LEM: (1) UD-Indo-GSD, and (2) the\ncorrected version of UD-Indo-PUD. As our reference dependency parser model, we use the BiAfﬁne\ndependency parser (Dozat and Manning, 2017), which has been shown to achieve strong performance\nfor English.\n4.2 Semantic Tasks\nSentiment analysis. There has been sentiment analysis for Indonesian domains/data sources including\npresidential elections (Ibrahim et al., 2015), stock prices (Cakra and Trisedya, 2015), Twitter (Koto and\nRahmaningtyas, 2017), and movie reviews (Nurdiansyah et al., 2018). Most previous work, however,\nhas used non-public and low-resource datasets.\nWe include in INDO LEM an Indonesian sentiment analysis dataset based on binary classiﬁcation. In\ntotal, the data distribution is 3638/399/1011 sentences for train/development/test, respectively. The data\nwas sourced from Twitter (Koto and Rahmaningtyas, 2017) and hotel reviews. 19 The hotel review data\nis annotated at the aspect level, where one review can have multiple polarities for different aspects. We\n15We do not include POS data from the Universal Dependency project, as we found the data to contain many foreign bor-\nrowings (without any attempt to translate them into Indonesian), and some sentences to be poor translations (a point we return\nto in the context of error analysis of dependency parsing in Section 7).\n16https://universaldependencies.org/\n17https://github.com/UniversalDependencies/UD_Indonesian-GSD\n18https://github.com/UniversalDependencies/UD_Indonesian-PUD\n19https://github.com/annisanurulazhar/absa-playground/\n761\n Premise\n Possible next tweets:\nIni kak Gracia sama kak Jerome saudaranya apa crushnya, serius\ntanya bang \nCrush, karena awal mereka kenal karena sama2 dapet beasiswa\nmitsui\nselamat pagi min. Jika ingin bertanya terkait Latsar Cpns Kemdikbud\nlewat kontak mana ya?? \nWaw terimakasii, maaf aku followers baru. \ntahun ajaran barunya januari terus nnt aku masuk masuk ke sekolah di\ntanyain malah jadi tukang keong\nlgi ap cantik?\n Premise\n Possible next tweets:\nSeriously ask, is Gracia Jerome's crush? or are they family?\nHis crush, they know each other when they got Mitsui scholarship\ngood morning admin. What is the contact for Latsar Cpns Kemdikbud??\nWow, thank you, sorry, I am a new follower.\nthe new school academic year is January, on the ﬁrst day I may get\nquestion about \"tukang keong\".\nwhat are you doing beautiful?\nFigure 1: Example for the next tweet prediction task. To the left is the original Indonesian version and\nto the right is an English translation. The tweet indicated in bold is the correct next tweet.\nsimply count the proportion of positive and negative polarity aspects, and label the sentence based on the\nmajority sentiment. We discard a review if there is a tie in positive and negative aspects.\nSummarization. From attention mechanisms (Rush et al., 2015; See et al., 2017) to pre-trained\nlanguage models (Liu and Lapata, 2019; Zhang et al., 2019), recent summarization work on English\nin terms of both extractive and abstractive methods has relied on ever-larger datasets and data-hungry\nmethods.\nIndonesian (single document) text summarization research has inevitably focused predominantly on\nextractive methods, based on small datasets. Aristoteles et al. (2012) deployed a genetic algorithm over\na 200-document summarization dataset, and Gunawan et al. (2017) performed unsupervised summa-\nrization over 3,075 news articles. As an attempt to create a standardized corpus, Koto (2016) released\na 300-document chat summarization dataset, and Kurniawan and Louvan (2018) released the IndoSum\n19K document–summary dataset. At the time we carried out this work, 20 IndoSum was the largest In-\ndonesian summarization corpus in the news domain, manually constructed from CNN Indonesia 21 and\nKumparan22 documents. IndoSum is a single-document summarization dataset where each article has\none abstractive summary. Kurniawan and Louvan (2018) released IndoSum together with the O RACLE\n— a set of extractive summaries generated automatically by maximizing ROUGE score between sen-\ntences of the article and its abstractive summary. We include IndoSum as the summarization dataset in\nINDO LEM, and evaluate the performance of extractive summarization in this paper.\n4.3 Discourse Coherence Tasks\nWe also introduce two tasks that measure the ability of models to measure discourse coherence in Indone-\nsian, based on message ordering in Twitter threads, namely: (1) next tweet prediction; and (2) message\nordering. Utilizing tweets instead of edited text arguably makes the task harder and allows us to assess\nthe robustness of models.\nFirst, we use the standard twitter API ﬁltered with the language parameter to harvest 9M Indonesian\ntweets from the period April–May 2020, covering the following topics: health, education, economy, and\ngovernment. We discard threads that contain more than three self-replies, and threads containing similar\ntweets (usually from Twitter bots). Speciﬁcally, we discard a thread if 90% of the tweets are similar,\nas based on simple lexical overlap. 23 We gather threads that contain 3–5 tweets, and anonymize all\nmentions. This data is used as the basis for the two discourse coherence tasks.\nNext tweet prediction. To evaluate model coherence, we design a next tweet prediction (NTP) task\nthat is similar to the next sentence prediction (NSP) task used to train BERT (Devlin et al., 2019). In\nNTP, each instance consists of a Twitter thread (2–4 tweets) that we call the premise, and four possible\noptions for the next tweet (see Figure 1 for an example), one of which is the actual response from the\n20Noting that the soon-to-be-released Liputan6 dataset (Koto et al., to appear) will be substantially larger, but was not\navailable when this research was carried out.\n21https://www.cnnindonesia.com/\n22https://kumparan.com/\n23Two tweets are considered to be similar if they have a vocabulary overlap≥80%.\n762\noriginal thread. In total, we construct 8,382 instances, where the distractors are obtained by randomly\npicking three tweets from the Twitter crawl. We ensure that there is no overlap between the next tweet\ncandidates in the training and test sets.\nTweet ordering. For the second task, we propose a related but more complex task of thread message\nordering, based on the sentence ordering task of Barzilay and Lapata (2008) to assess text relatedness.\nWe construct the data by shufﬂing Twitter threads (containing 3–5 tweets), and assessing the predicted\nordering in terms of rank correlation with the original. After removing all duplicates messages, we obtain\n7,608 instances for this task.\n5 Evaluation Methodology\nWe provide details of the evaluation methodology in this section.\nMorpho-syntax/Sequence Labelling. For POS tagging, we evaluate by 5-fold cross validation using\nthe partitions provided by Kurniawan and Aji (2018). Unlike Kurniawan and Aji (2018) who use macro-\naveraged F1, we use the standard POS tag accuracy for evaluation. For NER, both corpora (NER UI\nand NER UGM) are from the news domain. We convert them into IOB2 format, and reserve 10% of\nthe original training set as a validation set. We evaluate using entity-level F1 over the provided test\nset.24 In addition, we conducted our own in-house evaluation of the annotation quality of both datasets\nby randomly picking 100 sentences and counting the number of annotation errors. We found that NER\nUI has better quality than NER UGM with 1% vs. 30% errors, respectively. Annotation errors in NER\nUGM are largely due to low recall, i.e. annotating named entities with the tag O.\nFor dependency parsing we do not apply 5-fold cross-validation for UD-Indo-GSD, as it was released\nwith a pre-deﬁned test set, which allows us to directly benchmark against previous work. UD-Indo-\nPUD, on the other hand, only includes 1,000 sentences with no ﬁxed test set, so we evaluate via 5-fold\ncross-validation with ﬁxed splits. 25 Note that the text in UD-Indo-PUD was manually translated from\ndocuments in other languages, while UD-Indo-GSD was sourced from texts authored in Indonesian.\nAdditionally, the translation quality of UD-Indo-PUD is low in parts, which impacts on evaluation, as\nwe return to discuss in Section 7. We evaluate both dependency parsing datasets based on the unlabelled\nattachment score (UAS) and labelled attachment score (LAS).\nSemantics. Because the sentiment analysis data is low-resource and imbalanced, we use stratiﬁed\n5-fold cross-validation, and evaluate based on F1 score. For summarization, on the other hand, we use\nthe canonical splits provided by Kurniawan and Louvan (2018), and evaluate the resulting summary with\nROUGE (F1) (Lin, 2004) in the form of three different metrics: R1, R2, and RL.\nDiscourse Coherence. We do not perform 5-fold cross-validation over NTP for two reasons. First, we\nneed to ensure the distractors in the test set do not overlap with the training or development sets, to avoid\npossible bias because of dataset artefacts. Second, the size of the dataset in terms of pair-wise labelling is\nactually four times the reported size (Table 1) as there are three distractors for each thread. We evaluate\nthe NTP task based on accuracy, meaning the random baseline is 25%.\nFor tweet ordering, we evaluate using Spearman’s rank correlation ( ρ). Speciﬁcally, we average the\nrank correlation between the gold and predicted order of each thread in the dataset.\n6 Comparative Evaluation\n6.1 Baselines\nMost of our experiments use a BiLSTM with 300dfastText pre-trained Indonesian embeddings (Bo-\njanowski et al., 2016) as a baseline. Details of the baselines are provided in Table 2.\nFor extractive summarization baselines, we use the models of Kurniawan and Louvan (2018) and\nCheng and Lapata (2016) as baselines. Kurniawan and Louvan (2018) propose a sentence tagging ap-\nproach based on a hidden Markov model, while Cheng and Lapata (2016) use a hierarchical LSTM\n24We used the seqeval library to evaluate the POS and NER tasks.\n25The split for cross validation is 70/10/20 for train/development/test, respectively. We ﬁrst create 5 folds with non-\noverlapping test partitions, and for each fold we set the ﬁrst portion of the remaining data as the development (and the rest\nas training data).\n763\nTask Baselines BERT models\nPOS Tagging and NER\nLample et al. (2016)26\nA hierarchical BiLSTM + CRF with input:\ncharacter-level embedding (updated),\nand word-levelfastTextembedding (ﬁxed),\nlr: 0.001, epoch:100 with early stopping\n(patience = 5)\nFine-tuning:\nadding a classiﬁcation layer for each token,\nlr: 5e-5, epoch:100 with early stopping\n(patience = 5)\nDependency parsing\n1. Dozat and Manning (2017), Bi-Afﬁne\nparser, Embedding:fastText(ﬁxed)\n2. Rahman and Purwarianti (2020)†\n3. Kondratyuk and Straka (2019)†\n4. Alﬁna et al. (2019)†\nDozat and Manning (2017), Bi-Afﬁne\nparser, Embedding: BERT output (ﬁxed)\nSentiment Analysis\n1. 200-d BiLSTM\nEmbedding:fastText(ﬁxed),\nlr: 0.001, epoch:100 with early stopping\n(patience = 5)\n2. Naive Bayes and Logistic Regression\ninput: Byte-pair encoding (unigram+bigram)27\nFine-tuning:\nInput: 200 tokens; epoch: 20; lr: 5e-5;\nbatch size: 30; warm-up: 10% of the total steps;\nearly stopping (patience = 5);\nOutput layer uses the encoded [CLS]\nSummarization 1. Kurniawan and Louvan (2018)†\n2. Cheng and Lapata (2016)†\nLiu and Lapata (2019), extractive model,\n20,000 steps, lr: 2e-3, and tokens: 512.28\nNTP\n200-d BiLSTM (binary-class.)\nEmbedding: fastText (ﬁxed), lr: 0.001,\nepoch:100 with early stopping (patience = 20)\nFine-tuning:\nInput: 60 tokens (for 1 single tweet);\nepoch: 20; learning rate; 5e-5; batch size: 20;\nwarm-up: 10% of the total steps; early stopping\n(patience = 5); Output layer uses the\nencoded [CLS]\nTweet Ordering\nHierarchical 200-d BiLSTMs (multi-class.)\nEmbedding: fastText (ﬁxed), lr: 0.001,\nepoch:100 with early stopping (patience = 20)\nFine-tuning:\nInput: 50 tokens (for 1 single tweet);\nepoch: 20; learning rate; 5e-5; batch size: 20;\nwarm-up: 10% of the total steps; early stopping\n(patience = 5); BERT ﬁne-tuning is based on the\nLiu and Lapata (2019) trick (alternated seq.)\nTable 2: Comparison of baselines and BERT-based models for all I NDO LEM tasks. All listed models\nwere implemented and run by the authors, except for those marked with “†” where the results are sourced\nfrom the original paper.\nencoder with attention. In addition, we present O RACLE results, obtained by greedily maximizing the\nROUGE score between the reference summary and different combinations of sentences from the docu-\nment. O RACLE denotes the upper bound for the extractive summarization.\nFor next tweet prediction, we concatenate all premise tweets into a single document, and use a BiL-\nSTM and fastTextword embeddings to obtain the baseline document encoding. We structure this task\nas a binary classiﬁcation where we match the premise with each candidate next tweet. We pick the tweet\nwith the highest probability as the prediction. We use the same BiLSTM to encode the next tweet, and\nfeed the concatenated representations from the last hidden states into the output layer.\nFor tweet ordering, we use a hierarchical BiLSTM model. The ﬁrst BiLSTM is used to encode a\nsingle tweet by averaging all hidden states. We use the second BiLSTM to learn the inter-tweet ordering.\nWe design the tweet ordering task as a sequence labelling task, where we aim to obtain P(r|t), the\nprobability distribution across rank positions r for a given tweet t. Note that in this experiment, each\ninstance is comprised of 3–5 tweets, and we model the task via multi-classiﬁcation (with 5 classes/ranks).\nWe perform inference based on P(r|t), where we decide the ﬁnal rank based on the highest sum of\nprobabilities from the exhaustive enumeration of document ranks.\n764\n6.2 BERT Benchmarks\nTo benchmark I NDO BERT, we compare against two pre-existing BERT models: multilingual BERT\n(“MBERT”), and a monolingual BERT for Malay (“M ALAY BERT”).29 MBERT is trained by concate-\nnating Wikipedia documents for 104 languages including Indonesian, and has been shown to be effective\nfor zero-shot multilingual tasks (Wu and Dredze, 2019; Wang et al., 2019c). M ALAY BERT is a a pub-\nlicly available model that was trained on Malay documents from Wikipedia, local news sources, social\nmedia, and some translations from English. We expect M ALAY BERT to provide better representations\nthan MBERT for the Indonesian language, because Malay and Indonesian are mutually intelligible, with\nmany lexical similarities, but noticeable differences in grammar, pronunciation and vocabulary.\nFor the sequence labelling tasks (POS tagging and NER), sentiment analysis, NTP, and tweet ordering\ntask, the ﬁne-tuning procedure is detailed in Table 2.\nFor dependency parsing, we follow Nguyen and Nguyen (2020) in incorporating BERT into the Bi-\nAfﬁne dependency parser (Dozat and Manning, 2017) by replacing the word embeddings with the cor-\nresponding contextualized representations. Speciﬁcally, we generate the BERT embedding of the ﬁrst\nWordPiece token as the word embedding, and train the BiAfﬁne parser in its default conﬁguration. In\naddition, we also benchmark against a pre-existing ﬁne-tuned version of MBERT trained over 75 con-\ncatenated UD datasets in different languages (Kondratyuk and Straka, 2019).\nFor summarization, we follow Liu and Lapata (2019) in encoding the document by inserting the tokens\n[CLS] and [SEP] between sentences. We also apply alternating segment embeddings based on whether\nthe position of a sentence is odd or even. On top of the pre-trained model, we use a second transformer\nencoder to learn inter-sentential relationships. The input is the encoded [CLS] representation, and the\noutput is the extractive label y∈ {0,1} (1 = include in summary; 0 = don’t include).\n7 Results\nTable 3 shows the results for POS tagging and NER.MBERT, M ALAY BERT, and I NDO BERT perform\nvery similarly over the POS tagging task, well above the BiLSTM baseline. This indicates that all three\ncontextual embedding models are able to generalize well over low-level morpho-syntactic tasks. Given\nthat Indonesian and Malay share a large number of words, it is not surprising that M ALAY BERT per-\nforms on par with INDO BERT for POS tagging. On the NER tasks, both MALAY BERT and INDO BERT\noutperform MBERT, which performs similarly to or slightly above the BiLSTM. This is despiteMBERT\nhaving been trained on a much larger corpus, and having seen many more entities during training. I N-\nDOBERT slightly outperforms M ALAY BERT.\nIn Table 4, we show that augmenting the BiAfﬁne parser with the pre-trained models yields a strong\nresult for dependency parsing, well above previously-published results over the respective datasets. Over\nUD-Indo-GSD, I NDO BERT outperforms all methods on both metrics. The universal ﬁne-tuning ap-\nproach (Kondratyuk and Straka, 2019) yields similar performance as BiAfﬁne +fastText, while aug-\nmenting BiAfﬁne with MBERT and MALAY BERT yields lower UAS and LAS scores than INDO BERT.\nOver UD-Indo-PUD, we see that augmenting BiAfﬁne with MBERT outperforms all methods including\nINDO BERT. Note that Kondratyuk and Straka (2019) is trained on the original version of UD-Indo-\nPUD, and Alﬁna et al. (2019) is based on 10-fold cross-validation, meaning the results are not 100%\ncomparable.\nTo better understand why MBERT performs so well over UD-Indo-PUD, we randomly selected 100\ninstances for manual analysis. We found that 44 out of the 100 sentences contained direct borrowings\nof foreign words (29 names, 10 locations, and 15 organisations), some of which we would expect to be\nlocalized into Indonesian, such as: St. Rastislav, Star Reach, Royal National Park Australia, and Zettel’s\nTraum. We also thoroughly examined the translation quality and found that roughly 20% of the sentences\n26The baseline code is available as chars-lstm-lstm-crf at https://github.com/guillaumegenthial/\ntf_ner.\n27We also experimented with simple term frequency, but observed lower performance so omit the results from the paper.\n28We checkpoint every 2,500 steps, and perform inference over the test set based on the top-3 best checkpoints according to\nthe development set\n29https://huggingface.co/huseinzol05/bert-base-bahasa-cased\n765\nMethod POS tagging NER UGM NER UI\nAcc F1 F1\nBiLSTM-CRF (Lample et al., 2016) 95.4 70.9 82.2\nMBERT 96.8 71.6 82.2\nMALAYBERT 96.8 73.2 87.4\nINDO BERT 96.8 74.9 90.1\nTable 3: Results on POS and NER tasks using accuracy averaged over ﬁve folds for POS tagging task,\nand entity-level F1 over the test set for the NER tasks.\nMethod UD-Indo-GSD\nUAS LAS\nRahman and Purwarianti (2020)*82.56 76.04\nKondratyuk and Straka (2019) 86.45 80.10\nBiAfﬁne w/fastText 85.25 80.35\nBiAfﬁne w/MBERT 86.85 81.78\nBiAfﬁne w/ MALAYBERT 86.99 81.87\nBiAfﬁne w/ INDOBERT 87.12 82.32\nMethod UD-Indo-PUD\nUAS LAS\nAlﬁna et al. (2019)* 83.33 79.39\nKondratyuk and Straka (2019)*77.47 56.90\nBiAfﬁne w/fastText 84.04 79.01\nBiAfﬁne w/MBERT 90.58 85.44\nBiAfﬁne w/ MALAYBERT 88.91 83.56\nBiAfﬁne w/ INDOBERT 89.23 83.95\nTable 4: Results for dependency parsing. Methods marked with ‘*’ (from previous work) do not use the\nsame test partition.\nMethod Sentiment\nAnalysis (F1)\nNaive Bayes 70.95\nLogistic Regression 72.14\nBiLSTM w/fastText 71.62\nMBERT 76.58\nMALAYBERT 82.02\nINDOBERT 84.13\nMethod Summarization (F1)\nR1 R2 RL\nORACLE 79.27 72.52 78.82\nKurniawan and Louvan (2018) 17.62 4.70 15.89\nCheng and Lapata (2016) 67.96 61.65 67.24\nMBERT 68.40 61.66 67.67\nMALAYBERT 68.44 61.38 67.71\nINDOBERT 69.93 62.86 69.21\nTable 5: Results over the semantic tasks.\nare low-quality translations. For instance,Ketidaksesuaian data ekonomi dan retorika politik tidak asing,\natau seharusnya tidak asing is not a natural sentence in Indonesian.\nFor the semantic tasks, I NDO BERT outperforms all other methods for both sentiment analysis and\nextractive summarization (Table 5). For sentiment analysis, the improvement over the baselines is im-\npressive: +13.2 points over naive Bayes, and +7.5 points over MBERT. As expected, M ALAY BERT\nalso performs well for sentiment analysis, but substantially lower than INDO BERT. For summarization,\nMBERT and MALAY BERT achieve similar performance, and only outperform Cheng and Lapata (2016)\nby around 0.5 ROUGE points. INDO BERT, on the other hand, is 1–2 ROUGE points better.\nLastly, in Table 6, we observe that INDO BERT is once again substantially better than the other models\nat discourse coherence modelling, despite its training not including next sentence prediction (as per the\nEnglish BERT). To assess the difﬁculty of the NTP task, we randomly selected 100 test instances, and the\nﬁrst author (a native speaker of Indonesian) manually predicted the next tweet. The human performance\nwas 90%, lower than the pre-trained language models. For the tweet ordering task, we also assess human\nperformance by randomly selecting 100 test instances, and found the rank correlation score of ρ= 0.61\nto be slightly higher than I NDO BERT. The gap between I NDO BERT and the other BERT models was\nbigger on this task.\nOverall, with the possible exception of POS tagging and NTP, there is substantial room for improve-\n766\nMethod Next Tweet Tweet Ordering\nPrediction (Acc) ( ρ)\nRandom 25.0 0.00\nHuman (100 samples) 90.0 0.61\nBiLSTM w/ fastText 73.6 0.45\nMBERT 92.4 0.53\nMALAYBERT 93.1 0.51\nINDO BERT 93.7 0.59\nTable 6: Results for discourse coherence. “Human” is the oracle performance by a human annotator.\nment across all tasks, and our hope is that INDO LEM can serve as a benchmark dataset to track progress\nin Indonesian NLP.\n8 Conclusion\nIn this paper, we introduced I NDO LEM, a comprehensive dataset encompassing seven tasks, spanning\nmorpho-syntax, semantics, and discourse coherence. We also detailed I NDO BERT, a new BERT-style\nmonolingual pre-trained language model for Indonesian. We used INDO LEM to benchmark INDO BERT\n(including comparative evaluation against a broad range of baselines and competitor BERT models), and\nshowed it to achieve state-of-the-art performance over the dataset.\nAcknowledgements\nWe are grateful to the anonymous reviewers for their helpful feedback and suggestions. The ﬁrst author\nis supported by the Australia Awards Scholarship (AAS), funded by the Department of Foreign Af-\nfairs and Trade (DFAT), Australia. This research was undertaken using the LIEF HPC-GPGPU Facility\nhosted at The University of Melbourne. This facility was established with the assistance of LIEF Grant\nLE170100200.\nReferences\nMirna Adriani, Ruli Manurung, and Femphy Pisceldo. 2009. Statistical based part of speech tagger for Bahasa\nIndonesia. In Proceedings of the 3rd International MALINDO Workshop.\nEneko Agirre, Carmen Banea, Daniel M. Cer, Mona T. Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German\nRigau, and Janyce Wiebe. 2016. Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-\nlingual evaluation. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016),\npages 497–511.\nIka Alﬁna, Arawinda Dinakaramani, Mohamad Ivan Fanany, and Heru Suhartanto. 2019. Gold standard depen-\ndency treebank for Indonesia. In Proceeding of the 33rd Paciﬁc Asia Conference on Language, Information and\nComputation (PACLIC 33), Hakodate, Japan.\nAristoteles Aristoteles, Yeni Herdiyeni, Ahmad Ridha, and Julio Adisantoso. 2012. Text feature weighting for\nsummarization of document Bahasa Indonesia using genetic algorithm. IJCSI International Journal of Com-\nputer Science Issues, 9(1):1–6.\nSariwening Azhari and Sarah Lintang. 2020. Indobert: Transformer-based model for Indonesian language under-\nstanding. Undergraduate thesis, Universitas Gadjah Mada.\nRegina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entity-based approach. Computational\nLinguistics, 34(1):1–34.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016. Enriching word vectors with\nsubword information. arXiv preprint arXiv:1607.04606.\n767\nIndra Budi, Stphane Bressan, Gatot Wahyudi, Zainal A. Hasibuan, and Bobby A. A. Nazief. 2005. Named entity\nrecognition for the Indonesian language: combining contextual, morphological and part-of-speech features into\na knowledge engineering approach. Discovery Science, pages 57–69.\nYahya Eru Cakra and Bayu Distiawan Trisedya. 2015. Stock price prediction using linear regression based on\nsentiment analysis. In 2015 International Conference on Advanced Computer Science and Information Systems\n(ICACSIS), pages 147–154.\nJianpeng Cheng and Mirella Lapata. 2016. Neural summarization by extracting sentences and words. In Proceed-\nings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,\nvolume 1, pages 484–494.\nAlexis Conneau and Guillaume Lample. 2019. Cross-lingual language model pretraining. In NeurIPS 2019:\nThirty-third Conference on Neural Information Processing Systems, pages 7057–7067.\nMarie-Catherine de Marneffe and Christopher D. Manning. 2016. Stanford typed dependencies manual. Technical\nreport, Stanford University.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidi-\nrectional transformers for language understanding. In NAACL-HLT 2019: Annual Conference of the North\nAmerican Chapter of the Association for Computational Linguistics, pages 4171–4186.\nArawinda Dinakaramani, Fam Rashel, Andry Luthﬁ, and Ruli Manurung. 2014. Designing an Indonesian part of\nspeech tagset and manually tagged Indonesian corpus. In 2014 International Conference on Asian Language\nProcessing (IALP), pages 66–69.\nTimothy Dozat and Christopher D. Manning. 2017. Deep biafﬁne attention for neural dependency parsing. In\nProceedings of the 2016 International Conference on Learning Representations, pages 1–8.\nMuhammad Fachri. 2014. Named entity recognition for Indonesian text using hidden Markov model. Undergrad-\nuate Thesis.\nJenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into in-\nformation extraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting of the Association\nfor Computational Linguistics (ACL’05), pages 363–370.\nNathan Green, Septina Dian Larasati, and Zdenek Zabokrtsky. 2012. Indonesian dependency treebank: Annota-\ntion and parsing. In Proceedings of the 26th Paciﬁc Asia Conference on Language, Information, and Computa-\ntion, pages 137–145.\nYohanes Gultom and Wahyu Catur Wibowo. 2017. Automatic open domain information extraction from Indone-\nsian text. In 2017 International Workshop on Big Data and Information Security (IWBIS), pages 23–30.\nD Gunawan, A Pasaribu, R F Rahmat, and R Budiarto. 2017. Automatic text summarization for Indonesian\nlanguage using TextTeaser. IOP Conference Series: Materials Science and Engineering, 190(1):12048.\nKarl Moritz Hermann, Tom Koisk, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil\nBlunsom. 2015. Teaching machines to read and comprehend. In NIPS’15: Proceedings of the 28th Interna-\ntional Conference on Neural Information Processing Systems - Volume 1, pages 1693–1701.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020. Xtreme:\nA massively multilingual multi-task benchmark for evaluating cross-lingual generalization. arXiv preprint\narXiv:2003.11080.\nMochamad Ibrahim, Omar Abdillah, Alfan F. Wicaksono, and Mirna Adriani. 2015. Buzzer detection and senti-\nment analysis for predicting presidential election results in a twitter nation. In 2015 IEEE International Confer-\nence on Data Mining Workshop (ICDMW), pages 1348–1353.\nGanesh Jawahar, Benoˆıt Sagot, and Djam´e Seddah. 2019. What does BERT learn about the structure of language.\nIn ACL 2019 : The 57th Annual Meeting of the Association for Computational Linguistics, pages 3651–3657.\nMia Kamayani and Ayu Purwarianti. 2011. Dependency parsing for Indonesian. In Proceedings of the 2011\nInternational Conference on Electrical Engineering and Informatics, pages 1–5.\nNikita Kitaev, Steven Cao, and Dan Klein. 2019. Multilingual constituency parsing with self-attention and pre-\ntraining. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages\n3499–3505, Florence, Italy, July.\n768\nDan Kondratyuk and Milan Straka. 2019. 75 languages, 1 model: Parsing universal dependencies universally. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, pages 2779–2795.\nFajri Koto and Gemala Y . Rahmaningtyas. 2017. Inset lexicon: Evaluation of a word list for indonesian sentiment\nanalysis in microblogs. In 2017 International Conference on Asian Language Processing (IALP) , pages 391–\n394.\nFajri Koto, Jey Han Lau, and Timothy Baldwin. to appear. Liputan6: A large-scale Indonesian dataset for text\nsummarization. In Proceedings of the 1st Conference of the Asia-Paciﬁc Chapter of the Association for Com-\nputational Linguistics and the 10th International Joint Conference on Natural Language Processing (AACL-\nIJCNLP 2020).\nFajri Koto. 2016. A publicly available Indonesian corpora for automatic abstractive and extractive chat summa-\nrization. In Proceedings of LREC 2016.\nKemal Kurniawan and Alham Fikri Aji. 2018. Toward a standardized and more accurate Indonesian part-of-speech\ntagging. In 2018 International Conference on Asian Language Processing (IALP), pages 303–307.\nKemal Kurniawan and Samuel Louvan. 2018. Indosum: A new benchmark dataset for Indonesian text summa-\nrization. In 2018 International Conference on Asian Language Processing (IALP), pages 215–220.\nGuillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural\narchitectures for named entity recognition. In Proceedings of the 2016 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies , pages 260–270,\nSan Diego, California, June.\nSeptina Dian Larasati. 2012. IDENTIC corpus: Morphologically enriched Indonesian-English parallel corpus. In\nLREC, pages 902–906.\nPatrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. 2020. MLQA: Evaluating cross-\nlingual extractive question answering. In ACL 2020: 58th Annual Meeting of the Association for Computational\nLinguistics, pages 7315–7330.\nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin\nJiang, Guihong Cao, Xiaodong Fan, Bruce Zhang, Rahul Agrawal, Edward Cui, Sining Wei, Taroon Bharti,\nYing Qiao, Jiun-Hung Chen, Winnie Wu, Shuguang Liu, Fan Yang, Rangan Majumder, and Ming Zhou. 2020.\nXGLUE: A new benchmark dataset for cross-lingual pre-training, understanding and generation.arXiv preprint\narXiv:2004.01401.\nChin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization\nBranches Out: Proceedings of the ACL-04 Workshop, pages 74–81.\nYang Liu and Mirella Lapata. 2019. Text summarization with pretrained encoders. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing, pages 3728–3738.\nMinh-Thang Luong and Christopher D. Manning. 2016. Achieving open vocabulary neural machine translation\nwith hybrid word-character models. In Proceedings of 54th Annual Meeting of the Association for Computa-\ntional Linguistics (ACL 2016), Berlin, Germany.\nAndry Luthﬁ, Bayu Distiawan, and Ruli Manurung. 2014. Building an Indonesian named entity recognizer using\nWikipedia and DBPedia. In 2014 International Conference on Asian Language Processing (IALP) , pages 19–\n22.\nRyan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev,\nKeith Hall, Slav Petrov, Hao Zhang, Oscar Tckstrm, Claudia Bedini, Nria Bertomeu Castell, and Jungmee Lee.\n2013. Universal dependency annotation for multilingual parsing. In Proceedings of the 51st Annual Meeting of\nthe Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pages 92–97.\nMarek Medved and V´ıt Suchomel. 2017. Indonesian web corpus (idWac). In LINDAT/CLARIN digital library at\nthe Institute of Formal and Applied Linguistics (FAL), Faculty of Mathematics and Physics, Charles University.\nDat Quoc Nguyen and Anh Tuan Nguyen. 2020. PhoBERT: Pre-trained language models for Vietnamese. arXiv\npreprint arXiv:2003.00744.\nAllen Nie, Erin Bennett, and Noah Goodman. 2019. DisSent: Learning sentence representations from explicit dis-\ncourse relations. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics ,\npages 4497–4510, Florence, Italy, July.\n769\nJoakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, Glsen Eryigit, Sandra Kbler, Svetoslav Marinov, and\nErwin Marsi. 2005. MaltParser: A language-independent system for data-driven dependency parsing. Natural\nLanguage Engineering, 13(2):95–135.\nYanuar Nurdiansyah, Saiful Bukhori, and Rahmad Hidayat. 2018. Sentiment analysis system for movie review in\nBahasa Indonesia using nave bayes classiﬁer method. Journal of Physics: Conference Series, 1008:12011.\nFemphy Pisceldo, Ruli Manurung, and Mirna Adriani. 2009. Probabilistic part of speech tagging for Bahasa\nIndonesia. In Third International MALINDO Workshop, pages 1–6.\nValdi Rachman, Septiviana Savitri, Fithriannisa Augustianti, and Rahmad Mahendra. 2017. Named entity recog-\nnition on Indonesian twitter posts using long short-term memory networks. In 2017 International Conference\non Advanced Computer Science and Information Systems (ICACSIS).\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Building a discourse-tagged corpus\nin the framework of rhetorical structure theory. In CoRR, abs/1704.01444, 2017.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models\nare unsupervised multitask learners. OpenAI Blog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,\nand Peter J. Liu. 2019. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv\npreprint arXiv:1910.10683.\nArief Rahman and Ayu Purwarianti. 2020. Dense word representation utilization in Indonesian dependency\nparsing. Jurnal Linguistik Komputasional, 3(1):12–19.\nArief Rahman, Kuncoro Adhiguna, and Ayu Purwarianti. 2017. Ensemble technique utilization for Indonesian\ndependency parser. PACLIC, pages 64–71.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don’t know: Unanswerable questions for\nSQuAD. arXiv preprint arXiv:1806.03822.\nAlexander M. Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence\nsummarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,\npages 379–389.\nAbigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), volume 1, pages 1073–1083.\nChi Sun, Luyao Huang, and Xipeng Qiu. 2019. Utilizing BERT for aspect-based sentiment analysis via construct-\ning auxiliary sentence. In NAACL-HLT (1), pages 380–385.\nF. Tala, J. Kamps, K.E. Mller, and M. de Rijke. 2003. The impact of stemming on information retrieval in Bahasa\nIndonesia. In Proceedings of The 14th Meeting of Computational Linguistics in the Netherlands.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on\nNeural Information Processing Systems, pages 5998–6008.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and\nSamuel R. Bowman. 2019a. SuperGLUE: A stickier benchmark for general-purpose language understanding\nsystems. In Advances in Neural Information Processing Systems, pages 3266–3280.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019b. GLUE:\nA multi-task benchmark and analysis platform for natural language understanding. In ICLR 2019: 7th Interna-\ntional Conference on Learning Representations.\nYuxuan Wang, Wanxiang Che, Jiang Guo, Yijia Liu, and Ting Liu. 2019c. Cross-lingual BERT transformation\nfor zero-shot dependency parsing. In Proceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing, pages 5720–5726.\nBryan Wilie, Karissa Vincentio, Genta Indra Winata, Samuel Cahyawijaya, Xiaohong Li, Zhi Yuan Lim, Sidik\nSoleman, Rahmad Mahendra, Pascale Fung, Syafri Bahar, and Ayu Purwarianti. 2020. IndoNLU: Benchmark\nand resources for evaluating Indonesian natural language understanding. arXiv preprint arXiv:2009.05387.\n770\nAdina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence un-\nderstanding through inference. In NAACL HLT 2018: 16th Annual Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, volume 1, pages 1112–1122.\nShijie Wu and Mark Dredze. 2019. Beto, Bentz, Becas: The surprising cross-lingual effectiveness of BERT. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, pages 833–844.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. XLNet:\nGeneralized autoregressive pretraining for language understanding. In NeurIPS 2019: Thirty-third Conference\non Neural Information Processing Systems, pages 5754–5764.\nDaniel Zeman, Jan Haji, Martin Popel, Martin Potthast, Milan Straka, Filip Ginter, Joakim Nivre, and Slav Petrov.\n2018. CoNLL 2018 shared task: Multilingual parsing from raw text to universal dependencies. In Conference\non Computational Natural Language Learning (CoNLL), pages 1–21.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J Liu. 2019. PEGASUS: Pre-training with extracted\ngap-sentences for abstractive summarization. arXiv preprint arXiv:1912.08777.",
  "topic": "Indonesian",
  "concepts": [
    {
      "name": "Indonesian",
      "score": 0.8960767984390259
    },
    {
      "name": "Computer science",
      "score": 0.8098195791244507
    },
    {
      "name": "Natural language processing",
      "score": 0.6948238015174866
    },
    {
      "name": "Syntax",
      "score": 0.6450194716453552
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6430565714836121
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6278491616249084
    },
    {
      "name": "Benchmarking",
      "score": 0.611234188079834
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.5124066472053528
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.45149701833724976
    },
    {
      "name": "Language model",
      "score": 0.44386714696884155
    },
    {
      "name": "Standardization",
      "score": 0.4409574866294861
    },
    {
      "name": "Spoken language",
      "score": 0.4205898344516754
    },
    {
      "name": "Linguistics",
      "score": 0.27545833587646484
    },
    {
      "name": "Programming language",
      "score": 0.0992061197757721
    },
    {
      "name": "Geography",
      "score": 0.05604809522628784
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}