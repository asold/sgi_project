{
  "title": "Retrieving Skills from Job Descriptions: A Language Model Based Extreme Multi-label Classification Framework",
  "url": "https://openalex.org/W3117329469",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3115907296",
      "name": "Akshay Bhola",
      "affiliations": [
        "Indian Institute of Technology Kanpur",
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2688116378",
      "name": "Kishaloy Halder",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2751350969",
      "name": "Animesh Prasad",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A4202175338",
      "name": "Min-Yen Kan",
      "affiliations": [
        "National University of Singapore"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2958975280",
    "https://openalex.org/W2962958286",
    "https://openalex.org/W2736621900",
    "https://openalex.org/W2962902328",
    "https://openalex.org/W2756268972",
    "https://openalex.org/W2739996966",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1834987204",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2183087644",
    "https://openalex.org/W2880875857",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2118463056",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1940872118",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2150385485",
    "https://openalex.org/W2904333455",
    "https://openalex.org/W2154109204",
    "https://openalex.org/W3107138820",
    "https://openalex.org/W2798722581",
    "https://openalex.org/W1924770834"
  ],
  "abstract": "We introduce a deep learning model to learn the set of enumerated job skills associated with a job description. In our analysis of a large-scale government job portal mycareersfuture.sg, we observe that as much as 65% of job descriptions miss describing a significant number of relevant skills. Our model addresses this task from the perspective of an extreme multi-label classification (XMLC) problem, where descriptions are the evidence for the binary relevance of thousands of individual skills. Building upon the current state-of-the-art language modeling approaches such as BERT, we show our XMLC method improves on an existing baseline solution by over 9% and 7% absolute improvements in terms of recall and normalized discounted cumulative gain. We further show that our approach effectively addresses the missing skills problem, and helps in recovering relevant skills that were missed out in the job postings by taking into account the structured semantic representation of skills and their co-occurrences through a Correlation Aware Bootstrapping process. We further show that our approach, to ensure the BERT-XMLC model accounts for structured semantic representation of skills and their co-occurrences through a Correlation Aware Bootstrapping process, effectively addresses the missing skills problem, and helps in recovering relevant skills that were missed out in the job postings. To facilitate future research and replication of our work, we have made the dataset and the implementation of our model publicly available.",
  "full_text": "Proceedings of the 28th International Conference on Computational Linguistics, pages 5832–5842\nBarcelona, Spain (Online), December 8-13, 2020\n5832\nRetrieving Skills from Job Descriptions: A Language Model Based\nExtreme Multi-label Classiﬁcation Framework\nAkshay Bhola1,3 Kishaloy Halder4∗ Animesh Prasad2,3 Min-Yen Kan2,3\n1 Indian Institute of Technology Kanpur\n2 School of Computing, National University of Singapore\n3 Institute for Applied Learning Sciences and Educational Technology,\nNational University of Singapore\n4 Zalando SE\nakbhola.bhola@gmail.com kishaloy.halder@zalando.de\nanimesh.prasad@u.nus.edu kanmy@comp.nus.edu.sg\nAbstract\nWe introduce a deep learning model to learn the set of enumerated job skills associated with a job\ndescription. In our analysis of a large-scale government job portalmycareersfuture.sg, we\nobserve that as much as 65% of job descriptions miss describing a signiﬁcant number of relevant\nskills. Our model addresses this task from the perspective of an extreme multi-label classiﬁcation\n(XMLC) problem, where descriptions are the evidence for the binary relevance of thousands of\nindividual skills. Building upon the current state-of-the-art language modeling approaches such\nas BERT, we show our XMLC method improves on an existing baseline solution by over9% and\n7% absolute improvements in terms of recall and normalized discounted cumulative gain.\nWe further show that our approach effectively addresses the missing skills problem, and helps\nin recovering relevant skills that were missed out in the job postings by taking into account the\nstructured semantic representation of skills and their co-occurrences through a Correlation Aware\nBootstrapping process. To facilitate future research and replication of our work, we have made\nthe dataset and the implementation of our model publicly available.\n1 Introduction\nFinding prospective employees with the correct set of skills required for a job is an important aspect\nof the recruitment process (Bren ˇciˇc and Pahor, 2019; Lochner et al., 2020). In the current digital age,\nmany recruiters seek to ﬁnd suitable candidates through multiple channels — e.g., online job portals,\nprofessional networks — as well as traditional avenues, such as word of mouth and mass media (Shenoy\nand Aithal, 2018). In this work, we focus on online job portals ( e.g., LinkedIn, Glassdoor), which have\nemerged as critical players in the job market with millions of active users.\nDespite the reach of job portals, ﬁnding candidates with the right skill ﬁt still remains challenging.\nThese portals often receive thousands of applications, among which fewer than10% have the appropriate\nskills (cielotalent.com, 2014). This bottleneck has attracted both academic and industry researchers\nfrom social science and machine learning communities to build automated means to better organize the\ninformation present in job descriptions (Boselli et al., 2018). An approach facilitating the matching\nof jobs to candidates has been to associate a set of enumerated skills from the job descriptions (JDs).\nCandidate job-seekers can also list such skills as part of their online proﬁle explicitly, or implicitly via\nautomated extraction from r´esum´es and curriculum vitae(CVs). Such categorical skills can then be used\nto induce a match score between JDs and candidate proﬁles.\nWe examine the challenge of inferring appropriate job skills from JDs ( cf., Fig. 1). JDs consist of\na textual description of the job requirements and a list of required skills. The latter helps in indexing\nJDs to facilitate ease of searchthrough facet navigation. We note that there exist crucial skills that are\nmentioned in job requirements (in the job description), but are not listed as required skills (skill labels).\n∗work done while author was at NUS\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://\ncreativecommons.org/licenses/by/4.0/.\n5833\nJob Requirements: Looking for a dynamic individual keen to join a growing organization in the fast-paced and expanding\nSupply Chain Software Industry. As Singapore goes through the digital revolution, the individual will play a paramount role\nin developing and upgrading our SaaS tools and valued added services for our clients.\nResponsibilities:\n1. Work with Architect, Framework Designer to develop the next generation of Cloud base Micro Services Suite\n2. Design and implement Micro Service modules primary for Supply Chain systems. Design must be ﬂexible and scalable\nfor any future expansion or upgrade.\n3. Establish API services in the API gateway for both internal and external communications and integrations.\n4. Document all design works in proper standard formats with detail descriptions. Ensure all design documentations,\ninclude module, data are always follow standards and up to date.\n5. Conduct Agile development methodology in the Micro Services development life cycle.\n6. Carry out unit and integration testing regularly with the QC team.\n7. Intensive knowledge on several Java platform technologies, such as JavaEE, DOM/SAX, Annotation, AOP, DI, REST,\nworkﬂow, etc. Familiar with infra layer technology such as Docker\n.....\nRequired Skills: .NET, Agile Methodologies, AJAX, ASP.NET, C#, C++, HTML, Java, JavaScript, jQuery, Linux, Microsoft\nSQL Server, MySQL, Scrum, Software Development, SQL, Subversion, Web Applications, Web Services, XML\nFigure 1: Sample job description from the mycareersfuture.sg website. Note that Job Require-\nments constitutes the textual description and Required Skillsconstitutes the skill labels of the dataset.\nWe believe such inconsistencies may be due to the communication gap between the target em-\nployer (who are domain experts) and recruiters (who may have little in-domain expertise). To\nmeasure the extent of the mismatch, we analyzed JDs from a Singaporean government job portal,\nmycareersfuture.sg. We observe that 40% of JDs miss listing 20% or more explicitly-stated\nskills in the prose description. In total, 78.86% of such skills were missing.\nWe propose an automated system to predict the set of required skills given a textual JD. Formally,\ngiven an input text t ∈T, we ﬁnd a mapping f : T − →[0, 1]S, where S = |S|and S is the global skill\nset. f yields a probability score for each s ∈S labels given t,\nf(t) =P(ri = 1|t)\nwhere i ∈{1, ··· , S}, and ri is the label corresponding to ith skill.\nIn a nutshell, the task involves labeling the textual descriptions with its corresponding labels selected\nfrom a vast set of labels. We approach this problem from a natural language modeling perspective and\ntreat this as an Extreme Multi-label Classiﬁcation (XMLC) (Lui et al., 2017) task. Our contributions are\nas follows:\n1. We release the large mycareersfuture dataset, and provide benchmark results with state-of-\nthe-art baselines to foster research on this important problem1;\n2. We propose a deep learning-based model and a novel Correlation Aware Bootstrapping approach\nthat exploits the correlation between skills and their linguistic footprints;\n3. We provide the performance comparison of the proposed methodology with suitable state-of-the-art\nbaseline models.\n2 Related Work\nWe describe the key relevant work from extreme text classiﬁcation, language modeling and sequence\nlabeling pertinent in developing our approach.\n1Available at https://github.com/WING-NUS/JD2Skills-BERT-XMLC\n5834\nEXtreme Multi-label Text Classiﬁcation (XMLC) refers to the text classiﬁcation scenarios where\ncardinality of the set of labels is large, i.e., thousands or millions (Liu et al., 2017).\nOne-vs-All (OV A) (Lui et al., 2017) corresponds to a popular class of approaches for text classiﬁcation\ntask with high prediction accuracy. This approach is computationally efﬁcient for the XMLC task for\nmodest-sized label sets (up to an order of a few thousand labels).\nEmbedding-based approaches have also proven effective for this task. They effectively reduce the\nnumber of labels by assuming that the label space is well-approximated by a computed low-rank matrix.\nThey reliably learn embeddings for a lower-dimensional label space, then use suitable decompression\ntechniques to map them back to the original label space (Bhatia et al., 2015; Cisse et al., 2013). More\nrecently, methods have been proposed to reduce the information loss during the decompression phase,\nsuch as LPSR (Weston et al., 2013) and MLRF (Agrawal et al., 2013).\nFollowing the success of Computer Vision community, deep learning-based approaches have led to a\nsurge of performance in multiple natural language modeling tasks. Neural approaches to natural language\nprocessing (e.g., (Kim, 2014)) have also been applied to XMLC tasks (Halder et al., 2018). Methods\nsuch as XML-CNN have been proposed, which uses a bottleneck layer to reduce the number of learnable\nparameters (Liu et al., 2017). A cluster sensitive attention mechanism has also been explored to capture\nthe correlation between labels (Halder et al., 2018), in conjunction with RNN-based text encoders, such\nas the GRU (Chung et al., 2014). These methods have been used in downstream applications such as tag\nprediction on Wikipedia articles and news articles, as well as recommending textual articles to potentially\ninterested users.\nLanguage Modeling: Recent developments in language modeling are also relevant to our task. The\nobjective here is to model the syntactic and semantic structure of input language utterances through\nmodel training to predict tokens, based on the available contextual information. Models such as ELMo\n(Peters et al., 2018), Transformer and BERT (Vaswani et al., 2017; Devlin et al., 2019) have signiﬁcantly\nimproved a bevy of NLP tasks through their unsupervised pre-trained representations.\nAnother relevant and widely-used approach is Sequence Labelling. These tasks involve prediction of\na label for each of the tokens present in the textual content. Contiguous chunks of text are then treated\nas the prediction target. In the Named Entity Recognition (NER) task, entities such as organization,\nperson, locationare extracted from the text. Typically these methods employ Bidirectional Long Short\nTerm Memory (BiLSTM) stacked with Conditional Random Field (CRF) layers to decode the sequence\n(Huang et al., 2015; Ma and Hovy, 2016). Context-sensitive character embedding methods also enhance\nthe effectiveness (Akbik et al., 2018).\nAlthough our task can be formulated as a sequence labeling task, we take the multi-label classiﬁcation\nperspective for two reasons. First, the label space is ﬁnite: most job portal interfaces allow the user to\nchoose a subset from a predeﬁned list of skills in the system. Second, to the best of our knowledge, there\nis no existing dataset with manually-annotated labels from textual JDs, making it sub-optimal to train a\nsequence labeling model.\nThe existing approaches do not address all the unique challenges associated with our skill extraction\nproblem. Alternate techniques of exploiting large scale pre-trained language models for general informa-\ntion extraction tasks have been explored. X-Transformer is a solid representative of such methods (Chang\net al., 2020). This BERT-based model ﬁrst encodes the input query representation, then matches the rep-\nresentation with the clustered labels in an embedding space, formed either by ELMo (Peters et al., 2018)\nrepresentations or by tf-idf scores. However, obtaining reliable representation for sparse textual label\nsets featuring domain-speciﬁc terminology (e.g., JDs) is challenging. This motivates us to explore alter-\nnate approaches – techniques which potentially do not require explicit label representation or predeﬁned\nsemantic meaning of labels. To address these shortcomings, we develop a novel bootstrapping technique\nto achieve semantic label space mapping without explicitly labelled examples. We aim to deﬁne a model\nthat is end-to-end trainable and does not depend on large, domain-speciﬁc corpora to learn an embedding\nspace from scratch, in contrast to the higher prerequisite requirements of the X-Transformer.\n5835\n3 Method\nWe introduce a neural XMLC framework, BERT–XMLC. The architecture is inspired by the models\nproposed in (Chang et al., 2020) and (Halder et al., 2018). The model comprises two major components:\nFigure 2: Architecture of our proposed model BERT–XMLC.\n1. Text Encoder: The model takes the textual JD ( t) as input, consisting of n words ( t =\n{w1, w2, ..., wn}) . We use the pre-trained BERTBASE model to encode the text into low-dimensional\ndense vectors (Devlin et al., 2019). Internally, it ﬁrst looks up the WordPiece embeddings (Wu et\nal., 2016), then applies a pre-trained bi-directional transformer-based language model to yield a hid-\nden representation of the input. We consider the encoding of the[CLS] token as the representation\nof the textual input.\nht = BERTBASE(t) (1)\n2. Bottleneck Layer: Inspired by (Liu et al., 2017), we feed the encoded textual representation to a\nbottleneck layer. This layer alleviates overﬁtting by (signiﬁcantly) limiting the number of trainable\nparameters. It compresses the encoded representation ht using, zt = tanh(W ·ht + b), where W\nand b are the weight, and bias matrices respectively. Finally, zt is fed to a fully connected layer\nusing sigmoid activations with S output units, to induce the probability of each possible target label\n(i.e., skill). Following standard multi-labelling classiﬁcation approaches, the ﬁnal layer treats it as\nS independent binary classiﬁcation problems. It uses binary cross-entropy loss and optimizes the\nnetwork using Adam (Kingma and Ba, 2014).\n3.1 Correlation Aware Bootstrapping\nLet us take a step back and take a closer look at the data. As mentioned, along with the textual JD,\nwe also observe a (rather incomplete) list of required skills. Through our BERT–XMLC model, we are\ntrying to ﬁnd a relationship between the text and the associated labels (skills). We argue that there are\ntwo critical signals which are not explicitly captured in this traditional training method, i.e, (a) semantic\nrepresentation of the skill labels, and (b) co-occurrences of skills. We introduce a bootstrapping process\nthat uses these two assumptions to create artiﬁcial training data that guides the initial stages of training.\nSemantic Representation of Skills: As shown in Figure 1, the skill labels are essentially phrases in\nnatural language e.g., “Agile Methodologies”, “Software Development” and so on. These phrases (or its\n5836\nconstituent words) might also occur in the textual description. Hence, to get a richer representation of\nthe skills, we bootstrap the network with:\nDsem = {< s, encodeOH (s) > ∀s ∈S} (2)\nwhere encodeOH (s) represents a one-hot encoding for skill s, where S is the set of all skills. This\nprocess yields S additional training points, irrespective of the training data.\nCo-occurrence based Correlation of Skills: The network should also learn the correlation between the\nskills themselves. We direct the network to focus on the skill co-occurrences by additionally training on:\nDcorr = {< concatenate(s ∈Sk), label(k) > ∀k ∈{1, ··· , M}} (3)\nwhere label(·) is the original label of kth sample (binary vector), Sk is the set of all the skills mentioned\nin the kth sample, and M is the number of training samples. We obtain M additional training points by\nthis method.\nWe conduct standard training of the model after bootstrapping with these additional {Dsem ∪Dcorr}\nsamples, using the original learning objectives. Our bootstrapping process, which we term Corre-\nlation Aware Bootstrapping (CAB), sizeably increases the number of training examples from M, to\nM + |Dsem|+ |Dcorr|= 2∗M + S. We hypothesize that these added examples help the network to\nbetter capture the correlation among skills, which we validate later in our experiments. Note that this\nbootstrapping method is generic; it can be applied to any off-the-shelf XMLC model for text, as long as\nthe labels have titles that originate from the same vocabulary distribution of input text.\n4 Experiments\nWe have collected data from the Singaporean government website,mycareersfuture.sg with per-\nmission consisting of over 20, 000 richly-structured job posts having 16 informative ﬁelds about the\ndetails and the current status of the advertisements. The statistics of the dataset are shown in Table 1.\nTo the best of our knowledge, there is no relevant large-scale publicly available dataset comprising job\ndescriptions with their corresponding list of required skills at the time of conducting the experiments.\nFor our task, we consider concatenation of “roles & responsibilities” and “job requirements” ﬁelds\nas the textual description, and “required skills” as the set of target discrete labels. We perform the pre-\nprocessing operations on the text, inclusive of lower-casing, stopword removal, and rare word removal.\nWe split the dataset into assigned training, validation and testing dataset with an 80:10:10 proportion. To\naid the reproduction of our results and to encourage further research in this domain in general, we have\nmade both the code and the dataset publicly available. We might provide updated datasets in the future\nas more JDs are continuously being posted.\nTable 1: MyCareersFuture.sg Dataset (Version 1.0) Statistics.\n# of job posts 20, 298\n# of distinct skills 2, 548\n# of skills with 20 or more mentions 1, 209\nAverage skill tags per job post 19.98\nAverage token count per job post 162.27\nMaximum token count in a job post 1, 127\n4.1 Baseline Models\nTo show the effectiveness of different aspects of our approach, we evaluate our model perfor-\nmance against competitive XMLC baselines. These constitute (1) CNN-Kim (Kim, 2014), (2)\nLSTM (Rockt ¨aschel et al., 2015), (3) BiLSTM (Sun et al., 2017), (4) BiGRU (Halder et al., 2018),\n(5) BiGRU w/ Cluster Sensitive Attention (CSA) (Halder et al., 2018). For all the RNN-based methods\n5837\n(i.e., Models 2–5, and our proposed model), we ﬁx the architecture to 2 layers to ensure a fair compari-\nson. We implemented our model in PyTorch2. Further details about the hyperparameters can be found in\nour sources.\nMetrics: The central theme of this work centers around the concept that the required skills in the\nground truth are incomplete. As a result, we do not treat the negatives in the ground truth as true nega-\ntives, since they could have just missed out during the manual construction of the JD by the job poster.\nHence, we rely on recall-oriented metrics to evaluate our model. We use the following set of metrics:\n•Mean Reciprocal Rank (MRR) indicates the (reciprocal) position of ﬁrst true positive in the pre-\ndicted list of skills. It yields a score between 0 −1.\n•Recall@M captures the recall based on the top- M number of skills in the prediction. This ranges\nbetween 0 −100.\n•Normalized Discounted Cumulative Gain (nDCG@M) discounts the true positives that occur\nlater in the prediction rankings. This score ranges between 0 −100, similar to recall.\nNote that M varies between 5–100 for the metrics of recall and nDCG.\nTable 2: Mean Reciprocal Rank (MRR) Comparison.\nModel MRR\n1. CNN-Kim (Kim, 2014) 0.8195\n2. LSTM (Rockt¨aschel et al., 2015) 0.8417\n3. BiLSTM (Sun et al., 2017) 0.8565\n4. BiGRU (Halder et al., 2018) 0.8716\n5. BiGRU w/ CSA (Halder et al., 2018) 0.8840\n6. BERT–XMLC 0.9019\n7. BiGRU w/ CSA + CAB 0.8995∗\n8. BERT–XMLC + CAB (our proposal) 0.9049\nTable 3: Recall Comparison for various M.\nModel Recall\n@5 @10 @30 @50 @100\n1. CNN-Kim (Kim, 2014) 16.38 29.59 59.60 70.40 82.47\n2. LSTM (Rockt¨aschel et al., 2015) 16.83 29.35 57.09 68.65 81.46\n3. BiLSTM (Sun et al., 2017) 17.51 31.19 61.77 73.25 84.89\n4. BiGRU (Halder et al., 2018) 17.73 31.27 60.84 72.80 84.88\n5. BiGRU w/ CSA (Halder et al., 2018)18.34 32.52 64.19 75.75 87.02\n6. BERT–XMLC 19.60 35.58 70.10 80.91 90.26\n7. BiGRU w/ CSA + CAB 18.93∗ 33.84∗ 66.36∗ 77.66∗ 88.28∗\n8. BERT–XMLC + CAB 21.67* 40.49* 79.59* 86.60* 92.24*\n* using proposed methodology\n4.2 Quantitative Analysis\nTables 2, 3 and 4 present MRR, Recall@ M, and nDCG@ M performance on the skill prediction task.\nWe discuss our observations in two parts.\nEffectiveness of BERT–XMLC:Rows 1–6 correspond to the models when they are trained without the\nbootstrapping steps (cf Section 3.1). We observe that our BERT–XMLC model consistently outperforms\n2https://pytorch.org/\nBoldface results represent best results in corresponding categories\n5838\nTable 4: Normalized Discounted Cumulative Gain (nDGC) Comparison at various M.\nModel nDCG\n@5 @10 @30 @50 @100\n1. CNN-Kim (Kim, 2014) 28.21 40.23 60.60 66.37 71.96\n2. LSTM (Rockt¨aschel et al., 2015) 29.27 40.68 59.43 65.61 71.53\n3. BiLSTM (Sun et al., 2017) 30.32 42.77 63.50 69.64 75.04\n4. BiGRU (Halder et al., 2018) 30.80 43.11 63.09 69.49 75.09\n5. BiGRU w/ CSA (Halder et al., 2018)31.71 44.62 66.04 72.23 77.46\n6. BERT–XMLC 33.64 48.18 71.66 77.45 81.79\n7. BiGRU w/ CSA + CAB 32.72∗ 46.28∗ 68.33∗ 74.38∗ 79.30∗\n8. BERT–XMLC + CAB 35.93* 52.84* 79.32* 82.96* 85.41*\n* using proposed methodology\nthe baselines by comfortable margins over all metrics. Particularly, in terms of nDCG, we observe\na relative improvement of 6.08% for M as small as 5. Unlike the BiGRU with CSA (Row 5), our\nBERT–XMLC model (Row 6) lacks the additional Cluster Sensitive Attention layer on top of the text\nencoder. We believe our use of the Transformer model as a base provisions basic attention mechanisms,\nwhich partially compensates for the lack of cluster sensitivity.\nEffectiveness of Correlation Aware Bootstrapping: Rows 7–8 represent the performance of mod-\nels which are trained over additional instances imputed by our correlation aware bootstrapping (CAB)\nprocess. We observe a surge in model performance, as BERT–XMLC with CAB achieves the best\nscores consistently with a relative improvement as high as 10.5%, and 6.8% in terms of Recall@ 5 and\nnDCG@5, respectively, over its vanilla counterpart (Row 6).\nWe also note that CAB yields performance boosts consistently even across methods: we observe\na jump in the performance for BiGRU with CSA model as well ( 3.2%, and 3.18% respectively for\nRecall@5 and nDCG@5). These improvements give empirical evidence that support our hypothesis\nabout the correlation among related skills and skills’ intrinsic natural language representation.\n4.3 Qualitative Analysis\nCAB directs the network to learn the correlation among skills. It is helpful to also visualize this qualita-\ntively on actual instances, in addition to the macroscopic, quantitative improvement. Figure 3 presents a\ncase study sample job advertisement, along with the corresponding gold standard required skills, and the\nBERT–XMLC & BERT–XMLC+CAB predictions. In the sample, we validate BERT–XMLC+CAB’s:\n•Improved recall. BERT–XMLC with CAB predictions are visibly more accurate compared to\nBERT–XMLC. Skills such as “Web Services”, “C++”, “PHP”, “Linux” are skills predicted correctly\nby the bootstrapped model, whereas the vanilla model does not retrieve them.\n•Relevant missing skill prediction. BERT–XMLC with CAB predicts “Databases” as a re-\nquired skill, whereas BERT–XMLC does not. Upon manual inspection, we believe that the skill\n“Databases” is actually relevant and was probably missed out while creating the JD. BERT–\nXMLC+CAB retrieves this correctly, likely by associating “Databases”, and the tokens present\nin the text such as “SQL” and “databases”.\n•Recall of redundant and similar skills.The JD text has both “SQL” and “databases” tokens, which\nthe BERT–XMLC with CAB likely uses as a basis for predicting –“SQL”, “MySQL”, “Microsoft\nSQL Server”, “Databases”. We believe that the co-occurrence based correlation of skills (cf Section\n3.1) captured by our model is crucial in predicting semantically similar skills. This is a critical trait\nfor skills recall, as both prospective job seekers and job posters may not use the same terminology\nto indicate the same skill semantically. We observe that many skills in the top- 100 predictions\nlook relevant in our judgement, although they are missing in the ground truth (both in the JD and\ngold-standard required skill labels).\n5839\nJob Requirements: Requirements performing end end software development cycle coding using Java\nj2ee spring framework Oracle plSQL Multithreading angular js hibernate rest soap api oracledatabases\nshell scripting degree Information Technology Engineering background minimum 5 9 years experience\ninformation technology software development must proven experience performing end end software de-\nvelopment cycle strong experience coding using java j2ee spring framework strong knowledge angular\njs hibernate spring rest soap api experience knowledge css html must 4 6 years full stack development\nfrontend design development backend strong knowledge databases good experience agile methodology\ntest driven development self started keen learner new technologies good communication skills\nRequired Skills: Agile Methodologies, Java, Software Development, .NET, C#, C++ , HTML,\nJavascript, jQuery, Linux , Microsoft SQL Server, MySQL, SQL, Web Services , XML, C, CSS, PHP ,\nPython, Software Engineering\nBERT–XMLC Predictions: SQL, Java, XML, JavaScript, Software Development, Web Services, Ag-\nile Methodologies, MySQL, Microsoft SQL Server, C#, HTML, jQuery, .NET, Web Applications\nBERT–XMLC + CAB Predictions: Java, SQL, JavaScript, Software Development, XML, Agile\nMethodologies, HTML, MySQL, Web Services, jQuery, C#, C++, Linux, Scrum, PHP, Microsoft SQL\nServer, Web Applications, .NET, CSS,Databases\nFigure 3: Sample job description (omitting stop words), with required skills and those predicted by the\nBERT–XMLC and BERT–XMLC+CAB models.\n5 Discussion: On Implicit versus Explicit Skills\nWe can think of job descriptions and their required skills as two different forms of the same underly-\ning job. The job’s manifestation in both forms ideally should corroborate and support each other, but\nsometimes maybe incongruous. This results in incomplete ground truth in both forms. Our BERT–\nXMLC+CAB model is architected to overcome this challenge when the two forms do not reinforce each\nother as expected.\nWe now analyze how required skills are demographically represented in the JD explicitly (present as\nsubstrings in the JD) and implicitly (absent in the JD, but likely inferrable from context). We assess the\nlevel of implicitness in the required job skills. As an extreme, if all skills are explicitly mentioned in the\nJD, our task is easy: it is trivial to list the required skills, as they are all explicit substrings in the JD – a\nsimple string matching algorithm would sufﬁce. At the other extreme, if all required skills are implicit,\nour task is challenging: every skill needs to be inferred from the context given by the JD. The complexity\nFigure 4: Required Skill Histogram, binned by the percentage of occurrences where it is an implicit skill.\n5840\nFigure 5: EIM, RIIM, REIM measures over different models.\nof the task increases with presence of more implicit skills in the required skill set. Figure 4 shows a\nmacroscopic view of this phenomenon in our dataset through a skill histogram. Each of the 2,548 skills\nin the dataset is accounted in the ﬁgure. We place the skill in one of 30 bins by the percentage of times\nit occurs as an implicit skill in the JDs where it is required. The average shows that 86.13% of skills are\nimplicit. As the ﬁgure is strongly biased towards implicit skills (over 1,500 skills are always implied,\nnever explicitly mentioned), we can see the problem is difﬁcult, and extraction-based models hopelessly\nfail. Abstractive or generalising approaches such as our model are required for implicit skills.\nTo study this in more depth, we also propose and plot (Figure 5) the following metrics 3 that capture\nmodel performance in a manner sensitive to the implicit and explicit status of skills:\n1. EIM (Explicit Inference Measure): micro, instance-based measure of explicit skills predicted by\nthe model, compared against gold-standard explicit skills mentioned for a JD. As an example, if the\nmodel declares 4 substrings of the JD as skills, and there are 5 explicit skills present in skill labels\nof JD, EIM = 4\n5 = 0.8(80%)\n2. RIIM (Relative Implicit Inference Measure): macro, recall-based measure of implicit skills pre-\ndicted by the model, relative to the entire set of implicit skills. As an example, if the model recovers\n2 of 6 skills that are implied but not explicitly substrings in the JD, RIIM = 2\n6 = 0.33(33.3%)\n3. REIM (Relative Explicit Inference Measure): macro, recall-based measure of explicit skills pre-\ndicted by the model compared to the entire set of explicit skills.As an example, if the model de-\nclares 4 substrings of the JD as skills, and there are total 8 explicit skills present in JD, REIM =\n4\n8 = 0.5(50%)\nHere, again, the results are consistent: increasing model complexity (leftmost model to rightmost model\namong the eight models compared) improves both instance-level and macro-level recall. More impor-\ntantly, these results are also consistent for our BERT–XMLC+CAB model, which performs best in all\nthree metrics. Improvement is most pronounced with the implicit (EIM and RIIM) metrics, clearly show-\ning the improved abstractive generalisation capability of our model.\nNote also the EIM measure of BERT–XMLC+CAB tallies to over 100%, suggesting that the model\ncaptures a larger number of explicit skills than is actually mentioned in the skill labels (in job descrip-\ntions).\n3Note that these metrics are evaluated based on the skills retrieved by comparing the skill activation with the threshold activation\nof 0.5.\n5841\n6 Conclusion\nWe address the prediction of required skills from job descriptions (JDs). Although job descriptions\nand their associated required skills can be thought of as two views of the same underlying job, our\nanalysis reveals that many required skills are implicitly signaled. We develop an Extreme Multi-label\nClassiﬁcation method that utilizes BERT BASE within a Transformer model to predict the required skills\nfrom a textual job description. Importantly, we propose a novel bootstrapping approach that exploits\nthe underlying natural language representation of skills and their co-occurrence relationships with other\nskills. We perform experiments on a large real-world dataset from a popular Singaporean government job\nportal, mycareersfuture.sg. We show that our model outperforms recent competitive baselines,\nespecially in recalling such implicit skills.\nReferences\nRahul Agrawal, Archit Gupta, Yashoteja Prabhu, and Manik Varma. 2013. Multi-label learning with millions\nof labels: Recommending advertiser bid phrases for web pages. In Proceedings of the 22nd international\nconference on World Wide Web, pages 13–24. ACM.\nAlan Akbik, Duncan Blythe, and Roland V ollgraf. 2018. Contextual string embeddings for sequence labeling. In\nProceedings of the 27th International Conference on Computational Linguistics, pages 1638–1649, Santa Fe,\nNew Mexico, USA, August. Association for Computational Linguistics.\nKush Bhatia, Himanshu Jain, Purushottam Kar, Manik Varma, and Prateek Jain. 2015. Sparse local embeddings\nfor extreme multi-label classiﬁcation. In Advances in neural information processing systems, pages 730–738.\nRoberto Boselli, Mirko Cesarini, Stefania Marrara, Fabio Mercorio, Mario Mezzanzanica, Gabriella Pasi, and\nMarco Viviani. 2018. Wolmis: a labor market intelligence system for classifying web job vacancies. Journal\nof Intelligent Information Systems, 51(3):477–502.\nVera Brenˇciˇc and Marko Pahor. 2019. Exporting, demand for skills and skill mismatch: Evidence from employers’\nhiring practices. The World Economy.\nWei-Cheng Chang, Hsiang-Fu Yu, Kai Zhong, Yiming Yang, and Inderjit Dhillon. 2020. Recognizing text entail-\nment via bidirectional lstm model with inner-attention. In SIGKDD Conference on Knowledge Discovery and\nData Mining. ACM.\nJunyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated\nrecurrent neural networks on sequence modeling. In NIPS 2014 Workshop on Deep Learning, December 2014.\ncielotalent.com. 2014. So Long, Traditional Job Boards? https://www.cielotalent.com/insights/\ntalent-acquisition-fast-facts-so-long-traditional-job-boards . Online; accessed\n25 September 2019.\nMoustapha M Cisse, Nicolas Usunier, Thierry Artieres, and Patrick Gallinari. 2013. Robust bloom ﬁlters for large\nmultilabel classiﬁcation tasks. In Advances in Neural Information Processing Systems, pages 1851–1859.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirec-\ntional transformers for language understanding. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers), pages 4171–4186. ACL, June.\nKishaloy Halder, Lahari Poddar, and Min-Yen Kan. 2018. Cold start thread recommendation as extreme multi-\nlabel classiﬁcation. In Companion of the The Web Conference 2018 on The Web Conference 2018, pages\n1911–1918. International World Wide Web Conferences Steering Committee.\nZhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional lstm-crf models for sequence tagging. arXiv preprint\narXiv:1508.01991.\nYoon Kim. 2014. Convolutional neural networks for sentence classiﬁcation. In Proceedings of the 2014 Con-\nference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751, Doha, Qatar,\nOctober. Association for Computational Linguistics.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980.\n5842\nJingzhou Liu, Wei-Cheng Chang, Yuexin Wu, and Yiming Yang. 2017. Deep learning for extreme multi-label text\nclassiﬁcation. In SIGIR’17: Proceedings of the 40th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, pages 115–124. ACM.\nBenjamin Lochner, Christian Merkl, Heiko St¨uber, and Nicole G¨urtzgen. 2020. A note on recruiting intensity and\nhiring practices: Cross-sectional and time-series evidence.\nJingzhou Lui, Wei-Cheng Chang, Yuexin Wu, and Yiming Yang. 2017. Deep learning for extreme multi-label text\nclassiﬁcation. In International ACM SIGIR Conference on Research and Development in Infromation Retrieval\n(SIGIR), pages 115–124. ACM.\nXuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 1064–1074, Berlin, Germany, August. Association for Computational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 2227–2237, New Orleans, Louisiana, June. Association for Computational Linguistics.\nTim Rockt¨aschel, Edward Grefenstette, Karl Moritz Hermann, Tom´aˇs Koˇcisk`y, and Phil Blunsom. 2015. Reason-\ning about entailment with neural attention. arXiv preprint arXiv:1509.06664.\nVarun Shenoy and PS Aithal. 2018. Literature review on primary organizational recruitment sources. Interna-\ntional Journal of Management, Technology, and Social Sciences (IJMTS), 3(1):37–58.\nChengjie Sun, Yang Liu, Chang’e Jia, Bingquan Liu, and Lei Lin. 2017. Recognizing text entailment via bidirec-\ntional lstm model with inner-attention. In International Conference on Intelligent Computing, pages 448–457.\nSpringer.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and\nIllia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages\n5998–6008.\nJason Weston, Ameesh Makadia, and Hector Yee. 2013. Label partitioning for sublinear ranking. In International\nconference on machine learning, pages 181–189.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google’s neural machine translation system: Bridg-\ning the gap between human and machine translation. arXiv preprint arXiv:1609.08144.",
  "topic": "Bootstrapping (finance)",
  "concepts": [
    {
      "name": "Bootstrapping (finance)",
      "score": 0.7596672773361206
    },
    {
      "name": "Computer science",
      "score": 0.7579569816589355
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5164735317230225
    },
    {
      "name": "Task (project management)",
      "score": 0.44708871841430664
    },
    {
      "name": "Relevance (law)",
      "score": 0.4432523250579834
    },
    {
      "name": "Machine learning",
      "score": 0.4377847909927368
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.42476335167884827
    },
    {
      "name": "Natural language processing",
      "score": 0.42465782165527344
    },
    {
      "name": "Process (computing)",
      "score": 0.42050665616989136
    },
    {
      "name": "Representation (politics)",
      "score": 0.4158386290073395
    },
    {
      "name": "Information retrieval",
      "score": 0.3960287272930145
    },
    {
      "name": "Data science",
      "score": 0.36126261949539185
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Financial economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}