{
  "title": "JiuZhang: A Chinese Pre-trained Language Model for Mathematical Problem Understanding",
  "url": "https://openalex.org/W4282916217",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2752884039",
      "name": "Zhao, Wayne Xin",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2048259267",
      "name": "Zhou Kun",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2151817880",
      "name": "Gong Zheng",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A71533926",
      "name": "Zhang Beichen",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2356786011",
      "name": "Zhou, Yuanhang",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2226276654",
      "name": "Sha Jing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1758874643",
      "name": "Chen, Zhigang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1845590260",
      "name": "Wang Shijin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2107657322",
      "name": "Liu Cong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221399670",
      "name": "Wen, Ji-Rong",
      "affiliations": [
        "Renmin University of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963705839",
    "https://openalex.org/W4285247467",
    "https://openalex.org/W3104235802",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2251349042",
    "https://openalex.org/W2265846598",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2250769864",
    "https://openalex.org/W3185289994",
    "https://openalex.org/W3174396451",
    "https://openalex.org/W2757276219",
    "https://openalex.org/W2031071334",
    "https://openalex.org/W3187018546",
    "https://openalex.org/W2103680826"
  ],
  "abstract": "This paper aims to advance the mathematical intelligence of machines by presenting the first Chinese mathematical pre-trained language model~(PLM) for effectively understanding and representing mathematical problems. Unlike other standard NLP tasks, mathematical texts are difficult to understand, since they involve mathematical terminology, symbols and formulas in the problem statement. Typically, it requires complex mathematical logic and background knowledge for solving mathematical problems. Considering the complex nature of mathematical texts, we design a novel curriculum pre-training approach for improving the learning of mathematical PLMs, consisting of both basic and advanced courses. Specially, we first perform token-level pre-training based on a position-biased masking strategy, and then design logic-based pre-training tasks that aim to recover the shuffled sentences and formulas, respectively. Finally, we introduce a more difficult pre-training task that enforces the PLM to detect and correct the errors in its generated solutions. We conduct extensive experiments on offline evaluation (including nine math-related tasks) and online $A/B$ test. Experimental results demonstrate the effectiveness of our approach compared with a number of competitive baselines. Our code is available at: \\textcolor{blue}{\\url{https://github.com/RUCAIBox/JiuZhang}}.",
  "full_text": "JiuZhang: A Chinese Pre-trained Language Model for\nMathematical Problem Understanding\nWayne Xin Zhaoâ€ \nGaoling School of Artificial\nIntelligence, Renmin University of\nChina\nBeijing, China\nbatmanfly@gmail.com\nKun Zhouâˆ—\nSchool of Information, Renmin\nUniversity of China\nBeijing, China\nfrancis_kun_zhou@163.com\nZheng Gongâˆ—\nGaoling School of Artificial\nIntelligence, Renmin University of\nChina\nBeijing, China\ngongzheng0109@ruc.edu.cn\nBeichen Zhangâˆ—\nGaoling School of Artificial\nIntelligence, Renmin University of\nChina\nBeijing, China\nzhangbeichen724@gmail.com\nYuanhang Zhouâˆ—\nSchool of Information, Renmin\nUniversity of China\nBeijing, China\nsdzyh002@gmail.com\nJing Sha\niFLYTEK Research\nState Key Laboratory of Cognitive\nIntelligence\nHefei, China\njingsha@iflytek.com\nZhigang Chen\niFLYTEK Research\nState Key Laboratory of Cognitive\nIntelligence\nHefei, China\nzgchen@iflytek.com\nShijin Wang\nAI Research (Central China), iFLYTEK\nState Key Laboratory of Cognitive\nIntelligence\nWuhan, China\nsjwang3@iflytek.com\nCong Liu\niFLYTEK Research\nHefei, China\ncongliu2@iflytek.com\nJi-Rong Wenâ€ \nGaoling School of Artificial\nIntelligence, Renmin University of\nChina\nBeijing, China\njrwen@ruc.edu.cn\nABSTRACT\nThis paper aims to advance the mathematical intelligence of ma-\nchines by presenting the first Chinese mathematical pre-trained\nlanguage model (PLM) for effectively understanding and repre-\nsenting mathematical problems. Unlike other standard NLP tasks,\nmathematical texts are difficult to understand, since they involve\nmathematical terminology, symbols and formulas in the problem\nstatement. Typically, it requires complex mathematical logic and\nbackground knowledge for solving mathematical problems.\nâˆ—These four authors contributed equally to this research.\nâ€ Also with Beijing Key Laboratory of Big Data Management and Analysis\nMethods.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nKDD â€™22, August 14â€“18, 2022, Washington, DC, USA\nÂ© 2022 Association for Computing Machinery.\nACM ISBN 978-1-4503-9385-0/22/08. . . $15.00\nhttps://doi.org/10.1145/3534678.3539131\nConsidering the complex nature of mathematical texts, we de-\nsign a novel curriculum pre-training approach for improving the\nlearning of mathematical PLMs, consisting of both basic and ad-\nvanced courses. Specially, we first perform token-level pre-training\nbased on a position-biased masking strategy, and then design logic-\nbased pre-training tasks that aim to recover the shuffled sentences\nand formulas, respectively. Finally, we introduce a more difficult\npre-training task that enforces the PLM to detect and correct the\nerrors in its generated solutions. We conduct extensive experiments\non offline evaluation (including nine math-related tasks) and online\nğ´/ğµtest. Experimental results demonstrate the effectiveness of our\napproach compared with a number of competitive baselines. Our\ncode is available at: https://github.com/RUCAIBox/JiuZhang.\nCCS CONCEPTS\nâ€¢ Information systems â†’Language models.\nKEYWORDS\nChinese Pre-trained Language Model, Mathematical Logic Under-\nstanding\narXiv:2206.06315v1  [cs.CL]  13 Jun 2022\nKDD â€™22, August 14â€“18, 2022, Washington, DC, USA Zhao, et al.\nACM Reference Format:\nWayne Xin Zhao, Kun Zhou, Zheng Gong, Beichen Zhang, Yuanhang\nZhou, Jing Sha, Zhigang Chen, Shijin Wang, Cong Liu, and Ji-Rong Wen.\n2022. JiuZhang: A Chinese Pre-trained Language Model for Mathematical\nProblem Understanding. In Proceedings of the 28th ACM SIGKDD Confer-\nence on Knowledge Discovery and Data Mining (KDD â€™22), August 14â€“18,\n2022, Washington, DC, USA. ACM, New York, NY, USA, 11 pages. https:\n//doi.org/10.1145/3534678.3539131\n1 INTRODUCTION\nPure mathematics is, in its way, the poetry of logical ideas.\nAlbert Einstein\nMathematical ability is a human construct, referring to the ability\nto obtain, process, and retain mathematical information from the\ncognitive perspective [11]. It has been widely recognized that it is\ndifficult for machines to grasp such an intelligent ability via compu-\ntational models, which requires a wide spectrum of mathematical\nknowledge, logic and skills. Existing methods in the literature usu-\nally adopt a natural language processing (NLP) approach, focusing\non analyzing and understanding the semantics in the mathemati-\ncal texts (i.e., text that presents the problem statement or answer\nkey). Early studies design explicit extraction methods based on\nstatistical features [ 6], semantic parser [ 29] and other heuristic\nmethods [13]. In contrast, recent progress focuses on learning la-\ntent semantic representations for mathematical texts with deep\nneural networks [3, 18, 34]. However, due to complex mathematical\nlogic and knowledge, it is still challenging to accurately understand\nmathematical problems, which is the fundamental step to develop\nmathematical intelligence for machines.\nUnlike other standard NLP tasks ( e.g., part-of-speech tagging\nand named entity recognition), it is more difficult to understand\nmathematical texts, since they mix mathematical terminology, sym-\nbols and formulas in the problem statement, requiring complex\nmathematical logic and background knowledge for deriving the\nsolution. Recently, pre-trained language models (PLMs) [5, 22] have\nshed light on more effective approaches to the understanding and\nrepresentation of mathematical texts. After being pre-trained on\nlarge-scale general corpus, PLMs can encode rich semantic knowl-\nedge and linguistic characteristics by massive parameters of the\nTransformer architecture [26]. Furthermore, they can deal with\ndownstream tasks via fine-tuning or continual pre-training [7, 8].\nWhen adapting to mathematical texts, existing methods [ 22, 28]\neither adopt the masked language model task (MLM) to improve\nthe understanding of mathematical symbols and terminology [28],\nor devise specific pre-training tasks to enhance the semantic relat-\nedness between text description and formulas [7, 22].\nAlthough PLMs have achieved remarkable performance on pre-\nliminary mathematical tasks, they canâ€™t perform sufficiently well\non higher-level tasks that require a more deep understanding of ad-\nvanced mathematical knowledge and logic, e.g., high-school multi-\nchoice questions and proof problems. A major reason is that the\nadopted pre-training tasks ( e.g., the MLM task) mainly capture\ntextual semantics via contextual co-occurrences instead of mathe-\nmatical semantics via complex mathematical knowledge or logic.\nAs a result, these PLMs might produce a linguistically reasonable\nbut mathematically incorrect answer to a mathematical problem\n(e.g., one plus one equals to three), since it is not fully aware of the\nunderlying mathematical semantics. To accurately understand the\nmathematical semantics, it is essential to develop more effective\npre-training tasks or strategies for enhancing the representation\ncapacity of PLMs on mathematical texts. As another major limit, ex-\nisting mathematical PLMs are mostly pre-trained on English corpus,\nwhich is not directly applicable to non-English domains.\nTo address these issues, in this paper, we propose the first Chi-\nnese PLM for mathematical problem understanding, named as Ji-\nuZhang1. JiuZhang is developed based on the Transformer [ 33]\narchitecture, consisting of a shared Transformer encoder, a decoder\nfor the understanding tasks ( ğ‘ˆ-decoder) and a decoder for the\ngeneration tasks (ğº-decoder), which endows the model with the\nflexibility to deal with different downstream tasks. To pre-train\nJiuZhang, we collect a large-scale Chinese corpus consisting of\n1,276,952 high-school mathematical exercises or tests, covering a\nvariety of problem types such as multi-choice and blank-filling\nproblems.\nAs the major technical contribution, we design a curriculum\npre-training approach to improving the understanding of mathe-\nmatical knowledge and logic, from basic to advanced courses. As\nthe basic course, we aim to enhance the understanding of math\nsymbols and their semantic relatedness with the text, and design a\nposition-biased masking strategy that assigns larger masking prob-\nabilities to the tokens nearer to the answer in the solution text. As\nthe advanced courses, we focus on improving the capacity of math-\nematical logic reasoning and solution checking, which are essential\nto solve complex math tasks. For mathematical logic reasoning, we\nintroduce the reconstruction tasks that recover the shuffled sen-\ntences or formulas in the mathematical text. For solution checking,\nwe enforce the two decoders to detect and correct the incorrect\ngeneration contents from each other. Such a pre-training process re-\nsembles the student-learning progress [2], where she/he gradually\ngrasps a knowledge point from symbols, logic to problem solving.\nTo the best of our knowledge, it is the first Chinese PLM specially\nfor mathematical problem understanding. We conduct experiments\non nine tasks from high-school math education, including basic clas-\nsification tasks, math text retrieval tasks, question answering tasks\nand solution generation tasks. Experimental results have shown\nthat our approach outperforms eleven baseline models, including\ncompetitive Chinese PLMs. Besides, we also deploy our model in\nthe Zhixue app and onlineğ´/ğµtests further verify the effectiveness\nof our approach.\n2 RELATED WORK\nMathematical Problem Understanding. It is a key capacity to\nunderstand mathematical problems when developing artificial intel-\nligence algorithms for math education applications, e.g., mathemat-\nical problem retrieval [35], solving [15] and classification [22]. As a\nfundamental step, it is essential to learn effective representations\nfor mathematical texts, which involve symbols, formulas and texts\nthat describe the problem. Early methods adopt rule-based meth-\nods [6] to extract features for understanding the text and formulas,\ne.g., semantic parser [ 29], operator tree [ 35] and variable entity\nalignment [13]. With the development of deep learning, a surge\n1Named after one classical Chinese mathematical book.\nJiuZhang: A Chinese Pre-trained Language Model for\nMathematical Problem Understanding KDD â€™22, August 14â€“18, 2022, Washington, DC, USA\nof works adopt deep neural networks to develop more effective\napproaches for mathematical problem understanding. For math\nword problems, RNN [3] and Transformer [18] have been utilized\nto encode the mathematical text and generate the math equation.\nFor mathematical information retrieval, graph neural networks [30]\nhave been adopted to learn meaningful representations over the\nstructured formulas. Recently, the success of PLMs [22, 24] pushes\nforward the understanding and modeling of mathematical texts,\ndue to the excellent capacity of language modeling. For example,\nin order to enhance the understanding of complex math formulas\nand logic, MathBERT [ 22] and COMUS [ 7] pre-train BERT on a\nlarge-scale mathematical corpus with formula-related pre-training\ntasks. However, existing PLMs still rely on (or simply adapt) origi-\nnal pre-training tasks of BERT, which lack a full consideration of\nthe characteristics of mathematical texts. More recently, OpenAI\nproposes GPT-ğ‘“ for automated theorem proving [24], and also uses\nthe proofs generated by GPT- ğ‘“ to iteratively improve its perfor-\nmance [23]. Despite the remarkable performance, these methods\nrequire a million scale of parameters, which is not easy to be de-\nployed or applied in real-world applications.\nChinese Pre-trained Language Models. Pre-trained Language\nModels (PLMs) (e.g., BERT [5], BART [17] and T5 [25]) have largely\nadvanced the progress of language intelligence. Following this di-\nrection, our work is based on Chinese PLMs. The first line of works\nadapt BERT [5] by reusing masked language model (MLM) task to\npre-train Transformer encoders [4, 32] on Chinese corpus. They\nconsider modeling the linguistic characteristics or semantic knowl-\nedge of Chinese texts and devise special strategies to improve the\ntask performance, e.g., whole word masking [4], glyph and pinyin\nembedding [32] and entity enhanced embedding [10]. Focused on\nnatural language understanding, these methods canâ€™t be directly\napplied to text generation tasks. Another line of works pre-train\nTransformer via the auto-regressive task [ 36, 37] or the seq2seq\ntask [38]. They predict or recover the corrupted tokens from left to\nright. Furthermore, several studies attempt to endow Chinese PLMs\nwith both capacities of understanding and generation. For example,\nMengzi [38] and ERNIE-3.0 [31] consist of shared encoding layers\nand multiple task-specific decoders. With a similar architecture,\nCPT [27] adopts a deeper encoder and two shallower decoders, to\naccelerate the inference of text generation. Our work presents the\nfirst Chinese PLM for understanding mathematical texts, with a\nseries of specially designed pre-training tasks.\n3 APPROACH\nIn this section, we present the proposed PLM JiuZhang for math-\nematical problem understanding. We first formally describe the\nmathematical text as the pre-training corpus, then introduce the\nmodel architecture (consisting of a shared encoder and two task-\nspecific decoders) and the curriculum pre-training approach (grad-\nually learning the mathematical knowledge and logic from the\nmathematical corpus). Finally, we present the learning and discus-\nsion.\n3.1 Mathematical Text\nWe first formally describemathematical text, a general phrase refer-\nring to the text relating to a mathematical problem. In our corpus, a\nmathematical problem is associated with two texts, namely problem\nstatement and solution text (a.k.a., answer key). The problem state-\nment introduces necessary background information and explains\nthe problem that is to be solved, and the answer key describes the\nkey hints or complete solving procedure for deriving the answer.\nWe concatenate both problem statement and solution description\nas the mathematical text of a mathematical problem.\nOverall, a mathematical text is a textual description that mixes\ntext words with math symbols. Given a mathematical problemğ‘, the\ncorresponding mathematical text can be considered as a sequence\nof ğ‘› tokens, denoted as ğ‘ = {ğ‘¡1,ğ‘¡2,Â·Â·Â· ,ğ‘¡ğ‘›}, where each token ğ‘¡ğ‘–\nis either a text word or a math symbol (denoting a variable or an\noperator). Furthermore, a consecutive segment of ğ‘™ math symbols\nconstitute a math formula, denoted as ğ‘“ğ‘– = {ğ‘ 1,Â·Â·Â· ,ğ‘ ğ‘™}, where each\nmath symbol ğ‘ ğ‘— is from {ğ‘¡1,ğ‘¡2,Â·Â·Â· ,ğ‘¡ğ‘›}. There are usually multiple\nformulas in a mathematical text, denoted as {ğ‘“1,ğ‘“2,Â·Â·Â· ,ğ‘“ğ‘š}.\nBased on the above notations, this work focuses on pre-training\na PLM on a large-scale corpus consisting of mathematical texts.\nThen, the PLM can be fine-tuned on various mathematical tasks\n(e.g., knowledge point classification and similar question retrieval),\nand improve the corresponding task performance.\n3.2 Model Architecture\nFor dealing with various mathematical tasks, we borrow the idea\nof setting task-specific decoders from CPT [27] and Mengzi [38].\nThe base architecture of JiuZhang consists of a shared Transformer\nencoder, a decoder for understanding tasks (ğ‘ˆ-decoder) and a de-\ncoder for generation tasks (ğº-decoder). The two decoders endow\nour model with more flexibility to solve different downstream tasks.\n3.2.1 Shared Transformer Encoder. Given a mathematical problem\nğ‘= {ğ‘¡1,Â·Â·Â· ,ğ‘¡ğ‘›}, we adopt a shared Transformer encoder to learn\ncontextualized token representations, which can be further utilized\nby ğ‘ˆ-decoder or ğº-decoder for pre-training or fine-tuning. Follow-\ning the standard Transformer [33], our encoder is composed of an\nembedding layer and multiple Transformer layers. In the embed-\nding layer, we maintain a token embedding matrix and a position\nembedding matrix, and then project the mathematical problem into\ndense representations as Eğ‘‡ âˆˆRğ‘›Ã—ğ‘˜ and Eğ‘ƒ âˆˆRğ‘›Ã—ğ‘˜, where ğ‘˜is the\nembedding dimensionality. Note that math symbols are considered\nas special word tokens for embedding. Then, we sum the two em-\nbedding matrices as E = Eğ‘‡ +Eğ‘ƒ. Based on the embedding layer, we\nstack ğ¿Transformer layers (consisting of a multi-head self-attention\nlayer and a point-wise feed-forward network) to further encode\nthese contextualized embeddings E. After ğ¿Transformer layers, the\nrepresentations in the final layer {h(ğ¿)\n1 ,h(ğ¿)\n2 ,Â·Â·Â· ,h(ğ¿)\nğ‘› }are taken\nas the output of the shared Transformer encoder for subsequent\ndecoders.\n3.2.2 Understanding- and Generation-specific Decoders. To solve\ndifferent types of mathematical tasks, we design two task-specific\ndecoders (i.e., ğ‘ˆ-decoder and ğº-decoder). Following the unbalanced\ndesign [27], the two decoders contain much fewer layers than the\nshared encoder (i.e., 2 layers v.s. 10 layers). In this way, the number\nof the involved parameters (including the encoder and one decoder)\nfor a single task is still similar to the original scale of the PLM\narchitecture [5], while the decoding efficiency for generation tasks\nKDD â€™22, August 14â€“18, 2022, Washington, DC, USA Zhao, et al.\n(a) Masked Language Model and Denoised Autoencodingğº-DecoderQ1Q2[S]D1D2[S]ğ‘ˆ-DecoderQ2D2Q1Shared Encoder[C][M][S]D1[M][S][C]Q1[S]D1D2[S]Q2Random Masking[M]Input SequenceMasked Seqence[S] D1[S][C]Q1Q2[S]D1D2\nğº-Decoder(c) Solution CheckingD1D2D2ğ‘ˆ-DecoderD2D2 D3ğº-DecoderD1D2D3ğ‘ˆ-DecoderD1D2D3Shared Encoder[C]Q1Q2[S][S][M]Shared Encoder[C]Q1Q2[S]D3D4D1[S](b) Mathematical Logic Recovering.D2,[C]Q1Q2[S]D1D2D3[S],D4.ğº-DecoderQ1Q2[S]D1D2D3,D4.[S][M][C]Q1Q2[S]D1 [S]D2D3Random ShufflingRandom MaskingD2Correcting[C]Q1Q2[S]D1D2D3,D4.\nFigure 1: The overview of our curriculum pre-training approach, consisting of a basic course about masked token prediction,\nand two advanced courses about mathematical logic recovering and solution checking.\ncan be largely improved. Besides, since we use more layers in the\nshared encoder, it can better capture the underlying semantics of\nmathematical texts.\nThe ğ‘ˆ-decoder is a shallow bidirectional Transformer specially\nfor the understanding tasks, which adopts the similar architecture\nof the shared encoder consisting of ğ¿ğ‘ˆ bidirectional Transformer\nlayers. Following BERT [5], we insert a special symbol â€œ[CLS]â€ at\nthe beginning of a sentence and utilize its representation at the last\nlayer as the output of the ğ‘ˆ-decoder for understanding tasks.\nThe ğº-decoder is a shallow auto-regressive Transformer for gen-\neration tasks. It is composed by a stack of ğ¿ğº Transformer layers\nwith the masked self-attention mechanism, which prevents attend-\ning to subsequent positions during the teacher-forcing training of\ngeneration tasks. Each Transformer layer of the ğº-decoder con-\ntains three sub-layers, namely a multi-head self-attention layer, a\ncross-attention layer over the output representations of the shared\nTransformer encoder, and a point-wise feed-forward network. It\nperforms decoding to generate the sequence for generation tasks.\n3.3 Curriculum Pre-training\nBased on the above model architecture, we further propose a cur-\nriculum pre-training approach, which helps the model gradually\nlearn the complex mathematical knowledge and logic from large-\nscale mathematical corpus. To achieve it, we first utilize the basic\ncourse that consists of the masked language model and denoised\nauto-encoding tasks to learn basic mathematical semantics. Then,\nwe adopt the advanced course of mathematical logic recovering that\nreconstructs the shuffled sentences and formulas to improve the\nunderstanding of mathematical logic. Finally, we propose a more\ndifficult solution checking task to enhance the capacity to detect\nand correct the incorrect generations by itself.\n3.3.1 Basic Course: Masked Token Prediction.In the basic course,\nwe aim to enhance the understanding of math symbols and establish\nthe semantic relatedness between text words and math symbols.\nFollowing BERT [5], we predict the masked tokens based on the\ncontextualized representations. Since we have ğ‘ˆ-decoder and ğº-\ndecoder, we adopt the masked language model (MLM) and denoised\nauto-encoding (DAE) as the pre-training tasks for the two decoders,\nrespectively. Given the input text, we randomly mask part of tokens\nfrom it, and then utilize theğ‘ˆ-decoder to predict the masked tokens\nvia the MLM task and utilize the ğº-decoder to reconstruct the orig-\ninal sentence via the DAE task. In order to adapt to mathematical\ntexts, we further propose a new position-biased masking strategy\nto focus on more tokens at larger positions.\nMasked Token Prediction . Given a mathematical problem ğ‘, we\nconcatenate its question statement and solution text as a sequence,\ndenoted as ğ‘¥. Then, we randomly select 15% tokens of the input se-\nquence for masking (including both words and symbols), in which\n80% ones are replaced by the token â€œ[MASK]â€, 10% ones are replaced\nby a random token and the rest 10% ones remain unchanged. For-\nmally, let eğ‘¥ denote the masked sequence, ğ‘‰ğ‘šğ‘ğ‘ ğ‘˜ denote the selected\ntokens, and the MLM and DAE losses are defined as:\nğ¿ğ‘€ğ¿ğ‘€ =\nâˆ‘ï¸\nğ‘¡ğ‘–âˆˆğ‘‰ğ‘šğ‘ğ‘ ğ‘˜\nâˆ’log ğ‘(ğ‘¡ğ‘–|eğ‘¥; Î˜ğ¸,Î˜ğ‘ˆ), (1)\nğ¿ğ·ğ´ğ¸ =\nâˆ‘ï¸\nğ‘–\nâˆ’log ğ‘(ğ‘¡ğ‘–|ğ‘¡<ğ‘–,eğ‘¥; Î˜ğ¸,Î˜ğº), (2)\nwhere ğ‘(ğ‘¡ğ‘–|eğ‘¥; Î˜ğ¸,Î˜ğ‘ˆ)and ğ‘(ğ‘¡ğ‘–|ğ‘¡<ğ‘–; eğ‘¥; Î˜ğ¸,Î˜ğº)denote the predic-\ntion probabilities of the token ğ‘¡ğ‘– at the ğ‘–-th position according\nto ğ‘ˆ-decoder and ğº-decoder, respectively, ğ‘¡<ğ‘– stands for the pro-\nceeding tokens before position ğ‘–, and Î˜ğ¸, Î˜ğ‘ˆ and Î˜ğº denote the\nparameters of the shared-encoder, ğ‘ˆ-decoder and ğº-decoder, re-\nspectively. Since we deal with Chinese texts, we further adopt the\nwhole word masking strategy [4] to sample word spans for masking\n(all the tokens from a sampled word will be masked), which is more\nsuitable to capture the semantic information reflected by Chinese\nwords.\nPosition-biased Masking. Unlike general texts, the mathematical\ntext conveys important logic that threads the scattered evidences\nfor deriving the final solution. Intuitively, the contained tokens in\na mathematical text are not of equal roles: the words at smaller\npositions are more likely to be an auxiliary evidence, and the words\nat larger positions are more likely to be an important hint to the\nanswer2. Based on this idea, we employ a linearly increasing weight-\ning mechanism that assigns larger masking weights to words at\n2Since our corpus contains the solution text, the tokens at larger positions of a\nmathematical text are nearer to the answer.\nJiuZhang: A Chinese Pre-trained Language Model for\nMathematical Problem Understanding KDD â€™22, August 14â€“18, 2022, Washington, DC, USA\nlarger positions as:\nğ‘ ğ‘– = 30\nğ‘›âˆ’1 Ã—ğ‘–, (3)\nwhere ğ‘ ğ‘– is the sampling weight to be masked for the ğ‘–-th position,\nğ‘›is the sentence length, and ğ‘–is the position index ranging from 0\nto ğ‘›âˆ’1. With this weighting mechanism, the boundary words (i.e.,\nthe first and last words) correspond to the masking probabilities\nof 0% and 30%, respectively, having an average probability of 15%\n(equal to the original masking probability in BERT [ 5]). In other\nwords, the overall average masking probability still remains as 15%,\nwhile we re-distribute the sampling probabilities according to their\npositions. Such a masking strategy enforces the PLMs to focus more\non the tokens that are nearer to the answer.\n3.3.2 Advanced Course: Mathematical Logic Recovering.With the\nabove token-level pre-training tasks, PLM can grasp basic semantics\nabout math symbols and description words. We further capture\nthe mathematical logic involving deductions and derivations for\nsolving the problem, which is essential to the understanding of\nmathematical problems. In order to enhance the logic modeling\nability of PLMs, we design two logic-related pre-training tasks by\nrecovering shuffled sentences and formals, respectively.\nShuffled Sentences Recovering (SSR) . For the SSR task, we first\nshuffle the sentences in the solution text (denoted as ğ‘‘) for a given\nproblem ğ‘, and reconstruct the shuffled text with the ğº-decoder.\nSpecifically, given a solution textğ‘‘consisting of multiple sentences,\nwe randomly shuffle the order of these sentences to produce a\ncorrupted solution text denoted as eğ‘‘ğ‘†. The pre-training objective is\nto recover the original solution text ğ‘‘ based on eğ‘‘ğ‘†, which can be\nformulated by minimizing the conditional language model loss by:\nğ¿ğ‘†ğ‘†ğ‘… =\nâˆ‘ï¸\nğ‘–\nâˆ’log ğ‘(ğ‘¡ğ‘–|ğ‘¡<ğ‘–,eğ‘‘ğ‘†; Î˜ğ¸,Î˜ğº), (4)\nwhere we adopt an auto-regressive loss similar to BART [ 17] to\npredict the original content.\nShuffled Formulas Recovering (SFR) . In the mathematical text,\nformulas are the core elements that determine the underlying math-\nematical logic. Considering this point, we further extend the above\nlogic recovering task for the shuffled formulas. Specifically, we\nfirst shuffle the formulas {ğ‘“1,ğ‘“2,Â·Â·Â· ,ğ‘“ğ‘š}from the solution text to\nconstruct the corrupted solution text denoted as eğ‘‘ğ¹ and then re-\ncover the original content (only the formulas are shuffled). Such a\npre-training task can be defined as:\nğ¿ğ‘†ğ¹ğ‘… =\nâˆ‘ï¸\nğ‘–\nâˆ’log ğ‘(ğ‘¡ğ‘–|ğ‘¡<ğ‘–,eğ‘‘ğ¹; Î˜ğ¸,Î˜ğº). (5)\nThe two pre-training tasks are similar to the task of sentence\norder prediction in BERT [5]. However, we aim to capture the logic\nunderlying the mathematical text, instead of semantic relatedness\nbetween consecutive sentences. In our approach, we consider a\nmore difficult task setting, where the PLM is required to recover\nthe original content (token-level prediction) instead of predicting\nthe original order (sentence-level prediction). Such a pre-training\ntask is essential to enhance the capacity of mathematical logic\nreasoning for PLMs. Moreover, to avoid the catastrophic forgetting\nof basic courses, we also incorporate the MLM and DAE tasks in\nSection 3.3.1 as regularizers in this course. Finally, the logic-based\npre-training objective is given by combining the four losses as:\nğ¿ğ´ğ¶ = ğ¿ğ‘€ğ¿ğ‘€ +ğ¿ğ·ğ´ğ¸ +ğ¿ğ‘†ğ‘†ğ‘… +ğ¿ğ‘†ğ¹ğ‘…. (6)\n3.3.3 Advanced Course: Solution Checking. The above token- and\nlogic-based pre-training tasks can enhance the understanding ca-\npacity of mathematical knowledge and logic to some extent. We\nfurther consider a pre-training task that is directly related to the\nsolving procedure. For mathematical problems, a minor mistake\nat some intermediate step will cause a failed solution. In order to\nderive the correct answer, the PLM should be able to fully examine\nfine-grained evidences or elements in the solving procedure. Based\non this idea, we design a pre-training task of dual-decoder correc-\ntion, which mimics the problem solving process by human [9]: a\ndouble check is usually required to verify the final solution. In this\npre-training course, we first utilize the two decoders to generate\nthe masked words or symbols, and then train them to detect and\ncorrect the generated errors from each other.\nDual-Decoder Solution Checking (SC) . Specifically, given the\nsolution text ğ‘‘ = {ğ‘¡1,ğ‘¡2,Â·Â·Â· ,ğ‘¡ğ‘™}, we first mask a proportion of to-\nkens following the same strategy in Section 3.3.1 to obtain the\nmasked text eğ‘¥. Then, we utilize the ğ‘ˆ-decoder and ğº-decoder to\nproduce two recovered results based on their own generation prob-\nabilities of the masked tokens, denoted as eğ‘‘ğ‘ˆ and eğ‘‘ğº. Next, we\nemploy the two decoders to detect and correct the generated texts\nfrom each other, where the eğ‘‘ğº and eğ‘‘ğ‘ˆ will be examined and cor-\nrected by the ğ‘ˆ-decoder and ğº-decoder, respectively. In this way,\nthe incorrect generations is expected to be corrected in a dual way.\nSuch a pre-training process can be formulated as:\nğ¿ğ‘ˆğ‘†ğ¶ =\nâˆ‘ï¸\nğ‘–\nâˆ’log ğ‘(ğ‘¡ğ‘–|eğ‘‘ğº; Î˜ğ¸,Î˜ğ‘ˆ), (7)\nğ¿ğºğ‘†ğ¶ =\nâˆ‘ï¸\nğ‘–\nâˆ’log ğ‘(ğ‘¡ğ‘–|ğ‘¡<ğ‘–,eğ‘‘ğ‘ˆ; Î˜ğ¸,Î˜ğº), (8)\nwhere ğ¿ğ‘ˆğ‘†ğ¶ and ğ¿ğºğ‘†ğ¶ denote the self-correction losses for the ğ‘ˆ-\ndecoder and ğº-decoder, respectively. These tasks can enhance the\nself-diagnosis capacity of the PLM by ensuring that all key points\nleading to the solution are correct, which is especially useful for\ncomplex mathematical problems requiring a long solving procedure.\nBesides, we also incorporate the MLM and DAE tasks to prevent\nthe forgetting of previous courses, and the learning objective of the\nsolution checking course can be formulated as:\nğ¿ğ‘†ğ¶ = ğ¿ğ‘€ğ¿ğ‘€ +ğ¿ğ·ğ´ğ¸ +ğ¿ğ‘ˆğ‘†ğ¶ +ğ¿ğºğ‘†ğ¶. (9)\nTo enhance the capacity of detecting incorrectly generated to-\nkens, we enforce it to recover the entire sequence rather than the\ncorrupted part only. In this way, the model is required to distinguish\nwhether a token is corrupted, and then only corrects the corrupted\nones. To further reduce the influence of errors, the two decoders\nalso perform self-checking on the generated sentences. Note that\na problem might correspond to multiple solutions, here we only\nconsider the sample answer in the solution text as the ground truth.\nIn this setting, all the inconsistent generations are considered as\nincorrect predictions. The rationale is that we only mask a small\nproportion of tokens so that the recovered solution should remain\nthe same as original. We leave a more principled method to examine\nthe correctness of the generated solutions in future work.\nKDD â€™22, August 14â€“18, 2022, Washington, DC, USA Zhao, et al.\n3.4 Learning and Discussion\nIn this part, we present the learning and discussion of our model.\n3.4.1 Learning. We adopt the CPT model [27] as the base archi-\ntecture and utilize its pre-trained parameters to initialize the pa-\nrameters of our model. We also randomly initialize the embeddings\nof math symbols that are out of the vocabulary of CPT. During\npre-training, we design three courses to gradually learn the mathe-\nmatical knowledge and logic from the mathematical corpus. First,\nwe train our model on the pre-training tasks that consist of MLM\nand DAE tasks using Eq. (1) and Eq. (2) to learn basic mathemati-\ncal semantics, where we incorporate the position-biased masking\nstrategy to improve the adaptation to mathematical texts. Then, we\nadopt the logic-based course that recovers shuffled sentences and\nformulas using Eq. (6) to improve the understanding of mathemati-\ncal logic. Next, we employ the two decoders of our model to detect\nand correct the incorrect generations from each other using Eq. (9).\nNote that we also combine the MLM and DAE tasks in the latter\ntwo courses as regularizers to avoid the forgetting of basic mathe-\nmatical semantics. Finally, for downstream tasks, we can fine-tune\nour model with task-specific datasets or learning objectives, and\nutilize the ğ‘ˆ-decoder or ğº-decoder for making the predictions.\n3.4.2 Discussion. The focus of this paper is not to simply enhance\nthe mathematical capacity by increasing the number of model pa-\nrameters. Instead, we keep a base-sized PLM (about 121M param-\neters) for JiuZhang, and focus on designing more effective pre-\ntraining tasks that well adapt to Chinese mathematical texts. The\nmajor novelty of JiuZhang lies in the curriculum pre-training ap-\nproach consisting of the basic course (masked token prediction)\nand the advanced courses (mathematical logic recovering and so-\nlution checking). Although existing methods [ 7, 22] attempt to\nadapt PLMs to math domain, they still rely on pre-training tasks\nas BERT to learn semantic representations for mathematical text.\nAs a comparison, our proposed pre-training tasks are more capable\nof learning the essential knowledge and logic for mathematical\nproblem understanding, including position-biased masking, mathe-\nmatical logic recovering and solution checking. Furthermore, we\ndesign a curriculum pre-training approach to scheduling the three\nkinds of tasks in a basic-to-advanced manner. Similar to the actual\nstudent learning process [ 16], such a pre-training approach can\nhelp our model gradually understand and acquire mathematical\nknowledge and logic.\n4 EXPERIMENTS\nTo verify the effectiveness of our approach, we conduct experiments\non nine tasks from high-school math education.\n4.1 Experimental Setup\nPre-training Corpus. Our pre-training corpus is collected from\nZhixue, a widely-used Chinese online education website, consisting\nof 1,276,952 high school math problems. Each question is associated\nwith the problem type, problem statement and solution text. We\nfirst convert the formulas and math symbols into a unified LaTex\nformat. Then, we preprocess these texts by simple regular expres-\nsions (removing extra spaces or unnecessary special symbols), and\nTable 1: Statistics of the datasets for nine evaluation tasks.\nType Task Train Dev Test\nClassification\nKPC 8,721 991 1,985\nQRC 10,000 2,000 4,000\nQAM 14,000 2,000 4,000\nRetrieval SQR 249,902 11,462 56,329\nQAR 35,000 10,000 20,000\nQA tasks MCQ 22,000 3,982 7,466\nBFQ 14,795 1,786 1,778\nGeneration CAG 14,795 1,786 1,778\nBAG 16,000 1,976 1,977\ntokenize the Chinese sentences with the toolkit of Jieba3 for whole\nword masking. More details can be found in Appendix A.\nEvaluation Tasks. We conduct nine evaluation tasks based on the\ncollected math corpus, corresponding to different mathematical\nunderstanding and generation capacities, including three classi-\nfication tasks, two retrieval tasks, two question answering tasks\nand two analysis generation tasks. We split each task dataset into\ntraining/development/test sets. Note that the training set is used\nfor fine-tuning. The statistics of these tasks are shown in Table 1.\nâ€¢Knowledge Point Classification (KPC) is a multi-label clas-\nsification task. Given a mathematical problem, the task of KPC is to\nclassify it into a set of pre-defined knowledge points. In this task,\nthe knowledge points are created and annotated by professional\nteachers, and there are 387 knowledge points in total.\nâ€¢Question Relation Classification (QRC) is a six-class classi-\nfication task. Given two math questions, the goal is to predict their\nrelation from the given relation set: { equivalent, similar, problem\nvariant, conditional variant, situation variant, irrelevant }.\nâ€¢Question-Answer Matching (QAM) is a binary classification\ntask to predict if an answer is able to solve a given question. We\nsample an answer for each question as the negative example.\nâ€¢Similar Question Retrieval (SQR) is a ranking task to re-\ntrieve similar questions. Given a question, it aims to rank the can-\ndidate questions based on their similarities with the query.\nâ€¢Question-Answer Retrieval (QAR) is a ranking task to re-\ntrieve proper answers. Given a question, this task aims to rank\ncandidate answers based on their similarities with the question.\nâ€¢Multiple-Choice Question Answering (MCQ) is to select\nthe true answer from the multiple choices. The evaluation set is\nconstructed with high-school multi-choice mathematical problems.\nâ€¢Blank-Filling Question Answering (BFQ) is to generate the\nanswer to fill the blank within the problem statement. We adopt\nhigh-school blank-filling mathematical problems for evaluation.\nâ€¢Multiple-Choice Analysis Generation (CAG) aims to gen-\nerate the answer and the proper analysis that explains the reasoning\nprocess given the question and its choices.\nâ€¢Blank-Filling Analysis Generation (BAG) aims to generate\nthe answer and the proper analysis that explains the reasoning\nprocess for the blank-filling question.\nSince these tasks are in different difficulty levels, we divide them\ninto two groups: group ğ´contains KPC, QAM, QRC, SQR and QAR,\n3https://github.com/fxsjy/jieba\nJiuZhang: A Chinese Pre-trained Language Model for\nMathematical Problem Understanding KDD â€™22, August 14â€“18, 2022, Washington, DC, USA\nTable 2: Main results on three basic classification tasks and two math text retrieval tasks. The best and the second-best methods\nare denoted in bold and underlined fonts respectively.\nTasks KPC QAM QRC SQR QAR\nMetrics Accuracy F1-macro Accuracy F1-macro Accuracy F1-macro HR@3 NDCG@3 HR@3 NDCG@3\nTextCNN 47.4 26.8 90.4 90.4 73.3 52.9 0.332 0.338 0.637 0.486\nTextRCNN 55.3 38.8 88.3 88.3 79.6 59.0 0.333 0.324 0.653 0.499\nBERT 59.6 34.9 96.9 96.9 82.7 63.4 0.614 0.618 0.683 0.519\nRoBERTa-wwm 61.0 37.0 97.4 97.4 84.2 65.2 0.618 0.622 0.707 0.544\nMengzi 56.6 29.5 97.3 97.3 81.7 62.8 0.625 0.625 0.672 0.520\nBART 62.7 41.9 97.7 97.6 82.0 63.0 0.610 0.610 0.688 0.530\nCPT 66.2 48.4 97.8 97.8 82.8 63.4 0.613 0.613 0.702 0.544\nDAPT-BERT 68.7 46.5 98.2 98.1 86.5 68.5 0.650 0.653 0.718 0.548\nMathBert 68.9 47.1 98.9 98.9 85.3 69.8 0.652 0.656 0.705 0.545\nCOMUS 71.0 63.3 99.0 99.0 88.0 73.3 0.661 0.664 0.724 0.561\nDAPT-CPT 72.0 58.0 99.1 99.1 88.8 76.7 0.664 0.668 0.723 0.556\nOurs 73.3 59.4 99.4 99.4 89.4 79.2 0.667 0.672 0.724 0.556\nTable 3: Main results on two question answering tasks and two analysis generation tasks. The best and the second-best methods\nare denoted in bold and underlined fonts respectively.\nTasks MCQ BFQ CAG BAG\nMetrics Accuracy Accuracy BLEU-4 ROUGE-2 ROUGE-L Accuracy BLEU-4 ROUGE-2 ROUGE-L Accuracy\nSeq2Seq 37.61 44.32 39.91 47.79 67.88 42.63 39.86 48.15 68.06 39.91\nTransformer 35.33 46.57 41.39 48.50 67.09 41.02 41.91 48.80 67.76 45.95\nBART 36.15 46.82 39.26 49.37 67.73 42.57 37.16 47.18 66.73 32.61\nCPT 37.90 46.31 39.56 50.07 68.20 43.34 37.20 47.99 67.25 34.26\nDAPT-CPT 46.26 53.41 40.46 50.84 68.87 46.52 38.39 49.28 68.04 41.82\nOurs 47.73 54.60 40.81 51.09 69.45 48.51 39.28 49.62 68.37 44.03\nand group ğµcontains MCQ, BFQ, CAG and BAG. Overall, the tasks\nin group ğµ (question answering and generation tasks) are more\ndifficult than those in group ğ´(classification and retrieval tasks).\nEvaluation Metrics. For classification tasks (KPC, QAM and QRC),\nwe adopt Accuracy and F1-macro as the evaluation metrics. For the\nretrieval tasks (SQR and QAR), we employ top-ğ‘˜ Hit Ratio (HR@ğ‘˜)\nand top-ğ‘˜ Normalized Discounted Cumulative Gain (NDCG@ğ‘˜).\nSince the number of candidates is usually between 6 and 15, we\nadopt HR@3 and NDCG@3 for evaluation. For question answering\ntasks (MCQ, BFQ), we adopt Accuracy for evaluation. For the BFQ\ntask, we match the numerical value of the generated answer with\nthe golden answer to avoid the format mismatch (e.g., 1 and 1.0).\nFor generation tasks (CAG, BAG), we use BLEU-4 [21], ROUGE-2\nand ROUGE-L [19] to evaluate the quality of the generated analysis.\nWe also adopt Accuracy to evaluate the generated answers.\nBaseline Methods. We compare our proposed approach with the\nfollowing baseline methods, including non-pre-training methods,\npre-training methods and continual pre-training methods:\nâ€¢TextCNN [12] is a classic text representation model for text\nclassification, using CNN on top of word vectors.\nâ€¢TextRCNN [14] adopts RNN and CNN for classification tasks.\nâ€¢Seq2Seq [1] consists of a GRU encoder and a GRU decoder\nwith the attention mechanism for text generation tasks.\nâ€¢Transformer [33] adopts the multi-head self-attention based\nencoder-decoder framework for generation tasks.\nâ€¢BERT-Base [5] is pre-trained using the MLM task. We use the\nBERT-Base-Chinese checkpoint from Huggingface4.\nâ€¢BART-Base[17] is pre-trained by the denoising auto-encoding\ntask. We utilize BART-Base-Chinese released by Shaoet al. [27].\nâ€¢RoBERTa-wwm [4] introduces the whole word masking strat-\negy to mask whole words instead of individual Chinese characters.\nâ€¢MathBERT [22] continually pre-trains BERT on a large-scale\nmathematical corpus, which revises the self-attention mechanism\nfor encoding formulas and adopts MLM and formula-related tasks.\nâ€¢CPT [27] is a Chinese pre-trained language model consisting\nof a shared encoder, an understanding and a generation decoder.\nâ€¢Mengzi [38] is a family of Chinese pre-trained language mod-\nels. We adopt Mengzi-BERT for understanding tasks.\nâ€¢DAPT-BERT[8] conducts continual pre-training for BERT on\ndomain-related corpus. We use the same pre-training mathematical\ncorpus as our PLM, with the masked language model task.\nâ€¢DAPT-CPTfurther pre-trains CPT on our collected pre-training\ncorpus with the MLM and DAE tasks.\nâ€¢COMUS [7] adopts syntax-aware memory networks to cap-\nture structural semantics of math problems, and devises three pre-\ntraining tasks to further enhance the representations.\n4https://huggingface.co/bert-base-chinese\nKDD â€™22, August 14â€“18, 2022, Washington, DC, USA Zhao, et al.\n4.2 Main Results\nThe results of the comparison methods on the tasks of groupğ´and\nğµare shown in Table 2 and Table 3, respectively. In general, non-pre-\ntraining methods perform worse than pre-training methods. The\nreason is that pre-training methods have captured more semantic\nknowledge from large-scale pre-training corpus.\nAmong pre-training based baselines, CPT performs better than\nBERT, RoBERTa-wwm and BART mostly. A major reason is that\nCPT adopts an unbalanced architecture, consisting of a deep shared\nencoder and two shallow decoders for understanding and genera-\ntion, respectively. Such a framework enables CPT to better learn\ngeneral knowledge from multi-task pre-training and easily adapt\nto various tasks. As an exceptional case, BART outperforms CPT in\nCAG task, since its pre-training task is for text generation.\nBesides, we can see that continual pre-training methods ( i.e.,\nMathBert, DAPT-BERT, COMUS and DAPT-CPT) outperform other\nbaselines consistently. It is because mathematical texts contain\nimportant terminology and symbols, which canâ€™t be learned from\nthe general corpus. As shown in [8], continual pre-training is an\nimportant technique that adapts PLM to specific domains or tasks,\nwhich is also useful to improve the understanding of mathematical\nproblems. However, MathBERT does not perform well among the\nfour methods. It is likely because that its devised tasks focus on\nimproving the understanding of a single math formula, while the\ntest data from our corpus contains more formulas than the expected\nsetting of MathBERT. In contrast, DAPT-CPT performs better than\nother baselines in most tasks. As it adopts both the MLM and DAE\ntasks for continual pre-training, such a way helps DAPT-CPT better\nlearn mathematical knowledge for math-related tasks.\nFinally, on most tasks, our model performs consistently better\nthan baselines. With the specially designed curriculum pre-training\nstrategy, our model can gradually capture mathematical knowledge\nand logic from basic and advanced pre-training courses. Such a\ncurriculum pre-training strategy can improve the capacity of our\nmodel to solve various math-related tasks. However, Transformer\noutperforms our method in the BLEU-4 metric. After inspecting the\nresults, we find Transformer (without pre-training) generates more\nuninformative consecutive segments that match the reference text,\nbut it canâ€™t capture important keywords or short phrases (lower\nROUGE-2 and ROUGE-L).\n4.3 Ablation Study\nAs the major technical contribution, we design a curriculum pre-\ntraining approach consisting of three courses with different learning\npurposes. In this part, we examine the effect of each course on the\nperformance of our model, by removing each course individually.\nBesides, as we schedule these courses in a basic-to-advanced order,\nit is important to examine how the pre-training order affects the\nfinal performance. For this purpose, we prepare two variants: (1)\nintegrating the tasks within the above courses via a multi-task\nlearning manner and (2) reversing the order of these courses.\nIn Figure 2, we can see that removing any course or changing\nthe curriculum order will mostly lead to a performance decrease\non downstream tasks. It indicates the effectiveness of the current\ndesign for our curriculum pre-training approach. Among these vari-\nants, the model performance significantly drops when we remove\nTable 4: Online ğ´/ğµ test of our approach on the relevant\nproblem recommendation task.\nJiuZhang Wins Baseline Wins\nRatio 84.81 % 15.19%\nthe MLR course (i.e., mathematical logic recovering), which implies\nthat it is important to consider mathematical logic in solving math\ntasks. Besides, we can see that reversing the order of curriculum\npre-training can outperform our approach in BFQ task but results\nin a performance drop in other tasks. It indicates that the advanced-\nto-basic order might be useful to predict the masked tokens. A\npossible reason is that it is more suited to the setting of BFQ task\nthat predicts the blank of the given problems, but not other tasks.\n4.4 Online ğ´/ğµTest\nTo further examine the effectiveness of our approach, we conduct\nthe online ğ´/ğµtest on Zhixue5, a personalized learning app that\nassists teachers to better teach students. It has over 45 million users\nin China. Specially, we conduct the test through the function of\nrelevant problem recommendation (RPR) supported by this app. This\nfunction aims to recommendğ‘˜relevant problems (ğ‘˜ = 3 in this test)\ngiven a target problem. We obtain a small population of requests\nfor comparing our approach (fine-tuned with the training data\nprovided by this app) and the original method deployed on the\napp (a hybrid approach combining hand-crafted features and deep\nmodels). Given a request, a user will be asked to select one better\ngroup of recommendation results based on her/his preference.\nWe report the average winning ratios of the two methods in\nTable 4. As we can see, our model performs better than the online\ndeployed method. The major reason is that our model can produce\nbetter representations for mathematical problems (we use the sim-\nple inner product as the recommendation scores for our method),\nso that it can better model the similar relation between problems.\n5 CONCLUSION\nIn this paper, we proposed JiuZhang, a Chinese pre-trained language\nmodel for mathematical problem understanding. We devised a set of\npre-training tasks to effectively capture mathematical knowledge\nand logic, and organized these tasks as the training curriculum\nto gradually learn the model parameters, from basic to advanced\ncourses. In such a curriculum pre-training approach, we first learned\nbasic semantics about math symbols and established their semantic\nrelatedness with text words, then performed logic-based reasoning\nto recover shuffled sentences or formulas in the solution text, and\nfinally conducted the difficult pre-training task of solution checking.\nExperimental results have shown that our approach outperforms\nseveral competitive baselines on nine math-related tasks, including\nbasic classification tasks, text retrieval tasks, question answering\ntasks and analysis generation tasks.\nIn future work, we consider incorporating more knowledge re-\nsources from the math domain (e.g., the explanation of knowledge\npoints from math textbooks). Besides, we will also explore the\nscaling limit of our PLMs by increasing the parameter scale.\n5https://www.zhixue.com/\nJiuZhang: A Chinese Pre-trained Language Model for\nMathematical Problem Understanding KDD â€™22, August 14â€“18, 2022, Washington, DC, USA\nÂ¬ MTP\nÂ¬ MLR\nÂ¬ SC\nReverse\nMulti-Task\nOurs\n71.5\n71.9\n72.3\n72.7\n73.1\n73.5 KPC\nÂ¬ MTP\nÂ¬ MLR\nÂ¬ SC\nReverse\nMulti-Task\nOurs\n88.0\n88.3\n88.6\n88.9\n89.2\n89.5 QRC\nÂ¬ MTP\nÂ¬ MLR\nÂ¬ SC\nReverse\nMulti-Task\nOurs\n44.5\n45.3\n46.1\n46.9\n47.7\n48.5 MCQ\nÂ¬ MTP\nÂ¬ MLR\nÂ¬ SC\nReverse\nMulti-Task\nOurs\n52.0\n52.8\n53.6\n54.4\n55.2\n56.0 BFQ\nFigure 2: Ablation study of our approach on four tasks. â€œ Â¬â€ indicates that the corresponding course is removed in the curricu-\nlum pre-training stage, while the rest courses are kept. We abbreviate the courses of masked token prediction, mathematical\nlogic recovering and solution checking as MTP, MLR and SC, respectively.\nACKNOWLEDGEMENT\nThis work was partially supported by Beijing Natural Science Foun-\ndation under Grant No. 4222027, and National Natural Science Foun-\ndation of China under Grant No. 61872369, Beijing Outstanding\nYoung Scientist Program under Grant No. BJJWZYJH012019100020098,\nand the Outstanding Innovative Talents Cultivation Funded Pro-\ngrams 2021. This work is also partially supported by Beijing Acad-\nemy of Artificial Intelligence (BAAI). Xin Zhao is the corresponding\nauthor.\nREFERENCES\n[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine\nTranslation by Jointly Learning to Align and Translate. In ICLR 2015 .\n[2] Yoshua Bengio, JÃ©rÃ´me Louradour, Ronan Collobert, and Jason Weston. 2009.\nCurriculum Learning. In ICML (ICML â€™09) .\n[3] Ting-Rui Chiang and Yun-Nung Chen. 2019. Semantically-Aligned Equation\nGeneration for Solving and Reasoning Math Word Problems. In NAACL.\n[4] Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, and Ziqing Yang. 2021. Pre-\ntraining with whole word masking for chinese bert. TASLP (2021).\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nNAACL.\n[6] Charles R Fletcher. 1985. Understanding and solving arithmetic word problems:\nA computer simulation. Behavior Research Methods, Instruments, & Computers\n(1985).\n[7] Zheng Gong, Kun Zhou, Xin Zhao, Jing Sha, Shijin Wang, and Ji-Rong Wen. 2022.\nContinual Pre-training of Language Models for Math Problem Understanding\nwith Syntax-Aware Memory Network. In ACL. 5923â€“5933.\n[8] Suchin Gururangan, Ana MarasoviÄ‡, Swabha Swayamdipta, Kyle Lo, Iz Beltagy,\nDoug Downey, and Noah A. Smith. 2020. Donâ€™t Stop Pretraining: Adapt Language\nModels to Domains and Tasks. In ACL.\n[9] Tzu-Hua Huang, Yuan-Chen Liu, and Hsiu-Chen Chang. 2012. Learning achieve-\nment in solving word-based mathematical questions through a computer-assisted\nlearning system. Journal of Educational Technology & Society (2012).\n[10] Chen Jia, Yuefeng Shi, Qinrong Yang, and Yue Zhang. 2020. Entity Enhanced\nBERT Pre-training for Chinese NER. In EMNLP.\n[11] Ronnie Karsenty. 2014. Mathematical Ability . Springer Netherlands, Dordrecht.\n[12] Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification. In\nEMNLP.\n[13] Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. 2014. Learning\nto automatically solve algebra word problems. In ACL.\n[14] Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015. Recurrent convolutional\nneural networks for text classification. In AAAI.\n[15] Yihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan, Bing Tian Dai, Yan Wang,\nDongxiang Zhang, and Ee-Peng Lim. 2021. MWPToolkit: An Open-Source Frame-\nwork for Deep Learning-Based Math Word Problem Solvers. arXiv preprint\narXiv:2109.00799 (2021).\n[16] Diana Laurillard. 1979. The processes of student learning. Higher education 8, 4\n(1979), 395â€“409.\n[17] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman\nMohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART:\nDenoising Sequence-to-Sequence Pre-training for Natural Language Generation,\nTranslation, and Comprehension. In ACL.\n[18] Jierui Li, Lei Wang, Jipeng Zhang, Yan Wang, Bing Tian Dai, and Dongxiang\nZhang. 2019. Modeling intra-relation in math word problems with different\nfunctional multi-head attentions. In ACL.\n[19] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries.\nIn ACL 2004.\n[20] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization.\nIn ICLR.\n[21] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a\nMethod for Automatic Evaluation of Machine Translation. In ACL.\n[22] Shuai Peng, Ke Yuan, Liangcai Gao, and Zhi Tang. 2021. MathBERT: A Pre-Trained\nModel for Mathematical Formula Understanding. arXiv preprint arXiv:2105.00377\n(2021).\n[23] Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor\nBabuschkin, and Ilya Sutskever. 2022. Formal Mathematics Statement Curriculum\nLearning. arXiv:2202.01344 [cs.LG]\n[24] Stanislas Polu and Ilya Sutskever. 2020. Generative language modeling for auto-\nmated theorem proving. arXiv preprint arXiv:2009.03393 (2020).\n[25] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the Limits\nof Transfer Learning with a Unified Text-to-Text Transformer.JMLR (2020).\n[26] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A Primer in BERTology:\nWhat We Know About How BERT Works. TACL (2020).\n[27] Yunfan Shao, Zhichao Geng, Yitao Liu, Junqi Dai, Fei Yang, Li Zhe, Hujun Bao, and\nXipeng Qiu. 2021. CPT: A Pre-Trained Unbalanced Transformer for Both Chinese\nLanguage Understanding and Generation. arXiv preprint arXiv:2109.05729 (2021).\n[28] Jia Tracy Shen, Michiharu Yamashita, Ethan Prihar, Neil Heffernan, Xintao Wu,\nBen Graff, and Dongwon Lee. 2021. MathBERT: A Pre-trained Language Model\nfor General NLP Tasks in Mathematics Education.arXiv preprint arXiv:2106.07340\n(2021).\n[29] Shuming Shi, Yuehui Wang, Chin-Yew Lin, Xiaojiang Liu, and Yong Rui. 2015.\nAutomatically solving number word problems by semantic parsing and reasoning.\nIn EMNLP. 1132â€“1142.\n[30] Yujin Song and Xiaoyu Chen. 2021. Searching for Mathematical Formulas Based\non Graph Representation Learning. In CICM.\n[31] Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang,\nJiaxiang Liu, Xuyi Chen, Yanbin Zhao, Yuxiang Lu, Weixin Liu, Zhihua Wu,\nWeibao Gong, Jianzhong Liang, Zhizhou Shang, Peng Sun, Wei Liu, Xuan Ouyang,\nDianhai Yu, Hao Tian, Hua Wu, and Haifeng Wang. 2021. ERNIE 3.0: Large-scale\nKnowledge Enhanced Pre-training for Language Understanding and Generation.\nCoRR (2021).\n[32] Zijun Sun, Xiaoya Li, Xiaofei Sun, Yuxian Meng, Xiang Ao, Qing He, Fei Wu,\nand Jiwei Li. 2021. ChineseBERT: Chinese Pretraining Enhanced by Glyph and\nPinyin Information. In ACL.\n[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information processing systems . 5998â€“6008.\n[34] Yan Wang, Xiaojiang Liu, and Shuming Shi. 2017. Deep neural solver for math\nword problems. In EMNLP.\n[35] Richard Zanibbi and Dorothea Blostein. 2012. Recognition and retrieval of\nmathematical expressions. IJDAR 15, 4 (2012), 331â€“357.\n[36] Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, Zhiwei Wang, Xin Jiang,\nZhenZhang Yang, Kaisheng Wang, Xiaoda Zhang, et al. 2021. PanGu-ğ›¼: Large-\nscale Autoregressive Pretrained Chinese Language Models with Auto-parallel\nComputation. arXiv preprint arXiv:2104.12369 (2021).\n[37] Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin,\nYusheng Su, Haozhe Ji, Jian Guan, et al . 2021. CPM: A large-scale generative\nChinese pre-trained language model. AI Open 2 (2021), 93â€“99.\n[38] Zhuosheng Zhang, Hanqing Zhang, Keming Chen, Yuhang Guo, Jingyun Hua,\nYulong Wang, and Ming Zhou. 2021. Mengzi: Towards Lightweight yet Ingenious\nPre-trained Models for Chinese. arXiv preprint arXiv:2110.06696 (2021).\nKDD â€™22, August 14â€“18, 2022, Washington, DC, USA Zhao, et al.\nA PRE-TRAINING DATA COLLECTION\nThe dataset supplied by iFLYTEK Co., Ltd. was collected from the\nChinese education website Zhixue. The raw data is organized in the\nformat of HTML webpage. To extract the useful information, we\nparsed the HTML webpage with regular regressions, to obtain the\nproblem statements and solution text. However, the contained math\nsymbols and formulas within crawled mathematical problems arenâ€™t\nin a unified format (e.g., ğ‘¥ âˆ—âˆ—ğ‘¦and ğ‘¥ğ‘¦). We invited professionals\nand designed standards to unify the math symbols and operators\nwithin the above data. Next, we filtered low-quality examples with\nunidentifiable information or overlength text. We also removed the\noverlapping data from the corpus with the test sets of downstream\ntasks for a fair comparison. Finally, we obtained 1,276,952 examples\nfor pre-training (see a sample preprocessed mathematical text in\nTable 5).\nTable 5: A sample mathematical text after preprocessing.\nProblem Statement\nè®¾0 <\\\\alpha <\\\\pi, ä¸”\\\\sin \\\\frac { \\\\alpha } { 2 } = \\\\frac { \\\\sqrt { 3 } }\n{ 3 } . åˆ™\\\\sin \\\\alpha = \\\\LongUnderLine .\nSolution Text\nè§£0 <\\\\alpha <\\\\pi, ä¸”\\\\sin \\\\frac { \\\\alpha } { 2 } = \\\\frac {\\\\sqrt { 3 } }\n{ 3 } , å¯å¾— \\\\cos \\\\frac { \\\\alpha } { 2 } = \\\\sqrt { 1 - (\\\\frac { \\\\sqrt { 3 } }\n{ 3 } ) Ë† {2 } } = \\\\frac{ \\\\sqrt { 6 } } { 3 } . \\\\sin \\\\alpha = 2 \\\\sin \\\\frac {\n\\\\alpha } { 2 } \\\\cos \\\\frac { \\\\alpha } { 2 } = 2 \\\\times \\\\frac {\\\\sqrt { 3 } }\n{ 3 } \\\\ times \\\\frac { \\\\sqrt { 6 } } { 3 } = \\\\frac { 2 \\\\sqrt { 2 } }{ 3 } æ•…ç­”\næ¡ˆä¸º \\\\frac { 2 \\\\sqrt { 2 } } { 3 } .\nB IMPLEMENTATION DETAILS.\nIn our approach, the numbers of Transformer layers in the shared\nencoder, ğ‘ˆ-decoder and ğº-decoder are set to 10, 2, and 2, respec-\ntively. For each course, we pre-train the parameters with a batch\nsize of 256 for 100,000 steps. The max length of input sequences is\nset as 512. We use AdamW [20] optimization with the learning rate\nof 3eâˆ’5, and warmup the learning rate for the first 5,000 steps then\ndecay the weight with a ratio of 0.01. During fine-tuning, we use\nAdamW with the same setting as pre-training. For classification and\nretrieval tasks, we utilize the shared encoder and theğ‘ˆ-decoder for\nclassification. We set the initial learning rate as 3eâˆ’5 and batch size\nas 32. For QA and generation tasks, we utilize the shared encoder\nwith the ğº-decoder to accomplish them in a seq2seq fashion. Texts\nare tokenized with a maximum length of 512 and 128 for input and\noutput, respectively. We set the initial learning rate as 5e âˆ’5 and\nbatch size as 64. We perform the curriculum pre-training on 8 RTX\n3090 24G GPUs. During fine-tuning, we construct the model inputs\nof all downstream tasks as:\nQRC and SQR: [CLS] ğ‘1 [SEP] ğ‘2 [SEP].\nQAM: [CLS] ğ‘[SEP] ğ‘[SEP].\nQAR: [CLS] ğ‘[SEP] and [CLS] ğ‘[SEP].\nKPC, MCQ, BFQ, CAG and BAG: [CLS] ğ‘[SEP].\nFor baselines, we implement them based on HuggingFace Trans-\nformers6. We report the parameter settings of baseline models used\nthroughout the experiments in Table 6, and present the procedure\nof our curriculum pre-training in Algorithm 1.\n6https://huggingface.co/transformers/\nTable 6: Parameter settings of the baselines.\nModels Settings\nClassification and Retrieval Task\nTextCNN & TextRCNN\nAdamW, learning_rate=0.0001\nwarmup_ratio=0.05\nbatch_size=32\nBERT & RoBERTa-wwm & Mengzi\n& BART & CPT & MathBERT\n& DAPT-BERT & DAPT-CPT\nAdamW, learning_rate=0.00003\nwarmup_ratio=0.05\nbatch_size=32\nQA and Generation Task\nSeq2Seq & Transformer\nAdamW, learning_rate=0.001\nwarmup_ratio=0.05\nbatch_size=64\nBART & CPT & DAPT-CPT\nAdamW, learning_rate=0.00005\nwarmup_ratio=0.1\nbatch_size=64\nAlgorithm 1: The curriculum pre-training algorithm.\nInput : mathematical text corpus ğ‘‹ = {(ğ‘,ğ‘‘)}\nParameter :The parameters of the encoder Î˜ğ¸, ğ‘ˆ-decoder Î˜ğ‘ˆ,\nğº-decoder Î˜ğº, steps of the three courses ğ‘€1, ğ‘€2, ğ‘€3\n// Basic Course: Masked Token Prediction\n1 for ğ‘¡ â†1 to ğ‘€1 do\n2 Randomly sample half of examples from the batch as ğ‘‹ğ‘ˆ\nğ‘¡ and\nthe other half as ğ‘‹ğº\nğ‘¡ , and perform masking;\n3 Pre-train Î˜ğ¸ and Î˜ğ‘ˆ on ğ‘‹ğ‘ˆ\nğ‘¡ by MLM task using Eq. (1);\n4 Pre-train Î˜ğ¸ and Î˜ğº on ğ‘‹ğº\nğ‘¡ by DAE task using Eq. (2);\n5 end\n// Advanced Course: Mathematical Logic\nRecovering\n6 for ğ‘¡ â†1 to ğ‘€2 do\n7 Pre-train Î˜ğ¸, Î˜ğ‘ˆ and Î˜ğº by MLM and DAE tasks as lines 2-5;\n8 Sample half of examples from the batch as ğ‘‹ğ‘†\nğ‘¡ and the others as\nğ‘‹ğ¹\nğ‘¡ , and shuffle the sentences and formulas, respectively;\n9 Pre-train Î˜ğ¸ and Î˜ğº on ğ‘‹ğ‘†\nğ‘¡ by SSR task using Eq. (4);\n10 Pre-train Î˜ğ¸ and Î˜ğº on ğ‘‹ğ¹\nğ‘¡ by SFR task using Eq. (5);\n11 end\n// Advanced Course: Solution Checking\n12 for ğ‘ ğ‘¡ğ‘’ğ‘ â†1 to ğ‘€3 do\n13 Pre-train Î˜ğ¸, Î˜ğ‘ˆ and Î˜ğº by MLM and DAE tasks as lines 2-5,\nand obtain the generated text from ğ‘ˆ-decoder and ğº-decoder\nas fğ‘‘ğ‘ˆ and fğ‘‘ğº, respectively;\n14 Pre-train Î˜ğ¸ and Î˜ğ‘ˆ on fğ‘‘ğ‘ˆ by USC loss using Eq. (7);\n15 Pre-train Î˜ğ¸ and Î˜ğº on fğ‘‘ğº by GSC loss using Eq. (8);\n16 end\nC TUNING THE NUMBER OF PRE-TRAINING\nSTEPS\nFor pre-trained language models, it is important to conduct a suf-\nficient number of pre-training steps to achieve the desired perfor-\nmance. Here, we examine how the model performance changes\nw.r.t. the number of pre-training steps, denoted by ğ‘‡, which can\nprovide important practical guidelines for reproducible implemen-\ntation. Besides, since we have two different decoders, we need to\nschedule the number of pre-training steps assigned to each decoder.\nJiuZhang: A Chinese Pre-trained Language Model for\nMathematical Problem Understanding KDD â€™22, August 14â€“18, 2022, Washington, DC, USA\nTable 7: Case study on the MCQ task.\nMath Problem\nå·²çŸ¥ğ‘ ğ‘–ğ‘›(ğ›¼+ğœ‹\n6 )= 2\nâˆš\n5\n5 ,åˆ™cos(ğœ‹\n3 âˆ’ğ›¼)= ()\nIt is known that ğ‘ ğ‘–ğ‘›(ğ›¼+ğœ‹\n6 )= 2\nâˆš\n5\n5 ,then cos(ğœ‹\n3 âˆ’ğ›¼)= ()\nA.\nâˆš\n5\n5 B.âˆ’\nâˆš\n5\n5 C. 2\nâˆš\n5\n5 D.âˆ’2\nâˆš\n5\n5\nGround-Truth Analysis âˆµ cos(ğœ‹\n3 âˆ’ğ›¼)= cos [ğœ‹\n2 Ë˜(ğ›¼+ğœ‹\n6 )]= sin(ğ›¼+ğœ‹\n6 )= 2\nâˆš\n5\n5 . æ•…é€‰C\nâˆµ cos(ğœ‹\n3 âˆ’ğ›¼)= cos [ğœ‹\n2 Ë˜(ğ›¼+ğœ‹\n6 )]= sin(ğ›¼+ğœ‹\n6 )= 2\nâˆš\n5\n5 . Therefore, choose C\nDAPT-CPT cos(ğœ‹\n3 âˆ’ğ›¼)= sin ğ›¼ = âˆ’\nâˆš\n5 , åˆå› ä¸ºğ›¼ä¸ºé”è§’ï¼Œæ‰€ä»¥ğ›¼ = ğ‘\nğ‘ï¼Œæ•…é€‰C\ncos(ğœ‹\n3 âˆ’ğ›¼)= sin ğ›¼ = âˆ’\nâˆš\n5, and because ğ›¼ is an acute angle, we have ğ›¼ = ğ‘\nğ‘, so choose C\nOurs cos(ğœ‹\n3 âˆ’ğ›¼)= {cos[ğœ‹+ğœ‹\n3 âˆ’ğ›¼]= sin(ğ›¼+ğ‘›\n6 )= 2\nâˆš\n5\n5 æ•…é€‰C\ncos(ğœ‹\n3 âˆ’ğ›¼)= {cos[ğœ‹+ğœ‹\n3 âˆ’ğ›¼]= sin(ğ›¼+ğ‘›\n6 )= 2\nâˆš\n5\n5 . Therefore, choose C\nMath Problem\nè‹¥å¤æ•°ğ‘§ = 2ğ‘–\n1âˆ’ğ‘– ( { i } æ˜¯è™šæ•°å•ä½), åˆ™zçš„å…±è½­å¤æ•°ğ‘§ = ()\nIf the complex number ğ‘§ = 2ğ‘–\n1âˆ’ğ‘– ( { i } is the imaginary unit ), then the complex conjugate ğ‘§ = ()\nA. 1 +ğ‘– B. 1 âˆ’ğ‘–C. âˆ’1 +ğ‘– D. âˆ’1 âˆ’ğ‘–\nGround-Truth Analysis\nå¤æ•°ğ‘§ = 2ğ‘–\n1âˆ’ğ‘– = 2ğ‘–(1+ğ‘–)\n(1âˆ’ğ‘–)Â·(1+ğ‘–) = âˆ’1 +ğ‘–, åˆ™zçš„å…±è½­å¤æ•°ğ‘§ = âˆ’1 âˆ’ğ‘–. æ•…é€‰D\nComplex number ğ‘§ = 2ğ‘–\n1âˆ’ğ‘– = 2ğ‘–(1+ğ‘–)\n(1âˆ’ğ‘–)Â·(1+ğ‘–) = âˆ’1 +ğ‘– , then the complex conjugate ğ‘§ = âˆ’1 âˆ’ğ‘–. Therefore, choose D\nDAPT-CPT ğ‘§ = 2ğ‘–(1+ğ‘–)\n2 = ğ‘–+1, åˆ™z çš„å…±è½­å¤æ•°ğ‘§ = âˆ’1 âˆ’ğ‘–. æ•…é€‰D\nğ‘§ = 2ğ‘–(1+ğ‘–)\n2 = ğ‘–+1, then the complex conjugate ğ‘§ = âˆ’1 âˆ’ğ‘–. Therefore, choose D\nOurs\nå¤æ•°ğ‘§ = 2ğ‘–\n1âˆ’ğ‘– = (2ğ‘–)(1+ğ‘–)\n(1âˆ’ğ‘–)(1+ğ‘–) = 1 +ğ‘–, åˆ™å¤æ•°çš„å…±è½­å¤æ•°ğ‘§ = âˆ’1 âˆ’ğ’Š. æ•…é€‰D\nComplex number ğ‘§ = 2ğ‘–\n1âˆ’ğ‘– = (2ğ‘–)(1+ğ‘–)\n(1âˆ’ğ‘–)(1+ğ‘–) = 1 +ğ‘–then the complex conjugate ğ‘§ = âˆ’1 âˆ’ğ’Š. Therefore, choose D\n20k 40k 60k 80k 100k 120k 140k\n68\n70\n72\n74\n76\n78KPC (Accuracy)\nKPC\nQRC\n87\n88\n89\n90\n91\n92\nQRC (Accuracy)\n20k 40k 60k 80k 100k 120k 140k\n42\n44\n46\n48\n50\n52MCQ (Accuracy)\nMCQ\nBFQ\n52\n54\n56\n58\n60\nBFQ (Accuracy)\nFigure 3: Performance tuning w.r.t.the pre-training steps (ğ‘‡).\n0.0 0.25 0.5 0.75 1.0\n68\n70\n72\n74\n76\n78KPC (Accuracy)\nKPC\nQRC\n87\n88\n89\n90\n91\n92\nQRC (Accuracy)\n0.0 0.25 0.5 0.75 1.0\n42\n44\n46\n48\n50\n52MCQ (Accuracy)\nMCQ\nBFQ\n52\n54\n56\n58\n60\nBFQ (Accuracy)\nFigure 4: Performance tuning w.r.t.the training step ratio (ğ›¾).\nFor this purpose, we further incorporate a pre-training step ratio\n(denoted as ğ›¾) to distribute the training steps to the two decoders:\nğ›¾ Â·ğ‘‡ for ğ‘ˆ-decoder and (1 âˆ’ğ›¾)Â·ğ‘‡ for ğº-decoder. We conduct the\nperformance tuning experiments on the KPC, QRC, MCQ and BFQ\ntasks and show the performance curves of Accuracy in Figure 3\n(tuning ğ‘‡) and Figure 4 (tuning ğ›¾).\nFirst, in Figure 3, we can observe that our model achieves the best\nperformance with140ğ¾training steps for KPC and MCQ tasks while\nwith 80ğ¾ training steps for BFQ tasks. It indicates that different\ntasks may require different numbers of pre-training steps to achieve\nthe optimal performance: more training steps are required for KPC\nand MCQ tasks than the BFQ task.\nBesides, the ratio ğ›¾ controls the number of training steps sched-\nuled for each decoder. As we can see from Figure 4, the best per-\nformance for the tasks of KPC, MCQ and BFQ occur with extreme\ncases of ğ›¾ (either 0 or 1). The major reason is that the two decoders\nare designed for understanding and generation, respectively, and it\nis intuitive that we should train ğº-decoder (ğ‘ˆ-decoder) with more\nsteps for the generation (understanding) task. In practice, we have\nto deal with both generation and understanding tasks, by making a\ntrade-off between the two tasks via ğ›¾.\nD CASE STUDY\nIn this part, we display two sample analyses generated by our model\nfrom the MCQ task for a case study. We also show the analyses\ngenerated by the best baseline DAPT-CPT as a comparison. These\ncases are shown in Table 7.\nFirst, we can see that DAPT-CPT has made mistakes such as\ncalculation error and improper deduction. These minor mistakes\nare intolerant in solving mathematical problems and may lead to\nwrong predictions of answers. As a comparison, our model can\ngenerate a more complete analysis with clear logic for selecting the\ncorrect answer from the given choices. Such an inference process\nis similar to the ground-truth one produced by experts. Besides, it\nis interesting to see that our model can successfully accomplish the\ndeduction of the formals and obtain the true answer. It indicates\nthat our model can effectively deal with the math symbols with\nproper mathematical logic. These results show that it is promising\nto design proper AI models to solve mathematical problems and\nalso benefit the education of high-school students.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7456492781639099
    },
    {
      "name": "Terminology",
      "score": 0.6536625027656555
    },
    {
      "name": "Statement (logic)",
      "score": 0.644356906414032
    },
    {
      "name": "Language of mathematics",
      "score": 0.6371644139289856
    },
    {
      "name": "Mathematical model",
      "score": 0.6060110330581665
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5797076225280762
    },
    {
      "name": "Mathematical theory",
      "score": 0.5503355264663696
    },
    {
      "name": "Mathematical problem",
      "score": 0.5016374588012695
    },
    {
      "name": "Problem statement",
      "score": 0.4793561100959778
    },
    {
      "name": "Natural language processing",
      "score": 0.44798097014427185
    },
    {
      "name": "Mathematical structure",
      "score": 0.42957401275634766
    },
    {
      "name": "Management science",
      "score": 0.3018640875816345
    },
    {
      "name": "Mathematics",
      "score": 0.16009488701820374
    },
    {
      "name": "Linguistics",
      "score": 0.14307576417922974
    },
    {
      "name": "Mathematics education",
      "score": 0.07975682616233826
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I78988378",
      "name": "Renmin University of China",
      "country": "CN"
    }
  ],
  "cited_by": 9
}