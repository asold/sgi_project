{
  "title": "Mask Attention Networks: Rethinking and Strengthen Transformer",
  "url": "https://openalex.org/W3138763170",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2359442718",
      "name": "Fan Zhihao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223152173",
      "name": "Gong, Yeyun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222510414",
      "name": "Liu, Dayiheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2388023786",
      "name": "Wei, Zhongyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2061100179",
      "name": "Wang Si-yuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2135173743",
      "name": "Jiao Jian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2370849483",
      "name": "Duan, Nan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2366222013",
      "name": "ZHANG Ruofei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2378608106",
      "name": "Huang, Xuanjing",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2985808369",
    "https://openalex.org/W2963768805",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W2963641307",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W2955227499",
    "https://openalex.org/W2964302946",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2972245828",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2948981900",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2905016804",
    "https://openalex.org/W3014518624",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2612675303",
    "https://openalex.org/W2995428172",
    "https://openalex.org/W2767989436",
    "https://openalex.org/W2989571009",
    "https://openalex.org/W2899423466"
  ],
  "abstract": "Transformer is an attention-based neural network, which consists of two sublayers, namely, Self-Attention Network (SAN) and Feed-Forward Network (FFN). Existing research explores to enhance the two sublayers separately to improve the capability of Transformer for text representation. In this paper, we present a novel understanding of SAN and FFN as Mask Attention Networks (MANs) and show that they are two special cases of MANs with static mask matrices. However, their static mask matrices limit the capability for localness modeling in text representation learning. We therefore introduce a new layer named dynamic mask attention network (DMAN) with a learnable mask matrix which is able to model localness adaptively. To incorporate advantages of DMAN, SAN, and FFN, we propose a sequential layered structure to combine the three types of layers. Extensive experiments on various tasks, including neural machine translation and text summarization demonstrate that our model outperforms the original Transformer.",
  "full_text": "Mask Attention Networks: Rethinking and Strengthen Transformer\nZhihao Fan1∗, Yeyun Gong2, Dayiheng Liu3, Zhongyu Wei1,6†, Siyuan Wang1,\nJian Jiao4, Nan Duan2, Ruofei Zhang4, Xuanjing Huang5\n1School of Data Science, Fudan University, China\n2Microsoft Research Asia, 3DAMO Academy, 4Microsoft\n5School of Computer Science, Fudan University, China\n6Research Institute of Intelligent and Complex Systems, Fudan University, China\n{fanzh18,zywei,wangsy18,xjhuang}@fudan.edu.cn, liudayiheng.ldyh@alibaba-inc.com\n{yegong,Jian.Jiao,nanduan,bzhang}@microsoft.com\nAbstract\nTransformer is an attention-based neural net-\nwork, which consists of two sublayers, namely,\nSelf-Attention Network (SAN) and Feed-\nForward Network (FFN). Existing research ex-\nplores to enhance the two sublayers separately\nto improve the capability of Transformer for\ntext representation. In this paper, we present a\nnovel understanding of SAN and FFN as Mask\nAttention Networks (MANs) and show that\nthey are two special cases of MANs with static\nmask matrices. However, their static mask ma-\ntrices limit the capability for localness model-\ning in text representation learning. We there-\nfore introduce a new layer named dynamic\nmask attention network (DMAN) with a learn-\nable mask matrix which is able to model lo-\ncalness adaptively. To incorporate advantages\nof DMAN, SAN, and FFN, we propose a se-\nquential layered structure to combine the three\ntypes of layers. Extensive experiments on vari-\nous tasks, including neural machine translation\nand text summarization demonstrate that our\nmodel outperforms the original Transformer.\n1 Introduction\nRecently, Transformer (Vaswani et al., 2017)\nhas been widely applied in various natural lan-\nguage processing tasks, such as neural machine\ntranslation (Vaswani et al., 2017) and text sum-\nmarization (Zhang et al., 2019). To further im-\nprove the performance of the text representation,\nTransformer-based variants have attracted a lot\nof attention (Lu et al., 2019; Sukhbaatar et al.,\n2019a,b; Bugliarello and Okazaki, 2019; Ma et al.,\n2020).\nEach building block of Transformer has two\nsublayers: Self-Attention Network (SAN) and\nFeed-Forward Network (FFN). Shaw et al. (2018)\n∗Work is done during internship at Microsoft Research\nAsia.\n†Corresponding author.\nFigure 1: The mask matrices of (a) SAN, (b) DMAN\nand (c) FFN in Mask Attention Networks. Color that\nfades from black to white means the values in mask\nmatrices decrease from 1 to 0.\npresents an extension to SAN which incorpo-\nrates the relative positional information for the se-\nquence. Sukhbaatar et al. (2019a) proposes atten-\ntion span to control the maximum context size used\nin SAN and scales Transformer to long-range (∼\n8192 tokens) language modeling. Recently, some\nworks targeting on FFN have been proposed. Lu\net al. (2019) gives a new understanding of Trans-\nformer from a multi-particle dynamic system point\nof view and designs a macaron architecture follow-\ning Strang-Marchuk splitting scheme. Sukhbaatar\net al. (2019b) regards the FFN as the persistent\nmemory in SAN to augment SAN. These works\nfocus on enhancing SAN or FFN, but neglect the\ninner relationship between SAN and FFN that hin-\nders further improvement.\nIn this work, we present a more systematic\nanalysis for both SAN and FFN to reveal their\nconnections. We introduce Mask Attention Net-\nworks(MANs), in which each network has a mask\nmatrix that element-wise multiplies a key-query\nattention matrix. We show that SAN and FFN are\ntwo special cases in MANs with static mask matri-\nces. The mask matrix of SAN is an all-ones matrix,\nwhile that of FFN is an identity matrix, which is\nshown as (a) and (c) in Figure 1. Since the mask\nmatrix of SAN has no restriction on relationship\nmodeling with other tokens, SAN is expert in long-\nrange dependency modeling and capture the global\narXiv:2103.13597v1  [cs.CL]  25 Mar 2021\nsemantics. In contrast, mask of FFN disables it to\nperceive the information of other tokens and forces\nit into self-evolution. We believe that these two\nspecialties endowed by two mask matrices make\nthe success of Transformer in text representation.\nAlthough positive results of Transformer have\nbeen reported, recent works (Shaw et al., 2018;\nYang et al., 2018; Guo et al., 2019) have shown\nthat modeling localness would further improve the\nperformance through experiments. We argue that\ndeﬁciency of Transformer in local structure mod-\neling is caused by the attention computation with\nstatic mask matrix. In the framework of MANs,\nwe ﬁnd a problem that irrelevant tokens with over-\nlapping neighbors incorrectly attend to each other\nwith relatively large attention scores. For exam-\nple “a black dog jump to catch the frisbee”, though\n“catch” and “black” are neither relevant nor neigh-\nbors, for the reason that both of them are highly re-\nlated to their common neighbor “dog” in attention,\nwe demonstrate that the attention score from “catch”\nto “black” would be large, which also decreases the\nattention score from “catch” to “frisbee”. The issue\nin self-attention not only introduces noise to the\nsemantic modeling, but also mislead query tokens\nto overlook these neighbor tokens. This reveals that\nself-attention is insufﬁcient in localness modeling\nand inspires us to mask tokens that not appear in\nneighborhood.\nTo strengthen Transformer in localness modeling\nwith better keeping the advantage of SAN and FFN,\nwe propose a Dynamic Mask Attention Network\n(DMAN) as shown in Figure 1(b), which originates\nfrom MANs. Observations reveal that tokens have\ndifferent ranges of neighbors, for example, that of\n“dog”, which is also connected with “frisbee”, is\nlarger than “black” and “catch”. Instead of being\nstatic that determined in advance, the mask matrix\nof DMAN is dependent on the query context and\nrelative distance. In DMAN, the tokens in a speciﬁc\nneighborhood are able to receive more attention be-\nyond the normal self-attention mechanism. The\ndynamic endows DMAN with text representation\nin different scales, and we validate the superiority\nthrough experiments. In Transformer (Vaswani\net al., 2017), SAN and FFN cooperate in a se-\nquential layered structure SAN→FFN. Consider-\ning SAN, FFN, and DMAN all belong to MANs\nand have different advantages in text representa-\ntion, instead of directly replacing SAN in previous\nworks (Shaw et al., 2018; Yang et al., 2018; Guo\net al., 2019), we propose to incorporate them with\nthe architecture DMAN→SAN→FFN.\nThe main contributions of this work are three-\nfold:\n• We introduce Mask Attention Networks and\nreformulate SAN and FFN to point out that\nthey are two special cases with static mask in\nMANs. We analyze the advantages of SAN\nand FFN in text representation learning and\ndemonstrate that they are insufﬁcient for lo-\ncalness modeling.\n• Inspired by the different specialities of SAN\nand FFN, we propose Dynamic Mask Atten-\ntion Network (DMAN) to model localness\nmore effectively. We investigate the differ-\nent collaboration methods of SAN, FFN, and\nDMAN, and propose a sequential layered\nstructure DMAN→SAN→FFN.\n• We conduct experiments on machine transla-\ntion and abstract summarization. Experimen-\ntal results show that our method outperforms\noriginal Transformer. We also perform abla-\ntion study to verify the effectiveness of differ-\nent modules of our proposed model.\n2 Model\nIn § 2.1, we review the Transformer architecture.\nWe introduce Mask Attention Networks and refor-\nmulate SAN and FFN to point out they are two spe-\ncial cases in § 2.2, and analyze their deﬁciency in\nlocalness modeling in § 2.3. Then, in § 2.4, we de-\nscribe Dynamic Mask Attention Network (DMAN)\nin detail. At last, in § 2.5, we discuss the collabora-\ntion of DMAN, SAN and FFN.\n2.1 Transformer\nTransformer has two sublayers: Self-Attention\nNetwork (SAN) and Feed-Forward Network\n(FFN).\nAs discussed in Vaswani et al. (2017), an atten-\ntion function maps a query and a set of key-value\npairs to an output shown in Equation 1.\nA(Q,K,V ) = S(Q,K)V\nS(Q,K) =\n[\nexp\n(\nQiKT\nj /√dk\n)\n∑\nkexp\n(\nQiKT\nk /√dk\n)\n]\n(1)\nwhere the queries Q, keys K and values V ∈\nRT×dk are all matrices.\nSAN produces representations by applying atten-\ntion function to each pair of tokens from the input\nsequence. It is beneﬁcial to capture different con-\ntextual features with multiple individual attention\nfunctions. Given a text representation sequence\nHl ∈RT×d. in the l-the layer.\nHl =\n[\nA1,··· ,AI]\nWH\nAi = A\n(\nHlWi\nQ,HlWi\nK,HlWi\nV\n) (2)\nwhere {Wi\nQ,Wi\nK,Wi\nV}∈ Rd×dk are trainable pa-\nrameters, idenotes the attention head and dis the\nhidden size.\nIn FFN, the computation of each hl\nt in Hl is in-\ndependent of others. It consists of two afﬁne trans-\nformations with a pointwise non-linear function:\nHl+1 = ReLU\n(\nHlW1\n)\nW2 (3)\nwhere W1 and W2 are matrices of dimensiond×df\nand df ×d, respectively. Typically,df is set to be\n4 times larger than d.\n2.2 Mask Attention Networks\nOn the basis of attention function in Equation 1,\nwe deﬁne a new mask attention function:\nAM(Q,K,V ) = SM(Q,K)V\nSM(Q,K) =\n[\nMi,jexp\n(\nQiKT\nj /√dk\n)\n∑\nkMi,kexp\n(\nQiKT\nk /√dk\n)\n]\n(4)\nwhere M ∈RT×T,Mi,j ∈[0,1] is a mask matrix\nand can be static or dynamic. Intuitively, the value\nin each position of M can be viewed as the color\nshade in Figure 1.\nWith the knowledge of mask attention function,\nwe introduce Mask Attention Networks(MANs),\nin which each network can be written as Equation 5.\nHl+1 = F\n([\nA1\nM1 ,··· ,AI\nMI\n])\nWH\nAi\nMi = AMi\n(\nHlWi\nQ,HlWi\nK,HlWi\nV\n) (5)\nwhere Fis the activation function, Mi is the mask\nmatrix for the i-th attention head.\nNext, we show that SAN and FFN both belong\nto the Mask Attention Networks.\nFor SAN, let M = [1] ∈RT×T be an all-ones\nmatrix and F= Fid be the identity function, its\nmask attention function would be formalized:\nS[1](Q,K) =\n[\n1 ·exp\n(\nQiKT\nj /√dk\n)\n∑\nkexp\n(\nQiKT\nk /√dk\n)\n]\n= S(Q,K)\nA[1](Q,K,V ) = S[1](Q,K)V = A(Q,K,V )\n(6)\nFigure 2: Overview of our proposed model.\nLeft is the Transformer architecture, right is our\nDMAN→SAN→FFN one.\nThen, the MAN degenerates into SAN.\nHl+1 = Fid\n([\nA1\n[1],··· ,Ah\n[1]\n])\nWH\n=\n[\nA1,··· ,Ah]\nWH\n(7)\nFor FFN, let M = I ∈RT×T be the identity\nmatrix, F= ReLU and head number I = 1.\nSI(Q,K) =\n[\n1i(j) ·exp\n(\nQiKT\nj /√dk\n)\n∑\nk1i(k) ·exp\n(\nQiKT\nk /√dk\n)\n]\n= I\nAI(Q,K,V ) = SI(Q,K)V = IV = V\n(8)\nwhere 1i(x) is an indicator function that equal to 1\nif x= i, otherwise 0.\nThe MAN degenerates into FFN.\nHl+1 = ReLU\n([\nA1\nM\n])\nWH = ReLU\n(\nHlW1\nV\n)\nWH\n(9)\nIn summary, SAN and FFN are two special cases\nin MANs with different static mask matrices.\n2.3 Deﬁciency of SAN and FFN in Localness\nModeling\nThe mask matrix of SAN is an all-ones matrix\nand that of FFN is an identity matrix, they are two\nextreme cases in MANs. We analyze that these\ntwo static MANs are deﬁcient in localness mod-\neling. Intuitively, through blocking other tokens\nin advance, FFN focuses on its own information\nand is unable to perceive the information except\nitself, let alone its neighbors. In SAN, each token is\nequally accessible to any other ones. As the exam-\nple in Introduction shows, we ﬁnd that tokens not\nin neighborhood are also likely to attend to each\nother with relatively large scores. Therefore, SAN\nmight introduce noises to semantic modeling and\noverlook the relation of neighboring signals.\nWe demonstrate the issue of self-attention. Gen-\nerally assuming that\n[\na,b,c\n]\nappear in sequence,\nand (a,b),(b,c) are two neighbor pairs, buta,c are\nnot neighbors.\nFirst, to explicitly deﬁne the relationship of to-\nkens, we introduce Uδ(h) as the set of tokens at\nthe distance of δ from hwith key and query lin-\near transformation in SAN, in other words, u ∈\nUδ(h) ⇔||hWQ −uWK||2\n2 ≤δ. For example, if\n(a,b) is a neighbor pair, there would exist some\nsmall δ≥0 such that a∈Uδ(b) and b∈Uδ(a).\nSecond, we know that the larger the inner prod-\nuct is, the smaller the Euclidean distance is, and\nvice versa. With the awareness of the relation-\nships between\n[\na,b,c\n]\n, we have a,b ∈ Uδ(a),\nb,c ∈Uδ(c) and a,b,c ∈Uδ(b) for some small\nδ≥0.\nThird, we are able to estimate the semantic dis-\ntance between aand cas the Equation 10 shows.\n||aWQ −cWK||2\n2\n=||aWQ −bWK + bWK −bWQ + bWQ −cWK||2\n2\n≤3||aWQ −bWK||2\n2 + 3||bWK −bWQ||2\n2\n+3||bWQ −cWK||2\n2\n)\n≤9δ\n(10)\nThus, though aand care not neighbors, no matter\nhow irrelevant the semantics ofaand c, c∈U9δ(a)\nthat cwould play an important role in modeling\nsemantics of a.\nThe upper phenomenon illustrates following nor-\nmal attention function in Equation 1, some tokens\nnot in neighborhood not are still likely to occupy\nan important position in attention weight that can\nnot be ignored.\n2.4 Dynamic Mask Attention Network\nWith the knowledge of MANs, we propose to\nmask other tokens that not in neighborhood of the\ntarget token for better local semantic modeling.\nFor example, we build a distance-dependent\nmask matrix SM. If each token only model the\nrelationship with those tokens within b units of\nitself, we can set\nSM[t,s] =\n{ 0, |t−s|>b\n1, |t−s|≤ b (11)\nwhere t,s are the positions of query and key, and\nSM[t,s] is the value of thet-th row ands-th column\nof SM .\nBy means of SM, we take those tokens within\nb units into account and ignore others. The\nstatic mask does assign more weights to a spe-\nciﬁc neighborhood, but lacks ﬂexibility. Consid-\nering the neighborhood size varies with different\nquery tokens, number of tokens that beneﬁt for\ndifferent query tokens’ local semantic representa-\ntion are different. Moreover, their mask matrices\nshould match different attention heads and layers\nin MANs.\nWe propose Dynamic Mask Attention Network\n(DMAN) that replaces the static mask matrix. In-\ncorporating query tokens, relative distance, atten-\ntion head and layer, we build a dynamic mask func-\ntion which replaces the hard0/1 mask gate in Equa-\ntion 11 with a soft one through sigmoid activation\nfunction in Equation 12.\nDMl\ni[t,s] = σ\n(\nhl\ntWl + Pl\nt−s + Ul\ni\n)\n(12)\nwhere s,t are the positions of query and key,iis the\nattention head, lis the layer. Pl\nt−s is parameterized\nscalar for the positions t and s, Ul\ni is for the i-\nth head, and Wl ∈Rd×1. Wl, Pl\nt−s and Ul\ni are\ntrainable parameters.\n2.5 Collaboration of Mask Attention\nNetworks\nUntil here, we have three sub-networks of\nMANs, namely, SAN, FFN and DMAN. SAN that\ndoes not mask any tokens and specializes in global\nsemantic modeling. FFN that masks all tokens ex-\ncept itself and focuses on self-processing. DMAN\nmasks the tokens not in neighborhood and is able\nto model local structure more effectively.\nTransformer is composed of SAN and FFN that\nachieves positive results in various NLP tasks, the\nstacking method of Transformer inspires us to stack\nDMAN, SAN and FFN to incorporate their ad-\nvantages. We insert DMAN in the manner of\nDMAN→SAN→FFN, which is shown in Figure 2.\nWith this architecture, we ﬁrst model the localness\nthen globalness, and take the step for self-evolution\nin the end.\n3 Experiments\nIn this section, we introduce our experiments.\nWe ﬁrst describe the experimental details in § 3.1.\nThen we show the experimental results in § 3.2.\nModel IWSLT14 De-En WMT14 En-De\nsmall params base params big params\nTransformer (Vaswani et al., 2017) 34.4 36M 27.3 62M 28.4 213M\nConvolutional Transformer (Yang et al., 2019b) - - 28.2 88M 28.7 -\nWeighted Transformer (Ahmed et al., 2017) - - 28.4 65M 28.9 213M\nLocal Transformer (Yang et al., 2018) - - 28.5 89M 29.2 268M\nRelative Transformer (Shaw et al., 2018) - - 26.8 - 29.2 -\nScaling NMT (Ott et al., 2018) - - - - 29.3 213M\nDynamic Conv (Wu et al., 2019) 35.2 - - - 29.7 213M\nOurs 36.3 37M 29.1 63M 30.4 215M\nTable 1: Translation performance (BLEU) on IWSLT14 De-En and WMT14 En-De testsets.\nFinally we conduct the ablation study and analysis\nin § 4.\n3.1 Experimental Setting\n3.1.1 Machine Translation\nMachine translation is an important applica-\ntion of natural language processing (Vaswani\net al., 2017). We evaluate our methods on\ntwo widely used public datasets: IWSLT14\nGerman-to-English (De-En) and WMT14 English-\nto-German (En-De). IWSLT14 De-En dataset con-\nsists of about 153K/7K/7K sentence pairs for train-\ning/validation/testing. WMT14 En-De dataset con-\nsists of about 4.5M sentence pairs, and the models\nwere validated on newstest2013 and examined on\nnewstest2014.\nOur data processing follows Lu et al. (2019).\nFor IWSLT2014, we set our model into the small\none, the hidden size, embeddings and attention\nheads to 512, 512, and 4 respectively. For the\nWMT14 dataset, following the Transformer setting\nof Vaswani et al. (2017), we set our model into the\nbase and big ones which both consist of a 6-layer\nencoder and 6-layer decoder, the hidden nodes are\nset to 512 and 1024, and the number of attention\nheads are 8 and 16. For each setting (small, base\nand big), we replace all layers in Transformer by\nour MAN layer. To make a relatively fair compar-\nison, we set the dimensionality of the inner-layer\nof the FFN in the MAN layers to two times of the\ndimensionality of the hidden states.\nWe train our proposed model with cross-entropy\nwith 0.1 label smoothing rate. Inverse-sqrt learning\nrate scheduler are employed, the peak learning rates\nare 1.5e-2, 1e-2 and 7e-3 with 8k warmup, 50k\nupdate, 80k update and 80k update for transformer\nbig, base and small model with max-tokens 4096,\n12288 and 8192 per batch. The dropout rates are\n0.3, 0.1 and 0.3 for small, base and big models.\nThe optimizer of model is Adam with (0.9,0.98).\nThe beam size and length penalty for base and big\nmodels are 4 and 0.6, for small model is 5 and 1.0.\nThe base and large model are trained on 8 V100\nGPUs, and the small model is trained on 2 P40.\n3.1.2 Abstract Summarization\nAutomatic summarization aims to produce a con-\ncise and ﬂuent summary conveying the key infor-\nmation in the input text. We focus on abstractive\nsummarization, a generation task where the sum-\nmary is not limited in reusing the phrases or sen-\ntences in the input text. We use the CNN/Daily\nMail (See et al., 2017) and Gigaword (Rush et al.,\n2015) for model evaluation.\nFollowing Song et al. (2019), we set the hidden\nsize, embeddings and attention heads to 768, 768,\nand 12 respectively. Our model consists of a 6-layer\nencoder and 6-layer decoder. For the convenience\nof comparison, the training follows classic seq2seq\nmodel without copy, converge or RL. We remove\nduplicated trigrams in beam search (Paulus et al.,\n2018). Moreover, the dimensionality of the inner-\nlayer of the FFN in the MAN layers is set to two\ntimes of the dimensionality of the hidden states.\nIn training, inverse-sqrt learning rate scheduler\nis employed. The peak learning rates are 1e-3 and\n8e-4, max-tokens per batch are 8192 and 12288 for\nCNN/Daily Mail and Gigaword, respectively. The\nwarmup steps is 8k and the total updates is 50k.\nThe optimizer of model is Adam with (0.9,0.98).\nThe dropout and clip-norm are both 0.1. During\ndecoding, the beam size are both 5, the max length\nand length penalty are 50 and 2.0 for CNN/Daily\nMail, 30 and 1.0 for Gigaword. The models are\ntrained on 4 P40 GPUs.\n3.2 Experimental Results\n3.2.1 Machine Translation\nIn machine translation, BLEU (Papineni et al.,\n2002) is employed as the evaluation measure. Fol-\nlowing common practice, we use tokenized case-\nsensitive BLEU and case-insensitive BLEU for\nWMT14 En-De and IWSLT14 De-En, respectively.\nWe take Transformer (Vaswani et al., 2017) as\nthe baseline and compare with other concurrent\nmethods. Convolutional Transformer (Yang et al.,\n2019b) restricts the attention scope to a window\nof neighboring elements in order to model locality\nfor self-attention model. Local Transformer (Yang\net al., 2018) casts localness modeling as a learn-\nable Gaussian bias, which indicates the central and\nscope of the local region to be paid more attention.\nThe results for machine translation are shown\nin Table 1. Our model exceeds the baseline Trans-\nformer and other models. For the IWSLT14 dataset,\nour small model outperforms the Transformer small\nby 1.6 points in terms of BLEU. For the WMT14\ndataset, our base model exceeds its Transformer\ncounterpart by 1.8 BLEU points. Furthermore,\nthe performance of our base model is even bet-\nter than that of the Transformer big model reported\nin (Vaswani et al., 2017), but with much less param-\neters. Our big model outperforms the Transformer\nbig by 2.0 BLEU points.\nCompare with Convolutional Transformer and\nLocal Transformer, our model also achieve 1.7\nand 1.2 points improvement in BLEU, respectively.\nThis validates that the superiority of our model to\nsystematically solve the localness modeling prob-\nlem in Transformer.\n3.2.2 Abstractive Summarization\nWe use the F1 score of ROUGE (Lin and Hovy,\n2003) as the evaluation metric 1. In Table 2, we\ncompare our model against the baseline Trans-\nformer (Vaswani et al., 2017) and several gener-\nation models on CNN/Daily Mail and Gigaword.\nLEAD3 (Nallapati et al., 2016) extracts the ﬁrst\nthree sentences in a document as its summary. PT-\nGEN+Converage (See et al., 2017) is a sequence-\nto-sequence model based on the pointer-generator\nnetwork. As shown in Table 2, our model out-\n1https://github.com/pltrdy/files2rouge\nperforms Transformer by 1.4 in ROUGE-1, 2.2 in\nROUGE-2 and 1.2 in ROUGE-L in CNN/Daily\nMail. In Gigaword dataset, ours exceeds the base-\nline by 0.7 in ROUGE-1, 0.5 in ROUGE-2 and 0.7\nin ROUGE-L.\nAs a summary, in machine translation and\nabstractive summarization our proposed model\nachieves better results than the Original Trans-\nformer (Vaswani et al., 2017).\n4 Further Analysis\nIn this section, we conduct further analysis for\nour model. We ﬁrst investigate stacking methods\nfor different sublayers in § 4.1. Then we com-\npare strategies of static mask and dynamic mask in\n§ 4.2. Finally, we analyse the behavior of SAN and\nDMAN in localness modeling through attention\nscores in § 4.3.\n4.1 Investigate Stacking Methods for\nDifferent Sublayers\nHere, we investigate different collaboration\nmechanisms of the elements in MANs. Under our\ndesign principles, there are three elements: FFN,\nSAN, and DMAN. For the convenience of com-\nparison, we take FFN as the last component in the\nsequential layered structure. We try different col-\nlaboration methods and test them on IWSLT2014\nGerman-to-English (De-En). The results are shown\nin the Table 3. We conclude that:\n1. Our proposed C#5 achieves the best perfor-\nmance that verify the effectiveness of our pro-\nposed sequential layered structure.\n2. All of C#3, C#4 and C#5 outperform C#1\nand C#2, and the least improvement in BLEU\nis 0.2. This shows that no matter what collab-\noration method, models with the participation\nof DMAN perform better than models with-\nout DMAN, which validates the capability of\nDMAN.\n3. Both C#5 and C#4 are better than C#3 and\nC#2. This indicates that models without\nDMAN or SAN are not comparable to mod-\nels with all three modules. This shows that\nDMAN and SAN have their own strengths,\nnamely, localness modeling and globalness\nmodeling, and are able to make up for each\nother’s defects through collaboration.\nModel CNN/Daily Mail Gigaword\nR-1 R-2 R-L R-avg R-1 R-2 R-L R-avg\nLEAD-3 (Nallapati et al., 2016) 40.42 17.62 36.67 31.57 - - - -\nPTGEN+Coverage (See et al., 2017) 39.53 17.28 36.38 31.06 - - - -\nDynamic Conv (Wu et al., 2019) 39.84 16.25 36.73 30.94 - - - -\nTransformer (Vaswani et al., 2017) 39.50 16.06 36.63 30.73 37.57 18.90 34.69 30.38\nOurs 40.98 18.29 37.88 32.38 38.28 19.46 35.46 31.06\nTable 2: Evaluation results on CNN/Daily Mail and Gigaword. R is short for ROUGE.\n# Method BLEU\nC#1 FFN →SAN→FFN 35.51\nC#2 SAN →SAN→FFN 35.66\nC#3 DMAN →DMAN→FFN 35.86\nC#4 SAN →DMAN→FFN 35.91\nC#5 DMAN →SAN→FFN 36.35\nTable 3: Performance of different collaboration meth-\nods of DMAN, SAN and FFN. We evaluate on\nIWSLT2014 De-En.\n4. C#5 is better than C#4. This indicates that\nﬁrst modeling the localness and then global-\nness would be better than the inverse order.\n4.2 Static Mask and Dynamic Mask\nIn this section, we compare the performance of\nStatic Mask Attention Network (SMAN) and Dy-\nnamic Mask Attention Network (DMAN). Both\nof them follow the collaboration strategy of\nDMAN(SMAN)→SAN→FFN. In SMAN, we set\na ﬁxed mask boundary which has been determined\nin advance following Equation 11. Empirically, we\npropose two static mask strategies: (a) SMAN 1,\nthe boundary b depends on sentence length L,\nb =\n√\nL/2; (b) SMAN 2, b is set to 4, which is\nchosen from 2, 4, 6, 8 through validation.\nThe results in IWSLT2014 De-En are shown in\nTable 4. The performance of SMAN1 and SMAN2\nare very close. They both outperform the Trans-\nformer but fall behind our proposed DMAN. This\nindicates that our proposed DMAN is superior to\nSMAN. SMAN fails to manage various neighbor-\nhood for different query tokens, but DMAN can\nmodel localness with more ﬂexibility according to\nthese factors.\nmodel BLEU\nTransformer 34.40\nSMAN1 35.52\nSMAN2 35.55\nDMAN 36.35\nTable 4: Performance of SMAN and DMAN on\nIWSLT2014 De-En.\n4.3 Analysis of DMAN in Localness Modeling\nIn this section, we analyse the behavior of\nDMAN and SAN in localness modeling through\nattention scores in Equation 4. To quantify the role\nof neighbors in semantic modeling, we compute\nthe sum of attention scores within some particu-\nlar window size. Generally, if the attention score\nfrom ato cis bigger than bto c, we consider that\na contributes more to the semantic modeling of\nc compared to b, in other words, model utilizes\nmore information of athan bto learn the seman-\ntic representation of c. Therefore, larger attention\nscores mean that model utilizes more information\nof the corresponding tokens to learn the semantic\nrepresentation of query token.\nFor each sentence in dataset Xi = ( xi,1,··· ,\nxi,Ti) ∈D, we utilize ¯sl\ni,DMAN and ¯sl\ni,SAN ∈RTi×Ti\nto denote the average attention scoresSM(Q,K) in\nEquation 4 across different heads in the l-th layer\nfor DMAN and SAN, respectively. We sum the\nattention scores of these tokens xi,k within the win-\ndow size wof the query xi,j in the l-th layer, and\naverage the sum across Xi and dataset Dfollowing\nEquation 13.\nattn_sw,l,∗= 1\n|D|\n∑\nXi∈D\n1\nTi\n∑\nxi,j∈Xi\n∑\n|k−j|≤w\n¯sl\ni,∗\n[\nj,k\n]\n(13)\nw #1 #3 #6\nDMAN 1 76.58 60.43 60.86\nSAN 1 12.80 40.39 45.55\nDMAN 2 86.17 75.56 73.89\nSAN 2 18.73 45.62 52.72\nDMAN 4 95.09 86.20 85.58\nSAN 4 30.38 55.17 62.77\nTable 5: The values of attention scores attn_sw,l,DMAN\nand attn_sw,l,SAN, which is shown in Equation 13. D\nis the test set of IWSLT14 De-En, window size w =\n1,2,4 and encoder layers l= 1,3,6.\nwhere ∗ ∈ {DMAN,SAN}, and ¯sl\ni,∗\n[\nj,k\n]\nis the\nvalue of the j-th row and k-th column of ¯sl\ni,∗.\nattn_sw,l,∗ measures the overall contribution of\nthese neighbor tokens within the window size w\nto the query tokens’ semantic modeling. We take\nDas the test set of IWSLT14 De-En and compute\nattn_sw,l,∗with w= 1,2,4 and l= 1,3,6.\nThe result is shown in Table 5. We see that in\nlayer#1, #3 and #6, the sum attention scores of\nDMAN within the window size 2 are 50% more\nthan those of SAN, especially in layer#1 where\nthe gap is as much as ﬁve times between SAN\nand DMAN. This phenomenon validates that the\nattention scores of DMAN in neighbors are larger\nthan those of SAN, thus DMAN is more specialized\nin localness modeling than SAN.\n5 Related Work\nRecently, there is a large body of work on im-\nproving Transformer (Vaswani et al., 2017) for var-\nious issues. For recurrence modeling, Hao et al.\n(2019) introduces a novel attentive recurrent net-\nwork to leverage the strengths of both attention and\nrecurrent networks. For context modeling, Yang\net al. (2019a) focuses on improving self-attention\nthrough capturing the richness of context and pro-\nposes to contextualize the transformations of the\nquery and key layers. Wu et al. (2019) introduces\ndynamic convolutions to predict separate convolu-\ntion kernels solely based on the current time-step\nin order to determine the importance of context ele-\nments. In order to adjust attention weights beyond\nSAN, Shaw et al. (2018) extends the self-attention\nmechanism to efﬁciently consider representations\nof the relative positions or distances between se-\nquence elements through adding a relative posi-\ntion embedding to the key vectors; Bugliarello and\nOkazaki (2019) transfers the distance between two\nnodes in dependency trees with a pre-deﬁned Gaus-\nsian weighting function and multiply the distance\nwith the key-query inner product value; Dai et al.\n(2019) presents a relative position encoding scheme\nthat adds additional relative position representation\nto the key-query computation. Sukhbaatar et al.\n(2019a) proposes a parameterized linear function\nover self-attention to learn the optimal attention\nspan in order to extend signiﬁcantly the maximum\ncontext size used in Transformer. To merge FFN\nto SAN, Sukhbaatar et al. (2019b) proposes a new\nmodel that solely consists of attention layers and\naugments the self-attention layer with persistent\nmemory vectors that play a similar role as the feed-\nforward layer. As for the collaboration of SAN and\nFFN, Lu et al. (2019) introduces Macaron layer\nthat split the FFN into two half-steps based on\nStrang-Marchuk splitting scheme in ODE. For lo-\ncalness modeling, Yang et al. (2018) casts localness\nmodeling as a learnable Gaussian bias according\nto relative distance to external energy in softmax\nfunction as a new self-attention network. Zhao et al.\n(2019) explores parallel multi-scale representation\nlearning to capture both long-range and short-range\nlanguage structures with combination of convolu-\ntion and self-attention. In our work, DMAN, SAN\nand FFN are uniﬁed in Mask Attention Networks,\nwhere DMAN is a supplement of SAN and FFN\nthat specializes in localness modeling. Moreover,\nwe investigate different collaboration mechanisms.\n6 Conclusion\nIn this paper, we introduce Mask Attention Net-\nworks and reformulate SAN and FFN to point\nout they are two special cases with static mask\nin MANs. We analyze the the deﬁciency of\nSAN and FFN in localness modeling. Dynamic\nMask Attention Network is derived from MANs\nfor better local structure modeling. Consider-\ning the different specialities of SAN, FFN, and\nDMAN, we investigate a sequential layered struc-\nture DMAN→SAN→FFN for their collaboration.\nCompared with original Transformer, our proposed\nmodel achieves better performance in neural ma-\nchine translation and abstract summarization. For\nfuture work, we consider adding structure informa-\ntion or external knowledge, e.g., dependency tree,\nwith mask matrices in MANs.\n7 Acknowledgement\nThis work is partially supported by National Nat-\nural Science Foundation of China (No.71991471),\nScience and Technology Commission of Shanghai\nMunicipality Grant (No.20dz1200600).\nReferences\nKarim Ahmed, Nitish Shirish Keskar, and Richard\nSocher. 2017. Weighted transformer net-\nwork for machine translation. arXiv preprint\narXiv:1711.02132.\nEmanuele Bugliarello and Naoaki Okazaki. 2019.\nImproving neural machine translation with\nparent-scaled self-attention. arXiv preprint\narXiv:1909.03149.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nMaosheng Guo, Yu Zhang, and Ting Liu. 2019. Gaus-\nsian transformer: a lightweight approach for natu-\nral language inference. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence , volume 33,\npages 6489–6496.\nJie Hao, Xing Wang, Baosong Yang, Longyue Wang,\nJinfeng Zhang, and Zhaopeng Tu. 2019. Modeling\nrecurrence for transformer. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 1198–1207, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nChin-Yew Lin and Eduard Hovy. 2003. Auto-\nmatic evaluation of summaries using N-gram co-\noccurrence statistics. In Proceedings of the 2003\nConference of the North American Chapter of the As-\nsociation for Computational Linguistics on Human\nLanguage Technology-Volume 1, pages 71–78. As-\nsociation for Computational Linguistics.\nYiping Lu, Zhuohan Li, Di He, Zhiqing Sun, Bin\nDong, Tao Qin, Liwei Wang, and Tie-Yan Liu. 2019.\nUnderstanding and improving transformer from a\nmulti-particle dynamic system point of view. arXiv\npreprint arXiv:1906.02762.\nXutai Ma, Juan Miguel Pino, James Cross, Liezl Puzon,\nand Jiatao Gu. 2020. Monotonic multihead attention.\nIn International Conference on Learning Represen-\ntations.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nÇa˘glar Gulçehre, and Bing Xiang. 2016. Abstrac-\ntive text summarization using sequence-to-sequence\nRNNs and beyond. In Proceedings of The 20th\nSIGNLL Conference on Computational Natural Lan-\nguage Learning, pages 280–290, Berlin, Germany.\nAssociation for Computational Linguistics.\nMyle Ott, Sergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling neural machine trans-\nlation. In Proceedings of the Third Conference on\nMachine Translation: Research Papers , pages 1–9,\nBrussels, Belgium. Association for Computational\nLinguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th annual meeting on association for compu-\ntational linguistics, pages 311–318. Association for\nComputational Linguistics.\nRomain Paulus, Caiming Xiong, and Richard Socher.\n2018. A deep reinforced model for abstractive sum-\nmarization. In International Conference on Learn-\ning Representations.\nAlexander M. Rush, Sumit Chopra, and Jason Weston.\n2015. A neural attention model for abstractive sen-\ntence summarization. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 379–389, Lisbon, Portugal.\nAssociation for Computational Linguistics.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1073–\n1083, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 464–468,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. Mass: Masked sequence to se-\nquence pre-training for language generation. In In-\nternational Conference on Machine Learning, pages\n5926–5936.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bo-\njanowski, and Armand Joulin. 2019a. Adaptive at-\ntention span in transformers. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 331–335, Florence, Italy.\nAssociation for Computational Linguistics.\nSainbayar Sukhbaatar, Edouard Grave, Guillaume\nLample, Herve Jegou, and Armand Joulin. 2019b.\nAugmenting self-attention with persistent memory.\narXiv preprint arXiv:1907.01470.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nFelix Wu, Angela Fan, Alexei Baevski, Yann Dauphin,\nand Michael Auli. 2019. Pay less attention with\nlightweight and dynamic convolutions. In Interna-\ntional Conference on Learning Representations.\nBaosong Yang, Jian Li, Derek F Wong, Lidia S Chao,\nXing Wang, and Zhaopeng Tu. 2019a. Context-\naware self-attention networks. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence , vol-\nume 33, pages 387–394.\nBaosong Yang, Zhaopeng Tu, Derek F. Wong, Fandong\nMeng, Lidia S. Chao, and Tong Zhang. 2018. Mod-\neling localness for self-attention networks. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 4449–\n4458, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nBaosong Yang, Longyue Wang, Derek F. Wong,\nLidia S. Chao, and Zhaopeng Tu. 2019b. Convolu-\ntional self-attention networks. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 4040–4045, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nHaoyu Zhang, Jingjing Cai, Jianjun Xu, and Ji Wang.\n2019. Pretraining-based natural language genera-\ntion for text summarization. In Proceedings of the\n23rd Conference on Computational Natural Lan-\nguage Learning (CoNLL) , pages 789–797, Hong\nKong, China. Association for Computational Lin-\nguistics.\nGuangxiang Zhao, Xu Sun, Jingjing Xu, Zhiyuan\nZhang, and Liangchen Luo. 2019. MUSE: Parallel\nmulti-scale attention for sequence to sequence learn-\ning. arXiv preprint arXiv:1911.09483.",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9163795113563538
    },
    {
      "name": "Transformer",
      "score": 0.7883350253105164
    },
    {
      "name": "Computer science",
      "score": 0.7015543580055237
    },
    {
      "name": "Artificial neural network",
      "score": 0.5801957249641418
    },
    {
      "name": "Machine translation",
      "score": 0.4948539137840271
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48825696110725403
    },
    {
      "name": "Natural language processing",
      "score": 0.4010482728481293
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3311176300048828
    },
    {
      "name": "Electrical engineering",
      "score": 0.10374242067337036
    },
    {
      "name": "Engineering",
      "score": 0.09926530718803406
    },
    {
      "name": "Voltage",
      "score": 0.09346410632133484
    }
  ],
  "topic": "Automatic summarization"
}