{
  "title": "Pragmatic competence of pre-trained language models through the lens of discourse connectives",
  "url": "https://openalex.org/W3210694367",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3204041734",
      "name": "Lalchand Pandia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100964018",
      "name": "Yan Cong",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A2404566579",
      "name": "Allyson Ettinger",
      "affiliations": [
        "University of Chicago"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2250265609",
    "https://openalex.org/W2170496269",
    "https://openalex.org/W2604410277",
    "https://openalex.org/W3167227435",
    "https://openalex.org/W2406031065",
    "https://openalex.org/W2264742718",
    "https://openalex.org/W2954982705",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2952750383",
    "https://openalex.org/W2502528956",
    "https://openalex.org/W4230824599",
    "https://openalex.org/W2470656186",
    "https://openalex.org/W3153799850",
    "https://openalex.org/W3152698349",
    "https://openalex.org/W2924120895",
    "https://openalex.org/W2911435132",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2014466531",
    "https://openalex.org/W1577783238",
    "https://openalex.org/W3122811996",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2164567676",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3185980407",
    "https://openalex.org/W2050066130",
    "https://openalex.org/W2166957049",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W1600575693"
  ],
  "abstract": "As pre-trained language models (LMs) continue to dominate NLP, it is increasingly important that we understand the depth of language capabilities in these models. In this paper, we target pre-trained LMs' competence in pragmatics, with a focus on pragmatics relating to discourse connectives. We formulate cloze-style tests using a combination of naturally-occurring data and controlled inputs drawn from psycholinguistics. We focus on testing models' ability to use pragmatic cues to predict discourse connectives, models' ability to understand implicatures relating to connectives, and the extent to which models show humanlike preferences regarding temporal dynamics of connectives. We find that although models predict connectives reasonably well in the context of naturally-occurring data, when we control contexts to isolate high-level pragmatic cues, model sensitivity is much lower. Models also do not show substantial humanlike temporal preferences. Overall, the findings suggest that at present, dominant pre-training paradigms do not result in substantial pragmatic competence in our models.",
  "full_text": "Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL), pages 367–379\nNovember 10–11, 2021. ©2021 Association for Computational Linguistics\n367\nPragmatic competence of pre-trained language models through the lens of\ndiscourse connectives\nLalchand Pandia, Yan Cong1, Allyson Ettinger2\n1Department of Linguistics and Languages, Michigan State University\n2Department of Linguistics, University of Chicago\nlcpandia@gmail.com, congyan@msu.edu, aettinger@uchicago.edu\nAbstract\nAs pre-trained language models (LMs) con-\ntinue to dominate NLP, it is increasingly im-\nportant that we understand the depth of lan-\nguage capabilities in these models. In this pa-\nper, we target pre-trained LMs’ competence\nin pragmatics, with a focus on pragmatics re-\nlating to discourse connectives. We formu-\nlate cloze-style tests using a combination of\nnaturally-occurring data and controlled inputs\ndrawn from psycholinguistics. We focus on\ntesting models’ ability to use pragmatic cues\nto predict discourse connectives, models’ abil-\nity to understand implicatures relating to con-\nnectives, and the extent to which models show\nhumanlike preferences regarding temporal dy-\nnamics of connectives. We ﬁnd that although\nmodels predict connectives reasonably well in\nthe context of naturally-occurring data, when\nwe control contexts to isolate high-level prag-\nmatic cues, model sensitivity is much lower.\nModels also do not show substantial human-\nlike temporal preferences. Overall, the ﬁnd-\nings suggest that at present, dominant pre-\ntraining paradigms do not result in substantial\npragmatic competence in our models.\n1 Introduction\nPre-trained language models continue to display\nimpressive performance across various domains\nof NLP, raising the important question of exactly\nwhat level of linguistic competence these models\nhave acquired, particularly during the pre-training\nprocess. Although models show outstanding perfor-\nmance on downstream tasks, there is also evidence\nthat their handling of language is shallow (McCoy\net al., 2019; Sinha et al., 2021).\nIn this paper we examine aspects of pre-trained\nLMs competence in pragmatics, with a particu-\nlar focus on pragmatic reasoning surrounding dis-\ncourse connectives. Discourse connectives are\nlinguistic elements that connect two neighboring\nevents or sentences, signalling the discourse re-\nlation that exists between them. Discourse con-\nnectives reﬂect and convey pragmatic informa-\ntion about relationships between events being de-\nscribed, and connectives can also be associated\nwith pragmatically-enriched meanings that go be-\nyond their literal meanings. If pre-trained LMs\nhave acquired competence in pragmatics of a lan-\nguage, we would expect them to be able to use cues\nin context to infer what discourse relation holds\nbetween events, and by extension which discourse\nconnective is appropriate. Additionally, we would\nexpect them to be able to pick up on pragmatic in-\nferences generated by the connectives themselves.\nThese are the questions that we test in this paper.\nWe formulate all of our tests as cloze tasks, so\nthat we can test the pre-trained LMs without ﬁne-\ntuning. We begin with a general task of connec-\ntive prediction, to gauge how well models can use\nsurrounding sentence context to predict the appro-\npriate discourse connective. We then move on to\nmore controlled tests, inspired by psycholinguis-\ntics, to ask more targeted questions about models’\nuse of contextual cues and handling of implica-\nture. The ﬁrst of these examines models’ ability\nto use high-level pragmatic cues to infer appropri-\nate connectives, in the absence of clear syntactic\nor lexical cues. The second examines the extent\nto which model predictions reﬂect understanding\nof temporal implicatures in settings of implicature\ncancellation and reinforcement. Finally, we test\nwhether models show humanlike preferences in\nterms of temporal dynamics of event mentions.\nWe test a range of pre-trained LMs with these\nanalyses. Our results indicate that although mod-\nels show reasonable connective predictions in\nnaturally-occurring data, they lack the pragmatic\nsensitivity to perform well on our controlled tests,\nand they don’t show any strong humanlike prefer-\nence in terms of temporal dynamics. The results\nsuggest that for the moment, the currently domi-\nnant pre-training paradigms are not yielding clear\n368\npragmatic competence in NLP models, at least with\nrespect to discourse connectives. We make all code\nand test data available for additional testing.1\n2 Related Work\nConnectives have been studied in NLP models from\na number of angles. Some works have focused\non improving automatic extraction of connectives\nvia heuristics (Sileo et al., 2019) or dependency\nparses (Nie et al., 2019), so as to use connective\nprediction during model pre-training. While these\nworks share with ours the use of discourse con-\nnective prediction, we differ in focusing not on\nmodifying model training, but on evaluating and\nanalyzing existing models, to better understand the\nextent to which existing pre-training paradigms\nconfer sensitivity to pragmatic cues.\nOthers have studied models’ competence in clas-\nsifying discourse relations on the basis of connec-\ntives or contexts. Pitler and Nenkova (2009) show\nthat using syntactic features along with the con-\nnectives themselves, a supervised classiﬁer is able\nto identify the discourse relation that a connective\nrepresents. Kurfalı and Östling (2021) study com-\npetence of multilingual models in various discourse\ntasks by evaluating cross-lingual zero-shot trans-\nfer in a range of sentence encoders—among these\ntasks is classiﬁcation of discourse relations, though\nthey focus on implicit discourse relations, where\nconnectives are not the source of discourse infor-\nmation. Koto et al. (2021) also evaluate pre-trained\nlanguage models’ performance on discourse level\nrelations, including a task of discourse connective\nprediction: given two sentences, the task is to pre-\ndict the explicit connective. They ﬁne-tune pre-\ntrained models on this classiﬁcation task, ﬁnding\nthat for the discourse connective prediction task,\ndifferent models produce similar patterns. Patter-\nson and Kehler (2013) also examine connective\nprediction, training a classiﬁer to predict whether\na connective is present, and ﬁnding that classiﬁers\nare able to perform this prediction task on the basis\nof shallow linguistic features alone.\nOur work in this paper is similar to these prior\nworks in examining the ability of models to cap-\nture discourse information through tasks involving\nprediction of connectives and discourse relations.\nHowever, we differ critically from prior work in\nasking not whether we can train models to make\n1https://github.com/lalchand-pandia/\nPragmatic-competence-in-PLM\nthese predictions—rather, our question is to what\nextent pre-trained LMs have already developed\npragmatic knowledge relating to connectives, as\na byproduct of the pre-training process itself. Con-\nsequently, we deviate from this prior work in that\nwe do not ﬁne-tune models or do any supervised\ntraining for connective predictions—instead, we\nformulate our tests as word prediction tasks and test\nwhether model predictions reﬂect an understanding\nof the pragmatics surrounding connectives.\nOur work also distinguishes itself from prior\nwork on connectives in that we anchor our tests\nin insights and methods from neuro- and psycho-\nlinguistic experiments, which are well controlled\nand designed for assessing linguistic behavior and\ncompetence in a targeted manner. This helps us to\ntease out potential superﬁcial cues that may inﬂate\nperceived levels of pragmatic competence. Modiﬁ-\ncations that we make to the original psycholinguis-\ntic experimental items are furthermore grounded in\nestablished ﬁndings of pragmatic theory.\n3 Experiments\n3.1 Models\nWe apply our tests to examine three classes of pre-\ntrained LMs, testing various size settings within\neach class. For the models analyzed in this paper,\nwe use the implementation of Wolf et al. (2020).\nWe limit our investigation to masked language mod-\nels, since it is necessary that models be able to use\nright-hand context for predicting connectives.\nBERT (Devlin et al., 2019)We experiment with\ntwo variants: BERTBASE (110M parameters), and\nBERTLARGE (340M parameters). For both, we use\nthe uncased version.\nRoBERTa (Liu et al., 2019) We experiment\nwith RoBERTa BASE (125M parameters) and\nRoBERTaLARGE (355M parameters).\nALBERT (Lan et al., 2020) We experi-\nment with version 2 of ALBERT BASE (11M\nparameters), ALBERT LARGE (17M parame-\nters, ALBERT XLARGE (58M parameters) and\nALBERTXXLARGE (223M parameters).\n3.2 Input representation\nFor our inputs, we add a start of sentence token\n([CLS] for BERT, ALBERT; <s> for RoBERTa).\nSeparate sentences of a given input item are sepa-\nrated by a separator token, and the masked word\n369\nModel Accuracy\nBERTBASE 0.47\nBERTLARGE 0.51\nRoBERTaBASE 0.61\nRoBERTaLARGE 0.66\nALBERTBASE 0.42\nALBERTLARGE 0.48\nALBERTXLARGE 0.56\nALBERTXXLARGE 0.57\nTable 1: Connective prediction accuracy on PDTB data\nto be predicted is denoted by [MASK] for BERT\nand ALBERT, and <mask> for RoBERTa. Spe-\ncial tokens are selected for consistency with the\nimplementation of Wolf et al. (2020).2\n4 Predicting connectives in\nnaturally-occurring data\nWe begin by asking, generally speaking, how effec-\ntive pre-trained LMs are at using context to infer\nthe appropriate connective to join components of\na given discourse. With this experiment, we take\nadvantage of large amounts of naturally-occurring\ndata to gauge the general capacities of these models\nto use surrounding information to infer the most\nappropriate discourse connective.\nWe compile 17,476 input items, drawn from in-\nstances in the Penn Discourse Treebank (PDTB-\n2) (Prasad et al., 2008). We select instances based\non presence of explicit discourse connectives, ac-\ncording to dataset annotations. We ﬁlter out any\ninstances in which connectives are multi-word, to\nenable use of a masked single-word prediction set-\nting. For testing, we use a cloze approach, simply\nmasking the discourse connective and assessing\nthe probabilities that the masked language models\nassign to the correct connective given the context.\nThe models receive only a single PDTB instance at\na time as input. We measure prediction accuracy in\nrelative terms: models are considered accurate if\nthey assign a higher probability to the correct con-\nnective than to any other single-word connectives\nin PDTB (66 candidate connectives in total).\nWe note that of course even humans may strug-\ngle to predict many naturally-occurring connec-\n2One reviewer raised a concern about inputs of more than\ntwo clauses/sentences, as in Section 5. Note that such multi-\nsentence inputs are consistent with models’ pre-training, dur-\ning which input “sentences” are not deﬁned by actual sentence\nboundaries, but by selection of arbitrary spans of contiguous\ntext, thus allowing for multi-sentence inputs.\ntives, and a given context may be consistent with\nmultiple discourse relations. 3 This relative lack\nof control over item properties is a tradeoff that\ncomes with use of large-scale naturally-occurring\ndata, but the level of predictability in these items\ncan be assumed to mirror levels of predictability in\nthe models’ normal pre-training. More to the point,\nhowever, the experiments in this section serve only\nas a preliminary assessment of models’ ability to\nuse contextual information to infer discourse rela-\ntions and corresponding connectives used in orig-\ninal texts. Subsequent sections will shift to more\ntargeted tests of how models handle the pragmat-\nics of connectives. Note also that the difﬁculty of\npredicting connectives here would if anything lead\naccuracies in this section to be underestimates—\nbut even with this disadvantage, we will see that\nthese PDTB accuracies still appear to overestimate\nmodels’ actual pragmatic competence. This further\nhighlights the need for more controlled tests.\nTable 1 shows the overall accuracy results for\nthese PDTB items. We see that models prefer\nthe correct connective to other connectives ap-\nproximately half the time—well above chance—\nsuggesting reasonable ability to use contextual cues\nto infer the appropriate discourse connective. Ac-\ncuracy generally improves with model size within\nmodel class, and among the three model classes,\nRoBERTa shows the strongest performance overall.\nIf we take these results at face value, it appears that\nmodels may have a reasonable grasp on pragmatics\nof connectives—and increasing a given model size\nmay improve pragmatic competence still further.\nWhen we break accuracies down by relation\ntypes, however, we ﬁnd that accuracies vary dras-\ntically among different relations. In Table 2\nwe show the accuracy of selected relation types\nof Expansion.Conjunction (which can be sig-\nnalled by connectives like and, also, additionally),\nComparison.Concession.Contra-expectation (sig-\nnalled by connectives like but, however, although),\nTemporal.Asynchronous.Succession (signalled by\nconnectives like after, since, when ) and Causal\n(signalled by so, thus, therefore). Comparing be-\ntween relation types, we see that models (partic-\nularly BERT and RoBERTa) show much higher\n3It has also been observed PDTB-2 includes connectives\nthat can signal more than one discourse relation (Pitler and\nNenkova, 2009; Webber et al., 2019). PDTB-3 tries to resolve\nthis connective ambiguity by introducing new relations in the\nannotations, but for the purposes of our preliminary test here,\nPDTB-2 is sufﬁcient.\n370\nModel Expansion: Asynchronous: Concession: Causal:\nConjunction Succession contra-expectation Result\nBERTBASE 0.73 0.46 0.18 0.20\nBERTLARGE 0.74 0.5 0.25 0.24\nRoBERTaBASE 0.76 0.67 0.43 0.3\nRoBERTaLARGE 0.79 0.71 0.51 0.34\nALBERTBASE 0.42 0.52 0.37 0.09\nALBERTLARGE 0.49 0.61 0.38 0.17\nALBERTXLARGE 0.64 0.62 0.41 0.25\nALBERTXXLARGE 0.59 0.66 0.46 0.29\nTable 2: Connective prediction accuracy on PDTB data, broken down by speciﬁc discourse relations\naccuracy on Expansion.Conjunction, more mod-\nerate performance on Asynchronous.Succession\nand Concession, and generally quite weak perfor-\nmance at predicting Causal connectives. ALBERT\ndeviates somewhat from the pattern of BERT and\nRoBERTa, in that its highest performance is instead\ntypically on Asynchronous.Succession.\nThese slightly lopsided accuracies suggest a pic-\nture in which LMs may make some use of prag-\nmatic contextual cues, but they may also have cer-\ntain common, go-to connectives that serve as proba-\nble predictions across a wide range of contexts. Er-\nror analysis is consistent with this picture—Tables\n9 and 10 in the appendix show percentages of erro-\nneous predictions for which each candidate connec-\ntive is the top-ranked prediction. We see that across\nthe board, BERT and RoBERTa models have high\nrates of preferring and in cases of erroneous pre-\ndiction. ALBERT, by contrast, distributes errors\nacross a wider range of connectives. What we see\nthen, is a picture in which BERT and RoBERTa\nmodels seem to have settled on and as a common\ngo-to connective, contributing to the high accu-\nracy of those models on the Expansion.Conjunction\nrelation—while ALBERT has less of a go-to and\npreference, consistent with ALBERT’s lower accu-\nracy on Expansion.Conjunction, and slightly more\nbalanced accuracies overall.\nWhat do these results mean for our assessment\nof these LMs? The strategic beneﬁt of frequently\npredicting and is clear: and is a versatile, am-\nbiguous discourse connective that can appear in\nmany types of contexts, so it is probably among\nthe safest connective predictions. However, this\nrather coarse-grained predictive behavior suggests\nthat models may not be very sensitive to detailed\npragmatic cues that would enable more speciﬁc\nconnective predictions that ﬁt the contexts more\nprecisely. Since we have not controlled the con-\ntexts on which the models condition here, we can-\nnot make strong claims about the speciﬁc cues that\nmodels may or may not have had access to for each\nof these individual relations. To address this, below\nwe will use controlled sentence contexts aimed at\nisolating pragmatic information for a more targeted\ntest of connective prediction capabilities. The ﬁrst\nof these tests will focus on distinguishing causal\nand concessive connective environments.\nThese PDTB tests are also limited in what they\ncan tell us about models’ understanding of the\nmeanings of connectives—in particular, given mod-\nels’ inclination to over-predict and, it is difﬁcult\nto know the extent to which models have any un-\nderstanding of the implications of this connective\nin context. We will look further into this question\nbelow, by isolating a particular temporal implica-\nture of the connective and, and testing whether\nmodels can predict other temporal connectives re-\nﬂecting the meaning of that implicature. This test\nwill make use of the inﬂuences of two hallmarks of\nimplicature—reinforcement and cancellation—to\ntest models’ pragmatic sensitivities. Finally, we\nwill further probe models’ sensitivity to tempo-\nral dynamics of connectives, by testing whether\nmodels’ connective predictions reﬂect a humanlike\npreference for events to be mentioned in the order\nin which they occur in the real world.\nFor all of these follow-up experiments, we\nwill make use of insights and tests from\npsycholinguistics—this will enable us to execute\nmore controlled and targeted tests, while grounding\nour expectations in observed properties of human\nprocessing and interpretation of connectives. Adap-\ntations of the original experimental items will be\ngrounded in insights from pragmatic theory.\n371\nCondition Example item\nCausal connective\ncontext\nMr. Brown was planning to look for new glasses and shoes today. The\nglasses really are more urgent. [MASK], he now heads towards the\noptician that a friend recommended. (correct target:therefore, so)\nConcessive connec-\ntive context\nMr. Brown was planning to look for new glasses and shoes today. The\nglasses really are more urgent. [MASK], he now heads towards the\nshoe storethat a friend recommended. (correct target:however, but)\nTable 3: Example items from Drenhaus et al. (2014), adapted for testing connective prediction in controlled con-\ntexts. Models should be able to use pragmatic cues to infer appropriateness of causal vs concessive connective.\nModel Conces. Causal Pair\nBERTBASE 0.73 0.27 0\nBERTLARGE 0.6 0.43 0.03\nRoBERTaBASE 0.4 0.46 0\nRoBERTaLARGE 0.43 0.67 0.1\nALBERTBASE 0.9 0.1 0\nALBERTLARGE 0.6 0.4 0.03\nALBERTXLARGE 0.53 0.5 0.07\nALBERTXXLARGE 0.57 0.7 0.3\nTable 4: Prediction accuracy on contexts from Dren-\nhaus et al. (2014). “Conces” = Concessive; “Pair” =\nrate of correct prediction on both sentences of a pair\n5 Predicting connectives with controlled\ncontext\nThe results in Section 4 give a preliminary sense of\nmodels’ behavior in using context to predict con-\nnectives, but it is difﬁcult to discern from these\nuncontrolled data precisely what types of cues the\nmodels may be using to inform connective predic-\ntions. In particular, if we are interested in models’\nability to use high-level pragmatic information to\ninfer the appropriate discourse relation, it is impor-\ntant that we control for lower-level syntactic and\nlexical cues that may be predictive of connectives,\nbut that tell us less about models’ sensitivity to\npragmatics. Previous works have shown that lex-\nical and semantic cues can be used for predicting\nconnectives and discourse relations (Patterson and\nKehler, 2013; Pitler and Nenkova, 2009), and that\ncertain kinds of relations co-occur at rates greater\nthan chance (Pitler et al., 2008), supporting the pos-\nsibility that non-pragmatic cues alone can likely\nlead to strong connective prediction performance.\nIn order to better isolate high-level pragmatic\ncues, we take advantage of sentences designed\nby Drenhaus et al. (2014) for a psycholinguistic\nstudy of human language processing. The original\npsycholinguistic experiment tested how different\ndiscourse connectives facilitate human language\ncomprehension, and the extent to which connec-\ntives can elicit predictions of upcoming content.\nThe experimental items constitute minimal pairs\nwith nearly identical syntax and word content—but\na slight difference late in the context makes it such\nthat a causal connective is appropriate in one ver-\nsion, while a concessive connective is appropriate\nin the other.4 Taking advantage of this controlled\nminimal pair set-up, we adapt the items from this\nstudy to formulate a connective prediction task—\nTable 3 shows examples from these adapted items.\nTo do this task, the models need to identify that\nin one context Mr. Brown’s actions follow what\nis expected from the ﬁrst sentence, while in the\nother context, the actions deviate from what is ex-\npected. Our goal is to test whether models can use\npragmatic reasoning to infer that these contexts are\nconducive to a causal and a concessive connective,\nrespectively. In ﬁltering these items, we again leave\nout connectives that are multi-word. We derive a\ntotal of 30 item pairs for these experiments.5\nWhen testing on these items, we consider a\nmodel to be accurate if, from a list of causal and\nconcessive connectives, the model’s top prediction\nfalls in the correct category (causal versus con-\ncessive). For causal connectives we count any of\n{so, therefore}, and for concessive connectives we\ncount any of {however, instead, nevertheless}.\nTable 4 shows model prediction accuracy on\nthis test. When we look at accuracy for conces-\nsive and causal sentences separately, we see that\ncertain models do appear to have strong perfor-\nmance. However, performance on a single con-\n4Concessive relation is equivalent to PDTB\nCOMPARISON:Concession:contra-expectation; causal\nrelation is equivalent to CONTINGENCY:Cause:result.\n5In each item, the sentence containing the mask token is\nseparated from the preceding sentence with a [SEP] token.\n372\ndition is again susceptible to models simply pre-\nferring certain connectives in both contexts, and\nwe are interested in models’ ability to use subtle\npragmatic information to distinguish the minimal\npairs. Thus, we focus on the proportion of item\npairs on which the models manage to prefer the\ncorrect class of connective in both items of the pair.\nThis is shown in the “Pair” column of Table 4, and\nit is clear that models perform extremely poorly\nby this criterion. Most models hover around 0%\naccuracy—only ALBERT XXLARGE exceeds (nar-\nrowly) the roughly 25% threshold that would be\nexpected by chance. The driving reason for these\nfailures is the fact that for a given minimal pair,\nmodels continue to prefer the same completion in\nboth items, failing to respond to the subtle changes\nin contextual pragmatic cues. On the whole, the\nresults suggest that when we limit models’ cues to\nhigh-level pragmatic information of the type tar-\ngeted in these items, none of these tested models\nhave the capacity to use that information to dis-\ntinguish and predict both causal and concessive\nconnectives. Models’ difﬁculty on this test can also\nbe seen as somewhat consistent with the ﬁndings\nfrom Section 4 that models are weaker in general on\ncausal and concessive connectives—however, the\nmuch more dramatic failure here may indicate that\nwhere we do see correct predictions in Section 4,\nthose predictions may be informed by shallower\ncues, rather than subtle pragmatic information.\n6 Discourse connectives and implicature\nSection 4 suggests that models are quick to pre-\ndict and in contexts that generally support a dis-\ncourse connective. Here we test the extent to which\nmodels understand what the connective and ac-\ntually means—more speciﬁcally, we examine the\nextent to which models pick up on temporal impli-\ncatures that humans commonly interpret as part of\nthe meaning of and. This section will again use\ncontrolled minimal pairs of contexts, inspired by\nﬁndings in linguistics and psycholinguistics.\nOur tests here make use of the pragmatic no-\ntion of implicature: non-literal, enriched meaning\nof words, phrases, or sentences generated by in-\nferences about speaker intent. For instance, the\nsentence “Some people have pets” literally means\nthat “there are a non-zero number of people who\nhave pets”. In addition to this literal meaning, a\ncommon implicature in interpretation of this sen-\ntence is that “Some but not all people have pets”.\nWe focus on an implicature generated by the con-\nnective and when joining two events—namely, the\nimplicature that and actually means and then (i.e.,\nthe two events are being mentioned in the temporal\norder in which they actually occurred). This has\nbeen studied by Carston (1988), noting the oddness\nof sentences like “Jane got into bed and brushed her\nteeth”, which seems to carry the clear implication\nthat Jane brushed her teeth in bed. Noveck and\nChevaux (2002) also study this implicature in chil-\ndren, ﬁnding that compared to younger children,\nolder children and adults generate more and then\nimplicatures when events are joined by and. Our\ntests will focus ﬁrst on whether models seem to be\nsensitive to this and then implicature.\nTo test whether models pick up on this impli-\ncature, we cannot simply test models’ aptitude at\npredicting the connective and in context. Instead,\nwe make use of two additional hallmarks of im-\nplicatures (Grice, 1975, 1989): 1) that they can\nbe reinforced with, e.g., “which is to say”, and 2)\nthat they can be canceled with, e.g., “in fact”. (For\ninstance, we can reinforce the “some but not all”\nimplicature above by saying, “Some people have\npets—which is to say, some, but not all , people\nhave pets”. Alternatively, we can cancel the “some\nbut not all” implicature by saying “Some people\nhave pets—in fact, all people have pets”.) These\ntests are well established in the linguistics literature\nfor teasing apart implied meanings from compo-\nsitional truth conditions (Fox, 2007; Katzir, 2007;\nGeurts, 2010; Chierchia et al., 2012; Sauerland,\n2004; Sadock, 1978; Rett, 2014). We will leverage\nsentences featuring reinforcement and cancellation\nof the and then implicature to test our models.\nWe create a dataset of item pairs containing\nevents joined by and, followed by a reinforcement\nor cancellation. Table 5 shows example items. We\ndraw our events from the stimuli of Politzer-Ahles\net al. (2017), which tested humans’ sensitivity to\ntemporal order of events (in this section we simply\ninsert events from those stimuli into sentences of\nour chosen structure—in Section 7 we will make\nmore direct use of the stimuli from that study). We\ncreate 160 item pairs in total for use in this test.\nWhen we examine model predictions in the\nmasked positions of these items, the critical ques-\ntion is the relative probability that they assign to\ncompletions of before versus after. If models gen-\nerate the natural and then implicature for the con-\nnective and (and if they have the pragmatic compe-\n373\nCondition Example item\nReinforcement test Maggie did the paperwork by hand and the company bought new com-\nputers, which is to say, Maggie did the paperwork by hand [MASK] the\ncompany bought new computers. (ideal target probs:before > after)\nCancellation test Maggie did the paperwork by hand and the company bought new com-\nputers, in fact, Maggie did the paperwork by hand [MASK] the company\nbought new computers. (ideal target probs:after > before)\nTable 5: Example items for testing interpretation of and then implicature, using reinforcement and cancellation\nModel Cancel. Reinf. Pair\nBERTBASE 0.52 0.55 0.08\nBERTLARGE 0.24 0.79 0.04\nRoBERTaBASE 0.69 0.49 0.18\nRoBERTaLARGE 0.61 0.21 0.04\nALBERTBASE 0.66 0.3 0.02\nALBERTLARGE 0.83 0.37 0.2\nALBERTXLARGE 0.06 0.98 0.04\nALBERTXXLARGE 0.09 0.89 0.06\nTable 6: Prediction accuracy in implicature test, with\ncancellation and reinforcement settings\ntence to understand the effects of “which is to say”\nand “in fact”), then they should prefer before in\nthe case of reinforcement, and after in the case of\ncancellation. This approach to assessment is partic-\nularly important because these contexts are some-\nwhat complex, and the human standard to which we\nare comparing models in this case is not direct hu-\nman performance on these items, but rather related\nresults and theoretical foundation in the pragmatics\nliterature. For these reasons we seek to maximize\nthe fairness of the test through use of this relative\naccuracy: to be considered correct, models need\nonly prefer the temporal relation that better ﬁts the\ninvoked implicature, over the temporal relation that\nclashes with the invoked implicature.\nTable 6 shows the results. The “Cancel.” column\nshows the proportion of in fact items in which the\nmodel assigns higher probability to after than to\nbefore, and the “Reinf.” column shows the propor-\ntion of which is to say items in which the model\nassigns higher probability to before than to after.\nAs before, these single-condition accuracies are\nsusceptible to models simply preferring one con-\nnective across contexts, so we are most interested\nin the “Pair” column, which shows the proportion\nof minimal pairs (reinforcement + cancellation ver-\nsion) in which the model assigns higher probability\nto the correct target for both items.\nAs in Section 5, we see that although models\nmay make correct predictions on individual items,\ntheir ability to choose the correct connective in\nboth items of a pair is very limited. Chance-level\nperformance on this criterion is 25%, and it is clear\nthat most models are performing substantially be-\nlow chance level—and even the highest accuracies\nremain slightly below chance. The results indicate\nthat this form of pragmatic competence—inferring\ntemporal implicature and deploying it in cancel-\nlation and reinforcement environments—remains\noutside of models’ current capacity.\n6.1 Sensitivity to redundancy and\ncontradiction\nAs a follow-up to the implicature test above, we\nalso test how model behaviors change when the\nimplied meaning is made explicit. We pair each\nitem from the prior experiment (Table 5) with a\ncounterpart containing and then in place of and.\nThis time we focus on a single candidate prediction\nword at once, and compare the probability of that\nword in and versus and then versions of a given\nitem. In cancellation conditions, we examine how\nmodels rate a target of after—models should as-\nsign higher probability to after when the sentence\ncontains only and, because and then is contradic-\ntory with an after interpretation. In reinforcement\nsentences, we examine model probabilities for be-\nfore—models should assign higher probability to\nbefore in reinforcement sentences with only and,\nbecause restating that an event X happened before\nan event Y is redundant if and then is stated ex-\nplicitly. Example items from this test are listed in\nappendix Table 11. We create 320 sentence pairs\nin total for this test.6 We count model predictions\nas correct if the target word of interest is assigned\nhigher probability in the appropriate context than\n6The number of items is doubled relative to the previous\nexperiment because we now have both an and then version\nand an and version of each of the original items.\n374\nCondition Example item\nSentence-initial [MASK] the campaign ﬁnance laws changed, Albert ran for mayor of his\ncity. (temporal order preference:after > before)\nSentence-medial Albert ran for mayor of his city [MASK] the campaign ﬁnance laws\nchanged. (temporal order preference:before > after)\nTable 7: Example items from Politzer-Ahles et al. (2017), adapted for testing whether models prefer connectives\nthat indicate events are being mentioned in their chronological order. When connective is at the beginning of the\nsentence, after indicates that events will be mentioned in chronological order. When the connective is in the middle\nof the sentence, chronological ordering is signalled by before.\nModel Initial Medial Pair\nBERTBASE 0.93 0.45 0.38\nBERTLARGE 0.91 0.48 0.39\nRoBERTaBASE 0.85 0.39 0.24\nRoBERTaLARGE 0.86 0.4 0.26\nALBERTBASE 0.95 0.46 0.41\nALBERTLARGE 0.95 0.24 0.2\nALBERTXLARGE 0.75 0.54 0.31\nALBERTXXLARGE 0.68 0.56 0.26\nTable 8: Model preferences forafter/before in sentence-\ninitial and sentence-medial position. “Initial” shows\npercentage of sentence-initial predictions that prefer\nafter, while “Medial” shows percentage of sentence-\nmedial predictions that prefer before.\nin the inappropriate context, as outlined above.\nFor the sake of space, we show the results in\nTable 12 of the appendix. Model performance is ex-\ntremely weak across the board, with models assign-\ning higher probability in the better context no more\nthan 3% of the time—except for RoBERTa LARGE\nin Reinforcement conditions, at 21% (chance level\nof 50%). Overall, the results suggest that mod-\nels are sensitive neither to contradiction nor to\nredundancy—or at least that they prefer sentences\nwith these properties over sentences featuring rein-\nforcement and cancellation of an implicature.\n7 Sensitivity to event order and\ncorresponding connectives\nThe previous section tested whether models, like\nhumans, infer that a connective and joining two\nevents has an implied meaning of and then. A re-\nlated ﬁnding in psycholinguistics is from Politzer-\nAhles et al. (2017), who examine the effect of be-\nfore and after clauses on sentence processing in\nboth sentence-initial contexts and sentence-medial\ncontexts. Their results provide further evidence\nthat human brains have a preference for events to\nbe mentioned in chronological order. We leverage\nthis existing experiment to probe whether models,\nin their connective predictions, show similar pref-\nerences for events to be mentioned in the order in\nwhich they occur in real life. Importantly, this test\ndiffers from the above tests in that models cannot\nbe said to behave “correctly” or “incorrectly”, on\nthe basis of the presence/absence of this preference.\nRather, this serves as a more general test of whether\nmodels’ connective predictions reﬂect humanlike\ntrends with respect to temporal implications.\nWe adapt the Politzer-Ahles et al. (2017) mate-\nrials to form a connective prediction task. Exam-\nples are shown in Table 7. The central question is\nwhether models will prefer connectives that imply\nthat events are being mentioned in chronological or-\nder. Models are considered to have this preference\nif they assign higher probability to after (relative\nto before) in sentence-initial position, and to before\n(relative to after) in sentence-medial position.\nTable 8 shows the results. The patterns suggest\nthat models strongly prefer after at the start of the\nsentence, consistent with a preference for events to\nbe mentioned chronological order. However, the\nmodels don’t show the same level of preference for\nbefore in the middle of the sentence, showing in-\nstead more of an even split between the two connec-\ntives. The “Pair” column indicates the percentage\nof pairs in which models show the target prefer-\nence on both items. Half of models fall just about\nat chance level of 25%. The other half of models\nexceed this chance-level percentage—particularly\nthe BERT models and ALBERT BASE. However,\nthe percentages never exceed 41%, suggesting that\nwhile some models may trend a bit toward this\ntemporal ordering preference in their connective\npredictions, this trend is fairly weak.\nAs we have established above, models’ lack of\nthe target trend in this experiment cannot be con-\nsidered “incorrect” in any sense—however, it does\n375\nindicate that models lack yet another humanlike pat-\ntern with respect to processing of discourse connec-\ntives, where such a pattern would have suggested\nﬁner-grained sensitivity to temporal dynamics.\n8 Discussion\nIn the above experiments, we have studied the com-\npetence of pre-trained LMs in predicting and inter-\npreting connectives. Examining connective predic-\ntion in naturally-occurring data suggests that mod-\nels may have certain go-to connectives that they\npredict across a variety of contexts. So we turn to\nmore controlled tests inspired by psycholinguistics,\nto examine the extent to which models’ handling\nof connectives reﬂects pragmatic competence. The\nresults of these controlled tests suggest that mod-\nels are not yet equipped to use subtle pragmatic\ncues to inform connective predictions—whether\nthe test involves distinguishing causal from conces-\nsive discourse relations in settings of high syntactic\nand lexical overlap, or making predictions based\non inference, reinforcement, or cancellation of im-\nplicatures. We also ﬁnd that models appear to be\ninsensitive to redundancy or contradiction, and that\nalthough certain models may have a slight tendency\nto prefer connectives that suggest chronological\nevent mentions, this tendency is weak at best.\nVariation between models is fairly minor, though\nsome RoBERTa and ALBERT variants at times\ndistinguish themselves in coming closer to chance-\nlevel performance when other models are close to\n0% accuracy. This could be attributable to larger\npre-training data in the case of RoBERTa, and in\nthe case of ALBERT, we speculate that some bene-\nﬁt may be derived from the sentence order predic-\ntion loss (Lan et al., 2020), which may encourage\nsensitivity to certain discourse dynamics. How-\never, it is important to note that in these cases the\nmodels still at best barely surpass chance-level per-\nformance, so this does not indicate any particularly\nstrong pragmatic competence from these models.\nWhy do models perform so poorly on the con-\ntrolled tests? It is clear, of course, that these mod-\nels have not been trained to do these speciﬁc tasks.\nHowever, if models have competence in pragmatic\nreasoning, that competence should be reﬂected in\nthe preferences and distinctions tested for here. Our\nuse of minimal pairs creates a particular challenge,\nin reducing models’ capacity to succeed on the ba-\nsis of shallower types of cues—syntactic, lexical—\nthat they are more likely to have learned to priori-\ntize. However, this means that these tests hopefully\ngive us a clearer look at models’ ability to use prag-\nmatic cues per se. All in all, our results point to a\nsituation in which sophisticated pragmatic reason-\ning is not yet a property of current models—at least\nnot the aspects of pragmatics tested for here.\nWhat are the implications of these results for\nour approach to these models? In terms of gaug-\ning linguistic competence that emerges from pre-\ntraining, the results suggest that word prediction\nobjectives alone may not sufﬁce to force models\nto learn nuances of pragmatic reasoning. Models\ntested here, at least, do not suggest that such prag-\nmatic reasoning has been learned. The results on\nnaturally-occurring data may suggest one source\nof pre-training limitations: models may be able to\nachieve reasonable prediction outcomes simply by\ndefaulting to versatile connectives in a wide range\nof contexts, and by relying on lower-level syntactic\nand lexical cues. For models to learn pragmatics, it\nmay be necessary to reduce reliability of shallower\ncues, scaffold learning with more meaning-rich\nsupervision, or both. Enriched pragmatic compe-\ntence may be achievable through ﬁne-tuning of\npre-trained models, but a ﬁne-tuning approach will\nbe subject to the same risks of models defaulting to\nshallower heuristics—so the same considerations\nwill apply. We leave the problem of improving\nmodels’ pragmatic competence for future work.\n9 Conclusion\nThe above experiments have examined pragmatic\ncompetence in pre-trained language models, with\nrespect to discourse connectives. Results suggest\nthat models are not yet equipped to use high-level\npragmatic cues or reasoning to guide predictive be-\nhaviors, even if they show reasonable predictive\naccuracy in naturally-occurring data. We suggest\nthat arriving at more pragmatically competent mod-\nels may require greater control of shallow cues,\nor use of more meaning-rich training signal. We\nhope that this work will help to shed light on the\nlinguistic competence of pre-trained LMs, and ulti-\nmately contribute to advancement in the pragmatic\ncompetence of models in NLP.\nAcknowledgments\nWe would like to thank three anonymous reviewers\nfor helpful comments and suggestions. This mate-\nrial is based upon work supported by the National\nScience Foundation under Award No. 1941160.\n376\nReferences\nRobyn Carston. 1988. Implicature, explicature, and\ntruth-theoretic semantics. In Ruth M. Kempson, edi-\ntor, Mental representations: The interface between\nlanguage and reality , pages 155–181. Cambridge\nUniv. Press.\nGennaro Chierchia, Danny Fox, and Benjamin Spec-\ntor. 2012. Scalar implicature as a grammatical phe-\nnomenon. In Semantics, volume 3, pages 2297–\n2331. Mouton de Gruyter.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT (1).\nH. Drenhaus, V . Demberg, J. Koehne, and F. Delogu.\n2014. Incremental and predictive discourse process-\ning based on causal and concessive discourse mark-\ners: Erp studies on german and english. In Proceed-\nings of the Annual Meeting of the Cognitive Science\nSociety, 36.\nDanny Fox. 2007. Free choice and the theory of scalar\nimplicatures. In Uli Sauerland and Penka Stateva,\neditors, Presupposition and Implicature in Compo-\nsitional Semantics, pages 71–120. Palgrave Macmil-\nlan.\nB. Geurts. 2010. Quantity Implicatures. Cambridge\nUniversity Press.\nH. P. Grice. 1989. Studies in the Way of Words. ACLS\nHumanities E-Book. Harvard University Press.\nH.P. Grice. 1975. Logic and conversation. In Peter\nCole and Jerry L. Morgan, editors, Syntax and Se-\nmantics, volume 3, pages 41–58. Academic Press,\nNew York.\nRoni Katzir. 2007. Structurally-deﬁned alternatives.\nLinguistics and Philosophy, 30(6):669–690.\nFajri Koto, Jey Han Lau, and Timothy Baldwin. 2021.\nDiscourse probing of pretrained language models.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 3849–3864, Online. Association for Compu-\ntational Linguistics.\nMurathan Kurfalı and Robert Östling. 2021. Prob-\ning multilingual language models for discourse. In\nProceedings of the 6th Workshop on Representation\nLearning for NLP (RepL4NLP-2021) , pages 8–19,\nOnline. Association for Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. ICLR 2020.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 3428–3448.\nAllen Nie, Erin Bennett, and Noah Goodman. 2019.\nDisSent: Learning sentence representations from ex-\nplicit discourse relations. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4497–4510, Florence,\nItaly. Association for Computational Linguistics.\nI Noveck and Florelle Chevaux. 2002. The pragmatic\ndevelopment of and. In BUCLD Proceedings, vol-\nume 26, pages 453–463.\nGary Patterson and Andrew Kehler. 2013. Predicting\nthe presence of discourse connectives. In Proceed-\nings of the 2013 Conference on Empirical Methods\nin Natural Language Processing , pages 914–923,\nSeattle, Washington, USA. Association for Compu-\ntational Linguistics.\nEmily Pitler and A. Nenkova. 2009. Using syntax to\ndisambiguate explicit discourse connectives in text.\nIn ACL.\nEmily Pitler, Mridhula Raghupathy, Hena Mehta,\nA. Nenkova, Alan Lee, and A. Joshi. 2008. Easily\nidentiﬁable discourse relations. In COLING.\nStephen Politzer-Ahles, Ming Xiang, and Diogo\nAlmeida. 2017. \"before\" and \"after\": Investigat-\ning the relationship between temporal connectives\nand chronological ordering using event-related po-\ntentials. PloS one, 12(4):e0175199.\nRashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-\nsakaki, Livio Robaldo, Aravind Joshi, and Bon-\nnie Webber. 2008. The Penn Discourse TreeBank\n2.0. In Proceedings of the Sixth International\nConference on Language Resources and Evaluation\n(LREC’08), Marrakech, Morocco. European Lan-\nguage Resources Association (ELRA).\nJ. Rett. 2014. The Semantics of Evaluativity . Oxford\nStudies in Theoretical Linguistics. OUP Oxford.\nJ.M. Sadock. 1978. Pragmatics. In P. Cole, editor,\nPresupposition and Implicature in Compositional\nSemantics, volume 9, pages 281––298. Academic\nPress.\nUli Sauerland. 2004. Scalar implicatures in complex\nsentences. Linguistics and Philosophy , 27(3):367–\n391.\n377\nDamien Sileo, Tim Van De Cruys, Camille Pradel,\nand Philippe Muller. 2019. Mining discourse mark-\ners for unsupervised sentence representation learn-\ning. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n3477–3486, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nKoustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle\nPineau, Adina Williams, and Douwe Kiela. 2021.\nMasked language modeling and the distributional\nhypothesis: Order word matters pre-training for lit-\ntle. arXiv preprint arXiv:2104.06644.\nBonnie Webber, Rashmi Prasad, and Alan Lee. 2019.\nAmbiguity in explicit discourse connectives. In Pro-\nceedings of the 13th International Conference on\nComputational Semantics - Long Papers, pages 134–\n141, Gothenburg, Sweden. Association for Computa-\ntional Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\n10 Appendix\n378\nConnective BERTBASE BERTLARGE RoBERTaBASE RoBERTaLARGE\nafter 0.0206 0.0239 0.0546 0.0475\nalso 0.0278 0.0282 0.0284 0.0305\nalthough 0.0126 0.0147 0.0068 0.0097\nand 0.4834 0.4591 0.3503 0.34\nas 0.0623 0.0616 0.0819 0.0928\nbecause 0.068 0.0722 0.074 0.0659\nbefore 0.0098 0.0102 0.0122 0.0153\nbut 0.0354 0.0461 0.0927 0.1106\nfor 0.0172 0.0149 0.0098 0.0067\nif 0.0567 0.0427 0.0373 0.0265\nsince 0.0091 0.0108 0.0071 0.0104\nso 0.009 0.0113 0.0149 0.0114\nstill 0.0121 0.011 0.0133 0.0111\nthen 0.0108 0.0123 0.0109 0.0107\nthough 0.0037 0.0031 0.0128 0.0151\nuntil 0.01 0.0115 0.008 0.0084\nwhen 0.0844 0.0769 0.0625 0.0591\nwhile 0.037 0.0521 0.0727 0.0825\nyet 0.0012 0.0019 0.0028 0.0027\nTable 9: Error percentage of connectives for BERT and RoBERTa family\nConnective ALBERTBASE ALBERTLARGE ALBERTXLARGE ALBERTXXLARGE\nafter 0.0457 0.0782 0.0511 0.0263\nalso 0.0281 0.0275 0.0361 0.0161\nalthough 0.0096 0.0215 0.0437 0.1\nand 0.0285 0.067 0.15 0.0512\nas 0.018 0.0373 0.0409 0.0102\nbecause 0.11 0.23 0.19 0.15\nbut 0.16 0.16 0.12 0.09\nfor 0.0135 0.0094 0.0153 0.0049\nif 0.0565 0.0504 0.047 0.0232\nseparately 0.0041 0.0012 0.0023 0.0127\nsince 0.0069 0.0374 0.0127 0.0098\nso 0.0022 0.0067 0.0097 0.0058\nspeciﬁcally 0.0001 0.0011 0.0003 0.0013\nstill 0.0166 0.0141 0.0119 0.0124\nthen 0.0156 0.0113 0.0118 0.0069\nthough 0.0053 0.0085 0.0071 0.0062\nultimately 0.0002 0.0014 0.0009 0.0048\nunless 0.17 0.0456 0.0121 0.0619\nuntil 0.0057 0.0136 0.0095 0.0128\nwhen 0.10 0.0626 0.09 0.09\nwhereas 0.0228 0.0116 0.0023 0.0343\nwhile 0.11 0.0469 0.0622 0.12\nTable 10: Error percentage of connectives for ALBERT family\n379\nCondition Example item\nReinforcement test The wind dispersed the sheep and the wolves seized a lamb, which is to\nsay, the wind dispersed the sheep [MASK] the wolves seized a lamb.(ideal\ntarget probs:before ≫ after)\nThe wind dispersed the sheep and thenthe wolves seized a lamb, which\nis to say, the wind dispersed the sheep [MASK] the wolves seized a lamb.\n(ideal target probs:before > after)\nCancellation test The wind dispersed the sheep and the wolves seized a lamb, in fact the\nwind dispersed the sheep [MASK] the wolves seized a lamb. (ideal target\nprobs: after ≫ before)\nThe wind dispersed the sheep and thenthe wolves seized a lamb, in fact\nthe wind dispersed the sheep [MASK] the wolves seized a lamb. (ideal\ntarget probs:after > before)\nTable 11: Example items for testing models’ sensitivity to redundancy and contradiction, using implicature rein-\nforcement and cancellation environments (≫(models assigned probability) much bigger than; > bigger than)\nModel Cancellation Reinforcement Pair\nBERTBASE 0.0063 0.0125 0.0093\nBERTLARGE 0 0 0\nRoBERTaBASE 0 0 0\nRoBERTaLARGE 0.0063 0.21 0.0031\nALBERTBASE 0.025 0.006 0.02\nALBERTLARGE 0 0 0\nALBERTXLARGE 0.01 0.03 0.02\nALBERTXXLARGE 0 0.07 0.04\nTable 12: Accuracy in tests of sensitivity to contradiction and redundancy in cancellation and reinforcement envi-\nronments",
  "topic": "Pragmatics",
  "concepts": [
    {
      "name": "Pragmatics",
      "score": 0.8062798976898193
    },
    {
      "name": "Computer science",
      "score": 0.649093508720398
    },
    {
      "name": "Psycholinguistics",
      "score": 0.6488944888114929
    },
    {
      "name": "Competence (human resources)",
      "score": 0.533447802066803
    },
    {
      "name": "Natural language processing",
      "score": 0.5301167964935303
    },
    {
      "name": "Focus (optics)",
      "score": 0.5221105813980103
    },
    {
      "name": "Language understanding",
      "score": 0.4883818030357361
    },
    {
      "name": "Linguistics",
      "score": 0.4544345736503601
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4533233046531677
    },
    {
      "name": "Language model",
      "score": 0.43053779006004333
    },
    {
      "name": "Psychology",
      "score": 0.40665027499198914
    },
    {
      "name": "Cognitive psychology",
      "score": 0.37022095918655396
    },
    {
      "name": "Cognition",
      "score": 0.179203599691391
    },
    {
      "name": "Social psychology",
      "score": 0.08721083402633667
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I87216513",
      "name": "Michigan State University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I40347166",
      "name": "University of Chicago",
      "country": "US"
    }
  ]
}