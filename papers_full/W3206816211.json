{
    "title": "UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning",
    "url": "https://openalex.org/W3206816211",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2796257948",
            "name": "Yuning Mao",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A2266316079",
            "name": "Lambert Mathias",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2015690311",
            "name": "Rui Hou",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2224545695",
            "name": "Amjad Almahairi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2097317532",
            "name": "Hao Ma",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2103606203",
            "name": "Jiawei Han",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A2983023937",
            "name": "Scott Yih",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A296516693",
            "name": "Madian Khabsa",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3198571508",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W2150884987",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3152956381",
        "https://openalex.org/W3174702398",
        "https://openalex.org/W4286981949",
        "https://openalex.org/W3153675281",
        "https://openalex.org/W3171188212",
        "https://openalex.org/W4293718192",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W3176693010",
        "https://openalex.org/W3101498587",
        "https://openalex.org/W3115894062",
        "https://openalex.org/W3153427360",
        "https://openalex.org/W3173777717",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W4288026527",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W3173788106",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W3099793224",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W2989195139",
        "https://openalex.org/W4287122891",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3176828726"
    ],
    "abstract": "Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Scott Yih, Madian Khabsa. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
    "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 6253 - 6264\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nUNIPELT: A Uniﬁed Framework for Parameter-Efﬁcient\nLanguage Model Tuning\nYuning Mao1∗, Lambert Mathias2, Rui Hou2, Amjad Almahairi2,\nHao Ma2, Jiawei Han1, Wen-tau Yih2, Madian Khabsa2\n1University of Illinois Urbana-Champaign {yuningm2, hanj}@illinois.edu\n2Meta AI {mathiasl, rayhou, aalmah, haom, scottyih, mkhabsa}@fb.com\nAbstract\nRecent parameter-efﬁcient language model\ntuning (PELT) methods manage to match the\nperformance of ﬁne-tuning with much fewer\ntrainable parameters and perform especially\nwell when training data is limited. However,\ndifferent PELT methods may perform rather\ndifferently on the same task, making it non-\ntrivial to select the most appropriate method\nfor a speciﬁc task, especially considering the\nfast-growing number of new PELT methods\nand tasks. In light of model diversity and\nthe difﬁculty of model selection, we propose a\nuniﬁed framework, UNIPELT, which incorpo-\nrates different PELT methods as submodules\nand learns to activate the ones that best suit\nthe current data or task setup via gating mech-\nanism. On the GLUE benchmark, U NIPELT\nconsistently achieves 1~4% gains compared to\nthe best individual PELT method that it incor-\nporates and outperforms ﬁne-tuning under dif-\nferent setups. Moreover, U NIPELT generally\nsurpasses the upper bound that takes the best\nperformance of all its submodules used indi-\nvidually on each task, indicating that a mixture\nof multiple PELT methods may be inherently\nmore effective than single methods.1\n1 Introduction\nAs pre-trained language models (PLMs) (Devlin\net al., 2019) grow larger and larger (Brown et al.,\n2020), it becomes increasingly infeasible to per-\nform conventional ﬁne-tuning, where separate repli-\ncas of the model parameters are modiﬁed per single\ntask. To solve the issue, there has recently been\na surge of studies on parameter-efﬁcient language\nmodel tuning (PELT), namely how to effectively\ntune the PLMs with fewer trainable parameters.\nExisting PELT research generally aims at achiev-\ning performance comparable to ﬁne-tuning with\n∗Work was done during internship at Meta AI.\n1Our code can be found at https://github.com/\nmorningmoni/UniPELT.\nAdapter\nMulti-Head Attention\nAdd & Norm\nFeedforward\nAdd & Norm\nAdd & Norm\n+\nPreﬁx-tuning\nhin\nhFN\nhA\nhF\nLoRA\n+ +\nQ K V\nWQ WVWK\nWDown\nWUp\nPK PV\nWUpWUp\nWDown WDown\nG L\nG A\nG P\nFigure 1: Illustration of U NIPELT, which subsumes\nexisting PELT methods as submodules and controls\nthem via gating mechanismG. Different (combinations\nof) submodules can be activated for different samples.\nThe trainable parameters are shown in blue.\nas few trainable parameters as possible, which has\nseen signiﬁcant progress – the task-speciﬁc train-\nable parameters used in most recent approaches\n(Lester et al., 2021; Guo et al., 2021) are almost\nnegligible compared to the total parameters of the\nPLM (<1%). A more challenging yet less studied\nproblem is whether one can achieve better perfor-\nmance than ﬁne-tuning with fewer parameters. Re-\ncent studies (He et al., 2021; Li and Liang, 2021;\nKarimi Mahabadi et al., 2021b) ﬁnd that some\nPELT methods are more effective than ﬁne-tuning\non certain tasks when training data is limited, possi-\nbly due to the reduced risk of overﬁtting. However,\nas found in our experiments (Table 1), different\nPELT methods exhibit diverse characteristics and\nperform rather differently on the same task, which\n6253\nmakes it nontrivial to select the most appropriate\nmethod for a speciﬁc task, especially considering\nthe fast-growing number of new PELT methods\nand tasks (Ding and Hu, 2021).\nIn light of the diverse performance of PELT\nmethods and the cost of selecting the best method,\nwe propose a uniﬁed PELT framework, named\nUNIPELT, which incorporates different PELT\nmethods as submodules and learns to dynamically\nactivate the (combination of) submodules that best\nsuit the current data or task setup. As a result,\nmodel selection is no longer needed and consis-\ntently better performance is achieved under dif-\nferent setups. The activation of each submodule\nin UNIPELT is controlled by gating mechanism,\nwhich learns to favor (assign more weight to) the\nsubmodules that positively contribute to a given\ntask. In addition, since the number of parameters\nintroduced by each submodule is generally small,\ncombining multiple methods leads to negligible\nlosses in model efﬁciency.\nWe select four representative PELT methods for\nour study – adapter (Houlsby et al., 2019), preﬁx-\ntuning (Li and Liang, 2021), LoRA (Hu et al.,\n2021), and BitFit (Ben Zaken et al., 2021), which\nlargely cover the major categories of PELT meth-\nods. We perform two sets of analysis that carefully\nexamines (i) the characteristics of individual PELT\nmethods and (ii) their effectiveness when coordi-\nnated by UNIPELT under various setups.2\nExtensive experiments on the GLUE bench-\nmark (Wang et al., 2019), with 32 setups (8 tasks\n×4 data sizes) and 1,000+ runs, not only reveal the\ndiverse behavior of PELT methods, but also show\nthat UNIPELT is more effective and robust than\nusing each method alone in various task and data se-\ntups. Speciﬁcally, UNIPELT consistently improves\nthe best submodule that it incorporates by 1~4\npoints and even outperforms ﬁne-tuning, achieving\nthe best average performance on the GLUE bench-\nmark under different setups. Moreover, UNIPELT\ngenerally surpasses the upper bound that takes the\nbest performance of all its submodules used individ-\nually on each task, which suggests that UNIPELT\nmaintains (near) optimal performance under differ-\nent setups. The fact that UNIPELT outperforms the\nupper bound also implies that a mixture of PELT\nmethods involving different parts of the PLM ar-\nchitecture may be inherently more effective than\n2BitFit is not included inUNIPELT as it typically performs\nthe worst in our preliminary experiments.\nindividual methods.\nContributions. (1) We conduct a comprehensive\nstudy of representative PELT methods and care-\nfully examine their differences and commonalities\nin terms of performance and characteristics. (2)\nWe propose a uniﬁed PELT framework that can\nincorporate existing methods as submodules and\nautomatically learn to activate the appropriate sub-\nmodules for a given task. (3) Our proposed frame-\nwork achieves better average performance than ﬁne-\ntuning and the PELT methods that it incorporates\nunder various setups, often performing the best and\nnever the worst at per-task level, exhibiting supe-\nrior effectiveness and robustness with negligible\nlosses in model efﬁciency.\n2 Preliminaries\n2.1 PELT Methods without Additional\nParameters\nPLMs can be used as feature extractors where only\nthe top layers or prediction head are ﬁne-tuned\nwithout additional parameters (Lee et al., 2019).\nHowever, such ﬁne-tuning approaches generally\nlead to degenerate model performance that is much\nworse than ﬁne-tuning all parameters (Lee et al.,\n2019; Pfeiffer et al., 2021). A recent method BitFit\n(Ben Zaken et al., 2021) only tunes the bias terms\nof the PLM and is shown to achieve performance\ncomparable to ﬁne-tuning on certain tasks when\ntraining data is limited. Therefore, we select BitFit\nas the representative of this category for analysis.\n2.2 PELT Methods with Additional\nParameters\nAlternatively, one may ﬁx the entire PLM and intro-\nduce a small number of new trainable parameters.\nNotable examples in this category include adapter\n(Houlsby et al., 2019) and its extensions (Pfeif-\nfer et al., 2021; Karimi Mahabadi et al., 2021b),\npreﬁx-tuning (Li and Liang, 2021) and its exten-\nsions (Lester et al., 2021), and additive methods\n(Guo et al., 2021; Hu et al., 2021).\nNext, we will brieﬂy describe these methods to\nfacilitate the introduction of our proposed frame-\nwork. An illustration is shown in Fig. 1 for better\nunderstanding.\nAdapter. Adapter (Houlsby et al., 2019) adds a\ntrainable bottleneck layer after the feedforward net-\nwork in each Transformer layer of the PLM. A bot-\ntleneck layer consists of a down+up projection pair\nthat shrinks and recovers the size of token hidden\n6254\nstates. Mathematically, if we denote the output of\nthe feedforward network after residual connection\nand layer normalization as hFN with hidden size\nDhidden and bottleneck size Dmid, then the output\nof a bottleneck layer hA is:\nhA = W⊺\nupφ(W⊺\ndownhFN ), (1)\nwhere Wdown ∈ RDhidden×Dmid , Wup ∈\nRDmid×Dhidden , φis a nonlinear activation function,\nand the bias terms are omitted for brevity. The pa-\nrameters in layer normalization and the ﬁnal predic-\ntion head sometimes are also ﬁne-tuned depending\non the speciﬁc adapter variants.\nAdapter has shown to be on par with ﬁne-tuning\nand sometimes exhibits better effectiveness in the\nlow-resource setting (He et al., 2021). Later stud-\nies extend adapter to multi-lingual (Pfeiffer et al.,\n2020b) and multi-task (Karimi Mahabadi et al.,\n2021b) settings, or further reduce its trainable pa-\nrameters (Karimi Mahabadi et al., 2021a), which\ncan be easily incorporated into UNIPELT as a re-\nplacement of the vanilla adapter.\nPreﬁx-tuning. Preﬁx-tuning (Li and Liang, 2021)\nprepends a number of task-speciﬁc trainable vec-\ntors to the input of multi-head attention in each\nTransformer layer, which the original tokens can at-\ntend to as if they were virtual tokens. Speciﬁcally,\nwe denote the original sequence length L0, the\nnumber of trainable vectors (i.e., preﬁx length) L,\nand the Transformer layer input hin ∈RDhidden×L0 .\nFirst, three linear projections WQ, WK, WV ∈\nRDhidden×Dhidden transform hin into Query Q, Key\nK, and Value V . Then, two preﬁx matrices PK\nand PV ∈RDhidden×L are prepended to K and V .\nTo stabilize optimization, the preﬁx matrix P is\nreparameterized by a feedforward network:\nP′= W⊺\nupφ(W⊺\ndownP), (2)\nwhere Wdown ∈ RDhidden×Dmid , Wup ∈\nRDmid×2NlayerDhidden , and Nlayer denotes the number\nof Transformer layers. The parameters of this\nnetwork can be discarded after training, and only\n2Nlayer preﬁx matrices ∈RDhidden×L are needed (2\nmatrices for each layer).\nPreﬁx-tuning is originally evaluated on natural\nlanguage generation and we adapt it to understand-\ning tasks. A follow-up method named prompt-\ntuning (Lester et al., 2021) further reduces task-\nspeciﬁc parameters by limiting the preﬁx to the\nﬁrst layer but only performs competitively with\nvery large model sizes (billions of total parame-\nters), and is thus not considered in our study. Note\nthat preﬁx-tuning (or prompt-tuning) is different\nfrom prompt-based ﬁne-tuning methods (Schick\nand Schütze, 2021; Gao et al., 2021) (see App. A\nfor speciﬁc differences).\nAdditive Methods. Additive PELT methods treat\nthe model parameters after ﬁne-tuning as an ad-\ndition of the pre-trained parameters θpre-trained and\ntask-speciﬁc differences δtask, where θpre-trained is\nﬁxed and a new (sub)set of model parameters are\nadded on top: θtask = θpre-trained + δtask. There are\nvarious ways to parameterize δtask, leading to dif-\nferent additive methods such as LoRA (Hu et al.,\n2021), diff pruning (Guo et al., 2021), and side-\ntuning (Zhang et al., 2020). We take LoRA as a\nrepresentative and incorporate it into UNIPELT.\nOther methods are conceptually similar and can be\nincorporated in the same fashion.\nLoRA introduces trainable low-rank matrices\nand combines them with the original matrices\nin the multi-head attention. Speciﬁcally, two\nmatrices Wdown ∈ RDhidden×Dmid and Wup ∈\nRDmid×Dhidden are added for the query and key pro-\njections along with the original matrix WQ and\nWK ∈RDhidden×Dhidden :\nQ = (W⊺\nQ + αW⊺\nupW⊺\ndown)hin, (3)\nwhere αis a ﬁxed scalar hyperparameter for scaling\nthe task-speciﬁc differences. The form of the train-\nable matrices in LoRA is quite similar to those in\nadapter or preﬁx-tuning, but there is no activation\nfunction φin between.\n3 Unifying PELT Methods\n3.1 Task Formulation\nGiven a large PLMMwith size |M|that cannot be\nﬁne-tuned directly due to computational or storage\ncost, suppose that we have a list of PELT methods\n{mi}, the trainable parameters of which are negli-\ngible (i.e., ∑\ni |mi|≪|M| ), our goal is to design a\nuniﬁed PELT framework that incorporates {mi}as\nsubmodules and learns to dynamically activate (up-\nweight) different submodules when appropriate un-\nder different scenarios, such that one could achieve\nsatisfactory results in terms of both model effective-\nness and robustness without the hassle of permuting\nall the method×task×data combinations.\n6255\n3.2 Proposed Method\nMotivation & Intuition . During the analysis of\nindividual PELT methods, we observe that differ-\nent PELT methods exhibit diverse characteristics\nand perform rather differently on the same task.\nFor example, preﬁx-tuning generally performs well\non natural language inference tasks regardless of\nthe size of training data. Also, as can be seen in\nFig. 1 and Sec. 2, different PELT methods often in-\nvolve different parts of the PLM architecture (e.g.,\nbefore multi-head attention for preﬁx-tuning and\nafter feedforward layer for adapter), making it fea-\nsible to combine multiple PELT methods without\n(directly) interfering with each other.\nIn light of the two observations above, we pro-\npose a uniﬁed PELT framework, UNIPELT, which\ntakes a hybrid approach by incorporating multi-\nple PELT methods as submodules. At a high level,\nUNIPELT improves over single PELT methods due\nto two factors. First, UNIPELT learns to activate\n(upweight) the submodules that best suit the current\ntask or speciﬁc data sample and deactivate (down-\nweight) the rest. Second, we ﬁnd that UNIPELT\ngenerally performs better than taking the best per-\nformance of all its submodules used individually\non each task, suggesting that there could be some\ncompounding effects that lead to better model effec-\ntiveness when multiple PELT methods (that modify\ndifferent parts of the PLM) are used.\nNext, we will introduce how different PELT\nmethods can be incorporated into UNIPELT via\ngating mechanism.\nGating Mechanism. To achieve ﬁne-grained con-\ntrol of submodule (de)activation, we add a trainable\ngate Gmi for each submodule mi ∈{A, P, L}in\nevery Transformer layer (see Fig. 1). The letters A,\nP, L stand for Adapter, Preﬁx-tuning, and LoRA,\nrespectively. Intuitively, ifmi is useful for a given\ndata ×task setup (or a particular instance), the gate\noutput for mi would be higher such that mi plays\na more important role. The actual interplay of sub-\nmodules, however, is more complicated given the\ninterdependency of the submodules and the com-\npounding effects of multiple layers.\nSpeciﬁcally, for adapter, there is a residual con-\nnection between the feedforward network and the\nadapter submodule that sums the adapter input (be-\nfore normalization) hF and output hA as its ﬁnal\noutput: h′\nA = hA + hF . We design a gating func-\ntion GA ∈(0,1) that estimates the importance of\nadapter by its direct inputhFN using a feedforward\nnetwork with sigmoid activation and then scales its\noutput: h′\nA = GAhA+hF . The adapter submodule\nis effectively bypassed if GA ≈0.\nSimilarly, for preﬁx-tuning, we design a gating\nfunction GP ∈(0,1) that is applied to the preﬁx\nvectors (PK and PV ) with the representation of the\noriginal tokens (K and V ) intact. In this way, the\nimpact of the preﬁx would be diminished if the gate\noutput of the preﬁx-tuning submodule is low.3 The\ngating function GP is estimated by the Transformer\nlayer input hin with another feedforward network.\nAs for LoRA, we note that there is already a\nconstant scaling factor αin its original design that\nresembles the purpose of our gating mechanism.\nWe thus simply make the factor learnable per layer\nby a third feedforward network that takes hin as\ninput instead of specifying a constant manually:\nθtask = θpre-trained + GLδtask.\nDespite the seeming simplicity of UNIPELT,\nwe note that it is nontrivial for a uniﬁed approach\nto work well under different scenarios. Naively\ncombining different PELT methods as a hybrid ap-\nproach could lead to mixed or worse performance\nthan using individual methods, as observed in both\nour experiments and prior studies (Hu et al., 2021).\n4 Experiments\nWe conduct extensive experiments with 8 tasks×\n4 data sizes ×7 methods ×5 runs per setup, along\nwith additional analysis for particular methods, re-\nsulting in 1,000+ runs in total.\n4.1 Experiment Setup\nTask Setup. We conduct experiments on the Gen-\neral Language Understanding Evaluation (GLUE)\nbenchmark (Wang et al., 2019), which involves\nfour types of natural language understanding tasks\nincluding linguistic acceptability (CoLA), senti-\nment analysis (SST-2), similarity and paraphrase\ntasks (MRPC, STS-B, QQP), and natural language\ninference (MNLI, QNLI, RTE). We exclude the\nWNLI dataset following prior studies (Houlsby\net al., 2019; Devlin et al., 2019).\nData Setup. We mainly consider a low-resource\nsetting where training data is limited and the per-\nformance of different methods varies much. We\nsample a small subset of the training set for each\ntask with size K = {100,500,1000}. As it is in-\nfeasible to submit considerable runs to the GLUE\n3Preﬁx-tuning cannot be fully eliminated as adapter or\nLoRA due to the softmax operation in multi-head attention.\n6256\nMethod SST-2 MRPC CoLA RTE QNLI STS-B MNLI QQP Avg.\n[K= 100] Test Performance\nFine-tuning 79.61 4.25 81.810.35 16.564.34 55.881.64 69.255.94 74.076.51 42.563.43 60.416.42 60.021.84\nBitFit 62.94 4.85 81.090.17 2.711.57 47.653.56 42.461.37 54.530.56 38.160.53 59.560.39 48.640.78\nAdapter 80.48 2.94 81.400.19 2.024.04 52.780.27 72.250.49 77.321.54 38.813.64 60.884.00 58.240.99\nPreﬁx-tuning 60.87 12.47 81.220.00 0.000.00 55.962.00 71.912.69 57.690.02 40.582.49 15.680.12 47.991.77\n→L= 50 79.521.21 81.220.00 5.198.62 49.242.08 66.332.45 7.1510.37 33.662.21 58.323.18 47.561.37\nLoRA 81.56 0.94 81.660.81 13.3110.00 55.021.75 73.521.20 49.3521.87 39.604.98 0.090.02 49.262.19\nUNIPELT (AP) 77.223.75 81.860.70 14.4210.24 55.522.16 72.260.89 79.141.97 42.591.20 63.411.44 60.801.53\nUNIPELT (APL)82.360.86 81.710.72 23.628.83 55.451.28 73.190.93 79.371.07 42.301.88 62.702.55 62.591.44\n[K= 500] Test Performance\nFine-tuning 85.670.97 83.340.55 36.472.69 59.641.10 77.300.49 84.961.19 55.840.85 68.231.39 68.930.65\nBitFit 83.44 0.63 82.160.37 3.322.59 61.882.75 69.159.91 76.300.36 40.823.30 65.293.66 60.301.91\nAdapter 84.54 1.37 82.530.36 38.653.97 59.353.09 77.390.84 83.520.33 50.041.72 68.120.95 68.020.77\nPreﬁx-tuning 83.65 0.69 82.961.63 38.162.25 63.182.70 78.501.12 79.751.49 58.061.04 54.3425.91 67.323.42\nLoRA 84.98 1.10 82.530.70 39.862.71 63.032.57 79.460.66 65.0526.31 56.542.05 55.4627.74 65.864.18\nUNIPELT (AP) 84.840.28 83.250.51 39.845.01 63.321.72 78.361.06 84.530.48 56.083.26 68.141.39 69.791.02\nUNIPELT (APL) 84.911.41 83.560.59 39.812.55 64.122.45 79.280.63 85.260.70 54.073.74 68.870.41 69.980.42\n[K= 1000] Test Performance\nFine-tuning 86.54 1.01 84.870.64 43.262.60 62.312.10 79.031.11 86.390.34 61.951.20 71.090.77 71.930.37\nBitFit 83.99 0.39 83.950.81 22.4417.10 62.891.40 77.430.53 79.040.61 52.870.72 69.500.16 66.512.22\nAdapter 85.60 0.63 84.490.60 42.331.98 61.811.57 79.680.23 85.520.29 57.862.44 70.320.71 70.950.55\nPreﬁx-tuning 85.09 0.99 83.661.82 44.072.90 66.712.72 80.340.70 82.381.25 63.591.12 68.580.35 71.810.52\nLoRA 86.26 1.22 86.040.99 45.501.11 65.632.11 81.000.98 81.561.97 61.321.65 70.890.81 72.280.69\nUNIPELT (AP) 86.170.37 85.861.05 44.333.55 64.911.92 80.650.57 86.820.23 62.170.99 69.950.90 72.610.53\nUNIPELT (APL)87.060.81 86.651.10 45.441.97 65.491.92 81.220.51 87.100.21 62.490.94 70.990.95 73.310.52\nTable 1: Results on the GLUE benchmark with K = {100,500,1000}training samples. The evaluation metrics\nare Matthew’s Correlation for CoLA, F1 for MRPC and QQP, Spearman’s correlation for STS-B, and accuracy for\nthe rest. For MNLI, we evaluate on the matched dataset. We report average performance on ﬁve random seeds\nwith standard deviation as the subscript. Best and 2nd best methods under each setup are bold and underlined.\nleaderboard (2 submissions/day), we take 1,000\nsamples on the training set as the development set\nto select the best checkpoint and use the original\ndevelopment set as the test set. To reduce variance,\nwe shufﬂe the data with 5 random seeds and re-\nport the average performance. Additionally, we\nconsider a high-resource setting where the whole\ntraining set is used and the best performance on the\nGLUE development set is reported.\nCompared Methods . We mainly compare\nUNIPELT with ﬁne-tuning and four representa-\ntive PELT methods: adapter (Houlsby et al., 2019),\npreﬁx-tuning (Li and Liang, 2021), BitFit (Ben Za-\nken et al., 2021), and LoRA (Hu et al., 2021).\nFor completeness, we consider two model vari-\nants UNIPELT (AP) and UNIPELT (APL), which\nincorporate 2 and 3 PELT methods, respectively.\nImplementation Details. We use BERT base (De-\nvlin et al., 2019) as the base model in the experi-\nments. Consistent results are observed in our pre-\nliminary experiments with BARTlarge (Lewis et al.,\n2020) (provided in App. C). We implement and\nevaluate all the methods in the same codebase to\nensure a fair comparison. We largely follow the\ndefault hyperparameters of different methods and\nkeep them the same on all the tasks for generaliz-\nability. We set the preﬁx length L = 10, adapter\nbottleneck size Dmid = 48, LoRA rank Dmid = 8\nif not speciﬁed otherwise.4 More implementation\nand hyperparameter details can be found in App. B.\n4.2 Analysis of Individual PELT Methods\nIn Table 1, we show the performance of different\nmethods on the GLUE benchmark with various\nsizes of training data. The results on the devel-\nopment sets are generally consistent with the test\nsets and provided in App. D. Although the average\nperformance of different methods over 8 tasks is\nsometimes similar, the differences between tasks\nare quite signiﬁcant under certain setups and can\nbe as large as 5~9 points on a speciﬁc task ( e.g.,\nSTS-B and MNLI, K = 500) even when excluding\ncases where some methods fail to learn effectively\n(e.g., preﬁx-tuning on QQP, K = 100).\n4While these hyperparameters may lead to differences in\ntrainable parameters, we keep them for analysis as they are\nused by the ofﬁcial implementation. Also, we observe that\nmore trainable parameters do not guarantee better results.\n6257\nNext, we will analyze and examine each individ-\nual PELT method more closely.\nAnalysis of Adapter. The performance of adapter\nis relatively stable – there is no signiﬁcantly bet-\nter or worse result than ﬁne-tuning consistent on\ndifferent tasks or sizes of training data. In gen-\neral, adapter is slightly worse than ﬁne-tuning in\nmost cases. We do not observe that adapter consis-\ntently outperforms ﬁne-tuning in the low-resource\nsetting as in He et al. (2021), possibly because\nthey tune model hyperparameters on each task,\nwhich could be computationally prohibitive when\nthere are considerable tasks. For example, they\nchoose the bottleneck size Dmid from {64, 128,\n256}, while Dmid = 48is ﬁxed across tasks in our\nexperiments. Also, we only add one adapter in\neach Transformer layer instead of two following\nPfeiffer et al. (2021). These two differences result\nin 62.4%~90.5% fewer parameters than the adapter\nused in He et al. (2021).\n48 64 128 256\nBottleneck size Dmid\n0\n5\n10\n15\n20\n25\n30Score\nUniPELT (APL)\nUniPELT (AP)\nAdapter\nFigure 2: Performance changes when the bottleneck\nsize of adapter is increased (on CoLA, K = 100).\nTo further study the effect of bottleneck size\nDmid in adapter, we increase Dmid and re-evaluate\nadapter on a setup that it performs poorly (CoLA,\nK = 100). As shown in Fig. 2, the performance\nof adapter is increased gradually and becomes sig-\nniﬁcantly better only when Dmid = 256, which in-\nvolves 5.3×trainable parameters than the adapter\nused originally (Dmid = 48), 4.3×than UNIPELT\n(AP), and 3.4 ×than UNIPELT (APL), suggest-\ning that a larger bottleneck size could be beneﬁcial\nwhen adapter learns ineffectively.\nOn the other hand, there are certain tasks ( e.g.,\nSTS-B) that adapter largely outperforms compet-\nitive methods such as preﬁx-tuning and LoRA re-\ngardless of the size of training data, suggesting that\none should favor adapter over other PELT methods\nunder certain scenarios as well.\nAnalysis of Preﬁx-tuning. Preﬁx-tuning performs\npoorly with K = {100,500}and becomes on par\nwith ﬁne-tuning when Kreaches 1000. We also ob-\nserve that preﬁx-tuning fails to learn effectively on\ncertain tasks when the training data is limited (e.g.,\nK = 100on SST-2 and K = 500on QQP), lead-\ning to unsatisfactory performance and (or) large\nvariance across different runs. Similar phenomena\nhave been observed in a concurrent study (Gu et al.,\n2021) on few-shot prompt-tuning.\nTo ensure that the poor performance of preﬁx-\ntuning is not due to its fewer trainable parameters\n(based on its default setting), we further increase\nthe preﬁx length to L = 50 such that its train-\nable parameters are comparable to adapter, and re-\nevaluate preﬁx-tuning on all 8 tasks with K = 100.\nFor the 4 tasks where preﬁx-tuning (L= 10) per-\nforms poorly (SST2, CoLA, STS-B, and QQP),\nwhile its performance is signiﬁcantly improved on\n3 tasks, it also performs signiﬁcantly worse on the\nother task (STS-B), which suggests that training\ninstability in the low-resource regime is still an\nissue for preﬁx-tuning even with more trainable\nparameters.5 Besides, preﬁx-tuning (L= 50) still\nlags behind adapter or UNIPELT (AP) on 3 of the\n4 tasks. Furthermore, the average performance of\npreﬁx-tuning (L= 50) on 8 tasks is even slightly\nworse than with L = 10, which indicates that in-\ncreasing preﬁx length may not be a panacea for\nall the scenarios. A larger Lalso leads to signiﬁ-\ncant training/inference slowdown due to the costly\nmulti-head attention. More broadly, such results\nsuggest that using more trainable parameters does\nnot guarantee better performance.\nOn the bright side, preﬁx-tuning performs well\non certain tasks such as natural language inference\n(RTE and MNLI) with various sizes of training\ndata, which suggests that one should also prefer\npreﬁx-tuning in certain cases.\nAnalysis of BitFit & LoRA. Tuning only the bias\nterms of the model does not lead to very satisfac-\ntory results in our experiments – BitFit never per-\nforms the best and generally performs the worst in\ndifferent data and task setups. Therefore, we do\nnot consider BitFit in the following experiments\nand exclude BitFit as a submodule of UNIPELT.\nAs for LoRA, there are a few setups where LoRA\nfails to learn effectively as well, such as STS-B\nand QQP (K = {100,500}), leading to high vari-\nance across runs. Apart from that, LoRA performs\n5Tuning other hyperparameters like learning rate does not\nappear to alleviate the issue either.\n6258\n0.5 1 2 3 4\nScaling factor \n80\n82\n84\n86\n88\n90Score\nSST-2 (K = 100)\nSST-2 (K = 500)\nMRPC (K = 100)\nMRPC (K = 500)\nFigure 3: Performance comparison of various scaling\nfactors for LoRA on 2×2 task and data setups.\nquite competitively despite using fewer trainable\nparameters than methods like adapter, especially\nwhen K = 1000, achieving the best or 2nd best\nperformance on 4 of 8 tasks.\nAs LoRA has a scaling factor αthat can be seen\nas a static gating function under our formulation,\nwe further investigate its importance by evaluating\nLoRA with different α. As shown in Fig. 3, LoRA\nis quite sensitive to the scaling factor and there\nseems to be no single optimal value that works\nwell across multiple task and data setups. Such\nﬁndings suggest that gating is critical and motivate\nus to use more ﬁne-grained and dynamic control\nfor UNIPELT. Besides, we observe that increasing\nαconsistently results in faster convergence, possi-\nbly because the trainable parameters would receive\nlarger gradient updates with a larger α.\n4.3 Analysis of U NIPELT\nNext, we will turn to our proposed framework\nUNIPELT, which incorporates multiple existing\nPELT methods as submodules.\nLow-Resource Performance. Overall, UNIPELT\n(APL) and UNIPELT (AP) consistently achieve\nthe best and second best average performance on\nboth the development and test sets regardless of\nthe number of training samples. The gains are\ngenerally 1~4% over the submodule that performs\nthe best (when used individually). Such results\ndemonstrate the advantages of our hybrid approach\nregarding model effectiveness and generalizability.\nAt the per-task level, UNIPELT (APL) and\nUNIPELT (AP) perform the best or second best on\n7/6/7 of 8 tasks when trained with 100/500/1,000\nsamples, and never perform the worst in any setup.\nWhen comparing the two variants, UNIPELT\n(APL) outperforms UNIPELT (AP) on 4/6/8 of\n8 tasks when trained with 100/500/1,000 samples.\nSuch results indicate that UNIPELT is quite ro-\nbust and performs reliably under different scenar-\nios. The improvements of UNIPELT over its sub-\nmodules are generally larger when having fewer\ntraining samples, suggesting that UNIPELT per-\nforms especially well in the low-resource regime.\nIn particular, on the tasks where other PELT meth-\nods fail to learn effectively such as CoLA and QQP\n(K = 100), UNIPELT manages to achieve perfor-\nmance better than ﬁne-tuning.\nUNIPELT vs. Upper Bound . In Table 2, we\nshow the comparison of UNIPELT and the up-\nper bound that takes the best performance of its\nsubmodules on each task. We observe that both\nUNIPELT (AP) and UNIPELT (APL) perform\nsimilarly or even better than their upper bound,\nwhich suggests that UNIPELT successfully learns\nto leverage different submodules and maintains\n(near) optimal performance under different setups.\nThe fact that UNIPELT can outperform the upper\nbound also hints that a mixture of PELT methods\n(involving different parts of the PLM) might be in-\nherently more effective than single methods (with\na limited scope of the PLM architecture).\nK max({A,P}) UNIPELT max({A,P,L}) UNIPELT\n100 58.86 60.80 60.60 62.59\n500 69.69 69.79 70.02 69.98\n1000 72.58 72.61 73.19 73.31\nTable 2: Comparison of average test performance be-\ntween U NIPELT and the upper bound that takes the\nbest performance of its submodules on each task.\nHigh-Resource Performance . In Table 3, we\nlist the performance of different methods when\nall training samples are used. UNIPELT again\nachieves the best overall performance. The gains\nare not as signiﬁcant as in the low-resource set-\nting, which is somewhat expected as existing PELT\nmethods typically perform on par with ﬁne-tuning\ngiven abundant training data and the potential of\nimprovement is not as high. That said, the perfor-\nmance of UNIPELT is still the best or 2nd best on\nall 8 tasks, and generally comparable to the best\nsubmodule used individually on each task. Besides,\nsimply combining multiple PELT methods without\ngating does not work well in the high-resource set-\nting – although UNIPELT-NoGate never performs\nthe worst in each task, its average performance is\nunsatisfactory (-0.89 vs. UNIPELT).\n6259\nMethod SST-2 MRPC CoLA RTE QNLI STS-B MNLI QQP Avg.\n[K= all] Best Performance on GLUE Dev\nFine-tuning 91.63 90.94 62.08 66.43 89.95 89.76 83.23 87.35 82.67\nAdapter 91.86 89.86 61.51 71.84 90.55 88.63 83.14 86.78 83.02\nPreﬁx-tuning 90.94 91.29 55.37 76.90 90.39 87.19 81.15 83.30 82.07\nLoRA 91.51 90.03 60.47 71.48 89.93 85.65 82.51 85.98 82.20\nUNIPELT (AP) 91.86 90.28 61.15 71.84 90.77 88.86 83.41 86.74 83.12\n-NoGate 91.74 90.18 58.63 71.12 90.30 88.76 81.58 85.53 82.23\nUNIPELT (APL) 91.51 90.94 61.53 73.65 90.50 88.93 83.89 87.12 83.50\nTable 3: Results on the GLUE benchmark when all training samples are used.\nMethod #Param. Time T TimeI\nFine-tuning 110M (100%) 100% 100%\nBitFit 103K (0.09%) 65% 102%\nPreﬁx-tuning 184K (0.17%) 56% 114%\nLoRA 295K (0.27%) 53% 105%\nAdapter 895K (0.81%) 55% 107%\nUNIPELT (AP) 1.1M (0.99%) 55% 118%\nUNIPELT (APL) 1.4M (1.26%) 67% 127%\nTable 4: Number of trainable parameters and\nTraining/Inference time relative to ﬁne-tuning.\n4.4 Efﬁciency of PELT Methods\nWe benchmark the efﬁciency of PELT methods and\nlist in Table 4 their number of trainable parameters\nand training/inference time relative to ﬁne-tuning.\nParameter Efﬁciency. As the trainable parame-\nters in PELT methods are almost negligible, com-\nbining multiple methods does not lead to signiﬁcant\nlosses in parameter efﬁciency. UNIPELT still has\nfew trainable parameters compared to ﬁne-tuning\n(0.99%~1.26%). The parameters can be further re-\nduced if one uses more parameter-efﬁcient variants\n(e.g., Karimi Mahabadi et al. (2021a)), which can\nbe easily swapped with the vanilla version used in\nour current framework. Also, note that more train-\nable parameters do not always lead to better per-\nformance, as shown in our experiments and prior\nstudies (He et al., 2021; Pfeiffer et al., 2021).\nTraining and Inference Efﬁciency . Due to\nparameter efﬁciency, all PELT methods train\n30%~50% faster than ﬁne-tuning and incorporating\nmultiple PELT methods into UNIPELT does not\nsuffer from slower training. On the other hand, the\ninference time of PELT methods is generally longer\nsince they involve more FLOPs. UNIPELT has a\nslightly larger inference overhead (4%~11% com-\npared to its slowest submodule), which we argue is\ninsigniﬁcant since larger models that may achieve\nsimilar performance gains ( e.g., BERTlarge) need\naround 300% inference time (Wolf et al., 2020).\n5 Related Work\nParameter-Efﬁcient Tuning of PLMs . As it is\nincreasingly infeasible to train and store full copies\nof large PLMs for various downstream tasks, how\nto efﬁciently tune the PLMs with few trainable pa-\nrameters becomes critical. Existing PELT methods\ncan be largely divided into two categories based on\nwhether new trainable parameters are introduced.\nSpeciﬁcally, one may either train a subset of the\nmodel parameters such as the prediction head (Lee\net al., 2019) and bias terms (Ben Zaken et al., 2021),\nor introduce task-speciﬁc parameters to different\nparts of the PLM such as before multi-head at-\ntention (Li and Liang, 2021) or after feedforward\nlayer (Houlsby et al., 2019). As the number of\nPELT methods keeps increasing, the purpose of\nUNIPELT is to better understand and leverage the\ndistinctions of various methods instead of propos-\ning yet another method.\nMixture-of-Experts. UNIPELT is also related to\napproaches that involve a high-capacity network\nand activate (upweight) different parts of the net-\nwork given different inputs. One notable example\nis Mixture-of-Experts (MoE) (Jacobs et al., 1991;\nShazeer et al., 2017), which maintains a set of ex-\nperts (neural networks) and one or more trainable\ngates that select a combination of the experts spe-\nciﬁc to each input. Despite being conceptually\nsimilar, UNIPELT is different from MoE: the sub-\nmodules in UNIPELT are not combined explicitly\nby summation like MoE but in sequential order\nand affect each other implicitly. Moreover, the\n“experts” are diverse in UNIPELT while usually\nhomogeneous or identical in MoE methods.\n6 Conclusion\nIn this paper, we present a comprehensive study of\nrepresentative parameter-efﬁcient language model\n6260\ntuning (PELT) methods and propose a uniﬁed\nframework, which incorporates different PELT\nmethods as submodules and learns to activate the\nmost appropriate submodules for a given task or\ndata setup. Our proposed framework consistently\noutperforms conventional ﬁne-tuning as well as the\nsubmodules that it incorporates under different se-\ntups, and generally surpasses the upper bound that\ntakes the best performance of each submodule used\nindividually on each task. Our ﬁndings suggest that\na mixture of multiple PELT methods that involve\ndifferent parts of the PLM may be favorable regard-\ning both model effectiveness and robustness. For\nfuture work, we will try to better understand the\ndiscrepancy of various PELT methods in different\nscenarios. We also plan to investigate a multi-task\nsetting where multiple submodules can be activated\nand cooperate at the task level.\nAcknowledgements\nWe thank Xiang Lisa Li, Hai Ye, Rabeeh Karimi\nMahabadi, Junxian He, Yiqing Xie, Yaqing Wang,\nand Liyuan Liu for helpful discussions and feed-\nback. We thank anonymous reviewers for valuable\ncomments and suggestions.\nReferences\nElad Ben Zaken, Shauli Ravfogel, and Yoav Gold-\nberg. 2021. Bitﬁt: Simple parameter-efﬁcient\nﬁne-tuning for transformer-based masked language-\nmodels. arXiv e-prints, pages arXiv–2106.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nNing Ding and Shengding Hu. 2021. Must-read\npapers on prompt-based tuning for pre-trained\nlanguage models. https://github.com/\nthunlp/PromptPapers.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nYuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.\n2021. Ppt: Pre-trained prompt tuning for few-shot\nlearning. arXiv preprint arXiv:2109.04332.\nDemi Guo, Alexander Rush, and Yoon Kim. 2021.\nParameter-efﬁcient transfer learning with diff prun-\ning. In Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics and\nthe 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers) ,\npages 4884–4896, Online. Association for Computa-\ntional Linguistics.\nRuidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng\nDing, Liying Cheng, Jiawei Low, Lidong Bing, and\nLuo Si. 2021. On the effectiveness of adapter-\nbased tuning for pretrained language model adap-\ntation. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 2208–2222, Online. Association for Computa-\ntional Linguistics.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for nlp.\nIn International Conference on Machine Learning ,\npages 2790–2799. PMLR.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\nChen. 2021. Lora: Low-rank adaptation of large lan-\nguage models. arXiv preprint arXiv:2106.09685.\nRobert A Jacobs, Michael I Jordan, Steven J Nowlan,\nand Geoffrey E Hinton. 1991. Adaptive mixtures of\nlocal experts. Neural computation, 3(1):79–87.\nRabeeh Karimi Mahabadi, James Henderson, and Se-\nbastian Ruder. 2021a. Compacter: Efﬁcient low-\nrank hypercomplex adapter layers. arXiv preprint\narXiv:2106.04647.\nRabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa\nDehghani, and James Henderson. 2021b. Parameter-\nefﬁcient multi-task ﬁne-tuning for transformers via\nshared hypernetworks. In Proceedings of the 59th\nAnnual Meeting of the Association for Computa-\ntional Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers) , pages 565–576, Online. As-\nsociation for Computational Linguistics.\nJaejun Lee, Raphael Tang, and Jimmy Lin. 2019. What\nwould elsa do? freezing layers during transformer\nﬁne-tuning. arXiv preprint arXiv:1911.03090.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efﬁcient prompt\ntuning. arXiv preprint arXiv:2104.08691.\n6261\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n4582–4597, Online. Association for Computational\nLinguistics.\nJonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé,\nKyunghyun Cho, and Iryna Gurevych. 2021.\nAdapterFusion: Non-destructive task composition\nfor transfer learning. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume ,\npages 487–503, Online. Association for Computa-\ntional Linguistics.\nJonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aish-\nwarya Kamath, Ivan Vuli ´c, Sebastian Ruder,\nKyunghyun Cho, and Iryna Gurevych. 2020a.\nAdapterhub: A framework for adapting transform-\ners. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP 2020): Systems Demonstrations, pages 46–\n54, Online. Association for Computational Linguis-\ntics.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020b. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654–7673, Online. Association for Computa-\ntional Linguistics.\nTimo Schick and Hinrich Schütze. 2021. Exploiting\ncloze-questions for few-shot text classiﬁcation and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, pages 255–269, Online. Association for Com-\nputational Linguistics.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc Le, Geoffrey Hinton, and Jeff\nDean. 2017. Outrageously large neural networks:\nThe sparsely-gated mixture-of-experts layer. arXiv\npreprint arXiv:1701.06538.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2019.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. In International\nConference on Learning Representations.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Fun-\ntowicz, et al. 2019. Huggingface’s transformers:\nState-of-the-art natural language processing. arXiv\npreprint arXiv:1910.03771.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Fun-\ntowicz, Joe Davison, Sam Shleifer, Patrick von\nPlaten, Clara Ma, Yacine Jernite, Julien Plu,\nCanwen Xu, Teven Le Scao, Sylvain Gugger,\nMariama Drame, Quentin Lhoest, and Alexan-\nder M. Rush. 2020. How to benchmark trans-\nformer models. https://huggingface.co/\ntransformers/benchmarks.html.\nJeffrey O Zhang, Alexander Sax, Amir Zamir,\nLeonidas Guibas, and Jitendra Malik. 2020. Side-\ntuning: A baseline for network adaptation via ad-\nditive side networks. In Computer Vision–ECCV\n2020: 16th European Conference, Glasgow, UK, Au-\ngust 23–28, 2020, Proceedings, Part III 16 , pages\n698–714. Springer.\n6262\nA Preﬁx-tuning vs. Prompt-based\nFine-tuning\nWe note that preﬁx-tuning (or prompt-tuning)\nis different from prompt-based ﬁne-tuning meth-\nods (Schick and Schütze, 2021; Gao et al., 2021)\nin many ways: (1) Prompt-based ﬁne-tuning is not\nparameter-efﬁcient as it updates all model param-\neters while preﬁx-tuning only updates the preﬁx\nmatrix P. (2) The prompts are only used in model\ninput for prompt-based ﬁne-tuning but added to\nevery Transformer layer in preﬁx-tuning (stored as\ndifferent vectors). (3) Prompt-based ﬁne-tuning\ntypically leverages carefully designed natural lan-\nguage prompts while preﬁx-tuning uses continuous\nprompts (virtual tokens).\nB Implementation Details\nData Preparation. We shufﬂe the training set with\nseed s, take the ﬁrst Ksamples as the new training\nset, and the next 1,000 samples as the development\nset. We use s = {111,222,333,444,555}as the\ndata seeds and the same seed (s= 42) for model\ntraining. We also conduct another set of prelim-\ninary experiments by ﬁxing the data and using 5\ndifferent random seeds for model training, the re-\nsults of which are similar (Table 5).\nHyperparameters. We adopt AdapterHub (Pfeif-\nfer et al., 2020a), a library based on HuggingFace\nTransformers (Wolf et al., 2019), as our codebase.\nWe largely follow the recommended hyperparame-\nters used in different methods for a fair comparison.\nWe set the input length to 128 and the training\nbatch size to 16. We set the number of epochs to\n50 and adopt early stopping with a patience of 10\nnon-increasing epochs. We set the learning rate of\nﬁne-tuning and adapter to 2e-5 and 1e-4 according\nto the ﬁndings in prior studies (Pfeiffer et al., 2020a;\nHe et al., 2021). For preﬁx-tuning and UNIPELT,\nas they are not previously evaluated on NLU tasks,\nwe tune their learning rates from {1e-4, 2e-4, 5e-4}\non the development set and set to 2e-4 and 5e-4,\nrespectively. For BitFit and LoRA, we choose the\nlearning rates commonly used in their own experi-\nments (1e-4 and 5e-4, respectively). We set α= 2\nand r= 8in LoRA according to its ofﬁcial scripts.\nC BART Results\nIn our preliminary experiments, we also evaluated\nUNIPELT on BARTlarge (Lewis et al., 2020). We\nshow the results of ﬁne-tuning, adapter, preﬁx-\nSetup Fine-tuning Adapter Preﬁx-tuning UNIPELT (AP)\nModel seed 78.392.92 77.120.50 73.162.89 78.660.24\nData seed 77.552.94 76.870.55 71.902.47 79.020.44\nTable 5: Average performance with K = 1000on the\nGLUE benchmark with BART large as the base model.\nResults are averaged over 5 runs by changing the model\nor data seeds.\ntuning, and UNIPELT (AP) in Table 5. 1000 train-\ning examples are used and the average best perfor-\nmance on the GLUE development set is reported\n(excluding QQP). The results are largely consistent\nwith those on BERTbase. UNIPELT again achieves\nthe best performance with notably smaller variance.\nD Detailed Performance\nIn Table 6, we list the detailed results on both devel-\nopment and test sets of the GLUE benchmark. The\nobservations and ﬁndings are largely consistent on\nthe two evaluation splits.\n6263\nMethod SST-2 MRPC CoLA RTE QNLI STS-B MNLI QQP Avg.\n[K= 100] Dev Performance\nFine-tuning 81.24 0.98 81.460.78 16.942.38 58.081.63 69.665.03 60.646.97 43.183.13 61.636.30 59.101.87\nBitFit 62.06 4.62 80.660.39 5.731.46 50.260.91 42.022.29 31.182.47 38.400.84 61.550.52 46.480.66\nAdapter 80.60 0.85 81.110.78 2.194.38 53.161.99 72.580.66 66.003.66 40.302.82 62.323.20 57.280.60\nPreﬁx-tuning 66.24 12.03 80.510.31 0.000.00 56.601.25 71.942.58 42.811.93 42.261.89 15.140.95 46.941.43\nLoRA 82.54 0.84 80.820.50 14.388.57 56.622.01 74.260.89 47.8714.05 41.384.59 0.000.00 49.731.29\nUNIPELT (AP) 80.401.95 81.020.54 15.076.46 57.681.63 73.500.54 68.193.97 44.501.11 64.890.86 60.661.16\nUNIPELT (APL)83.080.54 81.080.53 23.525.71 57.961.49 74.000.46 68.293.01 43.101.13 63.412.93 61.800.77\n[K= 100] Test Performance\nFine-tuning 79.61 4.25 81.810.35 16.564.34 55.881.64 69.255.94 74.076.51 42.563.43 60.416.42 60.021.84\nBitFit 62.94 4.85 81.090.17 2.711.57 47.653.56 42.461.37 54.530.56 38.160.53 59.560.39 48.640.78\nAdapter 80.48 2.94 81.400.19 2.024.04 52.780.27 72.250.49 77.321.54 38.813.64 60.884.00 58.240.99\nPreﬁx-tuning 60.87 12.47 81.220.00 0.000.00 55.962.00 71.912.69 57.690.02 40.582.49 15.680.12 47.991.77\n→L= 50 79.521.21 81.220.00 5.198.62 49.242.08 66.332.45 7.1510.37 33.662.21 58.323.18 47.561.37\nLoRA 81.56 0.94 81.660.81 13.3110.00 55.021.75 73.521.20 49.3521.87 39.604.98 0.090.02 49.262.19\nUNIPELT (AP) 77.223.75 81.860.70 14.4210.24 55.522.16 72.260.89 79.141.97 42.591.20 63.411.44 60.801.53\nUNIPELT (APL)82.360.86 81.710.72 23.628.83 55.451.28 73.190.93 79.371.07 42.301.88 62.702.55 62.591.44\n[K= 500] Dev Performance\nFine-tuning 86.66 1.40 82.560.88 37.473.06 62.881.79 77.581.64 77.342.03 58.501.53 69.401.32 69.050.38\nBitFit 84.66 1.28 81.800.96 5.661.87 61.880.95 69.328.90 59.551.41 42.623.23 66.062.99 58.941.65\nAdapter 85.74 1.03 82.740.87 38.224.14 63.521.98 78.201.64 76.151.18 51.302.65 69.231.30 68.140.66\nPreﬁx-tuning 86.721.46 82.261.16 40.255.45 66.080.83 78.441.48 71.412.30 60.701.47 54.4725.86 67.543.45\nLoRA 86.36 1.37 82.381.35 42.603.13 65.461.74 79.341.23 60.5816.76 58.702.17 56.3928.20 66.484.02\nUNIPELT (AP) 86.261.90 82.771.09 42.483.38 65.081.65 78.861.45 77.831.29 59.463.71 68.952.14 70.210.78\nUNIPELT (APL) 86.101.28 83.160.92 43.834.73 64.022.99 79.561.49 78.541.95 57.083.87 69.560.89 70.230.55\n[K= 500] Test Performance\nFine-tuning 85.670.97 83.340.55 36.472.69 59.641.10 77.300.49 84.961.19 55.840.85 68.231.39 68.930.65\nBitFit 83.44 0.63 82.160.37 3.322.59 61.882.75 69.159.91 76.300.36 40.823.30 65.293.66 60.301.91\nAdapter 84.54 1.37 82.530.36 38.653.97 59.353.09 77.390.84 83.520.33 50.041.72 68.120.95 68.020.77\nPreﬁx-tuning 83.65 0.69 82.961.63 38.162.25 63.182.70 78.501.12 79.751.49 58.061.04 54.3425.91 67.323.42\nLoRA 84.98 1.10 82.530.70 39.862.71 63.032.57 79.460.66 65.0526.31 56.542.05 55.4627.74 65.864.18\nUNIPELT (AP) 84.840.28 83.250.51 39.845.01 63.321.72 78.361.06 84.530.48 56.083.26 68.141.39 69.791.02\nUNIPELT (APL) 84.911.41 83.560.59 39.812.55 64.122.45 79.280.63 85.260.70 54.073.74 68.870.41 69.980.42\n[K= 1000] Dev Performance\nFine-tuning 87.70 0.89 84.730.61 42.612.62 64.902.01 78.862.00 81.311.39 63.741.59 71.991.59 71.980.59\nBitFit 86.30 1.36 83.630.18 20.4516.56 64.241.55 76.760.84 66.650.87 53.221.73 68.952.32 65.022.12\nAdapter 87.06 1.44 84.790.42 43.481.46 65.620.93 79.881.26 80.881.89 59.562.46 70.521.48 71.470.33\nPreﬁx-tuning 87.86 1.23 83.481.15 44.042.74 68.080.81 79.601.61 75.472.92 65.480.48 68.940.93 71.620.54\nLoRA 87.50 1.01 85.091.02 47.113.02 67.200.78 80.861.88 76.331.28 62.861.53 71.481.45 72.300.52\nUNIPELT (AP) 87.321.73 85.520.63 45.483.52 66.600.99 80.701.59 82.961.47 65.562.09 70.581.44 73.090.46\nUNIPELT (APL)88.021.28 86.050.73 45.702.47 66.861.32 80.501.76 83.091.55 64.600.72 70.640.77 73.180.27\n[K= 1000] Test Performance\nFine-tuning 86.54 1.01 84.870.64 43.262.60 62.312.10 79.031.11 86.390.34 61.951.20 71.090.77 71.930.37\nBitFit 83.99 0.39 83.950.81 22.4417.10 62.891.40 77.430.53 79.040.61 52.870.72 69.500.16 66.512.22\nAdapter 85.60 0.63 84.490.60 42.331.98 61.811.57 79.680.23 85.520.29 57.862.44 70.320.71 70.950.55\nPreﬁx-tuning 85.09 0.99 83.661.82 44.072.90 66.712.72 80.340.70 82.381.25 63.591.12 68.580.35 71.810.52\nLoRA 86.26 1.22 86.040.99 45.501.11 65.632.11 81.000.98 81.561.97 61.321.65 70.890.81 72.280.69\nUNIPELT (AP) 86.170.37 85.861.05 44.333.55 64.911.92 80.650.57 86.820.23 62.170.99 69.950.90 72.610.53\nUNIPELT (APL)87.060.81 86.651.10 45.441.97 65.491.92 81.220.51 87.100.21 62.490.94 70.990.95 73.310.52\nTable 6: Results on the GLUE benchmark with K = {100,500,1000}training samples. The evaluation metrics\nare Matthew’s Correlation for CoLA, F1 for MRPC and QQP, Spearman’s correlation for STS-B, and accuracy for\nthe rest. For MNLI, we evaluate on the matched dataset. We report average performance on ﬁve random seeds\nwith standard deviation as the subscript. Best and 2nd best methods under each setup are bold and underlined.\n6264"
}