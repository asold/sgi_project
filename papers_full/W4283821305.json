{
    "title": "Uncertainty Modeling with Second-Order Transformer for Group Re-identification",
    "url": "https://openalex.org/W4283821305",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2094445602",
            "name": "Quan Zhang",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A4212162567",
            "name": "Jian-Huang Lai",
            "affiliations": [
                "Ministry of Public Security of the People's Republic of China"
            ]
        },
        {
            "id": "https://openalex.org/A2295972444",
            "name": "Zhanxiang Feng",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2092733050",
            "name": "Xiaohua Xie",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2094445602",
            "name": "Quan Zhang",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A4212162567",
            "name": "Jian-Huang Lai",
            "affiliations": [
                "Ministry of Public Security of the People's Republic of China",
                "Sun Yat-sen University",
                "Ministry of Education of the People's Republic of China"
            ]
        },
        {
            "id": "https://openalex.org/A2295972444",
            "name": "Zhanxiang Feng",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2092733050",
            "name": "Xiaohua Xie",
            "affiliations": [
                "Ministry of Education of the People's Republic of China",
                "Sun Yat-sen University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3173301140",
        "https://openalex.org/W6641017118",
        "https://openalex.org/W6675620397",
        "https://openalex.org/W6778485988",
        "https://openalex.org/W6786585107",
        "https://openalex.org/W3161901477",
        "https://openalex.org/W6676297131",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2624431344",
        "https://openalex.org/W3128723389",
        "https://openalex.org/W3179297424",
        "https://openalex.org/W3093079359",
        "https://openalex.org/W2946747499",
        "https://openalex.org/W2741851275",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3037923959",
        "https://openalex.org/W3163894936",
        "https://openalex.org/W2908106554",
        "https://openalex.org/W2783855081",
        "https://openalex.org/W3100506510",
        "https://openalex.org/W3195983806",
        "https://openalex.org/W3093804672",
        "https://openalex.org/W2998792609",
        "https://openalex.org/W3108651391",
        "https://openalex.org/W6680437308",
        "https://openalex.org/W2943407549",
        "https://openalex.org/W2981138657",
        "https://openalex.org/W2511135850",
        "https://openalex.org/W2605287558",
        "https://openalex.org/W3047381720",
        "https://openalex.org/W3171125843",
        "https://openalex.org/W2139763424",
        "https://openalex.org/W3136038792",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2102384126",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3180214828",
        "https://openalex.org/W2963371610",
        "https://openalex.org/W2963842104",
        "https://openalex.org/W2962793481",
        "https://openalex.org/W4287330514",
        "https://openalex.org/W4294558607",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2984145721",
        "https://openalex.org/W2187089797",
        "https://openalex.org/W4288944547",
        "https://openalex.org/W4287115214",
        "https://openalex.org/W3173909755",
        "https://openalex.org/W1955857676"
    ],
    "abstract": "Group re-identification (G-ReID) focuses on associating the group images containing the same persons under different cameras. The key challenge of G-ReID is that all the cases of the intra-group member and layout variations are hard to exhaust. To this end, we propose a novel uncertainty modeling, which treats each image as a distribution depending on the current member and layout, then digs out potential group features by random samplings. Based on potential and original group features, uncertainty modeling can learn better decision boundaries, which is implemented by two modules, member variation module (MVM) and layout variation module (LVM). Furthermore, we propose a novel second-order transformer framework (SOT), which is inspired by the fact that the position modeling in the transformer is coped with the G-ReID task. SOT is composed of the intra-member module and inter-member module. Specifically, the intra-member module extracts the first-order token for each member, and then the inter-member module learns a second-order token as a group feature by the above first-order tokens, which can be regarded as the token of tokens. A large number of experiments have been conducted on three available datasets, including CSG, DukeGroup and RoadGroup. The experimental results show that the proposed SOT outperforms all previous state-of-the-art methods.",
    "full_text": "Uncertainty Modeling with Second-Order Transformer for Group\nRe-identification\nQuan Zhang1, Jian-Huang Lai1,2,3,4*, Zhanxiang Feng1, Xiaohua Xie1,2,3\n1School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, China\n2Guangdong Key Laboratory of Information Security Technology, Guangzhou, China\n3Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China\n4Key Laboratory of Video and Image Intelligent Analysis and Applicaiton Technology, Ministry of Public Security, China\nzhangq48@mail2.sysu.edu.cn, {stsljh, fengzhx7, xiexiaoh6}@mail.sysu.edu.cn\nAbstract\nGroup re-identification (G-ReID) focuses on associating the\ngroup images containing the same persons under different\ncameras. The key challenge of G-ReID is that all the cases\nof the intra-group member and layout variations are hard to\nexhaust. To this end, we propose a novel uncertainty model-\ning, which treats each image as a distribution depending on\nthe current member and layout, then digs out potential group\nfeatures through random sampling. Based on potential and\noriginal group features, uncertainty modeling can learn bet-\nter decision boundaries, which is implemented by the mem-\nber variation module (MVM) and layout variation module\n(LVM). Furthermore, we propose a novel second-order trans-\nformer framework (SOT), which is inspired by the fact that\nthe position modeling in the transformer is coped with the G-\nReID task. SOT is composed of the intra-member module and\ninter-member module. Specifically, the intra-member module\nextracts the first-order token for each member, and then the\ninter-member module learns a second-order token as a group\nfeature by the above first-order tokens, which can be regarded\nas the token of tokens. A large number of experiments have\nbeen conducted on three available datasets, including CSG,\nDukeGroup and RoadGroup, which show that the proposed\nSOT outperforms all previous state-of-the-art methods.\nIntroduction\nGroup re-identification (G-ReID) aims to associate group\nimages containing the same members under different cam-\neras with non-overlapping views based on their similarity.\nG-ReID usually focuses on groups of 2 ∼6 members, and\nimages belonging to the same group class should contain\nat least 60% same members. G-ReID is a more critical and\nchallenging task than person re-id because people usually\nhave group and social attributes, which indicates people pre-\nfer group moving in most real scenes. Therefore, G-ReID\nneeds to deal with the member and layout variation. Specifi-\ncally, the member variation means the number of intra-group\nmembers could decrease due to the member leaving or serve\nocclusion, and the layout variation means that the spatial po-\nsitions may change under different cameras.\nAlthough there are some pioneering works (Huang et al.\n2021; Lin et al. 2021; Zhu et al. 2020; Yan et al. 2020) based\n*Corresponding Author.\nCopyright © 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nExisting Methods\nMember \nFeature\nC\nNN\nGroup\nFeatur\ne\nGNN\nMember \nFeature\nT\nrans-\nformer\nMVM\nLVM\nGroup\nFeature\nT\nrans-\nformer\nOurs Method (SOT)\nCertainty Modeling\nUncertainty Modeling\nData space\nFeatur\ne space\nClass 1 Class 2\nFigure 1: Certainty modeling versus uncertainty modeling.\nThe pure triangles and squares represent the feature repre-\nsentations of the corresponding images. The textured trian-\ngles and squares represent potential group features mined\nfrom the original image through the MVM and LVM. The\ndotted circles represent the class boundaries learned from\nthe given images by certainty modeling. The dotted lines\nrepresent the decision boundaries between the two classes.\non deep learning to address the above challenges, the per-\nformances are not satisfactory. The shortcomings are mainly\ndue to the following two reasons. 1) The features extracted\nby the existing works are the specific features of the group\nimage under the fixed member and layout. As shown in Fig.\n1, The class boundaries learned from the orange triangles\nand green squares are the local representations of the whole\nclasses, which leads to the fact that the decision boundary\n(red dotted line) based on the local boundaries cannot dis-\ntinguish well between two classes. 2) The existing models\nare based on a combined framework of CNN and GNN,\nwhich are weak to describe the group layout feature due to\nthe drawback of structure itself in position modeling, so the\nperformances are limited.\nIn this paper, we propose a novel uncertainty modeling,\nwhich is motivated by the fact that variations of member and\nlayout contained in each group are infinitely diverse. All sit-\nuations cannot be exhausted no matter how elaborately sam-\npling from the real world. Therefore, uncertainty is an inher-\nent attribute of the group image that cannot disappear by col-\nlecting large-scale data. The proposed uncertainty modeling\ntreats each group image as a distribution rather than a spe-\ncific sample, and then digs out several potential group fea-\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n3318\ntures of the current group under other possible members and\nlayouts by dynamically sampling on the distribution. Two\nmodules, the member variation module (MVM) and layout\nvariation module (LVM), are designed to construct the spe-\ncific probability distribution for each image. As shown in\nFig. 1, the group feature (triangles and squares) learned with\nuncertainty modeling are closer to the true boundary and\nconsistent with the real-world distribution. Training and op-\ntimizing this true boundary can obtain more separable deci-\nsion boundaries and more robust feature representations.\nSpecifically, the proposed MVM defines a random vari-\nable p to describe the probability distribution of intra-group\nmember variation. A standard formp is constructed with the\nfollowing properties. First, the group members tend to main-\ntain stability when they occur across multi-cameras. Second,\nthe variation probability will decrease with the increase of\ndisappeared members when variation happens occasionally.\nConsidering that the input does not always contain all the\nmembers of its group class, we will dynamically constrain\nthe standard p to fit each image.\nLVM focuses on the layout variation of each member. Be-\ncause it is hard to exhaust all the spatial positions, LVM nor-\nmalizes all possible spatial positions under a certain num-\nber of members into the same layout feature. To this end, a\nlearnable memory bank M is designed to describe the lay-\nout features. For a group with j members, the j-th column\nof M is adopted as the normalized layout feature for each\nmember. The advantage of LVM is that normalized layout\ncan avoid oversampling on continuous position distribution.\nFurthermore, we propose a Second-Order Transformer\nmodel (SOT) inspired by the position embedding in trans-\nformer, which is coped with layout feature in G-ReID. The\ntraditional CNN-GNN models lack spatial position model-\ning, which leads to low performance and can be overcome by\nour model. The proposed SOT consists of the intra-member\nand inter-member modules. For a group image, SOT crops\neach member firstly, and then partitions each member into\nseveral sub-patches. The intra-member module extracts the\nfirst-order token as each member feature by modeling the\nrelationship among sub-patches through the member feature\ntransformer. Then inter-member module models the group\nrelationship among members through uncertainty modeling\nand extracts the second-order token as the group feature\nthrough the group feature transformer which receives the\nfirst-order tokens and outputs token of tokens.\nOur contributions are summarized as follows.\n• We propose the uncertainty modeling, which regards\neach image as a distribution instead of a specific sample.\nUncertainty modeling aims to explore potential group\nvariations through random sampling on distributions,\nwhich is achieved by the proposed member variation\nmodule (MVM) and layout variation module (LVM).\n• We propose the second-order transformer (SOT), extract-\ning the token as the member feature and the token of to-\nkens as the group feature. SOT can efficiently extract the\nlayout feature, which is hard in the existing methods.\n• The SOT achieves the Rank-1/mAP of 91.7%/90.7%,\n72.7%/78.9%, and 86.4%/91.3% on CSG, DukeGroup,\nand RoadGroup datasets, outperforming the state-of-the-\nart method by 28.5%, 15.3%, and 1.9% on Rank-1.\nRelated Work\nPerson Re-identification. Person re-identification (ReID)\naims to associate individual pedestrians in a camera network\nwith non-overlapping views. Recently, many methods (Sun\net al. 2018; Wang et al. 2018; Dai et al. 2021; He et al.\n2021b; Bai et al. 2021; Zhao et al. 2021; Wu, Zhu, and Gong\n2022) based on deep learning have made significant progress\nin this field, including extracting more discriminative fea-\ntures and designing more suitable metrics. For example, OS-\nNet (Zhou et al. 2019) and OSNet-AIN (Zhou et al. 2021)\ndesigned a novel backbone that both consider the discrim-\ninative feature learning and the computational cost. AGW\n(Ye et al. 2021) proposed a weighted regularization triplet\nmetric learning method.\nHowever, the above works were not suitable for G-ReID,\nbecause these work only focused on the appearance feature\nof the individual pedestrian, and ignored the relationship be-\ntween intra-group members. The proposed SOT overcomes\nthe shortcomings of existing work, and explicitly models the\nnumber and layout relationships of members, which greatly\nimproves the performance.\nGroup Re-identification. Compared with ReID, G-ReID\nis less studied, and only a few pioneering works try to ad-\ndress this task. Some early works (Zheng, Gong, and Xi-\nang 2009; Cai, Takala, and Pietik ¨ainen 2010; Zhu, Chu,\nand Yu 2016; Lisanti et al. 2017) took the whole image as\nthe input of the model, and directly extracted group fea-\ntures. Because these works were based on hand-crafted fea-\ntures, and background information was considered, the per-\nformance was not satisfactory. Recently, CNN-based works\n(Mei et al. 2020, 2019, 2021) have become the mainstream\nresearch, which cropped the intra-group members, and then\nextracted the group features. For example, DotGNN (Huang\net al. 2019) adopted CycleGAN (Zhu et al. 2017) to ob-\ntain the style transfer, and then integrate the member fea-\ntures with GNN to extract group features. MRF (Xiao et al.\n2018; Lin et al. 2021) considered more granular member-\nships, and proposed a multi-order matching method to calcu-\nlate the similarity. GCGNN (Zhu et al. 2020) used K-nearest\nmembers to encode the each member, and then designed\ngroup context GNN to extract group features. MACG (Yan\net al. 2020) proposed a multi-attention context graph frame-\nwork which applied the complex attention mechanism to the\ngroup feature learning.\nThe performance of the above works are not satisfactory,\nmainly because: 1) They were based on the CNN and GNN\nframework, which were weak for modeling group layout; 2)\nThey belonged to the certainty modeling. The proposed SOT\ncan overcomes these shortcomings.\nTransformer. Transformer (Vaswani et al. 2017) was pro-\nposed to extract text features in NLP task, and then general-\nized to many CV tasks and achieved good performances. For\nexample, IPT (Chen et al. 2021) adopted the large-scale pre-\ntraining transformer to achieve good performance on many\n3319\nlow-level vision tasks. ViT (Dosovitskiy et al. 2021) is a pure\ntransformer which directly divided the image into several\npatches. SwinTransformer (Liu et al. 2021) achieved a sat-\nisfied performance on object detection. DETR (Carion et al.\n2020) proposed an end-to-end framework which combined\nthe encoder and decoder together on object detection. Tran-\nsReID (He et al. 2021a) first introduced the transformer into\nthe person re-identification. However, transformer has not\nreceived too much attention in the G-ReID. To this end, we\npropose the second-order transformer to deal with G-ReID.\nMethod\nIn this section, we firstly introduce the MVM and LVM of\nuncertainty modeling, and then describe the proposed SOT\nnetwork. Fig. 2 illustrates the method in detail.\nMember Variation Module (MVM)\nIn this paper, MVM aims to construct a specific probability\ndistribution for each image, and determines the existence of\nintra-group members by random sampling. Therefore, the\nkey issue is how to obtain the specific form of probabil-\nity distribution. We constrain the probability distribution to\nmeet the following two properties, so that it can simulate the\nvariations in the real-world scenes.\n• Stability: For a robust group, the number of intra-group\nmembers usually remains unchanged.\n• Randomness: When the robust group occasionally\nchange, the probability of changingZd members will de-\ncrease significantly as Zd increases.\nFormally, the probability distribution can be described as\nPr {p; Zc, Zt}, where Zt and Zc represent the number of\nmembers in the steady state and in the current image, and\nZt = Zc + Zd. We start with the trivial case Zc = Zt, and\nthe symbol Pr {p; Zt, Zt}can be abbreviated asPr {p}. Ac-\ncording to these two properties, the probability distribution\nfunction of p can be described as follows:\n{Pr {p = 0}= P0\nPr {0 < p⩽ pmax}=\n∫pmax\n0 f (p) dp = 1−P0\n, (1)\nwhere steady state probability P0 ∈(0, 1) determines the\nprobability that the group is in a stable state, cut-off proba-\nbility pmax ∈(0, 0.4] determines the upper bound of the p,\nand f(p) is the probability density function of p. Setting the\nupper bound of pmax to 0.4 is based on the member defini-\ntion of G-ReID for the same group class.\nNext, we derive the specific expression form of f(p).\nWe assume that the form of the f(p) follows the truncated\nGaussian distribution N(µ, σ) which is satisfied with “ran-\ndomness” of p. Due to the sampling space (−∞, +∞) of\nthe N(µ, σ) is not consistent with the sampling interval\n[0, pmax] of p, we impose the following two constraints on\nthe N(µ, σ). First, the probability of the N(µ, σ) sampling\nin the interval (−∞, 0) is mapped to the probability when\np = 0. Second, theµ+3σ in Gaussian distribution is mapped\nto pmax, which ensures thatPr {p ∈(pmax, +∞)}is a small\nprobability event. So far, p follows the conditional Gaussian\ndistribution N(µ, σ; P0, pmax) under the tolerable error.\nAfter that, the solution ofµ and σ can be obtained through\nthese two constrains, which can be described as follows:{∫0\n−∞\n1\n√\n2πσ e−(p−µ)2\n2σ2 dp = P0\nµ + 3σ = pmax\n. (2)\nSolving the Eq. 2, we can get the followings:∫0\n−∞\n1√\n2πσ e−(p−µ)2\n2σ2 dp = 1\n2\n[\n1 −erf\n( µ√\n2σ\n)]\n, (3)\nwhere erf(x) = 2√π\n∫x\n0 e−t2\ndt is Gauss error function, and\nerf(x) ∈[−1, 1]. Solving the Eq. 3, we can get the follow-\nings: µ√\n2σ = erf−1 (1 −2P0) . (4)\nFrom the Eq. 1, we can see the P0 ∈(0, 1). Therefore, Eq.\n4 is always satisfied because the interval of 1 −2P0 is in-\ncluded in the definition domain of erf−1(·). After that, the\nanalytical solutions for the µ and σ are described as follows:{\nσ = pmax√\n2er f−1(1−2P0)+3\nµ = pmax −3σ . (5)\nThe non-trivial case Zc<Zt is an extension of the above\nconclusion. The upper bound of the member variation needs\nto be corrected in order to meet the definition of G-ReID,\nwhich can be described and solved as follows:\nZc −Zcp\n′\nmax ⩾ Zt −Ztpmax (6)\n=⇒p\n′\nmax = max (0, 1 −(1 −pmax) Zt/Zc) , (7)\nwhere p\n′\nmax stands for the true cut-off probability under the\ncurrent image. In the non-trivial case, we use P0 and p\n′\nmax\nto solve the µ and σ via Eq. 5. In a training batch consist-\ning of several group images, we count the largest number of\nmembers contained in the current class as Zt. Each member\nis determined whether to change by the same p sampled on\nthe N(µ, σ; P0, p\n′\nmax), and the remaining members are then\nmodeled for layout features and group feature extraction.\nLayout Variation Module (LVM)\nCompared with MVM, the modeling of the layout is a more\nchallenging problem. Because the spatial position variation\nof each member is a continuous distribution in the image,\nwhich is hard to exhaust all cases. To this end, we propose\na normalized layout representation to avoid the above infi-\nnite enumerations. Specifically, a learnable layout embed-\nding bank M ∈RD×M0 is designed, and each column ofM\nrepresents the layout feature under a certain member num-\nber, where the D is the layout feature dimension and M0\nis the maximum number of group members in the training\nset. For a group with j remaining members after MVM, we\nselect the j-th column in M as the layout feature of each\nmember in the current group, which is shown in Fig. 2.\nIn the testing stage, if the members exceed M0, a ran-\ndom D-dim vector is used as the current layout feature. Be-\ncause the groups with more than M0 members are not satis-\nfied with the definition of G-ReID, and are regarded as the\ndistractors that needs to be discarded. Using a random lay-\nout feature can effectively reduce the similarity with other\ngroups, and avoid the wrong matching.\n3320\nGroup Feature \nTransformerMember\n \npatching\nMember Feature \nTransformer\nLVMMVM\nSliding \nwindow patching\nLayout \nEmbedding Bank\nLinear \nProject\nion\nTriplet\nLoss\nPersonID \nLoss\nTriplet \nLossFirst-order token\nPosition \nEmbedding\nSecond-order tokenSampling from the \nprobabi\nlity distribution\nSelect\nBatched \nImagesInter-member \nmodule\nIntra-member \nmodule\nGroupID \nLoss\nFirst-order \ntoken\nSecond-order\ntoken\nLayo\nut Embedding \nMember/Group Feature \nTransformer\nTransformer \nLayer\nTransformer \nLayer\n√\n√\n灅\nFirst-order token\nLayout \nEmb\nedding\nFigure 2: The illustrate of the proposed SOT. The pink square stands for the group feature, and the red, green and dark blue\nsquare represent the member features. Group and member feature transformer are consisted of transform layers with different\nL, and do not share parameters. ✓/×from MVM stands for the existence/disappearance of the current member.\nSecond-Order Transformer (SOT)\nThe whole structure of the SOT is shown in Fig. 2. The SOT\nis composed of the intra-member and inter-member mod-\nules. The intra-member module considers the feature ex-\ntraction of individual members in the group, and the inter-\nmember module focuses on the uncertainty modeling of\ngroup variation and the group feature extraction. We adopt a\ntransformer module to implement the extraction of member\nand group features, which is composed of different numbers\nof transformer layers. Notably, the member and group fea-\nture transformers do not share parameters.\nAs shown in Fig. 2, for a group image in the batch with\nN images, we first crop the region of each member ac-\ncording to the ground truth. Then, we send each member\npatch to the intra-member module and divide it into fixed-\nsize image sub-patches (specifically, 16×16), and add a first-\norder token and sequential position embeddings to these\nsub-patches. The first-order token is regarded as the mem-\nber feature after passing through the member feature trans-\nformer, which is supervised by the person identity and triplet\nloss function.\nLID = −1\nP\nP∑\nj=1\nC∑\ni=1\nyji log (ˆyji), (8)\nwhere P represents the total member number of the current\nbatch, C represents the total member classes, the indicator\nfunction yji = 1(j = i) equals to 1 when the j-th member\nbelongs to the i-th class, and ˆyji is the prediction of network\nabout the j-th member belongs to the i-th class.\nLTri = 1\nP\nP∑\ni=1\nmax\n(\nd\n(\nfi, f+\ni\n)\n−d\n(\nfi, f−\ni\n)\n+m, 0\n)\n, (9)\nwhere d(·, ·) represents the distance function between two\nfeatures such as the Euclidean distance, fi/f+\ni /f−\ni represent\nthe anchor/hard positive/hard negative feature in the current\nbatch, and m is the hyper-parameter of margin.\nLp = LID + LTri (10)\nIn the inter-member module, MVM dynamically samples\nprobability values from the proposed probability distribution\nin MVM for the current group, and then determines whether\nthe first-order token of each member is discarded. After that,\nLVM selects the corresponding column in the layout embed-\nding bank as the layout feature of each member according to\nthe number of remaining members. We add the second-order\ntoken as the token of these first-order tokens, and the second-\norder token can extract the group feature through the group\nfeature transformer. The loss function Lg of a second-order\ntoken is also composed of the group identity and triplet loss,\nwhich is similar to the LID and LTri . Overall, the whole\nloss function of the SOT is described as follows:\nL= Lp + Lg. (11)\nIt can be seen that in the training phase of the SOT, MVM\nand LVM will dynamically change the members and layout\nrepresentation of the current group, in order to mine more\npotential feature representations.\n3321\nMethod Publication CSG DukeGroup RoadGroup\nRank-1 Rank-5\nRank-10 mAP Rank-1 Rank-5\nRank-10 mAP Rank-1 Rank-5\nRank-10 mAP\nCRRRO-BR\nO BMVC 2009 10.4 25.8\n37.5 - 9.9 26.1\n40.2 - 17.8 34.6\n48.1 -\nCovariance ICPR 2010 16.5 34.1\n47.9 - 21.3 43.6\n60.4 - 38.0 61.0\n73.1 -\nPREF ICCV 2017 19.2 36.4\n51.8 - 30.6 55.3\n67.0 - 43.0 68.7\n77.9 -\nBSC+CM ICIP 2016 24.6 38.5\n55.1 - 23.1 44.3\n56.4 - 58.6 80.6\n87.4 -\nLIMI MM 2018 - -\n- - 47.4 68.1\n77.3 - 72.3 90.6\n94.1 -\nDOTGNN MM 2019 - -\n- - 53.4 72.7\n80.7 - 74.1 90.1\n92.6 -\nGCGNN TMM 2020 - -\n- - 53.6 77.0 91.4 - 81.7 94.3\n96.5 -\nMGR TCYB 2021 57.8 71.6\n76.5 - 48.4 75.2\n89.9 - 80.2 93.8\n96.3 -\nMACG TPAMI\n2020 63.2 75.4 79.7 - 57.4 79.0 90.3\n- 84.5 95.0 96.9 -\nSOT\n(Ours) - 91.7 96.5\n97.6 90.7 72.7 88.6\n93.2 78.9 86.4 96.3\n98.8 91.3\nTable 1: Comparison of the proposed method with the state-of-the-arts on CSG, DukeGroup and RoadGroup. The compared\nmethods are categorized into two groups, including hand-crafted methods and deep learning methods. The best and second best\nresults are shown in bold and underline respectively. The Rank-1, Rank-5, Rank-10 and mAP are reported(%).\nExperiments\nDatasets\nThe proposed SOT is evaluated on DukeGroup (Lin et al.\n2021), RoadGroup (Lin et al. 2021) and CSG (Yan et al.\n2020) datasets. The DukeGroup dataset contains 354 images\nincluding 177 group classes. The RoadGroup dataset con-\ntains 324 images including 162 group classes. Follow the\nprotocol in (Lin et al. 2021), the training and testing set of\nDukeGroup and RoadGroup are randomly and equally split.\nThe CSG dataset contains 3,839 images including 1,558\ngroup classes, where 859/699 groups are split for train-\ning/testing. Follow the protocol in (Yan et al. 2020), the im-\nages in the test set are sequentially selected as the probe,\nand all the remaining images are regarded as the gallery. In\naddition, CSG adds extra 5K group images as distractors in\nthe gallery. We do not use any extra datasets when training\non each G-ReID dataset for fair performance comparison.\nThe Cumulative Matching Characteristics (CMC) at Rank-\n1, Rank-5, Rank-10, and mean Average Precision (mAP) are\nused as evaluation metrics.\nDetails\nWe adopt ViT-Base (Dosovitskiy et al. 2021), pre-trained on\nImageNet (Deng et al. 2009), as the backbone of the member\nfeature transformer. We regard the SOT without LVM and\nMVM as the certainty modeling. For the input group image,\nwe crop all the member patches by the given bounding box\nand resize them to 256 ×128. In the training stage, the ran-\ndom horizontal flip and random erasing are performed with\na fixed probability of 0.5. Each mini-batch is sampled with\n16 group identities, and each group identity selects 4 im-\nages. We choose SGD (Bottou 2012) as the optimizer. Our\ntraining stage ends when the iteration number reaches 400\nepochs. We use a cosine annealing learning rate strategy.\nThe initial learning rate is 2e-3, and the minimum learning\nrate is 1.6e-4. The learning rate of the inter-member module\nis multiplied by 0.1. The weight decay is 1e-4. The selec-\ntion of hard samples in triplet loss adopts an online mining\nstrategy. In the testing stage, we do not use any data aug-\nmentation and re-ranking. We use the Euclidean distance to\nmeasure the normalized features. All ablation studies, pa-\nrameter analyses, and visualizations have been conducted on\nthe DukeGroup dataset if there is no additional comments.\nPerformance\nWe evaluate the proposed SOT method against the existing\nmethods on three available G-ReID datasets to show the su-\nperiority of our method. As shown in Table 1, we divide\nthe existing methods into two groups: hand-crafted G-ReID\nmethods including CRRRO-BRO (Zheng, Gong, and Xi-\nang 2009), Covariance (Cai, Takala, and Pietik ¨ainen 2010),\nPREF (Lisanti et al. 2017) and BSC+CM (Zhu, Chu, and Yu\n2016); deep learning G-ReID methods including LIMI (Xiao\net al. 2018), DOTGNN (Huang et al. 2019), GCGNN (Zhu\net al. 2020), MGR (Lin et al. 2021) and MACG (Yan et al.\n2020). The MACG is regarded as the state-of-the-art method\nin the existing methods according to the performance. Three\nconclusions can be drawn from Table 1.\nFirst, our SOT achieves very strong performance on CSG,\nDukeGroup and RoadGroup datasets, which far exceeds the\nMACG on Rank-1 and mAP. In the CSG datasets, the per-\nformance of our SOT achieves 91.7%/90.7% on Rank-1/\nmAP, and exceeds MACG by 28.5% on Rank-1. In the\nDukeGroup datasets, the performance of our SOT achieves\n72.7%/78.9% on Rank-1/mAP, and exceeds MACG by\n15.3% on Rank-1. In the RoadGroup datasets, the perfor-\nmance of our SOT achieves 86.4%/91.3% on Rank-1/mAP,\nand exceeds MACG by 1.9% on Rank-1. This shows that\nthe SOT brings different degrees of performance gain on all\ndatasets, proving that uncertainty is an attribute of group im-\nages and does not disappear as the data size increases. This\nalso shows that the SOT overcomes the inherent uncertainty\nof group images and brings significant improvements.\nSecond, the performances of the methods based on the\nhand-crafted features are relatively low. Different from these\nworks, the proposed SOT crops each member in the group\nto avoid background interference. Furthermore, the SOT de-\nsigns a second-order transformer to extract the feature of\neach member and the whole group, which is more robust\nand discriminative.\nFinally, the performances of deep learning methods are\nstill unsatisfactory, which is caused by the certainty model-\ning and insufficient layout modeling. Different from these\n3322\nCSG DukeGroup RoadGroup\nMVM L\nVM Rank1 mAP Rank1 mAP Rank1 mAP\n85.56 84\n.40 65.91 75.00 83.95 89.10\n✓ 88.92 87\n.31 67.05 75.10 85.19 89.89\n✓ 90.26 88\n.92 68.18 75.55 85.19 89.70\n✓\n✓ 91.70 90\n.70 72.73 78.90 86.40 91.30\nTable 2: Ablation study of the proposed SOT (%).\np = 0.1 p = 0.4 unif orm ours\n65\n70\n75\n80Performance (%)\nRank-1 mAP\n(a) Probability distribution.\nGNN our s\n65\n70\n75\n80Performance (%)\nRank-1 mAP (b) Inter-member Module.\nFigure 3: Comparisons with alternative variants.\nworks, the SOT designs the uncertainty modeling which\nmines potential group features by treating each image as a\nspecific distribution. In addition, the proposed SOT lever-\nages the transformer-based network to extract the member\nand group features, which is more suitable for G-ReID.\nAblation Study\nEffect of MVM and LVM. The ablation experiment\nmainly shows the effect of two proposed modules, MVM\nand LVM, on uncertainty modeling. We mainly analyze the\nresults on the DukeGroup, and there are similar conclusions\non the other two datasets. As shown in Table 2, two con-\nclusions can be drawn. First, each module can improve the\nperformance when used alone. Compared with the baseline\nmodel, MVM increased by +1.14%/+0.1% on Rank-1/mAP,\nand LVM increased by +2.27%/+0.55% on Rank-1/mAP.\nThis shows that each module digs for potential member vari-\nation and layout variation, respectively, making SOT more\ndiscriminative. Second, when two modules are both used,\nthe performance gain, +6.82%/+3.90% on Rank-1/mAP, is\nhigher than than the sum of individual modules. This shows\nthat member and layout variations are two complementary\naspects for modeling group variations. Using MVM and\nLVM simultaneously can explore more potential group fea-\ntures.\nAlternative Probability Distribution. In order to verify\nthe effectiveness of the proposed probability distribution\nN(µ, σ; P0, p\n′\nmax), we choose other distributions as compar-\nisons, including fixed probability p = 0.1 and p = 0.4, and\nuniform distribution in the interval[0, 0.4]. As shown in Fig.\n3(a), our distribution is better than other alternative distribu-\ntions. The proposed distribution is coped with the real scene,\ndue to the constraints of “stability” and “randomness”. In\naddition, we construct a specific distribution for each image,\nwhich alleviates the class confusion caused by the change of\nmembers.\nLayout modeling\nstrategy Rank-1 mAP\nno embedding 67.05 75.04\nrandom\nsequential embedding 68.18 76.62\nnormalized\nembedding (Ours) 72.73 78.90\nTable 3: Several alternative layout feature modeling (%).\nAlternative Inter-member Module. To verify that group\nfeature transformer is more suitable for G-ReID, we se-\nlect a classical GNN (Hamilton, Ying, and Leskovec 2017)\nmodel for comparison. As shown in Fig. 3(b), the existing\nGNN lacks the extraction of member layout information and\nonly learns group features through member appearance fea-\ntures, which is not sufficient and robust. On the contrary, our\nmethod can model layout features when extract group fea-\ntures, thus, it can bring more performance gain and is more\nsuitable for G-ReID than GNN.\nAlternative Layout Feature Modeling. We design sev-\neral other layout feature modeling strategies and compare\nthem with our method. No embedding means that the layout\nembedding bank in LVM is discarded, but we extract group\nfeatures through member appearance features. Random se-\nquential embedding is similar to the position embedding in\nViT, which means that we assign group members a random\nsequence to enumerate possible layout situations. As shown\nin Table 3, row 1 not only ignores layout variations but also\nignores the discrimination information contained in layout\nfeatures themselves. Row 2 proves that the layout variations\ncannot be exhausted by finite enumeration, so the perfor-\nmance is also limited. Our method brings the best perfor-\nmance when describing the layout uncertainty.\nParameter Analysis\nInfluence of P0. P0 controls the effect of stability in\nMVM, which determines that each input maintains the cur-\nrent members and layout by probability P0 during train-\ning. When P0 is too small, the stability of the SOT cannot\nbe guaranteed, which lead to the performance degradation;\nwhen P0 is too large, the SOT is too stable to ignore the min-\ning of potential features, which lead to insufficient model\ngeneralization and performance degradation. As shown in\nFig. 4(a), we set P0 = 0.5 for the best performance.\nInfluence of pmax. pmax controls the effect of random-\nness in MVM, which controls the maximum variation prob-\nability of each member in the current group during training,\nand also reflects the drastic degree of member variation. As\nshown in Fig. 4(a), the drastic degree of member variation\nbecomes greater with the increase of pmax, and the poten-\ntial group features mined are more and more diverse. When\npmax reaches 0.3, the model achieves the best performance,\nso we set pmax = 0.3. When pmax further increases, some\nunexpected variations that cause confusion of group class\nwill happen, which lead to the performance degradation.\nInfluence of L. We analyze the relationship between the\nnumber of layers in the group feature transformer and per-\nformance and show the results in Fig. 4(c). When L = 0,\nthere is no connection between the second-order token and\n3323\n0.1 0.3 0.5 0. 7 0. 9\nP0\n65\n70\n75\n80Performance (%)\nRank-1 mAP\n(a) Influence of P0.\n0.1 0.2 0.3 0. 4\npmax\n70\n75\n80Performance (%)\nRank-1 mAP (b) Influence of pmax.\n0 1 2 3\nL\n0\n65\n70\n75\n80Performance (%)\nRank-1 mAP\n(c) Influence of L.\n0.0 0.1 0.2 0. 3 0. 4\nm\n70\n75\n80Performance (%)\nRank-1 mAP (d) Influence of m.\nFigure 4: Parameter analysis of the proposed SOT.\nQuery Rank-1 ĺ\u0003Ra nk-3\n Rank-1 ĺ\u0003Ra nk-3\nRetrieval by certainty modeling Retrieval by SOT\nFigure 5: The top-three retrieval visualization of the cer-\ntainty modeling and the SOT. The red/green boxes indicate\nthe correct/wrong matches. In the DukeGroup dataset, each\nquery only has one correct matching in the gallery.\nmember features. Therefore, the performance is very low.\nWith the increase of L, the performance starts to improve\nand achieves the best with L = 2. WhenL further increases\nto 3, there are too many parameters resulting in over-fitting,\nwhich reduces the performance.\nInfluence of m. Margin m controls both intra-member/\ngroup-class consistency and inter-member/group-class dis-\ncrepancy. We select five different values {0, 0.1, 0.2, 0.3,\n0.4}to analyze the effect of m on the performance. As\nshown in Fig. 4(d), we set m = 0.3 as the best performance\nof the SOT.\nVisualization\nRetrieval visualization. Fig. 5 shows the top-three re-\ntrieval visualization of the certainty modeling and the pro-\nposed SOT. The advantages of SOT are reflected in the fol-\nlowing two aspects. 1) Certainty modeling tends to search\nID0\nID\n1\nID2\nID3\nID4\nID5\n(a) Certainty modeling.\nID0\nID\n1\nID2\nID3\nID4\nID5 (b) SOT.\nFigure 6: The feature visualization of the whole training\nset through t-SNE (van der Maaten and Hinton 2008). Each\ncolor represents a group class.\nfor images with similar layouts and cannot model layout\nvariations. For example, certainty modeling tends to search\nfor images with a horizontal layout in row 1 and images with\na tight layout in row 2. However, SOT can get the correct re-\nsults under the large layout differences between query and\ngallery. 2) Certainty modeling tends to search for the images\nwith the same number with query. For example, certainty\nmodeling tends to search for images with four members in\nrow 3. However, SOT extracts similar group features from\nthe images with different members of the same group.\nFeature visualization. Fig. 6 shows the feature distribu-\ntion visualization of six classes in the training set when the\ncertainty modeling and the proposed SOT are trained to con-\nverge. It is worth noting that there are only two images in\neach group class. In Fig. 6(a), certainty modeling learns poor\ndecision boundaries (cyan and purple circles, blue and red\ncircles) due to the lack of potential group features mining.\nWith the advantages of uncertainty modeling, the feature\ndistribution of the SOT shows obvious intra-class consis-\ntency and inter-class discrepancy.\nConclusion\nIn this paper, we focus on member and layout variation in\nG-ReID. To this end, firstly, we propose a novel uncertainty\nmethod to model the variation of intra-group members and\nlayout. The advantage of uncertainty modeling is that lots of\npotential group features can be explored and the training of\nthe model can be promoted. Secondly, we propose a second-\norder transformer (SOT) to extract the features of individ-\nual members and groups, respectively. Finally, the proposed\nSOT achieves the state-of-the-art performance on multiple\ndatasets, which greatly exceeds the existing methods.\nAcknowledgments\nThis project was supported by the NSFC (62076258), the\nProject of Natural Resources Department of Guangdong\nProvince ([2021]34), and the Project of Ministry of Public\nSecurity of China (2019GABJC39).\n3324\nReferences\nBai, Y .; Jiao, J.; Ce, W.; Liu, J.; Lou, Y .; Feng, X.; and Duan,\nL.-Y . 2021. Person30K: A Dual-Meta Generalization Net-\nwork for Person Re-Identification. In CVPR, 2123–2132.\nBottou, L. 2012. Stochastic gradient descent tricks. In Neu-\nral networks: Tricks of the trade, 421–436.\nCai, Y .; Takala, V .; and Pietik¨ainen, M. 2010. Matching\nGroups of People by Covariance Descriptor. InICPR, 2744–\n2747.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In ECCV, 213–229.\nChen, H.; Wang, Y .; Guo, T.; Xu, C.; Deng, Y .; Liu, Z.; Ma,\nS.; Xu, C.; Xu, C.; and Gao, W. 2021. Pre-Trained Image\nProcessing Transformer. In CVPR, 12299–12310.\nDai, Y .; Li, X.; Liu, J.; Tong, Z.; and Duan, L.-Y . 2021. Gen-\neralizable Person Re-Identification With Relevance-Aware\nMixture of Experts. In CVPR, 16145–16154.\nDeng, J.; Dong, W.; Socher, R.; Li, L.; Li, K.; and Li, F.\n2009. ImageNet: A large-scale hierarchical image database.\nIn CVPR, 248–255.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In ICLR.\nHamilton, W.; Ying, Z.; and Leskovec, J. 2017. Inductive\nRepresentation Learning on Large Graphs. In NeurIPS, vol-\nume 30.\nHe, S.; Luo, H.; Wang, P.; Wang, F.; Li, H.; and Jiang,\nW. 2021a. TransReID: Transformer-Based Object Re-\nIdentification. In ICCV, 15013–15022.\nHe, T.; Shen, X.; Huang, J.; Chen, Z.; and Hua, X.-S. 2021b.\nPartial Person Re-Identification With Part-Part Correspon-\ndence Learning. In CVPR, 9105–9115.\nHuang, Z.; Wang, Z.; Hu, W.; Lin, C.; and Satoh, S. 2019.\nDoT-GNN: Domain-Transferred Graph Neural Network for\nGroup Re-identification. In ACMMM, 1888–1896.\nHuang, Z.; Wang, Z.; Tsai, C.-C.; Satoh, S.; and Lin, C.-\nW. 2021. DotSCN: Group Re-Identification via Domain-\nTransferred Single and Couple Representation Learning.\nIEEE TCSVT, 31(7): 2739–2750.\nLin, W.; Li, Y .; Xiao, H.; See, J.; Zou, J.; Xiong, H.; Wang,\nJ.; and Mei, T. 2021. Group Reidentification with Multi-\ngrained Matching and Integration. IEEE TCYB, 51(3):\n1478–1492.\nLisanti, G.; Martinel, N.; Bimbo, A. D.; and Foresti, G. L.\n2017. Group Re-identification via Unsupervised Transfer of\nSparse Features Encoding. In ICCV, 2468–2477.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin Transformer: Hierarchical Vi-\nsion Transformer Using Shifted Windows. InICCV, 10012–\n10022.\nMei, L.; Lai, J.; Feng, Z.; and Xie, X. 2020. From pedes-\ntrian to group retrieval via siamese network and correlation.\nNeurocomputing, 412: 447–460.\nMei, L.; Lai, J.; Feng, Z.; and Xie, X. 2021. Open-World\nGroup Retrieval with Ambiguity Removal: A Benchmark.\nIn ICPR, 584–591. IEEE.\nMei, L.; Lai, J.; Xie, X.; Zhu, J.; and Chen, J.\n2019. Illumination-invariance optical flow estimation us-\ning weighted regularization transform. IEEE TCSVT, 30(2):\n495–508.\nSun, Y .; Zheng, L.; Yang, Y .; Tian, Q.; and Wang, S. 2018.\nBeyond Part Models: Person Retrieval with Refined Part\nPooling (and A Strong Convolutional Baseline). In ECCV,\n501–518.\nvan der Maaten, L.; and Hinton, G. 2008. Visualizing Data\nusing t-SNE. JMLR, 9(86): 2579–2605.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017.\nAttention is All you Need. In NeurIPS, volume 30.\nWang, G.; Yuan, Y .; Chen, X.; Li, J.; and Zhou, X. 2018.\nLearning Discriminative Features with Multiple Granulari-\nties for Person Re-Identification. In ACMMM, 274–282.\nWu, G.; Zhu, X.; and Gong, S. 2022. Learning hybrid rank-\ning representation for person re-identification. PR, 121:\n108239.\nXiao, H.; Lin, W.; Sheng, B.; Lu, K.; Yan, J.; Wang, J.; Ding,\nE.; Zhang, Y .; and Xiong, H. 2018. Group Re-Identification:\nLeveraging and Integrating Multi-Grain Information. In\nACMMM, 192–200.\nYan, Y .; Qin, J.; Ni, B.; Chen, J.; Liu, L.; Zhu, F.; Zheng, W.-\nS.; Yang, X.; and Shao, L. 2020. Learning Multi-Attention\nContext Graph for Group-Based Re-Identification. IEEE\nTPAMI, 1–1.\nYe, M.; Shen, J.; Lin, G.; Xiang, T.; Shao, L.; and Hoi, S. C.\n2021. Deep Learning for Person Re-identification: A Survey\nand Outlook. IEEE TPAMI.\nZhao, Y .; Zhong, Z.; Yang, F.; Luo, Z.; Lin, Y .; Li, S.; and\nSebe, N. 2021. Learning to Generalize Unseen Domains via\nMemory-based Multi-Source Meta-Learning for Person Re-\nIdentification. In CVPR, 6277–6286.\nZheng, W.; Gong, S.; and Xiang, T. 2009. Associating\nGroups of People. In BMVC, 1–11.\nZhou, K.; Yang, Y .; Cavallaro, A.; and Xiang, T. 2019.\nOmni-Scale Feature Learning for Person Re-Identification.\nIn ICCV, 3701–3711.\nZhou, K.; Yang, Y .; Cavallaro, A.; and Xiang, T. 2021.\nLearning Generalisable Omni-Scale Representations for\nPerson Re-Identification. IEEE TPAMI.\nZhu, F.; Chu, Q.; and Yu, N. 2016. Consistent match-\ning based on boosted salience channels for group re-\nidentification. In ICIP, 4279–4283.\nZhu, J.; Park, T.; Isola, P.; and Efros, A. A. 2017. Unpaired\nImage-to-Image Translation Using Cycle-Consistent Adver-\nsarial Networks. In ICCV, 2242–2251.\nZhu, J.; Yang, H.; Lin, W.; Liu, N.; Wang, J.; and Zhang, W.\n2020. Group Re-identification with Group Context Graph\nNeural Networks. IEEE TMM, 1–1.\n3325"
}